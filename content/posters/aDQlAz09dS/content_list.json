[{"type": "text", "text": "Efficient Contextual LLM Cascades through Budget-Constrained Policy Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Xuechen Zhang ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Zijian Huang   \nUniversity of Michigan   \nAnn Arbor, MI   \nzijianh@umich.edu ", "page_idx": 0}, {"type": "text", "text": "University of Michigan Ann Arbor, MI zxuechen@umich.edu ", "page_idx": 0}, {"type": "text", "text": "Ege Onur Taga   \nUniversity of Michigan   \nAnn Arbor, MI   \negetaga@umich.edu Carlee Joe-Wong   \nCarnegie Mellon University Pittsburgh, PA   \ncjoewong@andrew.cmu.edu   \nSamet Oymak   \nUniversity of Michigan   \nAnn Arbor, MI   \noymak@umich.edu   \nJiasi Chen   \nUniversity of Michigan   \nAnn Arbor, MI   \njiasi@umich.edu ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recent successes in natural language processing have led to the proliferation of large language models (LLMs) by multiple providers. Each LLM offering has different inference accuracy, monetary cost, and latency, and their accuracy further depends on the exact wording of the question (i.e., the specific prompt). At the same time, users often have a limit on monetary budget and latency to answer all their questions, and they do not know which LLMs to choose for each question to meet their accuracy and long term budget requirements. To navigate this rich design space, we propose TREACLE (Thrifty Reasoning via ContextAware LLM and Prompt Selection), a reinforcement learning policy that jointly selects the model and prompting scheme while respecting the user\u2019s monetary cost and latency constraints. TREACLE uses the problem context, including question text embeddings (reflecting the type or difficulty of a query) and the response history (reflecting the consistency of previous responses) to make smart decisions. Our evaluations on standard reasoning datasets (GSM8K, CSQA, and LLC) with various LLMs and prompts show that TREACLE enables cost savings of up to $85\\%$ compared to baselines, while maintaining high accuracy. Importantly, it provides the user with the ability to gracefully trade off accuracy for cost. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The success of large language models (LLMs) in recent years has led to a explosion of heterogeneous models and providers, including as Meta\u2019s Llama, OpenAI\u2019s ChatGPT, and Google\u2019s Gemini. As LLMs continue to proliferate in the near future, we envisage a generative AI marketplace with a large variety of providers, LLMs, and deployments. Notably, LLMs have widely varying capabilities and costs: capabilities in terms of accuracy in responding to different types of queries, and cost in terms of monetary price and query latency. As an illustration, the accuracy versus cost tradeoffs of various Llama and GPT LLMs are shown in Figure 1 on grade school math word problems [4]. As can be seen, GPT-3.5 tends to have lower accuracy than GPT-4 ", "page_idx": 0}, {"type": "image", "img_path": "aDQlAz09dS/tmp/008d1c0479af28ba6d7d2fe9096315605d48b3215b9c12e949f3a043be7a7019.jpg", "img_caption": ["Figure 1: TREACLE chooses LLMs to achieve high accuracy and ${\\sim}85\\%$ cost reduction, compared to individual LLMs. "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "$79\\%$ vs $92\\%$ respectively), but costs about 20 times less. This heterogeneous array of LLMs can bewilder users who must choose between them. ", "page_idx": 1}, {"type": "text", "text": "Another challenge is that the specific prompt included in the question plays a critical role in eliciting accurate responses. This is especially true for reasoning problems where prompting a model to explain its reasoning can produce more accurate, but often more costly, answers. Chain-of-thought (CoT) [17] is an example of such a prompting scheme, in which the question includes a few examples of worked out problems, which cost more (due to the additional words included in the question) but also produce more accurate responses. For example, in Figure 1, GPT-4 with CoT (pink triangle) achieves a $92\\%$ accuracy, compared to GPT-4 with a domain expert prompt (brown dot, reminding the LLM that it is a \u201cmath solver\u201d) that achieves $83\\%$ . However, using the CoT prompt costs $3.9\\times$ more due to the extra words included in the query. A final challenge is that the optimal choice of LLM and prompt depends on the specific question being asked; the accuracy of a particular LLM and prompt combination for a particular question is unknown in advance, requiring learning or prediction. ", "page_idx": 1}, {"type": "text", "text": "Thus, the heterogeneity of the LLM landscape and the tradeoffs between accuracy and cost make it challenging to determine the optimal strategy of: Which LLM to select and how to prompt it, in order to answer all questions while respecting cost constraints? To address this, we propose a Thrifty Reasoning via Context-Aware LLM and Prompt Selection (TREACLE) framework. TREACLE is a learning-based approach that solves reasoning questions by automatically selecting which LLM model and prompt to use for each question. Given a cost budget, including a total monetary price across all questions and an average per-query latency, its goal is to maximize the average accuracy of the responses. As shown in Figure 1, TREACLE achieves the Pareto front of individual LLMs by combining them intelligently. ", "page_idx": 1}, {"type": "text", "text": "Several recent works utilize multiple LLMs during inference with a cascade design, where queries propagate through a cascade of LLMs, considering the LLMs\u2019 accuracy-cost tradeoffs. Most aim to maximize accuracy and lack an explicit way to control long-term costs, as TREACLE has. By posing the problem of LLM and prompt selection as a budget-constrained policy optimization, TREACLE provides a unified approach to efficient LLM cascades (see Table 1). TREACLE makes informed decisions based on the full context of the LLM cascade, including the query embedding, answer statistics, and remaining budget. Overall, this paper makes the following contributions: ", "page_idx": 1}, {"type": "text", "text": "\u2022 Characterization of the accuracy, monetary cost, and latency of LLMs. To understand the trade-offs between the LLMs, we quantify the accuracy and cost of 5 different LLMs (Llama and GPT variants) with 3 different prompt strategies (standard, domain expert, and CoT) on 3 datasets (GSM8K, CSQA, and LLC). ", "page_idx": 1}, {"type": "text", "text": "\u2022 An adaptive LLM and prompt selection policy based on reinforcement learning. TREACLE dynamically chooses the right LLM and prompt for each question. It does this by leveraging context about the current question, re-querying the models if needed to verify the consistency of the responses, and thinking ahead about the remaining budget. We also provide some theoretical justification for TREACLE\u2019s key design choices. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Extensive evaluations. We show that TREACLE substantially saves on cost while maintaining high accuracy on mathematical and commonsense reasoning tasks. We demonstrate its robustness to different budgets, question difficulty, price changes, new LLMs, and new unseen task types. ", "page_idx": 1}, {"type": "text", "text": "The paper is organized as follows. We describe related work (\u00a72), the problem statement (\u00a73), and our framework (\u00a74). We then describe our experiments (\u00a75) and conclusions (\u00a76). ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "FrugalGPT [3] is perhaps the closest to this work, as they considered a similar cost-constrained LLM selection problem with a threshold-based policy to select from a sorted list of LLMs. Our approach differs in several key aspects: we utilize a reinforcement learning policy that chooses both LLMs and prompts, rather than a threshold-based scheme; we utilize the full context of the current question to make decisions, including the text embedding of the current question and the history of past responses; and our method can re-query the same LLM and aggregate previous responses to estimate the correctness of the current response. Mixture of Thought [20] explored the idea of response consistency in order to choose the right LLMs. The intuition is that higher consistency in the re-queries implies higher confidence in the correctness of the response. TREACLE employs response consistency as an input feature, along with other features, for LLM selection. AutoMix [7] introduces a \u201cmeta-verifier\u201d to estimate whether a response is correct or a more powerful LLM is needed. Both works measure cost as a by-product of combining multiple LLMs rather than long-term constraint across all questions, as we do. Other lines of work include uncertainty estimation or prompt engineering to improve accuracy [6, 18, 20, 13, 1, 9], which is complementary to our work. The related work is summarized in Table 1. ", "page_idx": 1}, {"type": "table", "img_path": "aDQlAz09dS/tmp/efd998d3e68f761324103063e5683a1f5ff766e5e016fa67574068b5499bcd88.jpg", "table_caption": ["Table 1: Comparison to related works. "], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "3 Problem Statement ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We study the natural language query problem of providing correct responses to a series of questions. We focus on reasoning problems (e.g., grade school math problems) because they are challenging with multiple logical steps required to reach a final correct response. The problem involves answering a sequence of $n$ questions $\\mathcal{Q}$ with correct responses $\\boldsymbol{\\wp}$ ; in other words, we have a set of questions and responses $\\{(Q_{1},Y_{1}),(Q_{2},Y_{2}),\\ldots,(Q_{n},Y_{n})\\}$ . We have a set $\\mathcal{M}$ of language models (LLMs) at our disposal, which can be accessed either locally or remotely through APIs: $\\bar{\\mathcal{M}}=\\{M_{1},M_{2},\\ldots,M_{m}\\}$ . Also, we have a choice between $p$ prompt types, $\\mathcal{P}=\\{\\dot{P}_{1},\\dot{P}_{2},\\dot{.}..,P_{p}\\}$ . These models and prompts have different costs (in terms of latency and monetary price) and accuracy. Each \u201cquestion\u201d can be asked multiple times to the same or different LLMs, which we call a \u201cre-query\u201d, in order to possibly obtain a more final accurate response. ", "page_idx": 2}, {"type": "text", "text": "The goal is to ensure that as many responses as possible are correct, while simultaneously minimizing the associated costs. This problem can be formulated as a Constrained Markov Decision Process (CMDP), which is represented by a tuple $(\\mathcal{Q},S,\\mathcal{A},T,r,c,\\gamma,B)$ , where $\\mathcal{Q}$ is the ordered question set; $\\boldsymbol{S}$ is the state space; $T:(\\mathcal{Q},\\mathcal{S})\\times\\mathcal{A}\\times(\\mathcal{Q},\\mathcal{S})\\rightarrow[0,1]$ is the transition probability function, i.e., $(Q,s)_{t+1}\\sim\\bar{T}(\\cdot|(Q,s)_{t},\\stackrel{.}{a}_{t})$ ; $r:\\left(\\mathcal{Q},S\\right)\\times\\mathcal{A}\\to\\left[R_{\\operatorname*{min}},R_{\\operatorname*{max}}\\right]$ and $c:(\\bar{\\mathcal{Q}},\\mathcal{S})\\to{\\bar{\\mathbb{R}}}^{+}$ denote the reward and cost function; $\\gamma\\in[0,1]$ is the discount factor for future reward and cost; and $B$ is the total budget. A policy $\\pi:(\\mathcal{Q},\\mathcal{S})\\to P(A)$ maps the question-state pairs to a probability distribution over actions. A trajectory $\\tau$ is composed of a sequence of (question, state)-action pairs: $\\tau=\\left\\{\\tau_{(Q,s)_{0}},\\tau_{a_{0}},\\tau_{\\underline{{{(Q,s)_{1}}}}},\\tau_{a_{1}},...,\\tau_{(Q,s)_{L}},\\tau_{a_{L}}\\right\\}$ , where $L$ is the total number of times the LLMs are queried. Note that $L\\geq{\\dot{n}}$ , due to the possible re-queries. The cumulative reward and cumulative cost of trajectory $\\tau$ are denoted as $\\begin{array}{r}{R(\\tau)=\\sum_{t=0}^{L}\\gamma^{t}r(\\tau_{(Q,s)_{t}},\\tau_{a_{t}})}\\end{array}$ and $\\begin{array}{r}{C(\\tau)=\\sum_{t=0}^{L}\\gamma^{t}c(\\tau_{(Q,s)_{t}},\\tau_{a_{t}})}\\end{array}$ , respectively. The goal of our problem is to learn a policy $\\pi$ from $\\mathcal{D}$ that maximizes the expected cumulative reward, while satisfying the cumulative cost at the trajectory level: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\pi}\\mathbb{E}_{\\tau\\sim\\pi,T}[R(\\tau)],\\ \\mathrm{s.t.}\\ \\forall\\tau\\sim\\pi,T\\quad C(\\tau)\\leq B.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\tau\\sim\\pi,T$ denotes that $\\tau$ is generated by executing $\\pi$ in $T$ . By grouping the rewards and costs by question instead of enumerating all re-queries, the cumulative reward and cost can be re-written as $\\begin{array}{r}{R(\\tau)=\\sum_{i=1}^{n}{\\tt r e w a r d}(Y_{i},\\hat{Y_{i}})}\\end{array}$ and $\\begin{array}{r}{C(\\tau)=\\sum_{i=1}^{n}\\tt{c o s t}(Q_{i})}\\end{array}$ , where ${\\hat{Y}}_{i}$ is the final response for question $Q_{i}$ , reward $(\\cdot)$ is the function that measures the correctness of the final response, $\\mathsf{c o s t}(\\cdot)$ is the cost function of giving a final response ${\\hat{Y}}_{i}$ for question $Q_{i}$ . Cost functions. We consider two types of costs in this work, monetary price and latency, resulting in two types of cost functions. $^{(I)}$ Pure monetary price. LLMs can run remotely, where the monetary price per token is set by the provider (e.g., OpenAI). LLMs can also run locally, where the monetary price depends on a number of factors such as capital expenditures, server cooling and maintenance, electricity, etc. In our setup, the GPT models run remotely and the Llama models, which are free and open-source, run locally. (2) Monetary price-latency combination. Monetary price is important for some users (e.g., small companies) while latency plays a more crucial role in other settings (e.g., real-time voice assistants). Users who are latency-sensitive may be willing to pay more for lower latency, whereas others might be more patient and prefer lower prices. TREACLE allows users to choose the trade-off between monetary cost and latency by adjusting a trade-off coefficient $\\beta$ , where cost $=$ latency $+\\,\\beta\\ast$ monetary price. ", "page_idx": 2}, {"type": "image", "img_path": "aDQlAz09dS/tmp/8cef6901a9a2087dd569f7459c4ef5fb35207c0da10a0026592520532e255143.jpg", "img_caption": ["Figure 2: Overview of TREACLE framework. TREACLE decides on the next (LLM, prompt) to query in a context-aware fashion, summarized in the state variable. It can adapt to unseen tasks by projecting the new queries into the text embedding space. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "4 Proposed Framework: TREACLE ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We propose the TREACLE framework, depicted in Figure 2. Let the possible unique combinations of language models and prompts be denoted by $\\{(\\bar{M}+P)_{1},(M+\\bar{P})_{2},\\dots,(\\bar{M}+P)_{K}\\}$ , where $K\\,\\leq\\,m p$ . When a new question $Q_{i}$ arrives, TREACLE starts by selecting a model $M\\ \\in\\ M$ and choosing an associated prompt $P\\in\\mathcal P$ to generate a response, denoted as ${\\hat{O}}_{i}\\,=\\,M(P(Q_{i}))$ . TREACLE returns this as the final response ${\\hat{Y}}_{i}$ for this question if it has a high degree of confidence in its correctness, and deducts the cost of the question and its response from the total budget $B$ . Otherwise, TREACLE can select another LLM $M$ and prompt $P$ (whose choice may be informed by the result of all previously chosen models, prompts, and their responses) and re-query. This iterative process continues until TREACLE returns a final response (based on its learned policy). TREACLE then proceeds to the next question with the remaining budget and repeats the process, until all questions have been answered or there is no remaining budget. We model the problem as a Markov decision process as described in Section 3. ", "page_idx": 3}, {"type": "text", "text": "States. The state vector contains the following information: ", "page_idx": 3}, {"type": "text", "text": "\u2022 Response consistency: Records all previous responses and the normalized frequency of their occurrences. The intuition is that the consistency of the previous responses can be used as a measure of confidence in the response correctness [16, 7].   \n\u2022 Input and output length: The number of tokens in the current query and any preceding responses to the same query. This helps TREACLE understand the monetary price of each query and response, which can differ for each query. It also helps capture the difficulty, as question with longer input or output tend to be harder.   \n\u2022 Current question\u2019s text embedding: Intuitively, we want to capture the question type or difficulty, which can impact the model and prompt selection decision. TREACLE does this using a text embedding of the query [5].   \n\u2022 Number of re-queries: The number of re-queries for each model-prompt pair helps TREACLE decide whether to re-query again or move to the next question.   \n\u2022 Normalized remaining budget: Based on the remaining budget, we compute the estimated number of queries for each model prompt pair as follows: $\\begin{array}{r l r l}{\\boldsymbol{B}_{k}}&{{}}&{=}&{{}}\\end{array}$ (# questions remaintiontagl) r(eamvga icnoinstg  pbeur dqgueetry of (M + P )k). The average cost per query is estimated based on the questions seen so far. If there is a large remaining budget, TREACLE may consider re-querying with large models.   \nActions. The action space $\\boldsymbol{\\mathcal{A}}$ consists of the following:   \n\u2022 Action $a_{1}$ : Return the current response for $Q_{i}$ and proceed to the next question $Q_{i+1}$ . If no models have been queried yet and this action is chosen, it is equivalent to skipping the question. \u2022 Action $a_{2}$ : Re-query the same model-prompt pair $(M+P)$ for $Q_{i}$ .   \n\u2022 Action $a_{3}$ : Select a new model-prompt pair $(M+P)^{\\prime}$ for $Q_{i}$ . ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "By allowing re-querying (action $a_{2}$ ), the current action influences the next state, by impacting the question under consideration and thus the relevant state features, making this a non-trivial MDP. For $a_{3}$ , we constrained the set of possible model-prompt pairs to a sorted list. In other words, instead of allowing TREACLE to select any possible model and prompting scheme, we sort the $(M+P)_{k}$ in ascending order of accuracy to cost ratio and only allow TREACLE to select the next element in this list $(M+P)_{k+1}$ . The ordering is based on Proposition 1 (discussed below), ", "page_idx": 4}, {"type": "text", "text": "Rewards. The reward function assigns a positive reward to correct responses. Specifically, $r_{\\tau_{a}}\\left(\\tau_{(Q,s)},\\tau_{(Q^{\\prime},s^{\\prime})}\\right)\\,=\\,\\mathbb{P}\\left[\\hat{Y}=Y|\\tau_{a}=a_{1}\\right]\\,+\\,\\lambda\\mathbb{P}\\left[\\hat{O}=Y|\\tau_{a}\\in\\{a_{1},a_{2},a_{3}\\}\\right]$ . For a given question, this combines the accuracy of the final response $\\hat{Y}$ with the accuracy of the current response $\\hat{O}$ (if there have been re-queries), with a scaling factor $\\lambda$ between the two terms. We introduced the second term because without it, we observed that if TREACLE repeatedly chose action $a_{2}$ (re-querying), this would result in multiple state transitions with 0 reward, until the final response was returned. In other words, including the second term avoids the issue of sparse rewards that resulted from the first term alone. Note that the correct response $Y$ is known only when training TREACLE; during test, the policy executes using the expected reward calculated by the trained policy. ", "page_idx": 4}, {"type": "text", "text": "Design choices and justifications. We next discuss two key design choices of TREACLE and their theoretical motivation. Proofs are in Appendix B. ", "page_idx": 4}, {"type": "text", "text": "$(I)$ How should the LLMs and prompts be ordered in the cascade? Recall that action $a_{3}$ moves to the next $(M+P)$ in the cascade. What is the best ordering of $(M+P)?$ Consider the following simplified setting. Suppose each of the $(M+P)$ have a probability of correct response $p_{k}$ and cost $c_{k}$ . If we had access to an oracle that could tell us when the response of a particular $(M+P)_{k}$ is incorrect, we could then move on and try the same question with the next option $(M+P)_{k+1}$ in the cascade. We could achieve the highest accuracy using the oracle, and would only have to worry about minimizing the cost to avoid exceeding the budget. In this setting, Proposition 1 below states that the best ordering of the $(M+P)$ options is according to the ratio $\\displaystyle{\\frac{p_{k}}{c_{k}}}$ . ", "page_idx": 4}, {"type": "text", "text": "Proposition 1. With $K$ (LLM, prompt) options, each with probability of correct answer $p_{k}$ and cost $c_{k}$ , ordering the options according to their cost-normalized accuracies $\\displaystyle{\\frac{p_{k}}{c_{k}}}$ minimizes the total cost. ", "page_idx": 4}, {"type": "text", "text": "This Proposition motivates TREACLE\u2019s ordering of the $(M+P)$ options in the cascade according to their accuracy and cost ratio. This is intuitive: instead of placing the most accurate (LLM, prompt) option early in the cascade, which might incur large cost, we first query LLMs that have high accuracy per unit cost. Note that although the setup of Proposition 1 differs from Equation (1), as the cost is the objective rather than a constraint, the trajectory resulting from the ordering in Proposition 1 is also a solution to Equation (1). ", "page_idx": 4}, {"type": "text", "text": "(2) Do policies that consider response consistency perform well? Recall that \u201cresponse consistency\u201d is one of the features in the state vector. We seek to understand the performance of policies that consider this feature; a simple such policy is described in Definition 1 below. It returns a final response to a question if the same response value is repeated $w$ times. ", "page_idx": 4}, {"type": "text", "text": "Definition 1. For each question $Q_{i}$ , an $w$ -consistent policy $(w\\geq2)$ ) sets the final response $\\hat{Y_{i}}=\\hat{O}_{i}$ as soon as $\\exists\\,\\hat{O}_{i}:\\mathsf{c o u n t}(\\hat{O}_{i})=w$ . If no such $\\hat{O}_{i}$ exists, fall back to $w-1,w-2$ , etc.   \nDefinition 2 below characterizes how likely the $(M+P)$ are to return an incorrect response. A question can be asked $\\Omega$ times to the $(M+P)$ options in the cascade, which may not be unique due to the re-queries.   \nDefinition 2. Denote the $\\Omega$ LLM-prompt options by $(M\\!+P)_{j=1}^{\\Omega}$ . Let $\\mathbb{P}(M_{j}(P_{j}(Q_{i})))$ be the output distribution of $(M+P)_{j}$ on problem $Q_{i}$ . Let $\\begin{array}{r}{\\epsilon:=\\sum_{i=1}^{n}\\operatorname*{sup}_{1\\le j\\le\\Omega}\\sum_{\\hat{O}\\neq Y_{i}}\\mathbb{P}(M_{j}(P_{j}(Q_{i}))=\\hat{O})^{2}}\\end{array}$ . With the definitions in hand, we can now lower bound the performance of a 2-consistent policy compared to the optimal learned algorithm in Proposition 2 below. Without loss of generality, we study the case when the reward function is the accuracy. ", "page_idx": 4}, {"type": "text", "text": "Proposition 2. For the problem stated in Equation (1) that achieves $C_{*}$ , the optimal expected accuracy subject to budget constraints, there exists a 2-consistent policy that achieves an accuracy of at least $\\begin{array}{r}{C_{*}-\\frac{1}{2}\\Omega^{2}\\epsilon}\\end{array}$ . ", "page_idx": 4}, {"type": "text", "text": "In other words, even a simple policy that allows for re-querying and considers response consistency can achieve close to the optimal reward. This motivates TREACLE inclusion of \u201cresponse consistency\u201d as a state feature. The proposition applies generally and allows for budgets or text embeddings, and does not require the $(M+P)$ to return accurate responses. Experimentally, we find that our learned RL policy is similar to a 2-consistent policy, as $93.02\\%$ of the responses are 2-consistent (GSM8K dataset, $\\mathbb{S}0.30$ budget, $\\begin{array}{r}{\\alpha=\\frac{1}{20}\\$ ). This suggests that our learned policy may not be far from optimal. ", "page_idx": 5}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We first describe the experiment setup (\u00a75.1) and then the main results (\u00a75.2). Specifically, we examine robustness to new LLMs and changing API prices (\u00a75.2.1), shifts in question difficulty (\u00a75.2.2), and different reasoning datasets $(\\S5.2.3)$ . ", "page_idx": 5}, {"type": "text", "text": "5.1 Experiment Setup ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We summarize the experiment setup, with full details in Appendix A. We use three representative datasets: GSM8K [4], which contains 8.5K high quality grade school math problems created by human writers; CSQA [11], which consists of 12102 multiple choice commonsense reasoning questions encountered in daily life; and LLC [17], where the task is to concatenate the last letters of words in a name (e.g., \u201cAmy Brown\u201d $\\rightarrow$ \u201cyn\u201d). To evaluate our methods, we perform two steps. ", "page_idx": 5}, {"type": "text", "text": "(1) Collect query-response pairs for (LLM, prompt) combinations. We collected query-response pairs from each dataset for different combinations of LLM, prompt, and LLM temperature. We used 5 different LLMs: Llama-2-7b-chat, Llama-2-13b-chat [15], GPT-3.5-turbo, GPT-4, and GPT-4- turbo [10]. These models are of varying sizes (7b, 13b, 154b and 1.76t respectively). The Llama models are open-source and run locally on our servers, while the GPT models rely on commercial APIs. We employ several prompting schemes. A prompt generally consists of two parts: the \u201ccontent message\u201d containing the question, and the \u201csystem message\u201d with additional context. ", "page_idx": 5}, {"type": "text", "text": "\u2022 The plain text prompt submits the questions to the LLM as the content message. ", "page_idx": 5}, {"type": "text", "text": "\u2022 The domain expert prompt feeds information about the question\u2019s domain as a system message (e.g., \u201cmath solver\u201d), and keeping the user\u2019s content message as plain text.   \n\u2022 The standard few-shot prompt includes a system message (\u201cFollow the given examples and answer the question\u201d [17]) and the content message, which consists of few-shot examples together with the plain text prompt.   \n\u2022 The Chain-of-Thought (CoT) few-shot prompt [17] adds some intermediate explanations to the few-shot examples. ", "page_idx": 5}, {"type": "text", "text": "(2) Train TREACLE with the query-response pairs. We used Deep Q-Network (DQN) [8] to train the reinforcement learning (RL) policy in TREACLE, consisting of a two-layer neural network. For the monetary prices, we use the published per-token prices for the GPT models. Since our local Llama deployments do not have API costs, we set Llama-2-7b\u2019s price as $\\alpha$ times Llama-2-13b\u2019s price, and Llama-2-13b\u2019s price as $\\alpha$ times GPT-3.5-turbo\u2019s price. $\\alpha$ varies betwee n110, 210 or510. Our pricing is grounded in reality and similar to actual market rates, as the offered price for Llama is approximately $15\\%$ of GPT-3.5-turbo according to current providers [14]. For the latency-accuracy tradeoff, we evaluate different trade-off parameters $\\beta=[\\bar{5}0\\mathbf{k},500\\mathbf{k},1\\mathbf{M}]$ in the cost function. ", "page_idx": 5}, {"type": "text", "text": "We evaluated the following baseline methods, reproducing the methods as faithfully as possible with a common set of LLMs and prompt options. ", "page_idx": 5}, {"type": "text", "text": "\u2022 FrugalGPT [3]. We reproduce FrugalGPT, which uses a DistilBERT model [12] to estimate the response accuracy. If this estimate is below a threshold, the next LLM in the cascade is queried. This baseline shows how TREACLE compares to the state-of-the-art that lacks re-querying. \u2022 Calibrated cascade. We build on FrugalGPT\u2019s response accuracy estimation and develop a 2-layer neural network, which takes as input TREACLE\u2019s state vector and outputs the estimated response accuracy. If this estimated accuracy is below a threshold, the next LLM in the cascade is queried. This baseline compares TREACLE to a modified FrugalGPT. \u2022 Majority Voting. For each query, we output the final response based on the majority vote from $\\Omega$ re-queries, based on [16, 20]. We set $N=2$ based on the best empirical results. The (LLM, prompt) combinations are progressively queried until their per-question budget runs out. This baseline allows comparison with TREACLE\u2019s response consistency feature in the state vector. ", "page_idx": 5}, {"type": "image", "img_path": "aDQlAz09dS/tmp/98de5c36beeb2d11c25e27914f275a4fcc45c99703e3c5f60f7e06f41ba7c56a.jpg", "img_caption": ["Figure 3: The performance of various methods for different cost functions and budget constraints. The dashed lines are methods that have ground knowledge, which is impractical but illustrates the best achievable performance. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "\u2022 Offilne and online knapsack. We formulate a multiple choice knapsack problem where the items are the $(M+P)$ combinations. We solve the offline knapsack to find the optimal solution when re-queries are not allowed, and also implement an online version [2]. These baselines show how TREACLE compares to methods with perfect knowledge of question costs and accuracy. ", "page_idx": 6}, {"type": "text", "text": "5.2 Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To evaluate the performance of TREACLE, we conduct experiments for different total budgets and cost functions. The results are presented in Figure 3 (additional results on CSQA and LLC are in Appendix C.4.) Across different settings of $\\alpha,\\beta$ , and total budget, TREACLE consistently outperforms the baselines and is close to the Offilne Knapsack\u2013 an approach not feasible in practical deployments. We note that the relatively good performance of the Calibrated Cascade is due to it using the same state vector we designed for TREACLE. We make the following additional observations. ", "page_idx": 6}, {"type": "text", "text": "\u2022 Observation 1: TREACLE can adapt to different budgets and cost parameters. All the results in Figure 3, with different budgets and $\\alpha,\\beta$ parameters were produced after training TREACLE only once. This highlights TREACLE\u2019s adaptability to different cost function variations. ", "page_idx": 6}, {"type": "text", "text": "\u2022 Observation 2: For limited budgets, TREACLE only answers questions that are more likely to produce accurate responses. For example, for $\\beta=50\\mathrm{k}$ in Figure 3, when the budget is only $\\mathbb{S}0.05$ and insufficient for all queries, $52.7\\%$ of the questions TREACLE chooses to answer are correct. For context, the cheapest model (Llama-2-7b) can only answer $23.65\\%$ of questions correctly. This suggests TREACLE can evaluate question difficulty and opt not to respond to some questions. ", "page_idx": 6}, {"type": "text", "text": "\u2022 Observation 3: For larger budgets, TREACLE chooses more powerful (LLM, prompt) combinations. This is shown in Figure 4a, where as the budget increases, the more powerful models (right side of $\\mathbf{X}$ -axis) are increasingly selected. Interestingly, we observe that for budgets $\\mathbb{S}0.3$ to $\\mathbb{S}10$ , the Llama-2-13b model is queried approximately once per question, despite its suboptimal performance. Even with these larger budgets, it\u2019s still beneficial to query Llama before moving onto more powerful models, to see whether its responses are consistent. ", "page_idx": 6}, {"type": "image", "img_path": "aDQlAz09dS/tmp/074cef0de1ba5e1a60f6566759ce7605ff9dd4ce221fcb59626dc2acc203a90b.jpg", "img_caption": ["Figure 4: Number of times each model is re-queried. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "\u2022 Observation 4: Re-querying helps. We trained both TREACLE and the Calibrated Cascade Algorithm baseline without the ability to re-query. The results are shown in Figure 5, where the dashed line represents method variants that permits re-querying. We observed a notable decrease in accuracy when re-querying was not allowed. Methods without re-querying eventually achieved comparable accuracy with those with re-querying capability, but with significantly larger budgets. ", "page_idx": 7}, {"type": "text", "text": "\u2022 Observation 5: TREACLE\u2019s choice of model and prompt is impacted by relative LLM prices. As the relative cost of Llama models decreases $\\alpha$ decreases), TREACLE increasingly utilizes Llama to answer queries, allowing for cost savings, as shown in Figure 4b. This shift enables use of more expensive models like GPT-4 when tackling complex problems, thereby enhancing overall accuracy. When Llama becomes more expensive, TREACLE no longer chooses it. This aligns with our intuition that using Llama to verify response consistency becomes less economical. ", "page_idx": 7}, {"type": "image", "img_path": "aDQlAz09dS/tmp/f03952f82cfebc49a01055cdd3460bf88692e1000756788f1b0b3b6e6e308f89.jpg", "img_caption": ["Figure 5: With and without re-querying. $\\begin{array}{r}{\\alpha=\\frac{1}{20}}\\end{array}$ . "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "5.2.1 Addition of new LLMs ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "LLM development is rapid, with better models continuously emerging, and the API prices set by providers can change at any time. TREACLE\u2019s ability to react to such changes is thus an important practical consideration. We show that TREACLE can adapt by fine-tuning itself using few samples. We study two types of LLM updates. (1) API price adjustment: In November 2023, OpenAI released GPT-4-turbo, offering performance on par with GPT-4 but at a more affordable price. Concurrently, the price for ", "page_idx": 7}, {"type": "image", "img_path": "aDQlAz09dS/tmp/61d7ee8c5c5488ce896406a55662c29e616dbdc64214ba4565c48a27e649ac70.jpg", "img_caption": ["Figure 6: Performance with new LLMs and lowered prices. Lines and dots in light (dark) colors are results with old (new) prices and LLMs. $\\begin{array}{r}{\\bar{\\alpha}=\\frac{1}{10}}\\end{array}$ "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "GPT-3.5-turbo was lowered. (2) Fine-tuned open-source LLMs: Several domain-specific fine-tuned models with higher accuracy have been released. Specifically, we exchanged Llama-2 for MetaMath [19], which is fine-tuned specifically for GSM8K. For both scenarios, we partitioned the GSM8K test data into $80\\%$ validation and $20\\%$ test samples, generated new state-action trajectories from the validation set, then fine-tuned TREACLE on these new trajectories. To create a comparable baseline, we also fine-tuned FrugalGPT\u2019s DistilBERT. ", "page_idx": 7}, {"type": "text", "text": "Firstly, we show the performance of TREACLE with both the API price adjustments and improved LLMs in Figure 6. The individual points on the plot illustrate the changes in the API prices for gpt-3.5-turbo. The lines show the performance of the new TREACLE with new models and prices and the old TREACLE (from previous subsections). The new TREACLE can achieve the peak accuracy with only a $\\mathbb{S}1$ budget, clearly benefiting from the new models and lowered prices. Benefits are also significant for lower budgets, where the improved TREACLE has significantly higher accuracy, because the lowest performing Llama-2 models were replaced by finetuned Metamaths. Finally, for FrugalGPT that relies on a fine-tuned DistilBERT accuracy estimator, performance didn\u2019t improve and can even degrade due to distribution shifts and overfitting. ", "page_idx": 7}, {"type": "image", "img_path": "aDQlAz09dS/tmp/7f1f5f02eaa17a284b2c0684bd50393228a8d2717194f9f09a1e1ff24800be31.jpg", "img_caption": ["Figure 8: Performance on \u201ceasy\u201d and \u201chard\u201d partitions of the test set. Models are trained on original training data, but must handle a distribution shift in difficulty during test. $\\begin{array}{r}{\\alpha=\\frac{1}{20}}\\end{array}$ . "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Secondly, in Figure 7 we investigate the sample efficiency of finetuning the model with new API prices and LLMs (\u201cFine-tune\u201d in the figure) compared to training TREACLE from scratch with the new prices and LLMs (\u201cScratch\u201d). The sample efficiency is important because it can be expensive to collect query-response pairs from new LLMs to further train TREACLE. The results indicate that when there are minor changes to the available LLMs, deploying the previously trained TREACLE can be sufficient. ", "page_idx": 8}, {"type": "image", "img_path": "aDQlAz09dS/tmp/33420f40b3b5ee9a308780f282007941264e1e3ef1b895b2b06fa6159404db9b.jpg", "img_caption": ["Figure 7: Sample complexity for different budgets with new LLMs. \u03b1 = $\\begin{array}{r}{\\alpha=\\frac{1}{10}}\\end{array}$ . "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "For instance, in Figure 7a when there is limited budget $(\\mathbb{S}0.15)$ and upgrades to the expensive models, deploying the previously trained TREACLE (# samples $=0$ ) achieves comparable performance to the fine-tuning TREACLE (# samples $=800$ ). On the other hand, when upgrades are introduced to cheaper models (Figure 7b), deploying the old TREACLE may initially result in poor accuracy, but TREACLE can quickly adapt to the new LLM options by fine-tuning with a few number of samples (around 300). ", "page_idx": 8}, {"type": "text", "text": "5.2.2 Shifts in Question Difficulty ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Thus far in the evaluations, easier and harder questions were randomly mixed throughout the training and test sets. In practice, question difficulty may not be uniformly distributed, so we study two types of difficulty distribution shifts: shifts across the training/test sets, and towards the end of the test set. ", "page_idx": 8}, {"type": "text", "text": "Difficulty shifts between training and test. We divided the GSM8K test set into \u201chard\u201d and \u201ceasy\u201d subsets based on the question difficulty. The difficulty is defined by the number of LLM models correctly answering the questions (more models answering a question correctly roughly means it is easier). Basic performance on the easy and hard questions is shown in Figure 8c. When the questions are hard, each question ends up consuming too much budget, leaving insufficient budget for subsequent questions that then go unanswered. The single model baseline does well in terms of cost and unanswered questions, but has low accuracy. We plot the performance for variable budgets in Figure 8, and find that TREACLE\u2019s accuracy remains stable, no matter whether the test distribution shifts to an easier level or a harder level. This is because TREACLE can dynamically adjust based on the remaining budget in online fashion. ", "page_idx": 8}, {"type": "text", "text": "Difficulty shifts within the test set. To further evaluate the robustness to question difficulty shifts, we test TREACLE with the full test set sorted from easy-to-hard queries or hard-to-easy queries. The hope is that with the help of query text embedding in the state vector (which should capture some estimate of difficulty), TREACLE can remain relatively stable in terms of accuracy even if the ordering of the questions changes. This hypothesis is borne out in Figure 9, while Online Knapsack performs significantly worse than TREACLE if the questions are sorted from hard to easy. This is because much of the budget is wasted on the difficult queries that arrive at the beginning. ", "page_idx": 8}, {"type": "image", "img_path": "aDQlAz09dS/tmp/aa67a9a8a59aac88fc9bf58964ab4656bc2c286a606d5aa821d33121ecaee310.jpg", "img_caption": [], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Figure 9: TREACLE is robust to   \nre-ordered question difficulty in   \nthe test set. $\\begin{array}{r}{\\dot{\\alpha}=\\frac{1}{20}}\\end{array}$ 20 Figure 10: Performance with different types of reasoning tasks. ", "page_idx": 9}, {"type": "text", "text": "5.2.3 Different types of reasoning tasks ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Mixture of tasks. We seek to examine whether one model can handle multiple types of tasks under one common budget (in contrast to the previous experiments with a specialized model for each task). We trained a single model with all 3 datasets and recorded the test accuracy on those datasets. The results shown in Figure 10a for \u201cTREACLE (all tasks)\u201d, offline knapsack, and online knapsack are the test accuracy from an equal mix of CSQA, GSM8K and LLC queries. \u201cTREACLE (individual tasks)\u201d is the test accuracy on the same mix of queries, using the models from previous subsections, where each model (corresponding to a task) is assigned to 1/3 of the common budget. \u201cTREACLE (all tasks)\u201d can handle a mixture of tasks under a common budget (e.g., outperforming online knapsack), and can significantly outperform the individual tasks baseline (\u201cTREACLE (individual tasks)\u201d) by effectively allocating its common budget across queries of different types. ", "page_idx": 9}, {"type": "text", "text": "New unseen task type. Consider the scenario where the model has not been trained on certain new tasks. To show that TREACLE can adapt to new tasks easily, we performed additional experiments. The base model is trained using the CSQA dataset, and the unseen new tasks are queries from GSM8K. Interestingly, in our design, we decouple decision making from the task embedding as follows. To transfer from CSQA to GSM8K, we freeze the base RL policy of CSQA (the decision making part), and fine-tune the \u201ctext embedding\u201d feature in the state vector (see bottom pink part of Figure 2). How many samples are needed to fine-tune the text embedding for the new task? As shown in Figure 10b, with a budget of 0.6, the original model fully-trained on GSM8K (\u201ctrain on GSM8K\u201d) achieves a test accuracy of 0.848, compared to 0.78 when trained on CSQA and fine-tuned with only 200 additional samples from GSM8K (\u201cfine-tune on 200 GSM8K\u201d). This highlights a relatively small accuracy loss when transferring to new types of unseen tasks. The results suggest that our method can easily adapt to new tasks with only a small amount of additional training. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We propose TREACLE, a learning-based LLM querying framework that intelligently chooses between LLM and prompt combinations based on question context and past response history. Our experiments show that TREACLE outperforms other baselines and is robust to different budgets, LLM availability and prices, and so on. For future work, we plan to incorporate other features such as privacy into the cost function. We hope our framework can help spur cost-efficient utilization of LLM systems. ", "page_idx": 9}, {"type": "text", "text": "Limitations. Our work focuses on reasoning problems, and could be extended to generative problems by incorporating new measures of response consistency. The RL policy\u2019s budget does not account for the cost of collecting the training data. We plan to freely release the datasets and code so that others can train the same basic policy and adapt that policy to new future LLMs and tasks (as shown through our experiments). ", "page_idx": 9}, {"type": "text", "text": "Broader impact. This work can make LLMs more accessible to cost-sensitive users. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported in part by an Adobe Data Science Research award, NSF CCF-2046816, a gift from Google Research, and credits from the Microsoft Accelerating Foundation Models Research grant program. Thank you to Dr. Koyel Mukherjee for helpful discussions and insights on this work. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Zefan Cai, Baobao Chang, and Wenjuan Han. Human-in-the-loop through chain-of-thought, 2023. [2] Deeparnab Chakrabarty, Yunhong Zhou, and Rajan Lukose. Online knapsack problems. In Workshop on internet and network economics (WINE), 2008.   \n[3] Lingjiao Chen, Matei Zaharia, and James Zou. Frugalgpt: How to use large language models while reducing cost and improving performance. arXiv preprint arXiv:2305.05176, 2023. [4] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [5] Ryan Greene, Ted Sanders, Lilian Weng, and Aarving Neelakantan. New and improved embedding model, 2022. Accessed: 2023-10-27.   \n[6] Stephanie Lin, Jacob Hilton, and Owain Evans. Teaching models to express their uncertainty in words, 2022.   \n[7] Aman Madaan, Pranjal Aggarwal, Ankit Anand, Srividya Pranavi Potharaju, Swaroop Mishra, Pei Zhou, Aditya Gupta, Dheeraj Rajagopal, Karthik Kappaganthu, Yiming Yang, Shyam Upadhyay, Mausam, and Manaal Faruqui. Automix: Automatically mixing language models, 2023.   \n[8] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. nature, 518(7540):529\u2013533, 2015.   \n[9] Ranjita Naik, Varun Chandrasekaran, Mert Yuksekgonul, Hamid Palangi, and Besmira Nushi. Diversity of thought improves reasoning abilities of large language models. arXiv preprint arXiv:2310.07088, 2023.   \n[10] OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023.   \n[11] Amrita Saha, Vardaan Pahuja, Mitesh Khapra, Karthik Sankaranarayanan, and Sarath Chandar. Complex sequential question answering: Towards learning to converse over linked question answer pairs with a knowledge graph. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018.   \n[12] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.   \n[13] Chenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng Wang, Jordan Boyd-Graber, and Lijuan Wang. Prompting gpt-3 to be reliable, 2023.   \n[14] together.ai. together pricing. https://www.together.ai/pricing.   \n[15] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \n[16] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.   \n[17] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824\u201324837, 2022.   \n[18] Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, and Bryan Hooi. Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms, 2023. ", "page_idx": 10}, {"type": "text", "text": "[19] Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models. arXiv preprint arXiv:2309.12284, 2023. ", "page_idx": 11}, {"type": "text", "text": "[20] Murong Yue, Jie Zhao, Min Zhang, Du Liang, and Ziyu Yao. Large language model cascades with mix-ture of thought representations for cost-efficient reasoning. arXiv preprint arXiv:2310.03094, 2023. ", "page_idx": 11}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "A Experiment Setup and Implementation Details ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "A.1 Datasets ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "We use three representative datasets for the experiments. ", "page_idx": 11}, {"type": "text", "text": "\u2022 GSM8K [4]: The Grade School Math 8K dataset contains $8.5\\mathrm{K}$ high quality grade school math problems created by human writers, in which 7.5K are in the training data and 1K are in the testing data. We further split the 7.5K training data into 6K training data and 1.5K validation data.   \n\u2022 CSQA [11]: The Complex Sequential Question Answering dataset consists of 12102 multiple choice commonsense reasoning questions encountered in daily life. The training set, validation set, and testing set contain 9741, 1221 and 1140 samples respectively.   \n\u2022 LLC [17] The Last Letter Concatenation task is to concatenate the last letters of words in a name (e.g., \u201cAmy Brown\u201d $\\rightarrow$ \u201cyn\u201d). ", "page_idx": 11}, {"type": "text", "text": "A.2 Data collection and training ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "To evaluate our methods, we perform two steps: (1) Collect query-response pairs for different combinations of LLMs and prompt, then (2) train TREACLE with these pairs. ", "page_idx": 11}, {"type": "text", "text": "(1) Collecting query-response pairs. We collect query-response pairs from each dataset for different combinations of LLM, prompt, and LLM temperature. The accuracy, latency, and monetary price of the best combinations are shown in Figure 11, with full results in Table 3 in the Appendix. We selected those combinations according to Proposition 1. ", "page_idx": 11}, {"type": "text", "text": "LLMs. We used 5 different LLMs: Llama-2-7b-chat, Llama-2-13b-chat [15], GPT-3.5-turbo, GPT-4, and GPT-4-turbo [10]. These models are of varying sizes (7b, 13b, 154b and 1.76t respectively). The Llama models are open-source and run locally on our servers (one A40 GPU for Llama-2-7b and two A40 for Llama-2-13b), while the GPT models rely on commercial APIs. ", "page_idx": 11}, {"type": "image", "img_path": "aDQlAz09dS/tmp/e6460b5e907b3754bff23dcf23ef0f5fd6e0933592622adea18868d762688a4f.jpg", "img_caption": ["Figure 11: Characterizing accuracy, cost, latency of different model-prompt pairs $(M+P)$ on the GSM8K test dataset. Higher accuracy corresponds to higher price or and lower latency. "], "img_footnote": [], "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Prompt types. We employ several prompting schemes to elicit the reasoning abilities of LLMs. A prompt generally consists of two parts: the \u201ccontent message\u201d containing the question, and the \u201csystem message\u201d with additional context. ", "page_idx": 11}, {"type": "text", "text": "\u2022 The plain text prompt submits the questions to the LLM as the content message (no system message).   \n\u2022 The domain expert prompt feeds information about the question\u2019s domain as a system message (e.g., \u201cmath solver\u201d), and keeping the user\u2019s content message as plain text.   \n\u2022 The standard few-shot prompt includes a system message (\u201cFollow the given examples and answer the question\u201d [17]) and the content message, which consists of few-shot examples together with the plain text prompt. It tends to improve response accuracy compared to the plain text prompt. ", "page_idx": 11}, {"type": "text", "text": "\u2022 The Chain-of-Thought (CoT) few-shot prompt [17] adds some intermediate explanations to the few-shot examples. ", "page_idx": 12}, {"type": "text", "text": "Temperature. The LLM temperature is a configurable parameter that influences the variety of the responses it generates. With a higher temperature, the model may output more diverse but possibly inaccurate responses. We set the temperature to 0 for a new query, and to 0.8 or 1.0 for a re-query for Llama and GPT, respectively. ", "page_idx": 12}, {"type": "text", "text": "(2) Training TREACLE. We used Deep Q-Network (DQN) [8] to train the reinforcement learning (RL) policy in TREACLE, consisting of a two-layer neural network. To generate diverse trajectories consisting of $(s_{t},a_{t},r_{t},s_{t+1})$ , we use the collected query-response data and employ $\\epsilon$ -greedy exploration. For the monetary prices, we use the published per-token prices for the GPT models. Since our local Llama deployments do not have API costs, we set the Llama-2-7b\u2019s price as $\\alpha$ times Llama-2-13b\u2019s price, and Llama-2-13b\u2019s price as $\\alpha$ times GPT-3.5-turbo\u2019s price. $\\alpha$ varies between $\\textstyle{\\frac{1}{10}}$ , $\\textstyle{\\frac{1}{20}}$ or $\\textstyle{\\frac{1}{50}}$ . Our pricing is grounded in reality and similar to actual market rates, as the offered price for Llama is approximately   \nlatency-accuracy tradeoff, we evaluate different trade-off parameters $\\beta=[50\\bar{\\bf k},500{\\bf k},1{\\bf M}]$ in the cost function. We set the smallest $\\beta$ to $50\\mathrm{k}$ because then the two terms in the cost function have similar order of magnitude. The details of the cost function values are provided in Table 4 in the Appendix. Unless stated otherwise, we set $\\lambda=5$ . ", "page_idx": 12}, {"type": "text", "text": "During training, we used the Adam optimizer with a learning rate $1\\times10^{-}4$ , Huber loss as the loss function, and a batch size of 64. Our DQN has three layers with ReLU and softmax activations, and the size of the hidden layer is 128. We set $\\lambda=5$ in the reward function. For re-queries, we set different temperature settings for Llama and GPT (0.8 and 1, respectively) because their ranges are different $([0,1]$ and $[0,2]$ respectively). The actions are selected according to an $\\epsilon$ -greedy policy. Simply put, the actions are sometimes chosen by the DQN and sometimes sampled uniformly. The probability of choosing a random action starts at $\\varepsilon_{\\mathrm{START}}=0.9$ and decays exponentially towards $\\varepsilon_{\\mathrm{END}}=0.05$ . For the reward decay, we use $\\gamma=0.99$ . ", "page_idx": 12}, {"type": "text", "text": "A.3 Baselines ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We evaluated the following baseline methods, reproducing the methods as faithfully as possible with a common set of LLMs and prompt options. ", "page_idx": 12}, {"type": "text", "text": "\u2022 FrugalGPT [3]. We reproduce FrugalGPT, which uses a DistilBERT model [12] to estimate the response accuracy. If this estimate is below a threshold, the next LLM in the cascade is queried. This baseline shows how TREACLE compares to the state-of-the-art that lacks re-querying.   \n\u2022 Calibrated cascade. We build on FrugalGPT\u2019s response accuracy estimation and develop a 2-layer neural network, whose input is a state vector to TREACLE and whose output is the estimated response accuracy. If this estimate is below a threshold (tuned on the validation set), the next LLM in the cascade is queried. This baseline shows how TREACLE compares to an improved version of FrugalGPT.   \n\u2022 Majority Voting. For each query, we output the final response based on the majority vote from $N$ re-queries, based on [16, 20]. We set $N=2$ based on the best empirical results. The (LLM, prompt) combinations are progressively queried until their per-question budget runs out. This baseline allows comparison with TREACLE\u2019s response consistency feature in the state vector.   \n\u2022 Offilne and online knapsack. Given the cost of LLM responses and their accuracy, we formulate a multiple choice knapsack problem where the items are the $(M+P)$ combinations, the values are the correctness probabilities, and the costs are the latency and monetary price functions. Solving this offilne knapsack problem gives the optimal solution when re-queries are not allowed. We also implement an online approximation algorithm [2]. These baselines show how TREACLE compares to methods with perfect knowledge of question costs and accuracy.   \n\u2022 Single model. The (LLM, prompt) combinations are sorted by increasing cost and accuracy, then the most capable option that fits within the allocated budget is selected for all questions. This baseline shows how TREACLE compares to a fixed single LLM and prompt. ", "page_idx": 12}, {"type": "text", "text": "B Theoretical Justifications for the Cascade Strategy ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "B.1 How should the $(M+P)$ be ordered in the cascade? ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Setup 1: Suppose there are $K$ LLM-prompt pairs each with probability of correct answer $p_{i}$ and inference cost $c_{i}$ for $1\\leq i\\leq K$ . During the cascade, we assume access to an oracle that tells when the answer of a model is incorrect so that we can move to the next model. The procedure continues until a correct answer is obtained. The goal is to minimize the expected cost of inference. ", "page_idx": 13}, {"type": "text", "text": "Lemma 1. Suppose there are 2 models with probability of correct answers $p_{1}$ and $p_{2}$ and inference costs $c_{1}$ and $c_{2}$ , respectively. Then, the optimal cascade rule is to first query the model with larger cost-normalized accuracy $\\frac{p_{i}}{c_{i}}$ . ", "page_idx": 13}, {"type": "text", "text": "Proof. The cascade terminates when the first correct answer is obtained. The expected inference cost if we query model 1 first is $C_{1}=c_{1}p_{1}+(c_{1}+c_{2})(1-p_{1})$ , and the expected inference cost if we query model 2 first is $C_{2}=c_{2}p_{2}+(c_{1}+c_{2})(1-p_{2})$ . Note that the accuracy is independent of the cascade order thanks to the oracle. It can be easily shown that $\\begin{array}{r}{C_{1}\\leq C_{2}\\iff\\frac{p_{1}}{c_{1}}\\leq\\frac{\\bar{p}_{2}}{c_{2}}}\\end{array}$ . \u53e3 ", "page_idx": 13}, {"type": "text", "text": "Proposition 1. With $K$ (LLM, prompt) options, each with probability of correct answer $p_{k}$ and cost $c_{k}$ , ordering the options according to their cost-normalized accuracies $\\displaystyle{\\frac{p_{k}}{c_{k}}}$ minimizes the total cost. ", "page_idx": 13}, {"type": "text", "text": "Proof. Suppose the (LLM, prompt) options are not ordered in terms of $\\displaystyle{\\frac{p_{k}}{c_{k}}}$ . We will prove that swapping the order results in a better cascade. Again recall that the accuracy is independent of the cost because we stop as soon as oracle confirms the answer. The expected accuracy is equal to the probability of at least one (LLM, prompt) option being correct. To proceed, suppose (LLM, prompt) options are not ordered and there is some $\\kappa$ such that $\\begin{array}{r}{\\frac{p_{\\kappa}}{c_{\\kappa}}~<~\\frac{p_{\\kappa+\\bar{1}}}{c_{\\kappa+1}}}\\end{array}$ . Let us compute the expected change in inference cost when we flip their order. ", "page_idx": 13}, {"type": "text", "text": "The change in inference cost arises from the scenarios where (LLM, prompt) option $\\kappa$ will be used but $\\kappa+1$ will not be, and vice versa. Define $\\begin{array}{r}{q_{\\kappa-1}=\\prod_{i=1}^{\\kappa-1}(1-p_{i})}\\end{array}$ which is the probability that the first $\\kappa-1$ (LLM, prompt) options fails. The probability of the \u201cexcess cost\u201d $(e c)$ associated with (LLM, prompt) option $\\kappa$ and then $\\kappa+1$ is ", "page_idx": 13}, {"type": "equation", "text": "$$\ne c_{\\kappa}=q_{\\kappa}c_{\\kappa}+q_{\\kappa}(1-p_{\\kappa})c_{\\kappa+1}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "This is because if the first $\\kappa-1$ (LLM, prompt) options fail, we definitely pay $c_{\\kappa}$ and only pay $c_{\\kappa+1}$ if (LLM, prompt) option $\\kappa$ fails. This \u201cexcess cost\u201d definition does not reflect the (LLM, prompt) options strictly before $\\kappa$ or strictly after $\\kappa+1$ . This is because the expected cost of other (LLM, prompt) options arise from the symmetric events with respect to (LLM, prompt) options $\\kappa$ and $\\kappa+1$ (either problem is already solved by the time we reach $\\kappa$ , or both $\\kappa$ and $\\kappa+1$ failed). ", "page_idx": 13}, {"type": "text", "text": "Conversely, the excess cost associated with $\\kappa+1$ is (i.e., using $\\kappa+1$ first and then $\\kappa$ ) ", "page_idx": 13}, {"type": "equation", "text": "$$\ne c_{\\kappa+1}=q_{\\kappa}c_{\\kappa+1}+q_{\\kappa}(1-p_{\\kappa+1})c_{\\kappa}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "To proceed, observe that if $\\begin{array}{r}{\\frac{p_{\\kappa}}{c_{\\kappa}}<\\frac{p_{\\kappa+1}}{c_{\\kappa+1}}}\\end{array}$ , then we would be better off by switching the models because ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{e c_{\\kappa+1}<e c_{\\kappa}\\iff q_{\\kappa}c_{\\kappa+1}+q_{\\kappa}(1-p_{\\kappa+1})c_{\\kappa}<q_{\\kappa}c_{\\kappa}+q_{\\kappa}(1-p_{\\kappa})c_{\\kappa+1}}\\\\ &{\\iff c_{\\kappa+1}+(1-p_{\\kappa+1})c_{\\kappa}<c_{\\kappa}+(1-p_{\\kappa})c_{\\kappa+1}}\\\\ &{\\iff p_{\\kappa}c_{\\kappa+1}<p_{\\kappa+1}c_{\\kappa}}\\\\ &{\\iff p_{\\kappa}/c_{\\kappa}<p_{\\kappa+1}/c_{\\kappa+1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Experimentally, we verify that our models used TREACLE are sorted according to the decreasing cost-normalized accuracies. The model order (using the purely monetary cost function with $\\begin{array}{r}{\\alpha=\\frac{1}{10}.}\\end{array}$ is shown in Figure 12. With other $\\alpha$ values, Llama-2-13b will be cheaper, so the cost-normalized accuracy will always be decreasing. ", "page_idx": 13}, {"type": "image", "img_path": "aDQlAz09dS/tmp/38df8f860a9df215dd222f9284fdada9abe5ccfd6d340a55bc72cafe3f172957.jpg", "img_caption": ["B.2 Do policies that consider response consistency perform well? ", "Figure 12: Cost-normalized accuracy "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "While Proposition 1 provides the optimal ordering rule of LLMs in the cascade, understanding the properties of the optimal trajectory on the cascade is more challenging. To understand the optimal decision rule, we study when a policy terminates for a given question. Given a question, suppose the policy issues up to $\\Omega$ queries of the same question to reach a response, where $1\\,\\leq\\,j\\,\\leq\\,\\Omega$ . Experimentally, we have found that the majority of the response statistics are 2-consistent, that is, the policy stops once it observes the same answer twice. Specifically, in our GSM8K experiments with a budget of $\\mathbb{S}0.30$ and $\\begin{array}{r}{\\alpha=\\frac{1}{20}}\\end{array}$ , only $6.98\\%$ of the responses achieve consistency greater than 2. This percentage increases modestly to $14.32\\%$ when the budget is raised to $\\mathbb{S}10$ (a factor of $33\\times)$ ). In this section, we develop a theoretical explanation for this observation by characterizing of the performance of 2-consistent policies and establishing a formal proof of Proposition 2 . The theory relies on Definition 2, which bounds how likely the wrong answers are to be repeated. ", "page_idx": 14}, {"type": "text", "text": "Proposition 2. For the problem stated in Equation (1) that achieves $C_{*}$ , the optimal expected accuracy subject to budget constraints, there exists a 2-consistent policy that achieves an accuracy of at least $\\begin{array}{r}{C_{*}-\\frac{1}{2}\\Omega^{2}\\epsilon}\\end{array}$ . ", "page_idx": 14}, {"type": "text", "text": "Proof. The proof will be achieved via a reduction. Let $\\pi_{*}$ be the optimal policy that achieves an expected accuracy of $C_{*}$ . We now derive a new policy $\\pi_{2}$ from $\\pi_{*}$ . Specifically, let $\\pi_{2}$ be same as $\\pi_{*}$ if the trajectory does not attain 2-consistency i.e. the answers are not repeated during the cascade. Otherwise, we let $\\pi_{2}$ terminate early once 2-consistency is achieved for the first time. By construction $\\pi_{2}$ will satisfy the budget constraints as it queries the LLMs strictly less or equal to $\\pi_{*}$ . Let $\\hat{Y}_{i}(\\pi)$ denote the final (stochastic) answer of a policy $\\pi$ given problem $Q_{i}$ . Recall that $Y_{i}$ denotes the correct answer. Let the random variable $C(\\pi_{2})$ be the total accuracy of $\\pi_{2}$ summed over all queries where randomness arises from the stochasticity of the LLM responses as well as the policy $\\pi_{*}$ . We can lower bound the accuracy of $\\pi_{2}$ as follows ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathbb{E}[C(\\pi_{2})]=\\sum_{i=1}^{n}\\mathbb{P}(\\hat{Y}_{i}(\\pi_{2})=Y_{i}\\mid Q_{i})}}\\\\ &{\\ge\\displaystyle\\sum_{i=1}^{n}\\mathbb{P}(\\hat{Y}_{i}(\\pi_{2})=Y_{i}\\mid Q_{i})-\\sum_{i=1}^{n}\\mathbb{P}(\\hat{Y}_{i}(\\pi_{2})\\ne\\hat{Y}_{i}(\\pi_{*})\\mid Q_{i})}\\\\ &{=C_{*}-\\displaystyle\\sum_{i=1}^{n}\\mathbb{P}(\\hat{Y}_{i}(\\pi_{2})\\ne\\hat{Y}_{i}(\\pi_{*})\\mid Q_{i})}\\\\ &{=C_{*}-\\displaystyle\\sum_{i=1}^{n}\\mathbb{P}(\\mathrm{Firs~}2\\mathrm{~consistent~answer~i~s~wrong~}\\big|\\mathcal{Q}_{i})}\\\\ &{\\ge C_{*}-\\displaystyle\\sum_{i=1}^{n}\\mathbb{P}(\\mathrm{Any~wrong~2-consistent~answer~within~}\\Omega\\~\\mathrm{responses}\\mid Q_{i}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "For example, if the responses are (5, 10, 3, 5, 10) to a given question $\\mathbb{P}$ (First 2-consistent answer is wrong) is the probability that $^{\\bullet\\leftarrow}5^{\\bullet}$ is the wrong answer, and $\\mathbb{P}(\\mathbb{W r o n g\\,2}$ -consistent answer within $\\Omega$ responses) is the probability that either $\\sqrt[6]{2}$ or $\"10\"$ is a wrong answer. To proceed, we will upper bound the right hand side of (10). Let $W_{a n y}$ be the event \u201cAny wrong 2-consistent answer within $\\Omega$ responses\u201d. Similarly let $W_{u v}$ be the event that the $(M+P)_{u}$ and $(M+P)_{v}$ return the same incorrect answer. Note that, for $W_{\\mathrm{any}}$ to happen, a $W_{u v}$ event has to happen. Thus, through union bound across $\\Omega$ queries given $Q_{i}$ , we have that ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{P}(W_{\\mathrm{any}}|Q_{i})\\leq\\sum_{u<v}\\mathbb{P}(W_{u v}|Q_{i}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Plugging in the definition of $W_{u v}$ , we obtain ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathbb{P}(W_{\\mathrm{ap}}|Q_{i})\\leq\\sum_{u<v}\\mathbb{P}(W_{u v}|Q_{i})}}\\\\ &{=\\displaystyle\\sum_{u<v}\\sum_{\\delta\\neq Y_{i}}\\mathbb{P}(M_{u}(P_{u}(Q_{i}))=\\hat{O})\\mathbb{P}(M_{v}(P_{v}(Q_{i}))=\\hat{O})}\\\\ &{\\leq\\displaystyle\\sum_{u<v}\\sum_{\\delta\\neq Y_{i}}\\frac{\\mathbb{P}(M_{u}(P_{u}(Q_{i}))=\\hat{O})^{2}+\\mathbb{P}(M_{v}(P_{v}(Q_{i}))=\\hat{O})^{2}}{2}}\\\\ &{\\leq\\displaystyle\\sum_{\\delta\\neq Y_{i}}\\frac{\\Omega^{2}}{2}\\operatorname*{sup}_{1\\leq u\\leq K}\\mathbb{P}(M_{u}(P_{u}(Q_{i}))=\\hat{O})^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "To finalize, we sum over all $n$ questions and use the definition of $\\epsilon$ to obtain ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}\\mathbb{P}(W_{\\mathrm{any}}|Q_{i})\\leq{\\frac{\\Omega^{2}}{2}}\\sum_{i=1}^{n}\\operatorname*{sup}_{1\\leq u\\leq K}\\sum_{{\\hat{O}}\\neq Y_{i}}\\mathbb{P}(M_{u}(P_{u}(Q_{i}))={\\hat{O}})^{2}={\\frac{\\Omega^{2}}{2}}\\epsilon.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Plugging the right hand side in equation 10, we conclude with the advertised theorem statement. ", "page_idx": 15}, {"type": "text", "text": "Further analysis of 2-consistent policies as a function of incorrect answer likelihoods. To provide further intuition on Definition 2, let us denote the probability of 1-shot correct answer of a model by $p_{c o r}=\\mathbb{P}(m_{i}(\\pmb{q})=a^{*})$ . Small $p_{c o r}$ implies that individual LLM responses are unreliable. However, if $\\epsilon$ is relatively small, 2-consistency should return the correct answer with high probability because the majority of the 2-consistent instances will arise from the correct answers. Formalizing this intuition, below we upper-bound the likelihood of error conditioned on 2-consistency as $\\mathcal{O}(\\epsilon/\\bar{p}_{c o r}^{2})$ . ", "page_idx": 15}, {"type": "text", "text": "Proposition 3. Recall that $((M+P)_{k})_{k=1}^{K}$ are the LLM-prompt pairs used in the cascade. To obtain a more refined bound in terms of the probabilities of correct and wrong answers, let $p_{k}\\ =\\ \\mathbb{P}(M_{k}(P_{k}(Q))\\ =\\ Y)$ for $1~\\le~k~\\le~K$ . Also set $p_{\\mathrm{min}}~=~\\mathrm{min}_{1\\le k\\le K}\\,p_{k}~>~0$ and $p_{\\mathrm{max}}\\,=\\,\\mathrm{max}_{1\\le k\\le K}\\,p_{k}$ . Let us suppose these hold uniformly over all problem instances $Q$ . As $\\epsilon\\to0$ , over the 2-consistent cascade instances, the probability of error is upper bounded by ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathrm{err}_{\\mathrm{upper}}=(1+o(1))\\frac{\\sum_{k=2}^{K}(1-p_{\\mathrm{max}})^{-2}(k-1)\\mathbf{Q}_{k}\\epsilon}{\\sum_{k=2}^{K}\\mathbf{P}_{k}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Here $\\mathbf{P}_{k}$ is the likelihood that 2-consistent correct answer will first be achieved at the $k$ \u2019th model and $Q_{k}$ is the likelihood that first $k-1$ models fail to achieve 2-consistency. $o(1)$ term captures the higher order terms that arise from the probability of the 2-consistent incorrect cascades (which is vanishing compared to $\\mathbf{P}_{k},\\mathbf{Q}_{k})$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. In equation 15, the $\\begin{array}{r}{{^{\\omega}\\!\\left(1\\right.-\\left.p_{\\mathrm{max}}\\right)}^{-2}{\\epsilon}\\!\\left(k-1\\right){\\bf Q}_{k}{^{\\,,\\ast}}^{\\,}}\\end{array}$ term in the numerator upper bounds the likelihood that, an incorrect 2-consistent answer will be generated precisely at the $k$ th model under 2. Denote the probability of the event \u201ccorrect answer never appears until model $k-1^{\\circ}$ by $F_{k}\\,=$ $\\textstyle\\prod_{j<k}(1-{\\bar{p_{j}}})$ . As $\\epsilon\\to0$ , for $2\\le k\\le K$ , these take the simplified closed forms ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{{\\displaystyle{\\bf P}_{k}=p_{k}F_{k}\\sum_{j<k}\\frac{p_{j}}{1-p_{j}}=p_{k}\\sum_{j<k}p_{j}\\prod_{l<k,l\\ne j}(1-p_{l})}}\\\\ {{\\displaystyle{\\bf Q}_{k}=F_{k}(1+\\sum_{j<k}\\frac{p_{j}}{1-p_{j}})=\\sum_{j\\le k}p_{j}\\prod_{l<k,l\\ne j}(1-p_{l})+\\prod_{j<k}(1-p_{j})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "image", "img_path": "aDQlAz09dS/tmp/73296c0e0675331bbfb13f5f7192f80353b6bf6c22297d96e29072606fb3cf10.jpg", "img_caption": ["Figure 13: End-to-end latency of querying LLM models over a 24-hour period. "], "img_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "aDQlAz09dS/tmp/95f0c6ce7019901d858e86bd020762c289b83bc5c4b7a6da12407748739ae825.jpg", "table_caption": ["Table 2: Performance with time-varying API query latency. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "For a fixed $k$ , let us set the ratio ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{rat}_{k}={\\frac{\\mathbf{Q}_{k}}{\\mathbf{P}_{k}}}\\leq{\\frac{1+\\sum_{j<k}{\\frac{p_{j}}{1-p_{j}}}}{p_{k}\\sum_{j<k}{\\frac{p_{j}}{1-p_{j}}}}}={\\frac{1+M}{p_{k}M}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where M = j<k1\u2212pjp . If $M\\ge1$ , we have that $\\begin{array}{r}{\\mathrm{rat}_{k}\\,\\leq\\,\\frac{2M}{p_{k}M}\\,\\leq\\,\\frac{2}{p_{k}}}\\end{array}$ . If $M\\leq1$ , we have that $\\begin{array}{r}{\\mathrm{rat}_{k}\\leq\\frac{2}{p_{k}M}\\leq\\frac{2}{p_{k}\\sum_{j<k}p_{j}}}\\end{array}$ . Thus, we obtain ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{\\mathbf{Q}_{k}}{\\mathbf{P}_{k}}\\leq\\frac{2}{p_{k}\\cdot\\operatorname*{min}(1,\\sum_{j<k}p_{j})}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "This in turn implies that the error is controlled by the worst case ratio $(k-1)\\mathbf{Q}_{k}/\\mathbf{P}_{k}$ given by $\\begin{array}{r}{\\operatorname*{max}_{k\\le K}\\frac{2(k-1)}{p_{k}\\cdot\\operatorname*{min}(1,\\sum_{j<k}p_{j})}}\\end{array}$ . Using $p_{k}\\cdot\\mathrm{min}(1,\\sum_{j<k}p_{j})\\,\\geq\\,\\mathrm{min}((k-1)p_{\\mathrm{min}}^{2},p_{\\mathrm{min}})\\,\\geq\\,p_{\\mathrm{min}}^{2}$ , we obtain the conclusion that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{err_{upper}}\\leq(2+o(1))\\frac{(1-p_{\\mathrm{max}})^{-2}\\epsilon}{\\operatorname*{min}(p_{\\mathrm{min}}^{2},p_{\\mathrm{min}}/(K-1))}\\leq(2+o(1))\\frac{(K-1)\\epsilon}{(1-p_{\\mathrm{max}})^{2}p_{\\mathrm{min}}^{2}}\\propto\\frac{(K-1)\\epsilon}{p_{\\mathrm{min}}^{2}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "C Additional Results ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "C.1 Time-Varying API Query Latency ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The latency of querying LLM APIs (such as OpenAI\u2019s GPT models) may vary over the short term time based on network congestion or LLM inference time. To showcase this, we recorded traces of the API latency (including communication and communication latency) over a 24-hour period. The measurements are shown in Figure 13. We also modified TREACLE and the experimental setup slightly. Each hour, TREACLE attempts to answer the entire GSM8K test set with a budget of $\\mathbb{S}0.6$ , using the historical average latency from the previous hour to update the per-query cost in the denominator of $B_{k}$ . The budget resets every hour. We compare this to the vanilla TREACLE shown previously, which has the same total budget and uses a fixed latency in the denominator of $\\boldsymbol{{\\beta}}_{k}$ for the entire 24-h period; for example, for GPT models, it assumes a fixed average latency of $6\\;\\mathrm{s}$ . The results in Table 2 indicate that TREACLE that adapts to time-varying achieves higher accuracy than the version that assumes constant latency. The update time to calculate the new $\\boldsymbol{{\\beta}}_{k}$ each hour is also minimal, at $20\\;\\mathrm{ms}$ . ", "page_idx": 16}, {"type": "text", "text": "C.2 Model and Prompt Characterization ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We show the model details, prompt strategies, temperature, and various details of each configuration in Table 3 Note that these are the model-prompt combinations chosen in our framework because of the evaluation in Table 4. ", "page_idx": 16}, {"type": "text", "text": "Table 3: Characterization of LLM performance in terms of accuracy, latency, and price, for a single query with temperature equal to 0. The Llama models do not have a direct monetary price because they are open-source and we run them locally. ", "page_idx": 17}, {"type": "table", "img_path": "aDQlAz09dS/tmp/3f828e89c66e8fe623693f7dda45643be992a9f6b614ef12b5ce80424a00b734.jpg", "table_caption": ["(a) GSM8K dataset "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "aDQlAz09dS/tmp/1b224a721c0a4729454c9021a6a345931240c7aa077e91a648bc491a232ab302.jpg", "table_caption": ["(b) CSQA dataset "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "aDQlAz09dS/tmp/fa29f1d160c038e06ed7776475c45b7fdc273cfab5510452c3d6cdec0bf5d202.jpg", "table_caption": ["(c) LLC dataset "], "table_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "aDQlAz09dS/tmp/12c75559cfcfa8bb332bd1bd46fb7188beb631daed8c14106d76fa84a6f7e97b.jpg", "img_caption": ["Figure 14: Fraction of questions that are solved by (LLM, prompt) combinations ordered from least to most powerful (\u201cin order\u201d). Minority slices are queries where less powerful combinations correctly answered. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "C.3 Different types of reasoning tasks ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "To visualize the differences between the three reasoning datasets, in Figure 14 we plotted the fraction of questions where the most powerful (LLM, prompt) combination in the sorted list correctly answered the question (the \u201cin order\u201d pie slice), versus those questions where a less powerful combination succeeded and a more powerful combination failed (all other slices of the pie). Interestingly for all datasets, there are minority cases where less powerful LLMs (the smaller pieces of the pie) can answer the question correctly. Such cases are most prevalent in the GSM8K dataset and least prevalent in LLC, possibly because the math questions of GSM8K are more difficult. Despite these dataset differences, TREACLE still chooses the right (LLM, prompt) combination to achieve higher accuracy in all datasets than the baselines. ", "page_idx": 17}, {"type": "text", "text": "C.4 Different cost function parameters and datasets ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Providers may adjust the per-token API prices, or the user may value latency and monetary price differently. Therefore, we conducted experiments using different settings of the $\\alpha$ (defined in Appendix A.2)and $\\beta$ (defined in Section 3) parameters in the cost function. In Figure 15, the cost ratio $\\alpha$ increases from left to right, and hence the cost difference between more powerful (GPT) and weaker (Llama) models gradually decreases according to the definition. Under different pricing policies, TREACLE consistently achieves better performance than the online baselines. In other words, a single TREACLE model can easily accommodate varying budget requirements and cost functions, since it was trained under heterogeneous parameter settings. ", "page_idx": 18}, {"type": "text", "text": "Also, we mainly show GSM8K results in the main paper, because of limited space. Across the additional datasets in Figure 15 (CSQA and LLC), the results consistently show good performance. ", "page_idx": 18}, {"type": "text", "text": "Table 4: Overview of average cost $(\\mathbb{S})$ per query for different models and prompting strategies\u2019 combinations in different pricing strategies. ", "page_idx": 18}, {"type": "table", "img_path": "aDQlAz09dS/tmp/4fe69fd069121e33a1a1a7eb42f97686aae85537a2f0433489f50c338b28b522.jpg", "table_caption": ["(a) GSM8K dataset "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "aDQlAz09dS/tmp/cf4637cac4dd7956eddc523b2bc026d91c527ccadd1c484afd727859248ada17.jpg", "table_caption": ["(b) CSQA dataset "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "aDQlAz09dS/tmp/15aa21941d955b179f3bc6364452aebc5b6a88ed74013596cf4d32ed91255edf.jpg", "table_caption": ["(c) LLC dataset "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "C.5 Ablation experiments ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We also run ablation experiments showing that prompt selection is useful, compared to using a fixed prompt (e.g., CoT). The results are shown in Figure 16, where TREACLE outperforms \u201cTREACLE (CoT only)\u201d, indicating that the ability to choose the prompt helps. ", "page_idx": 18}, {"type": "text", "text": "C.6 Additional new LLM experiments ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this subsection, we report additional results relating to Section 5.2.1. The performance of the fine-tuned models with the API price adjustments or the improved open-source LLM is shown in Figure 17 (\u201cFinetuned:GPT\u201d and \u201cFinetuned:Llama\u201d, respectively). The results show that the fine-tuned model with both improvements (\u201cFinetuned:all\u201d, same as Figure 6) performs the best. The sample efficiency results for fine-tuning these models with both types of changes (corresponding to \u201cFinetuned:all\u201d) are shown in Figure 17. ", "page_idx": 18}, {"type": "image", "img_path": "aDQlAz09dS/tmp/02b01efae08c3d0ed118bc5e531e83697dd8d5032931af7869b8c08ea2e7bed7.jpg", "img_caption": ["Figure 15: The performance of various methods for different cost functions and budget constraints. The dashed lines are methods that have ground knowledge, which is impractical but illustrates the best achievable performance. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Table 5: Overview of average accuracy of different models and different prompting strategies\u2019 combinations. In GSM8K table, simple CoT few-shot and complex CoT few-shot mean CoT few-shot prompts with easy and hard examples. ", "page_idx": 20}, {"type": "table", "img_path": "aDQlAz09dS/tmp/37a9b4ff7ee6223a8e161956b99fd9d288db5b661afd4bd21fe522741099e911.jpg", "table_caption": ["(a) GSM8K dataset "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "aDQlAz09dS/tmp/3771bd42cd21a908b77919e6d5f88e43a58d124450a5ba900325d583529710ba.jpg", "table_caption": ["(b) CSQA dataset "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "aDQlAz09dS/tmp/169bc9293707aa83629bf6782df88b1c57707f514c44be904c9c189f17e03b15.jpg", "table_caption": ["(c) LLC dataset "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "D API prices ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Table 4 shows further details on the parameters used in the cost functions described in Section 3. Table 6 shows the change of the GPT API\u2019s monetary price in different API versions, relating to Section 5.2.1. ", "page_idx": 20}, {"type": "text", "text": "Table 6: API price GPT API price of different versions, where NA means no corresponding model at that version. The price unit is $\\mathbb{S}/1\\mathbb{K}$ tokens. In our experiment setting, old version refers to the version of 0613 and the new version refers to the version 1106. ", "page_idx": 20}, {"type": "table", "img_path": "aDQlAz09dS/tmp/c1fe891c4b8e6fe19f80d5a4ccb74e1b5e3a7075a5028883ffb91b3ed2ecb03f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "E Full Prompts ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section, we show our full prompts. ", "page_idx": 20}, {"type": "image", "img_path": "aDQlAz09dS/tmp/9bae0027c286bb5fb79b003489fd2e94fe33bdd08f3d55792082243c4357cb05.jpg", "img_caption": ["Figure 16: Ablation study. Without TREACLE\u2019s re-query and prompt selection, the performance decreases dramatically. "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "aDQlAz09dS/tmp/fd08d5da5881d34f785ca383045f1acfd6db3853336f77218f9ac114c884e69d.jpg", "img_caption": ["Figure 17: Additional new LLM results. Left: Zoomed in view of the accuracy with new GPT models, new Llama models, or both. Right: Sample efficiency with both new GPT models and new LLama models. "], "img_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "aDQlAz09dS/tmp/5f7097e62426cb8a9e12f6c5f97d4ad380aff0cce2239cc9c3c27fc1ecd02ed8.jpg", "table_caption": ["Table 7: Domain expert prompting strategy (\u201cMath solver\" and \u201cMath assistant\") in GSM8K dataset, where {Question} means that original question text. "], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "aDQlAz09dS/tmp/de87ade30c9525d96ee1d442175f80edff8e57921a1f04bff24c8a5bcd027ccf.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "Table 9: Standard few-shot prompting strategy in CSQA dataset, where {Question} means that original question text. ", "text_level": 1, "page_idx": 23}, {"type": "table", "img_path": "aDQlAz09dS/tmp/5cc0177cacd3605efb64a2449f8beca00fdd4d0edf1eb1b60c49076491e26143.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "Table 10: Chain-of-Thought (CoT) few-shot prompting strategy in CSQA dataset, where {Question} means that original question text. ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "User Content Prompt ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Q: What do people use to absorb extra ink from a fountain pen? Answer Choices: (A) shirt pocket (B) calligrapher\u2019s hand (C) inkwell (D) desk drawer (E) blotter   \nA: The answer must be an item that can absorb ink. Of the above choices, only blotters are used to absorb ink. The answer is E. Q: What home entertainment equipment requires cable? Answer Choices: (A) radio shack (B) substation (C) television (D) cabinet A: The answer must require cable. Of the above choices, only television requires cable. The answer is C.   \nQ: The fox walked from the city into the forest, what was it looking for? Answer Choices: (A) pretty flowers (B) hen house (C) natural habitat (D) storybook   \nA: The answer must be something in the forest. Of the above choices, only natural habitat is in the forest. The answer is B. Q: Sammy wanted to go to where the people were. Where might he go? Answer Choices: (A) populated areas (B) race track (C) desert (D) apartment (E) roadblock   \nA: The answer must be a place with a lot of people. Of the above choices, only populated areas have a lot of people. The answer is A. Q: Where do you put your grapes just before checking out? Answer Choices: (A) mouth (B) grocery cart (C)supermarket (D) fruit basket (E) fruit market   \nA: The answer should be the place where grocery items are placed before checking out. Of the above choices, grocery cart makes the most sense for holding grocery items. The answer is B.   \nQ: Google Maps and other highway and street GPS services have replaced what? Answer Choices: (A) united states (B) mexico (C) countryside (D) atlas   \nA: The answer must be something that used to do what Google Maps and GPS services do, which is to give directions. Of the above choices, only atlases are used to give directions. The answer is D.   \nQ: Before getting a divorce, what did the wife feel who was doing all the work? Answer Choices: (A) harder (B) anguish (C) bitterness (D) tears (E) sadness   \nA: The answer should be the feeling of someone getting divorced who was doing all the work. Of the above choices, the closest feeling is bitterness. The answer is C.   \nQ: {Question }   \nA: ", "page_idx": 23}, {"type": "table", "img_path": "aDQlAz09dS/tmp/e868bc8cdbf7e71627fb746d28e18916da2c8899106af3f2618da181a1f38fc9.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "aDQlAz09dS/tmp/8fd678da04b2e5608fb170e581b9938455d5b33d50bc9380cd260b67059948e8.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The claims are summarized in the Abstract and in the bulleted list at the end of the Introduction. The claims are supported by the experimental results including adaptability to new LLMs (Section 5.2.1) and reasoning tasks (Section 5.2.3). ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 25}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification. We state some limitations of our work at the end of the Conclusions section. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 25}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We explain the theoretical results in Section 4. The related lemmas and full proofs are in Appendix B. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 26}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We describe the high-level framework in Section 4, summarize the experimental setup in Section 5.1 and provide full implementation details in Appendix A. We plan to release the code and training datasets if the paper is published. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 26}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We release our training and evaluation along with the data generation code in a zip file. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 27}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We describe the training and testing details in Section 5.1 and Appendix A. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 27}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We extensively sweep across different hyperparameters (e.g., accuracy, budget, cost function, number of training samples for fine-tuning). We also delve into the performance statistics on different questions within the same test set (Figure 8c). ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 27}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 28}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We state the hardware to run the local models and the exact versions of the LLMs in Appendix A.2. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 28}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: There are no potential harms caused by the research process and potential harmful societal impact. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 28}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We state the potential social benefti of our work at the end of the Conclusions section. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 29}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: Our work targets reducing the cost of LLM inference, and inherits the safeguards of the publicly available LLMs that we use. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 29}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We cite the dataset GSM8K, CSQA, LLC and also all the models used in our framework, e.g., Llama 2, GPT-3.5-turbo, GPT-4, etc. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We include our experiment code and data generation code in the supplementary files. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 30}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: Our work does not involve human subjects. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 30}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: Our work does not involve human subjects. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 30}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 31}]