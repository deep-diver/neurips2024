[{"figure_path": "jV6z08u7y0/figures/figures_5_1.jpg", "caption": "Figure 1: For all MLPs considered, lower value of Dirichlet Energy (Figure 1b) corresponds to larger robust accuracy on test-set attacked by L2-norm Auto-attack with \u20ac = 0.5. The dynamics of the co-correlation @ for linear and MLPs is shown in Figure 1c and 1d. The architectures of neural networks are defined in Equation (3b). The parameters to control the weight initialization in Assumption 5.1 is set to q = 0.25", "description": "This figure displays the relationship between Dirichlet energy, robust accuracy, and co-correlation dynamics for various MLPs.  Panel (a) shows that lower Dirichlet energy correlates with higher robust accuracy. Panel (b) shows Dirichlet energy values for different MLPs. Panels (c) and (d) show the dynamic changes in co-correlation over epochs for linear models and MLPs respectively, demonstrating a general trend of increasing co-correlation during gradient descent. The weight initialization parameter 'q' is set to 0.25, as defined in Assumption 5.1 of the paper.", "section": "6.1 Empirical Evidence for Proposed Theorem"}, {"figure_path": "jV6z08u7y0/figures/figures_7_1.jpg", "caption": "Figure 2: The dynamics of co-correlation for ResNet50 and WRN50 under different way of partition. The way of partition is illustrated in Figure 2a. A1-A2 and B1-B2 represent the separations that distinguish the head and tail separately.", "description": "This figure shows the dynamics of co-correlation for ResNet50 and WRN50 networks trained on CIFAR10.  Different partitioning strategies are used (A1-A2 and B1-B2, illustrated in 2a), separating the head and tail parts of the networks, to analyze the impact of different network sections on the co-correlation. The plots (2b and 2c) display the co-correlation change over training epochs for each partition strategy, highlighting the differences in the dynamics between the two network architectures.", "section": "6.2 The Impact of Width and Weight Initialization"}, {"figure_path": "jV6z08u7y0/figures/figures_8_1.jpg", "caption": "Figure 3: The dynamic of co-correlation under different set-up of weight initialization. MLP network defined in Equation (3) with ReLU activation function for width 32, 512, 2048 and 8192 are included.", "description": "This figure shows how the co-correlation changes over epochs for MLP networks with different widths (32, 512, 2048, and 8192) and various weight initializations (q = -0.15, -0.1, -0.05, 0, 0.05, 0.1, 0.15, 0.25).  Each subplot represents a different network width. The x-axis represents the number of epochs during training, and the y-axis shows the value of the co-correlation. The different colored lines within each subplot represent the various weight initializations. This figure illustrates the impact of network width and weight initialization on the dynamics of co-correlation during gradient descent.", "section": "6.2 The Impact of Width and Weight Initialization"}, {"figure_path": "jV6z08u7y0/figures/figures_9_1.jpg", "caption": "Figure 4: Accuracy and Co-correlation under different initialization and width. Figure 4a and 4b show the heat map for linear model, and Figure 4c and 4d is for MLP ReLU.", "description": "This figure displays the accuracy and co-correlation for both linear and MLP models as heatmaps. Each cell in the heatmap represents a trained network. The heatmaps visualize the relationship between accuracy, co-correlation, network width, and weight initialization (q) for both linear models and MLPs with ReLU activation function.  It shows how these factors influence the performance and robustness of the models.", "section": "6.2 The Impact of Width and Weight Initialization"}, {"figure_path": "jV6z08u7y0/figures/figures_23_1.jpg", "caption": "Figure 5: Linear Correlation & Relative Std. The linear correlation and the standard deviation over the mean are given for all MLPs with the initialization parameter q = 0.25.", "description": "Figure 5 presents two sub-figures. The left sub-figure (a) shows the relative standard deviation of the L2-norm of the Jacobian over its mean for various MLPs with different widths. The right sub-figure (b) displays the linear correlation between the L2-norms of Jacobians for adjacent layers of these MLPs. Both sub-figures illustrate the dynamics across 50 training epochs, with the initialization parameter q set to 0.25 for all MLPs.  The plots provide insights into the stability and correlation of feature selection across layers during training.", "section": "D Extra Experiment Results"}, {"figure_path": "jV6z08u7y0/figures/figures_23_2.jpg", "caption": "Figure 6: The accumulation of co-correlation under different width. X-axis denotes the width of the neural network, and the line in different color shows the result under different setting of weight initialization. The lower plot is the result for linear model and the result for shallow ReLU is at the top.", "description": "This figure shows how co-correlation accumulates during training in neural networks with different widths and weight initializations. The x-axis represents the width (2<sup>n</sup>), while different colored lines represent different weight initialization settings (q). The top plot shows results for a ReLU model, while the bottom plot shows results for a linear model.  The figure demonstrates the relationship between network width, weight initialization, and the accumulation of co-correlation.", "section": "Extra Experiment Results"}]