[{"figure_path": "UWUUVKtKeu/figures/figures_4_1.jpg", "caption": "Figure 1: The training pipeline of QVPO. In each training epoch, QVPO first utilizes the diffusion policy to generate multiple action samples for every state. Then, these action samples will be selected and endowed with different weights according to the Q value given by the value network. Besides, action samples from uniform distribution are also created for the diffusion entropy regularization term. With these action samples and weights, we can finally optimize the diffusion policy via the combined objective of Q-weighted VLO loss and diffusion entropy regularization term.", "description": "This figure illustrates the training pipeline of the Q-weighted Variational Policy Optimization (QVPO) algorithm.  It shows how the algorithm uses a diffusion policy to generate action samples, assigns weights based on the Q-value from a value network, incorporates samples from a uniform distribution for entropy regularization, and updates the diffusion policy using the combined Q-weighted VLO loss and diffusion entropy regularization.", "section": "4 Q-weighted Variational Policy Optimization"}, {"figure_path": "UWUUVKtKeu/figures/figures_6_1.jpg", "caption": "Figure 2: A toy example on continuous bandit to show the effect of diffusion entropy regularization term via the changes of the explorable area for diffusion policy with the training procedure. The contour lines indicate the reward function of continuous bandit, which is an arbitrarily selected function with 3 peaks.", "description": "This figure shows a comparison of the explorable area of a diffusion policy with and without the entropy regularization term. The top row illustrates the diffusion policy without the entropy term, and the bottom row illustrates the diffusion policy with the entropy term. The contour lines show an arbitrarily selected reward function with three peaks. The red dots represent the action samples obtained during training, which demonstrate that the entropy term effectively expands the policy's exploration range and helps it to discover areas with higher rewards.", "section": "4.3 Enhancing Policy Exploration via Diffusion Entropy Regularization"}, {"figure_path": "UWUUVKtKeu/figures/figures_8_1.jpg", "caption": "Figure 3: Learning Curves of different algorithms on 5 Mujoco locomotion benchmarks across 5 runs. The x-axis is the number of training epochs. The y-axis is the episodic reward. the plots smoothed with a window of 5000.", "description": "This figure shows the learning curves of different reinforcement learning algorithms on five MuJoCo locomotion tasks.  Each algorithm was run five times, and the average episodic reward is plotted against the number of training epochs.  The shaded areas represent the standard deviation across the five runs. The figure clearly demonstrates that QVPO converges faster and achieves higher rewards than other algorithms.", "section": "5.1 Comparative Evaluation"}, {"figure_path": "UWUUVKtKeu/figures/figures_8_2.jpg", "caption": "Figure 4: Comparison between QVPO with and without the diffusion entropy regularization.", "description": "The figure shows the learning curves of QVPO on the Ant-v3 environment with different settings: 20 diffusion steps with and without entropy regularization, and 100 diffusion steps with and without entropy regularization.  It demonstrates the impact of entropy regularization on the model's performance, especially when the number of diffusion steps is limited (20).  With entropy regularization, the performance with 20 steps comes close to that achieved with 100 steps without it, highlighting the effectiveness of entropy regularization in improving exploration and mitigating the performance loss due to fewer diffusion steps.", "section": "5.2 Ablation Study and Parameter Analysis"}, {"figure_path": "UWUUVKtKeu/figures/figures_8_3.jpg", "caption": "Figure 5: Comparison of QVPO with different action selection numbers for behavior policy K\u266d and for target policy Kt.", "description": "This figure shows the results of an ablation study on the effect of different action selection numbers (K<sub>b</sub> for behavior policy and K<sub>t</sub> for target policy) in the QVPO algorithm.  It compares the episodic reward achieved by QVPO using different combinations of K<sub>b</sub> and K<sub>t</sub> values (K<sub>b</sub>=1 & K<sub>t</sub>=1, K<sub>b</sub>=4 & K<sub>t</sub>=1, K<sub>b</sub>=4 & K<sub>t</sub>=4, K<sub>b</sub>=4 & K<sub>t</sub>=2, K<sub>b</sub>=20 & K<sub>t</sub>=2). The results demonstrate the impact of action selection on the algorithm's performance and sample efficiency. Different combinations lead to different levels of exploration and exploitation, influencing the final episodic reward.", "section": "5.2 Ablation Study and Parameter Analysis"}]