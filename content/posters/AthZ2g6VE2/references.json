{"references": [{"fullname_first_author": "Condat, L.", "paper_title": "Provably doubly accelerated federated learning: The first theoretically successful combination of local training and compressed communication", "publication_date": "2022-10-26", "reason": "This paper proposes a novel algorithm that combines local training and compressed communication, achieving a doubly-accelerated communication complexity."}, {"fullname_first_author": "Mishchenko, K.", "paper_title": "Distributed learning with compressed gradient differences", "publication_date": "2019-01-24", "reason": "This foundational paper introduced the DIANA algorithm, a linearly converging algorithm for distributed optimization with a large class of unbiased compressors."}, {"fullname_first_author": "McMahan, H. B.", "paper_title": "Communication-efficient learning of deep networks from decentralized data", "publication_date": "2017-01-01", "reason": "This seminal paper introduced Federated Averaging (FedAvg), a popular and effective technique for communication-efficient distributed learning, which reduces the communication frequency."}, {"fullname_first_author": "Richt\u00e1rik, P.", "paper_title": "EF21: A new, simpler, theoretically better, and practically faster error feedback", "publication_date": "2021-12-01", "reason": "This paper presents EF21, an algorithm with improved communication efficiency using biased compression, offering a unified theory of SGD with error feedback and variance reduction."}, {"fullname_first_author": "Sadiev, A.", "paper_title": "Communication acceleration of local gradient methods via an accelerated primal-dual algorithm with an inexact prox", "publication_date": "2022-12-01", "reason": "This paper proposes a new algorithm that accelerates local gradient methods and handles communication compression through a primal-dual framework."}]}