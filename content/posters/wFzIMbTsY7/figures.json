[{"figure_path": "wFzIMbTsY7/figures/figures_4_1.jpg", "caption": "Figure 1: The architecture of DM-H. During offline training, Mamba module generates sub-goals from long-term experience, where the long-term experience consists of multiple historical trajectories arranged in ascending order of the total rewards. Based on the generated sub-goals, the transformer is required to predict better actions by supervising the expert behaviors. Meanwhile, the linear layer feeds the valuable sub-goals into the transformer module and associates them with the generated actions. During online testing, DM-H can automatically improve its performance in a trial-and-error manner without requiring gradient updates.", "description": "This figure illustrates the architecture of the Decision Mamba-Hybrid (DM-H) model.  The model consists of two main components: a Mamba module for processing long-term dependencies and a Transformer module for generating high-quality predictions. During offline training, the Mamba module extracts valuable sub-goals from historical trajectories.  These sub-goals are then fed into the Transformer module along with short-term context, allowing the model to make better predictions.  The linear layer combines the sub-goals with transformer output. During online testing, DM-H uses the learned model to generate actions and improves its performance iteratively without needing gradient updates.", "section": "4 Method"}, {"figure_path": "wFzIMbTsY7/figures/figures_6_1.jpg", "caption": "Figure 2: Results for Grid World. An agent is expected to solve a new task by interacting with the environments for 20 episodes without online model updates. Our DM-H significantly outperforms baselines on long-term tasks with sparse rewards because it inherits the merits of transformers and Mamba in high-quality prediction and long-term memory.", "description": "This figure presents the results of the Grid World experiments, comparing DM-H's performance against other baselines.  The experiments tested the ability of different agents to solve unseen tasks within a limited number of episodes (20), without any online model updates.  The results show that DM-H significantly outperforms the baselines, particularly in long-term tasks with sparse rewards.  This superiority is attributed to the hybrid approach of DM-H combining the strengths of transformers for high-quality predictions and Mamba for efficient long-term memory processing.", "section": "5.2 Grid World Results"}, {"figure_path": "wFzIMbTsY7/figures/figures_7_1.jpg", "caption": "Figure 3: Results for (a) performance and (b) online testing times on Tmaze tasks. We train each method to address Tmaze tasks that have different horizons until we run out of GPU memory at context length to achieve 10k (DT, DM) or 20k (our DM-H). We report the online testing time for 20 episodes of Tmaze tasks.", "description": "This figure shows the performance and online testing time for the Tmaze task using three different methods: DM-H (ours), DM, and DT.  The x-axis represents the memory length (k), which is the length of the context.  The left subplot (a) displays the average return (performance) for each method as memory length increases.  DM-H outperforms DM and DT, maintaining high performance even with longer memory lengths.  The right subplot (b) shows the online learning time for each method; DM-H is significantly faster than both DM and DT as the memory length increases. The figure demonstrates the efficiency and effectiveness of DM-H for the Tmaze task.", "section": "5.3 Tmaze Results"}, {"figure_path": "wFzIMbTsY7/figures/figures_8_1.jpg", "caption": "Figure 4: (a) The ablation study on DM-H with or without valuable sub-goals. (b) The parameter sensitivity analysis of \"c\".", "description": "The figure presents the results of two experiments. In (a), an ablation study compares the performance of the Decision Mamba-Hybrid (DM-H) model with and without valuable sub-goals.  The results are shown for several GridWorld tasks, demonstrating the benefit of incorporating valuable sub-goals.  In (b), the parameter sensitivity analysis explores how changing the hyperparameter 'c' (which controls the number of steps of actions for one sub-goal) impacts DM-H's performance.   The results are shown for several D4RL tasks, illustrating the range of 'c' that yields good performance.", "section": "5.4 D4RL Results"}, {"figure_path": "wFzIMbTsY7/figures/figures_14_1.jpg", "caption": "Figure 5: Results for offline training times on Tmaze tasks. We train each method to address Tmaze tasks that have different horizons until we run out of GPU memory at context length to achieve 10k (DT, DM) or 20k (DM-H). We report the training times for 10k gradient updates on Tmaze tasks.", "description": "This figure shows the offline training time for different methods (DM-H, DT, and DM) on Tmaze tasks with varying memory lengths.  It highlights the efficiency gains of DM-H, which scales more favorably with increasing memory length (task horizon) compared to DT and DM, likely due to the inherent linear time complexity of Mamba.", "section": "5.3 Tmaze Results"}, {"figure_path": "wFzIMbTsY7/figures/figures_15_1.jpg", "caption": "Figure 6: The ablation study on DM-H with or without valuable sub-goals.", "description": "This figure shows the ablation study comparing the performance of Decision Mamba-Hybrid (DM-H) model with and without valuable sub-goals. The results are presented for three different D4RL environments (HalfCheetah, Hopper, Walker2d) and three different dataset variations (m-e: medium-expert, m: medium, m-r: medium-replay).  Each bar represents the average return, and error bars indicate standard deviation across multiple runs. The comparison demonstrates the significant positive impact of incorporating valuable sub-goals in improving DM-H's performance across various settings.", "section": "5.4 D4RL Results"}]