[{"type": "text", "text": "In-Context Learning of a Linear Transformer Block: Benefits of the MLP Component and One-Step GD Initialization ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ruiqi Zhang Jingfeng Wu Peter L. Bartlett UC Berkeley UC Berkeley UC Berkeley and Google DeepMind rqzhang@berkeley.edu uuujf@berkeley.edu peter@berkeley.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We study the in-context learning (ICL) ability of a Linear Transformer Block (LTB) that combines a linear attention component and a linear multi-layer perceptron (MLP) component. For ICL of linear regression with a Gaussian prior and a non-zero mean, we show that LTB can achieve nearly Bayes optimal ICL risk. In contrast, using only linear attention must incur an irreducible additive approximation error. Furthermore, we establish a correspondence between LTB and one-step gradient descent estimators with learnable initialization $\\left(\\mathsf{G D}\\!-\\!\\beta\\right)$ , in the sense that every GD- $_{\\beta}$ estimator can be implemented by an LTB estimator and every optimal LTB estimator that minimizes the in-class ICL risk is effectively a GD- $_{\\cdot\\beta}$ estimator. Finally, we show that GD- $_{\\beta}$ estimators can be efficiently optimized with gradient flow, despite a non-convex training objective. Our results reveal that LTB achieves ICL by implementing GD- $_{\\cdot\\beta}$ , and they highlight the role of MLP layers in reducing approximation error. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The recent dramatic progress in natural language processing can be attributed in large part to the development of large language models based on Transformers [34], such as BERT [11], LLaMA [32], PaLM [8], and the GPT series [28, 29, 6, 25]. In some pioneering pre-trained large language models, a new learning paradigm known as in-context learning (ICL) was observed (see, for example, [6, 7, 24, 20]). ICL refers to the capability of a pre-trained model to solve a new task based on a few in-context demonstrations without updating the model parameters. ", "page_idx": 0}, {"type": "text", "text": "A recent line of work quantifies ICL in tractable statistical learning setups such as linear regression with a Gaussian prior (see [14, 3] and references thereafter). Specifically, an ICL model takes a sequence $(\\mathbf{x}_{1},y_{1},...,\\mathbf{x}_{M},y_{M},\\mathbf{x}_{\\mathsf{q u e r y}})$ as input and outputs a prediction of $y_{\\mathsf{q u e r y}}$ .where $(\\mathbf{x}_{i},y_{i})_{i=1}^{M}$ and $\\left(\\mathbf{x_{query}},y_{\\mathsf{q u e r y}}\\right)$ are independent samples from an unknown task-specific distribution (where the task admits a prior distribution). The ICL model is pre-trained by fitting many empirical observations of such sequence-label pairs. Experiments show that Transformers can achieve an ICL risk close to that achieved by Bayes optimal estimators in many statistical learning tasks [14, 3]. ", "page_idx": 0}, {"type": "text", "text": "For ICL of linear regression with a Gaussian prior, the data is generated as ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\mathbf{x}\\sim\\mathcal{N}(\\mathbf{0},\\,\\mathbf{H}),\\quad\\mathit{y}\\mid\\widetilde{\\beta},\\mathbf{x}\\sim\\mathcal{N}(\\widetilde{\\beta}^{\\top}\\mathbf{x},\\,\\sigma^{2}),\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "where $\\widetilde{\\beta}$ is a task parameter that satisfies a Gaussian prior, $\\widetilde{\\beta}\\sim\\mathcal{N}\\left(\\mathbf{0},\\ \\psi^{2}\\mathbf{I}\\right)$ : In this setup, [14, 3] showed in experiments that Transformers can achieve nearly optimal ICL by matching the performance of the Bayes optimal estimator, that is, an optimally tuned ridge regression. Besides, [35] showed by construction that a single linear self-attention (LSA) can implement one-step gradient descent with zero initialization (GD-0), offering an insight into the ICL mechanism of the Transformer. ", "page_idx": 0}, {"type": "text", "text": "Later, theoretical works showed that optimal LSA models effectively correspond to GD-0 models [1], trained LSA models converge to GD-0 models [38], and GD-0 models (hence LSA models) can provably achieve nearly Bayes optimal ICL in certain regimes [37]. ", "page_idx": 1}, {"type": "text", "text": "Our contributions. In this paper, we consider ICL in a more general setup, that is, linear regression with a Gaussian prior and a non-zero mean (that is, $\\mathbb{E}[\\widetilde{\\beta}]=\\beta^{*}\\neq0)$ . The non-zero mean in task prior captures a common scenario where tasks share a signal. In this setting, we show that the LSA models considered in prior papers [1, 38, 37] incur an irreducible additive approximation error. Furthermore, we show this approximation error is mitigated by considering a linear Transformer block (LTB) that combines a linear multi-layer perceptron (MLP) component and an LSA component. Our results highlight the important role of MLP layers in Transformers in reducing the approximation error, and they suggest that theories about LSA [1, 38, 37] do not fully explain the power of Transformers. Motivated by this understanding, we investigate LTB in depth and obtain the following additional results: ", "page_idx": 1}, {"type": "text", "text": "\u00b7 We show that LTB can implement one-step gradient descent with learnable initialization, referred to by us as GD- $_{\\beta}$ . Additionally, we show that every optimal LTB estimator that minimizes the in-class ICL risk is effectively a GD- $_{\\beta}$ estimator.   \n\u00b7 Moreover, we show_ that the optimal GD- $_{\\cdot\\beta}$ , hence also the optimal LTB, nearly matches the performance of the Bayes optimal estimator for linear regression with a Gaussian prior and a non-zero mean, provided that the signal-to-noise ratio is upper bounded. These two results together suggest that LTB performs nearly optimal ICL by implementing GD- $\\beta$   \n\u00b7 Finally, we show that the GD- $_{\\beta}$ estimator can be efficiently optimized by gradient descent with an infinitesimal stepsize (that is, gradient fow) under the population ICL risk, despite the nonconvexity of the objective. ", "page_idx": 1}, {"type": "text", "text": "Paper organization. The remaining paper is organized as follows. We conclude this section by introducing a set of notations. We then discuss related papers in Section 2. In Section 3, we set up our ICL problems and define LTB and LSA models mathematically. In Section 4, we show a positive approximation error gap between LSA and LTB, which highlights the importance of the MLP component and motivates our subsequent efforts to study LTB. In Section 5, we connect LSA estimatorstoGD- $_{\\beta}$ estimators that are more interpretable for ICL of linear regression. In Section 6, we study the in-context learning and training of GD- $\\beta$ estimators. We conclude our paper and discuss future directions in Section 7. ", "page_idx": 1}, {"type": "text", "text": "Notations. We use lowercase bold letters to denote vectors and uppercase bold letters to denote matrices and tensors. For a vector $\\mathbf{x}$ and a positive semi-definite (PSD) matrix $\\pmb{A}$ , we write $\\left\\|\\mathbf{x}\\right\\|_{A}:=$ $\\sqrt{\\mathbf{x}^{\\top}A\\mathbf{x}}$ We write $\\langle\\cdot,\\cdot\\rangle$ for the inner product, which is defined as $\\langle\\mathbf{x},\\mathbf{y}\\rangle:=\\mathbf{x}^{\\top}\\mathbf{y}$ for vectors and $\\langle A,B\\rangle:=\\operatorname{tr}(A B^{\\top})$ for matrices. We write $A[i]$ as the $i^{\\th}$ -th row of the matrix $\\pmb{A}$ \uff0c $A_{m,n}$ as the $(m,n)$ -th entry, and $A_{-1,-1}$ as the right-bottom entry. We write $\\mathbf{0}_{n},\\mathbf{0}_{m\\times n},\\mathbf{I}_{n}$ for the zero vector, zero matrix and identity matrix, respectively. We denote the Kronecker product as $\\otimes$ . For two matrices $\\pmb{A}$ and $_B$ \uff0c $A\\otimes B$ is a linear mapping which operates as $(\\pmb{A}\\otimes\\pmb{B})\\circ\\pmb{C}=\\pmb{B}\\pmb{C}\\pmb{A}^{\\top}$ . For a positive semi-definite matrix $\\pmb{A}$ , we write A as its principle square root, which is the unique positive semi-definite matrix $_B$ such that $B B=B B^{\\top}=A$ . We also write $A^{-{\\frac{1}{2}}}=(A^{{\\frac{1}{2}}})^{+}$ , where $(\\cdot)^{+}$ denotes the Moore Penrose pseudo-inverse. For two sets $A,B$ we write $A+B$ for the Minkowski sum, which is defined as $\\{a+b:a\\in A,b\\in B\\}$ . We also write $a+B=\\{a\\}+B$ for an element $a$ and a set $B$ . We write null $(\\cdot)$ for the null set of a matrix or a tensor. ", "page_idx": 1}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Empirical results for ICL in controlled settings. The work by [14] first considered ICL in controlled statistical learning setups. For noiseless linear regression, [14] showed in experiments that Transformers match the performance of the optimal estimator (that is, Ordinary Least Square). Subsequent works by [3, 18] extended their result to noisy linear regression with a Gaussian prior and showed that Transformers can match the performance of the Bayesian optimal estimator (that is, an optimally tuned ridge regression). Besides, [30] showed that the above holds even when Transformers are pretrained on a limited number of linear regression tasks. These papers only considered a Gaussian prior with a zero mean. In contrast, we consider a Gaussian prior with a non-zero mean. Our setup better captures the common scenarios where the tasks share a signal. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "The empirical investigation of ICL in tractable statistical learning setups goes beyond linear regression settings. For more examples, researchers empirically studied the ICL ability of Transformers for decision trees (see [14], they also considered two-layer networks), algorithm selection [4], linear mixture models [26], learning Fourier series [2], discrete boolean functions [5], representation learning [13, 16], and reinforcement learning [17, 19]. Among all these settings, Transformers can either compete with the Bayes optimal estimators or expert-designed strong benchmarks. These works are not directly comparable with our paper. ", "page_idx": 2}, {"type": "text", "text": "Transformer implements gradient descent. A line of work interpreted the ICL of Transformers by their abilities to implement gradient descent (GD) [35, 3, 10, 1, 38, 4, 37]. In experiments, [35, 10] showed that (multi-layer) Transformer outputs are close to (multi-step) GD outputs. When specialized to linear regression tasks, [35] constructed a single linear self-attention (LSA) that implements onestep gradient descent with zero initialization (GD-0). Subsequently, [1] showed that optimal LSA models effectively correspond to GD-0 models, [38] proved that trained LSA models converge to GD-0 models, [37] showed that GD-0 models (hence LSA models) can provably achieve nearly Bayes optimal ICL in certain regimes and provided a sharp task complexity analysis of the pre-training. These papers focused on LSA models. Instead, we consider a linear Transformer block that also utilizes the MLP layer. ", "page_idx": 2}, {"type": "text", "text": "From an approximation theory perspective, [3, 4] showed that Transformers can implement multi-step GD under general losses. In comparison, we consider a limited setting of linear regression and show the LSA models can implement one-step GD with learnable initialization $\\left(\\mathsf{G D}\\!-\\!\\beta\\right)$ ,moreover,every optimal LTB model is effectively an GD- $_{\\beta}$ model. Both of our constructions utilize MLP layers in Transformers, highlighting its importance in reducing approximation error. Different from their results, we also show a negative approximation result that reveals the limitation of LSA models. ", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Model input. We use $\\textbf{x}\\in\\mathbb{R}^{d}$ and $y\\in\\mathbb R$ to denote a feature vector and its label, respectively. Throughout the paper, we assume a fixed number of context examples, denoted by $M>0$ . We denote the context examples by $(\\mathbf{X},\\mathbf{y})\\,\\in\\,\\mathbb{R}^{M\\times d}\\,\\times\\,\\mathbb{R}^{M}$ , where each row represents a context example, denoted by $(\\mathbf{x}_{i}^{\\top},y_{i})$ \uff0c $i=1,\\cdot\\cdot\\cdot,M$ . To formalize an ICL problem, the input of a model is a token matrix given by [1, 38] ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{E}:=\\binom{\\mathbf{X}^{\\top}}{\\mathbf{y}^{\\top}}\\quad\\mathbf{x}\\mathbf{\\Bigg)}\\in\\mathbb{R}^{(d+1)\\times(M+1)}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The output of a model corresponds to a prediction of $y$ ", "page_idx": 2}, {"type": "text", "text": "A Transformer block. Modern large language models are often made by stacking basic Transformer blocks (see, for example, [34]). A basic Transformer block consists of a self-attention layer and a multi-layer perceptron (MLP) layer [34], $\\mathbf{E}\\mapsto\\mathsf{M L P}$ [ATTN $\\mathbf{(E)}]$ . Here, the MLP layer is defined as MLP $(\\mathbf{E}):=\\mathbf{W}_{2}^{\\top}\\mathsf{R e L U}\\left(\\mathbf{W}_{1}\\mathbf{E}\\right)$ , where ReLU() refers to the entrywise rectified linear unit (ReLU) activation function, and $\\mathbf{W}_{1}$ \uff0c $\\mathbf{W}_{2}\\in\\mathbb{R}^{d_{f}\\times(d+1)}$ are two weight matrices. The self-attention layer (we focus on the single-head version in this paper) is defined as $\\mathsf{A T T N}(\\mathbf{E}):=\\mathbf{E}+\\mathbf{W}_{P}^{\\top}\\mathbf{W}_{V}\\mathbf{E}\\dot{\\mathbf{M}}\\,.$ $\\mathsf{s f m}\\!\\times\\!\\left(\\left(\\mathbf{W}_{K}\\mathbf{E}\\right)^{\\top}\\mathbf{W}_{Q}\\mathbf{E}\\right)$ , where $\\varsigma\\mathsf{f m}\\times(\\cdot)$ refers to the row-wise softmax operator, $\\mathbf{W}_{K}$ \uff0c $\\mathbf{W}_{Q}\\;\\in$ $\\mathbb{R}^{d_{k}\\times(d+1)}$ are the key and query matrices, respectively, $\\mathbf{W}_{P}$ \uff0c $\\mathbf{W}_{V}\\in\\mathbb{R}^{d_{v}\\times(d+1)}$ are the projection and value matrices, respectively, and $\\mathbf{M}$ is a fixed masking matrix given by ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{M}:=\\left(\\mathbf{I}_{M}\\quad0\\right)\\in\\mathbb{R}^{(M+1)\\times(M+1)}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "This mask matrix is included to reflect the asymmetric structure of a prompt since the label of query input $\\mathbf{x}$ is not included in the token matrix [1, 21]. In the above formulation, $d_{k},d_{v},d_{f}$ are three hyperparameters controlling the key size, value size, and width of the MLP layer, respectively. In the single-head case, it is common to set $d_{k}=d_{v}=d+1$ and $d_{f}=4(d+1)$ ,where $d+1$ corresponds to the embedding size (see, for example, [34]). Our formulation of a basic transformer block ignores all bias parameters and some popular techniques (such as layer normalization and dropout) to focus solely on the benefits brought by the model structure. ", "page_idx": 2}, {"type": "text", "text": "A linear Transformer block. To facilitate theoretical analysis, we ignore the non-linearities in the Transformer block (specifically, sfmx and ReLU) and work with a linear Transformer block (LTB) defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\nf_{\\mathbf{I}\\mathbf{T}\\mathbf{B}}:\\mathbb{R}^{(d+1)\\times(M+1)}\\rightarrow\\mathbb{R},\\quad\\mathbf{E}\\mapsto\\left[\\mathbf{W}_{2}^{\\top}\\mathbf{W}_{1}\\!\\left(\\mathbf{E}+\\mathbf{W}_{P}^{\\top}\\mathbf{W}_{V}\\mathbf{E}\\mathbf{M}\\frac{\\mathbf{E}^{\\top}\\mathbf{W}_{K}^{\\top}\\mathbf{W}_{Q}\\mathbf{E}}{M}\\right)\\right]_{-1,-1},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathbf{E}$ is the token matrix given by (3.1), $[\\mathbf{\\partial}\\cdot\\mathbf{\\partial}]_{-1,-1}$ refers to the bottom right entry of a matrix, $\\mathbf{M}$ is the fixed masking matrix given by (3.2). Here, the trainable parameters are $\\mathbf{W}_{P},\\mathbf{W}_{V}\\;\\in\\;\\mathbb{R}^{d_{v}\\times(d+1)}$ \uff0c $\\mathbf{W}_{K},\\mathbf{\\bar{W}}_{Q}\\;\\in\\;\\bar{\\mathbb{R}^{d_{k}\\times(d+1)}}$ , and $\\mathbf{W}_{1},\\mathbf{W}_{2}\\;\\;\\in\\;\\;\\mathbb{R}^{d_{f}\\times(d+1)}$ . We use the bottom right entry of the transformed token matrix as the model output to form a prediction of the label $y$ .The $1/M$ factor is a normalization factor in linear attention and can be absorbed into trainable parameters. Finally, we denote the hypothesis class formed by LTB models as $\\mathcal{F}_{\\mathrm{LTB}}:=\\{f_{\\mathrm{LTB}}:\\;\\mathbf{W}_{K},\\mathbf{W}_{Q},\\mathbf{W}_{V},\\mathbf{W}_{P},\\mathbf{W}_{1},\\mathbf{W}_{2},d_{k}\\stackrel{\\cdot}{\\geq}d,\\;d_{v}\\geq d\\}$ $d_{v}\\geq d+1$ \uff0c $d_{f}\\geq1\\}$ , where $f_{\\mathsf{L T B}}$ is defined in (3.3). ", "page_idx": 3}, {"type": "text", "text": "A linear self-attention. We will also consider a linear self-attention (LSA) defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\nf_{\\mathbf{LSA}}:\\mathbb{R}^{(d+1)\\times(M+1)}\\to\\mathbb{R},\\quad\\mathbf{E}\\mapsto\\left[\\mathbf{E}+\\mathbf{W}_{P}^{\\top}\\mathbf{W}_{V}\\mathbf{EM}\\frac{\\mathbf{E}^{\\top}\\mathbf{W}_{K}^{\\top}\\mathbf{W}_{Q}\\mathbf{E}}{M}\\right]_{-1,-1},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathbf{W}_{K},\\mathbf{W}_{Q},\\mathbf{W}_{P},\\mathbf{W}_{V}$ are trainable parameters. An LSA model can be viewed as an LTB model without the MLP layer (setting $d_{f}=d+1$ and ${\\bf W}_{1}={\\bf W}_{2}={\\bf I}$ 0. We remark that, unlike LTB, the residual connection in LSA plays no role because the bottom right entry of the prompt $\\mathbf{E}$ is zero (see (3.1). A variant of the LSA model has been studied by [1, 38, 37], where $\\mathbf{W}_{K}^{\\top}\\mathbf{W}_{Q}$ and $\\mathbf{W}_{P}^{\\top}\\mathbf{W}_{V}$ are respectively merged into one matrix parameter. Similarly, we denote the hypothesis class formed by LSA models as $\\mathcal{F}_{\\sf L S A}:=\\{f_{\\sf L S A}:\\mathbf{\\bar{W}}_{K},\\mathbf{W}_{Q},\\mathbf{W}_{V},\\mathbf{\\dot{W}}_{P},d_{k}\\geq d,d_{v}\\geq\\mathrm{i}\\}$ ,where $f_{{\\mathsf{L S A}}}$ is defined in (3.4). ", "page_idx": 3}, {"type": "text", "text": "Linear regression tasks with a shared signal. Assume that data and context examples are generated asfollows. ", "page_idx": 3}, {"type": "text", "text": "Assumption 3.1 (Distributional conditions). Assume that $(\\mathbf{X},\\mathbf{y},\\mathbf{x},y)$ are generated by: ", "page_idx": 3}, {"type": "text", "text": "\u00b7 First, a task parameter is independently generated by $\\widetilde{\\beta}\\sim\\mathcal{N}\\left(\\beta^{*},\\Psi\\right)$   \n\u00b7 The feature ectors are independently generaed by $\\mathbf{x},\\mathbf{x}_{1},\\ldots.\\mathbf{\\mu}.\\mathbf{x}_{M}\\overset{\\mathrm{i.i.d.}}{\\sim}\\mathcal{N}(0,\\mathbf{H})$   \n\u00b7 Then, the labels are generated by $y=\\langle\\widetilde{\\beta},\\mathbf{x}\\rangle+\\varepsilon,\\;\\;y_{i}=\\langle\\widetilde{\\beta},\\mathbf{x}_{i}\\rangle+\\varepsilon_{i},\\;i=1,\\ldots,M$ where $\\varepsilon$ and $\\varepsilon_{i}$ 's are independently generated by $\\varepsilon,\\varepsilon_{1},\\ldots,\\varepsilon_{M}\\overset{\\mathrm{i.i.d.}}{\\sim}\\mathcal{N}\\left(0,\\sigma^{2}\\right)$ ", "page_idx": 3}, {"type": "text", "text": "Here, $\\sigma^{2}\\geq0$ $\\mathbf{H}\\succeq\\mathbf{0}$ $\\Psi\\succeq{\\mathbf0}$ , and $\\beta^{*}\\in\\mathbb{R}^{d}$ are fixed but unknown quantities that govern the data distribution. We denote $\\boldsymbol{\\varepsilon}=\\left(\\varepsilon_{1},...,\\varepsilon_{M}\\right)^{\\top}$ \uff1a ", "page_idx": 3}, {"type": "text", "text": "We emphasize the importance of the mean of the task parameter $\\beta^{*}$ in Assumption 3.1. A non-zero $\\beta^{*}$ represents a shared signal across tasks, which is arguably common in practice. This assumption is implicitly used in [12] where they assumed task parameters are close to a meta parameter. In comparison, the prior works for ICL of linear regression [1, 38, 37] only considered a special case where $\\beta^{*}=0$ . In this special case, they showed that a single LSA layer can achieve nearly optimal ICL by approximating one-step gradient descent from zero initialization (GD-0). In more general caseswhere $\\beta^{*}$ is non-zero, we will show in Section 4 that LSA is insufficient to learn the shared signal and must incur an irreducible approximation error compared to LTB models. This sets our results apart from the prior papers. ", "page_idx": 3}, {"type": "text", "text": "ICL risk. We measure the ICL risk of a model $f$ by the mean squared error, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{R}(f):=\\mathbb{E}(f(\\mathbf{E})-y)^{2},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathbf{E}$ is defined in (3.1) and the expectation is over $\\mathbf{E}$ (equivalent to over $\\mathbf{X}$ \uff0c $\\mathbf{y}$ and $\\mathbf{x}$ and $y$ ", "page_idx": 3}, {"type": "text", "text": "4 Benefits of the MLP Component ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Our first main result separates the approximation abilities of the LTB and LSA models for ICL. Recall that an LSA model can be viewed as a special case of the LTB model with $\\mathbf{W}_{2}=\\mathbf{W}_{1}=\\mathbf{I}$ Sothe best ICL risk achieved by LTB models is no larger than the best ICL risk achieved by LSA models. However, our next theorem shows a strictly positive gap between the best ICL risks achieved by those two model classes. This result highlights the benefits of the MLP layer for reducing approximation error inTransformer. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Theorem 4.1 (Approximation gap). Consider the ICL risk defined by (3.5) and the two hypothesis classes $\\mathcal{F}_{\\sf L T B}$ and $\\mathcal{F}_{\\sf L S A}$ Suppose that Assumption 3.1 holds. Then we have ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{f\\in\\mathcal{F}_{\\mathrm{tSA}}}\\mathcal{R}\\left(f\\right)-\\operatorname*{inf}_{f\\in\\mathcal{F}_{\\mathrm{tTR}}}\\mathcal{R}\\left(f\\right)\\geq\\operatorname*{max}\\left\\{\\frac{2}{3\\left(M+1\\right)},\\frac{\\left(\\mathrm{tr}\\left(\\mathbf{H}\\Psi\\right)+\\sigma^{2}\\right)^{2}}{\\left(M+1\\right)^{2}\\mathrm{tr}\\left((\\mathbf{H}\\Psi)^{2}\\right)}\\right\\}\\left\\Vert\\beta^{*}\\right\\Vert_{\\mathbf{H}}^{2}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The proof of Theorem 4.1 is deferred to Appendix B. Theorem 4.1 reveals a gap in terms of the approximation abilities between the hypothesis set of LTB models and that of LSA models. Specifically, the best ICL performance achieved by LTB models is independent of $\\beta^{*}$ \uff0cwhilethe best ICL performance achieved by LSA models is sensitive to the norm of $\\beta^{*}$ . In particular, when $\\lVert\\beta^{*}\\rVert_{\\mathbf{H}}^{2}=\\Omega(M)$ , the best ICL risk of the former is smaller than that of the latter by at least a constant additive term. So when $\\beta^{*}$ is large, the hypothesis class formed by LSA models is restricted in its ability to perform effective ICL. We will show in Section 5 that the hypothesis class formed by LTB models can achieve nearly optimal ICL in this case. ", "page_idx": 4}, {"type": "text", "text": "We also remark that the $\\Theta(1/M)$ factor in the lower bound in Theorem 4.1 is not improvable. This is because LSA models can implement a one-step GD algorithm that is consistent for linear regression tasks (that is, the risk converges to the Bayes risk as the number of context examples goes to infinity), with an excess risk bound of $\\Theta(1/M)$ [38, 37]. So the approximation error gap is at most $\\Theta(1/{\\dot{M}})$ Nonetheles, the $\\Omega(\\lVert\\beta^{*}\\rVert_{\\mathbf{H}}^{2}/M)$ approximation gap between LSA models and LTB models shown by Theorem 4.1 suggests that $\\beta^{*}$ is not learnable by LSA models during pre-training. In contrast, we will show in Section 6 that LTB models can learn $\\beta^{*}$ during pre-training. ", "page_idx": 4}, {"type": "text", "text": "We emphasize that the ability of LTB to learn non-zero mean is a joint effect of an MLP component and a skip connection. Note that LSA also has a skip connection, but its skip connection is inactive. In comparison, the MLP component in LTB activates the skip connection. Therefore, we attribute the ability to learn non-zero mean to the MLP component, which is the only difference between LTB and LSA. Nonetheless, one can attribute the ability to learn non-zero mean to the skip connection: without a skip connection, LTB reduces to LSA with a potential rank constraint on the parameter, which cannot learn non-zero mean as we have proved. The above two explanations take different perspectives to interpret the same phenomenon. Finally, we remark that Theorem 4.1 holds even when $\\mathbf{H}$ and $\\Psi$ in Assumption 3.1 are not full rank. ", "page_idx": 4}, {"type": "text", "text": "Does scratchpad help? We have demonstrated that employing a single-layer LSA introduces an additional approximation error in the in-context learning problem for the linear regression task defined in Assumption 3.1. Nonetheless, are there alternative structures, apart from MLPs, that could potentially reduce this approximation error? One plausible strategy is to include a \u201cscratchpad\"' in the input token. Specifically, we construct the token matrix as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{E}:=\\left(\\mathbf{\\mathbf{X}}_{M}^{\\top}\\quad\\mathbf{x}\\right)\\in\\mathbb{R}^{(d+2)\\times(M+1)}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "which we then input into the LSA layer. We discuss the limitation of this scheme in Appendix J. Notably, this method does not successfully recover the GD- $_{\\beta}$ estimator defined in Section 5. We leave it as future work to see whether the token matrix with scratchpad could implement other types of estimators that more effectively address the linear regression tasks defined in Assumption 3.1, as well as whether additional structures could help alleviate this approximation error. ", "page_idx": 4}, {"type": "text", "text": "Experiments on GPT2. Theorem 4.1 shows the importance of the MLP component in LSA models for reducing approximation error. We also empirically validate this result by training a more complex GPT2 model [14] for the ICL tasks specified by Assumption 3.1. In the experiments, we use a GPT2-small model (with or without the MLP component) with 6 layers and 4 heads in each layer. The experiments follow the setting in [14], except that we train the model using a token matrix defined in (3.1). We considere two ICL settings, which instantiates Assumption 3.1 with $\\beta^{*}=(0,0,...,0)^{\\top}$ and $\\beta^{*}=(10,10,...,10)^{\\top}$ , respectively. We set $d=20,M=40,\\sigma=0,\\boldsymbol{\\Psi}=\\mathbf{H}=\\mathbf{I}_{d}$ in both settings. More experimental details are in Appendix I. In each setting, we train and test the model using the same data distribution. The experimental results are presented in Table 1. From Table 1, we observe that both models (with or without the MLP component) achieve a nearly zero loss when the task mean is zero. However, when the task mean is set away from zero, the GPT2 model with MLP component still performs relatively well while the GPT2 model without MLP component incurs a significantly larger loss. These empirical observations are consistent with our Theorem 4.1, indicating the benefits of MLP layers in reducing approximation error for ICL of linear regression with a shared signal. ", "page_idx": 4}, {"type": "image", "img_path": "Thou1rKdpZ/tmp/e46701bea8ce9bdeabd72385cdf03cd70e5d58b339cbaab61909be292acf9a80.jpg", "img_caption": ["Figure 1: The test loss along the training process for LTB and LSA layer. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Experiments on LSA and LTB. We trained both the LSA and LTB layers for $\\beta^{*}=(1,1,...,1)^{\\top}$ , and found that the trained LTB layer consistently achieved a significantly lower ICL risk than the LSA layer (see Figure 1). In these experiments, we adhered strictly to the previously defined LSA and LTB structures, setting the parameters as follows: $d\\,=\\,5$ $M=5$ $\\sigma=0$ , and $\\Psi=\\mathbf{H}=\\mathbf{I}_{d}$ . At each training step, we sampled $B=128$ new linear regression tasks. ", "page_idx": 5}, {"type": "table", "img_path": "Thou1rKdpZ/tmp/3dd77029b5632d49fd853c02272e43fc7e9c10b1b03ab71f6b374ca662652bb4.jpg", "table_caption": ["Table 1: Losses of GPT2 with or without MLP component for linear regression with a shared signal. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "In this part, we have shown that LSA models, the primary focus in previous ICL theory literature (see, e.g., [1, 38, 37] and references therein), are not sufficiently expressive for ICL of linear regression with a shared signal. In what follows, we will show LTB models are sufficient for this ICL problem. ", "page_idx": 5}, {"type": "text", "text": "5   LTB Implements One-Step GD with Learnable Initialization ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To understand the expressive power of the LTB models, we build a connection between $\\mathcal{F}_{\\sf L T B}$ and its subset of models which we call one-step $G D$ with learnable initialization (GD- $\\scriptstyle{\\mathcal{\\beta}}$ 0.We will first introduce GD- $\\beta$ models and then show that the best LTB models that minimize the ICL risk effectively belong to GD- $_{\\beta}$ models. ", "page_idx": 5}, {"type": "text", "text": "The GD- $_{/\\beta}$ models. A GD- $_{\\cdot\\beta}$ model is defined as ", "page_idx": 5}, {"type": "equation", "text": "$$\nf_{\\mathsf{G D-}\\beta}:\\mathbb{R}^{(d+1)\\times(M+1)}\\to\\mathbb{R},\\quad\\mathbf{E}\\mapsto\\left\\langle\\beta-\\mathbf{I}\\cdot{\\frac{1}{M}}\\mathbf{X}^{\\top}\\left(\\mathbf{X}\\beta-\\mathbf{y}\\right),\\mathbf{x}\\right\\rangle,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathbf{E}$ is the token matrix given by (3.1), $\\mathbf{T}\\in\\mathbb{R}^{d\\times d}$ and $\\beta\\in\\mathbb{R}^{d}$ are trainable parameters. Similarly, we define the function class formed by GD- $\\beta$ models as $\\begin{array}{r}{^{\\!}F_{\\mathsf{G D}-\\beta}:=\\{f_{\\mathsf{G D}-\\beta}:\\,\\dot{\\beta}\\in\\mathbb{R}^{d},}\\end{array}$ $\\mathbf{T}\\in\\mathbb{R}^{d\\times d}\\dot{\\}$ ", "page_idx": 5}, {"type": "text", "text": "A GD $_{\\beta}$ model computes a parameter that fits the context examples $(\\mathbf{X},\\mathbf{y})$ and uses that parameter to make a linear prediction of label $y$ on feature $\\mathbf{x}$ . More specifically, the first step is by using one gradient descent step with a matrix stepsize $\\mathbf{T}$ and an initialization $\\beta$ on the least square objective formed by context examples $(\\mathbf{X},\\mathbf{y})$ ", "page_idx": 6}, {"type": "text", "text": "LTB implements GD- $\\beta$ .AGD- $_{\\cdot\\beta}$ model (5.1) is a special case of an LTB model (3.3) by setting ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbf{W}_{2}^{\\top}\\mathbf{W}_{1}=\\left({\\bf\\Phi}_{\\beta^{\\top}}^{*}\\quad\\mathrm{\\boldmath~\\Omega~}_{1}^{*}\\right),\\quad\\mathbf{W}_{P}^{\\top}\\mathbf{W}_{V}=\\left({\\bf\\Phi}_{0_{d}^{\\top}}^{-\\mathbf{I}_{d}}\\quad\\mathrm{\\boldmath~\\Omega~}_{1}\\right),\\quad\\mathbf{W}_{K}^{\\top}\\mathbf{W}_{Q}=\\left({\\bf\\Phi}_{0_{d}^{\\top}}^{\\mathbf{\\top}}\\quad\\mathrm{\\boldmath~\\Omega~}_{*}\\right),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $^*$ denotes the entries that do not affect the model output (hence can be set to anything). Note that $\\mathbf{T}$ and $\\beta$ are free provided that $d_{v}\\geq d+1$ $d_{k}\\geq d$ and $d_{f}\\geq1$ (as required in $\\mathcal{F}_{\\sf L T B})$ . In sum, we have proved the following lemma showing that the set of GD- $\\beta$ models belongs to the set of LTB models. ", "page_idx": 6}, {"type": "text", "text": "Lemma 5.1.Wehave $\\begin{array}{r}{\\mathcal{F}_{\\mathtt{G D}\\cdot\\beta}\\subseteq\\mathcal{F}_{\\mathtt{L T B}\\cdot}\\,T h e r e f o r e,\\,\\operatorname*{inf}_{f\\in\\mathcal{F}_{\\mathtt{G D}\\cdot\\beta}}\\mathcal{R}(f)\\ge\\operatorname*{inf}_{f\\in\\mathcal{F}_{\\mathtt{L T B}}}\\mathcal{R}(f).}\\end{array}$ ", "page_idx": 6}, {"type": "text", "text": "OptimalGD $_{\\cdot\\beta}$ models for ICL. We now consider $\\mathcal{F}_{\\sf G D-\\beta}$ and its optimal ICL risk. We have the following theorem that computes the globally minimal ICL risk over $\\mathcal{F}_{\\sf G D-\\beta}$ and specifies the sufficient and necessary conditions for a global minimizer. The proof is deferred to Appendix C. ", "page_idx": 6}, {"type": "text", "text": "Theorem 5.2 (Optimal GD- $\\beta$ models). Consider the ICL risk defined by (3.5). Suppose that Assumption 3.1 holds. Then we have ", "page_idx": 6}, {"type": "text", "text": "\u00b7 The minimal ICL risk of $\\mathcal{F}_{\\sf G D-\\beta}$ is ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{f\\in{\\mathcal F}_{\\mathtt{G D}-\\beta}}\\mathcal{R}(f)=\\sigma^{2}+\\mathrm{tr}\\left(\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}^{\\frac{1}{2}}\\left(\\mathbf{I}-\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}^{\\frac{1}{2}}\\Omega^{-1}\\right)\\right),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "equation", "text": "where $\\Omega:=[(M+1){\\bf H}^{\\frac{1}{2}}\\Psi{\\bf H}^{\\frac{1}{2}}+(\\mathrm{tr}\\,({\\bf H}\\Psi)+\\sigma^{2}){\\bf I}_{d}]/M.$ ", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "\u00b7 The global optimal parameters for $f_{\\mathsf{G D}-\\beta}$ that attain the minimum (5.2) take the following form: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\beta=\\beta^{*}+\\mathrm{null\\left(H\\right),\\\\Delta{\\bfT}={\\bf T}^{*}+\\mathrm{null\\left(H^{\\otimes2}\\right),\\ \\ \\ }\\boldsymbol{w}h e r e\\ \\ \\ }{\\bf T}^{*}:=\\Psi{\\bf H}^{\\frac{1}{2}}\\Omega^{-1}{\\bf H}^{-\\frac{1}{2}}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "and $\\beta^{*}$ is the mean of the task parameter in Assumption 3.1 Here, null $\\left(\\mathbf{H}\\right):=\\left\\{z\\in\\mathbb{R}^{d}:\\mathbf{H}z=\\mathbf{0}\\right\\}$ is the null space of $\\mathbf{H}$ , and null $\\left(\\mathbf{H}^{\\otimes2}\\right)=\\left\\{\\pmb{Z}\\in\\mathbb{R}^{d\\times d}:\\mathbf{H}\\pmb{Z}\\mathbf{H}=\\mathbf{0}\\right\\}$ is the null space of $\\mathbf{H}^{\\otimes2}$ $I n$ particular, when $\\mathbf{H}$ is positive definite, the global optimal parameter is unique, $(\\beta,\\Gamma)=(\\beta^{*},\\Gamma^{*})$ ", "page_idx": 6}, {"type": "text", "text": "\u00b7 Under Assumption 3.1, the global optimal $f_{\\mathsf{G D}-\\beta}$ that attains the minimum (5.2) is unique as $a$ functionof $\\mathbf{E}$ (given by (3.1)) and takes the following form: ", "page_idx": 6}, {"type": "equation", "text": "$$\nf^{*}({\\bf E})=\\left\\langle\\beta^{*}-\\frac{1}{M}{\\bf T}^{*}{\\bf X}^{\\top}\\left({\\bf X}\\beta^{*}-{\\bf y}\\right),{\\bf x}\\right\\rangle.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Theorem 5.2 characterizes the optimal GD- $_{\\beta}$ models for ICL. In the above theorem, $\\Gamma^{*}\\to\\mathbf{H}^{-1}$ as the context length $M$ goes to infinity (assuming that $\\mathbf{H}$ is positive definite). In this case, the optimal GD- $_{\\beta}$ function (5.2) implements one Newton step from initialization $\\beta^{*}$ . With a finite context length $M$ , (5.2) implements one regularized Newton step from initialization $\\beta^{*}$ ", "page_idx": 6}, {"type": "text", "text": "Optimal LTB models for ICL. Lemma 5.1 shows that the best ICL risk achieved by an GD- $\\beta$ model is no smaller than the best ICL risk achieved by an LTB model. Surprisingly, our next theorem shows that the best ICL risk achieved by a GD- $_{\\cdot\\beta}$ is equal to that achieved by an LTB model. Therefore, the hypothesis set $\\mathcal{F}_{\\sf G D-\\beta}$ is diverse enough to match the approximation ability of the larger hypothesis set $\\mathcal{F}_{\\sf L T B}$ for ICL. ", "page_idx": 6}, {"type": "text", "text": "Theorem 5.3 (Optimal LTB models). Consider the ICL risk defined by (3.5). Suppose that Assumption 3.1 holds and that rank $\\begin{array}{r}{\\langle\\mathbf{H}^{\\frac{1}{2}}\\Psi^{\\frac{1}{2}}\\right\\rangle\\geq2}\\end{array}$ Thenwehave ", "page_idx": 6}, {"type": "text", "text": "\u00b7 The minimal ICL risk of $\\mathcal{F}_{\\sf L T B}$ and of $\\mathcal{F}_{\\sf G D-\\beta}$ are equal, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{f\\in\\mathcal{F}_{\\mathsf{L T B}}}\\mathcal{R}(f)=\\operatorname*{inf}_{f\\in\\mathcal{F}_{\\mathsf{G D}-\\beta}}\\mathcal{R}(f)=\\mathit{R H S}\\,o f(5.2)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "\u00b7 Rewrite an LTB model as fLTe in (3.3) with parameters ( $^*$ denotes parameters that do not affect the output) ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbf{W}_{2}^{\\top}\\mathbf{W}_{1}=\\left(\\!\\!\\begin{array}{l l}{*}&{*}\\\\ {\\gamma^{\\top}}&{*}\\end{array}\\!\\!\\right),\\quad\\mathbf{W}_{K}^{\\top}\\mathbf{W}_{Q}=\\left(\\!\\!\\begin{array}{l l}{\\mathbf{V}_{11}}&{*}\\\\ {\\mathbf{v}_{12}^{\\top}}&{*}\\end{array}\\!\\!\\right),\\quad\\mathbf{W}_{2}^{\\top}\\mathbf{W}_{1}\\mathbf{W}_{P}^{\\top}\\mathbf{W}_{V}=\\left(\\!\\!\\begin{array}{l l}{*}&{*}\\\\ {\\mathbf{v}_{21}^{\\top}}&{v_{-1}}\\end{array}\\!\\!\\right).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Then the suffcient and necessary conditions for $f_{\\mathsf{L T B}}\\in\\arg\\operatorname*{min}_{f\\in\\mathcal{F}_{\\mathsf{L T B}}}\\mathcal{R}(f)$ are ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{v_{-1}\\neq0,\\quad v_{-1}\\mathbf{v}_{12}\\in\\mathrm{null}\\left(\\mathbf{H}\\right),\\quad\\mathbf{v}_{21}\\in-v_{-1}\\beta^{*}+\\mathsf{n u l l}\\left(\\mathbf{H}\\right),\\quad\\gamma\\in\\beta^{*}+\\mathsf{n u l l}\\left(\\mathbf{H}\\right),}\\\\ &{v_{-1}\\mathbf{V}_{11}\\in\\mathbf{I}^{*^{\\top}}-v_{-1}\\beta^{*}\\mathbf{v}_{12}^{\\top}+\\mathsf{n u l l}\\left(\\mathbf{H}^{\\otimes2}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\beta^{*}$ is defined in Assumption 3.1 and $\\mathbf{T}^{*}$ is defined in (5.3). In particular, when $\\mathbf{H}$ is positive definite, the globally optimal parameter represented by $(\\mathbf{V}_{11},\\mathbf{v}_{12},\\mathbf{v}_{21},v_{-1},\\gamma)$ is unique up to $a$ rescaling of $v_{-1}$ ", "page_idx": 7}, {"type": "text", "text": "\u00b7Under Assumption 3.1, the globally optimal LTB model (that is, a function in ar $\\mathrm{g}\\,\\mathrm{min}_{f\\in\\mathcal{F}_{\\mathsf{L T B}}},$ is uniqueasafunctionof $\\mathbf{E}$ (given by (3.1)) and takes the form of (5.4) almost surely. ", "page_idx": 7}, {"type": "text", "text": "Lemma 5.1 and Theorem 5.3 together show that $\\mathcal{F}_{\\sf G D-\\beta}$ is a representative subset of $\\mathcal{F}_{\\sf L T B}$ that does not incur additional approximation error. In addition, every optimal LTB model is effectively an optimal GD- $_{\\beta}$ model when restricted to all possible token matrices. Note that the optimal model parameters for LTB or GD- $_{\\beta}$ are not unique because of redundant parameterization. But the optimal LTB and GD- $_{\\beta}$ models are unique as a function of all possible token matrices. The above holds even when $\\mathbf{H}$ and $\\Psi$ are potentially rank deficient. ", "page_idx": 7}, {"type": "text", "text": "Comparison with prior works. A line of papers considers LSA models for ICL under Assumption 3.1 with $\\beta^{*}=0$ [35, 1, 38, 37]. They show that LSA models can (effectively) implement all possible GD-0 models that specialize GD- $_{\\cdot\\beta}$ models by fixing $\\beta\\,=\\,{\\bf0}$ . In addition, they show that every optimal LSA model is (effectively) a GD-0 model for ICL under Assumption 3.1 with $\\beta^{*}=\\mathbf{0}$ (see, for example, Theorem 1 in [1]). In comparison, we consider a harder ICL problem that allows a large shared signal in tasks (that is, a large $\\beta^{*}$ in Assumption 3.1). In this setting, our Theorem 4.1 shows that $\\mathcal{F}_{\\sf L S A}$ (hence its subset formed by GD-0 models), as a subet of $\\mathcal{F}_{\\sf L T B}$ , incurs an additional approximation error propotional to $\\big\\|\\beta^{*}\\big\\|_{\\mathbf{H}}^{2}$ compared with $\\mathcal{F}_{\\sf L T B}$ . In contrast, $\\mathcal{F}_{\\sf G D-\\beta}$ , as a subset of $\\mathcal{F}_{\\sf L T B}$ , does not incur additional approximation error according to our Theorem 5.3. Thus the LSA and GD-0 models considered by [35, 1, 38, 37] are not capable of learning the shared signal $\\beta^{*}$ \uff0c while an LTB model can learn $\\beta^{*}$ through implementing GD- $_{\\beta}$ and encoding $\\beta^{*}$ in the initialization parameter. ", "page_idx": 7}, {"type": "text", "text": "6   Training and In-Context Learning of GD-beta ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We have shown that $\\mathcal{F}_{\\mathsf{G D}-\\beta}$ is a representative subset of $\\mathcal{F}_{\\sf L T B}$ that effectively contains every optimal LTB model. We now examine the ICL and training of GD- $_{\\beta}$ models. ", "page_idx": 7}, {"type": "text", "text": "Nearly optimal ICL with $\\mathsf{G D}\\!-\\!\\beta$ . We will compare the best ICL risk achieved by $\\mathsf{G D-}\\beta$ Withthe best ICL risk achieved by any estimator. The following lemma is an extension of Proposition 5.1 and Corollary 5.2 in [37] (which is based on [33]) that characterizes the Bayes optimal ICL risk among all estimators. ", "page_idx": 7}, {"type": "text", "text": "Lemma 6.1 (Bayes optimal ICL). Given a task-specific dataset $(\\mathbf{X},\\mathbf{y},\\mathbf{x},y)$ sampled according to Assumption 3.1, let $g\\left(\\mathbf{X},\\mathbf{y},\\mathbf{x}\\right)$ be an arbitrary estimator for $y$ andmeasure the average linear regression risk by $\\mathcal{L}\\left(g;\\dot{\\mathbf{X}}\\right):=\\mathbb{E}\\big[(g(\\mathbf{X},\\mathbf{y},\\mathbf{x})-y)^{2}\\mid\\mathbf{X}\\big]$ . It is clear that $\\mathbb{E}{\\mathcal{L}}\\left(g;\\mathbf{X}\\right)=\\mathcal{R}(g)$ Then, \u00b7The optimal estimator that minimizes the average linear regression risk ${\\mathcal{L}}(\\cdot;\\mathbf{X})$ is $g^{*}(\\mathbf{X},\\mathbf{y},\\mathbf{x})=$ $\\mathbf{x}^{\\top}{\\boldsymbol{\\beta}}^{*}+\\mathbf{x}^{\\top}\\Psi^{\\frac{1}{2}}\\left(\\Psi^{\\frac{1}{2}}\\mathbf{X}^{\\top}\\mathbf{X}\\Psi^{\\frac{1}{2}}+\\sigma^{2}\\mathbf{I}_{d}\\right)^{-1}\\!\\Psi^{\\frac{1}{2}}\\bar{\\mathbf{X}}^{\\top}\\left(\\mathbf{y}-\\mathbf{X}{\\boldsymbol{\\beta}}^{*}\\right).$ \u00b7 Assume the signal-to-noise ratio is upper bounded, that is, $\\mathrm{tr}\\left(\\mathbf{H}\\Psi\\right)\\,\\lesssim\\,\\sigma^{2}$ ,thenwithprobability at least $1\\mathrm{~-~}\\exp\\left(-\\Omega\\left(M\\right)\\right)$ over therandomness of $\\mathbf{X}$ , it holds that ${\\mathcal{L}}\\left(g^{*};\\mathbf{X}\\right)-{\\bar{\\sigma}}^{2}\\,\\simeq$ $\\textstyle\\sum_{i=1}^{d}\\operatorname*{min}\\{\\bar{\\phi},\\phi_{i}\\}$ , where $\\begin{array}{r}{\\bar{\\phi}\\approx\\frac{\\sigma^{2}}{M}}\\end{array}$ , and $(\\phi_{i})_{i\\geq1}$ are the eigenvalues of $\\Psi^{\\frac{1}{2}}\\mathbf{H}\\Psi^{\\frac{1}{2}}$ ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "The proof is deferred to Appendix F. Lemma 6.1 shows that the Bayes optimal estimator is a ridge regression estimator centered at $\\beta^{*}$ . This is consistent with [37] where the Bayes optimal estimator is a ridge regression estimator as they assumed $\\boldsymbol{\\beta}^{*}=0$ : The following corollary of Theorem 5.2 computes the rate of the ICL risk achieved by the optimal GD- $_{\\cdot\\beta}$ model. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Corollary 6.2. Under the setup of Theorem 5.2, additionaly assume the signal-to-noise ratio is upper bounded, that is, $\\mathrm{tr}\\left(\\Psi\\mathbf{H}\\right)\\lesssim\\sigma^{2}$ Then we have $\\begin{array}{r}{\\operatorname*{inf}_{f\\in\\mathcal{F}_{\\mathtt{G D},\\beta}}\\mathcal{R}(f)-\\sigma^{2}\\simeq\\sum_{i=1}^{d}\\operatorname*{min}\\left\\{\\bar{\\phi},\\phi_{i}\\right\\}}\\end{array}$ where $(\\phi_{i})_{i\\geq0}$ are the eigenvalues of $\\Psi^{\\frac{1}{2}}\\mathbf{H}\\Psi^{\\frac{1}{2}}$ and $\\bar{\\phi}:=[\\mathrm{tr}(\\Psi^{\\frac{1}{2}}{\\bf H}\\Psi^{\\frac{1}{2}})+\\sigma^{2}]/M\\simeq\\sigma^{2}/M$ ", "page_idx": 8}, {"type": "text", "text": "The optimal (expected) ICL risk achieved by $\\mathcal{F}_{\\sf G D-\\beta}$ in Corollary 6.2 matches the (high probability) Bayes optimal ICL risk in Lemma 6.1 ignoring constant factors, provided that the signal-to-noise ratio is upper bounded. Therefore $\\mathcal{F}_{\\sf G D-\\beta}$ achieves nearly Bayes optimal ICL risk. As a consequence, the larger hypothesis set $\\mathcal{F}_{\\sf L T B}$ also achieves nearly Bayes optimal ICL of linear regression under Assumption 3.1. ", "page_idx": 8}, {"type": "text", "text": "For the simplicity of discussion, we assume a fixed context length during pretraining and inference. Our discussions can be extended to allow a different context length during pretraining and inference using techniques in [37]. However, this is not the main focus of this work. ", "page_idx": 8}, {"type": "text", "text": "Optimization of GD- $_{\\cdot\\beta}$ with infinite tasks.We have shown that $\\mathcal{F}_{\\sf G D-\\beta}$ is a representative subset of $\\mathcal{F}_{\\sf L T B}$ that covers the optimal LTB models and achieves nearly optimal ICL risk. We now consider the optimization in the parameter space specified by $\\mathcal{F}_{\\sf G D-\\beta}$ . For simplicity, we follow [38] and consider gradient descent with an infinitesimal stepsize on the ICL objective with an infinite number of tasks. That is, we consider the optimization of gradient flow on the population ICL risk under the parameterization of GD- $_{\\beta}$ ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}\\beta(t)}{\\mathrm{d}t}=-\\frac{\\partial}{\\partial\\beta}\\mathcal{R}(f_{\\mathsf{G D-}\\beta}),\\quad\\frac{\\mathrm{d}\\Gamma(t)}{\\mathrm{d}t}=-\\frac{\\partial}{\\partial\\Gamma}\\mathcal{R}(f_{\\mathsf{G D-}\\beta}),\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\mathcal{R}$ is defined by (3.5) and $f_{\\mathsf{G D}-\\beta}$ is defined by (5.1). ", "page_idx": 8}, {"type": "text", "text": "The following theorem guarantees the global convergence of gradient flow. We introduce some notation to accommodate cases when $\\mathbf{H}$ is rank deficient. Let $\\mathcal{P}_{\\cal S}$ be the orthogonal projection operator onto a subspace $\\boldsymbol{S}$ . Let ${\\mathcal{H}}=\\mathsf{I m}\\left(\\mathbf{H}\\right)$ be the image space of matrix $\\mathbf{H}$ (viewing $\\mathbf{H}$ as a linear map) and ${\\mathcal{H}}^{\\perp}:=\\mathsf{n u l l}\\left(\\mathbf{H}\\right)$ be its orthogonal complement. Similarly, let $\\mathcal{Z}:=\\mathsf{I m}\\left(\\mathbf{H}^{\\otimes2}\\right)=$ ", "page_idx": 8}, {"type": "text", "text": "$\\{\\mathbf{H}Z\\mathbf{H},Z\\in\\mathbb{R}^{d\\times d}\\}$ be the image space of the operator $\\mathbf{H}^{\\otimes2}$ and $\\mathcal{Z}^{\\perp}$ be its orthogonal complement.   \nThen we have the following theorem. ", "page_idx": 8}, {"type": "text", "text": "Theorem 6.3. Consider the gradient fow defined by (6.1) with initialization $\\beta(0),\\Gamma(0)$ .We have, ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{P}_{\\mathcal{H}}\\left(\\beta(t)\\right)\\rightarrow\\mathcal{P}_{\\mathcal{H}}\\left(\\beta^{\\ast}\\right),\\quad\\mathcal{P}_{\\mathcal{H}^{\\bot}}\\left(\\beta(t)\\right)=\\mathcal{P}_{\\mathcal{H}^{\\bot}}\\left(\\beta(0)\\right),}\\\\ &{\\mathcal{P}_{\\mathcal{Z}}\\left(\\mathbf{r}(t)\\right)\\rightarrow\\mathcal{P}_{\\mathcal{Z}}\\left(\\Gamma^{\\ast}\\right),\\quad\\mathcal{P}_{\\mathcal{Z}^{\\bot}}\\left(\\mathbf{r}(t)\\right)=\\mathcal{P}_{\\mathcal{Z}^{\\bot}}\\left(\\mathbf{r}(0)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "as $t\\to\\infty$ . In particular, $i f\\mathbf{H}$ is positive definite, the gradient flow converges to the unique global minimizer of ICL risk over GD- $\\beta$ class, that is, $\\beta(t)\\to\\beta^{*}$ and $\\boldsymbol{\\Gamma}(t)\\to\\boldsymbol{\\Gamma}^{*}$ as $t\\to\\infty$ ", "page_idx": 8}, {"type": "text", "text": "The proof, as well as the convergence rate, is deferred to Appendix G. We remark that (6.1) is a complex dynamical system on a non-convex potential function of $\\beta$ and $\\mathbf{T}$ . We briefly discuss our proof techniques assuming that $\\mathbf{H}$ is full rank. The rank-deficient cases can be handled in the same way by applying appropriate project operators. To conquer the non-convex optimization issue, we observe that for every fixed $\\mathbf{T}$ , the potential as a function of $\\beta$ is smooth and strongly convex with a uniformly bounded condition number. This observation allows us to establish a uniform convergence for $\\beta$ .When $\\beta$ is sufficiently close to $\\beta^{*}$ , the potential as a function of $\\mathbf{T}$ is approximately convex, allowing us to track the convergence of $\\mathbf{T}$ ", "page_idx": 8}, {"type": "text", "text": "Theorem 6.3 shows that optimization of GD- $_{\\cdot\\beta}$ can be done efficiently by gradient flow without suffering from non-convexity. However, as we have shown in previous sections, LTB utilizes a more complex parameterization than $\\mathsf{G D}_{-}\\beta$ . So Theorem 6.3 does not imply optimization of LTB is easy. We leave it as future work to study the optimization and statistical complexity for directly learning LTB models. ", "page_idx": 8}, {"type": "text", "text": "7  Concluding Remarks ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this paper, we study the in-context learning of linear regression with a shared signal represented by a Gaussian prior with a non-zero mean. We show that although the linear self-attention layer discussed in prior works is consistent for this more complex task, its risk has an inevitable gap compared to that of the linear Transformer block (LTB), which is a linear self-attention layer followed by a linear multi-layer perception (MLP) layer. Next, we show that the effectiveness of the LTB arises because it can implement the one-step gradient descent estimator with learnable initialization $\\left(\\mathsf{G D}\\!-\\!\\beta\\right)$ Moreover, all global minimizers in the LTB class are equivalent to the unique global minimizer in the GD- $_{\\cdot\\beta}$ class, which can achieve nearly Bayes optimal in-context learning risk. Finally, we consider training on in-context examples and prove global convergence over the GD- $_{\\cdot\\beta}$ class of gradient flow on the population loss. Several future directions are worth discussing. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Optimization and statistical complexity. This paper provides an approximation theory of LTB and an optimization theory of $\\mathsf{G D-}\\beta$ . However, the statistical complexity of learning LTB or GD- $_{\\cdot\\beta}$ is not considered. The work by [37] provided techniques for analyzing the statistical task complexity for pre-training GD-o. An interesting direction is to extend their method to study the statistical complexity of learning GD- $_{\\beta}$ . However, their method crucially relies on the convexity of the risk induced GD-0, while we have shown that the risk induced by GD- $_{\\cdot\\beta}$ is non-convex. New ideas for dealing with non-convexity are needed here. ", "page_idx": 9}, {"type": "text", "text": "From LTB to Transformer block. We focus on LTB in this work, which simplifies a vanilla Transformer block by removing the non-linearities from the softmax self-attention and the ReLU activation in the MLP layers. This simplification allows us to obtain precise theoretical results for LTB (such as its connection to $\\mathsf{G D}_{-}\\beta$ ). On the other hand, non-linearities are arguably necessary for Transformers to work well in practice. An important next step is to further consider the theoretical benefits of non-linearities based on our current results. ", "page_idx": 9}, {"type": "text", "text": "Roles of MLP layers. The work by [15] (and references thereafter) empirically found that MLP layers operate as key-value memories that store human-interpretable patterns in some pre-trained Transformers. Their work motivated a method for locating and editing information stored in language models by modifying their MLP layers (see, e.g., [9, 23]). Our work proves that the MLP component enables LTB to learn the shared signal in linear regression tasks, which cannot be done by a single LSA component. We leave it as future work to theoretically clarify the information stored in the MLP component. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We gratefully acknowledge the support of the NSF and the Simons Foundation for the Collaboration on the Theoretical Foundations of Deep Learning through awards DMS-2031883 and #814639, and of the NSF through grant DMS-2023505. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1]  Kwangjun Ahn, Xiang Cheng, Hadi Daneshmand, and Suvrit Sra. Transformers learn to implement preconditioned gradient descent for in-context learning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[2]  Kabir Ahuja, Madhur Panwar, and Navin Goyal. In-context learning through the bayesian prism. In The Twelfth International Conference on Learning Representations, 2024.   \n[3] Ekin Akyirek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is in-context learning? investigations with linear models. In The Eleventh International Conference on Learning Representations, 2022.   \n[4] Yu Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei. Transformers as statisticians: Provable in-context learning with in-context algorithm selection. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[5] Satwik Bhattamishra, Arkil Patel, Phil Blunsom, and Varun Kanade. Understanding in-context learning in transformers and LLMs by learning to learn discrete functions. In The Twelfth International Conference on Learning Representations, 2024.   \n[6]  Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.   \n[7]  Yanda Chen, Ruiqi Zhong, Sheng Zha, George Karypis, and He He. Meta-learning via language model in-context tuning. arXiv preprint arXiv:2110.07814, 2021.   \n[8]  Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.   \n[9] Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei. Knowledge neurons in pretrained transformers. In Proceedings of the 6Oth Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8493-8502, 2022.   \n[10] Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma, Zhifang Sui, and Furu Wei. Why can GPT learn in-context? language models secretly perform gradient descent as meta-optimizers. In Findings of the Association for Computational Linguistics: ACL 2023, 2023.   \n[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv: 1810.04805, 2018.   \n[12]  Chelsea Finn, Pieter Abbel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In International conference on machine learning, pages 1126-1135. PMLR, 2017.   \n[13] Jingwen Fu, Tao Yang, Yuwang Wang, Yan Lu, and Nanning Zheng. How does representation impact in-context learning: A exploration on a synthetic task. arXiv preprint arXiv:2309.06054, 2023.   \n[14] Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can transformers learn in-context? a case study of simple function classes. Advances in Neural Information Processing Systems, 35:30583-30598, 2022.   \n[15] Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. Transformer feed-forward layers are key-value memories. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 2021.   \n[16] Tianyu Guo, Wei Hu, Song Mei, Huan Wang, Caiming Xiong, Silvio Savarese, and Yu Bai. How do transformers learn in-context beyond simple functions? a case study on learning with representations. In The Twelfth International Conference on Learning Representations, 2024.   \n[17] Jonathan N Lee, Annie Xie, Aldo Pacchiano, Yash Chandak, Chelsea Finn, Ofr Nachum, and Emma Brunskill. Supervised pretraining can learn in-context reinforcement learning. arXiv preprint arXiv:2306.14892, 2023.   \n[18] Yingcong Li, Muhammed Emrullah Idiz, Dimitris Papailiopoulos, and Samet Oymak. Transformers as algorithms: Generalization and stability in in-context learning. In International Conference on Machine Learning, pages 19565-19594. PMLR, 2023.   \n[19] Licong Lin, Yu Bai, and Song Mei. Transformers as decision makers: Provable in-context reinforcement learning via supervised pretraining. In The Twelfth International Conference on Learning Representations, 2024.   \n[20] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys, 55(9):1-35, 2023.   \n[21]  Arvind Mahankali, Tatsunori B Hashimoto, and Tengyu Ma One step of gradient descent is provably the optimal in-context learner with one layer of linear self-attention. In The Twelfth International Conference on Learning Representations, 2024.   \n[22]  AR Meenakshi and C Rajian. On a product of positive semidefinite matrices. Linear algebra and its applications, 295(1-3):3-6, 1999.   \n[23]  Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations in gpt. Advances in Neural Information Processing Systems, 35:17359-17372, 2022.   \n[24]  Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. Metaicl: Learning to learn in context. arXiv preprint arXiv:2110.15943, 2021.   \n[26] Reese Pathak, Rajat Sen, Weihao Kong, and Abhimanyu Das. Transformers can optimally learn regression mixture models. arXiv preprint arXiv:2311.08362, 2023.   \n[27] Kaare Brandt Petersen, Michael Syskind Pedersen, et al. The matrix cookbook. Technical University of Denmark, 7(15):510, 2008.   \n[28]  Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018.   \n[29] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.   \n[30] Allan Raventos, Mansheej Paul, Feng Chen, and Surya Ganguli. Pretraining task diversity and the emergence of non-bayesian in-context learning for regression. arXiv preprint arXiv:2306.15063, 2023.   \n[31] George AF Seber. A matrix handbook for statisticians. John Wiley & Sons, 2008.   \n[32] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.   \n[33]  Alexander Tsigler and Peter L Bartlett. Benign overfitting in ridge regression. J. Mach. Learn. Res., 24:123-1, 2023.   \n[34]  Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \n[35] Johannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, Joao Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. In International Conference on Machine Learning, pages 35151-35174. PMLR, 2023.   \n[36] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Huggingface's transformers: State-of-the-art natural language processing, 2020.   \n[37] Jingfeng Wu, Difan Zou, Zixiang Chen, Vladimir Braverman, Quanquan Gu, and Peter L Bartlett. How many pretraining tasks are needed for in-context learning of linear regression? In The Twelfth International Conference on Learning Representations, 2024.   \n[38]  Ruiqi Zhang, Spencer Frei, and Peter L Bartlett. Trained transformers learn linear models in-context. arXiv preprint arXiv:2306.09927, 2023. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A   Notation and Variable Transformation ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In order to simplify the proof, let's first describe a variable transformation scheme for our model and data coming from Assumption 3.1. ", "page_idx": 12}, {"type": "text", "text": "Definition A.1 (Variable Transformation). We fix $M$ as the length of the contexts. Recall that $\\mathbf{X},\\mathbf{x}$ are, respectively, the features in the context and the query input, while $\\widetilde{\\beta}$ and $\\beta^{*}$ are the true task parameter for the inference prompt and its expectation, respectively. Following the Assumption 3.1, wehave $\\widetilde{\\beta}\\sim\\mathcal{N}\\left(\\beta^{*},\\Psi\\right).$ From the definition for multivariate Gaussian distribution, we know there exists a random vector $\\widetilde{\\pmb{\\theta}}$ such that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\widetilde{\\boldsymbol{\\beta}}=\\boldsymbol{\\beta}^{*}+\\boldsymbol{\\Psi}^{\\frac{1}{2}}\\widetilde{\\boldsymbol{\\theta}},\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\widetilde{\\pmb{\\theta}}\\sim\\mathcal{N}\\left(0,\\mathbf{I}_{d}\\right).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Recallthe noise vector is defined as $\\boldsymbol{\\varepsilon}=\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\beta}$ and is generated from $\\varepsilon\\sim\\mathcal{N}\\left(\\mathbf{0},\\sigma^{2}\\cdot\\mathbf{I}_{M}\\right)$ ", "page_idx": 12}, {"type": "text", "text": "Rank deficient case Note that, even when $\\Psi$ is rank-deficient, the variable transformation in (A.1) and (A.2) still hold. This can be seen from the definition of the multivariate Gaussian distribution (Def 20.11 and the discussion below in [31]): A vector $\\widetilde{\\boldsymbol{\\beta}}\\in\\mathbb{R}^{d}$ with mean $\\beta^{*}$ and covariance matrix $\\Psi$ has a multivariate normal distribution, if it has the same distribution as $\\boldsymbol{A}\\widetilde{\\boldsymbol{\\theta}}+\\boldsymbol{\\beta}^{*}$ where $\\pmb{A}$ is any $d\\times m$ matrix satisfying $A\\pmb{A}^{\\top}=\\pmb{\\Psi}$ and $\\widetilde{\\pmb{\\theta}}\\sim\\mathcal{N}\\left(\\mathbf{0}_{m},\\mathbf{I}_{m}\\right)$ . Here, we can recover the variable transformation above if we take $m=d$ and $A=\\Psi^{\\frac{1}{2}}$ ", "page_idx": 12}, {"type": "text", "text": "Notation  Before we delve into the detailed proof, let's repeat the notation part with some additional notations. We use lowercase bold letters to note vectors and uppercase bold letters to denote matrices and tensors. For a vector $\\mathbf{x}$ and a positive semi-definite (PSD) matrix $\\pmb{A}$ , we denote $\\|x\\|_{A}:=\\sqrt{\\mathbf{x}^{\\top}A\\mathbf{x}}$ .We denote $\\langle\\cdot,\\cdot\\rangle$ as the inner product, which is defined as $\\langle\\mathbf{x},\\mathbf{y}\\rangle:=\\mathbf{x}^{\\top}\\mathbf{y}$ for vectors and $\\langle A,B\\rangle:=\\operatorname{tr}\\left(A B^{\\top}\\right)$ for matrices. For a matrix $\\pmb{A}$ , we denote $\\|A\\|_{o p}\\,,\\|A\\|_{F}$ as the operator norm and the Frobenius norm, respectively. We denote $A[i]$ as the $i$ -th row of the matrix $\\pmb{A}$ \uff0c $A_{m,n}$ as the $(m,n)$ -th entry, and $A_{-1,-1}$ as the right-botom entry. We denote $\\mathbf{0}_{n},\\mathbf{0}_{m\\times n},\\mathbf{I}_{n}$ as the zero vector, zero matrix and identity matrix, respectively. ", "page_idx": 12}, {"type": "text", "text": "For a positive semi-definite matrix $\\pmb{A}$ , we denote $A^{\\frac{1}{2}}$ for the principle square root of $\\pmb{A}$ , which is defined as the unique real matrix $_B$ such that $_B$ is positive semi-definite and $\\boldsymbol{B}\\boldsymbol{B}=\\boldsymbol{B}\\boldsymbol{B}^{\\intercal}=\\boldsymbol{A}$ For positive definite $\\pmb{A}$ , its principle square root is also positive definite. We denote $A^{+}$ as the Moore-Penrose pseudo-inverse for any matrix $\\pmb{A}$ . We also denote $A^{-{\\frac{1}{2}}}\\,=\\,\\left(A^{{\\frac{1}{2}}}\\right)^{+}$ . We denote $\\otimes$ as the Kronecker product. For compatible matrices of proper size $A,B,C,\\,B^{\\top}\\otimes A$ is a linear mapping which is defined by $\\left(\\dot{\\boldsymbol{B}^{\\top}}\\otimes\\boldsymbol{A}\\right)\\circ\\boldsymbol{C}=\\dot{\\boldsymbol{A C}}\\boldsymbol{B}$ We denote $\\begin{array}{r}{\\Lambda:=\\Psi^{\\frac{1}{2}}\\mathbf{H}\\Psi^{\\frac{1}{2}}}\\end{array}$ and $\\phi_{1}\\geq\\phi_{2}\\geq...\\geq\\phi_{d}\\geq0$ are its ordered eigenvalues. We also denote $\\lambda_{1}\\geq\\lambda_{2}\\geq...\\geq\\lambda_{d}\\geq0$ as the ordered eigenvalues of $\\mathbf{H}$ , and $\\lambda_{-1}>0$ as its minimal positive eigenvalue. Finally, we denote another important matrix ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\Omega:=\\frac{M+1}{M}\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}^{\\frac{1}{2}}+\\frac{\\mathrm{tr}\\left(\\mathbf{H}\\Psi\\right)+\\sigma^{2}}{M}\\cdot\\mathbf{I}_{d}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Note that, under this definition and our assumption on $\\mathbf{H}$ and $\\Psi$ wehave $\\Omega$ is invertible. ", "page_idx": 12}, {"type": "text", "text": "BProof of Theorem 4.1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Proof. The fact that $\\operatorname{inf}_{f\\in\\mathcal{F}_{\\lfloor\\mathsf{T B}}}\\mathcal{R}(f)$ does not depend on the vector $\\beta^{*}$ is subsumed in the Theorem 5.3, so we do not prove it here. We are going to prove the inequality in the theorem. First, from Theorem 5.3 we know that, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{f\\in\\mathcal{F}_{\\mathrm{LTB}}}\\mathcal{R}(f)=\\sigma^{2}+\\mathrm{tr}\\left(\\mathbf{H}\\boldsymbol{\\Psi}\\right)-\\mathrm{tr}\\left(\\left(\\mathbf{H}^{\\frac{1}{2}}\\boldsymbol{\\Psi}\\mathbf{H}^{\\frac{1}{2}}\\right)^{2}\\boldsymbol{\\Omega}^{-1}\\right),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\Omega$ is defined in (A.3). Then, it suffces to lower bound $\\operatorname{inf}_{f\\in\\mathcal{F}_{\\lfloor S\\mathsf{A}}}\\mathcal{R}(f)$ . So let's take an arbitrary $f\\in\\mathcal{F}_{\\mathsf{L S A}}$ , which is denoted as ", "page_idx": 13}, {"type": "equation", "text": "$$\nf(\\mathbf{E})=\\left[\\mathbf{E}+\\mathbf{W}_{P}^{\\top}\\mathbf{W}_{V}\\mathbf{EM}\\frac{\\mathbf{E}^{\\top}\\mathbf{W}_{K}^{\\top}\\mathbf{W}_{Q}\\mathbf{E}}{M}\\right]_{-1,-1},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\mathbf{E}$ is the token matrix defined in (3.1). Since the prediction is the right-bottom entry of the output matrix, we know only the last row of the product $\\mathbf{\\dot{W}}_{P}^{\\top}\\mathbf{W}_{V}$ attends the prediction. Similarly, only the last column of the $\\mathbf{E}$ on the far right in the above equation attends the prediction. Since the last column of $\\mathbf{E}$ .s $\\left(\\mathbf{x}^{\\top}\\mathbf{\\Sigma}_{0}\\right)^{\\top}$ , we know that only the first $d$ rows of the product $\\mathbf{W}_{K}^{\\top}\\mathbf{W}_{Q}$ enter the calculation (since other parts are multipled by zero). Therefore, we denote ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbf{W}_{P}^{\\top}\\,\\mathbf{W}_{V}=\\left({\\bf\\Phi}_{\\mathbf{u}_{21}^{\\top}}^{*}\\;\\;\\;\\;\\;*\\\\\\ {\\mathbf{u}_{21}^{\\top}}\\;\\;\\;\\;u_{-1}\\right),\\quad\\mathbf{W}_{K}^{\\top}\\,\\mathbf{W}_{Q}=\\left({\\bf U}_{11}\\;\\;\\;*\\right),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Where $\\mathbf{U}_{11}\\in\\mathbb{R}^{d\\times d},\\mathbf{u}_{12},\\mathbf{u}_{21}\\in\\mathbb{R}^{d\\times1},u_{-1}\\in\\mathbb{R}$ and $^*$ denotes entries that do not enter the final prediction. The model prediction can be written as ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{f({\\bf E})=\\left(\\!\\!\\begin{array}{c c}{\\!\\!\\mathbf{u}_{21}^{\\top}}&{\\!\\!\\!u_{-1}\\!}\\end{array}\\!\\!\\right)\\cdot\\frac{{\\bf E}{\\bf M}_{M}{\\bf E}^{\\top}}{M}\\cdot\\left(\\!\\!\\begin{array}{c}{\\!\\!\\mathbf{U}_{11}}\\\\ {\\!\\!\\mathbf{u}_{12}^{\\top}}\\end{array}\\!\\!\\right)\\cdot{\\bf x}}\\\\ &{=\\left[\\!\\!\\begin{array}{c c}{\\!\\!\\mathbf{u}_{21}^{\\top}\\cdot\\frac{1}{M}{\\bf X}^{\\top}{\\bf X}\\cdot{\\bf U}_{11}+\\!\\!\\mathbf{u}_{21}^{\\top}\\cdot\\frac{1}{M}{\\bf X}^{\\top}{\\bf y}\\cdot\\mathbf{u}_{12}^{\\top}+u_{-1}\\cdot\\frac{1}{M}{\\bf y}^{\\top}{\\bf X}\\cdot{\\bf U}_{11}+u_{-1}\\cdot\\frac{1}{M}{\\bf y}^{\\top}{\\bf y}\\cdot\\mathbf{u}_{12}^{\\top}}\\end{array}\\!\\!\\right]\\cdot{\\bf x}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Step 1: simplify the risk function. We use $\\widetilde{\\beta}$ to denote the task parameter. From the Assumption 3.1 and Definition A.1, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbf{y}=\\mathbf{X}{\\tilde{\\beta}}+\\boldsymbol{\\varepsilon},\\quad y=\\left\\langle{\\widetilde{\\beta}},\\mathbf{x}\\right\\rangle+\\boldsymbol{\\varepsilon},\\quad{\\widetilde{\\beta}}\\sim\\mathcal{N}\\left(\\beta^{*},\\Psi\\right),\\quad{\\widetilde{\\beta}}=\\beta^{*}+\\Psi^{\\frac{1}{2}}{\\widetilde{\\theta}};\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "and ", "page_idx": 13}, {"type": "equation", "text": "$$\n{\\bf X}[i],{\\bf x}\\stackrel{\\mathrm{i.i.d.}}{\\sim}\\mathcal{N}\\left({\\bf0},{\\bf H}\\right),\\quad\\varepsilon[i],\\varepsilon\\stackrel{\\mathrm{i.i.d.}}{\\sim}\\mathcal{N}\\left(0,\\sigma^{2}\\right),\\quad\\widetilde{\\pmb{\\theta}}\\sim\\mathcal{N}\\left({\\bf0},{\\bf I}_{d}\\right).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Then the model output can be written as ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{f({\\bf E})=\\left[{\\bf u}_{21}^{\\top}\\cdot\\frac{1}{M}{\\bf X}^{\\top}{\\bf X}\\cdot{\\bf U}_{11}+{\\bf u}_{21}^{\\top}\\cdot\\frac{1}{M}{\\bf X}^{\\top}{\\bf y}\\cdot{\\bf u}_{12}^{\\top}+u_{-1}\\cdot\\frac{1}{M}{\\bf y}^{\\top}{\\bf X}\\cdot{\\bf U}_{11}+u_{-1}\\cdot\\frac{1}{M}{\\bf y}^{\\top}{\\bf y}\\cdot{\\bf u}_{12}^{\\top}\\right]\\cdot{\\bf x}}}\\\\ {{\\displaystyle{=}\\left[{\\left({\\bf u}_{21}+u_{-1}{\\tilde{\\boldsymbol{\\beta}}}\\right)^{\\top}\\cdot\\frac{1}{M}{\\bf X}^{\\top}{\\bf X}\\cdot\\left({\\bf U}_{11}+{\\tilde{\\beta}}{\\bf u}_{12}^{\\top}\\right)}\\right]\\cdot{\\bf x}}\\\\ {{\\displaystyle\\qquad+\\left[{\\bf u}_{21}^{\\top}\\cdot\\frac{1}{M}{\\bf X}^{\\top}{\\boldsymbol{\\varepsilon}}\\cdot{\\bf u}_{12}^{\\top}+u_{-1}\\cdot\\frac{1}{M}{\\boldsymbol{\\varepsilon}}^{\\top}{\\bf X}\\cdot{\\bf U}_{11}+u_{-1}\\cdot\\frac{2}{M}{\\boldsymbol{\\varepsilon}}^{\\top}{\\bf X}{\\tilde{\\beta}}{\\bf u}_{12}^{\\top}+\\frac{1}{M}{\\boldsymbol{\\varepsilon}}^{\\top}{\\boldsymbol{\\varepsilon}}\\cdot u_{-1}{\\bf u}_{12}^{\\top}\\right]\\cdot{\\bf x}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "To simplify the presentation, we denote ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{z_{1}^{\\top}=\\left(\\mathbf{u}_{21}+u_{-1}\\widetilde{\\boldsymbol{\\beta}}\\right)^{\\top}\\cdot\\cfrac{1}{M}\\mathbf{X}^{\\top}\\mathbf{X}\\cdot\\left(\\mathbf{U}_{11}+\\widetilde{\\boldsymbol{\\beta}}\\mathbf{u}_{12}^{\\top}\\right),}\\\\ &{z_{2}^{\\top}=\\mathbf{u}_{21}^{\\top}\\cdot\\cfrac{1}{M}\\mathbf{X}^{\\top}\\boldsymbol{\\varepsilon}\\cdot\\mathbf{u}_{12}^{\\top}+u_{-1}\\cdot\\cfrac{1}{M}\\boldsymbol{\\varepsilon}^{\\top}\\mathbf{X}\\cdot\\mathbf{U}_{11}+u_{-1}\\cdot\\cfrac{2}{M}\\boldsymbol{\\varepsilon}^{\\top}\\mathbf{X}\\widetilde{\\boldsymbol{\\beta}}\\mathbf{u}_{12}^{\\top}}\\\\ &{z_{3}^{\\top}=\\cfrac{1}{M}\\boldsymbol{\\varepsilon}^{\\top}\\boldsymbol{\\varepsilon}\\cdot u_{-1}\\mathbf{u}_{12}^{\\top}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Since $\\mathbf{x},\\mathbf{X},\\varepsilon,\\widetilde{\\beta}$ are independent, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\mathcal R}\\left(f\\right)=\\mathbb{E}\\left(f(\\mathbf{E})-\\left\\langle\\widetilde{\\beta},\\mathbf{x}\\right\\rangle-\\varepsilon\\right)^{2}}\\\\ &{\\quad\\quad=\\sigma^{2}+\\mathbb{E}\\left(f(\\mathbf{E})-\\left\\langle\\widetilde{\\beta},\\mathbf{x}\\right\\rangle\\right)^{2}\\quad\\quad(\\varepsilon\\mathrm{~is~independent~from~other~variables~and~zero-ment~})}\\\\ &{\\quad\\quad=\\mathbb{E}\\left[\\left\\langle z_{1}+z_{2}+z_{3}-\\widetilde{\\beta},\\mathbf{x}\\right\\rangle^{2}\\right]+\\sigma^{2}}\\\\ &{\\quad\\quad=\\left\\langle\\mathbf{H},\\mathbb{E}\\left(z_{1}+z_{2}+z_{3}-\\widetilde{\\beta}\\right)\\left(z_{1}+z_{2}+z_{3}-\\widetilde{\\beta}\\right)^{\\top}\\right\\rangle+\\sigma^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Note that $z_{1}$ does not contain $\\varepsilon,\\,z_{2}$ is a linear form of $\\varepsilon$ , and $z_{3}$ is a quadratic form of $\\varepsilon$ . Using $\\varepsilon\\sim\\mathcal{N}(\\mathbf{0},\\sigma^{2}\\mathbf{I}_{d})$ , we have $\\mathbb{E}[(z_{1}-\\widetilde{\\beta})\\cdot z_{2}^{\\top}]=\\mathbf{0}$ and $\\mathbb{E}[z_{2}z_{3}^{\\top}]=\\mathbf{0}$ . Therefore, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{R}\\left(f\\right)-\\sigma^{2}=\\underbrace{\\left\\langle\\mathbf{H},\\mathbb{E}\\left(z_{1}-\\widetilde{\\beta}\\right)\\left(z_{1}-\\widetilde{\\beta}\\right)^{\\top}\\right\\rangle}_{S_{1}}+\\underbrace{\\left\\langle\\mathbf{H},\\mathbb{E}z_{2}z_{2}^{\\top}\\right\\rangle}_{S_{2}}+\\underbrace{\\left\\langle\\mathbf{H},\\mathbb{E}z_{3}z_{3}^{\\top}\\right\\rangle}_{S_{3}}}\\\\ &{\\quad\\quad\\quad+\\underbrace{2\\left\\langle\\mathbf{H},\\mathbb{E}\\left(z_{1}-\\widetilde{\\beta}\\right)z_{3}^{\\top}\\right\\rangle}_{S_{4}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Step 2: compute $S_{1}$ .By Lemma H.4 and that $\\mathbf{X}$ is independent of all other random variables, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\langle\\mathbf{H},\\mathbb{E}z_{1}z_{1}^{\\top}\\right\\rangle}\\\\ &{=\\!\\!\\!\\mathbb{E}\\!\\mathbf{tr}\\left[\\left(\\mathbf{U}_{11}+\\widetilde{\\beta}\\mathbf{u}_{12}^{\\top}\\right)^{\\top}\\cdot\\frac{1}{M}\\mathbf{X}^{\\top}\\mathbf{X}\\cdot\\left(\\mathbf{u}_{21}+u_{-1}\\widetilde{\\beta}\\right)\\left(\\mathbf{u}_{21}+u_{-1}\\widetilde{\\beta}\\right)^{\\top}\\cdot\\frac{1}{M}\\mathbf{X}^{\\top}\\mathbf{X}\\cdot\\left(\\mathbf{U}_{11}+\\widetilde{\\beta}\\mathbf{u}_{12}^{\\top}\\right)\\mathbf{H}\\right]}\\\\ &{=\\!\\!\\frac{M+1}{M}\\mathbb{E}_{\\widetilde{\\beta}}\\!\\mathrm{tr}\\left[\\left(\\mathbf{U}_{11}+\\widetilde{\\beta}\\mathbf{u}_{12}^{\\top}\\right)^{\\top}\\cdot\\mathbf{H}\\cdot\\left(\\mathbf{u}_{21}+u_{-1}\\widetilde{\\beta}\\right)\\left(\\mathbf{u}_{21}+u_{-1}\\widetilde{\\beta}\\right)^{\\top}\\cdot\\mathbf{H}\\cdot\\left(\\mathbf{U}_{11}+\\widetilde{\\beta}\\mathbf{u}_{12}^{\\top}\\right)\\mathbf{H}\\right]}\\\\ &{\\!\\!\\!\\!+\\!\\frac{1}{M}\\mathbb{E}_{\\widetilde{\\beta}}\\!\\mathrm{tr}\\left[\\mathrm{tr}\\left(\\left(\\mathbf{u}_{21}+u_{-1}\\widetilde{\\beta}\\right)\\left(\\mathbf{u}_{21}+u_{-1}\\widetilde{\\beta}\\right)^{\\top}\\mathbf{H}\\right)\\left(\\mathbf{U}_{11}+\\widetilde{\\beta}\\mathbf{u}_{12}^{\\top}\\right)^{\\top}\\mathbf{H}\\left(\\mathbf{U}_{11}+\\widetilde{\\beta}\\mathbf{u}_{12}^{\\top}\\right)\\mathbf{H}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We denote ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{b:=\\mathbf{u}_{21}+u_{-1}\\beta^{*}\\in\\mathbb{R}^{d},\\quad A:=\\mathbf{U}_{11}+\\beta^{*}\\mathbf{u}_{12}^{\\top}\\in\\mathbb{R}^{d\\times d},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "then applying $\\widetilde{\\beta}=\\beta^{*}+\\Psi^{\\frac{1}{2}}\\widetilde{\\pmb{\\theta}}$ , we get ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\mathbf{H},\\mathbb{E}z_{1}z_{1}^{\\top}\\rangle}\\\\ &{=\\frac{M+1}{M}\\mathbb{E}_{\\widetilde{\\theta}\\sim N(\\mathbf{0},\\mathbf{I}_{d})}\\mathrm{tr}\\left[\\left(A+\\Psi^{\\frac{1}{2}}\\widetilde{\\theta}\\mathbf{u}_{12}^{\\top}\\right)^{\\top}\\cdot\\mathbf{H}\\cdot\\left(b+u_{-1}\\Psi^{\\frac{1}{2}}\\widetilde{\\theta}\\right)\\left(b+u_{-1}\\Psi^{\\frac{1}{2}}\\widetilde{\\theta}\\right)^{\\top}\\cdot\\mathbf{H}\\cdot\\left(A+\\Psi^{\\frac{1}{2}}\\widetilde{\\theta}\\mathbf{u}_{12}^{\\top}\\right)\\right]}\\\\ &{\\vdash\\frac{1}{M}\\mathbb{E}_{\\widetilde{\\theta}\\sim N(\\mathbf{0},\\mathbf{I}_{d})}\\mathrm{tr}\\left(\\left(b+u_{-1}\\Psi^{\\frac{1}{2}}\\widetilde{\\theta}\\right)\\left(b+u_{-1}\\Psi^{\\frac{1}{2}}\\widetilde{\\theta}\\right)^{\\top}\\mathbf{H}\\right)\\mathrm{tr}\\left[\\left(A+\\Psi^{\\frac{1}{2}}\\widetilde{\\theta}\\mathbf{u}_{12}^{\\top}\\right)^{\\top}\\mathbf{H}\\left(A+\\Psi^{\\frac{1}{2}}\\widetilde{\\theta}\\mathbf{u}_{12}^{\\top}\\right)\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Note that the frst and third moments of $\\widetilde{\\pmb{\\theta}}$ are zero. So the above equation only involves the zeroth, second, and fourth moments of $\\widetilde{\\pmb{\\theta}}$ . Therefore we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left\\langle\\mathbf{H},\\mathbb{E}z_{1}z_{1}^{\\top}\\right\\rangle=\\underbrace{M+1}_{M}\\mathrm{tr}\\left(A^{\\top}\\mathbf{H}b b^{\\top}\\mathbf{H}A\\mathbf{H}\\right)+\\frac{1}{M}\\mathrm{tr}\\left(b b^{\\top}\\mathbf{H}\\right)\\mathrm{tr}\\left(A^{\\top}\\mathbf{H}A\\mathbf{H}\\right)_{+}+T_{2}+T_{4},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $T_{2}$ and $T_{4}$ are the second order and the fourth order term, respectively. More concretely, the second order term is ", "page_idx": 14}, {"type": "equation", "text": "$$\nT_{2}=\\frac{M+1}{M}\\mathbb{E}\\mathrm{tr}\\bigg\\{u_{-1}A^{\\top}\\mathbf{H}b\\widetilde{\\theta}^{\\top}\\Psi^{\\frac{1}{2}}\\mathbf{H}\\Psi^{\\frac{1}{2}}\\widetilde{\\theta}\\mathbf{u}_{12}^{\\top}\\mathbf{H}+u_{-1}\\mathbf{u}_{12}\\widetilde{\\theta}^{\\top}\\Psi^{\\frac{1}{2}}\\mathbf{H}\\Psi^{\\frac{1}{2}}\\widetilde{\\theta}b^{\\top}\\mathbf{H}A\\mathbf{H}\\bigg\\}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad_{\\mathbf{U},\\mathbf{U},\\mathbf{\\bar{U}}}\\equiv\\mathrm{P}\\mathbf{U}\\mathbf{U}\\bar{\\mathbf{A}}\\otimes\\mathrm{P}\\mathbf{U}\\mathbf{A}\\mathbf{H}\\mathbf{A}\\mathbf{H}+\\pi_{\\mathbf{A},\\mathbf{\\bar{A}}}\\mathbf{H}\\mathbf{\\bar{\\Phi}}\\mathbf{\\bar{A}}\\mathbf{H}+\\bar{\\Phi}\\mathbf{\\bar{A}}\\mathbf{H}\\mathbf{\\bar{a}}\\mathbf{H}}\\\\ &{\\quad+\\pi_{\\mathbf{U},\\mathbf{\\bar{U}}}\\otimes\\mathrm{P}\\mathbf{U}\\mathbf{A}\\mathbf{H}\\mathbf{\\bar{A}}\\mathbf{H}^{\\mathrm{op}}\\otimes\\mathrm{P}\\mathbf{U}_{i}\\mathbf{U},\\mathbf{\\bar{H}}+\\pi_{\\mathbf{A},\\mathbf{\\bar{A}}}^{\\mathrm{T}}\\mathbf{H}\\mathbf{\\bar{e}}\\mathbf{H}^{\\mathrm{in}}\\mathbf{A}\\mathbf{H}}\\\\ &{\\quad+\\frac{1}{\\mathbf{M}}\\mathbf{F}\\left\\{\\mathbf{\\bar{a}}_{i}^{2}}\\mathbf{\\Lambda}_{i}^{2}\\mathbf{\\bar{A}}^{\\mathrm{T}}\\mathbf{H}\\mathbf{\\bar{H}}^{\\mathrm{in}}\\mathbf{A}^{\\mathrm{T}}\\mathbf{\\bar{a}}\\mathbf{H}^{\\mathrm{in}}\\right\\}+\\delta\\mathbf{\\bar{V}}\\mathbf{H}\\mathbf{H}\\mathbf{\\bar{A}}\\mathbf{H}}\\\\ &{\\quad+2\\pi_{\\mathbf{A},\\mathbf{\\bar{A}}}^{2}N\\mathbf{\\bar{H}}^{\\mathrm{T}}\\mathbf{H}^{\\mathrm{in}}\\mathbf{\\bar{A}}\\mathbf{H}-\\left(\\alpha_{1}\\alpha_{1}^{\\mathrm{T}}\\mathbf{\\bar{A}}\\mathbf{H}\\mathbf{A}\\mathbf{H}\\right)+2\\alpha_{1}\\mathbf{\\bar{V}}\\mathbf{H}\\mathbf{\\bar{H}}^{\\mathrm{in}}\\mathbf{\\bar{A}}\\mathbf{\\bar{V}}\\mathbf{H}^{\\mathrm{in}}\\mathbf{A}^{\\mathrm{T}}\\mathbf{H}\\mathbf{\\bar{a}}\\mathbf{\\bar{A}}\\mathbf{H}}\\\\ &{\\quad-\\frac{M+1}{M}\\biggl\\{\\frac{M}{2}\\alpha_{1}\\mathbf{\\bar{A}}\\mathbf{H}^{\\mathrm{in}}\\mathbf{A}^{\\mathrm{T}}\\mathbf{H}\\mathbf{A}^{\\mathrm{T}}\\mathbf{H}^{\\mathrm{in}}\\mathbf{A}^{\\mathrm{T}}\\mathbf{H}^{\\mathrm{in}}\\mathbf{A}^{\\mathrm{T}}\\mathbf{H}^{\\\n$$The fourth order term is ", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T_{4}=\\cfrac{M+1}{M}\\mathbb{E}\\mathrm{tr}\\left\\{u_{-1}^{2}\\mathbf{u}_{12}\\widetilde{\\pmb{\\theta}}^{\\top}\\Psi^{\\frac{1}{2}}\\mathbf{H}\\Psi^{\\frac{1}{2}}\\widetilde{\\pmb{\\theta}}\\widetilde{\\pmb{\\theta}}^{\\top}\\Psi^{\\frac{1}{2}}\\mathbf{H}\\Psi^{\\frac{1}{2}}\\widetilde{\\pmb{\\theta}}\\mathbf{u}_{12}^{\\top}\\mathbf{H}\\right\\}}\\\\ &{\\qquad\\qquad\\qquad+\\cfrac{1}{M}\\mathbb{E}\\mathrm{tr}\\left\\{u_{-1}^{2}\\widetilde{\\pmb{\\theta}}^{\\top}\\Psi^{\\frac{1}{2}}\\mathbf{H}\\Psi^{\\frac{1}{2}}\\widetilde{\\pmb{\\theta}}\\cdot\\mathrm{tr}\\left(\\mathbf{u}_{12}\\widetilde{\\pmb{\\theta}}^{\\top}\\Psi^{\\frac{1}{2}}\\mathbf{H}\\Psi^{\\frac{1}{2}}\\widetilde{\\pmb{\\theta}}\\mathbf{u}_{12}^{\\top}\\mathbf{H}\\right)\\right\\}}\\\\ &{\\qquad=\\cfrac{M+2}{M}u_{-1}^{2}\\mathbf{u}_{12}^{\\top}\\mathbf{H}\\mathbf{u}_{12}\\cdot\\mathbb{E}\\left(\\widetilde{\\pmb{\\theta}}^{\\top}\\Psi^{\\frac{1}{2}}\\mathbf{H}\\Psi^{\\frac{1}{2}}\\widetilde{\\pmb{\\theta}}\\right)^{2}}\\\\ &{\\qquad=\\cfrac{M+2}{M}u_{-1}^{2}\\cdot\\left(2\\mathrm{tr}\\left(\\mathbf{H}\\Psi\\mathbf{H}\\Psi\\right)+\\mathrm{tr}\\left(\\mathbf{H}\\Psi\\right)^{2}\\right)\\cdot\\mathbf{u}_{12}^{\\top}\\mathbf{H}\\mathbf{u}_{12}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "For the cross term in $S_{1}$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\quad}&{\\left\\langle\\mathbf{H},\\mathbb{E}z_{1}\\widetilde{\\boldsymbol{\\beta}}^{\\top}\\right\\rangle}\\\\ &{=\\mathbb{E}\\Bigg\\{\\left(\\mathbf{u}_{21}+u_{-1}\\widetilde{\\boldsymbol{\\beta}}\\right)^{\\top}\\cdot\\frac{1}{M}\\mathbf{X}^{\\top}\\mathbf{X}\\cdot\\left(\\mathbf{U}_{11}+\\widetilde{\\boldsymbol{\\beta}}\\mathbf{u}_{12}^{\\top}\\right)\\mathbf{H}\\widetilde{\\boldsymbol{\\beta}}\\Bigg\\}}\\\\ &{=\\mathbb{E}\\Bigg\\{\\left(\\mathbf{u}_{21}+u_{-1}\\widetilde{\\boldsymbol{\\beta}}\\right)^{\\top}\\mathbf{H}\\left(\\mathbf{U}_{11}+\\widetilde{\\boldsymbol{\\beta}}\\mathbf{u}_{12}^{\\top}\\right)\\mathbf{H}\\widetilde{\\boldsymbol{\\beta}}\\Bigg\\}}&{\\mathrm{(From~the~distribution~of~}\\mathbf{X}}\\\\ &{=\\mathbb{E}\\Bigg\\{\\left(b+u_{-1}\\Psi^{\\top}\\widetilde{\\boldsymbol{\\beta}}\\right)^{\\top}\\mathbf{H}\\left(A+\\Psi^{\\top}\\widetilde{\\boldsymbol{\\beta}}\\mathbf{u}_{12}^{\\top}\\right)\\mathbf{H}\\left(\\beta^{\\ast}+\\Psi^{\\frac{1}{2}}\\widetilde{\\boldsymbol{\\theta}}\\right)\\Bigg\\}}&{\\mathrm{(By~(B.3)~}}\\\\ &{=b^{\\top}\\mathbf{H}\\mathbf{A}\\mathbf{H}\\beta^{\\ast}+\\mathbb{E}\\Bigg\\{u_{-1}\\widetilde{\\boldsymbol{\\beta}}^{\\top}\\Psi^{\\top}\\mathbf{H}\\Psi^{\\top}\\widetilde{\\boldsymbol{\\beta}}\\mathbf{u}_{12}^{\\top}\\mathbf{H}\\beta^{\\ast}+u_{-1}\\widetilde{\\boldsymbol{\\beta}}^{\\top}\\Psi^{\\bot}\\mathbf{H}\\mathbf{A}\\mathbf{H}\\Psi^{\\bot}\\widetilde{\\boldsymbol{\\theta}}+b^{\\top}\\mathbf{H}\\Psi^{\\bot}\\widetilde{\\boldsymbol{\\theta}}\\mathbf{u}_{12}^{\\top}\\mathbf{H}\\Psi^{\\bot}\\widetilde{\\boldsymbol{\\theta}}}\\\\ &{=b^{\\top}\\mathbf{H}\\mathbf{A}\\mathbf{H}\\beta^{\\ast}+u_{-1}\\mathrm{tr}\\left(\\mathbf{H}\\Psi\\right)\\cdot\\mathbf{u}_{12}^{\\top}\\mathbf{H}\\beta^{\\ast}+u_{-1}\\mathrm{tr}\\left(\\mathbf{H}\\mathbf{A}\\mathbf{H}\\Psi\\right)+\\mathbf{u}_{12}^{\\top}\\mathbf{H}\\Psi\\mathbf{H}b,}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the last row comes from the fact that . Moreover, we have ", "page_idx": 15}, {"type": "text", "text": "$\\widetilde{\\pmb{\\theta}}\\sim\\mathcal{N}\\left(\\mathbf{0},\\mathbf{I}_{d}\\right)$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left\\langle\\mathbf{H},\\mathbb{E}\\widetilde{\\beta}\\widetilde{\\boldsymbol{\\beta}}^{\\top}\\right\\rangle=\\boldsymbol{\\beta}^{*\\top}\\mathbf{H}\\boldsymbol{\\beta}^{*}+\\mathrm{tr}\\left(\\mathbf{H}\\boldsymbol{\\Psi}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Combining (B.4), (B.5), (B.6), (B.8), (B.9), we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{S_{1}=\\displaystyle\\frac{M+1}{M}\\mathrm{tr}\\left({\\cal A}^{\\top}{\\bf H}b b^{\\top}{\\bf H}A{\\bf H}\\right)+\\displaystyle\\frac{1}{M}\\mathrm{tr}\\left(b b^{\\top}{\\bf H}\\right)\\mathrm{tr}\\left({\\cal A}^{\\top}{\\bf H}A{\\bf H}\\right)}}\\\\ {{\\displaystyle\\quad+\\ \\frac{M+2}{M}u_{-1}^{2}\\cdot\\left(2\\mathrm{tr}\\left({\\bf H}\\Psi{\\bf H}\\Psi\\right)+\\mathrm{tr}\\left({\\bf H}\\Psi\\right)^{2}\\right)\\cdot{\\bf u}_{12}^{\\top}{\\bf H}{\\bf u}_{12}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{+\\;\\frac{2(M+1)}{M}u_{-1}b^{\\top}\\left[\\mathrm{tr}(\\mathbf{H}\\Psi)\\mathbf{H}+\\mathbf{H}\\Psi\\mathbf{H}\\right]A\\mathbf{H}\\mathbf{u}_{12}}\\\\ &{+\\;u_{-1}^{2}\\mathrm{tr}\\left(A^{\\top}\\left(\\frac{M+1}{M}\\mathbf{H}\\Psi\\mathbf{H}+\\frac{1}{M}\\mathrm{tr}(\\mathbf{H}\\Psi)\\mathbf{H}\\right)A\\mathbf{H}\\right)+\\frac{4}{M}u_{-1}\\cdot b^{\\top}\\mathbf{H}\\Psi\\mathbf{H}A\\mathbf{H}\\mathbf{u}_{12}}\\\\ &{-\\;2\\bigg[b^{\\top}\\mathbf{H}A\\mathbf{H}\\beta^{*}+u_{-1}\\mathrm{tr}\\left(\\mathbf{H}\\Psi\\right)\\cdot\\mathbf{u}_{12}^{\\top}\\mathbf{H}\\beta^{*}+u_{-1}\\mathrm{tr}\\left(\\mathbf{H}A\\mathbf{H}\\Psi\\right)}\\\\ &{+\\;\\mathbf{u}_{12}^{\\top}\\mathbf{H}\\Psi\\mathbf{H}b\\bigg]+\\beta^{*\\top}\\mathbf{H}\\beta^{*}+\\mathrm{tr}\\left(\\mathbf{H}\\Psi\\right)+b^{\\top}\\left(\\frac{M+1}{M}\\mathbf{H}\\Psi\\mathbf{H}+\\frac{1}{M}\\mathrm{tr}(\\mathbf{H}\\Psi)\\mathbf{H}\\right)b\\cdot\\mathbf{u}_{12}^{\\top}\\mathbf{H}\\mathbf{u}_{12}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Step 3: other terms. Let us compute $S_{2},S_{3}$ and $S_{4}$ . Using definitions, we rewrite $z_{2}$ as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{z_{2}^{\\top}=\\mathbf{u}_{21}^{\\top}\\cdot\\cfrac{1}{M}\\mathbf{X}^{\\top}\\varepsilon\\cdot\\mathbf{u}_{12}^{\\top}+u_{-1}\\cdot\\cfrac{1}{M}\\varepsilon^{\\top}\\mathbf{X}\\cdot\\mathbf{U}_{11}+u_{-1}\\cdot\\cfrac{2}{M}\\varepsilon^{\\top}\\mathbf{X}\\widetilde{\\beta}\\mathbf{u}_{12}^{\\top}}\\\\ {=b^{\\top}\\cdot\\cfrac{1}{M}\\mathbf{X}^{\\top}\\varepsilon\\cdot\\mathbf{u}_{12}^{\\top}+u_{-1}\\cdot\\cfrac{1}{M}\\varepsilon^{\\top}\\mathbf{X}\\cdot A+u_{-1}\\cdot\\cfrac{2}{M}\\varepsilon^{\\top}\\mathbf{X}\\Psi^{\\top}\\widetilde{\\theta}\\mathbf{u}_{12}^{\\top}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Since $\\boldsymbol{\\varepsilon}\\sim\\mathcal{N}\\left(\\mathbf{0},\\sigma^{2}\\mathbf{I}_{M}\\right),\\boldsymbol{\\widetilde{\\theta}}\\sim\\mathcal{N}\\left(\\mathbf{0},\\mathbf{I}_{d}\\right)$ and they are independent, al terms in $S_{2}$ vanish except if the term contains even orders of $\\widetilde{\\pmb{\\theta}}$ or $\\varepsilon$ . So we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{\\mathsf{S}}_{2}=\\left\\langle\\mathbf{H},\\mathbb{E}z_{2}z_{2}^{\\top}\\right\\rangle=\\cfrac{1}{M^{2}}\\mathbb{E}\\bigg\\lbrace b^{\\top}\\mathbf{X}^{\\top}\\boldsymbol{\\varepsilon}\\cdot\\mathbf{u}_{12}^{\\top}\\mathbf{H}\\mathbf{u}_{12}\\cdot\\boldsymbol{\\varepsilon}^{\\top}\\mathbf{X}\\cdot b+u_{-1}^{2}\\boldsymbol{\\varepsilon}^{\\top}\\mathbf{X}A\\mathbf{H}A^{\\top}\\mathbf{X}^{\\top}\\boldsymbol{\\varepsilon}}\\\\ &{\\phantom{=}+2u_{-1}\\cdot\\boldsymbol{\\varepsilon}^{\\top}\\mathbf{X}A\\mathbf{H}\\mathbf{u}_{12}\\cdot\\boldsymbol{\\varepsilon}^{\\top}\\mathbf{X}b+4u_{-1}^{2}\\cdot\\boldsymbol{\\varepsilon}^{\\top}\\mathbf{X}\\Psi^{\\frac{1}{2}}\\tilde{\\boldsymbol{\\theta}}\\cdot\\mathbf{u}_{12}^{\\top}\\mathbf{H}\\mathbf{u}_{12}\\cdot\\tilde{\\boldsymbol{\\theta}}^{\\top}\\mathbf{y}^{\\top}\\mathbf{X}^{\\top}\\boldsymbol{\\varepsilon}\\bigg\\rbrace}\\\\ &{\\phantom{=}\\,\\frac{\\sigma^{2}}{M}\\bigg\\lbrace b^{\\top}\\mathbf{H}b\\cdot\\mathbf{u}_{12}^{\\top}\\mathbf{H}\\mathbf{u}_{12}+u_{-1}^{2}\\mathrm{tr}\\left(\\mathbf{H}A\\mathbf{H}A^{\\top}\\right)+2u_{-1}\\mathbf{u}_{12}^{\\top}\\mathbf{H}A^{\\top}\\mathbf{H}b+4u_{-1}^{2}\\mathrm{tr}\\left(\\mathbf{H}\\Psi\\right)\\mathbf{u}_{12}^{\\top}\\mathbf{H}\\mathbf{u}_{12}\\cdot\\mathbf{\\Lambda}}\\\\ &{\\phantom{=}-\\cfrac{\\sigma^{2}}{M}\\bigg\\lbrace b^{\\top}\\mathbf{H}b\\cdot\\mathbf{u}_{12}^{\\top}\\mathbf{H}\\mathbf{u}_{12}+u_{-1}^{2}\\mathrm{tr}\\left(\\mathbf{H}A\\mathbf{H}A^{\\top}\\right)+2u_{-1}\\mathbf{u}_{12}^{\\top}\\mathbf{H}A^{\\top}\\mathbf{H}b+4u_{-1}^{2}\\mathrm{tr}\\left(\\mathbf{H}\\Psi\\right)\\mathbf{u}_{12}^{\\top}\\mathbf{H}\\mathbf{u}_{12}\\bigg\\rbrace}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "For the other two terms, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\nS_{3}=\\frac{1}{M^{2}}u_{-1}^{2}\\mathbb{E}\\big\\{\\boldsymbol{\\varepsilon}^{\\top}\\boldsymbol{\\varepsilon}\\mathbf{u}_{12}^{\\top}\\mathbf{H}\\mathbf{u}_{12}\\boldsymbol{\\varepsilon}^{\\top}\\boldsymbol{\\varepsilon}\\big\\}=\\frac{\\sigma^{4}(M+2)}{M}u_{-1}^{2}\\mathbf{u}_{12}^{\\top}\\mathbf{H}\\mathbf{u}_{12}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{S_{4}=2\\left\\langle\\mathbf{H},\\mathbb{E}\\left(z_{1}-\\widetilde{\\beta}\\right)z_{3}^{\\top}\\right\\rangle}\\\\ &{\\quad=2\\mathbb{E}\\Bigg\\{\\left[\\left(\\mathbf{u}_{21}+u_{-1}\\widetilde{\\beta}\\right)^{\\top}\\cdot\\frac{1}{M}\\mathbf{X}^{\\top}\\mathbf{X}\\cdot\\left(\\mathbf{U}_{11}+\\widetilde{\\beta}\\mathbf{u}_{12}^{\\top}\\right)-\\widetilde{\\beta}^{\\top}\\right]\\mathbf{H}\\cdot\\frac{1}{M}\\boldsymbol{\\varepsilon}^{\\top}\\boldsymbol{\\varepsilon}\\cdot u_{-1}\\mathbf{u}_{12}^{\\top}\\Bigg\\}}\\\\ &{\\quad=2\\sigma^{2}u_{-1}\\mathbb{E}\\Bigg\\{\\left[\\left(b+u_{-1}\\Psi^{\\perp}\\widetilde{\\theta}\\right)^{\\top}\\cdot\\mathbf{H}\\cdot\\left(A+\\Psi^{\\perp}\\widetilde{\\theta}\\mathbf{u}_{12}^{\\top}\\right)-\\widetilde{\\beta}^{\\ast\\top}-\\widetilde{\\theta}^{\\top}\\Psi^{\\perp}\\right]\\mathbf{H}\\mathbf{u}_{12}\\right\\}}\\\\ &{\\quad=2\\sigma^{2}u_{-1}\\left[b^{\\top}\\mathbf{H}A-\\widetilde{\\beta}^{\\top}\\right]\\mathbf{H}\\mathbf{u}_{12}+2\\sigma^{2}u_{-1}^{2}\\mathrm{tr}\\left(\\mathbf{H}\\Psi\\right)\\cdot\\mathbf{u}_{12}^{\\top}\\mathbf{H}\\mathbf{u}_{12}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Step 4: combine all parts. Combining the four parts (B.10), (B.12), (B.13) and (B.14), we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{R}\\left(f\\right)-\\sigma^{2}=S_{1}+S_{2}+S_{3}+S_{4}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We remark that in the (B.10), (B.12), (B.13) and (B.14), the parameters are $A,b,u_{-1}$ and $\\mathbf{u}_{12}$ , instead of the original parameter $\\mathbf{U}_{11},\\mathbf{u}_{12},\\mathbf{u}_{21},u_{-1}$ . From the definitions of $\\pmb{A}$ and $^{b}$ , we know there is a bijective map between $(\\mathbf{U}_{11},\\mathbf{u}_{12},\\mathbf{u}_{21},u_{-1})$ and $(A,b,{\\mathbf u}_{12},u_{-1})$ , so these two parameterizations are equivalent when computing the minimal risk achieved by a model in a hypothesis class. From the expression of $S_{1},S_{2},s_{3},S_{4}$ , we can rewrite the risk as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{R}\\left(f\\right)-\\sigma^{2}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "$\\begin{array}{r l}&{=b^{\\top}\\left(\\frac{M+1}{M}\\mathbf{H}\\bar{\\Psi}\\mathbf{H}+\\frac{1}{M}\\mathrm{tr}(\\mathbf{H}\\bar{\\Psi})\\mathbf{H}\\right)b\\cdot\\mathbf{u}_{12}^{\\top}\\mathbf{H}\\mathbf{u}_{12}+\\frac{\\sigma^{2}}{M}\\cdot b^{\\top}\\mathbf{H}b\\cdot\\mathbf{u}_{12}^{\\top}\\mathbf{H}\\mathbf{u}_{12}}\\\\ &{\\quad+\\,u_{-1}^{2}\\mathrm{tr}\\left(A^{\\top}\\left(\\frac{M+1}{M}\\mathbf{H}\\bar{\\Psi}\\mathbf{H}+\\frac{1}{M}\\mathrm{tr}(\\mathbf{H}\\bar{\\Psi})\\mathbf{H}\\right)A\\mathbf{H}\\right)+\\frac{\\sigma^{2}}{M}\\cdot\\mathrm{tr}\\left(\\mathbf{H}A\\mathbf{H}A^{\\top}\\right)}\\\\ &{\\quad+2u_{-1}b^{\\top}\\left[\\frac{1}{M}\\mathrm{tr}(\\mathbf{H}\\bar{\\Psi})\\mathbf{H}+\\frac{M+1}{M}\\mathbf{H}\\bar{\\Psi}\\mathbf{H}+\\frac{\\sigma^{2}}{M}\\cdot\\mathbf{H}\\right]A\\mathbf{H}\\mathbf{u}_{12}-2u_{-1}\\mathrm{tr}\\left(\\mathbf{H}A\\mathbf{H}\\bar{\\Psi}\\right)-2\\mathbf{u}_{12}^{\\top}\\mathbf{H}}\\end{array}$ \u4e9aHb$\\begin{array}{r l}&{\\displaystyle+\\underbrace{\\frac{1}{M}\\bigg[b^{\\top}\\mathbf{H}b\\cdot\\mathrm{tr}\\left(A^{\\top}\\mathbf{H}A\\mathbf{H}\\right)+4u_{-1}\\cdot b^{\\top}\\mathbf{H}\\Psi\\mathbf{H}A\\mathbf{H}\\mathbf{u}_{12}+4u_{-1}^{2}\\cdot\\mathrm{tr}\\left(\\mathbf{H}\\Psi\\mathbf{H}\\Psi\\right)\\cdot\\mathbf{u}_{12}^{\\top}\\mathbf{H}\\mathbf{u}_{12}\\bigg]}_{\\mathrm{~\\ensuremath{~u~l~f~}~}}}\\\\ &{\\displaystyle+\\underbrace{b^{\\top}\\left(\\frac{M+1}{M}\\mathbf{H}A\\mathbf{H}A^{\\top}\\mathbf{H}\\right)b-2b^{\\top}\\mathbf{H}A\\mathbf{H}\\beta^{*}+2u_{-1}\\mathrm{tr}(\\mathbf{H}\\Psi)\\cdot b^{\\top}\\mathbf{H}A\\mathbf{H}\\mathbf{u}_{12}+2\\sigma^{2}b^{\\top}\\mathbf{H}A\\mathbf{H}\\left(\\mathbf{H}\\right)}_{\\mathrm{~\\ensuremath{~w~}~}}}\\\\ &{\\displaystyle+\\underbrace{\\beta^{*\\top}\\mathbf{H}\\beta^{*}-2\\left(\\mathrm{tr}\\left(\\mathbf{H}\\Psi\\right)+\\sigma^{2}\\right)\\cdot\\beta^{*\\top}\\mathbf{H}\\left(u_{-1}\\mathbf{u}_{12}\\right)+\\mathrm{tr}\\left(\\mathbf{H}\\Psi\\right)}_{\\mathrm{~\\ensuremath{~w~}~}}}\\\\ &{\\displaystyle\\underbrace{+u_{-1}^{2}\\cdot\\left[\\left(2\\mathrm{tr}\\left(\\mathbf{H}\\Psi\\mathbf{H}\\Psi\\right)+\\frac{M+2}{M}\\mathrm{tr}\\left(\\mathbf{H}\\Psi\\right)^{2}\\right)\\cdot\\mathbf{u}_{12}^{\\top}\\mathbf{H}\\mathbf{u}_{12}+\\left(2+\\frac{4}{M}\\right)\\sigma^{2}\\mathrm{tr}\\left(\\mathbf{H}\\Psi\\right)+\\frac{M+1}{M}\\mathrm{tr}\\left(\\mathbf{H}\\Psi\\right)\\right]}_{\\mathrm{~\\ensuremath{~w~}~}},}\\end{array}$ uLHu12", "page_idx": 17}, {"type": "equation", "text": "$$\n=\\mathrm{I}+\\mathrm{II}+\\mathrm{III}+\\mathrm{IV},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where I, II, III, IV are defined as above. ", "page_idx": 17}, {"type": "text", "text": "Step 5: lower bound the risk function. Let's first define a new matrix, which by definition is invertible: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\Omega:=\\frac{M+1}{M}\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}^{\\frac{1}{2}}+\\frac{\\mathrm{tr}\\left(\\mathbf{H}\\Psi\\right)+\\sigma^{2}}{M}\\mathbf{I}_{d}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "So we can write I as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{I}=b^{\\top}\\mathbf{H}^{\\frac{1}{2}}\\mathbf{H}\\mathbf{H}^{\\frac{1}{2}}b\\cdot\\mathbf{u}_{12}^{\\top}\\mathbf{H}\\mathbf{u}_{12}+u_{-1}^{2}\\mathrm{tr}\\left(A^{\\top}\\mathbf{H}^{\\frac{1}{2}}\\mathbf{H}\\mathbf{H}^{\\frac{1}{2}}A\\mathbf{H}\\right)+2u_{-1}b^{\\top}\\mathbf{H}^{\\frac{1}{2}}\\mathbf{H}\\mathbf{H}^{\\frac{1}{2}}A\\mathbf{H}\\mathbf{u}_{12}}\\\\ &{\\mathrm{~\\-~}2u_{-1}\\mathrm{tr}\\left(\\mathbf{H}A\\mathbf{H}\\Psi\\right)-2\\mathbf{u}_{12}^{\\top}\\mathbf{H}\\Psi\\mathbf{H}b}\\\\ &{\\mathrm{~\\=~tr\\bigg[}\\bigg(\\mathbf{H}^{\\frac{1}{2}}\\mathbf{u}_{12}b^{\\top}\\mathbf{H}^{\\frac{1}{2}}+u_{-1}\\mathbf{H}^{\\frac{1}{2}}A^{\\top}\\mathbf{H}^{\\frac{1}{2}}-\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}^{\\frac{1}{2}}\\Omega^{-1}\\bigg)\\Omega}\\\\ &{\\mathrm{~\\\\\\\\\\\\\\}\\cdot\\bigg(\\mathbf{H}^{\\frac{1}{2}}\\mathbf{u}_{12}b^{\\top}\\mathbf{H}^{\\frac{1}{2}}+u_{-1}\\mathbf{H}^{\\frac{1}{2}}A^{\\top}\\mathbf{H}^{\\frac{1}{2}}-\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}^{\\frac{1}{2}}\\Omega^{-1}\\bigg)^{\\top}\\bigg]-\\mathrm{tr}\\left(\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}^{\\frac{1}{2}}\\Omega^{-1}\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}^{\\frac{1}{2}}\\right)}\\\\ &{\\mathrm{~\\\\\\\\\\\\}\\geq-\\mathrm{tr}\\left(\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}^{\\frac{1}{2}}\\Omega^{-1}\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}^{\\frac{1}{2}}\\right)}\\\\ &{=-\\mathrm{tr}\\left(\\bigg(\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}^{\\frac{1} \n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the last line comes from the fact that $\\pmb{\\Omega}$ and $\\mathbf{H}^{\\frac{1}{2}}\\boldsymbol{\\Psi}\\mathbf{H}^{\\frac{1}{2}}$ commute. Next, we claim $\\Pi\\geq0$ . To see this, it suffices to notice that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{4u_{-1}\\cdot b^{\\top}\\mathbf{H}\\Psi\\mathbf{H}A\\mathbf{H}\\mathbf{u}_{12}\\geq-4\\left\\|b^{\\top}\\mathbf{H}^{\\frac{1}{2}}\\right\\|_{F}\\cdot\\left\\|u_{-1}\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}^{\\frac{1}{2}}\\right\\|_{F}\\cdot\\left\\|\\mathbf{H}^{\\frac{1}{2}}A\\mathbf{H}^{\\frac{1}{2}}\\right\\|_{F}\\cdot\\left\\|\\mathbf{H}^{\\frac{1}{2}}\\mathbf{H}^{\\frac{1}{2}}\\right\\|_{F}\\cdot\\left\\|\\mathbf{H}^{\\frac{1}{2}}\\mathbf{u}_{12}\\right\\|_{F}}\\\\ {\\geq-\\left\\|b^{\\top}\\mathbf{H}^{\\frac{1}{2}}\\right\\|_{F}^{2}\\left\\|\\mathbf{H}^{\\frac{1}{2}}A\\mathbf{H}^{\\frac{1}{2}}\\right\\|_{F}^{2}-4\\left\\|u_{-1}\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}^{\\frac{1}{2}}\\right\\|_{F}^{2}\\left\\|\\mathbf{H}^{\\frac{1}{2}}\\mathbf{u}_{12}\\right\\|_{F}^{2}}\\\\ {\\geq-b^{\\top}\\mathbf{H}b\\cdot\\operatorname{tr}\\left(A^{\\top}\\mathbf{H}A\\mathbf{H}\\right)-4u_{-1}^{2}\\cdot\\operatorname{tr}\\left(\\mathbf{H}\\Psi\\mathbf{H}\\Psi\\right)\\cdot\\mathbf{u}_{12}^{\\top}\\mathbf{H}\\mathbf{u}_{12},\\ (\\mathbf{H}^{\\top})}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the last line comes from the fact that $\\left\\|A\\right\\|_{F}^{2}=\\operatorname{tr}\\left(A A^{\\top}\\right)$ for any matrix $\\pmb{A}$ ", "page_idx": 17}, {"type": "text", "text": "Then, let's consider Il. Notice that actually, III can be viewed as a quadratic function of $A^{\\top}\\mathbf{H}b$ which can be easily minimized. More concretely, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle{\\mathrm{III}}+\\frac{M}{M+1}\\biggl(\\beta^{*}-\\left(\\mathrm{tr}\\left(\\mathbf{H}\\boldsymbol{\\Psi}\\right)+\\sigma^{2}\\right)u_{-1}\\mathbf{u}_{12}\\biggr)^{\\top}\\mathbf{H}\\biggl(\\beta^{*}-\\left(\\mathrm{tr}\\left(\\mathbf{H}\\boldsymbol{\\Psi}\\right)+\\sigma^{2}\\right)u_{-1}\\mathbf{u}_{12}\\biggr)}}\\\\ {{\\displaystyle=\\left[A^{\\top}\\mathbf{H}b-\\frac{M}{M+1}\\biggl(\\beta^{*}-\\left(\\mathrm{tr}\\left(\\mathbf{H}\\boldsymbol{\\Psi}\\right)+\\sigma^{2}\\right)u_{-1}\\mathbf{u}_{12}\\biggr)\\right]^{\\top}\\left(\\frac{M+1}{M}\\mathbf{H}\\right)}}\\\\ {{\\displaystyle\\cdot\\left[A^{\\top}\\mathbf{H}b-\\frac{M}{M+1}\\biggl(\\beta^{*}-\\left(\\mathrm{tr}\\left(\\mathbf{H}\\boldsymbol{\\Psi}\\right)+\\sigma^{2}\\right)u_{-1}\\mathbf{u}_{12}\\biggr)\\right]}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Therefore, one has ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\Pi\\geq-\\frac{M}{M+1}\\Bigg(\\beta^{*}-\\big(\\mathrm{tr}\\left(\\mathbf{H}\\boldsymbol{\\Psi}\\right)+\\sigma^{2}\\big)\\,u_{-1}\\mathbf{u}_{12}\\Bigg)^{\\top}\\mathbf{H}\\bigg(\\beta^{*}-\\big(\\mathrm{tr}\\left(\\mathbf{H}\\boldsymbol{\\Psi}\\right)+\\sigma^{2}\\big)\\,u_{-1}\\mathbf{u}_{12}\\bigg).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Combining the three parts above, one has ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathcal{R}\\left(f\\right)-\\sigma^{2}}\\\\ &{\\geq\\mathbf{IV}-\\operatorname{tr}\\bigg(\\bigg(\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}^{\\frac{1}{2}}\\bigg)^{2}\\Omega^{-1}\\bigg)}\\\\ &{\\qquad\\qquad\\qquad-\\cfrac{M}{M+1}\\bigg(\\beta^{*}-\\left(\\operatorname{tr}\\left(\\mathbf{H}\\Psi\\right)+\\sigma^{2}\\right)u_{-1}\\mathbf{u}_{12}\\bigg)^{\\top}\\mathbf{H}\\bigg(\\beta^{*}-\\left(\\operatorname{tr}\\left(\\mathbf{H}\\Psi\\right)+\\sigma^{2}\\right)u_{-1}\\mathbf{u}_{12}\\bigg)}\\\\ &{=\\underbrace{\\operatorname{tr}\\left(\\mathbf{H}\\Psi\\right)-\\operatorname{tr}\\bigg(\\bigg(\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}^{\\frac{1}{2}}\\bigg)^{2}\\Omega^{-1}\\bigg)}_{\\operatorname*{inf}_{f\\in\\mathcal{F}_{\\lfloor\\mathbf{m}}}\\mathcal{R}(f)-\\sigma^{2}}+\\frac{1}{M+1}\\beta^{*\\top}\\mathbf{H}\\beta^{*}-\\frac{2\\left(\\operatorname{tr}\\left(\\mathbf{H}\\Psi\\right)+\\sigma^{2}\\right)}{M+1}\\beta^{*\\top}\\mathbf{H}\\left(u_{-1}\\mathbf{u}_{12}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n+\\left[2\\mathrm{tr}\\left(\\mathbf{H}\\boldsymbol{\\Psi}\\mathbf{H}\\boldsymbol{\\Psi}\\right)+\\frac{3M+2}{M(M+1)}\\left(\\mathrm{tr}\\left(\\mathbf{H}\\boldsymbol{\\Psi}\\right)+\\sigma^{2}\\right)^{2}\\right]\\left(u_{-1}\\mathbf{u}_{12}\\right)^{\\top}\\mathbf{H}\\left(u_{-1}\\mathbf{u}_{12}\\right).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Therefore, one has ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Xi\\left(f\\right)-\\underset{f\\in\\mathcal{F}_{\\mathrm{trb}}}{\\operatorname*{inf}}\\mathcal{R}(f)\\geq\\cfrac{1}{M+1}\\beta^{*\\top}\\mathbf{H}\\beta^{*}-\\cfrac{2\\left(\\mathrm{tr}\\left(\\mathbf{H}\\Psi\\right)+\\sigma^{2}\\right)}{M+1}\\beta^{*\\top}\\mathbf{H}\\left(u_{-1}\\mathbf{u}_{12}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\left[2\\mathrm{tr}\\left(\\mathbf{H}\\Psi\\mathbf{H}\\Psi\\right)+\\cfrac{3M+2}{M(M+1)}\\left(\\mathrm{tr}\\left(\\mathbf{H}\\Psi\\right)+\\sigma^{2}\\right)^{2}\\right]\\left(u_{-1}\\mathbf{u}_{12}\\right)^{\\top}\\mathbf{H}\\left(u_{-1}\\mathbf{u}_{12}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The right hand side in the above inequality is a quadratic function of $u_{-1}\\mathbf{u}_{12}$ , so we can take its global minimizer: ", "page_idx": 18}, {"type": "equation", "text": "$$\nu_{-1}\\mathbf{u}_{12}=\\frac{\\left(\\mathrm{tr}(\\mathbf{H}\\Psi)+\\sigma^{2}\\right)}{2\\mathrm{tr}\\left(\\mathbf{H}\\Psi\\mathbf{H}\\Psi\\right)+\\frac{3M+2}{M\\left(M+1\\right)}\\left(\\mathrm{tr}\\left(\\mathbf{H}\\Psi\\right)+\\sigma^{2}\\right)^{2}}\\beta^{*}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "to lower bound the risk gap as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{R}\\left(f\\right)-\\operatorname*{inf}_{f\\in\\mathcal{F}_{\\mathrm{tr8}}}\\mathcal{R}(f)\\geq\\left[\\frac{1}{M+1}-\\frac{\\left(\\mathrm{tr}(\\mathbf{H}\\Psi)+\\sigma^{2}\\right)^{2}}{2\\mathrm{tr}\\left(\\mathbf{H}\\Psi\\mathbf{H}\\Psi\\right)+\\frac{3M+2}{M\\left(M+1\\right)}\\left(\\mathrm{tr}\\left(\\mathbf{H}\\Psi\\right)+\\sigma^{2}\\right)^{2}}\\right]\\left\\Vert\\beta^{*}\\right\\Vert_{\\mathbf{H}}^{2}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Finally, to simplify the results, we notice that on the one hand, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{\\left(\\mathrm{tr}\\left(\\mathbf{H}\\Psi\\right)+\\sigma^{2}\\right)^{2}}{2\\mathrm{tr}\\left(\\mathbf{H}\\Psi\\mathbf{H}\\Psi\\right)+\\frac{3M+2}{M\\left(M+1\\right)}\\left(\\mathrm{tr}\\left(\\mathbf{H}\\Psi\\right)+\\sigma^{2}\\right)^{2}}\\le\\frac{\\left(\\mathrm{tr}\\left(\\mathbf{H}\\Psi\\right)+\\sigma^{2}\\right)^{2}}{2\\left(M+1\\right)^{2}\\mathrm{tr}\\left(\\mathbf{H}\\Psi\\mathbf{H}\\Psi\\right)}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "On the other hand, one has ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{\\left(\\mathrm{tr}(\\mathbf{H}\\Psi)+\\sigma^{2}\\right)^{2}}{2\\mathrm{tr}\\left(\\mathbf{H}\\Psi\\mathbf{H}\\Psi\\right)+\\frac{3M+2}{M(M+1)}\\left(\\mathrm{tr}\\left(\\mathbf{H}\\Psi\\right)+\\sigma^{2}\\right)^{2}}\\le\\frac{M}{(M+1)(3M+2)}\\le\\frac{1}{3(M+1)}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Therefore, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{R}\\left(f\\right)-\\operatorname*{inf}_{f\\in\\mathcal{F}_{\\mathrm{LrB}}}\\mathcal{R}(f)\\geq\\operatorname*{max}\\left\\{\\frac{2}{3(M+1)},\\frac{1}{M+1}-\\frac{\\left(\\mathrm{tr}\\left(\\mathbf{H}\\Psi\\right)+\\sigma^{2}\\right)^{2}}{2(M+1)^{2}\\mathrm{tr}\\left(\\mathbf{H}\\Psi\\mathbf{H}\\Psi\\right)}\\right\\}\\cdot\\Vert\\beta^{*}\\Vert_{\\mathbf{H}}^{2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "When M+1 $\\begin{array}{r}{\\frac{1}{M+1}-\\frac{\\left(\\mathrm{tr}(\\mathbf H\\Psi)+\\sigma^{2}\\right)^{2}}{2(M+1)^{2}\\mathrm{tr}(\\mathbf H\\Psi\\mathbf H\\Psi)}\\geq\\frac{2}{3(M+1)}}\\end{array}$ we have $\\begin{array}{r}{\\frac{1}{M+1}\\geq\\frac{3\\left(\\mathrm{tr}(\\mathbf{H}\\Psi)+\\sigma^{2}\\right)^{2}}{2(M+1)^{2}\\mathrm{tr}(\\mathbf{H}\\Psi\\mathbf{H}\\Psi)}}\\end{array}$ whih implies ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathfrak{L}\\left(f\\right)-\\operatorname*{inf}_{f\\in\\mathcal{F}_{\\mathrm{trB}}}\\mathcal{R}(f)\\geq\\left[\\frac{1}{M+1}-\\frac{\\left(\\operatorname{tr}\\left(\\mathbf{H}\\Psi\\right)+\\sigma^{2}\\right)^{2}}{2(M+1)^{2}\\mathrm{tr}\\left(\\mathbf{H}\\Psi\\mathbf{H}\\Psi\\right)}\\right]\\cdot\\left\\Vert\\beta^{*}\\right\\Vert_{\\mathbf{H}}^{2}\\geq\\frac{\\left(\\operatorname{tr}\\left(\\mathbf{H}\\Psi\\right)+\\sigma^{2}\\right)^{2}}{(M+1)^{2}\\mathrm{tr}\\left(\\mathbf{H}\\Psi\\mathbf{H}\\Psi\\right)}\\,.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Therefore, we finally have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{R}\\left(f\\right)-\\operatorname*{inf}_{f\\in\\mathcal{F}_{\\mathrm{UP}}}\\mathcal{R}(f)\\geq\\operatorname*{max}\\left\\{\\frac{2}{3(M+1)},\\frac{\\left(\\mathrm{tr}\\left(\\mathbf{H}\\Psi\\right)+\\sigma^{2}\\right)^{2}}{(M+1)^{2}\\mathrm{tr}\\left(\\mathbf{H}\\Psi\\mathbf{H}\\Psi\\right)}\\right\\}\\cdot\\left\\Vert\\beta^{*}\\right\\Vert_{\\mathbf{H}}^{2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Note that this holds for an arbitrary $f\\in\\mathcal{F}_{\\mathsf{L S A}}$ , so the proof finishes by taking infimum on the left hand side. \u53e3 ", "page_idx": 19}, {"type": "text", "text": "C Proof of Theorem 5.2 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Proof. Recall that from Assumption 3.1, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbf{x}\\sim\\mathcal{N}(0,\\mathbf{H}),\\quad y=\\mathbf{x}^{\\top}\\widetilde{\\beta}+\\varepsilon,\\quad\\mathbf{X}[i]\\sim\\mathcal{N}(0,\\mathbf{H}),\\quad\\mathbf{y}=\\mathbf{X}\\widetilde{\\beta}+\\varepsilon,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\widetilde{\\boldsymbol{\\beta}}\\sim\\mathcal{N}\\left(\\boldsymbol{\\beta}^{*},\\boldsymbol{\\Psi}\\right),\\quad\\boldsymbol{\\varepsilon}\\sim\\mathcal{N}\\left(0,\\sigma^{2}\\right),\\quad\\boldsymbol{\\varepsilon}\\sim\\mathcal{N}\\left(\\mathbf{0},\\sigma^{2}\\cdot\\mathbf{I}_{M}\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Step 1: compute the risk function. From the independence of $\\varepsilon,\\varepsilon$ with other random variables, wehave ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\mathscr R}_{M}\\left(\\beta,{\\bf F}\\right)={\\mathbb E}\\left({\\bf x}^{\\top}\\left(\\widetilde{\\beta}-\\beta\\right)+\\varepsilon+{\\bf x}^{\\top}{\\bf F}\\cdot\\frac{{\\bf X}^{\\top}{\\bf X}}{M}\\left(\\beta-\\widetilde{\\beta}\\right)-{\\bf x}^{\\top}{\\bf F}\\cdot\\frac{{\\bf X}^{\\top}\\varepsilon}{M}\\right)^{2}}\\\\ &{\\qquad\\qquad={\\mathbb E}\\left({\\bf x}^{\\top}\\left({\\bf I}_{d}-{\\bf F}\\cdot\\frac{{\\bf X}^{\\top}{\\bf X}}{M}\\right)\\cdot\\left(\\widetilde{\\beta}-\\beta\\right)\\right)^{2}+\\frac{1}{M^{2}}{\\mathbb E}\\left({\\bf x}^{\\top}{\\bf F}{\\bf X}^{\\top}\\varepsilon\\right)^{2}+\\sigma^{2}}\\\\ &{\\qquad=\\left\\langle{\\bf H},{\\mathbb E}\\left({\\bf I}_{d}-{\\bf F}\\cdot\\frac{{\\bf X}^{\\top}{\\bf X}}{M}\\right)^{\\otimes2}\\circ{\\mathbb E}\\left(\\widetilde{\\beta}-\\beta\\right)^{\\otimes2}\\right\\rangle+\\frac{1}{M^{2}}{\\mathbb E}\\left({\\bf x}^{\\top}{\\bf F}{\\bf X}^{\\top}\\varepsilon\\right)^{2}+\\sigma^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the last line comes from the independence between $\\widetilde{\\beta}$ and $\\mathbf{X},\\mathbf{x}$ , as well as the fact that $\\mathbf{x}\\sim\\mathcal{N}\\left(\\mathbf{0},\\mathbf{H}\\right)$ and the property of tensor product o. Note that, $\\beta$ ad $\\mathbf{T}$ are learnable parameters here, and we should set them apart from the task vector $\\widetilde{\\beta}$ or the prior mean vector $\\widetilde{\\boldsymbol{\\beta}}^{*}$ . Recall that $\\widetilde{\\beta}\\sim\\mathcal{N}\\left(\\beta^{*},\\Psi\\right)$ , we have $\\mathbb{E}\\left(\\bar{\\tilde{\\beta}}-\\beta\\right)^{\\otimes2}=(\\beta-\\beta^{*})^{\\otimes2}+\\Psi$ Moreover, using the following ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbf{x}\\sim\\mathcal{N}\\left(\\mathbf{0},\\mathbf{H}\\right),\\quad\\mathbf{X}[i]\\sim\\mathcal{N}\\left(\\mathbf{0},\\mathbf{H}\\right),\\quad\\varepsilon\\sim\\mathcal{N}\\left(\\mathbf{0},\\sigma^{2}\\cdot\\mathbf{I}_{M}\\right),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "we have that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left(\\mathbf{x}^{\\top}\\mathbf{T}\\mathbf{X}^{\\top}\\boldsymbol{\\varepsilon}\\right)^{2}=\\sigma^{2}\\mathbb{E}\\mathrm{tr}\\left(\\mathbf{X}\\mathbf{I}^{\\top}\\mathbf{x}\\mathbf{x}^{\\top}\\mathbf{I}\\mathbf{X}^{\\top}\\right)=M\\sigma^{2}\\cdot\\left\\langle\\mathbf{H}\\mathbf{I}\\mathbf{H},\\mathbf{I}^{\\top}\\right\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Bridging the two terms above into (C.1), we get ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\mathscr R}_{M}\\left(\\beta,\\mathbf{F}\\right)=\\left\\langle\\mathbf{H},\\mathbb{E}\\left(\\mathbf{I}_{d}-\\mathbf{F}\\cdot\\frac{\\mathbf{X}^{\\top}\\mathbf{X}}{M}\\right)^{\\otimes2}\\circ\\left(\\beta-\\beta^{*}\\right)^{\\otimes2}\\right\\rangle}\\\\ &{\\qquad\\qquad+\\left\\langle\\mathbf{H},\\mathbb{E}\\left(\\mathbf{I}_{d}-\\mathbf{F}\\cdot\\frac{\\mathbf{X}^{\\top}\\mathbf{X}}{M}\\right)^{\\otimes2}\\circ\\Psi+\\frac{\\sigma^{2}}{M}\\mathbf{F}\\mathbf{H}\\mathbf{F}^{\\top}\\right\\rangle+\\sigma^{2}}\\\\ &{\\qquad\\qquad=\\underbrace{\\left\\langle\\mathbb{E}\\left(\\mathbf{I}_{d}-\\mathbf{F}\\cdot\\frac{\\mathbf{X}^{\\top}\\mathbf{X}}{M}\\right)^{\\otimes2}\\circ\\mathbf{H},\\left(\\beta-\\beta^{*}\\right)^{\\otimes2}\\right\\rangle}_{V_{1}}}\\\\ &{\\qquad\\qquad+\\underbrace{\\left\\langle\\mathbf{H},\\mathbb{E}\\left(\\left(\\mathbf{I}_{d}-\\mathbf{F}\\cdot\\frac{\\mathbf{X}^{\\top}\\mathbf{X}}{M}\\right)^{\\top}\\right)^{\\otimes2}\\circ\\Psi+\\frac{\\sigma^{2}}{M}\\mathbf{F}\\mathbf{H}\\mathbf{F}^{\\top}\\right\\rangle}_{V_{2}}+\\sigma^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "First, let's compute $V_{1}$ . From the definition above, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\nV_{1}=\\left(\\beta-\\beta^{*}\\right)^{\\top}{\\bf H}_{\\Gamma}\\left(\\beta-\\beta^{*}\\right),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbf{H}_{\\mathbf{T}}:=\\mathbb{E}\\left(\\left(\\mathbf{I}_{d}-\\mathbf{I}\\cdot{\\frac{\\mathbf{X}^{\\top}\\mathbf{X}}{M}}\\right)^{\\top}\\right)^{\\otimes2}\\circ\\mathbf{H}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle=\\mathbb{E}\\left(\\mathbf{I}_{d}-\\mathbf{I}\\cdot\\frac{\\mathbf{X}^{\\top}\\mathbf{X}}{M}\\right)^{\\top}\\mathbf{H}\\left(\\mathbf{I}_{d}-\\mathbf{I}\\cdot\\frac{\\mathbf{X}^{\\top}\\mathbf{X}}{M}\\right)\\qquad\\mathrm{(the~definition~of~the~tensor~product)}}\\\\ &{\\displaystyle=\\mathbf{H}-\\mathbf{H}\\left(\\mathbf{I}+\\mathbf{I}^{\\top}\\right)\\mathbf{H}+\\frac{\\mathrm{tr}\\left(\\mathbf{H}\\mathbf{I}^{\\top}\\mathbf{H}\\mathbf{I}\\right)}{M}\\mathbf{H}+\\frac{M+1}{M\\mathbf{I}\\mathbf{I}\\sim\\mathbf{N}(\\mathbf{I},\\mathbf{H}\\mathbf{I}^{\\top}\\mathbf{H}\\mathbf{I}}\\\\ &{\\displaystyle\\qquad\\qquad=\\left(\\mathbf{I}_{d}-\\mathbf{I}\\mathbf{H}\\right)^{\\top}\\mathbf{H}\\left(\\mathbf{I}_{d}-\\mathbf{I}\\mathbf{H}\\right)+\\frac{\\mathrm{tr}\\left(\\mathbf{H}\\mathbf{I}^{\\top}\\mathbf{H}\\mathbf{I}\\right)}{M}\\mathbf{H}+\\frac{1}{M}\\mathbf{H}\\mathbf{I}^{\\top}\\mathbf{H}\\mathbf{I}\\mathbf{H}\\succeq0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then, let's compute $V_{2}$ . We have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\boldsymbol{\\uptau}\\left({\\bf I}_{d}-{\\bf F}\\cdot\\frac{{\\bf X}^{\\top}{\\bf X}}{M}\\right)^{\\leftmoon_{2}}\\circ\\boldsymbol{\\Psi}+\\frac{\\sigma^{2}}{M}{\\bf I}{\\bf H}{\\bf T}^{\\top}=\\mathbb{E}\\left({\\bf I}_{d}-{\\bf F}\\cdot\\frac{{\\bf X}^{\\top}{\\bf X}}{M}\\right)\\boldsymbol{\\Psi}\\left({\\bf I}_{d}-{\\bf F}\\cdot\\frac{{\\bf X}^{\\top}{\\bf X}}{M}\\right)^{\\top}+\\frac{\\sigma^{2}}{M}{\\bf I}{\\bf H}{\\bf I}}\\ ~}}\\\\ {{\\displaystyle=\\boldsymbol{\\Psi}-{\\bf I}{\\bf H}\\boldsymbol{\\Psi}-\\boldsymbol{\\Psi}{\\bf H}{\\bf T}^{\\top}+{\\bf r}\\left(\\frac{{\\bf t r}\\left({\\bf H}{\\boldsymbol{\\Psi}}\\right)+\\sigma^{2}}{M}\\cdot{\\bf H}+\\frac{M+1}{M}{\\bf H}\\boldsymbol{\\Psi}{\\bf H}\\right){\\bf T}^{\\top}}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "From the definition of $\\pmb{\\Omega}$ in (A.3), we know that it is invertible. Moreover, it holds that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left(\\mathbf{I}_{d}-\\mathbf{I}\\cdot\\frac{\\mathbf{X}^{\\top}\\mathbf{X}}{M}\\right)^{\\otimes2}\\circ\\Psi+\\frac{\\sigma^{2}}{M}\\mathbf{I}\\mathbf{H}\\mathbf{I}^{\\top}=\\Psi-\\mathbf{I}\\mathbf{H}\\Psi-\\Psi\\mathbf{H}\\mathbf{I}^{\\top}+\\mathbf{I}\\mathbf{H}^{\\frac{1}{2}}\\boldsymbol{\\Omega}\\mathbf{H}^{\\frac{1}{2}}\\mathbf{I}^{\\top}}\\\\ &{\\quad=\\bigg(\\mathbf{I}\\mathbf{H}^{\\frac{1}{2}}-\\Psi\\mathbf{H}^{\\frac{1}{2}}\\boldsymbol{\\Omega}^{-1}\\bigg)\\Omega\\bigg(\\mathbf{I}\\mathbf{H}^{\\frac{1}{2}}-\\Psi\\mathbf{H}^{\\frac{1}{2}}\\boldsymbol{\\Omega}^{-1}\\bigg)^{\\top}+\\Psi-\\Psi\\mathbf{H}^{\\frac{1}{2}}\\boldsymbol{\\Omega}^{-1}\\mathbf{H}^{\\frac{1}{2}}\\Psi.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Therefore, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{2}=\\mathrm{tr}\\left[\\left(\\mathbf{H}^{\\frac{1}{2}}\\Gamma\\mathbf{H}^{\\frac{1}{2}}-\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}^{\\frac{1}{2}}\\Omega^{-1}\\right)\\Omega\\left(\\mathbf{H}^{\\frac{1}{2}}\\Gamma\\mathbf{H}^{\\frac{1}{2}}-\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}^{\\frac{1}{2}}\\Omega^{-1}\\right)^{\\top}\\right]}\\\\ &{\\quad+\\left.\\mathrm{tr}\\left(\\mathbf{H}\\Psi\\right)-\\mathrm{tr}\\left(\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}^{\\frac{1}{2}}\\Omega^{-1}\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}^{\\frac{1}{2}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\mathcal{R}\\left(\\beta,\\mathbf{F}\\right)-\\sigma^{2}}\\\\ &{=\\left(\\beta-\\beta^{*}\\right)^{\\top}\\left[\\left(\\mathbf{I}_{d}-\\mathbf{F}\\mathbf{H}\\right)^{\\top}\\mathbf{H}\\left(\\mathbf{I}_{d}-\\mathbf{F}\\mathbf{H}\\right)+\\frac{\\mathrm{tr}\\left(\\mathbf{H}^{\\top}\\mathbf{H}\\mathbf{F}\\right)}{M}\\mathbf{H}+\\frac{1}{M}\\mathbf{H}^{\\top}\\mathbf{H}\\mathbf{F}\\mathbf{H}\\right]\\left(\\beta-\\beta^{*}\\right)}\\\\ &{+\\operatorname{tr}\\left[\\left(\\mathbf{H}^{\\bot}\\mathbf{F}\\mathbf{H}^{\\bot}-\\mathbf{H}^{\\bot}\\boldsymbol{\\Psi}\\mathbf{H}^{\\bot}\\boldsymbol{\\Omega}^{-1}\\right)\\boldsymbol{\\Omega}\\left(\\mathbf{H}^{\\bot}\\mathbf{F}\\mathbf{H}^{\\bot}-\\mathbf{H}^{\\bot}\\boldsymbol{\\Psi}\\mathbf{H}^{\\bot}\\boldsymbol{\\Omega}^{-1}\\right)^{\\top}\\right]}\\\\ &{+\\operatorname{tr}\\left(\\mathbf{H}\\boldsymbol{\\Psi}\\right)-\\operatorname{tr}\\left(\\mathbf{H}^{\\bot}\\boldsymbol{\\Psi}\\mathbf{H}^{\\bot}\\boldsymbol{\\Omega}^{-1}\\mathbf{H}^{\\bot}\\boldsymbol{\\Psi}\\mathbf{H}^{\\bot}\\right)}\\\\ &{\\geq\\operatorname{tr}\\left(\\mathbf{H}\\boldsymbol{\\Psi}\\right)-\\operatorname{tr}\\left(\\mathbf{H}^{\\bot}\\boldsymbol{\\Psi}\\mathbf{H}^{\\bot}\\boldsymbol{\\Omega}^{-1}\\mathbf{H}^{\\bot}\\boldsymbol{\\Psi}\\mathbf{H}^{\\bot}\\right)}\\\\ &{=\\operatorname{tr}\\left(\\mathbf{H}\\boldsymbol{\\Psi}\\right)-\\operatorname{tr}\\left(\\left(\\mathbf{H}\\boldsymbol{\\Psi}\\right)^{\\top}\\mathbf{H}\\boldsymbol{\\Psi}^{\\bot}\\boldsymbol{\\Omega}^{-1}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the last line comes from the fact that $\\Omega$ and $\\mathbf{H}^{\\frac{1}{2}}\\boldsymbol{\\Psi}\\mathbf{H}^{\\frac{1}{2}}$ commute. Taking infimum over all $\\rho,\\Gamma$ in the left hand side, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{f\\in\\mathcal{F}_{\\mathtt{G O};\\beta}}\\mathcal{R}(f)-\\sigma^{2}=\\operatorname*{inf}_{\\beta,\\Gamma}\\mathcal{R}\\left(\\beta,\\Gamma\\right)-\\sigma^{2}\\ge\\mathrm{tr}\\left(\\mathbf{H}\\boldsymbol{\\Psi}\\right)-\\mathrm{tr}\\left(\\left(\\mathbf{H}^{\\frac{1}{2}}\\boldsymbol{\\Psi}\\mathbf{H}^{\\frac{1}{2}}\\right)^{2}\\boldsymbol{\\Omega}^{-1}\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "On the other hand, we take ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\beta=\\beta^{*},\\quad\\Gamma=\\Gamma^{*}:=\\Psi\\mathbf{H}^{\\frac{1}{2}}\\Omega^{-1}\\mathbf{H}^{-\\frac{1}{2}},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\mathbf{H}^{\\frac{1}{2}}$ is the principle square root of $\\mathbf{H}$ and ${\\mathbf H}^{-\\frac{1}{2}}$ is its Moore-Penrose pseudo inverse (see notation part in Section A). Then, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathcal{R}\\left(\\beta^{\\ast},\\mathbf{F}^{\\ast}\\right)-\\sigma^{2}}\\\\ &{=\\mathrm{tr}\\left[\\left(\\mathbf{H}^{\\frac{1}{2}}\\mathbf{T}^{\\ast}\\mathbf{H}^{\\frac{1}{2}}-\\mathbf{H}^{\\frac{1}{2}}\\boldsymbol{\\Psi}\\mathbf{H}^{\\frac{1}{2}}\\boldsymbol{\\Omega}^{-1}\\right)\\Omega\\left(\\mathbf{H}^{\\frac{1}{2}}\\mathbf{T}^{\\ast}\\mathbf{H}^{\\frac{1}{2}}-\\mathbf{H}^{\\frac{1}{2}}\\boldsymbol{\\Psi}\\mathbf{H}^{\\frac{1}{2}}\\boldsymbol{\\Omega}^{-1}\\right)^{\\top}\\right]}\\\\ &{\\,+\\,\\mathrm{tr}\\left(\\mathbf{H}\\boldsymbol{\\Psi}\\right)-\\mathrm{tr}\\left(\\mathbf{H}^{\\frac{1}{2}}\\boldsymbol{\\Psi}\\mathbf{H}^{\\frac{1}{2}}\\boldsymbol{\\Omega}^{-1}\\mathbf{H}^{\\frac{1}{2}}\\boldsymbol{\\Psi}\\mathbf{H}^{\\frac{1}{2}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Since ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf H^{\\frac12}\\mathbf T^{*}\\mathbf H^{\\frac12}-\\mathbf H^{\\frac12}-\\mathbf H^{\\frac12}\\Psi\\mathbf H^{\\frac12}\\boldsymbol\\Omega^{-1}=\\mathbf H^{\\frac12}\\Psi\\mathbf H^{\\frac12}\\boldsymbol\\Omega^{-1}\\mathbf H^{-\\frac12}\\mathbf H^{\\frac12}-\\mathbf H^{\\frac12}\\Psi\\mathbf H^{\\frac12}-\\mathbf H^{\\frac12}\\Psi\\mathbf H^{\\frac12}\\boldsymbol\\Omega^{-1}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\Omega^{-1}\\mathbf H^{\\frac12}\\Psi\\mathbf H^{\\frac12}-\\Omega^{-1}\\mathbf H^{\\frac12}\\Psi\\mathbf H^{\\frac12}\\mathbf H^{-\\frac12}\\mathbf H^{\\frac12}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad(\\Omega\\mathrm{~and~}\\mathbf H^{\\frac12}\\Psi\\mathbf H^{\\frac12}\\mathrm\\ c o m m)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\Omega^{-1}\\mathbf H^{\\frac12}\\Psi\\mathbf H^{\\frac12}-\\Omega^{-1}\\mathbf H^{\\frac12}\\Psi\\mathbf H^{\\frac12}=\\mathbf0_{d\\times d\\cdot\\cdot}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad(\\mathbf H^{\\frac12}\\mathbf H^{-\\frac12}\\mathbf H^{\\frac12}=\\mathbf1}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Therefore, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathfrak{L}\\left(\\beta^{\\ast},\\Gamma^{\\ast}\\right)-\\sigma^{2}=\\operatorname{tr}\\left(\\mathbf{H}\\Psi\\right)-\\operatorname{tr}\\left(\\left(\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}^{\\frac{1}{2}}\\right)^{2}\\Omega^{-1}\\right)\\geq\\operatorname*{inf}_{f\\in\\mathcal{F}_{\\cos{\\beta}}}\\mathcal{R}(f)-\\sigma^{2}=\\operatorname*{inf}_{\\beta,\\Gamma}\\mathcal{R}\\left(\\beta,\\mathbf{F}\\right)-\\sigma^{2}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Combining both directions, we conclude that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{f\\in\\mathcal{F}_{\\in\\mathrm{GO};\\beta}}\\mathcal{R}(f)-\\sigma^{2}=\\operatorname*{inf}_{\\beta,\\Gamma}\\mathcal{R}\\left(\\beta,\\Gamma\\right)-\\sigma^{2}=\\operatorname{tr}\\left(\\mathbf{H}\\Psi\\right)-\\operatorname{tr}\\left(\\left(\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}^{\\frac{1}{2}}\\right)^{2}\\Omega^{-1}\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Step 3: identify all global minimizers  Above we show that $\\mathcal{R}\\left(\\beta^{*},\\mathbf{T}^{*}\\right)$ achieves the global minimum of the ICL risk over GD- $_{\\cdot\\beta}$ class. Now we will figure out the sufficient and necessary condition to achieve the global minimal risk. From the proof above, we know that for any $\\beta\\in$ $\\mathbb{R}^{d},\\mathbf{T}\\in\\mathbb{R}^{d\\times d}$ , it holds that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathfrak{L}\\left(\\beta,\\mathbf{F}\\right)=\\operatorname*{inf}_{f\\in\\mathcal{F}_{\\mathrm{GO},\\beta}}\\mathcal{R}\\left(f\\right)\\Longleftrightarrow\\left\\{\\begin{array}{l l}{V_{1}=\\left(\\beta-\\beta^{*}\\right)^{\\top}\\mathbf{H}_{\\Gamma}\\left(\\beta-\\beta^{*}\\right)=0}\\\\ {\\mathrm{tr}\\left[\\bigg(\\mathbf{H}^{\\frac{1}{2}}\\Gamma\\mathbf{H}^{\\frac{1}{2}}-\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}^{\\frac{1}{2}}\\Omega^{-1}\\bigg)\\Omega\\bigg(\\mathbf{H}^{\\frac{1}{2}}\\Gamma\\mathbf{H}^{\\frac{1}{2}}-\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}^{\\frac{1}{2}}\\Omega^{-1}\\bigg)^{\\frac{1}{2}}\\right].}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Since $\\pmb{\\Omega}$ is invertible, the second equation is equivalent to ", "page_idx": 22}, {"type": "equation", "text": "$$\n{\\bf H}^{\\frac{1}{2}}{\\bf I H}^{\\frac{1}{2}}-{\\bf H}^{\\frac{1}{2}}\\Psi{\\bf H}^{\\frac{1}{2}}\\Omega^{-1}={\\bf0}_{d\\times d},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which is a linear system of $\\mathbf{T}$ .Since $\\mathbf{T}^{*}$ is proved to be one solution, all solutions of this equation is ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Gamma=\\Gamma^{*}+\\left\\{Z\\in\\mathbb{R}^{d\\times d}:{\\bf H}^{\\frac{1}{2}}Z{\\bf H}^{\\frac{1}{2}}=\\mathbf{0}_{d\\times d}\\right\\}=\\Gamma^{*}+\\mathrm{Im}\\left({\\bf H}^{\\otimes2}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the last equation comes from Lemma H.6. Here, $^+$ denotes Minkowski sum. This is defined as $A+B=\\{a+{\\bar{b}},a\\in A,b\\in B\\}$ for two sets $A,B$ and $a+B=\\{a\\}+B$ for an entry $a$ and a set $B$ . Under this condition, we know that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbf{H}_{\\mathbf{T}}=\\mathbf{H}-\\mathbf{H}\\left(\\mathbf{\\mathbf{T}}^{*}+\\mathbf{\\mathbf{T}}_{M}^{*\\top}\\right)\\mathbf{H}+{\\frac{\\mathrm{tr}\\left(\\mathbf{H}\\mathbf{I}_{M}^{*\\top}\\mathbf{H}\\mathbf{I}^{*}\\right)}{M}}\\mathbf{H}+{\\frac{M+1}{M}}\\mathbf{H}\\mathbf{I}_{M}^{*\\top}\\mathbf{H}\\mathbf{I}^{*}\\mathbf{H}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Consider the following inequality: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbf{I}_{d}-\\mathbf{H}^{\\frac{1}{2}}\\left(\\mathbf{I}^{*}+\\mathbf{I}^{*\\top}\\right)\\mathbf{H}^{\\frac{1}{2}}+\\frac{\\operatorname{tr}\\left(\\mathbf{H}\\mathbf{I}_{M}^{*\\top}\\mathbf{H}\\mathbf{I}^{*}\\right)}{M}\\mathbf{I}_{d}+\\frac{M+1}{M}\\mathbf{H}^{\\frac{1}{2}}\\mathbf{I}_{M}^{*\\top}\\mathbf{H}\\mathbf{I}^{*}\\mathbf{H}^{\\frac{1}{2}}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\succeq}{\\bf{I}}_{d}-{\\bf{H}}^{\\frac{1}{2}}\\left({\\bf{T}}^{*}+{\\bf{I}}^{*\\top}\\right){\\bf{H}}^{\\frac{1}{2}}+\\frac{M+1}{M}{\\bf{H}}^{\\frac{1}{2}}{\\bf{P}}_{M}^{*\\top}{\\bf{H}}{\\bf{I}}^{*}{\\bf{H}}^{\\frac{1}{2}}}\\ ~}}\\\\ {{\\displaystyle=\\left(\\sqrt{\\frac{M}{M+1}}{\\bf{I}}_{d}-\\sqrt{\\frac{M+1}{M}}{\\bf{H}}^{\\frac{1}{2}}{\\bf{P}}^{*}{\\bf{H}}^{\\frac{1}{2}}\\right)\\cdot\\left(\\sqrt{\\frac{M}{M+1}}{\\bf{I}}_{d}-\\sqrt{\\frac{M+1}{M}}{\\bf{H}}^{\\frac{1}{2}}{\\bf{Gamma}}^{*}{\\bf{H}}^{\\frac{1}{2}}\\right)^{\\top}+\\frac{M}{M+1}{\\bf{I}}_{d}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "> Odxd. ", "page_idx": 23}, {"type": "text", "text": "Therefore, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\quad(\\beta-\\beta^{*})^{\\top}\\,\\mathbf H_{\\Gamma}\\left(\\beta-\\beta^{*}\\right)=0}\\\\ &{\\iff\\left[(\\beta-\\beta^{*})^{\\top}\\,\\mathbf H^{\\frac12}\\right]\\left(\\mathbf I_{d}-\\mathbf H^{\\frac12}\\left(\\mathbf F^{*}+\\mathbf T^{*^{\\top}}\\right)\\mathbf H^{\\frac12}+\\frac{\\mathrm{tr}\\,\\left(\\mathbf H\\Gamma_{M}^{*^{\\top}}\\mathbf H\\mathbf T^{*}\\right)}M_{d}\\right.}\\\\ &{\\qquad\\left.+\\frac{M+1}M\\mathbf H^{\\frac12}\\mathbf H^{*^{\\top}}\\mathbf H\\mathbf T^{*}\\mathbf H^{\\frac12}\\right)\\left[\\mathbf H^{\\frac12}\\left(\\beta-\\beta^{*}\\right)\\right]=0}\\\\ &{\\iff\\mathbf H^{\\frac12}\\left(\\beta-\\beta^{*}\\right)=\\mathbf0_{d}\\Longleftrightarrow\\ \\beta=\\beta^{*}+\\mathfrak{n u l l}\\left(\\mathbf H^{\\frac12}\\right)\\Longleftrightarrow\\ \\beta=\\beta^{*}+\\mathfrak{n u l l}\\left(\\mathbf H\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where null $(\\cdot)$ denotes the null space of a matrix and $^+$ denotes the Minkowski sum. The last equivalence comes from Lemma H.6. Therefore, we conclude that the $f_{\\beta,\\Gamma}$ achieves the minimal ICL risk in GD- $_{\\cdot\\beta}$ class if and only if ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\beta=\\beta^{*}+\\mathsf{n u l l}\\left(\\mathbf{H}\\right),\\quad\\Gamma=\\Gamma^{*}+\\mathsf{I m}\\left(\\mathbf{H}^{\\otimes2}\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Specially, if $\\mathbf{H}$ is positive definite, there is unique global minimizer in $\\mathsf{G D-}\\beta$ class with parameters ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\beta=\\beta^{*},\\quad\\mathbf{T}=\\mathbf{T}^{*}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Step 4: equivalence in the hypothesis class  Finally, we show when $\\mathbf{H}$ is rank-deficient, any global minimizer actually corresponds to one single function in $\\mathcal{F}_{\\sf G D-\\beta}$ almost surely. For an arbitrary global minimizer, we assume $\\beta=\\beta^{*}+\\mathbf{h},\\mathbf{T}=\\mathbf{T}^{*}+Z$ where $\\mathbf{h}\\in\\mathsf{n u l l}\\left(\\mathbf{H}\\right),Z\\in\\mathsf{I m}\\left(\\mathbf{H}^{\\otimes2}\\right)$ . Suppose we have a prompt $\\mathbf{X},\\mathbf{y},\\mathbf{x},y$ which follows the Assumption 3.1 and the token matrix is formed by (3.1), we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{\\bar{\\Psi}_{\\beta,{\\bf T}}({\\bf E})}=\\left\\langle\\beta-\\frac{\\Gamma}{M}{\\bf X}^{\\top}\\left({\\bf X}\\beta-{\\bf y}\\right),{\\bf x}\\right\\rangle}\\\\ &{\\qquad\\quad=\\left\\langle\\beta^{*}-\\frac{\\Gamma^{*}}{M}{\\bf X}^{\\top}\\left({\\bf X}\\beta^{*}-{\\bf y}\\right),{\\bf x}\\right\\rangle}\\\\ &{\\qquad\\qquad\\qquad+\\left.{\\bf h}^{\\top}{\\bf x}-\\frac{1}{M}{\\bf x}^{\\top}Z{\\bf X}^{\\top}{\\bf X}\\beta^{*}-\\frac{1}{M}{\\bf x}^{\\top}Z{\\bf X}^{\\top}{\\bf X}{\\bf h}-\\frac{1}{M}{\\bf x}^{\\top}{\\bf T}^{*}{\\bf X}^{\\top}{\\bf X}{\\bf h}+{\\bf x}^{\\top}Z{\\bf X}^{\\top}{\\bf y}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Notice that $\\mathbf{X}[i],\\mathbf{x}\\overset{\\mathrm{i.i.d.}}{\\sim}\\mathcal{N}\\left(\\mathbf{0}_{d},\\mathbf{H}\\right)$ we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left(\\mathbf{h}^{\\top}\\mathbf{x}\\right)^{2}=\\mathbf{h}^{\\top}\\mathbf{H}\\mathbf{h}=0\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "since $\\mathbf{h}\\in\\mathsf{n u l l}\\left(\\mathbf{H}\\right)$ . Therefore, we know $\\mathbf{h}^{\\top}\\mathbf{x}=0$ almost surely, and similarly, $\\mathbf{X}\\mathbf{h}=\\mathbf{0}_{d}$ almost surely. Finally, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left(\\frac{1}{M}\\mathbf{x}^{\\top}Z\\mathbf{X}^{\\top}\\right)\\cdot\\left(\\frac{1}{M}\\mathbf{x}^{\\top}Z\\mathbf{X}^{\\top}\\right)^{\\top}\\right]=\\frac{1}{M}\\mathbb{E}\\mathrm{tr}\\left[\\mathbf{H}Z\\mathbf{H}\\right]=0,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "which imples $\\mathbf{x}^{\\top}Z\\mathbf{X}^{\\top}=\\mathbf{0}_{d}$ almost surely. Therefore, we conclude ", "page_idx": 23}, {"type": "equation", "text": "$$\nf_{\\beta,\\Gamma}\\left(\\mathbf{E}\\right)=f_{\\beta^{*},\\Gamma^{*}}\\left(\\mathbf{E}\\right)\\quad{\\mathrm{almost~surely}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "D Proof of Theorem 5.3 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Proof. Recall the definition of Linear Transformer Block class: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f_{\\mathbf{LTB}}:\\mathbb{R}^{(d+1)\\times(M+1)}\\rightarrow\\mathbb{R}}\\\\ &{\\qquad\\quad\\mathbf{E}\\mapsto\\left[\\mathbf{W}_{2}^{\\top}\\mathbf{W}_{1}\\bigg(\\mathbf{E}+\\mathbf{W}_{P}^{\\top}\\mathbf{W}_{V}\\mathbf{EM}\\frac{\\mathbf{E}^{\\top}\\mathbf{W}_{K}^{\\top}\\mathbf{W}_{Q}\\mathbf{E}}{M}\\bigg)\\right]_{-1,-1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\mathbf{E}$ is the token matrix defined by (3.1). Let's take an arbitrary function $f$ in the LTB class with trainable matrices ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbf{W}_{K},\\,\\mathbf{W}_{Q}\\in\\mathbb{R}^{d_{k}\\times(d+1)},\\quad\\mathbf{W}_{P},\\,\\mathbf{W}_{V}\\in\\mathbb{R}^{d_{v}\\times(d+1)},\\quad\\mathbf{W}_{1},\\mathbf{W}_{2}\\in\\mathbb{R}^{d_{f}\\times(d+1)}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Similar to the proof of the Theorem 4.1, we know that only the last row of $\\mathbf{W}_{2}^{\\top}\\mathbf{W}_{1}$ and the first $d$ -columnsof $\\mathbf{W}_{K}^{\\top}\\mathbf{W}_{Q}$ attend the prediction. Therefore, we denote ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbf{W}_{2}^{\\top}\\mathbf{W}_{1}=\\left(\\!\\!\\begin{array}{l l}{*}&{*}\\\\ {\\gamma^{\\top}}&{*}\\end{array}\\!\\!\\right),\\quad\\mathbf{W}_{2}^{\\top}\\mathbf{W}_{1}\\mathbf{W}_{P}^{\\top}\\mathbf{W}_{V}=\\left(\\!\\!\\begin{array}{l l}{*}&{*}\\\\ {\\mathbf{v}_{21}^{\\top}}&{v_{-1}}\\end{array}\\!\\!\\right),\\quad\\mathbf{W}_{K}^{\\top}\\mathbf{W}_{Q}=\\left(\\!\\!\\begin{array}{l l}{\\mathbf{V}_{11}}&{*}\\\\ {\\mathbf{v}_{12}^{\\top}}&{*}\\end{array}\\!\\!\\right),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\gamma,\\mathbf{v}_{12},\\mathbf{v}_{21},\\in\\mathbb{R}^{d},v_{-1}\\in\\mathbb{R},\\mathbf{V}_{11}\\in\\mathbb{R}^{d\\times d}$ and $^*$ denotes entries that do not enter the prediction. Then, the prediction of LTB function can be written as ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{}^{\\mathrm{\\Delta}}({\\bf E})={\\boldsymbol\\gamma}^{\\top}{\\bf x}+\\left({\\bf v}_{21}^{\\top}{\\bf\\Sigma}_{0-1}\\right)\\cdot\\frac{{\\bf E}{\\bf M}_{M}{\\bf E}^{\\top}}{M}\\cdot\\left({\\bf v}_{11}^{\\top}\\right)\\cdot{\\bf x}}\\ ~}\\\\ {{\\displaystyle~~~~=\\left[{\\boldsymbol\\gamma}^{\\top}+{\\bf v}_{21}^{\\top}\\cdot\\frac{1}{M}{\\bf X}^{\\top}{\\bf X}\\cdot{\\bf V}_{11}+{\\bf v}_{21}^{\\top}\\cdot\\frac{1}{M}{\\bf X}^{\\top}{\\bf y}\\cdot{\\bf v}_{12}^{\\top}+v_{-1}\\cdot\\frac{1}{M}{\\bf y}^{\\top}{\\bf X}\\cdot{\\bf V}_{11}+v_{-1}\\cdot\\frac{1}{M}{\\bf y}^{\\top}{\\bf y}\\cdot{\\bf H}\\right]^{\\top}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Step 1: simplify the risk function. We use $\\widetilde{\\beta}$ to denote the task parameter. From the Assumption 3.1 and Definition A.1, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbf{y}=\\mathbf{X}{\\tilde{\\beta}}+\\boldsymbol{\\varepsilon},\\quad y=\\left\\langle{\\widetilde{\\beta}},\\mathbf{x}\\right\\rangle+\\boldsymbol{\\varepsilon},\\quad{\\widetilde{\\beta}}\\sim\\mathcal{N}\\left(\\beta^{*},\\Psi\\right),\\quad{\\widetilde{\\beta}}=\\beta^{*}+\\Psi^{\\frac{1}{2}}{\\widetilde{\\theta}};\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and ", "page_idx": 24}, {"type": "equation", "text": "$$\n{\\bf X}[i],{\\bf x}\\stackrel{\\mathrm{i.i.d.}}{\\sim}\\mathcal{N}\\left({\\bf0},{\\bf H}\\right),\\quad\\varepsilon[i],\\varepsilon\\stackrel{\\mathrm{i.i.d.}}{\\sim}\\mathcal{N}\\left(0,\\sigma^{2}\\right),\\quad\\widetilde{\\pmb{\\theta}}\\sim\\mathcal{N}\\left({\\bf0},{\\bf I}_{d}\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Then the model output can be written as ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\bf\\Pi}^{\\mathrm{\\tiny{r}}}({\\bf E})=\\left[\\gamma^{\\top}+{\\bf v}_{21}^{\\top}\\cdot\\frac{1}{M}{\\bf X}^{\\top}{\\bf X}\\cdot{\\bf V}_{11}+{\\bf v}_{21}^{\\top}\\cdot\\frac{1}{M}{\\bf X}^{\\top}{\\bf y}\\cdot{\\bf v}_{12}^{\\top}+v_{-1}\\cdot\\frac{1}{M}{\\bf y}^{\\top}{\\bf X}\\cdot{\\bf V}_{11}+v_{-1}\\cdot\\frac{1}{M}{\\bf y}^{\\top}{\\bf y}\\cdot{\\bf v}_{12}\\right]\\cdot{\\bf x}}}\\\\ {{\\displaystyle=\\left[\\gamma^{\\top}+\\Big({\\bf v}_{21}+v_{-1}\\Tilde{\\beta}\\Big)^{\\top}\\cdot\\frac{1}{M}{\\bf X}^{\\top}{\\bf X}\\cdot\\Big({\\bf V}_{11}+\\Tilde{\\beta}{\\bf v}_{12}^{\\top}\\Big)\\right]\\cdot{\\bf x}}}\\\\ {{\\displaystyle\\quad+\\left[{\\bf v}_{21}^{\\top}\\cdot\\frac{1}{M}{\\bf X}^{\\top}{\\boldsymbol\\varepsilon}\\cdot{\\bf v}_{12}^{\\top}+v_{-1}\\cdot\\frac{1}{M}{\\boldsymbol\\varepsilon}^{\\top}{\\bf X}\\cdot{\\bf V}_{11}+v_{-1}\\cdot\\frac{2}{M}{\\boldsymbol\\varepsilon}^{\\top}{\\bf X}\\Tilde{\\beta}{\\bf v}_{12}^{\\top}+\\frac{1}{M}{\\boldsymbol\\varepsilon}^{\\top}{\\boldsymbol\\varepsilon}\\cdot v_{-1}{\\bf v}_{12}^{\\top}\\right]\\cdot{\\bf x}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "To simplify the presentation, we denote ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{z_{1}^{\\top}=\\left(\\mathbf{v}_{21}+v_{-1}\\widetilde{\\boldsymbol{\\beta}}\\right)^{\\top}\\cdot\\cfrac{1}{M}\\mathbf{X}^{\\top}\\mathbf{X}\\cdot\\left(\\mathbf{V}_{11}+\\widetilde{\\boldsymbol{\\beta}}\\mathbf{v}_{12}^{\\top}\\right),}\\\\ &{z_{2}^{\\top}=\\mathbf{v}_{21}^{\\top}\\cdot\\cfrac{1}{M}\\mathbf{X}^{\\top}\\varepsilon\\cdot\\mathbf{v}_{12}^{\\top}+v_{-1}\\cdot\\cfrac{1}{M}\\varepsilon^{\\top}\\mathbf{X}\\cdot\\mathbf{V}_{11}+v_{-1}\\cdot\\cfrac{2}{M}\\varepsilon^{\\top}\\mathbf{X}\\widetilde{\\boldsymbol{\\beta}}\\mathbf{v}_{12}^{\\top}}\\\\ &{z_{3}^{\\top}=\\cfrac{1}{M}\\varepsilon^{\\top}\\varepsilon\\cdot v_{-1}\\mathbf{v}_{12}^{\\top}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Since $\\mathbf{x},\\mathbf{X},\\boldsymbol{\\varepsilon},\\widetilde{\\boldsymbol{\\beta}}$ are independent, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathcal{R}\\left(f\\right)=\\mathbb{E}\\left(f(\\mathbf{E})-\\left\\langle\\widetilde{\\beta},\\mathbf{x}\\right\\rangle-\\varepsilon\\right)^{2}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\sigma^{2}+\\mathbb{E}\\left(f(\\mathbf{E})-\\left\\langle\\widetilde{\\beta},\\mathbf{x}\\right\\rangle\\right)^{2}\\quad\\mathrm{~(}\\varepsilon\\mathrm{~is~independent~from~other~variables~}a^{\\prime}}\\\\ &{=\\mathbb{E}\\left[\\left\\langle z_{1}+z_{2}+z_{3}+\\gamma-\\widetilde{\\beta},\\mathbf{x}\\right\\rangle^{2}\\right]+\\sigma^{2}}\\\\ &{=\\left\\langle\\mathbf{H},\\mathbb{E}\\left(z_{1}+z_{2}+z_{3}+\\gamma-\\widetilde{\\beta}\\right)\\left(z_{1}+z_{2}+z_{3}+\\gamma-\\widetilde{\\beta}\\right)^{\\top}\\right\\rangle+\\sigma^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Note that $z_{1}$ does not contain $\\varepsilon,\\,z_{2}$ is a linear form of $\\varepsilon$ , and $z_{3}$ is a quadratic form of $\\varepsilon$ . Using $\\varepsilon\\sim\\mathcal{N}(\\mathbf{0},\\sigma^{2}\\mathbf{I}_{d})$ , we have $\\mathbb{E}[(z_{1}+\\gamma-\\widetilde{\\beta})\\cdot z_{2}^{\\top}]=\\mathbf{0}$ and $\\mathbb{E}[z_{2}z_{3}^{\\top}]=\\mathbf{0}$ . Therefore, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\mathscr R}\\left(f\\right)-\\sigma^{2}=\\underbrace{\\left\\langle{\\bf H},{\\mathbb E}\\left(z_{1}+\\gamma-\\widetilde{\\beta}\\right)\\left(z_{1}+\\gamma-\\widetilde{\\beta}\\right)^{\\top}\\right\\rangle}_{S_{1}^{\\prime}}+\\underbrace{\\left\\langle{\\bf H},{\\mathbb E}z_{2}z_{2}^{\\top}\\right\\rangle}_{S_{2}^{\\prime}}+\\underbrace{\\left\\langle{\\bf H},{\\mathbb E}z_{3}z_{3}^{\\top}\\right\\rangle}_{S_{3}^{\\prime}}}\\\\ &{\\qquad\\qquad+\\underbrace{2\\left\\langle{\\bf H},{\\mathbb E}\\left(z_{1}+\\gamma-\\widetilde{\\beta}\\right)z_{3}^{\\top}\\right\\rangle}_{S_{4}^{\\prime}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Step 2: compute the risk function.  Let's compute the risk function. Note that, the risk function is in the same form as the risk function of LSA layer, which is in the proof of Theorem 4.1 (see Section $\\mathbf{B}$ and equation (B.2). There are two differences: one is we replace $\\mathbf{U}_{11},\\mathbf{u}_{12},\\mathbf{u}_{21},u_{-1}$ here with $\\mathbf{V}_{11},\\mathbf{v}_{12},\\mathbf{v}_{21},v_{-1}.$ The second difference is that we replace $\\widetilde{\\beta}$ in (B.2) with $\\widetilde{\\beta}-\\gamma$ . This is equivalent to replacing $\\beta^{*}$ with $\\beta^{*}-\\gamma$ , since $\\widetilde{\\beta}-\\gamma\\sim\\mathcal{N}\\left(\\beta^{*}-\\gamma,\\Psi\\right)$ . Therefore, similar to (B.15), the risk function in (D.2) can be written as ", "page_idx": 25}, {"type": "text", "text": "$\\begin{array}{r l}&{\\quad\\mathcal{R}\\left(f\\right)-\\sigma^{2}}\\\\ &{=b^{\\top}\\left(\\frac{M+1}{M}\\mathbf{R}\\mathbf{R}\\mathbf{H}+\\frac{1}{M}\\mathbf{tr}(\\mathbf{H}\\bar{\\Psi})\\mathbf{H}\\right)b\\cdot\\mathbf{v}_{12}^{\\top}\\mathbf{H}\\mathbf{v}_{12}+\\frac{\\sigma^{2}}{M}\\cdot b^{\\top}\\mathbf{H}b\\cdot\\mathbf{v}_{12}^{\\top}\\mathbf{H}\\mathbf{v}_{12}}\\\\ &{\\quad+\\left.v_{-1}^{2}\\mathrm{tr}\\left(A^{\\top}\\left(\\frac{M+1}{M}\\mathbf{H}\\mathbf{\\Psi}\\mathbf{H}+\\frac{1}{M}\\mathbf{tr}(\\mathbf{H}\\bar{\\Psi})\\mathbf{H}\\right)A\\mathbf{H}\\right)+\\frac{\\sigma^{2}}{M}\\cdot\\mathrm{tr}\\left(\\mathbf{H}A\\mathbf{H}A^{\\top}\\right)}\\\\ &{\\quad\\underbrace{+2v_{-1}b^{\\top}\\left[\\frac{1}{M}\\mathrm{tr}(\\mathbf{H}\\bar{\\Psi})\\mathbf{H}+\\frac{M+1}{M}\\mathbf{H}\\Psi\\mathbf{H}+\\frac{\\sigma^{2}}{M}\\cdot\\mathbf{H}\\right]A\\mathbf{H}\\mathbf{v}_{12}-2v_{-1}\\mathrm{tr}\\left(\\mathbf{H}A\\mathbf{H}\\Psi\\right)-2\\mathbf{v}_{12}^{\\top}\\mathbf{H}\\Psi\\mathbf{H}}_{\\mathbf{V}}}\\\\ &{\\quad-\\frac{1}{M}\\left[b^{\\top}\\mathbf{H}b\\cdot\\mathrm{tr}\\left(A^{\\top}\\mathbf{H}A\\mathbf{H}\\right)+4v_{-1}\\cdot b^{\\top}\\mathbf{H}\\Psi\\mathbf{H}A\\mathbf{H}\\mathbf{v}_{12}+4v_{-1}^{2}\\cdot\\mathrm{tr}\\left(\\mathbf{H}\\bar{\\Psi}\\mathbf{H}\\Psi\\right)\\cdot\\mathbf{v}_{12}^{\\top}\\mathbf{H}\\mathbf{v}_{12}\\right]}\\end{array}$ b   \n$\\begin{array}{r l}{\\lefteqn{+\\underbrace{\\frac{1}{M}\\Big[b^{\\top}\\mathbf{H}b\\cdot\\mathrm{tr}\\left(A^{\\top}\\mathbf{H}A\\mathbf{H}\\right)+4v_{-1}\\cdot b^{\\top}\\mathbf{H}\\Psi\\mathbf{H}A\\mathbf{H}\\mathbf{v}_{12}+4v_{-1}^{2}\\cdot\\mathrm{tr}\\left(\\mathbf{H}\\Psi\\mathbf{H}\\Psi\\right)\\cdot\\mathbf{v}_{12}^{\\top}\\mathbf{H}\\mathbf{v}_{12}\\Big]}_{\\forall1}}}\\\\ &{+\\underbrace{b^{\\top}\\left(\\frac{M+1}{M}\\mathbf{H}A\\mathbf{H}A^{\\top}\\mathbf{H}\\right)b-2b^{\\top}\\mathbf{H}A\\mathbf{H}\\left(\\beta^{*}-\\gamma\\right)+2v_{-1}\\mathrm{tr}(\\mathbf{H}\\Psi)\\cdot b^{\\top}\\mathbf{H}A\\mathbf{H}\\mathbf{v}_{12}+2\\sigma^{2}b^{\\top}\\mathbb{I}}_{\\forall1}}\\\\ &{+\\left(\\beta^{*}-\\gamma\\right)^{\\top}\\mathbf{H}\\left(\\beta^{*}-\\gamma\\right)-2\\left(\\mathrm{tr}\\left(\\mathbf{H}\\Psi\\right)+\\sigma^{2}\\right)\\cdot\\left(\\beta^{*}-\\gamma\\right)^{\\top}\\mathbf{H}\\left(v_{-1}\\mathbf{v}_{12}\\right)+\\mathrm{tr}\\left(\\mathbf{H}\\Psi\\right)}\\\\ &{\\underbrace{+v_{-1}^{2}\\cdot\\left[\\left(2\\mathrm{tr}\\left(\\mathbf{H}\\Psi\\mathbf{H}\\Psi\\right)+\\frac{M+2}{M}\\mathrm{tr}\\left(\\mathbf{H}\\Psi\\right)^{2}\\right)\\cdot\\mathbf{v}_{12}^{\\top}\\mathbf{H}\\mathbf{v}_{12}+\\left(2+\\frac{4}{M}\\right)\\sigma^{2}\\mathrm{tr}\\left(\\mathbf{H}\\Psi\\right)+\\frac{M+2}{M}\\right]}_{\\forall1}}\\\\ &{=\\mathbf{V}+\\mathbf{V}\\mathbf{I}+\\mathbf{V}\\mathbf{I}+\\mathbf{V}\\mathbf{II},}\\end{array}$ H(v-12) VIHV12 (D.3) ", "page_idx": 25}, {"type": "text", "text": "$\\begin{array}{r}{\\Omega=\\frac{M+1}{M}{\\bf H}^{\\frac{1}{2}}\\Psi{\\bf H}^{\\frac{1}{2}}+\\frac{\\mathrm{tr}({\\bf H}\\Psi)+\\sigma^{2}}{M}\\cdot{\\bf L}_{d}}\\end{array}$ and $\\pmb{b}:=$ $\\mathbf{v}_{21}+v_{-1}\\beta^{*}\\in\\mathbb{R}^{d},\\pmb{A}:=\\mathbf{V}_{11}+\\beta^{*}\\mathbf{v}_{12}^{\\top}\\in\\mathbb{R}^{d\\times d}$ ", "page_idx": 25}, {"type": "text", "text": "Step 3: solve the global minimum of the risk function.  This is very similar to step 5 in Appendix B.The $\\mathrm{v}$ term in (D.3) is actually equal to the I term in (B.15), so we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\slash=\\mathrm{tr}\\Bigg[\\bigg(\\mathbf H^{\\frac12}\\mathbf v_{12}b^{\\top}\\mathbf H^{\\frac12}+v_{-1}\\mathbf H^{\\frac12}A^{\\top}\\mathbf H^{\\frac12}-\\mathbf H^{\\frac12}\\Psi H^{\\frac12}\\Omega^{-1}\\bigg)\\Omega\\bigg(\\mathbf H^{\\frac12}\\mathbf v_{12}b^{\\top}\\mathbf H^{\\frac12}+v_{-1}\\mathbf H^{\\frac12}A^{\\top}\\mathbf H^{\\frac12}-\\mathbf H^{\\frac12}\\mathbf H^{\\frac12}\\mathbf H^{\\frac12}\\bigg)}\\\\ &{\\qquad\\qquad\\qquad-\\mathrm{tr}\\left(\\mathbf H^{\\frac12}\\Psi H^{\\frac12}\\Omega^{-1}\\mathbf H^{\\frac12}\\Psi H^{\\frac12}\\right)}\\\\ &{\\geq-\\mathrm{tr}\\left(\\mathbf H^{\\frac12}\\Psi H^{\\frac12}\\Omega^{-1}\\mathbf H^{\\frac12}\\Psi H^{\\frac12}\\right)}\\\\ &{=-\\mathrm{tr}\\left(\\left(\\mathbf H^{\\frac12}\\Psi H^{\\frac12}\\right)^{2}\\Omega^{-1}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the last line comes from the fact that $\\Omega$ and $\\mathbf{H}^{\\frac{1}{2}}\\boldsymbol{\\Psi}\\mathbf{H}^{\\frac{1}{2}}$ commute. For the same reason, we know that the $V I$ term above is equal to the $\\mathrm{II}$ term in (B.15), which implies $\\mathrm{VI}\\geq0$ , since ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{4v_{-1}\\cdot b^{\\top}\\mathbf{H}\\Psi\\mathbf{H}A\\mathbf{H}\\mathbf{v}_{12}\\geq-4\\left\\|b^{\\top}\\mathbf{H}^{\\frac{1}{2}}\\right\\|_{F}\\cdot\\left\\|v_{-1}\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}^{\\frac{1}{2}}\\right\\|_{F}\\cdot\\left\\|\\mathbf{H}^{\\frac{1}{2}}A\\mathbf{H}^{\\frac{1}{2}}\\right\\|_{F}\\cdot\\left\\|\\mathbf{H}^{\\frac{1}{2}}\\mathbf{V}_{12}\\right\\|_{F}}\\\\ &{\\qquad\\qquad\\qquad\\geq-\\left\\|b^{\\top}\\mathbf{H}^{\\frac{1}{2}}\\right\\|_{F}^{2}\\left\\|\\mathbf{H}^{\\frac{1}{2}}A\\mathbf{H}^{\\frac{1}{2}}\\right\\|_{F}^{2}-4\\left\\|v_{-1}\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}^{\\frac{1}{2}}\\right\\|_{F}^{2}\\left\\|\\mathbf{H}^{\\frac{1}{2}}\\mathbf{v}_{12}\\right\\|_{F}^{2}}\\\\ &{\\qquad\\qquad\\qquad=-b^{\\top}\\mathbf{H}b\\cdot\\mathrm{tr}\\left(A^{\\top}\\mathbf{H}A\\mathbf{H}\\right)-4v_{-1}^{2}\\cdot\\mathrm{tr}\\left(\\mathbf{H}\\Psi\\mathbf{H}\\Psi\\right)\\cdot\\mathbf{v}_{12}^{\\top}\\mathbf{H}\\mathbf{v}_{12},\\quad(12)}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the last line comes from the fact that $\\left\\|A\\right\\|_{F}^{2}=\\operatorname{tr}\\left(A A^{\\top}\\right)$ for any matrix $\\pmb{A}$ . The term VII above is equal to II term in (B.15), except that we replace $\\beta^{*}$ With $\\beta^{*}-\\gamma$ . Therefore, we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{\\boldmath~\\vII}+\\frac{M}{M+1}\\bigg(\\beta^{*}-\\gamma-\\left(\\mathrm{tr}\\left(\\mathbf{H}\\boldsymbol{\\Psi}\\right)+\\sigma^{2}\\right)v_{-1}\\mathbf{v}_{12}\\bigg)^{\\top}\\mathbf{H}\\bigg(\\beta^{*}-\\gamma-\\left(\\mathrm{tr}\\left(\\mathbf{H}\\boldsymbol{\\Psi}\\right)+\\sigma^{2}\\right)v_{-1}\\mathbf{v}_{12}\\bigg)}\\\\ &{=\\Bigg[A^{\\top}\\mathbf{H}b-\\frac{M}{M+1}\\bigg(\\beta^{*}-\\gamma-\\left(\\mathrm{tr}\\left(\\mathbf{H}\\boldsymbol{\\Psi}\\right)+\\sigma^{2}\\right)v_{-1}\\mathbf{v}_{12}\\bigg)\\Bigg]^{\\top}\\left(\\frac{M+1}{M}\\mathbf{H}\\right)}\\\\ &{\\qquad\\qquad\\quad\\cdot\\left[A^{\\top}\\mathbf{H}b-\\frac{M}{M+1}\\bigg(\\beta^{*}-\\gamma-\\left(\\mathrm{tr}\\left(\\mathbf{H}\\boldsymbol{\\Psi}\\right)+\\sigma^{2}\\right)v_{-1}\\mathbf{v}_{12}\\bigg)\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "$\\ge0$ ", "page_idx": 26}, {"type": "text", "text": "which implies ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathrm{VII}\\geq-\\frac{M}{M+1}\\Bigg(\\beta^{*}-\\gamma-\\left(\\mathrm{tr}\\left(\\mathbf{H}\\Psi\\right)+\\sigma^{2}\\right)v_{-1}\\mathbf{v}_{12}\\Bigg)^{\\top}\\mathbf{H}\\bigg(\\beta^{*}-\\gamma-\\left(\\mathrm{tr}\\left(\\mathbf{H}\\Psi\\right)+\\sigma^{2}\\right)v_{-1}\\mathbf{v}_{12}\\bigg)\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Combining the three parts above, one has ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathcal{R}\\left(f\\right)-\\sigma^{2}}\\\\ &{\\geq\\nabla\\Pi-\\operatorname{tr}\\bigg(\\Big(\\mathbf{H}^{\\mathrm{H}}\\boldsymbol{\\Psi}\\mathbf{H}\\Big\\vert^{2}\\Big)^{2}\\mathbf{n}^{-1}\\bigg)}\\\\ &{-\\frac{M}{M+1}\\bigg(\\beta^{*}-\\gamma-\\left(\\operatorname{tr}(\\mathbf{H}\\boldsymbol{\\Psi}\\right)+\\sigma^{2}\\right)v_{-1}\\mathbf{v}_{12}\\bigg)^{\\top}\\mathbf{H}\\bigg(\\beta^{*}-\\gamma-\\left(\\operatorname{tr}(\\mathbf{H}\\boldsymbol{\\Psi})+\\sigma^{2}\\right)v_{-1}\\mathbf{v}_{12}\\bigg)}\\\\ &{=\\underbrace{\\operatorname{tr}(\\mathbf{H}\\boldsymbol{\\Psi})-\\operatorname{tr}\\bigg(\\Big(\\mathbf{H}^{\\mathrm{H}}\\boldsymbol{\\Psi}\\mathbf{H}\\Big\\vert^{2}\\Big)^{2}\\mathbf{n}^{-1}\\bigg)}_{\\mathrm{i}\\mathrm{f}\\;f\\in\\mathcal{C}_{\\mathrm{cos}}\\beta\\in\\mathcal{V}(f)-\\sigma^{2}}+\\frac{1}{M+1}\\left(\\beta^{*}-\\gamma\\right)^{\\top}\\mathbf{H}\\left(\\beta^{*}-\\gamma\\right)}\\\\ &{\\qquad\\qquad\\qquad-\\frac{2\\left(\\operatorname{tr}(\\mathbf{H}\\boldsymbol{\\Psi})+\\sigma^{2}\\right)}{M+1}\\left(\\beta^{*}-\\gamma\\right)^{\\top}\\mathbf{H}\\left(v_{-1}\\mathbf{v}_{12}\\right)}\\\\ &{\\qquad\\qquad\\qquad+\\left[2\\operatorname{tr}(\\mathbf{H}\\boldsymbol{\\Psi}\\mathbf{H}\\boldsymbol{\\Psi})+\\frac{3M+2}{M(M+1)}\\left(\\operatorname{tr}(\\mathbf{H}\\boldsymbol{\\Psi})+\\sigma^{2}\\right)^{2}\\right]\\left(v_{-1}\\mathbf{v}_{12}\\right)^{\\top}\\mathbf{H}\\left(v_{-1}\\mathbf{v}_{12}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Here, the global minimum of GD- $_{\\cdot\\beta}$ class is taken from Theorem 5.2, whose proof does not reply on the proof here. Therefore, one has ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbf{\\Pi}_{f}\\mathrm{inf}\\quad\\mathbf{\\Pi}_{f\\in\\mathcal{F}_{\\mathbb{G};0},\\mathcal{R}}^{\\mathsf{f}}(f)\\geq\\frac{1}{M+1}\\left(\\beta^{*}-\\gamma\\right)^{\\top}\\mathbf{H}\\left(\\beta^{*}-\\gamma\\right)-\\frac{2\\left(\\mathbf{tr}\\left(\\mathbf{H}\\Psi\\right)+\\sigma^{2}\\right)}{M+1}\\left(\\beta^{*}-\\gamma\\right)^{\\top}\\mathbf{H}\\left(v_{-1}\\mathbf{v}_{12}\\right)}}\\\\ &{}&{+\\left[\\mathrm{2tr}\\left(\\mathbf{H}\\Psi\\mathbf{H}\\Psi\\right)+\\frac{3M+2}{M\\left(M+1\\right)}\\left(\\mathrm{tr}\\left(\\mathbf{H}\\Psi\\right)+\\sigma^{2}\\right)^{2}\\right]\\left(v_{-1}\\mathbf{v}_{12}\\right)^{\\top}\\mathbf{H}\\left(v_{-1}\\mathbf{v}_{12}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The right hand side in the above inequality is a quadratic function of $v_{-1}\\mathbf{v}_{12}$ , so we can take its global minimizer: ", "page_idx": 27}, {"type": "equation", "text": "$$\nv_{-1}\\mathbf{v}_{12}={\\frac{\\left(\\operatorname{tr}(\\mathbf{H}\\Psi)+{\\boldsymbol{\\sigma}}^{2}\\right)}{2\\mathrm{tr}\\left(\\mathbf{H}\\Psi\\mathbf{H}\\Psi\\right)+{\\frac{3M+2}{M\\left(M+1\\right)}}\\left(\\operatorname{tr}\\left(\\mathbf{H}\\Psi\\right)+{\\boldsymbol{\\sigma}}^{2}\\right)^{2}}}\\left(\\beta^{*}-\\gamma\\right)\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "to lower bound the risk gap as ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\Im\\left(f\\right)-\\operatorname*{inf}_{f\\in\\mathcal{F}_{\\mathrm{GO},\\beta}}\\mathcal{R}(f)\\ge\\left[\\frac{1}{M+1}-\\frac{\\frac{\\left(\\mathrm{tr}\\left(\\mathbf{H}\\Psi\\right)+\\sigma^{2}\\right)^{2}}{\\left(M+1\\right)^{2}}}{2\\mathrm{tr}\\left(\\mathbf{H}\\Psi\\mathbf{H}\\Psi\\right)+\\frac{3M+2}{M\\left(M+1\\right)}\\left(\\mathrm{tr}\\left(\\mathbf{H}\\Psi\\right)+\\sigma^{2}\\right)^{2}}\\right]\\left\\Vert\\beta^{*}-\\gamma\\right\\Vert_{\\mathbf{H}}^{2}\\ge0\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Note that, taking infimum on the left hand side, we get ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{f\\in\\mathcal{F}_{\\mathsf{L T B}}}\\mathcal{R}\\left(f\\right)-\\operatorname*{inf}_{f\\in\\mathcal{F}_{\\mathsf{G D-\\beta}}}\\mathcal{R}(f)\\geq0.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "On the other hand, in the main text we have showed that $\\mathcal{F}_{\\mathsf{G D}-\\beta}\\subset\\mathcal{F}_{\\mathsf{L T B}}$ , which implies ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{f\\in\\mathcal{F}_{\\mathsf{L T B}}}\\mathcal{R}\\left(f\\right)-\\operatorname*{inf}_{f\\in\\mathcal{F}_{\\mathsf{G D-\\beta}}}\\mathcal{R}(f)\\leq0.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Therefore, we conclude ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{f\\in\\mathcal{F}_{\\mathrm{LTB}}}\\mathcal{R}\\left(f\\right)=\\operatorname*{inf}_{f\\in\\mathcal{F}_{\\mathrm{GD-}\\beta}}\\mathcal{R}(f)=\\mathrm{tr}\\left(\\mathbf{H}\\Psi\\right)-\\mathrm{tr}\\left(\\left(\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}^{\\frac{1}{2}}\\right)^{2}\\Omega^{-1}\\right),\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where the last equation is from Theorem 5.2. ", "page_idx": 27}, {"type": "text", "text": "Step 4: sufficient and necessary conditions for global minimizers.  Let's now verify the conditions for the global minimizers. For a function in LTB class, the suffcient and necessary condition for it to be a global minimizer is that inequalities in the above step all hold, which are ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{H}^{\\frac{1}{2}}\\mathbf{v}_{12}\\mathbf{b}^{\\top}\\mathbf{H}^{\\frac{1}{2}}+v_{-1}\\mathbf{H}^{\\frac{1}{2}}A^{\\top}\\mathbf{H}^{\\frac{1}{2}}=\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}^{\\frac{1}{2}}\\mathbf{J}^{-1},}\\\\ &{\\left\\|{\\mathbf{b}^{\\top}}\\mathbf{H}^{\\frac{1}{2}}\\right\\|_{F}^{2}\\left\\|\\mathbf{H}^{\\frac{1}{2}}A\\mathbf{H}^{\\frac{1}{2}}\\right\\|_{F}^{2}=4\\left\\|v_{-1}\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}^{\\frac{1}{2}}\\right\\|_{F}^{2}\\left\\|\\mathbf{H}^{\\frac{1}{2}}\\mathbf{v}_{12}\\right\\|_{F}^{2},}\\\\ &{v_{-1}\\cdot{\\mathbf{\\Phi}}^{\\mathrm{\\tiny{T}}}\\mathbf{H}\\Psi\\mathbf{H}A\\mathbf{H}\\mathbf{v}_{12}=\\left\\|{b^{\\top}}\\mathbf{H}^{\\frac{1}{2}}\\right\\|_{F}\\cdot\\left\\|v_{-1}\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}^{\\frac{1}{2}}\\right\\|_{F}\\cdot\\left\\|\\mathbf{H}^{\\frac{1}{2}}A\\mathbf{H}^{\\frac{1}{2}}\\right\\|_{F}\\cdot\\left\\|\\mathbf{H}^{\\frac{1}{2}}\\mathbf{v}_{12}\\right\\|_{F},}\\\\ &{\\mathbf{H}^{\\frac{1}{2}}\\left[A^{\\top}\\mathbf{H}b-\\frac{M}{M+1}\\!\\left({\\beta^{*}}-\\gamma-\\left(\\mathbf{t}\\left(\\mathbf{H}\\Psi\\right)+\\sigma^{2}\\right)v_{-1}\\mathbf{v}_{12}\\right)\\right]=\\mathbf{0}_{d\\times d},}\\\\ &{v_{-1}\\mathbf{v}_{12}=\\frac{\\left(\\frac{\\left(\\mathbf{t}\\left(\\mathbf{H}\\Psi\\right)+\\sigma^{2}\\right)}{M+1}\\right)^{2}}{2\\mathrm{tr}\\left(\\mathbf{H}\\Psi\\mathbf{H}\\Psi\\right)+\\frac{3M+2}{M(M+1)}\\left(\\operatorname{tr}\\left(\\mathbf{H}\\Psi\\right)+\\sigma^{2}\\right)^{2}}\\left(\\beta^{*}-\\gamma\\right)}\\\\ &{\\|\\beta^\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Let's first verify the necessary conditions of the system defined above. From (D.11) and Lemma H.5,weknow $\\begin{array}{r}{\\gamma\\in\\beta^{*}+\\mathsf{n u l l}\\left(\\mathbf{H}^{\\frac{1}{2}}\\right)=\\beta^{*}+\\mathsf{n u l l}\\left(\\mathbf{H}\\right).}\\end{array}$ Then,wehave $v_{-1}\\mathbf{v}_{12}\\in\\left.\\mathsf{n u l l}\\left(\\mathbf{H}\\right)\\right..$ Then, in (D.7),weknow ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\left|b^{\\top}\\mathbf{H}^{\\frac{1}{2}}\\right|\\left\\|_{F}^{2}\\left\\|\\mathbf{H}^{\\frac{1}{2}}A\\mathbf{H}^{\\frac{1}{2}}\\right\\|_{F}^{2}=4\\left\\|v_{-1}\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}^{\\frac{1}{2}}\\right\\|_{F}^{2}\\left\\|\\mathbf{H}^{\\frac{1}{2}}\\mathbf{v}_{12}\\right\\|_{F}^{2}=4\\left\\|\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}^{\\frac{1}{2}}\\right\\|_{F}^{2}\\left\\|v_{-1}\\mathbf{H}^{\\frac{1}{2}}\\mathbf{v}_{12}\\right\\|_{F}^{2}=0.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "which implies either $\\mathbf{H}^{\\frac{1}{2}}\\pmb{b}=\\mathbf{0}_{d}$ or ${\\bf H}^{\\frac{1}{2}}A{\\bf H}^{\\frac{1}{2}}={\\bf0}_{d\\times d}.$ If we assume $\\mathbf{H}^{\\frac{1}{2}}A\\mathbf{H}^{\\frac{1}{2}}=\\mathbf{0}_{d\\times d}$ then (D.6) implies $\\mathbf{H}^{\\frac{\\cdot}{2}}\\mathbf{v}_{12}b^{\\top}\\mathbf{H}^{\\frac{1}{2}}=\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}^{\\frac{1}{2}}\\Omega^{-1}$ From our assumption, we know r $\\mathsf{a n k}(\\mathbf{H}^{\\frac{1}{2}}\\Psi^{\\frac{1}{2}})\\geq2$ which implies rank $\\mathbf{\\langle(H^{\\frac{1}{2}}\\Psi H^{\\frac{1}{2}})\\rangle\\geq2}$ , since for any matrix $_{z}$ , it holds that $\\mathsf{r a n k}(Z)=\\mathsf{r a n k}(Z Z^{\\top})$ Since multiplication by an invertible matrix does not change the rank, we know rank $\\left(\\mathbf{H}^{\\frac{1}{2}}\\boldsymbol{\\Psi}\\mathbf{H}^{\\frac{1}{2}}\\boldsymbol{\\Omega}^{-1}\\right)\\geq2$ while $\\mathbf{H}^{\\frac{1}{2}}\\mathbf{v}_{12}\\boldsymbol{b}^{\\top}\\mathbf{H}^{\\frac{1}{2}}$ is a matrix of rank at most one, which contradicts with (D.6). Therefore, we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\mathbf{H}^{\\frac{1}{2}}b=\\mathbf{0},}}\\\\ {{v_{-1}\\mathbf{H}^{\\frac{1}{2}}A^{\\top}\\mathbf{H}^{\\frac{1}{2}}=\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}^{\\frac{1}{2}}\\Omega^{-1}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Recall in Theorem 5.2, we have defined ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\Gamma^{*}:=\\Psi\\mathbf{H}^{\\frac{1}{2}}\\Omega^{-1}\\mathbf{H}^{-\\frac{1}{2}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Simple calculation shows ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{1}^{\\frac{1}{2}}\\mathbf{\\Gamma}^{*}\\mathbf{H}^{\\frac{1}{2}}=\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}^{\\frac{1}{2}}\\boldsymbol{\\Omega}^{-1}\\mathbf{H}^{-\\frac{1}{2}}\\mathbf{H}^{\\frac{1}{2}}=\\boldsymbol{\\Omega}^{-1}\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}^{\\frac{1}{2}}\\mathbf{H}^{-\\frac{1}{2}}\\mathbf{H}^{\\frac{1}{2}}=\\boldsymbol{\\Omega}^{-1}\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}^{\\frac{1}{2}}=\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}^{\\frac{1}{2}}\\boldsymbol{\\Omega}^{-1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where the second and the last equalities come from the fact that $\\Omega$ and $\\mathbf{H}^{\\frac{1}{2}}\\boldsymbol{\\Psi}\\mathbf{H}^{\\frac{1}{2}}$ commute, and the third equality comes from the property of Moor Penrose pseudo-inverse. Therefore, one solution of (D.13) is $\\boldsymbol{v}_{-1}\\dot{\\boldsymbol{A}}=\\mathbf{I}^{*\\top}$ . Since (D.13) is a linear equation to $v_{-1}A$ , we know its full solution is ", "page_idx": 28}, {"type": "equation", "text": "$$\nv_{-1}A\\in\\Gamma^{{*}^{\\top}}+\\left\\{Z:\\mathbf{H}^{\\frac{1}{2}}Z\\mathbf{H}^{\\frac{1}{2}}=\\mathbf{0}_{d\\times d}\\right\\}=\\Gamma^{*^{\\top}}+\\mathsf{I m}\\left(\\mathbf{H}^{\\otimes2}\\right).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "The solution to (D.12) is ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbf{v}_{21}=-v_{-1}\\beta^{*}+\\mathsf{n u l l}\\left(\\mathbf{H}^{\\frac{1}{2}}\\right)=-v_{-1}\\beta^{*}+\\mathsf{n u l l}\\left(\\mathbf{H}\\right).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Therefore, we know the necessary conditions for (D.6) to (D.11) are ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\qquad v_{-1}\\neq0,}\\\\ {v_{-1}\\mathbf{v}_{12}\\in\\mathsf{n u l l}\\left(\\mathbf{H}\\right),}\\\\ {\\qquad\\mathbf{v}_{21}=-v_{-1}\\beta^{*}+\\mathsf{n u l l}\\left(\\mathbf{H}\\right),}\\\\ {v_{-1}\\mathbf{V}_{11}\\in\\mathbf{{T^{*}}}^{\\top}-v_{-1}\\beta^{*}\\mathbf{v}_{12}^{\\top}+\\mathsf{I m}\\left(\\mathbf{H}^{\\otimes2}\\right),}\\\\ {\\qquad\\gamma=\\beta^{*}+\\mathsf{n u l l}\\left(\\mathbf{H}\\right)}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "It is easy to verify these equations above are also sufficient conditions of (D.6) to (D.11) by directly replacing each variable with its value and validating equaions from (D.6) to (D.11). ", "page_idx": 28}, {"type": "text", "text": "Specially, if $\\mathbf{H}$ is positive definite and hence, invertible, the global minimizer is unique up to a scaling to $v_{-1}$ ", "page_idx": 28}, {"type": "equation", "text": "$$\nv_{-1}\\neq0,\\quad\\mathbf{v}_{12}=\\mathbf{0}_{d},\\quad\\mathbf{v}_{21}=-v_{-1}\\beta^{*},\\quad\\mathbf{V}_{11}={\\frac{1}{v_{-1}}}\\cdot\\mathbf{I}^{*^{\\top}},\\quad\\gamma=\\beta^{*}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Step 5: equivalence in the hypothesis class. Finally, we will prove that any global minimizer of $\\mathcal{F}_{\\sf L T B}$ is actually equivalent to one single function in $\\mathcal{F}_{\\sf L T B}$ almost surely. More concretely, let's take a function $f\\in\\mathcal{F}_{\\mathsf{L T B}}$ with parameters $\\mathbf{W}_{K}$ \uff0c $\\mathbf{W}_{Q}$ \uff0c $\\mathbf{W}_{P}$ \uff0c ${\\bf W}_{V},{\\bf W}_{1}$ \uff0c $\\mathbf{W}_{2}$ and recall the parameter transformation in (D.1). We assume equations in (D.14) and Assumption 3.1 hold. Then, for any vector ${\\textbf{a}}\\in$ null $(\\mathbf{H})$ , one has $\\mathbf{x}^{\\top}\\mathbf{a}=\\mathbf{x}_{i}^{\\top}\\mathbf{a}=0$ since $\\mathbf{x},\\mathbf{x}_{i}\\overset{\\mathrm{i.i.d.}}{\\sim}\\mathcal{N}\\left(\\mathbf{0}_{d},\\mathbf{H}\\right).$ We denote ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{v}_{21}=-v_{-1}\\beta^{*}+\\mathbf{a}_{1},\\quad v_{-1}\\mathbf{V}_{11}=\\mathbf{T^{*}}^{\\top}-v_{-1}\\beta^{*}\\mathbf{v}_{12}^{\\top}+\\mathbf{Z},\\quad\\gamma=\\beta^{*}+\\mathbf{a}_{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where ${\\bf a}_{1},{\\bf a}_{2}\\in{\\bf H},{\\bf H}^{\\frac{1}{2}}Z{\\bf H}^{\\frac{1}{2}}={\\bf0}_{d\\times d}$ . Then, we have ", "page_idx": 28}, {"type": "equation", "text": "$$\nf(\\mathbf{E})=\\left[\\mathbf{W}_{2}^{\\top}\\mathbf{W}_{1}\\bigg(\\mathbf{E}+\\mathbf{W}_{P}^{\\top}\\mathbf{\\Phi}\\mathbf{W}_{V}\\mathbf{E}\\mathbf{M}\\frac{\\mathbf{E}^{\\top}\\mathbf{W}_{K}^{\\top}\\mathbf{\\Phi}\\mathbf{W}_{Q}\\mathbf{E}}{M}\\bigg)\\right]_{-1,-1}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{\\Sigma}=(\\beta^{*}+\\mathbf{a}_{2})^{\\top}\\mathbf{x}+\\left(-v_{-1}\\beta^{*^{\\top}}+\\mathbf{a}_{1}^{\\top}\\mathbf{\\Sigma}\\mathbf{\\Sigma}\\mathbf{\\Sigma}\\mathbf{\\Sigma}-\\mathbf{\\Sigma}\\right)\\cdot\\frac{1}{M}\\left(\\mathbf{X}^{\\top}\\mathbf{X}\\mathbf{\\Sigma}\\mathbf{\\Sigma}\\mathbf{\\Sigma}\\mathbf{\\Sigma}\\mathbf{X}^{\\top}\\mathbf{y}\\right)\\cdot\\left(\\mathbf{V}_{11}^{\\top}\\right)\\cdot\\mathbf{x}}\\\\ &{\\mathbf{\\Sigma}=\\beta^{*\\top}\\mathbf{x}+\\left(-\\beta^{*^{\\top}}\\mathbf{\\Sigma}\\mathbf{\\Sigma}\\mathbf{\\Sigma}\\mathbf{\\Sigma}\\mathbf{\\Sigma}\\mathbf{\\Sigma}\\mathbf{\\Sigma}\\mathbf{\\Sigma}\\mathbf{\\Sigma}\\mathbf{\\Sigma}\\mathbf{X}^{\\top}\\mathbf{y}\\right)\\cdot\\left(v_{-1}\\mathbf{V}_{11}\\mathbf{x}\\right)\\mathbf{\\Sigma}\\mathbf{\\Sigma}\\mathbf{\\Sigma}\\mathbf{\\Sigma}\\mathbf{\\Sigma}\\mathbf{\\Sigma}}\\\\ &{=\\beta^{*\\top}\\mathbf{x}+\\left(-\\beta^{*^{\\top}}\\mathbf{\\Sigma}\\mathbf{\\Sigma}\\mathbf{\\Sigma}\\mathbf{\\Sigma}\\mathbf{\\Sigma}\\mathbf{\\Sigma}\\mathbf{\\Sigma}\\mathbf{\\Sigma}\\mathbf{\\Sigma}\\mathbf{\\Sigma}\\mathbf{\\Sigma}\\mathbf{X}^{\\top}\\mathbf{y}\\right)\\cdot\\left(\\mathbf{v}_{-1}\\mathbf{V}_{12}^{\\top}\\mathbf{x}\\right)\\mathbf{\\Sigma}\\mathbf{\\Sigma}\\mathbf{\\Sigma}\\left(\\mathbf{X}\\mathbf{a}_{1}=\\mathbf{0},\\mathbf{a}_{2}^{\\top}\\mathbf{x}=0\\right)}\\\\ &{=\\beta^{*\\top}\\mathbf{x}+\\left(-\\beta^{*^{\\top}}\\mathbf{\\Sigma}\\mathbf{\\Sigma}\\mathbf{\\Sigma}\\mathbf{\\Sigma}\\mathbf{\\Sigma}\\mathbf{\\Sigma}\\mathbf{}\\mathbf{\\Sigma}\\mathbf{\\Sigma}\\mathbf{\\Sigma}\\mathbf{}\\mathbf{\\Sigma}\\mathbf{\\Sigma}\\mathbf{X}^{\\top}\\mathbf{y}\\right)\\cdot\\left(\\mathbf{\\Sigma}\\mathbf{\\Sigma}\\mathbf{\\Sigma}\\mathbf{\\Sigma}\\mathbf{\\Sigma}\\mathbf{\\Sigma}\\mathbf{\\Sigma}\\mathbf{\\Sigma}\\mathbf{\\Sigma}\\mathbf{\\Sigma}\\mathbf{\\Sigma}\\mathbf{\\Sigma}\\mathbf{\\Sigma}\\mathbf{\\Sigma}\\mathbf{\\Sigma}\\mathbf{\\Sigma}\\mathbf{\\Sigma}\\mathbf{\\Sigma}\\mathbf{\\Sigma}\\mathbf{\\Sigma}\\mathbf{\\Sigma}\\mathbf{\\Sigma}\\mathbf{\\Sigma}\\mathbf{\\Sigma}\\mathbf{\\Sigma}\\mathbf{\\Sigma}\\mathbf{\\Sigma \n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $f_{\\beta^{*},\\Gamma^{*}}\\left(\\cdot\\right)$ is the GD- $_{\\beta}$ function defined in (5.1). ", "page_idx": 29}, {"type": "text", "text": "E Proof of Corollary 6.2 ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Proof. We denote $\\phi_{1}\\geq\\phi_{2}\\geq...\\geq\\phi_{d}\\geq0$ are ordered eigenvalues of $\\Psi^{\\frac{1}{2}}\\mathbf{H}\\Psi^{\\frac{1}{2}}$ . From Theorem 5.2, we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{f\\in\\mathcal{F}_{\\mathrm{GD};\\beta}}\\mathcal{R}(f)-\\sigma^{2}=\\mathrm{tr}\\left(\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}^{\\frac{1}{2}}\\right)-\\mathrm{tr}\\left(\\left(\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}^{\\frac{1}{2}}\\right)^{2}\\Omega^{-1}\\right),\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\Omega:=\\frac{M+1}{M}\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}^{\\frac{1}{2}}+\\frac{\\mathrm{tr}\\left(\\mathbf{H}\\Psi\\right)+\\sigma^{2}}{M}\\cdot\\mathbf{I}_{d}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Therefore, we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{inf}_{f\\in\\mathcal{F}_{\\mathbb{G};\\beta}}\\mathcal{R}(f)-\\sigma^{2}=\\mathrm{tr}\\left(\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}^{\\frac{1}{2}}\\cdot\\left(\\Omega-\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}^{\\frac{1}{2}}\\right)\\Omega^{-1}\\right)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad=\\displaystyle\\frac{1}{M}\\mathrm{tr}\\left(\\Omega^{-1}\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}^{\\frac{1}{2}}\\cdot\\left(\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}^{\\frac{1}{2}}+\\left(\\mathrm{tr}\\left(\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}^{\\frac{1}{2}}\\right)+\\sigma^{2}\\right)\\cdot\\mathbf{I}_{d}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Since ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\left.\\left(\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}^{\\frac{1}{2}}\\right)+\\sigma^{2}\\right)\\cdot\\mathbf{I}_{d}\\preceq\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}^{\\frac{1}{2}}+\\left(\\operatorname{tr}\\left(\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}^{\\frac{1}{2}}\\right)+\\sigma^{2}\\right)\\cdot\\mathbf{I}_{d}\\preceq2\\left(\\operatorname{tr}\\left(\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}^{\\frac{1}{2}}\\right)+\\sigma^{2}\\right)\\cdot\\mathbf{I}_{d},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\operatorname*{inf}_{f\\in\\mathcal{F}_{\\mathbb{G};\\beta}}\\mathcal{R}(f)-\\sigma^{2}\\simeq\\frac{\\mathrm{tr}\\left({\\bf H}^{\\frac12}\\Psi{\\bf H}^{\\frac12}\\right)+\\sigma^{2}}{M}\\mathrm{tr}\\left(\\Omega^{-1}{\\bf H}^{\\frac12}\\Psi{\\bf H}^{\\frac12}\\right)}}\\\\ {{\\displaystyle=\\bar{\\phi}\\cdot\\sum_{i=1}^{d}\\frac{\\phi_{i}}{\\frac{M+1}{M}\\phi_{i}+\\bar{\\phi}}}}\\\\ {{\\displaystyle\\simeq\\sum_{i=1}^{d}\\operatorname*{min}\\left\\{\\phi_{i},\\bar{\\phi}\\right\\}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "F Proof of Theorem 6.1 ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "F.1  Proof of first equation ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Proof. From the classical bias-variance decomposition, we know ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}\\left(g;\\mathbf{X}\\right):=\\mathbb{E}\\left[\\left(g(\\mathbf{X},\\mathbf{y},\\mathbf{x})-y\\right)^{2}\\mid\\mathbf{X}\\right]}\\\\ &{\\quad\\quad\\quad\\quad=\\mathbb{E}\\left[\\left(g(\\mathbf{X},\\mathbf{y},\\mathbf{x})-\\mathbb{E}\\left[y\\mid\\mathbf{X},\\mathbf{y},\\mathbf{x}\\right]\\right)^{2}\\mid\\mathbf{X}\\right]+\\mathbb{E}\\left[\\left(\\mathbb{E}\\left[y\\mid\\mathbf{X},\\mathbf{y},\\mathbf{x}\\right]-y\\right)^{2}\\mid\\mathbf{X}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "since the cross term vanishes. The second term does not depend on $g$ and hence, the Bayesian optimal estimator is given by the posterior mean, i.e., ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{y}_{\\sf B a y e s}=\\mathbb{E}\\left[y\\mid\\mathbf{X},\\mathbf{y},\\mathbf{x}\\right]=\\left\\langle\\mathbb{E}\\left[\\widetilde{\\beta}\\mid\\mathbf{X},\\mathbf{y},\\mathbf{x}\\right],\\mathbf{x}\\right\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Since $\\tilde{\\beta}\\,\\sim\\,\\mathcal{N}\\left(\\beta^{*},\\Psi\\right),$ we have there exists a random vector $\\widetilde{\\pmb{\\theta}}\\sim\\mathcal{N}\\left(\\mathbf{0}_{d},\\mathbf{I}_{d}\\right)$ such that $\\widetilde{\\beta}\\,=$ $\\beta^{*}+\\Psi^{\\frac{1}{2}}\\widetilde{\\pmb{\\theta}}$ almost surely. Therefore, one has ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\widehat{y}_{\\mathsf{B a y e s}}=\\langle\\beta^{*},\\mathbf{x}\\rangle+\\left\\langle\\mathbb{E}\\left[\\widetilde{\\pmb{\\theta}}\\mid\\mathbf{X},\\mathbf{y},\\mathbf{x}\\right],\\pmb{\\Psi}^{\\frac{1}{2}}\\mathbf{x}\\right\\rangle.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "To compute $\\mathbb{E}\\left[{\\widetilde{\\pmb{\\theta}}}\\mid\\mathbf{X},\\mathbf{y},\\mathbf{x}\\right]$ , it sufices to solve the posterior ditribution of $\\beta$ given $\\mathbf{X},\\mathbf{y}$ From $\\widetilde{\\pmb{\\theta}}\\sim\\mathcal{N}\\left(\\mathbf{0}_{d},\\mathbf{I}_{d}\\right)$ wehave ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathbb{P}\\left(\\widetilde{\\pmb{\\theta}}\\mid\\mathbf{X},\\mathbf{y},\\mathbf{x}\\right)\\propto\\mathbb{P}\\left(\\widetilde{\\pmb{\\theta}}\\right)\\mathbb{P}\\left(\\mathbf{y}\\mid\\mathbf{X},\\widetilde{\\pmb{\\theta}}\\right)\\propto\\exp\\left(-\\frac{\\left\\|\\mathbf{y}-\\mathbf{X}\\left(\\beta^{\\ast}+\\mathbf{\\Phi}\\mathbf{\\Phi}^{\\dagger}\\widetilde{\\pmb{\\theta}}\\right)\\right\\|_{2}^{2}}{2\\sigma^{2}}-\\frac{1}{2}\\widetilde{\\pmb{\\theta}}^{\\top}\\cdot\\widetilde{\\pmb{\\theta}}\\right)}&{}&\\\\ {\\propto\\exp\\left(-\\frac{1}{2\\sigma^{2}}\\left[\\widetilde{\\pmb{\\theta}}^{\\top}\\left(\\Psi^{\\frac{1}{2}}\\mathbf{X}^{\\top}\\mathbf{X}\\Psi^{\\frac{1}{2}}+\\sigma^{2}\\mathbf{I}_{d}\\right)\\widetilde{\\pmb{\\theta}}-2\\left(\\mathbf{y}-\\mathbf{X}\\beta^{\\ast}\\right)^{\\top}\\mathbf{X}\\Psi^{\\frac{1}{2}}\\widetilde{\\pmb{\\theta}}\\right]\\right).}&{}&\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Note that, the function above matches the probability density function of a multivariate Gaussian distrbution. Therefore, the posterior mean is given by ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\theta\\mathbf{\\Sigma}\\right]\\mathbf{X},\\mathbf{y},\\mathbf{x}\\right]=\\left(\\Psi^{\\frac{1}{2}}\\mathbf{X}^{\\top}\\mathbf{X}\\Psi^{\\frac{1}{2}}+\\sigma^{2}\\mathbf{I}_{d}\\right)^{-1}\\Psi^{\\frac{1}{2}}\\mathbf{X}^{\\top}\\left(\\mathbf{y}-\\mathbf{X}\\beta^{*}\\right).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Therefore, we conclude ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{y}_{\\mathrm{Bayes}}={\\mathbf x}^{\\top}\\Psi^{\\frac{1}{2}}\\left(\\Psi^{\\frac{1}{2}}{\\mathbf X}^{\\top}{\\mathbf X}\\Psi^{\\frac{1}{2}}+\\sigma^{2}\\mathbf I_{d}\\right)^{-1}\\Psi^{\\frac{1}{2}}{\\mathbf X}^{\\top}\\left(\\mathbf y-{\\mathbf X}\\boldsymbol{\\beta}^{*}\\right)+{\\mathbf x}^{\\top}\\boldsymbol{\\beta}^{*}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "F.2  Proof of second equation ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Proof. Let's first do a variable transformation. We denote ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\widetilde{{\\bf X}}={\\bf X}\\Psi^{\\frac{1}{2}},\\quad\\widetilde{{\\bf x}}=\\Psi^{\\frac{1}{2}}{\\bf x},\\quad\\widetilde{{\\bf y}}={\\bf y}-{\\bf X}\\beta^{*},\\quad{\\bf A}=\\Psi^{\\frac{1}{2}}{\\bf H}\\Psi^{\\frac{1}{2}}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Then, we know $\\widetilde{\\mathbf{X}}[i],\\widetilde{\\mathbf{x}}\\,\\overset{\\mathrm{i.i.d.}}{\\sim}\\mathcal{N}\\left(\\mathbf{0}_{d},\\mathbf{A}\\right).$ We can write the Bayesian optimal estimator in (F.1) as ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\widehat{y}_{\\sf B a y e s}=\\widetilde{\\bf x}^{\\top}\\left(\\widetilde{\\bf X}^{\\top}\\widetilde{\\bf X}+\\sigma^{2}{\\bf I}_{d}\\right)^{-1}\\widetilde{\\bf X}\\widetilde{\\bf y}+{\\bf x}^{\\top}\\beta^{*}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "The true label is ", "page_idx": 31}, {"type": "equation", "text": "$$\ny=\\mathbf{x}^{\\top}\\widetilde{\\boldsymbol{\\beta}}+\\varepsilon=\\mathbf{x}^{\\top}\\left(\\boldsymbol{\\beta}^{*}+\\Psi^{\\frac{1}{2}}\\widetilde{\\boldsymbol{\\theta}}\\right).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Therefore, we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathcal{L}\\left(\\widehat{y}_{\\mathsf{B a y e s}};\\mathbf{X}\\right)-\\sigma^{2}=\\mathbb{E}\\left(\\widetilde{\\mathbf{x}}^{\\top}\\left(\\widetilde{\\mathbf{X}}^{\\top}\\widetilde{\\mathbf{X}}+\\sigma^{2}\\mathbf{I}_{d}\\right)^{-1}\\widetilde{\\mathbf{X}}\\widetilde{\\mathbf{y}}+\\mathbf{x}^{\\top}\\beta^{*}-\\mathbf{x}^{\\top}\\left(\\beta^{*}+\\Psi^{\\frac{1}{2}}\\widetilde{\\pmb{\\theta}}\\right)\\right)^{2}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "equation", "text": "$$\n=\\mathbb{E}\\left(\\widetilde{\\mathbf{x}}^{\\top}\\left[\\left(\\widetilde{\\mathbf{X}}^{\\top}\\widetilde{\\mathbf{X}}+\\sigma^{2}\\mathbf{I}_{d}\\right)^{-1}\\widetilde{\\mathbf{X}}\\widetilde{\\mathbf{y}}-\\widetilde{\\pmb{\\theta}}\\right]\\right)^{2}=\\mathbb{E}\\left\\|\\widehat{\\pmb{\\theta}}-\\widetilde{\\pmb{\\theta}}\\right\\|_{\\Lambda}^{2},\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\widehat{\\pmb{\\theta}}:=\\left(\\widetilde{\\mathbf{X}}^{\\top}\\widetilde{\\mathbf{X}}+\\sigma^{2}\\mathbf{I}_{d}\\right)^{-1}\\widetilde{\\mathbf{X}}\\widetilde{\\mathbf{y}}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Note that, this is equivalent to the risk of estimating the ground true linear weight $\\widetilde{\\pmb{\\theta}}$ using $\\widehat{\\pmb{\\theta}}$ under the Gaussian prior $\\widetilde{\\pmb{\\theta}}\\sim\\mathcal{N}\\left(\\mathbf{0},\\mathbf{I}_{d}\\right)$ . The estimator 0 is a function of transformed input-output pairs in the context $\\left(\\widetilde{\\mathbf{X}},\\widetilde{\\mathbf{y}}\\right)$ and the transformed query input $\\mathbf{x}$ , and takes the form of standard ridge estimator with regularization coefficient being $\\sigma^{2}$ . We then revoke the standard results for the risk of a ridge estimator. Applying the Theorem 1 and 2 in [33], we know for a fixed weight vector 0, with probability at least $1-\\exp\\left(-\\Omega(M)\\right)$ we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\left\\lVert\\widehat{\\theta}-\\widetilde{\\theta}\\right\\rVert_{\\mathbf{H}}^{2}\\simeq\\left(\\frac{\\sigma^{2}+\\sum_{i>k^{*}}\\phi_{i}}{M}\\right)^{2}\\left\\lVert\\widetilde{\\theta}\\right\\rVert_{\\mathbf{H}_{0;k^{*}}^{-1}}^{2}+\\left\\lVert\\widetilde{\\theta}\\right\\rVert_{\\mathbf{H}_{k^{*};\\infty}^{2}}^{2}+\\sigma^{2}\\left(\\frac{k^{*}}{M}+\\frac{M\\sum_{i>k^{*}}\\phi_{i}^{2}}{\\sigma^{2}+\\sum_{i>k^{*}}\\phi_{i}}\\right),\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $\\phi_{1}\\geq\\phi_{2}\\geq...\\geq\\phi_{d}\\geq0$ are ordered eigenvalues of $\\Lambda$ ", "page_idx": 32}, {"type": "equation", "text": "$$\nk^{\\ast}:=\\operatorname*{min}\\left\\{k:\\phi_{k}\\geq c\\cdot\\frac{\\sigma^{2}+\\sum_{i>k^{\\ast}}\\phi_{i}}{M}\\right\\}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "and $c>1$ is an absolute constant. Here, $\\mathbf{H}_{0:k^{*}}$ is SVD approximation with respect to the largest $k^{*}$ singular values and $\\mathbf{H}_{k^{*};\\infty}$ is the SVD approximation in the remaining_ singular values. Namely, if we have the eigen-decomposition of $\\mathbf{H}\\dot{=}\\mathbf{\\bar{Q}}\\cdot\\mathrm{diag}\\left(\\phi_{1},\\phi_{2},...,\\phi_{d}\\right)\\cdot\\mathbf{Q}^{\\top}$ , where $\\mathbf{Q}$ is an orthogonal matrix, then $\\mathbf{H}_{0:k^{*}}$ and $\\mathbf{H}_{k^{*};\\infty}$ are given by ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbf{H}_{0:k^{*}}=\\mathbf{Q}\\cdot\\mathrm{diag}\\left(\\phi_{1},\\phi_{2},...,\\phi_{k^{*}},0,0,...,0\\right)\\cdot\\mathbf{Q}^{\\top},\\quad\\mathbf{H}_{k^{*}:\\infty}=\\mathbf{H}-\\mathbf{H}_{0:k^{*}}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Taking expectation over $\\widetilde{\\pmb{\\theta}}\\sim\\mathcal{N}\\left(0,\\mathbf{I}_{d}\\right)$ , we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathcal{L}\\left(\\widehat{y}_{\\mathrm{Bayes}};\\mathbf{X}\\right)-\\sigma^{2}=\\mathbb{E}_{\\widetilde{\\theta}\\sim\\mathcal{N}(0,\\mathbf{I}_{d})}\\left\\|\\widehat{\\theta}_{\\mathrm{Bayes}}-\\widetilde{\\theta}\\right\\|_{\\mathbf{H}}^{2}}\\\\ {\\displaystyle=\\left(\\frac{\\sigma^{2}+\\sum_{i>k^{*}}\\phi_{i}}{M}\\right)^{2}\\cdot\\sum_{i\\le k^{*}}\\frac{1}{\\phi_{i}}+\\sum_{i>k^{*}}\\phi_{i}+\\sigma^{2}\\left(\\frac{k^{*}}{M}+\\frac{M\\sum_{i>k^{*}}\\phi_{i}^{2}}{\\sigma^{2}+\\sum_{i>k^{*}}\\phi_{i}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Now we simplify this expression. First, we define ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\bar{\\phi}:=c\\cdot\\frac{\\sigma^{2}+\\sum_{i>k^{*}}\\phi_{i}}{M}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "From the definition, we see $\\begin{array}{r}{\\bar{\\phi}\\;\\ge\\;\\frac{c\\sigma^{2}}{M}}\\end{array}$ . On the other hand, from the assumption that $\\operatorname{tr}\\left(\\mathbf{H}\\right)\\ =$ $\\begin{array}{r}{\\mathrm{tr}\\left(\\Psi^{\\frac{1}{2}}\\mathbf{H}\\Psi^{\\frac{1}{2}}\\right)=\\sum_{i=1}^{d}\\phi_{i}\\lesssim\\sigma^{2}}\\end{array}$ ,we know that $\\begin{array}{r}{\\bar{\\phi}\\lesssim\\frac{c\\sigma^{2}}{M}}\\end{array}$ . Combining two parts, we get ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\bar{\\phi}\\simeq\\frac{\\sigma^{2}}{M}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Therefore, we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}\\left(\\widehat{y}_{\\mathrm{8ayes}};\\mathbf{X}\\right)-\\sigma^{2}\\simeq\\overline{{\\phi}}^{2}\\cdot\\displaystyle\\sum_{i\\leq k^{*}}\\frac{1}{\\phi_{i}}+\\displaystyle\\sum_{i>k^{*}}\\phi_{i}+\\frac{\\sigma^{2}}{M}\\cdot\\left(k^{*}+\\frac{\\sum_{i>k^{*}}\\phi_{i}^{2}}{\\widetilde{\\phi}^{2}}\\right)}\\\\ &{\\qquad\\qquad\\simeq\\displaystyle\\sum_{i}\\operatorname*{min}\\left\\{\\frac{\\overline{{\\phi}}^{2}}{\\phi_{i}},\\phi_{i}\\right\\}+\\bar{\\phi}\\cdot\\displaystyle\\sum_{i}\\operatorname*{min}\\left\\{1,\\frac{\\phi_{i}^{2}}{\\bar{\\phi}^{2}}\\right\\}}\\\\ &{\\qquad\\qquad\\simeq\\displaystyle\\sum_{i}\\left(\\operatorname*{min}\\left\\{\\frac{\\bar{\\phi}^{2}}{\\phi_{i}},\\phi_{i}\\right\\}+\\operatorname*{min}\\left\\{\\bar{\\phi},\\frac{\\phi_{i}^{2}}{\\bar{\\phi}}\\right\\}\\right)}\\\\ &{\\qquad\\qquad\\simeq\\displaystyle\\sum_{i}\\operatorname*{min}\\left\\{\\phi_{i},\\bar{\\phi}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "This finishes the proof. ", "page_idx": 32}, {"type": "text", "text": "G Proof of Theorem 6.3 ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "G.1 Training dynamics ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Now we consider doing gradient flow on the risk function, or equivalently, on the excess risk function which differs by only a constant: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\mathrm{d}\\beta}{\\mathrm{d}t}=-\\frac{1}{2}\\frac{\\partial}{\\partial\\beta}\\left[\\mathcal{R}(\\beta,\\mathbf{r})-\\operatorname*{min}\\mathcal{R}(\\cdot,\\cdot)\\right];}\\\\ {\\displaystyle\\frac{\\mathrm{d}\\Gamma}{\\mathrm{d}t}=-\\frac{1}{2}\\frac{\\partial}{\\partial\\mathbf{T}}\\left[\\mathcal{R}(\\beta,\\mathbf{r})-\\operatorname*{min}\\mathcal{R}(\\cdot,\\cdot)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "First, we have the following corollary of Theorem 5.2, which computes the excess risk ${\\mathcal R}(\\beta,{\\bf T})-$ $\\operatorname*{min}\\mathcal{R}(\\cdot,\\cdot)$ ", "page_idx": 33}, {"type": "text", "text": "Corollary G.1 (Excess Risk). We fix $M$ as the context length. Consider the ICL risk in (3.5), assume the data is generated following Assumption 3.1. Then, we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\mathcal{R}}(\\beta,\\mathbf{F})-\\operatorname*{min}\\mathcal{R}(\\cdot,\\cdot)}\\\\ &{\\;=\\left(\\beta-\\beta^{*}\\right)^{\\top}\\left[\\left(\\mathbf{I}_{d}-\\mathbf{F}\\mathbf{H}\\right)^{\\top}\\mathbf{H}\\left(\\mathbf{I}_{d}-\\mathbf{F}\\mathbf{H}\\right)+\\frac{\\mathrm{tr}\\left(\\mathbf{H}\\Gamma^{\\top}\\mathbf{H}\\mathbf{I}\\right)}{M}\\mathbf{H}+\\frac{1}{M}\\mathbf{H}\\Gamma^{\\top}\\mathbf{H}\\mathbf{I}\\mathbf{H}\\right]\\left(\\beta-\\beta^{*}\\right)}\\\\ &{\\;+\\mathrm{tr}\\left[\\left(\\mathbf{H}^{\\frac{1}{2}}\\mathbf{F}\\mathbf{H}^{\\frac{1}{2}}-\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}^{\\frac{1}{2}}\\Omega^{-1}\\right)\\Omega\\left(\\mathbf{H}^{\\frac{1}{2}}\\mathbf{F}\\mathbf{H}^{\\frac{1}{2}}-\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}^{\\frac{1}{2}}\\Omega^{-1}\\right)^{\\top}\\right].\\quad\\quad\\quad\\quad\\quad\\quad\\quad(\\mathrm{G}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Proof. This is obtained directly from equation (C.5) and (C.7) ", "page_idx": 33}, {"type": "text", "text": "Now, we write out the differential equations explicitly in the following lemma. ", "page_idx": 33}, {"type": "text", "text": "Lemma G.2 (Dynamical system). The dynamical system of gradient fow described in (G.1) and (G.2) is ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\mathrm{d}\\boldsymbol{\\beta}}{\\mathrm{d}t}=-\\left[\\left(\\mathbf{I}_{d}-\\mathbf{I}\\mathbf{H}\\right)^{\\top}\\mathbf{H}\\left(\\mathbf{I}_{d}-\\mathbf{I}\\mathbf{H}\\right)+\\frac{\\mathrm{tr}\\left(\\mathbf{H}\\mathbf{I}^{\\top}\\mathbf{H}\\mathbf{I}\\right)}{M}\\mathbf{H}+\\frac{1}{M}\\mathbf{H}\\mathbf{I}^{\\top}\\mathbf{H}\\mathbf{I}\\mathbf{H}\\right]\\left(\\boldsymbol{\\beta}-\\boldsymbol{\\beta}^{*}\\right)}\\\\ {\\displaystyle\\frac{\\mathrm{d}\\mathbf{I}}{\\mathrm{d}t}=-\\left(\\mathbf{H}\\mathbf{I}\\mathbf{H}^{\\frac{1}{2}}\\boldsymbol{\\Omega}\\mathbf{H}^{\\frac{1}{2}}-\\mathbf{H}\\boldsymbol{\\Psi}\\mathbf{H}\\right)-\\frac{M+1}{M}\\mathbf{H}\\mathbf{I}\\mathbf{H}\\left(\\boldsymbol{\\beta}-\\boldsymbol{\\beta}^{*}\\right)\\left(\\boldsymbol{\\beta}-\\boldsymbol{\\beta}^{*}\\right)^{\\top}\\mathbf{H}}\\\\ {\\displaystyle-\\left.\\frac{1}{M}\\left(\\boldsymbol{\\beta}-\\boldsymbol{\\beta}^{*}\\right)^{\\top}\\mathbf{H}\\left(\\boldsymbol{\\beta}-\\boldsymbol{\\beta}^{*}\\right)\\cdot\\mathbf{H}\\mathbf{I}\\mathbf{H}+\\mathbf{H}\\left(\\boldsymbol{\\beta}-\\boldsymbol{\\beta}^{*}\\right)\\left(\\boldsymbol{\\beta}-\\boldsymbol{\\beta}^{*}\\right)^{\\top}\\mathbf{H}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Proof. This can be obtained by directly calculating the derivatives over the excess risk (G.3). To write out the dynamics of $\\beta$ , it suffices to notice that $\\beta$ only attends the first term of the RHS of (G.3), which is a standard quadratic function. For the derivatives of $\\mathbf{T}$ ,wefirsthave ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\frac{1}{2}\\displaystyle\\frac{\\partial}{\\partial\\Gamma}\\mathrm{tr}\\left[\\left(\\mathbf{H}^{\\frac{1}{2}}\\Gamma\\mathbf{H}^{\\frac{1}{2}}-\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}^{\\frac{1}{2}}\\Omega^{-1}\\right)\\Omega\\left(\\mathbf{H}^{\\frac{1}{2}}\\Gamma\\mathbf{H}^{\\frac{1}{2}}-\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}^{\\frac{1}{2}}\\Omega^{-1}\\right)^{\\top}\\right]}\\\\ &{=\\mathbf{H}^{\\frac{1}{2}}\\left(\\mathbf{H}^{\\frac{1}{2}}\\Gamma\\mathbf{H}^{\\frac{1}{2}}-\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}^{\\frac{1}{2}}\\Omega^{-1}\\right)\\Omega^{\\frac{1}{2}}\\cdot\\Omega^{\\frac{1}{2}}\\mathbf{H}^{\\frac{1}{2}}}\\\\ &{=\\mathbf{H}\\Gamma\\mathbf{H}^{\\frac{1}{2}}\\Omega\\mathbf{H}^{\\frac{1}{2}}-\\mathbf{H}\\Psi\\mathbf{H}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Now it suffices to compute ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\frac{\\partial}{\\partial\\mathbf{T}}\\frac{1}{2}\\left(\\beta-\\beta^{*}\\right)^{\\top}\\left[\\left(\\mathbf{I}_{d}-\\mathbf{PH}\\right)^{\\top}\\mathbf{H}\\left(\\mathbf{I}_{d}-\\mathbf{PH}\\right)+\\frac{\\mathrm{tr}\\left(\\mathbf{H}\\mathbf{T}^{\\top}\\mathbf{H}\\mathbf{T}\\right)}{M}\\mathbf{H}+\\frac{1}{M}\\mathbf{H}\\mathbf{T}^{\\top}\\mathbf{H}\\mathbf{T}\\mathbf{H}\\right]\\left(\\beta-\\beta^{*}\\right).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Let's compute the partial derivativess separately. From Lemma H.3, we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\frac{\\partial}{\\partial{\\bf{T}}}\\frac{1}{2}\\left({\\boldsymbol{\\beta}}-{\\boldsymbol{\\beta}}^{*}\\right)^{\\top}\\left(-{\\bf{H}}{\\bf{T}}^{\\top}{\\bf{H}}\\right)\\left({\\boldsymbol{\\beta}}-{\\boldsymbol{\\beta}}^{*}\\right)=\\frac{\\partial}{\\partial{\\bf{T}}}\\frac{1}{2}\\left({\\boldsymbol{\\beta}}-{\\boldsymbol{\\beta}}^{*}\\right)^{\\top}\\left(-{\\bf{H}}{\\bf{I}}{\\bf{H}}\\right)\\left({\\boldsymbol{\\beta}}-{\\boldsymbol{\\beta}}^{*}\\right)}\\ ~}\\\\ {{\\displaystyle\\qquad\\qquad\\qquad\\qquad=-\\frac{1}{2}{\\bf{H}}\\left({\\boldsymbol{\\beta}}-{\\boldsymbol{\\beta}}^{*}\\right)\\left({\\boldsymbol{\\beta}}-{\\boldsymbol{\\beta}}^{*}\\right)^{\\top}{\\bf{H}};}}\\\\ {{\\displaystyle\\frac{\\partial}{\\partial{\\bf{T}}}\\frac{1}{2}\\left(\\frac{M+1}{M}\\left({\\boldsymbol{\\beta}}-{\\boldsymbol{\\beta}}^{*}\\right)^{\\top}{\\bf{H}}{\\bf{T}}^{\\top}{\\bf{H}}{\\bf{I}}\\left({\\boldsymbol{\\beta}}-{\\boldsymbol{\\beta}}^{*}\\right)\\right)=\\frac{M+1}{M}{\\bf{H}}{\\bf{T}}\\cdot{\\bf{H}}\\left({\\boldsymbol{\\beta}}-{\\boldsymbol{\\beta}}^{*}\\right)\\left({\\boldsymbol{\\beta}}-{\\boldsymbol{\\beta}}^{*}\\right)^{\\top}{\\bf{H}};}}\\\\ {{\\displaystyle\\frac{\\partial}{\\partial{\\bf{T}}}\\frac{1}{2}\\left(\\frac{\\mathrm{tr}\\left\\{{\\bf{H}}{\\bf{T}}^{\\top}{\\bf{H}}{\\bf{I}}\\right\\}}{M}\\cdot\\left({\\boldsymbol{\\beta}}-{\\boldsymbol{\\beta}}^{*}\\right)^{\\top}{\\bf{H}}\\left({\\boldsymbol{\\beta}}-{\\boldsymbol{\\beta}}^{*}\\right)\\right)=\\frac{1}{M}\\left({\\boldsymbol{\\beta}}-{\\boldsymbol{\\beta}}^{*}\\right)^{\\top}{\\bf{H}}\\left({\\boldsymbol{\\beta}}-{\\boldsymbol{\\beta}}^{*}\\right)\\cdot{\\bf{H}}{\\bf{I}}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Summing over the three equations above and applying the definition of gradient flow, we conclude the dynamics of $\\mathbf{T}$ in (G.5). \u53e3 ", "page_idx": 34}, {"type": "text", "text": "G.2  Proof of the global convergence ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Let's now prove Theorem 6.3. Since the first part of Theorem 6.3 is directly implied by the second part, we only deal with the case with general $\\mathbf{H}$ . In this section, we denote $\\lambda_{-1}>0$ as the minimal non-zeroeigenvalueof $\\mathbf{H}$ .As in the main text, we define ", "page_idx": 34}, {"type": "equation", "text": "$$\n{\\mathcal{H}}:=\\mathsf{I m}\\left(\\mathbf{H}\\right)\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "and $\\mathcal{H}^{\\perp}$ as its orthogonal complement. First, let's prove the convergence of $\\beta$ ", "page_idx": 34}, {"type": "text", "text": "Lemma G.3 (Convergence of $\\beta$ ). Under the dynamical system (G.4) and (G.5), one has ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\left\\Vert\\mathcal{P}_{\\mathcal{H}}\\left(\\beta(t)\\right)-\\mathcal{P}_{\\mathcal{H}}\\left(\\beta^{*}\\right)\\right\\Vert_{2}^{2}\\leq\\exp\\left(\\frac{-2\\lambda_{-1}t}{M+1}\\right)\\left\\Vert\\mathcal{P}_{\\mathcal{H}}\\left(\\beta(0)\\right)-\\mathcal{P}_{\\mathcal{H}}\\left(\\beta^{*}\\right)\\right\\Vert_{2}^{2},\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "which implies ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\left\\|{\\bf H}^{\\frac{1}{2}}\\left(\\beta(t)-\\beta^{*}\\right)\\right\\|_{2}^{2}\\leq\\lambda_{1}\\exp\\left(\\frac{-2\\lambda_{-1}t}{M+1}\\right)\\|\\beta(0)-\\beta^{*}\\|_{2}^{2}\\,,\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "and ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathcal{P}_{\\mathcal{H}}\\left(\\beta(t)\\right)\\to\\mathcal{P}_{\\mathcal{H}}\\left(\\beta^{*}\\right)\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "when $t\\to\\infty$ , from arbitrary initialization $\\beta(0)$ and $\\mathbf{\\delta}\\Gamma(0)$ . Moreover, one has for any $t>0$ , it holds that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathcal{P}_{\\mathcal{H}^{\\bot}}\\left(\\beta(t)\\right)=\\mathcal{P}_{\\mathcal{H}^{\\bot}}\\left(\\beta(0)\\right)\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Proof. We first consider the orthogonal projection operator $\\mathcal{P}$ . From Lemma H.5 and equation (G.4), weknow ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}\\mathcal{P}_{\\mathcal{H}}(\\beta(t)-\\beta^{*})}{\\mathrm{d}t}=-\\mathbf{H}\\mathbf{H}^{+}\\mathbf{H}_{\\Gamma}\\left(\\beta-\\beta^{*}\\right),\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbf{H}_{\\Gamma}:=\\left(\\mathbf{I}_{d}-\\mathbf{\\Gamma}\\mathbf{I}\\mathbf{H}\\right)^{\\top}\\mathbf{H}\\left(\\mathbf{I}_{d}-\\mathbf{\\Gamma}\\mathbf{I}\\mathbf{H}\\right)+\\frac{\\mathrm{tr}\\left(\\mathbf{H}\\mathbf{\\Gamma}^{\\top}\\mathbf{H}\\mathbf{I}\\right)}{M}\\mathbf{H}+\\frac{1}{M}\\mathbf{H}\\mathbf{\\Gamma}^{\\top}\\mathbf{H}\\mathbf{I}\\mathbf{I}\\mathbf{I}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "From the property of pseudo-inverse, we know ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}\\mathcal{P}_{\\mathcal{H}}(\\beta(t)-\\beta^{*})}{\\mathrm{d}t}=-\\mathbf{H}_{\\Gamma}\\left(\\beta-\\beta^{*}\\right)=-\\mathbf{H}_{\\Gamma}\\mathbf{H}^{+}\\mathbf{H}\\left(\\beta-\\beta^{*}\\right)=-\\mathbf{H}_{\\Gamma}\\mathcal{P}_{\\mathcal{H}}\\left(\\beta-\\beta^{*}\\right).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Therefore, we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t}\\left[\\frac{1}{2}\\left\\|\\mathcal{P}_{\\mathcal{H}}(\\beta(t)-\\beta^{*})\\right\\|_{2}^{2}\\right]=-\\mathcal{P}_{\\mathcal{H}}\\left(\\beta-\\beta^{*}\\right)^{\\top}\\cdot{\\bf H}_{\\Gamma}\\cdot\\mathcal{P}_{\\mathcal{H}}\\left(\\beta-\\beta^{*}\\right).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Notice that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbf{\\mathcal{I}}_{\\mathbf{T}}\\succeq\\left(\\sqrt{\\frac{M}{M+1}}\\mathbf{I}-\\sqrt{\\frac{M+1}{M}}\\mathbf{I}\\mathbf{H}\\right)^{\\top}\\mathbf{H}\\left(\\sqrt{\\frac{M}{M+1}}\\mathbf{I}-\\sqrt{\\frac{M+1}{M}}\\mathbf{I}\\mathbf{H}\\right)+\\frac{1}{M+1}\\mathbf{H}\\succeq\\frac{1}{M+1}\\mathbf{H}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "This implies ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\mathrm{d}}{\\mathrm{d}t}\\left[\\frac{1}{2}\\left\\|\\mathcal{P}_{\\mathcal{H}}(\\beta(t)-\\beta^{*})\\right\\|_{2}^{2}\\right]\\leq-\\frac{1}{M+1}\\mathcal{P}_{\\mathcal{H}}\\left(\\beta-\\beta^{*}\\right)^{\\top}\\cdot\\mathbf{H}\\cdot\\mathcal{P}_{\\mathcal{H}}\\left(\\beta-\\beta^{*}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq-\\frac{\\lambda_{-1}}{M+1}\\left\\|\\mathcal{P}_{\\mathcal{H}}\\left(\\beta-\\beta^{*}\\right)\\right\\|_{2}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where the last line comes from Lemma H.5 and $\\lambda_{-1}$ is the minimal non-zero eigenvalue of $\\mathbf{H}$ .Via standard integration method in ODE, we know this suggests ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\|\\mathcal{P}_{\\mathcal{H}}\\left(\\beta(t)-\\beta^{*}\\right)\\|_{2}^{2}\\leq\\exp\\left(\\frac{-2\\lambda_{-1}t}{M+1}\\right)\\left\\|\\mathcal{P}_{\\mathcal{H}}\\left(\\beta(0)-\\beta^{*}\\right)\\right\\|_{2}^{2}\\rightarrow0\\quad\\mathrm{~when~}t\\rightarrow\\infty.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "This indicates that $\\mathcal{P}_{\\mathcal{H}}\\left(\\beta(t)\\right)\\to\\mathcal{P}_{\\mathcal{H}}\\left(\\beta^{*}\\right)$ when $t\\to\\infty$ . Finally, we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}\\mathcal{P}_{\\mathcal{H}^{\\bot}}\\left(\\beta(t)-\\beta^{*}\\right)}{\\mathrm{d}t}=-\\left(\\mathbf{I}_{d}-\\mathbf{H}\\mathbf{H}^{+}\\right)\\mathbf{H}_{\\mathbf{T}}\\left(\\beta-\\beta^{*}\\right)=\\mathbf{0}_{d},\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "which implies $\\mathcal{P}_{\\mathcal{H}^{\\bot}}(\\beta(t)-\\beta^{*})=\\mathcal{P}_{\\mathcal{H}^{\\bot}}(\\beta(0)-\\beta^{*})$ and hence, $\\mathcal{P}_{\\mathcal{H}^{\\bot}}(\\beta(t))=\\mathcal{P}_{\\mathcal{H}^{\\bot}}(\\beta(0))$ for any $t>0$ \u53e3 ", "page_idx": 35}, {"type": "text", "text": "Next, let's consider the convergence of $\\mathbf{T}$ . As defined in the main text, we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathcal{Z}:=\\mathsf{I m}\\left(\\mathbf{H}\\otimes\\mathbf{H}\\right)\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "and $\\mathcal{Z}^{\\perp}$ is its orthogonal complement. We have the following lemma. ", "page_idx": 35}, {"type": "text", "text": "Lemma G.4. Under the dynamical system (G.4) and (G.5), one has ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\frac{\\mathrm{d}}{\\mathrm{li}}\\mathcal{P}_{\\mathcal{L}^{\\perp}}\\left({\\bf F}-{\\bf F}^{*}\\right)=\\mathbf{0}_{d\\times d},\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\mathrm{(G9)}}\\\\ {\\displaystyle\\frac{\\mathrm{d}}{\\mathrm{d}t}\\mathcal{P}_{\\mathcal{Z}}\\left({\\bf F}-{\\bf F}^{*}\\right)=-\\mathbf{H}\\cdot\\mathcal{P}_{\\mathcal{Z}}\\left({\\bf F}-{\\bf F}^{*}\\right)\\cdot\\mathbf{H}^{\\frac{1}{2}}\\Omega\\mathbf{H}^{\\frac{1}{2}}-\\frac{M+1}{M}\\mathbf{H}\\cdot\\mathcal{P}_{\\mathcal{Z}}\\left({\\bf F}-{\\bf F}^{*}\\right)\\cdot\\mathbf{H}\\left(\\beta-\\beta^{*}\\right)\\left(\\beta-\\beta^{*}\\right)}\\\\ {\\displaystyle-\\,\\frac{1}{M}\\left(\\beta-\\beta^{*}\\right)^{\\top}\\mathbf{H}\\left(\\beta-\\beta^{*}\\right)\\cdot\\mathbf{H}\\cdot\\mathcal{P}_{\\mathcal{Z}}\\left({\\bf F}-{\\bf F}^{*}\\right)\\cdot\\mathbf{H}\\quad\\ }\\\\ {\\displaystyle-\\,\\frac{1}{M}\\left(\\beta-\\beta^{*}\\right)^{\\top}\\mathbf{H}\\left(\\beta-\\beta^{*}\\right)\\cdot\\mathbf{H}\\mathbf{T}^{*}\\mathbf{H}-\\frac{1}{M}\\mathbf{H}\\mathbf{T}^{*}\\mathbf{H}\\left(\\beta-\\beta^{*}\\right)\\left(\\beta-\\beta^{*}\\right)^{\\top}\\mathbf{H}\\quad\\ }\\\\ {\\displaystyle+\\,\\frac{1}{M}\\mathbf{H}^{\\frac{1}{2}}\\Omega^{-1}\\left(\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}^{\\frac{1}{2}}+\\left(\\mathrm{tr}\\left(\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}^{\\frac{1}{2}}\\right)+\\sigma^{2}\\right)\\mathbf{I}_{d}\\right)\\mathbf{H}^{\\frac{1}{2}}\\left(\\beta-\\beta^{*}\\right)\\left(\\beta-\\beta^{*}\\right)^{\\top}}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Proof. The first ODE is trivial since the RHS of (G.5) always lies in $\\mathcal{Z}:=\\mathsf{I m}\\left(\\mathbf{H}\\otimes\\mathbf{H}\\right)$ soits projection on $\\mathcal{Z}^{\\perp}$ always vanishes. To prove the second equation, we can rewrite (G.5) as ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\mathrm{d}\\Gamma}{\\mathrm{d}t}=-\\left(\\mathbf{H}\\left(\\Gamma-\\Gamma^{*}\\right)\\mathbf{H}^{\\frac{1}{2}}\\boldsymbol{\\Omega}\\mathbf{H}^{\\frac{1}{2}}\\underbrace{\\mathbf{\\Gamma}\\mathbf{H}\\mathbf{H}+\\mathbf{H}\\mathbf{T}^{*}\\mathbf{H}^{\\frac{1}{2}}\\boldsymbol{\\Omega}\\mathbf{H}^{\\frac{1}{2}}}_{A}\\right)}\\\\ {\\displaystyle\\quad-\\,\\frac{M+1}{M}\\mathbf{H}\\left(\\mathbf{I}-\\Gamma^{*}\\right)\\mathbf{H}\\left(\\beta-\\beta^{*}\\right)\\left(\\beta-\\beta^{*}\\right)^{\\top}\\mathbf{H}}\\\\ {\\displaystyle\\quad-\\,\\frac{1}{M}\\left(\\beta-\\beta^{*}\\right)^{\\top}\\mathbf{H}\\left(\\beta-\\beta^{*}\\right)\\cdot\\mathbf{H}\\left(\\Gamma-\\Gamma^{*}\\right)\\mathbf{H}+\\underbrace{\\mathbf{H}\\left(\\beta-\\beta^{*}\\right)\\left(\\beta-\\beta^{*}\\right)^{\\top}\\mathbf{H}}_{B}}\\\\ {\\displaystyle\\quad-\\,\\frac{1}{M}\\left(\\beta-\\beta^{*}\\right)^{\\top}\\mathbf{H}\\left(\\beta-\\beta^{*}\\right)\\cdot\\mathbf{H}\\mathbf{T}^{*}\\mathbf{H}\\underbrace{-\\frac{M+1}{M}\\mathbf{H}^{\\top}\\mathbf{H}\\left(\\beta-\\beta^{*}\\right)\\left(\\beta-\\beta^{*}\\right)^{\\top}\\mathbf{H}}_{M}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "For term A, recalling $\\Gamma^{*}=\\Psi\\mathbf{H}^{\\frac{1}{2}}\\Omega^{-1}\\mathbf{H}^{-\\frac{1}{2}}$ , we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{H}\\mathbf{I}^{*}\\mathbf{H}^{\\frac{1}{2}}\\Omega\\mathbf{H}^{\\frac{1}{2}}=\\mathbf{H}\\Psi\\mathbf{H}^{\\frac{1}{2}}\\Omega^{-1}\\mathbf{H}^{-\\frac{1}{2}}\\mathbf{H}^{\\frac{1}{2}}\\Omega\\mathbf{H}^{\\frac{1}{2}}}\\\\ &{\\qquad\\qquad\\qquad=\\mathbf{H}^{\\frac{1}{2}}\\Omega^{-1}\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}^{\\frac{1}{2}}\\mathbf{H}^{-\\frac{1}{2}}\\mathbf{H}^{\\frac{1}{2}}\\Omega\\mathbf{H}^{\\frac{1}{2}}\\operatorname{\\!\\!\\!\\!\\!}\\qquad\\quad(\\Omega\\mathrm{~and~}\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}^{\\frac{1}{2}}\\operatorname{commute.})}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\mathbf{H}^{\\frac{1}{2}}\\Omega^{-1}\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}^{\\frac{1}{2}}\\Omega\\mathbf{H}^{\\frac{1}{2}}}\\\\ &{=\\mathbf{H}\\Psi\\mathbf{H}^{\\frac{1}{2}}\\Omega^{-1}\\Omega\\mathbf{H}^{\\frac{1}{2}}}\\\\ &{=\\mathbf{H}\\Psi\\mathbf{H}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "equation", "text": "$$\n(\\mathbf{H}^{\\frac{1}{2}}\\mathbf{H}^{-\\frac{1}{2}}\\mathbf{H}^{\\frac{1}{2}}=\\mathbf{H}^{\\frac{1}{2}})\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "This suggests $A=0$ . For $B+C$ , we have ", "page_idx": 36}, {"type": "equation", "text": "$$\nB+C=-\\frac{1}{M}\\mathbf{H}\\mathbf{T}^{*}\\mathbf{H}\\left(\\beta-\\beta^{*}\\right)\\left(\\beta-\\beta^{*}\\right)^{\\top}\\mathbf{H}+\\left(\\mathbf{I}_{d}-\\mathbf{H}\\mathbf{T}^{*}\\right)\\mathbf{H}\\left(\\beta-\\beta^{*}\\right)\\left(\\beta-\\beta^{*}\\right)^{\\top}\\mathbf{H}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "We can compute $(\\mathbf{I}_{d}-\\mathbf{H}\\mathbf{I}^{*})\\,\\mathbf{H}$ as ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\mathbf{I}_{d}-\\mathbf{H}\\mathbf{I}^{*}\\right)\\mathbf{H}=\\mathbf{H}^{\\frac{1}{2}}\\left(\\mathbf{H}^{\\frac{1}{2}}-\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}^{\\frac{1}{2}}\\Omega^{-1}\\mathbf{H}^{-\\frac{1}{2}}\\mathbf{H}\\right)}\\\\ &{\\qquad\\qquad\\qquad=\\mathbf{H}^{\\frac{1}{2}}\\left(\\mathbf{H}^{\\frac{1}{2}}-\\Omega^{-1}\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}^{\\frac{1}{2}}\\mathbf{H}^{-\\frac{1}{2}}\\mathbf{H}\\right)\\qquad\\quad(\\Omega\\mathrm{~and~}\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}^{\\frac{1}{2}}\\mathrm{~commute.})}\\\\ &{\\qquad\\qquad=\\mathbf{H}^{\\frac{1}{2}}\\left(\\mathbf{H}^{\\frac{1}{2}}-\\Omega^{-1}\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}\\right)\\qquad\\qquad\\qquad\\qquad\\qquad(\\mathbf{H}^{\\frac{1}{2}}\\mathbf{H}^{-\\frac{1}{2}}\\mathbf{H}^{\\frac{1}{2}}=\\mathbf{H}^{\\frac{1}{2}})}\\\\ &{\\qquad\\qquad=\\mathbf{H}^{\\frac{1}{2}}\\left(\\Omega^{-1}\\Omega\\mathbf{H}^{\\frac{1}{2}}-\\Omega^{-1}\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}\\right)}\\\\ &{\\qquad\\qquad=\\frac{1}{M}\\mathbf{H}^{\\frac{1}{2}}\\Omega^{-1}\\left(\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}^{\\frac{1}{2}}+\\left(\\mathrm{tr}\\left(\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}^{\\frac{1}{2}}\\right)+\\sigma^{2}\\right)\\mathbf{I}_{d}\\right)\\mathbf{H}^{\\frac{1}{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Therefore, ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{B+C=-\\frac{1}{M}{\\bf H}^{*}{\\bf H}\\left(\\beta-\\beta^{*}\\right)\\left(\\beta-\\beta^{*}\\right)^{\\top}{\\bf H}}\\ ~}}\\\\ {{\\displaystyle~~~~~~~+\\frac{1}{M}{\\bf H}^{\\frac{1}{2}}\\Omega^{-1}\\left({\\bf H}^{\\frac{1}{2}}\\Psi{\\bf H}^{\\frac{1}{2}}+\\left(\\mathrm{tr}\\left({\\bf H}^{\\frac{1}{2}}\\Psi{\\bf H}^{\\frac{1}{2}}\\right)+\\sigma^{2}\\right){\\bf I}_{d}\\right){\\bf H}^{\\frac{1}{2}}\\left(\\beta-\\beta^{*}\\right)\\left(\\beta-\\beta^{*}\\right)^{\\top}{\\bf H}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Bridging $A=0$ and the result of $B+C$ into the ODE, we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~~~\\displaystyle\\frac{\\mathrm{d}\\left(\\mathbf{T}-\\mathbf{F}^{*}\\right)}{\\mathrm{d}t}}\\\\ &{=\\,-\\,\\mathbf{H}\\left(\\mathbf{F}-\\mathbf{F}^{*}\\right)\\mathbf{H}^{\\frac{1}{2}}\\mathbf{\\Omega}\\mathbf{H}\\mathbf{H}^{\\frac{1}{2}}-\\frac{M+1}{M}\\mathbf{H}\\left(\\mathbf{F}-\\mathbf{T}^{*}\\right)\\mathbf{H}\\left(\\beta-\\beta^{*}\\right)\\left(\\beta-\\beta^{*}\\right)^{\\top}\\mathbf{H}}\\\\ &{-\\,\\displaystyle\\frac{1}{M}\\left(\\beta-\\beta^{*}\\right)^{\\top}\\mathbf{H}\\left(\\beta-\\beta^{*}\\right)\\cdot\\mathbf{H}\\left(\\mathbf{F}-\\mathbf{F}^{*}\\right)\\mathbf{H}}\\\\ &{-\\,\\displaystyle\\frac{1}{M}\\left(\\beta-\\beta^{*}\\right)^{\\top}\\mathbf{H}\\left(\\beta-\\beta^{*}\\right)\\cdot\\mathbf{H}\\mathbf{I}^{*}\\mathbf{H}-\\frac{1}{M}\\mathbf{H}\\mathbf{T}^{*}\\mathbf{H}\\left(\\beta-\\beta^{*}\\right)\\left(\\beta-\\beta^{*}\\right)^{\\top}\\mathbf{H}}\\\\ &{+\\,\\displaystyle\\frac{1}{M}\\mathbf{H}^{\\frac{1}{2}}\\mathbf{O}^{-1}\\left(\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}^{\\frac{1}{2}}+\\left(\\mathrm{tr}\\left(\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}^{\\frac{1}{2}}\\right)+\\sigma^{2}\\right)\\mathbf{I}_{d}\\right)\\mathbf{H}^{\\frac{1}{2}}\\left(\\beta-\\beta^{*}\\right)\\left(\\beta-\\beta^{*}\\right)^{\\top}\\mathbf{H}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Note that, all terms in the RHS above lie in $\\mathcal{Z}\\;=\\;\\mathsf{l m}\\left(\\mathbf{H}^{\\otimes2}\\right)$ , so this summation also equals $\\begin{array}{r}{\\frac{\\mathrm{d}}{\\mathrm{d}t}\\mathcal{P}_{\\mathcal{Z}}\\left(\\boldsymbol{\\Gamma}-\\boldsymbol{\\Gamma}^{*}\\right)}\\end{array}$ Moreover, since ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{H}^{\\frac{1}{2}}\\left(\\mathbf{I}-\\mathbf{I}^{*}\\right)\\mathbf{H}^{\\frac{1}{2}}=\\mathbf{H}^{\\frac{1}{2}}\\left[\\mathcal{P}_{\\mathrm{lm}}\\!\\left(\\mathbf{H}^{\\frac{1}{2}}\\otimes\\mathbf{H}^{\\frac{1}{2}}\\right)\\left(\\mathbf{I}-\\mathbf{I}^{*}\\right)+\\mathcal{P}_{\\mathrm{null}}\\!\\left(\\mathbf{H}^{\\frac{1}{2}}\\otimes\\mathbf{H}^{\\frac{1}{2}}\\right)\\left(\\mathbf{I}-\\mathbf{I}^{*}\\right)\\right]\\mathbf{H}^{\\frac{1}{2}}}\\\\ &{\\qquad\\qquad\\qquad=\\mathbf{H}^{\\frac{1}{2}}\\cdot\\mathcal{P}_{\\mathrm{lm}}\\!\\left(\\mathbf{H}^{\\frac{1}{2}}\\otimes\\mathbf{H}^{\\frac{1}{2}}\\right)\\left(\\mathbf{I}-\\mathbf{I}^{*}\\right)\\mathbf{H}^{\\frac{1}{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Bridging (G.12) into (G.11), we conclude ", "page_idx": 36}, {"type": "text", "text": "Then, let's upper bound the dynamics of $\\left\\|\\mathcal{P}_{\\mathcal{Z}}\\left(\\mathbf{r}-\\mathbf{Gamma}\\mathbf{r}^{*}\\right)\\right\\|_{F}^{2}$ in the following lemma. ", "page_idx": 36}, {"type": "text", "text": "Lemma G.5 (Dynamics of Frobenius norm). Under the dynamical system (G.4) and (G.5), one has ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t}\\left[\\frac12\\left\\|\\mathcal P_{\\mathcal Z}\\left(\\mathbf{T}-\\mathbf{T}^{*}\\right)\\right\\|_{F}^{2}\\right]\\le-A_{1}\\left\\|\\mathcal P_{\\mathcal Z}\\left(\\mathbf{T}-\\mathbf{T}^{*}\\right)\\right\\|_{F}^{2}+A_{2}\\exp\\left(\\frac{-2\\lambda_{-1}t}{M+1}\\right)\\left\\|\\mathcal P_{\\mathcal Z}\\left(\\mathbf{T}-\\mathbf{T}^{*}\\right)\\right\\|_{F},\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\cal A}_{1}=\\lambda_{-1}^{2}\\lambda_{\\mathrm{min}}\\left({\\pmb\\Omega}\\right),}}\\\\ {{\\displaystyle{\\cal A}_{2}=\\left(2+\\frac{2}{M}\\right)\\lambda_{1}^{2}\\left\\|\\beta(0)-\\beta^{*}\\right\\|_{2}^{2}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "are two positive constant. Here, $\\lambda_{-1}>0$ is the minimal positive eigenvalue of $\\mathbf{H}$ \uff0c $\\lambda_{1}$ is themaximal eigenvalue of $\\mathbf{H}$ \uff0c $\\lambda_{\\mathrm{min}}\\left(\\Omega\\right)$ is the minimal eigenvalue of $\\Omega$ (defined in (A.3)) and is strictly positive. ", "page_idx": 37}, {"type": "text", "text": "Proof. From the dynamics of $\\mathcal{P}_{\\mathcal{Z}}\\Gamma$ in (G.10), we know ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\mathrm{d}}{\\mathrm{d}t}\\left[\\frac{1}{2}\\left\\|\\mathcal{P}z\\left(\\mathbf{r}-\\mathbf{r}^{*}\\right)\\right\\|_{F}^{2}\\right]=\\mathrm{tr}\\left(\\frac{\\mathrm{d}}{\\mathrm{d}t}\\mathcal{P}z\\left(\\mathbf{r}-\\mathbf{r}^{*}\\right)\\cdot\\mathcal{P}z\\left(\\mathbf{r}-\\mathbf{r}^{*}\\right)^{\\top}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad=G_{1}+G_{2}+G_{3},}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{G_{1}=-\\mathrm{tr}\\left[\\mathbf{H}\\cdot\\mathcal{P}_{Z}\\left(\\mathbf{F}-\\mathbf{F}^{*}\\right)\\cdot\\mathbf{H}^{\\frac{1}{2}}\\mathbf{O}\\mathbf{H}^{\\frac{1}{2}}\\cdot\\mathcal{P}_{Z}\\left(\\mathbf{F}-\\mathbf{F}^{*}\\right)^{\\top}\\right],}\\\\ &{G_{2}=-\\mathrm{tr}\\left[\\frac{M+1}{M}\\mathbf{H}\\cdot\\mathcal{P}_{Z}\\left(\\mathbf{F}-\\mathbf{F}^{*}\\right)\\cdot\\mathbf{H}\\left(\\beta-\\beta^{*}\\right)\\left(\\beta-\\beta^{*}\\right)^{\\top}\\mathbf{H}\\cdot\\mathcal{P}_{Z}\\left(\\mathbf{F}-\\mathbf{F}^{*}\\right)^{\\top}\\right]}\\\\ &{\\phantom{G_{1}=-\\mathrm{tr}\\left[\\frac{1}{M}\\left(\\beta-\\beta^{*}\\right)^{\\top}\\mathbf{H}\\left(\\beta-\\beta^{*}\\right)\\cdot\\mathbf{H}\\cdot\\mathcal{P}_{Z}\\left(\\mathbf{F}-\\mathbf{F}^{*}\\right)\\cdot\\mathbf{H}\\cdot\\mathcal{P}_{Z}\\left(\\mathbf{F}-\\mathbf{F}^{*}\\right)^{\\top}\\right],}\\\\ &{G_{3}=-\\mathrm{tr}\\left[\\frac{1}{M}\\left(\\beta-\\beta^{*}\\right)^{\\top}\\mathbf{H}\\left(\\beta-\\beta^{*}\\right)\\cdot\\mathbf{H}\\mathbf{F}^{*}\\mathbf{H}\\cdot\\mathcal{P}_{Z}\\left(\\mathbf{F}-\\mathbf{F}^{*}\\right)^{\\top}\\right]}\\\\ &{\\phantom{G_{3}=-\\mathrm{tr}\\left[\\frac{1}{M}\\mathbf{H}^{*}\\mathbf{H}\\left(\\beta-\\beta^{*}\\right)\\left(\\beta-\\beta^{*}\\right)^{\\top}\\mathbf{H}\\cdot\\mathcal{P}_{Z}\\left(\\mathbf{F}-\\mathbf{F}^{*}\\right)^{\\top}\\right]}\\\\ &{\\phantom{G_{3}=+\\mathrm{tr}\\left[\\frac{1}{M}\\mathbf{H}^{\\frac{1}{2}}\\mathbf{O}^{-1}\\left(\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}^{\\frac{1}{2}}+\\left(\\mathrm{tr}\\left(\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}^{\\frac{1}{2}}\\right)+\\sigma^{2}\\right)\\mathbf{H}_{d}\\right)\\mathbf{H}^{\\frac{1}{2}}\\left(\\beta-\\beta^{*}\\right)\\left(\\beta-\\beta^{*}\\right)^{\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "From Lemma H.2, we know that $G_{2}\\leq0$ . Then, we consider $G_{1}$ and $G_{3}$ . For $G_{1}$ , we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{G_{1}=-\\mathrm{tr}\\left[\\left(\\mathbf{H}^{\\frac{1}{2}}\\mathcal{P}_{\\mathcal{Z}}\\left(\\mathbf{T}-\\mathbf{F}^{*}\\right)^{\\top}\\mathbf{H}^{\\frac{1}{2}}\\right)\\cdot\\left(\\mathbf{H}^{\\frac{1}{2}}\\mathcal{P}_{\\mathcal{Z}}\\left(\\mathbf{T}-\\mathbf{F}^{*}\\right)\\mathbf{H}^{\\frac{1}{2}}\\right)\\cdot\\Omega\\right]}\\\\ &{\\quad\\leq-\\lambda_{-1}^{2}\\mathrm{tr}\\left[\\mathcal{P}_{\\mathcal{Z}}\\left(\\mathbf{T}-\\mathbf{F}^{*}\\right)^{\\top}\\cdot\\mathcal{P}_{\\mathcal{Z}}\\left(\\mathbf{T}-\\mathbf{T}^{*}\\right)\\cdot\\Omega\\right]\\quad(3)\\;\\mathrm{in\\;Lemma\\;H.6\\;and\\;(2)\\;in\\;Lemma\\;H.2})}\\\\ &{\\quad\\leq-\\lambda_{-1}^{2}\\lambda_{\\operatorname*{min}}\\left(\\Omega\\right)\\left\\|\\mathcal{P}_{\\mathcal{Z}}\\left(\\mathbf{T}-\\mathbf{F}^{*}\\right)\\right\\|_{F}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where $\\lambda_{\\operatorname*{min}}(\\cdot)$ denote the minimal eigenvalue. Since $\\Omega$ is positive definite, we know $\\lambda_{\\operatorname*{min}}\\left(\\Omega\\right)>0$ Finally, let's upper bound $G_{3}$ . First, we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad-\\operatorname{tr}\\left[\\frac{1}{M}\\left(\\beta-\\beta^{*}\\right)^{\\top}\\mathbf{H}\\left(\\beta-\\beta^{*}\\right)\\cdot\\mathbf{H}\\mathbf{T}^{*}\\mathbf{H}\\cdot\\mathcal{P}_{Z}\\left(\\mathbf{T}-\\mathbf{T}^{*}\\right)^{\\top}\\right]}\\\\ &{\\leq\\!\\!\\frac{\\sqrt{d}}{M}\\left(\\beta-\\beta^{*}\\right)^{\\top}\\mathbf{H}\\left(\\beta-\\beta^{*}\\right)\\cdot\\left\\Vert\\mathbf{H}^{\\frac{1}{2}}\\mathbf{T}^{*}\\mathbf{H}^{\\frac{1}{2}}\\right\\Vert_{2}\\left\\Vert\\mathbf{H}^{\\frac{1}{2}}\\cdot\\mathcal{P}_{Z}\\left(\\mathbf{T}-\\mathbf{T}^{*}\\right)^{\\top}\\mathbf{H}^{\\frac{1}{2}}\\right\\Vert_{F}\\qquad\\mathrm{(Lemma~H.l)}}\\\\ &{\\leq\\!\\!\\frac{\\sqrt{d}\\lambda_{1}}{M}\\exp\\left(\\frac{-2\\lambda_{-1}t}{M+1}\\right)\\left\\Vert\\beta(0)-\\beta^{*}\\right\\Vert_{2}^{2}\\lambda_{\\operatorname*{max}}\\left(\\mathbf{H}^{\\frac{1}{2}}\\mathbf{T}^{*}\\mathbf{H}^{\\frac{1}{2}}\\right)\\cdot\\left\\Vert\\mathbf{H}\\right\\Vert_{F}\\left\\Vert\\mathcal{P}_{Z}\\left(\\mathbf{T}-\\mathbf{T}^{*}\\right)\\right\\Vert_{F}}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "equation", "text": "$$\n\\leq\\!\\frac{\\sqrt{d}\\lambda_{1}^{2}}{M}\\exp\\left(\\frac{-2\\lambda_{-1}t}{M+1}\\right)\\left\\Vert\\beta(0)-\\beta^{*}\\right\\Vert_{2}^{2}\\lambda_{\\operatorname*{max}}\\left(\\mathbf{H}^{\\frac{1}{2}}\\Gamma^{*}\\mathbf{H}^{\\frac{1}{2}}\\right)\\cdot\\left\\Vert\\mathcal{P}_{\\mathcal{Z}}\\left(\\mathbf{T}-\\mathbf{T}^{*}\\right)\\right\\Vert_{F}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "For $\\lambda_{\\operatorname*{max}}\\left(\\mathbf{H}^{\\frac{1}{2}}\\mathbf{T}^{*}\\mathbf{H}^{\\frac{1}{2}}\\right)$ , we recall $\\Gamma^{*}=\\Psi\\mathbf{H}^{\\frac{1}{2}}\\Omega^{-1}\\mathbf{H}^{-\\frac{1}{2}}$ and obtain ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbf{H}^{\\frac{1}{2}}\\mathbf{P}^{*}\\mathbf{H}^{\\frac{1}{2}}=\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}^{\\frac{1}{2}}\\boldsymbol{\\Omega}^{-1}\\mathbf{H}^{-\\frac{1}{2}}\\mathbf{H}^{\\frac{1}{2}}=\\boldsymbol{\\Omega}^{-1}\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}^{\\frac{1}{2}}\\mathbf{H}^{-\\frac{1}{2}}\\mathbf{H}^{\\frac{1}{2}}}&{\\ (\\boldsymbol{\\Omega}\\mathrm{~and~}\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}^{\\frac{1}{2}}\\mathrm{~commute})}\\\\ {=\\boldsymbol{\\Omega}^{-1}\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}^{\\frac{1}{2}}.}&{\\ (\\mathbf{H}^{\\frac{1}{2}}\\mathbf{H}^{-\\frac{1}{2}}\\mathbf{H}^{\\frac{1}{2}}=\\mathbf{H}^{\\frac{1}{2}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Since $\\Omega^{-1}$ $\\mathbf{H}^{\\frac{1}{2}}\\mathbf{H}^{-\\frac{1}{2}}\\mathbf{H}^{\\frac{1}{2}}$ commute, they are simultaneously diagonalizable. The eigenvalues of $\\Omega^{-1}\\mathbf{H}^{\\frac{1}{2}}\\Psi\\mathbf{H}^{\\frac{1}{2}}$ are ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\frac{\\phi_{i}}{\\frac{M+1}{M}\\phi_{i}+\\frac{\\sum_{i}\\phi_{i}+\\sigma^{2}}{M}},\\quad i=1,2,...,d.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Note that, every eigenvalue is upper bounded by 1, so we simply get ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\lambda_{\\operatorname*{max}}\\left(\\mathbf{H}^{\\frac{1}{2}}\\mathbf{T}^{*}\\mathbf{H}^{\\frac{1}{2}}\\right)\\leq1.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Therefore, we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad-\\operatorname{tr}\\left[\\displaystyle\\frac{1}{M}\\left(\\beta-\\beta^{*}\\right)^{\\top}\\mathbf{H}\\left(\\beta-\\beta^{*}\\right)\\cdot\\mathbf{H}\\mathbf{T}^{*}\\mathbf{H}\\cdot\\mathcal{P}_{\\mathcal{Z}}\\left(\\mathbf{I}-\\mathbf{T}^{*}\\right)^{\\top}\\right]}\\\\ &{\\quad\\le\\displaystyle\\frac{\\sqrt{d}\\lambda_{1}^{2}}{M}\\exp\\left(\\displaystyle\\frac{-2\\lambda_{-1}t}{M+1}\\right)\\left\\|\\beta(0)-\\beta^{*}\\right\\|_{2}^{2}\\cdot\\left\\|\\mathcal{P}_{\\mathcal{Z}}\\left(\\mathbf{I}-\\mathbf{T}^{*}\\right)\\right\\|_{F}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Similarly, we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\quad-\\operatorname{tr}\\left[\\frac{1}{M}\\mathbf{H}\\mathbf{T}^{*}\\mathbf{H}\\left(\\beta-\\beta^{*}\\right)\\left(\\beta-\\beta^{*}\\right)^{\\top}\\mathbf{H}\\cdot\\mathcal{P}_{Z}\\left(\\mathbf{T}-\\mathbf{T}^{*}\\right)^{\\top}\\right]}&\\\\ &{\\leq\\frac{\\sqrt{d}}{M}\\left\\|\\mathcal{P}_{Z}\\left(\\mathbf{T}-\\mathbf{T}^{*}\\right)^{\\top}\\right\\|_{F}\\left\\|\\mathbf{H}^{\\frac{1}{2}}\\left(\\beta-\\beta^{*}\\right)\\left(\\beta-\\beta^{*}\\right)^{\\top}\\mathbf{H}^{\\frac{1}{2}}\\right\\|_{F}\\left\\|\\mathbf{H}\\right\\|_{F}\\left\\|\\mathbf{H}^{\\frac{1}{2}}\\mathbf{T}^{*}\\mathbf{H}^{\\frac{1}{2}}\\right\\|_{2}\\ (\\mathrm{Lemma~H}.1)}\\\\ &{\\leq\\frac{\\sqrt{d}\\lambda_{1}^{2}}{M}\\exp\\left(\\frac{-2\\lambda_{-1}t}{M+1}\\right)\\left\\|\\beta(0)-\\beta^{*}\\right\\|_{2}^{2}\\cdot\\left\\|\\mathcal{P}_{Z}\\left(\\mathbf{T}-\\mathbf{T}^{*}\\right)\\right\\|_{F},}&{\\mathrm{(G.19)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "and ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathrm{tr}\\left[\\frac{1}{M}{\\bf H}^{\\frac{1}{2}}{\\bf\\Omega}^{-1}\\left({\\bf H}^{\\frac{1}{2}}\\Psi{\\bf H}^{\\frac{1}{2}}+\\left(\\mathrm{tr}\\left({\\bf H}^{\\frac{1}{2}}\\Psi{\\bf H}^{\\frac{1}{2}}\\right)+\\sigma^{2}\\right){\\bf I}_{d}\\right)\\,{\\bf H}^{\\frac{1}{2}}\\left(\\beta-\\beta^{*}\\right)\\left(\\beta-\\beta^{*}\\right)^{\\top}{\\bf H}\\cdot\\mathcal{P}_{Z}\\left({\\bf r}\\left({\\bf r}\\left({\\bf r}-\\beta^{*}\\right)\\right)\\right)\\right]}\\\\ &{\\le\\frac{\\sqrt{d}}{M}\\left\\|\\Omega^{-1}\\left({\\bf H}^{\\frac{1}{2}}\\Psi{\\bf H}^{\\frac{1}{2}}+\\left(\\mathrm{tr}\\left({\\bf H}^{\\frac{1}{2}}\\Psi{\\bf H}^{\\frac{1}{2}}\\right)+\\sigma^{2}\\right){\\bf I}_{d}\\right)\\right\\|_{2}\\left\\|\\bf H^{\\frac{1}{2}}\\left(\\beta-\\beta^{*}\\right)\\left(\\beta-\\beta^{*}\\right)^{\\top}{\\bf H}\\cdot\\mathcal{P}_{Z}\\left({\\bf r}-\\beta^{*}\\right)\\right\\|_{2}}\\\\ &{\\quad\\ \\ \\ \\ \\ \\mathrm{(Lemma~H.1}}\\\\ &{\\le\\frac{\\sqrt{d}}{M}\\cdot M\\cdot\\left\\|{\\bf H}^{\\frac{1}{2}}\\left(\\beta-\\beta^{*}\\right)\\left(\\beta-\\beta^{*}\\right)^{\\top}{\\bf H}^{\\frac{1}{2}}\\right\\|_{F}\\cdot\\|{\\bf H}\\|_{F}\\cdot\\|\\mathcal{P}z\\left({\\bf r}-{\\bf r}^{*}\\right)\\|_{F}}\\\\ &{\\le\\sqrt{d}\\lambda_{1}^{2}\\exp\\left(\\frac{-2\\lambda_{-1}t}{M+1}\\right)\\|\\beta(0)-\\beta^{*}\\|_{2}^{2}\\cdot\\|\\mathcal{P}z\\left({\\bf r}-{\\bf r}^{*}\\right)\\|_{F}}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Bridging (G.18), (G.19) and (G.20) into (G.16), we get ", "page_idx": 38}, {"type": "equation", "text": "$$\nG_{3}\\leq\\left(2+{\\frac{2}{M}}\\right)\\lambda_{1}^{2}\\exp\\left({\\frac{-2\\lambda_{-1}t}{M+1}}\\right)\\|\\beta(0)-\\beta^{*}\\|_{2}^{2}\\cdot\\|{\\mathcal{P}}_{\\mathcal{Z}}\\left(\\mathbf{Gamma}-\\mathbf{T}^{*}\\right)\\|_{F}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Finally, we finish this section by proving the convergence of $\\mathbf{T}$ ", "page_idx": 38}, {"type": "text", "text": "Lemma G.6 (Convergence of $\\mathbf{T}$ ). Under the dynamical system (G.4) and (G.5), one has ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{P}_{\\mathcal{Z}}\\left(\\mathbf{T}(t)\\right)\\rightarrow\\mathcal{P}_{\\mathcal{Z}}\\left(\\mathbf{T}^{*}\\right),\\quad\\mathcal{P}_{\\mathcal{Z}^{\\perp}}\\left(\\mathbf{T}(t)\\right)=\\mathcal{P}_{\\mathcal{Z}^{\\perp}}\\left(\\mathbf{T}(0)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "when $t\\to\\infty$ , from arbitrary initialization $\\beta(0)$ and $\\mathbf{\\delta}\\Gamma(0)$ ", "page_idx": 38}, {"type": "text", "text": "Proof. We observe that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t}\\left[\\frac{1}{2}\\left\\|\\mathcal{P}_{\\mathcal{Z}}\\left(\\mathbf{Gamma}-\\mathbf{Gamma}^{*}\\right)\\right\\|_{F}^{2}\\right]=\\left\\|\\mathcal{P}_{\\mathcal{Z}}\\left(\\mathbf{Gamma}-\\mathbf{Gamma}^{*}\\right)\\right\\|_{F}\\cdot\\frac{\\mathrm{d}}{\\mathrm{d}t}\\left\\|\\mathcal{P}_{\\mathcal{Z}}\\left(\\mathbf{Gamma}-\\mathbf{Gamma}^{*}\\right)\\right\\|_{F}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Combining it with Lemma G.5, we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t}\\left\\|\\mathcal{P}z\\left(\\mathbf{r}-\\mathbf{r}^{*}\\right)\\right\\|_{F}\\leq-A_{1}\\left\\|\\mathcal{P}z\\left(\\mathbf{r}-\\mathbf{r}^{*}\\right)\\right\\|_{F}+A_{2}\\exp\\left(\\frac{-2\\lambda_{-1}t}{M+1}\\right),\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where $A_{1}>0,A_{2}>0$ are defined in (G.14). Simple calculation shows that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t}\\left[\\exp\\left(A_{1}t\\right)\\left\\Vert\\mathcal{P}_{\\mathcal{Z}}\\left(\\mathbf{r}-\\mathbf{r}^{*}\\right)\\right\\Vert_{F}\\right]\\leq A_{2}\\exp\\left[\\left(A_{1}-\\frac{2\\lambda_{-1}}{M+1}\\right)t\\right].\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Wheg $\\begin{array}{r}{A_{1}\\neq\\frac{2\\lambda_{-1}}{M+1}}\\end{array}$ $t=0$ 10 $t=T$ $\\exp\\left(A_{1}T\\right)$ ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{P}z\\left(\\mathbf{r}(T)-\\mathbf{r}^{*}\\right)\\Vert_{F}\\leq\\frac{\\Vert\\mathcal{P}z\\left(\\mathbf{r}(0)-\\mathbf{r}^{*}\\right)\\Vert_{F}}{\\exp\\left(A_{1}T\\right)}+\\frac{A_{2}}{A_{1}-\\frac{2\\lambda_{-1}}{M+1}}\\cdot\\frac{\\exp\\left[\\left(A_{1}-\\frac{2\\lambda_{-1}}{M+1}\\right)T\\right]-1}{\\exp\\left(A_{1}T\\right)}}\\\\ &{\\qquad\\qquad=\\frac{\\Vert\\mathcal{P}z\\left(\\mathbf{r}(0)-\\mathbf{r}^{*}\\right)\\Vert_{F}}{\\exp\\left(A_{1}T\\right)}+\\frac{A_{2}}{A_{1}-\\frac{2\\lambda_{-1}}{M+1}}\\cdot\\left[\\exp\\left(-\\frac{2\\lambda_{-1}T}{M+1}\\right)-\\exp\\left(-A_{1}T\\right)\\right]\\rightarrow}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "$T\\ \\rightarrow\\ \\infty$ 0. $\\begin{array}{r c l}{A_{1}}&{=}&{\\frac{2\\lambda_{-1}}{M+1}}\\end{array}$ $\\exp\\left(A_{1}t\\right)\\|\\mathcal{P}_{\\mathcal{Z}}\\left(\\boldsymbol{\\Gamma}(t)-\\boldsymbol{\\Gamma}^{*}\\right)\\|_{F}$   \n$\\|\\mathcal{P}_{\\mathcal{Z}}\\left(\\mathbf{{T}}(t)-\\mathbf{{T}}^{*}\\right)\\|_{F}\\to0$ when $t\\to\\infty$ Together, in all cases, we have $\\|\\mathcal{P}_{\\mathcal{Z}}\\left(\\mathbf{{T}}(t)-\\mathbf{{T}^{*}}\\right)\\|_{F}\\stackrel{}{\\rightarrow}0$   \nFrom (G.9), we soon get $\\mathcal{P}_{\\mathcal{Z}}\\left(\\Gamma(t)\\right)=\\mathcal{P}_{\\mathcal{Z}}\\left(\\Gamma(0)\\right)$ foo all $t>0$ . Therefore, we conclude. \u53e3 ", "page_idx": 39}, {"type": "text", "text": "The Theorem 6.3 is proved by simplly combining Lemma G.3 and Lemma G.6. ", "page_idx": 39}, {"type": "text", "text": "H Technical lemmas ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Lemma H.1 (Von-Neumann's Trace Inequality). Let $U,V\\in\\mathbb{R}^{d\\times n}$ with $d\\leq n$ Wehave ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\mathrm{tr}\\left(U^{\\top}V\\right)\\leq\\sum_{i=1}^{d}\\sigma_{i}(U)\\sigma_{i}(V)\\leq\\|U\\|_{\\mathrm{op}}\\times\\sum_{i=1}^{d}\\sigma_{i}(V)\\leq\\sqrt{d}\\cdot\\|U\\|_{\\mathrm{op}}\\|V\\|_{F}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where $\\sigma_{1}(X)\\geq\\sigma_{2}(X)\\geq\\cdot\\cdot\\geq\\sigma_{d}(X)$ are the ordered singular values of $X\\in\\mathbb{R}^{d\\times n}$ ", "page_idx": 40}, {"type": "text", "text": "Lemma H.2 ([22]). For any two positive semi-definite matrices $A,B\\in\\mathbb{R}^{d\\times d}$ $w e$ have ", "page_idx": 40}, {"type": "text", "text": "$\\operatorname{tr}[A B]\\geq0$ $A B\\succeq0$ if and only if $A$ and $B$ commute. ", "page_idx": 40}, {"type": "text", "text": "Lemma H.3 (Derivatives, [27]). We denote ${\\bf A},{\\bf B},{\\bf C},{\\bf D},{\\bf X}$ as matrices and a, b, x as vectors. Then, wehave ", "page_idx": 40}, {"type": "text", "text": "tr [xTBxC] =BXC +BTxCT.   \n$\\begin{array}{r}{\\frac{\\partial\\mathbf{x}^{\\top}\\mathbf{B}\\mathbf{x}}{\\partial\\mathbf{x}}=\\left(\\mathbf{B}+\\mathbf{B}^{\\top}\\right)\\mathbf{x}.}\\end{array}$   \n$\\begin{array}{r}{\\frac{\\partial\\mathbf{a}^{\\top}\\mathbf{X}\\mathbf{b}}{\\partial\\mathbf{X}}=\\mathbf{a}\\mathbf{b}^{\\top}}\\end{array}$   \n$\\begin{array}{r}{\\frac{\\partial\\mathbf{a}^{\\top}\\mathbf{X}^{\\top}\\mathbf{b}}{\\partial\\mathbf{X}}=\\mathbf{b}\\mathbf{a}^{\\top}}\\end{array}$   \n$\\begin{array}{r}{\\frac{\\partial{\\bf b}^{\\top}{\\bf X}^{\\top}{\\bf D}{\\bf X}{\\bf c}}{\\partial{\\bf X}}={\\bf D}^{\\top}{\\bf X}{\\bf b}{\\bf c}^{\\top}+{\\bf D}{\\bf X}{\\bf c}{\\bf b}^{\\top}.}\\end{array}$   \n$\\begin{array}{r}{\\mathbf{\\dot{\\Sigma}}\\frac{\\partial}{\\partial\\mathbf{X}}\\mathrm{tr}\\left[(\\mathbf{AXB}+\\mathbf{C})(\\mathbf{AXB}+\\mathbf{C})^{\\top}\\right]=2\\mathbf{A}^{\\top}(\\mathbf{AXB}+\\mathbf{C})\\mathbf{B}^{\\top}\\mathbf{\\Sigma}}\\end{array}$ ", "page_idx": 40}, {"type": "text", "text": "Lemma H.4 (Lemma D.2 in [38], Lemma 4.2 in [37]), If x is Gaussian random vector of d dimension, meanzero and covariancematrix $\\mathbf{H}$ and $A\\in\\mathbb{R}^{d\\times d}$ is a fixedmatrix.Then ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\mathbf{xx}^{\\top}A\\mathbf{xx}^{\\top}\\right]=\\mathbf{H}\\left(\\mathbf{A}+\\mathbf{A}^{\\top}\\right)\\mathbf{H}+\\operatorname{tr}(A\\mathbf{H})\\mathbf{H}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "f $\\pmb{A}$ is symmetric and the rows in $\\mathbf{X}\\in\\mathbb{R}^{M\\times d}$ are generated independentlyfrom ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\mathbf{X}[i]\\sim{\\mathcal{N}}(0,\\mathbf{H}),\\quad i=1,\\ldots,M.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Then, it holds that ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\mathbf{X}^{\\top}\\mathbf{X}A\\mathbf{X}^{\\top}\\mathbf{X}\\right]=M\\cdot\\operatorname{tr}\\left(\\mathbf{H}A\\right)\\cdot\\mathbf{H}+M(M+1)\\cdot\\mathbf{H}A\\mathbf{H}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Lemma H.5 (Linear Algebra). Suppose $\\mathbf{H}$ is a (non-zero) positive semi-definite matrix in $\\mathbb{R}^{d\\times d}$ and $\\mathbf{H}^{\\frac{1}{2}}$ is its principle square root. We denote $\\lambda_{-1}$ as the minimal non-zero eigenvector of $\\mathbf{H}$ Then, we have ", "page_idx": 40}, {"type": "text", "text": "$\\mathbf{\\mu}^{\\bullet}\\,\\,l.\\,\\,\\mathsf{n u l l}\\left(\\mathbf{H}\\right)=\\mathsf{n u l l}\\left(\\mathbf{H}^{\\frac{1}{2}}\\right),\\mathsf{l m}\\left(\\mathbf{H}\\right)=\\mathsf{l m}\\left(\\mathbf{H}^{\\frac{1}{2}}\\right).$   \n\u00b7 2. $\\mathbf{H}\\mathbf{H}^{+}=\\mathbf{H}^{+}\\mathbf{H}$ where $(\\cdot)^{+}$ denotes the Moore-Penrose pseudo-inverse.   \n\u00b7 3. For any vector $\\alpha\\in\\mathbb{R}^{d}$ , the orthogonal projection operator on $\\mathsf{I m}\\left(\\mathbf{H}\\right)$ and null $(\\mathbf{H})$ are respectively $\\mathcal{P}_{\\mathrm{lm}(\\mathbf{H})}\\left(\\alpha\\right)=\\mathbf{H}\\mathbf{H}^{+}\\alpha=\\mathbf{H}^{+}\\mathbf{H}\\alpha,\\quad\\mathcal{P}_{\\mathrm{null}(\\mathbf{H})}\\left(\\alpha\\right)=\\left(\\mathbf{I}_{d}-\\mathbf{H}\\mathbf{H}^{+}\\right)\\alpha=\\left(\\mathbf{I}_{d}-\\mathbf{H}^{+}\\mathbf{H}\\right)\\alpha.$   \n. 4. For any vector $\\alpha\\in\\mathbb{R}^{d}$ , we have $\\mathcal{P}_{\\sf l m(H)}\\left(\\alpha\\right)^{\\top}\\mathbf{H}\\mathcal{P}_{\\sf l m(H)}\\left(\\alpha\\right)\\geq\\lambda_{-1}\\left\\Vert\\mathcal{P}_{\\sf l m(H)}\\left(\\alpha\\right)\\right\\Vert_{2}^{2}$ ", "page_idx": 40}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Proof. We consider the eigen-decomposition of matrix $\\mathbf{H}$ , which is ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathbf{H}=\\mathbf{Q}D\\mathbf{Q}^{\\top},\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Wwhere $\\boldsymbol{D}=\\operatorname{diag}\\left(\\lambda_{1},\\lambda_{2},...,\\lambda_{d}\\right)$ is a diagonal matrix with diagonal entries being the eigenvalues of $\\mathbf{H}$ ,and $\\mathbf{Q}$ is an orthogonal matrix. Then, from the definition of principle square root, we know ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathbf{H}^{\\frac{1}{2}}=\\mathbf{Q}D^{\\frac{1}{2}}\\mathbf{Q}^{\\top},\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where $D^{\\frac{1}{2}}=\\mathrm{diag}\\left(\\sqrt{\\lambda_{1}},\\sqrt{\\lambda_{2}},...,\\sqrt{\\lambda_{d}}\\right)$ . We denote columns of $\\mathbf{Q}$ $\\mathbf{q}_{1},\\ldots\\mathbf{q}_{d}$ . We know they are the eigenvectors of $\\mathbf{H}$ . From the eigen-decomposition of $\\mathbf{H}$ and $\\mathbf{H}^{\\frac{1}{2}}$ , we know they share the same set of eigenvectors. Without loss of generality, we assume $\\mathsf{r a n k}(\\mathbf{H})=r$ and $\\lambda_{1}\\geq\\lambda_{2}\\geq...\\geq\\lambda_{r}>$ $0,\\lambda_{i}=0,i=r+1,...,d$ Then, we denote $\\mathcal{H}$ as the linear vector space spanned by $\\mathbf{q}_{1},...,\\mathbf{q}_{r}$ and $\\mathcal{H}^{\\perp}$ as its orthogonal complement (which is the subspace spanned by $\\mathbf{q}_{r+1},...,\\mathbf{q}_{d})$ . For any vector $\\alpha\\in\\mathbb{R}^{d}$ , we can write is orthogonal decomposition as $\\textstyle\\alpha=\\sum_{i=1}^{d}a_{i}\\mathbf{q}_{i}$ , o we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathbf{H}\\alpha=\\sum_{i=1}^{d}a_{i}\\mathbf{H}\\mathbf{q}_{i}=\\sum_{i=1}^{r}a_{i}\\lambda_{i}\\mathbf{q}_{i}\\in\\mathcal{H},\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "which implies $\\mathfrak{m}\\left(\\mathbf{H}\\right)\\;\\subset\\;\\mathcal{H}$ . On the other hand, we know for any $\\alpha\\,\\in\\,{\\mathcal{H}}$ , we can write it as $\\begin{array}{r}{\\alpha=\\sum_{i=1}^{r}a_{i}\\mathbf{q}_{i}}\\end{array}$ for some $a_{1},...,a_{r}$ , then we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\alpha=\\sum_{i=1}^{r}\\frac{a_{i}}{\\lambda_{i}}\\cdot\\lambda_{i}\\mathbf{q}_{i}=\\mathbf{H}\\left(\\sum_{i=1}^{r}\\frac{a_{i}}{\\lambda_{i}}\\mathbf{q}_{i}\\right)\\in\\mathsf{I m}\\left(\\mathbf{H}\\right),\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "which implies ${\\mathcal{H}}\\subset\\mathsf{I m}\\left(\\mathbf{H}\\right)$ . This shows ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathsf{I m}\\left(\\mathbf{H}\\right)=\\mathcal{H}=\\mathsf{s p a n}\\left\\{\\mathbf{q}_{1},...\\mathbf{q}_{r}\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "and ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathsf{n u l l}\\left(\\mathbf{H}\\right)=\\mathcal{H}=\\mathsf{s p a n}\\left\\{\\mathbf{q}_{r+1},...\\mathbf{q}_{d}\\right\\},\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "since null $(\\mathbf{H})$ is the orthogonal complement of $\\mathsf{I m}\\left(\\mathbf{H}\\right)$ . Then, (1) is proved by noticing that $\\mathbf{H}$ and $\\mathbf{H}^{\\frac{1}{2}}$ share the same set of eigenvectors. To prove (2), it suffces to notice that ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathbf{H}\\mathbf{H}^{+}=\\mathbf{Q}\\cdot\\mathrm{diag}(\\underbrace{1,1,...,1}_{\\mathrm{r}},0,0,..,0)\\cdot\\mathbf{Q}^{\\top}=\\mathbf{H}^{+}\\mathbf{H}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "To prove (3), it suffices to notice that actually $\\mathbf{H}\\mathbf{H}^{+}\\alpha\\in\\mathsf{I m}\\left(\\mathbf{H}\\right)$ and ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\left\\langle\\mathrm{HH}^{+}\\alpha,\\left(\\mathrm{I}_{d}-\\mathrm{HH}^{+}\\right)\\alpha\\right\\rangle=\\left\\langle\\mathrm{H}^{+}\\mathrm{H}\\alpha,\\left(\\mathrm{I}_{d}-\\mathrm{HH}^{+}\\right)\\alpha\\right\\rangle=\\alpha^{\\top}\\left(\\mathrm{HH}^{+}-\\mathrm{HH}^{+}\\mathrm{HH}^{+}\\right)\\alpha=0.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Finally, to prove (4), we can write $\\textstyle\\alpha=\\sum_{i=1}^{d}a_{i}\\mathbf{q}_{i}$ for some $a_{1},...,a_{d}$ . Then, by orthogonality we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{\\mathcal{I}}_{\\mathrm{lm(\\mathbf{H})}}\\left(\\boldsymbol{\\alpha}\\right)^{\\top}\\mathbf{H}\\mathcal{P}_{\\mathrm{lm(\\mathbf{H})}}\\left(\\boldsymbol{\\alpha}\\right)=\\left(\\displaystyle\\sum_{i=1}^{d}a_{i}\\mathbf{q}_{i}\\right)^{\\top}\\mathbf{H}\\left(\\displaystyle\\sum_{i=1}^{d}a_{i}\\mathbf{q}_{i}\\right)=\\sum_{i=1}^{r}a_{i}^{2}\\lambda_{i}\\mathbf{q}_{i}^{\\top}\\mathbf{q}_{i}\\geq\\lambda_{-1}\\cdot\\sum_{i=1}^{r}a_{i}^{2}\\mathbf{q}_{i}^{\\top}\\mathbf{q}_{i}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\lambda_{-1}\\left\\|\\mathcal{P}_{\\mathrm{lm(\\mathbf{H})}}\\left(\\boldsymbol{\\alpha}\\right)\\right\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Therefore, we conclude. ", "page_idx": 41}, {"type": "text", "text": "Lemma H.6 (Tensor product). Suppose $\\mathbf{H}$ is a (non-zero) positive semi-defnite matrix in $\\mathbb{R}^{d\\times d}$ and $\\mathbf{H}^{\\frac{1}{2}}$ is its principle square root. We denote $\\otimes$ as Kronecker product, which is defined as ", "page_idx": 41}, {"type": "equation", "text": "$$\n(A\\otimes B)\\circ C=B C A^{\\top}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "We define ", "page_idx": 41}, {"type": "text", "text": "\u4e00 $\\begin{array}{r}{\\mathsf{m}\\left(\\mathbf{H}\\otimes\\mathbf{H}\\right):=\\left\\{\\left(\\mathbf{H}\\otimes\\mathbf{H}\\right)\\circ Z:Z\\in\\mathbb{R}^{d\\times d}\\right\\},\\quad\\mathsf{n u l l}\\left(\\mathbf{H}\\otimes\\mathbf{H}\\right):=\\left\\{Z\\in\\mathbb{R}^{d\\times d}:\\left(\\mathbf{H}\\otimes\\mathbf{H}\\right)\\circ Z=\\mathbf{0}\\right\\}}\\end{array}$ and define ${\\sf I m}\\left({\\bf H}^{\\frac{1}{2}}\\otimes{\\bf H}^{\\frac{1}{2}}\\right)$ and null $\\left(\\mathbf{H}^{\\frac{1}{2}}\\otimes\\mathbf{H}^{\\frac{1}{2}}\\right)$ similarly. Then, we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\bullet\\,\\,l.\\,\\,\\mathrm{lm}\\left(\\mathbf{H}^{\\frac{1}{2}}\\otimes\\mathbf{H}^{\\frac{1}{2}}\\right)=\\mathrm{lm}\\left(\\mathbf{H}\\otimes\\mathbf{H}\\right),\\mathrm{null}\\left(\\mathbf{H}^{\\frac{1}{2}}\\otimes\\mathbf{H}^{\\frac{1}{2}}\\right)=\\mathrm{null}\\left(\\mathbf{H}\\otimes\\mathbf{H}\\right).\n$$", "text_format": "latex", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\mathcal{P}_{\\mathcal{Z}}\\left(\\boldsymbol{Z}\\right)=\\left(\\mathbf{H}\\mathbf{H}^{+}\\right)^{\\otimes2}\\circ\\boldsymbol{Z}=\\mathbf{H}\\mathbf{H}^{+}\\boldsymbol{Z}\\mathbf{H}^{+}\\mathbf{H}=\\mathbf{H}^{\\frac{1}{2}}\\mathbf{H}^{-\\frac{1}{2}}\\boldsymbol{Z}\\mathbf{H}^{-\\frac{1}{2}}\\mathbf{H}^{\\frac{1}{2}}\\,}&{(\\mathrm{H}.3)}\\\\ &{\\mathcal{P}_{\\mathcal{Z}^{\\perp}}\\left(\\boldsymbol{Z}\\right)=\\left[\\mathbf{I}_{d}^{\\otimes2}-\\left(\\mathbf{H}\\mathbf{H}^{+}\\right)^{\\otimes2}\\right]\\circ\\boldsymbol{Z}=\\boldsymbol{Z}-\\mathbf{H}\\mathbf{H}^{+}\\boldsymbol{Z}\\mathbf{H}^{+}\\mathbf{H}=\\boldsymbol{Z}-\\mathbf{H}^{\\frac{1}{2}}\\mathbf{H}^{-\\frac{1}{2}}\\boldsymbol{Z}\\mathbf{H}^{-\\frac{1}{2}}\\mathbf{H}^{\\frac{1}{2}}.}&\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "\u00b7 3. For any matrix $Z\\in\\mathbb{R}^{d\\times d}$ , it holds that ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\left(\\left(\\mathbf{H}^{\\frac{1}{2}}\\otimes\\mathbf{H}^{\\frac{1}{2}}\\right)\\circ\\mathcal{P}_{\\mathcal{Z}}\\left(\\pmb{Z}\\right)\\right)\\cdot\\left(\\left(\\mathbf{H}^{\\frac{1}{2}}\\otimes\\mathbf{H}^{\\frac{1}{2}}\\right)\\circ\\mathcal{P}_{\\mathcal{Z}}\\left(\\pmb{Z}\\right)\\right)^{\\top}\\succeq\\lambda_{-1}^{2}\\mathcal{P}_{\\mathcal{Z}}\\left(\\pmb{Z}\\right)\\cdot\\mathcal{P}_{\\mathcal{Z}}\\left(\\pmb{Z}\\right)^{\\top},\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where $\\lambda_{-1}$ is the minimal positive eigenvector of $\\mathbf{H}$ ", "page_idx": 42}, {"type": "text", "text": "Proof. Let's first prove the second part. From the definition of tensor product and the fact that $\\mathbf{H}\\mathbf{H}^{+}=\\mathbf{H}^{+}\\mathbf{H}$ (see Lemma H.5), we know the second equation in H.3 holds. To prove the first equation, it sufces to notice that for any matrix $\\mathbf{Z}\\in\\mathbb{R}^{d\\times d}$ , the matrix $\\mathbf{H}\\mathbf{H}^{+}Z\\mathbf{H}^{+}\\mathbf{H}$ isin $\\mathsf{l m}\\left(\\mathbf{H}^{\\otimes2}\\right)$ together with the fact that ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{H}\\mathbf{H}^{+}Z\\mathbf{H}^{+}\\mathbf{H},Z-\\mathbf{H}\\mathbf{H}^{+}Z\\mathbf{H}^{+}\\mathbf{H}\\right\\rangle=\\mathrm{tr}\\left(\\mathbf{H}\\mathbf{H}^{+}Z\\mathbf{H}^{+}\\mathbf{H}Z^{\\top}\\right)-\\mathrm{tr}\\left(\\mathbf{H}\\mathbf{H}^{+}Z\\mathbf{H}^{+}\\mathbf{H}\\mathbf{H}\\mathbf{H}^{+}Z^{\\top}\\mathbf{H}^{+}\\mathbf{H}\\mathbf{H}^{-}\\mathbf{H}\\right)}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ =\\mathrm{tr}\\left(\\mathbf{H}\\mathbf{H}^{+}Z\\mathbf{H}^{+}\\mathbf{H}Z^{\\top}\\right)-\\mathrm{tr}\\left(\\mathbf{H}\\mathbf{H}^{+}\\mathbf{H}\\mathbf{H}^{+}Z\\mathbf{H}^{+}\\mathbf{H}\\mathbf{H}^{+}\\mathbf{H}Z^{\\top}\\right)}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\mathrm{(H\\mathbf{H}^{+}=H^{+}H)}}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ =0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "The proof of (H.4) is similar. Then, from Lemma H.5, we know $\\mathsf{I m}\\left(\\mathbf{H}\\right)\\ =\\ \\mathsf{I m}\\left(\\mathbf{H}^{\\frac{1}{2}}\\right)$ which implies the projection operator onto those two subspace are identical, indicating ${\\bf H}{\\bf H}^{+}={\\bf H}^{\\frac{1}{2}}{\\bf H}^{-\\frac{1}{2}}$ Therefore, we have ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathsf{D}_{\\mathrm{lm}(\\mathbf{H}\\otimes\\mathbf{H})}\\left(Z\\right)=\\left(\\mathbf{H}\\mathbf{H}^{+}\\right)^{\\otimes2}\\circ Z=\\mathbf{H}\\mathbf{H}^{+}Z\\mathbf{H}^{+}\\mathbf{H}=\\mathbf{H}^{\\frac{1}{2}}\\mathbf{H}^{-\\frac{1}{2}}Z\\mathbf{H}^{-\\frac{1}{2}}\\mathbf{H}^{\\frac{1}{2}}=\\mathcal{P}_{\\mathrm{lm}}\\!\\left(\\mathbf{H}^{\\frac{1}{2}}\\otimes\\mathbf{H}^{\\frac{1}{2}}\\right)\\left(Z\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Since this holds for any matrix $\\textbf{Z}\\in\\mathbb{R}^{d\\times d}$ , this suggests $\\mathsf{l m}\\left(\\mathbf{H}^{\\frac{1}{2}}\\otimes\\mathbf{H}^{\\frac{1}{2}}\\right)\\,=\\,\\mathsf{l m}\\left(\\mathbf{H}\\otimes\\mathbf{H}\\right)$ Similarly, we also have null $\\left(\\mathbf{H}^{\\frac{1}{2}}\\otimes\\mathbf{H}^{\\frac{1}{2}}\\right)\\,=\\,\\mathsf{n u l l}\\left(\\mathbf{H}\\otimes\\mathbf{H}\\right).$ Finally, to prove (3), we use the eigendecomposition of $\\mathbf{H}$ and $\\mathbf{H}^{\\frac{1}{2}}$ \uff1a ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathbf{H}=\\mathbf{Q}D\\mathbf{Q}^{\\top},\\quad\\mathbf{H}^{\\frac{1}{2}}=\\mathbf{Q}D^{\\frac{1}{2}}\\mathbf{Q}^{\\top},\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where $\\mathbf{Q}\\;=\\;(\\mathbf{q}_{1}\\;\\mathbf{q}_{2}\\;\\ldots\\;\\mathbf{q}_{d})$ is an orthogonal matrix and ${\\cal D}\\;=\\;\\mathrm{diag}\\left(\\lambda_{1},....,\\lambda_{d}\\right),$ where $\\lambda_{1}~\\geq$ $\\lambda_{2}~\\ge~....\\lambda_{d}~\\ge~0$ are ordered eigenvalues. We assume $\\mathsf{r a n k}(\\mathbf{H})\\;=\\;r$ and $\\lambda_{r}~>~0~=~\\lambda_{r+1}$ Then, from the property of Kronecker product (eg. see [27]), the eigenvalues of $\\mathbf{H}^{\\frac{1}{2}}\\otimes\\mathbf{H}^{\\frac{1}{2}}$ are $\\left\\{{\\sqrt{\\lambda_{i}\\lambda_{j}}}:1\\leq i,j\\leq d\\right\\}$ . The eigenvectors are $\\{\\mathbf{q}_{i}\\otimes\\mathbf{q}_{j}:1\\leq i,j\\leq d\\}$ , which forms an orthogonal unit basis of $\\mathbb{R}^{d\\times d}$ . We define ", "page_idx": 42}, {"type": "equation", "text": "$$\nS:=\\mathsf{s p a n}\\left\\{\\mathbf{q}_{i}\\otimes\\mathbf{q}_{j}:1\\leq i,j\\leq r\\right\\}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "as the eigenspace spanned by all eigenvectors corresponding to positive eigenvalues. For any matrix $Z$ ,wecan decompose it as ", "page_idx": 42}, {"type": "equation", "text": "$$\nZ=\\sum_{i,j}a_{i,j}\\cdot{\\bf q}_{i}\\otimes{\\bf q}_{j},\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "", "page_idx": 42}, {"type": "equation", "text": "$$\n(\\mathbf{H}\\otimes\\mathbf{H})\\circ Z=\\sum_{i,j=1}^{d}a_{i,j}\\left(\\mathbf{H}\\otimes\\mathbf{H}\\right)\\circ(\\mathbf{q}_{i}\\otimes\\mathbf{q}_{j})=\\sum_{i,j}\\left(\\mathbf{H}\\mathbf{q}_{i}\\right)\\otimes\\left(\\mathbf{H}\\mathbf{q}_{j}\\right)\n$$", "text_format": "latex", "page_idx": 42}, {"type": "equation", "text": "$$\n=\\sum_{i,j=1}^{d}a_{i,j}\\lambda_{i}\\lambda_{j}{\\bf q}_{i}\\otimes{\\bf q}_{j}=\\sum_{i,j=1}^{r}a_{i,j}\\lambda_{i}\\lambda_{j}{\\bf q}_{i}\\otimes{\\bf q}_{j}\\in\\mathcal{S},\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "which implies ${\\mathcal{Z}}\\subset S$ On the other hand, for any $\\mathbf{Z}\\in S$ , we can write it as $\\begin{array}{r}{Z=\\sum_{i,j=1}^{r}b_{i,j}\\cdot\\mathbf{q}_{i}\\otimes\\mathbf{q}_{j}}\\end{array}$ so we have ", "page_idx": 43}, {"type": "equation", "text": "$$\n:\\sum_{i,j=1}^{r}\\frac{b_{i,j}}{\\lambda_{i}\\lambda_{j}}\\cdot(\\lambda_{i}\\mathbf{q}_{i})\\otimes(\\lambda_{j}\\mathbf{q}_{j})=\\sum_{i,j=1}^{r}\\frac{b_{i,j}}{\\lambda_{i}\\lambda_{j}}\\cdot(\\mathbf{H}\\mathbf{q}_{i})\\otimes(\\mathbf{H}\\mathbf{q}_{j})=(\\mathbf{H}\\otimes\\mathbf{H})\\circ\\left[\\sum_{i,j=1}^{r}\\frac{b_{i,j}}{\\lambda_{i}\\lambda_{j}}\\left(\\mathbf{q}_{i}\\otimes\\mathbf{q}_{j}\\right)\\right]\\in\\mathcal{Z}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "which implies $s\\subset\\mathcal{Z}$ . Combining two directions, we have $s\\,=\\,{\\mathcal{Z}}$ . Therefore, for any matrix $Z\\in\\mathbb{R}^{d\\times d}$ we can write itas $\\begin{array}{r}{Z=\\sum_{i,j=1}^{d}c_{i,j}\\cdot\\mathbf{q}_{i}\\otimes\\mathbf{q}_{j}}\\end{array}$ forsome $c_{i,j}$ Then, we have ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left(\\mathbf{H}^{\\frac{1}{2}}\\otimes\\mathbf{H}^{\\frac{1}{2}}\\right)\\circ\\mathcal{P}_{\\mathcal{Z}}\\left(\\pmb{Z}\\right)=\\left(\\mathbf{H}^{\\frac{1}{2}}\\otimes\\mathbf{H}^{\\frac{1}{2}}\\right)\\circ\\mathcal{P}_{S}\\left(\\pmb{Z}\\right)=\\left(\\mathbf{H}^{\\frac{1}{2}}\\otimes\\mathbf{H}^{\\frac{1}{2}}\\right)\\circ\\displaystyle\\sum_{i,j=1}^{r}c_{i,j}\\cdot\\mathbf{q}_{i}\\otimes\\mathbf{q}_{j}}\\\\ {=\\displaystyle\\sum_{i,j=1}^{r}c_{i,j}\\left(\\mathbf{H}^{\\frac{1}{2}}\\mathbf{q}_{i}\\right)\\otimes\\left(\\mathbf{H}^{\\frac{1}{2}}\\mathbf{q}_{j}\\right)=\\displaystyle\\sum_{i,j=1}^{r}c_{i,j}\\sqrt{\\lambda_{i}\\lambda_{j}}\\cdot\\mathbf{q}_{i}\\otimes\\mathbf{q}_{j}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Therefore, we have ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\left(\\mathbf{H}^{\\frac{1}{2}}\\otimes\\mathbf{H}^{\\frac{1}{2}}\\right)\\circ\\mathcal{P}_{Z}\\left(Z\\right)\\right)\\left(\\left(\\mathbf{H}^{\\frac{1}{2}}\\otimes\\mathbf{H}^{\\frac{1}{2}}\\right)\\circ\\mathcal{P}_{Z}\\left(Z\\right)\\right)^{\\top}}\\\\ &{=\\left(\\displaystyle\\sum_{i,j=1}^{r}c_{i,j}\\sqrt{\\lambda_{i}\\lambda_{j}}\\cdot\\mathbf{q}_{i}\\otimes\\mathbf{q}_{j}\\right)\\left(\\displaystyle\\sum_{k,l=1}^{r}c_{k,l}\\sqrt{\\lambda_{k}\\lambda_{l}}\\cdot\\mathbf{q}_{k}\\otimes\\mathbf{q}_{j}\\right)^{\\top}}\\\\ &{=\\displaystyle\\sum_{i,j=1}^{r}c_{i,j}^{2}\\lambda_{i}\\lambda_{j}\\left(\\mathbf{q}_{i}\\otimes\\mathbf{q}_{j}\\right)\\left(\\mathbf{q}_{i}\\otimes\\mathbf{q}_{j}\\right)^{\\top}}\\\\ &{\\geq\\lambda_{-1}^{2}\\displaystyle\\sum_{i,j=1}^{r}c_{i,j}^{2}\\left(\\mathbf{q}_{i}\\otimes\\mathbf{q}_{j}\\right)\\left(\\mathbf{q}_{i}\\otimes\\mathbf{q}_{j}\\right)^{\\top}}\\\\ &{=\\lambda_{-1}^{2}P_{Z}\\left(Z\\right)\\cdot\\mathcal{P}_{Z}\\left(Z\\right)^{\\top}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Therefore, we conclude. ", "page_idx": 43}, {"type": "text", "text": "1 Experiment Details ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Our experiments on GPT2 mostly follow the setting in [14], except that we use the token matrix defined in (3.1). In our experiments, we use $d=20,M=40,\\Psi=\\mathbf{H}=\\mathbf{I}_{d}$ and $\\sigma=0$ . We train the GPT2 with and without MLP layers on two settings: $\\beta^{*}=(0,0,...,0)^{\\top}$ and $\\beta^{*}=(10,10,...,10)^{\\top}$ For GPT2 with and without MLP layers, we initialize the $\\mathbf{W}_{V}$ and $\\mathbf{W}_{2}$ by normal distribution with standard deviation 0.02/ \u221a/number of residual connections, where the number of residual connections equals the number of layers for GPT2 without MLP layers. For GPT2 with MLP layers, this is 2 times the number of layers. Initialization for other matrices follow the default setting in [36]. We use Adam with learning rate 0.0001. We train the model for 200000 steps and we sample a batch of 256 new tasks for each step. ", "page_idx": 43}, {"type": "text", "text": "J  Does scratchpad help? ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "In this section, we show the limitations of adding a scratchpad to the token matrix. We will show that by using a single LSA layer and the token matrix with scratchpad, one cannot recover the GD- $_{\\cdot\\beta}$ estimator defined in Section 5. We leave it as future work to see whether the token matrix with scratchpad could implement other types of estimators that more effectively address the linear regression tasks defined in Assumption 3.1, as well as whether additional structures could help alleviate this approximation error. We follow the notations in previous sections. The token matrix with scratchpad is defined as ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{E}:=\\left(\\begin{array}{l l}{\\mathbf{X}^{\\top}}&{\\mathbf{x}}\\\\ {\\mathbf{1}_{M}^{\\top}}&{1}\\\\ {\\mathbf{y}^{\\top}}&{0}\\end{array}\\right)\\in\\mathbb{R}^{(d+2)\\times(M+1)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "where $\\mathbf{1}_{M}\\in\\mathbb{R}^{M}$ refers to the vector filled with ones. ", "page_idx": 44}, {"type": "text", "text": "A LSA model $f$ , is defined by ", "page_idx": 44}, {"type": "equation", "text": "$$\nf(\\mathbf{E}):=\\left[\\mathbf{E}+\\mathbf{W}_{P}^{\\top}\\mathbf{W}_{V}\\mathbf{E}\\mathbf{M}\\frac{\\mathbf{E}^{\\top}\\mathbf{W}_{K}^{\\top}\\mathbf{W}_{Q}\\mathbf{E}}{M}\\right]_{-1,-1}=\\left[\\mathbf{W}_{P}^{\\top}\\mathbf{W}_{V}\\mathbf{E}\\mathbf{M}\\frac{\\mathbf{E}^{\\top}\\mathbf{W}_{K}^{\\top}\\mathbf{W}_{Q}\\mathbf{E}}{M}\\right]_{-1,-1},\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "where the second equality is because the bottom right entry of $\\mathbf{E}$ is zero (see (3.1)). Note that the prediction is the bottom right entry of the output matrix. So only the last row of $\\mathbf{W}_{P}^{\\top}\\,\\mathbf{W}_{V}$ and the lastcolumnof $\\mathbf{E}$ attend the prediction. Denote ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\mathbf{W}_{P}^{\\top}\\,\\mathbf{W}_{V}=\\left({\\bf*}\\quad\\ast\\quad\\ast\\quad\\ast\\right),\\quad\\mathbf{W}_{K}^{\\top}\\,\\mathbf{W}_{Q}=\\left({\\bf q}_{1}^{\\top}\\quad b_{1}\\quad\\ast\\right),\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "where ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{w}\\in\\mathbb{R}^{d},\\;\\pmb{a}_{1},\\pmb{a}_{2}\\in\\mathbb{R},\\;\\pmb{Q}\\in\\mathbb{R}^{d\\times d},\\;\\pmb{b},\\pmb{q}_{1},\\pmb{q}_{2}\\in\\mathbb{R}^{d},\\;\\pmb{b}_{1},\\pmb{b}_{2}\\in\\mathbb{R},}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "and $^*$ denotes entries that do not enter the final prediction. Then we have ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{f(\\mathbf{E})=\\left(w^{\\top}}&{a_{1}}&{a_{2}\\right)\\frac{\\mathbf{E}\\mathbf{M}_{M}\\mathbf{E}^{\\top}}{M}\\left(\\begin{array}{l l}{Q}&{b}&{*}\\\\ {q_{1}^{\\top}}&{b_{1}}&{*}\\\\ {q_{2}^{\\top}}&{b_{2}}&{*}\\end{array}\\right)\\binom{\\mathbf{X}}{\\bigcup}}\\\\ {=\\left(w^{\\top}}&{a_{1}}&{a_{2}\\right)\\frac{\\mathbf{E}\\mathbf{M}_{M}\\mathbf{E}^{\\top}}{M}\\left(\\begin{array}{l l}{Q}&{b}\\\\ {q_{1}^{\\top}}&{b_{1}}\\\\ {q_{2}^{\\top}}&{b_{2}}\\end{array}\\right)\\binom{\\mathbf{X}}{\\bot}}\\\\ {=\\left(w^{\\top}}&{a_{1}}&{a_{2}\\right)\\frac{1}{M}\\left(\\begin{array}{l l l}{\\mathbf{X}^{\\top}\\mathbf{X}}&{\\mathbf{X}^{\\top}\\mathbf{1}_{M}}&{\\mathbf{X}^{\\top}\\mathbf{y}}\\\\ {1_{M}^{\\top}\\mathbf{X}}&{M}&{1_{M}^{\\top}\\mathbf{y}}\\\\ {\\mathbf{y}^{\\top}\\mathbf{X}}&{\\mathbf{y}^{\\top}\\mathbf{1}_{M}}&{\\mathbf{y}^{\\top}\\mathbf{y}}\\end{array}\\right)\\left(\\begin{array}{l l}{Q}&{b}\\\\ {q_{1}^{\\top}}&{b_{1}}\\\\ {q_{2}^{\\top}}&{b_{2}}\\end{array}\\right)\\binom{\\mathbf{X}}{\\bot}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Following the Assumption 3.1, we use $\\widetilde{\\beta}$ to refer to the task parameter, then ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\widetilde{\\beta}:=\\beta^{*}+\\Psi^{\\frac{1}{2}}\\widetilde{\\theta},\\quad\\mathrm{where}\\quad\\widetilde{\\theta}\\sim\\mathcal{N}\\left(\\mathbf{0}_{d},\\mathbf{I}_{d}\\right).\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "We then decompose $f(\\mathbf{E})$ as ", "page_idx": 44}, {"type": "equation", "text": "$$\nf({\\bf E})=\\left({\\bf w}^{\\top}\\quad a_{1}\\quad a_{2}\\right)\\frac{1}{M}\\left(\\!\\!\\begin{array}{c c c}{{\\bf X}^{\\top}{\\bf X}}&{{\\bf X}^{\\top}{\\bf1}_{M}}&{{\\bf X}^{\\top}{\\bf y}}\\\\ {{\\bf1}_{M}^{\\top}{\\bf X}}&{M}&{{\\bf1}_{M}^{\\top}{\\bf y}}\\\\ {{\\bf y}^{\\top}{\\bf X}}&{{\\bf y}^{\\top}{\\bf1}_{M}}&{{\\bf y}^{\\top}{\\bf y}}\\end{array}\\!\\!\\right)\\left(\\!\\!\\begin{array}{c}{{Q}}\\\\ {{q_{1}^{\\top}}}\\\\ {{q_{2}^{\\top}}}\\end{array}\\!\\!\\right)\\cdot{\\bf x}+\\Pi,\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "where terms I and $\\mathrm{II}$ are independent of $\\mathbf{x}$ . Therefore, we have ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\mathscr R}\\left(f\\right):={\\mathbb E}\\left(f({\\mathbf E})-y\\right)^{2}}\\\\ &{\\qquad={\\mathbb E}\\left(f({\\mathbf E})-\\langle\\widetilde{\\beta},{\\mathbf x}\\rangle\\right)^{2}+\\sigma^{2}}\\\\ &{\\qquad={\\mathbb E}\\left({\\mathbf I}\\cdot{\\mathbf x}+{\\mathbf I}{\\mathbf I}-\\langle\\widetilde{\\beta},{\\mathbf x}\\rangle\\right)^{2}+\\sigma^{2}}\\\\ &{\\qquad={\\mathbb E}{\\|\\mathbf I^{\\top}-\\widetilde{\\beta}\\|_{\\mathbf H}^{2}}+{\\mathbb E}{\\mathbf I}\\Pi^{2}+\\sigma^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "equation", "text": "$$\n\\geq\\mathbb{E}\\Vert\\boldsymbol{\\mathrm{I}}^{\\top}-\\tilde{\\beta}\\Vert_{\\mathbf{H}}^{2}+\\sigma^{2}.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Note that the above equation holds if $\\mathrm{II}=0$ , which can be easily achieved by setting $\\pmb{b}=\\pmb{0}_{d},\\pmb{b}_{1}=$ $b_{2}=0$ . Therefore, without loss of generality, we can consider the LSA function which takes the followingform: ", "page_idx": 45}, {"type": "equation", "text": "$$\nf({\\bf E})={\\bf I}\\cdot{\\bf x}=\\left({\\bf w}^{\\top}\\quad a_{1}\\quad a_{2}\\right)\\frac{1}{M}\\left(\\!\\!\\begin{array}{c c c}{{\\bf X}^{\\top}{\\bf X}}&{{\\bf X}^{\\top}{\\bf1}_{M}}&{{\\bf X}^{\\top}{\\bf y}}\\\\ {{\\bf1}_{M}^{\\top}{\\bf X}}&{{M}}&{{\\bf1}_{M}^{\\top}{\\bf y}}\\\\ {{\\bf y}^{\\top}{\\bf X}}&{{\\bf y}^{\\top}{\\bf1}_{M}}&{{\\bf y}^{\\top}{\\bf y}}\\end{array}\\!\\!\\right)\\left(\\!\\!\\begin{array}{c}{{Q}}\\\\ {{q_{1}^{\\top}}}\\\\ {{q_{2}^{\\top}}}\\end{array}\\!\\!\\right)\\cdot{\\bf x}.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Let's then try to determine whether such a function can effectively represent a $\\mathsf{G D-}\\beta$ function, which takes the form of ", "page_idx": 45}, {"type": "equation", "text": "$$\nf_{\\mathsf{G D-},\\beta}(\\mathbf{E})=\\langle\\beta^{*},\\mathbf{x}\\rangle-\\left\\langle\\frac{\\Gamma^{*}\\mathbf{X}^{\\top}\\left(\\mathbf{X}\\beta^{*}-\\mathbf{y}\\right)}{M},\\mathbf{x}\\right\\rangle,\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "where $\\beta^{*}$ is the prior mean in Assumption 3.1 in our submission, and $\\mathbf{\\Gamma}^{*}\\,=\\,\\Psi\\mathbf{H}^{\\frac{1}{2}}\\Omega^{-1}\\mathbf{H}^{\\frac{1}{2}}$ and $\\begin{array}{r}{\\Omega=\\frac{M+1}{M}{\\bf H}^{\\frac{1}{2}}\\Psi{\\bf H}^{\\frac{1}{2}}+\\frac{\\sigma^{2}+\\mathrm{tr}({\\bf H}\\Psi)}{M}{\\bf I}_{d}}\\end{array}$ isddr one simple way is to let ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\pmb{w}=-\\beta^{*},\\quad\\pmb{a}_{1}=\\pmb{a}_{2}=1,\\quad\\pmb{Q}=(\\mathbf{r}^{*})^{\\top},\\quad\\pmb{q}_{1}=\\beta^{*},\\quad\\pmb{q}_{2}=\\pmb{0}_{d}.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "However, this will incur some additive terms and will potentially enlarge the ICL risk. Inserting the parameters above, we have ", "page_idx": 45}, {"type": "equation", "text": "$$\nf(\\mathbf{E})=f_{\\mathsf{G D}\\cdot\\beta}(\\mathbf{E})+\\frac{\\mathbf{1}_{M}^{\\top}}{M}\\left(\\mathbf{X}(\\mathbf{T}^{*})^{\\top}-\\mathbf{x}\\beta^{*}(\\beta^{*})^{\\top}+\\mathbf{y}(\\beta^{*})^{\\top}\\right)\\cdot\\mathbf{x}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "This shows that the extended token with a single LSA cannot easily implement the GD- $_{\\cdot\\beta}$ function class and may incur an additive ICL risk depending on $\\beta^{*}$ (as shown in Theorem 4.1). ", "page_idx": 45}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately refect the paper's contributions and scope? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: We believe our introduction and abstract are factually accurate in describing the contributions of the paper and of its result. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 46}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Justification: We discuss the limitation in Section 4 and the future work section ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should refect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should refect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 46}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: We provide the assumption in Assumption 3.1. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 47}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 47}, {"type": "text", "text": "Justification: We provide all detailed experiment setup in Appendix I. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. () If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 47}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 48}, {"type": "text", "text": "Justification: The data we use are simulated. We plan to release our code upon acceptance. Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 48}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 48}, {"type": "text", "text": "Justification: We include all details and hyperparameters in our experiments in Appendix I. Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 48}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 48}, {"type": "text", "text": "Answer: [No] ", "page_idx": 48}, {"type": "text", "text": "Justification: The cost for producing the experiments precludes us from reporting error bars.   \nMoreover, the experiment section is not the main contribution of this paper. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative errorrates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 48}, {"type": "text", "text": "", "page_idx": 49}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce theexperiments? ", "page_idx": 49}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 49}, {"type": "text", "text": "Justification: There is no strict requirement for the computer resources for our experiments. Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 49}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 49}, {"type": "text", "text": "Justification: None to report. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 49}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 49}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 49}, {"type": "text", "text": "Justification: This work focuses on the theory of in-context learning so it is not expected to have a direct societal impact. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 49}, {"type": "text", "text": "", "page_idx": 50}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 50}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 50}, {"type": "text", "text": "Justification: The data used in this paper are fully simulated, so there is no data or model that have a high risk. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safetyfilters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 50}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 50}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 50}, {"type": "text", "text": "Justification: We do not relase any asset. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 50}, {"type": "text", "text": "\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 51}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 51}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 51}, {"type": "text", "text": "Justification: We do not relase any asset. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 51}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 51}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 51}, {"type": "text", "text": "Justification: We do not involve humans in our research. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 51}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 51}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 51}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 51}]