{"references": [{"fullname_first_author": "Kwangjun Ahn", "paper_title": "Transformers learn to implement preconditioned gradient descent for in-context learning", "publication_date": "2023-XX-XX", "reason": "This paper provides a theoretical framework for understanding how transformers perform in-context learning by connecting them to preconditioned gradient descent, a key concept in the current work."}, {"fullname_first_author": "Shivam Garg", "paper_title": "What can transformers learn in-context? A case study of simple function classes", "publication_date": "2022-XX-XX", "reason": "This paper provides empirical and theoretical insights into the in-context learning capabilities of transformers in tractable settings, which informs the current work's theoretical analysis."}, {"fullname_first_author": "Ekin Aky\u00fcrek", "paper_title": "What learning algorithm is in-context learning? Investigations with linear models", "publication_date": "2022-XX-XX", "reason": "This paper investigates the learning algorithm underlying in-context learning using linear models, providing a foundation for the current work's focus on linear regression tasks."}, {"fullname_first_author": "Johannes Von Oswald", "paper_title": "Transformers learn in-context by gradient descent", "publication_date": "2023-XX-XX", "reason": "This paper demonstrates that transformers' in-context learning behavior can be interpreted as gradient descent, a core idea supporting the current work's connection between linear transformers and gradient descent."}, {"fullname_first_author": "Ruiqi Zhang", "paper_title": "Trained transformers learn linear models in-context", "publication_date": "2023-XX-XX", "reason": "This paper lays the groundwork for the current work by establishing a connection between trained transformers and linear models in the context of in-context learning, thus providing the background for this research."}]}