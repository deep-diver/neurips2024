[{"type": "text", "text": "Emergence of heavy tails in homogenized stochastic gradient descent ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Zhe Jiao ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Martin Keller-Ressel\u2217 ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "School of Mathematics and Statistics   \nNorthwestern Polytechnical University Xi\u2019an 710129, China zjiao@nwpu.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Institute of Mathematical Stochastics Technische Universit\u00e4t Dresden 01217 Dresden, Germany martin.keller-ressel@tu-dresden.de ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "It has repeatedly been observed that loss minimization by stochastic gradient descent (SGD) leads to heavy-tailed distributions of neural network parameters. Here, we analyze a continuous diffusion approximation of SGD, called homogenized stochastic gradient descent (hSGD), and show in a regularized linear regression framework that it leads to an asymptotically heavy-tailed parameter distribution, even though local gradient noise is Gaussian. We give explicit upper and lower bounds on the tail-index of the resulting parameter distribution and validate these bounds in numerical experiments. Moreover, the explicit form of these bounds enables us to quantify the interplay between optimization hyperparameters and the tail-index. Doing so, we contribute to the ongoing discussion on links between heavy tails and the generalization performance of neural networks as well as the ability of SGD to avoid suboptimal local minima. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Stochastic gradient descent (SGD) is the cornerstone of optimization in modern deep learning (cf. [Bottou et al., 2018]). In contrast to deterministic methods, it introduces stochasticity to the optimization procedure and therefore has to be analyzed from a probabilistic viewpoint. For instance, it has been observed by Martin and Mahoney [2019], Simsekli et al. [2019], Hodgkinson and Mahoney [2021], Gurbuzbalaban et al. [2021] and others, that the distributions of neural network parameters under loss minimization by SGD are typically heavy-tailed. This heavy-tailed behavior has been linked to the generalization performance of neural networks: Simsekli et al. [2019] give evidence that the extreme realizations of heavy-tailed random variables allows SGD to escape local minima of the loss landscape, and Hodgkinson and Mahoney [2021] argue for a negative correlation between the parameter distributions\u2019s tail-index and the network\u2019s generalization performance.2 For these reasons, it is important to understand the origin and effects of heavy-tailed behavior of neural network parameters in SGD. An important step in this direction has been taken in [Gurbuzbalaban et al., 2021], where the tail behavior of SGD iterates is characterized in dependence on optimization parameters, dimension and Hessian curvature at the loss minimum. One limitation of [Gurbuzbalaban et al., 2021] is that this link is described only qualitatively, but not quantitatively. Here, we provide an alternative approach through analyzing homogenized stochastic gradient descent, a diffusion approximation of SGD introduced in [Paquette et al., 2022b, Mori et al., 2022]. Leveraging It\u00f4 calculus for diffusion processes, we are able to provide more precise bounds and estimates of the tail behavior of SGD iterates, which we subsequently validate in numerical experiments. ", "page_idx": 0}, {"type": "text", "text": "1.1 Our contribution ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Our contribution to the analysis of heavy-tailed phenomena in SGD can be summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We introduce a new method, namely comparison results in convex stochastic order for homogenized stochastic gradient descent. These comparison results, given in Section 3 allow us to link SGD to the well-studied class of Pearson Diffusions (cf. [Forman and S\u00f8rensen, 2008]) and then to obtain bounds for their tail-index.   \n\u2022 Contrary to [Gurbuzbalaban et al., 2021], who describe the tail-index only implicitly (observing phase-transitions between different regimes) our tail-index bounds are fully explicit. Moreover, their explicit form is validated in numerical experiments in Section 4.   \n\u2022 Our results suggest (skew) Student- $t$ -distributions as surrogate for parameter distributions in neural networks under SGD, in contrast to the earlier work of [Gurbuzbalaban et al., 2021] where $\\alpha$ -stable distributions have been suggested. This proposal is validated by numerical experiments and statistical test in Section 4.   \n\u2022 Finally, our results challenge the claim that the \u2018observed heavy-tailed behavior of SGD in practice cannot be accurately represented by an SDE driven by a Brownian motion\u2019 put forward in [Simsekli et al., 2020]. Our modeling approach is based on hSGD \u2013 an SDE driven by Brownian motion \u2013 which asymptotically exhibits heavy-tailed behavior with a tail-index that, in experiments, matches the empirical tail index of SGD iterates on real data. ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Empirical risk minimization ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "The general framework for training deep neural networks is to solve the problem of empirical risk minimization ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x\\in\\mathbb{R}^{d}}\\left\\{L(x):=\\frac{1}{n}\\sum_{i=1}^{n}L_{i}(x)\\right\\}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $L_{i}$ denotes the loss induced by the data point $a_{i}\\in\\mathbb{R}^{d_{1}}$ with label/response $b_{i}\\in\\mathbb{R}$ , given the model\u2019s parameter vector $\\boldsymbol{x}\\in\\mathbb{R}^{\\dot{d}_{2}}$ . For our theoretical and numerical analysis of heavy-tailed phenomena we focus on the specific case of regularized linear regression. Hence, as in [Gurbuzbalaban et al., 2021], we assume a quadratic structure of $L_{i}(x)$ , setting $d=d_{1}=d_{2}$ and ", "page_idx": 1}, {"type": "equation", "text": "$$\nL_{i}(x)={\\frac{1}{2}}(a_{i}\\cdot x-b_{i})^{2}.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Including a regularization term weighted by $\\delta\\geq0$ , we arrive at the objective function ", "page_idx": 1}, {"type": "equation", "text": "$$\n{\\cal L}^{\\mathrm{reg}}(x)={\\cal L}(x)+\\frac{\\delta}{2n}|x|^{2}=\\frac{1}{n}\\left(\\sum_{i=1}^{n}{\\cal L}_{i}(x)+\\frac{\\delta}{2}|x|^{2}\\right),\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "which is the loss function of ridge regression (cf. [Hastie et al., 2009]). We arrange the training data into a design matrix $A\\in\\mathbb{R}^{n\\times d}$ and label vector $b\\in\\mathbb{R}^{n}$ , whose $i$ -th row are given by $a_{i}$ and $b_{i}$ respectively, allowing the write ( $\\boldsymbol{\\delta}$ -ERM) as s ", "page_idx": 1}, {"type": "equation", "text": "$$\nL^{\\mathrm{reg}}(x)=\\frac1{2n}|A x-b|^{2}+\\frac\\delta{2n}|x|^{2}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "with gradient given by $\\begin{array}{r}{\\nabla L^{\\mathrm{reg}}(x)=\\frac{1}{n}\\left(A^{\\top}(A x-b)+\\delta x\\right)}\\end{array}$ . ", "page_idx": 1}, {"type": "text", "text": "2.2 Stochastic gradient descent ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "The standard approach to solve the problem of empirical risk minimization in deep learning is to use stochastic gradient descent (SGD) or any of its generalizations involving momentum, adaptive learning rates, gradient rescaling, etc. (cf. [Goodfellow et al., 2016, Bottou et al., 2018]). As a first step, we consider plain SGD with constant learning rate $\\gamma$ , which can be written in recursive form as ", "page_idx": 1}, {"type": "equation", "text": "$$\nx_{k+1}=x_{k}-\\gamma\\nabla L_{\\Omega_{k}}^{\\mathrm{reg}}(x_{k})\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\begin{array}{r}{\\nabla L_{\\Omega_{k}}^{\\mathrm{reg}}(x_{k})\\,=\\,\\frac{1}{B}\\sum_{i\\in\\Omega_{k}}L_{i}^{\\mathrm{reg}}(x)}\\end{array}$ and $\\Omega_{k}$ is a batch of size $B\\geqslant1$ sampled uniformly and independently from $\\{1,\\cdot\\cdot\\cdot,n\\}$ . It will be convenient to rewrite (SGD) as ", "page_idx": 2}, {"type": "equation", "text": "$$\nx_{k+1}=x_{k}-\\gamma\\nabla L^{\\mathrm{reg}}(x_{k})+\\gamma\\varepsilon(x_{k})\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where the gradient noise is given by ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\varepsilon(x_{k})=-[\\nabla L_{\\Omega_{k}}(x_{k})-\\nabla L(x_{k})].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Note that the gradient noise is unbiased (i.e. $\\mathbb{E}\\varepsilon(x)=0\\,$ ) with covariance matrix given by3 ", "page_idx": 2}, {"type": "equation", "text": "$$\nC(x):=\\mathbb{E}\\left[\\varepsilon(x)\\varepsilon(x)^{\\top}\\right]=\\frac{1}{B}\\left(\\frac{1}{n}\\sum_{i=1}^{n}\\nabla L_{i}(x)\\nabla L_{i}(x)^{\\top}-\\frac{1}{n^{2}}\\nabla L(x)\\nabla L(x)^{\\top}\\right).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The theoretical properties of SGD can now be either analysed directly through the stochastic recurrence (1) (cf. [Bottou et al., 2018]) or through a continuous diffusion approximation, known in the general case as stochastic modified equation (SME), cf. [Mandt et al., 2016, Li et al., 2017]. This approximation is obtained by recognizing (1) as the Euler-Maruyama approximation (in the small learning-rate regime) of the stochastic differential equation (SDE) ", "page_idx": 2}, {"type": "equation", "text": "$$\nd X_{t}=-\\gamma\\nabla L^{\\mathrm{reg}}(X_{t})d t+\\gamma\\sqrt{C(X_{t})}d W_{t},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "driven by a $d$ -dimensional Brownian motion $(W_{t})_{t\\geq0}$ ; cf. Thm. 1 in [Li et al., 2017]. A common further simplification is to assume that the covariance matrix $C(x)$ is constant, yielding the OrnsteinUhlenbeck-approximation (also known as Langevin equation) of SGD, cf. [Mandt et al., 2016, Li et al., 2017]. ", "page_idx": 2}, {"type": "text", "text": "2.3 Homogenized Stochastic Gradient Descent ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Our analysis of SGD is based on homogenized stochastic gradient descent (hSGD), introduced concurrently in [Paquette et al., 2022a] and [Mori et al., 2022], which is another approximation of (SME). In contrast to the Ornstein-Uhlenbeck-approximation where the covariance matrix of gradient noise is assumed constant, hSGD uses the more elaborate \u2018decoupling approximation\u2019 ", "page_idx": 2}, {"type": "equation", "text": "$$\nC(x)\\approx\\frac{2}{B}L(x)\\nabla^{2}L(x),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "see [Paquette et al., 2022a] and [Mori et al., 2022] for a derivation. Hence, in our notation, hSGD for penalized empirical risk minimization is given by4 ", "page_idx": 2}, {"type": "equation", "text": "$$\nd X_{t}=-\\gamma\\nabla L^{\\mathrm{reg}}(X_{t})d t+\\gamma\\sqrt{\\frac{2}{B}L(X_{t})\\nabla^{2}L(X_{t})}d W_{t}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "In the regime where $n$ and $d$ are simultaneously large, and under certain assumptions on the distribution of the data $A$ and $b$ , [Paquette et al., 2022a] provide approximation guarantees of the following form: For any given $T>0$ and $D>0$ , there is a $C>0$ , such that ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\operatorname*{sup}_{0\\leq t\\leq T}\\left|\\mathcal{R}(x_{\\lfloor t n\\rfloor})-\\mathcal{R}(X_{t})\\right|>d^{-\\epsilon/2}\\right)\\leq C d^{-D},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "for quadratic statistics $\\mathcal{R}:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ and when $n\\geq d^{\\epsilon}$ for some $\\epsilon>0$ ; cf. Thm. 1.3 in [Paquette et al., 2022a] for details. Further empirical evidence for the approximation quality of hSGD with respect to SGD can is also given in [Paquette et al., 2022a, Mori et al., 2022], altogether providing a sufficient basis for analyzing the properties of SGD through hSGD. ", "page_idx": 2}, {"type": "text", "text": "Furthermore, the stochastic differential equation (hSGD) can be simplified by using the reduced singular value decomposition (SVD) of the design matrix $A$ . In detail, let $r=\\mathrm{rank}(A)\\leqslant d$ , and let $\\boldsymbol{A}=\\boldsymbol{P}\\boldsymbol{\\Sigma}\\boldsymbol{Q}^{\\intercal}$ be the reduced SVD of $A$ , where $Q$ is $d_{\\cdot}$ -by- $^r$ and satisfies $Q^{\\top}Q=I_{r}$ , $P$ is $n$ -by- $^r$ and satisfies $P^{\\top}P=I_{r}$ , and ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\Sigma=\\mathrm{diag}\\{\\lambda_{j}\\},\\quad\\lambda_{1}\\geqslant\\lambda_{2}\\geqslant\\dotsb\\geqslant\\lambda_{r}>0\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "is the diagonal matrix of non-zero singular values of $A$ . We distinguish the following two cases of hSGD: ", "page_idx": 2}, {"type": "text", "text": "\u2022 Underparametrized hSGD: $A x=b$ has no exact solution, \u2022 Overparametrized hSGD: $A x=b$ has an exact solution, and impose the following assumption: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Assumption 2.1. In the overparametrized case, we require $\\delta>0$ , i.e. the loss function must be regularized. ", "page_idx": 3}, {"type": "text", "text": "It is easily verified that $x_{*}=Q\\Sigma^{-1}P^{\\top}b$ is the unique global minimum of the unregularized loss in the underparametrized case and the global minimum of smallest norm in the overparametrized case. We set $\\begin{array}{r}{Y_{t}^{\\dagger}\\bar{=}\\,(Y_{t}^{i})_{i=1}^{r}=Q_{\\bullet}^{\\top}X_{t}-Q^{\\top}x_{\\ast}}\\end{array}$ and obtain the following system of $\\mathrm{SDEs^{5}}$ for the \u2018centered principal components\u2019 $(Y_{t}^{1},\\ldots,Y_{t}^{r})$ of (hSGD) ", "page_idx": 3}, {"type": "equation", "text": "$$\nd Y_{t}^{i}=-\\frac{\\gamma}{n}\\left[\\left(\\lambda_{i}^{2}+\\delta\\right)Y_{t}^{i}-\\delta\\alpha_{i}\\right]d t+\\frac{\\lambda_{i}\\gamma}{n}\\sqrt{\\frac{1}{B}\\left[\\sum_{j=1}^{r}(\\lambda_{j}Y_{t}^{j})^{2}+\\beta\\right]}d B_{t}^{i}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "with ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\alpha=(\\alpha_{i})_{i=1}^{r}=-\\Sigma^{-1}P^{\\top}b,\\qquad\\beta=b^{\\top}(I_{n}-P P^{\\top})b\\geq0,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "and $(B_{t})_{t\\geq0}$ a $r$ -dimensional Brownian motion, obtained as an orthogonal transformation $B_{t}\\,=$ $Q^{\\top}W_{t}$ of the $d$ -dimensional Brownian motion $(W_{t})_{t\\geq0}$ . Note that $P P^{\\top}b$ is the projection of $b$ onto the column space of $A$ . Thus, in the overparametrized case, $P P^{\\top}b=b$ and hence $\\beta=0$ , whereas in the underparametrized case $P P^{\\top}b\\neq b$ and hence $\\beta>0$ . Here, our main objective is to use (hSGD) to study the distributional properties, in particular the tail behavior, of SGD iterates. ", "page_idx": 3}, {"type": "text", "text": "2.4 Heavy-Tailed Distributions ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We collect some relevant definitions related to heavy-tailed distributions and their tail index (cf. [Resnick, 2007, Foss et al., 2011]). ", "page_idx": 3}, {"type": "text", "text": "Definition 2.2 (Cf. Def. 1.1 in Foss et al. [2011]). A distribution function $F(z)$ is said to be heavytailed (at the right end) if and only if ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{z\\to\\infty}\\operatorname*{lim}_{\\;\\;e^{-s z}}=\\infty,\\quad{\\mathrm{for~all~}}s>0.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "A real-valued random variable is said to be heavy-tailed if its distribution function is heavy-tailed. ", "page_idx": 3}, {"type": "text", "text": "Definition 2.3. An $\\mathbb{R}^{d}-$ valued random vector $X$ is heavy-tailed if $u^{\\mathrm{T}}X$ is heavy-tailed for some vector $u\\in\\mathbb{S}^{d-1}:=\\{u\\in\\mathbb{R}^{d}:|u|=1\\}$ . ", "page_idx": 3}, {"type": "text", "text": "Definition 2.4. The tail-index of an $\\mathbb{R}^{d}-$ valued random vector $X$ is defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\eta:=\\operatorname*{sup}\\{p\\geq0:\\mathbb{E}[|X|^{p}]<\\infty\\}\\in[0,\\infty].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In particular, a finite tail-index $\\eta<\\infty$ implies heavy-tailedness of $X$ , and lower values of $\\eta$ signify increased heaviness of tails and more extremal behavior. A tail index of $\\eta\\:<\\:2$ , for example, implies infinite variance and $\\eta<1$ implies non-existence of the mean of $X$ . Examples of heavytailed distributions are the lognormal distribution, the Student- $t$ -distribution, the Pareto (power-law) distribution, and $\\alpha$ -stable distributions. ", "page_idx": 3}, {"type": "text", "text": "Finally, we introduce a definition related to the asymptotic behavior of stochastic processes. ", "page_idx": 3}, {"type": "text", "text": "Definition 2.5. Let $X=(X_{t})_{t\\geq0}$ be a stochastic process. The asymptotic tail-index of $X$ is defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\eta:=\\operatorname*{sup}\\{p\\geq0:\\operatorname*{lim}_{t\\to\\infty}\\mathbb{E}[|X_{t}|^{p}]<\\infty\\}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "2.5 Pearson Diffusions ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To analyze its tail behavior, we perform a further rescaling of (4) by setting, for $i\\in\\{1,\\ldots,r\\}$ , ", "page_idx": 4}, {"type": "equation", "text": "$$\nZ_{t}^{i}=\\left\\{\\begin{array}{l l}{\\lambda_{i}\\mathrm{sign}(\\alpha_{i})Y_{t}^{i},}&{\\beta=0,}\\\\ {\\frac{\\lambda_{i}}{\\sqrt{\\beta}}Y_{t}^{i},}&{\\beta>0}\\end{array}\\right.\\quad\\mu_{i}=\\left\\{\\begin{array}{l l}{\\frac{n\\lambda_{i}|\\alpha_{i}|}{\\lambda_{i}^{2}+\\delta},}&{\\beta=0,}\\\\ {\\frac{n\\lambda_{i}\\alpha_{i}}{\\sqrt{\\beta}(\\lambda_{i}^{2}+\\delta)},}&{\\beta>0}\\end{array}\\right.\\quad\\chi=\\left\\{\\begin{array}{l l}{0,}&{\\beta=0,}\\\\ {1,}&{\\beta>0}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "equation", "text": "$$\n\\theta_{i}=\\frac{\\gamma}{n}\\left(\\lambda_{i}^{2}+\\delta\\right)>0\\quad\\mathrm{and}\\quad\\phi_{i}=\\frac{\\gamma\\lambda_{i}^{4}}{2n B(\\lambda_{i}^{2}+\\delta)}>0.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This recasts the system (4) to ", "page_idx": 4}, {"type": "equation", "text": "$$\nd Z_{t}^{i}=-\\theta_{i}(Z_{t}^{i}-\\mu_{i})d t+\\sqrt{2\\theta_{i}\\phi_{i}(|Z_{t}|^{2}+\\chi)}d B_{t}^{i}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "with $\\begin{array}{r}{|Z_{t}|^{2}=\\sum_{i=1}^{r}(Z_{t}^{i})^{2}}\\end{array}$ . These SDEs now have a clear structural resemblance to the system of independent one-dimensional SDEs ", "page_idx": 4}, {"type": "equation", "text": "$$\nd\\hat{Z}_{t}^{i}=-\\theta(\\hat{Z}_{t}^{i}-\\mu_{i})d t+\\sqrt{2\\theta_{i}\\phi_{i}((\\hat{Z}_{t}^{i})^{2}+\\chi)}d B_{t}^{i},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "with the only difference given by the coupling of (7) through the $|Z_{t}|^{2}$ -term in the diffusion coefficient.6 The components of (8) are independent Pearson diffusions. Pearson diffusions are a flexible class of SDEs with a unified theory for statistical inference and with stationary distributions known as Pearson distributions (cf. [Forman and S\u00f8rensen, 2008]). In more detail, we obtain from [Forman and S\u00f8rensen, 2008] the following properties: ", "page_idx": 4}, {"type": "text", "text": "Underparametrized hSGD $r\\beta>0$ ): $\\hat{Z}_{t}^{i}$ is $\\mathbb{R}$ -valued and the stationary distribution of ${\\hat{Z}}_{t}^{i}$ is called Pearson\u2019s type IV distribution (or skew Student $t$ -distribution) and has the unnormalized density ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{p_{i}(u)\\propto\\left[1+\\left(\\frac{u}{\\sqrt{\\nu_{i}}}+\\mu_{i}\\right)^{2}\\right]^{-\\frac{\\nu_{i}+1}{2}}\\exp\\left\\{\\mu_{i}(\\nu_{i}-1)\\arctan\\left(\\frac{u}{\\sqrt{\\nu_{i}}}+\\mu_{i}\\right)\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "with $\\nu_{i}=\\phi_{i}^{-1}+1$ . ", "page_idx": 4}, {"type": "text", "text": "Overparametrized hSGD $r\\beta=0$ ): $\\hat{Z}_{t}^{i}$ is $(0,\\infty)$ -valued and the stationary distribution of $\\hat{Z}_{t}^{i}$ is called Pearson\u2019s type $\\mathrm{v}$ distribution (or inverse Gamma distribution) and has the unnormalized density ", "page_idx": 4}, {"type": "equation", "text": "$$\np_{i}(u)\\propto u^{-\\nu_{i}-1}\\exp{\\left(-\\frac{\\mu_{i}(\\nu_{i}-1)}{u}\\right)}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "with $\\nu_{i}=\\phi_{i}^{-1}+1$ . ", "page_idx": 4}, {"type": "text", "text": "In both cases, the stationary distribution is heavy-tailed with tail-index given by $\\nu_{i}$ , thus providing a first connection between the SDE-approach and the emergence of heavy-tails. This connection will be quantified and made rigorous in Section 3. ", "page_idx": 4}, {"type": "text", "text": "2.6 Comparison to existing literature ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We compare our approach to studying the distributional properties of SGD through (hSGD) with other continuous-time approximations: The Ornstein-Uhlenbeck-approximation uses (SME) under the additional assumption that the covariance matrix $C(x)$ is constant. Thus, gradient noise is approximated by Gaussian noise and the Gaussian noise enters (SME) additively. The $\\alpha$ -stable Ornstein-Uhlenbeck-approximation of [Gurbuzbalaban et al., 2021] instead presumes (based on a generalized central limit theorem) that gradient noise is non-Gaussian and follows an $\\alpha$ -stable law. Moreover, the noise is assumed state-independent, and therefore also enters additively. In (hSGD), gradient noise is locally (i.e., conditionally on the state $X_{t}$ ) Gaussian, but state-dependent. The diffusion term in (7) reveals that the noise enters the SDE both multiplicatively (through the $|Z_{t}|^{2}$ -term) and additively (through the constant $\\chi,$ ). Moreover, $\\chi\\,=\\,0$ in the overparametrized case, such that we observe a phase transition from a mix of additive and multiplicative noise in the underparametrized case, to purely mulitiplicative noise in the overparametrized case. We note that the importance of multiplicative noise in models of SGD dynamics is discussed in great detail in [Hodgkinson and Mahoney, 2021]. We provide a summary of the comparison of these approaches in Table 1. ", "page_idx": 4}, {"type": "table", "img_path": "EFrgBP9au6/tmp/6e2f519e36d09889cb70105accca28c296aacdea539d837a4770b1b3939de0b1.jpg", "table_caption": ["Table 1: Comparison of continuous-time models of SGD "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "3 Theoretical results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "3.1 Moment comparison ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Our first result shows that the decoupled Pearson diffusions (8) are lower bounds, in convex stochastic order7, to the coupled hSGD process (7). In particular, a comparison result for moments holds. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.1. For $i\\,=\\,1,\\cdot\\cdot\\,,d,$ let $(Z_{t}^{i})_{t\\geqslant0}$ be the components of the rescaled (hSGD) from (7) and $(\\hat{Z}_{t}^{i})_{t\\geqslant0}$ be the independent Pearson diffusion from (8). Then for any $t\\geqslant0$ and convex function $g:\\mathbb{R}\\rightarrow\\mathbb{R}$ it holds that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}[g(Z_{t}^{i})]\\ge\\mathbb{E}[g(\\hat{Z}_{t}^{i})].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In particular this implies the ordering of $p$ -moments ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}[|Z_{t}^{i}|^{p}]\\geq\\mathbb{E}[|\\hat{Z}_{t}^{i}|^{p}]\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "for all $p\\geq1$ . ", "page_idx": 5}, {"type": "text", "text": "Note that finiteness of the expectations does not need to be assumed, i.e., the inequalities also hold if one of the expectations takes the value $+\\infty$ . Comparison results for SDEs generally require two conditions (cf. [Bergenthum and R\u00fcschendorf, 2007]): An ordering between the drift- and diffusioncoefficients of the two SDEs, and the \u2018propagation-of-order\u2019-property for one of the processes. Comparing (7) and (8), we see that the drift coefficients are identical, while the diffusion coefficients satisfy the required ordering condition $2\\theta_{i}\\phi_{i}(|z|^{2}+\\chi)\\,\\ge\\,2\\theta_{i}\\phi_{i}(z_{i}^{2}+\\chi)$ for any $z\\ \\in\\ \\mathbb{R}^{r}$ . The propagation-of-order property of $\\hat{Z}$ and the full proof of Theorem 3.1 are provided in Supplement A.3. ", "page_idx": 5}, {"type": "text", "text": "3.2 Upper and lower bounds for the asymptotic tail index ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Since the process $(Z_{t})_{t\\geq0}$ is a linear transformation of the hSGD process $(X_{t})_{t\\geq0}$ , it is clear that the tail behaviour of their marginal distributions \u2013 in particular the finiteness of $p$ -moments \u2013 is identical. Hence, an application of Thm. 3.1 provides an upper bound on the asymptotic tail index of (hSGD): ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.2. The asymptotic tail index $\\eta$ of (hSGD) has the upper bound ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\eta\\leq\\eta^{*}:=1+\\frac{2n B(\\lambda_{1}^{2}+\\delta)}{\\gamma\\lambda_{1}^{4}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Under conditions on the learning rate $\\gamma$ , a complementary lower bound can be derived from existing results on moment stability of SDEs, see Thm. 5.2 in [Li et al., 2019] and Supplement A.5 for details: ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.3. Suppose that the learning rate $\\gamma$ satisfies ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\gamma<\\overline{{\\gamma}}=:\\frac{2n B(\\lambda_{1}^{2}+\\delta)}{\\lambda_{1}^{2}\\sum_{i=1}^{r}\\lambda_{i}^{2}},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "then the asymptotic tail index $\\eta$ of (hSGD) has the lower bound ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\eta\\ge\\eta_{*}:=1+\\frac{2n B(\\lambda_{1}^{2}+\\delta)}{\\gamma\\lambda_{1}^{4}}-\\frac{\\sum_{i=2}^{r}\\lambda_{i}^{2}}{\\lambda_{1}^{2}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "3.3 Wasserstein convergence ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Theorems 3.2 and 3.3 are results on the asymptotic tail index, raising the question how fast convergence to the stationary distribution takes place. The next result shows that, under a suitable assumption on the learning rate, convergence takes place exponentially fast in 2-Wasserstein distance: ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.4. Suppose that the learning rate $\\gamma$ satisfies ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\gamma<\\gamma^{\\prime}=:\\frac{n B}{2}\\left\\{\\sum_{i=1}^{r}\\frac{\\lambda_{i}^{4}}{\\lambda_{i}^{2}+\\delta}\\right\\}^{-1}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Then the equation ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{r}\\frac{\\lambda_{i}^{4}}{\\lambda_{i}^{2}+\\delta-n\\rho/\\gamma}=\\frac{n B}{2\\gamma}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "has a unique positive solution $\\rho_{*}>0$ , and the marginal distribution $\\pi_{t}$ of the hSGD process $X_{t}$ converges in 2-Wasserstein distance $\\mathcal{W}_{2}$ to its unique invariant distribution $\\pi$ . Moreover, there exists $C>0$ , such that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{W}_{2}(\\pi_{t},\\pi)\\leq C e^{-t\\rho_{*}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We remark that if the conditions of Theorem 3.4 are satisfied, then the asymptotic tail-index $\\eta$ is necessarily greater than two, such that second moments, and in particular the 2-Wasserstein distance, are well-defined and finite. ", "page_idx": 6}, {"type": "text", "text": "3.4 Discussion of theoretical results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We compare our results to Gurbuzbalaban et al. [2021], who analyse the distributional properties of SGD directly through the stochastic recurrence (1) under the assumption of an isotropic Gaussian data distribution. In our setting, the data distribution is arbitrary, since all results are given conditional on the data matrix $A$ . On the other hand, we analyse SGD only through its diffusion approximation (hSGD) rather than directly. However, in contrast to [Gurbuzbalaban et al., 2021], we obtain the quantitative and explicit tail-index bounds (13) and (14), whereas Gurbuzbalaban et al. [2021] only describe the tail index through an implicit equation and derive qualitative results on its behaviour. ", "page_idx": 6}, {"type": "text", "text": "Parameter Dependency. Some interesting observations can be made when we consider the dependency of $\\eta$ on several meta-parameters of the stochastic gradient descent procedure: ", "page_idx": 6}, {"type": "text", "text": "Corollary 3.5. The upper and lower bounds of the tail-index are increasing in the regularization parameter $\\delta$ and batch size $B$ , and are decreasing in the learning rate $\\gamma$ and the first singular value $\\lambda_{1}$ of the data matrix $A$ . ", "page_idx": 6}, {"type": "text", "text": "This result agrees with Theorem 4 in [Gurbuzbalaban et al., 2021], obtained under the assumption of an isotropic data distribution $a_{i}\\sim N(0,\\sigma^{2}I_{d})$ , in all aspects, except the dependency on dimension $d$ . While Gurbuzbalaban et al. [2021] report decreasing dependency on $d$ , our tail-index bounds do not explicitly depend on dimension $d$ . Nevertheless, the two results can be reconciled as follows: Under the assumptions in [Gurbuzbalaban et al., 2021], the data matrix $A=\\left(a_{i}\\right)$ is random with $\\mathbb{E}(A^{\\mathrm{T}}A)\\,=\\,\\sigma^{2}I_{d}^{\\mathbf{2}}$ , and the product matrix $W\\,:=\\,A^{\\mathrm{T}}\\bar{A}$ follows the so-called Wishart ensemble (cf. [Wishart, 1928]). Moreover, from Theorem 1.1 in [Johnstone, 2001] it follows that for large $d$ the maximum eigenvalue of $W$ is ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\lambda_{1}^{2}=\\sigma^{2}\\left[\\left(\\frac{1}{\\sqrt{r}}+1\\right)^{2}d+r^{\\frac{1}{6}}\\left(\\frac{1}{\\sqrt{r}}+1\\right)^{\\frac{4}{3}}d^{\\frac{1}{3}}\\Psi\\right],\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where the ratio $\\begin{array}{r}{r=\\frac{d}{n-1}<1}\\end{array}$ and the distribution function of the random variable $\\Psi$ is the well-known Tracy-Widom distribution of order 1 (cf. [Tracy and Widom, 1996]). From (15), we can calculate the average of $\\lambda_{1}^{2}$ as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\lambda_{1}^{2}\\right]=\\sigma^{2}(\\frac{1}{\\sqrt{r}}+1)^{2}d=\\sigma^{2}(\\sqrt{n-1}+\\sqrt{d})^{2}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "and $\\lambda_{1}^{2}$ fluctuates around this expectation over a narrow region of width $O(d^{\\frac{1}{3}})$ . Substituting $\\lambda_{1}^{2}$ by its expectation in (13) and (14) we can now see that $\\eta_{*}$ and $\\eta^{*}$ increase in both variance $\\sigma^{\\bar{2}}$ and $d$ , consistent with [Gurbuzbalaban et al., 2021]. ", "page_idx": 6}, {"type": "text", "text": "Distributional properties. From Theorem 3.1 we see that the skew Student- $t$ distribution provides an asymptotic lower bound in convex order for the marginal distribution of hSGD. Empirically (see Section 4) we see that skewness is negligible and furthermore, that the Student- $\\cdot t$ -distribution not only provides a lower bound, but in fact a very good fit to the parameter distribution of SGD in general, surpassing the fit of the $\\alpha$ -stable distribution proposed in [Gurbuzbalaban et al., 2021]. For this reason, we propose to use the Student- $\\boldsymbol{t}$ -distribution, rather than $\\alpha$ -stable distribution, as a proxy for the parameter distribution in SGD. ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Based on the upper and lower bounds in Theorems 3.2 and 3.3, we present some experiments to illustrate the tail behavior of SGD and the factors influencing the tail index. The procedure of our experiments contains the following steps. ", "page_idx": 7}, {"type": "text", "text": "1. Given $[\\mathrm{data}|b]$ , we transform the data to be on a similar scale by the linear scaling ", "page_idx": 7}, {"type": "equation", "text": "$$\nA={\\frac{\\mathrm{data-min}\\{\\mathrm{data}\\}}{\\mathrm{max}\\{\\mathrm{data}\\}-\\mathrm{min}\\{\\mathrm{data}\\}}}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "2. Let $K$ be the iteration number of SGD. We apply (SGD) to solve (ERM). The final state $\\boldsymbol{x}_{K}\\in\\mathbb{R}^{d}$ is a random vector.   \n3. Repeat the second step 1000 times for different initial points and obtain 1000 different samples of $x_{K}$ .   \n4. For further distributional analysis we project $x_{K}$ via $\\mathrm{y}=q_{1}^{\\top}x_{K}$ on the dominant direction, given by the first right singular vector $q_{1}$ of $A$ . Then we utilize the 1000 samples to obtain the empirical complementary cumulative distribution function (ccdf) of y. ", "page_idx": 7}, {"type": "text", "text": "4.1 Datasets ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Synthetic data. We first validate our results in the same synthetic setup used in [Gurbuzbalaban et al., 2021]. All data points are drawn from isotropic Gaussian distributions, precisely, the $i$ -th row of $\\mathcal{X}\\in\\bar{\\mathbb{R}}^{n\\times d}$ contains $\\chi_{i}\\in\\mathbb{R}^{d}\\sim\\mathcal{N}(0,I_{d})$ . Then given $\\boldsymbol{x}\\in\\mathbb{R}^{d}\\sim\\mathcal{N}(0,3\\bar{I}_{d})$ we draw the response vextor $b\\in\\mathbb{R}^{n}$ with components $b_{i}\\sim\\mathcal{N}(\\chi_{i}x,3)$ . We set the number $n$ of the synthetic data to be 2000 through our experiments. ", "page_idx": 7}, {"type": "text", "text": "Real data. In our second setup we conduct our experiments on the handwritten digits dataset from the Scikit-learn python package (cf. [Pedregosa et al., 2011]) using a random feature model proposed in [Rahimi and Recht, 2007] and, in addition, a standard three-layer neural network. The digits dataset contains $n\\,=\\,1797$ images of handwritten digits in a $8\\times8$ pixel format. The pixels are stacked into vectors of length $\\bar{n_{0}^{\\star}}=8^{2}=64$ resulting in a raw data matrix $\\mathcal{D}\\in\\mathbb{R}^{n\\times n_{0}}$ and the class label $b_{i}=\\{0,1,\\cdots\\,,9\\}$ is used as response vector. For the random feature model, we choose a dimension $d$ and draw a random weight matrix $W\\,\\in\\,\\mathbb{R}^{n_{0}\\times d}$ having standard Gaussian entries. The feature matrix $\\mathcal{Z}\\in\\mathbb{R}^{n\\times d}$ is then given by ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathcal{Z}=\\sigma\\left(\\frac{\\ y W}{\\sqrt{n_{0}}}\\right)\\in\\mathbb{R}^{n\\times d},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\sigma(\\cdot)$ is a rescaled ReLu activation function. The neural-network model uses 64 neurons in each hidden layer and sigmoid activation functions. The precise parameter values used for the figures are reported in Tables 3 and 4 in the supplement. ", "page_idx": 7}, {"type": "text", "text": "4.2 Empirical results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To verify the heavy-tailed behavior of y as well as our tail-index bounds from Theorems 3.2 and 3.3 and the distributional approximation suggested by (9), we use MLE-estimation to fit our centered data as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathrm{z}:=\\mathrm{y}-\\mathrm{mean}\\mathrm{\\{y\\}}\\sim\\kappa t(\\nu).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $t(\\nu)$ denotes a Student- $\\cdot t$ -distribution with parameter $\\nu$ and $\\kappa$ is a fitted scaling factor.8 The QQ-plots in Figures 1, 2 (a)-(c) show that the Student- $\\cdot t$ -distribution provides a very good fit to the empirical data, validating our use of Pearson diffusions to approximate SGD. In comparison, it can be seen in Figures 1, 2 (d)-(f) that the ftited $\\alpha$ -stable distribution overestimates the heaviness of tails, in particular for the random feature model on real data. We complement Figure 1 by a KolmogorovSmirnov test (cf. Chapter 4.4 in [Corder and Foreman, 2014]) testing for the goodness-of-fit of the Student- $\\cdot t$ -distribution and the $\\alpha$ -stable distribution respectively; see Table 2 for detailed results. In all three settings, the hypothesis of a Student- $t$ -distribution is accepted, while the $\\alpha$ -stable distribution is rejected. ", "page_idx": 7}, {"type": "image", "img_path": "EFrgBP9au6/tmp/4707f4d494631233a71cb2b8d28c7c083b5046b24c06822b34dae2c4be708b50.jpg", "img_caption": ["Figure 1: Results for linear regression/random feature model trained on datasets $\\mathcal{X},\\,\\mathcal{Y}$ , and $\\mathcal{Z}$ . (a)-(c) Quantile-Quantile plots of fitted Student- $\\cdot t$ -distribution against empirical SGD iterates; (d)- (f) Quantile-Quantile plots of fitted $\\alpha$ -stable distribution against empirical SGD iterates; (g)-(i) Comparison between ccdf of empirical data and Student- $t$ -distribution parameterized by upper tailindex bound $\\eta^{*}$ and lower bound $\\eta_{*}$ . "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Moreover, in Figure 1 (g)-(i) we plot (in doubly logarithmic coordinates) the empirical ccdf of the SGD iterates $\\mathrm{^Z}$ , together with the ccdf of the Student- $\\cdot t$ -distribution parametrized by our lower and upper bound $\\eta_{*}$ and $\\eta^{*}$ . It can be seen that the empirical ccfd, including its tail, is nicely sandwiched between upper and lower bound, validating Theorems 3.2 and 3.3. Additionally, we once more confirm the heavy-tailed behavior of SGD iterates as already observed in [Simsekli et al., 2019, Hodgkinson and Mahoney, 2021, Gurbuzbalaban et al., 2021]. ", "page_idx": 8}, {"type": "text", "text": "Table 2: Kolmogorov-Smirnov test of theoretical distributions against observed SGD iterates of the linear regression/random feature model. The null hypothesis $H_{0}$ is that two distributions are identical, the alternative $H_{1}$ is that they are not identical. ", "page_idx": 8}, {"type": "table", "img_path": "EFrgBP9au6/tmp/2cd458da9882d60d8a6390e4ad9fe8311c16c410e2da29b95db5c5921d1bb19b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "EFrgBP9au6/tmp/5fc5e55182d20a2528fd87eddbd1135be458aa533dffe51439b219e64f29a668.jpg", "img_caption": ["Figure 2: Results for three-layer neural network model trained on datasets $\\mathcal{X},\\mathcal{V}$ , and $\\mathcal{Z}$ . (a)-(c) Quantile-Quantile plots of ftited Student- $t$ -distribution against empirical SGD iterates of second layer; (d)-(f) Quantile-Quantile plots of ftited $\\alpha$ -stable distribution against empirical SGD iterates of second layer. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "5 Conclusion and Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work analyses the emergence of heavy tails in the parameters of homogenized stochastic gradient descent applied to regularized linear regression. By establishing a connection between hSGD and Pearson diffusions, we derive explicit upper and lower bounds on the tail index of the parameter distribution. Our results demonstrate that heavy tails can emerge even in the presence of locally Gaussian gradient noise and provide insights into the influence of optimization hyperparameters on the tail index. However, it is essential to acknowledge that our analysis relies on the approximation of SGD by hSGD and is limited to the setting of linear regression with quadratic loss. Another limitation (see (14)) is that the tail-index of hSGD is lower-bounded by one, and thus hGSD can not be used to analyse \u2018ultra-heavy tails\u2019 with tail-index $\\eta\\leq1$ . Future work will be devoted to extending our results to non-linear models and to providing a tighter connection between the behaviour of hSGD and the discrete-time SGD algorithm. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Zhe Jiao\u2019s research is supported by the National Natural Science Foundation of China (12272297). Martin Keller-Ressel acknowledges support from the Center for Scalable Data Analytics and Artificial Intelligence (ScaDS.AI) in Lepizig/Dresden, Germany. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Joel Anderson. A secular equation for the eigenvalues of a diagonal matrix perturbation. Linear algebra and its applications, 246:49\u201370, 1996. ", "page_idx": 9}, {"type": "text", "text": "D. Azagra. Global and fine approximation of convex functions. Proceedings of the London Mathematical Society, 107(4):799\u2013824, 2013. ", "page_idx": 9}, {"type": "text", "text": "Paul R Beesack. Comparison theorems and integral inequalities for Volterra integral equations. Proceedings of the American Mathematical Society, 20(1):61\u201366, 1969. ", "page_idx": 9}, {"type": "text", "text": "J. Bergenthum and L. R\u00fcschendorf. Comparison of semimartingales and L\u00e9vy processes. The Annals of Probability, 35(1):228\u2013254, 2007.   \nL. Bottou, E. F. Curtis, and J. Nocedal. Optimization methods for large-scale machine learning. SIAM Review, 60(2):223\u2013311, 2018.   \nG.W. Corder and D.I. Foreman. Nonparametric Statistics: A Step-by-Step Approach. Wiley, 2014.   \nChrista Cuchiero, Martin Keller-Ressel, and Josef Teichmann. Polynomial processes and their applications to mathematical finance. Finance and Stochastics, 16:711\u2013740, 2012.   \nBenjamin Dupuis and Umut Simsekli. Generalization bounds for heavy-tailed SDEs through the fractional Fokker-Planck equation. In ICML, 2024.   \nDamir Filipovic\u00b4 and Martin Larsson. Polynomial diffusions and applications in finance. Finance and Stochastics, 20:931\u2013972, 2016.   \nJulie Forman and Michael S\u00f8rensen. The Pearson diffusions: a class of statistically tractable diffusion processes. Scandinavian Journal of Statistics, 35:438\u2013465, 2008.   \nSergey Foss, Dmitry Korshunov, Stan Zachary, et al. An introduction to heavy-tailed and subexponential distributions, volume 6. Springer, 2011.   \nMartin Friesen, Peng Jin, and Barbara R\u00fcdiger. Stochastic equation and exponential ergodicity in Wasserstein distances for affine processes. The Annals of Applied Probability, 30(5):2165\u20132195, 2020.   \nI. Goodfellow, Y. Bengio, and A. Courville. Deep learning. MIT Press, 2016.   \nMert Gurbuzbalaban, Umut Simsekli, and Lingjiong Zhu. The heavy-tail phenomenon in SGD. In International Conference on Machine Learning, pages 3964\u20133975, 2021.   \nTrevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of Statistical Learning. Springer, 2009.   \nJ.-B. Hiriart-Urruty and C. Lemar\u00e9chal. Convex analysis and minimization algorithms I: Fundamentals. Springer, 1996.   \nL. Hodgkinson and M. Mahoney. Multiplicative noise and heavy tails in stochastic optimization. In International Conference on Machine Learning, pages 4262\u20134274, 2021.   \nRoger A Horn and Charles R Johnson. Matrix analysis. Cambridge university press, 2012.   \nIain M. Johnstone. On the distribution of the largest eigenvalue in principal components analysis. The Annals of Statistics, 29(2):295\u2013327, 2001.   \nI. Karatzas and S. Shreve. Brownian motion and stochastic calculus. Springer, 2012.   \nIoannis Karatzas and Steven Shreve. Brownian motion and stochastic calculus, volume 113. springer, 2014.   \nPeter E. Kloeden and Eckhard Platen. Numerical Solution of Stochastic Differential Equations. Springer, 1999.   \nQianxiao Li, Cheng Tai, and E Weinan. Stochastic modified equations and adaptive stochastic gradient algorithms. In International Conference on Machine Learning, pages 2101\u20132110. PMLR, 2017.   \nXiaoyue Li, Xuerong Mao, and George Yin. Explicit numerical approximations for stochastic differential equations in finite and infinite horizons: truncation methods, convergence in $p_{\\|}$ -th moment and stability. IMA journal of Numerical Analysis, 39:847\u2013892, 2019.   \nS. Mandt, M. Hoffman, and D. A. Blei. A variational analysis of stochastic gradient algorithms. In International Conference on Learning Representations, 2016.   \nC. Martin and M. Mahoney. Traditional and heavy tailed self regularization in neural network models. In International Conference on Machine Learning, pages 4284\u20134293, 2019.   \nT. Mori, Ziyin Li, K. Liu, and M. Ueda. Power-law escape rate of SGD. In International Conference on Machine Learning, pages 15959\u201315975, 2022.   \nAlfred M\u00fcller and Dietrich Stoyan. Comparison Methods for Stochastic Models and Risks. Wiley, 2002.   \nBernt Oksendal. Stochastic differential equations: an introduction with applications. Springer Science & Business Media, 2013.   \nCourtney Paquette, Elliot Paquette, Ben Adlam, and Jeffrey Pennington. Homogenization of SGD in high-dimensions: Exact dynamics and generalization properties. Advances in Neural Information Processing Systems, 35:35984\u201335999, 2022a.   \nCourtney Paquette, Elliot Paquette, Ben Adlam, and Jeffrey Pennington. Implicit regularization or implicit conditioning? exact risk trajectories of SGD in high dimensions. Advances in Neural Information Processing Systems, 35:35984\u201335999, 2022b.   \nFabian Pedregosa, Ga\u00ebl Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, and \u00c9douard Duchesnay. Scikit-learn: Machine learning in python. Journal of Machine Learning Research, 12(85):2825\u2013 2830, 2011.   \nA. Rahimi and B. Recht. Random features for large-scale kernel machines. In Advances in Neural Information Processing Systems, 2007.   \nAnant Raj, Lingjiong Zhu, Mert Gurbuzbalaban, and Umut Simsekli. Algorithmic stability of heavytailed SGD with general loss functions. In International Conference on Machine Learning, pages 28578\u201328597. PMLR, 2023.   \nS. I. Resnick. Heavy-tail phenomena: probabilistic and statistical modeling. Springer, 2007.   \nM. Shaked and J. G. Shanthikumar. Stochastic orders. Springer, 2007.   \nUmut Simsekli, L. Sagun, and Mert Gurbuzbalaban. A tail-index analysis of stochastic gradient noise in deep neural networks. In International Conference on Machine Learning, pages 5287\u20135837, 2019.   \nUmut Simsekli, O. Sener, G. Deligiannidis, and M. A. Erdogdu. Hausdorff dimension, heavy tails, and generalization in neural networks. In Advances in Neural Information Processing Systems, pages 5138\u20135151, 2020.   \nVolker Strassen. The existence of probability measures with given marginals. The Annals of Mathematical Statistics, 36:432\u2013439, 1965.   \nCraig A. Tracy and Harold Widom. On orthogonal and symplectic matrix ensembles. Communications in Mathematical Physics, 177:727\u2013754, 1996.   \nJohn Wishart. The generalized product moment distribution in samples from a normal multivariate population. Biometrika, 20A(1/2):32\u201352, 1928. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Supplementary material ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A.1 Derivation of covariance matrix ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Consider the minibatch stochastic gradient ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\nabla L_{k}(x)=\\frac{1}{B}\\sum_{i\\in\\Omega_{k}}L_{i}(x)=\\frac{1}{B}\\sum_{i\\in\\Omega_{k}}\\nabla L_{i}(x),\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $B$ is the batchsize and the random set $\\Omega_{k}\\;=\\;\\{i_{1},\\cdot\\cdot\\cdot\\,,i_{B}\\}$ consists of $B$ independently identically distributed random integers sampled uniformly from $\\{1,{\\dot{2}},\\cdot\\cdot\\cdot,n\\}$ . ", "page_idx": 12}, {"type": "text", "text": "Let $\\begin{array}{r}{\\nabla\\tilde{L}_{k}(x)=\\frac{1}{B}\\sum_{i\\in\\Omega_{k}}\\nabla L_{i}(x)}\\end{array}$ . It can be rewritten as ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\nabla\\tilde{L}_{k}(x)=\\frac{1}{B}\\sum_{i=1}^{n}\\nabla L_{i}(x)\\mathrm{s}_{i},\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where the random variable $\\mathrm{s}_{i}=l$ if $l$ -multiple $i$ \u2019s are sampled in $\\Omega_{k}$ , with $0\\leqslant l\\leqslant B$ . The probability of $\\mathrm{s}_{i}=l$ is given by the multinomial distribution $\\mathbb{P}({\\mathrm{s}}_{i}\\stackrel{.}{=}l)=C_{B}^{l}(\\frac{1}{n})^{l}(1-\\frac{1}{n})^{B-l}$ . Moreover, we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathbf{s}_{i}]=\\frac{B}{n},\\quad\\mathbb{E}[\\mathbf{s}_{i}\\mathbf{s}_{j}]=\\frac{B(B-1)}{n^{2}},\\quad\\mathbb{E}[\\mathbf{s}_{i}\\mathbf{s}_{i}]=\\frac{B n+B(B-1)}{n^{2}}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "We can also compute ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\nabla\\tilde{L}_{k}(x)]=\\frac{1}{B}\\sum_{i=1}^{n}\\nabla L_{i}(x)\\mathbb{E}[\\mathrm{s}_{i}]=\\frac{1}{n}\\nabla L(x)\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "and ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}[\\nabla\\tilde{L}_{k}(x)\\nabla\\tilde{L}_{k}(x)^{\\top}]}\\\\ {\\displaystyle=\\frac{1}{B^{2}}\\left[\\displaystyle\\sum_{i=1}^{n}\\sum_{j=1}^{n}\\nabla L_{i}(x)\\nabla L_{j}(x)^{\\top}{\\bf s}_{i}s_{j}\\right]=\\frac{1}{B^{2}}\\sum_{i=1}^{n}\\sum_{j=1}^{n}\\left[\\nabla L_{i}(x)\\nabla L_{j}(x)^{\\top}\\mathbb{E}({\\bf s}_{i}s_{j})\\right]}\\\\ {\\displaystyle=\\frac{1}{B^{2}}\\sum_{i,j=1}^{n}\\nabla L_{i}(x)\\nabla L_{j}(x)^{\\top}\\frac{B(B-1)}{n^{2}}}\\\\ {\\displaystyle~~~+\\frac{1}{B^{2}}\\sum_{i=1}^{n}\\nabla L_{i}(x)\\nabla L_{i}(x)^{\\top}\\left[\\displaystyle\\frac{B n+B(B-1)}{n^{2}}-\\displaystyle\\frac{B(B-1)}{n^{2}}\\right]}\\\\ {\\displaystyle=\\frac{B-1}{B}\\frac{1}{n^{2}}\\nabla L(x)\\nabla L(x)^{\\top}+\\frac{1}{n B}\\displaystyle\\sum_{i=1}^{n}\\nabla L_{i}(x)\\nabla L_{i}(x)^{\\top}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Combining (16) with (17) gives ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{C(x)=\\mathbb{E}\\left\\{[\\nabla\\tilde{f}_{k}(x)-\\nabla f(x)][\\nabla\\tilde{f}_{k}(x)-\\nabla f(x)]^{\\top}\\right\\}}\\\\ &{\\phantom{\\sum}=\\mathbb{E}\\left\\{[\\nabla\\tilde{Z}_{k}(x)-\\frac{1}{n}\\nabla L(x)][\\nabla\\tilde{L}_{k}(x)-\\frac{1}{n}\\nabla L(x)]^{\\top}\\right\\}}\\\\ &{\\phantom{\\sum}=\\mathbb{E}[\\nabla\\tilde{L}_{k}(x)\\nabla\\tilde{L}_{k}(x)]^{\\top}-\\frac{1}{n^{2}}\\nabla L(x)\\nabla L(x)^{\\top}}\\\\ &{\\phantom{\\sum}=\\frac{1}{B}\\left[\\frac{1}{n}\\sum_{i=1}^{n}\\nabla L_{i}(x)\\nabla L_{i}(x)^{\\top}-\\frac{1}{n^{2}}\\nabla L(x)\\nabla L(x)^{\\top}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "A.2 Transformation of hSGD ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "By multiplying both sides of hSGD by $Q^{\\top}$ we obtain ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d(Q^{\\top}X_{t})=-\\gamma Q^{\\top}\\nabla L^{\\mathrm{reg}}(X_{t})d t+\\gamma Q^{\\top}\\sqrt{\\frac{2}{B}L(X_{t})\\nabla^{2}L(X_{t})}\\,d W_{t}}\\\\ &{\\qquad\\qquad=-\\frac{\\gamma}{n}Q^{\\top}\\left[A^{\\top}(A X_{t}-b)+\\delta X_{t}\\right]d t+\\frac{\\gamma}{n}\\sqrt{\\frac{1}{B}|A X_{t}-b|^{2}}Q^{\\top}\\sqrt{A^{\\top}A}\\,d W_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Due to ", "page_idx": 13}, {"type": "equation", "text": "$$\nQ^{\\top}\\left[A^{\\top}(A X_{t}-b)+\\delta X_{t}\\right]=\\left(\\Sigma^{2}+\\delta I_{r}\\right)Q^{\\top}X_{t}-\\Sigma P^{\\top}b\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "and ", "page_idx": 13}, {"type": "equation", "text": "$$\n|A X_{t}-b|^{2}=|\\Sigma Q^{\\top}X_{t}-P^{\\top}b|^{2},\\quad Q^{\\top}{\\sqrt{A^{\\top}A}}=\\Sigma Q^{\\top},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "(18) can be reformulated as ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d(Q^{\\top}X_{t})=-\\xrightarrow{\\gamma}\\left[\\left(\\Sigma^{2}+\\delta I_{r}\\right)Q^{\\top}X_{t}-\\Sigma P^{\\top}b\\right]d t}\\\\ &{\\qquad\\qquad\\qquad+\\,\\frac{\\gamma}{n}\\sqrt{\\frac{1}{B}|\\Sigma Q^{\\top}X_{t}-P^{\\top}b|^{2}}\\Sigma\\,d\\left(Q^{\\top}W_{t}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Let $B_{t}:=Q^{\\top}W_{t}$ , which is an $r$ -dimensional Brownian motion, due to $Q^{\\top}Q=I_{r}$ . From (19) it follows that $\\boldsymbol{Y_{t}}=\\boldsymbol{Q}^{\\top}\\boldsymbol{X_{t}}-\\boldsymbol{Q}^{\\top}\\boldsymbol{x_{*}}$ satisfies ", "page_idx": 13}, {"type": "equation", "text": "$$\nd Y_{t}=-\\frac{\\gamma}{n}\\left[\\left(\\Sigma^{2}+\\delta I_{r}\\right)Y_{t}-\\alpha\\right]d t+\\frac{\\gamma}{n}\\sqrt{\\frac{1}{B}[Y_{t}^{\\top}\\Sigma^{2}Y_{t}+\\beta]}\\Sigma d B_{t}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "with ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\alpha:=-\\Sigma^{-1}P^{\\top}b,\\qquad\\beta:=b^{\\top}(I_{n}-P P^{\\top})b\\geq0.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Reading (20) component by component, we obtain (4). ", "page_idx": 13}, {"type": "text", "text": "A.3 Proof of Theorem 3.1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We write the SDEs (7) and (8) in the form ", "page_idx": 13}, {"type": "equation", "text": "$$\nd Z_{t}^{i}=b_{i}(Z_{t}^{i})d t+\\sigma_{i}(Z_{t})d B_{t}^{i},\\quad d\\hat{Z}_{t}^{i}=b_{i}(\\hat{Z}_{t}^{i})d t+\\hat{\\sigma}_{i}(\\hat{Z}_{t}^{i})d B_{t}^{i},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where ", "page_idx": 13}, {"type": "equation", "text": "$$\nb_{i}(z_{i})=-\\theta_{i}(z_{i}-\\mu_{i}),\\quad\\sigma_{i}^{2}(z)=2\\theta_{i}\\phi_{i}(|z|^{2}+\\chi)\\quad\\mathrm{and}\\quad\\hat{\\sigma}_{i}(z_{i})^{2}=2\\theta_{i}\\phi_{i}(z_{i}^{2}+\\chi).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "While the drift coefficients are identical, the diffusion coefficients satisfy the inequality $\\sigma_{i}(z)\\geq\\hat{\\sigma}_{i}(z)$ for all $z\\in\\mathbb{R}^{r}$ and $i=1,\\hdots,r$ . Note that all coefficients are Lipschitz continuous and of bounded growth, such that the standard assumptions for uniqueness and existence of strong SDE solutions are satisfied. Moreover, the SDEs for $\\hat{Z}_{t}^{i}$ are decoupled and each is a Markov diffusion with generator given by ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\hat{\\mathcal{L}}_{i}=b_{i}(x)\\partial_{x}+\\frac{\\hat{\\sigma}_{i}(x)^{2}}{2}\\partial_{x x},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $x$ denotes the scalar state variable of ${\\hat{Z}}^{i}$ . Let $C_{P}^{l}(\\mathbb{R})$ denote the subspace of $C^{l}$ -functions for which all derivatives up to order $l$ have polynomial growth. Suppose that $g\\in C_{P}^{l}(\\mathbb{R})$ . From Theorem 4.8.6 in [Kloeden and Platen, 1999] the backward functional ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathcal{G}_{i}(t,x)=\\mathbb{E}[g(\\hat{Z}_{T}^{i})|\\hat{Z}_{t}^{i}=x],\\quad t\\in[0,T],\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "satisfies the backward Kolmogorov equation ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\partial_{t}\\mathcal{G}_{i}(t,x)+\\hat{\\mathcal{L}}_{i}\\mathcal{G}_{i}(t,x)=0\\quad t<T,}}\\\\ {{\\mathcal{G}_{i}(T,x)=g(x).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "with $\\partial_{t}\\mathcal{G}_{i}$ continuous and $\\mathcal{G}_{i}(t,\\cdot)\\in C_{P}^{l}(\\mathbb{R})$ for each $t\\in[0,T]$ . We now provide a Lemma showing the propagation-of-order property of $\\hat{Z}$ : ", "page_idx": 13}, {"type": "text", "text": "Lemma A.1. If $g\\in C_{P}^{l}(\\mathbb{R})$ is convex, so is $\\mathcal{G}_{i}(t,\\cdot)$ for all $t\\in[0,T]$ and $i=1,\\hdots,r$ . ", "page_idx": 13}, {"type": "text", "text": "Proof. For better readability we suppress the superscript and subscript $i$ in the SDE ", "page_idx": 13}, {"type": "equation", "text": "$$\nd\\hat{Z}_{t}^{i}=b_{i}(\\hat{Z}_{t}^{i})d t+\\hat{\\sigma}_{i}(\\hat{Z}_{t}^{i})d B_{t}^{i}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "and consider its Euler-Maruyama approximation ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{Z}_{K,t_{j+1}}=\\hat{Z}_{K,t_{i}}+b(\\hat{Z}_{K,t_{j}})\\Delta t_{j}+\\hat{\\sigma}(\\hat{Z}_{K,t_{j}})(B_{t_{j+1}}-B_{t_{j}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "with $\\begin{array}{r}{t_{j}=j\\frac{T-t}{K}+t,j=\\{0,1,\\cdots,K\\}}\\end{array}$ and $\\begin{array}{r}{\\Delta t_{j}=\\frac{T-t}{K}:=\\Delta}\\end{array}$ . Using Theorem 9.7.4 in [Kloeden and Platen, 1999] we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{G}_{K}(t,x)=\\mathbb{E}[g(\\hat{Z}_{K,T})|\\hat{Z}_{K,t}=x]\\rightarrow\\mathcal{G}(t,x),\\quad t\\in[0,T].}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Let $\\boldsymbol{\\mathcal{A}}$ be a transition operator given by ", "page_idx": 14}, {"type": "equation", "text": "$$\nA S=S+\\Delta b(S)+\\hat{\\sigma}(S)W\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "with $W\\sim N(0,\\Delta)$ . We will show that $\\boldsymbol{\\mathcal{A}}$ satisfies the convex-ordering property ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{E}h(S_{1})\\leqslant\\mathbb{E}h(S_{2})\\Rightarrow\\mathbb{E}h(A S_{1})\\leqslant\\mathbb{E}h(A S_{2})\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "for any convex function $h(\\cdot)$ . Let $S_{1}$ , $S_{2}$ be random vectors which are independent of $W$ and satisfy $\\mathbb{E}h(S_{1})\\leqslant\\mathbb{E}h(S_{2})$ . Due to Strassen\u2019s theorem in [Strassen, 1965], we can also assume that $\\mathbb{E}(S_{2}|S_{1})=\\dot{S}_{1}$ . It follows from conditional Jensen\u2019s inequality that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}h(A S_{2})=\\mathbb{E}h(S_{2}+\\Delta b(S_{2})+\\hat{\\sigma}(S_{2})W)}\\\\ &{\\quad\\quad\\quad=\\mathbb{E}[\\mathbb{E}h(S_{2}+\\Delta b(S_{2})+\\hat{\\sigma}(S_{2})W)|S_{1}]}\\\\ &{\\quad\\quad\\quad\\geqslant\\mathbb{E}[h(\\mathbb{E}(S_{2}|S_{1})+\\Delta\\mathbb{E}(b(S_{2})|S_{1})+\\mathbb{E}(\\hat{\\sigma}(S_{2})|S_{1})W)]}\\\\ &{\\quad\\quad\\quad=\\mathbb{E}[h(S_{1}+\\Delta b(S_{1})+\\mathbb{E}(\\hat{\\sigma}(S_{2})|S_{1})W)]}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Here, the linearity of $b(\\cdot)$ implies $\\mathbb{E}(b(S_{2})|S_{1})=b(S_{1})$ . Note that the function $f(x)=\\sqrt{x^{2}+\\chi}$ is convex. Similarly, $\\sigma(\\cdot)$ is convex. Using conditional Jensen\u2019s inequality again gives ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\varpi(S_{1}):=\\mathbb{E}(\\hat{\\sigma}(S_{2})|S_{1})\\geqslant\\hat{\\sigma}(\\mathbb{E}(S_{2}|S_{1}))=\\hat{\\sigma}(S_{1}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Due to ", "page_idx": 14}, {"type": "equation", "text": "$$\nS_{1}+\\Delta b(S_{1})+\\mathbb{E}(\\hat{\\sigma}(S_{2})|S_{1})W\\sim N(\\mu,\\varpi^{2}),\\quad S_{1}+\\Delta b(S_{1})+\\hat{\\sigma}(S_{1})W\\sim N(\\mu,\\hat{\\sigma}^{2})\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "with $\\mu=\\mathbb{E}(S_{1}+\\Delta b(S_{1}))$ , by Theorem 3.4.7 in [M\u00fcller and Stoyan, 2002], (25) implies that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{E}[h(S_{1}+\\Delta b(S_{1})+\\mathbb{E}(\\hat{\\sigma}(S_{2})|S_{1})W)]\\geqslant\\mathbb{E}[h(S_{1}+\\Delta b(S_{1})+\\hat{\\sigma}(S_{1})W)]=\\mathbb{E}h(\\mathcal{A}S_{1}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Combined with (24) we have proved the convex-ordering property (23). ", "page_idx": 14}, {"type": "text", "text": "By the Markov property of the Euler-Maruyama approximation we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{G}_{K}(t,x)=\\mathbb{E}[g(A^{K}x)].\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Let $Z$ be a Bernoulli random variable which takes the value $\\mathbf{z}_{1}\\in\\mathbb{R}$ with probability $\\mathrm{p}\\in(0,1)$ and the value $z_{1}\\in\\mathbb{R}$ with probability $1-\\mathrm{p}$ . Then $\\mathbb{E}(Z)=\\mathrm{p}z_{1}+(1-\\mathrm{p})z_{2}$ . Then we have ", "page_idx": 14}, {"type": "equation", "text": "$$\nh(\\mathbb{E}(Z))=h(\\mathtt{p}z_{1}+(1-\\mathtt{p})z_{2})\\leqslant\\mathrm{p}h(z_{1})+(1-\\mathtt{p})h(z_{2})=\\mathbb{E}h(Z).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Using the convex-ordering property (23) of the operator $\\boldsymbol{\\mathcal{A}}$ we obtain ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{G}_{K}(t,\\mathrm{p}z_{1}+(1-\\mathrm{p})z_{2})=\\mathcal{G}_{K}(t,\\mathbb{E}(Z))=\\mathbb{E}[g(\\mathcal{A}^{K}\\mathbb{E}(Z))]\\leqslant\\mathbb{E}[g(\\mathcal{A}^{K}Z)]=\\mathcal{G}_{K}(t,Z)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "due to $g$ is convex. Take expectation on both sides of (26) gives ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{G}_{K}(t,\\mathrm{p}z_{1}+(1-\\mathrm{p})z_{2})\\leqslant\\mathbb{E}[\\mathcal{G}_{K}(t,Z)]=\\mathrm{p}\\mathcal{G}_{K}(t,z_{1})+(1-\\mathrm{p})\\mathcal{G}_{K}(t,z_{2}),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which means $\\mathcal{G}_{K}(t,\\cdot)$ is convex. The approximation property (22) implies the convexity of $\\mathcal{G}(t,\\cdot)$ . ", "page_idx": 14}, {"type": "text", "text": "Next, we need a technical result that shows that each process $\\mathcal{G}_{i}(z,Z_{t}^{i})_{t\\in[0,T]}$ is of \u2018class $(\\mathrm{D})^{\\bullet}$ .9 ", "page_idx": 14}, {"type": "text", "text": "Lemma A.2. For each $i=1,\\ldots,r_{i}$ , the process $\\mathcal{G}_{i}(t,Z_{t}^{i})_{t\\in[0,T]}$ is of class (D). ", "page_idx": 14}, {"type": "text", "text": "Proof. Since the solution to (8) is a polynomial process (see example 3.6 in [Cuchiero et al., 2012]), it follows from Theorem 3.1 in [Filipovic\u00b4 and Larsson, 2016] that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{G}_{i}(t,Z_{t}^{i})=\\mathbb{E}[g(\\hat{Z}_{T}^{i})|\\hat{Z}_{t}^{i}=Z_{t}^{i}]=\\exp\\{(T-t)G\\}\\mathrm{P}(Z_{t}^{i}),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where ", "page_idx": 15}, {"type": "equation", "text": "$$\nG=\\left(\\begin{array}{c c c c c c c}{0}&{\\mathrm{g}_{0}}&{2\\times1\\mathrm{g}_{1}}&{0}&{\\cdots}&{0}\\\\ {0}&{\\mathrm{g}_{2}}&{2\\mathrm{g}_{0}}&{3\\times2\\mathrm{g}_{1}}&{0}&{\\vdots}\\\\ {0}&{0}&{2\\left(\\mathrm{g}_{2}+\\mathrm{g}_{3}\\right)}&{3\\mathrm{g}_{0}}&{\\ddots}&{0}\\\\ {0}&{0}&{0}&{3\\left(\\mathrm{g}_{2}+2\\mathrm{g}_{3}\\right)}&{\\ddots}&{p(p-1)\\mathrm{g}_{1}}\\\\ {\\vdots}&{}&{}&{0}&{\\ddots}&{p\\mathrm{g}_{0}}\\\\ {0}&{\\cdots}&{}&{}&{0}&{p\\left(\\mathrm{g}_{2}+(p-1)\\mathrm{g}_{3}\\right)}\\end{array}\\right)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "with ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathrm{g}_{0}=\\theta_{i}\\mu_{i},\\quad\\mathrm{g}_{1}=\\theta_{i}\\phi_{i}\\chi,\\quad\\mathrm{g}_{2}=-\\theta_{i},\\quad\\mathrm{g}_{3}=\\theta_{i}\\phi_{i},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and $\\mathrm{P}(Z_{t}^{i})=(0,1,Z_{t}^{i},(Z_{t}^{i})^{2},\\cdot\\cdot\\cdot\\,,(Z_{t}^{i})^{p})^{\\mathrm{T}}$ . Then there is a constant $C_{T}$ that depends on $T$ such that ", "page_idx": 15}, {"type": "equation", "text": "$$\n|\\mathcal{G}_{i}(t,Z_{t}^{i})|\\leqslant C_{T}(1+|Z_{t}^{i}|^{p}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Let $\\tau_{n}$ be a localizing sequence for $\\mathcal{G}(t,y_{t})$ . Then we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n|\\mathcal{G}_{i}(t\\wedge\\tau_{n},Z_{t\\wedge\\tau_{n}}^{i})|\\leqslant C_{T}(1+|Z_{t\\wedge\\tau_{n}}^{i}|^{p}),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which implies ", "page_idx": 15}, {"type": "equation", "text": "$$\n|{\\mathcal G}_{i}(t\\wedge\\tau_{n},Z_{t\\wedge\\tau_{n}}^{i})|^{2}\\leqslant C_{T}(1+|Z_{t\\wedge\\tau_{n}}^{i}|^{2p}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Taking ${\\mathcal{F}}_{0}$ -condition on both sides of (27) gives ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left\\{|\\mathcal{G}_{i}(t\\wedge\\tau_{n},Z_{t\\wedge\\tau_{n}}^{i})|^{2}\\right\\}\\leqslant C_{T}\\left(1+\\mathbb{E}|Z_{t\\wedge\\tau_{n}}^{i}|^{2p}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leqslant C_{T}\\left(1+\\mathbb{E}\\left[\\underset{n}{\\operatorname*{sup}}|Z_{t\\wedge\\tau_{n}}^{i}|^{2p}\\right]\\right)}\\\\ &{\\qquad\\qquad\\qquad\\leqslant C_{T}e^{C T}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Here, the last inequality holds based on Lemma 2.17 in [Cuchiero et al., 2012]. Thus, we complete the proof of this lemma. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "We are now prepared to give the proof of Theorem 3.1. ", "page_idx": 15}, {"type": "text", "text": "Proof. Let $g$ be a convex function and assume for now that $g\\in C_{P}^{2}(\\mathbb{R})$ . Define the local martingale ", "page_idx": 15}, {"type": "equation", "text": "$$\nL_{t}=\\int_{0}^{t}\\partial_{x}{\\mathcal G}_{i}(s,Z_{s}^{i})\\sigma_{i}(Z_{s})d B_{s}^{i}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Using It\u00f4\u2019s formula in the first step and (21) in the second step, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{{}}&{{\\mathcal{G}_{i}(t,Z_{t}^{i})-\\mathcal{G}_{i}(0,Z_{0}^{i})}}\\\\ {{}}&{{=\\displaystyle\\int_{0}^{t}\\partial_{t}\\mathcal{G}_{i}(s,Z_{s}^{i})d s+\\int_{0}^{t}\\left(b_{i}(Z_{t}^{i})\\partial_{x}+\\frac{\\sigma_{i}^{2}(Z_{t})}{2}\\partial_{x x}\\right)\\mathcal{G}_{i}(s,Z_{s}^{i})d s+L_{t}}}\\\\ {{}}&{{=-\\displaystyle\\int_{0}^{t}\\hat{\\mathcal{L}}_{i}\\mathcal{G}_{i}(s,Z_{s})d s+\\int_{0}^{t}\\left(b_{i}(Z_{t}^{i})\\partial_{x}+\\frac{\\sigma_{i}^{2}(Z_{t})}{2}\\partial_{x x}\\right)\\mathcal{G}_{i}(s,Z_{s}^{i})d s+L_{t}}}\\\\ {{}}&{{=\\displaystyle\\frac{1}{2}\\int_{0}^{t}[\\sigma_{i}^{2}(Z_{s})-\\hat{\\sigma}_{i}^{2}(Z_{s}^{i})]\\partial_{x x}\\mathcal{G}_{i}(s,Z_{s}^{i})d s+L_{t}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "By $\\mathcal{G}_{i}(t,\\cdot)\\in C_{P}^{2}(\\mathbb{R})$ and Lemma A.1 we obtain $\\partial_{x x}\\mathcal{G}_{i}(s,\\cdot)\\geqslant0$ for all $i\\in\\{1,\\ldots,d\\}$ . Thus, due to the ordering of $\\sigma_{i}^{2}$ and ${\\hat{\\sigma}}_{i}^{2}$ , the first term in the right hand side of (28) is nonnegative. Since $L$ is ", "page_idx": 15}, {"type": "text", "text": "a continuous local martingale with zero initial data, it follows that $\\mathcal{G}_{i}(t,Z_{t})-\\mathcal{G}_{i}(0,Z_{0})$ is a local submartingale. ", "page_idx": 16}, {"type": "text", "text": "Let $\\tau_{n}$ be a localizing sequence for $\\mathcal{G}_{i}(t,Z_{t})$ . For all $t\\in[0,T]$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{G}_{i}(t\\wedge\\tau_{n},Z_{t\\wedge\\tau_{n}})-\\mathcal{G}_{i}(0,Z_{0})\\xrightarrow[n\\to\\infty]{a.s.}\\mathcal{G}_{i}(t,Z_{t})-\\mathcal{G}_{i}(0,Z_{0}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Since $\\mathcal{G}_{i}(t,Z_{t})$ is a process of class (D) or locally $L^{p}$ -bounded, $p\\,>\\,1$ , it follows that $\\mathcal{G}_{i}(t\\wedge\\,)$ $\\tau_{n},Z_{t\\wedge\\tau_{n}})-\\mathcal{G}_{i}(0,Z_{0})$ is uniformly integrable. Combining almost-sure convergence with the uniformly integrable property, it implies that the convergence (29) also takes place in $L^{1}$ , and therefore, $\\mathcal{G}_{i}(t,\\dot{Z}_{t})-\\dot{\\mathcal{G}}_{i}(0,\\dot{Z}_{0})$ is a submartingale. By taking expectations on both sides of (28) and using the fact that $Z_{0}=\\hat{Z}_{0}$ , we obtain the comparison result ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}g(Z_{T}^{i})=\\mathbb{E}\\mathcal{G}_{i}(T,Z_{T}^{i})\\geqslant\\mathcal{G}(0,Z_{0}^{i})=\\mathbb{E}[g(\\hat{Z}_{T}^{i})]\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "for all convex $g\\in C_{P}^{2}(\\mathbb{R})$ . ", "page_idx": 16}, {"type": "text", "text": "Now let $g$ be arbitrary convex function on $\\mathbb{R}$ . From Theorem 3.1.4 in [Hiriart-Urruty and Lemar\u00e9chal, 1996] we can find, for each $n\\in\\mathbb N$ a convex Lipschitz function ${\\tilde{g}}_{n}$ such that ${\\tilde{g}}_{n}=g$ in $[-n,n]$ and ${\\tilde{g}}_{n}\\leq g$ in $\\mathbb{R}\\setminus[-n,n]$ . By [Azagra, 2013] we can find further smooth convex functions $\\bar{g_{n}}\\in C_{\\mathrm{Lip}}^{\\bar{\\infty}}(\\mathbb{R})$ such that $\\tilde{g}_{n}-\\frac{1}{n}\\leq g_{n}\\leq\\tilde{g}_{n}$ on all of $\\mathbb{R}$ . It follows that the sequence $g_{n}$ converges pointwise to $g$ from below. We observe that $C_{\\mathrm{Lip}}^{\\infty}(\\mathbb{R})\\subset C_{P}^{2}(\\mathbb{R})$ and equation (11) now follows from (30) by monotone convergence. Finally, equation (12) follows by choosing the convex function $g(z_{i})=|z_{i}|^{p}$ \uff16\u53e3 ", "page_idx": 16}, {"type": "text", "text": "A.4 Proof of Theorem 3.2 (upper bound) ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "From $X_{t}=Q Y_{t}+x_{*}$ , the triangle inequality and the unitary invariance of the Euclidean norm, it follows that $\\left|Y_{t}\\right|\\leq\\left|X_{t}\\right|+\\left|x_{*}\\right|$ . Thus, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{\\beta^{p/2}}{\\lambda_{1}^{p}}\\mathbb{E}[|Z_{t}^{1}|^{p}]=\\mathbb{E}[|Y_{t}^{1}|^{p}]\\le\\mathbb{E}[|Y_{t}|^{p}]\\le2^{p}\\left(\\mathbb{E}[|X_{t}|^{p}]+|x_{*}|^{p}\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Now, let $p>\\nu_{1}$ . By Theorem 3.1, Fatou\u2019s Lemma, and the properties of the distribution (9) or (10) ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{t\\to\\infty}\\operatorname{\\mathrm{sup}}_{\\mathbb{}}\\mathbb{E}[|Z_{t}^{1}|^{p}]\\geq\\operatorname*{lim}_{t\\to\\infty}\\operatorname{\\mathrm{id}}_{t\\to\\infty}|\\mathbb{E}[|Z_{t}^{1}|^{p}]\\geq\\operatorname*{lim}_{t\\to\\infty}\\operatorname{\\mathrm{inf}}_{\\mathbf{}}\\mathbb{E}[|\\hat{Z}_{t}^{1}|^{p}]\\geq\\mathbb{E}[|\\hat{Z}_{\\infty}^{1}|^{p}]=\\infty.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Together with (31) this implies that also ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{t\\to\\infty}\\operatorname{\\mathbb{E}}[|X_{t}|^{p}]=\\infty,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and it follows from (5) that the tail index satisfies $\\eta\\le p$ for all $p>\\nu_{1}$ . Finally, the parameter $\\nu_{1}$ in the limit distribution of ${\\hat{Z}}^{1}$ is given by $\\nu_{1}=1+\\phi_{1}^{-1}$ , where $\\phi_{1}$ can be found in (6). Thus, we obtain Theorem 3.2. ", "page_idx": 16}, {"type": "text", "text": "A.5 Proof of Theorem 3.3 (lower bound) ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "For better readability, we rewrite (hSGD) in the form ", "page_idx": 16}, {"type": "equation", "text": "$$\nd X_{t}=F(X_{t})d t+G(X_{t})d B_{t}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "with ", "page_idx": 16}, {"type": "equation", "text": "$$\nF(X_{t})=-\\frac{\\gamma}{n}\\left[A^{\\mathrm{T}}(A X_{t}-b)+\\delta X_{t}\\right],\\quad G(X_{t})=\\frac{\\gamma}{n}\\sqrt{\\frac{1}{B}|A X_{t}-b|^{2}A^{\\mathrm{T}}A}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Our goal is to show that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{|x|\\to\\infty}\\frac{(1+|x|^{2})\\left[2x^{\\mathrm{T}}F(x)+|G(x)|^{2}\\right]-(2-\\rho)|x^{\\mathrm{T}}G(x)|^{2}}{|x|^{4}}<-C_{1},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "for all $\\rho\\in(0,\\eta_{*})$ , where $C_{1}$ is a positive constant and ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\eta_{*}:=1+\\frac{2n(\\lambda_{1}^{2}+n\\delta)}{\\gamma\\lambda_{1}^{4}}-\\frac{\\sum_{i=2}^{d}\\lambda_{i}^{2}}{\\lambda_{1}^{2}}>0.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Under condition (33) it follows directly from Theorem 5.2 in [Li et al., 2019] that the solution $X_{t}$ of the SDE (32)) satisfies ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{0\\leqslant t<\\infty}\\mathbb{E}|X_{t}|^{\\rho}\\leqslant C_{2}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "with $C_{2}$ a positive constant, showing Theorem 3.3. ", "page_idx": 17}, {"type": "text", "text": "In order to show (33), let ", "page_idx": 17}, {"type": "equation", "text": "$$\nM(\\boldsymbol{x}):=\\frac{x^{\\mathrm{T}}A^{\\mathrm{T}}A x}{|x|^{2}},\\quad\\boldsymbol{x}\\in\\mathbb{R}^{d}\\setminus\\{0\\}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "denote the Rayleigh-quotient of $A^{\\mathrm{T}}A$ . From Chapter 1 in [Horn and Johnson, 2012] we have that the range of $M(x)$ is equal to the line segment $[\\lambda_{r}^{2},\\bar{\\lambda}_{1}^{2}]$ , i.e., ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left\\{M(x):x\\in\\mathbb{R}^{d}\\setminus\\{0\\}\\right\\}=[\\lambda_{r}^{2},\\lambda_{1}^{2}]\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Evaluating the condition (33), we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\left(1+|x|^{2}\\right)\\big[2x^{\\mathrm{T}}F(x)\\big]}{|x|^{4}}}\\\\ &{=\\frac{\\left(1+|x|^{2}\\right)\\big\\{-2\\frac{\\gamma}{n}x^{\\mathrm{T}}\\big[A^{\\mathrm{T}}\\left(A x-b\\right)+\\delta x\\big]\\big\\}}{|x|^{4}}}\\\\ &{=\\frac{\\left(1+|x|^{2}\\right)\\big[-2\\frac{\\gamma}{n}x^{\\mathrm{T}}\\left(A^{\\mathrm{T}}A+\\delta I_{r}\\right)x+2\\frac{\\gamma}{n}x^{\\mathrm{T}}A^{\\mathrm{T}}b\\big]}{|x|^{4}}}\\\\ &{=-\\frac{2\\frac{\\gamma}{n}x^{\\mathrm{T}}\\left(A^{\\mathrm{T}}A+\\delta I_{r}\\right)x}{|x|^{4}}-\\frac{2\\frac{\\gamma}{n}x^{\\mathrm{T}}\\left(A^{\\mathrm{T}}A+\\delta I_{r}\\right)x}{|x|^{2}}+\\frac{2\\frac{\\gamma}{n}\\left(1+|x|^{2}\\right)x^{\\mathrm{T}}A^{\\mathrm{T}}b}{|x|^{4}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{(1+|x|^{2})|G(x)|^{2}-(2-\\rho)|x^{\\top}G(x)|^{2}}{|x|^{4}}}\\\\ &{=\\frac{(1+|x|^{2})\\left[\\frac{\\gamma^{2}}{n^{2}B}\\sqrt{\\sqrt{|A x-b|^{2}A^{\\top}A}}|^{2}\\right]-(2-\\rho)\\frac{\\gamma^{2}}{n^{2}B}|x^{\\top}\\sqrt{|A x-b|^{2}A^{\\top}A}|^{2}}{|x|^{4}}}\\\\ &{=\\frac{\\frac{\\gamma^{2}}{n^{2}}(1+|x|^{2})|A x-b|^{2}|\\sqrt{A^{\\top}A}|^{2}-(2-\\rho)\\frac{\\gamma^{2}}{n^{2}}|A x-b|^{2}|x^{\\top}\\sqrt{A^{\\top}A}|^{2}}{|x|^{4}}}\\\\ &{=\\frac{\\frac{\\gamma^{2}}{n^{2}B}|A x-b|^{2}|\\sqrt{A^{\\top}A}|^{2}}{|x|^{4}}+\\frac{\\frac{\\gamma^{2}B}{n^{2}B}|A x-b|^{2}|\\sqrt{A^{\\top}A}|^{2}}{|x|^{2}}}\\\\ &{\\phantom{=}-\\frac{(2-\\rho)\\frac{\\gamma^{2}}{n^{2}B}|A x-b|^{2}}{|x|^{2}}\\frac{x\\,\\mathrm{T}A x-b|^{2}\\,x^{\\top}A x}{|x|^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "With $|{\\sqrt{A^{\\mathrm{T}}A}}|^{2}=\\operatorname{tr}(A^{\\mathrm{T}}A)$ and the positive constant $\\rho$ given below, we obtain ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{lim}\\operatorname*{sup}\\displaystyle\\frac{(1+|x|^{2})\\left[2x^{\\mathrm{T}}F(x)+|G(x)|^{2}\\right]-(2-\\rho)|x^{\\mathrm{T}}G(x)|^{2}}{|x|^{4}}}\\\\ &{=\\operatorname*{lim}\\operatorname*{sup}\\Big[-\\frac{2\\frac{\\gamma}{n}x^{\\mathrm{T}}\\left(A^{\\mathrm{T}}A+\\delta I_{n}\\right)x}{|x|^{2}}+\\frac{\\frac{\\gamma^{2}}{n^{2}B}|A x-b|^{2}|\\sqrt{A^{\\mathrm{T}}}A|^{2}}{|x|^{2}}}\\\\ &{\\quad-\\frac{(2-\\rho)\\frac{\\gamma^{2}}{n^{2}B}|A x-b|^{2}}{|x|^{2}}\\frac{x^{\\mathrm{T}}A^{\\mathrm{T}}A x}{|x|^{2}}\\Big]}\\\\ &{=-\\frac{\\gamma^{2}}{n^{2}B}\\operatorname*{liminf}_{|x|\\rightarrow\\infty}\\Big[\\frac{2n B(M(x)+\\delta)}{\\gamma}-\\mathrm{tr}(A^{\\mathrm{T}}A)M(x)+(2-\\rho)M(x)^{2}\\Big]}\\\\ &{=-\\frac{\\gamma^{2}}{n^{2}B}\\operatorname*{lim}_{m\\in[\\lambda,\\lambda]^{2}}q(m,\\rho),}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where ", "page_idx": 18}, {"type": "equation", "text": "$$\nq(m,\\rho)=\\frac{2n B(m+\\delta)}{\\gamma}-\\mathrm{tr}(A^{\\mathrm{T}}A)m+(2-\\rho)m^{2}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Set ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\vartheta:=2+\\frac{2n B(\\lambda_{1}^{2}+\\delta)}{\\gamma\\lambda_{1}^{4}}-\\frac{\\sum_{i=1}^{d}\\lambda_{i}^{2}}{\\lambda_{1}^{2}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Note that due to the assumption $\\gamma<\\bar{\\gamma}$ we have $\\vartheta>2$ . We claim that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{m\\in[\\lambda_{r}^{2},\\lambda_{1}^{2}]}q(m,\\rho)>q(\\lambda_{1}^{2},\\theta)=0\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for all $\\rho\\in[2,\\vartheta)$ . First, note that $m\\mapsto q(m,\\rho)$ is concave for any $\\rho\\in[2,\\vartheta)$ , such that its minimum must be attained at one of the boundary values $m\\in\\{\\lambda_{r}^{2},\\lambda_{1}^{2}\\}$ . Second, note that $\\rho\\mapsto q(m,\\rho)$ is strictly decreasing for any $m\\in(0,\\infty)$ , such that for (37) it is sufficient to show ", "page_idx": 18}, {"type": "equation", "text": "$$\nq(\\lambda_{r}^{2},\\theta)\\ge q(\\lambda_{1}^{2},\\theta)=0.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Using the assumption $\\gamma<\\bar{\\gamma}$ we obtain ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathfrak{q}(\\lambda_{r}^{2},\\theta)=\\frac{2n B}{\\gamma}(\\lambda_{r}^{2}+\\delta)-\\mathrm{tr}(A^{\\mathrm{T}}A)\\lambda_{r}^{2}+\\frac{2n B}{\\gamma}(\\lambda_{1}^{2}+\\delta)\\frac{\\lambda_{r}^{4}}{\\lambda_{1}^{4}}-\\mathrm{tr}(A^{\\mathrm{T}}A)\\frac{\\lambda_{r}^{4}}{\\lambda_{1}^{2}}}\\\\ {\\displaystyle\\geq\\mathrm{tr}(A^{\\mathrm{T}}A)\\left(\\frac{(\\lambda_{r}^{2}+\\delta)}{(\\lambda_{1}^{2}+\\delta)}\\lambda_{1}^{2}-\\lambda_{r}^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "For $\\delta=0$ the right hand side vanishes and (38) is shown. Differentiation shows that the right hand side is increasing in $\\delta$ , such that (38) holds for all $\\delta\\geq0$ . Altogether, we have shown that the right hand side of (35) is strictly negative. Thus, the SDE (32) satisfies the Assumption 5.1 in [Li et al., 2019]. Based on Theorem 5.2 in Li et al. [2019], the solution $X_{t}$ of the SDE (32) satisfies ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{0\\leqslant t<\\infty}\\mathbb{E}|X_{t}|^{\\rho}\\leqslant C\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for all $\\rho\\in[2,\\vartheta)$ . Therefore, the lower bound, denoted by $\\eta_{*}$ , for the asymptotic tail index of $X_{t}$ is ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\eta_{*}=\\vartheta=1+\\frac{2n B(\\lambda_{1}^{2}+\\delta)}{\\gamma\\lambda_{1}^{4}}-\\frac{\\sum_{i=2}^{d}\\lambda_{i}^{2}}{\\lambda_{1}^{2}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "A.6 Wasserstein convergence ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Lemma A.3. Let $Z$ and $\\tilde{Z}$ be two strong solutions of (7) with possibly different initial conditions $Z_{0},\\tilde{Z}_{0}\\in\\mathbb{R}^{r}$ . Suppose that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\gamma<\\gamma^{\\prime}=:\\frac{n B}{2}\\left\\{\\sum_{i=1}^{r}\\frac{\\lambda_{i}^{4}}{\\lambda_{i}^{2}+\\delta}\\right\\}^{-1}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then the equation ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{r}\\frac{\\lambda_{i}^{4}}{\\lambda_{i}^{2}+\\delta-n\\rho/\\gamma}=\\frac{n B}{2\\gamma}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "has a unique positive solution $\\rho_{*}>0$ and there exist constants $C,C^{\\prime}$ independent of $Z_{0},\\tilde{Z}_{0}$ , such that ", "page_idx": 18}, {"type": "text", "text": "and ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left|Z_{t}-\\tilde{Z}_{t}\\right|^{2}\\right]\\leq C e^{-2t\\rho_{*}}\\left|Z_{0}-\\tilde{Z}_{0}\\right|^{2}}\\\\ &{\\qquad\\quad\\mathbb{E}\\left[\\left|Z_{t}\\right|^{2}\\right]\\leq C^{\\prime}e^{-2t\\rho_{*}}\\left|Z_{0}\\right|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. We set $\\b=(\\mu_{1},\\b+\\ldots,\\mu_{r}),\\,\\Theta=\\mathrm{diag}(\\theta_{1},\\b+\\ldots,\\theta_{r}),\\,\\psi=(2\\phi_{1}\\theta_{1},\\b+\\ldots,2\\phi_{r}\\theta_{r})$ , and transform $Z$ into $\\stackrel{\\triangledown}{V_{t}}:=e^{\\ominus t}(Z_{t}-\\bar{\\mu})$ . Applying Ito\u2019s formula, we see that $V$ can be written as ", "page_idx": 18}, {"type": "equation", "text": "$$\nV_{t}=Z_{0}+\\int_{0}^{t}e^{\\Theta s}\\sqrt{\\mathrm{diag}(\\psi_{1},\\ldots,\\psi_{r})(|Z_{s}|^{2}+\\chi)}d B_{s}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The same representation holds for $\\tilde{V}$ in relation to $\\tilde{Z}$ . Setting $d(z,z^{\\prime})=\\sqrt{|z|^{2}+\\chi}-\\sqrt{|z^{\\prime}|^{2}+\\chi},$ we estimate ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left|V_{t}^{i}-\\tilde{V}_{t}^{i}\\right|^{2}\\leq4\\left\\{\\left|Z_{0}^{i}-\\tilde{Z}_{0}^{i}\\right|^{2}+\\psi_{i}\\cdot\\left(\\int_{0}^{t}e^{\\theta_{i}s}d(Z_{s},\\tilde{Z}_{s})d B_{s}^{i}\\right)^{2}\\right\\},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "for each $i=1\\ldots r$ . Using Ito isometry and the Lipschitz property $d(z,z^{\\prime})\\leq|z-z^{\\prime}|.$ , we obtain ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left|V_{t}^{i}-\\tilde{V}_{t}^{i}\\right|^{2}\\right]\\leq4\\left\\{\\left|Z_{0}^{i}-\\tilde{Z}_{0}^{i}\\right|^{2}+\\psi_{i}\\int_{0}^{t}e^{2\\theta_{i}s}\\mathbb{E}\\left[\\left|Z_{s}-Z_{s}^{\\prime}\\right|^{2}\\right]d s\\right\\}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Introducing $D_{t}\\,=\\,(D_{t}^{1},\\ldots,D_{t}^{r})$ , where $D_{t}^{i}=\\mathbb{E}\\left[\\left|Z_{t}^{i}-{\\tilde{Z}}_{t}^{i}\\right|^{2}\\right]$ and $M=\\psi\\mathbf{1}\\mathsf{\\bar{1}}=(\\psi_{i})_{i,j}$ , where $\\mathbf{1}=(1,\\ldots,1)$ , we can combine these inequalities into the vector-valued inequality ", "page_idx": 19}, {"type": "equation", "text": "$$\nD_{t}\\leq4\\left\\{e^{-2\\Theta t}D_{0}+e^{-2\\Theta t}\\int_{0}^{t}e^{2\\Theta s}M D_{s}d s\\right\\}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Now, consider the comparison equality ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\hat{D}_{t}=4\\left\\{e^{-2\\Theta t}D_{0}+e^{-2\\Theta t}\\int_{0}^{t}e^{2\\Theta s}M\\hat{D}_{s}d s\\right\\}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Differentiation shows that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{d}{d t}\\hat{D}_{t}=-2(\\Theta-2M)\\hat{D}_{t}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Applying the comparison result of Beesack [1969], we obtain ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[|Z_{s}-Z_{s}^{\\prime}|^{2}\\right]=\\mathbf{1}^{\\top}D_{t}\\leq\\mathbf{1}^{\\top}\\hat{D}_{t}=\\mathbf{1}^{\\top}e^{-2t(\\Theta-2M)}D_{0}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Hence, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[|Z_{s}-Z_{s}^{\\prime}|^{2}\\right]\\le C e^{-2\\rho_{*}t}|Z_{0}-Z_{0}^{\\prime}|^{2}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\rho_{*}$ is the smallest Eigenvalue of $\\Theta-2M$ . ", "page_idx": 19}, {"type": "text", "text": "Now, $M=\\psi\\mathbf{1}^{\\top}$ , i.e., $\\Theta-2M$ can be considered a rank-one perturbation of the diagonal matrix $\\Theta$ . By [Anderson, 1996], the Eigenvalues $\\rho_{1},\\ldots,\\rho_{r}$ of $\\Theta-2M$ are solutions of the secular equation ", "page_idx": 19}, {"type": "equation", "text": "$$\nF(\\rho):=1-\\sum_{i=0}^{r}\\frac{2\\psi_{i}}{\\theta_{i}-\\rho}=0.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Moreover, they interlace the diagonal values of $\\Theta$ , i.e., we have $\\rho_{*}=\\rho_{1}<\\theta_{1}<\\rho_{2}<\\cdot\\cdot<\\rho_{r}<\\theta_{r}$ . Therefore, all Eigenvalues of $\\Theta-2M$ are positive, except for $\\rho_{*}$ which may be either positive or negative. On $(-\\infty,\\theta_{1})$ the function $F$ is strictly decreasing from 1 to $-\\infty$ , such that its root $\\rho_{*}$ satisfies $\\rho_{*}>0$ if and only $F(0)>0$ . Rewriting this condition in terms of (6) yields (39); doing the same for the secular equation (42) yields (40). This completes the proof for the estimate of $\\mathbb{E}\\left[|Z_{s}-Z_{s}^{\\prime}|\\right]^{2}$ ; the proof for $\\mathbb{E}\\left[|Z_{s}|^{2}\\right]$ is completely analogous. \u53e3 ", "page_idx": 19}, {"type": "text", "text": "We are now prepared for the proof of Theorem 3.4, which uses some key ideas from [Friesen et al., 2020]: ", "page_idx": 19}, {"type": "text", "text": "Proof. Let $(Z_{t})_{t\\geq0}$ be the unique strong solution of (7) and denote by $p_{t}(z,d\\zeta)$ its Markov transition kernel. Moreover, for any Borel measure $\\mu$ on $\\mathbb{R}^{r}$ set ", "page_idx": 19}, {"type": "equation", "text": "$$\nP_{t}\\mu(d\\zeta):=\\int_{\\mathbb{R}^{r}}p_{t}(z,d\\zeta)\\mu(d z).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Note that $P_{t+s}=P_{t}P_{s}=P_{s}P_{t}$ by the Markov property of $Z$ . Denote by $\\mathcal{P}_{2}$ the set of all Borel measures $\\mu$ on ${\\mathbb R}^{r}$ with $\\begin{array}{r}{\\int|z|^{2}\\mu(d z)<\\infty}\\end{array}$ . From Lemma A.3 we see that under condition (39) $P_{t}$ maps $\\mathcal{P}_{2}$ into $\\mathcal{P}_{2}$ for any $t\\geq0$ . Moreover, the contraction estimate in Lemma A.3 implies that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{W}_{2}(P_{t}\\delta_{z},P_{t}\\delta_{z^{\\prime}})\\leq C e^{-t\\rho_{*}}|z-z^{\\prime}|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "with $\\delta_{z},\\delta_{z^{\\prime}}$ the Dirac measures in $z$ and $z^{\\prime}$ respectively. Using the convexity of the 2-Wasserstein distance (cf. Sec. A.2 in [Friesen et al., 2020]), it now follows that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{W}_{2}(P_{t}\\mu,P_{t}\\nu)\\leq C e^{-t\\rho_{*}}\\mathcal{W}_{2}(\\mu,\\nu)}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "for any $\\mu,\\nu$ in $\\mathcal{P}_{2}$ . ", "page_idx": 20}, {"type": "text", "text": "Let $\\mu\\in\\mathcal \u1e0a P \u1e0c _{2}$ . For any $n,k\\in\\ensuremath{\\mathbb{N}}_{0}$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{W}_{2}(P_{n+k}\\mu,P_{n}\\mu)=\\mathcal{W}_{2}(P_{n}P_{k}\\mu,P_{n}\\mu)\\leq C e^{-n\\rho_{*}}\\mathcal{W}_{2}(P_{k}\\mu,\\mu),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which shows that $(P_{n}\\mu)_{n\\in\\mathbb{N}_{0}}$ is a Cauchy sequence in $(\\mathcal{P}_{2},\\mathcal{W}_{2})$ . In particular there exists a limit $\\pi\\in\\mathcal{P}_{2}$ such that $\\begin{array}{r}{\\operatorname*{lim}_{n\\rightarrow\\infty}\\dot{\\mathcal{W}}_{2}(P_{n}\\mu,\\pi)=0}\\end{array}$ . Next, we show that $\\pi$ is an invariant measure for $Z$ Indeed, for any $h>0$ and $k\\in\\mathbb N$ , we can estimate ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{W}_{2}(P_{h}\\pi,\\pi)\\leq\\mathcal{W}_{2}(P_{h}\\pi,P_{h}P_{k}\\mu)+\\mathcal{W}_{2}(P_{k}P_{h}\\mu,P_{k}\\mu)+\\mathcal{W}_{2}(P_{k}\\mu,\\pi)\\leq}\\\\ &{\\qquad\\qquad\\leq C e^{-h\\rho_{*}}\\mathcal{W}_{2}(\\pi,P_{k}\\mu)+C e^{-k\\rho_{*}}\\mathcal{W}_{2}(P_{h}\\mu,\\mu)+\\mathcal{W}_{2}(\\pi,P_{k}\\mu),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the right hand side tends to zero as $k\\rightarrow\\infty$ . Finally, we show that the invariant measure $\\pi$ is unique. Suppose that there is another invariant measure $\\pi^{\\prime}\\in\\mathcal{P}_{2}$ . Then ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{W}_{2}(\\pi,\\pi^{\\prime})=\\mathcal{W}_{2}(P_{n}\\pi,P_{n}\\pi^{\\prime})\\leq C e^{-n\\rho_{*}}\\mathcal{W}_{2}(\\pi,\\pi^{\\prime}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which tends to zero as $n\\to\\infty$ . Together, this shows that under the conditions of Lemma A.3, $Z$ converges in $\\mathcal{W}_{2}$ -distance to its unique invariant distribution $\\pi$ , and hence completes the proof of Theorem 3.4. ", "page_idx": 20}, {"type": "text", "text": "A.7 Parameter Values ", "text_level": 1, "page_idx": 20}, {"type": "table", "img_path": "EFrgBP9au6/tmp/0ff4c0fa923cda4f5d22dcbb353234b058822c790e1d8e88d8ee7cfbdfcb8889.jpg", "table_caption": ["Table 3: Parameters used for Figure 1 "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "EFrgBP9au6/tmp/347e10bf9f1032269f3b88308ab4c5aa6eed99c8fcf29f6e9492516b85698c0f.jpg", "table_caption": ["Table 4: Parameters used for Figure 2 "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "A.8 Experimental configuration ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The computing device that we use for calculating our examples includes a single Intel Core i7-10710U CPU with 16GB memory. Our code is available at: https://github.com/zhezhejiao/hSGD. ", "page_idx": 20}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: see the Section 1.1. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 21}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: see the Sections 2.6 and 3.4. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 21}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: See the Supplement A.1-A.6 for all the proofs. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 22}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The experiments in the paper are reproducible; code will be released. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 22}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: See the Supplement A.8. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 23}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: See the Supplement A.7. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 23}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: see the Table 2. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: See the Supplement A.8. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 24}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The research in this paper conform with the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 24}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: There is no societal impact of the work performed. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to ", "page_idx": 24}, {"type": "text", "text": "generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 25}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 25}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 25}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: See the Supplement A.8. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 25}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: See the Supplement A.8. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 26}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 26}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 26}]