[{"figure_path": "apI1GltwSx/figures/figures_0_1.jpg", "caption": "Figure 1: Distill datasets to IPCN requires N*T iterations in traditional distillation processes (left) but fewer iteration processes (right).", "description": "This figure compares the computational cost of traditional dataset distillation methods and the proposed DELT method. Traditional methods require N \u00d7 T iterations for IPCN, where N is the number of images per class and T is the number of training iterations. The DELT method reduces the number of iterations needed by partitioning predefined IPC samples into smaller subtasks and employing local optimizations. This reduces the number of iterations to T + (T \u2212 RI) + ... + RI, where RI is the round iteration.  The figure visually illustrates this by showing a comparison between the two methods, highlighting that DELT requires significantly fewer iterations by optimizing the images from the subtasks in different phases.", "section": "1 Introduction"}, {"figure_path": "apI1GltwSx/figures/figures_1_1.jpg", "caption": "Figure 2: Left: Intra-class semantic cosine similarity after a pretrained ResNet-18 model on ImageNet-1K dataset, lower values are better. Right: Synthetic images from SRe2L, CDA and our DELT.", "description": "The figure shows a comparison of the intra-class semantic similarity and the generated synthetic images of different methods. The left panel displays the cosine similarity between samples of 50 classes in the ImageNet-1K dataset after passing through a pre-trained ResNet-18 model. Lower values indicate higher diversity and better separation between classes. The right panel shows sample synthetic images generated by SRe2L, CDA, and the proposed DELT method.  The goal is to illustrate that DELT produces more diverse synthetic images compared to SRe2L and CDA, as shown by the lower cosine similarity.", "section": "1 Introduction"}, {"figure_path": "apI1GltwSx/figures/figures_2_1.jpg", "caption": "Figure 3: Batch-to-batch vs. batch-to-global matching in dataset distillation. Of indicates weights are pretrained and frozen in this stage.", "description": "This figure illustrates the difference between batch-to-batch and batch-to-global matching mechanisms in dataset distillation.  Batch-to-batch matching involves matching gradients, trajectories, or features between local batches of the original and synthetic data. Batch-to-global matching uses global statistics (like mean and variance from Batch Normalization layers) derived from the entire original dataset to guide the generation of the synthetic dataset.  The weights (\u03b8) are pretrained and frozen for batch-to-global matching, while batch-to-batch matching involves updating the weights.", "section": "Related Work"}, {"figure_path": "apI1GltwSx/figures/figures_3_1.jpg", "caption": "Figure 4: The proposed DELT learning procedure via a multi-round EarlyLate scheme.", "description": "This figure illustrates the DELT (Diversity-driven Early-Late Training) learning procedure, a multi-round training scheme designed for dataset distillation.  It shows how predefined IPC (Images Per Class) samples are partitioned into smaller subtasks. Each subtask undergoes local optimization in distinct phases (Early and Late), enhancing the diversity of the generated images.  The output from each round of subtask optimizations is then concatenated and used as input for the next round, leading to a more diverse and efficient distillation process.", "section": "3 Our Approach"}, {"figure_path": "apI1GltwSx/figures/figures_3_2.jpg", "caption": "Figure 5: Selection criteria with a teach ranker.", "description": "This figure illustrates the process of selecting real image patches to initialize synthetic images.  A teacher ranker (a pre-trained model) assigns probabilities to image patches.  Patches with the lowest, medium, or highest probabilities are selected as initialization for the synthetic images in different groups. This diverse selection of initializations is a key component in enhancing the diversity of the generated images.", "section": "3 Our Approach"}, {"figure_path": "apI1GltwSx/figures/figures_6_1.jpg", "caption": "Figure 6: Mosaic splicing patterns on ImageNet-1K using real image patches as the initialization. In each block, the left column is the starting real image initialized samples and right is the final optimized syntheses. From top to bottom are images generated by early training and late training.", "description": "This figure shows the effect of different mosaic splicing patterns on the quality of synthesized images in the ImageNet-1K dataset.  The figure demonstrates how the starting image (left column) is transformed through the optimization process (the arrows show the flow), resulting in the final optimized synthesized image (right column). Each block represents a different pattern: 1x1, 2x2, 3x3, and 4x4. The images are arranged to show the progression from early training (top) to late training (bottom) in each pattern.", "section": "4.3 Ablation Study"}, {"figure_path": "apI1GltwSx/figures/figures_8_1.jpg", "caption": "Figure 7: Distilled dataset visualization compared with other image optimization-based methods.", "description": "This figure shows a comparison of synthetic images generated by different dataset distillation methods, including Minimax Diffusion, MTT, IDC, SRe2L, SCDD, CDA, G-VBSM, and the proposed DELT method.  The goal is to visualize the diversity and quality of the synthesized images produced by each method.  Each row represents a different class of images from the original dataset, and each column displays images generated by a particular method.  The 'Real' column shows actual images from the original dataset, providing a baseline for comparison.  The figure visually demonstrates that the proposed DELT method generates images that are both more diverse and realistic compared to the other methods. The images generated by other methods often show artifacts, distortions, or lack of detail.", "section": "Visualization of DELT"}, {"figure_path": "apI1GltwSx/figures/figures_8_2.jpg", "caption": "Figure 8: Continual learning results.", "description": "This figure shows the results of a continual learning experiment on ImageNet-1K, comparing the performance of DELT against G-VBSM and SRe\u00b2L. The x-axis represents the number of classes, and the y-axis represents the top-1 accuracy. The graph shows that DELT significantly outperforms G-VBSM, with an average improvement of about 10% in the 100-step class-incremental learning task. This highlights the benefits of DELT in mitigating the challenges of continual learning.", "section": "4.7 Application II: Continual Learning"}, {"figure_path": "apI1GltwSx/figures/figures_13_1.jpg", "caption": "Figure 6: Mosaic splicing patterns on ImageNet-1K using real image patches as the initialization. In each block, the left column is the starting real image initialized samples and right is the final optimized syntheses. From top to bottom are images generated by early training and late training.", "description": "This figure demonstrates different mosaic splicing patterns used for initializing synthetic images in the ImageNet-1K dataset. Each block shows a comparison between the initial real image patch used and the final optimized synthetic image generated after early and late training.  The patterns vary from 1x1 to 4x4 patches, showcasing the impact of different initialization strategies on the final synthesized images.", "section": "4 Experiments"}, {"figure_path": "apI1GltwSx/figures/figures_14_1.jpg", "caption": "Figure 10: Synthetic image visualizations on ImageNette generated by our DELT.", "description": "This figure displays a set of synthetic images generated by the DELT (Diversity-driven Early-Late Training) method on the ImageNette dataset.  Each column likely represents a different class within ImageNette, showing how the algorithm produces images for each class with varying degrees of optimization. The images at the top of each column likely underwent more optimization iterations, while those towards the bottom underwent fewer. This illustrates DELT's strategy of achieving diversity in synthetic data by applying varying levels of optimization to different samples.", "section": "More Visualizations"}, {"figure_path": "apI1GltwSx/figures/figures_15_1.jpg", "caption": "Figure 10: Synthetic image visualizations on ImageNette generated by our DELT.", "description": "This figure shows a grid of 100 synthetic images generated by the proposed DELT method on the ImageNette dataset.  Each row represents a different class within ImageNette, and the images within each row illustrate the diversity achieved by the DELT algorithm through its early-late training scheme, showcasing varying degrees of optimization and image quality.  The images demonstrate the capability of DELT to generate a diverse set of synthetic images for each class, enhancing the training efficiency and improving the generalization performance of machine learning models.", "section": "4 Experiments"}]