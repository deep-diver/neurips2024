[{"figure_path": "gBOQ0ACqoO/figures/figures_1_1.jpg", "caption": "Figure 1: Statistical and visualization analysis on the nuScenes-mini dataset. (a) The average numbers of points and pixels for each object at different depths. (b) Examples of near-range and long-range objects in images and point cloud. Points within the bounding boxes are colored red for observation.", "description": "This figure presents a statistical and visual analysis of the relationship between object representation (points in LiDAR point cloud and pixels in RGB images) and depth. (a) shows a bar chart indicating the average number of points and pixels per object across different depth ranges (0-10m, 10-20m, etc.).  (b) displays visualizations illustrating the difference in object representation in LiDAR and images at near and far ranges. Red points highlight points within the bounding box of detected objects, showing higher point density at shorter distances.", "section": "1 Introduction"}, {"figure_path": "gBOQ0ACqoO/figures/figures_3_1.jpg", "caption": "Figure 2: Overview of our method. It introduces depth encoding in both global and local feature fusion to obtain depth-adaptive multi-modal representations for detection.  \u2297 is the multiplication operation, and \u2295 is the merge operation.", "description": "This figure illustrates the overall architecture of the proposed Depth-Aware Hybrid Feature Fusion (DH-Fusion) method for 3D object detection.  It shows the flow of data from input (LiDAR point cloud and multi-view images) through encoding, depth-aware global and local feature fusion, and finally decoding and detection. The figure highlights the key components: 3D and 2D encoders for feature extraction, depth encoders to incorporate depth information, global and local fusion transformers for integrating features from LiDAR and image modalities based on depth, and a detection head for generating 3D bounding boxes and object categories.", "section": "3 Methodology"}, {"figure_path": "gBOQ0ACqoO/figures/figures_4_1.jpg", "caption": "Figure 3: Illustration of the DGF. It consists of a global fusion transformer with the depth encoder.", "description": "The figure illustrates the Depth-Aware Global Feature Fusion (DGF) module, a key component of the proposed DH-Fusion method.  The DGF module takes point cloud BEV features and image BEV features as input. A depth encoder generates a depth matrix which is used to dynamically adjust the weights of image BEV features during fusion. The global fusion transformer then combines these features using cross attention and feed-forward neural networks, generating depth-adaptive multi-modal global features.", "section": "3.2 Depth-Aware Global Feature Fusion"}, {"figure_path": "gBOQ0ACqoO/figures/figures_4_2.jpg", "caption": "Figure 2: Overview of our method. It introduces depth encoding in both global and local feature fusion to obtain depth-adaptive multi-modal representations for detection. \u25cf is the multiplication operation, and \u25cf is the merge operation.", "description": "This figure shows the overall architecture of the proposed Depth-Aware Hybrid Feature Fusion (DH-Fusion) method.  The method takes LiDAR point cloud and multi-view images as input. These inputs are encoded separately using a 3D encoder and a 2D encoder. The encoded features are then fused using two key components: Depth-Aware Global Feature Fusion (DGF) and Depth-Aware Local Feature Fusion (DLF). The DGF module dynamically adjusts the weights of image BEV features based on depth. DLF compensates for information loss during the BEV transformation by utilizing the original instance features. The final fused features are then used for object detection.", "section": "3 Methodology"}, {"figure_path": "gBOQ0ACqoO/figures/figures_8_1.jpg", "caption": "Figure 5: Attention weights applied on BEV image features in DGF vary with depth.", "description": "The figure shows the attention weights applied on BEV image features in the Depth-Aware Global Feature Fusion (DGF) module.  The weights change dynamically as depth varies, indicating that the image modality plays different roles at different depths. The graph shows how the feature weights increase significantly as the distance to the object increases, demonstrating the adaptive nature of the DGF module.", "section": "3.2 Depth-Aware Global Feature Fusion"}, {"figure_path": "gBOQ0ACqoO/figures/figures_21_1.jpg", "caption": "Figure 2: Overview of our method. It introduces depth encoding in both global and local feature fusion to obtain depth-adaptive multi-modal representations for detection.  is the multiplication operation, and  is the merge operation.", "description": "This figure provides a high-level overview of the proposed Depth-Aware Hybrid Feature Fusion (DH-Fusion) method for multi-modal 3D object detection. It illustrates the flow of data from input LiDAR point clouds and multi-view images through depth encoding, global feature fusion, local feature fusion, and finally to the detection head which produces object categories and 3D bounding boxes.  The diagram highlights the key components: depth encoders, global and local fusion transformers, and their interactions with LiDAR and image features.", "section": "3 Methodology"}, {"figure_path": "gBOQ0ACqoO/figures/figures_22_1.jpg", "caption": "Figure 2: Overview of our method. It introduces depth encoding in both global and local feature fusion to obtain depth-adaptive multi-modal representations for detection.  is the multiplication operation, and  is the merge operation.", "description": "This figure illustrates the overall architecture of the proposed Depth-Aware Hybrid Feature Fusion (DH-Fusion) method. It shows how LiDAR point cloud and multi-view images are processed through encoders to generate features.  Depth encoding is integrated into both global and local feature fusion modules (DGF and DLF) to dynamically adjust the weights of features based on depth. The resulting multi-modal features are then used for 3D object detection.", "section": "3 Methodology"}]