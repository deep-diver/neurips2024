{"importance": "This paper is crucial for NLP researchers because **it reveals a novel \"simplicity bias\" in transformers**, demonstrating that they sequentially learn interactions of increasing complexity. This understanding can **guide the design of more efficient and effective transformer models**, and it opens up **new avenues for studying how interactions of different orders in data impact learning**, advancing the field beyond current limitations.", "summary": "Transformers learn increasingly complex language patterns sequentially, starting with simpler interactions before mastering higher-order ones.", "takeaways": ["Transformers exhibit a \"simplicity bias\", learning low-order interactions before high-order ones.", "A novel method using factored self-attention generates data clones with controlled interaction complexity, enabling systematic study of learning dynamics.", "The sequential learning pattern applies to both MLM and next-token prediction tasks in transformers, impacting NLP model training and optimization."], "tldr": "Overparameterized neural networks' generalization ability is often attributed to a \"simplicity bias\", where they initially learn simple classifiers before tackling complex ones.  However, this bias wasn't well understood in transformers trained with self-supervised methods. This paper investigates whether transformers trained on natural language data also show this sequential learning behavior. Existing methods to analyze this phenomenon face the challenge of analyzing higher-order interactions due to high computational costs. \nThis research introduces a novel framework to address this issue. By training transformers with factored self-attention and using Monte Carlo sampling, the study generates \"clones\" of the original dataset with varying levels of interaction complexity. Experiments using these clones on standard BERT and GPT models reveal a clear simplicity bias:  transformers first learn low-order interactions, then progressively learn higher-order ones. The saturation point in error for low-degree interactions highlights this sequential learning. This finding significantly improves our understanding of transformer learning and opens new avenues for optimization.", "affiliation": "International School for Advanced Studies", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}, "podcast_path": "GgV6UczIWM/podcast.wav"}