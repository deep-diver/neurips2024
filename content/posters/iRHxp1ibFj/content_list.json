[{"type": "text", "text": "Approximate Size Targets Are Sufficient for Accurate Semantic Segmentation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 We propose a new general form of image-level supervision for semantic segmenta  \n2 tion based on approximate targets for the relative size of segments. At each training   \n3 image, such targets are represented by a categorical distribution for the \u201cexpected\u201d   \n4 average prediction over the image pixels. We motivate the zero-avoiding variant of   \n5 KL divergence as a general training loss for any segmentation architecture leading   \n6 to quality on par with the full pixel-level supervision. However, our image-level   \n7 supervision is significantly less expensive, it needs to know only an approximate   \n8 fraction of an image occupied by each class. Such estimates are easy for a human   \n9 annotator compared to pixel-accurate labeling. Our loss shows significant robust  \n10 ness to size target errors, which may even improve the generalization quality. The   \n1 proposed size targets can be seen as an extension of the standard class tags, which   \n2 correspond to non-zero size targets in each image. Using only a minimal amount   \n3 of extra information, our supervision improves and simplifies the training. It works   \n4 on standard segmentation architectures as is, unlike tag-based methods requiring   \n15 complex specialized modifications and multi-stage training. ", "page_idx": 0}, {"type": "text", "text": "16 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "17 Our image-level supervision approach applies to any semantic segmentation model and does not   \n18 require any modification. It can be technically described in one paragraph, as follows. Soft-max   \n19 prediction $\\boldsymbol{S_{p}}=(S_{p}^{1},\\ldots,S_{p_{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\! $ at any pixel $p$ is a categorical distribution over $K$ classe including   \n20 background. At any image, the average prediction over all image pixels, denoted by set $\\Omega$ , is ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\bar{S}:=\\frac{1}{|\\Omega|}\\sum_{p\\in\\Omega}S_{p}\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "21 where $\\bar{S}\\,=\\,(\\bar{S}^{1},\\ldots,\\bar{S}^{K})$ is also a categorical distribution over $K$ classes. It is an image-level   \n22 prediction of the relative or normalized sizes (volume, area, or cardinality) of the objects in the image.   \n2234 $\\boldsymbol{v}=(v_{k})_{k=1}^{K}$ , aet .tgr.a $v=(0,.15,0,\\ldots,0,.75)$ mfoatre t shiez em tiadrgdleet si rmeaprgees ienn tFeidg .b y1  cifa t\u201cegbiorridc\u201da li sd itshteri sbeutcioonnds   \n25 class and \u201cbackground\u201d is the last. This representation also applies to multi-label images. For each   \n26 training image, our size-target loss ", "page_idx": 0}, {"type": "equation", "text": "$$\nL_{s i z e}\\;\\;=\\;\\;K L(v\\|\\bar{S})\\;\\;=\\;\\;\\sum_{k}v_{k}\\ln\\frac{v_{k}}{\\bar{S}^{k}}\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "27 is based on Kullback\u2013Leibler $(K L)$ divergence. Figure 2(b) shows some results for a generic   \n28 segmentation network (ResNet101 [4] backbone) trained on PASCAL [5] using only image-level   \n29 supervision with approximate size targets $8\\%$ mean relative errors). Our total loss is very simple: it   \n30 combines size-target loss (2) and a common CRF loss (3) [6]. ", "page_idx": 0}, {"type": "image", "img_path": "iRHxp1ibFj/tmp/d681e54200cab503112442930d56d5db32e9a5ef25e57e6c7cb1805f33361d2d.jpg", "img_caption": ["Figure 1: Supervision types for segmentation: labeling speed and accuracy on PASCAL. The top-left corner of each image shows its estimated labeling time based on observed instances. The table shows per-image labeling times averaged over the data and mean Intersection-over-Union (mIoU) for comparable end-to-end methods with similar ResNet backbones (ResNet101 or WideResNet38 [1]), for fairness. We obtained mIoU scores, except for the \u201ctag\u201d and \u201cbox\u201d scores from [2] and [3]. Our supplemental materials detail evaluation of the labeling times and mIoU. For completeness, Tab.2 includes more complex architectures and multi-stage systems, e.g. for tags. This paper focuses on standard segmentation architectures for size supervision. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "31 1.1 Overview of weakly-supervised segmentation ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "32 By weakly-supervised semantic segmentation we refer to all methods that do not use full pixel  \n33 precise ground truth (GT) masks for training. Such full supervision is overwhelmingly expensive for   \n34 segmentation and is unrealistic for many practical purposes, see the right image in Fig. 1. There are   \n35 many forms of weak supervision for semantic segmentation, e.g. based on partial pixel-level ground   \n36 truth defined by \u201cseeds\u201d [6, 7], boxes [3], or image-level class-tags [2, 8, 9], see Fig. 1. It is also   \n37 common to incorporate self-supervision based on various augmentation ideas and contrastive losses   \n38 [10\u201312].   \n39 Lack of supervision also motivates unsupervised loss functions such as standard old-school regulariza  \n40 tion objectives for low-level segmentation or clustering. For example, many methods [13, 14, 12] use   \n41 variants of K-means objective (squared errors) enforcing the compactness of each class representation.   \n42 It is also very common to use CRF-based pairwise loss functions [6, 7] that encourage segment shape   \n43 regularity and alignment to intensity contrast edges in each image [15]. The last point addresses the   \n44 well-known limitation of standard segmentation networks that often output low-resolution segments.   \n45 Intensity contrast edges on the high-resolution input image is a good low-level cue of an object   \n46 boundary and it can improve the details and localization of the semantic segments.   \n47 Conditional or Markov random fields (CRF or MRF) are common basic examples of pairwise   \n48 graphical models. The corresponding unsupervised loss functions can be formulated for continuous   \n49 soft-max predictions $S_{p}$ produced by segmentation networks, e.g. [6, 7, 9]. Thus, it is natural to use   \n50 relaxations of the standard discrete CRF/MRF models, such as Potts [16] or its dense-CRF version   \n51 [17]. We use a bilinear relaxation of the general Potts model ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "equation", "text": "$$\nL_{c r f}(S)\\;\\;=\\;\\;\\sum_{k}({\\bf1}-S^{k})^{\\top}W S^{k}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "52 where $S:=(S_{p}\\,|\\,p\\in\\Omega)$ is a field of all pixel-level soft-max predictions $S_{p}$ in a given image, and   \n53 $S^{k}:=(S_{p}^{k}\\,|\\,p\\in\\Omega)$ is a vector of all pixel predictions specifically for class $k$ . Matrix $W=[w_{p q}]$   \n54 typically represents some given non-negative affinities $w_{p q}$ between pairs of pixels $p,q\\in\\Omega$ . It is   \n55 easy to interpret loss (3) assuming, for simplicity, that all pixels have confident one-hot predictions   \n56 $S_{p}$ so that each $S^{k}$ is a binary indicator vector for segment $k$ . The loss sums all weights $w_{p q}$ between   \n57 the pixels in different segments. Thus, the weights are interpreted as discontinuity penalties. The loss   \n58 minimizes the discontinuity costs [16].   \n59 In practice, affinity weights $w_{p q}$ are set close to 1 if two neighboring pixels $p,q$ have similar intensities,   \n60 and weight $w_{p q}$ is set close to zero either when two pixels are far from each other on the pixel grid or   \n61 if they have largely different intensities [6, 16, 17]. The affinity matrix $W$ could be arbitrarily dense   \n62 or sparse, e.g. many zeros when representing a 4-connected pixel grid. The non-zero discontinuity   \n63 costs between neighboring pixels are often set by a Gaussian kernel $\\begin{array}{r}{w_{p q}=\\exp\\frac{-\\|I_{p}-I_{q}\\|^{2}}{2\\sigma^{2}\\,.}}\\end{array}$ \u2212\u2225Ip\u2212Iq\u22252of given   \n64 bandwidth $\\sigma$ , which works as a soft threshold for detecting high-contrast intensity edges in the image.   \n65 Thus, loss (3) encourages both the alignment of the segmentation boundary to contrast edges in the   \n66 (high-resolution) input image and the shortness/regularity of this boundary.   \n67 Weakly supervised segmentation methods may also use partial pixel-level ground truth where only   \n68 some subset $S e e d s\\subset\\Omega$ of image pixels has class labels [6, 7, 9]. In this case it is common to use   \n69 partial cross-entropy loss ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "image", "img_path": "iRHxp1ibFj/tmp/42c82f21aea3fd29849280042c76992b268f2f6bc30d4a06ab3939ec77e69b21.jpg", "img_caption": ["Figure 2: Semantic segmentation with standard DeepLabV3+(R101) segmentation models [18]: PASCAL validation results for training with (a) log-barrier (9) using class tags, (b) KL-divergence (2) using our approximate size targets, (c) cross-entropy with full (ground truth mask) supervision. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\nL_{p c e}(S)\\;\\;=\\;\\;-\\sum_{p\\in S e e d s}\\ln S_{p}^{y_{p}}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "70 where $y_{p}$ is the ground truth label at a seed pixel $p$ . ", "page_idx": 2}, {"type": "text", "text": "71 1.2 Related balancing losses ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "72 Segmentation and classification methods often use \u201cbalancing\u201d losses. In the context of classification,   \n73 image-level predictions can be balanced over the whole training data. For segmentation problems,   \n74 pixel-level predictions can be balanced within each training image. Our loss is an example of size   \n75 balancing. Below we review some examples of related balancing loss functions used in prior work.   \n76 Fully supervised classification. It is common to modify the standard cross-entropy loss to account   \n77 for unbalanced training data where some classes are represented more than others. One common   \n78 example is weighted cross-entropy, e.g. defined in [19] for image-level predictions $S_{i}$ as ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\nL_{w c e}(S)\\;\\;=\\;\\;-\\sum_{i\\in D}w_{y_{i}}\\ln S_{i}^{y_{i}}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "79 where class weights $\\begin{array}{r}{w_{k}\\propto\\frac{1}{1-\\beta^{v_{k}}}}\\end{array}$ are motivated as a re-balancing factor based on the class distribution   \n80 $v$ in the training dataset $D$ and $\\beta$ is a hyper-parameter. In the fully supervised setting, the purpose   \n81 of re-weighting cross-entropy is not to make the predictions even closer to the known labels, but to   \n82 decrease over-fitting to over-represented classes, which improves the model\u2019s generality.   \n83 Unsupervised classification. In the context of clustering with soft-max models [20, 21] it is common   \n84 to use fairness loss encouraging equal-size clusters. In this case, there is no ground truth and   \n85 fairness is one of the discriminative properties enforced by the total loss in order to improve the model   \n86 predictions on unlabeled training data. The fairness was motivated by information-theoretic arguments   \n87 in [20] deriving it as a negative entropy of the data-set-level average prediction $\\begin{array}{r}{\\hat{S}:=\\frac{1}{|D|}\\sum_{i\\in D}S_{i}}\\end{array}$   \n88 for dataset $D$ ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{{{\\cal L}_{f a i r}({\\hat{S}})}}&{{=}}&{{-{\\cal H}({\\hat{S}})\\;\\;\\equiv\\;\\displaystyle\\sum_{k}{\\hat{S}}^{k}\\ln{\\hat{S}}^{k}}}\\\\ {{}}&{{}}&{{}}\\\\ {{}}&{{\\underline{{{\\mathrm{c}}}}}}&{{\\displaystyle\\sum_{k}{\\hat{S}}^{k}\\ln\\displaystyle\\frac{{\\hat{S}}^{k}}{1/K}\\:\\equiv\\;K L({\\hat{S}}||u)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "89 where $\\begin{array}{r}{u=\\big(\\frac{1}{K},\\cdot\\cdot\\cdot,\\frac{1}{K}\\big)}\\end{array}$ is a uniform categorical distribution, and symbol $\\circeq$ indicates that the equality   \n90 is up to some additive constant independent of $\\hat{S}$ . Perona et al. [21] pointed out the equivalent KL  \n91 divergence formulation of the fairness in (6) and generalized it to a balanced partitioning constraint ", "page_idx": 2}, {"type": "equation", "text": "$$\nL_{b a l}(\\hat{S})\\;\\;=\\;\\;K L(\\hat{S}\\|v)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "92 with any given prior distribution $v$ that could be different from uniform. ", "page_idx": 3}, {"type": "text", "text": "93 Semantic segmentation with image-level supervision. Most weakly-supervised semantic segmenta  \n94 tion methods use losses based on segment sizes. This is particularly true for image-level supervision   \n95 techniques [2, 9, 22, 23]. Clearly, segments for tag classes should have positive sizes, and segments   \n96 for non-tag classes should have zero sizes.   \n97 Similarly to our paper, size-based constraints are often defined for the image-level average prediction   \n98 $\\bar{S}$ , see (1), computed from pixel-level predictions $S_{p}$ . Many generalized forms of pixel-prediction   \n99 averaging can be found in the literature, where they are often referred to as prediction pooling. Some   \n100 decay parameter often provides a wide spectrum of options from basic averaging to max-pooling.   \n101 While the specific form of pooling matters, for simplicity, we discuss the corresponding balancing   \n102 loss functions assuming basic average prediction $\\bar{S}$ in (1).   \n103 One of the earliest works on tag-supervised segmentation [9] uses log-barriers to \u201cexpand\u201d tag   \n104 objects in each training image and to \u201csuppress\u201d the non-tag objects. Assuming image tags $T$ , their   \n105 suppression loss is defined as ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\nL_{s u p p r e s s}(\\bar{S})\\;\\;\\propto\\;\\;-\\sum_{k\\not\\in{\\cal T}}\\ln(1-\\bar{S}^{k})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "106 encouraging each non-tag class to have zero average prediction S\u00afk, which implies zero predictions   \n107 $S_{p}^{k}$ at each pixel. Their expansion loss ", "page_idx": 3}, {"type": "equation", "text": "$$\nL_{e x p a n d}(\\bar{S})\\;\\;\\propto\\;\\;-\\sum_{k\\in T}\\ln\\bar{S}^{k}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "108 encourages positive average predictions $\\bar{S}^{k}$ and non-trivial tag class segments. ", "page_idx": 3}, {"type": "text", "text": "109 We observe that the expansion loss (9) may have a bias to equal-size segments, as particularly evident   \n110 in the case of average predictions. Indeed, (9) implies ", "page_idx": 3}, {"type": "equation", "text": "$$\nL_{e x p a n d}(\\bar{S})\\;\\;\\propto\\;\\;K L(u_{\\mathrm{T}}||\\bar{S})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "111 which is a special case of our size loss (2) when the size target $v=u_{\\mathrm{T}}$ is a uniform distribution over   \n112 tag classes. The intention of the log barrier loss (9) is to push image-level size prediction $\\bar{S}$ from   \n113 the boundaries of the probability simplex $\\Delta_{K}$ corresponding to the zero-level for the tag classes   \n114 $T$ . Figure 2(a) shows the results for training based on the total loss combining CRF loss (3) with   \n115 the log-barrier loss (9). Its unintended bias to equal-size segments (10) is obvious. Note that the   \n116 mentioned decay parameter used for generalized average predictions should reduce such bias. ", "page_idx": 3}, {"type": "text", "text": "117 Alternatively, it may be safer to use barriers for $\\bar{S}$ like ", "page_idx": 3}, {"type": "equation", "text": "$$\nL_{f l a t}\\;\\;=\\;\\;-\\sum_{k\\in T}\\ln\\operatorname*{max}\\{\\bar{S}^{k},\\epsilon\\}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "118 that have flat bottoms to avoid unintended bias to some specific size target inside the probability   \n119 simplex $\\Delta_{K}$ . Similar thresholded barriers are common [22]. ", "page_idx": 3}, {"type": "text", "text": "120 1.3 Contributions ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "121 In general, it would be great to have effective image-level supervision for segmentation that only uses   \n122 barriers like (9) or (11) since they do not require any specific size targets. This corresponds to tag-only   \n123 supervision. However, our empirical results for semantic segmentation using such barriers were   \n124 poor and comparable with those in [9]. A number of more recent semantic segmentation methods   \n125 for tag-level supervision have considerably improved such results [12, 24\u201330], but they introduce   \n126 significantly more complex multi-stage training procedures and various architectural modifications,   \n127 which makes such methods hard to replicate, generalize, or to understand the results. We are focused   \n128 on general easy-to-understand end-to-end training methods. Our main contributions are:   \n129 \u2022 We propose and evaluate a new general form of weak supervision, size targets. The size  \n130 target supervision can be approximate and is relatively easy to get from human annotators.   \n131 \u2022 We propose the zero-avoiding variant of KL divergence as a general training loss, allowing   \n132 our end-to-end size-target approach to be integrated with any segmentation architecture.   \n133 \u2022 Comprehensive experiments with our size-target method demonstrate state-of-the-art perfor  \n134 mance across multiple datasets using standard segmentation models typically employed for   \n135 full supervision, without any architectural modifications. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "136 2 Size-target loss and its properties ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "137 Our proposed total loss is very simple ", "page_idx": 4}, {"type": "equation", "text": "$$\nL_{t o t a l}\\;:=\\;L_{s i z e}+L_{c r f}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "138 where the two terms are our size-target loss (2) and standard CRF loss (3). The core new com  \n139 ponent is our size-target loss based on the forward KL-divergence. Our size-target loss (2) en  \n140 courages specific target volumes for tag classes. Additionally, the size-target loss suppresses   \n141 non-tag classes, encouraging zero volumes for classes not in the image. The CRF loss also con  \n142 tributes to the suppression of redundant classes. Therefore, unlike most prior work on image  \n143 level supervision for semantic segmentation, e.g. [9, 2, 12], we do not need separate suppres  \n144 sion loss terms like (8). We validated this claim experimentally, they did not change the results. ", "page_idx": 4}, {"type": "image", "img_path": "iRHxp1ibFj/tmp/4b9597ae0ce1c909e5e464b70b6279c258bfbb3a18675ac15a67ad66c9dfe3ad.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "155 Figure 3: Forward vs reverse KL divergence. As  \n156 suming binary classification $K=2$ , we can repre  \n157 sent all possible probability distributions as points   \n158 on the interval [0,1]. The solid curves illustrate   \n159 our \u201cstrong\u201d size constraint, i.e. the forward KL  \n160 divergence $K L(v||\\bar{S})$ for the average prediction   \n116612 $\\bar{S}$ . We show two examples of volumetric prior $v_{1}\\,=\\,(0.9,0.1)$ (blue curve) and $v_{2}\\,=\\,(0.5,0.5)$   \n163 (red curve). For comparison, the dashed curves   \n164 represent reverse KL divergence $K L(\\bar{S}||v)$ .   \n165 ", "page_idx": 4}, {"type": "text", "text": "The size-target loss can also be integrated into other weakly-supervised settings, e.g. partial cross-entropy loss (4) commonly used for seeds. We show that using approximate size targets can significantly improve the seed-supervised segmentation in [6] when the seed lengths are short, see the right plot of Fig. 4. ", "page_idx": 4}, {"type": "equation", "text": "$$\nL_{t o t a l}^{'}\\ :=\\ L_{s i z e}+L_{c r f}+L_{p c e}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "As is well known, KL divergence is asymmetric. In our work on image-level supervised segmentation, the order of the estimated and target distributions is crucial. The forward KL divergence possesses a zero-avoiding property, as illustrated in Fig. 3. Specifically, forward KL divergence imposes an infinite penalty when any class with a non-zero target is predicted as zero. In contrast, the penalty of the reverse KL divergence is finite and much weaker. When using reverse KL divergence, segmentation models tend to generate trivial solutions, predicting all pixels as the background class. This issue likely arises due to dataset imbalance, where the background class ", "page_idx": 4}, {"type": "text", "text": "166 is prevalent. The zero-avoiding property of forward KL divergence ensures that segmentation models   \n167 do not produce trivial solutions and predict all classes in the image tag sets. ", "page_idx": 4}, {"type": "text", "text": "168 3 Experiments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "169 3.1 Experimental settings ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "170 Datasets. We evaluate our approach on three segmentation datasets: PASCAL VOC 2012 [5], MS   \n171 COCO 2014 [31], and 2017 ACDC Challenge1 [32]. The PASCAL dataset contains 21 classes. We   \n172 adopt the augmented training set with 10,582 images [33], following the common practice [34, 9].   \n173 Validation and testing contain 1449 and 1456 images. Seed supervision of the PASCAL dataset is   \n174 from [7]. COCO has 81 classes with 80K training and 40K validation images. ACDC Challenge is   \n175 to segment the left ventricular endocardium. The training and validation sets contain 1674 and 228   \n176 images. The exact size targets are extracted from the ground truth masks.   \n177 Approximate size targets. We train segmentation models using approximate size targets $v=$   \n178 $(\\bar{v_{k}})_{k=1}^{K}$ generated for each image either by human annotators or by corrupting the exact size targets   \n179 $\\hat{v}\\,=\\,(\\hat{v}_{k})_{k=1}^{K}$ with different levels of noise. In all cases, we report the segmentation accuracy on   \n180 validation data together with mean relative error (mRE) of the corresponding corrupted size targets.   \n181 For each training image containing class $k$ , the relative error for the size target $v_{k}$ is defined as ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\nR E(v_{k})=\\frac{|v_{k}-\\hat{v}_{k}|}{\\hat{v}_{k}}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "182 where $\\hat{v}_{k}$ is the exact size. mRE averages RE over all images and all classes. For human annotated   \n183 size targets $\\boldsymbol{v}=(v_{k})_{k=1}^{K}$ , the relative size errors are computed directly from the definition (14). ", "page_idx": 5}, {"type": "text", "text": "184 When used, synthetic targets $\\boldsymbol{v}=(v_{k})_{k=1}^{K}$ are generated by corrupting the exact targets $\\hat{v}=(\\hat{v}_{k})_{k=1}^{K}$ ", "page_idx": 5}, {"type": "equation", "text": "$$\nv_{k}\\longleftarrow(1+\\epsilon)\\hat{v}_{k}\\quad\\mathrm{for}\\quad\\epsilon\\sim\\mathcal{N}(0,\\sigma)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "185 where $\\epsilon$ is white noise with standard deviation $\\sigma$ controlling the level of corruption and operator $\\longleftarrow$   \n186 represents re-normalization ensuring corrupted targets $(v_{k})_{k=1}^{K}$ add up to one. Equation (15) defines   \n187 random variable $v_{k}$ as a function of $\\epsilon$ . Thus, in this case, mRE can be analytically estimated from $\\sigma$ ", "page_idx": 5}, {"type": "equation", "text": "$$\nm R E\\;=\\;E\\left(\\frac{|v_{k}-\\hat{v}_{k}|}{\\hat{v}_{k}}\\right)\\;\\approx\\;E(|\\epsilon|)\\;=\\;\\sqrt{\\frac{2}{\\pi}}\\;\\sigma\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "188 where $E$ is the expectation operator. The approximation in the middle uses (15) as an equality   \n189 ignoring re-normalization of the corrupted sizes, and the last equality is a closed-form expression for   \n190 the mean absolute deviation (MAD) of the Normal distribution $\\mathcal{N}(0,\\sigma)$ .   \n191 Evaluation metrics for segmentation. We employ mean Intersection-over-Union (mIoU) as the   \n192 evaluation criteria for PASCAL and COCO, and mean Dice similarity coefficient (DSC) for the   \n193 ACDC dataset. The quality on the PASCAL test set is assessed on the online evaluation server.   \n194 Implementation details. We evaluate our approach with two types of ResNet-based [4] and one vision   \n195 transformer (ViT) based [35] segmentation models on the PASCAL and COCO datasets. ResNet  \n196 based models follow the implementation of DeepLab $\\mathrm{V}3+$ [18] using the backbone of ResNet101   \n197 (R101) or the backbone of WideResNet-38 (WR38) [1]. For brevity, we name them R101-based or   \n198 WR38-based DeepLab ${\\mathrm{V}}3+$ models. For the ViT-based network, We follow the implementation of   \n199 Segmenter [36], adopting its ViT-B/16 backbone and linear decoder. For experiments on the ACDC   \n200 datasets, we use MobileNetV2-based [37] DeepLabv $^{3+}$ model. The R101, WR38, and MobileNetV2   \n201 backbones are ImageNet [38] pre-trained. ViT-B/16 backbone is pre-trained on ImageNet-21K [39]   \n202 and fine-tuned on ImageNet-1k [38]. We directly evaluate our size-target approach on top of the   \n203 standard architectures without any modification.   \n204 Images are resized to $512\\times512$ for PASCAL and COCO, and $256\\times256$ for ACDC. We employ   \n205 color jittering and horizontal flipping for data augmentation. Segmentation models are trained with   \n206 stochastic gradient descent on one RTX A6000 GPU with 48 GB GDDR6: 60 epochs for PASCAL   \n207 and COCO, and 200 epochs for ACDC, with a polynomial learning rate scheduler (power of 0.9).   \n208 Batch sizes are set to 16 for ResNet and 20 for ViT models on PASCAL, 12 on ACDC, and 12   \n209 (ResNet) and 16 (ViT) for MS COCO. The initial learning rate is 0.005 for ACDC and PASCAL\u2019s   \n210 ResNet models, and 0.0005 for PASCAL\u2019s ViT models. The initial learning rate on COCO is 0.0005   \n211 for ResNet and 0.0001 for ViT models. Loss function (12) is employed for size-target supervision.   \n212 Loss (13) is only used for seed supervision in Sec. 3.3. The implementation of CRF loss (3) is the   \n213 same as [6]. We use $\\mathrm{2e^{-9}}$ as the weight of the CRF term following the strategy in [6]. Size-target   \n214 loss (2) and pCE (4) are used for medical images. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "215 3.2 Robustness to Size Errors ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "216 We show the size targets can be approximate. The left plot in Fig. 4 illustrates the robustness of our   \n217 approach to size errors. Segmentation models are trained with synthetic size targets subjected to   \n218 varying levels of corruption, as defined in (15). The validation accuracy (solid red line) only drops   \n219 slightly when $m R E$ (16) remains below $16\\%$ . The CRF loss (3) further enhances the robustness   \n220 (solid blue line). When the relative error $(m R E)$ is $4\\%$ , there is a noticeable increase in validation   \n221 accuracy. The downward trend of the training accuracy (dashed blue line) suggests that the observed   \n222 increases in validation accuracy at $m R E=\\bar{4}\\%$ stem from improved neural network generalization. ", "page_idx": 5}, {"type": "text", "text": "223 3.3 Enhancing seed-based segmentation with size targets ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "224 Our size-target approach can be integrated with partial ground truth mask supervision (seeds). The   \n225 right plot in Fig. 4 demonstrates the results of seed-supervised semantic segmentation with and without   \n226 size-target supervision. Size targets significantly enhance performance, especially when the seed   \n227 lengths are short. Without size targets, segmentation performance degrades dramatically as the seed   \n228 length decreases. Notably, when only one pixel is labeled for each object (seed length ratio $=0.0$ ),   \n229 size-target supervision boosts accuracy from $66.6\\%$ to $74\\%$ , approaching the performance of full   \n230 seed supervision (seed length ratio $=1.0$ ). ", "page_idx": 5}, {"type": "image", "img_path": "iRHxp1ibFj/tmp/e10101d1c2b8723f52c1b8bfe740da0fc494dc3c79c8c66874b6b3ea86eb366e.jpg", "img_caption": ["Figure 4: Segmentation results on the PASCAL dataset with R101-based Deeplab $\\mathrm{V}3+$ networks. The green bar in both plots indicates the segmentation accuracy for full ground truth masks (i.e. full supervision). The left plot shows the training and validation accuracy using approximate size targets. The segmentation is trained using losses (2) (red curve) or (12) (blue curve), where size targets are subject to various levels of corruption (15,16). The right plot shows validation accuracy for seed supervision of varying lengths with (blue curve) and without (red curve) using size targets. The line styles of the blue curves differentiate among various levels of corruption. "], "img_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "iRHxp1ibFj/tmp/607df82c71420424c85fc8dadeb8f201e328cdc11ca651c9870dbfddc330858f.jpg", "img_caption": ["Figure 5: Left plot shows the quality of human annotations in terms of relative errors for the dog, cat, and bird classes within the PASCAl dataset. The histograms are normalized by the number of images in each class. The mean relative error for the three classes is $15.9\\%$ . For comparison, the dashed line shows the relative error distribution of synthetic size targets as defined in (15) for $\\sigma=20.0\\%$ which aligns with the $m R E$ of $15.9\\%$ , see (16). The right plot presents 4-way multi-class (cat, dog, bird, and background) segmentation accuracy using human-annotated (red star at $m R E=15.9\\%$ and synthetic (blue curve) size targets, employing ResNet101-based Deeplab $\\mathrm{V}3+$ networks. Consistent with experiments in Sec. 3.2, synthetic size targets are generated at various levels of corruption. The green line indicates the segmentation accuracy of full supervision using ground truth masks. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "231 3.4 Human-annotated size targets ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "232 Annotation tool. In this section, our approach is evaluated with size targets annotated by humans.   \n233 We annotated training images for a subset of PASCAL classes, including cat, dog, and bird. A   \n234 user interface with an assistance tool was developed to facilitate the annotation. The assistance tool   \n235 overlays grid lines partitioning the image into $5\\times4$ small rectangles or $3\\times3$ large rectangles. Users   \n236 can determine the size of a class in an image by counting rectangles (fractions allowed) or entering   \n237 the percentage relative to the image size. Annotators can choose finer or coarser partitioning for each   \n238 image depending on the object size. We evaluate relative errors with (14) for human annotations.   \n239 Empirical evidence shows that annotators are approximately two times more accurate with the   \n240 assistance tool, especially for small objects in the image. The last two columns of Table 1 report the   \n241 annotation speed per image and mean relative error (14) for each class. The left plot in Fig. 5 shows   \n242 the histograms of relative errors for human annotations. The histograms illustrate that annotated size   \n243 errors are mostly below $10\\%$ , but occasional large mistakes (heavy tails) raise the mean error.   \n244 Segmentation with human-annotated size. Segmentation models trained with human-annotated   \n245 size targets show robustness to human \u201cheavy tail\u201d errors. We compare the accuracy for human  \n246 annotated and synthetic size targets in the right plot of Fig. 5. The accuracy for human-annotated   \n247 size (indicated by the red star in the plot) approaches $97.2\\%$ $(89.6\\%/92.2\\%)$ of the full supervision   \n248 performance, demonstrating that size-target approach is significantly robust to human errors. Binary   \n249 segmentation accuracy for each class is reported in the shaded cells in Table 1. The performance of ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "table", "img_path": "iRHxp1ibFj/tmp/aa8670ea12768dda0639d41a7765f99bded700af43a344370a62ae77e18c08e6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 1: Human-annotated size targets. Two columns on the right show the average speed and relative error for each class we annotated. The shaded cells compare the accuracy of binary segmentation models trained with ground truth masks, ground truth size, and human-annotated size. ", "page_idx": 7}, {"type": "text", "text": "250 binary segmentation models trained with human-annotated size targets is comparable to those trained   \n251 with precise size targets. ", "page_idx": 7}, {"type": "text", "text": "252 3.5 Comparison with the state-of-the-art methods ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "253 Our general training losses are applied to three standard architectures (R101-DeepLab ${\\mathrm{V}}3+$ , WR38-   \n254 DeepLab $N3+$ , and ViT-Linear) for semantic segmentation as is, without any modification. Our results   \n255 are highlighted in Table 2. The models are trained using synthetic size targets with an approximate   \n256 mean relative error (mRE) of $8\\%$ . We chose this corruption level because its performance is close   \n257 to human annotations, as shown in the right plot of Figure 5. Since our single-stage (end-to-end)   \n258 approach is completely general, it is possible to use it in specialized architectures or complex   \n259 training procedures. Likely, this would further improve the results, but this is not the focus of   \n260 our work. The rest of Table 2 shows the results for semantic segmentation methods (of different   \n261 complexities) for weak and full supervision. Methods are divided into multi-stage and single-stage   \n262 methods, grouped by their backbones. Typical single-stage methods improve their results using   \n263 complex architectural or training modifications such as additional training branches, extra refinement   \n264 modules, or specialized training strategies. However, we achieve state-of-the-art using only standard   \n265 segmentation architectures, commonly used in full supervision. The R101-based DeepLab ${\\mathrm{V}}3+$ model   \n266 trained with approximate size targets approaches $92\\%$ (71.9/78.2) of its full supervision performance   \n267 on PASCAL. The WR38-based DeepLab $\\mathrm{V}3+$ model trained with approximate size-target supervision   \n268 surpasses other methods employing the same backbone by approximately $10\\%$ . Using the standard   \n269 vision transformer architecture [36], the size-target approach achieves approximately $96\\%$ of the   \n270 full supervision performance on the Pascal dataset. Despite its simplicity, the size-target approach   \n271 outperforms other complex single-stage methods on both datasets. ", "page_idx": 7}, {"type": "table", "img_path": "iRHxp1ibFj/tmp/87308be598d0c6570d3a20d508c5b234cea1be29219e3a928dcd68f8ddee9842.jpg", "table_caption": [], "table_footnote": ["Table 2: Semantic segmentation results $(\\mathrm{mIoU}\\%)$ ) on PASCAL and COCO. The supervision column indicates a form of supervision: image-level class tags, size targets (our highlighted results), or full supervision with pixel-accurate masks. The percentage after \u201csize\u201d is the accuracy (mRE) of our corrupted size targets (15,16). Our approach does not require any complex architectural modification or multi-stage training procedures needed for tag supervision, see \u201cModification\u201d column. "], "page_idx": 7}, {"type": "image", "img_path": "iRHxp1ibFj/tmp/ff1cfe9c49e32c519c7933824c5d09427aa527d04c8bc62c7d8ced9093206ed5.jpg", "img_caption": ["Figure 6: Size-targets (2) vs. size-barriers (17) on the ACDC dataset. The left plot shows the accuracy of the binary segmentation models (MobileNetV2-based DeeplabV3+) measured by DSC. The blue curve shows size-target accuracy with various levels of corruption. The dashed green line shows the accuracy of the size-barrier technique [22]. The dashed red line shows the accuracy using the mean size target for all training images. The gray line indicates the result of full supervision. The right image shows randomly selected qualitative results of size-barrier [22] and approximate size target $\\acute{m}\\bar{R}E=8\\%$ ). Yellow shows true positive pixels, green is false positive, and red is false negatives. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "272 3.6 Medical data: size-target vs. size-barrier ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "273 Our method is also promising for medical image segmentation, benefiting from the consistency in   \n274 object sizes across similar medical images, which healthcare professionals can easily estimate. We   \n275 compare our size-target approach with the thresholded size-barrier technique [22], proposed for the   \n276 weakly supervised medical image semantic segmentation. The size-barrier loss enforces inequality   \n277 size constraints. Given the lower bound of each class, the thresholded size-barrier loss is ", "page_idx": 8}, {"type": "equation", "text": "$$\nL_{f l a t\\_s q}(S)\\;\\;=\\;\\;\\sum_{k}\\left(\\operatorname*{max}\\{a_{k}-\\bar{S}^{k},0\\}\\right)^{2},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "278 where $a_{k}$ is a lower bound of class $k$ . We train binary segmentation models with a combination   \n279 of partial cross-entropy loss (4) and size constraint loss: size-target (2) or size-barrier (17). Seeds   \n280 used in the experiments are obtained using the same method provided in [22]. The object and   \n281 background barrier, $a_{o b j}$ and $a_{b g}$ are set based on [22]. In the size-barrier experiments, similarly to   \n282 [22], we suppress the non-tag classes, using the loss $L_{s u p}(S)=(\\bar{S}^{o b j})^{2}$ . Conversely, size-target   \n283 loss automatically suppresses non-tag classes as discussed in Sec. 2. The left plot in Fig. 6 displays   \n284 the segmentation accuracy against different levels of size target corruption. Our size-target loss   \n285 consistently outperforms size-barrier loss, maintaining its superiority even when using highly noisy   \n286 size targets. The peak in the accuracy curve aligns with the experimental results in Sec. 3.2 and   \n287 Sec. 3.4. The accuracy of the model trained using size targets with relative errors of $8\\%$ surpasses   \n288 the full supervision performance. Additionally, using a fixed average size target across all training   \n289 images can yield performance comparable to the size-barrier method, see the dashed red line in the   \n290 left plot of Fig. 6. The right image in Fig. 6 shows qualitative examples of both methods. ", "page_idx": 8}, {"type": "text", "text": "291 4 Conclusions ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "292 We proposed a new image-level supervision for semantic segmentation: size targets. Such targets   \n293 could be approximate. In fact, our results suggest that some errors can benefit generalization. The   \n294 size annotation by humans requires little extra effort compared to the standard image-level tags and it   \n295 is much cheaper than the full pixel-accurate ground truth masks. We proposed an effective size-target   \n296 loss based on forward KL divergence between the soft size targets and the average prediction. In   \n297 combination with the standard CRF-based regularization loss, our approximate size-target supervision   \n298 on standard segmentation architectures (DeepLab and ViT) achieves state-of-the-art performance.   \n299 Our general easy-to-understand approach outperforms significantly more complex weakly-supervised   \n300 techniques based on model modifications and multi-stage training procedures. ", "page_idx": 8}, {"type": "text", "text": "301 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "302 [1] Zifeng Wu, Chunhua Shen, and Anton van den Hengel. Wider or deeper: Revisiting the resnet   \n303 model for visual recognition, 2016.   \n304 [2] Nikita Araslanov and Stefan Roth. Single-stage semantic segmentation from image labels. In   \n305 Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages   \n306 4253\u20134262, 2020.   \n307 [3] V. Kulharia, S. Chandra, A. Agrawal, P. Torr, and A. Tyagi. Box2seg: Attention weighted loss   \n308 and discriminative feature learning for weakly supervised segmentation. In ECCV\u201920, 2020.   \n309 [4] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image   \n310 recognition, 2015.   \n311 [5] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman.   \n312 The pascal visual object classes (voc) challenge. International journal of computer vision,   \n313 88:303\u2013308, 2009.   \n314 [6] Meng Tang, Federico Perazzi, Abdelaziz Djelouah, Ismail Ben Ayed, Christopher Schroers, and   \n315 Yuri Boykov. On regularized losses for weakly-supervised cnn segmentation. In Proceedings of   \n316 the European Conference on Computer Vision (ECCV), pages 507\u2013522, 2018.   \n317 [7] Di Lin, Jifeng Dai, Jiaya Jia, Kaiming He, and Jian Sun. Scribblesup: Scribble-supervised   \n318 convolutional networks for semantic segmentation. In Proceedings of the IEEE conference on   \n319 computer vision and pattern recognition, pages 3159\u20133167, 2016.   \n320 [8] George Papandreou, Liang-Chieh Chen, Kevin P Murphy, and Alan L Yuille. Weakly-and   \n321 semi-supervised learning of a deep convolutional network for semantic image segmentation. In   \n322 Proceedings of the IEEE international conference on computer vision, pages 1742\u20131750, 2015.   \n323 [9] Alexander Kolesnikov and Christoph H Lampert. Seed, expand and constrain: Three principles   \n324 for weakly-supervised image segmentation. In Computer Vision\u2013ECCV 2016: 14th European   \n325 Conference, Amsterdam, The Netherlands, October 11\u201314, 2016, Proceedings, Part IV 14, pages   \n326 695\u2013711. Springer, 2016.   \n327 [10] Xu Ji, Joao F Henriques, and Andrea Vedaldi. Invariant information clustering for unsuper  \n328 vised image classification and segmentation. In Proceedings of the IEEE/CVF International   \n329 Conference on Computer Vision, pages 9865\u20139874, 2019.   \n330 [11] Jang Hyun Cho, Utkarsh Mall, Kavita Bala, and Bharath Hariharan. Picie: Unsupervised   \n331 semantic segmentation using invariance and equivariance in clustering. In Proceedings of the   \n332 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16794\u201316804, 2021.   \n333 [12] Tianfei Zhou, Meijie Zhang, Fang Zhao, and Jianwu Li. Regional semantic contrast and   \n334 aggregation for weakly supervised semantic segmentation. In Proceedings of the IEEE/CVF   \n335 Conference on Computer Vision and Pattern Recognition, pages 4299\u20134309, 2022.   \n336 [13] Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering   \n337 for unsupervised learning of visual features. In Proceedings of the European conference on   \n338 computer vision (ECCV), pages 132\u2013149, 2018.   \n339 [14] Jyh-Jing Hwang, Stella X Yu, Jianbo Shi, Maxwell D Collins, Tien-Ju Yang, Xiao Zhang,   \n340 and Liang-Chieh Chen. Segsort: Segmentation by discriminative sorting of segments. In   \n341 Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7334\u20137344,   \n342 2019.   \n343 [15] Yuri Y Boykov and M-P Jolly. Interactive graph cuts for optimal boundary & region segmenta  \n344 tion of objects in nd images. In Proceedings eighth IEEE international conference on computer   \n345 vision. ICCV 2001, volume 1, pages 105\u2013112. IEEE, 2001.   \n346 [16] Yuri Boykov, Olga Veksler, and Ramin Zabih. Fast approximate energy minimization via graph   \n347 cuts. IEEE Transactions on pattern analysis and machine intelligence, 23(11):1222\u20131239,   \n348 2001.   \n349 [17] Philipp Kr\u00e4henb\u00fchl and Vladlen Koltun. Efficient inference in fully connected CRFs with   \n350 Gaussian edge potentials. Advances in neural information processing systems, 24, 2011.   \n351 [18] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam.   \n352 Encoder-decoder with atrous separable convolution for semantic image segmentation. In   \n353 Proceedings of the European conference on computer vision (ECCV), pages 801\u2013818, 2018.   \n354 [19] Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced loss   \n355 based on effective number of samples. In IEEE conference on Computer Vision and Pattern   \n356 Recognition (CVPR), pages 9268\u20139277, 2019.   \n357 [20] John Bridle, Anthony Heading, and David MacKay. Unsupervised classifiers, mutual informa  \n358 tion and\u2019phantom targets. Advances in neural information processing systems, 4, 1991.   \n359 [21] Andreas Krause, Pietro Perona, and Ryan Gomes. Discriminative clustering by regularized   \n360 information maximization. Advances in neural information processing systems, 23, 2010.   \n361 [22] Hoel Kervadec, Jose Dolz, Meng Tang, Eric Granger, Yuri Boykov, and Ismail Ben Ayed.   \n362 Size-constraint loss for weakly supervised CNN segmentation. In Medical Imaging with Deep   \n363 Learning, 2018.   \n364 [23] Deepak Pathak, Philipp Krahenbuhl, and Trevor Darrell. Constrained convolutional neural   \n365 networks for weakly supervised segmentation. In Proceedings of the IEEE international   \n366 conference on computer vision, pages 1796\u20131804, 2015.   \n367 [24] Jiwoon Ahn and Suha Kwak. Learning pixel-level semantic affinity with image-level supervision   \n368 for weakly supervised semantic segmentation. In Proceedings of the IEEE conference on   \n369 computer vision and pattern recognition, pages 4981\u20134990, 2018.   \n370 [25] Zhaozheng Chen and Qianru Sun. Extracting class activation maps from non-discriminative   \n371 features as well. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern   \n372 Recognition, pages 3135\u20133144, 2023.   \n373 [26] Lianghui Zhu, Yingyue Li, Jieming Fang, Yan Liu, Hao Xin, Wenyu Liu, and Xinggang Wang.   \n374 Weaktr: Exploring plain vision transformer for weakly-supervised semantic segmentation. arXiv   \n375 preprint arXiv:2304.01184, 2023.   \n376 [27] Xiaobo Yang and Xiaojin Gong. Foundation model assisted weakly supervised semantic   \n377 segmentation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer   \n378 Vision, pages 523\u2013532, 2024.   \n379 [28] Lixiang Ru, Heliang Zheng, Yibing Zhan, and Bo Du. Token contrast for weakly-supervised   \n380 semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and   \n381 Pattern Recognition, pages 3093\u20133102, 2023.   \n382 [29] Zhiwei Yang, Kexue Fu, Minghong Duan, Linhao Qu, Shuo Wang, and Zhijian Song. Separate   \n383 and conquer: Decoupling co-occurrence via decomposition and representation for weakly   \n384 supervised semantic segmentation. arXiv preprint arXiv:2402.18467, 2024.   \n385 [30] Xinyu Yang, Hossein Rahmani, Sue Black, and Bryan M Williams. Weakly super  \n386 vised co-training with swapping assignments for semantic segmentation. arXiv preprint   \n387 arXiv:2402.17891, 2024.   \n388 [31] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr   \n389 Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer   \n390 Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014,   \n391 Proceedings, Part V 13, pages 740\u2013755. Springer, 2014.   \n392 [32] Olivier Bernard, Alain Lalande, Clement Zotti, Frederick Cervenansky, Xin Yang, Pheng-Ann   \n393 Heng, Irem Cetin, Karim Lekadir, Oscar Camara, Miguel Angel Gonzalez Ballester, et al. Deep   \n394 learning techniques for automatic mri cardiac multi-structures segmentation and diagnosis: is   \n395 the problem solved? IEEE transactions on medical imaging, 37(11):2514\u20132525, 2018.   \n396 [33] Bharath Hariharan, Pablo Arbel\u00e1ez, Lubomir Bourdev, Subhransu Maji, and Jitendra Malik.   \n397 Semantic contours from inverse detectors. In 2011 international conference on computer vision,   \n398 pages 991\u2013998. IEEE, 2011.   \n399 [34] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille.   \n400 Semantic image segmentation with deep convolutional nets and fully connected crfs. arXiv   \n401 preprint arXiv:1412.7062, 2014.   \n402 [35] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,   \n403 Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.   \n404 An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint   \n405 arXiv:2010.11929, 2020.   \n406 [36] Robin Strudel, Ricardo Garcia, Ivan Laptev, and Cordelia Schmid. Segmenter: Transformer for   \n407 semantic segmentation. In Proceedings of the IEEE/CVF international conference on computer   \n408 vision, pages 7262\u20137272, 2021.   \n409 [37] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen.   \n410 Mobilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference   \n411 on computer vision and pattern recognition, pages 4510\u20134520, 2018.   \n412 [38] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng   \n413 Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual   \n414 recognition challenge. International journal of computer vision, 115:211\u2013252, 2015.   \n415 [39] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large  \n416 scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern   \n417 recognition, pages 248\u2013255. Ieee, 2009.   \n418 [40] Sanghyun Jo, In-Jae Yu, and Kyungsu Kim. Mars: Model-agnostic biased object removal   \n419 without additional supervision for weakly-supervised semantic segmentation. arXiv preprint   \n420 arXiv:2304.09913, 2023.   \n421 [41] Changwei Wang, Rongtao Xu, Shibiao Xu, Weiliang Meng, and Xiaopeng Zhang. Treating   \n422 pseudo-labels generation as image matting for weakly supervised semantic segmentation. In   \n423 Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 755\u2013765,   \n424 2023.   \n425 [42] Lian Xu, Wanli Ouyang, Mohammed Bennamoun, Farid Boussaid, and Dan Xu. Multi  \n426 class token transformer for weakly supervised semantic segmentation. In Proceedings of the   \n427 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4310\u20134319, 2022.   \n428 [43] Zesen Cheng, Pengchong Qiao, Kehan Li, Siheng Li, Pengxu Wei, Xiangyang Ji, Li Yuan,   \n429 Chang Liu, and Jie Chen. Out-of-candidate rectification for weakly supervised semantic   \n430 segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern   \n431 Recognition, pages 23673\u201323684, 2023.   \n432 [44] Zhaozheng Chen, Tan Wang, Xiongwei Wu, Xian-Sheng Hua, Hanwang Zhang, and Qianru   \n433 Sun. Class re-activation maps for weakly-supervised semantic segmentation. In Proceedings of   \n434 the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 969\u2013978, 2022.   \n435 [45] Bingfeng Zhang, Jimin Xiao, Yunchao Wei, Mingjie Sun, and Kaizhu Huang. Reliability does   \n436 matter: An end-to-end weakly supervised semantic segmentation approach. In Proceedings of   \n437 the AAAI Conference on Artificial Intelligence, volume 34, pages 12765\u201312772, 2020.   \n438 [46] A. Bearman, O. Russakovsky, V. Ferrari, and F. Li. Semantic segmentation with point supervi  \n439 sion. In ECCV, 2015. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "image", "img_path": "iRHxp1ibFj/tmp/e16a734f5db8da609780246de4931590c4400b2c51082ea38e7d6950fa0c6006.jpg", "img_caption": ["Figure 7: Segmentation examples using size-target supervision $\\it m R E=8\\%$ ). Model backbones are shown in the top-left corner of the predictions, see Table 2 for decoders. "], "img_footnote": [], "page_idx": 12}, {"type": "text", "text": "440 A Appendix / supplemental material ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "441 A.1 Labeling costs and accuracies reported in Figure 1 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "442 Labelling costs. Figure 1 in the paper shows labeling speed and accuracy for different forms of   \n443 supervision on PASCAL VOC. The table at the bottom of Figure 1 shows ballpark estimates of   \n444 average labeling time per image in the whole dataset. We use the data in [46], as well as Table 1 in   \n445 the paper, and aggregate all labeling speeds from \u201cper class\u201d, \u201cper instance\u201d, or \u201cper point\u201d to \u201cper   \n446 image\u201d using the average number of instances or classes in each image and the aggregation rules   \n447 formulated in [46], see their Section 4. The top-left corner in each picture shows the corresponding   \n448 estimated labeling times for the representative multi-instance image. All the labeling times are only   \n449 rough estimates, but they are intuitive. The relative costs for point supervision seem underestimated,   \n450 but they follow evaluation conventions detailed in [46].   \n451 Accuracies. The values of \u201cpoint\u201d, \u201csize target\u201d and \u201cfull supervision\u201d accuracy $(\\mathrm{mIOU}\\%)$ ) are based   \n452 on the experiments in the paper (Figure 4). We follow the learning rate scheme in DeepLab $^{\\,}{\\cal V}3+$ [18]   \n453 for the training with full supervision. For fairness, we compare these with end-to-end methods using   \n454 similar ResNet backbones in tag- [2] and box-supervision [3]. Typical SOTA methods for tag and   \n455 box supervision use special architectural modifications, unlike our generic size-target loss, cannot be   \n456 seamlessly plugged into any segmentation model. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "457 A.2 Qualitative results ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "458 Figure 7 presents the qualitative examples of our method on PASCAL (left) and COCO (right)   \n459 validation sets. Despite size targets providing only image-level information, segmentation models   \n460 can precisely identify object locations, eliminating the need for localization methods like CAM. ", "page_idx": 12}, {"type": "text", "text": "461 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Justification: Contributions are included in the abstract and listed in Sec. 1.3 in the introduction. ", "page_idx": 13}, {"type": "text", "text": "9 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n0 made in the paper.   \n71 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n2 contributions made in the paper and important assumptions and limitations. A No or   \n73 NA answer to this question will not be perceived well by the reviewers.   \n4 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n75 much the results can be expected to generalize to other settings.   \n76 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n7 are not attained by the paper. ", "page_idx": 13}, {"type": "text", "text": "78 2. Limitations ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [No] ", "page_idx": 13}, {"type": "text", "text": "Justification: Although the limitations were not explicitly detailed in the paper, we mentioned that only a subset of the PASCAL dataset was labeled due to resource constraints, see Sec. 3.4. To address this, we generated approximate synthetic size targets by corrupting the exact size targets. This allowed us to evaluate our method on the entire PASCAL dataset, as well as on COCO and ACDC datasets. ", "page_idx": 13}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 13}, {"type": "text", "text": "513 3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "14 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n15 a complete (and correct) proof?   \n16 Answer:[NA]   \n17 Justification: The paper does not include theoretical results.   \n18 Guidelines:   \n19 \u2022 The answer NA means that the paper does not include theoretical results.   \n20 \u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross  \n21 referenced.   \n22 \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n23 \u2022 The proofs can either appear in the main paper or the supplemental material, but if   \n24 they appear in the supplemental material, the authors are encouraged to provide a short   \n25 proof sketch to provide intuition.   \n26 \u2022 Inversely, any informal proof provided in the core of the paper should be complemented   \n27 by formal proofs provided in appendix or supplemental material.   \n28 \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced.   \n529 4. Experimental Result Reproducibility   \n30 Question: Does the paper fully disclose all the information needed to reproduce the main ex", "page_idx": 14}, {"type": "text", "text": "perimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 14}, {"type": "text", "text": "Justification: Our size-target loss function is discussed in the 2. The experimental settings are discussed in the 3.1 ", "page_idx": 14}, {"type": "text", "text": "Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 14}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "69 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n70 tions to faithfully reproduce the main experimental results, as described in supplemental   \n71 material?   \n72 Answer: [No]   \n73 Justification: To preserve anonymity, the code will be released in the final version.   \n74 Guidelines:   \n75 \u2022 The answer NA means that paper does not include experiments requiring code.   \n76 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n77 public/guides/CodeSubmissionPolicy) for more details.   \n78 \u2022 While we encourage the release of code and data, we understand that this might not be   \n79 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n80 including code, unless this is central to the contribution (e.g., for a new open-source   \n81 benchmark).   \n82 \u2022 The instructions should contain the exact command and environment needed to run to   \n83 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n84 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n85 \u2022 The authors should provide instructions on data access and preparation, including how   \n86 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n87 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n88 proposed method and baselines. If only a subset of experiments are reproducible, they   \n89 should state which ones are omitted from the script and why.   \n90 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n91 versions (if applicable).   \n92 \u2022 Providing as much information as possible in supplemental material (appended to the   \n93 paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 15}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: The experimental setting is detailed in the Sec. 3.1 Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 15}, {"type": "text", "text": "06 7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "607 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n608 information about the statistical significance of the experiments? ", "page_idx": 15}, {"type": "text", "text": "Answer: [No] ", "page_idx": 15}, {"type": "text", "text": "Justification: Error bars are not reported because it would be too computationally expensive. Our plots in Figure 4, 5, 6 are smooth enough to verify our method. Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). ", "page_idx": 15}, {"type": "text", "text": "620 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n621 call to a library function, bootstrap, etc.)   \n622 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n623 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n624 of the mean.   \n625 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n626 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n627 of Normality of errors is not verified.   \n628 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n629 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n630 error rates).   \n631 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n632 they were calculated and reference the corresponding figures or tables in the text.   \n633 8. Experiments Compute Resources   \n634 Question: For each experiment, does the paper provide sufficient information on the com  \n635 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n636 the experiments?   \n637 Answer: [Yes]   \n638 Justification: The information on the computer resources is detailed in Sec. 3.1   \n639 Guidelines:   \n640 \u2022 The answer NA means that the paper does not include experiments.   \n641 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n642 or cloud provider, including relevant memory and storage.   \n643 \u2022 The paper should provide the amount of compute required for each of the individual   \n644 experimental runs as well as estimate the total compute.   \n645 \u2022 The paper should disclose whether the full research project required more compute   \n646 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n647 didn\u2019t make it into the paper).   \n648 9. Code Of Ethics   \n649 Question: Does the research conducted in the paper conform, in every respect, with the   \n650 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n651 Answer: [Yes]   \n652 Justification: The research in the paper conforms with the code of ethics.   \n653 Guidelines:   \n654 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n655 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n656 deviation from the Code of Ethics.   \n657 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n658 eration due to laws or regulations in their jurisdiction).   \n659 10. Broader Impacts   \n660 Question: Does the paper discuss both potential positive societal impacts and negative   \n661 societal impacts of the work performed?   \n662 Answer: [NA]   \n663 Justification: Our research on weakly-supervised semantic segmentation is a purely technical   \n664 advancement to improve image segmentation, with no direct societal impacts or associated   \n665 ethical concerns.   \n666 Guidelines:   \n667 \u2022 The answer NA means that there is no societal impact of the work performed.   \n668 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n669 impact or why the paper does not address societal impact.   \n670 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n671 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n672 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n673 groups), privacy considerations, and security considerations.   \n674 \u2022 The conference expects that many papers will be foundational research and not tied   \n675 to particular applications, let alone deployments. However, if there is a direct path to   \n676 any negative applications, the authors should point it out. For example, it is legitimate   \n677 to point out that an improvement in the quality of generative models could be used to   \n678 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n679 that a generic algorithm for optimizing neural networks could enable people to train   \n680 models that generate Deepfakes faster.   \n681 \u2022 The authors should consider possible harms that could arise when the technology is   \n682 being used as intended and functioning correctly, harms that could arise when the   \n683 technology is being used as intended but gives incorrect results, and harms following   \n684 from (intentional or unintentional) misuse of the technology.   \n685 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n686 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n687 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n688 feedback over time, improving the efficiency and accessibility of ML).   \n689 11. Safeguards   \n690 Question: Does the paper describe safeguards that have been put in place for responsible   \n691 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n692 image generators, or scraped datasets)?   \n693 Answer: [NA]   \n694 Justification: This paper poses no such risks.   \n695 Guidelines:   \n696 \u2022 The answer NA means that the paper poses no such risks.   \n697 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n698 necessary safeguards to allow for controlled use of the model, for example by requiring   \n699 that users adhere to usage guidelines or restrictions to access the model or implementing   \n700 safety filters.   \n701 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n702 should describe how they avoided releasing unsafe images.   \n703 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n704 not require this, but we encourage authors to take this into account and make a best   \n705 faith effort.   \n706 12. Licenses for existing assets   \n707 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n708 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n709 properly respected?   \n710 Answer: [Yes]   \n711 Justification: The owners of assets used in this paper are credited and the license is mentioned   \n712 and respected.   \n713 Guidelines:   \n714 \u2022 The answer NA means that the paper does not use existing assets.   \n715 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n716 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n717 URL.   \n718 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n719 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n720 service of that source should be provided.   \n721 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n722 package should be provided. For popular datasets, paperswithcode.com/datasets   \n723 has curated licenses for some datasets. Their licensing guide can help determine the   \n724 license of a dataset.   \n725 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n726 the derived asset (if it has changed) should be provided.   \n727 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n728 the asset\u2019s creators.   \n729 13. New Assets   \n730 Question: Are new assets introduced in the paper well documented and is the documentation   \n731 provided alongside the assets?   \n732 Answer: [NA]   \n733 Justification: The paper does not release new assets.   \n734 Guidelines:   \n735 \u2022 The answer NA means that the paper does not release new assets.   \n736 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n737 submissions via structured templates. This includes details about training, license,   \n738 limitations, etc.   \n739 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n740 asset is used.   \n741 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n742 create an anonymized URL or include an anonymized zip file.   \n743 14. Crowdsourcing and Research with Human Subjects   \n744 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n745 include the full text of instructions given to participants and screenshots, if applicable, as   \n746 well as details about compensation (if any)?   \n747 Answer: [NA]   \n748 Justification: The paper does not involve crowdsourcing or research with human subjects.   \n749 Guidelines:   \n750 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n751 human subjects.   \n752 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n753 tion of the paper involves human subjects, then as much detail as possible should be   \n754 included in the main paper.   \n755 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n756 or other labor should be paid at least the minimum wage in the country of the data   \n757 collector.   \n758 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n759 Subjects   \n760 Question: Does the paper describe potential risks incurred by study participants, whether   \n761 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n762 approvals (or an equivalent approval/review based on the requirements of your country or   \n763 institution) were obtained?   \n764 Answer: [NA]   \n765 Justification: The paper does not involve crowdsourcing or research with human subjects.   \n766 Guidelines:   \n767 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n768 human subjects.   \n769 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)   \n770 may be required for any human subjects research. If you obtained IRB approval, you   \n771 should clearly state this in the paper. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 19}]