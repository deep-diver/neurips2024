[{"figure_path": "iRHxp1ibFj/figures/figures_1_1.jpg", "caption": "Figure 1: Supervision types for segmentation: labeling speed and accuracy on PASCAL. The top-left corner of each image shows its estimated labeling time based on observed instances. The table shows per-image labeling times averaged over the data and mean Intersection-over-Union (mIoU) for comparable end-to-end methods with similar ResNet backbones (ResNet101 or WideResNet38 [1]), for fairness. We obtained mIoU scores, except for the \u201ctag\u201d and \u201cbox\u201d scores from [2] and [3]. Our supplemental materials detail evaluation of the labeling times and mIoU. For completeness, Tab.2 includes more complex architectures and multi-stage systems, e.g. for tags. This paper focuses on standard segmentation architectures for size supervision.", "description": "This figure compares different supervision methods for semantic segmentation on the PASCAL dataset.  It shows example images annotated with different supervision types: tag, point, size target (the authors' method), bounding box, and full supervision. Each example illustrates the annotation process and provides the estimated time required for labeling.  The figure also includes a table that summarizes the average per-image labeling time and mIoU (mean Intersection over Union) scores achieved by each method, highlighting the efficiency of the proposed \"size target\" approach.", "section": "1 Introduction"}, {"figure_path": "iRHxp1ibFj/figures/figures_2_1.jpg", "caption": "Figure 7: Segmentation examples using size-target supervision (mRE = 8%). Model backbones are shown in the top-left corner of the predictions, see Table 2 for decoders.", "description": "This figure shows three example images from the PASCAL and COCO datasets.  For each image, the ground truth segmentation mask is shown alongside the model's predictions using three different model backbones (WideResnet38, Resnet101, and Vision Transformer).  The image demonstrates the model's ability to accurately segment objects using only approximate size targets (with a mean relative error of 8%), highlighting the effectiveness of the proposed image-level supervision method.", "section": "A.2 Qualitative results"}, {"figure_path": "iRHxp1ibFj/figures/figures_4_1.jpg", "caption": "Figure 3: Forward vs reverse KL divergence. Assuming binary classification K = 2, we can represent all possible probability distributions as points on the interval [0,1]. The solid curves illustrate our \"strong\" size constraint, i.e. the forward KL divergence KL(v||S) for the average prediction S. We show two examples of volumetric prior v\u2081 = (0.9,0.1) (blue curve) and v\u2082 = (0.5,0.5) (red curve). For comparison, the dashed curves represent reverse KL divergence KL(S||v).", "description": "The figure illustrates the difference between forward and reverse KL divergence in the context of binary classification.  Forward KL divergence is shown to be more sensitive to zero values, which is advantageous for ensuring that classes predicted to have a non-zero size in the target distribution are not assigned a zero size in the model's prediction.", "section": "Size-target loss and its properties"}, {"figure_path": "iRHxp1ibFj/figures/figures_6_1.jpg", "caption": "Figure 4: Segmentation results on the PASCAL dataset with R101-based DeeplabV3+ networks. The green bar in both plots indicates the segmentation accuracy for full ground truth masks (i.e. full supervision). The left plot shows the training and validation accuracy using approximate size targets. The segmentation is trained using losses (2) (red curve) or (12) (blue curve), where size targets are subject to various levels of corruption (15,16). The right plot shows validation accuracy for seed supervision of varying lengths with (blue curve) and without (red curve) using size targets. The line styles of the blue curves differentiate among various levels of corruption.", "description": "This figure presents the results of experiments on the PASCAL dataset using a R101-based DeepLabV3+ network.  The left plot shows how the training and validation accuracy changes with different levels of corruption in the size targets, comparing using only the size target loss (2) versus the combined size target and CRF loss (12). The right plot shows how the validation accuracy changes based on the length of seed supervision, comparing using size targets versus not using size targets.", "section": "3 Experiments"}, {"figure_path": "iRHxp1ibFj/figures/figures_6_2.jpg", "caption": "Figure 5: Left plot shows the quality of human annotations in terms of relative errors for the dog, cat, and bird classes within the PASCAL dataset. The histograms are normalized by the number of images in each class. The mean relative error for the three classes is 15.9%. For comparison, the dashed line shows the relative error distribution of synthetic size targets as defined in (15) for \u03c3 = 20.0% which aligns with the mRE of 15.9%, see (16). The right plot presents 4-way multi-class (cat, dog, bird, and background) segmentation accuracy using human-annotated (red star at mRE = 15.9%) and synthetic (blue curve) size targets, employing ResNet101-based DeeplabV3+ networks. Consistent with experiments in Sec. 3.2, synthetic size targets are generated at various levels of corruption. The green line indicates the segmentation accuracy of full supervision using ground truth masks.", "description": "The left plot shows the distribution of relative size errors for human-annotated size targets for the three classes (cat, dog, bird). The mean relative error is 15.9%.  The right plot demonstrates the segmentation accuracy using human-annotated and synthetic size targets with various levels of noise (mRE).  The results show that the model's accuracy is robust to moderate levels of error in the size targets and approaches the accuracy achieved with full supervision when using human-annotated size targets.", "section": "3.4 Human-annotated size targets"}, {"figure_path": "iRHxp1ibFj/figures/figures_8_1.jpg", "caption": "Figure 6: Size-targets (2) vs. size-barriers (17) on the ACDC dataset. The left plot shows the accuracy of the binary segmentation models (MobileNetV2-based DeeplabV3+) measured by DSC. The blue curve shows size-target accuracy with various levels of corruption. The dashed green line shows the accuracy of the size-barrier technique [22]. The dashed red line shows the accuracy using the mean size target for all training images. The gray line indicates the result of full supervision. The right image shows randomly selected qualitative results of size-barrier [22] and approximate size target (mRE = 8%). Yellow shows true positive pixels, green is false positive, and red is false negatives.", "description": "The figure compares size-target and size-barrier approaches for binary segmentation on the ACDC dataset.  The left plot shows the DSC (Dice Similarity Coefficient) for various levels of size target error (mRE), comparing size-target, size-barrier, using the mean size target, and full supervision. The right side displays qualitative segmentation results for both methods, highlighting true/false positives/negatives.", "section": "3.6 Medical data: size-target vs. size-barrier"}, {"figure_path": "iRHxp1ibFj/figures/figures_12_1.jpg", "caption": "Figure 7: Segmentation examples using size-target supervision (mRE = 8%). Model backbones are shown in the top-left corner of the predictions, see Table 2 for decoders.", "description": "This figure shows qualitative results of the proposed size-target supervision method on the PASCAL VOC and COCO datasets.  For each dataset, the image, ground truth mask, and segmentation results using three different backbones (WideResNet-38, ResNet-101, Vision Transformer) are displayed. The top-left corner of each prediction indicates the backbone used for the segmentation model. Table 2 in the paper provides details about the specific decoders used with each backbone. The images illustrate the model's ability to accurately segment objects using only approximate size information (with a mean relative error of 8%).", "section": "A.2 Qualitative results"}]