[{"figure_path": "tb1MlJCY5g/figures/figures_1_1.jpg", "caption": "Figure 1: Illustration of KALM utilizing LLM to generate environmental rollouts at the numerical vector level. (1) Grounding phase that fine-tunes LLM with supervised fine-tuning on the environmental data. (2) Generation phase that prompts LLM to generate data for novel skills. KALM modifies the input/output layer of LLM, enabling it to process and interpret non-textual data.", "description": "This figure illustrates the two main phases of KALM's usage of LLMs: grounding and rollout generation.  The grounding phase involves fine-tuning the LLM using supervised learning on environmental data (numerical vectors). This allows the LLM to understand and process the environment's data.  The rollout generation phase uses the grounded LLM to generate imaginary rollouts (sequences of states and actions) for novel tasks, based on textual goal descriptions.  KALM modifies the LLM's input/output layers to handle the numerical vector data of the environment.", "section": "Method"}, {"figure_path": "tb1MlJCY5g/figures/figures_3_1.jpg", "caption": "Figure 2: Overall framework of KALM method. (A) Overall training procedure, consisting of three key steps: LLM grounding that fine-tunes LLM with environmental data, rollout generation that prompts the LLM to generate imaginary rollouts for novel tasks, and offline RL training that facilitates skill acquisition. The dashed line (- - ->) represents an optional online process, allowing for continuous improvement through iterative data collection and training. (B) The adapted network structure of the LLM. KALM adapts the architecture of LLM to process both text and environmental data.", "description": "This figure illustrates the overall framework of the KALM method. Panel (A) shows the three key steps involved in training the agent: LLM grounding (fine-tuning the LLM with environmental data), rollout generation (using the LLM to generate imaginary rollouts for new tasks), and offline RL training. Panel (B) shows the adapted network architecture of the LLM, which has been modified to process both textual and environmental data.", "section": "Method"}, {"figure_path": "tb1MlJCY5g/figures/figures_5_1.jpg", "caption": "Figure 3: A visualization of the environments in our experiments. (A) In the CLEVR-Robot environment, the agent (silverpoint) manipulates five movable balls to reach a goal configuration. (B) In Meta-world, the agent controls a Sawyer robot to manipulate various objects.", "description": "This figure visualizes the two robotic manipulation environments used in the paper's experiments: CLEVR-Robot and Meta-world.  CLEVR-Robot shows a simple setup where a robotic agent manipulates five colored balls to achieve a specific arrangement.  Meta-world depicts a more complex scenario with a Sawyer robot arm interacting with various objects, like doors, drawers, and other manipulanda.  These environments showcase the range of tasks used to evaluate the KALM model's performance.", "section": "4.1 Experimental Setting"}, {"figure_path": "tb1MlJCY5g/figures/figures_6_1.jpg", "caption": "Figure 4: Success rate bars of different methods on various levels of goals. The x-axis denotes the offline RL algorithm, and the y-axis denotes the success rate for completing various natural language goals. The success rate is calculated based on the average of the last five checkpoints, and the error bars stand for the half standard deviation over three random seeds. We present the training curves in Fig. 8.", "description": "This figure compares the success rates of different offline reinforcement learning (RL) algorithms on four types of tasks with varying levels of difficulty: tasks from offline data, rephrasing goals, unseen (easy) goals, and unseen (hard) goals.  Each bar represents the average success rate of a particular RL algorithm, with and without the KALM method. Error bars show the standard deviation, indicating the reliability of the results.  The results illustrate that KALM significantly improves the success rate, especially on more challenging, unseen tasks.", "section": "4.2 Main Results"}, {"figure_path": "tb1MlJCY5g/figures/figures_7_1.jpg", "caption": "Figure 5: Comparison with baseline method that directly utilizes LLM as policy.", "description": "This figure compares the performance of KALM with two baseline methods: using LLM directly as a policy and Decision Transformer (DT).  The x-axis shows the type of task: rephrasing goals, unseen (easy), and unseen (hard). The y-axis represents the success rate. KALM significantly outperforms both baselines, especially on more complex, unseen tasks.", "section": "4.2 Main Results"}, {"figure_path": "tb1MlJCY5g/figures/figures_8_1.jpg", "caption": "Figure 6: Examples of the imaginary rollouts generated by the fine-tuned LLM.", "description": "This figure shows two examples of imaginary rollouts generated by the fine-tuned Large Language Model (LLM) in the KALM (Knowledgeable Agent from Language Model Rollouts) method.  The top row shows a robotic arm successfully navigating an obstacle (a wall) to reach a target object. The bottom row shows a successful arrangement of colored balls in a circle, using a specified ball as the center. These examples demonstrate the LLM's capacity to generate realistic and task-appropriate sequences of actions.", "section": "Additional Results"}, {"figure_path": "tb1MlJCY5g/figures/figures_8_2.jpg", "caption": "Figure 7: Rollout explanation accuracy.", "description": "The figure shows the accuracy of the LLM in explaining rollouts, comparing performance on seen and unseen prompts.  The x-axis represents the training epoch, and the y-axis represents the accuracy in percentage. The results indicate the model's ability to explain rollouts improves with training, though it is generally better at explaining seen rollouts.", "section": "Additional Results"}, {"figure_path": "tb1MlJCY5g/figures/figures_8_3.jpg", "caption": "Figure 7: Rollout explanation accuracy.", "description": "This figure shows the accuracy of the LLM in explaining rollouts.  The accuracy is measured by comparing keywords in the LLM's explanation to the ground truth.  The results demonstrate high accuracy (over 85%) for both seen and unseen prompts, indicating that the LLM effectively captures the meaning of rollouts.  The graph shows accuracy over training epochs, demonstrating improvement over time.", "section": "4.3 Performance of LLM Grounding"}, {"figure_path": "tb1MlJCY5g/figures/figures_23_1.jpg", "caption": "Figure 8: Success rate curves of different methods on various levels of goals. The x-axis denotes the training epochs, and the y-axis denotes the success rate for completing various natural language goals. The shaded area stands for the half standard deviation over three random seeds.", "description": "This figure shows the training curves of different offline reinforcement learning methods on four types of goals: tasks in offline data, rephrasing goals, unseen (easy) goals, and unseen (hard) goals.  The x-axis represents the training epochs, and the y-axis shows the success rate (percentage) achieved by each method on each goal type. Error bars representing half standard deviation are included for each data point, indicating the variability in performance across multiple random runs.  The figure visually compares the performance of various offline RL algorithms with and without the incorporation of the KALM method, highlighting the improvements gained by using the proposed approach. It helps in understanding how KALM affects the learning progress and the ability of the agent to generalize to novel tasks.", "section": "C Additional Results\nC.1 Training Curves of Different Methods"}, {"figure_path": "tb1MlJCY5g/figures/figures_24_1.jpg", "caption": "Figure 9: Additional examples of the generated rollouts for rephrasing goal tasks.", "description": "This figure shows additional examples of generated rollouts for rephrasing goal tasks, in both Meta-world and CLEVR-Robot environments.  The top row depicts a successful rollout in Meta-world where the robot arm navigates around a wall to reach a target object. The bottom row shows a successful rollout in CLEVR-Robot where the red ball is moved to the left of the blue ball. A failure case is also presented for each environment, highlighting situations where the LLM's rollout generation struggles with complex scenarios or ambiguous instructions.", "section": "Additional Results"}, {"figure_path": "tb1MlJCY5g/figures/figures_24_2.jpg", "caption": "Figure 9: Additional examples of the generated rollouts for rephrasing goal tasks.", "description": "This figure shows additional examples of generated rollouts for rephrasing goal tasks in both Meta-world and CLEVR-Robot environments.  The top row illustrates a successful rollout in Meta-world where a robot arm opens a door, and the bottom row shows a successful rollout in CLEVR-Robot where the agent moves balls to the specified positions. A failure case is also shown for each environment, demonstrating situations where the LLM's generated rollout does not successfully complete the task.", "section": "C.2 More Examples of Generated Rollouts"}, {"figure_path": "tb1MlJCY5g/figures/figures_24_3.jpg", "caption": "Figure 9: Additional examples of the generated rollouts for rephrasing goal tasks.", "description": "This figure shows additional examples of generated rollouts for rephrasing goal tasks, comparing successful and unsuccessful examples in both Meta-world and CLEVR-Robot environments.  The Meta-world examples illustrate the robot's manipulation of objects, while the CLEVR-Robot examples demonstrate the manipulation of colored balls. The figure showcases the LLM's ability to generate rollouts that align with the given goals, but also highlights cases where the LLM struggles to generate successful plans, particularly in complex scenarios.", "section": "Additional Results"}, {"figure_path": "tb1MlJCY5g/figures/figures_25_1.jpg", "caption": "Figure 9: Additional examples of the generated rollouts for rephrasing goal tasks.", "description": "This figure shows additional examples of generated rollouts by the LLM for rephrasing goal tasks.  The top row shows successful rollouts for a Meta-world task (moving an object to a specific location), demonstrating the LLM's ability to generate a successful sequence of actions. The bottom row shows successful and failed rollouts for CLEVR-Robot tasks (repositioning colored balls). The failure example highlights that while the LLM can understand the goal, it may fail to generate a correct rollout if the initial state is not ideal.", "section": "C.2 More Examples of Generated Rollouts"}, {"figure_path": "tb1MlJCY5g/figures/figures_25_2.jpg", "caption": "Figure 9: Additional examples of the generated rollouts for rephrasing goal tasks.", "description": "This figure shows additional examples of generated rollouts for rephrasing goal tasks in both Meta-world and CLEVR-Robot environments.  The top row displays successful rollouts where the robot arm successfully navigates to its goal despite obstacles. The bottom row shows unsuccessful attempts, highlighting the limitations of the LLM in complex scenarios, such as navigating through obstacles or accurately understanding a complex instruction.", "section": "C.2 More Examples of Generated Rollouts"}, {"figure_path": "tb1MlJCY5g/figures/figures_25_3.jpg", "caption": "Figure 6: Examples of the imaginary rollouts generated by the fine-tuned LLM.", "description": "This figure shows two examples of imaginary rollouts generated by the fine-tuned large language model (LLM) for the CLEVR-Robot environment. The first example demonstrates the LLM's ability to generate a rollout that successfully moves a specific object to the desired location, even with an obstacle (wall) in the path. This showcases that the LLM understands the environment's dynamics and can plan accordingly. The second example demonstrates how the LLM can generate a rollout to arrange the balls in a circle around the green ball, even though this specific task was not present in the training data. This highlights the LLM's ability to generalize and perform tasks that require higher-level understanding and planning.", "section": "Additional Results"}, {"figure_path": "tb1MlJCY5g/figures/figures_25_4.jpg", "caption": "Figure 6: Examples of the imaginary rollouts generated by the fine-tuned LLM.", "description": "This figure shows two examples of imaginary rollouts generated by the fine-tuned Large Language Model (LLM) in the KALM method. The first example (a) shows a robotic arm navigating around a wall to pick up an object; the second example (b) shows several colored balls being arranged in a circle around a central green ball.  These examples illustrate the LLM's ability to generate plausible and goal-oriented sequences of actions and states, even for tasks or situations not explicitly present in its training data.", "section": "Additional Results"}, {"figure_path": "tb1MlJCY5g/figures/figures_25_5.jpg", "caption": "Figure 6: Examples of the imaginary rollouts generated by the fine-tuned LLM.", "description": "This figure shows two examples of imaginary rollouts generated by the fine-tuned Large Language Model (LLM) in the KALM method.  The top example depicts a robotic arm successfully navigating a wall to reach its goal. The bottom example shows the LLM's ability to generate rollouts for a complex task of arranging multiple balls around a central green ball according to a specific pattern. These examples highlight the LLM's ability to generate meaningful and physically realistic rollouts.", "section": "Additional Results"}, {"figure_path": "tb1MlJCY5g/figures/figures_25_6.jpg", "caption": "Figure 6: Examples of the imaginary rollouts generated by the fine-tuned LLM.", "description": "This figure shows two examples of imaginary rollouts generated by the fine-tuned Large Language Model (LLM) in the KALM method. The first example demonstrates a successful rollout in the Meta-world environment where the robot successfully navigates around a wall to reach a target location. The second example shows a successful rollout in the CLEVR-Robot environment where the robot successfully arranges multiple balls according to a complex goal description. These examples illustrate the LLM's ability to generate realistic and diverse rollouts for novel tasks.", "section": "Additional Results"}, {"figure_path": "tb1MlJCY5g/figures/figures_26_1.jpg", "caption": "Figure 9: Additional examples of the generated rollouts for rephrasing goal tasks.", "description": "This figure shows additional examples of generated rollouts for rephrasing goal tasks in both Meta-world and CLEVR-Robot environments.  In Meta-world, the successful example demonstrates the robot arm moving to grasp an object, navigating around an obstacle (a wall). The failed example shows an issue where the robot doesn't seem to correctly interpret the goal of opening a closed window. In CLEVR-Robot, the successful example illustrates a robot manipulating colored balls according to a given goal (moving one ball in front of another). The failed example shows a case where the robot's actions don't completely align with the desired outcome.", "section": "C.2 More Examples of Generated Rollouts"}, {"figure_path": "tb1MlJCY5g/figures/figures_26_2.jpg", "caption": "Figure 6: Examples of the imaginary rollouts generated by the fine-tuned LLM.", "description": "This figure shows two examples of imaginary rollouts generated by the fine-tuned Large Language Model (LLM) for unseen (hard) tasks in the CLEVR-Robot and Meta-world environments.  The top row displays a successful rollout in CLEVR-Robot where the LLM arranges colored balls around a green ball to form a circle, even though this task was not in the training data.  The bottom row shows a successful rollout in Meta-world where the robotic arm successfully interacts with objects and navigates around obstacles to accomplish a goal that was not in the training data.", "section": "Additional Results"}, {"figure_path": "tb1MlJCY5g/figures/figures_27_1.jpg", "caption": "Figure 12: Single step match rate of the generated rollouts, which is assessed by examining the alignment between the states and actions generated by the LLM and the labelled goals on unseen (easy) tasks. The horizontal axis represents the training epochs, while the vertical axis is the match rate of the generated states/actions.", "description": "This figure shows the accuracy of the LLM in generating states and actions for unseen (easy) tasks during the training process. The x-axis represents the training epochs, and the y-axis represents the match rate (percentage) of generated states and actions against the labeled goals.  The results indicate a relatively constant action generation accuracy, suggesting consistent ability to generate actions. However, the state generation accuracy improves over epochs, showing learning and better alignment with the goals.", "section": "C.4 Step Match Rate of the Generated Rollouts on CLEVR-Robot"}]