[{"figure_path": "q5CkneUn6K/tables/tables_5_1.jpg", "caption": "Table 1: Performance on LongBench datasets. The prefix Struct- indicates the data fed into LLMs is structurized by our StruXGPT-7B, while the evaluated LLMs themselves are unchanged. The results are acquired by LongBench's official protocol. Higher is better.", "description": "This table presents the performance of several LLMs on the LongBench dataset for question answering.  The performance is measured before and after applying the StruXGPT structurization technique.  Higher scores indicate better performance.  The table shows results across different LLMs and variations of the LongBench dataset, allowing for comparison of the method's impact across model architectures, sizes, and task types.", "section": "4 Experiments"}, {"figure_path": "q5CkneUn6K/tables/tables_6_1.jpg", "caption": "Table 2: Hallucination Evaluation on AttrScore.", "description": "This table presents the results of evaluating hallucination on the AttrScore dataset.  Several large language models (LLMs), both with and without the proposed structurization method, were used as evaluators. The table shows the performance of each model in terms of precision for three types of hallucinations: Attributable (Attr.), Contradictory (Contra.), and Extrapolatory (Extra.).  The average performance across all three hallucination types is also shown. The results demonstrate the effectiveness of the structurization technique in improving the accuracy of hallucination detection by LLMs.", "section": "4.2 Application on Exhaustive Hallucination Evaluation"}, {"figure_path": "q5CkneUn6K/tables/tables_7_1.jpg", "caption": "Table 3: Performance on BEIR subsets. Retrievers are trained with MS MARCO corpus and directly evaluated on BEIR without fine-tuning.", "description": "This table presents the performance of three different retrievers (BERT, SimLM, and coCondenser) on five subsets of the BEIR benchmark (NFCorpus, FiQA, ArguAna, SciDocs, and SciFact).  The retrievers were initially trained on the MS MARCO corpus and then directly evaluated on the BEIR benchmark without any fine-tuning.  The table shows the nDCG@10 scores for each retriever on each subset, with an average score across all subsets also provided.  The rows labeled '+StruXGPT (ours)' show the performance improvement obtained when the passages in the BEIR datasets were structurized using the proposed StruXGPT model before being input to the retrievers.", "section": "4.3 Application on Passage-level Dense Retrieval"}, {"figure_path": "q5CkneUn6K/tables/tables_7_2.jpg", "caption": "Table 4: Comprehensive comparison on structurization approaches.", "description": "This table compares several approaches to structurization, evaluating their performance across different metrics.  The approaches include using a few-shot commercial LLM (Qwen-max), few-shot smaller LLMs (Qwen-7B and LLaMA2-7B), and fine-tuned smaller LLMs (StruXGPT-7B-Q and StruXGPT-7B-L). The metrics used for evaluation include recall, precision, completeness, factuality, and anti-hallucination.  The table also shows the improvement in downstream applications (AppEval) and the semantic similarity (SemEval) score for each approach. The results show that the fine-tuned StruXGPT models perform better than the few-shot models.", "section": "4.4 Evaluation of the Structurization Approach Itself"}, {"figure_path": "q5CkneUn6K/tables/tables_9_1.jpg", "caption": "Table 5: Number of few-shot examples.", "description": "This table presents the results of ablation studies on the number of few-shot examples used for training the StruXGPT model.  It shows the impact of using 1, 2, or 3 examples on the model's performance, as measured by AppEval, BERTScore, and FormatError.  AppEval measures the improvement in question answering on the Qasper subset of the LongBench benchmark. BERTScore measures the semantic similarity between the original and the structurized texts. FormatError measures the percentage of incorrectly formatted structurization results.", "section": "4.5 Ablation Studies on StruXGPT's Establishment"}, {"figure_path": "q5CkneUn6K/tables/tables_9_2.jpg", "caption": "Table 6: Parameter capacity of StruXGPT.", "description": "This table presents the results of experiments conducted to investigate the relationship between model capacity and structurization quality using different sizes of Qwen models (1.8B, 7B, and 14B parameters).  The results are evaluated using AppEval, BERTScore, and FormatError metrics.  It shows the trade-off between model size (and thus training/inference efficiency) and structurization quality.", "section": "4.5 Ablation Studies on StruXGPT's Establishment"}, {"figure_path": "q5CkneUn6K/tables/tables_9_3.jpg", "caption": "Table 7: Training data filtering.", "description": "This table shows the ablation study on training data filtering. The first row shows the result of using vanilla training data, while the second row shows the result after filtering out 5% of low-quality training data.  The \"AppEval\" column shows the improvement in downstream question-answering performance on the Qasper subset of the LongBench benchmark. The \"BERTScore\" column indicates the semantic similarity between the original and structurized texts. The table demonstrates that filtering out low-quality data slightly reduces the performance but has little effect on the overall enhancement.", "section": "4.5 Ablation Studies on StruXGPT's Establishment"}, {"figure_path": "q5CkneUn6K/tables/tables_9_4.jpg", "caption": "Table 8: Inference results filtering.", "description": "This table presents the results of filtering inference results based on the quality of structurization.  The \"vanilla\" row shows the overall enhancement and declined ratio before filtering, while the \"filtered\" row shows the results after removing low-quality structurization results. Filtering improved the overall enhancement slightly while reducing the declined ratio.", "section": "4.6 Utilization of Structurization Quality Assessment"}, {"figure_path": "q5CkneUn6K/tables/tables_14_1.jpg", "caption": "Table A1: Ablation on training samples.", "description": "This table presents the results of ablation studies conducted to investigate the impact of training sample size on the performance of StruXGPT.  The experiment varied the number of training samples (2K, 5K, 10K, 15K, 22K) and measured the resulting enhancement in question-answering performance on the LongBench's Qasper subset using the LLaMA2-7B-Chat model.  The results show an initial decrease in performance with smaller sample sizes, followed by a steady increase as the sample size grows, indicating that an adequate amount of training data is crucial for effective structurization.", "section": "A.2 Additional Ablation Studies on Training Data"}, {"figure_path": "q5CkneUn6K/tables/tables_15_1.jpg", "caption": "Table 1: Performance on LongBench datasets. The prefix Struct- indicates the data fed into LLMs is structurized by our StruXGPT-7B, while the evaluated LLMs themselves are unchanged. The results are acquired by LongBench's official protocol. Higher is better.", "description": "This table presents the performance of various LLMs on the LongBench dataset.  It compares the performance of LLMs using vanilla contexts against LLMs using contexts that have been processed using the StruXGPT-7B model for structurization. The results are broken down by task type within the LongBench benchmark (SingleDoc QA, MultiDoc QA, and Synthetic Tasks), and an average performance across all tasks is also provided.  Higher scores indicate better performance.", "section": "4.1 Application on Context-based Question-Answering"}, {"figure_path": "q5CkneUn6K/tables/tables_17_1.jpg", "caption": "Table 1: Performance on LongBench datasets. The prefix Struct- indicates the data fed into LLMs is structurized by our StruXGPT-7B, while the evaluated LLMs themselves are unchanged. The results are acquired by LongBench's official protocol. Higher is better.", "description": "This table presents the performance of different LLMs on the LongBench dataset for various question answering tasks.  It compares the performance of LLMs when given the original text versus when given text that has been preprocessed using the StruXGPT-7B model.  Higher scores indicate better performance.  The table shows improvements across several LLMs and tasks after structurization.", "section": "4.1 Application on Context-based Question-Answering"}, {"figure_path": "q5CkneUn6K/tables/tables_18_1.jpg", "caption": "Table 2: Hallucination Evaluation on AttrScore.", "description": "This table presents the results of hallucination evaluation on the AttrScore dataset.  It compares the performance of several large language models (LLMs) in identifying whether a statement is Attributable (Attr.), Contradictory (Contra.), or Extrapolatory (Extra.) based on a reference passage.  The models evaluated include GPT-4, GPT-3.5-Turbo, Alpaca-13B, Alpaca-7B, and LLaMA2-7B. The \" +Ours\" row shows the results when the context structurization approach is used with each model to enhance performance.", "section": "4.2 Application on Exhaustive Hallucination Evaluation"}, {"figure_path": "q5CkneUn6K/tables/tables_18_2.jpg", "caption": "Table 2: Hallucination Evaluation on AttrScore.", "description": "This table presents the results of hallucination evaluation on the AttrScore dataset.  It compares the performance of several LLMs (GPT-4, GPT-3.5-Turbo, Alpaca-13B, Alpaca-7B, LLaMA2-7B, LLaMA2-70B, and GPT-3.5-1106) with and without the application of the proposed structurization method.  The metrics used are the percentage accuracy of correctly identifying attributable, contradictory, and extrapolatory statements. The table shows that structurization consistently improves the performance of the evaluated LLMs.", "section": "4.2 Application on Exhaustive Hallucination Evaluation"}, {"figure_path": "q5CkneUn6K/tables/tables_18_3.jpg", "caption": "Table 2: Hallucination Evaluation on AttrScore.", "description": "This table presents the results of hallucination evaluation on the AttrScore dataset.  It compares the performance of different language models (GPT-4, GPT-3.5-Turbo, Alpaca-13B, Alpaca-7B, LLaMA2-7B, LLaMA2-70B, GPT-3.5-1106, and GPT-3.5-1106 + CoT) in identifying attributable, contradictory, and extrapolatory claims. The \"+StruXGPT (ours)\" rows show the performance improvement achieved by incorporating the authors' proposed structurization technique.  Higher scores indicate better performance.", "section": "4.2 Application on Exhaustive Hallucination Evaluation"}, {"figure_path": "q5CkneUn6K/tables/tables_18_4.jpg", "caption": "Table 1: Performance on LongBench datasets. The prefix Struct- indicates the data fed into LLMs is structurized by our StruXGPT-7B, while the evaluated LLMs themselves are unchanged. The results are acquired by LongBench's official protocol. Higher is better.", "description": "This table presents the performance of various LLMs on the LongBench dataset for three question-answering tasks.  The results show the performance improvements achieved by using the StruXGPT-7B model to structurize the input context before feeding it to the LLMs.  The 'Struct-' prefix indicates that the input data has been processed by the Structurization model.  The table demonstrates that consistent improvements were observed across different LLMs and task types after structurization.  Higher scores indicate better performance.", "section": "4.1 Application on Context-based Question-Answering"}, {"figure_path": "q5CkneUn6K/tables/tables_18_5.jpg", "caption": "Table 3: Performance on BEIR subsets. Retrievers are trained with MS MARCO corpus and directly evaluated on BEIR without fine-tuning.", "description": "This table presents the performance of different retrieval models on the BEIR benchmark.  The models were initially trained on the MS MARCO corpus and then directly evaluated on BEIR without any further fine-tuning. The table shows the nDCG@10 scores for each retriever on five different subsets of the BEIR benchmark (NFCorpus, FiQA, ArguAna, SciDocs, and SciFact), as well as the average score across all five subsets. The \" +StruXGPT (ours)\" rows show the performance of the same retrievers when using the text that is structurized by StruXGPT.", "section": "4.3 Application on Passage-level Dense Retrieval"}, {"figure_path": "q5CkneUn6K/tables/tables_19_1.jpg", "caption": "Table 1: Performance on LongBench datasets. The prefix Struct- indicates the data fed into LLMs is structurized by our StruXGPT-7B, while the evaluated LLMs themselves are unchanged. The results are acquired by LongBench's official protocol. Higher is better.", "description": "This table presents the performance of several LLMs on the LongBench dataset, both with and without the application of structurization using the StruXGPT-7B model.  It shows the average scores across various subtasks within LongBench (SingleDoc QA, MultiDoc QA, and Synthetic Tasks) for different LLMs, allowing for a comparison of performance gains achieved through structurization.", "section": "4.1 Application on Context-based Question-Answering"}, {"figure_path": "q5CkneUn6K/tables/tables_19_2.jpg", "caption": "Table 1: Performance on LongBench datasets. The prefix Struct- indicates the data fed into LLMs is structurized by our StruXGPT-7B, while the evaluated LLMs themselves are unchanged. The results are acquired by LongBench's official protocol. Higher is better.", "description": "This table presents the performance of different LLMs on the LongBench dataset.  It compares the performance of several LLMs on various question answering tasks, both with and without the context structurization technique. The 'Struct-' prefix indicates that the input data was processed by the StruXGPT-7B model before being fed to the LLMs.  The table shows the average performance across multiple subsets of the LongBench dataset, providing a comprehensive view of the structurization method's effectiveness across different model architectures and scales.", "section": "4.1 Application on Context-based Question-Answering"}, {"figure_path": "q5CkneUn6K/tables/tables_20_1.jpg", "caption": "Table A10: Evaluation on general benchmarks.", "description": "This table presents the performance of LLaMA2-7B-Chat with and without structurization on two general benchmarks: MMLU and BBH.  It shows that structurization slightly improves performance on BBH but has a negligible impact on MMLU, highlighting that structurization primarily benefits tasks with long-form or logically complex contexts, while MMLU relies more on the model's inherent knowledge without context.", "section": "4.4 Evaluation of the Structurization Approach Itself"}]