[{"figure_path": "8IysmgZte4/tables/tables_7_1.jpg", "caption": "Table 1: Offline multitask RL on AntMaze and Kitchen. DiSPOs show superior transfer performance (in average episodic return) than successor features, model-based RL, and misspecified goal-conditioned baselines.", "description": "This table presents the average episodic return achieved by DiSPOs and several baseline methods across various AntMaze and Kitchen environments.  DiSPOs consistently outperforms the baselines (USFA, FB, RaMP, MOPO, COMBO, GC-IQL) demonstrating its superior transfer learning capabilities in multitask reinforcement learning settings. The results highlight DiSPOs' ability to generalize to unseen tasks without requiring further training.", "section": "6.2 Baseline Comparisons"}, {"figure_path": "8IysmgZte4/tables/tables_8_1.jpg", "caption": "Table 2: Evaluation on non-goal-conditioned tasks. DiSPOs are able to solve non-goal-conditioned tasks, taking different paths in preference antmaze (Fig 4), while goal-conditioned RL cannot optimize for arbitrary rewards.", "description": "This table presents the results of evaluating DiSPOs and baseline methods on non-goal-conditioned tasks.  The results show that DiSPOs can solve these tasks, demonstrating an ability to optimize for arbitrary reward functions, unlike goal-conditioned reinforcement learning (RL) methods which are limited to optimizing for specific goals. The preference Antmaze environment, shown in Figure 4, is used to highlight DiSPOs' ability to take different paths to achieve a goal. This contrasts with goal-conditioned RL which may only take a single optimal path.", "section": "6.3 Do DiSPOs enable zero-shot policy optimization across tasks?"}, {"figure_path": "8IysmgZte4/tables/tables_8_2.jpg", "caption": "Table 3: Evaluation of trajectory stitching ability of DiSPOs. DiSPOs outperform non-stitching baselines, demonstrating their abilities to recombine outcomes across trajectory segments", "description": "This table presents the results of evaluating the trajectory stitching capabilities of DiSPOs, a novel method for zero-shot policy optimization.  The results demonstrate that DiSPOs significantly outperforms other baseline methods (RaMP and Decision Transformer) in three complex robotic manipulation tasks (PickPlace, ClosedDrawer, and BlockedDrawer). This highlights DiSPOs' ability to effectively combine suboptimal trajectories to achieve optimal behavior.", "section": "6.3 Do DiSPOs enable zero-shot policy optimization across tasks?"}, {"figure_path": "8IysmgZte4/tables/tables_18_1.jpg", "caption": "Table 1: Offline multitask RL on AntMaze and Kitchen. DiSPOs show superior transfer performance (in average episodic return) than successor features, model-based RL, and misspecified goal-conditioned baselines.", "description": "This table presents a comparison of DiSPOs against several baseline methods on two multi-task reinforcement learning problems, AntMaze and Kitchen.  The results demonstrate DiSPOs' superior performance in terms of average episodic return across various task settings compared to approaches based on successor features, model-based reinforcement learning, and goal-conditioned baselines. The superior performance highlights DiSPOs' ability to transfer effectively to new tasks without requiring extensive retraining.", "section": "6.2 Baseline Comparisons"}, {"figure_path": "8IysmgZte4/tables/tables_18_2.jpg", "caption": "Table 1: Offline multitask RL on AntMaze and Kitchen. DiSPOs show superior transfer performance (in average episodic return) than successor features, model-based RL, and misspecified goal-conditioned baselines.", "description": "This table presents the results of offline multitask reinforcement learning experiments on AntMaze and Kitchen environments.  It compares the performance of DiSPOs (Distributional Successor Features for Zero-Shot Policy Optimization) against several baselines, including other successor feature methods (USFA, FB, RaMP), model-based RL methods (MOPO, COMBO), and a goal-conditioned method (GC-IQL). The key metric is the average episodic return, which demonstrates DiSPOs' superior transfer learning capabilities across different tasks within each environment.", "section": "6.2 Baseline Comparisons"}, {"figure_path": "8IysmgZte4/tables/tables_19_1.jpg", "caption": "Table 6: Ablation of feature dimension and type.", "description": "This table shows the ablation study on feature dimension and type.  It compares various versions of the DiSPOs method that use different numbers of dimensions (64, 32, and 16) for random Fourier features, as well as using simpler random features and two top-performing pretrained features from another paper. The results demonstrate the impact of the feature representation on the overall performance of the method. Lower dimensional features have less expressivity resulting in poorer performance.  Pretrained features show lower performance compared to randomly initialized Fourier features. This suggests that the random Fourier features strike a better balance between expressivity and the avoidance of overfitting to a specific objective.", "section": "E.3 Ablation of Feature Dimension and Type"}, {"figure_path": "8IysmgZte4/tables/tables_19_2.jpg", "caption": "Table 1: Offline multitask RL on AntMaze and Kitchen. DiSPOs show superior transfer performance (in average episodic return) than successor features, model-based RL, and misspecified goal-conditioned baselines.", "description": "This table presents the results of offline multitask reinforcement learning experiments on the AntMaze and Kitchen environments.  The table compares the performance of DiSPOs (Distributional Successor Features for Zero-Shot Policy Optimization) against several baseline methods, including various successor feature approaches (USFA, FB, RaMP), model-based RL methods (MOPO, COMBO), and a goal-conditioned method (GC-IQL). The performance metric is the average episodic return, indicating the cumulative reward obtained per episode.  The results demonstrate that DiSPOs achieves significantly higher average episodic returns compared to the baseline methods across different variations of the AntMaze and Kitchen tasks, showcasing its superior transfer learning capabilities.", "section": "6.2 Baseline Comparisons"}, {"figure_path": "8IysmgZte4/tables/tables_19_3.jpg", "caption": "Table 1: Offline multitask RL on AntMaze and Kitchen. DiSPOs show superior transfer performance (in average episodic return) than successor features, model-based RL, and misspecified goal-conditioned baselines.", "description": "This table presents the results of offline multitask reinforcement learning experiments on AntMaze and Kitchen environments.  It compares the performance of DiSPOs (the proposed method) against several baselines, including successor features (USFA, FB, RaMP), model-based RL (MOPO, COMBO), and goal-conditioned RL (GC-IQL). The metric used is the average episodic return, showing DiSPOs' superior transfer learning capability across different tasks.", "section": "6.2 Baseline Comparisons"}]