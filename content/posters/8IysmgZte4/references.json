{"references": [{"fullname_first_author": "A. Barreto", "paper_title": "Successor features for transfer in reinforcement learning", "publication_date": "2017-12-01", "reason": "This paper introduces the concept of successor features, a core concept that the current paper builds upon to create its novel DiSPOs model."}, {"fullname_first_author": "A. Barreto", "paper_title": "Transfer in deep reinforcement learning using successor features and generalised policy improvement", "publication_date": "2018-01-01", "reason": "This paper expands upon the original successor feature concept, improving its capabilities and directly informing the methodology used in the current work."}, {"fullname_first_author": "M. G. Bellemare", "paper_title": "A distributional perspective on reinforcement learning", "publication_date": "2017-08-06", "reason": "This paper provides the theoretical foundation for distributional reinforcement learning, which is leveraged in the current paper's DiSPOs model for improved performance."}, {"fullname_first_author": "J. Ho", "paper_title": "Denoising diffusion probabilistic models", "publication_date": "2020-01-01", "reason": "This paper introduces diffusion models, a crucial component of the DiSPOs model's practical implementation, enabling efficient and effective training."}, {"fullname_first_author": "J. Fu", "paper_title": "D4RL: Datasets for deep data-driven reinforcement learning", "publication_date": "2020-04-17", "reason": "This paper provides the D4RL dataset, which is essential to the empirical evaluation of the DiSPOs model, providing a standardized benchmark for performance assessment."}]}