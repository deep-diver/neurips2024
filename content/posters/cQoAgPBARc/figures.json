[{"figure_path": "cQoAgPBARc/figures/figures_3_1.jpg", "caption": "Figure 1: Generalized Policy Iteration (GPI) under the value and policy churn.", "description": "This figure illustrates the Generalized Policy Iteration (GPI) process with the inclusion of value and policy churn.  The standard GPI cycle of evaluation and improvement is shown, but with the addition of red curly arrows representing the churn phenomena.  The evaluation phase involves approximating the value function Q using the current policy \u03c0\u03c6, but this approximation is not perfect due to value churn (Qo \u2192 Qe). The improvement phase involves updating the policy based on the approximated value function Qe, which leads to a new policy \u03c0\u03c6. However, this new policy is not precisely the expected greedy policy due to policy churn (\u03c0\u03c6 \u2192 \u03c0\u03c6). The loop continues iteratively, but the churns introduce imprecision and bias in the process.", "section": "4 A Chain Effect of Value and Policy Churn"}, {"figure_path": "cQoAgPBARc/figures/figures_4_1.jpg", "caption": "Figure 1: Generalized Policy Iteration (GPI) under the value and policy churn.", "description": "This figure illustrates the Generalized Policy Iteration (GPI) process in reinforcement learning, highlighting the impact of value and policy churn.  The standard GPI process involves alternating between policy evaluation and policy improvement steps.  This figure extends this model to incorporate the concept of \"churn\", which refers to the unexpected changes in the network outputs on states not directly updated in a mini-batch. The figure shows that both value churn (changes in the value function) and policy churn (changes in the policy) occur and influence each other throughout the iterations.  It emphasizes the intertwined nature of these churns and how they affect each step of the GPI process.", "section": "4 A Chain Effect of Value and Policy Churn"}, {"figure_path": "cQoAgPBARc/figures/figures_6_1.jpg", "caption": "Figure 3: The value churn (left) and the greedy action deviation percentage (right) in Breakout w/ and w/o CHAIN.", "description": "This figure shows the impact of CHAIN on reducing value churn and greedy action deviation in the Breakout game. The left panel shows the value churn, which measures the change in Q-values for states not directly updated by mini-batch training. The right panel shows the percentage of greedy actions that changed after each update. The results indicate that CHAIN effectively reduces both value churn and greedy action deviation.", "section": "5.1 Consequences of the Chain Effect of Churn in Different DRL Scenarios"}, {"figure_path": "cQoAgPBARc/figures/figures_7_1.jpg", "caption": "Figure 4: The evaluation of CHAIN DoubleDQN in MinAtar regarding episode return. Curves and shades denote means and standard errors over six random seeds.", "description": "This figure shows the learning curves for the CHAIN DoubleDQN algorithm and the standard DoubleDQN algorithm across five different MinAtar environments.  The x-axis represents training timesteps, and the y-axis represents the average episode return.  Each line represents the average performance across six different random seeds, with shaded regions indicating the standard error. The figure demonstrates that CHAIN DoubleDQN consistently outperforms the standard DoubleDQN algorithm in terms of both sample efficiency and final performance across all environments.", "section": "6.1 Results for CHAIN DoubleDQN in MinAtar"}, {"figure_path": "cQoAgPBARc/figures/figures_7_2.jpg", "caption": "Figure 5: The evaluation of CHAIN PPO in MuJoCo and DeepMind Control (DMC) tasks regarding episode return. Curves and shades denote means and standard errors over twelve random seeds.", "description": "This figure presents the results of applying the CHAIN method to Proximal Policy Optimization (PPO) on various continuous control tasks from MuJoCo and DeepMind Control Suite (DMC).  It shows the learning curves (episode return) for both standard PPO and CHAIN PPO, with error bars indicating standard errors across multiple random seeds.  The results demonstrate that CHAIN PPO generally outperforms standard PPO, highlighting the effectiveness of CHAIN in improving learning performance across these environments.", "section": "Results for CHAIN PPO in MuJoCo and DMC"}, {"figure_path": "cQoAgPBARc/figures/figures_8_1.jpg", "caption": "Figure 11: Churn reduction of TD3 in MuJoCo. The 2nd-4th columns report the value of the four metrics defined in Table 4. Horizontal axes are numbers of parameter updates after which the statistics are computed. Each point is obtained by averaging all the quantities throughout learning.", "description": "This figure shows the results of applying CHAIN (VCR and PCR) to TD3 on four MuJoCo tasks.  The plots show the episode return over time, along with the value churn, policy churn, and policy value deviation. The results demonstrate that CHAIN effectively reduces the value and policy churn in all four tasks, leading to improved learning performance.  The figure also includes error bars to show statistical significance.", "section": "Results for Deep Actor-Critic Methods with CHAIN in MuJoCo and D4RL"}, {"figure_path": "cQoAgPBARc/figures/figures_9_1.jpg", "caption": "Figure 7: The results regarding episode return for scaling PPO via widening. CHAIN helps to scale almost across all the configurations. Similar results can be found for widening scaling in Figure 20.", "description": "This figure shows the results of scaling PPO by widening the network. The x-axis represents the scale-up ratio, and the y-axis represents the episode return.  Four different scenarios are plotted: standard PPO, PPO with a reduced learning rate (sqrt_lr), CHAIN PPO, and CHAIN PPO with a reduced learning rate (sqrt_lr). The results demonstrate that CHAIN improves the scaling performance of PPO across different scale-up ratios and learning rates, mitigating the performance degradation typically observed when scaling up DRL agents.", "section": "6.4 Results for Scaling DRL Agents with CHAIN"}, {"figure_path": "cQoAgPBARc/figures/figures_15_1.jpg", "caption": "Figure 1: Generalized Policy Iteration (GPI) under the value and policy churn.", "description": "This figure illustrates the Generalized Policy Iteration (GPI) process, a common framework in reinforcement learning, but with the added consideration of value and policy churn.  The standard GPI alternates between policy evaluation (estimating the value function) and policy improvement (updating the policy based on the estimated value). This figure highlights that churn in both the policy evaluation and improvement steps creates a feedback loop where changes in one step influence the next. The value churn (Qo \u2192 Qe) alters the value function, impacting the policy improvement.  The policy churn (\u03c0\u03c6 \u2194 \u03c0\u03c6) changes the policy, leading to a new value function estimation. This interplay of value and policy churn creates a chain reaction that biases the learning process.", "section": "4 A Chain Effect of Value and Policy Churn"}, {"figure_path": "cQoAgPBARc/figures/figures_20_1.jpg", "caption": "Figure 9: Different statistics on value churn of DoubleDQN in MinAtar. Horizontal axes are numbers of parameter updates after which the statistics are computed. Each point is obtained by averaging all the quantities throughout learning. Curves and shades denote means and standard errors across six random seeds.", "description": "This figure presents an empirical analysis of the value churn in the DoubleDQN algorithm applied to the MinAtar environment.  It displays three key metrics over the course of training: the percentage of greedy actions that change after each update, the value change of the greedy action, and the overall value change of all actions.  The shaded areas represent standard error across six different training runs. The figure helps to visualize how the value churn accumulates during training and how it is affected by the proposed Churn Approximated Reduction (CHAIN) method.", "section": "D.1 More Empirical Analysis on the Value Churn in DoubleDQN"}, {"figure_path": "cQoAgPBARc/figures/figures_21_1.jpg", "caption": "Figure 10: Results for CHAIN PPO, including the learning performance, the amount of the policy churn, the loss of conventional policy training, and the loss of regularization.", "description": "This figure presents the results of CHAIN PPO applied to four MuJoCo tasks. For each task, four sub-figures are shown: episode return, policy churn amount, policy loss, and regularization loss.  Each sub-figure displays the performance of standard PPO and CHAIN PPO using different hyperparameters (\u03bb\u03c0 = 5, 50, 2000).  The results show that CHAIN PPO generally reduces policy churn and improves learning performance compared to standard PPO, demonstrating the effectiveness of the CHAIN method in controlling policy churn and improving learning outcomes. ", "section": "More Results for CHAIN PPO"}, {"figure_path": "cQoAgPBARc/figures/figures_22_1.jpg", "caption": "Figure 11: Churn reduction of TD3 in MuJoCo. The 2nd-4th columns report the value of the four metrics defined in Table 4. Horizontal axes are numbers of parameter updates after which the statistics are computed. Each point is obtained by averaging all the quantities throughout learning.", "description": "This figure shows the results of applying the CHAIN method to TD3 on four MuJoCo locomotion tasks.  It presents four plots for each task, visualizing the episode return, absolute value churn, value churn of greedy action, policy churn and policy value deviation over training iterations. The plots illustrate how CHAIN reduces the various types of churn and improves the learning performance.", "section": "Results for Deep Actor-Critic Methods with CHAIN in MuJoCo and D4RL"}, {"figure_path": "cQoAgPBARc/figures/figures_23_1.jpg", "caption": "Figure 11: Churn reduction of TD3 in MuJoCo. The 2nd-4th columns report the value of the four metrics defined in Table 4. Horizontal axes are numbers of parameter updates after which the statistics are computed. Each point is obtained by averaging all the quantities throughout learning.", "description": "This figure shows the results of applying the CHAIN method to TD3 in four MuJoCo environments: HalfCheetah-v4, Hopper-v4, Walker2d-v4, and Ant-v4.  For each environment, the figure displays the episode return, absolute value churn, policy churn (measured using KL divergence), and policy value deviation over the course of training.  Different lines represent different variations of the CHAIN method (VCR, PCR, or both) with differing hyperparameters. The results illustrate the effectiveness of CHAIN in reducing churn and improving performance in these tasks.", "section": "Results for Deep Actor-Critic Methods with CHAIN in MuJoCo and D4RL"}, {"figure_path": "cQoAgPBARc/figures/figures_23_2.jpg", "caption": "Figure 11: Churn reduction of TD3 in MuJoCo. The 2nd-4th columns report the value of the four metrics defined in Table 4. Horizontal axes are numbers of parameter updates after which the statistics are computed. Each point is obtained by averaging all the quantities throughout learning.", "description": "This figure shows the effects of applying different churn reduction methods (VCR, PCR, and DCR) on TD3 in MuJoCo environments. It displays the changes in various metrics related to value and policy churn, including the value churn, policy churn, and value deviation of policy churn, across multiple time steps.  The results illustrate how the proposed methods reduce the level of churn, supporting the paper's claim.", "section": "More Results for CHAIN for Deep AC Methods"}, {"figure_path": "cQoAgPBARc/figures/figures_24_1.jpg", "caption": "Figure 10: Results for CHAIN PPO, including the learning performance, the amount of the policy churn, the loss of conventional policy training, and the loss of regularization.", "description": "This figure displays the learning curves of PPO and CHAIN PPO on four MuJoCo tasks, along with the policy churn, policy loss, and regularization loss.  It demonstrates CHAIN PPO's ability to reduce policy churn and improve learning performance in Ant-v4 and HalfCheetah-v4, while maintaining comparable performance in Hopper-v4 and Walker2d-v4.  The results are shown for different choices of the hyperparameter \u03bb\u03c0 (lambda-pi), which controls the strength of the policy churn regularization.", "section": "5.1 Consequences of the Chain Effect of Churn in Different DRL Scenarios"}, {"figure_path": "cQoAgPBARc/figures/figures_24_2.jpg", "caption": "Figure 13: Hyperparameter choices for the value and policy churn reduction regularization for TD3 in MuJoCo. Curves and shades denote means and standard errors across six random seeds.", "description": "This figure shows the effect of different hyperparameter choices for value and policy churn reduction regularization on TD3's performance across four MuJoCo locomotion tasks.  The hyperparameters \u03bb\u03b8 and \u03bb\u03c0 control the strength of the value and policy churn reduction terms, respectively. The figure visualizes the learning curves (episode return) for various combinations of these hyperparameters, showcasing how different settings impact the learning process.  Error bars indicate the standard deviation across multiple runs for each setting, offering insight into the robustness and stability of different parameter choices.", "section": "5.1 Consequences of the Chain Effect of Churn in Different DRL Scenarios"}, {"figure_path": "cQoAgPBARc/figures/figures_24_3.jpg", "caption": "Figure 4: The evaluation of CHAIN DoubleDQN in MinAtar regarding episode return. Curves and shades denote means and standard errors over six random seeds.", "description": "This figure shows the learning curves for CHAIN DoubleDQN and the standard DoubleDQN on six different MinAtar games.  The x-axis represents training timesteps, and the y-axis represents the average episode return.  Each line represents the average performance across six different random seeds, with shaded regions indicating the standard error. The results show that CHAIN DoubleDQN consistently outperforms the standard DoubleDQN, demonstrating improved sample efficiency and ultimately higher scores.", "section": "6.1 Results for CHAIN DoubleDQN in MinAtar"}, {"figure_path": "cQoAgPBARc/figures/figures_25_1.jpg", "caption": "Figure 10: Results for CHAIN PPO, including the learning performance, the amount of the policy churn, the loss of conventional policy training, and the loss of regularization.", "description": "This figure shows the learning performance of CHAIN PPO on four MuJoCo tasks.  It also displays the conventional PPO policy loss and the regularization loss during learning, in addition to showing how CHAIN reduces policy churn. The results indicate improved performance from CHAIN, particularly in Ant-v4 and HalfCheetah-v4.", "section": "5.1 Consequences of the Chain Effect of Churn in Different DRL Scenarios"}, {"figure_path": "cQoAgPBARc/figures/figures_25_2.jpg", "caption": "Figure 17: Reliable [Agarwal et al., 2021] metrics for TD3 with different churn reduction options.", "description": "This figure compares the performance of TD3 with different churn reduction methods (VCR, PCR, DCR) using Reliable metrics proposed by Agarwal et al. (2021).  The metrics shown (Median, IQR, Mean, Optimality Gap) are used to assess the quality of the policy learned by each method.  The comparison is done for the Ant environment and across all MuJoCo environments, providing a comprehensive evaluation of the various churn reduction techniques on the TD3 algorithm.", "section": "D.3 More Results of CHAIN for Deep AC Methods"}, {"figure_path": "cQoAgPBARc/figures/figures_27_1.jpg", "caption": "Figure 4: The evaluation of CHAIN DoubleDQN in MinAtar regarding episode return. Curves and shades denote means and standard errors over six random seeds.", "description": "This figure shows the learning curves of CHAIN DoubleDQN and regular DoubleDQN on six different games from the MinAtar environment.  The y-axis represents the episode return (reward accumulated over an episode), while the x-axis shows the number of timesteps.  The lines represent the average performance across six independent runs, and the shaded areas represent the standard error of the mean. The results demonstrate that CHAIN DoubleDQN consistently outperforms regular DoubleDQN across various games, indicating improved sample efficiency and final performance.", "section": "6.1 Results for CHAIN DoubleDQN in MinAtar"}, {"figure_path": "cQoAgPBARc/figures/figures_27_2.jpg", "caption": "Figure 7: The results regarding episode return for scaling PPO via widening. CHAIN helps to scale up almost across all the configurations. Similar results can be found for widening scaling in Figure 20.", "description": "This figure shows the results of scaling up PPO by widening the network layers. The x-axis represents the scale-up ratio, while the y-axis represents the episode return.  The lines in gray represent the performance of standard PPO under different scaling ratios and learning rates.  The red lines show the improved performance of PPO when a reduced learning rate is used (sqrt of the scale-up ratio). The dashed lines show the results achieved using CHAIN PPO. CHAIN PPO demonstrates a significant improvement in performance across different scaling ratios and learning rates, highlighting its effectiveness in enhancing the scalability of PPO.", "section": "6.4 Results for Scaling DRL Agents with CHAIN"}]