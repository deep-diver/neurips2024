{"references": [{"fullname_first_author": "Volodymyr Mnih", "paper_title": "Human-level control through deep reinforcement learning", "publication_date": "2015-02-26", "reason": "This is a foundational paper in deep reinforcement learning, introducing the use of deep neural networks for game playing, which is highly relevant to the current work."}, {"fullname_first_author": "Marc G. Bellemare", "paper_title": "A distributional perspective on reinforcement learning", "publication_date": "2017-07-07", "reason": "This paper introduces the distributional approach to reinforcement learning, which addresses limitations of traditional methods and is foundational to several algorithms discussed in the current work."}, {"fullname_first_author": "Sergey Levine", "paper_title": "Offline reinforcement learning with implicit q-learning", "publication_date": "2022-01-01", "reason": "This paper provides a novel approach to offline reinforcement learning which is relevant to the scaling setting discussed in the current work."}, {"fullname_first_author": "John Schulman", "paper_title": "Trust region policy optimization", "publication_date": "2015-06-01", "reason": "This paper introduces the Trust Region Policy Optimization (TRPO) algorithm, which is highly relevant to the current work because it deals with non-stationary nature of reinforcement learning and uses trust-region methods."}, {"fullname_first_author": "Timothy P. Lillicrap", "paper_title": "Continuous control with deep reinforcement learning", "publication_date": "2016-01-01", "reason": "This paper is highly relevant because it proposes the Deep Deterministic Policy Gradient (DDPG) algorithm that is discussed in the current work."}]}