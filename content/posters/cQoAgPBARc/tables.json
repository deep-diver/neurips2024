[{"figure_path": "cQoAgPBARc/tables/tables_8_1.jpg", "caption": "Table 1: Results for CHAIN IQL in Antmaze, with means and standard errors over twelve seeds.", "description": "This table presents the results of the IQL algorithm and its variations with CHAIN in Antmaze environment.  Three variations of the IQL algorithm are compared: the original IQL, IQL with policy churn reduction (CHAIN IQL (PCR)), and IQL with value churn reduction (CHAIN IQL (VCR)).  The table shows the mean and standard error of the performance (presumably an average reward or score) across twelve different random seeds for each algorithm and across six different Antmaze tasks (AM-umaze-v2, AM-umaze-diverse-v2, AM-medium-play-v2, AM-medium-diverse-v2, AM-large-play-v2, and AM-large-diverse-v2).  This allows for a comparison of the performance improvement resulting from the application of the CHAIN method to reduce churn.", "section": "Results for CHAIN IQL in Antmaze"}, {"figure_path": "cQoAgPBARc/tables/tables_8_2.jpg", "caption": "Table 2: Results for CHAIN AWAC in Adroit, with means and standard errors over twelve seeds.", "description": "This table presents the results of the CHAIN AWAC algorithm on the Adroit benchmark.  It shows the mean and standard error of the performance (likely a success metric) for the AWAC algorithm, CHAIN AWAC using Policy Churn Reduction (PCR), and CHAIN AWAC using Value Churn Reduction (VCR), each evaluated over twelve random seeds.  The results demonstrate the improvement in performance achieved by using CHAIN, specifically the PCR variant.", "section": "Results for Deep Actor-Critic Methods with CHAIN in MuJoCo and D4RL"}, {"figure_path": "cQoAgPBARc/tables/tables_9_1.jpg", "caption": "Table 3: Scaling PPO with CHAIN. Means and standard errors of final episode return over six seeds.", "description": "This table presents the results of scaling experiments using Proximal Policy Optimization (PPO) with and without the CHAIN method.  It shows the mean and standard error of the final episode return across six random seeds for different network scaling configurations (wider and deeper networks) and for both standard PPO and PPO with CHAIN.  The results demonstrate the impact of CHAIN on the scaling performance of PPO in various scenarios, particularly for larger network sizes.", "section": "6.4 Results for Scaling DRL Agents with CHAIN"}, {"figure_path": "cQoAgPBARc/tables/tables_17_1.jpg", "caption": "Table 4: Churn and deviation metrics used in our experiments. \u03b8t, \u03c6t and \u03b8\u00af, \u03c6\u00af are current networks and previous networks. The metrics are averaged over s, \u0101 in a reference buffer.", "description": "This table presents the metrics used to quantify the value and policy churn in the experiments described in the paper.  It breaks down the metrics into two categories: policy-based methods (like TD3, SAC, and PPO) and value-based methods (like DQN). For policy-based methods, the metrics include the value churn at a specific state-action pair, the overall value churn, the policy churn (measured using either L1 norm or KL divergence), and the deviation in the Q-value due to the change in policy. For value-based methods, the metrics measure the average Q-value change, the change in the maximum Q-value, and the deviation in greedy action selection.", "section": "C.2 Empirical Metrics for the Investigation of the Chain Effect"}, {"figure_path": "cQoAgPBARc/tables/tables_18_1.jpg", "caption": "Table 5: Hyperparameters of DoubleDQN used in MinAtar environments. The values of conventional hyperparameters are taken from the recommended values in [Young and Tian, 2019].", "description": "This table lists the hyperparameters used for the DoubleDQN algorithm in the MinAtar environments.  It shows both the standard hyperparameters for DoubleDQN (learning rate, training interval, discount factor, etc.) and the hyperparameters specific to the CHAIN method for reducing value churn (value regularization coefficient and target relative loss for automatic lambda adjustment). The conventional hyperparameter values are based on the recommendations in the cited paper by Young and Tian (2019).", "section": "Experimental Details"}, {"figure_path": "cQoAgPBARc/tables/tables_18_2.jpg", "caption": "Table 7: Hyperparameters of TD3 and SAC used in MuJoCo environments. The values of conventional hyperparameters are taken from the recommended values in CleanRL. '-' means 'not applicable'.", "description": "This table lists the hyperparameters used for TD3 and SAC algorithms in the MuJoCo environments.  It shows both the standard hyperparameters for each algorithm (taken from CleanRL, a publicly available implementation) and those specific to the CHAIN method (Churn Approximated Reduction) for value and policy churn reduction.  The table is crucial for reproducing the experiments detailed in the paper and for understanding how the CHAIN method affects algorithm performance.", "section": "Experimental Details"}, {"figure_path": "cQoAgPBARc/tables/tables_19_1.jpg", "caption": "Table 7: Hyperparameters of TD3 and SAC used in MuJoCo environments. The values of conventional hyperparameters are taken from the recommended values in CleanRL. '-' means 'not applicable'.", "description": "This table lists the hyperparameters used for TD3 and SAC algorithms in the MuJoCo environments. It includes learning rates for actor and critic networks, training intervals, exploration and target action noise parameters, discount factor, soft replacement ratio, initial random steps, replay buffer size, batch size, optimizer, and churn reduction hyperparameters.  The table specifies the values used for both algorithms and highlights the differences where applicable. The churn reduction hyperparameters are further elaborated upon in the paper's figures 6, 11, and 12.", "section": "C Experimental Details"}, {"figure_path": "cQoAgPBARc/tables/tables_26_1.jpg", "caption": "Table 8: Results for IQL, IQL (sequential) and CHAIN IQL in Antmaze.", "description": "This table presents the results of three different methods on six Antmaze tasks from the D4RL benchmark.  The methods compared are IQL (the standard iterative training method), IQL (sequential) which trains the value network first, and then the policy network with the frozen value network, and CHAIN IQL (with both PCR and VCR variants) which incorporates the proposed churn reduction method.  The table shows the mean \u00b1 standard error of the final scores achieved by each method on each task over twelve random seeds.  The results are intended to show the effectiveness of CHAIN IQL in improving the final performance of IQL across the Antmaze tasks.", "section": "Results for Deep Actor-Critic Methods with CHAIN in MuJoCo and D4RL"}, {"figure_path": "cQoAgPBARc/tables/tables_28_1.jpg", "caption": "Table 9: Different learning rates (lr) and target network replacement rates (trr), e.g., \"/ 2\" means \"divided by 2\". Mean final episode returns over six seeds are reported.", "description": "This table presents the mean final episode returns of TD3 and DDQN agents across six random seeds in four different MinAtar environments (Walker2d, Ant, Asterix, Freeway).  It compares the performance under different learning rates (lr) and target network replacement rates (trr).  The results show how changing the learning rate and the target network update frequency affect the performance.  The lr and trr values are modified by dividing or multiplying by factors of 2, 5, or 10.", "section": "6.4 Results for Scaling DRL Agents with CHAIN"}]