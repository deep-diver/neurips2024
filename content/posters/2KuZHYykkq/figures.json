[{"figure_path": "2KuZHYykkq/figures/figures_1_1.jpg", "caption": "Figure 1: (a) Standard Transformer architecture. MLP's and LM-Head's activation sequence length is annotated with S. (b) MINI-SEQUENCE TRANSFORMER is used to replace MLP blocks and LM-Head block, which splits the input sequence S into M mini-sequences with sequence length S/M, where M = 2 on this figure. (c) Max sequence size for training Llama2/Llama3 on A100-80GB GPU, with no degradation of throughput or convergence using our approach.", "description": "This figure illustrates the MINI-SEQUENCE TRANSFORMER (MST) architecture and its impact on maximum sequence length during training.  (a) Shows a standard Transformer architecture highlighting the activation sequence length (S) in MLP and LM-Head blocks. (b) Shows the MST architecture, where the input sequence (S) is divided into smaller mini-sequences (S/M) processed iteratively, reducing memory usage. (c) Compares the maximum sequence length achievable for Llama2 and Llama3 models using standard, activation recomputation, and MST methods.  MST demonstrates significant improvement in maximum sequence length without affecting throughput or convergence.", "section": "1 Introduction"}, {"figure_path": "2KuZHYykkq/figures/figures_5_1.jpg", "caption": "Figure 2: Distributed MINI-SEQUENCE TRANSFORMER.", "description": "This figure shows the architecture of the distributed MINI-SEQUENCE TRANSFORMER (MST). It illustrates how MST integrates with DeepSpeed-Ulysses for sequence parallelism. The input sequence (S) is partitioned across multiple devices, and each device processes mini-sequences.  The attention block uses DeepSpeed-Ulysses for efficient communication, while the MLP and LM-Head blocks utilize the mini-sequence technique.  The figure highlights the parallel processing and distributed computation of the various components of the model.", "section": "3 MINI-SEQUENCE TRANSFORMER (MST): Algorithm, Analysis, and Distributed Extensions"}, {"figure_path": "2KuZHYykkq/figures/figures_8_1.jpg", "caption": "Figure 3: Memory consumption of pre-training Llama3-8B and Gemma2-9B models with a batch size of 1 on a single A100 device, with activation recomputation and MST. Note that long-sequence training gradients overlap with activation, so gradients are not shown in bars.", "description": "This figure compares the memory usage of training Llama3-8B and Gemma2-9B models with different optimization techniques.  It shows the memory breakdown for weights, optimizer, gradients, and activations for vanilla training (BF16), activation recomputation, and MST, demonstrating how MST reduces memory consumption significantly, enabling longer sequence training.", "section": "Ablation Study"}, {"figure_path": "2KuZHYykkq/figures/figures_14_1.jpg", "caption": "Figure 1: (a) Standard Transformer architecture. MLP's and LM-Head's activation sequence length is annotated with S. (b) MINI-SEQUENCE TRANSFORMER is used to replace MLP blocks and LM-Head block, which splits the input sequence S into M mini-sequences with sequence length S/M, where M = 2 on this figure. (c) Max sequence size for training Llama2/Llama3 on A100-80GB GPU, with no degradation of throughput or convergence using our approach.", "description": "This figure illustrates the standard transformer architecture and the proposed Mini-Sequence Transformer (MST).  Panel (a) shows a conventional transformer with its components: attention, normalization, MLP, and LM head.  Panel (b) displays the MST architecture, highlighting how it partitions the input sequence into smaller mini-sequences to reduce memory usage. Panel (c) presents a graph showing maximum sequence lengths achieved with different model configurations (Llama2, Llama3) on A100-80GB GPUs, emphasizing that MST allows for significantly longer sequences without sacrificing performance.", "section": "1 Introduction"}, {"figure_path": "2KuZHYykkq/figures/figures_15_1.jpg", "caption": "Figure 3: Memory consumption of pre-training Llama3-8B and Gemma2-9B models with a batch size of 1 on a single A100 device, with activation recomputation and MST. Note that long-sequence training gradients overlap with activation, so gradients are not shown in bars.", "description": "This figure compares the memory usage of training Llama3-8B and Gemma2-9B models with different optimization techniques.  It shows a significant reduction in memory usage when using MINI-SEQUENCE TRANSFORMER (MST), especially for Gemma2-9B, demonstrating the effectiveness of MST in enabling longer sequence training.", "section": "Ablation Study"}, {"figure_path": "2KuZHYykkq/figures/figures_16_1.jpg", "caption": "Figure 3: Memory consumption of pre-training Llama3-8B and Gemma2-9B models with a batch size of 1 on a single A100 device, with activation recomputation and MST. Note that long-sequence training gradients overlap with activation, so gradients are not shown in bars.", "description": "This figure visualizes the memory usage during the pre-training of Llama3-8B and Gemma2-9B language models.  It compares memory consumption across three scenarios:  a standard approach (vanilla), an approach using activation recomputation, and an approach using both activation recomputation and the proposed MINI-SEQUENCE TRANSFORMER (MST) method. The models were trained with a batch size of 1 on a single A100 GPU. The bars represent the peak memory used during the training process.  It highlights that MST significantly reduces memory usage, particularly when combined with activation recomputation, enabling the training of much longer sequences.", "section": "5 Ablation Study"}, {"figure_path": "2KuZHYykkq/figures/figures_17_1.jpg", "caption": "Figure 3: Memory consumption of pre-training Llama3-8B and Gemma2-9B models with a batch size of 1 on a single A100 device, with activation recomputation and MST. Note that long-sequence training gradients overlap with activation, so gradients are not shown in bars.", "description": "This figure compares the memory usage during the pre-training of Llama3-8B and Gemma2-9B models.  The models are trained with a batch size of 1 on a single A100 GPU.  Three different scenarios are shown: vanilla training (no optimizations), training with activation recomputation, and training with both activation recomputation and the proposed MINI-SEQUENCE TRANSFORMER (MST) method.  The bars illustrate the relative contribution of parameters, optimizer states, gradients, and activations to the total memory usage. Notably, gradients are not shown as they overlap with the activation memory in long-sequence training. The visualization highlights how MST significantly reduces memory consumption compared to vanilla and activation recomputation only methods.", "section": "Ablation Study"}, {"figure_path": "2KuZHYykkq/figures/figures_17_2.jpg", "caption": "Figure 3: Memory consumption of pre-training Llama3-8B and Gemma2-9B models with a batch size of 1 on a single A100 device, with activation recomputation and MST. Note that long-sequence training gradients overlap with activation, so gradients are not shown in bars.", "description": "This figure compares the memory usage of training Llama3-8B and Gemma2-9B models under different memory optimization strategies.  It shows that the MINI-SEQUENCE TRANSFORMER (MST) significantly reduces the memory consumption compared to using activation recomputation alone or standard training. The results highlight MST's effectiveness in enabling the training of very long sequences on a single GPU.", "section": "Ablation Study"}, {"figure_path": "2KuZHYykkq/figures/figures_18_1.jpg", "caption": "Figure 3: Memory consumption of pre-training Llama3-8B and Gemma2-9B models with a batch size of 1 on a single A100 device, with activation recomputation and MST. Note that long-sequence training gradients overlap with activation, so gradients are not shown in bars.", "description": "This figure compares the memory consumption during the pre-training of Llama3-8B and Gemma2-9B language models.  It illustrates the memory usage breakdown for different components: weights, optimizer, gradients and activations. The comparison is shown for three scenarios: vanilla (standard) training, training with activation recomputation, and training with the proposed Mini-Sequence Transformer (MST) along with activation recomputation.  The figure highlights how MST significantly reduces memory usage, especially for Gemma2-9B, enabling the training of much longer sequences.", "section": "Ablation Study"}, {"figure_path": "2KuZHYykkq/figures/figures_19_1.jpg", "caption": "Figure 11: Memory Visualization. (a) The memory timeline of training Llama3-8B using standard transformer architecture, Red cycle highlights the intermediate memory (b) The memory timeline of training Llama3-8B using MsT, Red cycle highlights the intermediate memory has been narrowed.", "description": "This figure compares memory usage over time for training Llama3-8B using the standard transformer versus using MST.  The standard transformer shows a peak memory usage of 67GB, dominated by intermediate memory, optimizer state, and weights.  In contrast, MST significantly reduces the intermediate memory, resulting in a peak memory usage of only 48GB (a 30% reduction).", "section": "5 Ablation Study"}]