{"references": [{"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-12-01", "reason": "This paper introduced the Transformer architecture, which is the foundation of many large language models and the focus of this paper's optimizations."}, {"fullname_first_author": "Tri Dao", "paper_title": "FlashAttention: Fast and memory-efficient exact attention with IO-awareness", "publication_date": "2022-12-01", "reason": "This paper presents FlashAttention, a highly efficient attention mechanism used to improve the performance of LLMs, and directly compared in this paper."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Llama 2: Open foundation and fine-tuned chat models", "publication_date": "2023-07-01", "reason": "This paper introduced Llama 2, a large language model that this paper uses as a baseline for experiments and comparison."}, {"fullname_first_author": "Sam Ade Jacobs", "paper_title": "Deepspeed ulysses: System optimizations for enabling training of extreme long sequence transformer models", "publication_date": "2023-09-01", "reason": "This paper introduced DeepSpeed-Ulysses, a system for distributed training of long sequences, which is integrated with the proposed MST method in this paper."}, {"fullname_first_author": "Meta", "paper_title": "Introducing meta llama 3: The most capable openly available llm to date", "publication_date": "2024-04-01", "reason": "This paper introduced Llama 3, a large language model used as a primary experimental subject in this paper to demonstrate the effectiveness of the proposed method."}]}