[{"figure_path": "2KuZHYykkq/tables/tables_3_1.jpg", "caption": "Table 1: Intermediate value size analysis for transformer blocks", "description": "This table analyzes the peak intermediate value sizes for different transformer blocks (Attention, MLP, LM-Head) relative to their input/output sizes.  It shows how much larger the intermediate values are compared to the inputs and outputs, highlighting the memory challenges associated with these blocks. The ratios are calculated specifically for the Llama3 model settings, which helps to put the memory usage in context for training large language models.", "section": "Algorithms: Optimizing Intermediate Memory With Mini-Sequence Processing"}, {"figure_path": "2KuZHYykkq/tables/tables_6_1.jpg", "caption": "Table 2: Maximum sequence length of Llama3-8B and Llama2-7B.", "description": "This table compares the maximum sequence lengths achieved for Llama3-8B and Llama2-7B models using three different training methods: vanilla PyTorch, activation recomputation, and the proposed MINI-SEQUENCE TRANSFORMER (MST).  It demonstrates the significant increase in maximum sequence length enabled by MST compared to the standard implementation and activation recomputation alone, highlighting the effectiveness of MST in handling extremely long sequences during training. ", "section": "4.1 Longer Sequence Length with MINI-SEQUENCE TRANSFORMER (MST)"}, {"figure_path": "2KuZHYykkq/tables/tables_6_2.jpg", "caption": "Table 3: Maximum sequence length of various models.", "description": "This table presents the maximum sequence length achieved by three different models (Mistral-7B, Qwen2-7B, and gemma-2-9b) using three different training methods: vanilla, activation recomputation, and MST (MINI-SEQUENCE TRANSFORMER).  The results show a significant increase in maximum sequence length when using MST compared to the other two methods.", "section": "4.1 Longer Sequence Length with MINI-SEQUENCE TRANSFORMER (MST)"}, {"figure_path": "2KuZHYykkq/tables/tables_6_3.jpg", "caption": "Table 2: Maximum sequence length of Llama3-8B and Llama2-7B.", "description": "This table compares the maximum sequence length achieved by different training methods (vanilla, activation recomputation, and MST) for Llama3-8B and Llama2-7B models.  It shows a significant increase in maximum sequence length when using MST, demonstrating its effectiveness in handling longer sequences.", "section": "4.1 Longer Sequence Length with MINI-SEQUENCE TRANSFORMER (MST)"}, {"figure_path": "2KuZHYykkq/tables/tables_7_1.jpg", "caption": "Table 5: Maximum sequence length training with lossy method", "description": "This table compares the maximum sequence length achieved by different Llama3 implementations on a single A100 GPU.  The implementations include standard 8-bit and 4-bit quantization, as well as the proposed MINI-SEQUENCE TRANSFORMER (MST) alone and in combination with 8-bit and 4-bit quantization. The results demonstrate that MST significantly improves the maximum sequence length achievable, particularly when combined with quantization techniques.", "section": "4.1 Longer Sequence Length with MINI-SEQUENCE TRANSFORMER (MST)"}, {"figure_path": "2KuZHYykkq/tables/tables_7_2.jpg", "caption": "Table 6: Training performance using MST on single A100 80G GPU.", "description": "This table compares the training performance of MST against the vanilla PyTorch implementation and activation recomputation for Llama3-8B and Llama2-7B models. The metrics compared are training time per step and TFLOPS achieved using different batch sizes.  It highlights MST's ability to maintain comparable performance while enabling the use of larger batch sizes due to its memory efficiency.", "section": "4.2 Faster Long Sequence Training with MINI-SEQUENCE TRANSFORMER (MST)"}, {"figure_path": "2KuZHYykkq/tables/tables_7_3.jpg", "caption": "Table 7: LLAMA3-8b with MST, with 4times larger context length compared to activation recomputation.", "description": "This table presents the results of training Llama3-8B with different context lengths and methods. It compares the performance of training with activation recomputation and MST (Mini-Sequence Transformer) on the LongAlpaca dataset. The metrics evaluated are perplexity (ppl) and loss, along with the training time required for each configuration.  The table demonstrates MST's capability of handling much longer context lengths (up to 30k) while maintaining or improving performance compared to activation recomputation (8k).", "section": "4.3 Better Models with Longer Sequences"}, {"figure_path": "2KuZHYykkq/tables/tables_13_1.jpg", "caption": "Table 2: Maximum sequence length of Llama3-8B and Llama2-7B.", "description": "This table compares the maximum sequence length achieved by different implementations of Llama3-8B and Llama2-7B models using Hugging Face.  The implementations include the vanilla PyTorch implementation, the implementation with activation recomputation, and the implementation with MINI-SEQUENCE TRANSFORMER (MST). The results show that MST significantly increases the maximum sequence length compared to the other implementations, indicating its effectiveness in handling long sequences.", "section": "4.1 Longer Sequence Length with MINI-SEQUENCE TRANSFORMER (MST)"}, {"figure_path": "2KuZHYykkq/tables/tables_16_1.jpg", "caption": "Table 8: Memory overhead of training Llama3-8B on single A100 80G GPU.", "description": "This table shows a breakdown of memory usage during the training of the Llama3-8B model on a single A100 80G GPU using the vanilla PyTorch implementation. It lists the memory overhead for different components such as weights, activations, gradients, and optimizer, both separately and as the total peak memory usage.  It provides context for understanding the memory optimization strategies introduced later in the paper.", "section": "5.1 Memory Optimization of MINI-SEQUENCE TRANSFORMER (MST)"}, {"figure_path": "2KuZHYykkq/tables/tables_17_1.jpg", "caption": "Table 9: Memory overhead of training Llama3-8B on single A100 80G GPU. The optimizer in the Backward technique is deployed.", "description": "This table shows a breakdown of memory usage during the training of the Llama3-8B model on a single A100 80G GPU when using the \"optimizer-in-backward\" technique.  It lists the memory overhead for activations, weights, gradients, and the optimizer, and shows the total peak memory usage.  The optimizer-in-backward technique combines the optimizer update with the backward pass.  Note that gradients are not shown because they overlap with activations.", "section": "5.1 Memory Optimization of MINI-SEQUENCE TRANSFORMER (MST)"}, {"figure_path": "2KuZHYykkq/tables/tables_18_1.jpg", "caption": "Table 10: Memory overhead of training Llama3-8B on single A100 80G GPU. Activation Recomputation technique is deployed.", "description": "This table presents the memory usage breakdown during Llama3-8B training on a single A100 80G GPU, using the Activation Recomputation technique. It shows the memory overhead for different components: Activation, Weight, Gradient, and Optimizer. The \"Total\" column indicates the peak memory usage. Activation recomputation is employed, resulting in a significant reduction of activation memory overhead, and the gradient is computed and released immediately after use, hence no memory overhead.", "section": "5.1 Memory Optimization of MINI-SEQUENCE TRANSFORMER (MST)"}, {"figure_path": "2KuZHYykkq/tables/tables_18_2.jpg", "caption": "Table 11: Memory overhead of training Llama3-8B on single A100 80G GPU. MINI-SEQUENCE TRANSFORMER technique is deployed.", "description": "This table shows a breakdown of memory usage during the training of the Llama3-8B model using the MINI-SEQUENCE TRANSFORMER (MST) technique on a single A100 80G GPU. It lists the memory overhead for activations, weights, gradients, and the optimizer, as well as the total peak memory usage.  The MST technique is designed to reduce memory overhead, particularly for activations and intermediate values.", "section": "5 Ablation Study"}, {"figure_path": "2KuZHYykkq/tables/tables_19_1.jpg", "caption": "Table 12: Maximum sequence length of Llama3-8B, running on distributed setting.", "description": "This table shows the maximum sequence length achieved by Llama3-8B and Llama2-7B using MST with different numbers of GPUs.  The results demonstrate the scalability of MST for training with extremely long sequences on distributed hardware.", "section": "4 Experiment"}, {"figure_path": "2KuZHYykkq/tables/tables_20_1.jpg", "caption": "Table 13: LM-head time for different sequence lengths and mini-sequence settings", "description": "This table presents the execution time of the LM-head component for various sequence lengths (1024, 2048, 4096, 8192, 20000, 40000, 80000) under different mini-sequence settings (M=2, 4, 8, 16, 32). It demonstrates the effect of the mini-sequence technique on the computational time, comparing it to the standard implementation (standard). The results show that while increasing the number of mini-sequences may slightly increase the execution time for shorter sequences, the difference becomes negligible for longer sequences.", "section": "F How many mini-sequences are needed during pre-training"}, {"figure_path": "2KuZHYykkq/tables/tables_20_2.jpg", "caption": "Table 14: LM-head memory usage (in GB) for different sequence lengths and mini-sequence settings", "description": "This table presents the memory usage in gigabytes (GB) for the LM-Head component of the model, considering different sequence lengths and various mini-sequence settings.  It demonstrates how the memory usage decreases as the number of mini-sequences increases, showcasing the memory efficiency gains achieved by using mini-sequences in processing long sequences.", "section": "3.2 Analysis: Memory Efficiency of MINI-SEQUENCE TRANSFORMER (MST)"}, {"figure_path": "2KuZHYykkq/tables/tables_20_3.jpg", "caption": "Table 15: MLP time (in seconds) for different sequence lengths and mini-sequence settings", "description": "This table presents the execution time of the MLP component in the MINI-SEQUENCE TRANSFORMER (MST) model for various sequence lengths (1024, 2048, 4096, 8192, 20000, 40000, 80000) and different numbers of mini-sequences (M). It demonstrates the impact of the number of mini-sequences on the MLP's execution time. The results show that increasing the number of mini-sequences generally leads to slightly longer execution times, particularly for shorter sequences, but the effect is less pronounced for longer sequences.", "section": "3.3 Analysis: IO Complexity and Memory of MINI-SEQUENCE TRANSFORMER (MST)"}, {"figure_path": "2KuZHYykkq/tables/tables_21_1.jpg", "caption": "Table 16: MLP memory usage (in GB) for different sequence lengths and mini-sequence settings", "description": "This table presents the memory usage in gigabytes (GB) for the MLP component of the model when using different sequence lengths and numbers of mini-sequences. The \"standard\" row shows the memory usage without employing the mini-sequence technique. Subsequent rows demonstrate the memory usage when using 2, 4, 8 mini-sequences respectively.  It helps to visualize the memory efficiency improvement achieved by using mini-sequences, especially for longer sequences.", "section": "3.3 Analysis: IO Complexity and Memory of MINI-SEQUENCE TRANSFORMER (MST)"}]