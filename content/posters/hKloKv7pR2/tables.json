[{"figure_path": "hKloKv7pR2/tables/tables_6_1.jpg", "caption": "Table 1: Averaged normalized scores on Antmaze-v2 tasks. Reported scores are the results of the final 100 evaluations and 5 random seeds.", "description": "This table presents the averaged normalized scores achieved by different offline reinforcement learning algorithms on Antmaze-v2 tasks.  The scores are averages across 100 final evaluations, with 5 random seeds used for each algorithm.  The algorithms compared include IQL, OTR+IQL, CQL, the proposed PPL algorithm (both with CQL and ReBRAC cost functions), ReBRAC, and the proposed PPLR algorithm. The table provides a quantitative comparison of the performance of the proposed methods against state-of-the-art baselines on this specific set of benchmark tasks. ", "section": "5.1 Benchmarks and Baselines"}, {"figure_path": "hKloKv7pR2/tables/tables_6_2.jpg", "caption": "Table 2: Averaged normalized scores on MuJoCo tasks. Reported scores are the results of the final 10 evaluations and 5 random seeds.", "description": "This table presents the average normalized scores achieved by different offline reinforcement learning algorithms on three MuJoCo continuous control tasks: HalfCheetah, Hopper, and Walker.  The scores are averages over 10 final evaluations and 5 random seeds, offering a robust comparison of algorithm performance. Algorithms compared include behavior cloning (BC), One-Step RL, Conservative Q-Learning (CQL), Implicit Q-Learning (IQL), IQL with Optimal Transport Reward Labeling (OTR+IQL), Twin Delayed Deep Deterministic Policy Gradient with behavior cloning (TD3+BC), ReBRAC, and the proposed Partial Policy Learning (PPL) method.  The table allows for a quantitative assessment of the relative effectiveness of each algorithm on these benchmark tasks.", "section": "5.2 Settings"}, {"figure_path": "hKloKv7pR2/tables/tables_7_1.jpg", "caption": "Table 3: Averaged normalized scores on Android tasks. Reported scores are the results of the final 10 evaluations and 5 random seeds.", "description": "This table presents the averaged normalized scores achieved by different offline reinforcement learning algorithms on Android tasks from the D4RL benchmark.  The scores represent the average performance across 5 random seeds and the final 100 evaluation steps. The algorithms compared include Behavior Cloning (BC), Twin Delayed Deep Deterministic Policy Gradient with behavior Cloning (TD3+BC), Conservative Q-Learning (CQL), Implicit Q-Learning (IQL), IQL with Optimal Transport Reward Labeling (OTR+IQL), ReBRAC, and the proposed Partial Policy Learning (PPLR). The table is broken down by task (Human, Cloned, Expert) for each environment (Pen, Door, Hammer, Relocate).", "section": "5.3 Results"}, {"figure_path": "hKloKv7pR2/tables/tables_12_1.jpg", "caption": "Table 4: Averaged normalized scores on MuJoCo tasks. Results of the final 10 evaluations and 5 random seeds.", "description": "This table presents the averaged normalized scores achieved by the OneStep-RL and PPL (Partial Policy Learning) algorithms on three different MuJoCo tasks: HalfCheetah, Hopper, and Walker.  The results are further broken down into three dataset categories: Medium, Medium-Replay, and Medium-Expert, representing variations in the data used for training.  Each score is an average over 10 final evaluations and 5 random seeds, offering a measure of statistical reliability.  The table highlights the performance differences between OneStep-RL and the proposed PPL method across various scenarios.", "section": "5 D4RL Experiments"}, {"figure_path": "hKloKv7pR2/tables/tables_13_1.jpg", "caption": "Table 5: Optimal w values for different environments", "description": "This table shows the optimal values of the hyperparameter 'w' for different environments in the D4RL benchmark suite. The hyperparameter 'w' controls the size of the action subspace considered by the Partial Policy Learning (PPL) algorithm.  Different environments may require different values of 'w' to achieve optimal performance, reflecting the varying complexities and characteristics of the datasets.", "section": "5.4 Parameter w"}]