{"references": [{"fullname_first_author": "Martin Arjovsky", "paper_title": "Wasserstein generative adversarial networks", "publication_date": "2017-00-00", "reason": "This foundational paper introduced the Wasserstein GAN, a generative model based on optimal transport, which is central to the current paper's approach."}, {"fullname_first_author": "Justin Fu", "paper_title": "D4RL: Datasets for deep data-driven reinforcement learning", "publication_date": "2020-00-00", "reason": "This paper introduced the D4RL benchmark suite, which provides the datasets used for evaluating the algorithm proposed in the current paper."}, {"fullname_first_author": "Scott Fujimoto", "paper_title": "Addressing function approximation error in actor-critic methods", "publication_date": "2018-00-00", "reason": "This paper addresses function approximation errors in actor-critic methods, a critical challenge in reinforcement learning that is also tackled by the present work."}, {"fullname_first_author": "Aviral Kumar", "paper_title": "Conservative Q-learning for offline reinforcement learning", "publication_date": "2020-00-00", "reason": "This paper introduces Conservative Q-learning (CQL), a key baseline method used for comparison with the current work, showcasing the importance of this algorithm in the offline RL field."}, {"fullname_first_author": "David Brandfonbrener", "paper_title": "Offline RL without off-policy evaluation", "publication_date": "2021-00-00", "reason": "This paper proposes offline RL without off-policy evaluation, which is directly relevant to the current paper\u2019s focus on offline reinforcement learning and provides a key comparison method."}]}