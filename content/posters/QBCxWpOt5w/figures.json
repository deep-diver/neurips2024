[{"figure_path": "QBCxWpOt5w/figures/figures_2_1.jpg", "caption": "Figure 1: Arguably, reasoning involves updating an internal state (red) as new information is processed (green). The diagram above, where each element represents a piece of information, is an abstract depiction of this idea. This observation motivates our use of iterative tasks as a proxy for more general reasoning processes. At first glance, a limitation of transformers is their lack of an internal state, which makes it challenging to implement this diagram [32].", "description": "This figure illustrates the concept of reasoning as an iterative process of updating an internal state based on incoming information.  Each node represents a piece of information, with green nodes representing new information and the red node representing the internal state. The arrows show how the internal state is updated with each new piece of information. The figure highlights the challenge of implementing this type of iterative reasoning in transformers, which typically lack an explicit internal state mechanism.", "section": "2 Controlled Setup: Learning Iterative Algorithms"}, {"figure_path": "QBCxWpOt5w/figures/figures_2_2.jpg", "caption": "Figure 2: A single transformer layer cannot implement the diagram from 1, as it cannot access its previous outputs. This limitation can be bypassed by stacking transformer layers, as illustrated here. The red arrow indicates a residual connection. This naive method requires as many layers as there are reasoning hops.", "description": "This figure illustrates how a single transformer layer cannot maintain an internal state across multiple reasoning steps because it lacks access to previous outputs.  The diagram shows that to implement the iterative reasoning process, multiple layers are needed. Each layer processes the current input token and the previous internal state (residual connection) to generate the next internal state.  This approach, however, is inefficient as it needs a number of layers equal to the number of reasoning steps required.", "section": "Can a Transformer Learn Iterative Algorithms?"}, {"figure_path": "QBCxWpOt5w/figures/figures_2_3.jpg", "caption": "Figure 1: Arguably, reasoning involves updating an internal state (red) as new information is processed (green). The diagram above, where each element represents a piece of information, is an abstract depiction of this idea. This observation motivates our use of iterative tasks as a proxy for more general reasoning processes. At first glance, a limitation of transformers is their lack of an internal state, which makes it challenging to implement this diagram [32].", "description": "This figure shows a diagram illustrating how reasoning involves updating an internal state as new information is processed. Each element in the diagram represents a piece of information.  The figure highlights that transformers lack an internal state, making it challenging to directly implement the diagram's concept.", "section": "2 Controlled Setup: Learning Iterative Algorithms"}, {"figure_path": "QBCxWpOt5w/figures/figures_2_4.jpg", "caption": "Figure 4: Chain-of-thought addresses this issue by explicitly representing the reasoning process in token space. The auto-regressive nature of LLMs (blue arrow) allows for the implementation of iterative algorithms, as long as the states are encoded in token space. A concrete implementation of such a mechanism, which we call iteration head, is described in Section 3. In practical applications of LLMs, one could imagine earlier layers summarizing the t-th input sentence (or some other coherent semantic information of varying token length) into xt, as well as summarizing the generated CoT sentences into some st-1, with later layers translating the state st into readable text.", "description": "This figure illustrates how chain-of-thought reasoning allows transformers to solve iterative tasks.  It shows that by explicitly representing the reasoning steps as tokens, the autoregressive nature of LLMs can be leveraged to implement iterative algorithms.  The figure highlights the key concept of an \"iteration head\", a mechanism that enables the transformer to maintain and update an internal state (st) across multiple reasoning steps, eventually leading to a final answer.  The internal states are represented as tokens, enabling iterative processing within the transformer's autoregressive framework.  The diagram contrasts this approach with the limitations of single-token prediction, illustrating the benefits of CoT reasoning for handling iterative tasks.", "section": "Can a Transformer Learn Iterative Algorithms?"}, {"figure_path": "QBCxWpOt5w/figures/figures_4_1.jpg", "caption": "Figure 5: Implementation of an iteration head with a two-layer transformer. Contiguous box: superposition in high-dimensional space. Blue: information brought to working space thanks to residual connections. Red: information brought thanks to attention. Green: next-token prediction. The first layer MLP implements a subtraction t = (L + t) \u2013 (L + 1) + 1 for the second attention to be able to query pt from (PL+1,PL+t). The second layer MLP implements F to be able to predict st from (St-1, Xt), with the \"end-of-input\" mark assimilated to the initial state so of Algorithm 1.", "description": "This figure illustrates how a two-layer transformer can implement an iterative algorithm using an \"iteration head.\" The first layer uses attention to identify the end-of-input token and retrieves the position of the current token.  The second layer uses attention to retrieve both the previous state (st-1) and current token (xt) which allows the MLP to calculate the next state (st). This demonstrates how the transformer can effectively perform iterative reasoning.", "section": "One Head to Rule Them All"}, {"figure_path": "QBCxWpOt5w/figures/figures_5_1.jpg", "caption": "Figure 6: Left: attention maps learned for the parity problem when processing a sequence of length L = 29. Yellow indicates high attention score. The yellow line on the left plot shows that all the queries after the EoI token at position t = 30 point to the EoI token. In other terms, the first attention implements the \"Are you EoI?\" query of Figure 5, while the second implements the \"Are you pt?\" query. Right: accuracy dynamics for different sequence lengths when learning the parity problem. We observe fast learning of short sequences (we used the tab10 color scheme of Matplotlib [28] with L \u2208 {8, 11, 14, 17, . . ., 32}), and characteristic staircase behaviors.", "description": "The figure displays attention maps and accuracy dynamics when training a transformer on the parity problem. The left side shows that the first attention layer focuses on identifying the end-of-input token, while the second attention layer focuses on extracting relevant tokens for iterative processing. The right side illustrates how the model's accuracy on this task increases with the number of training epochs, particularly for shorter sequences, showcasing a characteristic pattern of stepwise improvements.", "section": "3.2 Learning an Iteration Head"}, {"figure_path": "QBCxWpOt5w/figures/figures_6_1.jpg", "caption": "Figure 7: Test accuracy (where red indicates better performance) after learning the polynomial iteration task with P(X,Y) = XY + 1 in F11 for 1000 epochs. The accuracy is reported as a function of the embedding dimension (on the y-axis), and the maximum sequence length Lmax (on the x-axis). The learning was conducted with a two-layer transformer with CoT (left), without CoT (middle), or with a one-layer transformer with CoT (right). This illustrates the usefulness of CoT and two-layer architectures.", "description": "This figure compares the test accuracy of three different model setups when learning the polynomial iteration task.  The x-axis shows the maximum sequence length, and the y-axis shows the embedding dimension.  The leftmost plot shows a two-layer transformer using chain-of-thought (CoT), the middle plot shows a two-layer transformer without CoT, and the rightmost plot shows a one-layer transformer with CoT.  Redder colors indicate better performance. The figure demonstrates that using CoT with a two-layer transformer leads to significantly better performance than the other methods.", "section": "3.3 Ablation Studies"}, {"figure_path": "QBCxWpOt5w/figures/figures_7_1.jpg", "caption": "Figure 6: Left: attention maps learned for the parity problem when processing a sequence of length L = 29. Yellow indicates high attention score. The yellow line on the left plot shows that all the queries after the EoI token at position t = 30 point to the EoI token. In other terms, the first attention implements the \"Are you EoI?\" query of Figure 5, while the second implements the \"Are you pt?\" query. Right: accuracy dynamics for different sequence lengths when learning the parity problem. We observe fast learning of short sequences (we used the tab10 color scheme of Matplotlib [28] with L \u2208 {8, 11, 14, 17, . . ., 32}), and characteristic staircase behaviors.", "description": "The figure shows the attention maps learned by a transformer when solving the parity problem. The left panel shows the attention weights for the first and second layers. The yellow lines highlight how the model focuses on the end-of-input (EoI) token in the first layer and on the current token in the second layer to perform iterative reasoning. The right panel illustrates the accuracy of the model during training as a function of the sequence length, showing fast learning for shorter sequences and slower learning for longer sequences.", "section": "3.2 Learning an Iteration Head"}, {"figure_path": "QBCxWpOt5w/figures/figures_8_1.jpg", "caption": "Figure 9: Left: Test accuracy as a function of the number of epochs, averaged over 100 runs, when learning the polynomial iteration task with P(X,Y) = XY + 1 in F11 (blue) and the parity problem (orange). Right: The second attention peakiness score indicates whether the network is learning the iteration head described in Figure 5. The green curve corresponds to the accuracy on the parity problem when learning the polynomial iteration for the first 200 epochs before switching the dataset to learn the parity problem.", "description": "This figure compares the learning curves of three scenarios: learning the polynomial iteration task, learning the parity problem, and learning the parity problem after pre-training on the polynomial iteration task. The left panel shows the test accuracy over epochs, while the right panel shows the attention peakiness score, indicating whether the network learns the iteration head. The green curve demonstrates the benefit of transfer learning, where pre-training on a related task improves learning efficiency for the target task. ", "section": "4.1 Inducing Induction"}, {"figure_path": "QBCxWpOt5w/figures/figures_9_1.jpg", "caption": "Figure 7: Test accuracy (where red indicates better performance) after learning the polynomial iteration task with P(X,Y) = XY + 1 in F11 for 1000 epochs. The accuracy is reported as a function of the embedding dimension (on the y-axis), and the maximum sequence length Lmax (on the x-axis). The learning was conducted with a two-layer transformer with CoT (left), without CoT (middle), or with a one-layer transformer with CoT (right). This illustrates the usefulness of CoT and two-layer architectures.", "description": "This figure shows the test accuracy of a two-layer transformer trained on the polynomial iteration task, comparing three different training scenarios: with chain-of-thought (CoT), without CoT, and with CoT using only one layer.  The accuracy is shown as a heatmap across different embedding dimensions and maximum sequence lengths. Red indicates better performance. The figure highlights the superior performance of the two-layer transformer with CoT, demonstrating the usefulness of this approach for solving iterative tasks.", "section": "3.3 Ablation Studies"}, {"figure_path": "QBCxWpOt5w/figures/figures_13_1.jpg", "caption": "Figure 8: Left: attention peakiness score after 1000 epochs of learning with the polynomial iteration task parameterized by P(X, Y) = XY + 1 in F11 as a function of the embedding dimension d and the maximum sequence length Lmax. Right: example of attention maps of sub-sampled iteration heads.", "description": "The figure shows the results of an experiment to learn the polynomial iteration task. The left panel displays a heatmap showing the \"peakiness\" score (a measure of how concentrated the attention is) for the first and second attention layers as a function of embedding dimension and maximum sequence length.  The right panel shows examples of attention maps that illustrate an alternative circuit learned by the model, where attention is not focused on all positions but rather subsampled, especially in lower embedding dimensions.", "section": "3.2 Learning an Iteration Head"}, {"figure_path": "QBCxWpOt5w/figures/figures_13_2.jpg", "caption": "Figure 12: Test accuracy for SGD and Adam after 100 epochs.", "description": "This figure shows a comparison of the test accuracy achieved using SGD and Adam optimizers across various learning rates and batch sizes. The contour plots illustrate the performance of each optimizer under different hyperparameter settings.  Redder colors indicate higher accuracy. The plot helps to visualize the optimal regions in the hyperparameter space for each optimizer, highlighting the effects of learning rate and batch size on model performance.", "section": "3.3 Ablation Studies"}, {"figure_path": "QBCxWpOt5w/figures/figures_14_1.jpg", "caption": "Figure 13: Recovering the \"who is pt?\" key-query association, yet shared across layers and heads when training a three layers transformer with two attention heads per layer.", "description": "The figure shows the attention maps for a three-layer transformer with two attention heads per layer when trained on an iterative task. It demonstrates that the \"who is pt?\" key-query association, crucial for iterative reasoning, is not confined to a single layer or head, but rather distributed across multiple layers and heads. This suggests a more complex and robust mechanism for iterative reasoning in larger transformer models than initially hypothesized.", "section": "3.2 Learning an Iteration Head"}, {"figure_path": "QBCxWpOt5w/figures/figures_14_2.jpg", "caption": "Figure 14: The effect of small embeddings when learning the parity problem. The top row corresponds to what has been learned after 1000 epochs. The bottom one corresponds to 5000 epochs.", "description": "This figure shows the effect of small embedding dimensions on the learning of the parity problem.  It presents heatmaps displaying the test accuracy as a function of embedding dimension and maximum sequence length, separately for learned and frozen positional embeddings.  The top row shows results after 1000 training epochs, while the bottom row shows results after 5000 epochs.  The differences between the two rows highlight how the model's performance evolves over time and how its capacity to handle longer sequences increases with training.", "section": "3.2 Learning an Iteration Head"}, {"figure_path": "QBCxWpOt5w/figures/figures_14_3.jpg", "caption": "Figure 15: Attention learned when studying frozen vs learned positional embedding.", "description": "This figure compares the attention peakiness scores (a measure of how closely the attention maps follow the patterns of Figure 6) for both learned and frozen positional embeddings. The top row shows the results when the positional embeddings are learned, while the bottom row shows the results when they are frozen. The left column shows the results for the first attention head, while the right column shows the results for the second attention head. The x-axis represents the maximum sequence length, and the y-axis represents the embedding dimension. The color scale represents the attention peakiness score, with higher values indicating greater concentration of attention. This figure helps to illustrate the effect of learned positional embeddings on the attention mechanism of the transformer model and their contribution to the emergence of iteration heads.", "section": "3.3 Ablation Studies"}, {"figure_path": "QBCxWpOt5w/figures/figures_15_1.jpg", "caption": "Figure 16: Attention learned when studying frozen vs learned positional embedding. The setting is slightly different, we fixed the token embedding dimension to 32, and added the position embedding only on the first p dimension, where p was varying from 2 to 32.", "description": "This figure displays the results of an experiment comparing learned and frozen positional embeddings in a transformer model. The experiment focused on learning the parity problem using a model with a token embedding dimension of 32. The positional embeddings were added only to the first p dimensions, with p ranging from 2 to 32. The figure shows the attention peakiness scores (measuring how concentrated the attention is) for both learned and frozen positional embeddings, broken down by the first and second attention heads, and plotted against the maximum sequence length and number of epochs. This helps understand the impact of positional embeddings on learning iteration heads and solving the parity problem.", "section": "3.3 Ablation Studies"}]