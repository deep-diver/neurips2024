[{"figure_path": "recsheQ7e8/figures/figures_1_1.jpg", "caption": "Figure 1: Previous LLMs are trained with homogeneous system messages reflecting general helpfulness and harmlessness. We propose training LLMs with diverse system messages, each representing an individual's multifaceted preferences, to generalize to unseen system messages. The resulting model, JANUS 7B, is adept at generating personalized responses for personalized system messages.", "description": "This figure illustrates the core idea of the paper.  Traditional LLMs are trained on general system messages (e.g., \"You are a helpful assistant\"), resulting in responses that reflect general helpfulness. The authors propose a new approach: training LLMs with diverse system messages, each reflecting a specific user's multifaceted preferences. This allows the LLM to generalize to unseen system messages and generate personalized responses that align with individual user preferences. The resulting model, JANUS 7B, demonstrates this ability.", "section": "1 Introduction"}, {"figure_path": "recsheQ7e8/figures/figures_3_1.jpg", "caption": "Figure 2: MULTIFACETED COLLECTION construction process. For each instruction, value descriptions are augmented from general to specific, allowing for multiple facets to branch out. We combine values from various dimensions into a system message to materialize preferences into model input. Following the system message and instruction, a proprietary LLM generates a gold response for training.", "description": "This figure illustrates the process of creating the MULTIFACETED COLLECTION dataset.  It starts with a single instruction, for which various user preferences are defined across multiple dimensions (Style, Background Knowledge, Informativeness, and Harmlessness). Each dimension has sub-dimensions and values, which are combined to form a comprehensive \"Preference Set.\"  This set is then transformed into a personalized system message that's prepended to the original instruction.  A large language model then generates a gold standard response based on the system message and instruction. This process is repeated for multiple instructions to build a training dataset.", "section": "3 MULTIFACETED COLLECTION for scalable individualized alignment"}, {"figure_path": "recsheQ7e8/figures/figures_5_1.jpg", "caption": "Figure 3: Human comparison of JANUS against Mistral Instruct 7B v0.2, GPT-3.5-Turbo-0125, and GPT-4-0613 on MULTIFACETED BENCH.", "description": "This figure shows the results of human evaluation comparing the performance of the JANUS model against three other models: Mistral Instruct 7B v0.2, GPT-3.5 Turbo-0125, and GPT-4-0613.  The comparison is based on the MULTIFACETED BENCH benchmark. The chart displays the percentage breakdown of outcomes for each model comparison: JANUS winning, both models being equally good, both being bad, and the opponent model winning.  This illustrates JANUS's competitive performance relative to established models on a multifaceted evaluation.", "section": "5 Experimental results"}, {"figure_path": "recsheQ7e8/figures/figures_8_1.jpg", "caption": "Figure 4: Length distribution of responses generated by LLMs and reference answer on MULTIFACETED BENCH.", "description": "This violin plot displays the distribution of the number of words in the responses generated by different LLMs (Mistral 7B Instruct v0.2, GPT-3.5 Turbo, GPT-4 Turbo, and Janus 7B) compared to the reference answers (GPT-4 Turbo) on the MULTIFACETED BENCH benchmark.  It shows that Janus 7B generates longer responses on average, more similar in length to those of the GPT-4 Turbo reference answers than the other models. This suggests that Janus 7B is better able to capture the nuance and detail of the prompts than the other models.", "section": "6 Analysis"}, {"figure_path": "recsheQ7e8/figures/figures_8_2.jpg", "caption": "Figure 5: Test-time system message ablation results on MULTIFACETED BENCH.", "description": "This figure displays the results of an ablation study evaluating the impact of system messages on the performance of different language models.  The models were evaluated using the MULTIFACETED BENCH benchmark.  Three conditions are shown: (1) Models were tested using system messages that reflect multifaceted user preferences; (2) Models were tested without any system messages at all; (3) The training process for the models included multifaceted system messages. The y-axis represents the average score obtained across various metrics and the x-axis denotes different models: Mistral 7B v0.2, Mistral 7B Instruct v0.2, GPT-4-Turbo-0125, Janus 7B, Janus+ORPO 7B, and Janus+DPO 7B. The results visually demonstrate the effect of using system messages on model performance, particularly for models trained with multifaceted messages.", "section": "6 Analysis"}, {"figure_path": "recsheQ7e8/figures/figures_8_3.jpg", "caption": "Figure 1: Previous LLMs are trained with homogeneous system messages reflecting general helpfulness and harmlessness. We propose training LLMs with diverse system messages, each representing an individual's multifaceted preferences, to generalize to unseen system messages. The resulting model, JANUS 7B, is adept at generating personalized responses for personalized system messages.", "description": "This figure illustrates the core concept of the paper:  previous large language models (LLMs) have been trained on a uniform system message (e.g., \"You are a helpful assistant\"), which limits their ability to adapt to diverse user preferences.  The authors propose a new approach \u2013 training the LLM on thousands of diverse system messages \u2013 that reflect different user values.  The resulting model, JANUS 7B, can generate personalized responses based on these diverse system messages reflecting user preferences. The figure shows a comparison between a previous LLM's homogeneous response to a default system message and JANUS 7B's more personalized and varied responses.", "section": "1 Introduction"}, {"figure_path": "recsheQ7e8/figures/figures_17_1.jpg", "caption": "Figure 6: Winogender accuracy comparison across models. The 'gotcha' scenarios refer to a subset of the dataset where the gender of the pronoun referring to an occupation does not match U.S. statistics on that occupation's majority gender, i.e., they challenge the model's reliance on stereotypes more.", "description": "This figure compares the performance of four different language models (Mistral 7B Instruct v0.2, Llama 3 8B Instruct, Gemma 2 9B IT, and Janus 7B) on the Winogender schema. The Winogender schema is a benchmark used to evaluate gender bias in language models. The figure shows the accuracy of each model in predicting the gender of a pronoun referring to an occupation. The figure also shows the accuracy of each model in 'gotcha' scenarios. Gotcha scenarios are situations where the gender of the pronoun does not match U.S. statistics on that occupation's majority gender. These scenarios are designed to challenge the model's reliance on stereotypes. The results show that Janus 7B performs comparatively well to other models, indicating a lower reliance on gender stereotypes.", "section": "5.3 Diversity and harmlessness"}, {"figure_path": "recsheQ7e8/figures/figures_18_1.jpg", "caption": "Figure 7: Jensen-Shannon distance between the human (US and Japan) and model distributions in GlobalOpinionQA and MPI answer choices. The probability of the model's next token prediction (logit) for each answer choice selection is calculated, and the probabilities are averaged over 5 runs of the same 3-shot prompt but with randomized answer choices in the few-shot examples. JANUS diverges less from the pre-trained distribution than Mistral 7B Instruct v0.2 does.", "description": "This figure displays the Jensen-Shannon divergence between human response distributions and model response distributions. The human distributions are based on data from the US and Japan, using GlobalOpinionQA and MPI datasets. The model distributions represent responses from Mistral 7B v0.2, Mistral 7B Instruct v0.2, and Janus 7B. The results illustrate that Janus 7B shows less divergence from the pre-trained distribution than Mistral 7B instruct v0.2, suggesting better calibration to human preferences.", "section": "Additional analyses"}, {"figure_path": "recsheQ7e8/figures/figures_18_2.jpg", "caption": "Figure 7: Jensen-Shannon distance between the human (US and Japan) and model distributions in GlobalOpinionQA and MPI answer choices. The probability of the model\u2019s next token prediction (logit) for each answer choice selection is calculated, and the probabilities are averaged over 5 runs of the same 3-shot prompt but with randomized answer choices in the few-shot examples. JANUS diverges less from the pre-trained distribution than Mistral 7B Instruct v0.2 does.", "description": "This figure displays the Jensen-Shannon divergence between human (US and Japan) and model answer distributions for GlobalOpinionQA and MPI datasets.  The metric quantifies the difference in probability distributions for answer choices. Lower scores indicate higher alignment with human responses.  The results show JANUS demonstrating less divergence from the pre-trained model than Mistral 7B Instruct v0.2, suggesting better calibration to human preferences.", "section": "Additional analyses"}, {"figure_path": "recsheQ7e8/figures/figures_24_1.jpg", "caption": "Figure 1: Previous LLMs are trained with homogeneous system messages reflecting general helpfulness and harmlessness. We propose training LLMs with diverse system messages, each representing an individual's multifaceted preferences, to generalize to unseen system messages. The resulting model, JANUS 7B, is adept at generating personalized responses for personalized system messages.", "description": "This figure illustrates the difference between traditional LLMs and the proposed JANUS model.  Traditional LLMs are trained with a generic system message (e.g., \"You are a helpful assistant\"), which limits their ability to adapt to diverse user preferences. In contrast, JANUS is trained with thousands of diverse system messages, each reflecting a unique set of user preferences. This allows JANUS to generalize better to unseen system messages and generate personalized responses.", "section": "1 Introduction"}, {"figure_path": "recsheQ7e8/figures/figures_25_1.jpg", "caption": "Figure 9: ROUGE-L scores between pair of preference descriptions in each instruction.", "description": "This figure displays the ROUGE-L scores, measuring the similarity between pairs of preference descriptions within the MULTIFACETED COLLECTION dataset.  The distribution of scores is shown for each of the four preference dimensions: Style, Informativeness, Background Knowledge, and Harmlessness.  Each dimension has multiple subdimensions, and within each subdimension, multiple specific preferences were generated. The figure helps illustrate the diversity of preferences within the dataset, showing the extent to which the various preferences are distinct from one another.", "section": "3. MULTIFACETED COLLECTION for scalable individualized alignment"}, {"figure_path": "recsheQ7e8/figures/figures_25_2.jpg", "caption": "Figure 12: Difficulty distribution of final MULTIFACETED BENCH instances by human evaluator", "description": "This figure shows a donut chart illustrating the difficulty distribution of the final instances in the MULTIFACETED BENCH dataset as assessed by human evaluators. The difficulty levels are categorized into three levels: Easy, Intermediate, and Hard.  Each segment of the chart represents the percentage of instances falling into each difficulty level, providing a visual representation of the overall difficulty distribution of the dataset.", "section": "4 Experimental setup"}, {"figure_path": "recsheQ7e8/figures/figures_28_1.jpg", "caption": "Figure 1: Previous LLMs are trained with homogeneous system messages reflecting general helpfulness and harmlessness. We propose training LLMs with diverse system messages, each representing an individual's multifaceted preferences, to generalize to unseen system messages. The resulting model, JANUS 7B, is adept at generating personalized responses for personalized system messages.", "description": "This figure illustrates the core idea of the paper: using diverse system messages to train LLMs for better generalization to unseen user preferences.  It contrasts traditional LLMs trained with a uniform system message (e.g., \"You are a helpful assistant\") against the proposed method that leverages personalized system messages reflecting user values. The resulting model, JANUS 7B, is highlighted as capable of providing personalized responses based on these diverse system messages.", "section": "1 Introduction"}]