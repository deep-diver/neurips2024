[{"figure_path": "recsheQ7e8/tables/tables_4_1.jpg", "caption": "Table 1: MULTIFACETED COLLECTION statistics", "description": "This table presents the statistics of the MULTIFACETED COLLECTION dataset.  It shows the number of unique user instructions, system messages, dimensions, subdimensions, and values present in the dataset.  This dataset is a key component of the research, used to train the JANUS language model to better generalize to diverse user preferences.", "section": "3 MULTIFACETED COLLECTION for scalable individualized alignment"}, {"figure_path": "recsheQ7e8/tables/tables_6_1.jpg", "caption": "Table 2: MULTIFACETED BENCH results. For each model's response, an evaluator LLM assigns a score ranging from 1 to 5. The average score is calculated by averaging all the scores for each sample. To ensure consistency, all scores are evaluated three times, and the averaged result is recorded.", "description": "This table presents the results of the MULTIFACETED BENCH benchmark.  It shows the average scores (ranging from 1 to 5, with higher scores being better) given by an LLM evaluator for responses generated by several different language models. Each model's responses were evaluated three times to ensure reliability, and the average of these three evaluations is reported. The benchmark consists of multiple sub-benchmarks (mf-AlpacaEval, mf-FLASK, mf-Koala, mf-MT-Bench, and mf-Self-Instruct), with the average score across all sub-benchmarks also provided for each model. The models are categorized into pretrained open models, instruction-tuned open models, the JANUS suite of models, and preference-optimized proprietary models.", "section": "5.1 Unseen multifaceted preference"}, {"figure_path": "recsheQ7e8/tables/tables_6_2.jpg", "caption": "Table 3: Best-of-64 sampling results on MULTIFACETED BENCH using JANUS-RM 7B", "description": "This table presents the results of using best-of-64 sampling with the JANUS reward model on the MULTIFACETED BENCH.  It compares the average scores achieved by JANUS* 7B and JANUS+DPO 7B  with and without best-of-64 sampling across five benchmarks: mf-AlpacaEval, mf-FLASK, mf-Koala, mf-MT-Bench, and mf-Self-Instruct.  The scores reflect the average rating from an LLM evaluator on a scale of 1 to 5 for each response.", "section": "5 Experimental results"}, {"figure_path": "recsheQ7e8/tables/tables_7_1.jpg", "caption": "Table 2: MULTIFACETED BENCH results. For each model's response, an evaluator LLM assigns a score ranging from 1 to 5. The average score is calculated by averaging all the scores for each sample. To ensure consistency, all scores are evaluated three times, and the averaged result is recorded.", "description": "This table presents the results of the MULTIFACETED BENCH benchmark, which evaluates the performance of various LLMs in generating responses tailored to specific user preferences.  The benchmark uses five different tasks (AlpacaEval 2.0, FLASK, Koala, MT-Bench, and Self-Instruct), each with multiple user instructions and associated system messages reflecting unseen user values. For each response, a large language model (LLM) acts as an evaluator, assigning a score from 1 to 5 based on the quality and relevance of the generated response in satisfying the specified preferences.  Scores are averaged across three evaluations to ensure reliability. The table compares the performance of several LLMs (pre-trained models, instruction-tuned models, and preference-optimized models) across all five tasks, providing a comprehensive comparison of their abilities to generate personalized responses.", "section": "5.1 Unseen multifaceted preference"}, {"figure_path": "recsheQ7e8/tables/tables_7_2.jpg", "caption": "Table 5: Harmlessness analysis on RealToxicityPrompts. \u2020 indicates the results from previous work [48]. Details regarding the score metric can be found in Appendix L.3.", "description": "This table presents the results of evaluating the harmlessness of different language models using the RealToxicityPrompts benchmark.  It compares the models across three key metrics: average maximum toxicity, the probability of toxicity, and the diversity of the generated text.  Lower scores for toxicity and higher scores for diversity are preferred, indicating more harmless and varied outputs. The table includes both open-source and proprietary models, allowing for a comparison of their performance on this critical aspect of responsible AI development.  The \"\u2020\" symbol indicates results taken from a previously published work.", "section": "5.3 Diversity and harmlessness"}, {"figure_path": "recsheQ7e8/tables/tables_8_1.jpg", "caption": "Table 6: Comparison of average and max ROUGE-L scores of three different personalized responses per instruction generated by LLMs in MULTIFACETED BENCH.", "description": "This table compares the average and maximum ROUGE-L scores achieved by three different large language models (Mistral 7B Instruct v0.2, GPT-4 Turbo, and JANUS 7B) when generating personalized responses for each instruction in the MULTIFACETED BENCH. ROUGE-L is a metric used to evaluate the similarity between two texts, reflecting the diversity and quality of the generated responses.  The higher the score, the more similar the generated response is to the reference answer, indicating higher quality and less repetition.  The table shows that JANUS 7B, despite being trained on a more diverse dataset,  has slightly lower scores on average and maximum ROUGE-L compared to GPT-4 Turbo and Mistral 7B Instruct v0.2.", "section": "6 Analysis"}, {"figure_path": "recsheQ7e8/tables/tables_17_1.jpg", "caption": "Table 2: MULTIFACETED BENCH results. For each model's response, an evaluator LLM assigns a score ranging from 1 to 5. The average score is calculated by averaging all the scores for each sample. To ensure consistency, all scores are evaluated three times, and the averaged result is recorded.", "description": "This table presents the results of the MULTIFACETED BENCH benchmark, which evaluates the performance of various LLMs in generating responses tailored to diverse user preferences.  For each model, the table shows the average score across five sub-benchmarks: mf-AlpacaEval, mf-FLASK, mf-Koala, mf-MT-Bench, and mf-Self-Instruct.  Each score represents the average of three separate evaluations for each sample, using an LLM evaluator to maintain consistency. The models are categorized into pretrained open models, instruction-tuned open models, and preference-optimized proprietary models to allow comparison across different model types and training approaches.", "section": "5.1 Unseen multifaceted preference"}, {"figure_path": "recsheQ7e8/tables/tables_17_2.jpg", "caption": "Table 8: Multi-turn scenarios results in MT-Bench.", "description": "This table presents the results of evaluating the performance of different LLMs on multi-turn conversations within the MT-Bench benchmark.  It compares the scores achieved by Mistral 7B Instruct v0.2, LLaMA 3 8B Instruct, and JANUS 7B, highlighting the superior performance of JANUS 7B on multi-turn conversations in this specific benchmark.", "section": "Experimental results"}, {"figure_path": "recsheQ7e8/tables/tables_19_1.jpg", "caption": "Table 10: Ablation for number of system messages.", "description": "This table shows the ablation study results by varying the number of system messages used per instruction during training.  The results are evaluated across five benchmarks: mf-AlpacaEval, mf-FLASK, mf-Koala, mf-MT-Bench, and mf-Self-Instruct.  The average scores for each benchmark are shown for training with 1, 2, and 3 system messages per instruction, respectively. The table demonstrates that increasing the number of system messages improves the overall performance, with the best result achieved using 3 system messages per instruction (JANUS).", "section": "C.2 Additional ablation studies"}, {"figure_path": "recsheQ7e8/tables/tables_19_2.jpg", "caption": "Table 11: Base model ablation.", "description": "This table presents the results of an ablation study comparing the performance of different base pre-trained language models when used with the proposed method. The models compared are Mistral 7B v0.2, LLaMA 2 7B, LLaMA 2 13B, and LLaMA 3 8B. The performance is evaluated across five benchmarks: mf-AlpacaEval, mf-FLASK, mf-Koala, mf-MT-Bench, and mf-Self-Instruct.  The table shows that larger model sizes (LLaMA 2 13B) do not always lead to improved performance, and that the choice of base model significantly impacts results.", "section": "5.2 General helpfulness"}, {"figure_path": "recsheQ7e8/tables/tables_20_1.jpg", "caption": "Table 2: MULTIFACETED BENCH results. For each model's response, an evaluator LLM assigns a score ranging from 1 to 5. The average score is calculated by averaging all the scores for each sample. To ensure consistency, all scores are evaluated three times, and the averaged result is recorded.", "description": "This table presents the results of the MULTIFACETED BENCH benchmark, which evaluates the performance of various LLMs in generating responses tailored to specific user preferences.  The benchmark uses five different datasets (AlpacaEval 2.0, FLASK, Koala, MT-Bench, Self-Instruct) and employs an LLM as an evaluator to assign a score (1-5) to each model's response.  The scores are averaged across three evaluations for improved consistency. The table compares the performance of pretrained, instruction-tuned, and preference-optimized models, including JANUS 7B and various baseline models.", "section": "5 Experimental results"}, {"figure_path": "recsheQ7e8/tables/tables_21_1.jpg", "caption": "Table 2: MULTIFACETED BENCH results. For each model's response, an evaluator LLM assigns a score ranging from 1 to 5. The average score is calculated by averaging all the scores for each sample. To ensure consistency, all scores are evaluated three times, and the averaged result is recorded.", "description": "This table presents the results of the MULTIFACETED BENCH benchmark, which evaluates the performance of various LLMs on diverse, multifaceted instructions.  Each model's response to each instruction was scored from 1 to 5 by an evaluator LLM, with this process repeated three times to ensure reliability.  The table displays the average score for each model across the benchmark, providing a comparative analysis of model performance across different instruction types and levels of complexity.  It includes pretrained open models, instruction-tuned open models, and preference-optimized proprietary models.", "section": "5.1 Unseen multifaceted preference"}, {"figure_path": "recsheQ7e8/tables/tables_22_1.jpg", "caption": "Table 2: MULTIFACETED BENCH results. For each model's response, an evaluator LLM assigns a score ranging from 1 to 5. The average score is calculated by averaging all the scores for each sample. To ensure consistency, all scores are evaluated three times, and the averaged result is recorded.", "description": "This table presents the results of the MULTIFACETED BENCH benchmark, where an LLM evaluator assessed the quality of model responses on a scale of 1 to 5.  The table shows the average scores for several models across five sub-benchmarks within the main benchmark.  Multiple evaluations were conducted for each model to ensure reliability and consistency.", "section": "5.1 Unseen multifaceted preference"}, {"figure_path": "recsheQ7e8/tables/tables_23_1.jpg", "caption": "Table 2: MULTIFACETED BENCH results. For each model's response, an evaluator LLM assigns a score ranging from 1 to 5. The average score is calculated by averaging all the scores for each sample. To ensure consistency, all scores are evaluated three times, and the averaged result is recorded.", "description": "This table presents the results of the MULTIFACETED BENCH benchmark, which evaluates the performance of various LLMs in generating responses tailored to specific user preferences.  Each model's response to each prompt is scored by an LLM evaluator on a scale of 1 to 5, based on how well the response aligns with the given user preferences. The average score for each model across all prompts is calculated by averaging the scores from three separate evaluations to ensure consistency.", "section": "5.1 Unseen multifaceted preference"}]