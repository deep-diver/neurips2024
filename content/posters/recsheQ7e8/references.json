{"references": [{"fullname_first_author": "Josh Achiam", "paper_title": "GPT-4 technical report", "publication_date": "2023-03-08", "reason": "This paper provides the technical details of GPT-4, a model frequently compared to the model presented in this paper."}, {"fullname_first_author": "Yuntao Bai", "paper_title": "Training a helpful and harmless assistant with reinforcement learning from human feedback", "publication_date": "2022-04-05", "reason": "This paper is foundational to the RLHF paradigm used as a basis for the paper's methodology."}, {"fullname_first_author": "Souradip Chakraborty", "paper_title": "Maxmin-rlhf: Towards equitable alignment of large language models with diverse human preferences", "publication_date": "2024-02-28", "reason": "This paper addresses the challenge of aligning LLMs with diverse preferences, which is a core focus of the current paper."}, {"fullname_first_author": "Pengyu Cheng", "paper_title": "Everyone deserves a reward: Learning customized human preferences", "publication_date": "2023-09-03", "reason": "This paper directly tackles the problem of personalized LLM alignment, a key aspect of the research presented."}, {"fullname_first_author": "Joel Jang", "paper_title": "Personalized soups: Personalized large language model alignment via post-hoc parameter merging", "publication_date": "2023-10-11", "reason": "This paper proposes a method for personalized LLM alignment, relevant to the approach and findings of the current study."}]}