{"references": [{"fullname_first_author": "Tom B. Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This paper is foundational to the field of large language models (LLMs), introducing the concept of few-shot learning which is crucial to the current state of LLM development and is directly related to the present work."}, {"fullname_first_author": "Takeshi Kojima", "paper_title": "Large language models are zero-shot reasoners", "publication_date": "2022-12-01", "reason": "This paper directly addresses the capabilities of LLMs in reasoning, a core topic of the current work, offering evidence that LLMs possess some zero-shot reasoning capabilities."}, {"fullname_first_author": "Jason Wei", "paper_title": "Chain-of-thought prompting elicits reasoning in large language models", "publication_date": "2022-12-01", "reason": "This paper introduces the Chain-of-Thought prompting technique, a key method in improving LLM reasoning performance, making it highly relevant to the present study's approach."}, {"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-12-01", "reason": "This paper introduced the Transformer architecture, the foundation of modern LLMs, which are the subject of this paper's investigation."}, {"fullname_first_author": "Gregor Bachmann", "paper_title": "The pitfalls of next-token prediction", "publication_date": "2024-03-01", "reason": "This paper challenges the common next-token prediction objective used in training LLMs, which the authors try to mitigate through a different approach, making it highly relevant to the current work."}]}