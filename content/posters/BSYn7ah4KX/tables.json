[{"figure_path": "BSYn7ah4KX/tables/tables_6_1.jpg", "caption": "Table 1: Results when adding different interaction phases. Column \"BOTH\" represents the ratio of converged hT* who correctly predict all 8 examples in d\u00ba and have screen: off (i.e., r20=off). The Mixtral model does not have self-refine results, as it violates the instructions too much.", "description": "This table presents the results of experiments comparing different methods for guiding the evolution of LLMs.  Three different LLMs (Old GPT3.5-Turbo, New GPT3.5-Turbo, and Claude3-haiku) were used, each with three different interaction phase methods: imitation-only, self-refine, and hypothesis-search. The table shows the correlation between the model's predictions and the ground truth (Corr. d), the percentage of times the model predicted that the screen was off (r20=off), and the percentage of times both the correlation and screen: off condition were met (BOTH).  The results demonstrate how different interaction phases affect the LLM's ability to learn correctly and reduce bias.", "section": "Influence of the interaction phase and Heff"}, {"figure_path": "BSYn7ah4KX/tables/tables_8_1.jpg", "caption": "Table 1: Results when adding different interaction phases. Column \"BOTH\" represents the ratio of converged h\u00b9 who correctly predict all 8 examples in d\u00ba and have screen: off (i.e., r20=off). The Mixtral model does not have self-refine results, as it violates the instructions too much.", "description": "This table presents the results of experiments conducted with different interaction phases to evaluate the impact on bias amplification in large language models (LLMs).  The experiments involved using various LLMs and different interaction strategies, including self-refine and hypothesis search.  The table reports three key metrics: the number of correctly predicted examples, the ratio of models with a bias towards \"screen: off\", and a combined metric representing successful bias amplification and high accuracy.  The results show that adding an interaction phase generally improves performance, especially when using hypothesis search, thereby demonstrating how the interaction phase is crucial for mitigating harmful biases.", "section": "5 Experimental Verifications when the Hypothesis is Explicit"}, {"figure_path": "BSYn7ah4KX/tables/tables_20_1.jpg", "caption": "Table 3: An example of coding the mappings, where \u03b1 is how many characters (including space and unique symbol, e.g., \u2192 and :) are used to express the grammar.", "description": "This table exemplifies three distinct mapping types (systematic, holistic, and degenerate) used in the iterated learning experiments described in the paper. Each type shows how the objects (blue circle, blue box, red circle, red box) are mapped to names (00, 01, 10, 11).  The '\u03b1' column represents the coding length, which reflects the complexity of the mapping: systematic mappings have a shorter coding length than holistic ones, while degenerate mappings have the shortest.", "section": "C.2 Iterated Learning of Bayesian Agents (re-implementation of results in Kirby et al. (2015))"}, {"figure_path": "BSYn7ah4KX/tables/tables_29_1.jpg", "caption": "Table 4: Claude3-haiku results under different Heff. We color the highest and lowest numbers in each column differently.", "description": "This table presents the results obtained using the Claude3-haiku model under five different settings for the interaction phase (Heff) in the iterated learning experiment. The five settings represent different ways of filtering or selecting data samples for the imitation phase. The table shows the ratio of easy samples, the average rank of the acronyms, and the average length of the acronyms for each setting and for different numbers of initial easy examples (Ne). The highest and lowest numbers in each column are highlighted in color to emphasize the trends.", "section": "5 Experimental Verifications when the Hypothesis is Explicit"}, {"figure_path": "BSYn7ah4KX/tables/tables_29_2.jpg", "caption": "Table 5: GPT3.5-Turbo 0125 plays with Claude3-haiku results when adding different Heff.", "description": "This table presents the results of experiments where GPT3.5-Turbo 0125 and Claude3-haiku models were used with different interaction phase settings (Heff).  The table shows the ratio of 'easy' samples (acronyms composed of common words), the average ranking of the acronyms used (lower ranks indicating more common words), and the average length of the acronyms generated, across different numbers (Ne) of 'easy' examples provided as initial data.  The results illustrate how different interaction strategies influence the amplification of biases during iterated learning.", "section": "5.1 How the Knowledge of LLM Agents Evolves"}]