[{"type": "text", "text": "Bias Amplification in Language Model Evolution: An Iterated Learning Perspective ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yi Ren Shangmin Guo Linlu Qiu UBC University of Edinburgh MIT renyi.joshua@gmail.com s.guo@ed.ac.uk linluqiu@mit.edu ", "page_idx": 0}, {"type": "text", "text": "Bailin Wang Danica J. Sutherland MIT UBC & Amii bailin.wang28@gmail.com dsuth@cs.ubc.ca ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "With the widespread adoption of Large Language Models (LLMs), the prevalence of iterative interactions among these models is anticipated to increase. Notably, recent advancements in multi-round on-policy self-improving methods allow LLMs to generate new examples for training subsequent models. At the same time, multiagent LLM systems, involving automated interactions among agents, are also increasing in prominence. Thus, in both short and long terms, LLMs may actively engage in an evolutionary process. We draw parallels between the behavior of LLMs and the evolution of human culture, as the latter has been extensively studied by cognitive scientists for decades. Our approach involves leveraging Iterated Learning (IL), a Bayesian framework that elucidates how subtle biases are magnified during human cultural evolution, to explain some behaviors of LLMs. This paper outlines key characteristics of agents\u2019 behavior in the Bayesian-IL framework, including predictions that are supported by experimental verification with various LLMs. This theoretical framework could help to more effectively predict and guide the evolution of LLMs in desired directions. The code for experiments is available at https://github.com/Joshua-Ren/iICL. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recent large language models (LLMs) have shown remarkable instruction-following ability and an increasing number of applications; it is thus reasonable to expect they are likely to become more widespread. Moreover, interactions between LLMs (either multiple models, or different generations of the same model) may also become very commonplace in the near future. In fact, many recent works consider iterative on-policy self-data-augmentation solutions to break through the bottleneck of human-generated supervisions, e.g., self-instruct (Y. Wang et al. 2022), self-refine (Madaan et al. 2023), hypothesis refinement (Qiu et al. 2024), self-distill (C. Xu et al. 2023), self-instruct (Y. Wang et al. 2022), self-reward (Weizhe et al. 2024), self-feedback (W. Xu et al. 2024), RAFT (Dong et al. 2023), ReST (Gulcehre et al. 2023), iterated DPO (Xiong, Dong, Ye, Zhong, et al. 2023), OAIF (Guo et al. 2024), SPIN (Z. Chen et al. 2024), and many more. Whether the model\u2019s knowledge is updated through in-weight or in-context mechanisms, these methods involve an LLM learning from a corpus (comprising data examples or evaluations) generated by another LLM (or itself), and subsequently transferring this acquired knowledge to others. Looking towards the long term, the future Internet may (for better or worse) contain substantial portions of LLM-generated text, which will, in turn, be employed for training the subsequent generation of models. It thus seems important to begin studying what this process will mean for future models. ", "page_idx": 0}, {"type": "text", "text": "Although these self-improving methods demonstrate considerable improvements on various benchmarks, a systematic understanding of why they work and what is their limitations is still missing. Some analysis of knowledge distillation might bring insights (Mobahi et al. 2020), as learning from data generated by another model is a type of distillation. But precisely analyzing the LLM\u2019s behavior on specific samples becomes increasingly difficult as it grows more complex. Instead, a behaviorallevel analysis might be fruitful, akin to how the Bayesian framework can aid in comprehending the human cognitive system (T. L. Grifftihs et al. 2023). By conceptualizing the LLM as an intelligent agent, we can draw parallels between its behaviors and the cultural evolution observed in humans. Iterated learning (IL), a framework proposed to study the evolution of knowledge and beliefs through a chain of learning among Bayesian agents (Kirby et al. 2007), stands out as a promising candidate for achieving our goals. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we start by introducing the Bayesian-IL framework, demonstrating that agents engaged in such a process gradually amplify bias in their priors. This amplification process can be steered by introducing an interaction phase that \u201cfilters\u201d or \u201cre-ranks\u201d the messages generated by the agents. Next, we theoretically justify that the in-context behavior of LLMs can be approximated by a Bayesian update, establishing a crucial link to the LLM system. To validate our claims, we conduct numerous experiments across different settings. Depending on the beneficial or detrimental nature of the bias, we propose various strategies to guide the evolution of LLM. The key contributions of this work are: 1) establishing the first Bayesian analysis of the full interactive learning process (including an interaction phase); 2) applying this framework to LLM agents and describing their evolution theoretically; 3) validating the theory and demonstrating how to guide LLM\u2019s evolution using experiments. We believe that our analysis can enhance our understanding of LLMs, and aid in designing more effective algorithms for alignment, bias mitigation or amplification, and similar tasks. ", "page_idx": 1}, {"type": "text", "text": "2 Background and Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Iterated Learning ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Iterated learning (IL) is a hypothetical procedure to simulate how the tendency of specific properties of human culture or language gradually emerges and becomes dominant. It is based on studying the behaviors of a chain of intelligent agents. From the perspective of an individual agent, the process involves initially acquiring knowledge from its predecessor (imitation), refining its beliefs while using them to conduct tasks (interaction with the world), and subsequently imparting its knowledge to the agents in the next generation (transmission). Cognitive scientists have applied this framework to explain various evolutionary phenomena of human society, including the emergence of compositionality in human language (Kirby et al. 2015), patterns in human object categorisation (T. L. Grifftihs et al. 2008), and the evolution of color naming systems (Carlsson et al. 2023). The framework has also seen recent success with neural network agents, including in emergent communication (Guo et al. 2019; Ren et al. 2020), machine translation (Y. Lu et al. 2020), visual question answering (Vani et al. 2021), large vision-language models (Chenhao et al. 2024), and representation learning (Ren et al. 2023), indicative that this framework could also be useful for more general deep learning systems, like LLMs. ", "page_idx": 1}, {"type": "text", "text": "2.2 Related On-policy Self-data-augmentation Methods ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "While the theoretical guarantees for the Bayesian-IL framework studied in this paper rely on several assumptions, we posit that the behaviors observed for many recent \u201citerative self-data-augmentation\u201d methods in LLM can be at least partially explained by the theory. We will now overview the basic assumptions, and how they fit with recent LLM approaches (more discussions in Appendix A). ", "page_idx": 1}, {"type": "text", "text": "First, the theory assumes \u201cself-evolution,\u201d where all agents in different generations share the same initial knowledge. Methods like self-refine (Madaan et al. 2023) and hypothesis refinement (Qiu et al. 2024), which require the LLM to refine its output by the feedback from an identical LLM for several rounds, satisfy this assumption. Self-distill (C. Xu et al. 2023) and self-instruct (Y. Wang et al. 2022), if the models involved in different generations are the same, do as well. On the contrary, the super-alignment setting (Burns et al. 2023), where a stronger model is trained using the data generated by another weaker model, do not strictly fit with our analysis. However, if all the models are trained using a similar corpus, so that their initial knowledge should be similar, our analysis might still hold partially. ", "page_idx": 1}, {"type": "text", "text": "The theory also assumes the information transferred among agents is in the form of data examples, as in RAFT (Dong et al. 2023) and ReST (Gulcehre et al. 2023). Both methods consider a multigeneration data-transferring process, during which the bias is introduced by re-ranking the transferred data. Methods like self-reward (Weizhe et al. 2024) and self-refine (Madaan et al. 2023), which requires one agent to evaluate another agent\u2019s response, do not directly fti this assumption. However, if we also consider the evaluation as part of the data generated by the agent, the Bayesian-IL framework can still bring some insights. Furthermore, as analyzed in Ren and Sutherland (2024) that many preference alignment methods like direct preference optimization (DPO, Rafailov et al. (2024)) will naturally amplify the preference hidden in the pretrained model\u2019s prior. Then, those multiplegeneration DPO variants, e.g., iterative DPO (Xiong, Dong, Ye, Z. Wang, et al. 2023), might face a more serious risk of amplifying malicious bias. ", "page_idx": 2}, {"type": "text", "text": "In summary, although the assumptions of our Bayesian-IL framework might not be satisfied by all practical algorithms, the general trends depicted by it, e.g., the bias amplification, the necessity of a good interaction phase, etc., would still hold. Please refer to Appendix A for more discussions. ", "page_idx": 2}, {"type": "text", "text": "3 Bayesian Analysis of Iterated Learning ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Notations and Basic Behaviors of Bayesian Agents ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We denote a data pair as $d=(x,y)$ , where $d\\in\\mathcal{D}=\\mathcal{X}\\times\\mathcal{Y}$ , with $x\\in\\mathscr{X}$ and $y\\in\\mathcal{V}$ . The $(x,y)$ pair can be question and answer in a QA problem, the input and label in a supervised setting, or any type of prompt and output for in-context learning. The hypothesis $h\\in\\mathcal{H}:\\mathcal{X}\\to\\mathcal{Y}$ describes the mapping between all possible $x$ and their corresponding $y$ . Note that $h$ can be either explicit or implicit, depending on the task. For instance, in inductive reasoning, $h$ represents the rule determining the output from input examples and is explicit, as the model can directly generate it using natural language. Conversely, in self-data-augmentation, where $x$ is a topic and $y$ is a paragraph generated based on $x$ , $h$ is likely to be implicit. In this context, $h$ can be highly abstract with varying interpretations, such as the level of conciseness, helpfulness, or even the writer\u2019s preference for using rhyme. ", "page_idx": 2}, {"type": "text", "text": "Consider a general Bayesian agent whose behavior can be depicted by two basic procedures: learning and sampling. Learning involves updating the agent\u2019s knowledge based on observations, while sampling is a procedure wherein the agent generates data based on its knowledge. In this context, the agent\u2019s knowledge is encapsulated by its posterior over the hypotheses, i.e., $\\bar{P_{l m}}(h)$ . ", "page_idx": 2}, {"type": "text", "text": "In Bayesian learning, we assume the agent holds a prior $P_{0}(h)$ at the beginning. Its posterior after observing $\\mathbf{d}=(x_{i},\\bar{y}_{i})_{i=1}^{N}$ is calculated as ", "page_idx": 2}, {"type": "equation", "text": "$$\nP_{l m}(h)=P(h\\mid\\mathbf{d})\\propto p(\\mathbf{d}\\mid h)\\cdot P_{0}(h),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $p(\\mathbf{d}\\mid h)$ is the likelihood of these $N$ data pairs under a specific $h$ ; this is usually hard to calculate in practice. ", "page_idx": 2}, {"type": "text", "text": "Assume the agent holds a posterior $P_{l m}(h)$ during sampling. Then, given the input signal $x$ , we can sample the corresponding $y\\sim P_{l m}(y\\mid x)$ . Based on the fact that $h$ determines the relationship between $x$ and $y$ , the above sampling procedure is equivalent to $\\boldsymbol{y}\\;\\sim\\;\\mathbb{E}_{h\\sim P_{l m}(h)}[{p(y\\;\\mid\\;h,x)}]$ , which can be rewritten as $h\\,\\sim\\,P_{l m}(h)$ ; $y\\ \\mid\\ h\\ \\sim\\ p(y\\ \\mid\\ h,x)$ . Following the definition of $d$ and the assumption that $x$ is uniformly distributed, the sampling procedure above is equivalent to $d\\sim P_{l m}(d)\\propto p(d\\mid h)\\cdot P_{l m}(h)$ . If we instead first decide the most probable $h$ rather than sampling $h$ from the agent (maximum a posteriori (MAP), as is perhaps common subconscious behavior for humans), we then generate $d$ by ", "page_idx": 2}, {"type": "equation", "text": "$$\nd\\sim p(d\\mid h^{*}),\\,\\,\\,h^{*}=\\operatorname*{argmax}_{h\\in\\mathcal{H}}P_{l m}(h).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "3.2 Iterated Learning of Bayesian Agents ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Iterated learning is a hypothetical process simulating how human language gradually evolves to become more efficient when transferred and utilized across generations. Typically, iterated learning repeats of the following three phases, as illustrated in Figure 1: an imitation phase, where an ignorant agent learns from the data generated by its predecessor; an interaction phase, where this agent uses the knowledge to accomplish the task, and hence refine its knowledge; and a transmission phase, where this agent generates useful data for the next generation. Combing with Section 3.1, we can get a picture of how $h$ and $d$ evolve as follows. ", "page_idx": 2}, {"type": "image", "img_path": "BSYn7ah4KX/tmp/5345e3b309c0f80a0870b8f75b9fffec1a571c04afae84938cfb7a351af54581.jpg", "img_caption": ["Figure 1: Examples of practical LLM systems that require knowledge transfer among different generations and how we use Bayesian agents to approximate their behaviors. $\\Phi$ , $\\circleddash$ , and $\\circledast$ denotes the imitation, interaction and transmission phases respectively. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Initialization: at the beginning of the tth generation, a new agentt, whose belief on $h$ follows a prior distribution $P_{0}(h)$ , is initialized. In lab experiments, $P_{0}(h)$ represents the belief of a well-educated participant who has not been previously involved in the target experiment. In in-context learning, a well-trained LLM also holds a complex and informative $P_{0}(h)$ based on the enormous corpus it is trained on and the task instructions in the prompt. ", "page_idx": 3}, {"type": "text", "text": "Imitation phase: after initialization, agentt then updates its knowledge by observing $N$ data samples $\\mathbf{d}^{t-1}$ . Following the above learning procedure, the model\u2019s posterior should be $P(\\bar{h}\\mid{\\bf d}^{t-1})$ . ", "page_idx": 3}, {"type": "text", "text": "Interaction phase: in this phase, the agent will accomplish specific tasks to refine its knowledge. The tasks involved in this phase can be diverse and complex. For example, in lab experiments (Kirby et al. 2015) and emergent communication (Ren et al. 2020), the agent plays a Leiws referential game (Lewis 2008) to rule out hypotheses representing a non-bijection between $\\mathcal{X}$ and $\\boldsymbol{\\wp}$ ; in representation learning (Ren et al. 2023), the agent directly conducts supervised learning on the downstream task to inhibit insufficient representations. Although it is hard to precisely formalize the behavior of the agent under these tasks precisely, their goals are consistent: we expect to \u201crule out\u201d unsuitable hypotheses with carefully designed interactions. In an idealized setting, we might expect the agent\u2019s posterior to become proportional to $\\mathbb{1}(h\\in\\mathcal{H}_{\\mathrm{eff}})P(h\\mid{\\bf d}^{t-1})$ , where $\\mathbb{1}(\\cdot)$ is an indicator function and $\\mathcal{H}_{\\mathrm{eff}}\\subset\\mathcal{H}$ is the subset of hypotheses that can accomplish the tasks. Broadly speaking, refining $h$ or flitering ${\\bf d}^{t}$ using the feedback from humans, LLM, or the environment, which is common in the aforementioned iterative self-data-augmentation methods, is also a type of task implicitly constraining $h\\in\\mathcal{H}_{\\mathrm{eff}}$ . ", "page_idx": 3}, {"type": "text", "text": "Transmission phase: $\\mathbf{agent}_{t}$ now comes to the transmission phase, where it generates multiple data samples $\\mathbf{d}^{t}$ for the next generation. The agent will first select the most probable hypothesis based on its current belief and then generate data samples, i.e., $d_{i}^{t}\\sim p(d\\mid h^{t*})$ , where $h^{t*}=\\operatorname{argmax}_{h\\in\\mathcal{H}_{\\mathrm{eff}}}P(h\\mid$ $\\mathbf{d}^{t-1},$ ). This accomplishes one generation of iterated learning. ", "page_idx": 3}, {"type": "text", "text": "3.3 Amplifying Biases is a Double-Edged Sword ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In iterated learning, we repeat the phases mentioned above to get better $h$ and d. The limiting behavior of this process can be described by the following proposition. In short, the bias in $P_{0}(h)$ is guaranteed to be amplified generation-by-generation. Imposing appropriate $\\mathcal{H}_{\\mathrm{eff}}$ (mainly through a carefully designed interaction phase) can control it. ", "page_idx": 3}, {"type": "text", "text": "Proposition 1. Consider several Bayesian agents sharing the same prior $P_{0}(h)$ are conducting iterated learning for $T$ generations. If $T$ is sufficiently large, any agentt with $t>T$ will have ", "page_idx": 3}, {"type": "equation", "text": "$$\nP_{l m}(h)\\to\\mathbb{1}(h=h^{T*})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $h^{T*}$ is a stationary point (e.g. a local maximum) of $P_{0}(h)$ subject to $h\\in\\mathcal{H}_{e\\!f\\!\\!f}.$ . ", "page_idx": 3}, {"type": "text", "text": "To prove this, we first analyze iterated learning without the interaction phase. By drawing parallels between $\\mathrm{IL}$ and EM (expectation-maximization) algorithms, we can prove that $h^{\\dot{T}*}$ converges to the $h$ with the maximum prior probability. We then consider the interaction phase, which introduces a \u201cselection\u201d pressure to rule out those $\\textit{h}\\not\\in\\,\\mathcal{H}_{\\mathrm{eff}}$ . By proving this process does not break the converging behavior of a non-interacting iterated learning, we achieve this proposition. (Please refer to Appendix B for more details.) ", "page_idx": 3}, {"type": "text", "text": "This proposition describes an inevitable bias amplification procedure as long as the model keeps learning from the data sampled from itself (via Bayesian update, distilling, or imitating, as long as the learning increases the model\u2019s confidence in these samples). However, in practical applications, we should bear in mind that bias amplification is a double-edged sword. If this bias is beneficial, like the simplicity bias in compositional language experiments, $\\mathrm{IL}$ will help the model generate the \u201ccorrect messages\u201d more robustly. Imagine if we only have two possible hypotheses, i.e., $h_{g o o d}$ and $h_{b a d}$ , where $\\bar{P}_{0}(h_{g o o d})$ is slightly larger than $P_{0}(\\bar{h_{b a d}})$ . Then by sampling $y\\sim p(y\\mid h,x)\\cdot P_{0}(h)$ , half of the chance we will get incorrect $y$ coming from $h_{b a d}$ . Although we can get $y=\\operatorname{argmax}_{y}p(y\\mid$ $h,x)\\cdot P_{0}(h)$ by using an extremely small temperature, the diversity of $y$ provided by the likelihood will then disappear, which is not what we expected. How could we get samples that are both diverse and correct? Iterated learning can help with this problem by providing a posterior where $P_{l m}(h_{g o o d})\\gg P_{l m}(h_{b a d})$ . With this posterior, sampling from $p(y\\mid h,x)\\cdot P_{l m}(h)$ would be similar to sampling from $p(y\\mid h_{g o o d},x)$ , which solves our problem. ", "page_idx": 4}, {"type": "text", "text": "Conversely, amplifying bias also negatively influences the system in several ways. Besides the cases where the bias is malicious (it can be solved by designing an appropriate interaction phase where $h_{b a d}\\notin\\mathcal{H}_{\\mathrm{eff}})$ , it also influences the model\u2019s creativity. Imagine we have multiple good $h$ where $P_{0}(h_{g1})>P_{0}(h_{g2})>P_{0}(h_{b a d})$ , then $\\mathrm{IL}$ will let us lose $h_{g2}$ even its prior is only slightly smaller than $h_{g1}$ . Such a mode decay phenomenon is quite similar to the \u201crecursion curse\u201d mentioned in (Shumailov et al. 2023): a more peaky $P_{l m}(h)$ will make those non-dominating $h$ have a smaller probability, hence it is harder to keep these modalities during evolution. Touvron et al. (2023) also mentioned that iteratively fine-tuning would harm the creativity of the model. The solution could be early stopping the iterated learning or manually introducing more $y$ that comes from $h_{g2}$ during imitation. ", "page_idx": 4}, {"type": "text", "text": "In summary, to guide the LLM to self-evolve in an expected direction, we need a good $P_{0}(h)$ , a carefully designed interaction phase, and an appropriate evolving time. ", "page_idx": 4}, {"type": "text", "text": "4 LLM-based Agents in Iterated Learning ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "4.1 LLM Behaves like a Bayesian Agent when Sampling ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To transfer the Bayesian-IL analysis to LLM, we start by showing that the sampling and learning behaviors of an LLM agent can be depicted by Bayesian inference, following a few-shot in-context learning (ICL) scenario demonstrated in (Xie et al. 2022). In this setting, the message feed to the agent would be an instruction prompt $\\mathbf{w}$ followed by $N$ examples, i.e., $\\check{\\mathbf{d}}_{N}=(x_{i},y_{i})_{i=1}^{N}$ . In other words, sampling $y$ given the prompt, the examples, and the question $x_{\\mathrm{{test}}}$ can be represented as: ", "page_idx": 4}, {"type": "equation", "text": "$$\ny\\sim P_{l m}(y\\mid x_{\\mathrm{test}},{\\mathbf{d}}_{N},{\\mathbf{w}})\\triangleq P_{l m w}(y\\mid x_{\\mathrm{test}},{\\mathbf{d}}_{N}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $P_{l m w}$ is the model\u2019s belief conditioned on the instruction $\\mathbf{w}$ . If we call ${\\bf d}_{N}$ as ${\\bf d}^{t-1}$ (i.e., assume the examples are generated by agents in the previous generation) and assume the test question $x_{\\mathrm{{test}}}$ is uniformly distributed, sampling new data based on instruction and few-shot examples can be expressed as $d^{t}\\stackrel{}{\\sim}P_{l m w}(d\\mid{\\bf d}^{t-1})$ , which is similar to the transmission phase in iterated learning. ", "page_idx": 4}, {"type": "text", "text": "Formally linking ICL and Bayesian- $\\mathrm{.IL}$ poses a non-trivial challenge, however, because the theoretical guarantee of Bayesian-IL relies on obtaining the MAP estimate of $h$ at each generation. This is not immediately evident in ICL. Inspired by Xie et al. (2022), we first de-marginalize this posterior predictive distribution using the latent variable $h$ , and then achieve the following proposition: ", "page_idx": 4}, {"type": "text", "text": "Proposition 2. Consider that agent $A$ is conducting in-context learning. If the prompt examples in $\\mathbf{d}^{t-\\bar{1}}$ are generated by another agent $B$ with the same prior knowledge (e.g., they come from the same checkpoint and use the same prompt), sampling from the posterior predictive distribution of agent $A$ , i.e., $d^{t}\\sim P_{l m w}(d\\mid\\mathbf{d}^{t-1})$ , can be decomposed into: 1.) $h^{t*}\\to\\operatorname{argmax}_{h}P_{l m w}(h\\mid\\mathbf{d}^{t-1})$ , and 2.) $d^{t}\\sim P_{l m w}(d\\mid h^{t*}).$ , where $h$ is a hidden variable that describes the mapping between $x$ and $y$ . ", "page_idx": 4}, {"type": "text", "text": "The proof is in Appendix B.3. This proposition bridges LLM and Bayesian agents using its incontext behavior, which we believe is a ubiquitous procedure in any LLM system, irrespective of the subsequent information-updating strategy or the final target. For example, in an LLM-agent system, where no in-weights update exists, the model interacts with other agents (e.g., the human, the internal block of an LLM agent, or the environment) by generating responses based on the prompt and dialog history. For LLM\u2019s finetuning, where various parameter updating strategies exist, the model also generates responses given the prompts, which is well depicted by the in-context behavior. Although the assumptions in this proposition do not exactly hold for all LLM systems, we believe our analysis can still roughly depict important trends of them. Please refer to Appendix A for more discussions. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "4.2 LLMs in Different Algorithms have a Similar Target to Bayesian Learning ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We then check the learning procedure. First, in a pure in-context learning setting like self-instruct (Y. Wang et al. 2022), self-refinement (Madaan et al. 2023), hypothesis search (Qiu et al. 2024), etc., the learning can be modeled by calculating the posterior $\\bar{P_{l m w}}(h\\mid{\\bf d}^{t-1})$ , which is identical to the Bayesian learning discussed previously. Then, for those algorithms that require in-weights updates, like self-reward (Weizhe et al. 2024), self-play instruction tuning ${\\boldsymbol{Z}}$ . Chen et al. 2024), iterative DPO (Xiong, Dong, Ye, Z. Wang, et al. 2023), etc., the LLM might update its $P_{l m w}(h)$ using different loss functions. However, as all of these methods contain a procedure that encourages the models to increase their likelihood of the training samples generated by their predecessors, we should expect $P_{l m w}(\\mathbf{d}^{t-1})$ to be increased after learning. As a result, the equivalent posterior $P_{l m w}(h)$ will implicitly favor those $h$ that can generate $\\mathbf{d}^{t-1}$ , which aligns with the Bayesian learning target. ", "page_idx": 5}, {"type": "text", "text": "5 Experimental Verifications when the Hypothesis is Explicit ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We directly verify our analysis above using an inductive reasoning task called Abstract Causal REasoning (Chi Zhang et al. 2021, ACRE), where all LLM agents update their knowledge via ICL. In this task, the model needs to infer and generate the shared rule by summarizing several input-output pairs. Specifically, assume there are $M$ different objects, say [A,B,C]. One data pair $\\bar{d}=(x,y)$ is composed of an input $x$ , i.e., a list of a subset of these objects, and an output $y$ that represents the status of the light (could be on, off, or undetermined). In this experiment, the existence of a specific object triggers the light to be on. The roles played by different objects are expressed by the rule $h$ . For example, in the learning stage in generation- $\\cdot t$ , the model sees three data pairs $\\mathbf{d}^{t-1}$ : ([B,C],undetermined), ([B],off), and ([A,B,C],on). We then expect the model to guess a rule like $h^{t}=\\{\\mathbb{A}\\colon\\mathsf{o n}$ , B:off, C:undetermined}, which means A can trigger the light to be on, B cannot, and C is not sure. In the sampling stage, we will feed the above $h^{t}$ together with the instructions to the model and hope it generates more examples following $p(\\mathbf{d}\\mid h^{t})$ . Hence the model\u2019s output might be ([A,C],on), ([A,B],on), and ([C],undetermined). Treating the above examples as ${\\bf d}^{t}$ , the model in the next generation can induce the corresponding $h^{t+1}$ by selecting the hypothesis with the largest of $P_{l m w}^{\\ \\ \\leftarrow}(h\\ |\\ \\mathbf d^{t})$ . To ensure the generalizability of our analysis, we conduct experiments on GPT3.5, GPT4, Claude3-haiku, and Mixtral- $\\cdot8\\mathbf{x}7\\mathbf{b}$ . Please refer to Figure 2 and Appendix D.3 for more details. ", "page_idx": 5}, {"type": "text", "text": "5.1 How the Knowledge of LLM Agents Evolves ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Convergence of the posterior. We start from the guarantees mentioned in Proposition 1 under an imitation-only setting. In this experiment, we choose $M=5$ to better illustrate the posterior distribution $P_{l m w}(h)$ (there are $3^{5}\\doteq243$ possible $h$ ). Thanks to the instruction-following ability, all LLMs we considered always return rules in the correct format, where the probabilities of all format-related tokens are almost one. We can then calculate $P_{l m w}(h)$ or $P_{l m w}(h\\mid\\mathbf{d})$ by multiplying the probabilities of specific tokens in their response (see Appendix D.1 for more details). ", "page_idx": 5}, {"type": "text", "text": "We first demonstrate the convergence of $P_{l m w}(h)$ , i.e., $P_{l m w}(h)\\rightarrow\\mathbb{1}(\\cdot)$ , which can be supported by the decreasing of the posterior\u2019s entropy, i.e., $H(P_{l m w}(h))$ . As illustrated in the first panel in Figure 3, $H(P_{l m w}(h))$ gradually decreases to almost zero as iterated learning goes on, which verifies our theory that $P_{l m w}(h)$ will converge to a one-hot-like distribution1. Smaller temperature $\\tau$ makes the convergence faster, which matches our intuitions as well. To better illustrate how different $h$ evolves during iterated learning, similar to what we did for the compositional language experiment in Appendix C.2, we also provide the probability of all possible $h\\in\\mathcal H$ in a similar fashion. Note that for this problem, it is impossible to get the prior distribution $P_{0}(h)$ , because we must give the model at least one example as ${\\bf d}^{0}$ . So in the rightmost two panels in Figure 3, we compare the posterior of the first and the sixth generations and see that the posterior becomes sparser. ", "page_idx": 5}, {"type": "image", "img_path": "BSYn7ah4KX/tmp/b6a3f855bb153cff9d8cefd0e22804f73dcb4bb998009239b346436f79c904bc.jpg", "img_caption": ["Figure 2: Demonstration of conducting iterated ICL on the ACRE task. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Converged $h$ under different likelihood and priors. We then show how iterated learning amplifies specific biases implied in the prior, i.e., $h^{T*}\\stackrel{}{\\rightarrow}\\mathrm{argmax}_{h}P_{0}(h)$ , and how the bias and likelihood influence the converging behavior. Note that $P_{0}(h)$ represents LLM\u2019s belief given the instruction prompt w, where the few-shot examples are not included. Thanks to the phenomenon mentioned in (McCoy et al. 2023), where the confidence of LLM\u2019s prediction is heavily influenced by its degree of familiarity with the output phrases, we can manipulate the prompt to create spurious correlations and hence implicitly control bias in $P_{0}(h)^{2}$ . Specifically, we change the name of the last object from \u201cE\u201d to \u201cscreen\u201d and add a sentence like \u201cTurn off the screen after the experiment.\u201d in the instruction prompt. Then all $h$ with screen:off would have higher prior under this prompt. We use six different prompts to introduce different levels of biases (see Appendix D.2 for more details). ", "page_idx": 6}, {"type": "text", "text": "We then control the strength of the likelihood by selecting different $h^{*}$ , i.e., the ground truth rule we want to recover. For the strong likelihood case, we select $h^{*}$ where four objects are being on while there is only one in the weak likelihood case. The status of screen in both cases is undetermined. Due to the nature of the ACRE task, i.e., the existence of an on-object in the input will trigger the light on, there might be more examples whose outputs are on when the likelihood is strong. Then it is harder for the model to amplify the prior bias that favors the status of screen to be off. Because the likelihood and prior compete with each other during iterated learning, as illustrated by Equation (1). ", "page_idx": 6}, {"type": "text", "text": "This competing relationship can be well depicted by the middle two panels in Figure 3, where we track the probability of $P_{l m w}(\\mathtt{s c r e e n}\\!:\\!\\circ\\!\\mathtt{f f})$ at the end of each generation. The converging speed under different settings correlates with the level of prior bias well. Furthermore, we find it is easier for the bias to be amplified when the likelihood is weaker, as five out of six curves converge to one in the right panel. This trend is more clear in Figure 12 and 13, where curves with the same level of bias are shown together. These results give us a good picture of how the likelihood and prior bias interact with each other during evolution and also verify the correctness of the Bayesian-IL framework for LLM agents. Plus, we plot the histograms of $P_{l m w}(h)$ under weak-likelihood-high-bias case in the rightmost two panels in Figure 3, which also demonstrates the amplified bias (the blue region grows). ", "page_idx": 6}, {"type": "text", "text": "Table 1: Results when adding different interaction phases. Column \"BOTH\" represents the ratio of converged $h^{T}$ who correctly predict all 8 examples in ${\\bf d}^{0}$ and have screen:off (i.e., $r_{20}{=}\\o{\\circ}{\\bf{f}}{\\bf{f}}$ ). The Mixtral model does not have self-refine results, as it violates the instructions too much. ", "page_idx": 6}, {"type": "table", "img_path": "BSYn7ah4KX/tmp/273ef5b05a5dbce9cd74f400f4d223e72c607626d1ce1ecaa8229b87c9e78512.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Influence of the interaction phase and $\\mathcal{H}_{\\mathrm{eff}}$ . Finally, we introduce the interaction phase and show that $h^{T*}\\to\\operatorname{argmax}_{h\\in\\mathcal{H}_{\\mathrm{eff}}}\\bar{P_{0}(h)}$ . Two mechanisms are considered in this experiment: self-refine (Madaan et al. 2023), where the feedback comes from the model\u2019s own response; and hypothesissearch (Qiu et al. 2024), where the feedback comes from an external ground-truth interpreter. We can consider the self-refine as using an imperfect $\\mathcal{H}_{\\mathrm{eff}}$ . In both settings, the LLM refines its proposed $h^{t}$ at the end of each generation by checking and reporting whether this $h^{t}$ can explain all samples in ${\\bf d}^{0}$ (details in Appendix D.3). ", "page_idx": 6}, {"type": "text", "text": "In this experiment, we give the model 8 different examples in ${\\bf d}^{0}$ , where all these examples can be explained by both $h^{*}$ and $\\hat{h}$ . We first select an $\\hat{h}$ from all 162 candidates $(3^{4}\\times2)$ and then create $h^{*}$ by changing the value of screen to off. Under this setting, both $h^{*}$ and $\\hat{h}$ belong to $\\mathcal{H}_{\\mathrm{eff}}$ (i.e., mappings with perfect training accuracy in $\\mathbf{d}^{0}$ ) and $h^{*}$ is what we want our model to converge to. See Table 1, where we run experiments under 10 different $h^{*}$ and report three quantitative metrics of the last iteration, i.e., $h^{T*}$ . We first report the number of correct predictions (mean and standard error) in $\\ensuremath{\\mathbf{d}}^{0}$ , which demonstrates how well the method constrains $h^{T*}\\in\\mathcal{H}_{\\mathrm{eff}}$ . The imitation-only method performs the worst, which warns us if the LLM keeps learning from the corpus generated by the agents in previous generations without any evaluation or flitering, even the training accuracy on given $\\mathbf{d}^{\\bar{0}}$ would be harmed. Because hallucination or incorrectness can aggregate through generations. Adding the interaction phase can mitigate this problem efficiently, which is why most of the related works contain a \u201cdata-selection\u201d or \u201cdata-reranking\u201d phase. The fact that the hypo-search outperforms self-refine indicates the importance of an appropriate $\\mathcal{H}_{\\mathrm{eff}}$ , which means a good reward (or evaluating) model is crucial for these iterated training methods. Another metric is the ratio of $h^{\\dot{T}*}$ with screen:off, which measures how well the bias is amplified (we here assume this bias is beneficial and wish it to be amplified, as the compositionality bias in emergent communication example showed in Appendix C.2). We find all these methods can amplify the bias to some extent and hypo-search also performs the best. Last, combined with the requirement of good training accuracy and amplifying bias, we report the ratio that the algorithm successfully chose $h^{T*}=h^{\\breve{*}}$ . As illustrated in the last column of the table, adding an interaction phase with good $\\mathcal{H}_{\\mathrm{eff}}$ always brings benefits. ", "page_idx": 6}, {"type": "image", "img_path": "BSYn7ah4KX/tmp/e68766fb5e37fca8397c2135239eb9c33aba8a6ea493d6d0df5e93193cad4acb.jpg", "img_caption": ["Figure 3: Left: the mean and standard deviations of $H(P_{l m w}(h))$ of experiments with different $h^{*}$ and $\\ensuremath{\\mathbf{d}}^{0}$ (5 different seeds). Middle two: the probability of screen being off, where different colors represent six different levels of spurious bias. Right: the histogram of all $P_{l m w}(h)$ in the first and sixth generation, where the bars are colored based on the value of the last object in $h$ . "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "In summary, this section verifies the correctness of the proposed analysis in LLM agents when the hypothesis is observable. The results remind us to pay more attention to whether the bias is beneficial or not and to design a better interaction phase as well. ", "page_idx": 7}, {"type": "text", "text": "6 Experimental Verifications when the Hypothesis is Implicit ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Section 5 demonstrates that the Bayesian-IL framework can predict the behavior of LLM agents when $h$ is explicitly defined and utilized when generating new examples. This section considers a hidden $h$ scenario that is more general in most LLM systems. We start from a few-shot self-data-augmentation task, where the LLM keeps generating new examples to augment the data pool. In this process, $h$ is implicitly selected when the few-shot examples are given, as stated in Proposition 2. ", "page_idx": 7}, {"type": "text", "text": "Experimental settings. We choose a scenario where on-policy self-data-augmentation is repeated for several generations. Consider using an LLM to generate multiple examples of an acronymbrainstorm task, where each example $d$ is composed of an acronym and the corresponding word list, e.g., Acronym:IL; List:[\"infinite\",\"loop\"]. The $h$ determines the properties of $d$ . We hold a data pool $\\mathcal{D}_{\\mathrm{pool}}$ , which contains 20 random samples as ${\\bf d}^{0}$ at the beginning of the experiment. In each generation, the model will generate 20 extra examples based on the data generated by itself in the previous round, i.e., $\\mathbf{d}^{t}\\,\\sim\\,P_{l m w}(\\mathbf{d}\\,\\mid\\,\\mathbf{d}^{t-1})$ . The generated ${\\bf d}^{t}$ will be pushed into $\\mathrm{\\mathcal{D}}_{\\mathrm{pool}}$ , which simulates a scenario in which the available data keeps growing when we conduct self-data augmentation. In this experiment, $h$ is hidden and might have different interpretations. We consider that $h$ represents two types of acronyms, i.e., $h_{\\mathrm{easy}}$ , where the acronym is a common word and $h_{\\mathrm{hard}}$ otherwise. As the training data of the LLMs in our experiments is private, we instead use the ranking of the frequency of a word that appeared in common English corpus3 as an approximation. We categorize a word as \u201ceasy\u201d if its ranking is below 60,000; otherwise, we label it as \u201chard\u201d. ", "page_idx": 7}, {"type": "text", "text": "Bias in prior is amplified during IL. Many recent works observe that the LLM prefers to output more common words (i.e., those with higher frequency in the pertaining corpus) (McCoy et al. 2023; ", "page_idx": 7}, {"type": "image", "img_path": "BSYn7ah4KX/tmp/1314cd7ec5ca7f6344d86694848f8f4321e4c3be5fbdbdbfe2c789a665ed9986.jpg", "img_caption": ["Figure 4: Leftmost three: experiments in Section 6. First: how the ratio of easy samples changes in ${\\bf d}^{t}$ . $N_{e}$ is the number of easy examples in ${\\bf d}^{0}$ . Second: how the average ranking of acronyms changes. Third: how the average length of acronyms changes. Rightmost two: results of on-policy DPO in Section 7. Fourth: average length of the responses. Fifth: win rate against the SFT baseline. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Z. Wu et al. 2023), which can be considered as a bias towards $h_{\\mathrm{easy}}$ , i.e., $P_{0}(h_{\\mathrm{easy}})>P_{0}(h_{\\mathrm{hard}}).$ . Since $h$ is hidden and we cannot directly observe it like in the previous experiment, we instead track three quantities: 1.) the proportion of easy samples in all 20 samples for each ${\\bf d}^{t};2.)$ the average ranking of ${\\bf d}^{t}$ , where all hard examples are ranked 60,001; and 3.) the average length of the acronyms for $\\mathbf{d}^{t}$ . As in the leftmost three panels Figure 4, the aforementioned bias is gradually amplified during iterated learning whatever the initial proportion of the easy samples in $\\mathbf{d}^{\\bar{0}}$ is. ", "page_idx": 8}, {"type": "text", "text": "Interaction phase when $h$ is hidden. As $h$ is inaccessible, which forbid us to directly apply hyposearch or self-refine, we instead add a fliter on the transmitted data across different generations, which plays a similar role as the interaction phase. Specifically, we use a sampled $\\hat{\\mathbf{d}}\\sim\\mathcal{D}_{\\mathrm{pool}}(d\\mid h\\in\\mathcal{H}_{\\mathrm{eff}})$ to replace original $\\mathbf{d}^{t-1}$ in the \u201cimitation-only\u201d setting. Based on how we sample $\\hat{\\bf d}.$ , different constraints on $\\mathcal{H}_{\\mathrm{eff}}$ are implicitly imposed. We compare the behavior of five different settings, they are: 1.) $\\mathcal{H}_{r a n d o m}$ , where $\\hat{\\bf d}$ is randomly sampled from $\\mathrm{\\Phi_{pool}}$ ; 2.) $\\mathcal{H}_{h a r d}$ where only hard examples can be sampled; 3.) $\\mathcal{H}_{e a s y}$ , opposite to the hard setting; 4.) $\\mathcal{H}_{e a s y l o n g}$ , where the easy acronyms with longer lengths are more likely to be sampled; 5.) $\\mathcal{H}_{e a s y s h o r t}$ , opposite to the easy-long setting. ", "page_idx": 8}, {"type": "text", "text": "See the first several columns of Table 2 that show the ratio of easy examples in $\\mathbf{d}^{t}$ . Compared with the random setting, all methods expect $\\mathcal{H}_{\\mathrm{hard}}$ finally converges to $\\check{\\mathbf{d}^{t}}$ with more easy examples, which means the bias towards easier acronyms would be amplified when $\\mathcal{H}_{\\mathrm{eff}}$ doesn\u2019t impede it. On the contrary, using $\\mathcal{H}_{\\mathrm{hard}}$ successfully restrain this bias, as the average number of easy samples in ${\\bf d}^{t}$ is even lower than that in $\\ensuremath{\\mathbf{d}}^{0}$ . We can also design composite $\\mathcal{H}_{\\mathrm{eff}}$ by choosing two properties of the data. For example, $\\mathcal{H}_{e a s y l o n g}$ restrains the samples with hard and short outputs, which is why they have more easy but long examples in their $\\mathbf{d}^{t}$ . ", "page_idx": 8}, {"type": "text", "text": "In summary, this experiment verifies that the Bayesian-IL framework still works when $h$ is hidden: the bias is amplified generation by generation, implicitly imposing $\\mathcal{H}_{\\mathrm{eff}}$ can still guide the evolution direction. Please also refer to Appendix E for more results and discussions. ", "page_idx": 8}, {"type": "text", "text": "Table 2: Results when adding different $\\mathcal{H}_{\\mathrm{eff}}$ . We color the highest and lowest numbers in each column.   \n$N_{e}$ is the number of easy examples in ${\\bf d}^{0}$ . Results under different settings are in Table 4 and 5. ", "page_idx": 8}, {"type": "table", "img_path": "BSYn7ah4KX/tmp/6852d7fc8aa35d30cca856d7ec131cd624590d1e35840c399d237955b30c036f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "7 Experiments on In-Weights Learning: On-Policy DPO as an Example ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Besides the manually designed experiments in the previous two sections, here we verify our analysis in a real preference-tuning task using on-policy DPO (Guo et al. 2024). In each round of the training, the model first samples multiple responses given the prompt (similar to the sampling stage in IL). Then, these responses are evaluated and ranked by another LLM annotator based on their level of helpfulness (the interaction phase in IL). Finally, we select the highest (lowest) ranked samples as the chosen (rejected) response and use a standard DPO algorithm (Rafailov et al. 2024) to train the policy network (the imitation phase in IL). As described before, each update of the on-policy DPO algorithm can be considered as one generation in iterated learning, because the model keeps updating its parameters using the responses generated by itself. Ranking the responses based on helpfulness is equivalent to imposing a $\\mathcal{H}_{\\mathrm{helpful}}$ . As a result, the phenomenon of bias amplification, the guiding effect of the interaction phase design, and the influence of spurious correlation, should still hold in this practical setting. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "To verify our analysis, we finetune a pretrained llama-2-7B model (Touvron et al. 2023) using Antropic-HH dataset (Bai et al. 2022) following the on-policy DPO recipe provided in (Guo et al. 2024). We study the length bias demonstrated in (Dubois et al. 2024), which means the LLM tends to prefer longer responses when answering questions. We first show that such a bias will be significantly amplified by a multi-generation self-improvement method (on-policy DPO) compared with a nonself-iterated method (RLHF, (Ouyang et al. 2022)). As demonstrated by the blue curve and the dotted line of the fourth panel in Figure 4, the average length of the responses from the model trained using on-policy DPO is much larger than the SFT baseline and RLHF counterparts. With the increase of the win rate against $\\mathrm{SFT^{4}}$ , the average response length also keeps increasing. To restrain this bias, we impose $\\mathcal{H}_{\\mathrm{short}}$ by adding a sentence like \u201cyou are a laconic agent and prefer concise answers\u201d to the annotator LLM, just like how we manipulate the spurious correlation between screen and off in Section 5. Then, combining with the existing interaction phase that requires $h\\in{\\mathcal{H}}_{\\mathrm{helpful}}$ , this design is equivalently imposing a constraint of $h\\in\\mathcal{H}_{\\mathrm{helpful}}\\cap\\mathcal{H}_{\\mathrm{short}}$ . Hence as illustrated by the orange curves in the last two panels, the on-policy DPO can then generate shorter responses (the increasing speed is also restrained) while keeping a high level of helpfulness. However, if our constraints of the length are too strong, which makes $\\mathcal{H}_{\\mathrm{helpful}}\\cap\\mathcal{H}_{\\mathrm{veryshort}}=\\Phi$ , the model\u2019s helpfulness will then be significantly harmed, as demonstrated by the pink curves in these two panels. ", "page_idx": 9}, {"type": "text", "text": "In summary, we find all our analysis on the Bayesian-IL still holds for a practical preference-tuning task: the biases would be amplified and a suitable interaction phase can control it as long as we can figure out them. However, some biases are inevitably hidden and are also amplified during LLM\u2019s evolution. Hence how to pinpoint these biases, or finding a method that can restrain malicious biases even without explicitly knowing them, would be interesting directions to explore in the future. ", "page_idx": 9}, {"type": "text", "text": "8 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper examines the potential and ongoing evolutions of LLM agents by drawing parallels with human cultural evolution, where the latter is a well-established subject in cognitive science. By demonstrating that the sampling and learning procedures of LLMs in various algorithms can be effectively approximated by Bayesian inference, we successfully apply the Bayesian-IL framework to elucidate and steer the evolution of LLM agents. The presented theory and accompanying experiments not only provide deeper insights into LLM behavior from a top-down perspective but also hold the potential to inspire the design of more efficient self-evolution algorithms. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported in part by the Natural Sciences and Engineering Resource Council of Canada, the Fonds de Recherche du Qu\u00e9bec - Nature et technologies (under grant ALLRP-57708- 2022), the Canada CIFAR AI Chairs program, the BC DRI Group, Calcul Qu\u00e9bec, Compute Ontario, and the Digital Resource Alliance of Canada ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Walter R Gilks, Sylvia Richardson, and David Spiegelhalter (1995). Markov chain Monte Carlo in practice. CRC Press.   \nS\u00f8ren Feodor Nielsen (2000). \u201cThe stochastic EM algorithm: estimation and asymptotic results.\u201d Bernoulli, pages 457\u2013489.   \nAad W Van der Vaart (2000). Asymptotic statistics. Volume 3. Cambridge university press.   \nSimon Kirby, Mike Dowman, and Thomas L Grifftihs (2007). \u201cInnateness and culture in the evolution of language.\u201d Proceedings of the National Academy of Sciences 104.12, pages 5241\u20135245.   \nMark Davies (2008). \u201cThe Corpus of Contemporary American English.\u201d URL: www.englishcorpora.org/coca/.   \nThomas L Griffiths, Brian R Christian, and Michael L Kalish (2008). \u201cUsing category structures to test iterated learning as a method for identifying inductive biases.\u201d Cognitive Science 32.1, pages 68\u2013107.   \nSimon Kirby, Hannah Cornish, and Kenny Smith (2008). \u201cCumulative cultural evolution in the laboratory: An experimental approach to the origins of structure in human language.\u201d PNAS 105.31, pages 10681\u201310686.   \nDavid Lewis (2008). Convention: A philosophical study. John Wiley & Sons.   \nAaron Beppu and Thomas Grifftihs (2009). \u201cIterated learning and the cultural ratchet.\u201d Proceedings of the Annual Meeting of the Cognitive Science Society. Volume 31.   \nFlorencia Reali and Thomas L Griffiths (2009). \u201cThe evolution of frequency distributions: Relating regularization to inductive biases through iterated learning.\u201d Cognition 111.3, pages 317\u2013328.   \nNicolas Fay, Simon Garrod, Leo Roberts, and Nik Swoboda (2010). \u201cThe interactive evolution of human communication systems.\u201d Cognitive science 34.3, pages 351\u2013386.   \nAmy Perfors, Joshua B Tenenbaum, Thomas L Griffiths, and Fei Xu (2011). \u201cA tutorial introduction to Bayesian models of cognitive development.\u201d Cognition 120.3, pages 302\u2013321.   \nElizabeth Bonawitz, Stephanie Denison, Thomas L Grifftihs, and Alison Gopnik (2014). \u201cProbabilistic models, learning algorithms, and response variability: sampling in cognitive development.\u201d Trends in cognitive sciences 18.10, pages 497\u2013500.   \nVanessa Ferdinand, Simon Kirby, and Kenny Smith (2014). \u201cRegularization in language evolution: On the joint contribution of domain-specific biases and domain-general frequency learning.\u201d Evolution of Language: Proceedings of the 10th International Conference (EVOLANG10). World Scientific, pages 435\u2013436.   \nSimon Kirby, Monica Tamariz, Hannah Cornish, and Kenny Smith (2015). \u201cCompression and communication in the cultural evolution of linguistic structure.\u201d Cognition 141, pages 87\u2013102.   \nDanielle J Navarro, Andrew Perfors, Arthur Kary, Scott D Brown, and Chris Donkin (2018). \u201cWhen extremists win: Cultural transmission via iterated learning when populations are heterogeneous.\u201d Cognitive Science 42.7, pages 2108\u20132149.   \nVanessa Ferdinand, Simon Kirby, and Kenny Smith (2019). \u201cThe cognitive roots of regularization in language.\u201d Cognition 184, pages 53\u201368.   \nShangmin Guo, Yi Ren, Serhii Havrylov, Stella Frank, Ivan Titov, and Kenny Smith (2019). \u201cThe emergence of compositional languages for numeric concepts through iterated learning in neural agents.\u201d The 3rd workshop on Emergent Communication, NeurIPS.   \nYasamin Motamedi, Marieke Schouwstra, Kenny Smith, Jennifer Culbertson, and Simon Kirby (2019). \u201cEvolving artificial sign languages in the lab: From improvised gesture to systematic sign.\u201d Cognition 192, page 103964.   \nYuchen Lu, Soumye Singhal, Florian Strub, Aaron Courville, and Olivier Pietquin (2020). \u201cCountering language drift with seeded iterated learning.\u201d International Conference on Machine Learning. PMLR, pages 6437\u20136447.   \nHossein Mobahi, Mehrdad Farajtabar, and Peter Bartlett (2020). \u201cSelf-distillation amplifies regularization in Hilbert space.\u201d Advances in Neural Information Processing Systems 33, pages 3351\u2013 3361.   \nYi Ren, Shangmin Guo, Matthieu Labeau, Shay B. Cohen, and Simon Kirby (2020). \u201cCompositional languages emerge in a neural iterated learning model.\u201d International Conference on Learning Representations.   \nAnkit Vani, Max Schwarzer, Yuchen Lu, Eeshan Dhekane, and Aaron Courville (2021). \u201cIterated learning for emergent systematicity in vqa.\u201d International Conference on Learning Representations.   \nChi Zhang, Baoxiong Jia, Mark Edmonds, Song-Chun Zhu, and Yixin Zhu (2021). \u201cAcre: Abstract causal reasoning beyond covariation.\u201d Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10643\u201310653.   \nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. (2022). \u201cTraining a helpful and harmless assistant with reinforcement learning from human feedback.\u201d arXiv: 2204.05862.   \nYasamin Motamedi, Lucie Wolters, Danielle Naegeli, Simon Kirby, and Marieke Schouwstra (2022). \u201cFrom improvisation to learning: How naturalness and systematicity shape language evolution.\u201d Cognition 228, page 105206.   \nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. (2022). \u201cTraining language models to follow instructions with human feedback.\u201d Advances in Neural Information Processing Systems 35, pages 27730\u201327744.   \nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi (2022). \u201cSelf-instruct: Aligning language model with self generated instructions.\u201d arXiv: 2212.10560.   \nSang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma (2022). \u201cAn Explanation of In-context Learning as Implicit Bayesian Inference.\u201d International Conference on Learning Representations.   \nCollin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen Baker, Leo Gao, Leopold Aschenbrenner, Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan Leike, et al. (2023). \u201cWeak-to-strong generalization: Eliciting strong capabilities with weak supervision.\u201d arXiv: 2312.09390.   \nEmil Carlsson, Devdatt Dubhashi, and Terry Regier (2023). \u201cIterated learning and communication jointly explain efficient color naming systems.\u201d arXiv: 2305.10154.   \nHanze Dong, Wei Xiong, Deepanshu Goyal, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, and Tong Zhang (2023). \u201cRaft: Reward ranked finetuning for generative foundation model alignment.\u201d arXiv: 2304.06767.   \nThomas L Grifftihs, Jian-Qiao Zhu, Erin Grant, and R Thomas McCoy (2023). \u201cBayes in the age of intelligent machines.\u201d arXiv: 2311.10206.   \nCaglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, et al. (2023). \u201cReinforced self-training (rest) for language modeling.\u201d arXiv: 2308.08998.   \nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. (2023). \u201cSelf-refine: Iterative refinement with self-feedback.\u201d arXiv: 2303.17651.   \nR Thomas McCoy, Shunyu Yao, Dan Friedman, Matthew Hardy, and Thomas L Griffiths (2023). \u201cEmbers of autoregression: Understanding large language models through the problem they are trained to solve.\u201d arXiv: 2309.13638.   \nYi Ren, Samuel Lavoie, Mikhail Galkin, Danica J. Sutherland, and Aaron Courville (2023). \u201cImproving Systematic Generalization using Iterated Learning and Simplicial Embeddings.\u201d Thirty-seventh Conference on Neural Information Processing Systems.   \nIlia Shumailov, Zakhar Shumaylov, Yiren Zhao, Yarin Gal, Nicolas Papernot, and Ross Anderson (2023). \u201cModel Dementia: Generated Data Makes Models Forget.\u201d arXiv: 2305.17493.   \nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. (2023). \u201cLlama 2: Open foundation and fine-tuned chat models.\u201d arXiv: 2307.09288.   \nZhaofeng Wu, Linlu Qiu, Alexis Ross, Ekin Aky\u00fcrek, Boyuan Chen, Bailin Wang, Najoung Kim, Jacob Andreas, and Yoon Kim (2023). \u201cReasoning or reciting? exploring the capabilities and limitations of language models through counterfactual tasks.\u201d arXiv: 2307.02477.   \nWei Xiong, Hanze Dong, Chenlu Ye, Ziqi Wang, Han Zhong, Heng Ji, Nan Jiang, and Tong Zhang (2023). \u201cIterative preference learning from human feedback: Bridging theory and practice for rlhf under kl-constraint.\u201d ICLR 2024 Workshop on Mathematical and Empirical Understanding of Foundation Models.   \nWei Xiong, Hanze Dong, Chenlu Ye, Han Zhong, Nan Jiang, and Tong Zhang (2023). \u201cGibbs sampling from human feedback: A provable kl-constrained framework for rlhf.\u201d arXiv: 2312.11456.   \nCanwen Xu, Daya Guo, Nan Duan, and Julian McAuley (2023). \u201cBaize: An open-source chat model with parameter-efficient tuning on self-chat data.\u201d arXiv: 2304.01196.   \nZixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu (2024). \u201cSelf-play finetuning converts weak language models to strong language models.\u201d arXiv: 2401.01335.   \nZheng Chenhao, Zhang Jieyu, Kembhavi Aniruddha, and Krishna Ranjay (2024). \u201cIterated Learning Improves Compositionality in Large Vision-Language Models.\u201d arXiv: 2402.04792.   \nYann Dubois, Bal\u00e1zs Galambosi, Percy Liang, and Tatsunori B Hashimoto (2024). \u201cLength-controlled alpacaeval: A simple way to debias automatic evaluators.\u201d arXiv: 2404.04475.   \nShangmin Guo, Biao Zhang, Tianlin Liu, Tianqi Liu, Misha Khalman, Felipe Llinares, Alexandre Rame, Thomas Mesnard, Yao Zhao, Bilal Piot, et al. (2024). \u201cDirect language model alignment from online ai feedback.\u201d arXiv: 2402.04792.   \nLinlu Qiu, Liwei Jiang, Ximing Lu, Melanie Sclar, Valentina Pyatkin, Chandra Bhagavatula, Bailin Wang, Yoon Kim, Yejin Choi, Nouha Dziri, et al. (2024). \u201cPhenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement.\u201d International Conference on Learning Representations.   \nRafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn (2024). \u201cDirect preference optimization: Your language model is secretly a reward model.\u201d Advances in Neural Information Processing Systems 36.   \nYi Ren and Danica J Sutherland (2024). \u201cLearning Dynamics of LLM Finetuning.\u201d arXiv: 2407. 10490.   \nYuan Weizhe, Pang Richard Yuanzhe, Cho Kyunghyun, Sukhbaatar Sainbayar, Xu Jing, and Weston Jason (2024). \u201cSelf-Rewarding Language Models.\u201d arXiv preprint arXiv:2401.10020.   \nWenda Xu, Guanglei Zhu, Xuandong Zhao, Liangming Pan, Lei Li, and William Yang Wang (2024). \u201cPerils of Self-Feedback: Self-Bias Amplifies in Large Language Models.\u201d arXiv: 2402.11436. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Discussions of the Proposed Theory and its Applicability to Real Methods ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Assumptions of Bayesian-IL and practical Scenarios ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "As typical in theoretical machine learning research, some assumptions are needed to prove results about models\u2019 behavior; these assumptions are often not exactly satisfied by practical algorithms. So, we elaborate here on the important assumptions we made and when practical algorithms break them. ", "page_idx": 13}, {"type": "text", "text": "1. Assumptions for the theoretical analysis. To derive the guarantees of Proposition 1, we first model the interaction phase as a binary fliter on $h\\in\\mathcal{H}_{\\mathrm{eff}}$ and also assume a shared prior $P_{0}(h)$ among all agents involved in Bayesian-IL. We also model the LLM\u2019s in-context behavior as a Bayesian agent and assume the number of samples during the imitation phase is sufficient. ", "page_idx": 13}, {"type": "text", "text": "2. Assumptions we can break for iterative ICL experiments. ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "\u2022 Binary filter on $h\\,\\in\\,\\mathcal{H}_{\\mathrm{eff}}$ . All our LLM experiments break this assumption (the pure Bayesian example in Appendix C.2 does not). For example, in the ACRE experiments, we use self-refine and hypothesis search as the interaction phase. Self-refine asks the model to evaluate the responses, and the hypothesis search uses an external interpreter: they both manipulate $h$ by feeding messages to the LLM, rather than a binary filter. (When using an external interpreter, $h$ is usually filtered before forming the refinement feedback.) For the experiments in Section 6 and 7, where $h$ is implicit, we re-rank all the generated samples in $\\mathrm{\\Phi_{pool}}$ and take a weighted sample during imitation, similar to re-ranking the generated examples in ReST. Since all these interaction designs are commonly applied in the community, and our theory describes their qualitative behaviors well despite strictly violating the assumption, we believe our methods can shed more light on other practical methods with similar designs, like self-reward (Weizhe et al. 2024), iterative-DPO (Xiong, Dong, Ye, Z. Wang, et al. 2023), etc.   \n\u2022 Identical $P_{0}(h)$ for agents in different generations. Although this assumption makes it easier to derive Proposition 1, slightly relaxing it will not change the whole story: we only require different agents to share a similar tendency towards a specific bias. To verify this, we conduct several experiments when the agents in different generations are different LLMs (e.g., GPT3.5 plays with Claude3 in Appendix D and E). The phenomena claimed by the theory still hold.   \n\u2022 The Bayesian learning assumption, i.e., $P_{l m}(h)=P(h\\mid\\mathbf{d})\\propto p(\\mathbf{d}\\mid h)P_{0}(h)$ . Although this assumption is necessary for drawing a parallel between iterated learning and the EM algorithm and hence getting a guarantee for the amplified bias, the practical in-weights learning (IWL for short) method usually does not strictly follow this assumption, because people usually early stop the training before the model perfectly learns all ${\\bf d}^{t}$ . However, results in Section 7 match our analysis well, which means the iterated IWL can also be depicted by iterated learning to some extent. That is because although there are plenty of finetuning methods with different targets or loss functions, their aims are consistent: increasing the likelihood of $p(\\mathbf{d}_{\\mathrm{train}}\\mid h)$ under instructions, which aligns with Bayesian targets well. Furthermore, we find the increased bias or decreased creativity during iterated finetuning has also been extensively mentioned in many related works (Touvron et al. 2023; W. Xu et al. 2024), which also supports our analysis. ", "page_idx": 13}, {"type": "text", "text": "A.2 Why we Start from Two \u201dToyish\u201d Tasks ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The experimental settings in Section 5 and 6 are relatively manual and toyish. The main reason for us to start from them is that we want to directly observe some quantitive numbers described by the theory, which we believe would provide stronger support for the analysis. For the explicit $h$ case, we chose the ACRE task because of its simple $\\mathcal{H}$ , making it possible to observe the distribution and entropy of all possible $h$ . We believe observing the model\u2019s logits supports our theory more directly than merely observing the accuracy or other quantitative metrics. ", "page_idx": 13}, {"type": "text", "text": "For the implicit $h$ case, we chose the acronym task, which is a prototype of self-data-augmentation in self-instruct. We initially consider the conditional creative writing task (quite common in many related works), where the model needs to write a passage (i.e., the list in our settings) based on several topic words (i.e., the acronym). However, constrained by the context length of LLMs, we can\u2019t generate more than 4 examples in one generation, which makes it hard to calculate the statistics of ${\\bf d}^{t}$ . Remember the model will generate 20 extra ${\\bf d}^{t}$ based on $20\\;\\mathbf{d}^{t-1}$ in our acronym experiments. In summary, although the experiments studied in our paper look artificial, they are reasonable approximations of real tasks. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Last, in a concurrent work W. Xu et al. (2024), the authors study practical applications like machine translation, creative writing, math reasoning, etc, in an iterative ICL setting. Their observations match our theoretical analysis quite well: bias is amplified generation by generation, and introducing external feedback can mitigate it. However, due to the complexity of the tasks they considered, they can only observe the average bias and the skew level using several conclusive quantitative metrics. Hence we believe that by combining our theoretical analysis, detailed observations on artificial examples, and the evidence from real applications in W. Xu et al. (2024), one can draw a good overview of how LLM would evolve in an iterated ICL setting. ", "page_idx": 14}, {"type": "text", "text": "A.3 How our analysis brings benefits to practical algorithms ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Besides the method proposed in Section 7, where we manipulate the instructions prompt of the annotator LLM during the interaction phase, our experiments and analysis also provide the following potential approaches to guide the model\u2019s evolution: ", "page_idx": 14}, {"type": "text", "text": "\u2022 Select ${\\bf d}^{0}$ that makes more confident and correct predictions on the target task. Manually selecting good in-context examples is intuitive. Our analysis, though, suggests taking the model\u2019s confidence (i.e., the logits) into account, because the theory claims that the likelihood and bias in prior are competing with each other during evolution. From Figure 3, we see the model evolves faster if the likelihood of ${\\bf d}^{0}\\mid h^{*}$ is weaker. The results in Figure 4 also provide similar insights: the related biases are amplified slower when the number of easy samples in ${\\bf d}^{0}$ decreases.   \n\u2022 Designing a good interaction phase is important: more accurate $\\mathcal{H}_{\\mathrm{eff}}$ leads to better performance. This can be supported by comparing self-refine and hypothesis-search in our paper. The paper W. Xu et al. (2024) also claims that external feedback with more accurate assessments or feedback from a larger model can reduce the amplified bias.   \n\u2022 Manipulating the instruction prompt: in our analysis, both $P_{0}(h)$ and $P_{l m}(h)$ are the model\u2019s predictions conditioned on the instruction prompt $\\mathbf{w}$ . Hence adding preference in the task instruction (or changing the system prompt) during evolution could be an effective way of guiding the model\u2019s evolution. Our ACRE experiments show the feasibility of this: remember we can introduce spurious correlation by adding one sentence to the instruction. Hence it is also possible to guide the model\u2019s evolution by feeding appropriate prompts during learning and sampling.   \n\u2022 Manipulating the temperature: Bayesian-IL theory studies the evolution of the distribution, so the temperature should also be an important factor for the evolution, as illustrated in Figure 3. We left the exploration between temperature and different phases in $\\mathrm{IL}$ in the future. ", "page_idx": 14}, {"type": "text", "text": "B Proofs related to Bayesian Agents ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Recap the Proof of Expectation-Maximization Algorithm ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "To get a clear picture of the asymptotic behavior of imitation-only iterated learning, we first recap how a typical expectation-maximization (EM) algorithm converges when the target function is posterior distribution5. Consider a statistical model that generates a set of observable data samples $\\{{\\bf x}_{i}\\}_{i=1}^{m}$ and the corresponding hidden variables $\\{{\\mathbf{z}}_{i}\\}_{i=1}^{m}$ . The generating mechanism can be expressed as $P(\\mathbf{x}\\mid\\mathbf{z},\\theta)$ , where $\\theta$ is a set of unknown parameters determining this distribution. To get a ", "page_idx": 14}, {"type": "image", "img_path": "BSYn7ah4KX/tmp/740257611c459c7da62db6a5b8f3ac22c4b40ba54c4fc4d5fee0673aff5d9de1.jpg", "img_caption": ["Figure 5: Illustrations of typical EM algorithm and an imitation-only iterated learning method. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "MAP(maximum a posterior) estimation of $\\theta$ , we need to optimize the following target function: ", "page_idx": 15}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{{\\mathcal{L}}(\\theta)=\\log P(\\theta\\mid\\mathbf{x}_{1},\\ldots,\\mathbf{x}_{m})}\\\\ &{\\qquad=\\log P_{0}(\\theta)+\\log P(\\mathbf{x}_{1},\\ldots,\\mathbf{x}_{m}\\mid\\theta)-\\log P(\\mathbf{x}_{1},\\ldots,\\mathbf{x}_{m})}\\\\ &{\\qquad=\\displaystyle\\sum_{i=1}^{m}{\\frac{1}{m}}\\log P_{0}(\\theta)+\\sum_{i=1}^{m}\\log P(\\mathbf{x}_{i}\\mid\\theta)-\\log P(\\mathbf{x}_{1},\\ldots,\\mathbf{x}_{m})}\\\\ &{\\qquad=\\displaystyle\\sum_{i=1}^{m}\\log\\left(P(\\mathbf{x}_{i}\\mid\\theta)P_{0}^{\\frac{1}{m}}(\\theta)\\right)-\\log P(\\mathbf{x}_{1},\\ldots,\\mathbf{x}_{m})}\\end{array}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $P_{0}(\\theta)$ is the prior distribution of parameters. As the marginal distribution $P(\\mathbf{x}_{i}\\mid\\theta)$ is hard to calculate due to the existence of the hidden variable $\\mathbf{z}_{i}$ , our target function can then be expressed as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\tilde{\\mathcal{L}}(\\theta)=\\mathcal{L}(\\theta)+\\mathrm{const}=\\sum_{i=1}^{m}\\log\\left(\\sum_{z_{i}}P(\\mathbf{x}_{i},\\mathbf{z}_{i}\\mid\\theta)P_{0}^{\\frac{1}{m}}(\\theta)\\right)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the term $\\log P(\\pmb{\\mathscr{x}}_{1},...\\,,\\pmb{\\mathscr{x}}_{m})$ is eliminated as it doesn\u2019t depend on $\\theta$ . The target function above is still hard to tackle due to the summation inside the logarithmic function. To solve this, we first introduce an auxiliary function $Q_{i}(\\mathbf{z}_{i})$ , which is a probability distribution over $\\mathbf{z}_{i}$ , and reformulate the target as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\tilde{C}(\\theta)=\\sum_{i=1}^{m}\\log\\left(\\sum_{z_{i}}Q_{i}(z_{i})\\frac{P(\\mathbf{x}_{i},\\mathbf{z}_{i}\\mid\\theta)P_{0}^{\\frac{1}{m}}(\\theta)}{Q_{i}(\\mathbf{z}_{i})}\\right)=\\sum_{i=1}^{m}\\log\\left(\\mathbb{E}_{\\mathbf{z}_{i}\\sim Q_{i}}\\left[\\frac{P(\\mathbf{x}_{i},\\mathbf{z}_{i}\\mid\\theta)P_{0}^{\\frac{1}{m}}(\\theta)}{Q_{i}(\\mathbf{z}_{i})}\\right]\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Given the concavity of the logarithmic function, we can use Jensen\u2019s inequality to get a lower bound of $\\tilde{\\mathcal{L}}(\\theta)$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\tilde{\\mathcal{L}}(\\theta)\\geq\\mathcal{I}(\\theta,Q)=\\sum_{i=1}^{m}\\mathbb{E}_{\\mathbf{z}_{i}\\sim Q_{i}}\\left[\\log\\frac{P(\\mathbf{x}_{i},\\mathbf{z}_{i}\\mid\\theta)P_{0}^{\\frac{1}{m}}(\\theta)}{Q_{i}(\\mathbf{z}_{i})}\\right].\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The EM algorithm then maximizes this lower bound by alternatively optimizing $\\theta$ and $Q_{i}$ for several rounds. ", "page_idx": 15}, {"type": "text", "text": "In the $\\mathbf{E}$ -step, as illustrated in Figure 5, we use the estimated $\\theta^{t-1}$ in the previous round to find $Q_{i}^{*}=\\operatorname{argmax}_{Q}\\mathcal{J}(\\theta^{t-1},Q)$ . Specifically, we need to push this lower bound to be tight by making the equality in Equation (7) hold. Following the properties of Jensen\u2019s inequality, the equality only holds when $\\frac{P(\\mathbf{x}_{i},\\mathbf{z}_{i}|\\theta^{t-1})P_{0}^{\\frac{1}{m}}(\\theta)}{Q_{i}(\\mathbf{z}_{i})}$ is a constant. Combining this requirement and the fact that $\\begin{array}{r}{\\sum_{\\mathbf{z}_{i}}Q_{i}(\\mathbf{z}_{i})=1,}\\end{array}$ we can calculate the optimal $Q_{i}^{*}(\\mathbf{z}_{i})=P(\\mathbf{z}_{i}\\mid\\mathbf{x}_{i},\\theta^{t-1}),$ , which is the posterior distribution of $\\mathbf{z}_{i}$ given the observable data $\\pmb{x}_{i}$ and the fixed parameters $\\theta^{t-1}$ . ", "page_idx": 15}, {"type": "text", "text": "In the M-step, we plug in the estimated $Q_{i}^{*}$ to ${\\mathcal{I}}(\\theta,Q)$ to get the target function as: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{I}(\\theta;Q^{*})=\\sum_{i=1}^{m}\\mathbb{E}_{\\mathbf{z}_{i}\\sim Q_{i}^{*}}\\left[\\log\\frac{P(\\mathbf{x}_{i},\\mathbf{z}_{i}\\mid\\theta)P_{0}^{\\frac{1}{m}}(\\theta)}{Q_{i}^{*}}\\right]=\\sum_{i=1}^{m}\\mathbb{E}_{\\mathbf{z}_{i}\\sim Q_{i}^{*}}\\left[\\log\\left(P(\\mathbf{x}_{i},\\mathbf{z}_{i}\\mid\\theta)P_{0}^{\\frac{1}{m}}(\\theta)\\right)\\right]-c,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $c=\\mathbb{E}_{\\mathbf{z}_{i}\\sim Q_{i}^{*}}[Q_{i}^{*}]$ is a constant term and can be neglected while optimizing $\\theta$ . In this step, we can calculate $\\theta^{t}=\\operatorname{argmax}_{\\theta}\\mathcal{I}(\\theta;Q^{*})$ using gradient descent or other parameter estimation methods. In summary, the E-step ensures a tight lower bound ${\\mathcal{I}}(\\theta,Q)$ and the M-step finds better $\\theta$ to make it larger. The two steps cooperate to ensure a series of estimations of $\\theta$ for which ${\\mathcal{L}}(\\theta)$ is non-decreasing. Finally, the estimation of parameters will converge to the one that maximizes the posterior distribution, i.e., $\\operatorname{\\ddot{E}}(\\theta^{*})=\\operatorname{argmax}_{\\theta}\\,P(\\theta\\mid\\times_{1},\\ldots,\\times_{m})$ , if it is convex. ", "page_idx": 16}, {"type": "text", "text": "B.2 Proof: Convergence Behavior of Bayesian Agents in Iterated Learning ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proposition 1. Consider several Bayesian agents sharing the same prior $P_{0}(h)$ are conducting iterated learning for $T$ generations. If $T$ is sufficiently large, any agentt with $t>T$ will have ", "page_idx": 16}, {"type": "equation", "text": "$$\nP_{l m}(h)\\to\\mathbb{1}(h=h^{T*})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $h^{T*}$ is a stationary point (e.g. a local maximum) of $P_{0}(h)$ subject to $h\\in\\mathcal{H}_{e\\!f\\!\\!f}.$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. The proof of this proposition can be divided into two steps. In the first step, we show that imitation-only iterated learning shares similar convergence behavior with a standard EM algorithm. In the second step, we show the \u201cselecting\u201d pressure introduced via the interaction phase doesn\u2019t break the necessary conditions of the convergence in the first step. Merging these two steps leads to the proposition. ", "page_idx": 16}, {"type": "text", "text": "Step 1: imitation-only iterated learning as a special EM ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Recall the imitation-only iterated learning illustrated in the bottom part in Figure 5, where the hypothesis held by the agent in the $(t-1)$ -th generation is represented by $h^{t-1}$ . With this hypothesis, the agent will generate $m$ data samples using $P(d\\mid h^{t-1})$ , denoted $\\mathbf{d}^{t-1}\\triangleq[d_{1}^{t-1},\\cdot\\cdot\\cdot,d_{m}^{t-1}]$ . In the $t$ -th generation, a new agent will first update its posterior probability using $P(h|\\mathbf{d}^{t-1})$ , and then select $h^{t}$ by picking the one with the largest posterior. As there are multiple data samples in ${\\bf d}^{t-1}$ , this process can be expressed as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\hbar^{t}=\\operatorname{argmax}\\log P(\\hbar\\mid d_{1}^{t-1},d_{2}^{t-1},\\dots,d_{m}^{t-1})}\\\\ {=\\ }&{\\operatorname{argmax}\\log\\left(\\frac{p(d_{1}^{t-1},d_{2}^{t-1},\\dots,q_{m}^{t-1}\\mid h)P_{0}(h)}{P(d_{1}^{t-1},d_{2}^{t-1},\\dots,d_{m}^{t-1})}\\right)}\\\\ {=\\ }&{\\operatorname{argmax}\\log\\left(P_{0}(h)\\prod_{i=1}^{m}{p(d_{i}^{t-1}\\mid h)}\\right)}\\\\ {=\\ }&{\\operatorname{argmax}\\frac{1}{n}\\log P_{0}(h)+\\frac{1}{m}\\sum_{i=1}^{m}\\log(d_{i}^{t-1}\\mid h)}\\\\ {\\approx\\operatorname{argmax}\\frac{\\prod}{m}\\log P_{0}(h)+\\frac{1}{m}\\sum_{i=1}^{m}\\log(d_{i}^{t-1}\\mid h)}\\\\ {\\approx\\operatorname{argmax}_{h}\\ \\frac{\\mathbb{E}[q]}{\\alpha\\cdot p(d_{1}^{t-1})}[\\log P_{0}^{\\frac{1}{m}}(h)]+\\frac{1}{2\\cdot e\\eta(d_{1}^{t}\\mid h-1)}[\\log p(d\\mid h)]}\\\\ {=\\ }&{\\operatorname{argmax}_{h\\in[0,t-1]}\\log p(d\\mid h)P_{0}^{\\frac{1}{m}}(h).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Based on the analysis above, we notice that the imitation-only iterated learning and the EM algorithm are almost identical: by replacing $\\theta$ and ${\\sf z}$ to $h$ and ${\\bf d}$ , and by removing the random variable $\\pmb{x}^{6}$ , we can also have similar theoretical guarantees for the imitation-only iterated learning algorithm. ", "page_idx": 16}, {"type": "text", "text": "To prove this, we can first verify the equivalence between the imitation phase and an M-step. By comparing the target functions when calculating hidden variables $h$ and $\\theta$ ) in these two algorithms, i.e., Equation (8) and (10), we can find two major differences. First, the expectation of observable samples (i.e., $\\mathbb{E}_{\\mathbf{x}_{i}}$ ) disappears in Equation (10), as we assume there are no \u201cobservations\u201d in iterated learning. We can also introduce a dummy variable named $\\pmb{x}$ to the iterated learning process, and find that the existence of $\\pmb{x}$ doesn\u2019t influence the aforementioned calculation at all. Second, in IL, we can only approximate $\\mathbb{E}_{d_{i}^{t-1}}[\\cdot]$ by sampling $d$ from $p(d\\,|\\,h^{t-1})$ , while in EM, the posterior distribution $P(\\mathbf{z}_{i}\\mid\\mathbf{x}_{i},\\theta^{t})$ is usually analytically calculated. Of this discrepancy, if our d is a good approximation of $p(d\\,|\\,h^{t-1})$ , the imitation phase in $\\mathrm{IL}$ is equivalent to an M-step in EM. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "We then verify whether the transmission phase is a good approximation of an E-step. The first thing to check is the tightness of the lower bound generated via Jensen\u2019s inequality, which guarantees the non-decreasing update of the target function across multiple generations. we can first assume the target function of the whole iterated learning process is $\\bar{\\mathcal{L}}(h\\bar{)}=\\log P_{0}(h)$ , and derives its lower bound $\\mathcal{I}(h;Q)$ following a similar procedure in EM: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}(h)=\\log P_{0}(h)}\\\\ &{\\phantom{\\hat{\\mathcal{L}}(h)}=m\\log P_{0}^{\\frac{1}{m}}(h)}\\\\ &{\\phantom{\\hat{\\mathcal{L}}(h)}=m\\log\\left(\\sum_{i}p(d_{i}\\ |\\ h)P_{0}^{\\frac{1}{m}}(h)\\right)}\\\\ &{\\phantom{\\hat{\\mathcal{L}}(h)}=m\\log\\left(\\sum_{i}q_{i}(d_{i}\\ |\\ h)P_{0}^{\\frac{1}{m}}(h)P_{0}^{\\frac{1}{m}}(h)\\right)}\\\\ &{\\phantom{\\hat{\\mathcal{L}}(h)}=m\\log\\left(\\sum_{i}q_{i}(d_{i})\\frac{p(d_{i}\\ |\\ h)P_{0}^{\\frac{1}{m}}(h)}{Q_{4}(d_{i})}\\right)}\\\\ &{\\phantom{\\hat{\\mathcal{L}}(h)}=m\\log\\mathbb{E}_{d_{i}\\sim\\mathcal{Q}_{i}}\\left[\\frac{p(d_{i}\\ |\\ h)P_{0}^{\\frac{1}{m}}(h)}{Q_{4}(d_{i})}\\right]}\\\\ &{\\phantom{\\hat{\\mathcal{L}}(h)}\\ge\\mathbb{E}_{d_{i}\\sim\\mathcal{Q}_{i}}\\log\\left[\\frac{p(d_{i}\\ |\\ h)P_{0}^{\\frac{1}{m}}(h)}{Q_{4}(d_{i})}\\right]\\triangleq\\mathcal{I}(h,Q)}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The equality of Jensen\u2019s inequality holds when $\\frac{p(d_{i}|h)P_{0}^{1/m}(h)}{Q_{i}(d_{i})}$ is a constant. Using the fact that $\\textstyle\\sum_{d_{i}}Q_{i}(d_{i})\\;=\\;1$ , we can have the optimal $Q_{i}^{*}\\;=\\;p(d_{i}\\;\\mid\\;h)$ . In summary, as we sample each $d_{i}^{t-1}\\sim p(d\\mid h^{t-1})$ , the transmission phase in $\\mathrm{IL}$ is equivalent to an E-step in EM. ", "page_idx": 17}, {"type": "text", "text": "Another interesting parameter to discuss is $m$ , i.e., the number of data samples generated by an agent in each generation. The choice of $m$ determines how well the sampled d can represent the ground truth $p(d\\mid h)$ . For large enough $m$ , we can prove the convergence guarantee using the above procedure. When $m=1$ , the standard EM algorithm becomes a stochastic EM approximation Gilks et al. 1995. The authors of Nielsen (2000) proved that in stochastic EM, $\\theta$ in different generations form a homogeneous Markov chain whose stationary distribution over hypotheses is approximately centered on the maximum-likelihood solution. In other words, when $t>T$ for sufficiently large $T,\\mathbb{E}[\\theta^{t}]$ optimizes $\\mathbb{E}_{\\mathbf{x}}[P(\\boldsymbol{\\theta}\\mid\\mathbf{x}_{1},...\\,,\\mathbf{x}_{m})]$ , and similarly, $\\mathbb{E}[h^{t}]$ is the optimizer of $P_{0}(h)$ . In other words, the dominating hypothesis in imitation-only iterated learning converges to the one with the highest prior, which is equivalent to the large $m$ case. ", "page_idx": 17}, {"type": "text", "text": "Although $m\\,=\\,1$ doesn\u2019t influence the converged estimation of $h$ , a too small $m$ will make the variance of estimation large, and hence impede the converging speed of $h$ . Then should we choose $m$ as large as possible? The answer is still no: too large $m$ will also impede the converging speed. We can get some intuition by observing Equation (10), where $P_{0}^{1/m}(h)$ determines the effect of the prior when selecting optimal $h$ in each generation. If $m$ is too large, this distribution would be very flat and the preference encoded in the prior cannot influence the choice of $h$ in this generation much \u2013 the likelihood term $P(d\\mid h)$ will dominate.7 Hence the resulting $h^{t}$ would be quite close to $h^{t-1}$ , which means the evolution of belief on $h$ would be slow. ", "page_idx": 17}, {"type": "text", "text": "Actually, the choice of $m$ is usually considered as the \u201cbottleneck\u201d parameter in different iterated learning algorithms. Almost all the related studies point out that the bottleneck should not be too wide or too tight (like experiments in Kirby et al. (2015) and Ren et al. (2020)). Our Bayesian analysis provides a theoretical explanation of the effect. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "Step 2: the influence of introducing $\\mathcal{H}_{\\mathrm{eff}}$ . Let us first check the imitation phase, i.e., the E-step where $h^{t}$ is calculated in Equation (9). Assume we have a perfect interaction phase that can rule our all $h\\not\\in\\mathcal{H}_{\\mathrm{eff}}$ , then the target function is: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{h^{t}=\\underset{h\\in\\mathcal{H}_{\\mathrm{eff}}}{\\mathrm{argmax}}\\log P(h\\mid d_{1}^{t-1},d_{2}^{t-1},\\ldots,d_{m}^{t-1})}\\\\ &{\\quad=\\underset{h}{\\mathrm{argmax}}\\log P(h\\mid d_{1}^{t-1},d_{2}^{t-1},\\ldots,d_{m}^{t-1})\\cdot\\mathbb{1}(h\\in\\mathcal{H}_{\\mathrm{eff}})}\\\\ &{\\quad=\\underset{h}{\\mathrm{argmax}}\\underset{d\\sim p(d\\mid h^{t-1})}{\\mathbb{E}}\\left[\\log p(d\\mid h)\\cdot\\left(P_{0}^{\\frac{1}{m}}(h)\\cdot\\mathbb{1}(h\\in\\mathcal{H}_{\\mathrm{eff}})\\right)\\right]}\\\\ &{\\quad\\triangleq\\underset{h}{\\mathrm{argmax}}\\underset{d\\sim p(d\\mid h^{t-1})}{\\mathbb{E}}\\left[\\log p(d\\mid h)\\cdot\\tilde{P}_{0}^{\\frac{1}{m}}(h)\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where we define a \u201cregularized\u201d prior as $\\begin{array}{r}{\\tilde{P}_{0}(h)\\triangleq c\\cdot P_{0}(h)\\cdot\\mathbb{1}(h\\in\\mathcal{H}_{\\mathrm{eff}}).}\\end{array}$ . Then, by substituting this prior back to $\\mathcal{L}(h)$ defined in Equation (11), we find the optimal $Q_{i}^{*}=p(d_{i}\\mid h)$ still holds. In other words, as long as we same $d_{i}^{t-1}\\sim p(d\\mid h^{t-1})$ , where $h^{t-1}\\in\\mathcal{H}_{\\mathrm{eff}}$ , all the conditions required for this proposition still hold. Furthermore, this proof also provides us an insight that adding constraints on $h\\in\\mathcal{H}_{\\mathrm{eff}}$ is required in both imitation and transmission phases. Hence having a powerful \u201cdata-fliter\u201d or \u201cdata-ranking\u201d design for the transmission phase would also make the evolution more robust, like those applied in RAFT (Dong et al. 2023) and ReST (Gulcehre et al. 2023). \u53e3 ", "page_idx": 18}, {"type": "text", "text": "B.3 Proof: LLM as a Bayesian Agent ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proposition 2. Consider that agent $A$ is conducting in-context learning. If the prompt examples in $\\mathbf{d}^{t-\\bar{1}}$ are generated by another agent $B$ with the same prior knowledge (e.g., they come from the same checkpoint and use the same prompt), sampling from the posterior predictive distribution of agent $A$ , i.e., $d^{t}\\sim P_{l m w}(d\\mid\\mathbf{d}^{t-1})$ , can be decomposed into: 1.) $h^{t*}\\to\\operatorname{argmax}_{h}P_{l m w}(h\\mid\\mathbf{d}^{t-1})$ , and 2.) $d^{t}\\sim P_{l m w}(d\\mid h^{t*}).$ , where $h$ is a hidden variable that describes the mapping between $x$ and $y$ . ", "page_idx": 18}, {"type": "text", "text": "Proof. The proof of this proposition can be divided into two steps. In the first part, we de-marginalize the posterior predictive distribution on a hidden variable $h$ , and then show that the model automatically \u201cselects\u201d a hypothesis $h^{t*}$ that generates the prompting examples. In the second part, we show that when the examples in ${\\bf d}^{t-1}$ are generated by another LLM with the same prior belief over $h$ , the MAP (maximize a posterior) estimation of $h$ can approximate $h^{t*}$ well. ", "page_idx": 18}, {"type": "text", "text": "Step 1: In our paper, we assume the query and answer sequences in each example, i.e., $d_{i}=(x_{i},y_{i})$ , are controlled by the hidden hypothesis $h$ , which plays a similar role to the \u201cconcept\u201d parameter $\\theta$ mentioned in Xie et al. $(2022)^{8}$ . Then the posterior predictive distribution can be decomposed as: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P(d\\mid\\mathbf d^{t-1})=\\displaystyle\\int_{h}p(d\\mid\\mathbf d^{t-1},h)P(h\\mid\\mathbf d^{t-1})\\,\\mathrm{d}h}\\\\ &{\\qquad\\qquad=\\displaystyle\\int_{h}p(d\\mid h)P(h\\mid\\mathbf d^{t-1})\\,\\mathrm{d}h}\\\\ &{\\qquad\\qquad\\propto\\displaystyle\\int_{h}p(d\\mid h)p(\\mathbf d^{t-1}\\mid h)P_{0}(h)\\,\\mathrm{d}h}\\\\ &{\\qquad\\quad\\propto\\displaystyle\\int_{h}p(d\\mid h)\\frac{p(\\mathbf d^{t-1}\\mid h)}{p(\\mathbf d^{t-1}\\mid h^{t})}P_{0}(h)\\,\\mathrm{d}h}\\\\ &{\\qquad\\qquad=\\displaystyle\\int_{h}p(d\\mid h)\\exp(n\\cdot r_{n}(h))P_{0}(h)\\,\\mathrm{d}h}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\begin{array}{r}{r_{n}(h)\\triangleq\\frac{1}{n}\\log\\frac{p(\\mathbf{d}^{t-1}|h)}{p(\\mathbf{d}^{t-1}|h^{t*})}}\\end{array}$ . In Equation (13), the second line follows the Markov property of hypothesis and data samples, the third line follows the Bayesian rule and drops a constant term, the fourth line is generated by dividing a constant $p(\\mathbf{d}^{t-1}\\mid h^{t*})$ , where $h^{t*}$ is a hypothesis that generates $\\mathbf{d}^{t-1}$ . Now we see the $r_{n}(h)$ has almost the same form as $r_{n}(\\theta)$ in Xie et al. (2022). By reusing the derivation in that paper (mainly in Section 3.2 and the proof of its Theorem 1), we can conclude that $\\exp(n\\cdot r_{n}(h))\\to0$ for any $h\\neq h^{t*}$ and $\\exp(n\\cdot r_{n}(\\Bar{h^{t*}}))\\to1$ . Hence Equation (13) becomes: ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "equation", "text": "$$\nP(d\\mid{\\bf d}^{t-1})=P(d\\mid h^{t*}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Step 2: In a general in-context learning setting, the prompting examples ${\\bf d}^{t-1}$ are usually sampled from an unknown distribution $P_{\\mathrm{prompt}}$ . For example, by analyzing different $x_{\\mathrm{{test}}}$ , the researchers can manually design effective prompting examples sharing similar chain-of-thought structures with the target question. Obviously, such $P_{\\mathrm{prompt}}$ is impossible to parameterize or analyze accurately. To approximate it, Xie et al. (2022) first assumes that both the prompting examples and the pretraining corpus are natural languages, and a well-trained LLM can approximate this \u201cnatural language distribution\u201d well. Then, $P_{\\mathrm{prompt}}$ can be well approximated by $P(\\cdot\\mid\\theta^{*})$ under some $\\theta^{*}$ , which is the prompt concept in that paper. ", "page_idx": 19}, {"type": "text", "text": "In our settings, as we assume the prompting examples ${\\bf d}^{t-1}$ are generated by another agent-B sharing the same prior, then the $h^{t*}$ triggered by feeding $\\mathbf{\\bar{d}}^{t-1}$ to agent-A would be exactly the same as that feeding that to agent-B, i.e., ", "page_idx": 19}, {"type": "equation", "text": "$$\nh^{t\\ast}=\\operatorname*{argmax}_{h}P_{l m w}(h\\mid{\\bf d}^{t-1}),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $P_{l m w}$ is model\u2019s belief after receiving the common instruction $\\mathbf{w}$ . ", "page_idx": 19}, {"type": "text", "text": "Combining these two steps, we can decompose the sampling procedure $d^{t}\\sim P(d\\mid\\mathbf{d}^{t-1})$ into two parts: first, inherently select $h^{t*}$ based on observations generated by another agent in the previous generation; then sample new $d$ conditioned on this $h^{t*}$ , which matches the Bayesian-IL procedure discussed in this paper. \u53e3 ", "page_idx": 19}, {"type": "text", "text": "C Experiments of Iterated Learning on Different Domains ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "To provide a panorama of iterated learning and its applications in different fields, this appendix will first give an intuitive explanation using some lab experimental results. Then, experiments on the emergence of compositional language among Bayesian agents are introduced to verify all the theoretical hypotheses. ", "page_idx": 19}, {"type": "text", "text": "C.1 Iterated Learning in Lab Experiments using Lewis Language Game ", "text_level": 1, "page_idx": 19}, {"type": "image", "img_path": "BSYn7ah4KX/tmp/f3d2cbc4373d1db0e5c475405116616de0120cfb441467fde3c61f1cb383a168.jpg", "img_caption": ["Figure 6: The lab experiments (algorithm, settings, and results) conducted in Kirby et al. 2015. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "As denoted in Kirby et al. 2008, iterated learning is a process where one individual learns by observing the output of another individual, who learned in the same way. In this multiple-generation learning procedure, the shared language (i.e., $h$ ) among learning agents will gradually become more systematic under the compressibility pressure (imposed during imitation, embodied in $P_{0}(h))$ and expressivity pressure (imposed during interaction, requiring $h\\in\\mathcal{H}_{\\mathrm{eff}})$ ). To simulate this process, authors of Kirby et al. 2015 design a two-phases learning procedure illustrated in Figure 6. In the interaction phase, the speaker (Alice) and listener (Bob) must cooperate to accomplish a Leiws referential game (see the middle panel in Figure 6). Specifically, Alice will first create a name for the given objects and talk that to Bob. After receiving this message, Bob needs to select the correct object shown to Alice among some candidates. If correct, both of them are rewarded. This phase terminates when they can achieve a high enough success rate. To succeed in this game, the shared language should be expressive enough to avoid any ambiguities \u2013 we expect the language to be a bijection. Then, we select another new naive candidate (Alice[t+1]) and let it learn the naming system created by Alice[t] and Bob[t]. In this phase, those highly structural mappings should be easier for a human to remember, which is how the compressibility pressure is imposed. After that, Alice[t+1] will play the same game with another Bob[t+1] and the interaction phase starts again. We provide the languages generated by Alice[0] and Alice[10] in the right panel in Figure 6: it is clear that an interesting structure emerges in the language generated by Alice[10]. There are also plenty of similar lab experiments that support the \u201ctwo pressures\u201d and cultural evolution hypothesis using IL-like training methods, e.g., Fay et al. 2010; Ferdinand et al. 2019; Motamedi et al. 2019; Motamedi et al. 2022. Although these methods have different types of input, game designs, learning procedures, vocabularies (like gesture language), etc., the conclusion of them is quite consistent: compressibility and expressivity pressures are crucial for the emergence of systematic mappings, iteratively learning and interacting can amplify these pressures a lot, which matches the Bayesian explanations well. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "C.2 Iterated Learning of Bayesian Agents (re-implementation of results in Kirby et al. (2015)) ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "To verify that the emergence of systematic mappings in iterated learning is not an accident, authors of Beppu and T. Grifftihs 2009 provide a guarantee by analyzing the behavior of Bayesian agents. There are also plenty of related works in cognitive science, like Reali and T. L. Griffiths 2009; Perfors et al. 2011; Bonawitz et al. 2014; Ferdinand et al. 2014; Navarro et al. 2018, discussing the influence of and theories behind iterated learning and Bayesian analysis. ", "page_idx": 20}, {"type": "text", "text": "To give the readers a better understanding of how iterated learning works, we re-implement the Bayesian experiments mentioned in Kirby et al. 2015. Consider the following toy example, where we have four different input objects: $\\mathcal{X}=\\{\\mathtt{b l u e\\ \\ c i r}\\$ cle, blue box, red circle, red box}, and four possible names: $\\mathcal{V}=\\{00\\,,01\\,,10\\,,11\\}$ . The hypothesis $h$ is defined as $h\\in\\mathcal{H}:\\mathcal{X}\\to\\mathcal{Y}$ . In this example, we have $|\\mathcal{H}|\\,=\\,256$ , which means $P(h)$ can be parameterized by a categorical distribution with 256 dimensions. In this analysis, we assume the prior distribution of a mapping is negatively correlated with its coding length $\\alpha$ , i.e., $P(h;\\alpha,c)\\propto\\mathbf{\\dot{2}}^{-\\frac{\\alpha}{c}}$ , where $c$ is a normalizing constant to make sure the prior distribution is not too peaky. Usually, the easier-to-learn mappings (i.e., more systematical ones) have higher prior. In Table 3, we demonstrate how to calculate the coding length for the three typical mappings. Note that the mapping that has the highest prior is a degenerate mapping, where $\\alpha=18$ and $P(h)\\approx0.6$ . The $P_{0}(h)$ for all possible mappings are demonstrated in Figure 7. ", "page_idx": 20}, {"type": "table", "img_path": "BSYn7ah4KX/tmp/3b40784511b8e173d59aef1a133c5357f7fd69625ed3758fd4bd57120816fb9d.jpg", "table_caption": ["Table 3: An example of coding the mappings, where $\\alpha$ is how many characters (including space and unique symbol, e.g., $\\rightarrow$ and :) are used to express the grammar. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "In this experiment, the knowledge of an agent is encoded in its posterior distribution, i.e., $P_{l m}(h)$ . We will observe how this distribution evolves when the agents conduct iterated learning. Recall the sampling behaviors discussed in Section 3. To get a data sample $d=(x,y)$ , we first randomly sample $x\\in\\mathscr{X}$ from a uniform distribution and then sample $y$ based on the given $x$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\nd\\sim P_{l m}(x,y)\\propto P_{l m}(y\\mid x)\\propto p(y\\mid h,x)\\cdot P_{l m}(h).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "image", "img_path": "BSYn7ah4KX/tmp/22cf572e394a817485a358eddd7188623371dc796b461c34bbd01604468ab26b.jpg", "img_caption": ["Figure 7: The prior probability of all possible $h\\in\\mathcal H$ . The systematic mappings are sandwiched between degenerate and holistic mappings, which means $P_{0}(h_{d e g e n})\\:>\\:P_{0}(h_{s y s})\\:>\\:P_{0}(h_{h o l i})$ . Some mappings in the \u201cother\u201d group also have relatively large prior, because they contain degenerate components (e.g., mapping two or three objects to the same message). "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "The likelihood $p(\\boldsymbol{y}\\mid\\boldsymbol{h},\\boldsymbol{x})$ is defined as: ", "page_idx": 21}, {"type": "equation", "text": "$$\np(y\\mid h,x)=\\left\\{\\frac{1-\\epsilon}{|\\frac{\\epsilon}{|\\mathcal{V}|-1}}\\right.\\qquad\\mathrm{otherwise},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\epsilon$ is a small positive value describing the systematic error during communication. ", "page_idx": 21}, {"type": "text", "text": "For the learning behavior, the agent will update the posterior based on the received data samples d = (xi, yi)iN=1: ", "page_idx": 21}, {"type": "equation", "text": "$$\nP_{l m}(h)=P(h\\mid\\mathsf{d})\\propto p(\\mathsf{d}\\mid h)\\cdot P_{0}(h)\\propto P_{0}(h)\\cdot\\prod_{i=1}^{N}p(y_{i}\\mid h,x_{i}).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Now, with the definition of learning and sampling for these Bayesian agents, we can describe how they conduct IL: ", "page_idx": 21}, {"type": "text", "text": "\u2022 Initialization: at the beginning of the $t$ -th generation, a new agent is initialized by $P_{0}(h)$   \n\u2022 Imitation: agent- $\\cdot t$ learns from $\\mathbf{d}^{t-1}$ , which is generated by agent in the previous generation following Equation (18)   \n\u2022 Interaction: to impose expressivity pressure, we let agent- $\\boldsymbol{\\cdot}$ (Alice) play a communication game in this phase. Specifically, we first create another agent Bob by copying $P_{l m}(h)$ from Alice. Then, Alice samples a data pair $d=(x,y)$ on a randomly chosen $x$ and sends it to Bob. Bob will estimate the object based on $y$ . If the estimated $x^{\\prime}=x$ , the game succeeds and data pair $(x,y)$ is added to a buffer named $\\mathbf{d}_{\\mathrm{comm}}$ . After several rounds, Alice updates its knowledge by learning from $\\mathbf{d}_{\\mathrm{comm}}$ . Note that in this phase, the pressure of $h\\in\\mathcal{H}_{\\mathrm{eff}}$ is induced implicitly: for the ambiguous $h$ , where multiple $x$ are mapped to the same $y$ , Bob\u2019s reconstruction $x^{\\prime}$ might not equal $x$ with high probability. Hence $\\mathbf{d}_{\\mathrm{comm}}$ will finally dominated by the samples generated by those $h\\in\\mathcal{H}_{\\mathrm{eff}}$ .   \n\u2022 Transmission: after the interaction phase, Alice will generate multiple samples ${\\bf d}^{t}$ for the next generation. ", "page_idx": 21}, {"type": "text", "text": "In the above procedure, the compressibility pressure is embodied in the prior distribution where more reusable principles lead to a higher probability, which aligns with the simplicity bias in the human cognition system. The expressivity pressure is imposed in the communication game because $\\mathbf{d}_{\\mathrm{comm}}$ only contains the unambiguous mappings. Under this setting, we can calculate the weighted proportion (i.e., the summation of the posteriors) of different types of languages and observe how they evolve during iterated learning, as illustrated in Figure 8. It is clear that the systematic mappings gradually dominate as the learning progresses. ", "page_idx": 21}, {"type": "text", "text": "To further verify our theory, we consider a compressibility-only case by removing the interaction phase, and an expressivity-only case using a uniform prior $P(l)=1/256$ . The results in Figure 8 match the theory quite well: ", "page_idx": 21}, {"type": "image", "img_path": "BSYn7ah4KX/tmp/a37136af90e7ba8e8270d3191f175ab3d5c70d8910a3b41907952ac3056a78a2.jpg", "img_caption": ["Figure 8: Ratio of three different types of mappings during iterated learning (curves are the average of 15 different runs, shadow region is the variance). Left to right: 1.) ${\\bf d}^{0}$ is a holistic mapping; 2.) $\\ensuremath{\\mathbf{d}}^{0}$ is a degenerate mapping; 3.) Starting from a holistic ${\\bf d}^{0}$ , but no longer conduct an interaction phase during training. Hence the degenerate language, which has the highest prior, will gradually dominate; 4.) Ablating the compressibility pressure by using a uniform prior distribution. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "BSYn7ah4KX/tmp/f89d9457ab762458eebc1ba5c399ab16922e8c81baaf7e428dc331da322bef56.jpg", "img_caption": ["Figure 9: The posterior probabilities of all at the end of different generations. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "\u2022 In an imitation-only iterated learning case, i.e., the third panel in Figure 8, $h^{T}$ converges to a degenerate mapping, which has the highest prior as illustrated in Figure 7;   \n\u2022 Introducing the interaction phase will rule out those ambiguous mappings (i.e., those $h\\not\\in\\mathcal{H}_{\\mathrm{eff}})$ , and hence $h^{T}$ converges to systematic mapping, which has the highest prior among all $h\\in\\mathcal{H}_{\\mathrm{eff}}$ ;   \n\u2022 By comparing the first and second panels in Figure 8, $h^{T}$ always converges to systematic mapping no matter $\\ensuremath{\\mathbf{d}}^{0}$ is holistic or degenerate. ", "page_idx": 22}, {"type": "text", "text": "Furthermore, we can directly observe the dynamics of $P_{l m}(h)$ from Figure 9, which provides a more detailed illustration of how the posterior of all mappings changes during training. In the first generation, we see the dominant mapping is a holistic one, which is our ${\\bf d}^{0}$ . Then gradually, under the two pressures, the posterior of systematic mappings gradually increases and finally dominates. ", "page_idx": 22}, {"type": "text", "text": "D More on GPT-based ACRE Experiments ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "D.1 How to Calculate the Model\u2019s Posterior on All Hypotheses (Figure 10) ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Thanks to the instruction-following ability, the GPT can always provide responses following the given format, as illustrated in Figure 10. The experiments demonstrated in this part come from OpenAI\u2019s playground. The model we use is gpt-3.5-turbo-instruct. The temperature is 0.1 and the probability feedback is enabled. We let the model return probabilities of the top 5 candidate tokens for each token in the response, as illustrated by the three sub-panels in the figure. Then, the posterior of specific $h$ can be calculated by multiplying the probability of all tokens with corresponding values. For example, $P_{l m w}(h=\\{\\mathbf{A}:\\mathsf{o n}$ , B:und, C:off, $\\mathsf{D}\\colon\\cdot\\cdot\\}$ ) can be calculated by $P(r_{5}={\\mathsf{o n}})\\cdot P(r_{9}={\\mathsf{u n d}})\\cdot P(r_{13}={\\mathsf{o f f}}\\,)\\cdot\\cdot\\cdot$ , where $r_{5,9,13,\\dots}$ are the tokens denoting the corresponding values of A, B, C. To further show the feasibility of this approach, we conduct the following two verifications. First, we calculate i\u2208Iformat and find that this value is always close to one $\\mathcal{T}_{\\mathrm{format}}$ denotes the indexes of those format-related tokens, e.g., Rule, $\\{,:,\\mathtt{A},\\mathtt{B},\\mathtt{C},$ , etc). This means the model reliably follows the instructions when generating responses. Second, we calculate $P_{l m w}(h)$ for all possible 243 different $h$ and verified that $\\textstyle\\sum_{h\\in{\\mathcal{H}}}P_{l m w}(h)$ is always close to one. ", "page_idx": 23}, {"type": "image", "img_path": "BSYn7ah4KX/tmp/ab42cb49d80d3203cc77eea7e637353c5e2707178b8b549b937c5777c5625ef7.jpg", "img_caption": ["Figure 10: How GPT provides the rule following the given format, which makes it possible to calculate $P_{l m w}(h)$ for all $h$ . "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "D.2 How to Control the Bias in the Prior ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We demonstrate how to manipulate bias in the prior $P_{0}(h)$ in the ACRE task by adding spurious correlations in the prompt and by changing the name of the object. The prompt we use is almost the same as that in Figure 10. We will analyze the value of $P(r_{20})$ , i.e., the probability of the token denoting the status of the last object in the rule. The ${\\bf d}^{0}$ in these experiments are all the same (i.e., $[\\mathtt{B},\\mathtt{D}]\\rightarrow\\mathtt{o n}$ ; [B, $\\mathrm{C}]\\rightarrow\\mathrm{und}$ ; [B,D,E] $\\rightarrow\\,\\circ\\mathbf{n}$ , as stated in the last several panels in Figure 11). Some subtle modifications under different settings will be described one by one in the following: ", "page_idx": 23}, {"type": "text", "text": "$\\textsuperscript{\\textregistered}$ : the default setting, where the object list is [A,B,C,D,E]. The $\\begin{array}{r l}{P_{0}(r_{20}}&{{}=}\\end{array}$ [on,off,und]) are $[11.\\bar{8}\\%,\\,6.46\\%,\\,81.68\\%]$ . This makes sense, as all these 3 statuses of E can describe all examples in ${\\bf d}^{0}$   \n$\\Phi$ : compared with $\\mathbb{O}$ , we change the object E to screen, and no extra text is added to the prompt. Then, as the screen is likely to be turned on during the experiment, $P_{0}(r_{20}=\\mathsf{o n})$ dominates the prediction;   \n$\\circleddash$ : compared with $\\Phi$ , we add a sentence \u201cTurn off the screen after the experiment\u201d to the task instruction. This misleading sentence introduces a bias towards screen:off by creating a spurious correlation;   \n$\\textcircled{3}$ : compared with $\\circledcirc$ , we use a synonym \u201cclose the screen\u201d to replace the \u201cturn off the screen\u201d in the prompt. As the word \u201coff\u201d does not exist in the prompt, the bias towards screen:off is weakened; ", "page_idx": 23}, {"type": "text", "text": "$\\textcircled{4}$ : compared with $\\circleddash$ , we change the name screen to Sony screen in the example, but left the prompt unchanged. We see the model is clever enough to distinguish which screen we refer to, and hence keeps the preference of $P_{0}(r_{20})$ demonstrated in $\\Phi$ ; ", "page_idx": 24}, {"type": "text", "text": "$\\mathfrak{G}$ : here we change the object to another name John, which also keeps the preference of $P_{0}(r_{20})$ demonstrated in $\\Phi$ ; ", "page_idx": 24}, {"type": "text", "text": "$\\circled{6}$ : compared with $\\mathfrak{G}$ , we add the sentence \u201cJohn will turn off the screen after experiment\u201d. Then we find the bias towards John:off is slightly increased, but not as strong as that in $\\circleddash$ , which provides us another way to control the strength of the bias; ", "page_idx": 24}, {"type": "text", "text": "$\\oslash$ : in the following three cases, we put the position of the misleading sentence before the examples ${\\bf d}^{0}$ . Compared with $\\circleddash$ , the bias towards screen:off is significantly amplified. This might be because the attention mechanism lets the model recite the fact that the screen is off before reading the examples (remember that screen:off also explains all examples); $\\textcircled{8}$ : compared with $\\textcircled{4}$ , where the object is also Sony screen. Here the bias is stronger than $\\textcircled{4}$ but weaker than $\\oslash$ , which verifies that adding spurious correlation before examples can amplify the bias while modifying the object name can reduce the bias; ", "page_idx": 24}, {"type": "text", "text": "$\\circledcirc$ : compared with $\\circledast$ , which also uses the synonym in the prompt, the bias becomes stronger. ", "page_idx": 24}, {"type": "text", "text": "In summary, we have several principles when controlling the strength of the bias in $P_{0}(h)$ : ", "page_idx": 24}, {"type": "text", "text": "\u2022 Adding spurious correlation before ${\\bf d}^{0}$ provide very strong bias;   \n\u2022 Using synonyms rather than phrases containing specific states (e.g., close/open v.s. turn off/on) weakens the bias;   \n\u2022 Using two slightly different object names (i.e., Sony screen v.s. screen) weakens the bias;   \n\u2022 Using indirect spurious correlation (e.g., John v.s. screen) weakens the bias. ", "page_idx": 24}, {"type": "image", "img_path": "BSYn7ah4KX/tmp/489555cf87f50f72ee1d6598463a1ff5e688edef08b7d16b37263e4dc6f15b2d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Figure 11: An example of how to manipulate $P_{0}(h)$ by adding spurious correlation in the prompt. The task instruction is the same as that provided in Figure 10, except the object name (in the blue box) and the added hints (in the red box). ", "page_idx": 24}, {"type": "text", "text": "For the experiments in Figure 13 and Figure 3, the six different levels of prior bias are controlled by the following prompts: ", "page_idx": 24}, {"type": "text", "text": "\u2022 Very high: add \u201cTurn off the screen after the experiment.\u201d before and after ${\\bf d}^{t}$ is given;   \n\u2022 High: add \u201cTurn off the screen after the experiment.\u201d before ${\\bf d}^{t}$ ;   \n\u2022 Medium: add \u201cTurn off the screen of the monitor after the experiment.\u201d before ${\\bf d}^{t}$ ;   \n\u2022 Mild: add \u201cJohn will turn off the screen after the experiment.\u201d before ${\\bf d}^{t}$ ;   \n\u2022 Low: add \u201cClose the screen after the experiment.\u201d before ${\\bf d}^{t}$ ;   \n\u2022 Very low: add \u201cClose the screen after the experiment.\u201d after ${\\bf d}^{t}$ ; ", "page_idx": 24}, {"type": "image", "img_path": "BSYn7ah4KX/tmp/774d2ac03ccf2e271c3f971a372421ded902e26017e9b84f1f4a5f55060a4a44.jpg", "img_caption": ["Figure 12: A more detailed analysis of the competition between the likelihood $p(\\mathbf{d}\\mid h)$ and bias in $\\bar{P_{0}(h)}$ . The first 4 columns are results from the new GPT3.5 and the last column is from GPT4. In each panel, the two curves represent strong and weak likelihood cases, which are controlled by the ground truth $h^{*}$ . The $h^{*}$ in strong cases contains 3 objects being \"on\" while the weak cases only have 1. The text in each panel represents the level of spurious correlation we introduce before and after d by manipulating the instruction prompt. For example, strong_strong means we put strong bias, i.e., \"Turn off the screen after experiments\", before and after ${\\bf d}^{t}$ in each generation. The trend of these panels aligns well with our previous results and analysis. "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "BSYn7ah4KX/tmp/5d46fad8c63496d1e39b16915caafd577e1a94725fa82b22113ac65d6c68f09e.jpg", "img_caption": ["Figure 13: The follow-up experiments of the one mentioned in Figure 3. In this figure, we put curves of the same level of prior bias (but with different likelihoods) in the same panel. It is clear that in most cases, the stronger likelihood will weaken the influence of the bias (that is why the orange curve is above the blue curve). "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "D.3 The Prompt Design for ACRE Task (Figures 14 to 16) ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Please also refer to the three figures and the log.txt file in our code base. ", "page_idx": 25}, {"type": "text", "text": "E More on Self-data-augmentation Task (Table 4 and 5; Figure 17, 18, and 19) ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In this appendix, we first introduce the prompt design of the experiment on both imitation-only settings, as well as the experiments with five different $\\mathcal{H}_{\\mathrm{eff}}$ . Please refer to Figure 17 for more details. ", "page_idx": 25}, {"type": "image", "img_path": "BSYn7ah4KX/tmp/0d045f47a9980e4a0adde02b8c467ab694898ba2d95e540cff635234730b43ae.jpg", "img_caption": ["System: "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Figure 14: Prompt design and an example dialogue for the imitation-only iterated learning on the ACRE dataset. The shaded region and arrows represent that we copy specific text to form the message. The messages starting with the role of system and user are sent to GPT, while those starting with assistant are the feedback from GPT. For this multi-round chat, we will feed all historical information to API, see our code for more details. ", "page_idx": 26}, {"type": "text", "text": "Then, we provide the full results of the experiment in Table 4, 5 and Figure 18, from which we can derive more interesting findings. In general, the figure demonstrates how different metrics evolve for different generations while the table reports the converged values at the last generation. ", "page_idx": 26}, {"type": "text", "text": "From the first column in this figure, we observe that other than the $\\mathcal{H}_{\\mathrm{hard}}$ setting, all curves in other settings show a clear trend of convergence towards the top of the figures. This means iterative learning indeed amplifies the hidden bias of $\\bar{P}_{0}(h_{\\mathrm{easy}})>P_{0}(h_{\\mathrm{hard}})$ when the imposed $\\mathcal{H}_{\\mathrm{eff}}$ doesn\u2019t impede this bias. In the $\\mathcal{H}_{\\mathrm{random}}$ setting, the $\\hat{\\mathbf{d}}$ is sampled from all the data generated by the previous generation, which makes those hard samples more likely to be sampled compared with the imitation-only settings or those with $\\mathcal{H}_{\\mathrm{{easylong/short}}}$ . Hence we observe the converging speed of it is slightly lower than these settings. On the contrary, when $\\mathcal{H}_{\\mathrm{hard}}$ is introduced, the bias towards $h_{\\mathrm{easy}}$ is successfully restrained. We observe a clear competition between these two pressures: the curve first goes up, which means the bias towards easy samples is stronger. However, as the learning goes on, the curves turn down again as we later have more hard samples in $\\mathrm{\\Phi_{pool}}$ . ", "page_idx": 26}, {"type": "text", "text": "The second column of the figure demonstrates the average ranking of words in ${\\bf d}^{t}$ . We observe a similar trend in the ratio of easy samples, although our $\\mathcal{H}_{\\mathrm{eff}}$ never explicitly constrains it. This phenomenon hints to us that when conducting an iterative self-data-augmentation algorithm, some unknown bias would be implicitly amplified although we already designed another $\\mathcal{H}_{\\mathrm{eff}}$ for other properties. Imagine we are conducting the ReST algorithm (Gulcehre et al. 2023). We can pursue the correctness of $\\mathbf{\\widetilde{d}}^{t}$ by ranking all examples by training a reward model that prefers more correct responses. However, some other subtle biases, like conciseness, informativeness, etc., might be ignored by the algorithm designer and are hence unexpectedly amplified. In summary, we should bear in mind that identifying the good and bad bias in $P_{0}(h)$ is quite important for an appropriate evolution. ", "page_idx": 26}, {"type": "text", "text": "Finally, we use the last column and the average length of the acronym to show how to make a composed $\\mathcal{H}_{\\mathrm{eff}}$ by combining more than one attribute of the data. It is clear that both $\\mathcal{H}_{\\mathrm{easylong}}$ and $\\mathcal{H}_{\\mathrm{easyshort}}$ did their jobs quite well: the converged $\\mathbf{d}^{6}$ contains the samples with desired properties as we expected. Another thing that heavily influences the results is the ratio of easy examples in d0. Although the theory claims that the converged results are irrespective of ${\\bf d}^{0}$ , the converging speed and the difficulty of amplifying specific bias heavily depends on ${\\bf d}^{0}$ . This claim can be well supported by ", "page_idx": 26}, {"type": "image", "img_path": "BSYn7ah4KX/tmp/ab57f4d89ed9c61f5bef070ccea91167e8cde2a58ed4dc4edb8388792886fb63.jpg", "img_caption": ["System: "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Figure 15: Prompt design for iterated learning with self-refine (Madaan et al. 2023) as the interaction phase. The text in gray is the same as the default imitation-only setting. Note that the format of the examples might be changed (like the examples in Gen-1: assistant), which doesn\u2019t influence the experimental results. ", "page_idx": 27}, {"type": "image", "img_path": "BSYn7ah4KX/tmp/6e3ab70298da8ed4ab15eeafc092ed81ba4e2e514ee4146e23df625441048a80.jpg", "img_caption": [], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Figure 16: Prompt design for iterated learning with hypothesis search (Qiu et al. 2024) as the interaction phase. Compared with self-refine, it introduces an external interpreter to refine $h$ proposed by the model, where $\\mathcal{H}_{\\mathrm{eff}}$ is the ground-truth one. ", "page_idx": 27}, {"type": "text", "text": "the fact that when $N_{e}$ is small, amplifying the bias of $h_{\\mathrm{easy}}$ is significantly harder than the large $N_{e}$ case. ", "page_idx": 27}, {"type": "image", "img_path": "BSYn7ah4KX/tmp/4c2b0d0265f8d1c162ba994f0d81d90c286ff4dcbe6a8c2a7ac27933ca40d3fd.jpg", "img_caption": ["Figure 17: Prompt design for iterated learning on the acronym data-generation task. "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "BSYn7ah4KX/tmp/66a1873910de6b5a009de39c703206ac2a16ede2a65cb5b326b8262f751a00b6.jpg", "img_caption": ["Figure 18: Results when adding different interaction phases (4 different seeds). All three settings demonstrate similar evolutionary trends, which match our theory quite well. "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "BSYn7ah4KX/tmp/e5e07cfdd6c77e5730e5debd9dc5a397e12ec06a0f556467bfc980afb001a6ec.jpg", "img_caption": [], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "Figure 19: An interesting observation of Mixtral series models: they have a bias toward alphabet examples. However, as the Mixtral model usually has typos in their response (like the right panel), we do not have the full results of these models. ", "page_idx": 28}, {"type": "text", "text": "Table 4: Claude3-haiku results under different $\\mathcal{H}_{\\mathrm{eff}}$ . We color the highest and lowest numbers in each column differently. ", "page_idx": 29}, {"type": "table", "img_path": "BSYn7ah4KX/tmp/d87ed57bd738841b5a16eb248cdbc530d856e04405baf5c60aa3780ab9c1f9db.jpg", "table_caption": [], "table_footnote": [], "page_idx": 29}, {"type": "text", "text": "Table 5: GPT3.5-Turbo 0125 plays with Claude3-haiku results when adding different $\\mathcal{H}_{\\mathrm{eff}}$ ", "text_level": 1, "page_idx": 29}, {"type": "table", "img_path": "BSYn7ah4KX/tmp/d5ac53fe98bb6924c0844a325346977f6b69ef99efb31951d13c8b4540c4fd56.jpg", "table_caption": [], "table_footnote": [], "page_idx": 29}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Claims are justified. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 30}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: We discuss this throughout. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 30}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Theorems include full statements and proofs. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 31}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Code to reproduce the experiments is provided. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 31}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: Our code is publicly released. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 32}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We describe settings in general in the main body, and details are provided in the appendix and in the published code. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 32}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: Results have error bars. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [No] ", "page_idx": 33}, {"type": "text", "text": "Justification: We did not carefully track our computational resources, but they were mostly limited to modest numbers of API calls to existing LLMs. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 33}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We follow the code of ethics. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 33}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: The paper is about the evolution of LLM agents and potentially steering that evolution; this has roughly the same potential for impact as any LLM work. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: We do not release any novel datasets or models. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 34}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: Datasets, models, etc. are cited. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 34}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 35}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: The code for experiments is fairly simple, with documentation inline. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 35}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: No human subjects. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 35}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: No human subjects. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 35}]