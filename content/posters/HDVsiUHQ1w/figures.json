[{"figure_path": "HDVsiUHQ1w/figures/figures_1_1.jpg", "caption": "Figure 1: Embeddings of L2 loss (a) vs SCOREQ (b) on TCD VOIP data [17]. Color shows quality labels (MOS) while markers identify the degradations. We compute the Normalised Mutual Information (NMI) between k-Means clusters and degradation labels, as well as the Pearson\u2019s Correlation (PC) between embedding distance with respect to random clean speech and MOS targets. Higher NMI indicates representations are clustered based on degradations while higher PC means representations are ordered with respect to MOS targets. Results indicate that the L2 loss embeddings tend to capture degradation information (NMI=0.39, PC=0.53) while SCOREQ quality (NMI=0.11, PC=0.80). See Appendix I for more details.", "description": "This figure compares the embeddings generated by using L2 loss and SCOREQ methods on the TCD VOIP dataset.  The left panel (a) shows the embeddings from the L2 loss approach, where points are clustered by degradation type (NMI=0.39) but less well-ordered by MOS score (PC=0.53). The right panel (b) shows SCOREQ embeddings; points are less clustered by degradation but far better ordered by MOS (NMI=0.11, PC=0.79).  This illustrates how SCOREQ improves MOS prediction by creating a representation that prioritizes quality score ordering over degradation type.", "section": "1 Introduction"}, {"figure_path": "HDVsiUHQ1w/figures/figures_4_1.jpg", "caption": "Figure 2: Example of the SCOREQ loss using 3 samples in a training batch with corresponding MOS labels 4.5, 2.0, and 1.5. The distance matrix entries are defined as Di,j,k = || f(g(x)) - f(g(x))||2 < ||f(g(xi)) - f(g(x))||2. The intuition behind this contrastive loss for regression is shown in how the negative embeddings change in the anchor sample 1 where MOS is 4.5. We observe that the negative (sample 3 with MOS 1.5 ) will be further from the anchor with respect to sample 2. Indeed, because of the anchor 2 loss (where MOS is 2.0), sample 3 embeddings are pushed towards sample 2.", "description": "This figure illustrates how the SCOREQ loss function works with a batch of three samples.  It shows how the distance between embeddings of different samples influences the training process. The focus is on how the negative samples are pushed away from the anchor sample and pulled closer to the positive sample, depending on their MOS values. This process helps the model to learn a continuous quality manifold in the embedding space where samples with similar MOS scores are closer together.", "section": "3 Contrastive Regression for MOS prediction"}, {"figure_path": "HDVsiUHQ1w/figures/figures_4_2.jpg", "caption": "Figure 1: Embeddings of L2 loss (a) vs SCOREQ (b) on TCD VOIP data [17]. Color shows quality labels (MOS) while markers identify the degradations. We compute the Normalised Mutual Information (NMI) between k-Means clusters and degradation labels, as well as the Pearson's Correlation (PC) between embedding distance with respect to random clean speech and MOS targets. Higher NMI indicates representations are clustered based on degradations while higher PC means representations are ordered with respect to MOS targets. Results indicate that the L2 loss embeddings tend to capture degradation information (NMI=0.39, PC=0.53) while SCOREQ quality (NMI=0.11, PC=0.80). See Appendix I for more details.", "description": "This figure compares the embeddings generated by a model trained with the L2 loss and SCOREQ.  The TCD VOIP dataset is used.  The x and y axes represent the embedding space, with each point representing an audio sample. Colors indicate the MOS score, and different markers represent different types of audio degradation.  The L2 loss model's embeddings show clear clustering based on degradation type (high NMI), but poor correlation with MOS scores.  In contrast, SCOREQ's embeddings show less clustering based on degradation but a much stronger correlation with MOS, suggesting better performance in capturing the continuous nature of speech quality.", "section": "1 Introduction"}, {"figure_path": "HDVsiUHQ1w/figures/figures_5_1.jpg", "caption": "Figure 4: SCOREQ modes. No-Reference (NR) mode is trained in 2 steps. We first pre-train the encoder g() with the SCOREQ loss. Next, we learn a linear layer (MOS head) that predicts an interpretable numerical MOS.", "description": "This figure illustrates the two modes of operation for the SCOREQ model: No-Reference (NR) and Non-Matching Reference (NMR).  The NR mode is a two-step training process. First, the encoder is pre-trained using the SCOREQ loss.  Then, a linear layer (MOS head) is added and trained to predict MOS scores. The NMR mode uses the pre-trained encoder from the NR mode's first step, and does not require further training to produce a quality score.", "section": "3 Contrastive Regression for MOS prediction"}, {"figure_path": "HDVsiUHQ1w/figures/figures_7_1.jpg", "caption": "Figure 5: Domain mismatch. Each dot is a dataset, while horizontal lines represent the PC average for each domain shift (IN, ODS, ODM) relative to the training set domains D1 (simulated telephone speech), D2 (deep learning-based speech enhancement), and D3 (speech synthesis).", "description": "This figure displays the Pearson Correlation (PC) values for different speech quality metrics across various datasets. Each point on the graph corresponds to a specific dataset tested on.  The datasets are categorized into three groups based on the domain shift from the training data: IN (in-domain), ODS (out-of-distribution), and ODM (out-of-domain).  Horizontal lines show the average PC for each group, indicating the generalization performance of the metrics under different testing conditions. The purpose is to show how well the metrics generalize beyond their training data; poor generalization across these categories indicates a lack of robustness.", "section": "4 Generalisation of Speech Quality Metrics"}, {"figure_path": "HDVsiUHQ1w/figures/figures_21_1.jpg", "caption": "Figure 1: Embeddings of L2 loss (a) vs SCOREQ (b) on TCD VOIP data [17]. Color shows quality labels (MOS) while markers identify the degradations. We compute the Normalised Mutual Information (NMI) between k-Means clusters and degradation labels, as well as the Pearson\u2019s Correlation (PC) between embedding distance with respect to random clean speech and MOS targets. Higher NMI indicates representations are clustered based on degradations while higher PC means representations are ordered with respect to MOS targets. Results indicate that the L2 loss embeddings tend to capture degradation information (NMI=0.39, PC=0.53) while SCOREQ quality (NMI=0.11, PC=0.80). See Appendix I for more details.", "description": "This figure compares the performance of L2 loss and SCOREQ methods on TCD VOIP dataset. The left panel shows the embeddings generated by L2 loss, while the right panel shows those generated by SCOREQ. Each point represents a speech sample with its color indicating the MOS score and its marker indicating the type of degradation applied.  The NMI (Normalized Mutual Information) and PC (Pearson Correlation) metrics are used to evaluate how well the embeddings capture the degradation information and MOS score respectively.  SCOREQ shows much better correlation with MOS scores, suggesting better performance in speech quality prediction than L2 loss.", "section": "1 Introduction"}]