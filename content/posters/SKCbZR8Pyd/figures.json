[{"figure_path": "SKCbZR8Pyd/figures/figures_2_1.jpg", "caption": "Figure 1: T-SNE visualization of representations of different AR tokens. Left: Golden AR tokens and synthetic AR tokens. Right: Golden AR tokens and aligned synthetic AR tokens.", "description": "This figure visualizes the distribution gap between golden AR tokens (from real speech) and synthetic AR tokens (generated by the model) using t-SNE. The left panel shows a clear separation between the two distributions, highlighting the distribution gap. The right panel demonstrates that after applying the SpeechAlign method, the distribution gap is significantly reduced, as the synthetic AR tokens are now more aligned with the golden AR tokens.", "section": "Preliminary Analysis on Distribution Gap"}, {"figure_path": "SKCbZR8Pyd/figures/figures_3_1.jpg", "caption": "Figure 2: AR LM refers to autoregressive models and NAR LM refers to non-autoregressive models. Left: Illustration of inference process of codec language models. Right: Illustration of SpeechAlign method.", "description": "This figure shows the inference process of codec language models and the proposed SpeechAlign method. The left panel shows the standard process: text is fed into an autoregressive language model (AR LM) to generate autoregressive tokens (AR tokens); these tokens and a prompt are fed into a non-autoregressive language model (NAR LM) to produce non-autoregressive tokens (NAR tokens); finally, NAR tokens are decoded to generate speech. The right panel illustrates the SpeechAlign method, where a preference dataset is created comparing golden AR tokens (from real speech) to synthetic AR tokens (from the AR LM).  This dataset is then used in preference optimization to iteratively improve the AR LM. The overall goal is to improve the alignment between the model's generated speech and human preferences.", "section": "3 SpeechAlign"}, {"figure_path": "SKCbZR8Pyd/figures/figures_6_1.jpg", "caption": "Figure 3: Qualitative side-by-side comparsion results of preference optimized models versus the baseline SFT model on zero-shot text-to-speech performance.", "description": "This figure presents a qualitative comparison of speech generated by different models using the zero-shot text-to-speech task.  It shows the results of a human evaluation where listeners compared the speech quality produced by the baseline model (SFT), and several models that have undergone preference optimization (SpeechAlign-RLHF-PPO, SpeechAlign-DPO-Iter1, SpeechAlign-DPO-Iter2, SpeechAlign-DPO-Iter3, SpeechAlign-BoN, and SpeechAlign-CoH). The comparison is done for two datasets, LibriSpeech and VCTK. The results are displayed in terms of win, tie, and lose rates showing which model's output was preferred by the human listeners.  The visualization helps to understand how preference optimization strategies affect the qualitative aspects of speech generation, comparing to a model only trained with supervised finetuning.", "section": "4.3 Main Results"}, {"figure_path": "SKCbZR8Pyd/figures/figures_7_1.jpg", "caption": "Figure 4: Zero-shot TTS performance of DPO-optimized models at different iterations on LibriSpeech test-clean.", "description": "This figure shows the results of zero-shot text-to-speech (TTS) experiments using different numbers of iterations of direct preference optimization (DPO).  The left panel displays WER (Word Error Rate) and SIM (Speaker Similarity) scores. Lower WER indicates better accuracy, while higher SIM indicates better voice similarity to the reference speaker. The right panel shows QMOS (Quality Mean Opinion Score) and SMOS (Speaker Mean Opinion Score). Higher QMOS suggests better overall speech quality, and higher SMOS implies greater similarity between the generated and original speech in terms of speaker characteristics. The results indicate that the iterative DPO process improves various metrics of the generated speech, up to a certain point, after which performance might decrease slightly.", "section": "5 Analysis"}, {"figure_path": "SKCbZR8Pyd/figures/figures_8_1.jpg", "caption": "Figure 5: Left: Performance of SpeechAlign across different preference data sizes. Right: Performance of SpeechAlign on small models. Results are evaluated from on LibriSpeech test-clean.", "description": "This figure shows the performance comparison of SpeechAlign with different preference data sizes and model sizes. The left subfigure shows that increasing the preference data size from 0 to 50k improves the model performance, but adding more data (250k) does not provide further improvement. This indicates an optimal size for the preference data. The right subfigure shows that SpeechAlign also works well with smaller models, improving the performance even with a relatively small model.", "section": "5. Analysis"}]