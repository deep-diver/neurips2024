[{"figure_path": "IdEaeCbhUW/figures/figures_3_1.jpg", "caption": "Figure 1: The Hindsight-Sequence-Planner (HSP) model. The HSP takes inspiration from the function of the basal ganglia (BG) (Top/Orange) and the prefrontal cortex (PFC) (Bottom/Blue). We train an actor with a gated recurrent unit that can produce sequences of arbitrary lengths given a single state. This is achieved by utilizing a critic and a model that acts at a finer temporal resolution during training/replay to provide an error signal to each primitive action of the action sequence.", "description": "This figure illustrates the Hindsight-Sequence-Planner (HSP) model, which is inspired by the basal ganglia and prefrontal cortex.  The model consists of an actor (GRU), a critic, and a model of the environment. The actor generates a sequence of actions given a single state. The critic evaluates the quality of the action sequence, while the model predicts the next state given the current state and action. The model operates at a finer temporal resolution during training than the actor, which allows the model to provide more precise feedback to the actor. The BG (basal ganglia) is represented by the top portion of the figure, and the PFC (prefrontal cortex) is represented by the bottom portion. The dashed lines show the flow of information between the different components of the model.", "section": "4 Hindsight Sequence Planner"}, {"figure_path": "IdEaeCbhUW/figures/figures_5_1.jpg", "caption": "Figure 2: Learning curves of HSP-J and Soft-Actor Critic (SAC) (58) over continuous control tasks. HSP and SAC are evaluated under different settings: SAC receives input after every primitive action, while HSP receives input after J primitive actions. Yet it demonstrates competitive performance on all environments, even outperforming SAC on LunarLander, Hopper and Humanoid environments. HSP demonstrates stable learning even with the added model and generative replay training. All curves are averaged over 5 trials, with shaded regions representing standard error.", "description": "This figure shows the learning curves for HSP with different action sequence lengths (ASL) and compares them to the Soft Actor-Critic (SAC) algorithm. It illustrates that HSP achieves competitive performance across various continuous control tasks, despite receiving input less frequently than SAC. The stable learning performance of HSP even with the addition of a model and generative replay training is also demonstrated.", "section": "Learning Curves"}, {"figure_path": "IdEaeCbhUW/figures/figures_6_1.jpg", "caption": "Figure 3: Performance of HSP, SAC, and TempoRL (55) at different Action Sequence Lengths (ASL). SAC and TempoRL repeat the same action for the duration, while HSP can perform a sequence of actions. Since it implements dynamic action repetition, we present the average ASL for TempoRL instead of a range of ASL. HSP demonstrates robust performance even at human-like reaction times (>150ms). All markers are averaged over 5 trials, with the error bars representing standard error. Going from left to right then top to bottom, the selected training ASL J for HSP are: 16, 16, 4, 16, 4, 8.", "description": "This figure compares the performance of HSP, SAC, and TempoRL algorithms across different action sequence lengths (ASL) on six continuous control tasks.  It highlights that HSP maintains robust performance even at human-like reaction times, unlike SAC and TempoRL which repeat the same action for the duration and struggle with longer sequences. The average ASL for TempoRL is shown due to its dynamic action repetition. Error bars represent standard error across 5 trials.", "section": "5 Experiments"}, {"figure_path": "IdEaeCbhUW/figures/figures_7_1.jpg", "caption": "Figure 4: Performance of HSP and model based online planning on different ASL. Both HSP and Online Planning utilize the same actor and model. HSP utilizes the actor to generate a sequence of actions while online planning utilizes the actor and the model to generate a sequence of actions. The same model is used to train the HSP action sequences. Yet, we find that while the model is not accurate enough to sustain performance for longer sequences, it can train the actor to produce accurate action sequences.", "description": "This figure compares the performance of the Hindsight Sequence Planner (HSP) and model-based online planning on the Humanoid-v2 task across different action sequence lengths (ASL).  Both methods use the same actor and model, but HSP generates sequences of actions while online planning uses the actor and model iteratively to produce single actions. The results show that HSP achieves better performance, particularly with longer sequences, despite using an imperfect model for sequence generation. This highlights HSP's ability to learn effective action sequences even with inaccurate environment models.", "section": "5 Experiments"}, {"figure_path": "IdEaeCbhUW/figures/figures_7_2.jpg", "caption": "Figure 3: Performance of HSP, SAC, and TempoRL (55) at different Action Sequence Lengths (ASL). SAC and TempoRL repeat the same action for the duration, while HSP can perform a sequence of actions. Since it implements dynamic action repetition, we present the average ASL for TempoRL instead of a range of ASL. HSP demonstrates robust performance even at human-like reaction times (>150ms). All markers are averaged over 5 trials, with the error bars representing standard error. Going from left to right then top to bottom, the selected training ASL J for HSP are: 16, 16, 4, 16, 4, 8.", "description": "This figure compares the performance of HSP, SAC, and TempoRL algorithms across various continuous control tasks with varying action sequence lengths (ASL).  It highlights HSP's ability to handle longer action sequences effectively, achieving competitive performance even at human-like reaction times, unlike SAC and TempoRL which struggle with longer sequences.  The results demonstrate HSP's robustness and efficiency, showcasing its adaptation to varying task complexities.", "section": "5 Experiments"}, {"figure_path": "IdEaeCbhUW/figures/figures_21_1.jpg", "caption": "Figure 6: Learning curves of Latent HSP-n and Soft-Actor Critic (SAC) over continuous control tasks.", "description": "This figure presents the learning curves, showing the average return over training steps for both Latent HSP (with different action sequence lengths) and SAC across six continuous control tasks.  It visually demonstrates the comparative performance of the proposed method against a well-established baseline algorithm in different scenarios. The shaded regions likely represent standard error, showing variability in performance across multiple trials.", "section": "Experiments"}]