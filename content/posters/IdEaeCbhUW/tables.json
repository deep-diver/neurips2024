[{"figure_path": "IdEaeCbhUW/tables/tables_19_1.jpg", "caption": "Table 1: List of Common hyperparameters", "description": "This table lists the hyperparameters used in the Hindsight Sequence Planner (HSP) and Soft Actor-Critic (SAC) algorithms.  It includes the size of hidden layers in neural networks, update frequencies, discount factor, learning rates, replay buffer size, batch size, and the initial number of steps using a random policy. These parameters are common across all environments used in the experiments.", "section": "A.2 Implementation Details"}, {"figure_path": "IdEaeCbhUW/tables/tables_20_1.jpg", "caption": "Table 1: List of Common hyperparameters", "description": "This table lists the hyperparameters used in the Hindsight Sequence Planner (HSP) and Soft Actor-Critic (SAC) algorithms.  It specifies values for parameters controlling learning rate, discount factor, update intervals, replay buffer size, and the initial exploration phase. These parameters influence the training process of both the HSP and SAC models and are crucial in achieving optimal performance in reinforcement learning tasks.", "section": "A.2 Implementation Details"}, {"figure_path": "IdEaeCbhUW/tables/tables_20_2.jpg", "caption": "Table 2: List of environment-specific hyperparameters", "description": "This table lists the maximum timestep and evaluation frequency for each of the six continuous control environments used in the experiments.  The maximum timestep indicates the total number of steps simulated for each environment during training, while the evaluation frequency specifies how often the agent's performance was assessed during training.", "section": "5 Experiments"}]