[{"type": "text", "text": "Continuous Contrastive Learning for Long-Tailed Semi-Supervised Recognition ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Zi-Hao Zhou1,2\u2217 Siyuan Fang1,2\u2217 Zi-Jing Zhou3 Tong Wei1,2\u2020 Yuanyu Wan4 Min-Ling Zhang1,2 ", "page_idx": 0}, {"type": "text", "text": "1School of Computer Science and Engineering, Southeast University, Nanjing, China 2Key Laboratory of Computer Network and Information Integration (Southeast University), Ministry of Education, China 3Xiaomi Inc., China 4School of Software Technology, Zhejiang University, Ningbo, China {zhouzih, syfang, weit}@seu.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Long-tailed semi-supervised learning poses a significant challenge in training models with limited labeled data exhibiting a long-tailed label distribution. Current state-of-the-art LTSSL approaches heavily rely on high-quality pseudo-labels for large-scale unlabeled data. However, these methods often neglect the impact of representations learned by the neural network and struggle with real-world unlabeled data, which typically follows a different distribution than labeled data. This paper introduces a novel probabilistic framework that unifies various recent proposals in long-tail learning. Our framework derives the class-balanced contrastive loss through Gaussian kernel density estimation. We introduce a continuous contrastive learning method, CCL, extending our framework to unlabeled data using reliable and smoothed pseudo-labels. By progressively estimating the underlying label distribution and optimizing its alignment with model predictions, we tackle the diverse distribution of unlabeled data in real-world scenarios. Extensive experiments across multiple datasets with varying unlabeled data distributions demonstrate that CCL consistently outperforms prior state-of-the-art methods, achieving over $4\\%$ improvement on the ImageNet-127 dataset. Our source code is available at https://github.com/zhouzihao11/CCL. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Semi-supervised learning (SSL) serves as a powerful approach for improving the generalization capabilities of deep neural networks (DNNs) in scenarios where labeled data is scarce [37, 59, 6, 23]. The core concept of SSL methods typically involves assigning pseudo-labels to unlabeled data and utilizing those with high confidence for model training [56, 71, 10]. However, many existing SSL algorithms presuppose a balanced label distribution across both labeled and unlabeled datasets. In real-world applications, datasets commonly exhibit a long-tailed label distribution [65, 28, 50, 67, 55], leading to biased pseudo-label generation favoring majority classes [40, 3, 68, 24]. This discrepancy challenges the effectiveness of SSL algorithms in addressing real-world datasets. ", "page_idx": 0}, {"type": "text", "text": "The exploration of long-tailed semi-supervised learning (LTSSL) has gained momentum to address the challenge of biased pseudo-label distribution arising from class imbalance in labeled and unlabeled data. Recent LTSSL approaches propose compensating for the learning of minority classes by distribution alignment [33, 63], data rebalancing [22, 38], and logit adjustment [64, 43] to rectify the pseudo-label distribution. However, existing approaches often assume the equivalence of the unlabeled data distribution with the labeled data or rely on predefined anchor distributions to estimate the unlabeled data distribution [64, 43]. Furthermore, these methods primarily focus on correcting model outputs without delving into the role of representation learning in improving performance. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "This paper explicitly introduces an approach to obtain effective representations for long-tail learning by adopting an information-theoretic view of DNNs. We present a probabilistic framework that utilizes the deep variational information bottleneck method [1] to learn good representations and demonstrate its unification of recent long-tail learning proposals, such as logit adjustment [44] and balanced softmax [51], through approximating the density of class-conditional distribution in different ways. Specifically, our framework encompasses class-balanced supervised contrastive learning [73, 15] via Gaussian kernel density estimation. We extend this framework to address LTSSL by adapting the supervised contrastive loss to unlabeled data using \u201ccontinuous pseudo-labels\u201d, derived from model predictions and propagated labels, to mitigate confirmation bias. To account for varying label distribution of unlabeled data, we progressively estimate the label distribution through a moving average and adjust model predictions to align with the estimated distribution. ", "page_idx": 1}, {"type": "text", "text": "In summary, our contributions are as follows: ", "page_idx": 1}, {"type": "text", "text": "1. We propose a probabilistic framework which unifies many recent proposals in long-tail learning. Specifically, popular class-balanced contrastive learning methods can be seen as special cases of our framework when approximating the density using a Gaussian kernel.   \n2. We generalize the proposed framework to LTSSL and present a continuous contrastive learning method based on reliable and smoothed pseudo-labels to address confirmation bias and improve the quality of learned representations.   \n3. We conduct extensive experiments across several LTSSL datasets with diverse label distributions of unlabeled data. The results show that our proposal substantially outperforms previous state-ofthe-art methods. ", "page_idx": 1}, {"type": "text", "text": "2 A Probabilistic Framework for Long-Tail Learning ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this section, we first introduce a general framework for learning good representations. Then, we expand this framework to long-tail learning and illustrate how recent proposals can be regarded as specific instances of our framework through three ways for density approximation. ", "page_idx": 1}, {"type": "text", "text": "Problem setup of long-tail learning. We consider a $C$ -class classification problem with instance space $\\mathcal{X}$ and target space $\\mathcal{V}\\,=\\,\\{1,\\hdots,C\\}$ . Let $P_{s}$ and $P_{t}$ denote the source (training) and test distributions on $(\\mathcal{X},\\mathcal{Y})$ , respectively. We denote by $\\mathbb{P}_{s}$ and $\\mathbb{P}_{t}$ the corresponding probability density (or mass) functions. Given a training dataset $\\{(\\pmb{x}_{i},y_{i})\\}_{i=1}^{N}$ , where $\\mathbf{\\boldsymbol{x}}_{i}\\in\\mathbb{R}^{d}$ is the training sample and $y_{i}$ is the ground-truth label. ", "page_idx": 1}, {"type": "text", "text": "2.1 Learning good representations from information theoretical view ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this subsection, we rethink one of the most popular approaches to deal with representation learning, i.e., contrastive learning. We derive many recent proposals in this branch from an informationtheoretic view. Let $Z$ denote the latent representation of $X$ induced by the encoder $\\operatorname{enc}(\\cdot)$ parameterized by $\\Theta$ . From an information-theoretic view, an optimal representation $Z$ is maximally informative about the target $Y$ , and minimally \u201cmemorizes\u201d $X$ . The information bottleneck [60] adopts mutual information $I(\\cdot)$ to measure information between two variables. Thus, optimal $Z$ can be obtained by maximizing the following objective: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\Theta^{\\ast}=\\arg\\operatorname*{max}_{\\Theta}\\,I(Z,Y;\\Theta)-\\delta I(Z,X;\\Theta),\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\delta\\geq0$ is a tradeoff parameter. The variational information bottleneck [1] solves the above objective by variational inference. Based on the definition of $I(\\cdot)$ , we can rewrite Eq. (1) as: ", "page_idx": 1}, {"type": "equation", "text": "$$\nI(Z,Y)-\\delta I(Z,X)=\\int d y d z\\mathbb{P}(y,z)\\log\\frac{\\mathbb{P}(y\\mid z)}{\\mathbb{P}(y)}-\\delta\\int d z d x\\mathbb{P}(x,z)\\log\\frac{\\mathbb{P}(z\\mid x)}{\\mathbb{P}(z)},\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where we omit $\\Theta$ for simplicity. Since $\\begin{array}{r}{\\mathbb{P}(y\\mid z)=\\int d\\pmb{x}\\mathbb{P}(\\pmb{x}\\mid z)\\mathbb{P}(y\\mid\\pmb{x})}\\end{array}$ is intractable in Eq. (2), let $\\widehat{\\mathbb{P}}(y\\mid z)$ be a variational approximation to $\\mathbb{P}(y\\mid z)$ and considering that the Kullback-Leibler (KL) ", "page_idx": 1}, {"type": "text", "text": "divergence ${\\mathrm{KL}}[\\mathbb{P}(y\\mid z),{\\widehat{\\mathbb{P}}}(y\\mid z)]$ is always positive, we have RHS of Eq. (2)\u2019s lower bounded: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\int d x d y d z\\mathbb{P}(x)\\mathbb{P}(y\\mid x)\\mathbb{P}(z\\mid x)\\log{\\widehat{\\mathbb{P}}(y\\mid z)}-\\delta\\int d x d z\\mathbb{P}(x)\\mathbb{P}(z\\mid x)\\log{\\frac{\\mathbb{P}(z\\mid x)}{\\mathbb{P}(z)}}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Suppose we use an encoder of the form $\\mathbb{P}(\\pmb{z}\\mid\\pmb{x})\\sim\\mathcal{N}(\\mathrm{enc}(\\pmb{x}),\\pmb{\\varepsilon}^{2}\\pmb{I})$ and $\\mathbb{P}(z)\\sim\\mathcal{N}(\\mathbf{0},I)$ , the second term of Eq. (3) equals the KL divergence $\\mathrm{KL}[\\mathbb{P}(z\\mid x),\\mathbb{P}(z)]$ . Since $\\mathbb{P}(z\\mid x)$ and $\\mathbb{P}(z)$ are normal distributions, it can be rewritten as: $-\\delta l(\\frac{1}{2}\\pmb{\\varepsilon}^{2}-1-2\\log\\pmb{\\varepsilon})\\stackrel{*}{-}\\delta\\|\\mathrm{enc}(\\pmb{x})\\|^{2}$ , where $l$ is the dimension of $_{\\textit{z}}$ . For a deterministic model, $_{z}$ is almost unique for each $\\textbf{\\em x}$ , thus assuming $\\varepsilon$ is a small constant close to 0. By integrating out $\\mathbb{P}(z\\mid x)$ and discarding constant terms, maximizing Eq. (3) can be approximated by minimizing: ", "page_idx": 2}, {"type": "equation", "text": "$$\n-\\int d x\\mathbb{P}(\\pmb{x})\\int d y\\mathbb{P}(y\\mid\\pmb{x})\\log{\\widehat{\\mathbb{P}}}(y\\mid\\mathrm{enc}(\\pmb{x}))+\\delta\\left\\|\\mathrm{enc}(\\pmb{x})\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "In the following of this paper, we denote the output of $\\operatorname{enc}(x)$ as $_{z}$ (or $z_{x}$ for a particular sample $\\pmb{x}_{,}$ ) for simplicity. Minimizing Eq. (4) is equivalent to minimizing the following objective in the distribution of test data on each $\\textbf{\\em x}$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n-\\sum_{k\\in[C]}\\mathbb{P}_{t}(Y=k\\mid x)\\log{\\widehat{\\mathbb{P}}}_{t}(Y=k\\mid z)+\\delta\\|z\\|^{2}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Notably, Eq. (5) can be seen as a general framework for learning good representations. If $\\mathbb{P}_{s}(Y)=$ $\\mathbb{P}_{t}(Y)$ , $\\mathbb{P}_{t}(Y=y\\mid x)$ can simply be substituted by the ground-truth labels of training samples. However, in long-tail learning, the class-probability function $\\mathbb{P}_{t}(Y=y\\mid x)$ is different from that of the training data due to label distribution shift. ", "page_idx": 2}, {"type": "text", "text": "2.2 Probabilistic framework for long-tailed supervised learning ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Since ${\\mathbb{P}}_{s}(Y=y\\mid x)\\neq{\\mathbb{P}}_{t}(Y=y\\mid x)$ , we cannot directly solve Eq. (5). However, since long-tail learning typically assumes that $\\mathbb{P}_{t}(Y)$ is uniform and we work with the label shift assumption, i.e., $\\mathbb{P}_{s}(\\pmb{x}\\mid\\pmb{Y}=\\pmb{y})=\\mathbb{P}_{t}(\\pmb{x}\\mid\\pmb{Y}=\\pmb{y})$ , we can obtain $\\mathbb{P}_{t}(Y=y\\mid x)$ by Bayes\u2019 theorem: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbb{P}_{t}(Y=y\\mid x)={\\frac{\\mathbb{P}(\\pmb{x}\\mid Y=y)}{\\sum_{k\\in[C]}\\mathbb{P}(\\pmb{x}\\mid Y=k)}}={\\frac{{\\frac{1}{\\mathbb{P}_{s}(Y=y)}}\\mathbb{P}_{s}(Y=y\\mid\\pmb{x})}{\\sum_{k\\in[C]}{\\frac{1}{\\mathbb{P}_{s}(Y=k)}}\\mathbb{P}_{s}(Y=k\\mid\\pmb{x})}}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Throughout the paper, we use the notation $\\mathbb{P}(\\pmb{x}\\mid\\,\\boldsymbol{Y}\\,=\\,\\boldsymbol{y})$ to represent either $\\mathbb{P}_{s}(\\pmb{x}\\mid\\,\\boldsymbol{Y}\\,=\\,y)$ or $\\mathbb{P}_{t}(\\pmb{x}\\mid\\boldsymbol{Y}=\\boldsymbol{y})$ . In practice, $\\|z\\|^{2}$ can be omitted in optimization because normalization is commonly adopted in deep learning. In long-tail learning, minimizing Eq. (5) equals to minimizing: ", "page_idx": 2}, {"type": "equation", "text": "$$\n-\\sum_{k\\in[C]}{\\frac{1}{\\mathbb{P}_{s}(Y=k)}}\\mathbb{P}_{s}(Y=k\\mid x)\\log{\\widehat{\\mathbb{P}}_{t}(Y=k\\mid z)}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "According to Jensen\u2019s inequality, Eq. (7) attains its minimum value if and only if $\\widehat{\\mathbb{P}}_{t}(Y\\,=\\,k\\ |$ $\\mathbf{\\Delta}\\mathbf{x})\\mathbb{P}_{s}(Y\\stackrel{!}{=}k)\\propto\\mathbb{P}_{s}(Y=\\bar{k^{'}}|\\textbf{}\\mathbf{x})$ for $k\\in[C]$ . Hence, Eq. (7) can be replaced as follow s: ", "page_idx": 2}, {"type": "equation", "text": "$$\nJ=-\\sum_{k\\in[C]}{\\frac{\\mathbb{P}(Y=k)}{\\mathbb{P}_{s}(Y=k)}}\\mathbb{P}_{s}(Y=k\\mid x)\\log{\\widehat{\\mathbb{P}}(Y=k\\mid z)},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where P (Y = y | z) = k\u2208P [tC(]YP  t=(yY z=)kP|(zY) P=(yY) =k) and $\\mathbb{P}(Y)$ is an arbitrarily label distribution. Eq. (8) ", "page_idx": 2}, {"type": "text", "text": "can be seen as an extension of  sample reweighting [52] and logit adjustment [44] using probabilistic labels rather than discrete labels when $\\mathbb{P}(Y)$ is specified as $\\mathbb{P}_{t}(Y)$ and $\\mathbb{P}_{s}(Y)$ , respectively. Besides, Eq. (8) presents a unified framework that consolidates existing long-tail learning methods by estimating ${\\widehat{\\mathbb{P}}}(z\\mid Y=y)$ or $\\widehat{\\mathbb{P}}_{t}(Y=y\\mid z)$ in different ways. In the following, we discuss three ways to estimate  these terms. ", "page_idx": 2}, {"type": "text", "text": "Method 1: Explicitly specify ${\\widehat{\\mathbb{P}}}(z\\mid Y=y)$ as a prior distribution such as vMF distribution [34] and Wrapped Cauchy Distribution  [27]. ", "page_idx": 2}, {"type": "text", "text": "Method 2: Approximate ${\\widehat{\\mathbb{P}}}(z\\mid Y=y)$ using a learnable linear classifier. Let $\\{w_{i},b_{i}\\}_{i=1}^{C}$ denote the parameters of a linear layer, which is followed by a softmax to obtain the normalized probability: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\widehat{\\mathbb{P}}(z\\mid Y=y)\\propto\\widehat{\\mathbb{P}}_{t}(Y=y\\mid z)=\\frac{\\exp\\left(z^{\\top}w_{y}+b_{y}\\right)}{\\sum_{k\\in[C]}\\exp\\left(z^{\\top}w_{k}+b_{k}\\right)}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "table", "img_path": "PaqJ71zf1M/tmp/819d3f395ba71abd5cd6769229dd06034df1866fa9e1218ae0a5e75d646adb08.jpg", "table_caption": ["Table 1: A unified view of popular long-tail learning methods from our framework. \u201c\u2013\u201d means that this method does not involve this issue and $\\mathbf{\\dot{\\omega}}\\times\\mathbf{\\dot{\\omega}}$ indicates that the method has not resolved the issue. "], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "Method 3: Approximate ${\\widehat{\\mathbb{P}}}(z\\mid Y=y)$ via the Gaussian kernel. A new sample from class $y$ should be closer to all samples within class $y$ and away from samples from other classes. Using the expected similarity among all samples within the class to measure distance, we derive: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\widehat{\\mathbb{P}}(z\\mid Y=y)\\propto\\widehat{\\mathbb{P}}_{t}(Y=y\\mid z)=\\frac{\\mathbb{E}_{x^{\\prime}\\sim\\mathbb{P}(\\cdot\\mid Y=y)}\\left[\\kappa\\left(z_{x},z_{x^{\\prime}}\\right)\\right]}{\\sum_{k\\in[C]}\\mathbb{E}_{x^{\\prime}\\sim\\mathbb{P}(\\cdot\\mid Y=k)}\\left[\\kappa\\left(z_{x},z_{x^{\\prime}}\\right)\\right]},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\kappa(\\cdot,\\cdot)$ represents the similarity between two samples, when we use Gaussian kernel $\\kappa(z_{x},z_{x^{\\prime}})=\\exp(z_{x}\\cdot z_{x^{\\prime}})$ and approximate expectation through empirical batch $\\begin{array}{r}{B=\\cup_{k\\in[C]}B_{k}}\\end{array}$ , that is $\\begin{array}{r}{\\mathbb{E}_{\\pmb{x}^{\\prime}\\sim\\mathbb{P}(\\cdot|Y=y)}[\\kappa(z_{\\pmb{x}},z_{\\pmb{x}^{\\prime}})]\\approx\\frac{1}{|\\mathcal{B}_{y}|}\\sum_{\\pmb{x}^{\\prime}\\in\\mathcal{B}_{y}}\\exp(z_{\\pmb{x}}\\cdot z_{\\pmb{x}^{\\prime}})}\\end{array}$ , Eq. (10) can be instantiated as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{\\mathbb{P}}_{t}(Y=y\\mid z)=\\frac{\\frac{1}{|\\mathcal{B}_{y}|-1}\\sum_{\\pmb{x}^{\\prime}\\in\\mathcal{B}_{y}\\backslash\\{\\pmb{x}\\}}\\exp{(z_{\\pmb{x}}\\cdot z_{\\pmb{x}^{\\prime}})}}{\\sum_{\\pmb{k}\\in[C]}\\frac{1}{|\\mathcal{B}_{\\pmb{k}}|}\\sum_{\\pmb{x}^{\\prime}\\in\\mathcal{B}_{\\pmb{k}}}\\exp{(z_{\\pmb{x}}\\cdot z_{\\pmb{x}^{\\prime}})}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Interestingly, we observe that Eq. (11) resembles class-balanced contrastive loss. In the appendix, we also show that the Gaussian kernel approximation is identical to conventional supervised contrastive learning if the training data are class-balanced. ", "page_idx": 3}, {"type": "text", "text": "Notably, to ensure the computability of $\\mathbb{E}_{\\mathbf{x}^{\\prime}\\sim\\mathbb{P}(\\cdot|Y=y)}\\big[\\kappa\\big(z_{x},z_{x^{\\prime}}\\big)\\big]$ in Eq. (10), it is essential to ensure that samples are available from each class. Existing methods address this by class-wise queues, class-wise centers, or class-wise vMF distribution, details of which are provided in the appendix. ", "page_idx": 3}, {"type": "text", "text": "Based on the above three density approximation methods, we find that many recent proposals in long-tail learning can be derived from our framework. In Table 1, we summarize existing methods based on the way they estimate the density, tackle the training/test label shift, and guarantee the computation of Eq. (10) in mini-batch training. ", "page_idx": 3}, {"type": "text", "text": "3 CCL: Continuous Contrastive Learning ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we introduce the proposed algorithm CCL, which extends the class-balanced contrastive learning presented in Eq. (8) with Gaussian kernel estimation in Eq. (11) to LTSSL. ", "page_idx": 3}, {"type": "text", "text": "3.1 Problem setup of long-tailed semi-supervised learning ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Let $P_{l}$ and $P_{u}$ denote the joint distribution $(\\mathcal{X},\\mathcal{Y})$ for labeled data and unlabeled data, respectively. We denote by $\\mathbb{P}_{l}$ and $\\mathbb{P}_{u}$ the corresponding probability density (or mass) functions. We possess a labeled dataset $\\{(\\pmb{x}_{i}^{l},y_{i}^{l})\\}_{i=1}^{N}$ of size $N$ and an unlabeled dataset $\\{\\pmb{x}_{j}^{u}\\}_{j=1}^{M}$ of size $M$ , where $\\mathbf{\\Delta}\\mathbf{x}_{i}^{l},\\mathbf{x}_{j}^{u}\\,\\in\\,\\mathbb{R}^{d}$ . The proportion of labeled data from the entire dataset is $\\begin{array}{r}{\\eta\\,=\\,\\frac{N}{M+N}}\\end{array}$ . Denote the number of labeled data for class $i$ as $N_{i}$ , we have $N_{1}\\ge N_{2}\\ge\\dots\\ge N_{C}$ if the classes are sorted by cardinality in decreasing order. The imbalance ratio of labeled data is given by \u03b3l = NNC1 , while the distribution of the label of the unlabeled data and its imbalance ratio $\\gamma_{u}$ are unknown. The components of CCL include a feature extractor, two linear classifiers $f_{s}(\\cdot),f_{b}(\\cdot)$ and a contrastive feature projection head $g(\\cdot)$ . ", "page_idx": 3}, {"type": "text", "text": "3.2 Balanced classifier training with estimated class prior ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We develop our method based on FixMatch [56] following previous works [48, 64], and its objective is: $\\widehat{\\mathcal{L}}_{\\mathrm{ssl}}=\\widehat{\\mathcal{L}}_{l}+\\widehat{\\mathcal{L}}_{u}$ , where $\\widehat{\\mathcal{L}}_{l}$ is a traditional cross-entropy loss. For unlabeled data, the method operates by first generating pseudo-labels for unlabeled data using the model\u2019s predictions and selecting unlabeled data whose predicted maximum confidence is higher than a predefined threshold. The consistency regularizer $\\widehat{\\mathcal{L}}_{u}$ is then applied to two views of each selected sample. ", "page_idx": 4}, {"type": "text", "text": "Balanced FixMatch for LTSSL. First, since the labeled data follow a long-tailed distribution, which we denote as $\\pi^{l},\\widehat{\\mathcal{L}}_{l}$ needs to be adjusted by logit adjustment [44] via Eq. (8): ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\widehat{\\mathcal{L}}_{l}(\\pmb{x}^{l},\\pmb{y}^{l})=-\\log\\frac{\\widehat{\\mathbb{P}}_{t}\\left(Y=y^{l}\\mid\\pmb{x}^{l};f_{b}\\right)\\pi_{y^{l}}^{l}}{\\sum_{k\\in[C]}\\widehat{\\mathbb{P}}_{t}\\left(Y=k\\mid\\pmb{x}^{l};f_{b}\\right)\\pi_{k}^{l}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Second, FixMatch is prone to fti wrong pseudo-labels with high predictive confidence during training [62, 3]. However, since the unlabeled data label distribution is inaccessible, pseudo-labels generated by the classifier may be sub-optimal for its adherence to a uniform distribution. By Bayes\u2019 theorem, with given estimated unlabeled data label distribution $\\widehat{\\pi}^{u}$ , the \u201cpost-adjusted\u201d model outputs for sample $\\pmb{x}^{u}$ can be formulated as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\widehat{\\mathbb{P}}_{u}\\left(Y=y\\mid\\pmb{x}^{u};f_{b}\\right)=\\frac{\\widehat{\\mathbb{P}}_{t}\\left(Y=y\\mid\\pmb{x}^{u};f_{b}\\right)\\widehat{\\pi}_{y}^{u}}{\\sum_{k\\in\\left[C\\right]}\\widehat{\\mathbb{P}}_{t}\\left(Y=k\\mid\\pmb{x}^{u};f_{b}\\right)\\widehat{\\pi}_{k}^{u}},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Given pseudo-label $\\widehat{\\boldsymbol{y}}\\,=\\,\\arg\\operatorname*{max}_{\\boldsymbol{k}\\in\\left[C\\right]}\\widehat{\\mathbb{P}}_{u}\\left(Y=k\\mid\\mathcal{A}_{w}(\\pmb{x}^{u});f_{b}\\right)$ , where $A_{w}(\\cdot)$ denotes the weak data augmentation, $\\widehat{\\mathcal{L}}_{u}$ is rewritten as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\widehat{\\mathcal{L}}_{u}(\\pmb{x}^{u},\\widehat{y})=-\\mathcal{M}(\\pmb{x}^{u})\\log\\frac{\\widehat{\\mathbb{P}}_{t}\\left(Y=\\widehat{y}\\;\\vert\\;\\mathcal{A}_{s}(\\pmb{x}^{u});f_{b}\\right)\\widehat{\\pi}_{\\widehat{y}}^{u}}{\\sum_{k\\in[C]}\\widehat{\\mathbb{P}}_{t}\\left(Y=k\\;\\vert\\;\\mathcal{A}_{s}(\\pmb{x}^{u});f_{b}\\right)\\widehat{\\pi}_{k}^{u}},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathcal{M}(\\cdot)$ denotes the sample mask to select reliable pseudo-labels. We progressively update $\\widehat{\\pi}^{u}$ using the exponential moving average (EMA) for each mini-batch by $\\widehat{\\pi}_{y}^{u}\\ =\\ (1\\,-\\,\\alpha)\\widehat{\\pi}_{y}^{u}\\,+$ $\\begin{array}{r}{\\frac{\\alpha}{\\left|\\mathcal B\\right|}\\sum_{\\pmb{x}^{u}\\in\\mathcal B}\\widehat{\\mathbb P}_{u}\\left(Y=y\\mid\\pmb{x}^{u};f_{b}\\right)}\\end{array}$ , where $\\alpha$ is a momentum updating parameter and $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ denotes an unlabeled d ata subset. Directly using confidence selection can lead to a selected $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ with poor calibration due to model overconfidence [41, 45]. Thus, the energy score [36] is adopted to filter out reliable unlabeled data, which is defined as $\\begin{array}{r}{E(\\pmb{x})=-T\\cdot\\log\\sum_{k\\in[C]}e^{f_{k}(\\pmb{x})/T}}\\end{array}$ , where $T$ is the temperature and $f({\\boldsymbol{x}})$ denotes the logits of $\\textbf{\\em x}$ . We select reliable unlabeled data by $\\mathbf{\\mathcal{M}}^{E}(\\mathbf{x}^{u}):=$ $\\mathbb{I}(E(\\pmb{x}^{u})\\leq\\zeta)$ using a predefined threshold $\\zeta$ , and construct $\\mathcal{B}=\\{\\pmb{x}\\mid\\pmb{x}\\in\\mathcal{B}^{u}\\land\\mathcal{M}^{E}(\\pmb{x})\\neq0\\}$ for the estimation of $\\widehat{\\pi}_{u}$ . ", "page_idx": 4}, {"type": "text", "text": "Dual-branches training. Based on the observation that class-balanced training can be harmful to representation learning, previous works [38, 64] have utilized another branch of the network for standard training. In contrast to the balanced branch, the standard branch, denoted as $f_{s}(\\cdot)$ , optimizes the cross-entropy loss without employing logit adjustment to fit the original training data distribution. We fuse the predictions of balanced and standard branches to reduce the confirmation bias of pseudo-labels by: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\widehat{\\mathbb{P}}^{\\mathrm{cls}}\\left(Y=y\\ |\\ x^{u}\\right)=\\frac{1}{2}\\widehat{\\mathbb{P}}_{u}\\left(Y=y\\ |\\ x^{u};f_{b}\\right)+\\frac{1}{2}\\frac{\\widehat{\\mathbb{P}}\\left(Y=y\\ |\\ x^{u};f_{s}\\right)\\widehat{\\pi}_{y}^{*}}{\\sum_{k\\in[C]}\\widehat{\\mathbb{P}}\\left(Y=k\\ |\\ x^{u};f_{s}\\right)\\widehat{\\pi}_{k}^{*}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The rationale behind the equation is that the standard branch necessitates the elimination of imbalanced label prior and then compensates for unlabeled label prior when predicting pseudo-labels, which is achieved by defining $\\begin{array}{r}{\\widehat{\\pmb{\\pi}}^{*}=\\frac{\\widehat{\\pmb{\\pi}}^{u}}{\\pmb{\\pi}^{l}+\\widehat{\\pmb{\\pi}}^{u}}}\\end{array}$ \u03c0l\u03c0 +u\u03c0u . Overall, the classification loss L cls is the combination of losses for learning $f_{s}(\\cdot)$ and $f_{b}(\\cdot)$ . ", "page_idx": 4}, {"type": "text", "text": "3.3 Continuous contrastive loss with reliable pseudo-labels ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Apart from the classification loss and consistency regularizer, we aim to improve the quality of representations by extending the framework presented in Section 2 to LTSSL. To achieve the adaptation of our framework, a primary obstacle must be addressed. The challenge arises from the unknown ground-truth label $\\bar{\\mathbb{P}_{u}}(Y=\\bar{y}\\mid x)$ for unlabeled data, which results in the calculation of Eq. (8) infeasible. We propose to utilize the continuous pseudolabel ${\\widehat{\\mathbb{P}}}^{\\mathrm{cls}}(Y\\,=\\,y\\mid\\,\\pmb{x}^{u})$ as derived from the classifier in Eq. (15). Furthermore, $\\mathbb{E}_{\\mathbf{x}^{\\prime}\\sim\\mathbb{P}(\\cdot|Y=y)}\\big[\\kappa\\left(z_{x},z_{x^{\\prime}}\\right)\\big]$ in Eq. (10) can be extended to unlabeled data and approximated by an empirical data subset $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ : ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}_{{\\pmb x}^{\\prime}\\sim\\mathbb{P}(\\cdot|Y=y)}\\left[\\kappa\\left(z_{x},z_{x^{\\prime}}\\right)\\right]\\approx\\frac{\\sum_{{\\pmb x}^{\\prime}\\in\\mathcal{B}}\\kappa\\left(z_{\\pmb x},z_{{\\pmb x}^{\\prime}}\\right)\\widehat{\\mathbb{P}}^{\\mathrm{cls}}\\left(Y=y\\mid\\pmb x^{\\prime}\\right)}{\\sum_{{\\pmb x}^{\\prime}\\in\\mathcal{B}}\\widehat{\\mathbb{P}}^{\\mathrm{cls}}\\left(Y=y\\mid\\pmb x^{\\prime}\\right)}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Plugging Eq. (16) into Eq. (10), we obtain the continuous pseudo-label $\\widehat{\\mathbb{P}}_{t}(Y=y\\mid x^{u};B)$ for $\\pmb{x}^{u}$ . Similar to Eq. (14), logit adjustment is used to handle label shift of unlabel ed data by Bayes\u2019 theorem, and we can obtain: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\widehat{\\mathcal{L}}_{\\mathrm{rpl}}=-\\sum_{k\\in[C]}\\widehat{\\mathbb{P}}^{\\mathrm{cls}}(Y=k\\mid x^{u})\\cdot\\log\\widehat{\\mathbb{P}}_{u}(Y=k\\mid x^{u};B),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "$\\begin{array}{r}{\\widehat{\\mathbb{P}}_{u}(Y\\,=\\,y\\ |\\ \\pmb{x}^{u};\\mathcal{B})\\,=\\,\\frac{\\widehat{\\mathbb{P}}_{t}(Y=y|\\pmb{x}^{u};\\mathcal{B})\\cdot\\widehat{\\pmb{\\pi}}_{y}^{u}}{\\sum_{k\\in[C]}\\widehat{\\mathbb{P}}_{t}(Y=k|\\pmb{x}^{u};\\mathcal{B})\\cdot\\widehat{\\pmb{\\pi}}_{k}^{u}}}\\end{array}$ . So far, the generalized framework using continuous pseudo-labels for LTSSL i s derived in Eq. (17), the critical issue is how to filter out a reliable unlabeled data subset $B^{u}$ such that the posterior estimation $\\widehat{\\mathbb{P}}^{\\mathrm{cls}}(Y=y\\mid x)$ in Eq. (16) is calibrated. Similarly, directly using confidence selection may lead to model overconfidence. To mitigate the confirmation bias in pseudo-labels produced by the learned classifier, we propose using the energy score for data selection to ensure model calibration [26]. Combining with labeled data, the loss $\\bar{\\mathcal{L}}_{\\mathrm{rpl}}$ is obtained with ${\\mathcal{B}}=\\{{\\pmb x}\\mid{\\pmb x}\\in{\\mathcal{B}}^{l}\\vee({\\pmb x}\\in{\\mathcal{B}}^{u}\\wedge{\\mathcal{M}}^{E}({\\pmb x})\\neq0)\\}$ . ", "page_idx": 5}, {"type": "text", "text": "3.4 Continuous contrastive loss with smoothed pseudo-labels ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To further mitigate the impact of inaccurate pseudo-labels $\\widehat{\\mathbb{P}}^{\\mathrm{cls}}(Y\\,=\\,y\\,\\mid\\,\\pmb{x}^{u})$ , we derive a complementary contrastive loss with smoothed pseudo-labels. Specifically, we propose aligning the representations of two views of a sample by imposing the weak-strong consistency regularization: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\widehat{\\mathcal{L}}_{\\mathrm{spl}}=-\\sum_{k\\in[C]}\\widehat{\\mathbb{P}}\\left(Y=k\\mid\\mathcal{A}_{w}\\left(\\pmb{x}^{u}\\right)\\right)\\log\\widehat{\\mathbb{P}}\\left(Y=k\\mid\\mathcal{A}_{s}\\left(\\pmb{x}^{u}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In this part, we aim to derive ${\\widehat{\\mathbb{P}}}(Y=y\\mid x^{u})$ by propagating labels from nearby samples in the contrastive space. On the one hand, we take labeled data for $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ in Eq. (11) and construct the posterior for unlabeled data. Logit adjustment is employed for tackling label shift of unlabeled data: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{\\mathbb{P}}(Y=y\\mid x^{u};\\mathcal{B}^{l})=\\frac{\\frac{1}{|\\mathcal{B}_{y}|}\\sum_{x^{\\prime}\\in\\mathcal{B}_{y}}\\kappa\\left(z_{x^{u}},z_{x^{\\prime}}\\right)\\cdot\\widehat{\\pi}_{y}^{u}}{\\sum_{k\\in[C]}\\frac{1}{|\\mathcal{B}_{k}|}\\sum_{x^{\\prime}\\in\\mathcal{B}_{k}}\\kappa\\left(z_{x^{u}},z_{x^{\\prime}}\\right)\\cdot\\widehat{\\pi}_{k}^{u}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Eq. (19) can be viewed as a process of propagating labels from labeled data to unlabeled data. On the other hand, we consider label propagation within unlabeled data, i.e., an unlabeled batch $B^{u}$ is used to estimate $\\widehat{\\mathbb{P}}(Y=y\\mid x^{u};B)$ . Assuming there is a sufficient amount of unlabeled data, we have $\\begin{array}{r}{\\frac{1}{|\\mathcal{B}^{u}|}\\sum_{\\pmb{x}^{u}\\in\\mathcal{B}^{u}}\\widehat{\\mathbb{P}}(Y=y\\mid\\pmb{x}^{u};\\mathcal{B}^{u})\\approx\\widehat{\\pi}_{y}^{u}}\\end{array}$ , hence the posterior can be approximated as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\widehat{\\mathbb{P}}(Y=y\\mid x^{u};\\mathcal{B}^{u})\\approx\\frac{\\sum_{x^{\\prime}\\in\\mathcal{B}^{u}}\\kappa\\left(z_{x^{u}},z_{x^{\\prime}}\\right)\\widehat{\\mathbb{P}}\\left(Y=y\\mid x^{\\prime};\\mathcal{B}^{u}\\right)}{\\sum_{x^{\\prime}\\in\\mathcal{B}^{u}}\\kappa\\left(z_{x^{u}},z_{x^{\\prime}}\\right)}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Let $\\mathbb{P}(Y\\mid X;B)$ represent a matrix stacked by $[\\mathbb{P}(Y=1\\mid x),.\\,.\\,.\\,.\\,,\\mathbb{P}(Y=C\\mid x)]^{\\intercal}$ of $\\textbf{\\em x}$ from $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ , we can rewrite Eq. (20) in the form of matrix multiplication: $\\widehat{\\mathbb{P}}\\left(Y\\mid X^{u};\\beta^{u}\\right)=G\\cdot\\widehat{\\mathbb{P}}\\left(Y\\mid X^{u};\\beta^{u}\\right),$ where $\\boldsymbol{G}$ is a similarity matrix and $\\begin{array}{r}{G_{i j}\\,=\\,\\frac{\\kappa(z_{{\\bf x}_{i}},z_{{\\bf x}_{j}})}{\\sum_{{\\bf x}_{j}\\in\\mathcal{B}_{u}}\\kappa(z_{{\\bf x}_{i}},z_{{\\bf x}_{j}})}}\\end{array}$ . It can be interpreted that similar samples possess similar labels. By aggregating the predictions of labeled data and unlabeled data with a fixed hyperparameter $\\beta$ , we obtain: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{\\mathbb{P}}\\left(Y\\mid X^{u}\\right)=\\beta\\pmb{G}\\cdot\\widehat{\\mathbb{P}}\\left(Y\\mid\\pmb{X}^{u}\\right)+(1-\\beta)\\widehat{\\mathbb{P}}\\left(Y\\mid\\pmb{X}^{u};\\pmb{B}^{l}\\right)}\\\\ &{\\quad\\Rightarrow\\widehat{\\mathbb{P}}\\left(Y\\mid\\pmb{X}^{u}\\right)=(1-\\beta)(\\pmb{I}-\\beta\\pmb{G})^{-1}\\cdot\\widehat{\\mathbb{P}}\\left(Y\\mid\\pmb{X}^{u};\\pmb{B}^{l}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Subsequently, Eq. (21) can be plugged into Eq. (18) for calculating $\\widehat{\\mathcal{L}}_{\\mathrm{spl}}$ . To sum up, the total objective of CCL is: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\widehat{\\mathcal{L}}_{\\mathrm{total}}=\\lambda_{1}\\widehat{\\mathcal{L}}_{\\mathrm{cls}}+\\left(1-\\lambda_{1}\\right)\\widehat{\\mathcal{L}}_{\\mathrm{rpl}}+\\lambda_{2}\\widehat{\\mathcal{L}}_{\\mathrm{spl}}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\lambda_{1}$ and $\\lambda_{2}$ are two hyperparamete rs. ", "page_idx": 5}, {"type": "table", "img_path": "PaqJ71zf1M/tmp/b1e0cafa2d9fdfa1e3d884eba567165eae5520f8e0bd98c1a66ae863715392a4.jpg", "table_caption": ["Table 2: Test accuracy in consistent setting on CIFAR10-LT and CIFAR100-LT datasets. The best results are in bold. "], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "PaqJ71zf1M/tmp/2e0529b18a14bb62ca6b63567fb7cc46608c30750a68228c66d48bf885098a9f.jpg", "table_caption": ["Table 3: Test accuracy under inconsistent setting $(\\gamma_{l}\\neq\\gamma_{u})$ ) on CIFAR10-LT and STL10-LT datasets. $\\gamma_{l}=100$ for CIFAR10-LT, and 10 and 20 for STL10-LT dataset. The best results are in bold. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we conducted comprehensive experiments to verify the effectiveness of the proposed continuous contrastive learning method (CCL) on CIFAR10-LT, CIFAR100-LT, STL10-LT, and ImageNet-127 [29, 18] datasets. To simulate real-world unlabeled data, we tested our method on diverse label distributions of unlabeled data. Due to limited space, we defer the detailed experimental settings to the appendix. ", "page_idx": 6}, {"type": "text", "text": "4.1 Results on CIFAR10/100-LT and STL10-LT ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "For consistent $\\langle\\gamma_{l}=\\gamma_{u}\\rangle$ ) setting, results are presented in Table 2. From the results, we can see that CCL consistently outperforms all comparison methods by a large margin. In particular, CCL improves the previous state-of-the-art approach ACR by $2.8\\%$ on average. This verifies that the representations learned by our proposed contrastive losses are more discriminative because both CCL and ACR utilize a dual-branch network. ", "page_idx": 6}, {"type": "text", "text": "For inconsistent $(\\gamma_{l}\\neq\\gamma_{u})$ setting, we present the results in Table 3 and Table 4. Following prior works, we compare all methods using unlabeled data following uniform and reversed label distributions on CIFAR10/100-LT datasets. On the STL10-LT dataset, the underlying unlabeled data distribution is naturally inaccessible. In general, CCL achieves the best results in all settings. Particularly, CCL obtains an average performance gain of $1.6\\%$ over ACR without using predefined anchor distributions. The results indicate that our method is able to accurately estimate the unlabeled data distribution with calibrated pseudo-labels. ", "page_idx": 6}, {"type": "text", "text": "4.2 Results on ImageNet-127 ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "ImageNet-127 is a naturally long-tailed dataset and has been used to test LTSSL methods in the recent literature [22, 64]. Following previous works, we downsample the original images to smaller sizes of $32\\!\\times\\!32$ or $64\\!\\times\\!64$ pixels using the box method from the Pillow library and randomly select $10\\%$ training samples to construct the labeled data. Learning discriminative representations and a balanced classifier is essential to achieve high performance. From the results in Table 5, we can see that CCLachieves superior results for image sizes of $32\\!\\times\\!32$ and $64\\!\\times\\!64$ . It outperforms ACR by $4.3\\%$ and $4.2\\%$ in test accuracy. ", "page_idx": 7}, {"type": "table", "img_path": "PaqJ71zf1M/tmp/1aefb6cd835da736edca8568ba4ecd1de09cbd448bf3c7275eecf010b783daad.jpg", "table_caption": ["Table 4: Test accuracy on CIFAR100-LT in uniform and reversed settings. The best results are in bold. ", "Table 6: Ablation studies of our proposed algorithm. \u201cCon\u201d, \u201cUni\u201d, and \u201cRev\u201d represent consistent, uniform, and reversed, respectively. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "PaqJ71zf1M/tmp/7e27a7b8e782a959324cfd1cc3eea528a2602c06a24725f15a9b9e79c4e550b4.jpg", "table_caption": ["Table 5: Test accuracy on ImageNet-127. The best results are in bold. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.3 Comprehensive evaluation of the proposed method ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Understanding of balanced Fixmatch and dual-branch. First, balanced Fixmatch can be viewed as a separate EM algorithm process [17, 21], where the E-step involves estimating suitable pseudo-labels for unlabeled data through $\\widehat{\\pi}^{u}$ , and the M-step updates the model and obtains a new $\\Bar{\\pi}^{u}$ . As can be seen in Table 6, balanced Fixmatch achieves performance comparable to the recent state-of-the-art method ACR. Furthermore, dual-branch significantly enhances the performance of data under highly skewed long-tail distributions (consistent setting), with an averaged $1.5\\%$ improvement. ", "page_idx": 7}, {"type": "image", "img_path": "PaqJ71zf1M/tmp/fe3b6c1ed8d70a076c3127d30924eacf7a4775e8bdbab7b03e9f18a2fc54c078.jpg", "img_caption": ["Figure 1: Comparison of class prior estimation error and ECE on CIFAR100-LT. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "How to estimate a relatively accurate $\\widehat{\\pi}_{u}\\widehat{\\prime}$ Our method for estimating $\\widehat{\\pi}_{u}$ is equivalent to MLLS [53], which is an EM process. Accura te estimation of $\\widehat{\\pi}_{u}$ is only achi evable when the model is calibrated [25]. Since the confirmation bias is induced by self-training, using confidence selection may result in overconfident but wrong pseudo-labels and hurt the calibration [45, 41]. In contrast, the energy score leverages the probability density of the predictions, exhibiting reduced vulnerability to overconfidence [39]. Thus, we propose energy selection for a reliable unlabeled data subset on which the model is calibrated, thereby enabling the accurate estimation of $\\widehat{\\pi}_{u}$ . We use expected calibration error [26] (ECE) to assess model calibration. The tail of the curve of Figure 1c and 1d can be interpreted as overconfidence in false pseudo-labels caused by self-training. As can be seen in Figure 1a and 1b, the $L_{1}$ distance between the true class prior of unlabeled data and ${\\widehat{\\pmb{\\pi}}}_{u}$ , estimated from data subset selected using energy, is significantly smaller compared to when confidence is used for selection, inducing a more balanced classifier training. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Continuous contrastive learning with reliable pseudo-labels. We carried out a comparative experiment by removing the continuous reliable pseudo-labels loss. The results reflect an averaged $0.\\bar{8}\\%$ drop on CIFAR10/100-LT, demonstrating its efficacy for learning high-quality representation. Moreover, we verified that the data subset filtered by energy selection obtains excellent model calibration. Figure 1c and 1d show energy achieves better calibration than confidence thresholding. ", "page_idx": 8}, {"type": "text", "text": "Continuous contrastive learning with smoothed pseudo-labels. Similarly, we conducted a comparative experiment by removing the continuous smoothed pseudo-labels loss. As can be seen in Table 6, the performance decreases in all three settings on CIFAR10/100-LT datasets, showing the necessity for a consistency regularization constraint for feature alignment within the contrastive learning space. ", "page_idx": 8}, {"type": "text", "text": "4.4 Results under more class distributions ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Similar to ACR, to evaluate our method\u2019s effectiveness under more imbalanced settings, we conducted further experiments on CIFAR100- LT, maintaining a fixed $\\gamma_{l}=20$ and adjusting the imbalance ratio $\\gamma_{u}$ of the unlabeled data from consistent to reversed. We set $N_{1}\\,=\\,50$ and $M_{1}~=~400$ (with $M_{C}~~=~400$ in the reversed scenario) and compared the results with ACR as shown in Figure 2. The results demonstrate that our method consistently outperforms ACR in all scenarios. ", "page_idx": 8}, {"type": "image", "img_path": "PaqJ71zf1M/tmp/54ce462eb6a8e627385ebee9165d390ec7370dedea6444ecf04917e719f1cac0.jpg", "img_caption": ["(a) CIFAR10-LT $\\gamma_{l}=100$ (b) CIFAR100-LT $\\gamma_{l}=20$ "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 2: Generalize to more realistic LTSSL settings for ACR and CCL on CIFAR10/100-LT dataset in fixed $\\gamma_{l}$ and various $\\gamma_{u}$ settings. ", "page_idx": 8}, {"type": "text", "text": "5 Related Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Long-tailed learning (LTL). Early strategies tackling LTL involve two aspects: resampling and reweighting. Resampling methods [9, 5, 8, 54] either undersample majority classes or oversample minority classes, which may result in information loss or overftiting. Reweighting methods [52, 16, 2, 12] assign different weights for each class or training sample. BBN [72] and Decoupling [31] claim that re-balancing can negatively impact representation. They propose a two-branch structure or a two-stage paradigm to address it. Logit adjustment methods [7, 44] learn larger margins for minority classes by obtaining optimal Bayesian classifiers. Recently, several methods [30, 15, 73, 20, 57] have been proposed to improve the representation learning based on supervised contrastive learning [32]. ", "page_idx": 8}, {"type": "text", "text": "Long-tailed semi-supervised learning (LTSSL). Most semi-supervised learning (SSL) methods use unlabeled data by assigning pseudo-labels to unlabeled data [37, 6, 56, 71, 10] or aligning predictions of different views of the input by consistency regularization [59]. PAWS [4] leverages self-supervised representations derived from unlabeled data, and RoPAWS [46] further refines the model predictions using labeled data through kernel density estimation. However, most of these works assume a balanced class distribution of labeled and unlabeled data, which may be violated in real-world applications. ", "page_idx": 8}, {"type": "text", "text": "Recently, LTSSL has gained considerable attention due to its applicability in numerous real-life scenarios. Recent works mitigate pseudo-labels bias by distribution alignment or label refinement [33, 63, 69]. Some others focus on balanced classifier training to overcome long-tailed label distribution [38, 22, 66]. Regrettably, these methods simply assume an identical long-tailed distribution for labeled and unlabeled data, which may still be unrealistic. Considering the unknown unlabeled data distribution, which can be mismatched with the labeled distribution, DASO [48] mixes the outputs of linear and semantic classifiers to improve the quality of pseudo-labels. ACR [64] and CPE [43] refine consistency regularization or train multiple expert branches based on predefined anchor distributions. However, how to improve representation learning in LTSSL is ignored in most existing works. ", "page_idx": 8}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper presents a probabilistic framework that unifies many recent methods in long-tail learning. Our framework is equivalent to supervised contrastive learning when approximating the classconditional function using the Gaussian kernel. We further extend the contrastive learning objective to LTSSL based on continuous pseudo-labels to improve the learned representations. We utilize both reliable pseudo-labels generated by the model and smoothed pseudo-labels propagated from nearby samples to mitigate confirmation bias. Extensive experiments demonstrate that our proposed method achieves state-of-the-art performance in all settings. We hope that our work can motivate more research for LTSSL from the perspective of representation learning. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by the National Science Foundation of China (62206049, 62225602), and the Big Data Computing Center of Southeast University. We would like to thank anonymous reviewers for their constructive suggestions. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Alexander A Alemi, Ian Fischer, Joshua V Dillon, and Kevin Murphy. Deep variational information bottleneck. International Conference on Learning Representations, 2017.   \n[2] Shaden Alshammari, Yu-Xiong Wang, Deva Ramanan, and Shu Kong. Long-tailed recognition via weight balancing. In IEEE Conference on Computer Vision and Pattern Recognition, pages 6897\u20136907, 2022.   \n[3] Eric Arazo, Diego Ortego, Paul Albert, Noel E O\u2019Connor, and Kevin McGuinness. Pseudo-labeling and confirmation bias in deep semi-supervised learning. In International Joint Conference on Neural Networks (IJCNN), pages 1\u20138, 2020.   \n[4] Mahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bojanowski, Armand Joulin, Nicolas Ballas, and Michael Rabbat. Semi-supervised learning of visual features by non-parametrically predicting view assignments with support samples. In IEEE Conference on Computer Vision and Pattern Recognition, pages 8443\u20138452, 2021.   \n[5] Colin Bellinger, Roberto Corizzo, and Nathalie Japkowicz. Calibrated resampling for imbalanced and long-tails in deep learning. In Discovery Science: 24th International Conference, DS 2021, Halifax, NS, Canada, October 11\u201313, 2021, Proceedings 24, pages 242\u2013252. Springer, 2021.   \n[6] David Berthelot, Nicholas Carlini, Ekin D Cubuk, Alex Kurakin, Kihyuk Sohn, Han Zhang, and Colin Raffel. Remixmatch: Semi-supervised learning with distribution alignment and augmentation anchoring. ArXiv Preprint ArXiv:1911.09785, 2019.   \n[7] Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, and Tengyu Ma. Learning imbalanced datasets with label-distribution-aware margin loss. In Advances in Neural Information Processing Systems, volume 32, pages 1567\u20131578, 2019.   \n[8] Nadine Chang, Zhiding Yu, Yu-Xiong Wang, Animashree Anandkumar, Sanja Fidler, and Jose M Alvarez. Image-level or object-level? a tale of two resampling strategies for long-tailed detection. In International Conference on Machine Learning, pages 1463\u20131472. PMLR, 2021.   \n[9] Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, and W Philip Kegelmeyer. Smote: synthetic minority over-sampling technique. Journal of Artificial Intelligence Research, 16:321\u2013357, 2002.   \n[10] Hao Chen, Ran Tao, Yue Fan, Yidong Wang, Jindong Wang, Bernt Schiele, Xing Xie, Bhiksha Raj, and Marios Savvides. Softmatch: Addressing the quantity-quality trade-off in semi-supervised learning. ArXiv Preprint ArXiv:2301.10921, 2023.   \n[11] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International Conference on Machine Learning, volume 119, pages 1597\u20131607, 2020.   \n[12] Xiaohua Chen, Yucan Zhou, Dayan Wu, Chule Yang, Bo Li, Qinghua Hu, and Weiping Wang. Area: adaptive reweighting via effective area for long-tailed classification. In IEEE International Conference on Computer Vision, pages 19277\u201319287, 2023.   \n[13] Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised feature learning. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, pages 215\u2013223. JMLR Workshop and Conference Proceedings, 2011.   \n[14] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data augmentation with a reduced search space. In Advances in Neural Information Processing Systems, 2020.   \n[15] Jiequan Cui, Zhisheng Zhong, Shu Liu, Bei Yu, and Jiaya Jia. Parametric contrastive learning. In IEEE International Conference on Computer Vision, pages 715\u2013724, 2021.   \n[16] Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced loss based on effective number of samples. In IEEE Conference on Computer Vision and Pattern Recognition, pages 9268\u20139277, 2019.   \n[17] AP Dempter. Maximum likelihood from incomplete data via the em algorithm. Journal of Royal Statistical Society, 39:1\u201322, 1977.   \n[18] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In IEEE Conference on Computer Vision and Pattern Recognition, pages 248\u2013255, 2009.   \n[19] Terrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks with cutout. ArXiv Preprint ArXiv:1708.04552, 2017.   \n[20] Chaoqun Du, Yulin Wang, Shiji Song, and Gao Huang. Probabilistic contrastive learning for long-tailed visual recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024.   \n[21] Marthinus Christoffel Du Plessis and Masashi Sugiyama. Semi-supervised learning of class balance under class-prior change by distribution matching. Neural Networks, 50:110\u2013119, 2014.   \n[22] Yue Fan, Dengxin Dai, Anna Kukleva, and Bernt Schiele. Cossl: Co-learning of representation and classifier for imbalanced semi-supervised learning. In IEEE Conference on Computer Vision and Pattern Recognition, pages 14574\u201314584, 2022.   \n[23] Kai Gan and Tong Wei. Erasing the bias: Fine-tuning foundation models for semi-supervised learning. In Proceedings of the 41st International Conference on Machine Learning, pages 14453\u201314470, 2024.   \n[24] Kai Gan, Tong Wei, and Min-Ling Zhang. Boosting consistency in dual training for long-tailed semisupervised learning. arXiv preprint arXiv:2406.13187, 2024.   \n[25] Saurabh Garg, Yifan Wu, Sivaraman Balakrishnan, and Zachary Lipton. A unified view of label shift estimation. Advances in Neural Information Processing Systems, 33:3290\u20133300, 2020.   \n[26] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In International Conference on Machine Learning, pages 1321\u20131330. PMLR, 2017.   \n[27] Boran Han. Wrapped cauchy distributed angular softmax for long-tailed visual recognition. In International Conference on Machine Learning, pages 12368\u201312388. PMLR, 2023.   \n[28] Youngkyu Hong, Seungju Han, Kwanghee Choi, Seokjun Seo, Beomsu Kim, and Buru Chang. Disentangling label distribution for long-tailed visual recognition. In IEEE Conference on Computer Vision and Pattern Recognition, pages 6626\u20136636, June 2021.   \n[29] Minyoung Huh, Pulkit Agrawal, and Alexei A Efros. What makes imagenet good for transfer learning? ArXiv Preprint ArXiv:1608.08614, 2016.   \n[30] Bingyi Kang, Yu Li, Sa Xie, Zehuan Yuan, and Jiashi Feng. Exploring balanced feature spaces for representation learning. In International Conference on Learning Representations, 2020.   \n[31] Bingyi Kang, Saining Xie, Marcus Rohrbach, Zhicheng Yan, Albert Gordo, Jiashi Feng, and Yannis Kalantidis. Decoupling representation and classifier for long-tailed recognition. In International Conference on Learning Representations, 2020.   \n[32] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. Advances in Neural Information Processing Systems, 33:18661\u201318673, 2020.   \n[33] Jaehyung Kim, Youngbum Hur, Sejun Park, Eunho Yang, Sung Ju Hwang, and Jinwoo Shin. Distribution aligning refinery of pseudo-label for imbalanced semi-supervised learning. In Advances in Neural Information Processing Systems, 2020.   \n[34] Takumi Kobayashi. T-vmf similarity for regularizing intra-class feature distribution. In IEEE Conference on Computer Vision and Pattern Recognition, pages 6616\u20136625, 2021.   \n[35] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.   \n[36] Yann LeCun, Sumit Chopra, Raia Hadsell, M Ranzato, and Fujie Huang. A tutorial on energy-based learning. Predicting Structured Data, 1(0), 2006.   \n[37] Dong-Hyun Lee. Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks. In Workshop On Challenges In Representation Learning, ICML, 2013.   \n[38] Hyuck Lee, Seungjae Shin, and Heeyoung Kim. Abc: Auxiliary balanced classifier for class-imbalanced semi-supervised learning. Advances in Neural Information Processing Systems, 34:7082\u20137094, 2021.   \n[39] Weitang Liu, Xiaoyun Wang, John Owens, and Yixuan Li. Energy-based out-of-distribution detection. Advances in Neural Information Processing Systems, 33:21464\u201321475, 2020.   \n[40] Ziwei Liu, Zhongqi Miao, Xiaohang Zhan, Jiayun Wang, Boqing Gong, and Stella X Yu. Large-scale long-tailed recognition in an open world. In IEEE Conference on Computer Vision and Pattern Recognition, pages 2537\u20132546, 2019.   \n[41] Charlotte Loh, Rumen Dangovski, Shivchander Sudalairaj, Seungwook Han, Ligong Han, Leonid Karlinsky, Marin Soljacic, and Akash Srivastava. On the importance of calibration in semi-supervised learning. ArXiv Preprint ArXiv:2210.04783, 2022.   \n[42] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. ArXiv Preprint ArXiv:1608.03983, 2016.   \n[43] Chengcheng Ma, Ismail Elezi, Jiankang Deng, Weiming Dong, and Changsheng Xu. Three heads are better than one: Complementary experts for long-tailed semi-supervised learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 14229\u201314237, 2024.   \n[44] Aditya Krishna Menon, Sadeep Jayasumana, Ankit Singh Rawat, Himanshu Jain, Andreas Veit, and Sanjiv Kumar. Long-tail learning via logit adjustment. In International Conference on Learning Representations, 2021.   \n[45] Shambhavi Mishra, Balamurali Murugesan, Ismail Ben Ayed, Marco Pedersoli, and Jose Dolz. Do not trust what you trust: Miscalibration in semi-supervised learning. ArXiv Preprint ArXiv:2403.15567, 2024.   \n[46] Sangwoo Mo, Jong-Chyi Su, Chih-Yao Ma, Mido Assran, Ishan Misra, Licheng Yu, and Sean Bell. Ropaws: Robust semi-supervised representation learning from uncurated data. ArXiv Preprint ArXiv:2302.14483, 2023.   \n[47] Yurii Nesterov. A method of solving a convex programming problem with convergence rate $o(1/k^{2})$ . Doklady Akademii Nauk SSSR, 269:543, 1983.   \n[48] Youngtaek Oh, Dong-Jin Kim, and In So Kweon. Daso: Distribution-aware semantics-oriented pseudolabel for imbalanced semi-supervised learning. In IEEE Conference on Computer Vision and Pattern Recognition, pages 9786\u20139796, 2022.   \n[49] Boris T Polyak. Some methods of speeding up the convergence of iteration methods. Ussr Computational Mathematics and Mathematical Physics, 4:1\u201317, 1964.   \n[50] Harsh Rangwani, Sumukh K Aithal, Mayank Mishra, et al. Escaping saddle points for effective generalization on class-imbalanced data. Advances in Neural Information Processing Systems, 35:22791\u201322805, 2022.   \n[51] Jiawei Ren, Cunjun Yu, shunan sheng, Xiao Ma, Haiyu Zhao, Shuai Yi, and hongsheng Li. Balanced meta-softmax for long-tailed visual recognition. In Advances in Neural Information Processing Systems, volume 33, pages 4175\u20134186, 2020.   \n[52] Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun. Learning to reweight examples for robust deep learning. In International Conference on Machine Learning, pages 4334\u20134343. PMLR, 2018.   \n[53] Marco Saerens, Patrice Latinne, and Christine Decaestecker. Adjusting the outputs of a classifier to new a priori probabilities: a simple procedure. Neural Computation, 14(1):21\u201341, 2002.   \n[54] Jiang-Xin Shi, Tong Wei, Yuke Xiang, and Yu-Feng Li. How re-sampling helps for long-tail learning? Advances in Neural Information Processing Systems, 36:75669\u201375687, 2023.   \n[55] Jiang-Xin Shi, Tong Wei, Zhi Zhou, Jie-Jing Shao, Xin-Yan Han, and Yu-Feng Li. Long-tail learning with foundation model: Heavy fine-tuning hurts. In Proceedings of the 41st International Conference on Machine Learning, pages 45014\u201345039, 2024.   \n[56] Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao Zhang, Nicholas Carlini, Ekin D Cubuk, Alex Kurakin, Han Zhang, and Colin Raffel. Fixmatch: Simplifying semi-supervised learning with consistency and confidence. In Advances in Neural Information Processing Systems, 2020.   \n[57] Min-Kook Suh and Seung-Woo Seo. Long-tailed recognition by mutual information maximization between latent features and ground-truth labels. In International Conference on Machine Learning, pages 32770\u2013 32782. PMLR, 2023.   \n[58] Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initialization and momentum in deep learning. In International Conference on Machine Learning, pages 1139\u20131147. PMLR, 2013.   \n[59] Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. In Advances in Neural Information Processing Systems, volume 30, pages 1195\u20131204, 2017.   \n[60] Naftali Tishby, Fernando C Pereira, and William Bialek. The information bottleneck method. ArXiv Preprint Physics/0004057, 2000.   \n[61] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of Machine Learning Research, 9(11), 2008.   \n[62] Xudong Wang, Zhirong Wu, Long Lian, and Stella X Yu. Debiased learning from naturally imbalanced pseudo-labels for zero-shot and semi-supervised learning. ArXiv Preprint ArXiv:2201.01490, 2022.   \n[63] Chen Wei, Kihyuk Sohn, Clayton Mellina, Alan Yuille, and Fan Yang. Crest: A class-rebalancing selftraining framework for imbalanced semi-supervised learning. In IEEE Conference on Computer Vision and Pattern Recognition, 2021.   \n[64] Tong Wei and Kai Gan. Towards realistic long-tailed semi-supervised learning: Consistency is all you need. In IEEE Conference on Computer Vision and Pattern Recognition, pages 3469\u20133478, 2023.   \n[65] Tong Wei and Yu-Feng Li. Does tail label help for large-scale multi-label learning? IEEE transactions on neural networks and learning systems, 31(7):2315\u20132324, 2019.   \n[66] Tong Wei, Qian-Yu Liu, Jiang-Xin Shi, Wei-Wei Tu, and Lan-Zhe Guo. Transfer and share: semi-supervised learning from long-tailed data. Machine Learning, pages 1\u201318, 2022.   \n[67] Tong Wei, Zhen Mao, Zi-Hao Zhou, Yuanyu Wan, and Min-Ling Zhang. Learning label shift correction for test-agnostic long-tailed recognition. In Proceedings of the 41st International Conference on Machine Learning, pages 52611\u201352631, 2024.   \n[68] Tong Wei, Jiang-Xin Shi, Wei-Wei Tu, and Yu-Feng Li. Robust long-tailed learning under label noise. ArXiv Preprint ArXiv:2108.11569, 2021.   \n[69] Zhuoran Yu, Yin Li, and Yong Jae Lee. Inpl: Pseudo-labeling the inliers first for imbalanced semisupervised learning. ArXiv Preprint ArXiv:2303.07269, 2023.   \n[70] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In British Machine Vision Conference, pages 87.1\u201387.12, 2016.   \n[71] Bowen Zhang, Yidong Wang, Wenxin Hou, Hao Wu, Jindong Wang, Manabu Okumura, and Takahiro Shinozaki. Flexmatch: Boosting semi-supervised learning with curriculum pseudo labeling. Advances in Neural Information Processing Systems, 34:18408\u201318419, 2021.   \n[72] Boyan Zhou, Quan Cui, Xiu-Shen Wei, and Zhao-Min Chen. Bbn: Bilateral-branch network with cumulative learning for long-tailed visual recognition. In IEEE Conference on Computer Vision and Pattern Recognition, pages 9719\u20139728, 2020.   \n[73] Jianggang Zhu, Zheng Wang, Jingjing Chen, Yi-Ping Phoebe Chen, and Yu-Gang Jiang. Balanced contrastive learning for long-tailed visual recognition. In IEEE Conference on Computer Vision and Pattern Recognition, pages 6908\u20136917, 2022. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Comparison with Supervised Contrastive Learning ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "As we have derived in Eq. (10) and use the Gaussian kernel density estimation in Eq. (11), if we simply assume the data are class-balanced, it simplifies to: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\widehat{\\mathbb{P}}(Y=y\\mid z)=\\frac{\\Big(\\frac{1}{|\\mathcal{B}_{y}|-1}\\sum_{x^{\\prime}\\in\\mathcal{B}_{y}\\setminus\\{x\\}}\\exp\\big(z_{x}\\cdot z_{x^{\\prime}}\\big)\\Big)\\,\\mathbb{P}(Y=y)}{\\sum_{k\\in[C]}\\Big(\\frac{1}{|\\mathcal{B}_{k}|}\\sum_{x^{\\prime}\\in\\mathcal{B}_{k}}\\exp\\big(z_{x}\\cdot z_{x^{\\prime}}\\big)\\Big)\\,\\mathbb{P}(Y=k)}}\\\\ {=\\frac{\\sum_{x^{\\prime}\\in\\mathcal{B}_{y}\\setminus\\{x\\}}\\exp\\big(z_{x}\\cdot z_{x^{\\prime}}\\big)}{\\sum_{k\\in[C]}\\sum_{x^{\\prime}\\in\\mathcal{B}_{k}}\\exp\\big(z_{x}\\cdot z_{x^{\\prime}}\\big)}=\\frac{\\sum_{x^{\\prime}\\in\\mathcal{B}_{y}\\setminus\\{x\\}}\\exp\\big(z_{x}\\cdot z_{x^{\\prime}}\\big)}{\\sum_{x^{\\prime}\\in\\mathcal{B}}\\exp\\big(z_{x}\\cdot z_{x^{\\prime}}\\big)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The loss of supervised contrastive learning has two forms, i.e., $\\widehat{\\mathcal{L}}_{\\mathrm{scl}}^{\\mathrm{in}}$ and $\\widehat{\\mathcal{L}}_{\\mathrm{scl}}^{\\mathrm{out}}$ , which is distinguished by the position of summation over positive samples at the $\\log(\\cdot)$ . Thus,  we get: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\widetilde{\\mathcal{L}}_{\\mathrm{scl}}^{\\mathrm{in}}(\\boldsymbol{x},\\boldsymbol{y})=-\\log\\frac{\\sum_{\\boldsymbol{x}^{\\prime}\\in B_{y}\\backslash\\{\\boldsymbol{x}\\}}\\exp\\big(\\boldsymbol{z}_{\\boldsymbol{x}}\\cdot\\boldsymbol{z}_{\\boldsymbol{x}^{\\prime}}\\big)}{\\sum_{\\boldsymbol{x}^{\\prime}\\in B}\\exp\\big(\\boldsymbol{z}_{\\boldsymbol{x}}\\cdot\\boldsymbol{z}_{\\boldsymbol{x}^{\\prime}}\\big)}}}\\\\ &{}&{\\propto-\\log\\frac{\\frac{1}{|B_{y}|-1}\\sum_{\\boldsymbol{x}^{\\prime}\\in B_{y}\\backslash\\{\\boldsymbol{x}\\}}\\exp\\big(\\boldsymbol{z}_{\\boldsymbol{x}}\\cdot\\boldsymbol{z}_{\\boldsymbol{x}^{\\prime}}\\big)}{\\sum_{\\boldsymbol{x}^{\\prime}\\in B}\\exp\\big(\\boldsymbol{z}_{\\boldsymbol{x}}\\cdot\\boldsymbol{z}_{\\boldsymbol{x}^{\\prime}}\\big)}}\\\\ {\\mathrm{\\normalfont~Jensen~}}&{}&{\\xrightarrow{\\mathrm{Iensen}}-\\frac{1}{|B_{y}|-1}\\sum_{\\boldsymbol{x}^{\\prime}\\in B_{y}\\backslash\\{\\boldsymbol{x}\\}}\\log\\frac{\\exp\\big(\\boldsymbol{z}_{\\boldsymbol{x}}\\cdot\\boldsymbol{z}_{\\boldsymbol{x}^{\\prime}}\\big)}{\\sum_{\\boldsymbol{x}^{\\prime}\\in B}\\exp\\big(\\boldsymbol{z}_{\\boldsymbol{x}}\\cdot\\boldsymbol{z}_{\\boldsymbol{x}^{\\prime}}\\big)}=\\widehat{\\mathcal{L}}_{\\mathrm{scl}}^{\\mathrm{out}}(\\boldsymbol{x},\\boldsymbol{y}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "which is consistent with the original paper\u2019s derivation. ", "page_idx": 13}, {"type": "text", "text": "B Analysis of Existing Long-Tail Learning Methods ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "As we have derived before, here we dive deep into the analysis of existing methods and demonstrate that they all belong to our unified framework. ", "page_idx": 13}, {"type": "text", "text": "B.1 Discussion of Gaussian kernel estimation ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Balanced contrastive learning [73] (BCL) was proposed to solve long-tailed problems with improved supervised contrastive learning. BCL involves two key techniques: class averaging and class complement. BCL averages out the contributions of different classes in the denominator to ensure class equal distribution, meanwhile, it takes nonlinear mapping of the classifier parameters to form a learnable class center to ensure that every class has at least one sample in a mini-batch: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{E}_{{\\pmb x}^{\\prime}\\sim\\mathbb{P}(\\cdot|Y=k)}\\left[\\kappa\\left(z_{\\pmb x},z_{\\pmb x^{\\prime}}\\right)\\right]\\approx\\frac{1}{|\\mathcal{B}_{k}|+1}\\sum_{{\\pmb x}^{\\prime}\\in\\mathcal{B}_{k}\\cup\\left\\{{\\pmb\\alpha}_{k}\\right\\}}\\exp\\left(z_{\\pmb x}\\cdot{\\pmb z}_{\\pmb x^{\\prime}}\\right).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "However, BCL ignores the problem of the original long-tailed distribution in the training dataset, necessitating a reweighting operation. Let $\\pi$ denote the class prior, we have: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\widehat{\\mathcal{L}}_{\\mathrm{bcl}}(x,y)=-\\frac{1}{\\pi_{y}}\\log\\frac{\\frac{1}{|\\mathcal{B}_{y}|}\\sum_{x^{\\prime}\\in\\mathcal{B}_{y}\\cup\\{c_{y}\\}\\setminus\\{x\\}}\\exp{(z_{x}\\cdot z_{x^{\\prime}})}}{\\sum_{k\\in[C]}\\frac{1}{|\\mathcal{B}_{k}|+1}\\sum_{x^{\\prime}\\in\\mathcal{B}_{k}\\cup\\{c_{k}\\}}\\exp{(z_{x}\\cdot z_{x^{\\prime}})}}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Gaussian mixture likelihood loss [57] (GML) initiates its approach from the concept of mutual information, employing the Gaussian kernel. GML integrates contrastive learning with logit adjustment to enhance its performance. ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\mathcal{L}}_{\\mathrm{gml}}(x,y)=-\\log\\frac{\\frac{1}{|B_{y}|+|Q_{y}|-1}\\sum_{x^{\\prime}\\in B_{y}\\cup Q_{y}\\setminus\\{x\\}}\\exp\\left(z_{x}\\cdot z_{x^{\\prime}}\\right)\\pi_{y}}{\\sum_{k\\in[C]}\\frac{1}{|B_{k}|+|Q_{k}|}\\sum_{x^{\\prime}\\in B_{k}\\cup Q_{k}}\\exp\\left(z_{x}\\cdot z_{x^{\\prime}}\\right)\\pi_{k}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "It proposes a class-wise queue $\\mathcal{Q}=\\cup_{k=1}^{C}\\mathcal{Q}_{k}$ to ensure a balanced class occurrence in a mini-batch.   \nHowever, it does not propose a unified framework, and the understanding is not deep enough. ", "page_idx": 13}, {"type": "text", "text": "Some other methods: In this section, we compare some other methods that use contrastive learning and analyze their mistakes. ", "page_idx": 13}, {"type": "text", "text": "K-positive contrastive learning [30] (KCL) is based on supervised contrastive learning, using K samples of the same class in the molecule to ensure balanced feature space. Putting in our unified framework, we can obtain the following: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{E}_{{\\pmb x}^{\\prime}\\sim\\mathbb{P}(\\cdot|Y=y)}\\left[\\kappa\\left(z_{x},z_{x^{\\prime}}\\right)\\right]\\approx\\frac{1}{|K|}\\sum_{{\\pmb x}^{\\prime}\\in\\mathcal{B}^{\\prime};\\mathcal{B}^{\\prime}\\subseteq\\mathcal{B}_{y},|\\mathcal{B}^{\\prime}|=K}\\exp\\left(z_{x}\\cdot z_{x^{\\prime}}\\right),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{\\mathcal{L}}_{\\mathrm{kcl}}^{\\mathrm{in}}(x,y)=-\\log\\frac{\\frac{1}{|K|}\\sum_{z_{x^{\\prime}}\\in B^{\\prime};B^{\\prime}\\subseteq B_{y},|B^{\\prime}|=K}\\exp{(z_{x}\\cdot z_{x^{\\prime}})}}{\\sum_{z_{x^{\\prime}}\\in B}\\exp{(z_{x}\\cdot z_{x^{\\prime}})}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\n\\stackrel{\\mathrm{Jensen}}{\\leq}-\\frac{1}{\\left\\vert K\\right\\vert}\\sum_{\\substack{z_{\\alpha^{\\prime}}\\in B^{\\prime};B^{\\prime}\\subseteq B_{y},\\left\\vert B^{\\prime}\\right\\vert=K}}\\log\\frac{\\exp\\left(z_{x}\\cdot z_{x^{\\prime}}\\right)}{\\sum_{z_{\\alpha^{\\prime}}\\in B}\\exp\\left(z_{x}\\cdot z_{x^{\\prime}}\\right)}=\\widehat{\\mathcal{L}}_{\\mathrm{kcl}}^{\\mathrm{out}}(x,y).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Compared with the above, it regrettably still uses the same unprocessed denominator of SCL and cannot ensure that each mini-batch contains an equal number of samples from each class, nor that each class contributes equally, rendering it suboptimal for long-tailed learning. In addition, Eq. (28) is equivalent to the resampling technique. ", "page_idx": 14}, {"type": "text", "text": "Parametric contrastive learning [15] $\\mathrm{(PaCo)}$ introduces a set of parametric class-wise learnable centers and uses adjustable parameters $\\alpha$ for loss with respect to them. Our framework is used to derive its original form. First, we can obtain: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbf{x}^{\\prime}\\sim\\mathbb{P}(\\cdot\\,|\\,Y=k)}\\left[\\kappa\\left(z_{x},z_{x^{\\prime}}\\right)\\right]\\approx\\frac{\\beta}{|\\mathcal{B}_{k}|}\\sum_{x^{\\prime}\\in\\mathcal{B}_{k}}\\exp\\left(z_{x}\\cdot z_{x^{\\prime}}\\right)+\\left(1-\\beta\\right)\\exp\\left(z_{x}\\cdot c_{k}\\right)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\beta$ is a fixed coefficient. Then, we can derive the PaCo loss as follows. ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{\\mathcal{L}}_{\\mathrm{paco}}(x,y)=}\\\\ &{-\\log\\frac{\\Big(\\frac{\\beta}{|B_{y}|}\\sum_{x^{\\prime}\\in B_{y}\\backslash\\{x\\}}\\exp\\left(z_{x}\\cdot z_{x^{\\prime}}\\right)+\\left(1-\\beta\\right)\\exp\\left(z_{x}\\cdot c_{y}\\right)\\Big)\\mathbb{P}(Y=y)}{\\sum_{k\\in[C]}\\Big(\\frac{\\beta}{|B_{k}|}\\sum_{x^{\\prime}\\in B_{k}}\\exp\\left(z_{x}\\cdot z_{x^{\\prime}}\\right)+\\left(1-\\beta\\right)\\exp\\left(z_{x}\\cdot c_{k}\\right)\\Big)\\mathbb{P}(Y=k)}}\\\\ &{\\approx-\\log\\frac{\\Big(\\alpha\\sum_{x^{\\prime}\\in B_{y}\\backslash\\{x\\}}\\exp\\left(z_{x}\\cdot z_{x^{\\prime}}\\right)+\\exp\\left(z_{x}\\cdot c_{y}\\right)\\Big)\\mathbb{P}(Y=y)}{\\sum_{k\\in[C]}\\left(\\alpha\\sum_{x^{\\prime}\\in B_{k}}\\exp\\left(z_{x}\\cdot z_{x^{\\prime}}\\right)+\\exp\\left(z_{x}\\cdot c_{k}\\right)\\right)\\mathbb{P}(Y=k)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where \u03b1 = $\\begin{array}{r}{\\alpha=\\frac{\\beta\\mathbb{P}(Y=y)}{(1-\\beta)|\\mathcal{B}_{y}|}}\\end{array}$ . PaCo explicitly uses the parametric class center to ensure balanced class occurrence in a mini-batch. However, It ignores the class-equal contribution in loss computation, which can still be suboptimal. ", "page_idx": 14}, {"type": "text", "text": "Probabilistic contrastive learning [20] (Proco) simply assumes that the normalized features in contrastive learning follows a mixture of von Mises-Fisher (vMF) distributions on a unit ball, its probability density function has the following form: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{c c c}{{f_{p}\\left(z;\\mu_{y},\\rho_{y}\\right)=\\displaystyle\\frac{1}{C_{p}\\left(\\kappa_{y}\\right)}e^{\\rho\\mu^{\\top}z},}}\\\\ {{C_{p}(\\rho)=\\displaystyle\\frac{(2\\pi)^{p/2}I_{(p/2-1)}(\\rho)}{\\rho^{p/2-1}}I_{(p/2-1)}(z)=\\displaystyle\\sum_{k=0}^{\\infty}\\frac{1}{k!\\Gamma(p/2-1+k+1)}\\left(\\frac{z}{2}\\right)^{2k+p/2-1}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where parameters $(\\pmb{\\mu}_{y},\\pmb{\\rho}_{y})$ need to be estimated. The advantage is that Proco can estimate $(\\pmb{\\mu}_{y},\\pmb{\\rho}_{y})$ using an online mini-batch, such that it can be derived as a closed form of expected contrastive loss. Despite the assumed vMF distribution, it still uses Gaussian kernel estimation: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbf{x}^{\\prime}\\sim\\mathbb{P}(\\cdot\\,|\\,Y=y)}\\left[\\kappa\\left(z_{\\mathbf{x}},z_{\\mathbf{x}^{\\prime}}\\right)\\right]\\approx\\mathbb{E}_{z_{\\mathbf{x}^{\\prime}}\\sim\\hat{\\mathbb{P}}_{\\mathrm{vMF}}(\\cdot\\,|\\,Y=y)}\\left[\\kappa\\left(z_{\\mathbf{x}},z_{\\mathbf{x}^{\\prime}}\\right)\\right]=\\frac{C_{p}(\\lambda(z_{x},y))}{C_{p}\\left(\\rho_{y}\\right)}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\lambda(z_{x},y)$ represents a fixed function related to $z_{x},\\pmb{\\mu}_{y},\\rho_{y}$ . Thus, the loss objective of Proco is: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\widehat{\\mathcal{L}}_{\\mathrm{proco}}(x,y)=-\\log\\widehat{\\mathbb{P}}_{s}(Y=y\\mid z)=-\\log\\frac{C_{p}(\\lambda(z_{x},y))\\cdot\\pi_{y}}{\\sum_{k\\in[C]}\\frac{C_{p}(\\lambda(z_{x},k))\\cdot\\pi_{k}}{C_{p}(\\rho_{k})}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "However, the assumed distribution of Proco stills needs to be estimated $(\\pmb{\\mu_{y}},\\pmb{\\rho_{y}})$ using EMA of different batches, which is essentially a similar approach to the momentum queue used in GML. There still exist problems of inconsistent distribution of $_{\\textit{z}}$ in different mini-batches, and the strong assumption about $\\mathbb{P}(z\\mid Y=y)$ which may not follow the vMF distribution. ", "page_idx": 14}, {"type": "text", "text": "B.2 Discussion of explicitly assigning a specified distribution ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Previous work mainly focused on $\\mathbb{P}(z\\mid Y=y)$ through modeling $\\cos\\theta_{y}=\\frac{\\mu_{y}^{\\top}z}{\\|\\mu_{y}\\|\\|z\\|}$ T-vMF [34] models $\\cos\\theta_{y}$ as vMF distribution: ", "page_idx": 15}, {"type": "equation", "text": "$$\nf\\left(\\cos\\theta_{y};\\rho_{y}\\right)=\\frac{1}{C\\left(\\rho_{y}\\right)}e^{\\rho_{y}\\cos\\theta_{y}}=C^{\\prime}(\\rho_{y})e^{\\rho_{y}\\left\\|z-\\pmb{\\mu}_{y}\\right\\|}=C^{\\prime}(\\rho_{y})s_{e}(\\left\\|z-\\pmb{\\mu}_{y}\\right\\|,\\rho_{y}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Due to the inherent properties of the exponential function, the posterior quickly converges to 0, despite a large $\\left\\|z-\\dot{\\mu}_{y}\\right\\|$ . Such a compact measuring function might hamper model training, since tailed samples hardly enjoy back-propagation due to vanishing gradient. To overcome this problem, T-vMF introduces a family of modeling methods as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\nf_{q}\\left(\\cos\\theta_{y};\\rho_{y}\\right)=C^{\\prime}(\\rho_{y})\\left[1-(1-q)\\frac{1}{2}\\rho_{y}\\left\\|z-\\mu_{y}\\right\\|\\right]^{\\frac{1}{1-q}}=C^{\\prime}(\\rho_{y})s_{q}(\\left\\|z-\\mu_{y}\\right\\|,\\rho_{y}),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\begin{array}{r}{s_{q}(\\|z-\\pmb{\\mu}_{y}\\|\\,,\\rho_{y})=\\left[1-(1-q)\\frac{1}{2}\\rho_{y}\\left\\|z-\\pmb{\\mu}_{y}\\right\\|\\right]^{\\frac{1}{1-q}}}\\end{array}$ . Technically, T-vMF models $\\widehat{\\mathbb{P}}_{t}(Y=$ $y\\mid z)$ as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\widehat{\\mathbb{P}}_{t}(Y=y\\mid z)=\\frac{e^{\\varphi_{q,\\rho}}\\langle z,\\mu_{y}\\rangle}{\\sum_{k\\in[C]}e^{\\varphi_{q,\\rho}}\\langle z,\\mu_{k}\\rangle},}\\\\ {\\displaystyle\\varphi_{q,\\rho}\\left\\langle z,\\mu_{y}\\right\\rangle=2\\frac{s_{q}(\\left\\|z-\\mu_{y}-\\right\\|,\\rho)-s_{q}(2,\\rho)}{s_{q}(0,\\rho)-s_{q}(2,\\rho)}-1\\in[-1,1]\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Thus, the loss objective of T-vMF is: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\widehat{\\mathcal{L}}_{\\mathrm{T-vMF}}(\\pmb{x},y)=-\\log\\widehat{\\mathbb{P}}_{s}(Y=y\\mid z)=-\\log\\frac{\\pi_{y}e^{\\varphi_{q,\\rho}\\left\\langle z,\\mu_{y}\\right\\rangle}}{\\sum_{k\\in[C]}\\pi_{k}e^{\\varphi_{q,\\rho}\\left\\langle z,\\mu_{k}\\right\\rangle}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "WCDAS [27] The accuracy of the posterior approximation is a crucial factor influencing the method\u2019s performance. Unlike t-vMF, which directly specifies the $\\mathbb{P}(z\\mid Y\\,=\\,y)$ with fixed parameters, WCDAS seeks an optimal parametric probability density function of $\\mathbb{P}(z\\mid Y=y)$ . ", "page_idx": 15}, {"type": "text", "text": "Modeling $\\cos\\theta_{y}$ as the Wrapped Cauchy distribution with trainable parametric $\\pmb{\\vartheta}=[\\vartheta_{1},\\ldots,\\vartheta_{C}]$ , WCDAS models $\\widehat{\\mathbb{P}}_{t}(Y=y\\mid z)$ as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{f\\left(\\cos\\theta_{y};\\boldsymbol{\\vartheta}_{y}\\right)=\\frac{1-\\vartheta_{y}^{2}}{2\\Pi(1+\\vartheta_{y}^{2}-2\\vartheta_{y}\\cos\\theta_{y})},}\\\\ {\\widehat{\\mathbb{P}}_{t}(Y=y\\mid z)=\\frac{e^{f(\\cos\\theta_{y};\\vartheta_{y})}}{\\displaystyle\\sum_{k\\in[C]}e^{f(\\cos\\theta_{k};\\vartheta_{k})}}.\\ \\ \\ }\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Thus, the loss objective of WCDAS is: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\widehat{\\mathcal{L}}_{\\mathrm{WCDAS}}(\\pmb{x},\\pmb{y})=-\\log\\widehat{\\mathbb{P}}_{s}(Y=y\\ |\\ \\pmb{z})=-\\log\\frac{\\pi_{y}e^{f(\\cos\\theta_{y};\\pmb{\\vartheta}_{y})}}{\\sum_{k\\in[C]}\\pi_{k}e^{f(\\cos\\theta_{k};\\pmb{\\vartheta}_{k})}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "C Mathematical Notations ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "To ensure clarity and precision throughout this paper, we provide a comprehensive list and definitions of the key mathematical symbols and terms used in this section. Each symbol is defined with its specific meaning and context to ensure consistency and accuracy across the document. ", "page_idx": 15}, {"type": "text", "text": "D Illustration of The Proposed Algorithm ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "D.1 Illustration of The Overall Proposed Algorithm ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "CCL consists of two parts: the classification part and the contrastive learning part. The classification part uses logit rectification of the classifier by class prior estimated with a dual-branch. For the contrastive learning part, the energy score is used to select reliable unlabeled data which are merged with labeled data for continuous contrastive loss to ensure calibration. Besides, information of labeled data and unlabeled are used in a decoupled manner while maintaining the constraints of aligning feature in the contrastive learning space, thereby forming a smoothed contrastive loss. ", "page_idx": 15}, {"type": "table", "img_path": "PaqJ71zf1M/tmp/bfef0276895784a65b30ec9850b657f910644c3f28d9c2a0595653b34d1eb725.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "PaqJ71zf1M/tmp/f5a0ff8ff6710818f84bc27320c87eb507ea19e5cd2f4ce7cbf98d6afaab0bc0.jpg", "img_caption": ["Figure 3: Illustration of the proposed framework. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Input: labeled dataset and unlabeled dataset, standard branch $f_{s}$ and balanced branch $f_{b}$ , projection   \nhead $g$ , class prior of labeled dataset $\\pi_{l}$ , estimated unlabeled dataset class distribution $\\widehat{\\pi}_{u}$ , number   \nof iterations in each epoch $T$ , scaling parameter $\\tau$ .   \nRequire: Weak augmentation $A_{w}(\\cdot)$ , strong augmentation $A_{s}(\\cdot)$ , loss weight coefficients $\\lambda_{1},\\lambda_{2}$ .   \nfor $\\{(x_{i}^{(l)},y_{i}^{(l)})\\}_{i=1}^{|B^{l}|}\\leftarrow\\mathrm{S}$ $t=1$ to $T$ do ample a batch of labeled data $\\{x_{j}^{(u)}\\}_{j=1}^{|B^{u}|}\\gets\\mathsf{S}$ ample a batch of unlabeled data # Balanced classifier training with estimated class prior Calculate pseudo label $\\begin{array}{r}{\\widehat{y}=\\arg\\operatorname*{max}_{k\\in[C]}\\widehat{\\mathbb{P}}^{\\mathrm{cls}}\\left(Y=k\\mid\\mathcal{A}_{w}(\\pmb{x}^{u})\\right)}\\end{array}$ via Eq. (15) Calculate classification loss $\\widehat{\\mathcal{L}}_{\\mathrm{cls}}$ Update estimated class distr ibution $\\widehat{\\pi}^{u}$ via EMA by energy score selection # Continuous contrastive loss with reliable pseudo-labels Merge reliable unlabeled data selected based on energy score with labeled data to construct $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ Calculate loss $\\widehat{\\mathcal{L}}_{\\mathrm{rpl}}$ via Eq. (17) with $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ # Continuous contrastive loss with smoothed pseudo-labels Calculate $\\pmb{G}$ using unlabeled data Compute posterior ${\\widehat{\\mathbb{P}}}\\left(Y\\mid A_{w}(x^{u})\\right)$ and ${\\widehat{\\mathbb{P}}}\\left(Y\\mid{\\mathcal{A}}_{s}(x^{u})\\right)$ using Eq. (21)3 Calculate consistency regularization loss $\\widehat{\\mathcal{L}}_{\\mathrm{spl}}$ via Eq. (18) $\\widehat{\\mathcal{L}}_{\\mathrm{total}}=\\lambda_{1}\\widehat{\\mathcal{L}}_{\\mathrm{cls}}+\\left(1-\\lambda_{1}\\right)\\widehat{\\mathcal{L}}_{\\mathrm{rpl}}+\\lambda_{2}\\widehat{\\mathcal{L}}_{\\mathrm{spl}}$ Update $f_{s}$ a nd $f_{b}$ and $g$ bas ed on $\\nabla\\mathcal{L}_{\\mathrm{total}}$ using SGD   \nend for ", "page_idx": 17}, {"type": "text", "text": "D.2 Illustration of Reliable Pseudo-labels and Smoothed Pseudo-labels ", "text_level": 1, "page_idx": 17}, {"type": "image", "img_path": "PaqJ71zf1M/tmp/d7e28ff1d670035c994581d576c99dd4cb553f5cfb58dca684162a3e1f0f828a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Figure 4: Illustration of reliable pseudo-labels and smoothed pseudo-labels in CCL. To generalize the framework in Section 2 to LTSSL, the main challenge is unknown $\\mathbb{P}_{u}(Y=y\\mid x^{u})$ , where $\\pmb{x}^{u}$ denotes a sample in the unlabeled dataset. We first approximate $\\mathbb{P}_{u}(Y=y\\mid x^{u})$ using the output of the calibrated and integrated classifier and use energy score to filter out reliable unlabeled data, ensuring the model\u2019s calibration, which constitutes the reliable pseudo-labels subset. Furthermore, we can also estimate the unknown $\\mathbb{P}_{u}(Y\\,=\\,y\\mid\\,x^{u})$ by leveraging the smoothness assumption. Specifically, we construct smoothed pseudo-labels by propagating labels from nearby samples using the Gaussian kernel density estimation. ", "page_idx": 17}, {"type": "text", "text": "E Pseudo Code of The Proposed Algorithm ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Algorithm 1 summarizes the whole framework of the proposed CCL, which is clearly divided into three components: balanced classifier, continuous contrastive learning with reliable and smoothed pseudo-labels, respectively. ", "page_idx": 17}, {"type": "text", "text": "F Experimental Setup ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Training datasets. Our experimental analysis uses a variety of commonly adopted SSL datasets, including CIFAR10-LT [35], CIFAR100-LT [35], STL10-LT [13], and ImageNet-127 [22] in various ratios of class imbalance $\\gamma$ and various ratios of the amount of labeled data $\\eta$ . To create imbalanced versions of these datasets, we consider the long-tailed imbalance where the frequency of data points decreases exponentially from the largest to the smallest class, that is, the number of samples in class $c$ is $N_{c}=N_{1}\\times\\gamma^{-\\frac{c-1}{C-1}}$ for labeled data and $M_{c}=M_{1}\\times\\gamma^{-\\frac{c-1}{C-1}}$ for unlabeled data. We use Cutout [19] and Randaugment [14] for strong augmentation on unlabeled data, and we use SimAugment [11] on labeled data for continuous contrastive loss with smoothed pseudo-labels. Like recent LTSSL works, we consider three class distribution patterns for unlabeled data, namely, consistent, uniform, and reversed settings. ", "page_idx": 18}, {"type": "text", "text": "\u2022 CIFAR10-LT: Following ACR [64], we conduct experiments with all comparison methods in settings where $N_{1}=500$ , $M_{1}=4000$ and $N_{1}=1500$ , $M_{1}=3000$ . We adopt imbalance ratios of $\\gamma_{l}=\\gamma_{u}=100$ and $\\gamma_{l}=\\gamma_{u}=150$ for consistent settings, while for uniform and reversed settings, we use $\\gamma_{l}=100,\\gamma_{u}=1$ and $\\gamma_{l}=100,\\gamma_{u}=1/100$ , respectively. \u2022 CIFAR100-LT: Like CIFAR10-LT, we perform experiments in configurations where $N_{1}=$ 50, $M_{1}=400$ and $N_{1}=150,M_{1}=300$ . For the consistent settings, we use imbalance ratios of $\\gamma_{l}=\\gamma_{u}=10$ and $\\gamma_{l}=\\gamma_{u}=20$ . In contrast, for the uniform and reversed settings, we apply $\\gamma_{l}=10$ , $\\gamma_{u}=1$ and $\\gamma_{l}=10$ , $\\gamma_{u}=1/10$ , respectively. \u2022 STL10-LT: Given the absence of ground-truth labels for the unlabeled data of the STL10 dataset, we manage the experiments by adjusting the imbalance ratio of the labeled data. Following ACR, we consider the labeled imbalance ratio of $\\gamma_{l}=10$ or $\\gamma_{l}=20$ . \u2022 ImageNet-127: ImageNet127 was first introduced in an earlier research [29] and utilized in LTSSL by CReST. This dataset consolidates the 1000 classes [18] from ImageNet into 127 classes, grouping them according to the WordNet hierarchy. For ImageNet-127, we follow the original setting in CoSSL [22] $\\langle\\gamma_{l}=\\gamma_{u}\\approx286\\rangle$ ). ", "page_idx": 18}, {"type": "text", "text": "Implementation details. Our experimental configuration largely aligns with Fixmatch [56] and ACR [64]. Specifically, we apply the Wide ResNet-28-2 [70] architecture to implement our method on the CIFAR10-LT, CIFAR100-LT and STL10-LT datasets; and ResNet-50 on ImageNet-127. We adopt the common training paradigm that the network is trained with standard SGD [47, 49, 58] for 500 epochs, where each epoch consists of 500 mini-batches, and a batch size of 64 for both labeled and unlabeled data. We use a cosine learning rate decay [42] where the initial rate is 0.03, we set $\\tau=2.0$ for logit adjustment on all datasets, except for ImageNet-127, where $\\tau=0.1$ . We set the temperature $T=1$ and the threshold $\\zeta=-8.75$ for the energy score following [69], and we set $\\lambda_{1}=0.7$ , $\\lambda_{2}=1.0$ on CIFAR10/100-LT and $\\lambda_{1}=0.7,\\lambda_{2}=1.5$ on STL10-LT and ImageNet-127 datasets for the final loss. We set $\\beta=0.2$ in Eq. (21) for smoothed pseudo-labels loss. To show the effectiveness of our approach, we perform a comparative analysis with several existing LTSSL algorithms, including DARP [33], CReST [63], DASO [48], ABC [38], and TRAS [66]. We also consider the most popular LTSSL methods ACR [64] and CPE [43]. The performance evaluation of these methods is based on the top-1 accuracy metric on the test set. We present the mean and standard deviation of the results from three independent runs for each method. In addition, our method is implemented using the PyTorch library and experimented on an NVIDIA RTX A6000 (48 GB VRAM) with an Intel Platinum 8260 (CPU, 2.30GHz, 220 GB RAM). ", "page_idx": 18}, {"type": "text", "text": "G In-depth Analysis ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "G.1 Sensitive analysis of hyperparameters ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "As outlined in figure 5a, CCL is relatively robust to the fluctuation of $\\beta$ from 0.1 to 0.4. However, when $\\beta$ is set to 0, the propagation within unlabeled data is ignored, resulting in a performance decrease of about $0.9\\%$ . Thus, the necessity of using Eq. (21) is verified. In addition, figures 5b and 5c both demonstrate that CCL is robust to loss weighting coefficients $\\lambda_{1}$ and $\\lambda_{2}$ within a certain range. ", "page_idx": 18}, {"type": "text", "text": "However, it is worth noting that when $\\lambda_{1}=1.0$ , the proposed continuous reliable pseudo-labels loss is ignored, resulting in performance degradation. ", "page_idx": 19}, {"type": "image", "img_path": "PaqJ71zf1M/tmp/8b11da78a0a071757f209a6a2e6a77662487ff25db931638a7e70b24f1f80c97.jpg", "img_caption": ["Figure 5: Sensitive analysis of hyperparameters under consistent setting of CIFAR100-LT. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "G.2 Time and Space Complexity Analyses ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we conduct analyses of the time and space complexity of the proposed method. Denote feature space dimension $D$ , batch size $B$ and the number of classes $C$ , the time and space complexity of CCL can be seen in Table 8 and analysis details as follows. ", "page_idx": 19}, {"type": "text", "text": "For time complexity, calculating $\\mathcal{L}_{r p l}$ requires two main parts, calculating kernel similarities by multiplying two matrix of size $B\\times D$ and $D\\times B$ with complexity $\\mathcal{O}(B^{2}D)$ , and calculating $\\mathbb{E}_{\\mathbf{x}^{\\prime}\\sim\\mathbb{P}(\\cdot|Y=y)}\\left[\\kappa\\left(z_{x},z_{x^{\\prime}}\\right)\\right]$ by multiplying two matrix of size $B\\times B$ and $B\\times C$ with complexity $O(B^{2}C)$ . Calculating $\\mathcal{L}_{s p l}$ requires a further calculating part compared to $\\mathcal{L}_{r p l}$ : inverse matrix $I-\\beta G$ in Eq.(21) with complexity $\\mathcal{O}(B^{3})$ by utilizing the fast singular value decomposition. ", "page_idx": 19}, {"type": "text", "text": "For space complexity, calculating two losses requires two additional storage spaces, first sample pairwise kernel similarity requires space with complexity $\\mathcal{O}(B^{2})$ , and ${\\widehat{\\mathbb{P}}}\\left(Y\\mid X^{u}\\right)$ requires space with complexity $\\mathcal{O}(B C)$ . ", "page_idx": 19}, {"type": "text", "text": "Generally, given $B\\,=\\,64$ , $C=100$ and $D\\,=\\,256$ . Compared to the scale of model parameters, CCL adds negligible overhead relative to the neural network\u2019s computational cost when computing loss. We further report the averaged mini-batch training time with a single 3090 GPU and the GPU memory usage in Table 9 and Table 10. As seen from these tables, the training time and space consumptions of CCL are comparable to the existing state-of-the-art method ACR when CCL applies additional data augmentations to labeled data for representation learning. ", "page_idx": 19}, {"type": "table", "img_path": "PaqJ71zf1M/tmp/1894bf022dac64137b07569adb1e51f74620ac760b7b2bc79567832ccfc73870.jpg", "table_caption": ["Table 8: Time and space complexity of two continuous contrastive loss of CCL. "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "PaqJ71zf1M/tmp/e4522471ee74b807a10f8824b9e386346d7c6ae06c9648d54544e28c8436b075.jpg", "table_caption": ["Table 9: Average batch time of each algorithm. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "G.3 Confusion matrix ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Figure 6 presents the confusion matrix on the test set generated by CCL and ACR, which is calculated on the CIFAR10-LT dataset under $\\gamma_{l}\\,=\\,\\gamma_{u}\\,=\\,100$ and $\\gamma_{l}\\,=\\,\\gamma_{u}\\,=\\,150$ settings. As we can see in the top row of the figure, ACR often misclassifies the minority class $^{\\bullet\\bullet}7^{\\bullet}$ and $^{\\bullet\\bullet}$ into the majority class $^{\\bullet\\bullet}4^{\\bullet}$ and $\\,^{\\bullet}0^{\\bullet}$ , respectively. In comparison, CCL effectively mitigates this misclassification phenomenon by achieving an average improvement of $7.5\\%$ . Similarly in the second row, CCL achieved an extraordinarily high accuracy of $78\\%$ for class \u201c9\u201d, which shows a significant gain of $37\\%$ compared to ACR. CCL also achieves higher overall accuracy. ", "page_idx": 19}, {"type": "image", "img_path": "PaqJ71zf1M/tmp/e7198327ff7db178bfd23026b672c752c2bb48de83c1824d347c1add545c016f.jpg", "img_caption": ["Figure 6: Confusion matrices of the predictions on the test set of CIFAR10-LT. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "G.4 Precision and recall ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "To conduct a more in-depth analysis of the effectiveness of pseudo-labels generated by the proposed dual-branch fusion approach, we calculated the precision and recall of the pseudo-labels assigned to unlabeled data by ACR and CCL on CIFAR10-LT and CIFAR100-LT datasets. Specifically, we use $\\gamma_{l}=\\gamma_{u}=100$ and $\\gamma_{l}=\\gamma_{u}=150$ settings on CIFAR10-LT dataset and all three consistent, uniform, reversed settings on CIFAR100-LT dataset and we grouped the results of CIFAR100 into 10 categories, each category containing 10 classes, since CIFAR100 comprises 100 classes. As can be seen in figure 8, CCL achieves significantly improved precision of pseudo-labels for tailed classes \u201c9\u201d and \u201c $\\cdot10^{\\circ}$ on CIFAR10 dataset, while also achieving better recall for head classes. Similarly in Figure 7, CCL achieves overall better precision and recall compared to ACR regardless of the distribution mismatch scenario. It clearly shows that the pseudo-labels generated by CCL are more capable of alleviating the confirmation bias of tailed classes without sacrificing the performance of head classes. ", "page_idx": 20}, {"type": "text", "text": "G.5 Visualization ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Furthermore, we employ the t-distributed stochastic neighbor embedding (t-SNE) [61] to visualize the representations learned by the CCL method and contrast them with those from the previous ACR method. The comparative results on the test set, under consistent settings, are depicted in Figure 9. The figure demonstrates that the representations derived from CCL provide more distinct classification boundaries. ", "page_idx": 20}, {"type": "text", "text": "H Limitation ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Our paper examines existing long-tailed learning methods through the lens of information theoretical view, proposing a unified framework. However, we have not established theoretical proof for the convergence of features within this framework. In the future, we intend to provide further theoretical analysis from the perspective of neural collapse. ", "page_idx": 20}, {"type": "image", "img_path": "PaqJ71zf1M/tmp/bda501bacf39dd51d0dd369391eae234e21c1defe24a02d0b0bbb86f0ec0684a.jpg", "img_caption": ["Figure 7: The precision and recall of pseudo-labels for ACR and CCL on CIFAR100-LT dataset in consistent, uniform, reversed settings. "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "PaqJ71zf1M/tmp/1072fa68642decf940eec42e35ea97cb88776293efed0129fca7be166ccdd473.jpg", "img_caption": ["Figure 8: The precision and recall of pseudo-labels for ACR and CCL on CIFAR10-LT dataset in consistent settings. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "I Broader Impact ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The positive impacts of this work are two-fold: 1) It enhances the fairness of the classifier in semisupervised learning, preventing potential biases in deep models, such as an unfair AI primarily serving the majority, which could lead to discrimination based on gender, race, or religion; 2) It enables the easy collection of larger image datasets without the need for mandatory class-balancing preprocessing. For example, in training classifiers for real-world natural image scenes using the proposed method, we do not need to consider whether the distribution of unlabeled data matches that of the labeled data or if every class in the labeled data has an equal number of samples. However, negative effects might occur if the proposed long-tailed semi-supervised classification technique is misused. In the wrong hands, this approach could be exploited for unethical purposes, such as targeting or identifying minority groups for detrimental reasons. ", "page_idx": 21}, {"type": "image", "img_path": "PaqJ71zf1M/tmp/1a96a275b6196c94dc5be95c1eb68bf3824701fd1c08daa8573839dcc6c763dd.jpg", "img_caption": ["Figure 9: The t-SNE visualization of the test set for ACR and CCL on CIFAR-10-LT dataset in consistent settings. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We clearly articulated our contributions at the end of the abstract and the introduction. Additionally, the research scope of this paper is introduced at the beginning of both the abstract and the introduction. ", "page_idx": 23}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The limitations of our method are analyzed in the appendix of this paper. ", "page_idx": 23}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper does not include theoretical results. ", "page_idx": 23}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: All experimental results were obtained by implementing the proposed method and running it on the dataset. All results are reproducible. ", "page_idx": 23}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We have made the source code publicly available via the link described in the abstract. ", "page_idx": 23}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: In the appendix, we provide a detailed description of the experimental setup, including the creation of the dataset, hyperparameter settings, as well as the pseudocode for our method and diagrams of the model structure. ", "page_idx": 23}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: In the experimental section, we present statistical results, including the mean and variance of accuracy, ECE calibration metrics, and line graphs estimating prior errors. ", "page_idx": 23}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The computational resources we used are detailed in the appendix. ", "page_idx": 24}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Our code adheres to the NeurIPS Code of Ethics and does not violate any ethical guidelines. ", "page_idx": 24}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We discuss societal impacts in the appendix ", "page_idx": 24}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] Justification: Our paper does not employ high-risk data or models, thus posing no such risks. ", "page_idx": 24}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The assets from several recent works that we utilized are all cited in our paper. ", "page_idx": 24}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 24}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 24}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects ", "page_idx": 24}]