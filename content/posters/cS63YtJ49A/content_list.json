[{"type": "text", "text": "Training-Free Adaptive Diffusion with Bounded Difference Approximation Strategy ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Hancheng $\\mathbf{Y}\\mathbf{e}^{1,*}$ , Jiakang $\\mathbf{Yuan}^{2,*}$ , Renqiu $\\mathbf{Xia^{3}}$ , Xiangchao $\\mathbf{Yan}^{1}$ , Tao Chen2, Junchi $\\mathbf{Yan}^{3}$ , Botian $\\mathbf{S}\\mathbf{h}\\mathbf{i}^{1}$ , Bo Zhang1,\u2021 ", "page_idx": 0}, {"type": "text", "text": "1Shanghai Artificial Intelligence Laboratory 2School of Information Science and Technology, Fudan University 3School of Artificial Intelligence, Shanghai Jiao Tong University yehancheng@pjlab.org.cn, jkyuan22@m.fudan.edu.cn, zhangbo@pjlab.org.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Diffusion models have recently achieved great success in the synthesis of highquality images and videos. However, the existing denoising techniques in diffusion models are commonly based on step-by-step noise predictions, which suffers from high computation cost, resulting in a prohibitive latency for interactive applications. In this paper, we propose AdaptiveDiffusion to relieve this bottleneck by adaptively reducing the noise prediction steps during the denoising process. Our method considers the potential of skipping as many noise prediction steps as possible while keeping the final denoised results identical to the original full-step ones. Specifically, the skipping strategy is guided by the third-order latent difference that indicates the stability between timesteps during the denoising process, which beneftis the reusing of previous noise prediction results. Extensive experiments on image and video diffusion models demonstrate that our method can significantly speed up the denoising process while generating identical results to the original process, achieving up to an average $2\\sim5\\times$ speedup without quality degradation. The code is available at https://github.com/UniModal4Reasoning/ AdaptiveDiffusion. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recently, Diffusion models [1, 11, 30, 33] have emerged as a powerful tool for synthesizing highquality images and videos. Their capability to generate realistic and detailed visual content has made them a popular choice in various applications, ranging from artistic creation to data augmentation, e.g., Midjourney, Sora [4], etc. However, the conventional denoising techniques employed in these models involve step-by-step noise predictions, which are computationally intensive and lead to significant latency, e.g., taking tens seconds for SDXL [32] to generate a high-quality image of $10\\bar{2}4\\!\\!\\times\\!1024$ resolutions. Diffusion acceleration as an effective technique has been deeply explored recently, which mainly focuses on three paradigms: (1) reducing sampling steps [39, 13, 23, 35, 24], (2) optimizing model architecture [15, 42, 9, 27] and (3) parallelizing inference [16, 38]. ", "page_idx": 0}, {"type": "text", "text": "Currently, most strategies are designed based on a fixed acceleration mode for all prompt data. However, in our experiments, it is observed that different prompts may require different steps of noise prediction to achieve the same content as the original denoising process, as presented in Fig. 1. Here, we compare the denoising paths using two different prompts for SDXL [32], both of which preserve rich content against the original full-step generation results. The denoising path denotes a bool-type sequence, where each element represents whether to infer the noise from the noise prediction model. It can be observed that Prompt 2 needs more steps to generate an almost lossless image than Prompt ", "page_idx": 0}, {"type": "image", "img_path": "cS63YtJ49A/tmp/9973097edb7fd48006863aa9af6bce35ea6de199182ef95e2432d4c1a03f0064.jpg", "img_caption": ["Figure 1: Different prompts may have different denoising paths to generate the high-quality image. For Prompt 1, we only need 20 steps out of 50 steps for noise predictions to generate an almost lossless image, while for Prompt 2, we need 26 steps out of 50 steps to achieve an almost lossless image. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "1 (A lower LPIPS [51] value means more similarity between two images generated by the original strategy and our strategy). Therefore, it is necessary to explore a prompt-adaptive acceleration paradigm to consider the denoising diversity between different prompts. ", "page_idx": 1}, {"type": "text", "text": "Motivated by this observation, in this paper, we deeply dive into the skipping scheme for the noise prediction model and propose AdaptiveDiffusion, a novel approach that adaptively accelerates the generation process according to different input prompts. The fundamental concept behind AdaptiveDiffusion is to adaptively reduce the number of noise prediction steps according to different input prompts during the denoising process, and meanwhile maintain the quality of the final output. The key insight driving our method is that the redundancy of noise prediction is highly related to the third-order differential distribution between temporally-neighboring latents. This relation can be leveraged to design an effective skipping strategy, allowing us to decide when to reuse previous noise prediction results and when to proceed with new calculations. Our approach utilizes the thirdorder latent difference to assess the redundancy of noise prediction at each timestep, reflecting our strategy\u2019s dependency on input information, thus achieving a prompt-adaptive acceleration paradigm. ", "page_idx": 1}, {"type": "text", "text": "Extensive experiments conducted on both image and video diffusion models demonstrate the effectiveness of AdaptiveDiffusion. The results show that our method can achieve up to a $5.6\\mathrm{x}$ speedup in the denoising process with better preservation quality. This improvement in acceleration quality opens up new possibilities for the application of diffusion models in real-time and interactive environments. ", "page_idx": 1}, {"type": "text", "text": "In summary, AdaptiveDiffusion represents a substantial advancement in adaptively efficient diffusion, offering a practical solution to the challenge of high computational costs associated with sequentially denoising techniques. The main contribution is threefold: (1) To our best knowledge, our method is the first to explore the adaptive diffusion acceleration from the step number reduction of noise predictions that makes different skipping paths for different prompts. (2) We propose a novel approach, namely AdaptiveDiffusion, which develops a plug-and-play criterion to decide whether the noise prediction should be inferred or reused from the previous noise results. (3) Extensive experiments conducted on various diffusion models [32, 33, 52, 44] and tasks [7, 21, 45, 8] demonstrate the superiority of our AdaptiveDiffusion to the existing acceleration methods in the trade-off among efficiency, performance and generalization ability. ", "page_idx": 1}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Diffusion Models ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Diffusion models [1, 11, 30, 33, 10] have achieved great success and served as a milestone in content generation. As a pioneer, Denoising Diffusion Probabilistic Models (DDPMs) [11] generate higherquality images compared to generative adversarial networks (GANs) [17, 6] through an iterative denoising process. To improve the efficiency of DDPM, Latent Diffusion Models (LDMs) [33] perform forward and reverse processes in a latent space of lower dimensionality which further evolves into Stable Diffusion (SD) family [36, 32]. Recently, video diffusion models [3, 52, 2, 44] have attracted increasing attention, especially after witnessing the success of Sora [4]. Stable Video Diffusion (SVD) [3] introduces a three-stage training pipeline and obtains a video generation model with strong motion representation. I2VGen-XL [52] first obtains a model with multi-level feature extraction ability, then enhances the resolution and injects temporal information in the second stage. Despite the high quality achieved by diffusion models, the inherent nature of the reverse process which needs high computational cost slows down the inference process. ", "page_idx": 1}, {"type": "image", "img_path": "cS63YtJ49A/tmp/b1a1fcc982b655e512c22fa213a8870a83d67eedde3e0abd3f449f8cc1271e96.jpg", "img_caption": ["Figure 2: Denoising process of the proposed AdaptiveDiffusion: We design a third-order estimator (Refer to Sec. 3.3 for details), which can find the redundancy between neighboring timesteps, and thus, the noise prediction model can be skipped or inferred according to the indicate from the estimator, achieving the adaptive diffusion process. Note that the timestep and text information embeddings are not shown for the sake of brevity. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2.2 Accelerating Diffusion Models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Current works accelerate diffusion models can be divided into the following aspects. (1) Reducing Sampling Steps [39, 23, 24, 49, 40, 25, 20, 37, 28, 29, 26]. DDIM [39] optimizes sampling steps by exploring a non-Markovian process. Further studies [23, 24, 49] such as DPM-Solver [23] propose different solvers for diffusion SDEs and ODEs to reduce sampling steps. Another way to optimize sampling steps is to train few-step diffusion models by distillation [40, 25, 20]. Among them, consistency models [40, 25] directly map noise to data to enable one-step generation. Other works [20, 37] explore progressive distillation or adversarial distillation to effectively reduce the reverse steps. Besides, some works [26, 42, 29] introduce early stop mechanism into diffusion models. (2) Optimizing Model Architecture [9, 19, 46, 27, 5, 47, 22, 41]. Another strategy to accelerate diffusion models is to optimize model efficiency to reduce the cost during inference. Diff-pruning [9] compresses diffusion models by employing Taylor expansion over pruned timesteps. DeepCache [27] notices the feature redundancy in the denoising process and introduces a cache mechanism to reuse pre-computed features. (3) Parallel Inference [16, 38, 12]. The third line lies in sampling or calculating in a parallel way. ParaDiGMS [38] proposes to use Picard iteration to run multiple steps in parallel. DistriFusion [16] introduces displaced patch parallelism by reusing the pre-computed feature maps. Compared with the existing paradigms designing a fixed acceleration mode for all input prompts, our method highlights the adaptive acceleration manner with a plug-and-play criterion based on the high-order latent differential distribution, which allows various diffusion models with different sampling schedulers to achieve significant speedup with a negligible performance drop and deployment cost. ", "page_idx": 2}, {"type": "text", "text": "3 The Proposed Approach ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Preliminary ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Reverse Denoising Process. Diffusion models [11, 39] are designed to learn two processes with noise addition (known as the forward process) and noise reduction (known as the reverse process). ", "page_idx": 2}, {"type": "image", "img_path": "cS63YtJ49A/tmp/2f8b884e8eb9069265d2cbf32eba5f9899ca09b6ee45b6f73ecdc3910e664429.jpg", "img_caption": ["Figure 3: Different update strategies. (a) The default SDXL [32] samples 50 steps of noise prediction followed by the latent update process. (b) Our AdaptiveDiffusion skips 25 steps of noise prediction according to the third-order estimator, while the latent is fully updated at all 50 steps. (c) SDXL samples 25 steps of the noise prediction and latent update process. (d) The default SDXL skips 25 steps of both noise prediction and latent update from its sampled 50 steps. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "During the inference stage, only the reverse denoising process is adopted that starts from the Gaussian noise $x_{T}\\sim\\mathcal{N}(0,I)$ and iteratively denoises the input sample under the injected condition to get the final clean image(s) $x_{0}$ , where $T$ is the predefined number of denoising steps. Specifically, given an intermediate noisy image $x_{i}$ at timestep $i$ $(i\\,=\\,1,...,T)$ , the noise prediction model $\\epsilon_{\\theta}$ (e.g., UNet [34]) takes $x_{i}$ , timestep $t_{i}$ and an additional condition $c$ (e.g., text, image, and motion embeddings, etc) as input to approximate the noise distribution in $x_{i}$ . The update from $x_{i}$ to $x_{i-1}$ is determined by different samplers (a.k.a, schedulers) that can be generally formulated as Eq. (1): ", "page_idx": 3}, {"type": "equation", "text": "$$\nx_{i-1}=f(i-1)\\cdot x_{i}-g(i-1)\\cdot\\epsilon_{\\theta}(x_{i},t_{i}),\\quad i=1,\\ldots,T,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $f(i)$ and $g(i)$ are step-related coefficients derived by specific samplers [23, 50]. The computation in the update process mainly involves a few element-wise additions and multiplications. Therefore, the main computation cost in the denoising process stems from the inference of noise prediction model $\\epsilon_{\\theta}$ [16]. ", "page_idx": 3}, {"type": "text", "text": "Step Skipping Strategy. As proved by previous works [27, 42, 15], features between consecutive timesteps present certain similarities in distribution, thus there exists a set of redundant computations that can be skipped. Previous works usually skip either the whole update process or the partial computation within the noise prediction model at redundant timesteps. However, as visualized in Fig. 3, the latent update process in those redundant timesteps may be important to the lossless image generation. Besides, the calculation redundancy of the noise prediction model within each denoising process is still under-explored. To reduce the computation cost from the noise prediction model, we consider directly reducing the number of noise prediction steps from the original denoising process, which will be proved more effective and efficient to accelerate the generation with almost no quality degradation in Sec. 3.3. Given a certain timestep $i$ to skip, our skipping strategy for the update process can be formulated using Eq. (2): ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{c}{x_{i}=f(i)\\cdot x_{i+1}-g(i)\\cdot\\epsilon_{\\theta}(x_{i+1},t_{i+1}),}\\\\ {x_{i-1}=f(i-1)\\cdot x_{i}-g(i-1)\\cdot\\epsilon_{\\theta}(x_{i+1},t_{i+1}).}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "3.2 Error Estimation of the Step Skipping Strategy ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To validate the effectiveness of our step-skipping strategy, we theoretically analyze the upper bound of the error between the original output and skipped output images. For simplicity, we consider skipping one step of noise prediction, e.g., the $i$ -th timestep. Specifically, we have the following theorem about the one-step skipping. ", "page_idx": 3}, {"type": "text", "text": "Theorem 1. Given the skipping timestep i $(i>0,$ ), the original output $x_{i-1}^{o r i}$ and the skipped output $x_{i-1}$ , then the following in-equation holds: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\varepsilon_{i-1}=\\|x_{i-1}-x_{i-1}^{o r i}\\|={\\mathcal{O}}(t_{i}-t_{i+1})+{\\mathcal{O}}(x_{i}-x_{i+1}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The proof can be found in Appendix A.2.1. From Eq. (3), it can be observed that the error of the noise-skipped output is upper-bounded by the first-order difference between the previous two outputs. Similarly, the error of the continuously skipped output is upper-bounded by the accumulative differences between multiple previous outputs, which can be found in Appendix A.2.2 and A.2.3. ", "page_idx": 3}, {"type": "text", "text": "Therefore, it can be inferred that as the difference between the previous outputs $x$ is continuously minor, it is possible to predict that the noise prediction at the next timestep can be skipped without damaging the output. This inspires us to utilize the distribution of previous outputs to indicate the skip potential of the next-step noise prediction, as detailed in the following section. ", "page_idx": 3}, {"type": "image", "img_path": "cS63YtJ49A/tmp/b85a9006f043968701818e022d3fc127dee2ca53a0b335309691502965ada12c.jpg", "img_caption": ["Figure 4: The relation between order differential distributions and the searched optimal skipping path for one prompt. (a) The 1st-order noise differential distribution of the original full-step generation shows no relation with the optimal skipping path. (b) The 1st latent differential distribution indicates the distribution of the optimal skipping path but with no explicit mapping with skipping decisions, while the relative 2nd-order latent differential distribution shows a certain skipping signal in its fluctuation, but this signal is buried in the unstable magnitude. (c) The relative 3rd-order latent differential distribution shows a clearer signal for skipping decisions. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "3.3 Third-order Estimation Criterion ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Observations. In this section, we take SDXL [32] with Euler sampling scheduler as an example to describe the effectiveness of the proposed third-order estimator. Before deriving the third-order estimation criterion, one thing is to calculate the optimal skipping path from the given timestep number, so that we can evaluate the effectiveness of our proposed estimator. Considering the explosive searching cost within a large search space (e.g., searching the optimal path of skipping $N$ steps within $T$ steps for one prompt requires $\\bigcirc_{T}^{N}$ search time cost), we design a greedy search algorithm to approximate the optimal skipping path under different skipping targets, which can be found in Alg. (1). ", "page_idx": 4}, {"type": "text", "text": "Here we randomly take the prompt \u201cA bustling 18th-century market scene with vendors, shoppers, and cobblestone streets, all depicted in the detailed oil painting style.\" as an example to visualize the optimal skipping path searched by the greedy search algorithm under the predefined skipping target constraint. Meanwhile, as the skipping error is upper-bounded by the constraint of the firstorder latent difference, it inspires us to explore the relationship between the first-order difference distribution and the ideal skipping path. ", "page_idx": 4}, {"type": "text", "text": "As shown in Fig. 4, we visualize two types of first-order difference (one in Fig. 4a representing the noise difference distribution, another in Fig. 4b representing latent difference distribution) in the original full-step diffusion to compare with the skipping path. It can be observed that both two distributions of first-order differences are smooth across the denoising process, showing little relationship with the optimal skipping path. A similar insight can be observed in the distribution of the second-order latent difference, as shown in Fig. 4b. ", "page_idx": 4}, {"type": "text", "text": "However, when considering the third-order latent difference distribution, it presents a significant fluctuation in the original full-step denoising process. As shown in Fig. 4c, the distribution of the skipping path is related to the distribution of the third-order latent difference, especially in the early denoising process (around 15 timesteps), where the noise densely updates when the third-order latent difference increases and can be skipped when the third-order difference decreases. As for the later denoising process, when most third-order differences are relatively minor, most noise prediction steps are also skipped. According to the first-order latent difference presented in Fig. 4b, the differences between consecutive latent in the early denoising process are significantly larger than those in the later denoising process. Therefore, the precise importance estimation of noise predictions in the early process is much more important and the third-order difference distribution can intuitively serve as the indicator of the noise prediction strategy. The theoretical relation between the third-order derivative of latents and the optimal skipping scheme is analyzed in Appendix A.2.4. ", "page_idx": 4}, {"type": "text", "text": "Criterion. Based on the above empirical observation of the relationship between the optimal skipping path and the high-order difference distributions, the third-order estimator is proposed to indicate the potential of skipping the noise computation. Specifically, the criterion is formulated as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\xi\\left(x_{i-1}\\right)=\\left\\|\\varDelta^{(3)}x_{i-1}\\right\\|\\geq\\delta\\|\\varDelta x_{i}\\|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\xi\\left(x_{i-1}\\right)$ is the indicator that takes $x_{i-1}$ and previous latents $x_{i},x_{i+1},x_{i+2}$ as input to estimate whether the next noise prediction can be skipped. If $\\xi(x_{i-1})$ returns False, then the noise from the previous step will be reused to update $x_{i-1}$ . $\\varDelta^{(3)}x_{i-1}$ denotes the third-order latent difference at timestep $i-1$ , i.e., $\\varDelta^{(3)}x_{i-1}=\\varDelta^{(2)}x_{i-1}-\\varDelta^{(2)}x_{i}=\\varDelta x_{i-1}-2\\varDelta x_{i}+\\varDelta x_{i+1}$ , and $\\varDelta x_{i}$ is defined as the difference between $x_{i}$ and $x_{i+1}$ $\\mathit{i}=0,...,T-1)$ ). $\\delta$ is a hyperparameter thresholding the relative scale of $\\varDelta^{(3)}x_{i-1}$ . The reason for selecting $\\varDelta x_{i}$ is that $\\varDelta^{(3)}x_{i-1}$ actually describes the distance between $(\\varDelta x_{i-1}+\\varDelta x_{i+1})/2$ and $\\varDelta x_{i}$ . Therefore, it is natural to utilize the relative distance against $\\varDelta x_{i}$ to indicate the stability of the denoising process. Fig. 4c present a strong relation between $\\lVert\\varDelta^{(3)}x_{i-1}/\\varDelta x_{i}\\rVert$ (the blue dashed line) and the optimal skipping path. ", "page_idx": 4}, {"type": "image", "img_path": "cS63YtJ49A/tmp/686644e69517e7b1b00a701eb0f9fa1149ac634dfaaddb4e7bef45f0a56f5870.jpg", "img_caption": ["Figure 5: The effectiveness of the proposed third-order estimator. (a) The third-order estimated skipping path shares a similar distribution with the optimal skipping path. (b) The latent error between the full-step update path and the estimated skipping path. (c) The $\\chi^{2}$ stats and $p$ -value between the greedy searched paths and the third-order estimated paths at different skipping targets. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Effectiveness of the Third-order Estimator. To validate the effectiveness of the proposed third-order estimator, we compare our third-order estimated path with the optimal skipping path searched by the greedy algorithm, which is shown in Fig. 5a. It can be observed that the distribution of our estimated path is largely similar to the optimal skipping path. The reason for continuous skipping in the later denoising process is that the third-order difference keeps approaching zero as illustrated in Fig. 4c. The accumulative error caused by skipping noise predictions is described in Fig. 5b, where it is observed that the error starts increasing quickly after continuously skipping the noise predictions. Thus, it is vital to introduce another hyperparameter, i.e., the maximum step number of continuous skipping $C_{\\mathrm{max}}$ , to control the accumulative error. Hyperparameter analyses are described in Sec. 4.3. ", "page_idx": 5}, {"type": "text", "text": "Furthermore, we analyze the statistical correlation between the estimated path and the optimal path to test whether the designed criterion is significantly correlated to the optimal skipping criterion. As shown in Fig. 5c, we compute the $\\chi^{2}$ stats and $p$ -values under different step numbers of skipping. The results indicate that when the skipping steps are moderate, the estimated skipping path and the optimal skipping path are significantly correlated. For those targeting at small and large numbers of skipping steps, the correlation is statistically insignificant. The test details can be found in Appendix A.3. The overall skipping algorithm is shown in Alg. (2) of Appendix A.2.5. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1 Experimental Setup ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Models. We conduct experiments in three prompt-based settings including text-to-image (T2I), imageto-video (I2V), and text-to-video (T2V) generation tasks. In addition, we also test the effectiveness of AdaptiveDiffusion on the conditional image generation task. For the T2I task, we use Stable Diffusion-v1-5 (SD-1-5) [33] and Stable Diffusion XL (SDXL) [32] and evaluate on three different sampling schedulers (i.e., DDIM [39], DPM-Solver $^{++}$ [24], and Euler). For the I2V and T2V tasks, we utilize I2VGen-XL [52] and ModelScopeT2V [44] respectively. Note that we use ZeroScope-v2 instead of the original ModelScopeT2V model to generate watermark-free videos. For conditional image generation, we use LDM-4 [33] as the baseline model. ", "page_idx": 5}, {"type": "text", "text": "Benchmark Datasets. Following [27], we use ImageNet [7] and MS-COCO 2017 [21] to evaluate the results on class-conditional image generation and T2I tasks, respectively. For the I2V task, we randomly sample 100 prompts and reference images in AIGCBench [8]. For the T2V task, we conduct experiments on a widely-used benchmark MSR-VTT [45] and sample one caption for each video in the validation set as the test prompt. More details can be found in Appendix A.3. ", "page_idx": 5}, {"type": "text", "text": "Comparison Baselines. We compare AdaptiveDiffusion against DeepCache and Adaptive DPMSolver in both generation quality and efficiency. Deepcache [27] caches high-level features of UNet to update the low-level features at each denoising step, thus reducing the computational cost of UNet. The latter [23] dynamically adjusts the step size by combining different orders of DPM-Solver. ", "page_idx": 5}, {"type": "text", "text": "Evaluation Metrics. For all tasks, we evaluate our proposed method in both quality and efficiency. We report MACs, latency, and speedup ratio to verify the efficiency. For the image generation task, following previous works [17, 16, 18], we evaluate image quality with commonly-used metrics, i.e., ", "page_idx": 5}, {"type": "table", "img_path": "cS63YtJ49A/tmp/fb41b2f9691507cca809e274ecb81c6f9931190cdc13e47aefedec72b78c09d5.jpg", "table_caption": ["Table 1: Quantitative results on MS-COCO 2017 [21]. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Peak Signal Noise Ratio (PSNR), Learned Perceptual Image Patch Similarity (LPIPS), and Fr\u00e9chet Inception Distance (FID). For the video generation task, we use per-frame PSNR and LPIPS to measure the quality of generated videos. Besides, Fr\u00e9chet Video Distance (FVD) [43] is also used to quantify the temporal coherence and quality of each frame. Note that since our method achieves adaptive acceleration results, all reported metrics of our method are averaged across all prompts. ", "page_idx": 6}, {"type": "text", "text": "Implementation Details. We conduct all experiments on RTX 3090 GPUs. For SD-1-5 and SDXL models, the original sampling timesteps $T$ are set as 50, and two hyperparameters are set as: $\\delta=0.01$ , $C_{\\mathrm{max}}=4$ . For LDM-4, $T\\stackrel{\"}{=}250,\\bar{\\delta}=0.005,C_{\\mathrm{max}}=10$ . For I2VGen-XL and ModelScopeT2V, $T=50,\\delta=0.007.$ , $C_{\\mathrm{max}}=4$ . More details of other methods are listed in Appendix A.3. ", "page_idx": 6}, {"type": "text", "text": "4.2 Main Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.2.1 Results on Image Generation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We first evaluate our method on T2I generation. As shown in Tab. 1, compared to DeepCache and Adaptive DPM methods, AdaptiveDiffusion achieves both higher quality and efficiency in various settings. For example, AdaptiveDiffusion achieves 0.092 LPIPS on SD-v1-5 [33], generating almost lossless images compared to those generated by the full-step denoising process. Meanwhile, the averaged speedup ratio across all testing prompts achieves $2.01\\times$ when using SDXL and Euler sampling scheduler. In addition, when comparing the generation performance between different models (e.g., SD-1-5 and SDXL [32])) and schedulers (e.g., DDIM, DPM-Solver++, and Euler), AdaptiveDiffusion shows stronger generalization capability to adapt to different settings. ", "page_idx": 6}, {"type": "table", "img_path": "cS63YtJ49A/tmp/9a100934c8b2acc2176b160ee412a62a0a3c1e5d619b863f03a8bd237ef0d164.jpg", "table_caption": ["Table 2: Quantitative results on ImageNet $256\\!\\times\\!256$ [7]. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "To further verify the effectiveness of our method on different image generation tasks, we also conduct experiments on LDM-4 for the conditional image generation task. As shown in Tab. 2, due to the larger timestep setting in the original denoising process, the slow change of latent change allows us to achieve faster generation with almost lossless quality at nearly 5.6x speedup. Compared with Deepcache, the AdpativeDiffusion obtains a better trade-off between generation quality and efficiency. ", "page_idx": 6}, {"type": "text", "text": "4.2.2 Results on Video Generation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We further conduct experiments on more tough tasks, e.g., video generation tasks including I2V and T2V generation. As shown in Tab. 3, AdaptiveDiffusion can generate videos of lossless frames and similar video quality with a significant speedup against the original models\u2019 full-step generated videos. Specifically, our proposed method can achieve a significantly higher quality in single frame evaluation which can be seen by LPIPS and PSNR (e.g., $+6.38\\mathrm{dB}$ PSNR compared to DeepCache using I2VGen-XL). On the other hand, AdaptiveDiffusion can ensure temporal consistency as the original models due to the property of lossless acceleration which can be reflected on FVD. ", "page_idx": 6}, {"type": "text", "text": "4.3 Ablation Studies ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Ablation Study on Skipping Threshold. As shown in the upper part of Tab. 4, we first analyze the effect of the skipping threshold $\\delta$ . It can be observed that when the skipping threshold is relatively small, there will be fewer skipping steps, resulting in a relatively small yet still clear acceleration with a much higher preservation quality. With the threshold gradually increasing, the speedup ratio will be largely improved but at the cost of quality degradation. It can be seen that the image quality does not significantly change with large thresholds (i.e., 0.015 and 0.02). This is because the pre-defined maximum skipping steps prevent the further increase of skip steps. In this paper, we set the skipping threshold to 0.01 which is a better trade-off between the generation quality and inference speed. ", "page_idx": 6}, {"type": "table", "img_path": "cS63YtJ49A/tmp/cac9e18af80b5b446254037dd2be3896542be1ba21f6061dfa89b17fcd2a4ff8.jpg", "table_caption": ["Table 3: Quantitative results on video generation tasks. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "cS63YtJ49A/tmp/a16c636cb9ae9c3b09a5213f52fd6717d840e9df4893129e5e89c1db61a5e0dd.jpg", "table_caption": ["Table 4: Ablation studies on hyperparameters using SDXL [32]. We conduct the ablation studies using 50-step Euler sampling scheduler for SDXL on COCO2017. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Ablation Study on Maximum Skipping Steps. Further, we conduct ablation studies on the maximum skipping steps $C_{\\mathrm{max}}$ , as shown in the lower part of Tab. 4. With the increase of the maximum skipping steps, the quality of generated images continuously decreases. The reason is that when the timestep $t_{i}$ approaches 0, a large number of denoising steps will be skipped due to the minor value of the third-order latent difference when the max-skip-step is relatively large. The phenomenon reveals that $C_{\\mathrm{max}}$ can effectively prevent the continuous accumulation of generated image errors and ensure image quality. ", "page_idx": 7}, {"type": "text", "text": "Analysis on Sampling Steps. To evaluate the effectiveness of AdaptiveDiffusion on few-step sampling. It can be seen from Tab. 5 that AdaptiveDiffusion can further accelerate the denoising process under the few-step settings. Note that the hyperparameters (i.e., $\\delta$ , and $C_{\\mathrm{max}}$ ) should be slightly adjusted according to the original sampling steps due to the varying updating magnitudes in different sampling steps. Specifically, a higher threshold $\\delta$ and lower max-skip-step $C_{\\mathrm{max}}$ can make better generation quality when reducing the sampling steps. ", "page_idx": 7}, {"type": "table", "img_path": "cS63YtJ49A/tmp/831be3b3f6ef7f4043723a577e163261032551fb610da428df93fa7ee70d5da6.jpg", "table_caption": ["Table 5: Study on few-step sampling. Acceleration results with different original sampling steps using SDXL [32] on COCO2017. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.4 Visualization Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Generation Comparisons. To demonstrate the effectiveness of AdaptiveDiffusion more intuitively, we show some visualization results. Fig. 6 shows the results of the text-to-image generation task, it can be seen that AdaptiveDiffuion can better maintain image quality compared to Deepcache with nearly equal acceleration. Since AdaptiveDiffusion can adaptively determine which steps can be skipped, unimportant steps that have little impact on the final generation quality will be skipped during inference. Besides, to demonstrate the generalization of AdaptiveDiffusion on different tasks, we provide video generation results in Fig. 7. More generalization results can be found in Appendix A.4 ", "page_idx": 7}, {"type": "image", "img_path": "cS63YtJ49A/tmp/778aee5192eb1ccc3a6f18634f4296a2d1a1e3ee79def481f7205051af98132d.jpg", "img_caption": ["Figure 6: Qualitative results of text-to-image generation task using SDXL and SD-1-5 on MS-COCO 2017 benchmark. Left: SDXL, Right: SD-1-5. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "cS63YtJ49A/tmp/1cb0b801e48fd2ec57afedd5c29371156fbfdec13c1d32ad447469093cd751a8.jpg", "img_caption": ["Figure 7: Qualitative results of image-to-video generation task using I2VGen-XL on AIGCBench. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "cS63YtJ49A/tmp/9d0ce9e3a987ee715a050b836cdeb3254210ee04c3139f60cb34bc01cdc82462.jpg", "img_caption": ["Figure 8: (a) Skipping paths under different skipping targets obtained by the greedy search algorithm. (b) Skipping paths under different skipping thresholds by the third-order estimator. (c) The frequency distribution of the skipping number of noise update steps for SDXL generating images on MS-COCO 2017 benchmark. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Denoising Path Comparisons. Fig. 8 illustrates the distribution of skipping paths at different skipping schemes. It can be observed from Fig. 8a and 8b that when the number of noise update steps keeps decreasing (more blank grids in the horizontal lines), both greedy searched paths and third-order estimated paths tend to prioritize the importance of early and late denoising steps. From Fig. 8c, we find that most prompts in the MS-COCO 2017 benchmark only need around 26 steps of noise update to generate an almost lossless image against the 50-step generation result. ", "page_idx": 8}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we explore the training-free diffusion acceleration and introduce AdaptiveDiffusion, which can dynamically select the denoising path according to given prompts. Besides, we perform the error analyses of the step-skipping strategy and propose to use the third-order estimator to indicate the computation redundancy. Experiments are conducted on MS-COCO 2017, ImageNet, AIGCBench and MSR-VTT, showing a good trade-off between high image quality and low inference cost. ", "page_idx": 9}, {"type": "text", "text": "6 Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The research was supported by the National Key R&D Program of China (Grant No. 2022ZD0160104), the Science and Technology Commission of Shanghai Municipality (Grant No. 22DZ1100102), Shanghai Municipal Science and Technology Major Project (Grant No. 2021SHZDZX0102), Shanghai Rising Star Program (Grant No. 23QD1401000), National Key Research and Development Program of China (No. 2022ZD0160101), Shanghai Natural Science Foundation (No. 23ZR1402900), Shanghai Municipal Science and Technology Major Project (No.2021SHZDZX0103), National Natural Science Foundation of China (No. 62071127 and 62101137). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, et al. ediff:i Text-to-image diffusion models with an ensemble of expert denoisers. arXiv preprint arXiv:2211.01324, 2022.   \n[2] Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Yuanzhen Li, Tomer Michaeli, et al. Lumiere: A space-time diffusion model for video generation. arXiv preprint arXiv:2401.12945, 2024.   \n[3] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023.   \n[4] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024.   \n[5] Xuanyao Chen, Zhijian Liu, Haotian Tang, Li Yi, Hang Zhao, and Song Han. Sparsevit: Revisiting activation sparsity for efficient high-resolution vision transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2061\u20132070, 2023.   \n[6] Yunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha. Stargan v2: Diverse image synthesis for multiple domains. In CVPR, 2020.   \n[7] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee, 2009.   \n[8] Fanda Fan, Chunjie Luo, Wanling Gao, and Jianfeng Zhan. Aigcbench: Comprehensive evaluation of image-to-video content generated by ai. BenchCouncil Transactions on Benchmarks, Standards and Evaluations, 3(4):100152, 2023. [9] Gongfan Fang, Xinyin Ma, and Xinchao Wang. Structural pruning for diffusion models. Advances in neural information processing systems, 36, 2024.   \n[10] Ben Fei, Zhaoyang Lyu, Liang Pan, Junzhe Zhang, Weidong Yang, Tianyue Luo, Bo Zhang, and Bo Dai. Generative diffusion prior for unified image restoration and enhancement. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9935\u20139946, 2023.   \n[11] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. NeurIPS, 2020.   \n[12] Akio Kodaira, Chenfeng Xu, Toshiki Hazama, Takanori Yoshimoto, Kohei Ohno, Shogo Mitsuhori, Soichi Sugano, Hanying Cho, Zhijian Liu, and Kurt Keutzer. Streamdiffusion: A pipeline-level solution for real-time interactive generation. arXiv preprint arXiv:2312.12491, 2023.   \n[13] Zhifeng Kong and Wei Ping. On fast sampling of diffusion probabilistic models. In ICML Workshop on Invertible Neural Networks, Normalizing Flows, and Explicit Likelihood Models, 2021.   \n[14] Alex Krizhevsky et al. Learning multiple layers of features from tiny images. 2009.   \n[15] Lijiang Li, Huixia Li, Xiawu Zheng, Jie Wu, Xuefeng Xiao, Rui Wang, Min Zheng, Xin Pan, Fei Chao, and Rongrong Ji. Autodiffusion: Training-free optimization of time steps and architectures for automated diffusion model acceleration. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7105\u20137114, 2023.   \n[16] Muyang Li, Tianle Cai, Jiaxin Cao, Qinsheng Zhang, Han Cai, Junjie Bai, Yangqing Jia, Ming-Yu Liu, Kai Li, and Song Han. Distrifusion: Distributed parallel inference for high-resolution diffusion models. arXiv preprint arXiv:2402.19481, 2024.   \n[17] Muyang Li, Ji Lin, Yaoyao Ding, Zhijian Liu, Jun-Yan Zhu, and Song Han. Gan compression: Efficient architectures for interactive conditional gans. In CVPR, 2020.   \n[18] Muyang Li, Ji Lin, Chenlin Meng, Stefano Ermon, Song Han, and Jun-Yan Zhu. Efficient spatially sparse inference for conditional gans and diffusion models. In NeurIPS, 2022.   \n[19] Yanyu Li, Huan Wang, Qing Jin, Ju Hu, Pavlo Chemerys, Yun Fu, Yanzhi Wang, Sergey Tulyakov, and Jian Ren. Snapfusion: Text-to-image diffusion model on mobile devices within two seconds. NeurIPS, 2023.   \n[20] Shanchuan Lin, Anran Wang, and Xiao Yang. Sdxl-lightning: Progressive adversarial diffusion distillation. arXiv preprint arXiv:2402.13929, 2024.   \n[21] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740\u2013755. Springer, 2014.   \n[22] Zhijian Liu, Zhuoyang Zhang, Samir Khaki, Shang Yang, Haotian Tang, Chenfeng Xu, Kurt Keutzer, and Song Han. Sparse refinement for efficient high-resolution semantic segmentation. arXiv preprint arXiv:2407.19014, 2024.   \n[23] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps. arXiv preprint arXiv:2206.00927, 2022.   \n[24] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solve $^{++}$ : Fast solver for guided sampling of diffusion probabilistic models. arXiv preprint arXiv:2211.01095, 2022.   \n[25] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthesizing high-resolution images with few-step inference. arXiv preprint arXiv:2310.04378, 2023.   \n[26] Zhaoyang Lyu, Xudong Xu, Ceyuan Yang, Dahua Lin, and Bo Dai. Accelerating diffusion models via early stop of the diffusion process. arXiv preprint arXiv:2205.12524, 2022.   \n[27] Xinyin Ma, Gongfan Fang, and Xinchao Wang. Deepcache: Accelerating diffusion models for free. In Conference on Computer Vision and Pattern Recognition (CVPR), 2024.   \n[28] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans. On distillation of guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14297\u201314306, 2023.   \n[29] Taehong Moon, Moonseok Choi, EungGu Yun, Jongmin Yoon, Gayoung Lee, and Juho Lee. Early exiting for accelerated inference in diffusion models. In ICML 2023 Workshop on Structured Probabilistic Inference $\\{\\backslash\\mathcal{E}\\}$ Generative Modeling, 2023.   \n[30] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In ICML, 2021.   \n[31] Gaurav Parmar, Richard Zhang, and Jun-Yan Zhu. On aliased resizing and surprising subtleties in gan evaluation. In CVPR, 2022.   \n[32] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M\u00fcller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. In ICLR, 2024.   \n[33] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022.   \n[34] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical Image Computing and Computer-Assisted Intervention\u2013MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18, pages 234\u2013241. Springer, 2015.   \n[35] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In ICLR, 2021.   \n[36] Axel Sauer, Frederic Boesel, Tim Dockhorn, Andreas Blattmann, Patrick Esser, and Robin Rombach. Fast high-resolution image synthesis with latent adversarial diffusion distillation. arXiv preprint arXiv:2403.12015, 2024.   \n[37] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion distillation. arXiv preprint arXiv:2311.17042, 2023.   \n[38] Andy Shih, Suneel Belkhale, Stefano Ermon, Dorsa Sadigh, and Nima Anari. Parallel sampling of diffusion models. NeurIPS, 2023.   \n[39] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In ICLR, 2020.   \n[40] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. 2023.   \n[41] Haotian Tang, Shang Yang, Zhijian Liu, Ke Hong, Zhongming Yu, Xiuyu Li, Guohao Dai, Yu Wang, and Song Han. Torchsparse $^{++}$ : Efficient training and inference framework for sparse convolution on gpus. In Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture, pages 225\u2013239, 2023.   \n[42] Shengkun Tang, Yaqing Wang, Caiwen Ding, Yi Liang, Yao Li, and Dongkuan Xu. Deediff: Dynamic uncertainty-aware early exiting for accelerating diffusion model generation. arXiv preprint arXiv:2309.17074, 2023.   \n[43] Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Rapha\u00ebl Marinier, Marcin Michalski, and Sylvain Gelly. Fvd: A new metric for video generation. 2019.   \n[44] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report. arXiv preprint arXiv:2308.06571, 2023.   \n[45] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for bridging video and language. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5288\u20135296, 2016.   \n[46] Xingyi Yang, Daquan Zhou, Jiashi Feng, and Xinchao Wang. Diffusion probabilistic model made slim. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22552\u2013 22562, 2023.   \n[47] Hancheng Ye, Chong Yu, Peng Ye, Renqiu Xia, Yansong Tang, Jiwen Lu, Tao Chen, and Bo Zhang. Once for both: Single stage of importance and sparsity search for vision transformer compression. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5578\u20135588, 2024.   \n[48] Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao. Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365, 2015.   \n[49] Qinsheng Zhang and Yongxin Chen. Fast sampling of diffusion models with exponential integrator. In ICLR, 2022.   \n[50] Qinsheng Zhang, Molei Tao, and Yongxin Chen. gddim: Generalized denoising diffusion implicit models. 2022.   \n[51] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In CVPR, 2018.   \n[52] Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao, Hangjie Yuan, Zhiwu Qin, Xiang Wang, Deli Zhao, and Jingren Zhou. I2vgen-xl: High-quality image-to-video synthesis via cascaded diffusion models. arXiv preprint arXiv:2311.04145, 2023. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Due to the nine-page limitation of the manuscript, we provide more details and visualizations from the following aspects: ", "page_idx": 12}, {"type": "text", "text": "\u2022 Sec. A.1: Limitations and Broader Impacts.   \n\u2022 Sec. A.2: Method Explanation. \u2013 Sec. A.2.1: Theoretical Proof of Single-step Skipping. \u2013 Sec. A.2.2: Theoretical Proof of Two-step Skipping. \u2013 Sec. A.2.3: Theoretical Proof of $k$ -step Skipping. \u2013 Sec. A.2.4: Theoretical Relation between the 3rd-order Estimator and Optimal Skipping Strategy. \u2013 Sec. A.2.5: Algorithms.   \n\u2022 Sec. A.3: More Experimental Details. \u2013 Sec. A.3.1: More Evaluation Details. \u2013 Sec. A.3.2: Prompts in AIGCBench.   \n\u2022 Sec. A.4: More Generation Visualizations. ", "page_idx": 12}, {"type": "text", "text": "A.1 Limitations and Broader Impacts ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Currently, AdaptiveDiffusion is only evaluated on image generation and video generation tasks. A long-term vision is to achieve lossless acceleration on any modalities generation such as 3D and speech. Besides, although AdaptiveDiffusion can improve the speed on few-step (e.g., 10 steps) settings, it may not work for extreme few-step generation for the rapid changes of the latent features. ", "page_idx": 12}, {"type": "text", "text": "A.2 Method Explanations ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A.2.1 Error Estimation Induced by Single-step Skipping ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In Sec. 3.2, the error between the latents of the continuous noise update and one-step skipping of noise update is stated to be upper-bounded of the difference between the previous two output latents. Here, we provide the detailed proof of this statement. ", "page_idx": 12}, {"type": "text", "text": "Assumptions We first make the assumptions that:   \n(1) $\\epsilon_{\\theta}(x,t)$ is Lipschitz w.r.t to its paramters $x$ and $t$ ;   \n(2) The 1st-order difference $\\Delta x_{i}=x_{i}-x_{i+1}$ exists and is continuous for $0\\leq i\\leq T-1$ . ", "page_idx": 12}, {"type": "text", "text": "Proof Take $i$ -th step as an example to perform the one-step skipping of noise prediction, we can obtain the following update formulations. ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{x_{i+1}=f(i+1)\\cdot x_{i+2}-g(i+1)\\cdot\\epsilon_{\\theta}\\big(x_{i+2},t_{i+2}\\big);}\\\\ &{\\quad x_{i}=f(i)\\cdot x_{i+1}-g(i)\\cdot\\epsilon_{\\theta}\\big(x_{i+1},t_{i+1}\\big);}\\\\ &{x_{i-1}=f(i-1)\\cdot x_{i}-g(i-1)\\cdot\\epsilon_{\\theta}\\big(x_{i+1},t_{i+1}\\big);}\\\\ &{x_{i-2}=f(i-2)\\cdot x_{i-1}-g(i-2)\\cdot\\epsilon_{\\theta}\\big(x_{i-1},t_{i-1}\\big);}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Here, the $i$ -th step of noise prediction is replaced by reusing the $i+1$ -th noise prediction. Then, the error caused by skipping the $i$ -th noise update, $\\varepsilon_{i-1}=\\overline{{\\left\\|x_{i-1}\\stackrel{.}{-}}}\\ x_{i-1}^{o r i}\\right\\|$ , can be derived as follows. ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\varepsilon_{i-1}\\|=\\|g(i-1)\\cdot\\lbrack\\epsilon_{\\theta}(x_{i+1},t_{i+1})-\\epsilon_{\\theta}(x_{i},t_{i})\\rbrack\\|}\\\\ &{\\qquad\\quad=\\|g(i-1)\\cdot\\lbrack\\epsilon_{\\theta}(x_{i+1},t_{i+1})-\\epsilon_{\\theta}(x_{i},t_{i+1})+\\epsilon_{\\theta}(x_{i},t_{i+1})-\\epsilon_{\\theta}(x_{i},t_{i})\\rbrack\\|}\\\\ &{\\qquad\\quad\\leq\\|g(i-1)\\cdot\\mathcal{O}(t_{i}-t_{i+1})\\|+\\|g(i-1)\\cdot\\mathcal{O}(x_{i}-x_{i+1})\\|}\\\\ &{\\qquad\\quad=\\mathcal{O}(t_{i}-t_{i+1})+\\mathcal{O}(x_{i}-x_{i+1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "The proof utilizes the property that $\\left\\|\\epsilon_{\\theta}(x_{i},t_{i+1})-\\epsilon_{\\theta}(x_{i+1},t_{i+1})\\right\\|$ and $\\lVert\\epsilon_{\\theta}(x_{i},t_{i})-\\epsilon_{\\theta}(x_{i},t_{i+1})\\rVert$ are upperbounded by ${\\mathcal{O}}(x_{i}-x_{i+1})$ and $\\mathcal{O}(t_{i}-t_{i+1})$ respectively according to the Lipschitz continuity. ", "page_idx": 12}, {"type": "text", "text": "A.2.2 Error Estimation Induced by Two-step Skipping ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We further estimate the situation of consecutive two-step skipping of the noise prediction model. Assuming that noise prediction is skipped at the $i$ -th and $(i-1)$ -th steps, then we have the following formulation. ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{x_{i+1}=f(i+1)\\cdot x_{i+2}-g(i+1)\\cdot\\epsilon_{\\theta}\\big(x_{i+2},t_{i+2}\\big);}\\\\ &{\\quad x_{i}=f(i)\\cdot x_{i+1}-g(i)\\cdot\\epsilon_{\\theta}\\big(x_{i+1},t_{i+1}\\big);}\\\\ &{x_{i-1}=f(i-1)\\cdot x_{i}-g(i-1)\\cdot\\epsilon_{\\theta}\\big(x_{i+1},t_{i+1}\\big);}\\\\ &{x_{i-2}=f(i-2)\\cdot x_{i-1}-g(i-2)\\cdot\\epsilon_{\\theta}\\big(x_{i+1},t_{i+1}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The error, $\\varepsilon_{i-2}=\\left\\|x_{i-2}-x_{i-2}^{o r i}\\right\\|$ , can be derived as follows. ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\varepsilon_{i-2}=\\|f(i-2)\\cdot\\left(x_{i-1}-x_{i-1}^{\\sigma_{i}}\\right)-g(i-2)\\cdot\\left[\\varepsilon_{\\theta}\\big(x_{i+1},t_{i+1}\\big)-\\epsilon_{\\theta}\\big(x_{i-1}^{\\sigma_{i}},t_{i-1}\\big)\\right]\\|}\\\\ &{\\quad=\\|f(i-2)\\cdot g(i-1)\\cdot\\left[\\varepsilon_{\\theta}\\big(x_{i},t_{i}\\big)-\\epsilon_{\\theta}\\big(x_{i+1},t_{i+1}\\big)\\right]}\\\\ &{\\quad\\quad-g(i-2)\\cdot\\big[\\varepsilon_{\\theta}\\big(x_{i+1},t_{i+1}\\big)-\\epsilon_{\\theta}\\big(x_{i-1}^{\\sigma_{i}},t_{i-1}\\big)\\big]\\|}\\\\ &{\\quad=\\|h^{2}(i-1)\\cdot\\big[\\epsilon_{\\theta}\\big(x_{i},t_{i}\\big)-\\epsilon_{\\theta}\\big(x_{i},t_{i+1}\\big)+\\epsilon_{\\theta}\\big(x_{i},t_{i+1}\\big)-\\epsilon_{\\theta}\\big(x_{i-1}^{\\sigma_{i}},t_{i-1}\\big)\\big]}\\\\ &{\\quad\\quad-g(i-2)\\cdot\\big[\\epsilon_{\\theta}\\big(x_{i+1},t_{i+1}\\big)-\\epsilon_{\\theta}\\big(x_{i-1}^{\\sigma_{i}},t_{i-1}\\big)\\big]\\|}\\\\ &{\\quad\\le\\|h^{2}(i-1)\\cdot\\mathcal{O}(t_{i}-t_{i+1})\\|+\\|h^{2}(i-1)\\cdot\\mathcal{O}(x_{i}-x_{i+1})\\|}\\\\ &{\\quad\\quad+\\|g(i-2)\\cdot\\mathcal{O}(x_{i-1}-x_{i-1}^{\\sigma_{i}})\\|}\\\\ &{\\quad=\\mathcal{O}(t_{i}-t_{i+1})+\\mathcal{O}(x_{i}-x_{i+1})+\\mathcal{O}(t_{i-1}-t_{i})+\\mathcal{O}(x_{i-1}-x_{i}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "From the above derivations, it can be observed that the skipping error is clearly related to and upper-bounded by the accumulation of previous latent differences. Here $h^{2}\\left(\\dot{i}-\\bar{1}\\right)$ is defined as $h^{2}\\left(i-1\\right):=g\\left(\\hat{i}-1\\right)f\\left(i-2\\right)$ . The above conclusion of skipping error\u2019s upper bound can be easily extended to any situation where finite-step skipping is used. ", "page_idx": 13}, {"type": "text", "text": "A.2.3 Error Estimation Induced by $k$ -step Skipping ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Take $i$ -th step as an example to perform the $k$ -step $\\left(k\\geq2\\right)$ ) skipping of noise prediction, we can obtain the following update formulations. ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad_{2}\\varepsilon_{i-2}=f\\big(i-2\\big)\\cdot x_{i-1}-g\\big(i-2\\big)\\cdot\\omega\\big(x_{i+1},t_{i+1}\\big);}\\\\ &{\\qquad\\vdots}\\\\ &{\\qquad x_{i-1}=f\\big(i-k\\big)\\cdot x_{i+1}-g\\big(i-k\\big)\\cdot\\omega\\big(x_{i+1},t_{i+1}\\big);}\\\\ &{\\Rightarrow_{\\ell_{i}\\to i-\\ell}\\Bigg[\\mathtt{l a s s}_{i-\\ell}\\times x_{\\ell+\\ell}^{\\mathrm{ach}}\\Bigg]}\\\\ &{\\Rightarrow_{\\ell_{i}}f\\big(i-k\\big)\\Big(x_{i-1}\\!\\!-\\!x_{\\ell+\\ell+1}^{\\mathrm{ach}}\\big)-g\\big(i-k\\big)\\big[\\varphi\\big(x_{i+1}\\!\\!+\\!\\ell_{i+1}\\big)-\\varphi\\big(x_{i}\\!\\!-\\!\\mathrm{i}\\!+\\!\\ell_{i}\\!\\big(x_{i}\\!\\!-\\!\\mathrm{i}\\!+\\!\\ell_{i+1}\\!\\big)\\big)\\Big]}\\\\ &{\\qquad\\leqslant\\int\\big(i-k\\big)\\varepsilon_{i-1}\\!\\!+\\!g\\big(i-k\\big)\\left[\\varphi\\big(x_{i+1}\\!\\!+\\!\\ell_{i+1}\\big)\\!-\\!e\\big(x_{i}\\!\\!+\\!\\ell_{i+1}\\!\\!+\\!\\ell_{i}\\!+\\!\\ell_{i+1}\\big)\\right]}\\\\ &{\\leqslant\\displaystyle\\sum_{i=1}^{k}\\left[\\mathtt{l a s s}^{-\\mathrm{ach}}\\big(i-m\\big)\\cdot\\mathcal{O}\\big(\\mathrm{i}_{\\ell-m+1}-t_{i-m+2}\\big)\\right]}\\\\ &{\\qquad+\\displaystyle\\sum_{i=1}^{k}\\left[\\mathtt{l a s s}^{-\\mathrm{ach}}\\big(i-m\\big)\\cdot\\mathcal{O}\\big(x_{i-m+1}-x_{\\ell-m+2}\\big)\\right]}\\\\ &{\\qquad+\\log\\big(\\mathrm{i}-k\\big)\\cdot\\mathcal{O}\\big(x_{i+1}\\!-\\!x_{\\ell+1}\\big)\\!+\\!\\|g\\big(i\\!-\\!k\\big)\\cdot\\mathcal{O}\\big(\\mathrm{i}_{\\ell-k+1}\\!-\\!t_{i-m+2}\\big)\\|}\\\\ &{=\\displaystyle\\sum_{i=1}^{k}\\mathcal{O}\\big(\\iota_{i-m+1}-t_{i-m+2}\\big)+\\mathcal{O}\\big(x_{i-m+1}-x_{\\ell-m+2}\\big).}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The derivation also utilizes the property that $\\left\\|\\epsilon_{\\theta}(x_{i},t_{i+1})-\\epsilon_{\\theta}(x_{i+1},t_{i+1})\\right\\|$ and $\\lVert\\epsilon_{\\theta}(x_{i},t_{i})-\\epsilon_{\\theta}(x_{i},t_{i+1})\\rVert$ are upper-bounded by ${\\mathcal{O}}(x_{i}-x_{i+1})$ and $\\mathcal{O}(t_{i}-t_{i+1})$ respectively according to the Lipschitz continuity. Here $h^{k-m+1}\\left(i-m\\right)$ is defined as $\\begin{array}{r}{h^{k-m+1}\\left(i-m\\right):=g\\left(i-m\\right)\\prod_{j=1}^{k-m}f\\left(i-m-j\\right)}\\end{array}$ . ", "page_idx": 13}, {"type": "text", "text": "From the above derivation, it can be observed that the error of an arbitrary $k$ -step skipping scheme is related to and upper-bounded by the accumulation of previous latent differences. Therefore, if the skipping step of noise ", "page_idx": 13}, {"type": "text", "text": "prediction is large, the upper bound of the error will naturally increase, which is also empirically demonstrated by Fig. 5b. ", "page_idx": 14}, {"type": "text", "text": "A.2.4 Theoretical Relation between the 3rd-order Estimator and Optimal Skipping Strategy ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "To explore the theoretical relationship between the third-order estimator and the skipping strategy, we need to formulate the difference between the neighboring noise predictions. According to Eq. (1), we can get the following first-order differential equations regarding the latent $_x$ . ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Delta x_{i}=x_{i}-x_{i+1}=\\left[1-f\\left(i\\right)\\right]x_{i+1}-g\\left(i\\right)\\cdot\\epsilon_{\\theta}\\left(x_{i+1},t_{i+1}\\right);}\\\\ {\\Delta x_{i-1}=x_{i-1}-x_{i}=\\left[1-f\\left(i-1\\right)\\right]x_{i}-g\\left(i-1\\right)\\cdot\\epsilon_{\\theta}\\left(x_{i},t_{i}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Now, let $u\\left(i\\right):=1-f\\left(i-1\\right)$ , and we further derive the second-order differential equations based on the above equations. ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathcal{A}_{x\\i-1}-\\varDelta x_{i}}\\\\ &{=u\\left(i\\right)x_{i}-u\\left(i+1\\right)x_{i+1}+g\\left(i\\right)\\cdot\\epsilon_{\\theta}\\left(x_{i+1},t_{i+1}\\right)-g\\left(i-1\\right)\\cdot\\epsilon_{\\theta}\\left(x_{i},t_{i}\\right)}\\\\ &{=u\\left(i\\right)\\left(x_{i}-x_{i-1}\\right)+u\\left(i\\right)x_{i-1}-u\\left(i+1\\right)\\left(x_{i+1}-x_{i}\\right)-u\\left(i+1\\right)x_{i}+g\\left(i\\right)\\cdot\\epsilon_{\\theta}\\left(x_{i+1},t_{i+1}\\right)}\\\\ &{\\quad-g\\left(i-1\\right)\\cdot\\epsilon_{\\theta}\\left(x_{i},t_{i}\\right)}\\\\ &{=u\\left(i\\right)\\varDelta x_{i-1}-u\\left(i+1\\right)\\varDelta x_{i}+\\varDelta\\left[u\\left(i\\right)x_{i-1}\\right]+g\\left(i\\right)\\cdot\\epsilon_{\\theta}\\left(x_{i+1},t_{i+1}\\right)-g\\left(i-1\\right)\\cdot\\epsilon_{\\theta}\\left(x_{i},t_{i}\\right)}\\\\ &{=u\\left(i\\right)\\varDelta x_{i-1}-u\\left(i+1\\right)\\varDelta x_{i}+\\varDelta\\left[u\\left(i\\right)x_{i-1}\\right]+g\\left(i\\right)\\left[\\epsilon_{\\theta}\\left(x_{i+1},t_{i+1}\\right)-\\epsilon_{\\theta}\\left(x_{i},t_{i}\\right)\\right]}\\\\ &{\\quad+\\left[g\\left(i\\right)-g\\left(i-1\\right)\\right]\\epsilon_{\\theta}\\left(x_{i},t_{i}\\right)}\\\\ &{=u\\left(i\\right)\\varDelta x_{i-1}-u\\left(i+1\\right)\\varDelta x_{i}+\\varDelta\\left[u\\left(i\\right)x_{i-1}\\right]-g\\left(i\\right)\\varDelta\\epsilon_{\\theta}^{i}-\\varDelta g\\left(i\\right)\\cdot\\epsilon_{\\theta}\\left(x_{i},t_{i}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "After simplification of the above equation, we can get the following formulation: ", "page_idx": 14}, {"type": "equation", "text": "$$\nf\\left(i-1\\right)\\Delta x_{i-1}-f\\left(i\\right)\\Delta x_{i}=\\Delta\\left[u\\left(i\\right)x_{i-1}\\right]-g\\left(i\\right)\\Delta\\epsilon_{\\theta}^{i}-\\Delta g\\left(i\\right)\\cdot\\epsilon_{\\theta}\\left(x_{i},t_{i}\\right).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "From the above equation, we can observe that the difference between noise predictions $\\varDelta\\epsilon_{\\theta}^{i}$ is related to the firstand second-order derivatives of $x_{i}$ , as well as the noise prediction $\\epsilon_{\\theta}\\left(x_{i},t_{i}\\right)$ . Therefore, it would be difficult to estimate the difference without $\\epsilon_{\\theta}\\left(x_{i},t_{i}\\right)$ . Now we consider the third-order differential equation. From the above equation, we further obtain the following formulation. ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad f\\left(i\\right)\\Delta x_{i}-f\\left(i+1\\right)\\Delta x_{i+1}=\\Delta\\left[u\\left(i+1\\right)x_{i}\\right]-g\\left(i+1\\right)\\Delta\\epsilon_{\\theta}^{i+1}-\\Delta g\\left(i+1\\right)\\cdot\\epsilon_{\\theta}\\left(x_{i+1},t_{i+1}\\right);}\\\\ &{\\Rightarrow\\Delta\\left[f\\left(i-1\\right)\\Delta x_{i-1}\\right]-\\Delta\\left[f\\left(i\\right)\\Delta x_{i}\\right]=\\Delta^{(2)}\\left[u\\left(i\\right)x_{i-1}\\right]-\\Delta\\left[g\\left(i\\right)\\Delta\\epsilon_{\\theta}^{i}\\right]-\\Delta\\left[\\Delta g\\left(i\\right)\\cdot\\epsilon_{\\theta}\\left(x_{i},t_{i}\\right)\\right];}\\\\ &{\\Rightarrow\\Delta\\left[\\Delta g\\left(i\\right)\\cdot\\epsilon_{\\theta}\\left(x_{i},t_{i}\\right)\\right]=-\\Delta^{(2)}\\left[f\\left(i-1\\right)\\Delta x_{i-1}\\right]+\\Delta^{(2)}\\left[u\\left(i\\right)x_{i-1}\\right]-\\Delta\\left[g\\left(i\\right)\\Delta\\epsilon_{\\theta}^{i}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "From the above equation, it can be observed that the difference of the neighboring noise predictions is explicitly related to the third- and second-order derivatives of $x_{i}$ , as well as the second-order derivative of $\\epsilon_{\\theta}^{i}$ . Since $\\begin{array}{r}{\\operatorname*{lim}_{i\\to0}f\\left(i\\right)\\,=\\,1,\\operatorname*{lim}_{i\\to0}u\\left(i\\right)\\,=\\,0,\\operatorname*{lim}_{i\\to0}g\\left(i\\right)\\,=\\,0}\\end{array}$ , we can finally get the conclusion that $\\Delta\\epsilon_{\\theta}^{i}|_{i\\rightarrow0}=$ ${\\mathcal O}\\left({\\varDelta^{(3)}x_{i-1}}\\right)$ . ", "page_idx": 14}, {"type": "text", "text": "A.2.5 Algorithms", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Algorithm of greedy search for optimal skipping path. The specific details of the greedy search algorithm used as the optimal denoising path are shown in Alg. (1). ", "page_idx": 14}, {"type": "text", "text": "Algorithm 1 Greedy Search for the Optimal Skipping Path. ", "page_idx": 15}, {"type": "text", "text": "Input: Noise Prediction Model $\\epsilon_{\\theta}$ , Sampling Scheduler $\\phi$ , Decoder $\\mathcal{F}_{d}$ , Target Skipping Step   \nNumber $N$ , Sample Step $T$ , Conditional embedding $c$ ;   \n1: Initialize Skipping Path $S=[\\mathrm{True}]*T$ , Current Skipping Step Number $n_{s k i p}=0$ .   \n2: Compute $x_{0}^{o r i}$ by Eq. (1).   \n3: while $n_{s k i p}<N$ do   \n4: Initialize $d f=[]$ ;   \n5: for $i$ in range $(T-1)$ do   \n6: if $\\scriptstyle S[i]==i$ True then   \n7: temp_path $=s$ .copy();   \n8: temp_path $[i]{=}]$ False;   \n9: Generate xt0empby Eq. (2);   \n10: Compute $\\ell_{1}$ difference $\\mathcal{L}_{0}=\\|x_{0}^{t e m p}-x_{0}^{o r i}\\|$ ;   \n11: $d f$ .append $(\\mathcal{L}_{0})$ ;   \n12: end if   \n13: end for   \n14: index $=\\operatorname{argmin}(d f)$ ;   \n15: Set $S[\\mathrm{index}]=\\mathrm{F}$ alse;   \n16: end while   \n17: return $\\boldsymbol{S}$ . ", "page_idx": 15}, {"type": "text", "text": "Algorithm of the overall skipping strategy in AdaptiveDiffusion. The designed skipping strategy used in our AdaptiveDiffusion are elaborated in Alg. (2). ", "page_idx": 15}, {"type": "text", "text": "Algorithm 2 AdaptiveDiffusion. ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Input: Noise Prediction Model $\\epsilon_{\\theta}$ , Sampling Scheduler $\\phi$ , Decoder $\\mathcal{F}_{d}$ , Sample Step $T$ , Conditional   \nembedding $c$ , Maximum Skipping Step Number $C_{\\mathrm{max}}$ , Threshold $\\delta$ ;   \n1: Initialize Random Noise $x$ , Skipping Path $\\begin{array}{r}{\\mathcal{S}=[]}\\end{array}$ , Previous Differential List $P_{\\mathrm{diff}}=[]$ , Previous   \nLatent List $P_{\\mathrm{latent}}=[]$ .   \n2: for $i$ in range $(T-1)$ do   \n3: if $i\\leq2$ then   \n4: $\\mathrm{O}_{p v}=\\mathrm{O}=\\epsilon(x,i);$ ;   \n5: if $i\\geq1$ then   \n6: Pdiff.append(\u2225x \u2212Platent[\u22121]\u2225);   \n7: end if   \n8: else   \n9: if $\\scriptstyle{S\\left[-1\\right]==}$ True then   \n10: $\\bar{\\mathrm{O}}_{p v}\\bar{=}\\mathrm{O}=\\epsilon(x,i);$ ;   \n11: else   \n12: $\\mathrm{O}=\\mathrm{O}_{p v}$ ;   \n13: end if   \n14: end if   \n15: Compute $\\phi(x)$ by Eq. (1);   \n16: if $i\\geq3$ then   \n17: $\\boldsymbol{S}$ .append $\\overleftarrow{\\{}\\leftarrow{}\\xi(x))}$ in Eq. (4);   \n18: end if   \n19: Platent.append(x);   \n20: end for   \n21: return ${\\mathcal{F}}_{d}(x)$ . ", "page_idx": 15}, {"type": "text", "text": "A.3 Experimental Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.3.1 More Evaluation Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Statistical Analysis of the Estimated Skipping Path. In Fig. 5, we aim to evaluate the correlation between the estimated skipping paths by our method and the searched optimal skipping paths under different target skipping numbers. This evaluation is crucial to validate the effectiveness of our approach in accurately predicting skipping paths. The paths are represented as sequences of binary choices (0 for a skipping step, 1 for a non-skipping step) at each timestep. We employ a $\\chi^{2}$ test of independence to assess the statistical correlation between the estimated and optimal paths. We first randomly select several prompts from the test set and use the greedy search algorithm to greedily search the optimal skipping paths under different skipping target steps. For the estimated skipping paths, we discard the setting of $C_{\\mathrm{max}}$ to test the effectiveness of the third-order estimation without regularization and refine the threshold $\\delta$ in a wide range to achieve various skipping paths with different skipping step numbers. Then, for each skipping step number, we construct a $2\\!\\times\\!2$ contingency table from the searched optimal skipping paths and the estimated skipping paths, with the same skipping step numbers. Finally, using the contingency table, we compute the $\\chi^{2}$ statistics and the corresponding $p$ -value to assess the independence between the estimated and the searched optimal skipping paths. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "Evaluation Tools and Details. Following [16], we use TorchMetrics \\* to calculate PSNR and LPIPS, and use CleanFID $[31]^{\\dagger}$ to calculate FID. Since our method is an adaptive method which means it can choose a suitable skipping path for different prompts, we set batch size to 1 during evaluation. The MACs reported in our experiments are the total MACs in the denoising process following [16]. Besides, the reported MACs, speedup ratio, and latency are the average over the whole dataset. ", "page_idx": 16}, {"type": "text", "text": "CodeBase. For a fair comparison, when compared with Adaptive DPM-Solver, we use the official codebase of DPM-Solver\u2021. Experiments for LDM-4-G are based on the official LDM codebase\u00a7and DeepCache codebase\u00b6. Other experiments are based on Diffusers||. ", "page_idx": 16}, {"type": "text", "text": "Details of Reproduced DeepCache. In this paper, we mainly compare our AdaptiveDiffusion with DeepCache. To ensure the generation quality, we set cache_interval $_{-3}$ and cache_branch_ $\\_\\mathrm{d}{=}3$ for SD-v1-5, SDXL, and ModelScope. Since the memory cost will be largely improved on video generation tasks especially high-resolution tasks (e.g., I2VGen-XL), We set cache_interva ${\\underline{{1}}}\\!=\\!3$ and cache_branch_i $\\mathrm{{d}}\\!=\\!0$ to reduce the memory costs. For class-conditional image generation task, we follow the official setting and set cache_interva $\\mathtt{1}\\!=\\!\\mathtt{1}\\,0$ . ", "page_idx": 16}, {"type": "text", "text": "A.3.2 Prompts in AIGCBench ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We randomly sample 100 prompts and their corresponding images as the test set for I2V task. Here, we give the list of the selected prompts in AIGCBench in Fig. 9 and Fig. 10. The selected test set includes different styles of images and prompts such as animation, realism, and oil painting. ", "page_idx": 16}, {"type": "text", "text": "A.3.3 Experiments on Unconditional Image Generation ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We provide the acceleration performance of different methods on pure image generation for further comparisons. Specifically, following the experimental setting in Deepcache, we conduct unconditional image generation experiments on CIFAR10 [14] and LSUN [48] datasets. As shown in Table 6, our method achieves a larger speedup ratio and higher image quality than Deepcache on both benchmarks. ", "page_idx": 16}, {"type": "table", "img_path": "cS63YtJ49A/tmp/a15289fcf74e61cae86490825326bac79bba236c3888198b3233012e43081e10.jpg", "table_caption": ["Table 6: Performance on Unconditional Image Generation. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "A.3.4 Effectiveness on SDE Solver ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We also explore the effectiveness of our work on SDE solver. Compared with the ODE solver, the SDE solver includes an additional noise item for the latent update, which is unpredictable by previous randomly generated noises. When the magnitude of random noise is not ignorable, the third-order derivative of the neighboring latents cannot accurately evaluate the difference between the neighboring noise predictions. Therefore, to apply our method to SDE solvers, we should design an additional indicator that decides whether the randomly generated noise is minor enough or relatively unchanged to trigger the third-order estimator. In this case, we design an additional third-order estimator for the scaled randomly generated noise. When the third-order derivatives of both the latent and the scaled randomly generated noises are under the respective threshold, the noise prediction can be skipped by reusing the cached noise. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "To validate the effectiveness of our improved method, we conduct experiments for SDXL with the SDE-DPM solver on COCO2017. The results are shown in the following table. Compared with Deepcache, our method can achieve higher image quality with a comparable speedup ratio, indicating the effectiveness of AdaptiveDiffusion on SDE solvers. ", "page_idx": 17}, {"type": "table", "img_path": "cS63YtJ49A/tmp/149bdd5fb888063eb43c43cf3d21871cfd78f5a6e47d5bba02c9df2c7f01e3df.jpg", "table_caption": ["Table 7: Performance on SDE-DPM Solver. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "A.4 More Generation Visualizations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Here, we further show more qualitative results tested on text-to-image task using LDM-4 on MS-COCO 2017 and text-to-video generation task using ModelScopeT2V on MSR-VIT, which are illustrated in Fig. 11 and Fig. 12 respectively. ", "page_idx": 17}, {"type": "text", "text": "1. Behold a valiant knight in the throes of exploring a cave surrounded by the secret chambers of an underground laboratory, envisioned as vibrant pixel art   \n2. Discover a mischievous fairy, eagerly searching for hidden treasure amidst the neon-lit skyscrapers of a futuristic city, artfully rendered through the   \nabstract lens of picasso   \n3. Behold a playful panda in the throes of carefully repairing a broken robot surrounded by the dusty streets of an old western town at high noon, envisioned   \nwith the soft touch of watercolor   \n4. Discover a mischievous fairy, running a marathon amidst the tranquil waters of a serene lake, artfully rendered with the soft touch of watercolor   \n5. Encounter a noble king as they are engaged in valiantly fighting a monstrous beast a mysterious crossroads in a mystical forest, all depicted with the soft   \ntouch of watercolor   \n6. Amidst the front lines of an ancient battlefield, a gentle ogre is building a snowman, captured like a lively cartoon   \n7. Behold a valiant knight in the throes of cleverly solving a puzzle surrounded by a bustling space station orbiting earth, envisioned as a stunning 3d render   \n8. Behold a wailing banshee in the throes of casting a spell surrounded by the crashing waves of the cerulean sea, envisioned in the impassioned strokes of   \nvan gogh   \n9. Behold a rogue robot in the throes of exploring a cave surrounded by a creaking pirate ship, envisioned with the soft touch of watercolor   \n10. Amidst the crashing waves of the cerulean sea, a wise old man is soaring through the sky, captured as vibrant pixel art   \n11. Amidst a sun-dappled forest, a valiant knight is casting a spell, captured with photorealistic precision   \n12. Within the realm of the neon-lit skyscrapers of a futuristic city, a rogue robot casting a spell, each moment immortalized in the style of an oil painting   \n13. Amidst the sandy shores of a deserted island, a noble king is casting a spell, captured with the stark realism of a photo   \n14. Discover a wailing banshee, hungrily eating a feast amidst the lush canopy of a deep jungle, artfully rendered with the soft touch of watercolor   \n15. Amidst the sandy shores of a deserted island, a brave girl is meticulously investigating a mysterious crime scene, captured as vibrant pixel art   \n16. Encounter a mischievous fairy as they are engaged in conducting an experiment the front lines of an ancient battlefield, all depicted as vibrant pixel art   \n17. Discover a forest-dwelling nymph, exploring a cave amidst the crashing waves of the cerulean sea, artfully rendered through the abstract lens of picasso   \n18. Amidst the tranquil waters of a serene lake, a mischievous fairy is hunting for ghosts, captured with photorealistic precision   \n19. Amidst the eerie halls of a haunted house, a playful panda is secretly whispering to animals, captured in the style of an oil painting   \n20. Amidst the crashing waves of the cerulean sea, a wise old man is performing magic tricks, captured like a lively cartoon   \n21. Discover a noble king, brewing a potion amidst the sandy shores of a deserted island, artfully rendered as a stunning 3d render   \n22. Amidst the sandy shores of a deserted island, a battle-scarred cyborg is building a snowman, captured with the stark realism of a photo   \n23. Amidst the sandy shores of a deserted island, a playful panda is casting a spell, captured in the style of an oil painting   \n24. Discover a fearsome dragon, secretly whispering to animals amidst the vibrant heart of a bustling market square, artfully rendered with the soft touch of   \nwatercolor   \n25. Behold a treasure-seeking pirate in the throes of exploring a cave surrounded by the echoing depths of a magical cave, envisioned through the abstract   \nlens of picasso   \n26. Discover a mischievous fairy, running a marathon amidst a bustling space station orbiting earth, artfully rendered as vibrant pixel art   \n27. Behold a wise old man in the throes of painting a masterpiece surrounded by the neon-lit skyscrapers of a futuristic city, envisioned as vibrant pixel art   \n28. Within the realm of the echoing depths of a magical cave, a gentle ogre casting a spell, each moment immortalized with the soft touch of watercolor   \n29. Discover a playful panda, running a marathon amidst the tranquil waters of a serene lake, artfully rendered in the impassioned strokes of van gogh   \n30. Encounter a noble king as they are engaged in building a snowman a blooming enchanted garden, all depicted with photorealistic precision   \n31. Within the realm of the echoing depths of a magical cave, a battle-scarred cyborg masterfully riding a bike, each moment immortalized through the   \nabstract lens of picasso   \n32. Amidst the narrow alleys of a medieval town, a valiant knight is secretly whispering to animals, captured in the style of an oil painting   \n33. Discover a forest-dwelling nymph, deciphering a map amidst a creaking pirate ship, artfully rendered like a lively cartoon   \n34. Behold a gentle ogre in the throes of masterfully riding a bike surrounded by the backdrop of an alien planet's red skies, envisioned in the impassioned   \nstrokes of van gogh   \n35. Encounter a mystical mermaid as they are engaged in cleverly solving a puzzle a mysterious crossroads in a mystical forest, all depicted with the stark   \nrealism of a photo   \n36. Behold a curious alien in the throes of playing the violin surrounded by a creaking pirate ship, envisioned as vibrant pixel art   \n37. Behold a curious alien in the throes of skillfully strumming the guitar surrounded by the narrow alleys of a medieval town, envisioned in the style of an oil   \npainting   \n38. Behold a noble king in the throes of casting a spell surrounded by the neon-lit skyscrapers of a futuristic city, envisioned with photorealistic precision   \n39. Discover a brave girl, secretly whispering to animals amidst the dusty streets of an old western town at high noon, artfully rendered as a stunning 3d   \nrender   \n40. Encounter a playful panda as they are engaged in playing the violin the tranquil waters of a serene lake, all depicted through the abstract lens of picasso   \n41. Encounter a treasure-seeking pirate as they are engaged in deciphering a map the backdrop of an alien planet's red skies, all depicted with the stark   \nrealism of a photo   \n42. Encounter a brave girl as they are engaged in conducting an experiment the sandy shores of a deserted island, all depicted with the soft touch of   \nwatercolor   \n43. Within the realm of the eerie halls of a haunted house, a treasure-seeking pirate hunting for ghosts, each moment immortalized in the style of an oil   \npainting   \n44. Encounter a fearsome dragon as they are engaged in exploring a cave the sandy shores of a deserted island, all depicted in the style of an oil painting   \n45. Discover a brave girl, skillfully strumming the guitar amidst a sun-dappled forest, artfully rendered like a lively cartoon   \n46. Discover a valiant knight, building a snowman amidst the backdrop of an alien planet's red skies, artfully rendered in the style of an oil painting   \n47. Encounter a playful panda as they are engaged in conducting an experiment the frosty peak of a snowy mountain, all depicted as a stunning 3d render   \n48. Encounter a noble king as they are engaged in hunting for ghosts the echoing depths of a magical cave, all depicted through the abstract lens of picasso   \n49. Encounter a mystical mermaid as they are engaged in hungrily eating a feast the ancient walls of a crumbling castle, all depicted in the impassioned   \nstrokes of van gogh   \n50. Amidst a sun-dappled forest, a mischievous fairy is carefully repairing a broken robot, captured in the style of an oil painting   \n51. Encounter a noble king as they are engaged in deciphering a map the frosty peak of a snowy mountain, all depicted in the style of an oil painting   \n52. Amidst a creaking pirate ship, a curious alien is conducting an experiment, captured in the impassioned strokes of van gogh   \n53. Discover a curious alien, carefully repairing a broken robot amidst the eerie halls of a haunted house, artfully rendered like a lively cartoon   \n54. Encounter a genius scientist as they are engaged in soaring through the sky the vibrant heart of a bustling market square, all depicted like a lively cartoon   \n55. Discover a forest-dwelling nymph, deciphering a map amidst the ancient walls of a crumbling castle, artfully rendered like a lively cartoon   \n56. Amidst the sandy shores of a deserted island, a mystical mermaid is painting a masterpiece, captured like a lively cartoon   \n57. Discover a time-traveling scholar, performing magic tricks amidst a sun-dappled forest, artfully rendered in the style of an oil painting   \n58. Amidst the vibrant heart of a bustling market square, a mystical mermaid is cleverly solving a puzzle, captured as vibrant pixel art   \n59. Behold a time-traveling scholar in the throes of painting a masterpiece surrounded by a sun-dappled forest, envisioned with the soft touch of watercolor   \n60. Encounter a gentle ogre as they are engaged in masterfully riding a bike a bustling space station orbiting earth, all depicted like a lively cartoon ", "page_idx": 18}, {"type": "text", "text": "61. Encounter an adventurous astronaut as they are engaged in brewing a potion a bustling space station orbiting earth, all depicted with photorealistic precision   \n62. Encounter a playful panda as they are engaged in cleverly solving a puzzle the narrow alleys of a medieval town, all depicted through the abstract lens of picasso   \n63. Discover a noble king, painting a masterpiece amidst the crashing waves of the cerulean sea, artfully rendered through the abstract lens of picasso   \n64. Amidst the ancient walls of a crumbling castle, a time-traveling scholar is painting a masterpiece, captured as vibrant pixel art   \n65. Discover a brave girl, secretly whispering to animals amidst a blooming enchanted garden, artfully rendered with photorealistic precision   \n66. Behold a gentle ogre in the throes of hungrily eating a feast surrounded by the eerie halls of a haunted house, envisioned with photorealistic precision   \n67. Behold a genius scientist in the throes of secretly whispering to animals surrounded by the sandy shores of a deserted island, envisioned like a lively cartoon   \n68. Encounter a brave girl as they are engaged in conducting an experiment a sun-dappled forest, all depicted with the stark realism of a photo   \n69. Within the realm of a blooming enchanted garden, a rogue robot deciphering a map, each moment immortalized like a lively cartoon   \n70. Within the realm of a sun-dappled forest, a curious alien valiantly fighting a monstrous beast, each moment immortalized through the abstract lens of picasso   \n71. Encounter a valiant knight as they are engaged in gracefully dancing under the moonlight the narrow alleys of a medieval town, all depicted with the soft touch of watercolor   \n72. Within the realm of a creaking pirate ship, a mystical mermaid cleverly solving a puzzle, each moment immortalized with the soft touch of watercolor   \n73. Amidst the sandy shores of a deserted island, a fearsome dragon is hunting for ghosts, captured with the soft touch of watercolor   \n74. Encounter a wailing banshee as they are engaged in conducting an experiment the ruins of an ancient temple, all depicted with the stark realism of a photo   \n75. Amidst a sun-dappled forest, a brave girl is meticulously investigating a mysterious crime scene, captured as a stunning 3d render   \n76. Amidst the frosty peak of a snowy mountain, a wise old man is performing magic tricks, captured like a lively cartoon   \n77. Discover a treasure-seeking pirate, exploring a cave amidst the vibrant heart of a bustling market square, artfully rendered in the impassioned strokes of van gogh   \n78. Amidst the narrow alleys of a medieval town, a gentle ogre is soaring through the sky, captured with the stark realism of a photo   \n79. Behold a treasure-seeking pirate in the throes of hungrily eating a feast surrounded by the ruins of an ancient temple, envisioned in the impassioned strokes of van gogh   \n80. Behold a wailing banshee in the throes of exploring a cave surrounded by the vibrant heart of a bustling market square, envisioned as a stunning 3d render   \n81. Amidst the ruins of an ancient temple, a playful panda is running a marathon, captured like a lively cartoon   \n82. Discover a wise old man, brewing a potion amidst the eerie halls of a haunted house, artfully rendered in the style of an oil painting   \n83. Within the realm of the dusty streets of an old western town at high noon, a fearsome dragon meticulously investigating a mysterious crime scene, each moment immortalized with photorealistic precision   \n84. Behold a genius scientist in the throes of playing the violin surrounded by the frosty peak of a snowy mountain, envisioned in the style of an oil painting   \n85. Discover a noble king, painting a masterpiece amidst the ruins of an ancient temple, artfully rendered with photorealistic precision   \n86. Discover an adventurous astronaut, conducting an experiment amidst the ruins of an ancient temple, artfully rendered like a lively cartoon   \n87. Discover a genius scientist, conducting an experiment amidst a bustling space station orbiting earth, artfully rendered as vibrant pixel art   \n88. Within the realm of the frosty peak of a snowy mountain, a fearsome dragon hunting for ghosts, each moment immortalized with photorealistic precision   \n89. Encounter a treasure-seeking pirate as they are engaged in gracefully dancing under the moonlight the dusty streets of an old western town at high noon, all depicted as a stunning 3d render   \n90. Within the realm of the front lines of an ancient battlefield, a mischievous fairy meticulously investigating a mysterious crime scene, each moment immortalized in the impassioned strokes of van gogh   \n91. Encounter a rogue robot as they are engaged in deciphering a map the sandy shores of a deserted island, all depicted with the soft touch of watercolor   \n92. Encounter a curious alien as they are engaged in deciphering a map the backdrop of an alien planet's red skies, all depicted like a lively cartoon   \n93. Discover a forest-dwelling nymph, brewing a potion amidst the lush canopy of a deep jungle, artfully rendered with photorealistic precision   \n94. Encounter a genius scientist as they are engaged in painting a masterpiece the sandy shores of a deserted island, all depicted like a lively cartoon   \n95. Within the realm of the vibrant heart of a bustling market square, a playful panda deciphering a map, each moment immortalized as vibrant pixel art   \n96. Within the realm of a sun-dappled forest, a mystical mermaid brewing a potion, each moment immortalized in the impassioned strokes of van gogh   \n97. Discover a curious alien, playing the violin amidst the secret chambers of an underground laboratory, artfully rendered through the abstract lens of picasso   \n98. Behold a brave girl in the throes of playing the violin surrounded by a mysterious crossroads in a mystical forest, envisioned through the abstract lens of picasso   \n99. Discover a mischievous fairy, painting a masterpiece amidst the secret chambers of an underground laboratory, artfully rendered with photorealistic precision   \n100.Within the realm of the frosty peak of a snowy mountain, a playful panda meticulously investigating a mysterious crime scene, each moment immortalized through the abstract lens of picasso ", "page_idx": 19}, {"type": "text", "text": "Figure 10: Prompts used in AIGCBench. ", "text_level": 1, "page_idx": 19}, {"type": "image", "img_path": "cS63YtJ49A/tmp/a00b221b090363b3fe9d9696c228324ddf93c9a18d920952f1e62b031824c452.jpg", "img_caption": ["Figure 11: Qualitative results of text-to-image generation task using LDM-4 on ImageNet 256x256 benchmark. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "cS63YtJ49A/tmp/7cdaae3f5e2d9e1501f8511ea7c7591aae1dc6465cd1e28b1620d5e07390f26b.jpg", "img_caption": ["Figure 12: Qualitative results of text-to-video generation task using ModelScopeT2V on MSR-VIT benchmark. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "1. Claims ", "page_idx": 21}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The motivations and contributions are well depicted and summarized in the abstract and introduction. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 21}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: Limitations are discussed in Appendix A.1. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 21}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The theory assumptions and proofs are described in Section 3 and Appendix A.2. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The selected models and benchmarks are clearly and fully presented in Section 4. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 22}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: The code is available at https://github.com/UniModal4Reasoning/ AdaptiveDiffusion. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: All details are carefully presented in Section 4.1. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.   \n\u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 23}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We report $\\chi^{2}$ stats and $p$ -value between the greedy searched paths and the proposed third-order estimated paths at different skipping targets. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 23}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The information of the employed compute resources is elaborated in the implementation details of Section 4. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We make sure that the research conforms with the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 24}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: Our proposed method currently has no apparent societal impacts. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 24}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: This paper poses no such risks. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. ", "page_idx": 24}, {"type": "text", "text": "\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 25}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We have cited all papers that we used for experiments. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 25}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: This paper does not release new assets. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 25}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 25}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: This paper does not involve research with human subjects. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 26}]