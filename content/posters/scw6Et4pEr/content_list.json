[{"type": "text", "text": "DeepLag: Discovering Deep Lagrangian Dynamics for Intuitive Fluid Prediction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Qilong Ma,\u2217 Haixu Wu,\u2217 Lanxiang Xing, Shangchen Miao, Mingsheng Long School of Software, BNRist, Tsinghua University, China {mql22,wuhx23,xlx22,msc21}@mails.tsinghua.edu.cn, mingsheng@tsinghua.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Accurately predicting the future fluid is vital to extensive areas such as meteorology, oceanology, and aerodynamics. However, since the fluid is usually observed from the Eulerian perspective, its moving and intricate dynamics are seriously obscured and confounded in static grids, bringing thorny challenges to the prediction. This paper introduces a new Lagrangian-Eulerian combined paradigm to tackle the tanglesome fluid dynamics. Instead of solely predicting the future based on Eulerian observations, we propose DeepLag to discover hidden Lagrangian dynamics within the fluid by tracking the movements of adaptively sampled key particles. Further, DeepLag presents a new paradigm for fluid prediction, where the Lagrangian movement of the tracked particles is inferred from Eulerian observations, and their accumulated Lagrangian dynamics information is incorporated into global Eulerian evolving features to guide future prediction respectively. Tracking key particles not only provides a transparent and interpretable clue for fluid dynamics but also makes our model free from modeling complex correlations among massive grids for better efficiency. Experimentally, DeepLag excels in three challenging fluid prediction tasks covering 2D and 3D, simulated and real-world fluids. Code is available at this repository: https://github.com/thuml/DeepLag. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Fluids, characterized by a molecular structure that offers no resistance to external shear forces, easily deform even under minimal stress, leading to highly complex and often chaotic dynamics [10]. Consequently, the solvability of fundamental theorems in fluid mechanics, such as Navier-Stokes equations, is constrained to only a limited subset of flows due to their inherent complexity and intricate multiphysics interactions [38]. In practical applications, computational fluid dynamics (CFD) is widely employed to predict fluid behavior through numerical simulations, but it is hindered by significant computational costs. Accurately forecasting future fluid dynamics remains a formidable challenge. Recently, deep learning models [8, 28, 22] have shown great promise for fluid prediction due to their exceptional non-linear modeling capabilities. These models, trained on CFD simulations or real-world data, can serve as efficient surrogate models, dramatically accelerating inference. ", "page_idx": 0}, {"type": "text", "text": "A booming direction for deep fluid prediction is learning deep models to solve partial differential equations (PDEs) [45]. However, most of these methods [48, 29, 22, 21] attempt to capture fluid dynamics from the Eulerian perspective, which means modeling spatiotemporal correlations among massive grids unchanging over time. From this perspective, the complicated moving dynamics in fluids could be seriously obscured and confounded in static grids, bringing challenges in both computational efficiency and learning difficulties for accurately predicting future fluids. ", "page_idx": 0}, {"type": "text", "text": "In parallel to the Eulerian method, we notice another major approach for elucidating fluid dynamics, the Lagrangian method [13], also known as the particle tracking method. This method primarily ", "page_idx": 0}, {"type": "image", "img_path": "scw6Et4pEr/tmp/80a8d98a96be0047ef0df9aa84f25617bea22c9bfcb69e58209f719c9c155a6c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: Comparison between Lagrangian (left) and Eulerian (right) perspectives. The left depicts the learned trajectories of Lagrangian particles overlaid on the mean state, while the right displays the positions of tracked particles in successive Eulerian frames. Fluid motion is more visibly represented through the dynamic Lagrangian view compared to the density variations in static Eulerian grids. ", "page_idx": 1}, {"type": "text", "text": "focuses on tracing individual fluid particles by modeling the temporal evolution of their position and velocity. Unlike the Eulerian methods, which observe fluid flow at fixed spatial locations, the Lagrangian approach describes the fluid dynamics through the moving trajectory of individual fluid particles, offering a more natural and neat representation of fluid dynamics with inherent advantages in capturing intricate flow dynamics. Moreover, it allows a larger inference time step tha Eulerian methods while complying with the Courant\u2013Friedrichs\u2013Lewy condition [6] that guarantees stability. In Figure 1, we can find that fluid dynamics is much more visually apparent in Lagrangian trajectories on the left compared to the density changes observed on static Eulerian grids on the right. ", "page_idx": 1}, {"type": "text", "text": "Building on the two perspectives mentioned earlier, we propose DeepLag as a Eulerian-Lagrangian Recurrent Network. Our aim is to integrate Lagrangian tracking into the deep model, thereby enhancing the dynamics modeling in Eulerian fluid prediction. To achieve this, we present the EuLag Block, a powerful module that accomplishes Lagrangian tracking and Eulerian predicting at various scales. By leveraging the cross-attention mechanism, the EuLag Block assimilates tracked Lagrangian particle dynamics into the Eulerian field, guiding fluid prediction. It also forecasts the trajectory and dynamics of Lagrangian particles with the aid of Eulerian features. This unique Eulerian-Lagrangian design harnesses the dynamics information captured by Lagrangian trajectories and the fluid-structure features learned in the Eulerian grid. In our experiments, DeepLag consistently outperforms existing models, demonstrating state-of-the-art performance across three representative datasets, covering 2D and 3D fluid at various scales. Our contributions are as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 Going beyond learning fluid dynamics at static grids, we propose DeepLag featuring the Eulerian-Lagrangian Recurrent Network, which integrates both Eulerian and Lagrangian frameworks from fluid dynamics within a pure deep learning framework concisely. \u2022 Inspired by Lagrangian mechanics, we present EuLag Block, which can accurately track particle movements and interactively utilize the Eulerian features and dynamic information in fluid prediction, enabling a better dynamics modeling paradigm. \u2022 DeepLag achieves consistent state-of-the art on three representative fluid prediction datasets with superior trade-offs for performance and efficiency, exhibiting favorable practicability. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Eulerian and Lagrangian Methods ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Eulerian and Lagrangian descriptions are two fundamental perspectives for modeling fluid motion. The Eulerian view, commonly used in practical applications [23], observes fluid at fixed points and records physical quantities, such as density, as a function of position and time, $\\mathbf{v}=\\mathbf{v}(\\mathbf{s},t)$ . Thus, future fluid can be predicted by integrating velocity along the temporal dimension and interpolating the results to observed grid points [49]. In contrast, the Lagrangian view focuses on the trajectory of individual particles, tracking a particular particle from initial position $\\mathbf{s}_{\\mathrm{0}}$ by its displacement $\\mathbf{d}=\\mathbf{d}(\\mathbf{s}_{0},t)$ at time $t$ . This approach reveals the intricate evolution of the fluid by following particle trajectories, making it convenient for describing complex phenomena like vortices, turbulence, and interface motions [40]. Two perspectives are constitutionally equivalent, as bridged by the velocity: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathbf{v}\\left(\\mathbf{d}(\\mathbf{s}_{0},t),t\\right)=\\frac{\\partial\\mathbf{d}}{\\partial t}\\left(\\mathbf{s}_{0},t\\right).\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Furthermore, the material derivative $\\frac{\\mathrm{D}\\mathbf{q}}{\\mathrm{D}t}$ that describes the change rate of a physical quantity q of a fluid parcel can be written as the sum of the two terms reflecting the spatial and temporal influence on $\\mathbf{q}$ [1], which represent the derivatives on Eulerian domain and Lagrangian convective respectively: ", "page_idx": 2}, {"type": "equation", "text": "$$\n{\\frac{\\mathrm{D}\\mathbf{q}}{\\mathrm{D}t}}\\equiv\\underbrace{\\frac{\\partial\\mathbf{q}}{\\partial t}}_{\\mathrm{Domain\\,derivative}}+\\underbrace{\\mathbf{u}\\cdot\\nabla\\mathbf{q}}_{\\mathrm{Convective\\,derivative}}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "This connection inspires us to incorporate Lagrangian descriptions into dynamics learning with Eulerian data, enabling a more straightforward decomposition of complex spatiotemporal dependencies. ", "page_idx": 2}, {"type": "text", "text": "While traditional particle-based (or mixed-representation) solvers demonstrate superior accuracy and adaptability in inferring small-scale phenomena and dealing with nonlinear and irregular boundary conditions, they require computing the acceleration of each particle through physical equations, followed by sequential updates of their velocity and position [31]. This pointwise modeling approach often demands a significant number of points to fully characterize the dynamics of the entire field to meet accuracy requirements. Moreover, the irregular arrangement of particles and difficulty in parallelization result in higher computational costs and challenges with particle interpolation and gridding [12]. This renders particle-based solvers suboptimal compared to Eulerian solvers, particularly in high-dimensional spaces and large-scale simulations. However, the proposed DeepLag leverages the strengths of both solvers, eschewing equations and instead utilizing Eulerian information to assist particle tracking directly. This greatly alleviates the pressure on Lagrangian representation, requiring significantly fewer representative particles to aggregate the dynamics of the entire field. ", "page_idx": 2}, {"type": "text", "text": "2.2 Neural Fluid Prediction ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "As computational fluid dynamics (CFD) methods often require hours or even days for simulations [41], deep models have been explored as efficient surrogate models that can provide near-instantaneous predictions. These neural fluid prediction models approximate the solutions of governing fluid equations differently and can be categorized into three mainstream paradigms as in Figure 2(a-c). ", "page_idx": 2}, {"type": "text", "text": "Classical ML methods As depicted in Figure 2(a), these methods either replace part of a numerical method with a neural surrogate [39] or encode multivariate fields into a single-variable latent state ${\\bf z}$ [4, 26, 55], on which they model an ODE governing the state function $\\mathbf{z}_{t}:\\breve{\\mathcal{T}}\\to\\mathbb{R}^{d}$ , representing the first-order time derivative, through neural networks. However, the absence of physical meaning and guidance for latent states in evolution leads to error accumulation and a generally short forecasting horizon. A detailed comparison between DeepLag and these models is provided in Appendix B. ", "page_idx": 2}, {"type": "text", "text": "Physics-Informed Neural Networks (PINNs) This branch of methods in Figure 2(b) adopts deep models to learn the mapping from coordinates to solutions and formalize PDE constraints along with initial and boundary conditions as the loss functions [48, 29, 46, 47]. Though this paradigm can explicitly approximate the PDE solution, they usually require exact formalization for coefficients and conditions, limiting their generality and applicability to real-world fluids that are usually partially observed [35]. Plus, the Eulerian input disables them from handling Lagrangian descriptions. ", "page_idx": 2}, {"type": "text", "text": "Neural Operators Recently, a new paradigm, illustrated in Figure 2(c), has emerged where deep models learn the neural operators between input and target functions, e.g., past observations to future fluid predictions. Since DeepONet [22], various neural operators have significantly advanced fluid prediction, which directly approximates mappings between equation parameters and solutions. For Eulerian grid data, models based on U-Net [32] and ResNet [15] architectures have been proposed [30, 27, 17], as well as variants addressing issues like generalizing to unseen domains [44], irregular mesh [11], and uncertainty quantification [51]. Transformer-based models [43] enhance modeling capabilities and efficiency by exploiting techniques like Galerkin attention [3], incorporating ensemble information from the grid [14], applying low-rank decomposition to the attention mechanism [20], and leveraging spectral methods in the latent space [52]. Additionally, FNO [21] learns mappings in the frequency domain, and MP-PDE [2] utilizes the message-passing mechanism. For Lagrangian fluid particle data, some CNN-based methods [34, 42] model particle interactions through redesigned basic modules, while GNN-based methods [33, 25] update particle positions using the EncodeProcess-Decode paradigm. Despite the progress made by these methods, they are limited to one description and do not seamlessly combine Eulerian and Lagrangian views. ", "page_idx": 2}, {"type": "text", "text": "3 DeepLag ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Following the convention of neural fluid prediction [21], we formalize the fluid prediction problem as learning future fluid given past observation, as shown in Figure 2(d). Given a bounded open subset of $d$ -dimensional Euclidean space $\\mathcal{D}\\subset\\mathbb{R}^{d}$ and a Eulerian space $\\mathcal{U}\\subset\\mathbb{R}^{o}$ with $o$ observed physical quantities, letting $\\mathbf{u}_{t}(\\mathbf{x})\\subset\\mathcal{U}$ and $\\mathbf{u}_{t+1}(\\mathbf{x})\\subset\\mathcal{U}$ represent the Eulerian fluid field observation at two consecutive time steps on a finite coordinate set $\\textbf{x}\\subset\\mathbf{\\mathcal{D}}$ , we aim at fitting the mapping $\\Phi:\\mathbf{u}_{t}(\\mathbf{x})\\rightarrow\\mathbf{u}_{t+1}(\\mathbf{x})$ . Concretely, provided initial $P$ step observations $U_{P}=\\{\\mathbf{u}_{1},\\dots,\\mathbf{u}_{P}\\}$ , the fluid prediction process can be written as the following autoregressive paradigm: ", "page_idx": 3}, {"type": "equation", "text": "$$\nU_{t}=\\{\\mathbf{u}_{t-P+1},\\ldots,\\mathbf{u}_{t}\\}\\xrightarrow{\\mathcal{F}_{\\theta}}\\mathbf{u}_{t+1},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $t\\geq P$ and ${\\mathcal{F}}_{\\theta}$ represents the learned mapping between $U_{t}$ and the predicted field $\\mathbf{u}_{t+1}$ . ", "page_idx": 3}, {"type": "text", "text": "Inspired by the material derivative in Eq. (2), we present DeepLag as a Eulerian-Lagrangian Recurrent Network, which utilizes the EuLag Block to learn Eulerian features and Lagrangian dynamics interactively at various scales to address the complex spatiotemporal correlations in fluid prediction. Specifically, we capture the temporal evolving features at fixed points from the Eulerian perspective and the spatial dynamic information of essential particles from the Lagrangian perspective through their movement. By integrating Lagrangian pivotal dynamic information into Eulerian features, we fully model the spatiotemporal evolution of the fluid field over time and motion. Moreover, DeepLag can obtain critical trajectories within fluid dynamics with high computational efficiency by incorporating high-dimensional Eulerian space into lower-dimensional Lagrangian space. ", "page_idx": 3}, {"type": "image", "img_path": "scw6Et4pEr/tmp/b8702e2d052b380b235ae71111e75b6e8f9d9bfc8713aea34eb934dba944fe92.jpg", "img_caption": ["Figure 2: Three types of neural fluid prediction models (a-c) and overview of DeepLag (d). The EuLag Block accumulates the previous dynamics at each time and scale to guide the Eulerian field update and then evolves the particle movement and dynamics conditioned on the updated field. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "3.1 Overall Framework ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "It is widely acknowledged that fluids exhibit different motion characteristics across varying scales [3, 52]. In order to capture the intrinsic dynamics information at different scales, we track the trajectories of the key particles on $L$ scales separately and propose a Eulerian-Lagrangian Recurrent Network to realize the interaction between Eulerian and Lagrangian information. For clarity, we omit the scale index $l$ for primary physical quantities in this subsection, where $l\\in\\{1,2,\\cdots,\\dot{L}\\}$ . ", "page_idx": 3}, {"type": "text", "text": "Initializing Lagrangian particles To better capture complex dynamics in the fluid field, we sample by importance of the points from Eulerian perspective observations to determine initial positions for Lagrangian tracking. This is done by our dynamics sampling module. For the first predicting step $t=P$ , given observations $\\mathbf{u}_{t}=\\{\\dot{\\mathbf{u_{t}}}(\\mathbf{x}_{k})|\\dot{\\mathbf{x}_{k}}\\in\\mathcal{D}_{l},1\\le\\dot{k}\\le N_{l}\\}\\in\\mathbb{R}^{N_{l}\\times o}$ on all $N_{l}$ points in observation domain $\\mathcal{D}_{l}\\subset\\mathbb{R}^{d}$ of each scale, we extract their spatial dynamics by a convolutional network. We then calculate the probability matrix $\\mathbf{S}\\in\\mathbb{R}^{N_{l}}$ via softmax along spatial dimension: ", "page_idx": 3}, {"type": "equation", "text": "$$\n{\\bf S}=\\mathrm{Softmax}\\left(\\mathrm{ConvNet}({\\bf u}_{t})\\right),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathrm{ConvNet}(\\cdot)$ consists of a convolutional layer and a linear layer, with activation function in-between. We then sample $M_{l}$ Lagrangian tracking particles by the probability matrix at each scale: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{p}_{t}=\\mathrm{Sample}(\\{\\mathbf{x}_{k}\\}_{k=1}^{N_{l}},\\mathbf{S}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathbf{p}_{t}=\\{\\mathbf{p}_{t,i}^{l}\\in\\mathcal{D}_{l}\\}_{i=1}^{M_{l}}\\in\\mathbb{R}^{M_{l}\\times d}$ represents the set of sampled $M_{l}$ particles. ", "page_idx": 4}, {"type": "text", "text": "Eulerian fluid prediction with Lagrangian dynamics For the $t$ -th timestep, we adopt a learnable embedding layer with multiscale downsampling $\\operatorname{Down}(\\cdot)$ to encode past observations or predictions $U_{t}=\\{\\mathbf{u}_{t-P+1},\\ldots,\\mathbf{u}_{t}\\}$ to obtain the Eulerian representations $\\mathbf{u}_{t}^{l}\\in\\mathbb{R}^{N_{l}\\times C_{l}}$ at each scale. ", "page_idx": 4}, {"type": "text", "text": "We integrate the EuLag Block to fuse the Lagrangian dynamics at each scale and direct the evolution of Eulerian features. This interaction simultaneously enables Eulerian features to guide the progression of Lagrangian dynamics. For the $l$ -th scale, we track $M_{l}$ key particles over time, with $\\bar{\\mathbf{p}_{t}^{\\prime}}=\\bar{\\{\\mathbf{p}_{t,i}^{l}\\in}}$ $\\mathcal{D}_{l}\\}_{i=1}^{M_{l}}$ representing their positions and $\\mathbf{h}_{t}\\;=\\;\\{\\mathbf{h}_{t,i}^{l}\\;\\in\\;\\mathbb{R}^{C_{l}}\\}_{i=1}^{M_{l}}$ denoting their learned particle dynamics. As shown in Figure 2(d), the positions and dynamics of the $M_{l}$ particles at the $l_{\\cdot}$ -th scale are learned autoregressively using the EuLag Block, which can be written as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{u}_{t+1},\\{\\mathbf{p}_{t+1}\\},\\{\\mathbf{h}_{t+1}\\}=\\mathrm{EuLag}(\\mathbf{u}_{t},\\{\\mathbf{p}_{t}\\},\\{\\mathbf{h}_{t}\\}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the scale index $l$ is omitted for notation simplicity. The EuLag Block learns to optimally leverage the complementary strengths of Eulerian and Lagrangian representations, facilitating mutual refinement between these two perspectives towards an exceeding performance. More details about the specific implementation of the EuLag Block are elaborated in the following subsection 3.2. ", "page_idx": 4}, {"type": "text", "text": "After evolving into new Eulerian features $\\mathbf{u}_{t+1}$ , Lagrangian particle position $\\mathbf{p}_{t+1}$ and dynamics $\\mathbf{h}_{t+1}$ at the $l$ -th scale, we further aggregate $\\mathbf{u}_{t+1}$ with the predicted Eulerian field at a coarser scale by upsampling $\\mathrm{{Up}(\\cdot)}$ . Eventually, the full-resolution prediction $\\mathbf{u}_{t+1}$ at step $t+1$ is decoded from $\\overline{{\\mathbf{u}_{t+1}^{1}}}$ with a projection layer. We unfold the implementation of the overall architecture in Appendix A.2. ", "page_idx": 4}, {"type": "image", "img_path": "scw6Et4pEr/tmp/5c9cae2f2e4bb5a1a1d5fc0b6a0a25c0efa4b325a2609d12f35bd7b21d1dedbc.jpg", "img_caption": ["Figure 3: Overview of the EuLag Block, which accumulates previous dynamics information to guide Eulerian evolution for predicting particle movement. Scale index $l$ is omitted for simplicity. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "3.2 EuLag Block ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "As stated in Eq. (6), we adopt a recurrent network to interactively exploit the information from two fluid descriptions, which consist of two main components: Lagrangian-guided feature evolving and Eulerian-conditioned particle tracking. The following quantities are all in the $l$ -th scale. ", "page_idx": 4}, {"type": "text", "text": "Lagrangian-guided feature evolving Classical theories [9] and numerical algorithms [31] show that fluid predictions can be solved by identifying the origin of the fluid parcel and interpolating the dependent variable from nearby grid points. However, without specifying certain PDEs, we cannot explicitly determine the former position of the particle on each Eulerian observed point. Thus, we first adaptively synthesize the Lagrangian dynamics of the tracked particles to guide the evolution of Eulerian features using a cross-attention mechanism. Formally, we adopt a Lagrangian-to-Eulerian cross-attention, where the Eulerian field $\\mathbf{u}_{t}$ serves as queries, and Lagrangian dynamics concatenated with particle position $\\mathbf{h}_{t}||\\mathbf{p}_{t}\\in\\mathbb{R}^{M_{l}\\times(C_{l}+d)}$ is used as keys and values: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{LagToEu-Attn}(\\mathbf{u}_{t},\\mathbf{h}_{t}||\\mathbf{p}_{t},\\mathbf{h}_{t}||\\mathbf{p}_{t})=\\mathrm{Softmax}\\left(\\frac{\\mathbf{W}_{Q}\\mathbf{u}_{t}(\\mathbf{W}_{K}\\cdot\\mathbf{h}_{t}||\\mathbf{p}_{t})^{\\top}}{\\sqrt{C_{l}+d}}\\right)\\mathbf{W}_{V}\\cdot\\mathbf{h}_{t}||\\mathbf{p}_{t},\\mathbf{h}_{t}||\\mathbf{p}_{t},\\mathbf{h}_{t}||\\mathbf{p}_{t},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathbf{W}_{Q}$ , $\\mathbf{W}_{K}$ and $\\mathbf{W}_{V}$ stand for linear projections. We wrap the attention in a Transformer block with residual connections to implement the Lagrangian-dynamics-guided Eulerian evolution process: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{u}_{t+1}=\\mathbf{u}_{t}+\\mathrm{LagToEu}(\\mathbf{u}_{t},\\mathbf{h}_{t}||\\mathbf{p}_{t},\\mathbf{h}_{t}||\\mathbf{p}_{t}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Eulerian-conditioned particle tracking Traditional Lagrangian methods rely on interactions among vast quantities of particles to estimate the future fluid fields. However, the high computational cost hinders the application in deep surrogate models. In other data-driven approaches, the sparse sampling of particles is computational-friendly but cannot directly derive Lagrangian dynamics for other particles. Considering the equivalence of the Eulerian and Lagrangian representations indicated by the material derivative in Eq. (1), we propose to learn particle movements based on the Eulerian conditions. Concretely, we utilize another Eulerian-to-Lagrangian cross-attention, where the evolved dense Eulerian features are used to navigate the Lagrangian dynamics of sparse particles: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathrm{EuToLag-Attn}(\\mathbf{h}_{t}||\\mathbf{p}_{t},\\mathbf{u}_{t+1},\\mathbf{u}_{t+1})=\\mathrm{Softmax}\\left(\\frac{(\\mathbf{W}_{Q}^{\\prime}\\cdot\\mathbf{h}_{t}||\\mathbf{p}_{t})(\\mathbf{W}_{K}^{\\prime}\\mathbf{u}_{t+1})^{\\top}}{\\sqrt{C_{l}}}\\right)\\mathbf{W}_{V}^{\\prime}\\mathbf{u}_{t+1},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where we use a different set of $\\mathbf{W}_{Q}^{\\prime},\\mathbf{W}_{K}^{\\prime}$ , and $\\mathbf{W}_{V}^{\\prime}$ . Similarly, the Transformer block EuToLag wrapping this attention produces the change of forecasted global Lagrangian dynamics $\\delta\\mathbf{h}_{\\mathrm{global},t}$ and movement of tracking particles $\\delta\\mathbf{p}_{t}$ , which leads to the next step by residual connections: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{h}_{\\mathrm{global},t+1}||\\mathbf{p}_{t+1}=\\mathbf{h}_{t}||\\mathbf{p}_{t}+\\mathrm{EuToLag}(\\mathbf{h}_{t}||\\mathbf{p}_{t},\\mathbf{u}_{t+1},\\mathbf{u}_{t+1}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "To better model the dynamic evolution of the particles, we gather local Lagrangian dynamic by employing bilinear interpolation to evolved Eulerian features $\\mathbf{u}_{t+1}$ on new particle position $\\mathbf{p}_{t+1}$ , then use a linear function to aggregate it with global dynamics information $\\mathbf{h}_{\\mathrm{global},t+1}$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{h}_{t+1}=\\mathrm{Aggregate}(\\mathrm{Interpolate}(\\mathbf{u}_{t+1},\\mathbf{p}_{t+1}),\\mathbf{h}_{\\mathrm{global},t+1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Additionally, particles could move out of the observation domain as the input field may have an open boundary. We check the updated position of tracking particles and resample from the latest probability matrix $\\mathbf{S}$ to substitute the ones that exit, ensuring the validity of the Lagrangian information. ", "page_idx": 5}, {"type": "text", "text": "Overall, the EuLag Block can fully utilize the complementary advantages of Eulerian and Lagrangian perspectives in describing fluid dynamics, thereby being better suited for fluid prediction. For more implementation details of the EuLag Block, please refer to Appendix A.3. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We evaluated DeepLag on three challenging benchmarks, including simulated and real-world scenarios, covering both 2D and 3D, as well as single and multi-physics fluids. Following the previous convention [21], we train DeepLag and the baselines for each task to predict ten future timesteps in an autoregressive fashion given ten past observations. Detailed benchmark information is listed in Table 1. We provide an elaborate analysis of the efficiency, parameter count, and performance difference in section 4.4 and Appendix E. Additionally, more detailed visualizations, besides in later this section, are provided in Appendix H. Furthermore, the trained models are engaged to perform 100 frames extrapolation to examine their long-term stability, whose results are in Appendix I. ", "page_idx": 5}, {"type": "text", "text": "Baselines To demonstrate the effectiveness of our model, we compare DeepLag with seven baselines on all benchmarks, including the classical multiscale model U-Net [32] and advanced neural operators for Navier-Stokes equations: FNO [21], Galerkin Transformer [3], Vortex for 2D image [7], GNOT [14], LSM [52] and FactFormer [20]. U-Net has been widely used in fluid modeling, which can model the multiscale ", "page_idx": 5}, {"type": "table", "img_path": "scw6Et4pEr/tmp/df34d1c61608f5d98d47ef93e1b749951db273cec2aa32efc4725137be9ef120.jpg", "table_caption": ["Table 1: Summary of the benchmarks. #Var refers to the number of observed physics quantities in fluid. #Space is the spatial resolution. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "property precisely. LSM [52] and FactFormer [20] are previous state-of-the-art neural operators. ", "page_idx": 5}, {"type": "text", "text": "Metrics For all three tasks, we follow the convention in neural fluid prediction [21, 52] and report relative L2 as the main metric. Implementations of the metrics are included in Appendix A.4. ", "page_idx": 5}, {"type": "text", "text": "Implementations Aligned with convention and the baselines, DeepLag is trained with relative L2 as the loss function on all benchmarks. We use the Adam [18] optimizer with an initial learning rate of $5\\times10^{-4}$ and StepLR learning rate scheduler. The batch size is set to 5, and the training process is stopped after 100 epochs. All experiments are implemented in PyTorch [24] and conducted on a single NVIDIA A100 GPU. Training curves are shown in Appendix D. ", "page_idx": 5}, {"type": "image", "img_path": "scw6Et4pEr/tmp/a89e7a2ce8cd0125f06301b0e8c404f6ac1dd33dbf845b9fdebe3a0a06ebdbe7.jpg", "img_caption": ["Figure 4: Showcases (left) and timewise relative L2 (right) on Bounded Navier-Stokes dataset. Both predictions (upper row) and absolute error maps (lower row) are plotted for intuitive comparison. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "4.1 Bounded Navier-Stokes ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Setups In real-world applications, handling complex boundary conditions in predicting fluid dynamics is indispensable. Thus, we experiment with the newly generated Bounded Navier-Stokes, which simulates a scenario where some colored dye flows from left to right through a 2D pipe with several fixed pillars as obstacles inside. Details about this benchmark can be found in Appendix C.1. ", "page_idx": 6}, {"type": "text", "text": "Quantitive results As shown in Table 2, DeepLag achieves the best performance on Bounded Navier-Stokes, demonstrating its advanced ability to handle complex boundary conditions. In comparison to the previous best model, DeepLag achieves a significant $13.8\\%$ (Relative L2: 0.0618 v.s. 0.0544) and $2.7\\%$ (Relative L2: 0.1020 v.s. 0.0993) relative promotion on short and long rollout. The timewise error curves of all the models are also included in Figure 4. We can find that DeepLag presents slower error growth and excels in long-term forecasting. This result may stem from the Lagrangian-guided fluid prediction, which can accurately capture the dynamics information over time, further verifying the effectiveness of our design. ", "page_idx": 6}, {"type": "table", "img_path": "scw6Et4pEr/tmp/75260bc15e4fcc004aedb57fe5f887c675045caa78ed6eec0713559fdafdff38.jpg", "table_caption": ["Table 2: Performance comparison on Bounded NavierStokes. Relative L2 of 10 frames and 30 frames prediction are recorded. Promotion represents the relative promotion of DeepLag w.r.t the second-best (underlined).\u201cNaN\u201d refers to the instability during rollout. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Showcases To intuitively present the forecasting skills of different models, we also provide showcase comparisons in Figure 4 and particle movements predicted by DeepLag in Appendix K. We can find that DeepLag can precisely illustrate the vortex in the center of the figure and give a reasonable motion mode of the K\u00e1rm\u00e1n vortex phenomenon formed behind the upper left pillar. As for U-Net and LSM, although they successfully predicted the position of the center vortex, the error map shows that they failed to predict the density field as accurately as DeepLag. In addition, FactFormer deteriorates on this benchmark. This may be because it is based on spatial factorization, which is unsuitable for irregularly placed boundary conditions. These results further highlight the beneftis of Eulerian-Lagrangian co-design, which can simultaneously help with dynamic and density prediction. ", "page_idx": 6}, {"type": "text", "text": "4.2 Ocean Current ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Setups Predicting large-scale ocean currents, especially in regions near tectonic plate boundaries prone to disasters such as tsunamis due to intense terrestrial activities, plays a crucial role in various domains. Hence, we also explore this challenging real-world scenario in our experiments. More details about the source and settings of this benchmark can be found in Appendix C.2. ", "page_idx": 6}, {"type": "image", "img_path": "scw6Et4pEr/tmp/68c65746117310c955c2fa31ce9779702e6e42d1189a59b38d8219f4abf251b9.jpg", "img_caption": ["Figure 5: Showcase comparison and visualization of Lagrangian trajectories learned by DeepLag on Ocean Current. Notably, potential temperatures predicted by different models are plotted. Error maps of predictions are normalized to $(-4,4)$ for a better view. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Quantitive results We report relative L2 for the Ocean Current dataset in Table 3, where DeepLag still achieves the best with $8.3\\%$ relative promotion w.r.t. the second-best model. Even in 30 days of forecasting, the number is $12.8\\%$ . These results show that DeepLag performs well in real-world, large-scale fluids, which usually involve more inherent stochasticity than simulated data. Moreover, we provide the ACC metric and timewise curve in Appendix F. ", "page_idx": 7}, {"type": "text", "text": "Showcases To provide an intuitive comparison, we plotted different model predictions in Figure 5. In comparison to other models, DeepLag exhibits the most minor prediction error. It accurately predicts the location of the high-temperature region to the south area and provides a clear depiction of the Kuroshio pattern [37] bounded by the red box. ", "page_idx": 7}, {"type": "text", "text": "Learned trajectory visualization To reveal the effect of learning Lagrangian trajectories, we visualize tracked particles in Figure 5. We observe that all the particles move from west to east, consistent with the Pacific circulation. Additionally, the tracked particles show distinct moving patterns, confirming their ability to represent complex dynamics. The move", "page_idx": 7}, {"type": "table", "img_path": "scw6Et4pEr/tmp/58e79f9028c197feb154e73e72960b8f07208e1f92b333077240bb7758993713.jpg", "table_caption": ["Table 3: Performance comparison on Ocean Current. We report the relative L2 of the short-term and longterm rollouts with their relative promotions. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "ment of upper particles matches the sinuous trajectory of the Kuroshio current, demonstrating the capability of DeepLag to provide interpretable evidence for prediction results. Visualizing the tracking points in Lagrangian space instills confidence in the reliability and interpretability of the predictions made by our model, which can provide valuable and intuitive insights for real-world fluid dynamics. ", "page_idx": 7}, {"type": "text", "text": "4.3 3D Smoke ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Setups 3D fluid prediction has been a longstanding challenge due to the tanglesome dynamics involved. Therefore, we generated this benchmark to describe a scenario in which smoke flows under the influence of buoyancy in a threedimensional bounding box. For more details, please refer to Appendix C.3. ", "page_idx": 7}, {"type": "text", "text": "Quantitive results Table 4 shows that DeepLag still performs best in 3D fluid. Note that in this benchmark, the canonical deep model UNet degenerates seriously, indicating that a pure Eulerian multiscale framework is insufficient to model complex dynamics. We also noticed that the Transformer-based neural operators, such as ", "page_idx": 7}, {"type": "table", "img_path": "scw6Et4pEr/tmp/7af369b5f441fc68e55b0ff1a51619a03da72161ff967e60728189c971da7356.jpg", "table_caption": ["Table 4: Performance comparison on the 3D Smoke dataset. Relative L2 with relative promotion w.r.t. the second-best model is recorded. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "GNOT and Galerkin Transformer, failed in this task. This may be because both of them are based on the linear attention mechanism [19, 54], which may depreciate under massive tokens. These comparisons further highlight the capability of DeepLag to handle high-dimensional fluid. ", "page_idx": 7}, {"type": "image", "img_path": "scw6Et4pEr/tmp/4a2858d7beb5f67344938cf6d3cdaeb0a226605541c2ea22da56f5e15a51692e.jpg", "img_caption": ["Figure 6: Showcases comparison of the whole space (left part) and a cross-section (right part) on the 3D Smoke dataset. For better visualization, we present the absolute value of the prediction error and normalize the whole space error into $(0,0.12)$ . As for the cross-section visualization, we choose the $x O y$ plane in the middle 3D fluid and normalize error maps to $(-0.5,0.5)$ . "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Showcases In Figure 6, we compared the prediction results on the 3D Smoke dataset. DeepLag demonstrates superior performance in capturing both the convection and diffusion of smoke within the bounding box. In contrast, the predictions made by U-Net tend to average value across various surfaces, resulting in blurred details, which also indicates its deficiency in dynamics modeling. Similarly, LSM and FactFormer exhibit more pronounced errors, particularly around the smoke boundaries, where complex wave interactions often occur. By comparison, our model significantly reduces both the overall and cross-sectional error, excelling in the prediction of fine-grained, subtle flow patterns and maintaining high accuracy even in challenging regions with intricate dynamics. ", "page_idx": 8}, {"type": "text", "text": "4.4 Model Analysis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Ablations To verify the effectiveness of detailed designs in DeepLag, we conduct exhaustive ablations in Table 5. In our original design, we track 512 particles (around $3\\%$ of Eulerian grids) in total on 4 hierarchical spatial scales with a latent dimension 64 in the Lagrangian space. ", "page_idx": 8}, {"type": "text", "text": "The experiments indicate that further increasing the number of particles, scales, or latent dimensions yields marginal performance improvements. Therefore, we opt for these values to strike a balance between efficiency and performance. In addition, we can conclude that all components proposed in this paper are indispensable. Especially, the lack of interaction between the Eulerian and Lagrangian space will cause a severe drop in accuracy, highlighting the dual cross-attention that exploits Lagrangian dynamics has a positive influence on the evolution of Eulerian features. Besides, rather than uniformly sampling particles, sampling from a learnable probability matrix also provides an upgrade (refer to Appendix G for visual results). When swapping the ", "page_idx": 8}, {"type": "image", "img_path": "scw6Et4pEr/tmp/a1e32e1a892ce166600f91e870024932ba6393a87f75e07360cbb1694902e26b.jpg", "img_caption": ["Figure 7: Efficiency comparison among all the models. Running time and Relative L2 are evaluated on the 3D Smoke benchmark. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 5: Ablations of (a) module removing and (b) hyperparameter sensitivity on Bounded NavierStokes, where (a) includes w/o LagToEu: removing Lagrangian-guided feature evolving, $w/o$ EuToLag: removing Eulerian-conditioned particle tracking and w/o Learnable Sampling: removing learnable particle sampling strategy, and (b) includes #Particle $(M_{1})$ : adjusting the number of tracking particles of the first layer, #Scale $(L)$ : adjusting the number of spatial scales and #Latent $(C_{1})$ : adjusting the number of latent dimension of the first layer. We use the control variable method, when one hyperparameter is changed, the values of the others are all original $(o r i)$ . Ablations of (c) attention swapping on Bounded Navier-Stokes $(2D)$ and 3D Smoke $(3D)$ , where the order of EuToLag and LagToEu blocks is swapped. Original and Swapped Relative L2 are reported. ", "page_idx": 8}, {"type": "text", "text": "(c) Attention Swapping ", "page_idx": 8}, {"type": "table", "img_path": "scw6Et4pEr/tmp/9c534195a9d0fcd678d2df559880bc1d0037953f299c964a640be035b070ca87.jpg", "table_caption": ["(a) Module Removing ", "(b) Hyperparameter Sensitivity "], "table_footnote": [""], "page_idx": 8}, {"type": "text", "text": "Table 6: Experiments of (a) model efficiency alignment on 3D Smoke and (b) high-resolution data on Bounded Navier-Stokes. To compare with an aligned efficiency, we add more convolutional layers into the standard U-Net and increase the latent dimension. In the high-resolution simulation, we trained a new DeepLag model on a newly generated $256\\!\\times\\!256$ Bounded Navier-Stokes dataset. Model parameters (#Param), GPU memory (Mem) and running time per epoch (Time) are reported. ", "page_idx": 9}, {"type": "table", "img_path": "scw6Et4pEr/tmp/aab628ee216982bdfac8620e18da1afe2e77a656754553842cc2042ed3f5b663.jpg", "table_caption": ["(a) Model Efficiency Alignment ", "(b) High-resolution Data "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "positions of the EuToLag and LagToEu blocks, the minimal performance change in both 2D and 3D benchmarks suggests the equivalence of the two perspectives, and the flow of information between them is bidirectional and insensitive to the order, underscoring the robustness of our approach. The above results provide solid support to our motivation in tracking essential particles and utilizing Eulerian-Lagrangian recurrence, further confirming the merits of our model. ", "page_idx": 9}, {"type": "text", "text": "Efficiency analysis We also include the efficiency comparison in Figure 7. DeepLag hits a favorable balance between efficiency and performance by simultaneously considering performance, model parameters, and running time, demonstrating superior performance with significantly less memory than GNOT and LSM, thereby minimizing storage complexity. Standard U-Net has a large number of parameters, and Transformers have quadratic memory, so in large-scale or complex fluid prediction scenarios, using linear complexity attention mechanisms like ours is necessary. This explains why, although U-Net and LSM are good at Bounded Navier-Stokes, they will worsen when it comes to complex fluid dynamics, such as the Ocean Current and the 3D Smoke benchmark. ", "page_idx": 9}, {"type": "text", "text": "Comparison under aligned efficiency As aforementioned, U-Net also presents competitive performance and efficiency in some cases. To carry out a fair comparison between baselines and DeepLag, we examine them on 3D Smoke, which scales up U-Net to have a running time similar to DeepLag as in Table 6(a). The results show that too many parameters overwhelm small baselines at the lower left corner in Figure 7 like U-Net, indicating that they have a shortcoming in scalability. ", "page_idx": 9}, {"type": "text", "text": "Generalization analysis To verify the generalizing ability of DeepLag on larger and new domains, we ran tests on high-resolution (HR) data as in Table 6(b) and unseen boundary conditions (BC) as in Appendix J, respectively. The HR simulation on the $256\\times256$ Bounded Navier-Stokes shows that the finer the data are, the more accurate our DeepLag is, which still has a $17\\%$ promotion w.r.t U-Net on relative L2 (DeepLag: 0.051, U-Net: 0.060). The comparison of GPU memory and running time per epoch also confirms that the increase of the space complexity is sublinear and the time complexity is minor, underscoring the scalability of DeepLag. For BCs, we ran a zero-shot test with the old model checkpoint on a newly generated Bounded N-S dataset, which has obstacles of different numbers, positions, and sizes. DeepLag still has a $7\\%$ promotion w.r.t the best baseline, U-Net, on relative L2 (DeepLag: 0.203, U-Net: 0.217). The visual comparison between the two models further shows that DeepLag adaptively generalizes well on unknown and more complex domains. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusions and Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "To tackle intricate fluid dynamics, this paper presents DeepLag by introducing the Lagrangian dynamics into Eulerian fluid, which can provide clear and neat dynamics information for prediction. A EuLag Block is presented in the Eulerian-Lagrangian Recurrent Network to utilize the complementary advantages of Eulerian and Lagrangian perspectives, which brings better particle tracking and Eulerian fluid prediction. DeepLag excels in complex fluid prediction with an average improvement of nearly $10\\%$ across three carefully selected complex benchmarks, even in 3D fluid, and can also provide interpretable evidence by plotting learned Lagrangian trajectories. However, the number of tracking particles in DeepLag is appointed as a fixed hyperparameter that needs to be adjusted for specific scenarios, and missing Lagrangian supervision in the data leads to a lack of judgment on learned Lagrangian dynamics, which leave space for future exploration. That said, results show that the performance always tends to incline as we simply scale up. Therefore, the few hyperparameters for tuning depend more on the limitation of computing resources rather than blind searching. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was supported by the National Natural Science Foundation of China (U2342217 and 62022050), the BNRist Project, and the National Engineering Research Center for Big Data Software. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] G. K. Batchelor. An Introduction to Fluid Dynamics. Cambridge Mathematical Library. Cambridge University Press, 2000.   \n[2] Johannes Brandstetter, Daniel E. Worrall, and Max Welling. Message passing neural PDE solvers. In International Conference on Learning Representations, 2022.   \n[3] Shuhao Cao. Choose a transformer: Fourier or galerkin. In Conference on Neural Information Processing Systems, 2021.   \n[4] Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differential equations. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018.   \n[5] CMEMS and MDS. Global ocean physics reanalysis. DOI: 10.48670/moi-00021 (Accessed on 23 September 2023), 2023.   \n[6] R. Courant, K. Friedrichs, and H. Lewy. On the partial difference equations of mathematical physics. IBM Journal of Research and Development, 11(2):215\u2013234, 1967.   \n[7] Yitong Deng, Hong-Xing Yu, Jiajun Wu, and Bo Zhu. Learning vortex dynamics for fluid inference and prediction. In ICLR, 2023.   \n[8] Mahesh Dissanayake and Nhan Phan-Thien. Neural-network-based approximations for solving partial differential equations. Communications in Numerical Methods in Engineering, 1994.   \n[9] L.C. Evans. Partial Differential Equations. Graduate studies in mathematics. American Mathematical Society, 2010.   \n[10] Joel Ferziger, Milovan Peri\u00b4c, and Robert Street. Computational Methods for Fluid Dynamics. Springer Nature Switzerland, 2020.   \n[11] Han Gao, Luning Sun, and Jian-Xun Wang. Phygeonet: Physics-informed geometry-adaptive convolutional neural networks for solving parameterized steady-state pdes on irregular domain. Journal of Computational Physics, 428:110079, March 2021.   \n[12] Robert A. Gingold and Joseph John Monaghan. Smoothed particle hydrodynamics: Theory and application to non-spherical stars. Monthly Notices of the Royal Astronomical Society, 181:375\u2013389, 1977.   \n[13] K.C. Gupta. Classical Mechanics of Particles and Rigid Bodies. Wiley, 1988.   \n[14] Zhongkai Hao, Chengyang Ying, Zhengyi Wang, Hang Su, Yinpeng Dong, Songming Liu, Ze Cheng, Jun Zhu, and Jian Song. Gnot: A general neural operator transformer for operator learning. In International Conference on Machine Learning, 2023.   \n[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition, 2015.   \n[16] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv: Learning, 2016.   \n[17] Zichao Jiang, Junyang Jiang, Qinghe Yao, and Gengchao Yang. A neural network-based pde solving algorithm with high precision. Nature News, Mar 2023.   \n[18] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations, 2015.   \n[19] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In International Conference on Learning Representations, 2020.   \n[20] Zijie Li, Dule Shu, and Amir Barati Farimani. Scalable transformer for pde surrogate modeling. In Conference on Neural Information Processing Systems, 2023.   \n[21] Zongyi Li, Nikola Borislavov Kovachki, Kamyar Azizzadenesheli, Burigede liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial differential equations. In International Conference on Learning Representations, 2021.   \n[22] Lu Lu, Pengzhan Jin, Guofei Pang, Zhongqiang Zhang, and George Em Karniadakis. Learning nonlinear operators via deeponet based on the universal approximation theorem of operators. Nature Machine Intelligence, 2021.   \n[23] Faith A Morrison. An introduction to fluid mechanics. Cambridge University Press, 2013.   \n[24] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in Neural Information Processing Systems, 2019.   \n[25] Tobias Pfaff, Meire Fortunato, Alvaro Sanchez-Gonzalez, and Peter Battaglia. Learning mesh-based simulation with graph networks. In International Conference on Learning Representations, 2021.   \n[26] Gavin D. Portwood, Peetak P. Mitra, Mateus Dias Ribeiro, Tan Minh Nguyen, Balasubramanya T. Nadiga, Juan A. Saenz, Michael Chertkov, Animesh Garg, Anima Anandkumar, Andreas Dengel, Richard Baraniuk, and David P. Schmidt. Turbulence forecasting via neural ode, 2019.   \n[27] Md Ashiqur Rahman, Zachary E Ross, and Kamyar Azizzadenesheli. U-no: U-shaped neural operators. TMLR, 2023.   \n[28] Maziar Raissi, Paris Perdikaris, and George Em Karniadakis. Physics informed deep learning (part i): Data-driven solutions of nonlinear partial differential equations. arXiv preprint, 2017.   \n[29] Maziar Raissi, Paris Perdikaris, and George Em Karniadakis. Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of Computational Physics, 2019.   \n[30] Bogdan Raonic, Roberto Molinaro, Tim De Ryck, Tobias Rohner, Francesca Bartolucci, Rima Alaifari, Siddhartha Mishra, and Emmanuel de Bezenac. Convolutional neural operators for robust and accurate learning of PDEs. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[31] Andr\u00e9 Robert. A stable numerical integration scheme for the primitive meteorological equations. Atmosphere-Ocean, 19(1):35\u201346, 1981.   \n[32] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In The Medical Image Computing and Computer Assisted Intervention Society, 2015.   \n[33] Alvaro Sanchez-Gonzalez, Jonathan Godwin, Tobias Pfaff, Rex Ying, Jure Leskovec, and Peter W. Battaglia. Learning to simulate complex physics with graph networks. In International Conference on Machine Learning, 2020.   \n[34] Connor Schenck and Dieter Fox. Spnets: Differentiable fluid dynamics for deep neural networks, 2018.   \n[35] Fabio Sim, Eka Budiarto, and Rusman Rusyadi. Comparison and analysis of neural solver methods for differential equations in physical systems. ELKHA: Jurnal Teknik Elektro, 2021.   \n[36] takah29, houkensjtu, and zemora. 2d incompressible fluid solver implemented in taichi, 2023.   \n[37] TY Tang, JH Tai, and YJ Yang. The flow pattern north of taiwan and the migration of the kuroshio. Continental Shelf Research, 2000.   \n[38] Roger Temam. Navier-Stokes equations: theory and numerical analysis. American Mathematical Soc., 2001.   \n[39] Jonathan Tompson, Kristofer Schlachter, Pablo Sprechmann, and Ken Perlin. Accelerating eulerian fluid simulation with convolutional networks. In Proceedings of the 34th International Conference on Machine Learning - Volume 70, ICML\u201917, page 3424\u20133433. JMLR.org, 2017.   \n[40] Eleuterio Toro. Riemann Solvers and Numerical Methods for Fluid Dynamics: A Practical Introduction. 2009.   \n[41] Nobuyuki Umetani and Bernd Bickel. Learning three-dimensional flow for interactive aerodynamic design. ACM Transactions on Graphics (TOG), 2018.   \n[42] Benjamin Ummenhofer, Lukas Prantl, Nils Thuerey, and Vladlen Koltun. Lagrangian fluid simulation with continuous convolutions. In International Conference on Learning Representations, 2020.   \n[43] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017.   \n[44] Nils Wandel, Michael Weinmann, Michael Neidlin, and Reinhard Klein. Spline-pinn: Approaching pdes without data using fast, physics-informed hermite-spline cnns, 2022.   \n[45] Hanchen Wang, Tianfan Fu, Yuanqi Du, Wenhao Gao, Kexin Huang, Ziming Liu, Payal Chandak, Shengchao Liu, Peter Van Katwyk, Andreea Deac, et al. Scientific discovery in the age of artificial intelligence. Nature, 2023.   \n[46] Sifan Wang, Yujun Teng, and Paris Perdikaris. Understanding and mitigating gradient pathologies in physics-informed neural networks. SIAM Journal on Scientific Computing, 2020.   \n[47] Sifan Wang, Xinling Yu, and Paris Perdikaris. When and why pinns fail to train: A neural tangent kernel perspective. Journal of Computational PhysicsF, 2020.   \n[48] E Weinan and Ting Yu. The deep ritz method: A deep learning-based numerical algorithm for solving variational problems. Communications in Mathematics and Statistics, 2017.   \n[49] F.M. White. Fluid Mechanics. McGraw-Hill series in mechanical engineering. 2011.   \n[50] R Wille. Karman vortex streets. Advances in Applied Mechanics, 1960.   \n[51] Nick Winovich, Karthik Ramani, and Guang Lin. Convpde-uq: Convolutional neural networks with quantified uncertainty for heterogeneous elliptic partial differential equations on varied domains. J. Comput. Phys., 394(C):263\u2013279, oct 2019.   \n[52] Haixu Wu, Tengge Hu, Huakun Luo, Jianmin Wang, and Mingsheng Long. Solving high-dimensional pdes with latent spectral models. In International Conference on Machine Learning, 2023.   \n[53] Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tie-Yan Liu. On layer normalization in the transformer architecture, 2020.   \n[54] Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn M. Fung, Yin Li, and Vikas Singh. Nystr\u00f6mformer: A nystr\u00f6m-based algorithm for approximating self-attention. Proceedings of the AAAI Conference on Artificial Intelligence, 2021.   \n[55] Cagatay Yildiz, Markus Heinonen, and Harri Lahdesmaki. Ode2vae: Deep generative second order odes with bayesian neural networks. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Implementation Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "This section provides the implementation details of DeepLag, including the configurations of model hyperparameters and the concrete design of modules. ", "page_idx": 13}, {"type": "text", "text": "A.1 Hyperparameters ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Detailed model configurations of DeepLag are listed in Table 7. Zero-padding is only used in the Ocean Current dataset to ensure the exact division in downsampling. ", "page_idx": 13}, {"type": "table", "img_path": "scw6Et4pEr/tmp/dd5f3fd796ff72b433ee124ccc51c630dbb0393dc817553fee7cb0960def764d.jpg", "table_caption": ["Table 7: Model configurations for DeepLag. "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "A.2 Sampling and multiscale architecture in overall framework ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.2.1 Sampling to initialize Lagrangian particles ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "At the first predicting step, the position of Lagrangian particles to track is initialized by the dynamic sampling module consisting of dynamics extracting and sampling. ", "page_idx": 13}, {"type": "text", "text": "The $\\mathrm{ConvNet}(\\cdot)$ to extract dynamics Given the Eulerian input field $\\{\\mathbf{u}_{t}^{l}(\\mathbf{x})\\}_{\\mathbf{x}\\subset\\mathcal{D}_{l}}$ at the $l$ -th scale, the $\\mathrm{ConvNet}(\\cdot)$ operation in Eq. (4) is to extract the local dynamics around each Eulerian observation point with $\\operatorname{Conv}()$ , BatchNorm() and ReLU() layers, which can be formalized as follows: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{ConvNet}(\\mathbf{u}_{t})=\\mathrm{Conv}\\left(\\mathrm{ReLU}\\left(\\mathrm{BatchNorm}\\left(\\mathrm{Conv}(\\{\\mathbf{u}_{t}^{l}(\\mathbf{x})\\}_{\\mathbf{x}\\subset\\mathcal{D}_{l}})\\right)\\right)\\right),\\ l\\ \\mathrm{from}\\ 1\\ \\mathrm{to}\\ L.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Here, the output channel of the outermost Conv is 1. ", "page_idx": 13}, {"type": "text", "text": "The $\\operatorname{Sample}(\\cdot)$ to select key particles Given the probability distribution matrix $\\mathbf{S}_{t}^{l}(\\mathbf{x})_{\\mathbf{x}\\subset\\mathcal{D}_{l}}$ and the number of particles to sample $M_{l}$ at the $l$ -th scale, we choose the particles for further tracking by Multinomial $(\\cdot)$ without replacement: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathrm{Sample}(\\mathbf{S}_{t}^{l})=\\mathrm{Multinomial}(\\mathbf{S}_{t}^{l},M_{l}),\\;l\\;\\mathrm{from}\\;1\\;\\mathrm{to}\\;L.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "A.2.2 Multiscale architecture ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Multiscale modeling is utilized in DeepLag as represented in Figure 2(d), where we need to maintain multiscale deep Eulerian features as follows: ", "page_idx": 13}, {"type": "text", "text": "Encoder Given Eulerian fluid observation $\\{{\\mathbf{u}}_{t}({\\mathbf{x}})\\}_{{\\mathbf{x}}\\subset{\\mathcal{D}}}$ at the $t$ -th time step, $\\{\\mathbf{u}_{(t-P+1):(t-1)}(\\mathbf{x})\\}_{\\mathbf{x}\\subset\\mathcal{D}}$ at previous time steps and the 0-1 boundary-geometry mask $\\{m({\\bf x})\\}_{{\\bf x}\\subset\\bar{D}}$ where 1 indicates the border and the unreachable area (like pillars in Bounded Navier-Stokes), the Encode() operation is to project original fluid properties in physical domain to deep representations with linear layer and position embedding, which can be formalized as follows: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbf{u}_{t}^{1}=\\mathrm{Linear}\\left(\\mathrm{Concat}\\left(\\{\\mathbf{u}_{(t-P+1):t}(\\mathbf{x}),m(\\mathbf{x})\\}_{\\mathbf{x}\\subset\\mathcal{D}}\\right)\\right)+\\mathrm{PosEmbedding}\\,.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "For position embedding, we concatenate two additional channels to the input (three for 3D Smoke), representing normalized $(x,y)$ coordinates (or $\\left(x,y,z\\right)$ for 3D Smoke). ", "page_idx": 13}, {"type": "text", "text": "Decoder Given evolved Eulerian deep representations in the finest scale $\\{\\mathbf{u}_{t+1}^{1}(\\mathbf{x})\\}_{\\mathbf{x}\\subset\\mathcal{D}}$ at the $(t+1)$ -th time step, the Decode() operation is to project deep representations back to predicted fluid properties with two linear layers and a GeLU activation [16], which can be formalized as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{u}_{t+1}=\\mathrm{Linear}\\Big(\\operatorname{GeLU}\\big(\\mathrm{Linear}(\\{\\mathbf{u}_{t+1}^{1}(\\mathbf{x})\\}_{\\mathbf{x}\\subset\\mathcal{D}})\\big)\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Downsample Given Eulerian deep representations $\\{\\mathbf{u}_{t}^{l}(\\mathbf{x})\\}_{\\mathbf{x}\\subset\\mathcal{D}_{l}}$ at the $l$ -th scale, the Down() operation is to concentrate local information of deep representations into a smaller feature map at the $(l+1)$ -th scale with MaxPooling() and $\\operatorname{Conv}()$ layers, which can be formalized as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbf{u}_{t}^{l+1}=\\operatorname{Conv}\\Big(\\operatorname{MaxPooling}\\big(\\{\\mathbf{u}_{t}^{l}(\\mathbf{x})\\}_{\\mathbf{x}\\subset\\mathcal{D}_{l}}\\big)\\Big),\\ l\\operatorname{from1}\\tan{(L-1)}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Upsample Given the evolved Eulerian deep representations $\\{\\mathbf{u}_{t+1}^{l}(\\mathbf{x})\\}_{\\mathbf{x}\\subset\\mathcal{D}_{l}}$ at the $l$ -th scale and $\\{\\mathbf{u}_{t+1}^{l+1}(\\mathbf{x})\\}_{\\mathbf{x}\\subset\\mathcal{D}_{l+1}}$ at the $(l+1)$ -th scale, respectively, the ${\\mathrm{Up}}()$ operation is to fuse information on corresponding position between two adjacent scales of deep representations into a feature map at the $l$ -th scale with Interpolate() operation and Conv() layers, which can be formalized as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbf{u}_{t+1}^{l}=\\operatorname{Conv}\\left(\\operatorname{Concat}\\left(\\left[\\operatorname{Interpolate}\\left(\\{\\mathbf{u}_{t+1}^{l+1}(\\mathbf{x})\\}_{\\mathbf{x}\\subset\\mathcal{D}_{l+1}}\\right),\\{\\mathbf{u}_{t+1}^{l}(\\mathbf{x})\\}_{\\mathbf{x}\\subset\\mathcal{D}_{l}}\\right]\\right)\\right),\\,l\\operatorname{from}\\,(L-1)\\,\\mathbf{u}\\,1\\ ,\\mathbf{v}\\subset\\mathcal{D}_{l}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "A.3 EuLag Block ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The $\\mathrm{LagToEu}(\\cdot)$ process Hereafter we denote by $\\mathbf{h}_{t}||\\mathbf{p}_{t}$ for the concatenated representations. As we stated in subsection 3.2, the Lagrangian-guided Eulerian feature evolving process, short as $\\mathrm{LagToEu}(\\cdot)$ , aggregates information from Lagrangian description to guide the update of Eulerian field with a single Transformer layer at each scale: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{u}_{t+1}^{l}=\\mathbf{u}_{t}^{l}+\\mathrm{LagToEu}(\\mathbf{u}_{t}^{l},\\mathbf{h}_{t}^{l}||\\mathbf{p}_{t}^{l},\\mathbf{h}_{t}^{l}||\\mathbf{p}_{t}^{l})}\\\\ &{\\qquad=\\mathbf{u}_{t}^{l}+\\mathrm{FFN}(\\mathrm{LagToEu-Attn}(\\mathbf{u}_{t}^{l},\\mathbf{h}_{t}^{l}||\\mathbf{p}_{t}^{l},\\mathbf{h}_{t}^{l}||\\mathbf{p}_{t}^{l})))+\\mathrm{LagToEu-Attn}(\\mathbf{u}_{t}^{l},\\mathbf{h}_{t}^{l}||\\mathbf{p}_{t}^{l},\\mathbf{h}_{t}^{l}||\\mathbf{p}_{t}^{l})),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where L $\\mathrm{\\imath\\mathrm{gToEu-Attn}(\\mathbf{u}_{t}^{l},\\mathbf{h}_{t}^{l}||\\mathbf{p}_{t}^{l},\\mathbf{h}_{t}^{l}||\\mathbf{p}_{t}^{l}))}$ is described as Eq. (7), $l\\in\\{1,2,\\ldots,L\\}$ . Moreover, we use the pre-normalization [53] technique for numerical stability. ", "page_idx": 14}, {"type": "text", "text": "The EuToLag(\u00b7) process Similar to above, we acquire the new particle position $\\mathbf{p}_{t+1}^{l}$ and the global dynamics $\\mathbf{h}_{\\mathrm{global},t+1}^{l}$ in the Eulerian-conditioned particle tracking by a Eulerian-Lagrangian cross-attention: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{h}_{\\mathrm{global},t+1}^{l}\\vert\\vert\\mathbf{p}_{t+1}^{l}=\\mathbf{h}_{t}^{l}\\vert\\vert\\mathbf{p}_{t}^{l}+\\mathrm{EuToLag}(\\mathbf{h}_{t}^{l}\\vert\\vert\\mathbf{p}_{t}^{l},\\mathbf{u}_{t+1}^{l},\\mathbf{u}_{t+1}^{l})}\\\\ &{\\qquad\\qquad\\qquad=\\mathbf{h}_{t}^{l}\\vert\\vert\\mathbf{p}_{t}^{l}+\\mathrm{FFN}(\\mathrm{EuToLag-Attn}(\\mathbf{h}_{t}^{l}\\vert\\vert\\mathbf{p}_{t}^{l},\\mathbf{u}_{t+1}^{l},\\mathbf{u}_{t+1}^{l}))}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\,\\mathrm{EuToLag-Attn}(\\mathbf{h}_{t}^{l}\\vert\\vert\\mathbf{p}_{t}^{l},\\mathbf{u}_{t+1}^{l},\\mathbf{u}_{t+1}^{l}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $l\\in\\{1,2,\\cdots L\\}$ . Then, we interpolate from $\\mathrm{\\mathbf{u}}_{t+1}^{l}$ by $\\mathbf{p}_{t+1}^{l}$ to be the local dynamics $\\mathbf{h}_{\\mathrm{local},t+1}^{l}$ and Aggregate it with the global dynamics $\\mathbf{h}_{\\mathrm{global},t+1}^{l}$ using a Linear layer: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{h}_{t+1}=\\mathrm{Aggregate}\\left(\\operatorname{Interpolate}(\\mathbf{u}_{t+1},\\mathbf{p}_{t+1}),\\mathbf{h}_{\\mathrm{global},t+1}\\right)}\\\\ &{\\qquad=\\mathrm{Linear}\\left(\\operatorname{Concat}\\left(\\mathbf{h}_{\\mathrm{global},t+1}^{l},\\operatorname{Interpolate}(\\mathbf{u}_{t+1}^{l},\\mathbf{p}_{t+1}^{l})\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "A.4 Metrics ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Relative L2 We use the relative L2 as the primary metric for all three tasks. Compared to MSE, Relative L2 is less influenced by outliers and is more robust. For given $n$ steps 2D predictions $\\hat{\\mathbf{x}}\\in\\mathbb{R}^{H\\times W\\times n}$ or 3D predictions $\\hat{\\mathbf{x}}\\in\\mathbb{R}^{H\\times W\\times C\\times n}$ and their corresponding ground truth $\\mathbf{x}$ of the same size, the relative L2 can be expressed as: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathrm{Relative~L2}=\\frac{\\|\\mathbf{x}-\\hat{\\mathbf{x}}\\|_{2}^{2}}{\\|\\mathbf{x}\\|_{2}^{2}},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\|\\cdot\\|_{2}$ represents the L2 norm. ", "page_idx": 14}, {"type": "text", "text": "B Comparison Between DeepLag and Classical ML Methods ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "DeepLag differs significantly from classical ML approaches like Neural ODE [4], as highlighted in the comparison Table 8: ", "page_idx": 15}, {"type": "table", "img_path": "scw6Et4pEr/tmp/ea7ebfb7ecbe65d0c3d93115125f04d7049d2c54dbc677ddc767f919737a1e62.jpg", "table_caption": ["Table 8: Comparison between DeepLag and classical ML model "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "While Neural ODE methods explicitly specify ODEs, fluid dynamics is governed by multi-variable PDEs, rendering the ODE formulation inadequate. Moreover, DeepLag is data-driven and does not require explicit ODE specification. Notably, we are the first to employ attention mechanisms for computing Lagrangian dynamics, which closely resemble operators in both deep learning and numerical Lagrangian methods. Additionally, DeepLag integrates both Eulerian and Lagrangian frameworks, whereas ODE-based methods operate solely in the Eulerian space. Finally, DeepLag models the complex mapping of high-dimensional spatiotemporal PDEs, which is significantly more intricate than the simpler processes modeled by single-variable (either temporal or spatial) ODEs. ", "page_idx": 15}, {"type": "text", "text": "C More Details about the Benchmarks ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "C.1 Bounded Navier-Stokes ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Here we provide some details about the benchmark Bounded Navier-Stokes. Just as its name implies, the motion of dye is simulated from the Navier-Stokes equation of incompressible fluid. 2000 sequences with spatial resolution of $128\\times128$ are generated for training and 200 new sequences are used for the test. We supplement important details indicating the difficulty of the Bounded Navier-Stokes dataset as follows: ", "page_idx": 15}, {"type": "text", "text": "About the Reynolds number The Reynolds number of the dataset is 256. At this Reynolds number, attached vortices dissipate and form a boundary layer separation. The downstream flow field behind the cylinder becomes unsteady, with vortices shedding periodically on both sides of the cylinder\u2019s rear edge, resulting in the well-known phenomenon of K\u00e1rm\u00e1n vortex street [50]. Additionally, due to the presence of multiple cylinders within the flow field and obstruction from downstream cylinders, more complex flow phenomena than flow around a cylinder occur, challenging the model\u2019s capacity. ", "page_idx": 15}, {"type": "text", "text": "Differences of data sequences Our data generation method involves running simulations for over $10^{5}$ steps after setting the initial conditions of the flow field. We then randomly select a starting time step and extract several frames as an example, which are further randomly divided into training and testing sets. The positions of the cylinders are fixed, but the initial condition varies in different samples, which can simulate a scenario like bridge pillars in a torrential river. Due to the highly unsteady nature of the flow field, the flow patterns observed by the model appear significantly different. ", "page_idx": 15}, {"type": "text", "text": "Numerical method used in data generation We utilized the Finite Difference Method (MAC Method) with CIP (Constrained Interpolation Profile) as the Advection Scheme for numerical simulations, implemented by [36]. This high-order interpolation-constrained format effectively reduces numerical dissipation, enhancing the accuracy and reliability of numerical simulations. ", "page_idx": 15}, {"type": "text", "text": "C.2 Ocean Current ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Some important details related to the Ocean Current benchmark are attached here. Learning ocean current patterns from data and providing long-term forecasts are of great significance for disaster prevention and mitigation, which motivates us to focus on this problem. ", "page_idx": 15}, {"type": "text", "text": "The procedures to make this dataset are as follows. First, we downloaded daily sea reanalysis data [5] from 2011 to 2020 provided by the ECMWF and selected five basic variables on the sea surface to construct the dataset, including velocity, salinity, potential temperature, and height above the geoid, which are necessary to identify the ocean state. Then, we crop a $180\\times300$ sub-area on the North Pacific from the global record, corresponding to a $375\\mathrm{km}{\\times}625\\mathrm{km}$ region. In total, this dataset consists of 3,653 frames, where the first 3000 frames are used for training, and the last 600 frames are used for testing. The training task is to predict the future current of 10 days based on the past 10 days\u2019 observation, after which we performed 30 days of inference with the trained model to examine the long-term stability of DeepLag. ", "page_idx": 16}, {"type": "text", "text": "C.3 3D Smoke ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "To verify our model effectiveness in this complex setting of the high-dimensional tanglesome molecular interaction, we also generate a 3D fluid dataset for the experiment. This benchmark consists of a scenario where smoke flows under the influence of buoyancy in a three-dimensional bounding box. This process is governed by the incompressible Navier-Stokes equation and the advection equation of fluid. 1000 sequences are generated for training, and 200 new samples are used for testing. Each case is in the resolution of $32^{3}$ . ", "page_idx": 16}, {"type": "text", "text": "D Training Curves ", "text_level": 1, "page_idx": 16}, {"type": "image", "img_path": "scw6Et4pEr/tmp/8830968c6150dcdd6fa61cc7402f0dc227496013907d598a1e3e4b396b8fca2f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Figure 8: Training curve comparison among all the models on Bounded Navier-Stokes dataset, Ocean Current, and 3D Smoke dataset. ", "page_idx": 16}, {"type": "text", "text": "We provide training curves on Bounded Navier-Stokes, Ocean Current, and 3D Smoke datasets in Figure 8. We can observe that DeepLag presents favorable training robustness and converges the fastest on the Bounded Navier-Stokes dataset. ", "page_idx": 16}, {"type": "text", "text": "E Analysis on the Parameter Count and Performance Difference ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In Table 1 in our paper, we present three different datasets with variations in data type, number of variables, number of dimensions, and spatial resolution. Specifically, the 2D fluid and 3D fluid represent entirely different dynamic systems, so it is normal for different baselines to perform inconsistently across different benchmarks. In the following results, patch_size of all models are set to 1 for fair comparison. For instance, U-Net performs well on datasets with distinct multiscale attributes (such as the Bounded Navier-Stokes dataset), while transformer-based methods excel on datasets with broad practical ranges (like the Ocean Current dataset) or high dimensions (such as the 3D Smoke dataset), where attention mechanisms can effectively model global features. Hence, our model\u2019s ability to handle multiscale and global modeling simultaneously highlights the challenge of achieving consistent state-of-the-art performance. Below are the parameter statistics for DeepLag and all baseline models across each benchmark, along with some experiments we conducted to ensure parity in the parameter count for each model. ", "page_idx": 16}, {"type": "text", "text": "E.1 Analysis on Bounded Navier-Stokes benchmark ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The parameter quantities for each model on the Bounded Navier-Stokes benchmark are shown in Table 9. When attempting to further increase the parameter count of smaller models for fair comparison, we encountered CUDA Out-of-memory errors. Specifically, these errors occurred when we increased the Dynamic_net_layer of Vortex from 4 to 50, resulting in a parameter count of 171,761, but encountered CUDA Out of Memory issues. ", "page_idx": 16}, {"type": "text", "text": "Table 9: Model parameter summary for Bounded Navier-Stokes dataset. ", "page_idx": 17}, {"type": "table", "img_path": "scw6Et4pEr/tmp/c15f5f94dd9839e92f48543a508df6be1cddfda3964c5a06317a66b436de5c0a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "E.2 Analysis on Ocean Current benchmark ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Table 10: Model parameter summary for Ocean Current dataset. ", "page_idx": 17}, {"type": "table", "img_path": "scw6Et4pEr/tmp/bc58f087d4d1c9eb89ffa85c0076101e05cf03317937be98ea818d9d27630f24.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "The parameter quantities for each model on the Ocean Current dataset are summarized in Table 10. When attempting to further increase the parameter count of smaller models for fair comparison, we encountered CUDA Out-of-memory errors. Specifically, these errors occurred when we increased the Dynamic_net_layer of Vortex from 4 to 50, resulting in a parameter count of 171,761, but encountered CUDA Out of Memory issues. Similarly, for GNOT, increasing the latent_dim from 64 to 96 resulted in a parameter count of 1,659,179, yet this configuration encountered CUDA Out of Memory errors. ", "page_idx": 17}, {"type": "text", "text": "E.3 Analysis on 3D Smoke benchmark ", "text_level": 1, "page_idx": 17}, {"type": "table", "img_path": "scw6Et4pEr/tmp/a453da220854430f9cf3b3e3c95e3487e30be2b6603f05dd8f0dbbed097eccbf.jpg", "table_caption": ["Table 11: Model parameter summary for 3D Smoke dataset. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "The parameter quantities for each model on the 3D Smoke dataset are shown in Table 11. When experimenting with increasing the parameter count of certain models for fair comparison, we encountered CUDA Out-of-memory errors. Specifically, these errors occurred when we increased the latent_dim of GNOT from 64 to 96, resulting in a parameter count of 1,450,768 for GNOT, which encountered CUDA Out of Memory issues. Similarly, when adjusting the encoder_transformer_layer of FactFormer from 3 to 13, the parameter count reached 7,382,084, yet this configuration encountered CUDA Out of Memory errors. ", "page_idx": 17}, {"type": "text", "text": "Figure 7 in our paper, along with the provided tables above, illustrate our rigorous comparison and efforts to standardize model parameters for a fair comparison. However, due to excessive GPU ", "page_idx": 17}, {"type": "text", "text": "Table 12: ACC results on the Ocean Current dataset and the curve of timewise ACC. Both ACC averaged from 10 prediction steps and ACC of the last prediction frame are recorded. A higher ACC value indicates better performance. Relative promotion is also calculated. ", "page_idx": 18}, {"type": "table", "img_path": "scw6Et4pEr/tmp/e31d2a0e740ff97a50ebe31a45ce99281f91c202d77d41301d1d02fccd62664c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "memory consumption of intermediate results in some baseline models, we could not conduct direct performance comparisons under matched parameter conditions. The ability of our model to efficiently utilize GPU memory resources is a valuable aspect of practical applications. ", "page_idx": 18}, {"type": "text", "text": "F ACC Metric on Ocean Current ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Latitude-weighted Anomaly Correlation Coefficient In meteorology, directly calculating the correlation between predictions and ground truth may obtain misleadingly high values because of the seasonal variations. To subtract the climate average from both the forecast and the ground truth, we utilize the Anomaly Correlation Coefficient to verify the forecast and observations. Moreover, since the observation grids are equally spaced in longitude, and the size of the different grids is related to the latitude, we calculate the latitude-weighted Anomaly Correlation Coefficient, which can be formalized as: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathrm{ACC}(v,t)=\\frac{\\sum_{i,j}\\mathrm{Lat}(\\phi_{i})\\hat{\\mathbf{x}}_{i,j,t}^{\\prime v}\\mathbf{x}_{i,j,t}^{\\prime v}}{\\sqrt{\\sum_{i,j}\\mathrm{Lat}(\\phi_{i})\\left(\\hat{\\mathbf{x}}_{i,j,t}^{\\prime v}\\right)^{2}\\times\\sum_{i,j}\\mathrm{Lat}(\\phi_{i})\\left(\\mathbf{x}_{i,j,t}^{\\prime v}\\right)^{2}}},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $v$ represents a certain observed variable, $\\hat{\\mathbf{x}}_{i,j,t}$ is the prediction of ground truth $\\mathbf{x}$ at position $i,j$ and forecast time $t$ . $\\mathbf{x}^{\\prime}=\\mathbf{x}-\\bar{\\mathbf{x}}$ represents the difference between $\\mathbf{x}$ and the climatology $\\bar{\\bf x}$ , that is, the long-term mean of observations in the dataset. $\\begin{array}{r}{\\mathrm{Lat}(\\phi_{i})=N_{\\mathrm{Lat}}\\,\\times\\frac{\\cos\\phi_{i}}{\\sum_{i^{\\prime}=1}^{N_{\\mathrm{Lat}}}\\cos\\phi_{i^{\\prime}}}}\\end{array}$ , where $N_{\\mathrm{Lat}}=180$ and $\\phi_{i}$ is the latitude of the $i$ -th row of output. ", "page_idx": 18}, {"type": "text", "text": "ACC result on the Ocean Current benchmark Notably, DeepLag also excels in the ACC metric, as shown in Table 12, which can better quantify the model prediction skill. As shown in the timewise ACC curve, DeepLag consistently achieves the highest ACC and holds more significant advantages in long-term prediction. Since ACC is calculated with long-term climate statistics, further improvements become increasingly challenging as it increases, which highlights the value of DeepLag. ", "page_idx": 18}, {"type": "text", "text": "G Visual Results for Learnable Sampling ", "text_level": 1, "page_idx": 18}, {"type": "image", "img_path": "scw6Et4pEr/tmp/55c3550308e7098b48e36be2ed65abdf1f8de6d492a71b0794a38e2e00c01504.jpg", "img_caption": [], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Figure 9: Visualization of the pointwise variance of vorticity (middle) and the sampling probability distribution (right) learned by DeepLag. We plot the $\\log(\\mathbf{S})$ here for a better view. ", "page_idx": 18}, {"type": "text", "text": "To demonstrate the effectiveness of our learnable probability, we visualize its distribution S with respect to the pointwise variance of vorticity, which is directly proportional to the local complexity of fluid dynamics, in Figure 9. It is evident that our sampling module tends to prioritize focusing on and sampling particles within regions having higher dynamical complexity, such as the wake flow and Karman vortex [50] near the domain borders and behind the pillar. The showcase in Figure 1 also demonstrates that the tracked particle can well present the dynamics of a certain area. This observation underscores that our design is very flexible at adapting to various complex boundary conditions and can effectively guide the model to track the most crucial particles, enhancing its performance in capturing the fine details in the wake zone where turbulence and vortex form. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "H More Showcases ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "As a supplement to the main text, we provide more showcases here for comparison (Figure 10-15). ", "page_idx": 19}, {"type": "image", "img_path": "scw6Et4pEr/tmp/29d9a322cdde9f7739df433001e45a699a8261f0f1456ab8affa9536817b0520.jpg", "img_caption": ["Figure 10: Showcases of the Bounded Navier-Stokes dataset. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "scw6Et4pEr/tmp/cb47d276c84e1686aa4ff94d06b10038411f4d3f4ea819973fdbd6c160f2454e.jpg", "img_caption": ["Figure 11: Showcases of DeepLag on the Bounded Navier-Stokes dataset. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "scw6Et4pEr/tmp/3bc5674d3942a0d998d7b1c3040b6f6a1121ff3b3e84a9a0e1e2eed00d4d6a18.jpg", "img_caption": ["Figure 12: Showcases of the 3D Smoke dataset. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "scw6Et4pEr/tmp/52b56d74ebfe7056a9c6fd50181f83832bb33b4c98ba1826b288754c31db2199.jpg", "img_caption": ["Figure 13: Showcases of DeepLag on the 3D Smoke dataset. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "scw6Et4pEr/tmp/0f01edf3a87ef5e642f254f0cd2d7af6581304750d6da0aaa5928848c2937843.jpg", "img_caption": ["Figure 14: Showcases of the Ocean Current dataset. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "scw6Et4pEr/tmp/1c9cdfc29c35219e1e37689d71c931c38a98a9125f4870ab64fab367aaa453d2.jpg", "img_caption": ["Figure 15: Showcases of DeepLag on the Ocean Current dataset. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "I Result of Long-term Prediction ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "I.1 Reason for predicting 10 steps ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Predicting the last 10 steps from the first 10 steps input is a convention (refer to FNO where $\\nu=1e-5$ , $T=20$ is the setting, and later several baselines using the NS dataset also followed this setting). We did this to follow the convention and for ease of comparison. Additionally, the Ocean Current dataset has a one-day interval between every two frames, and predicting 10 days ahead is already a long horizon. ", "page_idx": 21}, {"type": "text", "text": "I.2 Results of long-term rollout ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We conducted experiments for long-term (extrapolation beyond hundreds of frames) predictions. Accurately, we utilize trained 10-frame prediction models to perform 100-frame prediction. Our reason for not directly training a model in an autoregressive paradigm to predict the next 100 frames is due to insufficient memory capacity and the issue of gradient explosion or vanishing. The results are as follows. The best result is bold and the second is underlined. The reason we did not run the 3D Smoke dataset is that it is very challenging to load 110 frames of large 3D data at once, which overwhelms our machine. Extra video results are in supplementary materials, which effectively illustrate the performance and consistency of our model\u2019s predictions. ", "page_idx": 21}, {"type": "text", "text": "I.2.1 Bounded Navier-Stokes ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Quantitive results As depicted in Table13, the DeepLag model still outperforms the strong baselines on the Bounded Navier-Stokes dataset in predicting 100 frames into the future. With a Relative L2 of 0.1493, DeepLag achieves superior performance compared to all baselines. Additionally, the performance trends of all models are visually illustrated in Figure 16-20, revealing DeepLag\u2019s ability to maintain lower error growth rates over time, particularly in long-term predictions. This outcome suggests that the Lagrangian particle-based approach adopted by DeepLag effectively captures dynamic information, contributing to its robust forecasting capability in fluid dynamics modeling. ", "page_idx": 21}, {"type": "text", "text": "Table 13: Performance comparison for predicting 100 frames on the Bounded Navier-Stokes dataset.   \nRelative L2 is recorded. For clarity, the best result is in bold and the second best is underlined.   \nPromotion represents the relative promotion of our model w.r.t the second best model. ", "page_idx": 21}, {"type": "table", "img_path": "scw6Et4pEr/tmp/2695b948a7ebe1da0fd5a9357b6227ef1fe08cdfd4c02dd05a0a9dff22b50a36.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "Showcases To visually evaluate the predictive capabilities of our models on long-term, we present a showcase of last frame comparisons and time-wise prediction with error map in Figure 16-20, illustrating the long-term rollout performance on the Bounded Navier-Stokes dataset. Notably, DeepLag demonstrates remarkable accuracy in capturing complex flow phenomena, accurately predicting the formation and evolution of vortices, particularly the K\u00e1rm\u00e1n vortex street behind the upper left pillar. In contrast, U-Net and LSM exhibit moderate success in predicting the central vortex but struggle with accurately reproducing the density distribution of the flow field, as indicated by the error maps. FactFormer, however, shows subpar performance on this benchmark, likely due to its reliance on spatial factorization, which may not effectively handle irregular boundary conditions. These findings underscore the advantages of our Eulerian-Lagrangian co-design approach, which enables simultaneous prediction of dynamics and density, contributing to more accurate and comprehensive fluid modeling and forecasting capabilities. ", "page_idx": 21}, {"type": "image", "img_path": "scw6Et4pEr/tmp/c5b28427f8881a78815358db0fcdac0a1b891c6e21f6876726875a4a2026bb99.jpg", "img_caption": ["Figure 16: Showcases comparison between the most competitive models of long-term rollout on the Bounded Navier-Stokes benchmark. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "scw6Et4pEr/tmp/9326a1a0cf1eeeac20ababb3224d3a6ae7722bbcd953b93d415d4c7f36539e4c.jpg", "img_caption": ["Figure 17: Timewise showcases of DeepLag of long-term rollout on the Bounded Navier-Stokes benchmark. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "scw6Et4pEr/tmp/1199450bb8f2e74931f169b497ec97a8555355c7507f1556609151df4312fb83.jpg", "img_caption": ["Figure 18: Timewise showcases of FactFormer of the long-term rollout on the Bounded Navier-Stokes benchmark. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "scw6Et4pEr/tmp/744505144047d9db4a43e63689aab98cc46188ddd4fa7be90878376ee7f7888f.jpg", "img_caption": ["Figure 19: Timewise showcases of LSM of the long-term rollout on the Bounded Navier-Stokes benchmark. "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "scw6Et4pEr/tmp/e1072e1b0017d017cba8e6d1fa4d498a2da8851e04f05d6edcea9e6fe4cf0a25.jpg", "img_caption": ["Figure 20: Timewise showcases of U-Net of long-term rollout on the Bounded Navier-Stokes benchmark. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "I.2.2 Ocean Current ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Quantitive results We present a comparison of results for the Ocean Current dataset in Table 14, which includes relative L2, Last frame ACC, and Average ACC metrics. DeepLag maintains its superiority, achieving the lowest relative L2 among all models, with an $8.7\\%$ promotion compared to the second-best model. These findings highlight DeepLag\u2019s robust performance in predicting realworld large-scale fluid dynamics, which often exhibit inherent stochasticity. Furthermore, DeepLag outperforms other models in ACC metrics, indicating its superior predictive capability. This is further corroborated by the timewise ACC curve, where DeepLag consistently demonstrates the highest ACC values, particularly in long-term predictions. ", "page_idx": 23}, {"type": "text", "text": "Showcases To visually assess the long-term forecasting performance of each model, we showcase the last frame predictions along with their errors, and the time-wise prediction errors for each model in Figure 21-25. Visually, our DeepLag predictions closely resemble the Ground Truth compared to other models, demonstrating robust long-term extrapolation capabilities and accurate capture of the Kuroshio pattern [37]. It can be observed that FactFormer and LSM exhibit relatively large errors, while GNOT tends to average and loses fine texture details. However, DeepLag does not suffer from these issues. ", "page_idx": 23}, {"type": "text", "text": "I.3 Examination on the turbulent kinetic energy spectrum ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In the field of fluid mechanics, simulation results that better adhere to intrinsic physical laws are sometimes more valuable than those with smaller pointwise errors, often reflected in frequency domain analysis. To validate this point and to measure long-term forecasting ability, we introduced a metric on time-averaged turbulent statistics, namely the turbulent kinetic energy spectrum (TKES). Specifically, we computed the MAE and RMSE of TKES for the Bounded Navier-Stokes dataset, which exhibits the most prominent turbulent characteristics, and plotted the line graphs of the error of ", "page_idx": 23}, {"type": "image", "img_path": "scw6Et4pEr/tmp/17456c21385f4d0078662cd73f0ec93c027bb232a3c308683d54ef546df6ed42.jpg", "img_caption": ["Figure 21: Showcases comparison between the most competitive models of the long-term rollout on the Ocean Current benchmark. "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "scw6Et4pEr/tmp/eb0bb115206df387a4c74c0416ff4885c7e3b3320c221afbb161f1914a9b5a3a.jpg", "img_caption": ["Figure 22: Timewise showcases of DeepLag of the long-term rollout on the Ocean Current benchmark. "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "scw6Et4pEr/tmp/ff1e8f04ab32cc8db5d14523ccd5d5eb53b09ccaeddc551477b4a9cb01ccab78.jpg", "img_caption": [], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Figure 23: Timewise showcases of FactFormer of the long-term rollout on the Ocean Current benchmark. ", "page_idx": 24}, {"type": "image", "img_path": "scw6Et4pEr/tmp/0ec86f682994722459bd5b264057e73ab8c170a062300dc270bb5f91fcdc7944.jpg", "img_caption": ["Figure 24: Timewise showcases of LSM of the long-term rollout on the Ocean Current benchmark. "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "scw6Et4pEr/tmp/70841e0386a6d916a034b501b1f223eba4d94671dd0e1a095b6189bde990a4e9.jpg", "img_caption": [], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Figure 25: Timewise showcases of GNOT of the long-term rollout on the Bounded Navier-Stokes benchmark. ", "page_idx": 24}, {"type": "table", "img_path": "scw6Et4pEr/tmp/32d2925c155af76416c906a671817dce48bb39ee3bd2afa9bf8af97bf8b0b910.jpg", "table_caption": ["Table 14: Relative L2 for predicting 100 frames, Last frame ACC, and Average ACC "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "wave number and TKE, as shown in Table 15. It can be observed both numerically and visually that DeepLag consistently outperforms various baselines to different extents. ", "page_idx": 25}, {"type": "text", "text": "Table 15: Turbulent kinetic energy spectrum (TKES) results on the Bounded Navier-Stokes. Both the plot of error of TKES (left) and MAE and RMSE of TKES (right) are presented. A lower TKES error value indicates better performance. Relative promotion is also calculated. ", "page_idx": 25}, {"type": "image", "img_path": "scw6Et4pEr/tmp/5fd2a27759c2e1f70ef89c224ef96cb927aedd858a6d358007a5426f8bcf2505.jpg", "img_caption": [], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "J Visual Result of the Boundary Condition Generalization Experiment ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "To demonstrate the generalizing performance of DeepLag, we visualize the showcases in Figure 26 between DeepLag and the best baseline, U-Net. This intuitive result further shows that DeepLag adaptively generalizes well on new domains and handles complex boundaries well. ", "page_idx": 25}, {"type": "image", "img_path": "scw6Et4pEr/tmp/d07c086666fbacb2dbbbaebd5c7043488782524bbce7c2edfc86d0e6010248c3.jpg", "img_caption": ["Figure 26: The visual comparison of zero-shot inference on the new Bounded Navier-Stokes. "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "K Visualization of the Movement of the Particles ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We visualize the particle movements on the Bounded Navier-Stokes dataset in Figure 27. Given the dataset\u2019s complex and frequent motion patterns, we have plotted particle offsets between consecutive frames, which effectively reflect instantaneous particle movements. As depicted, DeepLag can still learn intuitive and reasonable motion without a standard physical velocity field. Notably, DeepLag performs best in this dataset, highlighting the benefits of learning Lagrangian dynamics. ", "page_idx": 26}, {"type": "image", "img_path": "scw6Et4pEr/tmp/f3414bc6b22741b9ce06e6f8ae6816293e35aadac01312ac113f89a9d12fd6dc.jpg", "img_caption": ["Figure 27: Visualization of the particle movements on the Bounded Navier-Stokes dataset. The plotted particle offsets between consecutive frames effectively reflect instantaneous particle movements. As depicted in the red boxes, DeepLag can accurately capture the motion mode of complex dynamics like K\u00e1rm\u00e1n vortex street. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: In the abstract and introduction, we point out that the Eulerian fluid prediction problem is addressed in this paper, and we tackle it by proposing a new architecture that incorporate Lagrangian dynamics to guide the Eulerian field update, which is our main contribution. Moreover, we made an exhaustive and fair comparison and the experiment result proves the effectiveness claim in the abstract and introduction. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 27}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: In the Section 5, we discussed the hyperparameter $M$ (number of Lagrangian tracking particles) can not automatically adjust to a certain task, which may be not convenient or out of the box for practical application of DeepLag. Plus, all the experiment are repeated 3 times to confirm our claims. Comparison between the benchmarks show that our model and baselines are slower at data of a finer resolution, which is inevitable. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best ", "page_idx": 27}, {"type": "text", "text": "judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 28}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Except for the basic relations (Eq. (1) - (2)) between the Eulerian and Lagrangian perspectives we only draw inspirations from, no theorem is derived or explicitly used in DeepLag, since they are usually connected to a specific physical equation which will be harmful to the generalization of our approach. All the knowledge from fluid dynamics or numerical method are properly referenced. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 28}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: To ensure a fair and reproducible result, we unify the settings of all models and conduct experiments 3 times under identical environments (hardware and software), and we make elaborate explanation on every detail of model module implementation in the main text and appendix, along with metric and visual results. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.   \n(b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: All the data are public. Code is available at this repository: https://github. com/thuml/DeepLag. We have provided detailed descriptions for these benchmarks and implementations to reproduce. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 29}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: We provide all the training and test details at the start of section 4, and details on the benchmarks in Appendix C, which is enough to understand and reproduce the results. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 29}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [No] ", "page_idx": 30}, {"type": "text", "text": "Justification: Error bars are not reported because it would be too computationally expensive. That being said, every number of results comes from multiple identical experiments with no statistical significant change. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 30}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: At the start of section 4, we provide the information of GPU that we use in training, and we included model efficiency comparison in subsection 4.4. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 30}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We have carefully read the NeurIPS Code of Ethics and our research conforms with it in every respect. We are sure that the anonymity is preserved during the whole procedure. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. ", "page_idx": 30}, {"type": "text", "text": "\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 31}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Beyond the importance mentioned at the setup of the Ocean Current dataset, positive societal impacts of a deep model for fluid prediction is in various domain including environmental protection (like pollution control or disaster prevention and mitigation), energy sector (like renewable energy utilization or resource management), economic beneftis (maritime navigation or agriculture) and scientific advancement itself. Since it does not involve the generation of subjective content which many people concern, there is seldom negative societal impacts unless society become overly reliant or blindly trust on automated predictions. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 31}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: All data sources in this paper have open access, and the model does not involve the generation of subjective content, their is no risk for misuse like this type. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. ", "page_idx": 31}, {"type": "text", "text": "\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 32}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: All assets used to generate dataset are properly cited in the paper. The url of repository for Bounded Navier-Stokes generation is https://github.com/takah29/ 2d-fluid-simulator, where no licence found. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 32}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We included the demo videos in the supplementary materials. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 32}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 32}, {"type": "text", "text": "\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 33}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 33}]