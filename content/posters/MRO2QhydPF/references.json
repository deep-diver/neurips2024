{"references": [{"fullname_first_author": "Joshua Achiam", "paper_title": "Constrained policy optimization", "publication_date": "2017-00-00", "reason": "This paper introduces a widely used risk-aware safe RL algorithm that is used as a benchmark for comparison in the paper's experiments."}, {"fullname_first_author": "Tuomas Haarnoja", "paper_title": "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor", "publication_date": "2018-00-00", "reason": "This paper introduces the Soft Actor-Critic algorithm, a state-of-the-art off-policy RL algorithm that is used as the basis for the RL agent in the proposed RL-AR algorithm."}, {"fullname_first_author": "Richard Cheng", "paper_title": "End-to-end safe reinforcement learning through barrier functions for safety-critical continuous control tasks", "publication_date": "2019-00-00", "reason": "This paper introduces a safe RL algorithm that uses barrier functions to enforce safety constraints, which is related to the policy regularization approach used in the proposed RL-AR algorithm."}, {"fullname_first_author": "Richard Cheng", "paper_title": "Control regularization for reduced variance reinforcement learning", "publication_date": "2019-00-00", "reason": "This paper introduces a control regularization technique that is used to reduce the variance of reinforcement learning, which is relevant to the policy regularization aspect of the proposed RL-AR algorithm."}, {"fullname_first_author": "Volodymyr Mnih", "paper_title": "Playing atari with deep reinforcement learning", "publication_date": "2013-00-00", "reason": "This paper is a foundational work in deep reinforcement learning and its use of a replay buffer is relevant to the experience replay mechanism used in the proposed RL-AR algorithm."}]}