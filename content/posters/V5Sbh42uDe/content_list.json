[{"type": "text", "text": "Potts Relaxations and Soft Self-labeling for Weakly-Supervised Segmentation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 We consider weakly supervised segmentation where only a fraction of pixels   \n2 have ground truth labels (scribbles) and focus on a self-labeling approach where   \n3 soft pseudo-labels on unlabeled pixels optimize some relaxation of the standard   \n4 unsupervised CRF/Potts loss. While WSSS methods can directly optimize CRF   \n5 losses via gradient descent, prior work suggests that higher-order optimization   \n6 can lead to better network training by jointly estimating pseudo-labels, e.g. using   \n7 discrete graph cut sub-problems. The inability of hard pseudo-labels to represent   \n8 class uncertainty motivates the relaxed pseudo-labeling. We systematically evaluate   \n9 standard and new CRF relaxations, neighborhood systems, and losses connecting   \n10 network predictions with soft pseudo-labels. We also propose a general continuous   \n11 sub-problem solver for such pseudo-labels. Soft self-labeling loss combining the   \n12 log-quadratic Potts relaxation and collision cross-entropy achieves state-of-the-art   \n13 and can outperform full pixel-precise supervision on PASCAL. ", "page_idx": 0}, {"type": "text", "text": "14 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "15 Full supervision for semantic segmentation requires thousands of training images with complete pixel  \n16 accurate ground truth masks. Their high costs explain the interest in weakly-supervised approaches   \n17 based on image-level class tags [21, 4], pixel-level scribbles [26, 36, 35], or boxes [23]. This paper   \n18 is focused on weak supervision with scribbles, which we also call seeds or partial masks. While   \n19 only slightly more expensive than image-level class tags, scribbles on less than $3\\%$ of pixels were   \n20 previously shown to achieve accuracy approaching full supervision without any modifications of the   \n21 segmentation models. In contrast, tag supervision typically requires highly specialized systems and   \n22 complex multi-stage training procedures, which are hard to reproduce. Our interest in the scribble  \n23 based approach is motivated by its practical simplicity and mathematical clarity. The corresponding   \n24 methodologies are focused on the design of unsupervised or self-supervised loss functions and   \n25 stronger optimization algorithms. The corresponding solutions are often general and can be used in   \n26 different weakly-supervised applications. ", "page_idx": 0}, {"type": "text", "text": "27 1.1 Scribble-supervised segmentation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "28 Assume that a set of image pixels is denoted by $\\Omega$ and a subset of pixels with ground truth labels is   \n29 $S\\subset\\Omega$ , which we call seeds or scribbles as subset $S$ is typically marked by mouse-controlled UI for   \n30 image annotations, e.g. see seeds over an image in Fig.7(a). The ground truth label at any given pixel   \n31 $i\\in S$ is an integer ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\bar{y}_{i}\\in\\{1,\\ldots,K\\}\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "32 where $K$ is the number of classes including the background. Without much ambiguity, it is convenient   \n33 to use the same notation ${\\bar{y}}_{i}$ for the equivalent one-hot distribution ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\bar{y}_{i}}&{\\equiv}&{\\big(\\bar{y}_{i}^{1},\\ldots,\\bar{y}_{i}^{K}\\big)\\,\\in\\Delta_{0,1}^{K}\\quad\\quad\\quad\\mathrm{for}\\quad\\bar{y}_{i}^{k}:=[k=\\bar{y}_{i}]\\,\\in\\{0,1\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\left[\\,\\cdot\\,\\right]$ is the True operator for the condition inside the brackets. Set $\\Delta_{0,1}^{K}$ represents $K$ possible one-hot distributions, which are vertices of the $K$ -class probability simplex ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\Delta^{K}\\;:=\\;\\;\\{p=(p^{1},\\ldots,p^{K})\\;|\\;p^{k}\\geq0,\\;\\sum_{k=1}^{K}p^{k}=1\\}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "34 representing all $K$ -categorical distributions. The context of specific expressions should make it   \n35 obvious if $\\bar{y}_{i}$ is a class index (1) or the corresponding one-hot distribution (2).   \n36 Loss functions for weakly supervised segmentation with scribbles typically use negative log  \n37 likelihoods (NLL) over scribbles $i\\in S\\subset\\Omega$ with ground truth labels ${\\bar{y}}_{i}$ ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "equation", "text": "$$\n-\\sum_{i\\in S}\\ln\\sigma_{i}^{\\bar{y}_{i}}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "38 where $\\sigma_{i}\\,=\\,(\\sigma_{i}^{1},\\dots,\\sigma_{i}^{K})\\,\\in\\,\\Delta^{K}$ is the model prediction at pixel $i$ . This loss is a standard in   \n39 full supervision where the only difference is that $S\\,=\\,\\Omega$ and usually, no other losses are needed   \n40 for training. However, in a weakly supervised setting the majority of pixels are unlabeled, and   \n41 unsupervised losses are needed for $i\\not\\in S$ . ", "page_idx": 1}, {"type": "text", "text": "The most common unsupervised loss in image segmentation is the Potts model and its relaxations. It is a pairwise loss defined on pairs of neighboring pixels $\\{i,j\\}\\in\\mathcal{N}$ for a given neighborhood system $\\mathcal{N}\\subset\\Omega\\times\\Omega$ , typically corresponding to the nearest-neighbor grid (NN) [6, 17], or other sparse (SN) [38] and dense neighborhoods (DN) [22]. The original Potts model is defined for discrete segmentation variables, e.g. as in ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\sum_{\\{i,j\\}\\in{\\cal N}}{\\cal P}(\\sigma_{i},\\sigma_{j})\\qquad\\mathrm{where}\\quad{\\cal P}(\\sigma_{i},\\sigma_{j})=[\\sigma_{i}\\not=\\sigma_{j}]\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "assuming integer-valued one-hot predictions $\\sigma_{i}\\in\\Delta_{0,1}^{K}$ . This regularization loss encourages smoothness between the pixels. Its popular self-supervised variant is ", "page_idx": 1}, {"type": "equation", "text": "$$\nP(\\sigma_{i},\\sigma_{j})=w_{i,j}\\cdot[\\sigma_{i}\\neq\\sigma_{j}]\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "42 where pairwise affinities $w_{i j}$ are based on local intensity edges [6, 17, 22]. Of course, in the context   \n43 of network training, one should use relaxations of $P$ applicable to (soft) predictions $\\sigma_{i}\\in\\Delta^{K}$ . Many   \n44 types of its relaxation [33, 42] were studied in segmentation, e.g. quadratic [17], bi-linear [36], total   \n45 variation [32, 8], and others [14]. ", "page_idx": 1}, {"type": "text", "text": "Another unsupervised loss highly relevant for training segmentation networks is the entropy of predictions, which is also known as decisiveness [7, 18] ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\sum_{i}H(\\sigma_{i})\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "46 where $H$ is the Shannon\u2019s entropy function. This loss can improve generalization and the quality of   \n47 representation by moving (deep) features away from the decision boundaries. Widely known in the   \n48 context of unsupervised or semi-supervised classification, this loss also matters in weakly-supervised   \n49 segmentation where it is used explicitly or implicitly1.   \n50 Other unsupervised losses (e.g. contrastive), clustering criteria (e.g. K-means), or specialized   \n51 architectures can be found in weakly-supervised segmentation [39, 31, 20, 9]. However, a lot can be   \n52 achieved simply by combining the basic losses discussed above ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "equation", "text": "$$\nL_{w s}(\\sigma)\\;:=\\;\\;-\\sum_{i\\in S}\\ln\\sigma_{i}^{\\bar{y}_{i}}\\;+\\;\\eta\\sum_{i\\notin S}H(\\sigma_{i})\\;+\\;\\lambda\\sum_{i j\\in N}P(\\sigma_{i},\\sigma_{j})\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "53 which can be optimized directly by gradient descent [36, 38] or using self-labeling techniques   \n54 [26, 28, 27] incorporating optimization of auxiliary pseudo-labels as sub-problems. ", "page_idx": 1}, {"type": "text", "text": "55 1.2 Soft pseudo-labels: motivation and contributions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "56 We observe that self-labeling with hard pseudo-labels $y_{i}$ , which is discussed in the Appendix A, is   \n57 inherently limited as such labels can not represent the uncertainty of class estimates at unlabeled   \n58 pixels $i\\in\\Omega\\backslash S$ . Instead, we focus on soft pseudo-labels ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{y_{i}}&{=}&{(y_{i}^{1},\\dots,y_{i}^{K})\\\\\\in\\Delta^{K}}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "59 which are general categorical distributions $p$ over $K$ -classes. It is possible that the estimated pseudo  \n60 label $y_{i}$ in (5) could be a one-hot distribution, which is a vertex of $\\Delta^{K}$ . In such a case, one can treat   \n61 $y_{i}$ as a class index, but we avoid this in the main part of our paper starting Section 2. However, the   \n62 ground truth labels ${\\bar{y}}_{i}$ are always hard and we use them either as indices (1) or one-hot distributions   \n63 (2), as convenient.   \n64 Soft pseudo-labels can be found in prior work on weakly-supervised segmentation [25, 41] using the   \n65 \u201csoft proposal generation\u201d. In contrast, we formulate soft self-labeling as a principled optimization   \n66 methodology where network predictions and soft pseudo-labels are variables in a joint loss, which   \n67 guarantees convergence of the training procedure. Our pseudo-labels are auxiliary variables for   \n68 ADM-based [5] splitting of the loss (4) into two simpler optimization sub-problems: one focused on   \n69 the Potts model over unlabeled pixels, and the other on the network training. While similar to [28],   \n70 instead of hard, we use soft auxiliary variables for the Potts sub-problem. Our work can be seen as a   \n71 study of the relaxed Potts sub-problem in the context of weakly-supervised semantic segmentation.   \n72 The related prior work is focused on discrete solvers fundamentally unable to represent class estimate   \n73 uncertainty. Our contributions can be summarized as follows:   \n74 \u2022 convergent soft self-labeling framework based on a simple joint self-labeling loss   \n75 \u2022 systematic evaluation of Potts relaxations and (cross-) entropy terms in our loss   \n76 \u2022 state-of-the-art in scribble-based semantic segmentation that does not require any modifica  \n77 tions of semantic segmentation models and is easy to reproduce   \n78 \u2022 using the same segmentation model, our self-labeling loss with $3\\%$ scribbles may outperform   \n79 standard supervised cross-entropy loss with full ground truth masks. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "80 2 Our soft self-labeling approach ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "First, we apply ADM splitting [5] to weakly supervised loss (4) to formulate our self-labeling loss (6) incorporating additional soft auxiliary variables, i.e. pseudo-labels (5). It is convenient to introduce pseudo-labels $y_{i}$ on all pixels in $\\Omega$ even though a subset of pixels (seeds) $S\\subset\\Omega$ have ground truth labels ${\\bar{y}}_{i}$ . We will simply impose a constraint that pseudo-labels and ground truth labels agree on $S$ . Thus, we assume the following set of pseudo-labels ", "page_idx": 2}, {"type": "equation", "text": "$$\nY_{\\Omega}:=\\{y_{i}\\in\\Delta^{K}\\mid i\\in\\Omega,\\mathrm{~s.t.~}y_{i}=\\bar{y}_{i}\\mathrm{~for~}i\\in S\\}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "We split the terms in (4) into two groups: one includes NLL and entropy $H$ terms keeping the original prediction variables $\\sigma_{i}$ and the other includes the Potts relaxation $P$ replacing $\\sigma_{i}$ with auxiliary variables $y_{i}$ . This transforms loss (4) into expression ", "page_idx": 2}, {"type": "equation", "text": "$$\n-\\sum_{i\\in S}\\ln\\sigma_{i}^{\\bar{y}_{i}}\\;+\\;\\eta\\sum_{i\\notin S}H(\\sigma_{i})\\;+\\;\\lambda\\sum_{i j\\in N}P(y_{i},y_{j})\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "81 equivalent to (4) assuming equality $\\sigma_{i}=y_{i}$ . The standard approximation is to incorporate constraint 82 $\\sigma_{i}\\approx\\,y_{i}$ directly into the loss, e.g. using $K L$ -divergence. For simplicity, we use weight $\\eta$ for $K L(\\sigma_{i},y_{i})$ to combine it with $H(\\sigma_{i})$ into a single cross-entropy term ", "page_idx": 2}, {"type": "equation", "text": "$$\n-\\sum_{i\\in S}\\ln\\sigma_{i}^{\\bar{y}_{i}}\\;+\\;\\underbrace{\\eta\\sum_{i\\notin S}H(\\sigma_{i})\\;+\\;\\eta\\sum_{i\\notin S}K L(\\sigma_{i},y_{i})}_{\\eta\\sum_{i\\notin S}H(\\sigma_{i},y_{i})}\\;+\\;\\lambda\\sum_{i j\\in\\mathcal{N}}P(y_{i},y_{j})\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "84 defining joint self-labeling loss for both predictions $\\sigma_{i}$ and pseudo-labels $y_{i}$ ", "page_idx": 2}, {"type": "equation", "text": "$$\nL_{s e l f}(\\sigma,y)\\;:=\\;\\;-\\sum_{i\\in S}\\ln\\sigma_{i}^{\\bar{y}_{i}}\\;+\\;\\eta\\sum_{i\\notin S}H(\\sigma_{i},y_{i})\\;+\\;\\lambda\\sum_{i j\\in N}P(y_{i},y_{j})\n$$", "text_format": "latex", "page_idx": 2}, {"type": "table", "img_path": "V5Sbh42uDe/tmp/6c1c29767078d58b7e6b6c9272ca63100090fe3a3a7931253624d87e96a32b38.jpg", "table_caption": [], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "Table 1: Second-order Potts relaxations, see Fig.1(a,b,c) ", "page_idx": 3}, {"type": "text", "text": "85 approximating the original weakly supervised loss (4). ", "page_idx": 3}, {"type": "text", "text": "86 Iterative minimization of this loss w.r.t. predictions $\\sigma_{i}$ (model parameters training) and pseudo  \n87 labels $y_{i}$ effectively breaks the original optimization problem for (4) into two simpler sub-problems,   \n88 assuming there is a good solver for optimal pseudo-labels. The latter seems plausible since the unary   \n89 term $H(\\sigma_{i},y_{i})$ is convex for $y_{i}$ and the Potts relaxations were widely studied in image segmentation   \n90 for decades.   \n91 Section 2.1 discusses standard and new relaxations of the Potts model $P$ . Section 2.2 discusses several   \n92 robust variants of cross-entropy $H$ for connecting predictions with uncertain (soft) pseudo-labels $y_{i}$   \n93 estimated for unlabeled points $i\\in\\Omega\\backslash S$ . Appendix B proposes an efficient general solver for the   \n94 corresponding pseudo-labeling sub-problems. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "95 2.1 Second-order relaxations of the Potts model ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "96 We focus on second-order relaxations for two reasons. First, to manage the scope of this study.   \n97 Second, this includes several important baseline cases (see Table 1): quadratic, the simplest convex   \n98 relaxation popularized by the random walker algorithm [17], and bi-linear, which is non-convex but   \n99 tight [33] w.r.t. the original discrete Potts model. The latter implies that optimizing it over relaxed   \n100 variables will lead to a solution consistent with a discrete Potts solver, e.g. graph cut [6]. On the   \n101 contrary, the quadratic relaxation will produce a significantly different soft solution. We investigate   \n102 such soft solutions. ", "page_idx": 3}, {"type": "text", "text": "Figure 2 shows two examples illustrating local minima for (a) the bi-linear and (b) quadratic relaxations of the Potts loss. In (a) two neighboring pixels attempt to jointly change the common soft label from $y_{i}=y_{j}=(1,0,0)$ to $y_{i}^{\\prime\\prime}=y_{j}^{\\prime\\prime}=(0,1,0)$ , which corresponds to a \u201cmove\u201d where the whole object is reclassified from A to B. This move does not violate smoothness within the region represented by the Potts model. But, the soft intermediate state $y_{i}^{\\prime}=y_{j}^{\\prime}=(\\textstyle{\\frac{1}{2}},\\textstyle{\\frac{1}{2}},0)$ will prevent this move in bi-linear case ", "page_idx": 3}, {"type": "equation", "text": "$$\nP_{\\mathrm{{BL}}}(y_{i}^{\\prime},y_{j}^{\\prime})=\\frac{1}{2}\\;\\;>\\;\\;0=P_{\\mathrm{{BL}}}(y_{i},y_{j})=P_{\\mathrm{{BL}}}(y_{i}^{\\prime\\prime},y_{j}^{\\prime\\prime})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "while quadratic relaxation assigns zero loss for all states during this move. On the other hand, the example in Figure $2({\\mathsf{b}})$ shows a move problematic for the quadratic relaxation. Two neighboring pixels have labels $y_{i}=(1,0,0)$ and $y_{j}=(0,0,1)$ corresponding to the boundary of objects A and C. The second object attempts to change from $\\mathbf{C}$ to B. This move does not affect the discontinuity between two pixels, but quadratic relaxation prefers that the second object is stuck in the intermediate state $y_{j}^{\\prime}=(0,{\\frac{1}{2}},{\\frac{1}{2}})$ ", "page_idx": 3}, {"type": "equation", "text": "$$\nP_{0}(y_{i},y_{j}^{\\prime})=\\frac{3}{4}\\;\\;<\\;\\;1=P_{0}(y_{i},y_{j})=P_{0}(y_{i},y_{j}^{\\prime\\prime})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "103 while bi-linear relaxation $P_{\\mathrm{BL}}(y_{i},y_{j})=1$ remains constant as $y_{j}$ changes. ", "page_idx": 3}, {"type": "text", "text": "104 We propose a new relaxation, normalized quadratic in Table 1. Normalization leads to equivalence   \n105 between quadratic and bi-linear formulations combining their beneftis. As easy to check, normalized   \n106 quadratic relaxation $P_{\\mathrm{NQ}}$ does not have local minima in both examples of Figure 2. Table 2 also   \n107 proposes \u201clogarithmic\u201d versions of the relaxations in Table 1 composing them with function \u2212 $\\ln(1-$   \n108 $x^{\\ '}$ ). As illustrated by Figure 1, the logarithmic versions in (d-f) addresses the \u201cvanishing gradients\u201d   \n109 evident in (a-c). ", "page_idx": 3}, {"type": "table", "img_path": "V5Sbh42uDe/tmp/6dd25827de06df2cc312943f8d4b863e4d15567abb3b19dbf878dd6d821143b1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 3}, {"type": "image", "img_path": "V5Sbh42uDe/tmp/bdbb6d9fd9afcbe149d60fc519fe34d9b2bd623b3a79f6438b3a0053724c3477.jpg", "img_caption": ["Figure 1: Second-order Potts relaxations in Tables 1 and 2: interaction potentials $P$ for pairs of predictions $(\\sigma_{i},\\sigma_{j})$ in (4) or pseudo-labels $(y_{i},y_{j})$ in (6) are illustrated for $K\\,=\\,2$ when each prediction $\\sigma_{i}$ or label $y_{i}$ , i.e. distribution in $\\Delta^{2}$ , can be represented by a single scalar as $(x,1-x)$ . The contour maps are iso-levels of $P((x_{i},1-x_{i}),(x_{j},1-x_{j}))$ over domain $(x_{i},x_{j})\\in[0,1]^{2}$ . The 3D plots above illustrate the potentials $P$ as functions over pairs of \u201clogits\u201d $(l_{i},l_{j})\\in\\mathbb{R}^{2}$ where each scalar logit $l_{i}$ defines binary distribution $(x_{i},1-x_{i})$ for $\\begin{array}{r}{x_{i}=\\frac{1}{1+e^{-2l_{i}}}\\in[0,1]}\\end{array}$ . "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "image", "img_path": "V5Sbh42uDe/tmp/0f93e7a4d79f337e1577c6b420a6aee21520572ca1de51b372c2941445d44b03.jpg", "img_caption": ["(a) both pixels change $(\\mathbf{A}\\to\\mathbf{B})$ ) "], "img_footnote": [], "page_idx": 4}, {"type": "image", "img_path": "V5Sbh42uDe/tmp/0488cdbf9df8e6efb97b62ad70901ba9798e9665aadbef9dfe4ad778a942a9f6.jpg", "img_caption": ["(b) only pixel $q$ changes $\\left(\\mathbf{C}\\rightarrow\\mathbf{B}\\right)$ ) "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 2: Examples of \"moves\" for neighboring pixels $\\{i,j\\}\\in\\mathcal{N}$ . Their (soft) pseudo-labels $y_{i}$ and $y_{j}$ are illustrated on the probability simplex $\\breve{\\Delta}^{\\dot{K}}$ for $K=3$ . In (a) both pixels $i$ and $j$ are inside a region/object changing its label from A to B. In (b) pixels $i$ and $j$ are on the boundary between two regions/objects; one is fixed to class A and the other changes from class $\\mathbf{C}$ to B. ", "page_idx": 4}, {"type": "text", "text": "110 2.2 Cross-entropy and soft pseudo-labels ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "111 Shannon\u2019s cross-entropy $H(y,\\sigma)$ is the most common loss for training network predictions $\\sigma$ from   \n112 ground truth labels $y$ in the context of classification, semantic segmentation, etc. However, this loss   \n113 may not be ideal for applications where the targets $y$ are soft categorical distributions representing   \n114 various forms of class uncertainty. For example, this paper is focused on scribble-based segmentation   \n115 where the ground truth is not known for most of the pixels, and the network training is done jointly   \n116 with estimating pseudo-labels $y$ for the unlabeled pixels. In this case, soft labels $y$ are distributions   \n117 representing class uncertainty. We observe that if such $y$ is used as a target in $H(y,\\sigma)$ , the network   \n118 is trained to reproduce the uncertainty, see Figure 3(a). This motivates the discussion of alternative   \n119 \u201ccross-entropy\u201d functions where the quotes indicate an informal interpretation of this information  \n120 theoretic concept. Intuitively, such functions should encourage decisiveness, as well as proximity ", "page_idx": 4}, {"type": "image", "img_path": "V5Sbh42uDe/tmp/7cda96445c03fc8761795185ac01c2be2f5c317a87cd5046984970535970b5d6.jpg", "img_caption": ["(a) standard $H_{\\mathrm{CE}}(y,\\sigma)$ (b) reverse $H_{\\mathrm{RCE}}(y,\\sigma)$ (c) collision $H_{\\mathrm{CCE}}(y,\\sigma)$ (d) empirical comparison "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 3: Illustration of cross-entropy functions: (a) standard (7), (b) reverse (8), and (c) collision (9). (d) shows the empirical comparison on the robustness to label uncertainty. The test uses ResNet-18 architecture on fully-supervised Natural Scene dataset [30] where we corrupted some labels. The horizontal axis shows the percentage $\\eta$ of training images where the correct ground truth labels were replaced by a random label. All losses trained the model using soft target distributions $\\hat{y}=\\eta\\!*\\!u\\!+\\!(1\\!\\!-\\!\\eta)\\!*\\!y$ representing the mixture of one-hot distribution $y$ for the observed corrupt label and the uniform distribution $u$ , following [29]. The vertical axis shows the test accuracy. Training with the reverse and collision cross-entropy is robust to much higher levels of label uncertainty. ", "page_idx": 5}, {"type": "text", "text": "121 between the predictions and pseudo-labels, but avoid mimicking the uncertainty in both directions:   \n122 from soft pseudo-labels to predictions and vice-versa. We show that the last property can be achieved   \n123 in a probabilistically principled manner. The following three paragraphs discuss different cross  \n124 entropy functions that we study in the context of our self-labeling loss (6).   \n125 Standard cross-entropy provides the obvious baseline for evaluating two alternative versions that   \n126 follow. For completeness, we include its mathematical definition ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\nH_{\\mathrm{CE}}(y_{i},\\sigma_{i})\\;\\;=\\;\\;H(y_{i},\\sigma_{i})\\;\\;\\equiv\\;\\;-\\sum_{k}y_{i}^{k}\\ln\\sigma_{i}^{k}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "127 and remind the reader that this loss is primarily used with hard or one-hot labels, in which case it is   \n128 also equivalent to NLL loss \u2212 $\\ln\\sigma_{i}^{y_{i}}$ previously discussed for ground truth labels (3). As mentioned   \n129 earlier, Figure 3(a) shows that for soft pseudo-labels like $y=(0.5,0.5)$ , it forces predictions to mimic   \n130 or replicate the uncertainty $\\sigma\\approx y$ . In fact, label $y=(0.5,0.5)$ just tells that the class is unknown   \n131 and the network should not be supervised by this point. This problem manifests itself in the poor   \n132 performance of the standard cross-entropy (7) in our experiment discussed in Figure 3 (d) (red curve). ", "page_idx": 5}, {"type": "text", "text": "133 Reverse cross-entropy switches the order of the label and prediction in (7) ", "page_idx": 5}, {"type": "equation", "text": "$$\nH_{\\mathrm{RCE}}(y_{i},\\sigma_{i})\\;\\;=\\;\\;H(\\sigma_{i},y_{i})\\;\\;\\equiv\\;\\;-\\sum_{k}\\sigma_{i}^{k}\\ln y_{i}^{k}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "134 which is not too common. Indeed, Shannon\u2019s cross-entropy is not symmetric and the first argument   \n135 is normally the target distribution and the second is the estimated distribution. However, in our   \n136 case, both distributions are estimated and there is no reason not to try the reverse order. It is worth   \n137 noting that our self-labeling formulation (6) suggests that reverse cross-entropy naturally appears   \n138 when the ADM approach splits the decisiveness and fairness into separate sub-problems. Moreover,   \n139 as Figure 3(b) shows, in this case, the network does not mimic uncertain pseudo-labels, e.g. the   \n140 gradient of the blue line is zero. The results for the reverse cross-entropy in Figure 3 (d) (green)   \n141 are significantly better than for the standard (red). Unfortunately, now pseudo-labels $y$ mimic the   \n142 uncertainty in predictions $\\sigma$ . ", "page_idx": 5}, {"type": "text", "text": "143 Collision cross-entropy resolves the problem in a principled way. We define it as ", "page_idx": 5}, {"type": "equation", "text": "$$\nH_{\\mathrm{CCE}}(y_{i},\\sigma_{i})\\;\\;\\equiv\\;\\;-\\ln\\sum_{k}\\sigma_{i}^{k}y_{i}^{k}\\;\\;\\equiv\\;\\;-\\ln\\sigma^{\\top}y\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "which is symmetric w.r.t. pseudo-labels and predictions. The dot product $\\sigma^{\\top}y$ can be seen as a probability that random variables represented by the distribution $\\sigma$ , the prediction class $C$ , and the distribution $y$ , the unknown true class $T$ , are equal. Indeed, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}(C=T)=\\sum_{k}P r(C=k)\\operatorname*{Pr}(T=k)=\\sigma^{\\top}y.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "144 Loss (9) maximizes this \u201ccollision\u201d probability rather than the constraint $\\sigma=y$ . Figure 3(c) shows no   \n145 mimicking of uncertainty (blue line). However, unlike reverse cross-entropy, this is also valid when $y$   \n146 is estimated from uncertain predictions $\\sigma$ since (9) is symmetric. This leads to the best performance   \n147 in Figure 3 (d) (blue). Our extensive experiments are conclusive that collision cross-entropy is the   \n148 best option for $H$ in self-labeling loss (6). ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "149 3 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "150 We conducted comprehensive experiments to demonstrate the choice of each element (cross-entropy,   \n151 pairwise term, and neighborhood) in the loss and compare our method to the state-of-the-art. In   \n152 Section 3.1, quantitative results are shown to compare different Potts relaxations. The qualitative   \n153 examples are shown in Figure 7. Then we compare several cross-entropy terms in Section 3.2.   \n154 Besides, we also compare our soft self-labeling approach on the nearest and dense neighborhood   \n155 systems in Section 3.3. We summarized the results in Section 3.4. In the last section, we show that   \n156 our method achieves the SOTA and even can outperform the fully-supervised method. More details   \n157 on the dataset, implementation, and additional experiments are given in Appendix C. ", "page_idx": 6}, {"type": "text", "text": "158 3.1 Comparison of Potts relaxations ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "159 To compare different Potts relaxations under   \n160 choose one cross-entropy term. Motivated   \n161 shown in Section 3.2, we use $H_{\\mathrm{CCE}}$ . The neighbor  \n162 hood system is the nearest neighbors. The quanti  \n163 tative results are in Table 3. First, One can see that   \n164 the pairwise terms with logarithm are better than   \n165 those without the logarithm because the logarithm   \n166 may help with the gradient vanishing problem in   \n167 softmax operation. Moreover, the logarithm does   \n168 not like abrupt change across the boundaries, so the   \n169 transition across the boundaries is smoother (see   \n170 Figure 7 in the appendix.). Note that it is reasonable   \n171 to have higher uncertainty around the boundaries.   \n172 Second, the results prefer the normalized version, ", "page_idx": 6}, {"type": "text", "text": "the self-labeling framework, we need to by the properties and empirical results ", "page_idx": 6}, {"type": "table", "img_path": "V5Sbh42uDe/tmp/7b5159f6a01151b5b5f1cf3127414fadc43a680335188ad150716f5f0342b7f7.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Table 3: Comparison of Potts relaxations with self-labeling. mIoUs on validation set are shown here. ", "page_idx": 6}, {"type": "text", "text": "173 which confirms the points made in Figure 2. Third, the simplest quadratic formulation $P_{\\mathrm{Q}}$ can be a   \n174 fairly good starting point to obtain decent results. Additionally, we specifically test $H_{0}+P_{0}$ due to   \n175 the existing closed-form solution [1, 17]. Since the pseudo-labels generated from this formula tend to   \n176 be overly soft, we explicitly add entropy terms during the training of network parameters and the   \n177 mIoU goes up to $68.97\\%$ from $67.8\\%$ . ", "page_idx": 6}, {"type": "text", "text": "178 3.2 Comparison of cross-entropy terms ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "179 In this section, we compare different cross-entropy terms while fixing the pairwise term to   \n180 $P_{\\mathrm{Q}}$ due to its simplicity and using the nearest neighborhood system. The results are shown   \n181 in Figure 4. One can see that $H_{\\mathrm{CCE}}$ performs the   \n182 best consistently across different supervision levels,   \n183 i.e. scribble lengths. Both $H_{\\mathrm{CCE}}$ and $H_{\\mathrm{RCE}}$ are con  \n184 sistently better than standard $H_{\\mathrm{CE}}$ with a noticeable   \n185 margin because they are more robust, as explained in   \n186 Section 2.2, to the uncertainty in soft pseudo-labels   \n187 when optimizing network parameters. We also test the   \n188 performance of using $H_{\\mathrm{CCE}}+P_{\\mathrm{Q}}$ with hard pseudo  \n189 labels obtained via the argmax operation on the soft   \n190 ones. The mIoU on the validation set is $69.8\\%$ under   \n191 the full scribble-length supervision. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "image", "img_path": "V5Sbh42uDe/tmp/66c72ac9b1271aa8c58d064293c0266bb9335c4838957b70f443c7369f2e6869.jpg", "img_caption": ["Figure 4: Comparison of cross-entropy terms. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "192 3.3 Comparison of neighborhood systems ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "193 Until now, we only used the four nearest neighbors for the pairwise term. In this section, we also use   \n194 the dense neighborhood and compare the results under the self-labeling framework.   \n195 Firstly, to optimize the pseudo-labels for the dense neighborhood, we still use the gradient descent   \n96 technique as detailed in Appendix B. The gradient computation employs the bilateral filtering   \n97 technique following [35]. For the pairwise term, we use $P_{\\mathrm{Q}}$ . The cross-entropy term is $H_{\\mathrm{CCE}}$ . Note   \n98 that the bilateral filtering technique only supports quadratic pairwise terms, i.e. $P_{\\mathrm{BL}}$ and $P_{\\mathrm{Q}}$ . Since   \n99 $P_{\\mathrm{BL}}$ leads to hard solutions, $P_{\\mathrm{Q}}$ is the only practical choice for soft self-labeling. We obtained $71.1\\%$   \n200 mIoU on nearest neighbors while only getting $67.9\\%$ on dense neighborhoods (bandwidth is 100).   \n201 Some qualitative results are shown in Figure 5. Clearly from this figure one can see that a larger   \n02 neighborhood size induces lower-quality pseudo-labels. A possible explanation is that the Potts   \n203 model gets closer to cardinality/volume potentials when the neighborhood size becomes larger [37].   \n204 The nearest neighborhood is better for edge alignment and thus produces cleaner results. ", "page_idx": 6}, {"type": "image", "img_path": "V5Sbh42uDe/tmp/62649b3be7e8eb7a5bcd975e10c3bb30007e3f2f4eeaae8c550093dc0f005a22.jpg", "img_caption": ["Figure 5: Pseudo-labels generated from given network predictions using different neighborhoods. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "205 3.4 Soft self-labeling vs. hard self-labeling vs. gradient descent ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "206 In this section, we give a summary in Table 4 as to what is the best framework for   \n207 the WSSS based on losses regularized by the Potts model. Firstly, to directly optimize ", "page_idx": 7}, {"type": "text", "text": "the network parameters via stochastic gradient descent on the regularized loss, one needs a larger neighborhood size. One possible explanation is that a larger neighborhood size induces a smoother Potts model and it helps the gradient descent [28]. However, larger neighborhood size is not preferred in the self-labeling framework. If we use Potts model on nearest neighborhoods, the self-labeling optimization should be applied and one should use ", "page_idx": 7}, {"type": "table", "img_path": "V5Sbh42uDe/tmp/416e8ed20c24d5a67fd926bcd737deaad7e64e3c095d88722dfcc743a573231f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 4: Summary of comparisons. \u201c\u2217\u201d stands for the reproduced results from their code repository. ", "page_idx": 7}, {"type": "text", "text": "217 soft pseudo-labels instead of hard ones. Note that with proper optimization the advantage of the Potts   \n218 model on small neighborhood size can show up. In Figure 6, we also compare these approaches   \n219 across different scribble lengths.   \n221 In this section, we use a different network architec  \n222 ture, ResNet101, to fairly compare our method with   \n223 the current state-of-the-art. We only compare the   \n224 results before applying any post-processing steps.   \n225 The results are shown in Table 5. Note that our   \n226 results can outperform the fully-supervised method   \n227 when using 12 as the batch size. We also observe   \n228 that a larger batch size usually improves the results   \n229 quite a lot. Our results with 12 batch size can out  \n230 perform several SOTA methods which use 16 batch   \n231 size. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "image", "img_path": "V5Sbh42uDe/tmp/415bd521fc932423f4721a3da0d2c1322e67ab9d1ef137a7a3e3bbae3c09e976.jpg", "img_caption": ["Figure 6: Comparison of different methods using Potts relaxations. The architecture is Deeplab ${\\mathrm{V}}3+$ with the backbone MobileNetV2. "], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "V5Sbh42uDe/tmp/809f208829b9a334a6e246a1897c05885f875fb3ac09eea5081997969e685ff6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 5: Comparison to SOTA methods (without CRF postprocessing) on scribble-supervised segmentation. The numbers are mIoU on the validation dataset of Pascal VOC 2012 and use full-length scribble. The backbone is ResNet101 unless stated otherwise. V2: deeplabV2. $\\mathrm{V}3+$ : deeplab ${\\mathrm{V}}3+$ . $\\mathcal{N}$ : neighborhood. $\\omega_{\\ast},$ : reproduced results. GD: gradient descent. SL: self-labeling. \u201cno pretrain\u201d means the segmentation network is not pretrained using cross-entropy on scribbles. ", "page_idx": 8}, {"type": "text", "text": "232 4 Conclusions ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "233 This paper proposed a convergent soft self-labeling framework based on a simple well-motivated loss   \n234 (6) for joint optimization of network predictions and soft pseudo-labels. The latter were motivated   \n235 as auxiliary optimization variables simplifying optimization of weakly-supervised loss (4). Our   \n236 systematic evaluation of the cross-entropy and the Potts terms in self-labeling loss (6) provides   \n237 clear recommendations based on the discussed conceptual advantages empirically confirmed by our   \n238 experiments. Specifically, our work recommends the collision cross-entropy, log-quadratic Potts   \n239 relaxations, and the earest-neighbor neighborhood. They achieve the best result that may even   \n240 outperform the fully-supervised method with full pixel-precise masks. Our method does not require   \n241 any modifications of the semantic segmentation models and it is easy to reproduce. Our general   \n242 framework and empirical findings can be useful for other weakly-supervised segmentation problems   \n243 (boxes, class tags, etc.). ", "page_idx": 8}, {"type": "text", "text": "244 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "245 [1] Multilabel random walker image segmentation using prior models. In 2005 IEEE computer   \n246 society conference on computer vision and pattern recognition (CVPR\u201905), pages 763\u2013770.   \n247 IEEE, 2005.   \n248 [2] Jiwoon Ahn and Suha Kwak. Learning pixel-level semantic affinity with image-level supervision   \n249 for weakly supervised semantic segmentation. In Proceedings of the IEEE conference on   \n250 computer vision and pattern recognition, pages 4981\u20134990, 2018.   \n251 [3] Jiwoon Ahn, Sunghyun Cho, and Suha Kwak. Weakly supervised learning of instance seg  \n252 mentation with inter-pixel relations. In Proceedings of the IEEE/CVF conference on computer   \n253 vision and pattern recognition, pages 2209\u20132218, 2019.   \n254 [4] Nikita Araslanov and Stefan Roth. Single-stage semantic segmentation from image labels. In   \n255 Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages   \n256 4253\u20134262, 2020.   \n257 [5] Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press,   \n258 2004.   \n259 [6] Yuri Y Boykov and M-P Jolly. Interactive graph cuts for optimal boundary & region segmenta  \n260 tion of objects in nd images. In Proceedings eighth IEEE international conference on computer   \n261 vision. ICCV 2001, pages 105\u2013112. IEEE, 2001.   \n262 [7] John Bridle, Anthony Heading, and David MacKay. Unsupervised classifiers, mutual informa  \n263 tion and\u2019phantom targets. Advances in neural information processing systems, 4, 1991.   \n264 [8] Antonin Chambolle and Thomas Pock. A first-order primal-dual algorithm for convex problems   \n265 with applications to imaging. Journal of mathematical imaging and vision, 40:120\u2013145, 2011.   \n266 [9] Hongjun Chen, Jinbao Wang, Hong Cai Chen, Xiantong Zhen, Feng Zheng, Rongrong Ji, and   \n267 Ling Shao. Seminar learning for click-level weakly supervised semantic segmentation. In   \n268 Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6920\u20136929,   \n269 2021.   \n270 [10] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille.   \n271 Semantic image segmentation with deep convolutional nets and fully connected crfs. arXiv   \n272 preprint arXiv:1412.7062, 2014.   \n273 [11] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille.   \n274 Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and   \n275 fully connected crfs. IEEE transactions on pattern analysis and machine intelligence, 40(4):   \n276 834\u2013848, 2017.   \n277 [12] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam.   \n278 Encoder-decoder with atrous separable convolution for semantic image segmentation. In   \n279 Proceedings of the European conference on computer vision (ECCV), pages 801\u2013818, 2018.   \n280 [13] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo   \n281 Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic   \n282 urban scene understanding. In Proc. of the IEEE Conference on Computer Vision and Pattern   \n283 Recognition (CVPR), 2016.   \n284 [14] Camille Couprie, Leo Grady, Laurent Najman, and Hugues Talbot. Power watershed: A   \n285 unifying graph-based optimization framework. IEEE transactions on pattern analysis and   \n286 machine intelligence, 33(7):1384\u20131399, 2010.   \n287 [15] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large  \n288 scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern   \n289 recognition, pages 248\u2013255. Ieee, 2009.   \n290 [16] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman.   \n291 The pascal visual object classes (voc) challenge. International journal of computer vision, 88:   \n292 303\u2013308, 2009.   \n293 [17] Leo Grady. Random walks for image segmentation. IEEE transactions on pattern analysis and   \n294 machine intelligence, 28(11):1768\u20131783, 2006.   \n295 [18] Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization.   \n296 Advances in neural information processing systems, 17, 2004.   \n297 [19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image   \n298 recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,   \n299 pages 770\u2013778, 2016.   \n300 [20] Tsung-Wei Ke, Jyh-Jing Hwang, and Stella X Yu. Universal weakly supervised segmentation   \n301 by pixel-to-segment contrastive learning. arXiv preprint arXiv:2105.00957, 2021.   \n302 [21] Alexander Kolesnikov and Christoph H Lampert. Seed, expand and constrain: Three principles   \n303 for weakly-supervised image segmentation. In Computer Vision\u2013ECCV 2016: 14th European   \n304 Conference, Amsterdam, The Netherlands, October 11\u201314, 2016, Proceedings, Part IV 14, pages   \n305 695\u2013711. Springer, 2016.   \n306 [22] Philipp Kr\u00e4henb\u00fchl and Vladlen Koltun. Efficient inference in fully connected CRFs with   \n307 Gaussian edge potentials. Advances in neural information processing systems, 24, 2011.   \n308 [23] V. Kulharia, S. Chandra, A. Agrawal, P. Torr, and A. Tyagi. Box2seg: Attention weighted loss   \n309 and discriminative feature learning for weakly supervised segmentation. In ECCV\u201920.   \n310 [24] Jungbeom Lee, Eunji Kim, Sungmin Lee, Jangho Lee, and Sungroh Yoon. Ficklenet: Weakly   \n311 and semi-supervised semantic image segmentation using stochastic inference. In Proceedings   \n312 of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5267\u20135276,   \n313 2019.   \n314 [25] Zhiyuan Liang, Tiancai Wang, Xiangyu Zhang, Jian Sun, and Jianbing Shen. Tree energy   \n315 loss: Towards sparsely annotated semantic segmentation. In Proceedings of the IEEE/CVF   \n316 Conference on Computer Vision and Pattern Recognition, pages 16907\u201316916, 2022.   \n317 [26] Di Lin, Jifeng Dai, Jiaya Jia, Kaiming He, and Jian Sun. Scribblesup: Scribble-supervised   \n318 convolutional networks for semantic segmentation. In Proceedings of the IEEE conference on   \n319 computer vision and pattern recognition, pages 3159\u20133167, 2016.   \n320 [27] Dmitrii Marin and Yuri Boykov. Robust trust region for weakly supervised segmentation. In   \n321 Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6608\u20136618,   \n322 2021.   \n323 [28] Dmitrii Marin, Meng Tang, Ismail Ben Ayed, and Yuri Boykov. Beyond gradient descent for   \n324 regularized segmentation losses. In Proceedings of the IEEE/CVF Conference on Computer   \n325 Vision and Pattern Recognition, pages 10187\u201310196, 2019.   \n326 [29] Rafael M\u00fcller, Simon Kornblith, and Geoffrey E Hinton. When does label smoothing help?   \n327 Advances in neural information processing systems, 32, 2019.   \n328 [30] NSD. Natural Scenes Dataset [NSD]. https://www.kaggle.com/datasets/   \n329 nitishabharathi/scene-classification, 2020.   \n330 [31] Zhiyi Pan, Peng Jiang, Yunhai Wang, Changhe Tu, and Anthony G Cohn. Scribble-supervised   \n331 semantic segmentation by uncertainty reduction on neural representation and self-supervision   \n332 on neural eigenspace. In Proceedings of the IEEE/CVF International Conference on Computer   \n333 Vision, pages 7416\u20137425, 2021.   \n334 [32] Thomas Pock, Antonin Chambolle, Daniel Cremers, and Horst Bischof. A convex relaxation   \n335 approach for computing minimal partitions. In IEEE Conference on Computer Vision and   \n336 Pattern Recognition, pages 810\u2013817, 2009.   \n337 [33] Pradeep Ravikumar and John Lafferty. Quadratic programming relaxations for metric labeling   \n338 and Markov Random Field MAP estimation. In The 23rd International Conference on Machine   \n339 Learning, page 737\u2013744, 2006.   \n340 [34] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen.   \n341 Mobilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference   \n342 on computer vision and pattern recognition, pages 4510\u20134520, 2018.   \n343 [35] Meng Tang, Abdelaziz Djelouah, Federico Perazzi, Yuri Boykov, and Christopher Schroers.   \n344 Normalized cut loss for weakly-supervised cnn segmentation. In Proceedings of the IEEE   \n345 conference on computer vision and pattern recognition, pages 1818\u20131827, 2018.   \n346 [36] Meng Tang, Federico Perazzi, Abdelaziz Djelouah, Ismail Ben Ayed, Christopher Schroers, and   \n347 Yuri Boykov. On regularized losses for weakly-supervised cnn segmentation. In Proceedings of   \n348 the European Conference on Computer Vision (ECCV), pages 507\u2013522, 2018.   \n349 [37] Olga Veksler. Efficient graph cut optimization for full crfs with quantized edges. IEEE   \n350 transactions on pattern analysis and machine intelligence, 42(4):1005\u20131012, 2019.   \n351 [38] Olga Veksler and Yuri Boykov. Sparse non-local crf. In Proceedings of the IEEE/CVF   \n352 Conference on Computer Vision and Pattern Recognition, pages 4493\u20134503, 2022.   \n353 [39] Bin Wang, Guojun Qi, Sheng Tang, Tianzhu Zhang, Yunchao Wei, Linghui Li, and Yongdong   \n354 Zhang. Boundary perception guidance: A scribble-supervised semantic segmentation approach.   \n355 In IJCAI International joint conference on artificial intelligence, 2019.   \n356 [40] Changwei Wang, Rongtao Xu, Shibiao Xu, Weiliang Meng, and Xiaopeng Zhang. Treating   \n357 pseudo-labels generation as image matting for weakly supervised semantic segmentation. In   \n358 Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 755\u2013765,   \n359 2023.   \n360 [41] Jingshan Xu, Chuanwei Zhou, Zhen Cui, Chunyan Xu, Yuge Huang, Pengcheng Shen, Shaoxin   \n361 Li, and Jian Yang. Scribble-supervised semantic segmentation inference. In Proceedings of the   \n362 IEEE/CVF International Conference on Computer Vision, pages 15354\u201315363, 2021.   \n363 [42] Christopher Zach, Christian H\u00e4ne, and Marc Pollefeys. What is optimized in tight convex   \n364 relaxations for multi-label problems? In IEEE Conference on Computer Vision and Pattern   \n365 Recognition, pages 1664\u20131671, 2012.   \n366 [43] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba.   \n367 Scene parsing through ade20k dataset. In Proceedings of the IEEE conference on computer   \n368 vision and pattern recognition, pages 633\u2013641, 2017.   \n369 [44] Xiaojin Zhu and Zoubin Ghahramani. Learning from labeled and unlabeled data with label   \n370 propagation. ProQuest Number: INFORMATION TO ALL USERS, 2002. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "371 A Self-labeling and hard pseudo-labels ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "372 One argument motivating self-labeling approaches to weakly-supervised segmentation comes from   \n373 well-known limitations of gradient descent when optimizing the Potts relaxatons, e.g. [28]. But even   \n374 when using convex Potts relaxations [17, 32, 8], they are combined with the concave entropy term in   \n375 (4) making their optimization challenging.   \n376 Typical self-labeling methods, including one of the first works on scribble-based semantic segmenta  \n377 tion [26], introduce a sub-problem focused on the estimation of pseudo-labels over unlabeled points,   \n378 separately from the network training by such labels. Pseudo-labeling is typically done by optimiza  \n379 tion algorithms or heuristics balancing unsupervised or self-supervised criteria, e.g. the Potts, and   \n380 proximity to current predictions. Then, network fine-tuning from pseudo-labels and pseudo-labeling   \n381 steps are iterated.   \n382 We denote pseudo-labels $y_{i}$ slightly differently from the ground truth labels ${\\bar{y}}_{i}$ by omitting the   \n383 bar. It is important to distinguish them since the ground truth labels $\\bar{y}_{i}$ for $i\\in S$ are given, while   \n384 the pseudo-labels $y_{i}$ for $i\\in\\Omega\\backslash S$ are estimated. The majority of existing self-labeling methods   \n385 [26, 2, 28, 3, 24, 27, 40] estimate hard pseudo-labels, which could be equivalently represented either   \n386 by class indices ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "equation", "text": "$$\ny_{i}\\in\\{1,\\ldots,K\\}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "387 or by the corresponding one-hot categorical distributions ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{y_{i}}&{\\equiv}&{\\big(y_{i}^{1},\\ldots,y_{i}^{K}\\big)\\,\\in\\Delta_{0,1}^{K}\\qquad\\quad\\mathrm{for}\\quad y_{i}^{k}:=[k=y_{i}]\\,\\in\\{0,1\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "388 analogously with the hard ground truth labels in (1) and (2). In part, hard pseudo-labels are motivated   \n389 by the network training where the default is NLL loss (3) assuming discrete labels. Besides, there are   \n390 powerful discrete solvers for the Potts model [6, 32, 8]. We discuss the potential advantages of soft   \n391 pseudo-labels in the next Section 1.2.   \n392 Joint loss vs \u201cproposal generation\u201d: The majority of self-labeling approaches can be divided into   \n393 two groups. One group designs pseudo-labeling and the network training sup-problems that are not   \n394 formally related, e.g. [26, 25, 41]. While pseudo-labeling typically depends on the current network   \n395 predictions and the network fine-tuning uses such pseudo-labels, the lack of a formal relation between   \n396 these sub-problems implies that iterating such steps does not guarantee any form of convergence.   \n397 Such methods are often referred to as proposal generation heuristics.   \n398 Alternatively, the pseudo-labeling sub-problem and the network training sub-problem can be formally   \n399 derived from a weakly-supervised loss like (4), e.g. by ADM splitting [28] or as high-order trust  \n400 region method [27]. Such methods often formulate a joint loss function w.r.t network predictions   \n401 and pseudo-labels and iteratively optimize it in a convergent manner that is guaranteed to decrease   \n402 the loss. We consider this group of self-labeling methods as better motivated, more principled, and   \n403 numerically safer. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "404 B Optimization Algorithm ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "405 In this section, we will focus on the optimization of (6) in steps iterating optimization of $y$ and $\\sigma$ .   \n406 The network parameters are optimized by standard stochastic gradient descent in all our experiments.   \n407 Pseudo-labels are also estimated online using a mini-batch. To solve $y$ at given $\\sigma$ , it is a large-scale   \n408 constrained convex problem. While there are existing general solvers to find global optima, such   \n409 as projected gradient descent, it is often too slow for practical usage. Instead, we reformulate our   \n410 problem to avoid the simplex constraints so that we can use standard gradient descent in PyTorch   \n411 library accelerated by GPU. Specifically, instead of directly optimizing $y$ , we optimize a set of new   \n412 variables $\\{l_{i}\\in\\mathbb{R}^{K},\\dot{i}\\in\\Omega\\}$ where $y_{i}$ is computed by $s o f t m a x(l_{i})$ . Now, the simplex constraint on   \n413 $y$ will be automatically satisfied. Note that the hard constraints on scribble regions still need to be   \n414 considered because the interaction with unlabeled regions through pairwise terms will influence the   \n415 optimization process. Inspired by [44], we can reset $s o f t m a x(l_{i})$ where $i\\in S$ back to the ground   \n416 truth at the beginning of each step of the gradient descent.   \n417 However, the original convex problem now becomes non-convex due to the Softmax operation. Thus,   \n418 initialization is important to help find better local minima or even the global optima. Empirically, we   \n419 observed that the network output logit can be a fairly good initialization. The quantitative comparison   \n420 uses a special quadratic formulation where closed-form solution and efficient solver [1, 17] exist.   \n421 We compute the standard soft Jaccard index for the pseudo-labels between the solutions given by   \n422 our solver and the global optima. The soft Jaccard index is $99.2\\%$ on average over 100 images.   \n423 Furthermore, our experimental results for all other formulations in Figure 7, 5, and Section 3 confirm   \n424 the effectiveness of our optimization solver. In all experiments, the number of gradient descent steps   \n425 for solving $y$ is 200 and the corresponding learning rate is 0.075. To test the robustness of the number   \n426 of steps here, we decreased 200 to 100 and the mIoU on the validation set just dropped from 71.05   \n427 by 0.72. This indicates that we can significantly accelerate the training without much sacrifice of   \n428 accuracy. When using 200 steps, the total time for the training will be about 3 times longer than the   \n429 SGD with dense Potts [36]. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "image", "img_path": "V5Sbh42uDe/tmp/17c39d3d630fb72fd20fa53c44a53b9ab26fbd4615fd64ef3348362184797df9.jpg", "img_caption": ["Figure 7: Illustration of the difference among Potts relaxations. The visualization of soft pseudolabels uses the convex combination of RGB colors for each class weighted by pseudo-label itself. "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "430 C Experimental settings ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "431 Dataset and evaluation We mainly use the standard PASCAL VOC 2012 dataset [16] and scribble  \n432 based annotations for supervision [26]. The dataset contains 21 classes including background.   \n433 Following the common practice [10, 35, 36], we use the augmented version which has 10,582 training   \n434 images and 1449 images for validation. We employ the standard mean Intersection-over-Union   \n435 (mIoU) on validation set as the evaluation metric. We also test our method on two additional datasets   \n436 in Section 3.5. One is Cityscapes [13] which is built for urban scenes and consists of 2975 and 500   \n437 fine-labeled images for training and validation. There are 19 out of 30 annotated classes for semantic   \n438 segmentation. The other one is ADE20k [43] which has 150 fine-grained classes. There are 20210   \n439 and 2000, images for training and validation. Instead of scribble-based supervision, we followed [25]   \n440 to use the block-wise annotation as a form of weak supervision.   \n441 Implementation details We adpoted DeepLabv $^{3+}$ [12] framework with two backbones, ResNet101   \n442 [19] and MobileNetV2 [34]. We use ResNet101 in Section 3.5, and use MobileNetV2 in other   \n443 sections for efficiency. All backbone networks (ResNet-101 and MobileNetV2) are pre-trained on   \n444 Imagenet [15]. Unless stated explicitly, we use batch 12 as the default across all the experiments.   \n445 Following [35], we adopt two-stage training, unless otherwise stated, where only the cross-entropy   \n446 loss on scribbles is used in the first stage. The optimizer for network parameters is SGD. The learning   \n447 rate is scheduled by a polynomial decay with a power of 0.9. Initial learning is set to 0.007 in the first   \n448 stage and 0.0007 in the second phase. 60 epochs are used to train the model with different losses   \n449 where hyperparameters are tuned separately for them. For our best result, we use $\\eta=0.3,\\lambda=6$ ,   \n450 $H_{\\mathrm{CCE}}$ and $P_{\\mathrm{CD}}$ . The color intensity bandwidth in the Potts model is set to 9 across all the experiments   \n451 on Pascal VOC 2012 and 3 for Cityscapes and ADE20k datasets.   \n453 The checklist is designed to encourage best practices for responsible machine learning research,   \n454 addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove   \n455 the checklist: The papers not including the checklist will be desk rejected. The checklist should   \n456 follow the references and follow the (optional) supplemental material. The checklist does NOT count   \n457 towards the page limit.   \n458 Please read the checklist guidelines carefully for information on how to answer these questions. For   \n459 each question in the checklist:   \n460 \u2022 You should answer [Yes] , [No] , or [NA] .   \n461 \u2022 [NA] means either that the question is Not Applicable for that particular paper or the   \n462 relevant information is Not Available.   \n463 \u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA).   \n464 The checklist answers are an integral part of your paper submission. They are visible to the   \n465 reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it   \n466 (after eventual revisions) with the final version of your paper, and its final version will be published   \n467 with the paper.   \n468 The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation.   \n469 While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a   \n470 proper justification is given (e.g., \"error bars are not reported because it would be too computationally   \n471 expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering   \n472 \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we   \n473 acknowledge that the true answer is often more nuanced, so please just use your best judgment and   \n474 write a justification to elaborate. All supporting evidence can appear either in the main paper or the   \n475 supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification   \n476 please point to the section(s) where related material for the question can be found. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "table", "img_path": "V5Sbh42uDe/tmp/92eaab48acbdbc0144994e946ffdc5691f56e95e179537bf83c11854ec522100.jpg", "table_caption": [], "table_footnote": ["Table 6: Comparison to SOTA methods (without CRF postprocessing) on segmentation with blockscribble supervision. The numbers are mIoU on the validation dataset of cityscapes [13] and ADE20k [43] and use $50\\%$ of full annotations for supervision following [25]. The backbone is ResNet101. $\\omega_{\\ast},$ : reproduced results. All methods are trained in a single-stage fashion. "], "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "477 IMPORTANT, please: ", "page_idx": 15}, {"type": "text", "text": "478 \u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\",   \n479 \u2022 Keep the checklist subsection headings, questions/answers and guidelines below.   \n480 \u2022 Do not modify the questions and only use the provided macros for your answers.   \n481 1. Claims   \n482 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n483 paper\u2019s contributions and scope?   \n484 Answer: [Yes]   \n485 Justification:   \n486 Guidelines:   \n487 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n488 made in the paper.   \n489 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n490 contributions made in the paper and important assumptions and limitations. A No or   \n491 NA answer to this question will not be perceived well by the reviewers.   \n492 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n493 much the results can be expected to generalize to other settings.   \n494 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n495 are not attained by the paper.   \n496 2. Limitations   \n497 Question: Does the paper discuss the limitations of the work performed by the authors?   \n498 Answer: [No]   \n499 Justification: The training time is longer and more details can be found in the end of   \n500 Appendix B.   \n501 Guidelines:   \n502 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n503 the paper has limitations, but those are not discussed in the paper.   \n504 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n505 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n506 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n507 model well-specification, asymptotic approximations only holding locally). The authors   \n508 should reflect on how these assumptions might be violated in practice and what the   \n509 implications would be.   \n510 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n511 only tested on a few datasets or with a few runs. In general, empirical results often   \n512 depend on implicit assumptions, which should be articulated.   \n513 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \n514 For example, a facial recognition algorithm may perform poorly when image resolution   \n515 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n516 used reliably to provide closed captions for online lectures because it fails to handle   \n517 technical jargon.   \n518 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n519 and how they scale with dataset size.   \n520 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n521 address problems of privacy and fairness.   \n522 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n523 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n524 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n525 judgment and recognize that individual actions in favor of transparency play an impor  \n526 tant role in developing norms that preserve the integrity of the community. Reviewers   \n527 will be specifically instructed to not penalize honesty concerning limitations.   \n528 3. Theory Assumptions and Proofs   \n529 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n530 a complete (and correct) proof?   \n531 Answer: [NA]   \n532 Justification:   \n533 Guidelines:   \n534 \u2022 The answer NA means that the paper does not include theoretical results.   \n535 \u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross  \n536 referenced.   \n537 \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n538 \u2022 The proofs can either appear in the main paper or the supplemental material, but if   \n539 they appear in the supplemental material, the authors are encouraged to provide a short   \n540 proof sketch to provide intuition.   \n541 \u2022 Inversely, any informal proof provided in the core of the paper should be complemented   \n542 by formal proofs provided in appendix or supplemental material.   \n543 \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced.   \n544 4. Experimental Result Reproducibility   \n545 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n546 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n547 of the paper (regardless of whether the code and data are provided or not)?   \n548 Answer: [Yes]   \n549 Justification: All the details are given in the Appendix C.   \n550 Guidelines:   \n551 \u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.   \n\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "606 \u2022 Providing as much information as possible in supplemental material (appended to the   \n607 paper) is recommended, but including URLs to data and code is permitted.   \n608 6. Experimental Setting/Details   \n609 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n610 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n611 results?   \n612 Answer: [Yes]   \n613 Justification: See Appendix C.   \n614 Guidelines:   \n615 \u2022 The answer NA means that the paper does not include experiments.   \n616 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n617 that is necessary to appreciate the results and make sense of them.   \n618 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n619 material.   \n620 7. Experiment Statistical Significance   \n621 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n622 information about the statistical significance of the experiments?   \n623 Answer: [No]   \n624 Justification: We reported the best following everyone else.   \n625 Guidelines:   \n626 \u2022 The answer NA means that the paper does not include experiments.   \n627 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n628 dence intervals, or statistical significance tests, at least for the experiments that support   \n629 the main claims of the paper.   \n630 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n631 example, train/test split, initialization, random drawing of some parameter, or overall   \n632 run with given experimental conditions).   \n633 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n634 call to a library function, bootstrap, etc.)   \n635 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n636 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n637 of the mean.   \n638 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n639 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n640 of Normality of errors is not verified.   \n641 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n642 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n643 error rates).   \n644 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n645 they were calculated and reference the corresponding figures or tables in the text.   \n646 8. Experiments Compute Resources   \n647 Question: For each experiment, does the paper provide sufficient information on the com  \n648 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n649 the experiments?   \n650 Answer: [Yes]   \n651 Justification: See end of Appendix B.   \n652 Guidelines:   \n653 \u2022 The answer NA means that the paper does not include experiments.   \n654 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n655 or cloud provider, including relevant memory and storage. ", "page_idx": 18}, {"type": "text", "text": "\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 19}, {"type": "text", "text": "666 Guidelines:   \n667 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n668 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n669 deviation from the Code of Ethics.   \n670 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n671 eration due to laws or regulations in their jurisdiction). ", "page_idx": 19}, {"type": "text", "text": "672 10. Broader Impacts ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "73 Question: Does the paper discuss both potential positive societal impacts and negative   \n74 societal impacts of the work performed?   \n76 Justification:   \n77 Guidelines:   \n78 \u2022 The answer NA means that there is no societal impact of the work performed.   \n79 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n80 impact or why the paper does not address societal impact.   \n81 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n82 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n83 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n84 groups), privacy considerations, and security considerations.   \n85 \u2022 The conference expects that many papers will be foundational research and not tied   \n86 to particular applications, let alone deployments. However, if there is a direct path to   \n87 any negative applications, the authors should point it out. For example, it is legitimate   \n88 to point out that an improvement in the quality of generative models could be used to   \n89 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n90 that a generic algorithm for optimizing neural networks could enable people to train   \n91 models that generate Deepfakes faster.   \n92 \u2022 The authors should consider possible harms that could arise when the technology is   \n93 being used as intended and functioning correctly, harms that could arise when the   \n94 technology is being used as intended but gives incorrect results, and harms following   \n95 from (intentional or unintentional) misuse of the technology.   \n96 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n97 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n98 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n99 feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "08 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n09 necessary safeguards to allow for controlled use of the model, for example by requiring   \n10 that users adhere to usage guidelines or restrictions to access the model or implementing   \n11 safety filters.   \n12 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n13 should describe how they avoided releasing unsafe images.   \n14 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n15 not require this, but we encourage authors to take this into account and make a best   \n16 faith effort. ", "page_idx": 20}, {"type": "text", "text": "717 12. Licenses for existing assets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "718 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n719 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n720 properly respected? ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 20}, {"type": "text", "text": "739 13. New Assets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "40 Question: Are new assets introduced in the paper well documented and is the documentation   \n41 provided alongside the assets? ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 20}, {"type": "text", "text": "753 14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "754 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n755 include the full text of instructions given to participants and screenshots, if applicable, as   \n756 well as details about compensation (if any)?   \n757 Answer: [NA]   \n758 Justification: ", "page_idx": 20}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. \u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 21}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Guidelines: \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. \u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. \u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 21}]