[{"figure_path": "pNQB78UDp4/tables/tables_5_1.jpg", "caption": "Table 1: Performance comparisons With VPT and CVPT on VTAB-1K benchmark of different number of prompt tokens.", "description": "This table compares the performance of VPT and CVPT on the VTAB-1K benchmark using different numbers of prompt tokens.  It shows the average accuracy achieved by each method for each number of prompt tokens (1, 5, 10, 20, 50, 100, 150, and 200).  The results highlight the impact of the number of prompts on the performance of both methods and demonstrate the improved performance of CVPT compared to VPT across different prompt numbers, except for the case of only 1 prompt.", "section": "3.3 Comparison with VPT"}, {"figure_path": "pNQB78UDp4/tables/tables_7_1.jpg", "caption": "Table 2: Performance comparisons on the VTAB-1k benchmark with ViT-B/16 models pre-trained on ImageNet-21K.", "description": "This table presents a comparison of the performance of different parameter-efficient fine-tuning (PEFT) methods on the VTAB-1k benchmark, using a ViT-B/16 model pre-trained on ImageNet-21k.  The table shows the top-1 accuracy for each method across 19 datasets categorized into Natural, Specialized, and Structured groups. The results highlight the performance of CVPT compared to other methods, including the original VPT and other state-of-the-art PEFT methods.", "section": "4.2 Comparison with the SOTA"}, {"figure_path": "pNQB78UDp4/tables/tables_7_2.jpg", "caption": "Table 2: Performance comparisons on the VTAB-1k benchmark with ViT-B/16 models pre-trained on ImageNet-21K.", "description": "This table presents a comparison of different parameter-efficient fine-tuning (PEFT) methods on the VTAB-1k benchmark using a pre-trained ViT-B/16 model.  The methods are categorized into prompt-based and adapter-based methods.  The table shows the average accuracy across 19 datasets within VTAB-1k, broken down into three categories: Natural, Specialized, and Structured.  The number of parameters (in millions) for each method is also provided.  The bold values within each group represent the best accuracy for that group.", "section": "4.2 Comparison with the SOTA"}, {"figure_path": "pNQB78UDp4/tables/tables_8_1.jpg", "caption": "Table 5: Performance comparisons of learnable CA and frozen CA with weight-sharing.", "description": "This table presents the results of ablation studies comparing the performance of using a learnable cross-attention (CA) layer versus a frozen CA layer (with weight sharing) in the CVPT model.  The performance is measured across three subcategories of the VTAB-1K benchmark (Natural, Specialized, Structured) and an average across all categories. Additionally, it includes the average performance across five fine-grained visual categorization (FGVC) datasets.  The learnable CA has a significantly larger number of parameters than the frozen CA (with weight sharing). The goal is to determine if the performance gain from the learnable CA outweighs the increased computational cost.", "section": "4.3 Ablation Studies"}]