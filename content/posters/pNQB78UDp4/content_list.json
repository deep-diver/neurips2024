[{"type": "text", "text": "CVPT: Cross-Attention help Visual Prompt Tuning adapt visual task ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 In recent years, the rapid expansion of model sizes has led to large-scale pre-trained   \n2 models demonstrating remarkable capabilities. Consequently, there has been a trend   \n3 towards increasing the scale of models. However, this trend introduces significant   \n4 challenges, including substantial computational costs of training and transfer to   \n5 downstream tasks. To address these issues, Parameter-Efficient Fine-Tuning (PEFT)   \n6 methods have been introduced. These methods optimize large-scale pre-trained   \n7 models for specific tasks by fine-tuning a select group of parameters. Among   \n8 these PEFT methods, adapter-based and prompt-based methods are the primary   \n9 techniques. Specifically, in the field of visual fine-tuning, adapters gain prominence   \n10 over prompts because of the latter\u2019s relatively weaker performance and efficiency.   \n11 Under the circumstances, we refine the widely-used Visual Prompt Tuning (VPT)   \n12 method, proposing Cross Visual Prompt Tuning (CVPT). CVPT calculates cross  \n13 attention between the prompt tokens and the embedded tokens, which allows us to   \n14 compute the semantic relationship between them and conduct the fine-tuning of   \n15 models exactly to adapt visual tasks better. Furthermore, we introduce the weight  \n16 sharing mechanism to initialize the parameters of cross-attention, which avoids   \n17 massive learnable parameters from cross-attention and enhances the representative   \n18 capability of cross-attention. We conduct comprehensive testing across 25 datasets   \n19 and the result indicates that CVPT significantly improves VPT\u2019s performance   \n20 and efficiency in visual tasks. For example, on the VTAB-1K benchmark, CVPT   \n21 outperforms VPT over $4\\%$ in average accuracy, rivaling the advanced adapter-based   \n22 methods in performance and efficiency. Our experiments confirm that prompt-based   \n23 methods can achieve exceptional results in visual fine-tuning. ", "page_idx": 0}, {"type": "text", "text": "24 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "25 Increasing the scale of the models is a common method to enhance the model\u2019s performance   \n26 (35)(9)(28)(29). In recent years, with the rapid development of computing devices, model sizes   \n27 have significantly increased (45)(6)(16)(47). For instance, the number of parameters in the GPT   \n28 series developed by OpenAI has surged from 117 million to 1.8 trillion in just five years (36)(37)(2).   \n29 The rapidly increasing number of parameters will lead to the problem of immense computational   \n30 overhead. Therefore, adapting those models to downstream tasks with the full-tuning method will   \n31 incur enormous costs. To resolve this issue, the PEFT approach has been proposed (19)(27)(1)(38)(5).   \n32 PEFT adapts those large-scale pre-trained models to downstream tasks in a more efficient way by   \n33 fine-tuning a subset of the models that contains much fewer parameters. Two mainstream methods   \n34 within PEFT are Adapter (18) and Prompt (27). During the training process, the Adapter inserts   \n35 adapters into each transformer block and tunes those adapters, while the Prompt inserts prompt tokens   \n36 into the embedded tokens to update the prompt tokens.   \n37 VPT, a prompt-based method is first introduced by Jia et al. (21) for visual fine-tuning tasks. Never  \n38 theless, research on the adapter-based method is prominent due to its superior performance. Although   \n39 some works have improved the performance of VPT (20)(12)(7), it is still challenging to match the   \n40 effectiveness to that of adapter-based methods. There appears to be a consensus that prompt-based   \n41 methods underperform adapter-based methods in the visual domain. But is this the case?   \n42 We conduct extensive experiments and analyses on VPT to uncover the reasons for its weaker   \n43 performance compared to the Adapter. According to our experiments, we consider that the primary   \n44 reason for the performance difference between VPT and adapters is that VPT\u2019s deployment directly   \n45 applies that used in NLP tasks (27), without any adaptation to visual tasks. In NLP tasks, prompts   \n46 usually contain rich semantic information that guides the fine-tuning process of the model. However,   \n47 in visual tasks, prompts lack representation information. Therefore, it is necessary for VPT to use an   \n48 abundant amount of prompts to fine-tune models. However, the design of VPT leads to computational   \n49 inefficiency and redundancy, as well as the disruption of the self-attention between embedded tokens   \n50 3.1. As the graph follows 1, VPT shows a significant decrease in performance and an increase in   \n51 costs when given a large number of prompts. Considering that, we think that VPT is unusable when   \n52 given a large number of prompts.   \n53 To handle the problem, we redesign VPT and in  \n54 troduced Cross Visual Prompt Tuning (CVPT). For   \n55 the prompt tokens in CVPT, we calculate the cross  \n56 attention with the embedded tokens and add the result   \n57 as residuals to the embedded tokens. This approach   \n58 avoids the computational complexity of self-attention   \n59 that is quadratically related to the number of prompts   \n60 and allows prompts to focus on the embedded token   \n61 to adapt to downstream tasks more efficiently. Addi  \n62 tionally, by maintaining consistency in token dimen  \n63 sions throughout the computation process, the results of   \n64 cross-attention can be directly summed with embedded   \n65 tokens as residuals and do not introduce additional com  \n66 putational overhead for subsequent MLP. Furthermore,   \n67 we share the weights of the self-attention layer with   \n68 the cross-attention layer during loading checkpoints,   \n69 keeping the cross-attention layer frozen alongside the   \n70 self-attention layer, which eliminates the requirement   \n71 for additional learned parameters for the cross-attention,   \n72 and utilizes the encoded information in self-attention   \n73 to help the fine-tuning of the model.   \n74 We validate the effectiveness of our method on 25 datasets, the results show that the CVPT achieves   \n75 a significant improvement in performance and efficiency compared to the VPT. CVPT shows an   \n76 average ${\\bf4\\%}$ improvement in accuracy on the 19 VTAB-1K datasets, $1\\%$ on the 5 FGVC datasets,   \n77 and ${\\mathfrak{s}}\\%$ on the ADE20K dataset. Additionally, if given fewer prompt tokens, CVPT achieves a   \n78 comparable performance with other advanced PEFT methods which significantly outperforms the   \n79 other prompt-based methods and needs fewer learnable parameters. If a large number of prompts   \n80 is allowed, our CVPT outperforms the SOTA methods on FGVC and ADE20K datasets. Besides,   \n81 although a large number of prompts are inserted, it does not introduce too much extra computational   \n82 overhead compared to VPT.   \n83 Finally, we explore the impact of the deployment\u2019s position and the effectiveness of the weight  \n84 sharing mechanism. The improvement on the model can be fully illustrated by the experimental   \n85 results above, indicating that prompt-based methods can also rival SOTA adapter-based methods. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "image", "img_path": "pNQB78UDp4/tmp/dbf77601708ec2451b8356881a5f3b427dfa462c3efe3509b540519af4b52bcd.jpg", "img_caption": ["Figure 1: Comparisons of performance and Flops between VPT and our CVPT with a pre-trained ViT-B/16 model on the VTAB-1k benchmark. We set the number of prompts to 1,10,20,50,100,150,200 respectively. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "86 Overall, our contributions are as follows: ", "page_idx": 1}, {"type": "text", "text": "87 \u2022 We provide a detailed analysis of the application of VPT to visual tasks, and propose that its   \n88 drawback can be summarised in three points which are lack of adaptation, computational   \n89 inefficiency and redundancy, destruction of self-attention.   \n90 \u2022 We propose CVPT, which introduces cross-attention and weight-sharing mechanisms, to   \n91 avoid the efficiency and performance problems caused by VPT, which allows us to use more   \n92 prompts to improve performance efficiently.   \n93 \u2022 We conducted experiments on 25 datasets with different downstream tasks. The results   \n94 show that our approach significantly outperforms the original VPT and other prompt-based   \n95 works in terms of performance and efficiency. It is also comparable to SOTA adapter-based   \n96 methods, demonstrating the usability of the prompt-based approach for visual fine-tuning. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "97 2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "98 PEFT. In the era of CNN, making bigger and deeper models was an effective way to improve   \n99 performance (26)(15)(43). With the rise of transformers, this trend became even more popular.   \n100 The introduction of ChatGPT further cemented the goal of the community to develop larger and   \n101 more powerful models. However, limited by their scale, despite their powerful performance and   \n102 generality, these large models are difficult to adapt downstream tasks by using traditional paradigms   \n103 (full-tuning). Consequently, NLP researchers first proposed PEFT methods. Their works demonstrate   \n104 that fine-tuning just a small number of parameters in a large-scale pre-trained model can achieve   \n105 nearly the same performance as full-tuning. Encouraged by the success in NLP, researchers began   \n106 to apply PEFT to large-scale vision models on different visual tasks (8)(44). After development in   \n107 the past several years, the mainstream PEFT methods can be broadly categorized into adapter-based   \n108 methods and Prompt-based methods.   \n109 Adapter. Jie et al. (18) proposed inserting adapters into the network to efficiently fine-tune the   \n110 model. These adapters are commonly a small network that usually contains an upsampling layer   \n111 and a downsampling layer. The input is multiplied with a scaling factor after passing through the   \n112 upsampling and downsampling layers and then the result is added as a residual to the input. The   \n113 general form of adapter can be expressed as: ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\nX_{o u t}=X_{i n}+\\gamma(W_{u p}(W_{d o w n}(X_{i n}))),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "114 where $X_{i n}$ denotes the input of Adapter, $\\gamma$ represents the scaling factor of Adapter, and $W_{u p}$ and   \n115 $W_{d o w n}$ correspond to the upsampling layer and downsampling layer, respectively. Some works did   \n116 some adaption to visual tasks based on Adapter, developing several variants such as AdaptFormer   \n117 (4), LoRA (19) and RepAdapter (30), etc. These adapter-based methods dominate the field of visual   \n118 fine-tuning.   \n119 Prompt. Prompt was originally used in the field of NLP which is added to the input text for   \n120 comprehension tasks. Lester et al. (27) proposed treating the prompt as a continuous vector and   \n121 fine-tuning the model by updating its gradients. Jia et al. (21) introduced this concept to visual   \n122 fine-tuning for the first time, naming it VPT. As shown in Fig.3, the embedded tokens are spliced with   \n123 the prompt tokens before entering each transformer block, allowing it to participate in every layer of   \n124 the network within the transformer block. Before entering the next transformer block, the prompt   \n125 tokens of the previous layer are discarded, and new prompt tokens are spliced with the embedded   \n126 token again (VPT-Deep). This can be formulated as shown below: ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\n[\\vec{x}_{i},\\underline{{{\\longrightarrow}}},\\vec{E_{i}}]=L_{i}([\\vec{x}_{i-1},\\vec{P}_{i-1},\\vec{E}_{i-1}]),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "127 where the red and blue indicate learnable and frozen parameters, respectively. $P$ denotes a learnable   \n128 d-dimensional vector, X is the CLS token, and $\\boldsymbol{\\mathrm E}$ is the patched image. Although there are improved   \n129 variants based on VPT, such as E2VPT (12), EXPRESS (7) and DAM-VP (20), a performance gap   \n130 remains between prompt-based and adapter-based approaches. ", "page_idx": 2}, {"type": "text", "text": "131 3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "132 3.1 Analysis of previous VPT ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "133 Firstly, we analyze VPT deeply to explore why it is not better than adapter in terms of performance   \n134 and efficiency, our analysis follows three points:   \n135 Lack of adaptation to visual tasks. In NLP, each token represents an actual word with rich semantic   \n136 information. Therefore, the processing of concatenating prompt tokens and embedded tokens is   \n137 natural and suitable for NLP tasks. However, in visual tasks, tokens represent image patches and   \n138 contain sparse semantic information compared to those in NLP. Therefore, simply splicing the prompt   \n139 tokens with the embedded tokens may not provide sufficient guidance information. Additionally,   \n140 visual tasks often require a deeper understanding of spatial relationships and structural features of an   \n141 image, which are difficult to achieve with prompt tokens.   \n142 Computational inefficiency and redundancy. When computing self-attention, the attention between   \n143 each token and all other tokens needs to be calculated. Its computational complexity is $n^{2}$ , where   \n144 $n$ is the number of embedded tokens. If $m$ represents the number of inserted prompt tokens, the   \n145 computational complexity of self-attention in VPT can be expressed as $(n+{\\dot{m}})^{2}$ . This increases   \n146 the computational overhead significantly, especially when using a larger number of prompt tokens.   \n147 Additionally, we found that prompt tokens are involved in the MLP computation process, which not   \n148 only adds computational overhead but also does not impact the results. Our experiments show that   \n149 removing the prompt token after self-attention does not affect the results.   \n150 Destruction of self-attention between embedded tokens. After softmax, the sum of the weights   \n151 of all tokens is normalized to 1. Whereas, due to the addition of the prompt tokens, the sum of   \n152 the weights of the embedded tokens is reduced by the prompt tokens, which corresponds to the   \n153 weakening of the representation ability of the self-attention between embedded tokens. Since the   \n154 prompt token is eventually removed, this is equivalent to multiplying the self-attention result between   \n155 the embedded tokens by a factor which less than one. To explore how large this effect is, we set the   \n156 number of prompts to 1,5,20,50,100,150,196 respectively, and visualize the tensor after the softmax   \nfunction, the results are shown in Fig.2 below.   \n157   \n158 As the number of prompts increases, the sum of the prompt\u2019s weight values exceeds 0.8, which is over   \n159 4 times that of embedded tokens, significantly disrupting the self-attention between the embedded   \n160 tokens. This explains why VPT performance decreases substantially with a larger number of prompts. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "image", "img_path": "pNQB78UDp4/tmp/0b3191ef63ec685035adafcaf10ea1fa3fe65ea6c16378537083cb151e8bf4b9.jpg", "img_caption": ["Figure 2: Self-attention weight obtained by prompt tokens and embedded tokens. We visualize the self-attention of $c l s_{t o k e n}$ and exclude itself to observe the attention of $c l s_{t o k e n}$ to other tokens. And the darker the color, the larger the weight. When giving 196 prompts, the attention weight obtained by prompts is over $80\\%$ , which greatly influences the self-attention received by embedded tokens. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "161 3.2 Cross Visual Prompt Tuning ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "162 Cross-Attention. Unlike self-attention (40), which computes the relationship between each element   \n163 in the input sequence, cross-attention computes attention on two different sequences to process the   \n164 semantic relationship between them (3). For example, in translation tasks, cross-attention is used to   \n165 compute the attention weights between the source language sentence and the target language sentence.   \n166 In our method, we introduce cross-attention to handle the semantic relationship between embedded   \n167 tokens and prompt tokens, guiding the fine-tuning of the model. Specifically, the input of cross  \n168 attention consists of two parts: $X_{1}$ and $X_{2}$ , in which $X_{1}\\in\\mathbb{R}^{n\\times d_{1}}$ and $X_{2}\\in\\mathbb{R}^{\\dot{m}\\times d_{2}}$ . And $X_{1}$ serves   \n169 as the query set and $X_{2}$ serves as the key-value set. We set $Q=X_{1}W^{Q}$ and $K=V=X_{2}W^{K}$ , and   \n170 then the cross-attention can be expressed as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\nC r o s s A t t e n t i o n(X_{1},X_{2})=S o f t m a x\\left(\\frac{Q\\cdot K}{\\sqrt{d_{k}}}\\right)V.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "171 In which $W^{Q}\\in\\mathbb{R}^{d_{1}\\times d_{k}}$ and $W^{K}\\in\\mathbb{R}^{d_{2}\\times d_{k}}$ are learned projection matrix, $d_{k}$ is the dimension of   \n172 value-key set. In our methods, $d_{1}=d_{2}=d_{k}$ . And the shape of output is $n\\times d_{k}$ , which is consistent   \n173 with $X_{1}$ .   \n174 Cross Visual Prompt Tuning. We redesign the prompt to better adapt visual tasks and proposed   \n175 CVPT. Our approach, as illustrated in Fig.3, follows the VPT, the main parameters of the network   \n176 remain frozen, and only the final classification layer and the prompt are trainable. The key difference   \n177 is that we allow the prompt token to perform cross-attention with the embedded tokens and the result   \n178 of cross-attention is added with the embedded tokens as residuals. This operation helps prompts adapt   \n179 visual tasks a lot, and we demonstrate how significant this improvement is in Sec.4.2. Specifically,   \n180 for any input $x_{i}$ of a transformer block, the forward flow can be represented as follows: ", "page_idx": 4}, {"type": "image", "img_path": "pNQB78UDp4/tmp/eca19d73481349200ee96fd0e6534942d5bfabd5eeb4298de33e9cc1649d4b26.jpg", "img_caption": ["Figure 3: Structure comparison of VPT and CVPT. In which blue represents frozen parameters and orange represents learnable parameters. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{X_{1}=X_{i}+S A(L N_{1}(X_{i})),}\\\\ {X_{2}=X_{1}+C A(X_{1},P r o m p t),}\\\\ {X_{o u t}=X_{2}+M L P(L N_{2}(X_{2})),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "181 where blue denotes frozen parameters and red denotes trainable parameters, SA denotes self-attention,   \n182 CA denotes cross-attention, and LN denotes layer normalization.   \n183 In CVPT, we only introduce linear computational overhead associated with the number of prompt   \n184 tokens. It allows CVPT to use a large number of prompt tokens to improve its performance by   \n185 introducing an acceptable overhead. Furthermore, CVPT preserves the original procedure of self  \n186 attention, keeping the complete representation ability of embedded tokens. We demonstrate the   \n187 improvement over VPT in terms of performance and efficiency in Sec.3.3. Finally, we set embedded   \n188 tokens as query set and prompt tokens as key-value set, so that we can maintain the unity of the   \n189 number of channels, allowing the result of cross-attention to be directly summed with the input as a   \n190 residual term.   \n191 Weight-sharing mechanism. The utilization of cross-attention, which requires a large number   \n192 of learnable parameters (usually $\\geq30\\%$ model\u2019s parameter number), leads to a major challenge   \n193 in computational overhead. Therefore, if the parameters of them are tunable, the computational   \n194 overhead of CVPT will even rival those using full-tuning. Therefore, we introduce the weight-sharing   \n195 mechanism. Due to the structure of cross-attention equals to that of self-attention, we consider that   \n196 the weight of self-attention is also instructive for the fine-tuning of cross-attention. Thus, we initialize   \n197 the weight of cross-attention with the parameters of self-attention when loading checkpoints. It   \n198 avoids the introduction of a huge number of learnable parameters in cross-attention and keeps the   \n199 efficiency of our CVPT. We explore the impact of weight-sharing in 4.3 and demonstrate that frozen   \n200 cross-attention is even more effective than learnable cross-attention. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "201 3.3 Comparison with VPT ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "202 Performance improvement. To investigate how much improvement CVPT makes and the effect   \n203 of the number of prompts on performance, we use different numbers of prompt tokens and conduct   \n204 experiments on VTAB-1K using VPT and CVPT, respectively. The results are shown in the following   \n205 Table.1:   \n206 These results show that our CVPT achieves better performance in almost every case except the number   \n207 of prompts equals 1. As we analyzed in Section 3.1, VPT represents a pool absolute performance   \n208 on account of the lack of adaptation to visual tasks. Besides, due to the corruption of self-attention   \n209 between embedded tokens, when given a larger number of prompt tokens, VPT shows significant   \n210 performance degradation or even crashes. In contrast, our CVPT avoids suffering from these problems.   \n211 Additionally, its performance improves as the number of prompt tokens increases. All these results   \n212 above indicate that cross-attention between prompt tokens and embedded tokens helps prompts   \n213 adapting the visual tasks and instruct the model\u2019s fine-tuning more exactly.   \n214 Efficiency improvement. To explore the improvement in efficiency of CVPT, we also recorded the   \n215 amount of GPU memory occupied by VPT and CVPT during training and testing as well as the total   \n216 computation of the two when conducting the above experiments, and the results are shown in Fig.4   \n217 follows:   \n218 It can be seen that our CVPT has made significant improvements in efficiency compared to VPT   \n219 especially given a large amount of prompt tokens. Although it requires slightly more GPU memory   \n220 during testing compared to full-tuning which is marginal compared to VPT. Additionally, the weight  \n221 sharing mechanism allows for targeted optimization in engineering applications, letting cross-attention   \n222 and self-attention share memory, further widening the efficiency gap with VPT. Moreover, the careful   \n223 design of CVPT prevents explosive growth in memory and computation as the number of prompts   \n224 increases. This means we can improve the performance of CVPT by increasing the number of   \n225 prompts, which is more computationally efficient than other methods.   \n226 In summary, our CVPT significantly improves the performance and efficiency of VPT by   \n227 introducing cross-attention and the weight-sharing mechanism, especially given a larger number   \n228 of prompts. Therefore, it allows us to introduce more prompts to the prompt-based method in an   \n229 efficient manner, thus improving its performance. We will demonstrate how much this improvement   \n230 is and compare it with the SOTA methods in the next section. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "table", "img_path": "pNQB78UDp4/tmp/d646cb5753bc912a3b00e891db2fdb22e690de0198a02f4bbad03e44d3e93949.jpg", "table_caption": ["Table 1: Performance comparisons With VPT and CVPT on VTAB-1K benchmark of different number of prompt tokens. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "image", "img_path": "pNQB78UDp4/tmp/3bbf8931ef12c8e3d6ed497c5df20d87b5aa53fdf205560b4c6968a33891417f.jpg", "img_caption": ["Figure 4: The trends of training memory, testing memory, and Flops with the variation in the number of prompt tokens. Where LP represents Linear Probing which only tunes the final classifier linear. We record those data on cifar100 in VTAB-1K, the batch_size is set to 32. Pre-trained model is ViT-B/16. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "231 4 Experiment ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "232 4.1 Experimental settings ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "233 Datasets. We evaluate our CVPT on both image classification and semantic segmentation tasks to   \n234 verify its effectiveness. The specific datasets involved in our work are presented in the following.   \n235 \u2022 VTAB-1K. VTAB-1K comprises 19 datasets from different domains, classified into   \n236 three main categories: the Natural group (natural images captured by standard cameras)   \n237 (25)(32)(10)(34), the Specialized group (professional images captured by specialized equip  \n238 ment, such as medical and remote sensing images) (41)(17), and the Structured group   \n239 (synthetic images from artificial environments). Each task contains only 1,000 training   \n240 samples (22)(11)(31). This is a primary metric for evaluating PEFT\u2019s performance.   \n241 \u2022 FGVC. FGVC consists of five fine-grained visual classification benchmarks, including CUB  \n242 200-2011 (42), NABirds (39), Oxford Flowers (33), Stanford-Dogs (23) and Stanford-Cars   \n243 (24). Unlike VTAB-1K, the datasets in FGVC benchmarks are complete.   \n244 \u2022 ADE20K. ADE20K (50) contains more than 25,000 images and is primarily used for scene   \n245 perception, parsing, segmentation, multi-object recognition, and semantic understanding.   \n246 This adaptation is challenging due to the huge gap between the objectives of pretraining and   \n247 downstream tasks.   \n248 Baseline. We primarily use CVPT to compare with the following methods: (1) Full-tuning, (2)   \n249 Adapter and its improved variants such as LoRA, Adaptformer, RepAdapter, and SPT, and (3) VPT   \n250 and its variants, including E2VPT, EXPRESS and so on.   \n251 Training. We use the ViT-Base-16 model as our main model and AdamW as our optimizer. The   \n252 other settings and training strategies follow those used in VPT. To avoid extensive hyperparameter   \n253 search, we only select the number of prompts from [1, 5, 10, 20] for VTAB-1K. Besides, we use   \n254 single NVIDIA 3090 on VTAB-1K and FGVC benchmark, and use NVIDIA $3090\\times8$ on ADE20k. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "255 4.2 Comparison with the SOTA ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "256 VTAB-1K. We compared our method with other baseline methods on the VTAB-1K benchmark. The   \n257 experimental results are shown in Table.2, where we report the top-1 accuracy of these methods. In   \n258 the table, we divide the prompt-based methods into one group and the other methods into another   \n259 group. The bold values in each group represent the best accuracy. ", "page_idx": 6}, {"type": "text", "text": "Table 2: Performance comparisons on the VTAB-1k benchmark with ViT-B/16 models pretrained on ImageNet-21K. ", "page_idx": 6}, {"type": "image", "img_path": "pNQB78UDp4/tmp/042e137c51e1266bd36afe8796b6f1434dea640d95e9e63832a592f8f66b7835.jpg", "img_caption": [], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "260 We first compare our method with other prompt-based methods. The results of our experiments show   \n261 that our method achieved the best performance among prompt-based methods in 16 out of 19 datasets,   \n262 significantly outperforming VPT and other VPT-based methods. Notably, CVPT achieves the highest   \n263 accuracy in all datasets within the structured group, indicating that the addition of cross-attention   \n264 significantly improves the adaptation of prompts. Therefore, CVPT performs better in those out-of  \n265 distribution (OOD) datasets. Additionally, since we use fewer than 20 prompts in VTAB-1K, CVPT   \n266 requires the lowest number of parameters.   \n267 When considering all PEFT methods, we find that on a small dataset like VTAB-1K, almost all   \n268 mainstream PEFT methods outperformed full-tuning in terms of performance. This suggests that   \n269 correctly selecting the parameters to fine-tune is crucial. For our CVPT, it shows an impressive   \n270 performance, which is only $0.2\\%$ behind SPT in accuracy while using fewer parameters than SPT,   \n271 and outperforms the other PEFT methods in performance. This indicates that CVPT reaches SOTA   \n272 in terms of both performance and parameter count. In particular, compared to other prompt-based   \n273 methods that show weaknesses, our CVPT deeply explores the potential of prompt-based methods   \n274 and demonstrates that prompt-based methods can also perform well in the field of visual fine-tuning.   \n275 FGVC. Performance on VTAB-1K alone is not enough to prove the superiority of CVPT. Therefore,   \n276 we introduce the experimental results of CVPT on FGVC to explore its performance on a complete   \n277 dataset of a certain scale. The results are shown in Table.3 below:   \n278 Similar to the results on VTAB-1K, our approach substantially outperforms other prompt-based   \n279 methods on FGVC benchmark. Additionally, it surpasses SPT and other adapter-based methods to   \n280 achieve the best performance. This suggests that CVPT exhibits better performance on relatively   \n281 large datasets like FGVC, which proves the adaptability of CVPT to the increasing scale of data in   \n282 the future. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "table", "img_path": "pNQB78UDp4/tmp/8dd42ae63a487e7bb344d6204191cc3b3d7e6feb613460529ced42233277d218.jpg", "table_caption": ["Table 3: Performance comparisons on five FGVC datasets with ViT-B/16 models pre-trained on ImageNet-21K. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "283 ADE20K. Finally, we apply CVPT to SETR(49) on the ADE20K dataset to explore its performance on downstream tasks of semantic segmentation. The results are shown in Table.4 below: ", "page_idx": 7}, {"type": "text", "text": "Table 4: Results of ADE20K datasets with ViT-L models. We report \"mIoU-SS\" and \"mIoU-Ms\" which denote single-scale and multi-scale, respectively ", "page_idx": 7}, {"type": "table", "img_path": "pNQB78UDp4/tmp/59cfbd8adb4d7081a5cdf493dbecbaf7d358fddcdc99f95d9aad03d343b8e9d4.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "284 ", "page_idx": 7}, {"type": "text", "text": "285 This task is quite challenging because of the huge distribution gap between pre-training datasets and   \n286 downstream tasks. In this situation, our CVPT shows a $1.7\\%$ enhancement of \"mIoU-SS\" over the   \n287 VPT with the same number of prompts. If we use 200 prompts for fine-tuning, CVPT represents a   \n288 significant improvement over the other PEFT methods. This fully demonstrates the adaptation of   \n289 CVPT to OOD datasets. Besides, due to our optimization of the deployment, even though the number   \n290 of learnable parameters increases by 4 million, our memory usage and training time increase by less   \n291 than $10\\%$ compared to linear probing and less than $5\\%$ compared to it when using 10 prompts during   \n292 training.   \n294 The impact of the location of the Cross-Attention (CA). We conducted experiments with the   \n295 following five positions to explore the optimal deployment of CA, and the results of the experiments   \n296 are displayed in Table.5:   \n297 We can see that inserting in prompt tokens after self-attention (SA) is the best way to perform.   \n298 However, if a slight performance decrease is acceptable, we can choose position 2 to insert in parallel   \n299 to improve the efficiency of the operation (this improvement is also slight). ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "image", "img_path": "pNQB78UDp4/tmp/47dd67f49b8a9e119080eb36142a9715857522f1871f4ff2e430d7eb53bd713b.jpg", "img_caption": ["Figure 5: (a) The deployments of cross-attention in ViT. Five possible positions can be inserted. Our final deployments are in dark blue. (b) Performance comparisons of different deployments of cross-attention. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "300 The impact of weight-sharing between CA and SA. We set CA to be learnable (without weight301 sharing) and frozen (with weight-sharing) respectively to investigate the impact of weight-sharing. The results on VTAB-1K and FGVC are shown in Table.5 below: ", "page_idx": 8}, {"type": "table", "img_path": "pNQB78UDp4/tmp/2251ad9032ed4147bfbdaa31aa3fedf0e3901560e8b023cb6419e32edd67afda.jpg", "table_caption": ["Table 5: Performance comparisons of learnable CA and frozen CA with weight-sharing. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "302 ", "page_idx": 8}, {"type": "text", "text": "303 We find that setting CA to tunable adds a significant number of parameters, substantially increasing   \n304 computational overhead. Despite the slight performance gain it brings on VTAB-1K, it lags behind   \n305 the frozen CA substantially in FGVC. Therefore, We believe that the parameters of SA are valuable   \n306 for guiding the fine-tuning of CA. Especially, when dealing with a complete dataset of a certain size,   \n307 such as FGVC, the weight-sharing mechanism can better utilize the pre-trained capabilities of the   \n308 model, thereby improving performance. ", "page_idx": 8}, {"type": "text", "text": "309 5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "310 In this paper, we explore the current mainstream prompt-based method VPT deeply and analyze the   \n311 reasons why it performs poorly. Consequently, we propose a simple and effective PEFT method,   \n312 CVPT, which introduces the cross-attention module to compute the cross-attention between the prompt   \n313 tokens and embedded tokens thus instructing the model\u2019s fine-tuning. What more, the weights of   \n314 cross-attention are come from self-attention, avoiding introducing an enormous number of additional   \n315 trainable parameters and achieving better performance. We conducted extensive experiments on   \n316 25 datasets, and the results demonstrate that CVPT achieves SOTA performance. Additionally,   \n317 we conducted extensive ablation experiments on CVPT, demonstrating the impact of introducing   \n318 cross-attention and weight-sharing, as well as its efficiency and performance improvements over VPT.   \n319 We hope our work will inspire prompt-based PEFT methods in the future. One limitation of our work   \n320 is that CVPT does not explore new strategies for the initialization of prompt tokens. In VPT, the   \n321 author made a complete comparison of different initialization methods. In our work, we take the   \n322 same strategy with VPT. However, we still think the optimized specific initialization method is better   \n323 than the general methods VPT used. Besides, this initialization will also help us understand how   \n324 prompts help the model\u2019s fine-tuning. ", "page_idx": 8}, {"type": "text", "text": "325 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "326 [1] Hyojin Bahng, Ali Jahanian, Swami Sankaranarayanan, and Phillip Isola. Exploring visual   \n327 prompts for adapting large-scale models. Mar 2022.   \n328 [2] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,   \n329 Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel   \n330 Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.   \n331 Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz   \n332 Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec   \n333 Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. CoRR,   \n334 abs/2005.14165, 2020.   \n335 [3] Chun-Fu Richard Chen, Quanfu Fan, and Rameswar Panda. Crossvit: Cross-attention multi  \n336 scale vision transformer for image classification. In Proceedings of the IEEE/CVF international   \n337 conference on computer vision, pages 357\u2013366, 2021.   \n338 [4] Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang, Yibing Song, Jue Wang, and Ping   \n339 Luo. Adaptformer: Adapting vision transformers for scalable visual recognition. CoRR,   \n340 abs/2205.13535, 2022.   \n341 [5] Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong Lu, Jifeng Dai, and Yu Qiao. Vision   \n342 transformer adapter for dense predictions. arXiv preprint arXiv:2205.08534, 2022.   \n343 [6] Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. Electra: Pre-training   \n344 text encoders as discriminators rather than generators. arXiv preprint arXiv:2003.10555, 2020.   \n345 [7] Rajshekhar Das, Yonatan Dukler, Avinash Ravichandran, and Ashwin Swaminathan. Learning   \n346 expressive prompting with residuals for vision transformers. In 2023 IEEE/CVF Conference on   \n347 Computer Vision and Pattern Recognition (CVPR), pages 3366\u20133377. IEEE Computer Society,   \n348 2023.   \n349 [8] Mohammad Mahdi Derakhshani, Enrique Sanchez, Adrian Bulat, Victor G Turrisi da Costa,   \n350 Cees GM Snoek, Georgios Tzimiropoulos, and Brais Martinez. Bayesian prompt learning   \n351 for image-language model generalization. In Proceedings of the IEEE/CVF International   \n352 Conference on Computer Vision, pages 15237\u201315246, 2023.   \n353 [9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of   \n354 deep bidirectional transformers for language understanding. In NAACL-HLT, 2019.   \n355 [10] Li Fei-Fei, R. Fergus, and P. Perona. One-shot learning of object categories. IEEE Transactions   \n356 on Pattern Analysis and Machine Intelligence, page 594\u2013611, Apr 2006.   \n357 [11] A Geiger, P Lenz, C Stiller, and R Urtasun. Vision meets robotics: The kitti dataset. The   \n358 International Journal of Robotics Research, page 1231\u20131237, Sep 2013.   \n359 [12] Cheng Han, Qifan Wang, Yiming Cui, Zhiwen Cao, Wenguan Wang, Siyuan Qi, and Dongfang   \n360 Liu. E2vpt: An effective and efficient approach for visual prompt tuning. In 2023 IEEE/CVF   \n361 International Conference on Computer Vision (ICCV), pages 17445\u201317456. IEEE Computer   \n362 Society, 2023.   \n363 [13] Haoyu He, Jianfei Cai, Jing Zhang, Dacheng Tao, and Bohan Zhuang. Sensitivity-aware visual   \n364 parameter-efficient fine-tuning. In 2023 IEEE/CVF International Conference on Computer   \n365 Vision (ICCV), pages 11791\u201311801. IEEE Computer Society, 2023.   \n366 [14] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross B. Girshick. Masked   \n367 autoencoders are scalable vision learners. In CVPR, 2022.   \n368 [15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image   \n369 recognition. In CVPR, 2016.   \n370 [16] Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced   \n371 bert with disentangled attention. arXiv preprint arXiv:2006.03654, 2020.   \n372 [17] Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel   \n373 dataset and deep learning benchmark for land use and land cover classification. IEEE Journal of   \n374 Selected Topics in Applied Earth Observations and Remote Sensing, page 2217\u20132226, Jul 2019.   \n375 [18] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe,   \n376 Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning   \n377 for NLP. In ICML, 2019.   \n378 [19] Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu   \n379 Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In ICLR,   \n380 2022.   \n381 [20] Qidong Huang, Xiaoyi Dong, Dongdong Chen, Weiming Zhang, Feifei Wang, Gang Hua,   \n382 and Nenghai Yu. Diversity-aware meta visual prompting. In 2023 IEEE/CVF Conference   \n383 on Computer Vision and Pattern Recognition (CVPR), pages 10878\u201310887. IEEE Computer   \n384 Society, 2023.   \n385 [21] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge J. Belongie, Bharath Hariharan,   \n386 and Ser-Nam Lim. Visual prompt tuning. In ECCV, 2022.   \n387 [22] Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick,   \n388 and Ross Girshick. Clevr: A diagnostic dataset for compositional language and elementary   \n389 visual reasoning. In 2017 IEEE Conference on Computer Vision and Pattern Recognition   \n390 (CVPR), Jul 2017.   \n391 [23] Aditya Khosla, Nityananda Jayadevaprakash, Bangpeng Yao, and Fei-Fei Li. Novel dataset for   \n392 fine-grained image categorization: Stanford dogs.   \n393 [24] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for   \n394 fine-grained categorization. In 3dRR, 2013.   \n395 [25] Alex Krizhevsky. Learning multiple layers of features from tiny images. Jan 2009.   \n396 [26] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep   \n397 convolutional neural networks. In NIPS, 2012.   \n398 [27] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient   \n399 prompt tuning. arXiv preprint arXiv:2104.08691, 2021.   \n400 [28] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,   \n401 Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence   \n402 pre-training for natural language generation, translation, and comprehension. arXiv preprint   \n403 arXiv:1910.13461, 2019.   \n404 [29] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike   \n405 Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining   \n406 approach. arXiv preprint arXiv:1907.11692, 2019.   \n407 [30] Gen Luo, Minglang Huang, Yiyi Zhou, Xiaoshuai Sun, Guannan Jiang, Zhiyu Wang, and   \n408 Rongrong Ji. Towards efficient visual adaption via structural re-parameterization. arXiv preprint   \n409 arXiv:2302.08106, 2023.   \n410 [31] Loic Matthey, Irina Higgins, Demis Hassabis, and Alexander Lerchner. dsprites: Disentangle  \n411 ment testing sprites dataset, 2017.   \n412 [32] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and AndrewY. Ng.   \n413 Reading digits in natural images with unsupervised feature learning. Jan 2011.   \n414 [33] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large   \n415 number of classes. In 2008 Sixth Indian Conference on Computer Vision, Graphics amp; Image   \n416 Processing, Dec 2008.   \n417 [34] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In   \n418 CVPR, 2012.   \n419 [35] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agar  \n420 wal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya   \n421 Sutskever. Learning transferable visual models from natural language supervision. In ICML,   \n422 Proceedings of Machine Learning Research, 2021.   \n423 [36] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language   \n424 understanding by generative pre-training. 2018.   \n425 [37] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.   \n426 Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.   \n427 [38] Yi-Lin Sung, Jaemin Cho, and Mohit Bansal. Vl-adapter: Parameter-efficient transfer learning   \n428 for vision-and-language tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision   \n429 and Pattern Recognition, pages 5227\u20135237, 2022.   \n430 [39] Grant Van Horn, Steve Branson, Ryan Farrell, Scott Haber, Jessie Barry, Panos Ipeirotis, Pietro   \n431 Perona, and Serge Belongie. Building a bird recognition app and large scale dataset with   \n432 citizen scientists: The fine print in fine-grained dataset collection. In 2015 IEEE Conference on   \n433 Computer Vision and Pattern Recognition (CVPR), Jun 2015.   \n434 [40] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,   \n435 \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information   \n436 processing systems (NeuIPS), 30, 2017.   \n437 [41] Bastiaan S. Veeling, Jasper Linmans, Jim Winkens, Taco Cohen, and Max Welling. Rotation   \n438 Equivariant CNNs for Digital Pathology, page 210\u2013218. Jan 2018.   \n439 [42] Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The   \n440 caltech-ucsd birds-200-2011 dataset. Jul 2011.   \n441 [43] Saining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, and Kaiming He. Aggregated residual   \n442 transformations for deep neural networks. In Proceedings of the IEEE conference on computer   \n443 vision and pattern recognition, pages 1492\u20131500, 2017.   \n444 [44] Taojiannan Yang, Yi Zhu, Yusheng Xie, Aston Zhang, Chen Chen, and Mu Li. Aim: Adapting   \n445 image models for efficient video action recognition. arXiv preprint arXiv:2302.03024, 2023.   \n446 [45] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V   \n447 Le. Xlnet: Generalized autoregressive pretraining for language understanding. Advances in   \n448 neural information processing systems, 32, 2019.   \n449 [46] Elad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. Bitfit: Simple parameter-efficient   \n450 fine-tuning for transformer-based masked language-models. In Smaranda Muresan, Preslav   \n451 Nakov, and Aline Villavicencio, editors, ACL, 2022.   \n452 [47] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transform  \n453 ers. In CVPR, 2022.   \n454 [48] Yuanhan Zhang, Kaiyang Zhou, and Ziwei Liu. Neural prompt search. CoRR, abs/2206.04673,   \n455 2022.   \n456 [49] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei   \n457 Fu, Jianfeng Feng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmentation   \n458 from a sequence-to-sequence perspective with transformers. In Proceedings of the IEEE/CVF   \n459 conference on computer vision and pattern recognition, pages 6881\u20136890, 2021.   \n460 [50] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, and Antonio   \n461 Torralba. Semantic understanding of scenes through the ade20k dataset. International Journal   \n462 of Computer Vision, 127(3):302\u2013321, 2019. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "463 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "470 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n471 made in the paper.   \n472 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n473 contributions made in the paper and important assumptions and limitations. A No or   \n474 NA answer to this question will not be perceived well by the reviewers.   \n475 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n476 much the results can be expected to generalize to other settings.   \n477 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n478 are not attained by the paper. ", "page_idx": 12}, {"type": "text", "text": "479 2. Limitations ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 12}, {"type": "text", "text": "Justification: See conclusion. We think a good strategy which we don\u2019t mention in this paper can help improving the performance based on our work. ", "page_idx": 12}, {"type": "text", "text": "4 Guidelines:   \n85 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n86 the paper has limitations, but those are not discussed in the paper.   \n87 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n88 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n89 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n90 model well-specification, asymptotic approximations only holding locally). The authors   \n91 should reflect on how these assumptions might be violated in practice and what the   \n92 implications would be.   \n3 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n4 only tested on a few datasets or with a few runs. In general, empirical results often   \n5 depend on implicit assumptions, which should be articulated.   \n96 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \n7 For example, a facial recognition algorithm may perform poorly when image resolution   \n98 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n99 used reliably to provide closed captions for online lectures because it fails to handle   \n00 technical jargon.   \n1 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n02 and how they scale with dataset size.   \n03 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n04 address problems of privacy and fairness.   \n05 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n6 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n7 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n8 judgment and recognize that individual actions in favor of transparency play an impor  \n9 tant role in developing norms that preserve the integrity of the community. Reviewers   \n0 will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 12}, {"type": "text", "text": "511 3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "512 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n513 a complete (and correct) proof? ", "page_idx": 12}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 12}, {"type": "text", "text": "515 Justification: We don\u2019t think our work involves that.   \n516 Guidelines: ", "page_idx": 13}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 13}, {"type": "text", "text": "527 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "528 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n529 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n530 of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 13}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 13}, {"type": "text", "text": "532 Justification: See experimental settings.   \n533 Guidelines: ", "page_idx": 13}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 13}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "66 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n67 tions to faithfully reproduce the main experimental results, as described in supplemental   \n68 material?   \n569 Answer: [No]   \n570 Justification: We need time to organise this part, but we can make sure that we will release   \n571 our code if it is accepted.   \n572 Guidelines:   \n573 \u2022 The answer NA means that paper does not include experiments requiring code.   \n574 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n575 public/guides/CodeSubmissionPolicy) for more details.   \n576 \u2022 While we encourage the release of code and data, we understand that this might not be   \n577 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n578 including code, unless this is central to the contribution (e.g., for a new open-source   \n579 benchmark).   \n580 \u2022 The instructions should contain the exact command and environment needed to run to   \n581 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n582 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n583 \u2022 The authors should provide instructions on data access and preparation, including how   \n584 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n585 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n586 proposed method and baselines. If only a subset of experiments are reproducible, they   \n587 should state which ones are omitted from the script and why.   \n588 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n589 versions (if applicable).   \n590 \u2022 Providing as much information as possible in supplemental material (appended to the   \n591 paper) is recommended, but including URLs to data and code is permitted.   \n592 6. Experimental Setting/Details   \n593 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n594 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n595 results?   \n596 Answer: [Yes]   \n597 Justification: See experimental settings.   \n598 Guidelines:   \n599 \u2022 The answer NA means that the paper does not include experiments.   \n600 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n601 that is necessary to appreciate the results and make sense of them.   \n602 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n603 material.   \n604 7. Experiment Statistical Significance   \n605 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n606 information about the statistical significance of the experiments?   \n607 Answer: [No]   \n608 Justification: We follow the previous works.   \n609 Guidelines:   \n610 \u2022 The answer NA means that the paper does not include experiments.   \n611 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n612 dence intervals, or statistical significance tests, at least for the experiments that support   \n613 the main claims of the paper.   \n614 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n615 example, train/test split, initialization, random drawing of some parameter, or overall   \n616 run with given experimental conditions).   \n617 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n618 call to a library function, bootstrap, etc.)   \n619 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n620 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n621 of the mean.   \n622 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n623 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n624 of Normality of errors is not verified.   \n625 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n626 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n627 error rates).   \n628 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n629 they were calculated and reference the corresponding figures or tables in the text.   \n630 8. Experiments Compute Resources   \n631 Question: For each experiment, does the paper provide sufficient information on the com  \n632 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n633 the experiments?   \n634 Answer: [Yes]   \n635 Justification: See experimental settings.   \n636 Guidelines:   \n637 \u2022 The answer NA means that the paper does not include experiments.   \n638 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n639 or cloud provider, including relevant memory and storage.   \n640 \u2022 The paper should provide the amount of compute required for each of the individual   \n641 experimental runs as well as estimate the total compute.   \n642 \u2022 The paper should disclose whether the full research project required more compute   \n643 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n644 didn\u2019t make it into the paper).   \n645 9. Code Of Ethics   \n646 Question: Does the research conducted in the paper conform, in every respect, with the   \n647 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n648 Answer: [Yes]   \n649 Justification: We don\u2019t think our works in relation to this.   \n650 Guidelines:   \n651 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n652 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n653 deviation from the Code of Ethics.   \n654 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n655 eration due to laws or regulations in their jurisdiction).   \n656 10. Broader Impacts   \n657 Question: Does the paper discuss both potential positive societal impacts and negative   \n658 societal impacts of the work performed?   \n659 Answer: [NA]   \n660 Justification: We don\u2019t think our work involves that.   \n661 Guidelines:   \n662 \u2022 The answer NA means that there is no societal impact of the work performed.   \n663 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n664 impact or why the paper does not address societal impact.   \n665 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n666 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n667 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n668 groups), privacy considerations, and security considerations.   \n669 \u2022 The conference expects that many papers will be foundational research and not tied   \n670 to particular applications, let alone deployments. However, if there is a direct path to   \n671 any negative applications, the authors should point it out. For example, it is legitimate   \n672 to point out that an improvement in the quality of generative models could be used to   \n673 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n674 that a generic algorithm for optimizing neural networks could enable people to train   \n675 models that generate Deepfakes faster.   \n676 \u2022 The authors should consider possible harms that could arise when the technology is   \n677 being used as intended and functioning correctly, harms that could arise when the   \n678 technology is being used as intended but gives incorrect results, and harms following   \n679 from (intentional or unintentional) misuse of the technology.   \n680 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n681 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n682 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n683 feedback over time, improving the efficiency and accessibility of ML).   \n684 11. Safeguards   \n685 Question: Does the paper describe safeguards that have been put in place for responsible   \n686 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n687 image generators, or scraped datasets)?   \n688 Answer: [NA]   \n689 Justification: We don\u2019t think our work involves that.   \n690 Guidelines:   \n691 \u2022 The answer NA means that the paper poses no such risks.   \n692 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n693 necessary safeguards to allow for controlled use of the model, for example by requiring   \n694 that users adhere to usage guidelines or restrictions to access the model or implementing   \n695 safety filters.   \n696 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n697 should describe how they avoided releasing unsafe images.   \n698 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n699 not require this, but we encourage authors to take this into account and make a best   \n700 faith effort.   \n701 12. Licenses for existing assets   \n702 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n703 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n704 properly respected?   \n705 Answer: [Yes]   \n706 Justification: We used publicly available datasets whose licenses allow research usage.   \n707 Guidelines:   \n708 \u2022 The answer NA means that the paper does not use existing assets.   \n709 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n710 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n711 URL.   \n712 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n713 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n714 service of that source should be provided.   \n715 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n716 package should be provided. For popular datasets, paperswithcode.com/datasets   \n717 has curated licenses for some datasets. Their licensing guide can help determine the   \n718 license of a dataset.   \n719 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n720 the derived asset (if it has changed) should be provided.   \n721 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n722 the asset\u2019s creators.   \n723 13. New Assets   \n724 Question: Are new assets introduced in the paper well documented and is the documentation   \n725 provided alongside the assets?   \n726 Answer: [NA]   \n727 Justification: We don\u2019t think our work involves that.   \n728 Guidelines:   \n729 \u2022 The answer NA means that the paper does not release new assets.   \n730 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n731 submissions via structured templates. This includes details about training, license,   \n732 limitations, etc.   \n733 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n734 asset is used.   \n735 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n736 create an anonymized URL or include an anonymized zip file.   \n737 14. Crowdsourcing and Research with Human Subjects   \n738 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n739 include the full text of instructions given to participants and screenshots, if applicable, as   \n740 well as details about compensation (if any)?   \n741 Answer: [NA]   \n742 Justification: We don\u2019t think our work involves that.   \n743 Guidelines:   \n744 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n745 human subjects.   \n746 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n747 tion of the paper involves human subjects, then as much detail as possible should be   \n748 included in the main paper.   \n749 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n750 or other labor should be paid at least the minimum wage in the country of the data   \n751 collector.   \n752 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n753 Subjects   \n754 Question: Does the paper describe potential risks incurred by study participants, whether   \n755 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n756 approvals (or an equivalent approval/review based on the requirements of your country or   \n757 institution) were obtained?   \n758 Answer: [NA]   \n759 Justification: We don\u2019t think our work involves that.   \n760 Guidelines:   \n761 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n762 human subjects.   \n763 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)   \n764 may be required for any human subjects research. If you obtained IRB approval, you   \n765 should clearly state this in the paper.   \n766 \u2022 We recognize that the procedures for this may vary significantly between institutions   \n767 and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the   \n768 guidelines for their institution.   \n769 \u2022 For initial submissions, do not include any information that would break anonymity (if   \n770 applicable), such as the institution conducting the review. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}]