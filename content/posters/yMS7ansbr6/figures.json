[{"figure_path": "yMS7ansbr6/figures/figures_0_1.jpg", "caption": "Figure 1: A visualization comparison between common deepfakes and our studied lip-syncing deepfakes (LipSync). The former exhibits a substantial forgery area and identity manipulation, such as face or gender swapping, whereas the latter, relies on the synchronization of the minor lip region and given audio, without any alterations to the subject's identity. As illustrated in the comparison above, discerning the authenticity of an image sequence becomes arduous in the absence of labels.", "description": "This figure compares different types of deepfakes.  The top row shows examples of facial-editing deepfakes, which involve significant alterations such as face swapping, gender swapping, and age regression. The bottom two rows show examples of lip-syncing deepfakes, where the only manipulation is the synchronization of lip movements with a given audio.  The figure highlights the difficulty of detecting lip-syncing deepfakes because they don't involve obvious visual artifacts or identity changes.", "section": "Introduction"}, {"figure_path": "yMS7ansbr6/figures/figures_2_1.jpg", "caption": "Figure 2: (a) shows the correlation between lip movements and corresponding spectrogram in genuine pattern. When the woman starts talking, the middle and high frequencies in the spectrum are lighted. Over time, the energy gradually fades and shifts from middle to lower frequencies. (b) the first two frames show a highlighted high-frequency spectrum, contradicting the man not speaking. In the third frame, an unexpected lip opening appears at the darkest part of the spectrum. The mouth cannot change so drastically within a single frame, and this lip shape contradicts the spectrum information.", "description": "This figure shows a comparison between real and fake lip-sync videos. The top part displays spectrograms which represent audio frequencies. The bottom part shows a sequence of frames. (a) shows a real video: a woman is speaking, the spectrogram shows that frequencies increase at the beginning of her speech and gradually decrease. (b) shows a fake video: a man's mouth movements are not synchronized with the audio spectrogram.", "section": "2 Related Work"}, {"figure_path": "yMS7ansbr6/figures/figures_3_1.jpg", "caption": "Figure 3: AVLips dataset construction. Utilizing static and dynamic methods, we generated high-quality videos with realistic lip movements. The diverse dataset includes various real-world scenarios. Perturbations were applied for robust model training.", "description": "The figure illustrates the process of creating the AVLips dataset. It starts with data samples from LRS3, FaceForensics++, DFDC, and real-world videos.  These videos and their corresponding audio are processed using static (MakeItTalk) and dynamic (Wav2Lip, TalkLip) LipSync generation methods.  Noise reduction is applied to the audio before video generation.  The resulting videos undergo several types of perturbations (noise, compression, etc.) for robustness. The final dataset includes diverse real-world scenarios and various lip movements.", "section": "3 LipSync Forgery Dataset"}, {"figure_path": "yMS7ansbr6/figures/figures_4_1.jpg", "caption": "Figure 4: Overview of LipFD framework. Blue components represent our main modules in LipFD. The input image was generated by pre-processing, which consists of T frames in the target video and their audio spectrogram. (a) The aim of Global Feature Encoder, a self-attention model, is to extract long-term information between video frames and audio, finding unreasonable correspondences between lip movements and audio. (b) EGR encodes three series of crops, focusing on different parts for each region, and concatenates them with global feature FG. (c) The Region Awareness module assigns corresponding weights to the features based on their importance. (d) All features are fused together into a unified representation F based on their respective weights for final inference.", "description": "This figure presents a detailed overview of the LipFD framework, highlighting its core modules and workflow. LipFD processes both video and audio inputs to detect lip-syncing inconsistencies. It consists of four main components: 1) Global Feature Encoder, which captures long-term audio-visual relationships using self-attention; 2) EGR (Encoder for Global-Region), encoding features from different facial regions; 3) Region Awareness, dynamically adjusting the attention weights based on region importance; 4) Fusion and Classification, combining features for the final decision. ", "section": "4 Method"}, {"figure_path": "yMS7ansbr6/figures/figures_7_1.jpg", "caption": "Figure 5: Robustness against various unseen corruptions. Average AUC scores across five intensity levels for various corruptions. For detailed analysis, please refer to the appendix.", "description": "This figure demonstrates the robustness of the proposed LipFD model against various image corruptions.  The AUC (Area Under the Curve) scores are shown for five different corruption types (Contrast, Saturation, Blur, Noise, Pixelation, JPEG Compression) across five different intensity levels. Each line represents the performance of a different model (RealForensics, LipForensics, and the proposed LipFD model).  The higher the AUC score, the better the model's performance in the presence of corruption. The appendix provides a more detailed breakdown of these results.", "section": "5.4 Robustness Evaluation"}, {"figure_path": "yMS7ansbr6/figures/figures_7_2.jpg", "caption": "Figure 6: Performance in real scenarios. The x-axis represents network delay time, where a higher delay indicates a degradation in image transmission quality and clarity. Consequently, this degradation adversely impacts the audio-video synchronization in WeChat video calls.", "description": "The figure shows the performance of the proposed LipFD method in real-world scenarios, specifically WeChat video calls and streaming media, under varying network delays (100ms, 200ms, and 500ms).  The results are presented as accuracy percentages.  The visualization demonstrates that the accuracy of the model in detecting LipSync decreases as network delay increases, especially in WeChat video calls. This is because increased latency leads to desynchronization between audio and video, which the model relies on for detection. The performance on streaming media is less affected by the delay.", "section": "5.5 Performance in Real Scenarios"}, {"figure_path": "yMS7ansbr6/figures/figures_8_1.jpg", "caption": "Figure 7: Content sensitivity. Left is real, right is fake. Visualization of the gradients from the last layer of the Global-Region encoder, which reflects the regions LipFD relies on.", "description": "This figure visualizes the gradients from the last layer of the Global-Region encoder in LipFD, a deepfake detection model.  The gradients highlight the image regions that most strongly influence the model's prediction.  The left column shows real videos, while the right column shows fake videos. The visualization shows that for real videos, the model focuses on a broader area including head, face and lip regions, indicating that it considers the overall consistency and coherence of these regions in the real video.  However, for fake videos, the model's attention is strongly focused on the lip region, highlighting the discrepancies between the real lip movements and those generated by the LipSync deepfake method. The differences in attention highlight the model's ability to differentiate between authentic and synthetic lip movements based on audio-visual consistency.", "section": "4.2 Region Awareness"}, {"figure_path": "yMS7ansbr6/figures/figures_14_1.jpg", "caption": "Figure 9: Expended data samples. Each sample consists of T frames of video images and their corresponding audio spectra, serving as a temporal representation of the audio-visual context.", "description": "This figure shows examples of expanded data samples from the AVLips dataset. Each sample is composed of a sequence of T frames from a video and its corresponding audio spectrogram.  This visual representation is designed to capture the temporal relationship between the visual lip movements and the audio signal, which is crucial for detecting inconsistencies in lip-synced deepfakes. The figure displays samples from four different sources: FF++, DFDC, LRS3, and real-world videos, showcasing the diversity of the dataset.", "section": "A AVLips Dataset"}, {"figure_path": "yMS7ansbr6/figures/figures_15_1.jpg", "caption": "Figure 10: Perturbed samples and average results. Real / Fake videos are corrupted using common perturbation methods at intensity level 3, followed by the extraction of video frames to obtain samples. Average AUC is the evaluation metric, indicating better robustness of detectors with higher values. R.F. stands for RealForensics detection.", "description": "This figure shows the robustness of the proposed method (LipFD) and the RealForensics method against common image corruptions.  Seven types of corruptions were applied to both real and fake videos at intensity level 3: Block Wise, Contrast, Saturation, Gaussian Blur, Gaussian Noise, Pixelation, and Compression. For each corruption type, sample images of real and fake videos are shown, followed by the AUC (Area Under the Curve) scores for both LipFD and RealForensics.  The results demonstrate that LipFD maintains significantly higher AUC scores across all corruption types, indicating better robustness against various distortions.", "section": "5.4 Robustness Evaluation"}]