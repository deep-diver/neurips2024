[{"figure_path": "bjFhVbky5A/figures/figures_2_1.jpg", "caption": "Figure 1: Mixture-of-Experts on a single GPU.", "description": "This figure illustrates the architecture of a Mixture-of-Experts (MoE) model on a single GPU.  The input (x) is fed into a gating network, which determines which expert networks should be activated for processing that input. The gating network's output G: R<sup>M</sup> \u2192 [1,N]<sup>K</sup> indicates the subset of experts selected, and the selected experts (E<sub>1</sub>(x), E<sub>2</sub>(x), ..., E<sub>n</sub>(x)) process the input and then their results are combined to produce the final output, f(x).  The figure shows a single-GPU setup, illustrating the basic structure of a MoE layer.", "section": "2.1 Mixtures-of-Expert Architecture"}, {"figure_path": "bjFhVbky5A/figures/figures_2_2.jpg", "caption": "Figure 2: Training Mixture-of-Experts on multiple GPUs as expert parallelism.", "description": "This figure illustrates the process of training a Mixture-of-Experts (MoE) model using expert parallelism across multiple GPUs.  The input data (tokens x0, x1, x2, x3) is first processed by a gating network on each GPU to determine which expert(s) each token should be routed to.  The green arrows represent communication within a single node (GPU), while the red arrows show inter-node communication (between GPUs).  All-to-all communication is required to send data to the appropriate expert GPUs and to collect results after the computations have finished.  This process highlights the communication overhead associated with this training approach, which is the major focus of the paper.", "section": "2.2 Challenges of Scaling MoE Model Training"}, {"figure_path": "bjFhVbky5A/figures/figures_3_1.jpg", "caption": "Figure 3: Proportion of all-to-all communication time relative to total training duration across different configurations: scaling the number of training servers (Figure 3(b)) and scaling the parameter size of models (Figure 3(c)).", "description": "This figure shows the proportion of time spent on all-to-all communication during the training of three different MoE models (ROBERTa-MoE, GPT-MoE, and Swin-MoE-L) across various configurations.  Subfigure (a) shows the baseline with 16 GPUs. Subfigure (b) doubles the number of GPUs to 32, demonstrating the impact on communication overhead as the cluster size scales. Subfigure (c) doubles the number of experts within the model on a 16 GPU cluster to show the effect of increasing model complexity. The results indicate that all-to-all communication constitutes a significant portion of the total training time, hindering scalability.", "section": "Challenges of Scaling MoE Model Training"}, {"figure_path": "bjFhVbky5A/figures/figures_4_1.jpg", "caption": "Figure 5: Schematic of MoE training with Locality-Sensitive Hashing (LSH-MoE).", "description": "This figure illustrates the LSH-MoE framework.  The process begins with Locality-Sensitive Hashing (LSH)-based clustering of tokens, grouping similar tokens into buckets and representing each bucket by its centroid. These centroids are then sent through the all-to-all communication phase to experts for processing, producing E(centroids). Another all-to-all communication phase returns the results. Finally, a residual-based error compensation step combines E(centroids) with the residuals (differences between original tokens and their centroids) to generate the final output E(tokens), approximating the results if all tokens were processed individually.", "section": "3.2 LSH-MOE"}, {"figure_path": "bjFhVbky5A/figures/figures_4_2.jpg", "caption": "Figure 4: Principal Component Analysis (PCA) Visualization of input tokens involved in all-to-all communication.", "description": "This figure shows the results of applying Principal Component Analysis (PCA) to reduce the dimensionality of input tokens involved in all-to-all communication during MoE training.  The visualization reveals distinct clusters of similar tokens, suggesting a high degree of token similarity. This similarity is attributed to both inherent characteristics of real-world data and the model architecture's attention mechanism, which enhances semantic relationships between tokens.  The presence of these clusters motivates the use of Locality-Sensitive Hashing (LSH) for efficient data compression in the proposed LSH-MoE model.", "section": "3.1 The Motivation of Token Similarity"}, {"figure_path": "bjFhVbky5A/figures/figures_8_1.jpg", "caption": "Figure 6: Comparative analysis of convergence performance. This includes a comparison between the original models, LSH-MoE without Error Compensation, and LSH-MoE implementations. The perplexity curves are applied 1D Gaussian smoothing with \u03c3 = 0.5.", "description": "This figure shows a comparison of the training convergence speed between the original models and the proposed LSH-MoE model for two different language models (RoBERTa-MoE and T5-MoE).  The x-axis represents the training time (wall time in hours), while the y-axis represents the validation perplexity.  The figure demonstrates that LSH-MoE converges faster than the original model for both language models, achieving speedups of 1.6x and 2.2x, respectively. It also shows that using error compensation significantly improves performance, as the model trained without it performs less efficiently.", "section": "4.4 Overall Performance"}, {"figure_path": "bjFhVbky5A/figures/figures_9_1.jpg", "caption": "Figure 7: An in-depth analysis of the compression rate and the model performance by adjusting the quantity and types of hash functions. The left and middle sub-figures are results for diverse quantities of hash functions. The right sub-figure is the result for diverse types of hash functions (CP for cross-polytope and SP for spherical) with different compression rates (20%, 15%, 10%).", "description": "This figure shows the ablation study results on the impact of the quantity and types of hash functions used in the LSH-MoE method.  The left column displays the accuracy achieved on MNLI and SST-2 datasets for different numbers of hash functions (2, 4, 6, 8, 10). The middle column shows the corresponding compression rates. The right column presents the results comparing two types of hash functions, cross-polytope (CP) and spherical (SP), at three different compression rates (10%, 15%, 20%).  The results demonstrate the optimal number of hash functions for balancing accuracy and compression.", "section": "4.5 Ablation Study"}]