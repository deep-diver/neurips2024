[{"figure_path": "QEaHE4TUgc/figures/figures_0_1.jpg", "caption": "Figure 1: Severe loss of plasticity in Procgen (Starpilot). There is a steady decline in reward with each distribution shift.", "description": "This figure shows the performance of an agent trained using a standard reinforcement learning algorithm on the Starpilot environment from the Procgen benchmark.  The x-axis represents the number of timesteps, and the y-axis represents the average reward obtained per episode.  The plot shows that the agent's performance steadily decreases with each distribution shift (change in the game's environment), indicating a significant loss of plasticity.  The agent struggles to adapt to the new environments after initial training, suggesting a limitation in its ability to retain previously learned knowledge and apply it to new scenarios. This highlights the key challenge of lifelong reinforcement learning: maintaining the ability to learn new tasks without forgetting previously acquired skills.", "section": "1 Introduction"}, {"figure_path": "QEaHE4TUgc/figures/figures_2_1.jpg", "caption": "Figure 2: Visualization of TRAC\u2019s key idea.", "description": "The figure illustrates the core concept of TRAC, a parameter-free optimizer for lifelong reinforcement learning.  It shows how TRAC uses a data-dependent regularizer to control the extent to which the iterates deviate from their initialization, thereby minimizing loss of plasticity. The 'tuner' (Algorithm 2) dynamically decides the strength of regularization, adapting to each task's distribution shifts and managing the drift of weights. The black curve represents the loss function, and the arrows show how the updated weights move to the new optimum after the distribution shift.", "section": "Method"}, {"figure_path": "QEaHE4TUgc/figures/figures_5_1.jpg", "caption": "Figure 3: Experimental setup for lifelong RL.", "description": "This figure illustrates the experimental setup for lifelong reinforcement learning (RL) across three different environments: Procgen, Atari, and Control.  In each environment, the agent is trained using the TRAC algorithm, which is the focus of the paper.  The figure visually represents how distribution shifts are introduced (new levels in Procgen, new games in Atari, and noise added to observations in Control) and how the TRAC optimizer helps the agent adapt to these shifts.  The images show examples of the visual inputs the agent receives in each environment.", "section": "4 Experiment"}, {"figure_path": "QEaHE4TUgc/figures/figures_6_1.jpg", "caption": "Figure 4: Reward in the lifelong Procgen environments for StarPilot, Dodgeball, Fruitbot, and Chaser. There is a steady loss of plasticity in agents using ADAM PPO and CReLU, characterized by their inability to maintain performance through succesive Procgen levels. In contrast, TRAC avoids this loss of plasticity, quickly achieving high performance with each new task.", "description": "This figure compares the performance of three different algorithms (ADAM PPO, TRAC PPO, and CReLU) across four different Procgen environments (StarPilot, Dodgeball, Fruitbot, and Chaser) in a lifelong learning setting.  Each environment presents a sequence of levels representing distribution shifts. The graph shows the mean episode rewards over time for each algorithm in each environment. ADAM PPO and CReLU exhibit a steady decline in performance as new levels are introduced, indicating a loss of plasticity. In contrast, TRAC PPO maintains or improves performance across levels, demonstrating its ability to adapt to distribution shifts and mitigate the loss of plasticity.", "section": "4 Experiment"}, {"figure_path": "QEaHE4TUgc/figures/figures_7_1.jpg", "caption": "Figure 3: Experimental setup for lifelong RL.", "description": "This figure shows the experimental setup used in the paper for lifelong reinforcement learning. It illustrates how distribution shifts are introduced in three different environments: Procgen, Atari, and Gym Control.  In Procgen, distribution shifts are created by changing levels of the game. In Atari, distribution shifts occur by switching between different games. Finally, in Gym Control, distribution shifts are introduced by adding noise to observations during specific time intervals. The figure highlights the varied nature of distribution shifts across different environments used for the lifelong RL experiment in the paper.", "section": "4 Experiment"}, {"figure_path": "QEaHE4TUgc/figures/figures_7_2.jpg", "caption": "Figure 6: Reward performance across CartPole, Acrobot, and LunarLander Gym Control tasks. Both ADAM PPO and CReLU experience extreme plasticity loss, failing to recover after the initial distribution shift. Conversely, TRAC PPO successfully avoids such plasticity loss, rapidly adapting when facing extreme distribution shifts.", "description": "This figure compares the performance of ADAM PPO, CReLU, and TRAC PPO on three Gym Control environments: CartPole-v1, Acrobot-v1, and LunarLander-v2.  Each environment is subjected to extreme distribution shifts. The results demonstrate that ADAM PPO and CReLU suffer significant plasticity loss, failing to recover after the initial shift. In contrast, TRAC PPO successfully avoids this loss and rapidly adapts to the new distribution.  This highlights TRAC's ability to mitigate the negative impact of prior learning on new tasks.", "section": "4 Experiment"}, {"figure_path": "QEaHE4TUgc/figures/figures_9_1.jpg", "caption": "Figure 7: For each Gym Control environment and the initial ten tasks, we identified the best \u03bb, which is the regularization strength that maximizes reward for each task's specific distribution shift. We also determined the best overall (well-tuned) \u03bb for each environment. The results demonstrate that each environment and each task's distribution shift is sensitive to different \u03bb and that TRAC PPO performs competitively with each environment's well-tuned \u03bb.", "description": "This figure visualizes the sensitivity of hyperparameter \u03bb (regularization strength) in the L2 regularization approach across different Gym Control environments (CartPole, Acrobot, LunarLander) and distribution shifts (tasks).  The bar chart shows the optimal \u03bb values for each task within each environment, highlighting that the optimal \u03bb significantly varies depending on both the environment and the specific task.  The line plots further illustrate the performance of TRAC PPO in comparison to the well-tuned \u03bb for each environment. This comparison demonstrates TRAC PPO's ability to achieve competitive performance without the need for hyperparameter tuning.", "section": "4 Experiment"}, {"figure_path": "QEaHE4TUgc/figures/figures_13_1.jpg", "caption": "Figure 8: Reward comparison of TRAC PPO, ADAM PPO, and privileged weight-resetting on Cartpole-v1, Acrobot-v1, and LunarLander-v2. TRAC PPO encourages positive transfer between tasks.", "description": "This figure compares the performance of TRAC PPO, ADAM PPO, and a privileged weight-reset baseline across three Gym control tasks: CartPole-v1, Acrobot-v1, and LunarLander-v2.  The privileged weight-reset baseline represents an agent that knows when a distribution shift occurs and resets its parameters to random values at the start of each new task.  The figure shows that TRAC PPO consistently achieves higher rewards than both ADAM PPO and the privileged weight-reset baseline, even at the peak learning phases of the privileged approach. Importantly, TRAC PPO's reward does not decline to the reward level observed at the start of new tasks with the privileged weight-resetting baseline, indicating that TRAC effectively transfers skills positively between tasks.", "section": "Appendix A TRAC Encourages Positive Transfer"}, {"figure_path": "QEaHE4TUgc/figures/figures_14_1.jpg", "caption": "Figure 9: Reward in the lifelong Procgen environments for StarPilot, Dodgeball, Fruitbot, and Chaser with warmstarted TRAC PPO and warmstarted ADAM PPO. Inital performance of TRAC PPO is improved with warmstarting and continues to avoid loss of plasticity.", "description": "The figure shows the results of a lifelong reinforcement learning experiment on four Procgen environments: StarPilot, Dodgeball, Fruitbot, and Chaser.  Two different optimization methods are compared: ADAM PPO (a baseline) and TRAC PPO (the proposed method).  Both methods are tested with and without warmstarting.  The graphs plot mean episode reward over time.  The main observation is that TRAC PPO, especially when warmstarted, consistently outperforms ADAM PPO and maintains high performance across multiple distribution shifts, showing resilience to the common problem of loss of plasticity (where previous learning hinders adaptation to new tasks).  Warmstarting provides a slight initial performance boost for both methods, but TRAC PPO's advantage remains significant.", "section": "4 Experiment"}, {"figure_path": "QEaHE4TUgc/figures/figures_14_2.jpg", "caption": "Figure 3: Experimental setup for lifelong RL.", "description": "This figure shows the experimental setup used in the paper for lifelong reinforcement learning. It includes three different environments: Procgen, Atari, and Control. Each environment has a different way of introducing distribution shifts. Procgen introduces distribution shifts by generating new levels after every 2 million time steps. Atari introduces distribution shifts by switching to a new game every 4 million time steps. Control introduces distribution shifts by perturbing each observation dimension with random noise every 200 steps. The figure also shows that TRAC PPO is used in all three environments.", "section": "4 Experiment"}, {"figure_path": "QEaHE4TUgc/figures/figures_15_1.jpg", "caption": "Figure 8: Reward comparison of TRAC PPO, ADAM PPO, and privileged weight-resetting on Cartpole-v1, Acrobot-v1, and LunarLander-v2. TRAC PPO encourages positive transfer between tasks.", "description": "This figure compares the performance of TRAC PPO, ADAM PPO, and a privileged weight-resetting baseline across three Gym control tasks: CartPole-v1, Acrobot-v1, and LunarLander-v2.  The privileged baseline represents a scenario where the agent is given the advantage of knowing when a distribution shift will occur and resets its parameters to a random initialization at the start of each new task. The results demonstrate that TRAC PPO consistently maintains higher rewards than the privileged baseline, even at its peak learning phases, avoiding the sharp drops in reward observed at the beginning of new tasks for the privileged approach. This suggests that TRAC successfully transfers skills positively between the tasks.", "section": "Appendix A TRAC Encourages Positive Transfer"}, {"figure_path": "QEaHE4TUgc/figures/figures_16_1.jpg", "caption": "Figure 3: Experimental setup for lifelong RL.", "description": "The figure shows the experimental setup for lifelong reinforcement learning using three different environments: Procgen, Atari, and Gym Control.  Each environment presents unique challenges for an RL agent. Procgen provides procedurally generated games with distribution shifts introduced by changing game levels. Atari presents classic arcade games, where distribution shifts are introduced by switching between games. Gym Control provides physics-based control tasks, where distribution shifts are introduced through changes in environment dynamics. This setup allows for a comprehensive evaluation of the TRAC algorithm's ability to handle diverse types of distribution shifts in lifelong RL.", "section": "4 Experiment"}, {"figure_path": "QEaHE4TUgc/figures/figures_16_2.jpg", "caption": "Figure 3: Experimental setup for lifelong RL.", "description": "The figure shows the experimental setup for lifelong reinforcement learning.  It displays three different environments: Procgen, Atari, and Control. Each environment has a series of tasks with varying observation noise. The figure illustrates how distribution shifts are introduced in each environment and that TRAC-PPO is used as the proposed method compared to the baseline. It also shows that each level represents a distinct task.", "section": "4 Experiment"}, {"figure_path": "QEaHE4TUgc/figures/figures_17_1.jpg", "caption": "Figure 14: Mean Episode Reward for ADAM PPO on CartPole-v1 with varying gravity. ADAM PPO demonstrates robust policy recovery across most gravity-based distribution shifts.", "description": "The figure shows the mean episode rewards for ADAM PPO in the CartPole-v1 environment with varying gravity. The gravity is manipulated periodically, introducing distribution shifts. ADAM PPO demonstrates a remarkable ability to quickly adapt to and recover from these changes in gravity.  The shaded area represents the standard deviation.", "section": "Experiment"}, {"figure_path": "QEaHE4TUgc/figures/figures_17_2.jpg", "caption": "Figure 15: Performance comparison of plasticity loss mitigation techniques across Gym Control environments. Both layer normalization and plasticity injection reduce plasticity loss when applied with ADAM. TRAC outperforms both layer norm ADAM and plasticity injection ADAM, with the combination of layer norm and TRAC achieving the highest performance.", "description": "This figure compares the performance of different methods for mitigating plasticity loss in three Gym Control environments: CartPole-v1, Acrobot-v1, and LunarLander-v2.  It shows that while Layer Normalization and Plasticity Injection (both used with the ADAM optimizer) help reduce the performance decline caused by plasticity loss, TRAC consistently outperforms these methods. The best performance is achieved when TRAC is combined with Layer Normalization.  The graph visually shows the average reward over multiple runs for each method in each environment. The bar chart summarizes the average normalized rewards across all environments.", "section": "Mitigating plasticity loss across policy methods"}, {"figure_path": "QEaHE4TUgc/figures/figures_18_1.jpg", "caption": "Figure 3: Experimental setup for lifelong RL.", "description": "The figure shows the experimental setup used for lifelong reinforcement learning (RL).  It depicts three different environments: Procgen, Atari, and Gym Control. In each environment, distribution shifts are introduced by changing the game level (Procgen), game (Atari), or adding noise to observations (Gym Control). This setup allows for evaluating the performance of lifelong RL algorithms in diverse settings and under different types of distribution shifts.  The figure highlights the non-stationary nature of the problem and the need for algorithms that can adapt quickly to changes in the environment.", "section": "4 Experiment"}, {"figure_path": "QEaHE4TUgc/figures/figures_18_2.jpg", "caption": "Figure 17: Convergence of the scaling parameter  St+1  in the Procgen environments.", "description": "The figure visualizes the convergence behavior of the scaling parameter  St+1  across four different Procgen environments: Starpilot, Dodgeball, Chaser, and Fruitbot. The x-axis represents the update step, and the y-axis represents the value of  St+1. The shaded area indicates the standard deviation across multiple runs. The plots show that the scaling parameter generally converges to a relatively stable value within a range of 0.02 to 0.03 across all four environments, suggesting that the strength of regularization towards the initialization point is consistently maintained during the training process.", "section": "4 Experiment"}, {"figure_path": "QEaHE4TUgc/figures/figures_19_1.jpg", "caption": "Figure 17: Convergence of the scaling parameter St+1 in the Procgen environments.", "description": "This figure visualizes the convergence behavior of the scaling parameter (St+1) within the TRAC algorithm across four different Procgen environments (Starpilot, Dodgeball, Chaser, and Fruitbot). The x-axis represents the update step, and the y-axis shows the value of St+1. The plots illustrate the convergence of St+1 toward a relatively stable value within each environment, indicating the stability of the regularization mechanism implemented by TRAC. These stable values generally fall within the range of [0.02, 0.03], indicating the effective strength of regularization.", "section": "4 Experiment"}, {"figure_path": "QEaHE4TUgc/figures/figures_19_2.jpg", "caption": "Figure 19: Convergence of the scaling parameter St+1 in the Gym Control environments.", "description": "The figure shows the convergence behavior of the scaling parameter (St+1) across three different Gym Control environments: CartPole-v1, Acrobot-v1, and LunarLander-v2.  The x-axis represents the update step, while the y-axis shows the value of St+1. The plots illustrate how this crucial parameter, which regulates the strength of regularization toward the initialization points, behaves across the various control environments.  In essence, it visually demonstrates the parameter's convergence during the optimization process within each environment.", "section": "H Experimental Setup"}]