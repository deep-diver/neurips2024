[{"figure_path": "mdK1vhgpa5/figures/figures_1_1.jpg", "caption": "Figure 1: In CTTA task, a BNN model is first trained on a source dataset, and then is used to adapt to updated with unreliable priors, which may result in error accumulations.", "description": "This figure illustrates the continual test-time adaptation (CTTA) problem.  A Bayesian neural network (BNN) is initially trained on a source dataset. During the testing phase, the model encounters a sequence of domain shifts.  After each shift, the model is updated using only unlabeled data from the new domain. This process introduces uncertainty, leading to unreliable priors for subsequent updates, and resulting in error accumulation over time.  The figure highlights how using unreliable priors in adaptation steps leads to error propagation and accumulation.", "section": "1 Introduction"}, {"figure_path": "mdK1vhgpa5/figures/figures_3_1.jpg", "caption": "Figure 2: VCOTTA is built on mean-teacher structure, and conducts VI in CTTA using a mixture of teacher prior and source prior. The next teacher prior is updated by the exponential moving average.", "description": "The figure illustrates the architecture of the VCoTTA method, which is based on a mean-teacher structure. It shows how the student model's posterior is updated using variational inference (VI), incorporating both teacher prior and source prior information. The teacher prior is updated iteratively using an exponential moving average (EMA).", "section": "4 Adaptation and Inference in VCoTTA"}, {"figure_path": "mdK1vhgpa5/figures/figures_12_1.jpg", "caption": "Figure 1: In CTTA task, a BNN model is first trained on a source dataset, and then is used to adapt to updated with unreliable priors, which may result in error accumulations.", "description": "This figure illustrates the process of continual test-time adaptation (CTTA). A Bayesian Neural Network (BNN) model is initially trained on a source dataset. During testing, the model encounters a sequence of domain shifts, meaning the data distribution changes over time.  Each time there is a domain shift, the model adapts using only unlabeled data from the new domain. However, this process introduces uncertainty due to using only unlabeled data.  The use of unreliable priors (previous model parameter estimations which are now less certain) can lead to the accumulation of errors and reduced performance. The figure shows how the use of unreliable priors at each adaptation step leads to error accumulation in the final BNN model.", "section": "1 Introduction"}, {"figure_path": "mdK1vhgpa5/figures/figures_18_1.jpg", "caption": "Figure 1: In CTTA task, a BNN model is first trained on a source dataset, and then is used to adapt to updated with unreliable priors, which may result in error accumulations.", "description": "This figure illustrates the continual test-time adaptation (CTTA) process using a Bayesian Neural Network (BNN).  A pretrained BNN is first trained on a source dataset. Then, during the testing phase, the model encounters a sequence of domain shifts. Each shift introduces new, unlabeled data. The model attempts to adapt using these unlabeled samples, but because the data is unlabeled, there is considerable uncertainty. The model updates using unreliable priors, which can lead to error accumulation over time.  The accumulation of errors is shown visually with a chain of BNNs, each representing adaptation to a new domain, with the error growing after each domain shift.", "section": "1 Introduction"}, {"figure_path": "mdK1vhgpa5/figures/figures_19_1.jpg", "caption": "Figure 6: 10 loops under a same corruption order (CIFAR10C).", "description": "This figure shows the classification error rates over 10 consecutive loops using the same corruption order on the CIFAR10C dataset.  Each loop represents a complete cycle of testing and adaptation to the same sequence of corruption types. The graph illustrates how different continual test-time adaptation (CTTA) methods perform over multiple rounds of exposure to the same sequence of corruptions.  It demonstrates error accumulation or mitigation properties of the algorithms.", "section": "I Corruption Loops"}]