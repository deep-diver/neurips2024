[{"figure_path": "iSMTo0toDO/tables/tables_8_1.jpg", "caption": "Table 1: Results (mean absolute error) on MD17 force prediction. The best and second best results are marked in bold and underlined.", "description": "This table presents the mean absolute error (MAE) results for predicting molecular forces on the MD17 dataset.  Different molecular representation learning methods are compared, including various self-supervised and contrastive learning techniques. The best and second-best performing methods for each molecule are highlighted in bold and underlined, respectively.  The results showcase the effectiveness of SubgDiff in improving molecular representation learning compared to several established baselines.", "section": "5.1 SubgDiff improves molecular representation learning"}, {"figure_path": "iSMTo0toDO/tables/tables_8_2.jpg", "caption": "Table 2: Results for MoleculeNet (with 2D topology only). We report the mean (and standard deviation) ROC-AUC of three random seeds with scaffold splitting for each task. The backbone is GIN. The best and second best results are marked bold and underlined, respectively.", "description": "This table presents the results of applying different pre-training methods on eight molecular property prediction tasks from the MoleculeNet dataset.  Only the 2D topological information of the molecules was used. The table shows the mean and standard deviation of the area under the ROC curve (AUC) for three different random seeds, using scaffold splitting for data division. The backbone graph neural network used was GIN. The best and second-best performing methods are highlighted.", "section": "5.1 SubgDiff improves molecular representation learning"}, {"figure_path": "iSMTo0toDO/tables/tables_8_3.jpg", "caption": "Table 3: Results for conformation generation on GEOM-QM9 dataset with different diffusion timesteps. DDPM [9] is the sampling method used in GeoDiff. Our proposed sampling method (Algorithm 2) can be viewed as a DDPM variant. / denotes SubgDiff outperforms/underperforms GeoDiff.", "description": "This table presents the results of conformation generation experiments conducted on the GEOM-QM9 dataset using different diffusion models and sampling methods.  It compares GeoDiff (using DDPM) and SubgDiff (using a modified DDPM) across various numbers of diffusion steps (5000, 500, and 200). The performance is evaluated using four metrics: COV-R, MAT-R, COV-P, and MAT-P, which measure the quality and accuracy of the generated molecular conformations. The results show that SubgDiff generally outperforms GeoDiff, especially with fewer diffusion steps, suggesting improved efficiency and better generation quality.", "section": "5.2 SubgDiff benefits conformation generation"}, {"figure_path": "iSMTo0toDO/tables/tables_9_1.jpg", "caption": "Table 4: Results on the GEOM-QM9 dataset for domain generalization. Except for GeoDiff and SubgDiff, the other methods are trained with in-domain data.", "description": "This table presents the results of domain generalization experiments conducted on the GEOM-QM9 dataset.  The models were either trained on the QM9 dataset (small molecules) and tested on the GEOM-Drugs dataset (larger molecules), or vice versa. The table compares the performance of SubgDiff to several baselines (CVGAE, GraphDG, CGCF, ConfVAE, GeoMol, and GeoDiff) across two metrics: COV-R (coverage recall) and MAT-R (matching root mean square deviation).  The results highlight SubgDiff's superior performance on domain generalization tasks, demonstrating its robustness and generalizability.", "section": "5.2 SubgDiff benefits conformation generation"}, {"figure_path": "iSMTo0toDO/tables/tables_15_1.jpg", "caption": "Table 5: Silhouette index (higher is better) of the molecule embeddings on Moleculenet dataset (with 2D topology only)", "description": "This table presents the Silhouette index, a metric used to measure the quality of clustering, for molecule embeddings generated using MoleculeSDE and the proposed SubgDiff method. Higher Silhouette index indicates better clustering quality and better representation learning of molecular structures. The results are shown for five different datasets from the MoleculeNet dataset.", "section": "A.2 Mask distribution"}, {"figure_path": "iSMTo0toDO/tables/tables_15_2.jpg", "caption": "Table 6: Additional hyperparameters of our SubgDiff.", "description": "This table presents the hyperparameters used in the SubgDiff model for the QM9 and Drugs datasets.  It shows the starting and ending values for the variance schedule (\u03b21, \u03b2T), the type of variance scheduler used ('sigmoid'), the total number of diffusion steps (T), the number of k-step same subgraph diffusion steps (k), the radius of the neighborhood considered for the interactions (\u03c4), the batch size used during training, and the total number of training iterations.", "section": "A.4 Settings and more results on molecular representation learning task"}, {"figure_path": "iSMTo0toDO/tables/tables_16_1.jpg", "caption": "Table 7: Additional hyperparameters of our SubgDiff with different timesteps.", "description": "This table lists the hyperparameters used for training SubgDiff with different numbers of diffusion steps.  It shows the starting and ending values of the variance schedule (\u03b2_1, \u03b2_T), the type of variance scheduler used, the total number of diffusion steps (T), the number of steps for k-step same subgraph diffusion (k), the temperature (\u03c4), batch size, and the number of training iterations for QM9 and Drugs datasets.", "section": "A.4 Settings and more results on molecular representation learning task"}, {"figure_path": "iSMTo0toDO/tables/tables_18_1.jpg", "caption": "Table 8: Results on 12 quantum mechanics prediction tasks from QM9. We take 110K for training, 10K for validation, and 11K for testing. The evaluation is mean absolute error (MAE), and the best and the second best results are marked in bold and underlined, respectively. The backbone is SchNet.", "description": "This table presents the results of 12 quantum mechanics prediction tasks using the QM9 dataset.  The model was trained on 110,000 molecules, validated on 10,000, and tested on 11,000. The evaluation metric is Mean Absolute Error (MAE). The best and second-best results for each task are highlighted.  The underlying neural network architecture used is SchNet.", "section": "5.1 SubgDiff improves molecular representation learning"}, {"figure_path": "iSMTo0toDO/tables/tables_18_2.jpg", "caption": "Table 8: Results on 12 quantum mechanics prediction tasks from QM9. We take 110K for training, 10K for validation, and 11K for testing. The evaluation is mean absolute error (MAE), and the best and the second best results are marked in bold and underlined, respectively. The backbone is SchNet.", "description": "This table presents the results of 12 quantum mechanics prediction tasks from the QM9 dataset.  The model was trained on 110,000 molecules, validated on 10,000, and tested on 11,000. The evaluation metric used is mean absolute error (MAE). The best and second-best results for each task are highlighted.  The SchNet architecture was used as the backbone for the model.", "section": "A.4.3"}, {"figure_path": "iSMTo0toDO/tables/tables_19_1.jpg", "caption": "Table 1: Results (mean absolute error) on MD17 force prediction. The best and second best results are marked in bold and underlined.", "description": "This table presents the results of the mean absolute error in predicting molecular forces on the MD17 dataset.  It compares the performance of SubgDiff against various baselines (including Type Prediction, Angle Prediction, 3D InfoGraph, InfoNCE, EBM-NCE, Denoising, GeoSSL, and MoleculeSDE). The best-performing method for each molecule is highlighted in bold, with the second-best underlined. This allows a direct comparison of SubgDiff's performance on a variety of molecular structures against established techniques in force prediction.", "section": "5.1 SubgDiff improves molecular representation learning"}, {"figure_path": "iSMTo0toDO/tables/tables_20_1.jpg", "caption": "Table 11: Results on the GEOM-Drugs dataset under different diffusion timesteps. DDPM [9] is the sampling method used in GeoDiff and Langevin dynamics [40] is a typical sampling method used in DPM. Our proposed sampling method (Algorithm 2) can be viewed as a DDPM variant. / denotes SubgDiff outperforms/underperforms GeoDiff. The threshold \u03b4 = 1.25\u00c5.", "description": "This table compares the performance of GeoDiff and SubgDiff on the GEOM-Drugs dataset using two different sampling methods (DDPM and Langevin dynamics) with different diffusion timesteps (500).  The results show the mean and median values of COV-R (%) and MAT-R (\u00c5).  The arrows indicate whether SubgDiff outperformed or underperformed GeoDiff.", "section": "5.2 SubgDiff benefits conformation generation"}, {"figure_path": "iSMTo0toDO/tables/tables_20_2.jpg", "caption": "Table 3: Results for conformation generation on GEOM-QM9 dataset with different diffusion timesteps. DDPM [9] is the sampling method used in GeoDiff. Our proposed sampling method (Algorithm 2) can be viewed as a DDPM variant. / denotes SubgDiff outperforms/underperforms GeoDiff.", "description": "This table presents the results of conformation generation experiments using the GEOM-QM9 dataset.  It compares the performance of SubgDiff, the proposed method, against GeoDiff, a baseline method, across various metrics (COV-R, MAT-R, COV-P, MAT-P). The table also shows the effects of varying the number of diffusion timesteps (500 and 5000).  The results demonstrate SubgDiff's superior performance and efficiency in conformation generation.", "section": "5.2 SubgDiff benefits conformation generation"}, {"figure_path": "iSMTo0toDO/tables/tables_20_3.jpg", "caption": "Table 4: Results on the GEOM-QM9 dataset for domain generalization. Except for GeoDiff and SubgDiff, the other methods are trained with in-domain data.", "description": "This table presents the results of a domain generalization experiment using the GEOM-QM9 dataset.  The goal was to evaluate how well different models generalize to out-of-domain data.  The models were trained either on the QM9 dataset (small molecules) or the Drugs dataset (larger molecules), and then tested on the opposite dataset.  The table shows that SubgDiff significantly outperforms other models, demonstrating its ability to generalize well across different molecular sizes and properties.  This highlights SubgDiff's robustness and effectiveness in molecular representation learning.", "section": "5.2 SubgDiff benefits conformation generation"}, {"figure_path": "iSMTo0toDO/tables/tables_21_1.jpg", "caption": "Table 1: Results (mean absolute error) on MD17 force prediction. The best and second best results are marked in bold and underlined.", "description": "This table presents the mean absolute error for force prediction on the MD17 dataset.  The model's performance is evaluated on eight different molecules (Aspirin, Benzene, Ethanol, Malonaldehyde, Naphthalene, Salicylic, Toluene, and Uracil). For each molecule, the mean absolute error is shown, along with a comparison of the performance of several different pre-training methods (random initialization, Type Prediction, Angle Prediction, 3D InfoGraph, InfoNCE, EBM-NCE, Denoising, GeoSSL, MoleculeSDE (VE), MoleculeSDE (VP)) and the proposed SubgDiff model. The best and second-best results for each molecule are highlighted in bold and underlined, respectively, illustrating the superior performance of the SubgDiff model.", "section": "5.1 SubgDiff improves molecular representation learning"}]