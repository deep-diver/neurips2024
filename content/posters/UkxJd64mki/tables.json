[{"figure_path": "UkxJd64mki/tables/tables_4_1.jpg", "caption": "Table 1: Experimental results on the math reasoning task. The numbers in parentheses represent the relative improvement compared to CoT-SC.", "description": "This table presents the performance of different methods on the math reasoning task, comparing their average accuracy across seven sub-datasets (Algebra, Prealgebra, Intermediate Algebra, Counting and Probability, Number Theory, Geometry, and Precalculus) of the MATH benchmark.  The results show StrategyLLM outperforming the other methods, particularly CoT-SC (Self-Consistency with Chain-of-Thought), which represents a strong baseline. The numbers in parentheses highlight the percentage improvement achieved by StrategyLLM over CoT-SC for each sub-dataset.", "section": "4 Experiments"}, {"figure_path": "UkxJd64mki/tables/tables_5_1.jpg", "caption": "Table 1: Experimental results on the math reasoning task. The numbers in parentheses represent the relative improvement compared to CoT-SC.", "description": "This table presents the performance of different methods on the MATH benchmark's math reasoning tasks.  It shows the average accuracy across seven sub-datasets (Algebra, Prealgebra, Intermediate Algebra, Counting and Probability, Number Theory, Geometry, and Precalculus) for various methods: Standard Prompting (SP), SolutionLLM, Chain-of-Thought (CoT), Self-Consistency with CoT (CoT-SC), and the proposed StrategyLLM (with three variants: StrategyLLM, StrategyLLM-SC, and StrategyLLM-ZS). The numbers in parentheses indicate the percentage improvement of each StrategyLLM variant over the CoT-SC baseline.", "section": "4 Experiments"}, {"figure_path": "UkxJd64mki/tables/tables_5_2.jpg", "caption": "Table 2: Experimental results on the commonsense, algorithmic, and symbolic reasoning tasks. The numbers in parentheses represent the relative improvement compared to CoT-SC.", "description": "This table presents the performance of different methods on three reasoning tasks: commonsense, algorithmic, and symbolic.  The methods compared include standard prompting (SP), SolutionLLM, chain-of-thought (CoT), self-consistent chain-of-thought (CoT-SC), and the proposed StrategyLLM (with three variants: StrategyLLM, StrategyLLM-SC, and StrategyLLM-ZS).  For each task, the average performance across multiple datasets is shown, along with the percentage improvement of StrategyLLM compared to the CoT-SC baseline.", "section": "4 Experiments"}, {"figure_path": "UkxJd64mki/tables/tables_6_1.jpg", "caption": "Table 3: Experimental results on two math reasoning datasets, namely AL and CP, with different groups of examples.", "description": "This table presents the experimental results obtained for two math reasoning datasets (AL and CP) using different groups of examples. The results are compared across several methods: Standard Prompting (SP), SolutionLLM, Chain-of-Thought (CoT), Self-Consistency with CoT (CoT-SC), StrategyLLM, StrategyLLM-SC, and StrategyLLM-ZS.  The table shows the performance of each method on the AL-dev, AL-random, CP-dev, and CP-random datasets, providing insights into the robustness of each approach with respect to variations in the training examples used.", "section": "4.1 Experimental Setup"}, {"figure_path": "UkxJd64mki/tables/tables_7_1.jpg", "caption": "Table 4: Experimental results of closed-source models on the CP, StrategyQA, and MA datasets. The numbers in parentheses represent the relative improvement compared to CoT-SC.", "description": "This table presents the performance comparison of different methods (SolutionLLM, CoT, CoT-SC, StrategyLLM, StrategyLLM-SC) on three different datasets (CP, StrategyQA, MA) using two closed-source language models (GPT-4 and Claude-3-Sonnet).  The results are presented as average scores across the datasets, with StrategyLLM-SC showing improvements over CoT-SC.  The improvements are quantified in percentage points in parentheses. ", "section": "4 Experiments"}, {"figure_path": "UkxJd64mki/tables/tables_7_2.jpg", "caption": "Table 2: Experimental results on the commonsense, algorithmic, and symbolic reasoning tasks. The numbers in parentheses represent the relative improvement compared to CoT-SC.", "description": "This table presents the performance comparison of different methods (SP, SolutionLLM, CoT, CoT-SC, StrategyLLM, StrategyLLM-SC, and StrategyLLM-ZS) across three reasoning tasks: commonsense reasoning, algorithmic reasoning, and symbolic reasoning.  Each task uses different datasets and metrics to evaluate the accuracy of the models.  The numbers in parentheses show the percentage improvement achieved by each method compared to the CoT-SC baseline.", "section": "4 Experiments"}, {"figure_path": "UkxJd64mki/tables/tables_8_1.jpg", "caption": "Table 6: Comparison of Plan-and-Solve, CoT+Strategy, and StrategyLLM.", "description": "This table compares the performance of three different methods (Plan-and-Solve, CoT+Strategy, and StrategyLLM) across three datasets (CP, StrategyQA, and MA).  Each method is evaluated with and without self-consistency (SC), which involves generating multiple solutions and taking a majority vote.  The average performance across the three datasets is shown for each method.", "section": "4 Experiments"}, {"figure_path": "UkxJd64mki/tables/tables_9_1.jpg", "caption": "Table 7: Average cost of prompt generation across four reasoning tasks.", "description": "This table presents the average cost of generating prompts for four reasoning tasks (Math, Commonsense, Algorithmic, and Symbolic) using two different versions of GPT-3.5-turbo.  The costs are broken down into input tokens, output tokens, and the monetary cost using each GPT model version.  It highlights the cost-effectiveness of the StrategyLLM framework across various tasks.", "section": "4.2 Main Results"}, {"figure_path": "UkxJd64mki/tables/tables_14_1.jpg", "caption": "Table 1: Experimental results on the math reasoning task. The numbers in parentheses represent the relative improvement compared to CoT-SC.", "description": "This table presents the performance comparison of different methods on the MATH reasoning benchmark.  It shows the average accuracy achieved by each method across seven sub-datasets (Algebra, Prealgebra, Intermediate Algebra, Counting and Probability, Number Theory, Geometry, and Precalculus) within the MATH benchmark.  The numbers in parentheses indicate the percentage improvement of each method compared to the baseline method, CoT-SC.", "section": "4.1 Experimental Setup"}, {"figure_path": "UkxJd64mki/tables/tables_14_2.jpg", "caption": "Table 9: Average inference cost of each test example across four reasoning tasks. # I and # O denote the number of input and output tokens, respectively.", "description": "This table shows the average number of input and output tokens consumed during inference for each reasoning task.  It compares the token usage for the SolutionLLM, CoT (Chain of Thought), and StrategyLLM methods, providing insight into the efficiency and resource requirements of each approach.  The '# I' column represents the average number of input tokens, while '# O' represents the average number of output tokens. The four reasoning tasks are Math, Commonsense, Algorithmic, and Symbolic.", "section": "4.2 Main Results"}, {"figure_path": "UkxJd64mki/tables/tables_15_1.jpg", "caption": "Table 1: Experimental results on the math reasoning task. The numbers in parentheses represent the relative improvement compared to CoT-SC.", "description": "This table presents the performance of different methods on the math reasoning task from the MATH benchmark.  It shows the average accuracy for each method across seven sub-datasets of varying difficulty (Algebra, Prealgebra, Intermediate Algebra, Counting and Probability, Number Theory, Geometry, and Precalculus).  The numbers in parentheses indicate the percentage improvement achieved by each method compared to the CoT-SC baseline.  The methods compared include standard prompting (SP), SolutionLLM, chain-of-thought prompting (CoT), self-consistent chain-of-thought (CoT-SC), and the proposed StrategyLLM with its self-consistent (StrategyLLM-SC) and zero-shot (StrategyLLM-ZS) variants.", "section": "4 Experiments"}, {"figure_path": "UkxJd64mki/tables/tables_15_2.jpg", "caption": "Table 11: Results of StrategyLLM using the best discovered strategy and the method employing inconsistent prompts.", "description": "This table compares the performance of StrategyLLM using its best-performing strategy against a method using inconsistent prompts.  The inconsistent prompt method randomly selects examples from different strategy-based prompts, demonstrating the negative impact of inconsistency on the model's performance. The table shows the average accuracy of both methods across three different datasets: CP, StrategyQA, and MA, representing mathematical, commonsense, and algorithmic reasoning, respectively. The results clearly demonstrate the superior performance of StrategyLLM with consistent prompts.", "section": "4 Experiments"}]