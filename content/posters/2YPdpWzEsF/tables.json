[{"figure_path": "2YPdpWzEsF/tables/tables_6_1.jpg", "caption": "Table 1: Results on benchmark designed for MLLMs. V-T Num means the visual tokens number. V-T Num influences the computation cost that the bigger the V-T Num the heavier the computation cost is. Speed here means the relative pre-training speed with respect to LLaVA-1.5.", "description": "This table presents the results of various models on benchmarks designed for Multimodal Large Language Models (MLLMs).  It compares the performance of the proposed AcFormer model against several existing models (MiniGPT-4, mPLUG-Owl2, InstructBLIP, LLaVA (v1), LLaMA-Adapter V2, Shikra, Qwen-VL, Qwen-VL-Chat, and LLaVA-1.5) across different LLM sizes (7B and 13B). Key metrics include accuracy on several benchmarks (Res, POPE, MME, MMB, MM-Vet) and the relative training speed compared to LLaVA-1.5. The number of visual tokens (V-T Num) used by each model is also shown, demonstrating the impact on computational cost. AcFormer achieves comparable or better accuracy with significantly reduced visual tokens, showcasing its efficiency.", "section": "4.2 Main Results"}, {"figure_path": "2YPdpWzEsF/tables/tables_6_2.jpg", "caption": "Table 2: Results on General VQA tasks. V-T Num means the visual tokens number. V-T Num influences the computation cost that the bigger the V-T Num the heavier the computation cost is. Speed here means the relative pre-training speed with respect to LLaVA-1.5.", "description": "This table presents the results of the proposed AcFormer model and several baseline models on general visual question answering (VQA) benchmarks.  It compares the average normalized accuracy and training speed for different models across several VQA datasets (TextVQA, GQA, VQAv2, VisWiz, SQAimg). The number of visual tokens (V-T Num) used by each model is also shown, highlighting the impact of reducing the number of visual tokens on computational cost.  The \"Speed (\u2191)\" column indicates the relative training speed compared to the LLaVA-1.5 baseline, demonstrating the efficiency gains achieved by AcFormer.", "section": "4.2 Main Results"}, {"figure_path": "2YPdpWzEsF/tables/tables_7_1.jpg", "caption": "Table 3: Ablation studies. \u201cPooling\u201d denotes direct pooling of visual token. \u201cPooling-PR\u201d employs the pooled tokens as queries for the Perceiver Resampler. \u201cRandom-PR\u201d means the Perceiver Resampler using randomly selected tokens from the vision feature map as query. \u201cPR\u201d refers to the Perceiver Resampler using learnable queries. \u201cAcFormer\u201d represents our proposed Anchor Former. V-T Num means the visual tokens number.", "description": "This table presents the ablation study results comparing different visual connectors: Pooling, Pooling-PR, Random-PR, PR, C-Abstractor, and AcFormer. It shows the performance of each connector on various benchmarks (TextVQA, GQA, MMB, MME) using different numbers of visual tokens (V-T Num).  The results illustrate the impact of different information aggregation strategies on the overall model performance and the effectiveness of the proposed AcFormer.", "section": "4.3 Ablation Results"}, {"figure_path": "2YPdpWzEsF/tables/tables_7_2.jpg", "caption": "Table 4: Ablation studies on whether to directly use the selected tokens as input.", "description": "This table presents ablation study results, comparing different methods for utilizing selected visual tokens in the Perceiver Resampler. It shows a comparison of the performance of using the selected tokens directly, using top-p selection, E-ViT, and the proposed AcFormer method, across various metrics (TextVQA, GQA, MMB, MME). The purpose is to demonstrate the effectiveness of the proposed anchor selection and aggregation method in the AcFormer model.", "section": "4.3 Ablation Results"}, {"figure_path": "2YPdpWzEsF/tables/tables_8_1.jpg", "caption": "Table 5: Ablation studies on the visual connector when scaling up the training data.", "description": "This table presents ablation studies comparing different visual connectors (PR, AcFormer, Top-P, E-ViT) under two different training data scales using OpenLLaMA-3B. It evaluates their performance on TextVQA, GQA, OKVQA, VQAv2, VizWiz, and MME benchmarks.  The dataset sizes used for pretraining and instruction finetuning are specified, highlighting the impact of larger datasets on model performance and demonstrating the effectiveness of the AcFormer.", "section": "4.3 Ablation Results"}, {"figure_path": "2YPdpWzEsF/tables/tables_15_1.jpg", "caption": "Table 6: Details on the training time. Pt Bz means the pre-train batch size. And IFT Bz means the instruction finetune batch size.", "description": "This table shows the training time of different models with various configurations. The training time is broken down into pre-training time and instruction finetuning time.  The table includes the model used, the large language model (LLM), the training resources (number of A100 80G GPUs), the number of visual tokens, the pre-training batch size, the instruction finetuning batch size, the pre-training time, and the instruction finetuning time.  This allows for a comparison of the training efficiency of different model and configuration.", "section": "4 Experiments"}, {"figure_path": "2YPdpWzEsF/tables/tables_16_1.jpg", "caption": "Table 7: Details on the chosen benchmark.", "description": "This table presents nine different benchmarks used to evaluate the performance of Multimodal Large Language Models (MLLMs). Each benchmark is described with its task description and evaluation metric. The benchmarks cover various aspects of MLLM capabilities including visual perception, complex reasoning, and knowledge integration.", "section": "4.1 Settings"}]