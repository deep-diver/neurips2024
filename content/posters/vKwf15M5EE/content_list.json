[{"type": "text", "text": "SegCSR: Weakly-Supervised Cortical Surfaces Reconstruction from Brain Ribbon Segmentations ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Deep learning-based cortical surface reconstruction (CSR) approaches typically   \n2 rely on supervision information provided by pseudo ground truth generated by   \n3 conventional CSR methods, subject to errors associated with the supervision in  \n4 formation and also increasing computational cost of training data preparation.We   \n5 propose a new method to jointly reconstruct multiple cortical surfaces using weak   \n6 supervision from brain MRI ribbon segmentation results. Our approach initializes a   \n7 midthickness surface, which is then deformed inward and outward to form the inner   \n8 (white matter) and outer (pial) cortical surfaces, respectively, by jointly learning   \n9 diffeomorphic flows by minimizing loss functions to optimize the surfaces towards   \n0 the boundaries of the cortical ribbon segmentation maps. Specifically, a boundary   \n11 surface loss drives the initialization surface to the inner and outer boundaries, while   \n2 an inter-surface normal consistency loss regularizes the pial surface in challenging   \n3 deep cortical sulci regions. Additional regularization terms are utilized to enforce   \n14 edge length uniformity and smoothness of the reconstructed surfaces. Our method   \n5 has been evaluated on two large-scale adult brain MRI datasets and one infant brain   \n16 MRI dataset, demonstrating comparable or superior performance in CSR in terms   \n17 of accuracy and surface regularity compared to alternative supervised deep learning   \n18 methods. ", "page_idx": 0}, {"type": "text", "text": "19 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "20 Cortical surface reconstruction (CSR) is a crucial step for both qualitative visualization and quan  \n21 titative characterization of cortical surfaces in imaging studies of brain morphology [15, 51], neu  \n22 rodegenerative diseases [6, 12, 43], and psychological disorders [42]. Well-established cortical   \n23 analysis pipelines, such as BrainSuite [48], FreeSurfer [17], Connectome Workbench [18], and   \n24 iBEAT V2.0 [52], have achieved significant success in reconstructing cortical surfaces from brain   \n25 MRI data. However, these pipelines typically involve multiple processing steps, including iterative   \n26 surface deformation and topology check and correction, resulting in lengthy processing time (e.g.,   \n27 \u223c6h/subject). Moreover, each pipeline requires meticulously tuned parameters, posing challenges for   \n28 generalization across diverse data domains, age groups, or acquisition protocols.   \n29 Deep learning (DL) approaches have significantly accelerated CSR, demonstrating orders of magni  \n30 tude faster inference speeds while maintaining high accuracy and topology correctness [8, 11, 13,   \n31 22, 26, 30\u201332, 41, 47, 54]. One line of research predicts implicit surface representations, such as   \n32 signed distance functions [13, 21] or level sets [41], from which 3D meshes are extracted using the   \n33 Marching Cube (MC) algorithm [27] and refined with topology correction algorithms [4] to detect   \n34 and rectify topology errors, ensuring that the reconstructed surface conforms to a sphere-like topology.   \n35 Another line of research focuses on learning explicit surface deformations, using methods such as   \n36 flow-based [8, 11, 22, 26, 47, 54] or NODE-based techniques [30, 31]), to deform an initial mesh   \n37 towards target cortical surfaces. However, all these methods heavily rely on supervision information   \n38 provided by pseudo ground truth (pGT) of cortical surfaces generated by conventional CSR methods ,   \n39 regardless of whether they use implicit or explicit surface representations. The prolonged processing   \n40 time for generating pGT surfaces limits the collection of sufficiently large datasets for training,   \n41 and a general pipeline capable of extracting pGT surfaces across various data domains (e.g., age,   \n42 modality) is currently lacking. Conversely, segmentation of brain structures is comparatively simpler,   \n43 inspiring us to explore avenues to eliminate the need for supervised learning in CSR and to generalize   \n44 DL-based CSR approaches to scenarios where ribbon segmentation results are readily available.   \n45 The key challenges for achieving accurate weakly supervised reconstruction of cortical surfaces   \n46 span three primary aspects. First, devising sub-voxel supervision signals presents a formidable   \n47 hurdle. While existing approaches can produce precise segmentations [7, 20, 29, 45, 52], voxel-level   \n48 representations may struggle to capture the intricate morphology of the cerebral cortex, especially   \n49 its thin and highly-folded structure, due to the partial volume effect (PVE) inherent in brain MRI   \n50 scans. This problem becomes particularly prominent in deep cortical sulci [17], where the two banks   \n51 of grooves nearly converge, or in low-resolution images [52], such as under-sampled or infant MRIs.   \n52 Second, effectively modeling the interdependence between multiple surfaces is crucial. Incorporating   \n53 this prior knowledge into the design of models and training algorithms can alleviate the complexity   \n54 of reconstructing both the inner (white matter) and outer (pial) surfaces, ensuring the spherical   \n55 topology of the reconstructed surfaces [8, 54]. However, in the absence of pGT, it becomes more   \n56 challenging to forcibly deform surfaces and less stable to optimize multiple surfaces concurrently.   \n57 Third, maintaining optimal surface topology is paramount. Mesh uniformity, smoothness, and   \n58 topology are susceptible to distortion during large deformations if networks are optimized based on   \n59 randomly sampled vertices in 3D space for dense volumetric fields.   \n60 In this paper, we introduce SegCSR, a novel weakly supervised DL framework aimed at reconstructing   \n61 multiple cortical surfaces using ribbon segmentations derived from brain MRIs. We address the   \n62 diffeomorphic deformation problem in a continues coordinate space, deforming the initialization   \n63 midthickness surface towards the target inner and outer surfaces via innovative loss functions.   \n64 Specifically, the boundary surface loss function based on the ribbon segmentations and the intensity   \n65 gradient loss function based on the raw image facilitate sub-voxel-level surface movement. The   \n66 inter-surface normal consistency loss function explicitly integrates the normal directions of the WM,   \n67 midthickness, and pial surfaces, thereby regularizing the pial surface in challenging deep cortical   \n68 sulci regions. Furthermore, we devise a customized edge length loss, in conjunction with the known   \n69 normal consistency loss, to ensure surface uniformity and smoothness. Our main contributions can   \n70 be summarized as follows:   \n71 \u2022 We propose a new weakly supervised paradigm for reconstructing multiple cortical surfaces,   \n72 reducing the dependence on pGT cortical surfaces in training, unlike existing DL methods.   \n73 \u2022 We design two loss functions to optimize the surfaces towards the boundary of the cortical   \n74 ribbon segmentation maps, along with regularization terms to enforce regularity of surfaces.   \n75 \u2022 We conduct extensive experiments on two large-scale adult brain MRI datasets and one   \n76 infant brain MRI dataset. Our new method achieves comparable or superior performance   \n77 compared to existing supervised DL-based CSR alternatives. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "78 2 Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "79 Cortical Surface Reconstruction (CSR). (I) Traditional CSR methods typically rely on empirically   \n80 defined automatic image/surface processing techniques to accomplish tissue segmentation (e.g., WM,   \n81 GM, cerebrospinal fluid (CSF)), hemisphere separation, subcortical fliling, topology correction, WM   \n82 surface reconstruction, and pial surface reconstruction sequentially. Established pipelines such as   \n83 FreeSurfer [17], BrainSuite [48], and HCP [18] are tailored for processing adult brain images, while   \n84 dHCP [34] and iBEAT V2.0 [52] are designed for neonatal brain images, which exhibit distinct   \n85 differences in intensity values, size, and shape compared to adult brains. Despite achieving sub-voxel   \n86 accuracy and maintaining spherical topology, the iterative surface deformation and topology check and   \n87 correction procedures lead to lengthy processing times. (II) DL-based CSR methods have significantly   \n88 enhanced reconstruction speed while preserving high accuracy. Approaches like SegRecon [19] and   \n89 DeepCSR [13] predict a signed distance map for implicit surface representation, embedding the target   \n90 surface as the zero level-set and extracting it using MC algorithms. However, these methods require   \n91 topology correction to eliminate artifacts and ensure spherical topology. Alternatively, PialNN [32],   \n92 TopoFit [22], Vox2cortex [8], the CorticalFlow series [26, 47], SurfFlow [11], CortexODE [31],   \n93 and CoCSR [54] leverage explicit representation to maintain good topology and overcome PVE by   \n94 learning volumetric or vertex-wise diffeomorphic deformations and progressively deforming genus-0   \n95 template meshes. However, both implicit and explicit methods heavily rely on the supervision of pGT   \n96 of cortical surfaces generated by traditional pipelines. Our proposed method is based on the explicit   \n97 representation but differs significantly from them by utilizing ribbon segmentation maps for weakly   \n98 supervising the model training process.   \n99 Weakly-/Un-supervised Mesh Reconstruction. Although geometric DL methods for general   \n100 computer vision tasks have been extensively studied, research on mesh reconstruction from 3D   \n101 images under weakly-/un-supervised settings is relatively underexplored. One approach involves   \n102 constructing mesh-to-image rasterizer loss functions, as demonstrated in [36], where 2D projection   \n103 views are extracted from predicted 3D meshes and compared with ground truth segmentations.   \n104 Another line of research, exemplified by [39], focuses on learning the correspondence between   \n105 a template image and a target image, which is then utilized to deform the template mesh to the   \n106 target location. However, these methods have primarily been applied to biomedical tasks involving   \n107 organs with relatively simple shapes, such as the liver and heart. But the cerebral cortex presents a   \n108 highly-folded thin structure with a significantly complex shape, necessitating more advanced methods.   \n109 Diffeomorphic Deformation. Diffeomorphic deformation is a spatial transformation that guarantees   \n110 both smoothness and invertibility in the mapping process [46]. It has been widely used in the   \n111 modeling and analysis of brain morphometry, including image registration and surface reconstruction   \n112 tasks. LDDMM [5] computes diffeomorphic deformation based on a time-dependent velocity vector   \n113 field, while Arsigny et al. [2] employ a stationary velocity field (SVF) in conjunction with the   \n114 scaling and squaring method to reduce computation complexity. Learning-based methods [3, 28,   \n115 38] improve the computation efficiency, with regularizations such as smoothness [3] and inverse  \n116 consistency [38] enchancing the diffeomorphic property of the deformation. In the CSR task,   \n117 diffeomorphic deformation strategies have been adopted to solve an ordinary differential equation   \n118 (ODE) modeling the trajectories of each vertex of a surface. For instance, CoticalFlow methods [26,   \n119 47] propose solving the ODE vertex-wise and derive a numerical condition to ensure homeomorphism   \n120 of integration by training a chain of diffeomorphic deformation models in sequential stages. Recently,   \n121 with the advances in neural ODE solver [10], CortexODE [31] parameterizes the trajectories of   \n122 vertices on the surface as ODEs and proposes a pipeline to reconstruct WM and pial surfaces   \n123 sequentially. Our method builds upon these works [31, 47, 54] and integrates multiple CSR tasks   \n124 into a single framework, leveraging the efficiency and diffeomorphic properties of these strategies. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "125 3 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "126 Our proposed framework, depicted in Fig. 1, is designed to reconstruct multiple cortical surfaces   \n127 simultaneously, eliminating the dependency on pGT generated by conventional and time-consuming   \n128 CSR pipelines. We leverage as weak supervision the brain ribbon segmentation maps that are less   \n129 accurate than pGT surfaces but more accessible. Section3.1 outlines the network structure that couples   \n130 multiple cortical surfaces to reduce the learning difficulty. Section 3.2 describes the loss functions   \n131 devised to supervise the network optimization, facilitating sub-voxel reconstruction accuracy and   \n132 preserving optimal surface topology. ", "page_idx": 2}, {"type": "text", "text": "133 3.1 Coupled Cortical Surface Reconstruction ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "134 Existing supervised methods require pGT obtained from traditional CSR pipelines to provide precise   \n135 sub-voxel supervision. They can effectively learn the deformation field, even from distant initial   \n136 locations, to accurately align the initialization surface with the target surfaces [11, 26, 47]. However,   \n137 brain ribbon segmentation maps are inherently discrete voxel grids, offering much coarser supervision.   \n138 Consequently, the selection of the initialization surface becomes more critical. Moreover, given the   \n139 intricate folded patterns of the cerebral cortex, the proximity of the two banks of grooves in deep   \n140 cortical sulci often poses a considerable risk of generating topology errors (e.g., handles, holes) in the   \n141 reconstructed surfaces. Conversely, voxels closer to the WM surface exhibit clearer contrast, enabling   \n142 a distinct separation between sulci (Fig. 2 (b)). Thus, following [54], we opt for the midthickness   \n143 layer, positioned midway between the WM and pial surfaces, to serve as a connection for coupling   \n144 the reconstructions of both surfaces and achieve a balanced performance for both surfaces.   \n145 As illustrated in Fig. 1, SegCSR employs a neural network to jointly model three diffeomorphic flows:   \n146 $F_{\\theta}(I,S_{0})=\\left(\\mathbf{v}^{m},\\mathbf{\\bar{v}}^{o},\\mathbf{v}^{i}\\right)$ . Here, $I$ represents a multi-channel input consisting of brain MRI, cortical   \n147 ribbon masks, and signed distance functions (SDFs); ${\\mathcal{S}}_{0}$ denotes the initialization midthickness   \n148 surface; and $\\mathbf{v}^{m}$ , $\\mathbf{v}_{}^{o}$ , $\\bar{\\mathbf{v}}^{i}$ correspond to the velocity fields that drive ${\\mathcal S}_{0}$ towards the true midthickness   \n149 surface ${\\cal S}_{M}$ , outward to the pial surface $S_{G}$ , and inward to the WM surface $S_{W}$ , respectively. The   \n150 SegCSR establishes an explicit one-to-one mapping between multiple surfaces and is trained by   \n151 minimizing weakly supervised losses between the predicted mesh and the ribbon segmentations.   \n152 The diffeomorphic deformation between the initialization surface and the target surface can be   \n153 computed as the integration of an ODE [1] based on the velocity field $\\mathbf{v}$ : ", "page_idx": 2}, {"type": "image", "img_path": "vKwf15M5EE/tmp/360ca455125694b5ab057a1cc0bb8e8892a13bb7820bb2954352acd0805250c3.jpg", "img_caption": ["Figure 1: The $\\mathrm{SegCSR}$ framework overview. SegCSR takes as input a brain MRI image, cortical ribbon segmentation maps, and signed distance maps of cortical surfaces, and simultaneously learns three diffeomorphic deformations to optimize the initial midthickness surface ${\\mathcal{S}}_{0}$ to align with the target midthickness surface $\\mathcal{S}_{M}$ , and then deform ${\\mathcal{S}}_{M}$ outwards and inwards to the pial surface $S_{G}$ and the WM surface $S_{W}$ , respectively. The model is optimized using weakly supervised loss functions: the mesh loss guides the surfaces towards the boundaries of the cortical ribbon segmentation maps; the inter-surface normal consistency loss regularizes the pial surface in deep cortical sulci; the intensity gradient loss facilitates sub-voxel-level movement; and additional regularization terms control the deformation trajectories of multiple surfaces as well as the uniformity and smoothness of the surfaces. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\frac{d\\Phi(\\mathbf{x},t)}{d t}=\\mathbf{v}(\\Phi(\\mathbf{x},t),t)\\;\\;\\mathrm{s.t.}\\;\\;\\Phi(\\mathbf{x},0)=\\mathbf{x}^{(0)},\\mathrm{and}\\;\\mathrm{thus}\\;\\Phi(\\mathbf{x},t)=\\mathbf{x}^{(0)}+\\int_{o}^{t}\\mathbf{v}(\\Phi(\\mathbf{x},s),s)d s,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "154 where $\\Phi({\\bf x},t)$ defines a trajectory from the source position $\\mathbf{x}^{(0)}=\\Phi(\\mathbf{x},0)$ to the target position   \n155 $\\mathbf{x}^{(1)}=\\Phi(\\mathbf{x},1)$ . According to the Cauchy-Lipschitz theorem [50], if the velocity field is Lipschitz   \n156 continuous, the resulting mapping $\\Phi$ is bijective with continuous inverse (i.e., a diffeomorphism).   \n157 To solve this initial value problem, we perform the integration on the predicted velocity fields   \n158 using standard numerical integration techniques, such as the Euler method and the Runge-Kutta   \n159 method [9]. Specifically, for each integration step $t\\in[0,1]$ , each vertex\u2019s coordinates can be updated   \n160 by $\\mathbf{x}^{(t+1)}=\\mathbf{x}^{(t)}+h\\mathbf{v}(\\Phi(\\mathbf{x},t),t)$ , where $\\textstyle h={\\frac{1}{T}}$ is the step size and $T$ is the total time steps, and   \n161 the velocity vector $\\mathbf{v}$ for a vertex is trilinearly interpolated from its neighboring velocity vectors [54]. ", "page_idx": 3}, {"type": "text", "text": "162 3.2 Weak Supervision Loss Functions ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "163 Mesh Loss. Weak supervision for SegCSR is derived from cortical ribbon segmentation maps of   \n164 WM and GM (see Fig. 1, the filled interior area of WM and pial surfaces), which can be obtained   \n165 from existing segmentation approaches [7, 20, 29, 45, 52]. Although these ribbon segmentation   \n166 maps do not perfectly represent the intricate pial surface, the WM surface is relatively easier to   \n167 recognize due to its clear local intensity contrast, providing a better-separable boundary (see Fig. 2   \n168 (a-b)). Therefore, we use the boundary of the pGT WM segmentation to supervise the WM surface   \n169 reconstruction. Inspired by [31, 54], we generate an SDF for the WM surface by using a distance   \n170 transform algorithm, where voxels with values of zero represent the surface boundaries and voxels   \n171 with negative or positive values encode their distances to the surface boundaries inward or outward,   \n172 respectively. We then apply a fast topology check and correction algorithm [4] to the SDF to ensure   \n173 the surface maintains spherical topology. The WM surface $\\boldsymbol{S_{W_{*}}}$ is extracted using the Marching   \n174 Cubes algorithm [27]. The distance of the vertices between the predicted surface $S_{W}$ and the pGT   \n175 surface $\\mathcal{S}_{W_{*}}$ is minimized using the bi-directional Chamfer distance [26]: ", "page_idx": 3}, {"type": "image", "img_path": "vKwf15M5EE/tmp/d8abac7c1da41914acfd8b00f868c8ae7d6630f385588e83c3e732340836fd63.jpg", "img_caption": ["Figure 2: (a) A brain MRI region. (b)-(g) are illustration of loss terms. (b) WM, midthickness, pial surfaces in a deep sulcus region. (c-1) Bi-directional Chamfer loss for the WM surface; (c-2) Uni-directional Chamfer loss for the pGT pial surface generated from the GM segmentation. (d) Normal consistency between three reconstructed surfaces. (e) Intensity gradient along the normal direction of a vertex in the surface. (f) The symmetric deformation trajectory. $\\mathbf{v}^{o}$ and $\\mathbf{v}^{i}$ are outward and inward velocity fields respectively. (g) The customized edge length loss. $A$ : area; $\\mu$ : edge length. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{c h W}=\\frac{1}{|S_{W}|}\\sum_{\\mathbf{p}\\in S_{W}}\\operatorname*{min}_{\\mathbf{p}_{*}\\in S_{W*}}\\Vert\\mathbf{p}-\\mathbf{p}_{*}\\Vert_{2}^{2}+\\frac{1}{|S_{W*}|}\\sum_{\\mathbf{p}_{*}\\in S_{W*}}\\operatorname*{min}_{\\mathbf{p}\\in S_{W}}\\Vert\\mathbf{p}_{*}-\\mathbf{p}\\Vert_{2}^{2},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "176 where $\\mathbf{p}$ and $\\mathbf{p}_{*}$ are the coordinates of vertices on meshes. See Fig. 2 (c-1) for illustration. ", "page_idx": 4}, {"type": "text", "text": "177 For the pial surface, GM segmentation may fail to delineate the boundary in deep cortical sulci.   \n178 As shown in Fig. 2 (c-2), using a similar pGT surface generation protocol as the WM surface to   \n179 generate the pial surface $\\boldsymbol{S}_{G_{*}}$ fail to capture cortical folding accurately. Directly fitting to $\\boldsymbol{S}_{G_{*}}$ with   \n180 bi-directional Chamfer loss causes the model to predict similarly inaccurate cortical sulci. To address   \n181 this issue, we propose the boundary surface loss, which uses a uni-directional Chamfer distance to   \n182 compute the shortest distance from the pGT pial surface $\\boldsymbol{S}_{G_{*}}$ to the predicted pial surface ${\\mathit{S}}_{G}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{c h G}=\\frac{1}{|S_{G}|}\\sum_{\\mathbf{p}\\in S_{G}}\\operatorname*{min}_{\\mathbf{p}_{\\ast}\\in S_{G_{\\ast}}}\\|\\mathbf{p}-\\mathbf{p}_{\\ast}\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "183 In this way, the deformed surface is not influenced by the inaccuracies of $\\boldsymbol{S}_{G_{*}}$ and does not move   \n184 outward from the deep sulci. The overall mesh loss is computed as $\\mathcal{L}_{m e s h}=\\mathcal{L}_{c h W}+\\mathcal{L}_{c h G}$ .   \n185 Inter-Mesh Normal Consistency Loss. To further alleviate the difficulty of constraining the pial   \n186 surface using the WM and midthickness surfaces, we propose leveraging the prior knowledge that   \n187 the cerebral cortex has a sheet-like topology (i.e., the inner, middle, and outer surfaces are locally   \n188 parallel to each other). As shown in Fig. 2 (d), this loss is defined to ensure that the deformation   \n189 of the midthickness surface aligns with its normal direction, thereby maintaining similar normal   \n190 directions on the target surfaces: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{i m n c}=\\frac{1}{|S_{M}|}\\sum_{\\mathbf{p}\\in S_{M}}(1-c o s(\\mathbf{n}_{\\mathbf{p}_{G}},\\mathbf{n}_{\\mathbf{p}_{W}})),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "191 where $\\mathbf{n}_{\\mathbf{p}_{G}}$ and $\\mathbf{n}_{\\mathbf{p}_{W}}$ are the normal vectors of the deformed vertex $\\mathbf{p}$ on $\\mathcal{S}_{M}$ and $S_{G}$ respectively. ", "page_idx": 5}, {"type": "text", "text": "192 Intensity Gradient Loss. In addition to ribbon segmentaions, inspired by the fact that traditional   \n193 methods utilize raw image intensity contrast to define and optimize the target surfaces, we propose to   \n194 adjust the nuance between GT target surface and the pGT segmentation boundaries. By definition [17,   \n195 52], the WM (or pial) surface lies at the WM/GM (or GM/CSF) interface where image intensity change   \n196 most drastically. We sample $K$ points along the extended lines on each side of the normal direction at   \n197 vertex $\\mathbf{p}$ , and compute the gradients of neighboring points: $\\begin{array}{r}{\\mathcal{L}_{g r a d}=\\frac{1}{|S_{W}|}\\sum_{\\mathbf{p}\\in S_{M}}\\sum_{i=1}^{K}g r a d_{i}(\\mathbf{p})+}\\end{array}$   \n198 $\\begin{array}{r}{\\frac{1}{|S_{G}|}\\sum_{\\mathbf{p}\\in S_{G}}\\sum_{i=1}^{K}g r a d_{i}(\\mathbf{p}).}\\end{array}$   \n199 Cycle Consistency Loss. We utilize the midthickness layer to establish a correspondence between   \n200 the inner and outer surfaces, thereby reducing the difficulty of learning large deformations. However,   \n201 there is no true midthickness surface available for supervision, nor a definitive criterion for choosing   \n202 between bi-directional or uni-directional approaches for different regions on the midthickness surface.   \n203 Additionally, the learned velocity fields $\\mathbf{v}_{}^{o}$ and $\\mathbf{v}^{i}$ could potentially cause non-inverse transformations   \n204 at the midthickness surface. To address these issues, we propose a loss function that enforces the   \n205 midthickness surface resides halfway between the WM and pial surfaces and maintains consistency   \n206 along the entire trajectory: ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{c y c}=\\frac{1}{|S_{M}|}\\sum_{\\mathbf{p}\\in\\mathcal{S}_{M}}\\|\\mathbf{p}_{\\Phi_{W}\\circ\\Phi_{G}}-\\mathbf{p}\\|_{2}^{2}+\\|\\mathbf{p}_{\\Phi_{G}\\circ\\Phi_{W}}-\\mathbf{p}\\|_{2}^{2}+\\|L_{M i d\\rightarrow G M}(\\mathbf{p})-L_{M i d\\rightarrow W M}(\\mathbf{p})\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "207 where $\\mathbf{p}_{\\Phi_{b}\\circ\\Phi_{a}}$ represents deforming a vertex $\\mathbf{p}\\in\\mathcal{S}_{M}$ with velocity field $\\mathbf{v}_{}^{a}$ and $\\mathbf{v}^{b}$ sequentially, and   \n208 $L_{M i d\\rightarrow G M}(\\mathbf{p})$ is the accumulated trajectory length over $T$ steps of deformation. For example, as   \n209 shown in Fig. 2 (f), the deformations move a vertex $\\mathbf{p}_{M i d}$ outward to $\\mathbf{p}_{G M}$ using $\\mathbf{v}^{o}$ and then inward   \n210 to $\\mathbf{p}_{M i d}^{\\prime}$ using $\\mathbf{v}^{i}$ , in which the two trajectories are aligned by minimizing the distance between $\\mathbf{p}_{M i d}$   \n211 and $\\mathbf{p}_{M i d}^{\\prime}$ . Similarly, we enforce the consistency between $\\mathbf{p}_{\\Phi_{G}\\circ\\Phi_{W}}$ and $\\mathbf{p}$ . Furthermore, starting   \n212 from the midthickness layer, the trajectory lengths of the vertex moving to the WM and pial surfaces   \n213 should be equal, which is regularized by the third term in the equation above.   \n214 Mesh Quality Loss. First, the reconstructed surface should be composed of uniformally distributed   \n215 triangles. To accommodate various sizes of brain volume and image resolution, we devise a cus  \n216 tomized edge length loss to constrain the size of triangles in the predicted meshes for each subject.   \n217 Specifically, we assume an ideal prediction where the faces are equilateral and of the same area $A$   \n218 and drive the edge length to the target edge length $\\begin{array}{r}{\\mu_{e l}=2\\sqrt{\\frac{A}{\\sqrt{3}}}}\\end{array}$ (see Fig. 2 (g)). Second, we employ   \n219 a normal consistency loss to promote the surfaces\u2019 smoothness. The mesh quality loss is defined as: ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{q u a}=\\frac{1}{|S|}\\left(\\sum_{\\mathbf{p}\\in\\mathcal{S}}\\frac{1}{|N(\\mathbf{p})|}\\sum_{\\mathbf{k}\\in\\mathcal{N}(\\mathbf{p})}(\\mu_{e l}-\\|\\mathbf{p}-\\mathbf{k}\\|_{2})^{2}+\\sum_{e\\in\\mathcal{S},\\,f_{0}\\cap f_{1}=e}(1-c o s(\\mathbf{n}_{f_{0}},\\mathbf{n}_{f_{1}}))\\right),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "220 where $\\boldsymbol{S}$ denotes the predicted mesh, $\\mathcal{N}(\\mathbf{p})$ are the neighbors of vertex p, $e$ is an edge, $f_{0}$ and $f_{1}$ are   \n221 $e$ \u2019s two neighboring faces with their unit normals $\\mathbf{n}_{f_{0}}$ and $\\mathbf{n}_{f_{1}}$ .   \n222 In summary, we combine all the losses to jointly optimize our $\\mathrm{SegCSR}$ model: $\\mathcal{L}=\\lambda_{1}\\mathcal{L}_{m e s h}\\;+$   \n223 $\\lambda_{2}\\mathcal{L}_{i m n c}+\\lambda_{3}\\mathcal{L}_{g r a d}+\\lambda_{4}\\mathcal{L}_{c y c}+\\lambda_{5}\\mathcal{L}_{q u a}$ , where $\\{\\lambda_{i}\\}_{i=1,\\cdots,5}$ are weights to balance the loss terms. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "224 4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "225 4.1 Experimental Setups ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "226 Datasets. We evaluate our method on two large-scale adult datasets and one infant dataset of low   \n227 resolution. The ADNI-1 [24] dataset consists of 817 subjects aged 55 to 90. We randomly split it into   \n228 subsets of 654, 50, and 113 subjects for training, validation, and testing, respectively. The OASIS  \n229 1 [35] dataset consists of 413 subjects aged 18 to 96. We randomly split it into subsets of 330, 25, and   \n230 58 subjects for training, validation, and testing, respectively. We followed a pre-processing protocol   \n231 used in previous works [8, 13, 26, 31] for fair comparison. The T1-weighted MRI scans were aligned   \n232 to the MNI152 template and clipped to the size of $192\\times224\\times192$ at $1m m^{3}$ isotropic resolution.   \n233 The pseudo ground-truth (pGT) of ribbon segmentation and cortical surfaces were generated using   \n234 FreeSurfer v7.2.0 [17]. The BCP [23] dataset consists of 121 subjects ranging in age from 2 weeks   \n235 to 12 months. We randomly allocate 90, 12, and 19 subjects for training, validation, and testing,   \n236 respectively. Rigid registration was applied to the T1w and T2w image pairs. The pGT of ribbon   \n237 segmentation and cortical surfaces were generated by the iBEAT v2.0 [52]. The intensity values of   \n238 MRI scans, ribbon segmentation maps, and SDFs were normalized to [0, 1] and the coordinates of the   \n239 vertices were normalized to $[-1,1]$ . All the models were trained on the training set until they reached   \n240 a loss plateau on the validation set and evaluated on the test set.   \n241 Implementation Details Our framework was implemented in PyTorch [40] and trained on a worksta  \n242 tion with 12 GB NVIDIA P100 GPU. The 3D U-Net [44] for segmentation of ribbons was trained for   \n243 200 epochs using Adam [25] optimization and achieved an average Dice index of 0.96 on the testing   \n244 set. The ${\\mathrm{SegCSR}}$ model utilized $T=5$ steps (i.e., step size is 0.2) in Euler solver. We trained our   \n245 SegCSR model using Adam optimizer $\\beta_{1}=0.9$ , $\\beta_{2}=0.999$ , $\\epsilon=1e^{-10}$ , learning rate $1e^{-4}$ ) for   \n246 400 epochs to reconstruct both WM, midthickness, and pial surfaces of both brain hemispheres. We   \n247 set $\\lambda_{1}=\\lambda_{4}=1$ and $\\lambda_{2}=\\lambda_{3}=\\lambda_{5}=0.1$ . The surface meshes had ${\\sim}130k$ vertices. More details   \n248 can be found in the Supplementary Materials.   \n249 Evaluation Metrics We utilized three distance-based metrics to measure the CSR accuracy: Chamfer   \n250 distance (CD), average symmetric surface distance (ASSD), and 90th-percentile Hausdorff distance   \n251 (HD). CD [16, 53] measures the mean distance between two sets of vertices. ASSD [13] and   \n252 HD [13, 49] measure the average and maximum distance between two surfaces. They were computed   \n253 bidirectionally over ${\\sim}130k$ points uniformly sampled from the predicted and target surfaces. A lower   \n254 distance means a better result. Since topology is also important in CSR, we utilized the ratio of   \n255 self-intersection faces (SIF) [13, 14, 31, 54] to measure reconstructed surface quality. ", "page_idx": 5}, {"type": "table", "img_path": "vKwf15M5EE/tmp/86c8725c04dbf988ba9efd97c070a478c6207ceaaf561628d3492745aa784f01.jpg", "table_caption": ["Table 1: Quantitative analysis of cortical surface reconstruction on geometric accuracy and self-intersections. The Chamfer distance (CD), average symmetric surface distance (ASSD), Hausdorff distance (HD), and the ratio of the self-intersecting faces (SIF) were measured for WM and pial surfaces on three datasets. The mean value and standard deviation are reported. Lower scores indicate better results for all metrics. \u201cS\u201d denotes the use of pGT surfaces from conventional pipelines, while \u201cW\u201d represents weak supervision by pGT ribbon segmentations. In each supervision setting, the best results are in bold, and the second best results are underlined. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "256 4.2 Comparison with Related Works ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "257 We compare SegCSR with both implicit and explicit learning-based cortical surface reconstruction   \n258 approaches described in Section 1 and summarize the experimental results in Table 5.   \n259 On Adult Datasets. (I) Comparison with Implicit Approaches. We compare $\\mathrm{SegCSR}$ with two   \n260 representative implicit representation approaches on the ADNI and OASIS datasets. As shown   \n261 in Table 5, SegCSR achieves superior geometric accuracy. Note that both DeepCSR [13] and 3D   \n262 U-Net [44] require post-processing to correct topology and extract a mesh, resulting in SIFs of 0.   \n263 Without post-processing, the SIFs for 3D U-Net\u2019s WM and pial surfaces range from $3\\%$ to $15\\%$ .   \n264 SegCSR produces a negligible number of self-intersecting faces, ${\\sim}0.3\\%$ on average for both white   \n265 and pial surfaces. Fig. 3 shows that SegCSR effectively deforms the pial surface into deep sulci,   \n266 while the baseline approaches exhibit large geometric errors due to the PVE problem of brain MRI.   \n267 Additionally, SegCSR requires only 0.37s of runtime per brain hemisphere, orders of magnitude   \n268 faster than traditional FreeSurfer pipelines. (II) Comparison with Explicit Approaches. We compare   \n269 SegCSR with explicit learning-based approaches, including CorticalFlow $^{++}$ [47], Vox2Cortex [8],   \n270 CortexODE [31], and CoCSR [54]. These methods are trained with pGT surfaces generated by   \n271 conventional pipelines, providing more accurate supervision than pGT ribbon segmentations. For   \n272 a fair comparison, we employ the same network structure for the current best CoCSR [54] and our   \n273 SegCSR, with CoCSR serving as an upper-bound performance benchmark for our weakly supervised   \n274 SegCSR. As shown in Table 5, SegCSR surprisingly surpasses some supervised baselines in terms of   \n275 both geometric and morphological accuracy, demonstrating its potential to replace existing methods   \n276 when accurate surface supervision is not available.   \n277 On Infant Dataset. Infant brain MRIs present additional challenges due to the smaller size of fetal   \n278 brains, limited image resolution, and lower image contrast, which together make the reconstruction   \n279 task more difficult. Consequently, overall performance is inferior compared to adult datasets. We   \n280 compare SegCSR with both implicit and explicit representation approaches. The results in Table 5   \n281 show that $\\mathrm{SegCSR}$ achieves superior performance than the implicit DeepCSR and 3D U-Net methods,   \n282 and comparable performance to explicit methods like CorticalFlow $^{\\mathrel{\\textstyle\\downarrow}}++$ , CortexODE, and CoCSR. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "image", "img_path": "vKwf15M5EE/tmp/eeae8909cccf81b16605891c09d25b8ff8bbe783fc41c0932455814edf462de6.jpg", "img_caption": ["Figure 3: Visualization of reconstructed pial surfaces compared to DeepCSR and CortexODE. CortexODE is trained with pGT from FreeSurfer; DeepCSR and ours are trained with pGT ribbon segmentations. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "283 4.3 Ablation Studies ", "text_level": 1, "page_idx": 7}, {"type": "table", "img_path": "vKwf15M5EE/tmp/83a6ab500daf1dbddbe2541b95bca5ed0fff56bda5e4165879c478c6657623be.jpg", "table_caption": ["Table 2: Ablation studies on the ADNI dataset. The setting S0 refers to our complete setting (cf. Table 5). Top: The impact of loss functions. Bottom: The impact of initialization surface location. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "284 Loss Functions. We evaluated the contribution of different losses of our method to the surface   \n285 reconstruction performance in terms of both accuracy (CD, ASSD, HD) and topological correctness   \n286 (SIF). The results are summarized in Table 2 (Top). The setting S4 represents using our proposed   \n287 Chamfer loss (i.e., uni-directional for the pial surface) alone, while $S4^{\\star}$ referes to using existing   \n288 bi-directional Chamfer loss for both WM and pial surfaces. The results of S4 and ${\\mathrm{S4^{\\star}}}$ indicated   \n289 that the model using bi-directional Chamfer loss overfitted to the pGT segmentation boundary and   \n290 failed to fit the deep cortical sulci. Another pair of comparison, S0 and $\\mathrm{S0^{\\star}}$ , showed a similar   \n291 phenomenon. Enforcing the inter-mesh normal consistency of the WM and pial surfaces (S3, $\\mathcal{L}_{i m n c})$ )   \n292 improved geometric accuracy by explicitly constraining the nromal direction of two surfaces but   \n293 slightly worsened the topology, which might be caused by the discrepancy between the midthickness   \n294 and the WM (and pial) surface. The proposed intensity gradient loss (S2, $\\mathcal{L}_{g r a d})$ helped adjust the   \n295 deformed surfaces locally, leading to slightly improved geometric accuracy and reduced topology   \n296 error. Enforcing equality of the trajectories from the midthickness surface to the WM and pial surfaces   \n297 and symmetric cycle consistency of two trajectories (S1, $\\mathcal{L}_{c y c,}$ ) helped optimize the midthickness   \n298 surface and promoted the invertibility of deformations. Moreover, the inclusion of regularization   \n299 terms on the uniformity and smoothness of the reconstructed surfaces (S0, ${\\mathcal{L}}_{q u a}]$ ) enhanced the   \n300 surface quality and significantly reduce the self-intersection face ratio. Overall, our proposed method   \n301 struck a balance between geometric accuracy and topology quality, with each component playing a   \n302 complementary role.   \n303 Initialization Surface Location. Table 2 (Bottom) shows the impact of the initialization surface   \n304 location. Starting from either the WM or midthickness surfaces leads to satisfactory results. Con  \n305 versely, initializing from the GM surface introduced more difficulty in learning large deformations   \n306 into deep sulci due to the severe partial volume effect, resulting in worse average geometric accuracy   \n307 for both surfaces. The results also indicated that the closer the initial surface was to its target surface,   \n308 the higher the reconstruction accuracy achieved. Therefore, starting from the midthickness surface   \n309 strikes a balance between WM and pial surface reconstruction outcomes. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "310 4.4 Reproducibility ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "311 We conducted an experiment on the Test-Retest   \n312 dataset [33], which comprises 40 MRIs collected within   \n313 a short period for each of the 3 subjects. The cor  \n314 tical surfaces of the same subject should be nearly   \n315 identical. Following the experimental setup outlined   \n316 in [8, 13, 31, 54], we utilized the iterative closest-point   \n317 algorithm to align image pairs and computed the ge  \n318 ometric distance between surfaces. The results for   \n319 the left hemisphere are presented in Table 3, showing   \n320 that SegCSR obtained superior reproducibility com  \n321 pared with DeepCSR (implicit representation; weakly   \n322 supervised) and was comparable to the conventional   \n323 FreeSurfer pipeline and supervised DL-based CSR   \n324 methods. This implied that the results generated by SegCSR can be reliably used for downstream   \n325 analyses, such as investigating cortical thickness changes in patients. ", "page_idx": 8}, {"type": "table", "img_path": "vKwf15M5EE/tmp/843709d23f578e6d232b9561328afd887ec775e0d7c17c8f5afef4c326c3cc55.jpg", "table_caption": ["Table 3: Reproducibility analysis. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "326 5 Conclusions ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "327 We introduce SegCSR, a novel approach to jointly reconstruct multiple cortical surfaces using   \n328 weak supervision from ribbon segmentations derived from brain MRIs. Our method initializes a   \n329 midthickness surface and then deforms it inward and outward to the inner and outer cortical surfaces by   \n330 jointly learning diffeomorphic flows. The new boundary loss function optimizes the surfaces toward   \n331 the boundaries of the cortical ribbon segmentation maps while the inter-surface normal consistency   \n332 loss regularizes the pial surface in complex and challenging cortical sulci regions. Additional   \n333 regularization terms are incorporated to enforce reconstructed surfaces\u2019 uniformity, smoothness,   \n334 and topology. Extensive experiments conducted on large-scale adult and infant brain MRI datasets   \n335 demonstrate superior performance in terms of accuracy and surface regularity compared to existing   \n336 supervised DL-based alternatives.   \n337 Limitations and Future Directions. The efficacy of SegCSR is influenced by the quality of pGT   \n338 segmentations. Also, We can utilize brain tissue segmentation as auxiliary functions to supervise the   \n339 model training. SegCSR constrains the inter-mesh consistency of the deformation on the midthickness   \n340 surface, potentially affecting anatomical fidelity of pial surfaces. The method should be tested on   \n341 more diverse cohorts of subjects to demonstrate its efficacy on real world neuroimage analysis tasks.   \n342 Societal Impact. Our proposed method has been rigorously evaluated on four real-world brain MRI   \n343 datasets, showcasing its capacity to assist doctors and scientists in both quantitative and qualitative   \n344 analyses of the cerebral cortex. Nonetheless it is imperative to conduct more thorough evaluation on   \n345 a larger cohort of subjects and across various imaging qualities. And the deployment of the model in   \n346 clinical settings should be approached with caution and under human supervision. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "347 References ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "348 [1] V. I. Arnold. Ordinary differential equations. Springer Science & Business Media, 1992. ", "page_idx": 8}, {"type": "text", "text": "349 [2] V. Arsigny, O. Commowick, X. Pennec, and N. Ayache. A log-euclidean framework for statistics   \n350 on diffeomorphisms. In International Conference on Medical Image Computing and Computer   \n351 Assisted Intervention, pages 924\u2013931. Springer, 2006.   \n352 [3] G. Balakrishnan, A. Zhao, M. R. Sabuncu, J. Guttag, and A. V. Dalca. Voxelmorph: a learning   \n353 framework for deformable medical image registration. IEEE Transactions on Medical Imaging,   \n354 38(8):1788\u20131800, 2019.   \n355 [4] P.-L. Bazin and D. L. Pham. Topology correction of segmented medical images using a fast   \n356 marching algorithm. Computer Methods and Programs in Biomedicine, 88(2):182\u2013190, 2007.   \n357 [5] M. F. Beg, M. I. Miller, A. Trouv\u00e9, and L. Younes. Computing large deformation metric   \n358 mappings via geodesic flows of diffeomorphisms. International Journal of Computer Vision,   \n359 61:139\u2013157, 2005.   \n360 [6] M. Bertoux, J. Lagarde, F. Corlier, L. Hamelin, J.-F. Mangin, O. Colliot, M. Chupin, M. N.   \n361 Braskie, P. M. Thompson, M. Bottlaender, et al. Sulcal morphology in alzheimer\u2019s disease: an   \n362 effective marker of diagnosis and cognition. Neurobiology of Aging, 84:41\u201349, 2019.   \n363 [7] B. Billot, D. N. Greve, O. Puonti, A. Thielscher, K. Van Leemput, B. Fischl, A. V. Dalca, J. E.   \n364 Iglesias, et al. Synthseg: Segmentation of brain mri scans of any contrast and resolution without   \n365 retraining. Medical image analysis, 86:102789, 2023.   \n366 [8] F. Bongratz, A.-M. Rickmann, S. P\u00f6lsterl, and C. Wachinger. Vox2Cortex: Fast explicit   \n367 reconstruction of cortical surfaces from 3D MRI scans with geometric deep neural networks. In   \n368 Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages   \n369 20773\u201320783, 2022.   \n370 [9] R. L. Burden, J. D. Faires, and A. M. Burden. Numerical analysis. Cengage learning, 2015.   \n371 [10] R. T. Chen, Y. Rubanova, J. Bettencourt, and D. K. Duvenaud. Neural ordinary differential   \n372 equations. Advances in Neural Information Processing Systems, 31:6572\u20136583, 2018.   \n373 [11] X. Chen, J. Zhao, S. Liu, S. Ahmad, and P.-T. Yap. SurfFlow: A flow-based approach for rapid   \n374 and accurate cortical surface reconstruction from infant brain mri. In International Conference   \n375 on Medical Image Computing and Computer-Assisted Intervention, pages 380\u2013388. Springer,   \n376 2023.   \n377 [12] S. J. Crutch, M. Lehmann, J. M. Schott, G. D. Rabinovici, M. N. Rossor, and N. C. Fox.   \n378 Posterior cortical atrophy. The Lancet Neurology, 11(2):170\u2013178, 2012.   \n379 [13] R. S. Cruz, L. Lebrat, P. Bourgeat, C. Fookes, J. Fripp, and O. Salvado. DeepCSR: A 3D deep   \n380 learning approach for cortical surface reconstruction. In Proceedings of the IEEE/CVF Winter   \n381 Conference on Applications of Computer Vision, pages 806\u2013815, 2021.   \n382 [14] R. Dahnke, R. A. Yotter, and C. Gaser. Cortical thickness and central surface estimation.   \n383 Neuroimage, 65:336\u2013348, 2013.   \n384 [15] A. M. Dale, B. Fischl, and M. I. Sereno. Cortical surface-based analysis: I. segmentation and   \n385 surface reconstruction. Neuroimage, 9(2):179\u2013194, 1999.   \n386 [16] H. Fan, H. Su, and L. J. Guibas. A point set generation network for 3d object reconstruction   \n387 from a single image. In Proceedings of the IEEE/CVF Conference on Computer Vision and   \n388 Pattern Recognition, pages 605\u2013613, 2017.   \n389 [17] B. Fischl. Freesurfer. Neuroimage, 62(2):774\u2013781, 2012.   \n390 [18] M. F. Glasser, S. N. Sotiropoulos, J. A. Wilson, T. S. Coalson, B. Fischl, J. L. Andersson, J. Xu,   \n391 S. Jbabdi, M. Webster, J. R. Polimeni, et al. The minimal preprocessing pipelines for the human   \n392 connectome project. Neuroimage, 80:105\u2013124, 2013.   \n393 [19] K. Gopinath, C. Desrosiers, and H. Lombaert. SegRecon: Learning joint brain surface re  \n394 construction and segmentation from images. In International Conference on Medical Image   \n395 Computing and Computer Assisted Intervention, pages 650\u2013659. Springer, 2021.   \n396 [20] L. Henschel, S. Conjeti, S. Estrada, K. Diers, B. Fischl, and M. Reuter. Fastsurfer-a fast and   \n397 accurate deep learning based neuroimaging pipeline. NeuroImage, 219:117012, 2020.   \n398 [21] Y. Hong, S. Ahmad, Y. Wu, S. Liu, and P.-T. Yap. Vox2Surf: Implicit surface reconstruction   \n399 from volumetric data. In Intl. Workshop on Machine Learning in Medical Imaging, pages   \n400 644\u2013653. Springer, 2021.   \n401 [22] A. Hoopes, J. E. Iglesias, B. Fischl, D. Greve, and A. V. Dalca. Topofti: Rapid reconstruction of   \n402 topologically-correct cortical surfaces. In Medical Imaging with Deep Learning, 2022.   \n403 [23] B. R. Howell, M. A. Styner, W. Gao, P.-T. Yap, L. Wang, K. Baluyot, E. Yacoub, G. Chen,   \n404 T. Potts, A. Salzwedel, et al. The unc/umn baby connectome project (bcp): An overview of the   \n405 study design and protocol development. NeuroImage, 185:891\u2013905, 2019.   \n406 [24] C. R. Jack Jr, M. A. Bernstein, N. C. Fox, P. Thompson, G. Alexander, D. Harvey, B. Borowski,   \n407 P. J. Britson, J. L. Whitwell, C. Ward, et al. The alzheimer\u2019s disease neuroimaging initiative   \n408 (ADNI): MRI methods. Journal of Magnetic Resonance Imaging: An Official Journal of the   \n409 International Society for Magnetic Resonance in Medicine, 27(4):685\u2013691, 2008.   \n410 [25] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In International   \n411 Conference on Learning Representations, 2015.   \n412 [26] L. Lebrat, R. Santa Cruz, F. de Gournay, D. Fu, P. Bourgeat, J. Fripp, C. Fookes, and O. Salvado.   \n413 CorticalFlow: A diffeomorphic mesh transformer network for cortical surface reconstruction.   \n414 Advances in Neural Information Processing Systems, 34:29491\u201329505, 2021.   \n415 [27] T. Lewiner, H. Lopes, A. W. Vieira, and G. Tavares. Efficient implementation of marching   \n416 cubes\u2019 cases with topological guarantees. Journal of Graphics Tools, 8(2):1\u201315, 2003.   \n417 [28] H. Li, Y. Fan, and A. D. N. Initiative. MDReg-Net: Multi-resolution diffeomorphic image   \n418 registration using fully convolutional networks with deep self-supervision. Human Brain   \n419 Mapping, 43(7):2218\u20132231, 2022.   \n420 [29] Y. Li, H. Li, and Y. Fan. ACEnet: Anatomical context-encoding network for neuroanatomy   \n421 segmentation. Medical image analysis, 70:101991, 2021.   \n422 [30] Q. Ma, L. Li, V. Kyriakopoulou, J. V. Hajnal, E. C. Robinson, B. Kainz, and D. Rueckert. Condi  \n423 tional temporal attention networks for neonatal cortical surface reconstruction. In International   \n424 Conference on Medical Image Computing and Computer-Assisted Intervention, pages 312\u2013322.   \n425 Springer, 2023.   \n426 [31] Q. Ma, L. Li, E. C. Robinson, B. Kainz, D. Rueckert, and A. Alansary. CortexODE: Learning   \n427 cortical surface reconstruction by neural ODEs. IEEE Trans. on Medical Imaging, 42(2):430\u2013   \n428 443, 2022.   \n429 [32] Q. Ma, E. C. Robinson, B. Kainz, D. Rueckert, and A. Alansary. PialNN: A fast deep learning   \n430 framework for cortical pial surface reconstruction. In International Workshop on Machine   \n431 Learning in Clinical Neuroimaging, pages 73\u201381. Springer, 2021.   \n432 [33] J. Maclaren, Z. Han, S. B. Vos, N. Fischbein, and R. Bammer. Reliability of brain volume   \n433 measurements: a test-retest dataset. Scientific Data, 1(1):1\u20139, 2014.   \n434 [34] A. Makropoulos, E. C. Robinson, A. Schuh, R. Wright, S. Fitzgibbon, J. Bozek, S. J. Counsell,   \n435 J. Steinweg, K. Vecchiato, J. Passerat-Palmbach, et al. The developing human connectome   \n436 project: A minimal processing pipeline for neonatal cortical surface reconstruction. Neuroimage,   \n437 173:88\u2013112, 2018.   \n438 [35] D. S. Marcus, T. H. Wang, J. Parker, J. G. Csernansky, J. C. Morris, and R. L. Buckner. Open   \n439 access series of imaging studies (OASIS): cross-sectional MRI data in young, middle aged,   \n440 nondemented, and demented older adults. Journal of Cognitive Neuroscience, 19(9):1498\u20131507,   \n441 2007.   \n442 [36] Q. Meng, W. Bai, D. P. O\u2019Regan, and D. Rueckert. Deepmesh: Mesh-based cardiac motion   \n443 tracking using deep learning. IEEE Transactions on Medical Imaging, 2023.   \n444 [37] M. Modat, D. M. Cash, P. Daga, G. P. Winston, J. S. Duncan, and S. Ourselin. Global   \n445 image registration using a symmetric block-matching approach. Journal of medical imaging,   \n446 1(2):024003\u2013024003, 2014.   \n447 [38] T. C. Mok and A. Chung. Fast symmetric diffeomorphic image registration with convolutional   \n448 neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern   \n449 Recognition, pages 4644\u20134653, 2020.   \n450 [39] D. H. Pak, M. Liu, S. S. Ahn, A. Caballero, J. A. Onofrey, L. Liang, W. Sun, and J. S. Duncan.   \n451 Weakly supervised deep learning for aortic valve finite element mesh generation from 3D CT   \n452 images. In International Conference on Information Processing in Medical Imaging, pages   \n453 637\u2013648. Springer, 2021.   \n454 [40] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin,   \n455 N. Gimelshein, L. Antiga, et al. PyTorch: An imperative style, high-performance deep learning   \n456 library. Advances in Neural Information Processing Systems, 32:8026\u20138037, 2019.   \n457 [41] J. Ren, Q. Hu, W. Wang, W. Zhang, C. S. Hubbard, P. Zhang, N. An, Y. Zhou, L. Dahmani,   \n458 D. Wang, et al. Fast cortical surface reconstruction from MRI using deep learning. Brain   \n459 Informatics, 9(1):1\u201316, 2022.   \n460 [42] L. M. Rimol, R. Nesv\u00e5g, D. J. Hagler Jr, \u00d8. Bergmann, C. Fennema-Notestine, C. B. Hartberg,   \n461 U. K. Haukvik, E. Lange, C. J. Pung, A. Server, et al. Cortical volume, surface area, and   \n462 thickness in schizophrenia and bipolar disorder. Biological Psychiatry, 71(6):552\u2013560, 2012.   \n463 [43] J. M. Roe, D. Vidal-Pi\u00f1eiro, \u00d8. S\u00f8rensen, A. M. Brandmaier, S. D\u00fczel, H. A. Gonzalez, R. A.   \n464 Kievit, E. Knights, S. K\u00fchn, U. Lindenberger, et al. Asymmetric thinning of the cerebral   \n465 cortex across the adult lifespan is accelerated in alzheimer\u2019s disease. Nature Communications,   \n466 12(1):721, 2021.   \n467 [44] O. Ronneberger, P. Fischer, and T. Brox. U-Net: Convolutional networks for biomedical image   \n468 segmentation. In International Conference on Medical Image Computing and Computer-Assisted   \n469 Intervention, pages 234\u2013241, 2015.   \n470 [45] A. G. Roy, S. Conjeti, N. Navab, C. Wachinger, A. D. N. Initiative, et al. Quicknat: A fully   \n471 convolutional network for quick and accurate segmentation of neuroanatomy. NeuroImage,   \n472 186:713\u2013727, 2019.   \n473 [46] D. Ruelle and D. Sullivan. Currents, flows and diffeomorphisms. Topology, 14(4):319\u2013327,   \n474 1975.   \n475 [47] R. Santa Cruz, L. Lebrat, D. Fu, P. Bourgeat, J. Fripp, C. Fookes, and O. Salvado. Corti  \n476 calFlow $^{\\cdot++}$ : Boosting cortical surface reconstruction accuracy, regularity, and interoperability.   \n477 In International Conferences on Medical Image Computing and Computer Assisted Intervention,   \n478 pages 496\u2013505. Springer, 2022.   \n479 [48] D. W. Shattuck and R. M. Leahy. Brainsuite: an automated cortical surface identification tool.   \n480 Medical Image Analysis, 6(2):129\u2013142, 2002.   \n481 [49] A. A. Taha and A. Hanbury. Metrics for evaluating 3D medical image segmentation: analysis,   \n482 selection, and tool. BMC Medical Imaging, 15(1):1\u201328, 2015.   \n483 [50] G. Teschl. Ordinary differential equations and dynamical systems, volume 140. American   \n484 Mathematical Soc., 2012.   \n485 [51] D. C. Van Essen, H. A. Drury, S. Joshi, and M. I. Miller. Functional and structural mapping of   \n486 human cerebral cortex: solutions are in the surfaces. Proceedings of the National Academy of   \n487 Sciences, 95(3):788\u2013795, 1998.   \n488 [52] L. Wang, Z. Wu, L. Chen, Y. Sun, W. Lin, and G. Li. ibeat v2.0: a multisite-applicable, deep   \n489 learning-based pipeline for infant cerebral cortical surface reconstruction. Nature protocols,   \n490 18(5):1488\u20131509, 2023.   \n491 [53] N. Wang, Y. Zhang, Z. Li, Y. Fu, H. Yu, W. Liu, X. Xue, and Y.-G. Jiang. Pixel2Mesh: 3D   \n492 mesh model generation via image guided deformation. IEEE Transactions on Pattern Analysis   \n493 and Machine Intelligence, 43(10):3600\u20133613, 2020.   \n494 [54] H. Zheng, H. Li, and Y. Fan. Coupled reconstruction of cortical surfaces by diffeomorphic mesh   \n495 deformation. Advances in Neural Information Processing Systems, 37, 2023. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "496 A Model Details ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "497 A.1 Cortical Ribbon Segmentation Network Architecture ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "498 Fig. 4 (Left) shows the detailed network architecture of our cortical ribbon segmentation network,   \n499 which is a 5-level hierarchical encoder-decoder with skip connections. The network processes a 3D   \n500 brain MRI to produce a cortical ribbon segmentation map. The white matter (WM) segmentation   \n501 includes the interior of the WM surface, encompassing cortical WM, deep gray matter, ventricles,   \n502 hippocampus, and other tissues within the surface. Similarly, the gray matter (GM) segmentation   \n503 includes the interior of the pial surface. The output map has five classes: left hemisphere WM and   \n504 GM, right hemisphere WM and GM, and background. In the encoder, each level uses a $3\\times3\\times3$   \n505 convolutional layer with a stride of 2 to downsample the features. In the decoder, features are   \n506 upsampled by $2\\times$ at each scale, concatenated with the corresponding features from the encoder via   \n507 skip connections, and then fused using a $3\\times3\\times3$ convolutional layer with a stride of 1. For feature   \n508 extraction at the input, a $3\\times3\\times3$ convolutional layer with a stride of 1 is used. Before the final   \n509 prediction, three consecutive convolutional layers are applied. Each convolutional layer is followed   \n510 by a leaky ReLU activation function, except for the last one, which uses a Softmax function before   \n511 computing the cross-entropy loss with the ground truth. ", "page_idx": 12}, {"type": "image", "img_path": "vKwf15M5EE/tmp/8943f0fbf85ad58e6f7010d89f5e07b1e8316b5b64a226978d356617ada43ace.jpg", "img_caption": [], "img_footnote": [], "page_idx": 12}, {"type": "text", "text": "Figure 4: Left: 3D U-Net architecture for ribbon segmentation. The output, i.e., the cortical ribbon map, is overlaid on the input image for illustration. Right: 3D U-Net architecture for cortical surface reconstruction. The learned velocity fields are used to calculate deformations. ", "page_idx": 12}, {"type": "text", "text": "512 A.2 Cortical Surface Reconstruction Network Architecture and Training details ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "513 As shown in Fig.4 (Right), our cortical surface reconstruction (CSR) network operates at five scales.   \n514 To conserve memory, we downsample the input image using a $3\\times3\\times3$ convolution with a stride of   \n515 2 and skip complex feature fusion via skip connections in the decoding path at this scale. To improve   \n516 the accuracy of the velocity fields (VFs), we use $2\\times2\\times2$ deconvolutions with a stride of 2 in the   \n517 decoding path instead of $2\\times$ trilinear upsampling. At the output stage, we employ three parallel   \n518 $3\\times3\\times3$ convolutional layers to generate VFs for the white matter (WM), midthickness, and pial   \n519 surfaces, respectively. ReLU activation functions are used after each convolutional layer, except for   \n520 the three parallel layers, where Softsign functions are applied. The VFs are then utilized to compute   \n521 diffeomorphic deformations. ", "page_idx": 12}, {"type": "text", "text": "523 B.1 Dataset Preprocessing ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "524 We preprocessed all the MRIs of the ADNI-1 [24] and OASIS-1 [35] datasets with the same protocols   \n525 as following: Based on the standard processing protocol in FreeSurfer V7.2.0 [17], the original   \n526 images were conformed and normalized (saved as orig.mgz), affinely registered to the MNI152   \n527 template [8] using the NiftyReg toolbox [37]. The respective ribbon segmentation maps, SDFs, and   \n528 pseudo-ground-truth surfaces were also transformed using the computed transformation. Similarly,   \n529 we utilize iBEAT V2.0 [52] to process the BCP [23] dataset and merge the brain tissue segmentation   \n530 results as the ribbon segmentation maps. ", "page_idx": 13}, {"type": "text", "text": "531 B.2 Baselines ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "532 We compared our SegCSR with representatives from the two categories of existing DL-based CSR   \n533 methods and evaluated their performance for both WM and pial surface reconstruction. DeepCSR [13]   \n534 and 3D U-Net [44] represent implicit surface reconstruction methods, while others fall into the   \n535 category of explicit methods. Note that we modify the 3D U-Net method to first generate SDFs   \n536 based on the ribbon segmentation results, then perform topology correction, and finally utilize   \n537 the Marching Cubes algorithm to extract the mesh. Since it does not require pGT surfaces from   \n538 FreeSurfer for training supervision, it can be treated as a weakly supervised learning-based baseline.   \n539 CorticalFlow $^{++}$ [47] utilizes smoothed convex hulls as the initialization template, trains a chain of   \n540 deformation fields, and employs a fourth-order Runge-Kutta (RK4) solver to compute the integration   \n541 for the initial value problem. CortexODE[31] uses WM segmentation for surface initialization and   \n542 Neural ODE for deformation computation. Vox2cortex [8] deforms averaged surface templates with   \n543 a GNN-based network to reconstruct multiple surfaces. CoCSR [54] integrates multiple cortical   \n544 surface reconstructions into a single network. A summary of the state-of-the-art CSR methods is   \n545 provided in Table 4. ", "page_idx": 13}, {"type": "table", "img_path": "vKwf15M5EE/tmp/649e90fb017c013f691c352945f98d22fedc83418c49e6f6e53023503550488f.jpg", "table_caption": ["Table 4: Summary of baseline methods in terms of surface representation, supervision in training, and loss functions. "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "546 C More Experimental Results ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "547 C.1 Quantitative comparison of our methods with Related Works ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "548 Due to space limit, we only showcase the quantitative results on left hemisphere in the main paper.   \n549 Quantitative comparison results on the right hemisphere are summarized as a supplement to Table 1   \n550 in the main paper. ", "page_idx": 13}, {"type": "text", "text": "Table 5: Quantitative analysis of cortical surface reconstruction on geometric accuracy and selfintersections. The Chamfer distance (CD), average symmetric surface distance (ASSD), Hausdorff distance (HD), and the ratio of the self-intersecting faces (SIF) were measured for WM and pial surfaces on three datasets. The mean value and standard deviation are reported. Lower scores indicate better results for all metrics. \u201cS\u201d denotes the use of pGT surfaces from conventional pipelines, while \u201cW\u201d represents weak supervision by pGT ribbon segmentations. In each supervision setting, the best results are in bold, and the second best results are underlined. ", "page_idx": 14}, {"type": "table", "img_path": "vKwf15M5EE/tmp/fffa8d0e7acfc6ee10e4c9cd92a9bd660ede58853eb9505f132d2ba1cff17259.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "551 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "553 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n554 paper\u2019s contributions and scope?   \n555 Answer: [Yes]   \n556 Justification: We clearly summarize the contributions in Section 1 Introduction.   \n557 Guidelines:   \n558 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n559 made in the paper.   \n560 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n561 contributions made in the paper and important assumptions and limitations. A No or   \n562 NA answer to this question will not be perceived well by the reviewers.   \n563 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n564 much the results can be expected to generalize to other settings.   \n565 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n566 are not attained by the paper.   \n567 2. Limitations   \n568 Question: Does the paper discuss the limitations of the work performed by the authors?   \n569 Answer: [Yes]   \n570 Justification: We discuss the limitations of the work in Section 5 Conclusions.   \n571 Guidelines:   \n572 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n573 the paper has limitations, but those are not discussed in the paper.   \n574 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n575 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n576 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n577 model well-specification, asymptotic approximations only holding locally). The authors   \n578 should reflect on how these assumptions might be violated in practice and what the   \n579 implications would be.   \n580 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n581 only tested on a few datasets or with a few runs. In general, empirical results often   \n582 depend on implicit assumptions, which should be articulated.   \n583 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \n584 For example, a facial recognition algorithm may perform poorly when image resolution   \n585 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n586 used reliably to provide closed captions for online lectures because it fails to handle   \n587 technical jargon.   \n588 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n589 and how they scale with dataset size.   \n590 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n591 address problems of privacy and fairness.   \n592 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n593 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n594 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n595 judgment and recognize that individual actions in favor of transparency play an impor  \n596 tant role in developing norms that preserve the integrity of the community. Reviewers   \n597 will be specifically instructed to not penalize honesty concerning limitations.   \n598 3. Theory Assumptions and Proofs   \n599 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n600 a complete (and correct) proof?   \n601 Answer: [NA]   \n602 Justification: This is not a theoretical paper.   \n603 Guidelines:   \n604 \u2022 The answer NA means that the paper does not include theoretical results.   \n605 \u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross  \n606 referenced.   \n607 \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n608 \u2022 The proofs can either appear in the main paper or the supplemental material, but if   \n609 they appear in the supplemental material, the authors are encouraged to provide a short   \n610 proof sketch to provide intuition.   \n611 \u2022 Inversely, any informal proof provided in the core of the paper should be complemented   \n612 by formal proofs provided in appendix or supplemental material.   \n613 \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced.   \n614 4. Experimental Result Reproducibility   \n615 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n616 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n617 of the paper (regardless of whether the code and data are provided or not)?   \n618 Answer: [Yes]   \n619 Justification: We clearly illustrate the methodology in Section 3, the experimental setups in   \n620 Section 4.1, and more implementation details in the Supplementary Materials.   \n621 Guidelines:   \n622 \u2022 The answer NA means that the paper does not include experiments.   \n623 \u2022 If the paper includes experiments, a No answer to this question will not be perceived   \n624 well by the reviewers: Making the paper reproducible is important, regardless of   \n625 whether the code and data are provided or not.   \n626 \u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken   \n627 to make their results reproducible or verifiable.   \n628 \u2022 Depending on the contribution, reproducibility can be accomplished in various ways.   \n629 For example, if the contribution is a novel architecture, describing the architecture fully   \n630 might suffice, or if the contribution is a specific model and empirical evaluation, it may   \n631 be necessary to either make it possible for others to replicate the model with the same   \n632 dataset, or provide access to the model. In general. releasing code and data is often   \n633 one good way to accomplish this, but reproducibility can also be provided via detailed   \n634 instructions for how to replicate the results, access to a hosted model (e.g., in the case   \n635 of a large language model), releasing of a model checkpoint, or other means that are   \n636 appropriate to the research performed.   \n637 \u2022 While NeurIPS does not require releasing code, the conference does require all submis  \n638 sions to provide some reasonable avenue for reproducibility, which may depend on the   \n639 nature of the contribution. For example   \n640 (a) If the contribution is primarily a new algorithm, the paper should make it clear how   \n641 to reproduce that algorithm.   \n642 (b) If the contribution is primarily a new model architecture, the paper should describe   \n643 the architecture clearly and fully.   \n644 (c) If the contribution is a new model (e.g., a large language model), then there should   \n645 either be a way to access this model for reproducing the results or a way to reproduce   \n646 the model (e.g., with an open-source dataset or instructions for how to construct   \n647 the dataset).   \n648 (d) We recognize that reproducibility may be tricky in some cases, in which case   \n649 authors are welcome to describe the particular way they provide for reproducibility.   \n650 In the case of closed-source models, it may be that access to the model is limited in   \n651 some way (e.g., to registered users), but it should be possible for other researchers   \n652 to have some path to reproducing or verifying the results.   \n653 5. Open access to data and code   \n654 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n655 tions to faithfully reproduce the main experimental results, as described in supplemental   \n656 material?   \n657 Answer: [No]   \n658 Justification: (1) We used and cited public datasets and gave pre-processing steps in Sec  \n659 tion 4.1 and more details in the Supplementary Materials. (2) We provided code links for   \n660 public baselines in the Supplementary Materials. (3) Code of our proposed method will be   \n661 made public upon acceptance of the paper.   \n662 Guidelines:   \n663 \u2022 The answer NA means that paper does not include experiments requiring code.   \n664 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n665 public/guides/CodeSubmissionPolicy) for more details.   \n666 \u2022 While we encourage the release of code and data, we understand that this might not be   \n667 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n668 including code, unless this is central to the contribution (e.g., for a new open-source   \n669 benchmark).   \n670 \u2022 The instructions should contain the exact command and environment needed to run to   \n671 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n672 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n673 \u2022 The authors should provide instructions on data access and preparation, including how   \n674 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n675 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n676 proposed method and baselines. If only a subset of experiments are reproducible, they   \n677 should state which ones are omitted from the script and why.   \n678 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n679 versions (if applicable).   \n680 \u2022 Providing as much information as possible in supplemental material (appended to the   \n681 paper) is recommended, but including URLs to data and code is permitted.   \n682 6. Experimental Setting/Details ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 16}, {"type": "text", "text": "686 Answer: [Yes]   \n687 Justification: We specify all the training and test details in Section 4.1 and the Supplementary   \n688 Materials.   \n689 Guidelines:   \n690 \u2022 The answer NA means that the paper does not include experiments.   \n691 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n692 that is necessary to appreciate the results and make sense of them.   \n693 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n694 material.   \n695 7. Experiment Statistical Significance   \n696 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n697 information about the statistical significance of the experiments?   \n698 Answer: [Yes]   \n699 Justification: We reported mean and standard deviation of each experimental setting and   \n700 computed the statistical significance.   \n701 Guidelines:   \n702 \u2022 The answer NA means that the paper does not include experiments.   \n703 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n704 dence intervals, or statistical significance tests, at least for the experiments that support   \n705 the main claims of the paper.   \n706 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n707 example, train/test split, initialization, random drawing of some parameter, or overall   \n708 run with given experimental conditions).   \n709 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n710 call to a library function, bootstrap, etc.)   \n711 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n712 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n713 of the mean.   \n714 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n715 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n716 of Normality of errors is not verified.   \n717 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n718 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n719 error rates).   \n720 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n721 they were calculated and reference the corresponding figures or tables in the text.   \n722 8. Experiments Compute Resources   \n723 Question: For each experiment, does the paper provide sufficient information on the com  \n724 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n725 the experiments?   \n726 Answer: [Yes]   \n727 Justification: We specify the computation resources for training and testing in Section 4.1   \n728 and more details in the Supplementary Materials.   \n729 Guidelines:   \n730 \u2022 The answer NA means that the paper does not include experiments.   \n731 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n732 or cloud provider, including relevant memory and storage.   \n733 \u2022 The paper should provide the amount of compute required for each of the individual   \n734 experimental runs as well as estimate the total compute.   \n735 \u2022 The paper should disclose whether the full research project required more compute   \n736 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n737 didn\u2019t make it into the paper).   \n738 9. Code Of Ethics   \n739 Question: Does the research conducted in the paper conform, in every respect, with the   \n740 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n741 Answer: [Yes]   \n742 Justification: We review and conform with the NeurIPS Code of Ethics.   \n743 Guidelines:   \n744 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n745 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n746 deviation from the Code of Ethics.   \n747 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n748 eration due to laws or regulations in their jurisdiction).   \n749 10. Broader Impacts   \n750 Question: Does the paper discuss both potential positive societal impacts and negative   \n751 societal impacts of the work performed?   \n752 Answer: [Yes]   \n753 Justification: We discuss the societal impacts of the work in Section 5 Conclusions.   \n754 Guidelines:   \n755 \u2022 The answer NA means that there is no societal impact of the work performed.   \n756 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n757 impact or why the paper does not address societal impact.   \n758 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n759 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n760 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n761 groups), privacy considerations, and security considerations.   \n762 \u2022 The conference expects that many papers will be foundational research and not tied   \n763 to particular applications, let alone deployments. However, if there is a direct path to   \n764 any negative applications, the authors should point it out. For example, it is legitimate   \n765 to point out that an improvement in the quality of generative models could be used to   \n766 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n767 that a generic algorithm for optimizing neural networks could enable people to train   \n768 models that generate Deepfakes faster.   \n769 \u2022 The authors should consider possible harms that could arise when the technology is   \n770 being used as intended and functioning correctly, harms that could arise when the   \n771 technology is being used as intended but gives incorrect results, and harms following   \n772 from (intentional or unintentional) misuse of the technology.   \n773 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n774 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n775 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n776 feedback over time, improving the efficiency and accessibility of ML).   \n777 11. Safeguards   \n778 Question: Does the paper describe safeguards that have been put in place for responsible   \n779 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n780 image generators, or scraped datasets)?   \n781 Answer: [NA]   \n782 Justification: Our paper poses no such risks.   \n783 Guidelines:   \n784 \u2022 The answer NA means that the paper poses no such risks.   \n785 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n786 necessary safeguards to allow for controlled use of the model, for example by requiring   \n787 that users adhere to usage guidelines or restrictions to access the model or implementing   \n788 safety filters.   \n789 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n790 should describe how they avoided releasing unsafe images.   \n791 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n792 not require this, but we encourage authors to take this into account and make a best   \n793 faith effort.   \n794 12. Licenses for existing assets   \n795 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n796 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n797 properly respected?   \n798 Answer: [Yes]   \n799 Justification: We properly cite relevant baseline methods and datasets.   \n800 Guidelines:   \n801 \u2022 The answer NA means that the paper does not use existing assets.   \n802 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n803 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n804 URL.   \n805 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n806 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n807 service of that source should be provided.   \n808 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n809 package should be provided. For popular datasets, paperswithcode.com/datasets   \n810 has curated licenses for some datasets. Their licensing guide can help determine the   \n811 license of a dataset.   \n812 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n813 the derived asset (if it has changed) should be provided.   \n814 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n815 the asset\u2019s creators.   \n816 13. New Assets ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We communicate the details of the dataset/code/model in our main paper and Supplementary Materials. We will make code and model public upon the acceptance of the paper. ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 19}, {"type": "text", "text": "32 14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "839   \n840   \n841   \n842   \n843   \n844   \n845   \n846   \n847   \n848   \n849   \n850   \n851   \n852   \n853   \n854   \n855   \n856   \n857   \n858   \n859   \n860   \n861   \n862   \n863   \n864   \n865 ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. \u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. \u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. \u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 20}]