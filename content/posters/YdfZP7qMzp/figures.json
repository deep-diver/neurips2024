[{"figure_path": "YdfZP7qMzp/figures/figures_1_1.jpg", "caption": "Figure 1: Comparison of classical pipelines for video classification and generation tasks with our proposed GenRec method. (a) Classification: Typical video classification focus on understanding complete videos. (b) Diffusion Generation: Diffusion models learn the noise reduction trajectory from videos with varying levels of noise. These two distinct training paradigms present challenges for task unification. To bridge this gap, we propose (c) GenRec: a learning framework that processes mask frames VM using a masking function M(\u00b7) and noise videos V with noise sampling N(\u00b7, \u03c3), aiming to simultaneously learn video understanding and content completion with the same partially observed visual content.", "description": "This figure compares three different approaches for video processing: (a) traditional video classification using full videos, (b) diffusion-based video generation using noisy videos, and (c) the proposed GenRec method. GenRec combines both tasks by processing masked and noisy video frames. The masking function and noise sampling are employed to learn generalized spatial-temporal representations.", "section": "1 Introduction"}, {"figure_path": "YdfZP7qMzp/figures/figures_3_1.jpg", "caption": "Figure 2: The pipeline of our proposed video processing method. The input video is first processed by a pretrained encoder E to produce a latent representation zo, then undergoes diffusion to generate a noisy latent \u017ei. The random mask m is used to create the masked latent Zo. During training, the noisy latent is concatenated with the masked latent as condition and fed into a Spatial-Temporal UNet, resulting in both reconstruction and recognition outputs. The reconstructed latent can be decoded by the pretrained decoder D to produce the final generated video.", "description": "This figure illustrates the GenRec model's architecture and training process. The input video is first encoded into a latent representation. Then, a diffusion process introduces noise, and a random mask is applied to a subset of the latent frames.  The noisy and masked latent representations are concatenated and fed into a spatial-temporal UNet, which learns to reconstruct the original latent representation and perform video classification. The reconstructed latent is decoded to generate the final video. This unified framework enables joint optimization of video generation and recognition.", "section": "3 GenRec"}, {"figure_path": "YdfZP7qMzp/figures/figures_7_1.jpg", "caption": "Figure 2: The pipeline of our proposed video processing method. The input video is first processed by a pretrained encoder E to produce a latent representation zo, then undergoes diffusion to generate a noisy latent \u017ei. The random mask m is used to create the masked latent Zo. During training, the noisy latent is concatenated with the masked latent as condition and fed into a Spatial-Temporal UNet, resulting in both reconstruction and recognition outputs. The reconstructed latent can be decoded by the pretrained decoder D to produce the final generated video.", "description": "This figure illustrates the pipeline of GenRec, a unified framework for video generation and recognition.  It shows how an input video is first encoded into a latent representation. Then, a random mask is applied to a subset of latent frames, and Gaussian noise is added to create a noisy latent representation. These representations are concatenated and fed into a Spatial-Temporal UNet for training. The UNet learns to reconstruct the original latent representation from the noisy and masked inputs, performing video generation, and simultaneously learns to classify the video content.", "section": "3 GenRec"}, {"figure_path": "YdfZP7qMzp/figures/figures_16_1.jpg", "caption": "Figure 2: The pipeline of our proposed video processing method. The input video is first processed by a pretrained encoder E to produce a latent representation zo, then undergoes diffusion to generate a noisy latent \u017e\u0165. The random mask m is used to create the masked latent Zo. During training, the noisy latent is concatenated with the masked latent as condition and fed into a Spatial-Temporal UNet, resulting in both reconstruction and recognition outputs. The reconstructed latent can be decoded by the pretrained decoder D to produce the final generated video.", "description": "This figure illustrates the GenRec framework's architecture.  An input video is encoded into a latent representation.  This latent representation is then noised and randomly masked.  Both the noisy and masked representations are fed into a Spatial-Temporal U-Net, which simultaneously learns to reconstruct the original video and perform video classification. The reconstructed latent representation is then decoded to produce the generated video. This unified training approach allows GenRec to excel in both video generation and recognition tasks.", "section": "3 GenRec"}, {"figure_path": "YdfZP7qMzp/figures/figures_17_1.jpg", "caption": "Figure 5: Video generation case study. We generate videos given the first frame and the last frame.", "description": "This figure shows four examples of video generation using only the first and last frames as input.  The top row of each example shows the ground truth video sequence. The bottom row shows the video generated by the GenRec model. The red boxes highlight that the model successfully generated the missing frames between the given first and last frame, demonstrating its ability to interpolate and generate temporally coherent video content.", "section": "4.2 Main Results"}, {"figure_path": "YdfZP7qMzp/figures/figures_18_1.jpg", "caption": "Figure 6: Video generation case study. We compare our methods with SEER [18] in the setting of generating videos given the first frame together with the classifier guidance. Cases are picked from the official website of SEER [18].", "description": "This figure compares the video generation results of the proposed GenRec method with the state-of-the-art method SEER for three different actions: Pushing something from left to right, Covering something with something, and Dropping something in front of something.  For each action, the ground truth video (GT) is shown along with the generated videos from SEER and GenRec.  The comparison highlights the visual quality and temporal consistency of the generated videos from each method.", "section": "4.2 Main Results"}]