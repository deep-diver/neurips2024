[{"figure_path": "S0Ci1AsJL5/figures/figures_9_1.jpg", "caption": "Figure 1: Subfigure (a): Rescaled error \u221an||\u03b8\u03b7 \u2013 \u03b8*||, averaged over N independent TD trajectories for different trajectory lengths n. Subfigure (b): approximate quantity \u0394n from (23) for different powers \u03b3 and n. Subfigure (c): \u0394n, rescaled by a factor n1/4, predicted by Theorem 2.", "description": "This figure presents the results of numerical experiments for the TD learning algorithm. Subfigure (a) shows the rescaled error of the Polyak-Ruppert averaged TD learning iterates for different trajectory lengths n and learning rates \u03b3. Subfigure (b) shows the approximate quantity \u0394n, which measures the accuracy of the normal approximation of the TD learning iterates, for different powers \u03b3 and n. Subfigure (c) shows the same quantity \u0394n rescaled by a factor of n1/4. The results show that the accuracy of the normal approximation is of order n\u22121/4 for \u03b3 = 1/2, and slower for other learning rates. This is consistent with the theoretical findings of Theorem 2 in the paper.", "section": "Applications to the TD learning and numerical results"}, {"figure_path": "S0Ci1AsJL5/figures/figures_45_1.jpg", "caption": "Figure 2: Subfigure (a): Rescaled error \u221an||\u03b8\u03b7 \u2013 \u03b8*||, averaged over N independent TD trajectories for different trajectory lengths n. Subfigure (b): same for ||\u03b8\u03b7 - \u03b8*||.", "description": "This figure shows the results of the temporal difference (TD) learning experiments.  Subfigure (a) displays the rescaled error, showing how the error decreases as the number of trajectories (n) increases. The rescaling is done to make it easier to see differences between learning rates (y). Subfigure (b) shows the same error, but without rescaling, making it apparent how the error magnitude changes.", "section": "5 Applications to the TD learning and numerical results"}, {"figure_path": "S0Ci1AsJL5/figures/figures_45_2.jpg", "caption": "Figure 3: Subfigure (a): Rescaled error \u2206n n\u00b9/\u2074. Subfigure (b): same with logarithmic scale on y-axis.", "description": "Figure 3 shows two plots that present the rescaled error \u2206n n\u00b9/\u2074, which represents the accuracy of the normal approximation for the distribution of the Polyak-Ruppert averaged LSA iterates in the context of temporal difference (TD) learning.  Subfigure (a) shows the rescaled error on a linear y-axis scale while Subfigure (b) uses a logarithmic y-axis for a clearer visualization of smaller error values. Different lines in the plots represent different step size decay rates (\u03b3 values) in the TD learning algorithm, illustrating how the choice of step size impacts the accuracy of the normal approximation. The figure aims to demonstrate the tightness of the theoretical bounds provided in Theorem 2, suggesting that the best approximation rate is achieved when \u03b3 = 1/2.", "section": "5 Applications to the TD learning and numerical results"}]