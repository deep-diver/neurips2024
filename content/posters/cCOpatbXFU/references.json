{"references": [{"fullname_first_author": "H. H. Bauschke", "paper_title": "Legendre functions and the method of random Bregman projections", "publication_date": "1997-01-01", "reason": "This paper provides foundational results on Bregman divergences and their applications to optimization algorithms, which are heavily used in the current work."}, {"fullname_first_author": "H. H. Bauschke", "paper_title": "A descent lemma beyond Lipschitz gradient continuity: first-order methods revisited and applications", "publication_date": "2017-01-01", "reason": "This paper extends the classic descent lemma to a more general setting, which is crucial for analyzing the convergence of mirror descent methods under relative smoothness assumptions."}, {"fullname_first_author": "L. Bottou", "paper_title": "Large-scale machine learning with stochastic gradient descent", "publication_date": "2010-01-01", "reason": "This paper is a seminal work in stochastic optimization, providing a comprehensive overview of SGD and its applications to large-scale machine learning problems, which are relevant to this work."}, {"fullname_first_author": "S. Bubeck", "paper_title": "Convex optimization: Algorithms and complexity", "publication_date": "2015-01-01", "reason": "This foundational work provides a comprehensive overview of convex optimization algorithms and their complexity, which is essential for understanding the theoretical underpinnings of this work."}, {"fullname_first_author": "R.-A. Dragomir", "paper_title": "Fast stochastic Bregman gradient methods: Sharp analysis and variance reduction", "publication_date": "2021-01-01", "reason": "This paper provides sharp convergence rates for stochastic Bregman gradient methods, which are directly relevant to the current work and are significantly improved by the new analysis proposed here."}]}