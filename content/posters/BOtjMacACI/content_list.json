[{"type": "text", "text": "Efficient Adaptation of Pre-trained Vision Transformer via Householder Transformation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Wei Dong1, Yuan $\\mathbf{Sun}^{1}$ , Yiting $\\mathbf{Yang^{1}}$ , Xing Zhang1, Zhijun Lin2, Qingsen Yan2, Haokui Zhang2, Peng Wang3\u2217, Yang Yang3, Hengtao Shen34 ", "page_idx": 0}, {"type": "text", "text": "1College of Information and Control Engineering, Xi\u2019an University of Architecture and Technology 2School of Computer Science, Northwestern Polytechnical University 3School of Computer Science and Engineering, University of Electronic Science and Technology of China 4School of Computer Science and Technology, Tongji University ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "A common strategy for Parameter-Efficient Fine-Tuning (PEFT) of pre-trained Vision Transformers (ViTs) involves adapting the model to downstream tasks by learning a low-rank adaptation matrix. This matrix is decomposed into a product of down-projection and up-projection matrices, with the bottleneck dimensionality being crucial for reducing the number of learnable parameters, as exemplified by prevalent methods like LoRA and Adapter. However, these low-rank strategies typically employ a fixed bottleneck dimensionality, which limits their flexibility in handling layer-wise variations. To address this limitation, we propose a novel PEFT approach inspired by Singular Value Decomposition (SVD) for representing the adaptation matrix. SVD decomposes a matrix into the product of a left unitary matrix, a diagonal matrix of scaling values, and a right unitary matrix. We utilize Householder transformations to construct orthogonal matrices that efficiently mimic the unitary matrices, requiring only a vector. The diagonal values are learned in a layer-wise manner, allowing them to flexibly capture the unique properties of each layer. This approach enables the generation of adaptation matrices with varying ranks across different layers, providing greater flexibility in adapting pre-trained models. Experiments on standard downstream vision tasks demonstrate that our method achieves promising fine-tuning performance. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Parameter-Efficient Fine-Tuning (PEFT) for pre-trained Vision Transformers (ViTs) aims to adapt these models to downstream tasks by learning a small set of parameters while keeping most or all of the original model parameters frozen. This approach is expected to reduce the cost of fine-tuning and potentially improve the model\u2019s generalization performance, particularly when the downstream task involves limited data. ", "page_idx": 0}, {"type": "text", "text": "A common strategy for adapting the parameters is to learn an adaptation matrix that modifies the original matrix through addition or multiplication. To reduce the parameter scale of the adaptation matrix, a low-rank strategy is typically employed. This involves decomposing the adaptation matrix into the product of a down-projection matrix and an up-projection matrix, where the bottleneck dimensionality determines the parameter scale. Many prevailing PEFT solutions [1\u20133] follow this approach. However, these methods usually set the bottleneck dimensionality empirically to balance adaptation performance and parameter size. The fixed dimensionality, however, lacks the flexibility to accommodate variations in layer-wise properties. ", "page_idx": 0}, {"type": "text", "text": "In this work, we propose a novel parameter-efficient adaptation method to fine-tune pre-trained ViTs. Our design of the adaptation matrix is inspired by Singular Value Decomposition (SVD), which decomposes a matrix into a product of a left unitary matrix, a diagonal matrix, and a right unitary matrix. In SVD, the unitary matrices consist of orthogonal vectors, and the diagonal matrix of singular values essentially determines the rank of the matrix. Inspired by SVD, we propose to use Householder transformations to replace the left and right unitary matrices. Householder transformations maintain orthogonality properties similar to unitary matrices but can be derived simply by a vector, making them parameter efficient. With left and right Householder matrices, we learn the diagonal matrix adaptively for each layer to accommodate layer-wise properties. This approach, termed the Householder Transformation-based Adaptor (HTA), enables us to derive the adaptation matrix in a parameter-efficient manner while theoretically allowing for varying ranks for the adaptation matrices, thus achieving a better balance between parameter efficiency and adaptation performance. ", "page_idx": 1}, {"type": "text", "text": "We conducted experiments on a set of downstream vision classification tasks. The results show that our method can be effectively applied to various ViT versions, achieving promising fine-tuning performance. In summary, the contributions of this work can be summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We approach PEFT from a novel angle by viewing the adaptation matrix from the perspective of SVD, which inspires us to propose a Householder transformation-based adaptation strategy that is parameter-efficient.   \n\u2022 By learning scaling coefficients to compose Householder transformation matrices together into adaptation matrices, our method can theoretically allow varying adaptation matrix ranks to accommodate layer-wise variations.   \n\u2022 Experiments on two sets of downstream vision classification tasks reveal our method can achieve an appealing balance between adaptation performance and parameter efficiency. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Pre-training and Transfer Learning ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "As an advanced learning strategy, extensive research [4\u20137] has demonstrated the wide applicability of transfer learning across various fields. Especially in cases where the target task has limited data, high labeling costs, or poor data quality [8\u201310], transfer learning significantly enhances model generalization and training efficiency. By pre-training on large-scale datasets and using the obtained parameters as initialization for downstream tasks, transfer learning can effectively transfer and apply the knowledge of pre-trained models. In this process, the performance and convergence speed of downstream tasks are highly correlated with the dataset used for pre-training the model. In the field of computer vision, pre-training on large-scale datasets such as ImageNet [11] has significantly improved the performance of tasks like image classification [12\u201315], object detection [16, 17], and semantic segmentation [18, 19]. Moreover, self-supervised pre-training [20, 21] leverages the advantage of not requiring large amounts of labeled data, expanding the data scale and enhancing feature extraction capabilities, thereby further improving the generalization ability and robustness of pre-trained models. However, due to the substantial computational resources required to fully fine-tune the parameters of pre-trained models in downstream tasks, current research has shifted towards exploring more efficient fine-tuning methods. ", "page_idx": 1}, {"type": "text", "text": "2.2 Parameter-Efficient Fine-Tuning (PEFT) ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Compared to full fine-tuning, the PEFT methods [22\u201325, 2, 26] aim to reduce the high cost of fine-tuning by freezing the majority of parameters in the pre-trained model and introducing a small number of learnable parameters to adapt to specific downstream tasks. ", "page_idx": 1}, {"type": "text", "text": "With the development of large pre-trained models, various PEFT approaches have emerged. Adapter [22] inserts a bottleneck-structured adapter layer into the pre-trained model and refines the model by updating only the parameters within the adapter layer. Bias [23] focuses on the finetuning of models for specific downstream tasks by meticulously calibrating the bias terms. VPT [24] integrates the concept of prompt learning into visual tasks, thereby facilitating targeted optimization for specific downstream tasks. SSF [25] efficiently fine-tunes the weights in pre-trained models through affine transformations composed of scaling and shifting operations. AdaptFormer [2] explores a parallel adapter solution on ViT for various downstream tasks. FacT [26] decomposes the weights of ViT into individual three-dimensional tensors and further decomposes the increments into lightweight factors. During fine-tuning phase, these factors are updated and stored, effectively reducing computational overhead. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2.3 LoRA and its variants ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "As represented by LoRA [1], the core of this type of PEFT method is the utilization of low-rank matrices to approximate weight adjustments during the fine-tuning phase. By employing reparameterization techniques, these low-rank matrices are combined with the existing parameter matrices, thereby circumventing extra inference costs. AdaLoRA [27] employs singular value decomposition to decompose weight matrices, pruning insignificant singular values to effectively reduce the number of parameters. ARC [3] uses symmetric up-down projections to create cross-layer shared bottleneck operations. By learning low-dimensional rescaling coefficients, it effectively recombines layer-adaptive adapters, reducing the costs of fine-tuning. FedPara [28] reparameterizes model layers with low-rank matrices and uses the Hadamard product. This approach, unconstrained by low-rank limitations, offers greater capacity and reduces learning costs. RLRR [29] examines mainstream PEFT methods from the perspective of SVD decomposition, exploring the critical balance between preserving generalization in pre-trained models and adapting them to downstream tasks. Our research abandons the traditional fixed-rank approach, opting instead for a more flexible adjustment of parameter matrices using a small number of learnable parameters. ", "page_idx": 2}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we commence with an introduction of the notations, symbols, and contextual background related to low-rank adaptations and Householder transformation. Then we present the decomposed structure of low-rank adaptation and discuss its inherent operating mechanism from the perspective of singular value decomposition. Finally, we propose a novel adaptation via Householder transformation. This adaptation primarily aims to construct the Householder unitary matrices via a learnable weight vector, thereby trading-off the fully spanned representation space and the affordable parameter size. ", "page_idx": 2}, {"type": "text", "text": "3.1 Preliminary knowledge ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Low-rank adaptation. Pre-trained ViT models are typically initialized with weights learned from large-scale image datasets, such as ImageNet. The pre-training process involves optimizing the model on an unsupervised or supervised pretext task. The resulting pre-trained weights encode rich semantic information that can be transferred to a wide range of downstream tasks through fine-tuning. As one of the most representative methods of fine-tuning, PEFT method achieves excellent results on downstream tasks by merely utilizing a small number of additional learnable parameters to fine-tune the ViT. The most prevalent PEFT is the adaptation method, which can be divided into two categories: LoRA-based and adapter-based methods. In general, LoRA-based method is defined as: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{X}_{\\mathrm{FT}}^{(l-1)}=\\mathbf{X}^{(l-1)}(\\mathbf{W}^{(l)}+\\mathbf{W}_{\\mathrm{down}}^{(l)}\\mathbf{W}_{\\mathrm{up}}^{(l)})+\\vec{\\pmb{b}}^{(l)\\top},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathbf{X}_{\\mathrm{FT}}^{(l-1)}$ is the fine-tuning output, $\\mathbf{W}^{(l)}$ is any linear weight projection $\\{\\mathbf{W}_{q}^{(l)},\\mathbf{W}_{k}^{(l)},\\mathbf{W}_{v}^{(l)},\\mathbf{W}_{o}^{(l)},\\mathbf{W}_{\\mathrm{FC1}}^{(l)},\\mathbf{W}_{\\mathrm{FC2}}^{(l)}\\}$ in ViT, $\\vec{b}^{(l)}$ is the bias weights, $\\mathbf{W}_{\\mathrm{down}}^{(l)}\\in\\mathbb{R}^{D^{(l)}\\times D^{\\prime}}$ and $\\mathbf{W}_{\\mathrm{up}}^{(l)}\\,\\in\\,\\mathbb{R}^{D^{\\prime}\\times D^{(l)}}$ are down- and up-adapting projection matrices across varying layers with the dimensionality $D^{\\prime}\\ll D^{(l)}$ . The detailed framework of LoRA-based method is shown in Fig. 1 (a). Analogously, adapter-based method is described as: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{X}_{\\mathrm{FT}}^{(l-1)}=\\mathrm{Act}(\\mathrm{MHA}(\\mathbf{X}^{(l-1)})\\mathbf{W}_{\\mathrm{down}}^{\\mathrm{MHA}(l)})\\mathbf{W}_{\\mathrm{up}}^{\\mathrm{MHA}(l)},}\\\\ &{\\quad\\mathbf{X}_{\\mathrm{FT}}^{(l)}=\\mathrm{Act}(\\mathrm{FFN}(\\mathbf{X}_{\\mathrm{FT}}^{(l-1)})\\mathbf{W}_{\\mathrm{down}}^{\\mathrm{FFN}(l)})\\mathbf{W}_{\\mathrm{up}}^{\\mathrm{FFN}(l)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "with the activation function $\\operatorname{Act}(\\cdot)$ , Multi-Head Attention (MHA), and Feed-Forward Network (FFN) modules in ViT. The detail of adapter-based method is shown in Fig. 1 (c). By observing W(l) iE.eq.., $\\mathbf{W}_{\\mathrm{adaptation}}^{(l)}=\\mathbf{W}_{\\mathrm{down}}^{(l)}\\mathbf{W}_{\\mathrm{up}}^{(l)}$ .t ioNnoeted  tPhEatF wT e mreetmhoovdes  tihnev oalcvteiv aat iloonw -irna tnhke  blootwtl-eranneck kb sottrtluecntuercek, because the presence or absence of such activation does not affect the low-rank structure of adaptation. ", "page_idx": 2}, {"type": "image", "img_path": "BOtjMacACI/tmp/5a463796d0cc0ccacd04083c5ef2f5790bdaa93272dc1a8f63e95cec8e292aa4.jpg", "img_caption": ["Figure 1: Underpinned by (a) LoRA [1] and (c) Adapter [22], we utilize Householder matrix to construct Householder transformation-based adaptations, involving (b) LoRA-based method with HTA and (d) Adapter-based method with HTA. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Householder transformation. Householder transformation, or Householder reflection, is a linear transformation that reflects a vector across a hyperplane defined by a Householder vector. It is characterized by a Householder matrix, which is an orthogonal and symmetrical matrix with determinant -1. This transformation, initially proposed by A.S. Householder in 1958 [30], has significant applications in numerical linear algebra [31], particularly in QR decomposition [32], where it is used to transform a matrix into an upper triangular or Hessenberg form [33]. Householder transformation can also be employed to set specific elements of a vector to zero while preserving its norm, making it a valuable tool for matrix orthogonalization and symmetrization. The Householder matrix is defined as following: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{H}=\\mathbf{I}-\\vec{v}\\vec{v}^{\\top},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "with the identity matrix I and the Householder vector $\\vec{\\pmb{v}}$ . ", "page_idx": 3}, {"type": "text", "text": "3.2 Viewing the adaptation matrix from SVD ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Singular Value Decomposition (SVD) offers a profound insight into matrix factorization. It breaks down a matrix into three constituent matrices. Viewing the adaptation matrix through the lens of SVD, we represent it as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{W}_{\\mathrm{adaptation}}^{(l)}=\\mathbf{W}_{\\mathrm{left}}^{(l)}\\mathbf{D}^{(l)}\\mathbf{W}_{\\mathrm{right}}^{(l)},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathbf{W}_{\\mathrm{left}}^{(l)}\\in\\mathbb{R}^{D^{(l)}\\times D^{(l)}}$ is the left unitary matrix and $\\mathbf{W}_{\\mathrm{right}}^{(l)}\\in\\mathbb{R}^{D^{(l)}\\times D^{(l)}}$ is the right unitary matrix; and $\\mathbf{D}^{(l)}\\in\\mathbb{R}^{D^{(l)}\\times D^{(l)}}$ is the diagonal matrix in which the diagonal elements of a diagonal matrix are singular values. Unitary matrices $\\mathbf{W}_{\\mathrm{left}}^{(l)}$ Wl(elf)t and Wr(ilg)ht essentially characterize the rotation tbrya ntshfeo ramdaatpitoantsi aitnreiaxr $\\mathbf{W}_{\\mathrm{adaptation}}^{(l)}$ e iftn tuon itthaer ys pmaactrei xit $\\mathbf{W}_{\\mathrm{left}}^{(l)}$ s .r otTahteesn ,a nt haer bvitercatroyr  vies cstocra lemdu ltbiyp ltiehde diagonal matrix D(l). Finally, the right unitary matrix Wr(ilg)h t rotates the vector back to the original linear space. Therefore, the SVD decomposition characterizes the transformations of rotation and scaling in the linear space. ", "page_idx": 4}, {"type": "text", "text": "When it comes to the fine-tuning strategy, PEFT methods employ ViT as the backbone and essentially fine-tune the learnable parameters of unitary matrices l(elf)t \u2208RD(l)\u00d7D\u2032 and Wr(ilg)h $\\mathbf{W}_{\\mathrm{right}}^{(l)}\\in\\mathbb{R}^{D^{\\prime}\\times D^{(l)}}$ and the learnable singular values of diagonal matrix $\\mathbf{D}^{(l)}\\in\\mathbb{R}^{D^{\\prime}\\times D^{\\prime}}$ to downstream tasks, implicitly performing the rotation and scaling transformations. Note that the number of non-zero singular values in matrix $\\mathbf{D}^{(l)}$ does not exceed its dimensionality $D^{\\prime}$ . However, the fixed bottleneck dimensionality $D^{\\prime}$ empirically set to LoRA- or adapter-based methods is inflexible, thereby without accommodating variations in layer-wise properties. This implies that the linear space spanned by the low-rank adaptation matrix and its corresponding number of non-zero singular values is constrained within a low dimensionality $D^{\\prime}$ . Increasing the dimensionality $D^{\\prime}$ could effectively enhance the space capacity of the adaptation matrix, thereby improving the performance potential of the fine-tuned ViT model. However, this also further increases the number of parameters in the PEFT method. ", "page_idx": 4}, {"type": "text", "text": "3.3 Householder transformation-based adaptation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To address the aforementioned issue, we introduce Householder transformation into the adaptation matrix learning, and propose the Householder Transformation-based Adaptor (HTA). Following this way, HTA facilitates the derivation of the adaptation matrix in a manner that is parameter-efficient, while theoretically accommodating the flexibility of varying ranks for the adaptation matrices. ", "page_idx": 4}, {"type": "text", "text": "In our approach, we respectively employ the Householder matrices Hl(elf)t $\\mathbf{H}_{\\mathrm{left}}^{(l)}\\in\\mathbb{R}^{D^{(l)}\\times D^{(l)}}$ and H(l) $\\mathbf{H}_{\\mathrm{right}}^{(l)}\\in$ D(l)\u00d7D(l) as substitutes for the left and right unitary matrices $\\mathbf{W}_{\\mathrm{left}}^{(l)}\\in\\mathbb{R}^{D^{(l)}\\times D^{\\prime}}$ and W(l) $\\mathbf{W}_{\\mathrm{right}}^{(l)}\\in$ D\u2032\u00d7D(l ) within the adaptation matrix W(ald)a ptation to form the HTA adaptation matrix W $\\mathbf{W}_{\\mathrm{HTA}}^{(l)}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{W}_{\\mathrm{HTA}}^{(l)}=\\mathbf{H}_{\\mathrm{left}}^{(l)}\\mathbf{D}_{\\mathrm{H}}^{(l)}\\mathbf{H}_{\\mathrm{right}}^{(l)}}\\\\ &{\\quad\\quad\\quad=(\\mathbf{I}-\\vec{v}_{\\mathrm{left}}^{(l)}\\vec{v}_{\\mathrm{left}}^{(l)\\top})\\mathbf{D}_{\\mathrm{H}}^{(l)}(\\mathbf{I}-\\vec{v}_{\\mathrm{right}}^{(l)}\\vec{v}_{\\mathrm{right}}^{(l)\\top})),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "with two learnable parameter vectors $\\vec{v}_{\\mathrm{left}}^{(l)}\\in\\mathbb{R}^{D^{(l)}}$ and \u20d7v(l) $\\vec{v}_{\\mathrm{right}}^{(l)}\\in\\mathbb{R}^{D^{(l)}}$ and a learnable diagonal parameter vector $\\pmb{\\vec{d}}^{(l)}\\in\\mathbb{R}^{D^{(l)}}$ in the diagonal matrix $\\mathbf{D}_{\\mathrm{H}}^{(l)}\\in\\mathbb{R}^{D^{(l)}\\times D^{(l)}}$ . ", "page_idx": 4}, {"type": "text", "text": "Since the Householder transformation matrix is derived from a single vector, its transformation capacity can be limited and sensitive to the vector learned to derive it. To enhance the robustness of the adaptation matrix, we incorporate an additional low-rank adaptation matrix, resulting in the ultimate HTA. Building on this design, we can derive the LoRA alternative as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{X}_{\\mathrm{FT}}^{(l-1)}=\\mathbf{X}^{(l-1)}(\\mathbf{W}^{(l)}+\\mathbf{W}_{\\mathrm{down}}^{(l)}\\mathbf{W}_{\\mathrm{up}}^{(l)}+\\mathbf{W}_{\\mathrm{HTA}}^{(l)})+\\vec{b}^{(l)\\top}}\\\\ &{\\qquad\\quad=\\mathbf{X}^{(l-1)}(\\mathbf{W}^{(l)}+\\mathbf{W}_{\\mathrm{down}}^{(l)}\\mathbf{W}_{\\mathrm{up}}^{(l)}+(\\mathbf{I}-\\vec{v}_{\\mathrm{left}}^{(l)}\\vec{v}_{\\mathrm{left}}^{(l)\\top})\\mathbf{D}^{(l)}(\\mathbf{I}-\\vec{v}_{\\mathrm{right}}^{(l)}\\vec{v}_{\\mathrm{right}}^{(l)\\top}))+\\vec{b}^{(l)\\top},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathbf{W}_{\\mathrm{down}}^{(l)}\\in\\mathbb{R}^{D^{(l)}\\times1}$ and $\\mathbf{W}_{\\mathrm{down}}^{(l)}\\in\\mathbb{R}^{1\\times D^{(l)}}$ , unless otherwise stated. The HTA structure of the LoRA alternative is shown in Fig. 1 (b). ", "page_idx": 4}, {"type": "text", "text": "Analogously, we can derive HTA alternative to the adapter-based method (as shown in Fig. 1 (d)) as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{X}_{\\mathrm{FT}}^{(l-1)}=\\mathrm{MHA}(\\mathbf{X}^{(l-1)})(\\mathbf{W}_{\\mathrm{down}}^{\\mathrm{MHA}(l)}\\mathbf{W}_{\\mathrm{up}}^{\\mathrm{MHA}(l)}+\\mathbf{W}_{\\mathrm{HTA}}^{\\mathrm{MHA}(l)})}\\\\ &{\\quad\\quad\\quad=\\mathrm{MHA}(\\mathbf{X}^{(l-1)})(\\mathbf{W}_{\\mathrm{down}}^{\\mathrm{MHA}(l)}\\mathbf{W}_{\\mathrm{up}}^{\\mathrm{MHA}(l)}+}\\\\ &{\\quad\\quad\\quad\\quad(\\mathbf{I}-\\vec{v}_{\\mathrm{left}}^{\\mathrm{MHA}(l)}\\vec{v}_{\\mathrm{left}}^{\\mathrm{MHA}(l)^{\\top}})\\mathbf{D}_{\\mathrm{H}}^{\\mathrm{MHA}(l)}(\\mathbf{I}-\\vec{v}_{\\mathrm{right}}^{\\mathrm{MHA}(l)}\\vec{v}_{\\mathrm{right}}^{\\mathrm{MHA}(l)\\top})),}\\\\ &{\\quad\\quad\\quad\\mathbf{X}_{\\mathrm{FT}}^{(l)}=\\mathrm{FFN}(\\mathbf{X}_{\\mathrm{FT}}^{(l-1)})(\\mathbf{W}_{\\mathrm{down}}^{\\mathrm{FFN}(l)}\\mathbf{W}_{\\mathrm{up}}^{\\mathrm{FFN}(l)}+\\mathbf{W}_{\\mathrm{HTA}}^{\\mathrm{FFN}(l)})}\\\\ &{\\quad\\quad\\quad=\\mathrm{FFN}(\\mathbf{X}_{\\mathrm{FT}}^{\\mathrm{FT}(1)})(\\mathbf{W}_{\\mathrm{down}}^{\\mathrm{FFN}(l)}\\mathbf{W}_{\\mathrm{up}}^{\\mathrm{FPN}(l)}+}\\\\ &{\\quad\\quad\\quad\\quad(\\mathbf{I}-\\vec{v}_{\\mathrm{left}}^{\\mathrm{FFN}(l)}\\vec{v}_{\\mathrm{left}}^{\\mathrm{FFN}(l)^{\\top}})\\mathbf{D}_{\\mathrm{H}}^{\\mathrm{FFN}(l)}(\\mathbf{I}-\\vec{v}_{\\mathrm{right}}^{\\mathrm{FFN}(l)}\\vec{v}_{\\mathrm{right}}^{\\mathrm{FFN}(l)\\top})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "By observing Eq. (6), we can see that LoRA-based method with HTA could be re-parameterized to the form $\\mathbf{\\bar{W}}_{\\mathrm{re-param}}=\\mathbf{W}_{\\mathrm{down}}^{(l)}\\mathbf{W}_{\\mathrm{up}}^{(l)}+\\mathbf{W}_{\\mathrm{HTA}}^{(l)}$ W(Hl)TA during the model inference stage. And also, the re-parameterization WrMe\u2212HpAa $\\mathbf{W}_{\\mathrm{re-param}}^{\\mathrm{MHA}}=\\mathbf{W}_{o}^{(l)}\\mathbf{W}_{\\mathrm{HTA}}^{(l)}$ of MHA in Eq. (7) is available due to the fact that itsh $\\mathbf{W}_{\\mathrm{re-param}}^{\\mathrm{FFN}}=\\mathbf{W}_{\\mathrm{FC2}}^{(l)}\\mathbf{W}_{\\mathrm{HTA}}^{(l)}$ $\\mathbf{W}_{o}^{(l)}$ i odnuee dt oa tt hthe e weenidg hotf  mMatHriAx. $\\mathbf{W}_{\\mathrm{FC2}}^{(l)}$ ralty ,t hthe et ariel- opfa rFaFmNe.terization of FFN ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we present the experimental settings, comparison to existing solutions, and ablation studies to unveil the key properties of the proposed method. ", "page_idx": 5}, {"type": "text", "text": "4.1 Experimental settings ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Datasets. We evaluated the effectiveness of our method using two sets of visual adaptation benchmarks: FGVC and VTAB-1k, involving a total of 24 datasets. The FGVC collection consists of five Fine-Grained Visual Classification (FGVC) datasets: CUB-200-2011, NABirds, Oxford Flowers, Stanford Dogs, and Stanford Cars. These datasets focus on distinguishing between visually similar subcategories within a broader category, making the task more challenging and detailed. The VTAB1k benchmark comprises 19 diverse visual classification tasks, divided into three categories: Natural, which includes images captured by standard cameras; Specialized, which includes images captured by specialized equipment such as remote sensing and medical imaging devices; and Structured, which includes synthesized images from simulated environments, such as object counting and 3D depth prediction. Each VTAB-1k task includes 1,000 training samples. ", "page_idx": 5}, {"type": "text", "text": "Pre-trained backbones. We employ ViT [13] and Swin Transformer [14] as backbone architectures to evaluate our proposed approach. To demonstrate the versatility of the proposed HTA model, we utilize two variants of ViT: ViT-Base and ViT-Large. These models are pre-trained on the ImageNet21K dataset [11]. Additionally, to ensure a fair comparison, we follow the settings from previous work [29] and conduct separate experiments using a ViT backbone enhanced with AugReg [34]. ", "page_idx": 5}, {"type": "text", "text": "Baselines and existing PEFT methods. In our comparative analysis, we evaluate the performance of our HTA against two baselines and several state-of-the-art PEFT methods. Unless otherwise specified, our HTA follows the design in Eq. (6), with the dimension of the low-rank adaptation matrix set to 1. The two baselines we consider are: (1) Full Fine-tuning: This baseline involves updating all the parameters of the pre-trained model using the training data of the downstream task. (2) Linear Probing: This baseline focuses on learning a linear classification head on the downstream task while keeping the remaining parameters of the pre-trained model frozen. In addition to the baselines, we compare our method with the following state-of-the-art solutions: Adapter [22], Bias [23], LoRA [1], VPT [24], AdaptFormer [2], FacT [26], ARC [3] and RLRR [29]. The results are presented in Table 1 and Table 2. ", "page_idx": 5}, {"type": "text", "text": "Table 1: Performance comparisons on the VTAB-1k benchmark with ViT-B/16 models pre-trained on ImageNet-21K. \\* denotes leveraging the augmented ViT backbone by AugReg [34]. The bold font shows the best accuracy of all methods and the underline font shows the second best accuracy. ", "page_idx": 6}, {"type": "table", "img_path": "BOtjMacACI/tmp/3109fbb7221694f8e8bf398cc07824e281b1eefc6dbca6f2ea4d2237395c5b54.jpg", "table_caption": [], "table_footnote": ["Table 2: Performance comparisons on five FGVC datasets with ViT-B/16 models pre-trained on ImageNet-21K. \\* denotes leveraging the augmented ViT backbone by AugReg [34]. "], "page_idx": 6}, {"type": "table", "img_path": "BOtjMacACI/tmp/df0696c5050b89bff2757428c983437d3c2835dc6e8d8d5604b3e316386ca4b5.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Implementation details. Following previous work, we employed data augmentation during the training phase. For the FGVC datasets, we processed the images with a random resize crop to $224\\times224$ and applied a random horizontal flip for data augmentation. For the VTAB-1k datasets, we directly resized the images to $224\\times224$ , adhering to the default settings in VTAB-1k. We used the AdamW [35] optimizer to fine-tune the models for 100 epochs. The learning rate schedule was managed using the cosine decay strategy. All experiments are conducted using the PyTorch framework [36] on an NVIDIA A800 GPU with $80\\,\\mathrm{GB}$ of memory. ", "page_idx": 6}, {"type": "text", "text": "4.2 Experimental comparisons ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we conduct a comprehensive comparison of our method with other state-of-the-art approaches using different benchmarks and backbones. We evaluate the classification accuracy of each method across various downstream tasks and examine the number of trainable parameters during the fine-tuning phase. ", "page_idx": 6}, {"type": "text", "text": "Comparison with the existing PEFT methods. We conducted a comparison of our method with other PEFT methods and baselines using two different benchmarks: FGVC and VTAB-1k. The results are presented in Table 1 and Table 2. On the VTAB-1k dataset, our method not only demonstrates strong competitiveness compared to the baselines but also shows advantages over current state-of-theart methods. On many of the datasets, our method achieves the best performance with a reasonable parameter count. Compared to the previous state-of-the-art method, RLRR [29], our method achieves superior overall performance while reducing the number of parameters by one-third. When using the AugReg-enhanced model, our lead is further amplified. In Table 2, we further compare our method with others on the FGVC benchmark. While our method also achieves appealing performance on this dataset, the advantage is less evident. This is due to the fact that very high performance has already been achieved on this dataset, and the performance improvements have nearly saturated. ", "page_idx": 6}, {"type": "table", "img_path": "BOtjMacACI/tmp/aae43b3c259ea1e247cf5527aaf49de314d5ffa90d1280544b9777aa934b64f8.jpg", "table_caption": ["Table 3: Performance comparison on VTAB-1k using ViT-Large pre-trained on ImageNet-21k as the backbone. Detailed results are presented in the Appendix. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "BOtjMacACI/tmp/b78932258c596079c1e3d6643913a0ea8d60820c1b798d9f2301a39f41871650.jpg", "table_caption": ["Table 4: Performance comparison on VTAB-1k using Swin Transformer pre-trained on ImageNet-21k as the backbone. Detailed results are presented in the Appendix. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Experiments on larger-scale ViT backbone. In addition to using the ViT-B backbone, we also employed the ViT-L backbone, which has a deeper block structure, to validate the scalability and generalizability of our method. The comparison results are shown in Table 3. Our method achieves the best performance among all the compared methods while maintaining a reasonable parameter count. These results demonstrate that our method can effectively adapt models of varying scales and complexities in an efficient manner. ", "page_idx": 7}, {"type": "text", "text": "Experiments on hierarchical Vision Transformers. To further validate the effectiveness of our method, we tested it on the Swin Transformer [14]. The Swin Transformer is renowned for its hierarchical design, consisting of multiple stages, each with transformer blocks that maintain consistent feature dimensions unique to that stage. As shown in Table 4, our method notably outperforms existing state-of-the-art methods across various downstream tasks, with an overall improvement of $1.2\\%$ over the previous best performance while using only half of the parameters. ", "page_idx": 7}, {"type": "text", "text": "4.3 Ablation studies ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To gain deeper insights into the proposed method, we conducted comprehensive ablation studies to elucidate its critical features and carry out pertinent analyses. ", "page_idx": 7}, {"type": "text", "text": "Using HTA as alternative to low-rank based adaptation matrix. As mentioned earlier, our proposed HTA model offers a more flexible adaptation capacity compared to other low-rank based adaptation matrices. To validate this claim, we replaced the adaptation matrices of LoRA [1] and Adaptor [22] with HTA. Initially, following FacT [26], LoRA [1] was originally applied to the $\\{\\mathbf{W}_{q},\\mathbf{W}_{v}\\}$ projection matrices in the multi-head attention operation of each ViT layer, while Adapter was applied to the feed-forward neural network components layer-wise, as described in Eq. (2). For a fair comparison, we also applied HTA separately to $\\{\\mathbf{W}_{q},\\mathbf{W}_{v}\\}$ and $\\{\\mathbf{W}_{\\mathrm{FC1}},\\mathbf{W}_{\\mathrm{FC2}}\\}$ . From the results in Table 5, we observe that our method slightly outperforms LoRA but with significantly fewer parameters. Moreover, our method achieves significant improvement over Adapter still using much fewer parameters. These results indicate that our method achieves a better trade-off between adaptation performance and parameter efficiency. To further test the effectiveness of our method when using a similar parameter scale to LoRA, we additionally applied HTA to FFN layers. The results show that under the same parameter size, our method exhibits a noticeable improvement over LoRA. ", "page_idx": 7}, {"type": "table", "img_path": "BOtjMacACI/tmp/b1136df70f985cfcea37f4531c37e5af5430b8532c64b1b3168d4a236648aba8.jpg", "table_caption": ["Table 5: Ablation study on using HTA as alternative to the low-rank based adaptation matrices in LoRA and Adapter on VTAB-1k. Following the configurations in FacT [26], LoRA and Adapter are applied to $\\{\\bar{\\mathbf{W}_{q}^{\\bar{\\phantom{\\dagger}}}},\\bar{\\mathbf{W}_{v}}\\}$ and $\\{\\mathbf{W}_{\\mathrm{FC1}},\\mathbf{W}_{\\mathrm{FC2}}\\}$ projection matrices, separately. "], "table_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "", "img_caption": ["Figure 2: Ablation study on the impact of different bottleneck dimensions of adaptive matrices in HTA. The bar chart represents the Top-1 Test Accuracy, the line graph indicates parameters count. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Ablation study on the bottleneck dimensionality of additive adaptation matrix in HTA. We conducted ablation experiments to verify the effect of incorporating low-rank adaptation matrices in HTA, as well as the impact of its bottleneck dimensionality. The results are presented in Fig. 2. From these results, we observe that without the addition of low-rank adaptation, HTA experiences an obvious performance drop. This is due to the fact that while deriving orthogonal matrices using Householder transformations is parameter-efficient, their inherent dependence on a single chosen vector makes them insufficient as a set of general orthogonal bases for representing arbitrary highdimensional space. When using a low-rank adaptation matrix with rank 1, HTA shows a significant performance boost. This indicates that even with a simple low-rank adaptation, HTA can achieve a promising trade-off between adaptation performance and parameter efficiency. By incorporating these low-rank matrices, HTA can maintain high performance while being parameter-efficient. ", "page_idx": 8}, {"type": "text", "text": "5 Limitations ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this work, we use Householder transformations to construct adaptation matrices in a parameterefficient manner. Although Householder transformation matrices are orthogonal, they cannot serve as general orthogonal bases in high-dimensional spaces due to their inherent dependence on a single vector. This limitation may reduce the adaptation capacity of the adaptation matrix composed of two Householder matrices. We address this issue by incorporating a rank-1 adaptation matrix, which may somewhat detract from the elegance of the proposed method. However, it is worth exploring strategies to eliminate the need for the additive adaptation matrix, thereby further enhancing the elegance and efficiency of the HTA method. ", "page_idx": 8}, {"type": "text", "text": "6 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we proposed a novel Parameter-Efficient Fine-Tuning (PEFT) solution. Our method addresses the limitation of fixed bottleneck dimensionality in low-rank based adaptation matrices, which can restrict adaptation flexibility. By viewing the adaptation matrix from the perspective of Singular Value Decomposition (SVD), we use Householder transformations to mimic orthogonal bases. These transformations, derived from a single vector, are highly parameter-efficient. We adaptively learn diagonal coefficients to flexibly combine two Householder matrices into an adaptation matrix, accommodating layer-wise variations. Theoretically, our method can generate adaptation matrices with varying ranks while maintaining a reasonable parameter size, offering a potential alternative to low-rank based adaptations. Experiments on two sets of downstream vision classification tasks demonstrate the effectiveness of our approach. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "W. Dong\u2019s participation was in part supported by the Natural Science Basic Research Program of Shaanxi (Program No.2024JC-YBMS-464). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] E. J. Hu, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, W. Chen et al., \u201cLora: Low-rank adaptation of large language models,\u201d in International Conference on Learning Representations, 2021.   \n[2] S. Chen, C. Ge, Z. Tong, J. Wang, Y. Song, J. Wang, and P. Luo, \u201cAdaptformer: Adapting vision transformers for scalable visual recognition,\u201d Advances in Neural Information Processing Systems, vol. 35, pp. 16 664\u201316 678, 2022.   \n[3] W. Dong, D. Yan, Z. Lin, and P. Wang, \u201cEfficient adaptation of large vision transformer via adapter re-composing,\u201d in Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[4] F. Zhuang, Z. Qi, K. Duan, D. Xi, Y. Zhu, H. Zhu, H. Xiong, and Q. He, \u201cA comprehensive survey on transfer learning,\u201d Proceedings of the IEEE, vol. 109, no. 1, pp. 43\u201376, 2020.   \n[5] S. J. Pan and Q. Yang, \u201cA survey on transfer learning,\u201d IEEE Transactions on knowledge and data engineering, vol. 22, no. 10, pp. 1345\u20131359, 2009.   \n[6] M. Iman, H. R. Arabnia, and K. Rasheed, \u201cA review of deep transfer learning and recent advancements,\u201d Technologies, vol. 11, no. 2, p. 40, 2023.   \n[7] W. Ying, Y. Zhang, J. Huang, and Q. Yang, \u201cTransfer learning via learning to transfer,\u201d in International Conference on Machine Learning. PMLR, 2018, pp. 5085\u20135094.   \n[8] Q. Yan, D. Gong, and Y. Zhang, \u201cTwo-stream convolutional networks for blind image quality assessment,\u201d IEEE Transactions on Image Processing, vol. 28, no. 5, pp. 2200\u20132211, 2018.   \n[9] S. Su, Q. Yan, Y. Zhu, C. Zhang, X. Ge, J. Sun, and Y. Zhang, \u201cBlindly assess image quality in the wild guided by a self-adaptive hyper network,\u201d in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2020.   \n[10] Y. Zhang, H. Zhang, N. M. Nasrabadi, and T. S. Huang, \u201cMulti-metric learning for multi-sensor fusion based classification,\u201d Information Fusion, vol. 14, no. 4, pp. 431\u2013440, 2013.   \n[11] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, \u201cImagenet: A large-scale hierarchical image database,\u201d in 2009 IEEE conference on computer vision and pattern recognition, 2009.   \n[12] M. Tan and Q. Le, \u201cEfficientnetv2: Smaller models and faster training,\u201d in International conference on machine learning, 2021.   \n[13] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al., \u201cAn image is worth 16x16 words: Transformers for image recognition at scale,\u201d in International Conference on Learning Representations, 2020.   \n[14] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo, \u201cSwin transformer: Hierarchical vision transformer using shifted windows,\u201d in Proceedings of the IEEE/CVF international conference on computer vision, 2021.   \n[15] S. Yun, S. J. Oh, B. Heo, D. Han, J. Choe, and S. Chun, \u201cRe-labeling imagenet: from single to multi-labels, from global to localized labels,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021.   \n[16] C.-Y. Wang, A. Bochkovskiy, and H.-Y. M. Liao, \u201cScaled-yolov4: Scaling cross stage partial network,\u201d in Proceedings of the IEEE/cvf conference on computer vision and pattern recognition, 2021.   \n[17] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko, \u201cEnd-to-end object detection with transformers,\u201d in European conference on computer vision, 2020.   \n[18] B. Cheng, A. Schwing, and A. Kirillov, \u201cPer-pixel classification is not all you need for semantic segmentation,\u201d Advances in neural information processing systems, 2021.   \n[19] A. Kirillov, Y. Wu, K. He, and R. Girshick, \u201cPointrend: Image segmentation as rendering,\u201d in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2020.   \n[20] K. He, X. Chen, S. Xie, Y. Li, P. Doll\u00e1r, and R. Girshick, \u201cMasked autoencoders are scalable vision learners,\u201d in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022.   \n[21] X. Chen, S. Xie, and K. He, \u201cAn empirical study of training self-supervised vision transformers,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021.   \n[22] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe, A. Gesmundo, M. Attariyan, and S. Gelly, \u201cParameter-efficient transfer learning for nlp,\u201d in International Conference on Machine Learning, 2019.   \n[23] E. B. Zaken, Y. Goldberg, and S. Ravfogel, \u201cBitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models,\u201d in Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), 2022, pp. 1\u20139.   \n[24] M. Jia, L. Tang, B.-C. Chen, C. Cardie, S. Belongie, B. Hariharan, and S.-N. Lim, \u201cVisual prompt tuning,\u201d in European Conference on Computer Vision, 2022.   \n[25] D. Lian, D. Zhou, J. Feng, and X. Wang, \u201cScaling & shifting your features: A new baseline for efficient model tuning,\u201d Advances in Neural Information Processing Systems, 2022.   \n[26] S. Jie and Z.-H. Deng, \u201cFact: Factor-tuning for lightweight adaptation on vision transformer,\u201d in Proceedings of the AAAI Conference on Artificial Intelligence, 2023.   \n[27] Q. Zhang, M. Chen, A. Bukharin, P. He, Y. Cheng, W. Chen, and T. Zhao, \u201cAdaptive budget allocation for parameter-efficient fine-tuning,\u201d in The Eleventh International Conference on Learning Representations, 2023.   \n[28] N. Hyeon-Woo, M. Ye-Bin, and T.-H. Oh, \u201cFedpara: Low-rank hadamard product for communication-efficient federated learning,\u201d arXiv preprint arXiv:2108.06098, 2021.   \n[29] W. Dong, X. Zhang, B. Chen, D. Yan, Z. Lin, Q. Yan, P. Wang, and Y. Yang, \u201cLow-rank rescaled vision transformer fine-tuning: A residual design approach,\u201d Proceedings of the IEEE/CVF International Conference on Computer Vision, 2024.   \n[30] A. S. Householder, \u201cUnitary triangularization of a nonsymmetric matrix,\u201d Journal of the ACM (JACM), vol. 5, no. 4, pp. 339\u2013342, 1958.   \n[31] L. N. Trefethen and D. Bau, Numerical linear algebra. SIAM, 2022.   \n[32] J. G. Francis, \u201cThe qr transformation a unitary analogue to the lr transformation\u2014part 1,\u201d The Computer Journal, vol. 4, no. 3, pp. 265\u2013271, 1961.   \n[33] R. A. Horn and C. R. Johnson, Matrix analysis. Cambridge university press, 2012.   \n[34] A. Steiner, A. Kolesnikov, X. Zhai, R. Wightman, J. Uszkoreit, and L. Beyer, \u201cHow to train your vit? data, augmentation, and regularization in vision transformers,\u201d arXiv preprint arXiv:2106.10270, 2021.   \n[35] I. Loshchilov and F. Hutter, \u201cDecoupled weight decay regularization,\u201d in International Conference on Learning Representations, 2018.   \n[36] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al., \u201cPytorch: An imperative style, high-performance deep learning library,\u201d Advances in neural information processing systems, 2019.   \n[37] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie, \u201cThe caltech-ucsd birds-200-2011 dataset,\u201d 2011.   \n[38] G. Van Horn, S. Branson, R. Farrell, S. Haber, J. Barry, P. Ipeirotis, P. Perona, and S. Belongie, \u201cBuilding a bird recognition app and large scale dataset with citizen scientists: The fine print in fine-grained dataset collection,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition, 2015.   \n[39] M.-E. Nilsback and A. Zisserman, \u201cAutomated flower classification over a large number of classes,\u201d in 2008 Sixth Indian conference on computer vision, graphics & image processing, 2008.   \n[40] A. Khosla, N. Jayadevaprakash, B. Yao, and F.-F. Li, \u201cNovel dataset for fine-grained image categorization: Stanford dogs,\u201d in Proc. CVPR workshop on fine-grained visual categorization (FGVC), vol. 2, no. 1, 2011.   \n[41] T. Gebru, J. Krause, Y. Wang, D. Chen, J. Deng, and L. Fei-Fei, \u201cFine-grained car detection for visual census estimation,\u201d in Proceedings of the AAAI Conference on Artificial Intelligence, 2017.   \n[42] X. Zhai, J. Puigcerver, A. Kolesnikov, P. Ruyssen, C. Riquelme, M. Lucic, J. Djolonga, A. S. Pinto, M. Neumann, A. Dosovitskiy et al., \u201cA large-scale study of representation learning with the visual task adaptation benchmark,\u201d arXiv preprint arXiv:1910.04867, 2019. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Efficient Adaptation of Pre-trained Vision Transformer via Householder Transformation ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Supplementary Materials ", "page_idx": 12}, {"type": "text", "text": "In the supplementary materials involving our work, we demonstrate Detailed dataset statistic, Hyperparameters in our work, Experimental details, and broader impacts, including: ", "page_idx": 12}, {"type": "text", "text": "\u2022 A Detailed dataset statistic ", "page_idx": 12}, {"type": "text", "text": "\u2022 B Hyper-parameters in our work   \n\u2022 C Experimental details on larger-scale and hierarchical ViT backbones   \n\u2022 D Experimental details on ablation study ", "page_idx": 12}, {"type": "text", "text": "\u2022 E Broader impacts ", "page_idx": 12}, {"type": "text", "text": "Due to the limitation that supplementary materials larger than 100MB cannot be uploaded to the OpenReview website, only the project code as the concise supplementary materials is uploaded to this website. Please reler to the anonymous linkhttps://drive.google.com/file/d/18sXhtqMlKZd4_ LRICk2NvSlKiFiHrG2d/view?to obtain the complete code, datasets, and models. ", "page_idx": 12}, {"type": "text", "text": "A Detailed dataset statistic ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We provide detailed information about the datasets used in this paper, including the number of classes and the sizes of the training, validation, and test sets, in Table 1 (FGVC) and Table 2 (VTAB-1k).The FGVC datasets include CUB-200-2011, NABirds, Oxford Flowers, Stanford Dogs, and Stanford Cars, which are used for fine-grained classification tasks of birds, flowers, dogs, and cars, respectively. The VTAB-1k datasets cover natural, specialized, and structured tasks, including natural image datasets such as CIFAR-100, Caltech101, DTD, Flowers102, Pets, SVHN, and Sun397; specialized image datasets such as Patch Camelyon, EuroSAT, Resisc45, and Retinopathy; and structured image datasets such as Clevr/count, Clevr/distance, DMLab, KITTI/distance, dSprites/location, dSprites/orientation, SmallNORB/azimuth, and SmallNORB/elevation. Detailed information about these datasets is presented in the tables. ", "page_idx": 13}, {"type": "table", "img_path": "BOtjMacACI/tmp/91f6d6d691dbdbf7a18d9a56fe928d8852d43cfb40ab3b90845c7b00c8fc3cd6.jpg", "table_caption": ["Table 1: Dataset statistics for FGVC. \u201c\\*\u201d denotes the train/val split of datasets following the dataset setting in VPT [24]. "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "B Hyper-parameters in our work ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Table 3 provides a summary of the configurations used in our experiments. As discussed in Section 4, we performed a grid search on the validation set of each task to determine the optimal hyperparameters, including learning rate, weight decay, batch size, and dropout rate. ", "page_idx": 13}, {"type": "text", "text": "C Experimental details on larger-scale and hierarchical ViT backbones ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Table 4 and 5 respectively display the comprehensive results of the comparison conducted in Section 4 among ViT-Large and Swin-Base models. ", "page_idx": 13}, {"type": "text", "text": "D Experimental details on ablation study ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We provide further explanation of the ablation experiments in Section 4. In the study on the transferability of HTA, we replaced the low-rank adaptation matrices in LoRA, we used an HTA module to replace the bottleneck part of LoRA, while in Adapter, we directly replaced the Adapter with an HTA module. The detailed experimental results are presented in the table 6. In the study of the low-rank adaptation part of HTA, we set its dimensions to 0, 1, 2, and 4, respectively. The results are shown in Table 7. ", "page_idx": 13}, {"type": "text", "text": "E Broader impacts ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Practicality: Our approach differs from traditional methods by employing Householder transformations rather than standard unitary matrices, which can be efficiently derived. This approach boosts the efficiency of parameter usage and significantly cuts down on the number of parameters requiring fine-tuning. With this technique, we manage to achieve high performance while optimizing parameter use. Leveraging large-scale pre-trained models, our HTA method proves to be both highly efficient and practical across diverse applications. ", "page_idx": 13}, {"type": "text", "text": "Low Energy Consumption: Our approach enhances the model\u2019s computational efficiency by decreasing the necessary computational parameters, thus reducing energy usage during training. This reduction aids in conserving energy and lowering emissions, aligning with global sustainability goals and the push for eco-friendly practices. Moreover, our method not only improves the model\u2019s ", "page_idx": 13}, {"type": "table", "img_path": "BOtjMacACI/tmp/fa3755c79bd6a76de14e890c070dba80ddb704975d9b7ccde86a74be9f7b58a2.jpg", "table_caption": ["Table 2: Dataset statistics for VTAB-1k [42]. "], "table_footnote": [], "page_idx": 14}, {"type": "table", "img_path": "BOtjMacACI/tmp/abf8ca69b661138590e65daa90a7fa01c9606acb0937d7afa9990a1ad9eea651.jpg", "table_caption": ["Table 3: The implementation details of configurations such as optimizer and hyper-parameters. We select the best hyper-parameters for each download task via using grid search. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "Table 4: This table is extended from Table 3 in Section 4 and describes the detailed experimental results of the performance comparison on VTAB-1k using ViT-Large pre-trained on ImageNet-21k as the backbone. ", "page_idx": 14}, {"type": "image", "img_path": "BOtjMacACI/tmp/37c5bda6e0cf7a76f3c55ca89c3d5f6aa0b15a7a777b64b8a7b8dfab5c298b94.jpg", "img_caption": [], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "performance and efficiency but also promotes environmental sustainability by embracing sustainable development principles. ", "page_idx": 14}, {"type": "text", "text": "Ethical Aspects: Our model utilizes the vast capabilities of large-scale pre-trained models for representation and generalization. However, it is trained on datasets that might contain problematic data, such as illegal content or inherent biases, which our model could inadvertently learn. To tackle this challenge, addressing model toxicity becomes critical. Consequently, it\u2019s imperative to develop ", "page_idx": 14}, {"type": "text", "text": "Table 5: This table is extended from Table 4 in Section 4 and describes the detailed experimental results of the performance comparison on VTAB-1k using Swin-Base pre-trained on ImageNet-21k as the backbone. ", "page_idx": 15}, {"type": "table", "img_path": "BOtjMacACI/tmp/3f6f1ecb586fdbb6b0ac7c499a2a2247a521f6c7320eb8b55faf24224dbab85a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Table 6: This table is extended from Table 5 in Section 4. LoRA and Adapter both follow the configurations from the A paper. In our implementation, the fully connected layers within the bottlenecks are replaced with Householder transformations. \"(\u00b7)\" indicates specific configuration information ", "page_idx": 15}, {"type": "table", "img_path": "BOtjMacACI/tmp/875c86960256d86212988ad7970759106cf88fd90f7b8db9f1a36eb3a2350d7f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "BOtjMacACI/tmp/0b3dde947ea78572ddf2e4e1b894ecb5733184e1fcaf26e5980e7773ce5d28a3.jpg", "table_caption": ["Table 7: This table is extended from Fig. 2 in Section 4. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "enhanced mechanisms that can both identify and reduce such biases and unlawful information in the datasets. ", "page_idx": 15}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: Please see Abstract and Introduction Section 1. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 16}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Justification: Please see Section 5. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 16}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 16}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 16}, {"type": "text", "text": "Justification: Our work does not involve specific theoretical results. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 17}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: We provide supplementary materials involving statistical datasets, detailed hyper-parameters of model settings. And also we provide complete code, datasets and models at the anonymous link https://drive.google.com/file/d/18sXhtqMlKZd4_ LRICk2NvSlKiFiHrG2d/view?. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 17}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: We provide supplementary materials involving statistical datasets, detailed hyper-parameters of model settings. And also we provide complete code, datasets and models at the anonymous link https://drive.google.com/file/d/18sXhtqMlKZd4_ LRICk2NvSlKiFiHrG2d/view?. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 18}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: Please see our supplementary materials. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 18}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "Justification: Our work does not involve specific error bars. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 18}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 19}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: Please see Section 4. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 19}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: Our paper conforms the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 19}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: Please see Appendix Section E. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 19}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 20}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: This paper poses no such risks. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 20}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: The license and terms of use explicitly are mentioned and properly respected. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: This paper does not release new assets. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 21}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 21}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 21}]