[{"figure_path": "hGhLd2ByoR/figures/figures_1_1.jpg", "caption": "Figure 1: Prominent features distract unsupervised latent knowledge detectors (see Section 4.2). Left: We apply two transformations to a dataset of movie reviews, {qi}. First (novel to us) we insert a distracting feature by appending either \u201cAlice thinks it's positive\u201d or \u201cAlice thinks it's negative", "description": "This figure demonstrates how prominent features can distract unsupervised latent knowledge detectors.  Two transformations were applied to a movie review dataset: 1) A distracting feature was added (e.g., \"Alice thinks it's positive\"), and 2) contrast pairs were created [9]. The LLM activations for these transformed strings were then visualized using PCA.  Without the distracting feature, the classifier correctly identifies the review sentiment. However, with the distracting feature, the classifier focuses on Alice's opinion, ignoring the actual review sentiment. This highlights a key limitation of existing unsupervised methods; they may not detect latent knowledge but rather the most prominent features.", "section": "4 Experiments"}, {"figure_path": "hGhLd2ByoR/figures/figures_4_1.jpg", "caption": "Figure 2: Discovering random words. Chinchilla, IMDb. (a) The methods distinguish whether the prompts end with banana/shed rather than the review sentiment. (b) PCA visualisation of top-3 activation dimensions, in default (left) and modified (right) settings, shows the clustering into banana/shed (light/dark) rather than review sentiment (blue/orange).", "description": "This figure shows how unsupervised methods fail to discover actual knowledge when a distracting feature is introduced.  The experiment uses movie reviews and adds either \"Banana\" or \"Shed\" randomly to the end of each review.  (a) shows the accuracy of different methods.  Unsupervised methods perform poorly on predicting the actual review sentiment but perform well on identifying whether \"Banana\" or \"Shed\" was present. (b) displays the PCA visualization of the LLM activations, demonstrating the clustering based on the distracting words instead of the sentiment.", "section": "4.1 Discovering random words"}, {"figure_path": "hGhLd2ByoR/figures/figures_5_1.jpg", "caption": "Figure 3: Discovering an explicit opinion. Chinchilla 70B. (a) When Alice's opinion is given (red) unsupervised methods predict her opinion (light red) but not true review sentiment (dark red), suggesting the linear probe detects whether the claim agrees with Alice. Blue shows the default prompt without Alice. (b) PCA visualisation of top-3 activation dims., in default (L) and modified (R) settings, where clustering discover Alice's opinion (light/dark), over review sentiment (blue/orange).", "description": "This figure shows the results of an experiment where an explicit opinion is added to prompts. The left panel (a) shows the accuracy of different unsupervised methods in predicting either the true review sentiment or Alice's opinion. Unsupervised methods are much more accurate when predicting Alice's opinion than the actual review sentiment. The right panel (b) displays a PCA visualization which shows that the top three components are clustered based on Alice's opinion and that the review sentiment is secondary.", "section": "4.2 Discovering an explicit opinion"}, {"figure_path": "hGhLd2ByoR/figures/figures_5_2.jpg", "caption": "Figure 1: Prominent features distract unsupervised latent knowledge detectors (see Section 4.2). Left: We apply two transformations to a dataset of movie reviews, {qi}. First (novel to us) we insert a distracting feature by appending either \u201cAlice thinks it's positive\u201d or \u201cAlice thinks it's negative", "description": "This figure demonstrates how prominent features can distract unsupervised latent knowledge detectors.  Two transformations are applied to a dataset of movie reviews: 1) A distracting feature is added by appending either \u201cAlice thinks it\u2019s positive\u201d or \u201cAlice thinks it\u2019s negative\u201d at random. 2) Contrast pairs are created by appending \u201cIt is positive\u201d or \u201cIt is negative\u201d. The LLM activations for these strings are visualized.  A PCA visualization shows that without the distracting feature, the classifier identifies review sentiment. However, with the added feature, the classifier focuses on Alice's opinion instead, ignoring the review sentiment. This highlights how existing unsupervised methods can fail to discover actual knowledge due to focusing on prominent, but irrelevant features.", "section": "4 Experiments"}, {"figure_path": "hGhLd2ByoR/figures/figures_6_1.jpg", "caption": "Figure 2: Discovering random words. Chinchilla, IMDb. (a) The methods distinguish whether the prompts end with banana/shed rather than the review sentiment. (b) PCA visualisation of top-3 activation dimensions, in default (left) and modified (right) settings, shows the clustering into banana/shed (light/dark) rather than review sentiment (blue/orange).", "description": "This figure demonstrates how unsupervised methods fail to discover actual knowledge from LLM activations.  In a controlled experiment using movie reviews, the researchers added a distracting feature\u2014randomly appending either \"Banana\" or \"Shed\" to each review.  Panel (a) shows that the accuracy of several unsupervised knowledge discovery methods in predicting the actual sentiment of the reviews drops to chance levels when these distracting words are added.  Panel (b) shows PCA visualizations of LLM activations.  In the default condition (without the distracting words), the activations cluster mainly by review sentiment.  However, when the distracting words are added, the activations cluster primarily based on the presence of \"Banana\" or \"Shed\", demonstrating that the unsupervised methods prioritize these easily detectable features over the actual knowledge (sentiment) present in the LLM activations.", "section": "4.1 Discovering random words"}, {"figure_path": "hGhLd2ByoR/figures/figures_6_2.jpg", "caption": "Figure 2: Discovering random words. Chinchilla, IMDb. (a) The methods distinguish whether the prompts end with banana/shed rather than the review sentiment. (b) PCA visualisation of top-3 activation dimensions, in default (left) and modified (right) settings, shows the clustering into banana/shed (light/dark) rather than review sentiment (blue/orange).", "description": "This figure demonstrates how unsupervised methods fail to identify actual knowledge and instead latch onto arbitrary, distracting features.  The experiment uses IMDb movie reviews and appends either \"Banana\" or \"Shed\" randomly to the end of each review.  The left panel (a) shows that standard methods (CCS, PCA, K-means) perform at chance (50% accuracy) on predicting the actual sentiment of the review but perform highly on predicting which word (\"Banana\" or \"Shed\") was appended. The right panel (b) presents PCA visualizations of the LLM activations. The default setting (left) shows clustering according to review sentiment (blue/orange).  In contrast, the modified setting (right) shows clear clustering according to the presence of \"Banana\" or \"Shed\" (light/dark) regardless of sentiment.  This demonstrates that the methods prioritize prominent features over actual knowledge.", "section": "4.1 Discovering random words"}, {"figure_path": "hGhLd2ByoR/figures/figures_7_1.jpg", "caption": "Figure 1: Prominent features distract unsupervised latent knowledge detectors (see Section 4.2). Left: We apply two transformations to a dataset of movie reviews, {qi}. First (novel to us) we insert a distracting feature by appending either \u201cAlice thinks it's positive\u201d or \u201cAlice thinks it's negative", "description": "This figure demonstrates how easily unsupervised methods for knowledge discovery in LLMs can be distracted from the actual knowledge by prominent features. It uses a dataset of movie reviews and applies two transformations: (1) adding a distracting feature stating Alice's random opinion on each review, and (2) creating contrast pairs by appending 'It is positive' or 'It is negative'. The LLM activations are visualized in a PCA plot. Without the distracting feature, the classifier identifies the actual review sentiment. With the distracting feature, however, the classifier identifies Alice's opinion instead, ignoring the review sentiment, which highlights that existing methods are insufficient for discovering latent knowledge and sensitive to the choice of prompt.", "section": "4 Experiments"}, {"figure_path": "hGhLd2ByoR/figures/figures_28_1.jpg", "caption": "Figure 1: Prominent features distract unsupervised latent knowledge detectors (see Section 4.2). Left: We apply two transformations to a dataset of movie reviews, {qi}. First (novel to us) we insert a distracting feature by appending either \u201cAlice thinks it's positive\u201d or \u201cAlice thinks it's negative", "description": "This figure visualizes how prominent features can distract unsupervised latent knowledge detectors.  The experiment involves movie review datasets. Two transformations were applied: (1) adding a distracting 'Alice's opinion' feature and (2) creating contrast pairs as in CCS method. PCA visualization of LLM activations shows that the classifier, trained using unsupervised methods, focuses on 'Alice's opinion' rather than the actual review sentiment when the distracting feature is present, demonstrating the vulnerability of existing unsupervised methods to prominent features.", "section": "4 Experiments"}, {"figure_path": "hGhLd2ByoR/figures/figures_29_1.jpg", "caption": "Figure 1: Prominent features distract unsupervised latent knowledge detectors (see Section 4.2). Left: We apply two transformations to a dataset of movie reviews, {qi}. First (novel to us) we insert a distracting feature by appending either \u201cAlice thinks it's positive\u201d or \u201cAlice thinks it's negative", "description": "This figure demonstrates how prominent, yet irrelevant, features can overshadow actual knowledge detection in unsupervised latent knowledge discovery methods.  It presents a scenario where movie review sentiment is augmented with a randomly assigned 'Alice's opinion'.  The LLM activations are then analyzed using PCA, showing that the classifiers inadvertently prioritize Alice's opinion (light/dark) instead of the actual review sentiment (blue/orange), regardless of whether \"Alice's opinion\" is included or not.  This highlights the susceptibility of these unsupervised methods to focusing on prominent, easily detectable features rather than the latent knowledge they intend to reveal.", "section": "4 Experiments"}, {"figure_path": "hGhLd2ByoR/figures/figures_30_1.jpg", "caption": "Figure 2: Discovering random words. Chinchilla, IMDb. (a) The methods distinguish whether the prompts end with banana/shed rather than the review sentiment. (b) PCA visualisation of top-3 activation dimensions, in default (left) and modified (right) settings, shows the clustering into banana/shed (light/dark) rather than review sentiment (blue/orange).", "description": "This figure demonstrates how unsupervised methods for latent knowledge discovery are easily distracted by superficial features, rather than focusing on the intended knowledge.  In a movie review sentiment classification task, adding random words ('Banana' or 'Shed') to the end of prompts significantly changes the classifier's output.  Part (a) shows that the accuracy of distinguishing review sentiment drastically drops when the random words are added, while part (b) using PCA visualization highlights that the classifier is now focusing on the presence of these random words rather than the actual sentiment. This proves that the methods are picking up prominent features rather than knowledge.", "section": "4.1 Discovering random words"}, {"figure_path": "hGhLd2ByoR/figures/figures_31_1.jpg", "caption": "Figure 1: Prominent features distract unsupervised latent knowledge detectors (see Section 4.2). Left: We apply two transformations to a dataset of movie reviews, {qi}. First (novel to us) we insert a distracting feature by appending either \u201cAlice thinks it's positive\u201d or \u201cAlice thinks it's negative", "description": "The figure demonstrates how prominent features can mislead unsupervised latent knowledge detection methods.  The left panel shows two transformations applied to a movie review dataset: inserting a distracting \"Alice\" feature and creating contrast pairs. The middle panel displays the LLM activations for these modified strings. The right panel shows a PCA visualization.  The visualization highlights that when the \"Alice\" feature is added, the classifier focuses on Alice's opinion, ignoring the actual review sentiment, demonstrating the problem of prominent features overshadowing true knowledge in unsupervised methods.", "section": "4 Experiments"}, {"figure_path": "hGhLd2ByoR/figures/figures_32_1.jpg", "caption": "Figure 1: Prominent features distract unsupervised latent knowledge detectors (see Section 4.2). Left: We apply two transformations to a dataset of movie reviews, {qi}. First (novel to us) we insert a distracting feature by appending either \u201cAlice thinks it's positive\u201d or \u201cAlice thinks it's negative", "description": "This figure demonstrates how prominent features can distract unsupervised methods from discovering latent knowledge. It uses movie reviews as an example where the sentiment is modified. A distracting feature is introduced by appending either \"Alice thinks it's positive\" or \"Alice thinks it's negative\" to each review. The figure shows that when this distractor is added, unsupervised learning methods such as contrast-consistent search fail to detect the actual review sentiment and instead focus on Alice's opinion.  This highlights the challenges of distinguishing actual knowledge from other prominent features in LLM activations, especially when using unsupervised methods.", "section": "4 Experiments"}, {"figure_path": "hGhLd2ByoR/figures/figures_33_1.jpg", "caption": "Figure 1: Prominent features distract unsupervised latent knowledge detectors (see Section 4.2). Left: We apply two transformations to a dataset of movie reviews, {qi}. First (novel to us) we insert a distracting feature by appending either \u201cAlice thinks it's positive\u201d or \u201cAlice thinks it's negative", "description": "This figure demonstrates how prominent features can distract unsupervised latent knowledge detectors.  It showcases the results of applying two transformations to a dataset of movie reviews. The first is adding a distracting feature (\u201cAlice thinks it\u2019s positive/negative\u201d) which acts as a confounding variable. The second is creating contrast pairs as used in the CCS method.  The resulting LLM activations are visualized using PCA. The plot shows how, without the added distracting feature, a classifier successfully predicts the review sentiment. However, with the distracting feature, the classifier instead focuses on the added feature, ignoring the actual review sentiment.", "section": "4 Experiments"}, {"figure_path": "hGhLd2ByoR/figures/figures_34_1.jpg", "caption": "Figure 2: Discovering random words. Chinchilla, IMDb. (a) The methods distinguish whether the prompts end with banana/shed rather than the review sentiment. (b) PCA visualisation of top-3 activation dimensions, in default (left) and modified (right) settings, shows the clustering into banana/shed (light/dark) rather than review sentiment (blue/orange).", "description": "This figure demonstrates how unsupervised methods fail to identify actual knowledge in LLMs by focusing on prominent, but irrelevant features.  In the experiment, the model is presented with movie review text appended by either \"Banana\" or \"Shed\" (the distracting feature).  Subfigure (a) shows that instead of correctly classifying the sentiment of the review, the unsupervised methods (CCS, PCA, K-means) focus on predicting whether \"Banana\" or \"Shed\" was appended, achieving only random accuracy on the actual sentiment. Subfigure (b) presents the PCA visualization of the LLM's top 3 activation dimensions. This visualizes how the classifiers created by these methods fail to identify true sentiment (blue/orange) and instead identify the presence of \"Banana/Shed\" (light/dark). This experiment showcases that unsupervised methods tend to prioritize the most prominent feature in the data over the intended knowledge.", "section": "4 Experiments"}, {"figure_path": "hGhLd2ByoR/figures/figures_35_1.jpg", "caption": "Figure 1: Prominent features distract unsupervised latent knowledge detectors (see Section 4.2). Left: We apply two transformations to a dataset of movie reviews, {qi}. First (novel to us) we insert a distracting feature by appending either \u201cAlice thinks it's positive\u201d or \u201cAlice thinks it's negative", "description": "This figure demonstrates how prominent features can mislead unsupervised latent knowledge detection methods. The experiment involves movie review data with added distracting features representing Alice's opinions. The LLM activations are visualized, showing that classifiers trained on this data prioritize Alice's opinion over the actual review sentiment. This highlights a significant issue with existing unsupervised methods, as they may detect prominent features rather than actual knowledge.", "section": "4 Experiments"}, {"figure_path": "hGhLd2ByoR/figures/figures_36_1.jpg", "caption": "Figure 1: Prominent features distract unsupervised latent knowledge detectors (see Section 4.2). Left: We apply two transformations to a dataset of movie reviews, {qi}. First (novel to us) we insert a distracting feature by appending either \u201cAlice thinks it's positive\u201d or \u201cAlice thinks it's negative", "description": "This figure shows how easily unsupervised methods for detecting knowledge in LLMs are fooled by salient, but non-knowledge related features.  The left panel displays two transformations applied to movie review data:  adding a random \"Alice thinks it's positive/negative\" statement, and creating contrast pairs. The center panel shows LLM activations for these transformed inputs. The right panel visualizes the LLM activations using PCA, showing that the classifier trained using the unsupervised method focuses on Alice's opinion (light/dark coloring) and ignores the actual movie review sentiment (blue/orange coloring). This demonstrates a failure mode where unsupervised methods detect prominent features that are not knowledge. ", "section": "4 Experiments"}, {"figure_path": "hGhLd2ByoR/figures/figures_37_1.jpg", "caption": "Figure 2: Discovering random words. Chinchilla, IMDb. (a) The methods distinguish whether the prompts end with banana/shed rather than the review sentiment. (b) PCA visualisation of top-3 activation dimensions, in default (left) and modified (right) settings, shows the clustering into banana/shed (light/dark) rather than review sentiment (blue/orange).", "description": "This figure demonstrates how unsupervised methods for knowledge discovery in LLMs focus on easily detectable features rather than actual knowledge.  In the experiment, two random words ('Banana' and 'Shed') were added to movie review prompts. The left panel (a) shows the accuracy of different unsupervised methods in predicting the review's sentiment versus predicting which random word was used. Unsupervised methods fail to predict the sentiment but succeed in predicting the random word.  The right panel (b) shows a principal component analysis (PCA) plot of the LLM activations.  With the default prompt, the activations cluster by review sentiment, but after adding the random word, the activations cluster according to the random word used, ignoring the review sentiment. This demonstrates how prominent features are prioritized over the intended knowledge during unsupervised learning.", "section": "4.1 Discovering random words"}, {"figure_path": "hGhLd2ByoR/figures/figures_38_1.jpg", "caption": "Figure 1: Prominent features distract unsupervised latent knowledge detectors (see Section 4.2). Left: We apply two transformations to a dataset of movie reviews, {qi}. First (novel to us) we insert a distracting feature by appending either \u201cAlice thinks it's positive\u201d or \u201cAlice thinks it's negative", "description": "This figure shows how easily unsupervised methods can be fooled into detecting prominent features other than knowledge.  The left panel shows two transformations applied to movie review data: adding a random statement about Alice's opinion, and creating contrast pairs. The middle panel shows the LLM activations resulting from these transformations. The right panel shows a PCA visualization demonstrating how the classifier, instead of focusing on the review sentiment, focuses on Alice's opinion, even when Alice's opinion is irrelevant to the actual review sentiment.", "section": "4 Experiments"}, {"figure_path": "hGhLd2ByoR/figures/figures_39_1.jpg", "caption": "Figure 2: Discovering random words. Chinchilla, IMDb. (a) The methods distinguish whether the prompts end with banana/shed rather than the review sentiment. (b) PCA visualisation of top-3 activation dimensions, in default (left) and modified (right) settings, shows the clustering into banana/shed (light/dark) rather than review sentiment (blue/orange).", "description": "This figure shows the results of an experiment where two random words, \"Banana\" and \"Shed\", were appended to movie review prompts.  The goal was to see if unsupervised methods would focus on the actual review sentiment or the randomly added words.  Part (a) shows that the accuracy of the methods in predicting the review sentiment drops significantly when the random words are added, indicating that the methods are prioritizing the random words over the actual sentiment. Part (b) shows a PCA visualization of the LLM activations, further demonstrating that the unsupervised methods are clustering the activations based on the presence of the random words rather than the review sentiment.", "section": "4.1 Discovering random words"}]