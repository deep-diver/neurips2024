[{"figure_path": "DSVGACQ3sO/figures/figures_4_1.jpg", "caption": "Figure 1: In-distribution generalization of CSIvA trained and tested on data generated according to the same structural causal models, fixing mechanisms, and noise distributions between training and testing). As baselines for comparison, we use DirectLiNGAM on linear SCMs and NoGAM on nonlinear ANM (we use their causal-learn and dodiscover implementations). CSIvA performance is clearly non-trivial and generalizing well.", "description": "This figure shows the in-distribution generalization performance of the CSIvA model on linear, nonlinear, and post-nonlinear structural causal models. The CSIvA model is trained and tested on datasets generated from the same type of structural causal model, with the same mechanisms and noise distributions. The results are compared against two benchmark models: DirectLiNGAM for linear SCMs and NoGAM for nonlinear ANMs. The figure demonstrates that CSIvA generalizes well in the in-distribution setting, achieving comparable performance to the benchmark models.", "section": "3.2 Warm up: is CSIvA capable of in and out-of-distribution generalization?"}, {"figure_path": "DSVGACQ3sO/figures/figures_5_1.jpg", "caption": "Figure 2: Out-of-distribution generalisation. We train three CSIvA models on data sampled from SCMs with linear, nonlinear additive, and post-nonlinear mechanisms; and noise fixed mlp noise distribution. In Figure (a) we test across different noise distributions, with test mechanisms fixed from training. In Figure (b) we test each network on different mechanisms and fixed mlp noise. CSIvA struggles to generalize to unseen causal mechanisms and often displays degraded performance over new noise distributions.", "description": "This figure shows the results of out-of-distribution generalization experiments on the CSIvA model.  Two sets of experiments are presented. In the first, the model is trained with a fixed mechanism type (linear, nonlinear, or post-nonlinear) and mlp noise and tested on various noise distributions while keeping the mechanism fixed.  The second set of experiments keeps the noise type fixed (mlp) and tests on different mechanism types.  The results demonstrate that CSIvA struggles to generalize well to unseen mechanisms and noise distributions, often performing near a random baseline.", "section": "3.2 Warm up: is CSIvA capable of in and out-of-distribution generalization?"}, {"figure_path": "DSVGACQ3sO/figures/figures_6_1.jpg", "caption": "Figure 3: Experiments on identifiability theory. In Figure (a) we test the performance on linear-Gaussian data. Models are trained with different ratios of samples from linear and nonlinear SCMs with Gaussian noise terms. The validation results showcase that the networks were trained successfully. Figure (b) shows the SHD of models trained on different ratios of linear and nonlinear invertible data of Example 2. CSIvA behaves according to identifiability theory, failing to predict on linear Gaussian models and invertible data (50:50 ratio).", "description": "This figure shows the results of experiments designed to test CSIvA's adherence to identifiability theory.  Part (a) demonstrates CSIvA's performance on linear Gaussian data when trained on varying ratios of linear and non-linear SCMs (with Gaussian noise). Part (b) shows CSIvA's performance on invertible data from Example 2 under different training ratios.  The results highlight that CSIvA's ability to correctly predict causal direction aligns with identifiability theory; it struggles with non-identifiable scenarios.", "section": "3.3 How does CSIvA relate to identifiability theory for causal graphs?"}, {"figure_path": "DSVGACQ3sO/figures/figures_7_1.jpg", "caption": "Figure 6: Mixtures of causal mechanisms, with varying amounts of training data. We train eight models on samples from structural casual models with different mechanisms. Four (in purple), were trained on 15,000 samples for each SCM type (so the \\\"lin,nl\\\" model saw 30, 000 samples in total, and the \\\"all\\\" model saw 45,000), and the other four (blue) are the same as in Fig. 4, and were trained on 15,000 samples in total, evenly split between the SCM types they were trained on. We compare their test SHD (the lower, the better) against networks trained on datasets generated according to a single type of mechanism. The dashed line indicates the test SHD of a model trained on samples with the same mechanisms as the test SCM. Training on multiple causal models with different mechanisms (mixed bars) always improves performance compared to training on single SCMs.", "description": "This figure compares the performance of CSIvA models trained on mixtures of causal mechanisms with those trained on a single mechanism type.  The models trained on mixtures consistently outperform those trained on single types, demonstrating the benefit of diverse training data. The figure shows that even when the amount of training data per mechanism type is the same in both mixed and single-mechanism training, the mixed training approach still yields better results.", "section": "3.5 Can we train CSIvA on multiple causal models for better generalization?"}, {"figure_path": "DSVGACQ3sO/figures/figures_8_1.jpg", "caption": "Figure 5: Mixture of noise distributions. We train three networks on samples from SCMs with different noise terms distributions and fixed mechanism types: linear, nonlinear, and post-nonlinear. We present their test SHD (the lower, the better) on data from SCMs with the mechanisms fixed with respect to training, and noise terms changing between each dataset. Training on multiple causal models with different noises (all distributions bars) always improves performance compared to training on single SCMs with fixed mlp noise (only mlp bars).", "description": "This figure displays the out-of-distribution generalization results of CSIvA when trained with multiple noise distributions (all distributions) and only with the mlp noise distribution (only mlp). It shows the structural Hamming Distance (SHD) for linear, non-linear, and post-nonlinear test data.  The results demonstrate that training CSIvA with multiple noise types enhances its ability to generalize to unseen noise distributions during testing compared to training with a single noise type (mlp).", "section": "3.5 Can we train CSIvA on multiple causal models for better generalization?"}, {"figure_path": "DSVGACQ3sO/figures/figures_14_1.jpg", "caption": "Figure 6: Mixtures of causal mechanisms, with varying amounts of training data. We train eight models on samples from structural casual models with different mechanisms. Four (in purple), were trained on 15,000 samples for each SCM type (so the \"lin,nl\" model saw 30, 000 samples in total, and the \"all\" model saw 45,000), and the other four (blue) are the same as in Fig. 4, and were trained on 15,000 samples in total, evenly split between the SCM types they were trained on. We compare their test SHD (the lower, the better) against networks trained on datasets generated according to a single type of mechanism. The dashed line indicates the test SHD of a model trained on samples with the same mechanisms as the test SCM. Training on multiple causal models with different mechanisms (mixed bars) always improves performance compared to training on single SCMs.", "description": "This figure compares the performance of CSIvA models trained on mixtures of causal mechanisms with different amounts of training data.  It shows that training on multiple causal model types (mixed training) generally leads to better generalization on unseen data compared to models trained on a single causal mechanism type (single training). This suggests that leveraging diverse training data can improve the model's ability to handle varied test scenarios.", "section": "C Further experiments"}]