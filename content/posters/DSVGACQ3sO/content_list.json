[{"type": "text", "text": "Demystifying amortized causal discovery with transformers ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Supervised learning approaches for causal discovery from observational data often   \n2 achieve competitive performance despite seemingly avoiding explicit assumptions   \n3 that traditional methods make for identifiability. In this work, we investigate CSIvA   \n4 [1], a transformer-based model promising to train on synthetic data and transfer   \n5 to real data. First, we bridge the gap with existing identifiability theory and show   \n6 that constraints on the training data distribution implicitly define a prior on the test   \n7 observations. Consistent with classical approaches, good performance is achieved   \n8 when we have a good prior on the test data, and the underlying model is identifiable.   \n9 At the same time, we find new trade-offs. Training on datasets generated from   \n10 different classes of causal models, unambiguously identifiable in isolation,   \n11 improves the test generalization. Performance is still guaranteed, as the ambiguous   \n12 cases resulting from the mixture of identifiable causal models are unlikely to occur   \n13 (which we formally prove). Overall, our study finds that amortized causal discovery   \n14 still needs to obey identifiability theory, but it also differs from classical methods   \n15 in how the assumptions are formulated, trading more reliance on assumptions on   \n16 the noise type for fewer hypotheses on the mechanisms. ", "page_idx": 0}, {"type": "text", "text": "17 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "18 Causal discovery aims to uncover the underlying causal relationships between variables of a system   \n19 from pure observations, which is crucial for answering interventional and counterfactual queries when   \n20 experimentation is impractical or unfeasible [2, 3, 4]. Unfortunately, causal discovery is inherently   \n21 ill-posed [5]: unique identification of causal directions requires restrictive assumptions on the class   \n22 of structural causal models (SCMs) that generated the data [6, 7, 8]. These theoretical limitations   \n23 often render existing methods inapplicable, as the underlying assumptions are usually untestable or   \n24 difficult to verify in practice [9].   \n25 Recently, supervised learning algorithms trained on synthetic data have been proposed to overcome   \n26 the need for specific hypotheses, which restrains the application of classical causal discovery methods   \n2 to real-world problems [1, 10, 11, 12, 13]. Seminal work from Lopez-Paz et al. [10] argues that   \n28 this learning-based approach to causal discovery would allow dealing with complex data-generating   \n29 processes and would greatly reduce the need for explicitly crafting identifiability conditions a-priori:   \n30 despite this ambitious goal, the output of these methods is generally considered unreliable, as no   \n31 theoretical guarantee is provided. A pair of non-identifiable structural causal models can be associated   \n32 with different causal graphs $\\mathcal{G}\\neq\\bar{\\tilde{\\mathcal{G}}}$ , while entailing the same joint distribution $p$ on the system\u2019s   \n33 variables. It is thus unclear how a learning algorithm presented with observational data generated from   \n34 $p$ would be able to overcome these theoretical limits and correctly identify a unique causal structure.   \n35 However, the available empirical evidence seems not to care about impossibility results, as these   \n36 methods yield surprising generalization results on several synthetic benchmarks. Our work aims to   \n37 bridge this gap by studying the performance of a transformer architecture for causal discovery through   \n38 the lens of the theory of identifiability from observational data. Specifically, we analyze the CSIvA   \n39 (Causal Structure Induction via Attention) model for causal discovery [1], focusing on bivariate graphs,   \n40 as they offer a controlled yet non-trivial setting for the investigation. As our starting point, we provide   \n41 closed-form examples that identify the limitations of CSIvA in recovering causal structures of linear   \n42 non-Gaussian and nonlinear additive noise models, which are notably identifiable, and demonstrate the   \n43 expected failures through empirical evidence. These findings suggest that the class of structural causal   \n44 models that can be identified by CSIvA is inherently dependent on the specific class of SCMs observed   \n45 during training. Thus, the need for restrictive hypotheses on the data-generating process is intrinsic   \n46 to causal discovery, both in the traditional and modern learning-based approaches: assumptions on   \n47 the test distribution either are posited when selecting the algorithm (traditional methods) or in the   \n48 choice of the training data (learning-based methods). To address this limitation, we theoretically and   \n49 empirically analyze when training CSIvA on datasets generated by multiple identifiable SCMs with   \n50 different structural assumptions improves its generalization at test time. In summary:   \n51 \u2022 We show that the class of structural causal models that CSIvA can identify is defined by the   \n52 class of SCMs observed through samples during the training. We reinforce the notion that   \n53 identifiability in causal discovery inherently requires assumptions, which must be encoded   \n54 in the training data in the case of learning-based approaches.   \n55 \u2022 To overcome this limitation, we study the beneftis of CSIvA training on mixtures of causal   \n56 models. We analyze when algorithms learned on multiple models are expected to identify   \n57 broad classes of SCMs (unlike many classical methods). Empirically, we show that training   \n58 on samples generated by multiple identifiable causal models with different assumptions on   \n59 mechanisms and noise distribution results in significantly improved generalization abilities.   \n60 Closely related works and their relation with CSIvA. In this paper, we study amortized inference   \n61 of causal graphs, i.e. optimization of an inference model to directly predict a causal structure from   \n62 newly provided data. This is the first work that attempts to understand the connection between   \n63 identifiability theory and amortized inference, while several algorithms have been proposed. In the   \n64 context of purely observational data, Lopez-Paz et al. [10] defines a distribution regression problem   \n65 [14] mapping the kernel mean embedding of the data distribution to a causal graph, while Li et al.   \n66 [11] relies on equivariant neural network architectures. More recently, Lippe et al. [12] and Lorch   \n67 et al. [13] proposed learning on interventional data, in addition to observations (in the same spirit as   \n68 CSIvA). Despite different algorithmic implementations, the target object of estimation of most of   \n69 these methods is the distribution over the space of all possible graphs, conditional on the input dataset   \n70 (similarly, the ENCO algorithm in Lippe et al. [12] models the conditional distribution of individual   \n71 edges). This justifies our choice of restricting our study to the CSIvA architecture (despite this   \n72 being a clear limitation), as in the infinite observational sample limit, these methods approximate the   \n73 same distribution. Methods necessarily requiring interventional data [15, 16, 17], and learning-based   \n74 algorithms unsuitable for amortized inference [18, 19, 20, 21, 22] are out of the scope of this work. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "75 2 Background and motivation ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "76 We start introducing structural causal models (SCMs), an intuitive framework that formalizes causal   \n77 relations. Let $X$ be a set of random variables in $\\mathbb{R}$ defined according to the set of structural equations: ", "page_idx": 1}, {"type": "equation", "text": "$$\nX_{i}:=f_{i}(X_{\\mathrm{PA}_{i}^{g}},N_{i}),\\enspace\\forall i=1,\\ldots,k.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "78 $N_{i}\\in\\mathbb{R}$ are noise random variables. The function $f_{i}$ is the causal mechanism mapping the set of direct   \n79 causes $X_{\\mathrm{PA}_{i}^{\\mathcal{G}}}$ of $X_{i}$ and the noise term $N_{i}$ , to $X_{i}$ \u2019s value. The causal graph $\\mathcal{G}$ is a directed acyclic   \n80 graph (DAG) with nodes $X=\\{X_{1},\\ldots,X_{k}\\}$ , and edges $\\{X_{j}\\,\\rightarrow\\,X_{i}\\,:\\,X_{j}\\,\\in\\,X_{\\mathrm{PA}_{i}^{\\mathcal{G}}}\\}$ , with $\\mathrm{PA}_{i}^{g}$   \n81 indices of the parent nodes of $X_{i}$ in $\\mathcal{G}$ . The causal model induces a density $p_{X}$ over the vector $X$ . ", "page_idx": 1}, {"type": "text", "text": "82 2.1 Causal discovery from observational data ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "83 Causal discovery from observational data is the inference of the causal graph $\\mathcal{G}$ from a dataset   \n84 of i.i.d. observations of the random vector $X$ . In general, without restrictive assumptions on the   \n85 mechanisms and the noise distributions, the direction of edges in the graph $\\mathcal{G}$ is not identifiable, i.e.   \n86 it can not be found from the population density $p_{X}$ . In particular, it is possible to identify only a   \n87 Markov equivalence class, which is the set of graphs encoding the same conditional independencies   \n88 as the density $p_{X}$ . To clarify with an example, consider the causal graph $X_{1}\\rightarrow X_{2}$ associated   \n89 with a structural causal model inducing a density $p_{X_{1},X_{2}}$ . If the model is not identifiable, there   \n90 exists an SCM with causal graph $X_{2}\\rightarrow X_{1}$ that entails the same joint density $p_{X_{1},X_{2}}$ . The set   \n91 $\\{X_{1}\\rightarrow X_{2},X_{2}\\rightarrow X_{1}\\}$ is the Markov equivalence class of the graph $X_{1}\\rightarrow X_{2}$ , i.e. the set of all   \n92 graphs with $X_{1},X_{2}$ mutually dependent. Clearly, in this setting, even the exact knowledge of $p_{X_{1},X_{2}}$   \n93 cannot inform us about the correct causal direction.   \n94 Definition 1 (Identifiable causal model). Consider a structural causal model with underlying graph $\\mathcal{G}$   \n95 and $p_{X}$ joint density of the causal variables. We say that the model is identifiable from observational   \n96 data if the density $p_{X}$ can not be entailed by a structural causal model with graph $\\tilde{\\mathcal{G}}\\neq\\mathcal{G}$ . ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "97 We define the post-additive noise model (post-ANM) as the causal model with the set of equations: ", "page_idx": 2}, {"type": "equation", "text": "$$\nX_{i}:=f_{2,i}(f_{1,i}(X_{\\mathrm{PA}_{i}^{g}})+N_{i}),\\;\\forall i=1,\\ldots,d,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "98 with $f_{2,i}$ invertible map and mutually independent noise terms. When $f_{2,i}$ is a nonlinear function,   \n99 the post-ANM amounts to the identifiable post-nonlinear model (PNL) [8]. When $f_{2,i}$ is the identity   \n100 function and $f_{1,i}$ nonlinear, it simplifies to the nonlinear additive noise model (ANM)[7, 23], which   \n101 is known to be identifiable, and is described by the set of structural equations: ", "page_idx": 2}, {"type": "equation", "text": "$$\nX_{i}:=f_{1,i}(X_{\\mathrm{PA}_{i}^{\\mathcal{G}}})+N_{i}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "102 If, additionally, we restrict the mechanisms $f_{1,i}$ to be linear and the noise terms $N_{i}$ to a non-Gaussian   \n103 distribution, we recover the identifiable linear non-Gaussian additive model or LiNGAM [6]: ", "page_idx": 2}, {"type": "equation", "text": "$$\nX_{i}=\\sum_{j\\in\\mathrm{PA}_{i}^{\\mathcal{G}}}\\alpha_{j}X_{j}+N_{i},\\quad\\alpha_{j}\\in\\mathbb{R}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "104 2.2 Motivation and problem definition ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "105 Causal discovery from observational data relies on specific assumptions, which can be challenging to   \n106 verify in practice [9]. To address this, recent methods leverage supervised learning for the amortized   \n107 inference of causal graphs [1, 10, 11, 12, 13, 16, 24], optimizing an inference model to directly   \n108 predict a causal structure from a provided dataset. While these approaches aim to reduce reliance on   \n109 explicit identifiability assumptions, they often lack a clear connection to the existing causal discovery   \n110 theory, making their outputs generally unreliable. We illustrate this limitation through an example.   \n111 Example 1. We consider the CSIvA transformer architecture proposed by Ke et al. [1], which can   \n112 learn a map from observational data to a causal graph. The authors of the paper show that, in the in  \n113 finite sample regime, the CSIvA architecture exactly approximates the conditional distribution $p(\\cdot|\\mathcal{D})$   \n114 over the space of possible graphs, given a dataset $\\mathcal{D}$ . Identifiability theory in causal discovery tells us   \n115 that if the class of structural causal models that generated the observations is sufficiently constrained,   \n116 then there is only one graph that can fit the data within that class. For example, consider the case   \n117 of a dataset that is known to be generated by a nonlinear additive noise model, and let $p(\\cdot|\\mathcal{D},\\mathrm{ANM})$   \n118 be the conditional distribution that incorporates this prior knowledge on the SCM: then $p(\\cdot|\\mathcal{D},\\mathrm{ANM})$   \n119 concentrates all the mass on a single point $g^{*}$ , the true graph underlying the $\\mathcal{D}$ observations. Instead,   \n120 in the absence of restrictions on the structural causal model, all the graphs in a Markov equivalence   \n121 class are equally likely to be the correct solution given the data. Hence, $p(\\cdot|\\mathcal{D})$ , the distribution   \n122 learned by CSIvA, assigns equal probability to each graph in the Markov equivalence class of $\\mathcal{G}^{*}$ .   \n123 Our arguments of Example 1 are valid for all learning methods that approximate the conditional   \n124 distribution over the space of graphs given the input data [1, 10, 11, 12, 13], and suggest that these   \n125 algorithms are at most informative about the equivalence class of the causal graph underlying the   \n126 observations. However, the available empirical evidence does not seem to highlight these limitations,   \n127 as in practice these methods can infer the true causal DAG on several synthetic benchmarks. Thus, fur  \n128 ther investigation is necessary if we want to rely on their output in any meaningful sense. In this work,   \n129 we analyze these \"black-box\" approaches through the lens of established theory of causal discovery   \n130 from observational data (causal inference often lacks experimental data, which we do not consider).   \n131 We study in detail the CSIvA architecture [1] (see Appendix A), a variation of the transformer neural   \n132 network [25] for the supervised learning of algorithms for amortized causal discovery. This model is   \n133 optimized via maximum likelihood estimation, i.e. finding $\\Theta$ that minimizes $-\\mathbf{E}_{\\mathcal{G},\\mathcal{D}}[\\ln\\hat{p}(\\mathcal{G}|\\mathcal{D};\\boldsymbol{\\Theta})]$ ,   \n134 where $\\hat{p}(\\mathcal{G}|\\mathcal{D};\\Theta)$ is the conditional distribution of a graph $\\mathcal{G}$ given a dataset $\\mathcal{D}$ parametrized by $\\Theta$ .   \n135 We limit the analysis to CSIvA as it is a simple yet competitive end-to-end approach to learning causal   \n136 models. While this is clearly a limitation of the paper, our theoretical and empirical conclusions   \n137 exemplify both the role of theoretical identifiability in modern approaches and the new opportunities   \n138 they provide. Additionally, it fits well within a line of works arguing that specifically transformers   \n139 can learn causal concepts [26, 27, 28] and identify different assumptions in context [29]. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "140 3 Experimental results through the lens of theory ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "141 In this section, we present a comprehensive analysis of causal discovery with transformers and its   \n142 relation to the theoretical boundaries of causal discovery from observational data. We show that   \n143 suitable assumptions must be encoded in the training distribution to ensure the identifiability of the   \n144 test data, and we additionally study the effectiveness of training on mixtures of causal models to   \n145 overcome these limitations, improving generalization abilities. ", "page_idx": 3}, {"type": "text", "text": "146 3.1 Experimental design ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "147 We concentrate our research on causal models of two variables, causally related according to one of the   \n148 two graphs $X\\rightarrow Y$ , $Y\\rightarrow X$ . Bivariate models are the simplest non-trivial setting with a well-known   \n149 theory of causality inference [7, 8, 23], but also amenable to manipulation. This allows for compre  \n150 hensive training and analysis of diverse SCMs and facilitates a clear interpretation of the results.   \n151 Datasets. Unless otherwise specified, in our experiments we train CSIvA on a sample of 15000   \n152 synthetically generated datasets, consisting of 1500 i.i.d. observations. Each dataset is generated ac  \n153 cording to a single class of SCMs, defined by the mechanism type and the noise terms distribution. The   \n154 coefficients of the linear mechanisms are sampled in the range $[-3,-0.5]\\cup[0.5,3]$ , removing small co  \n155 efficients to avoid close-to-unfaithful effects [30]. Nonlinear mechanisms are parametrized according   \n156 to a neural network with random weights, a strategy commonly adopted in the literature of causal dis  \n157 covery [1, 9]. The post-nonlinearity of the PNL model consists of a simple map $z\\mapsto z^{3}$ . Noise terms   \n158 are sampled from common distributions and a randomly generated density that we call $m l p$ , previously   \n159 adopted in Montagna et al. [9], defined by a standard Gaussian transformed by a multilayer perceptron   \n160 (MLP) (Appendix B.2). We name these datasets mechanism-noise to refer to their underlying causal   \n161 model. For example, data sampled from a nonlinear ANM with Gaussian noise are named nonlinear  \n162 gaussian. More details on the synthetic data generation schema are found in Appendix B.2. All data   \n163 are standardized by their empirical variance to remove opportunities to learn shortcuts [31, 32, 33].   \n164 Metric and random baseline. As our metric we use the structural Hamming distance (SHD), which   \n165 is the number of edge removals, insertions or flips to transform one graph to another. In the context   \n166 of bivariate causal graphs with a single edge, this is simply an error count, so correct inference corre  \n167 sponds to $\\mathrm{SHD}=0$ , and an incorrect prediction gives $\\mathrm{SHD}=1$ . Additionally, we define a reference   \n168 random baseline, which assigns a causal direction according to a fair coin, achieving $\\mathrm{{SHD}=0.5}$ in ex  \n169 pectation. Each architecture we analyze in the experiments is trained 3 times, with different parameter   \n170 initialization and training samples: the SHD presented in the plots is the average of each of the 3 mod  \n171 els on 1500 distinct test datasets of 1500 points each, and the error bars are $95\\%$ confidence intervals.   \n172 We detail the training hyperparameters in Appendix B.1. Next, we analyze our experimental results,   \n173 starting by investigating how well CSIvA generalizes on distributions unseen during training. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "174 3.2 Warm up: is CSIvA capable of in and out-of-distribution generalization? ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "175 In-distribution generalization. First, we investigate the generalization of CSIvA on datasets   \n176 sampled from the structural casual model that generates the train distribution, with mechanisms and   \n177 noise distributions fixed between training and testing. We call this in-distribution generalization. As   \n178 a benchmark, we present the performance of several state-of-the-art approaches from the literature   \n179 on causal discovery: we consider the DirectLiNGAM, and NoGAM algorithms [34, 35], respectively   \n180 designed for the inference on LiNGAM and nonlinear ANM generated data1. The results of Figure 1   \n181 show that CSIvA can properly generalize to unseen samples from the training distribution: the majority   \n182 of the trained models present SHD close to zero and comparable to the relative benchmark algorithm.   \n183 Out-of-distribution generalization. In practice, we generally do not know the SCM defining the   \n184 test distribution, so we are interested in CSIvA\u2019s ability to generalize to data sampled from a class   \n185 of causal models that is unobserved during training. We call this out-of-distribution generalization   \n186 (OOD). We study OOD generalization to different noise terms, analyzing the network performance   \n187 on datasets generated from causal models where the mechanisms are fixed with respect to the   \n188 training, while the noise distribution varies (e.g., given linear-mlp training samples, testing occurs   \n189 on linear-uniform data). Orthogonally to these experiments, we empirically validate CSIvA\u2019s OOD   \n190 generalization over different mechanism types (linear, nonlinear, post-nonlinear), while leaving the   \n191 noise distribution (mlp) fixed across test and training. In Figure 2a, we observe that CSIvA cannot   \n192 generalize across the different mechanisms, as the SHD of a network tested on unseen causal mech  \n193 anisms approximates that of the random baseline. Further, Figure 2b shows that out-of-distribution   \n194 generalization across noise terms does not work reliably, and it is hard to predict when it might occur.   \n195 Implications. CSIvA generalizes well to test data generated by the same class of SCMs used   \n196 for training, in line with the findings in Ke et al. [1], which validates our implementation and   \n197 training procedure. However, it struggles when the test data are out-of-distribution, not generated   \n198 by causal models with the same mechanisms and noise terms it was trained on. While training on   \n199 a wider class of SCMs might overcome this limitation, it requires caution. The identifiability of   \n200 causal graphs indeed results from the interplay between the data-generating mechanisms and noise   \n201 distribution. However, as we argue in our Example 1, the class of causal models that a supervised   \n202 learning algorithm can identify is generally not clear. In what follows, we investigate this point and   \n203 its implications for CSIvA, showing that the identifiability of the test samples can be ensured by   \n204 imposing suitable assumptions on the class of SCMs generating the training distribution. ", "page_idx": 3}, {"type": "image", "img_path": "DSVGACQ3sO/tmp/7f3888f4a482934efb465d51abcef378d8768fda67b833289f4ea24b7ded75fe.jpg", "img_caption": ["Figure 1: In-distribution generalization of CSIvA trained and tested on data generated according to the same structural causal models, fixing mechanisms, and noise distributions between training and testing). As baselines for comparison, we use DirectLiNGAM on linear SCMs and NoGAM on nonlinear ANM (we use their causallearn and dodiscover implementations). CSIvA performance is clearly non-trivial and generalizing well. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "205 3.3 How does CSIvA relate to identifiability theory for causal graphs? ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "206 The CSIvA algorithm does not make structural assumptions about the causal model underlying the   \n207 input data. This implies that the output of this method is unclear: as CSIvA targets the conditional dis  \n208 tribution $p(\\cdot|\\mathcal{D})$ over the space of graphs, in the absence of restrictions on the functional mechanisms   \n209 and the distribution of the noise terms, the causal graph $X\\rightarrow Y$ is indistinguishable from $Y\\rightarrow X$ ,   \n210 as they are both equally likely to underlie the joint density $p_{X,Y}$ generating the data. As we discuss in   \n211 Example 1, the graphical output of the trained architecture could at most identify the equivalence class   \n212 of the true causal graph. Yet, our experiments of Section 3.2 show that CSIvA is capable of good in  \n213 distribution generalization, often inferring the correct DAG at test time. We explain this seeming con  \n214 tradiction with the following hypothesis, which motivates the analysis in the remainder of this section.   \n215 Hypothesis 1. The class of structural causal models that can be identified by CSIvA is defined by the   \n216 class of structural causal models underlying the generation of the training data.   \n217 To support and clarify our statement, we present the following example, adapted from Hoyer et al. [7].   \n218 Example 2. Consider the causal model $Y\\;=\\;f(X)+N$ , where $f(X)\\;=\\;-X$ and $p_{X},p_{N}$ are   \n219 Gumbel densities $p_{X}(x)=\\exp(-x-\\exp(-x))$ and $p_{N}(n)=\\exp(-n-\\exp(-n))$ . This model   \n220 satisfies the assumptions of the LiNGAM, so it is identifiable, in the sense that a backward linear   \n221 model with the same distribution does not exist. However, in this special case, we can build a   \n222 backward nonlinear additive noise model $X=g(Y)+\\tilde{N}$ with independent noise terms: taking   \n223 $p_{Y}(y)\\,=\\,\\exp(-y\\,-\\,2\\log(1+\\exp(-y)))$ to be the density of a logistic distribution, $p_{\\tilde{N}}(\\tilde{n})\\;=$   \n224 $\\exp(-2\\tilde{n}-\\exp(-\\tilde{n}))$ and $g(y)=\\log(1+\\exp(-y))$ ; we see that $p_{X,Y}$ can factorize according   \n225 to two opposite causal directions, as $\\bar{p}_{X,Y}(x,\\bar{y})\\,=\\,\\bar{p}_{N}(y-f(x))\\bar{p_{X}}(\\bar{x})\\,=\\,p_{\\tilde{N}}(x-g(y))p_{Y}(y)$ .   \n226 Given a dataset $\\mathcal{D}$ of observations from the forward linear model, causal discovery methods like   \n227 DirectLiNGAM [34] can provably identify the correct causal direction $X\\,\\rightarrow\\,Y$ , assuming that   \n228 sufficient samples are provided. Instead, the behavior of CSIvA seems hard to predict: given that   \n229 the network approximates the conditional distribution $p(\\cdot|\\mathcal{D})$ over the possible graphs, for $\\mathcal{D}$ with   \n230 arbitrary many samples we have $p(X\\to Y|\\mathcal{D})=p(Y\\stackrel{.}{\\to}X|\\mathcal{D})=0.\\bar{5}$ . On the other hand, given   \n231 the prior knowledge that the data-generating SCM is a linear non-gaussian additive noise model, we   \n232 have $p(X\\to Y|\\bar{\\mathcal{D}},\\mathrm{LiNGAM})=\\bar{1}$ , because the LiNGAM is identifiable. In this sense, the class   \n233 of structural causal models that CSIvA correctly infers appears to be determined by the structural   \n234 causal models underlying the generation of the training data. Under our Hypothesis 1, training CSIvA   \n235 exclusively on LiNGAM-generated data is equivalent to learning the distribution $p(\\cdot|\\mathcal{D},\\mathrm{LiNGAM})$ ,   \n236 such that the network should be able to identify the forward linear model, whereas it could only infer   \n237 the equivalence class of the causal graph if its training datasets include observations from a nonlinear   \n238 additive noise model.   \n239 The empirical results of Figure 3a show that CSIvA behaves according to our hypothesis: when   \n240 training exclusively occurs on datasets $\\{\\ensuremath{\\mathcal{D}}_{i,\\to}\\}_{i}$ generated by the forward linear-gumbel model of   \n241 Example 2, the network can identify the causal direction of test data generated according to the same   \n242 SCM. Similarly, the transformer trained on datasets $\\{\\boldsymbol{D}_{i,\\leftarrow}\\}_{i}$ from the backward nonlinear model   \n243 of the example can generalize to test data coming from the same distribution. According to our claim,   \n244 instead, the network that is trained on the union of the training samples $\\{\\mathcal{D}_{i,\\to}\\}_{i}\\cup\\{\\mathcal{D}_{i,\\leftarrow}\\}_{i}$ from   \n245 the forward and backward models ( $50{:}50$ ratio in Figure 3a) displays the same test SHD (around   \n246 0.5) as a random classifier assigning the causal direction with equal probability.   \n247 Further, we investigate CSIvA\u2019s relation with known identifiability theory by training and testing the   \n248 architecture on data from a linear Gaussian model, which is well-known to be unidentifiable. Not   \n249 surprisingly, the results of Figure 3b show that none of the algorithms that we learn can infer the   \n250 causal order of linear Gaussian models with test SHD any better than a random baseline.   \n251 Implications. Our experiments show that CSIvA learns algorithms that closely follow identifiability   \n252 theory for causal discovery. In particular, while the method itself does not require explicit assumptions   \n253 on the data-generating process, the chosen training data ultimately determines the class of causal   \n254 models identifiable during inference. Notably, previous work has argued that supervised learning   \n255 approaches in causal discovery would help with \"dealing with complex data-generating processes and   \n256 greatly reduce the need of explicitly crafting identifiability conditions a-priori\", Lopez-Paz et al. [10].   \n257 In the case of CSIvA, this expectation does not appear to be fulfilled, as the assumptions still need   \n258 to be encoded explicitly in the training data. However, this observation opens two new important   \n259 questions: (1) Can we train a single network to encompass multiple (or even all) identifiable causal   \n260 structures? (2) How much ambiguity might exist between these identifiable models? ", "page_idx": 4}, {"type": "image", "img_path": "DSVGACQ3sO/tmp/bf42d54a48ad9b65ff997f4a8b2afd536cb866e61fc643f7be359d4c36d49abe.jpg", "img_caption": ["Figure 2: Out-of-distribution generalisation. We train three CSIvA models on data sampled from SCMs with linear, nonlinear additive, and post-nonlinear mechanisms; and noise fixed mlp noise distribution. In Figure (a) we test across different noise distributions, with test mechanisms fixed from training. In Figure (b) we test each network on different mechanisms and fixed mlp noise. CSIvA struggles to generalize to unseen causal mechanisms and often displays degraded performance over new noise distributions. ", "", ""], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "image", "img_path": "DSVGACQ3sO/tmp/b6439da4eee31bbf46a469a53119cce167d59806b6b13331d6f1ef752c33659e.jpg", "img_caption": ["Figure 3: Experiments on identifiability theory. In Figure (a) we test the performance on linear-Gaussian data. Models are trained with different ratios of samples from linear and nonlinear SCMs with Gaussian noise terms. The validation results showcase that the networks were trained successfully. Figure (b) shows the SHD of models trained on different ratios of linear and nonlinear invertible data of Example 2. CSIvA behaves according to identifiability theory, failing to predict on linear Gaussian models and invertible data (50:50 ratio). "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "261 3.4 A low-dimensions argument in favor of learning from multiple causal models ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "262 Example 2 of the previous section shows that elements of distinct classes of identifiable structural   \n263 causal models, such as LiNGAM and nonlinear ANM, may become non-identifiable when we   \n264 consider their union. In this section, we show that in the class of post-additive noise models given   \n265 by equation (2) (obtained as the union of the LiNGAM, the nonlinear ANM, and the post-nonlinear   \n266 model), the set of distributions that is non-identifiable is negligible. Our proposition extends the   \n267 results of Hoyer et al. [7], which are limited to the case of linear and nonlinear additive noise models,   \n268 and Zhang and Hyv\u00e4rinen [8], which provides the conditions of identifiability of the post-ANM   \n269 without bounding the set of non-identifiable distributions.   \n270 Let $X,Y$ be a pair of random variables generated according to the causal direction $X\\rightarrow Y$ and the   \n271 post-additive noise model structural equation: ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "equation", "text": "$$\nY=f_{2}(f_{1}(X)+N_{Y}),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "272 where $N_{Y}$ and $X$ are independent random variables, and $f_{2}$ is invertible. If the SCM is non  \n273 identifiable, the data-generating process can be described by a backward model with the structural   \n274 equation: ", "page_idx": 6}, {"type": "equation", "text": "$$\nX=g_{2}\\bigl(g_{1}(Y)+N_{X}\\bigr),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "275 $N_{X}$ independent from $Y$ , and $g_{2}$ invertible. We introduce the random variables $\\tilde{X},\\tilde{Y}$ , such that the   \n276 forward and backward equations can be rewritten as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{Y=f_{2}(\\tilde{Y}),\\quad\\tilde{Y}:=f_{1}(X)+N_{Y},}\\\\ {X=g_{2}(\\tilde{X}),\\quad\\tilde{X}:=g_{1}(Y)+N_{X}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "image", "img_path": "DSVGACQ3sO/tmp/c258f06fb9c730334a7d5e038a68907ceecc24ad013741543fbe6925a8845511.jpg", "img_caption": ["Figure 4: Mixture of causal mechanisms. We train four models on samples from structural casual models with different mechanism types. We compare their test SHD (the lower, the better) against networks trained on datasets generated according to a single type of mechanism. The dashed line indicates the test SHD of a model trained on samples with the same mechanisms as test SCM. Training on multiple causal models with different mechanisms (mixed bars) always improves performance compared to training on single SCMs. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "277 We note that this implies that the following invertible additive noise models on $\\tilde{X},\\tilde{Y}$ hold: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\tilde{Y}=h_{Y}(\\tilde{X})+N_{Y},\\quad h_{Y}:=f_{1}\\circ g_{2},}\\\\ &{}&{\\tilde{X}=h_{X}(\\tilde{Y})+N_{X},\\quad h_{X}:=g_{1}\\circ f_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "278 Proposition 1 (Adapted from Hoyer et al. [7]). Let $p_{N_{Y}},h_{X},h_{Y}$ be fixed, and define $\\nu_{Y}:=\\log p_{N_{Y}}$ ,   \n279 $\\xi:=\\log p_{\\tilde{X}}$ . Suppose that $p_{N_{Y}}$ and $p_{\\tilde{X}}$ are strictly positive densities, and that $\\nu_{Y},\\xi,f_{1},f_{2},g_{1}.$ , and   \n280 $g_{2}$ are three times differentiable. Further, assume that for a fixed pair $h_{Y},\\nu_{Y}$ exists $\\tilde{y}\\in\\mathbb R$ s.t. $\\nu_{Y}^{\\prime\\prime}(\\tilde{y}-$   \n281 $\\bar{h}_{Y}(\\tilde{x}))h_{Y}^{\\prime}(\\tilde{x})\\neq0$ is satisfied for all but a countable set of points $\\tilde{x}\\in\\mathbb R$ . Then, the set of all densities   \n282 $p_{\\tilde{X}}$ of $\\tilde{X}$ such that both equations (5) and (6) are satisfied is contained in a 2-dimensional space.   \n283 Implications. Our result is closely related to Theorem 1 of Hoyer et al. [7], which we simply   \n284 generalize to the post-ANM. Intuitively, it says that the space of all continuous distributions such that   \n285 the bivariate post-ANM is non-identifiable is contained in a 2-dimensional space. As the space of   \n286 continuous distributions of random variables is infinite-dimensional, we conclude that the post-ANM   \n287 is generally identifiable, which suggests that the setting of Example 2 is rather artificial. Our results   \n288 provide a theoretical ground for training causal discovery algorithms on datasets generated from   \n289 multiple identifiable SCMs. This is particularly appealing in the case of CSIvA, given the poor OOD   \n290 generalization ability observed in our experiments of Section 3.2. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "291 3.5 Can we train CSIvA on multiple causal models for better generalization? ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "292 In this section, we investigate the benefits of training over multiple causal models, i.e. on samples   \n293 generated by a combination of classes of identifiable SCMs characterized by different mechanisms   \n294 and noise terms distribution. Our motivation is as follows: given that our empirical evidence   \n295 shows that CSIvA is capable of in-distribution generalization, whereas dramatically degrades the   \n296 performance when testing occurs out-of-distribution, it is thus desirable to increase the class of   \n297 causal models represented in the training datasets. We separately study the effects of training over   \n298 multiple mechanisms and multiple noise distributions and compare the testing performance against   \n299 architectures trained on samples of a single SCM.   \n300 Mixture of causal mechanisms. We consider four networks optimized by training of CSIvA on   \n301 datasets generated from pairs (or triples) of distinct SCMs, with fixed mlp noise and which differ in   \n302 terms of their mechanisms type: linear and nonlinear; nonlinear and post-nonlinear; linear and post  \n303 nonlinear; linear, nonlinear and post-nonlinear. The number of training datasets for each architecture is   \n304 fixed (15000) and equally split between the causal models with different mechanism types. The results   \n305 of Figure 4 show that the networks trained on mixtures of mechanisms all present significantly better   \n306 test SHD compared to CSIvA models trained on a single mechanism type. We find that learning on   \n307 multiple SCMs improves the SHD from $\\sim\\!0.5$ to $\\sim\\!0.2$ both on linear and nonlinear test data (Figures   \n308 4a and 4b), and even better accuracy is achieved on post-nonlinear samples, as shown in Figure $4c$ .   \n309 Mixture of noise distributions. Next, we analyze the test performance of three CSIvA networks   \n310 optimized on samples from structural causal models that have different distributions for their noise   \n311 terms, while keeping the mechanism types fixed. Figure 5 shows that training over different noises   \n312 (beta, gamma, gumbel, exponential, mlp, uniform) always results in a network that is agnostic with   \n313 respect to the noise distributions of the SCM generating the test samples, always achieving $\\mathrm{SHD<0.1}$ ,   \n314 with the exception of datasets with mlp error terms (0.2 average SHD on nonlinear and pnl data).   \n315 Implications. We have shown that learning on mixtures of SCMs with different noise term dis  \n316 tributions and mechanism types leads to models generalizing to a much broader class of structural   \n317 causal models during testing. Hence, combining datasets generated from multiple models looks   \n318 like a promising framework to overcome the limited out-of-distribution generalization abilities of   \n319 CSIvA observed in Section 3.2. However, it is easier to incorporate prior assumptions on the class of   \n320 causal mechanisms (linear, non-linear, post-non-linear) compared to the noise distributions (which are   \n321 potentially infinite). This introduces a trade-off between amortized inference and classical methods   \n322 for causal discovery: for example, RESIT, NoGAM, and CAM [23, 35, 36] algorithms require no   \n323 assumptions on the noise type, but only work for a limited class of mechanisms (nonlinear). ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "image", "img_path": "DSVGACQ3sO/tmp/9633c20a9bf142d43471d94bf1e09fbfa289a672d30fbbcfa5bdd08a266a9545.jpg", "img_caption": ["Figure 5: Mixture of noise distributions. We train three networks on samples from SCMs with different noise terms distributions and fixed mechanism types: linear, nonlinear, and post-nonlinear. We present their test SHD (the lower, the better) on data from SCMs with the mechanisms fixed with respect to training, and noise terms changing between each dataset. Training on multiple causal models with different noises (all distributions bars) always improves performance compared to training on single SCMs with fixed mlp noise (only mlp bars). "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "324 4 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "325 In this work, we investigate the interplay between identifiability theory and supervised learning   \n26 for amortized inference of causal graphs, using CSIvA as the ground of our study. Consistent   \n27 with classical algorithms, we demonstrate that good performance can be achieved if (i) we have   \n28 a good prior on the structural causal model generating the test data (ii) the setting is identifiable.   \n29 In particular, prior knowledge of the test distribution is encoded in the training data in the form   \n30 of constraints on the structural causal model underlying their generation. With these results, we   \n31 highlight the need for identifiability theory in modern learning-based approaches to causality, while   \n32 past works have mostly disregarded this connection. Further, our findings provide the theoretical   \n33 ground for training on observations sampled from multiple classes of identifiable SCMs, a strategy   \n34 that improves test generalization to a broad class of causal models. Finally, we highlight an interesting   \n335 new trade-off regarding identifiability: traditional methods like LiNGAM, RESIT, and PNL require   \n36 strong restrictions on the structural mechanisms underlying the data generation (linear, nonlinear   \n37 or post-nonlinear) while generally being agnostic relative to the noise terms distribution. Training   \n338 on mixtures of causal models instead offers an alternative that is less reliant on assumptions on the   \n339 mechanisms, while incorporating knowledge about all possible noise distributions in the training data   \n340 is practically impossible to achieve. We leave it to future work to reproduce our analysis on a wider   \n341 class of architectures, as well as extending our study to interventional data with more than two nodes. ", "page_idx": 8}, {"type": "text", "text": "342 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "343 [1] Nan Rosemary Ke, Silvia Chiappa, Jane X. Wang, Jorg Bornschein, Anirudh Goyal, Melanie   \n344 Rey, Theophane Weber, Matthew Botvinick, Michael Curtis Mozer, and Danilo Jimenez   \n345 Rezende. Learning to Induce Causal Structure. In International Conference on Learning Repre  \n346 sentations, September 2022. URL https://openreview.net/forum?id $\\equiv$ hp_RwhKDJ5.   \n347 [2] Jonas Peters, Dominik Janzing, and Bernhard Sch\u00f6lkopf. Elements of Causal Inference:   \n348 Foundations and Learning Algorithms. Adaptive Computation and Machine Learning. The MIT   \n349 Press, Cambridge, Mass, 2017. ISBN 978-0-262-03731-0.   \n350 [3] Judea Pearl. Causality. Cambridge University Press, Cambridge, 2nd edition, 2009.   \n351 [4] Peter Spirtes. Introduction to causal inference. Journal of Machine Learning Research, 11(54):   \n352 1643\u20131662, 2010. URL http://jmlr.org/papers/v11/spirtes10a.html.   \n353 [5] Clark Glymour, Kun Zhang, and Peter Spirtes. Review of causal discovery methods based on   \n354 graphical models. Frontiers in Genetics, 10, 2019. ISSN 1664-8021. doi: 10.3389/fgene.2019.   \n355 00524. URL https://www.frontiersin.org/articles/10.3389/fgene.2019.00524.   \n356 [6] Shohei Shimizu, Patrik O. Hoyer, Aapo Hyv\u00e4rinen, and Antti Kerminen. A linear non-gaussian   \n357 acyclic model for causal discovery. Journal of Machine Learning Research, 7:2003\u20132030, dec   \n358 2006. ISSN 1532-4435.   \n359 [7] Patrik Hoyer, Dominik Janzing, Joris M Mooij, Jonas Peters, and Bernhard Sch\u00f6lkopf. Non  \n360 linear causal discovery with additive noise models. In D. Koller, D. Schuurmans, Y. Bengio,   \n361 and L. Bottou, editors, Advances in Neural Information Processing Systems, volume 21. Cur  \n362 ran Associates, Inc., 2008. URL https://proceedings.neurips.cc/paper/2008/file/   \n363 f7664060cc52bc6f3d620bcedc94a4b6-Paper.pdf.   \n364 [8] Kun Zhang and Aapo Hyv\u00e4rinen. On the identifiability of the post-nonlinear causal model. In   \n365 Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI \u201909,   \n366 page 647\u2013655, Arlington, Virginia, USA, 2009. AUAI Press. ISBN 9780974903958.   \n367 [9] Francesco Montagna, Atalanti Mastakouri, Elias Eulig, Nicoletta Noceti, Lorenzo Rosasco,   \n368 Dominik Janzing, Bryon Aragam, and Francesco Locatello. Assumption violations   \n369 in causal discovery and the robustness of score matching. In A. Oh, T. Neumann,   \n370 A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural   \n371 Information Processing Systems, volume 36, pages 47339\u201347378. Curran Associates,   \n372 Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/   \n373 93ed74938a54a73b5e4c52bbaf42ca8e-Paper-Conference.pdf.   \n374 [10] David Lopez-Paz, Krikamol Muandet, Bernhard Sch\u00f6lkopf, and Ilya Tolstikhin. Towards a   \n375 learning theory of cause-effect inference. In Proceedings of the 32nd International Conference   \n376 on International Conference on Machine Learning - Volume 37, ICML\u201915, page 1452\u20131461.   \n377 JMLR.org, 2015.   \n378 [11] Hebi Li, Qi Xiao, and Jin Tian. Supervised Whole DAG Causal Discovery, June 2020.   \n379 [12] Phillip Lippe, Taco Cohen, and Efstratios Gavves. Efficient neural causal discovery without   \n380 acyclicity constraints. In International Conference on Learning Representations, 2022. URL   \n381 https://openreview.net/forum?id=eYciPrLuUhG.   \n382 [13] Lars Lorch, Scott Sussex, Jonas Rothfuss, Andreas Krause, and Bernhard Sch\u00f6lkopf. Amortized   \n383 inference for causal structure learning. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and   \n384 Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. URL   \n385 https://openreview.net/forum?id=eV4JI-MMeX.   \n386 [14] Zoltan Szabo, Bharath Sriperumbudur, Barnabas Poczos, and Arthur Gretton. Learning theory   \n387 for distribution regression. Journal of Machine Learning Research, 17:1\u201340, 09 2016.   \n388 [15] Philippe Brouillard, S\u00e9bastien Lachapelle, Alexandre Lacoste, Simon Lacoste-Julien,   \n389 and Alexandre Drouin. Differentiable causal discovery from interventional data. In   \n390 H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neu  \n391 ral Information Processing Systems, volume 33, pages 21865\u201321877. Curran Associates,   \n392 Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/   \n393 f8b7aa3a0d349d9562b424160ad18612-Paper.pdf.   \n394 [16] Nan Rosemary Ke, Olexa Bilaniuk, Anirudh Goyal, Stefan Bauer, Hugo Larochelle, Bernhard   \n395 Sch\u00f6lkopf, Michael Curtis Mozer, Christopher Pal, and Yoshua Bengio. Neural causal structure   \n396 discovery from interventions. Transactions on Machine Learning Research, 2023. ISSN   \n397 2835-8856. URL https://openreview.net/forum?id $\\equiv$ rdHVPPVuXa. Expert Certification.   \n398 [17] Nino Scherrer, Olexa Bilaniuk, Yashas Annadani, Anirudh Goyal, Patrick Schwab, Bernhard   \n399 Sch\u00f6lkopf, Michael C. Mozer, Yoshua Bengio, Stefan Bauer, and Nan Rosemary Ke. Learning   \n400 neural causal models with active interventions, 2022.   \n401 [18] S\u00e9bastien Lachapelle, Philippe Brouillard, Tristan Deleu, and Simon Lacoste-Julien. Gradient  \n402 based neural dag learning. In International Conference on Learning Representations, 2020.   \n403 URL https://openreview.net/forum?id $\\cdot$ rklbKA4YDS.   \n404 [19] Ignavier Ng, AmirEmad Ghassami, and Kun Zhang. On the role of sparsity and dag constraints   \n405 for learning linear dags. In Proceedings of the 34th International Conference on Neural   \n406 Information Processing Systems, NIPS \u201920, Red Hook, NY, USA, 2020. Curran Associates Inc.   \n407 ISBN 9781713829546.   \n408 [20] Xun Zheng, Bryon Aragam, Pradeep Ravikumar, and Eric P. Xing. Dags with no tears:   \n409 Continuous optimization for structure learning. In Neural Information Processing Systems,   \n410 2018. URL https://api.semanticscholar.org/CorpusID:53217974.   \n411 [21] Zhen Zhang, Ignavier $\\mathrm{Ng}$ , Dong Gong, Yuhang Liu, Ehsan M Abbasnejad, Mingming Gong,   \n412 Kun Zhang, and Javen Qinfeng Shi. Truncated matrix power iteration for differentiable DAG   \n413 learning. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors,   \n414 Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/   \n415 forum?id $\\equiv$ I4aSjFR7jOm.   \n416 [22] Kevin Bello, Bryon Aragam, and Pradeep Kumar Ravikumar. DAGMA: Learning DAGs via   \n417 m-matrices and a log-determinant acyclicity characterization. In Alice H. Oh, Alekh Agarwal,   \n418 Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing   \n419 Systems, 2022. URL https://openreview.net/forum?id=8rZYMpFUgK.   \n420 [23] Jonas Peters, Joris M. Mooij, Dominik Janzing, and Bernhard Sch\u00f6lkopf. Causal discovery   \n421 with continuous additive noise models. J. Mach. Learn. Res., 15(1):2009\u20132053, jan 2014. ISSN   \n422 1532-4435.   \n423 [24] Sindy L\u00f6we, David Madras, Richard S. Zemel, and Max Welling. Amortized causal discovery:   \n424 Learning to infer causal graphs from time-series data. In CLEaR, 2020. URL https://api.   \n425 semanticscholar.org/CorpusID:219955853.   \n426 [25] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N   \n427 Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon,   \n428 U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, ed  \n429 itors, Advances in Neural Information Processing Systems, volume 30. Curran Associates,   \n430 Inc., 2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/file/   \n431 3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.   \n432 [26] Zhijing Jin, Yuen Chen, Felix Leeb, Luigi Gresele, Ojasv Kamal, Zhiheng Lyu, Kevin Blin,   \n433 Fernando Gonzalez Adauto, Max Kleiman-Weiner, Mrinmaya Sachan, et al. Cladder: A   \n434 benchmark to assess causal reasoning capabilities of language models. Advances in Neural   \n435 Information Processing Systems, 36, 2024.   \n436 [27] Jiaqi Zhang, Joel Jennings, Agrin Hilmkil, Nick Pawlowski, Cheng Zhang, and Chao Ma.   \n437 Towards causal foundation model: on duality between causal inference and attention, 2024.   \n438 [28] Meyer Scetbon, Joel Jennings, Agrin Hilmkil, Cheng Zhang, and Chao Ma. Fip: a fixed-point   \n439 approach for causal generative modeling, 2024.   \n440 [29] Shantanu Gupta, Cheng Zhang, and Agrin Hilmkil. Learned causal method prediction, 2023.   \n441 [30] Caroline Uhler, G. Raskutti, Peter B\u00fchlmann, and B. Yu. Geometry of the faithfulness assump  \n442 tion in causal inference. The Annals of Statistics, 41, 07 2012. doi: 10.1214/12-AOS1080.   \n443 [31] Robert Geirhos, J\u00f6rn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel,   \n444 Matthias Bethge, and Felix Wichmann. Shortcut learning in deep neural networks. Nature   \n445 Machine Intelligence, 2:665\u2013673, 11 2020. doi: 10.1038/s42256-020-00257-z.   \n446 [32] Alexander G. Reisach, Christof Seiler, and Sebastian Weichwald. Beware of the simulated dag!   \n447 causal discovery benchmarks may be easy to game. In Neural Information Processing Systems,   \n448 2021. URL https://api.semanticscholar.org/CorpusID:239998404.   \n449 [33] Francesco Montagna, Nicoletta Noceti, Lorenzo Rosasco, and Francesco Locatello. Shortcuts   \n450 for causal discovery of nonlinear models by score matching, 2023.   \n451 [34] Shohei Shimizu, Takanori Inazumi, Yasuhiro Sogawa, Aapo Hyvarinen, Yoshinobu Kawahara,   \n452 Takashi Washio, Patrik Hoyer, and Kenneth Bollen. DirectLiNGAM: A direct method for   \n453 learning a linear non-gaussian structural equation model. Journal of Machine Learning Research,   \n454 12, 01 2011.   \n455 [35] Francesco Montagna, Nicoletta Noceti, Lorenzo Rosasco, Kun Zhang, and Francesco Locatello.   \n456 Causal discovery with score matching on additive models with arbitrary noise. In 2nd Conference   \n457 on Causal Learning and Reasoning, 2023. URL https://openreview.net/forum?id $=$   \n458 rVO0Bx90deu.   \n459 [36] Peter B\u00fchlmann, Jonas Peters, and Jan Ernest. CAM: Causal additive models, high-dimensional   \n460 order search and penalized regression. The Annals of Statistics, 42(6), dec 2014. URL   \n461 https://doi.org/10.1214%2F14-aos1260.   \n462 [37] Jannik Kossen, Neil Band, Clare Lyle, Aidan Gomez, Tom Rainforth, and Yarin Gal. Self  \n463 attention between datapoints: Going beyond individual input-output pairs in deep learning.   \n464 In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, Advances in   \n465 Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id $=$   \n466 wRXzOa2z5T.   \n467 [38] Juan Lin. Factorizing multivariate function classes. In M. Jordan, M. Kearns, and   \n468 S. Solla, editors, Advances in Neural Information Processing Systems, volume 10. MIT   \n469 Press, 1997. URL https://proceedings.neurips.cc/paper_files/paper/1997/   \n470 file/8fb21ee7a2207526da55a679f0332de2-Paper.pdf. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "471 A Learning to induce: causal discovery with transformers ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "472 A.1 A supervised learning approach to causal discovery ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "473 First, we describe the training procedure for the CSIvA architecture, which aims to learn the dis  \n474 tribution of causal graphs conditioned on observational and/or interventional datasets. We omit   \n475 interventional datasets from the discussion as they are not of interest to our work. Training data are   \n476 generated from the joint distribution $p_{\\mathcal{G},\\mathcal{D}}$ between a graph $\\mathcal{G}$ and a dataset $\\mathcal{D}$ . First, we sample a set   \n477 of directed acyclic graphs $\\{\\mathcal{G}^{i}\\}_{i=1}^{n}$ with nodes $X_{1},\\ldots,X_{d}$ , from a distribution $p_{\\mathcal{G}}$ . Then, for each   \n478 graph we sample a dataset of $m$ observations of the graph nodes $\\mathcal{D}^{i}=\\{x_{1}^{j},\\dots,x_{d}^{j}\\}_{j=1}^{m}$ , $i=1,\\hdots,n$   \n479 Hence, we build a training dataset $\\{\\mathcal{G}^{i},\\mathcal{D}^{i}\\}_{i=1}^{n}$ . The CSIvA model defines a distribution $\\hat{p}_{\\mathcal{G}|D}(\\cdot;\\Theta)$ of graphs conditioned on the observational data and parametrized by $\\Theta$ . Given an invertible map $\\mathcal{G}\\mapsto A$ from a graph to its binary adjacency matrix ", "page_idx": 11}, {"type": "text", "text": "representation of $d\\times d$ entries (where $A_{i j}\\,=\\,1\\$ iff $X_{i}\\,\\to\\,X_{j}$ in $\\mathcal{G}$ ), we consider an equivalent estimated distribution $\\hat{p}_{A|D}(\\cdot;\\Theta)$ , which has the following autoregressive form: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\hat{p}_{A,\\mathcal{D}}(A|\\mathcal{D};\\Theta)=\\prod_{l=1}^{d^{2}}\\sigma(A_{l};\\rho=f_{\\Theta}(A_{1},\\ldots,A_{l-1},\\mathcal{D})),\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "480 where $\\sigma(\\cdot;\\rho)$ is a Bernoulli distribution parametrized by $\\rho,\\,\\rho$ itself is a function of $f_{\\Theta}$ defined by the   \n481 encoder-decoder transformer architecture, taking as input previous elements of the matrix $A$ (here   \n482 represented as a vector of $d^{2}$ entries) and the dataset $\\mathcal{D}$ . $\\Theta$ is optimized via maximum likelihood   \n483 estimation, i.e. $\\begin{array}{r}{\\Theta^{*}=\\mathrm{argmin}_{\\Theta}-\\mathbf{E}_{\\mathcal{G},\\mathcal{D}}[\\ln\\hat{p}(\\mathcal{G}|\\mathcal{D};\\Theta)]}\\end{array}$ , which corresponds to the usual cross-entropy   \n484 loss for the Bernoulli distribution. Training is achieved using stochastic gradient descent, in which   \n485 each gradient update is performed using a pair $(\\mathcal{D}^{i},A^{i})$ , $i=1\\ldots,d$ . In the infinite sample limit,   \n486 we have $\\hat{p}_{\\mathcal{G}|\\mathcal{D}}\\big(\\cdot;\\Theta^{*}\\big)=p_{\\mathcal{G}|\\mathcal{D}}\\big(\\cdot\\big)$ , while in the finite-capacity case, it is only an approximation of the   \n487 target distribution. ", "page_idx": 12}, {"type": "text", "text": "488 A.2 CSIvA architecture ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "489 In this section, we summarize the architecture of CSIvA, a transformer neural network that can learn   \n490 a map from data to causally interpreted graphs, under supervised training.   \n491 Transformer neural network. Transformers [25] are a popular neural network architecture for   \n492 modeling structured, sequential data data. They consist of an encoder, a stack of layers that learns   \n493 a representation of each element in the input sequence based on its relation with all the other   \n494 sequence\u2019s elements, through the mechanism of self-attention, and a decoder, which maps the learned   \n495 representation to the target of interest. Note that data for causal discovery are not sequential in their   \n496 nature, which motivates the adaptations introduced by Ke et al. [1] in their CSIvA architecture.   \n497 CSIvA embeddings. Each element $\\boldsymbol{x}_{i}^{j}$ of an input dataset is embedded into a vector of dimension  \n498 ality $E$ . Half of this vector is allocated to embed the value $\\boldsymbol{x}_{i}^{j}$ itself, while the other half is allocated   \n499 to embed the unique identity for the node $X_{i}$ . We use a node-specific embedding because the values   \n500 of each node may have very different interpretations and meanings. The node identity embedding   \n501 is obtained using a standard 1D transformer positional embedding over node indices. The value   \n502 embedding is obtained by passing $\\boldsymbol{x}_{i}^{j}$ , through a multi-layer perceptron (MLP).   \n503 CSIvA alternating attention. Similarly to the transformer\u2019s encoder, CSIvA stacks a number of   \n504 identical layers, performing self-attention followed by a nonlinear mapping, most commonly an   \n505 MLP layer. The main difference relative to the standard encoder is in the implementation of the   \n506 self-attention layer: as transformers are in their nature suitable for the representation of sequences,   \n507 given an input sample of $D$ elements, self-attention is usually run across all elements of the sequence.   \n508 However, data for causal discovery are tabular, rather than sequential: one option would be to unravel   \n509 the $n\\times d$ matrix of the data, where $n$ is the number of observations and $d$ the number of variables, into   \n510 a vector of $n\\cdot d$ elements, and let this be the input sequence of the encoder. CSIvA adopts a different   \n511 strategy: the self-attention in each encoder layer consists of alternate passes over the attribute and   \n512 the sample dimensions, known as alternating attention [37]. As a clarifying example, consider a   \n513 dataset $\\{(x_{1}^{i},x_{2}^{i})\\}_{i=1}^{n}$ of $n$ i.i.d. samples from the joint distribution of the pair of random variables   \n514 $X_{1},X_{2}$ . For each layer of the encoder, in the first step (known as attention between attributes),   \n515 attention operates across all nodes of a single sample $(\\dot{x}_{1}^{i},x_{2}^{i})$ to encode the relationships between   \n516 the two nodes. In the second step (attention between samples), attention operates across all samples   \n517 $(x_{k}^{1},\\ldots,x_{k}^{n}),k\\in\\{1,2\\}$ of a given node, to encode information about the distribution of single node   \n518 values.   \n519 CSIvA encoder summary. The encoder produces a summary vector $s_{i}$ with $H$ elements for each   \n520 node $X_{i}$ , which captures essential information about the node\u2019s behavior and its interactions with other   \n521 nodes. The summary representation is formed independently for each node and involves combining   \n522 information across the $n$ samples. This is achieved with a method often used with transformers that   \n523 involves a weighted average based on how informative each sample is. The weighting is obtained   \n524 using the embeddings of a summary \"sample\" $n+1$ to form queries, and embeddings of node\u2019s   \n525 samples $\\{x_{i}^{j}\\}_{j=1}^{n}$ to provide keys and values, and then using standard key-value attention.   \n526 CSIvA decoder. The decoder uses the summary information from the encoder to generate a   \n527 prediction of the adjacency matrix $A$ of the underlying $\\mathcal{G}$ . It operates sequentially, at each step   \n528 producing a binary output indicating the prediction $\\hat{A}_{i,j}$ of $A_{i,j}$ , proceeding row by row. The decoder   \n529 is an autoregressive transformer, meaning that each prediction $\\hat{A}_{i,j}$ is obtained based on all elements   \n530 of $A$ previously predicted, as well as the summary produced by the encoder. The method does not   \n531 enforce acyclicity, although Ke et al. [1] shows that in cyclic outputs genereally don\u2019t occur, in   \n532 practice. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "table", "img_path": "DSVGACQ3sO/tmp/64234a04f6ae4a235293340db4efb12211030207c2db5ecc31f9c5c6d14e4f22.jpg", "table_caption": ["Table 1: Hyperparameters for the training of the CSIvA models of the experiments in Section 3. "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "533 B Training details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "534 B.1 Hyperparameters ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "535 In Table 1 we detail the hyperparameters of the training of the network of the experiments. We define   \n536 an iteration as a gradient update over a batch of 5 datasets. Models are trained until convergence,   \n537 using a patience of 5 (training until five consecutive epochs without improvement) on the validation   \n538 loss - this always occurs before the 25-th epoch (corresponding to $\\approx150000$ iterations). The batch   \n539 size is limited to 5 due to memory constraints. ", "page_idx": 13}, {"type": "text", "text": "540 B.2 Synthetic data ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "541 In this section, we provide additional details on the synthetic data generation, which was performed   \n542 with the causally Python library [9]. Our data-generating framework follows that of Montagna   \n543 et al. [9], an extensive benchmark of causal discovery methods on different classes of SCMs.   \n544 Causal mechanisms. The nonlinear mechanisms of the PNL model and the nonlinear ANM model   \n545 are generated by a neural network with one hidden layer with 10 hidden units, with a parametric   \n546 ReLU activation function. The network weights are randomly sampled according to a standard   \n547 Gaussian distribution. The linear mechanisms are generated by sampling the regression coefficients   \n548 in the range $[-3,-0.5]\\cup[0.5,3]$ .   \n549 Distribution of the noise terms. We generated datasets from structural causal models with the   \n550 following distribution of the noise terms: Beta, Gamma, Gaussian (for nonlinear data), Gumbel,   \n551 Exponential, and Uniform. Additionally, we define the mlp distribution by nonlinear transformations   \n552 of gaussian samples from a guassian distribution centered at zero and with standard deviation $\\sigma$   \n553 uniformly sampled between 0.5 and 1. The nonlinear transformation is parametrized by a neural   \n554 network with one hidden layer with 100 units, and sigmoid activation function. The weights of the   \n555 network are uniformly sampled in the range $[-1.5,1.5]$ . We additionally standardized the output of   \n556 each mlp sample by the empirical variance computed over all samples.   \n557 Data are standardized with their empirical variance, which removes the presence of shortcuts which   \n558 could be learned by the network, notably varsortability [32] and score-sortability [33].   \n560 Our experiments were run on a local computing cluster, using any and all available GPUs (all   \n561 NVIDIA). For replication purposes, GTX 1080 Ti\u2019s are entirely suitable, as the batch size was set   \n562 to match their memory capacity, when working with bivariate graphs. All jobs ran with 10GB of   \n563 RAM and 4 CPU cores. The results presented in this paper were produced after 145 days of GPU   \n564 time, of which 68 were on GTX 1080 Ti\u2019s, 13 on RTX 2080 Ti\u2019s, 11 on A10s, 19 on A40s, and 35   \n565 on RTX 3090s. Together with previous experiments, while developing our code and experimental   \n566 design, we used 376 days of GPU time (for reference, at a total cost of 492.14 Euros), similarly split   \n567 across whichever GPUs were available at the time: 219 on GTX 1080 Ti\u2019s, 38 on RTX 2080 Ti\u2019s, 18   \n568 on A10s, 63 on RTX 3090s, 31 on A40s, and 6 on A100s. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "569 C Further experiments ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "570 We present our experimental results on one further question, to help clarify the results in the main text   \n571 of the paper. Our aim is to understand when to make tradeoffs between computational resources, and   \n572 having models that have been trained on a wider variety of SCMs. We compare training on multiple   \n573 SCMs to single-SCM training, when all models see the same amount of training data from each SCM   \n574 type as a non-mixed model (i.e. a mixed network trains on 15, 000 linear datasets and 15, 000 PNL   \n575 datasets, instead of 15, 000 divided between the two SCM types).   \n576 In the main text of this paper, we compare neural networks trained on a mix of structural causal   \n577 models (e.g. noise distributions, or mechanism types), to models trained on a single mechanism-noise   \n578 combination, where all models have the same amount of training data, 15, 000 datasets. In mixed   \n579 training, we split these evenly, so a \"lin, nl\" model is trained on 7, 500 datasets from linear SCMs, and   \n580 7, 500 from nonlinear SCMs. Our results in this framework are promising, and show that for many   \n581 combinations of SCM types, we can train one model instead of two, and achieve good progress, while   \n582 making a $50\\%$ savings on training costs. However, if our training budget is high/unlimited, we should   \n583 also ask whether there is a downside to mixed training - can we achieve the same performance as a   \n584 model trained on a single SCM type? Fig. 6 shows good results in this direction - the models trained   \n585 with the same number of datasets per SCM type as an unmixed model had similar (or even better,   \n586 for PNL data) performance as the un-mixed model trained on the same SCM type as the test data.   \n587 These mixed models are also significantly more useful than having 2 or 3 separate models per SCM   \n588 type, as they have good across-the-board performance. However, if we used the same computational   \n589 resources to train 3 separate networks (one for each mechanism type) and wanted to use them for   \n590 causal discovery on a dataset with unknown assumptions, we would be left with the rather difficult   \ntask of deciding which model to trust. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "image", "img_path": "DSVGACQ3sO/tmp/6e32c2b7e27bfec9cdcdf5e0472b9408600bf91893e97534186d0570d74745dc.jpg", "img_caption": ["Figure 6: Mixtures of causal mechanisms, with varying amounts of training data. We train eight models on samples from structural casual models with different mechanisms. Four (in purple), were trained on 15, 000 samples for each SCM type (so the \"lin,nl\" model saw 30, 000 samples in total, and the \"all\" model saw 45, 000), and the other four (blue) are the same as in Fig. 4, and were trained on 15, 000 samples in total, evenly split between the SCM types they were trained on. We compare their test SHD (the lower, the better) against networks trained on datasets generated according to a single type of mechanism. The dashed line indicates the test SHD of a model trained on samples with the same mechanisms as the test SCM. Training on multiple causal models with different mechanisms (mixed bars) always improves performance compared to training on single SCMs. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "592 D Theoretical results and proofs ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "593 Before stating the proof of Proposition 1, we show under which condition the pair of random   \n594 variables $X,Y$ satisfies the forward and backward models of equations (5), (6): this is relevant for   \n595 our discussion, as the proof of Proposition 1 consists of showing that this condition is almost never   \n596 satisfied.   \n597 Notation. We adopt the following notation: $\\nu_{X}:=\\log p_{N_{X}}$ , $\\nu_{Y}:=\\log p_{N_{Y}}$ , $\\xi:=\\log p_{\\tilde{X}},\\,\\eta:=$   \n598 log p Y\u02dc , and \u03c0 := log p X\u02dc, Y\u02dc .   \n599 Theorem 1 (Theorem 1 of Zhang and Hyv\u00e4rinen [8]). Assume that $X,Y$ satisfies both causal   \n600 relations of equations (5) and (6). Further, suppose that $p_{N_{Y}}$ and $p_{\\tilde{X}}$ are positive densities on the   \n601 support of $N_{Y}$ and $\\tilde{X}$ respectively, and that $\\nu_{Y},\\xi,f_{1},f_{2},g_{1}$ , and $g_{2}$ are third order differentiable.   \n602 Then, for each pair $(\\tilde{x},\\tilde{y})$ satisfying $\\nu_{_Y}^{\\prime\\prime}(\\tilde{y}-h_{_Y}(\\tilde{x}))h_{_Y}(\\tilde{x})\\neq0$ , the following differential equation   \n603 holds: ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "equation", "text": "$$\n\\xi^{\\prime\\prime\\prime}=\\xi^{\\prime\\prime}\\left(\\frac{h_{Y}^{\\prime\\prime}}{h_{Y}^{\\prime}}-\\frac{\\nu_{Y}^{\\prime\\prime\\prime}h_{Y}^{\\prime}}{\\nu_{Y}^{\\prime\\prime}}\\right)+\\frac{\\nu_{Y}^{\\prime\\prime\\prime}\\nu_{Y}^{\\prime}h_{Y}^{\\prime\\prime}h_{Y}^{\\prime}}{\\nu_{Y}^{\\prime\\prime}}-\\frac{\\nu_{Y}^{\\prime}(h_{Y}^{\\prime\\prime})^{2}}{h_{Y}^{\\prime}}-2\\nu_{Y}^{\\prime\\prime}h_{Y}^{\\prime\\prime}h_{Y}^{\\prime}+\\nu_{Y}^{\\prime}h_{Y}^{\\prime\\prime\\prime},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "604 and $h_{X}$ is constrained in the following way: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{1}{h_{X}^{\\prime}}=\\frac{\\xi^{\\prime\\prime}+\\nu_{Y}^{\\prime\\prime}(h_{Y}^{\\prime})^{2}-\\nu_{Y}^{\\prime}h_{Y}^{\\prime\\prime}}{\\nu_{Y}^{\\prime\\prime}h_{Y}^{\\prime}},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "605 where the arguments of the functions have been left out for clarity. ", "page_idx": 15}, {"type": "text", "text": "606 Proof of Theorem $^{\\,l}$ . We demonstrate separately the two statements of the theorem. ", "page_idx": 15}, {"type": "text", "text": "607 Part 1. Given that equations (5) and (6) hold, this implies that the forward and backward models   \n608 on $\\tilde{X},\\tilde{Y}$ of equations (7) and (8) are also valid, namely that: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{Y}=h_{Y}(\\tilde{X})+N_{Y},}\\\\ {\\tilde{X}=h_{X}(\\tilde{Y})+N_{X}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "These are the structural equations of two causal models, associated with the forward $\\tilde{X}\\rightarrow\\tilde{Y}$ and backward $\\tilde{Y}\\;\\rightarrow\\;\\tilde{X}$ graphs, respectively. Applying the Markov factorization of the distribution according to the forward direction, we get: ", "page_idx": 15}, {"type": "equation", "text": "$$\np_{\\tilde{x},\\tilde{Y}}(\\tilde{x},\\tilde{y})=p_{\\tilde{Y}\\mid\\tilde{x}}(\\tilde{y}|\\tilde{x})p_{\\tilde{x}}(\\tilde{x})=p_{N_{Y}}(\\tilde{y}-h_{Y}(\\tilde{x}))p_{\\tilde{x}}(\\tilde{x}),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "609 which implies ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\pi(\\tilde{x},\\tilde{y})=\\nu_{Y}\\big(\\tilde{y}-h_{Y}(\\tilde{x})\\big)+\\xi\\big(\\tilde{x}\\big),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "610 for any $\\tilde{x},\\tilde{y}$ . Similarly, the Markov factorization on the backward model implies: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\pi(\\tilde{x},\\tilde{y})=\\nu_{X}(\\tilde{x}-h_{X}(\\tilde{y}))+\\eta(\\tilde{y}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "611 From (11), we have that: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{\\partial^{2}}{\\partial\\tilde{x}^{2}}\\pi(\\tilde{x},\\tilde{y})=\\nu_{X}^{\\prime\\prime}(\\tilde{x}-h_{X}(\\tilde{y}))}\\\\ &{\\displaystyle\\frac{\\partial^{2}}{\\partial\\tilde{x}\\partial\\tilde{y}}\\pi(\\tilde{x},\\tilde{y})=-\\nu_{X}^{\\prime\\prime}(\\tilde{x}-h_{X}(\\tilde{y}))h_{X}^{\\prime}(\\tilde{y}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "612 which implies ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{\\partial}{\\partial\\tilde{x}}\\left(\\frac{\\partial^{2}}{\\partial\\tilde{x}^{2}}\\pi(\\tilde{x},\\tilde{y})\\right)=0.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "613 Computing the same set of partial derivatives from (10), we find: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{\\partial^{2}}{\\partial\\tilde{x}^{2}}\\pi(\\tilde{x},\\tilde{y})=\\nu_{Y}^{\\prime\\prime}(\\tilde{y}-h_{Y}(\\tilde{x}))(h_{Y}^{\\prime}(\\tilde{x}))^{2}-\\nu_{Y}^{\\prime}(\\tilde{y}-h_{Y}(\\tilde{x}))h_{Y}^{\\prime\\prime}(\\tilde{x})+\\xi^{\\prime\\prime}(\\tilde{x})}\\\\ &{\\displaystyle\\frac{\\partial^{2}}{\\partial\\tilde{x}\\partial\\tilde{y}}\\pi(\\tilde{x},\\tilde{y})=-\\nu_{Y}^{\\prime\\prime}(\\tilde{y}-h_{Y}(\\tilde{x}))h_{Y}^{\\prime}(\\tilde{x}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "614 from which follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{\\partial}{\\partial\\tilde{x}}\\left(\\frac{\\frac{\\partial^{2}}{\\partial\\tilde{x}^{2}}\\pi(\\tilde{x},\\tilde{y})}{\\frac{\\partial^{2}}{\\partial\\tilde{x}\\partial\\tilde{y}}\\pi(\\tilde{x},\\tilde{y})}\\right)=-2h_{Y}^{\\prime\\prime}+\\frac{\\nu_{Y}^{\\prime}h_{Y}^{\\prime\\prime\\prime}}{\\nu_{Y}^{\\prime\\prime}h_{Y}^{\\prime}}-\\frac{\\xi^{\\prime\\prime\\prime}}{\\nu_{Y}^{\\prime\\prime}h_{Y}^{\\prime}}+\\frac{\\nu_{Y}^{\\prime\\prime}\\nu_{Y}^{\\prime}h_{Y}^{\\prime\\prime}}{(\\nu_{Y}^{\\prime\\prime})^{2}}-\\frac{\\nu_{Y}^{\\prime}(h_{Y}^{\\prime\\prime})^{2}}{\\nu_{Y}^{\\prime\\prime}(h_{Y}^{\\prime})^{2}}+\\frac{\\xi^{\\prime\\prime}\\nu_{Y}^{\\prime\\prime\\prime}h_{Y}^{\\prime\\prime}}{(\\nu_{Y}^{\\prime\\prime})^{2}\\nu_{Y}^{\\prime\\prime}(h_{Y}^{\\prime})^{2}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "615 where we drop the input arguments for conciseness. The equality with 0 is given by the equality with   \n616 (12). Manipulating the above expression, the first claim follows.   \n617 Part 2. Next, we prove the constraint derived on $h_{X}$ . To do this, we exploit the fact that $\\tilde{Y}$ is   \n618 independent of $N_{X}$ , which implies the following condition [38]: ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{\\partial^{2}}{\\partial\\tilde{y}\\partial n_{x}}\\log p(\\tilde{y},n_{x})=0,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "619 for any $(\\tilde{y},n_{x})$ . According to equations (7), (8), we have that: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{Y}=h_{Y}(\\tilde{X})+N_{Y},}\\\\ {N_{X}=\\tilde{X}-h_{X}(\\tilde{Y}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "such that we can define an invertible map $\\Phi:(\\tilde{y},n_{x})\\mapsto(\\tilde{x},n_{Y})$ . It is easy to show that the Jacobian of the transformation has determinant $|J_{\\Phi}|=1$ , such that ", "page_idx": 16}, {"type": "equation", "text": "$$\np(\\tilde{y},n_{Y})=p(\\tilde{x},n_{Y}),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $(\\tilde{x},n_{Y})=\\Phi^{-1}(\\tilde{y},n_{X})$ . Thus, being $\\tilde{X},{N_{Y}}$ independent random variables, we have that: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\log p(\\tilde{y},n_{X})=\\log p(\\tilde{x})+\\log p(n_{Y})=\\xi(\\tilde{x})+\\nu_{Y}(n_{Y}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Given that $\\tilde{X}=h_{X}(\\tilde{Y})+N_{X}$ , we have that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{\\partial^{2}}{\\partial\\tilde{y}\\partial\\tilde{n}_{X}}\\log p(\\tilde{x})=\\xi^{\\prime\\prime}h_{X}^{\\prime},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "while $N_{Y}=\\tilde{Y}-h_{Y}(\\tilde{X})$ implies ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{\\partial^{2}}{\\partial\\tilde{y}\\partial\\tilde{n}_{X}}\\log p(n_{Y})=-\\nu_{Y}^{\\prime\\prime}h_{Y}^{\\prime}+\\nu_{Y}^{\\prime\\prime}h_{X}^{\\prime}(h_{Y}^{\\prime})^{2}-\\nu_{Y}^{\\prime}h_{X}^{\\prime}h_{Y}^{\\prime\\prime},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "such that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\log p(\\tilde{x},n_{Y})=\\xi^{\\prime\\prime}h_{X}^{\\prime}+-\\nu_{Y}^{\\prime\\prime}h_{Y}^{\\prime}+\\nu_{Y}^{\\prime\\prime}h_{X}^{\\prime}(h_{Y}^{\\prime})^{2}-\\nu_{Y}^{\\prime}h_{X}^{\\prime}h_{Y}^{\\prime\\prime},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which must be equal to zero, being equal to the LHS of (13). Thus, we conclude that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{1}{h_{X}^{\\prime}}=\\frac{\\xi^{\\prime\\prime}+\\nu_{Y}^{\\prime\\prime}(h_{Y}^{\\prime})^{2}-\\nu_{Y}^{\\prime}h_{Y}^{\\prime\\prime}}{\\nu_{Y}^{\\prime\\prime}h_{Y}^{\\prime}},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "620 proving the claim. ", "page_idx": 16}, {"type": "text", "text": "621 D.1 Proof of Proposition 1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "622 Proof. Under the hypothesis that equations (5), (6) hold, i.e. when the data generating process satisfy   \n623 both a forward and a backward model, by Theorem 1 we have that: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\xi^{\\prime\\prime\\prime}(\\tilde{x})=\\xi^{\\prime\\prime}(\\tilde{x})G(\\tilde{x},\\tilde{y})+H(\\tilde{x},\\tilde{y}),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "624 where ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{G(\\tilde{x},\\tilde{y})=\\left(\\frac{h_{Y}^{\\prime\\prime}}{h_{Y}^{\\prime}}-\\frac{\\nu_{{Y}}^{\\prime\\prime\\prime}h_{Y}^{\\prime}}{\\nu_{Y}^{\\prime\\prime}}\\right),}\\\\ &{H(\\tilde{x},\\tilde{y})=\\frac{\\nu_{{Y}}^{\\prime\\prime\\prime}\\nu_{{Y}}^{\\prime}h_{Y}^{\\prime\\prime}h_{Y}^{\\prime}}{\\nu_{{Y}}^{\\prime\\prime}}-\\frac{\\nu_{{Y}}^{\\prime}(h_{Y}^{\\prime\\prime})^{2}}{h_{Y}^{\\prime}}-2\\nu_{{Y}}^{\\prime\\prime}h_{Y}^{\\prime\\prime}h_{Y}^{\\prime}+\\nu_{Y}^{\\prime}h_{Y}^{\\prime\\prime\\prime}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "625 Define $z:=\\xi^{\\prime\\prime\\prime}$ , such that the above equation can be written as $z^{\\prime}(\\tilde{x})\\,=\\,z(\\tilde{x})G(\\tilde{x},\\tilde{y})+H(\\tilde{x},\\tilde{y})$ .   \n626 given that such function $z$ exists, it is given by: ", "page_idx": 17}, {"type": "equation", "text": "$$\nz(\\tilde{x})=z(\\tilde{x}_{0})e^{\\int_{\\tilde{x}_{0}}^{\\tilde{x}}G(t,y)d t}+\\int_{\\tilde{x}_{0}}^{\\tilde{x}}e^{\\int_{\\tilde{t}}^{\\tilde{x}}G(t,y)d t}H(\\hat{t},y)d\\hat{t}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Let $\\tilde{y}$ such that $\\nu_{Y}^{\\prime\\prime}(\\tilde{y}\\,-\\,h_{Y}(\\tilde{x}))h_{Y}^{\\prime}(\\tilde{x})\\,\\neq\\,0$ holds for all but countable values of $\\tilde{x}$ . Then, $z$ is determined by $z(\\bar{x}_{0})$ , as we can extend equation (15) to all the remaining points. The set of all functions $\\xi$ satisfying the differential equation (14) is a 3-dimensional affine space, as fixing $\\xi(\\tilde{x}_{0}),\\xi^{\\prime\\prime}(\\tilde{x}_{0}),\\xi^{\\prime\\prime}(\\tilde{x}_{0})$ for some point $\\scriptstyle{\\tilde{x}}_{0}$ completely determines the solution $\\xi$ . Moreover, given $\\nu_{Y},h_{X},h_{Y}$ fixed, $\\xi^{\\prime\\prime}$ is specified by (9) of theorem 1, which implies: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\xi^{\\prime\\prime}=\\frac{\\nu_{Y}^{\\prime\\prime}h_{Y}^{\\prime}}{h_{X}^{\\prime}}+\\nu_{Y}^{\\prime}h_{Y}^{\\prime\\prime}-\\nu_{Y}^{\\prime\\prime}(h_{Y}^{\\prime})^{2},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "627 which confines $\\xi$ solutions of (14) to a 2-dimensional affine space. ", "page_idx": 17}, {"type": "text", "text": "628 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "33 Justification: Supervised learning models in causal discovery do not provide connections   \n34 with the known identifiability theory. In the abstract, we present this open problem, and   \n35 highlight our main empirical findings and how they connect to the theory of identifiability in   \n36 causality. The content of the paper (mostly Section 3) unravels the abstract claims in all of   \n37 their details. ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 18}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: We discuss the limitations of our work in Section 1, paragraph \"Closely related works and their relation with CSIvA\", regarding the use of CSIvA as our only architecture for the experiments. Additionally, in the same paragraph, we remark that the scope of this study is limited to the context of causal discovery on observational data. Finally, in Section 2.2, we discuss our choice of limiting the empirical study to the case of bivariate graphs. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best ", "page_idx": 18}, {"type": "text", "text": "680 judgment and recognize that individual actions in favor of transparency play an impor  \n681 tant role in developing norms that preserve the integrity of the community. Reviewers   \n682 will be specifically instructed to not penalize honesty concerning limitations.   \n683 3. Theory Assumptions and Proofs   \n684 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n685 a complete (and correct) proof?   \n686 Answer: [Yes]   \n687 Justification: Proposition 1 is proved in detail in Appendix D.1, which is based on Theorem   \n688 1 of Zhang and Hyv\u00e4rinen [8], which we report in the Appendix together with its proof. We   \n689 do not provide an explicit sketch of the proof of our Proposition 1 in the main text, as we   \n690 already detail the intuition behind it in the content of Section 3.3.   \n691 Guidelines:   \n692 \u2022 The answer NA means that the paper does not include theoretical results.   \n693 \u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross  \n694 referenced.   \n695 \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n696 \u2022 The proofs can either appear in the main paper or the supplemental material, but if   \n697 they appear in the supplemental material, the authors are encouraged to provide a short   \n698 proof sketch to provide intuition.   \n699 \u2022 Inversely, any informal proof provided in the core of the paper should be complemented   \n700 by formal proofs provided in appendix or supplemental material.   \n701 \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced.   \n702 4. Experimental Result Reproducibility   \n703 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n704 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n705 of the paper (regardless of whether the code and data are provided or not)?   \n706 Answer: [Yes]   \n707 Justification: We have specified our data generation methods in Appendix B.2, as well   \n708 as the CSIvA method (which is a previously published model) in Appendix A, and our   \n709 hyperparameters for training in Appendix B.1. We will also release our implementation of   \n710 CSIvA, our data generation code (which is a thin wrapper around the causally https:   \n711 //causally.readthedocs.io/en/latest/ Python library), and our experimental code.   \n712 Guidelines:   \n713 \u2022 The answer NA means that the paper does not include experiments.   \n714 \u2022 If the paper includes experiments, a No answer to this question will not be perceived   \n715 well by the reviewers: Making the paper reproducible is important, regardless of   \n716 whether the code and data are provided or not.   \n717 \u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken   \n718 to make their results reproducible or verifiable.   \n719 \u2022 Depending on the contribution, reproducibility can be accomplished in various ways.   \n720 For example, if the contribution is a novel architecture, describing the architecture fully   \n721 might suffice, or if the contribution is a specific model and empirical evaluation, it may   \n722 be necessary to either make it possible for others to replicate the model with the same   \n723 dataset, or provide access to the model. In general. releasing code and data is often   \n724 one good way to accomplish this, but reproducibility can also be provided via detailed   \n725 instructions for how to replicate the results, access to a hosted model (e.g., in the case   \n726 of a large language model), releasing of a model checkpoint, or other means that are   \n727 appropriate to the research performed.   \n728 \u2022 While NeurIPS does not require releasing code, the conference does require all submis  \n729 sions to provide some reasonable avenue for reproducibility, which may depend on the   \n730 nature of the contribution. For example   \n731 (a) If the contribution is primarily a new algorithm, the paper should make it clear how   \n732 to reproduce that algorithm.   \n733 (b) If the contribution is primarily a new model architecture, the paper should describe   \n734 the architecture clearly and fully.   \n735 (c) If the contribution is a new model (e.g., a large language model), then there should   \n736 either be a way to access this model for reproducing the results or a way to reproduce   \n737 the model (e.g., with an open-source dataset or instructions for how to construct   \n738 the dataset).   \n739 (d) We recognize that reproducibility may be tricky in some cases, in which case   \n740 authors are welcome to describe the particular way they provide for reproducibility.   \n741 In the case of closed-source models, it may be that access to the model is limited in   \n742 some way (e.g., to registered users), but it should be possible for other researchers   \n743 to have some path to reproducing or verifying the results.   \n744 5. Open access to data and code   \n745 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n746 tions to faithfully reproduce the main experimental results, as described in supplemental   \n747 material?   \n748 Answer: [Yes]   \n749 Justification: We will release our implementation of CSIvA, our data generation code   \n750 (which is a thin wrapper around the causally https://causally.readthedocs.io/   \n751 en/latest/ Python library), and our experimental code.   \n752 Guidelines:   \n753 \u2022 The answer NA means that paper does not include experiments requiring code.   \n754 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n755 public/guides/CodeSubmissionPolicy) for more details.   \n756 \u2022 While we encourage the release of code and data, we understand that this might not be   \n757 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n758 including code, unless this is central to the contribution (e.g., for a new open-source   \n759 benchmark).   \n760 \u2022 The instructions should contain the exact command and environment needed to run to   \n761 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n762 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n763 \u2022 The authors should provide instructions on data access and preparation, including how   \n764 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n765 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n766 proposed method and baselines. If only a subset of experiments are reproducible, they   \n767 should state which ones are omitted from the script and why.   \n768 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n769 versions (if applicable).   \n770 \u2022 Providing as much information as possible in supplemental material (appended to the   \n771 paper) is recommended, but including URLs to data and code is permitted.   \n772 6. Experimental Setting/Details   \n773 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n774 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n775 results?   \n776 Answer: [Yes]   \n777 Justification: Yes, we provide these details in Section 3.1 and Appendix B.   \n778 Guidelines:   \n779 \u2022 The answer NA means that the paper does not include experiments.   \n780 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n781 that is necessary to appreciate the results and make sense of them.   \n782 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n783 material.   \n784 7. Experiment Statistical Significance   \n785 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n86 information about the statistical significance of the experiments?   \n87 Answer: [Yes]   \n88 Justification: For each plot, we provide error bars in the form of $95\\%$ confidence intervals   \n89 computed on $1.5\\mathrm{k}$ points (hence, it\u2019s reasonable to apply the central limit theorem to argue   \n90 that the confidence intervals are valid).   \n791 Guidelines:   \n92 \u2022 The answer NA means that the paper does not include experiments.   \n793 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n94 dence intervals, or statistical significance tests, at least for the experiments that support   \n795 the main claims of the paper.   \n796 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n97 example, train/test split, initialization, random drawing of some parameter, or overall   \n798 run with given experimental conditions).   \n99 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n800 call to a library function, bootstrap, etc.)   \n801 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n02 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n803 of the mean.   \n04 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n05 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n06 of Normality of errors is not verified.   \n07 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n08 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n09 error rates).   \n10 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n11 they were calculated and reference the corresponding figures or tables in the text.   \n12 8. Experiments Compute Resources   \n13 Question: For each experiment, does the paper provide sufficient information on the com  \n14 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n15 the experiments? ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: We provide all details on our computer resources in Appendix B.3. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 21}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 22}, {"type": "text", "text": "38 10. Broader Impacts   \n39 Question: Does the paper discuss both potential positive societal impacts and negative   \n40 societal impacts of the work performed?   \n41 Answer: [NA]   \n42 Justification: Our work is about assessing and studying pre-existing causal discovery models.   \n43 As we release no new model, there is no societal impact that could be caused by our work.   \n44 Guidelines:   \n45 \u2022 The answer NA means that there is no societal impact of the work performed.   \n46 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n47 impact or why the paper does not address societal impact.   \n48 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n49 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n850 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n851 groups), privacy considerations, and security considerations.   \n52 \u2022 The conference expects that many papers will be foundational research and not tied   \n53 to particular applications, let alone deployments. However, if there is a direct path to   \n854 any negative applications, the authors should point it out. For example, it is legitimate   \n855 to point out that an improvement in the quality of generative models could be used to   \n856 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n57 that a generic algorithm for optimizing neural networks could enable people to train   \n858 models that generate Deepfakes faster.   \n59 \u2022 The authors should consider possible harms that could arise when the technology is   \n60 being used as intended and functioning correctly, harms that could arise when the   \n61 technology is being used as intended but gives incorrect results, and harms following   \n62 from (intentional or unintentional) misuse of the technology.   \n63 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n64 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n65 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n66 feedback over time, improving the efficiency and accessibility of ML).   \n868 Question: Does the paper describe safeguards that have been put in place for responsible   \n869 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n870 image generators, or scraped datasets)?   \n871 Answer: [NA]   \n872 Justification: The data and models in this paper do not have high risk for misuse.   \n873 Guidelines:   \n874 \u2022 The answer NA means that the paper poses no such risks.   \n875 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n876 necessary safeguards to allow for controlled use of the model, for example by requiring   \n877 that users adhere to usage guidelines or restrictions to access the model or implementing   \n878 safety filters.   \n879 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n880 should describe how they avoided releasing unsafe images.   \n881 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n882 not require this, but we encourage authors to take this into account and make a best   \n883 faith effort.   \n884 12. Licenses for existing assets   \n885 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n886 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n887 properly respected?   \nAnswer: [Yes]   \n889 Justification: We cite the authors of all papers we build our work on. Additionally, we   \n890 provide the URL to all previously existing code we rely on, which is available in the form of   \n891 public GitHub repository under MIT license.   \n892 Guidelines:   \n893 \u2022 The answer NA means that the paper does not use existing assets.   \n894 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n895 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n896 URL.   \n897 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n898 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n899 service of that source should be provided.   \n900 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n901 package should be provided. For popular datasets, paperswithcode.com/datasets   \n902 has curated licenses for some datasets. Their licensing guide can help determine the   \n903 license of a dataset.   \n904 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n905 the derived asset (if it has changed) should be provided.   \n906 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n907 the asset\u2019s creators.   \n908 13. New Assets   \n909 Question: Are new assets introduced in the paper well documented and is the documentation   \n910 provided alongside the assets?   \n911 Answer: [Yes]   \n912 Justification: As our work is an analysis of pre-existing methods of causal discovery, we do   \n913 not release new assets other than the code strictly needed for reproducing our experimental   \n914 results. This code is attached to this submission to facilitate the reproducibility of our   \n915 results. All the documentation necessary for reproducing our results is provided in the main   \n916 manuscript.   \n917 Guidelines:   \n918 \u2022 The answer NA means that the paper does not release new assets.   \n919 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n920 submissions via structured templates. This includes details about training, license,   \n921 limitations, etc.   \n922 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n923 asset is used.   \n924 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n925 create an anonymized URL or include an anonymized zip file.   \n926 14. Crowdsourcing and Research with Human Subjects   \n927 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n928 include the full text of instructions given to participants and screenshots, if applicable, as   \n929 well as details about compensation (if any)?   \n930 Answer: [NA]   \n931 Justification: We do not work with human subjects or crowdsourcing.   \n932 Guidelines:   \n933 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n934 human subjects.   \n935 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n936 tion of the paper involves human subjects, then as much detail as possible should be   \n937 included in the main paper.   \n938 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n939 or other labor should be paid at least the minimum wage in the country of the data   \n940 collector.   \n941 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n942 Subjects   \n943 Question: Does the paper describe potential risks incurred by study participants, whether   \n944 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n945 approvals (or an equivalent approval/review based on the requirements of your country or   \n946 institution) were obtained?   \n947 Answer: [NA]   \n948 Justification: We do not work with human subjects or crowdsourcing.   \n949 Guidelines:   \n950 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n951 human subjects.   \n952 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)   \n953 may be required for any human subjects research. If you obtained IRB approval, you   \n954 should clearly state this in the paper.   \n955 \u2022 We recognize that the procedures for this may vary significantly between institutions   \n956 and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the   \n957 guidelines for their institution.   \n958 \u2022 For initial submissions, do not include any information that would break anonymity (if   \n959 applicable), such as the institution conducting the review. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}]