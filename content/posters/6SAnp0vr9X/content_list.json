[{"type": "text", "text": "Scale-invariant Optimal Sampling for Rare-events Data with Sparse Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jing Wang ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "HaiYing Wang ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Department of Statistics University of Connecticut Storrs, CT 06269 jing.7.wang@uconn.edu ", "page_idx": 0}, {"type": "text", "text": "Department of Statistics University of Connecticut Storrs, CT 06269 haiying.wang@uconn.edu ", "page_idx": 0}, {"type": "text", "text": "Hao Helen Zhang   \nDepartment of Mathematics University of Arizona   \nhzhang@math.arizona.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Subsampling is effective in tackling computational challenges for massive data with rare events. Overly aggressive subsampling may adversely affect estimation efficiency, and optimal subsampling is essential to mitigate the information loss. However, existing optimal subsampling probabilities depends on data scales, and some scaling transformations may result in inefficient subsamples. This problem is more significant when there are inactive features, because their influence on the subsampling probabilities can be arbitrarily magnified by inappropriate scaling transformations. We tackle this challenge and introduce a scale-invariant optimal subsampling function in the context of sparse models, where inactive features are commonly assumed. Instead of focusing on estimating model parameters, we define an optimal subsampling function to minimize the prediction error, using adaptive lasso as an example to outline the estimation procedure and study its theoretical guarantee. We first introduce the adaptive lasso estimator for rare-events data and establish its oracle properties, thereby validating the use of subsampling. Then we derive a scale-invariant optimal subsampling function that minimizes the prediction error of the inverse probability weighted (IPW) adaptive lasso. Finally, we present an estimator based on the maximum sampled conditional likelihood (MSCL) to further improve the estimation efficiency. We conduct numerical experiments using both simulated and real-world data sets to demonstrate the performance of the proposed methods. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Rare-events data refer to binary-response data that are highly imbalanced, i.e., the number of zeros (a.k.a \u201ccontrols\" or \u201cnegative instances\") are possibly hundreds or thousands of times as large as the number of ones (a.k.a. \u201ccases\" or \u201cpositive instances\"). This type of data is common in various fields, such as medicine, natural science, political science, and social science, where examples of rare events can be rare diseases, natural disasters, wars, and financial crises, respectively. Modern technologies also prompt us to pay more attention to rare-events data. For example, in modern online recommendation systems, clicks are usually rare events compared with nonclicks. Statistical analyses, including parameter estimation and inferences, pose unique challenges for rare-events data because of high imbalance. In addition, rare-events data often involve sparse models. For instance, rare diseases might be linked to a limited number of key genes. Therefore, researchers frequently adopt sparse models in genome-wide association studies for analyzing rare diseases. A different yet related example is the use of deep neural networks to predict click-through rates in modern online recommendation systems. These networks are typically overparameterized, necessitating methods that balance rare-events data with the sparsity of the underlying models. Data balancing is a popular approach to overcome challenges caused by imbalanced data and is usually accomplished through subsampling the zeros [5, 15] or oversampling the ones [3, 12, 16, 4]. In addition, rare-events data are often massive in order to obtain an adequate number of ones, and computation is demanding. Therefore, we focus on the subsampling approach since it addresses the imbalance issue and reduce the computational burden simultaneously. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "It is shown in [20] that the efficiency of parameter estimation is essentially determined by the number of ones for rare-events logistic regression, and subsampling does not reduce the estimation effciency as long as sufficient zeros are kept. In case of excessive removal of zeros, [22] developed an optimal sampling approach to minimize information loss. However, the optimal sampling probabilities in [22] are scale-dependent, which may lead to inefficient results. Figure 1 illustrates the issue using a simulated example, with details in Section D.1 of the appendix. We generate the data from the same logistic regression model and transform one of the covariates with different scales $s=0.01,0.1,1,10$ and 100. Then we apply two optimal subsampling methods in [22], labeled with \"A-OS\" and \"L-Os\" in Figure 1. It is observed that the prediction errors of A-OS and L-OS are significantly impacted by the data scaling. The A-OS may perform similarly to the Uni (simple random sampling or uniform sampling) in Figure 1a when $s\\,=\\,0.01$ ; so is the L-OS in Figure 1b when $s\\,=\\,100$ .This scaledependent issue is not specific to logistic regression and rare-events data in [22]; it is a wide concern in literature for various data types and models, including but not limited to [1, 29, 21, 14, 26, 25, 24]. In this paper, we propose a scale-invariant optimal subsampling method to overcome the issue. It is labeled \"P-OS\" in Figure 1. ", "page_idx": 1}, {"type": "image", "img_path": "6SAnp0vr9X/tmp/b3c44a1a7a6f7611b9c8906dc53f54a4491503af51b5e3dd6e50c10c67bb4cd5.jpg", "img_caption": ["Figure 1: Prediction errors with different scale transformation of the same model. (a): with non-sparse parameter $(-1,-1,-0.01,-0.01,-0.01,-0.01)^{\\mathrm{T}}$ . (b): with sparse parameter $(-1,0,0,0,0,\\mathbf{\\dot{0}})^{\\mathrm{T}}$ "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "The scale-dependence issue can seriously impact variable selection results for sparse models, where true parameters are zero for inactive covariates. In this case, inactive variables may be arbitrarily transformed without changing the underlying model, but the A-OS or L-OS would be highly influenced and may lead to misleading results. To resolve this issue, we investigate scale-invariant optimal subsampling in the context of variable selection, for which one main goal is to distinguish active and inactivefeatures. ", "page_idx": 1}, {"type": "text", "text": "Penalty-based feature selection methods are widely used. Specifically, the adaptive lasso is a popular choice due to its oracle properties, convexity, and practical ease of implementation [see 30, 28]. While penalization methods have been used for bias reduction in rare-events analysis [7], variable selection for rare-events data has not been investigated. Conducting effective variable selection is difficult in the context of rare-events data analysis, mainly due to the scarcity of information available for ones. An inaccurate variable selection result can subsequently impact both the effectiveness of optimal subsampling and the efficiency of parameter estimation. In this paper, we address the challenge of variable selection in the context of rare-events data. First, we propose the full data adaptive lasso and study its theoretical properties. Next, we introduce a novel subsampling estimator that seamlessly combines penalty-based variable selection and optimal sampling into one unified framework for rare-events data. The implementation of the adaptive lasso requires a pilot estimator to construct data-dependent weights for covariates. Given that optimal sampling also relies on pilot estimates [see 23, 1], the adaptive lasso emerges as a natural choice for conducting variable selection method in the context of subsampled rare-events data. We validate the new estimators by proving their oracle properties and also develop an efficient algorithm to facilitate their practical implementation when handling massive real-world data sets. In summary, our main contributions are listed as follows: ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "\u00b7 We propose scale-invariant optimal subsampling to enhance parameter estimation and variable selection. Existing optimal subsampling methods are scale-dependent, which may lead to unreliable or misleading results.   \n\u00b7 We define adaptive lasso and establish its oracle properties for rare-events data, which show that the asymptotic variances are determined by the number of ones in the data and the active features in the model.   \n\u00b7 We present a practical subsampling algorithm based on optimal probabilities that significantly reduces the computational burden and accelerates the optimization for penalty-based feature selection methods. ", "page_idx": 2}, {"type": "text", "text": "The rest of the paper is organized as follows. Section 2 introduces the model setup. Section 3 investigates nonuniform sampling and variable selection tailored for rare-events data. We propose new methods to construct scale-invariant optimal probabilities. Section 4 discusses theoretical properties of the MSCL estimator and presents a two-step algorithm to implement the proposed methods. Section 5 conducts numerical experiments on simulated and real data sets. Section 6 concludes the paper. Proofs and mathematical details are presented in the appendix. ", "page_idx": 2}, {"type": "text", "text": "2  Background and model setup ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We use the subscript $\\mathbf{t}$ to indicate the true parameters. For a $p$ -dimensional vector $\\textbf{\\em x}$ , we use $x_{(i)}$ to represent its $i$ -th element. For an index subset $\\mathcal{A}\\subset\\{i:1,2,...,p\\}$ , we use $\\pmb{x}_{(A)}$ to denote the subvector of $\\textbf{\\em x}$ , whose elements correspond to the indexes in $\\boldsymbol{\\mathcal{A}}$ . Furthermore, we use $\\pmb{x}^{\\otimes2}$ to denote $\\mathbf{\\deltax}^{\\mathrm{{T}}}$ ,use\u201c $\\xrightarrow{}^{,}$ ' to denote convergence in distribution, use \u201c $\\xrightarrow{\\;\\;P\\;}\\cdots$ to denote convergence in probability, and use $^{\\star}\\underline{{a.s}}_{\\dot{\\gamma}},,$ to denote convergence almost surely. We use $\\boldsymbol{\\mathit{I}}$ to denote an identity matrix of a suitable dimension and use O to denote a vector of zeros of a suitable dimension. ", "page_idx": 2}, {"type": "text", "text": "Let $({\\pmb x}_{1},y_{1}),({\\pmb x}_{2},y_{2}),...,({\\pmb x}_{N},y_{N})$ denote $N$ sample points from the joint distribution of $(\\pmb{x},y)$ where $\\{x_{i}\\}_{i=1}^{N}$ denote the $p$ -dimensional predictors and $\\{y_{i}\\}_{i=1}^{N}$ the binary responses. Assume that the probability of $y$ being a one ( $y=1$ ) given $\\textbf{\\em x}$ is ", "page_idx": 2}, {"type": "equation", "text": "$$\np(\\pmb{x};\\pmb{\\theta}_{\\mathrm{t}}):=\\mathbb{P}(y=1|\\pmb{x})=\\frac{e^{\\alpha_{\\mathrm{t}}+f(\\pmb{x};\\pmb{\\beta}_{\\mathrm{t}})}}{1+e^{\\alpha_{\\mathrm{t}}+f(\\pmb{x};\\pmb{\\beta}_{\\mathrm{t}})}}=\\frac{e^{g(\\pmb{x};\\pmb{\\theta}_{\\mathrm{t}})}}{1+e^{g(\\pmb{x};\\pmb{\\theta}_{\\mathrm{t}})}},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\pmb{\\theta}_{\\mathrm{t}}\\,=\\,(\\alpha_{\\mathrm{t}},\\beta_{\\mathrm{t}}^{\\mathrm{T}})^{\\mathrm{T}}$ is the vector of true parameters and $f(\\pmb{x};\\beta_{t})$ is a smooth function of $\\beta_{t}$ For rare-events data, $N_{1}\\ll N_{0}$ ,where $\\begin{array}{r}{N_{1}\\,=\\,\\sum_{i=1}^{N}y_{i}}\\end{array}$ is the number of ones (i.e. $y_{i}\\,=\\,1\\,$ and $N_{0}=N-N_{1}$ is the number of zeros (i.e. $y_{i}=0$ ). Following the model setup used in [22], we assume that $\\alpha_{\\mathrm{t}}\\rightarrow-\\infty$ as $N\\rightarrow\\infty$ , which implies that, under appropriate moment conditions, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\frac{N_{1}}{N_{0}}=\\frac{\\mathbb{E}\\{p(\\pmb{x};\\pmb{\\theta}_{\\mathrm{t}})\\}}{1-\\mathbb{E}\\{p(\\pmb{x};\\pmb{\\theta}_{\\mathrm{t}})\\}}+o(1)=\\mathbb{E}\\{e^{\\alpha_{\\mathrm{t}}+f(\\pmb{x};\\pmb{\\beta}_{\\mathrm{t}})}\\}+o(1)\\rightarrow0,\\ \\mathrm{almost\\;surely.}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Under this assumption, the asymptotic variance of the full data maximum likelihood estimator (MLE) is of order $1/N_{1}$ instead of $1/N$ , indicating that the estimation efficiency is determined by the number of rare ones. Therefore, we can keep all the ones and sample the zeros to save computational costs. There could be a variance inflation due to aggressive subsampling, and [22] developed optimal subsampling functions to reduce the variance inflation. Specifically, the authors proposed folows: non-uniform optimal sampling functions under the A- and $\\varphi_{\\mathrm{A-OS}}^{\\mathrm{scale}\\;^{\\star}}({\\pmb x})\\propto p(\\pmb{\\dot{x}};{\\pmb\\theta}_{\\mathrm{t}})\\lVert M^{-1}\\dot{\\pmb{g}}({\\pmb x};{\\pmb\\theta}_{\\mathrm{t}})\\rVert$ and $\\varphi_{\\mathrm{L-OS}}^{\\mathrm{scale}}(\\pmb{x})\\stackrel{-}{\\propto}p(\\pmb{x};\\pmb{\\theta}_{\\mathrm{t}})\\|\\dot{g}(\\pmb{x};\\pmb{\\theta}_{\\mathrm{t}})\\|$ $\\scriptstyle{\\mathrm{~L~}}$ -optimality criteria, respectively, as where $M=$ $\\mathbb{E}\\{e^{f(\\pmb{x};\\beta_{\\mathrm{t}})}\\dot{g}^{\\otimes2}(\\pmb{x};\\pmb{\\theta}_{\\mathrm{t}})\\}$ and $\\dot{g}(\\pmb{x};\\pmb{\\theta})$ denotes the derivative of $g(\\pmb{x};\\pmb{\\theta})$ with respect to $\\pmb{\\theta}$ However, the sampling functions $\\varphi_{\\mathrm{A-OS}}^{\\mathrm{scale}}(\\pmb{x})$ and $\\varphi_{\\mathrm{L-OS}}^{\\mathrm{scale}}(\\pmb{x})$ proposed in [22] depend on the scale of $\\textbf{\\em x}$ , and may not perform well for certain measurement scale of $\\textbf{\\em x}$ . For example, if $g(\\pmb{x};\\pmb{\\theta}_{\\mathrm{t}})=\\alpha_{\\mathrm{t}}+\\pmb{x}^{\\mathrm{T}}\\pmb{\\theta}_{\\mathrm{t}}$ , then $\\varphi_{\\mathrm{L-OS}}^{\\mathrm{scale}}(\\pmb{x})$ is proportional to $1+\\|x\\|$ , which will be influenced by the scale of $\\textbf{\\em x}$ . Similarly, scale changes in $\\textbf{\\em x}$ mayalsochange $\\varphi_{\\mathrm{A-OS}}^{\\mathrm{scale}}(\\pmb{x})$ , although the impact may not be in the same direction, as demonstrated in Figure 1. Besides parameter estimation, variable selection is another important topic, which has not been studied in the literature on rare-events data. This work aims to fill this gap. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "3  Nonuniform sampling with variable selection for rare-events data ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The adaptive lasso [30, 28] is a popular variable selection method because it has oracle properties and is easy to implement. We define the full data adaptive lasso for rare-events data as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{\\theta}_{\\mathrm{mle}}^{\\mathrm{adp}}:=\\arg\\operatorname*{max}\\left\\{\\sum_{i=1}^{N}[y_{i}g(\\pmb{x}_{i};\\pmb{\\theta})-\\log\\{1+e^{g(\\pmb{x}_{i};\\pmb{\\theta})}\\}]-\\lambda_{N}\\sum_{j=1}^{p}\\frac{|\\beta_{(j)}|}{|\\hat{\\beta}_{\\mathrm{pl}(j)}|\\gamma}\\right\\},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\lambda_{N}$ and $\\gamma$ are tuning parameters, and $\\hat{\\beta}_{\\mathrm{pl}}$ is a consistent pilot estimator of $\\beta_{t}$ . In practice, it is common to set $\\gamma=1$ . In the literature, iterative algorithms such as coordinate descent are commonly used to solve the adaptive lasso [8]. However, their computational demand can become prohibitive when dealing with massive data. It is feasible to alleviate the computational burden by subsampling zeros and create a smaller subset of data for adaptive lasso. To be specific, consider Algorithm 1. ", "page_idx": 3}, {"type": "table", "img_path": "6SAnp0vr9X/tmp/a77c8b4cc991a840038ce3fa0f372d349a77c4f0e874b43c057f82dcd0119fd7.jpg", "table_caption": [], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "The inclusion probability in Algorithm 1 for the $i$ thobservationis $\\pi(\\pmb{x}_{i},y_{i})=y_{i}+(1-y_{i})\\rho\\varphi(\\pmb{x}_{i})$ where $\\rho$ is the baseline sampling rate for the zeros and $\\varphi({\\pmb x})>0$ satisfies $\\mathbb{E}\\{\\varphi({\\pmb x})\\}=1$ . Let the subsample from Algorithm 1 be $\\{\\pmb{x}_{i}^{\\mathrm{sub}},{y}_{i}^{\\mathrm{sub}}\\}_{i=1}^{N_{\\mathrm{sub}}^{*}}$ . Which is biased since $\\pi(x_{i},y_{i})$ 's depend on the responses. We introduce an Inverse Probability Weighting (IPW) adaptive lasso estimator to correct for the bias, defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{\\theta}_{\\mathrm{w}}^{\\mathrm{adp}}:=\\arg\\operatorname*{max}\\left\\{\\sum_{i=1}^{N_{\\mathrm{sub}}^{*}}\\frac{[y_{i}^{\\mathrm{sub}}g(x_{i}^{\\mathrm{sub}};\\theta)-\\log\\{1+e^{g(x_{i}^{\\mathrm{sub}};\\theta)}\\}]}{\\pi(x_{i}^{\\mathrm{sub}},y_{i}^{\\mathrm{sub}})}-\\lambda_{N}\\sum_{j=1}^{p}\\frac{|\\beta_{(j)}|}{|\\hat{\\beta}_{\\mathrm{pl}(j)}|^{\\gamma}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "To save space, we put the general assumptions used throughout this paper in Section B.1 of the appendix. We use $\\boldsymbol{\\mathcal{A}}$ to denote the set of indexes of active variables, i.e., $A=\\{j:\\beta_{\\mathfrak{t}(j)}\\neq0\\}$ and $\\mathcal{A}^{c}$ to denote the set of indexes of inactive variables, i.e., $\\mathcal{A}^{c}=\\{j:\\beta_{\\mathrm{t}(j)}=0\\}$ . We first study the asymptotic properties of $\\hat{\\pmb{\\theta}}_{\\mathrm{w}}^{\\mathrm{adp}}$ in the following theorem. ", "page_idx": 3}, {"type": "text", "text": "Theorem 1. Let $\\hat{\\beta}_{\\mathrm{pl}}$ be a consistent pilot estimate such that $\\lambda_{N}/(\\sqrt{N_{1}}|\\hat{\\beta}_{\\mathrm{pl}(j)}|^{\\gamma})\\xrightarrow{P}\\infty.$ for $j\\in\\mathcal{A}^{c}$ Under Assumptions 1-4, if $\\dot{^{\\,}}\\lambda_{N}/\\sqrt{N_{1}}\\rightarrow0$ then the IPW adaptive lasso estimator defined in (3) has the following properties: ", "page_idx": 3}, {"type": "text", "text": "$\\hat{\\mathcal{A}}_{\\mathrm{w}}:=\\{j:\\hat{\\beta}_{\\mathrm{w}(j)}^{\\mathrm{adp}}\\neq0\\}$ $\\begin{array}{r}{\\operatorname*{lim}_{N\\to\\infty}\\mathbb{P}(\\hat{\\mathcal{A}}_{\\mathrm{w}}=\\mathcal{A})=1.}\\end{array}$ ", "page_idx": 3}, {"type": "text", "text": "2. Asymptotic normality: The estimator of the active parameter vector satisfes that ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\sqrt{N_{1}}V_{\\mathrm{w}(\\mathcal{A})}^{-1/2}(\\hat{\\theta}_{\\mathrm{w}(\\mathcal{A})}^{\\mathrm{adp}}-\\theta_{\\mathrm{t}(\\mathcal{A})})\\rightsquigarrow\\mathbb{N}(0,I),}&{}\\\\ {w h e r e\\emph{V}_{\\mathrm{w}(\\mathcal{A})}}&{=}&{\\mathbb{E}\\left\\{e^{f(x;\\beta_{\\mathrm{t}})}\\right\\}M_{(\\mathcal{A})}^{-1}M_{\\mathrm{w}(\\mathcal{A})}M_{(\\mathcal{A})}^{-1}\\ \\ =\\ \\ \\mathbb{E}\\left\\{e^{f(x;\\beta_{\\mathrm{t}})}\\right\\}\\left\\{M_{(\\mathcal{A})}^{-1}+c V_{\\mathrm{sub}(\\mathcal{A})}\\right\\},}\\\\ {M_{(\\mathcal{A})}}&{=}&{\\mathbb{E}\\left\\{e^{f(x;\\beta_{\\mathrm{t}})}\\dot{g}_{(\\mathcal{A})}^{\\otimes2}(x;\\theta_{\\mathrm{t}})\\right\\},\\ {V}_{\\mathrm{sub}(\\mathcal{A})}\\ =\\ \\ M_{(\\mathcal{A})}^{-1}\\mathbb{E}\\left\\{\\frac{e^{2f(x;\\beta_{\\mathrm{t}})}}{\\varphi(x)}\\dot{g}_{(\\mathcal{A})}^{\\otimes2}(x;\\theta_{\\mathrm{t}})\\right\\}M_{(\\mathcal{A})}^{-1},\\ \\ c=}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "$\\operatorname*{lim}_{N\\to\\infty}e^{\\alpha_{\\mathrm{t}}}/\\rho,$ and ${\\dot{g}}_{({\\mathcal{A}})}(x;\\theta_{\\mathrm{t}})$ consists of the elements of gradient vector $\\dot{g}(x;\\theta_{\\mathrm{t}})$ with indexesintheactiveset $A$ ", "page_idx": 4}, {"type": "text", "text": "Remark 1. Theorem $^{\\,I}$ shows that the etimaion efciency of $\\hat{\\theta}_{\\mathrm{w}(\\mathcal{A})}^{\\mathrm{adp}}$ is predominanty dtermined by the number of ones instead of the full data size. The term $c V_{\\mathrm{sub}(A)}$ is the variation inflation due to subsampling. The full data adaptive lasso in (2) correspond to the scenario with $\\rho=1$ and $\\varphi({\\pmb x})=1$ \uff0c for which $c=\\operatorname*{lim}_{N\\to\\infty}e^{\\alpha_{\\mathrm{t}}}/\\rho=0$ .Intuitively, c can be interpreted as the imbalance rate in the subsample. If we include sufficient zeros $\\left.\\tau\\right.=0$ ), the subsampling does not reduce the estimation efficiency of $\\hat{\\theta}_{\\mathrm{w}(\\mathcal{A})}^{\\mathrm{adp}}$ ", "page_idx": 4}, {"type": "text", "text": "From Theorem 1, we see that there maybe information loss reflected as an inflated variance if $c\\neq0$ . To minimize the information loss due to sampling, we derive optimal functions as follows, Where $\\varphi_{\\mathrm{A-OS}}^{\\mathrm{adp}}({\\pmb x})$ correspondsto the A-optimalitycriterion [17] and $\\varphi_{\\mathrm{L-OS}}^{\\mathrm{adp}}({\\pmb x})$ corresponds to the L-optimality criterion [17] in design of experiments. Here, the A-optimality minimizes the trace of theasymptoievarance of $\\hat{\\theta}_{\\mathrm{w}(\\mathcal{A})}^{\\mathrm{adp}}$   \ntransformed estimator $M_{(\\mathcal{A})}\\hat{\\theta}_{\\mathrm{w}(\\mathcal{A})}^{\\mathrm{adp}}$ whichispoportonal to $M_{\\mathrm{w}({\\cal A})}$ The A-optimality eriterion has a more direct interpretation, while an advantage of the -optimality criterion is that the resulting optimal function is often faster to calculate. ", "page_idx": 4}, {"type": "text", "text": "Proposition 1. The $A$ -optimal function that minimizes $t r(V_{\\mathrm{w}(A)})$ .s ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\varphi_{\\mathrm{A-OS}}^{\\mathrm{adp}}(\\pmb{x})=\\frac{p(\\pmb{x};\\pmb{\\theta}_{\\mathrm{t}})\\|\\pmb{M}_{(\\pmb{A})}^{-1}\\dot{g}_{(\\mathcal{A})}(\\pmb{x};\\pmb{\\theta}_{\\mathrm{t}})\\|}{\\mathbb{E}\\left\\{p(\\pmb{x};\\pmb{\\theta}_{\\mathrm{t}})\\|\\pmb{M}_{(\\mathcal{A})}^{-1}\\dot{g}_{(\\mathcal{A})}(\\pmb{x};\\pmb{\\theta}_{\\mathrm{t}})\\right\\}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The $L$ -optimal function that minimizes $t r(M_{\\mathrm{w}(\\mathcal{A})})$ .s ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\varphi_{\\mathrm{L-OS}}^{\\mathrm{adp}}(\\pmb{x})=\\frac{p(\\pmb{x};\\pmb{\\theta}_{\\mathrm{t}})\\|\\dot{g}_{(A)}(\\pmb{x};\\pmb{\\theta}_{\\mathrm{t}})\\|}{\\mathbb{E}\\left\\{p(\\pmb{x};\\pmb{\\theta}_{\\mathrm{t}})\\|\\dot{g}_{(A)}(\\pmb{x};\\pmb{\\theta}_{\\mathrm{t}})\\right\\}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Unl $\\varphi_{\\mathrm{A-OS}}^{\\mathrm{adp}}({\\pmb x})$ $\\varphi_{\\mathrm{L-OS}}^{\\mathrm{adp}}(\\pmb{x}))$ relies only onthe etive variables. This implies that a first-step pilot estimator given by the adaptive lasso algorithm can benefit from sparse estimation methods when calculating optimal probabilities. For example, employing the standard lasso can effectively eliminate a large number of inactive variables to facilitate the computation of optimal adp 1pa-pos(a) and g $\\varphi_{\\mathrm{L-OS}}^{\\mathrm{adp}}({\\pmb x})$ . However, in practice, pilot estimators are often obtained from a small subsample size, introducing additional uncertainty. Therefore, it becomes crucial to exercise caution and be conservative by over-selecting variables during the first step to prevent the exclusion of important variables. As a consequence, although theoretically $\\varphi_{\\mathrm{A-OS}}^{\\mathrm{adp}}(\\mathbf{x})$ $\\varphi_{\\mathrm{L-OS}}^{\\mathrm{adp}}({\\pmb x})$ ", "page_idx": 4}, {"type": "text", "text": "3.1  Scale invariant optimal function ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "As discussed in Section 1, scaling dependent optimal probabilities may impact the performance of variable selection in practice. To address the issue, we propose to construct a scale invariant optimal function by focusing on the prediction error of an estimator $\\hat{\\pmb\\theta}$ ,definedbelow. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{MSPE}(\\hat{\\pmb\\theta})=\\mathbb{E}_{\\mathbf{x}}\\left[\\left\\{p(\\mathbf{x};\\hat{\\pmb\\theta})-p(\\mathbf{x};\\pmb\\theta_{\\mathrm{t}})\\right\\}^{2}\\right]=\\int\\left\\{p(\\mathbf{x};\\hat{\\pmb\\theta})-p(\\mathbf{x};\\pmb\\theta_{\\mathrm{t}})\\right\\}^{2}\\mathrm{d}\\mathbb{P}_{\\mathbf{x}},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathbb{P}_{x}$ is the probability measure of $\\textbf{\\em x}$ . The probability term $p(x;\\theta_{\\mathrm{t}})$ involves both the covariates $\\textbf{\\em x}$ and the parameter vector $\\theta_{\\mathrm{t}}$ , and it often does not depend on the scale of $\\textbf{\\em x}$ . For example, in the logisticressionmdel, thva $p(x;\\theta_{\\mathrm{t}})$ is only related to $x^{\\mathrm{T}}\\beta_{\\mathrm{t}}$ If we change the scale of $x_{(j)}$ \uff0c the value of $\\theta_{\\mathrm{t}}$ would change accordingly under the same data-generating model and so $p(x;\\theta_{\\mathrm{t}})$ remains the same. Thus, re-scaling covariates would not affect this criterion. In the following, we give an optimal function that minimizes the prediction error. ", "page_idx": 4}, {"type": "text", "text": "Theorem 2. Under the assumptions of Theorem $^{\\,l}$ for the IPW adaptive lasso estimator defined in (3), its prediction error satisfies ", "page_idx": 4}, {"type": "equation", "text": "$$\nN_{1}e^{-2\\alpha_{\\mathrm{t}}}\\mathrm{MSPE}(\\hat{\\theta}_{\\mathrm{w}(A)}^{\\mathrm{adp}})\\mapsto\\mathbb{E}^{-1}\\left\\{e^{f(\\mathbf{x};\\beta_{\\mathrm{t}})}\\right\\}Z_{(A)}^{\\mathrm{T}}M_{\\mathrm{w}(A)}^{1/2}M_{(A)}^{-1}\\Omega_{(A)}M_{(A)}^{-1}M_{\\mathrm{w}(A)}^{1/2}Z_{(A)}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Wwhere $\\boldsymbol{Z}_{(A)}\\sim\\mathbb{N}(\\mathbf{0},\\boldsymbol{I})$ $\\Omega_{(A)}=\\mathbb{E}\\left[e^{2f(\\mathbf{x};\\beta_{\\mathrm{t}})}\\dot{g}_{(A)}^{\\otimes2}(\\mathbf{x},\\pmb{\\theta}_{\\mathrm{t}})\\right]$ the asymptotic mean of the prediction error in (6) is given as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\varphi_{\\mathrm{P-OS}}^{\\mathrm{adp}}({\\pmb x})=\\frac{p({\\pmb x};{\\pmb\\theta}_{\\mathrm{t}})\\|\\Omega_{({\\cal A})}^{\\frac{1}{2}}M_{({\\cal A})}^{-1}{\\dot{g}}_{({\\cal A})}({\\pmb x};{\\pmb\\theta}_{\\mathrm{t}})\\|}{\\mathbb{E}\\left[p({\\pmb x};{\\pmb\\theta}_{\\mathrm{t}})\\|\\Omega_{({\\cal A})}^{\\frac{1}{2}}M_{({\\cal A})}^{-1}{\\dot{g}}_{({\\cal A})}({\\pmb x};{\\pmb\\theta}_{\\mathrm{t}})\\|\\right]}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We refer this prediction oriented criterion as $\\mathbf{P}$ -optimality criterion. As we expect, the optimal function in (7) is unaffected by the scale of $\\textbf{\\em x}$ for a class of functions $g$ . The following proposition proves that $\\varphi_{\\mathrm{P-OS}}^{\\mathrm{adp}}(x)$ is invariant to rescaling of $\\textbf{\\em x}$ ", "page_idx": 5}, {"type": "text", "text": "Proposition 2. If $g(\\pmb{x};\\pmb{\\theta})$ satisfies that for every non-singular matrix $\\pmb{A}$ there exists a non-singular matrix $_B$ suchthat ", "page_idx": 5}, {"type": "equation", "text": "$$\ng(A x;B^{\\mathrm{T}}\\theta)=g(x;\\pmb\\theta),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "then, $\\varphi_{\\mathrm{P-OS}}^{\\mathrm{adp}}(\\mathbf{\\boldsymbol{x}})$ is invariant to scale changes of $\\textbf{\\em x}$ ", "page_idx": 5}, {"type": "text", "text": "Remark 2. The condition in (8) is not restrictive and it is quite easy to satisfy. One simple example of $g(x;\\pmb\\theta)$ thatsatisfiestheconditionisalinearfunction $g(\\mathbf{\\bar{x}};\\theta)=\\mathrm{\\bar{\\alpha}}+\\mathbf{x}^{\\mathrm{T}}\\mathrm{\\bar{\\beta}}$ whichcorrespondsto the logistic regression. The condition is also satisfied by more complex models. For example, consider an $L$ -layerneuralnetwork ", "page_idx": 5}, {"type": "equation", "text": "$$\ng(\\boldsymbol{x};\\boldsymbol{W}^{1},\\boldsymbol{W}^{2},...,\\boldsymbol{W}^{L},\\boldsymbol{b}^{1},...,\\boldsymbol{b}^{L})=f^{L}\\big(f^{L-1}(...f^{1}(\\boldsymbol{x}^{\\mathrm{T}}\\boldsymbol{W}^{1}+\\boldsymbol{b}^{1}))^{\\mathrm{T}}\\boldsymbol{W}+\\boldsymbol{b}^{L}\\big),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $W^{l}$ are the weights and $b^{l}$ are the biases in each layer, $l=1,2,...,L.\\mathit{I f}$ $\\textbf{\\em x}$ is rescaled to $_{A x}$ we can change $W^{1}$ to $(A^{T})^{-1}W^{1}$ so that the value of g does not change. That is ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{g(A x;(A^{T})^{-1}W^{1},W^{2},...,W^{L},b^{1},...,b^{L})=f^{L}(f^{L-1}(...f^{1}(x^{\\mathrm{T}}W^{1}+b^{1}))^{\\mathrm{T}}W+b^{L})}\\\\ &{\\ =g(x;W^{1},W^{2},...,W^{L},b^{1},...,b^{L}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "4 Penalized MSCL estimator ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The IPW estimator in (3) is not the most efficient estimator, because it assigns smaller weights for more informative data points with larger sampling probabilities. To improve the estimation efficiency, we propose the penalized MSCL estimator for variable selection given as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\hat{\\theta}_{\\mathrm{mscl}}^{\\mathrm{adp}}:=\\arg\\operatorname*{max}_{\\theta}\\left\\{\\sum_{i=1}^{N_{\\mathrm{sub}}^{*}}[y_{i}^{\\mathrm{sub}}g(x_{i}^{\\mathrm{sub}};\\theta)-\\log\\{1+e^{g(x_{i}^{\\mathrm{sub}};\\theta)+l_{i}^{\\mathrm{sub}}}\\}]-\\lambda_{N}\\sum_{j=1}^{p}\\frac{|\\beta_{(j)}|}{|\\hat{\\beta}_{\\mathrm{pl}(j)}|^{\\gamma}}\\right\\},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Where $l_{i}^{\\mathrm{sub}}=-\\log\\left\\{\\rho\\varphi(\\pmb{x}_{i}^{\\mathrm{sub}})\\right\\}$ The MSCL estimatorinroduced in [2 isdefined as the minimizer of the objective function in (9), excluding the penalization term. In this paper, we extend this approach by proposing a penalized MSCL estimator to ensure model sparsity. We present the oracle properties of the penalized MSCL estimator in the following theorem. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3. Let $\\hat{\\beta}_{\\mathrm{pl}}$ be a consistent pilot estimate such that $\\lambda_{N}/(\\sqrt{N_{1}}|\\hat{\\beta}_{\\mathrm{pl}(j)}|^{\\gamma})\\xrightarrow{P}\\infty$ for $j\\in\\mathcal{A}^{c}$ Under Assumptions 1-3 and $^{5}$ $i f\\lambda_{N}/\\sqrt{N_{1}}\\rightarrow0,$ the estimator based on MSCL function with adaptive lasso penalty defined in (9) have the following properties: ", "page_idx": 5}, {"type": "text", "text": "1. Consistency in variable selection: The estimated active set $\\hat{\\mathcal{A}}_{\\mathrm{mscl}}:=\\{j:\\hat{\\beta}_{\\mathrm{mscl}(j)}^{\\mathrm{adp}}\\neq0\\}$ Bimscl(i)#O}satisfies that $\\begin{array}{r}{\\operatorname*{lim}_{N\\to\\infty}\\mathbb{P}(\\hat{\\mathcal{A}}_{\\mathrm{mscl}}=\\mathcal{A})=1}\\end{array}$ ", "page_idx": 5}, {"type": "text", "text": "2. Asymptotic normality: The estimator of the active parameter vector satisfies that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\sqrt{N_{1}}V_{\\mathrm{mscl}(A)}^{-1/2}(\\hat{\\pmb{\\theta}}_{\\mathrm{mscl}(A)}^{\\mathrm{adp}}-\\pmb{\\theta}_{\\mathrm{t}(A)})\\rightsquigarrow\\mathbb{N}(\\mathbf{0},\\pmb{I}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{V_{\\mathrm{mscl}(A)}=\\mathbb{E}\\left\\{e^{f({\\bf x};\\beta_{\\mathrm{t}})}\\right\\}\\Lambda_{\\mathrm{mscl}(A)}^{-1}\\,a n d\\,\\Lambda_{\\mathrm{mscl}(A)}=\\mathbb{E}\\left[\\frac{e^{f({\\bf x};\\beta_{\\mathrm{t}})}{\\dot{g}}_{(A)}^{\\otimes2}({\\bf x};\\beta_{\\mathrm{t}})}{1+c\\varphi^{-1}({\\bf x})e^{f({\\bf x};\\beta_{\\mathrm{t}})}}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The penalized MSCL estimator has the same asymptotic variance as the MSCL estimator under the true model, indicating that it is more efficient than the penalized IPW estimator [22]. We prove this by comparing the asymptotic variances and present the result in the following theorem. ", "page_idx": 5}, {"type": "text", "text": "$V_{\\mathrm{w}(A)}\\,f\\!o r\\,\\hat{\\theta}_{\\mathrm{w}(A)}^{\\mathrm{adp}}$ $V_{\\mathrm{mscl}(A)}$ $\\hat{\\theta}_{\\mathrm{mscl}(A)}^{\\mathrm{adp}}$ finite, i.e. $0<V_{\\mathrm{w}(A)}$ \uff0c $V_{\\mathrm{mscl}(A)}<\\infty_{:}$ then $V_{\\mathrm{mscl}(A)}\\leq V_{\\mathrm{w}(A)}$ where the inequalities hold in the sense of Loewner ordering. ", "page_idx": 6}, {"type": "text", "text": "Thus, we give a practical two-step algorithm based on the penalized MSCL estimator. Since the optimal sampling functions contain unknown values and the adaptive lasso penalty also requires a consistent pilot estimator to build weights, it is natural to combine optimal sampling and the adaptive lasso into one unified framework. We recommend to use the lasso for pilot estimation. One reason is that it does estimation and variable selection simultaneously, and excluding some inactive variables improves the estimation accuracy of optimal probabilities. This also reduces the computational burden for subsequent steps. Another reason is that the lasso estimator tends to include more variables in practice and therefore has a low risk of excluding important variables in the pilot step. We present an outline of the practical implementation in Algorithm 2. More details are given in Section C. ", "page_idx": 6}, {"type": "text", "text": "Algorithm 2 Two-step subsampling adaptive lasso algorithm ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "1: First stage screening: \u00b7 Take a pilot sample of expected sample size $N_{\\mathrm{pl}}$ using $\\{\\pi(y_{i})=\\rho_{0}+y_{i}(\\rho_{1}-\\rho_{0})\\}_{i=1}^{N}$ and obtain a lasso penalized MSCL pilot estimator and an estimated active set $\\hat{A}_{\\mathrm{pl}}$ \u00b7 Calculate approximate optimal sampling probabilities $\\{\\hat{\\pi}(x_{i},y_{i})\\;\\;=\\;\\;y_{i}\\;+\\;\\hat{\\mathrm{(1~-~}}$ $y_{i})\\rho\\hat{\\varphi}(x_{i})\\}_{i=1}^{\\bar{N}^{\\mathrm{~}}}$ based on (4), (5), or (7).   \n2: Second stage screening: Use Algorithm 1 with the estimated optimal sampling probabilities to obtain a subsample of expected sample size $N_{\\mathrm{sub}}$ and compute the adaptive lasso penalized MSCL estimator based on $\\bar{\\hat{A}}_{\\mathrm{pl}}$ ", "page_idx": 6}, {"type": "text", "text": "5  Numerical experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we use numerical experiments on both simulated and real data to investigate the performances of our proposed optimal subsampling and variable selection procedures. ", "page_idx": 6}, {"type": "text", "text": "5.1  Simulation design ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We consider a logistic regression with $g(\\pmb{x};\\pmb{\\theta})=\\alpha+\\pmb{x}^{\\mathrm{T}}\\beta$ and the following three true parameters $\\beta_{\\mathrm{t}}$ of dimension 50. We set different $\\alpha_{\\mathrm{t}}$ so that the proportion of ones is 0.005: ", "page_idx": 6}, {"type": "text", "text": "(1) Case A: $\\beta_{\\mathrm{t}}=(0.75,0.75,\\mathbf{0}_{7}^{\\mathrm{T}},0.75,0,0.75,0.75,\\mathbf{0}_{37}^{\\mathrm{T}})^{\\mathrm{T}}$ and $\\alpha_{\\mathrm{t}}=-5.8$ (2) Case B: $\\beta_{\\mathrm{t}}=(3,-2,\\mathbf{0}_{7}^{\\mathrm{T}},0.85,0,-0.75,\\mathbf{0}_{38}^{\\mathrm{T}})^{\\mathrm{T}}$ and $\\alpha_{\\mathrm{t}}=-6.2$ (3) Case C: $\\beta_{\\mathrm{t}}=(3,2,{\\bf0}_{7}^{\\mathrm{T}},0.85,{\\bf0}_{40}^{\\mathrm{T}})^{\\mathrm{T}}$ and $\\alpha_{\\mathrm{t}}=-7.5$ ", "page_idx": 6}, {"type": "text", "text": "Here, $\\mathbf{0}_{d}$ denotes the zero vector of dimension $d$ .We use $p_{\\mathcal{A}}$ and $p_{\\mathcal{A}^{c}}$ to denote the number of active and inactive variables, respectively, and assume that $\\textbf{\\em x}$ is a normal random vector. The active components $x_{(\\mathcal{A},j)}$ \uff0c $1\\,\\leq\\,j\\,\\leq\\,p_{A}$ of $\\textbf{\\em x}$ have variances 0.25 and the inactive components $x_{(\\mathcal{A}^{c},j)}$ \uff0c $1\\,\\le\\,j\\,\\le\\,p_{\\mathcal{A}^{c}}$ of $\\textbf{\\em x}$ have variances $100/p_{A^{c}}^{3},100/(p_{A^{c}}-1)^{3},...,100/3^{3},100/2^{3},100/1^{3}$ The correlation between the $i$ -th and $j$ -th elements of $\\textbf{\\em x}$ is $0.5^{|i-j|},1\\,\\leq\\,i,j\\,\\leq\\,p$ . We repeat our experiments $S=500$ times generating $N=500000$ data points in each run and use a pilot sample of size $N_{\\mathrm{pl}}=500$ for obtaining pilot estimates based on the lasso. We consider uniform sampling, the full data lasso, and the full data adaptive lasso for comparison. We use the 5-fold cross-validation and Bayesian information criterion (BIC) to determine the tuning parameter $\\lambda$ for the lasso and the adaptive lasso, and choose $\\gamma=1$ for the adaptive lasso. ", "page_idx": 6}, {"type": "text", "text": "5.1.1  Estimation and prediction efficiency ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We present the empirical median squared error (eMSE) for parameter estimation in Figure 2. All optimal sampling estimators outperform the uniform sampling. As the sampling rate increases, sampling estimators outperform the full data lasso estimator eventually in all of the three cases. Among the three optimal subsampling methods, $\\hat{\\beta}_{\\mathrm{P-OS}}^{\\mathrm{adp}}$ performs better than the other two subsampling methods. ", "page_idx": 6}, {"type": "image", "img_path": "6SAnp0vr9X/tmp/db415cc52bf83914537a3bba7a809c093208b041f5fefd4e71d60855077669ae.jpg", "img_caption": ["Figure 2: eMSE for different true parameters with different sampling rates. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "6SAnp0vr9X/tmp/63c6b0e60f7b83dd5a5f8ca675291e0d3fbbc29e230ddf93113cc13ca803f620.jpg", "img_caption": ["Figure 3: eMPSE of estimated probability with different sampling rates. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 3 shows the results of the empirical median squared prediction error (eMSPE). Similarly to the results of eMSE, optimal sampling estimators perform better than the uniform sampling, meaning optimal sampling results in less information loss. It is possible that sampling estimators outperform the full data lasso estimator as the sampling rate increases, despite that the latter uses all of the data. In general, $\\hat{\\beta}_{\\mathrm{P-OS}}^{\\mathrm{adp}}$ performs the best among the tree optimal subsampling algorithms. ", "page_idx": 7}, {"type": "text", "text": "5.1.2   Variable selection and computational complexity ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we discuss the results of variable selection in terms of the first stage screening and the second stage screening. Table 1 presents the mean numbers of selected variables in Case C, where the numbers in the parentheses are the corresponding standard errors. Results for Cases A and B are similar so are put in Table 4 of the appendix. ", "page_idx": 7}, {"type": "table", "img_path": "6SAnp0vr9X/tmp/ed8cddfbbc8941299209ea41d1f2176962c52111096272322bfff5948c28dce2.jpg", "table_caption": ["Table 1: Mean number of selected variables in Case C "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "While the first stage screening significantly reduces the dimension in Table 1, it indeed includes inactive variables as expected. In the second stage screening, the mean numbers of selected variables are close to the true numbers of active variables for all subsampling methods. However, the mean number of selected variables from uniform sampling is smaller than the true number of active variables especially when the sampling rate is low. This indicates that the second-stage screening of uniform sampling may exclude active variable. We present the rates of missing active variables in Table 2 for Case C. It shows that uniform sampling has higher rates of excluding active variables than optimal subsampling procedures, so optimal sampling may be preferable in practice. Results for Cases A and B are similar and are put in Section E.1. We also investgate the rates of selecting the true model in that section. ", "page_idx": 7}, {"type": "table", "img_path": "6SAnp0vr9X/tmp/50f0e8012f67b30ee34b4e23aa9a48ca49fddccbb34c6f88ecf43aaa405a3afa.jpg", "table_caption": ["Table 2: Rates of excluding active variables (false negative rate) in Case C "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "5.1.3  Computational time ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We present the mean computational times of different algorithms in Table 3. Our codes are written in the julia programming language [2] and implemented on a Linux workstation. The lasso pathes are solved with Lasso.jl [13]. As shown in Table 3, subsampling algorithms significantly reduce the computational times compared with full data estimators. Although optimal sampling requires to calculate sampling probabilities, they use only about $0.77\\%$ of the computational time that the full data adaptive lasso requires. As we discussed in Section C.2, optimal sampling algorithms reduce both sample size and the data dimension. Therefore, the computational cost of the coordinate decent algorithm, which often requires a large number of iterations, is significantly reduced. ", "page_idx": 8}, {"type": "table", "img_path": "6SAnp0vr9X/tmp/a1bd4e5d373ff9f622ccd6313b628eb1f75ef8503e6fe1b13571c448d1cc9469.jpg", "table_caption": ["Table 3: Mean computational time (seconds) "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "5.2 Real data ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We evaluate the performances of proposed estimators on two real data sets. ", "page_idx": 8}, {"type": "text", "text": "(i) Covtype data set: It is available at https://archive.ics.uci.edu/ml/datasets/ covertype,with $N\\,=\\,581012$ observations and 54 covariates - 10 being quantitative and 44 being qualitative with dummy coding. We drop the 14th and 54th columns to avoid exact colinearity of the dummy variables. Our goal is to classify whether the forest cover type is Cottonwood/Willow (labeled as 1) or not (labeled as O). The proportion of Cottonwood/Willow is $0.473\\%$ , which is highly imbalanced. ", "page_idx": 8}, {"type": "text", "text": "(ii) Font data set: It is available at https://archive.ics.uci.edu/ml/datasets/ Character+Font+Images, with $0.50\\%$ of the $N\\,=\\,832670$ responses being the GADUGI font. The first 10 covariates are about the value, size, and style of the characters and there are additional 400 pixel values of the $20\\times20$ images. We remove the 4th, 9th, and 10th covariates because they are constants. ", "page_idx": 8}, {"type": "text", "text": "For both data sets, we apply Algorithm 2 on the logarithmic-transformed data. We use pilot samples of size $N_{\\mathrm{pl}}=1000$ for the covtype data and $N_{\\mathrm{pl}}=1500$ for the font data due to its higher dimension. Since we do not know the true parameter for real data, we use area under the curve (AUC) to measure the performances of subsampling algorithms. We repeat the experiment for $S=500$ and compute the empirical median AUC using the full data. The results are summarized in Figure 4. As shown in Figure 4, nonuniform sampling outperforms uniform sampling in general. There is one case for font data set that $\\hat{\\beta}_{\\mathrm{A-OS}}^{\\mathrm{adp}}$ is worse than theuniform sampling when the samling rate is high. For the covtype data set, among the three estimators based on optimal sampling, $\\hat{\\beta}_{\\mathrm{P-OS}}^{\\mathrm{adp}}$ performs the best and $\\hat{\\beta}_{\\mathrm{L-OS}}^{\\mathrm{adp}}$ is worst. For the font data set, $\\hat{\\beta}_{\\mathrm{A-OS}}^{\\mathrm{adp}}$ and $\\hat{\\beta}_{\\mathrm{L-OS}}^{\\mathrm{adp}}$ are similar, and $\\hat{\\beta}_{\\mathrm{P-OS}}^{\\mathrm{adp}}$ based on the scale invariant optimal sampling function is significantly better. ", "page_idx": 8}, {"type": "text", "text": "6 Conclusion and limitations ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this paper, we investigated the problem of scale-invariant optimal subsampling in the context of variable selection for rare-events data. We derived optimal probabilities based on the A- and Loptimality criteria, and discussed their limitations. Furthermore, we proposed scale-invariant optimal probabilities based on prediction errors to overcome the limitations. Both analytical and numerical results show the desirable properties of the proposed methods. ", "page_idx": 8}, {"type": "image", "img_path": "6SAnp0vr9X/tmp/ba84cb249d4101b2b6f11ca70f2a3481b3e72f830e376ec01bff2e8e8efd596d.jpg", "img_caption": ["Figure 4: Empirical median AUCs for two real data sets "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Our investigation has the following limitations. ", "page_idx": 9}, {"type": "text", "text": "\u00b7 Our proposed criterion optimizes the probabilities by minimizing the asymptotic mean squared error in estimating rare-event probabilities. While this prioritizes the accuracy of estimation, it puts less emphasis on the quality of variable selection. Further research is needed to devise optimal probabilities that focus on variable selection performance metrics.   \n\u00b7 Our theoretical analysis is based on asymptotic properties, with optimal probabilities defined through the asymptotic normality. Although our results may hold for sufficiently sparse models, they may not generalize to cases where the model is dense or over-parameterized, because asymptotic normality may no longer be applicable. Therefore, an important direction for future research is to study the non-asymptotic properties of our estimators, such as prediction error bounds. Non-asymptotic behaviors are particularly of interest in highdimensional regimes.   \n\u00b7 We employ Lasso as the pilot estimator. However, other variable selection methodologies, such as sure independence screening, can also be considered. Exploring the impact of different pilot estimators on our method's performance represents another avenue for future investigations.   \n\u00b7 We assume that the underlying full model is correctly specified and possesses a sparse structure. Our analysis does not account for model misspecification. Further research is required to address scenarios where the model is possibly misspecified or where the number of features vastly exceeds the number of observations. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The authors are grateful to Professor Kun Chen for the insightful comments and suggestions on the development of the manuscript. Funding in direct support of this work: NEI grant R21EY035710, NSF grant 2105571, UConn CLAS Research Funding in Academic Themes, GPUs donated by NVIDIA. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1]  M. Ai, J. Yu, H. Zhang, and H. Wang. Optimal subsampling algorithms for big data regressions. Statistica Sinica, 31(2):749-772, 2021.   \n[2] J. Bezanson, A. Edelman, S. Karpinski, and V. B. Shah. Julia: A fresh approach to numerical computing. SIAM review, 59(1):65-98, 2017. [3] N. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer. Smote: synthetic minority over-sampling technique. Journal of artificial intelligence research, 16:321-357, 2002.   \n[4]  G. Douzas and F. Bacao. Self-organizing map oversampling (somo) for imbalanced data set learning. Expert systems with Applications, 82:40-52, 2017.   \n[5] C. Drummond, R. C. Holte, et al. C4. 5, clas imbalance, and cost sensitivity: why undersampling beats over-sampling. In Workshop on learning from imbalanced datasets I, volume 11, 2003.   \n[6]  J. Fan and J. Lv. Sure independence screening for ultrahigh dimensional feature space. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 70(5):849-911, 2008.   \n[7] D. Firth. Bias reduction of maximum likelihood estimates. Biometrika, 80(1):27-38, 03 1993. [8]  J. Friedman, T. Hastie, H. Hofling, and R. Tibshirani. Pathwise coordinate optimization. The Annals of Applied Statistics, 1(2):302 - 332, 2007.   \n[9]  J. H. Friedman, T. Hastie, and R. Tibshirani. Regularization paths for generalized linear models via coordinate descent. Journal of Statistical Software, 33(1): 1-22, 2010.   \n[10] W.Fu and K. Knight. Asymptotics forlasso-type estimators. The Annals of statistics, 28(5):1356- 1378, 2000.   \n[11] C.J. Geyer. On the asymptotics of constrained m-estimation. The Annals of statistics, pages 1993-2010, 1994.   \n[12] H. Han, W.-Y. Wang, and B.-H. Mao. Borderline-smote: a new over-sampling method in imbalanced data sets learning. In International conference on intelligent computing, pages 878-887. Springer, 2005.   \n[13] JuliaStats. Lasso.jl. https : //github.com/JuliaStats/Lasso. jl, 2022.   \n[14]  N. Keret and M. Gorine. Analyzing big ehr data-optimal cox regression subsampling procedure with rare events. Journal of the American Statistical Association, 118(544):2262-2275, 2023.   \n[15] X.-Y. Liu, J. Wu, and Z.-H. Zhou. Exploratory undersampling for class-imbalance learning. IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), 39(2):539-550, 2008.   \n[16] J. Mathew, C. K. Pang, M. Luo, and W. H. Leong. Classification of imbalanced data by oversampling in kernel space of support vector machines. IEEE transactions on neural networks and learning systems, 29(9):4065-4076, 2017.   \n[17] F. Pukelsheim. Optimal design of experiments. SIAM, 2006.   \n[18] J. Shao. Mathematical Statistics, 2nd. Springer-Verlag, New York, 2003.   \n[19]  G. Tripathi. A matrix extension of the cauchy-schwarz inequality. Economics Letters, 63(1):1-3, 1999.   \n[20]  H. Wang. Logistic regression for massive data with rare events. In International Conference on Machine Learning, pages 9829-9836. PMLR, 2020.   \n[21] H. Wang and Y. Ma. Optimal subsampling for quantile regression in big data. Biometrika, 108(1):99-112, 2021.   \n[22] H. Wang, A. Zhang, and C. Wang. Nonuniform negative sampling and log odds correction with rare events data. Advances in Neural Information Processing Systems, 34, 2021.   \n[23]  H. Wang, R. Zhu, and P. Ma. Optimal subsampling for large sample logistic regression. Journal of the American Statistical Association, 113(522):829-844, 2018.   \n[24] J. Wang, J. Zou, and H. Wang. Sampling with replacement vs poisson sampling: a comparative study in optimal subsampling. IEEE Transactions on Information Theory, 68(10):6605-6630, 2022.   \n[25]  Y. Yao and H. Wang. Optimal subsampling for softmax regression. Statistical Papers, pages 585-599, 12 2018.   \n[26] J. Yu, H. Wang, M. Ai, and H. Zhang. Optimal distributed subsampling for maximum quasilikelihood estimators with massive data. Journal of the American Statistical Association, 0(0):1-12, 2020. D0I:10.1080/01621459.2020.1773832.   \n[27]  G.-X. Yuan, C.-H. Ho, and C.-J. Lin. An improved glmnet for l1-regularized logistic regression. J. Mach. Learn. Res., 13:1999-2030, 2012.   \n[28]  H. H. Zhang and W. Lu. Adaptive Lasso for Cox's proportional hazards model. Biometrika, 94(3):691-703, 05 2007.   \n[29] T. Zhang, Y. Ning, and D. Ruppert. Optimal sampling for generalized linear models under measurement constraints. Journal of Computational and Graphical Statistics, 30(1):106-114, 2021.   \n[30]  H. Zou. The adaptive lasso and its oracle properties. Journal of the American Statistical Association, 101(476):1418-1429, 2006. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Appendix / supplemental material ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "In this appendix, we present the details of the proof, the practical algorithm and simuation settings in the paper. Details of mathematical proofs are provided in Section B. Details of the practical algorithm are provided in Section C. We present the details of simulation settings in Section D and in Section E, we give some additional simulation results. ", "page_idx": 11}, {"type": "text", "text": "B  Details of mathematical proofs ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "In this section, we provide details of mathematical proofs. ", "page_idx": 11}, {"type": "text", "text": "B.1  General assumptions in the main paper ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "We begin with some general assumptions used throughout this paper. ", "page_idx": 11}, {"type": "text", "text": "Assumption 1. The first, second and third derivatives of $f(\\mathbf{\\boldsymbol{x}};\\mathbf{\\boldsymbol{\\theta}})$ and $e^{f(\\pmb{x};\\beta)}f(\\pmb{x};\\beta)$ withrespect to $\\beta$ are bouned by a square intergrable random variable $B(x)$ ", "page_idx": 11}, {"type": "text", "text": "Assumption 2. The matrix B $\\because\\{\\dot{g}^{\\otimes2}(x;\\pmb{\\theta})\\}$ is finite and positive definite. ", "page_idx": 11}, {"type": "text", "text": "Assumption 3. The subsampling rate $\\rho$ satisfies that $c_{N}=\\left.e^{\\alpha_{\\mathrm{t}}}/\\rho\\right.\\rightarrow c_{\\mathrm{g}}$ where $0\\leq c<\\infty$ is $a$ constant. ", "page_idx": 11}, {"type": "text", "text": "Assumption 4. The integral $\\mathbb{E}\\left[\\left\\{\\varphi({\\pmb x})+\\varphi^{-1}({\\pmb x})\\right\\}B^{2}({\\pmb x})\\right]$ is finite, where $B(x)$ is a squareintegrable function that dominates the frst, second, and third derivatives of $f(\\mathbf{\\boldsymbol{x}};\\pmb{\\theta})$ and $e^{f(\\pmb{x};\\beta)}f(\\pmb{x};\\beta)$ with respect to $\\beta$ ", "page_idx": 11}, {"type": "text", "text": "Assumption 5. The integral $\\mathbb{E}\\left\\{e^{f({\\pmb x};\\beta)}\\varphi^{-1}({\\pmb x})B({\\pmb x})\\right\\}$ is finite. ", "page_idx": 11}, {"type": "text", "text": "These assumptions are the same assumptions used in [22]. Here, we remind some notations used in the main paper: ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{p(x;\\theta)=\\displaystyle\\frac{e^{\\alpha+f(x;\\beta)}}{1+e^{\\alpha+f(x;\\beta)}},}}\\\\ {{\\phi(x;\\theta)=p(x;\\theta)\\left\\{1-p(x;\\theta)\\right\\},}}\\\\ {{M=\\mathbb{E}\\left\\{e^{f(x_{i};\\beta_{\\mathrm{t}})}\\dot{g}^{\\otimes2}(x_{i},\\theta_{\\mathrm{t}})\\right\\},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "and ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathbf{A}_{\\mathrm{mscl}}=\\mathbb{E}\\left[\\frac{e^{f(\\mathbf{x};\\beta_{\\mathrm{t}})}\\dot{g}^{\\otimes2}(\\pmb{x};\\beta_{\\mathrm{t}})}{1+c\\varphi^{-1}(\\pmb{x})e^{f(\\pmb{x};\\beta_{\\mathrm{t}})}}\\right].\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "To ease the presentation in the following sections, we denote ", "page_idx": 12}, {"type": "equation", "text": "$$\nM_{\\mathrm{w}(A)}=\\mathbb{E}\\left[\\left\\{1+\\frac{c e^{f({\\pmb x};\\beta_{\\mathrm{t}})}}{\\varphi({\\pmb x})}\\right\\}e^{f({\\pmb x};\\beta_{\\mathrm{t}})}\\dot{g}_{(A)}^{\\otimes2}({\\pmb x};{\\pmb\\theta}_{\\mathrm{t}})\\right],\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "and $a_{N}=\\sqrt{N e^{\\alpha_{\\mathrm{t}}}}$ in the appendix. Note that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{N_{1}=\\displaystyle\\sum_{i=1}^{N}y_{i}=N\\mathbb{E}\\left\\{\\frac{e^{\\alpha_{\\mathrm{t}}+f({\\pmb x};{\\beta}_{\\mathrm{t}})}}{1+e^{\\alpha_{\\mathrm{t}}+f({\\pmb x};{\\beta}_{\\mathrm{t}})}}\\right\\}\\left\\{1+o_{P}(1)\\right\\}}}\\\\ {{=N e^{\\alpha_{\\mathrm{t}}}\\mathbb{E}\\left\\{e^{f({\\pmb x};{\\beta}_{\\mathrm{t}})}\\right\\}\\left\\{1+o_{P}(1)\\right\\}=a_{N}^{2}\\mathbb{E}\\left\\{e^{f({\\pmb x};{\\beta}_{\\mathrm{t}})}\\right\\}\\left\\{1+o_{P}(1)\\right\\}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "B.2Proof of Theorem 1 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "ProofofTheorem $^{\\,l}$ . We consider the target of IPW adaptive lasso estimator: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle Q_{\\mathrm{w}}({\\pmb\\theta})=-\\sum_{i=1}^{N}\\frac{\\delta_{i}}{\\pi\\big({\\pmb x}_{i},y_{i}\\big)}\\big[y_{i}g({\\pmb x}_{i};{\\pmb\\theta})-\\log\\{1+e^{g({\\pmb x}_{i};{\\pmb\\theta})}\\}\\big]+\\lambda_{N}\\sum_{j=1}^{p}\\hat{w}_{j}\\vert\\beta_{(j)}\\vert}}\\\\ {~~}\\\\ {{\\displaystyle=-\\ell_{\\mathrm{w}}({\\pmb\\theta})+\\lambda_{N}\\sum_{j=1}^{p}\\hat{w}_{j}\\vert\\beta_{(j)}\\vert,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $\\hat{w}_{j}=1/|\\hat{\\beta}_{\\mathrm{pl}(j)}|,1\\le j\\le p$ Then, we have that $\\hat{\\pmb u}_{N}=a_{N}(\\hat{\\pmb\\theta}_{\\mathrm{w}}-\\pmb\\theta_{\\mathrm{t}})$ is the minimizer of ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\gamma_{\\mathrm{w}}^{N}(u)=Q_{\\mathrm{w}}(\\pmb{\\theta}_{\\mathrm{t}}+a_{N}^{-1}\\pmb{u})-Q_{\\mathrm{w}}(\\pmb{\\theta}_{\\mathrm{t}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Asymptotic normality: We prove the asymptotic normality part in this paragraph. By Taylor's expansion, ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\gamma_{\\mathrm{w}}^{N}({\\pmb u})=-\\frac{1}{a_{N}}u^{\\mathrm{T}}{\\dot{\\ell}}_{\\mathrm{w}}({\\pmb\\theta}_{\\mathrm{t}})+\\frac{1}{2a_{N}^{2}}\\sum_{i=1}^{N}\\frac{\\delta_{i}}{\\pi({\\pmb x}_{i},y_{i})}\\phi({\\pmb x}_{i};{\\pmb\\theta}_{\\mathrm{t}})\\{{\\pmb u}^{\\mathrm{T}}{\\dot{\\pmb y}}({\\pmb x}_{i};{\\pmb\\theta}_{\\mathrm{t}})\\}^{2}-\\Delta_{\\mathrm{w}}+R_{\\mathrm{w}}}}\\\\ {{\\displaystyle~~~~~~~~~~~~+\\frac{\\lambda_{N}}{a_{N}}\\sum_{j=1}^{p}\\hat{w}_{j}a_{N}\\left(\\left|\\beta_{\\mathrm{t}(j)}+\\frac{u_{(j)}}{a_{N}}\\right|-\\left|\\beta_{\\mathrm{t}(j)}\\right|\\right).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "We first consider the limit behavior of the IPW target function by prove the asymptotic normality. In [22], the authors established that under Assumptions 1 to 3, ", "page_idx": 12}, {"type": "equation", "text": "$$\na_{N}^{-1}\\dot{\\ell}_{\\mathrm{w}}(\\pmb{\\theta}_{\\mathrm{t}})\\backsim M_{\\mathrm{w}}^{1/2}Z,\n$$", "text_format": "latex", "page_idx": 12}, {"type": "equation", "text": "$$\n\\frac{1}{a_{N}^{2}}\\sum_{i=1}^{N}\\frac{\\delta_{i}}{\\pi(\\pmb{x}_{i},y_{i})}\\phi(\\pmb{x}_{i};\\pmb{\\theta}_{\\mathrm{t}})\\dot{g}^{\\otimes2}(\\pmb{x}_{i};\\pmb{\\theta}_{\\mathrm{t}})\\stackrel{P}{\\longrightarrow}M,\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "and ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\Delta_{\\mathrm{w}}=o_{P}(1),\\quad R_{\\mathrm{w}}=o_{P}(1).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Thus, ", "page_idx": 12}, {"type": "equation", "text": "$$\n-\\ell_{\\mathrm{w}}(\\pmb{\\theta}_{\\mathrm{t}})\\sim-\\pmb{u}^{\\mathrm{T}}\\pmb{M}_{\\mathrm{w}}^{1/2}\\pmb{Z}+\\frac{1}{2}\\pmb{u}^{\\mathrm{T}}\\pmb{M}\\pmb{u}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Next, we consider the limit behavior of the adaptive lasso penalty. Since we assume $\\hat{\\beta}_{\\mathrm{pl}}$ to be a consistent estimator, we know that when $j\\in A$ i.e., $\\beta_{\\mathrm{t}(j)}\\neq0$ ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{w}_{j}=|\\hat{\\beta}_{\\mathrm{pl}(j)}|^{-\\gamma}\\xrightarrow{P}|\\beta_{\\mathrm{t}(j)}|^{-\\gamma}>0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "and ", "page_idx": 12}, {"type": "equation", "text": "$$\na_{N}\\left(\\left|\\beta_{\\mathfrak{t}(j)}+\\frac{u_{(j)}}{a_{N}}\\right|-|\\beta_{\\mathfrak{t}(j)}|\\right)\\to\\mathbf{sgn}(\\beta_{\\mathfrak{t}(j)})u_{(j)}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Therefore, for $j\\in\\mathcal{A}$ , we have that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\frac{\\lambda_{N}}{a_{N}}\\hat{w}_{j}a_{N}\\left(\\left|\\beta_{\\mathfrak{t}(j)}+\\frac{u_{(j)}}{a_{N}}\\right|-|\\beta_{\\mathfrak{t}(j)}|\\right)=o_{P}(1)\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "since $\\lambda_{N}/a_{N}=\\lambda_{N}/\\sqrt{N e^{\\alpha_{\\mathrm{t}}}}\\rightarrow0.$ On the other hand, when $j\\in\\mathcal{A}^{c}$ ,i.e., $\\beta_{\\mathrm{t}(j)}=0$ , we have that for $u_{(j)}\\neq0$ \uff0c ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\frac{\\lambda_{N}}{a_{N}}\\hat{w}_{j}a_{N}\\left(\\left|\\beta_{\\mathfrak{t}(j)}+\\frac{u_{(j)}}{a_{N}}\\right|-|\\beta_{\\mathfrak{t}(j)}|\\right)=\\frac{\\lambda_{N}}{a_{N}}\\hat{w}_{j}|u_{(j)}|=\\frac{\\lambda_{N}}{a_{N}|\\hat{\\beta}_{\\mathfrak{p}1(j)}|\\gamma}|u_{(j)}|\\stackrel{P}{\\longrightarrow}\\infty,\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "since $\\lambda_{N}/(\\sqrt{N e^{\\alpha_{\\mathrm{t}}}}|\\hat{\\beta}_{\\mathrm{pl}(j)}|^{\\gamma})\\xrightarrow{P}\\infty$ . Then, we have that $\\gamma_{\\mathrm{w}}^{N}(u)\\sim\\gamma_{\\mathrm{w}}(u)$ , where ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\gamma_{\\mathrm{w}}(u)=\\left\\{\\begin{array}{l l}{\\frac{1}{2}u_{(A)}^{\\mathrm{T}}M_{(A)}u_{(A)}-u_{(A)}^{\\mathrm{T}}M_{\\mathrm{w}}^{1/2}Z_{(A)}}&{\\mathrm{if~}u_{(j)}=0,\\forall j\\in\\mathcal{A}^{c}}\\\\ {\\infty}&{\\mathrm{otherwise}.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Note that the unique minimizer of $\\gamma_{\\mathrm{w}}^{N}(u)$ $(M_{(A)}^{-1}M_{\\mathrm{w}}^{1/2}Z_{(A)}^{\\mathrm{T}},\\mathbf{0})^{\\mathrm{T}}$ if we put all the indexes o active variables in front. Thus, following the results of [11] and [10], we have the minimizer of $\\gamma_{\\mathrm{w}}^{N}(u)$ ,i.e., $\\hat{\\pmb u}_{N}$ ,satisfies that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\hat{\\pmb u}_{N(A)}\\sim M_{(A)}^{-1}M_{\\mathrm{w}}^{1/2}{\\cal Z}_{(A)}\\;\\mathrm{and}\\;\\hat{\\pmb u}_{N(A^{c})}\\sim{\\bf0}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Thus, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{u}}_{N(A)}=a_{N}(\\hat{\\theta}_{\\mathrm{w}(A)}-\\theta_{\\mathrm{t}(A)})\\sim\\mathbb{N}(\\mathbf{0},M_{(A)}^{-1}M_{\\mathrm{w}(A)}M_{(A)}^{-1}).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Since ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\sqrt{N_{1}}=a_{N}\\mathbb{E}^{1/2}\\left\\{e^{f({\\pmb x};{\\pmb\\beta}_{\\mathrm{t}})}\\right\\}\\left\\{1+o_{P}(1)\\right\\},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "applying Slusky's theorem, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\sqrt{N_{1}}V_{\\mathrm{w}(A)}^{-1/2}(\\hat{\\pmb{\\theta}}_{\\mathrm{w}(A)}-\\pmb{\\theta}_{\\mathrm{t}(A)})\\rightsquigarrow\\mathbb{N}(\\mathbf{0},\\pmb{I}).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Consistency in variable selection We prove the consistency in variable selection in this paragraph. From the result of asymptotic normality, we know that $\\hat{\\beta}_{\\mathrm{w}(j)}\\xrightarrow{P}\\beta_{\\mathrm{t}(j)}$ for every $j\\in A$ and therefore $\\mathbb{P}(j\\,\\in\\,\\hat{\\mathcal{A}}_{\\mathrm{w}})\\,\\rightarrow\\,1$ .Thus, we only consider $j^{\\prime}\\in\\mathcal{A}^{c}$ .When $j^{\\prime}\\in\\hat{\\mathcal{A}}_{\\mathrm{w}}$ , we know that by K-K-T optimality conditions, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\lambda_{N}\\hat{w}_{j^{\\prime}}\\mathrm{sgn}(\\hat{\\beta}_{(j^{\\prime})})=\\dot{\\ell}_{\\mathrm{w}}(\\hat{\\pmb{\\theta}}_{\\mathrm{w}}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "which means ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\lambda_{N}\\hat{w}_{j^{\\prime}}\\mathbf{s}\\mathbf{gn}(\\hat{\\beta}_{(j^{\\prime})})}{a_{N}}=\\frac{\\dot{\\ell}_{\\mathrm{w}}(\\hat{\\pmb{\\theta}}_{\\mathrm{w}})}{a_{N}}}\\\\ &{=\\frac{\\dot{\\ell}_{\\mathrm{w}}(\\pmb{\\theta}_{\\mathrm{t}})}{a_{N}}+\\frac{a_{N}\\,\\Big\\{\\dot{\\ell}_{\\mathrm{w}}(\\hat{\\pmb{\\theta}}_{\\mathrm{w}})-\\dot{\\ell}_{\\mathrm{w}}(\\pmb{\\theta}_{\\mathrm{t}})\\Big\\}}{a_{N}^{2}}=:I_{1}+I_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We have known that $I_{1}=\\dot{\\ell}_{\\mathrm{w}}(\\pmb{\\theta}_{\\mathrm{t}})/a_{N}\\backsim Z_{\\mathrm{w}}$ . We now prove that proof that $I_{2}=O_{P}(1)$ . We apply Taylor expansion to the $k$ -th element of $\\dot{\\ell}_{\\mathrm{w}}(\\hat{\\pmb{\\theta}}_{\\mathrm{w}})$ and have that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\frac{\\i_{N}\\left\\{\\dot{\\ell}_{(k)}(\\hat{\\theta}_{w})-\\dot{\\ell}_{(k)}(\\theta_{t})\\right\\}}{a_{N}^{2}}=-\\frac{1}{a_{N}^{2}}\\sum_{i=1}^{N}\\frac{\\delta_{i}}{\\pi(x,y_{i})}\\phi(x_{i};\\theta_{t})\\dot{g}_{(k)}(x_{i};\\theta_{t})\\dot{g}^{\\mathrm{T}}(x_{i};\\theta_{t})\\hat{u}_{N}+\\tilde{\\Delta}_{\\mathrm{w}(k)}+\\tilde{R}_{\\mathrm{w}(k)}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\hat{\\pmb u}_{N}=a_{N}(\\hat{\\pmb\\theta}_{\\mathrm{w}}-\\pmb\\theta_{\\mathrm{t}})=O_{P}(1),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "equation", "text": "$$\n\\tilde{\\Delta}_{\\mathbf{w}(k)}=\\frac{1}{a_{N}^{2}}\\sum_{i=1}^{N}\\frac{\\delta_{i}}{\\pi(\\mathbf{x}_{i},y_{i})}\\left\\{y_{i}-p(\\mathbf{x}_{i};\\theta_{\\mathrm{t}})\\right\\}\\sum_{j=1}^{d}\\ddot{g}_{(k j)}(\\mathbf{x}_{i};\\theta_{\\mathrm{t}})\\hat{u}_{N(j)},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\bar{R}_{\\mathrm{w}(k)}=-\\frac{1}{2a_{N}^{3}}\\sum_{i=1}^{N}\\frac{\\delta_{i}}{\\pi(x_{i},y_{i})}\\phi(x_{i};\\theta_{k})\\left\\{1-2p(x_{i};\\theta_{k})\\right\\}j_{(k)}(x_{i};\\theta_{k})\\hat{u}_{N}^{\\mathrm{T}}\\{\\mathcal{S}^{\\mathrm{Q2}}(x_{i};\\theta_{k})\\hat{u}_{N}\\}}\\\\ {\\displaystyle}&{\\displaystyle-\\,\\frac{2}{2a_{N}^{3}}\\sum_{i=1}^{N}\\frac{\\delta_{i}}{\\pi(x_{i},y_{i})}\\phi(x_{i};\\theta_{k})\\left\\{\\hat{u}_{N}^{\\mathrm{T}}\\frac{\\partial\\dot{g}_{(k)}(x_{i};\\theta_{k})}{\\partial\\theta}\\right\\}\\left\\{\\hat{u}_{N}^{\\mathrm{T}}\\hat{g}(x_{i};\\theta_{k})\\right\\}}\\\\ {\\displaystyle}&{\\displaystyle-\\,\\frac{1}{2a_{N}^{3}}\\sum_{i=1}^{N}\\frac{\\delta_{i}}{\\pi(x_{i},y_{i})}\\phi(x_{i};\\theta_{k})\\dot{g}_{(k)}(x_{i};\\theta_{k})\\left\\{\\hat{u}_{N}^{\\mathrm{T}}\\ddot{g}(x_{i};\\theta_{k})\\hat{u}_{N}\\right\\}}\\\\ {\\displaystyle}&{\\displaystyle+\\,\\frac{1}{2a_{N}^{3}}\\sum_{i=1}^{N}\\frac{\\delta_{i}}{\\pi(x_{i},y_{i})}\\left\\{y_{i}-p(x_{i};\\theta_{k})\\right\\}\\hat{u}_{N}^{\\mathrm{T}}\\frac{\\partial^{2}\\dot{g}_{(k)}(x_{i};\\theta_{k})}{\\partial\\theta^{2}}\\hat{u}_{N}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\acute{\\theta}_{k}$ is between $\\hat{\\pmb\\theta}_{\\mathrm{mle}}$ and $\\theta_{\\mathrm{t}}$ . First, we prove that $\\tilde{R}_{(k)}$ .is $o_{P}(1)$ . We have that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\beta_{\\mathrm{d}x,\\parallel}\\leq\\frac{1}{2\\sqrt{\\pi}}\\sum_{t=1}^{\\infty}\\frac{\\delta}{\\gamma}\\frac{\\delta}{\\gamma}\\exp\\left(\\alpha_{t}\\delta_{t}\\right)\\bigg\\vert1-2(\\delta_{t})\\frac{\\delta}{\\gamma}\\exp\\left\\vert\\frac{\\delta}{\\gamma}\\right\\vert\\frac{\\delta}{\\gamma}\\right\\vert\\delta_{t}(x,\\theta_{t})\\bigg\\vert\\bigg\\vert\\frac{\\delta}{\\gamma}\\Biggr}\\\\ &{\\leq\\frac{2(\\delta_{t})^{2}}{2(\\delta_{t})^{2}}\\sum_{t=1}^{\\infty}\\frac{\\delta}{\\gamma}\\exp\\left\\vert\\theta(x,\\theta_{t})\\right\\vert\\Biggl\\vert\\frac{\\delta}{\\gamma}\\Biggr[\\delta_{t}(x,\\theta_{t})\\Biggr]\\Biggr[\\delta_{t}(x,\\theta_{t})\\Biggr]}\\\\ &{+\\frac{16\\pi^{2}}{2(\\delta_{t})^{2}}\\sum_{t=1}^{\\infty}\\frac{\\delta}{\\gamma}\\frac{\\delta}{\\gamma}(x,\\theta_{t})\\sin(x,\\theta_{t})\\bigg\\vert\\frac{\\delta}{\\gamma}\\Biggr[\\delta_{t}(x,\\theta_{t})\\Biggr]\\Biggr[\\delta_{t}(x,\\theta_{t})\\Biggr]}\\\\ &{+\\frac{2(\\delta_{t})^{2}}{2(\\delta_{t})^{2}}\\sum_{t=1}^{\\infty}\\frac{\\delta}{\\gamma}(x,\\theta_{t})\\sin(x,\\theta_{t})\\bigg\\vert\\frac{\\delta}{\\gamma}\\Biggr[\\delta_{t}(x,\\theta_{t})\\Biggr]\\Biggr[\\delta_{t}(x,\\theta_{t})}\\\\ &{+\\frac{16\\pi^{2}}{2\\sqrt{\\pi}}\\sum_{t=1}^{\\infty}\\frac{\\delta}{\\gamma}(x,\\theta_{t})\\sin(x,\\theta_{t})\\bigg\\vert\\frac{\\delta}{\\gamma}\\Biggr]}\\\\ &{+\\frac{16\\pi^{2}}{2(\\delta_{t})^{2}}\\sum_{t=1}^{\\infty}\\frac{\\delta}{\\gamma}\\frac{\\delta}{\\gamma}\\frac{\\delta}{\\gamma}\\bigg[\\delta_{t}(x,\\theta_{t})\\bigg\\vert\\frac{\\delta}{\\gamma}\\Biggr]\\Biggr[\\delta_{t}(x,\\theta_{t})\\Biggr]}\\\\ &{\\leq\\frac{16\\pi^{2}}{2(\\delta_{t})^{2}}\\frac{\\delta}{\\gamma}\\frac{\\delta}{\\gamma}\\frac{\\delta}{\\gamma} \n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{C(\\pmb{x}_{i};\\pmb{\\dot{\\theta}})=\\left|\\dot{g}_{(k)}(\\pmb{x}_{i};\\pmb{\\dot{\\theta}})\\right|\\left\\{\\left\\|\\dot{g}(\\pmb{x}_{i};\\pmb{\\dot{\\theta}}_{k})\\right\\|^{2}+\\left\\|\\ddot{g}(\\pmb{x}_{i};\\pmb{\\dot{\\theta}})\\right\\|\\right\\}}\\\\ &{\\qquad\\qquad+\\left\\|\\frac{\\partial\\dot{g}_{(k)}(\\pmb{x}_{i};\\pmb{\\dot{\\theta}}_{k})}{\\partial\\pmb{\\theta}}\\right\\|\\left\\|\\dot{g}(\\pmb{x}_{i};\\pmb{\\dot{\\theta}}_{k})\\right\\|+\\left\\|\\frac{\\partial^{2}\\dot{g}_{(k)}(\\pmb{x}_{i};\\pmb{\\dot{\\theta}}_{k})}{\\partial\\pmb{\\theta}^{2}}\\right\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Therefore, we proved that $\\tilde{R}_{\\mathrm{w}(k)}\\,=\\,o_{P}(1)$ . Next, we prove that $\\tilde{\\Delta}_{\\mathrm{w}(k)}\\,=\\,o_{P}(1)$ . We know that $\\begin{array}{r}{\\mathbb{E}\\left[a_{N}^{-2}\\sum_{i=1}^{N}\\delta_{i}/\\pi(\\pmb{x}_{i},y_{i})\\left\\{y_{i}-p(\\pmb{x}_{i};\\pmb{\\theta}_{\\mathrm{t}})\\right\\}\\ddot{g}(\\pmb{x}_{i};\\pmb{\\theta}_{\\mathrm{t}})\\right]=\\mathbf{0}}\\end{array}$ Wealsave eyt of $\\begin{array}{r}{a_{N}^{-2}\\sum_{i=1}^{N}\\delta_{i}/\\pi(\\pmb{x}_{i},y_{i})\\left\\{y_{i}-p(\\pmb{x}_{i};\\pmb{\\theta}_{\\mathrm{t}})\\right\\}\\ddot{g}(\\pmb{x}_{i};\\pmb{\\theta}_{\\mathrm{t}})}\\end{array}$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{V}\\left[a_{N}^{-2}\\sum_{i=1}^{N}\\frac{\\delta_{i}}{\\pi(\\mathbf{x}_{i},y_{i})}\\left\\{y_{i}-p(\\pmb{x}_{i};\\pmb{\\theta}_{\\mathrm{t}})\\right\\}\\ddot{g}_{(j l)}(\\pmb{x}_{i};\\pmb{\\theta}_{\\mathrm{t}})\\right]\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\n\\leq\\frac{1}{a_{N}^{4}}\\sum_{i=1}^{N}\\mathbb{E}\\left\\{p(x_{i};\\pmb{\\theta}_{\\mathrm{t}})\\ddot{g}_{(j l)}^{2}(\\pmb{x}_{i};\\pmb{\\theta}_{\\mathrm{t}})\\right\\}\\leq\\frac{1}{a_{N}^{2}}\\mathbb{E}[e^{f(\\pmb{x};\\pmb{\\beta}_{\\mathrm{t}})}\\|\\ddot{g}(\\pmb{x};\\pmb{\\theta}_{\\mathrm{t}})\\|^{2}]\\rightarrow0.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Thus, due to Chebyshev's inequality, we know that $\\tilde{\\Delta}_{\\mathrm{w}}\\;\\;=\\;\\;o_{P}(1)$ .Since we know that $\\begin{array}{r}{\\frac{1}{a_{N}^{2}}\\sum_{i=1}^{N}\\delta_{i}/\\pi(\\pmb{x}_{i},y_{i})\\phi(\\pmb{x}_{i};\\pmb{\\theta}_{\\mathrm{t}})\\dot{g}^{\\otimes2}(\\pmb{x}_{i};\\pmb{\\theta}_{\\mathrm{t}})=H_{\\mathrm{w}}=O_{P}(1)}\\end{array}$ . Hence, we have that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{\\dot{\\ell}_{\\mathrm{w}}(\\hat{\\pmb{\\theta}}_{\\mathrm{w}})}{a_{N}}=O_{P}(1).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Note that we also have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{\\lambda_{N}\\hat{w}_{j^{\\prime}}}{a_{N}}=\\frac{\\lambda_{N}}{a_{N}}\\frac{1}{|\\hat{\\beta}_{\\mathrm{pl}(j^{\\prime})}|^{\\gamma}}\\stackrel{P}{\\longrightarrow}\\infty.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}(j^{\\prime}\\in\\hat{\\mathcal{A}}_{\\mathrm{w}})\\leq\\mathbb{P}\\left\\{\\lambda_{N}\\hat{w}_{j^{\\prime}}\\mathrm{sgn}(\\hat{\\beta}_{(j^{\\prime})})=\\dot{\\ell}_{\\mathrm{w}}(\\hat{\\pmb{\\theta}}_{\\mathrm{w}})\\right\\}}\\\\ &{=\\mathbb{P}\\left\\{\\frac{\\lambda_{N}\\hat{w}_{j^{\\prime}}\\mathrm{sgn}(\\hat{\\beta}_{(j^{\\prime})})}{a_{N}}=\\frac{\\dot{\\ell}_{\\mathrm{w}}(\\hat{\\pmb{\\theta}}_{\\mathrm{w}})}{a_{N}}\\right\\}\\rightarrow0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Thus, we prove the part of consistency of variable selection. ", "page_idx": 15}, {"type": "text", "text": "B.3 Proof of Proposition 1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We first give a lemma for general optimal functions. ", "page_idx": 15}, {"type": "text", "text": "Lemma 1. Assume that $h(\\pmb{x})^{2}$ and $\\varphi({\\pmb x})$ are integrable function with $\\mathbb{E}\\{\\varphi({\\pmb x})\\}=1$ The optimal function \\*\\*(\u03b1) that minimize the value E } is given as $\\begin{array}{r}{\\varphi^{**}(\\pmb{x})=\\frac{h(\\pmb{x})}{\\mathbb{E}\\{h(\\pmb{x})\\}}}\\end{array}$ ", "page_idx": 15}, {"type": "text", "text": "Proof. Appying Cauchy-Schwartz inequality, we have that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}\\{h(x)\\}^{2}=\\mathbb{E}\\left\\{\\frac{h(x)}{\\sqrt{\\varphi(x)}}\\sqrt{\\varphi(x)}\\right\\}^{2}\\leq\\mathbb{E}\\left\\{\\frac{h^{2}(x)}{\\varphi(x)}\\right\\}\\mathbb{E}\\{\\varphi(x)\\}=\\mathbb{E}\\left\\{\\frac{h^{2}(x)}{\\varphi(x)}\\right\\}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "$\\mathbb{E}\\left\\{{\\frac{h^{2}(\\pmb{x})}{\\varphi(\\pmb{x})}}\\right\\}\\geq\\mathbb{E}\\{h(\\pmb{x})\\}^{2}$ $\\sqrt{\\varphi(\\pmb{x})}=$ $K h(\\mathbf{x})/\\sqrt{\\varphi(\\mathbf{x})}$ where $K$ is a constant. Therefore, $\\varphi^{**}({\\pmb x})=K h({\\pmb x})$ , and since $\\mathbb{E}\\{\\varphi^{**}({\\pmb x})\\}=1$ we know that $\\varphi^{**}({\\pmb x})=h({\\pmb x})/\\mathbb{E}\\{h({\\pmb x})\\}$ \u53e3 ", "page_idx": 15}, {"type": "text", "text": "Now, we prove Proposition 1. ", "page_idx": 15}, {"type": "text", "text": "Proof. We first calculate the optimal function that minimizes $\\mathrm{tr}(V_{\\mathrm{w}(A)})$ . We have that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{tr}\\bigl(V_{\\mathrm{w}(\\mathcal{A})}\\bigr)=\\mathrm{tr}\\left\\{M_{(\\mathcal{A})}^{-1}M_{\\mathrm{w}(\\mathcal{A})}M_{(\\mathcal{A})}^{-1}\\right\\}}\\\\ &{\\mathrm{~}=\\mathrm{tr}\\left\\{M_{(\\mathcal{A})}^{-1}\\mathbb{E}\\left[\\left\\{1+\\frac{C e^{f(x;\\beta_{\\mathrm{t}})}}{\\varphi(x)}\\right\\}e^{f(x;\\beta_{\\mathrm{t}})}\\dot{g}_{(\\mathcal{A})}^{\\otimes2}(x;\\theta_{\\mathrm{t}})\\right]M_{(\\mathcal{A})}^{-1}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We focus on the values that related to $\\varphi(x)$ . We know that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left\\{\\varphi^{-1}({\\pmb x})e^{2f({\\pmb x};{\\pmb\\beta}_{\\ell})}\\dot{g}_{(A)}^{\\otimes2}({\\pmb x};{\\pmb\\theta}_{\\mathrm{t}})\\right\\}=e^{-2\\alpha_{\\mathrm{t}}}\\{1+o_{P}(1)\\}\\mathbb{E}\\left\\{\\varphi^{-1}({\\pmb x})p^{2}({\\pmb x};{\\pmb\\theta}_{\\mathrm{t}})\\dot{g}_{(A)}^{\\otimes2}({\\pmb x};{\\pmb\\theta}_{\\mathrm{t}})\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore, we need to minimize ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{tr}\\left[\\ensuremath{{\\mathbb E}}\\left\\{\\frac{p^{2}(x;\\theta_{\\mathrm{t}})M_{(A)}^{-1}\\dot{g}_{(A)}^{\\otimes2}(x;\\theta_{\\mathrm{t}})M_{(A)}^{-1}}{\\varphi(x)}\\right\\}\\right]}\\\\ &{=\\mathbb{E}\\left[\\mathrm{tr}\\left\\{\\frac{p^{2}(x;\\theta_{\\mathrm{t}})M_{(A)}^{-1}\\dot{g}_{(A)}^{\\otimes2}(x;\\theta_{\\mathrm{t}})M_{(A)}^{-1}}{\\varphi(x)}\\right\\}\\right]=\\mathbb{E}\\left[\\frac{p^{2}(x;\\theta_{\\mathrm{t}})\\|M_{(A)}^{-1}\\dot{g}_{(A)}(x;\\theta_{\\mathrm{t}})\\|^{2}}{\\varphi(x)}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Appying Lemma 1. We know that the minimizer is given as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\varphi_{\\mathrm{A-OS}}({\\pmb x})=\\frac{p({\\pmb x};{\\pmb\\theta}_{\\mathrm{t}})\\|{\\pmb M}_{({\\pmb A})}^{-1}{\\dot{g}}_{(A)}({\\pmb x};{\\pmb\\theta}_{\\mathrm{t}})\\|}{\\mathbb{E}\\left\\{p({\\pmb x};{\\pmb\\theta}_{\\mathrm{t}})\\|{\\pmb M}_{({\\pmb A})}^{-1}{\\dot{g}}_{(A)}({\\pmb x};{\\pmb\\theta}_{\\mathrm{t}})\\|\\right\\}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Next, we calculate the optimal function that minimize $\\mathrm{tr}(M_{\\mathrm{w}(\\mathcal{A})})$ . We have that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{tr}(M_{\\mathrm{w}(A)})=\\mathrm{tr}\\left\\{\\mathbb{E}\\left[\\left\\{1+{\\frac{c e^{f({\\pmb x};\\beta_{\\mathrm{t}})}}{\\varphi({\\pmb x})}}\\right\\}e^{f({\\pmb x};\\beta_{\\mathrm{t}})}\\dot{g}_{(A)}^{\\otimes2}({\\pmb x};{\\pmb\\theta}_{\\mathrm{t}})\\right]\\right\\}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore, we need to minimize ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{tr}\\left[\\mathbb{E}\\left\\{\\frac{p^{2}({\\pmb x};{\\pmb\\theta}_{\\mathrm{t}})\\dot{g}_{(A)}^{\\otimes2}({\\pmb x};{\\pmb\\theta}_{\\mathrm{t}})}{\\varphi({\\pmb x})}\\right\\}\\right]=\\mathbb{E}\\left[\\frac{p^{2}({\\pmb x};{\\pmb\\theta}_{\\mathrm{t}})\\|\\dot{g}_{(A)}({\\pmb x};{\\pmb\\theta}_{\\mathrm{t}})\\|^{2}}{\\varphi({\\pmb x})}\\right].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Appying Lemma 1. We know that the minimizer is given as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\varphi_{\\mathrm{L-OS}}(\\pmb{x})=\\frac{p(\\pmb{x};\\pmb{\\theta}_{\\mathrm{t}})\\|\\dot{g}_{(A)}(\\pmb{x};\\pmb{\\theta}_{\\mathrm{t}})\\|}{\\mathbb{E}\\left\\{p(\\pmb{x};\\pmb{\\theta}_{\\mathrm{t}})\\|\\dot{g}_{(A)}(\\pmb{x};\\pmb{\\theta}_{\\mathrm{t}})\\|\\right\\}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "B.4 Proof of Theorem 2 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof. In the proof of Thereom 1, we know that ", "page_idx": 16}, {"type": "equation", "text": "$$\na_{N}\\bigl(\\hat{\\pmb{\\theta}}_{\\mathrm{w}(A)}^{\\mathrm{adp}}-\\pmb{\\theta}_{\\mathrm{t}(A)}\\bigr)\\mapsto M_{(A)}^{-1}M_{\\mathrm{w}(A)}^{1/2}\\pmb{Z}_{(A)}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "To simplify the representation, We define a function ", "page_idx": 16}, {"type": "equation", "text": "$$\nh(\\pmb\\theta)=e^{-2\\alpha_{\\mathrm{t}}}\\mathbf{MSPE}(\\pmb\\theta)=e^{-2\\alpha_{\\mathrm{t}}}\\mathbb{E}\\left[\\{p(\\pmb x;\\pmb\\theta)-p(\\pmb x;\\pmb\\theta_{\\mathrm{t}})\\}^{2}\\right].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We have that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{\\partial\\left\\{p(\\pmb{x};\\pmb{\\theta})-p(\\pmb{x};\\pmb{\\theta}_{\\mathrm{t}})\\right\\}^{2}}{\\partial\\pmb{\\theta}}=2\\left\\{p(\\pmb{x};\\pmb{\\theta})-p(\\pmb{x};\\pmb{\\theta}_{\\mathrm{t}})\\right\\}\\phi(\\pmb{x};\\pmb{\\theta})\\dot{g}(\\pmb{x};\\pmb{\\theta}),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\frac{\\partial^{2}\\left\\{p(x;\\theta)-p(x;\\theta_{t})\\right\\}^{2}}{\\partial\\theta\\partial\\theta^{\\mathrm{T}}}=2\\phi^{2}(x;\\theta)\\dot{g}^{\\otimes2}(x;\\theta)+2\\left\\{p(x;\\theta)-p(x;\\theta_{t})\\right\\}\\frac{\\partial\\phi(x;\\theta)}{\\partial\\theta}\\dot{g}(x;\\theta)}\\\\ &{}&{\\qquad+\\;2\\left\\{p(x;\\theta)-p(x;\\theta_{t})\\right\\}\\phi(x;\\theta)\\ddot{g}(x;\\theta).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Note that $|p(\\mathbf{\\boldsymbol{x}};\\mathbf{\\boldsymbol{\\theta}})-p(\\mathbf{\\boldsymbol{x}};\\mathbf{\\boldsymbol{\\theta}}_{\\mathrm{t}})|\\leq2$ and thus, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left|\\frac{\\partial\\left\\{p(\\mathbf{x};\\theta)-p(\\mathbf{x};\\theta_{\\mathrm{t}})\\right\\}^{2}}{\\partial\\theta}\\right|\\leq2\\left|p(\\mathbf{x};\\theta)-p(\\mathbf{x};\\theta_{\\mathrm{t}})\\right||\\phi(\\mathbf{x};\\theta)\\dot{g}(\\mathbf{x};\\theta)|\\leq4B(\\mathbf{x}),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\frac{\\partial^{2}\\left\\{p(x;\\theta)-p(x;\\theta_{\\mathrm{t}})\\right\\}^{2}}{\\partial\\theta\\partial\\theta^{\\mathrm{T}}}\\right|}\\\\ &{\\leq2\\left|\\phi^{2}(x;\\theta)\\dot{g}^{\\otimes2}(x;\\theta)\\right|+2\\left|\\left\\{p(x;\\theta)-p(x;\\theta_{\\mathrm{t}})\\right\\}\\right|\\left|\\frac{\\partial\\phi(x;\\theta)}{\\partial t}\\dot{g}(x;\\theta)\\right|}\\\\ &{\\quad+\\,2\\left|\\left\\{p(x;\\theta)-p(x;\\theta_{\\mathrm{t}})\\right\\}\\right|\\left|\\phi(x;\\theta)\\ddot{g}(x;\\theta)\\right|\\leq10B(x).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Hence, due to donimating convergence theorem, we know that the expectation and derivitive are exchangable. Thus, we have that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{\\partial h(\\pmb\\theta)}{\\partial\\pmb\\theta}=e^{-2\\alpha_{\\mathrm{t}}}\\mathbb{E}\\left[2\\left\\{p(\\pmb x;\\pmb\\theta)-p(\\pmb x;\\pmb\\theta_{\\mathrm{t}})\\right\\}\\phi(\\pmb x;\\pmb\\theta)\\dot{g}(\\pmb x;\\pmb\\theta)\\right],\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\partial h(\\pmb{\\theta})}{\\partial\\theta\\partial\\theta^{\\mathrm{T}}}=e^{-2\\alpha_{\\mathrm{t}}}\\mathbb{E}\\Big[2\\phi^{2}(\\pmb{x};\\pmb{\\theta})\\dot{g}^{\\otimes2}(\\pmb{x};\\pmb{\\theta})+2\\left\\{p(\\pmb{x};\\pmb{\\theta})-p(\\pmb{x};\\pmb{\\theta}_{\\mathrm{t}})\\right\\}\\frac{\\partial\\phi(\\pmb{x};\\pmb{\\theta})}{\\partial\\theta}\\dot{g}(\\pmb{x};\\pmb{\\theta})}\\\\ {\\displaystyle\\qquad+\\;2\\left\\{p(\\pmb{x};\\pmb{\\theta})-p(\\pmb{x};\\pmb{\\theta}_{\\mathrm{t}})\\right\\}\\phi(\\pmb{x};\\pmb{\\theta})\\ddot{g}(\\pmb{x};\\pmb{\\theta})\\Big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Due to donimating convergence theorem, we also know that the first and second derivitive of $h(\\pmb\\theta)$ are continous. We have that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{\\partial h(\\pmb\\theta)}{\\partial\\pmb\\theta}\\Big|_{\\pmb\\theta=\\pmb\\theta_{\\mathrm{t}}}=\\mathbf0,\\frac{\\partial^{2}h(t)}{\\partial\\pmb\\theta\\partial\\pmb\\theta^{\\mathrm{T}}}\\Big|_{\\pmb\\theta=\\pmb\\theta_{\\mathrm{t}}}=2e^{-2\\alpha_{\\mathrm{t}}}\\mathbb E\\left[\\phi^{2}(\\pmb x;\\pmb\\theta_{\\mathrm{t}})\\dot{g}^{\\otimes2}(\\pmb x;\\pmb\\theta_{\\mathrm{t}})\\right]\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Note that $e^{-2\\alpha_{\\mathrm{t}}}\\mathbb{E}\\left[\\phi^{2}(\\pmb{x};\\pmb{\\theta}_{\\mathrm{t}})\\dot{g}^{\\otimes2}(\\pmb{x};\\pmb{\\theta}_{\\mathrm{t}})\\right]\\rightarrow\\Omega$ due to donimating covergence theorem. Now applying Theorem 1.12(ii) in [i8], we have that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{a_{N}^{2}\\left\\{h(\\widehat{\\pmb{\\theta}}_{\\mathrm{w}(A)}^{\\mathrm{adp}})-h(\\pmb{\\theta}_{\\mathrm{t}(A)})\\right\\}=a_{N}^{2}e^{-2\\alpha_{\\mathrm{t}}}\\mathbb{E}\\left[\\Big\\{p(\\pmb{x};\\widehat{\\pmb{\\theta}}_{\\mathrm{w}(A)})-p(\\pmb{x};\\pmb{\\theta}_{\\mathrm{t}(A)})\\Big\\}^{2}\\right]}\\\\ &{\\mapsto\\frac{1}{2!}2\\pmb{Z}_{(A)}^{\\mathrm{T}}M_{\\mathrm{w}(A)}^{1/2}M_{(A)}^{-1}\\Omega_{(A)}M_{(A)}^{-1}M_{\\mathrm{w}(A)}^{1/2}\\pmb{Z}_{(A)}}\\\\ &{=\\pmb{Z}_{(A)}^{\\mathrm{T}}M_{\\mathrm{w}(A)}^{1/2}M_{(A)}^{-1}\\Omega_{(A)}M_{(A)}^{-1}M_{\\mathrm{w}(A)}^{1/2}\\pmb{Z}_{(A)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Considering $N_{1}=a_{N}^{2}\\mathbb{E}\\left\\{e^{f({\\boldsymbol{x}};\\beta_{\\mathrm{t}})}\\right\\}\\left\\{1+o_{P}(1)\\right\\}$ applying Sutsky's theorem, wehave that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{N_{1}e^{-2\\alpha_{\\mathrm{t}}}\\mathbb{E}\\left[\\left\\{p(x;\\hat{\\theta}_{\\mathrm{w}(A)})-p(x;\\theta_{\\mathrm{t}(A)})\\right\\}^{2}\\right]}\\\\ &{\\rightsquigarrow\\mathbb{E}^{-1}\\left\\{e^{f(x;\\beta_{\\mathrm{t}})}\\right\\}Z_{(A)}^{\\mathrm{T}}M_{\\mathrm{w}(A)}^{1/2}M_{(A)}^{-1}\\Omega_{(A)}M_{(A)}^{-1}M_{\\mathrm{w}(A)}^{1/2}Z_{(A)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Since $Z_{\\mathrm{w}(A)}=\\mathbb{N}(\\mathbf{0},I)$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left\\{Z_{(A)}^{\\mathrm{T}}M_{\\mathrm{w}(A)}^{1/2}M_{(A)}^{-1}\\Omega_{(A)}M_{(A)}^{-1}M_{\\mathrm{w}(A)}^{1/2}Z_{(A)}\\right\\}=\\mathrm{tr}\\left\\{M_{(A)}^{-1}\\Omega_{(A)}M_{(A)}^{-1}M_{\\mathrm{w}(A)}\\right\\}}\\\\ &{\\,=\\mathrm{tr}\\left\\{M_{(A)}^{-1}\\Omega_{(A)}M_{(A)}^{-1}\\mathbb{E}\\left[\\left\\{1+\\frac{c e^{f(\\mathbf{x};\\beta_{\\mathrm{t}})}}{\\varphi(\\mathbf{x})}\\right\\}e^{f(\\mathbf{x};\\beta_{\\mathrm{t}})}\\dot{g}_{(A)}^{\\otimes2}(\\mathbf{x};\\theta_{\\mathrm{t}})\\right]\\cdot\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We focus on the values that related to $\\varphi(x)$ . We know that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left\\{\\varphi^{-1}({\\pmb x})e^{2f({\\pmb x};{\\pmb\\beta}_{\\ell})}\\dot{g}_{(A)}^{\\otimes2}({\\pmb x};{\\pmb\\theta}_{\\mathrm{t}})\\right\\}=e^{-2\\alpha_{\\mathrm{t}}}\\{1+o_{P}(1)\\}\\mathbb{E}\\left\\{\\varphi^{-1}({\\pmb x})p^{2}({\\pmb x};{\\pmb\\theta}_{\\mathrm{t}})\\dot{g}_{(A)}^{\\otimes2}({\\pmb x};{\\pmb\\theta}_{\\mathrm{t}})\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, we need to minimize ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{tr}\\left[{\\mathbb E}\\left\\{M_{(A)}^{-1}\\Omega_{(A)}M_{(A)}^{-1}\\frac{p^{2}(x;\\theta_{1})\\dot{g}_{(A)}^{(2)}(x;\\theta_{1})}{\\varphi(x)}}\\right\\}\\right]}\\\\ &{={\\mathbb E}\\left[\\mathrm{tr}\\left\\{{M_{(A)}^{-1}\\Omega_{(A)}M_{(A)}^{-1}\\frac{p^{2}(x;\\theta_{1})\\dot{g}_{(A)}^{(2)}(x;\\theta_{1})}{\\varphi(x)}}\\right\\}\\right]}\\\\ &{={\\mathbb E}\\left[\\mathrm{tr}\\left\\{{\\frac{p^{2}(x;\\theta_{1}){\\Omega_{(A)}^{1/2}}M_{(A)}^{-1}\\dot{g}_{(A)}^{(2)}(x;\\theta_{1})M_{(A)}^{-1}\\Omega_{(A)}^{1/2}}{\\varphi(x)}}\\right\\}\\right]}\\\\ &{={\\mathbb E}\\left[\\frac{p^{2}(x;\\theta_{1}){\\Omega_{(A)}^{1/2}}M_{(A)}^{-1}\\dot{g}_{(A)}(x;\\theta_{1})|^{2}}{\\varphi(x)}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Appying Lemma 1. We know that the minimizer is given as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\varphi_{\\mathrm{P-OS}}({\\pmb x})=\\frac{p({\\pmb x};{\\pmb\\theta}_{\\mathrm{t}})\\|\\Omega_{(A)}^{1/2}M_{(A)}^{-1}{\\dot{g}}_{(A)}({\\pmb x};{\\pmb\\theta}_{\\mathrm{t}})\\|}{{\\mathbb{E}}\\left\\{p({\\pmb x};{\\pmb\\theta}_{\\mathrm{t}})\\|\\Omega_{(A)}^{1/2}M_{(A)}^{-1}{\\dot{g}}_{(A)}({\\pmb x};{\\pmb\\theta}_{\\mathrm{t}})\\|\\right\\}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "B.5 Proof of Proposition 2 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proof. First, we know that $g(\\pmb{x};\\pmb{\\theta})=g(\\pmb{A}\\pmb{x};\\pmb{B}^{\\mathrm{T}}\\pmb{\\theta})$ . Since the equation holds for all $\\textbf{\\em x}$ and $\\pmb{\\theta}$ ,ifwe take derivitive with respect to $\\pmb{\\theta}$ on both sides, the equation still holds. Thus, we have that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\dot{g}(\\pmb{x};\\pmb{\\theta})=B\\dot{g}(A\\pmb{x};B^{\\mathrm{T}}\\pmb{\\theta}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "If we scale the whole covariate variable $\\textbf{\\em x}$ to $\\tilde{\\pmb{x}}=\\pmb{A x}$ , we need to reparameterize $\\theta_{\\mathrm{t}}$ to $\\tilde{\\pmb{\\theta}}_{\\mathrm{t}}=\\mathbf{B}^{\\mathrm{T}}\\pmb{\\theta}_{\\mathrm{t}}$ to remain the problem invariant. We have that for $\\tilde{\\pmb{x}}$ and $\\tilde{\\pmb{\\theta}}_{\\mathrm{t}}$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\dot{g}(\\pmb{x};\\tilde{\\pmb{\\theta}}_{\\mathrm{t}})=\\dot{g}(\\pmb{A}\\pmb{x};\\pmb{B}^{\\mathrm{T}}\\pmb{\\theta}_{\\mathrm{t}})=\\pmb{B}^{-1}\\dot{g}(\\pmb{x};\\pmb{\\theta}_{\\mathrm{t}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Now, we know that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\tilde{M}=\\mathbb{E}\\{e^{f(\\tilde{x};\\tilde{\\beta}_{\\mathfrak{t}})}\\dot{g}^{\\otimes2}(\\tilde{x};\\tilde{\\theta}_{\\mathfrak{t}})\\}=B^{-1}\\mathbb{E}\\{e^{f(x;\\beta_{\\mathfrak{t}})}\\dot{g}^{\\otimes2}(x;\\theta_{\\mathfrak{t}})\\}(B^{\\mathrm{T}})^{-1}=B^{-1}M(B^{\\mathrm{T}})^{-1},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\tilde{\\Omega}=\\mathbb{E}\\{e^{2f(\\tilde{x};\\tilde{\\beta}_{\\mathfrak{t}})}\\dot{g}^{\\otimes2}(\\tilde{x};\\tilde{\\theta}_{\\mathfrak{t}})\\}=B^{-1}\\mathbb{E}\\{e^{2f(x;\\theta_{\\mathfrak{t}})}\\dot{g}^{\\otimes2}(x;\\theta_{\\mathfrak{t}})\\}(B^{\\mathrm{T}})^{-1}=B^{-1}\\Omega(B^{\\mathrm{T}})^{-1}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Thus, we have that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\tilde{\\Omega}^{1/2}\\tilde{M}^{-1}\\dot{g}(\\tilde{x};\\tilde{\\theta}_{\\mathrm{t}})\\|^{2}=\\dot{g}^{\\mathrm{T}}(\\tilde{x};\\tilde{\\theta}_{\\mathrm{t}})\\tilde{M}^{-1}\\tilde{\\Omega}\\tilde{M}^{-1}\\dot{g}(\\tilde{x};\\tilde{\\theta}_{\\mathrm{t}})}\\\\ &{=\\dot{g}^{\\mathrm{T}}(x;\\theta_{\\mathrm{t}})(B^{-1})^{\\mathrm{T}}(B^{\\mathrm{T}})M^{-1}B B^{-1}\\Omega(B^{\\mathrm{T}})^{-1}B^{\\mathrm{T}}M^{-1}B B^{-1}\\dot{g}(x;\\theta_{\\mathrm{t}})}\\\\ &{=\\dot{g}^{\\mathrm{T}}(x;\\theta_{\\mathrm{t}})M^{-1}\\Omega M^{-1}\\dot{g}(x;\\theta_{\\mathrm{t}})=\\|\\Omega^{1/2}M^{-1}\\dot{g}(x;\\theta_{\\mathrm{t}})\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Therefore, the leveraging term is invariant. For the probability term, we know that is only related tovalue $g(\\pmb{x};\\pmb{\\theta}_{\\mathrm{t}})=g(\\tilde{\\pmb{x}};\\tilde{\\pmb{\\theta}}_{\\mathrm{t}})$ , we know that it does not change after scaling inactive variables. This complete the proof. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "B.6Proof of Theorem 3 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proof of Theorem 3. Consider maximum sampled conditional likeihood function with adaptive lasso penalty: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{Q_{\\mathrm{mscl}}^{\\hat{\\theta}_{\\mathrm{pl}}}(\\pmb\\theta)=-\\displaystyle\\sum_{i=1}^{N}\\delta_{i}^{\\hat{\\theta}_{\\mathrm{pl}}}[y_{i}g(\\pmb x_{i};\\pmb\\theta)-\\log\\{1+e^{g(\\pmb x_{i};\\pmb\\theta)+l_{i}}\\}]+\\lambda_{N}\\displaystyle\\sum_{j=1}^{p}\\hat{w}_{j}|\\beta_{(j)}[y_{\\pmb\\theta}]}}\\\\ {{=-\\ell_{\\mathrm{mscl}}^{\\hat{\\theta}_{\\mathrm{pl}}}(\\pmb\\theta)+\\lambda_{N}\\displaystyle\\sum_{j=1}^{p}\\hat{w}_{j}|\\beta_{(j)}|,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\hat{w}_{j}=1/|\\hat{\\beta}_{\\mathrm{pl}(j)}|^{\\gamma},1\\le j\\le p$ Then, we have that $\\hat{\\pmb u}_{N}=a_{N}(\\hat{\\pmb\\theta}_{\\mathrm{mscl}}^{\\hat{\\pmb\\theta}_{\\mathrm{pl}}}-\\pmb\\theta_{\\mathrm{t}})$ is the minimizer of ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\gamma_{\\mathrm{mscl}}^{\\hat{\\theta}_{\\mathrm{pl}}}(\\pmb{u})=Q_{\\mathrm{mscl}}^{\\hat{\\theta}_{\\mathrm{pl}}}(\\pmb{\\theta}_{\\mathrm{t}}+a_{N}^{-1}\\pmb{u})-Q_{\\mathrm{mscl}}^{\\hat{\\theta}_{\\mathrm{pl}}}(\\pmb{\\theta}_{\\mathrm{t}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Asymptotic normality: We prove the asymptotic normality part in this paragraph. By Taylor's expansion, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\gamma_{\\mathrm{mscl}}^{\\hat{\\theta}_{\\mathrm{pl}}}(u)=-\\frac{1}{a_{N}}u^{\\mathrm{T}}\\hat{\\ell}_{\\mathrm{mscl}}^{\\hat{\\theta}_{\\mathrm{pl}}}(\\theta_{\\mathrm{t}})+\\frac{1}{2a_{N}^{2}}\\displaystyle\\sum_{i=1}^{N}\\delta_{i}^{\\hat{\\theta}_{\\mathrm{pl}}}\\phi_{\\pi}^{\\hat{\\theta}_{\\mathrm{pl}}}(\\pmb{x}_{i};\\theta_{\\mathrm{t}})\\{u^{\\mathrm{T}}\\dot{g}(\\pmb{x}_{i};\\theta_{\\mathrm{t}})\\}^{2}-\\Delta_{\\mathrm{mscl}}^{\\hat{\\theta}_{\\mathrm{pl}}}+R_{\\mathrm{mscl}}^{\\hat{\\theta}_{\\mathrm{pl}}}}\\\\ &{\\quad\\quad\\quad\\quad+\\displaystyle\\frac{\\lambda_{N}}{a_{N}}\\displaystyle\\sum_{j=1}^{p}\\hat{w}_{j}a_{N}\\left(\\left|\\beta_{\\mathrm{t}(j)}+\\frac{u_{(j)}}{a_{N}}\\right|-|\\beta_{\\mathrm{t}(j)}|\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "First, we consider the limit behavior of the MSCL function. In [22], the authors proved that under Assumptions 1 and 3, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{a_{N}^{-1}\\dot{\\ell}_{\\mathrm{mscl}}^{\\hat{\\theta}_{\\mathrm{pl}}}(\\theta_{\\mathrm{t}})\\mapsto(\\mathbf{A}_{\\mathrm{mscl}}^{\\mathrm{pl}})^{1/2}Z.}}\\\\ {{\\displaystyle\\frac1{a_{N}^{2}}\\sum_{i=1}^{N}\\delta_{i}^{\\hat{\\theta}_{\\mathrm{pl}}}\\phi_{\\pi}^{\\hat{\\theta}_{\\mathrm{pl}}}(\\pmb{x}_{i};\\pmb{\\theta}_{\\mathrm{t}})\\dot{g}^{\\otimes2}(\\pmb{x}_{i};\\pmb{\\theta}_{\\mathrm{t}})\\stackrel{P}{\\longrightarrow}\\mathbf{A}_{\\mathrm{mscl}}^{\\mathrm{pl}},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\Delta_{\\mathrm{mscl}}^{\\hat{\\theta}_{\\mathrm{pl}}}=o_{P}(1),\\quad R_{\\mathrm{mscl}}^{\\hat{\\theta}_{\\mathrm{pl}}}=o_{P}(1).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Thus, ", "page_idx": 19}, {"type": "equation", "text": "$$\n-\\ell_{\\mathrm{w}}^{\\hat{\\theta}_{\\mathrm{pl}}}(\\pmb{\\theta}_{\\mathrm{t}})\\rightsquigarrow-\\pmb{u}^{\\mathrm{T}}(\\mathbf{A}_{\\mathrm{mscl}}^{\\mathrm{pl}})^{1/2}\\pmb{Z}+\\frac{1}{2}\\pmb{u}^{\\mathrm{T}}\\mathbf{A}_{\\mathrm{mscl}}^{\\mathrm{pl}}\\pmb{u}+o_{P}(1).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Next, we consider the limit behavior of the adaptive lasso penalty. Since we assume $\\hat{\\beta}_{\\mathrm{pl}}$ tpbe a consistent estimator, we know that when $j\\in A$ i.e., $\\beta_{\\mathrm{t}(j)}\\neq0$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{w}_{j}=|\\hat{\\beta}_{\\mathrm{pl}(j)}|^{-\\gamma}\\xrightarrow{P}|\\beta_{\\mathrm{t}(j)}|^{-\\gamma}>0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and ", "page_idx": 19}, {"type": "equation", "text": "$$\na_{N}\\left(\\left|\\beta_{\\mathfrak{t}(j)}+\\frac{u_{(j)}}{a_{N}}\\right|-|\\beta_{\\mathfrak{t}(j)}|\\right)\\to\\mathbf{sgn}(\\beta_{\\mathfrak{t}(j)})u_{(j)}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Therefore, for $j\\in\\mathcal{A}$ , we have that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{\\lambda_{N}}{a_{N}}\\hat{w}_{j}a_{N}\\left(\\left|\\beta_{\\mathfrak{t}(j)}+\\frac{u_{(j)}}{a_{N}}\\right|-|\\beta_{\\mathfrak{t}(j)}|\\right)\\xrightarrow{P}0,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "since $\\lambda_{N}/a_{N}=\\lambda_{N}/\\sqrt{N e^{\\alpha_{\\mathrm{t}}}}\\to0$ On the other hand, when $j\\in\\mathcal{A}^{c}$ ,i.e, $\\beta_{\\mathrm{t}(j)}=0$ ,we have that for $u_{(j)}\\neq0$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{\\lambda_{N}}{a_{N}}\\hat{w}_{j}a_{N}\\left(\\left|\\beta_{\\mathfrak{t}(j)}+\\frac{u_{(j)}}{a_{N}}\\right|-|\\beta_{\\mathfrak{t}(j)}|\\right)=\\frac{\\lambda_{N}}{a_{N}}\\hat{w}_{j}|u_{(j)}|=\\frac{\\lambda_{N}}{a_{N}|\\hat{\\beta}_{\\mathfrak{p}1(j)}|\\gamma}|u_{(j)}|\\stackrel{P}{\\longrightarrow}\\infty,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "since $\\lambda_{N}/(\\sqrt{N e^{\\alpha_{\\mathrm{t}}}}|\\hat{\\beta}_{\\mathrm{pl}(j)}|^{\\gamma})\\xrightarrow{P}\\infty$ Then, we have that $\\gamma_{\\mathrm{mscl}}^{\\hat{\\theta}_{\\mathrm{pl}}}(\\pmb{u})\\rightsquigarrow\\gamma_{\\mathrm{mscl}}(\\pmb{u})$ where ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\gamma_{\\mathrm{mscl}}(u)=\\left\\{\\begin{array}{l l}{\\frac{1}{2}u_{(A)}^{\\mathrm{T}}\\Lambda_{\\mathrm{mscl}}^{\\mathrm{pl}}u_{(A)}-u_{(A)}^{\\mathrm{T}}(\\Lambda_{\\mathrm{mscl}}^{\\mathrm{pl}})^{1/2}Z_{(A)}}&{\\mathrm{if~}u_{(j)}=0,\\forall j\\notin A}\\\\ {\\infty}&{\\mathrm{otherwise}.}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Note that the unique minimizer of $\\gamma_{\\mathrm{mscl}}(u)$ .s $((\\mathbf{A}_{\\mathrm{mscl}}^{\\mathrm{pl}})^{-1}Z_{(A)}^{\\mathrm{T}},\\mathbf{0})^{\\mathrm{T}}$ if we put all the indexes of $\\gamma_{\\mathrm{mscl}}^{\\hat{\\theta}_{\\mathrm{pl}}}(u),\\,\\hat{\\pmb u}_{N}$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\hat{u}_{N(A)}\\sim\\bigl(\\Lambda_{\\mathrm{mscl}}^{\\mathrm{pl}}\\bigr)^{1/2}Z_{(A)}\\;\\mathrm{and}\\;\\hat{u}_{N(A^{c})}\\rightsquigarrow\\mathbf{0}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Thus, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\hat{\\pmb u}_{N(A)}=a_{N}(\\hat{\\pmb\\theta}_{\\mathrm{mscl}(A)}-\\pmb\\theta_{\\mathrm{t}(A)})\\rightsquigarrow\\mathbb{N}(\\mathbf{0},(\\mathbf{A}_{\\mathrm{mscl}}^{\\mathrm{pl}})^{-1}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We know that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sqrt{N_{1}}V_{\\mathrm{mscl}}^{-1/2}(\\Lambda_{\\mathrm{mscl}}^{\\mathfrak{p}1})^{-1}=a_{N}\\mathbb{E}^{1/2}\\left\\{e^{f({\\boldsymbol{\\alpha}};\\beta_{\\mathrm{t}})}\\right\\}\\mathbb{E}^{-1/2}\\left\\{e^{f({\\boldsymbol{\\alpha}};\\beta_{\\mathrm{t}})}\\right\\}\\left\\{1+o_{P}(1)\\right\\}=a_{N}\\left\\{1+o_{P}(1)\\right\\}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Hence, applying Slusky's theorem, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sqrt{N_{1}}V_{\\mathrm{mscl}(A)}^{-1/2}(\\hat{\\theta}_{\\mathrm{mscl}(A)}-\\theta_{\\mathrm{t}(A)})\\rightsquigarrow\\mathbb{N}(\\mathbf{0},I).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Therefore, we prove the part of aymptotic normality. ", "page_idx": 19}, {"type": "text", "text": "Consistency in variable selection  We prove the consistency in variable selection in this paragraph. From the result of asymptotic normality, we know that ${\\hat{\\beta}}_{\\mathrm{mscl}(j)}\\ \\xrightarrow{P}\\ \\beta_{\\mathrm{t}(j)}$ for every $j\\,\\in\\,{\\mathcal{A}}$ and therefore $\\mathbb{P}(j\\in\\hat{\\mathcal{A}}_{\\mathrm{mscl}})\\to1$ . Thus, we only consider $j^{\\prime}\\in\\mathcal{A}^{c}$ . When $j^{\\prime}\\in\\hat{A}_{\\mathrm{mscl}}$ we know that by K-K-T optimality conditions, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\lambda_{N}\\hat{w}_{j^{\\prime}}\\mathrm{sgn}(\\hat{\\beta}_{\\mathrm{mscl}(j^{\\prime})})=\\dot{\\ell}_{\\mathrm{mscl}}^{\\hat{\\theta}_{\\mathrm{pl}}}(\\hat{\\pmb{\\theta}}_{\\mathrm{mscl}}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which means ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{\\lambda_{N}\\hat{w}_{j^{\\prime}}\\mathrm{sgn}(\\hat{\\beta}_{\\mathrm{mscl}(j^{\\prime})})}{a_{N}}=\\frac{\\dot{\\ell}_{\\mathrm{mscl}}^{\\hat{\\theta}_{\\mathrm{pl}}}(\\hat{\\pmb{\\theta}}_{\\mathrm{mscl}})}{a_{N}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n=\\frac{\\dot{\\ell}_{\\mathrm{mscl}}^{\\hat{\\theta}_{\\mathrm{pl}}}(\\theta_{\\mathrm{t}})}{a_{N}}+\\frac{a_{N}\\left\\{\\dot{\\ell}_{\\mathrm{mscl}}^{\\hat{\\theta}_{\\mathrm{pl}}}(\\hat{\\theta}_{\\mathrm{mscl}}^{\\hat{\\theta}_{\\mathrm{pl}}})-\\dot{\\ell}_{\\mathrm{mscl}}^{\\hat{\\theta}_{\\mathrm{pl}}}(\\theta_{\\mathrm{t}})\\right\\}}{a_{N}^{2}}=I_{1}+I_{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We have known that $I_{1}=\\dot{\\ell}_{\\mathrm{mscl}}^{\\hat{\\theta}_{\\mathrm{pl}}}(\\pmb{\\theta}_{\\mathrm{t}})/a_{N}\\backsim Z_{\\mathrm{mscl}}$ We now prove tat proo that $I_{2}=O_{P}(1)$ we apply Taylor expansion to the $k$ -th element of $\\dot{\\ell}_{\\mathrm{mscl}}^{\\hat{\\theta}_{\\mathrm{pl}}}(\\hat{\\theta}_{\\mathrm{mscl}}^{\\hat{\\theta}_{\\mathrm{pl}}})$ and have that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{\\i_{N}\\left\\{\\dot{\\ell}_{\\mathrm{(}k)}^{\\theta_{\\mathrm{p1}}}(\\hat{\\theta}_{\\mathrm{mscl}}^{\\theta_{\\mathrm{p1}}})-\\dot{\\ell}_{\\mathrm{(}k)}^{\\theta_{\\mathrm{p1}}}(\\theta_{\\mathrm{t}})\\right\\}}{a_{N}^{2}}=-\\frac{1}{a_{N}^{2}}\\sum_{i=1}^{N}\\delta_{i}^{\\theta_{\\mathrm{p1}}}\\phi_{\\pi}^{\\theta_{\\mathrm{p1}}}(x_{i};\\theta_{\\mathrm{t}})\\dot{g}_{(k)}(x_{i};\\theta_{\\mathrm{t}})\\dot{g}^{\\mathrm{T}}(x_{i};\\theta_{\\mathrm{t}})\\dot{u}_{N}+\\tilde{\\Delta}_{(k)}^{\\theta_{\\mathrm{p1}}}+\\tilde{R}_{(k)}^{\\theta_{\\mathrm{p1}}},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\hat{\\pmb u}_{N}=a_{N}(\\hat{\\pmb\\theta}_{\\mathrm{mscl}}^{\\hat{\\pmb\\theta}_{\\mathrm{pl}}}-\\pmb\\theta_{\\mathrm{t}})=O_{P}(1),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\n\\tilde{\\Delta}_{\\mathrm{mscl}(k)}^{\\hat{\\theta}_{\\mathrm{pl}}}=\\frac{1}{a_{N}^{2}}\\sum_{i=1}^{N}\\delta_{i}^{\\hat{\\theta}_{\\mathrm{pl}}}\\left\\{y_{i}-p_{\\pi}^{\\hat{\\theta}_{\\mathrm{pl}}}(\\pmb{x}_{i};\\pmb{\\theta}_{\\mathrm{t}})\\right\\}\\sum_{j=1}^{d}\\ddot{g}_{(k j)}(\\pmb{x}_{i};\\pmb{\\theta}_{\\mathrm{t}})\\tilde{u}_{(j)},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\tilde{R}_{\\mathrm{mscl}(k)}^{\\theta_{\\mathrm{pi}}}=-\\frac{1}{2a_{N}^{3}}\\sum_{i=1}^{N}\\delta_{i}^{\\theta_{\\mathrm{pi}}}\\phi_{\\pi}^{\\theta_{\\mathrm{pi}}}(x_{i};\\theta_{k})\\left\\{1-2p_{\\pi}^{\\theta_{\\mathrm{pi}}}(x_{i};\\theta_{k})\\right\\}\\dot{g}_{(k)}(x_{i};\\theta_{k})\\dot{u}_{N}^{\\mathrm{T}}\\dot{g}^{\\otimes2}(x_{i};\\dot{\\theta}_{k})\\hat{u}_{N}}}\\\\ {{\\displaystyle}}\\\\ {{\\displaystyle}}\\\\ {{\\displaystyle}}\\\\ {{\\displaystyle}}\\\\ {{\\quad-\\,\\frac{2}{2a_{N}^{3}}\\sum_{i=1}^{N}\\delta_{i}^{\\theta_{\\mathrm{pi}}}\\phi_{\\pi}^{\\theta_{\\mathrm{pi}}}(x_{i};\\dot{\\theta}_{k})\\left\\{\\dot{u}_{N}^{\\mathrm{T}}\\frac{\\partial\\dot{g}_{(k)}(x_{i};\\dot{\\theta}_{k})}{\\partial\\theta}\\right\\}\\left\\{\\dot{u}_{N}^{\\mathrm{T}}\\dot{g}(x_{i};\\dot{\\theta}_{k})\\right\\}}}\\\\ {{\\displaystyle}}\\\\ {{\\displaystyle}}\\\\ {{\\displaystyle}}\\\\ {{\\displaystyle}}\\\\ {{\\displaystyle}}\\\\ {{\\displaystyle}}\\\\ {{\\displaystyle}}\\\\ {{\\displaystyle}+\\frac{1}{2a_{N}^{3}}\\sum_{i=1}^{N}\\delta_{i}^{\\theta_{\\mathrm{pi}}}\\phi_{\\pi}^{\\theta_{\\mathrm{pi}}}(x_{i};\\dot{\\theta}_{k})\\dot{g}_{(k)}(x_{i};\\dot{\\theta}_{k})\\left\\{\\dot{u}_{N}^{\\mathrm{T}}\\ddot{g}(x_{i};\\dot{\\theta}_{k})\\dot{u}_{N}\\right\\}}}\\\\ {{\\displaystyle}}\\\\ {{\\displaystyle}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\acute{\\theta}_{k}$ is between $\\hat{\\pmb{\\theta}}_{\\mathrm{mscl}}$ and $\\theta_{\\mathrm{t}}$ . First, we prove that $\\tilde{R}_{\\mathrm{mscl}(k)}$ .s $o_{P}(1)$ . We have that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\langle P_{n+1}^{(a_{k})}|\\sum_{i=1}^{3}\\Bigg\\{P_{k}^{(a_{k})}\\Bigg(a_{k}^{2}(a_{k})\\delta_{i}(a_{k})\\Bigg|-2\\pi_{k}^{(a_{k})}(a_{k})\\delta_{i}\\Bigg)\\Bigg|\\Bigg|\\Phi_{(0,i)}(a_{k})\\Bigg|\\Bigg\\}\\Bigg|\\bar{\\Phi}_{(0,i)}(a_{k})\\Bigg|^{2}}\\\\ &{+\\frac{2\\pi_{k}^{(a_{k})}}{2}\\sum_{i=1}^{3}\\delta_{i}^{(a_{k})}\\phi_{(0,i)}^{(a_{k})}(a_{k})\\Bigg|\\Bigg|\\bar{\\Phi}_{(0,i)}(a_{k})\\Bigg|\\Bigg|\\bar{\\Phi}_{(0,i)}(a_{k})\\Bigg|}\\\\ &{+\\frac{16\\pi_{k}^{(a_{k})}}{2}\\sum_{i=1}^{5}\\delta_{i}^{(a_{k})}\\phi_{(0,i)}^{(a_{k})}(a_{k})\\Bigg|\\bar{\\Phi}_{(0,i)}(a_{k})\\Bigg|\\Bigg|\\bar{\\Phi}_{(0,i)}(a_{k})\\Bigg|}\\\\ &{+\\frac{16\\pi_{k}^{(a_{k})}}{2}\\sum_{i=1}^{5}\\delta_{i}^{(a_{k})}\\phi_{(0,i)}^{(a_{k})}(a_{k})\\Bigg|\\bar{\\Phi}_{(0,i)}^{(5)}(a_{k},\\delta_{i})\\Bigg|\\Bigg|\\bar{\\Phi}_{(0,i)}(a_{k})\\Bigg|}\\\\ &{+\\frac{16\\pi_{k}^{(a_{k})}}{2}\\sum_{i=1}^{5}\\delta_{i}^{(a_{k})}\\bar{\\phi}_{(0,i)}^{(a_{k})}(a_{k},\\delta_{i})\\Bigg|\\Bigg|\\bar{\\Phi}_{(0,i)}^{(5)}(a_{k},\\delta_{i})\\Bigg|+\\frac{16\\pi_{k}^{(a_{k})}}{2}\\sum_{i=1}^{5}\\delta_{i}^{(a_{k})}\\Bigg|\\bar{\\Phi}_{(0,i)}^{(5)}(a_{k})}\\\\ &{\\leq\\frac{16\\pi_{k}^{(a_ \n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\n\\leq\\frac{\\|\\hat{\\boldsymbol{u}}_{N}\\|^{2}e^{\\hat{\\alpha}_{k}-\\alpha_{\\mathrm{t}}}}{2N a_{N}\\rho}\\sum_{i=1}^{N}\\delta_{i}^{\\hat{\\theta}_{\\mathrm{pl}}}\\varphi^{-1}(\\boldsymbol{x}_{i})B(\\boldsymbol{x}_{i})+\\frac{\\|\\hat{\\boldsymbol{u}}_{N}\\|^{2}}{2a_{N}^{3}}\\sum_{i=1}^{N}y_{i}B(\\boldsymbol{x}_{i})\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{C(\\pmb{x}_{i};\\pmb{\\dot{\\theta}})=\\left|\\dot{g}_{(k)}(\\pmb{x}_{i};\\pmb{\\dot{\\theta}})\\right|\\left\\{\\left\\|\\dot{g}(\\pmb{x}_{i};\\pmb{\\dot{\\theta}}_{k})\\right\\|^{2}+\\left\\|\\ddot{g}(\\pmb{x}_{i};\\pmb{\\dot{\\theta}})\\right\\|\\right\\}}\\\\ &{\\qquad\\qquad+\\left\\|\\frac{\\partial\\dot{g}_{(k)}(\\pmb{x}_{i};\\pmb{\\dot{\\theta}}_{k})}{\\partial\\pmb{\\theta}}\\right\\|\\left\\|\\dot{g}(\\pmb{x}_{i};\\pmb{\\dot{\\theta}}_{k})\\right\\|+\\left\\|\\frac{\\partial^{2}\\dot{g}_{(k)}(\\pmb{x}_{i};\\pmb{\\dot{\\theta}}_{k})}{\\partial\\pmb{\\theta}^{2}}\\right\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Therefore, we proved that $\\tilde{R}_{\\mathrm{mscl}(k)}=o_{P}(1)$ . Next, we prove that $\\tilde{\\Delta}_{\\mathrm{mscl}(k)}=o_{P}(1)$ We know that $\\begin{array}{r}{\\mathbb{E}\\left[a_{N}^{-2}\\sum_{i=1}^{N}\\delta_{i}^{\\hat{\\theta}_{\\mathrm{pl}}}\\left\\{y_{i}-p_{\\pi}^{\\hat{\\theta}_{\\mathrm{pl}}}(\\pmb{x}_{i};\\pmb{\\theta}_{\\mathrm{t}})\\right\\}\\ddot{g}(\\pmb{x}_{i};\\pmb{\\theta}_{\\mathrm{t}})~\\Big|~\\hat{\\theta}_{\\mathrm{pl}}\\right]=\\pmb{0}}\\end{array}$ Wealaey of $\\begin{array}{r}{a_{N}^{-2}\\sum_{i=1}^{N}\\left\\lbrace y_{i}-p(\\pmb{x}_{i};\\pmb{\\theta}_{\\mathrm{t}})\\right\\rbrace\\ddot{g}(\\pmb{x}_{i};\\pmb{\\theta}_{\\mathrm{t}})}\\end{array}$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{V}\\left[a_{N}^{-2}\\sum_{i=1}^{N}\\delta_{i}^{\\hat{\\theta}_{\\mathrm{pl}}}\\left\\{y_{i}-p_{\\pi}^{\\hat{\\theta}_{\\mathrm{pl}}}(\\mathbf{x}_{i};\\pmb{\\theta}_{\\mathrm{t}})\\right\\}\\ddot{g}_{(j l)}(\\mathbf{x}_{i};\\pmb{\\theta}_{\\mathrm{t}})\\;\\Big|\\;\\hat{\\theta}_{\\mathrm{pl}}\\right]}\\\\ &{\\le\\displaystyle\\frac{1}{a_{N}^{4}}\\sum_{i=1}^{N}\\mathbb{E}\\left\\{\\delta_{i}^{\\hat{\\theta}_{\\mathrm{pl}}}p_{\\pi}^{\\hat{\\theta}_{\\mathrm{pl}}}(\\mathbf{x}_{i};\\pmb{\\theta}_{\\mathrm{t}})\\ddot{g}_{(j l)}^{2}(\\mathbf{x}_{i};\\pmb{\\theta}_{\\mathrm{t}})\\;\\Big|\\;\\hat{\\theta}_{\\mathrm{pl}}\\right\\}\\le\\frac{1}{a_{N}^{2}}\\mathbb{E}[e^{f(\\mathbf{x};\\beta_{\\mathrm{t}})}\\|\\ddot{g}(\\mathbf{x};\\pmb{\\theta}_{\\mathrm{t}})\\|^{2}]\\rightarrow0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Thus, due to Chebyshev's inequality, we know that sc $\\tilde{\\Delta}_{\\mathrm{mscl}}^{\\hat{\\theta}_{\\mathrm{pl}}}~=~o_{P}(1)$ .Since we know that $\\begin{array}{r}{\\frac{1}{a_{N}^{2}}\\sum_{i=1}^{N}\\delta_{i}^{\\hat{\\theta}_{\\mathrm{pl}}}\\phi_{\\pi}^{\\hat{\\theta}_{\\mathrm{pl}}}(\\pmb{x}_{i};\\pmb{\\theta}_{\\mathrm{t}})\\dot{g}^{\\otimes2}(\\pmb{x}_{i};\\pmb{\\theta}_{\\mathrm{t}})=O_{P}(1)}\\end{array}$ Hence, we have that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{\\dot{\\ell}_{\\mathrm{mscl}}(\\hat{\\pmb{\\theta}}_{\\mathrm{mscl}})}{a_{N}}=O_{P}(1).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Note that we also have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{\\lambda_{N}\\hat{w}_{j^{\\prime}}}{a_{N}}=\\frac{\\lambda_{N}}{a_{N}}\\frac{1}{|\\hat{\\beta}_{\\mathrm{pl}(j^{\\prime})}|^{\\gamma}}\\stackrel{P}{\\longrightarrow}\\infty.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Therefore, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}(j^{\\prime}\\in\\mathcal{A}_{N})\\leq\\mathbb{P}\\left\\{\\lambda_{N}\\hat{w}_{j^{\\prime}}\\mathrm{sgn}(\\hat{\\beta}_{\\mathrm{mscl}(j^{\\prime})})=\\dot{\\ell}_{\\mathrm{mscl}}^{\\hat{\\theta}_{p}}(\\hat{\\theta}_{\\mathrm{mscl}})\\right\\}}\\\\ &{=\\mathbb{P}\\left\\{\\frac{\\lambda_{N}\\hat{w}_{j^{\\prime}}\\mathrm{sgn}(\\hat{\\beta}_{\\mathrm{mscl}(j^{\\prime})})}{a_{N}}=\\frac{\\dot{\\ell}_{\\mathrm{mscl}}^{\\hat{\\theta}_{p}}(\\hat{\\theta}_{\\mathrm{mscl}})}{a_{N}}\\right\\}\\to0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Thus, we prove the part of consistency of variable selection. ", "page_idx": 21}, {"type": "text", "text": "B.7Proof of Theorem 4 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Proof. Letting $h=1+c\\{\\varphi({\\pmb x})\\}^{-1}e^{f({\\pmb x};\\beta_{\\mathrm{t}})},\\,{\\pmb v}=\\sqrt{e^{f({\\pmb x};\\beta_{\\mathrm{t}})}}\\dot{g}_{(A)}({\\pmb x};\\tilde{\\theta}),\\,{\\pmb f}=h^{\\frac{1}{2}}{\\pmb v}$ and $\\pmb{g}=h^{-\\frac{1}{2}}\\pmb{v}$ we have that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\mathbb{E}(g f^{\\mathrm{T}})=\\mathbb{E}(f g^{\\mathrm{T}})=\\mathbb{E}(v v^{\\mathrm{T}})=\\mathbb{E}\\left\\{e^{f(x;\\beta_{\\mathrm{t}})}\\dot{g}_{(A)}^{\\otimes2}(x;\\theta_{\\mathrm{t}})\\right\\}=M_{(A)},}\\\\ {\\mathbb{E}(f f^{\\mathrm{T}})=\\mathbb{E}(h v v^{\\mathrm{T}})=\\mathbb{E}\\left[\\left\\{1+\\frac{c e^{f(z;\\beta_{\\mathrm{t}})}}{\\varphi(x)}\\right\\}e^{f(x;\\beta_{\\mathrm{t}})}\\dot{g}_{(A)}^{\\otimes2}(x;\\theta_{\\mathrm{t}})\\right]=M_{\\mathrm{w}(A)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}(g g^{\\mathrm{T}})=\\mathbb{E}(h^{-1}v v^{\\mathrm{T}})=\\mathbb{E}\\left[\\frac{e^{f({\\boldsymbol{x}};\\beta_{\\mathrm{t}})}\\dot{g}_{({\\boldsymbol{A}})}^{\\otimes2}({\\boldsymbol{x}};\\theta_{\\mathrm{t}})}{1+c\\varphi^{-1}({\\boldsymbol{x}})e^{f({\\boldsymbol{x}};\\beta_{\\mathrm{t}})}}\\right]=\\Lambda_{\\mathrm{mscl}({\\boldsymbol{A}})}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Now, applying the matrix form of Cauchy-Schwartz's inequality (see [19]), we have that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\overset{\\cdot}{\\Lambda_{\\operatorname*{mscl}(A)}}=\\mathbb{E}(\\pmb{g}\\pmb{g}^{\\mathrm{T}})\\ge\\mathbb{E}(\\pmb{g}\\pmb{f}^{\\mathrm{T}})\\{\\mathbb{E}(\\pmb{f}\\pmb{f}^{\\mathrm{T}})^{-1}\\mathbb{E}(\\pmb{f}\\pmb{g}^{\\mathrm{T}})}\\\\ &{\\qquad\\qquad\\qquad=M_{(A)}\\{M_{\\mathrm{w}(A)}\\}^{-1}M_{(A)}=\\mathbb{E}\\left\\{e^{f(\\pmb{x};\\beta_{\\mathrm{t}})}\\right\\}\\{V_{\\mathrm{w}(A)}\\}^{-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Therefore, simple algebra shows that ", "page_idx": 21}, {"type": "equation", "text": "$$\nV_{\\mathrm{mscl}(A)}=\\mathbb{E}\\left\\{e^{f({\\boldsymbol{x}};\\beta_{\\mathrm{t}})}\\right\\}\\{\\Lambda_{\\mathrm{mscl}(A)}\\}^{-1}\\leq V_{\\mathrm{w}(A)},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "which complete the proof ", "page_idx": 21}, {"type": "text", "text": "C  Details about the practical algorithm and its complexity ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "C.1 Two-step algorithm ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We take a pilot sample by uniform sampling with the sampling rate $\\rho_{1}=N_{\\mathrm{pl}}/2N_{1}$ for the ones and $\\rho_{0}=N_{\\mathrm{pl}}/2N_{0}$ for the zerosDent a pilt samle of actual samle si $N_{\\mathrm{pl}}^{*}$ $\\{(\\mathbf{x}_{i}^{\\mathrm{pl}},y_{i}^{\\mathrm{pl}})\\}_{i=1}^{N_{\\mathrm{pl}}^{*}}$ \uff0c the pilot estimate of $\\pmb{\\theta}$ as $\\hat{\\pmb{\\theta}}_{\\mathrm{pl}}$ , and the pilot estimate of the active set as $\\hat{\\mathcal{A}}_{\\mathrm{pl}}=\\{j:\\hat{\\beta}_{\\mathrm{pl}(j)}\\neq0\\}$ .We propose the following moment estimators of $M_{({\\cal A})}$ and $\\Omega_{(\\mathcal{A})}$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\hat{M}_{(\\hat{\\mathcal{A}}_{\\mathrm{pl}})}^{\\mathrm{pl}}=\\frac{1}{N_{\\mathrm{pl}}}\\sum_{i=1}^{N_{\\mathrm{pl}}^{*}}\\frac{e^{f({\\pmb x}_{i}^{\\mathrm{plT}}\\hat{\\beta}_{\\mathrm{pl}})}\\dot{g}_{(\\hat{\\mathcal{A}}_{\\mathrm{pl}})}^{\\otimes2}({\\pmb x}_{i}^{\\mathrm{pl}};\\hat{\\theta}_{\\mathrm{pl}})}{\\rho_{0}+{y}_{i}^{\\mathrm{pl}}(\\rho_{1}-\\rho_{0})},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$$\n\\hat{\\Omega}_{(\\hat{A}_{\\mathrm{pl}})}^{\\mathrm{pl}}=\\frac{1}{N_{\\mathrm{pl}}}\\sum_{i=1}^{N_{\\mathrm{pl}}^{*}}\\frac{e^{2f(\\mathbf{x}_{i}^{\\mathrm{plT}}\\hat{\\beta}_{\\mathrm{pl}})}\\dot{g}_{(\\hat{A}_{\\mathrm{pl}})}^{\\otimes2}(\\mathbf{x}_{i}^{\\mathrm{pl}};\\hat{\\pmb{\\theta}}_{\\mathrm{pl}})}{\\rho_{0}+y_{i}^{\\mathrm{pl}}(\\rho_{1}-\\rho_{0})},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "respectively. We also use the following moment estimator to estimate the denominator of (4): ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{1}{N_{\\mathrm{pl}}}\\sum_{i=1}^{N_{\\mathrm{pl}}^{*}}\\frac{\\omega_{i}^{\\mathrm{A-OS}}}{\\rho_{0}+y_{i}^{\\mathrm{pl}}(\\rho_{1}-\\rho_{0})},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Wwherer $\\begin{array}{r}{\\omega_{i}^{\\mathrm{A-OS}}=p({\\pmb x}_{i}^{\\mathrm{pl}};\\hat{\\pmb\\theta}_{\\mathrm{pl}})\\|(\\hat{M}_{(\\hat{\\mathcal{A}}_{\\mathrm{pl}})}^{\\mathrm{pl}})^{-1}\\dot{g}_{(\\hat{\\mathcal{A}}_{\\mathrm{pl}})}({\\pmb x}_{i}^{\\mathrm{pl}};\\hat{\\pmb\\theta}_{\\mathrm{pl}})\\|}\\end{array}$", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\omega_{i}^{\\mathrm{L-OS}}=p({\\pmb x}_{i}^{\\mathrm{pl}};\\hat{\\pmb{\\theta}}_{\\mathrm{pl}})\\|\\dot{g}_{(\\hat{\\mathcal{A}}_{\\mathrm{pl}})}({\\pmb x}_{i}^{\\mathrm{pl}};\\hat{\\pmb{\\theta}}_{\\mathrm{pl}})\\|,\\mathrm{~or}}\\\\ &{\\omega_{i}^{\\mathrm{P-OS}}=p({\\pmb x}_{i}^{\\mathrm{pl}};\\hat{\\pmb{\\theta}}_{\\mathrm{pl}})\\|(\\hat{\\Omega}_{(\\hat{\\mathcal{A}}_{\\mathrm{pl}})}^{\\mathrm{pl}})^{1/2}(\\hat{M}_{(\\hat{\\mathcal{A}}_{\\mathrm{pl}})}^{\\mathrm{pl}})^{-1}\\dot{g}_{(\\hat{\\mathcal{A}}_{\\mathrm{pl}})}({\\pmb x}_{i}^{\\mathrm{pl}};\\hat{\\pmb{\\theta}}_{\\mathrm{pl}})\\|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "respectively, instead of $\\omega_{i}^{\\mathrm{A-OS}}$ in (13). Now, we present the proposed two-step procedure in Algorithm 3 with more details than Algorithm 2 in Section 4. ", "page_idx": 22}, {"type": "text", "text": "Algorithm 3 Subsampling adaptive lasso algorithm ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1: \u00b7 Take a pilot sample $\\{(x_{i}^{\\mathrm{pl}},y_{i}^{\\mathrm{pl}})\\}_{i=1}^{N_{\\mathrm{pl}}^{*}}$ of expeced sample size $N_{\\mathrm{pl}}$ using $\\left\\{\\pi(y_{i})\\right.=$ $\\rho_{0}+y_{i}(\\rho_{1}-\\rho_{0})\\}_{i=1}^{N}$ and obtain a pilot estimator ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\hat{\\theta}_{\\mathrm{pl}}:=\\arg\\operatorname*{max}_{\\pmb{\\theta}}\\left\\{\\sum_{i=1}^{N_{\\mathrm{pl}}^{*}}[y_{i}^{\\mathrm{pl}}g(\\pmb{x}_{i}^{\\mathrm{pl}};\\pmb{\\theta})-\\log\\{1+e^{g(\\mathbf{x}_{i}^{\\mathrm{pl}};\\pmb{\\theta})+l}\\}]-\\lambda_{\\mathrm{pl}}\\sum_{j=1}^{p}|\\beta_{(j)}|\\right\\},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $N_{\\mathrm{pl}}^{*}$ is the actual pilot sample size and $l=\\log(N_{0}/N_{1})$ . We call this fist stage screening. ", "page_idx": 22}, {"type": "text", "text": "\u00b7 Calculate approximate optimal sampling probabilities $\\{\\hat{\\pi}(x_{i},y_{i})\\;\\;=\\;\\;y_{i}\\;+\\;(1\\;-\\;$ $y_{i})\\rho\\hat{\\varphi}(x_{i})\\}_{i=1}^{N}$ by replacing $\\hat{\\varphi}(x_{i})$ with $\\varphi_{\\mathrm{A-OS}}^{\\mathrm{adp}}(\\mathbf{x}_{i};\\hat{\\pmb{\\theta}}_{\\mathrm{pl}})$ PL-Pos(c; 0pl), or $\\varphi_{\\mathrm{P-OS}}^{\\mathrm{adp}}(\\mathbf{x}_{i};\\hat{\\pmb{\\theta}}_{\\mathrm{pl}})$ basedon (4,(),or(TD.respetively The denominator of () is estimated using (13), and we replace A-OS with w L-OS or wt P-Os for the denominator ofr7,rtivelyfu $\\pi_{\\mathrm{A-OS}}^{\\mathrm{adp}}(x)$ $\\pi_{\\mathrm{P-OS}}^{\\mathrm{adp}}(\\pmb{x})$ , estimate $M_{({\\cal A})}$ and $\\Omega_{(\\mathcal{A})}$ using the moment estimators in (11) and (12), respectively. ", "page_idx": 22}, {"type": "text", "text": "2: Use Algorithm 1 with the estimated optimal sampling probabilities to obtain a subsample $\\{(\\mathbf{x}_{i}^{\\mathrm{sub}},y_{i}^{\\mathrm{sub}})\\}_{i=1}^{N_{\\mathrm{sub}}^{*}}$ andcomptetheadaptivelasoestimator? ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\hat{\\theta}_{\\mathrm{mscl}}^{\\mathrm{adp}}:=\\arg\\operatorname*{max}\\left\\{\\sum_{i=1}^{N_{\\mathrm{sub}}^{*}}[y_{i}^{\\mathrm{sub}}g(x_{i}^{\\mathrm{sub}};\\theta)-\\log\\{1+e^{g(x_{i}^{\\mathrm{sub}};\\theta)+l_{i}}\\}]-\\lambda_{N}\\sum_{j\\in\\mathcal{A}_{\\mathrm{p}}}\\frac{|\\beta_{(j)}|}{|\\hat{\\beta}_{\\mathrm{p}1(j)}|^{\\gamma}}\\right\\},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $N_{\\mathrm{sub}}^{*}$ is the actual subsample size, based on the smaller model obtain from the first stage screening. We call this step the second stage screening. ", "page_idx": 22}, {"type": "text", "text": "Remark 3. Our algorithm naturally integrates the MSCL function with the adaptive lasso penalty. Itcanalsobeimplementedwhen $p>N$ aslongasthedimensionofselectedvariablesissmaller than $N$ in thefirst-stagescreening.If the model is sparse and the data aremassive,this is usually possible in practice. Screning algorithms such as sure independence screening [6] can also be used for the first stage screening to guarantee that the dimension of second-stage screening is smaller than the subsample size. Furthermore, the first stage screening can help to speed up the computation, as shownby the analysis of computational complexity in thenext section. ", "page_idx": 23}, {"type": "text", "text": "We consider a coordinate desent method to calculate the estimators defined in Algorithm 3. (see [8], [9] and [27]). In each cycle, we need to find an optimal direction $^d$ at a starting point $\\tilde{\\pmb{\\theta}}$ .Weconsider thequardrati aproimation of $Q_{\\mathrm{mscl}}^{\\hat{\\theta}_{\\mathrm{pl}}}(\\tilde{\\theta}+d)-Q_{\\mathrm{mscl}}^{\\hat{\\theta}_{\\mathrm{pl}}}(\\tilde{\\theta})$ Wwhichis ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q_{\\operatorname*{max}(\\bar{\\theta}}^{\\theta_{\\mathrm{ph}}}(\\Tilde{\\theta}+d)-Q_{\\operatorname*{mix}(\\bar{\\theta})}^{\\theta_{\\mathrm{ph}}}(\\Tilde{\\theta})}\\\\ &{=\\displaystyle\\sum_{i=1}^{N}\\delta_{i}[-y_{i}g(x_{i};\\Tilde{\\theta}+d)+\\log\\{1+e^{g(x_{i};\\Tilde{\\theta}+d)+l_{i}}\\}]+\\lambda_{N}\\displaystyle\\sum_{j=1}^{p}\\hat{w}_{j}[\\beta_{(j)}+d_{(j)}]}\\\\ &{\\quad-\\displaystyle\\sum_{i=1}^{N}\\delta_{i}[-y_{i}g(x_{i};\\Tilde{\\theta})-\\log\\{1+e^{g(x_{i};\\Tilde{\\theta})+l_{i}}\\}]+\\lambda_{N}\\displaystyle\\sum_{j=1}^{p}\\hat{w}_{j}[\\beta_{(j)}]}\\\\ &{\\approx\\ell_{\\operatorname*{max}(\\bar{\\theta})}^{\\theta_{\\mathrm{ph}}}(\\Tilde{\\theta})^{T}d+\\frac{1}{2}d^{T}\\hat{\\ell}_{\\operatorname*{max}(\\bar{\\theta})}^{\\theta_{\\mathrm{ph}}}(\\Tilde{\\theta})d+\\lambda_{N}\\displaystyle\\sum_{j=1}^{p}\\left\\{\\hat{w}_{j}[\\Tilde{\\beta}_{(j)}+d_{(j)}]-\\hat{w}_{j}[\\Tilde{\\beta}_{(j)}]\\right\\}}\\\\ &{\\hat{\\ell}_{\\operatorname*{mex}(\\bar{\\theta})}^{\\theta_{\\mathrm{ph}}}(\\Tilde{\\theta})\\quad=\\quad-\\sum_{i=1}^{N}\\delta_{i}^{\\theta_{\\mathrm{ph}}}\\left\\{y_{i}-p_{\\pi}^{\\theta_{\\mathrm{ph}}}(x_{i},\\Tilde{\\theta})\\right\\}\\dot{\\xi}(x_{i};\\Tilde{\\theta})\\quad\\mathrm{and}\\quad\\hat{\\ell}_{\\operatorname*{max}(\\bar{\\theta})}^{\\theta_{\\mathrm{ph}}}(\\Tilde{\\theta})}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where   \n$\\begin{array}{r}{\\sum_{i=1}^{N}\\delta_{i}^{\\hat{\\theta}_{\\mathrm{pl}}}\\phi_{\\pi}^{\\hat{\\theta}_{\\mathrm{pl}}}(\\pmb{x}_{i},\\tilde{\\pmb{\\theta}})\\dot{g}^{\\otimes2}(\\pmb{x}_{i};\\tilde{\\pmb{\\theta}})}\\end{array}$ Thus, using cordinate desent to obtain the optimal dirc   \ntion, the quadratic approximation for the $j$ -th element is given as ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q_{\\mathrm{mscl}}^{\\hat{\\theta}_{\\mathrm{pl}}}(d+z e_{j})-Q_{\\mathrm{mscl}}^{\\hat{\\theta}_{\\mathrm{pl}}}(d)=\\dot{\\ell}_{\\mathrm{mscl}(j)}(\\tilde{\\theta})z+\\left\\{\\ddot{\\ell}_{\\mathrm{mscl}}(\\tilde{\\theta})d\\right\\}_{(j)}z+\\frac{1}{2}\\ddot{\\ell}_{\\mathrm{mscl}(j j)}(\\tilde{\\theta})z^{2}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad+\\,\\lambda_{N}\\hat{w}_{j}|\\tilde{\\beta}_{(j)}+d_{(j)}+z|-\\lambda_{N}\\hat{w}_{j}|\\tilde{\\beta}_{(j)}+d_{(j)}|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Then, we have the valueof $z$ that minimize $Q_{\\mathrm{mscl}}^{\\hat{\\theta}_{\\mathrm{pl}}}(d+z e_{j})-Q_{\\mathrm{mscl}}^{\\hat{\\theta}_{\\mathrm{pl}}}(d)$ ", "page_idx": 23}, {"type": "equation", "text": "$$\nz^{**}=\\left\\{\\begin{array}{l l}{\\frac{\\dot{\\hat{\\ell}}_{\\mathrm{mscl}(j)}^{\\hat{\\theta}_{\\mathrm{pl}}}(\\tilde{\\theta})+\\left\\{\\dot{\\hat{\\ell}}_{\\mathrm{mscl}}^{\\hat{\\theta}_{\\mathrm{pl}}}(\\tilde{\\theta})d\\right\\}_{(j)}+\\lambda\\hat{w}_{j}}{-\\dot{\\hat{\\ell}}_{\\mathrm{mscl}(j)}^{\\hat{\\theta}_{\\mathrm{pl}}}(\\tilde{\\theta})}}&{\\mathrm{if}\\;\\tilde{\\beta}_{(j)}+d_{(j)}+z\\ge0}\\\\ {\\frac{\\dot{\\hat{\\ell}}_{\\mathrm{mscl}(j)}^{\\hat{\\theta}_{\\mathrm{pl}}}(\\tilde{\\theta})+\\left\\{\\dot{\\hat{\\ell}}_{\\mathrm{mscl}}^{\\hat{\\theta}_{\\mathrm{pl}}}(\\tilde{\\theta})d\\right\\}_{(j)}-\\lambda\\hat{w}_{j}}{-\\dot{\\hat{\\ell}}_{\\mathrm{mscl}(j)}^{\\hat{\\theta}_{\\mathrm{pl}}}(\\tilde{\\theta})}}&{\\mathrm{if}\\;\\tilde{\\beta}_{(j)}+d_{(j)}+z\\le0}\\\\ {-\\tilde{\\ell}_{\\mathrm{mscl}(j)}^{\\hat{\\theta}_{\\mathrm{pl}}}-d_{(j)}}&{\\mathrm{otherwise},}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "which is the same as ", "page_idx": 23}, {"type": "equation", "text": "$$\nz^{**}=\\operatorname*{max}\\left\\{z_{1},-\\tilde{\\beta}_{(j)}-d_{(j)}\\right\\}-\\operatorname*{max}\\left\\{-z_{2},\\tilde{\\beta}_{(j)}+d_{(j)}\\right\\}+\\tilde{\\beta}_{(j)}+d_{(j)},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where ", "page_idx": 23}, {"type": "equation", "text": "$$\nz_{1}=\\frac{\\dot{\\ell}_{\\mathrm{mscl}(j)}^{\\hat{\\theta}_{\\mathrm{pl}}}(\\tilde{\\pmb{\\theta}})+\\left\\{\\ddot{\\ell}_{\\mathrm{mscl}}^{\\hat{\\theta}_{\\mathrm{pl}}}(\\tilde{\\pmb{\\theta}})\\pmb{d}\\right\\}_{(j)}+\\lambda\\hat{w}_{j}}{-\\ddot{\\ell}_{\\mathrm{mscl}(j j)}^{\\hat{\\theta}_{\\mathrm{pl}}}(\\tilde{\\pmb{\\theta}})},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and ", "page_idx": 23}, {"type": "equation", "text": "$$\nz_{2}=\\frac{\\dot{\\ell}_{\\mathrm{mscl}(j)}^{\\hat{\\theta}_{\\mathrm{pl}}}(\\tilde{\\pmb{\\theta}})+\\left\\{\\ddot{\\ell}_{\\mathrm{mscl}}^{\\hat{\\theta}_{\\mathrm{pl}}}(\\tilde{\\pmb{\\theta}})\\pmb{d}\\right\\}_{(j)}-\\lambda\\hat{w}_{j}}{-\\ddot{\\ell}_{\\mathrm{mscl}(j j)}^{\\hat{\\theta}_{\\mathrm{pl}}}(\\tilde{\\pmb{\\theta}})}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "For the special form of $g(\\pmb{x};\\pmb{\\theta})=\\alpha+f(\\pmb{x}^{\\mathrm{T}}\\beta)$ , we know that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\ddot{\\ell}_{\\mathrm{mscl}}^{\\hat{\\theta}_{\\mathrm{pl}}}(\\tilde{\\theta})=\\sum_{i=1}^{N}\\delta_{i}^{\\hat{\\theta}_{\\mathrm{pl}}}\\phi_{\\pi}^{\\hat{\\theta}_{\\mathrm{pl}}}(\\boldsymbol{x}_{i},\\tilde{\\theta})\\dot{g}^{\\otimes2}(\\boldsymbol{x}_{i};\\tilde{\\theta})=G^{\\mathrm{T}}\\Phi G,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where ", "page_idx": 24}, {"type": "equation", "text": "$$\nG=\\begin{array}{r}{\\left(\\begin{array}{c c c c}{1}&{\\dot{f}(x_{1}^{\\mathrm{T}}\\tilde{\\beta})x_{1}^{\\mathrm{T}}}\\\\ {1}&{\\dot{f}(x_{2}^{\\mathrm{T}}\\tilde{\\beta})x_{2}^{\\mathrm{T}}}\\\\ {\\vdots}&{\\vdots}\\\\ {1}&{\\dot{f}(x_{N}^{\\mathrm{T}}\\tilde{\\beta})x_{N}^{\\mathrm{T}}}\\end{array}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and $\\Phi=d i a g\\{\\delta_{i}^{\\hat{\\theta}_{\\mathrm{pl}}}\\phi_{\\pi}^{\\hat{\\theta}_{\\mathrm{pl}}}(\\mathbf{x}_{i},\\tilde{\\pmb{\\theta}})\\}$ Thus, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left\\{\\bar{\\ell}_{\\mathrm{mscl}}^{\\hat{\\theta}_{\\mathrm{pl}}}(\\tilde{\\theta})d\\right\\}_{(j)}=\\left(G^{\\mathrm{T}}\\Phi G d\\right)^{\\mathrm{T}}e_{j}=(G d)^{\\mathrm{T}}\\Phi(G e_{j})=(G d)^{\\mathrm{T}}\\Phi(G e_{j})=(G d)^{\\mathrm{T}}\\Phi G_{(j)}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Therefore, we can store $_{G d}$ and keep updating $_{G d}$ with ", "page_idx": 24}, {"type": "equation", "text": "$$\nG(d+z e_{j})=G d+z G e_{j}=G d+G_{(j)}z.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Thus, we do not need to obtain the full matrix $\\ddot{\\ell}_{\\mathrm{mscl}}(\\tilde{\\pmb{\\theta}})\\;=\\;\\pmb{G}^{\\mathrm{T}}\\pmb{\\Phi}\\pmb{G}$ .We only need to calculate the diagnoal elements: $\\ddot{\\ell}_{\\mathrm{mscl}(j j)}(\\Tilde{\\pmb{\\theta}})\\,=\\,{\\pmb{G}}_{(j)}^{\\mathrm{T}}\\pmb{\\Phi}{\\pmb{G}}_{(j)},j\\,=\\,1,...,p+1$ and $\\left\\{\\dot{\\ell}_{\\mathrm{mscl}}(\\tilde{\\pmb{\\theta}})\\pmb{d}\\right\\}_{(j)}=$ $(G d)^{\\mathrm{T}}\\Phi G_{(j)},j\\,=\\,1,...,p+1$ . From the analysis above, we can notice that the computaional complexity of one cycle calculating optimal direction $\\pmb{d}$ is $O(\\zeta_{\\mathrm{in}}N p)$ , where $\\zeta_{\\mathrm{in}}$ denotes the number of inner iteration. ", "page_idx": 24}, {"type": "text", "text": "C.2  Computational complexity ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We analyze the computational complexity of the two-step algorithm. To facilitate the presentation, we consider a special case for our model when $g(\\pmb{x};\\pmb{\\theta})=\\bar{\\alpha}+\\bar{f}(\\pmb{x}^{\\mathrm{T}}\\beta)$ , and assume that the number of variables selected at the first-stage screening is $q$ . Coordinate descent is a widely used optimization algorithm for solving lasso and adaptive lasso [see 9]. We consider the improved coordinate descent algorithm proposed in [27], which requires inner iterations to determine an optimal direction and outer iterations to update the estimator. Considering the form of $g(\\pmb{x};\\pmb{\\theta})=\\alpha+f(\\pmb{x}^{\\mathrm{T}}\\beta)$ , the computational complexity for coordinate descent with data of size $N$ and dimension $p$ is $O(\\zeta_{\\mathrm{in}}N p)$ per innercycle where $\\zeta_{\\mathrm{in}}$ represents the number of inner iterations (detailed derivations of this complexity is presented Section C). Thus, the computational complexity of full data lasso is $O(\\zeta_{\\mathrm{out}\\times\\mathrm{in}}N p)$ where $\\zeta_{\\mathrm{out}\\times\\mathrm{in}}=\\zeta_{\\mathrm{out}}\\zeta_{\\mathrm{in}}$ and $\\zeta_{\\mathrm{out}}$ is the number of outer iterations. The computational complexity of the fldatauv las is $O(\\zeta_{\\mathrm{pl}}^{\\mathrm{mle}}N p^{2}+\\zeta_{\\mathrm{out}\\times\\mathrm{in}}N p)$ Withhatltad $O(\\zeta_{\\mathrm{pl,out}\\times\\mathrm{in}}^{\\mathrm{las}}N p+\\zeta_{\\mathrm{out}\\times\\mathrm{in}}N q)$ wihlailttmaw $\\zeta_{\\mathrm{pl}}^{\\mathrm{MLE}}$ and Spl,out x in are the iteration numbers in the two pilot estimators, respectively. The coordinate descent algorithm often requires a large $\\zeta_{\\mathrm{out}\\times\\mathrm{in}}$ or plout xin while Newton's algorithm requires a small Cle, mle, so it is often the case that $\\zeta_{\\mathrm{pl}}^{\\mathrm{mle}}p<\\zeta_{\\mathrm{out}\\times\\mathrm{in}}$ Therefoe,the time complexity of the adaptiv las is $O(\\zeta_{\\mathrm{out}\\times\\mathrm{in}}N p)$ \uff0c which is the same as the full data lasso estimator. ", "page_idx": 24}, {"type": "text", "text": "Now, we analyze the time complexity of Algorithm 3.  We start with the computational complexity of the optimal probabilities, for which the main computational cost is to approximate $\\|M_{(\\mathcal{A})}^{-1}\\dot{g}_{(\\mathcal{A})}(\\pmb{x}_{i};\\pmb{\\theta})\\|$ or $\\|\\Omega_{(A)}^{1/2}M_{(A)}^{-1}\\dot{g}_{(A)}(\\pmb{x}_{i};\\pmb{\\theta})\\|$ , respectively, for $\\textit{i}=\\mathrm{~1,...,N~}$ Since $\\dot{g}_{(\\hat{A}_{\\mathrm{pl}})}(\\pmb{x}_{i};\\pmb{\\theta})=(1,\\dot{f}(\\pmb{x}_{i}^{\\mathrm{T}}\\beta)\\pmb{x}_{i(\\hat{A}_{\\mathrm{pl}})}^{\\mathrm{T}})^{\\mathrm{T}}$ lf $\\dot{g}_{(\\hat{A}_{\\mathrm{pl}})}(\\pmb{x}_{i};\\pmb{\\theta})$ \uff0cs is $O(N q)$ , and the computational complexity of $\\hat{M}_{(\\hat{\\mathcal{A}}_{\\mathrm{pl}})}^{\\mathrm{pl}}$ or S\u03a92pl $\\hat{\\Omega}_{(\\hat{\\mathcal{A}}_{\\mathrm{pl}})}^{\\mathrm{pl}}$ is $O\\left\\{N_{\\mathrm{pl}}(q+1)^{2}\\right\\}\\ =$ $O(N_{\\mathrm{pl}}q^{2})$ .Taking the inverse (A)1 and fnding the square rot ( $(\\hat{\\Omega}_{(\\hat{\\mathcal{A}}_{\\mathrm{pl}})}^{\\mathrm{pl}})^{1/2}$ both take $O(q^{3})$ time. Thus, the computational complexity of calculating Ma)g(An)(m; 0)s or $\\lVert(\\hat{\\Omega}_{(\\hat{\\mathcal{A}}_{\\mathrm{pl}})}^{\\mathrm{pl}})^{1/2}(\\hat{M}_{(\\hat{\\mathcal{A}}_{\\mathrm{pl}})}^{\\mathrm{pl}})^{-1}\\dot{g}_{(\\hat{\\mathcal{A}}_{\\mathrm{pl}})}(\\pmb{x}_{i};\\pmb{\\theta})\\rVert$ $O(N q\\!+\\!N_{\\mathrm{pl}}q^{2}\\!+\\!q^{3}\\!+\\!N q^{2})=O(N q^{2})$ $O(N q^{2})$ complexity of approximating the optimal probabilities in (5) is only $O(N q)$ , because there is no need to compute Mrpl or Spl $\\hat{\\Omega}_{(\\hat{\\mathcal{A}}_{\\mathrm{pl}})}^{\\mathrm{pl}}$ . Next, we analyze the complexity of parameter estimation. The average subsample size with the sampling rate $\\rho$ is on average ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}(N_{1})+\\rho\\{N-\\mathbb{E}(N_{1})\\}=N\\mathbb{E}\\{f(x;\\theta_{\\mathrm{t}})\\}\\{(1-\\rho)e^{\\alpha_{\\mathrm{t}}}+\\rho+o(1)\\}=O\\{N(e^{\\alpha_{\\mathrm{t}}}+\\rho)\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "$O\\{\\zeta_{\\mathrm{pl,out}\\times\\mathrm{in}}^{\\mathrm{las}}N_{\\mathrm{pl}}p+N q^{2}+\\zeta_{\\mathrm{out}\\times\\mathrm{in}}N(e^{\\alpha_{\\mathrm{t}}}+\\rho)q\\}$ a $O\\{\\zeta_{\\mathrm{pl,out}\\times\\mathrm{in}}^{\\mathrm{las}}N_{\\mathrm{pl}}p+N q+\\zeta_{\\mathrm{out}\\times\\mathrm{in}}N(e^{\\alpha_{\\mathrm{t}}}+\\rho)q\\}$ using hol rbablesi )Forpimal probabilities in (4) or (7), when $\\zeta_{\\mathrm{out}\\,\\times\\mathrm{in}}>q/(e^{\\alpha}+\\rho)$ , the dominating term of the complexity is $\\zeta_{\\mathrm{out}\\times\\mathrm{in}}N(e^{\\alpha_{\\mathrm{t}}}+\\rho)q$ . Remember that $\\zeta_{\\mathrm{out}\\times\\mathrm{in}}=\\zeta_{\\mathrm{out}}\\zeta_{\\mathrm{in}}$ and $\\zeta_{\\mathrm{out}}$ is usually large for the coordinate descent algorithm. Therefore, $\\zeta_{\\mathrm{out}\\,\\times\\mathrm{in}}>q/(e^{\\alpha}+\\rho)$ is often satisfied in practice. The dominating term for the time complexity of optimal probabilities in (5) is also $\\zeta_{\\mathrm{out}\\times\\mathrm{in}}\\bar{N}(e^{\\alpha_{\\mathrm{t}}}+\\rho)q$ . Compared with full data estimators, both the sample size and the dimension are reduced. If we set the subsample size to be the same order of $N_{1}$ , which is often the case in practice for balancing the ones and zeros, the time complexity of Algorithm 2 is of order $O(\\zeta_{\\mathrm{out}\\times\\mathrm{in}}N e^{\\alpha_{\\mathrm{t}}}q)$ , which is significantly faster than that of the full data estimator. ", "page_idx": 25}, {"type": "text", "text": "D  Detalis of simulation settings ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In this section, we present more details of the simulation settings in the main paper. In Section D.1, we provide detailed simulation settings of the example in Section 1. In Section D.2, we present detailed settings in Section 5. ", "page_idx": 25}, {"type": "text", "text": "D.1  Simulation in Section 1 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We first present the detailed settings of the simulations in Section 1, where we illustrate the scaledependent issues of optimal subsampling probabilities. Our simulation based on logistic regression models with the true parameter $\\beta_{\\mathrm{t}}$ to be 6-dimentional vectors and covariates $\\pmb{x}\\sim l o g n o r m a l(\\mathbf{0},\\pmb{\\Sigma})$ with the $(i,j)$ -th element of $\\Sigma$ is given as $\\Sigma_{i j}=0.5^{|i-j|},1\\leq i,j\\leq6$ We consider two cases of parameters: ", "page_idx": 25}, {"type": "text", "text": "(a)  Non-sparse parameter: $\\beta_{\\mathrm{t}}=(-1,-1,-0.01,-0.01,-0.01,-0.01)$ and $\\alpha_{\\mathrm{{t}}}=-4$ (b) Sparse parameter: $\\beta_{\\mathrm{t}}=(-1,0,0,0,0,0)$ and $\\alpha_{\\mathrm{{t}}}=-5$ ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "We generate full data of size $N=500000$ according to the above logistic models. To investigate the effects of scale transformation, we multiply the $\\pmb{x}_{(6)}$ with $s\\left(s=0.01,0.1,1,10,100\\right)$ and divide $\\beta_{\\mathrm{t}(6)}$ with the same $s$ to remain $x^{\\mathrm{T}}\\beta_{\\mathrm{t}}$ to be the same and thus the logistics regression model does not change. We obtain subsamples with optimal subsampling probabilities described in [22] with transformed $\\textbf{\\em x}$ under each $s$ and calculate the resultant subsampling estimators. We set the nominal pilot sample size to $N_{\\mathrm{pl}}=800$ and nominal subsample size $N_{\\mathrm{sub}}=1000$ (see details in [22]). We repeat the experiment for 500 times under each scale and compute the mean prediction error. ", "page_idx": 25}, {"type": "text", "text": "D.2 Simulations in Section 5 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "For the estimation procedures in the second step of our two-step algorithm, we choose $\\gamma=1$ , which means that the weights in the adaptive lasso penalty are $\\hat{w}_{j}=\\underline{{1}}/|\\hat{\\beta}_{\\mathrm{pl}(j)}|,1\\leq j\\leq q$ with $q$ being the number of selected variables in the first stage screening. Furthermore, we consider uniform sampling, the full data lasso and the full data adaptive lasso as baselines for comparison. For the uniform sampling method, we use a similar two-step algorithm as presented in Algorithm 2 but set the sampling function in the second step as $\\varphi({\\pmb x})=1$ , which means the sampling probabilities are a constant $\\rho$ . We use lasso to implement the first stage screening and adaptive lasso with $\\gamma=1$ to implement the second stage screening for a fair comparison. For full data lasso, we directly apply the lasso algorithm to the full data. For the full data adaptive lasso, we use the full data MLE estimator as the pilot estimator to construct the weights and then apply the adaptive lasso algorithm to the full data set. ", "page_idx": 25}, {"type": "text", "text": "E Additional simuations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In this Section, we give some addtional simulation results. More simulation results of variable selection is provided in Section E.1. We also provide addtional simulation results to compare our approach with standardization to resolve scale dependent issues in Section E.2. ", "page_idx": 25}, {"type": "table", "img_path": "6SAnp0vr9X/tmp/f3f32e73709c20a706d7cdbddd60ee9e96c4e258e84c287846ea4ff48b73596e.jpg", "table_caption": ["Table 4: Mean number of selected variables in Case A and Case B "], "table_footnote": [], "page_idx": 26}, {"type": "table", "img_path": "6SAnp0vr9X/tmp/7a3bb4e7f80f3c00ff80f3033ff8c65313da6991e6a82c227a5c3dd28147e649.jpg", "table_caption": ["Table 5: Rates of excluding active variables (false negative rate) in Case A and Case B "], "table_footnote": [], "page_idx": 26}, {"type": "table", "img_path": "6SAnp0vr9X/tmp/bb4ff20141381e6a6ff84a52eb3dbaa8259aa4a4db17c8f3e4d1b89e692655cc.jpg", "table_caption": ["Table 6: Rates of selecting the true model "], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "It is seen in Table 6 that, no subsampling method dominates others. Table 2 and Table 5 shows that uniform sampling has higher rates of excluding active variables than optimal subsampling procedures. ", "page_idx": 26}, {"type": "text", "text": "Although uniform sampling may have a higher rate of selecting the true model in some cases, given that it is more likely to exclude important variables, optimal sampling may be preferable in practice. ", "page_idx": 27}, {"type": "text", "text": "E.2  Comparison with standardization ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Another approach to avoid scale-dependency is to standardize the data. We compare the proposed scale-independent optimal probabilities with the approach of data standardization here. For the data standardization approach, we standardize the data, calcualte the optimal probabilities, and then implement subsampled adpative lasso algorithm. We used the same pilot estimation methods for fair comparisons. ", "page_idx": 27}, {"type": "text", "text": "We first compare the eMSE and eMSPE in Figure 5 and Figure 6, respectively. We use sP-OS to denote the approach with data standardization and use P-OS to denote the approach without data standardization. ", "page_idx": 27}, {"type": "image", "img_path": "6SAnp0vr9X/tmp/70e0b36a73a2e6466f0a572e43e0aab089b7d44ee9f0d6286da01103f0d1723b.jpg", "img_caption": ["Figure 5: Empirical median squred error of estimated probability for different parameters with different sampling rates. The same pilot sample size is $N_{\\mathrm{pl}}=500$ "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "In Figure 5, we notice that the performances of P-OS and sP-OS are similar, this is also true for eMSPE. However, standardization may decrease the rate of selecting the true model. We present results of variable selection in Table 7 and Table 8. ", "page_idx": 27}, {"type": "image", "img_path": "6SAnp0vr9X/tmp/4b4108bb392af71275eaaf0692be7f1be09703ed42a9a2685198650094337637.jpg", "img_caption": ["Figure 6: Empirical median squred error of estimated probability for different parameters with different sampling rates. The same pilot sample size is $N_{\\mathrm{pl}}=500$ "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "We notice in Table 7 and Table 8 that the rates of selecting true models by $\\hat{\\beta}_{\\mathrm{P-OS}}^{\\mathrm{adp}}$ is higher than $\\hat{\\beta}_{\\mathrm{sP-OS}}^{\\mathrm{adp}}$ without much increase on the rates of excluding active variables. Therefore, although standardization is an approach to solve the scale-dependency issues, it may decrease the rates of selecting true models in practice. ", "page_idx": 27}, {"type": "table", "img_path": "6SAnp0vr9X/tmp/2a17a68c25e64e19f8731ed98e5ae23084671403fb5b0e83ed4b772fe31cde54.jpg", "table_caption": ["Table 7: Rates of selecting true models "], "table_footnote": [], "page_idx": 28}, {"type": "table", "img_path": "6SAnp0vr9X/tmp/87f42ba08ba992c7fb4822dcff905b006107eebda7fe3550dc3865c4566ed945.jpg", "table_caption": ["Table 8: Rates of excluding active variables (false negtive rate) "], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "[Yes] [No] [NA] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately refect the paper's contributions and scope? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 29}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: In Section 6. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 29}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Detailed proofs and required assumptions are provided in the appendix. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 30}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: All details for numerical experiments are provided in Section 5 and in the appendix. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 30}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Codes are submitted as supplement for anonymity. They will be released in a public github repository after the review period. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : / /nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 31}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: In Section 5. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 31}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We provide standard errors in Tables 1 and 2. We perform a large number of repetitions of the simulation experiments to calculate the empirical median squared error, so error bars are not relevant in Figures 1, 2, 3, and 4. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 31}, {"type": "text", "text": "\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 32}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: In Section 5.1.3. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 32}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 32}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] Justification: This paper is theoretical research. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed. \u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 32}, {"type": "text", "text": "\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 33}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks. ", "page_idx": 33}, {"type": "text", "text": "\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safetyfilters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 33}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: URLs for real data provided in Section 5.2. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetisused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 34}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 34}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 34}]