[{"figure_path": "C4SInFLvuB/tables/tables_3_1.jpg", "caption": "Table 1: Exemplary parametrizations in Equation (1) for resamplings; see Appendix E for details.", "description": "This table shows the parameterizations of the variance (\u03c3\u00b2) and correlation (\u03c4\u00b2) terms in Equation (1) for different resampling methods.  These parameters relate to how reshuffling affects the empirical loss surface in hyperparameter optimization.  The table lists values for holdout, reshuffled holdout, M-fold cross-validation, reshuffled M-fold cross-validation, M-fold holdout (subsampling/Monte Carlo CV), and reshuffled M-fold holdout.  Appendix E provides detailed derivations of these parameterizations.", "section": "2.2 How Reshuffling Affects the Loss Surface"}, {"figure_path": "C4SInFLvuB/tables/tables_15_1.jpg", "caption": "Table 1: Exemplary parametrizations in Equation (1) for resamplings; see Appendix E for details.", "description": "This table shows exemplary parameterizations for the equation (1) that is used to calculate the resampling-related component of the validation loss covariance. It presents values for \u03c3\u00b2 (increase in variance) and \u03c4\u00b2 (decrease in correlation) for different resampling methods (holdout, reshuffled holdout, M-fold CV, reshuffled M-fold CV, M-fold holdout, reshuffled M-fold holdout).  The parameters \u03c3\u00b2 and \u03c4\u00b2 describe the behavior of the loss surface affected by reshuffling, and a precise computation is provided in the appendix.", "section": "2 Theoretical Analysis"}, {"figure_path": "C4SInFLvuB/tables/tables_17_1.jpg", "caption": "Table 3: Exemplary Treatment of Resamplings in HPO Libraries and Software", "description": "This table summarizes how resampling is handled in various popular HPO libraries and software.  It indicates whether each library uses reshuffling (a technique where the training and validation splits are changed for each hyperparameter configuration) or uses fixed resampling splits.  A checkmark indicates that reshuffling is explicitly used in the core functionality or examples, a question mark denotes ambiguity or inconsistency across examples and core functionality, and an 'X' represents the absence of reshuffling.  The table highlights the overall lack of attention to reshuffling techniques in common HPO tools.", "section": "Extended Related Work"}, {"figure_path": "C4SInFLvuB/tables/tables_25_1.jpg", "caption": "Table 4: List of datasets used in benchmark experiments. All information can be found on OpenML (Vanschoren et al., 2014).", "description": "This table lists the ten datasets used in the benchmark experiments of the paper.  Each dataset is identified by its OpenML ID, name, and size (number of instances x number of features).  These datasets represent a variety of classification tasks and are used to evaluate the performance of different hyperparameter optimization strategies. The table provides a summary of the characteristics of each dataset, allowing for a better understanding of the experimental context and the potential challenges posed by each.", "section": "4.1 Experimental Setup"}, {"figure_path": "C4SInFLvuB/tables/tables_27_1.jpg", "caption": "Table 5: Search Space for Funnel-Shaped MLP Classifier.", "description": "This table presents the search space used for hyperparameter optimization (HPO) of a funnel-shaped Multilayer Perceptron (MLP) classifier.  It lists each hyperparameter, its data type (integer or numerical), the range of values it can take, and whether a logarithmic scale was used for the search.", "section": "4 Benchmark Experiments"}, {"figure_path": "C4SInFLvuB/tables/tables_27_2.jpg", "caption": "Table 1: Exemplary parametrizations in Equation (1) for resamplings; see Appendix E for details.", "description": "This table shows exemplary parametrizations used in Equation (1) for various resampling methods.  Each method (holdout, reshuffled holdout, M-fold CV, reshuffled M-fold CV, M-fold holdout, reshuffled M-fold holdout) is characterized by its \u03c3\u00b2 and \u03c4\u00b2 values, which quantify the variance increase and correlation decrease in the loss surface due to the resampling strategy.  Appendix E provides more details on the derivation of these parameters.", "section": "2.2 How Reshuffling Affects the Loss Surface"}, {"figure_path": "C4SInFLvuB/tables/tables_27_3.jpg", "caption": "Table 1: Exemplary parametrizations in Equation (1) for resamplings; see Appendix E for details.", "description": "This table shows exemplary parametrizations for Equation (1) in the paper, which describes how reshuffling affects the loss surface in hyperparameter optimization.  It provides values for \u03c3\u00b2, \u03c4\u00b2, and 1/\u03b1 for different resampling methods (holdout, reshuffled holdout, M-fold CV, reshuffled M-fold CV, M-fold holdout, and reshuffled M-fold holdout). These parameters quantify the increase in variance and the decrease in correlation of the loss surface due to reshuffling, which are important factors influencing the effectiveness of the reshuffling technique. Details of the calculations are provided in Appendix E.", "section": "2 Theoretical Analysis"}, {"figure_path": "C4SInFLvuB/tables/tables_27_4.jpg", "caption": "Table 8: Search Space for CatBoost Classifier.", "description": "This table presents the hyperparameter search space used for the CatBoost classifier in the benchmark experiments. It lists three hyperparameters: learning_rate, depth, and l2_leaf_reg, along with their data type (numerical or integer), range of values, and whether a logarithmic scale was used for the range.", "section": "4.1 Experimental Setup"}]