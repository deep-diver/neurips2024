[{"figure_path": "1wxFznQWhp/figures/figures_1_1.jpg", "caption": "Figure 1: Manifestation and impact of the reversal curse and thinking bias on diverse task settings. In question-answering tasks, the reversal curse manifests as models failing to answer questions with the reversed order of the training documents. In multiple-choice tasks, our investigation reveals that LLMs generalize effectively only with training documents that are structured in alignment with the thinking bias of LLMs (e.g., with name as the subject of the biographical fact).", "description": "This figure shows how the reversal curse and thinking bias affect LLMs' performance on different tasks.  The left side demonstrates question-answering tasks where the order of information in the training data affects the model's ability to answer questions with reversed information.  The right side shows multiple-choice questions where the LLM's ability to generalize is strongly linked to the alignment of the training data structure with the model's inherent thinking bias (a preference for using names to initiate the problem-solving process).  Different question types (Name-to-Description and Description-to-Name) are used to illustrate these points.", "section": "1 Introduction"}, {"figure_path": "1wxFznQWhp/figures/figures_4_1.jpg", "caption": "Figure 1: Manifestation and impact of the reversal curse and thinking bias on diverse task settings. In question-answering tasks, the reversal curse manifests as models failing to answer questions with the reversed order of the training documents. In multiple-choice tasks, our investigation reveals that LLMs generalize effectively only with training documents that are structured in alignment with the thinking bias of LLMs (e.g., with name as the subject of the biographical fact).", "description": "This figure demonstrates how the reversal curse and thinking bias affect LLMs' performance on different tasks.  The left side shows question-answering, where the model's inability to answer questions with reversed information from the training data exemplifies the reversal curse.  The right side uses multiple-choice questions to illustrate the thinking bias, showing that LLMs generalize well only when the training data structure aligns with their bias (e.g., using the name as the subject in a biographical fact).", "section": "1 Introduction"}, {"figure_path": "1wxFznQWhp/figures/figures_7_1.jpg", "caption": "Figure 2: Relative intensities of Snt and Sat across all layers of LLaMA2-7B and 13B models on celebrities dataset. Orange lines denote the relative intensity of the information flow from names. Blue lines denote the relative intensity of the information flow from descriptions.", "description": "This figure shows the relative intensities of information flow from names and descriptions to the answer position in the LLaMA2 models across different layers.  It visualizes the \"thinking bias\" of the models, where they prioritize name information when answering questions, even when the structure of the training data might suggest otherwise.  The orange lines represent the information flow from names, while the blue lines represent information flow from descriptions.  The pattern of the lines across layers supports the hypothesis that the models have a strong tendency to initiate their reasoning processes using names mentioned in the question.", "section": "3.2 Internal interactions via saliency score"}, {"figure_path": "1wxFznQWhp/figures/figures_7_2.jpg", "caption": "Figure 4: Multiple-choice test accuracies on the DescriptionIsName subset across training. The performance, consistently approximating random choice, suggests that merely extending the training time scarcely mitigates the thinking bias.", "description": "This figure shows the results of an experiment to determine if longer training times would mitigate the \"thinking bias\" identified in the paper.  The experiment used the DescriptionIsName subset of the training data, which the authors found particularly problematic for LLMs.  The graph plots the accuracy of four different LLMs across 20 epochs of training.  The results show that the accuracy remains consistently low (near random chance) despite the longer training time, indicating that the bias is not easily addressed through this method.", "section": "4 Attempts on thinking bias mitigation"}, {"figure_path": "1wxFznQWhp/figures/figures_8_1.jpg", "caption": "Figure 5: Results from mix training and QA finetuning mitigation experiments. Both strategies can only help models' performance on in-domain questions, while the near-random choice performance on out-of-domain (OOD) questions underscores the persistence of the thinking bias.", "description": "This figure shows the results of two mitigation strategies: mix training and QA finetuning, on the reversal curse phenomenon.  The results for both in-domain and out-of-domain questions are presented, for both N2D and D2N subtasks.  The key finding is that neither strategy effectively mitigates the thinking bias, as evidenced by the near-random performance on out-of-domain questions.", "section": "4.2 Mix training and QA finetuning"}, {"figure_path": "1wxFznQWhp/figures/figures_16_1.jpg", "caption": "Figure 1: Manifestation and impact of the reversal curse and thinking bias on diverse task settings. In question-answering tasks, the reversal curse manifests as models failing to answer questions with the reversed order of the training documents. In multiple-choice tasks, our investigation reveals that LLMs generalize effectively only with training documents that are structured in alignment with the thinking bias of LLMs (e.g., with name as the subject of the biographical fact).", "description": "This figure demonstrates how the \"reversal curse\" and \"thinking bias\" affect LLMs' performance on different tasks.  The \"reversal curse\" is shown in the question-answering section, where models struggle to answer questions when the question's order is reversed compared to the training data. The \"thinking bias\" is shown in the multiple-choice questions, where models perform well only if the training data's structure aligns with their inherent bias (e.g., name before description). This highlights how LLMs' generalization depends on both the content and structure of the training data.", "section": "1 Introduction"}, {"figure_path": "1wxFznQWhp/figures/figures_19_1.jpg", "caption": "Figure 1: Manifestation and impact of the reversal curse and thinking bias on diverse task settings. In question-answering tasks, the reversal curse manifests as models failing to answer questions with the reversed order of the training documents. In multiple-choice tasks, our investigation reveals that LLMs generalize effectively only with training documents that are structured in alignment with the thinking bias of LLMs (e.g., with name as the subject of the biographical fact).", "description": "This figure shows the results of two experiments designed to test the generalization abilities of large language models (LLMs). The first experiment uses a question-answering task, while the second uses a multiple-choice task. The results of the question-answering task show that LLMs struggle to generalize knowledge when the order of the facts is reversed from how they were presented during training. The results of the multiple-choice task show that LLMs perform better when the structure of the training data aligns with their inherent biases. This figure is intended to illustrate the manifestation of the \"reversal curse\" and \"thinking bias\" phenomena.", "section": "1 Introduction"}, {"figure_path": "1wxFznQWhp/figures/figures_19_2.jpg", "caption": "Figure 1: Manifestation and impact of the reversal curse and thinking bias on diverse task settings. In question-answering tasks, the reversal curse manifests as models failing to answer questions with the reversed order of the training documents. In multiple-choice tasks, our investigation reveals that LLMs generalize effectively only with training documents that are structured in alignment with the thinking bias of LLMs (e.g., with name as the subject of the biographical fact).", "description": "This figure illustrates how the reversal curse and thinking bias affect the performance of LLMs on different tasks.  The reversal curse is demonstrated by the inability of LLMs to answer questions when the order of information is reversed compared to their training data. The thinking bias is revealed by how LLMs only generalize effectively when the training data structure aligns with their internal processing preferences (e.g., prioritizing names as the subject in biographical facts). The figure showcases question-answering and multiple-choice question examples that highlight these phenomena.", "section": "1 Introduction"}, {"figure_path": "1wxFznQWhp/figures/figures_20_1.jpg", "caption": "Figure 1: Manifestation and impact of the reversal curse and thinking bias on diverse task settings. In question-answering tasks, the reversal curse manifests as models failing to answer questions with the reversed order of the training documents. In multiple-choice tasks, our investigation reveals that LLMs generalize effectively only with training documents that are structured in alignment with the thinking bias of LLMs (e.g., with name as the subject of the biographical fact).", "description": "This figure illustrates how the \"reversal curse\" and \"thinking bias\" affect LLMs' performance on different tasks.  The reversal curse is shown in the question-answering section, where models struggle when the question's order is reversed from the training data.  The thinking bias is highlighted in the multiple-choice section, where successful generalization only occurs when training data aligns with the model's bias (using names as subjects in biographical facts).", "section": "1 Introduction"}, {"figure_path": "1wxFznQWhp/figures/figures_26_1.jpg", "caption": "Figure 1: Manifestation and impact of the reversal curse and thinking bias on diverse task settings. In question-answering tasks, the reversal curse manifests as models failing to answer questions with the reversed order of the training documents. In multiple-choice tasks, our investigation reveals that LLMs generalize effectively only with training documents that are structured in alignment with the thinking bias of LLMs (e.g., with name as the subject of the biographical fact).", "description": "This figure illustrates the performance of LLMs on two different types of tasks: question answering and multiple choice.  The question answering task demonstrates the \"reversal curse\", where LLMs fail to generalize knowledge from \"A is B\" to \"B is A\". However, the multiple choice test reveals that LLMs can generalize better when the question and answer options align with an inherent \"thinking bias\" favoring names as subjects in biographical facts.  This suggests the generalization ability of LLMs is closely linked to the structure of the training data.", "section": "1 Introduction"}, {"figure_path": "1wxFznQWhp/figures/figures_29_1.jpg", "caption": "Figure 1: Manifestation and impact of the reversal curse and thinking bias on diverse task settings. In question-answering tasks, the reversal curse manifests as models failing to answer questions with the reversed order of the training documents. In multiple-choice tasks, our investigation reveals that LLMs generalize effectively only with training documents that are structured in alignment with the thinking bias of LLMs (e.g., with name as the subject of the biographical fact).", "description": "This figure shows the results of two experiments designed to test the generalization abilities of LLMs. The first experiment used a question-answering task, and the second experiment used a multiple-choice task. The results of the question-answering task showed that LLMs struggled to answer questions when the order of the information in the training data was reversed. However, the results of the multiple-choice task showed that LLMs were able to generalize better when the training data was structured in a way that aligned with their thinking bias. This suggests that LLMs have a bias towards using names to initiate their thinking process.", "section": "1 Introduction"}, {"figure_path": "1wxFznQWhp/figures/figures_29_2.jpg", "caption": "Figure 1: Manifestation and impact of the reversal curse and thinking bias on diverse task settings. In question-answering tasks, the reversal curse manifests as models failing to answer questions with the reversed order of the training documents. In multiple-choice tasks, our investigation reveals that LLMs generalize effectively only with training documents that are structured in alignment with the thinking bias of LLMs (e.g., with name as the subject of the biographical fact).", "description": "This figure illustrates how the reversal curse and thinking bias affect the performance of large language models (LLMs) on different tasks. The reversal curse is demonstrated by the failure of LLMs to answer questions where the order of information is reversed from how it was presented in the training data. The thinking bias is highlighted by the observation that LLMs generalize better when the training data aligns with their tendency to prioritize specific types of information (in this case, names). The figure contrasts the performance of LLMs on question-answering tasks and multiple-choice questions under both conditions.", "section": "1 Introduction"}, {"figure_path": "1wxFznQWhp/figures/figures_32_1.jpg", "caption": "Figure 1: Manifestation and impact of the reversal curse and thinking bias on diverse task settings. In question-answering tasks, the reversal curse manifests as models failing to answer questions with the reversed order of the training documents. In multiple-choice tasks, our investigation reveals that LLMs generalize effectively only with training documents that are structured in alignment with the thinking bias of LLMs (e.g., with name as the subject of the biographical fact).", "description": "This figure illustrates how the reversal curse and thinking bias affect LLMs' performance on different tasks.  The left side shows the question-answering task, where the model struggles when the question is a reversed version of the training data. The right side shows multiple choice questions, where the model's ability to generalize depends on whether the training data aligns with its inherent bias towards using names as the starting point for its analysis.  It highlights that the LLM's generalization ability is linked to the structure of the training data.", "section": "1 Introduction"}]