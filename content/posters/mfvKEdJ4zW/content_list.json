[{"type": "text", "text": "Latent Functional Maps: a spectral framework for representation alignment ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Marco Fumero\u2217 IST Austria marco.fumero@ist.ac.at ", "page_idx": 0}, {"type": "text", "text": "Marco Pegoraro\u2217 Sapienza, University of Rome pegoraro@di.uniroma1.it ", "page_idx": 0}, {"type": "text", "text": "Valentino Maiorca\u2020 Sapienza, University of Rome maiorca@di.uniroma1.it ", "page_idx": 0}, {"type": "text", "text": "Francesco Locatello IST Austria francesco.locatello@ist.ac.at ", "page_idx": 0}, {"type": "text", "text": "Emanuele Rodol\u00e0 Sapienza, University of Rome rodola@di.uniroma1.it ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Neural models learn data representations that lie on low-dimensional manifolds, yet modeling the relation between these representational spaces is an ongoing challenge. By integrating spectral geometry principles into neural modeling, we show that this problem can be better addressed in the functional domain, mitigating complexity, while enhancing interpretability and performances on downstream tasks. To this end, we introduce a multi-purpose framework to the representation learning community, which allows to: (i) compare different spaces in an interpretable way and measure their intrinsic similarity; (ii) find correspondences between them, both in unsupervised and weakly supervised settings, and (iii) to effectively transfer representations between distinct spaces. We validate our framework on various applications, ranging from stitching to retrieval tasks, and on multiple modalities, demonstrating that Latent Functional Maps can serve as a swiss-army knife for representation alignment. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Neural Networks (NNs) learn to represent high-dimensional data as elements of lower-dimensional latent spaces. While much attention is given to the model\u2019s output for tasks such as classification, generation, reconstruction, or denoising, understanding the internal representations and their geometry is equally important. This understanding facilitates reusing representations for downstream tasks [62, 14] and the comparison between different models [23, 39], thereby broadening the applicability of NNs, and understanding their structure and properties. Moreover, recent studies have shown that distinct neural models often develop similar representations when exposed to similar stimuli, a phenomenon observed in both biological [16, 25, 63] and artificial settings [38, 23, 39]. Notably, even when neural networks have different architectures, internal representations of distinct models can often be aligned through a linear transformation [61, 50]. This indicates a level of consistency in how NNs process information, highlighting the importance of exploring and understanding these internal representations. ", "page_idx": 0}, {"type": "image", "img_path": "mfvKEdJ4zW/tmp/faef1cf3feb3173cf1f589b2a848ebd55b6524f2728b7ff859df2e61f15895ef.jpg", "img_caption": ["Figure 1: Framework overview: given two spaces $\\mathcal{X}$ , $\\boldsymbol{\\wp}$ their samples lie on two manifolds $\\mathcal{M},\\mathcal{N}$ , which can be approximated with the KNN graphs $G_{\\mathcal{X}}\\mathrm{,}G_{\\mathcal{Y}}$ . We can optimize for a latent functional map $C$ between the eigenbases of operators defined on the graphs. This map serves as a map between functions defined on the two manifolds and can be leveraged for (i) comparing representational spaces, (ii) solving correspondence problems, and (iii) transferring information between the spaces. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "In this paper, we shift our focus from characterizing relationships between samples in distinct latent spaces to modeling a map among function spaces defined on these representational spaces. To this end, we propose leveraging spectral geometry principles by applying the framework of functional maps [44] to the field of representation learning. Functional maps represent correspondences between function spaces on different manifolds, providing a new perspective on the problem of representational alignment. In this setting, many complex constraints can be easily expressed compactly [45]. For instance, as shown in Figure 1, the mapping $(C)$ between two spaces $\\mathcal{M}$ and $\\mathcal{N}$ can be represented in the functional space as a linear map with a sparse structure. ", "page_idx": 1}, {"type": "text", "text": "Originally used in 3D geometry processing and more recently on graphs applications [60, 46], we extend this framework to handle arbitrary large dimensional latent spaces. Our proposed method, Latent Functional Map (LFM), is a flexible tool that allows (i) to compare distinct representational spaces, (ii) to find correspondences between them both in an unsupervised and weakly supervised way, and (iii) to transfer information between them. Our contributions can be listed as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We introduce the framework of Latent Functional Maps as a way to model the relation between distinct representational spaces of neural models.   \n\u2022 We show that LFM allows us to find correspondences between representational spaces, both in weakly supervised and unsupervised settings, and to transfer representations across distinct models.   \n\u2022 We showcase LFM capabilities as a meaningful and interpretable similarity measure between representational spaces.   \n\u2022 We validate our findings in retrieval and stitching tasks across different models, modalities and datasets, demonstrating that LFMs can lead to better performance and sample efficiency than other methods. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Similarity between latent spaces. Comparing representations learned by neural models is of fundamental importance for a diversity of tasks [63], ranging from representation analysis to latent space alignment and neural dynamics. In order to do so, a similarity measure between different spaces must be defined [22]. This can range from functional similarity (matching the performance of two models) to similarity defined in representational space [23], which is where our framework falls in. A classical statistical method is Canonical Correlation Analysis (CCA) [18], known for its invariance to linear transformations. Various adaptations of CCA aim to enhance robustness, such as Singular ", "page_idx": 1}, {"type": "text", "text": "Vector Canonical Correlation Analysis (SVCCA) [48], or to decrease sensitivity to perturbations using methods like Projection-Weighted Canonical Correlation Analysis (PWCCA) [38]. Closely related to these approaches, Centered Kernel Alignment (CKA) [23] measures the similarity between latent spaces while ignoring orthogonal transformations. However, recent research [10] reveals that CKA is sensitive to local shifts in the latent space. ", "page_idx": 2}, {"type": "text", "text": "We propose to leverage LFMs as a tool to measure the similarity, or how much two spaces differ from an isometry w.r.t. to the metric that has been used to construct the graph. ", "page_idx": 2}, {"type": "text", "text": "Representation alignment. Complementary to measuring the similarity of distinct representational spaces, several methods have been proposed to optimize for a transformation to align them [1]. The concept of latent space communication, introduced by [39], builds on the hypothesis that latent spaces across neural networks, varying random seed initialization to architecture or even data modality, are intrinsically compatible. This notion is supported by numerous empirical studies [38, 30, 23, 6, 55, 3, 58, 27, 29, 4, 40, 9, 15, 8], with the phenomenon being particularly evident in large and wide models [51, 33]. The core idea is that relations between data points (i.e., distances according to some metric) are preserved across different spaces because the high-level semantics of the data are the same and neural networks learn to encode them similarly [19]. With this \"relative representation\", the authors show that it is possible to stitch [29] together model components coming from different models, with little to no additional training, as long as a partial correspondence of the spaces involved is known. Indeed, [26, 32, 35, 37] show that a simple linear transformation is usually enough to effectively map one latent space into another, measuring the mapping performance via desired downstream tasks. ", "page_idx": 2}, {"type": "text", "text": "With LFMs, we change the perspective from merely relating samples of distinct latent spaces to relating function spaces defined on the manifold that the samples approximate, showing that processing information in this dual space is convenient as it boosts performance while also being interpretable. ", "page_idx": 2}, {"type": "text", "text": "Functional Maps. The representation we propose is directly derived from the functional maps framework for smooth manifolds introduced in the seminal work by [44]. This pioneering study proposed a compact and easily manipulable mapping between 3D shapes. Subsequent research has aimed at enhancing this framework. For instance, [41] introduced regularization techniques to improve the informativeness of the maps, while [34] developed refinement methods to achieve more accurate mappings. The functional map framework has been extended as well outside the 3d domain, for example, in [60] and [17], who applied the functional framework to model correspondences between graphs, and in [46], who demonstrated its utility in graph learning tasks. In particular, they have shown that the functional map representation retains its advantageous properties even when the Laplace basis is computed on a graph. ", "page_idx": 2}, {"type": "text", "text": "Inspired by these advancements, our work leverages the functional representation of latent spaces of neural models. We demonstrate how this representation can be easily manipulated to highlight similarities and facilitate the transfer of information between different spaces, thereby extending the applicability of the functional maps framework to the domain of neural latent spaces. ", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Background ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "This section provides the basic notions to understand the framework of functional maps applied to manifolds. We refer to [45] for a comprehensive overview. ", "page_idx": 2}, {"type": "text", "text": "Consider two manifolds $\\mathcal{M}$ and $\\mathcal{N}$ equipped with a basis $\\Phi^{\\mathcal{M}}$ (respectively $\\Phi^{N}$ ). Any squared integrable function $f:\\mathcal{M}\\rightarrow\\mathbb{R}$ can be represented as a linear combination of basis functions $\\Phi_{\\mathcal{M}}$ : $\\begin{array}{r}{f=\\sum_{i}a_{i}\\Phi_{i}^{\\mathcal{M}}=\\mathbf{a}\\Phi_{\\mathcal{M}}}\\end{array}$ . Given a bijective correspondence $T:\\mathcal{M}\\to\\mathcal{N}$ between points on these man ifolds, for any real-valued function $f:\\mathcal{M}\\rightarrow\\mathbb{R}$ , one can construct a corresponding function $g:\\mathcal{N}\\to\\mathbb{R}$ such that $g\\,=\\,f\\,\\circ\\,T^{-1}$ . In other words, the correspondence $T$ defines a mapping between two function spaces $T_{F}:\\mathcal{F}(\\mathcal{M},\\mathbb{R})\\to\\mathcal{F}(\\mathcal{N},\\mathbb{R})$ . Such a mapping is linear [44] and can be represented as a matrix $\\mathbf{C}$ such that for any function $f$ represented as a vector of coefficients a, we have $T_{F}(\\mathbf{a})=\\mathbf{C}\\mathbf{a}$ . ", "page_idx": 2}, {"type": "text", "text": "The functional representation is particularly well-suited for map inference (i.e., constrained optimization problems). When the underlying map $T$ (and by extension the matrix $\\mathbf{C}$ ) is unknown, many natural constraints on the map become linear constraints in its functional representation. In practice, the simplest method for recovering an unknown functional map is to solve the following optimization problem: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\underset{\\mathbf{C}}{\\arg\\operatorname*{min}}||\\mathbf{C}\\mathbf{A}-\\mathbf{B}||^{2}+\\rho(\\mathbf{C})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where A and $\\mathbf{B}$ are sets of corresponding functions, denoted as descriptors, expressed in the bases on $\\mathcal{M}$ and $\\mathcal{N}$ , respectively, and $\\bar{\\rho({\\bf C})}$ represents additional constraints deriving from the properties of the matrix $\\mathbf{C}$ [45]. When the manifolds $\\mathcal{M}$ and $\\mathcal{N}$ are approximately isometric and the descriptors are well-preserved by the (unknown) map, this procedure provides a good approximation of the underlying map. In cases where the correspondence $T$ is encoded as a permutation matrix $\\mathbf{P}$ , the functional map can be retrieved as $\\mathbf{C}=\\boldsymbol{\\Phi}_{\\mathcal{N}}^{\\dagger}\\mathbf{\\bar{P}}\\boldsymbol{\\Phi}_{\\mathcal{M}}$ where $\\Phi_{\\mathcal{M}}$ and $\\Phi_{\\mathcal{N}}$ are the bases of the functional spaces $\\mathcal{F}(\\mathcal{M},\\mathbb{R})$ and $\\mathcal{F}(\\mathcal{N},\\mathbb{R})$ , respectively, and $\\dagger$ denotes the Moore-Penrose pseudo-inverse. ", "page_idx": 3}, {"type": "text", "text": "3.2 Latent Functional Maps ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "3.2.1 Setting ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We consider deep neural networks $f:=\\;f_{1}\\circ f_{2}\\circ...f_{n}$ where each layer $f_{i}$ is associated to a representational space $\\mathcal{X}_{i}$ corresponding to the image of $f_{i}$ . In the following we\u2019re gonna drop the subscript $i$ , when the layer considered is clear form the context. We assume that elements $x\\in\\mathscr{X}$ are sampled from a latent manifold $\\mathcal{M}$ . Considering pairs of spaces $(\\mathcal{X},\\mathcal{Y})$ , and corresponding manifolds $\\mathcal{M},\\mathcal{N}$ our objective is to characterize the relation between them by mapping the space of functions ${\\mathcal{F}}({\\mathcal{M}})$ to ${\\mathcal{F}}({\\mathcal{N}})$ . Our framework can be summarized in three steps, which we are going to describe in the following sections: (i) how to represent $\\mathcal{M}$ from a sample estimate $X$ , by building a graph in the latent space $\\mathcal{X}$ (ii) how to encode any known preserved quantity between the two spaces by defining a set of descriptor function for each domain (iii) optimizing for the latent functional map between $\\mathcal{M}$ and $\\mathcal{N}$ . An illustrative description of our framework is depicted in Figure 1. ", "page_idx": 3}, {"type": "text", "text": "Building the graph. To leverage the geometry of the underlying manifold, we model the latent space of a neural network building a symmetric $\\boldsymbol{\\mathrm{k}}$ -nearest neighbor (k-NN) graph [31]. Given a set of samples $X=\\{x_{1},\\ldots,x_{n}\\}$ , we construct an undirected weighted graph $G=(X,E,\\mathbf{W})$ with nodes $X$ , edges $E$ , and weight matrix W. The weight matrix is totally characterized by the choice of a distance function $d:\\mathcal{X}\\times\\mathcal{X}\\mapsto\\mathbb{R}.$ .Suitable choices for $d$ include the $L2$ metric or the angular distance. Edges $E$ are defined as $E=\\{(x_{i},x_{j})\\in X\\times X\\mid x_{i}\\sim_{\\mathrm{k}}x_{j}$ or $x_{j}\\sim_{\\mathrm{k}}x_{i}\\}$ , where $x_{i}\\sim_{\\mathbf{k}}x_{j}$ indicates that $x_{j}$ is among the $k$ nearest neighbors of $x_{i}$ . The weight matrix $\\mathbf{W}\\in\\mathbb{R}_{\\geq0}^{n\\times n}$ assigns a weight $\\omega(x_{i},x_{j})\\,=\\,\\alpha(d(x_{i},x_{j}))$ to each edge $(x_{i},x_{j})\\,\\in\\,E$ , and $\\mathbf{W}_{i,j}\\,=\\,0$ otherwise, with $\\alpha$ being some monotonic function. Next, we define the associated weighted graph Laplacian operator $\\mathcal{L}_{G}=\\mathbf{I}-\\mathbf{D}^{-1/2}\\mathbf{W}\\mathbf{D}^{-1/2}$ , where $\\mathbf{D}$ is the diagonal degree matrix with entries $\\begin{array}{r}{\\mathbf{D}_{i,i}=\\sum_{j=1}^{n}\\mathbf{W}_{i,j}}\\end{array}$ . $\\mathcal{L}_{G}$ is a positive semi-definite, self-adjoint operator [57]), therefore, it admits an eigendecomposition $\\mathcal{L}_{G}=\\dot{\\Phi}_{G}\\Lambda_{G}\\Phi_{G}^{T}$ , where $\\Lambda_{G}$ is a diagonal matrix containing the eigenvalues, and $\\Phi_{G}$ is a matrix whose columns are the corresponding eigenvectors. The eigenvectors form an orthonormal basis for the space of functions defined on the graph nodes (i.e., $\\Phi_{G}^{T}\\mathbf{\\bar{\\Phi}}_{G}=\\mathbf{I})$ . We give comprehensive details on the choice of $k$ in Appendix A.1. ", "page_idx": 3}, {"type": "text", "text": "Choice of descriptors. To define the alignment problem between pair of spaces $\\mathcal{X},\\mathcal{Y}$ we will introduce the notion of descriptor function. We define as descriptor function any real valued function $f:G\\mapsto\\mathbb{R}^{k}$ defined on the nodes of the graph. Informally descriptors should encode the information which is shared between $\\mathcal{X}$ and $\\boldsymbol{\\wp}$ , either explicitly or implicitly . We categorize descriptor functions into supervised, weakly supervised and unsupervised descriptors. For the former we assumed to have access to a partial correspondence between the domains $\\mathcal{X},\\mathcal{Y}$ , defined as a bijective mapping $\\Gamma_{S}:{\\mathcal{A}}_{{\\mathcal{X}}}\\mapsto{\\mathcal{A}}_{\\mathcal{Y}}$ where $\\mathcal{A}_{\\mathcal{X}}\\subset\\mathcal{X},\\mathcal{A}_{\\mathcal{Y}}\\subset\\mathcal{Y}$ . Then descriptors takes the form of distance functions $d_{\\mathcal{X}}:\\mathcal{X}\\times\\mathcal{A}_{\\mathcal{X}}\\mapsto\\mathbb{R}^{+}$ , $d_{\\mathcal{Y}}:\\mathcal{Y}\\times\\Gamma_{S}(\\mathcal{A}_{\\mathcal{X}})\\mapsto\\mathbb{R}^{+}$ . As an example considering the geodesic distance (shortest path distance on the k-NN graph) from each node of the graph to the nodes in the anchor set, is an instance of supervised descriptor. In the case of weakly supervised descriptor, we assume to have access to an equivalence relation defined $\\Gamma_{W}:\\mathcal{A}_{\\mathcal{X}}\\times\\mathcal{A}_{\\mathcal{Y}}\\mapsto\\{0,1\\}$ . Example of these are multi-to-multi mappings, like mappings between labels. Unsupervised descriptors are quantities that depends on the topology and the geometry of the graph itself and they don\u2019t rely on any pre-given correspondence or equivalence relation. An example of this is the Heat Kernel Signature [53] node descriptor, based on heat diffusion over the manifold. We give examples of descriptors and ablation in Appendix C.2. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Computing LFMs. We now describe the optimization problem to compute a latent functional map between $\\mathcal{X}$ and $\\boldsymbol{\\wp}$ . We model each space using a subset of training samples $X=\\{x_{1},\\ldots,x_{n}\\}$ and $Y=\\{y_{1},\\ldots,y_{n}\\}$ and build the $\\boldsymbol{\\mathrm{k}}$ -NN graphs $G_{X}$ and $G_{Y}$ from these samples, respectively. For each graph, we compute the graph Laplacian ${\\mathcal{L}}_{G}$ and derive the first $k$ eigenvalues $\\Lambda_{G}$ and eigenvectors $\\Phi_{G}=[\\phi_{1},\\ldots,\\phi_{k}]$ , which serve as the basis for the function space defined on the latent spaces. Given the set of descriptors $\\mathbf{F}_{G_{X}}\\,=\\,[f_{1}^{G_{X}},\\dots,f_{n_{f}}^{G_{X}}]$ and $\\mathbf{F}_{G_{Y}}\\,=\\,[f_{1}^{G_{Y}},\\dots,f_{n_{f}}^{G_{Y}}]$ , we consider the optimization problem defined in Equation 1 and incorporate two regularizers which enforce commutativity for the Laplacian and the descriptors, respectively, with the map $\\mathbf{C}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\underset{\\mathbf{C}}{\\arg\\operatorname*{min}}\\|\\mathbf{C}\\hat{\\mathbf{F}}_{G_{X}}-\\hat{\\mathbf{F}}_{G_{Y}}\\|_{F}^{2}+\\alpha\\rho_{\\mathcal{L}}(\\mathbf{C})+\\beta\\rho_{f}(\\mathbf{C})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\hat{\\mathbf{F}}_{G}=\\Phi_{G}^{T}\\mathbf{F}_{G}$ are the spectral coefficients of the functions $\\mathbf{F}_{G},\\rho_{\\mathcal{L}}$ and $\\rho_{f}$ are the Laplacian and descriptor operator commutativity regularizers respectively, akin to [41]. We give full details on the regularizers in Appendix A. Once we have solved the optimization problem defined in Equation 2, we refine the resulting functional map $\\mathbf{C}$ using the spectral upsampling algorithm proposed by [34], as detailed in Appendix A.3. ", "page_idx": 4}, {"type": "text", "text": "3.3 LFMs as a similarity measure ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Once computed, the functional map $\\mathbf{C}$ can serve as a measure of similarity between spaces. The reason is that for isometric transformations between manifolds, the functional map is volume preserving (see Thm 5.1 in [44]), and this is manifested in orthogonal C. By defining the inner product between functions $h_{1},h_{2}\\,\\in\\,{\\mathcal{F}}(M)$ as $\\begin{array}{r}{\\langle h_{1},h_{2}\\rangle\\,=\\,\\int_{\\mathcal{M}}h_{1}(x)h_{2}(x)\\mu(x)}\\end{array}$ , it holds that $\\langle h_{1},\\dot{h}_{2}\\rangle\\,=\\,\\langle\\hat{h}_{1},\\hat{h}_{2}\\rangle$ when the map preserves the local area, where $\\hat{h}$ denotes the functional representation of $h$ . In other words, when the transformation between the two manifolds is an isometry, the matrix $\\mathbf{C}^{T}\\mathbf{C}$ will be diagonal. By measuring the ratio between the norm of the off-diagonal elements of $\\mathbf{C}^{T}\\mathbf{C}$ and the norm of the full matrix, we can define a measure of similarity $\\begin{array}{r}{s i m(X,Y)=1-\\frac{||\\mathrm{off}((\\mathbf{C}^{T}\\mathbf{C})||_{F}^{2}}{||\\mathbf{C}^{T}\\mathbf{C}||_{F}^{2}}}\\end{array}$ . In Appendix A.4 we prove that this similiarity measure is a proper distance on the space of functional maps, allowing to compare measurements from collections of spaces. Furthermore, this quantity is interpretable; the first eigenvector of $\\mathbf{C}^{T}\\mathbf{C}$ can act as a signal to localize the area of the target manifold where the map has higher distortion [43], as we show in the experiment in Figure 3a. ", "page_idx": 4}, {"type": "text", "text": "3.4 Transfering information with LFM ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The map C computed between two latent spaces can be utilized in various ways to transfer information from one space to the other. In this paper, we focus on two methods: (i) Expressing arbitrary points in the latent space as distance function on the graph and transferring them through the functional domain; (ii) Obtaining a point-to-point correspondence between the representational spaces from the LFM (see the first step in Appendix A.3) , starting from none to few known pairs, and leverage off-the-shelf methods to learn a transformation between the spaces. Additional strategies could be explored in future work. ", "page_idx": 4}, {"type": "text", "text": "Space of Functional Coefficients. The space of functional (or spectral) coefficients offers an alternative representation for points in the latent space $\\mathcal{X}$ . Using the equation $\\hat{f}_{G}=\\Phi_{G}^{T}f_{G}$ , any function $f_{G}\\in\\mathcal{F}(G,\\mathbb{R})$ can be uniquely represented by its functional coefficients $\\hat{f}_{G}$ . We leverage this property to represent any point $x\\in\\mathscr{X}$ as a distance function $f_{d}\\in\\mathcal{F}(G,\\mathbb{R})$ from the set of points $X_{G}$ , which correspond to the nodes of the graph $G$ . The functional map $\\mathbf{C}$ between two latent spaces $\\mathcal{X}$ and $\\boldsymbol{\\wp}$ aligns their functional representations, enabling the transfer of any function from the first space to the second. This functional alignment can be used similarly to the method proposed by [39] to establish a shared space where the representational spaces $\\mathcal{X}$ and $\\boldsymbol{\\wp}$ are aligned. ", "page_idx": 4}, {"type": "text", "text": "Finding correspondences. As explained in Section 3, the functional map $\\mathbf{C}$ represents the bijection $T$ in a functional form. [44] demonstrated that this bijection can be retrieved as a point-to-point map by finding the nearest neighbor for each row of $\\Phi_{G_{Y}}\\mathbf{C}$ in $\\Phi_{G_{X}}$ . This process can be efficiently implemented and scaled up using kd-tree structures or approximation strategies [21, 20]. Starting from an empty set or few known correspondences (anchors) between the two spaces $\\mathcal{X}$ and $\\boldsymbol{\\wp}$ , we can extend the correspondence to the entire set of nodes $X$ and $Y$ . This extended set of anchors can then be used to fit an arbitrary transformation between the latent spaces, for example an orthogonal mapping [32]. In our experiments, we demonstrate that by using a very small number of anchors (typically $\\leq50$ ), we can retrieve optimal transformations that facilitate near-perfect stitching and retrieval tasks. ", "page_idx": 4}, {"type": "image", "img_path": "mfvKEdJ4zW/tmp/7789ff21e1593142e443f3d5c87686d737d53b14d6257d7d426d681354272af4.jpg", "img_caption": ["Figure 2: Similarity across layers Similarity matrices between internal layer representations of CIFAR10 comparing our LFM-based similarity with the CCA and CKA baselines, averaged across 10 models. For each method, we report the accuracy scores for matching the corresponding layer by maximal similarity. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we present a series of experiments designed to evaluate the effectiveness of the Latent Functional Map (LFM) framework. We explore its application across various tasks, including similarity evaluation, stitching, retrieval performance, and robustness to perturbations in latent space. By comparing LFM to existing methods under different conditions, we aim to demonstrate its versatility and superior performance in aligning and analyzing neural network representations. Additional ablations and qualitative visualizations on the choice of distance metric to construct the graph and descriptors selection are reported in the Appendix C. ", "page_idx": 5}, {"type": "text", "text": "4.1 Analysis ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We demonstrate the benefits of using latent functional maps for comparing distinct representational spaces, using the similarity metric $s i m(X,Y)$ defined in Section 3.3 ", "page_idx": 5}, {"type": "text", "text": "Experimental setting In order to validate experimentally if LFMs can serve as a good measure of similarity between distinct representational spaces, we run the same sanity check as [23]. We train 10 CNN models (the architecture is depicted in Appendix B.1) on the CIFAR-10 dataset [24], changing the initialization seed. We compare their inner representations at each layer, excluding the logits and plot them as a similarity matrix, comparing with Central Kernel Alignment (CKA) measure [23] and Canonical Correlation Analysis (CCA) [18, 48]. We then measure the accuracy of identifying corresponding layers across models and report the results comparing with CKA and CCA as baselines. For CCA, we apply average pooling on the spatial dimensions to the embeddings of the internal layers, making it more stable numerically and boosting the results for this baseline compared to what was observed in [23]. ", "page_idx": 5}, {"type": "text", "text": "Result analysis Figure 2 shows that our LFM-based similarity measure behaves correctly as CKA does. Furthermore, the similarities are less spread around the diagonal, favoring a slightly higher accuracy score in identifying the corresponding layers across models. ", "page_idx": 5}, {"type": "text", "text": "While CKA (Centered Kernel Alignment) is a widely used similarity metric in deep learning, recent research by [10] has shown that it can produce unexpected or counter-intuitive results in certain situations. Specifically, CKA is sensitive to transformations that preserve the linear separability of two spaces, such as local translations. Our proposed similarity measure is robust to these changes and demonstrates greater stability compared to CKA. ", "page_idx": 5}, {"type": "image", "img_path": "mfvKEdJ4zW/tmp/19861c6d326607407a36414fb0fdaffdec3fc333aa94386d93565db86c5c64bc.jpg", "img_caption": ["Figure 3: Robustness of LFM similarity Left: Similarity scores as a function of perturbation strength: while the CKA baseline degrades, our LFM similarity scores are robust to perturbations that preserve linear separability of the space. Right: Visualization of area distortion of the map by projecting the first singular component of the LFM in the perturbed space: the distortion localizes on the samples of the perturbed class, making LFM similarity interpretable. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Experimental setting We compute the latent representations from the pooling layer just before the classification head for the CIFAR10 train and test sets. Following the setup in [10], we train a Support Vector Machine (SVM) classifier on the latent representations of the training samples to find the optimal separating hyperplane between samples of one class and others. We then perturb the samples by translating them in a direction orthogonal to the hyperplane, ensuring the space remains linearly separable. We measure the CKA and LFM similarities as functions of the perturbation vector norm, as shown in Figure 3a. In the accompanying plot on the right, we visualize the area distortion of the map by projecting the first singular component of the LFM C into the perturbed space and plotting it on a 2d TSNE [56] projection of the space. ", "page_idx": 6}, {"type": "text", "text": "Result Analysis We start by observing that when the latent space is perturbed in a way that still preserves its linear separability, it should be considered identical from a classification perspective, as this does not semantically affect the classification task. Figure 3a shows that while CKA degrades as a function of perturbation intensity, the LFM similarity remains stable to high scores. To understand this difference, we can visualize the area distortion as a function of the samples by projecting the first singular component of $\\mathbf{C}$ onto the perturbed space. In Figure 3b, we use t-SNE [56] to project the perturbed samples and the distortion function into 2D. The visualization reveals that distortion is localized to the samples corresponding to the perturbed class. ", "page_idx": 6}, {"type": "text", "text": "4.2 Zero-shot stitching ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In latent communication tasks, a common challenge is combining an encoder that embeds data with a decoder specialized in a downstream task, such as classification or reconstruction\u2014this process is often referred to as stitching. Previous works, like [29] and [2], have used trainable linear projections, known as stitching layers, to enable the swapping of different network components. However, [39] introduced the concept of zero-shot stitching, where neural components can be marged without any additional training procedure. It\u2019s important to note that while [39] still required to trained a decoder module once for processing relative representations before stitching could be performed, our method is fully zero-shot, with no need for additional training. In the following experiments, stitching is conducted without any training or fine-tuning of the encoder or decoder, adhering strictly to a zero-shot fashion. ", "page_idx": 6}, {"type": "text", "text": "Experimental Setting We consider four pre-trained image encoders (see Appendix B.2 for details) and stitch their latent spaces to perform classification using a Support Vector Machine (SVM) on five different datasets: CIFAR100 [24] with coarse and fine-grained labelling, ImageNet [11], CaltechUCSD Birds-200-2011 (CUB) [59], MNIST [12], AgNews [65]. These datasets were chosen to encompass both large-scale, complex datasets like ImageNet and a variety of modalities, including text and images. To evaluate the effectiveness of integrating the functional map, we extend the correspondences to determine an orthogonal transformation [32] between the latent spaces. For each encoder, we compute a graph of 3,000 points with 300 neighbors per node. We optimize the problem in Equation 2 using the first 50 eigenvectors of the graph Laplacian and consider two different descriptors: the distance functions defined from the anchors $(\\mathrm{LFM\\mathrm{+}O r t h o)}$ ) and the labels (LFM+Ortho (Labels)). For each dataset class, the latter provides an indicator function with 1 if the point belongs to the class and 0 otherwise. This descriptor does not require any anchor as input, representing a pioneering example of stitching requiring no additional information beyond the dataset. ", "page_idx": 6}, {"type": "image", "img_path": "mfvKEdJ4zW/tmp/d2fcd8999d634fb48df3015ca257591eb8dfaa5e4bae3e37270efa36aa76328b.jpg", "img_caption": ["Figure 4: Stitching results. Accuracy performance in stitching between image encoders trained on 6 datasets, comparing orthogonal transformation (Ortho) and LFM $^+$ Ortho at varying anchor counts. Also shown is LFM $\\pm$ Ortho (Labels), which uses the dataset labels instead of anchors. Results are presented with mean accuracy values reported on each box. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "mfvKEdJ4zW/tmp/faeb352c6450bca523c72a30d7e11f3348239416101943ebc2177360a6cc4ade.jpg", "img_caption": ["Figure 5: Qualitative results on stitching. We combine the encoder decoder modules of two convolutional autoencoder (E1 D1, E2 D2) using three different approaches: no transformation (Absol.), the orthogonal transformation from [32] (Ortho), the orthogonal transformation augmented with LFM (LFM). "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Result Analysis Figure 4 shows the accuracy results for all possible combinations of encoder stitching. The latent functional map (LFM $^+$ Ortho) consistently outperforms other methods across all datasets, particularly with a low number of anchors. Even without anchors, using label descriptors (LFM $\\cdot+$ Ortho (Labels)) achieves strong performance across different labeling schemes. The orthogonal transformation computed directly from the anchors (Ortho) only matches LFM\u2019s performance when more than 50 anchors are used, at which point LFM\u2019s effectiveness is constrained by the number of eigenvectors. Comparing the CIFAR100 coarse and fine-grained labeling, the fine-grained task is more complex due to the larger number of classes, making the estimation of the map more challenging. This complexity is especially evident when aligning self-supervised vision models with classification-based ones (e.g., DINO vs. ViT), where the training strategies differs. ", "page_idx": 7}, {"type": "image", "img_path": "mfvKEdJ4zW/tmp/5f772083d0ecb2121237ca4a90ff0d3565938a0548be3917e157137bc84dee41.jpg", "img_caption": ["(a) MRR score at increasing number of anchors "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "mfvKEdJ4zW/tmp/23c3d5c974bc7091c6e76dd58e9a79b0af250f4eb9728b4835cf675d390de1ca.jpg", "img_caption": ["Figure 6: Retrieval of word embeddings. We compare the retrieval performance of the functional map framework with state-of-the-art models as the number of anchors increases. The left panel shows the Mean Reciprocal Rank (MRR) across different numbers of anchors. The right panels depict the first two components of PCA for a subsample of the latent space (b) and the functional space (c), both before and after alignment using the functional map. ", "(c) Functional space "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "This experiment shows that the latent functional map is highly effective when few anchors are available $(\\le50)$ . It significantly enhances performance in zero-shot stitching tasks, outperforming direct orthogonal transformations at low or no anchor counts. This suggests that the latent functional map method provides a robust means of aligning latent spaces with minimal correspondence data, making it a valuable tool for tasks requiring the integration of independently trained models. ", "page_idx": 8}, {"type": "text", "text": "4.2.1 Qualitative example ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In Figure 5, we present qualitative experiments on the MNIST, FashionMNIST [64], and CIFAR-10 datasets, focusing on visualizing the reconstructions of stitched autoencoders. For these experiments, we trained convolutional autoencoders using two different seeds and stitched their encoder and decoder modules together. As a baseline, we used no transformation (Absol.), where the latent representation from the first encoder is directly input into the second decoder. We then compared this baseline with the orthogonal transformation from [32] (Ortho) and its LFM-augmented version (LFM). For both our method and [32], we used 10, 10 and 50 correspondences for the MNIST, FashionMNIST, and CIFAR-10 datasets, respectively. Our method consistently produced superior reconstruction quality compared to both the baseline and the method from [32]. ", "page_idx": 8}, {"type": "text", "text": "4.3 Retrieval ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We extend our analysis to the retrieval task, where we look for the most similar embedding in the aligned latent space. ", "page_idx": 8}, {"type": "text", "text": "Experimental Setting We consider two different English word embeddings, FastText [5] and Word2Vec [36]. Following the approach of [39], we extract embeddings of 20K words from their shared vocabulary using pre-trained models. We use 2K random corresponding samples to construct the k-NN graphs and evaluate the retrieval performance on the remaining 18K word embeddings. We test two settings in our experiments: ", "page_idx": 8}, {"type": "text", "text": "1. Aligning functional coefficients (LFM Space). ", "page_idx": 8}, {"type": "text", "text": "For this experiment, we construct k-NN graphs with a neighborhood size of 300 and compute the functional map using the first 50 eigenvectors. We evaluate the methods\u2019 performance using the Mean Reciprocal Rank (MRR), as detailed in Appendix B.4. Our functional map methods are compared with the method proposed by [39] (Relatives) and the orthogonal transformation method proposed by [32] (Ortho). We choose to fit the same orthogonal transformation on top of the correspondence found by LFM to make the comparison the fairest possible, although in principle any off-the-shelf methods could be used to estimate the transformation once the new correspondence is found. We illustrate this versatility in Table 3 of the Appendix, where LFM is combined with other methods in the same task. ", "page_idx": 9}, {"type": "text", "text": "Result Analysis Figure 6 shows the performance of these methods as the number of anchors increases. Numerical results are detailed in Table 2 in the Appendix. The functional map significantly improves performance with just 5 anchors, achieving an MRR of over 0.8. As the number of anchors increases, the performance of competing methods improves but still falls short of FMAP $^+$ Transform at 300 anchors, which reaches an MRR of 0.99. Interestingly, the performance of the functional map methods does not improve beyond 5 anchors, suggesting that this number of anchors is sufficient to achieve an optimal functional map between the spaces. These findings are further supported by Table 3 in the Appendix, where LFM consistently achieves superior MRR scores, particularly with fewer anchors, outperforming other baselines such as [29]. In Appendix C.3, we analyze how the results improve as the number of eigenvectors used to compute the functional map increases. Notably, the MRR score drastically increases to over 0.6 with more than 25 eigenvectors. ", "page_idx": 9}, {"type": "text", "text": "These results confirm that the latent functional map is a valuable tool in settings with little knowledge about correspondences. It significantly enhances retrieval performance with a minimal number of anchors, making it an efficient approach for aligning latent spaces. Moreover, its performance can be improved using a higher number of eigenvectors. In particular, our framework comprises at the same time (i) an interpretable similarity measure, (ii) a way to find correspondences, and (iii) solving for an explicit mapping between different spaces in a single framework, differently from [39] and [32] which attempt to solve just the latter implicitly. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we explored the application of latent functional maps (LFM) to model and analyze the relationships between different latent spaces. Our approach leverages the principles of spectral geometry, providing a robust framework for comparing and aligning representations across various models and settings. By extending the functional map framework to high-dimensional latent spaces, we offer a versatile method for comparing distinct representational spaces, finding correspondences between them in both unsupervised and weakly supervised settings (few or no anchors) and transferring information across different models. ", "page_idx": 9}, {"type": "text", "text": "Limitations and future work The performance of LFM can be sensitive to the number of eigenvectors used, as observed in some of our experiments (see Appendix C.3). Finding the optimal number of eigenvectors without extensive experimentation remains an open problem. The current framework assumes a correspondence between the domains, and is not directly adapted in order to deal with partial mappings, where just a subset of one domain can be mapped to other. Extending the results in [49, 47] to our neural representation setting, is a promising way to overcome the full correspondence assumption. Future research can further explore the potential of LFM in other domains and tasks, and modalities. The versatility of the functional representation can be further explored to define new ad-hoc constraints and regularizers for different settings. In particular, ts performance in fully unsupervised settings, while promising is still an area requiring further research and improvement. In conclusion, the Latent Functional Map framework offers a novel and effective approach to understanding and utilizing neural network representations, promising significant advancements in both theoretical and practical aspects of machine learning. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "MF is supported by the MSCA IST-Bridge fellowship which has received funding from the European Union\u2019s Horizon 2020 research and innovation program under the Marie Sk\u0142odowska-Curie grant agreement No 101034413. ER and VM are supported by the PNRR MUR project PE0000013-FAIR. MP is supported by the Sapienza grant \"Predicting and Explaining Clinical Trial Outcomes\", prot. RG12218166FA3F13. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] D. Alvarez-Melis and T. S. Jaakkola. Gromov-wasserstein alignment of word embedding spaces. arXiv preprint arXiv:1809.00013, 2018.   \n[2] Y. Bansal, P. Nakkiran, and B. Barak. Revisiting model stitching to compare neural representations. Advances in neural information processing systems, 34:225\u2013236, 2021.   \n[3] S. Barannikov, I. Trofimov, N. Balabin, and E. Burnaev. Representation topology divergence: A method for comparing neural network representations. In K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, and S. Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 1607\u20131626. PMLR, 2022.   \n[4] Y. Bengio, A. Courville, and P. Vincent. Representation Learning: A Review and New Perspectives. ArXiv preprint, abs/1206.5538, 2012.   \n[5] P. Bojanowski, E. Grave, A. Joulin, and T. Mikolov. Enriching word vectors with subword information. Transactions of the association for computational linguistics, 5:135\u2013146, 2017.   \n[6] L. Bonheme and M. Grzes. How do variational autoencoders learn? insights from representational similarity. ArXiv preprint, abs/2205.08399, 2022.   \n[7] J. Calder and N. G. Trillos. Improved spectral convergence rates for graph laplacians on $\\varepsilon$ -graphs and k-nn graphs. Applied and Computational Harmonic Analysis, 60:123\u2013175, 2022.   \n[8] I. Cannistraci, L. Moschella, M. Fumero, V. Maiorca, and E. Rodol\u00e0. From bricks to bridges: Product of invariances to enhance latent space communication. In International Conference on Learning Representations, 2024.   \n[9] T. A. Chang, Z. Tu, and B. K. Bergen. The geometry of multilingual language model representations. ACL, 2022.   \n[10] M. Davari, S. Horoi, A. Natik, G. Lajoie, G. Wolf, and E. Belilovsky. Reliability of cka as a similarity measure in deep learning. arXiv preprint arXiv:2210.16156, 2022.   \n[11] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee, 2009.   \n[12] L. Deng. The mnist database of handwritten digit images for machine learning research [best of the web]. IEEE signal processing magazine, 29(6):141\u2013142, 2012.   \n[13] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.   \n[14] M. Fumero, F. Wenzel, L. Zancato, A. Achille, E. Rodol\u00e0, S. Soatto, B. Sch\u00f6lkopf, and F. Locatello. Leveraging sparse and shared feature activations for disentangled representation learning. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 27682\u201327698. Curran Associates, Inc., 2023.   \n[15] F. Guth, B. M\u00e9nard, G. Rochette, and S. Mallat. A rainbow in deep network black boxes. arXiv preprint arXiv:2305.18512, 2023.   \n[16] J. V. Haxby, M. I. Gobbini, M. L. Furey, A. Ishai, J. L. Schouten, and P. Pietrini. Distributed and overlapping representations of faces and objects in ventral temporal cortex. Science, 293(5539):2425\u20132430, 2001.   \n[17] J. Hermanns, A. Tsitsulin, M. Munkhoeva, A. Bronstein, D. Mottin, and P. Karras. Grasp: Graph alignment through spectral signatures. In Asia-Pacific Web (APWeb) and Web-Age Information Management (WAIM) Joint International Conference on Web and Big Data, pages 44\u201352. Springer, 2021.   \n[18] H. Hotelling. Relations between two sets of variates. Breakthroughs in statistics: methodology and distribution, pages 162\u2013190, 1992.   \n[19] M. Huh, B. Cheung, T. Wang, and P. Isola. Position: The platonic representation hypothesis. In Forty-first International Conference on Machine Learning, 2024.   \n[20] P. Indyk and R. Motwani. Approximate nearest neighbors: towards removing the curse of dimensionality. In Proceedings of the thirtieth annual ACM symposium on Theory of computing, pages 604\u2013613, 1998.   \n[21] J. Johnson, M. Douze, and H. J\u00e9gou. Billion-scale similarity search with GPUs. IEEE Transactions on Big Data, 7(3):535\u2013547, 2019.   \n[22] M. Klabunde, T. Schumacher, M. Strohmaier, and F. Lemmerich. Similarity of neural network models: A survey of functional and representational measures. arXiv preprint arXiv:2305.06329, 2023.   \n[23] S. Kornblith, M. Norouzi, H. Lee, and G. Hinton. Similarity of neural network representations revisited. In International conference on machine learning, pages 3519\u20133529. PMLR, 2019.   \n[24] A. Krizhevsky, G. Hinton, et al. Learning multiple layers of features from tiny images. Technical Report, 2009.   \n[25] A. Laakso and G. Cottrell. Content and cluster analysis: Assessing representational similarity in neural systems. Philosophical Psychology, 13:47 \u2013 76, 2000.   \n[26] Z. L\u00e4hner and M. Moeller. On the direct alignment of latent spaces. In Proceedings of UniReps: the First Workshop on Unifying Representations in Neural Models, volume 243 of Proceedings of Machine Learning Research, pages 158\u2013169. PMLR, 2024.   \n[27] G. Lample, A. Conneau, M. Ranzato, L. Denoyer, and H. J\u00e9gou. Word translation without parallel data. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018.   \n[28] G. Lample, A. Conneau, M. Ranzato, L. Denoyer, and H. J\u00e9gou. Word translation without parallel data. In International conference on learning representations, 2018.   \n[29] K. Lenc and A. Vedaldi. Understanding image representations by measuring their equivariance and equivalence. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA, USA, June 7-12, 2015, pages 991\u2013999. IEEE Computer Society, 2015.   \n[30] Y. Li, J. Yosinski, J. Clune, H. Lipson, and J. E. Hopcroft. Convergent learning: Do different neural networks learn the same representations? In Y. Bengio and Y. LeCun, editors, 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016.   \n[31] M. Maier, M. Hein, and U. von Luxburg. Optimal construction of k-nearest-neighbor graphs for identifying noisy clusters. Theoretical Computer Science, 410(19):1749\u20131764, 2009. Algorithmic Learning Theory.   \n[32] V. Maiorca, L. Moschella, A. Norelli, M. Fumero, F. Locatello, and E. Rodol\u00e0. Latent space translation via semantic alignment. Advances in Neural Information Processing Systems, 36, 2024.   \n[33] R. Mehta, V. Albiero, L. Chen, I. Evtimov, T. Glaser, Z. Li, and T. Hassner. You only need a good embeddings extractor to fix spurious correlations. ArXiv, 2022.   \n[34] S. Melzi, J. Ren, E. Rodol\u00e0, A. Sharma, P. Wonka, M. Ovsjanikov, et al. Zoomout: spectral upsampling for efficient shape correspondence. ACM TRANSACTIONS ON GRAPHICS, 38(6):1\u2013 14, 2019.   \n[35] J. Merullo, L. Castricato, C. Eickhoff, and E. Pavlick. Linearly mapping from image to text space. In The Eleventh International Conference on Learning Representations, 2023.   \n[36] T. Mikolov, Q. V. Le, and I. Sutskever. Exploiting similarities among languages for machine translation. arXiv preprint arXiv:1309.4168, 2013.   \n[37] M. Moayeri, K. Rezaei, M. Sanjabi, and S. Feizi. Text-to-concept (and back) via cross-model alignment. In International Conference on Machine Learning, pages 25037\u201325060. PMLR, 2023.   \n[38] A. Morcos, M. Raghu, and S. Bengio. Insights on representational similarity in neural networks with canonical correlation. Advances in Neural Information Processing Systems, 31, 2018.   \n[39] L. Moschella, V. Maiorca, M. Fumero, A. Norelli, L. Francesco, E. Rodola, et al. Relative representations enable zero-shot latent space communication. In International Conference on Learning Representations, 2023.   \n[40] Y. Movshovitz-Attias, A. Toshev, T. K. Leung, S. Ioffe, and S. Singh. No fuss distance metric learning using proxies. In IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017, pages 360\u2013368. IEEE Computer Society, 2017.   \n[41] D. Nogneng and M. Ovsjanikov. Informative descriptor preservation via commutativity for shape matching. In Computer Graphics Forum, volume 36, pages 259\u2013267. Wiley Online Library, 2017.   \n[42] M. Oquab, T. Darcet, T. Moutakanni, H. Vo, M. Szafraniec, V. Khalidov, P. Fernandez, D. Haziza, F. Massa, A. El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023.   \n[43] M. Ovsjanikov, M. Ben-Chen, F. Chazal, and L. Guibas. Analysis and visualization of maps between shapes. In Computer Graphics Forum, volume 32, pages 135\u2013145. Wiley Online Library, 2013.   \n[44] M. Ovsjanikov, M. Ben-Chen, J. Solomon, A. Butscher, and L. Guibas. Functional maps: a flexible representation of maps between shapes. ACM Transactions on Graphics (ToG), 31(4):1\u201311, 2012.   \n[45] M. Ovsjanikov, E. Corman, M. Bronstein, E. Rodol\u00e0, M. Ben-Chen, L. Guibas, F. Chazal, and A. Bronstein. Computing and processing correspondences with functional maps. In SIGGRAPH ASIA 2016 Courses, pages 1\u201360. ACM, 2016.   \n[46] M. Pegoraro, R. Marin, A. Rampini, S. Melzi, L. Cosmo, and E. Rodol\u00e0. Spectral maps for learning on subgraphs. In NeurIPS 2023 Workshop on Symmetry and Geometry in Neural Representations, 2023.   \n[47] E. Postolache, M. Fumero, L. Cosmo, and E. Rodol\u00e0. A parametric analysis of discrete hamiltonian functional maps. Computer Graphics Forum, 39(5):103\u2013118, 2020.   \n[48] M. Raghu, J. Gilmer, J. Yosinski, and J. Sohl-Dickstein. Svcca: Singular vector canonical correlation analysis for deep learning dynamics and interpretability. Advances in neural information processing systems, 30, 2017.   \n[49] E. Rodol\u00e0, L. Cosmo, M. M. Bronstein, A. Torsello, and D. Cremers. Partial functional correspondence, 2015.   \n[50] G. Roeder, L. Metz, and D. Kingma. On linear identifiability of learned representations. In International Conference on Machine Learning, pages 9030\u20139039. PMLR, 2021.   \n[51] G. Somepalli, L. Fowl, A. Bansal, P. Yeh-Chiang, Y. Dar, R. Baraniuk, M. Goldblum, and T. Goldstein. Can neural nets learn the same model twice? investigating reproducibility and double descent from the decision boundary perspective. IEEE CVF, 2022.   \n[52] J. T. Springenberg, A. Dosovitskiy, T. Brox, and M. Riedmiller. Striving for simplicity: The all convolutional net. arXiv preprint arXiv:1412.6806, 2014.   \n[53] J. Sun, M. Ovsjanikov, and L. Guibas. A concise and provably informative multi-scale signature based on heat diffusion. In Computer graphics forum, volume 28, pages 1383\u20131392. Wiley Online Library, 2009.   \n[54] D. Ting, L. Huang, and M. Jordan. An analysis of the convergence of graph laplacians. arXiv preprint arXiv:1101.5435, 2011.   \n[55] A. Tsitsulin, M. Munkhoeva, D. Mottin, P. Karras, A. M. Bronstein, I. V. Oseledets, and E. M\u00fcller. The shape of data: Intrinsic distance for data distributions. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020.   \n[56] L. Van der Maaten and G. Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(11), 2008.   \n[57] U. Von Luxburg. A tutorial on spectral clustering. Statistics and computing, 17:395\u2013416, 2007.   \n[58] I. Vulic\u00b4, S. Ruder, and A. S\u00f8gaard. Are all good word vector spaces isomorphic? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3178\u20133192, Online, 2020. Association for Computational Linguistics.   \n[59] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. The caltech-ucsd birds-200-2011 dataset. 2011.   \n[60] F.-D. Wang, N. Xue, Y. Zhang, G.-S. Xia, and M. Pelillo. A functional representation for graph matching. IEEE transactions on pattern analysis and machine intelligence, 2019.   \n[61] L. Wang, L. Hu, J. Gu, Y. Wu, Z. Hu, K. He, and J. Hopcroft. Towards understanding learning representations: To what extent do different neural networks learn the same representation, 2018.   \n[62] K. Weiss, T. M. Khoshgoftaar, and D. Wang. A survey of transfer learning. Journal of Big data, 3:1\u201340, 2016.   \n[63] A. H. Williams, E. Kunz, S. Kornblith, and S. Linderman. Generalized shape metrics on neural representations. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages 4738\u20134750. Curran Associates, Inc., 2021.   \n[64] H. Xiao, K. Rasul, and R. Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.   \n[65] X. Zhang, J. Zhao, and Y. LeCun. Character-level convolutional networks for text classification. Advances in neural information processing systems, 28, 2015. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 14}, {"type": "text", "text": "Justification: The abstract and introduction gives an overview of the paper\u2019s scope clearly stating the contributions in a dotted list. ", "page_idx": 14}, {"type": "text", "text": "Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 14}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Justification: We put our limitations in the conclusion, after analyzing the results of our method. ", "page_idx": 14}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 14}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 14}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 15}, {"type": "text", "text": "Justification: The paper does not include new theoretical results. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 15}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: In the Experiment and Appendix, we report all the information needed to reproduce the experiments. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 15}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: We will share the code after the paper acceptance. All the data we use are open-source and publically available. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 16}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: We report all the training and test details in the Experiment and Appendix. Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 16}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: The experiments that are run over multiple settings are reported as error bars that depict the whole distribution of the results obtained. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: In the Appendix we report information about the resources used for our experiments. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 17}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: We have read and respect the NeurIPS Code of Ethics. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 17}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 17}, {"type": "text", "text": "Justification: Our method is not tied to particular applications. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 17}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 18}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "Justification: There are no risks posed by our work. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 18}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: All the models and data used in our experiments are properly cited. Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 19}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 19}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 19}, {"type": "text", "text": "A Latent Functional Maps ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "A.1 Building the graph ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In the following section we give more details about how to construct the graph in the latent space in order to approximate the manifold domain from a sample estimate. ", "page_idx": 20}, {"type": "text", "text": "Throughout the paper, we assume the eigenvalues (and corresponding eigenvectors) are sorted in non-descending order $0=\\Lambda_{1}\\leq\\Lambda_{2}\\leq\\cdot\\cdot\\cdot\\leq\\Lambda_{n}$ . One may consider a subset of eigenvectors, namely those associated with the $k$ smallest eigenvalues, to compactly approximate a graph signal, employing techniques akin to Fourier analysis. As demonstrated in multiple recent works [54, 7], the eigenvalues and eigenvectors of the graph Laplacian associated with a $\\mathbf{k}$ -NN graph approximate the weighted Laplace-Beltrami operator, placing us in a setting similar to the original one of [44]. In particular in [7] it has been derived optimal bounds for the choices of $k$ , bounds based on the dimensionality of the underlying manifold and the number of sampled points. In our experiments we choose our parameter $k$ according to these estimates. For an estimate of manifold dimensionality we use the latent space dimensionality, which correspond to an upper bound to it. In practice, we typically set $k=300$ with a graph in the order of few thousands nodes. ", "page_idx": 20}, {"type": "text", "text": "A.2 Additional Regularizers ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In Equation 2, we improve the computation of the functional map by incorporating two additional regularizers: Laplacian commutativity and descriptor operator commutativity. Both regularizers exploit the preservation of linear functional operators $\\mathbf{S}^{G}:\\mathcal{F}(G,\\mathbb{R})\\to\\mathcal{F}(G,\\dot{\\mathbb{R}})$ , enforcing that the functional map $\\mathbf{C}$ commutes with these operators: $\\|\\mathbf{S}_{i}^{G}\\mathbf{C}-\\mathbf{CS}_{i}^{G_{X}}\\|_{F}=0$ . ", "page_idx": 20}, {"type": "text", "text": "The Laplacian commutativity regularizer, first introduced by [44], is formulated as: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\rho_{\\mathcal{L}}(\\mathbf{C})=\\|\\mathbf{A}_{G_{Y}}\\mathbf{C}-\\mathbf{C}\\mathbf{A}_{G_{X}}\\|_{F}^{2}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\Lambda_{G}$ represents the diagonal matrices of eigenvalues. This regularizer ensures that the functional map $\\mathbf{C}$ preserves the spectral properties of the Laplacian. ", "page_idx": 20}, {"type": "text", "text": "The descriptor operator commutativity regularizer, introduced by [41], extracts more detailed information from a given descriptor, resulting in a more accurate functional map even with fewer descriptors. The formulation of this regularizer is as follows: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\rho_{f}(\\mathbf{C})=\\sum_{i}\\|\\mathbf{S}_{i}^{G_{Y}}\\mathbf{C}-\\mathbf{C}\\mathbf{S}_{i}^{G_{X}}\\|_{F}^{2}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\mathbf{S}_{i}^{G}=\\Phi_{G}^{T}(f_{i}^{G})\\Phi_{G}$ are the descriptor operators. ", "page_idx": 20}, {"type": "text", "text": "A.3 Spectral refinement ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Once we have solved the optimization problem defined in Equation 2, we refine the resulting functional map $\\mathbf{C}$ using the spectral upsampling algorithm proposed by [34]. We start by considering the matrix $\\mathbf{C}$ as the submatrix of size $k\\times k$ of the current LFM estimate. Then we apply iteratively the following two steps: ", "page_idx": 20}, {"type": "text", "text": "1. Compute the pointwise map $T$ encoded as matrix $\\mathbf{\\delta}\\pi$ from the current LFM estimate $\\mathbf{C}$ via solving: $\\arg\\operatorname*{min}_{\\boldsymbol{\\cdot}}\\big\\|\\mathbf{C}(\\boldsymbol{\\Phi}_{G_{Y}}^{\\cdot,y})-\\boldsymbol{\\Phi}_{G_{X}}^{\\cdot,x}\\big)\\big\\|_{2}\\ \\forall\\boldsymbol{x}\\in\\boldsymbol{X}$ , where $\\Phi^{:,x}$ correspond to the $x$ -th column y of the matrix   \n2. Set Ck+1 = \u03a6:G,kXT \u03a0 , where $\\Phi^{:,k}$ correspond to the $k$ -th column of the matrix $\\Phi$ . ", "page_idx": 20}, {"type": "text", "text": "A.4 LFM similarity properties ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section we will demonstrate that the LFM similarity property sim(X, Y ) = 1 \u2212\u2225o\u2225ff((C(TC TC )C\u2225)2F\u22252F, defined in Section 3.3 is a metric, as it satisfies, positiveness, simmetry and triangle inequality. ", "page_idx": 20}, {"type": "text", "text": "Namely, for a pairs of spaces $X,Y$ : ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle s i m(X,Y)\\geq0}\\\\ {\\displaystyle s i m(X,Y)=s i m(Y,X)}\\\\ {\\displaystyle s i m(X,Z)\\leq s i m(X,Y)+s i m(Y,Z)}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Since $\\begin{array}{r l r}{\\|\\mathrm{diag}({\\bf C}^{T}{\\bf C})\\|_{F}^{2}~+~\\|\\mathrm{off}({\\bf C}^{T}{\\bf C})\\|_{F}^{2}}&{=}&{\\|({\\bf C}^{T}{\\bf C})\\|_{F}^{2}}\\end{array}$ we will consider $\\begin{array}{r l}{s i m(X,Y)}&{{}=}\\end{array}$ $\\begin{array}{r}{s i m(X,Y)\\,=\\,\\frac{\\|\\mathrm{diag}(\\mathbf{C}^{T}\\mathbf{C})\\|_{F}^{2}}{\\|(\\mathbf{C}^{T}\\mathbf{C})\\|_{F}^{2}}}\\end{array}$ for simplicity. The former property is trivial as the ration between two norms is positive. For proving simmetry we have that: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{s i m(\\boldsymbol{X},\\boldsymbol{Y})=\\frac{\\|\\mathrm{diag}(\\mathbf{C}^{T}\\mathbf{C})\\|_{F}^{2}}{\\|(\\mathbf{C}^{T}\\mathbf{C})\\|_{F}^{2}}}\\\\ &{s i m(\\boldsymbol{X},\\boldsymbol{Y})=\\frac{\\|\\mathrm{diag}(\\Phi_{X}^{T}\\Phi_{Y})\\|_{F}^{2}}{\\|(\\Phi_{X}^{T}\\Phi_{Y})\\|_{F}^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "for the numerator $\\|\\big(\\Phi_{X}^{T}\\Phi_{Y}\\big)\\|_{F}$ we trivially have $d i a g(\\Phi_{X}^{T}\\Phi_{Y})=d i a g(\\Phi_{Y}^{T}\\Phi_{X})$ . For the denominator: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname{Since}:\\lvert\\lvert\\Phi\\rvert\\rvert_{F}=\\sqrt{\\sum_{i,j}\\lvert\\Phi_{i,j}\\rvert^{2}}=\\sqrt{T r(\\Phi^{T}\\Phi)}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We have that: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\Phi_{X}^{T}\\Phi_{Y}\\|_{F}^{2}\\stackrel{?}{=}\\|\\Phi_{Y}^{T}\\Phi_{X}\\|_{F}^{2}}\\\\ &{T r(\\Phi_{X}^{T}\\Phi_{Y})^{T}(\\Phi_{X}^{T}\\Phi_{Y})\\stackrel{?}{=}T r(\\Phi_{Y}^{T}\\Phi_{X})^{T}(\\Phi_{Y}^{T}\\Phi_{X})}\\\\ &{T r(\\Phi_{Y}^{T}\\Phi_{X}\\Phi_{X}^{T}\\Phi_{Y})=T r(\\Phi_{X}^{T}\\Phi_{Y}\\Phi_{Y}^{T}\\Phi_{X})}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the last equality is verified for the Trace operator being invariant under cyclic permutations. ", "page_idx": 21}, {"type": "text", "text": "For proving triangle inequality we will assume that the matrices of eigenvectors are full rank (i.e. all eigenvectors are considered). We have to prove the following: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{1-\\frac{\\|\\mathrm{diag}\\left(\\Phi_{X}^{T}\\Phi_{Z}\\right)\\|_{F}^{2}}{\\|\\left(\\Phi_{X}^{T}\\Phi_{Z}\\right)\\|_{F}^{2}}\\leq2-(\\frac{\\|\\mathrm{diag}\\left(\\Phi_{X}^{T}\\Phi_{Y}\\right)\\|_{F}^{2}}{\\|\\left(\\Phi_{X}^{T}\\Phi_{Y}\\right)\\|_{F}^{2}}+\\frac{\\|\\mathrm{diag}\\left(\\left(\\Phi_{Y}^{T}\\Phi_{Z}\\right)\\|_{F}^{2}\\right)}{\\|\\left(\\Phi_{Y}^{T}\\Phi_{Z}\\right)\\|_{F}^{2}})}\\\\ {\\frac{\\|\\mathrm{diag}\\left(\\Phi_{X}^{T}\\Phi_{Z}\\right)\\|_{F}^{2}}{\\|\\left(\\Phi_{X}^{T}\\Phi_{Z}\\right)\\|_{F}^{2}}\\geq(\\frac{\\|\\mathrm{diag}\\left(\\Phi_{X}^{T}\\Phi_{Y}\\right)\\|_{F}^{2}}{\\|\\left(\\Phi_{X}^{T}\\Phi_{Y}\\right)\\|_{F}^{2}}+\\frac{\\|\\mathrm{diag}\\left(\\left(\\Phi_{Y}^{T}\\Phi_{Z}\\right)\\|_{F}^{2}\\right)}{\\|\\left(\\Phi_{Y}^{T}\\Phi_{Z}\\right)\\|_{F}^{2}})-1}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Using the fact that the Frobenius norm is invariant under multiplication by orthogonal matrices the denominator of each term in Eq14 becomes: $\\|(\\Phi_{X}^{T}\\Phi_{Y})\\|_{F}^{2}=\\|\\Phi_{X}\\|_{F}^{2}=\\'{T r}[\\Phi_{X}^{T}\\Phi_{X}]={T r}[I d]=$ $N$ . Then: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{\\|\\mathrm{diag}(\\Phi_{X}^{T}\\Phi_{Z})\\|_{F}^{2}}{N}\\geq(\\frac{\\|\\mathrm{diag}(\\Phi_{X}^{T}\\Phi_{Y})\\|_{F}^{2}}{N}+\\frac{\\|\\mathrm{diag}((\\Phi_{Y}^{T}\\Phi_{Z})\\|_{F}^{2}}{N})-1\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Since $\\|\\mathrm{diag}(\\mathbf{X}_{X}^{T}\\mathbf{Y})\\|_{F}^{2}\\leq\\|(\\mathbf{X}_{X}^{T}\\mathbf{Y})\\|_{F}^{2}$ for any $\\mathbf{X},\\mathbf{Y}$ then the RHS of Eq. 16 will always be smaller than the LHS. ", "page_idx": 21}, {"type": "text", "text": "B Experimental details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "B.1 Architecture Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "All non-ResNet architectures are based on All-CNN-C [52] ", "page_idx": 21}, {"type": "text", "text": "B.2 Pre-trained models ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In Section 4.2 we used four pretrained models: 3 variations of [13] (\u2019google-vit-base-patch16-224\u2019, \u2019google-vit-large-patch16-224\u2019, \u2019WinKawaks-vit-small-patch16-224\u2019) and the model proposed by [42] ( \u2019facebook-dinov2-base\u2019). ", "page_idx": 21}, {"type": "table", "img_path": "mfvKEdJ4zW/tmp/bbb84318f8b31f2d3624d77aa81fbf46df04d9c123074020f0544792dd4f9a1a.jpg", "table_caption": ["Table 1 "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "B.3 Parameters and resources ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In all our experiments we used gpu rtx 3080ti and 3090. In order to compute the eigenvector and functional map on a graph of $3\\mathbf{k}$ nodes we employ not more than 2 minutes. ", "page_idx": 22}, {"type": "text", "text": "B.4 Mean Reciprocal Rank (MRR) ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Mean Reciprocal Rank (MRR) is a commonly used metric to evaluate the performance of retrieval systems [39]. It measures the effectiveness of a system by calculating the rank of the first relevant item in the search results for each query. ", "page_idx": 22}, {"type": "text", "text": "To compute MRR, we consider the following steps: ", "page_idx": 22}, {"type": "text", "text": "1. For each query, rank the list of retrieved items based on their relevance to the query. 2. Determine the rank position of the first relevant item in the list. If the first relevant item for query $i$ is found at rank position $r_{i}$ , then the reciprocal rank for that query is $\\textstyle{\\frac{1}{r_{i}}}$ . 3. Calculate the mean of the reciprocal ranks over all queries. If there are $Q$ queries, the MRR is given by: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathrm{MRR}=\\frac{1}{Q}\\sum_{i=1}^{Q}\\frac{1}{r_{i}}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Here, $r_{i}$ is the rank position of the first relevant item for the $i^{\\th}$ -th query. If a query has no relevant items in the retrieved list, its reciprocal rank is considered to be zero. ", "page_idx": 22}, {"type": "text", "text": "MRR provides a single metric that reflects the average performance of the retrieval system, with higher MRR values indicating better performance. ", "page_idx": 22}, {"type": "text", "text": "C Additional Results ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this section, we provide additional results and ablation studies that further support and expand upon the experiments presented in Section 4. ", "page_idx": 22}, {"type": "text", "text": "C.1 Functional maps structure ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In Section 3.2.1, we have shown that latent graphs can be constructed using different distance metrics. While angular distance was primarily used in our experiments, we now highlight its effectiveness and the resulting functional maps. ", "page_idx": 22}, {"type": "text", "text": "In Figure 7, we present a visualization of the structure of the functional map in a synthetic setting. The experiment was conducted as follows: given an input set $X$ consisting of test embeddings extracted from MNIST, we aimed to observe the degradation of the functional map structure as the space is perturbed. The perturbation involved an orthogonal transformation combined with additive Gaussian noise at increasing levels. In the first row, the functional maps were computed from k-nearest neighbor (knn) graphs using the angular distance metric, while in the second row, knn graphs were constructed using the L2 distance metric. Below each functional map, the LFM similarity score and MRR retrieval scores are displayed. ", "page_idx": 22}, {"type": "image", "img_path": "mfvKEdJ4zW/tmp/94f8268f1233c2656bf27c2fa68cac0a781c824c47f3313048172b6954c1ba06.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Figure 7: Functional maps structure at increasing level noise. Given a set of MNIST embeddings, we plot the degradation of the functional map structure as the space is perturbed. We compare the graph built with two different metrics (Angular, L2) and report MRR and LFM similarity score. ", "page_idx": 23}, {"type": "image", "img_path": "mfvKEdJ4zW/tmp/ab6e2911091633190e9253b417397429e9621eba9e08afaba9469759c1768f06.jpg", "img_caption": ["Figure 8: Descriptors ablation. We compare the latent functional maps computed on MNIST using different descriptors. For each map we report the MRR score. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "We observed that (i) when noise is absent (first column), the two spaces are isometric, and the functional map is diagonal, (ii) constructing the graph with the cosine distance metric is more robust to increasing noise, and (iii) the LMF similarity score correlates with the MRR retrieval metric, indicating that more structured functional maps reflect better alignment between spaces. ", "page_idx": 23}, {"type": "text", "text": "C.2 Ablation on the choice of descriptors ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In Figure 8, we performed an ablation study on the choice of descriptors, comparing supervised descriptors (geodesic distance and cosine distance using 10 correspondences), weakly supervised descriptors (label descriptor), and fully unsupervised descriptors (heat kernel signature [53] and wave kernel signature [53]). ", "page_idx": 23}, {"type": "text", "text": "To conduct this study, we performed a retrieval task on the test embeddings of two convolutional autoencoders trained on MNIST, which differed by their parameter initialization. We visualized the structure of the functional map and reported the performance in terms of mean reciprocal rank (MRR), observing the following: (i) Geodesic and cosine descriptors performed best (ii) The geodesic distance (shortest path) is a good choice as it is agnostic of the metric chosen to build the initial graph, yet provides the same result as using the metric itself; (iii) The structure of the functional map reflects the performance of retrieval. ", "page_idx": 23}, {"type": "text", "text": "C.3 Retrieval ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In Table 2, we report the numerical results for the experiment in Figure 6 adding more transformations from the method of [32]: orthogonal (Ortho), linear (Linear) and affine (Affine). From the value in the table, we can see that all the methods that involve the latent functional map (LFM) saturate at 5 anchors, reaching top performance. In Figure 9, we show how the performance of the latent functional map methods depends on the number of eigenvectors used to compute the map. In particular, we notice that the performance drastically increases at 25 eigenvectors, reaching the same score when using the functional map computed from the ground truth correspondences (LFMGT). ", "page_idx": 23}, {"type": "image", "img_path": "mfvKEdJ4zW/tmp/e99a151bc85df7a489250a1341a3bb799a63a52e6e1723ae8a2491dca64f72c7.jpg", "img_caption": ["Figure 9: Retrieval of word embeddings for increasing number of eigenvectors. We report the MMR score for the methods in Figure 6 with fixed number of anchors (5) and increasing number of eigenvectors. "], "img_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "mfvKEdJ4zW/tmp/39aac80f47cd9854917cb6a61d5a5f6b85f1922a46f0b2434ca4703479e69c93.jpg", "table_caption": ["Table 2: MRR Score for the retrieval of word embeddings. Table 3: MRR Score for the retrieval We report the value of the results depicted in Figure 6 adding of CUB. We report the results on the more kind transformation between spaces (Orthogonal, Lin- CUB including the additional baseline ear and Affine). Procustes [29]. "], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "mfvKEdJ4zW/tmp/79da59650806d1f54305d632d9a9706685839ab229f5917b1ddddac6750f3b72.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "C.3.1 Extended comparison ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In Table 3 we added a comparison with multiple baselines: Relative [39], Ortho [32], Linear[28], Procrustes [29]. With the same foundation models and settings used in Section 4.3, we perform a retrieval task on the Caltech-UCSD Birds-200-2011 (CUB) dataset. We demonstrate that LFM is consistently superior in performance and can be used on top of any method which computes an explicit mapping between spaces. ", "page_idx": 24}, {"type": "text", "text": "C.4 Additional qualitative results ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In Figure 10 we show additional qualitative results on theMNIST, FMNIST, Cifar10 datasets, akin to the experiment in Figure 5 in the main manuscript. ", "page_idx": 24}, {"type": "image", "img_path": "mfvKEdJ4zW/tmp/a49a716cc13a2b71e6209eeeaaee5f3e5afe7c3f9f5dca2591d7a39d142ee7f4.jpg", "img_caption": ["Figure 10: Additional qualitative results on stitching. "], "img_footnote": [], "page_idx": 25}]