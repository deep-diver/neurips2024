[{"type": "text", "text": "Privacy without Noisy Gradients: Slicing Mechanism for Generative Model Training ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Kristjan Greenewald Yuancheng Yu MIT-IBM Watson AI Lab, IBM Research UIUC kristjan.h.greenewald@ibm.com yyu51@illinois.edu ", "page_idx": 0}, {"type": "text", "text": "Hao Wang Kai Xu MIT-IBM Watson AI Lab, IBM Research MIT-IBM Watson AI Lab, IBM Research hao@ibm.com xuk@ibm.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Training generative models with differential privacy (DP) typically involves injecting noise into gradient updates or adapting the discriminator\u2019s training procedure. As a result, such approaches often struggle with hyper-parameter tuning and convergence. We consider the slicing privacy mechanism that injects noise into random low-dimensional projections of the private data, and provide strong privacy guarantees for it. These noisy projections are used for training generative models. To enable optimizing generative models using this DP approach, we introduce the smoothed-sliced $f$ -divergence and show it enjoys statistical consistency. Moreover, we present a kernel-based estimator for this divergence, circumventing the need for adversarial training. Extensive numerical experiments demonstrate that our approach can generate synthetic data of higher quality compared with baselines. Beyond performance improvement, our method, by sidestepping the need for noisy gradients, offers data scientists the flexibility to adjust generator architecture and hyper-parameters, run the optimization over any number of epochs, and even restart the optimization process\u2014all without incurring additional privacy costs. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Just as oil fueled the industrial revolution, data now propels innovation and progress in today\u2019s digital life. However, data sharing faces challenges due to privacy risks and regulatory policies (e.g., GDPR, CCPA, FTC). Synthetic data offers a promising solution as it closely resembles real data, retains its formats and essential properties, and can be seamlessly integrated into existing workflows. Nonetheless, modern generative models are vulnerable to privacy attacks that could expose individuals\u2019 sensitive information in the original data [HMDDC17, CYZF20, SOT22]. Today, differential privacy (DP) [DR14] stands as the de facto standard for privacy protection and it plays a crucial role in guiding the design of synthetic data. For example, DP was used in the release of microdata from Israel\u2019s National Registry of live births in 2014 to protect the privacy of mothers and newborns [HC24], as well as in the release of global victim-perpetrator data to ensure the privacy of victims [fM22]. Additionally, a recent publication $[\\mathrm{NDL}^{+}2\\bar{3}]$ by the National Institute of Standards and Technology (NIST) recommends the use of DP algorithms in generating synthetic data to provide robust privacy protection against rapid developments in privacy attacks. ", "page_idx": 0}, {"type": "text", "text": "Existing approaches to training generative models often integrate DP by injecting noise into gradient updates or adapting the discriminator\u2019s training procedure $\\bar{[\\mathrm{XLW}^{+}18}$ , TKP19, $\\mathrm{\\bar{C}B V}^{+}21$ , TTCY21, $\\mathrm{BJWW^{+}19}$ , JYVDS19, $\\mathrm{TWB}^{+}19$ , BKZ23, NPA23, $\\mathrm{LJW}^{+}20$ , ${\\mathrm{XGJ}}^{+}22$ , COF20]. They provide several benefits, such as the ability to generate diverse data types (including continuous data, timeseries, and images), scalability to high-dimensional data, and accelerated runtime using GPUs. However, fine-tuning hyper-parameters within these frameworks can be challenging [NWD20, PS22, LT19]. Additionally, they encounter a dilemma when determining the number of training epochs: with a fixed privacy budget, increasing the number of epochs requires injecting more noise into gradient update per iteration, while fewer epochs might not be sufficient for the optimizer to converge. In such a setting, if the training does not converge, the only recourse may be to increase the privacy budget. This motivates a fundamental question: ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "How can we train generative models with DP guarantees while ensuring easy fine-tuning, stable convergence, and high utility? ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this paper, we introduce a new learning paradigm for training privacy-preserving generative models. Our approach decouples the training process into two steps: (i) computing noisy low-dimensional projections of the private data along random directions, and (ii) updating the generative models to fit these noisy projections. We establish DP guarantees for the first step and leverage the random projection to further tighten our privacy bound. This decoupling strategy offers several advantages. The post-processing property of DP ensures that any deep learning techniques can be applied in the second step. In other words, our approach is model-agnostic, allowing for smooth integration into existing training pipelines of generative models. With our method, data scientists have the flexibility to adjust generator architecture and hyper-parameters, optimize the generative model for any number of epochs, and even restart the optimization\u2014all without worrying about additional privacy costs. ", "page_idx": 1}, {"type": "text", "text": "Based on this paradigm, we are motivated to introduce a new information-theoretic measure: the smoothed-sliced $f$ -divergence. This divergence (randomly) projects the original and synthetic data distributions onto lower-dimensional spaces, followed by smoothing with isotropic Gaussian noise, and averaging their $f$ -divergence over all projections. We prove that using this divergence as the loss function in generative model training is equivalent to the aforementioned two-step training process. Additionally, we present a kernel-based, differentiable estimator for this divergence. It circumvents the need for adversarial training in generative models, thereby enhancing convergence stability and robustness to different choices of hyper-parameters. Finally, we establish the statistical consistency of training generative models using this divergence. ", "page_idx": 1}, {"type": "text", "text": "In terms of the slicing mechanism, we build upon the work of [RL21]. They introduced the smoothedsliced Wasserstein distance and applied it to generative models and domain adaptation tasks. However, their approach is limited to 1-dimensional projection spaces $\\left.k=1\\right.$ ), exploiting the closed-form expression of Wasserstein distances in 1D. Moreover, their privacy analysis contains a significant flaw in its derivation (see Remark 1 for detailed discussions). In contrast, we propose a generic framework for training privacy-preserving generative models, applicable to any $k$ -dimensional projections. Empirically, we observe that setting the projection dimension to a small number (e.g., $k\\,=\\,2,3)$ ) significantly improves the quality of synthetic data compared with $k=1$ under the same privacy budget. Additionally, our kernel-based estimator eliminates the need for adversarial training in generative models, facilitating stable convergence. Finally, we present a completely new proof for establishing the DP guarantees of our framework. This analysis also applies to any smoothsliced divergence objective, for instance, we provide corrected (and improved!) DP bounds for the smoothed-sliced Wasserstein framework of [RL21]. ", "page_idx": 1}, {"type": "text", "text": "We demonstrate the effectiveness of our method through numerical experiments. We compare generative models trained using our method with those trained by standard privacy mechanisms (DP-SGD, PATE, or smoothed-sliced Wasserstein distance) among various real-world datasets. The results indicate that our approach consistently produces synthetic data of higher quality compared with baseline methods. ", "page_idx": 1}, {"type": "text", "text": "In summary, our main contributions are: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We introduce a new framework for training privacy-preserving generative models. It offers easy hyper-parameter tuning and allows for optimizing generative models over any number of training epochs without extra privacy costs.   \n\u2022 We propose a new information-theoretic divergence and provide a kernel-based estimator. This estimator enables the training of generative models without relying on adversarial training, enhancing convergence stability. ", "page_idx": 1}, {"type": "text", "text": "\u2022 For the slicing mechanism, we extend the work of [RL21] and allow for projecting data onto any $k$ -dimensional space. Using an entirely new proof technique, we provide DP guarantees, which in the $k=1$ setting correct (and strengthen!) the privacy analysis in [RL21]. \u2022 We validate our method through numerical experiments. The results show that our method produces synthetic data of higher quality compared with baselines. ", "page_idx": 2}, {"type": "text", "text": "Additional Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Recent work has proposed alternative approaches for training DP generative models without adversarial networks. For example, $[\\mathrm{CBV}^{+}21]$ considered using the Sinkhorn divergence as the loss function, but their method adds noise to gradient updates to ensure DP, which leads to the challenges discussed in the introduction. Another line of work [HAP21, ${\\mathrm{VCH}}^{+}22$ , HJSP23, $\\mathrm{YAS}^{+}23]$ used the maximum mean discrepancy (MMD) to train DP generative models. They inject noise into the embedding of the private data distribution to maintain DP. However, minimizing their loss function to zero does not guarantee perfect matching between the synthetic and real data distributions, due to either not using a characteristic kernel or the approximation errors stemming from using finite-dimensional feature mappings to approximate the kernel function. In contrast, Proposition 1 proves that our smoothed-sliced $f$ -divergence equals zero iff the synthetic and real distributions are identical; Corollary 1 establishes the statistical consistency of training generative models using our loss function. Additionally, we amplify our privacy bound by leveraging random projections in Theorem 1. ", "page_idx": 2}, {"type": "text", "text": "There is significant research introducing DP mechanisms tailored to generating tabular synthetic data [e.g., $Z_{\\mathrm{CP}}^{+}17$ , MSM19, MMS21, MMSM22, $\\mathrm{GAH^{+}14}$ , $\\mathrm{VTB}^{+}20$ , $\\mathrm{ABK}^{+}\\bar{2}1$ , $Z\\mathrm{WL}^{+}21$ , LVW21, ${\\mathrm{V}}\\mathbf{A}\\mathbf{A}^{+}22$ , DAHY24]. They select a set of workload queries (e.g., low-order marginal queries) and generates synthetic data to minimize approximation errors on these queries. These methods often maintain statistical properties of the original data with high accuracy, particularly for the selected workload queries; downstream predictive models trained on such synthetic data often achieve high performance when deployed on real data $[\\mathrm{TMH}^{+}21$ , $\\mathrm{WSH}^{+}23]$ . However, they only apply to categorical features1, rely on special generative model architectures, or struggle to scale effectively to high-dimensional data. More broadly, there are several works analyzing DP synthetic data from a theoretical perspective or investigating other properties of synthetic data (e.g., missing values) [NTZ13, UV20, GMHI20, CLWX21, BSV22b, BSV22a, RTMT21, MZKH23]. ", "page_idx": 2}, {"type": "text", "text": "A burgeoning line of work has explored slicing and smoothing to improve sample complexity in estimating divergence and optimal transport measures. These measures often suffer from extreme curses of dimensionality (e.g., $n^{-1/d}$ for Wasserstein distance [Dud69]). Previous studies have shown that both slicing [RPDB11, $\\mathrm{VFT}^{+}19$ , $L Z C^{+}21$ , $\\mathrm{NDC}^{+}20$ , GG21] and smoothing [GGNWP20, GG20] facilitate convergence at the parametric rate for both $f$ -divergences and optimal transport distances, while preserving key properties of the original divergence (e.g., being zero iff the two distributions are identical). These modified divergences are also used as objective functions for training generative models. In our context, the use of smoothed-sliced divergence is less motivated by sample complexity (indeed using both smoothing and slicing would be unnecessary for achieving the parametric rate). Instead, it is motivated by the slicing privacy mechanism. Also, while we enjoy the sample complexity beneftis of slicing, we do not benefti from the sample complexity improvements of smoothing [GGK20]. This is because achieving DP requires injecting a finite number (typically just one) of noise realizations per data point. Full smoothing results are still useful, however, for our consistency results in the asymptotic regime. ", "page_idx": 2}, {"type": "text", "text": "While the 1-dimensional sliced Wasserstein distance can be computed via a simple sorting algorithm [DZS18], there is in general no closed-form sample-based estimator for $f$ -divergence, even in one dimension. While nearest neighbor [WKV09] and kernel-based [MSGHI18] estimators do exist, they often suffer from scalability issues and are not friendly to gradient-based optimization. Hence, it is a common practice to use dual forms of $f$ -divergence in deriving adversarial-based training procedures for generative models [NCT16]. In the present work, we avoid this costly adversarial training using our moment matching estimator. ", "page_idx": 2}, {"type": "text", "text": "The supplementary material of this paper includes: (i) omitted proofs of all theoretical results and (ii) supporting experimental results. ", "page_idx": 2}, {"type": "text", "text": "2 Preliminaries and Problem Setup ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we review the concepts of differential privacy (DP), $f$ -divergence, and a moment matching method used for estimating $f$ -divergence. ", "page_idx": 3}, {"type": "text", "text": "2.1 Differential Privacy ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We denote the original dataset as a matrix $\\mathbf{X}\\in\\mathbb{R}^{n\\times d}$ , where $n$ denotes the number of records and $d$ represents the number of real-valued features per record. ", "page_idx": 3}, {"type": "text", "text": "Definition 1 (Dataset adjacency). Two datasets $\\mathbf{X}$ and $\\mathbf{X^{\\prime}}$ are considered adjacent if they differ in a single row, say the $i$ -th row, such that $\\|\\mathbf{X}_{i,:}-\\mathbf{X}_{i,:}^{\\prime}\\|_{2}\\leq1$ where $\\mathbf{X}_{i},$ : and $\\mathbf{X}_{i,}^{\\bar{\\prime}}$ : are the $i$ -th row of $\\mathbf{X}$ and $\\mathbf{X^{\\prime}}$ , respectively. ", "page_idx": 3}, {"type": "text", "text": "Next, we recall the definition of differential privacy (DP) [DR14]. ", "page_idx": 3}, {"type": "text", "text": "Definition 2 ( $[\\epsilon,\\delta)$ differential privacy). A randomized mechanism $\\mathcal{M}\\,:\\,\\mathbb{R}^{n\\times d}\\,\\rightarrow\\,\\mathbb{O}$ satisfies $(\\epsilon,\\delta)$ -differential privacy if for any adjacent datasets $\\mathbf{X}$ , $\\mathbf{X^{\\prime}}$ and all possible outcomes $\\mathbb{S}\\subseteq\\mathbb{O}$ , we have: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}(\\mathcal{M}(\\mathbf{X})\\in\\mathbb{S})\\leq\\exp(\\epsilon)\\operatorname*{Pr}(\\mathcal{M}(\\mathbf{X}^{\\prime})\\in\\mathbb{S})+\\delta.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "DP has many compelling properties. The post-processing property states that if a mechanism $\\mathcal{M}$ is $(\\epsilon,\\delta)$ -DP, its outcome remains $(\\epsilon,\\delta)$ -DP even after applying a (potentially randomized) function; the basic composition rule states that given a sequence of mechanisms $\\mathcal{M}_{1},\\ldots\\mathcal{M}_{k}$ , if $\\mathcal{M}_{i}$ is $(\\epsilon_{i},\\delta_{i})$ -DP, then their composition $\\mathcal{M}(\\mathcal{D})=\\left(\\mathcal{M}_{1}(\\mathcal{D}),\\ldots,\\mathcal{M}_{k}(\\mathcal{D})\\right)$ will satisfy $(\\textstyle\\sum_{i=1}^{k}\\epsilon_{i},\\textstyle\\sum_{i=1}^{k}\\delta_{i})$ -DP. ", "page_idx": 3}, {"type": "text", "text": "2.2 f-divergence and Moment Matching ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We first recall the definition of $f$ -divergence [Chapter 7 in PW23]. ", "page_idx": 3}, {"type": "text", "text": "Definition 3. Let $f:(0,\\infty)\\rightarrow\\mathbb{R}$ be a convex function with $f(1)=0$ and $f(0)\\triangleq f(0+)$ . Let $P_{\\mathrm{{X}}}$ and $Q_{\\mathrm{X}}$ be two probability distributions on $\\mathcal{X}$ . If $P\\ll Q$ , then the $f$ -divergence is defined as $\\begin{array}{r}{\\operatorname{D}_{f}(P\\|Q)\\triangleq\\operatorname{\\mathbb{E}}_{Q}\\left[f\\left({\\frac{\\mathrm{d}P}{\\mathrm{d}Q}}\\right)\\right]}\\end{array}$ where $\\frac{\\mathrm{d}P}{\\mathrm{d}Q}$ is a Radon-Nikodym derivative. Additionally, we denote the density ratio by $\\textstyle r(x)\\triangleq{\\frac{\\mathrm{d}P}{\\mathrm{d}Q}}(x)$ . ", "page_idx": 3}, {"type": "text", "text": "$f$ -divergences have many nice properties. For example, it is always non-negative; and assuming $f$ is strictly convex at 1, then $\\mathsf{D}_{f}(\\bar{P||Q\\rangle}=0$ if and only $P=Q$ . ", "page_idx": 3}, {"type": "text", "text": "There is a burgeoning field of research focusing on $f$ -divergence estimation [see e.g., GGNWP20, WY20, SXGS20]. Here we revisit a framework based on kernel mean matching, a special instance of moment matching methods [Chapter 3 in SSK12]. Given a reproducing kernel $\\mathsf{K}(\\bar{\\mathbf{x}},\\pmb{x}^{\\prime})$ , one can solve the following optimization problem to estimate the density-ratio function: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{r\\in\\mathcal{R}}\\left\\|\\int\\mathsf{K}(\\pmb{x},\\cdot)\\mathsf{d}P(\\pmb{x})-\\int\\mathsf{K}(\\pmb{x},\\cdot)r(\\pmb{x})\\mathsf{d}Q(\\pmb{x})\\right\\|_{\\mathcal{R}}^{2},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\Vert\\cdot\\Vert_{\\mathcal{R}}$ denotes the norm of a reproducing kernel Hilbert space $\\mathcal{R}$ . One example of reproducing kernels is the Gaussian kernel $\\begin{array}{r}{{\\sf K}({\\pmb x},{\\pmb x}^{\\prime})\\,=\\,\\exp\\left(\\frac{-\\|{\\pmb x}-{\\pmb x}^{\\prime}\\|_{2}^{2}}{2\\sigma_{g}^{2}}\\right)}\\end{array}$ \u2212\u2225x2\u2212\u03c3g2x\u2032\u222522 . Given {xip }in=p1 drawn from P and $\\{\\pmb{x}_{j}^{q}\\}_{j=1}^{n_{q}}$ drawn from $Q$ , we can optimize an empirical version of the above optimization to obtain an analytical solution: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{\\pmb r}_{q}=\\frac{n_{q}}{n_{p}}\\mathbf K_{q,q}^{-1}\\mathbf K_{q,p}\\mathbf1_{n_{p}},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\hat{r}_{q}\\in\\mathbb{R}^{n_{q}}$ is the (empirically) optimal density ratio values at samples drawn from $Q,\\mathbf{K}_{q,q}\\in$ $\\mathbb{R}^{n_{q}\\times n_{q}}$ and $\\mathbf{K}_{q,p}\\in\\mathbb{R}^{n_{q}\\times n_{p}}$ are the kernel Gram matrices: ", "page_idx": 3}, {"type": "equation", "text": "$$\n[\\hat{\\mathbf{r}}_{q}]_{j}=\\hat{r}\\left(\\pmb{x}_{j}^{q}\\right),\\quad[\\mathbf{K}_{q,q}]_{j,j^{\\prime}}=\\mathsf{K}\\left(\\pmb{x}_{j}^{q},\\pmb{x}_{j^{\\prime}}^{q}\\right),\\quad[\\mathbf{K}_{q,p}]_{j,i}=\\mathsf{K}\\left(\\pmb{x}_{j}^{q},\\pmb{x}_{i}^{p}\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We extend the definition of $f$ to a vector by applying it to each element of the vector. Then we have ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{D}}_{f}(P\\Vert Q)=\\frac{1}{n_{q}}\\mathbf{1}_{n_{q}}^{T}f(\\hat{\\mathbf{r}}_{q})=\\frac{1}{n_{q}}\\mathbf{1}_{n_{q}}^{T}f\\left(\\frac{n_{q}}{n_{p}}\\mathbf{K}_{q,q}^{-1}\\mathbf{K}_{q,p}\\mathbf{1}_{n_{p}}\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "3 Main Results ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We introduce a new information-theoretic measure\u2014the smoothed-sliced $f$ -divergence. We show that this divergence can measure the difference between distributions, with only access to noised $k$ -dimensional slices of the distributions. This finding motivates a new DP mechanism: the $k$ -slicing privacy mechanism. Finally, using the non-adversarial estimator (1), we apply the smoothed-sliced $f$ -divergence as a new loss function to train privacy-preserving generative models. ", "page_idx": 4}, {"type": "text", "text": "3.1 Smoothed-sliced f-divergence ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We start with giving a formal definition of the smoothed-sliced $f$ -divergence. ", "page_idx": 4}, {"type": "text", "text": "Definition 4. Denote the Stiefel manifold of $d\\!\\times\\!k$ matrices with orthonormal columns by $\\mathbb{S}_{k}(\\mathbb{R}^{d})$ . Let $\\Theta\\sim\\mathrm{Unif}\\big(\\mathbb{S}_{k}(\\mathbb{R}^{d})\\big)$ and $\\mathbf{N}\\sim\\mathcal{N}(\\mathbf{0},\\sigma^{2}\\mathbf{I}_{k})$ . The smoothed-sliced $f$ -divergence between distributions $P_{\\mathrm{{X}}}$ and $Q_{\\mathrm{X}}$ on $\\mathbb{R}^{d}$ is defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{SD}_{f,k,\\sigma^{2}}(P_{\\mathrm{X}}\\|Q_{\\mathrm{X}})\\triangleq\\mathsf{D}_{f}(P_{\\Theta^{T}\\mathrm{X}+\\mathrm{N}|\\Theta}\\|Q_{\\Theta^{T}\\mathrm{X}+\\mathrm{N}|\\Theta}|P_{\\Theta})}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\frac{1}{\\mathrm{vol}(\\mathbb{S}_{\\mathbf{k}}(\\mathbb{R}^{d}))}\\int_{\\theta\\in\\mathbb{S}_{k}(\\mathbb{R}^{d})}\\mathsf{D}_{f}(P_{\\theta^{T}\\mathrm{X}+\\mathrm{N}}\\|Q_{\\theta^{T}\\mathrm{X}+\\mathrm{N}})\\mathrm{d}\\theta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Next, we discuss some basic properties of this new divergence. ", "page_idx": 4}, {"type": "text", "text": "Proposition 1. The smoothed-sliced $f$ -divergence is non-negative: $\\operatorname{SD}_{f,k,\\sigma^{2}}(P_{\\mathrm{X}}\\|Q_{\\mathrm{X}})\\geq0$ for any $k\\geq1$ and $\\sigma\\geq0$ . If $f$ is strictly convex at 1 and $P_{\\mathrm{X}},Q_{\\mathrm{X}}$ have moment generating functions, then $\\operatorname{SD}_{f,k,\\sigma^{2}}(P_{\\mathrm{X}}\\|Q_{\\mathrm{X}})=0$ if and only if $P_{\\mathrm{X}}=Q_{\\mathrm{X}}$ . ", "page_idx": 4}, {"type": "text", "text": "Given a set of real data $\\{{\\pmb x}_{i}\\}_{i=1}^{n}$ (i.e., rows of $\\mathbf{X}$ ) and synthetic data $\\{{\\pmb x}_{i}^{\\mathrm{syn}}\\}_{i=1}^{n_{\\mathrm{syn}}}$ , let $\\hat{\\mathbf{D}}_{f}$ denote any estimator of -dimensional $f$ -divergence (e.g., (1)). We draw random directions $\\pmb{\\theta}_{s}$ and additive noise $\\pmb{v}_{s,i}$ and $\\bar{\\pmb{v}}_{s,i}$ . Then we can estimate the smoothed-sliced $f$ -divergence by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\widehat{\\mathrm{SD}}_{f,k,\\sigma^{2}}(P_{\\mathrm{X}^{\\mathrm{sp}}}\\|P_{\\mathrm{X}})=\\frac{1}{m}\\sum_{s=1}^{m}\\hat{\\mathrm{D}}_{f}\\left(\\{x_{i}^{\\mathrm{sp}}\\pmb\\theta_{s}+\\bar{v}_{s,i}\\}_{i=1}^{n_{\\mathrm{sp}}}\\|\\{\\pmb x_{i}\\pmb\\theta_{s}+\\pmb v_{s,i}\\}_{i=1}^{n}\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "As $\\widehat{\\mathrm{SD}}_{f,k,\\sigma^{2}}$ is an empirical average of $m$ estimates of $k$ -dimensional divergences, this estimator will inherit any statistical convergence bounds that apply to the chosen $\\hat{\\mathbf{D}}_{f}$ . ", "page_idx": 4}, {"type": "text", "text": "3.2 Slicing Privacy Mechanism ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The loss function in (2) accesses the original data solely through their noisy projections along random directions. This observation motivates the following $k$ -slicing privacy mechanism. ", "page_idx": 4}, {"type": "text", "text": "Definition 5. Let $k$ denote the dimension of the random projections, and $m$ denote the number of them, yielding $m^{\\prime}=m k$ . Let $\\mathbf{X}\\in\\mathbb{R}^{n\\times d}$ represent the original dataset where we assume $\\|\\mathbf{X}_{i,:}\\|_{2}\\leq1$ for all $i$ . Let $\\mathbf{U}\\in\\mathbb{R}^{d\\times m^{\\prime}}$ and $\\mathbf{V}\\in\\mathbb{R}^{n\\times m^{\\prime}}$ be random slicing and noise matrices with each element independently drawn from $\\mathrm{U}_{i,j}\\sim\\mathcal{N}(0,d^{-1})$ and $\\mathbf{V}_{i,j}\\sim\\mathcal{N}(0,\\sigma^{2})$ , respectively. The slicing privacy mechanism is defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{M}(\\mathbf{X})\\triangleq(\\mathbf{U},\\mathbf{X}\\mathbf{U}+\\mathbf{V}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Remark 1. Our mechanism outputs the random slicing matrix $\\mathbf{U}$ , as it will be used to project the synthetic data onto the same spaces during the training of generative models. In contrast, [RL21] did not include $\\mathbf{U}$ in their privacy mechanism, leading them to give an incorrect derivation of the privacy guarantee. Similarly, our approach differs from using the Johnson-Lindenstrauss transform to preserve DP [BBDS12, EKKL20], as these methods do not reveal the random matrix. ", "page_idx": 4}, {"type": "text", "text": "In Definition 5, we draw random projections from a Gaussian distribution instead of uniformly from the Stiefel manifold. This choice simplifies the sampling procedure and streamlines our privacy analysis by making the the mechanism output jointly Gaussian with zero mean, conditioned on the data. Next, we establish the DP guarantees for our noisy slicing mechanism. ", "page_idx": 4}, {"type": "text", "text": "Theorem 1. Assume $\\gamma\\;\\triangleq\\;\\sigma^{-2}(\\alpha^{2}\\,-\\,\\alpha)\\;<\\;d.$ . For all $\\delta\\ \\in\\ (0,1)$ and $\\alpha\\,>\\,1$ , the mechanism $\\mathcal{M}(\\mathbf{X})=(\\mathbf{U},\\mathbf{X}\\mathbf{U}+\\mathbf{V})$ in Definition $^{5}$ satisfies ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\left(\\frac{m^{\\prime}\\alpha}{2\\sigma^{2}(d-\\gamma)}+\\frac{\\ln(1/\\delta)}{\\alpha-1},\\delta\\right)-D P.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $(\\epsilon,\\delta)$ -DP is as defined in Definition 2 with dataset adjacency specified in Definition 1. ", "page_idx": 5}, {"type": "text", "text": "Our privacy bound relies on $k$ (the dimension of projection spaces) and $m$ (the number of slices) only through their product $m^{\\prime}=m k$ , which reveals a trade-off: with a fixed privacy budget, increasing $k$ aids generative models in capturing higher-order information of the original data, while increasing $m$ improves the accuracy of learning lower-order information. This trade-off resembles marginal-based mechanisms, where adjusting $k$ to enable generative models to learn the $k$ -way marginal queries [Definition 1 in LVW21] involves a similar effect. However, we remark that even one-dimensional slices suffice to fully characterize a distribution since $\\mathrm{SD}_{f,1,\\sigma^{2}}(P_{X}\\|P_{\\mathrm{X^{syn}}})=0$ implies $P_{\\mathrm{X}}=P_{\\mathrm{X^{syn}}}$ by Proposition 1. In contrast, matching all $k$ -way marginal distributions for $k<d$ does not necessarily ensure $P_{\\mathrm{X}}=P_{\\mathrm{X^{syn}}}$ . ", "page_idx": 5}, {"type": "text", "text": "We prove in Appendix Proposition 3 that the slicing privacy mechanism is $\\begin{array}{r}{\\left(\\frac{m\\alpha}{2\\sigma^{2}}+\\frac{\\ln(1/\\delta)}{\\alpha-1},\\delta\\right)}\\end{array}$ -DP if the projection matrix $\\mathbf{U}$ is deterministic. Comparing it with Theorem 1, we observe that by randomly selecting the projection matrix, we can achieve a tighter privacy bound by reducing a factor of $\\!\\,{\\frac{k}{d-\\gamma}}$ (for the first-term), even if U is disclosed by the privacy mechanism. The rationale behind this lies in the fact that a deterministic projection matrix allows the model designer/adversary to target specific individual records through carefully designed projection directions. ", "page_idx": 5}, {"type": "text", "text": "Remark 2 (Choosing $\\alpha$ ). The bound in Theorem 1 can be optimized in terms of $\\alpha$ for a desired fixed $\\delta$ and this optimization can be done numerically. Alternatively, there is an ad hoc strategy for \u2018approximately optimizing\u2019 $\\alpha$ . Let us assume $\\gamma\\leq d/2$ , i.e. $\\alpha^{2}-\\alpha\\stackrel{.}{\\leq}d\\sigma^{2}/2$ . Then ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\epsilon\\leq\\frac{m^{\\prime}\\alpha}{\\sigma^{2}d}+\\frac{\\ln(1/\\delta)}{\\alpha-1}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Minimizing the right hand side expression w.r.t. $\\alpha>1$ yields an optimal ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\alpha^{*}=1+\\sqrt{\\frac{\\sigma^{2}d\\ln(1/\\delta)}{m^{\\prime}}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Substituting it into the (true) expression for $\\epsilon$ yields ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\epsilon^{*}=\\frac{m^{\\prime}}{2\\sigma^{2}(d-\\gamma^{*})}+\\left(1+\\sqrt{\\frac{d}{2(d-\\gamma)}}\\right)\\sqrt{\\frac{m^{\\prime}\\ln(1/\\delta)}{\\sigma^{2}d}}\\leq\\frac{m^{\\prime}}{\\sigma^{2}d}+2\\sqrt{\\frac{m^{\\prime}\\ln(1/\\delta)}{\\sigma^{2}d}},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\gamma^{*}=\\sigma^{-2}(\\alpha^{2}-\\alpha)$ . ", "page_idx": 5}, {"type": "text", "text": "3.3 Training Generative Models ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We outline our approach for training privacy-preserving generative models using the smoothed-sliced $f$ -divergence (see Algorithm 1 for more details). First, we transform the output of the slicing privacy mechanism $\\mathcal{M}(\\mathbf{X})\\overset{\\cdot}{=}(\\mathbf{U},\\mathbf{X}\\mathbf{U}+\\mathbf{V})$ into $\\mathcal{O}=\\{(\\pmb{\\theta}_{s},\\pmb{o}_{i,s})\\}_{s\\in[m],i\\in[n]}$ . Here $\\pmb{\\theta}_{s}=\\mathbf{U}_{:,(s-1)k+1:s k}\\stackrel{.}{\\in}$ $\\mathbb{R}^{d\\times k}$ is the random slicing directions and $o_{i,s}\\,=\\,(\\mathbf{XU}+\\mathbf{V})_{i,(s-1)k+1:s k}\\,\\in\\,\\mathbb{R}^{k}$ is the (noisy) projection of the $i$ -th data point onto the $s$ -th slice. ", "page_idx": 5}, {"type": "text", "text": "Let $G_{\\omega}$ be a generative model with trainable parameters $\\omega\\in\\Omega$ . At each iteration, we sample a mini-batch from $\\scriptscriptstyle\\mathcal{O}$ as $\\{(\\pmb{\\theta}_{s},\\pmb{o}_{i,s})\\}_{s\\in[m],i\\in[b]}$ , where $b$ is the batch size. ", "page_idx": 5}, {"type": "text", "text": "To produce synthetic data, we draw random noise $z_{j}$ for $j\\in[b]$ and feed them into the generative model ${\\pmb x}_{j}^{\\mathrm{syn}}=G_{\\omega}({\\pmb z}_{j})$ . Additionally, we draw random noise $\\bar{\\pmb v}_{j,s}\\sim\\mathcal{N}(\\mathbf{0},\\sigma^{2}\\mathbf{I}_{k})$ for $j\\,\\in\\,[b]$ and $s\\,\\in\\,[m]$ that will be added to the projected synthetic data.2 Given these samples, we train the generative model $G_{\\omega}$ to generate samples close to the real data, using an estimator for the sliced $f$ -divergence to measure the discrepancy between the noisy-sliced real data $\\sigma_{i,s}$ and the noisy-sliced synthetic data xsjy $\\pmb{x}_{j}^{\\mathrm{syn}}\\pmb{\\theta}_{s}+\\bar{\\pmb{v}}_{j,s}$ . The smoothed slice $f$ -divergence (2) yields the following loss function: ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "equation", "text": "$$\nL(\\omega)=\\frac{1}{m}\\sum_{s=1}^{m}\\hat{\\mathrm{D}}_{f}\\left(\\{x_{j}^{\\mathrm{syn}}\\pmb\\theta_{s}+\\bar{v}_{s,j}\\}_{j=1}^{b}\\|\\{\\pmb\\omega_{i,s}\\}_{i=1}^{b}\\right).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The following corollary is a direct application of Proposition 1. ", "page_idx": 6}, {"type": "text", "text": "Corollary 1 (Consistency). Consider a dataset $\\mathbf{X}\\in\\mathbb{R}^{n\\times d}$ of n i.i.d. samples from a $d$ -dimensional distribution $P_{X}$ whose moment generating function exists, and the slicing privacy mechanism $\\mathcal{M}(\\mathbf{X})$ as in Definition 5. Suppose the noise level $\\sigma$ is fixed while $m\\rightarrow\\infty$ and $n\\to\\infty$ .3 Additionally, suppose that the chosen $f$ -divergence estimator $\\hat{\\mathrm{D}}_{f}$ is consistent and $f$ is strictly convex at $^{\\,I}$ . Then $\\begin{array}{r}{\\operatorname*{lim}_{n,m,b\\to\\infty}L(\\omega^{*})=0}\\end{array}$ for some $\\omega^{\\ast}\\in\\Omega$ if and only if $G_{\\omega^{*}}(Z)\\sim P_{X}$ . ", "page_idx": 6}, {"type": "text", "text": "Inserting our kernel-based estimator (1) yields for kernel K ", "page_idx": 6}, {"type": "equation", "text": "$$\nL(\\omega)=\\frac{1}{m b}\\sum_{s=1}^{m}\\mathbf{1}_{b}^{T}f\\left(\\left((\\mathbf{K}_{s}+\\tau\\mathbf{I}_{b})^{-1}\\mathbf{K}_{s,\\omega}\\mathbf{1}_{b}\\right)_{+}\\right),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where we compute the kernel Gram matrices $\\mathbf{K}_{s},\\mathbf{K}_{s,\\omega}\\in\\mathbb{R}^{b\\times b}$ along each slice: ", "page_idx": 6}, {"type": "equation", "text": "$$\n[\\mathbf{K}_{s}]_{i,i^{\\prime}}=\\mathsf{K}\\left(o_{i,s},o_{i^{\\prime},s}\\right),\\quad[\\mathbf{K}_{s,\\omega}]_{i,j}=\\mathsf{K}\\left(o_{i,s},x_{j}^{\\mathrm{syn}}\\theta_{s}+\\bar{v}_{j,s}\\right),\\quad\\mathrm{for}\\;s\\in[m].\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "To ensure the stability of computing matrix inversion, we include a $\\tau\\mathbf{I}_{b}$ where $\\tau\\,>\\,0$ is a small constant. Given that the domain of $f$ is $[0,\\infty)$ , we clip the density ratio estimator to make it non-negative. ", "page_idx": 6}, {"type": "text", "text": "Remark 3. Note that to compute the kernel Gram matrices $\\mathbf{K}_{s}$ and $\\mathbf{K}_{s,\\omega}$ , we need to apply the Gaussian kernel, which requires a constant $\\sigma_{g}$ . A heuristic of choosing this $\\sigma_{g}$ is to make it the median distance between all samples\u2014note that this will lead to different kernels along different slices. In practice, we use an ensemble of $\\sigma_{g}$ and average their density ratio estimators. ", "page_idx": 6}, {"type": "text", "text": "Algorithm 1 Training DP generative modes with the smoothed-sliced $f$ -divergence. ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Input: training data $\\mathbf{X}=\\{{\\pmb{x}}_{i}\\}_{i=1}^{n}$ ; slicing dimension $k$ ; number of slices $m$ ; batch size; max number of   \niterations $T$ ; learning rate $\\eta$ ; privacy budget $\\epsilon$ , $\\delta$ .   \nApply the noisy slicing mechanism $\\bar{\\mathcal{M}}(\\bar{\\bf X})$ with $(\\epsilon,\\delta)$ -DP, finding a noise variance $\\sigma_{(\\epsilon,\\delta)}^{2}$ and transforming   \nthe output into {(\u03b8\u03b8\u03b8s, oi,s)}s\u2208[m],i\u2208[n].   \nfor $t=1,\\cdot\\cdot\\cdot,T$ do Sample a mini-batch of data from $\\{(\\pmb{\\theta}_{s},\\pmb{O}_{i,s})\\}_{s\\in[m],i\\in[n]}$ , choosing a batch subset of the $i$ . Generate synthetic data $\\mathbf{X}_{\\mathrm{syn}}$ by feeding random noise into $G_{\\omega}$ . Slice and noise the synthetic data as $\\pmb{o}_{s}^{(\\mathrm{syn})}=\\pmb{\\theta}_{s}\\mathbf{X}_{\\mathrm{syn}}+\\sigma_{(\\epsilon,\\delta)}\\mathcal{N}(0,\\mathbf{I})$ , redrawing the noise each time   \n(no privacy cost). Compute the kernel Gram matrices (6) under Gaussian kernels in each slice. Compute the loss function $L(\\omega)$ in (5). Run stochastic-gradient optimization $\\omega\\gets\\omega-\\eta\\nabla_{\\omega}L(\\omega)$ .   \nend for   \nOutput: generative model $G_{\\omega}$ . ", "page_idx": 6}, {"type": "text", "text": "4 Numerical Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We validate our approach and compare it with baselines through numerical experiments. Additional experimental results and details of our setup are reported in Appendix C. ", "page_idx": 6}, {"type": "text", "text": "4.1 Synthetic Tabular Data ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Baselines. We compare our Algorithm 1 with four DP mechanisms: DP-SGD $[\\mathrm{XLW^{+}18}$ , $\\mathrm{RLP}^{+}20\\mathrm{J}$ , PATE [JYVDS19], MERF [HAP21] and SliceWass [RL21]. Like our algorithm, these baselines can ", "page_idx": 6}, {"type": "table", "img_path": "3mCr7ZNdSw/tmp/679f832e4dcf6cdd5c2976a26a96b08c45743b8af5573bbc6fe40b2d81edabdb.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 1: We compare synthetic tabular data generated by our Algorithm 1 with baselines, all under the same privacy budget $\\epsilon=5.1$ . We demonstrate them on the US Census data derived from the American Community Survey (ACS) with various evaluation metrics. These metrics range from 0 to 1, with higher scores indicating better performance. Since Employment has only one numerical column and CorrSim requires at least two numerical columns, we skip its values. The highest scores are highlighted in bold. If two methods have the same average score, only the one with lower standard deviation is highlighted. We remark that KSComp and CorrSim are less significant since they are designed for numerical columns. However, the benchmark datasets contain a limited number of numerical columns, with the majority being categorical (see Table 2). ", "page_idx": 7}, {"type": "text", "text": "handle both numerical and categorical data types without requiring data discretization and return a generative model that can produce any number of synthetic data. The implementations of the first two are adapted from an open-source Python library [Sma23], and the MERF implementation is from https://github.com/ParkLabML/DP-MERF. For SliceWass, we combine our slicing privacy mechanism with their main algorithm, which fixes the flaw in their privacy analysis and improves their privacy guarantees (and therefore quality results). Additionally, since their public Github repo does not include their synthetic data code, we implement their algorithm ourselves. ", "page_idx": 7}, {"type": "text", "text": "Data. We validate both our method and baselines using the US Census data derived from the American Community Survey (ACS) Public Use Microdata Sample (PUMS). Using the API of the Folktables package [DHMS21], we access the 2018 California data. Additionally, Folktables package provides five prediction tasks (Income , Coverage, Mobility, Employment, TravelTime) based on a target column and a set of mixed-type features. Details about these data, including the number of records and columns, are provided in Table 2 in the appendix. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Evaluation metrics. We follow the evaluation principles in $[\\mathrm{TMH}^{+}21]$ for assessing the quality of synthetic data and leverage the APIs from an open-source library in our implementation [Dat23]. ", "page_idx": 8}, {"type": "text", "text": "\u2022 KSComplement (numerical columns) and TVComplement (categorical columns). They measure the (average) similarity of one-way marginals (i.e., histograms of individual columns) between real and synthetic data.   \n\u2022 ContingencySimilarity. It measures the (average) similarity of pairs of categorical columns between real and synthetic data.   \n\u2022 CorrelationSimilarity. It measures the (average) correlations among numerical column pairs and computes the similarity between real and synthetic data.   \n\u2022 BinaryLogisticRegression. It measures the downstream classifier\u2019s F-1 score when trained on synthetic and test on real data. ", "page_idx": 8}, {"type": "text", "text": "Note that the benchmark datasets contain a limited number of numerical columns, with the majority being categorical (see Table 2). Hence, the applicability of CorrelationSimilarity and KSComplement is constrained as they are tailored for numerical columns. ", "page_idx": 8}, {"type": "text", "text": "Main results and observations. We present the experimental results for $\\epsilon=5.1$ in Table 1 and show results for $\\epsilon\\,=\\,8.1$ in Appendix C. As shown, the two methods using the slicing privacy mechanism (Algorithm 1 and SliceWass) consistently outperform the other baselines. MERF has better numbers for three instances of KSComp and one of LogitRegression, but is signficantly worse in other instances and for the remaining metrics. For both Algorithm 1 and SliceWass, we used the same hyper-parameter initialization and neural network architecture across all datasets. We believe that with more extensive hyperparameter tuning, which incurs no extra privacy cost as previously discussed, their performance could be even further improved. ", "page_idx": 8}, {"type": "text", "text": "Algorithm 1 exhibits a more favorable performance compared with SliceWass in most settings. The rationale behind this observation lies in the fact that SliceWass is limited to 1-dimensional slices $(k=1)$ ), relying on the closed-form expression of Wasserstein distances in 1D. In contrast, our Algorithm 1 is applicable to higher-dimensional slices. This capability enables the generative models to capture higher-order statistical information, leading to better performance in the pairwise ContingencySimilarity statistic and higher robustness in the BinaryLogisticRegression downstream task. ", "page_idx": 8}, {"type": "text", "text": "4.2 Synthetic Image Data ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We conduct an experiment to generate DP synthetic image data using the MNIST dataset [LBBH98]. We train separate generative models for each of the 10 classes in MNIST, with $10\\%$ of the data randomly sampled for each class. We evaluate the quality of the synthetic images by measuring the downstream accuracy of a classifier trained on the synthetic data and deployed on the real MNIST test set. We compare Algorithm 1 and SliceWass (combined with our slicing privacy mechanism), with MERF, which is a state-of-the-art MMD-based method for generating synthetic images [HAP21]. We observe that MERF outperforms the two slicing-mechanism-based algorithms at lower privacy budgets but underperforms at higher budgets. We hypothesize that MERF achieves superior performance in the low-budget regime since it privatizes only the mean embedding of the data for each class; its generative model is designed to recover this mean rather than the full data distribution. As using only the mean involves very little privacy budget, they are able to add less noise and outperform in the low-budget regime. Our approach, which models the full data distribution, does not benefti from these savings. However, as the privacy budget increases, our method better captures the true data distribution, leading to improved results over MERF. ", "page_idx": 8}, {"type": "image", "img_path": "3mCr7ZNdSw/tmp/350aa14af5313111e2d762a1cd95255349a0248510452e03e033253b174ab228.jpg", "img_caption": ["Figure 1: We compare accuracy for downstream classification as a function of privacy budget (\u03f5) for synthetic MNIST data created by MERF with our Algorithm 1 and SliceWass. Note that the two slicing-mechanism-based approaches outperform MERF for higher privacy budgets. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "4.3 Domain Adaptation ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Following the experiments in [RL21], for completeness we also apply our smoothed-sliced $f$ -divergence in domain adaptation tasks. We benchmark our approach with SliceWass [RL21] using the implementation available on their Github repo, modified to use our DP bound. This experiment aims to privately train a classifier using labeled data from a source domain and unlabelled data from a target domain. We consider the target and source domains being MNIST and USPS, as well as the reverse. Results for varying $\\epsilon$ are depicted in Figure 2. As shown, our Algorithm 1 using 100 slices improves on SliceWass. ", "page_idx": 9}, {"type": "image", "img_path": "3mCr7ZNdSw/tmp/715f9850f08b471114e1bba97a6438ad905e4f262bee34173bab815eb3e156df.jpg", "img_caption": ["Figure 2: Unsupervised domain adaptation between from MNIST to USPS and vice versa. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "5 Final Remarks and Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we consider the slicing mechanism for training privacy-preserving generative models and derive strong privacy guarantees for it. This mechanism motivates us to combine it with $f$ - divergence to yield a smoothed-sliced $f$ -divergence that can be estimated with a kernel-based density ratio estimator. Our approach circumvents the need for injecting noise into gradient updates and avoids adversarial training for optimizing generative models. It provides flexibility in selecting neural architectures and tuning hyper-parameters without incurring additional privacy costs. Through experiments on synthetic data generation, we validate our approach and compare it with existing baselines. Our findings suggest that the slicing privacy mechanism is a powerful tool for training generative models to create private synthetic data, and the smoothed-sliced $f$ -divergence provides a promising avenue for advancing the field of privacy-preserving data synthesis in sensitive, highdimensional datasets. Additionally, we hope our effort can be of particular interest to the information theory community and open up a new application frontier for classical information-theoretic tools. ", "page_idx": 9}, {"type": "text", "text": "There are several promising avenues worth exploring. The $f$ -divergence has many nice properties, including (strong) data processing inequalities, inequalities among different $f$ -divergences, and variational representation. It would be interesting to investigate whether similar properties extend to the smoothed-sliced $f$ -divergence $\\mathrm{SD}_{f,k,\\sigma^{2}}$ . Additionally, deriving sample-complexity bounds for estimating $\\mathrm{SD}_{f,k,\\sigma^{2}}$ from finite samples would offer valuable insights. Finally, the use of synthetic data presents both opportunities and challenges. On the one hand, synthetic data is well-suited for various tasks like early model development, educational demonstrations, simulation, and testing. On the other, synthetic data can never fully replicate all aspects of the original data, and adding DP guarantees introduces an additional layer of complexity requiring a careful balance between privacy and utility. Therefore, any machine learning models trained on synthetic data or any insights drawn from synthetic data should undergo thorough evaluation before deployment in real-world scenarios. If biases are detected, it is crucial to diagnose their source\u2014whether stemming from the synthetic data generation algorithm, the noise added to the real data, or other factors\u2014to ensure that the models perform robustly and reliably in practice. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We would like to thank Akash Srivastava for his insights on connecting sliced divergences to synthetic data generation, and for sharing [RL21] with us. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "$[\\mathrm{ABK}^{+}21]$ Sergul Aydore, William Brown, Michael Kearns, Krishnaram Kenthapadi, Luca Melis, Aaron Roth, and Ankit A Siva. Differentially private query release through adaptive projection. In International Conference on Machine Learning, pages 457\u2013467. PMLR, 2021.   \n[BBDS12] Jeremiah Blocki, Avrim Blum, Anupam Datta, and Or Sheffet. The JohnsonLindenstrauss transform itself preserves differential privacy. In 2012 IEEE 53rd Annual Symposium on Foundations of Computer Science, pages 410\u2013419. IEEE, 2012. [BBG18] Borja Balle, Gilles Barthe, and Marco Gaboardi. Privacy amplification by subsampling: Tight analyses via couplings and divergences. Advances in neural information processing systems, 31, 2018.   \n[BJWW+19] Brett K Beaulieu-Jones, Zhiwei Steven Wu, Chris Williams, Ran Lee, Sanjeev P Bhavnani, James Brian Byrd, and Casey S Greene. Privacy-preserving generative deep neural networks support clinical data sharing. Circulation: Cardiovascular Quality and Outcomes, 12(7):e005122, 2019. [BKZ23] Alex Bie, Gautam Kamath, and Guojun Zhang. Private GANs, revisited. Transactions on Machine Learning Research, 2023. [BS16] Mark Bun and Thomas Steinke. Concentrated differential privacy: Simplifications, extensions, and lower bounds. In Theory of Cryptography: 14th International Conference, TCC 2016-B, Beijing, China, October 31-November 3, 2016, Proceedings, Part I, pages 635\u2013658. Springer, 2016. [BSV22a] March Boedihardjo, Thomas Strohmer, and Roman Vershynin. Privacy of synthetic data: A statistical framework. IEEE Transactions on Information Theory, 69(1):520\u2013 527, 2022. [BSV22b] March Boedihardjo, Thomas Strohmer, and Roman Vershynin. Private sampling: a noiseless approach for generating differentially private synthetic data. SIAM Journal on Mathematics of Data Science, 4(3):1082\u20131115, 2022.   \n$[\\mathbf{CBV}^{+}21]$ Tianshi Cao, Alex Bie, Arash Vahdat, Sanja Fidler, and Karsten Kreis. Don\u2019t generate me: Training differentially private generative models with sinkhorn divergence. Advances in Neural Information Processing Systems, 34:12480\u201312492, 2021.   \n[CLWX21] Kuntai Cai, Xiaoyu Lei, Jianxin Wei, and Xiaokui Xiao. Data synthesis via differentially private markov random fields. Proceedings of the VLDB Endowment, 14(11):2190\u20132202, 2021. [COF20] Dingfan Chen, Tribhuvanesh Orekondy, and Mario Fritz. Gs-wgan: A gradientsanitized approach for learning differentially private generators. Advances in Neural Information Processing Systems, 33:12673\u201312684, 2020.   \n[CYZF20] Dingfan Chen, Ning Yu, Yang Zhang, and Mario Fritz. Gan-leaks: A taxonomy of membership inference attacks against generative models. In Proceedings of the 2020 ACM SIGSAC conference on computer and communications security, pages 343\u2013362, 2020.   \n[DAHY24] Konstantin Donhauser, Javier Abad, Neha Hulkund, and Fanny Yang. Privacypreserving data release leveraging optimal transport and particle gradient descent. arXiv preprint arXiv:2401.17823, 2024. [Dat23] DataCebo. Synthetic data metrics. https://docs.sdv.dev/sdmetrics/, 2023.   \n[DHMS21] Frances Ding, Moritz Hardt, John Miller, and Ludwig Schmidt. Retiring adult: New datasets for fair machine learning. Advances in neural information processing systems, 34:6478\u20136490, 2021. [DR14] Cynthia Dwork and Aaron Roth. The algorithmic foundations of differential privacy. Foundations and Trends\u00ae in Theoretical Computer Science, 9(3\u20134):211\u2013407, 2014. [DR16] Cynthia Dwork and Guy N Rothblum. Concentrated differential privacy. arXiv preprint arXiv:1603.01887, 2016. [Dud69] R. M. Dudley. The speed of mean Glivenko-Cantelli convergence. Ann. Math. Stats., 40(1):40\u201350, Feb. 1969. [DZS18] Ishan Deshpande, Ziyu Zhang, and Alexander G Schwing. Generative modeling using the sliced Wasserstein distance. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3483\u20133491, 2018.   \n[EKKL20] Marek Eli\u00e1\u0161, Michael Kapralov, Janardhan Kulkarni, and Yin Tat Lee. Differentially private release of synthetic graphs. In Proceedings of the Fourteenth Annual ACMSIAM Symposium on Discrete Algorithms, pages 560\u2013578. SIAM, 2020. [fM22] International Organization for Migration. The global victim-perpetrator synthetic data dashboard. https://www.ctdatacollaborative.org/ global-victim-perpetrator-synthetic-dataset, 2022.   \n$[\\mathrm{GAH^{+}14}]$ Marco Gaboardi, Emilio Jes\u00fas Gallego Arias, Justin Hsu, Aaron Roth, and Zhiwei Steven Wu. Dual query: Practical private query release for high dimensional data. In International Conference on Machine Learning, pages 1170\u20131178. PMLR, 2014. [GAL13] Manuel Gil, Fady Alajaji, and Tamas Linder. R\u00e9nyi divergence measures for commonly used univariate continuous distributions. Information Sciences, 249:124\u2013131, 2013. [GG20] Ziv Goldfeld and Kristjan Greenewald. Gaussian-smoothed optimal transport: Metric structure and statistical efficiency. In International Conference on Artificial Intelligence and Statistics, pages 3327\u20133337. PMLR, 2020. [GG21] Ziv Goldfeld and Kristjan Greenewald. Sliced mutual information: A scalable measure of statistical dependence. Advances in Neural Information Processing Systems, 34:17567\u201317578, 2021. [GGK20] Ziv Goldfeld, Kristjan Greenewald, and Kengo Kato. Asymptotic guarantees for generative modeling based on the smooth Wasserstein distance. Advances in neural information processing systems, 33:2527\u20132539, 2020.   \n[GGNWP20] Z. Goldfeld, K. Greenewald, J. Niles-Weed, and Y. Polyanskiy. Convergence of smoothed empirical measures with applications to entropy estimation. IEEE Trans. Inf. Theory, 66(7):4368\u20134391, Jul. 2020.   \n[GMHI20] Chang Ge, Shubhankar Mohapatra, Xi He, and Ihab F Ilyas. Kamino: Constraint-aware differentially private data synthesis. arXiv preprint arXiv:2012.15713, 2020. [HAP21] Frederik Harder, Kamil Adamczewski, and Mijung Park. Dp-merf: Differentially private mean embeddings with randomfeatures for practical privacy-preserving data generation. In International conference on artificial intelligence and statistics, pages 1819\u20131827. PMLR, 2021. [HC24] Shlomi Hod and Ran Canetti. Differentially private release of israel\u2019s national registry of live births. arXiv preprint arXiv:2405.00267, 2024. [HJSP23] Frederik Harder, Milad Jalali, Danica J Sutherland, and Mijung Park. Pre-trained perceptual features improve differentially private image generation. Transactions on Machine Learning Research, 2023.   \n[HMDDC17] Jamie Hayes, Luca Melis, George Danezis, and Emiliano De Cristofaro. Logan: Membership inference attacks against generative models. arXiv preprint arXiv:1705.07663, 2017.   \n[JYVDS19] James Jordon, Jinsung Yoon, and Mihaela Van Der Schaar. PATE-GAN: Generating synthetic data with differential privacy guarantees. In International conference on learning representations, 2019.   \n$[\\mathrm{KLN}^{+}11]$ Shiva Prasad Kasiviswanathan, Homin K Lee, Kobbi Nissim, Sofya Raskhodnikova, and Adam Smith. What can we learn privately? SIAM Journal on Computing, 40(3):793\u2013826, 2011.   \n[LBBH98] Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u2013 2324, 1998.   \n$[\\mathrm{LJW}^{+}20]$ Zinan Lin, Alankar Jain, Chen Wang, Giulia Fanti, and Vyas Sekar. Using gans for sharing networked time series data: Challenges, initial promise, and open questions. In Proceedings of the ACM Internet Measurement Conference, pages 464\u2013483, 2020. [LQS12] Ninghui Li, Wahbeh Qardaji, and Dong Su. On sampling, anonymization, and differential privacy or, $\\mathbf{k}$ -anonymization meets differential privacy. In Proceedings of the 7th ACM Symposium on Information, Computer and Communications Security, pages 32\u201333, 2012. [LT19] Jingcheng Liu and Kunal Talwar. Private selection from private candidates. In Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing, pages 298\u2013309, 2019.   \n[LTVW23] Terrance Liu, Jingwu Tang, Giuseppe Vietri, and Steven Wu. Generating private synthetic data with genetic algorithms. In International Conference on Machine Learning, pages 22009\u201322027. PMLR, 2023. [LVW21] Terrance Liu, Giuseppe Vietri, and Steven Z Wu. Iterative methods for private synthetic data: Unifying framework and new methods. Advances in Neural Information Processing Systems, 34:690\u2013702, 2021.   \n$[\\mathrm{LZC}^{+}21]$ T. Lin, Z. Zheng, E. Chen, M. Cuturi, and M. Jordan. On projection robust optimal transport: Sample complexity and model misspecification. In International Conference on Artificial Intelligence and Statistics (AISTATS-2019), pages 262\u2013270, Online, 2021. [Mir17] Ilya Mironov. R\u00e9nyi differential privacy. In 2017 IEEE 30th computer security foundations symposium (CSF), pages 263\u2013275. IEEE, 2017. [MMS21] Ryan McKenna, Gerome Miklau, and Daniel Sheldon. Winning the NIST contest: A scalable and general approach to differentially private synthetic data. arXiv preprint arXiv:2108.04978, 2021.   \n[MMSM22] Ryan McKenna, Brett Mullins, Daniel Sheldon, and Gerome Miklau. Aim: An adaptive and iterative mechanism for differentially private synthetic data. arXiv preprint arXiv:2201.12677, 2022.   \n[MSGHI18] Kevin R Moon, Kumar Sricharan, Kristjan Greenewald, and Alfred O Hero III. Ensemble estimation of information divergence. Entropy, 20(8):560, 2018.   \n[MSM19] Ryan McKenna, Daniel Sheldon, and Gerome Miklau. Graphical-model based estimation and inference for differential privacy. In International Conference on Machine Learning, pages 4435\u20134444. PMLR, 2019.   \n[MZKH23] Shubhankar Mohapatra, Jianqiao Zong, Florian Kerschbaum, and Xi He. Differentially private data generation with missing data. arXiv preprint arXiv:2310.11548, 2023. [NCT16] S. Nowozin, B. Cseke, and R. Tomioka. $f$ -GAN: Training generative neural samplers using variational divergence minimization. In Proceedings of the Annual Conference on Advances in Neural Information Processing Systems (NeurIPS-2016), pages 271\u2013279, Barcelona, Spain, Dec. 2016.   \n$[\\mathrm{NDC}^{+}20]$ K. Nadjahi, A. Durmus, L. Chizat, S. Kolouri, S. Shahrampour, and U. Simsekli. Statistical and topological properties of sliced probability divergences. In Proceedings of the Annual Conference on Advances in Neural Information Processing Systems (NeurIPS-2020), Online, Dec. 2020.   \n$[\\mathrm{NDL}^{+}23]$ Joseph P Near, David Darais, Naomi Lefkovitz, Gary Howarth, et al. Guidelines for evaluating differential privacy guarantees. Technical report, National Institute of Standards and Technology, 2023. [NPA23] Valtteri A Nieminen, Tapio Pahikkala, and Antti Airola. Empirical evaluation of amplifying privacy by subsampling for gans to create differentially private synthetic tabular data. TKTP 2023: Annual Symposium for Computer Science, 2023. [NTZ13] Aleksandar Nikolov, Kunal Talwar, and Li Zhang. The geometry of differential privacy: the sparse and approximate cases. In Proceedings of the forty-fifth annual ACM symposium on Theory of computing, pages 351\u2013360, 2013.   \n[NWD20] Marcel Neunhoeffer, Zhiwei Steven Wu, and Cynthia Dwork. Private post-gan boosting. arXiv preprint arXiv:2007.11934, 2020. [PS22] Nicolas Papernot and Thomas Steinke. Hyperparameter tuning with renyi differential privacy. In International Conference on Learning Representations, 2022. [PW23] Yury Polyanskiy and Yihong Wu. Information theory: From coding to learning. Cambridge university press, $^{2023+}$ . [RL21] Alain Rakotomamonjy and Ralaivola Liva. Differentially private sliced Wasserstein distance. In International Conference on Machine Learning, pages 8810\u20138820. PMLR, 2021.   \n$[{\\mathrm{RLP}}^{+}20]$ Lucas Rosenblatt, Xiaoyan Liu, Samira Pouyanfar, Eduardo de Leon, Anuj Desai, and Joshua Allen. Differentially private synthetic data: Applied evaluations and enhancements. arXiv preprint arXiv:2011.05537, 2020.   \n[RPDB11] J. Rabin, G. Peyr\u00e9, J. Delon, and M. Bernot. Wasserstein barycenter and its application to texture mixing. In Proceedings of the International Conference on Scale Space and Variational Methods in Computer Vision (SSVM-2011), pages 435\u2013446, Gedi, Israel, May 2011.   \n[RTMT21] Diane Ridgeway, Mary F Theofanos, Terese W Manley, and Christine Task. Challenge design and lessons learned from the 2018 differential privacy challenges. NIST Technical Note 2151, 2021. [Sma23] SmartNoise. Smartnoise sdk: Tools for differential privacy on tabular data. https: //github.com/opendp/smartnoise-sdk, 2023. [SOT22] Theresa Stadler, Bristena Oprisanu, and Carmela Troncoso. Synthetic data\u2013 anonymisation groundhog day. In 31st USENIX Security Symposium, pages 1451\u20131468, 2022. [SSK12] Masashi Sugiyama, Taiji Suzuki, and Takafumi Kanamori. Density ratio estimation in machine learning. Cambridge University Press, 2012.   \n[SXGS20] Akash Srivastava, Kai Xu, Michael U. Gutmann, and Charles Sutton. Generative ratio matching networks. In International Conference on Learning Representations, 2020. [TKP19] Reihaneh Torkzadehmahani, Peter Kairouz, and Benedict Paten. Dp-cgan: Differentially private synthetic data and label generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 0\u20130, 2019.   \n$[\\mathrm{TMH}^{+}21]$ Yuchao Tao, Ryan McKenna, Michael Hay, Ashwin Machanavajjhala, and Gerome Miklau. Benchmarking differentially private synthetic data generation algorithms. arXiv preprint arXiv:2112.09238, 2021.   \n[TTCY21] Shun Takagi, Tsubasa Takahashi, Yang Cao, and Masatoshi Yoshikawa. P3gm: Private high-dimensional data release via privacy preserving phased generative model. In 2021 IEEE 37th International Conference on Data Engineering (ICDE), pages 169\u2013180. IEEE, 2021.   \n[TWB+19] Uthaipon Tantipongpipat, Chris Waites, Digvijay Boob, Amaresh Ankit Siva, and Rachel Cummings. Differentially private mixed-type data generation for unsupervised learning. arXiv preprint arXiv:1912.03250, 1:13, 2019. [UV20] Jonathan Ullman and Salil Vadhan. PCPs and the hardness of generating synthetic data. Journal of Cryptology, 33(4):2078\u20132112, 2020.   \n$[\\mathrm{VAA}^{+}22]$ Giuseppe Vietri, Cedric Archambeau, Sergul Aydore, William Brown, Michael Kearns, Aaron Roth, Ankit Siva, Shuai Tang, and Steven Z Wu. Private synthetic data for multitask learning and marginal queries. Advances in Neural Information Processing Systems, 35:18282\u201318295, 2022.   \n$[\\mathrm{VCH}^{+}22]$ Margarita Vinaroz, Mohammad-Amin Charusaie, Frederik Harder, Kamil Adamczewski, and Mi Jung Park. Hermite polynomial features for private data generation. In International Conference on Machine Learning, pages 22300\u201322324. PMLR, 2022.   \n$[\\mathrm{VFT}^{+}19]$ T. Vayer, R. Flamary, R. Tavenard, L. Chapel, and N. Courty. Sliced GromovWasserstein. In Proceedings of the Annual Conference on Advances in Neural Information Processing Systems (NeurIPS-2019), Vancouver, Canada, Dec. 2019.   \n$[\\mathrm{VTB}^{+}20]$ Giuseppe Vietri, Grace Tian, Mark Bun, Thomas Steinke, and Steven Wu. New oracleefficient algorithms for private synthetic data release. In International Conference on Machine Learning, pages 9765\u20139774. PMLR, 2020.   \n[WKV09] Qing Wang, Sanjeev R Kulkarni, and Sergio Verd\u00fa. Divergence estimation for multidimensional densities via $k$ -nearest-neighbor distances. IEEE Transactions on Information Theory, 55(5):2392\u20132405, 2009.   \n$[\\mathrm{WSH}^{+}23]$ Hao Wang, Shivchander Sudalairaj, John Henning, Kristjan Greenewald, and Akash Srivastava. Post-processing private synthetic data for improving utility on selected measures. In Conference on Neural Information Processing Systems, 2023. [WY20] Yihong Wu and Pengkun Yang. Polynomial methods in statistical inference: Theory and practice. Foundations and Trends\u00ae in Communications and Information Theory, 17(4):402\u2013586, 2020.   \n$[\\mathbf{XG}\\mathbf{J}^{+}22]$ ] Kai Xu, Georgi Ganev, Emile Joubert, Rees Davison, Olivier Van Acker, and Luke Robinson. Synthetic data generation of many-to-many datasets via random graph generation. In International Conference on Learning Representations, 2022.   \n$[\\mathrm{XLW^{+}18}]$ Liyang Xie, Kaixiang Lin, Shu Wang, Fei Wang, and Jiayu Zhou. Differentially private generative adversarial network. arXiv preprint arXiv:1802.06739, 2018.   \n$[\\mathrm{YAS}^{+}23]$ Yilin Yang, Kamil Adamczewski, Danica J Sutherland, Xiaoxiao Li, and Mijung Park. Differentially private neural tangent kernels for privacy-preserving data generation. arXiv preprint arXiv:2303.01687, 2023.   \n$[Z C P^{+}17]$ Jun Zhang, Graham Cormode, Cecilia M Procopiuc, Divesh Srivastava, and Xiaokui Xiao. PrivBayes: Private data release via Bayesian networks. ACM Transactions on Database Systems (TODS), 42(4):1\u201341, 2017.   \n$[Z\\mathrm{WL}^{+}21]$ Zhikun Zhang, Tianhao Wang, Ninghui Li, Jean Honorio, Michael Backes, Shibo He, Jiming Chen, and Yang Zhang. $\\{{\\mathrm{PrivSyn}}\\}$ : Differentially private data synthesis. In 30th USENIX Security Symposium (USENIX Security 21), pages 929\u2013946, 2021. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Privacy Analysis ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Before diving into the proof of Theorem 1, we will first revisit the definition of R\u00e9nyi differential privacy (RDP) [Mir17, DR16, BS16] and its connection with differential privacy (DP). Additionally, we will recall a useful property of DP known as privacy amplification by subsampling. ", "page_idx": 15}, {"type": "text", "text": "Definition 6. A randomized mechanism $\\mathcal{M}:\\mathbb{R}^{n\\times d}\\rightarrow\\mathbb{O}$ satisfies $(\\alpha,\\epsilon)$ -RDP if for any adjacent datasets X, $\\mathbf{\\Delta},\\mathbf{X}^{\\prime}\\in\\mathbb{R}^{n\\times d}$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\mathbf{X}\\sim\\mathbf{X}^{\\prime}}\\mathrm{D}_{\\alpha}(\\mathcal{M}(\\mathbf{X})||\\mathcal{M}(\\mathbf{X}^{\\prime}))\\leq\\epsilon,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\textstyle\\mathrm{D}_{\\alpha}$ is the R\u00e9nyi- $\\alpha$ divergence. ", "page_idx": 15}, {"type": "text", "text": "Proposition 2 (Prop. 3 in [Mir17]). If $\\mathcal{M}$ is an $(\\alpha,\\epsilon)$ -RDP mechanism, it also satisfies $(\\epsilon\\!+\\!\\frac{\\ln(1/\\delta)}{\\alpha\\!-\\!1},\\delta)$ DP for all $\\delta\\in(0,1)$ . ", "page_idx": 15}, {"type": "text", "text": "We remind readers that two datasets $\\mathbf{X}$ and $\\mathbf{X^{\\prime}}$ are considered adjacent if they differ in a single row, say the $i$ -th row, such that $\\|\\mathbf{X}_{i,:}-\\mathbf{X}_{i,:}^{\\prime}\\|_{2}\\leq1$ where $\\mathbf{X}_{i}$ ,: and $\\mathbf{X}_{i,}^{\\prime}$ : are the $i$ -th row of $\\mathbf{X}$ and $\\mathbf{X^{\\prime}}$ , respectively. This condition applies, for example, to unbounded DP (add/remove one record) when the $L_{2}$ norm of each record is upper bounded by 1. ", "page_idx": 15}, {"type": "text", "text": "We revisit privacy amplification by subsampling, a useful technique for handling large-scale datasets that can save computational resources and memory. It proposes that the privacy guarantees of a DP mechanism can be improved by randomly subsampling the private dataset before applying the DP mechanism $[\\mathrm{KLN}^{+}11$ , LQS12, BBG18]. ", "page_idx": 15}, {"type": "text", "text": "Lemma 1. Given a dataset $\\mathbf{X}=[\\pmb{x}_{1}^{T},\\cdots,\\pmb{x}_{n}^{T}]^{T}$ , PoissonSample\u03c4 independently draws Bernoulli random variables $\\sigma_{i}\\sim\\mathsf{B e r n}(\\tau)$ for $i\\,\\in\\,[n]$ and outputs a subset $[{\\pmb x}_{i}^{T}]_{\\sigma_{i}=1,i\\in[n]}^{\\hat{T}}$ . If a mechanism $\\mathcal{M}$ is $(\\epsilon,\\delta)$ -DP, then $\\mathcal{M}\\circ$ PoissonSample $\\tau$ satisfies $(\\epsilon^{\\prime},\\delta^{\\prime})$ -DP where $\\epsilon^{\\prime}=\\log(1+\\tau(e^{\\epsilon}-1))$ and $\\delta^{\\prime}=\\tau\\delta$ . ", "page_idx": 15}, {"type": "text", "text": "We introduce some notations and a result from [GAL13] that will be used in proving the following lemma. For a matrix $\\mathbf{A}\\in\\mathbb{R}^{m\\times n}$ , its determinant is denoted by $|\\mathbf{A}|$ . The vectorization of $\\mathbf{A}$ is defined as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathrm{vec}(\\mathbf{A})\\triangleq[a_{1,1},\\cdot\\cdot\\cdot\\mathbf{\\Phi},a_{m,1},\\cdot\\cdot\\cdot\\mathbf{\\Phi},a_{1,n},\\cdot\\cdot\\cdot\\mathbf{\\Phi},a_{m,n}]^{T}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "For two matrices $\\mathbf{A}\\in\\mathbb{R}^{m\\times n}$ and $\\mathbf{B}\\in\\mathbb{R}^{p\\times q}$ , their Kronecker product $\\mathbf{A}\\otimes\\mathbf{B}$ is the $p m\\times q n$ block matrix: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{A}\\otimes\\mathbf{B}\\triangleq\\left[\\!\\!\\begin{array}{c c c}{a_{1,1}\\mathbf{B}}&{\\cdots}&{a_{1,n}\\mathbf{B}}\\\\ {\\vdots}&{\\ddots}&{\\vdots}\\\\ {a_{m,1}\\mathbf{B}}&{\\cdots}&{a_{m,n}\\mathbf{B}}\\end{array}\\!\\!\\right].\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We recall a result from [GAL13, Table 2]: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{D}_{\\alpha}(\\mathcal{N}(\\mu,\\Sigma)\\|\\mathcal{N}(\\mu^{\\prime},\\Sigma^{\\prime}))}\\\\ &{\\quad=\\frac{\\alpha}{2}(\\mu-\\mu^{\\prime})^{T}((1-\\alpha)\\Sigma+\\alpha\\Sigma^{\\prime})^{-1}(\\mu-\\mu^{\\prime})-\\frac{1}{2(\\alpha-1)}\\ln\\frac{\\left|(1-\\alpha)\\pmb{\\Sigma}+\\alpha\\Sigma^{\\prime}\\right|}{|\\Sigma|^{1-\\alpha}|\\Sigma^{\\prime}|^{\\alpha}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "whenever $\\alpha\\pmb{\\Sigma}^{-1}+(1-\\alpha)\\pmb{\\Sigma}^{\\prime}{}^{-1}>0$ . ", "page_idx": 15}, {"type": "text", "text": "Next, we establish the RDP guarantees for our mechanism. ", "page_idx": 15}, {"type": "text", "text": "Lemma 2. Assume $\\alpha\\,>\\,1$ and $\\gamma\\;\\triangleq\\;\\sigma^{-2}(\\alpha^{2}\\,-\\,\\alpha)\\;<\\;d.$ . Let $\\textbf{U}\\in\\mathbb{R}^{d\\times m^{\\prime}}$ and $\\textbf{V}\\in\\,\\mathbb{R}^{n\\times m^{\\prime}}$ be two random matrices whose elements are drawn independently from $\\mathcal{N}(0,d^{-1})$ and ${\\mathcal{N}}(0,\\sigma^{2})$ , respectively. Then the mechanism $\\mathcal{M}(\\mathbf{X})=(\\mathbf{U},\\mathbf{X}\\mathbf{U}+\\mathbf{V})$ satisfies $(\\alpha,\\epsilon)$ -RDP where ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\epsilon=\\frac{m^{\\prime}\\alpha}{2\\sigma^{2}(d-\\gamma)}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof of Lemma 2. Since $\\mathbf{X}$ is not random, the mechanism $\\mathcal{M}(\\mathbf{X})$ maps $\\mathbf{X}$ to a $(d m^{\\prime}\\!+\\!n m^{\\prime})$ -variate Gaussian distribution with mean zero and covariance ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Cov}\\left(\\left[\\begin{array}{c}{\\mathrm{vec}(\\mathbf{U}^{T})}\\\\ {\\mathrm{vec}((\\mathbf{XU}+\\mathbf{V})^{T})}\\end{array}\\right]\\right)=d^{-1}\\left[\\begin{array}{c c}{\\mathbf{I}_{d m^{\\prime}}}&{\\mathbf{X}^{T}\\otimes\\mathbf{I}_{m^{\\prime}}}\\\\ {\\mathbf{X}\\otimes\\mathbf{I}_{m^{\\prime}}}&{(\\mathbf{XX}^{T}\\otimes\\mathbf{I}_{m^{\\prime}})+d\\sigma^{2}\\mathbf{I}_{n m^{\\prime}}}\\end{array}\\right]}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad=d^{-1}\\left[\\begin{array}{c c}{\\mathbf{I}_{d}}&{\\mathbf{X}^{T}}\\\\ {\\mathbf{X}}&{\\mathbf{XX}^{T}+d\\sigma^{2}\\mathbf{I}_{n}}\\end{array}\\right]\\otimes\\mathbf{I}_{m^{\\prime}}}\\\\ &{\\quad\\quad\\quad\\quad\\quad=d^{-1}\\mathbf{B}\\otimes\\mathbf{I}_{m^{\\prime}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where we define $\\mathbf{B}\\triangleq\\left[\\begin{array}{l l}{\\mathbf{I}_{d}}&{\\mathbf{X}^{T}}\\\\ {\\mathbf{X}}&{\\mathbf{X}\\mathbf{X}^{T}+d\\sigma^{2}\\mathbf{I}_{n}}\\end{array}\\right]$ Combining (7) with (8) yields ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{D}_{\\alpha}(\\mathcal{M}(\\mathbf{X})\\|\\mathcal{M}(\\mathbf{X}^{\\prime}))=-\\frac{m^{\\prime}}{2\\left(\\alpha-1\\right)}\\ln\\frac{|(1-\\alpha)\\mathbf{B}+\\alpha\\mathbf{B}^{\\prime}|}{|\\mathbf{B}|^{1-\\alpha}|\\mathbf{B}^{\\prime}|^{\\alpha}}}\\\\ &{\\qquad\\qquad\\qquad=\\frac{m^{\\prime}}{2\\left(\\alpha-1\\right)}\\left[\\left((1-\\alpha)\\ln|\\mathbf{B}|+\\alpha\\ln|\\mathbf{B}^{\\prime}|\\right)-\\ln|(1-\\alpha)\\mathbf{B}+\\alpha\\mathbf{B}^{\\prime}|\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Next, we compute the determinants in the above R\u00e9nyi divergence. Using the formula for determinants of block matrices, we know ", "page_idx": 16}, {"type": "equation", "text": "$$\n|\\mathbf{B}|=|\\mathbf{B}^{\\prime}|=|d\\sigma^{2}\\mathbf{I}_{n}|.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "For the linear combination, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|(1-\\alpha)\\mathbf{B}+\\alpha\\mathbf{B^{\\prime}}|}\\\\ &{=\\left|\\left[\\begin{array}{c c}{\\mathbf{I}_{d}}&{(1-\\alpha)X^{T}+\\alpha\\mathbf{X^{\\prime}}^{T}}\\\\ {(1-\\alpha)\\mathbf{X}+\\alpha\\mathbf{X^{\\prime}}}&{(1-\\alpha)\\mathbf{X}\\mathbf{X^{T}}+\\alpha\\mathbf{X^{\\prime}}\\mathbf{X^{\\prime}}^{T}+d\\sigma^{2}\\mathbf{I}_{n}}\\end{array}\\right]\\right|}\\\\ &{=\\left|d\\sigma^{2}\\mathbf{I}_{n}+\\left[(1-\\alpha)\\mathbf{X}\\mathbf{X^{T}}+\\alpha\\mathbf{X^{\\prime}}\\mathbf{X^{\\prime}}^{T}-((1-\\alpha)\\mathbf{X}+\\alpha\\mathbf{X^{\\prime}})\\left((1-\\alpha)\\mathbf{X}+\\alpha\\mathbf{X^{\\prime}}\\right)^{T}\\right]\\right|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We denote ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta\\triangleq\\left(1-\\alpha\\right)\\mathbf{X}\\mathbf{X}^{T}+\\alpha\\mathbf{X}^{\\prime}\\mathbf{X}^{\\prime}^{T}-\\left((1-\\alpha)\\mathbf{X}+\\alpha\\mathbf{X}^{\\prime}\\right)\\left((1-\\alpha)\\mathbf{X}+\\alpha\\mathbf{X}^{\\prime}\\right)^{T}}\\\\ &{\\quad=-\\alpha(\\mathbf{X}\\mathbf{X}^{T}-\\mathbf{X}^{\\prime}\\mathbf{X}^{\\prime}^{T})-\\alpha^{2}(\\mathbf{X}-\\mathbf{X}^{\\prime})(\\mathbf{X}-\\mathbf{X}^{\\prime})^{T}+\\alpha\\mathbf{X}(\\mathbf{X}-\\mathbf{X}^{\\prime})^{T}+\\alpha(\\mathbf{X}-\\mathbf{X}^{\\prime})\\mathbf{X}^{T}}\\\\ &{\\quad=-\\alpha(-(\\mathbf{X}-\\mathbf{X}^{\\prime})(\\mathbf{X}-\\mathbf{X}^{\\prime})^{T}+\\mathbf{X}(\\mathbf{X}-\\mathbf{X}^{\\prime})^{T}+(\\mathbf{X}-\\mathbf{X}^{\\prime})\\mathbf{X}^{T})}\\\\ &{\\quad\\quad-\\alpha^{2}(\\mathbf{X}-\\mathbf{X}^{\\prime})(\\mathbf{X}-\\mathbf{X}^{\\prime})^{T}+\\alpha\\mathbf{X}(\\mathbf{X}-\\mathbf{X}^{\\prime})^{T}+\\alpha(\\mathbf{X}-\\mathbf{X}^{\\prime})\\mathbf{X}^{T}}\\\\ &{\\quad=(\\alpha-\\alpha^{2})(\\mathbf{X}-\\mathbf{X}^{\\prime})(\\mathbf{X}-\\mathbf{X}^{\\prime})^{T}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Since $\\mathbf{X}$ and $\\mathbf{X^{\\prime}}$ only differ in the $i$ -th row, only one entry of this matrix can be nonzero, specifically, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\alpha-\\alpha^{2}\\leq\\Delta_{i i}\\leq0.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\mathrm{D}_{\\alpha}(\\mathcal{M}({\\bf X})||\\mathcal{M}({\\bf X}^{\\prime}))=\\frac{m^{\\prime}}{2(\\alpha-1)}\\left[\\left((1-\\alpha)\\ln|{\\bf B}|+\\alpha\\ln|{\\bf B}^{\\prime}|\\right)-\\ln|(1-\\alpha){\\bf B}+\\alpha{\\bf B}^{\\prime}|\\right]}\\ ~}\\\\ {{\\displaystyle=\\frac{m^{\\prime}}{2(\\alpha-1)}\\left[\\ln|d\\sigma^{2}{\\bf I}_{n}|-\\ln|d\\sigma^{2}{\\bf I}_{n}+\\Delta|\\right]}\\ ~}\\\\ {{\\displaystyle=\\frac{m^{\\prime}}{2(\\alpha-1)}\\left[\\ln(d\\sigma^{2})-\\ln(d\\sigma^{2}+\\Delta_{i i})\\right].}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Recall the assumption that $|\\Delta_{i i}|\\leq\\alpha^{2}-\\alpha\\leq\\gamma\\sigma^{2}$ for $\\gamma\\in(0,d)$ . Then using the fact that $\\log(a)$ is concave in $a$ and has derivative $1/a$ for $a>0$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{D}_{\\alpha}(\\boldsymbol{M}(\\mathbf{X})||\\boldsymbol{M}(\\mathbf{X}^{\\prime}))\\leq-\\frac{m^{\\prime}}{2(\\alpha-1)}\\frac{1}{d\\sigma^{2}+\\Delta_{i i}}\\Delta_{i i}}\\\\ &{\\phantom{D}\\leq\\frac{m^{\\prime}}{2(\\alpha-1)}\\frac{1}{d\\sigma^{2}+(\\alpha-\\alpha^{2})}(\\alpha^{2}-\\alpha)}\\\\ &{\\phantom{D}\\leq\\frac{m^{\\prime}}{2(\\alpha-1)}\\frac{1}{(d-\\gamma)\\sigma^{2}}(\\alpha^{2}-\\alpha)}\\\\ &{\\phantom{D}\\leq\\frac{m^{\\prime}}{2(d-\\gamma)\\sigma^{2}(\\alpha-1)}\\frac{1}{(\\alpha^{2}-\\alpha)}(\\alpha^{2}-\\alpha)}\\\\ &{\\phantom{D}\\leq\\frac{m^{\\prime}}{2(d-\\gamma)\\sigma^{2}(\\alpha-1)}(\\alpha^{2}-\\alpha)}\\\\ &{\\phantom{D}\\leq\\frac{m^{\\prime}\\alpha}{2\\sigma^{2}(d-\\gamma)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof of Theorem 1. Theorem 1 follows directly from combining Lemma 2 with Proposition 2. ", "page_idx": 17}, {"type": "text", "text": "Next, we establish a privacy bound assuming the projection matrix $\\mathbf{U}$ is deterministic. Comparing this with Theorem 1, we observe that by randomly selecting the projection matrix, we can achieve a tighter privacy bound, even if it is disclosed by the privacy mechanism. ", "page_idx": 17}, {"type": "text", "text": "Proposition 3. Let $\\mathbf{X}\\,\\in\\,\\mathbb{R}^{n\\times d}$ represents the original dataset. Let $\\mathbf{V}\\,\\in\\,\\mathbb{R}^{n\\times m^{\\prime}}$ be a random noise matrix with each element independently drawn from $\\mathbf{V}_{i,j}\\sim\\mathcal{N}(0,\\sigma^{2})$ . Let $\\mathbf{U}\\in\\mathbb{R}^{d\\times m^{\\prime}}$ be $a$ deterministic matrix such that for any $j\\in[m]$ ", "page_idx": 17}, {"type": "text", "text": "The privacy mechanism $\\mathcal{M}_{d e t}(\\mathbf{X})\\triangleq\\mathbf{X}\\mathbf{U}+\\mathbf{V}$ satisfies ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left(\\frac{m\\alpha}{2\\sigma^{2}}+\\frac{\\ln(1/\\delta)}{\\alpha-1},\\delta\\right)-D P.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof of Proposition 3. The privacy mechanism $\\mathcal{M}_{\\mathrm{det}}$ maps $\\mathbf{X}$ to a $n m^{\\prime}$ -variate Gaussian distribution with mean and covariance being: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname{mean}(\\mathbf{X}\\mathbf{U}+\\mathbf{V})=\\operatorname{vec}(\\mathbf{X}\\mathbf{U}),\\quad\\operatorname{Cov}(\\mathbf{X}\\mathbf{U}+\\mathbf{V})=\\sigma^{2}\\mathbf{I}_{n m^{\\prime}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Using (7) yields ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{D}_{\\alpha}(\\mathcal{M}(\\mathbf{X})\\|\\mathcal{M}(\\mathbf{X}^{\\prime}))=\\frac{\\alpha}{2}\\mathrm{vec}(\\mathbf{XU}-\\mathbf{X}^{\\prime}\\mathbf{U})^{T}(\\sigma^{2}\\mathbf{I}_{n m^{\\prime}})^{-1}\\mathrm{vec}(\\mathbf{XU}-\\mathbf{X}^{\\prime}\\mathbf{U})}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\frac{\\alpha}{2\\sigma^{2}}\\mathrm{vec}((\\mathbf{X}-\\mathbf{X}^{\\prime})\\mathbf{U})^{T}\\mathrm{vec}((\\mathbf{X}-\\mathbf{X}^{\\prime})\\mathbf{U})}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\frac{\\alpha}{2\\sigma^{2}}\\|(\\mathbf{X}_{i,:}-\\mathbf{X}_{i,:}^{\\prime})\\mathbf{U}\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Without loss of generality, we assume ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbf{X}_{i,:}-\\mathbf{X}_{i,:}^{\\prime}=[x,0,\\cdots{\\mathrm{~,~}}0]\\quad{\\mathrm{with~}}|x|\\leq1.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Additionally, recall the assumption that for any $j\\in[m],\\mathbf{U}_{:,(j-1)k+1},\\cdot\\cdot\\cdot\\mathbf{\\epsilon},\\mathbf{U}_{:,j k}$ are orthonormal. Hence, $U_{1,(j-1)k+1}^{2}+\\cdot\\cdot\\cdot+U_{1,j k+1}^{2}\\leq1$ , leading to ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathrm{D}_{\\alpha}(\\boldsymbol{\\mathcal{M}}(\\mathbf{X})\\|\\boldsymbol{\\mathcal{M}}(\\mathbf{X}^{\\prime}))\\leq\\frac{\\alpha}{2\\sigma^{2}}\\sum_{j=1}^{m^{\\prime}}U_{1,j}^{2}\\leq\\frac{\\alpha m}{2\\sigma^{2}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "As a result, our privacy mechanism is $\\left(\\alpha,\\,{\\frac{\\alpha m}{2\\sigma^{2}}}\\right)$ -RDP. Combining it with Proposition 2 yields the desired DP guarantee. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "B Other Omitted Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proof of Proposition $^{\\,l}$ . The property $\\mathrm{SD}_{f,k,\\sigma^{2}}(P_{\\mathrm{X}}\\|Q_{\\mathrm{X}})\\ge0$ follows by the non-negativity of the $f$ -divergence in the integrand. If $P_{X}=Q_{X}$ , then clearly ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathrm{SD}_{f,k,\\sigma^{2}}(P_{\\mathrm{X}}\\|Q_{\\mathrm{X}})=\\frac{1}{\\mathrm{vol}(\\mathrm{S}_{\\mathrm{k}}(\\mathbb{R}^{\\mathrm{d}}))}\\int_{\\pmb{\\theta}\\in\\mathbb{S}_{k}(\\mathbb{R}^{d})}\\mathrm{D}_{f}(P_{\\pmb{\\theta}^{T}\\mathrm{X}+\\mathrm{N}}\\|Q_{\\pmb{\\theta}^{T}\\mathrm{X}+\\mathrm{N}})\\mathbf{d}\\pmb{\\theta}=0\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "since the $f$ -divergences $\\mathsf{D}_{f}(P_{\\pmb{\\theta}^{T}\\mathrm{X}+\\mathbf{N}}||Q_{\\pmb{\\theta}^{T}\\mathrm{X}+\\mathbf{N}})$ are all zero by definition. ", "page_idx": 18}, {"type": "text", "text": "Conversely, by the nonnegativity of the $f$ -divergence, $\\operatorname{SD}_{f,k,\\sigma^{2}}(P_{\\mathrm{X}}\\|Q_{\\mathrm{X}})=0$ implies ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathrm{D}_{f}\\big(P_{\\pmb{\\theta}^{T}\\mathrm{X+N}}\\big|\\rvert Q_{\\pmb{\\theta}^{T}\\mathrm{X+N}}\\big)=0\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for almost every $\\pmb{\\theta}\\in\\mathbb{S}_{k}(\\mathbb{R}^{d})$ . Additionally, since $f$ is strictly convex at 1, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\nP_{\\pmb{\\theta}^{T}\\mathrm{X+N}}=Q_{\\pmb{\\theta}^{T}\\mathrm{X+N}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "As a result, by the invertibility of Gaussian convolution, ", "page_idx": 18}, {"type": "equation", "text": "$$\nP_{\\pmb{\\theta}^{T}\\mathbf{X}}=Q_{\\pmb{\\theta}^{T}\\mathbf{X}}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for almost every $\\pmb{\\theta}\\in\\mathbb{S}_{k}(\\mathbb{R}^{d})$ since $f$ -divergence nullifies iff the arguments are equal. Since the distributions are equal, the moment generating functions of these projections are equal, i.e. for all $t\\in\\mathbb{R}^{k}$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}_{Z\\sim P_{\\pmb{\\theta}^{T}\\mathrm{X}}}[\\boldsymbol{e}^{t^{T}Z}]=\\mathbb{E}_{Z\\sim Q_{\\pmb{\\theta}^{T}\\mathrm{X}}}[\\boldsymbol{e}^{t^{T}Z}]\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "i.e. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}_{X\\sim P_{\\mathrm{X}}}[e^{t^{T}\\pmb{\\theta}^{T}\\mathrm{X}}]=\\mathbb{E}_{X\\sim Q_{\\mathrm{X}}}[e^{t^{T}\\pmb{\\theta}^{T}\\mathrm{X}}].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Since $\\pmb{\\theta}t$ is dense in $\\mathbb{R}^{d}$ when $\\pmb{\\theta}\\in\\mathbb{S}_{k}(\\mathbb{R}^{d})$ and $t\\in\\mathbb{R}^{k}$ , we have that for any $s\\in\\mathbb{R}^{d}$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}_{X\\sim P_{\\mathrm{X}}}[e^{s^{T}\\mathrm{X}}]=\\mathbb{E}_{X\\sim Q_{\\mathrm{X}}}[e^{s^{T}\\mathrm{X}}].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "But these are just the moment generating functions of $P_{\\mathrm{{X}}}$ and $Q_{\\mathrm{X}}$ . Since the moment generating functions exist and are equal for all $s$ , the densities $P_{\\mathrm{X}},Q_{\\mathrm{X}}$ must be equal. ", "page_idx": 18}, {"type": "text", "text": "Proof of Corollary $^{\\,l}$ . By the assumption that $\\hat{\\mathrm{D}}_{f}$ is a consistent estimator of the $f$ -divergence, $b\\rightarrow\\infty$ with $\\sigma$ constant implies that for any fixed $\\omega$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n,b\\to\\infty}L(\\omega)=\\frac{1}{m}\\sum_{s=1}^{m}\\mathsf{D}_{f}\\left(P_{\\pmb{\\theta}_{s}^{T}G_{\\omega}(Z)+\\mathrm{N}}\\|P_{\\pmb{\\theta}_{s}^{T}X+\\mathrm{N}}\\right),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\mathbf{N}\\sim\\mathcal{N}(\\mathbf{0},\\sigma^{2}\\mathbf{I}_{k})$ . ", "page_idx": 18}, {"type": "text", "text": "In the first case, we assume $G_{\\omega^{*}}(Z)\\sim P_{X}$ . Substituting $\\omega=\\omega^{*}$ into (9) yields ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n,b\\to\\infty}L(\\omega^{*})=\\frac{1}{m}\\sum_{s=1}^{m}\\mathsf{D}_{f}\\left(P_{\\pmb{\\theta}_{s}^{T}X+\\mathsf{N}}\\|P_{\\pmb{\\theta}_{s}^{T}X+\\mathsf{N}}\\right)=0,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where equality with zero follows because the $f$ -divergence nullifies if its arguments are the same distribution. ", "page_idx": 18}, {"type": "text", "text": "For the second case, we assume that for some $\\omega^{\\ast}\\in\\Omega$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{m,n,b\\to\\infty}L(\\omega^{*})=\\operatorname*{lim}_{m\\to\\infty}\\frac{1}{m}\\sum_{s=1}^{m}\\mathsf{D}_{f}\\left(P_{\\theta_{s}^{T}G_{\\omega^{*}}(Z)+\\mathrm{N}}\\|P_{\\theta_{s}^{T}X+\\mathrm{N}}\\right)=0.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Since the $\\mathrm{D}_{f}\\left(P_{\\pmb{\\theta}_{s}^{T}G_{\\omega^{*}}(Z)+\\mathbf{N}}\\right)\\rvert\\lvert P_{\\pmb{\\theta}_{s}^{T}X+\\mathbf{N}}\\rvert$ terms are $\\mathrm{all}\\geq0$ , the fact that this limit exists implies that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{lim}_{m,n,b\\rightarrow\\infty}L(\\omega^{*})=\\frac{1}{\\mathrm{vol}(\\mathrm{S}_{\\mathrm{k}}(\\mathbb{R}^{\\mathrm{d}}))}\\int_{\\theta\\in\\mathbb{S}_{k}(\\mathbb{R}^{d})}\\mathrm{D}_{f}\\big(P_{\\theta^{T}G_{\\omega^{*}}(Z)+\\mathrm{N}}\\|P_{\\theta^{T}G_{\\omega^{*}}(Z)+\\mathrm{N}}\\big)\\mathrm{d}\\theta}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad=\\mathrm{SD}_{f,k,\\sigma^{2}}\\big(P_{G_{\\omega^{*}}(Z)}\\|P_{\\mathrm{X}}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "By Proposition 1, this can equal zero only if $P_{G_{\\omega^{*}}(Z)}=P_{\\mathrm{X}}$ . ", "page_idx": 18}, {"type": "table", "img_path": "3mCr7ZNdSw/tmp/9684a5cc2837bb497ecc6d9bd47e36ca23325602dcf9eb0176bb3f8adc2e4845.jpg", "table_caption": [], "table_footnote": ["Table 2: Dataset details. In addition to the described columns, each dataset has a binary outcome column, which is used by the evaluation metric LogitRegression. "], "page_idx": 19}, {"type": "text", "text": "C Details on the Experimental Results ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Datasets We demonstrate our method and baselines using the US Census data derived from the American Community Survey (ACS) Public Use Microdata Sample (PUMS). Using the API of the Folktables package [DHMS21], we access the 2018 California data. Additionally, Folktables package provides five prediction tasks (Income , Coverage, Mobility, Employment, TravelTime) based on a target column and a set of mixed-type features. We provide more details about these datasets in Table 2. ", "page_idx": 19}, {"type": "text", "text": "Data pre-processing. We pre-process each dataset using the open-source Python library [Sma23]. It normalizes numerical columns and one-hot encodes categorical columns, all within a privacypreserving way using an $\\epsilon=0.5$ . Note that the pre-processing budget is not included in the stated $\\epsilon$ in our results. To satisfy the bounded norm assumption in our privacy analysis, we normalize the resulting dataset so that the la\u221argest row 2-norm is upper bounded by 1. This can be achieved by e.g., normalizing each row by $1/\\sqrt{d}$ , where $d$ is the number of columns. We subsample each dataset at a rate of 0.25 using the Poisson mechanism, which amplifies the DP guarantees according to Lemma 1. For all our experiments, we report the amplified $\\epsilon$ . ", "page_idx": 19}, {"type": "text", "text": "More details. For our method and SliceWass, all experiments used batch size of 128 and learning rate $2\\cdot10^{-5}$ , and ran for 200 epochs. For our method and baselines, each model was trained using a V100 GPU, with runtimes typically less than 2 hours for our method (200 epochs). ", "page_idx": 19}, {"type": "text", "text": "Additional results. We repeat our experiments with a different privacy budget $\\epsilon=8.1$ and report the results in Table 3. ", "page_idx": 19}, {"type": "table", "img_path": "3mCr7ZNdSw/tmp/f6b00b42ec86059ec8447bc1448fc47693108a0155b3998547ebff239d26fad5.jpg", "table_caption": ["Table 3: We repeat the experiment in Table 1 with a different privacy budget $\\epsilon=8.1$ . "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The main claims in the abstract and introduction are consistent with the rest of the paper. They are supported by both theorems and experiments. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 21}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: Please refer to Section 5. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 21}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Please refer to Appendix A and B. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 22}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: We provide experimental details in Section 4 and Appendix C for reproducibility. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 22}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The datasets used in this paper are all publicly available. We also provide details in Section 4 and Appendix C for reproducing our experimental results. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 23}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Please refer to Section 4 and Appendix C. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 23}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We report error bars for our experimental results. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We report the compute resources in Section 4 and Appendix C. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 24}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: All research results presented in this paper conform with the NeurIPS Code of Ethics. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 24}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Please refer to Section 5. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 25}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We provide references to the code, data, and models used in this paper (see Section 4 and Appendix C). Their licenses can be located within the references. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 25}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 26}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We provide details and discuss limitations about our algorithm in the paper. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 26}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 26}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 26}]