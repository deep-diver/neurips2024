[{"figure_path": "Es2Ey2tGmM/tables/tables_28_1.jpg", "caption": "Table 1: Parameters of U-net Model used as noise predictor", "description": "This table lists the hyperparameters of the U-Net model used as the noise predictor in the diffusion model.  It specifies the number of ResNet layers per U-Net block, the number of ResNet down/upsampling blocks, and the number of output channels for each U-Net block.", "section": "4.3 Practical dual training algorithm"}, {"figure_path": "Es2Ey2tGmM/tables/tables_29_1.jpg", "caption": "Table 2: Hyperparameter values used in the main experiments. MC denotes Minority Class experiments and FT denotes Fine-Tuning experiments. - denotes that resilience was not used for the experiment.", "description": "This table lists the hyperparameter settings used in the main experiments of the paper.  It includes details for Minority Class (MC) and Fine-Tuning (FT) experiments on MNIST, CelebA, and ImageNet datasets. The hyperparameters shown are: the number of training epochs, the number of primal steps per dual step, primal and dual batch sizes, primal and dual learning rates, resilience relaxation cost, the size of the main dataset, and the size of the constraint dataset(s). The '-' indicates that resilience was not used in that particular experiment.", "section": "Computational experiments"}, {"figure_path": "Es2Ey2tGmM/tables/tables_30_1.jpg", "caption": "Table 3: Constrained model trained on MNIST with different Primal/Dual Batch sizes", "description": "This table shows the FID scores achieved by the constrained diffusion model trained on MNIST dataset using different combinations of primal and dual batch sizes.  The results illustrate the impact of varying these batch size parameters on the model's performance, as measured by the FID score.", "section": "Computational experiments"}]