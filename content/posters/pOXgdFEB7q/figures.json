[{"figure_path": "pOXgdFEB7q/figures/figures_1_1.jpg", "caption": "Figure 1: The tunnel effect. The tunnel impedes OOD generalization, which we study using linear probes trained on ID and OOD datasets for each layer. In this example, identical VGGm-17 architectures are trained on identical ID datasets, where only the resolution is changed. Probe accuracy on OOD datasets decreases once the tunnel is reached (denoted by \u2729), where the model trained on low-resolution (32 \u00d7 32) images creates a longer tunnel (layers 9-16) than the one (layers 13-16) trained on higher-resolution (224 \u00d7 224) images. The Y-axis shows the normalized accuracy. The OOD curve is the average of 8 OOD datasets (Sec. 3.3), with the standard deviation denoted with shading.", "description": "This figure shows the results of an experiment designed to test the tunnel effect hypothesis.  Two identical VGGm-17 neural networks were trained on the same in-distribution (ID) dataset, but one with low-resolution (32x32) images and the other with high-resolution (224x224) images.  Linear probes were trained on the embeddings from each layer of the networks to assess their performance on both ID and out-of-distribution (OOD) datasets. The graph shows that ID accuracy increased monotonically for both models, as expected.  However, OOD accuracy showed a different trend. For the low-resolution model, OOD accuracy initially increased, but then sharply dropped after layer 9, indicating the presence of a long 'tunnel'. The high-resolution model showed a similar trend but with a much shorter tunnel (layers 13-16), demonstrating that higher resolution training data mitigates the tunnel effect and improves the model's ability to generalize to OOD data. The shaded area represents the standard deviation across eight different OOD datasets, and the star symbols (\u2729) highlight where the tunnel effect begins.", "section": "2 Related Work"}, {"figure_path": "pOXgdFEB7q/figures/figures_4_1.jpg", "caption": "Figure 2: SHAP Results. SHAP slope shows the individual contribution of variables to various targets. Positive values indicate enhanced OOD generalization, and vice-versa for negative values.", "description": "This figure displays the SHAP (SHapley Additive exPlanations) values for various factors influencing out-of-distribution (OOD) generalization.  SHAP analysis is used to show the relative importance of each variable. The left chart shows the impact of each variable on the percentage of OOD performance retained. The right chart shows the impact of the variables on the ID/OOD alignment (a metric reflecting how well the model's performance on in-distribution and out-of-distribution data align).  Positive SHAP values for a variable suggest it improves OOD generalization or ID/OOD alignment, while negative values indicate it harms it. The magnitude of the SHAP value indicates the strength of the effect.", "section": "4 Experiments & Results"}, {"figure_path": "pOXgdFEB7q/figures/figures_5_1.jpg", "caption": "Figure 3: Augmentation greatly reduces the tunnel effect. In (a), augmentation shifts the tunnel from layer 14 to 22, and in (b) from block 11 to 15. The OOD curve is the average of 8 OOD datasets with a shaded area indicating a 95% confidence interval. \u2606 denotes the start of the tunnel.", "description": "This figure shows how data augmentation affects the tunnel effect.  Two plots are presented, one for a ResNet-34 model trained on 32x32 images, and another for a ViT-T+ model trained on 224x224 images. In both cases, the plots show normalized accuracy curves for in-distribution (ID) and out-of-distribution (OOD) data.  The plots demonstrate that augmentation reduces the length of the \"tunnel\" (the region where OOD accuracy decreases rapidly) in both ResNet and ViT architectures.  The shaded areas represent the 95% confidence intervals for the OOD curves.  The star symbol indicates where the tunnel effect starts.", "section": "4.1.2 Augmentation Results"}, {"figure_path": "pOXgdFEB7q/figures/figures_6_1.jpg", "caption": "Figure 4: High-resolution model does not exhibit representation compression. The t-SNE comparison between VGGm-11 models trained on low- (1st row) and high-resolution (2nd row) images of the same ID dataset (ImageNet-100) in an augmentation-free setting. Layer 8 marks the start of the tunnel in VGGm-11 trained on 32 \u00d7 32 images whereas 224 \u00d7 224 resolution does not create any tunnel. Layer 10 is the penultimate layer. The tunnel layers (layers 8-10) progressively compress representations for 32\u00d732 resolution whereas corresponding layers for 224\u00d7224 resolution do not exhibit similar compression. For clarity, we show 5 classes from ImageNet-100 and indicate each class by a distinct color. The formation of distinct clusters in the 32 \u00d7 32 model is indicative of representation compression and intermediate neural collapse [10], which impairs OOD generalization.", "description": "This figure shows t-SNE visualizations comparing the representations learned by a VGGm-11 model trained on low-resolution (32x32) and high-resolution (224x224) images from the ImageNet-100 dataset.  The low-resolution model shows clear representation compression (clusters) in layers 8-10 (the \"tunnel\"), consistent with the tunnel effect hypothesis and leading to impaired out-of-distribution (OOD) generalization. In contrast, the high-resolution model lacks this compression, suggesting that higher resolution training data mitigates the tunnel effect.", "section": "4.1.2 Augmentation Results"}, {"figure_path": "pOXgdFEB7q/figures/figures_7_1.jpg", "caption": "Figure 3: Augmentation greatly reduces the tunnel effect. In (a), augmentation shifts the tunnel from layer 14 to 22, and in (b) from block 11 to 15. The OOD curve is the average of 8 OOD datasets with a shaded area indicating a 95% confidence interval.  denotes the start of the tunnel.", "description": "This figure shows the impact of data augmentation on the tunnel effect.  Two examples are shown: one using a ResNet-34 model with 32x32 images and another using a ViT-T+ model with 224x224 images.  Both models are trained on an in-distribution (ID) dataset, and the results show the normalized accuracy of linear probes on ID and out-of-distribution (OOD) datasets for each layer.  The results clearly indicate that data augmentation significantly reduces the length of the tunnel and improves out-of-distribution generalization.", "section": "4.1.2 Augmentation Results"}, {"figure_path": "pOXgdFEB7q/figures/figures_8_1.jpg", "caption": "Figure 3: Augmentation greatly reduces the tunnel effect. In (a), augmentation shifts the tunnel from layer 14 to 22, and in (b) from block 11 to 15. The OOD curve is the average of 8 OOD datasets with a shaded area indicating a 95% confidence interval. \u2606 denotes the start of the tunnel.", "description": "This figure shows the impact of data augmentation on the tunnel effect.  Two examples are given: a ResNet-34 model trained on 32x32 images and a ViT-T+ model trained on 224x224 images.  In both cases, the addition of augmentations significantly reduces the length of the tunnel, improving out-of-distribution (OOD) generalization. The plots show the normalized accuracy of linear probes trained on in-distribution (ID) and OOD datasets for each layer of the models, with and without augmentations.", "section": "4.1.2 Augmentation Results"}, {"figure_path": "pOXgdFEB7q/figures/figures_20_1.jpg", "caption": "Figure 2: SHAP Results. SHAP slope shows the individual contribution of variables to various targets. Positive values indicate enhanced OOD generalization, and vice-versa for negative values.", "description": "This figure displays the SHAP (SHapley Additive exPlanations) values, showing the impact of each variable on two different target metrics: percentage of OOD (out-of-distribution) performance retained and ID/OOD alignment.  Positive SHAP values mean that a variable increases OOD generalization or ID/OOD alignment, while negative values indicate the opposite. The magnitude of the SHAP value shows the importance of the variable. This helps determine which variables most influence a model's ability to generalize to unseen data.", "section": "3.4 Statistical Analysis"}, {"figure_path": "pOXgdFEB7q/figures/figures_22_1.jpg", "caption": "Figure 2: SHAP Results. SHAP slope shows the individual contribution of variables to various targets. Positive values indicate enhanced OOD generalization, and vice-versa for negative values.", "description": "This figure presents the SHAP (SHapley Additive exPlanations) analysis results, illustrating the individual contributions of different variables to the overall OOD (out-of-distribution) generalization performance.  Two target metrics were analyzed:  % OOD performance retained (left) and ID/OOD alignment (right).  Each bar represents a variable (e.g., ID class count, augmentations, resolution, etc.), and the bar's length indicates the magnitude of the variable's impact on each target metric. Positive SHAP values suggest that increasing the variable's value improves OOD generalization, while negative values indicate the opposite.  The analysis disentangles the relative importance of different factors such as the dataset\u2019s size and diversity, augmentation strategies, and the model's architecture in improving out-of-distribution generalisation.", "section": "4 Experiments & Results"}, {"figure_path": "pOXgdFEB7q/figures/figures_27_1.jpg", "caption": "Figure 3: Augmentation greatly reduces the tunnel effect. In (a), augmentation shifts the tunnel from layer 14 to 22, and in (b) from block 11 to 15. The OOD curve is the average of 8 OOD datasets with a shaded area indicating a 95% confidence interval.  denotes the start of the tunnel.", "description": "This figure shows the impact of data augmentation on the tunnel effect.  Two different model architectures, ResNet-34 and ViT-T+, are trained on the same dataset with and without data augmentation. The results show that data augmentation significantly reduces the tunnel effect, shifting the point at which OOD accuracy begins to decrease to a later layer in the network. This improvement is observed in both model architectures, suggesting that data augmentation is a generalizable technique for mitigating the tunnel effect and improving OOD generalization.", "section": "4.1.2 Augmentation Results"}, {"figure_path": "pOXgdFEB7q/figures/figures_28_1.jpg", "caption": "Figure 3: Augmentation greatly reduces the tunnel effect. In (a), augmentation shifts the tunnel from layer 14 to 22, and in (b) from block 11 to 15. The OOD curve is the average of 8 OOD datasets with a shaded area indicating a 95% confidence interval. \u2606 denotes the start of the tunnel.", "description": "This figure shows how data augmentation impacts the tunnel effect.  Two models (ResNet-34 and ViT-T+) are trained with and without augmentations on the ImageNet-100 dataset.  Linear probes trained on ID and OOD datasets are used to measure accuracy at each layer. The shaded areas show the 95% confidence intervals around the mean OOD accuracies. The figures demonstrate that with augmentation, the drop in OOD accuracy (indicating the tunnel effect) is delayed to a later layer or block, showcasing the effectiveness of augmentation in mitigating the tunnel effect and improving out-of-distribution generalization.", "section": "4.1.2 Augmentation Results"}, {"figure_path": "pOXgdFEB7q/figures/figures_28_2.jpg", "caption": "Figure 11: Over-parameterization level. This figure exhibits % OOD performance retained computed across over-parameterization levels. This is based on models trained on ImagerNet-100 at resolutions 32 \u00d7 32 and 224 \u00d7 224. Increasing the over-parameterization level reduces the % OOD performance retained and intensifies the tunnel effect.", "description": "This boxplot shows the relationship between the overparameterization level of the model and the average percentage of out-of-distribution (OOD) performance retained across various OOD datasets.  The results suggest that as overparameterization increases, the percentage of OOD performance retained decreases, indicating a stronger tunnel effect.  The data comes from models trained on ImageNet-100 at both 32x32 and 224x224 resolutions.", "section": "4.1.2 Augmentation Results"}, {"figure_path": "pOXgdFEB7q/figures/figures_29_1.jpg", "caption": "Figure 12: Stem. This figure exhibits % OOD performance retained and ID/OOD alignment computed across stems and averaged across OOD datasets. This is based on models trained on ImagerNet-100 (ID) with 32 \u00d7 32 and 224 \u00d7 224 resolutions. When comparing CNNs (VGGm's 3 \u00d7 3 and ResNet's 7 \u00d7 7), increasing stem exhibits a negative impact on % OOD performance retained. Across all models, increasing stem negatively impacts ID/OOD alignment.", "description": "This figure shows the impact of the stem size on OOD generalization performance.  It compares three different stem sizes (3, 7, and 8) across two image resolutions (32x32 and 224x224) and shows that increasing the stem size negatively affects both the percentage of OOD performance retained and the ID/OOD alignment.", "section": "4.1.4 DNN Architecture Results"}, {"figure_path": "pOXgdFEB7q/figures/figures_29_2.jpg", "caption": "Figure 12: Stem. This figure exhibits % OOD performance retained and ID/OOD alignment computed across stems and averaged across OOD datasets. This is based on models trained on ImagerNet-100 (ID) with 32 \u00d7 32 and 224 \u00d7 224 resolutions. When comparing CNNs (VGGm's 3 \u00d7 3 and ResNet's 7 \u00d7 7), increasing stem exhibits a negative impact on % OOD performance retained. Across all models, increasing stem negatively impacts ID/OOD alignment.", "description": "This figure shows the impact of stem size on the tunnel effect. The box plots present the average percentage of OOD performance retained and ID/OOD alignment across different stem sizes (3x3, 7x7, and 8x8) for models trained on ImageNet-100 with both 32x32 and 224x224 resolutions.  Increasing the stem size negatively affects both metrics, indicating a stronger tunnel effect and less alignment between in-distribution and out-of-distribution performance.", "section": "4.1.4 DNN Architecture Results"}, {"figure_path": "pOXgdFEB7q/figures/figures_29_3.jpg", "caption": "Figure 3: Augmentation greatly reduces the tunnel effect. In (a), augmentation shifts the tunnel from layer 14 to 22, and in (b) from block 11 to 15. The OOD curve is the average of 8 OOD datasets with a shaded area indicating a 95% confidence interval. \u2729 denotes the start of the tunnel.", "description": "This figure shows how data augmentation affects the tunnel effect by comparing the linear probe accuracy curves for models trained with and without augmentations.  The left panel shows the results for a ResNet-34 model trained on 32x32 images, while the right panel shows the results for a ViT-T+ model trained on 224x224 images. In both cases, the addition of augmentations significantly reduces the length of the 'tunnel' region where out-of-distribution (OOD) accuracy begins to decrease sharply. This indicates that data augmentation mitigates the negative effect of the tunnel on the model's ability to generalize to OOD data.", "section": "4.1.2 Augmentation Results"}, {"figure_path": "pOXgdFEB7q/figures/figures_30_1.jpg", "caption": "Figure 1: The tunnel effect. The tunnel impedes OOD generalization, which we study using linear probes trained on ID and OOD datasets for each layer. In this example, identical VGGm-17 architectures are trained on identical ID datasets, where only the resolution is changed. Probe accuracy on OOD datasets decreases once the tunnel is reached (denoted by \u2606), where the model trained on low-resolution (32 \u00d7 32) images creates a longer tunnel (layers 9-16) than the one (layers 13-16) trained on higher-resolution (224 \u00d7 224) images. The Y-axis shows the normalized accuracy. The OOD curve is the average of 8 OOD datasets (Sec. 3.3), with the standard deviation denoted with shading.", "description": "This figure shows the effect of image resolution on the tunnel effect. Two VGG17 models are trained with the same ImageNet-100 dataset, but one with 32x32 resolution images and the other with 224x224 resolution images.  Linear probes are trained and evaluated on both in-distribution (ID) and out-of-distribution (OOD) datasets for each layer of the models. The plot demonstrates that the model trained on lower resolution images exhibits a longer tunnel, where OOD accuracy drops significantly after a certain layer, while the model trained on higher resolution images has a shorter tunnel and better OOD generalization.", "section": "2 Related Work"}, {"figure_path": "pOXgdFEB7q/figures/figures_30_2.jpg", "caption": "Figure 15: Class Count vs. Data Quantity. The figure shows the trend that models trained with more classes and more samples have better OOD accuracy (%). We use C and S to denote the number of classes and number of samples per class respectively, in the X-axis of the figure.", "description": "This figure shows the impact of the number of classes and the number of training samples per class on the average out-of-distribution (OOD) accuracy.  It demonstrates that increasing the number of classes significantly improves OOD accuracy, while increasing the number of samples per class has a less pronounced effect.", "section": "C Results & Insights"}, {"figure_path": "pOXgdFEB7q/figures/figures_31_1.jpg", "caption": "Figure 1: The tunnel effect. The tunnel impedes OOD generalization, which we study using linear probes trained on ID and OOD datasets for each layer. In this example, identical VGGm-17 architectures are trained on identical ID datasets, where only the resolution is changed. Probe accuracy on OOD datasets decreases once the tunnel is reached (denoted by \u2729), where the model trained on low-resolution (32 \u00d7 32) images creates a longer tunnel (layers 9-16) than the one (layers 13-16) trained on higher-resolution (224 \u00d7 224) images. The Y-axis shows the normalized accuracy. The OOD curve is the average of 8 OOD datasets (Sec. 3.3), with the standard deviation denoted with shading.", "description": "This figure shows the tunnel effect in two VGGm-17 models trained with different image resolutions (32x32 and 224x224).  It demonstrates how the accuracy of linear probes trained on out-of-distribution (OOD) datasets decreases sharply after a certain layer (the 'tunnel'), especially for the low-resolution model. The high-resolution model has a shorter and less pronounced tunnel. The graph highlights that higher-resolution images mitigate the tunnel effect, improving out-of-distribution generalization.", "section": "2 Related Work"}, {"figure_path": "pOXgdFEB7q/figures/figures_31_2.jpg", "caption": "Figure 3: Augmentation greatly reduces the tunnel effect. In (a), augmentation shifts the tunnel from layer 14 to 22, and in (b) from block 11 to 15. The OOD curve is the average of 8 OOD datasets with a shaded area indicating a 95% confidence interval. \u2729 denotes the start of the tunnel.", "description": "This figure shows the effect of using data augmentation on the tunnel effect.  Two examples are shown; (a) shows the results for ResNet-34 trained on 32x32 images, and (b) shows the results for ViT-T+ trained on 224x224 images.  The plots show the normalized accuracy of linear probes trained on in-distribution (ID) and out-of-distribution (OOD) datasets for embeddings from each layer of the model.  The shaded region represents the 95% confidence interval of the OOD accuracy.  Data augmentation pushes back the onset of the tunnel effect. The start of the tunnel is marked by a star symbol.", "section": "4.1.2 Augmentation Results"}, {"figure_path": "pOXgdFEB7q/figures/figures_32_1.jpg", "caption": "Figure 18: Summary plots for large pre-trained models. The left figure exhibits % OOD performance retained computed across training methods (SL and SSL). The right figure displays % OOD performance retained computed across distinct architectures. SSL models show higher % OOD performance retained than SL models.", "description": "This figure compares the percentage of out-of-distribution (OOD) performance retained for self-supervised learning (SSL) and supervised learning (SL) methods across different model architectures. The left plot shows a direct comparison between SSL and SL, while the right plot shows a breakdown of performance for different architectures within each learning paradigm.  The key observation is that SSL models generally retain a higher percentage of OOD performance than their SL counterparts.", "section": "4.2 Analysis of Widely Used Pre-trained Backbones"}, {"figure_path": "pOXgdFEB7q/figures/figures_32_2.jpg", "caption": "Figure 3: Augmentation greatly reduces the tunnel effect. In (a), augmentation shifts the tunnel from layer 14 to 22, and in (b) from block 11 to 15. The OOD curve is the average of 8 OOD datasets with a shaded area indicating a 95% confidence interval. \u2606 denotes the start of the tunnel.", "description": "This figure shows how data augmentation affects the tunnel effect.  Two examples are shown: a ResNet-34 model trained on 32x32 images (a) and a ViT-T+ model trained on 224x224 images (b).  In both cases, the addition of augmentation significantly reduces the length of the tunnel, that is the range of layers where the out-of-distribution (OOD) accuracy decreases.  The plots show the ID and OOD accuracy of linear probes trained on embeddings from different layers.  The shaded region represents the 95% confidence interval for the OOD curves.", "section": "4.1.2 Augmentation Results"}, {"figure_path": "pOXgdFEB7q/figures/figures_32_3.jpg", "caption": "Figure 3: Augmentation greatly reduces the tunnel effect. In (a), augmentation shifts the tunnel from layer 14 to 22, and in (b) from block 11 to 15. The OOD curve is the average of 8 OOD datasets with a shaded area indicating a 95% confidence interval.  denotes the start of the tunnel.", "description": "This figure shows how data augmentation impacts the tunnel effect. Two models, ResNet-34 and ViT-T+, are trained with and without augmentations on ImageNet-100. The tunnel effect, which is characterized by a decrease in out-of-distribution (OOD) accuracy while in-distribution (ID) accuracy continues to improve, is observed in both models trained without augmentation. However, when augmentation is used, the tunnel effect is reduced, and OOD accuracy drops less sharply after the extractor.", "section": "4.1.2 Augmentation Results"}, {"figure_path": "pOXgdFEB7q/figures/figures_32_4.jpg", "caption": "Figure 3: Augmentation greatly reduces the tunnel effect. In (a), augmentation shifts the tunnel from layer 14 to 22, and in (b) from block 11 to 15. The OOD curve is the average of 8 OOD datasets with a shaded area indicating a 95% confidence interval. \u2729 denotes the start of the tunnel.", "description": "This figure shows the effect of augmentation on the tunnel effect for two different architectures: ResNet-34 and ViT-T+.  It demonstrates that data augmentation significantly reduces the length of the tunnel, thereby improving out-of-distribution (OOD) generalization. The plots display the ID and OOD accuracy of linear probes applied to different layers of the networks, trained with and without augmentation. The starting point of the tunnel is marked with a star. The shaded region represents the 95% confidence interval.", "section": "4.1.2 Augmentation Results"}, {"figure_path": "pOXgdFEB7q/figures/figures_33_1.jpg", "caption": "Figure 1: The tunnel effect. The tunnel impedes OOD generalization, which we study using linear probes trained on ID and OOD datasets for each layer. In this example, identical VGGm-17 architectures are trained on identical ID datasets, where only the resolution is changed. Probe accuracy on OOD datasets decreases once the tunnel is reached (denoted by \u2729), where the model trained on low-resolution (32 \u00d7 32) images creates a longer tunnel (layers 9-16) than the one (layers 13-16) trained on higher-resolution (224 \u00d7 224) images. The Y-axis shows the normalized accuracy. The OOD curve is the average of 8 OOD datasets (Sec. 3.3), with the standard deviation denoted with shading.", "description": "This figure shows the impact of image resolution on the tunnel effect. Two VGGm-17 models were trained on the same dataset, with the only difference being the image resolution (32x32 and 224x224). Linear probes were trained and evaluated on both in-distribution (ID) and out-of-distribution (OOD) datasets for each layer of the models. The results demonstrate that increasing the image resolution leads to a shorter tunnel, reducing the negative impact on OOD generalization.", "section": "2 Related Work"}, {"figure_path": "pOXgdFEB7q/figures/figures_33_2.jpg", "caption": "Figure 1: The tunnel effect. The tunnel impedes OOD generalization, which we study using linear probes trained on ID and OOD datasets for each layer. In this example, identical VGGm-17 architectures are trained on identical ID datasets, where only the resolution is changed. Probe accuracy on OOD datasets decreases once the tunnel is reached (denoted by \u2729), where the model trained on low-resolution (32 \u00d7 32) images creates a longer tunnel (layers 9-16) than the one (layers 13-16) trained on higher-resolution (224 \u00d7 224) images. The Y-axis shows the normalized accuracy. The OOD curve is the average of 8 OOD datasets (Sec. 3.3), with the standard deviation denoted with shading.", "description": "This figure shows how the \"tunnel effect\" impacts out-of-distribution (OOD) generalization.  Two VGGm-17 models are trained identically, except one uses low-resolution (32x32) images and the other uses high-resolution (224x224) images. Linear probes are used to evaluate the accuracy of embeddings from each layer on both in-distribution (ID) and OOD datasets.  The plot demonstrates that ID accuracy increases monotonically with layer depth, whereas OOD accuracy decreases sharply after a certain layer (the beginning of the 'tunnel').  The high-resolution model shows a much shorter 'tunnel' and better OOD generalization than the low-resolution model, illustrating that higher resolution mitigates the tunnel effect. The shaded area shows the standard deviation across multiple OOD datasets.", "section": "2 Related Work"}, {"figure_path": "pOXgdFEB7q/figures/figures_34_1.jpg", "caption": "Figure 3: Augmentation greatly reduces the tunnel effect. In (a), augmentation shifts the tunnel from layer 14 to 22, and in (b) from block 11 to 15. The OOD curve is the average of 8 OOD datasets with a shaded area indicating a 95% confidence interval. \u2606 denotes the start of the tunnel.", "description": "This figure shows the impact of data augmentation on the tunnel effect in two different model architectures, ResNet-34 and ViT-T+.  In both cases, adding augmentation significantly reduces the length of the tunnel (the section of the network where out-of-distribution accuracy decreases).  The plots show the accuracy of linear probes trained on in-distribution (ID) and out-of-distribution (OOD) data at each layer of the network.  The shaded region represents the 95% confidence interval of the OOD accuracy.", "section": "4.1.2 Augmentation Results"}, {"figure_path": "pOXgdFEB7q/figures/figures_34_2.jpg", "caption": "Figure 3: Augmentation greatly reduces the tunnel effect. In (a), augmentation shifts the tunnel from layer 14 to 22, and in (b) from block 11 to 15. The OOD curve is the average of 8 OOD datasets with a shaded area indicating a 95% confidence interval. \u2729 denotes the start of the tunnel.", "description": "This figure shows how data augmentation affects the tunnel effect.  Two example models (ResNet-34 and ViT-T+) are shown, trained with and without augmentation.  The plots show the accuracy of linear probes trained on both in-distribution (ID) and out-of-distribution (OOD) data at each layer of the network. In both models, augmentation reduces the length of the tunnel, meaning that the detrimental effect of deeper layers on out-of-distribution generalization is less significant when augmentation is used during training.", "section": "4.1.2 Augmentation Results"}, {"figure_path": "pOXgdFEB7q/figures/figures_35_1.jpg", "caption": "Figure 1: The tunnel effect. The tunnel impedes OOD generalization, which we study using linear probes trained on ID and OOD datasets for each layer. In this example, identical VGGm-17 architectures are trained on identical ID datasets, where only the resolution is changed. Probe accuracy on OOD datasets decreases once the tunnel is reached (denoted by \u2729), where the model trained on low-resolution (32 \u00d7 32) images creates a longer tunnel (layers 9-16) than the one (layers 13-16) trained on higher-resolution (224 \u00d7 224) images. The Y-axis shows the normalized accuracy. The OOD curve is the average of 8 OOD datasets (Sec. 3.3), with the standard deviation denoted with shading.", "description": "This figure demonstrates the tunnel effect, where the out-of-distribution (OOD) accuracy of a model decreases after a certain layer, while in-distribution (ID) accuracy increases monotonically. Two identical VGGm-17 networks are trained on the same dataset, but with different resolutions (32x32 and 224x224). The higher-resolution model shows a shorter tunnel (layers where OOD accuracy drops), indicating that higher resolution data mitigates the tunnel effect.", "section": "2 Related Work"}, {"figure_path": "pOXgdFEB7q/figures/figures_35_2.jpg", "caption": "Figure 1: The tunnel effect. The tunnel impedes OOD generalization, which we study using linear probes trained on ID and OOD datasets for each layer. In this example, identical VGGm-17 architectures are trained on identical ID datasets, where only the resolution is changed. Probe accuracy on OOD datasets decreases once the tunnel is reached (denoted by \u2729), where the model trained on low-resolution (32 \u00d7 32) images creates a longer tunnel (layers 9-16) than the one (layers 13-16) trained on higher-resolution (224 \u00d7 224) images. The Y-axis shows the normalized accuracy. The OOD curve is the average of 8 OOD datasets (Sec. 3.3), with the standard deviation denoted with shading.", "description": "This figure shows the impact of image resolution on the tunnel effect. Two identical VGGm-17 models are trained on the same in-distribution (ID) dataset, one with 32x32 resolution images and the other with 224x224 resolution images. Linear probes are used to evaluate the accuracy of embeddings from each layer on both ID and out-of-distribution (OOD) datasets. The results show that the model trained with low-resolution images (32x32) exhibits a longer tunnel effect (layers 9-16) than the model trained with high-resolution images (224x224) (layers 13-16). The tunnel effect impedes the OOD generalization.", "section": "2.1 The Tunnel Effect"}]