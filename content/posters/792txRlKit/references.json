{"references": [{"fullname_first_author": "Eugene Bagdasaryan", "paper_title": "How to backdoor federated learning", "publication_date": "2020-00-00", "reason": "This paper is foundational to the field of backdoor attacks in federated learning, introducing a key vulnerability that DataStealing builds upon."}, {"fullname_first_author": "Peva Blanchard", "paper_title": "Machine learning with adversaries: Byzantine tolerant gradient descent", "publication_date": "2017-00-00", "reason": "This paper is highly influential in the understanding of robust aggregation techniques against adversarial attacks in federated learning, which DataStealing seeks to circumvent."}, {"fullname_first_author": "Jonathan Ho", "paper_title": "Denoising diffusion probabilistic models", "publication_date": "2020-00-00", "reason": "This paper introduced the foundational denoising diffusion probabilistic models (DDPMs), the core technology that DataStealing exploits for its attacks."}, {"fullname_first_author": "Brendan McMahan", "paper_title": "Communication-efficient learning of deep networks from decentralized data", "publication_date": "2017-00-00", "reason": "This seminal paper introduced federated averaging (FedAvg), a widely-used federated learning algorithm, which is the target of DataStealing attacks."}, {"fullname_first_author": "Jonas Geiping", "paper_title": "Inverting gradients-how easy is it to break privacy in federated learning?", "publication_date": "2020-00-00", "reason": "This paper details gradient inversion attacks, a crucial concept in understanding the privacy implications of federated learning that DataStealing further explores in the context of diffusion models."}]}