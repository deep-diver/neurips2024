[{"figure_path": "792txRlKit/figures/figures_1_1.jpg", "caption": "Figure 1: Overview of DataStealing. To steal data under strict management, the attacker can poison the global diffusion model in FL with multiple Trojans. After releasing the global model for users, the attacker extracts massive local data in high quality from outside with multiple triggers (ComboTs).", "description": "The figure illustrates the DataStealing attack.  Multiple Trojan models are embedded within a federated learning system training a diffusion model.  After the global model is released, the attacker uses Combinatorial Triggers (ComboTs) to extract high-quality private data from users who utilize the poisoned model.  The process occurs despite strict data management measures, highlighting the vulnerability of diffusion models to data theft in a federated learning setting.", "section": "3 DataStealing: Task and Algorithms"}, {"figure_path": "792txRlKit/figures/figures_2_1.jpg", "caption": "Figure 2: Overview of ComboTs and AdaSCP. (a) ComboTs choose two points from candidate positions to form multiple triggers for mapping target images. (b) The forward and backward Trojan diffusion process. After training with ComboTs, the poisoned model can restore the target images in high quality from Trojan noise, thereby enabling DataStealing. (c) AdaSCP achieves the purpose of DataStealing and defeats the advanced defenses by training critical parameters and adaptively scaling the updates before uploading.", "description": "This figure illustrates the ComboTs and AdaSCP methods.  (a) shows how ComboTs select multiple trigger points to map to target images. (b) details the Trojan diffusion process, showing how the poisoned model can reconstruct target images from the added noise. (c) depicts the AdaSCP attack, which adapts the scale of updates to bypass defenses.", "section": "3 DataStealing: Task and Algorithms"}, {"figure_path": "792txRlKit/figures/figures_5_1.jpg", "caption": "Figure 2: Overview of ComboTs and AdaSCP. (a) ComboTs choose two points from candidate positions to form multiple triggers for mapping target images. (b) The forward and backward Trojan diffusion process. After training with ComboTs, the poisoned model can restore the target images in high quality from Trojan noise, thereby enabling DataStealing. (c) AdaSCP achieves the purpose of DataStealing and defeats the advanced defenses by training critical parameters and adaptively scaling the updates before uploading.", "description": "This figure illustrates the proposed ComboTs and AdaSCP attack methods.  (a) shows how ComboTs selects multiple trigger points to map to target images for data exfiltration. (b) demonstrates the Trojan diffusion process, where the poisoned model uses ComboTs to reconstruct target images from Trojan noise.  Finally, (c) explains AdaSCP, which circumvents defenses by training on critical parameters and adaptively scaling updates before uploading.", "section": "3 DataStealing: Task and Algorithms"}, {"figure_path": "792txRlKit/figures/figures_6_1.jpg", "caption": "Figure 3: Qualitative Comparison. The generated image with the lowest MSE in one trigger-target pair is presented under every attack and defense method. More visual results are in Appendix A.10.", "description": "This figure shows a qualitative comparison of the generated images produced by different attack methods under various defense mechanisms.  The image with the lowest mean squared error (MSE) for each attack and defense combination is displayed. This visually demonstrates the effectiveness (or lack thereof) of each attack in generating realistic images while evading the defenses. More detailed visual results are available in Appendix A.10.", "section": "4 Experiments"}, {"figure_path": "792txRlKit/figures/figures_7_1.jpg", "caption": "Figure 4: Ablation Study on Trigger Number.", "description": "The figure shows the impact of the number of triggers used in the AdaSCP attack on the FID and MSE scores.  The results indicate a trade-off: using more triggers can lead to more successful data extraction (lower MSE) but comes at the cost of potentially harming the generative quality of the model (higher FID).  The experiment used the FedAvg and Multi-Krum defense mechanisms in a non-IID setting on the CIFAR10 dataset.", "section": "4.2 Ablation Study"}, {"figure_path": "792txRlKit/figures/figures_14_1.jpg", "caption": "Figure 5: Attacking in an Extended Training. Given more time, AdaSCP can attack successfully under the defense of Multi-Krum in the Non-IID distribution of CIFAR10.", "description": "This figure shows the results of an experiment designed to demonstrate that existing defense mechanisms struggle to prevent successful attacks by adversaries over extended training periods.  The experiment used the AdaSCP attack against the Multi-Krum defense on the CIFAR10 dataset with a non-IID data distribution.  The x-axis represents the training round, while the y-axis shows both FID (Frechet Inception Distance) and MSE (Mean Squared Error). The plot shows that while AdaSCP initially fails to succeed within 300 rounds, it becomes successful after extending the training duration to 1500 rounds.  This indicates that longer training times can make the AdaSCP attack more effective against Multi-Krum. The images embedded within the graph illustrate sample generated images at different points in the training process.", "section": "A.1 Attacking in Extended Training"}, {"figure_path": "792txRlKit/figures/figures_14_2.jpg", "caption": "Figure 5: Attacking in an Extended Training. Given more time, AdaSCP can attack successfully under the defense of Multi-Krum in the Non-IID distribution of CIFAR10.", "description": "This figure shows the results of an experiment designed to demonstrate that existing defense mechanisms struggle to prevent successful attacks by adversaries over extended training periods.  The experiment used the AdaSCP attack against the Multi-Krum defense mechanism on the CIFAR10 dataset with a non-IID data distribution.  The x-axis represents the training round, and the y-axis shows both FID (Frechet Inception Distance) and MSE (Mean Squared Error) values. The plot shows that AdaSCP attack initially fails within 300 rounds, but after extending the training to 1500 rounds, AdaSCP successfully defeats the Multi-Krum defense, indicating that longer training times can make the attack more stealthy and effective.", "section": "A.1 Attacking in Extended Training"}, {"figure_path": "792txRlKit/figures/figures_15_1.jpg", "caption": "Figure 7: Relative Distance. Ratio of malicious updates distance to mean benign updates distance when attacking with AdaSCP on CIFAR10 in the first 100 rounds.", "description": "This figure shows the relative distance between malicious updates and the mean distance of benign updates during the first 100 training rounds when using AdaSCP attack against Krum and Multi-Krum defense mechanisms on CIFAR10 dataset.  The graph illustrates how AdaSCP successfully reduces the distance of malicious updates to the mean benign updates over time, allowing it to evade the defenses.", "section": "A.3 Analysis on Defending against Malicious Updates"}, {"figure_path": "792txRlKit/figures/figures_18_1.jpg", "caption": "Figure 2: Overview of ComboTs and AdaSCP. (a) ComboTs choose two points from candidate positions to form multiple triggers for mapping target images. (b) The forward and backward Trojan diffusion process. After training with ComboTs, the poisoned model can restore the target images in high quality from Trojan noise, thereby enabling DataStealing. (c) AdaSCP achieves the purpose of DataStealing and defeats the advanced defenses by training critical parameters and adaptively scaling the updates before uploading.", "description": "This figure illustrates the ComboTs and AdaSCP methods.  ComboTs (a) selects multiple points to create triggers for embedding target images into a diffusion model. The Trojan diffusion process (b) adds noise to the original images and uses the triggers to embed the target images. The AdaSCP attack (c) adaptively scales the malicious updates based on gradient importance and evades distance-based defenses by making the malicious updates appear similar to benign updates.", "section": "3 DataStealing: Task and Algorithms"}, {"figure_path": "792txRlKit/figures/figures_19_1.jpg", "caption": "Figure 2: Overview of ComboTs and AdaSCP. (a) ComboTs choose two points from candidate positions to form multiple triggers for mapping target images. (b) The forward and backward Trojan diffusion process. After training with ComboTs, the poisoned model can restore the target images in high quality from Trojan noise, thereby enabling DataStealing. (c) AdaSCP achieves the purpose of DataStealing and defeats the advanced defenses by training critical parameters and adaptively scaling the updates before uploading.", "description": "This figure illustrates the ComboTs and AdaSCP attack methods.  (a) shows how ComboTs selects multiple trigger points within an image to enable the mapping of multiple target images. (b) depicts the forward and reverse diffusion processes involved in embedding and extracting these images, using Trojan noise. Finally, (c) illustrates how AdaSCP adaptively scales critical parameters in the poisoned model updates to evade distance-based defenses, thereby successfully performing DataStealing.", "section": "3 DataStealing: Task and Algorithms"}, {"figure_path": "792txRlKit/figures/figures_20_1.jpg", "caption": "Figure 10: Visual Results of LSUN Bedroom in Non-IID Distribution.", "description": "This figure shows the visual results of applying various attacks (Data Poison, Model Poison, PGD Poison, BC Layer Sub, AdaSCP) against LSUN bedroom dataset under different defense mechanisms (FedAvg, Krum, Multi-Krum, Foolsgold, RFA, Multi-metrics).  Each cell in the figure represents the generated image for a specific attack and defense combination.  The goal is to show the effectiveness of different attacks against various defenses in stealing private data from the diffusion model. The image quality and the success of the attack vary depending on the combination of attack and defense.", "section": "4 Experiments"}, {"figure_path": "792txRlKit/figures/figures_20_2.jpg", "caption": "Figure 10: Visual Results of LSUN Bedroom in Non-IID Distribution.", "description": "This figure shows the visual results of the LSUN Bedroom dataset under various attack methods (Data Poison, Model Poison, PGD Poison, BC Layer Sub, AdaSCP) and defense mechanisms (FedAvg, Krum, Multi-Krum, Foolsgold, RFA, Multi-metrics) in a non-IID data distribution. Each cell in the figure represents a generated image under a specific attack and defense combination. The target image is displayed in the first column. The figure demonstrates the effectiveness of AdaSCP in generating high-quality images, even when defenses are in place, and how different attacks and defenses affect the quality and fidelity of the generated images.", "section": "4 Experiments"}, {"figure_path": "792txRlKit/figures/figures_21_1.jpg", "caption": "Figure 11: Sampled Images with Different Noise. We visualize the sampled images in CIFAR10 and CelebA attacked by our AdaSCP under FedAvg. These images are generated with the same trigger and different Gaussian noise.", "description": "This figure visualizes images sampled from a diffusion model that has been attacked using the AdaSCP method.  The images are generated using the same trigger but with different Gaussian noise added during the sampling process. This shows the effect of noise on the generation process and the ability of the attack method to produce slightly varied outputs from the same trigger. The figure contains two parts, one for the CIFAR10 dataset and another for the CelebA dataset.", "section": "4 Experiments"}, {"figure_path": "792txRlKit/figures/figures_21_2.jpg", "caption": "Figure 1: Overview of DataStealing. To steal data under strict management, the attacker can poison the global diffusion model in FL with multiple Trojans. After releasing the global model for users, the attacker extracts massive local data in high quality from outside with multiple triggers (ComboTs).", "description": "This figure illustrates the DataStealing attack.  Multiple poisoned clients inject Trojan backdoors into a global diffusion model trained via federated learning (FL).  The attacker then uses these backdoors to extract private data by using multiple triggers, called ComboTs, to manipulate the generated images from the released model.  The figure shows the process from the poisoned clients, the central server with defenses, and the final generation of data from triggers. ", "section": "3 DataStealing: Task and Algorithms"}]