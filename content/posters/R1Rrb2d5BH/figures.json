[{"figure_path": "R1Rrb2d5BH/figures/figures_1_1.jpg", "caption": "Figure 1: Comparison of zero-shot HOI detection paradigms. (a) Methods that align HOI features with fixed VLMs [44, 33, 4, 41]. (b) Prompt learning methods to adapt VLMs for downstream tasks [22, 57]. (c) Our efficient zero-shot HOI detection (EZ-HOI). (d) Unseen, seen, and full mAP indicate the performance for unseen-verb, seen-verb, and full sets on the HICO-DET dataset [6]. Our EZ-HOI shows superior performance in these categories, with competitive trainable parameters and training epochs.", "description": "This figure compares three different approaches for zero-shot human-object interaction (HOI) detection.  (a) shows traditional methods aligning HOI models with pre-trained Vision-Language Models (VLMs). (b) illustrates prompt tuning, an alternative approach where VLMs are adapted for HOI tasks.  This often overfits to seen classes. (c) presents the authors' proposed method, EZ-HOI, which efficiently adapts VLMs using a novel prompt learning approach. (d) provides a quantitative comparison of the three approaches on the HICO-DET dataset, highlighting EZ-HOI's superior performance and efficiency in terms of trainable parameters and training epochs.", "section": "1 Introduction"}, {"figure_path": "R1Rrb2d5BH/figures/figures_3_1.jpg", "caption": "Figure 2: Overview of our EZ-HOI framework. Learnable text prompts capture detailed HOI class information from the LLM. To enhance their generalization ability, we introduce the Unseen Text Prompt Learning (UTPL) module. Meanwhile, visual learnable prompts are guided by a frozen VLM visual encoder. These learnable text and visual prompts are then separately input into the text and visual encoder. Finally, HOI predictions are made by calculating the cosine similarity between the text encoder output and the HOI image features. MHCA denotes multi-head cross-attention.", "description": "This figure illustrates the EZ-HOI framework's architecture.  It shows how learnable text and visual prompts are generated and used, with guidance from LLMs and VLMs respectively. The UTPL module is highlighted to show how unseen prompts are learned.  The final prediction is made through multi-head cross-attention and cosine similarity.", "section": "3 Methodology"}, {"figure_path": "R1Rrb2d5BH/figures/figures_4_1.jpg", "caption": "Figure 3: Detailed architecture of Unseen Text Prompt Learning (UTPL). In the figure, we take the \"hose a dog\" unseen HOI class in the unseen-verb zero-shot setting as an example. We first utilize the HOI class text embeddings to identify the most connected seen HOI class to \u201chose a dog\u201d. After selecting the seen class, we generate an input prompt to obtain disparity information from LLM. Finally, the unseen learnable prompt learns from the selected seen class prompt and the disparity information through MHCA.", "description": "This figure details the Unseen Text Prompt Learning (UTPL) module.  The UTPL module addresses the challenge of adapting Vision-Language Models (VLMs) to unseen Human-Object Interaction (HOI) classes by leveraging information from related seen classes.  It uses cosine similarity to find the most similar seen class to an unseen class, then queries a Large Language Model (LLM) to highlight the differences between the seen and unseen classes. This disparity information, along with the prompt for the similar seen class, is then used to refine the prompt for the unseen class via Multi-Head Cross-Attention (MHCA).", "section": "3.2 Unseen-Class Text Prompt Learning (UTPL)"}, {"figure_path": "R1Rrb2d5BH/figures/figures_9_1.jpg", "caption": "Figure 6: Qualitative comparison of zero-shot HOI detection between our method and MaPLe [22]. We use orange color to represent unseen HOI classes and use blue color for seen ones. For images containing multiple HOI results, we only present one prediction for clearer demonstration and comparison.", "description": "This figure compares the zero-shot human-object interaction (HOI) detection performance of the proposed EZ-HOI method against the MaPLe baseline.  The images show various scenes containing HOI instances. For each image, both methods' predictions are displayed, with correct predictions marked by a green checkmark and incorrect predictions marked by a red 'X'. The orange color highlights predictions of unseen HOI classes, while blue indicates predictions of seen classes. The bar charts provide a visual comparison of the methods' performance for each image.", "section": "Qualitative Results"}, {"figure_path": "R1Rrb2d5BH/figures/figures_16_1.jpg", "caption": "Figure 2: Overview of our EZ-HOI framework. Learnable text prompts capture detailed HOI class information from the LLM. To enhance their generalization ability, we introduce the Unseen Text Prompt Learning (UTPL) module. Meanwhile, visual learnable prompts are guided by a frozen VLM visual encoder. These learnable text and visual prompts are then separately input into the text and visual encoder. Finally, HOI predictions are made by calculating the cosine similarity between the text encoder output and the HOI image features. MHCA denotes multi-head cross-attention.", "description": "This figure illustrates the EZ-HOI framework's architecture.  It shows how learnable text prompts, guided by an LLM, capture detailed HOI class information.  The UTPL module enhances the generalization of these prompts.  Visual prompts are guided by a frozen VLM visual encoder. Both text and visual prompts are fed into their respective encoders, and final HOI predictions are generated based on the cosine similarity between text encoder outputs and HOI image features.", "section": "3. Methodology"}, {"figure_path": "R1Rrb2d5BH/figures/figures_19_1.jpg", "caption": "Figure 1: Comparison of zero-shot HOI detection paradigms. (a) Methods that align HOI features with fixed VLMs [44, 33, 4, 41]. (b) Prompt learning methods to adapt VLMs for downstream tasks [22, 57]. (c) Our efficient zero-shot HOI detection (EZ-HOI). (d) Unseen, seen, and full mAP indicate the performance for unseen-verb, seen-verb, and full sets on the HICO-DET dataset [6]. Our EZ-HOI shows superior performance in these categories, with competitive trainable parameters and training epochs.", "description": "This figure compares three different approaches to zero-shot human-object interaction (HOI) detection: (a) aligning an HOI model with a Vision-Language Model (VLM), (b) prompt tuning to adapt the VLM to HOI tasks, and (c) the authors' proposed method, EZ-HOI.  It highlights the limitations of existing methods, such as high computational cost and overfitting to seen classes. The figure shows that EZ-HOI achieves state-of-the-art performance with significantly fewer parameters and training epochs. The performance comparison uses mean average precision (mAP) across unseen, seen, and all classes in the HICO-DET dataset.", "section": "1 Introduction"}, {"figure_path": "R1Rrb2d5BH/figures/figures_20_1.jpg", "caption": "Figure 2: Overview of our EZ-HOI framework. Learnable text prompts capture detailed HOI class information from the LLM. To enhance their generalization ability, we introduce the Unseen Text Prompt Learning (UTPL) module. Meanwhile, visual learnable prompts are guided by a frozen VLM visual encoder. These learnable text and visual prompts are then separately input into the text and visual encoder. Finally, HOI predictions are made by calculating the cosine similarity between the text encoder output and the HOI image features. MHCA denotes multi-head cross-attention.", "description": "This figure illustrates the EZ-HOI framework, highlighting the use of learnable text and visual prompts guided by LLMs and VLMs. The UTPL module addresses the challenge of unseen classes. The framework incorporates multi-head cross-attention (MHCA) for integrating information and generating HOI predictions.", "section": "3 Methodology"}]