[{"figure_path": "uHml6eyoVF/figures/figures_4_1.jpg", "caption": "Figure 1: The performance of an exhaustive-search algorithm corroborates the presence of a phase transition for \u03b8 = 1, as suggested by theorem 2. Success rate of an exponential-time search algorithm over all the possible spikes in the d-hypercube as a function of the exponent \u03b8 that quantifies as n = d\u03b8 the samples used in the log-likelihood test (8), in the g ~Radem(1/2) case.", "description": "This figure shows the success rate of an exhaustive search algorithm for finding a spike in a high-dimensional space, as a function of the number of samples used. The algorithm searches through all possible spike directions in a d-dimensional hypercube. The x-axis represents the exponent \u03b8, such that the number of samples n scales with the dimension d as n = d\u03b8. The success rate increases sharply around \u03b8 = 1, indicating a phase transition in the distinguishability of the spiked cumulant model from the isotropic Gaussian model. This corroborates the theoretical findings presented in Theorem 2.", "section": "3 How many samples do we need to learn?"}, {"figure_path": "uHml6eyoVF/figures/figures_7_1.jpg", "caption": "Figure 2: Learning the spiked Wishart task with neural networks and random features. (A,B) Test accuracy of random features (RF) and early-stopping test accuracy of two-layer ReLU neural networks (NN) on the spiked Wishart task, eq. (1), with linear and quadratic sample complexity (nclass= d, d\u00b2, respectively, where d is the input dimension). Predictions for the performance of random features obtained using replicas are shown in black. (C,D) Maximum normalised overlaps of the networks' first-layer weights with the spike u, eq. (1). Parameters: \u03b2 = 5. Neural nets and random features have m = 5d hidden neurons. Full experimental details in appendix A.", "description": "This figure shows the results of experiments on the spiked Wishart model, comparing the performance of neural networks and random features in both linear and quadratic sample complexity regimes.  Panels (A) and (B) display the test accuracy, showing neural networks successfully learn the task in both regimes while random features only succeed in the quadratic regime. Panels (C) and (D) illustrate the overlap between the first-layer weights of the neural network and the spike, demonstrating efficient feature extraction by the neural network.", "section": "4 Learning from HOCs with neural networks and random features"}, {"figure_path": "uHml6eyoVF/figures/figures_8_1.jpg", "caption": "Figure 3: Learning the spiked cumulant task with neural networks and random features. (A, B) Test accuracy of random features (RF) and early-stopping test accuracy of two-layer ReLU neural networks (NN) on the spiked cumulant task eq. (3) with linear and quadratic sample complexity (nclass = d, d\u00b2, respectively, where d is the input dimension). (C, D) Maximum normalised overlaps of the networks\u2019 first-layer weights with the spike u, (3). Parameters: \u03b2 = 10. Neural nets and random features have m = 5d hidden neurons, same optimisation as in fig. 2. Full experimental details in appendix A.", "description": "This figure shows the results of experiments comparing the performance of neural networks and random features on a spiked cumulant task.  Panels A and B show the test accuracy of both methods at linear and quadratic sample complexities (nclass/d and nclass/d^2, respectively), demonstrating the quadratic sample complexity required for random features to succeed.  Panels C and D depict the maximum normalized overlaps between the networks' first-layer weights and the spike for linear and quadratic sample complexities, further illustrating the efficiency of neural networks in this scenario.", "section": "4 Learning from HOCs with neural networks and random features"}, {"figure_path": "uHml6eyoVF/figures/figures_9_1.jpg", "caption": "Figure 4: A phase transition in the fourth-order cumulant precedes learning from the fourth cumulant. (A) We train neural networks to discriminate inputs sampled from a simple non-Gaussian model for images introduced by Ingrosso & Goldt [13] (top) from Gaussians with the same mean and covariance (bottom). (B) Test error of two-layer neural networks interpolating between the fully-trained (\u03b1 = 1) and lazy regimes (large \u03b1) \u2013 see section 4.3. (C) The localisation of the leading CP-factor of the non-Gaussian inputs (dashed purple line) and the first-layer weights of the trained networks, as measured by the inverse participation ratio (IPR), eq. (13). Large IPR denotes a more localised vector w. Parameters: g = 3, \u03be = 1, d = 20, m = 100. Full details in appendix A.", "description": "This figure shows a phase transition in the fourth-order cumulant that precedes learning from the fourth cumulant in a simple image model.  Panel A displays the two classes of images used in the binary classification task: a non-Gaussian image model and a Gaussian image with the same mean and covariance. Panel B shows the test error of two-layer neural networks as a function of the number of training samples (n/d), where the networks interpolate between the \"fully-trained\" (\u03b1 = 1) and \"lazy\" regimes (large \u03b1). The inverse participation ratio (IPR) in Panel C measures the localization of the leading CP-factor of the non-Gaussian inputs and the first-layer weights of trained networks.  High IPR values indicate more localized vectors. The results suggest that a phase transition in the fourth-order cumulant occurs before the networks learn the task efficiently.", "section": "4.3 Phase transitions and neural network performance in a simple model for images"}, {"figure_path": "uHml6eyoVF/figures/figures_15_1.jpg", "caption": "Figure 2: Learning the spiked Wishart task with neural networks and random features. (A,B) Test accuracy of random features (RF) and early-stopping test accuracy of two-layer ReLU neural networks (NN) on the spiked Wishart task, eq. (1), with linear and quadratic sample complexity (nclass= d, d\u00b2, respectively, where d is the input dimension). Predictions for the performance of random features obtained using replicas are shown in black. (C,D) Maximum normalised overlaps of the networks' first-layer weights with the spike u, eq. (1). Parameters: \u03b2 = 5. Neural nets and random features have m = 5d hidden neurons. Full experimental details in appendix A.", "description": "This figure shows the results of experiments comparing the performance of neural networks and random features on a spiked Wishart classification task.  Panels A and B show the test accuracy of random features (RF) and neural networks (NN) respectively, under both linear and quadratic sample complexity scaling.  The black lines in A and B represent theoretical predictions from replica analysis. Panels C and D display the overlap between the networks' first-layer weights and the spike vector, again for linear and quadratic sample complexities.  The results demonstrate that neural networks efficiently learn the task in both regimes while random features struggle in the linear regime, achieving near-chance performance.", "section": "4 Learning from HOCs with neural networks and random features"}, {"figure_path": "uHml6eyoVF/figures/figures_26_1.jpg", "caption": "Figure 6: Graphs of f, defined in (7), when g ~Rademacher(1/2).", "description": "The figure shows the graphs of function f(\u03b2,\u03bb) for different values of \u03b2, where \u03bb is the overlap between two independent replicas of spike u and v and g is drawn from the Rademacher distribution. This function is important in calculating the likelihood ratio norm for the spiked cumulant model.", "section": "B.5.3 Consequences of theorem 2"}, {"figure_path": "uHml6eyoVF/figures/figures_28_1.jpg", "caption": "Figure 7: On the left, LR norm when g ~ Rademacher(1/2) in the regime n = \u03b3d with \u03b3 = 1. When \u03b2 < \u03b2\u03b3 \u2248 10.7 the likelihood ratio remains bounded, whereas it goes to +\u221e for \u03b2 > \u03b2\u03b3. On the right, the lower bound on ||LD(n)|| given by 73 goes to +\u221e for \u03b8 > 2. Parameters for the plot: g ~Radem(1/2), \u03b2 = 10, D(n) = log3/2 (n)", "description": "This figure shows the behavior of the likelihood ratio (LR) norm and its lower bound for different values of \u03b2 (signal-to-noise ratio) and \u03b8 (sample complexity exponent). The left panel shows that the LR norm remains bounded for \u03b2 < \u03b2\u03b3 \u2248 10.7, while it diverges for \u03b2 > \u03b2\u03b3. The right panel shows that the lower bound on the LR norm given by equation (73) in the paper diverges for \u03b8 > 2, indicating that the sample complexity for distinguishing the two models (spiked cumulant and isotropic Gaussian) is at least quadratic in d for polynomial-time algorithms. This corroborates the theoretical findings of the paper concerning the statistical and computational trade-offs in learning from higher-order correlations.", "section": "3.3 Statistical-to-computational gaps in the spiked cumulant model"}, {"figure_path": "uHml6eyoVF/figures/figures_36_1.jpg", "caption": "Figure 8: Linear and quadratic sample regimes for different synthetic data models. (Right) Generalization error of the hidden manifold model (top), the teacher-student setup (center) and a mixture of two Gaussians as a function of the ratio of the number of samples and the input dimension. (Left) Same except that the number of samples scales with the square of the input dimension. The solid black line corresponds to the replica theory prediction while the coloured dots display the outcome of the numerical simulations averaged over 10 different seeds. In all the panels, d = 1000 and d = 20 for linear and quadratic sample regimes respectively, X = 0.01 for both the teacher-student setup and the hidden manifold model while X = 0.1 for Gaussian mixtures. In the Gaussian mixture case, \u03bc\u00b0 = (\u00b11,0, ..., 0) \u2208 Rd while the covariance matrices are both isotropic and, in particular, both equal to the identity matrix: + = \u0399.", "description": "This figure compares the generalization error of random features for three different synthetic data models: Hidden Manifold model, teacher-student setup, and Gaussian mixture model. It shows how the generalization error changes as the number of samples scales linearly and quadratically with the input dimension. The results from numerical simulations are plotted against the theoretical predictions obtained from replica theory, illustrating the linear and quadratic regimes of sample complexity for these models.", "section": "4 Learning from HOCs with neural networks and random features"}]