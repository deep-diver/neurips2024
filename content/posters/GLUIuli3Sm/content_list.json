[{"type": "text", "text": "On the Convergence of Loss and Uncertainty-based Active Learning Algorithms ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Daniel Haimovich Meta, Central Applied Science danielha@meta.com ", "page_idx": 0}, {"type": "text", "text": "Dima Karamshuk Meta, Central Applied Science karamshuk@meta.com ", "page_idx": 0}, {"type": "text", "text": "Fridolin Linder Meta, Central Applied Science flinder@meta.com ", "page_idx": 0}, {"type": "text", "text": "Niek Tax Meta, Central Applied Science niek@meta.com ", "page_idx": 0}, {"type": "text", "text": "Milan Vojnovic\u00b4 London School of Economics m.vojnovic@lse.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We investigate the convergence rates and data sample sizes required for training a machine learning model using a stochastic gradient descent (SGD) algorithm, where data points are sampled based on either their loss value or uncertainty value. These training methods are particularly relevant for active learning and data subset selection problems. For SGD with a constant step size update, we present convergence results for linear classifiers and linearly separable datasets using squared hinge loss and similar training loss functions. Additionally, we extend our analysis to more general classifiers and datasets, considering a wide range of loss-based sampling strategies and smooth convex training loss functions. We propose a novel algorithm called Adaptive-Weight Sampling (AWS) that utilizes SGD with an adaptive step size that achieves stochastic Polyak\u2019s step size in expectation. We establish convergence rate results for AWS for smooth convex training loss functions. Our numerical experiments demonstrate the efficiency of AWS on various datasets by using either exact or estimated loss values. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In practice, when training machine learning models for prediction tasks (classification or regression), one often has access to an abundance of unlabeled data, while obtaining the corresponding labels may entail high costs. This may especially be the case in fields like computer vision, natural language processing, and speech recognition. Active learning algorithms are designed to efficiently learn a prediction model by employing a label acquisition, with the goal of minimizing the number of labels used to train an accurate prediction model. ", "page_idx": 0}, {"type": "text", "text": "Various label acquisition strategies have been proposed, each aiming to select informative points for the underlying model training task; including query-by-committee [Seung et al., 1992], expected model change [Settles et al., 2007], expected error reduction [Roy and McCallum, 2001], expected variance reduction [Wang et al., 2016], and mutual information maximization [Kirsch et al., 2019, Kirsch and Gal, 2022]. ", "page_idx": 0}, {"type": "text", "text": "A common label acquisition strategy involves estimating uncertainty, which can be viewed as self-disagreement about predictions made by a given model. Algorithms using an uncertainty acquisition strategy are referred to as uncertainty-based active learning algorithms. Different variants of uncertainty strategies include margin of confidence, least confidence, and entropy-based sampling [Nguyen et al., 2022]. Recently, a loss-based active learning approach gained attention in research Yoo and Kweon [2019], Lahlou et al. [2022], Nguyen et al. [2021], Luo et al. [2021], and is now applied at scale in industry, such as for training integrity violation classifiers at Meta. This method involves selecting points for which there is a disagreement between the predicted label and the true label, as measured by a loss function. Since the true loss of a data point is unknown prior to the acquisition of the label, in practice, it is estimated using supervised learning. Loss-based sampling aligns with the spirit of the perceptron algorithm [Rosenblatt, 1958], which updates the model only for falsely-classified points. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Convergence guarantees for some uncertainty-based active learning algorithms have recently been established, such as for margin of confidence sampling [Raj and Bach, 2022]. By contrast, there are only limited results on the convergence properties of loss-based active learning algorithms, as these only recently been started to be studied, e.g., Liu and Li [2023]. ", "page_idx": 1}, {"type": "text", "text": "The primary focus of this paper is to establish convergence guarantees for stochastic gradient descent (SGD) algorithms where points are sampled based on their loss. Our work provides new results on conditions that ensure certain convergence rates and bounds on the expected sample size, accommodating various data sampling strategies. Our theoretical results are under assumption that the active learner has access to an oracle that provides unbiased estimate of the conditional expected loss for a point, given the feature vector of the point and the current prediction model. In practice, the loss cannot be evaluated at acquisition time since labels are yet unknown. Instead, a separate prediction model is used for loss estimation. In our experiments, we assess the impact of the bias and noise in such a loss estimator. Our convergence rate analysis accommodates also uncertainty-based data selection, for which we provide new results. ", "page_idx": 1}, {"type": "text", "text": "Uncertainty and loss-based acquisition strategies are also of interest for the data subset selection problem, often referred to as core-set selection or data pruning. This problem involves finding a small subset of training data such that the predictive performance of a classifier trained on it is close to that of a classifier trained on the full training data. Recent studies have explored this problem in the context of training neural networks, as seen in works like Toneva et al. [2019], Coleman et al. [2020], Paul et al. [2021], Sorscher et al. [2022], Mindermann et al. [2022]. In such scenarios, the oracle can evaluate an underlying loss function exactly, avoiding the need for using a loss estimator. ", "page_idx": 1}, {"type": "text", "text": "There is a large body of work on convergence of SGD algorithms, e.g. see Bubeck [2015] and Nesterov [2018]. These results are established for SGD algorithms under either constant, diminishing or adaptive step sizes. Recently, Loizou et al. [2021], studied SGD with the stochastic Polyak\u2019s step size, depending on the ratio of the loss and the squared gradient of the loss of a point. Our work proposes an adaptive-window sampling algorithm and provides its convergence analysis, with the algorithm defined as SGD with a sampling of points and an adaptive step size update that conform to the stochastic Polyak\u2019s step size in expectation. This is unlike to the adaptive step size SGD algorithm by Loizou et al. [2021] which does not use sampling. ", "page_idx": 1}, {"type": "text", "text": "1.1 Summary of our Contributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Our contributions can be summarizes as given in the following points: ", "page_idx": 1}, {"type": "text", "text": "\u2022 For SGD with a constant step size, we present conditions under which a non-asymptotic convergence rate of order ${\\cal O}(1/n)$ holds, where $n$ represents the number of iterations of the algorithm, i.e., the number of unlabeled points presented to the algorithm. These conditions enable us to establish convergence rate results for loss-based sampling in the case of linear classifiers and linearly separable datasets, with the loss function taking on various forms such as the squared hinge loss function, generalized hinge loss function, or satisfying other specified conditions. Our results provide bounds for both expected loss and the number of sampled points, encompassing different loss-based strategies. These results are established by using a convergence rate lemma that may be of independent interest. ", "page_idx": 1}, {"type": "text", "text": "\u2022 For SGD with a constant step size, we provide new convergence rate results for more general classifiers and datasets, with sampling of points according to an increasing function $\\pi$ of the conditional expected loss of a point. In this case, we present conditions for smooth c\u221aonvex training loss functions under which a non-asymptotic convergence rate of order $O(\\Pi^{-1}(1/\\sqrt{n}))$ holds, where $\\Pi$ is the primitive function of $\\pi$ . These results are established by leveraging the fact that the algorithm behaves akin to a SGD algorithm with an underlying objective function, as referred to as an equivalent loss in Liu and Li [2023], allowing us to apply known convergence rate results for SGD algorithms. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "\u2022 We propose Adaptive-Weight Sampling (AWS), a novel learning algorithm that combines a samplingbased acquisition strategy with an adaptive step-size SGD update, achieving the stochastic Polyak\u2019s step size update in expectation, which can be used with any differentiable loss function. We establish a condition under which a non-asymptotic convergence rate of order $O(1/n)$ holds for AWS with smooth convex loss functions. We present uncertainty and loss-based strategies that satisfy this condition for binary classification, as well as an uncertainty strategy for multi-class classification. ", "page_idx": 2}, {"type": "text", "text": "\u2022 We present numerical results that demonstrate the efficiency of AWS on various datasets. ", "page_idx": 2}, {"type": "text", "text": "1.2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The early proposal of the query-by-committee (QBC) algorithm by [Seung et al., 1992] demonstrated the benefits of active learning, an analysis of which was conducted under the selective sampling model by Freund et al. [1997] and Gilad-bachrach et al. [2005]. Dasgupta et al. [2009] showed that the performance of QBC can be efficiently achieved by a modified perceptron algorithm with adaptive flitering. The efficient and label-optimal learning of halfspaces was studied by Yan and Zhang [2017] and, subsequently, by Shen [2021]. Online active learning algorithms, studied under the name of selective sampling, include works by [Cesa-Bianchi et al., 2006, 2009, Dekel et al., 2012, Orabona and Cesa-Bianchi, 2011, Cavallanti et al., 2011, Agarwal, 2013]. For a survey, refer to Settles [2012]. ", "page_idx": 2}, {"type": "text", "text": "Uncertainty sampling has been utilized for classification tasks since as early as [Lewis and Gale, 1994], and subsequently in many other works, such as [Schohn and Cohn, 2000, Zhu et al., 2010, Yang et al., 2015, Yang and Loog, 2016, Lughofer and Pratama, 2018]. Mussmann and Liang [2018] demonstrated that threshold-based uncertainty sampling on a convex loss can be interpreted as performing a pre-conditioned stochastic gradient step on the population zero-one loss. However, none of these works have provided theoretical convergence guarantees. ", "page_idx": 2}, {"type": "text", "text": "The convergence of margin of confidence sampling was recently studied by Raj and Bach [2022], who demonstrated linear convergence for linear classifiers and linearly separable datasets, specifically for the hinge loss function, for a family of selection probability functions and an algorithm that performs a SGD update with respect to the squared hinge loss function. However, our results for linear classifiers and linearly separable datasets differ, as our focus lies on loss-based sampling strategies and providing bounds on the convergence rate of a loss function and the expected number of sampled points. These results are established using a convergence rate lemma, which may be of independent interest. It is noteworthy that the convergence rate for uncertainty-based sampling, as in Theorem 3.1 of Raj and Bach [2022], can be derived by checking the conditions of the convergence rate lemma. ", "page_idx": 2}, {"type": "text", "text": "A loss-based active learning algorithm was proposed by Yoo and Kweon [2019], comprising a loss prediction module and a target prediction model. The algorithm uses the loss prediction module to compute a loss estimate and prioritizes sampling points with a high estimated loss under the current prediction model. Lahlou et al. [2022] generalize this idea within a framework for uncertainty prediction. However, neither Yoo and Kweon [2019] nor Lahlou et al. [2022] provided theoretical guarantees for convergence rates. Recent analysis of convergence for loss and uncertainty-based active learning strategies has been presented by Liu and Li [2023]. Specifically, they introduced the concept of an equivalent loss, demonstrating that a gradient descent algorithm employing point sampling can be viewed as a SGD algorithm optimizing an equivalent loss function. While they focused on specific cases like sampling proportional to conditional expected loss, our results allow for sampling based on any continuous increasing function of expected conditional loss, and provide explicit convergence rate bounds in terms of the underlying sampling probability function. ", "page_idx": 2}, {"type": "text", "text": "In addition, Loizou et al. [2021] introduced a SGD algorithm featuring an adaptive stochastic Polyak\u2019s step size, which has theoretical convergence guarantees under various assumptions. This algorithm showcased robust performance in comparison to state-of-the-art optimization methods, especially when training over-parametrized models. Our work proposes a novel sampling method that employs stochastic Polyak\u2019s step size in expectation, offering a convergence rate guarantee for smooth convex loss functions, contingent on a condition related to the sampling probability function. Notably, we demonstrate the fulfillment of this condition for logistic regression and binary cross-entropy loss functions, encompassing both a loss-based strategy involving proportional sampling to absolute error loss and an uncertainty sampling strategy. Furthermore, we extend this condition to hold for an uncertainty sampling strategy designed for multi-class classification. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "2 Problem Statement ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We consider the setting of streaming algorithms where a machine learning model parameter $\\theta_{t}$ is updated sequentially, upon encountering each data point, with $(x_{1},y_{1}),\\dotsc,(x_{n},y_{n})\\in{\\mathcal{X}}\\times{\\mathcal{Y}}$ denoting the sequence of data points with the corresponding labels, assumed to be independent and identically distributed with distribution $\\mathcal{D}$ . Specifically, we consider the class of projected SGD algorithms defined as: given an initial value $\\theta_{1}\\in\\Theta$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\theta_{t+1}=\\mathcal{P}_{\\Theta_{0}}\\left(\\theta_{t}-z_{t}\\nabla_{\\theta}\\ell(x_{t},y_{t},\\theta_{t})\\right),\\;\\mathrm{for}\\;t\\ge1\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\ell:\\,\\mathcal{X}\\times\\mathcal{Y}\\times\\Theta\\,\\to\\,\\mathbb{R}$ is a training loss function, $z_{t}$ is a stochastic step size with mean $\\zeta(x_{t},y_{t},\\theta_{t})$ for some function $\\zeta:\\mathcal{X}\\times\\mathcal{Y}\\times\\Theta\\mapsto\\mathbb{R}_{+}$ , $\\Theta_{0}\\subseteq\\Theta$ , and ${\\mathcal{P}}_{\\Theta_{0}}$ is the projection function, i.e., $\\begin{array}{r}{\\mathcal{P}_{\\Theta_{0}}(u)=\\arg\\operatorname*{min}_{v\\in\\Theta_{0}}||u-v||}\\end{array}$ . Unless specified otherwise, we consider the case $\\Theta_{0}=\\Theta$ , which requires no projection. For binary classification tasks, we assume ${\\boldsymbol{\\mathcal{V}}}=\\{-1,1\\}$ . For every $t>0$ , we define $\\begin{array}{r}{\\bar{\\theta}_{t}=(1/t)\\sum_{s=1}^{t}\\theta_{s}}\\end{array}$ . ", "page_idx": 3}, {"type": "text", "text": "By defining the distribution of the stochastic step size $z_{t}$ in Equation (1) appropriately, we can accommodate different active learning and data subset selection algorithms. In the context of active learning algorithms, at each step $t$ , the algorithm observes the value of $x_{t}$ and decides whether or not to observe the value of the label $y_{t}$ . The value of $z_{t}$ determine whether or not we observe the label $y_{t}$ . Deciding not to observe the value of the label $y_{t}$ implies the step size $z_{t}$ of value zero (not updating the machine learning model). ", "page_idx": 3}, {"type": "text", "text": "For the choice of the stochastic step size, we consider two cases: (a) Constant-Weight Sampling: a Bernoulli sampling with a constant step size, and (b) Adaptive-Weight Sampling: a sampling that achieves stochastic Polyak\u2019s step size in expectation. For case (a), $z_{t}$ is the product of a constant step size $\\gamma$ and a Bernoulli random variable with mean $\\pi(x_{t},y_{t},\\theta_{t})$ . For case (b), $\\zeta(x,y,\\theta)$ is the \"stochastic\" Polyak\u2019s step size, and $z_{t}$ is equal to $\\zeta(x_{t},y_{t},\\theta_{t})/\\pi(x_{t},y_{t},\\theta_{t})$ with probability $\\pi(x_{t},y_{t},\\theta_{t})$ and is equal to 0 otherwise. Note that using the notation $\\pi(x,y,\\theta)$ allows for the case when the sampling probability does not depend on the value of the label $y$ . ", "page_idx": 3}, {"type": "text", "text": "For a loss-based sampling, $\\pi$ is an increasing function of some loss function $\\ell^{\\star}$ , which does not necessarily correspond to the training loss function $\\ell$ . Specifically, for a binary classifier with $p(x,y,\\theta)$ denoting the expected prediction label, sampling proportional to the absolute error loss is defined as $\\pi(\\ell^{*})=\\omega\\ell^{*}$ where $\\ell^{*}(x,y,\\theta)=|y-p(x,y,\\theta)|$ and $\\omega\\,\\in\\,(0,1/2]$ . For an uncertainty-based sampling, $\\pi$ is a function of some quantity reflecting the uncertainty of the prediction model. ", "page_idx": 3}, {"type": "text", "text": "Our focus is on finding convergence conditions for algorithm (1) and convergence rates under these conditions, as well as bounds on the expected number of points sampled by the algorithm. ", "page_idx": 3}, {"type": "text", "text": "Additional Assumptions and Notation For binary classification, we say that data is separable if, for every point $(x,y)\\in\\mathcal{X}\\times\\mathcal{Y}$ , either $y=1$ with probability 1 or $y=-1$ with probability 1. The data is linearly separable if there exists $\\theta^{*}\\in\\Theta$ such that $y=\\operatorname{sgn}(x^{\\top}\\theta^{*})$ for every $x\\in\\mathscr{X}$ . Linearly separable data has a $\\rho^{*}$ -margin if $|x^{\\top}\\theta^{*}|\\geq\\rho^{*}$ for every $x\\in\\mathscr{X}$ , for some $\\theta^{*}\\in\\Theta$ . ", "page_idx": 3}, {"type": "text", "text": "Some of our results are for linear classifiers, where the predicted label of a point $x$ is a function of $x^{\\top}\\theta$ . For example, a model with a predicted label $\\operatorname{sgn}(x^{\\top}\\theta)$ is a linear classifier. For logistic regression, the predicted label is 1 with probability $\\sigma(x^{\\top}\\theta)$ and $-1$ otherwise, where $\\sigma$ is the logistic function defined as $\\sigma(z)=1/(1+e^{-z})$ . For binary classification, we model the prediction probability of the positive label as $\\sigma(x^{\\top}\\theta)$ , where $\\sigma:\\mathbb{R}_{+}\\rightarrow[0,1]$ is an increasing function, and $\\sigma(-u)+\\sigma(u)=1$ for all $u\\in\\mathbb R$ . The absolute error loss takes value $1-\\sigma(x^{\\top}\\theta)$ if $y=1$ or value $\\sigma(x^{\\top}\\theta)$ if $y=-1$ , which corresponds to $1-\\sigma(\\boldsymbol{y}\\boldsymbol{x}^{\\top}\\boldsymbol{\\theta})$ . The binary cross-entropy loss for a point $(x,y)$ under model parameter $\\theta$ can be written as $\\ell(x,y,\\theta)\\,=\\,-\\log(\\sigma(y x^{\\top}\\theta))$ . Hence, absolute error loss-based sampling corresponds to the sampling probability function $\\pi(\\ell)=1-e^{-\\ell}$ . ", "page_idx": 3}, {"type": "text", "text": "For any given $(x,y)\\,\\in\\,\\mathcal{X}\\times\\mathcal{Y}$ , the loss function $\\ell(x,y,\\theta)$ is considered smooth on $\\Theta^{\\prime}\\subseteq\\,\\Theta$ if it has a Lipschitz continuous gradient on $\\Theta^{\\prime}$ , i.e., there exists $L_{x,y}$ such that $\\|\\nabla_{\\theta}\\ell(x,y,\\theta_{1})\\,-$ $\\nabla_{\\theta}\\ell(x,y,\\bar{\\theta_{2}})||~\\leq~L_{x,y}||\\theta_{1}~-^{*}\\!\\theta_{2}||$ for all $\\theta_{1},\\theta_{2}\\;\\in\\;\\Theta^{\\prime}$ . For any distribution $q$ over $\\mathcal X\\,\\times\\,\\mathcal Y$ , $\\mathbb{E}_{(x,y)\\sim q}[\\ell(x,y,\\theta)]$ is $\\mathbb{E}_{(x,y)\\sim q}[L_{x,y}]$ -smooth. ", "page_idx": 3}, {"type": "text", "text": "3 Convergence Rate Guarantees ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we present conditions on the stochastic step size of algorithm (1) under which we can bound the total expected loss and the expected number of samples. For the Constant-Weight Sampling, we provide conditions that allow us to derive bounds for linear classifiers and linearly separable datasets and more general cases. For Adaptive-Weight Sampling, we offer a condition that allows us to establish convergence bounds for both loss and uncertainty-based sampling. ", "page_idx": 4}, {"type": "text", "text": "3.1 Constant-Weight Sampling ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Linear Classifiers and Linearly Separable Datasets We focus on binary classification and briefly discuss extension to multi-class classification. We consider the linear classifier with the predicted label $\\operatorname{sgn}(x^{\\top}\\theta)$ . With a slight abuse of notation, let $\\ell(x,y,\\theta)\\equiv\\ell(u)$ and $\\pi(x,y,\\theta)\\equiv\\pi(\\bar{u})$ where $u=y x^{\\top}\\theta$ . We assume that the domain $\\mathcal{X}$ is bounded, i.e., there exists $R$ such that $||x||\\leq R$ for all $x\\in\\mathscr{X}$ , $\\lvert|\\theta_{1}-\\theta^{*}\\rvert|\\leq S$ for some $S\\geq0$ , and that the data is $\\rho^{*}$ -margin linearly separable. ", "page_idx": 4}, {"type": "text", "text": "We present convergence rate results for the training loss function corresponding to the squared hinge loss function, i.e. $\\ell(u)\\,=\\,(1/2)\\,\\mathrm{max}\\{1\\,-\\,u,0\\}^{\\frac{\\sqrt}{2}}$ . Our additional results also cover other cases, including a class of smooth convex loss functions and a generalized smooth hinge loss function, which are presented in the Appendix. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.1. Assume that $\\rho^{*}>1$ , the loss function is the squared hinge loss function, and the sampling probability function $\\pi$ is such that for all $u\\leq1$ , $\\pi(u)\\bar{\\le}\\,\\beta/2$ and ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\pi(u)\\geq\\pi^{*}(\\ell(u)):=\\frac{\\beta}{2}\\left(1-\\frac{1}{1+\\mu\\sqrt{\\ell(u)}}\\right)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "for some constants $0\\,<\\,\\beta\\,\\leq\\,2$ and $\\mu\\,\\geq\\,\\sqrt{2}/(\\rho^{*}\\,-\\,1)$ . Then, for any initial value $\\theta_{1}$ such that $\\quad||\\theta_{1}-\\theta^{*}||\\leq S$ and $\\{\\theta_{t}\\}_{t>1}$ according to algorithm $(I)$ with $\\gamma=1/R^{2}$ , ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\ell(y x^{\\top}\\bar{\\theta}_{n})\\right]\\leq\\mathbb{E}\\left[\\frac{1}{n}\\sum_{t=1}^{n}\\ell(y_{t}x_{t}^{\\top}\\theta_{t})\\right]\\leq\\frac{R^{2}S^{2}}{\\beta}\\frac{1}{n},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $(x,y)$ is an independent sample of a labeled data point from $\\mathcal{D}$ . ", "page_idx": 4}, {"type": "text", "text": "Moreover, if the sampling is according to $\\pi^{*}$ , then the expected number of sampled points satisfies ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\sum_{t=1}^{n}\\pi^{*}(\\ell(y_{t}x_{t}^{\\top}\\theta_{t}))\\right]\\leq\\operatorname*{min}\\left\\{\\frac{1}{2}R S\\mu\\sqrt{\\beta}\\sqrt{n},\\frac{1}{2}\\beta n\\right\\}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Condition (2) requires that the sampling probability function $\\pi$ is lower bounded by an increasing, concave function $\\pi^{*}$ of the loss value. This fact, along with the expected loss bound, implies \u221athe asserted bound for the expected number of samples. The expected number of samples is $O({\\sqrt{n}})$ concerning the number of iterations and is $O(1/(\\rho^{*}-1))$ concerning the margin $\\rho^{*}-1$ . ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.1, and our other results for linear classifiers and linearly separable datasets, are established using a convergence rate lemma, which is presented in Appendix A.2, along with its proof. This lemma generalizes the conditions used to establish the convergence rate for an uncertainty-based sampling algorithm by Raj and Bach [2022], with the sampling probability function $\\pi(u)=\\dot{1}/(1+\\mu|u|)$ , for some constant $\\mu>0$ . It can be readily shown that Theorem 3.1 in Raj and Bach [2022] follows from our convergence rate lemma with the training loss function corresponding to the squared hinge loss function and the evaluation loss function (used for convergence rate guarantee) corresponding to the hinge loss function. Further details on the convergence rate lemma are discussed in Appendix A.2.1. ", "page_idx": 4}, {"type": "text", "text": "The convergence rate conditions for multi-class classification with the set of classes $\\boldsymbol{\\wp}$ are the same as for binary classification, with $u(x,y,\\theta):=x^{\\top}\\theta_{y}-\\operatorname*{max}_{y^{\\prime}\\in\\mathcal{Y}\\setminus\\{y\\}}x^{\\top}\\theta_{y^{\\prime}}$ , except for an additional factor of 2 in one of the conditions (see Lemma A.10 in the Appendix). Hence, all the observations remain valid for the multi-class classification case. ", "page_idx": 4}, {"type": "table", "img_path": "GLUIuli3Sm/tmp/7d9e1c421b53b179a21ecbd84d9cb8ab3be4a1f512159457d5f261f0ab439908.jpg", "table_caption": ["Table 1: Examples of sampling probability functions. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "More General Classifiers and Datasets We consider algorithm (1) where $z_{t}$ is product of a fixed step size $\\gamma$ and a Bernoulli random variable $\\zeta_{t}$ with mean $\\pi(x,y,\\theta)$ . Let $g_{t}=\\zeta_{t}\\nabla_{\\theta}\\ell(x_{t},y_{t},\\theta_{t})$ , which is random vector because $\\zeta_{t}$ is a random variable and $\\left({{x}_{t}},{{y}_{t}}\\right)$ is a sampled point. Following Liu and Li [2023], we note that the algorithm (1) is an SGD algorithm with respect to an objective function $\\tilde{\\ell}$ with gradient ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\nabla_{\\boldsymbol{\\theta}}\\tilde{\\ell}(\\boldsymbol{\\theta})=\\mathbb{E}[\\pi(x,y,\\boldsymbol{\\theta})\\nabla_{\\boldsymbol{\\theta}}\\ell(x,y,\\boldsymbol{\\theta})]\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the expectation is with respect to $x$ and $y$ . This observation allows us to derive convergence rate results by deploying convergence rate results that are known to hold for SGD under various assumptions on function \u2113\u02dc, variance of stochastic gradient vector and step size. A function \u2113\u02dcsatisfying condition (3) is referred to as an equivalent loss in Liu and Li [2023]. ", "page_idx": 5}, {"type": "text", "text": "Assume that the sampling probability $\\pi$ is an increasing function of the conditional expected loss $\\ell(x,\\theta)=\\mathbb{E}_{y}[\\ell(x,y,\\bar{\\theta})\\mid\\bar{x}]$ . With a slight abuse of notation, we denote this probability as $\\pi(\\ell(x,\\theta))$ where $\\pi:\\mathbb{R}_{+}\\rightarrow[0,1]$ is an increasing and continuous function. Let $\\Pi$ be the primitive of $\\pi$ , i.e. $\\Pi^{\\prime}=\\pi$ . We then have ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\tilde{\\ell}(\\theta)=\\mathbb{E}[\\Pi(\\ell(x,\\theta))].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "If $\\ell(x,y,\\theta)$ is a convex function, for every $(x,y)\\in\\mathcal{X}\\times\\mathcal{Y}$ , then $\\tilde{\\ell}$ is a convex function. ", "page_idx": 5}, {"type": "text", "text": "This framework for establishing convergence rates allows us to accommodate different sampling strategies and loss functions. The next lemma allows us to derive convergence rate results for expected loss with respect to loss function $\\ell$ by applying convergence rate results for expected loss with respect to loss function $\\tilde{\\ell}$ (which, recall, is the equivalent loss function). ", "page_idx": 5}, {"type": "text", "text": "Lemma 3.2. Assume that for algorithm $(I)$ with loss-based sampling according to $\\pi$ , for some functions $f_{1},\\ldots,f_{m}$ , we have ", "page_idx": 5}, {"type": "equation", "text": "$$\n{\\bf E}\\left[\\frac{1}{n}\\sum_{t=1}^{n}\\tilde{\\ell}(\\theta_{t})\\right]\\leq\\operatorname*{inf}_{\\theta}\\tilde{\\ell}(\\theta)+\\sum_{i=1}^{m}f_{i}(n).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Then, it holds: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathbf{E}\\left[\\displaystyle\\frac{1}{n}\\sum_{t=1}^{n}\\ell(\\boldsymbol{\\theta}_{t})\\right]}&{\\le}&{\\displaystyle\\operatorname*{inf}_{\\theta}\\Pi^{-1}(\\widetilde{\\ell}(\\theta))+\\sum_{i=1}^{m}\\Pi^{-1}(f_{i}(n)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We apply Lemma 3.2 to obtain the following result. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.3. Assume that $\\ell$ is a convex function, $\\tilde{\\ell}$ is $L$ -smooth, $\\Theta_{0}$ is a convex set, $\\textit{S}=$ $\\operatorname*{sup}_{\\theta\\in\\Theta_{0}}||\\theta-\\theta_{1}||$ , and $\\begin{array}{r}{\\mathbb{E}[\\pi(\\ell(x,\\theta))||\\nabla_{\\theta}\\ell(x,y,\\theta)||^{2}]-||\\nabla_{\\theta}\\tilde{\\ell}(\\theta)||^{2}\\,\\le\\,\\sigma_{\\pi}^{2}}\\end{array}$ . Then, for algorithm $(I)$ with $\\gamma=1/(L+(\\sigma/R)\\sqrt{n/2})$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\ell(\\bar{\\theta}_{n})]\\le\\mathbb{E}\\left[\\frac{1}{n}\\sum_{t=1}^{n}\\ell(\\theta_{t})\\right]\\le\\operatorname*{inf}_{\\theta}\\Pi^{-1}(\\tilde{\\ell}(\\theta))+\\Pi^{-1}\\left(\\frac{\\sqrt{2}S\\sigma_{\\pi}}{\\sqrt{n}}\\right)+\\Pi^{-1}\\left(\\frac{L S^{2}}{n}\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Note that the bound on the expected loss in Theorem 3.3 depends on $\\pi$ through $\\Pi^{-1}$ and $\\sigma_{\\pi}^{2}$ . Specifically, we have a bound depending on $\\pi$ only through $\\bar{\\Pi}^{-1}$ by upper bounding $\\sigma_{\\pi}^{2}$ with $\\bar{\\operatorname*{sup}}_{\\theta\\in\\Theta_{0}}\\mathbb{E}\\[||\\nabla_{\\theta}\\ell(x,y,\\theta)||^{2}]$ . ", "page_idx": 5}, {"type": "text", "text": "For convergence rates for large values of the number of iterations $n$ , the bound in Theorem 3.2 crucially depends on how $\\Pi^{-\\overline{{1}}}(x)$ behaves for small values of $x$ . In Table 1, we show $\\Pi$ and $\\Pi^{-1}$ for several examples of sampling probability function $\\pi$ . For all examples in the table, $\\Pi^{-1}(x)$ is sub-linear in $x$ for small $x$ . For instance, for absolute error l\u221aoss sampling under binary cross-entropy loss function, $\\pi(x)=1-e^{-x}$ , $\\Pi^{-1}(x)$ is approximately $\\sqrt{2x}$ for small $x$ . For this case, we have the following corollary. ", "page_idx": 6}, {"type": "text", "text": "Corollary 3.4. Under assumptions of Theorem 3.3, sampling probability $\\pi(x)=1-e^{-x}$ , and $\\begin{array}{r}{n\\geq\\operatorname*{max}\\left\\{\\left(\\frac{16}{9}\\right)^{2}2S\\sigma_{\\pi}^{2},\\frac{16}{9}L S^{2}\\right\\}}\\end{array}$ it holds ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\ell(\\bar{\\theta}_{n})]\\leq\\mathbb{E}\\left[\\frac{1}{n}\\sum_{t=1}^{n}\\ell(\\theta_{t})\\right]\\leq\\operatorname*{inf}_{\\theta}\\Pi^{-1}(\\tilde{\\ell}(\\theta))+2^{5/4}\\sqrt{S\\sigma_{\\pi}}\\frac{1}{\\sqrt[4]{n}}+2\\sqrt{L}S\\frac{1}{\\sqrt{n}}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "By using a bound on the expected total loss, we can bound the expected total number of sampled points under certain conditions as follows. ", "page_idx": 6}, {"type": "text", "text": "Lemma 3.5. The following bounds hold: ", "page_idx": 6}, {"type": "text", "text": "We remark that $\\pi$ is a concave function for all examples in Table 1 without any additional conditions, except for $\\pi(\\ell)=\\operatorname*{min}\\{(\\ell/b)^{a},1\\}$ which is concave under assumption $0<a\\le1$ . We remark also that for every example in Table 1 except the last one, $\\pi(\\ell)\\leq\\operatorname*{min}\\!\\left\\{K\\ell,1\\right\\}$ for some $K>0$ . Hence, for all examples in Table 1, we have a bound for the expected number of sampled points provided we have a bound for the expected loss. ", "page_idx": 6}, {"type": "text", "text": "3.2 Adaptive-Weight Sampling ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section we propose the Adaptive-Weight Sampling (AWS) algorithm that combines Bernoulli sampling and an adaptive SGD update, and provide a convergence rate guarantee. The algorithm is defined by (1) with the stochastic step size $z_{t}$ being a binary random variable that takes value $\\gamma_{t}:=\\zeta(x_{t},\\dot{y}_{t},\\theta_{t})/\\pi(x_{t},y_{t},\\theta_{t})$ with probability $\\pi(x_{t},y_{t},\\theta_{t})$ and takes value 0 otherwise, where $\\pi$ is some sampling probability function. Here, $\\zeta(x,y,\\theta)$ is the expected SGD (1) step size, defined as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\zeta(x,y,\\theta)=\\beta\\operatorname*{min}\\left\\{\\frac{1}{\\psi(x,y,\\theta)},\\rho\\right\\}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "whenever $||\\nabla_{\\theta}\\ell(x,y,\\theta)||>0$ and $\\zeta(x,y,\\theta)=0$ otherwise, for constants $\\beta,\\rho>0$ , where ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\psi(x,y,\\theta):=\\frac{||\\nabla_{\\theta}\\ell(x,y,\\theta)||^{2}}{\\ell(x,y,\\theta)-\\operatorname*{inf}_{\\theta^{\\prime}}\\ell(x,y,\\theta^{\\prime})}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The expected step size $\\zeta(x,y,\\theta)$ corresponds to the stochastic Polyak\u2019s step size used by a gradient descent algorithm proposed by [Loizou et al., 2021], which is accommodated as a special case when $\\pi(x,y,\\theta)={\\overline{{1}}}$ for all $x,y,\\theta$ . AWS introduces a sampling component and re-weighting of the update to ensure that the step size remains according to the stochastic Polyak\u2019s step size in expectation. For many loss functions, inf\u03b8\u2032 $\\ell(x,y,\\theta)=0$ , for every $x,y$ . In these cases, $\\psi(\\bar{x},y,\\theta)=$ $||\\dot{\\nabla}_{\\theta}\\ell(x,y,\\theta)||^{2}/\\ell(\\dot{x_{,}}y,\\theta)$ . For instance, for binary cross-entropy loss function, inf\u03b8\u2032 $\\ell(x,y,\\theta)=$ $\\operatorname*{inf}_{\\theta^{\\prime}}(-\\log(\\sigma(y x^{\\top}\\theta^{\\prime})))=0$ , for all $x,y$ . ", "page_idx": 6}, {"type": "text", "text": "We next show a convergence rate guarantee for AWS. ", "page_idx": 6}, {"type": "text", "text": "Theorem 3.6. Assume that $\\ell$ is a convex, $L$ -smooth function, there exists $\\Lambda^{*}$ such that $\\mathbb{E}[\\ell(x,y,\\theta^{*})]-$ $\\mathbb{E}[\\operatorname*{inf}_{\\boldsymbol{\\theta}}\\ell(x,y,\\boldsymbol{\\theta})]\\,\\leq\\,\\Lambda^{*}$ , and the sampling probability function $\\pi$ is such that, for some constant $c\\in(0,1).$ , for all $x,y,\\theta$ such that $||\\nabla_{\\theta}\\ell(x,y,\\theta)||>0$ , ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\pi(x,y,\\theta)\\geq\\frac{\\beta}{2(1-c)}\\operatorname*{min}\\left\\{\\rho\\psi(x,y,\\theta),1\\right\\}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Then, we have ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\frac{1}{n}\\sum_{t=1}^{n}(\\ell(x_{t},y_{t},\\theta_{t})-\\ell(x_{t},y_{t},\\theta_{t}^{*}))\\right]\\leq\\frac{\\rho\\beta}{c\\kappa}\\Lambda^{*}+\\frac{1}{2c\\kappa}||\\theta_{1}-\\theta^{*}||^{2}\\frac{1}{n}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\kappa=\\beta\\operatorname*{min}\\{1/(2L),\\rho\\}$ and $\\theta_{t}^{*}$ is a minimizer of $\\ell(x_{t},y_{t},\\theta^{\\prime})$ over $\\theta^{\\prime}$ . ", "page_idx": 7}, {"type": "text", "text": "The bound on the expected average loss in Theorem 3.6 boils down to $\\Lambda^{*}/c\\!+\\!\\left(L/(c\\beta)\\right)\\lvert\\lvert\\theta_{1}-\\theta^{*}\\rvert\\rvert^{2}/n$ by taking $\\rho=1/(2\\bar{L})$ . Notably, under the condition on the sampling probability in Theorem 3.6, the convergence rate is of order ${\\cal O}(1/n)$ . A similar bound is known to hold for SGD with adaptive stochastic Polyak step size for the finite-sum problem, as seen in Theorem 3.4 of Loizou et al. [2021]. A difference is that Theorem 3.6 allows for sampling of the points. ", "page_idx": 7}, {"type": "text", "text": "Loss and Uncertainty-based Sampling for Linear Binary Classifiers We consider linear binary classifiers, focusing particularly on logistic regression and the binary cross-entropy training loss function. The following corollaries of Theorem 3.6 hold for sampling proportional to absolute error loss and an uncertainty-based sampling probability function, respectively. ", "page_idx": 7}, {"type": "text", "text": "Corollary 3.7. For sampling proportional to absolute error loss, $\\pi(u)~=~\\omega(1\\,-\\,\\sigma(u))$ , with $\\beta/(4(1-c)L^{\\prime})\\leq\\omega\\leq1$ and $\\rho=1/(2L)$ , the bound on the expected loss in Theorem 3.6 holds. ", "page_idx": 7}, {"type": "text", "text": "Corollary 3.8. For the uncertainty-based sampling according to ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\pi(u)=\\frac{\\beta}{2(1-c)}\\operatorname*{min}\\left\\{\\rho R^{2}\\frac{1}{H(a)+(1-a)|u|},1\\right\\}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $a\\in(0,1/2]$ and $H(a)=a\\log(1/a)+(1-a)\\log(1/(1-a))$ , the bound on the expected loss in Theorem 3.6 holds. ", "page_idx": 7}, {"type": "text", "text": "Other Cases For a constant sampling probability function with a value of at least $\\kappa/{\\left(2(1-c)\\right)}$ , condition (6) holds when $\\kappa\\leq2(1-c)$ . When $\\pi(x,\\stackrel{\\cdot}{y},\\theta)=\\zeta(x,y,\\theta)^{\\eta}$ , where $\\eta\\geq0$ and $\\rho\\beta\\in(0,1]$ , condition (6) holds under $\\beta^{1-\\eta}\\le2(1-c)(1/(2L))^{\\eta}$ , as shown in Appendix A.14. Condition (6) also holds for an uncertainty-based sampling in multi-class classification, as shown in Appendix A.15. ", "page_idx": 7}, {"type": "text", "text": "4 Numerical Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section we evaluate our AWS algorithm, defined in Section 3.2. In particular, we focus on an instance of AWS with stochastic Polyak\u2019s expected step size for logistic regression and the loss-based sampling proportional to absolute error loss, which we refer to as Adaptive-Weight Sampling - Polyak Absloss (AWS-PA). By, Corollary 3.7, AWS-PA converges according to Theorem 3.6. Here we demonstrate convergence on real-world datasets and compare with other algorithms. ", "page_idx": 7}, {"type": "text", "text": "The implementation of AWS-PA algorithm along with all the other code run the experimental setup that is described in this section is available at https://www.github.com/facebookresearch/ AdaptiveWeightSampling. ", "page_idx": 7}, {"type": "text", "text": "We use a modified version of the mushroom binary classification dataset [Chang and Lin, 2011] that was used by Loizou et al. [2021] for evaluation of their algorithm. This modification uses RBF kernel features, resulting in a linearly separable dataset for a linear classifier like logistic regression. Furthermore, we include five datasets that we selected at random from the 44 real-world datasets that were used in Yang and Loog [2018], a benchmark study of active learning for logistic regression: MNIST 3 vs 5 LeCun et al. [1998], parkinsons Little et al. [2007], splice Noordewier et al. [1990], tictactoe Aha [1991], and credit Quinlan [1987]. While these datasets are not necessarily linearly separable, Yang and Loog [2018] has shown that logistic regression achieves a good quality-of-fit. ", "page_idx": 7}, {"type": "text", "text": "In our evaluation, we deliberately confine the training to a single epoch. Throughout this epoch, we sequentially process each data instance, compute the loss for each individual instance, and subsequently update the model\u2019s weights. This approach, known as progressive validation [Blum et al., 1999], enables us to monitor the evolution of the average loss. The constraint to a single epoch ensures that we calculate losses only for instances that haven\u2019t influenced model weights. For each sampling scheme, we conduct a hyper-parameter sweep to minimize the average progressive loss and apply a procedure to ensure that all algorithms sample comparable numbers of instances. ", "page_idx": 7}, {"type": "image", "img_path": "GLUIuli3Sm/tmp/cf5058dd3ded8b9e0b3ed2519e443716394caaacc6bed53fd945921aafc6a6e4.jpg", "img_caption": ["Figure 1: Convergence in terms of average cross-entropy progressive loss of random sampling, lossbased sampling based on the absolute error loss, and our proposed algorithm (loss-based sampling with stochastic Polyak\u2019s step size). Our proposed algorithm outperforms the baselines in most cases. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "In Appendix B.1 we include further details on this procedure, the hyper-parameter tuning, and other aspects of the experimental setup. ", "page_idx": 8}, {"type": "text", "text": "Figure 1 demonstrates that AWS-PA leads to faster convergence than the traditional loss-based sampling with a constant step size (akin to Yoo and Kweon [2019]). It also shows that the traditional loss-based sampling approach converges more rapidly than random sampling on five of the six datasets. These results are obtained under a hyper-parameter tuning such that different algorithms have comparable data sampling rates. We provide additional experimental results that demonstrate the efficiency of AWS-PA in Appendix B.2. ", "page_idx": 8}, {"type": "text", "text": "In active learning applications, the true loss of a point cannot be computed before the corresponding label is obtained. Hence, in practice we do not know the true loss at the moment of making the sampling decision. Therefore, we assess the effect of using a loss estimator, instead of using the true loss values. We use a Random Forest regressor to estimate absolute error loss based on the same set of features as the target model and the target\u2019s model prediction as an extra feature. We retrain this estimator on every sampling step using the labeled points observed so far. ", "page_idx": 8}, {"type": "text", "text": "Figure 2 demonstrates that AWS-PA with the estimated absolute error losses performs similarly on all datasets to AWS-PA with the true absolute error losses. Moreover, for a majority of the datasets, the two variants of AWS-PA achieve similar data sampling rates; this is shown Appendix B.3 along with further discussion. ", "page_idx": 8}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We have provided convergence rate guarantees for loss and uncertainty-based active learning algorithms under various assumptions. Furthermore, we introduced the novel Adaptive-Weight Sampling (AWS) algorithm that combines sampling with an adaptive size, conforming to stochastic Polyak\u2019s step size in expectation, and demonstrated its convergence rate guarantee, contingent on a condition related to the sampling probability function. ", "page_idx": 8}, {"type": "image", "img_path": "GLUIuli3Sm/tmp/eff8b6b40189b87f3041619474fc85520bec58cc718164467bc49a25ac193e0d.jpg", "img_caption": ["Figure 2: Active learning sampling based on an estimator of the absolute error loss performs on par with the sampling based on the ground truth value of absolute error loss. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "For future research, it would be interesting to establish tight convergence rates for the training loss function and the sampling cost, especially comparing policies using sampling with a constant probability with those using adaptive loss-based sampling probabilities. It would be interesting to explore adaptive-weight sampling algorithms with adaptive sizes different than those studied in this paper. Additionally, exploring a theoretical study on the impact of bias and noise in the loss estimator, used for evaluating the sampling probability function, on the convergence properties of algorithms could open up a valuable avenue for investigation. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Alekh Agarwal. Selective sampling algorithms for cost-sensitive multiclass prediction. In Sanjoy Dasgupta and David McAllester, editors, Proceedings of the 30th International Conference on Machine Learning, volume 28 of Proceedings of Machine Learning Research, pages 1220\u20131228, Atlanta, Georgia, USA, 17\u201319 Jun 2013. PMLR.   \nDavid Aha. Tic-Tac-Toe Endgame. UCI Machine Learning Repository, 1991. DOI: https://doi.org/10.24432/C5688J.   \nJames Bergstra, R\u00e9mi Bardenet, Yoshua Bengio, and Bal\u00e1zs K\u00e9gl. Algorithms for hyper-parameter optimization. Advances in Neural Information Processing Systems, 24, 2011.   \nJames Bergstra, Daniel Yamins, and David Cox. Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures. In Proceedings of the 30th International Conference on Machine Learning, pages 115\u2013123. PMLR, 2013.   \nAvrim Blum, Adam Kalai, and John Langford. Beating the hold-out: Bounds for $\\mathbf{k}$ -fold and progressive cross-validation. In Proceedings of the Twelfth Annual Conference on Computational Learning Theory (COLT), pages 203\u2013208, 1999.   \nS\u00e9bastien Bubeck. Convex optimization: Algorithms and complexity. Found. Trends Mach. Learn., 8 (3\u20134):231\u2013357, nov 2015.   \nGiovanni Cavallanti, Nicol\u00f2 Cesa-Bianchi, and Claudio Gentile. Learning noisy linear classifiers via adaptive and selective sampling. Machine Learning, 83(1):71\u2013102, 2011.   \nNicol\u00f3 Cesa-Bianchi, Claudio Gentile, and Luca Zaniboni. Worst-case analysis of selective sampling for linear classification. Journal of Machine Learning Research, 7(44):1205\u20131230, 2006.   \nNicol\u00f2 Cesa-Bianchi, Claudio Gentile, and Francesco Orabona. Robust bounds for classification via selective sampling. In Proceedings of the 26th Annual International Conference on Machine Learning, ICML $^{\\circ9}$ , page 121\u2013128, New York, NY, USA, 2009. Association for Computing Machinery.   \nChih-Chung Chang and Chih-Jen Lin. Libsvm: a library for support vector machines. ACM transactions on intelligent systems and technology (TIST), 2(3):1\u201327, 2011.   \nCody Coleman, Christopher Yeh, Stephen Mussmann, Baharan Mirzasoleiman, Peter Bailis, Percy Liang, Jure Leskovec, and Matei Zaharia. Selection via proxy: Efficient data selection for deep learning. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020, 2020.   \nKoby Crammer and Yoram Singer. On the algorithmic implementation of multiclass kernel-based vector machines. J. Mach. Learn. Res., 2:265\u2013292, mar 2002. ISSN 1532-4435.   \nSanjoy Dasgupta, Adam Tauman Kalai, and Claire Monteleoni. Analysis of perceptron-based active learning. Journal of Machine Learning Research, 10(11):281\u2013299, 2009.   \nOfer Dekel, Claudio Gentile, and Karthik Sridharan. Selective sampling and active learning from single and multiple teachers. J. Mach. Learn. Res., 13(1):2655\u20132697, sep 2012. ISSN 1532-4435.   \nYoav Freund, H. Sebastian Seung, Eli Shamir, and Naftali Tishby. Selective sampling using the query by committee algorithm. Machine Learning, 28(2):133\u2013168, 1997.   \nRan Gilad-bachrach, Amir Navot, and Naftali Tishby. Query by committee made real. In Y. Weiss, B. Sch\u00f6lkopf, and J. Platt, editors, Advances in Neural Information Processing Systems, volume 18. MIT Press, 2005.   \nAndreas Kirsch and Yarin Gal. Unifying approaches in active learning and active sampling via fisher information and information-theoretic quanties. Transactions on Machine Learning Research, 2022.   \nAndreas Kirsch, Joost van Amersfoort, and Yarin Gal. Batchbald: Efficient and diverse batch acquisition for deep bayesian active learning. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.   \nSalem Lahlou, Moksh Jain, Hadi Nekoei, Victor I Butoi, Paul Bertin, Jarrid Rector-Brooks, Maksym Korablyov, and Yoshua Bengio. Deup: Direct epistemic uncertainty prediction. Transactions on Machine Learning Research, 2022.   \nYann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.   \nDavid D. Lewis and William A. Gale. A sequential algorithm for training text classifiers. In Bruce W. Croft and C. J. van Rijsbergen, editors, SIGIR \u201994, pages 3\u201312, London, 1994. Springer London.   \nMax Little, Patrick Mcsharry, Stephen Roberts, Declan Costello, and Irene Moroz. Exploiting nonlinear recurrence and fractal scaling properties for voice disorder detection. Nature Precedings, pages 1\u20131, 2007.   \nShang Liu and Xiaocheng Li. Understanding uncertainty sampling, 2023. URL https://arxiv. org/abs/2307.02719.   \nNicolas Loizou, Sharan Vaswani, Issam Hadj Laradji, and Simon Lacoste-Julien. Stochastic Polyak step-size for SGD: An adaptive learning rate for fast convergence. In Arindam Banerjee and Kenji Fukumizu, editors, Proceedings of The 24th International Conference on Artificial Intelligence and Statistics, volume 130 of Proceedings of Machine Learning Research, pages 1306\u20131314. PMLR, 13\u201315 Apr 2021.   \nEdwin Lughofer and Mahardhika Pratama. Online active learning in data stream regression using uncertainty sampling based on evolving generalized fuzzy models. IEEE Transactions on Fuzzy Systems, 26(1):292\u2013309, 2018.   \nJian Luo, Jianzong Wang, Ning Cheng, and Jing Xiao. Loss prediction: End-to-end active learning approach for speech recognition. In 2021 International Joint Conference on Neural Networks (IJCNN), pages 1\u20137. IEEE, 2021.   \nS\u00f6ren Mindermann, Jan M Brauner, Muhammed T Razzak, Mrinank Sharma, Andreas Kirsch, Winnie Xu, Benedikt H\u00f6ltgen, Aidan N Gomez, Adrien Morisot, Sebastian Farquhar, et al. Prioritized training on points that are learnable, worth learning, and not yet learnt. In International Conference on Machine Learning, pages 15630\u201315649. PMLR, 2022.   \nStephen Mussmann and Percy S Liang. Uncertainty sampling is preconditioned stochastic gradient descent on zero-one loss. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018.   \nYuri Nesterov. Lectures on Convex Optimization. Springer, 2018.   \nMinh-Tien Nguyen, Guido Zuccon, Gianluca Demartini, et al. Loss-based active learning for named entity recognition. In 2021 International Joint Conference on Neural Networks (IJCNN), pages 1\u20138. IEEE, 2021.   \nVu-Linh Nguyen, Mohammad Hossein Shaker, and Eyke H\u00fcllermeier. How to measure uncertainty in uncertainty sampling for active learning. Machine Learning, 111(1):89\u2013122, 2022.   \nMichiel Noordewier, Geoffrey Towell, and Jude Shavlik. Training knowledge-based neural networks to recognize genes in dna sequences. Advances in neural information processing systems, 3, 1990.   \nFrancesco Orabona and Nicol\u00f2 Cesa-Bianchi. Better algorithms for selective sampling. In Proceedings of the 28th International Conference on International Conference on Machine Learning, ICML\u201911, page 433\u2013440, Madison, WI, USA, 2011. Omnipress. ISBN 9781450306195.   \nMansheej Paul, Surya Ganguli, and Gintare Karolina Dziugaite. Deep learning on a data diet: Finding important examples early in training. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages 20596\u201320607. Curran Associates, Inc., 2021.   \nJ. Ross Quinlan. Simplifying decision trees. International journal of man-machine studies, 27(3): 221\u2013234, 1987.   \nAnant Raj and Francis Bach. Convergence of uncertainty sampling for active learning. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 18310\u201318331. PMLR, 17\u201323 Jul 2022.   \nJason Rennie. Smooth hinge classification, 2005. URL http://qwone.com/\\~jason/writing/ smoothHinge.pdf.   \nJason Rennie and Nathan Srebro. Loss functions for preference levels: Regression with discrete ordered labels. Proceedings of the IJCAI Multidisciplinary Workshop on Advances in Preference Handling, 01 2005.   \nF. Rosenblatt. he perceptron: a probabilistic model for information storage and organization in the brain. Psychological review, 65(6):386\u2013408, 1958.   \nNicholas Roy and Andrew McCallum. Toward optimal active learning through sampling estimation of error reduction. In Proceedings of the Eighteenth International Conference on Machine Learning, page 441\u2013448, San Francisco, CA, USA, 2001. Morgan Kaufmann Publishers Inc.   \nGreg Schohn and David Cohn. Less is more: Active learning with support vector machines. In Proceedings of the Seventeenth International Conference on Machine Learning, ICML \u201900, page 839\u2013846, San Francisco, CA, USA, 2000. Morgan Kaufmann Publishers Inc.   \nBurr Settles. Active learning: Synthesis lectures on artificial intelligence and machine learning. Springer Cham, 2012.   \nBurr Settles, Mark Craven, and Soumya Ray. Multiple-instance active learning. In J. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems, volume 20. Curran Associates, Inc., 2007.   \nH. S. Seung, M. Opper, and H. Sompolinsky. Query by committee. COLT \u201992, page 287\u2013294, New York, NY, USA, 1992. Association for Computing Machinery. ISBN 089791497X.   \nJie Shen. On the power of localized perceptron for label-optimal learning of halfspaces with adversarial noise. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 9503\u20139514, 18\u201324 Jul 2021.   \nBen Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and Ari Morcos. Beyond neural scaling laws: beating power law scaling via data pruning. Advances in Neural Information Processing Systems, 35:19523\u201319536, 2022.   \nMariya Toneva, Alessandro Sordoni, Remi Tachet des Combes, Adam Trischler, Yoshua Bengio, and Geoffrey J. Gordon. An empirical study of example forgetting during deep neural network learning. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019, 2019.   \nRan Wang, Chi-Yin Chow, and Sam Kwong. Ambiguity-based multiclass active learning. IEEE Transactions on Fuzzy Systems, 24(1):242\u2013248, 2016.   \nSongbai Yan and Chicheng Zhang. Revisiting perceptron: Efficient and label-optimal learning of halfspaces. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.   \nY. Yang and M. Loog. Active learning using uncertainty information. In Proceedings of the International Conference on Pattern Recoginition (ICPR), page 2646\u20132651, 2016.   \nYazhou Yang and Marco Loog. A benchmark and comparison of active learning for logistic regression. Pattern Recognition, 83:401\u2013415, 2018.   \nYi Yang, Zhigang Ma, Feiping Nie, Xiaojun Chang, and Alexander G. Hauptmann. Multi-class active learning by uncertainty sampling with diversity maximization. International Journal of Computer Vision, 113(2):113\u2013127, 2015.   \nDonggeun Yoo and In So Kweon. Learning loss for active learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.   \nJingbo Zhu, Huizhen Wang, Benjamin K. Tsou, and Matthew Ma. Active learning with sampling by uncertainty and density for data annotations. IEEE Transactions on Audio, Speech, and Language Processing, 18(6):1323\u20131331, 2010. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Appendix: Proofs, Discussion, and Additional Results ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Limitations & Discussion ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "It remains an open problem to investigate the tightness of convergence rate bounds for constant-weight sampling under the assumptions outlined in Theorem 3.3. ", "page_idx": 14}, {"type": "text", "text": "The convergence rate results presented in Theorems 3.3 and 3.6 pertain to smooth convex training loss functions. Future research may explore weaker assumptions regarding the training loss function. ", "page_idx": 14}, {"type": "text", "text": "Regarding loss-based sampling strategies, our theoretical analysis assumes an unbiased and noiseless loss estimator. Extending this to account for estimation bias and noise would be a valuable avenue for further investigation. ", "page_idx": 14}, {"type": "text", "text": "Our numerical results demonstrate the effectiveness of our proposed algorithm and the robustness of our theoretical findings to loss estimation bias and noise across different datasets, utilizing the logistic regression model as a binary classifier. Future work could explore the application of other classification models, such as multi-layer neural networks. ", "page_idx": 14}, {"type": "text", "text": "A.2 A Set of Convergence Rate Conditions ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We present and prove two convergence rate lemmas for algorithm (1): the first providing sufficient conditions for a certain convergence rate and the second restricted to linear classifiers and linearly separable datasets. ", "page_idx": 14}, {"type": "text", "text": "The first lemma relies on the following condition involving the loss function $\\ell$ used by algorithm (1) and the loss function $\\tilde{\\ell}$ used for evaluating the performance of the algorithm. ", "page_idx": 14}, {"type": "text", "text": "Assumption A.1. There exist constants $\\alpha,\\beta>0$ such that for all $(x,y)\\in\\mathcal{X}\\times\\mathcal{Y}$ and $\\theta\\in\\Theta$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\pi(x,y,\\theta)||\\nabla_{\\theta}\\ell(x,y,\\theta)||^{2}\\leq\\alpha\\tilde{\\ell}(x,y,\\theta)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\pi(x,y,\\theta)\\nabla_{\\theta}\\ell(x,y,\\theta)^{\\top}(\\theta-\\theta^{*})\\geq\\beta\\tilde{\\ell}(x,y,\\theta).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Lemma A.2. Under Assumption A.1, for any $\\theta_{1}$ and $\\{\\theta_{t}\\}_{t>1}$ according to algorithm $(I)$ with $\\gamma=\\beta/\\alpha$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\sum_{t=1}^{n}\\tilde{\\ell}(x_{t},y_{t},\\theta_{t})\\right]\\leq||\\theta_{1}-\\theta^{*}||^{2}\\frac{\\alpha}{\\beta^{2}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Moreover, if for every $(x,y)\\in\\mathcal{X}\\times\\mathcal{Y},\\,\\tilde{\\ell}(x,y,\\theta)$ is a convex function in $\\theta$ , then we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\tilde{\\ell}(x,y,\\bar{\\theta}_{n})]\\leq||\\theta_{1}-\\theta^{*}||^{2}\\frac{\\alpha}{\\beta^{2}}\\frac{1}{n}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\begin{array}{r}{\\bar{\\theta}_{n}=(1/n)\\sum_{t=1}^{n}\\theta_{t}}\\end{array}$ . ", "page_idx": 14}, {"type": "text", "text": "Proof. For any $\\theta^{*}\\in\\Theta$ and $t\\geq1$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{||\\theta_{t+1}-\\theta^{*}||^{2}=||\\theta_{t}-\\theta^{*}||^{2}-2z_{t}\\nabla_{\\theta}\\ell(x_{t},y_{t},\\theta_{t})^{\\top}(\\theta_{t}-\\theta^{*})+z_{t}^{2}||\\nabla_{\\theta}\\ell(x_{t},y_{t},\\theta_{t})||^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Taking expectation in both sides of the equation, conditional on $x_{t},\\,y_{t}$ and $\\theta_{t}$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathbb{E}[||\\theta_{t+1}-\\theta^{*}||^{2}\\mid x_{t},y_{t},\\theta_{t}]}&{=}&{||\\theta_{t}-\\theta^{*}||^{2}-2\\gamma\\pi(x_{t},y_{t},\\theta_{t})\\nabla_{\\theta}\\ell(x_{t},y_{t},\\theta_{t})^{\\top}(\\theta_{t}-\\theta^{*})}\\\\ &&{+\\gamma^{2}\\pi(x_{t},y_{t},\\theta_{t})||\\nabla_{\\theta}\\ell(x_{t},y_{t},\\theta_{t})||^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Under Assumption A.1, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[||{\\theta}_{t+1}-{\\theta}^{*}||^{2}\\mid x_{t},y_{t},\\theta_{t}]\\leq||{\\theta}_{t}-{\\theta}^{*}||^{2}-2\\gamma\\beta\\tilde{\\ell}(x_{t},y_{t},\\theta_{t})+{\\gamma}^{2}\\alpha\\tilde{\\ell}(x_{t},y_{t},\\theta_{t}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Hence, it holds ", "page_idx": 14}, {"type": "equation", "text": "$$\n(2\\gamma\\beta-\\gamma^{2}\\alpha)\\mathbb{E}[\\tilde{\\ell}(x_{t},y_{t},\\theta_{t})]\\leq\\mathbb{E}[||\\theta_{t}-\\theta^{*}||^{2}]-\\mathbb{E}[||\\theta_{t+1}-\\theta^{*}||^{2}].\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Summing over $t$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n(2\\gamma\\beta-\\gamma^{2}\\alpha)\\sum_{t=1}^{n}\\mathbb{E}[\\widetilde{\\ell}(x_{t},y_{t},\\theta_{t})]\\leq||\\theta_{1}-\\theta^{*}||^{2}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "By taking $\\gamma=\\beta/\\alpha$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{n}\\mathbb{E}[\\tilde{\\ell}(x_{t},y_{t},\\theta_{t})]\\leq||\\theta_{1}-\\theta^{*}||^{2}\\frac{\\alpha}{\\beta^{2}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The second statement of the lemma follows from the last above inequality and Jensen\u2019s inequality. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "For the case of linear classifiers and linearly separable datasets, it can be readily checked that Lemma A.2 implies the following lemma. ", "page_idx": 15}, {"type": "text", "text": "Lemma A.3. Assume that there exist constants $\\alpha,\\beta>0$ such that for all $u\\in\\mathbb R$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\pi(u)\\ell^{\\prime}(u)^{2}R^{2}\\le\\alpha\\tilde{\\ell}(u)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\pi(u)(-\\ell^{\\prime}(u))(\\rho^{*}-u)\\geq\\beta\\tilde{\\ell}(u).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then, for any $\\theta_{1}$ such that $\\lvert|\\theta_{1}-\\theta^{*}\\rvert|\\leq S$ and $\\{\\theta_{t}\\}_{t>1}$ according to algorithm $(I)$ with $\\gamma=\\beta/\\alpha$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\sum_{t=1}^{n}\\tilde{\\ell}(y_{t}x_{t}^{\\top}\\theta_{t})\\right]\\leq S^{2}\\frac{\\alpha}{\\beta^{2}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Moreover, if $\\tilde{\\ell}$ is a convex function, then $\\mathbb{E}\\left[\\widetilde{\\ell}(y x^{\\top}\\bar{\\theta}_{n})\\right]\\leq S^{2}\\alpha/(\\beta^{2}n)$ where $\\begin{array}{r}{\\bar{\\theta}_{n}=(1/n)\\sum_{t=1}^{n}\\theta_{t}}\\end{array}$ . ", "page_idx": 15}, {"type": "text", "text": "A.2.1 Discussion ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We discuss some implications of conditions (9) and (10). Some of this discussion will help us identify the types of loss functions to which the conditions cannot be applied. ", "page_idx": 15}, {"type": "text", "text": "First, we note that $\\tilde{\\ell}(u)>0$ implies $\\pi(u)>0$ . Hence, equivalently, $\\pi(u)=0$ implies $\\tilde{\\ell}(u)=0$ . Second, we note that under conditions (9) and (10) it is necessary that for all $u\\in\\mathbb{R}$ , either ", "page_idx": 15}, {"type": "equation", "text": "$$\n-\\ell^{\\prime}(u)\\leq\\frac{\\alpha}{\\beta R^{2}}\\operatorname*{max}\\{\\rho^{*}-u,0\\}\\;\\mathrm{or}\\;\\pi(u)=0.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Hence, whenever $\\tilde{\\ell}(u)>0$ (and thus $\\pi(u)>0)$ , then $\\begin{array}{r}{-\\ell^{\\prime}(u)\\,\\le\\,\\frac{\\alpha}{\\beta R^{2}}\\operatorname*{max}\\{\\rho^{*}-u,0\\}}\\end{array}$ . The latter condition means that $\\ell^{\\prime}(u)=0$ whenever $u\\geq\\rho^{*}$ and otherwise the derivative of $\\ell$ at $u$ is bounded such that $\\ell^{\\prime}(u)\\geq-(\\alpha/(\\beta R^{2}))(\\rho^{*}-u)$ . In other words, function $\\ell$ must not decrease too fast on $(-\\infty,\\rho^{*}]$ . ", "page_idx": 15}, {"type": "text", "text": "Third, assume $\\ell$ and $\\tilde{\\ell}$ are such that $\\ell(u)=0$ and $\\tilde{\\ell}(u)=0$ for all $u\\geq1$ and $\\tilde{\\ell}(u)>0$ for all $u<1$ . Then, for every $u\\leq1$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\int_{u}^{1}(-\\ell(v))d v\\le\\frac{\\alpha}{\\beta R^{2}}\\int_{u}^{1}(\\rho^{*}-v)d v\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which by integrating is equivalent to ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\ell(u)\\leq\\frac{\\alpha}{\\beta R^{2}}\\left((\\rho^{*}-1)(1-u)+\\frac{1}{2}(1-u)^{2}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "This shows that $\\ell$ must be upper bounded by a linear combination of hinge and squared hinge loss function. ", "page_idx": 15}, {"type": "text", "text": "Forth, assume that $\\pi$ is decreasing in $u$ , then for every fixed $u_{0}\\in\\mathbb{R}$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\tilde{\\ell}(u)\\geq\\frac{\\pi(u_{0})R^{2}}{\\alpha}(-\\ell^{\\prime}(u))^{2}\\;\\mathrm{for}\\;\\mathrm{every}\\;u\\leq u_{0}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "If $\\tilde{\\ell}=\\ell$ , then ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\ell(u)\\leq\\left(\\sqrt{\\ell(u_{0})}+\\frac{1}{2R}\\sqrt{\\frac{\\alpha}{\\pi(u_{0})}}(u_{0}-u)\\right)^{2}{\\mathrm{~for~all~}}u\\leq u_{0}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Fifth, and last, assume that $\\pi$ is an even function and that there exists $c>0$ and $u_{0}\\leq\\rho^{*}$ such that $\\tilde{\\ell}(u)\\geq c$ for every $u\\leq u_{0}$ . Then, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\pi(u)=\\Omega\\left(\\frac{1}{|u|^{2}}\\right)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which limits the rate at which $\\pi(u)$ is allowed to decrease with $|u|$ . To see, this, from conditions (9) and (10), for every $u\\leq\\rho^{*}$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\tilde{\\ell}(u)\\leq\\frac{\\alpha}{\\beta^{2}R^{2}}\\pi(u)(\\rho^{*}-u)^{2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Hence, $\\pi(u)\\geq(c\\beta^{2}R^{2}/\\alpha)/(\\rho^{*}-u))$ for every $u\\leq u_{0}$ . The lower bound is tight in case when $\\ell$ is squared hinge loss function and ${\\tilde{\\ell}}(u)=c$ for every $u\\leq u_{0}<1$ . In this case, from conditions (9) and (10), for every $u\\leq u_{0}$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\nc\\beta\\frac{1}{(1-u)(\\rho^{*}-u)}\\leq\\pi(u)\\leq\\frac{c\\alpha}{R^{2}}\\frac{1}{(1-u)^{2}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which implies $\\pi(u)=\\Theta(1/|u|^{2})$ . ", "page_idx": 16}, {"type": "text", "text": "A.3 Proof of Theorem 3.1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We show that conditions of the theorem imply conditions (9) and (10) to hold, for $\\tilde{\\ell}=\\ell$ , which in turn imply conditions (7) and (8) and hence we can apply the convergence result of Lemma A.3. ", "page_idx": 16}, {"type": "text", "text": "We first consider condition (7). For squared hinge loss function $\\ell$ and $\\tilde{\\ell}=\\ell$ , clearly condition (7) holds for every $u\\geq1$ as in this case both side of the inequality are equal to zero. For every $u\\leq1$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{\\ell(u)}{\\ell^{\\prime}(u)^{2}}=\\frac{1}{2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Since by assumption $\\pi(u)\\le\\beta/2$ for all $u$ , condition $\\alpha/\\beta\\geq R^{2}$ implies condition (7). ", "page_idx": 16}, {"type": "text", "text": "We next consider condition (8). Again, clearly, condition holds for every $u\\geq1$ as in this case both sides of the inequality are equal to zero. For $u\\leq1$ , we can write (8) as follows ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\pi(u)\\ge\\beta\\frac{\\ell(u)}{\\left(-\\ell^{\\prime}(u)\\right)(\\rho^{*}-u)}}&{=}&{\\displaystyle\\frac{\\beta}{2}\\frac{1-u}{\\rho^{*}-u}}\\\\ &{=}&{\\displaystyle\\frac{\\beta}{2}\\left(1-\\frac{1}{1+(1/(\\rho^{*}-1))(1-u)}\\right)}\\\\ &{=}&{\\displaystyle\\frac{\\beta}{2}\\left(1-\\frac{1}{1+(\\sqrt{2}/(\\rho^{*}-1))\\sqrt{\\ell(u)}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "This condition is implied by $\\pi(u)\\geq\\pi^{*}(\\ell(u))$ where ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\pi^{*}(\\ell)=\\frac{\\beta}{2}\\left(1-\\frac{1}{1+\\mu\\sqrt{\\ell}}\\right)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "with $\\mu\\geq\\sqrt{2}/(\\rho^{*}-1)$ . ", "page_idx": 16}, {"type": "text", "text": "The result of the theorem follows from Lemma A.3 with $\\alpha/\\beta=R^{2}$ , $0<\\beta\\leq2$ and $\\pi(u)\\geq\\pi^{*}(\\ell(u))$ for all $u\\leq1$ . ", "page_idx": 16}, {"type": "text", "text": "For the expected number of samples we proceed as follows. First by concavity and monotonicity of function $\\pi^{*}$ and the expected loss bound, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\sum_{t=1}^{n}\\pi^{*}(\\ell(x_{t},y_{t},\\theta_{t}))\\right]\\leq\\pi^{*}\\left(\\mathbb{E}\\left[\\frac{1}{n}\\sum_{t=1}^{n}\\ell(x_{t},y_{t},\\theta_{t})\\right]\\right)\\,n\\leq\\pi^{*}\\left(\\frac{\\|\\theta_{1}-\\theta^{*}\\|^{2}R^{2}}{\\beta n}\\right)\\,n.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then, combined with the fact $\\begin{array}{r}{\\pi^{\\ast}(v)\\leq\\frac{\\beta\\mu}{2}\\sqrt{v}}\\end{array}$ for all $v\\geq0$ , it follows ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\sum_{t=1}^{n}\\pi^{*}(\\ell(x_{t},y_{t},\\theta_{t}))\\right]\\leq||\\theta_{1}-\\theta^{*}||\\frac{\\mu\\sqrt{\\beta}}{2}\\sqrt{n}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Since $\\pi^{*}(v)\\leq\\beta/2$ for all $v$ , it obviously holds $\\begin{array}{r}{\\mathbb{E}\\left[\\sum_{t=1}^{n}\\pi^{*}(\\ell(x_{t},y_{t},\\theta_{t}))\\right]\\leq(\\beta/2)n}\\end{array}$ , which completes the proof of the theorem. ", "page_idx": 17}, {"type": "text", "text": "A.4 Linear Classifiers: Zero-one Loss and Absolute Error Loss-based Sampling ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Here we consider training loss functions satisfying: ", "page_idx": 17}, {"type": "text", "text": "Assumption A.4. Function $\\ell$ is continuously differentiable on $(-\\infty,0].$ , convex, and $\\ell^{\\prime}(0)\\le-c_{1}$ and $\\begin{array}{r}{\\operatorname*{lim}_{u\\to-\\infty}\\ell^{\\prime}(u)\\geq-c_{2},}\\end{array}$ , for some constants $c_{1},c_{2}>0$ . ", "page_idx": 17}, {"type": "text", "text": "We consider sampling proportional to either zero-one loss or absolute error loss. The zero-one loss is defined as $\\ell_{01}(u):=\\mathbb{1}_{\\{u\\leq0\\}}=\\mathbb{1}_{\\{y\\neq\\mathrm{sgn}(x^{\\top}\\theta)\\}}$ . The absolute error loss is defined as $\\ell_{\\mathrm{abs}}(u):=$ $\\begin{array}{r}{2\\left(\\mathbb{1}_{\\{u<0\\}}+\\frac{1}{2}\\mathbb{1}_{\\{u=0\\}}\\right)\\,=\\,\\lvert y-\\mathrm{sgn}(x^{\\top}\\theta)\\rvert}\\end{array}$ . Sampling proportional to zero-one loss is defined as $\\pi(u)=\\omega\\ell_{01}(u)$ for some constant $\\omega\\in(0,1]$ , while sampling proportional to absolute error loss is defined as $\\pi(\\dot{u})=\\omega\\ell_{\\mathrm{abs}}(u)$ for some constant $\\omega\\in(0,1/2]$ . ", "page_idx": 17}, {"type": "text", "text": "Theorem A.5. Assume that the loss function $\\ell$ satisfies Assumption A.4 and $\\rho^{*}>0$ . Then, under sampling proportional to zero-one loss, for any initial value $\\theta_{1}$ such that $||\\theta_{1}-\\theta^{*}||\\leq S$ and $\\{\\theta_{t}\\}_{t>1}$ according to algorithm $(I)$ with $\\gamma=c_{1}\\dot{\\rho}^{*}/(c_{2}^{2}R^{2})$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\sum_{t=1}^{n}\\ell_{01}(y_{t}x_{t}^{\\top}\\theta_{t})\\right]\\leq\\frac{c_{2}^{2}R^{2}S^{2}}{c_{1}^{2}\\omega}\\frac{1}{\\rho^{*\\,2}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Furthermore, under sampling proportional to absolute error loss and $\\gamma=2c_{1}\\rho^{*}/(c_{2}^{2}R^{2}),$ , the bound in $(I I)$ holds but with an additional factor of 2. ", "page_idx": 17}, {"type": "text", "text": "Proof is provided in Appendix A.4.1. ", "page_idx": 17}, {"type": "text", "text": "The bound in (11) is a well-known bound on the number of mistakes made by the perceptron algorithm, of the order $O(1/\\rho^{*2})$ . It can be readily observed that under sampling proportional to zero-one loss, the expected number of sampled points is bounded by $(c_{2}/c_{1})^{2}R^{2}S^{2}/\\rho^{*}{}^{2}$ , which also holds for sampling proportional to absolute error loss but with an additional factor of 4. ", "page_idx": 17}, {"type": "text", "text": "A.4.1 Proof of Theorem A.5 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We first consider the case when sampling is proportional to zero-one loss. We show that conditions of the theorem imply conditions (9) and (10) to hold, for $\\tilde{\\ell}=\\ell_{01}$ and $\\pi=\\omega\\ell_{01}$ , which in turn imply conditions (7) and (8) and hence we can apply the convergence result of Lemma A.3. ", "page_idx": 17}, {"type": "text", "text": "Conditions (9) and (10) are equivalent to: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\omega\\ell^{\\prime}(u)^{2}R^{2}\\leq\\alpha,\\;\\mathrm{for}\\;\\mathrm{every}\\;u\\leq0\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and ", "page_idx": 17}, {"type": "equation", "text": "$\\omega(-\\ell^{\\prime}(u))(\\rho^{*}-u)\\geq\\beta$ ", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Since $\\ell$ is a convex function $\\ell^{\\prime}(u)^{2}$ is decreasing in $u$ and $(-\\ell^{\\prime}(u))(\\rho^{*}\\mathrm{~-~}u)$ is decreasing in $u$ . Therefore, conditions are equivalent to ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\omega(\\operatorname*{lim}_{u\\rightarrow-\\infty}(-\\ell^{\\prime}(u)))^{2}R^{2}\\leq\\alpha\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\omega(-\\ell^{\\prime}(0))\\rho^{*}\\geq\\beta.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "These conditions hold true by taking $\\alpha=\\omega c_{2}^{2}R^{2}$ and $\\beta=\\omega c_{1}\\rho^{*}$ . ", "page_idx": 17}, {"type": "text", "text": "We next consider the case when sampling is proportional to absolute error loss. We show that conditions of the theorem imply conditions (9) and (10) to hold, for $\\tilde{\\ell}=\\ell_{01}$ and $\\pi=\\omega\\ell_{\\mathrm{abs}}$ , which in turn imply conditions (7) and (8) and hence we can apply the convergence result of Lemma A.3. ", "page_idx": 17}, {"type": "text", "text": "Conditions (9) and (10) correspond to ", "page_idx": 18}, {"type": "text", "text": "and ", "page_idx": 18}, {"type": "text", "text": "Again, since $(-\\ell^{\\prime}(u))^{2}$ is decreasing in $u$ and $(-\\ell^{\\prime}(u))(\\rho^{*}-u)$ is decreasing in $u$ , it follows that the conditions are equivalent to ", "page_idx": 18}, {"type": "equation", "text": "$$\n2\\omega(\\operatorname*{lim}_{u\\rightarrow-\\infty}(-\\ell^{\\prime}(u)))^{2}R^{2}\\leq\\alpha\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\omega(-\\ell^{\\prime}(0))\\rho^{*}\\geq\\beta.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Hence, conditions (9) and (10) hold by taking $\\alpha=2\\omega c_{2}^{2}R^{2}$ and $\\beta=\\omega c_{1}\\rho^{*}$ . ", "page_idx": 18}, {"type": "text", "text": "A.5 Linear Classifiers: Generalized Smooth Hinge Loss Function ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Here we consider the training loss function corresponding to the generalized smooth hinge loss function [Rennie, 2005], defined for $a\\ge1$ as follows: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\ell(u)={\\left\\{\\begin{array}{l l}{{\\frac{a}{a+1}}-u}&{{\\mathrm{if~}}u\\leq0}\\\\ {{\\frac{a}{a+1}}-u+{\\frac{1}{a+1}}u^{a+1}}&{{\\mathrm{if~}}0\\leq u\\leq1}\\\\ {0}&{{\\mathrm{otherwise}}.}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This is a continuously differentiable function that converges to the value of the hinge loss function, $\\operatorname*{max}\\{1-u,0\\}$ , as $a$ goes to infinity. The family of loss functions parameterized by $a$ accommodates the smooth hinge loss function with $a=1$ , introduced by Rennie and Srebro [2005]. ", "page_idx": 18}, {"type": "text", "text": "Theorem A.6. Assume that $\\rho^{*}>1$ , the loss function is the generalized smooth hinge loss function, and the sampling probability is according to the function $\\pi^{*}$ , which, for $\\beta\\in(0,1]$ and $\\rho\\in(1,\\rho^{*}],$ , is defined as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\pi^{*}(u)=\\left\\{\\begin{array}{l l}{\\beta\\frac{\\frac{a}{a+1}-u}{\\rho-u}}&{i f u\\leq0}\\\\ {\\beta\\frac{\\frac{a}{a+1}\\frac{1}{1-u^{a}}(1-u)-\\frac{1}{a+1}u}{\\rho-u}}&{i f0\\leq u\\leq1}\\\\ {0}&{i f u\\geq1.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then, for any initial value $\\theta_{1}$ such that $||\\theta_{1}-\\theta^{*}||\\leq S$ and $\\{\\theta_{t}\\}_{t>1}$ according to algorithm $(I)$ with $\\gamma=\\Bar1/(c_{a,\\rho}\\Bar{R}^{2})$ , where $c_{a,\\rho}=a/(a(\\rho-1)+1)$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\ell(y x^{\\top}\\bar{\\theta}_{n})\\right]\\leq\\mathbb{E}\\left[\\frac{1}{n}\\sum_{t=1}^{n}\\ell(y_{t}x_{t}^{\\top}\\theta_{t})\\right]\\leq c_{a,\\rho}\\frac{R^{2}S^{2}}{\\beta}\\frac{1}{n}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Furthermore, the same bound holds for every $\\pi$ such that $\\pi^{*}(u)\\leq\\pi(u)\\leq\\beta$ for all $u\\in\\mathbb{R},$ , with $c_{a,\\rho}=2a$ . ", "page_idx": 18}, {"type": "text", "text": "Proof is provided in Appendix A.5.1. ", "page_idx": 18}, {"type": "text", "text": "Note that $\\pi^{*}$ is increasing in $a$ and is upper-bounded by $\\pi^{**}(u)=\\beta(1-u)/(\\rho-1)$ , which may be regarded as the limit for the hinge loss function. See Figure 3 for an illustration. ", "page_idx": 18}, {"type": "text", "text": "We remark that the expected loss bound in Theorem A.6 is the same as for the squared hinge loss function in Theorem 3.1, except for an additional factor $c_{a,\\rho}=a/(a(\\rho-1)+\\bar{1})$ . This factor is increasing in $a$ but always lies in the interval $[1/\\rho,1/(\\rho\\!-\\!1)]$ , where the boundary values are achieved for $a=1$ and $a\\to\\infty$ , respectively. ", "page_idx": 18}, {"type": "text", "text": "Theorem A.7. Assuming $\\rho^{*}\\,>\\,1$ , the loss function is the generalized smooth hinge loss, and the sampling probability function $\\pi^{*}$ satisfies the assumptions in Theorem A.6, with $n\\,>\\,((a\\,+$ $1)/a)c_{a,\\rho}\\bar{R^{2}S^{2}}/\\beta$ , the expected number of sampled points is bounded as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\sum_{t=1}^{n}\\pi^{*}(y_{t}x_{t}^{\\top}\\theta_{t})\\right]\\leq\\kappa\\operatorname*{max}\\left\\{\\frac{\\sqrt{\\beta c_{a,\\rho}}R S}{(\\rho-1)\\sqrt{a}}\\sqrt{n},\\frac{c_{a,\\rho}R^{2}S^{2}}{\\rho-1}\\right\\},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\kappa$ is some positive constant. ", "page_idx": 18}, {"type": "image", "img_path": "GLUIuli3Sm/tmp/1743fbc16b92b054aab34a8ca5f1f70598f474d317a9712966912ca75439b25a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 3: Sampling probability function for the family of generalized smooth hinge loss functions. ", "page_idx": 19}, {"type": "text", "text": "Proof is provided in Appendix A.5.2. ", "page_idx": 19}, {"type": "text", "text": "For any fixed number of iterations $n$ satisfying the condition of the theorem, the expected number of sampled points is bounded by a constant for sufficiently large value of parameter $a$ . The bound in Theorem A.7 depends on how the loss function $\\ell(u)$ varies with $u$ . When $a$ is large, $\\ell(u)$ is approximately $1-u$ (hinge loss), otherwise, it is approximately $\\textstyle{\\frac{a}{2}}(1-u)^{2}$ for $0\\leq u\\leq1$ (squared hinge loss). ", "page_idx": 19}, {"type": "text", "text": "A.5.1 Proof of Theorem A.6 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Assume that $\\pi^{*}$ is such that for given $\\rho\\in(1,\\rho^{*}],\\,\\pi^{*}(u)(-\\ell^{\\prime}(u))(\\rho-u)=\\beta\\ell(u)$ for all $u\\in\\mathbb R$ . Then, $\\pi^{*}$ satisfies equation (10) for all $u\\in\\mathbb R$ . Note that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\pi^{*}(u)=\\beta{\\frac{\\ell(u)}{(-\\ell^{\\prime}(u))(\\rho-u)}}={\\left\\{\\begin{array}{l l}{\\beta{\\frac{{\\frac{a}{a+1}}-u}{\\rho-u}}}&{{\\mathrm{~if~}}u\\leq0}\\\\ {\\beta\\left({\\frac{a}{a+1}}{\\frac{1}{1-u^{a}}}(1-u)-{\\frac{1}{a+1}}u\\right){\\frac{1}{\\rho-u}}}&{{\\mathrm{~if~}}0\\leq u\\leq1}\\\\ {0}&{{\\mathrm{~if~}}u\\geq1.}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "By condition (9), we must have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{-\\ell^{\\prime}(u)}{\\rho-u}\\leq\\frac{\\alpha}{\\beta R^{2}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Note that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\ell^{\\prime}(u)=\\left\\{\\begin{array}{l l}{-1}&{\\mathrm{~if~}u\\leq0}\\\\ {-(1-u^{a})}&{\\mathrm{~if~}0\\leq u\\leq1}\\\\ {0}&{\\mathrm{~otherwise.}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Hence, for every $u\\leq0$ , it must hold ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{1}{\\rho-u}\\leq\\frac{\\alpha}{\\beta R^{2}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which is equivalent to $\\alpha/\\beta\\le R^{2}/\\rho$ . For every $0\\leq u\\leq1$ , it must hold ", "page_idx": 19}, {"type": "equation", "text": "$$\nf_{a}(u):=\\frac{1-u^{a}}{\\rho-u}\\leq\\frac{\\alpha}{\\beta R^{2}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Thus, it must hold $\\alpha/\\beta\\geq c_{a,\\rho}R^{2}$ where $c_{a,\\rho}=\\operatorname*{sup}_{u\\in[0,1]}f_{a}(u)$ . ", "page_idx": 19}, {"type": "text", "text": "Function $f_{a}$ has boundary values $f_{a}(0)=1/\\rho$ and $f_{a}(1)=0$ . Furthermore, note ", "page_idx": 19}, {"type": "equation", "text": "$$\nf_{a}^{\\prime}(u)=\\frac{1+(a-1)u^{a}-\\rho a u^{a-1}}{(\\rho-u)^{2}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Note $f_{a}(0)=1/\\rho^{2}>0$ and $f_{a}^{\\prime}(1)=-(\\rho-1)a<0$ . Let $u_{*}$ be such that $f_{a}^{\\prime}(u_{*})=0$ , which holds if, and only if, $g_{a}(u_{*}):=(a-1){u_{*}}^{a}-{\\rho}a u_{*}^{\\ a-1}+1=0$ . ", "page_idx": 19}, {"type": "text", "text": "Note that ", "page_idx": 20}, {"type": "equation", "text": "$$\nc_{a,\\rho}=\\operatorname*{sup}_{u\\in[0,1]}f_{a}(u)=f_{a}(u_{*})=a u_{*}^{a-1}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "For $a=1$ , $f_{1}$ is decreasing on $[0,1]$ hence $c_{1,\\rho}=f_{1}(0)=1/\\rho$ . For $a=2$ , $u_{*}$ is a solution of a quadratic equation, and it can be readily shown that $c_{2,\\rho}=2\\rho(1-\\sqrt{\\rho^{2}-1})$ . For every $a\\ge1$ , we have $c_{a,\\rho}\\leq a/(1+a(\\rho-1))$ . This obviously holds with equality for $a=1$ , hence it suffices to show that the inequality holds for $a>1$ . ", "page_idx": 20}, {"type": "text", "text": "Consider the case $a>1$ . Note that $g_{a}(u_{*})=0$ is equivalent to ", "page_idx": 20}, {"type": "equation", "text": "$$\na u_{*}^{a-1}=\\frac{1}{\\rho-\\frac{a-1}{a}u_{*}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Combined with the fact $u_{*}\\in[0,1]$ , it immediately follows ", "page_idx": 20}, {"type": "equation", "text": "$$\na u_{*}^{a-1}\\leq\\frac{a}{a(\\rho-1)+1}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Furthermore, note that $\\begin{array}{r}{\\operatorname*{lim}_{a\\to\\infty}c_{a,\\rho}=1/(\\rho-1)}\\end{array}$ . To see this, consider (13). Note that $u_{*}$ goes to 1 as $a$ goes to infinity. This can be shown by contradiction as follows. Assume that there exists a constant $c\\in[0,1)$ and $a_{0}$ such that $u_{*}\\leq c$ for all $a\\geq a_{0}$ . Then, from (13), $a c^{a-1}\\geq1/\\rho^{*}$ . The left-hand side in the last inequality goes to 0 as $a$ goes to infinity while the right-hand side is a constant greater than zero, which yields a contradiction. From (13), it follows that $a\\bar{u}_{*}^{a-1}$ goes to $1/(\\rho-1)$ as $a$ goes to infinity. ", "page_idx": 20}, {"type": "text", "text": "We prove the second statement of the theorem as follows. It suffices to show that condition (9) holds true as condition (10) clearly holds for every $\\pi$ such that $\\pi(u)\\geq\\pi^{*}(u)$ for every $u\\in\\mathbb R$ . For condition (9) to hold, it is sufficient that ", "page_idx": 20}, {"type": "equation", "text": "$$\nf(u):={\\frac{\\ell(u)}{\\ell^{\\prime}(u)^{2}}}\\geq{\\frac{\\beta R^{2}}{\\alpha}},{\\mathrm{~for~all~}}u\\leq1.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Note that ", "page_idx": 20}, {"type": "equation", "text": "$$\nf(u)={\\left\\{\\begin{array}{l l}{{\\frac{a}{a+1}}-u}&{{\\mathrm{~if~}}u\\leq0}\\\\ {{\\frac{1}{a+1}}{\\frac{u^{a+1}-(a+1)u+a}{(1-u^{a})^{2}}}}&{{\\mathrm{~if~}}0\\leq u\\leq1.}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Function $f$ is a decreasing function. This is obviously true for $u\\leq0$ . For $0\\leq u\\leq1$ , we show this as follows. Note that ", "page_idx": 20}, {"type": "equation", "text": "$$\nf^{\\prime}(u)={\\frac{2a u^{2a}-(2a-1)(a+1)u^{a}+2a^{2}u^{a-1}-(a+1)}{(a+1)(1-u^{a})^{3}}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Hence, $f^{\\prime}(u)\\leq0$ is equivalent to ", "page_idx": 20}, {"type": "equation", "text": "$$\n2a u^{a-1}(u^{a+1}+a)\\leq(a+1)(1-(2a-1)u^{a}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "In the last inequality, the left-hand side is increasing in $u$ and the right-hand side is decreasing in $u$ Hence the inequality holds for every $u\\in[0,1]$ is equivalent to the inequality holding for $u=1$ . For $u=1$ , the inequality holds with equality. ", "page_idx": 20}, {"type": "text", "text": "Note that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{f(1)}&{=}&{\\displaystyle\\frac{0}{0}}\\\\ &{=}&{\\displaystyle\\frac{1}{a+1}\\frac{(a+1)u^{a}-(a+1)}{-2(1-u^{a})a u^{a-1}}|_{u=1}=\\frac{0}{0}}\\\\ &{=}&{\\displaystyle\\frac{1}{a+1}\\frac{(a+1)a u^{a-1}}{2a^{2}u^{2(a-1)}-2(1-u^{a})a(a-1)u^{a-2}}|_{u=1}=\\frac{1}{a+1}\\frac{(a+1)a}{2a^{2}}}\\\\ &{=}&{\\displaystyle\\frac{1}{2a}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "It follows that $\\begin{array}{r}{\\operatorname*{inf}_{u\\leq1}f(u)=f(1)=\\frac{1}{2a}\\leq\\alpha/(\\beta R^{2})}\\end{array}$ , hence it is suffices that $\\alpha/\\beta\\geq R^{2}/(2a)$ . ", "page_idx": 20}, {"type": "text", "text": "A.5.2 Proof of Theorem A.7 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Lemma A.8. For every $a\\geq1$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\pi^{\\ast}(u)\\leq\\pi^{\\ast\\ast}(u)=\\beta\\frac{1-u}{\\rho-1}\\,f o r\\,e\\nu e r y\\,u\\leq1.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. For $u\\leq1$ , $\\pi^{*}(u)=\\beta(a/(a+1)-u)/(\\rho-u)$ , so obviously, $\\pi^{*}(u)\\leq\\beta(1-u)/(\\rho-u)$ . For $0\\leq u\\leq1$ , we show that next that $a(1\\stackrel{.}{-}u)/(1\\stackrel{.}{-}u^{a})-u\\leq a(1\\stackrel{.}{-}u)$ , which implies that $\\pi^{*}(u)\\leq(a/(a+1))\\beta(1-u)/(\\rho-u)$ . To show the asserted inequality, by straightforward calculus it can be shown that the inequality is equivalent to $(1-(1-u))^{\\cdot1-a}\\stackrel{\\cdot}{\\geq}1-(1\\stackrel{\\cdot}{-}a)(1-u)$ which clearly holds true. \u53e3 ", "page_idx": 21}, {"type": "text", "text": "Lemma A.9. For every $c\\in(1,6/5)$ , for every $0\\leq u\\leq1$ , $i f(a+1)(1-u)\\geq c,$ , then ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\ell(u)\\geq\\left(1-{\\frac{1}{c}}\\right)(1-u)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and, otherwise, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\ell(u)\\geq\\left(1-{\\frac{1}{3}}{\\frac{c}{1-c/2}}\\right){\\frac{a}{2}}(1-u)^{2}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. We first show the first inequality. For $u\\leq1$ , it clearly holds $\\ell(u)\\leq1-u$ , and ", "page_idx": 21}, {"type": "equation", "text": "$$\n{\\frac{{\\frac{a}{a+1}}-u}{1-u}}={\\frac{1-u-{\\frac{1}{a+1}}}{1-u}}=1-{\\frac{1}{(a+1)(1-u)}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "thus, for every $c>1$ , $\\ell(u)\\geq(1-1/c)(1-u)$ whenever $(a+1)(1-u)\\geq c$ . ", "page_idx": 21}, {"type": "text", "text": "For $0\\leq u<1$ , it holds $\\ell(u)=1-u-(1/(a+1))(1-u^{a+1})$ , hence it clearly holds $\\ell(u)\\leq1-u$ . Next, note ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{\\ell(u)}{1-u}=1\\frac{1-u^{a+1}}{(a+1)(1-u)}\\geq1-\\frac{1}{(a+1)(1-u)}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Hence, again, for every $c>1,\\ell(u)\\geq(1-1/c)(1-u)$ whenever $(a+1)(1-u)\\geq c$ . ", "page_idx": 21}, {"type": "text", "text": "We next show the second inequality. For $0<u\\leq1$ , note that $\\ell^{\\prime}(u)=-(1-u^{a})$ , $\\ell^{\\prime\\prime}(u)=a u^{a-1}$ and $\\ell^{\\prime\\prime\\prime}(u)=a(a-1)u^{a-2}$ . In particular, $\\ell^{\\prime}(1)=0$ , $\\ell^{\\prime\\prime}(1)=a$ and $\\ell^{\\prime\\prime\\prime}(1)=a(a-1)$ . By limited Taylor development, for some $u_{0}\\in[u,1]$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\ell(u)=\\frac{a}{2}(1-u)^{2}-\\frac{a(a-1)}{6}u_{0}^{a-2}(1-u)^{3}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "From this, it immediately follows that $\\begin{array}{r}{\\ell(u)\\leq\\frac{a}{2}(1-u)^{2}}\\end{array}$ for every $u\\leq1$ . For the case $a\\ge2$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\ell(u)\\geq{\\frac{a}{2}}(1-u)^{2}\\left(1-{\\frac{1}{3}}(a-1)(1-u)\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Hence, for every $\\begin{array}{r}{c\\geq0,\\ell(u)\\geq\\frac{a}{2}(1-u)^{2}(1-c/3)}\\end{array}$ whenever $(a+1)(1-u)\\leq c.$ . For $1\\le a\\le2$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\ell(u)\\geq{\\frac{a}{2}}(1-u)^{2}\\left(1-{\\frac{1}{3}}{\\frac{(a-1)(1-u)}{u^{2-a}}}\\right)\\geq{\\frac{a}{2}}(1-u)^{2}\\left(1-{\\frac{1}{3}}{\\frac{(a-1)(1-u)}{u}}\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Under $(a+1)(1-u)\\leq c$ , with $0\\le c<2$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n{\\frac{(a-1)(1-u)}{u}}\\leq{\\frac{c}{1-{\\frac{c}{a+1}}}}\\leq{\\frac{c}{1-{\\frac{c}{2}}}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Hence, it follows ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\ell(u)\\geq{\\frac{a}{2}}(1-u)^{2}\\left(1-{\\frac{1}{3}}{\\frac{c}{1-{\\frac{c}{2}}}}\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Next, note that for every $x,y,\\theta$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\pi^{*}(y x^{\\top}\\theta)\\leq\\pi^{**}(y x^{\\top}\\theta)\\leq\\frac{\\beta}{\\rho^{*}-1}(1-y x^{\\top}\\theta)=\\frac{\\beta}{\\rho^{*}-1}(1-\\ell^{-1}(\\ell(x,y,\\theta)))\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\ell^{-1}$ is the inverse function of $\\ell(u)$ for $u<1$ and $\\ell^{-1}(0)=0$ . ", "page_idx": 22}, {"type": "text", "text": "Hence, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{n}\\pi^{*}(y_{t}x_{t}^{\\top}\\theta_{t})\\right]}&{\\leq}&{\\displaystyle\\frac{\\beta}{\\rho-1}\\left(1-\\ell^{-1}\\left(\\mathbb{E}\\left[\\frac{1}{n}\\sum_{t=1}^{n}\\ell(x_{t},y_{t},\\theta_{t})\\right]\\right)\\right)n}\\\\ &{\\leq}&{\\displaystyle\\frac{\\beta}{\\rho-1}\\left(1-\\ell^{-1}\\left(c_{a,\\rho}\\frac{\\lvert|\\theta_{1}-\\theta^{*}\\rvert|^{2}R^{2}}{\\beta}\\frac{1}{n}\\right)\\right)n}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the first inequality is by concavity of the function $1-\\ell^{-1}(\\ell)$ and the second inequality is by Theorem A.6. ", "page_idx": 22}, {"type": "text", "text": "To apply Lemma A.9, we need that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\ell^{-1}\\left(c_{a,\\rho}\\frac{||\\theta_{1}-\\theta^{*}||^{2}R^{2}}{\\beta}\\frac{1}{n}\\right)>0\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which is equivalent to ", "page_idx": 22}, {"type": "text", "text": "i.e. ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle{c_{a,\\rho}\\frac{||\\theta_{1}-\\theta^{*}||^{2}R^{2}}{\\beta}\\frac{1}{n}<\\ell(0)=\\frac{a}{a+1}}}\\\\ {\\displaystyle{n>\\frac{a+1}{a}c_{a,\\rho}\\frac{||\\theta_{1}-\\theta^{*}||^{2}R^{2}}{\\beta}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "By Lemma A.9, we can distinguish two cases when ", "page_idx": 22}, {"type": "equation", "text": "$$\n1-\\ell^{-1}\\left(c_{a,\\rho}\\frac{||\\theta_{1}-\\theta^{*}||^{2}R^{2}}{\\beta}\\frac{1}{n}\\right)\\geq\\frac{c}{a+1}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "or, otherwise, where $c$ is an arbitrary constant in $(1,6/5)$ . ", "page_idx": 22}, {"type": "text", "text": "In the first case, ", "page_idx": 22}, {"type": "equation", "text": "$$\n1-\\ell^{-1}\\left(c_{a,\\rho}\\frac{||\\theta_{1}-\\theta^{*}||^{2}R^{2}}{\\beta}\\frac{1}{n}\\right)\\leq c_{1}c_{a,\\rho}\\frac{||\\theta_{1}-\\theta^{*}||^{2}R^{2}}{\\beta}\\frac{1}{n}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $c_{1}=1/(1-1/c)$ while in the second case ", "page_idx": 22}, {"type": "equation", "text": "$$\n1-\\ell^{-1}\\left(c_{a,\\rho}\\frac{||\\theta_{1}-\\theta^{*}||^{2}R^{2}}{\\beta}\\frac{1}{n}\\right)\\leq c_{2}\\sqrt\\frac2a\\sqrt{c_{a,\\rho}\\frac{||\\theta_{1}-\\theta^{*}||^{2}R^{2}}{\\beta}\\frac{1}{n}}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $c_{2}=1/(1-(1/3)c/(1-c/2)$ . ", "page_idx": 22}, {"type": "text", "text": "It follows that for some constant $\\kappa>0$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\sum_{t=1}^{n}\\pi^{*}(y_{t}x_{t}^{\\top}\\theta_{t})\\right]\\leq\\kappa\\operatorname*{max}\\left\\{\\frac{\\sqrt{\\beta c_{a,\\rho}}}{(\\rho-1)\\sqrt{a}}||\\theta_{1}-\\theta^{*}||R\\sqrt{n},\\frac{c_{a,\\rho}}{\\rho-1}||\\theta_{1}-\\theta^{*}||^{2}R^{2}\\right\\}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "A.6 Multi-class Classification for Linearly Separable Data ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We consider multi-class classification with $k\\geq2$ classes. Let $\\mathcal{V}=\\left\\{1,\\ldots,k\\right\\}$ denote the set of classes. For every $y\\in\\mathcal{V}$ , let $\\theta_{y}\\in\\mathbb{R}^{d}$ and let $\\theta=(\\theta_{1}^{\\top},\\cdot\\cdot\\cdot,\\theta_{k}^{\\top})^{\\top}\\in\\mathbb{R}^{k d}$ be the parameter. For given $x$ and $\\theta$ , predicted class is an element of arg $\\operatorname*{max}_{y\\in{\\mathcal{Y}}}x^{\\top}\\theta_{y}$ . ", "page_idx": 22}, {"type": "text", "text": "The linear separability condition is defined as follows: there exists $\\theta^{*}\\in\\mathbb{R}^{k d}$ such that for some $\\rho^{*}>1$ , for every $x\\in\\mathscr{X}$ and $y\\in\\mathcal{V}$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\boldsymbol{x}^{\\top}\\boldsymbol{\\theta}_{y}^{*}-\\operatorname*{max}_{y^{\\prime}\\in\\mathcal{Y}\\setminus\\{y\\}}\\boldsymbol{x}^{\\top}\\boldsymbol{\\theta}_{y^{\\prime}}^{*}\\geq\\boldsymbol{\\rho}^{*}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Let ", "page_idx": 23}, {"type": "equation", "text": "$$\nu(x,y,\\theta)=x^{\\top}\\theta_{y}-\\operatorname*{max}_{y^{\\prime}\\in\\mathcal{Y}\\setminus\\{y\\}}x^{\\top}\\theta_{y^{\\prime}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We consider margin loss functions which are according to a decreasing function of $u(x,y,\\theta)$ , i.e. $\\ell(x,y,\\theta)\\equiv\\ell(u(x,y,\\theta))$ and $\\tilde{\\ell}(x,y,\\theta)\\equiv\\tilde{\\ell}(u(x,y,\\theta))$ . For example, this accomodates hinge loss function for multi-class classification Crammer and Singer [2002]. ", "page_idx": 23}, {"type": "text", "text": "Lemma A.10. Conditions in Assumption A.1 hold provided that for every $x$ , y and $\\theta$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\pi(x,y,\\theta)(-\\ell^{\\prime}(u(x,y,\\theta))^{2}2R^{2}\\leq\\alpha\\tilde{\\ell}(u(x,y,\\theta))\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\pi(x,y,\\theta)(-\\ell^{\\prime}(u(x,y,\\theta)))(\\rho^{*}-u(x,y,\\theta))\\geq\\beta\\tilde{\\ell}(u(x,y,\\theta)).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. For $x\\in\\mathscr{X}$ and $y\\in\\mathcal{V}$ , let $\\phi(x,y)=(\\phi_{1}(x,y)^{\\top},\\ldots,\\phi_{k}(x,y)^{\\top})^{\\top}$ where $\\phi_{i}(x,y)=x$ if $i=y$ and $\\phi_{i}(x,y)$ is the $d$ -dimensional null-vector, otherwise. Note that ", "page_idx": 23}, {"type": "equation", "text": "$$\nu(x,y,\\theta)=\\theta^{\\top}\\phi(x,y)-\\frac{1}{|y^{*}(x,y,\\theta)|}\\sum_{y^{\\prime}\\in\\mathcal{Y}^{*}(x,y,\\theta)}\\theta^{\\top}\\phi(x,y^{\\prime})\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathcal{V}^{*}(x,y,\\theta)=\\arg\\operatorname*{max}_{y^{\\prime}\\in\\mathcal{V}^{*}(x,y,\\theta)}\\theta_{y^{\\prime}}^{\\top}x.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Note that ", "page_idx": 23}, {"type": "equation", "text": "$$\n||\\nabla_{\\theta}\\ell(x,y,\\theta)||^{2}=\\ell^{\\prime}(u(x,y,\\theta))^{2}||\\nabla_{\\theta}u(x,y,\\theta)||^{2}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "From (14), ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}u(x,y,\\theta)=\\phi(x,y)-\\frac{1}{|\\mathcal{V}^{*}(x,y,\\theta)|}\\sum_{y^{\\prime}\\in\\mathcal{V}^{*}(x,y,\\theta)}\\phi(x,y^{\\prime}).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "It can be readily shown that ", "page_idx": 23}, {"type": "equation", "text": "$$\n||\\nabla_{\\theta}u(x,y,\\theta)||^{2}=\\left(1+\\frac{1}{|\\mathcal{V}^{*}(x,y,\\theta)|}\\right)||x||^{2}\\leq2||x||^{2}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Hence, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n||\\nabla_{\\theta}\\ell(x,y,\\theta)||^{2}\\leq2(-\\ell^{\\prime}(u(x,y,\\theta))^{2}||x||^{2}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Next, note that $\\nabla_{\\boldsymbol{\\theta}}\\ell(x,y,\\boldsymbol{\\theta})=\\ell^{\\prime}(u(x,y,\\boldsymbol{\\theta}))\\nabla_{\\boldsymbol{\\theta}}u(x,y,\\boldsymbol{\\theta}),$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}\\ell(x,y,\\theta)^{\\top}(\\theta-\\theta^{*})=(-\\ell^{\\prime}(u(x,y,\\theta)))\\nabla_{\\theta}u(x,y,\\theta)^{\\top}(\\theta^{*}-\\theta).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\nabla_{\\boldsymbol{\\theta}}u(x,y,\\theta)^{\\top}\\boldsymbol{\\theta}^{*}=x^{\\top}\\boldsymbol{\\theta}_{y}^{*}-\\operatorname*{max}_{y^{\\prime}\\in\\mathcal{V}\\setminus\\{y\\}}x^{\\top}\\boldsymbol{\\theta}_{y^{\\prime}}^{*}\\geq\\rho^{*}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "It follows ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}\\ell(x,y,\\theta)^{\\top}(\\theta-\\theta^{*})\\geq(-\\ell^{\\prime}(u(x,y,\\theta))(\\rho^{*}-u(x,y,\\theta)).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Using (15) and (16), for conditions (7) and (8) to hold, it suffices that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\pi(x,y,\\theta)(-\\ell^{\\prime}(u(x,y,\\theta)))^{2}2R^{2}\\leq\\alpha\\tilde{\\ell}(u(x,y,\\theta))\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\pi(x,y,\\theta)(-\\ell^{\\prime}(u(x,y,\\theta))(\\rho^{*}-u(x,y,\\theta))\\geq\\beta\\tilde{\\ell}(u(x,y,\\theta)).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Note that these conditions are equivalent to those for the binary case in (9) and (10) except for an additional factor 2 in the first of the last above inequalities. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "A.7 Proof of Lemma 3.2 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Function $\\Pi$ is a convex function because, by assumption, $\\pi$ is an increasing function. By (4 and Jensen\u2019s inequality, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\mathbb{E}\\left[\\frac{1}{n}\\sum_{t=1}^{n}\\tilde{\\ell}(\\boldsymbol{\\theta}_{t})\\right]}&{=}&{\\displaystyle\\mathbb{E}\\left[\\frac{1}{n}\\sum_{t=1}^{n}\\Pi(\\ell(x_{t},\\boldsymbol{\\theta}_{t}))\\right]}\\\\ &{\\displaystyle\\ge}&{\\displaystyle\\Pi\\left(\\mathbb{E}\\left[\\frac{1}{n}\\sum_{t=1}^{n}\\ell(x_{t},\\boldsymbol{\\theta}_{t})\\right]\\right)}\\\\ &{\\displaystyle=}&{\\displaystyle\\Pi\\left(\\mathbb{E}\\left[\\frac{1}{n}\\sum_{t=1}^{n}\\ell(x_{t},y_{t},\\boldsymbol{\\theta}_{t})\\right]\\right)}\\\\ &{\\displaystyle=}&{\\displaystyle\\Pi\\left(\\mathbb{E}\\left[\\frac{1}{n}\\sum_{t=1}^{n}\\ell(\\boldsymbol{\\theta}_{t})\\right]\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Therefore, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\frac{1}{n}\\sum_{t=1}^{n}\\ell(\\boldsymbol{\\theta}_{t})\\right]\\leq\\Pi^{-1}\\left(\\mathbb{E}\\left[\\frac{1}{n}\\sum_{t=1}^{n}\\tilde{\\ell}(\\boldsymbol{\\theta}_{t})\\right]\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Combined with condition (5), we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\mathbf{E}\\left[\\frac{1}{n}\\sum_{t=1}^{n}\\ell(\\boldsymbol{\\theta}_{t})\\right]}&{\\le}&{\\displaystyle\\Pi^{-1}\\left(\\operatorname*{inf}_{\\theta}\\tilde{\\ell}(\\boldsymbol{\\theta})+\\sum_{i=1}^{m}f_{i}(n)\\right)}\\\\ &{\\le}&{\\displaystyle\\operatorname*{inf}_{\\theta}\\Pi^{-1}(\\tilde{\\ell}(\\boldsymbol{\\theta}))+\\sum_{i=1}^{m}\\Pi^{-1}(f_{i}(n))}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the last inequality holds because $\\Pi^{-1}$ is a concave function, and hence, it is a subadditive function. ", "page_idx": 24}, {"type": "text", "text": "A.8 Proof of Theorem 3.3 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Under assumptions of the theorem, by Theorem 6.3 Bubeck [2015], ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\tilde{\\ell}(\\bar{\\theta}_{n})\\right]\\leq\\mathbb{E}\\left[\\frac{1}{n}\\sum_{t=1}^{n}\\tilde{\\ell}(\\theta_{t})\\right]\\leq\\operatorname*{inf}_{\\theta}\\tilde{\\ell}(\\theta)+\\sqrt{2}S\\sigma_{\\pi}\\frac{1}{\\sqrt{n}}+L S^{2}\\frac{1}{n}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Combining with Lemma 3.2, we obtain the assertion of the theorem. ", "page_idx": 24}, {"type": "text", "text": "A.9 Proof of Corollary 3.4 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Lemma A.11. For $\\Pi(x)=x-1+e^{-x},$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\Pi^{-1}(y)\\leq2\\sqrt{y}\\,f o r\\,y\\in[0,(3/4)^{2}].\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. We consider ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\Pi(x)=x-(1-e^{-x}).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "By limited Taylor development, ", "page_idx": 24}, {"type": "equation", "text": "$$\n1-e^{-x}\\leq x={\\frac{1}{2}}x^{2}+{\\frac{1}{6}}x^{3}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Hence, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\Pi(x)\\geq{\\frac{1}{2}}x^{2}\\left(1-{\\frac{1}{3}}x\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Note that $\\Pi(x)\\geq c x^{2}$ for some constant $c>0$ provided that ", "page_idx": 25}, {"type": "equation", "text": "$$\n{\\frac{1}{2}}x^{2}\\left(1-{\\frac{1}{3}}x\\right)\\geq c x^{2}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "which is equivalent to $x\\leq3(1-2c)$ . Hence, for any fixed $c\\in[0,1/2)$ ), we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\Pi(x)\\geq c x^{2},{\\mathrm{~for~every~}}x\\in[0,3(1-2c)].\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Now, condition $x\\leq3(1-2c)$ is implied by $\\sqrt{\\Pi(x)/c}\\leq3(1-2c)$ , i.e. $\\Pi(x)\\leq9c(1-2c)^{2}$ . Hence, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\Pi^{-1}(y)\\leq{\\sqrt{\\frac{1}{c}}}y\\,\\mathrm{for}\\,y\\in[0,9c(1-2c)^{2}].\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "In particular, by taking $c=1/4$ , we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\Pi^{-1}(y)\\leq2{\\sqrt{y}}\\;{\\mathrm{for}}\\;y\\in[0,(3/4)^{2}].\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We have the bound in Theorem 3.3. Under $\\sqrt{2}S\\sigma_{\\pi}/\\sqrt{n}\\le(3/4)^{2}$ , we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\Pi^{-1}\\left(\\sqrt{2}S\\sigma_{\\pi}\\frac{1}{\\sqrt{n}}\\right)\\leq2^{5/4}\\sqrt{S\\sigma_{\\pi}}\\frac{1}{\\sqrt[4]{n}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Under $L S^{2}/n\\leq(3/4)^{2}$ , we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\Pi^{-1}\\left(L S^{2}\\frac{1}{n}\\right)\\leq2\\sqrt{L}S\\frac{1}{\\sqrt{n}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "This completes the proof of the corollary. ", "page_idx": 25}, {"type": "text", "text": "A.10 Proof of Theorem 3.6 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "To simplify notation, we write $\\ell_{t}(\\theta)\\equiv\\ell(x_{t},y_{t},\\theta)$ , $\\gamma_{t}=\\gamma(x_{t},y_{t},\\theta_{t})$ and $\\pi_{t}=\\pi(x_{t},y_{t},\\theta_{t})$ . Since $\\ell$ is an $L$ -smooth function, we have $\\begin{array}{r}{\\ell(x,y,\\theta)-\\operatorname*{inf}_{\\theta^{\\prime}}\\ell(x,y,\\theta^{\\prime})\\ge1/(2L^{2})\\|\\nabla_{\\theta}\\ell(x,y,\\theta)\\|^{2}.}\\end{array}$ . Hence, for any $x,y,\\theta$ such that $||\\nabla_{\\theta}\\ell(x,y,\\theta)||>0$ , we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\frac{\\ell(x,y,\\theta)-\\operatorname*{min}_{\\theta^{\\prime}}\\ell(x,y,\\theta^{\\prime})}{||\\nabla_{\\theta}\\ell(x,y,\\theta)||^{2}}\\geq\\frac{1}{2L}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Combined with the definition of $\\zeta$ and the fact $\\zeta(x_{t},y_{t},\\theta_{t})=\\gamma_{t}\\pi_{t}$ , we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\kappa:=\\beta\\operatorname*{min}\\left\\{{\\frac{1}{2L}},\\rho\\right\\}\\leq\\gamma_{t}\\pi_{t}{\\mathrm{~whenever~}}||\\nabla_{\\theta}\\ell(x,y,\\theta)||>0.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "From the definition of $\\zeta$ and the fact $\\zeta(x_{t},y_{t},\\theta_{t})=\\gamma_{t}\\pi_{t}$ , we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\gamma_{t}\\pi_{t}\\le\\rho\\beta.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Next, note ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\vert\\vert\\theta_{t+1}-\\theta^{*}\\vert\\vert^{2}\\;\\vert x_{t},y_{t},\\theta_{t}\\vert]}\\\\ {=}&{\\vert\\vert\\theta_{t}-\\theta^{*}\\vert\\vert^{2}-2\\mathbb{E}[z_{t}\\;\\vert x_{t},y_{t},\\theta_{t}]\\nabla_{\\theta}\\ell_{t}(\\theta_{t})^{\\top}(\\theta_{t}-\\theta^{*})+\\mathbb{E}[z_{t}^{2}\\;\\vert\\;x_{t},y_{t},\\theta_{t}]\\vert\\vert\\nabla_{\\theta}\\ell_{t}(\\theta_{t})\\vert\\vert^{2}}\\\\ {=}&{\\vert\\vert\\theta_{t}-\\theta^{*}\\vert\\vert^{2}-2\\gamma_{t}\\pi_{t}\\nabla_{\\theta}\\ell_{t}(\\theta_{t})^{\\top}(\\theta_{t}-\\theta^{*})+\\gamma_{t}^{2}\\pi_{t}\\vert\\vert\\nabla_{\\theta}\\ell_{t}(\\theta_{t})\\vert\\vert^{2}}\\\\ {\\leq}&{\\vert\\vert\\theta_{t}-\\theta^{*}\\vert\\vert^{2}-2\\gamma_{t}\\pi_{t}(\\ell_{t}(\\theta_{t})-\\ell_{t}(\\theta^{*}))+\\gamma_{t}^{2}\\pi_{t}\\frac{\\vert\\vert\\nabla_{\\theta}\\ell_{t}(\\theta_{t})\\vert\\vert^{2}}{\\ell_{t}(\\theta_{t})-\\ell_{t}(\\theta_{t}^{*})}(\\ell_{t}(\\theta_{t})-\\ell_{t}(\\theta_{t}^{*}))}\\\\ {=}&{\\vert\\vert\\theta_{t}-\\theta^{*}\\vert\\vert^{2}-\\gamma_{t}\\pi_{t}\\left(2-\\gamma_{t}\\frac{\\vert\\vert\\nabla_{\\theta}\\ell_{t}(\\theta_{t})\\vert\\vert^{2}}{\\ell_{t}(\\theta_{t})-\\ell_{t}(\\theta_{t}^{*})}\\right)(\\ell_{t}(\\theta_{t})-\\ell_{t}(\\theta_{t}^{*}))+2\\gamma_{t}\\pi_{t}(\\ell_{t}(\\theta^{*})-\\ell_{t}(\\theta_{t}^{*}))}\\\\ {\\leq}&{\\vert\\vert\\theta_{t}-\\theta^{*}\\vert\\vert^{2}-2c\\kappa(\\ell_{t}(\\theta_{t})-\\ell_{t}(\\theta_{t}^{*}))+2\\rho\\beta \n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the first inequality is by convexity of $\\ell$ , the second inequality is by condition (6), and (18) and (19). Hence, we have ", "page_idx": 26}, {"type": "text", "text": "$\\mathbb{E}[\\Vert\\theta_{t+1}-\\theta^{*}\\Vert^{2}]\\leq\\mathbb{E}[\\Vert\\theta_{t}-\\theta^{*}\\Vert^{2}]-2c\\kappa_{1}\\mathbb{E}[\\ell_{t}(\\theta_{t})-\\ell_{t}(\\theta_{t}^{*})]+2\\kappa_{0}\\mathbb{E}[\\ell_{t}(\\theta^{*})-\\ell_{t}(\\theta_{t}^{*})].$ By summing over $t$ from 1 to $n$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathbb{E}\\left[\\frac{1}{n}\\sum_{t=1}^{n}(\\ell_{t}(\\theta_{t})-\\ell_{t}(\\theta_{t}^{*}))\\right]}&{\\leq}&{\\displaystyle\\frac{\\rho\\beta}{c\\kappa}\\mathbb{E}\\left[\\frac{1}{n}\\sum_{t=1}^{n}(\\ell_{t}(\\theta^{*})-\\ell_{t}(\\theta_{t}^{*}))\\right]+\\frac{1}{2c\\kappa}||\\theta_{1}-\\theta^{*}||^{2}\\frac{1}{n}}\\\\ &{\\leq}&{\\displaystyle\\frac{\\rho\\beta}{c\\kappa}(\\mathbb{E}[\\ell(x,y,\\theta^{*})]-\\mathbb{E}[\\operatorname*{inf}_{\\theta}\\ell(x,y,\\theta)])+\\frac{1}{2c\\kappa}||\\theta_{1}-\\theta^{*}||^{2}\\frac{1}{n}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "A.11 Proofs of Corollaries 3.7 and 3.8 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "For linear classifiers, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\frac{||\\nabla_{\\theta}\\ell(x,y,\\theta)||^{2}}{\\ell(x,y,\\theta)}=h(y x^{\\top}\\theta)||x||^{2}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $h(u)=\\ell^{\\prime}(u)^{2}/\\ell(u)$ which plays a pivotal role in condition (6). ", "page_idx": 26}, {"type": "text", "text": "For the condition (6) to hold it suffices that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\pi(x,y,\\theta)\\geq\\frac{\\beta}{2(1-c)}\\operatorname*{min}\\left\\{\\rho R^{2}h(y x^{\\top}\\theta),1\\right\\}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Note that under assumption that $\\ell(u)$ is an $L^{\\prime}$ -smooth function in $u$ $,\\ell(y x^{\\top}\\theta)$ is an $L^{\\prime}||x||^{2}$ -smooth function in $\\theta$ . Taking $\\rho=1/(2L)$ with $L=L^{\\prime}R^{2}$ , we have $\\rho R^{2}=1/\\dot{(}2L^{\\prime})$ . ", "page_idx": 26}, {"type": "text", "text": "For the binary cross-entropy loss function, we have $h(u)=\\sigma^{\\prime}(u)^{2}/(\\sigma(u)^{2}(-\\log(\\sigma(u))))$ . Specifically, for the logistic regression case ", "page_idx": 26}, {"type": "equation", "text": "$$\nh(u)=\\frac{1}{(1+e^{u})^{2}\\log(1+e^{-u})}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "which is increasing in $u$ for $u~\\leq~0$ and is decreasing in $u$ otherwise. Note that $h(u)\\;=\\;(1\\mathrm{~-~}$ $e^{-\\ell(u)})^{2}/\\ell(u)$ . ", "page_idx": 26}, {"type": "text", "text": "A.11.1 Proof of Corollary 3.7 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We first note the following lemma, whose proof is provided in Appendix A.12. ", "page_idx": 26}, {"type": "text", "text": "Lemma A.12. Function $h,$ defined in (20), satisfies ", "page_idx": 26}, {"type": "equation", "text": "$$\nh(u)\\leq\\frac{1}{1+e^{u}}=1-\\sigma(u)\\,f o r\\,a l l\\,u\\in\\mathbb{R}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Furthermore $h(u)\\sim1-\\sigma(u)$ for large $u$ . ", "page_idx": 26}, {"type": "text", "text": "See See Figure 4, left, for a graphical illustration. ", "page_idx": 26}, {"type": "text", "text": "By Lemma A.12, condition (6) in Theorem 3.6 is satisfied with $\\rho=1/(2L)$ , by sampling proportional to absolute error loss $\\pi^{*}(u)=\\omega(1-\\sigma(u))$ with $\\beta/(4(1-c)L^{\\prime})\\le\\omega\\le1$ . ", "page_idx": 26}, {"type": "text", "text": "A.11.2 Proof of Corollary 3.8 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We have the following lemma, whose proof is provided in A.13. ", "page_idx": 26}, {"type": "text", "text": "Lemma A.13. Function $h,$ defined in (20), satisfies, for every fixed $a\\in(0,1/2]$ , ", "page_idx": 26}, {"type": "equation", "text": "$$\nh(u)\\leq\\frac{1}{H(a)+(1-a)|u|}\\,f o r\\,a l l\\,u\\in\\mathbb{R}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $H(a)=a\\log(1/a)+(1-a)\\log(1/(1-a))$ . Furthermore, $h(u)\\sim1/|u|$ as u tends to $-\\infty$ . ", "page_idx": 26}, {"type": "text", "text": "See See Figure 4, right, for a graphical illustration. ", "page_idx": 26}, {"type": "text", "text": "By Lemma A.13, it follows that condition (6) in Theorem 3.6 is satisfied by uncertainty sampling according to ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\pi^{*}(u)=\\frac{\\beta}{2(1-c)}\\operatorname*{min}\\left\\{\\rho R^{2}\\frac{1}{H(a)+(1-a)|u|},1\\right\\}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "image", "img_path": "GLUIuli3Sm/tmp/5b321a14e57ce8ca613e809a6e462a681f5044b66c26b9080fa52302e3f0e821.jpg", "img_caption": [], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Figure 4: Upper bounds for function $h$ defined in (20): (left) bound of Lemma A.12, (right) bounds of Lemma A.13. ", "page_idx": 27}, {"type": "text", "text": "A.12 Proof of Lemma A.12 ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "We need to prove that for every $u\\in\\mathbb R$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n(1+e^{u})^{2}\\log(1+e^{-u})\\geq1+e^{u}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "By dividing both sides in the last inequality with $(1\\!+\\!e^{u})^{2}$ and the fact $1/(1\\!+\\!e^{u})=1-1/(1\\!+\\!e^{-u})$ , we note that the last above inequality is equivalent to ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\log(1+e^{-u})\\geq1-{\\frac{1}{1+e^{-u}}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "By straightforward calculus, this can be rewritten as ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\log\\left(1-\\left(1-\\frac{1}{1+e^{-u}}\\right)\\right)\\leq-\\left(1-\\frac{1}{1+e^{-u}}\\right).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "This clearly holds true because $1-1/(1+e^{-u})\\in(0,1)$ and $\\log(1-z)\\leq-z$ for every $z\\in(0,1)$ . It remains only to show that $\\begin{array}{r}{\\operatorname*{lim}_{u\\rightarrow\\infty}h(u)/(1-\\sigma(u))=1}\\end{array}$ . This is clearly true as ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\frac{h(u)}{1-\\sigma(u)}=\\frac{1}{(1+e^{u})\\log(1+e^{-u})}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "which goes to 1 as $u$ goes to infinity. ", "page_idx": 27}, {"type": "text", "text": "A.13 Proof of Lemma A.13 ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "We first consider the case $u\\leq0$ . Fix an arbitrary $v\\leq0$ . Since $u\\mapsto\\log(1+e^{u})$ is a convex function it is lower bounded by the tangent passing through $v$ , i.e. ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\log(1+e^{-u})\\geq\\log(1+e^{-v})-{\\frac{1}{1+e^{v}}}(u-v).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Now, let $a$ be such that $1-a=1/(1+e^{v})$ . Since $v\\leq0$ , we have $a\\in(0,1/2]$ . It follows that for any fixed $a\\in(0,1/2]$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\log(1+e^{-u})\\geq H(a)-(1-a)u.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Using this along with the obvious fact $(1+e^{u})^{2}\\geq1$ , we have that for every $u\\leq0$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\nh(u)\\leq\\frac{1}{\\log(1+e^{-u})}\\leq\\frac{1}{H(a)+(1-a)|u|}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We next consider the case $u\\geq0$ . It suffices to show that for every $u\\geq0$ , $h(u)\\leq h(-u)$ , and hence the upper bound established for the previous case applies. The condition $h(u)\\leq h(-u)$ is equivalent to ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\frac{1}{(1+e^{u})^{2}\\log(1+e^{-u})}\\leq\\frac{1}{(1+e^{-u})^{2}\\log(1+e^{u})}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "By straightforward calculus, this is equivalent to ", "page_idx": 28}, {"type": "equation", "text": "$$\nf(u):=(1-e^{-2u})\\log(1+e^{u})-u\\geq0.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "This holds because function (i) $f$ is increasing on $[0,u_{0}]$ and decreasing on $[u_{0},\\infty)$ , for some $u_{0}\\geq0$ , (ii) $f(0)=0$ and (iii) $\\begin{array}{r}{\\operatorname*{lim}_{u\\to\\infty}f(u)=0}\\end{array}$ . Properties (ii) and (iii) are easy to check. We only show that property (i) holds true. By straightforward calculus, ", "page_idx": 28}, {"type": "equation", "text": "$$\nf^{\\prime}(u)=e^{-2u}(2\\log(1+e^{u})-e^{u}).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "It suffices to show that there is a unique $u^{*}\\in\\mathbb{R}$ such that $f^{\\prime}(u^{*})=0$ . For any such $u^{*}$ it must hold $2\\log(1+e^{u^{*}})-e^{u^{*}}$ . Let $v=e^{v^{*}}$ . Then, $2\\log(1+v)=v$ , which is equivalent to ", "page_idx": 28}, {"type": "equation", "text": "$$\n1+v=e^{\\frac{v}{2}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Both sides of the last equation are increasing in $v$ , and the left-hand side is larger than the right-hand side for $v=1$ . Since the right-hand side is larger than the left-hand side for any large enough $v$ , it follows that there is a unique point $v$ at which the sides of the equation are equal. This shows that there is a unique $u^{*}\\geq0$ such that $f^{\\prime}(u^{*})=0$ . ", "page_idx": 28}, {"type": "text", "text": "It remains to show that $\\begin{array}{r}{\\operatorname*{lim}_{u\\rightarrow-\\infty}h(u)/(1/|u|)=1}\\end{array}$ , i.e. ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{u\\rightarrow-\\infty}\\frac{-u}{(1+e^{u})^{2}\\log(1+e^{-u})}=1\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "which clearly holds true as both $1/(1+e^{u})^{2}$ and $-u/\\log(1+e^{-u})$ go to $1$ as $u$ goes to $-\\infty$ . ", "page_idx": 28}, {"type": "text", "text": "A.14 Convergence Conditions for $\\pi(x,y,\\theta)=\\zeta(x,y,\\theta)^{\\eta}$ ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "It suffices to show that under given conditions, the sampling probability function satisfies condition (6). Using the definition of the sampling probability function, condition (6) can be written as follows ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\bigg(\\frac{\\ell(x,y,\\theta)-\\operatorname*{inf}_{\\theta^{\\prime}}\\ell(x,y,\\theta^{\\prime})}{||\\nabla_{\\theta}\\ell(x,y,\\theta)||^{2}}\\bigg)^{\\eta}\\geq\\frac{1}{2(1-c)}\\operatorname*{min}\\bigg\\{\\beta,\\rho\\beta\\frac{||\\nabla_{\\theta}\\ell(x,y,\\theta)||^{2}}{\\ell(x,y,\\theta)-\\operatorname*{inf}_{\\theta^{\\prime}}\\ell(x,y,\\theta)}\\bigg\\}^{1-\\eta}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "In the inequality (23), by (17), the left-hand side is at least $(1/(2L))^{\\eta}$ and clearly the right-hand side is at most $\\beta^{1-\\eta}/(2(1-c))$ . Hence, it follows that it suffices that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\left(\\frac{1}{2L}\\right)^{\\eta}\\geq\\frac{1}{2(1-c)}\\beta^{1-\\eta}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "A.15 Uncertainty-based Sampling for Multi-class Classification ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "We consider multi-class classification according to prediction function ", "page_idx": 28}, {"type": "equation", "text": "$$\np(y\\mid x,\\theta)={\\frac{e^{x^{\\top}\\theta_{y}}}{\\sum_{y^{\\prime}\\in\\mathcal{Y}}e^{x^{\\top}\\theta_{y^{\\prime}}}}},{\\;\\mathrm{for}\\;}y\\in\\mathcal{Y}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Assume that $\\ell$ is the cross-entropy function. Let ", "page_idx": 28}, {"type": "equation", "text": "$$\nu(x,y,\\theta)=-\\log\\left(\\sum_{y^{\\prime}\\in\\mathcal{Y}\\backslash\\{y\\}}e^{-(x^{\\top}\\theta_{y}-x^{\\top}\\theta_{y^{\\prime}})}\\right).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "It can be shown that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\frac{||\\nabla_{\\theta}\\ell(x,y,\\theta)||^{2}}{\\ell(x,y,\\theta)}\\leq2||x||^{2}h(u(x,y,\\theta))\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where function $h$ is defined in (20). Hence, condition of Theorem 3.6 holds under ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\pi(u)\\geq\\frac{\\beta}{2(1-c)}\\operatorname*{min}\\left\\{2\\rho R^{2}h(u),1\\right\\}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "For given $\\theta$ and $x$ , let $\\theta_{(1)},\\ldots,\\theta_{(k)}$ be an ordering of $\\theta_{1},\\ldots,\\theta_{k}$ such that $x^{\\top}\\theta_{(1)}\\geq\\cdot\\cdot\\geq x^{\\top}\\theta_{(k)}$ . Sampling according to function $\\pi^{*}$ of the gap $g=|x^{\\top}\\theta_{(1)}-x^{\\top}\\theta_{(k)}|$ , ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\pi^{*}(g)=\\frac{\\beta}{2(1-c)}\\operatorname*{min}\\left\\{2\\rho R^{2}h^{*}(g),1\\right\\},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where ", "page_idx": 29}, {"type": "equation", "text": "$$\nh^{\\ast}(g)=\\frac{1}{H(a)+(1-a)\\operatorname*{max}\\{g-\\log(k-1),0\\}},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "satisfies condition of Theorem 3.6. ", "page_idx": 29}, {"type": "text", "text": "We next provide proofs for assertions made above. The loss function is assumed to be the crossentropy loss function, i.e. ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\ell(x,y,\\theta)=-\\log\\left(\\frac{e^{x^{\\top}\\theta_{y}}}{\\sum_{y^{\\prime}\\in y}e^{x^{\\top}\\theta_{y^{\\prime}}}}\\right).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Note that we can write ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\ell(x,y,\\theta)=-\\left(\\phi(x,y)^{\\top}\\theta-\\log\\left(\\sum_{y^{\\prime}\\in\\mathcal{Y}\\setminus\\{y\\}}e^{\\phi(x,y^{\\prime})^{\\top}\\theta}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "We consider ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\frac{||\\nabla_{\\theta}\\ell(x,y,\\theta)||^{2}}{\\ell(x,y,\\theta)}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "which is plays a key role in the condition of Theorem 3.6. ", "page_idx": 29}, {"type": "text", "text": "It holds ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}\\ell(x,y,\\theta)=-\\left(\\phi(x,y)-{\\frac{\\sum_{y^{\\prime}\\in{\\mathcal{V}}\\backslash\\{y\\}}e^{\\phi(x,y^{\\prime})^{\\top}\\theta}\\phi(x,y^{\\prime})}{\\sum_{y^{\\prime}\\in{\\mathcal{V}}\\backslash\\{y\\}}e^{\\phi(x,y^{\\prime})^{\\top}\\theta}}}\\right)\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "and ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{|\\nabla_{\\theta}\\ell(x,y,\\theta)||^{2}}&{=}&{\\left(1-\\displaystyle\\frac{e^{\\phi(x,y)^{\\top}\\theta}}{\\sum_{z\\in\\mathcal{Y}\\setminus\\{y\\}}e^{\\phi(x,z)^{\\top}\\theta}}\\right)^{2}||x||^{2}+\\displaystyle\\sum_{y^{\\prime}\\in\\mathcal{Y}\\setminus\\{y\\}}\\left(\\displaystyle\\frac{e^{\\phi(x,y^{\\prime})^{\\top}\\theta}}{\\sum_{z\\in\\mathcal{Y}\\setminus\\{y\\}}e^{\\phi(x,z)^{\\top}\\theta}}\\right)^{2}||x||^{2}}\\\\ &{=}&{\\left(\\left(1-e^{-\\ell(x,y,\\theta)}\\right)^{2}+\\displaystyle\\sum_{y^{\\prime}\\in\\mathcal{Y}\\setminus\\{y\\}}\\left(e^{-\\ell(x,y^{\\prime},\\theta)}\\right)^{2}\\right)||x||^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "From the last equation, it follows ", "page_idx": 29}, {"type": "equation", "text": "$$\n||x||^{2}\\left(1-e^{-\\ell(x,y,\\theta)}\\right)^{2}\\leq||\\nabla_{\\theta}\\ell(x,y,\\theta)||^{2}\\leq2||x||^{2}\\left(1-e^{-\\ell(x,y,\\theta)}\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Note that $\\ell(x,y,\\theta)=\\log(1+e^{-u(x,y,\\theta)})$ where ", "page_idx": 29}, {"type": "equation", "text": "$$\nu(x,y,\\theta)=-\\log\\left(\\sum_{y^{\\prime}\\in\\mathcal{Y}\\backslash\\{y\\}}e^{-(x^{\\top}\\theta_{y}-x^{\\top}\\theta_{y^{\\prime}})}\\right).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "It follows ", "page_idx": 29}, {"type": "equation", "text": "$$\n||x||^{2}h(u(x,y,\\theta))\\leq\\frac{||\\nabla_{\\theta}\\ell(x,y,\\theta)||^{2}}{\\ell(x,y,\\theta)}\\leq2||x||^{2}h(u(x,y,\\theta))\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $h$ is function defined in (20). ", "page_idx": 29}, {"type": "text", "text": "The following equation holds ", "page_idx": 29}, {"type": "equation", "text": "$$\nu(x,y,\\theta)=\\theta_{y}^{\\top}x-\\operatorname*{max}_{z\\in\\mathcal{V}\\setminus\\{y\\}}x^{\\top}\\theta_{z}-\\log\\left(\\sum_{y^{\\prime}\\in\\mathcal{V}\\setminus\\{y\\}}e^{-(\\operatorname*{max}_{z\\in\\mathcal{V}\\setminus\\{y\\}}x^{\\top}\\theta_{z}-x^{\\top}\\theta_{y^{\\prime}})}\\right).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Note that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l l}{|u(x,y,\\theta)|}&{\\geq}&{|x^{\\top}\\theta_{y}-\\underset{z\\in\\mathcal{P}\\backslash\\{y\\}}{\\operatorname*{max}}x^{\\top}\\theta_{z}|-\\log\\left(\\underset{y^{\\prime}\\in\\mathcal{P}\\backslash\\{y\\}}{\\sum}e^{-(\\operatorname*{max}_{z\\in\\mathcal{P}\\backslash\\{y\\}}x^{\\top}\\theta_{z}-x^{\\top}\\theta_{y^{\\prime}})}\\right)}\\\\ &{\\geq}&{|x^{\\top}\\theta_{(1)}-x^{\\top}\\theta_{(2)}|-\\log(k-1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Combining with Lemma A.13, for every $a\\in(0,1/2]$ , ", "page_idx": 30}, {"type": "equation", "text": "$$\nh(u(x,y,\\theta))\\leq\\frac{1}{H(a)+(1-a)|u|}\\leq h^{*}(|x^{\\top}\\theta_{(1)}-x^{\\top}\\theta_{(2)}|)\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where ", "page_idx": 30}, {"type": "equation", "text": "$$\nh^{\\ast}(g)=\\left\\{\\begin{array}{l l}{\\frac{1}{H(a)}}&{\\mathrm{~if~}g\\leq\\log(k-1)}\\\\ {\\frac{1}{H(a)-(1-a)\\log(k-1)+(1-a)g}}&{\\mathrm{~if~}g>\\log(k-1).}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "B Appendix: Additional Material for Numerical Experiments ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "B.1 Further Details on Experimental Setup ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Hyperparameter Tuning We used the Tree-structured Parzen Estimator (TPE) [Bergstra et al., 2011] algorithm in the hyperopt package [Bergstra et al., 2013] to tune the relevant hyperparameters for each method and minimize the average progressive cross entropy loss. For Polyak absloss and Polyak exponent we set the search space of $\\eta$ to [0.01, 1] and the search space of $\\rho$ to $[0,1]$ . Note that the values of $\\eta$ and $\\rho$ influence the rate of sampling. ", "page_idx": 30}, {"type": "text", "text": "In line with the typical goal of active learning, we aim to learn efficiently and minimize loss under some desired rate of sampling. Therefore, for every configuration of $\\eta$ and $\\rho$ we use binary search to find the value of $\\beta$ that achieves some target empirical sampling rate. ", "page_idx": 30}, {"type": "text", "text": "Observe that if we would not control for $\\beta$ , then our hyperparameter tuning setup would simply find values of $\\eta$ and $\\rho$ that lead to very high sampling rates, which is not in line with the goal of active learning. In the hyperparameter tuning we set the target empirical sampling rate to $50\\%$ . ", "page_idx": 30}, {"type": "text", "text": "Compute Resources All experiments were performed on a single machine with 72 CPU cores and 228 GB RAM. It took us around 2,000 seconds to complete a training run for an AWS-PA with an absloss estimator on Mushrooms dataset, our slowest experiment. The training runs for other datasets and algorithms were considerably faster. ", "page_idx": 30}, {"type": "text", "text": "B.2 Further Details on Numerical Experiments with Different Algorithms ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "In Section 4, we presented numerical results for comparing AWS-PA with other algorithms. These results are shown in Figure 1. Below are some additional details for these experiments. ", "page_idx": 30}, {"type": "text", "text": "Tuning Sampling Rate In Figure 1 we compare Polyak absolute loss sampling to absolute loss sampling and random sampling. In this setting we have no control over the sampling rate of absolute loss sampling. Hence, we first run absolute loss sampling to find an empirical sampling rate of $14.9\\%$ . We then again use binary search to find the value of $\\beta$ to match this sampling rate with Polyak absolute loss sampling. Again, this setup is conservative with respect to the gains of Polyak absolute loss sampling as $\\eta$ and $\\rho$ were optimized for a sampling rate of $50\\%$ . ", "page_idx": 30}, {"type": "text", "text": "Sampling Efficiency of AWS-PA In Figure 1 we had demonstrated on various datasets that AWSPA leads to faster convergence than the traditional loss-based sampling Yoo and Kweon [2019]. Figure 5 presents results as a function of the number of sampled instances, i.e., the number of labeled instances that were selected for training (i.e., cost). This contrasts Figure 1, which showed on the X-axis the total number of iterations. The results confirm that sampling with AWS-PA not only leads to faster convergence than traditional loss-based sampling when expressed in terms of number of iterations, but also when expressed in the number of sampled instances. ", "page_idx": 30}, {"type": "image", "img_path": "GLUIuli3Sm/tmp/ae466b18dd6024f9eb318bb6c0b912c2eeacc49f8d48f8d8e9edcae8bbd91d11.jpg", "img_caption": ["Figure 5: Average cross entropy loss as a function of labeling cost for different sampling methods. "], "img_footnote": [], "page_idx": 31}, {"type": "image", "img_path": "GLUIuli3Sm/tmp/9fecd8daa1b4c59f774afa5c0fac74336292d8dcfbd512e0e119250290dc2a2d.jpg", "img_caption": ["Figure 6: Average cross entropy loss on a hold-out testing set for different sampling methods. "], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "AWS-PA Results on a Holdout Test Set The results in Figure 1 were obtained using a progressive validation Blum et al. [1999] procedure where the average loss is measured during an online learning procedure where for each instance the loss is calculated prior to the weight update. Figures 6 and 7 show that our finding that AWS-PA leads to faster convergence than traditional loss-based sampling and than random sampling also holds true on a separate hold out test set. ", "page_idx": 31}, {"type": "text", "text": "B.3 Further Details on the Robustness of AWS-PA to Loss Estimation ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "In Section 4, we presented numerical results for comparison of the training loss achieved by our AWS-PA algorithm using the ground truth absolute error loss and estimated absolute error loss. These results are shown in Figure 2. Here, we provide more details for the underlying setup of experiments, and the number of sampled points. ", "page_idx": 31}, {"type": "text", "text": "Details on the Absloss Estimator For the experiments in Figure 2 we use a separate Random Forest (RF) regressor which estimates absolute error loss based on the same set of features as the target model with an addition of the target\u2019s model prediction as an extra feature. The estimator is retrained on every sampling step using the labeled points observed so far. We used the scikit-learn implementation of the RF regressor and manually tuned two hyperparameters for different datasets: (a) number of tree estimators (b) number of \"warm-up\" steps during which we sample content with a constant probability until we collect enough samples to train an RF estimator. We parallelized training of the RF estimator across all available CPU cores and used default values for all other hyperparameters. ", "page_idx": 31}, {"type": "text", "text": "The statistics of the absloss as well as the parameters of the RF estimators for different datasets are summarized in Table 2. From the table, we note that the mean ground truth values of the absloss are largely in line with the mean estimated absloss. This suggests that it is possible to train absloss estimator with low bias or is even unbiased. ", "page_idx": 31}, {"type": "image", "img_path": "GLUIuli3Sm/tmp/2b06fd30c71c57cbda5fd1b251c23a276c25118acd2a656197c52c4721e1f6d3.jpg", "img_caption": ["Figure 7: Test accuracy for different sampling methods. "], "img_footnote": [], "page_idx": 31}, {"type": "table", "img_path": "GLUIuli3Sm/tmp/4d867ca2ae45f516343b78aa123efc7c0bd73429d4b9028173e000309c2c74c1.jpg", "table_caption": ["Table 2: Hyperparameters of the absloss estimator and the comparison of the mean of ground truth and the mean of estimated absolute loss values. "], "table_footnote": [], "page_idx": 32}, {"type": "image", "img_path": "GLUIuli3Sm/tmp/bc39cd884ceb1fb697dcff1a37d0f855c6d4b91a7712c3f51658fc84db2b1fd0.jpg", "img_caption": ["Figure 8: Sampling efficiency for sampling based on the ground truth of absolute loss v.s. on estimated absolute loss. "], "img_footnote": [], "page_idx": 32}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "Sampling efficiency of the absloss estimator In Figure 8 we compare the cost of sampling based on the ground truth absolute loss versus sampling based on the estimated absloss. We note that in 4 out 6 datasets, the sampling cost closely matches that of sampling based on the ground truth absloss. However, in one of the cases (Splice) the sampling cost is lower and in one of the cases (Credit) it is higher than the baseline. ", "page_idx": 32}, {"type": "image", "img_path": "GLUIuli3Sm/tmp/4ca02ebb0a3fc0e4161c3683662e5b99a572471a7e27e8c40e4e24ec1e9898e6.jpg", "img_caption": ["Figure 9: Credit dataset with an MLP neural network loss estimator. "], "img_footnote": [], "page_idx": 33}, {"type": "image", "img_path": "GLUIuli3Sm/tmp/52aea5ed599817ed7e1da89e610d8b279c111c794eab59719772891c189682c4.jpg", "img_caption": ["Figure 10: Average cross-entropy progressive loss of Polyak\u2019s step size compared to SGD with constant step size, for $1\\%$ and $10\\%$ sampling from the mushrooms data. "], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "B.4 Additional experiments ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Experiments with different sampling rates Loizou et al. [2021] demonstrated that stochastic gradient descent with a step size corresponding to their stochastic Polyak\u2019s step size converges faster than gradient descent. Figure 10 illustrates that these findings extend to scenarios where we selectively sample from the dataset rather than training on the full dataset, and the step size is according to stochastic Polyak\u2019s step size only in expectation. ", "page_idx": 33}, {"type": "text", "text": "To perform these experiments, similarly to the procedure described in Appendix B.1, we used binary search to find the value of $\\beta$ that correspondingly achieves the two target values $1\\%$ and $10\\%$ with Polyak power function, while using the values of $\\eta$ and $\\rho$ that were optimised for a sampling rate of $50\\%$ . Therefore, our findings of the gains achieved for selective sampling according to stochastic Polyak\u2019s step size are likely conservative since $\\eta$ and $\\rho$ were not optimised for specifically these sampling rates. ", "page_idx": 33}, {"type": "text", "text": "Experiments with synthetic absloss estimator We simulate a noisy estimator of the absolute error loss in AWS-PA. We model an unbiased noisy estimator $\\hat{\\ell}_{\\mathrm{abs}}$ of the absolute error loss $\\ell_{\\mathrm{abs}}\\in[0,1]$ as a random variable following the beta distribution, denoted as $\\widehat{\\ell}_{\\mathrm{abs}}\\sim\\operatorname{Beta}(\\alpha,\\beta)$ , where $\\alpha$ and $\\beta$ are parameters set to ensure $\\mathbb{E}[\\hat{\\ell}_{\\mathrm{abs}}]=\\ell_{\\mathrm{abs}}$ . The variance of the noise can be controlled by the tuning parameter $\\alpha$ , given by ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathrm{var}[\\hat{\\ell}_{\\mathrm{abs}}]=\\frac{\\ell_{\\mathrm{abs}}(1-\\ell_{\\mathrm{abs}})}{\\alpha+\\ell_{\\mathrm{abs}}}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Figure 11 shows that the convergence results are robust against estimation noise of the absolute error loss for a wide range of values for $\\alpha\\geq1$ . ", "page_idx": 33}, {"type": "image", "img_path": "GLUIuli3Sm/tmp/42176e182b9e53d828d5ecf3828d6c0489e27ea7def1ea96d0b9213b6fd06f56.jpg", "img_caption": ["Figure 11: Robustness of the proposed sampling approach with adaptive Polyak\u2019s step size for different variance $\\mathrm{var}[\\hat{\\ell}_{\\mathrm{abs}}]=\\ell_{\\mathrm{abs}}^{2}(1-\\ell_{\\mathrm{abs}})/(\\alpha+\\ell_{\\mathrm{abs}})$ noise levels of absolute error loss estimator: (low) $\\alpha=100$ , (medium) $\\alpha=2.5$ , and (high) $\\alpha=1$ . "], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: In the abstract and introduction, we summarize the problem settings that we consider and our results in these settings. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 34}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: In the limitations section (Section A.1), we indicate several avenues for future work that could be pursued to extend our work. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 35}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We present basic assumptions and definitions in Section 2. We list all the assumptions in the statements of the theorems. Complete proofs of our theoretical results are provided in Appendix A. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 35}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: In Section 4, we provide the information for reproducing our numerical results, along with more details provided in Appendix B. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed ", "page_idx": 35}, {"type": "text", "text": "instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. ", "page_idx": 36}, {"type": "text", "text": "\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 36}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: The datasets that we use are available in public domain and we have provided references for these datasets. We have provided detailed information about algorithms and experimental setups, which should be sufficient for reproducing our experiments. The code itself is publicly available at https://github.com/facebookresearch/ AdaptiveWeightSampling. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 36}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We have explained how the data is fed to the SGD algorithms that we consider, along with the sampling component and setting of the step size, hyperparameter tuning and other relevant details. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 37}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 37}, {"type": "text", "text": "Answer: [No] ", "page_idx": 37}, {"type": "text", "text": "Justification: Our experimental results show the training loss versus the number of iterations for a single epoch. This is a standard way for comparing convergence rates of different algorithms. Consistency of our results is demonstrated through validation by using different training datasets. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 37}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: We have provided details on the compute resources we had used for running our experiments in Appendix B.1. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. ", "page_idx": 37}, {"type": "text", "text": "\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 38}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Justification: We have reviewed the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 38}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: Given that this study primarily focuses on theoretical analysis, we do not foresee any negative social consequences. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 38}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: This paper poses no such risks. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 38}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 39}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: We have cited references for the datasets that we used as well as for any other assets we used. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 39}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: This paper does not release new assets. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 39}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 40}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 40}]