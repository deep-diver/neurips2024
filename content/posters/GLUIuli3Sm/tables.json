[{"figure_path": "GLUIuli3Sm/tables/tables_5_1.jpg", "caption": "Table 1: Examples of sampling probability functions.", "description": "This table presents examples of sampling probability functions \u03c0(x) and their corresponding primitive functions \u03a0(x) and inverse functions \u03a0\u207b\u00b9(x).  These functions are used in the paper's analysis of active learning algorithms, where the probability of sampling a data point is related to its loss or uncertainty value.  The table shows how various choices of the sampling probability function affect the algorithm's convergence rate and the expected number of samples. The approximations for small x illustrate the asymptotic behavior of the functions.", "section": "3.1 Constant-Weight Sampling"}, {"figure_path": "GLUIuli3Sm/tables/tables_32_1.jpg", "caption": "Table 2: Hyperparameters of the absloss estimator and the comparison of the mean of ground truth and the mean of estimated absolute loss values.", "description": "This table presents the hyperparameters used for training the absolute error loss estimator (absloss) which is a Random Forest regressor.  It shows the number of trees and warm-up steps used in the RF regressor for each dataset.  The warm-up steps are the number of initial samples collected before training the RF estimator. The table also compares the mean ground truth absolute loss to the mean of the estimated absolute losses for each dataset.  This comparison helps evaluate the accuracy of the absloss estimator.", "section": "B.3 Further Details on the Robustness of AWS-PA to Loss Estimation"}]