[{"figure_path": "xrbgXJomJp/figures/figures_6_1.jpg", "caption": "Figure 1: An overview of our network architecture.", "description": "This figure presents a detailed illustration of the multi-agent inverse factorized Q-learning (MIFQ) network architecture.  It shows the interplay between three main network components: agent local networks, value mixing networks, and hyper-networks.  The agent local networks process individual agent observations and actions to produce local Q-values and V-values. These local values are then aggregated by the mixing networks, which use hyper-network outputs (weights and biases) modulated by the global state. The hyper-networks themselves take the global state as input, providing a dynamic weighting scheme for the mixing networks. The final output represents the joint Q-value and V-value, which contribute to the model\u2019s training objective. The GRU components represent the recurrent neural network used to handle sequential data. The figure visually demonstrates the centralized training with decentralized execution (CTDE) paradigm employed by MIFQ, where global information (from the hyper-network) is used to learn decentralized policies.", "section": "4.3 MIFQ Algorithm"}, {"figure_path": "xrbgXJomJp/figures/figures_7_1.jpg", "caption": "Figure 2: Learning curves", "description": "The figure shows learning curves for different multi-agent reinforcement learning algorithms across various tasks.  The x-axis represents the training steps, and the y-axis represents the performance metric (winrate or reward), depending on the specific task.  The plot visualizes the training progress of each algorithm, allowing for a comparison of their learning efficiency and convergence. Different colored lines indicate different algorithms, and the legend specifies them. Each subplot represents a distinct task, encompassing several scenarios from StarCraft II (SMACv2), Gold Miner, and Multi-Particle Environments (MPE).  The figure illustrates the effectiveness of the proposed algorithm (MIFQ) in comparison to other state-of-the-art methods across diverse environments.", "section": "5 Experiments"}, {"figure_path": "xrbgXJomJp/figures/figures_7_2.jpg", "caption": "Figure 3. Comparison with different numbers of demonstrations. X-axis: winning rate. Y-axis: number of demonstrations.", "description": "This figure compares the performance of various multi-agent imitation learning algorithms across different numbers of expert demonstrations.  The x-axis represents the winning rate achieved by each algorithm, and the y-axis indicates the number of expert demonstrations used for training.  The box plots summarize the distribution of winning rates across multiple trials for each algorithm and demonstration count. This visualization helps to assess how the algorithms' performance scales with the amount of training data available.", "section": "5 Experiments"}, {"figure_path": "xrbgXJomJp/figures/figures_17_1.jpg", "caption": "Figure 1: An overview of our network architecture.", "description": "This figure presents a detailed illustration of the multi-agent inverse factorized Q-learning (MIFQ) network architecture. It comprises three main components: agent local networks, value mixing networks, and hyper-networks. The agent local networks generate local Q-values and V-values for individual agents based on their observations and actions. These local values are then aggregated into global Q-values and V-values by the value mixing networks.  Finally, the hyper-networks determine the weights of the mixing networks dynamically based on the global state. This centralized training with decentralized execution approach allows for the effective integration of individual agents' learning to solve the multi-agent imitation learning problem.", "section": "4.2 Inverse Factorized Soft Q-Learning"}, {"figure_path": "xrbgXJomJp/figures/figures_18_1.jpg", "caption": "Figure 1: An overview of our network architecture.", "description": "This figure shows the network architecture used in the proposed Multi-agent Inverse Factorized Q-learning (MIFQ) algorithm.  It details the components, including agent local networks which process local observations and actions, mixing networks combining these to compute global Q and V values, and hyper-networks providing weights to the mixing networks based on the global state.  The figure illustrates the flow of information and the interaction between the different network components, emphasizing the centralized training with decentralized execution (CTDE) approach.", "section": "4.3 MIFQ Algorithm"}, {"figure_path": "xrbgXJomJp/figures/figures_19_1.jpg", "caption": "Figure 3: Comparison with different numbers of demonstrations. X-axis: winning rate. Y-axis: number of demonstrations.", "description": "This figure compares the performance of different multi-agent imitation learning algorithms across various tasks (Protoss, Terran, Zerg in SMACv2; easy, medium, hard in Gold Miner) with varying numbers of expert demonstrations (128, 256, 512, 1024, 2048, 4096).  The x-axis represents the winning rate achieved by each algorithm, and the y-axis indicates the number of expert demonstrations used for training. The box plots show the distribution of winning rates across multiple runs for each algorithm and number of demonstrations, providing insights into the performance consistency and sensitivity to the amount of expert data.", "section": "5 Experiments"}, {"figure_path": "xrbgXJomJp/figures/figures_21_1.jpg", "caption": "Figure 1: An overview of our network architecture.", "description": "This figure shows the network architecture used in the Multi-agent Inverse Factorized Q-learning (MIFQ) algorithm.  It details the three main components: agent local networks (producing local Q-values), value mixing networks (combining local values into joint values), and hyper-networks (generating weights for the mixing networks based on global state information).  The figure illustrates how local Q-values are processed and aggregated to produce global Q-values and the global value function, facilitating centralized training with decentralized execution (CTDE).", "section": "4.3 MIFQ Algorithm"}]