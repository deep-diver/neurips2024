[{"type": "text", "text": "Discrete-state Continuous-time Diffusion for Graph Generation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Zhe Xu\u2217 Ruizhong Qiu\u2217 Yuzhong Chen\u2020 Huiyuan Chen\u2020 Xiran Fan\u2020 Menghai Pan\u2020 ", "page_idx": 0}, {"type": "text", "text": "Zhichen Zeng\u2217 Mahashweta Das\u2020 Hanghang Tong\u2217 ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Graph is a prevalent discrete data structure, whose generation has wide applications such as drug discovery and circuit design. Diffusion generative models, as an emerging research focus, have been applied to graph generation tasks. Overall, according to the space of states and time steps, diffusion generative models can be categorized into discrete-/continuous-state discrete-/continuous-time fashions. In this paper, we formulate the graph diffusion generation in a discrete-state continuous-time setting, which has never been studied in previous graph diffusion models. The rationale of such a formulation is to preserve the discrete nature of graph-structured data and meanwhile provide flexible sampling trade-offs between sample quality and efficiency. Analysis shows that our training objective is closely related to the generation quality and our proposed generation framework enjoys ideal invariant/equivariant properties concerning the permutation of node ordering. Our proposed model shows competitive empirical performance against state-ofthe-art graph generation solutions on various benchmarks and at the same time can flexibly trade off the generation quality and efficiency in the sampling phase. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Graph generation has been studied for a long time with broad applications, based on either the one-shot (i.e., one-step) [50, 39, 56, 51, 72, 32] or auto-regressive generation paradigm [82, 29, 42, 52]. The former generates all the graph components at once and the latter does that sequentially. A recent trend of applying diffusion generative models [67, 23, 70] to graph generation tasks attracts increasing attentions because of its excellent performance and solid theoretical foundation. In this paper, we follow the one-shot generation paradigm, the same as most graph diffusion generative models. ", "page_idx": 0}, {"type": "image", "img_path": "YkSKZEhIYt/tmp/afaa6ef9b76e0b2a6a9614cdbcb04979dce1344887af9acc9da2a54f50999d02.jpg", "img_caption": ["Figure 1: A taxonomy of graph diffusion models. "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "Some earlier attempts at graph diffusion models treat the graph data in a continuous state space by viewing the graph topology and features as continuous variables [56]. Such a formulation departs from the discrete nature of graph-structured data; e.g., topological sparsity is lost and the discretization in the generation process requires extra hyper-parameters. DiGress [73] is one of the early efforts applying discrete-state diffusion models to graph generation tasks and is the current state-of-the-art graph diffusion generative model. However, DiGress is defined in the discrete time space whose generation is inflexible. This is because, its number of sampling steps must match the number of forward diffusion steps, which is a fixed hyperparameter after the model finishes training. A unique advantage of the continuous-time diffusion models [70, 32] lies in their flexible sampling process, and its simulation complexity is proportional to the number of sampling steps, determined by the step size of various numerical approaches (e.g., $\\tau$ -leaping [18, 8, 71]) and decoupled from the models\u2019 training. Thus, a discrete-state continuous-time diffusion model is highly desirable for graph generation tasks. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Driven by the recent advance of continuous-time Markov Chain (CTMC)-based diffusion generative model [8], we incorporate the ideas of CTMC into the corruption and denoising of graph data and propose the first discrete-state continuous-time graph diffusion generative model. It shares the same advantages as DiGress by preserving the discrete nature of graph data and meanwhile overcomes the drawback of the nonadjustable sampling process in DiGress. This Discrete-state Continuous-time graph diffusion model is named DISCO. ", "page_idx": 1}, {"type": "text", "text": "DISCO bears several desirable properties and advantages. First, despite its simplicity, the training objective has a rigorously proved connection to the sampling error. Second, its formulation includes a parametric graph-to-graph mapping, named backbone model, whose input-output architecture is shared between DISCO and DiGress. Therefore, the graph transformer (GT)-based backbone model [54] from DiGress can be seamlessly plugged into DISCO. Third, a concise message-passing neural network backbone model is explored with DISCO, which is simpler than the GT backbone and has decent empirical performance. Last but not least, our analyses show that the forward and reverse diffusion process in DISCO can retain the permutation-equivariant/invariant properties for its training loss and sampling distribution, both of which are critical and practical inductive biases on graph data. ", "page_idx": 1}, {"type": "text", "text": "Comprehensive experiments on plain and molecule graphs show that DISCO can obtain competitive or superior performance against state-of-the-art graph generative models and provide additional sampling flexibility. Our main contributions are summarized: ", "page_idx": 1}, {"type": "text", "text": "\u2022 Model. We propose the first discrete-state continuous-time graph diffusion model, DISCO. We utilize the successful graph-to-graph neural network architecture from DiGress and further explore a new lightweight backbone model with decent efficacy. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Analysis. Our analysis reveals (1) the key connection between the training loss and the approximation error (Theorem 3.3) and (2) invariant/equivariant properties of DISCO in terms of the permutation of nodes (Theorems 3.8 and 3.9). ", "page_idx": 1}, {"type": "text", "text": "\u2022 Experiment. Extensive experiments validate the empirical performance of DISCO. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Discrete-State Continuous-time Diffusion Models ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "A $D$ -dimensional discrete state space is represented as $\\mathcal{X}\\,=\\,\\{1,\\dots,C\\}^{D}$ . A continuous-time Markov Chain (CTMC) $\\{\\mathbf{x}_{t}\\,=\\,[\\dot{x}_{t}^{1},\\cdot\\cdot\\cdot x_{t}^{D}]\\}_{t\\in[0,T]}$ is characterized by its (time-dependent) rate matrix ${\\bf R}_{t}\\in\\mathbb{R}^{|\\mathcal{X}|\\times|\\mathcal{X}|}$ . Here $\\mathbf{x}_{t}$ is the state at the time step $t$ . The transition probability $q_{t\\mid s}$ between from time $s$ to $t$ satisfies the Kolmogorov forward equation, for $s<t$ , ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\frac{d}{d t}q_{t|s}(\\mathbf{x}_{t}|\\mathbf{x}_{s})=\\sum_{\\xi\\in\\mathcal{X}}q_{t|s}(\\xi|\\mathbf{x}_{s})\\mathbf{R}_{t}(\\xi,\\mathbf{x}_{t}),\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "The marginal distribution can be represented as $\\begin{array}{r}{q_{t}(\\mathbf{x}_{t})\\ =\\ \\sum_{\\mathbf{x}_{0}\\in\\mathcal{X}}q_{t|0}(\\mathbf{x}_{t}|\\mathbf{x}_{0})\\pi_{\\mathtt{d a t a}}(\\mathbf{x}_{0})}\\end{array}$ where $\\pi_{\\mathsf{d a t a}}(\\mathbf{x}_{0})$ is the data distribution. If the CTMC is defined in time interval $[0,T]$ and if the rate matrix ${\\bf R}_{t}$ is well-designed, the final distribution $q_{T}(\\mathbf{x}_{T})$ can be close to a tractable reference distribution $\\pi_{\\mathrm{ref}}\\left(\\mathbf{x}_{T}\\right)$ , e.g., uniform distribution. We notate the reverse stochastic process as $\\tilde{\\mathbf{x}}_{t}=\\mathbf{x}_{T-t}$ ; a well-known fact (e.g., Section 5.9 in [63]) is that the reverse process $\\{\\tilde{\\mathbf{x}}_{t}\\}_{t\\in[0,T]}$ is also a CTMC, characterized by the reverse rate matrix: $\\begin{array}{r}{\\tilde{\\mathbf{R}}_{t}(\\mathbf{x},\\mathbf{y})=\\frac{q(\\mathbf{y})}{q(\\mathbf{x})}\\mathbf{R}_{t}(\\mathbf{y},\\mathbf{x})}\\end{array}$ . The goal of the CTMC-based diffusion models is an accurate estimation of the reverse rate matrix $\\tilde{\\mathbf{R}}_{t}$ so that new data can be generated by sampling the reference distribution $\\pi_{\\mathtt{r e f}}$ and then simulating the reverse CTMC [16, 17, 18, 1]. However, the complexity of the rate matrix is prohibitively high because there are $C^{D}$ possible states. A reasonable simplification is to factorize the process over dimensions [8, 71, 73, 2]. Specifically, the forward process is factorized as $\\begin{array}{r}{q_{t|s}(\\mathbf{x}_{t}|\\mathbf{x}_{s})=\\prod_{d=1}^{D}q_{t|s}(x_{t}^{d}|x_{s}^{d})}\\end{array}$ , for $s<t$ . Then, the forward diffusion of each dimension is independent and i s governed by dimension-specific forward rate matrices $\\{\\mathbf{R}_{t}^{d}\\}_{d=1}^{D}$ . With such a factorization, the goal is to estimate the dimension-specific reverse rate matrices { \u02dcRtd }dD=1. ", "page_idx": 1}, {"type": "image", "img_path": "YkSKZEhIYt/tmp/49facdfc602262cf91a886a937e5ff1c8874137e3f7971a8f7a67e1d7b6b5d53.jpg", "img_caption": ["Figure 2: An overview of DISCO. A transition can happen at any time in $[0,T]$ . "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "The dimension-specific reverse rate is represented as $\\tilde{\\mathbf{R}}_{t}^{d}(x^{d},y^{d})$ $=$ $\\begin{array}{r}{\\sum_{x_{0}^{d}}\\mathbf{R}_{t}^{d}(y^{d},x^{d})\\frac{q_{t|0}(y^{d}|x_{0}^{d})}{q_{t|0}(x^{d}|x_{0}^{d})}q_{0|t}(x_{0}^{d}|\\mathbf{x})}\\end{array}$ )qqtt||00((yxd||xx0d))q0|t(x0d|x). Campbell et al. [8] estimate q0|t(x0d|x) via a neural network $p_{\\theta}$ such that $p_{\\theta}(x_{0}^{d}|\\mathbf{x},t)\\approx q_{0|t}(x_{0}^{d}|\\mathbf{x})$ ; Sun et al. [71] propose another singleton conditional distribution-based objective p\u03b8(ydd|x\\\\dd,t) \u2248 q(ydd|x\\\\dd) whose rationale is Brook\u2019s Lemma [5, 49]. ", "page_idx": 2}, {"type": "text", "text": "2.2 Graph Generation and Notations ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We study the graphs with categorical node and edge attributes. A graph with $n$ nodes is represented by its edge type matrix and node type vector: ${\\mathcal{G}}=({\\bf E},{\\bf F})$ , where $\\mathbf{E}=(e^{(i,j)})_{i,j\\in\\mathbb{N}_{\\leq n}^{+}}\\in\\{1,\\dots,a+\\}$ $1\\}^{n\\times n}$ $\\mathbf{\\nabla},\\mathbf{F}=(f^{i})_{i\\in\\mathbb{N}_{\\leq n}^{+}}\\in\\{1,\\dots,b\\}^{n}$ , $a$ and $b$ are the numbers of node and edge types, respectively. Notably, the absence of an edge is viewed as a special edge type, so there are $(a+1)$ edge types in total. The problem we study is graph generation where $N$ graphs $\\{\\mathcal{G}^{i}\\}_{i\\in\\mathbb{N}_{\\leq N}^{+}}$ from an inaccessible graph data distribution $\\mathfrak{G}$ are given and we aim to generate $M$ graphs $\\{\\mathcal{G}^{i}\\}_{i\\in\\mathbb{N}_{\\leq M}^{+}}$ from $\\mathfrak{G}$ . ", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "This section presents the proposed discrete-state continuous-time graph diffusion model, DISCO whose overview is Figure 2. Section 3.1 introduces the necessity to factorize the diffusion process and Section 3.2 details the forward process. Our training objective and its connection to sampling are introduced in Sections 3.3 and 3.4, respectively. Last but not least, a specific neural architecture of the graph-to-graph backbone model and its properties regarding the permutation of node ordering are introduced in Sections 3.5 and 3.6, respectively. All proofs are in Appendix. ", "page_idx": 2}, {"type": "text", "text": "3.1 Factorized Discrete Graph Diffusion Process ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The number of possible states of an $n$ -node graph is $(a+1)^{n^{2}}\\times b^{n}$ which is intractably large. Thus, we follow existing discrete models [2, 8, 71, 73] and formulate the forward processes on every node/edge to be independent. Mathematically, the forward diffusion process for $s<t$ is factorized as ", "page_idx": 2}, {"type": "equation", "text": "$$\nq_{t|s}(\\mathcal{G}_{t}|\\mathcal{G}_{s})=\\prod_{i,j=1}^{n}q_{t|s}(e_{t}^{(i,j)}|e_{s}^{(i,j)})\\prod_{i=1}^{n}q_{t|s}(f_{t}^{i}|f_{s}^{i})\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where the edge type transition probabilities $\\{q_{t|s}(e_{t}^{(i,j)}|e_{s}^{(i,j)})\\}_{i,j\\in\\mathbb{N}_{\\leq n}^{+}}$ and node type transition probabilities $\\{q_{t|s}(f_{t}^{i}|f_{s}^{i})\\}_{i\\in\\mathbb{N}_{\\leq n}^{+}}$ are characterized by their forward ra\u2264tne matrices $\\{\\mathbf{R}_{t}^{(i,j)}\\}_{i,j\\in\\mathbb{N}_{\\leq n}^{+}}$ and $\\{\\mathbf{R}_{t}^{i}\\}_{i\\in\\mathbb{N}_{\\leq n}^{+}}$ , respectively. The forward processes, i.e., the forward rate matrices in our context, are predefined, which will be introduced in Section 3.2. Given the factorization of forward transition probability in Eq. (2), a question is raised: what is the corresponding factorization of the forward rate matrix $({\\bf R}_{t})$ and the reverse rate matrix $(\\tilde{\\mathbf{R}}_{t})$ ? Remark 3.1 shows such a factorization. ", "page_idx": 2}, {"type": "table", "img_path": "YkSKZEhIYt/tmp/0a3c95c83474098ec354708543d58057570f6d842c723fcb918b0259dc686656.jpg", "table_caption": [], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "Remark 3.1. (Factorization of rate matrices, extended from Proposition 3 of [8]) Given the factorized forward process Eq. (2), the overall rate matrices are factorized as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{{\\bf{R}}_{t}(\\bar{\\mathcal{G}},\\mathcal{G})=\\displaystyle\\sum_{i}A_{t}^{i}+\\sum_{i,j}B_{t}^{(i,j)}}\\ ~}\\\\ {{\\tilde{{\\bf{R}}}_{t}(\\mathcal{G},\\bar{\\mathcal{G}})=\\displaystyle\\sum_{i}A_{t}^{i}\\sum_{f_{0}^{i}}\\frac{q_{t|0}\\bigl(\\bar{f}^{i}|f_{0}^{i}\\bigr)}{q_{t|0}\\bigl(f^{i}|f_{0}^{i}\\bigr)}q_{0|t}(f_{0}^{i}|\\mathcal{G})+\\sum_{i,j}B_{t}^{(i,j)}\\sum_{e_{0}^{(i,j)}}\\frac{q_{t|0}\\bigl(\\bar{e}^{(i,j)}|e_{0}^{(i,j)}\\bigr)}{q_{t|0}\\bigl(e^{(i,j)}|e_{0}^{(i,j)}\\bigr)}q_{0|t}(e_{0}^{(i,j)}|\\mathcal{G})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $A_{t}^{i}\\ =\\ {\\bf R}_{t}^{i}(\\bar{f}^{i},f^{i})\\delta_{\\bar{\\mathcal{G}}\\setminus\\bar{f}^{i},\\mathcal{G}\\setminus f^{i}}$ , $B_{t}^{(i,j)}\\ =\\ {\\bf R}_{t}^{(i,j)}(\\bar{e}^{(i,j)},e^{(i,j)})\\delta_{\\bar{g}\\backslash\\bar{e}^{(i,j)},\\bar{g}\\backslash e^{(i,j)}}$ , the operator $\\delta_{\\bar{\\mathcal{G}}\\backslash\\bar{f}^{i},\\mathcal{G}\\backslash f^{i}}$ (or $\\delta_{\\bar{\\mathcal{G}}\\backslash\\bar{e}^{(i,j)},\\mathcal{G}\\backslash e^{(i,j)}})$ checks whether two graphs $\\bar{\\mathcal G}$ and $\\mathcal{G}$ are exactly the same except for node $i$ (or the edge between nodes $i$ and $j$ ). ", "page_idx": 3}, {"type": "text", "text": "Note that this factorization itself is not our contribution but a necessary part of our framework, so we mention it here for completeness. Its full derivation is in Appendix - Section A. Next, we detail the design of forward rate matrices. ", "page_idx": 3}, {"type": "text", "text": "3.2 Forward Process ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "A proper choice of the forward rate matrices $\\{\\mathbf{R}_{t}^{(i,j)}\\}_{i,j\\in\\mathbb{N}_{\\leq n}^{+}}$ and $\\{\\mathbf{R}_{t}^{i}\\}_{i\\in\\mathbb{N}_{\\leq n}^{+}}$ is important because (1) the probability distributions of node and edge types, $\\{q(f_{t}^{i})\\}_{i\\in\\mathbb{N}_{\\leq n}^{+}}$ and $\\{\\bar{q}(e_{t}^{(i,j)})\\}_{i,j\\in\\mathbb{N}_{\\leq n}^{+}}$ , should converge to their reference distributions within $[0,T]$ and (2) the reference distributions should be easy to sample (e.g., uniform distribution). We follow [8] to formulate ${\\bf R}_{t}^{(i,j)}=\\beta(t){\\bf R}_{e}^{(i,j)}$ , $\\forall i,j$ and $\\mathbf{R}_{t}^{i}=\\beta(t)\\mathbf{R}_{f}^{i}$ , $\\forall i$ , where $\\beta(t)$ is a corruption schedule, $\\{\\mathbf{R}_{e}^{(i,j)}\\}$ and $\\{\\mathbf{R}_{f}^{i}\\}$ are the base rate matrices. For brevity, we set all the nodes/edges to share a common node/edge rate matrix, i.e., ${\\bf R}_{e}^{(i,j)}={\\bf R}_{e}$ and $\\mathbf{R}_{f}^{i}=\\mathbf{R}_{f},\\forall i,j.$ Then, the forward transition probability for all the nodes and edges are $q_{t|0}(f_{t}=\\dot{v}|f_{0}\\,=\\,u)\\,=\\,(e^{\\int_{0}^{t}\\beta(s)\\mathbf{R}_{f}d s})_{u v}$ and $q_{t|0}(e_{t}=v|e_{0}=u)=(e^{\\int_{0}^{t}\\beta(s)\\mathbf{R}_{e}d s})_{u v}$ , respectively. We omit the superscript $i$ (or $(i,j);$ ) because the transition probability is shared by all the nodes (or edges). The detailed derivation of the above analytic forward transition probability is provided in Appendix - Section B. ", "page_idx": 3}, {"type": "text", "text": "For categorical data, a reasonable reference distribution is a uniform distribution, i.e., $\\begin{array}{r}{\\pi_{f}=\\frac{1}{b}}\\end{array}$ for nodes and $\\begin{array}{r}{\\pi_{e}=\\frac{1}{a+1}}\\end{array}$ for edges. In addition, inspired by [73], we find that node and edge marginal distributions $\\mathbf{m}_{f}$ and ${\\bf m}_{e}$ are good choices as the reference distributions. Concretely, an empirical estimation of $\\mathbf{m}_{f}$ and ${\\bf m}_{e}$ is to count the number of node/edge types and normalize them. The following proposition shows how to design the rate matrices to guide the forward process to converge to uniform and marginal distributions. ", "page_idx": 3}, {"type": "text", "text": "Proposition 3.2. The forward processes for nodes and edges converge to uniform distributions if $\\mathbf{R}_{f}\\,\\,{\\dot{=}}\\,\\mathbf{11}^{\\top}-b\\mathbf{I}$ and $\\bar{\\mathbf{R}}_{e}=\\mathbf{1}\\mathbf{1}^{\\dagger}-(a+\\bar{1})\\mathbf{I}$ ; they converge to marginal distributions $\\mathbf{m}_{f}$ and $\\mathbf{m}_{e}\\ i f$ $\\mathbf{R}_{f}=\\mathbf{1}\\mathbf{m}_{f}^{\\top}-\\mathbf{I}$ and $\\mathbf{R}_{e}=\\mathbf{1}\\mathbf{m}_{e}^{\\top}-\\mathbf{I}.$ . 1 is an all-one vector and I is an identity matrix. ", "page_idx": 3}, {"type": "text", "text": "Regarding the selection of $\\beta(t)$ , we follow [23, 70, 8] and set $\\beta(t)=\\alpha\\gamma^{t}\\log(\\gamma)$ for a smooth change of the rate matrix. $\\alpha$ and $\\gamma$ are hyperparameters. Detailed settings are in Appendix F.3. ", "page_idx": 3}, {"type": "text", "text": "3.3 Parameterization and Optimization Objective ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Next, we introduce the estimation of the reverse process from its motivation. The reverse process is essentially determined by the reverse rate matrix $\\tilde{\\mathbf{R}}_{t}$ in Eq. (4), whose computation needs $q_{0|t}(f_{0}^{i}|\\mathcal{G})$ and $q_{0|t}(e_{0}^{(i,j)}|\\mathcal{G}),\\,\\forall i,j$ ; their exact estimation is expensive because according to Bayes\u2019 rule, $p_{t}(\\mathcal{G})$ is needed, whose computation needs to enumerate all the given graphs: $\\begin{array}{r}{p_{t}(\\mathcal{G})\\overset{\\cdot}{=}\\sum_{\\mathcal{G}_{0}}q_{t|0}(\\dot{\\mathcal{G}}|\\dot{\\mathcal{G}}_{0})\\pi_{\\mathtt{d a t a}}(\\mathcal{G}_{0})}\\end{array}$ . ", "page_idx": 4}, {"type": "text", "text": "Thus, we propose parameterizing the reverse transition probabilities via a neural network $\\theta$ whose specific architecture is introduced in Section 3.5. The terms $\\{q_{0|t}(f_{0}^{i}|\\mathcal{G})\\}_{i\\in\\mathbb{N}_{\\leq n}^{+}}$ and $\\{q_{0|t}(e_{0}^{(i,j)}|\\mathcal{G})\\}_{i,j\\in\\mathbb{N}_{\\leq n}^{+}}$ in Eq. (4) are replaced with the parameterized $\\{p_{0\\vert t}^{\\theta}(f^{i}\\vert\\mathcal{G})\\}_{i\\in\\mathbb{N}_{\\leq n}^{+}}$ and {p\u03b80|t(e(i,j)|G)}i,j\u2208N+\u2264n. Thus, a parameterized reverse rate matrix $\\tilde{\\mathbf{R}}_{\\theta,t}(\\mathcal{G},\\bar{\\mathcal{G}})$ is represented as $\\begin{array}{r l r}{\\tilde{\\bf R}_{\\theta,t}(\\mathcal{G},\\bar{\\mathcal{G}})}&{=}&{\\sum_{i}\\tilde{\\bf R}_{\\theta,t}^{i}(f^{i},\\bar{f}^{i})\\;+\\;\\sum_{i,j}\\tilde{\\bf R}_{\\theta,t}^{(i,j)}(e^{(i,j)},\\bar{e}^{(i,j)})}\\end{array}$ where $\\begin{array}{r l}{\\tilde{\\mathbf{R}}_{\\theta,t}^{i}(f^{i},\\bar{f}^{i})}&{=}\\end{array}$ $\\begin{array}{r}{A_{t}^{i}\\sum_{f_{0}^{i}}\\frac{q_{t\\mid0}(\\bar{f}^{i}\\mid f_{0}^{i})}{q_{t\\mid0}(f^{i}\\mid f_{0}^{i})}p_{0\\mid t}^{\\theta}(f_{0}^{i}|\\mathcal{G}),\\tilde{\\mathbf{R}}_{\\theta,t}^{(i,j)}(e^{(i,j)},\\bar{e}^{(i,j)})=B_{t}^{(i,j)}\\sum_{e_{0}^{(i,j)}}\\frac{q_{t\\mid0}(\\bar{e}^{(i,j)}\\mid e_{0}^{(i,j)})}{q_{t\\mid0}(e^{(i,j)}\\mid e_{0}^{(i,j)})}p_{0\\mid t}^{\\theta}(e_{0}^{(i,j)}|\\mathcal{G})}\\end{array}$ , and the remaining notations are the same as Eq. (4). Note that all the terms $\\{p_{0\\mid t}^{\\theta}(f^{i}|\\mathcal{G})\\}_{i\\in\\mathbb{N}_{\\leq n}^{+}}$ and $\\{p_{0|t}^{\\theta}(e^{(i,j)}|\\mathcal{G})\\}_{i,j\\in\\mathbb{N}_{\\leq n}^{+}}$ can be viewed together as a graph-to-graph mapping $\\theta:\\mathfrak{G}\\mapsto\\mathfrak{G}$ , whose input is the noisy graph $\\mathcal{G}_{t}$ and its output is the predicted clean graph probabilities, concretely, the node/edge type probabilities of all the nodes and edges. ", "page_idx": 4}, {"type": "text", "text": "Intuitively, the discrepancy between the groundtruth $\\tilde{\\mathbf{R}}_{t}$ (from Eq. (4)) and the parametric $\\tilde{\\mathbf{R}}_{\\theta,t}$ should be small. Theorem 3.3 establishes a cross-entropy (CE)-based upper bound of such a discrepancy, where the estimated probability vectors (sum is 1) are notated as ${\\hat{f}}_{0}^{i}\\,=\\,[p_{0|t}^{\\theta}(f^{i}\\,=$ $1|{\\mathcal{G}}_{t}),\\ldots,p_{0|t}^{\\theta}(f^{i}\\;=\\;b|{\\mathcal{G}}_{t})]^{\\top}\\;\\in\\;[0,1]^{b}$ and $\\hat{e}_{0}^{(i,j)}\\;=\\;[p_{0|t}^{\\theta}(e^{(i,j)}\\;=\\;1|{\\mathcal G}_{t}),\\..\\.,p_{0|t}^{\\theta}(e^{(i,j)}\\;=\\;a\\;+\\;$ 1|Gt)]\u22a4\u2208[0, 1]a+1. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.3 (Approximation error). for $\\mathcal G\\neq\\bar{\\mathcal G}$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Big|\\tilde{\\mathbf{R}}_{t}(\\mathcal{G},\\bar{\\mathcal{G}})-\\tilde{\\mathbf{R}}_{\\theta,t}(\\mathcal{G},\\bar{\\mathcal{G}})\\Big|^{2}\\leq C_{t}+C_{t}^{n o d e}\\mathbb{E}_{\\mathcal{G}_{0}}q_{t|0}(\\mathcal{G}|\\mathcal{G}_{0})\\sum_{i}\\mathcal{L}_{\\mathrm{cE}}\\big(0\\mathrm{ne}\\!-\\!\\!\\mathrm{Hot}(f_{0}^{i}),\\hat{f}_{0}^{i}\\big)}\\\\ {+\\;C_{t}^{e d g e}\\mathbb{E}_{\\mathcal{G}_{0}}q_{t|0}(\\mathcal{G}|\\mathcal{G}_{0})\\sum_{i,j}\\mathcal{L}_{\\mathrm{cE}}\\big(0\\mathrm{ne}\\!-\\!\\!\\mathrm{Hot}(e_{0}^{(i,j)}),\\hat{e}_{0}^{(i,j)}\\big)~.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $C_{t},\\,C_{t}^{n o d e}$ , and $C_{t}^{e d g e}$ are constants independent on $\\theta$ but dependent on $t,\\,\\mathcal{G}_{i}$ , and $\\bar{\\mathcal G}$ ; One-Hot transforms $f_{0}^{i}$ and $e_{0}^{(i,j)}$ into one-hot vectors. ", "page_idx": 4}, {"type": "text", "text": "The bound in Theorem 3.3 is tight, i.e., the right-hand side of Eq. (5) is 0, whenever ${\\hat{f}}_{0}^{i}\\;=\\;$ $q_{0|t}(f_{0}^{i}|\\mathcal{G}_{t}),\\forall i$ and $\\hat{e}_{0}^{(i,j)}\\;=\\;q_{0|t}(e_{0}^{(i,j)}|\\mathcal{G}_{t}),\\forall i,j$ . Guided by Theorem 3.3, we (1) take expectation of $t$ by sampling $t$ from a uniform distribution $t\\sim\\mathcal{U}_{(0,T)}$ and (2) simplify the right-hand side of Eq. (5) by using the unweighted CE loss as our training objective: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta}~T\\mathbb{E}_{t}\\mathbb{E}_{\\mathcal{G}_{0}}\\mathbb{E}_{q_{t\\mid0}(\\mathcal{G}_{t}|\\mathcal{G}_{0})}\\Big[\\sum_{i}\\mathcal{L}_{\\mathrm{cE}}\\big(0\\mathrm{ne}\\mathrm{-}\\mathrm{Hot}\\big(f_{0}^{i}\\big),\\hat{f}_{0}^{i}\\big)+\\sum_{i,j}\\mathcal{L}_{\\mathrm{cE}}\\big(0\\mathrm{ne}\\mathrm{-}\\mathrm{Hot}\\big(e_{0}^{(i,j)}\\big),\\hat{e}_{0}^{(i,j)}\\big)\\Big]\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "A step-by-step training algorithm is in Algorithm 1. Note that the above CE loss has been used in some diffusion models (e.g., [2, 8]) but lacks a good motivation, especially in the continuous-time setting. We motivate it based on the rate matrix discrepancy, as a unique contribution of this paper. ", "page_idx": 4}, {"type": "text", "text": "3.4 Sampling Reverse Process ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Given the parametric reverse rate matrix $\\tilde{\\mathbf{R}}_{\\theta,t}(\\mathcal{G},\\bar{\\mathcal{G}})$ , the graph generation process can be implemented by two steps: (1) sampling the reference distribution $\\pi_{\\mathtt{r e f}}$ (i.e., $\\pi_{f}$ for nodes and $\\pi_{e}$ for edges) and (2) numerically simulating the CTMC from time $T$ to 0. The exact simulation of a CTMC has been studied for a long time, e.g., [16, 17, 1]. However, their simulation strategies only allow one transition (e.g., one edge/node type change) per step, which is highly inefficient for graphs as the number of nodes and edges is typically large; once a(n) node/edge is updated, $\\tilde{\\mathbf{R}}_{\\theta,t}$ requires recomputation. A practical approximation is to assume $\\tilde{\\mathbf{R}}_{\\theta,t}$ is fixed during a time interval $[t-\\tau,t]$ , i.e., delaying the happening of transitions in $[t-\\tau,t]$ and triggering them all together at the time $t-\\tau$ ; this strategy is also known as $\\tau$ -leaping [18, 8, 71], and DISCO adopts it. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "We elaborate on $\\tau$ -leaping for transitions of node types; the transitions of edge types are similar. The rate matrix of the $i$ -th node is fixed as $\\begin{array}{r}{\\tilde{\\mathbf{R}}_{\\theta,t}^{i}(f^{i},\\bar{f}^{i})=\\mathbf{R}_{t}^{i}(\\bar{f}^{i},f^{i})\\sum_{f_{0}^{i}}\\frac{q_{t|0}(\\bar{f}^{i}|f_{0}^{i})}{q_{t|0}(f^{i}|f_{0}^{i})}p_{0|t}^{\\theta}(f^{i}|\\mathcal{G}_{t})}\\end{array}$ , during $[t-\\tau,t]$ . According to the definition of rate matrix, in $[t-\\tau,t]$ , the number of transitions from $f^{i}$ to ${\\bar{f}}^{i}$ , namely $J_{f^{i},{\\bar{f}}^{i}}$ , follows the Poisson distribution, i.e., $J_{f^{i},\\bar{f}^{i}}^{\\bar{}}\\,\\sim\\,\\mathrm{Poisson}(\\tau\\tilde{\\bf R}_{\\theta,t}^{i}(f^{i},\\bar{f}^{i}))$ . For categorical data (e.g., node type), multiple transitions in $[t-\\tau,t]$ are invalid and meaningless. In other words, for the $i$ -th node, if the total number of transitions $\\sum\\bar{f}^{i}\\;{\\cal J}_{f^{i},\\bar{f}^{i}}>1$ , $f^{i}$ keeps unchanged in $[t-\\tau,t]$ ; otherwise, if $\\sum_{\\bar{f}^{i}}J_{f^{i},\\bar{f}^{i}}=1$ and $J_{f^{i},s}=1$ , i.e., there is exact 1 transition, $f^{i}$ jumps to $s$ . A step-by-step sampling algorithm (Algorithm 2) is in Appendix. ", "page_idx": 5}, {"type": "text", "text": "Remark 3.4. The sampling error of $\\tau$ -leaping is linear to $C_{\\mathsf{e r r}}$ [8], the approximation error of the reverse rates: $\\begin{array}{r}{\\sum_{\\mathcal{G}\\neq\\bar{\\mathcal{G}}}\\left|\\tilde{\\mathbf{R}}_{t}(\\mathcal{G},\\bar{\\mathcal{G}})-\\tilde{\\mathbf{R}}_{\\theta,t}(\\mathcal{G},\\bar{\\mathcal{G}})\\right|\\leq C_{\\mathrm{err}}.}\\end{array}$ . Interested readers are referred to Theorem 1 from [8]. Our Theorem 3.3 shows the connection between our training loss and $C_{\\mathsf{e r r}}$ , which further verifies the correctness of our training loss. ", "page_idx": 5}, {"type": "text", "text": "3.5 Model Instantiation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "As mentioned in Section 3.3, the parametric backbone $p_{0|t}^{\\theta}(\\mathcal{G}_{0}|\\mathcal{G}_{t})$ is a graph-to-graph mapping whose input is the noisy graph $\\mathcal{G}_{t}$ and its output is the predicted denoised graph $\\mathcal{G}_{0}$ . There exists a broad range of neural network architectures. Notably, DiGress [73] uses a graph Transformer (GT) as $p_{0|t}^{\\theta}$ a decent reference for our continuous-time framework. We name our model with the GT backbone as DISCO-GT and its detailed configuration is in Appendix F.3. The main advantage of the GT is its long-range interaction thanks to the complete self-attention graph; however, the architecture is very complex and includes multi-head self-attention modules, leading to expensive computation. ", "page_idx": 5}, {"type": "text", "text": "Beyond GTs, in this paper, we posit that a regular message-passing neural network (MPNN) [19] should be a promising choice for $p_{0|t}^{\\theta}(\\mathcal{G}_{0}|\\mathcal{G}_{t})$ . It is recognized that the MPNNs\u2019 expressiveness might not be as good as GTs\u2019 [33, 7], e.g., in terms of long-range interactions. However, in our setting, the absence of an edge is viewed as a special type of edge and the whole graph is complete; therefore, such a limitation of MPNN is naturally mitigated, which is verified by our empirical evaluations. ", "page_idx": 5}, {"type": "text", "text": "Concretely, an MPNN-based graph-to-graph mapping is presented as follows, and DISCO with MPNN backbone is named DISCO-MPNN. Given a graph $\\mathcal{G}\\,\\bar{=}\\,({\\bf E},{\\bf F})$ , where ${\\bf E}\\in\\{1,\\dots,a,a+1\\}^{n\\times n}$ , $\\mathbf{F}\\,\\in\\,\\{1,\\dots,b\\}^{n}$ , we first transform both the matrix $\\mathbf{E}$ and $\\mathbf{F}$ into one-hot embeddings $\\mathbf{E}_{0\\mathrm{H}}\\ \\in$ $\\{0,1\\}^{n\\times n\\times(a+1)}$ and ${\\bf F}_{0\\mathrm{H}}\\in\\{0,1\\}^{n\\times b}$ . Then, some auxiliary features (e.g., the # of specific motifs) are extracted: $\\mathbf{F}_{\\mathrm{aux}},\\mathbf{y}_{\\mathrm{aux}}=\\mathsf{A u x}(\\mathbf{E}_{0\\mathrm{H}})$ to overcome the expressiveness limitation of MPNNs [11]. Here $\\mathbf{F}_{\\mathrm{aux}}$ and $\\mathbf{y_{aux}}$ are the node and global auxiliary features, respectively. Note that a similar auxiliary feature engineering is also applied in DiGress [73]. More details about the Aux can be found in Appendix E. Then, three multi-layer perceptrons (MLPs) are used to map node features $\\mathbf{F}_{0\\mathrm{H}}\\oplus\\mathbf{F}_{\\mathrm{aux}}$ , edge features ${\\bf E}_{0\\mathrm{H}}$ , and global features $\\mathbf{y_{aux}}$ into a common hidden space as $\\mathbf{F}_{\\mathrm{hidden}}=$ $\\mathtt{M L P}(\\mathbf{F}_{0\\mathrm{H}}\\oplus\\mathbf{F}_{\\mathrm{aux}})$ , $\\mathbf{E}_{\\mathrm{hidden}}=\\mathtt{M L P}(\\mathbf{E}_{0\\mathrm{H}})$ , $\\mathbf{y}_{\\mathrm{hidden}}=\\mathtt{M L P}(\\mathbf{y}_{\\mathrm{aux}})$ , where $\\bigoplus$ is a concatenation operator. The following formulas present the update of node embeddings (e.g., $\\mathbf{r}^{i}=\\mathbf{F}(i,:))$ , edge embedding (e.g., $\\mathbf{r}^{(i,j)}=\\mathbf{E}(i,j,:))$ , and global embedding $\\mathbf{y}$ in an MPNN layer, where we omit the subscript hidden if it does not cause ambiguity: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{r}^{i}\\leftarrow\\mathbf{FiLM}\\bigg(\\mathbf{FiLM}\\bigg(\\mathbf{r}^{i},\\mathtt{M L P}\\bigg(\\displaystyle\\sum_{j=1}^{n}\\mathbf{r}^{(j,i)}/n\\bigg)\\bigg),\\mathbf{y}\\bigg),\\ \\mathbf{r}^{(i,j)}\\leftarrow\\mathbf{FiLM}\\big(\\mathbf{FiLM}(\\mathbf{r}^{(i,j)},\\mathbf{r}^{i}\\odot\\mathbf{r}^{j}),\\mathbf{y}\\big),}\\\\ &{\\ \\mathbf{y}\\leftarrow\\mathbf{y}+\\mathbf{PMA}\\big(\\big\\{\\mathbf{r}^{i}\\big\\}_{i=1}^{n}\\big)+\\mathbf{PNA}\\big(\\big\\{\\mathbf{r}^{(i,j)}\\big\\}_{i,j=1}^{n}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The edge embeddings are aggregated by mean pooling (i.e., $\\textstyle\\sum_{j=1}^{n}\\mathbf{r}^{(j,i)}/n)$ ; the node pair embeddings are passed to edges by Hadamard product (i.e., $\\mathbf{r}^{i}\\odot\\mathbf{r}^{j},$ ); edge/node embeddings are merged to the global embedding y via the PNA module [12]; Some FiLM modules [57] are used for the interaction between node/edge/global embeddings. More details about the PNA and FiLM are in Appendix E. In this paper, we name Eqs. (7) and (8) on all nodes/edges together as an MPNN layer, $\\mathbf{F},\\mathbf{E},\\mathbf{y}\\gets\\mathbb{M P N N}(\\mathbf{F},\\mathbf{E},\\mathbf{y})$ . Stacking multiple MPNN layers leads to larger model capacity. ", "page_idx": 5}, {"type": "text", "text": "Finally, two readout MLPs are used to project the node/edge embeddings into input dimensions, $\\mathtt{M L P}(\\mathbf{F})\\in\\mathbb{R}^{n\\times b}$ and $\\mathtt{M L P}(\\mathbf{E})\\in\\mathbb{R}^{n\\times n\\times(\\bar{a}+1)}$ , which are output after wrapped with softmax. Both the proposed MPNN and the GT from DiGress [73] use the PNA and FiLM to merge embeddings, but MPNN does not have multi-head self-attention layers so that the computation overhead is lower. ", "page_idx": 6}, {"type": "text", "text": "3.6 Permutation Equivariance and Invariance ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Reordering the nodes keeps the property of a given graph, which is known as permutation invariance. In addition, for a given function if its input is permuted and its output is permuted accordingly, such a behavior is known as permutation equivariance. In this subsection, we analyze permutationequivariance/invariance of the (1) diffusion framework (Lemmas 3.5, 3.6, and 3.7), (2) sampling density (Theorem 3.8), and (3) training loss (Theorem 3.9). ", "page_idx": 6}, {"type": "text", "text": "Lemma 3.5 (Permutation-equivariant layer). The proposed MPNN layer (Eqs. (7) and (8)) is permutation-equivariant. ", "page_idx": 6}, {"type": "text", "text": "The auxiliary features from the Aux are also permutation-equivariant (see Appendix E). Thus, the whole MPNN-based backbone $p_{0|t}^{\\theta}$ is permutation-equivariant. Note that the GT-based backbone from DiGress [73] is also permutation-equivariant whose proof is omitted as it is not our contribution. Next, we show the permutation invariance of the rate matrices. ", "page_idx": 6}, {"type": "text", "text": "Lemma 3.6 (Permutation-invariant rate matrices). The forward rate matrix of DISCO is permutationinvariant if it is factorized as Eq. (3). The parametric reverse rate matrix of DISCO $(\\tilde{\\mathbf{R}}_{\\theta,t})$ is permutation-invariant whenever the graph-to-graph backbone $p_{0|t}^{\\theta}$ is permutation-equivariant. ", "page_idx": 6}, {"type": "text", "text": "Lemma 3.7 (Permutation-invariant transition probability). For CTMC satisfying the Kolmogorov forward equation (Eq. (1)), if the rate matrix is permutation-invariant (i.e., $\\begin{array}{r l}{\\mathbf{R}_{t}(\\mathbf{x}_{i},\\mathbf{x}_{j})}&{{}=}\\end{array}$ $\\mathbf{R}_{t}(\\mathcal{P}(\\mathbf{x}_{i}),\\mathcal{P}(\\mathbf{x}_{j}))$ , the transition probability is permutation-invariant (i.e., $\\begin{array}{r l}{q_{t|s}(\\mathbf{x}_{t}|\\mathbf{x}_{s})}&{{}=}\\end{array}$ $q_{t\\mid s}(\\mathcal{P}(\\mathbf{x}_{t})\\mid\\mathcal{P}(\\mathbf{x}_{s}))$ , where $\\mathcal{P}$ is a permutation. ", "page_idx": 6}, {"type": "text", "text": "Based on Lemmas 3.6 and 3.7, DISCO\u2019s parametric reverse transition probability is permutationinvariant. The next theorem shows the permutation-invariance of the sampling probability. ", "page_idx": 6}, {"type": "text", "text": "Theorem 3.8 (Permutation-invariant sampling probability). If both the reference distribution $\\pi_{r e f}$ and the reverse transition probability are permutation-invariant, the parametric sampling distribution $p_{0}^{\\theta}(\\mathcal{G}_{0})$ is permutation-invariant. ", "page_idx": 6}, {"type": "text", "text": "In addition, the next theorem shows the permutation invariance of the training loss. ", "page_idx": 6}, {"type": "text", "text": "Theorem 3.9 (Permutation-invariant training loss). The proposed training loss Eq. (6) is invariant to any permutation of the input graph $\\mathcal{G}_{0}$ if $p_{0|t}^{\\theta^{\\leftarrow}}$ is permutation-equivariant. ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "This section includes: an effectiveness evaluation on plain graphs (Section 4.1) and molecule graphs (Section 4.2), an efficiency study (Section 4.3), and an ablation study (Section 4.4). Detailed settings (Sections F.1-F.3), additional effectiveness evaluation (Sections F.4, additional ablation study (Section F.5), convergence study (Section F.6), and visualization (Section F.7) are in Appendix. Our code is released 3. ", "page_idx": 6}, {"type": "text", "text": "4.1 Plain Graph Generation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Datasets and metrics. Datasets SBM, Planar [51], and Community [82] are used. The relative squared Maximum Mean Discrepancy (MMD) for degree distributions (Deg.), clustering coefficient distributions (Clus.), and orbit counts (Orb.) distributions (the number of occurrences of substructures with 4 nodes), Uniqueness $\\%)$ , Novelty $\\%)$ , and Validity $\\%)$ are chosen as metrics. Details about the datasets, metrics, baselines (Section F.2.2), and results on Community (Table 8) are in Appendix. ", "page_idx": 6}, {"type": "text", "text": "Results. Table 1 shows the effectiveness evaluation on SBD and Planar from which we observe: ", "page_idx": 6}, {"type": "table", "img_path": "YkSKZEhIYt/tmp/b54464f2f9edb7e0aaf3e30e9282eb9e129db7adda633eed556eb77ef45fb483.jpg", "table_caption": ["Table 1: Performance (mean\u00b1std) on SBM and Planar datasets. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "YkSKZEhIYt/tmp/d46797be50d57c3103868fbd74197766f280ad32f7afb473e06d9f7ed1edac21.jpg", "table_caption": ["Table 2: Performance (mean $\\pm\\mathrm{std}\\%)$ on QM9 dataset. V., U., and N. mean Valid, Unique, and Novel. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "\u2022 DISCO-GT can obtain competitive performance against the SOTA, DiGress, which is reasonable because both models share the graph Transformer backbone. Note that DiGress\u2019s performance in terms of Validity is not the statistics reported in the paper but from their latest model checkpoint 4. In fact, we found it very hard for DiGress and DISCO-GT to learn to generate valid SBM/Planar graphs. These two datasets have only 200 graphs, but sometimes only after $>\\,10$ , 000 epochs training, the Validity percentage can be $>50\\bar{\\%}$ . Additionally, DISCO-GT provides extra flexibility during sampling by adjusting the $\\tau$ . This is important: our models can still trade-off between the sampling efficiency and quality even after the model is trained and frozen. \u2022 In general, DISCO-MPNN has competitive performance against DISCO-GT in terms of Deg., Clus., and Orb. However, its performance is worse compared to DISCO-GT in terms of Validity, which might be related to the different model expressiveness. Studying the graph-to-graph model expressiveness would be an interesting future direction, e.g., generating valid Planar graphs. ", "page_idx": 7}, {"type": "table", "img_path": "YkSKZEhIYt/tmp/66cef7c66be5adb8b08fe81e5147011be2ed518285b6a5014a6c275a29aae78c.jpg", "table_caption": ["Table 3: Performance on MOSES. VAE, JT-VAE, and GraphINVENT have hard-coded rules to ensure high validity. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "YkSKZEhIYt/tmp/f405b1d9a9852b5ef08be938b2560bb6dec7c3d3d1fac46dda64d780dfc2feae.jpg", "table_caption": ["Table 4: Performance on GuacaMol. LSTM, NAGVAE, and MCTS are tailored for molecule datasets; ConGress, DiGress, and DISCO are general graph generation models. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.2 Molecule Graph Generation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Dataset and metrics. The datasets QM9 [62], MOSES [58], and GuacaMol [6] are chosen. For MOSES, metrics including Uniquess, Novelty, Validity, Filter, FCD, SNN, and Scaf are reported in Table 3. For QM9, metrics include Uniqueness, Novelty, and Validity. For GuacaMol, metrics include Valid, Unique, Novel, KL div, and FCD. Details about the datasets, metrics, and baseline methods are in Appendix F.2.3. ", "page_idx": 8}, {"type": "text", "text": "Results. Table 2 shows the performance on QM9 dataset. Our observation is consistent with the performance comparison on plain datasets: (1) DISCO-GT obtains slightly better or at least competitive performance against DiGress due to the shared graph-to-graph backbone, but our framework offers extra flexibility in the sampling process; (2) DISCO-MPNN obtains decent performance in terms of Validity, Uniqueness, and Novelty comparing with DISCO-GT. ", "page_idx": 8}, {"type": "text", "text": "Tables 3 and 4 show the performance on MOSES and GuacaMol which further verifies that (1) performance of DISCO-GT is on par with the SOTA general graph generative models, DiGress and (2) DISCO-MPNN has decent performance, but worse than DISCO-GT and DiGress. ", "page_idx": 8}, {"type": "text", "text": "4.3 Efficiency Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "A major computation bottleneck is the graph-to-graph backbone $p_{0|t}^{\\theta^{-}}$ , which is GT or MPNN. We compare the number of parameters, the forward and backpropagation time of GT and MPNN in Table 5. For a fair comparison, we set all the hidden dimensions of GT and MPNN as 256 and the number of layers as 5. We use the Community [82] dataset and set the batch size as 64. Table 5 shows that GT has a larger capacity and more parameters at the expense of more expensive training. ", "page_idx": 8}, {"type": "table", "img_path": "YkSKZEhIYt/tmp/4ab475e1ce7b7726165173cb4372051c62a37e121c70bd0f884e4508d44986a4.jpg", "table_caption": ["Table 5: Efficiency comparison in terms of number of parameters, forward and backpropagation time (second/iteration). "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.4 Ablation Study ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "An ablation study on DISCO-GT for reference distributions (marginal vs. uniform), and sampling steps (1 to 500) is presented in Table 6. The number of sampling steps is $\\mathtt{r o u n d}(\\frac{1}{\\tau})$ if $T\\,=\\,1$ . QM9 dataset is chosen. A similar ablation study on DISCO-MPNN is in Table 9 in Appendix. We observe that first, generally, the fewer sampling steps, the lower the generation quality. In some cases (e.g., the marginal distribution) with the sampling steps decreasing significantly (e.g., from 500 to 30), the performance degradation is still very slight, implying our method\u2019s high robustness in sampling steps. Second, the marginal reference dis", "page_idx": 9}, {"type": "table", "img_path": "YkSKZEhIYt/tmp/d75fa4595ccc43306f3827bd3ad5416af9c3519795df142db0c175544cc7f640.jpg", "table_caption": ["Table 6: Ablation study $(\\mathrm{mean}{\\pm}\\mathrm{std}\\%)$ with GT backbone. V., U., and N. mean Valid, Unique, and Novel. "], "table_footnote": ["tribution is better than the uniform distribution, consistent with the observation from DiGress [73]. "], "page_idx": 9}, {"type": "text", "text": "5 Related Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Diffusion models [80] can be interpreted from both the score-matching [69, 70] or the variational autoencoder perspective [23, 35, 34]. Pioneering efforts on diffusion generative modeling study the process in continuous-state [67, 23, 68] whose typical reference distribution is Gaussian. Beyond that, some efforts propose discrete-state models [24] to . E.g., D3PM [2] designs the discrete diffusion process by multiplication of transition matrices; $\\tau$ -LDR [8] generalizes D3PM by formulating a continuous-time Markov chain; [71] proposes a singleton conditional distribution-based objective for the continuous-time Markov chain-based model whose rationale is Brook\u2019s Lemma [5, 49]. ", "page_idx": 9}, {"type": "text", "text": "Diffusion models are widely used in graph generation tasks [44, 13, 85, 86, 84, 15, 83, 14, 61, 74, 47, 79, 78, 59, 3] such as molecule design [65, 25, 27, 43]. Pioneering works such as EDP-GNN [56] and GDSS [32] diffuse graph data in a continuous state space [26]. DiscDDPM [22] is an early effort to modify the DDPM architecture into a discrete state. In addition, DiGress [73] is also a one-shot discrete-state diffusion model, followed by a very recent work MCD [46], both in the discrete-time setting. Beyond the above-mentioned efforts, DruM [31] proposes to mix the diffusion process. EDGE [10] proposes an interesting process: diffusing graphs into empty graphs. Besides, GRAPHARM [36] proposes an autoregressive graph diffusion model, and [45] applies the diffusion models for molecule property prediction tasks. In addition to the above-mentioned general graph diffusion models, there are many other task-tailored graph diffusion generative models [48, 30, 41, 60, 77, 76, 4, 75], which incorporate more in-depth domain expertise into the model design. Interested readers are referred to this survey [44]. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper introduces the first discrete-state continuous-time graph diffusion generative model, DISCO. Our model effectively marries continuous-time Markov Chain formulation with the discrete nature of graph data, addressing the fundamental sampling limitation of prior models. DISCO\u2019s training objective is concise with a solid theoretical foundation. We also propose a simplified message-passing architecture to serve as the graph-to-graph backbone, which theoretically has desirable properties against permutation of node ordering and empirically demonstrates decent performance against existing graph generative models in tests on various datasets. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "ZX, RQ, ZZ, and HT are partially supported by NSF (2324770). The content of the information in this document does not necessarily reflect the position or the policy of the Government, and no official endorsement should be inferred. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] David F Anderson. A modified next reaction method for simulating chemical systems with time dependent propensities and delays. The Journal of chemical physics, 127(21), 2007.   \n[2] Jacob Austin, Daniel D. Johnson, Jonathan Ho, Daniel Tarlow, and Rianne van den Berg. Structured denoising diffusion models in discrete state-spaces. In Marc\u2019Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 17981\u201317993, 2021.   \n[3] Yikun Ban, Yunzhe Qi, and Jingrui He. Neural contextual bandits for personalized recommendation. In Tat-Seng Chua, Chong-Wah Ngo, Roy Ka-Wei Lee, Ravi Kumar, and Hady W. Lauw, editors, Companion Proceedings of the ACM on Web Conference 2024, WWW 2024, Singapore, Singapore, May 13-17, 2024, pages 1246\u20131249. ACM, 2024.   \n[4] Fan Bao, Min Zhao, Zhongkai Hao, Peiyao Li, Chongxuan Li, and Jun Zhu. Equivariant energy-guided sde for inverse molecular design. In The eleventh international conference on learning representations, 2023.   \n[5] D Brook. On the distinction between the conditional probability and the joint probability approaches in the specification of nearest-neighbour systems. Biometrika, 51(3/4):481\u2013483, 1964.   \n[6] Nathan Brown, Marco Fiscato, Marwin H. S. Segler, and Alain C. Vaucher. Guacamol: Benchmarking models for de novo molecular design. J. Chem. Inf. Model., 59(3):1096\u20131108, 2019.   \n[7] Chen Cai, Truong Son Hy, Rose Yu, and Yusu Wang. On the connection between MPNN and graph transformer. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 3408\u20133430. PMLR, 2023.   \n[8] Andrew Campbell, Joe Benton, Valentin De Bortoli, Thomas Rainforth, George Deligiannidis, and Arnaud Doucet. A continuous time framework for discrete denoising models. In NeurIPS, 2022.   \n[9] Nicola De Cao and Thomas Kipf. Molgan: An implicit generative model for small molecular graphs. CoRR, abs/1805.11973, 2018.   \n[10] Xiaohui Chen, Jiaxing He, Xu Han, and Li-Ping Liu. Efficient and degree-guided graph generation via discrete diffusion modeling. In Proceedings of the 40th International Conference on Machine Learning, pages 4585\u20134610, 2023.   \n[11] Zhengdao Chen, Lei Chen, Soledad Villar, and Joan Bruna. Can graph neural networks count substructures? In Hugo Larochelle, Marc\u2019Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.   \n[12] Gabriele Corso, Luca Cavalleri, Dominique Beaini, Pietro Li\u00f2, and Petar Velickovic. Principal neighbourhood aggregation for graph nets. In Hugo Larochelle, Marc\u2019Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.   \n[13] Kaize Ding, Zhe Xu, Hanghang Tong, and Huan Liu. Data augmentation for deep graph learning: A survey. SIGKDD Explor., 24(2):61\u201377, 2022.   \n[14] Dongqi Fu and Jingrui He. Natural and artificial dynamics in graphs: Concept, progress, and future. Frontiers Big Data, 5, 2022.   \n[15] Dongqi Fu, Zhe Xu, Hanghang Tong, and Jingrui He. Natural and artificial dynamics in gnns: A tutorial. In Tat-Seng Chua, Hady W. Lauw, Luo Si, Evimaria Terzi, and Panayiotis Tsaparas, editors, Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining, WSDM 2023, Singapore, 27 February 2023 - 3 March 2023, pages 1252\u20131255. ACM, 2023.   \n[16] Daniel T Gillespie. A general method for numerically simulating the stochastic time evolution of coupled chemical reactions. Journal of computational physics, 22(4):403\u2013434, 1976.   \n[17] Daniel T Gillespie. Exact stochastic simulation of coupled chemical reactions. The journal of physical chemistry, 81(25):2340\u20132361, 1977.   \n[18] Daniel T Gillespie. Approximate accelerated stochastic simulation of chemically reacting systems. The Journal of chemical physics, 115(4):1716\u20131733, 2001.   \n[19] Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural message passing for quantum chemistry. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learning Research, pages 1263\u20131272. PMLR, 2017.   \n[20] Rafael G\u00f3mez-Bombarelli, David Duvenaud, Jos\u00e9 Miguel Hern\u00e1ndez-Lobato, Jorge AguileraIparraguirre, Timothy D. Hirzel, Ryan P. Adams, and Al\u00e1n Aspuru-Guzik. Automatic chemical design using a data-driven continuous representation of molecules. CoRR, abs/1610.02415, 2016.   \n[21] Rafael G\u00f3mez-Bombarelli, Jennifer N Wei, David Duvenaud, Jos\u00e9 Miguel Hern\u00e1ndez-Lobato, Benjam\u00edn S\u00e1nchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D Hirzel, Ryan P Adams, and Al\u00e1n Aspuru-Guzik. Automatic chemical design using a data-driven continuous representation of molecules. ACS central science, 4(2):268\u2013276, 2018.   \n[22] Kilian Konstantin Haefeli, Karolis Martinkus, Nathana\u00ebl Perraudin, and Roger Wattenhofer. Diffusion models for graphs benefit from discrete state spaces. CoRR, abs/2210.01549, 2022.   \n[23] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Hugo Larochelle, Marc\u2019Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.   \n[24] Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forr\u00e9, and Max Welling. Argmax flows and multinomial diffusion: Learning categorical distributions. In Marc\u2019Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 12454\u201312465, 2021.   \n[25] Emiel Hoogeboom, Victor Garcia Satorras, Cl\u00e9ment Vignac, and Max Welling. Equivariant diffusion for molecule generation in 3d. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesv\u00e1ri, Gang Niu, and Sivan Sabato, editors, International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pages 8867\u20138887. PMLR, 2022.   \n[26] Han Huang, Leilei Sun, Bowen Du, Yanjie Fu, and Weifeng Lv. Graphgdp: Generative diffusion processes for permutation invariant graph generation. In Xingquan Zhu, Sanjay Ranka, My T. Thai, Takashi Washio, and Xindong Wu, editors, IEEE International Conference on Data Mining, ICDM 2022, Orlando, FL, USA, November 28 - Dec. 1, 2022, pages 201\u2013210. IEEE, 2022.   \n[27] Lei Huang, Hengtong Zhang, Tingyang Xu, and Ka-Chun Wong. MDM: molecular diffusion model for 3d molecule generation. In Brian Williams, Yiling Chen, and Jennifer Neville, editors, Thirty-Seventh AAAI Conference on Artificial Intelligence, AAAI 2023, Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence, IAAI 2023, Thirteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2023, Washington, DC, USA, February 7-14, 2023, pages 5105\u20135112. AAAI Press, 2023.   \n[28] Jan H Jensen. A graph-based genetic algorithm and generative model/monte carlo tree search for the exploration of chemical space. Chemical science, 10(12):3567\u20133572, 2019.   \n[29] Wengong Jin, Regina Barzilay, and Tommi S. Jaakkola. Junction tree variational autoencoder for molecular graph generation. In Jennifer G. Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm\u00e4ssan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pages 2328\u20132337. PMLR, 2018.   \n[30] Bowen Jing, Gabriele Corso, Jeffrey Chang, Regina Barzilay, and Tommi S. Jaakkola. Torsional diffusion for molecular conformer generation. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022.   \n[31] Jaehyeong Jo, Dongki Kim, and Sung Ju Hwang. Graph generation with destination-predicting diffusion mixture. OpenReview, 2023.   \n[32] Jaehyeong Jo, Seul Lee, and Sung Ju Hwang. Score-based generative modeling of graphs via the system of stochastic differential equations. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesv\u00e1ri, Gang Niu, and Sivan Sabato, editors, International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pages 10362\u201310383. PMLR, 2022.   \n[33] Jinwoo Kim, Dat Nguyen, Seonwoo Min, Sungjun Cho, Moontae Lee, Honglak Lee, and Seunghoon Hong. Pure transformers are powerful graph learners. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022.   \n[34] Diederik P. Kingma and Ruiqi Gao. Understanding diffusion objectives as the ELBO with simple data augmentation. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023.   \n[35] Diederik P. Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. CoRR, abs/2107.00630, 2021.   \n[36] Lingkai Kong, Jiaming Cui, Haotian Sun, Yuchen Zhuang, B. Aditya Prakash, and Chao Zhang. Autoregressive diffusion model for graph generation. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 17391\u201317408. PMLR, 2023.   \n[37] Igor Krawczuk, Pedro Abranches, Andreas Loukas, and Volkan Cevher. Gg-gan: A geometric graph generative adversarial network, 2021. In URL https://openreview. net/forum, 2020.   \n[38] Matt J. Kusner and Jos\u00e9 Miguel Hern\u00e1ndez-Lobato. GANS for sequences of discrete elements with the gumbel-softmax distribution. CoRR, abs/1611.04051, 2016.   \n[39] Youngchun Kwon, Dongseon Lee, Youn-Suk Choi, Kyoham Shin, and Seokho Kang. Compressed graph representation for scalable molecular graph generation. J. Cheminformatics, 12(1):58, 2020.   \n[40] Youngchun Kwon, Dongseon Lee, Youn-Suk Choi, Kyoham Shin, and Seokho Kang. Compressed graph representation for scalable molecular graph generation. Journal of Cheminformatics, 12:1\u20138, 2020.   \n[41] Seul Lee, Jaehyeong Jo, and Sung Ju Hwang. Exploring chemical space with score-based out-of-distribution generation. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 18872\u201318892. PMLR, 2023.   \n[42] Renjie Liao, Yujia Li, Yang Song, Shenlong Wang, William L. Hamilton, David Duvenaud, Raquel Urtasun, and Richard S. Zemel. Efficient graph generation with graph recurrent attention networks. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d\u2019Alch\u00e9- Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 4257\u20134267, 2019.   \n[43] Haitao Lin, Yufei Huang, Meng Liu, Xuanjing Li, Shuiwang Ji, and Stan Z. Li. Diffbp: Generative diffusion of 3d molecules for target protein binding. CoRR, abs/2211.11214, 2022.   \n[44] Chengyi Liu, Wenqi Fan, Yunqing Liu, Jiatong Li, Hang Li, Hui Liu, Jiliang Tang, and Qing Li. Generative diffusion models on graphs: Methods and applications. In Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI 2023, 19th-25th August 2023, Macao, SAR, China, pages 6702\u20136711. ijcai.org, 2023.   \n[45] Gang Liu, Eric Inae, Tong Zhao, Jiaxin Xu, Tengfei Luo, and Meng Jiang. Data-centric learning from unlabeled graphs with diffusion model. Advances in neural information processing systems, 36, 2023.   \n[46] Gang Liu, Jiaxin Xu, Tengfei Luo, and Meng Jiang. Inverse molecular design with multiconditional diffusion guidance. arXiv preprint arXiv:2401.13858, 2024.   \n[47] Zhining Liu, Ruizhong Qiu, Zhichen Zeng, Hyunsik Yoo, David Zhou, Zhe Xu, Yada Zhu, Kommy Weldemariam, Jingrui He, and Hanghang Tong. Class-imbalanced graph learning without class rebalancing. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024.   \n[48] Shitong Luo, Chence Shi, Minkai Xu, and Jian Tang. Predicting molecular conformation via dynamic graph score matching. In Marc\u2019Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 19784\u201319795, 2021.   \n[49] Siwei Lyu. Interpretation and generalization of score matching. In Jeff A. Bilmes and Andrew Y. Ng, editors, UAI 2009, Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, Montreal, QC, Canada, June 18-21, 2009, pages 359\u2013366. AUAI Press, 2009.   \n[50] Kaushalya Madhawa, Katushiko Ishiguro, Kosuke Nakago, and Motoki Abe. Graphnvp: An invertible flow model for generating molecular graphs. CoRR, abs/1905.11600, 2019.   \n[51] Karolis Martinkus, Andreas Loukas, Nathana\u00ebl Perraudin, and Roger Wattenhofer. SPECTRE: spectral conditioning helps to overcome the expressivity limits of one-shot graph generators. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesv\u00e1ri, Gang Niu, and Sivan Sabato, editors, International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pages 15159\u201315179. PMLR, 2022.   \n[52] Roc\u00edo Mercado, Tobias Rastemo, Edvard Lindel\u00f6f, G\u00fcnter Klambauer, Ola Engkvist, Hongming Chen, and Esben Jannik Bjerrum. Graph networks for molecular design. Mach. Learn. Sci. Technol., 2(2):25023, 2021.   \n[53] Roc\u00edo Mercado, Tobias Rastemo, Edvard Lindel\u00f6f, G\u00fcnter Klambauer, Ola Engkvist, Hongming Chen, and Esben Jannik Bjerrum. Graph networks for molecular design. Machine Learning: Science and Technology, 2(2):025023, 2021.   \n[54] Erxue Min, Runfa Chen, Yatao Bian, Tingyang Xu, Kangfei Zhao, Wenbing Huang, Peilin Zhao, Junzhou Huang, Sophia Ananiadou, and Yu Rong. Transformer for graphs: An overview from architecture perspective. CoRR, abs/2202.08455, 2022.   \n[55] Joshua Mitton, Hans M. Senn, Klaas Wynne, and Roderick Murray-Smith. A graph VAE and graph transformer approach to generating molecular graphs. CoRR, abs/2104.04345, 2021.   \n[56] Chenhao Niu, Yang Song, Jiaming Song, Shengjia Zhao, Aditya Grover, and Stefano Ermon. Permutation invariant graph generation via score-based generative modeling. In Silvia Chiappa and Roberto Calandra, editors, The 23rd International Conference on Artificial Intelligence and Statistics, AISTATS 2020, 26-28 August 2020, Online [Palermo, Sicily, Italy], volume 108 of Proceedings of Machine Learning Research, pages 4474\u20134484. PMLR, 2020.   \n[57] Ethan Perez, Florian Strub, Harm de Vries, Vincent Dumoulin, and Aaron C. Courville. Film: Visual reasoning with a general conditioning layer. In Sheila A. McIlraith and Kilian Q. Weinberger, editors, Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, pages 3942\u20133951. AAAI Press, 2018.   \n[58] Daniil Polykovskiy, Alexander Zhebrak, Benjam\u00edn S\u00e1nchez-Lengeling, Sergey Golovanov, Oktai Tatanov, Stanislav Belyaev, Rauf Kurbanov, Aleksey Artamonov, Vladimir Aladinskiy, Mark Veselov, Artur Kadurin, Sergey I. Nikolenko, Al\u00e1n Aspuru-Guzik, and Alex Zhavoronkov. Molecular sets (MOSES): A benchmarking platform for molecular generation models. CoRR, abs/1811.12823, 2018.   \n[59] Yunzhe Qi, Yikun Ban, and Jingrui He. Graph neural bandits. In Ambuj K. Singh, Yizhou Sun, Leman Akoglu, Dimitrios Gunopulos, Xifeng Yan, Ravi Kumar, Fatma Ozcan, and Jieping Ye, editors, Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD 2023, Long Beach, CA, USA, August 6-10, 2023, pages 1920\u20131931. ACM, 2023.   \n[60] Zhuoran Qiao, Weili Nie, Arash Vahdat, Thomas F. Miller III, and Anima Anandkumar. Dynamic-backbone protein-ligand structure prediction with multiscale generative diffusion models. CoRR, abs/2209.15171, 2022.   \n[61] Ruizhong Qiu, Dingsu Wang, Lei Ying, H. Vincent Poor, Yifang Zhang, and Hanghang Tong. Reconstructing graph diffusion history from a single snapshot. In Ambuj K. Singh, Yizhou Sun, Leman Akoglu, Dimitrios Gunopulos, Xifeng Yan, Ravi Kumar, Fatma Ozcan, and Jieping Ye, editors, Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD 2023, Long Beach, CA, USA, August 6-10, 2023, pages 1978\u20131988. ACM, 2023.   \n[62] Raghunathan Ramakrishnan, Pavlo O Dral, Matthias Rupp, and O Anatole Von Lilienfeld. Quantum chemistry structures and properties of 134 kilo molecules. Scientific data, 1(1):1\u20137, 2014.   \n[63] Sidney I Resnick. Adventures in stochastic processes. Springer Science & Business Media, 1992.   \n[64] Marwin HS Segler, Thierry Kogej, Christian Tyrchan, and Mark P Waller. Generating focused molecule libraries for drug discovery with recurrent neural networks. ACS central science, 4(1):120\u2013131, 2018.   \n[65] Chence Shi, Shitong Luo, Minkai Xu, and Jian Tang. Learning gradient fields for molecular conformation generation. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 9558\u20139568. PMLR, 2021.   \n[66] Martin Simonovsky and Nikos Komodakis. Graphvae: Towards generation of small graphs using variational autoencoders. In Vera Kurkov\u00e1, Yannis Manolopoulos, Barbara Hammer, Lazaros S. Iliadis, and Ilias Maglogiannis, editors, Artificial Neural Networks and Machine Learning - ICANN 2018 - 27th International Conference on Artificial Neural Networks, Rhodes, Greece, October 4-7, 2018, Proceedings, Part I, volume 11139 of Lecture Notes in Computer Science, pages 412\u2013422. Springer, 2018.   \n[67] Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In Francis R. Bach and David M. Blei, editors, Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, volume 37 of JMLR Workshop and Conference Proceedings, pages 2256\u20132265. JMLR.org, 2015.   \n[68] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021.   \n[69] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d\u2019Alch\u00e9- Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 11895\u201311907, 2019.   \n[70] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021.   \n[71] Haoran Sun, Lijun Yu, Bo Dai, Dale Schuurmans, and Hanjun Dai. Score-based continuous-time discrete diffusion models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023.   \n[72] Cl\u00e9ment Vignac and Pascal Frossard. Top-n: Equivariant set and graph generation without exchangeability. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022.   \n[73] Cl\u00e9ment Vignac, Igor Krawczuk, Antoine Siraudin, Bohan Wang, Volkan Cevher, and Pascal Frossard. Digress: Discrete denoising diffusion for graph generation. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023.   \n[74] Dingsu Wang, Yuchen Yan, Ruizhong Qiu, Yada Zhu, Kaiyu Guan, Andrew Margenot, and Hanghang Tong. Networked time series imputation via position-aware graph enhanced variational autoencoders. In Ambuj K. Singh, Yizhou Sun, Leman Akoglu, Dimitrios Gunopulos, Xifeng Yan, Ravi Kumar, Fatma Ozcan, and Jieping Ye, editors, Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD 2023, Long Beach, CA, USA, August 6-10, 2023, pages 2256\u20132268. ACM, 2023.   \n[75] Tomer Weiss, Eduardo Mayo Yanes, Sabyasachi Chakraborty, Luca Cosmo, Alex M Bronstein, and Renana Gershoni-Poranne. Guided diffusion for inverse molecular design. Nature Computational Science, 3(10):873\u2013882, 2023.   \n[76] Minkai Xu, Alexander S Powers, Ron O Dror, Stefano Ermon, and Jure Leskovec. Geometric latent diffusion models for 3d molecule generation. In International Conference on Machine Learning, pages 38592\u201338610. PMLR, 2023.   \n[77] Minkai Xu, Lantao Yu, Yang Song, Chence Shi, Stefano Ermon, and Jian Tang. Geodiff: A geometric diffusion model for molecular conformation generation. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022.   \n[78] Zhe Xu, Yuzhong Chen, Menghai Pan, Huiyuan Chen, Mahashweta Das, Hao Yang, and Hanghang Tong. Kernel ridge regression-based graph dataset distillation. In Ambuj K. Singh, Yizhou Sun, Leman Akoglu, Dimitrios Gunopulos, Xifeng Yan, Ravi Kumar, Fatma Ozcan, and Jieping Ye, editors, Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD 2023, Long Beach, CA, USA, August 6-10, 2023, pages 2850\u20132861. ACM, 2023.   \n[79] Zhe Xu, Boxin Du, and Hanghang Tong. Graph sanitation with application to node classification. In Fr\u00e9d\u00e9rique Laforest, Rapha\u00ebl Troncy, Elena Simperl, Deepak Agarwal, Aristides Gionis, Ivan Herman, and Lionel M\u00e9dini, editors, WWW \u201922: The ACM Web Conference 2022, Virtual Event, Lyon, France, April 25 - 29, 2022, pages 1136\u20131147. ACM, 2022.   \n[80] Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao, Yingxia Shao, Wentao Zhang, Ming-Hsuan Yang, and Bin Cui. Diffusion models: A comprehensive survey of methods and applications. CoRR, abs/2209.00796, 2022.   \n[81] Jiaxuan You, Jonathan Michael Gomes Selman, Rex Ying, and Jure Leskovec. Identity-aware graph neural networks. In Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021, pages 10737\u201310745. AAAI Press, 2021.   \n[82] Jiaxuan You, Rex Ying, Xiang Ren, William L. Hamilton, and Jure Leskovec. Graphrnn: Generating realistic graphs with deep auto-regressive models. In Jennifer G. Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm\u00e4ssan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pages 5694\u20135703. PMLR, 2018.   \n[83] Zhichen Zeng, Ruizhong Qiu, Zhe Xu, Zhining Liu, Yuchen Yan, Tianxin Wei, Lei Ying, Jingrui He, and Hanghang Tong. Graph mixup on approximate gromov-wasserstein geodesics. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024.   \n[84] Lecheng Zheng, Dawei Zhou, Hanghang Tong, Jiejun Xu, Yada Zhu, and Jingrui He. Fairgen: Towards fair graph generation. In 40th IEEE International Conference on Data Engineering, ICDE 2024, Utrecht, The Netherlands, May 13-16, 2024, pages 2285\u20132297. IEEE, 2024.   \n[85] Dawei Zhou, Lecheng Zheng, Jiawei Han, and Jingrui He. A data-driven graph generative model for temporal interaction networks. In Rajesh Gupta, Yan Liu, Jiliang Tang, and B. Aditya Prakash, editors, KDD \u201920: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, CA, USA, August 23-27, 2020, pages 401\u2013411. ACM, 2020.   \n[86] Dawei Zhou, Lecheng Zheng, Jiejun Xu, and Jingrui He. Misc-gan: A multi-scale generative model for graphs. Frontiers Big Data, 2:3, 2019. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The organization of this appendix is as follows ", "page_idx": 17}, {"type": "text", "text": "\u2022 Section A: the derivation of the factorized rate matrices.   \n\u2022 Section B: the forward transition probability matrix given a rate matrix.   \n\u2022 Section C: all the proofs. \u2013 Section C.1: proof of Proposition 3.2 \u2013 Section C.2: proof of Theorem 3.3 \u2013 Section C.3: proof of Lemma 3.5 \u2013 Section C.4: proof of Lemma 3.6 \u2013 Section C.5: proof of Lemma 3.7 \u2013 Section C.6: proof of Theorem 3.8 \u2013 Section C.7: proof of Theorem 3.9   \n\u2022 Section D: a step-by-step sampling algorithm.   \n\u2022 Section E: the auxiliary features and neural modules used by DISCO.   \n\u2022 Section F: detailed experimental settings and additional experimental results. \u2013 Section F.1: hardware and software \u2013 Section F.2: dataset setup \u2013 Section F.3: hyperparameter settings \u2013 Section F.4: additional effectiveness evaluation on Community Dataset \u2013 Section F.5: additional ablation study with the MPNN backbone \u2013 Section F.6: convergence study \u2013 Section F.7: visualization   \n\u2022 Section G: this paper\u2019s limitations and future work.   \n\u2022 Section H: the broad impact of this paper. ", "page_idx": 17}, {"type": "text", "text": "A Details of the Factorization of Rate Matrices ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we detail the derivation of Remark 3.1, which is extended from the following Proposition 3 of [8]. ", "page_idx": 17}, {"type": "text", "text": "Proposition A.1 (Factorization of the rate matrix, Proposition 3 from [8]). If the forward process factorizes as $\\begin{array}{r}{q_{t|s}(\\mathbf{x}_{t}|\\mathbf{x}_{s})=\\prod_{d=1}^{D}q_{t|s}(x_{t}^{d}|x_{s}^{d}),t>s}\\end{array}$ , then the forward and reverse rates are of the form ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{R}_{t}(\\bar{\\mathbf{x}},\\mathbf{x})=\\displaystyle\\sum_{d=1}^{D}\\mathbf{R}_{t}^{d}(\\bar{x}^{d},x^{d})\\delta_{\\bar{\\mathbf{x}}\\backslash\\bar{x}^{d},\\mathbf{x}\\backslash x^{d}}}\\\\ &{\\tilde{\\mathbf{R}}_{t}(\\mathbf{x},\\bar{\\mathbf{x}})=\\displaystyle\\sum_{d=1}^{D}\\mathbf{R}_{t}^{d}(\\bar{x}^{d},x^{d})\\delta_{\\bar{\\mathbf{x}}\\backslash\\bar{x}^{d},\\mathbf{x}\\backslash x^{d}}\\sum_{x_{0}^{d}}q_{0\\mid t}(x_{0}^{d}|\\mathbf{x})\\frac{q_{t\\mid0}(\\bar{x}^{d}|x_{0}^{d})}{q_{t\\mid0}(x^{d}|x_{0}^{d})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\delta_{\\bar{\\mathbf{x}}\\setminus\\bar{x}^{d},\\mathbf{x}\\setminus x^{d}}=1$ when all dimensions except for $d$ are equal. ", "page_idx": 17}, {"type": "text", "text": "As all the nodes and edges are categorical, applying the above proposition of all the nodes and edges leads to our Remark 3.1. ", "page_idx": 17}, {"type": "text", "text": "B Details of Forward Transition Probability ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we present the derivation of the forward transition probability for nodes; the forward process for edges can be derived similarly. Note that this derivation has been mentioned in [8] for generic discrete cases; we graft it to the graph settings and include it here for completeness. The core derivation of the forward transition probability is to prove the following proposition. ", "page_idx": 17}, {"type": "text", "text": "Proposition B.1 (Analytical forward process for commutable rate matrices, Proposition 10 from [8]). $i f\\mathbf{R}_{t}$ and $\\mathbf{R}_{t^{\\prime}}$ commute $\\forall t,t^{\\prime}$ , $q_{t|0}\\bar{(x_{t}=j|x_{0}=i)}=(e^{\\int_{0}^{t}\\mathbf{R}_{s}d s})_{i j}$ ", "page_idx": 18}, {"type": "text", "text": "Proof. If $\\begin{array}{r}{q_{t|0}=\\exp\\left(\\int_{0}^{t}\\mathbf{R}_{s}d s\\right)}\\end{array}$ is the forward transition probability matrix, it should satisfy the Kolmogorov forward equation $\\begin{array}{r}{\\frac{d}{d t}q_{t|0}=q_{t|0}\\mathbf{R}_{s}}\\end{array}$ . The transition probability matrix ", "page_idx": 18}, {"type": "equation", "text": "$$\nq_{t|0}=\\sum_{k=0}^{\\infty}\\frac{1}{k!}\\bigg(\\int_{0}^{t}\\mathbf{R}_{s}d s\\bigg)^{k},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and, based on the fact that $\\mathbf{R}_{t}$ and $\\mathbf{R}_{t}^{\\prime}$ commute $\\forall t,t^{\\prime}$ , its derivative is ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{d}{d t}q_{t|0}=\\sum_{k=1}^{\\infty}\\frac{1}{(k-1)!}\\bigg(\\int_{0}^{t}\\mathbf{R}_{s}d s\\bigg)^{(k-1)}=q_{t|0}\\mathbf{R}_{t}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Thus, $\\begin{array}{r}{q_{t|0}=\\exp\\left(\\int_{0}^{t}\\mathbf{R}_{s}d s\\right)}\\end{array}$ is the solution of Kolmogorov forward equation. ", "page_idx": 18}, {"type": "text", "text": "For the node $i$ , if its forward rate matrix is set as ${\\bf R}_{t}^{i}\\,=\\,\\beta(t){\\bf R}_{f}$ , we have $\\mathbf{R}_{t}^{i}$ and $\\mathbf{R}_{t^{\\prime}}^{i}$ commute, $\\forall t,t^{\\prime}$ . Thus, the transition probability for node $i$ is $q_{t|0}(f_{t}^{i}=v|f_{0}^{i}=u)=(e^{\\int_{0}^{t}\\beta(s)\\mathbf{R}_{f}d s})_{u v}$ . Based on similar derivation, we have the transition probability for the edge $(i,j)$ as $q_{t|0}(e_{t}^{(i,j)}=v|e_{0}^{(i,j)}=$ $u)=(e^{\\int_{0}^{t}\\beta(s){\\bf R}_{e}d s})u v$ . ", "page_idx": 18}, {"type": "text", "text": "C Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "C.1 Proof of Proposition 3.2 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proposition 3.2 claims the forward process converges to uniform distributions if $\\mathbf{R}_{f}=\\mathbf{11}^{\\top}-b\\mathbf{I}$ and $\\mathbf{R}_{e}=\\mathbf{11}^{\\top}-(a+1)\\mathbf{I}$ and it converges to marginal distributions $\\mathbf{m}_{f}$ and ${\\bf m}_{e}$ if $\\mathbf{R}_{f}=\\mathbf{1}\\mathbf{m}_{f}^{\\top}-\\mathbf{I}$ and $\\mathbf{R}_{e}=\\mathbf{1}\\mathbf{m}_{e}^{\\top}-\\mathbf{I}$ . ", "page_idx": 18}, {"type": "text", "text": "Proof. If we formulate the rate matrices for nodes and edges as ${\\bf R}_{t}^{(i,j)}=\\beta(t){\\bf R}_{e},$ $\\forall i,j$ and ${\\bf R}_{t}^{i}=$ $\\beta(t)\\mathbf{R}_{f}$ , $\\forall i$ , every rate matrix is commutable for any time steps $t$ and $t^{\\prime}$ . In the following content, we show the proof for the node rate matrix $\\mathbf{R}_{t}^{i}=\\beta(t)\\mathbf{R}_{f}$ ; the converged distribution of edge can be proved similarly. Based on Proposition B.1, the transition probability matrix between time steps $t$ and $t+\\Delta t$ is ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{q_{t+\\Delta t|t}={\\bf I}+\\int_{t}^{t+\\Delta t}\\beta(s){\\bf R}_{f}d s+O((\\Delta t)^{2})}}\\\\ {{\\mathrm{{\\scriptsize~\\stackrel{(*)}{=}~}}{\\bf I}+\\Delta t\\beta(\\xi){\\bf R}_{f}+O((\\Delta t)^{2}),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $(\\ast)$ is based on the Mean Value Theorem. If the high-order term $O((\\Delta t)^{2})$ is omitted and we short $\\beta_{\\Delta t}=\\Delta t\\beta(\\xi)$ , for $\\mathbf{R}_{f}=\\mathbf{11}^{\\top}-b\\mathbf{I}$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{q_{t+\\Delta t|t}\\approx\\beta_{\\Delta t}\\mathbf{1}\\mathbf{1}^{\\top}+(1-\\beta_{\\Delta t}b)\\mathbf{I},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which is the transition matrix of the uniform diffusion in the discrete-time diffusion models [67, 2]. Thus, with $T\\rightarrow\\infty$ and $q_{t+\\Delta t}|t$ to the power of infinite, the converged distribution is a uniform distribution. Similarly, for $\\mathbf{R}_{f}=\\mathbf{1}\\mathbf{m}_{f}^{\\top}-\\mathbf{I}$ the transition matrix is ", "page_idx": 18}, {"type": "equation", "text": "$$\nq_{t+\\Delta t|t}\\approx\\beta_{\\Delta t}\\mathbf{1}\\mathbf{m}_{f}^{\\top}+(1-\\beta_{\\Delta t})\\mathbf{I}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which is a generalized transition matrix of the \u2018absorbing state\u2019 diffusion [2]. The difference lies at for the \u2018absorbing state\u2019 diffusion [2], $\\mathbf{m}_{f}$ is set as a one-hot vector for the absorbing state, and here we set it as the marginal distribution. Thus, with $T\\to\\infty$ and $q_{t+\\Delta t}|t$ to the power of infinite, the converged distribution is a marginal distribution $\\mathbf{m}_{f}$ . \u53e3 ", "page_idx": 18}, {"type": "text", "text": "C.2 Proof of Theorem 3.3 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Theorem 3.3 says for $\\mathcal G\\neq\\bar{\\mathcal G}$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Big|\\tilde{\\mathbf{R}}_{t}(\\mathcal{G},\\bar{\\mathcal{G}})-\\tilde{\\mathbf{R}}_{\\theta,t}(\\mathcal{G},\\bar{\\mathcal{G}})\\Big|^{2}\\leq C_{t}+C_{t}^{\\mathrm{node}}\\mathbb{E}_{\\mathcal{G}_{0}}q_{t|0}(\\mathcal{G}|\\mathcal{G}_{0})\\sum_{i}\\mathcal{L}_{\\mathrm{cE}}(\\mathrm{0ne-Hot}(f_{0}^{i}),\\hat{f}_{0}^{i})}\\\\ {+\\;C_{t}^{\\mathrm{edge}}\\mathbb{E}_{\\mathcal{G}_{0}}q_{t|0}(\\mathcal{G}|\\mathcal{G}_{0})\\sum_{i,j}\\mathcal{L}_{\\mathrm{cE}}(\\mathrm{0ne-Hot}(e_{0}^{(i,j)}),\\hat{e}_{0}^{(i,j)})\\;)}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the node and edge estimated probability vector (sum is 1) is notated as ${\\hat{f}}_{0}^{i}\\,=\\,[p_{0|t}^{\\theta}(f^{i}\\,=$ $1|{\\mathcal{G}}_{t}),\\ldots,p_{0|t}^{\\theta}(f^{i}\\;=\\;b|{\\mathcal{G}}_{t})]^{\\top}\\;\\in\\;[0,1]^{b}$ and $\\hat{e}_{0}^{(i,j)}\\;=\\;[p_{0|t}^{\\theta}(e^{(i,j)}\\;=\\;1|{\\mathcal G}_{t}),\\..\\.,p_{0|t}^{\\theta}(e^{(i,j)}\\;=\\;a\\;+\\;$ $1|\\mathcal{G}_{t})]^{\\top}\\in[0,1]^{a+1}$ . ", "page_idx": 19}, {"type": "text", "text": "Proof. ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\Big|\\widehat{\\mathbf{R}}_{t}(\\mathcal{G},\\bar{G})-\\bar{\\mathbf{R}}_{0,t}(\\mathcal{G},\\bar{G})\\Big|}\\\\ &{=\\Big|\\displaystyle\\sum_{i}A_{t}^{i}\\sum_{j_{0}}\\frac{q_{\\mathrm{t}}(|\\bar{\\mathcal{G}}|)_{i}}{q_{\\mathrm{t}}(|\\mathcal{G}|)_{i}}(q_{0}|\\mathcal{G}|)-p_{0|t}^{\\theta}(f_{0}|\\mathcal{G}))}\\\\ &{\\quad+\\displaystyle\\sum_{i,j}B_{t}^{i(i,j)}\\sum_{\\epsilon_{0}(i)}\\frac{q_{\\mathrm{t}}(|\\bar{\\mathcal{G}}|)_{i}(e^{\\mathrm{i}(\\bar{\\delta},j)})}{q_{\\mathrm{t}}(|\\mathcal{G}|)_{i}(e^{\\mathrm{i}(\\bar{\\delta},j)})}(q_{0}|e^{\\mathrm{i}(\\bar{\\delta},j)}|\\mathcal{G})-p_{0|t}^{\\theta}(e_{0}^{\\mathrm{i}(\\bar{\\delta},j)}|\\mathcal{G}))\\Big|}\\\\ &{\\le\\displaystyle\\Big|\\sum_{i}A_{t}^{i}\\sum_{j_{0}}\\frac{q_{\\mathrm{t}}(|\\bar{\\mathcal{G}}|)_{i}(e_{j})}{q_{\\mathrm{t}}(|\\mathcal{G}|)_{i}(|\\mathcal{G}|)_{i}()}(q_{0}|\\mathcal{G}|)-p_{0|t}^{\\theta}(f_{0}|\\mathcal{G}))\\Big|}\\\\ &{\\quad+\\displaystyle\\Big|\\sum_{i,j}B_{t}^{i(i,j)}\\sum_{\\epsilon_{0}(i)}\\frac{q_{\\mathrm{t}}(|\\bar{\\mathcal{G}}^{(i,j)}|)}{q_{\\mathrm{t}}(|\\alpha^{(\\epsilon,i)}|)\\,|\\mathcal{G}|)_{i}(|\\mathcal{G}|)}(q_{0|t}(e_{0}^{(i,j)}|\\mathcal{G})-p_{0|t}^{\\theta}(e_{0}^{(i,j)}|\\mathcal{G}))\\Big|}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We check the first term of Eq. (20): ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Big|\\displaystyle\\sum_{i}A_{j}\\sum_{k=1}^{q}\\frac{q_{1}q_{2}(|I_{j}|\\bar{f}_{k})}{f_{k}}\\cos(f_{k}(\\bar{f})(\\mathcal{S})-p_{01}(f_{k}|\\bar{g}))\\Big|}\\\\ &{\\le\\displaystyle\\sum_{i}A_{j}\\sum_{k=1}^{q}\\frac{q_{1}q_{1}(|I_{j}|\\bar{f}_{k})}{f_{k}}\\sum_{l=1}^{q}\\Big|\\partial_{\\theta_{l}}(f_{l}|\\bar{g})-p_{01}^{\\theta_{l}}(f_{k}|\\bar{g})}\\\\ &{=\\displaystyle\\sum_{i}C_{j_{1}}\\sum_{l=1}^{q}\\Big|\\partial_{\\theta_{l}}(f_{l}|\\bar{g})-p_{02}^{\\theta_{l}}(f_{l}|\\bar{g})\\Big|}\\\\ &{\\overset{(i)}{\\le}\\displaystyle\\sum_{i}C_{j_{1}}\\sum_{k=1}^{q}\\Big(C_{j_{1}}-q_{01}(f_{k}|\\bar{g})\\log p_{01}^{\\theta_{l}}(f_{l}|\\bar{g})\\Big)}\\\\ &{\\overset{(i)\\le}C_{1}\\sqrt{\\displaystyle\\sum_{i}\\sum_{j_{1}=1}^{i}\\left(C_{j_{1}}-q_{01}(f_{k}|\\bar{g})\\log p_{01}^{\\theta_{l}}(f_{l}|\\bar{g})\\right)}}\\\\ &{=C_{1}\\sqrt{C_{2}-\\displaystyle\\sum_{i}\\sum_{j_{1}=i}^{i}q_{1}(f_{k}|\\bar{g})\\log p_{01}^{\\theta_{l}}(f_{l}|\\bar{g})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where Ci = Ait supf 0i qtt||00(f i|f 00i) , $C_{f_{0}^{i}}=q_{0|t}(f_{0}^{i}|\\mathcal{G})\\log q_{0|t}(f_{0}^{i}|\\mathcal{G}).$ , $(^{*})$ is based on the Pinsker\u2019s i\u221anequality, $({**})$ is based on Cauchy\u2013Schwarz inequality: $\\begin{array}{r}{\\sum_{i=1}^{n}\\sqrt{x_{i}}~\\le~\\sqrt{n\\sum_{i=1}^{n}x_{i}}}\\end{array}$ , ${\\cal C}_{1}~=$ $\\sqrt{2n}\\operatorname*{sup}_{i}\\{C_{i}\\}$ , $\\begin{array}{r}{C_{2}=\\sum_{i}\\sum_{f_{0}^{i}}C_{f_{0}^{i}}}\\end{array}$ . Next, the term $\\begin{array}{r}{-\\sum_{i}\\sum_{f_{0}^{i}}q_{0|t}(f_{0}^{i}|\\mathcal{G})\\log p_{0|t}^{\\theta}(f_{0}^{i}|\\mathcal{G})}\\end{array}$ is equiva", "page_idx": 19}, {"type": "text", "text": "lent to: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad-\\displaystyle\\sum_{i\\neq j}\\sum_{q_{i}}q_{\\alpha\\alpha}((f_{i}|\\mathcal{O})|\\log P_{0}\\{f_{i}|\\mathcal{O}\\})}\\\\ &{=\\displaystyle-\\frac{1}{P_{0}(\\mathcal{O})}\\sum_{i\\neq j}\\sum_{q_{i},\\ldots,q_{i}}(f_{i},Q_{i}^{\\alpha},Q)\\log P_{0}(f_{i}|\\mathcal{O})}\\\\ &{=-\\displaystyle\\frac{1}{P_{0}(\\mathcal{O})}\\sum_{i\\neq j}\\sum_{q_{i},\\ldots,q_{i}}\\sum_{\\alpha_{k},\\alpha_{j},\\ldots,(Q_{i})}\\log P_{0}\\{f_{i}|\\mathcal{O}\\}}\\\\ &{=-\\displaystyle\\frac{1}{P_{0}(\\mathcal{O})}\\sum_{i\\neq j}\\sum_{q_{i},\\ldots,q_{i}}\\sum_{\\alpha_{k}\\in\\{0,\\atop(0,i),\\ldots,q_{k}\\}}\\pi_{\\alpha\\alpha}(\\mathcal{O})_{q_{i}|\\mathcal{O}|}(\\mathcal{O})_{\\alpha}\\log P_{0}(f_{i}|\\mathcal{O})}\\\\ &{=-\\displaystyle\\frac{1}{P_{0}(\\mathcal{O})}\\sum_{i\\neq j}\\sum_{q_{i},\\ldots,q_{i}/2\\}\\sum_{q_{k}\\in\\{0,\\atop(0,i),\\ldots,q_{k}\\}}\\pi_{\\alpha\\alpha}(\\mathcal{O})_{q_{i}|\\mathcal{O}|}(\\mathcal{O})_{\\alpha}\\log P_{0}(f_{i}|\\mathcal{O})}\\\\ &{=\\displaystyle\\frac{1}{P_{0}(\\mathcal{O})}\\sum_{i\\neq j}\\sum_{q_{i},\\ldots,q_{i}/2\\}\\sum_{q_{k},\\ldots,q_{k}}\\;\\pi_{\\alpha\\alpha_{i}}(\\mathcal{O})_{q_{i}|\\mathcal{O}|}(\\mathcal{O})_{i}\\mathcal{O}()_{\\alpha}\\sum_{i=0}(\\mathrm{mode}(f_{i}),\\hat{f}_{i})}\\\\ &{=\\displaystyle\\frac{1}{P_{0}(\\mathcal{O})}\\sum_{i\\in\\mathcal{I}_{k}}\\sum_{q_{i},\\ldots,q_{k}}(\\mathcal{O})_{i q_{i}|\\mathcal{O}|}(\\mathcal{O})_{i}\\sum_{i=1}^{Z}C_{\\alpha\\alpha}(\\mathrm{mode}(f_{i}),\\hat{f}_{i}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\sum_{\\mathcal{G}_{0}(f_{0}^{i})}$ marginalizing all the graphs at time 0 whose $i$ -th node is $f_{0}^{i};p_{0,t}(f_{0}^{i},\\mathcal{G})$ is the joint probability of a graph whose $i$ -th node is $f_{0}^{i}$ at time 0 and it is $\\mathcal{G}$ at time $t$ ; $p_{0,t}(\\mathcal{G}_{0},\\mathcal{G})$ is the joint probability of a graph which is $\\mathcal{G}_{0}$ at time 0 and it is $\\mathcal{G}$ at time $t$ . Plugging Eq. (33) into Eq. (26): ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~~\\Big|\\sum_{i}A_{t}^{i}\\sum_{f_{0}^{i}}\\frac{q_{t|0}(\\bar{f}^{i}|f_{0}^{i})}{q_{t|0}(f^{i}|f_{0}^{i})}(q_{0|t}(f_{0}^{i}|\\mathcal{G})-p_{0|t}^{\\theta}(f_{0}^{i}|\\mathcal{G}))\\Big|}\\\\ &{{\\le}C_{1}\\sqrt{C_{2}+C_{5}\\mathbb{E}_{\\mathcal{G}_{0}}q_{t|0}(\\mathcal{G}|\\mathcal{G}_{0})\\sum_{i}{\\mathcal{L}}_{\\mathtt{C E}}(\\mathtt{O n e-H o t}(f_{0}^{i}),\\hat{f}_{0}^{i})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "dwirheecrtel $\\begin{array}{r}{C_{5}=\\frac{1}{p_{t}(\\mathcal{G})}}\\end{array}$ .h eAre :similar analysis can be conducted about the second term of Eq. (20) and we ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Big|\\displaystyle\\sum_{i,j}B_{t}^{(i,j)}\\sum_{e_{0}^{(i,j)}}\\frac{q_{t|0}(\\bar{e}^{(i,j)}|e_{0}^{(i,j)})}{q_{t|0}(e^{(i,j)}|e_{0}^{(i,j)})}(q_{0|t}(e_{0}^{(i,j)}|\\mathcal{G})-p_{0|t}^{\\theta}(e_{0}^{(i,j)}|\\mathcal{G}))\\Big|}\\\\ &{\\le\\!C_{3}\\sqrt{C_{4}+C_{5}\\mathbb{E}_{\\mathcal{G}_{0}}q_{t|0}(\\mathcal{G}|\\mathcal{G}_{0})\\displaystyle\\sum_{i,j}\\mathcal{L}_{\\mathtt{c x}}(\\mathtt{m e}\\cdot\\mathtt{H o r}(e_{0}^{(i,j)}),\\hat{e}_{0}^{(i,j)})}}&{\\qquad\\qquad\\qquad}\\\\ &{\\qquad=\\qquad\\sqrt{2}n\\operatorname*{sup}_{i,j}\\{C_{i,j}\\},\\quad C_{4}\\qquad=\\qquad\\sum_{i,j}\\sum_{e_{0}^{(i,j)}}C_{e_{0}^{(i,j)}},\\quad C_{i,j}\\qquad=}\\\\ &{\\overset{,,}{\\mathcal{I}_{\\mathtt{f}_{0}|0}(\\epsilon^{(i,j)}|e_{0}^{(i,j)})}\\Big\\},C_{e_{0}^{(i,j)}}=q_{0|t}(e_{0}^{(i,j)}|\\mathcal{G})\\log q_{0|t}(e_{0}^{(i,j)}|\\mathcal{G}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Plugging Eqs. (34) and (35) into Eq. (20), being aware that $C_{1},C_{2},C_{3},C_{4},C_{5}$ are all $t$ -related: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Big|\\tilde{\\mathbf{R}}_{t}(\\mathcal{G},\\bar{\\mathcal{G}})-\\tilde{\\mathbf{R}}_{\\theta,t}(\\mathcal{G},\\bar{\\mathcal{G}})\\Big|\\leq C_{1}\\sqrt{C_{2}+C_{5}\\mathbb{E}_{\\mathcal{G}}q_{t\\mid0}(\\mathcal{G}|\\mathcal{G}_{0})\\sum_{i}\\mathcal{L}_{\\mathrm{cx}}(0\\mathrm{ne}\\!-\\!\\mathrm{Ho}\\mathrm{e}\\!+\\!\\mathrm{Ho}\\mathrm{e}(f_{0}^{i}),\\hat{f}_{0}^{i})}}\\\\ &{\\phantom{\\quad\\quad\\quad}+C_{3}\\sqrt{C_{4}+C_{5}\\mathbb{E}_{\\mathcal{G}}q_{t\\mid0}(\\mathcal{G}|\\mathcal{G}_{0})\\sum_{i,j}\\mathcal{L}_{\\mathrm{cx}}(0\\mathrm{ne}\\!-\\!\\mathrm{Ho}\\mathrm{e}(e_{0}^{(i,j)}),\\hat{e}_{0}^{(i,j)})}}\\\\ &{\\phantom{\\quad\\quad\\quad}\\overset{(*)}{\\leq}\\Big(C_{t}+C_{t}^{\\mathrm{node}}\\mathbb{E}_{\\mathcal{G}_{0}}q_{t\\mid0}(\\mathcal{G}|\\mathcal{G}_{0})\\sum_{i}\\mathcal{L}_{\\mathrm{cx}}(0\\mathrm{ne}\\!-\\!\\mathrm{Ho}\\mathrm{e}(f_{0}^{i}),\\hat{f}_{0}^{i})}\\\\ &{\\phantom{\\quad\\quad\\quad}+C_{t}^{\\mathrm{node}}\\mathbb{E}_{\\mathcal{G}_{0}}q_{t\\mid0}(\\mathcal{G}|\\mathcal{G}_{0})\\sum_{i,j}\\mathcal{L}_{\\mathrm{cx}}(0\\mathrm{ne}\\!-\\!\\mathrm{Ho}\\mathrm{e}(e_{0}^{(i,j)}),\\hat{e}_{0}^{(i,j)})\\Big)^{1/2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $({*})$ is based on Cauchy\u2013Schwarz inequality, $C_{t}\\ =\\ 2C_{1}^{2}C_{2}\\,+\\,2C_{3}^{2}C_{4}$ , $C_{t}^{\\mathrm{node}}\\;=\\;2C_{1}^{2}C_{5}$ , $C_{t}^{\\mathsf{e d g e}}=2C_{3}^{2}C_{5}$ . \u53e3 ", "page_idx": 21}, {"type": "text", "text": "C.3 Proof of Lemma 3.5 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We clarify that the term \"permutation\" in this paper refers to the reordering of the node indices, i.e., the first dimension of $\\mathbf{F}$ and the first two dimensions of $\\mathbf{E}$ . ", "page_idx": 21}, {"type": "text", "text": "Proof. The input of an MPNN layer is $\\mathbf{F}=\\{\\mathbf{r}_{i}\\}_{i=1}^{n}\\in\\mathbb{R}^{n\\times d},\\mathbf{E}=\\{\\mathbf{r}_{i,j}\\}_{i,j=1}^{n}\\in\\mathbb{R}^{n\\times n\\times d},\\mathbf{y}\\in\\mathbb{R}^{d},$ where $d$ is the hidden dimension. The updating formulas of an MPNN layer can be presented as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbf{r}^{i}\\leftarrow\\mathsf{F i L M}\\Bigg(\\mathsf{F i L M}\\Bigg(\\mathbf{r}^{i},\\mathsf{M L P}\\Bigg(\\frac{\\sum_{j=1}^{n}\\mathbf{r}^{(j,i)}}{n}\\Bigg)\\Bigg),\\mathbf{y}\\Bigg),}\\\\ &{\\mathbf{r}^{(i,j)}\\leftarrow\\mathsf{F i L M}\\big(\\mathsf{F i L M}\\big(\\mathbf{r}^{(i,j)},\\mathbf{r}^{i}\\odot\\mathbf{r}^{j}\\big),\\mathbf{y}\\big),}\\\\ &{\\quad\\mathbf{y}\\leftarrow\\mathbf{y}+\\mathsf{P N A}\\big(\\{\\mathbf{r}^{i}\\}_{i=1}^{n}\\big)+\\mathsf{P N A}\\big(\\{\\mathbf{r}^{(i,j)}\\}_{i,j=1}^{n}\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The permutation $\\mathcal{P}$ of the input of an MPNN layer can be presented as $\\mathcal{P}(\\mathbf{F}\\,=\\,\\{\\mathbf{r}_{i}\\}_{i=1}^{n},\\mathbf{E}\\,=$ $\\{\\mathbf{r}_{i,j}\\}_{i,j=1}^{n},\\mathbf{y}\\}=\\left(\\{\\mathbf{r}_{\\sigma(i)}\\}_{i=1}^{n},\\{\\mathbf{r}_{\\sigma(i),\\sigma(j)}\\}_{i,j=1}^{n},\\mathbf{y}\\right)$ where $\\sigma:\\{1,\\ldots,n\\}\\mapsto\\{1,\\ldots,n\\}$ is a bijection. ", "page_idx": 21}, {"type": "text", "text": "For PNA (Eq. (70)), it includes operations max, min, mean, and std which are all permutation-invariant and thus, the PNA module is permutation-invariant. Then, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbf{y}+\\mathsf{P N A}\\big(\\{\\mathbf{r}^{i}\\}_{i=1}^{n}\\big)+\\mathsf{P N A}\\big(\\{\\mathbf{r}^{(i,j)}\\}_{i,j=1}^{n}\\big)=\\mathbf{y}+\\mathsf{P N A}\\big(\\{\\mathbf{r}^{\\sigma(i)}\\}_{i=1}^{n}\\big)+\\mathsf{P N A}\\big(\\{\\mathbf{r}^{(\\sigma(i),\\sigma(j))}\\}_{i,j=1}^{n}\\big)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Because $\\begin{array}{r}{\\sum_{j=1}^{n}\\mathbf{r}^{(j,i)}=\\sum_{j=1}^{n}\\mathbf{r}^{(\\sigma(j),\\sigma(i))},\\mathbf{r}^{i}\\odot\\mathbf{r}^{j}=\\mathbf{r}^{\\sigma(i)}\\odot\\mathbf{r}^{\\sigma(j)}}\\end{array}$ , and the FiLM module (Eq. (71)) is not rel ated to the nod e ordering, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbf{r}^{(\\sigma(i),\\sigma(j))}\\gets\\mathbf{F}\\mathtt{i L M}\\big(\\mathbf{F}\\mathtt{i L M}\\big(\\mathbf{r}^{(\\sigma(i),\\sigma(j))},\\mathbf{r}^{\\sigma(i)}\\odot\\mathbf{r}^{\\sigma(j)}\\big),\\mathbf{y}\\big)=\\mathbf{F}\\mathtt{i L M}\\big(\\mathbf{F}\\mathtt{i L M}\\big(\\mathbf{r}^{(i,j)},\\mathbf{r}^{i}\\odot\\mathbf{r}^{j}\\big),\\mathbf{y}\\big)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{r}^{\\sigma(i)}\\leftarrow\\mathbf{F}\\mathrm{i}\\mathrm{LM}\\Bigg(\\mathbf{F}\\mathrm{i}\\mathrm{LM}\\Bigg(\\mathbf{r}^{\\sigma(i)},\\mathtt{M L P}\\Bigg(\\frac{\\sum_{j=1}^{n}\\mathbf{r}^{(\\sigma(j),\\sigma(i))}}{n}\\Bigg)\\Bigg),\\mathbf{y}\\Bigg)}\\\\ &{\\ \\ \\ \\ \\ =\\mathbf{F}\\mathrm{i}\\mathrm{LM}\\Bigg(\\mathbf{F}\\mathrm{i}\\mathrm{LM}\\Bigg(\\mathbf{r}^{i},\\mathtt{M L P}\\Bigg(\\frac{\\sum_{j=1}^{n}\\mathbf{r}^{(j,i)}}{n}\\Bigg)\\Bigg),\\mathbf{y}\\Bigg)}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Thus, we proved that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname{MPNN}\\!\\left(\\mathcal{P}\\big(\\mathbf{F},\\mathbf{E},\\mathbf{y}\\big)\\right)=\\mathcal{P}\\bigg(\\!\\operatorname{MPNN}(\\mathbf{F},\\mathbf{E},\\mathbf{y})\\bigg)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "C.4 Proof of Lemma 3.6 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Proof. The forward rate matrix (Eq. (3)) is the sum of component-specific forward rate matrices $(\\{\\mathbf{R}_{t}^{\\left(i,j\\right)}\\}_{i,j\\in\\mathbb{N}_{\\leq n}^{+}}$ and $\\{\\mathbf{R}_{t}^{i}\\}_{i\\in\\mathbb{N}_{\\leq n}^{+}})$ . It is permutation-invariant because the summation is permutationinvariant. ", "page_idx": 21}, {"type": "text", "text": "The parametric reverse rate matrix is ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\tilde{\\mathbf{R}}_{\\theta,t}(\\mathcal{G},\\bar{\\mathcal{G}})=\\sum_{i}\\tilde{\\mathbf{R}}_{\\theta,t}^{i}(f^{i},\\bar{f}^{i})+\\sum_{i,j}\\tilde{\\mathbf{R}}_{\\theta,t}^{(i,j)}(e^{(i,j)},\\bar{e}^{(i,j)})\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{r l r}&{\\mathrm{where}}&{\\tilde{\\mathbf{R}}_{\\theta,t}^{i}(f^{i},\\bar{f}^{i})}&{=}&{A_{t}^{i}\\sum_{f_{0}^{i}}\\frac{q_{t\\mid0}(\\bar{f}^{i}|f_{0}^{i})}{q_{t\\mid0}(f^{i}|f_{0}^{i})}p_{0\\mid t}^{\\theta}(f_{0}^{i}|\\mathcal{G}_{t}),\\mathrm{~}}&{\\tilde{\\mathbf{R}}_{\\theta,t}^{(i,j)}(e^{(i,j)},\\bar{e}^{(i,j)})<\\theta,}\\\\ &{B_{t}^{(i,j)}\\sum_{e_{0}^{(i,j)}}\\frac{q_{t\\mid0}(\\bar{e}^{(i,j)}|e_{0}^{(i,j)})}{q_{t\\mid0}(e^{(i,j)}|e_{0}^{(i,j)})}p_{0\\mid t}^{\\theta}(e_{0}^{(i,j)}|\\mathcal{G}_{t}).}&{\\mathrm{~If~}\\mathrm{we~}\\mathrm{present~the~}\\mathrm{permutation~}\\mathcal{P}\\mathrm{~on~}\\mathrm{~ever}}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "as a bijection $\\sigma:\\{1,\\ldots,n\\}\\mapsto\\{1,\\ldots,n\\}$ , the term ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{\\mathbf{R}}_{\\theta,t}^{i}(f^{i},\\tilde{f}^{i})=\\mathcal{A}_{i}^{i}\\sum_{\\tilde{f}_{i}}\\frac{q(t_{1}(\\tilde{f}^{i}|f_{i}))}{q(t_{1}(\\tilde{f}^{i}|f_{i}))}p_{0}^{\\theta}_{0}(f_{i}^{i}|f_{i})}\\\\ &{=\\mathbf{R}_{i}^{i}(\\tilde{f}^{i},f^{i})\\delta_{\\tilde{\\mathcal{O}}\\setminus f,\\theta^{\\prime}\\setminus f}\\sum_{\\tilde{f}_{i}}\\frac{q_{\\tilde{f}_{1}(\\tilde{f}_{i})}(\\tilde{f}^{i}|f_{i})}{q(t_{1}(\\tilde{f}^{i}|f_{i}))}p_{0}^{\\theta}_{0}(f_{i}^{i}|\\mathcal{Q}_{i})}\\\\ &{\\overset{(*)}{=}{\\mathbf{R}}_{t}^{i}(\\tilde{f}^{i})(\\tilde{f}^{\\sigma(i)},f^{\\sigma(i)})\\delta_{\\mathcal{P}(\\tilde{g})\\setminus f^{\\sigma(i)},\\mathcal{P}(\\mathcal{O})\\setminus f^{\\sigma(i)}}\\sum_{\\tilde{f}_{0}^{\\sigma(i)}}^{{\\sigma(i)}}q_{\\tilde{\\mu}_{1}(0}^{\\tilde{f}^{\\sigma(i)}|f_{i}^{\\sigma(i)})}p_{0}^{\\theta}_{0}(f_{i}^{i}|\\mathcal{Q}_{i})}\\\\ &{\\overset{(*)}{=}{\\mathbf{R}}_{t}^{\\sigma(i)}(\\tilde{f}^{\\sigma(i)},f^{\\sigma(i)})\\delta_{\\mathcal{P}(\\mathcal{O})\\setminus f^{\\sigma(i)},\\mathcal{P}(\\mathcal{O})\\setminus f^{\\sigma(i)}}\\sum_{f_{0}^{\\sigma(i)}}^{{\\sigma(i)}}q_{\\tilde{\\mu}_{1}(0}^{\\mathcal{P}^{(i)}(i f_{i}))}\\frac{p_{0}^{\\theta}_{0}(f_{i}^{\\sigma(i)})}{p_{0}^{\\theta}_{1}(f_{i})}\\rho_{0}^{\\theta}_{0}(f_{0}^{i}|\\mathcal{P}(\\mathcal{O}))}\\\\ &{=\\tilde{\\mathbf{R}}_{\\theta,t}^{\\sigma(i)}(f\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $({*})$ is based on the permutation invariant of the forward process and its rate matrix; $({**})$ is based on the permutation equivariance of the graph-to-graph backbone p\u03b80|t. \u53e3 ", "page_idx": 22}, {"type": "text", "text": "C.5 Proof of Lemma 3.7 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Recall the Kolmogorov forward equation, for $s<t$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{d}{d t}q_{t|s}(\\mathbf{x}_{t}|\\mathbf{x}_{s})=\\sum_{\\xi\\in\\mathcal{X}}q_{t|s}(\\xi|\\mathbf{x}_{s})\\mathbf{R}_{t}(\\xi,\\mathbf{x}_{t}).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. We aim to show that $q_{t|s}(\\mathcal{P}(\\mathbf{x}_{t})|\\mathcal{P}(\\mathbf{x}_{s}))$ is a solution of Eq. (52). Because the permutation $\\mathcal{P}$ is a bijection, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{d}{d t}q_{t|s}(\\mathcal{P}(\\mathbf{x}_{t})|\\mathcal{P}(\\mathbf{x}_{s}))}\\\\ &{=\\displaystyle\\sum_{\\xi\\in\\mathcal{X}}q_{t|s}(\\mathcal{P}(\\xi)|\\mathcal{P}(\\mathbf{x}_{s}))\\mathbf{R}_{t}(\\mathcal{P}(\\xi),\\mathcal{P}(\\mathbf{x}_{t}))}\\\\ &{\\displaystyle\\overset{(*)}{=}\\sum_{\\xi\\in\\mathcal{X}}q_{t|s}(\\mathcal{P}(\\xi)|\\mathcal{P}(\\mathbf{x}_{s}))\\mathbf{R}_{t}(\\xi,\\mathbf{x}_{t})}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $(^{*})$ is because $\\mathbf{R}_{t}$ is permutation-invariant. As Eq. 55 and Eq. 52 share the same rate matrix, and the rate matrix completely determines the CTMC (and its Kolmogorov forward equation) [63], thus, their solutions are the same: $q_{t|s}(\\mathbf{x}_{t}|\\mathbf{x}_{s})=q_{t|s}(\\mathcal{P}(\\mathbf{x}_{t})|\\mathcal{P}(\\mathbf{x}_{s}))$ , i.e., the transition probability is permutation-invariant. \u53e3 ", "page_idx": 22}, {"type": "text", "text": "C.6 Proof of Theorem 3.8 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Proof. We start from a simple case where the parametric rate matrix is fixed all the time, ", "page_idx": 22}, {"type": "equation", "text": "$$\np_{0}^{\\theta}(\\mathcal{G}_{0})=\\sum_{\\mathcal{G}_{T}}q_{0|T}^{\\theta}(\\mathcal{G}_{0}|\\mathcal{G}_{T})\\pi_{\\mathbf{ref}}(\\mathcal{G}_{T}),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the transition probability is by solving the Kolmogorov forward equation ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{d}{d t}q_{t|s}^{\\theta}(\\mathcal{G}_{t}|\\mathcal{G}_{s})=\\sum_{\\xi}q_{t|s}^{\\theta}(\\xi|\\mathcal{G}_{s})\\tilde{\\mathbf{R}}_{\\theta}(\\xi,\\mathcal{G}_{t}).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Thus, the sampling probability of permuted graph $\\mathcal{P}(\\mathcal{G}_{0})$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p_{0}^{\\theta}(\\mathcal{P}(\\mathcal{G}_{0}))=\\displaystyle\\sum_{\\mathcal{G}_{T}}q_{0|T}^{\\theta}(\\mathcal{P}(\\mathcal{G}_{0})|\\mathcal{P}(\\mathcal{G}_{T}))\\pi_{\\mathtt{r e f}}(\\mathcal{P}(\\mathcal{G}_{T}))}\\\\ &{\\qquad\\qquad\\overset{(*)}{=}\\displaystyle\\sum_{\\mathcal{G}_{T}}q_{0|T}^{\\theta}(\\mathcal{G}_{0}|\\mathcal{G}_{T})\\pi_{\\mathtt{r e f}}(\\mathcal{P}(\\mathcal{G}_{T}))}\\\\ &{\\qquad\\overset{(**)}{=}\\displaystyle\\sum_{\\mathcal{G}_{T}}q_{0|T}^{\\theta}(\\mathcal{G}_{0}|\\mathcal{G}_{T})\\pi_{\\mathtt{r e f}}(\\mathcal{G}_{T})}\\\\ &{\\qquad=p_{0}^{\\theta}(\\mathcal{G}_{0})}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $({^*})$ is based on Lemma 3.6 and Lemma 3.7, the transition probability of DISCO is permutationinvariant and $(^{**})$ is from the assumption that the reference distribution $\\pi_{\\mathtt{r e f}}(\\mathcal{G}_{T})$ is permutationinvariant. Thus, we proved that for the simple case, $\\tilde{\\mathbf{R}}_{\\theta,t}$ fixed $\\forall t$ , the sampling probability is permutation-invariant. ", "page_idx": 23}, {"type": "text", "text": "For the practical sampling, as we mentioned in Section 3.4, the $\\tau$ -leaping algorithm assumes that the time interval $[0,T]$ is divided into various length- $\\tau$ intervals $[0,\\tau),[\\tau,2\\tau),...\\,,[T-\\tau,T]$ (here both close sets or open sets work) and assume the reverse rate matrix is fixed as $\\tilde{\\mathbf{R}}_{\\theta,t}$ within every length- $\\tau$ interval, such as $(t-\\tau,t]$ . Thus, the sampling probability can be computed as ", "page_idx": 23}, {"type": "equation", "text": "$$\np_{0}^{\\theta}(\\mathcal{G}_{0})=\\sum_{\\mathcal{G}_{T},\\mathcal{G}_{T-\\tau},\\ldots,\\mathcal{G}_{\\tau}}q_{0|\\tau}(\\mathcal{G}_{0}|\\mathcal{G}_{\\tau})\\cdot\\cdot\\cdot q_{T-\\tau|T}(\\mathcal{G}_{T-\\tau}|\\mathcal{G}_{T})\\pi_{\\mathbf{r}\\in\\mathbb{F}}(\\mathcal{G}_{T}).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The conclusion from the simple case can be generalized to this $\\tau$ -leaping-based case because all the transition probability $q_{t-\\tau|t}(\\bar{\\mathcal{G}}_{t-\\tau}|\\mathcal{G}_{t})$ and the reference distribution are permutation-invariant. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "Note that $\\mathrm{Xu}$ et al. [77] have a similar analysis in their Proposition 1 on a DDPM-based model. ", "page_idx": 23}, {"type": "text", "text": "C.7 Proof of Theorem 3.9 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Recall our training objective is ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta}T\\mathbb{E}_{t\\sim\\mathcal{U}_{(0,T)}}\\mathbb{E}_{\\mathcal{G}_{0}}\\mathbb{E}_{q_{t\\mid0}(\\mathcal{G}_{t}|\\mathcal{G}_{0})}\\left[\\sum_{i}\\mathcal{L}_{\\mathtt{C E}}(0\\mathrm{ne}\\!-\\!\\mathrm{Hot}(f_{0}^{i}),\\hat{f}_{0}^{i})+\\sum_{i,j}\\mathcal{L}_{\\mathtt{C E}}(0\\mathrm{ne}\\!-\\!\\mathrm{Hot}(e_{0}^{(i,j)}),\\hat{e}_{0}^{(i,j)})\\right]\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\hat{f}_{0}^{i}\\ =\\ [p_{0|t}^{\\theta}(f^{i}\\ =\\ 1|{\\mathcal G}_{t}),\\dots,p_{0|t}^{\\theta}(f^{i}\\ =\\ b|{\\mathcal G}_{t})]^{\\top}\\ \\in\\ [0,1]^{b}$ and $\\hat{e}_{0}^{(i,j)}~=~[p_{0|t}^{\\theta}(e^{(i,j)}{}~=$ $1|\\mathcal{G}_{t}),\\ldots,p_{0|t}^{\\theta}(e^{(i,j)}=a+1|\\mathcal{G}_{t})]^{\\top}\\in[0,1]^{a+1}$ ", "page_idx": 23}, {"type": "text", "text": "Proof. We follow the notation and present the permutation $\\mathcal{P}$ on every node as a bijection $\\sigma:$ $\\{1,\\cdot\\cdot\\cdot,n\\}\\mapsto\\{1,\\cdot\\cdot\\cdot,n\\}$ . We first analyze the cross-entropy loss on the nodes for a single training graph $\\mathcal{G}_{\\mathrm{0}}$ and taking expectation $\\mathbb{E}_{\\mathcal{G}_{0}}$ keeps the permutation invariance: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{\\mathtt{n o d e}}(\\mathcal{G}_{0})=T\\mathbb{E}_{t\\sim\\mathcal{U}_{(0,T)}}\\mathbb{E}_{q_{t}\\mid0}(\\mathcal{G}_{t}|\\mathcal{G}_{0})\\sum_{i}\\mathcal{L}_{\\mathtt{c x}}(0{\\mathrm{ne-Hot}(f_{0}^{i})},\\hat{f}_{0}^{i})}\\\\ &{\\qquad\\qquad\\qquad=T\\mathbb{E}_{t\\sim\\mathcal{U}_{(0,T)}}\\sum_{\\ell_{t}}q_{\\ell_{0}}(\\mathcal{G}_{t}|\\mathcal{G}_{0})\\sum_{i}\\mathcal{L}_{\\mathtt{c x}}(0{\\mathrm{ne-Hot}(f_{0}^{i})},\\hat{f}_{0}^{i})}\\\\ &{\\qquad\\quad\\overset{(*)}{=}T\\mathbb{E}_{t\\sim\\mathcal{U}_{(0,T)}}\\sum_{\\ell_{t}}q_{t|0}(\\mathcal{P}(\\mathcal{G}_{t})|\\mathcal{P}(\\mathcal{G}_{0}))\\sum_{i}\\mathcal{L}_{\\mathtt{c x}}(0{\\mathrm{ne-Hot}(f_{0}^{i})},\\hat{f}_{0}^{i})}\\\\ &{\\qquad\\overset{(**)}{=}T\\mathbb{E}_{t\\sim\\mathcal{U}_{(0,T)}}\\sum_{\\ell_{t}}q_{t|0}(\\mathcal{P}(\\mathcal{G}_{t})|\\mathcal{P}(\\mathcal{G}_{0}))\\sum_{i}\\mathcal{L}_{\\mathtt{c x}}(0{\\mathrm{ne-Hot}(f_{0}^{\\sigma(i)})},\\hat{f}_{0}^{\\sigma(i)})}\\\\ &{\\qquad=\\mathcal{L}_{\\mathtt{n o d e}}(\\mathcal{P}(\\mathcal{G}_{0}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $(^{*})$ is from the permutation invariance of the forward process and $(^{**})$ is from the permutation equivariance of the graph-to-graph backbone and the permutation invariance of the cross-entropy ", "page_idx": 23}, {"type": "text", "text": "loss. A similar result can be analyzed on the cross-entropy loss on the edges ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{edge}}(\\mathcal{G}_{0})=T\\mathbb{E}_{t\\sim\\mathcal{U}_{(0,T)}}\\mathbb{E}_{q_{t\\mid0}(\\mathcal{G}_{t}\\mid\\mathcal{G}_{0})}\\sum_{i,j}\\mathcal{L}_{\\mathtt{c E}}(\\mathtt{O n e}\\!-\\!\\mathrm{Hot}(e_{0}^{(i,j)}),\\hat{e}_{0}^{(i,j)})=\\mathcal{L}_{\\mathrm{edge}}(\\mathcal{P}(\\mathcal{G}_{0}))\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and we omit the proof here for brevity. ", "page_idx": 24}, {"type": "text", "text": "D Sampling Algorithm ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "A Step-by-step procedure about the $\\tau$ -leaping graph generation is presented in Algorithm 2. ", "page_idx": 24}, {"type": "text", "text": "Algorithm 2 \u03c4-Leaping Graph Generation   \n1: t \u2190T   \n2: $\\mathcal{G}_{t}=(\\{e^{(i,j)}\\}_{i,j\\in\\mathbb{N}_{\\leq n}^{+}},\\{f^{i}\\}_{i\\in\\mathbb{N}_{\\leq n}^{+}})\\leftarrow\\pi_{\\mathbf{ref}}(\\mathcal{G})$   \n3: while $t>0$ do   \n4: for $i=1,\\hdots,n$ do   \n5: for $s=1,\\ldots,b$ do   \n6: $\\begin{array}{r l}&{{\\tilde{\\mathbf{R}}}_{\\theta,t}^{i}(f^{i},s)=\\mathbf{R}_{t}^{i}(s,f^{i})\\sum_{f_{0}^{i}}\\frac{q_{t|0}(s|f_{0}^{i})}{q_{t|0}(f^{i}|f_{0}^{i})}p_{\\theta}(f^{i}|\\mathcal{G}_{t},t)}\\\\ &{{\\ J_{f^{i},s}}\\gets\\mathrm{Poisson}(\\tau\\mathbf{R}_{t}^{i}(s,f^{i}))}\\end{array}$   \n7: \u25b7# of transition for every node   \n8: end for   \n9: end for   \n10: for $i,j=1,\\dots,n$ do   \n11: for $s=1,\\ldots,a$ do   \n12: $\\begin{array}{r l}&{\\tilde{\\mathbf{R}}_{\\theta,t}^{(i,j)}(e^{(i,j)},s)=\\mathbf{R}_{t}^{(i,j)}(s,e^{(i,j)})\\sum_{e_{0}^{(i,j)}}\\frac{q_{t|0}(s|e_{0}^{(i,j)})}{q_{t|0}(e^{(i,j)}|e_{0}^{(i,j)})}p_{\\theta}(e^{(i,j)}|\\mathcal{G}_{t},t)}\\\\ &{J_{e^{(i,j)},s}\\gets\\mathrm{Poisson}(\\tau\\mathbf{R}_{t}^{(i,j)}(s,e^{(i,j)}))\\phantom{\\sum_{e_{0}^{(i,j)}}\\frac{q_{t|0}(s|e_{0}^{(i,j)})}{(i,j)!}}\\mathsf{P}*\\mathrm{of~transition~for~}}\\end{array}$   \n13: r every edge   \n14: end for   \n15: end for   \n16: for $i=1,\\hdots,n$ do   \n17: if b $\\textstyle\\sum_{s=1}^{b}J_{f^{i},s}>1$ or $\\textstyle\\sum_{s=1}^{b}J_{f^{i},s}=0$ then   \n18: $f^{i}\\leftarrow f^{i}$ \u25b7stay the same   \n19: else   \n20: $\\begin{array}{r l}&{\\;\\;\\;s^{*}=\\arg\\operatorname*{max}_{s}\\{J_{f^{i},s}\\}_{s=1}^{b}}\\\\ &{\\;\\;f^{i}\\leftarrow s^{*}}\\end{array}$   \n21: \u25b7update node   \n22: end if   \n23: end for   \n24: for $i$ , $j=1,\\dots,n$ do   \n25: if $\\textstyle\\sum_{s=1}^{a}J_{e^{(i,j)},s}>1$ or $\\textstyle\\sum_{s=1}^{a}J_{e^{(i,j)},s}=0$ then   \n26: $e^{(i,j)}\\leftarrow e^{(i,j)}$ \u25b7stay the same   \n27: else   \n28: $\\begin{array}{r}{\\boldsymbol{s}^{*}=\\arg\\operatorname*{max}_{\\boldsymbol{s}}\\{J_{e^{(i,j)},\\boldsymbol{s}}\\}_{s=1}^{b}}\\\\ {\\boldsymbol{e}^{(i,j)}\\leftarrow\\boldsymbol{s}^{*}\\quad\\quad\\quad\\quad\\quad\\quad}\\end{array}$   \n29: \u25b7update edge   \n30: end if   \n31: end for   \n32: $t\\leftarrow t-\\tau$   \n33: end while ", "page_idx": 24}, {"type": "text", "text": "E Auxiliary Features, PNA and FiLM Modules ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "For learning a better graph-to-graph mapping $p_{0|t}^{\\theta}(\\mathcal{G}_{0}|\\mathcal{G}_{t})$ , artificially augmenting the node-level features and graph-level features is proved effective to enhance the expressiveness of graph learning models [81, 73]. For this setting, we keep consistent with the state-of-the-art model, DiGress [73], and extract the following three sets of auxiliary features. Note that the following features are extracted on the noised graph $\\mathcal{G}_{t}$ . ", "page_idx": 24}, {"type": "text", "text": "We binarize the edge tensor $\\mathbf{E}$ into an adjacency matrix $\\mathbf{A}\\in\\{0,1\\}^{n\\times n}$ whose 1 entries denote the corresponding node pair is connected by any type of edge. ", "page_idx": 25}, {"type": "text", "text": "Motif features. The number of length- $3/4/5$ cycles every node is included in is counted as the topological node-level features; also, the total number of length- $3/4/5/6$ cycles is the topological graph-level features. ", "page_idx": 25}, {"type": "text", "text": "Spectral features. The graph Laplacian is decomposed. The number of connected components and the first 5 non-zero eigenvalues are selected as the spectral graph-level features. An estimated indicator of whether a node is included in the largest connected component and the first 2 eigenvectors of the non-zero eigenvalues are selected as the spectral node-level features. ", "page_idx": 25}, {"type": "text", "text": "Molecule features. On molecule datasets, the valency of each atom is selected as the node-level feature, and the total weight of the whole molecule is selected as the graph-level feature. ", "page_idx": 25}, {"type": "text", "text": "The above node-level features and graph-level features are concatenated together as the auxiliary nodelevel features $\\mathbf{F}_{\\mathrm{aux}}$ and graph-level features y. An important property is that the above node-level features are permutation-equivariant and the above graph-level features are permutation-invariant, whose proof is straightforward so we omit it here. ", "page_idx": 25}, {"type": "text", "text": "Next, two important modules used in the MPNN backbone: PNA and FiLM are detailed. ", "page_idx": 25}, {"type": "text", "text": "PNA module. The PNA module [12] is implemented as follows, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathtt{P M A}\\big(\\{\\mathbf{x}_{i}\\}_{i=1}^{n}\\big)=\\mathtt{M L P}(\\mathtt{m i n}(\\{\\mathbf{x}_{i}\\}_{i=1}^{n})\\oplus\\mathtt{m a x}(\\{\\mathbf{x}_{i}\\}_{i=1}^{n})\\oplus\\mathtt{m e a n}(\\{\\mathbf{x}_{i}\\}_{i=1}^{n})\\oplus\\mathtt{s t d}(\\{\\mathbf{x}_{i}\\}_{i=1}^{n}))\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $\\oplus$ is the concatenation operator, $\\mathbf{x}_{i}\\in\\mathbb{R}^{d}$ ; min, max, mean, and std are coordinate-wise, e.g., $\\mathtt{m i n}(\\{\\mathbf{x}_{i}\\}_{i=1}^{n})\\in\\mathbb{R}^{d}$ . ", "page_idx": 25}, {"type": "text", "text": "FiLM module. FiLM [57] is implemented as follows, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname{FiLM}(\\mathbf{x}_{i},\\mathbf{x}_{j})=\\operatorname{Linear}(\\mathbf{x}_{i})+\\operatorname{Linear}(\\mathbf{x}_{i})\\odot\\mathbf{x}_{j}+\\mathbf{x}_{j}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where Linear is a single fully-connected layer without activation function and $\\odot$ is the Hadamard product. ", "page_idx": 25}, {"type": "text", "text": "F Supplementary Details about Experiments ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "F.1 Hardware and Software ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We implement DISCO in PyTorch5 and PyTorch-geometric6. All the efficiency study results are from one NVIDIA Tesla V100 SXM2-32GB GPU on a server with 96 Intel(R) Xeon(R) Gold 6240R CPU $\\textcircled{a}2.40\\mathrm{GHz}$ processors and 1.5T RAM. The training on QM9 and Community can be finished in 2 hours. For the training on SBM, Planar, it can be finished within 48 hours to get decent validity. The training on MOSES and GuacaMol can be finished within 96 hours. ", "page_idx": 25}, {"type": "text", "text": "F.2 Dataset Setup ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "F.2.1 Dataset Statistics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "The statistics about all the datasets used in this paper are presented in Table 7, where $a$ is the number of edge types, $b$ is the number of node types, $|\\bf E|$ is the number of edges and $\\left|\\mathbf{F}\\right|$ is the number of nodes. ", "page_idx": 25}, {"type": "text", "text": "F.2.2 Detailed Settings on Plain Graph Datasets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Dataset Split. We follow the settings of SPECTRE [51] and DiGress [73] to split the SBM, Planar [51], and Community [82] datasets into $64/16/20\\%$ for training/validation/test set. ", "page_idx": 25}, {"type": "table", "img_path": "YkSKZEhIYt/tmp/9d85d1ce891ad30789b85645eeefb2db12be3dc8218a234984b9fe58ebfc72f5.jpg", "table_caption": ["Table 7: Dataset statistics. "], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "Metrics. The Maximum Mean Discrepancy (MMD) [82] measures the discrepancy between two sets of distributions. The relative squared MMD [73]is defined as follows ", "page_idx": 26}, {"type": "equation", "text": "$$\ns c o r e=\\frac{\\mathbf{M}\\mathbf{M}\\mathbf{D}^{2}(\\{\\mathcal{G}\\}_{\\mathbf{gen}}||\\{\\mathcal{G}\\}_{\\mathbf{test}})}{\\mathbf{M}\\mathbf{M}\\mathbf{D}^{2}(\\{\\mathcal{G}\\}_{\\mathbf{train}}||\\{\\mathcal{G}\\}_{\\mathbf{test}})},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $(\\{\\mathcal{G}\\}_{\\mathtt{g e n}}$ , $(\\{\\mathcal{G}\\}_{\\mathtt{t r a i n}}$ , and $(\\{\\mathcal{G}\\}_{\\tt t e s t}$ are the sets of generated graphs, training graphs, and test graphs, respectively. We report the above relative squared MMD for degree distributions (Deg.), clustering coefficient distributions (Clus.), and average orbit counts (Orb.) statistics (the number of occurrences of all substructures with 4 nodes). In addition, the Uniqueness, Novelty, and Validity are chosen. Uniqueness reports the fraction of the generated nonisomorphic graphs; Novelty reports the fraction of the generated graphs not isomorphic with any graph from the training set; Validity checks the fraction of the generated graphs following some specific rules. For the SBM dataset, we follow the validity check from [51] whose core idea is to check whether real SBM graphs are statistically indistinguishable from the generated graphs; for the Planar dataset, we check whether the generated graphs are connected and are indeed planar graphs. Because the Community dataset does not have the Validity metric, we only report the Uniqueness, Novelty, and Validity results on the SBM and Planar datasets. ", "page_idx": 26}, {"type": "text", "text": "We report mean\u00b1std in 5 runs. ", "page_idx": 26}, {"type": "text", "text": "Baseline methods. GraphRNN [82], GRAN [42], GG-GAN [37], MolGAN [9], SPECTRE [51], EDP-GNN [56], GraphGDP [26], DiscDDPM [22], EDGE [10], ConGress [73], DiGress [73] are chosen. ", "page_idx": 26}, {"type": "text", "text": "F.2.3 Detailed Settings on Molecule Graph Datasets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Dataset Split. We follow the split of QM9 from DiGress [73] and follow the split of MOSES [58] and GuacaMol [6] according to their benchmark settings. Their statistics are presented in Table 7. ", "page_idx": 26}, {"type": "text", "text": "Metrics. For QM9, Uniqueness, Novelty, and Validity are chosen as metrics. The first two are the same as introduced in Section F.2.2. The Validity is computed by building a molecule with RdKit and checking if we can obtain a valid SMILES string from it. ", "page_idx": 26}, {"type": "text", "text": "For MOSES, the chosen metrics include Uniqueness, Novelty, Validity, Filters, Fr\u00e9chet ChemNet Distance (FCD), Similarity to a nearest neighbor (SNN), and Scaffold similarity (Scaf), which is consistent with DiGress [73]. The official evaluation code 8 is used to report the performance. ", "page_idx": 26}, {"type": "text", "text": "For GuacaMol, the chosen metrics include Uniqueness, Novelty, Validity, KL Divergence, and Frec\u00b4het ChemNet Distance (FCD), which is consistent with DiGress [73]. The official evaluation code 9 is used to report the performance. ", "page_idx": 26}, {"type": "text", "text": "We report mean\u00b1std in 5 runs except MOSES and GuacaMol, whose computations are too expensive to repeat multiple times. ", "page_idx": 26}, {"type": "text", "text": "Baseline methods. CharacterVAE [20], GrammarVAE [38], GraphVAE [66], GT-VAE [55], Set2GraphVAE [72], GG-GAN [37], MolGAN [9], SPECTRE [51], GraphNVP [50], GDSS [32], ", "page_idx": 26}, {"type": "table", "img_path": "YkSKZEhIYt/tmp/2cad2a083764ecd9dbf0823d307ac46274c48c77fc9db01e5e9af8474d65a254.jpg", "table_caption": ["Table 8: Generation performance (mean\u00b1std) on the Community dataset. "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "EDGE [10], ConGress [73], DiGress [73], GRAPHARM [36],VAE [21], JT-VAE [29], GraphINVENT [53], LSTM [64], NAGVAE [40], and MCTS [28] are chosen. ", "page_idx": 27}, {"type": "text", "text": "F.3 Hyperparameter Settings ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Forward Diffusion Settings. As we introduced in Proposition 3.2, we tried two sets of rate matrices for the node and edge forward diffusion, so that the converged distribution is either uniform or marginal distribution. We found the marginal distribution leads to better results than the uniform distribution. Thus, the reference distribution is the marginal distribution for all the main results, except Tables 6 and 9. The performance comparison between the marginal diffusion and uniform diffusion is presented in the ablation study in Sections 4.4 andF.5. The $\\beta(t)$ controls how fast the forward process converges to the reference distribution, which is set as $\\beta(t)\\overset{!}{=}\\alpha\\gamma^{t}l o g(\\gamma)$ , which is consistent with many existing works [23, 70, 8]. In our implementation, we assume the converged time $T=1$ and for the forward diffusion hyperparameters $(\\alpha,\\gamma)$ we tried two sets: (1.0, 5.0) and (0.8, 2.0) where the former one can ensure at $T=1$ the distribution is very close to the reference distribution, and the latter one does not fully corrupt the raw data distribution so the graph-to-graph model $p_{0|t}^{\\theta}$ is easier to train. ", "page_idx": 27}, {"type": "text", "text": "Reverse Sampling Settings. The number of sampling steps is determined by $\\tau$ , which is $\\scriptstyle{\\mathtt{r o u n d}}({\\frac{1}{\\tau}})$ if we set the converged time $T=1$ . We select the number of sampling steps from $\\{50,100,500\\}$ , which is much smaller the number of sampling steps of DiGress [73] from $\\{500,1\\dot{0}00\\}$ . For the number of nodes $n$ in every generated graph, we compute a graph size distribution of the training set by counting the number of graphs for different sizes (and normalize the counting to sum it up to 1). Then, we will sample the number of nodes from this graph size distribution for graph generation. ", "page_idx": 27}, {"type": "text", "text": "Neural Network Settings. For DISCO-GT, the parametric graph-to-graph model $p_{0|t}^{\\theta}$ is graph transformer (GT). We use the exactly same GT architecture as DiGress [73] and adopt their recommended configurations 10. The reason is that this architecture is not our contribution, and setting the graph-to-graph model p\u03b80|t s ame can ensure a fair comparison between the discrete-time graph diffusion framework (from DiGress) and the continuous-time graph diffusion framework (from this work). For DISCO-MPNN, we search the number of MPNN layers from $\\{3,5,8\\}$ , set all the hidden dimensions the same, and search it from $\\{256,512\\}$ . For both variants, the dropout is set as 0.1, the learning rate is set as $2e^{-4}$ , and the weight decay is set as 0. ", "page_idx": 27}, {"type": "text", "text": "F.4 Additional Results on Community ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Additional Community plain graph dataset results are in Table 8. Our observation is consistent with the main content: both variants of DISCO are on par with, or even better than the SOTA general graph diffusion generative model, DiGress. ", "page_idx": 27}, {"type": "text", "text": "Table 9: Ablation study $(\\mathrm{mean}{\\pm}\\mathrm{std}\\%)$ with MPNN backbone. V., U., and N. mean Valid, Unique, and Novel. ", "page_idx": 28}, {"type": "table", "img_path": "YkSKZEhIYt/tmp/acd3de128c63230b1fa9b209291dd5163c6da9de0f6288eeec22939bd33cca35.jpg", "table_caption": [], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "F.5 Additional Ablation Study ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Table 9 shows the ablation study of DISCO-MPNN on QM9 dataset. Our observations are consistent with the main content: (1) generally, the fewer sampling steps, the lower the generation quality but method\u2019s performance is robust in terms of the decreasing of sampling steps; (2) the marginal reference distribution is better than the uniform distribution, consistent with the observation from DiGress [73]. ", "page_idx": 28}, {"type": "text", "text": "F.6 Convergence Study ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Figure 3 shows the training loss of DISCO-GT and DISCO-MPNN on four datasets, whose X-axis is the number of iterations (i.e., the number of epochs $\\times$ the number of training samples / batch size). We found that overall the training losses converge smoothly on 4 datasets. ", "page_idx": 28}, {"type": "image", "img_path": "YkSKZEhIYt/tmp/0fa27f45c8b221a6acca82371ed1d005408bfdbccb85c32011e4dad7acca9656.jpg", "img_caption": ["Figure 3: Training loss of DISCO on different datasets and backbone models. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "F.7 Visualization ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "The generated graphs on the SBM and Planar datasets are presented in Figure 4. We clarify that the generated planar graphs are selected to be valid because, as Table 1 shows, not all the generated graphs are valid planar graphs, but the planar layout can only visualize valid planar graphs in our setting 11. The generated SBM graphs are not selected; even if a part of them cannot pass the strict SBM statistic test (introduced in Section F.2.2 - Metrics), most, if not all, of them still form $2-5$ densely connected clusters. ", "page_idx": 29}, {"type": "text", "text": "The generation trajectory of SBM graphs is presented in Figure 5 which demonstrates the reverse denoising process visually. ", "page_idx": 29}, {"type": "image", "img_path": "YkSKZEhIYt/tmp/8491e88f97594726359e2588e1d11a8ddaab4ab25193abac82d219e33db51203.jpg", "img_caption": ["Figure 4: Generated graphs. ", "(b) Planar "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "G Limitation and Future Work ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "In this paper, we study the generation of graphs with categorical node and edge types. The current model DISCO cannot be applied to generate graphs with multiple node/edge features (e.g., multiplex networks) and this is an important future work to study. Also, we view the absence of edge as a special type of edge, which forms a complete graph and promotes the expressiveness of our MPNN backbone model. However, it will lead to quadratic complexity concerning the number of nodes. For our current dataset (e.g. graphs with $<1000$ nodes) the complexity is still acceptable but for future studies on generating large graphs, we aim to design more efficient diffusion generative models. ", "page_idx": 29}, {"type": "image", "img_path": "YkSKZEhIYt/tmp/c680785e09ff58de06af947f066e2e3df99de8a5c625a305593c3f9dd4371ace.jpg", "img_caption": [], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "Figure 5: Generation trajectory of SBM graphs with different sizes. Every row is the generation trajectory of one graph from time $t=T$ (left) to $t=0$ (right) with equal time intervals. ", "page_idx": 30}, {"type": "text", "text": "H Broader Impact ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here. ", "page_idx": 30}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The abstract and introduction summarize all the theoretical and experimental contributions of this paper. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 31}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: The limitation is mentioned in Section G. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 31}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Detailed assumptions and proofs are included in the Section C. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 32}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: Detailed experimental settings are included in Section F. Also, the code of this paper is released in the supplementary materials. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 32}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: All the datasets are publicly available and their links are included in Section F. The code is released in the supplementary materials; we will formally release the code after acceptance. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 33}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: All the training details are included in Section F and the released codes. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 33}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: We report average results with standard deviation on all the datasets, except MOSES and GuacaMol, whose computations are too expensive to repeat multiple times. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: The compute resources are detailed in Section F.1. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 34}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We checked the code of Ethics and the paper conforms with the Code of Ethics. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 34}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: It is mentioned in Section H. Our work is general graph generative modeling, which shares potential societal consequences with many established graph generative models. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 35}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: We did not scrape any dataset from Internet and our released model is of low risk for misuse. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 35}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: All the datasets and codes of baseline methods are publicly available and for the academic purpose. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 36}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: We did not release any new dataset and the code released will be well documented after the acceptance. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 36}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 36}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 36}]