[{"type": "text", "text": "Asynchronous Perception Machine for Efficient Test Time Training ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Rajat Modi\u2217 Yogesh Singh Rawat Centre for Research in Computer Vision University of Central Florida Orlando, FL 32765   \nrajat.modi@ucf.edu,yogesh@crcv.ucf.edu   \nhttps://rajatmodi62.github.io/apm_project_page ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In this work, we propose Asynchronous Perception Machine (APM), a computationally-efficient architecture for test-time-training (TTT). APM can process patches of an image one at a time in any order asymmetrically, and still encode semantic-awareness in the net. We demonstrate APM\u2019s ability to recognize out-of-distribution images without dataset-specific pre-training, augmentation or any-pretext task. APM offers competitive performance over existing TTT approaches. To perform TTT, APM just distills test sample\u2019s representation once. APM possesses a unique property: it can learn using just this single representation and starts predicting semantically-aware features. ", "page_idx": 0}, {"type": "text", "text": "APM demostrates potential applications beyond test-time-training: APM can scale up to a dataset of 2D images and yield semantic-clusterings in a single forward pass. APM also provides first empirical evidence towards validating GLOM\u2019s insight, i.e. if input percept is a field. Therefore, APM helps us converge towards an implementation which can do both interpolation and perception on a sharedconnectionist hardware. Our code is publicly available at this link. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In these past centuries, computing-machines have become a lot faster [98]. This made it possible to train higher-parameterized neural nets and led to interesting emergent abilities [86]. As was predicted by Turing himself, and as were his suspicions of Lady Lovelace\u2019s arguments against learning machines[96]: neural-nets can now finally learn without human-feedback [7], paint pictures [82] and even compose a sonnet [98]. Even with such impressive-progress, a key question still remains: how can these nets recognize images whose distribution is far different from the ones which were used during training? For e.g., consider a self-driving car trying to stop when it encounters a pedestrian crossing a road. Such practical scenarios require \u2018instantaneous-decisions\u20192 for ensuring human-safety in autonomous-systems [70]. ", "page_idx": 0}, {"type": "text", "text": "Test-time-training (TTT) [91] is one of the promising techniques for handling such distribution shifts: a neural net adapts to a test sample \u2018on the fly\u2019. Since the label of the sample is not known, the net performs some auxiliary pre-text task like data augmentation [17], rotation [17, 92] or prompt tuning [90] on it. After several such iterations, the net recognizes the test sample. The key idea is that the net is allowed to dynamically adjust its decision boundary even after it has been trained, thereby bringing it much closer to how humans keep learning \u2018continuously\u2019 throughout their lifespans [92]. ", "page_idx": 0}, {"type": "image", "img_path": "7Ye12RLZ4P/tmp/8b477557ac23f2a6d8bd8101a3ab9ce5862d29c677fab167513621d8c43ba034.jpg", "img_caption": ["Figure 1: (i) Asynchronous Perception Machine (APM): An image $I$ passes through a column module and routes to a trigger column $T_{i}$ . $T_{i}$ then unfolds and generates $h\\times w$ location-specific queries. These queries are i.i.d and can be parallelized across cores of a gpu [53]. Each query $T_{i}$ is passed through a shared MLP and yields the vector $f_{x y}^{\\prime}$ and $f_{r g b}^{\\prime}$ . MLP is queried iteratively until whole grid $f^{'}$ comes into existence. Classification then involves comparing the averaged representation $f$ with class-specific textual representations in the contrastive space . (ii) Folded State: The parameters which the net learns are parameters of $T$ and MLP. (iii) Unfolded State: $T$ expands to yield $h\\times w$ queries \u2018on-the-fly\u2019. Learning involves oscillating this net between folded and unfolded states. This net can be trained via backpropogation [84, 33]. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Despite the success of existing TTT approaches, several limitations need to be addressed [90, 17, 92]: 1) The Information Bottleneck Problem [70]: Multiple TTT iterations requires feed-forward through many hidden layers multiple times, making it computationally expensive. 2) Reliance on a surrogate pre-text task: the optimal data-augmentation pipeline or the best pretext task is not known beforehand, worsening the issue even further in an online setting. 3) Furthermore, TTT leverages architectures like transformers which rely on parallel perception: this requires projecting all input patches into a shared representational space, thereby consuming significant memory. ", "page_idx": 1}, {"type": "text", "text": "Inspired by GLOM\u2019s philosophy [34], we hereby propose Asynchronous Perception Machine (APM), a new architecture for efficient test-time-training. 1) It handles the information-bottleneck problem by directly learning a shortcut from input image to final representation from last layer of a model [33]. 2) During TTT, we compute test-sample\u2019s representation only once. Subsequent iterations involve over-fitting on this representation only and doesn\u2019t require any data-augmentation/pretext task. 3) APM can operate on a single patch at any location asynchronously [97] and still encode semantic-awareness in the net, thereby offering a fresh perspective towards machine-perception. ", "page_idx": 1}, {"type": "text", "text": "We make the following contributions in this work: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We propose APM, a GLOM-inspired architecture that can perform test-time-training without requiring data augmentation/auxilary-tasks. APM is a step towards validating GLOM\u2019s philosophical insight: a percept is really a field [34].   \n\u2022 APM is computationally efficient, i.e it almost halves the number of FLOPs over existing methods [17, 92]. APM matches/surpasses TTT performance by $0.4\\%\\mathrm{~-~}8\\%$ across 16 datasets.   \n\u2022 APM is architecturally simple: a convolutional layer and a single MLP of 5 layers. APM can still learn using one sample and semantically clusters on a collection of images [33]. ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We draw from insights previously philosophically mentioned in GLOM [34]. Consider how machine perception has been done classically: an input $\\boldsymbol{x}^{\\grave{\\mathbf{\\upvarepsilon}}}\\in\\mathbb{R}^{c\\times h\\times w}$ is transformed by a non-linear function $f$ to a perceptual feature grid $\\boldsymbol{y}\\,\\in\\,\\mathbb{R}^{c^{\\prime}\\times\\bar{h}\\times w}$ , $c$ denotes image channels, $h,w$ are input spatial dimensions. ", "page_idx": 1}, {"type": "text", "text": "Islands of agreement [34] Rather than viewing this matrix $y$ as a cuboidal grid, one can now see this as column vectors $v_{c^{\\prime}}$ at each location. There are $h\\times w$ such columns in $y$ . Therefore, there exists a one-to-one mapping, between each input patch i.e. $(x,y)$ with the vector $v_{c^{\\prime}}$ at that location. A neural net $f$ can then thus learn to fti $f(I,x,y)\\rightarrow v_{c^{\\prime}}$ . These vectors $v_{c^{\\prime}}$ have been termed islands of agreement in GLOM [34]. They were recently demonstrated on realistic datasets by [70], showing some promise that trained models can serve as free supervision-sources for vectors $v_{c^{\\prime}}$ . ", "page_idx": 1}, {"type": "text", "text": "Next, we imagine feed-forwarding one patch $(x,y)$ into the neural net $f$ and estimating the response $v_{c^{\\prime}}$ . It thus becomes possible to process these patches one-by-one rather than keeping them all together in memory [100]. We then further imagine auto-regressively querying $f$ until the whole grid $y$ has been brought into existence. ", "page_idx": 2}, {"type": "text", "text": "Implicit-representations/Neural-Fields: The neural net $f$ inputs a coordinate based query, $(I,x,y)$ where $(x,y)$ is the 2D location in an image $I$ . It then gives the answer $v_{c^{\\prime}}$ [34]. Such implicit representations have been studied extensively in 3D novel-view synthesis [65]. However, in the above problem $I$ is a simple 2D image. In a recent work [34], it was hypothesized that neural-implicit representations can work on 2D images as well without having to retrain it every-time. ", "page_idx": 2}, {"type": "text", "text": "Layer Skipping [33]: We now combine the previous two insights together. Imagine that $v_{c^{\\prime}}$ has been estimated from a model after travelling through several layers. We can distill these $v_{c^{\\prime}}$ into an implicit representation $f$ and learn a direct mapping between input $x$ and last layer of a model [33]. During inference, we can skip feed-forwarding through all the model layers. The only feed-forward cost we then pay is what it costs to travel through $f$ . Provided that the number of parameters in $f$ are lesser than parameters in the teacher model, we can expect computational speedups. ", "page_idx": 2}, {"type": "text", "text": "We now introduce a connectionist-net $f$ motivated by these insights. It can perform efficient testtime-training on a given 2D input. ", "page_idx": 2}, {"type": "text", "text": "3 Asynchronous Perception Machine (APM) ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "APM processes an input image $I$ via two novel mechanisms: 1) A column module $T$ which is said to contain an input image $I,2$ ) a column folding-unfolding mechanism that operates during each forward-pass. We first provide a technical analogy to better understand APM. ", "page_idx": 2}, {"type": "text", "text": "3.1 A Technical Analogy[34] ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "A neural field does 3D novel view synthesis by querying an MLP with a point $\\left(x,y,z\\right)$ and yielding corresponding rgb. In a similar way, APM does $2D$ perception by querying an MLP with an image $I$ , and a location $(x,y)$ to yield location-specific feature $f_{x y}$ . APM features a new mechanism to query the MLP, i.e a column module $T$ . Next, we define this column representation $T$ . ", "page_idx": 2}, {"type": "text", "text": "3.2 Global Column Module: Defining compressed representation T ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We define a column $T$ as a vector of dimensions $1\\times1\\times d$ . Our aim is to map image $I$ in this $T$ , so that $T$ can summarize its entire identity. Given an image of dimensions $c\\times h\\times w$ , we run a 2D convolution on it. Number of filters are set as accordingly. The resultant $1\\times h^{\\prime}\\times w^{\\prime}$ feature map is then flattened into a single column vector T of dimensions $d=h^{\\prime}\\times w^{\\prime}$ . We shall refer to this column $\\mathrm{T}$ as \"triggering hyper-column\"(seed). The only learnable parameters in this column are parameters of a convolution filter.3[98, 97]. ", "page_idx": 2}, {"type": "text", "text": "3.3 Abstract view: The Column Unfolding-Folding Mechanism ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The trigger column T now starts undergoing cycles of folding-unfolding (Fig 1). During unfolding, the column T copies itself to yield $h\\times w$ location-aware columns. During folding, these $h\\times w$ columns collapse back into a single column $T$ . The neural-net then oscillates between these folded-unfolded phases during learning iterations. ", "page_idx": 2}, {"type": "text", "text": "3.4 Computational Principle: Location-aware columns and their collapse ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The unfolding-process shall now be concerned with generating location-aware column $T_{i j}$ from $T$ . We generate $h*w$ 2-D non-parametric positional encoding similar to the ones being used in transformers [100] and neural fields [65]. The trigger column $T_{i j}$ is then given by $T_{i j}\\,=\\,(T|p_{i j})$ , where | denotes the stacking operator and $(1,1)\\leq\\bar{(i,j)}\\leq(H,W)$ . T can be said to encode identity of an image. ", "page_idx": 2}, {"type": "text", "text": "The folding-process involves collapsing all columns $T_{i j}$ back into $T$ from which they had begun. This is achievable since the $p_{i j}$ used in $T_{i j}$ was deterministic. An abstract-mathematical intuition on folding is that all $p_{i j}$ in $T_{i j}$ get deleted/annihilated at the end of every forward-pass, and only $T$ is left. Positional-encodings contain periodic-sinusoids which offers a strong positional-prior in practice [100]. Gradients collected from all $T_{i j}$ then update the parameters of the convolutional fliter in the column $T$ . This sharing of $\\mathrm{T}$ across different locations $(i,j)$ now induces a new fundamental behavior [34] already hypothesized in GLOM (more details in section 5.3). ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "This representation $T_{i j}\\,=\\,(T|p_{i j})$ exhibits a strong-symmetry breaking behavior $[34]^{4}$ . For e.g., consider different locations across same image $I$ . Although the identity $\\mathrm{T}$ will be the same, the positional encoding $p_{i j}$ shall differentiate among different locations. The converse holds true: given the same position $p_{i j}$ in two different images, the column $T$ shall differentiate among identities of different images. ", "page_idx": 3}, {"type": "text", "text": "All the experiments in this paper are performed without-injecting the local patch $I_{x y}$ in the trigger column $T_{i j}$ . This showcases the strong-symmetry breaking behaviour of positional-encodings[100]: they can disentangle location-information from global $I$ without-requiring an additional patch-prior. ", "page_idx": 3}, {"type": "text", "text": "3.5 Firing location-aware columns into a shared MLP ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Each column $T_{i j}$ is passed through an MLP to yield location-specific features $f_{i j}$ and RGB values $R G B_{i j}$ . Number of neurons in the first layer of the MLP is same as dimensionality of column $T_{i j}$ . ", "page_idx": 3}, {"type": "text", "text": "Column Independence: The MLP is shared across all the columns. One column is also independent of another as illustrated in Fig1. Therefore, the MLP can be queried sequentially. Firing a column $T_{i j}$ into the MLP yields a column vector $v_{c}$ . Once $h\\times w$ columns are finished firing, we get a feature grid $f$ of dimensions $h\\times w\\times c$ . Note that the number of columns can be as low as 1. ", "page_idx": 3}, {"type": "text", "text": "3.6 Training and Losses ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We detail how the $t^{t h}$ iteration of test-time-training could be performed. First, the obtained feature grid $f\\epsilon\\mathbb{R}^{h\\times w\\times c}$ from APM is averaged to yield $f_{a v g}\\epsilon\\mathbf{R}^{c}$ . For the first TTT iteration, i.e. $t=1$ , the image $I$ is feed-forwarded through a multi-modal teacher like CLIP to get a CLS token $f_{c l s}$ and corresponding text representation $t_{c l s}$ . APM then learns to estimate this same target feature $f_{c l s}$ . We enforce this by a simple $L_{2}$ constraint as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n{\\cal L}_{i}={\\cal L}_{2}(f_{a v g,t},f_{c l s})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $f_{a v g,t}$ is the averaged output feature from APM at a particular TTT iteration t. Note that the target $f_{c l s}$ is only estimated in $t=1$ and remains the same for $t\\geq2$ , i.e. subsequent feed-forward through teacher is not needed. ", "page_idx": 3}, {"type": "text", "text": "Memory-efficient estimation of $f_{a v g,t}$ : During a TTT iteration $t$ , $f_{a v g}$ is computed as a simple average of $f\\epsilon\\mathbb{R}^{h\\times w\\times c}$ . This would require $h\\times w$ columns to exist in the memory simultaneously. APM\u2019s design assumes column independence which allows estimating $f_{a v g}$ as a statistical running average, i.e. ", "page_idx": 3}, {"type": "equation", "text": "$$\nf_{a v g}=\\frac{n\\times f_{a v g}+f_{i,j}}{n+1}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "assuming, $n$ columns have already been fired into the APM and one additional column corresponding to position $(i,j)$ of image $I$ is in the process of firing. This procedure is repeated until all positions $(i,j)$ are exhausted. We represent the sequential column-firing by a \u2018Gather-Grid\u2019 operator in Fig1. ", "page_idx": 3}, {"type": "text", "text": "Predicting image class-label: After certain TTT iterations, (say $\\mathrm{t}=20$ ), the output feature $f_{a v g,t}$ and the textual features $t_{c l s}$ are obtained. Image-classification then follows the standard practice of comparing the distance of $f_{a v g,t}$ with each plausible class feature $t_{c l s}$ in the contrastive space and choosing the closest one as the prediction [81]. ", "page_idx": 3}, {"type": "text", "text": "4 Experimenting with APM ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We quantitatively evaluate APM on zero-shot test-time-training on popular benchmarks containing distribution-shifts [17, 92, 90]. Next, we quantitatively explore its computational efficiency. ", "page_idx": 3}, {"type": "text", "text": "Datasets: Cifar-10C [92] contains 5 level of corruptions on the test-set with 15 types of noises. Larger datasets with significant distribution shifts consists of ImageNet val/other curated splits. For e.g., ImageNet-V2 contains natural images consisting of $10k$ images across 1000 classes. ImageNetA contains 7500 adversarial-images consisting of 200 categories. ImageNet-R consists of 30000 artistic-images across 200 ImageNet categories. ImageNet-Sketch consists of black/white sketches of 50000 images for 1000 classes. ImageNet-C consisting of 15 types of corruptions with 5 levels of severity. There are additional 9 cross-dataset generalization datasets [90]. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Baselines: We compare against standard TTT-online [92], TTT-MAE [17], TPT [90], CLIP VIT-B/16, Coop, CocoOP. We also benchmark CLIP VIT-L/14 and the strongest OpenCLIP VIT-H/14-quickgelu variant pre-trained on dfn5b. ", "page_idx": 4}, {"type": "table", "img_path": "7Ye12RLZ4P/tmp/8fdd43eea36b467a900a31412d9c1f798e0f1eb0fdc5c54f65b1430b3fa0f922.jpg", "table_caption": ["Table 1: APM\u2019s Robustness to Natural Distribution Shifts. CoOp and CoCoOp are tuned on ImageNet using 16-shot training data per category. Baseline CLIP, prompt ensemble, TPT and our APM do not require training data. A $\\checkmark$ in P means that method leveraged pre-trained weights on clean variant of train set aka, Image-net and downstream-ttt on corrupted version. "], "table_footnote": [], "page_idx": 4}, {"type": "table", "img_path": "7Ye12RLZ4P/tmp/eb01051852af21b90955c2ae3e33264828c8eb768fee61e0a8a16ea97528783b.jpg", "table_caption": ["Table 2: APM\u2019s performance on ImageNet-C, level 5. The first three rows are fixed models without test-time training. The third row, ViT probing, is the baseline used in [17]. A $\\checkmark$ in $\\mathbf{P}$ means that method leveraged pre-trained weights on clean variant of train set aka, Image-net and downstream-ttt on corrupted version. CLIP VIT-L/14 is generally more robust. APM does better on $11/15$ noises with an average accuracy score of 50.3. "], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "Results and Analysis: We study APM\u2019s performance on test-time-training on several datasets. APM processes each test sample individually: i.e. the weights are drawn from a normal distribution after processing every sample to prevent information leakage. For zero-shot classification of a test-sample, APM leverages the 80 textual-prompt templates similar to the ones used in CLIP. In Tab 1, APM scales up to zero-shot classification task to datasets with 1000 classes. Using CLIP-ViT B/16 as a teacher, we surpass TPT [90] with an avg score of 62.6 and avg ood-score of 61.2. Next, we benchmark OpenCLIP-VITH/14 against all these splits. Using the same model as our teacher, we get an absolute improvement of $3\\%\\uparrow[84.6\\%]$ on ImageNet val set, $5.1\\%$ \u2191 $[84.2\\%]$ on ImageNet-A, $\\bar{3.2\\%}$ \u2191 $[83.9\\%]$ on ImageNet-V2, $2\\%\\bar{\\uparrow}\\,[94.9\\bar{\\%}]$ on ImageNet-R, $4.3\\%\\uparrow[77.1\\%]$ on ImageNet-Sketch respectively. A similar trend is observed with a VIT-L/14 backbone. This might lead to the conclusion that a stronger teacher seems to benefit APM. In Table 2, we show results on Imagenet-C. Using a CLIP VIT-L/14 teacher, APM gets highest accuracy on $11/15$ noises with the highest average score of 50.3. Note that TTT-MAE also uses a VIT-L encoder, and is pretrained. In contrast, APM doesn\u2019t need any dataset specific-pretraining. Finally, in Tab 3, APM improves upon $4/9$ datasets, comes close on remaining 5 and gets the highest average accuracy score of 65.5. ", "page_idx": 4}, {"type": "text", "text": "APM is computationally efficient: All experiments are run on a same desktop-workstation containing 1x rtx a6000/96GB ram/Ubuntu 22.04/2TB ssd. Flops were counted with meta\u2019s fvcore library [95]. In Tab4, we perform 20 TTT iterations. TTT on CLIP-VIT B/16 baseline used in [90] consumes 462Gflops. Next, we do TTT leveraging APM. At $t=1$ , feed-forwards involves clip-teacher and consumes 20.5 flops. Remaining 19 TTT iterations only involve overfitting on distilled image token at $t=1$ , and consumes 10 flops/TTT-iteration. The entire proflie-dump yields 241.7 flops, which is an almost $50\\%$ reduction over 462 flops. The total memory occupied by APM i.e. $2.7G B$ is more than CLIP-VIT/B $2.3G B$ since teacher is kept in memory during TTT. However, APM only occupies $600M B$ and reduces actual consumed-flops by $50\\%$ . ", "page_idx": 4}, {"type": "table", "img_path": "7Ye12RLZ4P/tmp/725a1eda56874d3639d7b19953178a251f06ca971e8e7bc0d044ffc8432cc354.jpg", "table_caption": ["Table 3: Cross-dataset generalization from ImageNet to fine-grained classification datasets. CoOp and CoCoOp are tuned on ImageNet using 16-shot training data per category. Baseline CLIP, prompt ensemble, TPT and APM do not require training data or annotations. We report top-1 accuracy. ", "Table 4: APM\u2019s computational analysis: TTT for 20 iterations on APM. Baseline is CLIP VIT-B/16 which is used as a teacher in [90]. $M_{m e a s}(G B)$ , $G F l o p s_{m e a s}(G B)$ are tmeasured stats. $M_{i}(G B)$ , $G F l o p s_{i}(G B)$ are idealistic estimates. (t): $t^{t h}$ ttt iteration, (s): student, (u): teacher, $(\\mathrm{s}\\mathrm{+}\\mathrm{u})$ : portion of memory/flops consumed by student/teacher respectively. Note that APM is a $25M$ net. "], "table_footnote": [], "page_idx": 5}, {"type": "image", "img_path": "7Ye12RLZ4P/tmp/396ae6e022de1a5bc7342e63aba68d6942324b67b7d08799ae7c57e1924d5b0a.jpg", "img_caption": ["Figure 2: APM\u2019s analysis with variable number of patches: (left) Gflops of CLIP VIT-B/16 and APM as a function of number of processed patches. (right) Feed-forward time vs number of patches. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "APM does patch-based processing: In Fig 2, we estimate GFlops/time to process a $224\\times224$ image using CLIP VIT-B/16 & APM. CLIP VIT-B/16 consumes 20.5 Gflops. On the other hand, APM takes only 10 Gflops in total, in part due to lesser parameters. APM\u2019s effectiveness comes from processing as low as 7 patches at the same time: it occupies lesser memory but takes more time (i.e. 1.5 seconds). $1.5s e c$ is still lower than VIT-B/16\u2019s $2.2s e c$ . The extreme lies when all patches are processed: Inference time in APM goes down to 0.002 seconds compared to VIT-B/16\u2019s 2.2 seconds, thereby indicating its effectiveness. Note that VIT-B/16 can\u2019t do this patch-based processing. ", "page_idx": 5}, {"type": "text", "text": "APM\u2019s computational effectiveness stems from this unique ability to overfti on a single test sample\u2019s embedding which was distilled only at $t=1$ . This merits a deeper investigation. ", "page_idx": 5}, {"type": "image", "img_path": "7Ye12RLZ4P/tmp/10db109554fefd6f2364640662528ad7de698743fde4bac57bb9d5898410244b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 3: Overfitting on a single distilled token representation leads to islands of agreement[34]: APM is overfti on a test-sample\u2019s representation distilled from a teacher. We plot t-sne clustering of output features over 250ttt iterations. $L_{2}$ loss between predicted features and distilled sample falls from 1e-3 to 1e-12. Moving left to right shows that wholes break into smaller parts. ", "page_idx": 5}, {"type": "text", "text": "APM can learn on a single sample: In Fig 3, we feed-forward a single image through our APM and perform 250 TTT iterations5. We only overfti on one distilled test-sample embedding and plot the 2D t-sne clustering of predicted APM. It can be seen that the elements of the scene have gradually started to cluster. This suggests that APM solves an inverse problem: given a single test-sample embedding, what features in the image led to its formation? Over several TTT iterations, APM\u2019s features become representative enough to explain different parts of a scene[34]. Therefore, the same-net could be made to move up-down the part-whole hierarchy[33], although it requires TTT-iterations for now. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Next, we explore if data augmentation improves APM\u2019s performance. We perform TTT iterations on CIFAR-10\u2019s test set and find that data augmentation drops APM\u2019s performance from 98.6 to 76.7. This quantitatively demonstrates that APM works best when it does one-sample overfitting. It now aligns with the earlier qualitative experiment: over TTT-iterations, the network is learning to cluster the elements in the scene3. Data augmentation distorts the sample and makes it difficult for APM\u2019s predicted features to agree on a stable, relaxed-representation that explains the scene[47, 30] ", "page_idx": 6}, {"type": "text", "text": "Till now, APM\u2019s operation has involved random-initialization of weights for every test-sample and performing test-time-training. There is another mode that it can be made to operate in. ", "page_idx": 6}, {"type": "text", "text": "5 APM Training (Qualitative Analysis) ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "APM can also scale up and do learning on a batch of samples (for e.g., COCO images[55]) distilled from a teacher[36]. This requires introducing several new mechanisms. Note that this section is meant to qualitatively demonstrate how scaling up APM can improve the net\u2019s interpretability, and help seed future research. Quantitative experiments beyond test-time-training remain out of the scope of this paper. APM\u2019s training follows a standard setup in self-supervised-learning[7]. We have provided the full algorithm for SSL-training/test-time-training in the AppendixC. During inference, APM takes any 2D image $x_{k}$ and predicts its RGB reconstruction $R G B k_{\\l}$ /higher dimensional features $f_{k}$ . The net then begins to demonstrate several interesting properties, which we will discuss next. ", "page_idx": 6}, {"type": "text", "text": "5.1 APM can do RGB reconstruction for any 2D input. ", "text_level": 1, "page_idx": 6}, {"type": "image", "img_path": "7Ye12RLZ4P/tmp/7f374ff6fad5cfe70e24849ae6df68d86a96addeb2343d72da55dc30e950e243.jpg", "img_caption": ["Figure 4: RGB Decoding in APM: Input trigger column $T_{i j}$ is concatenated with predicted feature $f_{i j}$ and fed to downstream RGB head. This decodes RGB logit at location $(\\mathrm{i,j})$ for any 2D input $x_{k}$ . (ii) Input $x_{k}$ sampled from Coco-val set. $R G B_{o u t}$ : reconstructed RGB, $f_{o u t}$ : Predicted feature grid. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Given a sample $x_{k}$ , APM can reconstruct its RGB. In Fig4, we achieve this by estimating $f_{r g b,i j}=$ $(T_{i j}|f_{i j})$ . This skip-connection from trigger column $T_{i j}$ to output feature has a subtle reason: consider a white dog and brown dog. The predicted object-level feature for both will be almost identical[34, 2]. However, $T_{i j}$ is different for both since it contains lower patch-level features[34]. Therefore, this helps us break symmetry. Without this skip-connection[25], the net fails to predict RGB. The network is trained to reconstruct RGB for a batch of images, $\\begin{array}{r}{L_{r g b}=\\sum_{i=1}^{N}\\sum_{j=1}^{h*\\bar{w}}L_{2}(p_{j}^{\\prime},p_{j})}\\end{array}$ , where $p_{j}/p_{j}^{'}$ are ground truth/predicted RGB-logits respectively. ", "page_idx": 6}, {"type": "text", "text": "5.2 APM is asynchronous yet encodes semantic-awareness in the net. ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Given a sample $x_{k}$ , APM can directly learn to mimic the entire last layer feature-grid which a teacher model would have generated. We enforce this by a $\\begin{array}{r}{L_{g r i d}=\\sum_{i=1}^{N}\\sum_{j=1}^{h*w}L_{2}(f_{j}^{\\prime},f_{g r i d})}\\end{array}$ . In Fig 5(ii) we estimate the error map between the features predicted by our APM and Dinov2[73]. The error map is mostly black which shows that it closely approximates Dinov2\u2019s grid6. Note that APM does patch-based asynchronous processing whereas DINOv2 relies on parallel perception. Finally, Fig 5(iii) shows a simple feed-forward of a CIFAR-10 sample through APM. We can see semantically-aware features. Note that this is a single feed-forward through APM[34]. Predicting output feature grid allows the net to encode dark knowledge, i.e. the knowledge of both correct and incorrect classes[35]. This is better than just mimicking one hot vector of a target class since it manages to encode lower probabilities of wrong classes also. ", "page_idx": 6}, {"type": "image", "img_path": "7Ye12RLZ4P/tmp/6e216a56c22381d2d9052a4d427fa22484e2ab30daf85cd8b572b264f36091b8.jpg", "img_caption": ["Figure 5: APM feature Analysis: (i) TTT iterations on an input image leads to semantically aware clustering. top: 2D t-sNE. bottom: 3D t-sNE. [70, 34]. (ii) APM is trained via self-supervision using DINOv2-Teacher. (from left) Input, Dinov2 grid, APM grid. APM\u2019s grid closely approximates Dinov2 grid evident from black regions in error map. Note that APM does asynchronous patch-based processing whereas Dinov2 does parallel perception. (iii) Cifar-10 samples feed-forwarded through SSL-trained APM yields features of significant semantic quality.[34] "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "5.3 APM is a step towards validating GLOM\u2019s insight: input percept is a field[41]. ", "text_level": 1, "page_idx": 7}, {"type": "image", "img_path": "7Ye12RLZ4P/tmp/42bc110fe00ffd342974e62e604e856e4100a73b3e4c32b277af96c7971ad925.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 6: APM is a step towards validating GLOM\u2019s insight [34]: input percept is a field. An interpolation between any two images in the wild. This field arises in APM\u2019s MLP consisting of $^{5}$ layers. Trigger column $T$ acts as a key which retrieves an image from the APM\u2019s memory. T resides in a continuous embedding space, not discrete addressing space. ", "page_idx": 7}, {"type": "text", "text": "In Fig 6, we show that APM can interpolate between any two images in the wild. We choose two images $I_{1}$ and $I_{2}$ . These images are then funneled through the trigger column $T$ and yield two vectors $v_{1}$ and, $v_{2}$ respectively. Next, we generate n intermediate latents separated by an equal linear distance by $\\begin{array}{r}{v_{j}=v_{1}+\\frac{v_{2}-v_{1}}{n}}\\end{array}$ . Each latent then brings into existence its own set of location-aware columns and decodes an image from the MLP. Such an interpolation has been previously observed in other models [22]. APM now functions as a new form of addressing mechanism: the trigger column T acts a key. Copying T across locations yields image-specific queries[34]. Values are synapses triggered in the MLP. RGB decoding happens in the output head. Hence, such continuous keys and queries exist outside the net[34]. ", "page_idx": 7}, {"type": "text", "text": "Classically, auto-regression has unrolled a shared-decoder over time[105]. In contrast, APM holds the whole sequence $I$ in $T$ , and directly hops onto a space/time-step[13] by querying the MLP with a location-column $T_{i j}$ . Note that $T_{i j}$ is generated by unfolding $T$ . Recurrence/feedback-loops are compensated for by a form of feature-expression[34]. This is a step towards validating GLOM\u2019s insight, i.e. input-percept is a field[34] and one can now interpolate in it (gestalt psychology). Furthermore, the trigger column $T$ resides in a continous embedding space, and not discrete hardware locations(classical AI)[34]. Therefore, APM tries to integrate insights from both fields. ", "page_idx": 7}, {"type": "text", "text": "6 Ablations on APM ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The experiments on TTT had relied on a curious ability of APM: it could simply overfit on a test-sample\u2019s distilled representation at $t=1$ . This merits further investigation: ", "page_idx": 7}, {"type": "text", "text": "Effect of one-sample test-time training: In Table 5a, we investigate whether existing networks are capable of one-sample test-time-training. We employ a randomly-initialized network to overfit on a distilled test sample\u2019s token obtained in the first TTT iteration [33]. Standard MLP achieves low accuracies of 9.0 and 3.8 on CIFAR-10 and CIFAR-100, respectively. Notably, an 11.4M parameter ResNet18 outperforms the larger ResNet34 with 21.5M parameters. A reason might be that too many parameters in ResNet34 gives it too many degrees of freedom[37]: it finds it hard to overfit on one ", "page_idx": 7}, {"type": "text", "text": "Table 5: Ablations on APM. All nets except CLIP VIT-L/14 use random weights b) $T_{c}$ : trigger column contains convolutions. $T_{v i t}$ : Trigger column contains a routed VIT representation. C-10: CIFAR-10, C-100: CIFAR-100. Accuracy is reported. ", "page_idx": 8}, {"type": "table", "img_path": "7Ye12RLZ4P/tmp/ba1fc110f60bb5175f8b9077aefaa026727800da47b85ffa4f32e1d96834cef9.jpg", "table_caption": ["(a) Ablation to evaluate abilities of existing nets to learn (b) Ablations of our APM on $C{-}10.I_{x y}$ means that from a single sample[25, 83]. local patch was injected into the column $T_{c}$ . "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "sample. Our APM demonstrates strong performance on both CIFAR-10 and CIFAR-100, surpassing the CLIP VIT-L/14 baseline. ", "page_idx": 8}, {"type": "text", "text": "Effect of various losses on APM: We analyze each row in Table 5b. Initially, only the CLS token from the teacher was distilled into our network, resulting in a yield of $94.2\\%$ . When we added RGB reconstruction loss for input, accuracy dropped to 91.0, attributed to the difficulty of breaking RGB symmetry [26]. Subsequently, mimicking the entire feature grid and CLS token from the teacher increased accuracy to 96.1. Adding both $L_{r g b}$ and $L_{g r i d}$ further improved performance to 96.5. $L_{g r i d}$ here refers to the last-layer of the teacher. Notably, while simple RGB reduction decreased performance (91.0), a combination of $L_{g r i d}$ and $L_{r g b}$ enhanced our network, as lower RGB and higher object features complement each other [34]. Injecting local patch information $I_{x y}$ into the trigger column improved performance to 96.8. Finally, routing output tokens from a single VIT layer into the trigger column $T_{v i t}$ strengthened the column and improved performance. ", "page_idx": 8}, {"type": "text", "text": "Effect of increasing number of convolution in T: Increasing number of convolutional kernels from 1 to 3 improves from 96.08 to 97.67. ", "page_idx": 8}, {"type": "text", "text": "These ablations reveal: 1) APM can do one-sample overfitting for a test sample, 2) It helps to have both local patch and a strong image representation in the trigger column T, and 3) Increasing the levels of part-whole supervision strengthens the net. ", "page_idx": 8}, {"type": "text", "text": "7 Related Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Prompting Approaches: Prompting is a mechanism to adapt a foundational-model to a downstream task in a zero-shot manner[63]. However, prompting typically requires well-designed hand-crafted prompts. Prompt-tuning methods consist of learnable prompts which enable a parameter-efficient approach to fine-tune a foundational-model. CoOp[113] applied prompt-tuning to CLIP. However, CoOp[113] is sensitive to OOD-data, which CoCoOp[112] in turn compensates for by conditioning the prompts on model inputs. Similarly, TPT[90] optimizes a prompt to encourage consistent predictions across multiple augmented views of the same test sample, and uses confidence-selection to fliter out noisy-predictions. Note that TPT[90] performs prompt-tuning over ViT-B/16 but requires feed-forward through all the model-layers for every iteration. ", "page_idx": 8}, {"type": "text", "text": "Test-Time-Optimization: Introduces the notion where a model adjusts its decision boundary dynamically during testing, for eg, improving robustness to distribution shifts. Test-time training (TTT) generally adds a self-supervised multi-task branch, and performs a SSL-task like rotation, or maskedreconstruction to adapt the network to the test-sample[17]. These approaches typically initialize the net with pre-trained weights, for eg, Imagenet before undergoing ttt-iterations on a downstream corrupted-dataset. An alternate line of work, for eg, TENT[102] proposes to minimize entropy of batch-wise test-samples. However, TENT[102] requires more than one test-sample to converge towards an optimal solution, whereas APM can also operate on one test sample. Another line of work adjusts internal batch-norm-stats of a network[87]. However, this makes the network-architecture inflexible and requires more than one test sample for optimization. Several other papers following the original pioneering-TTT paper[92] have worked on different problem formulations, for eg, assuming access to an entire dataset (e.g. [56, 78, 103, 110, 20, 111, 21]) or a batch (e.g. TENT [102]) of test inputs. ", "page_idx": 8}, {"type": "text", "text": "In contrast, APM evaluates on a each test-sample independently. Inductively, APM does not require any dataset specific pre-training, a pretext task or prompt tuning. Some approaches also use higher-parameterized transformer/diffusion models[78] making the optimization compute-intensive. However in APM, for TTT iteration $t>1$ , feed-forward is done through only 25M parameter APM and not the 149.2M Clip VIT-B/16, resulting in computational efficient test-time-training(Fig 2). APM also inherits zero-shot behaviour from CLIP, which allows it to bypass training a separate dataset-specific linear-probe for downstream TTT. Finally, some works in areas like source-free domain-adaptation do perform TTT on smaller datasets like Cifar-10 etc[102]. APM additionally shows results on Imagenet splits (Tab1,2) and various cross-generalization datasets(Tab3). ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Part-whole hierarchies: The idea of encoding part-whole dynamic-parse-trees as distributed representations in neural nets can be traced back to [38], with recent attempts leading to capsule networks[47, 85]. However, the EM[47]/attention-based[85] routing forces each capsule to represent only one part[70, 68, 34]. This fundamental flaw prevents capsules from scaling-up and generalizing to multiple OOD objects[70]. Recently, GLOM[34] proposed a theoretical system of representing each input pixel as containing a column-vector. These vectors then undergo a routing procedure such that pixels corresponding to same part come to \u2018agree\u2019 with each other.[18] demonstrated GLOM on Cifar splits. APM does not need any routing because it processes location-aware columns independently. APM now reveals an additional perceptual-interpolation property not shown earlier(Fig6). ", "page_idx": 9}, {"type": "text", "text": "Knowledge Distillation: There has been a long history of training data-specific mixture/product of experts and having their ensemble vote towards a prediction, with the key idea to make the experts as different from as each other as possible, and only one expert deciding on the predictions of a particular input sample[51]. This is better than having all experts give equal opinion on a sample[39]. Other methods attempted to \u2018gather\u2019 their collective knowledge to a single model for edge-device deployment[36]. Recently, knowledge has been transferred from a larger teacher model to smaller ones[6], for eg, a foundational model[94, 80]. The student often retains/becomes-better than its teacher[1]. In practice, the weights of teachers are fluctuated slowly (EMA) as compared to the student[93]. This mechanism then helps realize Kahnman\u2019s theory of slow-fast thinking[52]. ", "page_idx": 9}, {"type": "text", "text": "Typically, distillation requires boltzmann-matching[5] predicted distribution between students and teachers, with the distribution\u2019s sharpness being governed by a temperature parameter[45]. In contrast, APM directly mimics the entire last layer feature grid of a teacher via $L_{2}$ norm[99]. Furthermore, APM possesses a novel-inductive bias that can recover semantic-features from a single CLS-token distilled from a teacher[Fig 5]. This validates the intuition that CLS tokens encode useful geometricinformation of a scene after cross-attention of CLS token with patch tokens of an input image[12]. ", "page_idx": 9}, {"type": "text", "text": "Routing Mechanisms: Routing mechanisms involve routing correct object-specific information to correct neurons. [59] proposed slots which perform binding by iterative rounds of self-attention. L\u00f6we et Al[61] proposed 2D complex autoencoder and rotating features[62] where presence of an object is encoded in phase of a neuron and follows the minimum description length principle[44]. In APM, binding is done via the location itself[34]. GroupVIT [108] routes information to multiple group tokens and shows that semantic segmentation emerges with just image-text contrastive supervision. ", "page_idx": 9}, {"type": "text", "text": "8 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We propose APM, inspired from the insights presented in GLOM[34, 43]. APM promises to be an efficient architecture for test-time-training and asynchronous patch-processing[97]. APM shows robustness to extreme distribution shifts[28]. APM demonstrates that MLP\u2019s can be made to semantically cluster a given image. We hope that APM will help inspire further research on simpler weight-sharing, lower-memory, higher-bandwidth baby-friendly7 efficient-nets [33]. We love GLOM shoo much and everyone who keeps helping it. [34]. ", "page_idx": 9}, {"type": "text", "text": "little_dogzilla ", "page_idx": 9}, {"type": "text", "text": "Limitations: In this work, we have mainly-focused on image-classification. Furthermore, APM requires multiple TTT iterations for now, although they might be reduced by exploring pre-training on a source-dataset[33]. APM can work on just 1 sample with randomly-initialized weights. However, it still requires a single CLS token which has been distilled from a teacher pre-trained on a large-scale dataset. Our preliminary-experiments have revealed that APM can still do RGB-reconstruction without a teacher[26], showing potential that APM can be self-sufficient and independent. ", "page_idx": 9}, {"type": "text", "text": "9 Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This research is supported by the Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/ Interior Business Center (DOI/IBC) contract number 140D0423C0074. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, DOI/IBC, or the U.S. Government. We also thank many other people and MLCollective whose support made this work possible. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Shahzad Ahmad, Sukalpa Chanda, and Yogesh S Rawat. Ez-clip: Efficient zeroshot video action recognition. arXiv preprint arXiv:2312.08010, 2023.   \n[2] Shir Amir, Yossi Gandelsman, Shai Bagon, and Tali Dekel. Deep vit features as dense visual descriptors. arXiv preprint arXiv:2112.05814, 2(3):4, 2021.   \n[3] Yoshua Bengio. The consciousness prior. arXiv preprint arXiv:1709.08568, 2017. [4] Yoshua Bengio, Geoffrey Hinton, Andrew Yao, Dawn Song, Pieter Abbeel, Trevor Darrell, Yuval Noah Harari, Ya-Qin Zhang, Lan Xue, Shai Shalev-Shwartz, et al. Managing extreme ai risks amid rapid progress. Science, 384(6698):842\u2013845, 2024.   \n[5] Ludwig Boltzmann. Lectures on gas theory. Univ of California Press, 2022. [6] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In European conference on computer vision, pages 213\u2013229. Springer, 2020.   \n[7] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 9650\u20139660, 2021. [8] Carlo Collodi, Shirley Goulden, et al. The Adventures of Pinocchio. Lancer Books, 1968.   \n[9] Francis Crick and Graeme Mitchison. The function of dream sleep. Nature, 304(5922): 111\u2013114, 1983.   \n[10] Charles Darwin. On the origin of species, 1859, 2016.   \n[11] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:8780\u20138794, 2021.   \n[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.   \n[13] Albert Einstein. On the electrodynamics of moving bodies. 1905.   \n[14] Albert Einstein. Quantentheorie des einatomigen idealen gases. zweite abhandlung. Albert Einstein: Akademie-Vortr\u00e4ge: Sitzungsberichte der Preu\u00dfischen Akademie der Wissenschaften 1914\u20131932, pages 245\u2013257, 2005.   \n[15] Richard Phillips Feynman. Space-time approach to quantum electrodynamics. In Quantum Electrodynamics, pages 178\u2013198. CRC Press, 2018.   \n[16] Brendan J Frey, Peter Dayan, and Geoffrey E Hinton. A simple algorithm that discovers efficient perceptual codes. Computational and Psychophysical Mechanisms of Visual Coding. Cambridge University Press, Cambridge, UK, 1997.   \n[17] Yossi Gandelsman, Yu Sun, Xinlei Chen, and Alexei Efros. Test-time training with masked autoencoders. Advances in Neural Information Processing Systems, 35:29374\u201329385, 2022.   \n[18] Nicola Garau, Niccol\u00f3 Bisagno, Zeno Sambugaro, and Nicola Conci. Interpretable part-whole hierarchies and conceptual-semantic relationships in neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13689\u201313698, 2022.   \n[19] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 249\u2013256. JMLR Workshop and Conference Proceedings, 2010.   \n[20] Taesik Gong, Jongheon Jeong, Taewon Kim, Yewon Kim, Jinwoo Shin, and Sung-Ju Lee. Note: Robust continual test-time adaptation against temporal correlation. Advances in Neural Information Processing Systems, 35:27253\u201327266, 2022.   \n[21] Taesik Gong, Yewon Kim, Taeckyung Lee, Sorn Chottananurak, and Sung-Ju Lee. Sotta: Robust test-time adaptation on noisy data streams. Advances in Neural Information Processing Systems, 36, 2024.   \n[22] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139\u2013144, 2020.   \n[23] David Ha. Announcing neurips preschool track. https://twitter.com/hardmaru/ status/1779391168377430309, 2024. Accessed: 2024-09-23.   \n[24] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pages 1026\u20131034, 2015.   \n[25] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.   \n[26] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16000\u201316009, 2022.   \n[27] Will Douglas Heaven. Geoffrey hinton tells us why he\u2019s now scared of the tech he helped build. MIT Technology Review, 2:2023, 2023.   \n[28] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. arXiv preprint arXiv:1903.12261, 2019.   \n[29] Peter Robin Hiesinger. The self-assembling brain: how neural networks grow smarter. 2021.   \n[30] G Hinton. Using relaxation to find a puppet. In Proceedings of the 2nd Summer Conference on Artificial Intelligence and Simulation of Behaviour, pages 148\u2013157, 1976.   \n[31] Geoffrey Hinton. Some demonstrations of the effects of structural descriptions in mental imagery. Cognitive Science, 3(3):231\u2013250, 1979.   \n[32] Geoffrey Hinton. Where do features come from? Cognitive science, 38(6):1078\u20131101, 2014.   \n[33] Geoffrey Hinton. The forward-forward algorithm: Some preliminary investigations. arXiv preprint arXiv:2212.13345, 2022.   \n[34] Geoffrey Hinton. How to represent part-whole hierarchies in a neural network. Neural Computation, 35(3):413\u2013452, 2023.   \n[35] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Dark knowledge. Presented as the keynote in BayLearn, 2(2), 2014.   \n[36] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.   \n[37] Geoffrey E Hinton. The horizontal\u2014vertical delusion. Perception, 16(5):677\u2013680, 1987.   \n[38] Geoffrey E Hinton. Mapping part-whole hierarchies into connectionist networks. Artificial Intelligence, 46(1-2):47\u201375, 1990.   \n[39] Geoffrey E Hinton. Training products of experts by minimizing contrastive divergence. Neural computation, 14(8):1771\u20131800, 2002.   \n[40] Geoffrey E Hinton. To recognize shapes, first learn to generate images. Progress in brain research, 165:535\u2013547, 2007.   \n[41] Geoffrey E Hinton and James A Anderson. Parallel models of associative memory: updated edition. Psychology press, 2014.   \n[42] Geoffrey E Hinton and Zoubin Ghahramani. Generative models for discovering sparse distributed representations. Philosophical Transactions of the Royal Society of London. Series B: Biological Sciences, 352(1358):1177\u20131190, 1997.   \n[43] Geoffrey E Hinton and James McClelland. Learning representations by recirculation. In Neural information processing systems, 1987.   \n[44] Geoffrey E Hinton and Drew Van Camp. Keeping the neural networks simple by minimizing the description length of the weights. In Proceedings of the sixth annual conference on Computational learning theory, pages 5\u201313, 1993.   \n[45] Geoffrey E Hinton, Terrence J Sejnowski, and David H Ackley. Boltzmann machines: Constraint satisfaction networks that learn. Carnegie-Mellon University, Department of Computer Science Pittsburgh, PA, 1984.   \n[46] Geoffrey E Hinton, Peter Dayan, Brendan J Frey, and Radford M Neal. The\" wake-sleep\" algorithm for unsupervised neural networks. Science, 268(5214):1158\u20131161, 1995.   \n[47] Geoffrey E Hinton, Sara Sabour, and Nicholas Frosst. Matrix capsules with em routing. In International conference on learning representations, 2018.   \n[48] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840\u20136851, 2020.   \n[49] John J Hopfield. Neural networks and physical systems with emergent collective computational abilities. Proceedings of the national academy of sciences, 79(8):2554\u20132558, 1982.   \n[50] Edwin Hubble. The evolution of the universe. Proceedings of the National Academy of Sciences, 15(3):168\u2013173, 1929.   \n[51] Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. Adaptive mixtures of local experts. Neural computation, 3(1):79\u201387, 1991.   \n[52] Daniel Kahneman. Thinking fast and slow. Farrar, Strauss and Giroux, 2011.   \n[53] Alex Krizhevsky. One weird trick for parallelizing convolutional neural networks. arXiv preprint arXiv:1404.5997, 2014.   \n[54] Chen-Hsuan Lin, Wei-Chiu Ma, Antonio Torralba, and Simon Lucey. Barf: Bundle-adjusting neural radiance fields. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5741\u20135751, 2021.   \n[55] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740\u2013755. Springer, 2014.   \n[56] Yuejiang Liu, Parth Kothari, Bastien Van Delft, Baptiste Bellot-Gurlet, Taylor Mordan, and Alexandre Alahi. Ttt++: When does self-supervised test-time training fail or thrive? Advances in Neural Information Processing Systems, 34:21808\u201321820, 2021.   \n[57] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pages 10012\u201310022, 2021.   \n[58] Ziming Liu, Yixuan Wang, Sachin Vaidya, Fabian Ruehle, James Halverson, Marin Soljac\u02c7ic\u00b4, Thomas Y Hou, and Max Tegmark. Kan: Kolmogorov-arnold networks. arXiv preprint arXiv:2404.19756, 2024.   \n[59] Francesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold, Jakob Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf. Object-centric learning with slot attention. Advances in Neural Information Processing Systems, 33:11525\u201311538, 2020.   \n[60] Sindy L\u00f6we, Peter O\u2019Connor, and Bastiaan Veeling. Putting an end to end-to-end: Gradientisolated learning of representations. Advances in neural information processing systems, 32, 2019.   \n[61] Sindy L\u00f6we, Phillip Lippe, Maja Rudolph, and Max Welling. Complex-valued autoencoders for object discovery. arXiv preprint arXiv:2204.02075, 2022.   \n[62] Sindy L\u00f6we, Phillip Lippe, Francesco Locatello, and Max Welling. Rotating features for object discovery. Advances in Neural Information Processing Systems, 36, 2024.   \n[63] Neelu Madan, Andreas M\u00f8gelmose, Rajat Modi, Yogesh S Rawat, and Thomas B Moeslund. Foundation models for video understanding: A survey. arXiv preprint arXiv:2405.03770, 2024.   \n[64] Sadhika Malladi, Tianyu Gao, Eshaan Nichani, Alex Damian, Jason D Lee, Danqi Chen, and Sanjeev Arora. Fine-tuning language models with just forward passes. Advances in Neural Information Processing Systems, 36:53038\u201353075, 2023.   \n[65] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99\u2013106, 2021.   \n[66] Marvin L Minsky and Seymour A Papert. Perceptrons: expanded edition, 1988.   \n[67] Rajat Modi. Detection with r-cnn, 2020. URL https://rajatmodi62.github.io/2020/ 05/17/detection-rcnn/.   \n[68] Rajat Modi, Naman Garg, and Harish Parthasarathy. Steganography using wavelets with statistical performance analysis. In 2017 1st International Conference on Electronics, Materials Engineering and Nano-Technology (IEMENTech), pages 1\u20138. IEEE, 2017.   \n[69] Rajat Modi, Aayush Jung Rana, Akash Kumar, Praveen Tirupattur, Shruti Vyas, Yogesh Singh Rawat, and Mubarak Shah. Video action detection: Analysing limitations and challenges. arXiv preprint arXiv:2204.07892, 2022.   \n[70] Rajat Modi, Vibhav Vineet, and Yogesh S Rawat. On occlusions in video action detection: Benchmark datasets and training recipes. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023.   \n[71] Alexander Mordvintsev, Christopher Olah, and Mike Tyka. Inceptionism: Going deeper into neural networks. Google research blog, 20(14):5, 2015.   \n[72] J von Neumann. Theory of self-reproducing automata. Edited by Arthur W. Burks, 1966.   \n[73] Maxime Oquab, Timoth\u00e9e Darcet, Th\u00e9o Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023.   \n[74] Roger Penrose. The emperor\u2019s new mind: Concerning computers, minds, and the laws of physics. Oxford Paperbacks, 1999.   \n[75] Roger Penrose. Before the big bang: an outrageous new perspective and its implications for particle physics. In Proceedings of EPAC, pages 2759\u20132763, 2006.   \n[76] Roger Penrose, Stuart R Hameroff, and Subhash Kak. Consciousness and the universe: Quantum physics, evolution, brain & mind. Cosmology science Publishers., 2017.   \n[77] Stefano Piraino, Ferdinando Boero, Brigitte Aeschbach, and Volker Schmid. Reversing the life cycle: medusae transforming into polyps and cell transdifferentiation in turritopsis nutricula (cnidaria, hydrozoa). The Biological Bulletin, 190(3):302\u2013312, 1996.   \n[78] Mihir Prabhudesai, Tsung-Wei Ke, Alexander Cong Li, Deepak Pathak, and Katerina Fragkiadaki. Diffusion-tta: Test-time adaptation of discriminative models via generative feedback. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[79] AC Bhaktivedanta Swami Prabhupada and Bhaktivedanta Swami. Bhagavad-Gita as it is, Chapter 8, Verse 18-19. Bhaktivedanta Book Trust Los Angeles, 1972.   \n[80] Biqing Qi, Xinquan Chen, Junqi Gao, Dong Li, Jianxing Liu, Ligang Wu, and Bowen Zhou. Interactive continual learning: Fast and slow thinking. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12882\u201312892, 2024.   \n[81] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.   \n[82] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684\u201310695, 2022.   \n[83] Frank Rosenblatt. The perceptron: a probabilistic model for information storage and organization in the brain. Psychological review, 65(6):386, 1958.   \n[84] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by back-propagating errors. nature, 323(6088):533\u2013536, 1986.   \n[85] Sara Sabour, Nicholas Frosst, and Geoffrey E Hinton. Dynamic routing between capsules. Advances in neural information processing systems, 30, 2017.   \n[86] Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo. Are emergent abilities of large language models a mirage? Advances in Neural Information Processing Systems, 36, 2024.   \n[87] Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bringmann, Wieland Brendel, and Matthias Bethge. Improving robustness against common corruptions by covariate shift adaptation. Advances in neural information processing systems, 33:11539\u201311551, 2020.   \n[88] Terrence J Sejnowski, Paul K Kienker, and Geoffrey E Hinton. Learning symmetry groups with hidden units: Beyond the perceptron. Physica D: Nonlinear Phenomena, 22(1-3):260\u2013275, 1986.   \n[89] Claude Elwood Shannon. A mathematical theory of communication. The Bell system technical journal, 27(3):379\u2013423, 1948.   \n[90] Manli Shu, Weili Nie, De-An Huang, Zhiding Yu, Tom Goldstein, Anima Anandkumar, and Chaowei Xiao. Test-time prompt tuning for zero-shot generalization in vision-language models. Advances in Neural Information Processing Systems, 35:14274\u201314289, 2022.   \n[91] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time training with self-supervision for generalization under distribution shifts. In International conference on machine learning, pages 9229\u20139248. PMLR, 2020.   \n[92] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time training with self-supervision for generalization under distribution shifts. In International conference on machine learning, pages 9229\u20139248. PMLR, 2020.   \n[93] Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems, 30, 2017.   \n[94] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.   \n[95] The FAIR Computer Vision Team. fvcore: A light-weight core library for common computer vision functionality. https://github.com/facebookresearch/fvcore, 2024.   \n[96] Alan M Turing. Computing machinery and intelligence. Springer, 2009.   \n[97] Alan Mathison Turing. The chemical basis of morphogenesis. Bulletin of mathematical biology, 52:153\u2013197, 1990.   \n[98] Alan Mathison Turing et al. On computable numbers, with an application to the entscheidungsproblem. J. of Math, 58(345-363):5, 1936.   \n[99] Haithem Turki, Jason Y Zhang, Francesco Ferroni, and Deva Ramanan. Suds: Scalable urban dynamic scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12375\u201312385, 2023.   \n[100] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \n[101] John Von Neumann, Arthur Walter Burks, et al. Theory of self-reproducing automata. 1966.   \n[102] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. arXiv preprint arXiv:2006.10726, 2020.   \n[103] Qin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai. Continual test-time domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7201\u20137211, 2022.   \n[104] Max Welling, Andriy Mnih, and Geoffrey E Hinton. Wormholes improve contrastive divergence. Advances in Neural Information Processing Systems, 16, 2003.   \n[105] Paul J Werbos. Backpropagation through time: what it does and how to do it. Proceedings of the IEEE, 78(10):1550\u20131560, 1990.   \n[106] Stephen Wolfram et al. A new kind of science, volume 5. Wolfram media Champaign, IL, 2002.   \n[107] Junfeng Wu, Qihao Liu, Yi Jiang, Song Bai, Alan Yuille, and Xiang Bai. In defense of online models for video instance segmentation. In European Conference on Computer Vision, pages 588\u2013605. Springer, 2022.   \n[108] Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon, Thomas Breuel, Jan Kautz, and Xiaolong Wang. Groupvit: Semantic segmentation emerges from text supervision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18134\u201318144, 2022.   \n[109] Hong-Xing Yu, Leonidas J Guibas, and Jiajun Wu. Unsupervised discovery of object radiance fields. arXiv preprint arXiv:2107.07905, 2021.   \n[110] Longhui Yuan, Binhui Xie, and Shuang Li. Robust test-time adaptation in dynamic scenarios. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15922\u201315932, 2023.   \n[111] Longhui Yuan, Binhui Xie, and Shuang Li. Robust test-time adaptation in dynamic scenarios. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15922\u201315932, 2023.   \n[112] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Conditional prompt learning for vision-language models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16816\u201316825, 2022.   \n[113] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-language models. International Journal of Computer Vision, 130(9):2337\u20132348, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Appendix: Asynchronous Perception Machine for Efficient Test-Time Training ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "A Broader Impact ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "There are two main ideas that led us to Asynchronous Perception Machine[45, 34]. The first idea is that instead of thinking of features as a cuboidal feature grid[32], one can think of it as a column vector at each location[34, 70]. This helped us learn one to one mapping between the input rgb patch and the vector at a particular location. It led to the net being able to process one patch at a time. ", "page_idx": 16}, {"type": "text", "text": "The second idea is this notion of collapsing information into a single starting point. Our previous understanding was that this \u2018collapse\u2019 leads to degeneracy[45]. In this paper, the information can be recovered from the starting point by copying it many times and breaking symmetry with positional encoding. By asking the right questions at the right place at the right time, a net can thus learn to express correct features[97, 34]. Although the information can \u2018degenerate\u2019 to a single starting point, the net can still learn thanks to the strong positional-prior injected by such periodic-encodings[100]. ", "page_idx": 16}, {"type": "text", "text": "This notion of combining information coming from many locations appears to have connections to Kolmogorov-Arnold superposition theorem[34, 58]. The single convolutional fliter in APM might be considered an encoder and five layered MLP as a decoder. Convolution fliter can also be considered to be a tape[98] on which symbols are written, processed by a learning machine(MLP)[98] which speaks the correct answers. We have observed that MLP\u2019s have this ability to cluster elements in any image. This seems exciting for dense visual tasks with potential for new insights. Finally, we are led to believe that the networks could be made even smaller with higher bandwidth [34, 33]. Of course, it shall mean defining a metric called bandwidth, where the performance of the learning machine shall be measured by a three-tuple of <parameters, accuracy, bandwidth>[33, 2]. ", "page_idx": 16}, {"type": "text", "text": "Knowledge-transfer can then be a consequence of sharing folded-embeddings which could grow to form dynamically connected-networks rather than mimicking unfolded-outputs among multiple neural-nets. Knowledge transfer between nets of same structure is as simple as copying weights from one to another thereby making them immortal. There already exist approaches which share trees or share knowledge between different neural nets, for e.g. dropout. A higher bandwidth way might involve exchanging folded network-vectors in higher dimensional space, which then reduces to the setting of distributed federated-learning that could run in low-cost hardware8. ", "page_idx": 16}, {"type": "text", "text": "A Turing machine gives us a sense of closure[74]: the input tape is shared for both input and output to the machine. However, existing neural nets are mostly bottom-up, with feature expression limited to last layer of the net. In contrast, neurons of a boltzmann machine were clamped, to allow datavectors to be presented to the net via the environment as well as express the generated perceptual codes[16] sampled from the net itself on a same set of neurons[40, 42]. Modelling this closure presents problems for training a neural net: for backpropagation cant work in circles of synapstic connections, even though there is mounting evidence of such connections in the brain[33]. Modelling top-down influences in GLOM then has to resort to leveraging top-down influence in a previous time-step to influence lower embeddings estimated in the current time step via an auto-encoder or a neural field. A key challenge then remains to propogate top-down influences in the current step, for eg, via a recirculation-procedure[43] which could be trained via backprop or some other learning algorithm we are yet to discover[33] and at the same time entirely avoid the representational/mode collapse which comes from such local forms of learning[22, 34, 70]. ", "page_idx": 16}, {"type": "text", "text": "B Future Work ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "APM offers a fresh perspective towards machine-perception: i.e. patches can now be lazily-processed one-by-one asynchronously[34]. It shall be very-exciting to see APM\u2019s potential on dense-tasks, video-understanding[69] and alternative testing-schemes i.e. few-shot scenarios or testing with a \u2018batch-of-samples\u2019. Another direction might involve making test-time-training faster/more-efficient by exploring alternate zeroth-order optimization techniques[64, 33]. Finally, APM contains a local-field which emerges as a consequence of folding-unfolding: this might have potential-applications to generative[22, 48, 11]/dreaming[71]/sleeping-machines9[9]. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "C Pseudo-code for APM\u2019s operation. ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In Algorithm1, we have inflated the entire pseudo-code to train APM beyond the applications of test-time-training. The idea is that given an input image $x_{k}$ APM can learn to predict its entire feature grid $f_{k}$ and its rgb logits $R G B_{k}$ . First, the net inputs an image $x_{k}$ . $x_{k}$ is then routed to a trigger column $T$ . T then brings several columns $T_{i j}$ into existence. Each of these columns is fired into the MLP to yield location-aware features $f_{i j}$ . The loss is collected for all locations and backpropagation then estimates the gradients required to update the parameters of the APM. In this entire process, there were no labels being used. The feature grids could have been any layer of a net like DINOv2. APM thus manages to learn a perception field within itself [34]. It can be then frozen, and used as a computational equivalent of any feature-extractor for a downstream-perception task. ", "page_idx": 17}, {"type": "text", "text": "Algorithm 1: Training APM in a self-supervised manner using a teacher U ", "text_level": 1, "page_idx": 17}, {"type": "table", "img_path": "7Ye12RLZ4P/tmp/52694b4cec8c7dab0ac0068cdebe42651cb2da6c9ac1f760005d4588e768a209.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Furthermore, we present the pseudo-code of APM for test-time-training in Algorithm2. First, the textual encoder of the teacher is used for estimating representations of each ground truth class. Then over multiple ttt-iterations, the predicted feature of APM, a.ka. $f$ is refined via statistical running-average10. During each learning iteration, the trigger column $T$ is undergoing phases of folding-unfolding. Finally $f$ is being used to perform zero-shot-classification with prior-computed representations $R_{g t}$ . ", "page_idx": 17}, {"type": "text", "text": "A question may be posed on how to decide the optimal number of ttt iterations $t$ to achieve optimal performance. Indeed, one might build additional inductive-bias in a student (aka APM) to estimate when its own fantasies[46]/predicted semantic-features are better than the teacher\u2019s and stop dynamically/recurse until kickoff. Notions on soft decision-making for higher-level cognition are subtly embedded in [101, 3]. ", "page_idx": 17}, {"type": "image", "img_path": "7Ye12RLZ4P/tmp/8cdb0b0810247e27014b8a8e64eeec97010c7b975a434f04de9f54dd43ac2914.jpg", "img_caption": [], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "D Reproducibility Statement ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In order to ensure the reproducibility of our experiments, we have shared the model in supplementary during review process. The code, model weights shall be released post-review. APM can work with a single GPU like pascal in less than 2 GB of memory. It can also parallelize on a cluster containing 2 nodes of 8 A6000 amperes. We have provided details of hyper-parameters used in test-time-training (Tab7), and precise details of each layer of APM(Tab 6). ", "page_idx": 18}, {"type": "text", "text": "E Implementation Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "E.1 Architecture and Hyperparameters ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Architecture: We inflate APM\u2019s architecture in Table6. For the TTT experiments, APM consists of only a single convolutional layer, and 5 MLP layers. Additionally, APM consists of a feature projection head containing a single linear layer, and an optional RGB head. It maybe noted that the number of kernels in the convolutional filter is only 1. This creates a subtle issue: the RGB reconstruction in Fig4 is black and white. This is because a single kernel loses RGB channel information. Put simply, assume a tuple of 3 numbers representing RGB values, $<\\,1,2,3\\,>$ , $<4,1,1>$ . For a convolution operation with a single kernel assuming unit weights, the answer is 6. If this 6 gets injected in the net, it is equally certain that the input was $<1,2,3>$ or $\\mathrm{a}<4,1,1>$ making reconstruction from the RGB head of APM difficult. We found that this symmetry-breaking issue could be noticeably fixed with $n_{k e r n e l s}\\ge n_{c h a n n e l s}$ , where $n_{c h a n n e l s}$ is the number of channels $c$ in the input. Historically, various other rotational/translational/mirror-symmetries have played an important role in designing boltzmann machines[88]. The architecture in Table 6 is then meant to showcase APM\u2019s potential to an extrema: how much can it do even with a single convolutional fliter? ", "page_idx": 18}, {"type": "text", "text": "Hyperparameters: All hyper-parameters utilized for APM during test-time-training are detailed in 7. We leveraged the seed 42 in most of our experiments, and also conducted experiments with multiple seeds. The weight matrices in APM were initilized with from a random distribution with $\\mu=0$ and $\\sigma=0.01$ . All the code has been written in Pytorch version 1.13.0. We also note that performing test-time-training with 16 bit floating point allows us to effectively use recent GPU architectures for eg, Ampere: they contain a larger number of tensor cores in addition to CUDA cores which results in significant speedups during the exprimentation process. Finally, we normalize an input image using standard Imagenet stats, and dont resort to any other form of augmentation, thereby making the pipeline far-simpler. ", "page_idx": 18}, {"type": "table", "img_path": "7Ye12RLZ4P/tmp/b3da4c51e25df9e45ec5733f205aacaf51470b5f49c4dfaa317acbea6594e451.jpg", "table_caption": ["Table 6: APM architecture for TTT: with input dimensions $h,w,c$ and feature dimension $d_{p}$ : dimensionality of positional encoding. $s$ : stride of convolutional filter in encoder, $d_{c}$ : dimension of the CLS token of teacher on which APM learns. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "table", "img_path": "7Ye12RLZ4P/tmp/e9c3d8140e3957255900f47340a67699120765884cf006a2940b3ffcada35afb.jpg", "table_caption": ["Table 7: APM hyperparameters during test-time-training. "], "table_footnote": ["ViT Encoder: During our experiments in test-time-training, APM relies on higher-dimensional CLS token distilled from a teacher trained on a large-scale-dataset, often via contrastive image-text objectives. We showed quantitative results with CLIP, OpenCLIP and qualitative semantic-clusterings with DinoV2. "], "page_idx": 19}, {"type": "text", "text": "CLIP is a zero-shot model from OpenAI which contains a vision encoder, and a textual encoder. The textual encoder tokenises input class names to features. Both image/text encoder project them to common dimensionality, and classification happens by measuring distances in contrastive space, thereby offering a higher degree of freedom, as opposed to training a class-sensitive linear-probe. CLIP VIT-L features an output CLS token of 768 dimensions, while CLIP VIT-H outputs 1024 dimensions both of which have been accommodated in Tab6. DinoV2[73] is a foundational-model trained via SSL-objectives and predicts significant semantically-aware representations, which are widely used in various downstream computer-vision tasks. ", "page_idx": 19}, {"type": "text", "text": "Inductively, both CLIP/Dinov2 rely on VIT, which operates on the principle of parallel attention[100]: image-tokens a.k.a patches can flow along parallel paths among different layers stacked over each other without any loss of spatial-resolution which was also a key-shortcoming of convolutional-nets. Even though the transformers have no bottleneck issue, the attention-operation in each layer still occupies a significant amount of memory[70]. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "E.2 Datasets: ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "To evaluate model\u2019s robustness to distribution shifts, it is necessary to test them on datasets which contain corruptions on a variety of scenarios for eg, fog, rain, snow, etc.. One standard practice has been to take the test set of larger datasets like ImageNet, and create synthetic corruptions to establish appropriate-benchmarks on which the performance of these models can be compared. Alternatively, certain test splits have been manually-curated from-scratch over the internet, for eg, sketches, artisticdrawings etc. Below, we detail some of the splits which were used in this paper for APM\u2019s robustness experiments. ", "page_idx": 20}, {"type": "text", "text": "Cifar-10C: is a test-split consisting of 10000 test-samples of Cifar-10, corrupted with 15 noises, across 5 levels of noise-severities. In this paper, we have shown results on the highest level, a.k.a level 5 owing to the resource-constraints. ", "page_idx": 20}, {"type": "text", "text": "Imagenet-C: ImageNet-C is a dataset split for recognizing objects under distribution shifts, with 1000 classes like original Imagenet. This split contains 15 types of corruptions, with each type containing 5 level of noise severities, aka, the percentage of the image region which is being typically impacted by the corruption. ", "page_idx": 20}, {"type": "text", "text": "ImageNet-V2: is an independent test set containing images sampled from naturally occuring scenarios, with 10000 images of 1000 ImageNet classes. ImageNet-V2 typically consists of 3 splits, with varying levels of difficulties. ", "page_idx": 20}, {"type": "text", "text": "ImageNet-A: refers to a curated test-set containing \"natural occurring adversarial samples\", which were misclassified by the Resnet-50. This particular split contains 7500 images of 200 ImageNet categories. ", "page_idx": 20}, {"type": "text", "text": "ImageNet-R: refers to a novel test-set of several Imagenet categories, which contain artistic renditions.   \nThere are 30000 images in this split spanning across 200 ImageNet categories. ", "page_idx": 20}, {"type": "text", "text": "ImageNet-Sketch: is a challenging test-split which consists of only black-and-white sketches of 1000 ImageNet categories. This split originally consists of 50,000 images in total. ", "page_idx": 20}, {"type": "text", "text": "Typically, methods like CLIP evaluate on these ImageNet using a prompt-ensemble of 80 handcrafted templates. APM results were also shown using this ensemble for fair comparisons. ", "page_idx": 20}, {"type": "text", "text": "F Additional Ablations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Here we perform some additional ablations to understand how varying one of the parameters of APM, while holding others hyper-parameters constant impacts the performance during test-time-training. ", "page_idx": 20}, {"type": "text", "text": "Effect of varying number of ttt iterations: We perform varying number of ttt iterations, evaluate the performance of APM on three seeds, and report the mean and standard deviations. We note that at $t=25$ , APM obtains $49.5\\%$ accuracy with a minimum observed standard-deviation of 0.2. ", "page_idx": 20}, {"type": "text", "text": "Table 8: Ablation with variable ttt iterations on DTD dataset: We pick the best performing 53M param net in tab 7 of the main paper. We show mean/std over 3 runs with seeds 0/7/42. The net settles on the best result of 49.5 and std reduces to 0.2 at $n_{i t e r}=25$ . ", "page_idx": 20}, {"type": "table", "img_path": "7Ye12RLZ4P/tmp/66df6cd14af6dffe58688bffaab390e85413ae21179dc0af39fb857b1c152e47.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Effect of varying number of parameters: In Tab9, we perform an additional ablation: the number of parameters inside APM\u2019s linear layers are gradually changed from 7M to 120M. We perform TTT iterations on the DTD dataset. As can be observed, the top-1 classification-accuracy of APM increases from 47.0 to 49.1, thereby indicating that a $53M$ net was optimal for this particular instance of the problem. Beyond that, we observe a gradual drop in the performance, thereby indicating that the net has started to overfit. In an ideal scenario, we would want that lower number of parameters should yield higher performance. However, this would then require some more fundamental changes which allows the net to achieve higher-bandwidths, which remains a matter for the future work[33]. ", "page_idx": 20}, {"type": "text", "text": "Table 9: Ablation on APM parameter count on DTD dataset: Increasing the number of parameters to 53M improves APM\u2019s performance to 49.1 beyond which it starts to drop. Top 1 classification accuracy is being reported. ", "page_idx": 21}, {"type": "table", "img_path": "7Ye12RLZ4P/tmp/16dd55ea41aa2e6f46c0d6da4cd58c30126333fb2eccff268d68c6736dbfbbf2.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "Effect of removing the teacher from APM: APM can perform test-time-training on a single testsample by relying on a CLS-token distilled from a teacher trained on-scale. This might lend itself to the assumption that APM really requires a teacher in order to learn semantically-aware representations. In this ablation, we remove the teacher entirely and have APM perform RGB-reconstruction on COCO-train set. The $L_{2}$ RGB-reconstruction loss on COCO-val loss then falls to 0.0027. Training took far longer than if last-layer feature-vectors were also distilled from a teacher into APM. ", "page_idx": 21}, {"type": "text", "text": "Higher-dimensional vector-spaces carry more bits[89], but incur significant randomness[5]. Island/vector-distillation speeds-up the learning-process which otherwise might only be informed via RGB-reconstruction and take a lot of time[26, 34]. This helps guide APM\u2019s ship to correct points in the subspace. Progressing from VIT $\\scriptstyle{\\ b->{\\ h}}$ in Tab1, shows APM becomes more competitive thereby validating this intuition. ", "page_idx": 21}, {"type": "text", "text": "G Additional Results ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Results on Cifar-10C: In Tab10, we show additional results on Cifar 10-C dataset widely used in test-time-training [92]. Cifar-10C consists of 15 types of noise corruptions for all the 10,000 samples present in the Cifar-10 dataset. As evident from the table, CLIP is not naturally robust on Cifar-10 and achieves an error rate as high as $24.5\\%$ . This is worse than existing ttt-methods like TTT-Online and UDA-SS. Therefore, CLIP VIT-L is not naturally-robust on Cifar-10c. ", "page_idx": 21}, {"type": "text", "text": "Using it as a teacher, we get the lowest average error rate of $14.8\\%$ , thereby even improving upon the performance of CLIP VIT-L/14. Note that our APM is reinitialized with random weights after everytest sample in contrast to methods like TTT-Online which retain the weights after every ttt-iteration. Inspite of that, we get a lower error rate i.e. $14.8\\%$ than TTT-Online $19.\\bar{1}\\%$ . Another benefit which APM gains is that it can directly use the textual-encoder of the Clip VIT-L/14 teacher to classify on cifar-10 test set: this allows us to bypass the requirement of training a separate dataset-specific linear probe on top of our net. ", "page_idx": 21}, {"type": "table", "img_path": "7Ye12RLZ4P/tmp/5a60e004a97768482fb777d169d0094745e5ade15f6fae0682b372e0ebba4aa2.jpg", "table_caption": ["Table 10: Cifar 10-C results at highest severity level of 5. We report Error Rate. Lower numbers are better. t- model acts as teacher for our APM. TTT was done on test set with randomly initialized weights. APM weights were reinitialized after each TTT iteration to prevent information leakage. Lower is better. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "Results on ImageNet-C: In Tables11,12,13,14, we perform test-time-training on APM for different imagenet splits across increasing levels of noise-severities. We observe that APM continues to obtain competitive performance over it\u2019s teacher. ", "page_idx": 21}, {"type": "text", "text": "H Some helpful analogies ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "APM proposes two technical ideas. 1) The first idea is the proposed column representation $T$ 2) The second idea is the folding-unfolding mechanism. However, there are several deeper non-technical/nonscientific inspirations which motivated the design of APM. We discuss some of those, to help facilitate a deeper-connection and ground our intuitions. ", "page_idx": 21}, {"type": "text", "text": "A biological analogy[34]: Consider how an organism starts its existence from a cell. The cell is copied across different body locations. Each location possesses identical DNA. However, depending on the location, the DNA decides whether to form an eye or nose. We term this process as unfolding, i.e. a cell \u2018expands\u2019 to yield an organism. Next, there is evidence of jellyfish like Turritopsis dohrnii reverting from their fully grown form to younger polyp states [77]. We term this process as folding, i.e. cells of an organism collapse back to the single cell it began from. ", "page_idx": 21}, {"type": "table", "img_path": "7Ye12RLZ4P/tmp/f6dcb60d9caf1e95a5de1fe6f4e2358cc144fe3d53963575fd7fa603d23e9bcb.jpg", "table_caption": ["Table 11: APM\u2019s performance on ImageNet-C, level 1. The first two rows are same as the supplementary materials of [17]. A $\\checkmark$ in P means that method leveraged pre-trained weights on clean variant of train set aka, Image-net and downstream-ttt on corrupted version. OpenCLIP VIT-L/14 is in general more robust. APM can surpass OpenCLIP VIT-L/14. "], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "7Ye12RLZ4P/tmp/0e82b45d280dd81dca7a9259e667884d44c45aba4559e63b00a2917b81defa9d.jpg", "table_caption": ["Table 12: APM\u2019s performance on ImageNet-C, level 2. The first two rows are same as the supplementary materials of [17]. A $\\checkmark$ in $\\mathbf{P}$ means that method leveraged pre-trained weights on clean variant of train set aka, Image-net and downstream-ttt on corrupted version. OpenCLIP VIT-L/14 is in general more robust. APM can surpass OpenCLIP VIT-L/14. "], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "7Ye12RLZ4P/tmp/f4e6019ebe5131e68d4b60d42ce1e7a3919c406dd0402e1e3260883f8875a561.jpg", "table_caption": ["Table 13: APM\u2019s performance on ImageNet-C, level 3. The first two rows are same as the supplementary materials of [17]. A $\\checkmark$ in $\\mathbf{P}$ means that method leveraged pre-trained weights on clean variant of train set aka, Image-net and downstream-ttt on corrupted version. OpenCLIP VIT-L/14 is in general more robust. APM can surpass OpenCLIP VIT-L/14. "], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "7Ye12RLZ4P/tmp/bd18d6d993ab21c1ec5c0d1c9f307a97650db8f5dc9892027af4ee1895f1a5ca.jpg", "table_caption": ["Table 14: APM\u2019s performance on ImageNet-C, level 4. The first two rows are same as the supplementary materials of [17]. A $\\checkmark$ in $\\mathbf{P}$ means that method leveraged pre-trained weights on clean variant of train set aka, Image-net and downstream-ttt on corrupted version. OpenCLIP VIT-L/14 is in general more robust. APM can surpass OpenCLIP VIT-L/14. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "A computational analogy[34]: We now start treating an image $I$ as a digital organism. It starts from some compressed representation $T$ . $T$ unfolds to yield the image $I,\\;I$ then folds back to yield the compressed representation $T$ . Learning proceeds by oscillating between these unfolded and folded phases. At every step, the net is trying to reconstruct image $I$ from $T$ . $T$ is then expected to be a dense vector-space. ", "page_idx": 22}, {"type": "text", "text": "A cellular-automaton analogy[106]: On surface it seems a pretty trivial matter to discuss: a point can expand and yield beautiful patterns which can either be an entire universe in accordance with the theory of big-bang, or can be reproduction of an organism from a singular zygote. But, it is funny: if you start from a point, and unfold it, then all you can get is a sphere. This appears to be true for the behaviour of light, in accordance to Huygens principle11. However, we observe non-spherical objects around us all the time. Turing posited that the symmetry breaking in the sphere must happen somewhere while the organism unfolds: such patterns could then be explained a variety of diffusion based equations[48]. ", "page_idx": 22}, {"type": "image", "img_path": "7Ye12RLZ4P/tmp/ed80e6885246ce525e7fc22a7c1d4c422d3ab43a12cc47e3f2b244c3408db0f7.jpg", "img_caption": ["Figure 7: Cifar 10 islands: Individual part-wholes are clearly observed in APM features. These features are used for downstream classification.We leverage the visualization mechanism by [70]. These islands are not been hand-picked[34] "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "This idea has been explored in cellular automaton: different replication rules of starting point can yield different final patterns12. Scientists then continue to derive different rules which yield different patterns, which is akin to how we were resorting to hand-engineering features in deep-learning for a long time. APM attempts to answer the question: Is it possible to build a learning machine which can start from a point, unfold, and then express correct features at the correct place?. We want to then push the job of rule-learning to what backpropogation does best. We have lost the \"why\" for the knowledge was encoded in the weights of the net, but we seem to have gained the ability of correct features presenting themselves at correct locations. This location-aware-disentanglement procedure thereby represents a step towards solving Arnold\u2019s superposition theoram and Hilbert\u2019s thirteenth problem[34]. However, backpropogation can only approximate solutions and not yet reach exact ones[33], and mathematical formulations are lost into the weights of the neural-net. We then begin to imagine learning machines[97, 98, 96] which can solve a complex problem like cryptography/breaking-a-cipher in two phases 1) relax the system towards an approximate solution [45] 2) have the system spit-out which parts of the solution are uncertain, and brute-force towards the remaining solution. Or, we could make the loss of the learning-machine reach perfectly zero, thereby representing a perfect solution13. Hard problems like recognizing faces are approximately solved as a consequence of a single forward pass through a learning machine. If the loss could be made to reach zero, then we could consider the problem to be perfectly solved. Solvability can then happen in a feed-forward phase, which for practical purposes appears to be polynomial.14 ", "page_idx": 23}, {"type": "image", "img_path": "7Ye12RLZ4P/tmp/0c62e82371e7883aa5ab6951ec105d68712def39d7441a3afced436150acf194.jpg", "img_caption": ["Figure 8: Qualitative Results on COCO Val set: Our APM is trained on COCO-train set, and islands on COCO-val set are visualized[34]. We leverage the visualization mechanism by [70]. Note that these islands are a consequence of single feed-forward through the net and not an iterative routing mechanism as in [18]. These islands are not been hand-picked.[34] "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "Next, we redirect the reader\u2019s attention to neumann\u2019s theory of self-reproducing automaton[72]. His idea of a self-replicating colony was that there is an infinite source of resources a.ka. reservoir which is shared by the automaton which operate at different locations of a colony. The colony uses up the shared resources, does self-replication and in this way converts raw materials/matter into useful intelligent-behaviour. The infinite reservoir of machines he talks about then reduces to the trigger column $T$ in APM: since features are sampled from a same space, they automatically become aware of themselves, thereby making explicit attention unnecessary. This also then is same as how latents have been classically sampled in the generative models[22]. One might argue that multiple automaton although starting their lives at the same point will need to communicate among themselves, as they differ among their configurations at later point in their lifespans15. Fluctuations in $T$ are then akin to mutations. We compensate for this fact by weight-sharing the MLP across different locations in APM. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "A cosmological analogy: In physics, one of the famous theories of the origin of universe has been starting from a single point, and undergoing a continuous expansion[50]. There are alternate theories for eg, Conformal Cyclic Cosmology[75] which hypothesize the universe undergoing periodic cycles of expansion and contraction[79]. Drawing inspiration from these fundamental insights, the trigger column $T$ undergoes these cycles of folding and unfolding during the learning iterations. ", "page_idx": 25}, {"type": "text", "text": "I A new representation: Hinton\u2019s Islands of agreement[70, 34] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "APM is inspired from GLOM\u2019s philosophy. This section explains the principles behind island of agreement in more detail to facilitate an easier understanding. GLOM [34] assumes that each pixel of an input image contains a higher dimensional column vector over it. Therefore, for an image of $h\\times w$ , there is a column vector at every location. Next, all these column vectors undergo some sort of message passing between themselves, for eg, via attention. At the end of these procedure, the idea is that the vectors belonging to identical objects should start pointing in the same directions in the higher dimensional embedding space. A cluster of such higher dimensional vectors is then called islands of agreement. To see such islands in practice, one can then apply a dimensionality reduction procedure like t-sne to 3 dimensions, and visualize the obtained feature map by backprojecting the obtained features to the RGB range of [0,255][70]. Note that t-sNE preserves the higher dimensional spatial structure among the vectors since it fits gaussians. This visualization mechanism does not require any feed-forwarding or backpropogation through the net: the islands are already there, and clustering merely helps them to reveal their presence16. ", "page_idx": 25}, {"type": "text", "text": "We present the islands of agreement which can be observed now in APM in Fig 7,10,11. Notice how the part-wholes in the images are clearly observable in different colours. Finally, we scaled up our APM and trained it on COCO train set. Fig 13,12,8,14 visualizes the islands of agreement on COCO val set. It can clearly be seen that the net develops a semantic clustering and shows the potential to do a dense task. APM offers a unique advantage over parallel perception (DINOv2): features at a particular location can be queried serially. One can \u2018selectively\u2019 query the locations which correspond to a particular object[109], instead of bringing the whole feature grid in existence and then choosing the relevant objects[6]. Such an inductive bias of choosing which of the input rays/columns in a neural field corresponds to which object has already been explored by Yu et al[109]. ", "page_idx": 25}, {"type": "text", "text": "In the presented islands of agreement, one can see that the parts and wholes of the object are all entangled into one image. In an idealistic scenario, the net should learn to map the whole <part, whole , relationship> triplet[34]. GLOM looks at the features predicted by different layers of a transformer in a different way: lower layers are predicting object parts, and higher layers are predict the full object. For now, APM has only modelled the last layer of the transformer. In an idealistic scenario, we want the net to traverse the whole up-down part-whole hierarchy as well as learn the pose-transformation matrix which can get us from one part to its whole[34]. ", "page_idx": 25}, {"type": "text", "text": "Let us next consider a 2D image of a man[8] gazing at the ground in front of him. At the object level it does not make sense for embeddings of the nose to jump to the embeddings of the ground. However, it is okay to learn pose-transformation matrices which can get us from one point on the man\u2019s body to another. This means that this restricted movement has to be informed by a top-down feedback which we have not yet modelled. Furthermore, this pose prediction can happen by a big fat-net like APM17 shared across locations and levels of the part whole hierarchy. This is different from capsules, which contain only a few convolution fliters at a particular location. In computer graphics, the pose matrix is defined a four by four matrix where the first three by three elements are rotation components, last row are homogenous coordinates, and first three numbers in the last column are camera translations. If the net predicts a $4\\times4$ matrix each of whose element can be any constrained number, we lose the ability to make interpretations about what the predicted matrix actually represents since it is no longer in a well defined world-coordinate frame. This problem is same as how the \u2018learnt\u2019 camera poses are aligned to \u2018real\u2019 camera poses in neural fields while doing bundle adjustment[54]. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "Note that islands of agreement operate at the finest spatial granularity: there is one column vector for each pixel of the input percept. However, in the case of perceptual overcrowding [47], there might be some hidden part of an object whose inference might be made seeing the visible pixels around it. However, the island in our case does not model this, since agreement is only established for one object at a particular location $[70]^{18}$ . ", "page_idx": 26}, {"type": "text", "text": "Quod errat demonstrandum. Q.E.D[15]. May these humble neural-nets[66] now fold themselves[31] and stay safe[4]. For all of us. The last section of forward-forward paper talks about Mortal computation and it now appears that we might reach that point[33]. AI safety however still needs a lot more work[27]. ", "page_idx": 26}, {"type": "image", "img_path": "7Ye12RLZ4P/tmp/0fe524978f6e53aa01d22fdb3938576ebbeaf6f8ec03c07e8a3d6d40cf9dc802.jpg", "img_caption": ["Till the next time the space-curves[13], little_dogzilla aboard \u2018Starship Connectionist Enterprise\u2019. End of transmission. ", "Figure 9: A cute-little-godzilla trying its best to navigate wormholes [104]. Hiyaaa!! "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "7Ye12RLZ4P/tmp/55a394737b32561dd3d8ef035bff0ed07b65095eb9a57556dbbeac012c4d83c5.jpg", "img_caption": ["Figure 10: Cifar 10 islands: Notice how individual parts are clearly observed in APM features. These features are used for downstream classification. We leverage the visualization mechanism by [70]. These islands are not been hand-picked.[34] "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "1. Claims ", "page_idx": 27}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The first claim is that we improve on ttt\u2019s computational performance, second claim is that APM performs better than existing baselines, third claim is that APM\u2019s architecture is a single CNN filter and a MLP of 5 layers. Fourth claim, to the best of our knowledge is that we are providing the first evidence of GLOM\u2019s insight: i.e. percept is really a field . The first claim of computational complexity is analyzed in Tab 4 and Fig 2. Second claim of improved performance over ttt methods is present in tables 132.Third claim of APM\u2019s architecture will be evident in the codebase shared with this manuscript. Final claim confirming GLOM\u2019s insight i.e. percept really is a field is evident in Fig6. A kind reader can also interpolate any two images in the wild using the codebase we shall share with this manuscript in the review process. ", "page_idx": 27}, {"type": "image", "img_path": "7Ye12RLZ4P/tmp/0d7fd4a03f1efa34da4d5afb0ca5c701a4811638a011ca2c1a6a82f99f212743.jpg", "img_caption": ["Figure 11: Cifar 10 islands: Notice how individual parts are clearly observed in APM features. These features are used for downstream classification.We leverage the visualization mechanism by [70]. These islands are not been hand-picked.[34] "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 28}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 28}, {"type": "image", "img_path": "7Ye12RLZ4P/tmp/4d0912447be461b7e8143877d28e5234453852f6a6edbc24f164690399010b5e.jpg", "img_caption": ["Figure 12: Qualitative Results on COCO Val set: Our APM is trained on COCO-train set, and islands on COCO-val set are visualized[34]. Note that these islands are a consequence of single feed-forward through the net and not an iterative routing mechanism as in [18]. DINOv2 does parallel perception: i.e. all tokens are kept in the memory. However, APM does asynchronous perception: it can predict the column vector at any location asynchronously. The error map shows the error between the grid predicted by Dinov2 and the grid predicted by APM. It is mostly black which shows APM closely approximates Dinov2 grid as well as can be memory efficient. The islands shown in this figure are not hand-picked[34]. "], "img_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "7Ye12RLZ4P/tmp/669cdc93d26c696c1a8a180cbe5fba0ffdc3a4ab63e12e674120d689a5aa7424.jpg", "img_caption": ["Input ", "Figure 13: Qualitative Results on COCO Val set: Our APM is trained on COCO-train set, and islands on COCO-val set are visualized[34]. Note that these islands are a consequence of single feed-forward through the net and not an iterative routing mechanism as in [18]. DINOv2 does parallel perception: i.e. all tokens are kept in the memory. However, APM does asynchronous perception: it can predict the column vector at any location asynchronously. The error map shows the error between the grid predicted by Dinov2 and the grid predicted by APM. It is mostly black which shows APM closely approximates Dinov2 grid as well as can be memory efficient. The islands shown in this figure are not hand-picked[34]. "], "img_footnote": [], "page_idx": 30}, {"type": "image", "img_path": "7Ye12RLZ4P/tmp/a1eb84aa721839f41025b302829873999d8131e26b8a2e6e61559be2c368234a.jpg", "img_caption": ["Input ", "Figure 14: Qualitative Results on COCO Val set: Our APM is trained on COCO-train set, and islands on COCO-val set are visualized[34]. Note that these islands are a consequence of single feed-forward through the net and not an iterative routing mechanism as in [18]. DINOv2 does parallel perception: i.e. all tokens are kept in the memory. However, APM does asynchronous perception: it can predict the column vector at any location asynchronously. The error map shows the error between the grid predicted by Dinov2 and the grid predicted by APM. It is mostly black which shows APM closely approximates Dinov2 grid as well as can be memory efficient. The islands shown in this figure are not hand-picked[34]. "], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: See limitations sections at the end of conclusion. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 32}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The paper has presented a practical computational-efficient architecture for test-time-training. Its computational efficiency and results have been empirically validated. Furthermore, codebase shall be shared for transparency. The theoretical explanation of the entire algorithm has been presented in the section 3. Full pseudo-code has been presented in Algorithm 1. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 32}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: Model Codebase has been shared. We shall release the full repo post-review. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 33}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: To preserve anonymity during the review process, code shall be hosted on anonymousgithub and shared in supplemental during review. post-review , we shall release the code on github, and maintain it regularly. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 34}, {"type": "text", "text": "Answer:[Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 34}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: We follow the standard practice followed in test-time-training literature [90, 17, 92]. Furthermore, all the experiments are run with the same seed, thereby ensuring reproducibility. All the nets are initialized with same random weight matrices to help ensure experimental consistency. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 34}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: There is a whole discussion on computational complexity. ll experiments are run on a same desktop-workstation containing 1x rtx a6000/96GB ram/Ubuntu 22.04/2TB ssd. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 35}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: read the code of ethics. no human subjects were used in this work. existing open-source datasets were used. work was done by a small student in academia: there is no issue of license. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 35}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: We have added a broader impact section in the supplementary. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. ", "page_idx": 35}, {"type": "text", "text": "\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 36}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [No] ", "page_idx": 36}, {"type": "text", "text": "Justification: we have used open sourced data. so there are no asset issues. our own model weights shall be released later. The presented model APM is a very small model, with potential for going into a toaster for less than a dollar, thereby making ai more accessible and useful in lives of every day people. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 36}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Justification: yes, APM is inspired from GLOM whose reference we have added. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 36}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 36}, {"type": "text", "text": "Answer: [No] ", "page_idx": 36}, {"type": "text", "text": "Justification: we have introduced a new network called APM. that is well documented. this paper uses well-known datasets in the existing test-time-training literature. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 37}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [No] ", "page_idx": 37}, {"type": "text", "text": "Justification: no human subjectes were used here. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 37}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 37}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Justification: The worker was a graduate student who was paid 20 hours. But he worked 90 hours because he loved it ehh. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "\u2022 Wait for it, says Barney. Now is the time for post-credits. The fun is not yet over. This message has been subtly embedded into this list, because apparently noone scrolls down till the bottom. Since you stayed with us, big kudos to you. The rise of mortal machines shall now begin lolzy. We shall see each other on the other end. Hiya hiya hiya. All credits go to shinchan, pikachu and big godzillas. But who are they?. That\u2019s a teeny-tiny secret. We might whisper it to you someday. Apparently walls also have ears.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 37}, {"type": "text", "text": "", "page_idx": 38}]