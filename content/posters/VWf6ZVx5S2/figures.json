[{"figure_path": "VWf6ZVx5S2/figures/figures_1_1.jpg", "caption": "Figure 1: FFN as Mixture of Low-rank Experts. Given an up-projection weight matrix in FFN, a straightforward way of splitting it into MoE is to divide every K channels into separate experts, resulting in highly dissimilar experts and a high-rank MoE, which is inherently unsuitable for integration with LoRA. In contrast, our proposed MoLE approach rearranges the weight matrix into groups of similar channels as experts, creating specialized low-rank experts that are better suited for integrating with LoRA.", "description": "This figure illustrates two different ways of splitting the feed-forward network (FFN) layer into Mixture of Experts (MoE). The top half shows the traditional approach, which results in experts with dissimilar weights and a high-rank MoE.  The bottom half shows the proposed MoEfied LoRA approach, which rearranges the channels to create experts with similar weights, resulting in a low-rank MoE better suited for integration with Low-Rank Adaptation (LoRA). This improves efficiency and effectiveness of multi-task learning.", "section": "2 The Proposed Approach"}, {"figure_path": "VWf6ZVx5S2/figures/figures_2_1.jpg", "caption": "Figure 2: Summary of representative architectures of multi-task learning.", "description": "This figure compares three different approaches to multi-task learning: the conventional MoE approach, the LoRA Experts approach, and the proposed MoEfied LoRA approach. The conventional MoE approach uses multiple expert networks and a gating mechanism to dynamically select the most relevant experts for each input. The LoRA Experts approach uses unified low-rank adaptation modules to achieve parameter efficiency. The proposed MoEfied LoRA approach groups similar weights into specialized low-rank experts, enabling seamless integration with LoRA to create an efficient multi-task learner. It combines this with a router fading strategy to ensure both training and inference efficiency while substantially reducing storage overhead.", "section": "2 The Proposed Approach"}, {"figure_path": "VWf6ZVx5S2/figures/figures_3_1.jpg", "caption": "Figure 3: Illustration of the proposed EMTAL framework. Given a pre-trained ViT, we firstly decompose it into a MoE-based multi-task learner by using the balanced k-means. LoRA is then applied to the low-rank experts, creating an efficient multi-task learner dubbed MoEfied LoRA. During multi-task optimization, the Quality Retaining is employed to maintain the high-quality knowledge for tasks that have already converged. Finally, with the aid of the router fading strategy, the learned knowledge is reparameterized back into the pre-trained ViT, eliminating the extra inference cost.", "description": "This figure illustrates the EMTAL framework's five stages.  It starts with a pre-trained Vision Transformer (a).  The FFN layer is then decomposed into a Mixture of Low-rank Experts (MoE) using balanced k-means clustering (b). Low-Rank Adaptation (LoRA) is applied to these low-rank experts, forming the MoEfied LoRA (c).  The Quality Retaining (QR) optimization method is applied during training to maintain the performance of well-trained tasks (d). Finally, a router fading strategy seamlessly integrates the learned parameters into the original transformer, enabling efficient inference without extra overhead (e).", "section": "2 The Proposed Approach"}, {"figure_path": "VWf6ZVx5S2/figures/figures_9_1.jpg", "caption": "Figure 4: Comparison of the low-rank properties by using the vanilla MoE and the proposed MOLE, based on the Ky Fan 2-k norm [51]. A higher value signifies a stronger low-rank property.", "description": "This figure compares the low-rank properties of experts generated using the vanilla MoE and the proposed MoLE method.  The Ky Fan 2-k norm is used as a metric to measure the low-rank properties, with higher values indicating a stronger low-rank property.  The left subplot shows how the low-rank properties vary across different ranks (k) for experts within a specific transformer block (block 4).  The right subplot shows how these properties change across different layers (l) of the transformer for a fixed rank (k=1).  The results demonstrate that the MoLE method consistently generates experts with stronger low-rank characteristics, especially in the lower layers of the transformer.", "section": "3.5 Visualization on the Low-rank Property of MoLE"}]