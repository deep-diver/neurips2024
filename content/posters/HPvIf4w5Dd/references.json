{"references": [{"fullname_first_author": "Peter L. Bartlett", "paper_title": "REGAL: A regularization based algorithm for reinforcement learning in weakly communicating MDPs", "publication_date": "2009", "reason": "This paper introduces REGAL, a regularization-based algorithm for reinforcement learning in weakly communicating Markov Decision Processes (MDPs), which is highly relevant to the current paper's focus on average-reward MDPs."}, {"fullname_first_author": "Thomas Jaksch", "paper_title": "Near-optimal regret bounds for reinforcement learning", "publication_date": "2010-MM-DD", "reason": "This paper provides near-optimal regret bounds for reinforcement learning, which are foundational results used for the analysis of sample complexity in reinforcement learning algorithms."}, {"fullname_first_author": "Mengdi Wang", "paper_title": "Primal-dual \u03c0 learning: Sample complexity and sublinear run time for ergodic Markov Decision Problems", "publication_date": "2017-MM-DD", "reason": "This paper establishes sample complexity and sublinear runtime bounds for primal-dual \u03c0 learning in ergodic Markov Decision Processes, providing a benchmark for the current paper's algorithm."}, {"fullname_first_author": "Jinghan Wang", "paper_title": "Near sample-optimal reduction-based policy learning for average reward MDP", "publication_date": "2022-MM-DD", "reason": "This paper presents a near sample-optimal algorithm for average-reward Markov Decision Processes that uses a reduction-based approach, which is closely related to the methodology of the current paper."}, {"fullname_first_author": "Matthew Zurek", "paper_title": "Span-based optimal sample complexity for average reward MDPs", "publication_date": "2023-MM-DD", "reason": "This paper establishes span-based optimal sample complexity bounds for average-reward MDPs, directly addressing a key complexity measure relevant to the current paper\u2019s analysis."}]}