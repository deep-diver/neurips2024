[{"type": "text", "text": "Treeffuser: Probabilistic Predictions via Conditional Diffusions with Gradient-Boosted Trees ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Nicolas Beltran-Velez\u22171 Alessandro Antonio Grande\u22172,3 Achille Nazaret\u22171,3 Alp Kucukelbir1,4 David Blei1,5 ", "page_idx": 0}, {"type": "text", "text": "1Department of Computer Science, Columbia University, New York, USA   \n2Halvorsen Center for Computational Oncology, Memorial Sloan Kettering Cancer Center, New York, USA   \n3Irving Institute for Cancer Dynamics, Columbia University, New York, USA 4Fero Labs, New York, USA 5Department of Statistics, Columbia University, New York, USA ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Probabilistic prediction aims to compute predictive distributions rather than single point predictions. These distributions enable practitioners to quantify uncertainty, compute risk, and detect outliers. However, most probabilistic methods assume parametric responses, such as Gaussian or Poisson distributions. When these assumptions fail, such models lead to bad predictions and poorly calibrated uncertainty. In this paper, we propose Treeffuser, an easy-to-use method for probabilistic prediction on tabular data. The idea is to learn a conditional diffusion model where the score function is estimated using gradient-boosted trees. The conditional diffusion model makes Treeffuser flexible and non-parametric, while the gradient-boosted trees make it robust and easy to train on CPUs. Treeffuser learns well-calibrated predictive distributions and can handle a wide range of regression tasks\u2014including those with multivariate, multimodal, and skewed responses. We study Treeffuser on synthetic and real data and show that it outperforms existing methods, providing better calibrated probabilistic predictions. We further demonstrate its versatility with an application to inventory allocation under uncertainty using sales data from Walmart. We implement Treeffuser in https://github.com/blei-lab/treeffuser. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In this paper, we develop a new method for probabilistic prediction from tabular data. This problem is important to many fields, such as power generation [1], finance [2], and healthcare [3]. It drives decision processes such as supply chain planning [4], risk assessment [5, 6], and policy-making [7]. ", "page_idx": 0}, {"type": "text", "text": "Example: Manufacturing plants measure information as they operate. This information\u2014properties of raw materials, operational flows, temperatures, and level measurements\u2014collectively determine the output of the plant [8]. From this data, how can manufacturers adapt operations to reduce emissions while maximizing profits? The answer requires both good predictions and estimates of uncertainty, so as to trade off the risk of failure with the reward of lower emissions and higher profit. More broadly, such industrial workflow problems often rely on predictions from vast amounts of tabular data\u2014observations of variables arranged in a table [9, 10]. ", "page_idx": 0}, {"type": "image", "img_path": "4KeSvAvNMr/tmp/6c83a30a7a38fa7be805e78551730ea53bd45be084642cdf4077dc079dd9adc8.jpg", "img_caption": [], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: Samples ${\\textbf{\\em y}}|{\\textbf{\\em x}}$ from Treeffuser vs. true densities, for multiple values of $\\textbf{\\em x}$ under three different scenarios. Treeffuser captures arbitrarily complex conditional distributions that vary with $\\textbf{\\em x}$ . ", "page_idx": 1}, {"type": "text", "text": "Our method builds on two ideas: diffusions [11] and gradient-boosted trees [12]. Diffusions accurately estimate conditional distributions, but existing methods are not designed for tabular data. Decision trees excel at analyzing tabular data, but do not provide probabilistic predictions. Our method, which we call Treeffuser, combines the advantages of both formalisms. It defines an accurate tree-based diffusion model for probabilistic prediction which can easily handle large datasets. ", "page_idx": 1}, {"type": "text", "text": "Fig. 1 shows Treeffuser in action. We feed it data from three complex distributions\u2013a multimodal response with varying components, an inflated response with shifting support, and a multivariate response with dynamic correlations. For each task, Treeffuser uses trees to learn a diffusion model and then outputs samples from the target conditional. These samples fully capture the complexity of the response functions. ", "page_idx": 1}, {"type": "text", "text": "Treeffuser exhibits several advantages: ", "page_idx": 1}, {"type": "text", "text": "\u2022 It is nonparametric. It makes few assumptions about the form of the conditional response distribution. For example, it can estimate response distributions that are multivariate, multimodal, skewed, and heavy-tailed.   \n\u2022 It is efficient. Diffusions can be slow to train [13]. Treeffuser relies on trees and is fast. For instance, in section 5.3, Treeffuser trains from a table with 112,000 observations and 27 variables in 53 seconds on a laptop.   \n\u2022 It is accurate. On benchmark datasets, Treeffuser outperforms NGBoost [14], IBUG [15], quantile regression [16] and deep ensembles [17]. It provides better probabilistic predictions, including more precise quantile estimations and accurate mean predictions. ", "page_idx": 1}, {"type": "text", "text": "The rest of the paper is organized as follows. Section 2 reviews diffusions and gradient-boosted trees. Section 3 integrates these two ideas into Treeffuser and justifies the method. Section 4 discusses related work. Section 5 studies synthetic and standard benchmark datasets. ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Treeffuser combines two ideas: diffusion models and gradient-boosted trees (GBTs). This section provides a gentle introduction to both topics. Familiar readers can skip ahead to Section 3. ", "page_idx": 1}, {"type": "text", "text": "2.1 Diffusion Models ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "A diffusion model is a generative model that learns an arbitrarily complex distribution $\\pi(\\boldsymbol{y})$ . It consists of two processes: forward and reverse diffusion. ", "page_idx": 1}, {"type": "text", "text": "The forward process takes the target distribution $\\pi(\\boldsymbol{y})$ and continuously transforms it into a simple distribution. It does so by progressively adding noise to samples from $\\pi$ : ", "page_idx": 1}, {"type": "text", "text": "where Eq. (1) defines a stochastic differential equation (SDE) with a standard Brownian motion $w(t)$ , and drift and diffusion functions $f$ and $g$ . The time horizon $T$ is large, such that the resulting marginal distribution $\\begin{array}{r}{p_{\\mathrm{simple}}:=p_{T}(\\pmb{y}(T))=\\int_{\\pmb{y}^{\\prime}}p_{T}(\\pmb{y}(T)\\mid\\pmb{y}(0)=\\pmb{y}^{\\prime})\\pi(\\pmb{\\dot{y}^{\\prime}})\\mathrm{d}\\pmb{y}^{\\prime}}\\end{array}$ is agnostic to $\\pi$ . ", "page_idx": 2}, {"type": "text", "text": "The reverse process transforms the simple distribution back into the target distribution by denoising perturbed samples from $p_{\\mathrm{simple}}$ . It posits the following model, which runs backward from $T$ to $0$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\tilde{\\pmb{y}}(T)\\sim p_{\\mathrm{simple}},}\\\\ {\\mathsf{d}\\tilde{\\pmb{y}}_{t}=[f(\\tilde{\\pmb{y}}(t),t)-g(t)^{2}\\nabla_{\\tilde{\\pmb{y}}(t)}\\log p_{t}(\\tilde{\\pmb{y}}(t))]\\mathsf{d}t+g(t)\\mathsf{d}\\tilde{\\pmb{w}}(t),}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\tilde{\\pmb{w}}(t)$ is a standard Brownian with reversed time. Anderson [18] shows that $\\tilde{\\pmb{y}}(t)\\triangleq\\pmb{y}(t)$ for each time $t$ , and so $\\tilde{\\pmb{y}}(0)\\sim\\pi$ . This means that by drawing a sample from $p_{\\mathrm{simple}}$ and then numerically approximating the reverse process in Eq. (2), we obtain samples from $\\pi$ . ", "page_idx": 2}, {"type": "text", "text": "However, the score function, $y\\mapsto\\nabla_{y}\\log p_{t}(y)$ , is usually unknown. Vincent [19] shows it can be estimated from the trajectories of the forward process as the minimizer of the following objective: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\left[y\\mapsto\\nabla_{y}\\log p_{t}(y)\\right]=\\underset{s\\in\\mathcal{S}}{\\arg\\operatorname*{min}}\\,\\mathbb{E}_{t}\\mathbb{E}_{\\pi}\\mathbb{E}_{p_{0t}}\\left[\\left\\|\\nabla_{y(t)}\\log p_{0t}(y(t)\\mid y(0))-s(y(t),t)\\right\\|^{2}\\right],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\boldsymbol{S}$ is the set of all possible functions of $\\textit{\\textbf{y}}$ indexed by time $t$ , and $p_{0t}$ denotes the conditional distribution of $\\pmb{y}(t)$ given ${\\pmb y}(0)$ . In practice, we set $t\\sim\\mathrm{Uniform}([0,1])$ and choose the drift $f$ and diffusion function $g$ so that $p_{0t}$ is Gaussian. We detail our choice of $f$ and $g$ in Appendix C. ", "page_idx": 2}, {"type": "text", "text": "We approximate the expectations in Eq. (3) with the empirical distribution of the observed $\\textit{\\textbf{y}}$ for $\\mathbb{E}_{\\pi}$ , and with the known Gaussian trajectories $\\pmb{y}(t)\\mid\\pmb{y}(0)\\bar{\\pmb{\\jmath}}=\\pmb{y}$ for $\\mathbb{E}_{p_{0t}}$ . The objective can then be minimized by parametrizing the score $s$ with any function approximator, such as a neural network or, as we do, with trees. By estimating the score function, we effectively learn the distribution $\\pi$ . ", "page_idx": 2}, {"type": "text", "text": "For a more detailed introduction to diffusion models, we provide an expanded version of this section in Appendix A. ", "page_idx": 2}, {"type": "text", "text": "2.2 Gradient Boosted Trees ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Consider the following common machine learning task. Given variables $(a,b)\\sim\\pi$ , where $b$ is scalar, learn the function $F^{*}(\\pmb{a})$ that best approximates $b$ from $\\textbf{\\em a}$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\nF^{*}=\\underset{F}{\\arg\\operatorname*{min}}\\,\\mathbb{E}_{(\\mathbf{a},b)\\sim\\pi}\\big[L(F(\\mathbf{a}),b)\\big],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $L$ is a loss function measuring the quality of $F$ , such as the squared loss $L(u,v)=(u-v)^{2}$ . Gradient-boosted trees (GBTs) are a widely used non-parametric machine learning model for this task [12]. GBTs use decision trees [20] as simple building block functions $f:\\mathbb{R}^{d}\\xrightarrow{}\\mathbb{R}$ to form a good approximation $\\hat{F}$ of $F^{*}$ . GBTs sequentially build approximations $\\hat{F}_{i}$ defined as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\hat{F}_{i}(\\pmb{a})=\\sum_{m=0}^{i-1}\\varepsilon f_{m}(\\pmb{a}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\varepsilon\\in(0,1)$ is a parameter akin to a learning rate. ", "page_idx": 2}, {"type": "text", "text": "Each decision tree $f_{i}$ is constructed to minimize the squared error between the current approximation $\\hat{F}_{i}$ and the negative gradient of the loss function: ", "page_idx": 2}, {"type": "equation", "text": "$$\nf_{i}=\\underset{f\\in\\mathrm{Trees}}{\\arg\\operatorname*{min}}\\,\\mathbb{E}_{\\pmb{a},b}\\left[\\left(f(\\pmb{a})+\\frac{\\partial L(F_{i}(\\pmb{a}),b)}{\\partial F_{i}(\\pmb{a})}\\right)^{2}\\right].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "As the number of trees $i$ increases, the approximation $\\hat{F}_{i}$ becomes a better minimizer of Eq. (4). Various modifications to this basic algorithm have been proposed, including different loss functions in Eq. (4), higher order optimizations for Eq. (6) and general heuristics for faster training [21\u201323]. ", "page_idx": 2}, {"type": "text", "text": "3 Probabilistic Prediction via Treeffuser ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Treeffuser tackles probabilistic prediction by learning a diffusion model with gradient-boosted trees. It is particularly well-suited to modeling tabular data. Trees offer useful inductive biases, natural handling of categorical and missing data, and fast and robust training procedures. Diffusions eliminate the need for restrictive parametric families of distributions and protect against model misspecification. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Denote an independently distributed set of observations as $D=\\{(\\pmb{x}_{i},\\pmb{y}_{i})\\}_{i=1}^{n}$ . Treeffuser forms the predictive distribution $\\pi(\\boldsymbol{y}\\mid\\boldsymbol{x})$ as a function of inputs $\\textbf{\\em x}$ . We first introduce conditional diffusion models and discuss the conditional score estimation problem for both univariate and multivariate outcomes. We then outline the training and sampling procedures for Treeffuser. ", "page_idx": 3}, {"type": "text", "text": "3.1 The Conditional Diffusion Model ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Treeffuser produces a distribution over $\\textit{\\textbf{y}}$ for each value of $\\textbf{\\em x}$ using conditional diffusion models. Unlike approaches that guide an unconditional model to achieve conditionality, Treeffuser follows the line of work that directly fits the conditional score function [24\u201326]. ", "page_idx": 3}, {"type": "text", "text": "Conditional diffusion models. The diffusion models introduced in Section 2.1 target the marginal distribution of $\\textit{\\textbf{y}}$ . Here, we extend them to conditional distributions $\\pi_{\\pmb{x}}(\\pmb{y}):=\\pi(\\pmb{y}\\mid\\pmb{x})$ . ", "page_idx": 3}, {"type": "text", "text": "To model $\\pi_{x}$ , assign a diffusion process $\\begin{array}{r}{y_{x}(t)}\\end{array}$ to each value of $\\textbf{\\em x}$ . These processes share the same diffusion equation but have different boundary conditions corresponding to their target conditionals: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{y_{x}(0)\\sim\\pi_{x}(\\pmb{y}),}\\\\ {\\mathrm{d}\\pmb{y}_{x}(t)=f(\\pmb{y}_{x}(t),t)\\mathrm{d}t+g(t)\\mathrm{d}\\pmb{w}(t).}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "As before, $f$ and $g$ are simple functions such that $y_{x}(T)\\sim p_{\\mathrm{simple}}$ for all $\\textbf{\\em x}$ . Denote the marginal distribution of $\\pmb{y}_{\\pmb{x}}(t)$ as $p_{x,t}$ . For two time points $t$ and $u$ , where $t\\,>\\,u$ , denote the conditional distribution $y_{x}(t)\\mid y_{x}(u)$ as $p_{u t}$ . (Note how $p_{u t}$ does not vary in $\\textbf{\\em x}$ .) ", "page_idx": 3}, {"type": "text", "text": "To match each $\\textbf{\\em x}$ -dependent forward SDE we have an $\\textbf{\\em x}$ -dependent reverse SDE of the form: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{d}\\tilde{y}_{x}(t)=\\left[f(\\tilde{y}_{x}(t),t)-g(t)^{2}\\nabla_{\\tilde{y}_{x}(t)}\\log p_{x,t}(\\tilde{y}_{x}(t))\\right]\\mathrm{d}t+g(t)\\mathrm{d}\\tilde{w}(t),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the score function $\\nabla\\log p_{{\\mathbf x},t}$ now also depends on $\\textbf{\\em x}$ . Similar to unconditional diffusions, by estimating the conditional score, we can sample from $\\pmb{y}_{x}$ by first sampling $\\tilde{y}_{x}(T)$ from $p_{\\mathrm{simple}}(\\pmb{y})$ and then solving the corresponding reverse SDE. ", "page_idx": 3}, {"type": "text", "text": "Conditional score estimation objective. Estimating the conditional score follows a similar strategy as the unconditional version, but with the added requirement of simultaneously estimating the score for all $\\textbf{\\em x}$ . Recall that by Eq. (3), for a fixed $\\textbf{\\em x}$ , the function $s_{\\pmb{x}}^{*}$ defined by ", "page_idx": 3}, {"type": "equation", "text": "$$\ns_{x}^{*}=\\underset{s\\in\\mathcal{S}}{\\mathrm{arg\\,min}}\\,\\mathbb{E}_{t}\\mathbb{E}_{\\pi_{\\alpha}(y_{\\alpha}(0))}\\mathbb{E}_{p_{0t}(y_{\\alpha}(t)|y_{\\alpha}(0))}\\left[\\left\\|\\nabla_{y_{\\alpha}(t)}\\log p_{0t}(y_{x}(t)\\mid y_{x}(0))-s(y_{x}(t),t)\\right\\|^{2}\\right]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "satisfies $s_{x}^{*}(y_{x},t)=\\nabla_{y_{x}(t)}\\log p_{x,t}(y_{x}(t))$ for all $\\pmb{y},t$ and the fixed $\\textbf{\\em x}$ . Intuitively, if we allow $s\\in S$ to also take $\\textbf{\\em x}$ as input, we can gather all of these individual optimization problems into a single large problem by taking an additional expectation over $x\\sim\\pi$ , that is: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\underset{S\\in S^{+}}{\\arg\\operatorname*{min}}\\mathbb{E}_{\\pi(x)}\\mathbb{E}_{t}\\mathbb{E}_{\\pi(y_{\\alpha}(0))}\\mathbb{E}_{p_{0t}}\\left[\\left\\|\\nabla_{y_{\\alpha}(t)}\\log p_{0t}(y_{x}(t)\\mid y_{x}(0))-S(y_{x}(t),t,x)\\right\\|^{2}\\right].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Here, $S^{+}$ represents the set of functions that take $\\textbf{\\em x}$ as an extra input along $\\textit{\\textbf{y}}$ and $t$ . The uppercase $S$ emphasizes that $S$ takes $\\textbf{\\em x}$ as input, contrary to lowercase $s$ . The validity of this objective is given by the following result. ", "page_idx": 3}, {"type": "text", "text": "Theorem 1 (Optimal Conditional Objective). Define $S^{*}$ as the solution of Eq. (9). Then, for almost all $\\pmb{x},\\pmb{y},t$ with respect to $\\pi(x,y)$ and the Lebesgue measure on $t\\in[0,T]$ , we have ", "page_idx": 3}, {"type": "equation", "text": "$$\nS^{*}(\\pmb{y},t,\\pmb{x})=\\nabla_{\\pmb{y}}\\log p_{\\pmb{x},t}(\\pmb{y}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We refer to Eq. (9) as the conditional score objective. The proof is provided in Appendix B. ", "page_idx": 3}, {"type": "text", "text": "3.2 The Trees ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Treeffuser uses gradient-boosted trees (GBTs) to minimize Eq. (9). As GBTs work on scalar outputs, we separate the conditional score objective into an equivalent set of $d_{y}$ independent scalar-valued ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Algorithm 2: Treeffuser Sampling   \nData: $\\mathcal{D},R$ , SDE-specific $(h,T)$ Data: GBTs $(U_{1},...,U_{d_{y}})$ , input instance $\\textbf{\\em x}$ ,   \nResult: GBTs $\\left(u_{1},...,u_{d}\\right)$ optimizing Eq. (12) discretization steps $n_{d}$ , SDE-specific   \n$\\mathcal{D}^{\\prime}\\gets\\mathcal{D}$ $(p_{\\mathrm{simple}},T,f,g,\\sigma)$ $(\\pmb{x}^{(i)},\\pmb{y}^{(i)})\\in\\mathcal{D}$ do Result: A sample ${\\pmb y}\\sim\\pi({\\pmb y}\\mid{\\pmb x})$ r $r=1,...,R$ do \u03b4 \u2190T/nd $t\\sim\\mathrm{Uniform}[0,T]$ y \u223cpsimple(y) \u03b6 \u223cN(0, Idy) t T $\\mathcal{D}^{\\prime}\\leftarrow\\mathcal{D}^{\\prime}\\cup((h(\\zeta,t,{\\pmb y}^{(i)}),t,{\\pmb x}^{(i)}),-\\zeta)$ for $i=1,\\ldots,n_{d}$ do $\\pmb{w}\\sim\\mathcal{N}(0,I_{d_{y}})$ $k=1,...,d_{y}$ do $\\tilde{f}\\gets f(\\pmb{y},t)-g(t)^{2}U(\\pmb{y},t,\\pmb{x})/\\sigma(t;\\pmb{y})$ $\\mathcal{D}^{k}\\leftarrow\\{(a,b_{k})\\mid(a,b)\\in\\mathcal{D}^{\\prime}\\}$ $\\pmb{y}\\leftarrow\\pmb{y}-(\\tilde{f}\\delta+g(t)\\delta\\pmb{w})$ TrainGBT( k) t t \u2206 $(U_{1},\\cdot\\cdot\\cdot,U_{d})$ return y ", "page_idx": 4}, {"type": "text", "text": "sub-problems ", "page_idx": 4}, {"type": "equation", "text": "$$\nS_{k}^{*}=\\underset{S_{k}\\in\\mathrm{GST}}{\\arg\\operatorname*{min}}\\mathbb{E}_{t}\\mathbb{E}_{\\pi(x,y)}\\mathbb{E}_{p_{0t}(y_{x}(t)\\mid y)}\\left[\\left(\\frac{\\partial\\log p_{0t}(y_{x}(t)\\mid y)}{\\partial y_{x}(t)_{k}}-S_{k}(y_{x}(t),t,x)\\right)^{2}\\right],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathbf{\\nabla}_{a_{k}}$ denotes the $k$ -th component of vector $\\textbf{\\em a}$ . ", "page_idx": 4}, {"type": "text", "text": "Recall that the drift and diffusion functions of the forward process are chosen such that $p_{0t}$ is Gaussian. Let $m=m(t;{\\pmb y})$ and $\\sigma=\\sigma(t)$ denote the corresponding mean and standard deviation, respectively. Treeffuser replaces the partial derivative in Eq. (11) with its closed-form expression as a function of $m$ and $\\sigma$ . Treeffuser further reparametrizes $S(\\boldsymbol{y},t,\\boldsymbol{x})$ with $U(\\pmb{y},t,\\pmb{x})/\\sigma(t)$ and defines $h(\\zeta,t,{\\pmb y})=m(t;{\\pmb y})+\\zeta\\sigma(t)$ , the process by which a sample $\\textit{\\textbf{y}}$ at time 0 gets diffused into a sample at time $t$ with Gaussian noise $\\zeta$ . The optimization problems in Eq. (11) are then: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\forall k\\in\\{1,...,d_{y}\\},\\quad U_{k}^{*}=\\underset{U_{k}}{\\mathrm{arg\\,min}}\\,\\mathbb{E}_{(x,y)\\sim\\pi}\\mathbb{E}_{t}\\mathbb{E}_{\\zeta\\sim\\mathcal{N}(0,I_{d_{y}})}\\left[\\big(\\zeta_{k}+U_{k}\\big(h(\\zeta,t,y),t,x)\\big)^{2}\\right].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The next theorem justifies how the individual problems in Eq. (12) estimate the conditional score. ", "page_idx": 4}, {"type": "text", "text": "Theorem 2 (Treeffuser One-Dimensional Objectives). Denote $U^{*}=(U_{1}^{*},...,U_{d_{y}}^{*})$ . Then for almost all $\\pmb{x},\\pmb{y},t$ with respect to $\\pi(x,y)$ and the Lebesgue measure on $t\\in[0,T]$ , we have ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\nabla_{\\pmb{y}}\\log p_{\\pmb{x},t}(\\pmb{y})=\\frac{U^{*}(\\pmb{y},t,\\pmb{x})}{\\sigma(t)}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Each problem in Eq. (12) is a GBT problem where the notation within Eq. (4) corresponds to $F:=U_{k}$ , $\\pmb{a}:=\\bar{(}h(\\zeta,t,\\pmb{y}),t,\\stackrel{\\cdot}{\\pmb{x}})\\in\\mathbb{R}^{d_{y}+1+d_{x}^{-}}$ , $b:=-\\zeta_{k}\\in\\mathbb{R}$ and $L$ is the square loss. We note that the noise scaling reparametrization, $S=U/\\sigma$ , is key to stabilizing the learning process; see the ablation study in Appendix G. ", "page_idx": 4}, {"type": "text", "text": "Finally, Treeffuser approximates the expectations in Eq. (12) with Monte Carlo sampling. For each sample $\\left({\\pmb x},{\\pmb y}\\right)$ from the dataset $\\mathcal{D}$ , Treeffuser samples $R$ pairs of $(t,\\zeta)\\sim\\mathrm{Uniform}([0,\\bar{1}])\\,\\bar{\\otimes}\\mathcal{N}(0,I_{d_{y}})$ and creates new datasets $\\mathcal{D}^{k}$ containing $R\\cdot n$ datapoints of the form $\\left((h(\\zeta,t,{\\pmb y}),t,{\\pmb x}),-\\zeta_{k}\\right)$ , one $\\mathcal{D}^{k}$ per dimension of $\\textit{\\textbf{y}}$ . Then, each of these datasets is given to a standard GBT algorithm, such as LightGBM [22] or XGBoost [21]. Our implementation of Treeffuser uses LightGBM. Algorithm 1 details this procedure. ", "page_idx": 4}, {"type": "text", "text": "3.3 Sampling and Probabilistic Predictions ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Treeffuser provides samples from the probabilistic predictive distribution $\\pi(\\boldsymbol{y}\\mid\\boldsymbol{x})$ . It does so as in standard unconditional models by plugging in the GBT-estimated conditional score from Eq. (12) ", "page_idx": 4}, {"type": "text", "text": "into a numerical approximation of the SDE in Eq. (8). While Treeffuser is compatible with any SDE solver, our implementation leverages Euler-Maruyama [27] due to its good balance between accuracy and the number of function evaluations. In general, we found good performance with as few as 50 steps. Algorithm 2 implements this procedure. ", "page_idx": 5}, {"type": "text", "text": "The samples generated by Treeffuser can then be used to estimate means, quantiles, probability intervals, expectations, or any other quantity of interest that depends on the response distribution. ", "page_idx": 5}, {"type": "text", "text": "3.4 Limitations ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The design of Treeffuser offers advantages in terms of usability, versatility, and robustness. But it also comes with a few limitations. First, the diffusion process is theoretically defined to only model continuous responses $\\textit{\\textbf{y}}$ . However, count and other forms of discrete responses are common in the probabilistic modeling of tabular data. While our experiments show that this limitation does not prevent Treeffuser from outperforming comparable methods on these kinds of data, there may be opportunities for further improvement with direct modeling of discrete outcomes. Second, Treeffuser does not offer a closed-form density and must solve an SDE to generate samples. This sampling process, in contrast with the fast training, can become expensive when many samples per datapoint $\\textbf{\\em x}$ are required. ", "page_idx": 5}, {"type": "text", "text": "4 Related Work ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Treeffuser builds on advances in diffusion models to form probabilistic predictions from tabular data. ", "page_idx": 5}, {"type": "text", "text": "Diffusion models. Diffusion models excel at learning complex unconditional distributions on a range of data, such as images [28\u201330], molecules [31], time series [32], and graphs [33]. A common task is conditional generation, where the goal is to generate samples from a distribution conditioned on features. There are two approaches to this objective. One approach is to use guidance methods by which the score of an unconditional diffusion model is altered during generation to mimic the score of the conditional distribution [11, 34, 35]. This approach is especially popular for inverse problems [11, 36, 37]. Another approach is to train a conditional model from the start, incorporating the conditioning information during training [24\u201326, 38, 39]. This is the approach adopted by Treeffuser. ", "page_idx": 5}, {"type": "text", "text": "Treeffuser contributes to a recent line of work that applies diffusions to tabular data. This includes deep learning approaches for data generation [40], probabilistic regression [41], and missing data imputation [42]. Among these methods, CARD [41] uses neural-net based diffusions for probabilistic predictions and thus is most similar to Treeffuser in scope. We attempted to include it in our experiments using the implementation from Lehmann [43], but returned very poor results. We therefore excluded it from our testbed. The imputation of missing data has been recently extended to gradient-boosted trees [44]. ", "page_idx": 5}, {"type": "text", "text": "Probabilistic prediction for tabular data Probabilistic prediction for tabular data can be classified into parametric and non-parametric methods based on their assumptions about the likelihood shape. Parametric tree-based methods include NGBoost [14] and PBGM [45]. NGBoost uses natural gradients to optimize a scoring rule, while PBGM sequentially updates the mean and standard deviation for predictions. DRFs obtain maximum likelihood estimates by using this criteria to choose splits. Neural-based parametric methods include Bayesian Neural Networks [46], MC Dropout [47], and Deep Ensembles [17]. Notably these methods are all indirectly or directly Bayesian. Another approach, normalizing flows, transforms a latent distribution via an invertible neural network [48] and has been applied to tabular data [49]. Non-parametric methods are often tree-based, such as Quantile Regression Forests [16], Distributional Random Forests (DRF) [50], and IBUG [15]. Quantile Regression Forests approximate the inverse cumulative distribution function by minimizing pinball loss, while DRF and IBUG use a tree-based similarity metric to weight training data for predictions. These methods are baselines in our empirical studies, with detailed descriptions in Appendix F.1. ", "page_idx": 5}, {"type": "text", "text": "5 Empirical studies ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We demonstrate Treeffuser across three settings: synthetic data, standard UCI datasets [51], and sales data from Walmart. We find that Treeffuser outperforms state-of-the-art methods [15, 17, 50]; ", "page_idx": 5}, {"type": "table", "img_path": "4KeSvAvNMr/tmp/eda8636df5d712f43f36ba3af067d07d0bef4dac44f2beabc3e8b63fc2218a12.jpg", "table_caption": [], "table_footnote": ["Table 1: CRPS (lower is better) by dataset and method. $\\times$ indicates the method failed to run, and NA that the method is not directly applicable to multivariate outputs. Standard deviations are measured with 10-fold cross-validation. For each dataset, the two best methods are bolded. Treeffuser provides the most accurate probabilistic predictions, even with default hyperparameters (no tuning). Powers of ten are factorized out of each row in the rightmost column. "], "page_idx": 6}, {"type": "text", "text": "it can capture complex distributions with multimodal, inflated, or multivariate responses; it achieves better probabilistic predictions on many real-world datasets and it produces more accurate sales forecasts on Walmart data. We also find that Treeffuser can outperform other (tuned) methods without dataset-specific hyperparameter tuning. We provide the code for Treeffuser at https://github.com/blei-lab/treeffuser. ", "page_idx": 6}, {"type": "text", "text": "5.1 Probabilistic prediction on synthetic data ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We show the flexibility of Treeffuser on three probabilistic regression tasks that are difficult to model with standard parametric models. We also show its competitive performance on Gaussian data when compared to methods that posit a Gaussian likelihood. ", "page_idx": 6}, {"type": "text", "text": "Treeffuser vs. complex response functions. We design three difficult probabilistic regression tasks: a multimodal response where the number of modes changes with $\\textbf{\\em x}$ , an inflated response with support that changes with $\\textbf{\\em x}$ , and a multivariate response with nontrivial covariance structure. The data generation processes for these datasets are detailed in Appendix E. For each dataset, we generate 10,000 observations $\\left({x_{i},y_{i}}\\right)$ and train Treeffuser with its default parameters (see Appendix C). ", "page_idx": 6}, {"type": "text", "text": "Fig. 1 shows the histograms of 1,000 Treeffuser samples for three different values of $\\textbf{\\em x}$ and compares them with the true conditional densities of the response. Treeffuser captures the conditional distribution for all values of $x$ across all settings. For multimodal data, Treeffuser correctly detects the modes of the distribution without knowing their number a priori. For inflated data, it recovers the peak at the inflation point and the $x$ -dependent support of the response. For multivariate data, it recovers the complex covariance structures. ", "page_idx": 6}, {"type": "text", "text": "Treeffuser vs. parametric oracles. We simulate Gaussian responses with a linear response function and compare Treeffuser to other parametric methods that assume a Gaussian likelihood. These methods serve as oracles in these experiments as they are informed by the true functional form of the conditional response distribution. Results and further details are reported in Appendix E. Among all methods, Treeffuser consistently ranks second best, only behind the Deep Ensemble oracle. ", "page_idx": 6}, {"type": "text", "text": "5.2 Probabilistic prediction on real-world datasets ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We compare Treeffuser with state-of-the-art methods for probabilistic predictions on standard UCI datasets [51]. A detailed description of the baseline models can be found in Appendix F.1. ", "page_idx": 6}, {"type": "text", "text": "Metrics. We evaluate probabilistic predictions with the continuous ranked probability score (CRPS) and standard accuracy metrics. ", "page_idx": 6}, {"type": "text", "text": "CRPS is defined as $\\begin{array}{r}{\\mathrm{CRPS}(F,y)\\,=\\,\\int_{-\\infty}^{\\infty}(F(y^{\\prime})\\,-\\,\\mathbb{1}(y\\,\\le\\,y^{\\prime}))^{2}d y^{\\prime}}\\end{array}$ , where $F$ is the cumulative distribution function of the predicted distribution $p(y\\mid x)$ and $y$ is the true observed value [52]. CRPS is a proper scoring rule, in that its expected value is minimized under the true conditional distribution of the response [53]. CRPS is usually preferred over the log-likelihood, which is also a proper scoring rule, but is sensitive to the estimation of the tail densities [54]. Also, CRPS can readily be estimated from samples of $p(y\\mid x)$ , which is our setting with the non-parametric methods we evaluate. We evaluate CRPS by generating 100 samples from $p(y\\mid x)$ for each $\\textbf{\\em x}$ . For evaluating multivariate responses, we report the average marginal CRPS over each dimension. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "We also measure the quality of point predictions for each model. This is the ability to predict conditional means $\\mathbb{E}[y\\mid x]$ . We approximate $\\mathbb{E}[y\\mid x]$ using 50 samples and evaluate the accuracy using the mean absolute error (MAE) and the root mean squared error (RMSE). ", "page_idx": 7}, {"type": "text", "text": "Experimental setup. We performed 10-folds cross-validation. For each fold, we tuned the hyperparameters of the methods using Bayesian optimization for 25 iterations, using $20\\%$ of the current fold\u2019s training data as a validation set. Additional Bayesian optimization\u2019s iterations did not change the results. We detail the search space of hyperparameters for each method in Appendix F.2. ", "page_idx": 7}, {"type": "text", "text": "Results. Table 1 presents CRPS by dataset and method. Treeffuser consistently provides the most accurate predictions across datasets, as measured by CRPS. Notably, it outperforms other methods even when initialized with its default parameters. There is no consistent runner-up: among parametric methods, deep ensemble does overall better than NGBoost and IBUG; quantile regression does well on some datasets but underperforms in others. ", "page_idx": 7}, {"type": "text", "text": "We find that Treeffuser also returns the best point predictions of $\\mathbb{E}[y\\mid x]$ , as reported in Table 4. For comparison, we report the accuracy of point predictions from vanilla XGBoost and LightGBM in Table 6 in Appendix F.3. These methods do not provide probabilistic predictions but are tailored for point predictions. As expected, XGBoost and LightGBM outperform or tie with all the probabilistic methods. In particular, they often tie with Treeffuser, suggesting that Treeffuser provides probabilistic prediction without sacrificing average point predictions. ", "page_idx": 7}, {"type": "text", "text": "Finally, we conducted an ablation study to investigate the impact of the noise-scaling reparametrization of the score function on Treeffuser\u2019s performance. As detailed in Appendix G, noise scaling is key to achieving top accuracy and stability. ", "page_idx": 7}, {"type": "table", "img_path": "4KeSvAvNMr/tmp/af8fdcd2bea127cdd6368ff190ade28fcfe6c91c152ef8bda00dc715e637a91d.jpg", "table_caption": ["5.3 Sales forecasting with long tails and count data "], "table_footnote": ["Table 2: Walmart dataset metrics (lower is better). The evaluation is on the last 30 days of data. Treeffuser provides the best probabilistic predictions alongside NGBoost Poisson. Deep ensembles excel at point predictions. Powers of tens are factorized out of each row in rightmost column. "], "page_idx": 7}, {"type": "text", "text": "We further demonstrate the applicability of Treeffuser on a publicly available dataset [55] for sales forecasting under uncertainty. The goal is to forecast the number of units sold for a product given features such as its price, its type (e.g., food, cloths), and past sales. ", "page_idx": 7}, {"type": "text", "text": "This task is challenging due to zero inflation and long tails in the distribution of item sales. (E.g., umbrella sales are typically low but can spike during rainy weeks.) It is even more challenging for a diffusion model like Treeffuser, which is designed for continuous responses and not count data. ", "page_idx": 7}, {"type": "text", "text": "We use five years of sales data from ten Walmart stores (a large American retail chain). We randomly select 1,000 products, training on 112,000 data points from the first 1,862 days and evaluating 10,000 other data points for the remaining 30 days. ", "page_idx": 7}, {"type": "text", "text": "In addition to the previous baselines, we include NGBoost Poisson, a parametric model specifically designed for count data. We evaluate the predictions returned by each method in three ways. ", "page_idx": 7}, {"type": "text", "text": "CRPS and accuracy metrics. First, we compute the same evaluation metrics as in the previous experiments. We benchmark against the methods in the experiment and the methods in section 5.2. ", "page_idx": 7}, {"type": "text", "text": "The results are reported in Table 2. We find that Treeffuser again proves competitive, achieving a better CRPS than all methods and comparable MAE and RMSE. ", "page_idx": 8}, {"type": "text", "text": "Posterior predictive checks. Second, we perform held-out predictive checks [56], examining six statistics on the number of items sold: the total count of zero sales, the highest sales figure, and sales figures at the 0.99, 0.999, and 0.9999 quantiles (lower quantiles are well captured by all methods). We produce probabilistic predictions of these quantities by returning their empirical distributions as induced by the samples generated by the models. Fig. 2 compares the observed values against the probabilistic predictions. Treeffuser best captures the proportion of zeros and performs as well as NGBoost-Poisson in modeling the behavior of the tails. ", "page_idx": 8}, {"type": "image", "img_path": "4KeSvAvNMr/tmp/cf347dee39bc75be559d29667ab56da5f475160e7c62c622686819acc2c1bd7f.jpg", "img_caption": ["Figure 2: Posterior predictive checks for Treeffuser, NGBoost Poisson, and quantile regression. Red dashed line shows the realized value on the test set. Treeffuser best captures the inflation point at zero and performs well on the tails. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Newsvendor model. Finally, we illustrate the practical relevance of accurate probabilistic predictions with an application to inventory management, using the newsvendor model [57]. Assume that every day we decide how many units $q$ of an item to buy. We buy at a cost $c$ and sell at a price $p$ . However, the demand $y$ is random, introducing uncertainty in our decision. The goal is to maximize the expected profit: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\boldsymbol q}p\\mathbb{E}\\left[\\operatorname*{min}(\\boldsymbol q,\\boldsymbol y)\\right]-c\\boldsymbol q.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "The solution to the newsvendor problem is to buy $\\begin{array}{r}{q=F^{-1}\\left(\\frac{p-c}{p}\\right)}\\end{array}$ units, where $F^{-1}$ is the quantile function of the distribution of $y$ . ", "page_idx": 8}, {"type": "text", "text": "We apply this model to evaluate the inventory decisions induced by each method on the Walmart dataset. To compute proftis, we use the observed prices and assume a margin of $50\\%$ over all products. We let Treeffuser, NGBoost-Poisson, and quantile regression learn the conditional distribution of the demand of each item, estimate their quantiles, and thus determine the optimal quantity to buy. ", "page_idx": 8}, {"type": "text", "text": "Fig. 3 plots the cumulative profits over the last 30 days of data. Treeffuser outperforms quantile regression by a large margin and performs comparably to NGBoost-Poisson. This is coherent with our PPC results in Fig. 2, showing better quantile estimations for Treeffuser. This demonstrates that Treeffuser delivers competitive probabilistic predictions even for count data responses, a scenario it was not specifically designed to handle. ", "page_idx": 8}, {"type": "text", "text": "5.4 Runtime Performance Overview ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We measure Treeffuser\u2019s performance in terms of training and inference speed across different datasets. On the M5 dataset, using default parameters, Treeffuser completed training in 53.2 seconds on a MacBook Pro M3 Max and generated 10,000 samples (one per test data point) in 2.53 seconds. We conducted additional benchmarking experiments on both the UCI and M5 datasets, with results detailed in Appendix H. (Further discussion on the model\u2019s time complexity is also provided in that appendix.) Details about the computational resources used in all of our experiments are available in Appendix D. ", "page_idx": 8}, {"type": "image", "img_path": "4KeSvAvNMr/tmp/82e1ad271ae7c68b2526ac11c6912f8a52a540fa3553392984aab8ee06199406.jpg", "img_caption": [], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Figure 3: Cumulative proftis by method on an inventory management problem. Treeffuser produces more accurate probabilistic predictions yielding higher profits. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We have introduced Treeffuser, a new model for probabilistic prediction from tabular data. ", "page_idx": 9}, {"type": "text", "text": "Treeffuser combines conditional diffusions models with gradient-boosted trees. It can capture arbitrarily complex distributions without requiring any data-specific modeling or tuning. It is amenable to fast CPU learning and naturally handles categorical data and missing values. We have demonstrated that Treeffuser outperforms state-of-the-art methods in probabilistic regression across datasets and metrics. These characteristics make Treeffuser a flexible, easy-to-use, and robust model for probabilistic predictions. ", "page_idx": 9}, {"type": "text", "text": "One limitation of our diffusion-based approach is the need to numerically solve an SDE to generate samples, which can be costly when producing many samples. Recent advances, such as progressive distillation [58] and consistency models [59], address this issue. Applying these methods to Treeffuser is a direction for future work. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Aristeidis Mystakidis, Evangelia Ntozi, Konstantinos Afentoulis, Paraskevas Koukaras, Paschalis Gkaidatzis, Dimosthenis Ioannidis, Christos Tjortjis, and Dimitrios Tzovaras. Energy generation forecasting: Elevating performance with machine and deep learning. Computing, 105(8):1623\u20131645, 2023.   \n[2] Jillian M. Clements, Di Xu, Nooshin Yousef,i and Dmitry Efimov. Sequential deep learning for credit risk monitoring with tabular financial data, 2020. arXiv preprint arXiv:2012.15330.   \n[3] Snigdha Somani, Adam J Russak, Filippo Richter, Simon Zhao, Akhil Vaid, Farah Chaudhry, Jason K De Freitas, Niyati Naik, Riccardo Miotto, Girish N Nadkarni, Jagat Narula, Edgar Argulian, and Benjamin S Glicksberg. Deep learning and the electrocardiogram: Review of the current state-of-the-art. Europace, 23(8):1179\u20131191, 2021.   \n[4] Anshuman Gupta and Costas D Maranas. Managing demand uncertainty in supply chain planning. Computers & Chemical Rngineering, 27(8-9):1219\u20131227, 2003.   \n[5] Yacov Y Haimes. Risk Modeling, Assessment, and Management. John Wiley & Sons, 2011.   \n[6] Philippe Jorion. Value at Risk: The New Benchmark for Managing Financial Risk. McGraw-Hill, 2007.   \n[7] James W. Taylor and Kimberly S. Taylor. Combining probabilistic forecasts of covid-19 mortality in the united states. European Journal of Operational Research, 304(1):25\u201341, 2023.   \n[8] Ziqiu Kang, Cagatay Catal, and Bedir Tekinerdogan. Machine learning applications in production lines: A systematic literature review. Computers & Industrial Engineering, 149:106773, 2020.   \n[9] Anna L. Buczak and Erhan Guven. A survey of data mining and machine learning methods for cyber security intrusion detection. IEEE Communications Surveys and Tutorials, 18(2): 1153\u20131176, 2016.   \n[10] Thi-Thu-Huong Le, Yustus Eko Oktian, and Howon Kim. XGBoost for imbalanced multiclass classification-based industrial Internet of Things intrusion detection systems. Sustainability, 14 (14), 2022.   \n[11] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2020.   \n[12] Jerome H Friedman. Greedy function approximation: A gradient boosting machine. Annals of Statistics, 29(5):1189\u20131232, 2001.   \n[13] Zhendong Wang, Yifan Jiang, Huangjie Zheng, Peihao Wang, Pengcheng He, Zhangyang Wang, Weizhu Chen, and Mingyuan Zhou. Patch Diffusion: Faster and more data-efficient training of diffusion models, 2023. arXiv preprint arXiv:2304.12526.   \n[14] Tony Duan, Anand Avati, Daisy Yi Ding, Khanh K. Thai, Sanjay Basu, Andrew Y. Ng, and Alejandro Schuler. NGBoost: Natural gradient boosting for probabilistic prediction, 2020. arXiv preprint arXiv:1910.03225.   \n[15] Jonathan Brophy and Daniel Lowd. Instance-based uncertainty estimation for gradient-boosted regression trees. In Advances in Neural Information Processing Systems, volume 35, pages 11145\u201311159, 2022.   \n[16] Nicolai Meinshausen and Greg Ridgeway. Quantile regression forests. Journal of Machine Learning Research, 7(6), 2006.   \n[17] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. In Advances in Neural Information Processing Systems, volume 30, 2017.   \n[18] Brian D.O. Anderson. Reverse-time diffusion equation models. Stochastic Processes and their Applications, 12(3):313\u2013326, 1982.   \n[19] Pascal Vincent. A connection between score matching and denoising autoencoders. Neural Computation, 23(7):1661\u20131674, 2011.   \n[20] Trevor Hastie, Robert Tibshirani, and Jerome H Friedman. The Elements of Statistical Learning: Data Mining, Inference, and Prediction, volume 2. Springer, 2009.   \n[21] Tianqi Chen and Carlos Guestrin. XGBoost: A scalable tree boosting system. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201916. ACM, 2016.   \n[22] Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu. LightGBM: A highly efficient gradient boosting decision tree. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.   \n[23] Liudmila Prokhorenkova, Gleb Gusev, Aleksandr Vorobev, Anna Veronika Dorogush, and Andrey Gulin. CatBoost: Unbiased boosting with categorical features, 2019. arXiv preprint arXiv:1706.09516.   \n[24] Georgios Batzolis, Jan Stanczuk, Carola-Bibiane Sch\u00f6nlieb, and Christian Etmann. Conditional image generation with score-based diffusion models, 2021. arXiv preprint arXiv:2111.13606.   \n[25] Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J. Fleet, and Mohammad Norouzi. Image super-resolution via iterative refinement, 2021. arXiv preprint arXiv:2104.07636. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "[26] Yusuke Tashiro, Jiaming Song, Yang Song, and Stefano Ermon. CSDI: Conditional score-based diffusion models for probabilistic time series imputation. In Advances in Neural Information Processing Systems, volume 34, pages 24804\u201324816. Curran Associates, Inc., 2021. ", "page_idx": 11}, {"type": "text", "text": "[27] Peter E. Kloeden and Eckhard Platen. Numerical Solution of Stochastic Differential Equations. Stochastic Modelling and Applied Probability. Springer Berlin, Heidelberg, 1992. ", "page_idx": 11}, {"type": "text", "text": "[28] Stability AI. Introducing stable diffusion. https://stability.ai/blog/ stable-diffusion-public-release, 2022. ", "page_idx": 11}, {"type": "text", "text": "[29] MidJourney. Midjourney. https://www.midjourney.com/, 2022. ", "page_idx": 11}, {"type": "text", "text": "[30] OpenAI. Dall\u00b7e 2. https://openai.com/index/dall-e-2/, 2022. ", "page_idx": 11}, {"type": "text", "text": "[31] Joseph L. Watson, David Juergens, Nathaniel R. Bennett, Brian L. Trippe, Jason Yim, Helen E. Eisenach, Woody Ahern, Andrew J. Borst, Robert J. Ragotte, Lukas F. Milles, Basile I. M. Wicky, Nikita Hanikel, Samuel J. Pellock, Alexis Courbet, William Sheffler, Jue Wang, Preetham Venkatesh, Isaac Sappington, Susana V\u00e1zquez Torres, Anna Lauko, Valentin De Bortoli, Emile Mathieu, Sergey Ovchinnikov, Regina Barzilay, Tommi S. Jaakkola, Frank DiMaio, Minkyung Baek, and David Baker. De novo design of protein structure and function with rfdiffusion. Nature, 620(7976):1089\u20131100, 2023.   \n[32] Kashif Rasul, Calvin Seward, Ingmar Schuster, and Roland Vollgraf. Autoregressive denoising diffusion models for multivariate probabilistic time series forecasting. In International Conference on Machine Learning, pages 8857\u20138868. PMLR, 2021.   \n[33] Chenhao Niu, Yang Song, Jiaming Song, Shengjia Zhao, Aditya Grover, and Stefano Ermon. Permutation invariant graph generation via score-based generative modeling. In International Conference on Artificial Intelligence and Statistics, pages 4474\u20134484. PMLR, 2020.   \n[34] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. In Advances in Neural Information Processing Systems, volume 34, pages 8780\u20138794, 2021.   \n[35] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance, 2022. arXiv preprint arXiv:2207.12598.   \n[36] Hyungjin Chung, Jeongsol Kim, Michael T. Mccann, Marc L. Klasky, and Jong Chul Ye. Diffusion posterior sampling for general noisy inverse problems, 2023. arXiv preprint arXiv:2209.14687.   \n[37] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided image synthesis and editing with stochastic differential equations, 2022. arXiv preprint arXiv:2108.01073.   \n[38] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents, 2022. arXiv preprint arXiv:2204.06125.   \n[39] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. Highresolution image synthesis with latent diffusion models. In Conference on Computer Vision and Pattern Recognition, pages 10684\u201310695, 2022.   \n[40] Akim Kotelnikov, Dmitry Baranchuk, Ivan Rubachev, and Artem Babenko. TabDDPM: Modelling tabular data with diffusion models. In International Conference on Machine Learning, pages 17564\u201317579. PMLR, 2023.   \n[41] Xizewen Han, Huangjie Zheng, and Mingyuan Zhou. CARD: Classification and regression diffusion models. In Advances in Neural Information Processing Systems, volume 35, pages 18100\u201318115, 2022.   \n[42] Shuhan Zheng and Nontawat Charoenphakdee. Diffusion models for missing value imputation in tabular data, 2022. arXiv preprint arXiv:2210.17128.   \n[43] Nils Lehmann. lightning-uq-box 0.1.0. https://pypi.org/project/lightning-uq-box/, 2024.   \n[44] Alexia Jolicoeur-Martineau, Kilian Fatras, and Tal Kachman. Generating and imputing tabular data via diffusion and flow-based gradient-boosted trees. In International Conference on Artificial Intelligence and Statistics, pages 1288\u20131296. PMLR, 2024.   \n[45] Olivier Sprangers, Sebastian Schelter, and Maarten de Rijke. Probabilistic gradient boosting machines for large-scale probabilistic regression. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD \u201921. ACM, 2021.   \n[46] Radford M Neal. Bayesian Learning for Neural Networks, volume 118. Springer Science & Business Media, 2012.   \n[47] Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In International Conference on Machine Learning, pages 1050\u20131059. PMLR, 2016.   \n[48] Ivan Kobyzev, Simon J.D. Prince, and Marcus A. Brubaker. Normalizing flows: An introduction and review of current methods. IEEE Transactions on Pattern Analysis and Machine Intelligence, 43(11):3964\u20133979, 2021.   \n[49] Pavel Izmailov, Polina Kirichenko, Marc Finzi, and Andrew Gordon Wilson. Semi-supervised learning with normalizing flows, 2019. arXiv preprint arXiv:1912.13025.   \n[50] Domagoj Cevid, Loris Michel, Jeffrey N\u00e4f, Peter B\u00fchlmann, and Nicolai Meinshausen. Distributional random forests: Heterogeneity adjustment and multivariate distributional regression. Journal of Machine Learning Research, 23(333):1\u201379, 2022.   \n[51] Markelle Kelly, Rachel Longjohn, and Kolby Nottingham. The UCI machine learning repository, 2023. URL https://archive.ics.uci.edu.   \n[52] James E Matheson and Robert L Winkler. Scoring rules for continuous probability distributions. Management Science, 22(10):1087\u20131096, 1976.   \n[53] Tilmann Gneiting and Adrian E Raftery. Strictly proper scoring rules, prediction, and estimation. Journal of the American Statistical Association, 102(477):359\u2013378, 2007.   \n[54] Mathias Blicher Bjerreg\u00e5rd, Jan Kloppenborg M\u00f8ller, and Henrik Madsen. An introduction to multivariate probabilistic forecast evaluation. Energy and AI, 4:100058, 2021.   \n[55] Spyros Makridakis Addison Howard. M5 Forecasting - Accuracy, 2020. URL https:// kaggle.com/competitions/m5-forecasting-accuracy.   \n[56] Andrew Gelman, Xiao-Li Meng, and Hal Stern. Posterior predictive assessment of model ftiness via realized discrepancies. Statistica Sinica, 6:733\u2013807, 1996.   \n[57] Kenneth J Arrow, Theodore Harris, and Jacob Marschak. Optimal inventory policy. Econometrica: Journal of the Econometric Society, pages 250\u2013272, 1951.   \n[58] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models, 2022. arXiv preprint arXiv:2202.00512.   \n[59] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models, 2023. arXiv preprint arXiv:2303.01469.   \n[60] Roger Koenker and Kevin F Hallock. Quantile regression. Journal of Economic Perspectives, 15(4):143\u2013156, 2001. ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Acronyms ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "CRPS continuous ranked probability score. 7, 8, 9 ", "page_idx": 12}, {"type": "text", "text": "GBT gradient-boosted tree. 2, 3, 4, 5, 16   \nMAE mean absolute error. 8, 9   \nRMSE root mean squared error. 8, 9   \nSDE stochastic differential equation. 3, 4, 6, 14, 15, 16 ", "page_idx": 12}, {"type": "text", "text": "A A primer on unconditional diffusion models ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A diffusion model is a generative model that consists of two processes, a forward diffusion, and a reverse diffusion. The forward process takes an unknown target distribution $\\pi(\\boldsymbol{y})$ and continuously transforms it into a simple distribution $p_{\\mathrm{simple}}(\\boldsymbol{y})$ , typically a Gaussian. It does so by progressively adding noise to samples from $\\pi$ , such as our data. ", "page_idx": 13}, {"type": "text", "text": "The reverse process transforms the simple distribution back into the target distribution by denoising any perturbed samples from $p_{\\mathrm{simple}}$ . If we can learn the reverse process, we can effectively access the target distribution $\\pi$ : first draw samples from $p_{\\mathrm{simple}}$ and then use the reverse transformation, as guided by the score, to map them back to $\\pi$ . ", "page_idx": 13}, {"type": "text", "text": "One way to learn this noising-denoising process is to estimate the score function, the gradient of the log probability of the noisy data with respect to the data itself. ", "page_idx": 13}, {"type": "text", "text": "We detail the three main components of score-based diffusions\u2013the forward process, the reverse process, and the estimation of the score function\u2013below. ", "page_idx": 13}, {"type": "text", "text": "The forward diffusion process. A diffusion model is described by the following generative process: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\mathrm{Draw}\\ y(0)\\sim\\pi,}\\\\ {\\mathrm{Evolve}\\ y(t)\\mathrm{~until~}T\\mathrm{~according~to:~}\\ \\mathbf{d}y(t)=f(y(t),t)\\mathbf{d}t+g(t)\\mathbf{d}w(t),}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where the process of evolving the stochastic differential equation (SDE) involves a standard Brownian motion ${\\pmb w}(t)$ and model parameters $f:\\mathbb{R}^{d_{y}}\\times[0,T]\\rightarrow\\dot{\\mathbb{R}}^{d_{y}}$ and $g:[0,T]\\rightarrow\\mathbb{R}$ , respectively called the drift and diffusion functions. ", "page_idx": 13}, {"type": "text", "text": "The SDE in Eq. (14) determines the evolution of $\\pmb{y}(t)$ over time, thereby inducing a distribution of $\\pmb{y}(t)$ at each $t$ , that we write $p_{t}(\\pmb{y}(t))$ . We further write the distribution of $\\pmb{y}(t)$ conditional on another $\\pmb{y}(u)$ as $p_{u t}({\\pmb y}(t)\\mid{\\pmb y}(u))$ . In practice, $f$ and $g$ are simple functions such that $p_{0t}({\\pmb y}(t)\\mid{\\pmb y}(0))$ has a closed form. In fact, if the drift $f$ is affine in $\\pmb{y}(t)$ and the diffusion function $g$ is a scalar that does not depend on $\\pmb{y}(t)$ , the distribution $p_{0t}$ is always Gaussian, with a mean we write $m(t;{\\pmb y}(0))$ and a covariance multiple of the identity we write $\\sigma(\\dot{t})^{2}I$ . ", "page_idx": 13}, {"type": "text", "text": "W choose the time horizon $T$ such that the resulting marginal distribution $\\begin{array}{r}{p_{T}(\\pmb{y}(T))=\\int_{\\pmb{y}^{\\prime}}p_{T}(\\pmb{y}(T)\\mid}\\end{array}$ ${\\pmb y}(0)={\\pmb y}^{\\prime})\\pi({\\pmb y}^{\\prime})\\mathrm{d}{\\pmb y}^{\\prime}$ is a simple distribution $p_{\\mathrm{simple}}$ agnostic to $\\pi$ . The intuition is that as the diffusion progresses, the initial density $\\pi$ is forgotten. ", "page_idx": 13}, {"type": "text", "text": "We detail our choice of $f,g$ and $T$ in Appendix C, which is the variance exploding SDE (VESDE) in Song et al. [11]. This choice induces a trivial closed form expression for the functions $m$ and $\\sigma$ . ", "page_idx": 13}, {"type": "text", "text": "There are no model parameters to learn in the diffusion model, and the conditional distributions $p_{0t}({\\pmb y}(t)\\mid{\\pmb y}(0))$ are trivial to compute. The challenge is to learn how to compute (or sample from) the conditional distribution $p_{T t}(\\pmb{y}(t)\\mid\\pmb{y}(T))$ . This is called reversing the diffusion process. Knowing $p_{T t}$ is key to learning the target distribution $\\pi$ , which can be written as $\\pi(\\pmb{y}(0))=\\mathbb{E}_{p_{T}}[p_{T0}(\\pmb{y}(0)\\mid\\pmb{y}(\\bar{T}))]$ where $p_{T}=p_{\\mathrm{simple}}$ is known. ", "page_idx": 13}, {"type": "text", "text": "The reverse diffusion process. Consider the following SDE that is reversed in time from $T$ to $0$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\tilde{\\pmb{y}}(T)\\sim p_{\\mathrm{simple}},}\\\\ {\\mathsf{d}\\tilde{\\pmb{y}}_{t}=[f(\\tilde{\\pmb{y}}(t),t)-g(t)^{2}\\nabla_{\\tilde{\\pmb{y}}(t)}\\log p_{t}(\\tilde{\\pmb{y}}(t))]\\mathsf{d}t+g(t)\\mathsf{d}\\tilde{\\pmb{w}}(t),}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\tilde{\\pmb{w}}(t)$ is a standard Brownian with reversed time and $p_{t}$ is the density of $\\pmb{y}(t)$ defined by the forward SDE (1). Similar to the forward case, the reverse SDE induces a distribution $\\tilde{p}_{t}$ for $\\tilde{\\pmb{y}}(t)$ at each $t$ . Anderson [18] shows that, if we denote by $\\tilde{p}_{T t}$ the conditional distribution of $\\tilde{\\pmb y}(t)$ given $\\tilde{\\pmb{y}}(T)$ , then $\\tilde{p}_{T t}=p_{T t}$ for any time $t$ . In other words, $\\tilde{\\pmb{y}}(t)\\mid\\tilde{\\pmb{y}}(T)=\\pmb{a}$ and ${\\pmb y}(t)\\mid{\\pmb y}(T)={\\pmb a}$ have the same distribution for all $\\textbf{\\em a}$ . Since we designed the diffusion such that $p_{T}\\approx p_{\\mathrm{simple}}=\\tilde{p}_{T}$ , we have in particular $\\tilde{\\pmb{y}}(t)\\sim\\pmb{y}(t)$ . ", "page_idx": 13}, {"type": "text", "text": "SDE solvers let us sample from distributions that are solutions of an SDE; hence, we can obtain samples of $\\pi(\\boldsymbol{y})$ by solving Eq. (15). However, the function $y\\mapsto\\nabla_{y}\\log p_{t}(y)$ , called the score, is usually unknown and needs to be estimated. ", "page_idx": 13}, {"type": "text", "text": "Estimating the score. Vincent [19] shows that the score can be estimated from trajectories of the SDE as the minimizer of the following objective function: ", "page_idx": 14}, {"type": "equation", "text": "$$\n[y\\mapsto\\nabla_{y}\\log p_{t}(y)]=\\mathop{\\mathrm{arg\\,min}}_{s\\in{\\mathcal{S}}}\\mathbb{E}_{t}\\mathbb{E}_{\\pi}\\mathbb{E}_{p_{0t}}\\left[\\left\\|\\nabla_{y(t)}\\log p_{0t}(y(t)\\mid y(0))-s(y(t),t)\\right\\|^{2}\\right],\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $S=\\{s:\\mathbb{R}^{d_{y}}\\times[0,T]\\rightarrow\\mathbb{R}^{d_{y}}\\}$ is the set of all possible score functions indexed by time $t$ , and $\\mathbb{E}_{t}$ can be an expectation over any distribution of $t$ whose support is exactly $[0,T]$ . Since $p_{0t}$ is a normal distribution $\\mathcal{N}(m(t;\\pmb{y}(0)),\\sigma(t)^{2}I)$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\nabla_{\\pmb{y}(t)}\\log p_{0t}(\\pmb{y}(t)\\mid\\pmb{y}(0))=\\frac{m(t;\\pmb{y}(0))-\\pmb{y}(t)}{\\sigma(t)^{2}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "In practice, the expectations in Eq. (16) are approximated using the empirical distribution formed by the observed $\\textit{\\textbf{y}}$ for $\\mathbb{E}_{\\pi}$ , and by sampling the simple and known forward SDE trajectories $\\pmb{y}(t)\\mid$ ${\\pmb y}(0)\\,=\\,{\\pmb y}$ for $\\mathbb{E}_{p_{0t}}$ . The objective can then be minimized by parametrizing $s$ with any function approximator, e.g., a neural network. The score estimator can then be used to reverse the diffusion process and estimate the target distribution $\\pi(\\pmb{y})$ . ", "page_idx": 14}, {"type": "text", "text": "B Proofs of the main theorems ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we provide the proofs for the two main theorems in the text. ", "page_idx": 14}, {"type": "text", "text": "Theorem 1 (Optimal Conditional Objective). Define $S^{*}$ as the solution of Eq. (9). Then, for almost all $\\pmb{x},\\pmb{y},t$ with respect to $\\pi(x,y)$ and the Lebesgue measure on $t\\in[0,T]$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\nS^{*}(\\pmb{y},t,\\pmb{x})=\\nabla_{\\pmb{y}}\\log p_{\\pmb{x},t}(\\pmb{y}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. For conciseness, define for any function $s:\\mathbb{R}^{d_{y}}\\,\\times\\,[0,T]\\,\\rightarrow\\,\\mathbb{R}^{d_{y}}$ the quantity: $r(\\pmb{x},s)=$ $\\begin{array}{r l}&{\\mathbb{E}_{t}\\mathbb{E}_{\\pi(y_{\\alpha}(0))}\\mathbb{E}_{p_{x,0t}}\\left[\\big\\|\\nabla_{y_{x}(t)}\\log p_{0t}\\big(y_{x}\\big(t)\\mid y_{x}(0)\\big)-s(y_{x}(t),t,x)\\big\\|^{2}\\right]}\\\\ &{S:\\mathbb{R}^{d_{y}}\\times[0,T]\\times\\mathbb{R}^{d_{x}}\\to\\mathbb{R}^{d_{y}}\\mathrm{~and~}x\\in\\mathbb{R}^{d_{x}}\\mathrm{~the~function~}S_{x}:(y,t)\\mapsto}\\end{array}$ . Also write for any function $S_{x}:({\\pmb y},t)\\mapsto S({\\pmb y},t,{\\pmb x})$ . ", "page_idx": 14}, {"type": "text", "text": "The function $S^{*}$ is characterized as, $S^{*}\\in\\arg\\operatorname*{min}_{S\\in S^{+}}\\mathbb{E}_{\\pi(\\pmb{x})}[r(\\pmb{x},S_{\\pmb{x}})]$ where $S^{+}=\\{S:\\mathbb{R}^{d_{y}}\\ \\times$ $[0,T]\\times\\mathbb{R}^{d_{x}}\\rightarrow\\mathbb{R}^{d_{y}}\\}$ ", "page_idx": 14}, {"type": "text", "text": "Define $\\Omega=\\{{\\pmb x}\\in\\mathbb{R}^{d_{x}}\\mid S^{*}({\\pmb y},t,{\\pmb x})\\not\\in\\arg\\operatorname*{min}_{s}r({\\pmb x},s)\\}$ , we will show that $\\pi(\\Omega)=0$ . ", "page_idx": 14}, {"type": "text", "text": "By [19], we know that $\\nabla_{\\pmb{y}}\\log p_{\\pmb{x},t}(\\pmb{y})\\in\\arg\\operatorname*{min}_{s}r(\\pmb{x},s)$ for all $\\textbf{\\em x}$ , so by definition of $\\Omega$ , we have: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\forall x\\in\\Omega,\\quad r({\\pmb x},S_{{\\pmb x}}^{*})>r({\\pmb x},({\\pmb y},t)\\mapsto\\nabla_{\\pmb y}\\log p_{{\\pmb x},t}({\\pmb y})).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We further have for any other x $\\cdot\\not\\in\\Omega,\\,r(\\pmb{x},S_{x}^{\\ast})=r(\\pmb{x},(\\pmb{y},t)\\mapsto\\nabla_{\\pmb{y}}\\log p_{\\pmb{x},t}(\\pmb{y})).$ ", "page_idx": 14}, {"type": "text", "text": "If $\\pi(\\Omega)\\quad>\\quad0.$ , then integrating Eq. (18) over $\\pi(x)$ will yield $\\begin{array}{r l}{\\mathbb{E}_{\\pi(\\pmb{x})}[r(\\pmb{x},S_{\\pmb{x}}^{*})]}&{{}>}\\end{array}$ $\\mathbb{E}_{\\pi(\\pmb{x})}[r(\\pmb{x},((\\pmb{y},t,\\pmb{x})\\~\\mapsto~\\nabla_{\\pmb{y}}\\log p_{\\pmb{x},t}(\\pmb{y}))_{\\pmb{x}})]$ , which is not possible by definition of $S^{*}$ . Hence $\\pi(\\Omega)=0$ . ", "page_idx": 14}, {"type": "text", "text": "Hence we have $S_{\\pmb{x}}^{*}\\in\\arg\\operatorname*{min}_{s}r(\\pmb{x},s)$ for almost any $x$ , so by [19] again, we can conclude that $S^{*}(\\pmb{y},t,\\pmb{x})=\\nabla_{\\pmb{y}}\\log p_{\\pmb{x},t}(\\pmb{y})$ for almost any $\\mathbf{\\boldsymbol{x}},t,\\mathbf{\\boldsymbol{y}}$ . ", "page_idx": 14}, {"type": "text", "text": "Theorem 2 (Treeffuser One-Dimensional Objectives). Denote $U^{*}=(U_{1}^{*},...,U_{d_{y}}^{*})$ . Then for almost all $\\pmb{x},\\pmb{y},t$ with respect to $\\pi(x,y)$ and the Lebesgue measure on $t\\in[0,T]$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\nabla_{\\pmb{y}}\\log p_{\\pmb{x},t}(\\pmb{y})=\\frac{U^{*}(\\pmb{y},t,\\pmb{x})}{\\sigma(t)}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. By definition in Eq. (12), we have ", "page_idx": 14}, {"type": "equation", "text": "$$\nU_{k}^{*}=\\underset{U_{k}}{\\arg\\operatorname*{min}}\\,\\mathbb{E}_{\\boldsymbol{x},\\boldsymbol{y}\\sim\\pi}\\mathbb{E}_{t}\\mathbb{E}_{\\zeta\\sim\\mathcal{N}(0,I_{d_{y}})}\\left[\\left(\\zeta_{k}-U_{k}\\big(h(\\zeta,t,\\boldsymbol{y}),t,\\boldsymbol{x})\\big)^{2}\\right].\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "With Theorem 1, we have $S^{*}(\\pmb{y},t,\\pmb{x})=\\nabla_{\\pmb{y}}\\log p_{\\pmb{x},t}(\\pmb{y})$ almost everywhere, with $S^{*}$ defined as ", "page_idx": 15}, {"type": "equation", "text": "$$\nS^{*}\\in\\underset{S\\in\\mathcal{S}^{+}}{\\mathrm{arg\\,min}}\\,\\mathbb{E}_{\\pi(x)}\\mathbb{E}_{t}\\mathbb{E}_{\\pi(y_{\\alpha}(0))}\\mathbb{E}_{p_{\\pi,0t}}\\left[\\left\\|\\nabla_{y_{\\alpha}(t)}\\log p_{0t}(y_{x}(t)\\mid y_{x}(0))-S(y_{x}(t),t,x)\\right\\|^{2}\\right].\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We have: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\pi(x)}\\mathbb{E}_{t}\\mathbb{E}_{\\pi(y_{\\alpha}(0))}\\mathbb{E}_{p_{\\pi,0}}\\left[\\left\\|\\nabla_{y_{\\alpha}(t)}\\log p_{0t}(y_{x}(t)\\mid y_{x}(0))-S(y_{x}(t),t,x)\\right\\|^{2}\\right]}\\\\ &{=\\mathbb{E}_{\\pi(x)}\\mathbb{E}_{t}\\mathbb{E}_{\\pi(y_{\\alpha}(0))}\\mathbb{E}_{p_{\\pi,0}}\\left[\\left\\|\\frac{m(t;y_{x}(0))-y_{x}(t)}{\\sigma(t)^{2}}-S(y_{x}(t),t,x)\\right\\|^{2}\\right]}\\\\ &{=\\displaystyle\\sum_{k=1}^{d_{y}}\\mathbb{E}_{\\pi(x)}\\mathbb{E}_{t}\\mathbb{E}_{\\pi(y_{\\alpha}(0))}\\mathbb{E}_{p_{\\pi,0}}\\left[\\frac{1}{\\sigma(t)}\\left(\\frac{m(t;y_{x}(0))_{k}-y_{x}(t)_{k}}{\\sigma(t)}-\\sigma(t)S(y_{x}(t),t,x)_{k}\\right)\\right]^{2}}\\\\ &{=\\displaystyle\\sum_{k=1}^{d_{y}}\\mathbb{E}_{\\pi(x,y)}\\mathbb{E}_{t}\\mathbb{E}_{\\zeta\\sim N(0,I_{d_{y}})}\\left[\\frac{1}{\\sigma(t)}\\left(-\\zeta_{k}-\\sigma(t)S(y+\\sigma(t)\\zeta,t,x)_{k}\\right)\\right]^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where we used the following facts: ", "page_idx": 15}, {"type": "text", "text": "\u2022 from Eq. (19) to Eq. (20): the closed-form expression of the score $S$ from Eq. (17), \u2022 from Eq. (20) to Eq. (21): expanding the norm and switching the expectations with the finite sum over $k$ , \u2022 from Eq. (21) to Eq. (22): reparametrizing the expectation of $y_{x}(t)\\ |\\ \\ y_{x}(0)$ which by definition is a normal distribution with mean $m(t;{\\pmb y}_{x}(0))$ and variance $\\sigma(t)^{2}$ . ", "page_idx": 15}, {"type": "text", "text": "The final manipulation is to remember that theorem 1 and the theorems from Vincent [19] are valid with expectations $\\mathbb{E}_{t}$ against any strictly positive measure of $t$ over $[0,T]$ . In particular, we can absorb \u03c3(1t) as a reweighted non-negative measure, and we obtain exactly the definition of U k\u2217 by defining $U(\\pmb{y},t,\\pmb{x})=\\sigma(t)S\\big(\\pmb{y}+\\sigma(t)\\zeta,t,\\pmb{x}\\big)$ which concludes the proof. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "C Treeffuser default configuration ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We use the following configuration as defaults for Treeffuser. ", "page_idx": 15}, {"type": "text", "text": "Forward diffusion. We use the variance exploding SDE [11] for all experiments, defined by setting: ", "page_idx": 15}, {"type": "equation", "text": "$$\nf(\\pmb{y},t)=0\\quad\\mathrm{and}\\quad g(t)=\\sqrt{\\frac{\\mathrm{d}[\\sigma(t)^{2}]}{\\mathrm{d}t}},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\sigma$ is a given increasing function defined by, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sigma(t)=\\alpha_{\\mathrm{min}}\\left(\\frac{\\alpha_{\\mathrm{max}}}{\\alpha_{\\mathrm{min}}}\\right)^{t}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and $\\alpha_{\\mathrm{min}}=0.01$ and $\\alpha_{\\mathrm{max}}=20$ . For all our experiments we let $t\\in[0,1]$ (i.e. $T=1$ ). ", "page_idx": 15}, {"type": "text", "text": "Gradient-boosted tree (GBT) parameters and dataset repetitions. We provide a short description of each hyper-parameter of the model alongside the default value. Treeffuser uses LightGBM [22] to learn the GBTs. ", "page_idx": 15}, {"type": "text", "text": "\u2022 n estimators (3000): Specifies the maximum number of trees that will be fit, regardless of whether the stopping criterion is met.   \n\u2022 learning rate (0.1): Specifies the shrinkage to use for every tree.   \n\u2022 num leaves (31): Specifies the maximum number of leaves a tree can have.   \n\u2022 early stopping rounds (50): Specifies how long to wait without a validation loss improvement before stopping.   \n\u2022 n repeats (30): Specifies how many Monte Carlo samples to draw per data point to estimate $\\mathbb{E}_{t,\\zeta}$ in equation Eq. (9). ", "page_idx": 15}, {"type": "image", "img_path": "4KeSvAvNMr/tmp/f3a3af58c25e1559f1508b30f355323dd20f8514a25fca3e8d142bbe7f4bba55.jpg", "img_caption": ["Figure 4: Visualization of ground-truth samples for the one-dimensional synthetic datasets used in the empirical studies. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "4KeSvAvNMr/tmp/7e461c7dab43bb9ed83e3ba9bb122885e2715fe19bf28314bb6b2d399e5c1606.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "D Experiments: description of computer resources ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "All benchmark tasks presented in Section 5.1 and Section 5.2 were run on a cluster with one job per triplet of (model, dataset, split index). ", "page_idx": 16}, {"type": "text", "text": "The real world experiments totalled 1135h for 700 tasks (10 datasets, 7 methods, 10 splits). Each task was allocated 4 cpus. ", "page_idx": 16}, {"type": "text", "text": "The synthetic experiments totalled 290 hours, for 800 tasks (16 datasets, 5 methods, 10 splits). Each task was allocated 2 CPU. ", "page_idx": 16}, {"type": "text", "text": "The M5 experiments were run on a single MacBook Pro over a couple hours. Other figures such as Fig. 1 were also generated on a MacBook pro in a few seconds. ", "page_idx": 16}, {"type": "text", "text": "E Experiments on synthetic data (supplement) ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "E.1 Arbitrarily complex synthetic data ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We provide details on the three synthetic data experiments presented in Section 5.1 and illustrated in Fig. 1. Each experiment introduces a different distribution of the response variable $y$ given the covariate $x$ . The first experiment generates multimodal responses from a branching Gaussian mixture, the second experiment generates inflated responses from a mixture of a shifted Gamma distribution and an atomic measure, and the third experiment generates two-dimensional responses from a nonlinear multioutput regression. ", "page_idx": 16}, {"type": "text", "text": "We provide additional visualization of samples for the one-dimensional datasets in Fig. 4. ", "page_idx": 16}, {"type": "text", "text": "Models We assume that $x\\sim\\mathrm{Uniform}([0,1])$ and model the conditional distribution of the response $y$ given $x$ differently for each of the synthetic dataset. Scatter plots of synthetic data from these distributions are provided in Fig. 4 ", "page_idx": 16}, {"type": "text", "text": "Branching Gaussian mixture. We generate multimodal responses from a mixture of equally-weighted Gaussians. In our experiments, we fix the scale $\\sigma=0.05$ and let the number of components and their means scale with $x$ as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\ny\\mid x\\sim\\left\\{\\begin{array}{l l}{\\mathrm{GaussianMixture}_{\\sigma}(x,-x),}&{0\\leq x\\leq1/3;}\\\\ {\\mathrm{GaussianMixture}_{\\sigma}(x,2/3-x,-x),}&{1/3<x\\leq2/3;}\\\\ {\\mathrm{GaussianMixture}_{\\sigma}(x,4/3-x,2/3-x,-x),}&{2/3\\leq x\\leq1;}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where GaussianMixture $_\\sigma(\\mu_{1},\\mu_{2},\\dots,\\mu_{K})$ denotes a Gaussian mixture distribution with $K$ equallyweighted components, each with mean $\\mu_{k}$ and scale $\\sigma$ . ", "page_idx": 16}, {"type": "text", "text": "Covariate inflated Gamma. We generate inflated responses through the following mixture of a shifted Gamma distribution and an atomic measure: ", "page_idx": 17}, {"type": "equation", "text": "$$\nz\\sim\\mathbf{Bernoulli}(p),\\qquad y\\mid x,z\\sim\\left\\{{\\mathrm{ShiftedGamma}}(k,\\theta,x),\\quad z=0;\\atop\\delta_{x},\\right.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where ShiftedGamma $(k,\\theta,x)$ denotes a Gamma distribution with scale $k$ and shape $\\theta$ shifted by $x$ . In words, given $x$ , we set $y$ to be equal to $x$ with probability $p$ , and to be drawn from a Gamma shifted by $x$ otherwise. Hence, the conditional distribution of $y$ given $x$ is $x$ -inflated and its support $[x,\\infty)$ changes with $x$ . In our experiments, we set $p=0.15,k=2,\\theta=1$ . ", "page_idx": 17}, {"type": "text", "text": "Nonlinear multioutput regression. We generate two-dimensional responses from a density $p(\\boldsymbol{y}\\mid$ $\\begin{array}{r}{x)\\propto\\exp\\left(-\\frac{\\mathrm{dist}(\\pmb{y},C(x))^{2}}{2\\sigma^{2}}\\right)}\\end{array}$ where $\\sigma\\,=\\,0.05,\\,\\mathrm{dist}(a,B)$ measures the shortest Euclidean distance from $a\\in\\mathbb R$ to the set $B\\subset\\mathbb{R}^{2}$ , and $C(x)$ is a circular arc centered at $(0,0)$ with radius radius $(x)$ and angles of extremities $\\theta_{1},\\theta_{2}$ evolving as functions of $x$ . Intuitively, they interpolate the circular arc represented in Section 1. ", "page_idx": 17}, {"type": "text", "text": "The functions are defined as follows. ", "page_idx": 17}, {"type": "text", "text": "\u2022 If $x\\leq0.5$ $\\begin{array}{r l}&{-\\mathrm{\\radius}(x)=\\ell(x)\\cdot1+r(x)\\cdot0.1,}\\\\ &{-\\ \\theta_{0}(x)=\\ell(x)\\cdot(-0.05)+r(x)\\cdot(-0.375),}\\\\ &{-\\ \\theta_{1}(x)=\\ell(x)\\cdot0.3+r(x)\\cdot0.625,}\\end{array}$ wher $:\\ell(x)=(0.5-x)/(0.5-0.17),r(x)=1-\\ell(x).$   \n\u2022 If $x>0.5$ D ${\\begin{array}{r l}&{-\\ \\operatorname{radius}(x)=\\ell(x)\\cdot0.1+r(x)\\cdot1,}\\\\ &{-\\ \\theta_{0}(x)=\\ell(x)\\cdot0.125+r(x)\\cdot0.45,}\\\\ &{-\\ \\theta_{1}(x)=\\ell(x)\\cdot1.125+r(x)\\cdot0.8,}\\\\ &{{\\mathrm{~}}\\operatorname{\\rhere}\\ \\ell(x)=(0.83-x)/(0.83-0.5),r(x)=1-\\ell(x).}\\end{array}}$ ", "page_idx": 17}, {"type": "text", "text": "E.2 Data with normal likelihood ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We study the performance of Treeffuser in a setting where a simpler model is better adjusted to the generating process. In particular, we let $N$ be number of data points sampled and $d_{x}$ the dimension and use a linear generative model ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\beta\\sim{\\mathcal{N}}(0,I_{d_{x}})}\\\\ &{{\\pmb x}_{i}\\sim{\\mathcal{N}}(0,I_{d_{x}})\\quad i\\in[N]}\\\\ &{{\\pmb y}_{i}\\sim{\\mathcal{N}}(10{\\beta}^{\\top}{\\pmb x},\\sigma^{2})\\quad i\\in[N]}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\sigma^{2}=1$ . The results are provided in table 3 using the same configuration described in section section 5.2. ", "page_idx": 17}, {"type": "text", "text": "Results From Table 3, we observe that while Treeffuser remains competitive, Deep Ensembles and NGBoost now demonstrate either equal or very close performance. This is expected since both models assume a normal distribution for the outcomes and the model assumed by Deep Ensembles closely resembles the real generative model. Interestingly, Treeffuser outperforms both iBUG and quantile regression (QReg), suggesting that its superior performance is not merely due to using LightGBM or XGBoost. Finally, we note that Treeffuser continues to perform well in low-data situations compared to other models. ", "page_idx": 17}, {"type": "text", "text": "F Experiments on standard datasets (supplement) ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "F.1 Baseline methods. ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We briefly describe the methods used as baselines in our empirical studies. ", "page_idx": 17}, {"type": "table", "img_path": "4KeSvAvNMr/tmp/b178d6feaab31f1792c2a43a4accd7de4e839a1072c23184ef55cacf410f89d7.jpg", "table_caption": ["Table 3: CRPS (lower is better) by dataset and method. Treeffuser provides comparable predictions to methods with parametric assumptions that match the generating process. The standard deviations are measured with 10-fold cross-validation. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "NGBoost (parametric, tree-based). NGBoost [14] models the conditional distribution of the target variable using a parametric family of distributions whose parameters are predicted by a gradientboosting algorithm. It uses natural gradients for more stable, accurate, and faster learning. In our experiments, we set the parametric model to be Gaussian. ", "page_idx": 18}, {"type": "text", "text": "Deep ensembles (parametric, nnet-based). A Deep ensemble [17] is a collection of neural networks that are individually trained to model a parametric conditional distribution $p(y|x)$ . The neural networks are then combined to obtain a mixture. We set the parametric model to be Gaussian. ", "page_idx": 18}, {"type": "text", "text": "Quantile regression (nonparametric, tree-based). Quantile regression [60] estimates the quantiles of the conditional distribution $p(y|x)$ . We implemented a GBT-based version of the method by fitting trees with inputs $\\textbf{\\em x}$ and $q$ , where $q$ is the probability of the desired quantile. During training, we optimized the objective $\\mathbb{E}_{q}\\mathbb{E}_{\\mathbf{x},\\mathbf{y}}[L(Q(\\mathbf{x},\\pmb{y}),q)]$ , where $L$ is the pinball loss and $q$ is sampled from a zero-one uniform distribution. In our experiments, we used LightGBM as a GBT method. ", "page_idx": 18}, {"type": "text", "text": "IBUG (parametric, tree-based). IBUG [15] extends any GBT into a probabilistic estimator. Given an input instance $x$ , it outputs a distribution around the prediction using the $k$ -nearest training data points. The distance between a training instance $x_{0}$ and $x$ depends on the number of co-occurrences of $x_{0}$ and $x$ across the leaves of the ensemble. In our experiments, we used XGBoost and a Gaussian likelihood, following the default specifications of the method. ", "page_idx": 18}, {"type": "text", "text": "DRF (nonparametric, tree-based). DRF [50] grows a random forest by maximizing the differences in the response distributions across split groups with a distributional metric. Repeated randomization induces a weighting function that assesses how relevant a training data point is to a given input $x$ . These weights are used to return a weighted empirical distribution of the response. ", "page_idx": 18}, {"type": "text", "text": "F.2 Hyperparameters of the methods. ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Unless specified otherwise, we tuned the hyperparameters of each method using Bayesian optimization for 25 iterations. We used the following search space for each method (where $[\\![a,b]\\!]$ denotes the set of integers from $a$ to $b$ ): ", "page_idx": 18}, {"type": "text", "text": "\u2022 Treeffuser \u2013 n estimators $\\in[100,3000]$ \u2013 n repeats $\\in[10,50]$ \u2013 learning $\\mathtt{r a t e}\\in[0.01,1]$ (log-uniform) \u2013 early stopping rounds $\\in[10,100]$ \u2013 num leaves $\\in$ 10, 100 ", "page_idx": 18}, {"type": "text", "text": "\u2022 Quantile regression (QReg) \u2013 n estimators $\\in[100,3000]$ (log-uniform) \u2013 n repeats $\\in[10,100]$ \u2013 learning rat e $\\in[0.01,$ 1] \u2013 early stopping rounds $\\in[10,100]$ \u2013 num leaves $\\in[10,100]$ ", "page_idx": 19}, {"type": "text", "text": "\u2022 IBUG \u2013 $\\mathbf{k}\\in[20,250]$ \u2013 n estimators $\\in[10,1000]$ \u2013 learning rate $\\in[0.01,0.5]$ (log-uniform) \u2013 max depth $\\in\\mathbb{I},100]$ ", "page_idx": 19}, {"type": "text", "text": "\u2022 DRF \u2013 min node size $\\in[5,30]$ \u2013 num trees $\\in[250,3000]$ \u2022 NGBOOST \u2013 n estimators $\\in[100,10000]$ \u2013 learning rat $\\mathfrak{z}\\in[0.005,0.2]$ \u2022 Deep ensemble \u2013 n layers $\\in[1,5]$ \u2013 hidden size $\\in[10,500]$ \u2013 learning rate $\\in[10^{-5},10^{-2}]$ (log-uniform) \u2013 n ensembles $\\in[2,10]$ ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "For the methods that only return point predictions, we used the following search spaces: ", "page_idx": 19}, {"type": "text", "text": "\u2022 XGBoost \u2013 n estimators $\\in[10,1000]$ \u2013 learning rate $\\in[0.01,0.5]$ (log-uniform) \u2013 max depth 1, 100 ", "page_idx": 19}, {"type": "text", "text": "\u2022 LightGBM \u2013 n estimators $\\in[10,1000]$ \u2013 learning rate $\\in[0.01,0.5]$ (log-uniform) \u2013 num leaves $\\in$ 10, 100 \u2013 early stopping rounds $\\in$ 10, 100 ", "page_idx": 19}, {"type": "table", "img_path": "4KeSvAvNMr/tmp/e62318ac33b19313e27fc92ca51211387d60ae5dd6373a6ae1cf10991c99eddb.jpg", "table_caption": ["F.3 Accuracy and calibration results. "], "table_footnote": ["Table 4: RMSE (lower is better) by dataset and method. $\\times$ indicates the method failed to run, and NA that the method is not directly applicable to multivariate outputs. The standard deviations are measured with 10-fold cross-validation. For each dataset, the two best methods are bolded. Treeffuser provides the most accurate probabilistic predictions, even when initialized with defaults. "], "page_idx": 20}, {"type": "table", "img_path": "4KeSvAvNMr/tmp/6538e008046e02539f3e8a7d96a23c674d1b172d85ce98184aafb7b549249cca.jpg", "table_caption": [], "table_footnote": ["Table 5: MACE (lower is better) by dataset and method. $\\times$ indicates the method failed to run, and NA that the method is not directly applicable to multivariate outputs. The standard deviations are measured with 10-fold cross-validation. For each dataset, the two best methods are bolded. Treeffuser has a competitive calibration error across most datasets. "], "page_idx": 20}, {"type": "table", "img_path": "4KeSvAvNMr/tmp/db1c1f097811800e7e755786402a7e5f05c726dad3558da3b671aa8973178a42.jpg", "table_caption": [], "table_footnote": ["Table 6: RMSE (lower is better) by dataset and method. This table extends Table 4 by includes vanilla XGBoost and LightGBM, alongside iBUG and Treeffuser. The standard deviations are measured with 10-fold cross-validation. For each dataset, the two best methods are bolded. "], "page_idx": 20}, {"type": "text", "text": "G Ablation study on noise scaling ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We evaluate the impact of including noise scaling in the score parametrization of Eq. (13), specifically $S(\\pmb{y},t,\\pmb{x})=U(\\pmb{y},\\dot{t},\\pmb{x})/\\sigma(t)$ , where $S(\\pmb{y},t,\\pmb{x})$ denotes the score function of $\\mathbf{\\boldsymbol{y}}(t)\\mid\\mathbf{\\boldsymbol{x}},U$ the gradientboosted trees, and $\\sigma(t)$ the noise schedule of the diffusion process. We compare Treeffuser with and without noise scaling under the same experimental setup of Section 5.2. ", "page_idx": 20}, {"type": "text", "text": "Table 7 presents the CRPS by dataset and method. Treeffuser without noise scaling fails to achieve any reasonable CRPS. We believe Treeffuser without noise scaling may require extra model capacity (e.g., deeper trees and more training iterations) to handle the varying noise levels adequately. However, we did not further optimize it, as noise scaling provides state-of-the-art results without the need for additional complexity. ", "page_idx": 21}, {"type": "table", "img_path": "4KeSvAvNMr/tmp/70dba74856361673e991aeeef73ac34e808a859e162351b3d239c6aa73710f9f.jpg", "table_caption": [], "table_footnote": ["Table 7: CRPS (lower is better) by dataset for Treeffuser with and without the noise scaling reparametrization in Eq. (13). Without scaling, Treeffuser does not produce meaningful results. "], "page_idx": 21}, {"type": "text", "text": "H Runtime and Complexity ", "text_level": 1, "page_idx": 21}, {"type": "table", "img_path": "4KeSvAvNMr/tmp/39f02660aa7d0c45e9024df8ed83ed0b0930e29245ad1bae7ef935a3ec364be6.jpg", "table_caption": ["Table 8: Average running time in milliseconds for producing a sample from the model. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "To demonstrate the efficiency of Treeffuser, we present the results of the following experiments and benchmarks: ", "page_idx": 21}, {"type": "text", "text": "1. We run Treeffuser on subsets of the M5 dataset (section 5.3) of various sizes and report the training time in Fig. 5.   \n2. We report the training time for running Treeffuser on the datasets used in the paper in Fig. 6.   \n3. We report in Table 8 the average runtime for generating a single sample after training Treeffuser. The average is calculated by sampling five thousand points after training the model with default settings and 50 discretization steps. ", "page_idx": 21}, {"type": "text", "text": "We conducted all of these experiments on a 2020 MacBook Pro with a 2.6 GHz 6-Core Intel Core i7 processor. ", "page_idx": 21}, {"type": "text", "text": "From these results we find that: ", "page_idx": 21}, {"type": "text", "text": "\u2022 Treeffuser is very fast at fitting moderately sized datasets, with runtime increasing linearly with dataset size.   \n\u2022 Sampling a single points is very fast $(\\approx10^{-3}$ seconds) yet, drawing many samples can become significant, e.g., with a large test set. ", "page_idx": 21}, {"type": "text", "text": "With respect to the time complexity of the model we note the following. If the time complexity of fitting a single GBT is of order $O(F(|D|))$ , where $F\\,:\\,\\mathbb{R}_{+}\\,\\rightarrow\\,\\mathbb{R}_{+}$ and $|\\mathcal D|$ is the size of the dataset, then the time complexity of fitting Treeffuser is $O(d\\times F({\\bf n_{-}r e p e a t s\\times|\\mathcal{D}|}))$ ), where $d$ is the dimension of $\\textit{\\textbf{y}}$ and n_repeats is the parameter that determines how many noisy versions of the dataset are sampled. Similarly, assuming constant time per GBT evaluation, the complexity of sampling is $O({\\bf n_{-}s a m p l e s}\\times{\\bf n_{-}}$ discretization_steps), where n_samples is the number of samples drawn from the model, and n_discretization_steps is the number of function evaluations used by the SDE solver. ", "page_idx": 21}, {"type": "image", "img_path": "4KeSvAvNMr/tmp/e28b51395b83e467a88dce1aef84ac31c7e0d8a854c2285f8d42906e6732392d.jpg", "img_caption": ["Figure 5: Dataset size vs training time on subsets of the M5 dataset. Error bars are computed over 5 runs. Treeffuser training speed grows linearly with the size of the training points. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "4KeSvAvNMr/tmp/eb92cd5ba6a2dcd59bd2e3027f1f69cd37bd8238016c554740a7405b6de20dd3.jpg", "img_caption": ["Figure 6: Dataset size vs training time on benchmark datasets. Error bars are computed over 5 runs. Treeffuser trains in under 120 seconds for all datasets. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We propose a method for probabilistic prediction from tabular data. In our abstract and introduction, we make two central claims: ", "page_idx": 23}, {"type": "text", "text": "\u2022 Treeffuser is accurate, in that it outperforms state-of-the-art methods in probabilistic prediction from tabular data on standard UCI datasets. This is discussed in detail in Section 5.2 and Appendix F. Table 1, Table 4, and Table 5 present the results. \u2022 Treeffuser is flexible, in that it adapts to complex distributions. This is shown in Fig. 1, which presents the results of our synthetic experiments, and in Section 5.1 and Appendix E, which provide the details. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 23}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: We discuss the limitations of our method in Section 3.4. Treeffuser has two limitations: the sampling process can become expensive when many samples are required for a given input instance $\\textbf{\\em x}$ ; the diffusion model is specifically designed to generate only continuous responses, which may not be suitable for count data. However, in Section 5.3, we demonstrate that Treeffuser outperforms other methods even on this kind of data. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. ", "page_idx": 23}, {"type": "text", "text": "\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 24}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We state two theorems, Theorem 1 and Theorem 2, and discuss their implications in Section 3.1 and Section 3.2, respectively. Proofs are given in Appendix B. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 24}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: The details needed to reproduce our experiments are stated in Section 5 and Appendices C, E and F. We also provide an implementation of Treeffuser in the supplemental material. The notebooks with the experiments can be found in testbed/notebooks/paper_notebooks. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.   \n(b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We provide an implementation of Treeffuser in the supplemental material. The notebooks with the experiments can be found in testbed/notebooks/paper_notebooks. For our experiments on real data, we use publicly available datasets and point to their references [51, 55]. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 25}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The details needed to reproduce our experiments are stated in Section 5 and Appendices C, E and F. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 25}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: All the accuracy and calibration metrics reported in our tables present standard deviations computed with 10-fold cross validation. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 26}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Yes, we provide details in Appendix D: the number of tasks, the number of cpus and the total time taken. The memory is not reported as our work has no particular needs for memory and no particular configuration was necessary. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 26}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We provide full transparency in our methodologies and support reproducibility through shared code and datasets. There are no conflicts of interest to declare. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 27}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 27}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: We are not releasing pre-trained models or datasets. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 27}, {"type": "text", "text": "12. Licenses for existing assets ", "page_idx": 27}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: For our experiments on real data, we use publicly available datasets and point to their references [51, 55]. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 28}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We provide an implementation of Treeffuser in the supplemental material, which includes an anonymized zip flie containing notebooks that document the usage of our method. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 28}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. ", "page_idx": 28}, {"type": "text", "text": "\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 29}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 29}]