[{"figure_path": "PTxRRUEpHq/tables/tables_2_1.jpg", "caption": "Table 1: We include related works from online DR-submodular optimization with constant or stochastic long term constraint functions. (Works handling adversarial long-term constraints require a different definition of regret.) All methods require a gradient oracle for feedback, and 'Noise' lists whether the gradient is exact or there is stochastic noise. '# Grad.' is the number gradient evaluations required per-round. 'Con. Viol.' is the bound on the constraint violation. \u2020 [31] considered constraint set being convex while all other works consider linear constraint. In \u2018# Grad.' column, 2\u221aT means this work needs \u221aT gradients on both f and g. \u2021 While all actions will be feasible, some gradient queries will be in the convex hull of K\u222a {0}.", "description": "This table compares the proposed algorithms in this paper to existing state-of-the-art algorithms for online DR-submodular maximization with long-term constraints.  It shows the region (whether 0 is in the constraint set K), whether the gradient information is exact or noisy, the number of gradient evaluations per round, the approximation ratio achieved, the regret bound, and the constraint violation bound for each algorithm.", "section": "Related Works"}]