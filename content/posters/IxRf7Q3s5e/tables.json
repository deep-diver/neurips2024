[{"figure_path": "IxRf7Q3s5e/tables/tables_5_1.jpg", "caption": "Table 1: Extrapolation accuracy on the same-size tasks benchmark proposed in Schwarzschild et al. [5] and the Thin-Maze environment, with corresponding training and evaluation sizes (st, ST). Higher is better. All results are averaged over 10 randomly-selected seeds. We highlight the best average results. We use (\u2020) to indicate stochastic dominance (min = 0) and (*) to indicate almost stochastic dominance (Emin < 0.5) of NeuralSolver over the baseline. We evaluate Bansal et al. [6] trained with progressive loss (PL) and without it.", "description": "This table compares the extrapolation accuracy of NeuralSolver against several baseline models on same-size tasks.  The tasks include Prefix-Sum, Maze, Thin-Maze, and Chess, each with varying training and testing sizes.  The results show NeuralSolver's superior performance in extrapolating to larger problems, particularly when compared to Bansal et al. [6], both with and without progressive loss.", "section": "4.2 Scenarios"}, {"figure_path": "IxRf7Q3s5e/tables/tables_6_1.jpg", "caption": "Table 2: Total parameter count of the used models on the same-size tasks benchmark proposed in Schwarzschild et al. [5] and the Thin-Maze environment, in the scale of millions of parameters.", "description": "This table presents the total number of parameters (in millions) for different models evaluated on same-size tasks (Prefix-Sum, Maze, Thin-Maze, Chess). It shows the parameter efficiency of NeuralSolver compared to the baselines (Bansal et al. [6] and FeedForward). NeuralSolver demonstrates significantly fewer parameters.", "section": "4.3 Baselines"}, {"figure_path": "IxRf7Q3s5e/tables/tables_6_2.jpg", "caption": "Table 3: Extrapolation accuracy on the different-size tasks with training and evaluation sizes (st, ST). We employ curriculum learning to train all methods, with training sizes indicated in the table. Higher is better. All results are averaged over 10 randomly-selected seeds. We highlight the best average results. We use (\u2020) to indicate stochastic dominance (min = 0) of NeuralSolver over the baseline.", "description": "This table presents the extrapolation accuracy results for four different-size tasks (1S-Maze, GoTo, Pong, DoorKey).  The results compare the performance of NeuralSolver against three baseline methods: Bansal et al. [6], FeedForward, and Random.  Curriculum learning was used in training. The table highlights the best average results across ten random seeds and uses the \u2020 symbol to indicate when NeuralSolver shows stochastic dominance over the baselines.", "section": "5 Results"}, {"figure_path": "IxRf7Q3s5e/tables/tables_7_1.jpg", "caption": "Table 4: Extrapolation accuracy of different ablated versions of NeuralSolver in the proposed different-size tasks. Higher is better. All results are averaged over 10 randomly-selected seeds. We highlight the best average results. We use (\u2020) to indicate stochastic dominance (\u20acmin = 0) and (*) to indicate almost stochastic dominance (\u20acmin < 0.5) of our default model over the ablated versions.", "description": "This table presents the ablation study on the NeuralSolver model by removing different components of the model to demonstrate how each component contributes to the overall performance.  It shows the extrapolation accuracy on four different-size tasks (1S-Maze, GoTo, Pong, DoorKey) for the following variations:\n\n- **NeuralSolver:** The complete model.\n- **Use AvgPool:** Replaced the global max-pooling layer with an average-pooling layer.\n- **Use 5L:** Used 5 convolutional layers in the recurrent module instead of 1.\n- **No CL:** Removed the curriculum-based training scheme.\n- **No LSTM:** Replaced the LSTM with a ResNet block.\n\nThe results demonstrate the impact of each component on the model's ability to extrapolate.", "section": "5.3 Ablation Study"}, {"figure_path": "IxRf7Q3s5e/tables/tables_11_1.jpg", "caption": "Table 6: Additional details on the datasets used in the evaluation of NeuralSolver. For the different-size tasks we show the dimensionality of the training examples used for curriculum learning.", "description": "This table shows the training and test sizes for each task used in the paper's experiments.  It also details the number of training and test examples for each task. For the different-size tasks, which involve a curriculum-based training scheme, the table specifies the dimensions of training examples at different stages of the curriculum.", "section": "A Additional Details on the Evaluation Scenarios"}, {"figure_path": "IxRf7Q3s5e/tables/tables_12_1.jpg", "caption": "Table 1: Extrapolation accuracy on the same-size tasks benchmark proposed in Schwarzschild et al. [5] and the Thin-Maze environment, with corresponding training and evaluation sizes (st, ST). Higher is better. All results are averaged over 10 randomly-selected seeds. We highlight the best average results. We use (\u2020) to indicate stochastic dominance (min = 0) and (*) to indicate almost stochastic dominance (Emin < 0.5) of NeuralSolver over the baseline. We evaluate Bansal et al. [6] trained with progressive loss (PL) and without it.", "description": "This table compares the extrapolation accuracy of NeuralSolver against baselines (Bansal et al. [6] with and without progressive loss, and a feedforward network) on same-size tasks.  It shows the accuracy achieved on four tasks (Prefix-Sum, Maze, Thin-Maze, and Chess) with different training and evaluation sizes.  The results highlight NeuralSolver's superior performance, particularly in extrapolating to larger problems.", "section": "4.2 Scenarios"}, {"figure_path": "IxRf7Q3s5e/tables/tables_14_1.jpg", "caption": "Table 1: Extrapolation accuracy on the same-size tasks benchmark proposed in Schwarzschild et al. [5] and the Thin-Maze environment, with corresponding training and evaluation sizes (st, ST). Higher is better. All results are averaged over 10 randomly-selected seeds. We highlight the best average results. We use (\u2020) to indicate stochastic dominance (min = 0) and (*) to indicate almost stochastic dominance (Emin < 0.5) of NeuralSolver over the baseline. We evaluate Bansal et al. [6] trained with progressive loss (PL) and without it.", "description": "This table shows the extrapolation accuracy of NeuralSolver and other baseline models on same-size tasks.  It compares the performance of NeuralSolver against Bansal et al. [6] with and without progressive loss, as well as a feedforward network.  The results are averaged over 10 random seeds, and statistical significance is indicated.  The table demonstrates NeuralSolver's superior extrapolation capabilities by highlighting the best average results and noting where it stochastically dominates the baselines.", "section": "4.2 Scenarios"}, {"figure_path": "IxRf7Q3s5e/tables/tables_14_2.jpg", "caption": "Table 1: Extrapolation accuracy on the same-size tasks benchmark proposed in Schwarzschild et al. [5] and the Thin-Maze environment, with corresponding training and evaluation sizes (st, ST). Higher is better. All results are averaged over 10 randomly-selected seeds. We highlight the best average results. We use (\u2020) to indicate stochastic dominance (min = 0) and (*) to indicate almost stochastic dominance (Emin < 0.5) of NeuralSolver over the baseline. We evaluate Bansal et al. [6] trained with progressive loss (PL) and without it.", "description": "This table presents the extrapolation accuracy results of NeuralSolver and three baseline models (Bansal et al. [6] with and without progressive loss, and a feedforward network) on four same-size tasks (Prefix-Sum, Maze, Thin-Maze, Chess).  It shows the accuracy achieved when the model is trained on smaller problem sizes and then tested on larger problem sizes (extrapolation).  The table highlights the superior performance of NeuralSolver in consistently achieving near-perfect accuracy even when extrapolating to significantly larger problem sizes than those seen during training.  Stochastic dominance is used to statistically compare NeuralSolver against the baselines, demonstrating its superiority.", "section": "4.1 Methods"}, {"figure_path": "IxRf7Q3s5e/tables/tables_15_1.jpg", "caption": "Table 10: Computational complexity (in gigaMACs) of different models across all tasks, measured in terms of the amount of Multiply-Add Operations necessary to run a single training example.", "description": "This table presents the computational complexity, measured in Giga Millions of Multiply-Accumulate operations (GMACs), for different models across various tasks.  It compares NeuralSolver against the Bansal et al. [6] and FeedForward models. Lower GMACs indicate greater computational efficiency.", "section": "4.1 Methods"}, {"figure_path": "IxRf7Q3s5e/tables/tables_17_1.jpg", "caption": "Table 3: Extrapolation accuracy on the different-size tasks with training and evaluation sizes (st, ST). We employ curriculum learning to train all methods, with training sizes indicated in the table. Higher is better. All results are averaged over 10 randomly-selected seeds. We highlight the best average results. We use (\u2020) to indicate stochastic dominance (min = 0) of NeuralSolver over the baseline.", "description": "This table shows the extrapolation accuracy of NeuralSolver and baseline models on four different-size tasks.  It highlights the best performance for each task and indicates when NeuralSolver stochastically dominates the baseline models.  Curriculum learning was used in training, and training sizes are specified.", "section": "5 Results"}, {"figure_path": "IxRf7Q3s5e/tables/tables_17_2.jpg", "caption": "Table 3: Extrapolation accuracy on the different-size tasks with training and evaluation sizes (st, ST). We employ curriculum learning to train all methods, with training sizes indicated in the table. Higher is better. All results are averaged over 10 randomly-selected seeds. We highlight the best average results. We use (\u2020) to indicate stochastic dominance (min = 0) of NeuralSolver over the baseline.", "description": "This table presents the extrapolation accuracy results for different-size tasks.  It compares the performance of NeuralSolver against baselines (Bansal et al. [6], FeedForward, Random).  The table shows accuracy with standard deviations, highlights the best-performing model for each task, and uses a symbol (\u2020) to indicate when NeuralSolver stochastically dominates a baseline.", "section": "5 Results"}, {"figure_path": "IxRf7Q3s5e/tables/tables_18_1.jpg", "caption": "Table 13: Extrapolation accuracy for different values of \u03b1 on the different-size tasks and in the Thin-Maze task.", "description": "This table presents the extrapolation accuracy achieved by the NeuralSolver model across five different-size tasks (1S-Maze, GoTo, Pong, DoorKey, Thin-Maze) for various values of the alpha (\u03b1) parameter used in the progressive loss training scheme.  Each row represents a different \u03b1 value, and each column shows the average accuracy and standard deviation for a specific task.  The results highlight how the choice of \u03b1 impacts the model's ability to extrapolate across tasks of varying size and complexity.", "section": "5. Results"}, {"figure_path": "IxRf7Q3s5e/tables/tables_18_2.jpg", "caption": "Table 14: Extrapolation performance of NeuralSolver in the proposed different-size tasks with different number of channels in the LSTM's output and hidden state (model width). Higher is better.", "description": "This table presents the results of an ablation study conducted to evaluate the impact of varying the number of channels in the LSTM's output and hidden state (model width) on the extrapolation performance of the NeuralSolver model across four different-size tasks.  The table shows that a model width of 64 (the default setting) achieves the best performance overall. Reducing the model width leads to a decrease in performance, particularly in the 1S-Maze and DoorKey tasks. ", "section": "5.2 Different-size Tasks"}, {"figure_path": "IxRf7Q3s5e/tables/tables_18_3.jpg", "caption": "Table 15: Extrapolation accuracy of NeuralSolver with different recurrent modules in the proposed different-size tasks. Higher is better.", "description": "This table presents the extrapolation accuracy results achieved by the NeuralSolver model using three different recurrent modules: LSTM (the default module used in the paper), GRU, and LocRNN.  The results are shown for four different-size tasks (1S-Maze, GoTo, Pong, and Doorkey).  The table highlights the performance of each recurrent module in terms of accuracy and helps determine the optimal recurrent module for the NeuralSolver model in different-size tasks.", "section": "5.2 Different-size Tasks"}, {"figure_path": "IxRf7Q3s5e/tables/tables_18_4.jpg", "caption": "Table 16: Total parameter count of NeuralSolver with different recurrent modules in the proposed different-size tasks, in the scale of millions of parameters. Lower is better.", "description": "This table presents a comparison of the total number of parameters (in millions) for the NeuralSolver model when using different recurrent modules (LSTM, GRU, LocRNN) across four different-size tasks.  The goal is to show the parameter efficiency of NeuralSolver, with lower numbers indicating better performance.  The default model uses an LSTM.", "section": "5.2 Different-size Tasks"}, {"figure_path": "IxRf7Q3s5e/tables/tables_18_5.jpg", "caption": "Table 16: Total parameter count of NeuralSolver with different recurrent modules in the proposed different-size tasks, in the scale of millions of parameters. Lower is better.", "description": "This table presents the total number of parameters (in millions) used by the NeuralSolver model with different recurrent modules (LSTM, GRU, LocRNN) across different tasks.  A lower number of parameters indicates higher parameter efficiency.  The table highlights the parameter efficiency of the NeuralSolver model, especially when compared to other models discussed in the paper.", "section": "5.2 Different-size Tasks"}, {"figure_path": "IxRf7Q3s5e/tables/tables_19_1.jpg", "caption": "Table 4: Extrapolation accuracy of different ablated versions of NeuralSolver in the proposed different-size tasks. Higher is better. All results are averaged over 10 randomly-selected seeds. We highlight the best average results. We use (\u2020) to indicate stochastic dominance (\u2208min = 0) and (*) to indicate almost stochastic dominance (\u2208min < 0.5) of our default model over the ablated versions.", "description": "This table presents the ablation study of the NeuralSolver model. It shows the extrapolation accuracy of different versions of the model where some components are removed or replaced to analyze their impact on the overall performance.  The results show the performance of different-size tasks with different components removed or replaced. The comparison is done against the default model using Almost Stochastic Dominance test. The table highlights the best performing model for each task and identifies which alterations resulted in a statistically significant drop in performance. ", "section": "5.3 Ablation Study"}, {"figure_path": "IxRf7Q3s5e/tables/tables_19_2.jpg", "caption": "Table 3: Extrapolation accuracy on the different-size tasks with training and evaluation sizes (st, ST). We employ curriculum learning to train all methods, with training sizes indicated in the table. Higher is better. All results are averaged over 10 randomly-selected seeds. We highlight the best average results. We use (\u2020) to indicate stochastic dominance (min = 0) of NeuralSolver over the baseline.", "description": "This table presents the extrapolation accuracy of NeuralSolver and baseline models on four different-size tasks.  Curriculum learning was used during training.  The results show NeuralSolver's superior performance in extrapolation, consistently achieving higher accuracy compared to the baselines across different tasks and larger problem sizes.", "section": "5 Results"}, {"figure_path": "IxRf7Q3s5e/tables/tables_19_3.jpg", "caption": "Table 3: Extrapolation accuracy on the different-size tasks with training and evaluation sizes (st, ST). We employ curriculum learning to train all methods, with training sizes indicated in the table. Higher is better. All results are averaged over 10 randomly-selected seeds. We highlight the best average results. We use (\u2020) to indicate stochastic dominance (min = 0) of NeuralSolver over the baseline.", "description": "This table presents the extrapolation accuracy of NeuralSolver and baseline models on four different-size tasks.  It shows the performance of each model when trained on smaller datasets and tested on larger datasets, highlighting the model's ability to extrapolate.  The results are averaged over ten runs, and statistical significance is assessed using the Almost Stochastic Order (ASO) test.", "section": "5.2 Different-size Tasks"}]