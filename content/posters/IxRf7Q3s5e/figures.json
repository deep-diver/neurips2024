[{"figure_path": "IxRf7Q3s5e/figures/figures_1_1.jpg", "caption": "Figure 1: Observations of the 1S-Maze environment of sizes 7\u00d77, 11\u00d711, 33\u00d733 and 129\u00d7129, where the agent (green square) must go to the goal (red square). The light green arrow represents the next target action that the model needs to predict, while the purple path represents the sequence of actions required to solve the maze.", "description": "This figure shows four different sizes of the 1S-Maze environment.  The green square represents the agent, and the red square represents the goal. The agent must navigate the maze to reach the goal. The light green arrow indicates the next action the agent should take, and the purple path shows the optimal solution path.", "section": "1 Introduction"}, {"figure_path": "IxRf7Q3s5e/figures/figures_2_1.jpg", "caption": "Figure 2: We design NeuralSolver with two fundamental components: (i) a recurrent module (purple), responsible for iteratively processing the input data regardless of its size; (ii) processing module (green), with an optional aggregation layer (A), responsible for generating the output and allowing our architecture to be used both in same-size and different-size tasks. Additionally, we employ a curriculum-based training scheme to improve the extrapolation performance of our architecture.", "description": "This figure illustrates the architecture of NeuralSolver, highlighting its two main components: a recurrent module for iterative data processing and a processing module for output generation.  The recurrent module handles inputs of varying sizes, while the processing module (with optional aggregation) allows for both same-size and different-size tasks.  A curriculum-based training scheme enhances extrapolation.", "section": "3 The NeuralSolver Architecture"}, {"figure_path": "IxRf7Q3s5e/figures/figures_3_1.jpg", "caption": "Figure 3: Propagation of information in NeuralSolver in a maze-like environment: the goal is for the agent (green) to find the goal position (red). Top: the difference between the value of the internal state of the recurrent module at each iteration step and the value at the final iteration. Larger differences are shown in dark blue and smaller differences in white. Bottom: additionally, we show the action probabilities predicted by the processing module of the model at different iterations, where the agent can move right (R), down (D), left (L), or up (U).", "description": "The figure visualizes how information propagates through NeuralSolver's recurrent module during maze solving. The top row shows the difference between the internal state at each iteration and the final state, indicating convergence. The bottom row displays the action probabilities at each iteration, illustrating how the model's certainty about the next action increases as the internal state converges.", "section": "3.2 Propagation of Information in NeuralSolver"}, {"figure_path": "IxRf7Q3s5e/figures/figures_4_1.jpg", "caption": "Figure 4: We introduce a set of different-size classification tasks to evaluate the performance of recurrent solvers. In all tasks, the input is an image observation of the environment with arbitrary size. The output is an n-dimensional one-hot vector with: a) n = 4; b) n = 4; c) n = 3; d) n = 4.", "description": "This figure shows four different classification tasks used to evaluate the NeuralSolver model.  Each task involves an image input of arbitrary size, representing different game-like scenarios (GoTo, 1S-Maze, Pong, DoorKey).  The model must predict a one-hot encoded vector indicating the appropriate action.  The number of possible actions varies across the tasks (4, 4, 3, and 4 respectively). These tasks test the NeuralSolver's ability to extrapolate to different sizes.", "section": "4.2 Scenarios"}, {"figure_path": "IxRf7Q3s5e/figures/figures_7_1.jpg", "caption": "Figure 5: Training efficiency of NeuralSolver and Bansal et al. [6] on same-size tasks: we present the accuracy of the learned algorithms on extrapolating to problems with different dimensionality (columns). Each color represents a different training size, specific to each task, detailed in Appendix A.3 In the dashed line we show the upper-bound on the performance.", "description": "This figure compares the training efficiency of NeuralSolver and Bansal et al.'s model on same-size tasks.  It shows the accuracy of learned algorithms when extrapolating to problems of varying sizes, with different training sizes represented by different colors.  The dashed line indicates the upper bound on performance.", "section": "4.3 Baselines"}, {"figure_path": "IxRf7Q3s5e/figures/figures_8_1.jpg", "caption": "Figure 5: Training efficiency of NeuralSolver and Bansal et al. [6] on same-size tasks: we present the accuracy of the learned algorithms on extrapolating to problems with different dimensionality (columns). Each color represents a different training size, specific to each task, detailed in Appendix A.3 In the dashed line we show the upper-bound on the performance.", "description": "The figure shows the performance of NeuralSolver and Bansal et al. [6] on same-size tasks, demonstrating training efficiency.  It displays the accuracy of learned algorithms when extrapolating to problems of varying dimensionality (different problem sizes).  Each color represents a different training size for each task, with details in Appendix A.3.  A dashed line indicates the upper performance bound.", "section": "4.3 Baselines"}, {"figure_path": "IxRf7Q3s5e/figures/figures_12_1.jpg", "caption": "Figure 8: Extrapolation accuracy of the different methods as a function of the number of iterations in the same-size task benchmark. All results are averaged over 10 randomly-selected seeds.", "description": "This figure compares the extrapolation accuracy of four different models (NeuralSolver, Bansal et al., FeedForward, and Random) across four different same-size tasks (Prefix-Sum, Maze, Thin-Maze, and Chess).  The x-axis represents the number of iterations performed, and the y-axis represents the accuracy achieved.  The shaded regions represent the standard deviations across 10 different runs.  This figure demonstrates the extrapolation capabilities of each model by showing how accuracy increases (or decreases) with more iterations, showcasing the performance gains of NeuralSolver over the other models.", "section": "4.2 Scenarios"}, {"figure_path": "IxRf7Q3s5e/figures/figures_13_1.jpg", "caption": "Figure 5: Training efficiency of NeuralSolver and Bansal et al. [6] on same-size tasks: we present the accuracy of the learned algorithms on extrapolating to problems with different dimensionality (columns). Each color represents a different training size, specific to each task, detailed in Appendix A.3 In the dashed line we show the upper-bound on the performance.", "description": "This figure compares the training efficiency of NeuralSolver and Bansal et al.'s model on same-size tasks.  It shows how well each model extrapolates to problems of different sizes after being trained on smaller datasets.  Each color represents a different training set size for each task.  The dashed line indicates the upper-bound performance, showcasing the optimal performance possible.", "section": "4.1 Methods"}, {"figure_path": "IxRf7Q3s5e/figures/figures_15_1.jpg", "caption": "Figure 10: Simplified comparison of the architectural differences between NeuralSolver and Bansal et al. [6].", "description": "The figure shows a simplified comparison of the architectures of NeuralSolver and the Bansal et al. recurrent solver.  NeuralSolver removes the projection layer present in Bansal et al. and replaces the ResNet recurrent block with a convolutional LSTM.  A key difference is the addition of an aggregation layer in NeuralSolver's processing module, enabling it to handle both same-size and different-size tasks.", "section": "3 The NeuralSolver Architecture"}, {"figure_path": "IxRf7Q3s5e/figures/figures_16_1.jpg", "caption": "Figure 11: Diagram of layernorm convolutional LSTM used in NeuralSolver. After a desired amount of recurrent iterations, we use the hidden state h as input to the processing module, to obtain the output.", "description": "This figure shows the architecture of the layernorm convolutional LSTM used in the NeuralSolver model.  The diagram details the flow of information through the LSTM cell, including the input (x), hidden state (ht), cell state (ct), convolutional and layernorm layers, and dropout layer. The pre-computed convolutional and layernorm layers are highlighted to show computational efficiency. The output (ht+1) of the recurrent module is then passed to the processing block for final output generation.", "section": "3.1 Model Architecture"}, {"figure_path": "IxRf7Q3s5e/figures/figures_16_2.jpg", "caption": "Figure 5: Training efficiency of NeuralSolver and Bansal et al. [6] on same-size tasks: we present the accuracy of the learned algorithms on extrapolating to problems with different dimensionality (columns). Each color represents a different training size, specific to each task, detailed in Appendix A.3 In the dashed line we show the upper-bound on the performance.", "description": "This figure compares the training efficiency of NeuralSolver and Bansal et al.'s model on same-size tasks. It shows how the accuracy of learned algorithms changes when extrapolating to problems of different dimensionalities, using different training sizes for each task. The dashed line represents the upper bound of performance.", "section": "4.3 Baselines"}, {"figure_path": "IxRf7Q3s5e/figures/figures_19_1.jpg", "caption": "Figure 5: Training efficiency of NeuralSolver and Bansal et al. [6] on same-size tasks: we present the accuracy of the learned algorithms on extrapolating to problems with different dimensionality (columns). Each color represents a different training size, specific to each task, detailed in Appendix A.3 In the dashed line we show the upper-bound on the performance.", "description": "The figure shows the accuracy of NeuralSolver and Bansal et al. [6] on same-size tasks when trained with different training sizes.  The x-axis represents the size of the problem being tested, and the y-axis represents the accuracy achieved.  Different colors correspond to models trained on problems of different sizes. The dashed line indicates the upper bound of performance achievable on each task.  This illustrates the training efficiency of each model; NeuralSolver is more efficient in training and extrapolating to larger problems than Bansal et al. [6].", "section": "4.3 Baselines"}, {"figure_path": "IxRf7Q3s5e/figures/figures_20_1.jpg", "caption": "Figure 14: Hyperparameter scan of NeuralSolver on the GoTo task. Each plot makes a single change from the hyperparameters in Table 9. The blue whiskers represent the confidence intervals at 95%. The green and red arrows represent the maximum and minimum values of the bootstrap distribution, respectively.", "description": "This figure shows the results of a hyperparameter search for the NeuralSolver model on the GoTo task.  Each subplot shows the effect of varying a single hyperparameter (optimizer, learning rate, warm-up, epochs, clip value, curriculum learning epochs, weight decay, standard PyTorch dropout, and Gal dropout) while holding others constant at the values specified in Table 9 of the paper.  The mean performance is shown with error bars representing 95% confidence intervals.  The green and red arrows indicate the maximum and minimum values observed during bootstrapping, providing insight into the range of performance.", "section": "5.3 Ablation Study"}, {"figure_path": "IxRf7Q3s5e/figures/figures_21_1.jpg", "caption": "Figure 15: Almost Stochastic Order scores of the same-size task results presented in Section 5.1. ASO scores are expressed in \nimin, with a significance level \na= 0.05 that is adjusted accordingly by using the Bonferroni correction [17]. Read from row to column: e.g., NeuralSolver (row) is almost stochastically dominant over Bansal et al. [6] (column) in the Prefix-Sum task with \nemin of 0.28.", "description": "This figure shows the results of the Almost Stochastic Order (ASO) test comparing different recurrent solvers on same-size tasks.  The ASO test determines statistical significance by measuring the stochastic dominance of one model over another.  The color intensity represents the \nemin value, indicating how much one model outperforms another.  Darker colors mean more significant dominance.  For instance, NeuralSolver is shown to significantly outperform Bansal et al. in most cases.", "section": "5.1 Same-size Tasks"}, {"figure_path": "IxRf7Q3s5e/figures/figures_22_1.jpg", "caption": "Figure 16: Almost Stochastic Order scores of the different-size task results presented in Section 5.2. ASO scores are expressed in  min, with a significance level  \u03b1 = 0.05 that is adjusted accordingly by using the Bonferroni correction [17]. Read from row to column: e.g., NeuralSolver (row) is stochastically dominant over Bansal et al. [6] (column) in the 1S-Maze task with  min of 0.00.", "description": "This figure shows the results of the Almost Stochastic Order (ASO) test comparing the performance of NeuralSolver against the baselines on the different-size tasks. The ASO test determines the statistical significance of the performance difference between two models.  A lower  min score indicates that the model in the row is stochastically dominant over the model in the column. The results demonstrate NeuralSolver's significant performance improvement compared to the baselines across all tasks.", "section": "5 Results"}, {"figure_path": "IxRf7Q3s5e/figures/figures_23_1.jpg", "caption": "Figure 17: Almost Stochastic Order scores of the ablation study on the different-size tasks presented in Section 5.3. ASO scores are expressed in  min, with a significance level \u03b1 = 0.05 that is adjusted accordingly by using the Bonferroni correction [17]. Read from row to column: e.g., NeuralSolver (row) is stochastically dominant over No LSTM (column) in the 1S-Maze task with  min of 0.00.", "description": "This figure shows the results of an ablation study on different components of the NeuralSolver model for different-size tasks.  The Almost Stochastic Order (ASO) test is used to compare the performance of different model variants.  The heatmap visually represents the statistical significance of performance differences. For example, NeuralSolver is significantly better than the model without LSTMs in the 1S-Maze task.", "section": "5.3 Ablation Study"}, {"figure_path": "IxRf7Q3s5e/figures/figures_24_1.jpg", "caption": "Figure 1: Observations of the 1S-Maze environment of sizes 7\u00d77, 11\u00d711, 33\u00d733 and 129\u00d7129, where the agent (green square) must go to the goal (red square). The light green arrow represents the next target action that the model needs to predict, while the purple path represents the sequence of actions required to solve the maze.", "description": "This figure shows examples of the 1S-Maze environment with different sizes (7x7, 11x11, 33x33, and 129x129). Each image shows the agent (green square), the goal (red square), the next action the agent should take (light green arrow), and the optimal path to the goal (purple line).  The figure illustrates the increasing complexity of the task with larger maze sizes.", "section": "1 Introduction"}, {"figure_path": "IxRf7Q3s5e/figures/figures_24_2.jpg", "caption": "Figure 16: Almost Stochastic Order scores of the different-size task results presented in Section 5.2. ASO scores are expressed in min, with a significance level \u03b1 = 0.05 that is adjusted accordingly by using the Bonferroni correction [17]. Read from row to column: e.g., NeuralSolver (row) is stochastically dominant over Bansal et al. [6] (column) in the 1S-Maze task with \u2208min of 0.00.", "description": "This figure shows the results of the Almost Stochastic Order (ASO) test performed on the different-size tasks. The ASO test compares the performance of different algorithms by considering their scores across multiple runs. The results are presented as a heatmap, with each cell representing the minimum value (\u2208min) of the ASO test comparing two algorithms.  A value of \u2208min < 0.5 indicates almost stochastic dominance, while \u2208min = 0.0 indicates stochastic dominance.  The heatmap helps visualize which algorithm performs statistically significantly better than others for each task.", "section": "5 Results"}, {"figure_path": "IxRf7Q3s5e/figures/figures_25_1.jpg", "caption": "Figure 3: Propagation of information in NeuralSolver in a maze-like environment: the goal is for the agent (green) to find the goal position (red). Top: the difference between the value of the internal state of the recurrent module at each iteration step and the value at the final iteration. Larger differences are shown in dark blue and smaller differences in white. Bottom: additionally, we show the action probabilities predicted by the processing module of the model at different iterations, where the agent can move right (R), down (D), left (L), or up (U).", "description": "This figure visualizes how information propagates through NeuralSolver's recurrent module during maze solving. The top row shows the difference between the internal state at each iteration and the final state, highlighting convergence. Darker blue indicates larger differences. The bottom shows action probabilities at each iteration, demonstrating how the model's certainty increases as the recurrent module converges.", "section": "3.2 Propagation of Information in NeuralSolver"}, {"figure_path": "IxRf7Q3s5e/figures/figures_26_1.jpg", "caption": "Figure 20: Propagation of information in NeuralSolver for the 1S-Maze task: Top: we highlight the difference between the current iteration and last iteration (not represented) of the internal state of the recurrent module of our model in a 35 \u00d7 35 environment. The white pixels represent the pixel positions of the recurrent state that converged to a fixed final state. Larger differences are represented in deep blue color. Bottom: the predicted action probabilities by the model at different iterations, where the agent can move right (R), down (D), left (L), or up (U).", "description": "This figure visualizes how information propagates through the NeuralSolver's recurrent module when solving the 1S-Maze task. The top row shows the difference between the internal state at each iteration and the final state, highlighting areas that have converged. Darker blue indicates larger differences. The bottom row displays the model's predicted action probabilities (R, D, L, U) at each iteration.", "section": "3.2 Propagation of Information in NeuralSolver"}, {"figure_path": "IxRf7Q3s5e/figures/figures_26_2.jpg", "caption": "Figure 3: Propagation of information in NeuralSolver in a maze-like environment: the goal is for the agent (green) to find the goal position (red). Top: the difference between the value of the internal state of the recurrent module at each iteration step and the value at the final iteration. Larger differences are shown in dark blue and smaller differences in white. Bottom: additionally, we show the action probabilities predicted by the processing module of the model at different iterations, where the agent can move right (R), down (D), left (L), or up (U).", "description": "This figure shows how information propagates through the NeuralSolver model during maze solving. The top row displays the differences between the internal state at each iteration and the final state, visualizing the convergence of the model's internal representation. Darker blue indicates larger differences, showing which parts of the maze are still being processed.  The bottom row presents the action probabilities predicted by the model at different iteration steps, illustrating how the model's understanding of the optimal path evolves over time. ", "section": "3.2 Propagation of Information in NeuralSolver"}, {"figure_path": "IxRf7Q3s5e/figures/figures_27_1.jpg", "caption": "Figure 3: Propagation of information in NeuralSolver in a maze-like environment: the goal is for the agent (green) to find the goal position (red). Top: the difference between the value of the internal state of the recurrent module at each iteration step and the value at the final iteration. Larger differences are shown in dark blue and smaller differences in white. Bottom: additionally, we show the action probabilities predicted by the processing module of the model at different iterations, where the agent can move right (R), down (D), left (L), or up (U).", "description": "The figure shows how information propagates through the NeuralSolver model's recurrent module during the execution of a maze-solving task. The top row visualizes the differences between the internal state at each iteration and the final state, highlighting how the model focuses on certain areas as it progresses through the iterations. The bottom row shows the model's prediction of the agent's next action (R, D, L, or U) at each iteration, illustrating how the uncertainty of the prediction decreases as the recurrent module's state converges towards the final solution.", "section": "3.2 Propagation of Information in NeuralSolver"}, {"figure_path": "IxRf7Q3s5e/figures/figures_27_2.jpg", "caption": "Figure 3: Propagation of information in NeuralSolver in a maze-like environment: the goal is for the agent (green) to find the goal position (red). Top: the difference between the value of the internal state of the recurrent module at each iteration step and the value at the final iteration. Larger differences are shown in dark blue and smaller differences in white. Bottom: additionally, we show the action probabilities predicted by the processing module of the model at different iterations, where the agent can move right (R), down (D), left (L), or up (U).", "description": "This figure visualizes how information propagates through the NeuralSolver model during the maze-solving process. The top row shows the difference in internal state values between each iteration and the final iteration; larger differences are depicted in darker blue, while smaller differences are in white. This illustrates how the model's internal representation of the maze evolves with each step.  The bottom row displays the action probabilities predicted by the model's processing module at different iterations. The arrows (R, D, L, U) represent the agent's possible actions: right, down, left, and up.", "section": "3.2 Propagation of Information in NeuralSolver"}, {"figure_path": "IxRf7Q3s5e/figures/figures_28_1.jpg", "caption": "Figure 3: Propagation of information in NeuralSolver in a maze-like environment: the goal is for the agent (green) to find the goal position (red). Top: the difference between the value of the internal state of the recurrent module at each iteration step and the value at the final iteration. Larger differences are shown in dark blue and smaller differences in white. Bottom: additionally, we show the action probabilities predicted by the processing module of the model at different iterations, where the agent can move right (R), down (D), left (L), or up (U).", "description": "This figure visualizes how information propagates through the NeuralSolver model during the execution of a maze-solving task. The top row shows the difference between the internal state of the recurrent module at each iteration and its final state. Darker blue indicates larger differences, meaning that those parts of the maze are still being processed. The bottom row displays the action probabilities at each iteration. It shows how the model's certainty about the next action increases as the number of iterations increases and the internal state converges to the solution.", "section": "3.2 Propagation of Information in NeuralSolver"}, {"figure_path": "IxRf7Q3s5e/figures/figures_29_1.jpg", "caption": "Figure 21: Propagation of information in NeuralSolver for the 1S-Maze task: Top: we highlight the difference between the current iteration and last iteration (not represented) of the internal state of the recurrent module of our model in a 61 \u00d7 61 environment. The white pixels represent the pixel positions of the recurrent state that converged to a fixed final state. Larger differences are represented in deep blue color. Bottom: the predicted action probabilities by the model at different iterations, where the agent can move right (R), down (D), left (L), or up (U).", "description": "This figure visualizes how information propagates through the NeuralSolver model while solving a 1S-Maze task. The top row shows the differences between the internal state at each iteration and the final state, with white pixels indicating convergence. The bottom row displays the action probabilities predicted by the model at different iterations.", "section": "E Learned Algorithms with NeuralSolver"}, {"figure_path": "IxRf7Q3s5e/figures/figures_29_2.jpg", "caption": "Figure 28: Propagation of information in NeuralSolver for the DoorKey task: Top: we highlight the difference between the current iteration and last iteration (not represented) of the internal state of the recurrent module of our model in a 64 \u00d7 64 environment. The black pixels represent the pixel positions of the recurrent state that converged to a fixed final state. Larger differences are represented in deep blue color. Bottom: the predicted action probabilities by the model at different iterations, namely Forward (L), Rotate Right (R), Pickup (P), and Toggle (T).", "description": "This figure visualizes how information propagates through the neural network's recurrent module during the DoorKey task.  The top row shows the difference in the internal state between consecutive iterations, with white representing convergence to a stable state and dark blue showing large differences. The bottom row shows the model's predicted probabilities for different actions (Forward, Rotate Right, Pickup, Toggle) at each iteration.  It illustrates the network's learning process by showing how its internal representation evolves and leads to the correct action prediction.", "section": "E Learned Algorithms with NeuralSolver"}, {"figure_path": "IxRf7Q3s5e/figures/figures_30_1.jpg", "caption": "Figure 28: Propagation of information in NeuralSolver for the DoorKey task: Top: we highlight the difference between the current iteration and last iteration (not represented) of the internal state of the recurrent module of our model in a 64 \u00d7 64 environment. The black pixels represent the pixel positions of the recurrent state that converged to a fixed final state. Larger differences are represented in deep blue color. Bottom: the predicted action probabilities by the model at different iterations, namely Forward (L), Rotate Right (R), Pickup (P), and Toggle (T).", "description": "This figure shows how information propagates through the network when solving the Doorkey task. The top row shows the difference between the internal state at the current iteration and the last iteration. The bottom row shows the predicted action probabilities. The figure demonstrates how NeuralSolver solves the task by iteratively processing information and converging to a solution.", "section": "E Learned Algorithms with NeuralSolver"}, {"figure_path": "IxRf7Q3s5e/figures/figures_31_1.jpg", "caption": "Figure 29: Visualization of the trajectories of NeuralSolver, Bansal et al. [6], and FeedForward models against the Oracle trajectory. The first row is a task of size 32 \u00d7 32, second and third of 64 \u00d7 64, and fourth and fifth of 128 \u00d7 128.", "description": "This figure visualizes the trajectories of four different models (Oracle, NeuralSolver, Bansal et al., and FeedForward) in a Minigrid Doorkey environment with varying sizes (32x32, 64x64, and 128x128). Each row shows a different task, illustrating how well each model performs compared to the optimal Oracle trajectory in navigating the environment.", "section": "F Additional Example Trajectories"}, {"figure_path": "IxRf7Q3s5e/figures/figures_32_1.jpg", "caption": "Figure 30: Example of a 1S-Maze task with 512 \u00d7 512 observations.", "description": "This figure shows an example of a 1S-Maze task used in the paper's experiments.  The image is significantly larger than those used during training (512 x 512 pixels), demonstrating the model's ability to extrapolate to larger problem sizes.  The maze is complex and the path to the goal is not immediately obvious, highlighting the challenge addressed by the NeuralSolver model.", "section": "G Examples of tasks with observations of size 512 \u00d7 512"}, {"figure_path": "IxRf7Q3s5e/figures/figures_33_1.jpg", "caption": "Figure 3: Propagation of information in NeuralSolver in a maze-like environment: the goal is for the agent (green) to find the goal position (red). Top: the difference between the value of the internal state of the recurrent module at each iteration step and the value at the final iteration. Larger differences are shown in dark blue and smaller differences in white. Bottom: additionally, we show the action probabilities predicted by the processing module of the model at different iterations, where the agent can move right (R), down (D), left (L), or up (U).", "description": "This figure shows how information propagates through the NeuralSolver model during maze solving. The top panel displays the difference between the internal state at each iteration and the final state, visualizing the convergence of the model.  Darker blue indicates larger differences. The bottom panel shows the action probabilities predicted at each step, demonstrating the decision-making process of the model.", "section": "3.2 Propagation of Information in NeuralSolver"}]