[{"type": "text", "text": "UQE: A Query Engine for Unstructured Databases ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Hanjun Dai\u2020 , Bethany Yixin $\\mathbf{Wang}^{\\ddagger}\\mathbf{\\mathcal{N}}\\mathbf{ang}^{\\ddagger}$ , Xingchen $\\mathbf{Wan\\ddagger}$ , Bo Dai\u2020\u00b6, Sherry Yang\u2020, Azade  \u201cNova\u2020, Pengcheng $\\mathbf{Y}\\mathbf{in}^{\\dagger}$ , \u201c Phitchaya Mangpo Phothilimthana\u2020,\u2217 Charles Sutton\u2020, Dale Schuurmans\u2020\u00a7 ", "page_idx": 0}, {"type": "text", "text": "\u2020 Google DeepMind \u2021 Google Cloud \u00a7 University of Alberta $^\\mathparagraph$ Georgia Institute of Technology ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Analytics on structured data is a mature field with many successful methods. However, most real world data exists in unstructured form, such as images and conversations. We investigate the potential of Large Language Models (LLMs) to enable unstructured data analytics. In particular, we propose a new Universal Query Engine (UQE) that directly interrogates and draws insights from unstructured data collections. This engine accepts queries in a Universal Query Language (UQL), a dialect of SQL that provides full natural language flexibility in specifying conditions and operators. The new engine leverages the ability of LLMs to conduct analysis of unstructured data, while also allowing us to exploit advances in sampling and optimization techniques to achieve efficient and accurate query execution. In addition, we borrow techniques from classical compiler theory to better orchestrate the workflow between sampling methods and foundation model calls. We demonstrate the efficiency of UQE on data analytics across different modalities, including images, dialogs and reviews, across a range of useful query types, including conditional aggregation, semantic retrieval and abstraction aggregation. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Data analysis [13] is essential for making well founded decisions and enabling businesses and society to function more effectively. Relational databases [12, 32] and the Structured Query Language (SQL) [7] have delivered huge successes in structured data management and analysis. Typically, such data is collected and organized in a pre-defined schema [14], where the data properties and relationships have been pre-specified, and downstream analysis is restricted to this schema. ", "page_idx": 0}, {"type": "text", "text": "In most real-world applications, however, data exists in unstructured formats, such as images, documents and audio recordings. Without preprocessing such data into structured forms, traditional SQL engines can only support limited queries. Preprocessing, including document entity retreival [45] and form understanding [42], also require training on downstream tasks given a predefined taxonomy. This naturally motivates the question we consider in this paper: ", "page_idx": 0}, {"type": "text", "text": "How can one perform unstructured data analysis in a flexible and efficient way? ", "page_idx": 0}, {"type": "text", "text": "In the literature, full-text search engines [20] support scalable regexp-matching search on unstructured data, but this becomes infeasible for more complex semantic reasoning queries. Retrieval-Augmented Generation (RAG) [39, 24, 18] allows question answering on a subset of related data, but is not directly applicable to generic analytical tasks with aggregation and semantic queries that spans over an entire large database. Recent advances in Large Language Models (LLMs) [4, 2] unlock the ability to perform flexible question answering, especially with recent long-context models [33]. However, setting aside the cost per query, data analytics can still be challenging for LLMs without fine-tuning [25] or few-shot demonstrations [9], even given structured tables. ", "page_idx": 0}, {"type": "image", "img_path": "t7SGOv5W5z/tmp/3547c535b41ca7ed51ebc236031c97b77828a4302d10376d5b9c7b3d7884635b.jpg", "img_caption": ["Figure 1: Illustration of unstructured data analysis defined in Section 2. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Recently, a promising line of work considered marrying LLMs with programming frameworks [38], where logical or arithmetic operations are offloaded to program interpreters [17]. A most relevant example for analytics and table understanding tasks is Cheng et al. [11], which augments classical SQL semantics with LLMs as user-defined functions (UDF). While promising, the execution of such SQL programs that embed LLM calls still requires sweeping over the entire database, which is too costly for large collections of unstructured content. To overcome this barrier, we leverage the synergy between LLMs and programmatic execution to define an Unstructured Query Language (UQL) that augments SQL for flexible semantic queries, with a focus on improving scalability and efficiency. ", "page_idx": 1}, {"type": "text", "text": "A key observation is that the efficiency of classical SQL engines relies on (1) indexing structures that avoid the need to scan the entire database, and (2) a compilation system that determines the best execution order for operations. Based on these ideas, we propose the Unstructured Query Engine (UQE), which refines and extends this design principle to unstructured data analytics. To achieve similar effect to indexing, UQE casts the problem as learning to search or sample, seeking to avoid a full database scan with statistically sound methods. Additionally, a compilation system is developed that determines the best execution order and operator combination for different clauses in a UQL query, with the goal of minimizing LLM calls while preserving query semantics. ", "page_idx": 1}, {"type": "text", "text": "As part of this project, we have created four new benchmark datasets, with both text and image modalities, along with three common analytic tasks. Compared to baseline methods, such as longcontext LLMs and embedding based retrieval, UQE achieves significant improvements in terms of the accuracy and cost reduction on these benchmarks. ", "page_idx": 1}, {"type": "text", "text": "2 Problem ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Before defining the problem we are solving, we first establish the terminology and notation we will use throughout the paper. A concrete illustration of the following terms is given in Figure 1. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Table / database: We define a table $\\mathcal{T}=\\{\\mathcal{T}_{i}\\}_{i=1}^{N}$ as an unordered set of $|\\mathcal{T}|=N$ rows, where each row $\\mathcal{T}_{i}=[\\mathcal{T}_{i,1},\\mathcal{T}_{i,2},\\ldots,\\mathcal{T}_{i,M}]$ is an array of $M$ elements such that $M$ is the total number of columns in the table. Each row can consist of elements ${\\mathcal{T}}_{i},$ ,\u00b7 of heterogeneous types (e.g., datetime, float, enum) with different modalities (e.g., text, image), while elements in each column $\\tau_{\\cdot,j}$ must be of the same format and modality. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Structured data: A column $\\tau_{\\cdot,j}$ is structured w.r.t. a query if it can be accessed quantitatively, such as by algebraic operations over numeric data, comparison over string labels with predefined vocabulary (e.g., categorical labels), datetime functions, etc.. \u2022 Unstructured data: A column $\\tau_{\\cdot,j}$ is unstructured if a query cannot access it using standard quantitative access. Typically, such a column does not belong to a predefined taxonomy. Examples include text (e.g., dialogs), images, videos, and other forms of data that usually require semantic understanding and preprocessing before performing any algebraic operations. \u2022 Concrete column: A column is concrete if it already exists in the table. \u2022 Virtual column: A column is virtual if it does not already exist in the table, but a query is able to operate on it. Conceptually, one needs to derive (partial rows of) these columns by processing the data from concrete columns. In our work, we bypass this step by creating such columns lazily and selectively, which is the key to achieving efficiency and performance gains. ", "page_idx": 1}, {"type": "text", "text": "SQL engines can perform analytic queries on databases by manipulating structured data in concrete columns. The focus of this paper is to propose a new query engine that can perform analytics on databases with both structured and unstructured data, with queries that operate over both concrete and virtual columns. Standard analytics tasks that we seek to enable over unstructured data are: ", "page_idx": 1}, {"type": "text", "text": "\u2022 Conditional aggregation: perform aggregation operations on a sub-table filtered by a condition.   \n\u2022 Semantic retrieval: collect relevant rows specified by semantic filters.   \n\u2022 Abstraction and aggregation: group the rows based on abstractions and then performs aggregation. ", "page_idx": 2}, {"type": "text", "text": "Since optimizing queries on structured data within concrete columns is well-studied, we focus instead on techniques for handling queries on unstructured data over virtual columns. However, the UQE implementation also supports operations over structured data within concrete columns. In the following, unless stated otherwise, the term unstructured databases refer to databases containing both structured and unstructured data. ", "page_idx": 2}, {"type": "text", "text": "3 Unstructured query language ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "First, we need to formally define the query language, UQL, that talks to the unstructured databases. The idea of defining a natural query language for unstructured data is not completely new (e.g., Cheng et al. [11], even though UQL has richer semantics), nor is the specific syntax or design of UQL the main focus of this paper. However, we need to define the scope of queries that the engine can handle, and breakdown the semantic meaning of each clause. ", "page_idx": 2}, {"type": "text", "text": "3.1 UQL semantics ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We assume a basic familiarity of SQL, upon which UQL is based. UQL can be considered to be a dialect of SQL that has augmented functionalities for handling unstructured and virtual column queries. The SQL clauses that we support in UQL, along with necessary modifications to support unstructured semantic queries, are described as follows. ", "page_idx": 2}, {"type": "text", "text": "SELECT is a mapping function that maps the operand (usually a row or collection of rows in a grouped query) to a new row of elements. In traditional SQL, this mapping is usually a subset selection over concrete columns, or algebraic operators over those columns. UQL provides additional semantic mapping capability as: ", "page_idx": 2}, {"type": "text", "text": "SELECT \"the attribute specified by natural language\" AS attribute_name ", "page_idx": 2}, {"type": "text", "text": "For example, one can write SELECT \"the sentiment of the movie review\" over an unstructured movie review column, and retrieve \"positive\" or \"negative\" as a structured output. ", "page_idx": 2}, {"type": "text", "text": "FROM specifies the source of the table. In SQL one can additionally specify table joins, but we limit our attention to sourcing from a single table in this paper. ", "page_idx": 2}, {"type": "text", "text": "WHERE intrinsically specifies a binary classifier over rows, which is used to retrieve a subset of the database. In addition to comparator operators on structured columns, we also allow semantic specifications in the form of: ", "page_idx": 2}, {"type": "text", "text": "WHERE \"the row satisfies some natural language specifications\" ", "page_idx": 2}, {"type": "text", "text": "The predicates in WHERE are organized in disjunctive normal form (DNF) with AND and OR syntax, so a user can arbitrarily express predicates over concrete and virtual columns. ", "page_idx": 2}, {"type": "text", "text": "GROUP BY partitions the table into groups, where rows within each group share the same attributes over the keys being grouped by. UQL allows partitioning over virtual columns via natural language: ", "page_idx": 2}, {"type": "text", "text": "GROUP BY \"the abstraction criteria specified in natural language\" ", "page_idx": 2}, {"type": "text", "text": "Similar to WHERE, one can GROUP BY over both concrete and virtual columns by concatenating multiple criteria, with the resulting partition corresponding to grouping by a tuple of these keys. ", "page_idx": 2}, {"type": "text", "text": "We also reuse other clauses from SQL including: ORDER BY, which simply inherits the SQL semantics to rank the resulting rows according to a specific concrete column. In most analytics tasks, sorting is applied over structured columns with well defined ordering comparators. LIMIT is applied during processing in the form of LIMIT num_rows, which limits the number of output rows. ", "page_idx": 2}, {"type": "text", "text": "Assumptions: We rely on the ability of an LLM to perform intra-row semantic understanding and analysis tasks. For example, we assume that LLMs are able to correctly judge the specification in WHERE for a single row. Similarly, LLMs should be able to extract the information specified by SELECT or GROUP BY for a single row. We build programmatic functionality on top of this fundamental ability of LLMs to handle analytics for large databases. ", "page_idx": 2}, {"type": "text", "text": "SELECT reason, COUNT(\\*) as count SELECT agent_name, \"reason to cancel\"   \nFROM movie_reviews FROM airline_customer_service_log   \nWHERE movie_year $<$ 2020 WHERE \"the customer asked to cancel   \nGROUP BY \"the reason why the the flight\" review is positive\" ORDER BY ticket_price AS reason LIMIT 100 ", "page_idx": 3}, {"type": "text", "text": "Figure 2: Aggregation (left) v.s. Non-aggregation (right) queries written in UQL. ", "page_idx": 3}, {"type": "text", "text": "3.2 UQL queries ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "A UQL query is a composition of clauses that can be categorized as an aggregation or a nonaggregation, as illustrated in Figure 2. Aggregation queries perform a summary on groups of aggregated rows, such as COUNT the number of rows, or summarize a common attribute in a group of rows (as defined in GROUP BY above). Non-aggregation queries perform operations on individual rows, which usually means the rows can be processed in parallel. A UQL query will only belong to one of the above two types and we do not consider nested queries for now. The query type determines how UQE will optimize and execute the query. ", "page_idx": 3}, {"type": "text", "text": "4 Unstructured Query Engine ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "One straightforward way to run UQL is to use an interpreter that executes queries imperatively. Cheng et al. [11] implements an engine in this form, which is able to handle tables of relatively small size. By analogy, this is similar to executing an SQL program using a linear database scan. While it is valid, the latency and cost are prohibitive and generally prevent scaling to real world scenarios. ", "page_idx": 3}, {"type": "text", "text": "There are (at least) two key techniques in SQL databases for making query execution efficient: ", "page_idx": 3}, {"type": "text", "text": "\u2022 Indexing, which organizes concrete columns via data structures for fast search with sublinear cost.   \n\u2022 Compilation, which considers alternative query plans and executes the most efficient one. ", "page_idx": 3}, {"type": "text", "text": "UQL queries over virtual columns pose challenges in both indexing and compilation. In this section, we present effective approaches to indexing (Section 4.1) and compilation (Section 4.2) for unstructured databases, along with the implementation details of low-level primitives (Section 4.2.2). ", "page_idx": 3}, {"type": "text", "text": "4.1 Indexing ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Executing traditional SQL queries over indexed columns can be made efficient by avoiding an entire database scan to find the relevant rows to process. However, for UQL queries over virtual columns, it is hard to predict or predefine an index that can enable efficient searching, since these columns are not concrete and defined via arbitrary natural language specifications. ", "page_idx": 3}, {"type": "text", "text": "Our first key contribution is to introduce a proxy for \"indexing\" that allows one to leverage the intrinsic semantic content of a virtual column to efficiently execute queries without scanning the entire database. The main idea is to use statistically sound sampling techniques to approximately retrieve relevant rows for processing. Based on the two types of queries defined in Section 3.2, we develop corresponding \"indexing\" counterparts. ", "page_idx": 3}, {"type": "text", "text": "4.1.1 Unbiased estimation for aggregation queries ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We use a simple query to illustrate the idea of unbiased estimation to obtain a query result without scanning over an entire virtual column. ", "page_idx": 3}, {"type": "text", "text": "SELECT COUNT $(\\ast)$ as count FROM movie_reviews WHERE \"the review is positive\" ", "page_idx": 3}, {"type": "text", "text": "Given a row $\\mathcal{T}_{i}$ (a natural language movie review), it is relatively easy for an LLM to tell whether it satisfies the WHERE condition. If we use $f:(T_{i},\\mathsf{c o n d})\\mapsto\\{0,1\\}$ to represent the LLM\u2019s classification of whether row $\\mathcal{T}_{i}$ satisfies the conditions specified in cond, then the goal is to estimate the quantity ", "page_idx": 3}, {"type": "text", "text": "There are many approaches that can be used to estimate the finite sum in the above equation, with different tradeoffs between bias and variance. One unbiased but potentially high variance estimator is to simply use Monte Carlo samples from a uniform distribution over $1\\dots N$ . A typical technique for reducing variance is to use importance sampling with a proposal $p$ , according to ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{i\\in\\{1,\\dots N\\}}\\left[f(\\mathcal{T}_{i},\\mathbf{cond})\\right]=\\sum_{i=1}^{N}\\frac{p_{i}}{N p_{i}}f(\\mathcal{T}_{i},\\mathbf{cond})=\\mathbb{E}_{i\\sim p}\\left[\\frac{1}{N p_{i}}f(\\mathcal{T}_{i},\\mathbf{cond})\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "A theoretically optimal proposal $p$ is given as follows: ", "page_idx": 3}, {"type": "text", "text": "Algorithm 1 Stratified sampling for unbiased aggregation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "\u2022 Embed each row $\\mathcal{T}_{i}$ as $\\vec{e_{i}}$ , using a multi-modal embedding over the unstructured columns of $\\mathcal{T}_{i}$ . \u2022 Cluster the embeddings $\\{\\vec{e}_{i}\\}_{i=1}^{N}$ into $K$ disjoint groups $\\bar{\\{C_{k}\\}}_{k=1}^{K}$ , $C_{k}\\subseteq\\{1\\ldots N\\}$ , where $k$ can be a predefined constant or automatically selected [48]. Each group has size $|C_{k}|$ and $\\begin{array}{r}{\\sum_{k=1}^{K}|C_{k}|=N}\\end{array}$ We use $c:\\{1\\ldots N\\}\\mapsto\\{1\\ldots K\\}$ to denote the cluster index of each row. \u2022 Perform stratified sampling over these groups and obtain samples $S\\subseteq\\{1\\ldots N\\}$ . Then we can obtain the following estimator for the expectation: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{E}_{i\\in\\{1,\\dots,N\\}}\\left[f(\\mathcal{T}_{i},\\mathrm{cond})\\right]\\simeq|S|\\sum_{i\\in S}\\frac{w_{i}}{\\sum_{j\\in S}w_{j}}f(\\mathcal{T}_{i},\\mathrm{cond}),\\mathrm{where~}w_{i}=\\frac{|C_{c(i)}|}{\\sum_{j\\in S}\\mathbb{I}\\left[c(j)=c(i)\\right]}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Algorithm 2 Online active learning for non-aggregation retrieval ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "1. Embed each row $\\mathcal{T}_{i}$ as $\\vec{e_{i}}$ , using multi-modal embedding over the unstructured columns of $\\mathcal{T}_{i}$ .   \n2. Maintain ${\\hat{g}}(i)$ that approximates $f(\\tau_{i},\\mathsf{c o n d})$ . Initialize $\\begin{array}{r}{\\hat{g}(i)\\propto U(0,1)}\\end{array}$ uniformly.   \n3. Maintain the collection of sampled rows $S=\\emptyset$ at step $t=0$ .   \n4. At step $t$ , obtain a batch $S_{t}$ of samples where $\\begin{array}{r}{S_{t}\\;\\stackrel{.}{=}\\;\\arg\\operatorname*{max}_{S_{t}\\subseteq\\{1\\dots N\\}\\backslash S}\\sum_{i\\in S_{t}}\\hat{g}(i)+\\epsilon_{t,i}}\\end{array}$ ; observe $f(\\tau_{i},\\mathsf{c o n d})$ for each sample $i\\in S_{t}$ ; Update $S\\gets S\\cup S_{t}$ .   \n5. Fit $\\hat{g}$ with samples and corresponding observations from $S$ . Go to step 4 if $|S|<B$ or return the positive samples found in $S$ . ", "page_idx": 4}, {"type": "text", "text": "Proposition 1 The optimal proposal distribution p that minimizes the variance of estimation in Eq 2 is $\\bar{p_{i}}\\propto f(T_{i},c o n d)$ , which achieves zero variance. ", "page_idx": 4}, {"type": "text", "text": "Prop 1 indicates that an ideal proposal should sample rows that have positive $f$ with equal probability, while sampling negative rows with zero probability. However, given that $f$ is the response of an LLM it is expensive to execute over all rows, forcing us to consider efficient approximations. ", "page_idx": 4}, {"type": "text", "text": "Stratified sampling leverages the ability to partition a population into homogeneous subpopulations. As shown in Prop 1, a good proposal $p_{i}$ should predict whether a row $\\mathcal{T}_{i}$ satisfies the target property cond. To trade-off between cost and variance reduction, we propose Algorithm 1. In Eq 3 we normalize the importance weights $w_{i}$ to further reduce the estimation variance. ", "page_idx": 4}, {"type": "text", "text": "Extension The above estimator can be used for other aggregation operations such as SUM and AVERAGE, including GROUP BY, and allowing concrete columns as operands as well. However, some aggregations such as MAX does not admit such an estimator. UQE in this case can only provide estimates with greater effort. We discuss limitations in Section 7. ", "page_idx": 4}, {"type": "text", "text": "4.1.2 Online learning for non-aggregation queries ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "A non-aggregation query can be viewed as a search problem where we want to find a relevant subset of rows to process. As before, we begin with a concrete example: ", "page_idx": 4}, {"type": "text", "text": "SELECT dialog_ID FROM dialogs WHERE \"the customer is unhappy with the agent $^1\\,\\mathrm{s}$ manner\" In practice, we aim to identify as many rows as possible that satisfy the given condition while adhering to budget constraints (e.g., the total number of tokens allowed to expense). This can be formulated as an online learning problem: given the token budget (or approximately, the number of LLM calls over individual rows, denoted as $B$ ), we seek to balance exploration (to better understand the semantic landscape of all rows) with exploitation (to maximize recall). Drawing inspiration from Bayesian optimization, which employs a surrogate model learned on-the-fly to inform sequential decision-making [36, 19, 22], we use a cheap proxy $\\hat{g}$ as a surrogate for $f({\\mathcal{T}}_{i},{\\mathsf{c o n d}})$ . At each step $t$ , we re-train $\\hat{g}$ with the observed data and select its maximizer to query in the next step. See Algorithm 2. Here we use random noise $\\epsilon_{t,i}$ to allow some degree of exploration that decays with $t$ . Compared to the typical max inner-product search method prevalent in RAG systems, we rely on the online learning to adjust the beliefs, instead of solely relying on predefined embedding similarities. ", "page_idx": 4}, {"type": "text", "text": "4.2 Compilation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Classically, the goal of a compiler is to translate a high level program to low-level machine code, maintaining exact execution results with improved execution speed. Such a lowering process is usually accompanied by optimizations e.g., fusing, selecting the optimal instructions, and kernel optimizations. Our goal is similar: we would like to compile high-level UQL into low-level machine code, with the distinction that the \"machine\" is an LLM, and the \"low-level code\" is the orchestration of prompt calls to the LLM. Given that the primary bottleneck is the LLM API calls, we attempt to maintain the execution semantics while minimizing the cost of LLM calls, as in Figure 3. ", "page_idx": 4}, {"type": "image", "img_path": "t7SGOv5W5z/tmp/0c26f1f946db9fbbcd3aed986481413d424340928fedac58653ede77dfbae520.jpg", "img_caption": ["Figure 3: UQL compiler, in analogy to a typical $C++$ program compiler. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "4.2.1 Planning ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Lowering a query into sequences of concrete execution units is a planning problem: The action space includes the order of clause execution, as well as ways to fuse clauses to execute together. The objective is to minimize the (estimated) LLM cost. Figuring out the best decomposition and combination is usually an NP-hard problem. Fortunately, the number of clauses is very limited for a single query, so we can enumerate possible combinations of ordering and fusions with little overhead. ", "page_idx": 5}, {"type": "text", "text": "The outcome of planning is a specification of a sequence of kernel executions. The input and output of each kernel can be one of the following: ", "page_idx": 5}, {"type": "text", "text": "\u2022 Concrete table: a standard table with only concrete columns. \u2022 Stochastic table: the outcome of unbiased sampling of a table. Importance weights will be attached to each row of the table, and the operation (e.g., SUM, AVG) on this table takes weights into account. ", "page_idx": 5}, {"type": "text", "text": "In the following 3 sections we will explain the building blocks of the compiler, including the kernel implementation, the cost estimation and final instantiation in detail. ", "page_idx": 5}, {"type": "text", "text": "4.2.2 Kernel implementation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Each kernel is an standalone execution unit that reads and produces a (stochastic) table. ", "page_idx": 5}, {"type": "text", "text": "SELECT on structured columns is straightforward. When operating on unstructured columns, we prompt the LLM to extract semantic attributes from the input data. If several extractions share the same source column, we can also group these together into a single prompt to reduce cost. ", "page_idx": 5}, {"type": "text", "text": "WHERE takes a logical formula in disjunctive normal form, such that each conjunction can contain predicates over both unstructured and structured columns. One optimization we make in this case is to perform evaluations over the structured columns first, then simplify (e.g., remove a conjunction if any of the structured column evaluates to false) the logical formula. Any remaining predicates over unstructured columns are then executed on the table filtered by predicates over structured columns. ", "page_idx": 5}, {"type": "text", "text": "GROUP BY first gathers a representative subset of rows from the table, then calls an LLM to extract a taxonomy (i.e., the description of each cluster) for a cluster abstraction. Then the taxonomy is used to classify rows sampled according to the methods defined in Section 4.1. Finally, each row is classified into one of the clusters with the corresponding cluster description in the taxonomy. ", "page_idx": 5}, {"type": "text", "text": "Other standard kernels like ORDER BY are implemented as-is since they are efficient to execute. ", "page_idx": 5}, {"type": "text", "text": "Kernel fusion: Certain clauses can be fused together to achieve significant efficiency gains. ", "page_idx": 5}, {"type": "text", "text": "\u2022 WHERE $^+$ LIMIT can be terminated earlier for non-aggregation queries, once the number of rows specified by LIMIT are retrieved. This is particularly useful for rare event finding. \u2022 SELECT $^+$ GROUPBY when executed together, the semantic attribute extraction of SELECT and taxonomy classification in GROUPBY can be done in the same LLM call to save cost. \u2022 GROUPBY $^+$ WHERE can share the same sampling proposal for the aggregation queries. ", "page_idx": 5}, {"type": "text", "text": "When and how to fuse clauses relies on the planning technique introduced in Section 4.2.1. ", "page_idx": 5}, {"type": "text", "text": "4.2.3 Cost estimation for each kernel ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We only consider the cost of calling the LLM, as this dominates the overall cost per query. Assuming the length of each row in the unstructured data is more or less uniform across rows, then the cost is proportional to the number of rows that fed to the LLM, which we use as the surrogate for estimation. ", "page_idx": 6}, {"type": "text", "text": "\u2022 SELECT maps each row, hence the cost is $|\\tau|$ for the table $\\tau$ fed to SELECT. \u2022 GROUP BY consists of two steps, where taxonomy construction consumes a subset of the input table $\\tau$ , and classification runs $|\\tau|$ LLM calls in parallel. \u2022 WHERE depends on the proposal $p$ . In practice we set a budget $B$ and try to minimize the variance of unbiased estimator or maximize the recall in online learning, as explained in Section 4.1. \u2022 Whenever clauses are fused together, each implementation is responsible for providing a reasonable cost estimate. For example when SELECT and GROUP BY are fused, the estimated cost is the same as GROUP BY alone, as the classification stage of GROUP BY shares the input tokens with SELECT. ", "page_idx": 6}, {"type": "text", "text": "4.2.4 Instantiation of kernels ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The last step of compilation is to generate the machine specific code (e.g., $\\mathbf{x}86$ assembly code) from the intermediate representations (IR). For UQE, this is the process of generating the LLM-specific prompts. For example, when GPT is deployed as the \"machine\", a system prompt like \"You are a helpful assistant\" will be added to the queries. This step also sets the correct context (e.g., the correct structured/unstructured column to associate to, the description of the databases) for the LLM. When such information is not available, one can also leverage the LLM to provide a good suggestion. ", "page_idx": 6}, {"type": "text", "text": "5 Related work ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "While the unstructured data analytics engine is relatively new, there are several related works in the context of unstructured data query and analysis. Approaches like pattern or regexp matching [20] is scalable but not feasible for complex semantic reasoning. RAG [24, 18] based approaches rely on the retrieval quality and is not directly suitable for aggregation queries over entire database. LLMs [4, 2, 33] depict the ability of table analytics [15] to some extent [9, 25], but are still not reliable for large unstructured database analytics yet. ", "page_idx": 6}, {"type": "text", "text": "Our work is closely related to neural symboilic approaches for unstructured data analytics. Early attempts in this line aim to design specialized neural architectures with inductive biases (e.g., attention) to capture a particular form of operation (e.g., filtering a list of objects based on a natural language predicate by their attention scores) [43, 29, 3]. Those differentiable neural \u201coperators\u201d can then be chained together to model more compositional queries, and trained end-to-end using gradient descent. Another direction, in line with our work, is to augment symbolic programs with learnable operators parameterized by neural networks [10]. Those programs are often modeled as discrete latent variables, which can be hard to optimize. In contrast, UQE leverages predictions from LLMs as supervision to train an efficient proxy query model in an online fashion. Similar to UQE, some recent work [11, 38] also adopts LLMs as fuzzy query operators. However, the generated programs treat LLMs as an UDF in a SQL program, which can be very expensive to execute on large databases. Our UQE implements similar but augmented semantics with the focus on the cost efficiency and scalability. Liu et al. [26] optimizes a similar query engine from the system perspective like cache optimization and deduplication, while our work mainly considers algorithmic improvements and is considered as an approximate query engine [28]. These system and algorithm optimizations are actually orthogonal and can be beneficial to jointly consider both for future works. ", "page_idx": 6}, {"type": "text", "text": "In a distantly related topic, text2SQL [47, 46, 21, 35] also leverages models talking to databases, but is mainly for semantic parsing purpose. While it also leverages the advances in LLMs [44, 16, 31, 37], the execution is still on pure SQL and thus is not suitable for unstructured databases. There are also works on leveraging formal query languages to better query LLMs [34, 6], with the focus on controllability of the LLM itself rather than performing analytics on external unstructured data. ", "page_idx": 6}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We benchmark the accuracy and incurred cost of UQE on multimodal unstructured data analytics tasks, with the goal to show and understand when and why UQE can improve accuracy while keeping the cost low. Since the unstructured database analytics is a relatively new task, we construct and compare against several baseline approaches, on a set of tasks created from existing datasets. ", "page_idx": 6}, {"type": "table", "img_path": "t7SGOv5W5z/tmp/769bd816751e132153bc8c0c7dc8683c9a96d15d001f8878fc1c46c857187941.jpg", "table_caption": ["Table 1: Conditional aggregation results on benchmark datasets. We report the relative error and the average cost per query. $^{\\ast}\\mathrm{gpt}{\\cdot}40$ is $50\\%$ cheaper than gpt-4-turbo so we double its budget of tokens; we use claude-3-haiku as the backend LLM. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "t7SGOv5W5z/tmp/c3b2ad4b117a8f7819d403e5bd651979cd5b3a45dc6d04125b591ce20850bee2.jpg", "table_caption": ["Table 2: Semantic retrieval results on several benchmark dataset. We report the F1 score of the retrieved rows and the average cost per query. We run 8 independent queries and report the average F1 and its standard deviation. The result of MIPS is deterministic, so no standard deviation is reported. ", "Baselines: We design the following baselines for comparison "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "\u2022 lc-LLM denotes the long-context LLMs that can directly take a subset of database and a natural language question as input, and produce the desired analysis. We mainly evaluate against several model families, including GPT-4 [2] and Claude-3 [4]. Of course, when evaluating the lc-LLM based approaches, we use the natural language instead of UQL as the prompt. ", "page_idx": 7}, {"type": "text", "text": "\u2022 RAG-based can be applied to some non-aggregation queries, such as semantic retrieval. For the retrieval part we use max inner-product search (MIPS) on top of the same embeddings that are used by UQE, for a controlled experiment. ", "page_idx": 7}, {"type": "text", "text": "\u2022 UDF simply treats the LLM calls as User-defined function of an SQL engine, with the same budget as UQE by default. This approach will not have the advanced sampling / search algorithm as used in UQE, which also serves as an ablation for the effectiveness of our UQE. ", "page_idx": 7}, {"type": "text", "text": "Datasets: We evaluate different approaches on common analytical tasks in three widely used application domains. We use the datasets that were previously created for discriminative tasks, as these datasets contain both the unstructured columns and the structured ones (the labels in the corresponding dataset). We then hide these structured label columns and perform analytical tasks on the unstructured columns, where these hidden structured columns will be used to compute the ground-truth. The text based tasks include IMDB [27] movie reviews, customer service dialogs including Action-Based Conversations Dataset (ABCD [8]) and AirDialog [41], and image based Clevr [23] dataset. Please refer to Appendix C.1 for more information. ", "page_idx": 7}, {"type": "text", "text": "Setup: We use voyage-2 [1] to embed the text-based unstructured columns, and Vertex [40] for multimodal embeddings. For budget constraint queries, we allow different approachces to access at most 128 rows in the database by default. ", "page_idx": 7}, {"type": "table", "img_path": "t7SGOv5W5z/tmp/a8603f7648922f0396d547ae2ea82e6b36a557fe674f71a0bf3f82d3fc78b74f.jpg", "table_caption": ["Table 3: Conditional abstraction and aggregation. "], "table_footnote": ["Figure 4: Variance of different sampling approaches for aggregation queries over 3 text datasets. "], "page_idx": 8}, {"type": "text", "text": "6.1 Main results ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We run queries on different datasets by instantiating the template shown in each of the sections below. The exact natural language and UQL queries can be found in Appendix D and more information including statistics of conditions we used for query and hyperparameters (for UQE we simply use the default hyperparameters for sampling and online learning) can be found in Appendix C. ", "page_idx": 8}, {"type": "text", "text": "6.1.1 Conditional aggregation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "This task provides aggregated statistics over databases with specified conditions, with the template as: ", "page_idx": 8}, {"type": "text", "text": "SELECT COUNT( $^{*},$ ) FROM {table} WHERE {\"satisfies natural language specified condition\"} We report the relative estimation error (i.e., |predict\u2212true_count|/true_count) and its standard deviation in Table 1. For lc-LLM baselines we estimate the count based on groups of unbiased data samples that fed into the prompt. ", "page_idx": 8}, {"type": "text", "text": "For text based aggregation we use claude-3-haiku as the backbone model, where UQE deploys $10\\times$ reduction in relative errors while reducing the cost by a factor of $20\\times$ or more. For the image dataset, since only limited set of LLMs are capable right now, we use gpt-4o as the backbone, and compare with lc-LLM baselines. Thanks to the improved sampling method in UQE, the same gpt-4o consistently achieves improved performance out-of-the-box. To verify this, we feed one image at a time to gpt-4o and manually aggregate the count, the estimation error would be $17.10\\%\\pm13.\\bar{9}5$ and $19.35\\%\\pm13.81\\%$ for the two queries of Clevr, which is twice higher than UQE in the worst case. ", "page_idx": 8}, {"type": "text", "text": "6.1.2 Semantic retrieval ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "This task filters rows in databases that satisfy specified conditions, with the template as: SELECT $^*$ FROM {table} WHERE {\"satisfies natural language specified condition\"} LIMIT B While we limit the output size to be $\\texttt{B}=\\,256$ to keep the total cost within a reasonable budget. The challenging scenarios are when the number of rows that satisfy the predicate is few (i.e., \"rare event finding\"). Table 2 shows similar sets of comparison, but the metric is F1 score which evaluates the quality of SELECT-ed rows. Overall UQE (with claude-3-haiku as backbone LLM) consistently achieves comparable or better performance than the baseline methods. MIPS which uses the same embedding of unstructured data as UQE, has high variance across different types of queries. The queries such as \"dialogs with account access issues\" would be very suitable for MIPS as the embedding similarity is able to capture that well. For queries involving reasoning (e.g., find the images with less than 4 objects), it is pretty hard for pretrained embeddings to express this. ", "page_idx": 8}, {"type": "text", "text": "6.1.3 Abstraction and aggregation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "This task abstracts the intrinsics of each row, and then performs semantics-based GROUP BY, grouping   \nthe common intrinsics across all rows. Finally, it provides aggregated statistics over each group:   \nSELECT derived_attribute, COUNT $(*)$ ) FROM {table}   \nGROUP BY {\"extract an abstract intrinsic attribute specified in natural language\"}   \nAS derived_attribute LIMIT 10 ", "page_idx": 8}, {"type": "text", "text": "The challenging problems in this task are (i) building a taxonomy with good coverage, and (2) bias and variance reduction for groups with small population. The result of this query is a list of ", "page_idx": 8}, {"type": "text", "text": "Figure 5: Recall (moving average with window size 16) against the number of iterations on (from left to right) AirDialog with condition {cancel, no_flight} and Clevr with {obj_count $<~4$ , #spheres $>\\;3\\;.$ }. Colored lines and shades denote median and interquartile ranges across 8 independent queries and gray lines denote individual queries. The gray dashed lines denote the fraction of the positive population in the entire dataset. ", "page_idx": 9}, {"type": "table", "img_path": "t7SGOv5W5z/tmp/992a41c672d1ee7e22d9d05e5b4fb0dac5afb7422202d394ac0b0faa749ff10b.jpg", "table_caption": ["Table 4: Quality at different compute budget B. Table 5: Error of the aggregation operation on "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "tuples of derived attributes and their number of occurrences in the dataset. We use the earth mover\u2019s distance (EMD [30]) as the evaluation metric to compare the extracted tuples and ground-truth tuples. The distance between a pair of attributes is defined by one minus the cosine similarity of their text embeddings. We can see from Table 3 that UQE consistently outperforms baselines while achieving much lower cost. We also show in Appendix C with more qualitative results comparisons. ", "page_idx": 9}, {"type": "text", "text": "6.2 Ablation studies ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We study the effectiveness of UQE for aggregation queries in Section 6.2.1 and non-aggregation queries in Section 6.2.2, and the quality/cost trade-off of UQE in Section 6.2.3. In appendix we provide more results on other modalities C.3, consistency C.4 and latency C.5. ", "page_idx": 9}, {"type": "text", "text": "6.2.1 Variance of different sampling approaches for aggregation queries ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "To decouple the variance introduced by the algorithm and the bias introduced by the LLM based predictors, here we use the ground-truth label as the predictive result and focus on the effectiveness of variance reduction. Figure 4 shows the box plot of different sampling methods. We can see using stratified sampling over the embeddings of unstructured content achieves significant lower variance compared to the uniform random sampling. Also both of these achieve similar expected values, which also justifies the correctness or unbiasedness. ", "page_idx": 9}, {"type": "text", "text": "6.2.2 Efficiency of online learning for non-aggregation queries ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We show the effectiveness of the online learning in terms of the recall as a function of the iteration steps in Figure 5. Compared to the dashed line in the figure which indicates the results of uniform random sampling, the online learning can achieve significant boost in terms of the recall. While for some queries the variance at early iterations can be high, these all converge well in the end. ", "page_idx": 9}, {"type": "text", "text": "6.2.3 Trade-off between cost/latency and accuracy ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Generally the larger compute budget $B$ the better quality UQE will get, and we verify this in Table 4 5. UQE can achieve pretty good quality even with very low budget, and notably compared to the baseline, it achieves similar quality with 16x reduction of the compute needed. We show more results in Section C.5 regarding the compute efficiency. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper proposed an unstructured query engine that leverages 1) the flexibility of LLMs for data understanding; 2) the advances in sampling and online learning for efficient data scanning; 3) and the compiler that bridges these algorithmic workflows with LLMs. We demonstrated its efficiency and accuracy over three analytic tasks on four datasets with two different modalities. However the current work is still very limited in terms of 1) the semantics it lacks, including table join and other types of aggregations; 2) an automated selection of LLMs and sampling configurations; 3) and scaling to even larger databases. We hope to investigate these further in future works. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We thank Carsten Binnig, Howie Xu, Ras Bodik, Xinyi Chen, and the anonymous reviewers for providing helpful discussion and suggestions. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Voyage 2. Voyage 2. https://github.com/voyage-ai/voyageai-python.   \n[2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [3] Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Neural module networks. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 39\u201348, 2015. URL https://api.semanticscholar.org/CorpusID:5276660.   \n[4] AI Anthropic. The claude 3 model family: Opus, sonnet, haiku. Claude-3 Model Card, 2024.   \n[5] S\u00f6ren Becker, Johanna Vielhaben, Marcel Ackermann, Klaus-Robert M\u00fcller, Sebastian Lapuschkin, and Wojciech Samek. Audiomnist: Exploring explainable artificial intelligence for audio analysis on a simple benchmark. Journal of the Franklin Institute, 2023. ISSN 0016-0032. doi: https://doi.org/10.1016/j.jfranklin.2023.11.038. URL https://www.sciencedirect.com/science/article/pii/S0016003223007536.   \n[6] Luca Beurer-Kellner, Marc Fischer, and Martin Vechev. Prompting is programming: A query language for large language models. Proceedings of the ACM on Programming Languages, 7 (PLDI):1946\u20131969, 2023.   \n[7] Don Chamberlin. A complete guide to DB2 universal database. Morgan Kaufmann, 1998.   \n[8] Derek Chen, Howard Chen, Yi Yang, Alex Lin, and Zhou Yu. Action-based conversations dataset: A corpus for building more in-depth task-oriented dialogue systems. arXiv preprint arXiv:2104.00783, 2021.   \n[9] Wenhu Chen. Large language models are few (1)-shot table reasoners. arXiv preprint arXiv:2210.06710, 2022.   \n[10] Xinyun Chen, Chen Liang, Adams Wei Yu, Denny Zhou, Dawn Xiaodong Song, and Quoc V. Le. Neural symbolic reader: Scalable integration of distributed and symbolic representations for reading comprehension. In International Conference on Learning Representations, 2020. URL https://api.semanticscholar.org/CorpusID:212814759.   \n[11] Zhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu Li, Rahul Nadkarni, Yushi Hu, Caiming Xiong, Dragomir Radev, Mari Ostendorf, Luke Zettlemoyer, et al. Binding language models in symbolic languages. arXiv preprint arXiv:2210.02875, 2022.   \n[12] Edgar F Codd. The relational model for database management: version 2. Addison-Wesley Longman Publishing Co., Inc., 1990.   \n[13] Nada Elgendy and Ahmed Elragal. Big data analytics: a literature review paper. In Advances in Data Mining. Applications and Theoretical Aspects: 14th Industrial Conference, ICDM 2014, St. Petersburg, Russia, July 16-20, 2014. Proceedings 14, pages 214\u2013227. Springer, 2014.   \n[14] Ramez Elmasri and Shamkant B Navathe. Database systems: models, languages, design, and application programming. Pearson, 2013.   \n[15] Xi Fang, Weijie Xu, Fiona Anting Tan, Jiani Zhang, Ziqing Hu, Yanjun Qi, Scott Nickleach, Diego Socolinsky, Srinivasan Sengamedu, and Christos Faloutsos. Large language models on tabular data\u2013a survey. arXiv preprint arXiv:2402.17944, 2024.   \n[16] Dawei Gao, Haibin Wang, Yaliang Li, Xiuyu Sun, Yichen Qian, Bolin Ding, and Jingren Zhou. Text-to-sql empowered by large language models: A benchmark evaluation. $A r X i\\nu$ , abs/2308.15363, 2023. URL https://api.semanticscholar.org/CorpusID:261276437.   \n[17] Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. Pal: Program-aided language models. In International Conference on Machine Learning, pages 10764\u201310799. PMLR, 2023.   \n[18] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang. Retrieval-augmented generation for large language models: A survey. arXiv preprint arXiv:2312.10997, 2023.   \n[19] Roman Garnett. Bayesian optimization. Cambridge University Press, 2023.   \n[20] Otis Gospodnetic, Erik Hatcher, and Michael McCandless. Lucene in action. Simon and Schuster, 2010.   \n[21] Jiaqi Guo, Zecheng Zhan, Yan Gao, Yan Xiao, Jian-Guang Lou, Ting Liu, and D. Zhang. Towards complex text-to-sql in cross-domain database with intermediate representation. ArXiv, abs/1905.08205, 2019. URL https://api.semanticscholar.org/CorpusID:159041042.   \n[22] Frank Hutter, Holger H Hoos, and Kevin Leyton-Brown. Sequential model-based optimization for general algorithm configuration. In Learning and Intelligent Optimization: 5th International Conference, LION 5, Rome, Italy, January 17-21, 2011. Selected Papers 5, pages 507\u2013523. Springer, 2011.   \n[23] Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick. Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2901\u20132910, 2017.   \n[24] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:9459\u20139474, 2020.   \n[25] Peng Li, Yeye He, Dror Yashar, Weiwei Cui, Song Ge, Haidong Zhang, Danielle Rifinski Fainman, Dongmei Zhang, and Surajit Chaudhuri. Table-gpt: Table-tuned gpt for diverse table tasks. arXiv preprint arXiv:2310.09263, 2023.   \n[26] Shu Liu, Asim Biswal, Audrey Cheng, Xiangxi Mo, Shiyi Cao, Joseph E Gonzalez, Ion Stoica, and Matei Zaharia. Optimizing llm queries in relational workloads. arXiv preprint arXiv:2403.05821, 2024.   \n[27] Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 142\u2013 150, Portland, Oregon, USA, June 2011. Association for Computational Linguistics. URL http://www.aclweb.org/anthology/P11-1015.   \n[28] Barzan Mozafari and Ning Niu. A handbook for building an approximate query engine. IEEE Data Eng. Bull., 38(3):3\u201329, 2015.   \n[29] Arvind Neelakantan, Quoc V. Le, and Ilya Sutskever. Neural programmer: Inducing latent programs with gradient descent. CoRR, abs/1511.04834, 2015. URL https://api.semanticscholar.org/CorpusID:6715185.   \n[30] Ofir Pele and Michael Werman. Fast and robust earth mover\u2019s distances. In 2009 IEEE 12th international conference on computer vision, pages 460\u2013467. IEEE, 2009.   \n[31] Mohammad Reza Pourreza and Davood Rafiei. Din-sql: Decomposed in-context learning of text-to-sql with self-correction. ArXiv, abs/2304.11015, 2023. URL https://api.semanticscholar.org/CorpusID:258291425.   \n[32] Raghu Ramakrishnan and Johannes Gehrke. Database management systems. McGraw-Hill, Inc., 2002.   \n[33] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jeanbaptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024.   \n[34] Mohammed Saeed, Nicola De Cao, and Paolo Papotti. Querying large language models with sql. arXiv preprint arXiv:2304.00472, 2023.   \n[35] Torsten Scholak, Nathan Schucher, and Dzmitry Bahdanau. Picard: Parsing incrementally for constrained auto-regressive decoding from language models. ArXiv, abs/2109.05093, 2021. URL https://api.semanticscholar.org/CorpusID:237491759.   \n[36] Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P Adams, and Nando De Freitas. Taking the human out of the loop: A review of bayesian optimization. Proceedings of the IEEE, 104 (1):148\u2013175, 2015.   \n[37] Ruoxi Sun, Sercan O Arik, Hootan Nakhost, Hanjun Dai, Rajarishi Sinha, Pengcheng Yin, and Tomas Pfister. Sql-palm: Improved large language modeladaptation for text-to-sql. arXiv preprint arXiv:2306.00739, 2023.   \n[38] D\u00eddac Sur\u00eds, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution for reasoning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11888\u201311898, 2023.   \n[39] James Thorne, Majid Yazdani, Marzieh Saeidi, Fabrizio Silvestri, Sebastian Riedel, and Alon Halevy. Neural databases. arXiv preprint arXiv:2010.06973, 2020.   \n[40] Vertex. Vertex API. https://cloud.google.com/vertex-ai/generativeai/docs/embeddings/get-multimodal-embeddings.   \n[41] Wei Wei, Quoc Le, Andrew Dai, and Jia Li. Airdialogue: An environment for goal-oriented dialogue research. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3844\u20133854, 2018.   \n[42] Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, and Ming Zhou. Layoutlm: Pretraining of text and layout for document image understanding. In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining, pages 1192\u20131200, 2020.   \n[43] Pengcheng Yin, Zhengdong Lu, Hang Li, and Ben Kao. Neural enquirer: Learning to query tables. $A r X i\\nu$ , abs/1512.00965, 2015. URL https://api.semanticscholar.org/CorpusID:6715526.   \n[44] Pengcheng Yin, Graham Neubig, Wen tau Yih, and Sebastian Riedel. Tabert: Pretraining for joint understanding of textual and tabular data. ArXiv, abs/2005.08314, 2020. URL https://api.semanticscholar.org/CorpusID:218674345.   \n[45] Lijun Yu, Jin Miao, Xiaoyu Sun, Jiayi Chen, Alexander G Hauptmann, Hanjun Dai, and Wei Wei. Documentnet: Bridging the data gap in document pre-training. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: Industry Track, pages 707\u2013722, 2023.   \n[46] Tao Yu, Michihiro Yasunaga, Kai-Chou Yang, Rui Zhang, Dongxu Wang, Zifan Li, and Dragomir R. Radev. Syntaxsqlnet: Syntax tree networks for complex and cross-domain text-to-sql task. ArXiv, abs/1810.05237, 2018. URL https://api.semanticscholar.org/CorpusID:52979524.   \n[47] Tao Yu, Rui Zhang, Kai-Chou Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Z Li, Qingning Yao, Shanelle Roman, Zilin Zhang, and Dragomir R. Radev. Spider: A large-scale human-labeled dataset for complex and crossdomain semantic parsing and text-to-sql task. ArXiv, abs/1809.08887, 2018. URL https://api.semanticscholar.org/CorpusID:52815560.   \n[48] Chunhui Yuan and Haitao Yang. Research on k-value selection method of k-means clustering algorithm. J, 2(2):226\u2013235, 2019. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Proof ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Proposition: The optimal proposal distribution $p$ that minimizes the variance of estimation in Eq 2 is $\\bar{p_{i}}\\propto f(T_{i},c o n d)$ . The variance gets $\\boldsymbol{O}$ with this proposal. ", "page_idx": 13}, {"type": "text", "text": "Let\u2019s simplify the notation a bit and use $f(x)$ for $f(\\tau_{i},\\mathsf{c o n d})$ and prove the variance reduction in general cases for binary function $f$ . For the simplicity let\u2019s omit the constant $|\\tau|$ and focus on the estimation of the expectation term. If we come up with a new proposal distribution $p:\\mathcal{T}\\mapsto[0,1]$ where $\\textstyle\\sum_{x\\in{\\mathcal{T}}}p(x)\\,{\\bar{=}}\\,1$ , then we get a new estimator in the following form: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{E}_{x\\sim q}[f(x)]=\\sum_{x\\in{\\mathcal{T}}}[p(x){\\frac{q(x)}{p(x)}}f(x)]=\\mathbb{E}_{x\\sim p}[{\\frac{q(x)}{p(x)}}f(x)]\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We hope this new estimator would have lower variance. Let\u2019s define $\\begin{array}{r}{u(x)=\\frac{q(x)}{p(x)}f(x)}\\end{array}$ for the ease of notation, and look at its variance first: ", "page_idx": 13}, {"type": "equation", "text": "$$\nV a r_{p}(u(x))=\\mathbb{E}_{p}[u^{2}(x)]-\\mathbb{E}_{p}^{2}[u(x)]\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Since our estimator is unbiased, $\\mathbb{E}_{p}^{2}[u(x)]=E_{q}^{2}[f(x)]$ and thus has nothing to do with $p$ , let\u2019s focus on minimizing the first term. More specifically we have the optimization as: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{p}{\\operatorname*{min}}}&{\\mathbb{E}_{p}[u^{2}(x)]}\\\\ {\\mathit{s.t.}}&{p(x)\\geqslant0,\\forall x,}\\\\ &{\\displaystyle\\sum_{x\\in\\mathcal{T}}p(x)=1}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Let: ", "page_idx": 13}, {"type": "equation", "text": "$$\nL(p,\\{\\lambda_{x}\\},\\lambda_{2})=\\sum_{x\\in{\\cal{T}}}\\frac{(q(x)f(x))^{2}}{p(x)}-\\sum_{x\\in{\\cal{T}}}\\lambda_{x}p(x)+\\lambda_{2}(\\sum_{x\\in{\\cal{T}}}p(x)-1)\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "and we can find the saddle point of $\\mathrm{min}_{p}\\,\\mathrm{max}_{\\lambda_{x}\\lambda_{2}}(L(p,\\{\\lambda_{x}\\},\\lambda_{2}))$ using K.K.T condition. ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{-\\frac{(q(x)f(x))^{2}}{p(x)^{2}}-\\lambda_{x}+\\lambda_{2}=0,\\forall x\\in\\mathcal{T}}\\\\ {\\lambda_{x}p(x)=0,p(x)\\geqslant(0),\\forall x}\\\\ {\\lambda_{2}\\sum_{x}(p(x)-1)=0,\\sum_{x}p(x)=1}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "and we can get the optimal solution of ", "page_idx": 13}, {"type": "equation", "text": "$$\np(x)={\\frac{q(x)f(x)}{E_{q}[f(x)]}}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "and put it back into $\\operatorname{Eq}(1)$ we can see the optimal variance would be ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\quad V a r_{p}(u(x))=\\!\\sum_{x}\\frac{(q(x)f(x))^{2}}{\\frac{q(x)f(x)}{\\mathbb{E}_{q}[f(x)]}}-\\mathbb{E}_{p}^{2}[u(x)]}&{}\\\\ {=\\!\\mathbb{E}_{q}[f(x)]\\sum_{x}q(x)f(x)-\\mathbb{E}_{q}^{2}[f(x)]=0}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "which means one sample would be good enough! In our context, $q(x)$ is usually just a constant (e.g., $\\textstyle q(x)\\,=\\,{\\frac{1}{\\tau}}$ for the [Case Count]), and $f(x)\\in\\{0,1\\}$ . In this case, a simplified optimal proposal would be: ", "page_idx": 13}, {"type": "equation", "text": "$$\np(x)\\propto f(x)\\;\\mathrm{and\\;the\\;partition\\;function\\;is}\\;\\mathbb{E}_{q}[f(x)]\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "or in another word, ideally we should have zero-chance to sample from regions where $f(x)=0$ , and have equal chances to sample where $f(x)=1$ . ", "page_idx": 14}, {"type": "text", "text": "B UQL specifications ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Tokenizer ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We use the following pattern matching to tokenize the UQL programs/queries. ", "page_idx": 14}, {"type": "text", "text": "import ply.lex as lex   \nreserved $=$ 'select ' 'SELECT ' 'from ' 'FROM ' 'where ': 'WHERE ', 'as' : 'AS' 'limit ': 'LIMIT ' 'group ': 'GROUP 'order ': 'ORDER ', 'by': 'BY' 'to': 'TO', 'and': 'AND', 'or': 'OR' 'count 'COUNT ', 'avg': 'AVG' 'sum': 'SUM', 'desc ': 'DESC '   \ntokens = [ 'SEPARATOR ', 'ALL', 'NL_LITERAL ' 'VAR_NAME ', 'TABLE_URL ', 'COMPARE_OPERATOR ', 'INTEGER ', 'FLOAT ', 'LEFT_PARENTHESIS ', 'RIGHT_PARENTHESIS '   \n] + list(reserved.values ())   \nt_SEPARATOR $\\mathbf{\\Sigma}=\\mathbf{\\Sigma}\\,\\mathbf{r}\\mathbf{\\Sigma}^{\\prime}$ ,   \nt_ALL $=$ r'\\\\*'   \nt_NL_LITERAL $=x^{\\prime}+\"((?:))\\cdot1[\\neg\"\\{}]\\div1\\cdots$   \nt_COMPARE_ $0\\,\\mathsf{P E R A T0R}\\;\\;=\\;\\;\\mathbf{r}^{\\mathrm{~\\tiny~\\mathsf~{~\\#~}}}(<>|{\\mathrm{~\\tiny~>~}}=|{\\mathrm{~\\tiny~\\mathsf~{~\\underline{~{~\\E}~}}}}=|{\\mathrm{~\\tiny~\\mathsf{~\\underline{~{\\y}~}}}}|<|{\\mathrm{~\\tiny~=~}})$ '   \nt_INTEGER $=\\textbf{r}^{\\prime}\\ \\left[\\mathbf{\\Lambda}-\\right]\\uparrow\\setminus\\mathbb{d}+\\mathbf{\\Lambda}$   \n$\\begin{array}{r l r}{\\mathsf{t\\_F L O A T}}&{{}=}&{\\textbf{r}^{\\prime}\\;\\left[+-\\displaystyle\\right]\\,?\\,\\left[\\,0-9\\,\\right]\\,*\\!\\mathrm{~`~`~}.\\,\\left[\\,0-9\\,\\right]\\,+\\,\"}\\end{array}$   \nt_LEFT_PARENTHESIS $=$ r'\\('   \nt_RIGHT_PARENTHESIS = r'\\)'   \ndef t_VAR_NAME(t): r'[a-zA -Z_][a-zA -Z_0 -9]\\*(\\.[a-zA -Z_][a-zA -Z_0 -9]\\*)\\*' t.type $=$ reserved.get(t.value.lower (), 'VAR_NAME ') return t ", "page_idx": 14}, {"type": "text", "text": "B.2 Grammar ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Below we show the context free grammar of the UQL. Note that this represents a subset of the \"natural query\" analogy to SQL, and we leave other clauses like table join into the future work. ", "page_idx": 14}, {"type": "table", "img_path": "t7SGOv5W5z/tmp/e0aefde14a952426545988e66793183be55b84af2f72847494787d72f8cd2c1e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "uql_query : s e l e c t _ c l a u s e from_clause | s e l e c t _ c l a u s e from_clause optional_clause_combo optional_clause_combo : optional_clause_combo o p t i o n a l _ c l a u s e o p t i o n a l _ c l a u s e ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "o p t i o n a l _ c l a u s e : l i m i t _ c l a u s e | t o _ c l a u s e | where_clause group_by_clause | order_by_cla us e s e l e c t _ c l a u s e : SELECT s e l e c t _ e x p r e s s i o n s e l e c t _ e x p r e s s i o n : s e l e c t _ e x p r e s s i o n SEPARATOR s e l e c t _ l i t e r a l | s e l e c t _ l i t e r a l ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "s e l e c t _ l i t e r a l : ALL | v a r i a b l e _ l i t e r a l | n l _ l i t e r a l a g g r e g a t i o n | INTEGER ", "page_idx": 15}, {"type": "text", "text": "a g g r e g a t i o n : agg_op LEFT_PARENTHESIS VAR_NAME RIGHT_PARENTHESIS | agg_op LEFT_PARENTHESIS ALL RIGHT_PARENTHESIS | agg_op LEFT_PARENTHESIS VAR_NAME RIGHT_PARENTHESIS AS VAR_NAME | agg_op LEFT_PARENTHESIS ALL RIGHT_PARENTHESIS AS VAR_NAME ", "page_idx": 15}, {"type": "text", "text": "agg_op : AVG | COUNT | SUM ", "page_idx": 15}, {"type": "text", "text": "v a r i a b l e _ l i t e r a l : VAR_NAME | VAR_NAME AS VAR_NAME n l _ l i t e r a l : NL_LITERAL | NL_LITERAL AS VAR_NAME from_clause : FROM VAR_NAME where_clause : WHERE where_expression where_expression : where_expression AND p r e d i c a t e | where_expression OR p r e d i c a t e | p r e d i c a t e ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "group_by_clause : GROUP BY group_by_expression group_by_expression : group_by_expression SEPARATOR g r o u p _ b y _ l i t e r a l | g r o u p _ b y _ l i t e r a l ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "g r o u p _ b y _ l i t e r a l : v a r i a b l e _ l i t e r a l | n l _ l i t e r a l ", "page_idx": 15}, {"type": "text", "text": "or der_by_clause : ORDER BY o r d e r _ b y _ e x p r e s s i o n | ORDER BY o r d e r _ b y _ e x p r e s s i o n DESC o r d e r_ b y_ e xp r es si o n : o rd e r_ b y _ e x p r e s s i o n SEPARATOR o r d e r _ b y _ l i t e r a l | o r d e r _ b y _ l i t e r a l ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "table", "img_path": "t7SGOv5W5z/tmp/e804f5bb82733096a9d31122dc5e9a59babc8c3186582483e097283991909a2c.jpg", "table_caption": [], "table_footnote": ["Table 6: Dataset statistics "], "page_idx": 16}, {"type": "text", "text": "| VAR_NAME COMPARE_OPERATOR FLOATl i m i t _ c l a u s e : LIMIT INTEGERt o _ c l a u s e : TO VAR_NAME", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "C Experiments ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "C.1 Datasets ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We evaluate different approaches on common analytical tasks in three widely used application domains. We use the datasets that were previously created for discriminative tasks, as these datasets contain both the unstructured columns and the structured ones (the labels in the corresponding dataset). We then hiden these structured label columns and perform analytical tasks on the unstructured ones, where these hidden structured columns will be used to compute the groundtruth. ", "page_idx": 16}, {"type": "text", "text": "\u2022 User review mining. We use IMDB [27] for semantic analysis of user sentiment. The entire dataset contains 50K highly polar movie reviews with positive and negative sentiment labels. ", "page_idx": 16}, {"type": "text", "text": "\u2022 Goal-oriented (customer service) dialogue systems. We use two datasets for this category, including 1) Action-Based Conversations Dataset (ABCD [8]) for intent identification, which has 10,042 dialogs with 10 distinct user intents requiring unique sequences of actions constrained by policies to achieve task success; and 2) AirDialog [41] for conversation outcome understanding. It contains 402,037 goal-oriented conversation on filght booking with 5 possible ground-truth states. \u2022 Image understanding. We use Clevr [23] dataset for multimodal data understanding and retrieving. Specifically we use the val split of the dataset, which contains 15,000 images of objects containing different number of cylinders, cubes and spheres with different sizes/colors. We down-sample the image to size no more than $128\\times128$ , and only feed the images to the LLMs while holding out scene metadata for evaluation only. ", "page_idx": 16}, {"type": "text", "text": "Also see Table 6 for the detailed statistics of different query conditions. Rare events like cancel in AirDialog is typically challenging to find and aggregate on. ", "page_idx": 16}, {"type": "text", "text": "C.2 Parameter and experiment setup ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "For the embeddings, we use the voyage-2 for text and for images, we use Google Vertex API with dimensionality of 512. We preprocess the embeddings for all the datasets and keep them static during the queries. ", "page_idx": 16}, {"type": "text", "text": "For the aggregation queries, we use faiss 2 to cluster the embeddings into 10 groups, and perform stratified sampling on top. ", "page_idx": 16}, {"type": "text", "text": "For the online learning setting for non-aggregation queries, in our experiment we simply set $\\epsilon_{t,i}$ to be 0. We start training the function $g$ when we collect at least one possible and one negative example labeled by the LLM. Then after every minibatch of samples collected, we train $g$ via linear logistic regression and simply leverage sklearn for that. ", "page_idx": 16}, {"type": "text", "text": "Other parameters that might matter include: the sampling budget $B$ for aggregation queries is 128 and for non-aggregation queries it is 256. For group-by queries the UQE needs a step in building the taxonomy, where the budget we use for that is 16. ", "page_idx": 16}, {"type": "table", "img_path": "t7SGOv5W5z/tmp/e684daddd5bbcf816f9ac98f7026f56304e081ca5669e9b7fca70f83c4ffee0d.jpg", "table_caption": ["Table 7: Retrieval F1 score on AudioMnist dataset. "], "table_footnote": ["We set these parameters based on educated guess and keep them as default across all the queries over all the datasets. "], "page_idx": 17}, {"type": "text", "text": "C.3 Audio modality ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We use the audio MNIST [5] data for this experiment. This dataset contains 30k wav files from 60 speakers pronouncing digits 0-9. We perform the audio semantic retrieval experiments. The query is first converted to audio space using TTS and the corresponding audio embedding is used for MIPS search. We use Gemini Pro 1.5 as the backend model, as it supports the audio inputs nicely. As is shown in Table 7, the proposed UQE consistently does better than alternatives, and it also allows complex queries that require reasoning, while embedding based MIPS is limited to certain types of queries. ", "page_idx": 17}, {"type": "text", "text": "C.4 Consistency of the execution results ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "While UQE is able to leverage the foundation models to analyze the unstructured databases directly, the execution result is not deterministic due to the nature of the stochasticity of the algorithm and the LLMs themselves. In our paper we aim at reducing the variance so as to improve the consistency. However when the backend LLM gets updated, it might also cause the potential inconsistency. Below we analyze the effect of different LLM backends. ", "page_idx": 17}, {"type": "text", "text": "Table 8 shows the effect on the IMDB dataset, where we see little variation. Actually the difference caused by the model switching is much lower than using a worse query engine. Of course, this behavior shift would be task related, but with the advances of LLMs we believe this variation would converge and be more stable to different prompts in the future. ", "page_idx": 17}, {"type": "text", "text": "C.5 Latency ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We report the runtime of UQE with claude-3-haiku as backbone, and lc-gpt-4-turbo as the baseline method in Table 9. We can see UQE achieves low latency in aggregation operations, but higher latency in retrieval. This is due to the online update and re-evaluation of the $g$ function described in Section 4.1.2. The experiments were run on MacBook Pro CPU, so we expect this bottleneck would be alleviated with better engineered system, which we will focus in our future works. ", "page_idx": 17}, {"type": "text", "text": "C.6 Group By qualitative results ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "For single_item_query in ABCD, the items mentioned in the dialogs found by UQE is: ", "page_idx": 17}, {"type": "text", "text": "[ [ ' boots ' ' 3 3 8 ' ] [ ' j a c k e t ' 2 8 2 ' ] [ ' jeans ' 2 6 8 ' ] [ ' s h i r t ' 1 9 0 ' ] ] ", "page_idx": 17}, {"type": "text", "text": "For account_access in ABCD, the issues mentioned in the dialogs found by UQE is: ", "page_idx": 17}, {"type": "text", "text": "[ [ ' Forgot Password ' ' 4 5 7 ' ]   \n[ ' Forgot Username ' $\\phantom{+}40\\,6\\phantom{+}1$   \n[ ' Lost phone f o r two\u2212f a c t o r a u t h e n t i c a t i o n ' ' 3 6 2 ' ] ] ", "page_idx": 17}, {"type": "text", "text": "Which is very close to the ground truth (recover_username, reset_2fa, recover_password). ", "page_idx": 17}, {"type": "table", "img_path": "t7SGOv5W5z/tmp/30577ed400ec2e3554b72120a584d5cafb9e69ebae127ba90a48a51c2fcb80a0.jpg", "table_caption": ["Table 9: Runtime (in seconds) comparison for different types of queries over different benchmarks. "], "table_footnote": ["[ [ ' F l i g h t Ticket Booked ' '199383 '] [ ' No F l i g h t s Available ' '100884 '] [ ' No R e s e r v a t i o n Found ' '80775 '] [ ' F l i g h t R e s e r v a t i o n Cancelled ' ' 5 1 9 3 7 ' ] ] "], "page_idx": 18}, {"type": "text", "text": "Where the ground truth has one more additional outcome (cancel). But since the percentage of cancellation is very small, it is expected that this might be missing from the group by abstraction when number of occurs are very limited. ", "page_idx": 18}, {"type": "text", "text": "D Prompts ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "D.1 Prompts for lc-LLMs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "D.1.1 Task: Conditional aggregation ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "IMDB dataset ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "System prompt ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Read the following movie reviews, and categorize them into either positive or negative class, depending on the sentiment of the review.If the movie has a mixed sentiment, try your best to classify into positive or negative class based on the overall sentiment.In the end, please just output a single number, which is [the total number of positive reviews] ", "page_idx": 18}, {"type": "text", "text": "User prompt ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Below are the reviews:   \n[Review 0]: Some TV programs continue into embarrassment (my beloved 'X$\\hookrightarrow$ Files' comes to mind.)...   \n[Review 1]: The tale of the titular Adam (Mark O' Halloran) and Paul (Tom $\\hookrightarrow$ Murphy), ... ", "page_idx": 18}, {"type": "text", "text": "ABCD dataset System prompt ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The following dialogs between a customer service agent and a customer. Dialogs start by headers such as $^{\\ast\\ast}$ Dialog $^{1\\ast\\ast}$ , \\*\\*Dialog $^{2\\ast\\ast}$ , and so on. Your task is to classify whether the dialog content is about the theme \"account access issue\". Then count how many dialogs are talking about this theme. In the end, output the count as a single number. ", "page_idx": 18}, {"type": "text", "text": "Here is more detailed explanation about Theme \"<THEME>\". Be sure to use this information when you classify. ", "page_idx": 18}, {"type": "text", "text": "Theme \"<THEME>\" dialogs content is about THEME_EXPLANATION. Please perform thorough analysis for each of the dialog. In the end, please \\*\\*only\\*\\* output a single number, which is [the total number of dialogs] that talks about the theme \"account access issue\". ", "page_idx": 18}, {"type": "text", "text": "<THEME> $=$ account access issue   \n<THEME> $=$ requesting detailed specifications of a certain item sold on the   \nwebsite ", "page_idx": 18}, {"type": "text", "text": "<THEME_EXPLANATION> $=$ the customer could not access the account and was locked out, such as couldn\u00b4t recall their username, couldn\u2019t perform two-factor authentication, or forgot their password and couldn\u2019t access their account ", "page_idx": 18}, {"type": "text", "text": "<THEME_EXPLANATION> $=$ the customer needed help with the detailed specifications of a certain item sold on the website, such as inquiries about detailed information of a specific retail item about materials, whether it shrinks, stock availability, etc., but NOT inquiries about promotions, order status, shipping status, questions about how to use the website to purchase an item, or difficulties on using the website to add to cart or purchase an item, or inquiries about subscription ", "page_idx": 19}, {"type": "text", "text": "User prompt ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Below are the dialogs:   \n$^{\\ast\\ast}$ Dialog $0**$ :   \n[agent]: Hello, how can i help you today   \n[customer]: Hello my name is Alessandro Phoenix and I need to make sure   \nthe shipping cost is included on my order ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "AirDialog dataset ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "System prompt ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The following are dialogs between a airline ticketing agent and a customer. Dialogs start by headers such as \\*\\*Dialog $^{1\\ast\\ast}$ , \\*\\*Dialog $^{2\\ast\\ast}$ , and so on. The outcome of the dialog will be one of the following 5 categories: [book]: the agent has booked a flight for the customer (not including the flight change);   \n[cancel]: the agent canceled the existing valid reservation for the customer;   \n[no_reservation]: the customer wants to change or cancel the flight 1037 but there is no valid reservation under this customer;   \n[no_flight]: the customer aims to book a flight from departure to   \ndestination but finds no flights between departure and destination;   \nYour task is to count how many dialogs have outcome <OUTCOME>. Please $**0\\mathtt{n l y}**$ output a single number, which is [the total number of dialogs] that satisfied the above requirements.   \n<OUTCOME> $=$ [book] ", "page_idx": 19}, {"type": "text", "text": "<OUTCOME> $=$ [cancel]<OUTCOME> $=$ [no_reservation]<OUTCOME> $=$ [no_flight]", "page_idx": 19}, {"type": "text", "text": "User prompt ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Below are the dialogs:   \n\\*\\*Dialog $0**$ :   \ncustomer: Hello.   \nagent: Hello, how can I help you today?   \ncustomer: Can you please find a flight from DFW to SEA? \\*\\*Dialog 1\\*\\*:   \ncustomer: Hi, I am Melissa Thompson.   \nagent: Hello, how may I support you today?   \ncustomer: I want to celebrate Thanks giving day 11/23 with my friends at New York. Can you book a ticket for me? ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "Clevr dataset ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Task: Conditional aggregation ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "System prompt ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Please read the following images, and count how many of them show that <CONDITION>. ", "page_idx": 19}, {"type": "text", "text": "<CONDITION> $=$ there are less than 4 objects in the image <CONDITION> $=$ there are more than 3 spheres in the image ", "page_idx": 20}, {"type": "text", "text": "User prompt ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "image\\_0: <base64\\_encoded\\_image> image\\_1: <base64\\_encoded\\_image> ", "page_idx": 20}, {"type": "text", "text": "Please output a single number, which is the total number of images that $\\hookrightarrow$ satisfy the condition. ", "page_idx": 20}, {"type": "text", "text": "D.1.2 Task: Semantic retrieval ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "IMDB dataset ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "System prompt ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Read the following movie reviews, and list the indices of reviews with positive sentiment. If the movie has a mixed sentiment, try your best to classify into positive or negative class based on the overall sentiment.In the end, please only output a list of indices in the format of [review_3, review_7, ...] ", "page_idx": 20}, {"type": "text", "text": "User prompt ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Below are the reviews:   \n[Review 0]: Some TV programs continue into embarrassment (my beloved 'X$\\hookrightarrow$ Files' comes to mind.)...   \n[Review 1]: The tale of the titular Adam (Mark ${0}^{\\prime}$ Halloran) and Paul (Tom $\\hookrightarrow$ Murphy), ... ", "page_idx": 20}, {"type": "text", "text": "ABCD dataset ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "System prompt ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The following are dialogs between a customer service agent and a customer. Dialogs start by headers such as \\*\\*dialog_1\\*\\*, \\*\\*dialog_2\\*\\*, and so on. Your task is to find out the dialogs where <CONDITION>. Please only output a list of indices in the format of [dialog_3, dialog_7, ...] ", "page_idx": 20}, {"type": "text", "text": "<CONDITION> $=$ the customer could not access the account and was locked out, such as couldn\u00b4t recall their username, couldn\u2019t perform two-factor authentication, or forgot their password and couldn\u2019t access their account <CONDITION> $=$ the customer needed help with the detailed specifications of a certain item sold on the website, such as inquiries about detailed information of a specific retail item about materials, whether it shrinks, stock availability, etc., but NOT inquiries about promotions, order status, shipping status, questions about how to use the website to purchase an item, or difficulties on using the website to add to cart or purchase an item, or inquiries about subscription ", "page_idx": 20}, {"type": "text", "text": "User prompt ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Below are the dialogs:   \n\\*\\*dialog_0\\*\\*:   \n[agent]: Hello! Welcome to AcmeBrands, how cani help you?   \n[customer]: Hi. I am very frustrated because I am trying to use your $\\hookrightarrow$ website and it is running SO slowly!   \n\\*\\*dialog_1\\*\\*:   \n[customer]: hi there   \n[agent]: Hi! What can I help you with today?   \n[customer]: i wanted to know if you'd be able to tell me the arm length on $\\hookrightarrow\\;{\\mathfrak{a}}$ shirt i'm thinking of buying? ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "AirDialog dataset ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "System prompt The following are dialogs between a airline ticketing agent and a customer. Dialogs start with headers such as \\*\\*dialog_1\\*\\*, \\*\\*dialog_2\\*\\*, and so on. Your task is to find out the dialogs where CONDITION. Please only output a list of indices in the format of [dialog_3, dialog_7, ...] ", "page_idx": 21}, {"type": "text", "text": "<CONDITION> $=$ the agent has booked a flight for the customer (not including   \nthe flight change)   \n<CONDITION> $=$ the agent canceled the existing valid reservation for the   \ncustomer   \n<CONDITION> $=$ the customer aims to book a flight from departure to   \ndestination but finds no flights between departure and destination   \n<CONDITION> $=$ the customer wants to change or cancel the flight but there   \nis no valid reservation under this customer ", "page_idx": 21}, {"type": "text", "text": "User prompt ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Below are the dialogs:   \n\\*\\*Dialog $0**$ :   \ncustomer: Hello.   \nagent: Hello, how can I help you today?   \ncustomer: Can you please find a flight from DFW to SEA? \\*\\*Dialog 1\\*\\*:   \ncustomer: Hi, I am Melissa Thompson.   \nagent: Hello, how may I support you today?   \ncustomer: I want to celebrate Thanks giving day 11/23 with my friends at New York. Can you book a ticket for me? ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "Clevr dataset ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "System prompt ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Please read and parse the following images. Images start with labels such as \\*\\*image_ $^{0\\ast\\ast}$ , \\*\\*image_ $^{1**}$ , and so on. Your task is to find out the images where <CONDITION>. Please only output a list of indices in the format of [image_3, image_7, ...] <CONDITION> $=$ there are less than 4 objects in the image <CONDITION> $=$ there are more than 3 spheres in the image ", "page_idx": 21}, {"type": "text", "text": "User prompt ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "image_0: <base64_encoded_image> image_1: <base64_encoded_image> ", "page_idx": 21}, {"type": "text", "text": "Given above, the relevant images are: ", "page_idx": 21}, {"type": "text", "text": "D.1.3 Task: Abstraction and aggregation ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "ABCD dataset ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "System prompt ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The following are dialogs between a customer service agent and a customer. Dialogs start with headers such as \\*\\*dialog_1\\*\\*, \\*\\*dialog_ $^{2\\ast\\ast}$ , and so on. Your task is to analyze all the dialogs, and summarize   \n\"<ABSTRACT_ATTRIBUTE>\" into groups. Please output the table of   \nyour analysis, in the format of pairs of (\"<ABSTRACT_ATTRIBUTE>\",   \nnumber_of_dialogs belong to that). Specifically in the format as:   \ngroup 1,number_of_dialogs   \ngroup 2,number_of_dialogs ", "page_idx": 21}, {"type": "text", "text": "<ABSTRACT_ATTRIBUTE> $=$ the type of account access issue <ABSTRACT_ATTRIBUTE> $=$ the single item involved in the dialog ", "page_idx": 22}, {"type": "text", "text": "User prompt ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Below are the dialogs:   \n\\*\\*dialog_ $0**$ :   \n[agent]: good afternoon, how can I help you?   \n[customer]: hey think i mixed up or forgot which username I'm in with you $\\hookrightarrow$ guys as ", "page_idx": 22}, {"type": "text", "text": "\\*\\*dialog_1\\*\\*: [agent]: Hi there, thanks for contacting Acme! How can I help you? ", "page_idx": 22}, {"type": "text", "text": "AirDialog dataset ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "System prompt ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "The following are dialogs between a airline ticketing agent and a customer. Dialogs start with headers such as \\*\\*dialog_1\\*\\*, \\*\\*dialog_ $^{2\\ast\\ast}$ , and so on. Your task is to analyze all the dialogs, and summarize   \n\"<ABSTRACT_ATTRIBUTE>\" into groups. Please output the table of   \nyour analysis, in the format of pairs of (\"<ABSTRACT_ATTRIBUTE>\",   \nnumber_of_dialogs belong to that). Specifically in the format as:   \ngroup 1,number_of_dialogs   \ngroup 2,number_of_dialogs ", "page_idx": 22}, {"type": "text", "text": "<ABSTRACT_ATTRIBUTE> $=$ the outcome of the dialog ", "page_idx": 22}, {"type": "text", "text": "User prompt ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Below are the dialogs:   \n\\*\\*dialog_ $0**$ :   \ncustomer: Hi.   \nagent: Hello. How may I help you?   \ncustomer: I need to book a flight ticket from DEN to EWR to enjoy music $\\hookrightarrow$ festivals. \\*\\*dialog_1\\*\\*:   \ncustomer: Hi.   \nagent: Hello, how may I help you? ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "D.2 Prompts for UQE-orchestrated LLMs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "D.2.1 Task: Conditional aggregation, Semantic retrieval ", "page_idx": 22}, {"type": "text", "text": "IMDB dataset ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "System prompt ", "page_idx": 22}, {"type": "text", "text": "Please analyze the following movie review, and only reply <True> if <WHERE_CLAUSE>, or <False> otherwise. ", "page_idx": 22}, {"type": "text", "text": "<WHERE_CLAUSE> $=$ the review sentiment is overall positive ", "page_idx": 22}, {"type": "text", "text": "User prompt ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "[Movie review]: May I please have my $\\Phi13.00\\$ back? I would have rather watched \"Hydro- Electric Power Comes to North America\"... ", "page_idx": 22}, {"type": "text", "text": "ABCD dataset ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "System prompt ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Read the following customer support dialog between an agent and a customer, and only reply <True> if <WHERE_CLAUSE>, or <False> otherwise. ", "page_idx": 23}, {"type": "text", "text": "<WHERE_CLAUSE> $=$ the customer could not access the account and was locked out, such as couldn\u00b4t recall their username, couldn\u2019t perform two-factor authentication, or forgot their password and couldn\u2019t access their account <WHERE_CLAUSE> $=$ the customer needed help with the detailed specifications of a certain item sold on the website, such as inquiries about detailed information of a specific retail item about materials, whether it shrinks, stock availability, etc., but NOT inquiries about promotions, order status, shipping status, questions about how to use the website to purchase an item, or difficulties on using the website to add to cart or purchase an item, or inquiries about subscription ", "page_idx": 23}, {"type": "text", "text": "User prompt ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "[Dialog]: [agent]: Hi! Thank you for contacting us today. How can I help you?   \n[customer]: I\u2019m pretty upset that a jacket that I ordered is now saying that it is out of stock. Do you know when it will be back in stock? [agent]: I am so sorry that happened to you   \n[agent]: Yes, let me look in to that for you   \n[action]: Searching the FAQ pages ... ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "AirDialog dataset ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "System prompt ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Read the following airline ticketing dialog between the customer and the agent, and only reply <True> if <WHERE_CLAUSE>, or <False> otherwise. ", "page_idx": 23}, {"type": "text", "text": "<WHERE_CLAUSE> $=$ the agent has booked a flight for the customer (not   \nincluding the flight change)   \n<WHERE_CLAUSE> $=$ the agent canceled the existing valid reservation for the   \ncustomer   \n<WHERE_CLAUSE> $=$ the customer aims to book a flight from departure to   \ndestination but finds no flights between departure and destination   \n<WHERE_CLAUSE> $=$ the customer wants to change or cancel the flight but   \nthere is no valid reservation under this customer ", "page_idx": 23}, {"type": "text", "text": "User prompt ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "[Dialog]: customer: Hello.   \nagent: Hello, how can I help you?   \ncustomer: Please book a flight ticket from CLT to DEN. agent: Sure, let me know your travelling dates. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "Clevr dataset ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "System prompt ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Read the following image, and only reply <True> if <WHERE_CLAUSE>, or <False> otherwise. ", "page_idx": 23}, {"type": "text", "text": "<WHERE_CLAUSE> $=$ there are less than 4 objects in the image <WHERE_CLAUSE> $=$ there are more than 3 spheres in the image ", "page_idx": 23}, {"type": "text", "text": "User prompt ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Image: <base64_encoded_image> ", "page_idx": 23}, {"type": "text", "text": "D.2.2 Task: Abstraction and aggregation ", "page_idx": 23}, {"type": "text", "text": "ABCD dataset ", "page_idx": 23}, {"type": "text", "text": ". Building taxonomy ", "page_idx": 23}, {"type": "text", "text": "System prompt ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "The following are dialogs between a customer service agent and a customer. Dialogs start with headers such as \\*\\*dialog_1\\*\\*, \\*\\*dialog_ $^{2\\ast\\ast}$ , and so on.   \nYour task is to analyze all the dialogs, and summarize   \n\"<ABSTRACT_ATTRIBUTE>\" into groups. Please output the table of your analysis, in the format of pairs of (\"<ABSTRACT_ATTRIBUTE>\", number_of_dialogs belong to that). Specifically in the format as: group 1,number_of_dialogs   \ngroup 2,number_of_dialogs ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "<ABSTRACT_ATTRIBUTE> $=$ the type of account access issue <ABSTRACT_ATTRIBUTE> $=$ the single item involved in the dialog ", "page_idx": 24}, {"type": "text", "text": "User prompt ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Below are the dialogs:   \n\\*\\*dialog_ $^{0\\ast\\ast}$ :   \n[agent]: good afternoon, how can I help you?   \n[customer]: hey think i mixed up or forgot which username I'm in $\\hookrightarrow$ with you guys as   \n\\*\\*dialog_1\\*\\*:   \n[agent]: Hi there, thanks for contacting Acme! How can I help you \u2192? ", "page_idx": 24}, {"type": "text", "text": "2. Group-wise conditional aggregation ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "System prompt ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Read the given airline ticketing dialog between an agent and a $\\hookrightarrow$ customer, and classify issue, into one or several $\\hookrightarrow$ categories below. Here are the description of the 2 $\\hookrightarrow$ categories:   \n[0]: Forgot Username   \n[1]: Forgot PasswordOnly reply the index of the category, $\\hookrightarrow$ separated by \",\". Here is the example format:   \n[0, 3] ", "page_idx": 24}, {"type": "text", "text": "User prompt ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Here is the customer support dialog:   \n[agent]: Hello, how can i help you today   \n[customer]: Hi. I seem to have forgotten my username   \n[agent]: Okay lets get that for you, could i get your Full Name $\\hookrightarrow$ Zip Code Email Adress and Phone Number please   \n[customer]: Sanya Afzal ", "page_idx": 24}, {"type": "text", "text": "AirDialog dataset ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Building taxonomy ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "System prompt ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "The following are dialogs between a airline ticketing agent and a customer. Dialogs start with headers such as \\*\\*dialog_1\\*\\*, \\*\\*dialog_ $^{2\\ast\\ast}$ , and so on. ", "page_idx": 24}, {"type": "text", "text": "Your task is to analyze all the dialogs, and summarize \"<ABSTRACT_ATTRIBUTE>\" into groups. Please output the table of your analysis, in the format of pairs of (\"<ABSTRACT_ATTRIBUTE>\", number_of_dialogs belong to that). Specifically in the format as: group 1,number_of_dialogs ", "page_idx": 24}, {"type": "text", "text": "group 2,number_of_dialogs ", "page_idx": 25}, {"type": "text", "text": "<ABSTRACT_ATTRIBUTE> $=$ the outcome of the dialog ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "User prompt ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Below are the dialogs:   \n\\*\\*dialog_ $^{0\\ast\\ast}$ :   \ncustomer: Hi.   \nagent: Hello. How may I help you?   \ncustomer: I need to book a flight ticket from DEN to EWR to enjoy $\\hookrightarrow$ music festivals.   \n\\*\\*dialog_1\\*\\*:   \ncustomer: Hi.   \nagent: Hello, how may I help you? ", "page_idx": 25}, {"type": "text", "text": "2. Group-wise conditional aggregation System prompt ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Read the given airline ticketing dialog between the customer and $\\hookrightarrow$ the agent, and classify outcome, into one or several $\\hookrightarrow$ categories below. Here are the description of the 2 $\\hookrightarrow$ categories:   \n[0]: Reservation cancelled   \n[1]: Ticket bookedOnly reply the index of the category, separated $\\hookrightarrow$ by \",\". Here is the example format:   \n[0, 3] ", "page_idx": 25}, {"type": "text", "text": "User prompt ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Here is the airline ticketing dialog: customer: Hello   \nagent: Hello, how may I help you?   \ncustomer: Can you help me to book a flight ticket from SEA to AUS $\\hookrightarrow\\;?$   \nagent: Sure, we are glad to help you. May I know your travelling $\\hookrightarrow$ dates? ", "page_idx": 25}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We provided thorough experiments and explanations of the algorithms to jusify the claim. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 26}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: We discussed that in the conclusion section and we pointed out three major limitations. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 26}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We have the proof for the optimal variance reduction proposal in the appendix. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 27}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We use public LLMs on public benchmarks, and the prompts are all included in the appendix. For our work we have provided enough details for reimplementation, and we will work on open sourcing as well. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 27}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 28}, {"type": "text", "text": "Answer: [No] ", "page_idx": 28}, {"type": "text", "text": "Justification: The data and prompts are all public and we have provided the information. We are working on open sourcing the code after going through the internal approval process. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 28}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We have provided the information in the appendix and main paper. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 28}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: We provided the variance of the results in almost all tables when applicable. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We mainly use public API. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 29}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We have checked. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 29}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: This work is a pure algorithmic and enginnering study for new forms of databases. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 29}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 30}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: No risk as far as we can see. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 30}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: We have properly cited the models (GPT4, Claude3) we used in the paper. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA]   \nJustification: No new assets. Guidelines:   \n\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA]   \nJustification: Not involved. Guidelines:   \n\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: no crowdsourcing nor research ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 31}]