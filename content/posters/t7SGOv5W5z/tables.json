[{"figure_path": "t7SGOv5W5z/tables/tables_7_1.jpg", "caption": "Table 1: Conditional aggregation results on benchmark datasets. We report the relative error and the average cost per query. *gpt-4o is 50% cheaper than gpt-4-turbo so we double its budget of tokens; \u2020 we use claude-3-haiku as the backend LLM.", "description": "This table presents the results of conditional aggregation experiments performed on four benchmark datasets (IMDB, ABCD, AirDialog, and Clevr).  For each dataset, it shows the relative error and average cost per query for several methods: lc-gpt-4-turbo, lc-claude-3-opus, UDF, and UQE.  The relative error measures the accuracy of each method in performing conditional aggregations. The average cost per query reflects the computational expense.  The table highlights the significant cost reduction and improved accuracy achieved by UQE compared to other methods, especially when using the gpt-4o model.", "section": "6 Experiments"}, {"figure_path": "t7SGOv5W5z/tables/tables_7_2.jpg", "caption": "Table 2: Semantic retrieval results on several benchmark dataset. We report the F1 score of the retrieved rows and the average cost per query. We run 8 independent queries and report the average F1 and its standard deviation. The result of MIPS is deterministic, so no standard deviation is reported.", "description": "This table presents the results of semantic retrieval experiments on several benchmark datasets.  The F1 score, a metric evaluating the accuracy of retrieved data, is reported for each dataset, along with the average cost per query.  Eight independent queries were run for each dataset, and the average F1 score and its standard deviation are given.  Note that the MIPS (Max Inner Product Search) method produced deterministic results, so no standard deviation is shown.", "section": "6 Experiments"}, {"figure_path": "t7SGOv5W5z/tables/tables_8_1.jpg", "caption": "Table 1: Conditional aggregation results on benchmark datasets. We report the relative error and the average cost per query. *gpt-4o is 50% cheaper than gpt-4-turbo so we double its budget of tokens; \u2020 we use claude-3-haiku as the backend LLM.", "description": "This table presents the results of conditional aggregation experiments performed on four different benchmark datasets (IMDB, ABCD, AirDialog, and Clevr).  For each dataset, the relative error and average cost per query are reported for different methods: lc-gpt-4-turbo, lc-claude-3-opus, UDF+ (with gpt-4-turbo), UDF-gpt-4o, and UQE+ (with gpt-4-turbo) and UQE-gpt-4o.  The table highlights the superior performance of UQE in terms of both accuracy and cost.", "section": "6 Experiments"}, {"figure_path": "t7SGOv5W5z/tables/tables_9_1.jpg", "caption": "Table 1: Conditional aggregation results on benchmark datasets. We report the relative error and the average cost per query. *gpt-4o is 50% cheaper than gpt-4-turbo so we double its budget of tokens; \u2020 we use claude-3-haiku as the backend LLM.", "description": "This table presents the results of conditional aggregation experiments conducted on four benchmark datasets (IMDB, ABCD, AirDialog, and Clevr).  The table compares the performance of UQE (Unstructured Query Engine) against several baseline methods, including different LLM (Large Language Model) configurations and a UDF (User-Defined Function) approach.  For each dataset and method, the relative error (the percentage difference between the estimated and true counts) and the average cost per query (in USD) are reported. The table highlights that UQE achieves significantly lower relative errors and costs than baseline methods across all datasets.", "section": "6 Experiments"}, {"figure_path": "t7SGOv5W5z/tables/tables_14_1.jpg", "caption": "Table 1: Conditional aggregation results on benchmark datasets. We report the relative error and the average cost per query. *gpt-4o is 50% cheaper than gpt-4-turbo so we double its budget of tokens; \u2020we use claude-3-haiku as the backend LLM.", "description": "This table presents the results of conditional aggregation queries on four benchmark datasets (IMDB, ABCD, AirDialog, and Clevr).  It compares the performance of UQE against several baselines (lc-gpt-4-turbo, lc-claude-3-opus, UDF+, and UDF-gpt-4o). The table shows the relative error (percentage difference between predicted and actual counts) and the average cost (in USD) per query for each method and dataset.  The results highlight UQE's superior accuracy and cost efficiency, especially when using gpt-4o as the LLM.", "section": "6 Experiments"}, {"figure_path": "t7SGOv5W5z/tables/tables_16_1.jpg", "caption": "Table 6: Dataset statistics", "description": "This table presents the statistics of four benchmark datasets used in the paper's experiments.  For each dataset (Airdialog, ABCD, IMDB, Clevr), it shows the different conditions or queries used and the percentage of the data that satisfies each condition.  This helps to understand the distribution of data within each dataset and the relative difficulty or rarity of different query types.", "section": "6 Experiments"}, {"figure_path": "t7SGOv5W5z/tables/tables_17_1.jpg", "caption": "Table 8: Performance of UQE with different LLM backends.", "description": "This table presents the performance comparison of UQE (Unstructured Query Engine) using different Large Language Models (LLMs) for both retrieval and aggregation tasks.  It compares the F1 scores for retrieval and the relative errors for aggregation, showing how UQE performs with different LLMs as its backend. The results demonstrate the impact of the LLM choice on UQE's accuracy and efficiency.", "section": "6 Experiments"}, {"figure_path": "t7SGOv5W5z/tables/tables_18_1.jpg", "caption": "Table 9: Runtime (in seconds) comparison for different types of queries over different benchmarks.", "description": "This table compares the runtime of UQE and lc-gpt-4-turbo in seconds for different types of queries (Conditional Aggregation and Semantic Retrieval) across different benchmark datasets (Clevr, ABCD, IMDB, AirDialog).  It shows that UQE generally has much lower runtime than lc-gpt-4-turbo, highlighting its efficiency in executing unstructured queries.", "section": "4.2 Compilation"}]