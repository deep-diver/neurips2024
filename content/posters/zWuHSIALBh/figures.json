[{"figure_path": "zWuHSIALBh/figures/figures_1_1.jpg", "caption": "Figure 1: Models' helpfulness on Alpaca Eval vs factuality on biography. Helpfulness is measured by models' win rate over our baseline SFT + DPO on Alpaca Eval. Dot size represents average length of bio generation.", "description": "This figure shows the trade-off between helpfulness and factuality of different large language models (LLMs).  Helpfulness is determined by the model's win rate against a baseline model (SFT + DPO) on the Alpaca Eval benchmark.  Factuality is measured using FACTSCORE on a biography generation task.  The size of each dot corresponds to the average length of the generated biographies, indicating a correlation between response length and the tendency to hallucinate.  The models shown include SFT (supervised fine-tuning), SFT + DPO (SFT with direct preference optimization), and RLHF (reinforcement learning from human feedback).  The figure highlights that while increasing helpfulness, standard alignment techniques can negatively affect factuality.", "section": "1 Introduction"}, {"figure_path": "zWuHSIALBh/figures/figures_4_1.jpg", "caption": "Figure 3: Illustrations of (a) response generation using a pre-trained LLM (PT) with few-shot demonstration; (b) factuality-aware alignment.", "description": "This figure illustrates the process of response generation using a pre-trained large language model (LLM) and the proposed factuality-aware alignment method. (a) shows the generation process with few-shot learning from the pre-trained model, where a small number of instructions and corresponding responses are used to guide the generation process. (b) illustrates the factuality-aware alignment, in which the LLM is trained to classify instructions as either fact-based or non-fact-based, with distinct training procedures for each type of instruction. For fact-based instructions, the model is trained on the responses generated by the pre-trained model to enhance factuality; whereas, for non-fact-based instructions, the model is trained on human-generated responses. This approach improves both the factuality and the instruction-following capability of the LLM.", "section": "4 Factuality-Aware Alignment"}, {"figure_path": "zWuHSIALBh/figures/figures_17_1.jpg", "caption": "Figure 1: Models' helpfulness on Alpaca Eval vs factuality on biography. Helpfulness is measured by models' win rate over our baseline SFT + DPO on Alpaca Eval. Dot size represents average length of bio generation.", "description": "This figure compares the helpfulness and factuality of different language models on two tasks: Alpaca Eval and biography generation.  Helpfulness is assessed using the win rate against a baseline model (SFT + DPO) on the Alpaca Eval benchmark. Factuality is implicitly represented through the FACTSCORE metric applied to biographies.  The size of the dots corresponds to the average length of the generated biographies.  The figure suggests a trade-off between helpfulness (instruction following) and factuality (accuracy of generated content) in the language models, with longer biographies tending to be less factual.", "section": "1 Introduction"}, {"figure_path": "zWuHSIALBh/figures/figures_17_2.jpg", "caption": "Figure 1: Models' helpfulness on Alpaca Eval vs factuality on biography. Helpfulness is measured by models' win rate over our baseline SFT + DPO on Alpaca Eval. Dot size represents average length of bio generation.", "description": "This figure shows a scatter plot comparing the helpfulness and factuality of different language models on two tasks: Alpaca Eval (measuring helpfulness) and biography generation (measuring factuality).  Helpfulness is represented by the win rate against a baseline model (SFT + DPO) on Alpaca Eval. Factuality is measured using FACTSCORE.  The size of each data point corresponds to the average length of the generated biographies, illustrating a potential correlation between response length and factuality (longer responses tend to be less factual).", "section": "1 Introduction"}, {"figure_path": "zWuHSIALBh/figures/figures_17_3.jpg", "caption": "Figure 1: Models' helpfulness on Alpaca Eval vs factuality on biography. Helpfulness is measured by models' win rate over our baseline SFT + DPO on Alpaca Eval. Dot size represents average length of bio generation.", "description": "This figure shows the trade-off between helpfulness and factuality of different language models.  Helpfulness is assessed using the Alpaca Eval benchmark, measuring the model's win rate against a baseline model (SFT + DPO). Factuality is evaluated on a biography generation task, and is represented by the FACTSCORE.  The size of each dot in the scatter plot corresponds to the average length of the generated biographies, indicating a potential correlation between response length and hallucination.", "section": "1 Introduction"}, {"figure_path": "zWuHSIALBh/figures/figures_17_4.jpg", "caption": "Figure 1: Models' helpfulness on Alpaca Eval vs factuality on biography. Helpfulness is measured by models' win rate over our baseline SFT + DPO on Alpaca Eval. Dot size represents average length of bio generation.", "description": "This figure shows the trade-off between helpfulness and factuality of different language models.  Helpfulness is assessed using the Alpaca Eval benchmark, measuring the model's win rate against a baseline model (SFT + DPO). Factuality is evaluated specifically on biography generation tasks.  The size of each data point corresponds to the average length of the generated biographies. The figure suggests that models prioritizing helpfulness (longer responses) tend to sacrifice factuality, as indicated by a lower FACTSCORE.", "section": "1 Introduction"}]