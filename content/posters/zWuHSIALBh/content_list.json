[{"type": "text", "text": "FLAME : Factuality-Aware Alignment for Large Language Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Sheng-Chieh ${\\mathbf{Lin}^{1*}}$ \u2217, Luyu $\\mathbf{Gao}^{2}$ , Barlas $\\mathbf{0guz^{3}}$ , Wenhan Xiong3, Jimmy $\\mathbf{Lin^{1}}$ , Wen-tau $\\mathbf{Y}\\mathbf{i}\\mathbf{h}^{3}$ , Xilun Chen3\u2020 ", "page_idx": 0}, {"type": "text", "text": "University of Waterloo1, Carnegie Mellon University2, Meta AI3 s269lin@uwaterloo.ca, xilun@meta.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Alignment is a procedure to fine-tune pre-trained large language models (LLMs) to follow natural language instructions and serve as helpful AI assistants. We have observed, however, that the conventional alignment process fails to enhance the factual accuracy of LLMs, and often leads to the generation of more false facts (i.e., hallucination). In this paper, we study how to make the LLM alignment process more factual, by first identifying factors that lead to hallucination in both alignment steps: supervised fine-tuning (SFT) and reinforcement learning (RL). In particular, we find that training the LLM on new or unfamiliar knowledge can encourage hallucination. This makes SFT less factual as it trains on humanlabeled data that may be novel to the LLM. Furthermore, reward functions used in standard RL often inadequately capture factuality and favor longer and more detailed responses, which inadvertently promote hallucination. Based on these observations, we propose FactuaLity-aware AlignMEnt (FLAME ), comprised of factuality-aware SFT and factuality-aware RL through direct preference optimization. Experiments show that our proposed FLAME guides LLMs to output more factual responses while maintaining their instruction-following capability. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Alignment [Ouyang et al., 2022] is a procedure to make pre-trained large language models (LLMs) [Brown et al., 2020, Touvron et al., 2023] follow human instructions and serve as helpful AI assistants. Despite significant progress in general LLM alignment [Ouyang et al., 2022, Bai et al., 2022, Yuan et al., 2024], state-of-the-art aligned LLMs are still prone to generate false claims [OpenAI, 2023, Min et al., 2023]. In this work, we therefore attempt to advance the understanding of the underlying causes of LLM hallucination as well as its relation to the alignment procedure. ", "page_idx": 0}, {"type": "text", "text": "We consider the commonly seen alignment process consisting of two training phases: (1) supervised fine-tuning (SFT) [Sanh et al., 2022]; (2) reinforcement learning (RL) with human [RLHF, Ouyang et al., 2022, Bai et al., 2022] or automated feedback [RLAIF, Bai et al., 2023]. In our study, we find that both the SFT and RL steps in the standard alignment process may actually encourage LLMs to hallucinate. First, in the SFT stage, LLMs are fine-tuned with diverse instructions paired with humancreated high-quality responses. While this leads to strong instruction-following capability [Ouyang et al., 2022, K\u00f6pf et al., 2023, Zhou et al., 2023, Touvron et al., 2023], our study shows that such human-labeled responses may present new or unknown information to the LLM. This, in turn, may inadvertently promote hallucination. Second, we find that the standard reward used in the RL stage often prefers longer and more detailed responses [Singhal et al., 2023, Chen et al., 2024b, Yuan et al., 2024]. Consequently, a reward-hacking model ends up with a tendency to produce longer claims with more non-factual information, as shown in the black dots in Figure 1. One possible reason is that most existing RLHF or RLAIF approaches rely on a single scalar reward to represent preference, which struggles to cover multiple alignment skill sets [Ye et al., 2024] and is likely to under-present the aspect of factuality [Hosking et al., 2024]. ", "page_idx": 0}, {"type": "image", "img_path": "zWuHSIALBh/tmp/3999bbc0ed755432c6237c5c70444e1b1ba87a8b0ff847d2f8cdf9166eb701f9.jpg", "img_caption": ["Figure 1: Models\u2019 helpfulness on Alpaca Eval vs factuality on biography. Helpfulness is measured by models\u2019 win rate over our baseline $\\mathrm{SFT}+\\mathrm{DPO}$ on Alpaca Eval. Dot size represents average length of bio generation. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To address the aforementioned issues, we study the key factors which impact factuality during alignment. In particular, we first conduct a pilot study on the biography generation task [Min et al., 2023] in a more controlled setting where the alignment process focuses solely on factuality (Section 3). Our key observation is that an LLM hallucinates more if it is fine-tuned on new knowledge in either the SFT or the RL stage. For example, an LLM becomes significantly less factual when fine-tuned on responses produced by a model with access to external knowledge (e.g. a retrieval augmented LLM), even though those responses are more factual themselves. Similarly, hallucination is greatly increased if RLAIF is performed on preference pairs that consist of retrieval-augmented LLM output as positive examples and the LLM\u2019s own output as negative examples. In comparison, we discover that fine-tuning a pre-trained LLM on a subset of its own generations selected by factuality yields more factual responses and reduces hallucinations. ", "page_idx": 1}, {"type": "text", "text": "Next, we apply our findings to improve the factuality of the general LLM alignment process, which is more challenging due to the diversity of instructions. As shown in Figure 2, we observe that some instructions require factual responses while the others do not, and therefore would require different alignment treatments. We first identify fact-based instructions that require factual responses and leverage the findings in our pilot study to create additional training data at both SFT and RL stages to explicitly guide LLMs to output factual responses. Specifically, at the SFT stage, for fact-based instructions, instead of using human created seed training data, we elicit knowledge from the pre-trained LLM and construct training data using its own pre-trained knowledge. This can prevent fine-tuning the LLM on knowledge unknown to itself. At the RL stage, we create additional preference pairs focused on factuality for fact-based instructions, which are combined with the standard preference pairs for instruction following during Direct Preference Optimization [DPO; Rafailov et al., 2023]. ", "page_idx": 1}, {"type": "text", "text": "We evaluate models on Alpaca Eval [Dubois et al., 2024] and Biography, using win rate for instructionfollowing capability and FACTSCORE [Min et al., 2023] for factuality evaluation. As shown in Figure 1, using our FLAME method $(\\mathrm{SFT}^{\\ast}+\\mathrm{DPO}^{\\ast})$ ), a significantly higher FACTSCORE $(+5.6$ pts) is achieved compared to the standard alignment process $(\\mathrm{SFT+DPO)}$ ), without sacrificing the LLM\u2019s instruction-following capability $51.2\\%$ win rate). Our ablation study also indicates that identifying fact-based instructions is the key to factual alignment in the general alignment setting. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Alignment. Since pre-trained LLMs cannot accurately follow human instructions, a bunch of work has been proposed to improve LLM alignment through SFT and RL. Some propose to improve SFT through data curation [Zhou et al., 2023, Chen et al., 2024a], diverse instruction augmentation [Wang et al., 2023a, Li et al., 2024] while others focus on RL with human feedback [Ouyang et al., 2022, Bai et al., 2022], AI feedback [Bai et al., 2023, Sun et al., 2024, Yuan et al., 2024]. The main goal of these alignment approaches is instruction-following capability (or helpfulness), which may guide LLMs to output detailed and lengthy responses [Singhal et al., 2023] but inevitably encourage hallucination. ", "page_idx": 2}, {"type": "text", "text": "Factuality. Prior work has highlighted the issue of hallucination in LLMs [Gao et al., 2022, Kandpal et al., 2023, Mallen et al., 2023]. To address the issue, important research lines are factuality evaluation [Min et al., 2023, Wang et al., 2023b, Chern et al., 2023] and improvement. Some trainingfree approaches to improve LLMs\u2019 factuality include external knowledge augmentation [Gao et al., 2022, Kandpal et al., 2023, Cheng et al., 2023, Jiang et al., 2023] and specialized decoding [Li et al., 2023, Chuang et al., 2024]. ", "page_idx": 2}, {"type": "text", "text": "Recent studies apply RL to improve LLMs\u2019 factuality. For example, Tian et al. [2024] propose to construct factuality preference pairs for direct preference optimization [DPO; Rafailov et al., 2023], which is closely related to our work. However, they focus solely on enhancing LLMs\u2019 factuality through DPO but overlook its potential impact on the models\u2019 instruction-following capability, as demonstrated in our experiments. In contrast, our work provides a comprehensive examination of improving LLMs\u2019 factuality and instruction-following ability through fine-tuning approaches encompassing both SFT and DPO. Concurrent to our work, Kang et al. [2024] find that LLMs tend to hallucinate when facing unfamiliar queries. They consider improving LLMs\u2019 factuality as teaching LLMs to output abstaining or less detailed responses on such unfamiliar queries, a similar behavior observed from our LLMs fine-tuned with FLAME (see case studies in Section 6.5). It is worth mentioning that both prior studies focus on a simplified scenario as our pilot study in Section 3: fine-tuning LLMs to improve factuality on a single task (e.g., fine-tuning and evaluating on biography generation). In contrast, we consider the general alignment task, where LLMs are given diverse and complex instructions. ", "page_idx": 2}, {"type": "text", "text": "3 A Pilot Study on Factual Alignment ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we first study how to align large language models (LLMs) to be more factual. We use biography generation as the task of our pilot study for two main reasons: (1) Biography generation is a simplified setting where factuality is the sole focus of the alignment process. As we will discuss in Section 4, studying factual alignment on diverse human instructions is more complex, as the alignment process encompasses aspects beyond factuality, such as helpfulness and safety. (2) Evaluating the factuality of biography generation is relatively easy since Wikipedia covers sufficient information for public figures and most of the facts about a person are non-debatable [Min et al., 2023]. ", "page_idx": 2}, {"type": "text", "text": "3.1 Alignment for Biography Generation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "A standard alignment procedure consists of supervised fine-tuning (SFT) and reinforcement learning (RL). In this pilot study, our main goal is to teach LLMs to generate biography with reduced misinformation. For the experiment, we compile training and evaluation datasets comprising 500 and 183 diverse human entities, respectively (further details provided in Appendix A.1). We employ FACTSCORE [FS; Min et al., 2023] as the automated metric for assessing factuality, given its finegrained evaluation capabilities for long-form text generation and its strong correlation with human judgments.3 To study factuality alignment in this pilot study, we posit that training data is needed where the responses are more factual than the LLM\u2019s own generations. Thus, we use retrievalaugmented LLMs [RAG; Lewis et al., 2020] to generate training data, which has been shown to output more factual responses [Mialon et al., 2023]. ", "page_idx": 2}, {"type": "text", "text": "Throughout the paper, we refer to the pre-trained (PT), supervised fine-tuned (SFT), and direct preference optimization (DPO) fine-tuned LLMs as PT, SFT, and DPO, respectively.4 ", "page_idx": 2}, {"type": "table", "img_path": "zWuHSIALBh/tmp/547a2e824c634ec6919426acbf4f5fce2ec29ca866ed6f9e9b3dad735c526199.jpg", "table_caption": [], "table_footnote": ["Figure 2: Instructions from Open Assistant dataset. The instructions are classified with SFT model using the prompt in Appendix Figure 4. "], "page_idx": 3}, {"type": "text", "text": "SFT. We explore two sources of supervision to generate training data (detailed in Appendix A.1): (1) using $\\mathrm{PT}^{\\mathrm{RAG}}$ with few-shot demonstration to generate biographies for each name entity in training data, where $\\mathrm{PT}^{\\mathrm{RAG}}$ is PT augmented with an off-the-shelf retriever [Lin et al., 2023]; (2) using vanilla PT with few-shot demonstration to generate training data as a baseline. As shown in Table 1, $\\mathrm{PT}^{\\mathrm{RAG}}$ is indeed much more factual than PT. However, a surprising discovery in the pilot study is that fine-tuning on such more factual instruction\u2013biography pairs generated by $\\mathrm{PT}^{R A\\bar{G}}$ results in a less factual SFT model (row 4 vs 3). ", "page_idx": 3}, {"type": "text", "text": "DPO. We further fine-tune the LLMs to be more factual through DPO. An intuitive way to create factuality preference pairs is to directly use the samples from $\\mathrm{PT}^{\\mathrm{RAG}}$ and PT as positives and negatives since PTRAG generates more factual biographies than PT (row 2 vs 1). Another approach is to employ FACTSCORE (FS) as the reward to select positive and negative samples among the generations from PT itself [Tian et al., 2024] (detailed in Appendix A.1). As shown in Table 1, DPO fine-tuned on self-generated data with FS reward guides models to generate more factual responses (row 5 vs 3); however, DPO fine-tuned with the supervision of $\\mathrm{PT}^{\\mathrm{RAG}}$ makes the models hallucinate even more than its SFT counterpart (6 vs 4). ", "page_idx": 3}, {"type": "table", "img_path": "zWuHSIALBh/tmp/72c8970d0f98113295c49444f40b1291971fbf6c1e7957b97ed117411069c3d7.jpg", "table_caption": ["Table 1: Pilot study on bio generation. Pos. denotes the positives for SFT or DPO. Neg. denotes the negatives for DPO. FS denotes FACTSCORE. "], "table_footnote": ["FACTSCORE is used to select positives and negatives. "], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "This outcome suggests that compelling models to generate responses akin to $\\mathrm{PT}^{\\mathrm{RAG}}$ prompts increases hallucination. Conversely, fine-tuning LLMs on their own generations appears to be crucial for factual alignment, a finding applicable to both SFT and DPO fine-tuning. ", "page_idx": 3}, {"type": "text", "text": "3.2 Strategies for Factual Alignment ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "From the pilot study, we find that better quality data (in terms of factuality) for SFT and DPO does not necessarily yield models with better factual alignment. This is likely because the supervision from RAG contains information unknown to the LLM; thus, fine-tuning on RAG generated responses may inadvertently encourage the LLM to output unfamiliar information. To avoid unknown knowledge from being presented to the LLM, a viable strategy is to create SFT and DPO training data using the generated responses from the LLM itself. ", "page_idx": 3}, {"type": "text", "text": "4 Factuality-Aware Alignment ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In the section, we further extend our discussion of factual alignment to encompass more general instructions. Unlike biography generation in Section 3, where factuality is the main alignment objective, human instructions are diverse and complex, necessitating a range of alignment skill sets beyond factuality alone; e.g., logical thinking, problem handling and user alignment [Ye et al., 2024]. Thus, conducting factual alignment with the diverse instructions face two main challenges: (1) different instructions may demand distinct skill sets. For example, in Figure 2, instruction 3, \u201cPlease give me a brief history of coffee\u201d, necessitates factual accuracy and concise summarization, while instruction 8, \u201cTell me a story about a pig who goes to the moon\u201d, prioritizes creativity and imagination over strict factuality. (2) As recent studies have emphasized [Ye et al., 2024, Hosking et al., 2024], using a single scalar for reward modeling fails to adequately address multiple alignment skill sets and often under-presents the aspect of factuality. ", "page_idx": 3}, {"type": "image", "img_path": "zWuHSIALBh/tmp/eeeca4a8af29837ff475cd368c27a48cd88975780768f335e71a04e2d09482f9.jpg", "img_caption": ["Figure 3: Illustrations of (a) response generation using a pre-trained LLM (PT) with few-shot demonstration; (b) factuality-aware alignment. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "To tackle the aforementioned challenges, we propose factuality-aware alignment (FLAME ). To address the first challenge, we propose to prompt LLMs to classify whether a given instruction demands the response to be factual, as shown in Figure 2. We then apply the factuality fine-tuning strategy for SFT and DPO discussed in Section 3.2 to those fact-based instructions. Furthermore, to address the second challenge, we employ separate rewards to evaluate the factuality and instructionfollowing capability of an LLM. For simplicity, our work only considers two alignment skill sets: instruction following and factuality. We leave more comprehensive reward modeling to future work. ", "page_idx": 4}, {"type": "text", "text": "In the following, we first describe our baseline alignment approach and introduce our proposed factuality-aware alignment built on top of the baseline alignment procedure. ", "page_idx": 4}, {"type": "text", "text": "4.1 Baseline Alignment ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We initialize PT from Llama-2 70B pre-trained model5 and build our baseline alignment procedure following self-rewarding language models [Yuan et al., 2024] due to its simplicity and independence of other strong LLMs (e.g., GPT4) or human evaluators as a reward model. The alignment comprises two steps: (1) building SFT model fine-tuned on a high-quality seed data consisting of 3,200 instructions and each instruction is paired with the best response created by humans from Open Assistant dataset [OASST; K\u00f6pf et al., 2023]; (2) further fine-tuning SFT through DPO on instructionfollowing preference data $(x,y_{+},y_{-})$ constructed by itself (SFT) as the reward model, $\\mathrm{RM}^{\\mathrm{IF}}$ , where $y_{+}$ and $y_{-}$ are the positive and negative responses for a given prompt $x$ , respectively. The resulting fine-tuned model is denoted as $\\mathrm{SFT}+\\mathrm{DPO}$ . Note that, following Yuan et al. [2024], we use additional augmented 20K instructions to create the preference training data for DPO fine-tuning. Further details are provided in Appendix A.3. ", "page_idx": 4}, {"type": "text", "text": "4.2 Our Approach ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "4.2.1 Factuality-Aware SFT $\\mathrm{(SFT^{\\ast}}$ ) ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Although leveraging human created high-quality seed data is a reasonable choice for SFT [Zhou et al., 2023], our study in Section 3 suggests that fine-tuning on such high-quality data generated by models other than the LLM itself may present unknown information to the LLM, which may in turn encourage hallucination. To address the above issue, for each instruction from the seed data, we elicit the knowledge from the pre-trained LM itself by generating the responses with a few-shot demonstration. Furthermore, to better use the knowledge from both humans and the pre-trained LLM itself, we propose to utilize human generated responses for non-fact-based instructions, while leveraging the responses sampled from pre-trained LLMs for fact-based instructions to mitigate the introduction of unknown knowledge. ", "page_idx": 4}, {"type": "text", "text": "Specifically, we create factuality-aware alignment training data for SFT with two steps. (1) Classifying instructions: we first prompt SFT to judge whether an instruction from the seed data is fact-based $x\\in$ $X^{\\mathrm{fact}}$ ) or not.6 (2) Eliciting knowledge from PT: as illustrated in Figure 3(a), we sample 10 responses from PT with 5-shot demonstration, $(x_{0},\\mathrm{Human}(x_{0}))\\cdot\\cdot\\cdot(x_{4},\\mathrm{Human}(x_{4}))$ , where $x_{k}$ is the top- $k$ similar instruction to $x$ retrieved by DRAGON+ [Lin et al., 2023] from the seed data. ${\\mathrm{Human}}(x_{k})$ denotes the corresponding human response to $x_{k}$ in the seed data. As illustrated in Figure 3(b) (upper), the resulting training data for SFT is $(x\\notin X^{\\mathrm{fact}},\\mathrm{Human}(x)),(x\\in X^{\\mathrm{fact}},\\mathrm{PT}(x))$ , where $\\mathrm{PT}(x)$ denotes the set of responses to $x$ sampled from PT. The fine-tuned model is denoted as $\\mathrm{SFT^{6}}$ . ", "page_idx": 5}, {"type": "text", "text": "4.2.2 Factuality-Aware DPO $(\\mathrm{DPO^{\\circ}}$ ) ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "At the second stage of alignment with DPO, we use $\\mathrm{SFT^{6}}$ to generate multiple responses $y_{0},y_{1},\\cdot\\cdot\\cdot$ for a given instruction $x$ ; then, using $\\mathrm{SFT^{6}}$ itself as the reward model $(\\mathrm{RM}^{\\mathrm{IF}})$ to create a preference pair: $(x,y_{+},y_{-})$ .7 The above data creation procedure is the same as the second stage of our baseline alignment in Section 4.1. However, recent studies [Saha et al., 2024, Hosking et al., 2024, Ye et al., 2024] indicate that a single scalar reward from human feedback or LLM reward models may underrepresent the aspect of factuality. To address this limitation, we introduce another factuality reward model $(\\mathrm{RM}^{\\mathrm{fact}})$ to evaluate factuality of responses and create a factuality preference pair for fact-based instructions: $(x\\in X^{\\mathrm{fact}},y_{\\mathrm{tue}},y_{\\mathrm{false}})$ . ", "page_idx": 5}, {"type": "text", "text": "Specifically, we build $\\mathrm{RM}^{\\mathrm{fact}}$ with retrieval augmentation to measure the percentage of facts in a response that are correct. $\\mathrm{RM}^{\\mathrm{fact}}$ comprises two main components: atomic fact decomposition and retrieval augmented claim verification. We detail the components and ablate their impacts on the quality of $\\mathrm{RM}^{\\mathrm{fact}}$ in Appendix A.5. We compute factuality reward for the same responses sampled from $\\mathrm{SFT}^{\\delta}\\colon\\mathrm{RM}^{\\mathrm{fact}}(x,y_{0}),\\mathrm{RM}^{\\mathrm{fact}}(x,y_{1}),\\cdot\\cdot\\cdot$ . The response with the highest (lowest) factuality reward is chosen as $y_{\\mathrm{true}}~(y_{\\mathrm{false}})$ . Note that if the chosen paired responses show large difference in instruction-following reward, we discard the pair; i.e., $|\\bar{\\mathrm{RM}}^{\\mathrm{IF}}(x,\\bar{y_{\\mathrm{tue}}})-\\mathrm{RM}^{\\mathrm{IF}}(x,\\bar{y_{\\mathrm{false}}})|>0.5.$ . As illustrated in Figure 3(b) (lower), in factuality-aware DPO training, the model is initialized from $\\mathrm{SFT^{6}}$ and the fine-tuned model is our final factuality-aware aligned model, denoted $\\mathrm{SFT}^{\\ast}+\\mathrm{DPO}^{\\ast}$ . The specific procedures for fine-tuning models in both the SFT and DPO are described in Appendix A.6. ", "page_idx": 5}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "5.1 Evaluation Datasets and Metrics ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Instruction Following. We use the 805 instruction-following tasks from Alpaca Eval [Dubois et al., 2024] to evaluate models head-to-head win rate against our baselines using the recommended evaluator: alpaca_eval_gpt4_turbo_fn. We use SFT and $\\mathrm{SFT}+\\mathrm{DP}\\bar{\\mathrm{O}}$ described in Section 4.1 as the baselines for win rate comparisons. ", "page_idx": 5}, {"type": "text", "text": "Factuality. We evaluate models on three datasets with diverse knowledge-intensive instructions for factuality. (1) Biography: a knowledge insensitive sub-task of instruction-following tasks. Following our pilot study in Section 3, we use the 183 human entities provided by Min et al. [2023] with the prompt \u201cTell me a bio of entity name\u201d. (2) Alpaca Fact: we extract the fact-based instructions from the 803 instructions using our SFT model (with the prompt shown in Appendix Figure 4), resulting in 241 instructions. (3) FAVA [Mishra et al., $2024]^{8}$ : the 141 knowledge-intensive instructions from multiple sources, including Open Assistant [K\u00f6pf et al., 2023], No Robots [Rajani et al., 2023], WebNLG [Gardent et al., 2017] and manually created datasets. We report FACTSCORE (FS) without length penalty as the metric for all the three datasets. Note that original FS computes proportion of correct facts with additional penalty on short generations with less than 10 atomic facts. This penalty aims to address situations where models provide insufficiently detailed answers. We assume that this aspect is considered in the evaluation of instruction following in Alpaca Eval. In addition, we also ", "page_idx": 5}, {"type": "text", "text": "Table 2: Experimental results of supervised fine-tuning on Open Assistant dataset. PT denotes pre-trained Llama2 70B with 5-shot demonstration. $\\mathrm{SFT}^{\\mathrm{fact}}$ denotes the variant which only optimizes factuality. FS denotes FACTSCORE. ", "page_idx": 6}, {"type": "table", "img_path": "zWuHSIALBh/tmp/1b96166b3510486149f705465bdc4453214f769983b06f214b36d3a5e8400235.jpg", "table_caption": [], "table_footnote": ["\u2217 $\\mathrm{SFT^{6}}$ uses supervision from Human and PT for non-fact-based and fact-based instructions, respectively. "], "page_idx": 6}, {"type": "text", "text": "Table 3: Experiments of direct preference optimization (DPO). IF. and Fact. denote instruction following $(x,y_{+},y_{-})$ and factuality $(x\\in X^{\\mathrm{fact}},y_{\\mathrm{true}},y_{\\mathrm{false}})$ preference data, where $X^{\\mathrm{fact}}$ denotes the set of fact-based instructions. $\\mathrm{DPO}^{\\mathrm{fact}}$ denotes the variant which only optimizes factuality. The preference data statistics is listed in Appendix, Table 11. ", "page_idx": 6}, {"type": "table", "img_path": "zWuHSIALBh/tmp/cede2b78403657c42b8e8922745e39a2f89feb0a6d9089b92fae363814650532.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "report the number of correct and erroneous facts. All the numbers reported are averaged over the instructions in each dataset. ", "page_idx": 6}, {"type": "text", "text": "In addition, we also evaluate our fine-tuned models\u2019 truthfulness using TruthfulQA [Lin et al., 2022]. We evaluate model performance in the generation task and use ROUGE [Lin, 2004] and BLEU [Papineni et al., 2002] to measure the quality of responses. ", "page_idx": 6}, {"type": "text", "text": "5.2 Comparisons of SFT ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Table 2 compares the pre-trained Llama-2 70B fine-tuned on OASST dataset with responses from different sources. We list the FACTSCORE (FS) of biography generation using the pre-trained model through Bio 5-shot demonstration as reference (row 0) and SFT, which is fine-tuned on our seed data with human-created responses, is our baseline (row 1). We first notice that SFT shows significant FACTSCORE degradation (53.1 vs 44.7) compared to Bio 5-shot with the pre-trained model. It seems that SFT tends to generate more lengthy responses but with more erroneous facts. ", "page_idx": 6}, {"type": "text", "text": "When eliciting the knowledge from PT by fine-tuning on its own generated responses, SFTfact generates more factual responses in Biography and Alpaca (row 2 vs 1). However, it shows slightly inferior instruction-following capability in Alpaca Eval. This result demonstrates that human responses indeed teach LLMs how to better follow instructions but also encourage LLMs to output more false facts. On the other hand, eliciting the knowledge from the pre-trained model itself avoids the encouragement of hallucination albeit with a slight reduction in instruction-following capability. Finally, SFT combining supervision from humans and PT, shows comparable instruction-following capability and output more factual responses on fact-based instructions (row 3 vs 1). ", "page_idx": 6}, {"type": "text", "text": "5.3 Comparisons of DPO ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Table 3 compares different DPO training recipes. First, we conduct DPO fine-tuning on our SFT baseline, SFT. When further aligning the model to follow instructions, DPO sees a significant improvement in instruction-following capability (row 2 vs 1) with win rate 72.9 over SFT; however, the instruction aligned model tends to output lengthy responses with more factual errors (see examples in Appendix Figure 10). On the other hand, when only aligned with factual preference data, DPOfact shows less improvement in instruction-following capability (row 1 vs 3). These results indicate that preference optimization for either instruction following or factuality alone may come at the expense of the other since the former encourages models to output long and detailed responses while the later discourages models to output false claims. When jointly conducting instruction and factuality alignment, $\\mathrm{DPO^{6}}$ not only better follows instructions but also outputs more factual responses (row 4 vs 1, 2). Finally, initializing from $\\mathrm{SFT^{6}}$ , the DPO fine-tuned models are more factual than their counterparts (i.e., 6 vs 2 and 7 vs 4) without instruction-following capability degrade. We also list the results from Llama-2-Chat 70B (row 0) and observe that despite of its strong instruction-following capability, it tends to output many more incorrect facts. These results demonstrate that standard alignment, even on proprietary commercial data, may encourage LLMs to hallucinate. In contrast, our factuality-aware alignment guides LLMs to output more factual responses without degradation in their general instruction-following capabilities. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "It is worth noting that $\\mathrm{SFT}^{\\mathrm{fact}}$ and DPOfact are similar to SFT and DPO fine-tuning proposed by Tian et al. [2024], which improve LLMs\u2019 factuality but degrade their instruction-following capability. Also, we do not observe our SFT and DPO variants outperform the pre-trained model with few-shot demonstrations on biography generation (row 0 in Table 2. This is possibly due to the alignment tax found in previous work [Ouyang et al., 2022], which degrades LLMs\u2019 accuracy on the standard knowledge benchmarks. How to improve both models\u2019 instruction-following capability and their accuracy on standard knowledge benchmarks is worth exploring, which we leave for future work. ", "page_idx": 7}, {"type": "text", "text": "5.4 Results on TruthfulQA ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Table 4 compares models performance on TruthfulQA. Generally, we observe that our factualityaware alignment training guides LLMs to output more truthful responses. For example, factualityaware SFT improves LLMs\u2019 truthfulness (row 5 vs 1). In addition, DPO fine-tuning on the factuality preference data guides LLMs to output more truthful responses (rows 3,4 vs 2 and 7 vs 6). Note that we observe that SFT and DPO models show a reverse trend in BLUE and ROUGE. This is likely because SFT models tend to generate shorter responses than the DPO ones do. ", "page_idx": 7}, {"type": "text", "text": "In addition, Table 5 reports models\u2019 accuracy in tasks of multiple choices from TruthfulQA. No significant differences between models are observed. This is possibly because we mainly focus on the tasks of long-form response generation while TruthfulQA-MC task is formed by short-form answers. The discrepancy between improving LLMs\u2019 factuality on long-form and short-form generation is also found by the previous work [Chuang et al., 2024]. Appendix Table 9 reports more evaluation results on other NLP benchmarks. ", "page_idx": 7}, {"type": "table", "img_path": "zWuHSIALBh/tmp/06206f3c5b2c8e216426910b63685e85d4963e1987d53a399e9940a613b8c9ef.jpg", "table_caption": ["Table 4: Results on TruthfulQA. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "zWuHSIALBh/tmp/f32ebea443f401f5750d84a88fe3b85b3949b68116fa48d71dcb114a2b2b3f4f.jpg", "table_caption": ["Table 5: Results on TruthfulQA multiple choices. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "6 Discussion ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "6.1 Effects of Fact-Based Instruction Classification ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In our factuality-aware alignment, we prompt SFT to judge whether an instruction requires a factual response and apply our factuality alignment strategy to the fact-based instruction. Without the instruction classification, in our factuality-aware SFT, we cannot create supervision from Human and PT responses for respective non-fact-based and fact-based instructions. Instead, for each instruction, we create instruction\u2013response pairs from 1 and 10 responses from Human and PT as supervisions, respectively. Note that, during fine-tuning, for each instruction, we randomly sample instruction\u2013 response pair either created from Human or PT with same probability. The SFT model shows degradation in both instruction-following capability and factuality results, as shown in row 1 vs 2 of Table 6. Second, for factuality-aware DPO, without the instruction classification, we create factuality preference pairs from all instructions instead of fact-based instructions. The DPO fine-tuned model outputs slightly more factual responses but sacrifice instruction-following capability, as shown in row 3 vs 4 of Table 6. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "6.2 Effects of Fact-Based Sentence Classification ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In addition, we observe that not all the sentences in a response to a fact-based instruction require fact check. For example, given the response, \u201cOf course. The Commodore 64 is a 8-bit home computer that was released by Commodore International in August 1982.\u201d, conducting fact check for the first sentence \u201cOf course.\u201d is not necessary and may make the factuality reward less accurate. To address this issue, we prompt SFT to judge whether each sentence in a response required fact check using the prompt in Appendix Figure 6. We only conduct fact check and compute factu", "page_idx": 8}, {"type": "table", "img_path": "zWuHSIALBh/tmp/e2e9071affab3188ce7bc2fe98d0651e6d04b294f1ecc134feab320680e56255.jpg", "table_caption": ["Table 6: Effects of fact-based classification. "], "table_footnote": ["\u2217comparing with SFT baseline, SFT. \u25b3comparing with DPO baseline, $\\mathrm{SFT}+\\mathrm{DPO}$ . "], "page_idx": 8}, {"type": "text", "text": "ality rewards for those fact-based sentences. However, as shown in Table 6, computing factuality rewards for fact-based sentences makes our factual alignment less effective (row 5 vs 4). This is likely because the fact-based sentence classifier is not accurate enough and brings noise into our factuality reward model (see examples in Appendix Figure 7). ", "page_idx": 8}, {"type": "text", "text": "6.3 Ablations on Factuality Preference Data Creation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we examine different ways of creating factuality preference data for factualityaware DPO training. First, for each fact-based instruction, instead of choosing the responses (among the 4 generated responses) with the maximum and minimum factuality rewards $(\\mathrm{RM}^{\\mathrm{fact}})$ as the respective positive and negative samples, we enumerate all the possible response pairs and choose the response with higher (lower) RMfact as the positive (negative) sample from each enu", "page_idx": 8}, {"type": "table", "img_path": "zWuHSIALBh/tmp/ea4b515aa79ddb1a8d6b08f08458370ccf96bb35c25e1cd13f7e6ddb3719db30.jpg", "table_caption": ["Table 7: Ablation on factuality preference data. "], "table_footnote": ["\u25b3comparing with DPO baseline, SFT $^+$ DPO. "], "page_idx": 8}, {"type": "text", "text": "merated pair. If the difference of $\\mathrm{RM}^{\\mathrm{fact}}$ is smaller than 0.2, we treat them as equal and discard the pairs. Note that for both row 1 and 2 in Table 7, we also discard the pairs with the difference of instruction-following rewards $(\\mathrm{RM}^{\\mathrm{IF}})$ larger than 0.5 (as mentioned in Section 4.2.2). Alternatively, for each response, we linearly combine the rewards, $\\mathrm{RM}^{\\mathrm{IF}}$ (1\u20135 scale) and $\\mathrm{RM}^{\\mathrm{fact}}$ (0\u20131 scale), with the respective weight of 1 and 5 as a composite reward. For each instruction, we choose the responses with the maximum and minimum composite rewards as the positive and negative. As a result, both data creation approaches increase the number of factuality preference pairs; however, they yield trivial improvement in factuality but slight degrade in instruction following (rows 2, 3 vs 1). This result also indicates that leveraging a single reward model, which can be incorporated with PPO [Schulman et al., 2017], is possible to improve both models\u2019 instruction-following capability and factuality ", "page_idx": 8}, {"type": "text", "text": "6.4 Impacts of DPO on Generation Length ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Table 8 lists the average length of models\u2019 responses for each dataset. We observe that DPO fine-tuned models tend to output lengthy responses than SFT except for $\\mathrm{DPO}^{\\mathrm{fact}}$ on Biography. This trend indicates that our instructionfollowing reward model $\\mathrm{RM}^{\\mathrm{IF}}$ guides LLMs to output more detailed and lengthy responses. In addition, we observe that although $\\mathrm{DPO^{\\circ}}$ out", "page_idx": 8}, {"type": "table", "img_path": "zWuHSIALBh/tmp/310065dff2a85d66de7599e3237516220fcae75fe11ed4ca719cadc75b2600e8.jpg", "table_caption": ["Table 8: Effects of DPO on response length. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "puts responses with similar length as DPO on Alpaca Eval, $\\mathrm{DPO^{\\ast}}$ generates a slightly shorter responses for the fact-based instructions in the other three datasets. This results show that our factuality-aware DPO training mainly impacts models\u2019 responses for fact-based instructions. The impact is mainly to reduce the false claims, evidenced by the numbers of erroneous facts in rows 2 and 4 of Table 3). ", "page_idx": 9}, {"type": "text", "text": "6.5 Case Studies ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Figure 10 (in Appendix) showcases the generations of different models, SFT, $\\mathrm{SFT}+\\mathrm{DPO}$ and $\\mathrm{SFT^{*}+D P O^{*}}$ , on Alpaca Eval and Biography. Given the instruction, \u201cWhat are the names of some famous actors that started their careers on Broadway?\u201d, SFT only lists some names of Broadway actors while DPO fine-tuned models generate detailed information for each listed Broadway actor. As for biography generations, we observe that given the instruction to generate a biography for a rare name entity, Marianne McAndrew, $\\mathrm{SFT}+\\mathrm{DPO}$ generates a detailed response but with many wrong facts while SFT and $\\mathrm{SFT^{*}+D P O^{*}}$ give relatively short responses. For the frequent entity, Ji Sung, all the models generate detailed and mostly correct responses. This qualitative analysis shows that $\\mathrm{SFT^{\\ast}+D P O^{\\ast}}$ tends to generate detailed responses for most instructions, but for those instructions required tailed knowledge (e.g., rare entity) likely unknown to LLMs [Mallen et al., 2023], it reduces erroneous facts by giving less detailed responses, which is also observed by Kang et al. [2024]. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion and Future Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we present a study to enhance the factuality of large language models (LLMs). We first identify that the standard alignment approach, comprising SFT and RLAIF with DPO, may inadvertently encourage LLMs to produce more erroneous facts. Specifically, during the SFT stage, fine-tuning LLMs with high-quality human responses may introduce unfamiliar information, prompting LLMs to output unknown facts. Additionally, during the DPO stage, enhancing LLMs\u2019 ability to follow instructions may result in more detailed and lengthy responses but often leads to increased hallucination. To tackle the shortcomings of the standard alignment, we propose a factuality-aware alignment method, which includes factuality-aware SFT and DPO. Quantitative and qualitative analyses demonstrate that our factuality-aware alignment not only guides LLMs to generate detailed and helpful responses but also helps prevent the generation of false claims. ", "page_idx": 9}, {"type": "text", "text": "While we have successfully integrated factuality into standard alignment procedure, our work only considers two alignment skill sets: instruction following (or helpfulness) and factuality. In practice, each instruction may require consideration of multiple and distinct alignment skill sets [Saha et al., 2024]. The method to optimize for these skill sets tailored to each query requires further study. In our experiments, we note that optimizing preferences solely for instruction following or factuality could potentially compromise the other. While our factuality-aware alignment demonstrated improvements in both aspects, it is uncertain whether there is a trade-off between the two aspects when integrating our approach to large-scale alignment [Touvron et al., 2023]. Finally, as shown in Appendix Figure 7, not all the claims (or sentences) in a response require fact verification, a more accurate factuality reward model should take this factor into account. While our preliminary experiment, which removes non-fact-based sentences from the factuality reward modeling (Section 6.2), shows suboptimal performance, we believe that further study can bring more insights. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We thank Bhargavi Paranjape for sharing fine-tuned Llama-2 7B for atomic fact decomposition and Jing Xu, Weizhe Yuan and Jason Weston for their helpful suggestions. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. ", "page_idx": 9}, {"type": "text", "text": "Training a helpful and harmless assistant with reinforcement learning from human feedback.   \narXiv:2204.05862, 2022.   \nYuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan. Constitutional AI: Harmlessness from AI feedback. arXiv:2212.08073, 2023.   \nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Proc. NeurIPS, pages 1877\u20131901, 2020.   \nJifan Chen, Aniruddh Sriram, Eunsol Choi, and Greg Durrett. Generating literal and implied subquestions to fact-check complex claims. In Proc. EMNLP, pages 3495\u20133516, 2022.   \nLichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, and Hongxia Jin. AlpaGasus: Training a better Alpaca model with fewer data. In Proc. ICLR, 2024a.   \nLichang Chen, Chen Zhu, Davit Soselia, Jiuhai Chen, Tianyi Zhou, Tom Goldstein, Heng Huang, Mohammad Shoeybi, and Bryan Catanzaro. ODIN: Disentangled reward mitigates hacking in RLHF. arXiv:2402.07319, 2024b.   \nSilei Cheng, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng Wang, Jordan Boyd-Graber, and Lijuan Wang. Prompting GPT-3 to be reliable. In Proc. ICLR, 2023.   \nI-Chun Chern, Steffi Chern, Shiqi Chen, Weizhe Yuan, Kehua Feng, Chunting Zhou, Junxian He, Graham Neubig, Pengfei Liu, et al. FacTool: Factuality detection in generative AI\u2013a tool augmented framework for multi-task and multi-domain scenarios. arXiv:2307.13528, 2023.   \nYung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James R. Glass, and Pengcheng He. DoLa: Decoding by contrasting layers improves factuality in large language models. In Proc. ICLR, 2024.   \nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems, 2021.   \nYann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. AlpacaFarm: A simulation framework for methods that learn from human feedback. arXiv:2305.14387, 2024.   \nLuyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent Zhao, N. Lao, Hongrae Lee, Da-Cheng Juan, and Kelvin Guu. RARR: Researching and revising what language models say, using language models. In Proc. ACL, 2022.   \nClaire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini. The WebNLG challenge: Generating text from RDF data. In Proc. INLG, pages 124\u2013133, 2017.   \nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In Proc. ICLR, 2021.   \nTom Hosking, Phil Blunsom, and Max Bartolo. Human feedback is not gold standard. In Proc. ICLR, 2024.   \nGautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Atlas: Few-shot learning with retrieval augmented language models. Journal of Machine Learning Research, pages 1\u201343, 2023.   \nZhengbao Jiang, Frank Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig. Active retrieval augmented generation. In Proc. EMNLP, pages 7969\u20137992, 2023.   \nNikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. Large language models struggle to learn long-tail knowledge. In Proc ICML, 2023.   \nKatie Kang, Eric Wallace, Claire Tomlin, Aviral Kumar, and Sergey Levine. Unfamiliar finetuning examples control how language models hallucinate. arXiv:2403.05612, 2024.   \nAndreas K\u00f6pf, Yannic Kilcher, Dimitri von R\u00fctte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens, Abdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Rich\u00e1rd Nagyf,i Shahul ES, Sameer Suri, David Glushkov, Arnav Dantuluri, Andrew Maguire, Christoph Schuhmann, Huu Nguyen, and Alexander Mattick. OpenAssistant Conversations \u2013 democratizing large language model alignment. arXiv:2304.07327, 2023.   \nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, Sebastian Riedel, and Douwe Kiela. Retrieval-augmented generation for knowledge-intensive NLP tasks. In Proc. NeurIPS, pages 9459\u20139474, 2020.   \nKenneth Li, Oam Patel, Fernanda Vi\u00e9gas, Hanspeter Pfister, and Martin Wattenberg. Inference-time intervention: Eliciting truthful answers from a language model. In Proc. NeurIPS, 2023.   \nXian Li, Ping Yu, Chunting Zhou, Timo Schick, Omer Levy, Luke Zettlemoyer, Jason E Weston, and Mike Lewis. Self-alignment with instruction backtranslation. In Proc. ICLR, 2024.   \nChin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74\u201381, July 2004.   \nSheng-Chieh Lin, Akari Asai, Minghan Li, Barlas Oguz, Jimmy Lin, Yashar Mehdad, Wen-tau Yih, and Xilun Chen. How to train your DRAGON: Diverse augmentation towards generalizable dense retrieval. In Proc. Findings of EMNLP, pages 6385\u20136400, 2023.   \nStephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring how models mimic human falsehoods. In Proc. ACL, pages 3214\u20133252, 2022.   \nYixin Liu, Alex Fabbri, Pengfei Liu, Yilun Zhao, Linyong Nan, Ruilin Han, Simeng Han, Shafiq Joty, Chien-Sheng Wu, Caiming Xiong, and Dragomir Radev. Revisiting the gold standard: Grounding summarization evaluation with robust human evaluation. In Proc. ACL, pages 4140\u20134170, 2023.   \nChaitanya Malaviya, Subin Lee, Sihao Chen, Elizabeth Sieber, Mark Yatskar, and Dan Roth. ExpertQA: Expert-curated questions and attributed answers. arXiv:2309.07852, 2023.   \nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. When not to trust language models: Investigating effectiveness of parametric and non-parametric memories. In Proc. ACL, pages 9802\u20139822, 2023.   \nGr\u00e9goire Mialon, Roberto Dessi, Maria Lomeli, Christoforos Nalmpantis, Ramakanth Pasunuru, Roberta Raileanu, Baptiste Roziere, Timo Schick, Jane Dwivedi-Yu, Asli Celikyilmaz, Edouard Grave, Yann LeCun, and Thomas Scialom. Augmented language models: a survey. Transactions on Machine Learning Research, 2023.   \nSewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. FActScore: Fine-grained atomic evaluation of factual precision in long form text generation. In Proc. EMNLP, pages 12076\u201312100, 2023.   \nAbhika Mishra, Akari Asai, Vidhisha Balachandran, Yizhong Wang, Graham Neubig, Yulia Tsvetkov, and Hannaneh Hajishirzi. Fine-grained hallucination detection and editing for language models. arXiv:2401.06855, 2024. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In Proc. NeurIPS, pages 27730\u201327744, 2022. ", "page_idx": 12}, {"type": "text", "text": "Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. BLEU: a method for automatic evaluation of machine translation. In Proc. ACL, pages 311\u2013318, 2002. ", "page_idx": 12}, {"type": "text", "text": "Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. In Proc. NeurIPS, pages 53728\u201353741, 2023. ", "page_idx": 12}, {"type": "text", "text": "Nazneen Rajani, Lewis Tunstall, Edward Beeching, Nathan Lambert, Alexander M. Rush, and Thomas Wolf. No robots. Hugging Face repository, 2023. ", "page_idx": 12}, {"type": "text", "text": "Swarnadeep Saha, Omer Levy, Asli Celikyilmaz, Mohit Bansal, Jason Weston, and Xian Li. BranchSolve-Merge improves large language model evaluation and generation. In Proc. NAACL, pages 8352\u20138370, 2024. ", "page_idx": 12}, {"type": "text", "text": "Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M Rush. Multitask prompted training enables zero-shot task generalization. In Proc. ICLR, 2022. ", "page_idx": 12}, {"type": "text", "text": "John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv:1707.06347, 2017. ", "page_idx": 12}, {"type": "text", "text": "Prasann Singhal, Tanya Goyal, Jiacheng Xu, and Greg Durrett. A long way to go: Investigating length correlations in RLHF. arXiv:2310.03716, 2023. ", "page_idx": 12}, {"type": "text", "text": "Zhiqing Sun, Yikang Shen, Hongxin Zhang, Qinhong Zhou, Zhenfang Chen, David Daniel Cox, Yiming Yang, and Chuang Gan. SALMON: Self-alignment with principle-following reward models. In Proc. ICLR, 2024. ", "page_idx": 12}, {"type": "text", "text": "Katherine Tian, Eric Mitchell, Huaxiu Yao, Christopher D Manning, and Chelsea Finn. Fine-tuning language models for factuality. In Proc. ICLR, 2024. ", "page_idx": 12}, {"type": "text", "text": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. arXiv:2307.09288, 2023. ", "page_idx": 12}, {"type": "text", "text": "Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, ", "page_idx": 12}, {"type": "text", "text": "Rushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta Patro, Tanay Dixit, and Xudong Shen. Super-NaturalInstructions: Generalization via declarative instructions on $1600+$ NLP tasks. In Proc. EMNLP, pages 5085\u20135109, 2022.   \nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-Instruct: Aligning language models with self-generated instructions. In Proc. ACL, pages 13484\u201313508, 2023a.   \nYuxia Wang, Revanth Gangi Reddy, Zain Muhammad Mujahid, Arnav Arora, Aleksandr Rubashevskii, Jiahui Geng, Osama Mohammed Afzal, Liangming Pan, Nadav Borenstein, Aditya Pillai, Isabelle Augenstein, Iryna Gurevych, and Preslav Nakov. Factcheck-GPT: End-to-end fine-grained document-level fact-checking and correction of LLM output. arXiv:2311.09000, 2023b.   \nSeonghyeon Ye, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, Seungone Kim, Yongrae Jo, James Thorne, Juho Kim, and Minjoon Seo. FLASK: Fine-grained language model evaluation based on alignment skill sets. In Proc. ICLR, 2024.   \nWeizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Xian Li, Sainbayar Sukhbaatar, Jing Xu, and Jason E Weston. Self-Rewarding language models. In Proc. ICML, pages 57905\u201357923, 2024.   \nChunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. LIMA: Less is more for alignment. In Proc. NeurIPS, pages 55006\u201355021, 2023. ", "page_idx": 13}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Biography Data Generation ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Entities for Training and Evaluation. We use 500 diverse human entities to create training data for SFT and DPO; then, evaluate LLMs\u2019 generation factuality on another 183 human entities from Min et al. [2023].9 Note that the human entities for training and evaluation are uniformly sampled from entities across diverse nationalities, professions, and rarities. The instruction is generated with the format: Tell me a bio of entity name. ", "page_idx": 14}, {"type": "text", "text": "Creating Training Data for SFT. We randomly sample 5 human entities among the 500 entities for training and generate their biographies using Llama-2-Chat 70B as 5-shot demonstration.10 With the 5-shot demonstration, we use pre-trained Llama-2 7B to generate 10 biographies for each human entity from the remaining 495 ones.11 We set temperature 0.7 and top- $\\mathrm{~p~}0.9$ when generate multiple responses from LLMs in all our experiments. We use the created 4,950 name entity\u2013biography pairs to fine-tune the pre-trained Llama-2 7B. As for generating training data with RAG, we prepend the top-10 passages from our retrieval system (detailed in Appendix A.2) to each instruction and generate 10 biographies for each entity from RAG with 5-shot demonstrations. Note that we only prepend top-1 passage for each instruction in the demonstration. ", "page_idx": 14}, {"type": "text", "text": "Creating Factuality Preference Pairs for DPO. To construct factuality preference pairs, we first compute FACTSCORE (FS) for all the 4,950 biographies previously created by PT. Then, for each name entity, we compare the FS for all the possible 45 pairs from the 10 generated biographies and construct DPO pairs using the biography with a higher (lower) FS as a positive (negative). Note that we discard the pairs if they show tied FS. ", "page_idx": 14}, {"type": "text", "text": "A.2 Retrieval Models ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "For each query, we retrieve top-20 candidate passages from Wikipedia using DRAGON $^+$ [Lin et al., 2023] and re-rank the candidates using a 12-layer cross-encoder12. We use the Wikipedia version from the Dec. 20, 2021 dump released by Izacard et al. [2023] in this work. ", "page_idx": 14}, {"type": "text", "text": "A.3 Alignment with Self Rewarding ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "SFT. At SFT stage, we fine-tune PT on two seed datasets: (1) Instruction-following training (IFT) data from Li et al. [2024], consisting of 3200 instruction\u2013response pairs created by humans from Open Assistant dataset [OASST; K\u00f6pf et al., 2023], where we only use the first conversational turns in the English that are annotated rank 0;13 (2) evaluation following training (EFT) data from Yuan et al. [2024], the LLM-as-a-Judge data consists of 1630 samples, each of which contains instruction, human response and the corresponding score of 1-5 scale (with chain-of-though evaluation reasoning): $(x,y,r)$ , where $(x,y)$ pairs are also selected from OASST other than training pairs and $r$ is created by the model fine-tuned only on IFT with manual filtering. The purpose of EFT is to enhance a LLM\u2019s capability as a reward model to judge the quality of a response in terms of relevance, coverage, usefulness, clarity and expertise. We refer readers to Yuan et al. [2024] for how EFT is created and flitered with minimum human efforts. The prompt template for LLM-as-a-Judge in EFT and an EFT training sample are shown in Appendix, Figure 8 and 9. We refer the baseline model fine-tuned on the IFT and EFT datasets as SFT. ", "page_idx": 14}, {"type": "text", "text": "DPO for Instruction Following. At the subsequent preference learning with DPO, following Wang et al. [2023a], we augment additional 20K instructions with Llama-2 70B chat model.14 For each augmented instruction $x$ , we use SFT to generate 4 responses and evaluate how well the responses follow the instruction with score of 1\u20135 scale: $\\mathrm{RM}^{\\mathrm{IF}}({\\hat{x}},y_{0})\\cdot\\cdot\\cdot\\cdot;\\mathrm{RM}^{\\mathrm{IF}}(x,y_{3})$ , where $y_{0},\\cdot\\cdot\\cdot\\,,y_{3}\\;\\in\\;\\mathrm{SFT}(x)$ and $\\mathrm{RM}^{\\mathrm{IF}}$ is the instruction-following reward model. Note that, in selfrewarding [Yuan et al., 2024], $\\mathrm{RM}^{\\mathrm{IF}}$ is the same as SFT model. In addition, for each instruction\u2013 response pair, we use the same prompt in EFT seed data to sample the chain-of-thought evaluation three times and average the scores as the reward. Finally, for each instruction, we use the response with the highest (lowest) reward as the positive (negative) sample to form a preference pair for DPO training: $(x,y_{+},y_{-})$ . We discard the pair, if $\\mathrm{RM}^{\\mathrm{IF}}\\bar{(x,y_{+})}=\\dot{\\mathrm{RM}}^{\\mathrm{IF}}(x,y_{-})$ . In the DPO training, the model is initialized from SFT and the fine-tuned model is denoted $\\mathrm{\\ddot{\\sFT}+D P O}$ . ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "A.4 More Evaluation Results on Standard Benchmarks ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Table 9 compares the instruction fine-tuned models\u2019 (w/o and w/ involving FLAME) accuracy the in tasks of MMLU [Hendrycks et al., 2021] and GSM8K [Cobbe et al., 2021]. A slight drop from FLAME is observed. This is possibly because we mainly focus on the tasks of long-form response generation while MMLU and GSM8K are the ", "page_idx": 15}, {"type": "table", "img_path": "zWuHSIALBh/tmp/8a88fcf570028375ad46e596bd38882ab6d83b44843a77e3b86714406a22b11e.jpg", "table_caption": ["Table 9: Results on MMLU and GSM8K. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "benchmarks with short-form answers. The discrepancy between improving LLMs\u2019 factuality on long-form and short-form generation is also found by the previous work [Chuang et al., 2024]. ", "page_idx": 15}, {"type": "text", "text": "A.5 Factuality Reward Modeling ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Factuality Reward Models. We build a reward model $\\mathrm{RM}^{\\mathrm{fact}}$ to measure the factuality of each response. The factuality reward model consists of two main modules. (1) fact decomposition: we first use nltk.tokenize to split a response into sentences; then, use our Llama-2 7B model fine-tuned on public datasets [Liu et al., 2023, Chen et al., 2022, Malaviya et al., 2023] to conduct atomic fact decomposition for each sentence.15 (2) Retrieval augmented claim verification: for each decomposed fact (or claim), we use the instruct Llama 7B fine-tuned on Super Natural Instructions [Wang et al., 2022] to do fact check with the prompt shown in Figure 5.16 We append 10 retrieved supports (using the instruction as query) from our retrieval and re-ranking pipeline in Appendix A.2. Then, we compute the proportion of correct atomic facts in a response as a factuality reward. ", "page_idx": 15}, {"type": "text", "text": "Quality of Factuality Reward Models. We conduct ablation study on our factuality reward models. Specifically, we use our factuality reward models to detect the number of error facts in each instruction\u2013response pair. We try different models for fact check using the prompt shown in Figure 5 with different numbers of retrieved supports. We use the LLMs\u2019 generated responses with human annotated hallucination provided by Mishra et al. [2024] to evaluate the quality of the factuality reward models.17 Specifically, we rank the responses by numbers of errors detected and calculate the Kendall rank correlation $(\\tau)$ ", "page_idx": 15}, {"type": "table", "img_path": "zWuHSIALBh/tmp/82d4510ebfa02417059244f6cce4690bfe5f444457c37381420f8d18102d7db6.jpg", "table_caption": ["Table 10: A comparison of factuality reward models. $\\tau$ denotes the correlation between human annotation. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "between the rank lists by our factuality reward models and humans. As shown in Table 10, conducing fact check with more retrieved supports improves the accuracy of the factuality reward models (row 2 vs 1). In addition, our SFT, only fine-tuned on the IFT and EFT data, is capable of doing fact check, compared to Instruct Llama 7B fine-tuned on Super Natural Instructions [Wang et al., 2022]. Finally, instead of computing the number of error facts from decomposed atomic facts, we conduct fact check directly for each sentence in a response and calculate the number of false sentences as error facts. However, the quality of the reward models shows significant decrease (rows 5,6 vs 1,2). We finally adopt row 2 as our factuality reward model. ", "page_idx": 15}, {"type": "text", "text": "Table 11: Training data statistics for different variants. IF. and Fact. denote instruction following $(x,y_{+},y_{-})$ and factuality $(x\\ \\in\\ X^{\\mathrm{fact}},y_{\\mathrm{true}},y_{\\mathrm{false}})$ preference data, where $X^{\\mathrm{fact}}$ denotes the set of fact-based instructions. ", "page_idx": 16}, {"type": "table", "img_path": "zWuHSIALBh/tmp/bff60a08edbe0e2acff74a9526024f15159c4f428184d4b54cb2f43b886f90eb.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "A.6 Training Details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We fine-tune our models for 500 steps with a batch size of 32 and 64 on respective SFT and DPO stages. The learning rate and maximum sequence length is set to $1e-6$ (which decays to $1e-7)$ and 2048, respectively. At SFT stage, we mix the IFT and EFT while at DPO stage, we set $\\beta=0.1$ and uniformly sample between self rewarding $(x,y_{+},y_{-})$ and factuality reward $(x,y_{\\mathrm{true}},y_{\\mathrm{false}})$ preference data. Note that SFT $(\\mathrm{SFT}^{\\ast})+\\mathrm{DPO}$ meaning that we use SFT $(\\mathrm{SFT^{\\circ}})$ to create preference data, serve as instruction-following reward model $\\mathrm{RM}^{\\mathrm{IF}}$ and as the initialization of DPO. The data used to fine-tune different variants are listed in Table 11. We conduct fine-tuning with full parameters on 64 NVIDIA A100 (80GB) GPUs. SFT and DPO require around 1 and 4 hours to complete, respectively. ", "page_idx": 16}, {"type": "text", "text": "A.7 Limitations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Limitations. In this paper, we use FACTSCORE as the main metric of factuality. However, as highlighted by Min et al. [2023], \u201cFACTSCORE focuses on factual precision is only one aspect of the broader factuality problem.\u201d In addition, the validity of applying FACTSCORE to broader domains other than Biography has not been carefully studied. In our experiments, we can only claim that models\u2019 generation shows high agreement with the facts from Wikipedia, without considering whether the claims are open-ended and debatable. Finally, reward hacking is the potential issue of LLM alignment [Chen et al., 2024b] but the risk of reward hacking is still unknown when considering factuality. ", "page_idx": 16}, {"type": "text", "text": "A.8 Broader Impacts ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Increased factual accuracy in AI-generated responses may contribute to building trust in AI technology. Users are more likely to trust and rely on AI systems if they consistently provide accurate information, leading to broader adoption and acceptance of AI-driven solutions. However, FLAME does not deal with the facts which are open-ended and debatable and uses Wikipedia as the only source of facts. This may inadvertently reinforce some biases in the AI system\u2019s responses, potentially perpetuating societal inequalities. ", "page_idx": 16}, {"type": "table", "img_path": "zWuHSIALBh/tmp/e41e4a0f01c6baebe7bf06142028c08fd292f3a041bea0dfe1cc7779e21c7766.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "zWuHSIALBh/tmp/4c3b5f503d67af964c7a9eb154d7d88370326b6e93e61e63edbaef377c8b708a.jpg", "img_caption": ["Figure 4: Prompt to check whether an instruction is fact-based. ", "Figure 5: Prompt for fact check. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "zWuHSIALBh/tmp/215d898538d3fd2d3d6aa5c1953cfeb0862f041bc4bd9a14352e5e61ae66f709.jpg", "img_caption": ["Figure 6: Prompt to check whether a claim is fact-based. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "zWuHSIALBh/tmp/8e4b697bb49a87d46ad729019dfd5c1db3ecab7b46e95a3c9a930b3d2f63695a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Figure 7: The results of whether a sentence is fact-based or not classified by SFT with prompt in Figure 6. ", "page_idx": 17}, {"type": "image", "img_path": "zWuHSIALBh/tmp/05a9b572afa6ba0a2aee41e66e8e4dc84179b1f2339af7ad376ec535bdb901ea.jpg", "img_caption": ["Figure 8: Prompt to evaluate models\u2019 instruction-following capability from Yuan et al. [2024]. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "[INST] Review the user's question and the corresponding response using the additive 5-point scoring system described below. Points are accumulated based on the satisfaction of each criterion: $\\therefore\\Omega\\backslash\\Omega-$ Add 1 point if the response is relevant and provides some information related to the user's inquiry, even if it is incomplete or contains some irrelevant content.\\n- Add another point if the response addresses a substantial portion of the user's question, but does not completely resolve the query or provide a direct answer.\\n- Award a third point if the response answers the basic elements of the user's question in a useful way, regardless of whether it seems to have been written by an AI Assistant or if it has elements typically found in blogs or search results.\\n- Grant a fourth point if the response is clearly written from an AI Assistant's perspective, addressing the user's question directly and comprehensively, and is well-organized and helpful, even if there is slight room for improvement in clarity, conciseness or focus.\\n- Bestow a fifth point for a response that is impeccably tailored to the user's question by an AI Assistant, without extraneous information, reflecting expert knowledge, and demonstrating a high-quality, engaging, and insightful answer.\\n\\n\\nUser:  Write a series of steps explaining how Edward Snowden leaked classified government documents to the press, starting from when he first got employed by the U.S. government and ending shortly after he landed in Russia. \\n\\n<response> Here is a series of steps outlining the events surrounding Edward Snowden's leaking of classified government documents:\\n\\n Edward Snowden was employed by the U.S. government as a contractor for the National Security Agency (NSA) in 2006.\\n\\n Over the next several years, Snowden worked on various projects for the NSA and gained access to highly classified information and documents.\\n\\n   In early 2013, Snowden became increasingly concerned about the government's widespread surveillance activities and felt that the public had a right to know about them.\\n\\n Snowden began copying and collecting classified documents from the NSA's systems, taking care to cover his tracks.\\n\\n In May 2013, Snowden left his job in Hawaii and traveled to Hong Kong, where he met with journalists from The Guardian newspaper and provided them with a selection of the classified documents he had obtained.\\n\\n The Guardian published a series of articles based on the leaked documents, starting in  June 2013, which revealed the extent of the NSA's surveillance activities, including its collection of metadata on millions of American citizens and its access to the servers of major tech companies like Google and Facebook.\\n\\n The U.S. government charged Snowden with violating the Espionage Act and sought his extradition from Hong Kong.\\n\\n Fearing for his safety and freedom, Snowden left Hong Kong and traveled to Moscow, where he was granted asylum by the Russian government.\\n\\n Snowden has been living in Russia since 2013 and continues to speak out against government surveillance and advocate for privacy and civil liberties.\\n\\nThese are the major steps in the story of Edward Snowden's leaking of classified government documents to the press. The aftermath of these events continues to shape the ongoing debates around privacy, security, and government surveillance. </response>\\n\\n After examining the user's instruction and the response:\\n\\n- Briefly justify your total score, up to 100 words.\\n- Conclude with the score using the format: \\\"Score: <total points>\\\"\\n\\nRemember to assess from the AI Assistant perspective, utilizing web search knowledge as necessary. [/INST] To evaluate the response in alignment with this additive scoring model, we'll systematically attribute points based on the outlined criteria.\\n\\n1. Relevance: 1 point - The response effectively addresses the user's question by outlining the key steps involved in Edward Snowden's disclosure of classified documents.\\n2. Substantial portion: 1 point - The response comprehensively covers the essential events and elements of Snowden's story, from his employment to his asylum in Russia.\\n3. Useful answer: 1 point - The answer delivers a well-structured account of Snowden's actions, government surveillance, and the consequences of his disclosures, providing valuable information for the user.\\n4. AI Assistant perspective: 1 point - The response is written in a clear and organized manner, effectively summarizing the events surrounding Snowden's leaks.\\n5. Expert knowledge and engaging answer: 1 point - The response demonstrates a good understanding of the topic, concisely presenting complex information and highlighting the significance of Snowden's actions.\\n\\n Score: 5/5 \\n\\nPlease note that the scoring system is subjective and might vary based on individual interpretations. The provided response effectively addresses the user's question and provides a clear, well-structured account of the events surrounding Edward Snowden's disclosure of classified documents. ", "page_idx": 18}, {"type": "text", "text": "Figure 9: An example of EFT data. The texts with the colors of green, red and blue are the instruction, response and the LLM-as-a-judge results (explanation and score), respectively ", "page_idx": 18}, {"type": "table", "img_path": "zWuHSIALBh/tmp/b955aeea7d24b06ade8f3fa998ea0fdc4cda84ee0c15a7332e041e1ddf596e60.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "zWuHSIALBh/tmp/3882e356b665e9dc5d71ca1d08a7b8014640d5bd6d6cdb305c2822fa225b2f58.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 10: Generation comparisons for instructions from Alpaca Eval and Biography (very rare and frequent entities). Determined through manual verification using Google search, red denotes incorrect identified facts while pink indicates unverified facts; e.g., we cannot search relevant pages about Ji Sung\u2019s involvement in charitable causes but also cannot dismiss the possibility of his contributions. Note that the popularity of an entity is defined by its occurrence and page views in Wikipedia, which are provided by Min et al. [2023]. ", "page_idx": 19}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The main claim of our paper is to advance the understanding of the underlying causes of LLM hallucination as well as its relation to the alignment procedure (from lines 22\u201323) and study how to make the LLM alignment process more factual (lines 5\u20136). We thus conduct a pilot study to identify the key factors which impact the LLMs\u2019 factuality during alignment procedure in Section 3. Based on the finding, we implement the factualityaware alignment in Section 4 and demonstrate the effectiveness of our approach through comprehensive experiments in Section 5. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 20}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We include the discussion of limitations in Appendix A.7. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 20}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: This is an empirical paper; thus, there is no theoretical result included. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 21}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We include all the high-level information to reproduce our models in Section 4.1 and 4.2, and due to space limitation, more detailed information are included in Appendix A.3, A.5 and A.6. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. ", "page_idx": 21}, {"type": "text", "text": "In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 22}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: While we do not provide the code to reproduce the main experimental results, we provide all the necessary information and URL links to training and evaluation data. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 22}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We provide training details in Appendix A.6 and test details in Section 5.1 Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 22}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: We follow the instruction following evaluation in Alpaca Eval [Dubois et al., 2024] to report the win rate comparisons between models, which is not suitable for statistical significance test. For FACTSCORE, we follow the established procedure to compare models\u2019 average FACTSCORE [Min et al., 2023], which is correlated to human evaluation. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 23}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We provide the required computation resources in Appendix A.6. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 23}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We have reviewed and made sure our paper conforms the NeurIPS Code of Ethics. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 23}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We have included the discussion of broader impacts in Appendix A.8. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 24}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: We do not release any new data and models in the paper. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 24}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We have cited all the papers, which provide the models and datasets used in our paper. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets. \u2022 The authors should cite the original paper that produced the code package or dataset. \u2022 The authors should state which version of the asset is used and, if possible, include a URL. ", "page_idx": 24}, {"type": "text", "text": "\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 25}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 25}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 25}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 25}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 26}]