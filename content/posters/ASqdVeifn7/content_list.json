[{"type": "text", "text": "4-bit Shampoo for Memory-Efficient Network Training ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Sike Wang Beijing Normal University sikewang@mail.bnu.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Pan Zhou Singapore Management University panzhou@smu.edu.sg ", "page_idx": 0}, {"type": "text", "text": "Jia Li\u2020 Beijing Normal University jiali@bnu.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Hua Huang Beijing Normal University huahuang@bnu.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Second-order optimizers, maintaining a matrix termed a preconditioner, are superior to first-order optimizers in both theory and practice. The states forming the preconditioner and its inverse root restrict the maximum size of models trained by second-order optimizers. To address this, compressing 32-bit optimizer states to lower bitwidths has shown promise in reducing memory usage. However, current approaches only pertain to first-order optimizers. In this paper, we propose the first 4-bit second-order optimizers, exemplified by 4-bit Shampoo, maintaining performance similar to that of 32-bit ones. We show that quantizing the eigenvector matrix of the preconditioner in 4-bit Shampoo is remarkably better than quantizing the preconditioner itself both theoretically and experimentally. By rectifying the orthogonality of the quantized eigenvector matrix, we enhance the approximation of the preconditioner\u2019s eigenvector matrix, which also benefits the computation of its inverse 4-th root. Besides, we find that linear square quantization slightly outperforms dynamic tree quantization when quantizing second-order optimizer states. Evaluation on various networks for image classification and natural language modeling demonstrates that our 4-bit Shampoo achieves comparable performance to its 32-bit counterpart while being more memory-efficient\\*. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Deep neural networks (DNNs) have achieved great success in numerous fields, e.g., computer vision [20], natural language processing [38], and speech recognition [16]. A significant part of such success is attributed to first-order optimizers such as stochastic gradient descent with momentum (SGDM) [31] and AdamW [29]. Second-order optimizers, including K-FAC [30], Shampoo [18], AdaBK [41], CASPR [13], and Sophia [27], show great convergence properties, but often involve noticeable computation and memory costs. Anil et al. [2] provided several practical techniques for second-order optimizers to achieve substantial wall-clock time improvements over traditional first-order optimizers. The fast convergence property of second-order optimizers benefits from preconditioning the gradient with a matrix known as a preconditioner. The optimizer states for constructing the preconditioner and its inverse root can speed up optimization compared to first-order optimizers, but consume memory that could be used for model parameters, limiting the maximum model size trained within a given memory budget. With the increase in model size, the memory utilized by optimizer states can become a predominant factor in memory usage. This is the primary obstacle hindering the widespread use of second-order optimizers in the era of large models. ", "page_idx": 0}, {"type": "text", "text": "There are two main attempts to reduce memory consumed by optimizer states. Factorization uses lowrank approximation to optimizer states. This strategy has been applied to first-order optimizers [35, 3] and second-order optimizers [14, 40]. In a comparable but distinct line of work, quantization utilizes low-bit to compress 32-bit optimizer states. Quantization is attractive due to its simplicity and wide applicability, which has been applied to first-order optimizers [8, 26]. Applying quantization to second-order optimizers poses a greater challenge, as first-order optimizers\u2019 states are elementwise, whereas second-order optimizers rely on matrix operations. To our knowledge, it has not been attempted before. ", "page_idx": 0}, {"type": "image", "img_path": "ASqdVeifn7/tmp/6da65ec47cd3c7623b8c31bd5c123cb4b012aa528176ed6e629f90e92448095b.jpg", "img_caption": ["Figure 1: Visualization of test accuracies and total GPU memory costs of vision transformers. 4-bit Shampoo (naive) quantizes the preconditioner, while 4-bit Shampoo (our) quantizes its eigenvector matrix. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Contributions: In this paper, we present the first second-order optimizers with 4-bit optimizer states by taking Shampoo [18] as an example, while preserving the performance achieved with 32-bit optimizer states. While our focus is on Shampoo, we believe that our approach could also be applied to other second-order optimizers (see Table 4). Our main contributions are highlighted below. ", "page_idx": 1}, {"type": "text", "text": "Firstly, to maintain 32-bit performance, we propose quantizing the eigenvector matrix of a preconditioner in 4-bit Shampoo, rather than the preconditioner itself. The reason is that the small singular values of the preconditioner matter. Directly quantizing the preconditioner via block-wise quantization [8] at 4-bit precision can significantly alter the small singular values, leading to a drastic change in its inverse 4-th root and thus harming 4-bit Shampoo\u2019s performance. Quantizing the eigenvector matrix can help alleviate this issue, which is supported by experimental validation and theoretical insight. Additionally, with the eigenvector matrix, computing the inverse 4-th root is straightforward, ensuring that quantizing the eigenvector matrix does not lead to a rise in the total wall-clock time compared to quantizing the preconditioner (see Figure 1). ", "page_idx": 1}, {"type": "text", "text": "Secondly, we present two techniques for enhancing performance. As the eigenvector matrix of a preconditioner is orthogonal, we apply Bj\u00f6rck orthonormalization [4] to rectify the orthogonality of the quantized eigenvector matrix, leading to improved approximation of preconditioner\u2019s eigenvector matrix and facilitating computation of its inverse 4-th root. Additionally, we observe that linear square quantization outperforms dynamic tree quantization [7] marginally when quantizing second-order optimizer states. The superiority of our developed 4-bit Shampoo is demonstrated in Figure 1. ", "page_idx": 1}, {"type": "text", "text": "Finally, we evaluate our 4-bit Shampoo on different image classification and natural language modeling tasks using convolutional neural network (CNN) and transformer architectures. Across all these benchmarks, our 4-bit Shampoo achieves similarly fast convergence comparable to its 32-bit counterpart, with no significant increase in losses for the trained models. Our 4-bit Shampoo uses less memory than its 32-bit counterpart, allowing for training of larger models with given resources. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this section, we present Shampoo and its implementation in our experiments. We also discuss quantization-based compression methods in a general formulation. ", "page_idx": 1}, {"type": "text", "text": "Notations. We use a non-bold letter like $a$ or $A$ to denote a scalar, a boldfaced lower-case letter like $\\textbf{\\em a}$ to denote a vector, and a boldfaced upper-case letter such as $\\pmb{A}$ to denote a matrix. $\\pmb{u}\\!=\\![u_{i}]^{\\top}$ means that the $i$ -th element of column vector $\\textbf{\\em u}$ is $u_{i}$ and $U\\!=\\![u_{i}]$ means the $i$ -th column vector of matrix $U$ is $\\pmb{u}_{i}$ . Let $\\pmb{A}$ be a positive definite (PD) matrix and $s\\in\\mathbb{R}$ , we define $A^{s}\\!=\\!U\\Lambda^{s}U^{\\top}$ , where $U\\mathbf{A}U^{\\mathsf{T}}$ is the Singular Value Decomposition (SVD) of $\\pmb{A}$ . $\\operatorname{tr}(A)$ represents the trace of a matrix $\\pmb{A}$ . The inner product of two matrices $\\pmb{A}$ and $_B$ is denoted as $\\langle A,B\\rangle\\!=\\!\\mathrm{tr}(A^{\\top}B)$ . The Frobenius norm of a matrix $\\pmb{A}$ is $\\|A\\|_{F}\\!=\\!\\sqrt{\\langle A,A\\rangle}$ . $A\\odot B$ means the elementwise matrix product (Hadamard product). ", "page_idx": 1}, {"type": "text", "text": "$\\operatorname{Diag}(a)$ is a diagonal matrix with diagonal vector $\\textbf{\\em a}$ , while $\\operatorname{diag}(A)$ means the diagonal vector of matrix $\\pmb{A}$ . ", "page_idx": 2}, {"type": "text", "text": "2.1 Shampoo for Matrices ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The update rule of Shampoo in the matrix case combined with a first-order optimizer $\\mathcal{F}$ is ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{Shampoo}(W_{t-1},L_{t-1},R_{t-1},s_{t-1},G_{t})=\\left\\{\\begin{array}{l l}{L_{t}\\!=\\!L_{t-1}\\!+\\!G_{t}G_{t}^{\\top}}\\\\ {R_{t}\\!=\\!R_{t-1}\\!+\\!G_{t}^{\\top}G_{t}}\\\\ {\\widehat{G}_{t}\\!=\\!L_{t}^{-1/4}G_{t}R_{t}^{-1/4}}\\\\ {\\widetilde{G}_{t}\\!=\\!\\widehat{G}_{t}(\\lVert G_{t}\\rVert_{F}/\\lVert\\widehat{G}_{t}\\rVert_{F})}\\\\ {W_{t},s_{t}\\!=\\!\\mathcal{F}(W_{t-1},s_{t-1},\\widetilde{G}_{t})}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $W_{t}$ is the model parameters in matrix form, $\\pmb{L}_{t}$ and $\\textstyle R_{t}$ are called preconditioners, $s_{t}$ is the optimizer state of F, and Gt is the gradient at Wt\u22121. Note that Lt, Rt, Lt\u22121/4, and Rt\u22121/4a re PD matrices. The penultimate step in (1) is the grafting trick [1], which enables Shampoo to roughly apply the well-tuned learning rate schedule of $\\mathcal{F}$ . The optimization variable $W_{t}$ does not represent all model parameters. It denotes a tensor of the model [18] or one block of a tensor [2]. In practice, we adopt an efficient and effective implementation of Shampoo for training DNNs following [2, 41] as described in Algorithm 4. In order to achieve efficient training, Lt, Rt, Lt\u22121/4, a nd Rt\u22121/4ar e computed once every few hundred iterations. In this case, besides $\\pmb{L}_{t}$ and $\\textstyle R_{t}$ , their inverse 4-th roots should also be stored in memory, as computing them is computationally expensive. So training large models with Shampoo can be memory-intensive, consuming a significant amount of memory. ", "page_idx": 2}, {"type": "text", "text": "2.2 Quantization-based Compression Methods ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Quantizing updated optimizer states using a quantizer and then dequantizing them with a dequantizer prior to use is an effective method for conserving memory. We focus exclusively on vectors, as tensors can be reshaped into vectors. ", "page_idx": 2}, {"type": "text", "text": "Quantization. According to the idea in [8, 26], a $b$ -bit quantizer $\\mathcal{Q}$ for $p$ -dimensional real vectors is a mapping given by ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{Q}=(\\mathcal{T}\\circ\\mathcal{N},\\mathcal{M}):\\mathbb{R}^{p}\\rightarrow\\mathbb{T}_{b}^{p}\\times\\mathbb{R}^{p},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathcal{N}$ is a normalization operator on $\\mathbb{R}^{p},\\mathcal{T}$ is an elementwise function mapping any real number to an element of $\\mathbb{T}_{b}\\!=\\!\\{0,1,\\dotsc,2^{b}\\!-\\!1\\}$ , and $\\mathcal{M}$ is a maximum operator on $\\mathbb{R}^{p}$ . For any $\\pmb{x}\\in\\mathbb{R}^{p}$ , $\\mathcal{N}$ and $\\mathcal{M}$ satisfy $\\mathcal{N}(\\pmb{x})\\odot\\mathcal{M}(\\pmb{x})\\!=\\!\\pmb{x}$ . ", "page_idx": 2}, {"type": "text", "text": "A normalization operator $\\mathcal{N}$ for $p$ -dimensional vectors is a transformation on $\\mathbb{R}^{p}$ . It scales each element of a vector $\\pmb{x}\\in\\mathbb{R}^{p}$ into $[-1,1]$ . A block-wise normalization operator for a $p$ -dimensional vector $\\mathbf{\\boldsymbol{x}}=[x_{1},x_{2},\\ldots,x_{p}]^{\\mathsf{T}}$ is defined as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{N}(\\pmb{x})_{i}=\\frac{x_{i}}{\\operatorname*{max}_{j\\in\\mathbb{X}_{i}}\\{x_{j}\\}},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where ${\\mathcal{N}}({\\boldsymbol{x}})_{i}$ is the $i$ -th element of ${\\mathcal{N}}({\\boldsymbol{x}})$ , and $\\mathbb{X}_{i}$ is a set satisfying $i\\in\\mathbb{X}_{i}\\subset\\{1,\\ldots,p\\}$ . Usually, $\\mathbb{X}_{i}$ should also satisfy ${\\mathbb X}_{i}\\!=\\!{\\mathbb X}_{j}$ or ${\\mathbb X}_{i}\\cap{\\mathbb X}_{j}\\!=\\!\\emptyset$ for $i,j\\in\\{1,\\ldots,p\\}$ . In this case, for any $\\pmb{x}\\in\\mathbb{R}^{p}$ , the number of different elements in $\\mathcal{M}(\\pmb{x})$ is equal to the number of elements in set $\\{\\mathbb{X}_{i}|i=1,\\ldots,p\\}$ . Meanwhile, the number of the elements in ${\\mathbb X}_{i}$ for any $i$ should be as close as possible to a value called block size. ", "page_idx": 2}, {"type": "text", "text": "The mapping $\\mathcal{T}$ for $x\\in\\mathbb R$ in a $b$ -bit quantizer $\\mathcal{Q}$ is defined as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{Z}(\\boldsymbol{x})=\\underset{\\boldsymbol{j}\\in\\mathbb{T}_{b}}{\\operatorname{argmin}}\\left|\\boldsymbol{x}-\\mathcal{R}(\\boldsymbol{j})\\right|,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathcal{R}$ named quantization mapping is an elementwise function that maps any element in $\\mathbb{T}_{b}$ into $[-1,1]$ , and $|\\cdot|$ is the absolute operator for a scalar. There are three typical quantization mappings: linear quantization, dynamic quantization, and quantile quantization. Their specifications and visualizations can be found in [8]. ", "page_idx": 2}, {"type": "text", "text": "Dequantization. Given a $b$ -bit quantizer $\\scriptstyle Q\\;=\\;({\\cal Z}\\;\\circ\\;{\\mathcal N},\\;{\\mathcal M})$ for a $p$ -dimensional real vector $\\pmb{x}\\in\\mathbb{R}^{p}$ , the corresponding dequantizer $\\mathcal{D}$ is a mapping defined as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{D}(\\boldsymbol{\\mathcal{Q}}(\\boldsymbol{x}))\\!=\\!\\mathcal{D}(\\boldsymbol{\\mathcal{Z}}\\circ\\mathcal{N}(\\boldsymbol{x}),\\mathcal{M}(\\boldsymbol{x}))\\!=\\!\\mathcal{R}(\\boldsymbol{\\mathcal{Z}}\\circ\\mathcal{N}(\\boldsymbol{x}))\\odot\\mathcal{M}(\\boldsymbol{x}):\\mathbb{T}_{b}^{p}\\times\\mathbb{R}^{p}\\to\\mathbb{R}^{p}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we describe the design of our quantization-based compression method to realize 4-bit Shampoo with fast and high precision quantization. Let $\\scriptstyle Q\\;=\\;({\\cal Z}\\;\\circ\\;{\\mathcal N},\\;{\\mathcal M})$ be a quantizer and $\\mathcal{D}$ be its corresponding dequantizer as described in Subsection 2.2. ", "page_idx": 3}, {"type": "text", "text": "3.1 Quantizing the Eigenvector Matrices ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "A naive approach to realize 4-bit Shampoo is applying the compression methods proposed in [8, 26] to Lt, Rt, Lt\u22121/4, and R\u22121/4i n Shampoo (see (1)). A slightly improved approach is to quantize the four PD matrices excluding their diagonal elements, which are typically much larger than their nondiagonal counterparts due to the non-negativity of the elements in $\\operatorname{{\\mathrm{Hiag}}}(\\pmb{G}_{t}\\pmb{G}_{t}^{\\top})$ and $\\mathrm{diag}(G_{t}^{\\top}G_{t})$ . ", "page_idx": 3}, {"type": "text", "text": "However, the naive approach can cause large quantization errors at 4-bit precision. This is because the quantization errors (or called perturbations) of quantizing Lt and Rt will transfer to Lt\u22121/4 and $R_{t}^{-1/4}$ . To verify this, we first introduce two criteria to evaluate the quantization errors of matrices. We do not use the elementwise criterion in [8]. Let $\\pmb{A}$ denote a 32-bit matrix, $g$ represent a transformation (can formed by quantization), and $f$ stand for a mapping, e.g., $f(A)\\!=\\!A^{-1/4}$ . Then we define the normwise relative error (NRE) and angle error (AE) in $f$ of $g$ at $\\pmb{A}$ as ", "page_idx": 3}, {"type": "equation", "text": "$$\n{\\mathrm{NRE}}\\!=\\!{\\frac{\\|f(A)-f(g(A))\\|_{F}}{\\|f(A)\\|_{F}}},\\quad{\\mathrm{AE}}\\!=\\!\\operatorname{arccos}\\left({\\frac{\\langle f(A),f(g(A))\\rangle}{(\\|f(A)\\|_{F}\\|f(g(A))\\|_{F}}}\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We choose two PD matrices of order 1200. The first one $A_{1}$ is derived from the real world. It is a preconditioner in 32-bit Shampoo combined with AdamW for training a Swin-Tiny model. The second one $A_{2}\\!=\\!U\\Lambda U^{\\top}$ is synthetic, constructed from a random orthogonal matrix $U$ and a diagonal matrix $\\pmb{\\Lambda}$ with only two distinct diagonal values. Table 1 shows the quantization errors in $\\bar{f}(A)=A^{-1/4}$ of the naive approach at these two matrices, which are remarkably high. More analyses are given in Appendix D. The key point is that the singular values of $A_{i}(i\\!=\\!1,2)$ follow a specific distribution (see Figure 2). In this scenario, a slight perturbation of $\\boldsymbol{A}_{i}$ will significantly alter its small singular values, resulting in a drastic change to Ai\u22121/4. ", "page_idx": 3}, {"type": "text", "text": "To address this issue, we propose quantizing the eigenvector matrix of a preconditioner in Shampoo, rather than the preconditioner itself. Namely, a preconditioner $\\pmb{A}$ is a PD matrix, and its SVD is $U\\mathbf{A}U^{\\mathsf{T}}$ , where $U$ represents the eigenvector matrix and $\\pmb{\\Lambda}$ denotes the singular value matrix. Given that $\\Lambda$ is a diagonal matrix, we can focus on quantizing $U$ using $\\mathcal{Q}$ while leaving $\\pmb{\\Lambda}$ unchanged. From Table 1, one can observe that quantizing $U$ can significantly reduce the quantization errors. We will theoretically discuss the advantages of quantizing $U$ compared to quantizing $\\pmb{A}$ in Section 4. In practice, the randomized SVD method [19] is adopted to compute the SVD of $\\pmb{A}$ efficiently, as shown in [40]. We want to highlight that quantizing the original $\\pmb{L}_{t}$ and $\\textstyle R_{t}$ in Shampoo involves significant computational burdens to compute their inverse 4-th roots $L_{t}^{-1/4}$ and $R_{t}^{-1/4}$ , whereas quantizing the eigenvector matrices of $\\pmb{L}_{t}$ and $\\textstyle R_{t}$ allows for rapid inverse root calculation. So the computational time required for both approaches is comparable (see Figure 1). ", "page_idx": 3}, {"type": "text", "text": "Table 1: Quantization errors in $A^{-1/4}$ of different quantization schemes at a PD matrix $\\pmb{A}$ . We employ block-wise normalization with a block size of 64. $U$ is the eigenvector matrix of $\\pmb{A}$ , ${\\mathrm{QM}}={}$ quantized matrix, and $\\mathrm{OR}=$ orthogonal rectification. ", "page_idx": 3}, {"type": "table", "img_path": "ASqdVeifn7/tmp/f00d1d9c080c947e0adcbab0507c05a48e7ce48ea2ff8c87a4a49cd965f3c3ff.jpg", "table_caption": [], "table_footnote": [], "page_idx": 3}, {"type": "image", "img_path": "ASqdVeifn7/tmp/084dc43073c63458b251e46806c88475cccf07e7ecc75b3de676cbbb15a45408.jpg", "img_caption": ["Figure 2: Singular value distributions of PD matrices (real) and their 4-bit compressions (quan) used in Table 1 with $\\scriptstyle{\\mathcal{R}}=\\scriptstyle\\mathbf{D}\\mathbf{T}$ , $\\scriptstyle{\\mathrm{QM}}=\\mathbf{A}$ . Singular values are shown on a $\\log_{10}$ scale. "], "img_footnote": [], "page_idx": 4}, {"type": "image", "img_path": "ASqdVeifn7/tmp/e8ab8735bab6fe53e65010fcc3b91d734d45aa08cac84d0d5dbddededffd9366.jpg", "img_caption": ["Figure 3: Elementwise mean errors between $(V_{t_{2}}\\mathbf{\\Lambda}^{s}V_{t_{2}}^{\\top})^{-1/s}(V_{t_{2}}\\mathbf{\\Lambda}\\mathbf{\\Lambda}V_{t_{2}}^{\\top})$ and identity matrix $\\boldsymbol{\\mathit{I}}$ . Mean errors are shown on a $\\log_{10}$ scale. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "3.2 Rectifying the Orthogonality of Eigenvector Matrices ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Let $\\pmb{A}$ be a PD matrix with SVD $U\\mathbf{A}U^{\\mathsf{T}}$ . Note that the eigenvector matrix $U$ is orthogonal, whereas $V\\!=\\!{\\mathcal{D}}({\\mathcal{Q}}(U))$ may not be. To further mitigate the quantization errors mentioned in Subsection 3.1, we propose employing Bj\u00f6rck orthonormalization [4] to orthogonalize $V$ . Particularly, given $V_{0}\\!=\\!V$ , we iterate ", "page_idx": 4}, {"type": "equation", "text": "$$\nV_{t}\\,{=}\\,1.5V_{t-1}\\,{-}\\,0.5V_{t-1}V_{t-1}^{\\top}V_{t-1},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "for $t_{1}\\!\\geq\\!1$ times and take $V_{t_{1}}$ as the rectified result. Equation (2) can also be interpreted as the gradient descent of problem minV $\\|\\b{V}^{\\top}\\b{V}-\\b{I}\\|_{F}^{2}$ using a step size of 0.5, where $\\boldsymbol{\\mathit{I}}$ denotes the identity matrix. We empirically find that only one iteration (i.e., $t_{1}\\!=\\!1$ ) is enough. Table 1 illustrates the benefit of rectifying $V$ into $V_{1}$ . ", "page_idx": 4}, {"type": "text", "text": "The update frequencies for the preconditioners and their inverse 4-th roots differ (see Algorithm 3). Given $V$ and $\\pmb{\\Lambda}$ , we also require orthogonal rectification to compute $A^{s}$ rapidly for any $s\\,\\in\\,\\mathbb{R}$ . The reason is as follows. It is easy to compute $A^{s}=U\\Lambda^{s}U^{\\top}$ by definition. However, ${\\bar{\\b{U}}}{\\bf{A}}^{s}{\\pmb{U}}^{\\top}$ can be very sensitive to the orthogonality of $U$ for $s<0$ , making $V\\Lambda^{s}V^{\\top}$ largely deviate from $(V\\Lambda V^{\\top})^{s^{\\prime}}{\\approx}\\,A^{s}$ . Similarly, we can approximate $A^{s}$ by $V_{t_{2}}\\mathbf{\\Lambda}\\mathbf{\\Lambda}^{s}V_{t_{2}}^{\\tilde{\\mathsf{T}}}$ , where $V_{t_{2}}$ is generated by (2). Figure 3 illustrates the elementwise mean errors between $(V_{t_{2}}\\mathbf{\\Lambda}^{s}V_{t_{2}}^{\\top})^{-1/s}(V_{t_{2}}\\mathbf{\\Lambda}\\mathbf{\\Lambda}V_{t_{2}}^{\\top})$ and $\\boldsymbol{\\mathit{I}}$ for various $s$ and $t_{2}$ , where $\\pmb{A}$ is the real-world matrix used in Table 1. Based on the observation from Figure 3, we set $t_{2}\\!=\\!4$ in our experiments. ", "page_idx": 4}, {"type": "text", "text": "3.3 Selecting the Quantizer ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The quantizer $\\mathcal{Q}$ is defined by the normalization operator $\\mathcal{N}$ and mapping $\\mathcal{R}$ , and $\\mathcal{N}$ is determined by ${\\mathbb X}_{i}$ . Since an eigenvector has a unit length, the elements in $\\mathbb{X}_{i}$ should belong to the same column of an eigenvector matrix, i.e., they are from the same eigenvector. Instead of employing dynamic tree (DT) quantization as mapping $\\mathcal{R}$ , we recommend utilizing linear square (Linear-2) quantization as $\\mathcal{R}$ , particularly when $b\\!=\\!4$ . Linear-2 quantization is defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{R}(j)=\\left\\{\\begin{array}{c c}{-\\left(-1+2j/(2^{b}\\!-\\!1)\\right)^{2},}&{j\\!<\\!2^{b\\!-\\!1}\\!-\\!1;}\\\\ {0,}&{j\\!=\\!2^{b\\!-\\!1}\\!-\\!1;}\\\\ {\\left(-1\\!+\\!2j/(2^{b}\\!-\\!1)\\right)^{2},}&{j\\!>\\!2^{b\\!-\\!1}\\!-\\!1,}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $j\\in\\mathbb{T}_{b}\\!=\\!\\{0,1,\\dots,2^{b}\\!-\\!1\\}$ . As shown in Table 1, Linear-2 quantization has lower quantization errors compared to DT quantization at 4-bit precision. ", "page_idx": 4}, {"type": "text", "text": "3.4 Overall Algorithm ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We first describe the update processes of the preconditioners and their inverse 4-th roots in our 4-bit Shampoo. A preconditioner $\\pmb{A}$ is a PD matrix and its SVD is $U\\mathbf{A}U^{\\top}$ . We can compress $\\pmb{A}$ into a pair $(\\lambda,\\overline{{U}})=(\\mathrm{diag}(\\mathbf{A}),\\mathcal{Q}(U))$ and decompress it into $(\\Lambda,V)=(\\mathrm{Diag}(\\lambda),{\\mathcal D}(\\overline{{U}}))$ . Algorithm 1 (Preconditioner Update, PU) shows the update rule of $\\pmb{A}$ . Similarly, we compress $\\widehat{\\pmb{A}}\\approx\\pmb{A}^{-1/4}$ into a pair $(a,\\overline{{{\\cal A}}})=(\\mathrm{diag}(\\widehat{\\cal A}),\\mathcal{Q}(\\widehat{\\cal A}\\!-\\!\\mathrm{Diag}(\\dot{a})))$ and decompress it into $\\mathrm{Diag}(a)+{\\mathcal{D}}({\\overline{{\\mathbf{A}}}})$ . Algorithm 2 (Preconditioner\u2019s Inverse 4-th Root Update, PIRU) gives the update rule ofA. Based on the above update rules, we can summarize our 4-bit Shampoo in Algorithm 3. Note th at we omit some input parameters of PU and PIRU because they can be found in Algorithm 3 in the same form. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "table", "img_path": "ASqdVeifn7/tmp/728e624fd29c5adc240a8325953ae785909471b1e987341376a1825e76965497.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "4 Theoretical Analysis ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we analyze why quantizing the eigenvector matrix of a preconditioner in Shampoo is better than quantizing the preconditioner itself under a certain singular value distribution. Furthermore, we consider quantization as a perturbation and prove the convergence of the perturbed Shampoo (Algorithm 6) in Appendix E. The following lemma reveals some good properties of perturbing the eigenvector matrix of a PD matrix. ", "page_idx": 5}, {"type": "text", "text": "Lemma 1. Let A be a $P D$ matrix whose SVD is $U\\mathbf{A}U^{\\mathsf{T}}$ , where $U\\!=\\![u_{i}]$ is an orthogonal matrix and $\\mathbf{A}{=}\\mathrm{diag}([\\lambda_{i}]^{\\mathsf{T}})$ is a diagonal matrix. Given a perturbation $\\Delta U\\!=\\![\\Delta u_{i}]$ and $s\\in\\mathbb{R}$ , we define $\\pmb{B}\\!:=\\!(\\pmb{U}\\pmb{\\Lambda}\\pmb{U}^{\\top})^{s}$ and $\\Delta B\\!:=((U\\!+\\!\\Delta U)\\Lambda(U\\!+\\!\\Delta U)^{\\top})^{s}-B$ . ", "page_idx": 5}, {"type": "text", "text": "$(I)$ If $U\\!+\\!\\Delta U$ is orthogonal and there exists $\\alpha\\in\\mathbb{R}$ such that $\\|\\Delta\\pmb{u}_{i}\\|_{2}\\leq\\alpha$ , then ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\frac{\\|\\Delta B\\|_{F}}{\\|B\\|_{F}}\\le2\\alpha.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "(2) If $U\\!+\\!\\Delta U$ is orthogonal and there exists $\\beta\\in\\mathbb{R}$ such that $\\langle{\\pmb u}_{i},{\\pmb u}_{i}\\!+\\!\\Delta{\\pmb u}_{i}\\rangle\\geq1\\!-\\!\\beta\\geq0,$ , then ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\frac{\\langle B,B+\\Delta B\\rangle}{\\|B\\|_{F}\\|B+\\Delta B\\|_{F}}\\geq(1\\!-\\!\\beta)^{2}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "From Lemma 1, it is evident that the normwise relative error and angle error in $f(A)\\,=\\,A^{s}$ of perturbing $U$ at $\\pmb{A}=\\pmb{U}\\pmb{\\Lambda}\\pmb{U}^{\\top}$ are independent of $\\pmb{\\Lambda}$ and $s$ . Moreover, these errors are well-bounded under some mild conditions. Empirically, for 4-bit quantization, $\\alpha=0.1$ and $\\beta=0.005$ roughly meet the conditions of Lemma 1, leading to $\\frac{\\|\\Delta B\\|_{F}}{\\|B\\|_{F}}\\leq0.2$ and \u2225B\u2225F \u2225B+\u2206B\u2225F \u22650.99. ", "page_idx": 6}, {"type": "text", "text": "It is very complicated to generally analyze the perturbation in $f(A)=A^{s}$ of perturbing $\\pmb{A}$ . Thus, we focus on perturbing the singular values of $\\pmb{A}$ . For simplicity, we assume that both $\\pmb{A}$ and $A+\\Delta A$ have only two distinct singular values, where $\\Delta{A}$ is a perturbation of $\\pmb{A}$ . The following lemma gives the perturbation in $A^{s}$ of perturbing the smaller singular value of $\\pmb{A}$ . ", "page_idx": 6}, {"type": "text", "text": "Lemma 2. Let $\\pmb{A}$ be a PD matrix of order $m{\\mathrel{+{n}}}$ whose SVD is $U\\mathbf{A}U^{\\mathsf{T}}$ , where $m,n\\in\\mathbb{N}_{+}$ , $n=l m,$ , $U\\,=\\,[u_{i}]$ is an orthogonal matrix and $\\mathbf{A}=\\mathrm{diag}([\\lambda_{i}]^{\\mathsf{T}})$ is a diagonal matrix. Assume that $\\pmb{\\Lambda}=$ $\\mathrm{diag}([\\bar{c}\\lambda\\mathbf{\\bar{1}}_{m\\times1}^{\\top},\\lambda\\mathbf{1}_{n\\times1}^{\\top}]^{\\top})$ , $c\\geq1$ , and $\\lambda>0$ . Given a perturbation $\\Delta\\mathbf{A}=\\operatorname{diag}([\\mathbf{0}_{m\\times1}^{\\mathsf{T}},\\Delta\\boldsymbol{\\lambda}_{n\\times1}^{\\mathsf{T}}]^{\\mathsf{T}})$ and $s\\in\\mathbb R$ , we define $\\pmb{B}:=(\\pmb{U}\\pmb{\\Lambda}\\pmb{U}^{\\top})^{s}$ and $\\Delta B\\!:=(U(\\mathbf{A}\\!+\\!\\Delta\\mathbf{A})U^{\\top})^{s}-B$ . ", "page_idx": 6}, {"type": "text", "text": "$(I)$ If $\\Delta\\pmb{\\lambda}_{n\\times1}=(k-1)\\lambda\\mathbf{1}_{n\\times1}$ where $k>0$ , then ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\frac{\\|\\Delta\\pmb{B}\\|_{F}}{\\|\\pmb{B}\\|_{F}}=\\frac{\\sqrt{l}|k^{s}-1|}{\\sqrt{c^{2s}+l}}=h_{1}(s,l).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Moreover, $h_{1}(s,l)$ decreases monotonically with s over $(-\\infty,0)$ and increases monotonically with $l$ over $(0,+\\infty)$ . ", "page_idx": 6}, {"type": "text", "text": "(2) I $f\\Delta\\lambda_{n\\times1}=(t c-1)\\lambda\\mathbf{1}_{n\\times1}$ where $t>0,$ , then ", "page_idx": 6}, {"type": "equation", "text": "$$\n{\\frac{\\langle B,B+\\Delta B\\rangle}{\\|B\\|_{F}\\|B+\\Delta B\\|_{F}}}={\\frac{l t^{s}+c^{s}}{\\sqrt{(1+l t^{2s})(l+c^{2s})}}}=h_{2}(l).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Moreover, $h_{2}(l)$ decreases monotonically with $l$ over $(0,(c/t)^{s}]$ and increases monotonically with $l$ over $((c/t)^{s},+\\infty)$ . ", "page_idx": 6}, {"type": "text", "text": "(3) I $f\\Delta\\lambda_{n\\times1}=(t c-1)\\lambda\\mathbf{1}_{n\\times1}$ where $k=t c>0$ and $l=(c/t)^{s}$ , then ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\frac{\\|\\Delta B\\|_{F}}{\\|B\\|_{F}}=\\frac{|k^{s}-1|}{\\sqrt{k^{s}+1}},\\quad\\frac{\\langle B,B+\\Delta B\\rangle}{\\|B\\|_{F}\\|B+\\Delta B\\|_{F}}=\\frac{2}{\\sqrt{2+k^{s}+1/k^{s}}}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Let us make some comments on the above lemma. First, from Lemma 2(1) we have $h_{1}(1,l)\\,=$ $\\begin{array}{r}{\\frac{\\|\\Delta\\pmb{A}\\|_{F}}{\\|\\pmb{A}\\|_{F}}=\\frac{\\sqrt{l}\\|k-1\\|}{\\sqrt{c^{2}+l}}}\\end{array}$ \u221al|k\u22121|. If k \u22651 , \u2225\u2225\u2206AA\u2225\u2225F = \u2225\u2225\u2206\u039b\u039b\u2225\u2225F is bounded by kc l = t l. Second, if k = tc \u22651 and $s<0$ , one can deduce $h_{2}(l)\\geq\\sqrt{l t^{2s}/(1+l t^{2s})}$ from Lemma 2(2), which indicates that a small $l t^{2s}$ is needed to achieve small $h_{2}(l)$ . We can set $t=0.02$ to simulate 4-bit quantization. Based on Lemma 1 and Lemma 2(3), we have the following proposition. ", "page_idx": 6}, {"type": "text", "text": "Proposition 1. Let $\\pmb{A}$ be a $P D$ matrix of order $m\\!+\\!n$ whose SVD is $U\\mathbf{A}U^{\\mathsf{T}}$ , where $m,n\\in\\mathbb{N}_{+}$ , $n=l m$ , $U=[{\\pmb u}_{i}]$ is an orthogonal matrix, $\\mathbf{A}=\\mathrm{diag}([c\\lambda\\mathbf{1}_{m\\times1}^{\\top},\\lambda\\mathbf{1}_{n\\times1}^{\\top}]^{\\top})$ , $c\\ge1000$ , and $\\lambda>0$ . Given $\\Delta U=[\\Delta{\\pmb u}_{i}]$ , $\\Delta\\mathbf{A}=\\operatorname{diag}([\\mathbf{0}_{m\\times1}^{\\mathsf{T}},\\Delta\\boldsymbol{\\lambda}_{n\\times1}^{\\mathsf{T}}]^{\\mathsf{T}})$ , and $s\\leq-0.25$ , we define $B:=(U\\Lambda U^{\\top})^{s}$ , $B_{1}:=((U+\\Delta U)\\Lambda(U\\!+\\!\\Delta U)^{\\top})^{s}$ , and $B_{2}:=(U(\\mathbf{A}+\\Delta\\mathbf{A})U^{\\top})^{s}$ . If $U+\\Delta U$ is orthogonal, $\\|\\Delta\\pmb{u}_{i}\\|_{2}\\leq0.1$ , $\\langle{\\pmb u}_{i},\\Delta{\\pmb u}_{i}\\rangle\\ge-0.005$ , $\\Delta\\pmb{\\lambda}_{n\\times1}\\!=\\!(0.02c\\!-\\!1)\\lambda\\mathbf{1}_{n\\times1}$ , and $l\\!=\\!(c/0.02)^{s}$ , then ", "page_idx": 6}, {"type": "equation", "text": "$$\n2\\frac{\\|B_{1}-B\\|_{F}}{\\|B\\|_{F}}\\!\\le\\!0.4\\le\\frac{\\|B_{2}-B\\|_{F}}{\\|B\\|_{F}},\\quad6\\left(1-\\frac{\\langle B,B_{1}\\rangle}{\\|B\\|_{F}\\|B_{1}\\|_{F}}\\right)\\!\\le\\!0.06\\le\\left(1-\\frac{\\langle B,B_{2}\\rangle}{\\|B\\|_{F}\\|B_{2}\\|_{F}}\\right).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Proposition 1 requires very strong assumptions. Nevertheless, it provides insight into why quantizing $\\pmb{A}$ can result in a greater normwise relative error and angle error in $A^{s}$ , compared to quantizing $U$ . Complete proofs of Lemma 1, Lemma 2, and Proposition 1 can be found in Appendix F. ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we compare our 4-bit Shampoo combined with SGDM or AdamW to their 32-bit counterparts, as well as the first-order optimizers on various image classification tasks. See more experimental results on image classification and natural language modeling tasks in Appendix H. ", "page_idx": 6}, {"type": "text", "text": "Models, datasets, and hyperparameters. We train VGG19 [36], ResNet34 [20], ViT-Small [10], and Swin-Tiny [28] on the CIFAR-100 [23] and Tiny-ImageNet [24] datasets with one RTX3060Ti GPU, and train ResNet50 and ViT-Base/32 on the ImageNet-1k dataset [34] with one A800 GPU. ", "page_idx": 6}, {"type": "text", "text": "Table 2: Performance, wall-clock time and memory cost on various image classification tasks. TA $=$ test accuracy, ${\\mathrm{WCT}}=$ wall-clock time, and TMC $=$ total GPU memory cost. ", "page_idx": 7}, {"type": "table", "img_path": "ASqdVeifn7/tmp/7262000df52237b5376ca54e6faa371a4003bc65d9df1c981f3362810a58fd11.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "ASqdVeifn7/tmp/2555ba8c4d459b8e9505b1f63c4f9febf82fb76ebcd08b2c1bc1962e6390a36b.jpg", "img_caption": ["Figure 4: Visualization of test accuracies on the CIFAR-100 and ImageNet-1k datasets. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "For hyperparameter settings, we mainly follow [41] to train CNNs and [25, 44] to train vision transformers. For all the tasks, we keep the common hyperparameters of optimizers the same values. See Appendix G for experimental details. ", "page_idx": 7}, {"type": "text", "text": "Main results. We show the performance, wall-clock time, and memory cost in Table 2. Firstorder optimizers run $1.2\\mathbf{x}$ to $1.5\\mathrm{x}$ epochs, resulting in longer wall-clock time, yet yielding lower test accuracies compared to second-order optimizers. In comparison to 32-bit Shampoo, our 4-bit Shampoo shows comparable test accuracies with differences ranging from - $-0.7\\%$ to $0.5\\%$ , increases in wall-clock time varying from $-0.2\\%$ to $9.5\\%$ , and memory savings of $4.5\\%$ to $41\\%$ . Compared to the first-order optimizers, the memory costs of our 4-bit Shampoo only rise by $0.8\\%$ to $12.7\\%$ . This represents a significant advancement in the utilization of second-order optimizers. Following [26], we report the total peak GPU memory consumption rather than the optimizer\u2019s peak GPU memory consumption. Our main focus is on quantizing the states for constructing preconditioners and their inverse roots, which are approximately $7\\mathbf{x}$ smaller for 4-bit Shampoo compared to 32-bit Shampoo (see Appendix G). Figure 4 shows the test accuracy curves on the CIFAR-100 and ImageNet-1k datasets. The test accuracy curves of 4-bit Shampoo and 32-bit Shampoo are very close, both of which are above the test accuracy curves of the first-order optimizers. ", "page_idx": 7}, {"type": "table", "img_path": "ASqdVeifn7/tmp/9edede6829aeac6691efd5cbc336aa7bb7423101957fd482dd25d83eb4c8b688.jpg", "table_caption": ["Table 3: Ablation study on the impact of different quantization techniques to Swin-Tiny training on the CIFAR-100 dataset. $U$ is the eigenvector matrix of a preconditioner $\\pmb{A}$ . $\\mathrm{{QM}=}$ quantized matrix, $\\mathrm{OR}=$ orthogonal rectification in Algorithm 1, $\\mathrm{TL=}$ training loss, and $\\mathrm{TA}=$ test accuracy. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Ablations. We investigate the effectiveness of our proposed quantization techniques. Table 3 indicates that quantizing the eigenvector matrix of a preconditioner is crucial for $b$ -bit $[b=3,4]$ Shampoo to maintain 32-bit performance, and orthogonal rectification is highly beneficial for 3-bit Shampoo. As for quantization mapping, linear square (Linear-2) quantization is comparable to dynamic tree (DT) quantization. We further apply our 4-bit quantization techniques to K-FAC [30], AdaBK [41] and CASPR [13] and the results are shown in Table 4. We can see that the 4-bit optimizers match the performance of their 32-bit counterparts, and reduce memory by over $20\\%$ . ", "page_idx": 8}, {"type": "text", "text": "6 Related Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Second-order optimizers. Different second-order optimizers apply different second-order information. Hessian-based optimizers [39, 27] use the Hessian matrix or its approximation. Fisher-based optimizers [30, 41] utilize the covariance matrix of the accumulated gradients or its approximation based on Kronecker product. Shampoo [18] and CASPR [13] approximate the full AdaGrad [12] preconditioner by a set of small preconditioning matrices. ", "page_idx": 8}, {"type": "table", "img_path": "ASqdVeifn7/tmp/dfb4c55eb9dbd373b8b741e012af54751c3315d58649f8c2926ced4f426fba76.jpg", "table_caption": ["Table 4: Performance and memory cost of training Swin-Tiny on CIFAR-100. TA $=$ test accuracy and $\\mathbf{TMC=}$ total GPU memory cost. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Memory efficient optimizers based on factorization. Adafactor [35] employs the outer product of two vectors to approximate the second moment of Adam [22]. SM3 [3] considers approximating the second moment of Adam by its covers\u2019 statistics. [14] and [40] reduce memory cost of the preconditioner in a second-order optimizer with its low-rank approximation through truncated SVD. ", "page_idx": 8}, {"type": "text", "text": "Memory efficient optimizers based on quantization. Dettmers et al. [8] introduce block-wise dynamic quantization that enables the use of first-order optimizers with 8-bit states. Li et al. [26] push the optimizer states of Adam/AdamW to 4-bit. ", "page_idx": 8}, {"type": "text", "text": "7 Conclusions, Limitations, and Broader Impact ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We propose 4-bit Shampoo, the first low-bit second-order optimizer, designed for memory-efficient training of DNNs. We find that quantizing the eigenvector matrix of the preconditioner is essential to minimize quantization errors in its inverse 4-th root at 4-bit precision, given its sensitivity to alterations in small singular values. We further introduce orthogonal rectification and linear square quantization mapping to improve performance. 4-bit Shampoo achieves lossless performance to 32-bit counterpart in training different DNNs on various tasks. ", "page_idx": 8}, {"type": "text", "text": "Limitations. Preconditioners in Shampoo are symmetric matrices and can be stored as upper triangular matrices, saving almost half of the memory usage. However, the eigenvector matrix of a preconditioner is not symmetric, causing an 8-bit preconditioner to occupy the same memory as its 4-bit eigenvector matrix. Notably, a comparison of Table 1 and Table 7 in Appendix D shows that the 4-bit quantization of the eigenvector matrix has smaller quantization errors than the 8-bit quantization of the preconditioner. Our evaluation is currently limited to image classification and natural language modeling tasks. Due to limitations in computing resources, we do not test our 4-bit Shampoo on large-scale models with billions of parameters. ", "page_idx": 8}, {"type": "text", "text": "Broader Impact. Our work can facilitate training large models with second-order optimizers. This could open up new research possibilities that were previously unattainable due to GPU memory constraints, especially benefiting researchers with limited resources. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Jia Li and Hua Huang were supported by the NSF of China (grant no. 62131003). Jia Li was also supported by the NSF of China (grant no. 62102034). Pan Zhou was supported by the Singapore Ministry of Education (MOE) Academic Research Fund (AcRF) Tier 1 grants (project ID: 23-SISSMU-028 and 23-SIS-SMU-070). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Naman Agarwal, Rohan Anil, Elad Hazan, Tomer Koren, and Cyril Zhang. Disentangling adaptive gradient methods from learning rates. arXiv preprint arXiv:2002.11803, 2020.   \n[2] Rohan Anil, Vineet Gupta, Tomer Koren, Kevin Regan, and Yoram Singer. Scalable second order optimization for deep learning. arXiv preprint arXiv:2002.09018, 2020.   \n[3] Rohan Anil, Vineet Gupta, Tomer Koren, and Yoram Singer. Memory efficient adaptive optimization. Advances in Neural Information Processing Systems, 32, 2019.   \n[4] \u00c5. Bj\u00f6rck and C. Bowie. An iterative algorithm for computing the best estimate of an orthogonal matrix. SIAM Journal on Numerical Analysis, 8(2):358\u2013364, 1971.   \n[5] R.L. Burden, J.D. Faires, and A.M. Burden. Numerical Analysis. Cengage Learning, 2015.   \n[6] Aaron Defazio, Xingyu Yang, Harsh Mehta, Konstantin Mishchenko, Ahmed Khaled, and Ashok Cutkosky. The road less scheduled. arXiv preprint arXiv:2405.15682, 2024.   \n[7] Tim Dettmers. 8-bit approximations for parallelism in deep learning. In Proceedings of the International Conference on Learning Representations, 2016.   \n[8] Tim Dettmers, Mike Lewis, Sam Shleifer, and Luke Zettlemoyer. 8-bit optimizers via blockwise quantization. In Proceedings of the International Conference on Learning Representations, 2022.   \n[9] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. QLoRA: Efficient finetuning of quantized LLMs. Advances in Neural Information Processing Systems, 2023.   \n[10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In Proceedings of the International Conference on Learning Representations, 2021.   \n[11] Timothy Dozat. Incorporating Nesterov momentum into Adam. In Proceedings of the International Conference on Learning Representations Workshop, 2016.   \n[12] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(61):2121\u20132159, 2011.   \n[13] Sai Surya Duvvuri, Fnu Devvrit, Rohan Anil, Cho-Jui Hsieh, and Inderjit S Dhillon. Combining axes preconditioners through Kronecker approximation for deep learning. In Proceedings of the International Conference on Learning Representations, 2024.   \n[14] Vladimir Feinberg, Xinyi Chen, Y. Jennifer Sun, Rohan Anil, and Elad Hazan. Sketchy: Memory-efficient adaptive regularization with frequent directions. Advances in Neural Information Processing Systems, 2023.   \n[15] Elias Frantar, Eldar Kurtic, and Dan Alistarh. M-FAC: Efficient matrix-free approximations of second-order information. Advances in Neural Information Processing Systems, 2021.   \n[16] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, and Ruoming Pang. Conformer: Convolutionaugmented transformer for speech recognition. In Proceedings of the Conference of the International Speech Communication Association, 2020.   \n[17] Chun-Hua Guo and Nicholas J. Higham. A Schur\u2013Newton method for the matrix pth root and its inverse. SIAM Journal on Matrix Analysis and Applications, 28(3):788\u2013804, 2006.   \n[18] Vineet Gupta, Tomer Koren, and Yoram Singer. Shampoo: Preconditioned stochastic tensor optimization. In Proceedings of the International Conference on Machine Learning, 2018.   \n[19] N. Halko, P. G. Martinsson, and J. A. Tropp. Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions. SIAM Review, 53(2):217\u2013288, 2011.   \n[20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, June 2016.   \n[21] Roger A. Horn and Charles R. Johnson. Matrix Analysis. Cambridge university press, 2012.   \n[22] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings of the International Conference on Learning Representations, 2015.   \n[23] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.   \n[24] Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge. CS 231N, 7(7):3, 2015.   \n[25] Seung Hoon Lee, Seunghyun Lee, and Byung Cheol Song. Vision transformer for small-size datasets. arXiv preprint arXiv:2112.13492, 2021.   \n[26] Bingrui Li, Jianfei Chen, and Jun Zhu. Memory efficient optimizers with 4-bit states. Advances in Neural Information Processing Systems, 2023.   \n[27] Hong Liu, Zhiyuan Li, David Leo Wright Hall, Percy Liang, and Tengyu Ma. Sophia: A scalable stochastic second-order optimizer for language model pre-training. In Proceedings of the International Conference on Learning Representations, 2024.   \n[28] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision, October 2021.   \n[29] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In Proceedings of the International Conference on Learning Representations, 2019.   \n[30] James Martens and Roger Grosse. Optimizing neural networks with Kronecker-factored approximate curvature. In Proceedings of the International Conference on Machine Learning, 2015.   \n[31] Ning Qian. On the momentum term in gradient descent learning algorithms. Neural Networks, 12(1):145\u2013151, 1999.   \n[32] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, and others. Language models are unsupervised multitask learners. OpenAI blog, 2019.   \n[33] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1\u201367, 2020.   \n[34] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael S. Bernstein, Alexander C. Berg, and Li FeiFei. ImageNet large scale visual recognition challenge. International Journal of Computer Vision, 115(3):211\u2013252, 2015.   \n[35] Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost. In Proceedings of the International Conference on Machine Learning, 2018.   \n[36] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In Proceedings of the International Conference on Learning Representations, 2015.   \n[37] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, and others. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \n[38] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural Information Processing Systems, 30, 2017.   \n[39] Zhewei Yao, Amir Gholami, Sheng Shen, Mustafa Mustafa, Kurt Keutzer, and Michael W. Mahoney. AdaHessian: an adaptive second order optimizer for machine learning. In Proceedings of the AAAI Conference on Artificial Intelligence, 2021.   \n[40] Jui-Nan Yen, Sai Surya Duvvuri, Inderjit S. Dhillon, and Cho-Jui Hsieh. Block low-rank preconditioner with shared basis for stochastic optimization. Advances in Neural Information Processing Systems, 2023.   \n[41] Hongwei Yong, Ying Sun, and Lei Zhang. A general regret bound of preconditioned gradient method for DNN training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, June 2023.   \n[42] Lin Zhang, Shaohuai Shi, and Bo Li. Eva: Practical second-order optimization with Kroneckervectorized approximation. In Proceedings of the International Conference on Learning Representations, 2023.   \n[43] Jiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima Anandkumar, and Yuandong Tian. GaLore: Memory-efficient LLM training by gradient low-rank projection. In Proceedings of the International Conference on Machine Learning, 2024.   \n[44] Pan Zhou, Xingyu Xie, and Shuicheng Yan. Win: Weight-decay-integrated Nesterov acceleration for adaptive gradient algorithms. In Proceedings of the International Conference on Learning Representations, 2023. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Implementation Details of Shampoo, CASPR, K-FAC and AdaBK ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "The implementation of 32-bit Shampoo used in our experiments is described in Algorithm 4. Our Pytorch implementation of Shampoo is partially based on the code provided by [2]. We implement CASPR by replacing $\\widehat{G}_{t}=\\widehat{L}_{t}\\widehat{G}_{t}\\widehat{R}_{t}$ with $J_{t}\\overset{\\cdot}{=}\\widehat{L}_{t}G_{t}+G_{t}\\widehat{R}_{t};\\widehat{G}_{t}=\\widehat{L}_{t}J_{t}\\overset{\\cdot}{+}J_{t}\\widehat{R}_{t}$ in line 12 of Algorithm 4 and line 14 of Algorith m 3. We summarize the im plementation of 32-bit  K-FAC/AdaBK in Algorithm 5, where $X_{t}$ is the input feature and $\\mathbf{Y}_{t}$ is the output feature gradient. Both power iteration [5] and Schur-Newton iteration [17] are run for 10 iterations. Our implementation of 4-bit K-FAC/AdaBK is similar to 4-bit Shampoo (i.e., compressing $\\mathbf{{L}}_{t},R_{t},\\widehat{L}_{t}$ , and $\\mathbf{\\widehat{R}}_{t}$ ). ", "page_idx": 12}, {"type": "text", "text": "Algorithm 4 Practical 32-bit Shampoo ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Input: initial parameter $W_{0}\\in\\mathbb{R}^{m\\times n}$ , left preconditioner $\\pmb{L}_{0}=\\epsilon\\pmb{I}_{m}$ , right preconditioner $R_{0}={}$ $\\epsilon I_{n}$ , inverse root of left preconditioner $\\hat{\\pmb{L}}_{0}=\\pmb{I}_{m}$ , inverse root of right preconditioner $\\hat{R}_{0}=I_{n}$ , total number of steps $T$ , interval of up dating preconditioners $T_{1}$ , interval of updatin g inverse roots of preconditioners $T_{2}$ , exponential decay rate for preconditioners $\\beta\\in(\\bar{0},1)$ , first-order optimizer $\\mathcal{F}$ , first-order optimizer state $s_{0}=\\mathbf{0}$ . ", "page_idx": 12}, {"type": "text", "text": "Output: final parameter $W_{T}$ .   \n1: for $t=1,2,\\ldots,T$ do   \n2: Receive loss function $\\mathcal{L}_{t}:\\mathbb{R}^{m\\times n}\\mapsto\\mathbb{R}$ and compute gradient $G_{t}=\\nabla{\\mathcal{L}}_{t}(W_{t})$   \n3: if $t\\%T_{1}\\equiv0$ then   \n4: $\\dot{L_{t}}\\dot{=\\beta}\\dot{L_{t-1}}+(1-\\beta)G_{t}G_{t}^{\\top};\\quad R_{t}=\\beta R_{t-1}+(1-\\beta)G_{t}^{\\top}G_{t}$   \n5: else   \n6: $L_{t}=L_{t-1};\\quad R_{t}=R_{t-1}$   \n7: if $t\\%T_{2}\\equiv0$ then   \n8: Compute maximum eigenvalues $\\lambda_{\\mathrm{max}}^{L}$ and $\\lambda_{\\operatorname*{max}}^{R}$ of $\\pmb{L}_{t}$ and $\\textstyle R_{t}$ by power iteration   \n9: Compute $\\widehat{L}_{t}=(L_{t}+\\lambda_{\\operatorname*{max}}^{L}\\epsilon I_{m})^{-1/4}$ and $\\widehat{R}_{t}=(R_{t}+\\lambda_{\\operatorname*{max}}^{R}\\epsilon I_{n})^{-1/4}$ by Schur-Newton   \niteration   \n10 ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\overset{\\iff\\sum_{t=1}}{\\widehat{L}}_{t}=\\widehat{L}_{t-1};\\quad\\widehat{R}_{t}=\\widehat{R}_{t-1}}\\\\ &{\\widehat{G}_{t}=\\widehat{L}_{t}G_{t}\\widehat{R}_{t};\\quad\\widetilde{G}_{t}=\\widehat{G}_{t}\\big(\\|G_{t}\\|_{F}/\\|\\widehat{G}_{t}\\|_{F}\\big)}\\\\ &{W_{t},s_{t}=\\mathcal{F}(W_{t-1},s_{t-1},\\widetilde{G}_{t})}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Algorithm 5 Practical 32-bit K-FAC/AdaBK ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Input: initial parameter $W_{0}\\in\\mathbb{R}^{m\\times n}$ , left preconditioner $\\scriptstyle L_{0}\\;=\\;{\\bf0}$ , right preconditioner $\\mathbf{R}_{0}=\\mathbf{0}$ , inverse root of left preconditioner $\\widehat{L}_{0}=\\mathbf{\\overset{}{I}}_{m}$ , inverse root of right preconditioner $\\widehat{\\pmb{R}}_{0}=\\pmb{I}_{n}$ , total number of steps $T$ , interval of updating preconditioners $T_{1}$ , interval of updating inverse roots of preconditioners $T_{2}$ , $\\epsilon$ , exponential decay rate for preconditioners $\\beta\\in(0,1)$ , $\\alpha=1$ for K-FAC / $\\alpha=2$ for AdaBK, first-order optimizer $\\mathcal{F}$ , first-order optimizer state $s_{0}=\\mathbf{0}$ .   \nOutput: final parameter $W_{T}$ .   \n1: for $t=1,2,\\ldots,T$ do   \n2: Receive loss function $\\mathcal{L}_{t}:\\mathbb{R}^{m\\times n}\\mapsto\\mathbb{R}$ and compute gradient $G_{t}=\\nabla{\\mathcal{L}}_{t}(W_{t})$   \n3: Receive $X_{t}$ by forward propagation and $\\mathbf{Y}_{t}$ by backward propagation   \n4: if $t\\%T_{1}\\equiv0$ then   \n5: $\\dot{L_{t}}\\dot{=}\\beta\\dot{L}_{t-1}+(1-\\beta)Y_{t}Y_{t}^{\\top};\\quad R_{t}=\\beta R_{t-1}+(1-\\beta)X_{t}X_{t}^{\\top}$   \n6: else   \n7: $L_{t}=L_{t-1};\\quad R_{t}=R_{t-1}$   \n8: if $t\\%T_{2}\\equiv0$ then   \n9: Compute maximum eigenvalues $\\lambda_{\\mathrm{max}}^{L}$ and $\\lambda_{\\operatorname*{max}}^{R}$ of $\\pmb{L}_{t}$ and $\\textstyle R_{t}$ by power iteration   \n10: Compute $\\widehat{L}_{t}=(L_{t}+\\lambda_{\\operatorname*{max}}^{L}\\epsilon I_{m})^{-1/\\alpha}$ and $\\widehat{R}_{t}=(R_{t}+\\lambda_{\\operatorname*{max}}^{R}\\epsilon I_{n})^{-1/\\alpha}$ by Schur-Newton iteration   \n11: else   \n12: $\\begin{array}{r l}&{\\overset{\\rightharpoonup}{\\hat{L}}_{t}=\\widehat{L}_{t-1};\\quad\\widehat{R}_{t}=\\widehat{R}_{t-1}}\\\\ &{\\widehat{G}_{t}=\\widehat{L}_{t}G_{t}\\widehat{R}_{t};\\quad\\widetilde{G}_{t}=\\widehat{G}_{t}\\big(\\|G_{t}\\|_{F}/\\|\\widehat{G}_{t}\\|_{F}\\big)}\\\\ &{W_{t},s_{t}=\\mathcal{F}(W_{t-1},s_{t-1},\\widetilde{G}_{t})}\\end{array}$   \n13: ", "page_idx": 12}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 12}, {"type": "image", "img_path": "ASqdVeifn7/tmp/5dd08871b0b4e94c159aa8838f725a30c77f8f32f443882b08e28c8d62c47353.jpg", "img_caption": ["Figure 5: Visualization of DT quantization and Linear-2 quantization at $b$ -bit $(b=3,4)$ ) precision. "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "B Randomized SVD Method ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Given an initial matrix $P_{0}\\in\\mathbb{R}^{n\\times n}$ , randomized SVD method computes the eigenvector matrix of a PD matrix $A\\in\\mathbb{R}^{n\\times n}$ by iterating ", "page_idx": 13}, {"type": "equation", "text": "$$\nP_{t}=\\mathsf{Q R}(A P_{t-1}),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\operatorname{QR}(X)$ denotes the QR decomposition of matrix $\\mathbf{\\deltaX}$ , returning an orthogonal matrix. Since we can initialize $P_{0}$ with the previous result (e.g., $V$ in Algorithm 1), only a few iterations are enough to obtain an accurate estimation in practice. In our experiments, we iterate (4) once for Shampoo/CASPR, and iterate (4) twice for K-FAC/AdaBK. ", "page_idx": 13}, {"type": "text", "text": "C Quantization Mappings ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We present the constructions of different quantization mappings in $b$ -bit quantizers ( $\\mathcal{R}$ in $\\mathcal{Q}$ ). See Figure 5 for the illustration of them. Note that $\\mathbb{T}_{b}\\!=\\!\\{0,1,\\!\\dots,2^{\\overline{{b}}}\\!-\\!1\\}$ . ", "page_idx": 13}, {"type": "text", "text": "Dynamic tree (DT) quantization for $b$ -bit quantization maps $\\mathbb{T}_{b}$ onto $\\{0,1\\}\\cup G$ , where $G$ is a set of numbers with the following properties: the number in $G$ looks like $\\pm q_{k}\\times10^{-E}$ , where a) $b=2+E+F$ , where $E,F$ are integers; b) $q_{k}\\,=\\,(p_{k}+p_{k+1})/2$ , where $\\bar{k^{\\bigstar}}\\in\\{0,\\ldots,2^{F}-1\\}$ ; c) $p_{j}=0.9j/2^{F}+0.1$ , where $j\\in\\{0,\\bar{\\dots},2^{F}\\}$ . For 4-bit quantization, DT quantization maps $\\mathbb{T}_{4}$ onto {-0.8875, -0.6625, -0.4375, -0.2125, -0.0775, -0.0325, -0.0055, 0.0000, 0.0055, 0.0325, 0.0775, 0.2125, 0.4375, 0.6625, 0.8875, 1.0000}. For 3-bit quantization, DT quantization maps $\\mathbb{T}_{3}$ onto {-0.7750, -0.3250, -0.0550, 0.0000, 0.0550, 0.3250, 0.7750, 1.0000}. ", "page_idx": 13}, {"type": "text", "text": "For 4-bit quantization, linear square (Linear-2) quantization maps $\\mathbb{T}_{4}$ onto {-1.0000, -0.7511, -0.5378, -0.3600, -0.2178, -0.1111, -0.0400, 0.0000, 0.0044, 0.0400, 0.1111, 0.2178, 0.3600, 0.5378, 0.7511, 1.0000}. For 3-bit quantization, Linear-2 quantization maps $\\mathbb{T}_{3}$ onto {-1.0000, -0.5102, -0.1837, 0.0000, 0.0204, 0.1837, 0.5102, 1.0000}. ", "page_idx": 13}, {"type": "text", "text": "D Quantization Error Analyses ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We present more quantization error analyses of the preconditioners. Recall that we define two kinds of quantization errors in mapping $f$ of transformation $g$ at $A\\in\\mathbb{R}^{m\\times n}$ (in short errors in $f(A)$ of $g$ ) in Subsection 3.1. Here we extend them as follows: define the normwise relative error (NRE) in $f$ of $(g_{1},g_{2})$ at $\\pmb{A}$ as ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathrm{NRE}=\\frac{\\|f(\\pmb{A})-g_{2}\\circ f\\circ g_{1}(\\pmb{A})\\|_{F}}{\\|f(\\pmb{A})\\|_{F}},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "and the angle error (AE) in $f$ of $(g_{1},g_{2})$ at $\\pmb{A}$ as ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname{AE}=\\operatorname{arccos}\\left({\\frac{\\langle f(\\pmb{A}),g_{2}\\circ f\\circ g_{1}(\\pmb{A})\\rangle}{\\|f(\\pmb{A})\\|_{F}\\|g_{2}\\circ f\\circ g_{1}(\\pmb{A})\\|_{F}}}\\right).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "D.1 Static Analysis ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Table 5 is an extension of Table 1 for $\\mathrm{Bit=}4$ . Since the diagonal elements of $A^{-1/4}$ are usually much larger than its non-diagonal elements where $\\pmb{A}$ is a PD matrix, we further consider the quantization errors in $f(A)\\!=\\!A^{-1/4}\\!-\\!\\operatorname{Diag}(\\operatorname{diag}(A^{-1/4}))$ at 4-bit precision as shown in Table 6. Table 7 shows the quantization errors at 8-bit precision. ", "page_idx": 14}, {"type": "text", "text": "A large condition number of a PD matrix $\\pmb{A}$ is indispensable for the superiority of quantizing $U$ over quantizing $\\pmb{A}$ , where $U$ is the eigenvector matrix of $\\pmb{A}$ . We consider contracting the singular value distribution of $A=A_{1}$ with SVD $U\\mathrm{Diag}(\\lambda)U^{\\top}$ used in Table 5 by mapping each singular value $\\lambda$ of $\\pmb{A}$ to $h(\\lambda)=\\tau(\\lambda-\\lambda_{\\operatorname*{min}}^{A})+\\lambda_{\\operatorname*{min}}^{A}$ , where $\\lambda_{\\operatorname*{min}}^{A}$ is the minimum singular value of $\\pmb{A}$ and $\\tau\\,>\\,0$ is the contraction coefficient. Figure 6 shows 4-bit quantization errors in $A^{-1/4}$ or $\\pmb{A}^{-1/4}\\!-\\!\\mathrm{Diag}(\\mathrm{diag}(\\pmb{A}^{-1/4}))$ of quantizing $U$ or $\\pmb{A}$ at $A\\!=\\!U\\mathrm{Diag}(h(\\lambda))U^{\\top}$ . ", "page_idx": 14}, {"type": "table", "img_path": "ASqdVeifn7/tmp/d13696c2eabf6ebac77bbf73b1099b478006d7b4cde302b2bd3cde00e1da5cbc.jpg", "table_caption": ["Table 5: Quantization errors in $f(\\pmb{A})=\\pmb{A}^{-1/4}$ of different 4-bit quantization schemes at a PD matrix $\\pmb{A}$ . We employ block-wise normalization with a block size of 64. $U$ is the eigenvector matrix of $\\pmb{A}$ and $B=(\\bar{g_{1}}(\\dot{A}))^{-1/4}$ . $\\mathrm{{QM}=}$ quantized matrices and $\\mathrm{OR}=$ orthogonal rectification. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "Table 6: Quantization errors in $f(A)\\!=\\!A^{-1/4}\\!-\\!\\operatorname{Diag}(\\operatorname{diag}(A^{-1/4}))$ of different 4-bit quantization schemes at a PD matrix $\\pmb{A}$ . We employ block-wise normalization with a block size of 64. $U$ is the eigenvector matrix of $\\pmb{A}$ and $B=(g_{1}(A))^{-1/4}$ . ${\\sf Q M}=$ quantized matrices and $\\mathrm{OR}=$ orthogonal rectification. ", "page_idx": 14}, {"type": "table", "img_path": "ASqdVeifn7/tmp/08a4299665f2d7f849780365ff9acf16701883e180b6351542c727ebbac6f6e5.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "ASqdVeifn7/tmp/07f500874ea08627d3a9aa293695d009c9e1c3a2602dde783181fdaea2f84176.jpg", "img_caption": ["Figure 6: 4-bit quantization errors in $f(A)$ of quantizing $U$ or $\\pmb{A}$ at $\\pmb{A}=\\pmb{U}\\mathrm{Diag}(h(\\pmb{\\lambda}))\\pmb{U}^{\\top}$ . We use linear square quantization and orthogonal rectification. The condition number $\\operatorname{cond}(A)\\,=$ $\\lambda_{\\operatorname*{max}}^{A}/\\lambda_{\\operatorname*{min}}^{A}$ is around 37235, where \u03bbAmax and \u03bbAmin are the maximum and minimum singular values of $\\pmb{A}$ respectively. Contraction coefficients are shown on a $\\mathrm{log_{2}}$ scale. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Table 7: Quantization errors in $f(A)$ of different 8-bit quantization schemes at a PD matrix $\\pmb{A}$ , where $A=A_{1}$ is derived from the real world as described in Subsection 3.1. We employ block-wise normalization with a block size of 256. $U$ is the eigenvector matrix of $\\pmb{A}$ and $B\\stackrel{\\cdot}{=}(g_{1}(A))^{-1/4}$ . $\\mathrm{QM}=$ quantized matrices and $\\mathrm{OR}=$ orthogonal rectification. ", "page_idx": 15}, {"type": "table", "img_path": "ASqdVeifn7/tmp/e6cc32ab94f2c2154b091cb5b7a5de2ee7b0a839f3d5416fda5e19b2e8311055.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "D.2 Dynamic Analysis ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We define the normwise relative error (NRE) and angle error (AE) of $_B$ deviating from $\\pmb{A}$ as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathrm{NRE}\\!=\\!\\frac{\\|\\boldsymbol{B}-\\boldsymbol{A}\\|_{F}}{\\|\\boldsymbol{A}\\|_{F}},\\quad\\mathrm{AE\\!=\\!arccos}\\left(\\frac{\\langle\\boldsymbol{A},\\boldsymbol{B}\\rangle}{\\|\\boldsymbol{A}\\|_{F}\\|\\boldsymbol{B}\\|_{F}}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Consider Shampoo using 4-bit preconditioners for parameter updates, but also recording 32-bit preconditioners at the same time. We extract the left preconditioners $L_{4}$ and $\\pmb{L}_{32}\\in\\mathbb{R}^{1200\\times1200}$ of a specific model parameter block $W\\in\\mathbb{R}^{1200\\times768}$ every 8000 steps in the Swin-Tiny training on CIFAR-100 with AdamW $\\dot{\\mathbf{\\zeta}}+$ Shampoo. Here $L_{4}$ is a decompressed 4-bit preconditioner, and $\\pmb{L}_{32}$ is a 32-bit preconditioner. ", "page_idx": 15}, {"type": "text", "text": "Figure 7 shows the quantization errors during training. For naive 4-bit Shampoo, $L_{32}^{-1/4}$ 4and L4\u22121/4 are computed by Schur-Newton iteration used in Algorithm 4 where $\\epsilon\\!=\\!10^{-4}$ . For our 4-bit Shampoo, $L_{32}^{-1/4}$ is computed by Schur-Newton iteration used in Algorithm 4 where $\\epsilon\\!=\\!10^{-4}$ , and $L_{4}^{-1/4}$ i computed by Algorithm 2 without quantization where $\\epsilon\\!=\\!10^{-4},t_{2}\\!=\\!4$ . We find that $\\epsilon\\!=\\!10^{\\bar{-6}}$ for Algorithm 2 used in our main experiments though is effective, yet it can cause a large numerical instability in the later stage of training (see Figure 8). ", "page_idx": 15}, {"type": "image", "img_path": "ASqdVeifn7/tmp/b044386a1f5800d8b333d6dd24755ace06b265520e960b9e06b0efa093ff2af9.jpg", "img_caption": ["Figure 7: Quantization errors during Swin-Tiny training on the CIFAR-100 dataset. We use dampening term $\\epsilon=10^{-4}$ to compute $L_{4}^{-1/4}$ /4and L3\u221221/4. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "ASqdVeifn7/tmp/d901aff55fe1d6fdc4058bb7b59c079b9229d76c5fef9a30d2a62404ca0bd350.jpg", "img_caption": ["Figure 8: Quantization errors during Swin-Tiny training on the CIFAR-100 dataset. We use dampening term $\\epsilon=10^{-6}$ to compute $\\mathbf{\\bar{L}}_{4}^{-1/4}$ and L3\u221221 /4 "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "E Convergence Analysis ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "More notations. Given a symmetric real matrix $\\pmb{A}$ , $A\\succeq0$ means that $\\pmb{A}$ is positive semidefinite (PSD), and $A\\succ0$ means that $\\pmb{A}$ is positive definite (PD). Assume that symmetric matrices $\\pmb{A}$ and $_B$ are symmetric, the notations $A\\succeq B$ and $A\\succ B$ mean that $A-B\\succeq0$ and $A-B\\succ0$ respectively. Let $\\pmb{A}$ be a PSD matrix and $s\\in\\mathbb{R}$ , we define $A^{s}\\!=\\!U\\Lambda^{s}U^{\\top}$ , where $U\\mathbf{A}U^{\\mathsf{T}}$ is the Singular Value Decompo\u221asition (SVD) of $\\pmb{A}$ . The Mahalanobis norm of a vector $\\textbf{\\em x}$ induced by a \u221aPD matrix $\\pmb{A}$ is $\\|x\\|_{A}={\\sqrt{x^{\\mathsf{T}}A x}}$ . The dual norm of $\\|\\cdot\\|_{A}$ is denoted by $\\|\\cdot\\|_{A}^{*}$ , where $\\|\\mathbf{x}\\|_{A}^{*}=\\sqrt{\\mathbf{x}^{\\mathsf{T}}A^{-1}x}$ . The spectral norm of matrix $\\pmb{A}$ is $\\|\\pmb{A}\\|_{2}=\\operatorname*{sup}_{\\pmb{x}\\neq\\mathbf{0}}\\{\\|\\pmb{A}\\pmb{x}\\|_{2}/\\|\\pmb{x}\\|_{2}\\}$ . $A\\otimes B$ means the (right) Kronecker product of matrices $\\pmb{A}$ and $_B$ . ${\\overline{{\\operatorname{Vec}}}}(A)$ means the vectorization (stacking the rows) of $\\pmb{A}$ . ", "page_idx": 16}, {"type": "text", "text": "", "text_level": 1, "page_idx": 16}, {"type": "table", "img_path": "ASqdVeifn7/tmp/5510030ec4dcc12cf9a47112ca81ec1e91396a3ef3d4ce9460aed44cc26bfd3e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "We consider quantization as a perturbation and present the perturbed Shampoo in Algorithm 6 for convergence analysis. The regret bound of the perturbed Shampoo can be found in Theorem 1. Complete proofs can be found in Appendix F. We first introduce some basic technical tools, and the details of them are in [18, 21]. ", "page_idx": 16}, {"type": "text", "text": "Lemma 3. Let $A,A^{\\prime},B,B^{\\prime}$ be matrices of appropriate dimensions, and ${\\pmb u},{\\pmb v}$ be two column vectors. The following properties hold: ", "page_idx": 16}, {"type": "equation", "text": "$$\n(A\\otimes B)(A^{\\prime}\\otimes B^{\\prime})=(A A^{\\prime})\\otimes(B B^{\\prime});\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "(2) $(A\\otimes B)^{\\mathsf{T}}=(A^{\\mathsf{T}}\\otimes B^{\\mathsf{T}})$ ;   \n(3) I $c_{A,B}\\succeq0$ and $s\\in\\mathbb{R},$ then $(A\\otimes B)^{s}=(A^{s}\\otimes B^{s})$ ;   \n(4) If $A\\succeq A^{\\prime}$ and $B\\succeq B^{\\prime}$ , then $\\pmb{A}\\otimes\\pmb{B}\\succeq\\pmb{A}^{\\prime}\\otimes\\pmb{B}^{\\prime}$ ;   \n(5) $\\operatorname{tr}(A B)=\\operatorname{tr}(A)\\operatorname{tr}(B)$ ;   \n(6) $\\overline{{\\mathrm{vec}}}(\\boldsymbol{u}\\boldsymbol{v}^{\\top})=\\boldsymbol{u}\\otimes\\boldsymbol{v}$ . ", "page_idx": 17}, {"type": "text", "text": "Lemma 4. Let $\\pmb{G}\\in\\mathbb{R}^{m\\times n},\\pmb{L}\\in\\mathbb{R}^{m\\times m},\\pmb{R}\\in\\mathbb{R}^{n\\times n},$ , then it holds that ", "page_idx": 17}, {"type": "equation", "text": "$$\n(L\\otimes R^{\\mathsf{T}}){\\overline{{\\mathrm{vec}}}}(G)={\\overline{{\\mathrm{vec}}}}(L G R).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Lemma 5. Assume that $0\\preceq X_{i}\\preceq Y_{i}$ for $i=1,\\ldots,n.$ . Assume further that all $X_{i}$ commute with each other and all $\\mathbf{{Y}}_{i}$ commute with each other. Let $\\alpha_{1},\\ldots,\\alpha_{n}\\geq0$ such that $\\textstyle\\sum_{i=1}^{n}\\alpha_{i}=1_{}$ , then ", "page_idx": 17}, {"type": "equation", "text": "$$\nX_{1}^{\\alpha_{1}}\\cdot\\cdot\\cdot X_{n}^{\\alpha_{n}}\\preceq Y_{1}^{\\alpha_{1}}\\cdot\\cdot\\cdot Y_{n}^{\\alpha_{n}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Lemma 6. Let $0\\leq\\alpha\\leq1$ and $0\\preceq X\\preceq Y$ , then $X^{\\alpha}\\preceq Y^{\\alpha}$ . ", "page_idx": 17}, {"type": "text", "text": "Lemma 7. Let $A\\succ0$ and $B\\succ0,$ , then it holds that $A\\succeq B$ if and only if $B^{-1}\\succeq A^{-1}$ ", "page_idx": 17}, {"type": "text", "text": "Lemma 8 (von Neumann). Let $\\pmb{A}$ $\\mathbf{1},B\\in\\mathbb{R}^{m\\times n}$ and $q=\\operatorname*{min}\\{m,n\\}$ . Let $\\sigma_{1}(A)\\geq\\cdot\\cdot\\cdot\\geq\\sigma_{q}(A)$ and $\\sigma_{1}(B)\\geq\\cdot\\cdot\\cdot\\geq\\sigma_{q}(B)$ denote the non-increasingly ordered singular values of $\\pmb{A}$ and $_B$ , respectively. Then ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\langle A,B\\rangle\\leq\\sum_{i=1}^{q}\\sigma_{i}(A)\\sigma_{i}(B).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Lemma 9. Assume that function $f_{t}$ is continuously differentiable and convex on $\\mathbb{R}^{d}$ , and matrix $H_{t}~\\succ~0$ for $t\\ =\\ 1,\\ldots,T$ . Given $\\pmb{w}_{0}\\;\\in\\;\\mathbb{R}^{d},\\eta\\;>\\;\\tilde{0}$ , define $\\pmb{w}_{t+1}\\;=\\;\\pmb{w}_{t}\\;-\\;\\eta\\pmb{H}_{t}^{-1}\\pmb{g}_{t}$ , where $\\pmb{g}_{t}=\\nabla f_{t}(\\pmb{w}_{t})$ . Then for any $\\pmb{w}^{*}\\in\\mathbb{R}^{d}$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}f_{t}(w_{t})-\\sum_{t=1}^{T}f_{t}(w^{*})\\leq\\frac{1}{2\\eta}\\sum_{t=1}^{T}(\\|w_{t}-w^{*}\\|_{H_{t}}^{2}-\\|w_{t+1}-w^{*}\\|_{H_{t}}^{2})+\\frac{\\eta}{2}\\sum_{t=1}^{T}(\\|g_{t}\\|_{H_{t}}^{*})^{2}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Lemma 10. Let $g_{1},\\dots,g_{T}$ be a sequence of vectors. For $\\rho>0$ , define $\\widehat{\\pmb{H}}_{t}=(\\rho\\pmb{I}+\\sum_{s=1}^{t}\\pmb{g}_{s}\\pmb{g}_{s}^{\\top})^{1/2}$ . Then we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}(\\|\\pmb{g}_{t}\\|_{\\widehat{\\pmb{H}_{t}}}^{*})^{2}\\leq2\\mathrm{tr}(\\widehat{\\pmb{H}_{T}}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Lemma 11. Assume that $G_{1},\\hdots,G_{T}\\in\\mathbb{R}^{m\\times n}$ are matrices of rank at most r. Let s for $t=1,\\dots,T$ . Then for any $\\epsilon\\geq0$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\epsilon I_{m n}+\\frac{1}{r}\\sum_{t=1}^{T}g_{t}g_{t}^{\\top}\\preceq(\\epsilon I_{m}+\\sum_{t=1}^{T}G_{t}G_{t}^{\\top})^{1/2}\\otimes(\\epsilon I_{n}+\\sum_{t=1}^{T}G_{t}^{\\top}G_{t})^{1/2}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The key to the convergence proof of Algorithm 6 is forming a PD matrix sequence $\\{H_{i}\\}_{i=1}^{T}$ , which satisfies $0\\prec H_{1}\\preceq\\cdots\\preceq H_{T}$ . To achieve it, we gives the following lemma extended from Lemma 2 in the Appendix of [40]. ", "page_idx": 17}, {"type": "text", "text": "Lemma 12. Let $\\{X_{t}\\}_{t=1}^{t=T}$ be a sequence of symmetric matrices, and $\\begin{array}{r}{A_{t}\\;=\\;\\sum_{s=1}^{t}X_{s}}\\end{array}$ , where $t=1,\\ldots,T$ . Suppose we have two sequences of symmetric matrices $\\{Y_{t}\\}_{t=1}^{t=T},\\{Z_{t}\\}_{t=0}^{t=T}$ , and $a$ sequence real numbers $\\{\\rho_{t}\\}_{t=0}^{t=T}$ satisfying ", "page_idx": 17}, {"type": "equation", "text": "$$\nY_{t}=Z_{t-1}+X_{t},\\quad\\rho_{t}=\\rho_{t-1}+\\|Y_{t}-Z_{t}\\|_{2},\\quad Z_{0}=\\mathbf{0},\\rho_{0}=0.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Define $\\boldsymbol{B}_{t}=\\rho_{t}\\boldsymbol{I}+\\boldsymbol{Z}_{t}$ , where $\\boldsymbol{\\mathit{I}}$ denotes the identity matrix. Then for $t=1,\\dots,T$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\nB_{t}\\succeq B_{t-1}+X_{t},\\quad A_{t}\\preceq B_{t}\\preceq2\\rho_{t}I+A_{t}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Theorem 1. Assume that the gradients $G_{1},\\hdots,G_{T}\\in\\mathbb{R}^{m\\times n}$ are matrices of rank at most $r$ . Then for any $W^{\\ast}\\in\\mathbb{R}^{m\\times n}$ and $\\epsilon>0$ , if $\\dot{\\boldsymbol\\eta}={\\boldsymbol D}/\\sqrt{2{\\boldsymbol r}}$ , the regret of Algorithm $6$ is bounded as follows, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}f_{t}(W_{t})-\\sum_{t=1}^{T}f_{t}(W^{*})\\leq\\sqrt{2r}D[2^{1/4}m\\rho_{T}^{1/4}+\\mathrm{tr}(\\tilde{L}_{T}^{1/4})][2^{1/4}n\\mu_{T}^{1/4}+\\mathrm{tr}(\\tilde{R}_{T}^{1/4})],\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\begin{array}{r}{D=\\operatorname*{max}_{t\\in[T]}\\|W_{t}-W^{*}\\|_{F},\\tilde{L}_{t}=\\epsilon I_{m}+\\sum_{t=1}^{T}G_{t}G_{t}^{\\top}}\\end{array}$ , and $\\begin{array}{r}{\\tilde{R}_{t}=\\epsilon I_{n}+\\sum_{t=1}^{T}{G}_{t}^{\\top}{G}_{t}.}\\end{array}$ . ", "page_idx": 17}, {"type": "text", "text": "Though we get a convergence guarantee of Algorithm 6, the upper bound given by Theorem 1 is very slack, since 21/4m\u03c11T/4 is about the same as $\\mathrm{tr}(\\tilde{L}_{T}^{1/4})$ for 4-bit quantization schemes in practice. ", "page_idx": 17}, {"type": "text", "text": "F Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Lemma 1. Let $\\pmb{A}$ be a PD matrix whose SVD is $U\\mathbf{A}U^{\\mathsf{T}}$ , where $U\\!=\\![u_{i}]$ is an orthogonal matrix and $\\mathbf{A}{=}\\mathrm{diag}([\\lambda_{i}]^{\\mathsf{T}})$ is a diagonal matrix. Given a perturbation $\\Delta U\\!=\\![\\Delta u_{i}]$ and $s\\in\\mathbb{R}$ , we define $\\pmb{B}\\!:=\\!(\\pmb{U}\\pmb{\\Lambda}\\pmb{U}^{\\top})^{s}$ and $\\Delta B\\!:=\\!((U\\!+\\!\\Delta U)\\Lambda(U\\!+\\!\\Delta U)^{\\top})^{s}\\!-\\!B.$ . ", "page_idx": 18}, {"type": "text", "text": "$(I)$ If $U\\!+\\!\\Delta U$ is orthogonal and there exists $\\alpha\\in\\mathbb{R}$ such that $\\|\\Delta\\pmb{u}_{i}\\|_{2}\\leq\\alpha$ , then ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{\\|\\Delta B\\|_{F}}{\\|B\\|_{F}}\\le2\\alpha.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "(2) If $U\\!+\\!\\Delta U$ is orthogonal and there exists $\\beta\\in\\mathbb{R}$ such that $\\langle{\\pmb u}_{i},{\\pmb u}_{i}\\!+\\!\\Delta{\\pmb u}_{i}\\rangle\\geq1\\!-\\!\\beta\\geq0,$ , then ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{\\langle B,B+\\Delta B\\rangle}{\\|B\\|_{F}\\|B+\\Delta B\\|_{F}}\\geq(1\\!-\\!\\beta)^{2}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. (1) Since $U$ and $U+\\Delta U$ are orthogonal, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\nB=U\\Lambda^{s}U^{\\top},\\quad B+\\Delta B=(U+\\Delta U)\\Lambda^{s}(U+\\Delta U)^{\\top},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "by definition. This leads to ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\Delta\\pmb{B}=\\pmb{U}\\pmb{\\Lambda}^{s}\\Delta\\pmb{U}^{\\top}+\\Delta\\pmb{U}\\pmb{\\Lambda}^{s}(\\pmb{U}+\\Delta\\pmb{U})^{\\top}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The Frobenius norm satisfies the triangle inequality and is orthogonality invariant. Hence, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\Delta B\\|_{F}=\\|U\\Lambda^{s}\\Delta U^{\\top}+\\Delta U\\Lambda^{s}(U+\\Delta U)^{\\top}\\|_{F}}\\\\ &{\\qquad\\qquad\\leq\\|U\\Lambda^{s}\\Delta U^{\\top}\\|_{F}+\\|\\Delta U\\Lambda^{s}(U+\\Delta U)^{\\top}\\|_{F}}\\\\ &{\\qquad\\qquad=\\|\\Lambda^{s}\\Delta U^{\\top}\\|_{F}+\\|\\Delta U\\Lambda^{s}\\|_{F}=2\\|\\Delta U\\Lambda^{s}\\|_{F}}\\\\ &{\\qquad\\qquad=2\\sqrt{\\displaystyle\\sum_{i}\\|\\lambda_{i}^{s}\\Delta u_{i}\\|_{2}^{2}}=2\\sqrt{\\displaystyle\\sum_{i}\\lambda_{i}^{2s}\\|\\Delta u_{i}\\|_{2}^{2}}}\\\\ &{\\qquad\\qquad\\leq2\\sqrt{\\displaystyle\\sum_{i}\\lambda_{i}^{2s}\\alpha^{2}}=2\\alpha\\sqrt{\\displaystyle\\sum_{i}\\lambda_{i}^{2s}}=2\\alpha\\|\\Lambda^{s}\\|_{F}}\\\\ &{\\qquad=2\\alpha\\|B\\|_{F}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "(2) Similar to (1), we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\Delta B=U\\mathbf{A}^{s}\\Delta U^{\\mathsf{T}}+\\Delta U\\mathbf{A}^{s}U^{\\mathsf{T}}+\\Delta U\\mathbf{A}^{s}\\Delta U^{\\mathsf{T}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "From $\\langle{\\pmb u}_{i},{\\pmb u}_{i}+\\Delta{\\pmb u}_{i}\\rangle\\geq1-\\beta\\geq0$ , we get $0\\geq\\left<\\pmb{u}_{i},\\Delta\\pmb{u}_{i}\\right>\\geq-\\beta\\geq-1$ because ", "page_idx": 18}, {"type": "equation", "text": "$$\n1=\\|\\pmb{u}_{i}\\|_{2}\\|\\pmb{u}_{i}+\\Delta\\pmb{u}_{i}\\|_{2}\\geq\\langle\\pmb{u}_{i},\\pmb{u}_{i}+\\Delta\\pmb{u}_{i}\\rangle=1+\\langle\\pmb{u}_{i},\\Delta\\pmb{u}_{i}\\rangle\\geq1-\\beta\\geq0,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "holds due to the orthogonality of $U$ and $U+\\Delta U$ . Hence, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle B,\\Delta B\\rangle=\\mathrm{tr}(2U\\Lambda^{2s}\\Delta U^{\\top}+U\\Lambda^{s}U^{\\top}\\Delta U\\Lambda^{s}\\Delta U^{\\top})}\\\\ &{\\quad\\quad=\\mathrm{tr}\\biggl[\\sum_{i}2\\lambda_{i}^{2s}u_{i}\\Delta u_{i}^{\\top}\\biggr)+\\mathrm{tr}\\biggl[\\biggl(\\sum_{i}\\lambda_{i}^{s}u_{i}u_{i}^{\\top}\\biggr)\\biggl(\\sum_{j}\\lambda_{j}^{s}\\Delta u_{j}\\Delta u_{j}^{\\top}\\biggr)\\biggr]}\\\\ &{\\quad\\quad=\\biggl(\\sum_{i}2\\lambda_{i}^{2s}\\langle u_{i},\\Delta u_{i}\\rangle\\biggr)+\\biggl(\\sum_{i j}\\lambda_{i}^{s}\\lambda_{j}^{s}\\langle u_{i},\\Delta u_{j}\\rangle^{2}\\biggr)}\\\\ &{\\quad\\quad\\geq\\biggl(\\sum_{i}2\\lambda_{i}^{2s}\\langle u_{i},\\Delta u_{i}\\rangle\\biggr)+\\biggl(\\sum_{i}\\lambda_{i}^{2s}\\langle u_{i},\\Delta u_{i}\\rangle^{2}\\biggr)}\\\\ &{\\quad\\quad=\\sum_{i}\\lambda_{i}^{2s}[(1+\\langle u_{i},\\Delta u_{i}\\rangle)^{2}-1]}\\\\ &{\\quad\\quad\\geq\\sum_{i}\\lambda_{i}^{2s}[(1-\\beta)^{2}-1]=[(1-\\beta)^{2}-1]\\|\\Lambda^{s}\\|_{F}^{2}}\\\\ &{\\quad\\quad=[(1-\\beta)^{2}-1]\\|B\\|_{F}^{2}=[(1-\\beta)^{2}-1]\\langle B,B\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Therefore, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{\\langle B,B+\\Delta B\\rangle}{\\|B\\|_{F}\\|B+\\Delta B\\|_{F}}=\\frac{\\langle B,B+\\Delta B\\rangle}{\\langle B,B\\rangle}=1+\\frac{\\langle B,\\Delta B\\rangle}{\\langle B,B\\rangle}\\geq(1-\\beta)^{2}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The proof is completed. ", "page_idx": 18}, {"type": "text", "text": "Lemma 2. Let $\\pmb{A}$ be a PD matrix of order $m{\\mathrel{+{n}}}$ whose SVD is $U\\mathbf{A}U^{\\mathsf{T}}$ , where $m,n\\in\\mathbb{N}_{+}$ , $n=l m,$ , $U\\,=\\,[u_{i}]$ is an orthogonal matrix and $\\mathbf{A}=\\mathrm{diag}([\\lambda_{i}]^{\\mathsf{T}})$ is a diagonal matrix. Assume that $\\pmb{\\Lambda}=$ $\\mathrm{diag}([\\bar{c}\\lambda\\mathbf{\\bar{1}}_{m\\times1}^{\\top},\\lambda\\mathbf{1}_{n\\times1}^{\\top}]^{\\top})$ , $c\\geq1$ , and $\\lambda>0$ . Given a perturbation $\\Delta\\mathbf{A}=\\operatorname{diag}([\\mathbf{0}_{m\\times1}^{\\mathsf{T}},\\Delta\\boldsymbol{\\lambda}_{n\\times1}^{\\mathsf{T}}]^{\\mathsf{T}})$ and $s\\in\\mathbb R$ , we define $\\pmb{B}:=(\\pmb{U}\\pmb{\\Lambda}\\pmb{U}^{\\top})^{s}$ and $\\Delta B\\!:=(U(\\mathbf{A}\\!+\\!\\Delta\\mathbf{A})U^{\\top})^{s}-B$ . ", "page_idx": 19}, {"type": "text", "text": "$(I)$ If $\\Delta\\pmb{\\lambda}_{n\\times1}=(k-1)\\lambda\\mathbf{1}_{n\\times1}$ where $k>0$ , then ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{\\|\\Delta\\pmb{B}\\|_{F}}{\\|\\pmb{B}\\|_{F}}=\\frac{\\sqrt{l}|k^{s}-1|}{\\sqrt{c^{2s}+l}}=h_{1}(s,l).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Moreover, $h_{1}(s,l)$ decreases monotonically with s over $(-\\infty,0)$ and increases monotonically with $l$ over $(0,+\\infty)$ . ", "page_idx": 19}, {"type": "text", "text": "(2) If $\\Delta\\pmb{\\lambda}_{n\\times1}=(t c-1)\\lambda\\mathbf{1}_{n\\times1}$ where $t>0$ , then ", "page_idx": 19}, {"type": "equation", "text": "$$\n{\\frac{\\langle B,B+\\Delta B\\rangle}{\\|B\\|_{F}\\|B+\\Delta B\\|_{F}}}={\\frac{l t^{s}+c^{s}}{\\sqrt{(1+l t^{2s})(l+c^{2s})}}}=h_{2}(l).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Moreover, $h_{2}(l)$ decreases monotonically with $l$ over $(0,(c/t)^{s}]$ and increases monotonically with $l$ over $((\\dot{c}/t)^{s},+\\infty)$ . ", "page_idx": 19}, {"type": "text", "text": "(3) If $\\Delta\\pmb{\\lambda}_{n\\times1}=(t c-1)\\lambda\\mathbf{1}_{n\\times1}$ where $k=t c>0$ and $l=(c/t)^{s}$ , then ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{\\|\\Delta B\\|_{F}}{\\|B\\|_{F}}=\\frac{|k^{s}-1|}{\\sqrt{k^{s}+1}},\\quad\\frac{\\langle B,B+\\Delta B\\rangle}{\\|B\\|_{F}\\|B+\\Delta B\\|_{F}}=\\frac{2}{\\sqrt{2+k^{s}+1/k^{s}}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. (1) Since $U$ is orthogonal, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\|\\Delta B\\|_{F}=\\|(\\mathbf{A}+\\Delta\\mathbf{A})^{s}-\\mathbf{A}^{s}\\|_{F}=\\sqrt{n}|k^{s}-1|\\lambda^{s},\\quad\\|B\\|_{F}=\\|\\mathbf{A}^{s}\\|_{F}=\\sqrt{m c^{2s}+n}\\lambda^{s}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Hence, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{\\|\\Delta B\\|_{F}}{\\|B\\|_{F}}=\\frac{\\sqrt{n}|k^{s}-1|}{\\sqrt{m c^{2s}+n}}=\\frac{\\sqrt{l}|k^{s}-1|}{\\sqrt{c^{2s}+l}}=h_{1}(s,l)\\geq0.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "It is easy to check that $h_{1}$ increases monotonically with $l$ over $(0,+\\infty)$ . To prove $h_{1}$ decreases monotonically with $s$ over $(-\\infty,0)$ , define ", "page_idx": 19}, {"type": "equation", "text": "$$\ng_{1}(s)={\\frac{1}{l}}(h_{1}(s,l))^{2}={\\frac{(k^{s}-1)^{2}}{c^{2s}+l}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Consider the derivative of $g_{1}$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{g_{1}^{\\prime}(s)=\\frac{(c^{2s}+l)2(k^{s}-1)k^{s}\\ln k-(k^{s}-1)^{2}c^{2s}2\\ln c}{(c^{2s}+l)^{2}}}\\\\ &{}&{=\\frac{2(k^{s}-1)\\left((c^{2s}+l)k^{s}\\ln k-(k^{s}-1)c^{2s}\\ln c\\right)}{(c^{2s}+l)^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "If $s<0$ and $k>1$ , then $k^{s}-1<0,k^{s}\\ln k>0$ leading to $g_{1}^{\\prime}(s)<0$ since $c\\geq0$ ; Similarly, if $s<0$ and $0<k\\leq1$ , then $k^{s}-1\\geq0,k^{s}\\ln k\\leq0$ leading to $g_{1}^{\\prime}(s)\\leq0$ . Thus $g_{1}(s)$ is a monotonically decreasing function for $s<0$ , which implies that $h_{1}$ decreases monotonically with $s$ over $(-\\infty,0)$ . (2) Similar to (1), we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\|\\pmb{B}\\|_{F}=\\sqrt{m c^{2s}+n}\\lambda^{s},\\quad\\|\\pmb{B}+\\Delta\\pmb{B}\\|_{F}=\\sqrt{n t^{2s}+m}c^{s}\\lambda^{s}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Besides, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\langle B,B+\\Delta B\\rangle=\\operatorname{tr}(U\\Lambda^{s}(\\Lambda+\\Delta\\Lambda)^{s}U^{\\top})=\\operatorname{tr}(\\Lambda^{s}(\\Lambda+\\Delta\\Lambda)^{s})=(m c^{2s}+n c^{s}t^{s})\\lambda^{2s}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Hence, we get ", "page_idx": 19}, {"type": "equation", "text": "$$\n{\\frac{\\langle B,B+\\Delta B\\rangle}{\\|B\\|_{F}\\|B+\\Delta B\\|_{F}}}={\\frac{n t^{s}+m c^{s}}{\\sqrt{(m+n t^{2s})(n+m c^{2s})}}}={\\frac{l t^{s}+c^{s}}{\\sqrt{(1+l t^{2s})(l+c^{2s})}}}=h_{2}(l)\\geq0.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "To prove $h_{2}$ decreases monotonically with $l$ over $(0,(c/t)^{s}]$ and increases monotonically with $l$ over $((c/t)^{s},+\\infty)$ , we define ", "page_idx": 20}, {"type": "equation", "text": "$$\ng_{2}(l)=(h_{2}(l))^{2}={\\frac{(l t^{s}+c^{s})^{2}}{(1+l t^{2s})(l+c^{2s})}},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "whose monotonicity is equivalent to that of $h_{2}$ for $l>0$ . Consider the derivative of $g_{2}$ ", "page_idx": 20}, {"type": "equation", "text": "$$\ng_{2}^{\\prime}(l)=\\Big(\\frac{t^{2s}l^{2}+2t^{s}c^{s}l+c^{2s}}{t^{2s}l^{2}+l+t^{2s}c^{2s}l+c^{2s}}\\Big)^{\\prime}=\\frac{(t^{s}-t^{2s}c^{s})^{2}l^{2}-(c^{s}-t^{s}c^{2s})^{2}}{(t^{2s}l^{2}+l+t^{2s}c^{2s}l+c^{2s})^{2}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "If $s=0$ or $t c=1$ , then $g_{2}(l)\\equiv1$ . If $s\\neq0$ and $t c\\neq1$ , then $(t^{s}-t^{2s}c^{s})^{2}>0,(c^{s}-t^{s}c^{2s})^{2}>0$ . In this case, let $g_{2}^{\\prime}(l)=\\bar{0}$ , we get ", "page_idx": 20}, {"type": "equation", "text": "$$\nt^{2s}(1-t^{s}c^{s})^{2}l^{2}=c^{2s}(1-t^{s}c^{s})^{2},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which implies that $l=(c/t)^{s}$ . It is easy to see that $g_{2}$ decreases monotonically with $l$ over $(0,(c/t)^{s}]$ and increases monotonically with $l$ over $((c/t)^{s},+\\infty)$ . ", "page_idx": 20}, {"type": "text", "text": "(3) According to (1)(2), we can easily get ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{\\|\\Delta B\\|_{F}}{\\|B\\|_{F}}=\\frac{|k^{s}-1|}{\\sqrt{k^{s}+1}},\\quad\\frac{\\langle B,B+\\Delta B\\rangle}{\\|B\\|_{F}\\|B+\\Delta B\\|_{F}}=\\frac{2}{\\sqrt{2+k^{s}+1/k^{s}}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The proof is completed. ", "page_idx": 20}, {"type": "text", "text": "Proposition 1. Let $\\pmb{A}$ be a PD matrix of order $m\\!+\\!n$ whose SVD is $U\\mathbf{A}U^{\\mathsf{T}}$ , where $m,n\\in\\mathbb{N}_{+}$ , $n=l m$ , $U=[{\\pmb u}_{i}]$ is an orthogonal matrix, $\\mathbf{A}=\\mathrm{diag}([c\\lambda\\mathbf{1}_{m\\times1}^{\\top},\\lambda\\mathbf{1}_{n\\times1}^{\\top}]^{\\top})$ , $c\\ge1000$ , and $\\lambda>0$ . Given $\\Delta U=[\\Delta{\\pmb u}_{i}]$ , $\\Delta\\mathbf{A}=\\operatorname{diag}([\\mathbf{0}_{m\\times1}^{\\mathsf{T}},\\Delta\\boldsymbol{\\lambda}_{n\\times1}^{\\mathsf{T}}]^{\\mathsf{T}})$ , and $s\\leq-0.25$ , we define $B:=(U\\Lambda U^{\\top})^{s}$ , $B_{1}:=((U+\\Delta U)\\Lambda(U\\!+\\!\\Delta U)^{\\top})^{s}$ , and $B_{2}:=(U(\\mathbf{A}+\\Delta\\mathbf{A})U^{\\top})^{s}$ . If $U+\\Delta U$ is orthogonal, $\\|\\Delta\\pmb{u}_{i}\\|_{2}\\leq0.1$ , $\\langle{\\pmb u}_{i},\\Delta{\\pmb u}_{i}\\rangle\\ge-0.005$ , $\\Delta\\pmb{\\lambda}_{n\\times1}\\!=\\!(0.02c\\!-\\!1)\\lambda\\mathbf{1}_{n\\times1}$ , and $l\\!=\\!(c/0.02)^{s}$ , then ", "page_idx": 20}, {"type": "equation", "text": "$$\n2\\frac{\\|B_{1}-B\\|_{F}}{\\|B\\|_{F}}\\!\\le\\!0.4\\le\\frac{\\|B_{2}-B\\|_{F}}{\\|B\\|_{F}},\\quad6\\left(1-\\frac{\\langle B,B_{1}\\rangle}{\\|B\\|_{F}\\|B_{1}\\|_{F}}\\right)\\!\\le\\!0.06\\le\\left(1-\\frac{\\langle B,B_{2}\\rangle}{\\|B\\|_{F}\\|B_{2}\\|_{F}}\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. According to Lemma 1, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n{\\frac{\\|B_{1}-B\\|_{F}}{\\|B\\|_{F}}}\\leq0.2,\\quad{\\frac{\\langle B,B_{1}\\rangle}{\\|B\\|_{F}\\|B_{1}\\|_{F}}}\\geq(1-0.005)^{2}\\geq0.99.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "On the other hand, from Lemma 2(3), we get ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{\\|B_{2}-B\\|_{F}}{\\|B\\|_{F}}=\\frac{|x-1|}{\\sqrt{x+1}}=f_{1}(x),\\quad\\frac{\\langle B,B_{2}\\rangle}{\\|B\\|_{F}\\|B_{2}\\|_{F}}=\\frac{2}{\\sqrt{2+x+1/x}}=f_{2}(x),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $x\\,=\\,(0.02c)^{s}\\,\\in\\,(0,20^{-1/4}]$ . It is easy to verify that $f_{1}$ decreases monotonically and $f_{2}$ increases monotonically for $0<x<1$ . Hence ", "page_idx": 20}, {"type": "equation", "text": "$$\nf_{1}(x)\\geq f_{1}(20^{-1/4})\\geq0.4,\\quad f_{2}(x)\\leq f_{2}(20^{-1/4})\\leq0.94.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The proof is completed. ", "page_idx": 20}, {"type": "text", "text": "Lemma 12. Let $\\{X_{t}\\}_{t=1}^{t=T}$ be a sequence of symmetric matrices, and $\\begin{array}{r}{A_{t}\\;=\\;\\sum_{s=1}^{t}X_{s}}\\end{array}$ , where $t=1,\\ldots,T$ . Suppose we have two sequences of symmetric matrices $\\{Y_{t}\\}_{t=1}^{t=T},\\{Z_{t}\\}_{t=0}^{t=T}$ , and $a$ sequence real numbers $\\{\\rho_{t}\\}_{t=0}^{t=T}$ satisfying ", "page_idx": 20}, {"type": "equation", "text": "$$\nY_{t}=Z_{t-1}+X_{t},\\quad\\rho_{t}=\\rho_{t-1}+\\|Y_{t}-Z_{t}\\|_{2},\\quad Z_{0}=\\mathbf{0},\\rho_{0}=0.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Define $\\boldsymbol{B}_{t}=\\rho_{t}\\boldsymbol{I}+\\boldsymbol{Z}_{t}$ , where $\\boldsymbol{\\mathit{I}}$ denotes the identity matrix. Then for $t=1,\\dots,T$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\nB_{t}\\succeq B_{t-1}+X_{t},\\quad A_{t}\\preceq B_{t}\\preceq2\\rho_{t}I+A_{t}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. Note that for any symmetric matrix $\\boldsymbol{S}$ , it holds that $\\|S\\|_{2}\\pmb{I}\\succeq\\pmb{S}$ . Then we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n(\\rho_{t}-\\rho_{t-1})I+Z_{t}=\\|Y_{t}-Z_{t}\\|_{2}I+Z_{t}\\succeq Y_{t}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Adding $\\rho_{t-1}I$ on both sides, we get ", "page_idx": 21}, {"type": "equation", "text": "$$\nB_{t}=\\rho_{t}I+Z_{t}\\succeq\\rho_{t-1}I+Y_{t}=\\rho_{t-1}I+Z_{t-1}+X_{t}=B_{t-1}+X_{t}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Hence ", "page_idx": 21}, {"type": "equation", "text": "$$\nB_{t}=\\sum_{s=1}^{t}(B_{s}-B_{s-1})\\succeq\\sum_{s=1}^{t}X_{s}=A_{t}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "On the other hand, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\nZ_{t}\\preceq\\|Z_{t}-Y_{t}\\|_{2}I+Y_{t}=(\\rho_{t}-\\rho_{t-1})I+Y_{t}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Adding $\\rho_{t}I$ on both sides, we get ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\boldsymbol{B}}_{t}=\\rho_{t}{\\boldsymbol{I}}+{\\boldsymbol{Z}}_{t}\\preceq(2\\rho_{t}-\\rho_{t-1}){\\boldsymbol{I}}+{\\boldsymbol{Y}}_{t}}\\\\ &{\\quad=2(\\rho_{t}-\\rho_{t-1}){\\boldsymbol{I}}+\\rho_{t-1}{\\boldsymbol{I}}+{\\boldsymbol{Z}}_{t-1}+{\\boldsymbol{X}}_{t}}\\\\ &{\\quad={\\boldsymbol{B}}_{t-1}+2(\\rho_{t}-\\rho_{t-1}){\\boldsymbol{I}}+{\\boldsymbol{X}}_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Hence ", "page_idx": 21}, {"type": "equation", "text": "$$\nB_{t}=\\sum_{s=1}^{t}(B_{s}-B_{s-1})\\preceq\\sum_{s=1}^{t}2(\\rho_{s}-\\rho_{s-1})I+\\sum_{s=1}^{t}X_{s}=2\\rho_{t}I+A_{t}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The proof is completed. ", "page_idx": 21}, {"type": "text", "text": "Theorem 1. Assume that the gradients $G_{1},\\hdots,G_{T}\\in\\mathbb{R}^{m\\times n}$ are matrices of rank at most $r$ . Then for any $W^{\\ast}\\in\\mathbb{R}^{m\\times n}$ and $\\epsilon>0,$ , i $f\\eta=D/\\sqrt{2r}$ , the regret of Algorithm $6$ is bounded as follows, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}f_{t}(W_{t})-\\sum_{t=1}^{T}f_{t}(W^{*})\\leq\\sqrt{2r}D[2^{1/4}m\\rho_{T}^{1/4}+\\mathrm{tr}(\\tilde{L}_{T}^{1/4})][2^{1/4}n\\mu_{T}^{1/4}+\\mathrm{tr}(\\tilde{R}_{T}^{1/4})],\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\begin{array}{r}{D=\\operatorname*{max}_{t\\in[T]}\\|W_{t}-W^{*}\\|_{F},\\tilde{L}_{t}=\\epsilon I_{m}+\\sum_{t=1}^{T}G_{t}G_{t}^{\\top},}\\end{array}$ , and $\\begin{array}{r}{\\tilde{R}_{t}=\\epsilon I_{n}+\\sum_{t=1}^{T}{G_{t}^{\\top}G_{t}}}\\end{array}$ . ", "page_idx": 21}, {"type": "text", "text": "Proof. Define $\\hat{\\boldsymbol{L}}_{t}=(\\epsilon+\\rho_{t})\\boldsymbol{I}_{m}+\\boldsymbol{L}_{t},\\hat{\\boldsymbol{R}}_{t}=(\\epsilon+\\mu_{t})\\boldsymbol{I}_{n}+\\boldsymbol{R}_{t}$ . According to Lemma 12, $\\hat{L}_{t}$ and $\\hat{R}_{t}$ are positive definite. Recall the update performed in Algorithm 6, ", "page_idx": 21}, {"type": "equation", "text": "$$\nW_{t+1}=W_{t}-\\eta\\hat{L}_{t}^{-1/4}G_{t}\\hat{R}_{t}^{-1/4}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For $t>0$ , let $H_{t}=\\hat{L}_{t}^{1/4}\\otimes\\hat{R}_{t}^{1/4}$ , $g_{t}=\\overline{{\\mathrm{vec}}}(G_{t})$ and $w_{t}=\\overline{{\\mathrm{vec}}}(W_{t})$ . Due to Lemma 3(3) and Lemma 4, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\pmb{w}_{t+1}=\\pmb{w}_{t}-\\eta\\pmb{H}_{t}^{-1}\\pmb{g}_{t}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Lemma 12 implies $0\\prec\\hat{L}_{1}\\preceq\\cdot\\cdot\\cdot\\preceq\\hat{L}_{T},0\\prec\\hat{R}_{1}\\preceq\\cdot\\cdot\\cdot\\preceq\\hat{R}_{T}$ . Thus, according to Lemma 3(3)(4) and Lemma 6, we get ", "page_idx": 21}, {"type": "equation", "text": "$$\n0\\prec H_{1}\\preceq\\cdots\\preceq H_{T}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Let $H_{0}=\\mathbf{0}$ . By invoking Lemma 9 and Lemma 8, we obtain the regret bound ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}f_{t}(W_{t})-\\sum_{t=1}^{T}f_{t}(W^{*})\\leq\\frac{1}{2\\eta}\\sum_{t=1}^{T}(w_{t}-w^{*})^{\\top}(H_{t}-H_{t-1})(w_{t}-w^{*})+\\frac{\\eta}{2}\\sum_{t=1}^{T}(\\|g_{t}\\|_{H_{t}}^{*})^{2}}\\\\ &{\\displaystyle\\leq\\frac{D^{2}}{2\\eta}\\sum_{t=1}^{T}\\mathrm{tr}(H_{t}-H_{t-1})+\\frac{\\eta}{2}\\sum_{t=1}^{T}(\\|g_{t}\\|_{H_{t}}^{*})^{2}}\\\\ &{\\displaystyle=\\frac{D^{2}}{2\\eta}\\mathrm{tr}(H_{T})+\\frac{\\eta}{2}\\sum_{t=1}^{T}(\\|g_{t}\\|_{H_{t}}^{*})^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Define $\\begin{array}{r}{\\widehat{\\pmb{H}}_{t}=(r\\epsilon\\pmb{I}+\\sum_{s=1}^{t}\\pmb{g}_{s}\\pmb{g}_{s}^{\\top})^{1/2}}\\end{array}$ . Lemma 11 and Lemma 12 imply that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\widehat{H}_{t}\\preceq\\sqrt{r}\\tilde{L}_{t}^{1/4}\\otimes\\tilde{R}_{t}^{1/4}\\preceq\\sqrt{r}H_{t}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Using Lemma 7 and Lemma 10 along with the above equation, we obtain ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}(\\lVert g_{t}\\rVert_{H_{t}}^{*})^{2}\\leq\\sqrt{r}\\sum_{t=1}^{T}(\\lVert g_{t}\\rVert_{\\widehat{H}_{t}}^{*})^{2}\\leq2\\sqrt{r}\\mathrm{tr}(\\widehat{H}_{T})\\leq2r\\mathrm{tr}(H_{T}).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Consequently, using Lemma 3(5) and Lemma 12, we get the desired regret bound ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{t=1}^{T}f_{t}(W_{t})-\\sum_{t=1}^{T}f_{t}(W^{*})\\leq\\Big(\\frac{D^{2}}{2\\eta}+\\eta r\\Big)\\mathrm{tr}(H_{T})=\\sqrt{2r}D\\mathrm{tr}(\\hat{L}_{T}^{1/4})\\mathrm{tr}(\\hat{R}_{T}^{1/4})}}\\\\ &{\\leq\\sqrt{2r}D\\big[2^{1/4}m\\rho_{T}^{1/4}+\\mathrm{tr}(\\tilde{L}_{T}^{1/4})\\big]\\big[2^{1/4}n\\mu_{T}^{1/4}+\\mathrm{tr}(\\tilde{R}_{T}^{1/4})\\big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "by choosing $\\eta=D/\\sqrt{2r}$ . The proof is completed. ", "page_idx": 22}, {"type": "text", "text": "G Experimental Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We use one RTX3060Ti GPU under the PyTorch $2.0.1{+}\\mathrm{CUDA}11.8$ framework for DNN training on the CIFAR-100 and Tiny-ImageNet datasets, use one A800 GPU under the PyTorch 2.0.1+CUDA11.7 framework for DNN training on the ImageNet-1k and C4 datasets, and use two NVIDIA L40S GPUs under the PyTorch $2.0.1{+}\\mathrm{CUDAl}\\,1.8$ framework for DNN training on the OWT dataset. To obtain the total peak memory consumption per GPU, we call \"torch.cuda.max_memory_allocated\". ", "page_idx": 22}, {"type": "text", "text": "We set \"torch.backends.cudnn.benchmark\" to \"False\" for all the experiments, except when training ViT-Base/32 on the ImageNet-1k dataset. We report the total memory consumption instead of the memory consumption of the second-order optimizer. This total memory includes data, model parameters, activations, gradients, states forming the preconditioners and their inverse roots, states for the used first-order optimizer, and memory fragments. Our focus lies in quantizing the states for constructing preconditioners and their inverse roots, which are approximately $7x$ smaller for 4-bit Shampoo compared to 32-bit Shampoo. Because the block size is 64, its maximum value should be calculated every $64$ elements and saved as a 32-bit value, resulting in an additional overhead of 0.5 bits (32/64). Consequently, the memory savings are approximately 7 times, calculated as $32/(4\\!+\\!0.5)$ . In the future, we may adopt double quantization [9] to further reduce memory consumption. ", "page_idx": 22}, {"type": "text", "text": "For SGDM, Adagrad or AdamW used in second-order optimizers, we use 32-bit optimizer states on image classification tasks and 16-bit optimizer states on natural language modeling tasks by default. For SGDM, we set the momentum to 0.9 and use an initial learning rate of 0.1. For Adagrad, we set $\\epsilon=10^{-10}$ and use an initial learning rate of 0.01. For AdamW, we set $\\beta_{1}=0.9$ , $\\beta_{2}=0.999$ , and $\\epsilon=10^{-8}$ and use an initial learning rate of 0.001. For quantization settings, we employ block-wise normalization with a block size of 64 and linear square quantization by default. Matrices with a size smaller than 4096 will not be quantized. For Shampoo and CASPR, we use $\\epsilon=10^{-6}$ , $\\beta=0.95$ and $t_{1}=1,t_{2}=4$ by default. Shampoo and CASPR precondition blocks from large matrices and the maximum order of a preconditioner is 10000 for 130M LLAMA-2 and is 1200 for other models. For training loss, we use cross-entropy loss. For image classification tasks, automatic mixed precision is enabled except for training transformers on the CIFAR-100 and Tiny-ImageNet datasets. ", "page_idx": 22}, {"type": "text", "text": "Settings on training CNNs on CIFAR-100 or Tiny-ImageNet. Minibatch size is set to 128. Weight decay is 0.0005. Data augmentation includes random crop and horizontal filp. For Shampoo, we set $T_{1}=100$ and $T_{2}=500$ . In Section 5, we run SGDM for 300 epochs and SGDM $\\cdot+$ Shampoo for 200 epochs on the CIFAR-100 dataset. We run SGDM for 150 epochs and SGDM $^+$ Shampoo for 100 epochs on the Tiny-ImageNet dataset. We adopt the multi-step learning rate schedule (the learning rate is multiplied by 0.1 for every $30\\%$ epochs with a linear warmup at the first 5 epochs). ", "page_idx": 22}, {"type": "text", "text": "Settings on training transformers on CIFAR-100 or Tiny-ImageNet. We set a patch size of 4 for ViT-small on the CIFAR-100 dataset, and a patch size of 8 for ViT-small on the Tiny-ImageNet dataset. For training Swin-Tiny on the CIFAR-100 dataset, we use a patch size of 2 and window size of 4. For training Swin-Tiny on the Tiny-ImageNet dataset, we use a patch size of 4 and window size of 7. Minibatch size is set to 128. We run Adagrad/AdamW/NadamW for 150 epochs and Adagrad/AdamW $^+$ Shampoo for 100 epochs. Weight decay is 0.0005 for Adagrad, and is 0.05 for AdamW/NadamW. We use the cosine learning rate schedule. Data augmentation follows the source code in [25]. For Shampoo, we set $T_{1}=100$ and $T_{2}=500$ . With the exception of certain optimizer settings, the configurations used for ablation studies are identical to those outlined above. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "Settings on training ResNet50 on ImageNet-1k. We run SGDM for 120 epochs and SGDM $\\pm$ Shampoo for 100 epochs. Minibatch size is set to 256. Weight decay is 0.0001. We adopt the multi-step learning rate schedule (the learning rate is multiplied by 0.1 for every $30\\%$ epochs with a linear warmup at the first 5 epochs). Data augmentation includes random resized crop, horizontal flip, and color jitter. For Shampoo, we set $T_{1}=200$ and $T_{2}=1000$ . ", "page_idx": 23}, {"type": "text", "text": "Settings on training ViT-Base/32 on ImageNet-1k. We run AdamW for 150 epochs and AdamW $\\dotplus$ Shampoo for 120 epochs. Minibatch size is set to 512. Weight decay is 0.05. We use the cosine learning rate schedule. Data augmentation follows the configuration for training ViT-Base/16 in [44], excluding repeated augmentation. For Shampoo, we set $T_{1}=200$ and $T_{2}=1000$ . ", "page_idx": 23}, {"type": "text", "text": "Settings on training GPT-2 on OWT. We run AdamW with $10\\%$ warmup steps. Total batch size is set to 480. Batch size is set to 24 for training 124M GPT-2. Dtype is bfloat16. Weight decay is 0.1. For Shampoo, we set $T_{1}=200$ and $T_{2}=200$ . For our 4-bit Shampoo, we use Schur-Newton iteration used in Algorithm 4 to compute the inverse root of a preconditioner for training stability. ", "page_idx": 23}, {"type": "text", "text": "Settings on training LLAMA-2 on C4. We run AdamW with $10\\%$ warmup steps. Total batch size is set to 512. Batch size is set to 256 for training 130M LLAMA-2 and is set to 128 for training 350M LLAMA-2. Dtype is bfloat16. Weight decay is 0. For Shampoo, we set $T_{1}=200$ and $T_{2}=200$ . ", "page_idx": 23}, {"type": "text", "text": "Settings on K-FAC and AdaBK. K-FAC/AdaBK preconditions layers without limiting the size of a preconditioner. We set $\\beta=0.9$ , $T_{1}=200$ , and $T_{2}=2000$ . We use $\\epsilon=0.1$ for K-FAC and $\\epsilon\\,=\\,0.001$ for AdaBK. For 4-bit K-FAC/AdaBK, we set $t_{1}\\,=\\,0$ and $t_{2}\\,=\\,0$ (i.e., no orthogonal rectification). ", "page_idx": 23}, {"type": "text", "text": "Settings on schedule free optimization. We use the code from [6] to train ResNet34 with SGDScheduleFree and Swin-Tiny with AdamWScheduleFree. For SGDScheduleFree, we set $1\\mathrm{r}{=}1.0$ , weight_decay $=\\!0.0005$ and warmup_step $\\scriptstyle=2000$ . For AdamWScheduleFree, we set $\\scriptstyle\\ln=0.0025$ , weight_decay $=\\!0.05$ and warmup_steps $=10000$ . ", "page_idx": 23}, {"type": "text", "text": "Settings on M-FAC. We use the code from [15] and set ngrads $\\scriptstyle=32$ , damp $_{=0.1}$ . The other hyperparameter settings of M-FAC is the same as that of SGDM used for ResNet34 training. ", "page_idx": 23}, {"type": "text", "text": "H Additional Results ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "H.1 Image Classification ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "More learning rate schedulers. Table 8 shows the performance and wall-clock time of training ResNet34 on CIFAR-100 with cosine learning rate decay. By comparison, SGDM $^+$ Shampoo still converges faster than SGDM, and have slightly better test performance. ", "page_idx": 23}, {"type": "text", "text": "Table 8: Performance and wall-clock time of training ResNet34 on the CIFAR-100 dataset with cosine learning rate decay. $\\mathrm{TA}=$ test accuracy, and $\\mathrm{{WCT=}}$ wall-clock time. ", "page_idx": 23}, {"type": "table", "img_path": "ASqdVeifn7/tmp/21df972d2dac4145506b4a2bcb18c299d28ac45a9eb781722c31d930b6a3fa6a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "We also provide the results of training ResNet34 and Swin-Tiny on CIFAR-100 with schedulefree approach [6] in Table 9. From it one can see that AdamWScheduleFree achieves comparable performance to AdamW with cosine decay, while SGDScheduleFree underperforms compared to SGDM. We observe that this schedule-free algorithm shows rapid improvements in training and test accuracy during the early training stages, but may fail to achieve a higher test accuracy ultimately (see Figure 9). Anyway, these methods are still worse than our AdamW $+4$ -bit Shampoo. ", "page_idx": 23}, {"type": "text", "text": "Table 9: Performance and wall-clock time of training on the CIFAR-100 dataset with cosine learning rate decay and schedule-free approach. ResNet34 is trained for 300 epochs and Swin-Tiny is trained for 150 epochs. $\\mathrm{TA}=$ test accuracy, and ${\\mathrm{WCT}}=$ wall-clock time. ", "page_idx": 24}, {"type": "table", "img_path": "ASqdVeifn7/tmp/4d9a360a3f5411dc545775068d8458dd7c1db6d4c97a73a082eab8ce40396e73.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "ASqdVeifn7/tmp/221d09fe22aa8530ffb67c1245e3c561dfbe987c9db5adc18a7c81b560869c38.jpg", "img_caption": ["Figure 9: Visualization of test accuracies on the CIFAR-100 dataset with cosine learning rate decay and schedule-free approach. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "More optimizers. Table 10 shows results of training Swin-Tiny on CIFAR-100 with NadamW, Adagrad and Adagrad $^+$ Shampoo. One can see that Adagrad $^{+4}$ -bit Shampoo converges faster than Adagrad with ignorable extra memory overhead, and also has higher test accuracy. Besides, though NadamW [11] is slightly better than AdamW, it is still worse than our Adam $\\mathrm{W}{+4}$ -bit Shampoo. ", "page_idx": 24}, {"type": "table", "img_path": "ASqdVeifn7/tmp/a2901b1c8900195ec4b955cdff59f30ae9809613ec53e1c16587fd14605f6306.jpg", "table_caption": ["Table 10: Performance, wall-clock time, and memory cost of training Swin-Tiny on the CIFAR-100 dataset. TA $=$ test accuracy, ${\\mathrm{WCT}}=$ wall-clock time, and $\\mathbf{TMC=}$ total GPU memory cost. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "M-FAC [15] is a matrix-free method computing inverse-Hessian vector products with many gradient copies. It is not memory-efficient for M-FAC to maintain $m$ dense gradient copies $m=1024$ in its official code). Table 11 shows that both $\\mathrm{SGDM}{+}32\\cdot$ -bit Shampoo and $\\mathrm{SGDM+4}.$ -bit Shampoo enjoy much higher efficiency than M-FAC ( $m=32$ ) for training ResNet34 on CIFAR-100, and enjoy higher test accuracy. EVA [42] is a rank-one second-order optimizer and is memory-efficient. We train ResNet34 on CIFAR-100 with $\\mathrm{SGDM+EVA}$ , but despite extensive hyper-parameter tuning, we fail to achieve acceleration over SGDM. Instead, we cite EVA\u2019s result of training VGG-19 on CIFAR-100 for 200 epochs (see Table 2 in [42]). The test accuracies of SGDM+EVA and SGDM $\\cdot+$ Shampoo are $73\\%$ and $74.5\\%$ , respectively. ", "page_idx": 24}, {"type": "text", "text": "Table 11: Performance and memory cost of training ResNet34 on the CIFAR-100 dataset with cosine learning rate decay. All the optimizers are run for 200 epochs. TA $=$ test accuracy, and TMC $=$ total GPU memory cost. ", "page_idx": 25}, {"type": "table", "img_path": "ASqdVeifn7/tmp/4764b3a1da84701c212f0511f4485a5e6d8f118d86155a4bcb8aba00908ec7ec.jpg", "table_caption": [], "table_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "ASqdVeifn7/tmp/ca0927c73cb5f69e944938a4ffa198b848579f841c18ad7f95907a070c3129c5.jpg", "table_caption": ["Table 12: Performance, wall-clock time, and memory usage per GPU on natural language modeling tasks. $\\mathrm{VL}=$ validation loss, ${\\mathrm{WCT}}=$ wall-clock time, and $\\mathbf{TMC=}$ total GPU memory cost. "], "table_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "ASqdVeifn7/tmp/0a95205f2baa4bff75893e288a89fba920d5e9711860bb4b213ecff8535794ab.jpg", "img_caption": ["Figure 10: Visualization of validation loss on the C4 and OWT datasets. "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "H.2 Natural Language Modeling ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Models, datasets, and hyperparameters. We train 124M GPT-2 [32] for $60\\mathrm{k}$ steps on the OpenWebText (OWT) dataset \\* following the nanoGPT codebase \u2020 with two NVIDIA L40S GPUs, and train 130M LLAMA-2 [37] for $20\\mathbf{k}$ steps and 350M LLAMA-2 for $60\\mathrm{k}$ steps on the C4 dataset [33] following [43] with one A800 GPU. See Appendix G for experimental details. ", "page_idx": 25}, {"type": "text", "text": "Main results. We show the performance, wall-clock time, and memory cost in Table 12, and the validation loss curves in Figure 10. As with the vision tasks, our $\\scriptstyle\\mathrm{AdamW}+4\\cdot$ -bit Shampoo consistently outperformed AdamW and naive AdamW $+4$ -bit Shampoo in terms of performance, and $\\mathrm{AdamW}{+}32$ -bit Shampoo in terms of memory usage. ", "page_idx": 25}, {"type": "text", "text": "Memory efficiency. We further check the memory usage by increasing token batch size for a language model, which is calculated as the batch size multiplied by the context length (see [43]). To train LLAMA2-7B on the C4 dataset using a single A800 GPU (with a maximum memory of 81,920 ", "page_idx": 25}, {"type": "text", "text": "MB), we set the context length to 256 and then determine the maximum batch size allowed by each optimizer. For Shampoo, the maximum order of a preconditioner for training LLAMA2-7B is 2048. In all experiments, gradient checkpointing is enabled. Table 13 summarizes the evaluation results. By comparison, the 32-bit Shampoo runs out of memory with a batch size of 2, while our 4-bit Shampoo supports a batch size of 64 for standard training and only encounters memory issues at a batch size of 128. These results clearly demonstrate that our 4-bit Shampoo significantly conserves memory compared to the 32-bit version. ", "page_idx": 26}, {"type": "table", "img_path": "ASqdVeifn7/tmp/0a88b66802f54913415fa2b074c925e3866518676ae16a22909b2ed0a22e9a30.jpg", "table_caption": ["Table 13: Memory cost of training LLAMA2-7B on the C4 dataset with different optimizers. One A800 GPU with a maximum memory of $81{,}920\\,\\mathrm{MB}$ is enabled. TMC $=$ total GPU memory cost, and $\\mathrm{{OOM}=}$ out of memory. "], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We propose the first second-order optimizers with 4-bit states by taking Shampoo as an example, while preserving the performance achieved with 32-bit optimizer states. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 27}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We discuss the limitations of the work at the end of the paper. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 27}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes]   \nJustification: We provide all the proofs in the Appendix. Guidelines: \u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?   \nAnswer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We present the implementation details of all the experiments in the Appendix. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 28}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We use publicly available datasets and will release our source code. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code. \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details. \u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). ", "page_idx": 28}, {"type": "text", "text": "\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https:// nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 29}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We present the implementation details of all the experiments in the Appendix. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 29}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate   \ninformation about the statistical significance of the experiments?   \nAnswer: [No] ", "page_idx": 29}, {"type": "text", "text": "Justification: Error bars are not reported because it would be too computationally expensive. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 29}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 29}, {"type": "text", "text": "Justification: We present the implementation details of all the experiments in the main paper and   \nAppendix.   \nGuidelines: \u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We review the NeurIPS Code of Ethics and our paper conforms it. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 30}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 30}, {"type": "text", "text": "Justification: We discuss both potential positive societal impacts and negative societal impacts at the end of the paper.   \nGuidelines:   \n\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 30}, {"type": "text", "text": "Justification: We present 4-bit Shampoo for memory efficient training of deep models. It poses no   \nsuch risks.   \nGuidelines: \u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 30}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety fliters. \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. \u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 31}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 31}, {"type": "text", "text": "Justification: We properly mention all the existing assets. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 31}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 31}, {"type": "text", "text": "Justification: We provide anonymized zip file of our code. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 31}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?   \nAnswer: [NA]   \nJustification: Our paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 31}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA]   \nJustification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:   \n\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 32}]