{"references": [{"fullname_first_author": "Greg Yang", "paper_title": "Wide feedforward or recurrent neural networks of any architecture are gaussian processes", "publication_date": "2019-00-00", "reason": "This paper lays the theoretical foundation for understanding the behavior of infinitely wide neural networks, a concept crucial to the paper's analysis of state-space models."}, {"fullname_first_author": "Greg Yang", "paper_title": "Tensor programs IV: Feature learning in infinite-width neural networks", "publication_date": "2021-00-00", "reason": "This paper introduces the Tensor Programs framework and the Maximal Update Parameterization, which are central to the paper's theoretical analysis and proposed scaling rules for SSMs."}, {"fullname_first_author": "Greg Yang", "paper_title": "Tensor programs ivb: Adaptive optimization in the infinite-width limit", "publication_date": "2023-00-00", "reason": "This paper extends the Tensor Programs framework to adaptive optimization algorithms, providing crucial context for the paper's discussion of training stability and hyperparameter transferability."}, {"fullname_first_author": "Albert Gu", "paper_title": "Efficiently modeling long sequences with structured state spaces", "publication_date": "2021-00-00", "reason": "This paper introduces the S4 model, a structured state-space model, which serves as a key example and basis for the paper's theoretical analysis."}, {"fullname_first_author": "Albert Gu", "paper_title": "On the parameterization and initialization of diagonal state space models", "publication_date": "2022-00-00", "reason": "This paper provides a detailed analysis of the parameterization and initialization of state-space models, which is essential to the paper's findings on the scaling behavior of SSMs."}]}