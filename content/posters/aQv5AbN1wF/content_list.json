[{"type": "text", "text": "On Feature Learning in Structured State Space Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Leena Chennuru Vankadara\u20201 Jin Xu\u20202 Moritz Haas 3 Volkan Cevher1,4 ", "page_idx": 0}, {"type": "text", "text": "1AGI Foundations, Amazon 2University of Oxford\u2217   \n3University of T\u00fcbingen, T\u00fcbingen AI Center,\u2217 4LIONS, EPFL\u2217 ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "This paper studies the scaling behavior of state-space models (SSMs) and their structured variants, such as Mamba, that have recently arisen in popularity as alternatives to transformer-based neural network architectures. Specifically, we focus on the capability of SSMs to learn features as their network width approaches infinity. Our findings reveal that established scaling rules, such as the Maximal Update Parameterization, fail to support feature learning as these models cannot be represented in the form of Tensor Programs. Additionally, we demonstrate that spectral scaling conditions, shown to be effective for feature learning in a host of other architectures, do not hold the same implications for SSMs. Through a detailed signal propagation analysis in SSMs, both forward and backward, we identify the appropriate scaling necessary for non-trivial feature evolution in the infinite-width limit. Our proposed scaling shows behavior akin to the Maximal Update Parameterization, such as improved stability, better generalization, and transferability of optimal hyper-parameters from small to large scale SSMs. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "State-space models (SSMs), such as Mamba (Gu and Dao, 2023), have become popular in deep learning as alternatives to transformers like GPT and BERT series (Radford et al., 2019, Brown et al., 2020, Achiam et al., 2023, Devlin et al., 2018, Touvron et al., 2023, Chowdhery et al., 2023, Gemini Team et al., 2023). SSMs integrate elements from RNNs, CNNs, and control models, excelling in inference and handling long contexts (Gu et al., 2021, Gupta et al., 2022, Gu et al., 2022, Smith et al., 2022). ", "page_idx": 0}, {"type": "text", "text": "The success of foundation models based on transformers and SSMs alike is largely attributed to their scale\u2014both in terms of data and model size. However, this increased scale often introduces challenges, such as precision issues due to instability or the vanishing/exploding gradient problems. Additionally, the sequential nature of state-space models (SSMs) makes them notoriously difficult to train. Therefore, developing a rigorous understanding of how SSMs scale as their dimensions increase and identifying optimal scaling rules is crucial. ", "page_idx": 0}, {"type": "text", "text": "In this vein, infinite-width asymptotics, such as Neural tangent kernel (NTK) analyses have been a central tool providing key insights in DL theory. However, a key limitation of the NTK analysis is the lack of feature learning in the infinite width limit (Yang and Littwin, 2023). Feature learning addresses the more realistic training setting where we have unrestricted movement of the neural network parameters (Yang and Littwin, 2023). ", "page_idx": 0}, {"type": "text", "text": "Intriguingly, recent work by Yang and Hu (2021) showed that under an expanded space of hyperparameters, which includes layer-wise scaling of the learning rates, one can find a unique parameterization called Maximal Update pPrameterization $(\\mu{\\bf P})$ that induces non-trivial feature evolution even in the infinite width limit for a host of key architectures. The results are obtained via the mathematical framework of Tensor Programs, which provides the foundation to study the effects of parameterizations on the learning dynamics in the limit. To this end, it is natural to ask: ", "page_idx": 0}, {"type": "image", "img_path": "aQv5AbN1wF/tmp/1e80c3ab0b99f1d43b7076601a071d35410466f8840b3e459248beeb5421f47d.jpg", "img_caption": ["Figure 1: Under our derived scaling $\\mu\\mathrm{P-}\\mathrm{SSM}$ , Mamba achieves feature learning in all three SSM layers. In contrast, both Standard Parametrization (SP) and $\\mu\\mathrm{P}$ (heuristic) lead to instability or vanishing updates for either the latent states xl(i)or the output signal yl or both in each SSM layer. The figures above illustrate the scalings when $l=1$ , but they exhibit the same trend across recurrence steps. We simultaneously scale up $N_{x}$ and $N_{u}$ . We run each experiment 10 times, and the shaded areas indicate the standard deviation of these runs. Both Zero-Order Hold (ZOH) and Euler discretization of $\\mathbf{B}_{l}$ are studied and indicated in the subtitle. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "When do SSMs admit feature learning in the infinite-width limit? ", "page_idx": 1}, {"type": "text", "text": "Our research tackles this question by investigating the behavior of SSMs as network width approaches infinity. We examine how SSMs learn and evolve features in this context and assess the adequacy of recent scaling rules such as $\\mu\\mathrm{P}$ and spectral scaling conditions. ", "page_idx": 1}, {"type": "text", "text": "Our contributions: ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "\u2022 We provide a detailed scaling analysis of forward and backward signal propagation in SSMs as the width approaches infinity, identifying that standard scalings lead to unbounded signals. \u2022 We demonstrate theoretically and empirically that popular scaling rules like $\\mu\\mathrm{P}$ do not yield correct scaling for SSMs due to\u2014as we prove\u2014their non-representability as Tensor Programs (cf., Figure 1). \u2022 We derive a unique correction that ensures correctly balanced signals in both the forward and backward passes, stabilizing the training process and enhancing model performance. \u2022 Empirically, we validate that our proposed scaling facilitates hyper-parameter transfer from small-scale to large-scale SSMs, similar to the effects observed in MLPs and transformers. ", "page_idx": 1}, {"type": "text", "text": "2 On the lack of feature learning in SSMs at infinite width ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Feature learning in sequence models ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Sequence models are built by combining sequential layers that transform input sequences into output sequences. A sequence layer can be represented as $\\mathbf{y}_{1:L}\\,=\\,f_{\\mathrm{seq}}(\\mathbf{u}_{1:L})$ where $\\mathbf{u}_{1:L}$ is a compact notation for sequence $\\mathbf{u}_{1},\\ldots,\\mathbf{u}_{L}$ , with $\\mathbf{u}_{l}\\in\\mathbb{R}^{N_{u}}$ , $\\mathbf{y}_{l}\\in\\mathbb{R}^{N_{y}}$ . Note that $f_{\\mathrm{seq}}$ can be easily generalized to accept and output multiple sequences as in residual connections and multi-branch architectures (see Appendix B.1). Here $N_{u}$ and $N_{y}$ are treated as widths of the sequence model, which can be scaled up to create larger sequence models. To denote the backward pass of a model concisely, let $\\mathcal{L}$ be the overall training loss, and we write $\\begin{array}{r}{\\bar{\\bullet}=\\frac{\\partial\\mathcal{L}}{\\partial\\bullet}}\\end{array}$ . Instead of studying the scaling of each element in a \u221avector , we study the norm $\\Vert\\mathbf{u}\\Vert_{2}$ . When elements in $\\mathbf{u}$ are in $\\Theta(1)$ , we have $\\|\\mathbf{u}\\|_{2}\\in\\Theta(\\sqrt{N_{u}})$ . Throughout this work, we will use $\\Theta$ to represent the asymptotic order in the limit ( $\\boldsymbol{N}_{u}\\to\\infty$ in this case), but omit the precise notation for readability. ", "page_idx": 1}, {"type": "text", "text": "In layerwise sequence models, where the $k$ -th layer is denoted as $\\mathbf{u}_{1:n}^{(k)}=f_{k}(\\mathbf{u}_{1:n}^{(k-1)})$ with ${\\bf u}_{l}^{(k)}\\in{\\bf\\Xi}$ $\\mathbb{R}^{N_{k}}$ , we have the following definition for feature learning: ", "page_idx": 2}, {"type": "text", "text": "Definition 2.1 (Feature Learning in Layerwise Sequence Models). A layerwise sequence model is in the feature learning regime if for any $k\\in[K]$ , the features $\\mathbf{u}_{1:L}^{(k)}$ and the updates of features $\\Delta\\mathbf{u}_{1:L}^{(k)}$ after one gradient update have the following scaling: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\exists\\,l\\in[L],\\ \\lVert\\mathbf{u}_{l}^{(k)}\\rVert_{2}\\in\\Theta(\\sqrt{N_{k}})\\ \\ (\\mathrm{Stability~at\\,initialization})}\\\\ &{\\exists\\,l\\in[L],\\ \\lVert\\Delta\\mathbf{u}_{l}^{(k)}\\rVert_{2}\\in\\Theta(\\sqrt{N_{k}})\\ \\ (\\mathrm{Non-trivial\\,feature~updates})}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Note that the first condition is a stability condition that demands the activations to remain coordinatewise $\\Theta(1)$ in the forward pass, whereas the second condition ensures nonvanishing and nonexploding activation updates. This definition can be easily generalized to general cases for arbitrary sequence models in Appendix B.1. ", "page_idx": 2}, {"type": "text", "text": "For a particular sequence layer in the model, if we assume the inputs to the layer are asymptotically independent and identically distributed (i.i.d), correctly scaled, and the gradients backpropagated into the sequence layer have the correct scaling (as in Assumption 3.1), and this sequence layer satisfies the two conditions (INIT) and $(\\Delta)$ , we say this sequence layer admits feature learning. The corrected scaling $\\mu P$ -SSM we propose in Section 3 satisfies an even stronger condition: The updates of all trainable weights should have nonvanishing and nonexploding effect on the activation and output function updates. We then say that the sequence model is effectively feature learning. This is similar in spirit to the requirements for $\\mu P$ from Yang and Hu (2021) that ensure maximal stable updates of all trainable weights in standard architectures like MLPs, but requires different initialization and learning rate scaling rules for the trainable weights in SSMs, as we show in Section 3. ", "page_idx": 2}, {"type": "text", "text": "2.2 Tensor Programs, spectral scaling, and the Maximal Update Parmeterization ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Tensor Programs. The framework of Tensor Programs (TP) (Yang, 2019) was initially developed to understand the behavior of wide neural networks both at initialization and during training. While there are many versions of Tensor Programs, the most general version (that subsumes all the previous versions) is referred to as the $\\mathbf{NE}{\\otimes}\\mathbf{OR}\\,^{\\top}$ program (Yang and Littwin, 2023). A $\\mathbf{NE}{\\otimes}\\mathbf{OR}^{\\top}$ program constitutes a sequence of vectors in $\\mathbb{R}^{n}$ and a sequence of scalars in $\\mathbb{R}$ inductively generated from an initial set of random scalars, vectors, and matrices following a specified set of instructions: matrix multiplications, non-linear outer products, and vector averages. Yang and Littwin (2023) show that for any architecture whose forward pass can be represented as a $\\mathbf{NE}{\\otimes}\\mathbf{OR}^{\\top}$ program (which includes many modern architectures used in practice including transformers and convolutional networks), there exists a unique scaling rule called Maximal Update Parametrization $(\\mu{\\bf P})$ under which features in every single layer evolve in a width-independent fashion with scale. Interestingly, it has been shown that unlike standard parameterizations, $\\mu\\mathrm{P}$ allows transferability of optimal hyper-parameters from small to large scale models. ", "page_idx": 2}, {"type": "text", "text": "The key idea of the \u201cTensor Program Ansatz\u201d (Yang, 2019) is that pre-activations and their gradients arising in training most neural network architectures via standard update rules (such as SGD and ADAM) can be represented as vectors in a $\\mathbf{NE}{\\otimes}\\mathbf{OR}^{\\top}$ program. The Ansatz suggests that vectors in a TP become asymptotically independent as well as identically distributed both at initialization as well as during training. Leveraging this, the framework allows for a mechanistic tracking of the behaviour of such vectors by assigning a random variable $Z^{v}$ to represent the (asymptotically identical) distribution of the coordinates of the vector $v$ . This principle underlies the development of the key theoretical result of the Tensor Program machinery \u2014 the Master Theorem \u2014 which can be viewed as the compositional, non-linear generalization of the law of large numbers. It allows the theoretical computation of infinite width limits of different quantities such as (pre-)activations or the output of a neural network. ", "page_idx": 2}, {"type": "text", "text": "While the following proposition also holds for other structured SSMs such as MAMBA, for clarity, we first introduce the simpler S4 model. ", "page_idx": 2}, {"type": "text", "text": "The S4 recurrent layer. The S4 recurrent layer ${\\bf y}_{1:L}=f_{{\\cal S}4}({\\bf u}_{1:L};{\\bf w}=\\{{\\bf B},{\\bf C}\\})$ can be viewed as a discretization of a continuous-time SSM given by ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\frac{d}{d t}\\mathbf{x}_{t}=\\mathbf{A}\\mathbf{x}_{t}+\\mathbf{B}\\mathbf{u}_{t}\\quad\\mathrm{and}\\quad\\mathbf{y}_{t}=\\mathsf{R e}[\\mathbf{C}\\mathbf{x}_{t}]\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "for $l=1,\\dots,L$ with $\\mathbf{x}_{0}=0$ and where $\\mathsf{R e}(\\cdot)$ gives the real part of a complex vector, and where we let $\\mathbf{A}=\\mathrm{Diag}(\\mathbf{a})$ with $\\mathbf{a}\\in\\mathbb{C}^{N_{x}}$ , $\\mathbf{B}\\in\\mathbb{C}^{N_{x}\\times\\hat{N}_{u}}$ , $\\mathbf{C}\\in\\mathbb{C}^{N_{y}\\times N_{x}^{\\star}}$ . We have the discretized sequence to sequence mapping $f_{\\mathrm{S4}}$ as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{x}_{l}=\\mathbf{A}^{\\prime}\\mathbf{x}_{l-1}+\\mathbf{B}^{\\prime}\\mathbf{u}_{l}\\quad\\mathrm{and}\\quad\\mathbf{y}_{l}=\\mathsf{R e}[\\mathbf{C}^{\\prime}\\mathbf{x}_{l}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where we follow the Zero-Order-Hold (ZOH) discretization method, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{A}^{\\prime}=\\exp(\\tau\\cdot\\mathbf{A}),\\quad\\mathbf{B}^{\\prime}=(\\mathbf{A}^{\\prime}-\\mathbf{I})\\mathbf{A}^{-1}\\mathbf{B},\\quad{\\mathrm{~and~}}\\quad\\mathbf{C}^{\\prime}=\\mathbf{C}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In practice, simple Euler discretization can also be used for $\\mathbf{B}$ , i.e., $\\mathbf{B}^{\\prime}=\\tau\\mathbf{B}$ . To model long-range dependency, S4 advocates for HiPPO theory (Gu et al., 2020). There are many possible Hippo-based initializations possible for the structured transition matrices A (e.g., Hippo-Leg-S, S4D-Inv, and S4D-Real) and our result holds for all the parameterizations. ", "page_idx": 3}, {"type": "text", "text": "While the training of most common architectures including transformers or convolutional networks via update rules such as SGD or ADAM are representable as a $\\mathbf{NE}{\\otimes}\\mathbf{OR}^{\\top}$ program, Proposition 2.2 shows that this does not hold in general for structured state space models. ", "page_idx": 3}, {"type": "text", "text": "Proposition 2.2 (Structured SSMs are not generally representable as $\\mathbf{NE}{\\otimes}\\mathbf{OR}^{\\top}$ programs). The forward and backward signal propagation of structured SSMs including S4 (2) and (3) and MAMBA (Section 3.1) trained via standard algorithms such as SGD or ADAM are not representable as a NE\u2297OR\u22a4program. Indeed this holds for all existing Hippo-parameterizations of the structured matrix A including HiPPO-LegS, HiPPO-LegS-N, HiPPO-LegS-D, S4D-Inv, S4D-Lin, and S4D-Real. ", "page_idx": 3}, {"type": "text", "text": "Proof sketch. The formal proofs are provided in Appendix C. The key architectural component that is not representable in a $\\mathbf{NE}{\\otimes}\\mathbf{OR}\\,^{\\top}$ program is the Hippo-based structured transition matrix A. Due to the rapid decay of the diagonal entries of ${\\bf A}^{-1}$ , the hidden states $\\mathbf{x}_{\\mathrm{0}}$ are not even asymptotically identically distributed, already at initialization. Therefore coordinates of $\\mathbf{x}_{\\mathrm{0}}$ cannot be generated by a sequence of $\\mathbf{NE}{\\otimes}\\mathbf{OR}\\,^{\\top}$ computations, neither at initialization nor over the course of training. As we will discuss in Section 6, developing a TP-like framework to cover SSMs requires a substantial generalization of the existing TP framework and is beyond the scope of the current paper. Nevertheless, in Section 6, we outline the key challenges and potential paths toward such a generalization. ", "page_idx": 3}, {"type": "text", "text": "Spectral scaling condition. Yang et al. (2023a) showed that many practical architectures admit feature learning if certain spectral scaling conditions are satisfied. Let $\\mathbf{W}_{l}$ and $\\Delta\\mathbf{W}_{l}\\in\\mathbb{R}^{N_{l}\\times N_{l-1}}$ denote the lth layer weight matrices and their gradient updates. The condition states that for feature learning to hold, the spectral norms of the matrices should satisfy: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\left\\|\\mathbf{W}_{l}\\right\\|_{*}\\in\\Theta\\left(\\sqrt{\\frac{N_{l}}{N_{l-1}}}\\right)\\quad\\mathrm{~and~}\\quad\\left\\|\\Delta\\mathbf{W}_{1}\\right\\|_{*}\\in\\Theta\\left(\\sqrt{\\frac{N_{l}}{N_{l-1}}}\\right)\\quad\\mathrm{~for~all~}l\\in[L].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Under $\\mu\\mathrm{P},$ , architectures that are representable as a $\\mathbf{NE}{\\otimes}\\mathbf{OR}^{\\top}$ program satisfy the spectral scaling condition (Yang et al., 2023a). Here, we show that for structured SSMs, architectures that satisfy spectral scaling conditions do not in general satisfy conditions for feature learning. ", "page_idx": 3}, {"type": "text", "text": "Proposition 2.3 (Spectral scaling does not generally imply feature learning in SSMs). Structured SSMs including S4 and Mamba trained via standard algorithms such as SGD or ADAM that satisfy spectral scaling conditions $(*)$ do not satisfy conditions for feature learning given in condition $(\\Delta)$ . Indeed this holds for all well-known Hippo-parameterizations of the structured matrix A including HiPPO-LegS, HiPPO-LegS-N, HiPPO-LegS-D, S4D-Inv, S4D-Lin, and S4D-Real. ", "page_idx": 3}, {"type": "text", "text": "Proof sketch. The formal proofs are provided in Appendix C. To gather some intuition for this result in a simplified setting, consider the scale of the hidden states for token index 0, which is already wrong at initialization. By definition, $\\left\\|\\mathbf{x}_{1}\\right\\|_{2}=\\left\\|\\mathbf{B}^{\\prime}\\mathbf{u}_{1}\\right\\|_{2}=\\left\\|\\mathbf{A}\\mathbf{B}\\mathbf{u}_{1}\\right\\|_{2}$ , where $\\mathbf{A}=(\\mathbf{A}^{\\prime}-\\mathbf{I})\\mathbf{A}^{-\\mathrm{{i}}}$ . Now, observe that $\\left\\|\\mathbf{B}^{\\prime}\\mathbf{u}_{1}\\right\\|_{2}^{2}$ is a sum of independent random variables that satisfy the Kolmogorov condition, so that the sum behaves according to the strong law of large numbers. For spectral scaling conditions to yield the right scaling of the initialization variance, it is crucial that the following condition holds: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\left\\|\\mathbf{B}^{\\prime}\\mathbf{u}_{1}\\right\\|_{2}\\in\\Theta\\left(\\left\\|\\mathbf{B}^{\\prime}\\right\\|_{*}\\left\\|\\mathbf{u}_{1}\\right\\|_{2}\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "However, using standard tools from random matrix theory one can show that(\u2225B\u2225\u2032\u2225B\u2217\u2225uu\u222512\u22252) $\\begin{array}{r}{\\frac{\\left\\|\\mathbf{B}^{\\prime}\\mathbf{u}\\right\\|_{2}}{\\left(\\|\\mathbf{B}^{\\prime}\\|_{*}\\left\\|\\mathbf{u}_{1}\\right\\|_{2}\\right)}\\in\\Theta(\\frac{1}{\\sqrt{N_{u}}})}\\end{array}$ which clearly violates (4). ", "page_idx": 3}, {"type": "table", "img_path": "aQv5AbN1wF/tmp/d448cbc9b75292b6de6e84d3840f6aa4957b7ef258f75ece201203614df0f251.jpg", "table_caption": [], "table_footnote": ["Table 1: Overview of the different parameterizations and their corresponding scaling for latent states, outputs and their updates. Results for ZOH and Euler discretization are separated by |. "], "page_idx": 4}, {"type": "text", "text": "3 Identifying the unique scaling for effective feature learning in SSMs ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we analyze the forward and backward signal propagation in structured SSMs. Due to the generality of the architecture, we consider MAMBA as the basis for the analysis. Similar arguments apply for other SSMs such as S4, S5, H3, or DSS. In the appendix, we also provide a detailed analysis of signal propagation in S4 and identify the correct scaling conditions for maximal stable weight updates. ", "page_idx": 4}, {"type": "text", "text": "3.1 Selective State Space Models ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Selective SSMs in the Mamba architecture ${\\bf y}_{1:L}=f_{\\mathrm{Mamba}}({\\bf u}_{1:L})$ can be written as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{B}_{l}=\\mathrm{Lin}_{N_{x}}(\\mathbf{u}_{l};\\mathbf{W}_{B},\\mathbf{b}_{B})\\in\\mathbb{R}^{N_{x}},}\\\\ &{\\mathbf{C}_{l}=\\mathrm{Lin}_{N_{y}}(\\mathbf{u}_{l};\\mathbf{W}_{C},\\mathbf{b}_{C})\\in\\mathbb{R}^{N_{x}},}\\\\ &{\\tau_{l}=\\mathrm{Softplus}(\\tau_{0}+\\mathrm{Broadcast}_{N_{u}}(\\mathrm{Lin}_{1}(\\mathbf{u}_{l};\\mathbf{W}_{\\tau},\\mathbf{b}_{\\tau})),\\ \\tau_{0},\\tau_{l}\\in\\mathbb{R}^{N_{u}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "For $i=1,\\dots,N_{u}$ , we have $N_{u}$ 1D SSMs: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{A}^{(i)}=\\mathrm{Diag}(-\\exp(\\mathbf{a}_{\\mathrm{log}}^{(i)})),\\ \\mathbf{a}_{\\mathrm{log}}^{(i)}\\in\\mathbb{R}^{N_{x}},\\quad\\mathbf{x}_{l}^{(i)}=\\mathbf{A}_{l}^{\\prime(i)}\\mathbf{x}_{l-1}^{(i)}+u_{l}^{(i)}\\mathbf{B}_{l}^{\\prime(i)},\\ u_{l}^{(i)}\\in\\mathbb{R},\\ \\mathbf{x}_{l}^{(i)}\\in\\mathbb{R}^{N_{x}},}\\\\ {\\mathbf{A}_{l}^{\\prime(i)},\\mathbf{B}_{l}^{\\prime(i)}=\\mathrm{ZOH}(\\tau_{l}^{(i)},\\mathbf{A}^{(i)},\\mathbf{B}_{l}),\\ \\tau_{l}^{(i)}\\in\\mathbb{R},\\ \\ \\ \\ y_{l}^{(i)}=\\mathbf{C}_{l}\\tau_{\\mathbf{X}_{l}^{(i)}},\\ y_{l}^{(i)}\\in\\mathbb{R}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where x0 $\\mathbf{x}_{0}^{(i)}\\;=\\;\\mathbf{0},\\;\\mathbf{a}_{\\mathrm{log}}^{(i)},\\;\\tau_{0}\\;\\in\\;\\mathbb{R}^{N_{u}}$ , and all linear layer weights $\\mathbf{W}_{B},\\mathbf{b}_{B},\\mathbf{W}_{C},\\mathbf{b}_{C},\\mathbf{W}_{\\tau},\\mathbf{b}_{\\tau}$ are trainable parameters. The Zero-Order-Hold (ZOH) discretization procedure ${\\bf A}_{l}^{\\prime(i)},{\\bf B}_{l}^{\\prime(i)}\\;=$ $\\mathrm{ZOH}(\\tau,\\mathbf{A}^{(i)},\\mathbf{B}_{l}^{\\prime(i)})$ can be written as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{A}_{l}^{\\prime(i)}=\\exp(\\tau_{l}^{(i)}\\cdot\\mathbf{A}^{(i)}),\\qquad\\mathbf{B}_{l}^{\\prime(i)}=(\\mathbf{A}_{l}^{\\prime(i)}-\\mathbf{I})\\mathbf{A}^{(i)}{}^{-1}\\mathbf{B}_{l}^{(i)}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In a nutshell, Mamba can be seen as $N_{u}$ SSMs, one for each input channel. Weights for these SSMs are shared and depend on the input at that recurrent step. Because of the dependency on inputs, Mamba can model non-stationary sequences. Weight matrices are initialized as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{W}_{B}\\sim{\\mathcal{N}}(\\mathbf{0},\\sigma_{B}^{2}\\mathbf{I})\\in\\mathbb{R}^{N_{x}\\times N_{u}},\\ \\ \\mathbf{W}_{C}\\sim{\\mathcal{N}}(\\mathbf{0},\\sigma_{C}^{2}\\mathbf{I})\\in\\mathbb{R}^{N_{x}\\times N_{u}},\\ \\ \\mathbf{W}_{\\tau}\\sim{\\mathcal{N}}(\\mathbf{0},\\sigma_{\\tau}^{2}\\mathbf{I})\\in\\mathbb{R}^{1\\times N_{u}},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "and zero initialization for all biases. For each SSM, $\\mathbf{A}^{(i)}$ is still initialized according to the HiPPO theory. When $\\mathbf{A}^{(i)}$ is real-valued, we let $\\mathbf{a}_{\\mathrm{log}}^{(i)}[j]=\\log(j+1)$ and $\\tau_{0}$ can be seen as a bias term initialized to $\\tau_{0}\\sim\\mathrm{Softplus}^{-1}(\\mathcal{U}(0.001,0.1))$ . We will assume that $\\tau_{0}$ is not trained, as this does not have any effect on the scale of the different quantities under consideration. This is a minor technical assumption and our results would also hold if we considered $\\tau$ as a trainable parameter. ", "page_idx": 4}, {"type": "image", "img_path": "aQv5AbN1wF/tmp/02ea00d80b5c32eed69efb55e0773ef1d6025ac2408fc13b6c69237b6ad7a23b.jpg", "img_caption": ["Figure 2: Illustration of the Mamba S6 Layer. The computation is modularized into three components: selection, discretization, and per-channel linear recurrence. Mamba introduces a selection mechanism where weight matrices $\\mathbf{B}_{l}$ , $\\mathbf{C}_{l}$ depend on the inputs $\\mathbf{u}_{l}$ . These weight matrices are then separated into per-channel parameters, and discretized using either the ZOH or Euler methods. The discretized, per-channel weights are then applied in a linear recurrence, allowing each channel to perform computations in parallel. Trainable parameters are shown in blue. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "3.2 Forward signal propagation through a S6 Mamba layer ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To derive the correct choice of initialization scalings $\\sigma_{B}$ and $\\sigma_{C}$ , we begin by analyzing the scale of activations (i.e., hidden states and outputs) in a S6 Mamba layer in the first forward pass as $N_{u}\\to\\infty$ then $N_{x}\\rightarrow\\infty$ . We believe that our results would also hold in the proportional limit where $N_{u}\\to\\infty$ and $N_{x}\\to\\infty$ with $\\textstyle{\\frac{N_{x}}{N_{u}}}\\in\\Theta(1)$ . However deriving the results in this setting would incur a significant technical overhead and we defer this analysis to a future work. We briefly discuss this in Section 6. As is common in practice, we assume that the SSM layer is embedded into a neural network architecture containing standard architectural blocks such as MLPs, normalization layers, or residual connections. All proofs are provided in Appendix C.2. ", "page_idx": 5}, {"type": "text", "text": "Assumption 3.1. Assume that the forward pass of all the components of the network except the SSM layer are expressible as NE $\\otimes$ OR\u22a4programs and are parameterized according to \u00b5P. ", "page_idx": 5}, {"type": "text", "text": "This assumption ensures that the inputs to the SSM layer are asymptotically i.i.d and correctly scaled. It also ensures that gradients into the SSM layer have the correct scaling. All results in this section are stated under Assumption 3.1. Through the forward signal propagation analysis, we identify the correct scale of initialization for weight matrices $\\mathbf{W}_{B}$ and $\\mathbf{W}_{C}$ . We show that both standard parameterization as well as spectral scaling conditions do not yield the correct scale of initialization for the weight matrices. The key results are summarized in Table 1. ", "page_idx": 5}, {"type": "text", "text": "For simplicity of exposition, first consider the scale of $\\mathbf{x}_{1}^{(i)}\\,=\\,(\\mathbf{A}_{l}^{\\prime(i)}\\,-\\,\\mathbf{I})\\mathbf{A}^{(i)}\\,^{-1}\\mathbf{B}_{1}u_{1}^{(i)}$ before generalizing to arbitrary $l\\in[L]$ . ", "page_idx": 5}, {"type": "text", "text": "Proposition 3.2 (Scale of hidden states $\\mathbf{x}_{1}^{(i)}$ in Mamba at initialization). Under the ZOH discretization procedure, as $N_{u}$ then $N_{x}$ approach infinity, for any $i\\in[N_{u}].$ , the squared $l_{2}$ -norm of the hidden states $\\|\\mathbf{x}_{1}^{(i)}\\|_{2}^{2}$ is a.s. scaled as $\\|\\mathbf{x}_{1}^{(i)}\\|_{2}^{2}\\in\\Theta\\left(\\zeta(2)\\,\\sigma_{B}^{2}\\,\\|\\mathbf{u}_{1}\\|^{2}\\,(u_{1}^{(i)})^{2}\\right)$ , where $\\zeta(2)$ denotes the Riemann zeta function at 2. ", "page_idx": 5}, {"type": "text", "text": "Following condition (INIT), the 1D SSM admits stability at initialization under the following conditions: ", "page_idx": 5}, {"type": "text", "text": "Therefore, for stability at initialization, the initialization should scale as $\\begin{array}{r}{\\sigma_{B}\\,\\in\\,\\Theta\\big(\\sqrt{\\frac{N_{x}}{N_{u}}}\\big)}\\end{array}$ . Note that, under both standard parameterization (e.g., Kaiming or LeCun initialization) and spectral scaling conditions, $\\sigma_{B}$ is initialized as $\\Theta\\big(\\sqrt{\\frac{1}{N_{u}}}\\big)$ , which leads to vanishing hidden states according to Proposition 3.2. We empirically verify this fact in Figure 1.   \nProposition 3.3 provides the scale of the output of a Mamba layer. ", "page_idx": 6}, {"type": "text", "text": "Proposition 3.3 (Scale of outputs $\\mathbf{y}_{1}^{(i)}$ of a Mamba layer at initialization). Under the ZOH discretization procedure as $N_{u}$ then $N_{x}$ approach infinity, for any $i\\in[N_{u}]$ , the output $y_{1}^{(i)}$ converges in distribution to a Gaussian with mean 0 and standard deviation $C\\sigma_{B}\\sigma_{C}\\left\\lVert\\mathbf{u}_{1}\\right\\rVert_{2}^{2}$ for some widthindependent constant $C>0$ . ", "page_idx": 6}, {"type": "text", "text": "Accordingly, imposing the conditions for stability of $y_{1}^{(i)}$ according to condition (INIT) implies the initialization scaling condition $\\begin{array}{r}{\\sigma_{C}\\in\\Theta(\\sqrt{\\frac{1}{N_{x}N_{u}}})}\\end{array}$ . Note again that standard parameterization suggests initializing $\\begin{array}{r}{\\sigma_{C}\\,\\in\\,\\Theta\\big(\\sqrt{\\frac{1}{N_{u}}}\\big)}\\end{array}$ under which the outputs would diverge with scale. Under spectral scaling, $\\sigma_{C}$ is initialized much larger. However, since the hidden states vanish with width, the outputs of the SSM admit the correct scaling here as demonstrated in Figure 1. ", "page_idx": 6}, {"type": "text", "text": "Generalizing to arbitrary $l\\in[L]$ . The corrected scalings of $\\sigma_{B}$ and $\\sigma_{C}$ derived above generalize to the entire sequence, as a sum over the sequence usually does not cancel out the scaling. More formally, for all $l\\in[L]$ , we have ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbf{x}_{l}^{(i)}=\\sum_{m=0}^{l-1}(\\mathbf{A}_{l}^{\\prime(i)})^{m}\\mathbf{B}_{l-m}^{\\prime(i)}u_{l-m}^{(i)}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "First, observe that the operator $(\\mathbf{A}_{l}^{\\prime(i)})^{m}$ does not change the width-scaling. To see this note that since $\\mathbf{A}_{l}^{\\prime(i)}=\\mathrm{Diag}(a_{1}^{\\prime},\\ldots,a_{N_{x}}^{\\prime})$ with $a_{n}^{\\prime}=e^{-\\frac{1}{2}\\tau_{l}^{(i)}}(\\cos(\\tau_{l}^{(i)}\\pi n)+i\\sin(\\tau_{l}^{(i)}\\pi n))$ , we have that, for all complex vectors $\\mathbf{v}\\in\\mathbb{C}^{N_{x}}$ it holds that $\\|(\\mathbf{A}_{l}^{\\prime(i)})^{m}\\mathbf{v}\\|_{2}=e^{-m\\tau_{l}^{(i)}/2}\\|\\mathbf{v}\\|_{2}$ for any $m\\in[L]$ . Now since, for any $\\mathbf{u}_{l}$ , setting $\\begin{array}{r}{\\sigma_{B}\\in\\Theta(\\sqrt{\\frac{N_{x}}{N_{u}}})}\\end{array}$ yields $\\left\\|\\mathbf{B}_{l}^{\\prime(i)}\\mathbf{u}_{l}\\right\\|_{2}\\in\\Theta(\\sqrt{N_{x}})$ , each term in the summation is of order $\\Theta(\\sqrt{N_{x}})$ . Unless, for every $l$ , the term $\\mathbf{B}_{l}^{\\prime(i)}\\mathbf{u}_{l}$ perfectly cancels out with the terms before to affect the width scaling , we have that $\\left\\|\\mathbf{x}_{l}^{(i)}\\right\\|_{2}\\in\\Theta(\\sqrt{N_{x}})$ . The same argument can be used to show the stability of $y_{l}^{(i)}$ . Concluding this argument, we have derived the correct scaling of the initialization variances $\\sigma_{B}$ and $\\sigma_{C}$ for feature stability in a Mamba layer summarized below. ", "page_idx": 6}, {"type": "text", "text": "Conditions for stability of a S6 Mamba layer at initialization. The features of a S6 Mamba recurrent layer $\\mathbf{y}_{1:L}=f_{\\mathrm{mamba}}(\\mathbf{u}_{1:L};\\mathbf{w})$ are stable at initialization in the infinite-width limit under the following scaling conditions: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\sigma_{B}\\in\\Theta\\left(\\sqrt{\\frac{N_{x}}{N_{u}}}\\right)\\quad\\mathrm{and}\\quad\\sigma_{C}\\in\\Theta\\left(\\sqrt{\\frac{1}{N_{x}N_{u}}}\\right)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "3.3 Backward signal propagation in a S6 Mamba Layer ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we provide the correct scaling of the learning rates $\\eta_{a},\\,\\eta_{B}$ , and $\\eta_{C}$ by a detailed analysis of the backward signal propagation in the limit of $N_{u}\\to\\infty$ then $N_{x}\\rightarrow\\infty$ . Specifically, we analyze the scale of the activation updates for both hidden states and outputs in the first backward pass through a Mamba layer. We also show that both standard parameterization as well as spectral scaling conditions do not yield the correct scale of learning rates for the weight matrices. The key results are summarized in Table 1. ", "page_idx": 6}, {"type": "text", "text": "Proposition 3.4 (Scale of the updates of hidden states $\\Delta\\mathbf{x}_{1}^{(i)}$ after 1 step of SGD). Under the ZOH discretization procedure, as $N_{x}$ then $N_{u}$ approach infinity, for every $I D$ SSM, the squared $l_{2}$ -norm of the updates $\\|\\Delta\\mathbf{x}_{1}^{(i)}\\|_{2}$ of the hidden states after one step of SGD is a.s. scaled as $\\begin{array}{r}{\\|\\Delta\\mathbf{x}_{1}^{(i)}\\|_{2}\\in\\Theta\\left(\\eta_{B}\\frac{1}{\\sqrt{N_{u}}}\\sigma_{C}\\left\\|\\mathbf{u}_{1}\\right\\|_{2}^{3}\\zeta(4)^{\\frac{1}{2}}\\right)}\\end{array}$ , where $\\zeta(4)$ denotes the Riemann zeta function at 4. ", "page_idx": 6}, {"type": "text", "text": "The Riemann zeta function at 4 evaluates to a width-independent constant and $\\begin{array}{r}{\\sigma_{C}\\in\\Theta(\\sqrt{\\frac{1}{N_{x}N_{u}}})}\\end{array}$ due to Pr\u221aoposition 3.3 for stability of outputs. Therefore, for the scale of the hidden state updates to be $\\Theta(\\sqrt{N_{x}})$ , the correct scaling of the learning rate $\\eta_{B}$ is given by $\\begin{array}{r}{\\Theta\\big(\\frac{N_{x}}{\\sqrt{N_{u}}}\\big)}\\end{array}$ . On the other hand, spectral scaling suggests that $\\eta_{B}$ must scale as $\\Theta\\big(\\frac{N_{x}}{N_{u}}\\big)$ . Under this scaling, however, updates of the hidden states would vanish with width and therefore the first block of the SSM is in the lazy regime. This is corroborated by our experiments in Figure 1. ", "page_idx": 7}, {"type": "text", "text": "Next, to derive the correct scaling of the learning rate $\\eta_{C}$ , we consider the scale of output updates. ", "page_idx": 7}, {"type": "text", "text": "Proposition 3.5 (Scale of the updates of outputs $\\Delta\\mathbf{y}_{1}^{(i)}$ after 1 step of SGD). Under the ZOH discretization procedure, as $N_{u}$ then $N_{x}$ approach infinity, for every $I D$ SSM, the squared $l_{2}$ -norm of the updates of the hidden states after one step of SGD scales as $\\begin{array}{r}{|\\Delta y_{1}^{(i)}|^{2}\\in\\Theta\\left(\\eta_{C}\\sigma_{B}^{2}\\sqrt{\\frac{1}{N_{u}}}\\left\\|\\mathbf{u}_{1}\\right\\|_{2}^{4}\\right)}\\end{array}$ . The result suggests that for the correct scaling of the updates, the learning rate $\\eta_{C}$ must scale as $\\Theta\\big(\\frac{1}{N_{x}\\sqrt{N_{u}}}\\big)$ . Under spectral scaling, $\\eta_{C}$ scales much larger as $\\Theta\\big(\\frac{N_{x}}{N_{u}}\\big)$ . However, since the updates of the hidden states vanish under spectral scaling, this larger incorrect scaling of $\\eta_{C}$ downward corrects the scale of the updates in the outputs as shown in Table 1 and empirically verified in Figure 1. ", "page_idx": 7}, {"type": "text", "text": "On the correct scaling of $\\eta_{a}$ . It turns out that the scaling of the learning rate $\\eta_{a}$ does not play a role in either stability at initialization or for non-trivial updates with scale. However, if $\\eta_{a}$ is not scaled correctly, then the transition matrix A is not updated. In particular, as shown in Appendix C.2, $\\eta_{a}$ needs to scale as $\\Theta(N_{u})$ . Below we summarize the correct scaling conditions to achieve non-trivial feature updates in the infinite-width limit of a Mamba layer. ", "page_idx": 7}, {"type": "text", "text": "In the same vein as the discussion in Section 3.2, it is straightforward to verify that our results hold for arbitrary $l\\in[L]$ and more gradient steps as soon as we assume that the updates of the weight matrices and activations do not perfectly cancel out the corresponding initial quantities. ", "page_idx": 7}, {"type": "text", "text": "Conditions for non-trivial feature updates in a S6 Mamba Layer. The updates in a S6 Mamba recurrent layer $\\mathbf{y}_{1:L}=f_{\\mathrm{mamba}}(\\mathbf{u}_{1:L};\\mathbf{w})$ evolve non-trivially in the infinite-width limit under the following conditions: ", "page_idx": 7}, {"type": "image", "img_path": "aQv5AbN1wF/tmp/4510ed8fbc0049ec6419c3b25d04f04f2a51ec9323059dce55f2edc656a29499.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "4 $\\mu\\mathbf{P}$ -SSM implies stability and feature learning in Mamba ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Empirical verification of different scalings in Table 1. First, we verify that our derived $\\mu\\mathrm{P-}\\mathrm{SSM}$ scaling (see Table 1) for Mamba indeed leads to feature learning, i.e., it ensures stability at initialization as defined in condition (INIT) and non-trivial feature updates during training as defined in condition $(\\Delta)$ . We scale up the SSM latent state size $N_{x}$ and the SSM output dimension $N_{u}$ simultaneously and track the scaling of both features and feature updates. Due to the linear decay in the eigenvalues of the transition matrix ${\\bf A}^{-1}$ , we typically observe a strong finite sample effect at small $N_{x}$ . Constrained by computational resources, we opt for a much smaller $N_{u}$ $(N_{u}=N_{x}/8)$ ) than is usually employed in practice. This adjustment enables us to scale up $N_{x}$ effectively, thus mitigating the finite-sample effect and to more clearly demonstrate the scaling behavior in the asymptotic limit in Figure 1. For all experiments in this section, we train Mamba with 3 SSM blocks for language modelling on the wikitext dataset (Merity et al., 2016) and use plain Stochastic Gradient Descent (SGD) to perform gradient updates. We use the huggingface (Wolf et al., 2019) Mamba implementation and the $\\mu\\mathrm{P}$ package (Yang et al., 2022) for scaling in our experiments. ", "page_idx": 7}, {"type": "text", "text": "As shown in Figure 1, under Standard Parametrization (SP), both the SSM latent states and the outputs explode at initialization, and their updates also explode, leading to instability both at initialization and during training. Under the spectral scaling parameterization prescribed in Yang et al. (2023a), other layers except for the SSM layers have the correct scaling by design. However, in the SSM layer, when using Zero-Order Hold (ZOH) discretization for $\\mathbf{B}_{l}$ , the latent states at initialization and their updates vanish when scaling up the width, while the output signals still have the right scaling as predicted by theory. On the other hand, when Euler discretization is used for $\\mathbf{B}_{l}$ , the latent states will have the correct scaling, but the output signals will explode. The results clearly highlight the importance of correcting the scaling of the $\\mu\\mathrm{P}$ parameterization. When using the corrected scaling which we call $\\mu\\mathrm{P}$ -SSM, both the latent states and the output signals have the right scaling at initialization, and their updates have the same correct scaling. This holds true for both discretization schemes. Note that the slight shift in scaling when the width is small is due to finite sample effects. It stabilizes once the width is sufficiently large. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Stability, generalization, and hyper-parameter transfer. In Figure 3, we employ Mamba as a generative model on the wikitext-103 dataset and conduct single-epoch training for $20K$ iterations. We plot the test loss against the learning rate on a logarithmic scale and compare the results across different model widths (both $N_{u}$ and $N_{x}$ ). In this experiment, we use the standard setting where $N_{u}\\gg N_{x}$ $(N_{u}~=~16N_{x}$ in this case). Using $\\mu\\mathrm{P-}\\mathrm{SSM}$ scaling or $\\mu\\mathrm{P}$ (heuristic) significantly improves test performance compared to standard parameterization of Mamba for this task. For larger learning rate, $\\mu\\mathrm{P-}\\mathrm{SSM}$ shows better stability compared to $\\mu\\mathrm{P}$ (heuristic), highlighting the importance of deriving the correct scaling for SSMs rather than heuristically adopting $\\mu\\mathrm{P}$ (heuristic) without investigation. Furthermore, previously unreported, we observe stable HP transfer from small to large widths and monotonically improving performance with increasing model widths in structured SSMs. In contrast, we observe completely non-monotonic behavior under standard scaling of SSMs. ", "page_idx": 8}, {"type": "text", "text": "Note that optimal learning rate also appears to transfer under spectral scaling. The reasoning behind why optimal hyper-parameters transfer across scales is not completely understood. For instance, the optimal learning rate has been empirically shown to transfer across depth in transformers under appropriate depth-dependent scaling of the residual branches (Bordelon et al., 2023). However, from a theoretical standpoint, layers within each residual block of a transformer are in the lazy regime in the limit (Yang et al., 2023b). This is very similar to Mamba under spectral scaling. Under the ZOH discretization, the first block of the SSM is in the lazy regime but the outputs themselves are updated non-trivially. This suggests that a more thorough understanding of both necessary and sufficient conditions of the transferrability of hyper-parameters is warranted. ", "page_idx": 8}, {"type": "image", "img_path": "aQv5AbN1wF/tmp/e396eb5f8fe0d4848bcdcda4a02704bf6e7891e8d5a640347e8001a1b3b63b22.jpg", "img_caption": ["Figure 3: Test loss against learning rate on Mamba with varying widths ( $N_{u}$ and $N_{x}$ ). Using $\\mu\\mathrm{P-}\\mathrm{SSM}$ scaling leads to substantially improved test performance compared to the SP scaling. Compared to $\\mu\\mathrm{P}$ (heuristic), $\\mu\\mathrm{P-}\\mathrm{SSM}$ scaling provides greater stability when utilizing large learning rates. Notably, we observe stable learning rate transfer from small to large model widths. Performance improves monotonically across widths in structured SSMs under $\\mu\\mathrm{P-}\\mathrm{SSM}$ scaling, as opposed to standard scaling where performance actually drops with scale after a certain width. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "5 Related work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Signal propagation. Our work can be seen as scaling theory or signal propagation theory with the goal of preventing both vanishing and exploding signals in forward and backward passes. In this sense, we build on a rich literature, often restricted to an analysis at or close to initialization (Schoenholz et al., 2016, Poole et al., 2016, Hanin and Rolnick, 2018, Xiao et al., 2020). Towards understanding infinite-width limits of neural networks, kernel-based approaches (Neal, 1996, Jacot et al., 2018) and applications of mean-field theory (Mei et al., 2018) have yielded valuable insights. ", "page_idx": 8}, {"type": "text", "text": "Tensor Programs. Most promisingly, the Tensor Programs framework (Yang, 2019, Yang and Hu, 2021, Yang and Littwin, 2023, Yang et al., 2022, 2023b) covers many modern deep learning architectures, optimization algorithms and arbitrary abc-parameterizations. Each abc-parameterization is essentially defined by a layerwise scaling of initialization variance and learning rate as a function of network width. Seminal work by Yang and Hu (2021) shows that there exists a unique maximal update parameterization $(\\mu{\\sf P})$ that attains a stable feature learning infinite-width limit. This parameterization has since been shown to be a good model for understanding the properties of large models (Vyas et al., 2024), and has been extended to infinite width and depth limits of ResNets (Hayou et al., 2021, Li et al., 2021, Bordelon et al., 2023, Yang et al., 2023b) and Transformers (Noci et al., 2022, 2024). ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Structured SSMs. Our analysis focuses on structured state space models (SSMs). The S4 model (Gu et al., 2021) is inspired by continuous-time linear SSMs, which are well-studied in control systems, and its specific initialization is motivated by the HiPPO theory (Gu et al., 2020). S4 and its variants e.g. DSS(Gu et al., 2022), S4D(Gupta et al., 2022), S5(Smith et al., 2022), etc., demonstrate impressive long-range dependency and overcome the quadratic computational cost of transformer models (Vaswani et al., 2017) w.r.t. sequence length. However, these models are less effective at modeling text, or even perform simple tasks such as selective copying (Gu and Dao, 2023). Mamba (Gu and Dao, 2023) is proposed to address such issues with selection mechanism. This line of work has also inspired revisiting Recurrent Neural Networks (RNNs) (Orvieto et al., 2023, De et al., 2024, Beck et al., 2024), leading to the growing interest in RNN-based sequence models. ", "page_idx": 9}, {"type": "text", "text": "6 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we study the scaling behavior of forward and backward signal propagation in structured state space models \u2013 a promising class of recent architectures. We show that existing scaling rules such as standard parameterization, $\\mu P$ , or spectral scaling conditions do not yield desirable properties such as feature learning in SSMs at scale. Through our analysis, we propose the correct scaling of state space models under which we empirically observe feature learning and transferability of hyper-parameters from small to large scale models. ", "page_idx": 9}, {"type": "text", "text": "On Generalizing Tensor Programs. While our proposed scaling has been derived by a thorough analysis of signal propagation in SSM layers, our results are still limited to the $N_{u}$ then $N_{x}$ tend to infinity setting. A completely rigorous analysis of SSMs in the proportional limit where $N_{u}$ and $N_{x}$ approach infinity with $\\frac{N_{x}}{N_{u}}$ held roughly constant requires us to carefully track how the different activations and the updates are correlated with each other. For most standard architectures, the Tensor Program (TP) machinery provides the appropriate tools to do precisely this. Therefore, it is of considerable interest to generalize the TP framework. The key assumption that requires relaxation is that the different vectors in a TP (such as activations or updates) are asymptotically identically distributed. As discussed earlier, this assumption crucially underlies the key theoretical results of TP called Master Theorems. However, a potential path toward generalizing TP may be found by noting that the Master Theorems can be viewed as a non-linear compositional form of the law of large numbers or central limit theorem. Accordingly, it may be possible to relax the assumption of being identically distributed in the limit and instead ask that the entries of the vectors in a TP asymptotically satisfy weaker conditions such as Lindenberg or Kolmogorov conditions. Note however, that any such generalization is highly technical and is beyond the scope of the current work. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.   \nMaximilian Beck, Korbinian P\u00f6ppel, Markus Spanring, Andreas Auer, Oleksandra Prudnikova, Michael Kopp, G\u00fcnter Klambauer, Johannes Brandstetter, and Sepp Hochreiter. xlstm: Extended long short-term memory. arXiv preprint arXiv:2405.04517, 2024.   \nBlake Bordelon, Lorenzo Noci, Mufan Bill Li, Boris Hanin, and Cengiz Pehlevan. Depthwise hyperparameter transfer in residual networks: Dynamics and scaling limit. arXiv:2309.16620, 2023.   \nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.   \nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1\u2013113, 2023.   \nSoham De, Samuel L Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, et al. Griffin: Mixing gated linear recurrences with local attention for efficient language models. arXiv preprint arXiv:2402.19427, 2024.   \nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.   \nGemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.   \nAlbert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023.   \nAlbert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Hippo: Recurrent memory with optimal polynomial projections. In Advances in Neural Information Processing Systems, volume 33, pages 1474\u20131487, 2020.   \nAlbert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. In International Conference on Learning Representations, 2021.   \nAlbert Gu, Karan Goel, Ankit Gupta, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models. Advances in Neural Information Processing Systems, 35:35971\u2013 35983, 2022.   \nAnkit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces. Advances in Neural Information Processing Systems, 35:22982\u201322994, 2022.   \nBoris Hanin and David Rolnick. How to start training: The effect of initialization and architecture. Advances in neural information processing systems, 31, 2018.   \nSoufiane Hayou, Eugenio Clerico, Bobby He, George Deligiannidis, Arnaud Doucet, and Judith Rousseau. Stable resnet. In International Conference on Artificial Intelligence and Statistics, pages 1324\u20131332. PMLR, 2021.   \nArthur Jacot, Franck Gabriel, and Cl\u00e9ment Hongler. Neural Tangent Kernel: Convergence and generalization in neural networks. In Advances in Neural Information Processing Systems (NeurIPS), pages 8571\u20138580, 2018. ", "page_idx": 10}, {"type": "text", "text": "Mufan Li, Mihai Nica, and Dan Roy. The future is log-gaussian: Resnets and their infinite-depth-andwidth limit at initialization. In Advances in Neural Information Processing Systems (NeurIPS), volume 34, pages 7852\u20137864, 2021. ", "page_idx": 11}, {"type": "text", "text": "Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of twolayer neural networks. Proceedings of the National Academy of Sciences, 115(33):E7665\u2013E7671, 2018. ", "page_idx": 11}, {"type": "text", "text": "Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. In International Conference on Learning Representations, 2016.   \nRadford M. Neal. Priors for Infinite Networks, pages 29\u201353. Springer New York, 1996.   \nLorenzo Noci, Sotiris Anagnostidis, Luca Biggio, Antonio Orvieto, Sidak Pal Singh, and Aurelien Lucchi. Signal propagation in transformers: Theoretical perspectives and the role of rank collapse. Advances in Neural Information Processing Systems, 35:27198\u201327211, 2022.   \nLorenzo Noci, Chuning Li, Mufan Li, Bobby He, Thomas Hofmann, Chris J Maddison, and Dan Roy. The shaped transformer: Attention models in the infinite depth-and-width limit. Advances in Neural Information Processing Systems, 36, 2024.   \nAntonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. In International Conference on Machine Learning, pages 26670\u201326698. PMLR, 2023.   \nBen Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. Exponential expressivity in deep neural networks through transient chaos. Advances in neural information processing systems, 29, 2016.   \nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019. URL https://api.semanticscholar.org/ CorpusID:160025533.   \nSamuel S Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep information propagation. arXiv:1611.01232, 2016.   \nPranab K Sen and Julio M Singer. Large sample methods in statistics: an introduction with applications, volume 25. CRC press, 1994.   \nJimmy TH Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. In The Eleventh International Conference on Learning Representations, 2022.   \nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \nRoman Vershynin. Spectral norm of products of random and deterministic matrices. Probability theory and related fields, 150(3):471\u2013509, 2011.   \nNikhil Vyas, Alexander Atanasov, Blake Bordelon, Depen Morwani, Sabarish Sainathan, and Cengiz Pehlevan. Feature-learning networks are consistent across widths at realistic scales. Advances in Neural Information Processing Systems, 36, 2024.   \nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, et al. Huggingface\u2019s transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019.   \nLechao Xiao, Jeffrey Pennington, and Samuel Schoenholz. Disentangling trainability and generalization in deep neural networks. In International Conference on Machine Learning, pages 10462\u201310472. PMLR, 2020.   \nGreg Yang. Wide feedforward or recurrent neural networks of any architecture are gaussian processes. Advances in Neural Information Processing Systems, 32, 2019.   \nGreg Yang and Edward J. Hu. Tensor programs iv: Feature learning in infinite-width neural networks. In International Conference on Machine Learning (ICML), 2021.   \nGreg Yang and Etai Littwin. Tensor programs ivb: Adaptive optimization in the infinite-width limit. arXiv:2308.01814, 2023.   \nGreg Yang, Edward J Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, and Jianfeng Gao. Tensor programs v: Tuning large neural networks via zero-shot hyperparameter transfer. arXiv:2203.03466, 2022.   \nGreg Yang, James B Simon, and Jeremy Bernstein. A spectral condition for feature learning. arXiv preprint arXiv:2310.17813, 2023a.   \nGreg Yang, Dingli Yu, Chen Zhu, and Soufiane Hayou. Tensor programs vi: Feature learning in infinite-depth neural networks. arXiv preprint arXiv:2310.02244, 2023b. ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Background ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 $\\mathbf{NE}{\\otimes}\\mathbf{OR}^{\\top}$ , abc-parameterizations and $\\mu\\mathbf{P}$ ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "$\\mathbf{NE}{\\otimes}\\mathbf{OR}^{\\top}$ programs. For the precise definitions and theorems of the $\\mathbf{NE}{\\otimes}\\mathbf{OR}\\,^{\\top}$ framework we refer to Yang and Littwin (2023). Here we try to provide an intuitive introduction. ", "page_idx": 13}, {"type": "text", "text": "A NE\u2297OR\u22a4program consists of a set of vectors inductively generated from a set of initial matrices, vectors, and scalars following a set of allowed instructions: vector averages (Avg), matrix multiplications (MatMul), and non-linear outer products (OuterNonlin). ", "page_idx": 13}, {"type": "text", "text": "All entries of an initial vector $v$ are sampled iid from $N(0,1)$ . Every entry of an initial matrix $A$ is sampled independently from distributions that all have mean 0, variance $n^{-1}$ and that satisfy a technical boundedness assumption on all higher-order moments. Nonlinearities $\\psi$ used for OuterNonlin operations are either pseudo-Lipschitz or polynomially smooth. Initial scalars $c$ converge to 0 a.s. ", "page_idx": 13}, {"type": "text", "text": "New scalars $c$ can be introduced into the program by the Avg operation over a vector $v$ , defined as $\\textstyle c={\\frac{1}{n}}\\sum_{\\alpha}^{n}v_{\\alpha}$ . Now the $\\mathbf{NE}{\\otimes}\\mathbf{OR}^{\\top}$ Master Theorem implies $c\\stackrel{\\cdot}{\\rightarrow}\\mathring{c}:=\\mathbb{E}Z^{v}$ almost surely. Matrix multiplications $A v$ are defined as usual. In the limit, a non-trivial interaction term arises if $A^{\\top}$ had appeared in a $\\mathbf{NE}{\\otimes}\\mathbf{OR}^{\\top}$ operation that has generated the vector $v$ . To define the OuterNonlin operation, aggregate a fixed amount $|\\mathbf{x}|$ of $\\mathbf{NE}{\\otimes}\\mathbf{OR}^{\\top}$ vectors in $\\mathbf{x}$ and fix $r\\,\\in\\,\\mathbb{N}$ . Denote the collection of all $l$ defined $\\mathbf{NE}{\\otimes}\\mathbf{OR}^{\\top}$ scalars by c. Then a OuterNonlin operation with nonlinearity $\\psi:\\mathbb{R}^{|\\mathbf{x}|(r+1)+l}\\rightarrow\\mathbb{R}$ , is given by ", "page_idx": 13}, {"type": "equation", "text": "$$\ny_{\\alpha}={\\frac{1}{n^{r}}}\\sum_{\\beta_{1},\\ldots,\\beta_{r}}^{n}\\psi(\\mathbf{x}_{\\alpha},\\mathbf{x}_{\\beta_{1}},\\ldots,\\mathbf{x}_{\\beta_{r}};\\mathbf{c}),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "and, due to the $\\mathbf{NE}{\\otimes}\\mathbf{OR}^{\\top}$ Master Theorem, behaves in the limit as ", "page_idx": 13}, {"type": "equation", "text": "$$\nZ^{\\mathbf{y}}=f(Z^{\\mathbf{x}}),\\quad\\mathrm{where~}f:\\mathbb{R}^{|\\mathbf{x}|}\\rightarrow\\mathbb{R},\\:\\:\\:f(Z^{\\mathbf{x}})=\\mathbb{E}[\\psi(Z^{\\mathbf{x}},Z_{1}^{\\mathbf{x}},\\ldots,Z_{r}^{\\mathbf{x}},\\bar{c})],\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $Z^{\\mathbf{x}}$ is an $r$ -dimensional random vector with the limiting distribution of $\\mathbf{x}$ , and $Z_{1}^{\\mathbf{x}},\\ldots,Z_{r}^{\\mathbf{x}}$ are iid copies of $Z^{\\mathbf{x}}$ . ", "page_idx": 13}, {"type": "text", "text": "The Maximal Update Parameterization $(\\mu{\\bf P})$ . For any architecture/component whose forward pass is representable as a $\\mathbf{NE}{\\otimes}\\mathbf{OR}^{\\top}$ program, $\\mu\\mathrm{P}$ can be derived following the instructions below. Let each parameter tensor $W$ be parameterized as $W=n^{-a_{W}}w$ where $w$ is a trainable parameter with initialization $w_{\\alpha\\beta}\\sim\\mathcal{N}(0,\\bar{n^{-2b_{W}}})$ . Let the learning rate be parameterized as $\\eta n^{-c}$ for some width-independent $\\eta\\,>\\,0$ . Then $\\mu\\mathrm{P}$ is prescribed as follows: Set $c\\,=\\,0$ and $b_{W}\\,=\\,1/2$ for all parameter tensors $W$ . ", "page_idx": 13}, {"type": "text", "text": "\u2022 $a_{W}=0$ if the input and output to $W$ are width-independent (e.g., scalars), \u2022 $a_{W}=2$ if both input and output to $W$ scale with width (e.g., attention matrices). \u2022 $a_{W}=1$ if input is width-independent and output scales with width (e.g., input embeddings). \u2022 $a_{W}=1/2$ if input scales with width and output is width-independent (e.g., output matrices). ", "page_idx": 13}, {"type": "text", "text": "A.2 Classical Limit Theorems ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Here we recapitulate classical results about large sums of random variables that we use in our proofs. We present the versions of the Kolmogorov strong law of large numbers and the Lindenberg-Feller Central Limit Theorem provided in Sen and Singer (1994, Theorems 3.2.10 and 3.3.3). ", "page_idx": 13}, {"type": "text", "text": "Theorem A.1 (Kolmogorov Strong Law of Large Numbers). Let $X_{i}$ , $i\\geq1$ , be independent random variables such that $\\mathbb{E}X_{i}=\\mu_{i}$ and $V a r(X_{i})=\\overline{{\\sigma_{i}^{2}}}$ exist for every $i\\geq1$ . Then ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\sum_{k\\geq1}k^{-2}\\sigma_{k}^{2}<\\infty\\quad\\Longrightarrow\\quad n^{-1}\\sum_{i=1}^{n}X_{i}-n^{-1}\\sum_{i=1}^{n}\\mu_{i}\\xrightarrow{a.s.}0.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Theorem A.2 (Lindenberg-Feller-Central Limit Theorem). Let $X_{i},\\,i\\geq1$ , be independent random variables such that $\\mathbb{E}X_{i}=\\mu_{i}$ and $V a r(X_{i})=\\sigma_{i}^{2}$ exist for every $i\\geq1$ . Also let $\\textstyle s_{n}^{2}=\\sum_{i=1}^{n}\\sigma_{i}^{2}$ and $\\begin{array}{r}{Z_{n}=s_{n}^{-1}(\\sum_{i=1}^{n}X_{i}-\\sum_{i=1}^{n}\\mu_{i})}\\end{array}$ . Then the Lindenberg-Feller condition ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\forall\\varepsilon>0,\\qquad\\frac{1}{s_{n}^{2}}\\sum_{i=1}^{n}\\mathbb{E}\\left[(X_{i}-\\mu_{i})^{2}\\mathbb{I}_{\\{|X_{i}-\\mu_{i}|\\geq\\varepsilon s_{n}\\}}\\right]\\to0,\\quad a s\\quad n\\to\\infty,\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "holds if and only if both $\\begin{array}{r}{\\operatorname*{max}_{1\\leq i\\leq n}\\frac{\\sigma_{i}^{2}}{s_{n}^{2}}\\rightarrow0}\\end{array}$ as $n\\to\\infty$ , and (ii) $Z_{n}$ converges in distribution to a Gaussian with mean 0 and standard deviation 1. ", "page_idx": 14}, {"type": "text", "text": "B Definitions ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Definition B.1 (Softplus). The softplus function is defined as Softplus $\\begin{array}{r}{(x)\\,=\\,\\frac{1}{\\beta}\\log(1+\\exp(\\beta x))}\\end{array}$ with default smoothing value $\\beta=1$ . ", "page_idx": 14}, {"type": "text", "text": "B.1 Feature Learning in General Sequence Models ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Let $\\mathcal{G}$ be a directed acyclic graph (DAG) with vertices $\\mathscr{V}(\\mathscr{G})$ and edges $\\mathcal E(\\mathcal G)$ . For each node $\\mathbf{v}\\in\\mathcal{V}(\\mathcal{G})$ , its parent nodes are denoted by $\\mathcal{P}\\mathcal{A}(\\mathbf{v})$ . In a sequence model, each node $\\mathbf{v}$ corresponds to a sequence $\\mathbf{v}_{1:L}$ with length $L$ , computed from its parent node sequences with operation $\\overset{\\overline{{\\mathbf{v}_{1:L}}}}{\\mathbf{v}_{1:L}}=f_{v}(\\{\\mathbf{u}_{1:L}|\\mathbf{u}\\in$ $\\mathcal{P}\\mathcal{A}(\\mathbf{v})\\}$ ). These sequence layers can be shared instance-wise multi-layer perceptrons (MLPs), normalizations, residual summation, or transformers/recurrent layers, etc. ", "page_idx": 14}, {"type": "text", "text": "Definition B.2 (Feature Learning in Sequence Models). A sequence model is in the feature learning regime if for any $\\mathbf{v}\\in\\mathcal{V}(\\mathcal{G})$ , the features $\\mathbf{v}_{1:L}$ where $\\mathbf{v}\\in\\mathbb{R}^{\\hat{N}_{v}}$ and the updates of features $\\Delta\\mathbf{v}_{1:L}$ after one gradient update have the following scaling: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\exists\\,l\\in[L],\\ \\lVert\\mathbf{v}_{l}\\rVert_{2}=\\Theta(\\sqrt{N_{v}})\\ \\ (\\mathrm{Stability~at~initialization})}\\\\ &{\\exists\\,l\\in[L],\\ \\lVert\\Delta\\mathbf{v}_{l}\\rVert_{2}=\\Theta(\\sqrt{N_{v}})\\ \\ (\\mathrm{Non-trivial~feature~updates})}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "C Proofs ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "C.1 Structured SSMs are not covered by previous approaches ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Proposition C.1 (Structured SSMs are not generally representable as $\\mathbf{NE}{\\otimes}\\mathbf{OR}^{\\top}$ programs). The forward and backward signal propagation of structured SSMs including S4 and MAMBA trained via standard algorithms such as SGD or ADAM are not representable as a NE\u2297OR\u22a4program. Indeed this holds for all existing Hippo-parameterizations of the structured matrix A including HiPPO-LegS, HiPPO-LegS-N, HiPPO-LegS-D, S4D-Inv, S4D-Lin, and S4D-Real. ", "page_idx": 14}, {"type": "text", "text": "Proof. Recall the definition of a $\\mathbf{NE}{\\otimes}\\mathbf{OR}^{\\top}$ program (Yang and Littwin, 2023). Let A be any of the HiPPO or S4D structured matrices. As the entries of A at initialization are deterministic and differ, A can neither be an initial $\\mathbf{NE}{\\otimes}\\mathbf{OR}^{\\top}$ matrix, nor be generated by an OuterNonlin of random $\\mathbf{NE}{\\otimes}\\mathbf{OR}^{\\top}$ vectors (acting on all coordinate dimensions in the same way). This is because no allowed NE\u2297OR\u22a4operation can generate a $\\mathbf{NE}{\\otimes}\\mathbf{OR}\\,^{\\top}$ vector with differing variance in its coordinates, whereas the multiplication with A clearly produces differing variances in each entry. All that is left to do is to show this claim via induction. ", "page_idx": 14}, {"type": "text", "text": "Claim: All $\\mathbf{NE}{\\otimes}\\mathbf{OR}^{\\top}$ vectors have the same variance in each coordinate. ", "page_idx": 14}, {"type": "text", "text": "To start the induction, all entries of an initial vector $v$ are sampled iid from $N(0,1)$ . ", "page_idx": 14}, {"type": "text", "text": "Now assume all vectors $v$ currently defined in the $\\mathbf{NE}{\\otimes}\\mathbf{OR}^{\\top}$ program have the same variance. The allowed operations to generate a $\\mathbf{NE}{\\otimes}\\mathbf{OR}\\,^{\\top}$ vector are matrix multiplication with an initial matrix and OuterNonlin. As initial matrices ${\\bf A}_{0}$ have independent entries with mean 0 and variance $n^{-1}$ , the multiplication $\\mathbf{A}_{0}v$ will again generate a vector with the same variance in each coordinate. ", "page_idx": 14}, {"type": "text", "text": "An OuterNonlin operation takes in previously defined $\\mathbf{NE}{\\otimes}\\mathbf{OR}\\,^{\\top}$ vectors and treats all coordinates in the same way. Consequently it will again generate a NE\u2297OR\u22a4vector with the same variance in each coordinate. ", "page_idx": 14}, {"type": "text", "text": "There is no other way to generate a new NE\u2297OR\u22a4vector, which concludes the induction. ", "page_idx": 14}, {"type": "text", "text": "To see that the structured matrix A produces nonisotropic coordinate distributions, we detail the argument for an S4 recurrent layer. The other choices of A follow via analogous arguments. Recall that the S4 recurrent layer ${\\bf y}_{1:L}=f_{{\\cal S}4}({\\bf u}_{1:L};{\\bf w}=\\{{\\bf B},{\\bf C}\\})$ ) can be viewed as a discretization of a continuous-time SSM given by ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{d}{d t}\\mathbf{x}_{t}=\\mathbf{A}\\mathbf{x}_{t}+\\mathbf{B}\\mathbf{u}_{t}\\quad\\mathrm{and}\\quad\\mathbf{y}_{t}=\\mathsf{R e}[\\mathbf{C}\\mathbf{x}_{t}]\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "for $l=1,\\dots,L$ with $\\mathbf{x}_{0}=0$ and where $\\mathsf{R e}(\\cdot)$ gives the real part of a complex vector, and where we let $\\mathbf{A}=\\mathrm{diag}(\\mathbf{a})$ with a $\\bar{\\mathbf{\\Omega}}\\in\\mathbb{C}^{N_{x}}$ , $\\mathbf{B}\\in\\mathbb{C}^{N_{x}\\times N_{u}^{'}}$ , $\\mathbf{C}\\in\\mathbb{C}^{N_{y}\\times N_{x}^{\\mathbf{^{\\mathbf{x}}}}}$ . We have the discretized sequence to sequence mapping $f_{\\mathrm{S4}}$ as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{x}_{l}=\\mathbf{A}^{\\prime}\\mathbf{x}_{l-1}+\\mathbf{B}^{\\prime}\\mathbf{u}_{l}\\quad\\mathrm{and}\\quad\\mathbf{y}_{l}=\\mathsf{R e}[\\mathbf{C}^{\\prime}\\mathbf{x}_{l}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where we follow the Zero-Order-Hold (ZOH) discretization method, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{A}^{\\prime}=\\exp(\\tau\\cdot\\mathbf{A}),\\quad\\mathbf{B}^{\\prime}=(\\mathbf{A}^{\\prime}-\\mathbf{I})\\mathbf{A}^{-1}\\mathbf{B},\\quad{\\mathrm{~and~}}\\quad\\mathbf{C}^{\\prime}=\\mathbf{C}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "To model long-range dependency, S4 advocates for HiPPO theory (Gu et al., 2020). While there are many possible Hippo-based initializations possible for the structured transition matrices (e.g., Hippo-Leg-S, S4D-Inv, and S4D-Real) and our result holds for all the parameterizations, let us consider a simplified initialization for clarity. Following S4D-Lin, we can set $a_{n}=-{\\textstyle{\\frac{1}{2}}}+i\\pi n$ . B and $\\mathbf{C}$ are initialized such that the entries are i.i.d and follow a Gaussian distribution with mean 0 and variance $\\sigma_{B}^{2}$ and $\\sigma_{C}^{2}$ respectively (for both real and imaginary parts). ", "page_idx": 15}, {"type": "text", "text": "Assume for now that the inputs to the SSM layer are i.i.d which holds asymptotically in general say if they are outputs of a previous layer such as an MLP. Let us consider the distribution of the hidden states for token index 0 at initialization: $\\mathbf{x}_{0}=\\mathbf{B}^{\\prime}\\mathbf{u}_{0}$ , where $\\mathbf{B}^{\\prime}=(\\mathbf{A}^{\\prime}-\\mathbf{I})\\mathbf{A}^{-1}\\mathbf{B}$ , and $\\mathbf{A}^{\\prime}=\\mathrm{diag}(a_{1}^{\\prime},\\ldots,a_{N_{x}}^{\\prime})$ with $a_{n}^{\\prime}=e^{-{\\frac{1}{2}}\\tau}(\\cos(\\tau\\pi n)+i\\sin(\\tau\\pi n))$ , then the eigenvalues of $\\mathbf{A}^{\\prime}$ are clearly $\\Theta(1)$ . To further simplify, if we set $\\tau=2$ , then $\\|\\mathbf{A}^{\\prime}\\|_{*}=e^{-1}$ and for all complex vectors $\\mathbf{v}\\in\\dot{\\mathbb{C}}^{N_{x}}$ it holds that $\\mathbf{A}^{\\prime}\\mathbf{v}=\\overline{{-e^{-1}}}\\mathbf{v}$ . Since $B$ is an i.i.d Gaussian matrix, for sufficiently large width, $\\mathbf{B}\\mathbf{u}_{0}$ is distributed i.i.d by a simple Central Limit Theorem argument. However, $\\mathbf{B^{\\prime}}$ also depends on $\\mathbf{A}^{-1}=\\mathrm{diag}(a_{n}^{-1})$ with $\\begin{array}{r}{\\dot{a}_{n}^{-1}=\\dot{-}\\frac{1+2\\pi n i}{1/2+2\\pi^{2}n^{2}}}\\end{array}$ . Due to the linear decay of the diagonal entries of ${\\bf A}^{-1}$ , the hidden states $x_{0}$ already at initialization are not (even asymptotically) identically distributed. Therefore, there cannot exist a coordinate distribution that represents the entries of vectors arising in modern SSMs such as MAMBA neither at initialization nor over the course of training. ", "page_idx": 15}, {"type": "text", "text": "Proposition C.2 (Spectral scaling does not generally imply feature learning in SSMs). Structured SSMs including S4 and MAMBA trained via standard algorithms such as SGD or ADAM that satisfy spectral scaling conditions $(*)$ do not satisfy conditions for feature learning given in Equation $(\\Delta)$ . Indeed this holds for all well-known Hippo-parameterizations of the structured matrix A including HiPPO-LegS, HiPPO-LegS-N, HiPPO-LegS-D, S4D-Inv, S4D-Lin, and S4D-Real. ", "page_idx": 15}, {"type": "text", "text": "Proof. Let A be any of the HiPPO or S4D structured matrices. ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Consider the scale of the hidden states at initialization for token index 0. By definition, $\\left\\|\\mathbf{x}_{1}\\right\\|_{2}\\;=\\;\\left\\|\\mathbf{B^{\\prime}}\\mathbf{u}_{1}\\right\\|_{2}\\;=\\;\\left\\|\\mathbf{A}\\mathbf{B}\\mathbf{u}_{1}\\right\\|_{2}$ , where $\\mathbf{A}\\,=\\,(\\mathbf{A}^{\\prime}\\,-\\,\\mathbf{I})\\mathbf{A}^{-1}$ . Then, observe that $\\left\\|\\mathbf{B}^{\\prime}\\mathbf{u}_{1}\\right\\|_{2}^{2}=$ $\\sum_{i=1}^{N_{x}}\\Lambda_{i}^{2}\\Big(\\sum_{j=1}^{N_{u}}B_{i,j}u_{1,j}\\Big)^{2}$ is a sum of $N_{x}$ independent (but not identically distributed) random variables. However, it is easy to verify that they satisfy the Kolmogorov condition and therefore the sum behaves according to the strong law of large numbers. Intuitively, this is allowed by the polynomial decay of eigenvalues of $\\Lambda$ and the correct scaling of the inputs to the S4 layer. Applying the Kolmogorov SLLN (Theorem A.1), we obtain ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{N_{x}}\\Lambda_{i}^{2}\\big(\\sum_{j=1}^{N_{u}}B_{i,j}u_{1,j}\\big)^{2}\\xrightarrow{a.s_{\\cdot}}\\sigma_{B}^{2}\\left\\Vert\\mathbf{u}_{1}\\right\\Vert^{2}c\\zeta(2),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\zeta(2)$ denotes the Riemann zeta function at 2 and equates to $\\pi^{2}/6$ , and $c>0$ is some widthindependent constant. The zeta function appears due to c \u03b6(2) \u2264 iN=x1 \u039bi2 \u2264 iN=x1(i+11)2 \u2264\u03b6(2) for $N_{x}$ large enough (see (37) for more details). Here and throughout the paper, for the formal application of the LLN or the CLT, we first adequately normalize all quantities to arrive at a welldefined width-independent limit statements, but do not write out such technicalities for conciseness. ", "page_idx": 15}, {"type": "text", "text": "Observe that, for spectral scaling conditions to yield the right scaling of the initialization variance, it is crucial that the following condition holds: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left\\|\\mathbf{B}^{\\prime}\\mathbf{u}_{1}\\right\\|_{2}\\in\\Theta\\left(\\left\\|\\mathbf{B}^{\\prime}\\right\\|_{*}\\left\\|\\mathbf{u}_{1}\\right\\|_{2}\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Since the spectrum of $(\\mathbf{A}^{\\prime}\\mathrm{~-~}\\mathbf{I})\\mathbf{A}^{-1}$ is less than 1, an upper bound on the spectral norm of $\\mathbf{B}^{\\prime}\\ =\\ (\\mathbf{A}^{\\prime}\\ -\\ \\mathbf{I})\\mathbf{A}^{-1}\\mathbf{B}$ can be found in Vershynin (2011) and is given by $c(\\sqrt{N_{x}}\\,+\\,\\sqrt{N_{u}})$ for some width-independent constant $c$ . A matching lower bound can be easily found by noting that the spectral norm is lower bounded by the maximal row and column norm: $\\|\\mathbf{B}^{j}\\|_{*}\\geq$ max $\\left\\{\\operatorname*{max}_{i}\\|\\mathbf{B}_{i:}^{\\prime}\\|_{2}\\,,\\operatorname*{max}_{j}\\left\\|\\mathbf{B}_{:j}^{\\prime}\\right\\|_{2}\\right\\}\\geq\\sqrt{N_{u}}\\sigma_{B}$ . Therefore $\\begin{array}{r l r}{\\lefteqn{\\frac{\\left\\|\\mathbf{B}^{\\prime}\\mathbf{u}\\right\\|_{2}}{\\left(\\|\\mathbf{B}^{\\prime}\\|_{*}\\left\\|\\mathbf{u}_{1}\\right\\|_{2}\\right)}\\in\\Theta(\\frac{1}{\\sqrt{N_{u}}})}}\\end{array}$ which clearly violates (14). This spectral decay is induced by all of the considered choices of $A$ . ", "page_idx": 16}, {"type": "text", "text": "C.2 S6 Mamba recurrent layer ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Recall that a S6 Mamba layer ${\\bf y}_{1:L}=f_{\\mathrm{Mamba}}({\\bf u}_{1:L})$ can be written as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{B}_{l}=\\mathrm{Lin}_{N_{x}}(\\mathbf{u}_{l};\\mathbf{W}_{B},\\mathbf{b}_{B})\\in\\mathbb{R}^{N_{x}},}\\\\ &{\\mathbf{C}_{l}=\\mathrm{Lin}_{N_{y}}(\\mathbf{u}_{l};\\mathbf{W}_{C},\\mathbf{b}_{C})\\in\\mathbb{R}^{N_{x}},}\\\\ &{\\tau_{l}=\\mathrm{Softplus}(\\tau_{0}+\\mathrm{Broadcast}_{N_{u}}(\\mathrm{Lin}_{1}(\\mathbf{u}_{l};\\mathbf{W}_{\\tau},\\mathbf{b}_{\\tau})),\\ \\tau_{0},\\tau_{l}\\in\\mathbb{R}^{N_{u}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "For $i=1,\\dots,N_{u}$ , we have $N_{u}$ 1D SSMs: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{A}^{(i)}=\\mathrm{Diag}(-\\exp(\\mathbf{a}_{\\mathrm{log}}^{(i)})),\\ \\mathbf{a}_{\\mathrm{log}}^{(i)}\\in\\mathbb{R}^{N_{x}},\\quad\\mathbf{x}_{l}^{(i)}=\\mathbf{A}_{l}^{\\prime(i)}\\mathbf{x}_{l-1}^{(i)}+u_{l}^{(i)}\\mathbf{B}_{l}^{\\prime(i)},\\ u_{l}^{(i)}\\in\\mathbb{R},\\mathbf{x}_{l}^{(i)}\\in\\mathbb{R}^{N_{x}},}\\\\ {\\mathbf{A}_{l}^{\\prime(i)},\\mathbf{B}_{l}^{\\prime(i)}=\\mathrm{ZOH}(\\tau_{l}^{(i)},\\mathbf{A}^{(i)},\\mathbf{B}_{l}),\\ \\tau_{l}^{(i)}\\in\\mathbb{R},\\ \\quad\\ y_{l}^{(i)}=\\mathbf{C}_{l}\\tau_{\\mathbf{A}}^{(i)},\\ y_{l}^{(i)}\\in\\mathbb{R}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where x0 $\\mathbf{x}_{0}^{(i)}\\;=\\;\\mathbf{0},\\;\\mathbf{a}_{\\mathrm{log}}^{(i)},\\;\\tau_{0}\\;\\in\\;\\mathbb{R}^{N_{u}}$ , and all linear layer weights $\\mathbf{W}_{B},\\mathbf{b}_{B},\\mathbf{W}_{C},\\mathbf{b}_{C},\\mathbf{W}_{\\tau},\\mathbf{b}_{\\tau}$ are trainable parameters. The Zero-Order-Hold (ZOH) discretization procedure ${\\bf A}_{l}^{\\prime(i)},{\\bf B}_{l}^{\\prime(i)}\\;=$ $\\mathrm{ZOH}(\\tau,\\mathbf{A}^{(i)},\\mathbf{B}_{l}^{\\prime(i)})$ can be written as: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{A}_{l}^{\\prime(i)}=\\exp(\\tau_{l}^{(i)}\\cdot\\mathbf{A}^{(i)}),\\qquad\\mathbf{B}_{l}^{\\prime(i)}=(\\mathbf{A}_{l}^{\\prime(i)}-\\mathbf{I})\\mathbf{A}^{(i)}{}^{-1}\\mathbf{B}_{l}^{(i)}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "In a nutshell, Mamba can be seen as $N_{u}$ SSMs, one for each input channel. Weights for these SSMs are shared and depend on the input at that recurrent step. Because of the dependency on inputs, Mamba can model non-stationary sequences. Weight matrices are initialized as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{W}_{B}\\sim{\\mathcal{N}}(\\mathbf{0},\\sigma_{B}^{2}\\mathbf{I})\\in{\\mathbb{R}}^{N_{x}\\times N_{u}},\\ \\mathbf{W}_{C}\\sim{\\mathcal{N}}(\\mathbf{0},\\sigma_{C}^{2}\\mathbf{I})\\in{\\mathbb{R}}^{N_{x}\\times N_{u}},\\ \\mathbf{W}_{\\tau}\\sim{\\mathcal{N}}(\\mathbf{0},\\sigma_{\\tau}^{2}\\mathbf{I})\\in{\\mathbb{R}}^{1\\times N_{u}},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and zero initialization for all biases. For each SSM, $\\mathbf{A}^{(i)}$ is still initialized according to the HiPPO theory. When A(i) is real-valued, we let al(oi)g[j] = log(j+1) and \u03c40 can be seen a bias term initialized to $\\tau_{0}\\sim\\mathrm{Softplus}^{-1}(\\mathcal{U}(0.001,0.1))$ . ", "page_idx": 16}, {"type": "text", "text": "Below we restate Propositions 3.2 and 3.3 followed by their proofs. ", "page_idx": 16}, {"type": "text", "text": "Proposition 3.2 (Scale of hidden states $\\mathbf{x}_{1}^{(i)}$ in Mamba at initialization). Under the ZOH discretization procedure, as $N_{u}$ then $N_{x}$ approach infinity, for any $i\\in[N_{u}].$ , the squared $l_{2}$ -norm of the hidden states $\\|\\mathbf{x}_{1}^{(i)}\\|_{2}^{2}$ is a.s. scaled as $\\|\\mathbf{x}_{1}^{(i)}\\|_{2}^{2}\\in\\Theta\\left(\\zeta(2)\\,\\sigma_{B}^{2}\\,\\|\\mathbf{u}_{1}\\|^{2}\\,(u_{1}^{(i)})^{2}\\right)$ , where $\\zeta(2)$ denotes the Riemann zeta function at 2. ", "page_idx": 16}, {"type": "text", "text": "Proposition 3.3 (Scale of outputs $\\mathbf{y}_{1}^{(i)}$ of a Mamba layer at initialization). Under the ZOH discretization procedure as $N_{u}$ then $N_{x}$ approach infinity, for any $i\\in[N_{u}].$ , the output $y_{1}^{(i)}$ converges in distribution to a Gaussian with mean 0 and standard deviation $C\\sigma_{B}\\sigma_{C}\\left\\lVert\\mathbf{u}_{1}\\right\\rVert_{2}^{2}$ for some widthindependent constant $C>0$ . ", "page_idx": 16}, {"type": "text", "text": "Proofs for Proposition 3.2 and Proposition 3.3. For the following 1D linear state space model: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\mathbf{x}_{l}^{(i)}=\\mathbf{A}_{l}^{\\prime(i)}\\mathbf{x}_{l-1}^{(i)}+\\mathbf{B}_{l}^{\\prime(i)}u_{l}^{(i)},\\qquad u_{l}^{(i)}\\in\\mathbb{R},\\,\\mathbf{x}_{l}^{(i)}\\in\\mathbb{R}^{N_{x}},\\right.}\\\\ &{\\left.y_{l}^{(i)}=\\mathbf{C}_{l}^{\\top}\\mathbf{x}_{l}^{(i)},\\;y_{l}^{(i)}\\in\\mathbb{R}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Following condition (INIT), the 1D SSM admits stability at initialization, when the following condition is satisfied: ", "page_idx": 17}, {"type": "text", "text": "For simplicity of exposition, let\u2019s begin by considering the scale of $\\begin{array}{r l r}{{\\bf x}_{1}^{(i)}}&{{}=}&{{\\bf(A}_{l}^{\\prime(i)}\\mathrm{~\\boldmath~-~}}\\end{array}$ $\\mathbf{I})(\\mathbf{A}^{(i)})^{-1}\\mathbf{B}_{1}^{(i)}u_{l}^{(i)}$ before generalizing to arbitrary $l\\in[L]$ . ", "page_idx": 17}, {"type": "text", "text": "Since $\\mathbf{B_{1}}=\\mathbf{W}_{B}\\mathbf{u}_{1}+\\mathbf{b}_{B}$ with $\\mathbf{b}_{B}=\\mathbf{0}$ at initialization, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left\\|\\mathbf{x}_{1}^{(i)}\\right\\|_{2}^{2}=\\left\\|(\\mathbf{A}_{l}^{\\prime(i)}-\\mathbf{I})(\\mathbf{A}^{(i)})^{-1}\\mathbf{W}_{B}\\mathbf{u}_{1}u_{1}^{(i)}\\right\\|_{2}^{2}=\\sum_{m=1}^{N_{x}}(\\Lambda_{i})_{m,m}^{2}(u_{1}^{(i)})^{2}\\big(\\sum_{n=1}^{N_{u}}(\\mathbf{W}_{B})_{m,n}(u_{1}^{(n)})\\big)^{2},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\Lambda_{i}=(\\mathbf{A}_{l}^{\\prime(i)}-\\mathbf{I})(\\mathbf{A}^{(i)})^{-1}$ . ", "page_idx": 17}, {"type": "text", "text": "Note that the inner summation can be expressed as ", "page_idx": 17}, {"type": "equation", "text": "$$\n(\\sum_{n=1}^{N_{u}}(\\mathbf{W}_{B})_{m,n}(u_{1}^{(n)}))^{2}=\\sum_{n=1}^{N_{u}}(\\mathbf{W}_{B})_{m,n}^{2}(u_{1}^{(n)})^{2}+\\sum_{n^{\\prime}\\neq n^{\\prime\\prime}=1}^{N_{u}}(\\mathbf{W}_{B})_{m,n^{\\prime}}u_{1}^{(n^{\\prime\\prime})}(\\mathbf{W}_{B})_{m,n^{\\prime\\prime}}u_{1}^{(n^{\\prime\\prime})}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Since $\\mathbf{W}_{B}$ has i.i.d Gaussian entries and due to Assumption 3.1, as $N_{u}\\to\\infty$ , the first term behaves according to law of large numbers and converges almost surely to $\\sigma_{B}^{2}\\|\\mathbf{u}_{1}\\|^{2}$ . The second term behaves according to central limit theorem and converges in distribution to a Gaussian with mean 0 and variance \u03c34B $\\sigma_{B}^{4}\\sum_{n^{\\prime}\\neq n^{\\prime\\prime}=1}^{N_{u}}u_{1}^{(n^{\\prime})}u_{1}^{(n^{\\prime\\prime})}$ . Therefore, as $N_{u}\\to\\infty$ , $\\Big(\\sum_{n=1}^{N_{u}}(\\mathbf{W}_{B})_{m,n}\\big(u_{1}^{(n)}\\big)\\Big)^{2}$ converges to a Gaussian distribution with mean \u03c32B\u2225u1\u22252 and variance \u03c34B $\\sigma_{B}^{4}\\sum_{n^{\\prime}\\neq n^{\\prime\\prime}=1}^{N_{u}}u_{1}^{(n^{\\prime})}u_{1}^{(n^{\\prime\\prime})}$ ", "page_idx": 17}, {"type": "text", "text": "For applying Kolmogorov SLLN (Theorem A.1), note that $\\begin{array}{r c l}{c\\cdot\\zeta(2)\\!}&{\\le}&{\\!\\sum_{m=1}^{N_{x}}(\\Lambda_{i})_{m,m}^{2}\\!}\\end{array}\\le$ $\\begin{array}{r}{\\sum_{m=1}^{N_{x}}\\frac{1}{(i+1)^{2}}\\le\\zeta(2)}\\end{array}$ and $\\begin{array}{r}{c\\cdot\\zeta(4)\\leq\\sum_{m=1}^{N_{x}}(\\Lambda_{i})_{m,m}^{4}\\leq\\sum_{m=1}^{N_{x}}{\\frac{1}{(i+1)^{4}}}\\leq\\zeta(4)}\\end{array}$ for $c\\in$ $(0,1)$ and for $N_{x}$ large enough (see (37) for more details). For random variables $v_{m}\\sim\\mathcal{N}(\\sigma_{B}^{2},C^{2})$ for some $C^{2}>0$ , it holds that $V a r((\\Lambda_{i})_{m,m}^{2}v_{m})\\le\\smash{\\mathbb{E}[(\\Lambda_{i})_{\\underline{{m}},m}^{4}v_{m}^{2}]\\leq(\\Lambda_{i})_{m,m}^{4}(\\sigma_{B}^{4}+C^{2})}$ , so that the Kolmogorov condition is fulfilled, and we have that as $N_{u}^{'}$ then $N_{x}$ approach infinity, for some width-independent constant $c>0$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{m=1}^{N_{x}}(\\Lambda_{i})_{m,m}^{2}(u_{1}^{(i)})^{2}\\bigl(\\sum_{n=1}^{N_{u}}(\\mathbf{W}_{B})_{m,n}(u_{1}^{(n)})\\bigr)^{2}\\xrightarrow{a.s_{\\downarrow}}\\sigma_{B}^{2}(u_{1}^{(i)})^{2}\\left\\lVert\\mathbf{u}_{1}\\right\\rVert^{2}c\\zeta(2).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, for stability at initialization, the scale of initialization $\\sigma_{B}\\in\\Theta\\big(\\sqrt{\\frac{N_{x}}{N_{u}}}\\big)$ . ", "page_idx": 17}, {"type": "text", "text": "Scale of $y_{l}^{(i)}$ . When $l=1$ , $y_{1}^{(i)}=\\mathbf{C}_{1}\\mathbf{x}_{1}^{(i)}$ , where $\\mathbf{x}_{1}^{(i)}=\\mathbf{B}_{1}^{\\prime(i)}u_{1}^{(i)}$ and we have ", "page_idx": 17}, {"type": "equation", "text": "$$\ny_{1}^{(i)}=u_{1}^{(i)}\\sum_{m=1}^{N_{x}}(\\Lambda_{i})_{m,m}\\sum_{j,k=1}^{N_{u}}(\\mathbf{W}_{C})_{m,j}(\\mathbf{W}_{B})_{m,k}u_{1}^{(j)}u_{1}^{(k)}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Applying the Lindenberg-Feller CLT (Theorem A.2), we have that, as $N_{u}$ then $N_{x}$ approach infinity, $u_{1}^{(i)}\\sum_{m=1}^{N_{x}}(\\Lambda_{i})_{m,m}\\sum_{j,k=1}^{N_{u}}(\\mathbf{W}_{C})_{m,j}(\\mathbf{W}_{B})_{m,k}u_{1}^{(j)}u_{1}^{(k)}$ c onverges to a Gaussian distribution with mean 0 and variance $c\\,\\zeta(2)u_{1}^{(i)}\\sigma_{B}\\sigma_{C}\\left\\lVert\\mathbf{u}_{1}\\right\\rVert_{2}^{2}$ . ", "page_idx": 17}, {"type": "text", "text": "Accordingly, imposing the conditions for stability of $y_{1}^{(i)}$ requires that $\\begin{array}{r}{\\sigma_{C}\\in\\Theta(\\sqrt{\\frac{1}{N_{x}N_{u}}})}\\end{array}$ ", "page_idx": 17}, {"type": "text", "text": "Stability of xl(i) and yl(i) foFrir sat,r boitbrsaerrvye $\\textit{l}\\in\\textit{}[L]$ .e $\\textit{l}\\in\\textit{}[L]$ h awviet $\\begin{array}{r l}{\\mathbf{x}_{l}^{(i)}}&{{}=}\\end{array}$ $\\begin{array}{r}{\\sum_{m=0}^{l-1}({\\bf A}_{l}^{\\prime(i)})^{m}{\\bf B}_{l-m}^{'(i)}u_{l-m}^{(i)}}\\end{array}$ $\\mathbf{A}^{\\prime}\\ =\\ \\mathrm{diag}(a_{1}^{\\prime},\\ldots,a_{N_{x}}^{\\prime})$ $\\begin{array}{r l}{a_{n}^{\\prime}}&{{}=}\\end{array}$ $e^{-\\frac{1}{2}\\tau_{l}^{(i)}}(\\cos(\\tau_{l}^{(i)}\\pi n)+i\\sin(\\tau_{l}^{(i)}\\pi n))$ , we have that, for all complex vectors $\\textbf{v}\\in\\mathbb{C}^{N_{x}}$ it holds that for any $m\\;\\in\\;[L],\\;\\|(\\mathbf{A}^{\\prime})^{m}\\mathbf{v}\\|_{2}\\,=\\,e^{-m\\tau_{l}^{(i)}/2}\\|\\mathbf{v}\\|_{2}$ . Therefore, the operator $(\\mathbf{A}_{i}^{\\prime})^{m}$ does not change the width-scaling. Since, for any $\\mathbf{u}_{l}$ , setting $\\sigma_{B}\\in\\Theta\\big(\\sqrt{\\frac{N_{x}}{N_{u}}}\\big)$ yields $\\left\\|\\mathbf{B}_{l}^{\\prime(i)}\\mathbf{u}_{l}\\right\\|_{2}\\in\\Theta(\\sqrt{N_{x}})$ . Therefore, each term in the summation is of order $\\Theta\\big(\\sqrt{N_{x}}\\big)$ and unless, for every $l$ , the term $\\mathbf{B}_{l}^{\\prime(i)}\\mathbf{u}_{l}$ perfectly cancels out with the terms before to affect the width scaling, we have that $\\|\\mathbf{x}_{l}^{(i)}\\|\\in\\Theta(\\sqrt{N_{x}})$ . The same argument can be used to show the stability of $y_{l}^{(i)}$ . \u53e3 ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "Proposition 3.4 (Scale of the updates of hidden states $\\Delta\\mathbf{x}_{1}^{(i)}$ after 1 step of SGD). Under the ZOH discretization procedure, as $N_{x}$ then $N_{u}$ approach infinity, for every $I D$ SSM, the squared $l_{2}$ -norm of the updates $\\|\\Delta\\mathbf{x}_{1}^{(i)}\\|_{2}$ of the hidden states after one step of SGD is a.s. scaled as $\\begin{array}{r}{\\|\\Delta\\mathbf{x}_{1}^{(i)}\\|_{2}\\in\\Theta\\left(\\eta_{B}\\frac{1}{\\sqrt{N_{u}}}\\sigma_{C}\\left\\|\\mathbf{u}_{1}\\right\\|_{2}^{3}\\zeta(4)^{\\frac{1}{2}}\\right)\\!.}\\end{array}$ , where $\\zeta(4)$ denotes the Riemann zeta function at 4. ", "page_idx": 18}, {"type": "text", "text": "Proof of Proposition 3.4. Following condition $(\\Delta)$ , the features of the 1D SSM evolve non-trivially with width, when the following condition is satisfied: ", "page_idx": 18}, {"type": "text", "text": "Scale of the updates $\\Delta x_{l}^{(i)}$ after 1 SGD step. ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "First, note that, for the discretization step ZOH, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{{\\displaystyle\\bar{\\mathbf{A}}^{(i)}=\\boldsymbol{\\tau}^{(i)}\\bar{\\mathbf{A}}^{\\prime(i)}\\mathbf{A}^{\\prime(i)}+(\\boldsymbol{\\tau}^{(i)}\\mathbf{A}^{\\prime(i)}\\mathbf{A}^{(i)-1}-(\\mathbf{A}^{(i)})^{-2}(\\mathbf{A}^{\\prime(i)}-\\mathbf{I}))(\\bar{\\mathbf{B}}_{l}^{\\prime(i)}\\mathbf{B}_{l}^{(i)\\top}\\odot\\mathbf{I})}}&{{\\displaystyle\\bar{\\mathbf{x}}_{l}^{(i)}=\\bar{y}_{l}^{(i)}\\mathbf{C}_{l}^{\\top}}}\\\\ {{\\displaystyle\\bar{\\mathbf{\\sigma}}_{l==1}^{N_{u}}(\\mathbf{A}^{\\prime(i)}-\\mathbf{I})(\\mathbf{A}^{(i)})^{-1}\\bar{\\mathbf{B}}_{l}^{\\prime(i)}}}&{{\\displaystyle\\bar{\\mathbf{A}}^{\\prime(i)}=\\sum_{l=1}^{L}(\\bar{\\mathbf{x}}_{l}^{(i)}-\\mathbf{I})}}\\\\ {{\\displaystyle\\bar{\\mathbf{D}}\\mathrm{lag}(\\bar{\\mathbf{A}}_{10}^{(i)})=\\bar{\\mathbf{A}}^{(i)}\\mathbf{A}^{(i)}}}&{{\\displaystyle\\bar{\\mathbf{B}}_{l}^{\\prime(i)}=u_{l}^{(i)}\\bar{\\mathbf{x}}_{l}^{(i)}}}\\\\ {{\\displaystyle\\bar{\\mathbf{N}}_{B}=\\bar{\\mathbf{B}}_{l}\\mathbf{u}_{l}^{\\top}}}&{{\\displaystyle\\bar{\\mathbf{C}}_{l}=\\sum_{i=1}^{N_{u}}\\bar{\\mathbf{y}}_{l}^{(i)}\\mathbf{x}_{l}^{(i)}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "For any quantity $\\cdot$ , letting\u02dc\u00b7 denote the updated quantity after one step of SGD, we can write ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{\\mathbf{A}}^{(i)}=\\mathrm{Diag}(\\exp{(\\mathbf{a}_{\\mathrm{log}}^{(i)}-\\eta_{a}\\bar{\\mathbf{A}}_{\\mathrm{log}}^{(i)})})=\\mathbf{A}^{(i)}\\big(\\exp{(-\\eta_{a}\\bar{\\mathbf{A}}^{(i)}\\mathbf{A}^{(i)})}\\odot\\mathbf{I}\\big),}\\\\ &{\\ \\tilde{\\mathbf{B}}_{l}=\\tilde{\\mathbf{W}}_{B}\\tilde{\\mathbf{u}}_{l}=(\\mathbf{W}_{B}-\\eta_{B}\\bar{\\mathbf{B}}_{l}\\mathbf{u}_{l}^{T})(\\mathbf{u}_{l}+\\Delta\\mathbf{u}_{l}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "For clarity, let us again begin by considering the scale of $\\mathbf{x}_{1}^{(i)}$ . We can follow the same argumentation $\\tilde{\\mathbf{B}}_{1}^{\\prime(i)}\\tilde{u}_{1}^{(i)}=\\mathbf{B}_{1}^{\\prime(i)}u_{1}^{(i)}$ , where $\\tilde{\\mathbf{B}}_{1}^{\\prime(i)}=((\\tilde{\\mathbf{A}}^{\\prime})^{(i)}-I)(\\tilde{\\mathbf{A}}^{(i)})^{-1}\\tilde{\\mathbf{B}}_{1}$ $l\\in[L]$ $\\Delta\\mathbf{x}_{1}^{(i)}$ $\\tilde{\\mathbf{A}}^{\\prime(i)}=\\exp(\\tau\\tilde{\\mathbf{A}}^{(i)})$ . Since $u_{l}^{(i)}$ $\\tilde{u}_{l}^{(i)}$ $\\Theta(1)$ $\\Delta\\mathbf{x}_{1}^{(i)}$ of \u02dcB\u20321(i) and $\\tilde{\\mathbf{B}}_{1}^{\\prime(i)}-\\mathbf{B}_{1}^{\\prime(i)}$ . ", "page_idx": 18}, {"type": "text", "text": "Scaling of $\\tilde{\\mathbf{B}}_{l}^{\\prime(i)}$ . Recall that $\\tilde{\\mathbf{B}}_{1}^{\\prime(i)}\\ =\\ (\\tilde{\\mathbf{A}}^{\\prime(i)}\\,-\\,\\mathbf{I})(\\tilde{\\mathbf{A}}^{(i)})^{-1}\\tilde{\\mathbf{B}}_{1}\\ =\\ (\\tilde{\\mathbf{A}}^{\\prime(i)}\\,-\\,\\mathbf{I})(\\tilde{\\mathbf{A}}^{(i)})^{-1}(\\mathbf{W}_{B}\\,+\\,\\mathbf{I})(\\tilde{\\mathbf{A}}^{(i)})^{-1}\\tilde{\\mathbf{B}}_{1}\\,-\\,\\mathbf{I})(\\tilde{\\mathbf{A}}^{(i)})^{-1}\\tilde{\\mathbf{B}}_{2}\\,.$ $\\Delta\\mathbf{W}_{B})(\\mathbf{u}_{1}+\\overline{{\\Delta}}\\mathbf{u}_{1})$ . ", "page_idx": 18}, {"type": "text", "text": "Let\u2019s begin by understanding the scale of $(\\tilde{\\mathbf{A}}^{\\prime(i)}\\,-\\,\\mathbf{I})(\\tilde{\\mathbf{A}}^{(i)})^{-1}(\\Delta\\mathbf{W}_{B}\\mathbf{u}_{1})$ . Since $\\Delta\\mathbf{W}_{B}\\ =\\mathbf{\\Psi}$ $-\\eta_{B}\\bar{\\bf B}_{1}{\\bf u}_{1}^{T}$ , letting $\\tilde{\\textbf{A}}_{i}\\;\\;=\\;\\;\\mathbf{\\dot{(}}\\tilde{\\textbf{A}}^{\\prime(i)}\\;-\\;\\mathbf{I})(\\tilde{\\textbf{A}}^{(\\dot{i})})^{-1}$ , we can express the $l_{2}$ -norm of $\\left(\\tilde{\\mathbf{A}}^{\\prime(i)}\\mathbf{\\Lambda}-\\right.$ $\\mathbf{I})(\\tilde{\\mathbf{A}}^{(i)})^{-1}\\Delta\\mathbf{W}_{B}\\mathbf{u}_{1}$ as follows: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left\\|(\\tilde{\\mathbf{A}}^{\\prime(i)}-\\mathbf{I})(\\tilde{\\mathbf{A}}^{(i)})^{-1}\\Delta\\mathbf{W}_{B}\\mathbf{u}_{1}\\right\\|_{2}=\\left\\|\\mathbf{A}_{i}\\eta_{B}\\bar{\\mathbf{B}}_{1}\\mathbf{u}_{1}^{T}\\mathbf{u}_{1}\\right\\|_{2}=|\\eta_{B}|\\left\\|\\mathbf{A}_{i}\\bar{\\mathbf{B}}_{1}\\right\\|_{2}\\left\\|\\mathbf{u}_{1}\\right\\|_{2}^{2}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Note that since $\\tilde{\\mathbf{A}}_{i}\\Delta\\mathbf{W}_{B}$ is a rank-one matrix, its spectral norm can be decomposed as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left\\|\\mathbf{\\boldsymbol{\\Lambda}}_{i}\\boldsymbol{\\Delta}\\mathbf{\\mathbf{W}}_{B}\\mathbf{\\boldsymbol{u}}_{1}\\right\\|_{*}=\\left|\\eta_{B}\\right|\\left\\|\\mathbf{\\boldsymbol{\\Lambda}}_{i}\\bar{\\mathbf{B}}_{1}\\right\\|_{2}\\left\\|\\mathbf{\\boldsymbol{u}}_{1}\\right\\|_{2}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Therefore, since $\\|\\mathbf{u}_{1}\\|_{2}\\in\\Theta(\\sqrt{N_{u}})$ , if the spectral norm of $\\Lambda_{i}\\Delta\\mathbf{W}_{B}$ is scaled as $\\Theta\\big(\\sqrt{\\frac{N_{x}}{N_{u}}}\\big)$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|(\\tilde{\\mathbf{A}}^{\\prime(i)}-\\mathbf{I})(\\tilde{\\mathbf{A}}^{(i)})^{-1}\\Delta\\mathbf{W}_{B}\\mathbf{u}_{1}\\right\\|_{2}\\in\\Theta(\\sqrt{N_{x}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Claim. If $\\begin{array}{r}{\\|\\Delta\\mathbf{W}_{B}\\|_{*}\\in\\Theta(\\sqrt{\\frac{N_{x}}{N_{u}}})}\\end{array}$ $\\begin{array}{r}{\\frac{\\overline{{I_{x}}}}{\\overline{{\\lceil\\mathbf{\\Omega}}}_{u}}),\\operatorname{then}\\left\\|(\\tilde{\\mathbf{A}}^{\\prime(i)}-\\mathbf{I})(\\tilde{\\mathbf{A}}^{(i)})^{-1}\\Delta\\mathbf{W}_{B}\\mathbf{u}_{1}\\right\\|_{2}\\in\\Theta(\\sqrt{N_{x}}).}\\end{array}$ ", "page_idx": 19}, {"type": "text", "text": "Essentially, this follows if $(\\tilde{\\mathbf{A}}^{\\prime(i)}\\!-\\!\\mathbf{I})(\\tilde{\\mathbf{A}}^{(i)})^{-1}$ does not affect the width-scaling of the spectral norm of $\\Delta\\mathbf{W}_{B}$ , that is, $\\left\\Vert\\mathbf{A}_{i}\\Delta\\mathbf{W}_{B}\\right\\Vert_{*}=\\left\\Vert\\Delta\\mathbf{W}_{B}\\right\\Vert_{*}$ . This is equivalent to showing that $\\left\\|\\tilde{\\mathbf{A}}_{i}\\bar{\\mathbf{B}}_{1}\\right\\|_{2}=\\left\\|\\bar{\\mathbf{B}}_{1}\\right\\|_{2}$ . Note the following Lemma which states that the scaling of $\\|\\tilde{\\boldsymbol{\\Lambda}_{i}}\\bar{\\mathbf{B}_{1}}\\|_{2}$ is identical to that of $\\|\\mathbf{A}^{-1}\\bar{\\mathbf{B}_{1}}\\|_{2}$ . ", "page_idx": 19}, {"type": "text", "text": "Proof of Lemma C.3. Recall that $\\tilde{\\mathbf{A}}_{i}=(\\tilde{\\mathbf{A}}^{\\prime(i)}-\\mathbf{I})\\tilde{\\mathbf{A}}_{i}^{-1}$ , where $\\tilde{\\mathbf{A}}_{i}=\\mathbf{A}^{(i)}\\mathbf{Q}_{i,1}$ and ( $\\mathbf{\\widetilde{J}}_{i,1}=\\left(\\exp\\left(-\\eta_{a}\\bar{\\mathbf{A}}^{(i)}\\mathbf{A}^{(i)}\\right)\\odot\\mathbf{I}\\right)=\\exp\\left(-\\,\\eta_{a}((\\tau^{(i)}\\mathbf{A}_{l}^{\\prime(i)}-(\\mathbf{A}^{(i)})^{-1}(\\mathbf{A}_{l}^{\\prime(i)}-\\mathbf{I}))(\\bar{\\mathbf{B}}_{1}^{\\prime(i)}\\mathbf{B}_{1}^{(i)T}\\odot\\mathbf{I})\\right)$ \u2299I. Since $[\\mathbf{A}^{(i)}]_{j}\\,=\\,-(j+1)$ , it\u2019s easy to see that, for any positive value of $\\tau^{(i)}$ , the eigenvalues of $\\tau^{(i)}\\mathbf{A}_{l}^{\\prime(i)}-(\\mathbf{A}^{(i)})^{-1}(\\mathbf{A}_{l}^{\\prime(i)}-\\mathbf{I})$ are negative, in $\\Theta(1)$ , and converge to 0 from below. The expression evaluates to ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\tau^{(i)}\\mathbf{A}_{l}^{\\prime(i)}-(\\mathbf{A}^{(i)})^{-1}(\\mathbf{A}_{l}^{\\prime(i)}-\\mathbf{I})=\\tau^{(i)}\\exp(-\\tau^{(i)}(j+1))+\\frac{\\exp(-\\tau^{(i)}(j+1))-1}{(j+1)}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Since $\\bar{\\mathbf{B}}_{1}^{\\prime(i)}=u_{1}^{(i)}\\bar{\\mathbf{x}}_{1}^{(i)}$ and $\\bar{\\mathbf{x}}_{1}^{(i)}=\\bar{y}_{1}^{(i)}C_{1}^{T}$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n[\\bar{\\mathbf{B}}_{1}^{\\prime(i)}\\mathbf{B}_{1}^{(i)T}]_{j,j}=(\\sum_{m=1}^{N_{u}}[\\mathbf{W}_{B}]_{j,m}u_{1}^{(m)})(\\sum_{m^{\\prime}=1}^{N_{u}}[\\mathbf{W}_{C}]_{j,m^{\\prime}}u_{1}^{(m^{\\prime})})u_{1}^{(i)}\\bar{y}_{1}^{(i)}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Due to Assumption 3.1, $\\bar{y}_{1}^{(i)}$ is distributed i.i.d as $N_{u}\\,\\to\\,\\infty$ and $\\bar{y}_{1}^{(i)}\\,\\in\\,\\Theta(\\frac{1}{N_{u}})$ (Yang and $\\mathrm{Hu}$ , 2021). A CLT argument reveals that, under the scaling $\\sigma_{B}\\in\\Theta\\big(\\sqrt{\\frac{N_{x}}{N_{u}}}\\big)$ and $\\begin{array}{r}{\\sigma_{C}\\in\\Theta(\\sqrt{\\frac{1}{N_{x}N_{u}}})}\\end{array}$ , the eigenvalues of $(\\bar{\\mathbf{B}}_{1}^{\\prime(i)}\\mathbf{B}_{1}^{(i)T}\\odot\\mathbf{I})$ are in $\\Theta\\big(\\frac{1}{N_{u}}\\big)$ . Lemma C.4 follows as a consequence of this result. ", "page_idx": 19}, {"type": "text", "text": "Lemma C.4. As $N_{u}$ then $N_{x}$ approach infinity, $\\|\\tilde{\\mathbf{A^{(i)}}}-\\mathbf{A}^{(i)}\\|_{*}\\in\\Theta(1)$ and $\\lVert\\tilde{\\mathbf{A^{(i)}}}\\rVert_{*}\\in\\Omega(1)$ if and only if $\\eta_{a}\\in\\Theta(N_{u})$ . ", "page_idx": 19}, {"type": "text", "text": "Accordingly, all the eigenvalues of $\\mathbf{Q}_{i,1}$ are in $\\Theta(1)$ and therefore, for any complex vector $\\mathbf{v}$ , $\\|\\mathbf{Q}_{i,1}\\mathbf{v}\\|_{2}\\in\\Theta(\\|\\mathbf{v}\\|_{2})$ . Together with the fact that the spectrum of $\\mathbf{A}^{\\prime(i)}$ is $\\Theta(1)$ , we have $\\tilde{\\mathbf{A}_{i}}^{\\prime}\\in\\Theta(1)$ . Further note that the entries of $\\tilde{\\mathbf{A}^{(i)}}$ and consequently those of $\\tilde{\\Lambda_{i}}$ are independent of each other since the entries $[\\mathbf{B}_{1}^{\\overline{{\\prime}}(i)}\\mathbf{B}_{1}{}^{(i)T}]_{m,m}$ are independent for different $m\\in[N_{x}]$ .   \nSince $\\bar{\\mathbf{B}}_{1}=\\sum_{i=1}^{N_{u}}(\\mathbf{A}^{\\prime(i)}-\\mathbf{I})(\\mathbf{A}^{(i)})^{-1}\\bar{\\mathbf{B}}_{1}^{\\prime(i)}$ , it follows from Lemma C.3 that $\\left\\|\\tilde{\\mathbf{A}}_{i}\\bar{\\mathbf{B}}_{1}\\right\\|_{2}=\\left\\|\\bar{\\mathbf{B}}_{1}\\right\\|_{2}.$ . Therefore, for the correct scaling of the updates, $\\eta_{B}$ should be scaled as $\\begin{array}{r}{\\Theta\\big(\\frac{N_{x}}{\\sqrt{N_{u}}}\\big)}\\end{array}$ . Similar computations reveal that \u03b7C should be scaled as \u0398(\u221aN1N  ). ", "page_idx": 19}, {"type": "text", "text": "Proposition 3.5 (Scale of the updates of outputs $\\Delta\\mathbf{y}_{1}^{(i)}$ after 1 step of SGD). Under the ZOH discretization procedure, as $N_{u}$ then $N_{x}$ approach infinity, for every $I D$ SSM, the squared $l_{2}$ -norm of the updates of the hidden states after one step of SGD scales as $\\begin{array}{r}{|\\Delta y_{1}^{(i)}|^{2}\\in\\Theta\\left(\\eta_{C}\\sigma_{B}^{2}\\sqrt{\\frac{1}{N_{u}}}\\left\\|\\mathbf{u}_{1}\\right\\|_{2}^{4}\\right)}\\end{array}$ . ", "page_idx": 19}, {"type": "text", "text": "Before we present the proof of Proposition 3.5, we first provide the following Lemma. ", "page_idx": 20}, {"type": "text", "text": "Lemma C.5. As $N_{u}\\rightarrow\\infty,\\sum_{i=1}^{N_{u}}\\bar{y}_{1}^{(i)}u_{1}^{(i)}$ converges to a normal distribution with mean 0 and variance $N_{u}V a r(\\bar{y}_{1}^{(i)}u_{1}^{(i)})$ . Note that since we assume that the rest of the network is scaled correctly (under $\\mu P),\\;\\bar{y}_{1}^{(i)}$ is distributed i.i.d asymptotically and $\\begin{array}{r}{\\bar{y}_{1}^{(i)}\\,\\in\\,\\Theta\\big(\\frac{1}{N_{u}}\\big)}\\end{array}$ (Yang and Hu, 2021). Therefore $\\sum_{i=1}^{N_{u}}\\bar{y}_{1}^{(i)}u_{1}^{(i)}\\in\\Theta(\\frac{1}{\\sqrt{N_{u}}}).$ ", "page_idx": 20}, {"type": "text", "text": "Proof. For $i\\in[N_{u}]$ , the output of the $1D$ SSM is given by $\\boldsymbol{y}_{1}^{(i)}=\\mathbf{C}_{1}\\mathbf{x}_{1}^{(i)}\\;\\mathrm{~where~}\\;\\mathbf{C}_{1}=(\\mathbf{W}_{C}\\mathbf{u}_{1})^{T}\\;\\mathrm{~and~}\\;\\mathbf{x}_{1}^{(i)}=\\mathbf{B}_{1}^{\\prime(i)}\\boldsymbol{u}_{1}^{(i)}=(\\mathbf{A}^{\\prime(i)}-\\mathbf{I})\\mathbf{A}^{(i)^{-1}}\\mathbf{W}_{B}\\mathbf{u}_{1}.$ (20) Using the notation\u02dc\u00b7 to denote the updated quantity $\\cdot$ after 1 step of SGD, ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "equation", "text": "$$\n\\tilde{y}_{1}^{(i)}=\\tilde{\\mathbf{C}}_{1}\\tilde{\\mathbf{x}}_{1}^{(i)}=(\\tilde{\\mathbf{W}}_{C}\\tilde{\\mathbf{u}}_{1})^{T}(\\tilde{\\mathbf{A}}_{i}\\tilde{\\mathbf{W}}_{B}\\tilde{\\mathbf{u}}_{1})\\tilde{\\mathbf{u}}_{1}^{(i)}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The updated quantities $\\tilde{\\mathbf{W}}_{B}$ and $\\tilde{\\mathbf{W}}_{C}$ are derived as ", "page_idx": 20}, {"type": "text", "text": "1 $\\tilde{N}_{B}={\\bf W}_{B}+\\Delta{\\bf W}_{B},\\quad\\tilde{\\bf W}_{C}={\\bf W}_{C}+\\Delta{\\bf W}_{C}\\quad\\mathrm{with}\\quad\\Delta{\\bf W}_{B}=-\\eta_{B}\\bar{\\bf W}_{B},\\quad\\Delta{\\bf W}_{C}=-\\eta_{C}\\bar{\\bf W}_{C},$ where $\\bar{\\mathbf{W}}_{B}$ and $\\bar{\\mathbf{W}}_{C}$ denote the gradient of the loss with respect to the quantities $\\mathbf{W}_{B}$ and $\\mathbf{W}_{C}$ respectively and can be computed according to ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\bar{\\mathbf{W}}_{B}=\\sum_{i=1}^{N_{u}}{\\bar{y}_{1}^{(i)}}u_{1}^{(i)}\\mathbf{A}_{i}\\mathbf{W}_{B}\\mathbf{u}_{1}\\mathbf{u}_{1}^{\\textit{T}}\\quad\\mathrm{and}\\quad\\bar{\\mathbf{W}}_{C}=\\sum_{i=1}^{N_{u}}{\\bar{y}_{1}^{(i)}}u_{1}^{(i)}{\\mathbf{A}_{i}}{\\mathbf{C}_{1}}^{T}\\mathbf{u}_{1}^{\\textit{T}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Also, $\\tilde{\\mathbf{A}_{i}}\\,=\\,(\\tilde{\\mathbf{A}^{\\prime}}^{(i)}\\,-\\,\\mathbf{I})\\tilde{\\mathbf{A}^{(i)}}^{-1}$ , where $\\tilde{\\mathbf{A}^{(i)}}\\,=\\,\\mathrm{Diag}(\\exp{(\\mathbf{a}_{i}^{\\mathrm{log}}-\\eta_{a}\\bar{\\mathbf{A}}_{\\mathrm{log}}^{(i)})})\\,=\\,\\mathbf{A}^{(i)}\\mathbf{Q}_{i}$ , with $\\mathbf{Q}_{i}=\\left(\\exp\\left(-\\eta_{a}\\bar{\\mathbf{A}}^{(i)}\\mathbf{A}^{(i)}\\right)\\odot\\mathbf{I}\\right)$ and ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\bar{\\mathbf{A}}^{(i)}=(\\tau^{(i)}\\mathbf{A}^{\\prime(i)}(\\mathbf{A}^{(i)})^{-1}-(\\mathbf{A}^{(i)})^{-2}(\\mathbf{A}^{\\prime(i)}-\\mathbf{I}))(\\bar{\\mathbf{B}}_{1}^{\\prime(i)}\\mathbf{B}_{1}^{(i)\\top}\\odot\\mathbf{I}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Therefore $\\tilde{y}_{1}^{(i)}$ can be represented as follows ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\tilde{y}_{1}^{(i)}=\\left[(\\mathbf{W}_{C}+\\Delta\\mathbf{W}_{C})(\\mathbf{u}_{1}+\\Delta\\mathbf{u}_{1})\\right]^{T}\\left[\\tilde{\\Lambda}_{i}(\\mathbf{W}_{B}+\\Delta\\mathbf{W}_{B})(\\mathbf{u}_{1}+\\Delta\\mathbf{u}_{1})\\right](u_{1}^{(i)}+\\Delta u_{1}^{(i)}),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and the updates i)\u2212y(1i)can be expressed as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{y}_{1}^{(i)}-y_{1}^{(i)}=(\\mathbf{W}_{C}\\mathbf{u}_{1})^{T}\\tilde{\\mathbf{A}}_{i}\\mathbf{W}_{B}\\mathbf{u}_{1}u_{1}^{(i)}-(\\mathbf{W}_{C}\\mathbf{u}_{1})^{T}\\mathbf{A}_{i}\\mathbf{W}_{B}\\mathbf{u}_{1}u_{1}^{(i)}+\\mathcal{M},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\boldsymbol{\\mathcal{M}}\\,=\\,\\left[(\\mathbf{W}_{C}+\\Delta\\mathbf{W}_{C})(\\mathbf{u}_{1}+\\Delta\\mathbf{u}_{1})\\right]^{T}\\left[\\tilde{\\mathbf{A}}_{i}(\\mathbf{W}_{B}+\\Delta\\mathbf{W}_{B})(\\mathbf{u}_{1}+\\Delta\\mathbf{u}_{1})\\right](u_{1}^{(i)}+\\Delta u_{1}^{(i)}),$ (W $\\phantom{}/\\phantom{}_{C}\\mathbf u_{1}\\phantom{}^{T}\\tilde{\\Lambda}_{i}\\mathbf W_{B}\\mathbf u_{1}u_{1}^{(i)}-(\\mathbf W_{C}\\mathbf u_{1})^{T}\\Lambda_{i}\\mathbf W_{B}\\mathbf u_{1}u_{1}^{(i)}$ )represents the remainder of the terms in (23). First, let us consider the scaling of the update term without remainder term, ", "page_idx": 20}, {"type": "equation", "text": "$$\n(\\mathbf{W}_{C}\\mathbf{u}_{1})^{T}(\\tilde{\\mathbf{A}}_{i}-\\mathbf{A}_{i})\\mathbf{W}_{B}\\mathbf{u}_{1}u_{1}^{(i)}=\\sum_{m=1}^{N_{x}}K_{m,m}\\sum_{m^{\\prime},m^{\\prime\\prime}=1}^{N_{u}}\\mathbf{W}_{C m,m^{\\prime}}\\mathbf{W}_{B m,m^{\\prime\\prime}}\\mathbf{u}_{1}(^{m^{\\prime}})\\mathbf{u}_{1}(^{m^{\\prime\\prime}}),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\boldsymbol{K}=(\\tilde{\\mathbf{A}_{i}}-\\mathbf{A}_{i})$ . ", "page_idx": 20}, {"type": "text", "text": "Letting $\\mathbf{v}_{m}=\\sum_{m^{\\prime},m^{\\prime\\prime}=1}^{N_{u}}\\mathbf{W}_{C m,m^{\\prime}}\\mathbf{W}_{B_{m,m^{\\prime\\prime}}}\\mathbf{u}_{1}^{\\:(m^{\\prime})}\\mathbf{u}_{1}^{\\:(m^{\\prime\\prime})},\\mathrm{as}\\;N_{m^{\\prime}}$ $N_{u}\\to\\infty$ , $\\{\\mathbf{v}_{m}\\}_{m=1}^{N_{x}}$ are independent and are distributed normally with mean 0 and variance $\\sigma_{B}^{2}\\sigma_{C}^{2}\\Vert\\mathbf{u}_{1}\\Vert^{4}$ by an application of LindenbergFeller Central Limit Theorem. ", "page_idx": 20}, {"type": "text", "text": "Independence of entries of $\\kappa$ . Next, we show that the entries of the diagonal matrix $\\kappa$ are independent of each other. ", "page_idx": 20}, {"type": "text", "text": "Observe that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{\\cal K}=(\\tilde{\\bf A}_{i}-{\\bf A}_{i})=({\\tilde{\\bf A^{\\prime}}}(i)-{\\bf I}){\\tilde{\\bf A^{(i)}}}^{-1}-({\\bf A^{\\prime(i)}}-{\\bf I})({\\bf A}^{(i)})^{-1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\tilde{\\mathbf{A}^{(i)}}=\\mathrm{Diag}(\\exp{(\\mathbf{a}_{i}^{\\mathrm{log}}-\\eta_{a}\\bar{\\mathbf{A}}_{\\mathrm{log}}^{(i)})})=\\mathbf{A}^{(i)}\\mathbf{Q}_{i}$ , with $\\mathbf{Q}_{i}=\\left(\\exp\\left(-\\eta_{a}\\bar{\\mathbf{A}}^{(i)}\\mathbf{A}^{(i)}\\right)\\odot\\mathbf{I}\\right)$ and ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\bar{\\mathbf{A}}^{(i)}=(\\tau^{(i)}\\mathbf{A}^{\\prime(i)}(\\mathbf{A}^{(i)})^{-1}-(\\mathbf{A}^{(i)})^{-2}(\\mathbf{A}^{\\prime(i)}-\\mathbf{I}))(\\bar{\\mathbf{B}}_{1}^{\\prime(i)}\\mathbf{B}_{1}^{(i)\\top}\\odot\\mathbf{I}).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Since the entries of $\\mathbf{A}$ are deterministic, and we assume that $\\tau^{(i)}$ is some fixed deterministic scalar, the entries of $\\kappa$ are independent of each other iff the entries of $(\\bar{\\mathbf{B}}_{1}^{\\prime(i)}\\mathbf{B}_{1}^{(i)\\top}\\odot\\mathbf{I})$ are independent. Now consider ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbf{[B_{1}^{\\overline{{\\prime}}}}^{(i)}\\mathbf{B}_{1}^{(i)T}]_{m,m}=\\sum_{m^{\\prime},m^{\\prime\\prime}=1}^{N_{u}}\\mathbf{W}_{B m,m^{\\prime}}\\mathbf{W}_{C m,m^{\\prime\\prime}}\\mathbf{u_{1}}^{(m^{\\prime})}\\mathbf{u_{1}}^{(m^{\\prime\\prime})}u_{1}^{(i)}\\bar{y}_{1}^{(i)}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Since $\\mathbf{W}_{B}$ and $\\mathbf{W}_{C}$ are Gaussian matrices with i.i.d entries and the entries of $\\mathbf{u}_{1}$ are assumed to be i.i.d, the entries $[\\bar{\\mathbf{B}}_{1}^{\\prime(i)}\\mathbf{B}_{1}^{(i)T}]_{m,m}$ are independent for different $m\\in[N_{x}]$ . ", "page_idx": 21}, {"type": "text", "text": "Furthermore, following the same line of argumentation as in Lemma C.3, the matrix $\\kappa$ can be represented as $K=K^{\\prime}\\mathbf{A}^{-1}$ , where the entries of $K^{\\prime}$ are $\\Theta(1)$ and accordingly for any vector the scaling of $v$ remains invariant under the operator $K^{\\prime}$ . ", "page_idx": 21}, {"type": "text", "text": "To compute the scale of $\\sum_{m=1}^{N_{x}}\\boldsymbol{K}_{m,m}\\mathbf{v}_{m}$ , we can therefore instead consider the scaling of $\\sum_{n=1}^{N_{x}}\\mathbf{A}_{m,m}^{-1}\\mathbf{v}_{m}$ where $\\{\\mathbf{v}_{m}\\}_{m=1}^{N_{x}}$ are independent and are distributed normally with mean 0 and variance $\\sigma_{B}^{2}\\sigma_{C}^{2}\\Vert\\mathbf{u}_{1}\\Vert^{4}$ As $N_{x}~\\rightarrow~\\infty$ , $\\sum_{m=1}^{N_{x}}\\mathbf{A}_{m,m}^{-1}\\mathbf{v}_{m}$ converges to a normal distribution with mean 0 and variance $c\\,\\zeta(2)\\sigma_{B}^{2}\\sigma_{C}^{2}\\|\\mathbf{u}_{1}\\|^{4}$ , where $\\zeta(2)$ denotes the Riemann zeta function at 2 and $c\\zeta(2)>0$ evaluates to a width-independent constant. ", "page_idx": 21}, {"type": "text", "text": "When $\\sigma_{B}\\in\\Theta\\big(\\sqrt{\\frac{N_{x}}{N_{u}}}\\big)$ and $\\begin{array}{r}{\\sigma_{C}\\in\\Theta(\\frac{1}{\\sqrt{N_{x}N_{u}}})}\\end{array}$ , we get $(\\mathbf{W}_{C}\\mathbf{u}_{1})^{T}(\\tilde{\\mathbf{A}_{i}}-\\mathbf{A}_{i})\\mathbf{W}_{B}\\mathbf{u}_{1}u_{1}^{(i)}\\in\\Theta(1).$ ", "page_idx": 21}, {"type": "text", "text": "Scale of $\\mathcal{M}$ . ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Note that since $u_{1}^{(i)}$ and $\\Delta u_{1}^{(i)}$ are scalars and in order 1, the scale of $\\mathcal{M}$ is determined by that of $[(\\mathbf{W}_{C}+\\Delta\\mathbf{W}_{C})(\\mathbf{u}_{1}+\\Delta\\mathbf{u}_{1})]^{T}[\\tilde{\\mathbf{A}}_{i}(\\mathbf{W}_{B}+\\Delta\\mathbf{W}_{B})(\\mathbf{u}_{1}+\\Delta\\mathbf{u}_{1})].$ ", "page_idx": 21}, {"type": "text", "text": "Recall that both $\\mathbf{u}_{1}$ and $\\Delta{\\bf u}_{1}$ are assumed have i.i.d coordinates and have the scaling $\\|\\mathbf{u}_{1}\\|\\in\\Theta(\\sqrt{N_{x}})$ and $\\|\\Delta\\mathbf{u}_{1}\\|\\in\\Theta(\\sqrt{N_{x}}).$ . Therefore, we only need to consider the scaling of ", "page_idx": 21}, {"type": "equation", "text": "$$\n[(\\mathbf{W}_{C}+\\Delta\\mathbf{W}_{C})\\mathbf{u}_{1}]^{T}[\\tilde{\\mathbf{A}}_{i}(\\mathbf{W}_{B}+\\Delta\\mathbf{W}_{B})(\\mathbf{u}_{1})].\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "It is easy to verify that the scaling of $(\\mathbf{W}_{C}\\mathbf{u}_{1})^{T}(\\tilde{\\mathbf{M}}_{i}\\ -\\ \\mathbf{A}_{i})\\mathbf{W}_{B}\\mathbf{u}_{1}$ is the same as that of $(\\mathbf{W}_{C}\\mathbf{u}_{1})^{T}(\\tilde{\\mathbf{M}_{i}})\\mathbf{W}_{B}\\mathbf{u}_{1}u_{1}^{(i)}$ following the same arguments as above. ", "page_idx": 21}, {"type": "text", "text": "Scale of $(\\Delta\\mathbf{W}_{C}\\mathbf{u}_{1})^{T}(\\tilde{\\mathbf{M}}_{i})\\mathbf{W}_{B}\\mathbf{u}_{1}u_{1}^{(i)}.$ ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Recall that the updates $\\Delta\\mathbf{W}_{B}$ and $\\Delta\\mathbf{W}_{C}$ are defined as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\Delta\\mathbf{W}_{B}=-\\eta_{B}\\bar{\\mathbf{W}}_{B}\\quad\\mathrm{and}\\quad\\Delta\\mathbf{W}_{C}=-\\eta_{C}\\bar{\\mathbf{W}}_{C},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\bar{\\mathbf{W}}_{B}$ and $\\bar{\\mathbf{W}}_{C}$ denote the gradient of the loss with respect to the quantities $\\mathbf{W}_{B}$ and $\\mathbf{W}_{C}$ respectively and computed as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\bar{\\mathbf{W}}_{B}=\\sum_{i=1}^{N_{u}}{\\bar{y}_{1}^{(i)}}u_{1}^{(i)}\\mathbf{A}_{i}\\mathbf{W}_{B}\\mathbf{u}_{1}\\mathbf{u}_{1}^{\\textit{T}}\\quad\\mathrm{and}\\quad\\bar{\\mathbf{W}}_{C}=\\sum_{i=1}^{N_{u}}{\\bar{y}_{1}^{(i)}}u_{1}^{(i)}{\\mathbf{A}_{i}}{\\mathbf{C}_{1}}^{T}\\mathbf{u}_{1}^{\\textit{T}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "$\\mathbf{\\Delta},\\mathbf{\\Delta}\\mathbf{W}_{C}\\mathbf{u}_{1}=-\\eta_{C}\\sum_{i=1}^{N_{u}}{\\bar{y}}_{1}^{(i)}u_{1}^{(i)}\\mathbf{\\Delta}_{\\mathbf{1}}\\mathbf{W}_{B}\\mathbf{u}_{1}\\mathbf{u}_{1}^{\\mathrm{~}T}\\mathbf{u}_{1}=-\\eta_{C}\\lVert\\mathbf{u}_{1}\\rVert^{2}\\mathbf{\\Delta}_{\\mathbf{1}}\\mathbf{W}_{B}\\mathbf{u}_{1}\\sum_{i=1}^{N_{u}}{\\bar{y}}_{1}^{(i)}u_{1}^{(i)}\\left(\\mathbf{A}_{i}\\mathrm{~is~iden-~}\\frac{\\mathbf{u}_{1}}{\\omega_{1}}\\right),$ tical for all $i\\in[N_{u}])$ . ", "page_idx": 21}, {"type": "text", "text": "Therefore, letting $\\Xi=-\\eta_{C}\\|\\mathbf{u}_{1}\\|^{2}\\sum_{i=1}^{N_{u}}\\bar{y}_{1}^{(i)}u_{1}^{(i)}$ , we get ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(\\Delta\\mathbf{W}_{C}\\mathbf{u}_{1})^{T}(\\tilde{\\mathbf{A}}_{i}\\mathbf{W}_{B}\\mathbf{u}_{1})=\\Xi(\\mathbf{\\Lambda}_{i}\\mathbf{W}_{B}\\mathbf{u}_{1})^{T}(\\tilde{\\Lambda}_{i}\\mathbf{W}_{B}\\mathbf{u}_{1})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\Xi\\displaystyle\\sum_{m=1}^{N_{x}}\\mathbf{A}_{i m,m}\\tilde{\\Lambda}_{i m,m}\\displaystyle\\sum_{m^{\\prime},m^{\\prime\\prime}=1}^{N_{u}}\\mathbf{W}_{B_{m,m^{\\prime}}}\\mathbf{W}_{B_{m,m^{\\prime\\prime}}\\mathbf{u}_{1}}{(m^{\\prime})}_{\\mathbf{u}_{1}}{(m^{\\prime\\prime})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Letting $\\mathbf{w}_{m}\\,=\\,\\sum_{m^{\\prime},m^{\\prime\\prime}=1}^{N_{u}}\\mathbf{W}_{B m,m^{\\prime}}\\mathbf{W}_{B m,m^{\\prime\\prime}}\\mathbf{u}_{1}{}^{(m^{\\prime})}\\mathbf{u}_{1}{}^{(m^{\\prime\\prime})}$ , it is easy to verify that as $N_{x}\\,\\rightarrow\\,\\infty$ each $v_{m}$ is distributed normally with mean $\\sigma_{B}^{2}\\|\\mathbf{u}_{1}\\|^{2}$ and variance $\\sigma_{B}^{4}\\sum_{m^{\\prime}\\neq m^{\\prime\\prime}=1}^{N_{u}}\\mathbf{u}_{1}^{(m^{\\prime})^{2}}\\mathbf{u}_{1}^{(m^{\\prime\\prime})^{2}}.$ $\\{\\mathbf{w}_{m}\\}_{m=1}^{N_{x}}$ ", "page_idx": 22}, {"type": "text", "text": "Furthermore are independent. ", "page_idx": 22}, {"type": "text", "text": "Therefore, due to Kolmogorov\u2019s SLLN, as $N_{x}\\rightarrow\\infty$ , it holds that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sum_{m=1}^{N_{x}}\\mathbf{\\Lambda}_{{\\uppercase{\\mathbf{A}}}_{i m,m}{\\uppercase{\\mathbf{\\tilde{A}}}}_{i m,m}{\\mathbf{w}}_{B}}\\stackrel{\\mathrm{{a.s}}}{\\rightarrow}\\sigma_{B}^{2}\\|\\mathbf{u}_{1}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Combining this with the result from Lemma C.5, we conclude that as $N_{u}\\to\\infty$ then $N_{x}\\to\\infty$ , $\\begin{array}{r}{(\\Delta\\mathbf{W}_{C}\\mathbf{u}_{1})^{\\top}(\\tilde{\\mathbf{A}_{i}}\\mathbf{W}_{B}\\mathbf{u}_{1})\\in\\Theta(\\eta_{C}\\sigma_{B}^{2}\\|\\mathbf{u}_{1}\\|^{4}\\frac{1}{\\sqrt{N_{u}}}).}\\end{array}$ . ", "page_idx": 22}, {"type": "text", "text": "Therefore, for the updates $\\Delta y_{1}^{(i)}$ to be of order 1, $\\eta_{C}$ must be scaled as $\\Theta\\big(\\frac{1}{N_{x}\\sqrt{N_{u}}}\\big)$ . ", "page_idx": 22}, {"type": "text", "text": "Scale of $(\\Delta\\mathbf{W}_{C}\\mathbf{u}_{1})^{T}(\\tilde{\\mathbf{M}_{i}})\\Delta\\mathbf{W}_{B}\\mathbf{u}_{1}u_{1}^{(i)}.$ ", "page_idx": 22}, {"type": "text", "text": "Recall that the updates $\\Delta\\mathbf{W}_{B}$ and $\\Delta\\mathbf{W}_{C}$ are defined as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\Delta\\mathbf{W}_{B}=-\\eta_{B}\\bar{\\mathbf{W}}_{B}\\quad\\mathrm{and}\\quad\\Delta\\mathbf{W}_{C}=-\\eta_{C}\\bar{\\mathbf{W}}_{C},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\bar{\\mathbf{W}}_{B}$ and $\\bar{\\mathbf{W}}_{C}$ denote the gradient of the loss with respect to the quantities $\\mathbf{W}_{B}$ and $\\mathbf{W}_{C}$ respectively and computed as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\bar{\\mathbf{W}}_{C}=\\sum_{i=1}^{N_{u}}{\\bar{y}_{1}^{(i)}}u_{1}^{(i)}\\mathbf{A}_{i}\\mathbf{W}_{B}\\mathbf{u_{1}}\\mathbf{u_{1}}^{T}\\quad\\mathrm{and}\\quad\\bar{\\mathbf{W}}_{B}=\\sum_{i=1}^{N_{u}}{\\bar{y}_{1}^{(i)}}u_{1}^{(i)}{\\mathbf{A}_{i}}{\\mathbf{C_{1}}}^{T}\\mathbf{u_{1}}^{T}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Hence, we get $\\Delta\\mathbf{W}_{C}\\mathbf{u}_{1}=-\\eta_{C}\\sum_{i=1}^{N_{u}}{\\bar{y}}_{1}^{(i)}u_{1}^{(i)}\\mathbf{A}_{i}\\mathbf{W}_{B}\\mathbf{u}_{1}\\mathbf{u}_{1}^{T}\\mathbf{u}_{1}=-\\eta_{C}\\lVert\\mathbf{u}_{1}\\rVert^{2}\\mathbf{A}_{i}\\mathbf{W}_{B}\\mathbf{u}_{1}\\sum_{i=1}^{N_{u}}{\\bar{y}}_{1}^{(i)}u_{1}^{(i)}$ ( $\\Lambda_{i}$ is identical for all $i\\in[N_{u}])$ and $\\Delta\\mathbf{W}_{B}\\mathbf{u}_{1}=\\mathbf{\\Lambda}_{i}\\mathbf{W}_{C}\\mathbf{u}_{1}\\mathbf{u}_{1}{}^{T}\\sum_{i=1}^{N_{u}}\\bar{y}_{1}^{(i)}u_{1}^{(i)}$ , which yields ", "page_idx": 22}, {"type": "equation", "text": "$$\n(\\Delta\\mathbf{W}_{C}\\mathbf{u}_{1})^{T}(\\tilde{\\mathbf{A}}_{i}\\Delta\\mathbf{W}_{B}\\mathbf{u}_{1})=(\\sum_{i=1}^{N_{u}}{\\bar{y}_{1}^{(i)}{u}_{1}^{(i)}})^{2}\\eta_{C}\\eta_{B}\\left\\lVert\\mathbf{u}_{1}\\right\\rVert^{4}(\\mathbf{A}_{i}\\mathbf{W}_{B}\\mathbf{u}_{1})^{T}(\\tilde{\\mathbf{A}}_{i}\\mathbf{W}_{C}\\mathbf{u}_{1}).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Following the same line of argumentation as before, we obtain that as $N_{u}\\to\\infty$ then $N_{x}\\rightarrow\\infty$ , $\\begin{array}{r}{(\\Delta\\mathbf{W}_{C}\\mathbf{u}_{1})^{T}(\\tilde{\\mathbf{A}_{i}}\\Delta\\mathbf{W}_{B}\\mathbf{u}_{1})\\in\\Theta(\\sigma_{B}\\sigma_{C}\\eta_{B}\\eta_{C}\\|\\mathbf{u}_{1}\\|^{6}\\frac{1}{N_{u}})}\\end{array}$ . ", "page_idx": 22}, {"type": "text", "text": "Substituting $\\begin{array}{r}{\\sigma_{B}\\,\\in\\,\\Theta(\\sqrt{\\frac{N_{x}}{N_{u}}}),\\sigma_{C}\\,\\in\\,\\Theta(\\frac{1}{\\sqrt{N_{x}N_{u}}})}\\end{array}$ , $\\begin{array}{r}{\\eta_{B}\\,\\in\\,\\Theta\\big(\\frac{N_{x}}{\\sqrt{N_{u}}}\\big)}\\end{array}$ yields the same scaling: $\\eta_{C}~\\in$ \u0398(Nx\u221a1Nu ). ", "page_idx": 22}, {"type": "text", "text": "Scale of $(\\mathbf{W}_{C}\\mathbf{u}_{1})^{T}(\\tilde{\\mathbf{\\Delta}}_{i})\\Delta\\mathbf{W}_{B}\\mathbf{u}_{1}u_{1}^{(i)}.$ . Following the same steps as for the previous terms, we obtain $\\begin{array}{r}{(\\mathbf{W}_{C}\\mathbf{u}_{1})^{T}(\\tilde{\\Lambda_{i}})\\Delta\\mathbf{W}_{B}\\mathbf{u}_{1}u_{1}^{(i)}\\in\\Theta(\\eta_{B}\\frac{1}{\\sqrt{N_{u}}}\\sigma_{C}^{2}\\left\\|\\mathbf{u}_{1}\\right\\|^{2})}\\end{array}$ . This term vanishes under the scaling $\\eta_{B}\\in$ $\\begin{array}{r}{\\Theta\\big(\\frac{N_{x}}{\\sqrt{N_{u}}}\\big)}\\end{array}$ and $\\begin{array}{r}{\\sigma_{C}\\in\\Theta(\\frac{1}{\\sqrt{N_{x}N_{u}}})}\\end{array}$ . ", "page_idx": 22}, {"type": "text", "text": "C.3 Time-invariant S4 recurrent layer ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "The S4 recurrent layer ${\\bf y}_{1:L}=f_{{\\cal S}4}({\\bf u}_{1:L};{\\bf w}=\\{{\\bf B},{\\bf C}\\})$ ) is a sequence to sequence mapping: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{x}_{l}=\\mathbf{A}^{\\prime}\\mathbf{x}_{l-1}+\\mathbf{B}^{\\prime}\\mathbf{u}_{l}}\\\\ &{\\mathbf{y}_{l}=\\mathsf{R}[\\mathbf{C}^{\\prime}\\mathbf{x}_{l}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "for $l=1,\\dots,L$ with $\\mathbf{x}_{0}=0$ , which is a discretization of the continuous-time SSM: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\frac{d}{d t}}{\\bf x}_{t}={\\bf A}{\\bf x}_{t}+{\\bf B}{\\bf u}_{t}}}\\\\ {{\\displaystyle{\\bf y}_{t}={\\sf R}[{\\bf C}{\\bf x}_{t}]}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\mathsf{R}(\\cdot)$ gives the real part of a complex vector, $\\mathbf{A}=\\,\\mathrm{diag}(\\mathbf{a})$ and $\\mathbf{a}\\in\\mathbb{C}^{N_{x}}$ , $\\mathbf{B}\\,\\in\\,\\mathbb{C}^{N_{x}\\times N_{u}}$ , C \u2208CNy\u00d7Nx. ", "page_idx": 23}, {"type": "text", "text": "The Zero-Order-Hold (ZOH) discretization method gives that: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{A}^{\\prime}=\\exp(\\tau_{l}^{(i)}\\cdot\\mathbf{A})}\\\\ &{\\mathbf{B}^{\\prime}=(\\mathbf{A}^{\\prime}-\\mathbf{I})\\mathbf{A}^{-1}\\mathbf{B}}\\\\ &{\\mathbf{C}^{\\prime}=\\mathbf{C}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "To model long-range dependency, S4 advocates for HiPPO theory. While our results hold for every Hippo initialization matrix, here, we show the result for S4D-Lin and set $\\mathbf{a}_{n}=-{\\textstyle{\\frac{1}{2}}}+i\\pi n$ . Note that A is not parameterized and will not be trained. It has been observed that complex values are not essential and S4D-real with $\\mathbf{a}_{n}=-(n+1)$ may perform similarly well. ", "page_idx": 23}, {"type": "text", "text": "The backward pass for $f_{S4}$ is as follows: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{{\\bar{\\bf x}}_{l}}={\\bf C}^{\\prime}{\\boldsymbol{\\mathsf{T}}}{\\boldsymbol{\\bar{\\bf y}}}_{l}+{\\bf A}^{\\prime\\intercal}{\\bar{\\bf x}}_{l+1}}\\ ~}\\\\ {~}\\\\ {{\\displaystyle{\\bar{\\bf C}}^{\\prime}=\\sum_{l=1}^{L}{\\bar{\\bf y}}_{l}{\\bf x}_{l}^{\\intercal}}\\ ~}\\\\ {~}\\\\ {{\\displaystyle{\\bar{\\bf B}}^{\\prime}=\\sum_{l=1}^{L}{\\bar{\\bf x}}_{l}{\\bf u}_{l}^{\\intercal}}\\ ~}\\\\ {~}\\\\ {{\\displaystyle{\\bar{\\bf u}}_{l}={\\bf B}^{\\prime\\intercal}{\\bar{\\bf x}}_{l}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Since we want all learnable weight matrices to learn features, we split each S4 layer into two sublayers and demand them both to admit feature learning. The first one being the state space equation (26) and the second one being the decoder (27). ", "page_idx": 23}, {"type": "text", "text": "Claim C.1 (Scale of hidden states $\\mathbf{x}_{1}$ in S4 at initialization). Under the ZOH discretization procedure, as $N_{x}$ and $N_{u}$ approach infinity with $N_{u}/N_{x}\\,\\in\\,\\Theta(1)$ , for any $i\\,\\in\\,[N_{u}]$ , the squared $l_{2}$ -norm of the hidden states $\\left\\Vert\\mathbf{x}_{1}^{(i)}\\right\\Vert_{2}^{2}$ is almost surely scaled as $\\left\\|\\mathbf{x}_{1}^{(i)}\\right\\|_{2}^{2}\\in\\Theta\\left(\\zeta(2)\\sigma_{B}^{2}\\left\\|\\mathbf{u}\\right\\|^{2}\\right)$ , where $\\zeta(2)$ is the Riemann zeta function at 2. ", "page_idx": 23}, {"type": "text", "text": "Proof. Stability at initialization. ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "First, we show that the size of $\\mathbf{x}_{1}=\\mathbf{B}^{\\prime}\\mathbf{u}_{1}$ at initialization is $\\Theta(\\sqrt{N_{x}})$ . By definition, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left\\|\\mathbf{x}_{1}\\right\\|_{2}=\\left\\|\\mathbf{B}^{\\prime}\\mathbf{u}_{1}\\right\\|_{2}=\\left\\|\\Lambda\\mathbf{B}\\mathbf{u}_{1}\\right\\|_{2},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\Lambda=({\\bf A}^{\\prime}-{\\bf I}){\\bf A}^{-1}$ . Then, writing $\\mathbf{u}=\\mathbf{u}_{1}$ for readability, observe that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left\\|\\mathbf{B}^{\\prime}\\mathbf{u}\\right\\|_{2}^{2}=\\sum_{i=1}^{N_{x}}\\Lambda_{i}^{2}\\big(\\sum_{j=1}^{N_{u}}B_{i,j}u_{j}\\big)^{2}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "is a sum of $N_{x}$ independent but not identically distributed random variables. However, we show that they satisfy the Kolmogorov condition and therefore the sum behaves according to the strong law ", "page_idx": 23}, {"type": "text", "text": "of large numbers. Intuitively, this is allowed by the polynomial decay of eigenvalues of $\\Lambda$ and the correct scaling of the inputs to the S4 layer. ", "page_idx": 24}, {"type": "text", "text": "Verifying that the sequence satisfies Kolmogorov condition. ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "For any $i\\in[N_{x}]$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{V a r(X|\\sum_{p=1}^{N}a_{p,q})^{*}=E[\\mathcal{X}_{p}(\\sum_{q=1}^{N}a_{p,q})^{q}]-E[\\mathcal{X}_{q}(\\sum_{p=1}^{N}a_{p,q})^{q}]^{*}}\\\\ {\\leq}&{\\xi C\\frac{1}{N}(\\sum_{q=1}^{N}a_{p,q})^{\\frac{1}{p}}}\\\\ &{=-\\xi E[\\mathcal{X}_{q}(\\sum_{p=1}^{N}a_{p,q})\\mathcal{X}_{p}(\\sum_{q=1}^{N}a_{p,q})}\\\\ &{=-\\xi E[\\mathcal{X}_{p}(\\sum_{q=1}^{N}a_{p,q})\\mathcal{X}_{p}(\\sum_{q=1}^{N}a_{p,q})}\\\\ &{=\\delta\\Big((E_{p}\\sum_{q=1}^{N}a_{p,q})+E_{p,q}\\sum_{q=1}^{N}a_{p,q})^{2q}]}\\\\ &{=\\delta\\Big((E_{p}\\sum_{q=1}^{N}a_{p,q})^{\\frac{1}{p}}+E_{p,q}\\sum_{q=1}^{N}a_{p,q})}\\\\ &{\\leq\\delta\\eta\\delta\\Big(\\sum_{p=1}^{N}a_{p,q}\\sum_{q=1}^{N}a_{p,q}}\\\\ {\\sum_{p=1}^{N}V a c(\\sum_{q=1}^{N}a_{p,q})^{\\frac{1}{p}}\\sum_{q=1}^{N}\\delta\\eta\\delta\\Big(\\sum_{p=1}^{N}a_{p,q}}\\\\ &{=\\delta\\eta\\delta\\Big(\\sum_{p=1}^{N}(\\sum_{q=1}^{N}a_{p,q})+\\sum_{q=1}^{N}\\sum_{p=1}^{N}a_{p,q}}\\\\ &{\\leq\\delta\\eta\\delta\\Big)\\Big)}\\\\ &{\\leq\\delta\\eta\\delta\\Bigg(\\sum_{p=1}^{N}a_{p,q}\\sum_{q=1}^{N}(1-\\sum_{p=1}^{N}a_{p,q})}\\\\ &{=\\delta\\eta\\delta\\Big(\\sum_{p=1}^{N}a_{p,q}\\Big(\\sum_{q=1}^{N}(\\sum_{p=1}^{N}a_{p,q})}\\\\ &{\\sum_{q=1}^{N}\\sum_{p=1}^{N}a_{p,q}\\Big(\\sum_{p=1}^{N}a_{p,q}\\Big(\\sum_{q \n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where \u03b6(4) denotes the Riemann zeta function at 4 for which a closed form is given by |B42|\u00b7(42!\u03c0)4. Note that, due to the rapid decay of $\\exp(-(i+1)\\cdot\\tau_{l}^{(i)})\\to0$ with $i\\rightarrow\\infty$ , the zeta function also lower bounds the spectral sums $\\sum_{i=1}^{N_{x}}\\Lambda_{i}^{2}\\ge c\\cdot\\zeta(2)$ and $\\sum_{i=1}^{N_{x}}\\Lambda_{i}^{4}\\ge c\\cdot\\zeta(4)$ for some width-independent constant $c\\in(0,1)$ for $N_{x}$ large enough. ", "page_idx": 24}, {"type": "text", "text": "By assumption, the inputs to the S4 layer are scaled such that $\\|\\mathbf{u}\\|\\in\\Theta(\\sqrt{N_{x}})$ and therefore, as long as $\\sigma_{B}^{2}\\in\\dot{\\mathcal{O}}(1)$ the sequence of random variables satisfies the Kolmogorov condition. ", "page_idx": 24}, {"type": "text", "text": "Limiting behavior of $\\left\\|\\mathbf{B}^{\\prime}\\mathbf{u}\\right\\|_{2}^{2}$ . ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Applying SLLN, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\sum_{i=1}^{N_{x}}\\Lambda_{i}^{2}\\Bigl(\\sum_{j=1}^{N_{u}}B_{i,j}u_{j}\\Bigr)^{2}\\longrightarrow\\sum_{i=1}^{N_{x}}\\Lambda_{i}^{2}E\\Bigl[\\bigl(\\sum_{j=1}^{N_{u}}B_{i,j}u_{j}\\bigr)^{2}\\Bigr]}}\\\\ {{\\displaystyle}}\\\\ {{\\qquad=\\sum_{i=1}^{N_{x}}\\Lambda_{i}^{2}E\\bigl[\\sum_{j=1}^{N_{u}}B_{i,j}^{2}u_{j}^{2}\\bigr]}}\\\\ {{\\displaystyle}}\\\\ {{\\qquad=\\sigma_{B}^{2}\\sum_{i=1}^{N_{x}}\\Lambda_{i}^{2}\\sum_{j=1}^{N_{u}}u_{j}^{2}}}\\\\ {{\\displaystyle}}\\\\ {{\\displaystyle}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\zeta(2)$ denotes the Riemann zeta function at 2 and equates to $\\pi^{2}/6$ , and $c\\,\\in\\,(0,1)$ is some width-independent constant. ", "page_idx": 25}, {"type": "text", "text": "Spectral scaling does not yield the correct scale of initialization for $\\sigma_{B}$ . ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "For spectral scaling conditions to yield the right scaling of the initialization variance, it is crucial that the following condition holds: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\|\\mathbf{B}^{\\prime}\\mathbf{u}\\|_{2}\\in\\Theta(\\|\\mathbf{B}^{\\prime}\\|_{*}\\,\\|\\mathbf{u}\\|_{2}).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Since the spectrum of $(\\mathbf{A}^{\\prime}-\\mathbf{I})\\mathbf{A}^{-1}$ is less than 1, an upper bound on the\u221a spectral\u221a norm of $\\mathbf{B}^{\\prime}=$ $(\\mathbf{A}^{\\prime}-\\mathbf{I})\\mathbf{A}^{-1}\\mathbf{B}$ can be found in (Vershynin, 2011) and is given by $C(\\sqrt{N_{x}}\\,+\\,\\sqrt{N_{u}})$ for some width-independent constant $C$ . ", "page_idx": 25}, {"type": "text", "text": "A matching lower bound can be easily found by noting that the spectral norm is lower bounded by the maximal row and column norm. ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathbf{B}^{\\prime}\\|_{*}\\geq\\operatorname*{max}\\left\\{\\displaystyle\\operatorname*{max}_{i}\\|B_{i:\\cdot}^{\\prime}\\|_{2},\\operatorname*{max}_{j}\\|B_{i j}^{\\prime}\\|_{2}\\right\\}}\\\\ &{\\qquad\\geq\\displaystyle\\operatorname*{max}_{i}\\|B_{i:\\cdot}^{\\prime}\\|_{2}}\\\\ &{\\qquad=\\operatorname*{max}_{i}|\\Lambda_{i}|\\,\\|B_{i:}\\|_{2}}\\\\ &{\\qquad=\\displaystyle\\operatorname*{max}_{i}|\\Lambda_{i}|(\\sum_{j=1}^{N_{u}}B_{i,j}^{2})^{1/2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad j=1}\\\\ &{\\approx\\operatorname*{max}_{i}|\\Lambda_{i}|\\sqrt{N_{u}}\\sigma_{B}}\\\\ &{\\geq\\sqrt{N_{u}}\\sigma_{B}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Therefore(\u2225B\u2032\u2225\u2217\u2225u2\u22252) $\\begin{array}{r l}{\\left\\|\\mathbf{B}^{\\prime}\\mathbf{u}\\right\\|_{2}}&{}\\\\ {\\frac{\\left(\\left\\|\\mathbf{B}^{\\prime}\\right\\|_{*}\\left\\|\\mathbf{u}\\right\\|_{2}\\right)}{\\left(\\left\\|\\mathbf{B}^{\\prime}\\right\\|_{*}\\left\\|\\mathbf{u}\\right\\|_{2}\\right)}\\in\\Theta(\\frac{1}{\\sqrt{N_{u}}})}\\end{array}$ ", "page_idx": 25}, {"type": "text", "text": "Correct scale of $\\sigma_{B}$ for stability at initialization. ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We know that $\\|\\mathbf{B^{\\prime}}\\mathbf{u}\\|_{2}\\;\\in\\;\\Theta(\\sigma_{B}\\;\\|\\mathbf{u}\\|_{2})$ and since $\\left\\|\\mathbf{u}\\right\\|^{2}\\,\\in\\,\\Theta(\\sqrt{N_{u}})$ (since we assumed stablity of activations in the previous layer), imposing stability of $\\left\\|\\mathbf{B}^{\\prime}\\mathbf{u}\\right\\|_{2}$ yields the following scaling: $\\sigma_{B}\\in\\Theta\\big(\\sqrt{\\frac{N_{x}}{N_{u}}}\\big)$ . ", "page_idx": 25}, {"type": "text", "text": "Stability of $\\mathbf{x}_{l}$ for arbitrary $l\\in[L]$ . ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "For all $l\\in[L]$ , we have $\\begin{array}{r}{\\mathbf{x}_{l}=\\sum_{m=1}^{l-1}(\\mathbf{A}^{\\prime})^{m}\\mathbf{B}^{\\prime}\\mathbf{u}_{l-m}=\\mathbf{A}^{\\prime}(\\sum_{m=1}^{l-2}(\\mathbf{A}^{\\prime})^{m}\\mathbf{B}^{\\prime}\\mathbf{u}_{l-m})+\\mathbf{B}^{\\prime}\\mathbf{u}_{l}.}\\end{array}$ First, observe that since $\\mathbf{A}^{\\prime}=\\mathrm{diag}(\\mathbf{a}_{1}^{\\prime},\\ldots,\\mathbf{a}_{N_{x}}^{\\prime})$ with $\\mathbf{a}_{n}^{\\prime}=e^{-\\frac{1}{2}\\tau_{l}^{(i)}}(\\cos(\\tau_{l}^{(i)}\\pi n)+i\\sin(\\tau_{l}^{(i)}\\pi n))$ , we have that, for all complex vectors $\\mathbf{v}\\in\\mathbb{C}^{N_{x}}$ it holds that for any $\\iota\\in[L],\\|(\\mathbf{A}^{\\prime})^{m}\\mathbf{v}\\|_{2}=e^{-m\\tau_{l}^{(i)}/2}\\|\\mathbf{v}\\|_{2}$ . Therefore, the operator $(\\mathbf{A}^{\\prime})^{m}$ does not change the width-scaling. We showed earlier that for any $\\mathbf{u}_{l}$ , setting $\\sigma_{B}\\in\\Theta\\big(\\sqrt{\\frac{N_{x}}{N_{u}}}\\big)$ yields $\\left\\|\\mathbf{B}^{\\prime}\\mathbf{u}_{l}\\right\\|_{2}\\in\\Theta(\\sqrt{N_{x}})$ . Therefore, each term in the summation is of order $\\Theta(\\sqrt{N_{x}})$ and unless, for every $l$ , the term\u221a $\\mathbf{B}^{\\prime}\\mathbf{u}_{l}$ perfectly cancels out with the terms before to affect the width scaling, we have that $\\mathbf{x}_{l}\\in\\Theta(\\sqrt{N_{x}})$ for all $l\\in[L]$ . ", "page_idx": 25}, {"type": "text", "text": "Claim C.2 (Scale of output $\\boldsymbol y^{(i)}$ of S4 at initialization). Under the ZOH discretization procedure as $N_{x}$ and $N_{u}$ approach infinity with $N_{u}/N_{x}\\in\\Theta(1),$ , for any $i\\in[N_{u}]$ , the output $y_{1}^{(i)}$ converges in distribution to a Gaussian with mean 0 and standard deviation $C\\sigma_{B}\\sigma_{C}\\left\\lVert\\mathbf{u}_{1}\\right\\rVert_{2}^{2}$ for some widthindependent constant $C$ . ", "page_idx": 25}, {"type": "text", "text": "Proof. Correct scaling of updates (and learning rate) can be achieved by correctly scaling the spectral norm of the updates of weight matrices ( $\\sigma_{B}$ and $\\sigma_{C}$ ). ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "To give concrete update/learning rate scaling rules, we need to choose a concrete update rule. We first consider SGD as an example. First, we show that while spectral scaling of weight matrices does ", "page_idx": 25}, {"type": "text", "text": "not imply the correct scaling of (pre-)activations, spectral scaling conditions on the updates of weight matrices imply the correct scaling of the activation updates. ", "page_idx": 26}, {"type": "text", "text": "To simplify cumbersome notation, lets first consider the scale of updates $\\mathbf{x}_{1}=\\mathbf{B}^{\\prime}\\mathbf{u}_{1}$ after a single step of SGD on $((\\mathbf{u}_{1},y_{1}))$ . ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\left\\|\\Delta\\mathbf{B^{\\prime}}\\mathbf{u}_{1}\\right\\|_{2}=\\left\\|\\Lambda\\Delta\\mathbf{B}\\mathbf{u}_{1}\\right\\|_{2}=\\left\\|\\Lambda\\eta\\bar{\\mathbf{x}}_{1}\\mathbf{u}_{1}^{T}\\mathbf{\\bar{u}}_{1}\\right\\|_{2}=|\\eta|\\left\\|\\mathbf{u}_{1}\\right\\|_{2}^{2}\\left\\|\\Lambda\\bar{\\mathbf{x}}_{1}\\right\\|_{2}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Since the update matrix $\\Delta\\mathbf{B}^{\\prime}$ is a rank one matrix, we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\left\\|\\Delta\\mathbf{B}^{\\prime}\\right\\|_{*}=\\left\\|\\Lambda\\Delta\\mathbf{B}\\right\\|_{*}=\\eta\\left\\|(\\Lambda\\bar{\\mathbf{x}}_{1})\\mathbf{u}_{1}^{T}\\right\\|_{*}=|\\eta|\\left\\|\\mathbf{u}_{1}\\right\\|_{2}\\left\\|\\Lambda\\bar{\\mathbf{x}}_{1}\\right\\|_{2}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "The updates $\\Delta{\\bf x}_{1}$ are given by $\\Delta\\mathbf{x}_{1}=(\\mathbf{B}^{\\prime}+\\Delta\\mathbf{B}^{\\prime})(\\mathbf{u}_{1}+\\Delta\\mathbf{u}_{1})-\\mathbf{B}^{\\prime}\\mathbf{u}_{1}=\\mathbf{B}^{\\prime}\\Delta\\mathbf{u}_{1}+\\Delta\\mathbf{B}^{\\prime}\\mathbf{u}_{1}+$ \u2206B\u2032\u2206u1. ", "page_idx": 26}, {"type": "text", "text": "If the spectral norm of $\\Delta\\mathbf{B}^{\\prime}$ is scaled as $\\Theta(\\sqrt{\\frac{N_{x}}{N_{u}}})$ , then $\\Delta\\mathbf{B}^{\\prime}\\mathbf{u}_{1}\\,\\in\\,\\Theta(\\sqrt{N_{x}})$ . We also have that $\\Delta\\mathbf{B}^{\\prime}\\Delta\\mathbf{u}_{1}\\leq\\|\\Delta\\mathbf{B}^{\\prime}\\|_{*}\\,\\|\\Delta\\mathbf{u}_{1}\\|_{2}\\in O(\\sqrt{N_{x}})$ . Since $\\lVert\\Delta\\mathbf{u}_{1}\\rVert_{2}\\in\\Theta(\\sqrt{N_{u}})$ , when $\\sigma_{B}$ is set to $\\Theta\\big(\\sqrt{\\frac{N_{x}}{N_{u}}}\\big)$ , $\\mathbf{B}^{\\prime}\\Delta\\mathbf{u}_{1}\\in\\Theta(\\sqrt{N_{x}})$ . The\u221arefore, unless the scale of $\\Delta\\mathbf{B}^{\\prime}\\mathbf{u}_{1}$ perfectly cancels out with the remaining two terms, $\\|\\Delta\\mathbf{x}_{1}\\|\\in\\Theta(\\sqrt{N_{x}})$ . ", "page_idx": 26}, {"type": "text", "text": "Generalizing to more gradient steps (and $l>1$ ) follows immediately if we assume that updates do not perfectly cancel out initial quantities, combined with the fact that $\\mathbf{A}^{\\prime}$ does not change the width scaling. \u53e3 ", "page_idx": 26}, {"type": "text", "text": "D Additional Experiments ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We conducted additional experiments to validate our theoretical derivations and to compare with baseline parameterizations for SSMs. Focusing exclusively on the SSM components, as parameterization for other parts of the model are already addressed in Yang and Hu (2021), Yang et al. (2023a), we decoupled the learning rates for SSM and non-SSM layers. First, we tuned the learning rate for the non-SSM layers and then evaluated test performance across various SSM learning rates, using the optimal non-SSM learning rate. The results, presented in Figure 4, show that the $\\mu\\mathrm{P-}\\mathrm{SSM}$ parameterization exhibits better monotonicity and stability. ", "page_idx": 26}, {"type": "image", "img_path": "aQv5AbN1wF/tmp/57f97a9078c42b9afc888bae334c714287953173bad7ec7f6935841b32f002a0.jpg", "img_caption": ["Figure 4: Decoupled learning rates. Test loss against SSM learning rate in Mamba with varying widths ( $N_{u}$ and $N_{x}$ ) on the 4M tokens sampled from the WikiText-103 dataset. The learning rate for non-SSM components is fixed and chosen via hyper-parameter tuning. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "In Figure 5, we present results on a randomly sampled subset of the Fineweb dataset. While computational constraints prevented us from training on the entire dataset or using larger model widths, at small scales, our observations on Fineweb align with the wikitext-103 results reported in the main paper. ", "page_idx": 26}, {"type": "image", "img_path": "aQv5AbN1wF/tmp/7bc92a3d82c9d9ce612a7588bd0c43d51b8b7f64d96b5012aadced3f62de7757.jpg", "img_caption": ["Figure 5: Results on the FineWeb dataset. Test loss against SSM learning rate in Mamba with varying widths $N_{u}$ and $N_{x}$ ) on the 20M tokens from the FineWeb dataset. "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We state all our contributions while acknowledging related work. All main claims are theoretically proven or empirically verified. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 28}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: Limitations are discussed in Section 6. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 28}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We state all assumptions in the main paper and provide all formal proofs in the appendix. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 29}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 29}, {"type": "text", "text": "Answer:[Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: Experimental details relevant for reproducing the results to the extent that it affects the main claims are disclosed in the main paper. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 29}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 30}, {"type": "text", "text": "Answer: [No] ", "page_idx": 30}, {"type": "text", "text": "Justification: We only propose a correction of the width-dependent scaling of hyperparameters of existing architectures, and precisely specify the corrected scalings in the main paper. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 30}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 30}, {"type": "text", "text": "Answer: [No] ", "page_idx": 30}, {"type": "text", "text": "Justification: Some experimental details are disclosed in Section 5 but not all details. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 30}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: In Figure 1, we report $\\sigma$ -confidence intervals over 10 runs. Due to limited computational resources, we were unable to repeat the large-scale experiment for Figure 3 multiple times. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 30}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 31}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [No] ", "page_idx": 31}, {"type": "text", "text": "Justification: All experiments run within 24 hours on 24 NVIDIA A10G GPUs. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 31}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We provide a theoretical analysis of widely used architectures and propose a correction. We do not foresee any ethical concerns. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 31}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: This paper provides fundamental research toward understanding and improving existing neural architectures. We do not release any model or data. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 31}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 32}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: We only use standard datasets in our experiments and do not release any data or models. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 32}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We cite the code and data assets according to standard practice. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 33}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 33}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}]