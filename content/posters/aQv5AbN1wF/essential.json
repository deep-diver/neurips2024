{"importance": "This paper is crucial for researchers working with **state-space models (SSMs)**, a rapidly growing area in deep learning. It addresses critical scaling challenges in SSMs, offering **novel scaling rules** that improve stability, generalization, and the transferability of hyperparameters.  These findings are highly relevant to ongoing research trends and open new avenues for investigation in SSM design and training.", "summary": "Unlocking the scaling secrets of structured state-space models, this research identifies novel scaling rules for improved stability, generalization, and hyperparameter transferability, revolutionizing deep learning.", "takeaways": ["Established scaling rules like Maximal Update Parameterization fail for SSMs due to their non-representability as Tensor Programs.", "A novel scaling (\u00b5P-SSM) is proposed for SSMs, enabling non-trivial feature evolution in the infinite-width limit.", "\u00b5P-SSM improves SSM stability, generalization, and facilitates hyperparameter transfer from small to large models."], "tldr": "State-space models (SSMs) are gaining popularity as alternatives to transformers in deep learning, but their scaling behavior, particularly concerning feature learning, is poorly understood.  Existing scaling rules, often effective in other architectures, fail to address the unique challenges posed by the sequential nature and internal dynamics of SSMs.  This lack of understanding hinders efficient training and limits the application of SSMs to large-scale problems.  This is due to issues such as vanishing gradients and instability during training which impede feature learning as network width increases.\nThis paper offers a rigorous analysis of signal propagation (both forward and backward) within SSMs to derive an appropriate scaling rule.  The proposed scaling, named \u00b5P-SSM, is theoretically justified and empirically shown to significantly improve model stability and generalization. Unlike previous methods, \u00b5P-SSM enables non-trivial feature evolution even as network width approaches infinity. This improved scaling also facilitates the transfer of optimal hyperparameters from smaller SSMs to larger ones. This addresses a major limitation in scaling neural networks in general.", "affiliation": "AGI Foundations", "categories": {"main_category": "AI Theory", "sub_category": "Generalization"}, "podcast_path": "aQv5AbN1wF/podcast.wav"}