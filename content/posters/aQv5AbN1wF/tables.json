[{"figure_path": "aQv5AbN1wF/tables/tables_4_1.jpg", "caption": "Table 1: Overview of the different parameterizations and their corresponding scaling for latent states, outputs and their updates. Results for ZOH and Euler discretization are separated by |.", "description": "This table summarizes the scaling behavior of latent states, output signals, and their updates under different parameterizations (Standard Parameterization (SP), Maximal Update Parameterization (\u00b5P) (heuristic), and the proposed \u00b5P-SSM) for structured state-space models (SSMs). It compares the scaling behavior for two different discretization methods (Zero-Order Hold (ZOH) and Euler).  The table shows how the norms of latent states (||x||2), output signals (||y1||2), and their updates (||\u2206x||2, ||\u2206y1||2) scale with respect to the network width (Nx and Nu) under each parameterization and discretization scheme.  This highlights which parameterization leads to stable and non-trivial feature learning in SSMs.", "section": "Identifying the unique scaling for effective feature learning in SSMS"}]