{"references": [{"fullname_first_author": "Jean-Baptiste Alayrac", "paper_title": "Flamingo: a visual language model for few-shot learning", "publication_date": "2022-12-01", "reason": "This paper introduces Flamingo, a visual language model that serves as a foundational model for several other models in this paper and is directly referenced as inspiration for the proposed approach."}, {"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This paper is highly influential in the field of large language models (LLMs), which are central to the current work on video-language streaming."}, {"fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-12-01", "reason": "This work details training language models to follow instructions, which is highly relevant to the task of creating an online video assistant."}, {"fullname_first_author": "Joya Chen", "paper_title": "VideoLLM-online: Online video large language model for streaming video", "publication_date": "2024-01-01", "reason": "This paper introduces VideoLLM-online, a key model that is directly compared with and improved upon in this paper."}, {"fullname_first_author": "William Fedus", "paper_title": "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity", "publication_date": "2022-01-01", "reason": "This paper introduces the Mixture-of-Experts (MoE) and Mixture-of-Depth (MoD) concepts, which inspire the core approach proposed in this paper."}]}