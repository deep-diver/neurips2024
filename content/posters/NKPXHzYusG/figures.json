[{"figure_path": "NKPXHzYusG/figures/figures_1_1.jpg", "caption": "Figure 1: Comparison of efficient processing challenges in offline and online video. Online video introduces distinct challenges due to the need for real-time processing as frames stream continuously.", "description": "This figure compares the challenges of processing videos in offline versus online settings. Offline video processing methods often involve sparse sampling of frames or attention-based merging, which can lead to incomplete contexts or high latency.  Online video processing, on the other hand, requires dense frame processing and real-time responses, necessitating more efficient approaches.  The figure highlights three key challenges of transitioning from offline to online video processing: maintaining causal context, managing the heavy computational cost and latency, and preventing performance degradation.", "section": "1 Introduction"}, {"figure_path": "NKPXHzYusG/figures/figures_1_2.jpg", "caption": "Figure 2: Training Computation Cost. VIDEOLLM-MOD exhibits greater efficiency compared to the baseline.", "description": "This figure shows a comparison of the training computation cost and memory usage between the baseline model and VIDEOLLM-MOD. The x-axis represents the training memory in GB, and the y-axis represents the single forward pass time in seconds. The baseline model shows a steep increase in computation cost and memory consumption as the number of vision tokens increases, while VIDEOLLM-MOD demonstrates significantly better efficiency with approximately 1.5x speedup and 0.3x GPU memory savings. This highlights the effectiveness of VIDEOLLM-MOD in reducing the computational burden associated with processing numerous vision tokens in long-term or streaming video.", "section": "1 Introduction"}, {"figure_path": "NKPXHzYusG/figures/figures_4_1.jpg", "caption": "Figure 3: VIDEOLLM-MOD selects the top-k vision tokens within each frame in certain layers via LayerExpert. We observe that performance drops dramatically with Early-exit as critical vision tokens miss subsequent processing. By retaining crucial vision tokens in certain layers and reducing redundant tokens that may mislead understanding, VIDEOLLM-MOD achieves better performance with significantly lower computation costs compared to Full-computation baseline.", "description": "This figure illustrates the three different approaches for processing vision tokens in VIDEOLLM-MOD compared to a baseline.  The first is the full computation of every vision token in every layer. The second, 'Early Exit', stops computing tokens early in the layers which causes a loss of important information and performance. The third, 'Top-k route via LayerExpert', intelligently selects and processes only the most crucial vision tokens per frame at certain layers, preserving performance and significantly reducing computation costs compared to the full computation method.", "section": "3 Method"}, {"figure_path": "NKPXHzYusG/figures/figures_5_1.jpg", "caption": "Figure 4: Efficiency analysis of VIDEOLLM-MOD in both training and inference phase.", "description": "This figure shows a comparison of VIDEOLLM-MOD and the baseline model in terms of computational cost and GPU memory usage during both training and inference.  The left subplot (a) illustrates how the training FLOPs (floating-point operations) of VIDEOLLM-MOD scale with the vision keep ratio (r), demonstrating a significant reduction in computational cost compared to the baseline (0.6x FLOPs). The right subplot (b) highlights the improvement in memory efficiency during inference, enabling VIDEOLLM-MOD to handle 1.7 times longer videos than the baseline by saving on the KV cache storage of historical states.", "section": "4 Experiments"}, {"figure_path": "NKPXHzYusG/figures/figures_5_2.jpg", "caption": "Figure 4: Efficiency analysis of VIDEOLLM-MOD in both training and inference phase.", "description": "This figure shows a comparison of VIDEOLLM-MOD and the vanilla model in terms of training FLOPs and inference GPU memory usage.  Panel (a) shows that VIDEOLLM-MOD requires only 0.6x the FLOPs of the vanilla model during training, demonstrating its efficiency.  Panel (b) shows that VIDEOLLM-MOD supports 1.7x longer video contexts than the vanilla model during inference, indicating its ability to handle long videos effectively.  This improved efficiency is due to VIDEOLLM-MOD's ability to skip redundant computation on vision tokens, resulting in significant memory savings.", "section": "4 Experiments"}, {"figure_path": "NKPXHzYusG/figures/figures_9_1.jpg", "caption": "Figure 3: VIDEOLLM-MOD selects the top-k vision tokens within each frame in certain layers via LayerExpert. We observe that performance drops dramatically with Early-exit as critical vision tokens miss subsequent processing. By retaining crucial vision tokens in certain layers and reducing redundant tokens that may mislead understanding, VIDEOLLM-MOD achieves better performance with significantly lower computation costs compared to Full-computation baseline.", "description": "This figure illustrates the VIDEOLLM-MOD approach and compares it to other methods for processing vision tokens in video.  It shows how VIDEOLLM-MOD's selective token processing (top-k route) leads to better performance and efficiency compared to full computation, where all tokens are processed in all layers, and early exit, which drops computation in later layers.  The selective processing avoids losing important information while saving computation.", "section": "3.2 Scale up Vision Resolution for Online Video"}, {"figure_path": "NKPXHzYusG/figures/figures_9_2.jpg", "caption": "Figure 5: Cases of VIDEOLLM-MOD on Ego4D GoalStep [70] video data.", "description": "This figure shows a comparison of the performance of VIDEOLLM-MOD and VideoLLM-online+ (an improved baseline) on the Ego4D GoalStep dataset. The top row shows the video frames, and the bottom two rows show the results of each method. The results show that VIDEOLLM-MOD is able to correctly identify actions and provide more accurate narrations than VideoLLM-online+, even though it uses fewer computational resources. This demonstrates the effectiveness of VIDEOLLM-MOD for online video understanding.", "section": "4.2 Online Experiments"}, {"figure_path": "NKPXHzYusG/figures/figures_17_1.jpg", "caption": "Figure 3: VIDEOLLM-MOD selects the top-k vision tokens within each frame in certain layers via LayerExpert. We observe that performance drops dramatically with Early-exit as critical vision tokens miss subsequent processing. By retaining crucial vision tokens in certain layers and reducing redundant tokens that may mislead understanding, VIDEOLLM-MOD achieves better performance with significantly lower computation costs compared to Full-computation baseline.", "description": "This figure illustrates three different approaches for processing vision tokens in a video-language model: full computation, early exit, and top-k routing via LayerExpert.  The full computation approach processes all vision tokens in all layers. Early exit skips processing of a portion of vision tokens in intermediate layers which often leads to a performance degradation. The top-k routing via LayerExpert, which is VIDEOLLM-MOD's approach, dynamically selects only the most critical vision tokens within each frame for processing in specific layers.  This approach reduces computation cost without significantly harming model performance because it preserves essential visual information while eliminating redundant ones.", "section": "3 Method"}]