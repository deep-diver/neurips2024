[{"figure_path": "NKPXHzYusG/tables/tables_6_1.jpg", "caption": "Table 1: Online experiments on the Ego4D narration benchmark. VIDEOLLM-MOD achieves comparable metrics to the Full-computation baseline with less computation cost.", "description": "This table presents the results of online experiments conducted on the Ego4D narration benchmark.  It compares the performance of VIDEOLLM-MOD against several baseline methods, including VideoLLM-online, a full-computation model, an EarlyExit approach, and a LayerSkip approach. The metrics used for comparison are Language Model Perplexity (LM-PPL), Time Difference (TimeDiff), Fluency, and LM-Correctness. The table also shows the FLOPS (floating-point operations per second) and training time for each method, highlighting the computational efficiency gains achieved by VIDEOLLM-MOD.", "section": "4.2 Online Experiments"}, {"figure_path": "NKPXHzYusG/tables/tables_7_1.jpg", "caption": "Table 2: Ablations on the insertion strategy of LayerExpert in transformer layers. The Interleaved strategy strikes the best trade-off among the variants.", "description": "This ablation study examines the impact of different LayerExpert insertion strategies on model performance.  The \"All\" strategy inserts LayerExpert in every layer, while \"All-Deep\" only inserts it in the deeper layers.  The \"Interleaved\" strategy inserts LayerExpert in every other layer, and \"Interleaved-Deep\" does the same but only in deeper layers.  The table shows that the Interleaved strategy provides the best balance between performance (Fluency and LM-Correctness) and computational cost (FLOPs).", "section": "4.3 Ablation Study"}, {"figure_path": "NKPXHzYusG/tables/tables_7_2.jpg", "caption": "Table 3: Ablations on different vision selection strategies. Choosing which vision tokens to process is crucial for efficient vision computation allocation.", "description": "This table presents the ablation study on different vision token selection strategies used in VIDEOLLM-MOD.  It compares the performance (LM-PPL, TimeDiff, Fluency, LM-Correctness) and computational cost (FLOPs) across three strategies: random selection, uniform selection, and the learnable approach (LayerExpert).  The learnable method, which dynamically selects tokens based on importance, demonstrates the best performance while balancing efficiency.  Different keep ratios (r) are also tested within the learnable method to demonstrate performance across different levels of vision token sparsity.", "section": "4.3 Ablation Study"}, {"figure_path": "NKPXHzYusG/tables/tables_7_3.jpg", "caption": "Table 4: Results on COIN benchmarks (left to right): step recognition, task recognition, next forecasting, procedure forecasting, procedure forecasting with a goal.", "description": "This table presents the results of the VIDEOLLM-MOD model and several baseline models on six common benchmarks from the COIN dataset.  The benchmarks assess the model's performance on various tasks related to instructional videos, including step recognition, task recognition, and several forecasting tasks. The \"Not use HowTo100M\" column indicates whether the HowTo100M dataset was used in training.", "section": "4.4 Offline Experiments"}, {"figure_path": "NKPXHzYusG/tables/tables_8_1.jpg", "caption": "Table 1: Online experiments on the Ego4D narration benchmark. VIDEOLLM-MOD achieves comparable metrics to the Full-computation baseline with less computation cost.", "description": "This table presents the results of online experiments conducted on the Ego4D narration stream benchmark.  It compares the performance of VIDEOLLM-MOD against several baseline methods, including the full-computation model (which processes all vision tokens), early exit, and LayerSkip. The metrics used for comparison include Language Modeling Perplexity (LM-PPL), Time Difference (TimeDiff), Fluency, and LM-Correctness.  The table highlights that VIDEOLLM-MOD achieves similar performance to the full-computation model but at a significantly reduced computational cost.", "section": "4.2 Online Experiments"}, {"figure_path": "NKPXHzYusG/tables/tables_8_2.jpg", "caption": "Table 1: Online experiments on the Ego4D narration benchmark. VIDEOLLM-MOD achieves comparable metrics to the Full-computation baseline with less computation cost.", "description": "This table presents the results of online experiments conducted on the Ego4D narration benchmark.  Several models are compared, including a baseline with full computation and the proposed VideoLLM-MoD. Metrics shown include language model perplexity (LM-PPL), time difference (TimeDiff), fluency, and LM correctness. The table highlights VideoLLM-MoD's ability to achieve comparable performance to the full-computation model while significantly reducing computational cost, indicating improved efficiency.", "section": "4.2 Online Experiments"}, {"figure_path": "NKPXHzYusG/tables/tables_8_3.jpg", "caption": "Table 7: Results on general image benchmarks.", "description": "This table presents the results of the proposed VIDEOLLM-MOD and the baseline method LLaMA-VID on four general image benchmarks: GQA, MME, POPE, and SQA.  It compares the performance (accuracy scores) of both models while highlighting the significant reduction in training cost achieved by VIDEOLLM-MOD (5.8 TFLOPs and 10.5 hours compared to 9.8 TFLOPs and 52.5 hours for LLaMA-VID). This demonstrates the efficiency gains offered by VIDEOLLM-MOD without compromising performance.", "section": "4.5 Experiments on General Benchmarks"}, {"figure_path": "NKPXHzYusG/tables/tables_8_4.jpg", "caption": "Table 8: Results on general video benchmarks.", "description": "This table presents the results of the proposed VIDEOLLM-MOD model and the baseline model LLaMA-VID on three common video question answering benchmarks: MSVD-QA, MSRVTT-QA, and ActivityNet-QA.  The results show accuracy (Acc) and score for each benchmark and highlight the improvement in performance and training efficiency achieved by VIDEOLLM-MOD compared to LLaMA-VID.", "section": "4.5 Experiments on General Benchmarks"}, {"figure_path": "NKPXHzYusG/tables/tables_18_1.jpg", "caption": "Table 1: Online experiments on the Ego4D narration benchmark. VIDEOLLM-MOD achieves comparable metrics to the Full-computation baseline with less computation cost.", "description": "This table presents the results of online experiments conducted on the Ego4D narration benchmark, comparing the performance of different methods.  It shows that VIDEOLLM-MOD achieves comparable results to the full computation method, but with significantly reduced computational cost.  The methods compared include the VideoLLM-online baseline, Full-computation, EarlyExit and LayerSkip. The metrics used for comparison include Language Modeling Perplexity (LM-PPL), Time Difference (TimeDiff), Fluency, and Language Model Correctness (LM-Correctness).", "section": "4.2 Online Experiments"}]