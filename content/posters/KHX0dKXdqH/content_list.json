[{"type": "text", "text": "Causal Imitation for Markov Decision Processes: a Partial Identification Approach ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Kangrui Ruan\u2217 Columbia University kr2910@columbia.edu ", "page_idx": 0}, {"type": "text", "text": "Junzhe Zhang\u2217 Syracuse University jzhan403@syr.edu ", "page_idx": 0}, {"type": "text", "text": "Xuan Di Columbia University sharon.di@columbia.edu ", "page_idx": 0}, {"type": "text", "text": "Elias Bareinboim Columbia University eb@cs.columbia.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Imitation learning enables an agent to learn from expert demonstrations when the performance measure is unknown and the reward signal is not specified. Standard imitation methods do not generally apply when the learner and the expert\u2019s sensory capabilities mismatch and demonstrations are contaminated with unobserved confounding bias. To address these challenges, recent advancements in causal imitation learning have been pursued. However, these methods often require access to underlying causal structures that might not always be available, posing practical challenges. In this paper, we investigate robust imitation learning within the framework of canonical Markov Decision Processes (MDPs) using partial identification, allowing the agent to achieve expert performance even when the system dynamics are not uniquely determined from the confounded expert demonstrations. Specifically, first, we theoretically demonstrate that when unobserved confounders (UCs) exist in an MDP, the learner is generally unable to imitate expert performance. We then explore imitation learning in partially identifiable settings \u2014 either transition distribution or reward function is non-identifiable from the available data and knowledge. Augmenting the celebrated GAIL method (Ho & Ermon, 2016), our analysis leads to two novel causal imitation algorithms that can obtain effective policies guaranteed to achieve expert performance. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Children often learn how to behave in an unfamiliar environment by imitating adults. Imitation learning (IL) enables a learning agent to behave in an unknown environment by observing expert demonstrations. It provides a viable approach for policy learning from demonstrations when the reward function is not fully known and reward signals are not specified [2, 30, 8, 20, 31]. Imitation learning has been widely applied across disciplines, such as autonomous driving [13, 39], robotics [18], natural language processing [10, 11, 40, 41], and chronic disease management [52, 44]. ", "page_idx": 0}, {"type": "text", "text": "It has been acknowledged in the literature that imitation learning could face significant challenges when unobserved confounding bias in expert demonstrations cannot be ruled out a priori [17, 57, 27, 42]. For illustration with simplicity, consider a Multi-Armed Bandit (MAB) model [28] described in Fig. $1;X\\in\\{0,1\\}$ is a binary action, and ", "page_idx": 0}, {"type": "image", "img_path": "KHX0dKXdqH/tmp/48cf21fcbc4881179ca183cc843d0bd3b57efb4140a2501da06ef7cd8dab59c6.jpg", "img_caption": ["Figure 1: A multi-armed bandit model. "], "img_footnote": [], "page_idx": 0}, {"type": "table", "img_path": "KHX0dKXdqH/tmp/1510e14972ea7aa738d54650a701654b872a3a28d2bac3e6ebb1820c8a875410.jpg", "table_caption": ["Table 1: Summary of main contributions in this paper, including the analysis and proposed algorithms. "], "table_footnote": [], "page_idx": 1}, {"type": "text", "text": "$Y$ is the reward; $U$ is a latent covariate (to the imitator) uniformly drawn over a binary domain $\\{0,1\\}$ . Values of the reward are decided by a reward function $Y\\leftarrow X\\oplus U$ where $\\bigoplus$ is a \u201cxor\u201d operator. An expert demonstrator, having access to covariate $U$ , selecting action based on an expert policy $X\\leftarrow\\neg U$ . Evaluating the expert\u2019s performance gives $\\mathbb{E}[Y]=\\mathbb{E}[\\bar{\\neg}U\\oplus U]=1$ . On the other hand, an imitator, mimicking the expert\u2019s behavior, will follow a policy $\\textstyle\\pi(X)={\\bar{P(X)}}=0.5$ , selecting action uniformly at random. Evaluating the imitator\u2019s performance gives $\\begin{array}{r}{\\mathbb{E}_{\\pi}\\left[Y\\right]=\\sum_{x}\\pi(x)\\mathbb{E}[x\\oplus U]=0.5}\\end{array}$ , far from the expert\u2019s performance $\\mathbb{E}[Y]=1$ . ", "page_idx": 1}, {"type": "text", "text": "Causal Inference (CI) addresses the challenges of unobserved confounding bias within the observational data [32, 47, 53]. It leverages causal knowledge integral to the data generation process, typically represented as a causal diagram or potential outcomes [32, 43, 5]. More recently, incorporating causal inference methods into the imitation learning paradigm, causal imitation learning has evolved into a critical area of research [16, 57, 27, 7, 49, 42]. To compensate for the presence of unobserved confounding bias, these methods rely on additional structural or parametric knowledge about causal relationships among variables in the environment. By utilizing such domain knowledge, the imitator is able to recover the underlying system dynamics (i.e., causal effect) from confounded expert demonstrations and, in turn, obtain an imitating policy that can achieve the expert\u2019s performance. ", "page_idx": 1}, {"type": "text", "text": "By and large, the combination of causal knowledge and observational data does not always allow one to point-identify the causal effect, called the non-identifiable. That is, more than one parametrization of the target effect is compatible with the same observational data and model assumptions [32, Def. 3.2.2]. For instance, in the MAB environment described previously, the imitator\u2019s performance $\\mathbb{E}_{\\pi}\\left[Y\\right]$ is not identifiable from the confounded observational distribution $P(X,Y)$ [32, Thm. 3.4.1]. Partial identification methods concerned with inferring about target causal effects in non-identifiable settings, and has been a target of growing interest in the domains of causal inference [3, 12, 37, 14, 58], econometrics [21, 35, 38, 48], and more recently, in machine learning [25, 24, 23]. Among these works, two approaches are often employed: (1) bounds are derived for the target effect under minimal assumptions, or (2) additional untestable assumptions are invoked under which the causal effect is identifiable, and then sensitivity analysis is conducted to assess how the target causal effect varies as the untestable assumptions are changed. Despite their effectiveness in addressing data bias, partial identification has still been rarely explored in the context of imitation learning. ", "page_idx": 1}, {"type": "text", "text": "This paper studies the partial identification for imitation learning in a generalized sequential decisionmaking environment of Markov Decision Processes (MDPs, [36]). The imitator must determine a sequence of actions, while unobserved confounders cannot be ruled out a priori in expert demonstrations. We discuss the solutions case-by-case, depending on the identifiability of the underlying system dynamics from the confounded data, including the reward function $\\mathcal{R}$ and the transition distribution $\\tau$ . Specifically, our contributions can be summarized as follows. (1) We theoretically prove that when unobserved confounders generally exist, it is infeasible to learn a robust policy that is guaranteed to achieve expert performance from the demonstration data. (2) When only the transition distribution $\\tau$ is identifiable, we propose a novel imitation algorithm that leverages the bounds over the non-identifiable reward $\\mathcal{R}$ ; by matching the weighted occupancy measure, the imitator is able to obtain a policy that can outperform the expert. (3) We propose an alternative algorithm when the reward $\\mathcal{R}$ is identifiable, but there is unobserved confounding affecting the transition $\\tau$ . Our proposed algorithms could be implemented by augmenting the celebrated generative adversarial imitation learning framework (GAIL, [19]). Table 1 briefly summarizes this paper\u2019s main contributions. Due to space constraints, all proofs are provided in Appendices A and B. ", "page_idx": 1}, {"type": "text", "text": "1.1 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "This section introduces the basic notations and definitions used throughout the paper. We use capital letters to denote random variables $(X)$ , small letters for their values $(x)$ , and ${\\mathcal{D}}_{X}$ for the domain of $X$ . For an arbitrary set $\\mathbf{\\deltaX}$ , let $|X|$ be its cardinality. Fix indices $i,j\\in\\mathbb{N}$ . Let $X_{i:j}$ stand for a sequence of variables $\\{X_{i},X_{i+1},\\ldots,X_{j}\\}$ ; for consistency, the sequence $X_{i:j}\\,=\\,\\emptyset$ if $j<i$ . We denote by $P(X)$ represents a probability distribution over variables $\\mathbf{\\deltaX}$ , and will consistently use $P(x)$ as abbreviations for probabilities $P(X=x)$ . Finally, $\\mathbb{1}\\{Z=z\\}$ is an indicator function that returns 1 if event $z=z$ holds true; otherwise, it returns 0. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "The basic semantical framework of our analysis rests on structural causal models (SCMs) [32, 4]. An SCM $M$ is a tuple $\\langle V,U,{\\mathcal{F}},P(U)\\rangle$ , where $V$ is a set of endogenous variables and $U$ is a set of exogenous variables. $\\mathcal{F}$ is a set of functions s.t. each $f_{V}~\\in~{\\mathcal{F}}$ decides values of an endogenous variable $V\\in V$ taking as argument a combination of other variables in the system. That is, $V\\,\\leftarrow\\,f_{V}(\\pmb{{{\\cal P}}}\\pmb{{\\cal A}}_{V},\\pmb{{U}}_{V}),\\pmb{{{\\cal P}}}\\pmb{{\\cal A}}_{V}\\,\\subseteq\\,V,U_{V}\\,\\subseteq\\,U$ . Exogenous variables $U\\in U$ are mutually independent, values of which are drawn from the exogenous distribution $P(U)$ . Naturally, $M$ induces a joint distribution $P(V)$ over endogenous variables $V$ , called the observational distribution. An intervention on a subset $X\\subseteq V$ , denoted by $\\operatorname{do}(x)$ , is an operation where values of $\\mathbf{\\deltaX}$ are set to constants $\\textbf{\\em x}$ , replacing the functions $\\{f_{X}:\\forall X\\in X\\}$ that would normally determine their values. For an SCM $M$ , let $M_{x}$ be a submodel of $M$ induced by intervention $\\operatorname{do}(x)$ . For a set $Y\\subseteq V$ , the interventional distribution $P_{x}\\left(Y\\right)$ induced by $\\operatorname{do}(x)$ is defined as the joint distribution over $\\mathbf{\\deltaY}$ in the submodel $M_{x}$ , i.e., $P_{\\mathbf{\\boldsymbol{x}}}\\left(\\mathbf{\\boldsymbol{Y}};M\\right)\\triangleq P\\left(\\mathbf{\\boldsymbol{Y}};M_{\\mathbf{\\boldsymbol{x}}}\\right)$ . We leave $M$ implicit when it is obvious from the context. For a detailed survey on SCMs, we refer readers to [32, Ch. 7] and [4]. ", "page_idx": 2}, {"type": "text", "text": "2 Challenges of Unobserved Confounding ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We focus on the sequential decision-making setting of an agent operating in a MDP environment [36] over a series of interventions $t=1,2,\\ldots$ At each time step $t$ , the agent observes the current state $S_{t}$ , performs an action $\\operatorname{do}(X_{t})$ , receives a subsequent reward $Y_{t}$ , and moves to the next observed state $S_{t+1}$ . Values of the action $X_{t}$ are selected by sampling from a stationary policy $\\pi(x\\mid s)$ , which is a function mapping from the domain of the observed state $S_{t}$ to the probability space over the domain of every action $X_{t}$ . Let $U_{t}$ be an unobserved noise independently drawn from an exogenous distribution $P(U)$ . Values of the subsequent reward $Y_{t}$ and the next state $S_{t+1}$ are, respectively, determined by structural functions $y_{t}\\gets f_{Y}\\big(s_{t},x_{t},\\mathbf{u}_{t}\\big)$ and $s_{t+1}\\gets f_{S}(s_{t},x_{t},\\mathbf{u}_{t})$ , taking as input the current state $S_{t}$ , action $X_{t}$ , and latent noise $U_{t}$ . The initial state $S_{1}$ is drawn from an initial distribution $P(S_{1})$ . We will consistently use $\\mathcal{X},\\mathcal{S}$ , and $\\boldsymbol{\\wp}$ to denote the domain of action $X_{t}$ , state $S_{t}$ , and reward $Y_{t}$ . Like a standard discrete MDP, domains of actions $\\mathcal{X}$ and states $\\boldsymbol{S}$ are assumed to be finite; rewards are bounded in a real interval $\\mathcal{V}\\triangleq[0,1]\\subset\\mathbb{R}$ . Naturally, the agent operating in this environment defines an interventional distribution $P_{\\pi}$ , summarizing the consequences of its actions. ", "page_idx": 2}, {"type": "text", "text": "Fig. 2a shows a graph describing this generative process; where nodes represent observed variables and directed arrows represent the functional relationships between them. For every time step $t>1$ , the current state $S_{t}$ \u201cblocks\u201d all pathways from previous nodes (e.g., $S_{t-1})$ to the future nodes (e.g., $S_{t+1}$ ) [32, Def. 1.2.3]. Applying ${\\mathrm{d}}\\cdot$ -separation rules leads to the following independence. ", "page_idx": 2}, {"type": "text", "text": "Definition 1 (Markov Property [36]). For a joint distribution $P_{*}$ over sequences of states $S_{1},S_{2},\\ldots,$ , actions $X_{1},X_{2},\\ldots$ , and rewards $Y_{1},Y_{2},\\ldots$ , the Markov property holds with regard to distribution $P_{*}$ , if for every time step $t=1,2,\\stackrel{\\cdot\\,^{\\cdot\\,^{\\cdot\\,}}}{\\dots},\\stackrel{\\cdot}{\\left(\\bar{S}_{1:t-1},\\bar{X}_{1:t-1},\\vec{Y}_{1:t-1}\\stackrel{\\cdot\\,^{\\cdot\\,}}{\\perp}\\bar{X}_{t:\\infty},\\bar{S}_{t+1:\\infty},\\stackrel{\\sim}{Y}_{t:\\infty}|\\begin{array}{l}{S_{t}}\\end{array}\\right)}$ . It follows from Def. 1 that for any horizon $T$ , the joint distribution $P_{\\pi}\\left(\\bar{X}_{1:T},\\bar{S}_{1:T},\\bar{Y}_{1:T}\\right)$ generated by a policy $\\pi(X\\mid S)$ factorizes as follows,2 ", "page_idx": 2}, {"type": "equation", "text": "$$\nP_{\\pi}\\left(\\Bar{x}_{1:T},\\Bar{s}_{1:T},\\Bar{y}_{1:T}\\right)=P(s_{1})\\prod_{t=1}^{T}\\pi(x_{t}\\mid s_{t})\\mathcal{T}(s_{t},x_{t},s_{t+1})\\mathcal{R}(s_{t},x_{t},y_{t}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where the transition distribution $\\tau$ and the reward distribution $\\mathcal{R}$ are interventional queries given by ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{T}(s_{t},x_{t},s_{t+1})=P_{x_{t}}\\left(s_{t+1}\\mid s_{t}\\right)=\\displaystyle\\int_{u_{t}}\\mathbb{1}\\left\\{s_{t+1}=f_{S}(s_{t},x_{t},u_{t})\\right\\}P(u_{t})}\\\\ {\\mathcal{R}(s_{t},x_{t},y_{t})=P_{x_{t}}\\left(y_{t}\\mid s_{t}\\right)=\\displaystyle\\int_{u_{t}}\\mathbb{1}\\left\\{y=f_{Y}(s_{t},x_{t},u_{t})\\right\\}P(u_{t})}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "image", "img_path": "KHX0dKXdqH/tmp/a59c5375fd96d533e9d099dea237ce1ebdd49524f25c31d6903fa4542dfc2b96.jpg", "img_caption": ["Figure 2: Causal diagrams where $S_{t}$ represents the state, $X_{t}$ represents the action (shaded blue) and $Y_{t}$ represents the latent reward (shaded red). (a) $\\mathbf{MDP_{\\mathrm{exp}}}$ describes the imitator\u2019s interaction with the environment; (b) $\\mathbf{MDP_{obs}}$ shows the data-generating process for the expert demonstrations. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "For analytical clarity, we define reward function $\\mathcal{R}(s,x)$ as the expected value $\\textstyle\\sum_{y}y^{\\mathcal{R}}(s,x,y)$ . Fix the discounted factor $\\gamma\\in[0,1]$ . A common objective for an agent to optimize is the cumulative return $\\textstyle R_{t}=Y_{t}+\\gamma Y_{t+1}+{\\bar{\\gamma}}^{2}{\\bar{Y_{t+2}}}+\\cdot\\cdot\\cdot=\\sum_{k=0}^{\\infty}\\gamma^{k}Y_{t+k}$ . ", "page_idx": 3}, {"type": "text", "text": "Imitation Learning. When a detailed parametrization of the transition distribution $\\tau$ and the reward function $\\mathcal{R}$ is available, the agent can obtain an optimal policy using standard planning algorithms [36, 6]. However, in many practical applications, complete knowledge of these parametrizations is often unavailable, necessitating a learning process. In this paper, we consider the imitation learning setting, where the agent has access to observed trajectories generated by the expert. More specifically, at each time step $t$ , the expert selects an action $X_{t}\\gets f_{X}\\big(s_{t},\\mathbf{u}_{t}\\big)$ based on the current state $S_{t}\\,=\\,s_{t}$ and latent noise ${U_{t}}\\,=\\,u_{t}$ . Fig. 2b shows the graphical representation of the datagenerating process of the expert; the highlighted bi-directed arrows, e.g., $X_{t}\\leftrightarrow Y_{t}$ , indicate the presence of an unobserved confounder $U\\in U_{t}$ affecting both the action $X_{t}$ and outcome $Y_{t}$ . We summarize the expert trajectories using the observational distribution $P(X,S,Y)$ over sequences of variables $\\pmb{X}=\\bar{\\{X_{1},X_{2},\\dots\\}}$ , $\\pmb{S}=\\{S_{1},S_{2},\\ldots\\}$ , and $\\pmb{Y}=\\{Y_{1},Y_{2},\\cdot\\cdot\\cdot\\}$ . It is verifiable from Fig. 2b that Markov property holds with regard to distribution $P(X,S,Y)$ . For any horizon $T$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\nP\\left(\\Bar{x}_{1:T},\\Bar{s}_{1:T},\\Bar{y}_{1:T}\\right)=P(s_{1})\\prod_{t=1}^{\\infty}P(x_{t}\\mid s_{t})\\widetilde{\\mathcal{T}}(s_{t},x_{t},s_{t+1})\\widetilde{\\mathcal{R}}(s_{t},x_{t},y_{t})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\tilde{\\tau}$ and $\\widetilde{\\mathcal{R}}$ are the expert\u2019s nominal transition distribution and reward function computed from the observational distribution as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\widetilde{\\mathcal{T}}\\left(s,x,s^{\\prime}\\right)=P\\left(S_{t+1}=s^{\\prime}\\mid S_{t}=s,X_{t}=x\\right),\\qquad\\widetilde{\\mathcal{R}}\\left(s,x\\right)=\\mathbb{E}\\left[Y_{t}\\mid S_{t}=s,X_{t}=x\\right].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "By convention in imitation learning, we assume the rewards $Y_{t}$ are generally unobserved to the learner; instead, it has access to a parametric family $\\mathcal{R}$ containing the expert\u2019s nominal reward function $\\mathbb{E}\\left[Y_{t}\\mid s_{t},x_{t}\\right]$ . Given the expert demonstrations $\\mathcal{D}$ sampled from $P(X_{1},X_{2},\\dots,S_{1},S_{2},\\dots)$ and the parametric reward family $\\mathcal{R}$ , the imitator attempts to learn policy $\\pi$ that can achieve expert performance, i.e., $\\begin{array}{r}{\\mathbb{E}_{\\boldsymbol\\pi}\\left[\\sum_{t=1}^{\\infty}\\dot{\\gamma}^{t-1}Y_{t}\\right]\\geq\\mathbb{E}\\left[\\sum_{t=1}^{\\infty}\\gamma^{t-1}Y_{t}\\right]}\\end{array}$ . Standard imitation methods focus on the identifiable setting where the imitator\u2019s transition distribution and reward function is consistent with the expert\u2019s nominal transition $\\tilde{\\tau}$ and reward $\\widetilde{\\mathcal{R}}$ . Formally, ", "page_idx": 3}, {"type": "text", "text": "Definition 2 (Causal Consistency). For an interventional distribution $P_{\\pi}$ and an observational distribution $P$ satisfying the Markov property (Def. 1), Causal Consistency is said to hold with respect to $P_{\\pi}$ and $P$ if the following statement is true, for every time step $t=1,2,\\ldots$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\nP_{x_{t}}\\left(s_{t+1}\\mid s_{t}\\right)=P\\left(s_{t+1}\\mid s_{t},x_{t}\\right),\\quad{\\mathrm{and}}\\quad P_{x_{t}}\\left(y_{t}\\mid s_{t}\\right)=P\\left(y_{t}\\mid s_{t},x_{t}\\right)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "When the invariances of Def. 2 hold, the learner could recover the parametrization of the transition distribution $\\tau$ from observational data $P(X,S)$ and infer about the reward function $\\mathcal{R}$ from the parametric family $\\mathcal{R}$ . An imitating policy $\\pi$ is obtainable by solving the following minimax program, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\nu^{*}=\\underset{\\pi}{\\mathrm{min}}\\underset{\\mathcal{R}\\in\\mathcal{R}}{\\mathrm{max}}\\sum_{s,x}\\mathcal{R}(s,x)\\left(P(x\\mid s)\\rho(s)-\\pi(x\\mid s)\\rho_{\\pi}(s)\\right)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the imitator\u2019s $\\rho_{\\pi}$ and the expert\u2019s $\\rho$ occupancy measures are defined as, respectively, $\\rho_{\\pi}(s)=$ $\\begin{array}{r}{\\sum_{t=0}^{\\infty}\\gamma^{t}P_{\\pi}\\left(S_{t}=s\\right)}\\end{array}$ and $\\begin{array}{r}{\\rho(s)=\\dot{\\sum}_{t=0}^{\\infty}\\,\\dot{\\gamma}^{t}P\\left(\\bar{S_{t}}=\\bar{s}\\right)}\\end{array}$ . The solution $\\pi$ is guaranteed to achieve expert performance when the gap $\\nu^{*}\\leq0$ . This means that the imitator following policy $\\pi$ performs as well as the expert, even in the worst-case environment instance compatible with the demonstration data and model assumption. Several imitation learning algorithms have been proposed to solve the optimization problem in Eq. (7), including [1, 50, 19]. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Graphical criteria exist [33, 46, 34] to examine whether causal consistency (Def. 2) holds from causal knowledge of the environment, including the celebrated backdoor condition [32, Def. 3.3.1],[15]. In MDPs, this means that the causal links between the latent noise $U_{t}$ and action $X_{t}$ are not effective - the graphical representation of the imitator\u2019s (Fig. 2a) and the expert\u2019s (Fig. 2b) data-generating process coincide. However, in practice, causal consistency could be fragile and does not necessarily hold due to the presence of unobserved confounders in the demonstration data [57, 39]. The remainder of this paper studies imitation learning when violations occur in the invariance relationships of Eq. (6). ", "page_idx": 4}, {"type": "text", "text": "2.1 Imitation with Non-Identifiable Transition and Reward ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We first consider the imitation setting described in Fig. 2b where unobserved confounders generally exist in the expert demonstrations; both the transition distribution $\\tau$ and reward function $\\mathcal{R}$ are not identifiable from Eq. (6). Here, we will show that expert performance is not imitable by constructing worst-case MDP instances where the expert always outperforms the imitator. ", "page_idx": 4}, {"type": "text", "text": "The state value function $V_{\\pi}(s)$ is defined as the expected return given the imitator\u2019s starting state $S_{t}~=~s$ following a policy $\\pi$ , i.e., $V_{\\pi}(s)\\;=\\;\\mathbb{E}_{\\pi}\\left[\\bar{R}_{t}\\;|\\;S_{t}=s\\right]$ . For any policy $\\pi$ , the imitator\u2019s performance can be written as $\\begin{array}{r}{\\mathbb{E}_{\\pi}[R_{1}]=\\sum_{s_{1}}\\!P(s_{1})V_{\\pi}(\\underline{{s}}_{1})}\\end{array}$ . The value function of any state $s$ can thus be recursively defined using the celebrated Bellman Equation [6]: ", "page_idx": 4}, {"type": "equation", "text": "$$\nV_{\\pi}(s)=\\sum_{x}\\pi(x\\mid s)\\left(\\mathcal{R}(s,x)+\\gamma\\sum_{s^{\\prime}}\\mathcal{T}(s,x,s^{\\prime})V_{\\pi}(s^{\\prime})\\right)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\gamma$ denotes the discount factor. While the transition distribution $\\tau$ and the reward function $\\mathcal{R}$ are not uniquely discernible from the observational distribution due to the unobserved confounding, it is still possible to learn about them from demonstrations using partial identification. Without loss of generality, the reward $Y_{t}$ is normalized in a real interval $[0,\\bar{1}]$ . Through rigorous adaptation of the bounding strategies established in [29, 55], we successfully derive the bounds for the transition distribution $\\tau$ and reward function $\\mathcal{R}$ , for every realization $(s,{\\dot{x}},s^{\\prime})\\in S\\times{\\mathcal{X}}\\times S$ , ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{T}\\left(s,x,s^{\\prime}\\right)\\in\\left[\\widetilde{\\mathcal{T}}\\left(s,x,s^{\\prime}\\right)P(x\\mid s),\\;\\widetilde{\\mathcal{T}}\\left(s,x,s^{\\prime}\\right)P(x\\mid s)+P\\left(\\neg x\\mid s\\right)\\right]}\\\\ &{\\mathcal{R}\\left(s,x\\right)\\in\\left[\\widetilde{\\mathcal{R}}\\left(s,x\\right)P(x\\mid s),\\;\\widetilde{\\mathcal{R}}\\left(s,x\\right)P(x\\mid s)+P(\\neg x\\mid s)\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Among the above quantities, $\\tilde{\\tau}$ and $\\widetilde{\\mathcal{R}}$ are the expert\u2019s nominal transition distribution and reward function in Eq. (5); $\\overline{{P(x\\mid s)}}$ stands for the propensity score $P\\left(X_{t}=x\\mid S_{t}=s\\right)$ and $P(\\lnot x\\mid s)=$ $1-P(x\\mid s)$ . We can then construct a worst-case MDP for any policy $\\pi$ at state $s$ by solving the following optimization program: minimize the Bellman\u2019s equation in Eq. (8) as the objective function, subject to the observational constraints in Eqs. (9) and (10). Solving this program enables a valid MDP construction since the transition distribution $\\tau$ and the reward function $\\mathcal{R}$ are independent components induced by the underlying model and can be optimized separately. ", "page_idx": 4}, {"type": "text", "text": "Theorem 1. Given any positive observational distribution $P(X,S,Y)>0,$ , there exists an MDP model $\\hat{M}$ compatible with the causal graph of Fig. $2b$ such that $P(X,S,Y;\\hat{M})=P(X,S,Y)$ and for any policy $\\pi$ , any time step $t=1,2,\\dots$ , any state $s\\in S$ , ", "page_idx": 4}, {"type": "equation", "text": "$$\nV_{\\pi}\\left(s;\\hat{M}\\right)<\\mathbb{E}\\left[R_{t}\\mid S_{t}=s;\\hat{M}\\right].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In other words, there always exists a candidate MDP instance $\\hat{M}$ compatible with the demonstration data such that an imitator is always unable to achieve expert performance (r.h.s. in Eq. (11)), regardless of the deployed policy $\\pi$ . It follows from Thm. 1 that there is no policy $\\pi$ learnable from confounded demonstrations that is guaranteed to perform at least as the expert in all possible scenarios. This means that expert performance is not imitable when unobserved confounding generally exists. The following example demonstrates the challenges of unobserved confounding in a single-stage MDP. ", "page_idx": 4}, {"type": "text", "text": "Example 1 (Single-Stage MDP). Consider a 1-stage MDP model with horizon $T=1$ . For any policy $\\pi(X_{1}\\,\\,|\\,\\,S_{1})$ , the imitator\u2019s expected return is $\\begin{array}{r}{\\mathbb{E}_{\\pi}\\left[\\bar{Y_{1}}\\right]=\\sum_{s_{1},x_{1}}\\mathcal{R}(s_{1},x_{1})\\pi(x_{1}\\mid s_{1})P(s_{1})}\\end{array}$ . It follows from the tight lower bound in Eq. (10) that there exists an worst-case MDP model $\\hat{M}$ compatible with the observational distribution $P(X_{1},S_{1},Y_{1})$ such that $\\mathcal{R}(s_{1},x_{1})=\\mathbb{E}\\left[Y_{1}\\ |\\ s_{1},x_{1}\\right]P(\\bar{x_{1}}\\ |\\ s_{1})$ . In this MDP instance $\\hat{M}$ , the imitator\u2019s expected return can be further written as ", "page_idx": 4}, {"type": "image", "img_path": "KHX0dKXdqH/tmp/7a4b8d3e0b94eeacaf1a9ff5ed65f41e42203ca657cce25485ac46d1d7977812.jpg", "img_caption": ["Figure 3: (a) $\\mathrm{MDP_{obs-Y}}$ shows a data-generating process for expert demonstrations where only the reward $Y_{t}$ is confounded with the action $X_{t}$ ; (b) $\\mathrm{MDP}_{\\mathrm{obs-S}}$ shows a data-generating process for expert demonstrations where only the next state $S_{t+1}$ is confounded with the action $X_{t}$ . "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\pi}\\left[Y_{1}\\right]=\\sum_{s_{1},x_{1}}\\mathbb{E}[Y_{1}\\mid s_{1},x_{1}]P(x_{1}|s_{1})\\pi(x_{1}|s_{1})P(s_{1})<\\sum_{s_{1},x_{1}}\\mathbb{E}[Y_{1}\\mid s_{1},x_{1}]P(x_{1}|s_{1})P(s_{1})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The last step holds since probabilities of the policy $\\pi(x_{1}\\mid s_{1})\\in[0,1]$ and $\\begin{array}{r}{\\sum_{x_{1}}\\pi(x_{1}\\mid s_{1})=1}\\end{array}$ . Marginalizing the above equation gives $\\mathbb{E}_{\\pi}\\left[Y_{1}\\right]<\\mathbb{E}[Y_{1}]$ - the imitator is unable to achieve expert performance regardless of the deployed policy $\\pi$ . This analysis applies analogously to the MAB model in Fig. 1, which can be thought of as a 1-stage MDP with no initial state $\\bar{S}_{1}=\\dot{\\emptyset}$ . We refer the readers to Appendix F for more examples about the 2-stage MDP. ", "page_idx": 5}, {"type": "text", "text": "3 Partial Identification for Robust Imitation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The impossibility results in Thm. 1 imply that robust imitation cannot be guaranteed when unobserved confounders generally exist in the demonstration data. This means we must explore alternative assumptions to learn an imitating policy guaranteed to achieve expert performance. Meanwhile, standard imitation methods apply when causal consistency of Def. 2 holds, and no unobserved confounder affects the transition or reward function. A natural question at this point arises: whether robust imitation is feasible for settings between the unconfounded (Fig. 1b) and fully confounded cases (Fig. 1a), where unobserved confounding bias affects only either the transition distribution or reward function? This section aims to answer this question. ", "page_idx": 5}, {"type": "text", "text": "3.1 Imitation with Identifiable Transition and Non-Identifiable Reward ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We first examine the setting graphically described in Fig. 3a where the reward function is confounded, while the transition distribution is identifiable from the demonstration data. In this case, the first equation of Def. 2 holds while the second one fails. To initiate the discussion, we write the expected return of a candidate policy $\\pi$ in an MDP environment as follows [36], ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\pi}\\left[R_{1}\\right]=\\sum_{s,x}\\mathcal{R}\\left(s,x\\right)\\pi(x\\mid s)\\rho_{\\pi}(s)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Among quantities in the above equation, the state occupancy measure $\\textstyle\\rho_{\\pi}(s)=\\sum_{t=0}^{\\infty}\\gamma^{t}P_{\\pi}$ ( $[S_{t}=s)$ is a function of the initial state distribution $P(s)\\,=\\,\\bar{P}(S_{1}\\,=\\,s)$ and the tran sition distribution $\\tau$ Specifically, $\\rho_{\\pi}(s)$ can be recursively written as $\\begin{array}{r}{\\rho_{\\pi}^{-}(s)=P\\left(s\\right)\\!+\\!\\dot{\\gamma}\\sum_{s^{\\prime},x}\\mathcal T\\left(s^{\\prime},x,s\\right)\\pi(x\\mid s^{\\prime})\\rho_{\\pi}(s^{\\prime})}\\end{array}$ When the transition distribution is unconfounded (Fig. 3a), one could recover its parametrization $\\boldsymbol{\\mathcal{T}}(s,\\boldsymbol{x},s^{\\prime})$ following the first formula of Def. 1. Therefore, what remains undetermined in Eq. (13) is the non-identifiable reward function $\\mathcal{R}$ . It follows from Eq. (10) that parametrization of $\\mathcal{R}(s,x)$ can be bounded from the observational distribution. The imitator\u2019s expected return could thus be lower bounded as $\\begin{array}{r}{\\mathbb{E}_{\\pi}\\left[R_{1}\\right]\\;\\ge\\;\\sum_{s,x}\\widetilde{\\mathcal{R}}\\left(s,x\\right)P(x\\mid s)\\pi(x\\mid s)\\rho_{\\pi}(s)}\\end{array}$ , whereR  is the nominal reward function defined in Eq. (5). Simi larly, the expert\u2019s expected return could be decomposed as $\\begin{array}{r}{\\mathbb{E}\\left[R_{1}\\right]=\\sum_{x,s}\\widetilde{\\mathcal{R}}(s,x)P(x\\mid s)\\rho(s)}\\end{array}$ , where $\\begin{array}{r}{\\rho(s)=\\sum_{t=0}^{\\infty}\\gamma^{t}P\\left(S_{t}=s\\right)}\\end{array}$ is the expert\u2019s occupancy measure. Optim izing the worst-case gap between the imitator $\\mathbb{E}_{\\pi}\\left[R_{1}\\right]$ and expert $\\mathbb{E}\\left[R_{1}\\right]$ leads to a minimax optimization problem, the solution of which leads to a possible imitating policy. ", "page_idx": 5}, {"type": "text", "text": "Algorithm 1: Causal GAIL with Confounded Reward $\\mathcal{R}$ (CAIL- $\\mathcal{R}$ ) ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "1: Input: Expert demonstrations $\\mathcal{D}=\\{(S_{i},X_{i})\\}_{i=1}^{N}$   \n2: for iteration $k=0,1,2,.\\,.$ . do   \n3: Collect expert trajectories from $\\mathcal{D}$   \n4: Collect imitator trajectories based on the policy $\\pi_{k}(x\\mid s)$   \n5: Update the parameters $w$ of discriminator $D_{k}$ with gradient ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\mathbb{E}}[\\nabla_{w}\\log(D_{k}(s,x))]+\\hat{\\mathbb{E}}_{\\pi_{k}}[\\nabla_{w}P(x\\mid s)\\log(1-D_{k}(s,x))]}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "6: Update the policy $\\pi_{k+1}=\\arg\\operatorname*{min}_{\\pi}\\mathbb{E}_{\\pi}[P(x\\mid s)\\log(1-D(s,x))]$ using any forward RL algorithm ", "page_idx": 6}, {"type": "text", "text": "7: end for ", "page_idx": 6}, {"type": "text", "text": "Algorithm 2: Causal GAIL with Confounded Transition T (CAIL-T ) ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "1: Input: Expert demonstrations $\\mathcal{D}=\\{(S_{i},X_{i})\\}_{i=1}^{N}$   \n2: for iteration $k=0,1,2,.\\,.$ . do   \n3: Collect expert trajectories from $\\mathcal{D}$   \n4: Collect imitator trajectories based on the policy $\\pi_{k}(x\\mid s)$ from the worst-case occupancy   \nmeasure by solving the optimization problem presented in Eq. (19) and Eq. (20)   \n5: Update the parameters $w$ of discriminator $D_{k}$ with gradient ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathbb{\\hat{E}}[\\nabla_{w}\\log(D_{k}(s,x))]+\\mathbb{\\hat{E}}_{\\pi_{k}}[\\nabla_{w}\\log(1-D_{k}(s,x));T]}\\quad}&{{}}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "6: Update the policy $\\pi_{k+1}=\\arg\\operatorname*{min}_{\\pi}\\mathbb{E}_{\\pi}[\\log(1-D(s,x));T]$ with any forward RL algorithm   \n7: end for ", "page_idx": 6}, {"type": "text", "text": "Theorem 2. Given an MDP $M$ compatible with the causal graph of Fig. 3a, let $\\mathcal{R}$ be a parametric family containing the conditional reward $\\mathbb{E}[Y_{t}\\mid s_{t},x_{t}]$ . Consider the following optimization program, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\nu^{*}=\\operatorname*{min}_{\\pi}\\operatorname*{max}_{{\\tilde{\\mathcal{R}}}\\in{\\mathcal{R}}}\\sum_{s,x}{\\widetilde{\\mathcal{R}}}(s,x)P(x\\mid s)\\left(\\rho(s)-\\pi(x\\mid s)\\rho_{\\pi}(s)\\right)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "When the gap $\\nu^{*}\\leq0$ , the solution $\\pi^{*}$ is an imitating policy satisfying $\\mathbb{E}_{\\pi^{*}}\\left[R_{1}\\right]\\geq\\mathbb{E}[R_{1}]$ . ", "page_idx": 6}, {"type": "text", "text": "In other words, Thm. 2 computes an imitating policy within the environment depicted in Fig. 3a by finding a policy maximizing the worst-case reward function compatible with the demonstration data and the expert\u2019s nominal reward. Later in Sec. 4, we will demonstrate that such a solution exists and robust imitation learning is feasible in Fig. 3a. ", "page_idx": 6}, {"type": "text", "text": "The optimization program in Thm. 2 could be solved by augmenting some standard imitation learning such as GAIL [19]. To make the argument more precise, let the parametric family $\\mathcal{R}$ be a set of reward function $\\mathcal{R}(s,x)$ taking values in the real space $\\mathbb{R}$ . We penalize the complexity of a reward function $\\mathcal{R}$ by subtracting a convex regularization function $\\psi(\\mathcal{R})$ from Eq. (14); the detailed definition of $\\psi(\\mathcal{R})$ is given by [19, Eq. 13]. Solving the optimization program of Eq. (14) is equivalent to matching weighted occupancy measures between the imitator and the expert, shown in Appendix B, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nu^{*}=\\underset{\\pi}{\\mathrm{min}}\\,\\psi^{*}\\left(P(x\\mid s)\\rho(s)-P(x\\mid s)\\pi(x\\mid s)\\rho_{\\pi}(s)\\right)}\\\\ &{\\quad=\\underset{\\pi}{\\mathrm{min}}\\,\\underset{D\\in(0,1)^{S}\\times x}{\\mathrm{max}}\\,\\mathbb{E}[\\mathrm{log}(D(S,X))]+\\mathbb{E}_{\\pi}\\left[P(x\\mid s)\\log(1-D(S,X))\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\psi^{*}=\\operatorname*{max}_{\\mathcal{R}}\\boldsymbol{a}^{\\top}\\mathcal{R}-\\psi(\\mathcal{R})$ is a conjugate function of $\\psi$ ; function $D\\in\\mathcal{S}\\times\\mathcal{X}\\mapsto(0,1)$ is a discriminator classifier (e.g, a neural network). The above optimization problem is in the form of two neural networks competing against each other in a zero-sum game. The detailed implementation of our proposed algorithm, called CAIL- $\\mathcal{R}$ , is provided in Alg. 1. Compared to the standard GAIL algorithm, Alg. 1 adds weight to the signal generated by the discriminator for the imitator and then attempts to match the distribution between the weighted samples and expert demonstrations. ", "page_idx": 6}, {"type": "text", "text": "3.2 Imitation with Non-Identifiable Transition and Identifiable Reward ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we examine the $\\mathrm{MDP}_{\\mathrm{obs-S}}$ environment as graphically depicted in Fig. 3b, where the reward function is unconfounded, but UCs affect the action $X_{t}$ and the next state $S_{t+1}$ simultaneously. In this setting, the second equation of Causal Consistency (Def. 2) is satisfied, aligning the reward function $\\mathcal{R}$ with the expert\u2019s nominal reward function. However, the first equation of Def. 2 does not generally hold due to confounding bias, making the transition distribution $\\tau$ not identifiable from demonstrations. Despite these challenges, we utilize partial identification techniques to bound the transition function $\\tau$ , and subsequently estimate the imitator\u2019s performance. ", "page_idx": 7}, {"type": "text", "text": "More precisely, consider again the expected return decomposition in Eq. (13). The identifiable reward function $\\mathcal{R}$ must be contained in the parametric space of the expert\u2019s nominal reward $\\mathcal{R}$ . The transition distribution $\\tau$ can be bounded from the demonstration data using Eq. (9). One could thus obtain a lower bound over the imitator\u2019s performance by reasoning about the worst-case occupancy measure compatible with demonstrations. Formally, with the fixed reward function $\\mathcal{R}$ and the fixed policy $\\pi$ , the imitator\u2019s return $\\mathbb{E}_{\\pi}\\left[R_{1}\\right]$ is bounded by: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\pi}\\left[R_{1}\\right]\\geq\\operatorname*{min}_{\\mathcal T,\\rho_{\\pi}}~~~\\sum_{s,x}\\mathcal{R}(s,x)\\pi(x\\mid s)\\rho_{\\pi}(s)\n$$", "text_format": "latex", "page_idx": 7}, {"type": "equation", "text": "$$\n\\rho_{\\pi}(s)\\geq0,\\quad\\sum_{s}\\rho_{\\pi}(s)=\\frac{1}{1-\\gamma},\\quad\\mathrm{and}\\;\\rho_{\\pi}\\left(s\\right)=P\\left(s\\right)+\\gamma\\sum_{s^{\\prime},x}\\mathcal{T}(s^{\\prime},x,s)\\,\\pi(x\\mid s^{\\prime})\\rho_{\\pi}(s^{\\prime})\n$$", "text_format": "latex", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{s.~Constraints~}\\mathcal{T}:\\quad\\left\\{\\sum_{s}\\mathcal{T}\\left(s^{\\prime},x,s\\right)=1,\\quad\\mathrm{and~}\\mathcal{T}\\left(s,x,s^{\\prime}\\right)\\geq\\widetilde{\\mathcal{T}}\\left(s,x,s^{\\prime}\\right)P(x\\mid s)\\right\\}}&{{}}\\\\ {\\quad\\forall\\left(s,x,s^{\\prime}\\right)\\leq\\widetilde{\\mathcal{T}}\\left(s,x,s^{\\prime}\\right)P(x\\mid s)+P\\left(\\rightarrow x\\mid s\\right)}&{{}}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The above optimization problem is similar to the classical linear program for planning in MDPs [36]. The main difference is that the transition distribution $\\tau$ is no longer fixed but bounded in a convex space $\\mathcal{T}$ specified from the observational data. Therefore, we develop an imitating policy by minimizing the performance gap between the imitator and the expert in the worst-case environment compatible with the observational data and prior knowledge. ", "page_idx": 7}, {"type": "text", "text": "Theorem 3. Given an MDP $M$ compatible with the causal graph of Fig. $3b$ , let $\\mathcal{R}$ be a parametric family containing the conditional reward $\\mathbb{E}[Y_{t}~\\vert~s_{t},x_{t}]$ , and $\\mathcal{T}$ be a parametric family over conditional probabilities $P\\left(s_{t+1}\\mid s_{t},x_{t}\\right)$ defined in Eq. (20). Consider the following program, ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\nu^{*}=\\underset{\\pi}{\\mathrm{min}}\\ \\underset{\\mathcal{R}\\in\\mathcal{R}}{\\mathrm{max}}\\ \\underset{\\mathcal{T}\\in\\mathcal{T}}{\\mathrm{max}}\\ \\sum_{s,x}\\mathcal{R}(s,x)\\left(P(x\\mid s)\\rho(s)-\\pi(x\\mid s)\\rho_{\\pi}\\left(s;\\mathcal{T}\\right)\\right)\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "When the gap $\\nu^{*}\\leq0$ , the solution $\\pi^{*}$ is an imitating policy satisfying $\\mathbb{E}_{\\pi^{*}}\\left[R_{1}\\right]\\geq\\mathbb{E}[R_{1}]$ . ", "page_idx": 7}, {"type": "text", "text": "We solve the optimization program in Thm. 3 by augmenting GAIL, a standard imitation method [19]. By penalizing the complexity of a reward function $\\mathcal{R}$ using a convex regularization function $\\psi(\\mathcal{R})$ from Eq. (14), Eq. (21) is reducible to the following distribution matching problem, ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\nu^{*}=\\operatorname*{min}_{\\pi}\\,\\operatorname*{max}_{D\\in(0,1)^{S\\times x}}\\,\\operatorname*{max}_{T\\in\\mathcal{T}}\\,\\mathbb{E}[\\log(D(S,X))]+\\mathbb{E}_{\\pi}\\,[\\log(1-D(S,X));T]\\,,\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "We present the step-by-step implementation of our imitation method, CAIL- $\\mathcal{T}$ , in Alg. 2. It is similar to the standard GAIL [19]; however, a significant distinction arises at step 4, where the imitator collects trajectories from the worst-case occupancy measure as presented in Eq. (19) and Eq. (20), which is obtainable by iteratively solving a series of linear programs. We refer readers to Appendix C for a more detailed discussion, where we propose an iterative algorithm designed to find the worst-case occupancy measure efficiently. ", "page_idx": 7}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we validate the theoretical findings presented in Thm. 1 and illustrate the applications of the proposed CAIL algorithms (Alg. 1 and Alg. 2) on various causal imitation learning tasks. Such tasks range from synthetic causal models to real-world scenarios. To summarize, when both the transition and the reward are confounded, there always exists a worst-case MDP instance $\\hat{M}$ compatible with the expert demonstrations, but the imitator consistently fails to match expert performance, aligning with the proof provided in Sec. 2.1. When either the transition or the reward is confounded, we systematically evaluate our algorithms against the standard BC and GAIL methods, highlighting the importance of optimizing within the worst-case SCM. Standard BC mimics the expert\u2019s nominal behavior policy ${\\bar{P}}(X|S)$ via supervised learning; standard GAIL learns a policy by solving a min-max game [19]. We provide in Appendix D more details on the experiment setup. ", "page_idx": 7}, {"type": "image", "img_path": "KHX0dKXdqH/tmp/40775f4abac6699a9ea7f6ff5f6f91d714626985211734b743fbe92650a362e9.jpg", "img_caption": ["Figure 4: Simulation results for our experiments. Fig. 4a illustrates the performance gap histogram for the experiment $\\mathrm{MDP_{obs}}$ , where negative values indicate performance worse than expert performance. Fig. 4b shows the convergence plot for CAIL, GAIL, and BC performance. Fig. 4c shows the final performance, where y-axis represents the expected return. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "$\\mathbf{MDP_{obs}}$ \u2013 Random Instances. This experiment aims to empirically validate the theoretical findings discussed in Thm. 1. Consider SCM instances compatible with Fig. 2b including binary observed variables $S_{t},X_{t},Y_{t}\\in\\{0,1\\}$ . 1000 random discrete MDPs are sampled, in other words, the reward functions and the transition probabilities are generally different among these models. The expert is able to observe the state $S_{t}$ , the unobserved variable $U_{t}$ . However, the imitator, lacking access to both $U_{t}$ or the reward $\\mathbb{E}_{\\pi}[Y_{t}]$ , makes decisions solely on $S_{t}$ . As shown in Fig. 4a, imitators consistently failed to match expert performance. Specifically, prevalent negative performance gaps indicate that most of imitators were consistently worse than experts; only in rare cases did the performance gaps near $-0.5$ , supporting our theoretical insights in Thm. 1. In summary, imitators fail to achieve the expert\u2019s performance when both the reward and the transition are confounded. ", "page_idx": 8}, {"type": "text", "text": "$\\mathbf{MDP_{obs-Y}}$ \u2013 Driving. To demonstrate the proposed framework, as outlined in Alg. 1, we consider a scenario when an autonomous vehicle (\u2018ego vehicle\u2019) aims to learn optimal driving strategies from expert demonstrations. The state $S_{t}$ contains some critical driving information, e.g., the velocities of the ego vehicle and the leading vehicle and the spatial distance between them. The action $X_{t}$ represents acceleration or deceleration decisions the ego vehicle makes. The unobserved variable $U_{t}$ represents some information accessible to the expert but inaccessible to the imitator, e.g. slippery road conditions [26]. The reward $Y_{t}$ is designed to reflect multiple realistic driving objectives, e.g., safety, comfort, efficiency, and so on. $U_{t}$ has an effect on the reward $Y_{t}$ . Unlike the scenarios described in [39, 42], due to UCs between $X_{t}$ and $Y_{t}$ at each step $t$ , it is impossible to find a $\\pi$ -backdoor admissible set. BC, GAIL, and CAIL utilize the same policy space $\\pi(x\\mid s)$ . The major difference between CAIL and GAIL lies in that CAIL optimizes the imitator by the weighted reward generated from the discriminator \u2013 $P(x\\mid s)\\log(1-{\\bar{D}}(s,x))$ . As illustrated in Fig. 4b, where means and standard deviations are computed over 100 trajectories, CAIL consistently outperforms BC and GAIL. ", "page_idx": 8}, {"type": "text", "text": "$\\mathbf{MDP_{obs-S}}$ \u2013 Medical Treatment. Consider the challenge of providing medical treatment to acutely ill patients, where the primary goal is to learn a policy so that the morality rate can be decreased. We utilize the real-world medical treatment dataset, i.e., Medical Information Mart for Intensive Care III (MIMIC-III) dataset [22]. MIMIC-III consists trajectories of clinical information (e.g., heart rate, oxygen saturation, and so on) recorded at various time intervals. However, due to privacy concerns, certain essential variables are masked or not properly recorded [45], e.g., socioeconomic status or the experience levels of caregivers [9, 56]. Specifically, the state $S_{t}$ encapsulates the critical health information for the patients, e.g., prolonged elevated heart rate (peHR). The action $X_{t}$ represents whether to treat the medicine or not. The reward $Y_{t}$ is designed to represent the intent of the doctor as much as possible, e.g., avoiding the patient\u2019s mortality. The unobserved confounded ", "page_idx": 8}, {"type": "text", "text": "$U_{t}$ simultaneously affects the action $X_{t}$ and the next state $S_{t+1}$ . Simulation results are illustrated in Fig. 4c, which shows that the proposed framework performs the best among all strategies. BC and IRL fail to obtain an imitating policy that could match expert performance. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper investigates imitation learning in Markov Decision Processes where the unobserved confounding bias cannot be ruled out a priori. We establish theoretically that when such unobserved confounders generally exist, it is infeasible to obtain a robust imitating policy that can perform at least as well as the expert across all possible environments compatible with the demonstration data and prior knowledge. Departing from this critical realization, our research diverges into two distinct problem settings \u2013 one where only the transition distribution is unconfounded, but the reward function is non-identifiable due to unobserved confounding; and the other where the reward function is unconfounded and the transition distribution is non-identifiable. We then propose novel imitation learning algorithms using partial identification techniques, which allow the imitator to obtain effective policies that can achieve expert performance for both problem settings. Through extensive experiments, we empirically validate the theoretical findings and systematically evaluate our algorithms on different scenarios, ranging from simulated causal models to real-world datasets. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This research was supported in part by the NSF, ONR, AFOSR, DoE, Amazon, JP Morgan, and The Alfred P. Sloan Foundation. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] P. Abbeel and A. Y. Ng. Apprenticeship learning via inverse reinforcement learning. In Proceedings of the twenty-first international conference on Machine learning, page 1. ACM, 2004.   \n[2] B. D. Argall, S. Chernova, M. Veloso, and B. Browning. A survey of robot learning from demonstration. Robotics and autonomous systems, 57(5):469\u2013483, 2009.   \n[3] A. Balke and J. Pearl. Counterfactuals and policy analysis in structural models. In Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence, pages 11\u201318. Morgan Kaufmann Publishers Inc., 1995.   \n[4] E. Bareinboim, J. D. Correa, D. Ibeling, and T. Icard. On pearl\u2019s hierarchy and the foundations of causal inference. In Probabilistic and Causal Inference: The Works of Judea Pearl, page 507\u2013556. Association for Computing Machinery, New York, NY, USA, 1st edition, 2022.   \n[5] E. Bareinboim and J. Pearl. Causal inference and the data-fusion problem. Proceedings of the National Academy of Sciences, 113:7345\u20137352, 2016.   \n[6] R. Bellman. Dynamic programming. Science, 153(3731):34\u201337, 1966.   \n[7] I. Bica, D. Jarrett, and M. van der Schaar. Invariant causal imitation learning for generalizable policies. Advances in Neural Information Processing Systems, 34:3952\u20133964, 2021.   \n[8] A. Billard, S. Calinon, R. Dillmann, and S. Schaal. Survey: Robot programming by demonstration. Handbook of robotics, 59, 2008.   \n[9] N. B. Carnegie, M. Harada, and J. L. Hill. Assessing sensitivity to unmeasured confounding using a simulated potential confounder. Journal of Research on Educational Effectiveness, 9(3):395\u2013420, 2016.   \n[10] K.-W. Chang, A. Krishnamurthy, A. Agarwal, H. Daum\u00b4e III, and J. Langford. Learning to search better than your teacher. In International Conference on Machine Learning, pages 2058\u20132066. PMLR, 2015.   \n[11] A. Chen, J. Scheurer, J. A. Campos, T. Korbak, J. S. Chan, S. R. Bowman, K. Cho, and E. Perez. Learning from natural language feedback. Transactions on Machine Learning Research, 2024.   \n[12] D. Chickering and J. Pearl. A clinician\u2019s apprentice for analyzing non-compliance. In Proceedings of the Twelfth National Conference on Artificial Intelligence, volume Volume II, pages 1269\u20131276. MIT Press, Menlo Park, CA, 1996.   \n[13] S. Choi, J. Kim, and H. Yeo. Trajgail: Generating urban vehicle trajectories using generative adversarial imitation learning. Transportation Research Part C: Emerging Technologies, 128:103091, 2021.   \n[14] C. Cinelli, D. Kumor, B. Chen, J. Pearl, and E. Bareinboim. Sensitivity analysis of linear structural causal models. In International Conference on Machine Learning, pages 1252\u20131261, 2019.   \n[15] J. Correa and E. Bareinboim. A calculus for stochastic interventions: Causal effect identification and surrogate experiments. In Proceedings of the 34th AAAI Conference on Artificial Intelligence, New York, NY, 2020. AAAI Press.   \n[16] P. de Haan, D. Jayaraman, and S. Levine. Causal confusion in imitation learning. In Advances in Neural Information Processing Systems, pages 11693\u201311704, 2019.   \n[17] J. Etesami and P. Geiger. Causal transfer for imitation learning and decision making under sensor-shift. In Proceedings of the 34th AAAI Conference on Artificial Intelligence, New York, NY, 2020. AAAI Press.   \n[18] B. Fang, S. Jia, D. Guo, M. Xu, S. Wen, and F. Sun. Survey of imitation learning for robotic manipulation. International Journal of Intelligent Robotics and Applications, 3:362\u2013369, 2019.   \n[19] J. Ho and S. Ermon. Generative adversarial imitation learning. Advances in neural information processing systems, 29, 2016.   \n[20] A. Hussein, M. M. Gaber, E. Elyan, and C. Jayne. Imitation learning: A survey of learning methods. ACM Computing Surveys (CSUR), 50(2):1\u201335, 2017.   \n[21] G. W. Imbens and D. B. Rubin. Bayesian inference for causal effects in randomized experiments with noncompliance. The annals of statistics, pages 305\u2013327, 1997.   \n[22] A. E. Johnson, T. J. Pollard, L. Shen, L.-w. H. Lehman, M. Feng, M. Ghassemi, B. Moody, P. Szolovits, L. Anthony Celi, and R. G. Mark. Mimic-iii, a freely accessible critical care database. Scientific data, 3(1):1\u20139, 2016.   \n[23] S. Joshi, J. Zhang, and E. Bareinboim. Towards safe policy learning under partial identifiability: A causal approach. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 13004\u201313012, 2024.   \n[24] N. Kallus, A. M. Puli, and U. Shalit. Removing hidden confounding by experimental grounding. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems 31, pages 10911\u201310920. Curran Associates, Inc., 2018.   \n[25] N. Kallus and A. Zhou. Confounding-robust policy improvement. In Proceedings of the 32nd International Conference on Neural Information Processing Systems, pages 9289\u20139299, 2018.   \n[26] R. Krajewski, J. Bock, L. Kloeker, and L. Eckstein. The highd dataset: A drone dataset of naturalistic vehicle trajectories on german highways for validation of highly automated driving systems. In 2018 21st International Conference on Intelligent Transportation Systems (ITSC), pages 2118\u20132125, 2018.   \n[27] D. Kumor, J. Zhang, and E. Bareinboim. Sequential causal imitation learning with unobserved confounders. Advances in Neural Information Processing Systems, 2021.   \n[28] T. L. Lai and H. Robbins. Asymptotically efficient adaptive allocation rules. Advances in Applied Mathematics, 6(1):4 \u2013 22, 1985.   \n[29] C. Manski. Nonparametric bounds on treatment effects. American Economic Review, Papers and Proceedings, 80:319\u2013323, 1990.   \n[30] A. Y. Ng, D. Harada, and S. Russell. Policy invariance under reward transformations: Theory and application to reward shaping. In Icml, volume 99, pages 278\u2013287, 1999.   \n[31] T. Osa, J. Pajarinen, G. Neumann, J. A. Bagnell, P. Abbeel, J. Peters, et al. An algorithmic perspective on imitation learning. Foundations and Trends in Robotics, 7(1-2):1\u2013179, 2018.   \n[32] J. Pearl. Causality: Models, Reasoning, and Inference. Cambridge University Press, New York, 2000.   \n[33] J. Pearl and J. Robins. Probabilistic evaluation of sequential plans from causal models with hidden variables. In P. Besnard and S. Hanks, editors, Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence (UAI 1995), pages 444\u2013453. Morgan Kaufmann, San Francisco, 1995.   \n[34] E. Perkovi\u00b4c, J. Textor, M. Kalisch, and M. H. Maathuis. A complete generalized adjustment criterion. arXiv preprint arXiv:1507.01524, 2015.   \n[35] D. J. Poirier. Revising beliefs in nonidentified models. Econometric theory, 14(4):483\u2013509, 1998.   \n[36] M. L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley & Sons, Inc., 1994.   \n[37] A. Richardson, M. G. Hudgens, P. B. Gilbert, and J. P. Fine. Nonparametric bounds and sensitivity analysis of treatment effects. Statistical science: a review journal of the Institute of Mathematical Statistics, 29(4):596, 2014.   \n[38] J. P. Romano and A. M. Shaikh. Inference for identifiable parameters in partially identified econometric models. Journal of Statistical Planning and Inference, 138(9):2786\u20132807, 2008.   \n[39] K. Ruan and X. Di. Learning human driving behaviors with sequential causal imitation learning. Proceedings of the AAAI Conference on Artificial Intelligence, 36(4):4583\u20134592, 2022.   \n[40] K. Ruan, X. He, J. Wang, X. Zhou, H. Feng, and A. Kebarighotbi. S2e: Towards an end-to-end entity resolution solution from acoustic signal. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 10441\u201310445. IEEE, 2024.   \n[41] K. Ruan, X. Wang, and X. Di. From twitter to reasoner: Understand mobility travel modes and sentiment using large language models. In 2024 IEEE 27th International Conference on Intelligent Transportation Systems (ITSC), 2024.   \n[42] K. Ruan, J. Zhang, X. Di, and E. Bareinboim. Causal imitation learning via inverse reinforcement learning. In The Eleventh International Conference on Learning Representations, 2023.   \n[43] D. Rubin. Direct and indirect causal effects via potential outcomes. Scandinavian Journal of Statistics, 31:161\u2013170, 2004.   \n[44] S. I. H. Shah, A. Coronato, M. Naeem, and G. De Pietro. Learning and assessing optimal dynamic treatment regimes through cooperative imitation learning. IEEE Access, 10:78148\u2013 78158, 2022.   \n[45] Z. Shahn, N. I. Shapiro, P. D. Tyler, D. Talmor, and L.-w. H. Lehman. Fluid-limiting treatment strategies among sepsis patients in the icu: a retrospective causal analysis. Critical Care, 24:1\u20139, 2020.   \n[46] I. Shpitser, T. VanderWeele, and J. Robins. On the validity of covariate adjustment for estimating causal effects. In Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence, pages 527\u2013536. AUAI, Corvallis, OR, 2010.   \n[47] P. Spirtes, C. N. Glymour, and R. Scheines. Causation, prediction, and search. MIT press, 2000.   \n[48] J. Stoye. More on confidence intervals for partially identified parameters. Econometrica, 77(4):1299\u20131315, 2009.   \n[49] G. Swamy, S. Choudhury, D. Bagnell, and S. Wu. Causal imitation learning under temporally correlated noise. In International Conference on Machine Learning, pages 20877\u201320890. PMLR, 2022.   \n[50] U. Syed and R. E. Schapire. A game-theoretic approach to apprenticeship learning. In Advances in neural information processing systems, pages 1449\u20131456, 2008.   \n[51] J. Tian. Studies in Causal Reasoning and Learning. PhD thesis, Computer Science Department, University of California, Los Angeles, CA, November 2002.   \n[52] L. Wang, W. Yu, X. He, W. Cheng, M. R. Ren, W. Wang, B. Zong, H. Chen, and H. Zha. Adversarial cooperative imitation learning for dynamic treatment regimes. In Proceedings of The Web Conference 2020, pages 1785\u20131795, 2020.   \n[53] T.-Z. Wang, T. Qin, and Z.-H. Zhou. Estimating possible causal effects with latent variables via adjustment. In International Conference on Machine Learning, pages 36308\u201336335. PMLR, 2023.   \n[54] J. Zhang and E. Bareinboim. Markov decision processes with unobserved confounders: A causal approach. Technical Report R-23, Colummbia Causal AI Lab, 2016, https://causalai. net/mdp-causal.pdf.   \n[55] J. Zhang and E. Bareinboim. Near-optimal reinforcement learning in dynamic treatment regimes. In Advances in Neural Information Processing Systems, pages 13401\u201313411, 2019.   \n[56] J. Zhang and E. Bareinboim. Can humans be out of the loop? Technical Report R-64, Causal Artificial Intelligence Lab, Columbia University, 2020. Also, to appear: Proc. of the 1st Conference on Causal Learning and Reasoning (CLeaR), 2022.   \n[57] J. Zhang, D. Kumor, and E. Bareinboim. Causal imitation learning with unobserved confounders. Advances in Neural Information Processing Systems, 33:12263\u201312274, 2020.   \n[58] J. Zhang, J. Tian, and E. Bareinboim. Partial counterfactual identification from observational and experimental data. In International Conference on Machine Learning, pages 26548\u201326558. PMLR, 2022. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Proofs ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we provide proofs for the theoretical claims delineated in the paper. Throughout this paper, it is important to note that detailed parametrizations of the underlying SCM are not known to the agent. Instead, the agent has access to the expert\u2019s demonstrations, which are summarized as the observational distribution $P(X,S,Y)$ . ", "page_idx": 13}, {"type": "text", "text": "We begin by revisiting the distribution of state visitation. Specifically, $\\rho_{\\pi}(s)$ can be calculated by: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\rho_{\\pi}\\left(s\\right)=P\\left(s\\right)+\\gamma\\sum_{s^{\\prime},x}\\mathcal{T}\\left(s^{\\prime},x,s\\right)\\pi(x\\mid s^{\\prime})\\rho_{\\pi}(s^{\\prime})\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $P\\left(s\\right)$ represents the initial state distribution, $\\gamma$ represents the discount factor, $\\tau$ represents the transition probabilities for the imitator. Subsequently, we are able to develop the occupancy measure for the policy $\\pi$ : ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\rho_{\\pi}(s,x)=\\rho_{\\pi}(s)\\pi(x\\mid s)\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "It is important to note that, although the format of the occupancy measure $\\rho_{\\pi}(s,x)$ shares a formal resemblance to the one presented in GAIL [19], $\\rho_{\\pi}(s,x)$ specifically represents an interventional distribution with policy $\\operatorname{do}(\\pi)$ . The identifiability of the transition $\\boldsymbol{\\mathcal{T}}(s,\\boldsymbol{x},s^{\\prime})$ directly impacts the identifiability of $P_{\\pi}\\left(s_{t}\\right)$ . If $\\scriptstyle P_{\\pi}\\left(s_{t}\\right)$ is not identifiable, $\\rho_{\\pi}(s)$ and $\\rho_{\\pi}(s,x)$ are consequently not identifiable. ", "page_idx": 13}, {"type": "text", "text": "Theorem 1. Given any positive observational distribution $P(X,S,Y)>0,$ , there exists an MDP model $\\hat{M}$ compatible with the causal graph of Fig. $2b$ such that $P(X,S,Y;\\hat{M})=P(X,S,Y)$ and for any policy $\\pi$ , any time step $t=1,2,\\ldots$ , any state $s\\in S$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\nV_{\\pi}\\left(s;\\hat{M}\\right)<\\mathbb{E}\\left[R_{t}\\mid S_{t}=s;\\hat{M}\\right].\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof. Without loss of generality, the reward $Y$ is normalized so that it has a range of $[0,1]$ Based on the value function defined in Eq. (8), we first show how to expand it into a recursive version: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{\\pi}\\left(s_{t}\\right)=\\mathbb{E}_{\\pi}\\left[\\displaystyle\\sum_{k=0}^{\\infty}\\gamma^{k}Y_{t+k}\\ |\\ s_{t}\\right]}\\\\ &{\\ \\ \\ \\ \\ =\\mathbb{E}_{\\pi}[Y_{t}\\ |\\ s_{t}]+\\mathbb{E}_{\\pi}\\left[\\displaystyle\\sum_{k=1}^{\\infty}\\gamma^{k}Y_{t+k}\\ |\\ s_{t}\\right]}\\\\ &{\\ \\ \\ \\ \\ =\\mathbb{E}_{\\pi}[Y_{t}\\ |\\ s_{t}]+\\gamma\\mathbb{E}_{\\pi}\\left[\\displaystyle\\sum_{k=1}^{\\infty}\\gamma^{k-1}Y_{t+k}\\ |\\ s_{t}\\right]}\\\\ &{\\ \\ \\ \\ \\ =\\mathbb{E}_{\\pi}[Y_{t}\\ |\\ s_{t}]+\\gamma\\displaystyle\\sum_{s\\neq1}\\displaystyle P_{\\pi}(s_{t+1}\\ |\\ s_{t})\\mathbb{E}_{\\pi}\\left[\\displaystyle\\sum_{k=0}^{\\infty}\\gamma^{k}Y_{t+1+k}\\ |\\ s_{t},s_{t+1}\\right]}\\\\ &{\\ \\ \\ \\ \\ =\\mathbb{E}_{\\pi}[Y_{t}\\ |\\ s_{t}]+\\gamma\\displaystyle\\sum_{s\\neq1}P_{\\pi}(s_{t+1}\\ |\\ s_{t})V_{\\pi}(s_{t+1}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\gamma$ is the discount factor, $P_{\\pi}(s_{t+1}\\mid s_{t})$ denotes the transition probability when executing policy $\\pi$ . ", "page_idx": 13}, {"type": "text", "text": "From the second last line to the last line is justified by the experimental markovian property, as discussed in Sec. 2, following the graph Fig. 2a. More details could be found in [54]. $\\mathbb{E}_{\\pi}[Y_{t}\\ |\\$ $s_{t}]=\\mathbb{E}[Y_{t}\\mid s_{t},\\mathbf{do}(\\pi)]$ denotes the expected reward received by the agent when executing policy $\\pi$ . Similarly, the transition probability ", "page_idx": 13}, {"type": "equation", "text": "$$\nP_{\\pi}(s_{t+1}\\mid s_{t})=\\sum_{x_{t}}P_{x_{t}}(s_{t+1}\\mid s_{t})\\pi(x_{t}\\mid s_{t}),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "and $P_{x_{t}}(s_{t+1}\\ |\\ s_{t})\\,=\\,P(s_{t+1}\\ |\\ s_{t},\\mathrm{do}(x_{t}))\\,=\\,{\\mathcal{T}}(s_{t},x_{t},s_{t+1})$ . Generally speaking, when any unobserved confounder exists between $S_{t+1}$ and $X_{t}$ , the causal query $P_{x_{t}}(s_{t+1}\\mid\\Bar{s}_{t})$ is not identifiable ", "page_idx": 13}, {"type": "text", "text": "[32, 43, 5, 51]. Building on the previous derivations, we arrive at the recursive formulation of the value function under policy $\\pi$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{\\pi}(s_{t})=\\displaystyle\\sum_{x_{t}}{\\mathcal{R}}\\left(s_{t},x_{t}\\right)\\pi(x_{t}\\mid s_{t})+\\gamma\\displaystyle\\sum_{s_{t+1}}{\\mathcal{T}}(s_{t},x_{t},s_{t+1})\\pi(x_{t}\\mid s_{t})V_{\\pi}(s_{t+1})}\\\\ &{\\qquad=\\displaystyle\\sum_{x_{t}}\\pi(x_{t}\\mid s_{t})\\left({\\mathcal{R}}\\left(s_{t},x_{t}\\right)+\\gamma\\displaystyle\\sum_{s_{t+1}}{\\mathcal{T}}(s_{t},x_{t},s_{t+1})V_{\\pi}(s_{t+1})\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Next, to establish the validity of the preceding claim, we proceed by applying the technique of mathematical induction. Let $|S|$ denote the number of distinct states for $S$ . ", "page_idx": 14}, {"type": "text", "text": "Base case $t=T$ . For the final timestep $T$ , for each state index $j$ where $\\forall j,1\\leq j\\leq|S|$ , the value function $V_{\\pi}\\big(s_{(T,j)}\\big)$ cane be defined as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{V_{\\pi}\\big(s_{(T,j)}\\big)=\\mathbb{E}_{\\pi}\\left[Y_{T}\\mid S_{T}=s_{(T,j)}\\right]}\\quad}&{}\\\\ &{=\\displaystyle\\sum_{x_{t}}\\mathbb{E}_{x_{t}}\\left[Y_{T}\\mid S_{T}=s_{(T,j)}\\right)\\right]\\pi(x_{t}\\mid s_{(T,j)})}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $s_{(T,j)}$ refers to the scenario where the state at the final timestep $S_{T}$ is equal the specific state $j$ . ", "page_idx": 14}, {"type": "text", "text": "In order to obtain the worst-case SCM $\\hat{M}$ , we need to minimize $V_{\\pi}(s_{T})-V(s_{T})$ compatible with the observational distribution, by establishing its lower bound. To this end, we directly employ the natural bound [29], which has been discussed in Sec. 2.1: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{M}{\\mathrm{sin}}\\ }&{V_{\\pi}\\big(s_{(T,j)};M\\big)-V\\big(s_{(T,j)};M\\big)}\\\\ &{=\\displaystyle\\sum_{x_{t}}\\mathbb{E}_{x_{t}}\\left[Y_{T}\\ |\\ S_{T}=s_{(T,j)};M\\right]\\pi(x_{t}\\ |\\ s_{(T,j)})-V\\big(s_{(T,j)};M\\big)}\\\\ &{=\\displaystyle\\sum_{x_{t}}\\mathbb{E}\\left[Y_{T}\\ |\\ s_{(T,j)},x_{t}\\right]P(x_{t}\\ |\\ s_{(T,j)})\\pi(X_{T}=x_{t}\\ |\\ s_{(T,j)})-\\displaystyle\\sum_{x_{t}}\\mathbb{E}\\left[Y_{T}\\ |\\ s_{(T,j)},x_{t}\\right]P(x_{t}\\ |\\ s_{(T,j)})}\\\\ &{\\phantom{=}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The last step is justified because $P(X,S,Y)>0$ and $0\\leq\\pi(X_{T}=x_{t}\\mid s_{(T,j)})\\leq1$ . Intuitive examples illustrating this conclusion are provided in Sec. 2.1 and Appendix F. Therefore, this confirms the validity of the inequality for the base case. ", "page_idx": 14}, {"type": "text", "text": "Specifically, in certain degenerate cases where there is only one possible action, the imitator has no choice but to follow that single option. Consequently, unobserved confounders are less likely to introduce significant effects in these scenarios. However, under such conditions, pursuing imitation learning is not meaningful, as there is no variability in choice for the imitator to learn from. Therefore, such cases are of limited relevance to the scope of this analysis. ", "page_idx": 14}, {"type": "text", "text": "Induction case. Suppose at $t+1$ , $V_{\\pi}(s_{t+1})<V(s_{t+1})$ , we need to prove $V_{\\pi}(s_{t})<V(s_{t})$ . ", "page_idx": 14}, {"type": "equation", "text": "$$\nV_{\\pi}(s_{t})=\\mathbb{E}_{\\pi}[Y_{t}\\mid s_{t}]+\\gamma\\sum_{j}P_{\\pi}(s_{(t+1,j)}\\mid s_{t})\\underbrace{V_{\\pi}(s_{(t+1,j)})}_{<V(s_{(t+1,j)})}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Without loss of generality, we assume that the state with the minimal value at $t+1$ is denoted as $s(t{+}1,\\lvert S\\rvert)$ . Our approach is founded on the premise that in obtaining the worst-case $\\mathsf{S C M}\\,\\hat{M}$ , it is strategic to allocate the lowest possible transition probabilities to the state with the highest value, while preferentially assigning higher probabilities to states demonstrating smaller values. Specifically, one starts with the estimate $P(S_{t+1}=s_{(t+1,1)},x_{t}\\mid S_{t})$ for $P_{x_{t}}(S_{t+1}=s_{(t+1,1)}\\mid S_{t})$ . Following this logic, we systematically allocate probability masses for indices $1\\leq j\\leq|S|-1$ as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\nP_{x_{t}}(S_{t+1}=s_{(t+1,j)}\\mid S_{t})\\gets P(S_{t+1}=s_{(t+1,j)},x_{t}\\mid S_{t})\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "In accordance with the established properties of probability distributions, it follows that: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{j=1}^{|S|}P_{x_{t}}(S_{t+1}=s_{(t+1,j)}\\mid S_{t})=1.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Considering the state $s_{(t+1,|S|)}$ , the corresponding probability can be assigned as: ", "page_idx": 15}, {"type": "equation", "text": "$$\nP_{x_{t}}\\big(S_{t+1}=s_{(t+1,\\mid S\\mid)}\\mid S_{t}\\big)=1-\\sum_{j=1}^{|S|-1}P_{x_{t}}\\big(S_{t+1}=s_{(t+1,j)}\\mid S_{t}\\big).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "By substituting the assigned values, we are able to derive the following expression: ", "page_idx": 15}, {"type": "text", "text": "Pxt(St $_{\\mathbf{\\delta}+1}=s_{(t+1,\\mid S\\mid)}\\mid S_{t}\\rangle\\leftarrow1-P(s_{(t+1,1)},x_{t}\\mid S_{t})-P(s_{(t+1,2)},x_{t}\\mid S_{t})\\cdot\\cdot\\cdot-P(s_{(t+1,\\mid S\\mid-1)},x_{t}\\mid S_{t})\\,,$ where the right-hand side simplifies to: ", "page_idx": 15}, {"type": "equation", "text": "$$\n^{-1,1),\\,x_{t}\\mid S_{t})+P(s_{(t+1,2)},x_{t}\\mid S_{t})\\cdot\\cdot\\cdot+P(s_{(t+1,\\mid S\\mid-1)},x_{t}\\mid S_{t}))=\\left(\\sum_{j=1}^{\\lvert S\\mid-1}P\\left(s_{(t+1,j)},x_{t}\\mid S_{t}\\right)\\right)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "It is established that the expression $0\\le1-P(x_{t}\\mid S_{t})+P(s_{(t+1,|S|)},x_{t}\\mid S_{t})\\le1$ holds true. This inequality is supported by the following equation: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{j=1}^{|S|}P(s_{(t+1,j)},x_{t}\\mid S_{t})=P(x_{t}\\mid S_{t}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "To further analyze the expert policy, the associated value function $V(s_{t})$ can be expanded as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{V(s_{t})=\\mathbb{E}\\left[\\sum_{k=0}^{\\infty}\\gamma^{k}Y_{t+k}\\mid S_{t}=s_{t}\\right]}}\\\\ &{}&{=\\mathbb{E}[Y_{t}\\mid s_{t}]+\\gamma\\sum_{j}P(s_{(t+1,j)}\\mid s_{t})V(s_{(t+1,j)}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\gamma$ is the discount factor, $P(s_{(t+1,j)}~\\vert~s_{t})$ denotes the observational transition probability. Notably, $P(s_{(t+1,j)}~\\vert~s_{t})$ and $P_{\\pi}(s_{(t+1,j)}~|~s_{t})$ are generally different, because they reflect two distinct probabilities: $P(s_{(t+1,j)}\\mid s_{t})$ corresponding to the observational distribution and the other, $P_{\\pi}\\big(s_{(t+1,j)}\\mid s_{t}\\big)$ , representing the imitator\u2019s transition dynamics. ", "page_idx": 15}, {"type": "text", "text": "In accordance with the established properties of probability distributions, it follows that: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{j=1}^{|S|}P(S_{t+1}=s_{(t+1,j)}\\mid S_{t})=1.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Without loss of generality, suppose the policy is a deterministic policy. Actually, the following proof holds true regardless of the choice of $x_{t}$ . Subsequently, we analyze the gap between $V_{\\pi}(s_{t})$ and $V(s_{t})$ as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{V_{\\pi}(s_{t})-V(s_{t})}\\\\ {=\\Bigg(\\mathbb{E}_{\\pi}[Y_{t}\\mid s_{t}]+\\gamma\\underset{j=1}{\\overset{\\lfloor S\\rfloor}{\\sum}}P_{\\pi}(s_{(t+1,j)}\\mid s_{t})V_{\\pi}(s_{(t+1,j)})\\Bigg)}\\\\ {-\\Bigg(\\mathbb{E}[Y_{t}\\mid s_{t}]+\\gamma\\underset{j=1}{\\overset{\\lfloor S\\rfloor}{\\sum}}P(s_{(t+1,j)}\\mid s_{t})V(s_{(t+1,j)})\\Bigg)}\\\\ {=\\Bigg(\\mathbb{E}_{\\pi}[Y_{t}\\mid s_{t}]+\\gamma\\underset{j=1}{\\overset{\\lfloor S\\rfloor}{\\sum}}\\sum_{\\pi}P_{s_{t}}(s_{(t+1,j)}\\mid s_{t})\\pi(x_{t}\\mid s_{t})V_{\\pi}(s_{(t+1,j)})\\Bigg)}\\\\ {-\\Bigg(\\mathbb{E}[Y_{t}\\mid s_{t}]+\\gamma\\underset{j=1}{\\overset{\\lfloor S\\rfloor}{\\sum}}P(s_{(t+1,j)}\\mid s_{t})V(s_{(t+1,j)})\\Bigg)}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{M}{\\operatorname*{min}}\\ }&{V_{\\pi}\\big(s_{\\ell};M\\big)-V\\big(s_{\\ell};M\\big)}\\\\ &{=\\mathbb{E}_{\\pi}\\big[Y_{\\ell}\\ \\big|\\ s_{\\ell};M\\big]-\\mathbb{E}\\big[Y_{\\ell}\\ \\big|\\ s_{\\ell};M\\big]+\\gamma\\underset{j=1}{\\overset{|S|-1}{\\sum}}\\big(P\\big(s_{\\ell+1,j}\\big),x_{\\ell}\\ \\big|s_{\\ell}\\big)-P\\big(s_{\\ell+1,j}\\big)\\ \\nu_{\\ell}\\big)}\\\\ &{+\\gamma\\left(1-\\left(\\underset{j=1}{\\overset{|S|-1}{\\sum}}P\\big(s_{\\ell+1,j}\\big),x_{\\ell}\\ \\big|s_{\\ell}\\right)\\right)-P\\big(s_{\\ell+1,j}\\ x_{\\ell}\\big)\\ \\nu_{\\ell}\\Bigg)\\ V\\big(s_{\\ell+1,j}\\big|)}\\\\ &{=\\underset{M}{\\underline{{\\mathbb{E}}}}\\big[Y_{\\ell}\\ \\big|s_{\\ell};M\\big]-\\mathbb{E}\\big[Y_{\\ell}\\ \\big|s_{\\ell};M\\big]}\\\\ &{+\\gamma\\underset{M}{\\overset{|S|-1}{\\sum}}\\left(P\\big(s_{\\ell+1,j}\\big),x_{\\ell}\\ \\big|s_{\\ell}\\right)-P\\big(s_{\\ell+1,j}\\ \\big)\\ \\big(V\\big(s_{\\ell+1,j}\\big)-V\\big(s_{\\ell+1,j}\\big)\\big)\\right)}\\\\ &{<0}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\operatorname*{min}_{M}\\mathbb{E}_{\\pi}[Y_{t}\\mid s_{t};M]-\\mathbb{E}[Y_{t}\\mid s_{t};M]<0$ follows a similar logic as previously introduced in the base case, and $V(s_{(t+1,j)})-V\\big(s_{(t+1,|S|)}\\big)>0$ is consistent with the ordering assumption, where the state $s_{(t+1,|S|)}$ represents the minimal value. ", "page_idx": 16}, {"type": "text", "text": "In some degenerated cases when $\\mathbb{E}_{\\pi}[Y_{t}\\mid s_{t}]=0$ and $\\mathbb{E}[Y_{t}\\mid s_{t}]=0$ , it might coincidentally follow that $\\bar{V_{\\pi}}\\bar{(}s_{t}\\bar{)}=0$ , which is equal to $V(s_{t})=0$ . Another instance of degeneracy occurs when the value function $V\\big(s_{(t+1,j)}\\big)$ remains the same across all states. Such occurrences are extremely unlikely in practical scenarios, especially when $P(X,S,Y)>0$ . \u53e3 ", "page_idx": 16}, {"type": "text", "text": "B Derivations for Causal GAIL Algorithms ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Theorem 2. Given an MDP $M$ compatible with the causal graph of Fig. 3a, let $\\mathcal{R}$ be a parametric family containing the conditional reward $\\mathbb{E}[Y_{t}\\mid s_{t},x_{t}]$ . Consider the following optimization program, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\nu^{*}=\\operatorname*{min}_{\\pi}\\operatorname*{max}_{{\\tilde{\\mathcal{R}}}\\in{\\mathcal{R}}}\\sum_{s,x}{\\widetilde{\\mathcal{R}}}(s,x)P(x\\mid s)\\left(\\rho(s)-\\pi(x\\mid s)\\rho_{\\pi}(s)\\right)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "When the gap $\\nu^{*}\\leq0$ , the solution $\\pi^{*}$ is an imitating policy satisfying $\\mathbb{E}_{\\pi^{*}}\\left[R_{1}\\right]\\geq\\mathbb{E}[R_{1}]$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. Based on Eq. (13), we have: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\pi}\\left[R_{1}\\right]=\\sum_{s,x}\\mathcal{R}\\left(s,x\\right)\\pi(x\\mid s)\\rho_{\\pi}(s)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "It follows from Eq. (10) that parametrization of $\\mathcal{R}(s,x)$ can be bound from the observational distribution. The imitator\u2019s expected return could thus be lower bounded as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\pi}\\left[R_{1}\\right]\\geq\\sum_{s,x}\\widetilde{\\mathcal{R}}\\left(s,x\\right)P(x\\mid s)\\pi(x\\mid s)\\rho_{\\pi}(s)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Note that the expert\u2019s expected return could be similarly decomposed as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[R_{1}\\right]=\\sum_{x,s}\\widetilde{\\mathcal{R}}(s,x)P(x\\mid s)\\rho(s),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\begin{array}{r}{\\rho(s)=\\sum_{t=0}^{\\infty}\\gamma^{t}P\\left(S_{t}=s\\right)}\\end{array}$ is the expert\u2019s occupancy measure. ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\nu^{*}=\\underset{\\pi}{\\mathrm{min}}\\underset{M}{\\mathrm{max}}}&{{}\\mathbb{E}\\left[R_{1};M\\right]-\\mathbb{E}_{\\pi}\\left[R_{1};M\\right]}\\\\ {=\\underset{\\pi}{\\mathrm{min}}\\underset{\\Tilde{R},\\mathcal{R}}{\\mathrm{max}}}&{{}\\sum_{x,s}\\Tilde{\\mathcal{R}}(s,x)P(x\\mid s)\\rho(s)-\\sum_{s,x}\\mathcal{R}\\left(s,x\\right)\\pi(x\\mid s)\\rho_{\\pi}(s)}\\\\ {=\\underset{\\pi}{\\mathrm{min}}\\underset{\\Tilde{\\mathcal{R}}}{\\mathrm{max}}}&{{}\\sum_{s,x}\\Tilde{\\mathcal{R}}(s,x)P(x\\mid s)\\left(\\rho(s)-\\pi(x\\mid s)\\rho_{\\pi}(s)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which is the ultimate target expression. ", "page_idx": 16}, {"type": "text", "text": "Next, we will show the derivation details for matching weighted occupancy measures between the imitator and the expert. Suppose $\\psi^{*}\\,=\\,\\operatorname*{max}_{\\mathcal{R}}\\,a^{\\top}\\mathcal{R}\\,-\\,\\mathbf{\\bar{\\psi}}(\\mathcal{R})$ is a conjugate function of $\\psi$ . Following a similar logic in [19], we utilize a smiliar cost regularizer $\\psi_{G A}$ , leading to the formulation of Alg. 1. Basically, Alg. 1 minimizes Jensen-Shannon divergence between $P(x\\mid s)\\rho(s)$ and $P(x\\mid s)\\pi(x\\mid s)\\rho_{\\pi}(\\stackrel{\\cdot}{s})$ . ", "page_idx": 17}, {"type": "text", "text": "First, we reformulate the equation into state-action occupancy measures: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\psi^{*}\\left(P(x\\mid s)\\rho(s)-P(x\\mid s)\\pi(x\\mid s)\\rho_{\\pi}(s)\\right)=\\psi^{*}\\left(\\rho(s,x)-P(x\\mid s)\\rho_{\\pi}(s,x)\\right)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Based on the definition of $\\psi^{*}$ , we have: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\psi^{*}\\left(\\rho(s,x)-P(x\\mid s)\\rho_{\\pi}(s,x)\\right)}\\\\ &{\\ =\\displaystyle\\operatorname*{max}_{\\mathcal{R}}\\sum_{s,x}\\left(\\rho(s,x)-P(x\\mid s)\\rho_{\\pi}(s,x)\\right)\\mathcal{R}(s,x)-\\sum_{s,x}P(x\\mid s)\\rho_{\\pi}(s,x)g_{\\phi}(\\mathcal{R}(s,x))}\\\\ &{\\ =\\displaystyle\\sum_{s,x}\\operatorname*{max}_{\\mathcal{R}}\\rho(s,x)\\mathcal{R}-P(x\\mid s)\\rho_{\\pi}(s,x)\\phi\\left(-\\phi^{-1}(-\\mathcal{R})\\right)}\\\\ &{\\ =\\displaystyle\\sum_{s,x}\\operatorname*{max}_{\\mathcal{R}^{\\prime}}\\rho(s,x)(-\\phi(\\mathcal{R}^{\\prime}))-P(x\\mid s)\\rho_{\\pi}(s,x)\\phi\\left(-\\phi^{-1}(\\phi(\\mathcal{R}^{\\prime}))\\right)}\\\\ &{\\ =\\displaystyle\\sum_{s,x}\\operatorname*{max}_{\\mathcal{R}^{\\prime}}\\rho(s,x)(-\\phi(\\mathcal{R}^{\\prime}))-P(x\\mid s)\\rho_{\\pi}(s,x)\\phi\\left(-\\mathcal{R}^{\\prime}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where we make the change of variables $\\mathcal{R}\\,\\rightarrow\\,-\\phi(\\mathcal{R}^{\\prime})$ . Suppose $D\\,\\in\\,\\mathcal{S}\\,\\times\\,\\mathcal{X}\\,\\mapsto\\,(0,1)$ is a discriminator classifier (e.g, a neural network). Using the logistic loss $\\phi(x)=\\log{(1+e^{-x})}$ , we can get: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\psi^{*}\\left(\\rho(s,x)-P(x\\mid s)\\rho_{\\pi}(s,x)\\right)}\\\\ &{=\\displaystyle\\sum_{s,x}\\operatorname*{max}_{\\mathcal{R}^{\\prime}}\\rho(s,x)\\log\\left(\\frac{1}{1+e^{-\\mathcal{R}^{\\prime}}}\\right)+P(x\\mid s)\\rho_{\\pi}(s,x)\\log\\left(1-\\frac{1}{1+e^{-\\mathcal{R}^{\\prime}}}\\right)}\\\\ &{=\\displaystyle\\operatorname*{max}_{D\\in(0,1)^{{s}\\times\\,x}}\\mathbb{E}[\\log(D(S,X))]+\\mathbb{E}_{\\pi}\\left[P(x\\mid s)\\log(1-D(S,X))\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which is the ultimate target expression. ", "page_idx": 17}, {"type": "text", "text": "Theorem 3. Given an MDP $M$ compatible with the causal graph of Fig. $3b$ , let $\\mathcal{R}$ be a parametric family containing the conditional reward $\\mathbb{E}[Y_{t}~\\vert~s_{t},x_{t}]$ , and $\\mathcal{T}$ be a parametric family over conditional probabilities $P\\left(s_{t+1}\\mid s_{t},x_{t}\\right)$ defined in Eq. (20). Consider the following program, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\nu^{*}=\\underset{\\pi}{\\mathrm{min}}\\ \\underset{\\mathcal{R}\\in\\mathcal{R}}{\\mathrm{max}}\\ \\underset{\\mathcal{T}\\in\\mathcal{T}}{\\mathrm{max}}\\ \\sum_{s,x}\\mathcal{R}(s,x)\\left(P(x\\mid s)\\rho(s)-\\pi(x\\mid s)\\rho_{\\pi}\\left(s;\\mathcal{T}\\right)\\right)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "When the gap $\\nu^{*}\\leq0$ , the solution $\\pi^{*}$ is an imitating policy satisfying $\\mathbb{E}_{\\pi^{*}}$ $\\tau^{*}\\,\\left[R_{1}\\right]\\geq\\mathbb{E}[R_{1}]$ . ", "page_idx": 17}, {"type": "text", "text": "Proof. Based on Eq. (13), we have: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\mathbb{E}_{\\pi}\\left[R_{1}\\right]=\\displaystyle\\sum_{s,x}{\\mathcal{R}\\left(s,x\\right)\\pi(x\\mid s)\\rho_{\\pi}(s)}}\\\\ {=\\displaystyle\\sum_{s,x}{\\mathcal{R}\\left(s,x\\right)\\underbrace{\\rho_{\\pi}(s,x)}_{\\mathrm{Non-ID}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The reward function $\\mathcal{R}$ is identifiable and must be contained in the parametric space of the expert\u2019s nominal reward $\\mathcal{R}$ . In other words, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{R}\\left(s,x\\right)=\\widetilde{\\mathcal{R}}\\left(s,x\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The transition distribution $\\tau$ can be bounded from the demonstration data using Eq. (9). Therefore, we get: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\nu^{*}=\\underset{\\pi}{\\underset{\\pi}{\\operatorname*{min}}}\\underset{M}{\\operatorname*{max}}}&{\\mathbb{E}\\left[R_{1};M\\right]-\\mathbb{E}_{\\pi}\\left[R_{1};M\\right]}\\\\ {=\\underset{\\pi}{\\operatorname*{min}}\\ \\underset{T,\\mathcal{R},\\mathcal{R}}{\\operatorname*{max}}}&{\\underset{s,x}{\\sum}\\,\\widetilde{\\mathcal{R}}(s,x)P(x\\mid s)\\rho(s)-\\mathcal{R}\\left(s,x\\right)\\rho_{\\pi}(s,x;T)}\\\\ {=\\underset{\\pi}{\\operatorname*{min}}\\ \\underset{T,\\mathcal{R}}{\\operatorname*{max}}}&{\\underset{s,x}{\\sum}\\,\\mathcal{R}\\left(s,x\\right)\\left(P(x\\mid s)\\rho(s)-\\rho_{\\pi}(s,x;T)\\right)}\\\\ {=\\underset{\\pi}{\\operatorname*{min}}\\ \\underset{T\\in\\mathcal{T},\\mathcal{R}\\in\\mathcal{R}}{\\operatorname*{max}}\\ }&{\\underset{s,x}{\\sum}\\,\\mathcal{R}(s,x)\\left(P(x\\mid s)\\rho(s)-\\pi(x\\mid s)\\rho_{\\pi}\\left(s;T\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which is the ultimate desired expression. ", "page_idx": 18}, {"type": "text", "text": "Consider again the expected return decomposition in Eq. (13). The reward function $\\mathcal{R}$ is identifiable and must be contained in the parametric space of the expert\u2019s nominal reward $\\mathcal{R}$ . The transition distribution $\\tau$ can be bounded from the demonstration data using Eq. (9). One could thus obtain a lower bound over the imitator\u2019s performance by reasoning about the worst-case occupancy measure compatible with demonstrations. Formally, with the fixed reward function $\\mathcal{R}$ and the fixed policy $\\pi$ , the imitator\u2019s return is bounded by ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{\\pi}\\left[R_{1}\\right]\\geq\\underset{T,\\rho_{\\pi}}{\\mathrm{min}}}&{\\displaystyle\\sum_{s,x}\\mathcal{R}(s,x)\\pi(x\\mid s)\\rho_{\\pi}(s)}\\\\ {\\mathrm{subject~to}:}&{\\displaystyle\\rho_{\\pi}(s)\\geq0,\\quad\\mathrm{and}\\ \\underset{s}{\\sum}\\rho_{\\pi}(s)=\\frac{1}{1-\\gamma}}\\\\ &{\\rho_{\\pi}\\left(s\\right)=P\\left(s\\right)+\\displaystyle\\gamma\\sum_{s^{\\prime},x}\\mathcal{T}(s^{\\prime},x,s)\\,\\pi(x\\mid s^{\\prime})\\rho_{\\pi}(s^{\\prime})}\\\\ {\\vdots}&{\\mathrm{Constraints~}\\mathcal{T}:}&{\\displaystyle\\left\\{\\sum_{T}\\mathcal{T}\\left(s^{\\prime},x,s\\right)=1,\\quad\\mathrm{and}\\,\\mathcal{T}\\left(s,x,s^{\\prime}\\right)\\geq\\widetilde{T}\\left(s,x,s^{\\prime}\\right)P(x\\mid s)\\right.}\\\\ {\\vdots}&{\\displaystyle\\left.\\left\\{\\mathcal{T}\\left(s,x,s^{\\prime}\\right)\\leq\\widetilde{T}\\left(s,x,s^{\\prime}\\right)P(x\\mid s)+P\\left(-x\\mid s\\right)\\right.\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The above optimization problem is similar to the classic linear program for planning in MDPs [36]. The main difference is that the transition distribution $\\tau$ is no longer fixed but bounded in a convex space $\\mathcal{T}$ specified from the observational data. Similar to the previous setting, we could solve an imitating policy by minimizing the performance gap between the imitator and the expert in the worst-case environment compatible with the observational data and prior knowledge. ", "page_idx": 18}, {"type": "text", "text": "Next, we will provide a heuristic algorithm to solve the optimization program presented in Eq. (19) and Eq. (20). Specifically, as discussed in Eq. (9), we are able to bound the transition distribution $\\tau$ by: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{T}\\left(s,x,s^{\\prime}\\right)\\in\\left[\\widetilde{\\mathcal{T}}\\left(s,x,s^{\\prime}\\right)P(x\\mid s),\\widetilde{\\mathcal{T}}\\left(s,x,s^{\\prime}\\right)P(x\\mid s)+P\\left(\\neg x\\mid s\\right)\\right].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The intuition for Alg. 3 is: in order to find the worst case, we need to put as less transition probability mass as possible to the state with maximal values, and allocate higher transition probabilities to states with smaller values. Without loss of generality, suppose $V_{x_{t}}\\big(s_{(t+1,|S|)}\\big)$ is found to have the smallest relative value. For all other states $j\\neq|S|$ , we need to allocate as less transition probability mass as possible. Therefore, we take the lower bound: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P_{x_{t}}(S_{t+1}=s_{(t+1,j)}\\mid s_{t}):=P(S_{t+1}=s_{(t+1,j)},x_{t}\\mid s_{t})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad:=P(S_{t+1}=s_{(t+1,j)}\\mid s_{t},x_{t})P(x_{t}\\mid s_{t}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $P_{x_{t}}(s_{t+1}\\ \\mid\\ s_{t})\\ =\\ P(s_{t+1}\\ \\mid\\ s_{t},\\mathrm{do}(x_{t}))\\ =\\ T(s_{t},x_{t},s_{t+1})$ , and $P(s_{(t+1)}\\;\\;\\mid\\;s_{t},x_{t})\\;=$ $\\widetilde{\\mathcal{T}}(s_{t},x_{t},s_{t+1})$ . For the state $s_{(t+1,|S|)}$ , we have: ", "page_idx": 18}, {"type": "equation", "text": "$$\nP_{x_{t}}(S_{t+1}=s_{(t+1,\\mid S\\mid)}\\mid s_{t}):=1-\\left(\\sum_{j=1}^{\\left\\lvert S\\right\\rvert-1}P(S_{t+1}=s_{(t+1,j)},x_{t}\\mid s_{t})\\right).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Algorithm 3: Find Worst-Case Discounted Future Reward ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "1: Input: $P(s_{t+1},x_{t}|s_{t})$ , the value function $V_{x_{t}}(s_{t})$   \n2: Output: Probability mass assignments for non-ID transitions $\\tau$   \n3: Let $V_{x_{t}}\\big(s_{(t+1,|S|)}\\big)$ is determined to have the minimal relative value   \n4: Set ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad P_{x_{t}}\\big(S_{t+1}=s_{(t+1,j)}\\mid s_{t}\\big):=P\\big(S_{t+1}=s_{(t+1,j)},x_{t}\\mid s_{t}\\big),\\mathrm{where~}j\\neq\\mid S_{t}\\mid}\\\\ &{P_{x_{t}}\\big(S_{t+1}=s_{(t+1,\\mid S\\mid)}\\mid s_{t}\\big):=1-\\left(\\displaystyle\\sum_{j=1}^{\\lfloor S\\mid-1}P\\big(S_{t+1}=s_{(t+1,j)},x_{t}\\mid s_{t}\\big)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "5: return ", "page_idx": 19}, {"type": "text", "text": "Following a similar logic in Alg. 1: we reformulate the equation into state-action occupancy measures: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\psi^{*}\\left(P(x\\mid s)\\rho(s)-\\pi(x\\mid s)\\rho_{\\pi}(s;T)\\right)=\\psi^{*}\\left(\\rho(s,x)-\\rho_{\\pi}(s,x;T)\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Based on the definition of $\\psi^{*}$ , we have: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\psi^{*}\\left(\\rho(s,x)-\\rho_{\\pi}(s,x;T)\\right)}\\\\ &{=\\displaystyle\\operatorname*{max}_{T,\\,\\mathcal{R}}\\sum_{\\substack{s,x}}\\left(\\rho(s,x)-\\rho_{\\pi}(s,x;T)\\right)\\mathcal{R}(s,x)-\\displaystyle\\sum_{s,x}\\rho_{\\pi}(s,x;T)g_{\\phi}(\\mathcal{R}(s,x))}\\\\ &{=\\displaystyle\\sum_{s,x}\\operatorname*{max}_{T,\\,\\mathcal{R}}\\rho(s,x)\\mathcal{R}-\\rho_{\\pi}(s,x;T)\\phi\\left(-\\phi^{-1}(-\\mathcal{R})\\right)}\\\\ &{=\\displaystyle\\sum_{s,x}\\operatorname*{max}_{T,\\,\\mathcal{R}^{\\prime}}\\rho(s,x)(-\\phi(\\mathcal{R}^{\\prime}))-\\rho_{\\pi}(s,x;T)\\phi\\left(-\\phi^{-1}(\\phi(\\mathcal{R}^{\\prime}))\\right)}\\\\ &{=\\displaystyle\\sum_{s,x}\\operatorname*{max}_{T,\\,\\mathcal{R}^{\\prime}}\\rho(s,x)(-\\phi(\\mathcal{R}^{\\prime}))-\\rho_{\\pi}(s,x;T)\\phi\\left(-\\mathcal{R}^{\\prime}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Suppose $D\\in\\mathcal{S}\\times\\mathcal{X}\\mapsto(0,1)$ is a discriminator classifier (e.g, a neural network). Using the logistic loss $\\phi(x)=\\log{(1+e^{-x})}$ , we can get: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\psi^{*}\\left(\\rho(s,x)-\\rho_{\\pi}(s,x;T)\\right)}\\\\ &{=\\displaystyle\\sum_{s,x}\\operatorname*{max}_{T,\\mathcal{R}^{\\prime}}\\rho(s,x)\\log\\left(\\frac{1}{1+e^{-\\mathcal{R}^{\\prime}}}\\right)+\\rho_{\\pi}(s,x;T)\\log\\left(1-\\frac{1}{1+e^{-\\mathcal{R}^{\\prime}}}\\right)}\\\\ &{=\\displaystyle\\operatorname*{max}_{T,D}\\mathbb{E}[\\log(D(S,X))]+\\mathbb{E}_{\\boldsymbol\\pi}\\left[\\log(1-D(S,X));T\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Therefore, we are able to obtain the ultimate target expression: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\nu^{*}=\\operatorname*{min}_{\\pi}\\operatorname*{max}_{\\substack{T\\in\\mathcal{T},D\\in(0,1)^{s}\\times x}}\\mathbb{E}[\\log(D(S,X))]+\\mathbb{E}_{\\pi}\\left[\\log(1-D(S,X));T\\right].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "C Finding the Worst-Case Transition Distribution ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we provide a practical algorithm, Alg. 3, designed to solve the optimization problem formulated in Eq. (19) and Eq. (20). The underlying rationale of Alg. 3 is to search for the worst-case scenario by allocating the minimal transition probability mass to the state with the highest value while assigning greater transition probabilities to states with lower values. The resulting solution should still adhere to a set of predefined observational constraints to ensure feasibility. This approach ensures that the most \u201cadversarial\u201d outcome is prioritized during the optimization process. ", "page_idx": 19}, {"type": "text", "text": "To further clarify the approach above, consider the following numerical example. Suppose there are only two states. The value function $V_{x_{t}}(s_{t+1})$ takes on two values: $V_{x_{t}}(s_{(t+1,1)})\\,=\\,0.8$ and $V_{x_{t}}(s_{(t+1,2)})\\,=\\,0.2$ . Because $V_{x_{t}}(s_{(t+1,1)})\\,>\\,V_{x_{t}}(s_{(t+1,2)})$ , the algorithm seeks the worst-case discounted future reward by allocating $P_{x_{t}}(s_{(t+1,1)}\\mid s_{t})\\gets P(s_{(t+1,1)},x_{t}\\mid s_{t})$ and $P_{x_{t}}(s_{(t+1,2)}\\mid$ $s_{t})\\gets1-P(s_{(t+1,1)},x_{t}\\mid s_{t})^{3}$ . As such, we are able to collect trajectories from the imitator, even though $P_{\\pi}(s_{t+1}\\mid s_{t})$ is not identifiable. ", "page_idx": 19}, {"type": "text", "text": "D More Details for the Experiments ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "All experiments were conducted using Intel Cascade Lake processors, with 30 vCPUs and 120 GB memory on a system running Ubuntu 18.04. Upon acceptance of this manuscript, we intend to make the source code available in the camera-ready version of the paper. ", "page_idx": 20}, {"type": "text", "text": "$\\mathbf{MDP_{obs}}$ Previously, 1000 random discrete causal models are sampled and all the performance gaps are less than 0. In other words, when both the reward and the transition are confounded, all imitators fail to match expert performance. ", "page_idx": 20}, {"type": "text", "text": "Specifically, let\u2019s take a look at one example instance of those randomly sampled SCM instances. Its detailed parameterization is provided as follows: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P(s_{0})=0.5,}\\\\ &{P(x_{0},y_{0},s_{0}^{\\prime}\\mid s_{0})=0.1888,\\quad P(x_{0},y_{0},s_{1}^{\\prime}\\mid s_{0})=0.2099,}\\\\ &{P(x_{0},y_{1},s_{0}^{\\prime}\\mid s_{0})=0.0294,\\quad P(x_{0},y_{1},s_{1}^{\\prime}\\mid s_{0})=0.2116,}\\\\ &{P(x_{1},y_{0},s_{0}^{\\prime}\\mid s_{0})=0.1465,\\quad P(x_{1},y_{0},s_{1}^{\\prime}\\mid s_{0})=0.026,}\\\\ &{P(x_{1},y_{1},s_{0}^{\\prime}\\mid s_{0})=0.0645,\\quad P(x_{1},y_{1},s_{1}^{\\prime}\\mid s_{0})=0.1267,}\\\\ &{P(x_{0},y_{0},s_{0}^{\\prime}\\mid s_{1})=0.1762,\\quad P(x_{0},y_{0},s_{1}^{\\prime}\\mid s_{1})=0.1775,}\\\\ &{P(x_{0},y_{1},s_{0}^{\\prime}\\mid s_{1})=0.0290,\\quad P(x_{0},y_{1},s_{1}^{\\prime}\\mid s_{1})=0.1786,}\\\\ &{P(x_{1},y_{0},s_{0}^{\\prime}\\mid s_{1})=0.1761,\\quad P(x_{1},y_{0},s_{1}^{\\prime}\\mid s_{1})=0.0893,}\\\\ &{P(x_{1},y_{1},s_{0}^{\\prime}\\mid s_{1})=0.1472,\\quad P(x_{1},y_{1},s_{1}^{\\prime}\\mid s_{1})=0.0261,}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $s^{\\prime}$ denotes the next state; $P(x_{0},y_{0},s_{0}^{\\prime}\\mid s_{0})$ is the abbreviation format for $P(X_{t}=x_{0},Y_{t}=$ $y_{0},S_{t+1}=s_{0}^{\\prime}\\mid S_{t}=s_{0}\\rangle$ . ", "page_idx": 20}, {"type": "text", "text": "The expert is able to observe the state $S_{t}$ , the unobserved variable $U_{t}$ , and the reward $Y_{t}$ . However, the imitator, lacking access to both $U_{t}$ or the reward $\\mathbb{E}_{\\pi}[Y_{t}]$ , makes decisions solely on $S_{t}$ . In other words, all methods utilize the same policy scope $\\pi(x\\mid s)$ . As shown in Fig. 4a, imitators consistently failed to match expert performance. Prevalent negative performance gaps indicate that most of imitators were significantly worse than experts; only in rare cases did the performance gaps near $-0.5$ , supporting our theoretical insights presented in Thm. 1. Furthermore, as depicted in Fig. 5a, CAIL does not achieve expert-level performance, specifically, $\\mathbb{E}_{\\pi}\\left[R_{t}\\right]-\\mathbb{E}\\left[R_{t}\\right]=-1.9019$ . However, although CAIL performs worse than the expert, CAIL still consistently outperforms BC and GAIL by effectively learning from the constructed worst-case MDP instances. ", "page_idx": 20}, {"type": "image", "img_path": "KHX0dKXdqH/tmp/d8b9224fb4ed34cf2a39a629c558c23ca533641381a585c10dd182ce8ff1a97c.jpg", "img_caption": ["Figure 5: Simulation results for experiments that are not included in the main manuscript. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "$\\mathbf{MDP_{obs-Y}}$ : Additional Experiment. Consider an SCM instance compatible with Fig. 3a including binary observed variables $S_{t},X_{t},Y_{t}\\in\\{0,1\\}$ . $S_{t}$ represents the state at each time step. $X_{t}$ denotes the action. The unobserved variable $U_{t}$ represents some information accessible to the expert but inaccessible to the imitator. Additionally, the imitator lacks access to the reward $\\mathbb{E}_{\\pi}[Y_{t}]$ . Its detailed parameterization is provided as follows: ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{P(s_{0})=0.5,}\\\\ {P(x_{0},y_{0},s_{0}^{\\prime}\\mid s_{0})=0.1775,}&{P(x_{0},y_{0},s_{1}^{\\prime}\\mid s_{0})=0.2029,}\\\\ {P(x_{0},y_{1},s_{0}^{\\prime}\\mid s_{0})=0.0001,}&{P(x_{0},y_{1},s_{1}^{\\prime}\\mid s_{0})=0.0001,}\\\\ {P(x_{1},y_{0},s_{0}^{\\prime}\\mid s_{0})=0.0993,}&{P(x_{1},y_{0},s_{1}^{\\prime}\\mid s_{0})=0.0199,}\\\\ {P(x_{1},y_{1},s_{0}^{\\prime}\\mid s_{0})=0.2001,}&{P(x_{1},y_{1},s_{1}^{\\prime}\\mid s_{0})=0.3001,}\\\\ {P(x_{0},y_{0},s_{0}^{\\prime}\\mid s_{1})=0.2859,}&{P(x_{0},y_{0},s_{1}^{\\prime}\\mid s_{1})=0.1359,}\\\\ {P(x_{0},y_{1},s_{0}^{\\prime}\\mid s_{1})=0.0001,}&{P(x_{0},y_{1},s_{1}^{\\prime}\\mid s_{1})=0.0001,}\\\\ {P(x_{1},y_{0},s_{0}^{\\prime}\\mid s_{1})=0.2969,}&{P(x_{1},y_{0},s_{1}^{\\prime}\\mid s_{1})=0.2809,}\\\\ {P(x_{1},y_{1},s_{0}^{\\prime}\\mid s_{1})=0.0001,}&{P(x_{1},y_{1},s_{1}^{\\prime}\\mid s_{1})=0.0001,}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $s^{\\prime}$ denotes the next state; $P(x_{0},y_{0},s_{0}^{\\prime}\\mid s_{0})$ is the abbreviation format for $P(X_{t}=x_{0},Y_{t}=$ $y_{0},S_{t+1}=s_{0}^{\\prime}\\mid S_{t}=s_{0}\\rangle$ . As depicted in Fig. 5b, CAIL performs the best among all strategies. Both BC and GAIL fail to match expert performance. Such result shows the effectiveness of Alg. 1. ", "page_idx": 21}, {"type": "text", "text": "E Broader Impacts ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "This paper investigates the theoretical framework of causal imitation learning from confounded demonstrations. Our framework is versatile, applicable to various real-world domains such as autonomous driving, robotics, industrial automation, and medical decisions modeling. One of the positive impacts of this study is the exploration of the risks associated with training IRL algorithms when demonstrations are generally contaminated by unobserved confounders. We theoretically prove that when both the transition distribution $\\tau$ and reward function $\\mathcal{R}$ are not identifiable, there is no policy $\\pi$ learnable from confounded demonstrations that is guaranteed to perform at least as the expert in all possible scenarios. Such theoretical findings have been validated through extensive randomly generated causal models. When either the reward function or the transition distribution is confounded, we augment the GAIL framework by utilizing partial identification techniques, so that the imitator is optimized within the worst-case scenarios. Specfically, the worst-case reward function in Alg. 1 and the worst-case occupancy measure in Alg. 2. By mitigating the risks associated with unobserved confounders in expert demonstrations, our framework supports the development of more transparent and accountable AI systems. This transparency is crucial in high-stakes areas such as healthcare and transportation, where decision-making errors can have significant repercussions. More broadly, our framework significantly enhances the reliability and safety of autonomous systems in various fields, which prioritize safety and robustness during their decision-making processes. They are increasingly important because black-box AI systems, \u2013 whose internal workings remain opaque \u2013 become more and more prevalent, and our understandings of their potential implications remain limited. ", "page_idx": 21}, {"type": "text", "text": "F Impossibility Result in Two-Stage MDPs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this extension of the MAB model introduced in Sec. 1, we explore a two-stage framework (see Fig. 2b). Our previous discussions demonstrated that in MAB settings affected by unobserved confounders, the expert consistently outperforms the imitator; that is, i.e., $\\mathbb{E}_{x}\\left[Y\\right]<\\mathbb{E}[Y]$ . ", "page_idx": 21}, {"type": "text", "text": "We now extend our analysis to the two-stage MDPs. Specifically, the agent first observes the state $S_{1}$ , selects an action $X_{1}$ , and subsequently, it receives a reward $Y_{1}$ . The process then progresses to the second stage, where the agent transitions to state $S_{2}$ . It chooses an action $X_{2}$ , and then it receives a further reward $Y_{2}$ . A pivotal distinction between this scenario and prior examples lies in the transition probability $P_{\\pi_{1}}(S_{2}\\mid S_{1})$ . Therefore, we investigate their cumulative reward: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\pi_{1},\\pi_{2}}[Y_{1}+Y_{2}]\\qquad\\mathrm{and}\\qquad\\mathbb{E}[Y_{1}+Y_{2}].\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "As a motivating example, we assume that all variables are binary. Our analysis begins by comparing the performance at the final stage, specifically, $\\mathbb{E}_{\\pi_{1},\\pi_{2}}[Y_{2}]$ . ", "page_idx": 21}, {"type": "text", "text": "Suppose $f(S_{2})=\\mathbb{E}[Y_{2}\\mid S_{2},X_{2}]P(X_{2}\\mid S_{2})$ . Without loss of generality, we assume an ordering in the functional values associated with different states: $f(S_{2}\\bar{=}\\,0)>\\bar{f}(S_{2}=1)$ . To address the non-identifiability issue caused by the transition distribution $P_{\\pi_{1}}(S_{2}\\mid S_{1})$ , as discussed in Eq. (9), we formulate the worst-case SCM by allocating $f(S_{2}=0)$ with probability mass $P(S_{2}=0,\\bar{X}_{1}\\mid S_{1})$ . In other words, we assign the lower bound $P(S_{2}\\;=\\;0,x_{1}\\;\\;|\\;\\;Z_{1})$ to the non-identifiable query $P_{x_{1}}(S_{2}=0\\mid Z_{1})$ . As such, we are able to rewrite the expert\u2019s rewards as follows: ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[Y_{2}]=f(S_{2}=0)*P(S_{2}=0,X_{1}=0|Z_{1})P(Z_{1})}\\\\ {+f(S_{2}=0)*P(S_{2}=0,X_{1}=1|Z_{1})P(Z_{1})}\\\\ {+f(S_{2}=1)*P(S_{2}=1,X_{1}=0|Z_{1})P(Z_{1})}\\\\ {+f(S_{2}=1)*P(S_{2}=1,X_{1}=1|Z_{1})P(Z_{1})}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and the imitator\u2019s reward can be written as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\pi_{1},\\pi_{2}}[Y_{2}]=\\pi_{1}(X_{1}=0|Z_{1})\\cdot A+\\pi_{1}(X_{1}=1|Z_{1})\\cdot B}\\\\ &{\\quad\\quad\\quad\\quad\\;A=f(S_{2}=0)*P(S_{2}=0,X_{1}=0|Z_{1})P(Z_{1})}\\\\ &{\\quad\\quad\\quad\\quad\\;+f(S_{2}=1)*(1-P(S_{2}=0,X_{1}=0|Z_{1}))P(Z_{1})}\\\\ &{\\quad\\quad\\quad\\;\\;B=f(S_{2}=0)*P(S_{2}=0,X_{1}=1|Z_{1})P(Z_{1})}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\;+f(S_{2}=1)*(1-P(S_{2}=0,X_{1}=1|Z_{1}))P(Z_{1})}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where is $\\mathbb{E}_{\\pi_{1},\\pi_{2}}[Y_{2}]$ a convex combination of the quantities $A$ and $B$ . Therefore, $\\mathbb{E}_{\\pi_{1},\\pi_{2}}[Y_{2}]\\le$ $\\operatorname*{max}\\{A,B\\}$ . Given that $f(S_{2}=0)>f(S_{2}=1)$ , we are able to establish that $A\\,<\\,\\underline{{\\mathbb{E}}}[Y_{2}]$ and $B<\\mathbb{E}[Y_{2}]$ . Therefore, $\\mathbb{E}_{\\pi_{1},\\pi_{2}}[Y_{2}]<\\mathbb{E}[Y_{2}]$ . Using a similar rationale introduced in Sec. 1, we get $\\mathbb{E}_{\\pi_{1}}[Y_{1}]<\\mathbb{E}[Y_{1}]$ . Consequently, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\pi_{1},\\pi_{2}}[Y_{1}+Y_{2}]<\\mathbb{E}[Y_{1}+Y_{2}].\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "In other words, the imitator is unable to learn a policy that can obtain the expert\u2019s performance in the worst-case 2-stage MDP compatible with the observational distribution $P(\\bar{X_{1}},\\bar{X_{2}},S_{1},S_{2},Y_{1},Y_{2})$ . ", "page_idx": 22}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The major claims made in the abstract and introduction has accurately reflect the paper\u2019s contributions and scope. Specifically, we summarized our contributions in Sec. 1, e.g., the theoretical findings and the propose innovative algorithms. Additionally, Table 1 provides a brief summary of this paper\u2019s main contributions. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We have discussed the requirements and limitations of the work in Sec. 1 and Sec. 2. To address infinite horizon decision-making challenges, we utilize the Markov Property, as outlined in Def. 1. However, our study generalizes standard imitation methods by focusing on scenarios where causal consistency does not universally hold true. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \u201dLimitations\u201d section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 23}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: The assumptions and problem settings can be found in Sec. 2. Due to space constraints, all detailed proofs are provided in Appendices A and B. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 23}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: All the important details to reproduce the major experimental results in this paper can be found Sec. 4 and Appendix D. Proposed algorithms are provided in Alg. 1 and Alg. 2. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 24}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 24}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: If the paper is accepted, we intend to make the source code available in the camera-ready version of the paper. During the meantime, all the important details to reproduce the major experimental results in this paper can be found in Sec. 4 and Appendix D. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. ", "page_idx": 24}, {"type": "text", "text": "\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). \u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 25}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: All the important training and test details in this paper can be found Sec. 4 and Appendix D. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 25}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Error bars and other appropriate information about the statistical significance the experiments could be found in Sec. 4 and Appendix D. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \u201dYes\u201d if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 25}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Detailed information on the computer resources can be found in Sec. 4 and Appendix D. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 26}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: This paper introduces novel causal imitation learning algorithms that adapt to confounded expert demonstrations within MDPs by using partial identification techniques. The research conducted in this conform with paper NeurIPS Code of Ethics. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 26}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: Societal impacts are discussed in Sec. 1 and Appendix E. Our framework can be applied to various fields in reality, including autonomous driving, robotics, industrial automation, medical decisions modeling and so on. One of positive impacts of this work is that we discuss the potential risk of training IRL algorithms when demonstrations are contaminated by unobserved confounders, and how to utilize partial identification techniques to make the imitator robust. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. ", "page_idx": 26}, {"type": "text", "text": "\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 27}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: This paper poses no such risks. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 27}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 27}, {"type": "text", "text": "Answer:[Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Please check Sec. 4 and Appendix D. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 27}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Please check Sec. 4 and Appendix D. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not release new assets. ", "page_idx": 27}, {"type": "text", "text": "\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 28}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing with human subjects. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 28}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing with human subjects. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 28}]