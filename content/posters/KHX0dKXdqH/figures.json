[{"figure_path": "KHX0dKXdqH/figures/figures_0_1.jpg", "caption": "Figure 1: A multi-armed bandit model.", "description": "This figure illustrates a multi-armed bandit model with a binary action X \u2208 {0, 1}, a binary reward Y, and a latent covariate U \u2208 {0, 1} that affects both the action and reward. The reward function is defined as Y \u2190 X \u2295 U, where \u2295 denotes the XOR operator.  The figure shows two scenarios: (a) represents the joint probability distribution P(X, Y) and (b) the interventional distribution P\u03c0(X, Y), where \u03c0 is a policy. This example demonstrates how unobserved confounding can affect imitation learning.", "section": "1 Introduction"}, {"figure_path": "KHX0dKXdqH/figures/figures_3_1.jpg", "caption": "Figure 2: Causal diagrams where St represents the state, Xt represents the action (shaded blue) and Yt represents the latent reward (shaded red). (a) MDPexp describes the imitator's interaction with the environment; (b) MDPobs shows the data-generating process for the expert demonstrations.", "description": "This figure shows two causal diagrams representing Markov Decision Processes (MDPs).  The left diagram (a) shows the MDP from the imitator's perspective, illustrating how the imitator interacts with the environment using actions (Xt) based on observed states (St) to receive rewards (Yt). The right diagram (b) represents the observational data available to the imitator, demonstrating how the expert's actions and rewards are influenced by unobserved confounders (Ut) that are not visible to the imitator. This highlights the challenge of imitation learning when unobserved confounding is present.", "section": "2 Challenges of Unobserved Confounding"}, {"figure_path": "KHX0dKXdqH/figures/figures_5_1.jpg", "caption": "Figure 2: Causal diagrams where St represents the state, Xt represents the action (shaded blue) and Yt represents the latent reward (shaded red). (a) MDPexp describes the imitator's interaction with the environment; (b) MDPobs shows the data-generating process for the expert demonstrations.", "description": "This figure displays two causal diagrams illustrating Markov Decision Processes (MDPs). Diagram (a) shows the MDP from the imitator's perspective, while diagram (b) illustrates the MDP generating the expert's demonstration data. The key difference lies in the presence of an unobserved confounder U in diagram (b), affecting both the expert's actions (Xt) and rewards (Yt). This unobserved confounder is crucial in highlighting the challenges posed by unobserved confounding bias in imitation learning.  The diagrams use shaded blue for actions (Xt) and shaded red for latent rewards (Yt).", "section": "Challenges of Unobserved Confounding"}, {"figure_path": "KHX0dKXdqH/figures/figures_8_1.jpg", "caption": "Figure 4: Simulation results for our experiments. Fig. 4a illustrates the performance gap histogram for the experiment MDPobs, where negative values indicate performance worse than expert performance. Fig. 4b shows the convergence plot for CAIL, GAIL, and BC performance. Fig. 4c shows the final performance, where y-axis represents the expected return.", "description": "This figure presents the results of experiments comparing three imitation learning algorithms (CAIL, GAIL, and BC) against an expert in the MDPobs scenario.  Panel (a) shows a histogram of the performance gap (imitator's performance minus expert's performance) for many runs of the MDPobs experiment, revealing a significant portion of runs where the imitator underperformed the expert. Panel (b) presents learning curves that illustrate the average return of each algorithm across epochs, demonstrating the convergence of the algorithms. Panel (c) displays the final average return for each method, indicating the relative performance of CAIL, GAIL, and BC compared to the expert. ", "section": "4 Experiments"}, {"figure_path": "KHX0dKXdqH/figures/figures_20_1.jpg", "caption": "Figure 4: Simulation results for our experiments. Fig. 4a illustrates the performance gap histogram for the experiment MDPobs, where negative values indicate performance worse than expert performance. Fig. 4b shows the convergence plot for CAIL, GAIL, and BC performance. Fig. 4c shows the final performance, where y-axis represents the expected return.", "description": "This figure presents simulation results comparing the performance of three imitation learning algorithms (CAIL, GAIL, and BC) against an expert.  Subfigure (a) shows a histogram of the performance gap between each algorithm and the expert for a specific experiment (MDPobs). Subfigure (b) displays the learning curves of the three algorithms showing their performance over epochs. Subfigure (c) shows a bar chart of the final average return for each algorithm.", "section": "4 Experiments"}]