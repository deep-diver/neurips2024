[{"figure_path": "uyLtEFnpQP/figures/figures_0_1.jpg", "caption": "Figure 1: Overall illustration of sEEG decoding setup and comparison with SOTA baselines.", "description": "This figure provides a visual overview of the study's experimental setup.  The left side depicts the process: a subject reads words displayed on a screen, while their intracranial sEEG signals are recorded. This data is then input into the Du-IN model, which performs a 61-word classification task. The right side shows a radar chart comparing the performance of Du-IN against other state-of-the-art (SOTA) baseline methods across 12 subjects.  Each point on the radar chart represents a subject, and the distance from the center indicates the model's accuracy for that subject.  This visualization highlights Du-IN's superior performance compared to existing methods.", "section": "Introduction"}, {"figure_path": "uyLtEFnpQP/figures/figures_3_1.jpg", "caption": "Figure 2: The overall architecture of Du-IN Encoder. Du-IN Encoder is used as an encoder in all Du-IN models (i.e., Du-IN VQ-VAE, Du-IN MAE, Du-IN CLS (classification)), see Appendix C for more details.", "description": "The figure shows the architecture of the Du-IN Encoder, a core component of the Du-IN model for decoding speech from intracranial neural signals.  The encoder takes raw sEEG signals as input and processes them through a spatial encoder (fusing channels within brain regions), temporal embedding (adding positional information), and a transformer encoder (capturing temporal relationships). The output is a sequence of contextual embeddings used for downstream tasks.  The figure highlights the stages of patch segmentation, spatial encoding (using linear projection and convolution layers), temporal embedding, and the multi-layered transformer encoder.", "section": "3.2 Model Architecture"}, {"figure_path": "uyLtEFnpQP/figures/figures_4_1.jpg", "caption": "Figure 3: Overview of Du-IN VQ-VAE training and Du-IN MAE training. (a). We train the Du-IN Encoder in the Du-IN VQ-VAE to discretize sEEG signals into discrete neural tokens by reconstructing the original sEEG signals. (b). During the training of Du-IN MAE, part of SEEG patches are masked while the objective is to predict masked tokens from visible patches.", "description": "This figure shows the architecture of the Du-IN VQ-VAE and Du-IN MAE models used for pre-training the Du-IN model. The Du-IN VQ-VAE model is used to discretize sEEG signals into discrete neural tokens by reconstructing the original sEEG signals. The Du-IN MAE model is used to predict masked tokens from visible patches, which helps the model learn contextual representations. Both models utilize the Du-IN encoder, which extracts contextual embeddings based on region-level tokens through discrete codex-guided mask modeling.", "section": "3.3 Du-IN VQ-VAE Training"}, {"figure_path": "uyLtEFnpQP/figures/figures_7_1.jpg", "caption": "Figure 4: The channel contribution analysis. (a). The channel contribution map. (b). The effect of the number of channels (sorted according to channel contribution scores) on decoding performance.", "description": "This figure shows the results of an analysis performed to determine the contribution of each channel to the overall decoding performance.  Panel (a) is a brain map visualizing the contribution scores of different channels, with hotter colors indicating higher contribution. Panel (b) shows a graph plotting the decoding accuracy against the number of channels used, demonstrating the impact of channel selection on model performance.  This analysis highlights that only a subset of channels, primarily located in specific brain regions relevant to speech production, are crucial for high decoding accuracy.  The findings underscore the localized nature of brain activity related to speech.", "section": "4.3 Channel Contribution and Selection"}, {"figure_path": "uyLtEFnpQP/figures/figures_9_1.jpg", "caption": "Figure 1: Overall illustration of sEEG decoding setup and comparison with SOTA baselines.", "description": "This figure provides a visual overview of the experimental setup used for decoding speech from intracranial neural signals recorded via stereo-electroencephalography (sEEG).  The left side shows the overall process, from sEEG recordings to the 61-word classification task.  The right side presents a comparison of the Du-IN model's performance against several state-of-the-art (SOTA) baseline methods on the same classification task.  The radar chart visually represents the relative performance of each model across different subjects.", "section": "Abstract"}, {"figure_path": "uyLtEFnpQP/figures/figures_15_1.jpg", "caption": "Figure 1: Overall illustration of sEEG decoding setup and comparison with SOTA baselines. Du-IN refers to the phonetic transcription of \"\u8b80\u97f3\" (i.e., pronunciation) in Chinese.", "description": "This figure shows a schematic of the sEEG decoding setup used in the study.  It compares the performance of the proposed Du-IN model against several state-of-the-art (SOTA) baselines on a 61-word classification task using intracranial neural signals.  The visual representation highlights the superior performance of Du-IN compared to other methods. The figure includes a visualization of the sEEG data, the Du-IN model, and the performance of various models, showing probability scores for different words.", "section": "Introduction"}, {"figure_path": "uyLtEFnpQP/figures/figures_24_1.jpg", "caption": "Figure 7: The visualization of Vector-Quantized sEEG Regression. (a). The reconstruction loss curve during the training process of the Du-IN VQ-VAE model. (b). The visualization of reconstructed sEEG signals.", "description": "This figure visualizes the vector-quantized sEEG regression process. Panel (a) shows the reconstruction loss curve during the training of the Du-IN VQ-VAE model.  It demonstrates a decrease in loss over epochs, indicating successful learning. Panel (b) compares original and reconstructed sEEG signals, showing that the model can effectively reconstruct the signals while capturing major trends, even though fine details might be missing.", "section": "3.3 Du-IN VQ-VAE Training"}, {"figure_path": "uyLtEFnpQP/figures/figures_24_2.jpg", "caption": "Figure 1: Overall illustration of sEEG decoding setup and comparison with SOTA baselines. Du-IN refers to the phonetic transcription of \"\u8b80\u97f3\" (i.e., pronunciation) in Chinese.", "description": "This figure provides a visual summary of the study's experimental setup and results. The left panel shows a diagram of the intracranial stereo-electroencephalography (sEEG) recordings setup, used for decoding speech from brain signals. The right panel presents a comparison of the model's performance against state-of-the-art (SOTA) baselines. The figure highlights the superior performance of the proposed Du-IN model in a 61-word classification task.", "section": "Introduction"}, {"figure_path": "uyLtEFnpQP/figures/figures_29_1.jpg", "caption": "Figure 1: Overall illustration of sEEG decoding setup and comparison with SOTA baselines.", "description": "This figure provides a visual overview of the experimental setup for decoding speech from intracranial neural signals using sEEG.  It shows the sEEG recordings being processed by the Du-IN model, and a comparison of its performance against other state-of-the-art (SOTA) baselines on a 61-word classification task. The comparison highlights the Du-IN model's superior performance.  The visual representation includes a schematic of the sEEG recording setup and a polar plot comparing the accuracy of different models for each subject. ", "section": "1 Introduction"}, {"figure_path": "uyLtEFnpQP/figures/figures_30_1.jpg", "caption": "Figure 1: Overall illustration of sEEG decoding setup and comparison with SOTA baselines. Du-IN refers to the phonetic transcription of \"\u8b80\u97f3\" (i.e., pronunciation) in Chinese.", "description": "This figure provides a visual overview of the study's experimental setup for decoding speech from intracranial neural signals using stereo-electroencephalography (sEEG).  The left side shows a schematic of the sEEG recording setup. The right side displays a comparison of the Du-IN model's performance against several state-of-the-art (SOTA) baseline models on a 61-word classification task. The polar plot visually represents the classification accuracy of different models for each subject, highlighting the superior performance of the Du-IN model.", "section": "Introduction"}, {"figure_path": "uyLtEFnpQP/figures/figures_30_2.jpg", "caption": "Figure 1: Overall illustration of sEEG decoding setup and comparison with SOTA baselines.", "description": "This figure shows a schematic of the sEEG decoding setup used in the study. It also presents a comparison of the proposed Du-IN model's performance against other state-of-the-art (SOTA) baselines on a 61-word classification task using sEEG recordings.  The left side illustrates the process: sEEG recordings are used as input, processed by the Du-IN model, and result in a word classification. The right side is a radar chart comparing the accuracy of the Du-IN model against several baselines across different subjects.  The chart visually demonstrates that Du-IN outperforms the other models.", "section": "Introduction"}, {"figure_path": "uyLtEFnpQP/figures/figures_30_3.jpg", "caption": "Figure 1: Overall illustration of sEEG decoding setup and comparison with SOTA baselines.", "description": "This figure provides a visual overview of the study's experimental setup. The left panel shows the overall sEEG decoding setup, illustrating how sEEG recordings are collected, processed, and used to perform word classification.  The right panel presents a comparison of the proposed Du-IN model's performance against several state-of-the-art (SOTA) baselines on a 61-word classification task.  The radar chart visually represents the performance of each model for each subject, highlighting the superior accuracy of Du-IN across various subjects.", "section": "1 Introduction"}, {"figure_path": "uyLtEFnpQP/figures/figures_30_4.jpg", "caption": "Figure 1: Overall illustration of sEEG decoding setup and comparison with SOTA baselines.", "description": "This figure shows a schematic of the sEEG decoding setup used in the study.  It depicts sEEG recordings being fed into the Du-IN model for speech decoding. The figure also provides a comparison of the Du-IN model's performance against other state-of-the-art (SOTA) baselines on a 61-word classification task, visually representing the superior performance of the Du-IN model.", "section": "Introduction"}, {"figure_path": "uyLtEFnpQP/figures/figures_31_1.jpg", "caption": "Figure 1: Overall illustration of sEEG decoding setup and comparison with SOTA baselines.", "description": "The figure shows a schematic diagram of the sEEG decoding setup.  The left side depicts the sEEG recordings from the subject during a 61-word classification task, which are then inputted into the Du-IN model for decoding. The right side presents a comparison of the Du-IN model's performance against state-of-the-art (SOTA) baselines using a polar plot. The plot visualizes the accuracy of different models for each subject, demonstrating the superior performance of the Du-IN model.", "section": "Introduction"}, {"figure_path": "uyLtEFnpQP/figures/figures_31_2.jpg", "caption": "Figure 9: Electrode locations from subjects (01-04).", "description": "This figure shows the locations of implanted sEEG electrodes for four subjects (subj-01 to subj-04). Red channels represent the top 10 channels selected based on their contribution to speech decoding performance.  The figure uses side views of the brain to show electrode placement, as many subjects had electrodes implanted primarily on one side of the brain to target epilepsy sources.", "section": "O Subject-Wise Electrode Locations"}, {"figure_path": "uyLtEFnpQP/figures/figures_31_3.jpg", "caption": "Figure 1: Overall illustration of sEEG decoding setup and comparison with SOTA baselines.", "description": "This figure provides a visual overview of the study's setup for decoding speech from intracranial neural signals using stereo-electroencephalography (sEEG). It shows the overall workflow, including sEEG recordings, the Du-IN model, and a comparison of its performance against state-of-the-art (SOTA) baselines on a 61-word classification task. The figure helps to illustrate the model's architecture and its superior performance in decoding speech from sEEG data.", "section": "Introduction"}, {"figure_path": "uyLtEFnpQP/figures/figures_31_4.jpg", "caption": "Figure 1: Overall illustration of sEEG decoding setup and comparison with SOTA baselines.", "description": "This figure shows a comparison of the proposed Du-IN model's performance against other state-of-the-art (SOTA) baselines for decoding speech from intracranial neural signals.  It provides a visual representation of the sEEG decoding setup, illustrating the process of recording signals and classifying words.  The comparison with SOTA baselines highlights the superior performance of the Du-IN model in a 61-word classification task.", "section": "Introduction"}]