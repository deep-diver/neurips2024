[{"figure_path": "XVfevb9XFx/tables/tables_8_1.jpg", "caption": "Table 1: Average normalized D4RL scores of our methods shown in AntMaze navigation tasks after 200k interactions with the environment. (U=umaze, D=diverse)", "description": "This table presents the average normalized D4RL scores achieved by various offline-to-online (O2O) reinforcement learning methods on AntMaze navigation tasks.  The scores are obtained after 200,000 interactions with the environment.  The table compares the proposed O2O methods (O2TD3, O2SAC, O2PPO) against existing methods (IQL, PEX, Cal-QL).  The results are divided into two AntMaze variations (U-v2 and U-D-v2) and an overall total score.  The notation U-v2 represents the umaze task, and U-D-v2 the umaze diverse task.  The scores are presented as ranges, reflecting the performance variation across different runs.", "section": "5 Experiments"}, {"figure_path": "XVfevb9XFx/tables/tables_13_1.jpg", "caption": "Table 1: Average normalized D4RL scores of our methods shown in AntMaze navigation tasks after 200k interactions with the environment. (U=umaze, D=diverse)", "description": "This table presents the average normalized D4RL scores achieved by various offline-to-online reinforcement learning (RL) methods on AntMaze navigation tasks.  The scores are obtained after 200,000 interactions with the environment. The table compares the performance of the proposed methods (O2TD3, O2SAC, O2PPO) against several baseline methods (IQL, PEX, Cal-QL, ACA).  The results are broken down by AntMaze environment variations (U-v2, U-D-v2) and provide a total score across all variations. The abbreviation \"U\" stands for umaze and \"D\" stands for diverse.", "section": "5 Experiments"}, {"figure_path": "XVfevb9XFx/tables/tables_13_2.jpg", "caption": "Table 1: Average normalized D4RL scores of our methods shown in AntMaze navigation tasks after 200k interactions with the environment. (U=umaze, D=diverse)", "description": "This table presents the average normalized D4RL scores achieved by different Offline-to-Online RL methods on AntMaze navigation tasks.  The scores are obtained after 200,000 interactions with the environment.  The table compares the proposed O2TD3, O2SAC, and O2PPO methods against several state-of-the-art baselines (IQL, PEX, Cal-QL, AWAC, and Off2On).  The results are broken down by AntMaze environment variations (U-v2, U-D-v2), indicating the performance across different levels of task complexity. The 'total' row sums the performance across all environments, providing an overall performance comparison of the methods.", "section": "5 Experiments"}, {"figure_path": "XVfevb9XFx/tables/tables_16_1.jpg", "caption": "Table 3: The performance of the offline policy \u03c0off and the policy \u03c0on obtained after policy re-evaluation and value alignment", "description": "This table presents the performance comparison between the offline policy (\u03c0off) and the online policy (\u03c0on) after applying policy re-evaluation and value alignment. The results show the performance scores for different MuJoCo locomotion tasks, demonstrating the effectiveness of the proposed methods in improving policy performance from offline to online stages.", "section": "C.1 Difference in policy performance caused by optimistic critic reconstruction"}, {"figure_path": "XVfevb9XFx/tables/tables_17_1.jpg", "caption": "Table 4: Average normalized D4RL scores of our methods and some baselines on AntMaze navigation tasks.", "description": "This table presents the average normalized D4RL scores achieved by various offline-to-online reinforcement learning (RL) methods on AntMaze navigation tasks.  The methods compared include AWAC, IQL, PEX, O2SAC, and O2PPO.  The table shows the performance ranges (min-max) obtained across different AntMaze environments (medium-play-v2, medium-diverse-v2, large-play-v2, large-diverse-v2) and provides the total scores across all environments. Finally, it shows the improvement (\"\u0394\") in total score for each method compared to the baseline, indicating the effectiveness of the proposed methods in enhancing online RL performance.", "section": "5.1 Comparison with State-of-the-Art Methods"}, {"figure_path": "XVfevb9XFx/tables/tables_20_1.jpg", "caption": "Table 1: Average normalized D4RL scores of our methods shown in AntMaze navigation tasks after 200k interactions with the environment. (U=umaze, D=diverse)", "description": "This table presents the average normalized D4RL scores achieved by various offline-to-online reinforcement learning (RL) methods on AntMaze navigation tasks.  The scores are calculated after 200,000 interactions with the environment.  The table compares the proposed O2O methods (O2TD3, O2SAC, O2PPO) against existing baselines (IQL, PEX, Cal-QL, AWAC, ACA). The results are categorized by the type of AntMaze environment (umaze or diverse) to assess performance across different task complexities.", "section": "5 Experiments"}, {"figure_path": "XVfevb9XFx/tables/tables_29_1.jpg", "caption": "Table 1: Average normalized D4RL scores of our methods shown in AntMaze navigation tasks after 200k interactions with the environment. (U=umaze, D=diverse)", "description": "This table presents the average normalized D4RL scores achieved by various offline-to-online reinforcement learning (RL) methods on AntMaze navigation tasks.  The scores are obtained after 200,000 interactions with the environment. The table compares the proposed methods (O2TD3, O2SAC, O2PPO) with several existing offline RL methods (IQL, PEX, Cal-QL, ACA).  The results are broken down by dataset variant (U-v2, U-D-v2), and a total score is also provided.  The abbreviations U and D refer to \"umaze\" and \"diverse\", respectively, indicating different variations of the AntMaze environment.", "section": "Experiments"}]