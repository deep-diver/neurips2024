{"references": [{"fullname_first_author": "Joshua Achiam", "paper_title": "Constrained policy optimization", "publication_date": "2017-00-00", "reason": "This paper introduces constrained policy optimization, a crucial technique used in the proposed method to ensure stable and efficient online fine-tuning."}, {"fullname_first_author": "Aviral Kumar", "paper_title": "Conservative Q-learning for offline reinforcement learning", "publication_date": "2020-00-00", "reason": "This paper introduces conservative Q-learning, a foundational offline RL algorithm that addresses the issue of pessimistic Q-value estimations, which is directly addressed in this paper."}, {"fullname_first_author": "Tuomas Haarnoja", "paper_title": "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor", "publication_date": "2018-00-00", "reason": "Soft Actor-Critic (SAC) is one of the core online RL algorithms used for comparison in this paper, highlighting its importance in the field."}, {"fullname_first_author": "Scott Fujimoto", "paper_title": "Addressing function approximation error in actor-critic methods", "publication_date": "2018-00-00", "reason": "This paper addresses function approximation error in actor-critic methods, a key challenge in offline-to-online RL that is directly relevant to the proposed method."}, {"fullname_first_author": "Ashvin Nair", "paper_title": "Awac: Accelerating online reinforcement learning with offline datasets", "publication_date": "2020-00-00", "reason": "This paper introduces AWAC, another important online RL algorithm used for comparison, demonstrating its significance in the context of offline-to-online RL."}]}