[{"type": "text", "text": "Model Collapse Demystified: The Case of Regression ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Elvis Dohmatob\u00a7 \u2217 Yunzhen Feng\u2020 Julia Kempe\u00a7 \u2020 \u2021 \u00a7FAIR, Meta ", "page_idx": 0}, {"type": "text", "text": "\u2020Center for Data Science, New York University \u2021Courant Institue of Mathematical Sciences, New York University \u2217Correspondence to dohmatob@meta.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The era of proliferation of large language and image generation models begs the question of what happens if models are trained on the synthesized outputs of other models. The phenomenon of \"model collapse\" refers to the situation whereby as a model is trained recursively on data generated from previous generations of itself over time, its performance degrades until the model eventually becomes completely useless, i.e. the model collapses. In this work, we investigate this phenomenon within the context of high-dimensional regression with Gaussian data, considering both low- and high-dimensional asymptotics. We derive analytical formulas that quantitatively describe this phenomenon in both under-parameterized and over-parameterized regimes. We show how test error increases linearly in the number of model iterations in terms of all problem hyperparameters (covariance spectrum, regularization, label noise level, dataset size) and further isolate how model collapse affects both bias and variance terms in our setup. We show that even in the noise-free case, catastrophic (exponentially fast) model-collapse can happen in the over-parametrized regime. In the special case of polynomial decaying spectral and source conditions, we obtain modified scaling laws which exhibit new crossover phenomena from fast to slow rates. We also propose a simple strategy based on adaptive regularization to mitigate model collapse. Our theoretical results are validated with experiments. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Model collapse describes the situation where the performance of large language models (LLMs) or large image generators degrade as more and more AI-generated data becomes present in their training dataset [44]. Indeed, in the early stages of the generative AI evolution (e.g the ChatGPT-xyz series of models), there is emerging evidence suggesting that retraining a generative AI model on its own outputs can lead to various anomalies in the model\u2019s later outputs. This phenomenon has been particularly observed in LLMs, where retraining on their generated content introduces irreparable defects, resulting in what is known as \u201cmodel collapse\", the production of nonsensical or gibberish output [44, 8]. Though several recent works demonstrate facets of this phenomenon empirically in various settings [23, 32, 33, 8, 9, 21], a theoretical understanding is still missing. ", "page_idx": 0}, {"type": "text", "text": "In this work, we initiate a theoretical study of model collapse in the setting of high-dimensional supervised-learning with linear regression. This is equivalent to kernel regression1, which serves as an effective proxy for neural networks in various regimes, for instance in the infinite-width limit [37, 49, 25, 28] or in the lazy regime of training [12]. [11] characterize the power-law generalization error of regularized least-squares kernel algorithms, assuming a power-decay spectrum of the kernel (capacity) and of the coefficients of the target function (source). Source and capacity power decay (a) Isotropic covariance spectrum $\\Sigma=I_{d}$ . Here, we (b) Power-law covariance spectrum. Refer to Eqn show the evolution of test error for different sample (23). The setup is: $d=300$ , $T_{0}=600$ , $\\sigma=\\sigma_{0}=1$ , size $(T)$ , different levels of ridge-regularization $(\\lambda)$ , and $\\Sigma\\,=\\,\\operatorname{diag}(\\bar{\\lambda_{1}},\\ldots,\\lambda_{d})$ , where $\\lambda_{k}~\\propto~k^{-2}$ . Left training data from different generations $(n)$ of fake data. plot corresponds to $T\\,=\\,10,000$ and Right plot The setup is: input-dimension $d\\,=\\,300$ , sample size corresponds to adaptive regularization $\\lambda=\\bar{T}^{-\\ell_{c r i t}}$ , for fake data generator $T_{0}~=~600$ , noise levels $\\sigma=$ where $\\lambda=\\lambda(T)$ as proposed in [14]. See Section 0.1 and $\\sigma_{0}~=~0.2$ . Left plot is for $T\\,=\\,1000$ and D for details. The broken curves are as predicted by different values of $\\lambda$ . Notice the U-shape of the curves our Theorem 5.1. Though $\\ell=\\ell_{c r i t}$ is optimal in for large values of $_n$ , indicating the existence of a sweet classical case, it is not in the setup of model collapse spot (optimal regularization parameter). Right plot is In fact here, the test error diverges with sample size for $\\lambda=10^{-3}$ and different values of $T$ . The broken $T$ . Our theory proposes a corrected value of this lines correspond to the theoretical result established in exponent which gracefully adapts to synthesized data Theorem 4.1. See Figure 4 (Appendix) for results on MNIST [16]. ", "page_idx": 0}, {"type": "image", "img_path": "bioHNTRnQk/tmp/22b4c6ee2568af4ea7434edc4c8dbb47e1fa579200a9446c1261a55327978093.jpg", "img_caption": [], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Figure 1: Demystifying model collapse. Refer to Appendix D for details on the experimental setup. ", "page_idx": 1}, {"type": "text", "text": "capture properties of the data and the model that give rise to power law scaling of test error in terms of data set size and model capacity, as empirically observed e.g. in [26, 24]. More recently, scaling laws have been shown for kernel models under the Gaussian design, e.g. in [46, 13, 14] for regression and [15] for classification. [39, 43, 31] study scaling laws for regression in the random feature model. ", "page_idx": 1}, {"type": "text", "text": "Summary of Main Contributions. Following the rich tradition in prior works outlined above, we study the Gaussian design where the input $x$ is sampled from a multivariate zero-mean Gaussian $\\mathcal{N}(0,\\Sigma)$ and labels $y$ are determined by a linear ground truth function with independent label noise $\\epsilon$ as $y=x^{\\top}w_{0}+\\epsilon$ (we present the linear regression setting for ease, the generalization to the kernel setting is straightforward). At each generation step, an approximation to $w_{0}$ is learned from the data, and used to generate new, fake /synthetic labels for the next generation. Note that the machine learner has no control over the fake data generation process. It only sees data from a stage $n$ of this process, which is then used to fit a downstream predictor. Our main findings can be summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "$(I)$ Exact Characterization of Test Error under Iterative Retraining on Synthesized Data. In Section 4 (Theorem 4.3), we obtain analytic formulae for test error under the influence of training data with fake $/$ synthesized labels. For $n$ -fold iteration of data-generation, this formula writes ", "page_idx": 1}, {"type": "equation", "text": "$$\nE_{t e s t}=E_{t e s t}^{c l e a n}+\\Delta B i a s+n\\cdot\\sigma_{0}\\rho(\\lambda,T,T_{0},\\sigma,\\Sigma),\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where Etcelestan is the usual test error of the model trained on clean data (not AI-generated) and $\\sigma_{0}^{2}$ the label noise level in the clean data distribution. The non-negative term $\\rho$ precisely highlights the effects of all the relevant problems parameters: the feature covariance matrix $\\Sigma$ , sample size $T$ , original data size $T_{0}$ , label noise level in the fake data distribution $\\sigma^{2}$ , and regularization $\\lambda$ The non-negative term $\\Delta B i a s$ is an increase in bias brought about by the iterative synthetic data generation process. This term disappears in the under-parametrized regime (Corollary 4.4), if each stage in the process was fitted on sufficiently many samples $T_{0}$ compared to the input dimension $d$ (i.e if $T_{0}\\geq d,$ ). In the over-parametrized case where $T_{0}<d$ , this term is either a constant (Theorem 4.5) or an increasing function of $n$ , depending on whether the design matrix stays the same or is resampled across different generations (Theorem 4.6). Notably, even in the case of noiseless labels (when $\\sigma_{0}=0$ ), the downstream model converges to a Gaussian process around zero exponentially fast with the number of iterations $n$ , leading to \u201ccatastrophic\" model collapse. ", "page_idx": 1}, {"type": "text", "text": "A direct consequence of (1) is that, as the number of generations $n$ becomes large, the effect of re-synthesizing will make learning impossible. We note that the multiplicative degradation in scaling with the number of generations $n$ is completely analogous to what has been shown in [18] for infinite memory models and their variants and empirically observed there. Illustration in Figures 1a and 2. ", "page_idx": 1}, {"type": "text", "text": "(2) Modified Scaling Laws. Turning to the special case of power-law spectra of the covariance matrix $\\Sigma$ , which allows to derive test-error scaling laws [11, 46, 14, 29], we obtain in Section 5 (see Theorem 5.1) precise new scaling laws of the test error that quantitatively highlight the negative effect of training ", "page_idx": 1}, {"type": "image", "img_path": "bioHNTRnQk/tmp/af643797bc61fef5e45d68ef823e45c7ad7477c900c8e773bd96e0d304946e5c.jpg", "img_caption": ["(a) Identical intermediate design matrices $X_{n}=X_{0}\\,\\forall n$ "], "img_footnote": [], "page_idx": 2}, {"type": "image", "img_path": "bioHNTRnQk/tmp/35c9273b3f6e003da632ba61c083a2086d47ac1317f193a236b59377fe05c147.jpg", "img_caption": ["(b) Independent intermediate design matrices. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Figure 2: Model collapse in the case of noiseless over-parametrized synthetic data generator. Here $d=300$ , the sample sizes for the different versions of the fake data generator are equal, i.e $T_{n}=T_{0}=d/2$ for all $n$ , and noise levels are $\\sigma_{0}=0$ and $\\sigma=0.1$ . Everything else is as in the setting of Figure 1a. Broken lines correspond to the theoretical estimates given in Theorem 4.3. As predicted by our theory, the test error of the model ftited on synthetic data $(n\\geq1)$ ) increases (relative to the baseline $n=0$ , corresponding to training on clean data). The model collapse here, even in the absence of noise $(\\sigma_{0}=0)$ ), is due to the fact that the synthetic data-generator does not have access to enough data to capture the true labelling function. (a) Importantly, and in accordance to our theory, the amount of model collapse in the case $X_{n}\\equiv X_{0}$ is due to an increase in bias term of the test error of the model and does not depend on the number of generations $_n$ as long as $n\\geq1$ . (b) In contrast, for the case where the $X_{n}$ \u2019s are independent, the increase in bias term grows with $n$ , leading to \u201ccatastrophic\" model collapse (Theorem 4.6). Refer to Appendix $\\mathrm{D}$ for the experimental setup. ", "page_idx": 2}, {"type": "text", "text": "on synthetically generated data. Further exploiting our analytic estimates, we obtain (Corollary 5.2) the optimal ridge regularization parameter $\\lambda$ as a function of all the problem parameters (sample size, spectral exponents, strength of fake data-generator, etc.). This new regularization parameter corresponds to a correction of the the value proposed in the classical theory on clean data [14], and highlights a novel crossover phenomenon where for an appropriate tuning of the regularization parameter, the effect of training on fake data is a degradation of the fast error rate in the noiseless regime [14, 11] to a much slower error rate which depends on the amount of true data on which the fake data-generator was trained in the first place. On the other hand, a choice of regularization which is optimal for the classical setting (training on real data), might lead to catastrophic failure: the test error diverges. See Figure 1b for an illustration. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Current LLMs [17, 30, 10, 47], including GPT-4 [1], were trained on predominantly human-generated text; similarly, diffusion models like DALL-E [40], Stable Diffusion [42], Midjourney [35] are trained on web-scale image datasets. Their training corpora already potentially exhaust all the available clean data on the internet. A growing number of synthetic data generated with these increasingly popular models starts to populate the web, often indistinguishable from \u201creal\" data. Recent works call attention to the potential dramatic deterioration in the resulting models, an effect referred to as \u201cmodel collapse\" [44]. ", "page_idx": 2}, {"type": "text", "text": "Empirical evidence of model collapse has been reported across various domains [23, 32, 33, 8, 9, 21]. Some theoretical studies [44, 7, 2, 18] have begun exploring this phenomenon. [44] attribute collapse to finite sampling bias and function approximation errors in the (single) Gaussian case but only provide lower bounds without detailed analytic expressions. [7] analyze the training process at the distribution level using both clean and synthetic data and provide stability results. However, these results do not account for finite samples and are only valid locally in parameter space, making them more relevant to fine-tuning rather than training from scratch. [2] examine \u201cself-consuming loops\u201d in the Gaussian case by assuming a sampling bias that reduces data variance with each generation\u2014a (martingale) assumption that we do not require. These studies lack a comprehensive theoretical framework to quantify model collapse and its impact on scaling laws. Our work addresses these gaps by providing an analytic theory that captures how model collapse emerges from training on synthetic data, providing a deeper understanding that goes beyond merely identifying the collapse. A concurrent study by [18] demonstrate that model collapse in foundation models can be attributed to a breakdown in scaling laws [26, 24], where increasing the sample size eventually fails to improve model performance. This finding, theoretically shown for discrete data in variants of the infinite memory model, complements our analytical results on how synthetic data alters the rate of scaling laws, as discussed in Section 5. ", "page_idx": 2}, {"type": "text", "text": "3 Theoretical Setup ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We now present a setup which is simple enough to be analytically tractable, but rich enough to exhibit a wide range of regimes to illustrate a range of new phenomena that emerge with model collapse. ", "page_idx": 3}, {"type": "text", "text": "Data Distribution and Synthetized Data. Consider the distribution $P_{\\Sigma,w_{0},\\sigma^{2}}$ on $\\mathbb{R}^{d}\\times\\mathbb{R}$ given by (Input) $\\boldsymbol{x}\\sim N(0,\\Sigma)$ , $(\\mathbf{Noise})\\mathbf{\\Psi}\\epsilon\\sim N(0,\\sigma^{2})$ , indep. of $x$ (Output/Label) $y=x^{\\top}w_{0}+\\epsilon.$ ", "page_idx": 3}, {"type": "text", "text": "The positive integer $d$ is the input-dimension, the vector $w_{0}\\in\\mathbb{R}^{d}$ defines the ground-truth labelling function $x\\mapsto x^{\\top}w_{0}$ , the matrix $\\Sigma\\,\\in\\,\\mathbb{R}^{d\\times d}$ is the covariance structure of the inputs. The scalar $\\sigma^{2}$ is the level of label noise. Here, we consider the linear case for clarity. We describe the extension to the kernel setting in Appendix C. Thus, in classical linear regression, given a sample $(X,Y)\\equiv\\{(x_{1},y_{1}),\\dots,(x_{T},{y_{T}})\\}$ of size $T$ from $P_{\\Sigma,w_{0},\\sigma^{2}}$ , one seeks a linear model $\\widehat{w}\\in\\mathbb{R}^{d}$ with small test error ", "page_idx": 3}, {"type": "equation", "text": "$$\nE_{t e s t}(\\widehat{w}):=\\mathbb{E}_{x,y}[(x^{\\top}\\widehat{w}-y)^{2}]-\\sigma^{2}=\\|\\widehat{w}-w_{0}\\|_{\\Sigma}^{2},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $(x,y)\\sim P_{\\Sigma,w_{0},\\sigma^{2}}$ is a random clean test point. In our setup for studying model collapse, the training data $(X,Y)$ is sampled from an iterative loop where each generation of the model serves as the labeller for the data for the next generation. This process is described below. ", "page_idx": 3}, {"type": "text", "text": "Structure of the Synthesized / Fake Data Generator. Consider a sequence of data distributions ", "text_level": 1, "page_idx": 3}, {"type": "equation", "text": "$$\nP_{\\Sigma,w_{0},\\sigma_{0}^{2}}\\to P_{\\Sigma,\\widehat{w}_{1},\\sigma_{1}^{2}}\\to.\\dots\\to P_{\\Sigma,\\widehat{w}_{n},\\sigma_{n}^{2}\\to...},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\widehat{w}_{n}$ \u2019s is defined recursively by $\\widehat{w}_{n}=w_{0}$ , and ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{w}_{n}=\\mathrm{Fit}(X_{n-1},\\overline{{Y}}_{n-1}),\\;\\mathrm{for}\\;n\\geq1,}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where ${\\overline{{Y}}}_{n}:=X_{n}{\\widehat{w}}_{n}+E_{n}$ and $\\mathrm{Fit}(A,B)=\\mathrm{OLS}(A,B):=A^{\\dagger}B$ is ordinary-least squares (OLS). The design   matrices $(X_{n})_{n\\geq0}$ are of shapes $T_{n}\\times d$ , each with iid rows from $N(0,\\Sigma)$ . ", "page_idx": 3}, {"type": "text", "text": "The sequence of noise vectors $(E_{n})_{n\\geq0}$ forms an independent collection, which is independent of the $(X_{n})_{n\\geq0}$ ; each $E_{n}\\in\\mathbb{R}^{T_{n}}$ has iid components $\\epsilon_{n,i}$ from $N(0,\\sigma_{n}^{2})$ . Refer to Figure 3. ", "page_idx": 3}, {"type": "image", "img_path": "bioHNTRnQk/tmp/f735f38f495f86b3a492db4502c203b2845e57c709c903fd249300a8ec6713c9.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 3: Illustration of the theoretical framework. The process begins with the original model $\\widehat{w}_{0}(w_{0})$ and the original dataset $(X_{0},\\overline{{Y}}_{0})$ . $n$ synthetic data generators $\\widehat{w}_{1}$ to $\\widehat{w}_{n}$ are iteratively fti on data labelled by the previous model with label noise $\\sigma_{0}$ , using $T_{0}$ samples each. We evaluate the test error (with respect to the ground truth labels from $w_{0}$ ) of $\\widehat{w}_{n}^{p r e d}$ , trained on $(X,Y):=(X_{n},{\\overline{{Y}}}_{n})$ using $T$ samples with label noise $\\sigma$ and a regularization coefficient $\\lambda$ . ", "page_idx": 3}, {"type": "text", "text": "Thus, in summary, eachwn results from ftiting a model on a dataset of size Tn\u22121 from P\u03a3,wn\u22121,\u03c32n\u22121, for every generation index $n\\geq1$ . ", "page_idx": 3}, {"type": "text", "text": "The Downstream Model: Ridge Regression. For a number of iterations $n\\geq0$ , noise levels $\\sigma_{0}$ and $\\sigma$ , dataset sizes $T_{0}$ and $T$ , and regularization parameter $\\lambda\\geq0$ , let $\\widehat{w}_{n}^{p r e d}=\\widehat{w}_{n,T_{0},\\sigma_{0}^{2},T,\\sigma,\\lambda}^{p r e d}\\in\\mathbb{R}^{d}$ the ridge predictor constructed from an iid sample $\\{(x_{1},y_{1}),.~.~.~,(x_{T},y_{T})\\}$ of size $T$ from the $n$ -fold fake data distribution $P_{\\Sigma,\\widehat{w}_{n},\\sigma_{n}^{2}}$ , where for ease of presentation of our results we will assume that ", "page_idx": 3}, {"type": "equation", "text": "$$\nT_{n-1}=\\ldots=T_{1}=T_{0},\\quad T_{n}=T\\quad{\\mathrm{and}}\\quad\\sigma_{n-1}}&{=\\ldots=\\sigma_{1}=\\sigma_{0},\\quad\\sigma_{n}=\\sigma.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "For an $n$ -fold fake data generator $P_{\\Sigma,\\widehat{w}_{n},\\sigma_{n}^{2}}$ , we denote with $X:=X_{n}\\in\\mathbb{R}^{T\\times d}$ the design matrix with iid rows from $N(0,\\Sigma)$ , with $E:=E_{n}\\stackrel{\\cdot\\cdot}{\\in}\\mathbb{R}^{T}$ the stage- $n$ label-noise vector with components in $N(0,\\sigma_{n}^{2})$ , and $Y:={\\overline{{Y}}}_{n}=X{\\widehat{w}}_{n}+E\\in\\mathbb{R}^{T}$ the labels generated by $P_{\\Sigma,\\widehat{w}_{n},\\sigma_{n}^{2}}$ . Let $\\widehat{\\Sigma}:=X^{\\top}X/T\\in$ $\\mathbb{R}^{d\\times d}$ is the sample covariance matrix, and $R=R(\\lambda):=(\\widehat\\Sigma+\\lambda I_{d})^{-1}$ denote its resolvent, so that ", "page_idx": 3}, {"type": "text", "text": "We are interested in the dynamics of the test error $E_{t e s t}(\\widehat{w}_{n}^{p r e d})$ of this linear model. Importantly, the evaluation of the model is performed on the true dat a  distribution $P_{\\Sigma,w_{0},\\sigma_{0}^{2}}$ , even though the model is trained on the fake data distribution $P_{\\Sigma,\\widehat{w}_{n},\\sigma^{2}}$ . Note that for $n=0$ , $E_{t e s t}^{c l e\\bar{a}n}:=E_{t e s t}(\\widehat{w}_{n}^{p r e d})$ corresponds to the usual test error when the dow n stream model is trained on clean data. Importantly, the downstream model has no control over this process. It will only see training data from a given version $P_{\\Sigma,\\widehat{w}_{n},\\sigma_{n}^{2}}$ , but evaluation will be on the true distribution $P_{\\Sigma,w_{0},\\sigma_{0}^{2}}$ . ", "page_idx": 4}, {"type": "text", "text": "The mental picture is as follows: each generation $\\widehat{w}_{n}$ can be seen as a proxy for a specific version of ChatGPT, for example. The sample size $T_{0}$ used to create the fake labelling functions $\\widehat{w}_{n}$ is a proxy for the strength of the fake data-generator thus constructed. Other works which ha v e considered model collapse under such a self-looping training process include [44, 2, 7, 18]. ", "page_idx": 4}, {"type": "text", "text": "4 Exact Test Error Characterization ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section we establish generic analytic formulae for the test error of the downstream model $\\widehat{w}_{n}^{p r e d}$ (7) trained on $n$ -fold fake data-generation as outlined in Section 3. The fully general technical k ey Theorem F.1 detailing formula (1), with a trace expression for $\\rho$ , (as well as proofs) are given in Appendix F; consult part F.1 for an exposition. Notations are standard (summarized in Appendix E). ", "page_idx": 4}, {"type": "text", "text": "4.1 Warm-up: Ordinary Least Squares on Isotropic Data ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "For a start, let us first consider the case of unregularized regression, where $\\lambda=0$ in Equation (7). ", "page_idx": 4}, {"type": "text", "text": "Theorem 4.1. For an $n$ -fold fake data generation process with $T_{0}\\geq d+2$ samples, the test error for the linear predictor ${\\widehat{w}}_{n}^{p r e d}$ in Equation (7) learned on $T\\geq d+2$ samples, with $\\lambda=0$ , is given by ", "page_idx": 4}, {"type": "equation", "text": "$$\nE_{t e s t}\\big(\\widehat{w}_{n}^{p r e d}\\big)\\simeq\\frac{\\sigma^{2}\\phi}{1-\\phi}+\\frac{n\\sigma_{0}^{2}\\phi_{0}}{1-\\phi_{0}},\\;w i t h\\;\\phi=\\frac{d}{T},\\;\\phi_{0}=\\frac{d}{T_{0}},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the notation $f(T)\\simeq g(T)$ means $f(T)/g(T)\\to1$ , for large $T$ . ", "page_idx": 4}, {"type": "text", "text": "The first term $E_{t e s t}(\\widehat{w}_{0}^{p r e d})\\simeq\\sigma^{2}\\phi/(1-\\phi)$ in the above decomposition corresponds to the usual error when the downstream model is ftited on clean data (see [22], for example). The additional term $n\\sigma_{0}^{2}\\phi_{0}/(1-\\phi_{0})$ , proportional to the number of generations $n$ , is responsible for model collapse. ", "page_idx": 4}, {"type": "text", "text": "Model collapse versus more training data. Note that the linear degeneration in test error highlighted by Equation (8) is a direct consequence of using the same dataset size $T_{0}$ across the fake data generator. Of course, if the underlying synthetic generating process has access to a larger data budget across generations, this decay can be significantly alleviated. For instance, if fake data increases gradually with the number of generations $m\\geq2$ as $T_{m}=(m\\log^{2}m)T_{0}$ (and, to simplify, $\\sigma=\\sigma_{0}$ ) a trivial extension of Theorem 4.1 yields ", "page_idx": 4}, {"type": "equation", "text": "$$\nE_{t e s t}(\\hat{w}_{n}^{p r e d})\\simeq(1+\\frac{1}{2\\log^{2}2}+\\frac{1}{3\\log^{2}3}+\\ldots)E_{t e s t}(\\hat{w}_{0}^{p r e d})\\simeq E_{t e s t}(\\hat{w}_{0}^{p r e d}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "which will keep collapse at bay at the expense of largely increased training data ([44] also has a similar formula). This does not avoid model collapse; rather, it trades additional data generation and training effort against deterioration from generations of fake data. Thus, while for clean data increasing the dataset size $\\mathbf{n}$ -fold leads to better scaling, with synthetic data, we forfeit this improvement. Also, note that we do not assume access to samples from any of the intermediate generation steps $\\widehat{w}_{0},\\ldots,\\widehat{w}_{n-1}$ ; we only train the downstream model ${\\widehat{w}}_{n}^{p r e d}$ on data from the last step $\\widehat{w}_{n}$ . ", "page_idx": 4}, {"type": "text", "text": "Model Collapse as Change of Scaling Laws. In the low-dimensional regime (fixed $d$ ), Theorem 4.1 already predicts a change of scaling law from $\\sigma^{2}T^{-1}$ to $\\sigma^{2}T^{-1}+n\\sigma_{0}^{2}T_{0}^{-1}$ . Thus, as the sample size $T$ is scaled up, the test error eventually plateaus at the value $n\\sigma_{0}^{2}T_{0}^{-1}$ and does not vanish. This phenomenon, also established in [18] in the context of large language models, is clearly visible in Figure 1a. In the rest of this section and also in Section 5, we shall establish an analogous picture for high-dimensional regimes $(d\\to\\infty)$ ). ", "page_idx": 4}, {"type": "text", "text": "Mitigation via Regularization. Note that the test error of the null predictor $w_{n u l l}~=~0$ is $E_{t e s t}(w_{n u l l})=\\|w_{0}\\|_{\\Sigma}^{2}$ , and so ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{E_{t e s t}(\\widehat{w}_{n}^{p r e d})}{E_{t e s t}(w_{n u l l})}=\\frac{1}{\\mathrm{SNR}}\\frac{\\phi}{1-\\phi}+\\frac{n}{\\mathrm{SNR}_{0}}\\frac{\\phi_{0}}{1-\\phi_{0}},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathrm{SNR}:=\\|w_{0}\\|_{\\Sigma}^{2}/\\sigma^{2}$ and $\\mathrm{SNR}_{0}:=\\|w_{0}\\|_{\\Sigma}^{2}/\\sigma_{0}^{2}$ . We deduce that if $n\\gg\\mathrm{SNR}_{0}/(1/\\phi_{0}-1)$ , then the learned model is already much worse than the null predictor! This suggests that a possible strategy for mitigating the negative effects on learning on AI-generated data is regularization, as empirically illustrated in Figures 1a, 1b, 2, and also in 4 of Appendix D. ", "page_idx": 5}, {"type": "text", "text": "Furthermore, in Section 5 we shall establish that the optimal regularization parameter established in [14], in the case of polynomially decreasing spectra (a regime which is relevant to wide neural networks), must be modified in the presence of synthetic training data in order to prevent the generalization error to diverge to infinity (i.e catastrophic failure). ", "page_idx": 5}, {"type": "text", "text": "4.2 High-Dimensional Regimes ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In order to analyze the trace term $\\rho$ appearing in Equation (1) (and spelled out in (32) in Appendix F.1), we need some tools from RMT, and ultimately obtain analytic formulae for $E_{t e s t}(\\widehat{w}_{n}^{p r e d})$ in Theorem 4.3. Such tools have been used extensively to analyze anisotropic ridge regressio n  [41, 22, 4]. ", "page_idx": 5}, {"type": "text", "text": "Random Matrix Equivalents. For any sample size $T\\geq1$ and $\\lambda\\geq0$ , define $\\kappa(\\lambda,T)$ implicitly by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\kappa(\\lambda,T)-\\lambda=\\kappa(\\lambda,T)\\cdot\\mathrm{df_{1}}(\\kappa(\\lambda,T))/T,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where, for any $\\lambda\\geq0$ and $m\\in\\mathbb{N}_{\\star}$ , $\\mathrm{df}_{m}(\\lambda)$ is the mth order \"degree of freedom\" of the covariance matrix $\\Sigma$ is given by $\\mathrm{d}\\mathrm{f}_{m}(\\lambda)=\\mathrm{d}\\mathrm{f}_{m}(\\lambda;\\Sigma):=\\mathrm{tr}\\,\\Sigma^{m}(\\Sigma+\\lambda I_{d})^{-m}$ . ", "page_idx": 5}, {"type": "text", "text": "The effect of ridge regularization at level $\\lambda\\geq0$ is to improve the condition of the empirical covariance matrix $\\widehat{\\Sigma}$ ; what the $\\kappa$ -function does is translate this into regularization on $\\Sigma$ at level $\\kappa(\\lambda,T)$ , so as cont rol the capacity of the former, i.e. the \"effective dimension\" of the underlying problem. Quantitatively, there is an equivalence of the form $\\mathrm{df}_{1}(\\lambda;\\widehat{\\Sigma})\\approx\\mathrm{df}_{1}(\\kappa(\\lambda,T);\\Sigma)$ . Roughly speaking, RMT is the business of formalizing such a relationship an d derivatives (w.r.t. $\\lambda$ ) thereof. A standard reference on the subject is [5]. ", "page_idx": 5}, {"type": "text", "text": "Example: Isotropic Data. As an illustration, note that $\\mathrm{d}\\mathrm{f}_{m}(\\lambda)\\equiv d/(1+\\lambda)^{m}$ (polynomial decay) in the isotropic case where $\\Sigma=I_{d}$ . Consequently, we have ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\kappa(\\lambda,T)-\\lambda=\\phi\\cdot\\kappa(\\lambda,T)/(1+\\kappa(\\lambda,T)),\\;\\mathrm{with}\\;\\phi:=d/T.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In this case, it is easy to obtain the following well-known formula for $\\kappa=\\kappa(\\lambda,T)$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\kappa=\\frac{\\lambda+\\overline{{{\\phi}}}+\\sqrt{(\\lambda+\\overline{{{\\phi}}})^{2}+4\\lambda}}{2},\\;\\mathrm{with}\\;\\overline{{{\\phi}}}:=\\phi-1,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "which is reminiscent of the celebrated Marchenko-Pastur law [34]. ", "page_idx": 5}, {"type": "text", "text": "Asymptotic Regime. We shall work in the following so-called proportionate asymptotic scaling regime which is a standard analysis based on random matrix theory (RMT): ", "page_idx": 5}, {"type": "equation", "text": "$$\nT,d\\to\\infty,\\quad d/T\\to\\phi,\\quad\\|\\Sigma\\|_{o p},\\|\\Sigma^{-1}\\|_{o p}={\\cal O}(1).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Later in Section 5 when we consider power-law spectra, this scaling will be extended to account for the more realistic case where $d$ and $T$ are of the same order on log scale, i.e ", "page_idx": 5}, {"type": "equation", "text": "$$\nT,d\\to\\infty,\\quad d^{1/C}\\lesssim T\\lesssim d^{C},\\quad\\|\\Sigma\\|_{o p},\\|\\Sigma^{-1}\\|_{o p}=O(1),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "for some absolute constant $C\\geq1$ . Such non-proportionate settings are covered by the theory developed in [27, 48]. For clarity of presentation, even in this more general regime of Equations (12), we will still continue to write $\\dot{\\phi_{0}}:=\\dot{d}/T_{0}$ and $\\phi:=d/T$ . ", "page_idx": 5}, {"type": "text", "text": "Bias-Variance Decomposition. With everything now in place, let us recall for later use, the classical bias-variance decomposition for ridge regression (for example, see [41, 22, 4]): ", "page_idx": 5}, {"type": "text", "text": "Proposition 4.2. In the RMT limit (12), the test error of a ridge predictor $w(\\lambda)$ based on $T$ iid samples from the true data distribution $P_{\\Sigma,w_{0},\\sigma^{2}}$ is given by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{E_{t e s t}(w(\\lambda))=\\mathbb{E}\\left\\|w(\\lambda)-w_{0}\\right\\|_{\\Sigma}^{2}\\simeq B i a s+V a r,}\\\\ &{\\quad w i t h\\;B i a s\\simeq\\frac{\\kappa^{2}w_{0}^{\\top}\\Sigma(\\Sigma+\\kappa I)^{-2}w_{0}}{1-\\operatorname{df}_{2}(\\kappa)/T},\\quad V a r\\simeq\\frac{\\sigma^{2}\\mathrm{d}\\mathrm{f}_{2}(\\kappa)}{T}\\cdot\\frac{1}{1-\\operatorname{df}_{2}(\\kappa)/T},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\kappa=\\kappa(\\lambda,T)$ is as given in Equation (9). ", "page_idx": 5}, {"type": "text", "text": "4.3 Analytic Formula for Test Error ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The following result gives the test error for the downstream ridge predictor ${\\widehat{w}}_{n}^{p r e d}$ defined in Equation (7), in the context of fake training data, and will be heavily exploited later  to  obtain precise estimates in different regimes. Define generic $V a r$ and Bias by: ", "page_idx": 6}, {"type": "equation", "text": "$$\nV a r=\\mathbb{E}\\,\\|R X^{\\top}E/T\\|_{\\Sigma}^{2}=\\sigma^{2}\\frac{1}{T}\\,\\mathrm{tr}\\,\\Sigma R^{2}\\widehat{\\Sigma}\\qquad B i a s=\\mathbb{E}\\,\\|\\widehat{\\Sigma}R w_{0}-w_{0}\\|_{\\Sigma}^{2},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "and note that $E_{t e s t}^{c l e a n}:=B i a s+V a r$ , for standard ridge regression ftited on clean data from the true data distribution $P_{\\Sigma,w_{0},\\sigma^{2}}$ (e.g., see Hastie et al. [22]). Let $Q_{n-1}=P_{n-1}P_{n-2}\\dots P_{0}$ where $P_{m}$ is the orthogonal projection unto the subspace of $\\mathbb{R}^{d}$ spanned by the rows of $X_{m}$ and define ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\Delta B i a s}&{:=\\mathbb{E}\\,\\|\\widehat\\Sigma R(Q_{n-1}w_{0}-w_{0})\\|_{\\Sigma}^{2}\\geq0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Theorem 4.3. For an $n$ -fold fake data-generation process, the test error of a ridge predictor $\\widehat{w}_{n}^{p r e d}$ based on a sample of size $T$ with regularization parameter $\\lambda$ , is given in the RMT limit (12) b y ", "page_idx": 6}, {"type": "equation", "text": "$$\nE_{t e s t}(\\widehat{w}_{n}^{p r e d})\\simeq\\widetilde{B i a s}+V a r+n\\sigma_{0}^{2}\\rho,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\rho$ is as given in Theorem $F.l$ , and Bi as satisfies ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\widetilde{B i a s}\\geq B i a s+\\Delta B i a s\\geq B i a s\\,(w i t h\\,e q u a l i t y\\,i f T_{0}\\geq d),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "and \u2206Bias as given in (15). Furthermore, if one of the following conditions holds ", "page_idx": 6}, {"type": "text", "text": "then, we have the following explicit formula for $\\rho$ ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\rho=\\frac{\\mathrm{tr}\\,\\Sigma^{4}(\\Sigma+\\kappa_{0}I)^{-2}(\\Sigma+\\kappa I)^{-2}}{T_{0}-\\mathrm{df}_{2}(\\kappa_{0})}+\\frac{\\kappa^{2}\\,\\mathrm{tr}\\,\\Sigma^{2}(\\Sigma+\\kappa_{0}I)^{-2}(\\Sigma+\\kappa I)^{-2}}{T_{0}-\\mathrm{df}_{2}(\\kappa_{0})}\\cdot\\frac{\\mathrm{df}_{2}(\\kappa)}{T-\\mathrm{df}_{2}(\\kappa)},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "with $\\kappa=\\kappa(\\lambda,T)$ and $\\kappa_{0}:=\\kappa(0,T_{0})$ are as given in Equation (9). ", "page_idx": 6}, {"type": "text", "text": "Instructively, the term $\\Delta B i a s$ measures how biased the synthetic data-generation process away from ground-truth model $w_{0}$ . This term disappears if the generator was fitted on sufficiently many samples (i.e. if $T_{0}\\geq d)$ . More quantitatively, when $T_{0}<d$ and $X_{n}=X_{0}$ , it is easy to see that $\\Delta B i a s\\geq\\mathbb{E}\\left[\\|\\Sigma^{1/2}\\widehat{\\Sigma}R\\|_{o p}^{2}\\right]\\cdot B i a s_{0}$ , where $B i a s_{0}:=\\mathbb{E}\\left||P_{0}w_{0}-w_{0}|\\right|_{2}^{2}$ measures the inability due to lack of enough data, of the first generation $(n=1)$ ) to reliably estimate $w_{0}$ even in the absence of noise $(\\sigma_{0}=0)$ ) in the data-generating process. This gap propagates over to higher generations of the process. The situation is illustrated in Figure 2. In the case where $T_{0}<d$ and the $X_{n}$ \u2019s are independent, we shall see in Section 4.5 that this increase in bias actually grows with $n$ , even in the case of fake data generation without label noise (i.e. $\\sigma_{0}=0$ ). ", "page_idx": 6}, {"type": "text", "text": "4.4 Model Collapse in the Case of Under-Parametrized Fake Data-Generator ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We now consider the scenario of under-parameterization, where $T_{0}\\geq d$ , indicating that the number of data points exceeds the number of dimensions. This condition typically results in a unique solution for the regression. In this case, $P_{0}=I_{d}$ a.s., leading to Bi as = Bias (given as in formula (14)), and $\\,\\kappa_{0}=0$ in (18), and so Theorem 4.3 gives ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\rho={\\frac{\\mathrm{d}\\mathrm{f}_{2}(\\kappa)}{T_{0}-d}}+{\\frac{\\kappa^{2}\\,\\mathrm{tr}\\,(\\Sigma+\\kappa I)^{-2}}{T_{0}-d}}{\\frac{\\mathrm{d}\\mathrm{f}_{2}(\\kappa)}{T-\\mathrm{d}\\mathrm{f}_{2}(\\kappa)}}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We have the following corollary to Theorem 4.3. ", "page_idx": 6}, {"type": "text", "text": "Corollary 4.4. Consider the setting of Theorems 4.3 and F.1. If $T_{0}\\geq d$ additionally, then it holds in the RMT limit (12) that $E_{t e s t}(\\hat{w}_{n}^{p r e d})\\simeq B i a s+V a r+n\\sigma_{0}^{2}\\rho,$ , where Bias and V ar are as given in formula (14), and $\\rho$ is as given in Equation (19). ", "page_idx": 6}, {"type": "text", "text": "Moreover, in the special case of isotropic features, it holds that ", "page_idx": 6}, {"type": "equation", "text": "$$\nB i a s+V a r\\simeq\\frac{\\kappa^{2}\\|w_{0}\\|_{2}^{2}+\\sigma^{2}\\phi}{(1+\\kappa)^{2}-\\phi},\\quad\\rho\\simeq\\frac{\\phi_{0}}{1-\\phi_{0}}\\left(\\frac{1}{(1+\\kappa)^{2}}+\\frac{1}{(1+\\kappa)^{2}}\\frac{\\phi\\kappa^{2}}{(1+\\kappa)^{2}-\\phi}\\right),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "with $\\phi:=d/T,\\,\\phi_{0}:=d/T_{0}$ , and $\\kappa=\\kappa(\\lambda,T)$ as in Equation (10). ", "page_idx": 6}, {"type": "text", "text": "Note that Theorem 4.1 is special case of the above result corresponding to $\\lambda=0$ and $\\phi\\geq1$ . A result like Corollary 4.4 gives us the needed analytical handle for understanding $n$ -fold model collapse in terms of all problem hyper-parameters (covariance spectrum, regularization, label-noise level, etc.). ", "page_idx": 6}, {"type": "text", "text": "4.5 Model Collapse in the Absence of Label Noise ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We now consider the over-parametrized regime, where the different iterations of the synthetic datagenerator (refer to the illustration in Figure 3) are fitted on insufficient data. For simplicity of exposition, we restrict our presentation to isotropic covariance $\\Sigma=I_{d}$ . Since we will be focusing on the possible increase $\\Delta B i a s$ above the bias (defined in Equation (14)) due to $n\\geq1$ generations as predicted by Theorem 4.3, we further restrict ourselves to the noiseless regime where the fake data-generating process has no label noise, i.e. $\\sigma_{0}=0$ . Thanks to Lemma F.4, we know that the generation- $n$ fake labelling vector $\\widehat{w}_{n}$ (defined in Eqn. (5)) is given explicitly as a series of projections ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{w}_{n}=Q_{n-1}w_{0}=P_{n-1}P_{n-2}\\dots.\\ P_{0}w_{0}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Further, for simplicity we will assume $T=T_{n}>d$ , i.e the downstream model has access to enough data. We shall focus on two important special cases. ", "page_idx": 7}, {"type": "text", "text": "The Dependent Case. We first consider the case where $T_{m}\\,=\\,T_{0}\\,<\\,d$ and $X_{m}\\,=\\,X_{0}$ for all $m\\leq n-1$ . It is clear that Equation (20) reduces to $\\widehat{w}_{n}=P_{0}w_{0}$ , with rank $P_{0}=T_{0}<d$ . ", "page_idx": 7}, {"type": "text", "text": "Theorem 4.5. In the limit $\\lambda\\to0^{+}$ and $d,T_{0}\\rightarrow\\infty$ with $d/T_{0}\\to\\phi_{0}>1$ , it holds that ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\|\\widehat{w}_{n}\\|^{2}\\simeq\\|w_{0}\\|^{2}/\\phi_{0},\\quad\\Delta B i a s\\simeq\\|w_{0}\\|^{2}(1-1/\\phi_{0}).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "We see that in this setting, the increase in bias $\\Delta B i a s\\simeq(1\\!-\\!1/\\phi_{0})\\|w_{0}\\|^{2}$ brought about by synthetic data is a positive constant which does not grow with the number of generations $n\\geq1$ . This increase in bias (i.e compared to training on clean data) is due to the fact that, with probability 1, the random subspace of Rd spanned by $X_{0}$ does not contain the ground truth model $w_{0}$ . The expression is nothing but a RMT estimate of $\\|P_{0}w_{0}-w_{0}\\|^{2}$ , i.e. the squared norm of the projection of $w_{0}$ onto the orthogonal complement of this subspace. The result is illustrated in Figure 2(a). ", "page_idx": 7}, {"type": "text", "text": "The Independent Case. For our second example, we remove the assumption that $T_{m}=T_{0}$ and $X_{m}=X_{0}$ for all $m\\leq n-1$ considered in the previous case (Theorem 4.5). We instead assume that (A) The $X_{m}$ \u2019s are assumed to be independent, and (B) we are in the following high-dimensional limit ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\lambda\\rightarrow0^{+},\\quad d,T_{1},\\ldots,T_{n-1}\\rightarrow\\infty,\\quad d/T_{m}\\rightarrow\\phi_{m},\\quad\\mathrm{~for~some~}\\phi_{1},\\ldots,\\phi_{n-1}>0.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Define $\\begin{array}{r}{\\eta:=\\prod_{m=0}^{n-1}\\operatorname*{min}(1/\\phi_{m},1)\\in(0,1]}\\end{array}$ . We have the following theorem. ", "page_idx": 7}, {"type": "text", "text": "Theorem 4.6. In the limit (22), it holds that $\\|\\widehat{w}_{n}\\|^{2}\\simeq\\|w_{0}\\|^{2}\\eta$ and $\\Delta B i a s\\simeq\\|w_{0}\\|^{2}\\left(1-\\eta\\right).\\,I n$ particular, if $n\\to\\infty$ with infinitely many $\\phi_{m}>1$ , then $\\widehat{w}_{n}\\to0$ and $\\Delta B i a s\\rightarrow\\|w_{0}\\|^{2}$ . ", "page_idx": 7}, {"type": "text", "text": "The theorem predicts that a sequence of over-parametrized fake data-generators $(\\widehat{w}_{n})_{n}$ collapses to zero (and thus, effectively escapes from the ground truth model $w_{0}$ ). Consequently, the downstream model $\\widehat{w}_{n}^{p r e d}$ convergences to a Gaussian process around zero, instead of the true model $w_{0}$ , leading to an increase in the bias term of the test error! ", "page_idx": 7}, {"type": "text", "text": "For example if $\\phi_{n}=\\phi_{0}>1$ , then Theorem 4.6 predicts that $\\Delta B i a s\\simeq(1-\\phi_{0}^{-n})\\|w_{0}\\|^{2}$ , which grows exponentially fast towards $\\|w_{0}\\|^{2}$ , the test error of the null predictor. This compounding effect is due to the fact that in (20), each projection $P_{m}$ spins the fake data labelling vector $\\widehat{w}_{n}$ further away from the ground-truth $w_{0}$ . The result is illustrated in Figure 2(b). ", "page_idx": 7}, {"type": "text", "text": "Comparing the dependent case and the independent case, 4.6 shows that the increase in bias is proportional to $1-\\eta_{0}\\eta_{1}\\dots\\eta_{n-1}$ , which is typically much larger than $1-\\eta_{0}$ , which is the increase in the dependent case. Sampling different design matrices results in a more pronounced model collapse. ", "page_idx": 7}, {"type": "text", "text": "5 The Case of Heavy Tails (Power Law) ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Neural scaling laws [26, 24], relate a model\u2019s test error to the sample size, model size, and computational resources, and are critical tools for practitioners in strategically allocating resources during the design and implementation of large language models. Previous theoretical works [11, 41, 14] have examined scaling laws in our tractable setting of linear regression with Gaussian design in the context of a power-law covariance spectrum. Now we explore how synthetic data alters these scaling laws in this setting. ", "page_idx": 7}, {"type": "text", "text": "Let the spectral decomposition of the covariance matrix $\\Sigma$ be $\\Sigma=\\lambda_{1}v_{1}v_{1}^{\\top}+\\ldots+\\lambda_{d}v_{d}v_{d}^{\\top}$ , where $\\lambda_{1}\\geq.\\,.\\,.\\geq\\lambda_{d}\\geq0$ are the eigenvalues and $v_{1},\\ldots,v_{d}\\in\\mathbb{R}^{d}$ are the eigenvectors. For any feature index $j\\in[d]$ , define a coefficient $c_{j}:=w_{0}^{\\top}v_{j}$ , i.e the projection of $w_{0}$ along the $j$ th eigenvector of $\\Sigma$ . We shall work under the following well studied spectral conditions ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\mathbf{Capacity\\;Condition}\\right)\\lambda_{j}\\asymp j^{-\\beta}\\;\\mathrm{for}\\;\\mathrm{all}\\;j\\in[d],}\\\\ &{\\left(\\mathbf{Source\\;Condition}\\right)\\|{\\Sigma}^{1/2-r}w_{0}\\|=O(1),}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\beta\\,>\\,1$ and $r~>~0$ . The parameter $r$ measures the amount of dispersion of $w_{0}$ relative to the spectrum of $\\Sigma$ ; a large value of $r$ means $w_{0}$ is concentrated only along a few important eigen-directions (i.e. the learning problem is easy). For later convenience, define $\\delta,\\underline{{r}}$ , and $c$ by ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\delta:=1+\\beta(2r-1)\\in\\mathbb{R},\\quad\\underline{{r}}:=\\operatorname*{min}(r,1)\\in(0,1),\\quad c:=2\\beta\\underline{{r}}/(2\\beta\\underline{{r}}+1)\\in(0,1).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "As noted in [14], the source condition in (23) is satisfied if $c_{j}\\asymp j^{-\\delta/2}$ for all $j\\in[d]$ . ", "page_idx": 8}, {"type": "text", "text": "Consider adaptive ridge regularization strength of the form ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\lambda=\\lambda(T)\\asymp T^{-\\ell},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "for fixed $\\ell\\geq0$ . The case where $\\ell=0$ corresponds to non-adaptive regularization; otherwise, the level of regularization decays polynomially with the sample size $T$ . Define ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\ell_{c r i t}:=\\beta/(2\\beta\\underline{{r}}+1).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "In [14], KRR under normal circumstances (corresponding to $n=0$ , i.e. no fake data) was considered and it was shown that this value for the regularization exponent in (25) is minimax-optimal for normal test error in the noisy regime $(\\sigma>0)$ ), namely $E_{t e s t}(\\stackrel{.}{w}_{0}^{p r e d})\\asymp T^{-c}$ . This represents a crossover from the noiseless regime where it was shown that the test error scales like $E_{t e s t}(\\widehat{w}_{0}^{p r e d})\\asymp T^{-2\\beta\\underline{{r}}}$ , a much faster rate. In the context of training on fake data, which is the object of t h is manuscript, we shall establish new scaling laws which paint a drastically different picture. ", "page_idx": 8}, {"type": "text", "text": "A \"Collapsed\" Scaling Law. The following result shows that model collapse is a modification of usual scaling laws induced by fake data. All proofs of this section can be found in Appendix $\\mathrm{H}$ . Here, for simplicity of presentation, we restrict to the case $T_{0}\\geq d+2$ to make the results easier to present. This condition can be removed as in Theorem 4.3. ", "page_idx": 8}, {"type": "text", "text": "Theorem 5.1. Consider $n$ -fold fake-data generation with sample size $T_{0}\\,\\geq\\,d+2$ . For a ridge predictorwpnredgi ven in Equation (7) based on a fake data sample of size $T$ , with regularization paramete r $\\lambda=\\lambda(T)$ tuned adaptively as in Equation (25) with exponent $\\ell\\in[0,\\beta)$ , the test error satisfies the following scaling law in the RMT limit (12): ", "page_idx": 8}, {"type": "equation", "text": "$$\nE_{t e s t}(\\overbrace{w_{n}^{p r e d}}^{\\prime})\\asymp\\operatorname*{max}(\\sigma^{2},T^{1-2\\underline{{r}}\\ell-\\frac{\\ell}{\\beta})})\\cdot T^{-(1-\\frac{\\ell}{\\beta})}+\\frac{n\\sigma_{0}^{2}}{1-\\phi_{0}}\\operatorname*{max}\\left(T/T_{0},\\phi_{0}\\right)\\cdot T^{-(1-\\frac{\\ell}{\\beta})}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "We now provide an instructive interpretation of Theorem 5.1 and outline the effect of regularization. ", "page_idx": 8}, {"type": "text", "text": "The Noiseless Regime. First consider the case $\\sigma\\,=\\,0$ (or equivalently, exponentially small in $T)$ and $\\phi_{0}\\,\\in\\,(0,1)$ is fixed, and consider a number of generations $n$ such that $n\\sigma_{0}^{2}\\asymp\\dot{T}^{a}$ , where $0\\leq a\\leq1-\\ell/\\beta\\leq1$ . Note that $a=0$ corresponds to a constant number of generations. Also take $T_{0}=T^{b}$ , for some constant $b\\in(0,\\infty)$ . According to Theorem 5.1, if we want to balance out the model-collapsing negative effect of training on fake data, we should chose $\\ell$ so as to balance the second term $n(\\bar{T^{}}/T_{0}\\bar{)}T^{-(1-\\ell/\\beta)}=T^{-(b-\\ell\\bar{/}\\beta-a)}$ and the first term $T^{-2\\ell_{\\underline{{r}}}}$ . We have the following: ", "page_idx": 8}, {"type": "text", "text": "Corollary 5.2. In the setting of Theorem 5.1 with $T_{0}\\asymp T^{b}$ and $n\\asymp T^{b}$ , the optimal exponent of the ridge regularization parameter in Equation (25) is $\\ell=\\ell_{\\star}$ , where ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\ell_{\\star}=\\operatorname*{min}((b-a)\\ell_{c r i t},\\beta),\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "and $\\ell_{c r i t}$ is as in Eqn. (26), with corresponding optimal test error $\\operatorname*{inf}_{\\ell\\geq0}E_{t e s t}(\\widehat{w}_{n}^{p r e d})\\asymp T^{-(b-a)c}$ . ", "page_idx": 8}, {"type": "text", "text": "Observe that when $(b\\mathrm{~-~}a)c\\,<\\,2\\beta\\underline{{r}}$ , which is the case when $n\\,=\\,O(1)$ , $r\\,\\geq\\,1$ and $b\\leq a+1$ , this corresponds to the condition $T\\gtrsim T_{0}$ . The above result represents a crossover from the fast rate $E_{t e s t}(\\widehat{w}_{0}^{p r e d})\\;\\asymp\\;T^{-2\\beta\\underline{{r}}}$ in the case of training on clean data [14], to a much slower rate ", "page_idx": 8}, {"type": "text", "text": "$E_{t e s t}(\\hat{w}_{n}^{p r e d})\\asymp T^{-(b-a)c}$ , attained by the adaptive regularization $\\lambda\\asymp T^{-\\ell_{\\star}}$ , which is optimal in this se tt ing. Furthermore, in this setting if we still use $\\lambda\\asymp T^{-\\ell_{c r i t}}$ as proposed in [14] in the clean data setting, Corollary 5.2 predicts that ", "page_idx": 9}, {"type": "equation", "text": "$$\nE_{t e s t}(\\hat{w}_{n}^{p r e d})\\gtrsim T^{-(b-\\ell_{c r i t}/\\beta-a)}=T^{-(c+b-a-1)},\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "which diverges to infinity if $b\\geq a+1-c$ . This is a catastrophic form of model collapse, and is empirically illustrated in Figures 1b and 4. ", "page_idx": 9}, {"type": "text", "text": "The Noisy Regime. This discussion can be found in Appendix G. ", "page_idx": 9}, {"type": "text", "text": "Remark. In all the analyses above, we quantitatively demonstrate how model collapse manifests as a change in scaling laws within a setting commonly used to understand scaling behavior in current foundation models [46, 13, 14]. Our results indicate that, in the presence of synthetic data, scaling laws with respect to dataset size slow down (i.e., exhibit smaller exponents), meaning a much larger sample size is needed to achieve the same reduction in test error as with real data. Furthermore, the optimal scaling law with synthetic data requires different regularization; the optimal settings for real data could lead to catastrophic model collapse. Related findings are reported in Dohmatob et al. [18] in the setting of discrete data for infinite memory models and their variants. ", "page_idx": 9}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "To further support our theoretical findings, we conduct experiments using kernel ridge regression on the MNIST dataset [16], as detailed in Appendix D.2. Our experiments validate the theoretical predictions for both RBF and Polynomial kernels, demonstrating the parallels between linear regression and kernel regression, and highlighting the relevance of our theory to more complex settings. ", "page_idx": 9}, {"type": "text", "text": "We also explore the behavior of real neural networks by training two-layer networks in two different settings: fixing the first layer or training both layers (see Appendix D.3). Consistent with our theoretical insights, we observe a linear pattern of model collapse when the first layer is fixed. However, a more severe, nearly quadratic model collapse is observed when both layers are trained, with our theory providing a lower bound for this behavior. These results reinforce the ability of our theory to capture the dynamics of model collapse across varying complexities. Full experimental details and results are provided in Appendix D. ", "page_idx": 9}, {"type": "text", "text": "7 Concluding Remarks ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "As we navigate the \"synthetic data age\", our findings signal a departure from traditional test error rates (e.g. neural scaling laws), introducing novel challenges and phenomena with the integration of synthetic data from preceding AI models into training sets. Our work provides a solid analytical handle for demystifying the model collapse phenomenon as a modification of usual scaling laws caused by fake / synthesized training data. ", "page_idx": 9}, {"type": "text", "text": "On the practical side, our analysis reveals that AI-generated data alters the optimal regularization for downstream models and changes the scaling laws. Drawing from the insight that regularization mirrors early stopping [3], our study suggests that models trained on mixed real and AI-generated data may initially improve but later decline in performance (model collapse), necessitating early detection of this inflection point. To preserve model quality when scaling laws are altered, it is essential to employ data flitering and watermarking techniques to distinguish real data from synthetic content. Recent studies have also explored methods for data selection [19] and correction [20]. These observations prompt a re-evaluation of current training approaches and underscores the complexity of model optimization in the era of synthetic data. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "YF and JK are supported by the National Science Foundation under NSF Award 1922658. Part of this work was done while JK and YF were hosted by the Centre Sciences de Donn\u00e9es at the \u00c9cole Normale Sup\u00e9rieure (ENS) in 2023/24, whose hospitality they gratefully acknowledge. This work was partially supported through the NYU IT High Performance Computing (HPC) resources, services, and staff expertise. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.   \n[2] Sina Alemohammad, Josue Casco-Rodriguez, Lorenzo Luzi, Ahmed Imtiaz Humayun, Hossein Babaei, Daniel LeJeune, Ali Siahkoohi, and Richard G. Baraniuk. Self-consuming generative models go mad. arXiv preprint arxiv:2307.01850, 2023.   \n[3] Alnur Ali, J. Zico Kolter, and Ryan J. Tibshirani. A continuous-time view of early stopping for least squares regression. In Kamalika Chaudhuri and Masashi Sugiyama, editors, Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics, volume 89 of Proceedings of Machine Learning Research, pages 1370\u20131378. PMLR, 16\u201318 Apr 2019.   \n[4] Francis Bach. High-dimensional analysis of double descent for linear regression with random projections. 2023.   \n[5] Zhidong. Bai and Jack W. (Jack William) Silverstein. Spectral analysis of large dimensional random matrices. Springer series in statistics. Springer, New York ;, 2nd ed. edition, 2010. ISBN 9781441906601.   \n[6] Rapha\u00ebl Berthier, Francis R. Bach, and Pierre Gaillard. Tight nonparametric convergence rates for stochastic gradient descent under the noiseless linear model. CoRR, abs/2006.08212, 2020. URL https://arxiv.org/abs/2006.08212.   \n[7] Quentin Bertrand, Avishek Joey Bose, Alexandre Duplessis, Marco Jiralerspong, and Gauthier Gidel. On the stability of iterative retraining of generative models on their own data. arXiv preprint arxiv:2310.00429, 2023.   \n[8] Matyas Bohacek and Hany Farid. Nepotistically trained generative-ai models collapse, 2023.   \n[9] Martin Briesch, Dominik Sobania, and Franz Rothlauf. Large language models suffer from their own output: An analysis of the self-consuming training loop, 2023.   \n[10] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 1877\u20131901. Curran Associates, Inc., 2020.   \n[11] Andrea Caponnetto and Ernesto de Vito. Optimal rates for the regularized least-squares algorithm. Foundations of Computational Mathematics, 7:331\u2013368, 2007.   \n[12] Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming. Advances in neural information processing systems, 32, 2019.   \n[13] Hugo Cui, Bruno Loureiro, Florent Krzakala, and Lenka Zdeborova. Generalization error rates in kernel regression: The crossover from the noiseless to noisy regime. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, 2021.   \n[14] Hugo Cui, Bruno Loureiro, Florent Krzakala, and Lenka Zdeborov\u00e1. Generalization error rates in kernel regression: the crossover from the noiseless to noisy regime. Journal of Statistical Mechanics: Theory and Experiment, 2022(11):114004, nov 2022.   \n[15] Hugo Cui, Bruno Loureiro, Florent Krzakala, and Lenka Zdeborov\u00e1. Error scaling laws for kernel classification under source and capacity conditions. Machine Learning: Science and Technology, 4(3):035033, August 2023. ISSN 2632-2153.   \n[16] Li Deng. The mnist database of handwritten digit images for machine learning research. IEEE Signal Processing Magazine, 29(6):141\u2013142, 2012.   \n[17] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.   \n[18] Elvis Dohmatob, Yunzhen Feng, Pu Yang, Fran\u00e7ois Charton, and Julia Kempe. A tale of tails: Model collapse as a change of scaling laws. In Forty-first International Conference on Machine Learning, 2024. URL https://openreview.net/forum?id=KVvku47shW.   \n[19] Yunzhen Feng, Elvis Dohmatob, Pu Yang, Francois Charton, and Julia Kempe. Beyond model collapse: Scaling up with synthesized data requires reinforcement. arXiv preprint arXiv:2406.07515, 2024.   \n[20] Nate Gillman, Michael Freeman, Daksh Aggarwal, HSU Chia-Hong, Calvin Luo, Yonglong Tian, and Chen Sun. Self-correcting self-consuming loops for generative model training. In Forty-first International Conference on Machine Learning, 2024.   \n[21] Yanzhu Guo, Guokan Shang, Michalis Vazirgiannis, and Chlo\u00e9 Clavel. The curious decline of linguistic diversity: Training language models on synthetic text, 2023.   \n[22] Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J. Tibshirani. Surprises in highdimensional ridgeless least squares interpolation. The Annals of Statistics, 50(2), 2022.   \n[23] Ryuichiro Hataya, Han Bao, and Hiromi Arai. Will large-scale generative models corrupt future datasets? In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 20555\u201320565, October 2023.   \n[24] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.   \n[25] Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and generalization in neural networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018.   \n[26] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.   \n[27] Antti Knowles and Jun Yin. Anisotropic local laws for random matrices. Probability Theory and Related Fields, 169(1):257\u2013352, 2017.   \n[28] Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S. Schoenholz, Jeffrey Pennington, and Jascha Sohl-Dickstein. Deep neural networks as gaussian processes. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018.   \n[29] Tengyuan Liang and Alexander Rakhlin. Just interpolate: Kernel \u201cRidgeless\u201d regression can generalize. The Annals of Statistics, 48(3), 2020.   \n[30] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.   \n[31] Alexander Maloney, Daniel A. Roberts, and James Sully. A solvable model of neural scaling laws, 2022.   \n[32] Gonzalo Mart\u00ednez, Lauren Watson, Pedro Reviriego, Jos\u00e9 Alberto Hern\u00e1ndez, Marc Juarez, and Rik Sarkar. Combining generative artificial intelligence (ai) and the internet: Heading towards evolution or degradation? arXiv preprint arxiv: 2303.01255, 2023.   \n[33] Gonzalo Mart\u00ednez, Lauren Watson, Pedro Reviriego, Jos\u00e9 Alberto Hern\u00e1ndez, Marc Juarez, and Rik Sarkar. Towards understanding the interplay of generative artificial intelligence and the internet. arXiv preprint arxiv: 2306.06130, 2023.   \n[34] V.A. Marc\u02c7enko and Leonid Pastur. Distribution of eigenvalues for some sets of random matrices. Math USSR Sb, 1:457\u2013483, 01 1967.   \n[35] Midjourney. Midjourney ai, 2023. URL https://www.midjourney.com/.   \n[36] Hossein Mobahi, Mehrdad Farajtabar, and Peter Bartlett. Self-distillation amplifies regularization in Hilbert space. In Advances in Neural Information Processing Systems, volume 33, pages 3351\u20133361. Curran Associates, Inc., 2020.   \n[37] Radford M. Neal. Priors for infinite networks. In Bayesian Learning for Neural Networks, pages 29\u201353. Springer, New York, 1996.   \n[38] Loucas Pillaud-Vivien, Alessandro Rudi, and Francis R. Bach. Statistical optimality of stochastic gradient descent on hard learning problems through multiple passes. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicol\u00f2 Cesa-Bianchi, and Roman Garnett, editors, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, pages 8125\u20138135, 2018.   \n[39] Ali Rahimi and Benjamin Recht. Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning. In Advances in Neural Information Processing Systems. Curran Associates, Inc., 2008.   \n[40] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 8821\u20138831. PMLR, 18\u201324 Jul 2021.   \n[41] Dominic Richards, Jaouad Mourtada, and Lorenzo Rosasco. Asymptotics of ridge(less) regression under general source condition. In Proceedings of The 24th International Conference on Artificial Intelligence and Statistics, volume 130 of Proceedings of Machine Learning Research. PMLR, 2021.   \n[42] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 10684\u201310695, June 2022.   \n[43] Alessandro Rudi and Lorenzo Rosasco. Generalization properties of learning with random features. In Advances in Neural Information Processing Systems. Curran Associates Inc., 2017. ISBN 9781510860964.   \n[44] Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Yarin Gal, Nicolas Papernot, and Ross Anderson. The curse of recursion: Training on generated data makes models forget. arXiv preprint arxiv:2305.17493, 2023.   \n[45] James B. Simon, Madeline Dickens, and Michael Robert DeWeese. Neural tangent kernel eigenvalues accurately predict generalization. 2021.   \n[46] Stefano Spigler, Mario Geiger, and Matthieu Wyart. Asymptotic learning curves of kernel methods: empirical data versus teacher\u2013student paradigm. Journal of Statistical Mechanics: Theory and Experiment, 2020(12), 2020.   \n[47] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \n[48] Alexander Wei, Wei Hu, and Jacob Steinhardt. More than a toy: Random matrix models predict how real-world neural representations generalize. In Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research. PMLR, 2022.   \n[49] Christopher Williams. Computing with infinite networks. In M.C. Mozer, M. Jordan, and T. Petsche, editors, Advances in Neural Information Processing Systems, volume 9. MIT Press, 1996. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Appendix / Supplementary Material for Model Collapse Demystified: The Case of Regression ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Contents ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A Differences to Self-Distillation 15 ", "page_idx": 14}, {"type": "text", "text": "B Related work on Kernel Ridge Regression with Gaussian Design 16 ", "page_idx": 14}, {"type": "text", "text": "C Extension to Kernel Methods 16 ", "page_idx": 14}, {"type": "text", "text": "D Details of Experiments 17 ", "page_idx": 14}, {"type": "text", "text": "D.1 Simulated Data 17   \nD.2 Real Data: Kernel Ridge Regression on MNIST 17   \nD.3 Neural Networks on MNIST 17 ", "page_idx": 14}, {"type": "text", "text": "E Notations 19 ", "page_idx": 14}, {"type": "text", "text": "F.1 A General Formula for Test Error . . 19   \nF.2 Proof of Theorem 4.1 (Rigeless Regression) 20   \nF.3 Proof of Theorem F.1 (Ridge Regression $^+$ General Covariance) 21   \nF.4 Proof of Theorem 4.3 . . 22   \nF.5 Proof of Corollary 4.4 . . . 23   \nF.6 A Note on Proposition 4.2 24   \nF.7 Proof of Theorem 4.5 and Theorem 4.6 (Model Collapse in the Absence of Label   \nNoise) . 24   \nG The Noisy Regime for Power Law Spectra 26   \nH Proof of Results for Power-Law Covariance Spectrum 26   \nH.1 Proof of Theorem 5.1 . . 26   \nH.2 Representation of Clean Test Error . 26 ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "I Auxiliary Lemmas 27 ", "page_idx": 14}, {"type": "text", "text": "A Differences to Self-Distillation ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "An important point we wish to make is that the fake data generation process that we analyse should not be confused with self-distillation as formulated in Mobahi et al. [36] for example. Our setting is inspired by the model collapse phenomenon, where increasingly vast amounts of synthetic data generated by users is posted online, and will necessarily enter the training set of the next foundation model. In this case, we do not have ground truth labels, nor is the generation of synthetic data not controlled by us, but by other users. Therefore, we adopt the setting of solely synthetic labels with added noise. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Specifically, in our setup, at generation $n\\,>\\,0$ , we do not have access to the true labels $Y_{0}\\,=$ $f_{0}(X)+n o i s e$ for the training samples $X$ , but rather to some $\\hat{Y}_{n}\\,=\\,\\hat{f}_{n}(X)+n o i s e$ , where $\\hat{f}_{n}$ is an unknown function, which synthesizes fake labels iteratively; the integer $n$ is the number of iterations. In our work, we make the structural assumption that $\\hat{f}_{n}$ is obtained by iterative / successive regressions on a true dataset $D_{0}=(X_{0},Y_{0})$ . We do not have any control over the creation of these labels, which is reflected by the noise injected at each stage. ", "page_idx": 15}, {"type": "text", "text": "In the self-distillation setting, the data generation process actually helps performance of the downstream model. The model has access to training labels from the true data distribution $Y$ , but decides to fit a model on this data, and then use its outputs as the new labels $Y_{n}:=F_{n}(X,Y)$ , iterating this process possibly over severable steps. Thus, self-distillation has control over the data generating process, which is carefully optimized for the next stage training. Specifically, [36] study self-distillation in the same Gaussian regression model underlying our analysis, but in each distillation generation are able to tune the regularization parameter for downstream performance as a function of the original data labels (with the data being the same at each generation). In the setting of model collapse, there is no control over the data generation process, since it constitutes synthesized data which typically comes from the wide web. ", "page_idx": 15}, {"type": "text", "text": "Self-distillation for linear regression would amount to a very special instance of our analysis where (1) $X_{0}=X_{1}=\\ldots=X_{n-1}=X_{n}=X$ and (2) $\\sigma_{0}=\\ldots=\\sigma_{n-1}=0$ . That is, there is exactly one design matrix which is used in the data generation process and in the downstream estimator, and also no additional source of label noise is present at the end of each generation. ", "page_idx": 15}, {"type": "text", "text": "In the general setup considered in our work, (1) is not imposed. We typically assume that $X_{0},X_{1},\\ldots,X_{n-1},X_{n}$ with $X_{n}\\ =\\ X$ , are all independent random matrices. An exception is line 247 (\u201cThe Dependent Case\") of Section 3.5, where we assume $X_{m}=X_{0}$ for all $m\\leq n-1$ , and independent of $X_{n}=X$ . That setup (considered for the purposes of showing that model collapse can still occur in the absence of label noise) also assumes $\\sigma_{m}=0$ for all $m$ ; the analytic picture which emerges (Theorem 3.5) is already drastically different from what one would get from self-distllation (corresponding to additional assumption that $X=X_{0}$ ). ", "page_idx": 15}, {"type": "text", "text": "B Related work on Kernel Ridge Regression with Gaussian Design ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "This model has been studied by a vast body of works. For example, Richards et al. [41], Hastie et al. [22], Bach [4] analyze the classical bias-variance decomposition of the test error for ridge regression in the high dimensional setting where dataset size and dimension diverge proportionately, using tools from Random Matrix Theory (RMT). In Section 4 we significantly extend this type of analysis to training on iteratively generated synthetic data. This model is also particularly attractive because it allows to analyze an important trade-off: the relative decay of the eigenvalues of the kernel (capacity) and the coefficients of the target function in feature space (source). Sizeable effort has been dedicated to characterize the influence on the decay rate of the test error as a function of these two relative decays (aka power laws) [11, 38, 6, 41, 46, 14, 15]. In Section 5 we extend these efforts, in particular based on works of Cui et al. [13, 14] which has given a full characterization of all regimes and test error decay that can be observed at the interplay of noise and regularization, characterizing a crossover transition of rates in the noisy setting. Our work uncovers fascinating new effects as a result of iterative training on synthetic data. ", "page_idx": 15}, {"type": "text", "text": "C Extension to Kernel Methods ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Though we present our results in the case of linear regression in $\\mathbb{R}^{d}$ for clarity, they can be rewritten in equivalent form in the kernel setting. Indeed, as in [11, 45, 14, 29], it suffices to replace $x$ with a feature map induced by a kernel $K$ , namely $\\psi(\\boldsymbol{x}):=K_{x}\\in\\mathcal{H}_{K}$ . Here, $\\mathcal{H}_{K}$ is the reproducing kernel Hilbert space (RKHS) induced by $K$ . In the data distribution (2), we must now replace the Gaussian marginal distribution condition $\\boldsymbol{x}\\sim N(0,\\Sigma)$ with $\\psi(x)\\sim N(0,\\Sigma)$ . The ground-truth labeling linear function in (2) is now just a general function $f_{0}\\in L^{2}$ . The predictor (7) is then given by (Representer Theorem) $\\widehat f_{n}^{p r e d}(x):=K(X,x)^{\\top}\\widehat{c}_{n}$ , with ${\\widehat{c}}_{n}\\,=\\,(G+\\lambda T I_{d})^{-1}Y\\,\\in\\,\\mathbb{R}^{n}$ , where $K(X,x):=(K(x_{1},x),\\ldots,\\bar{K}(x_{T},x))$ , and $G=K(X,X)\\in\\mathbb{R}^{n\\times n}$ is the Gram matrix. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "D Details of Experiments ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We perform the following experiments on both simulated and real data to empirically validate our theoretical results. ", "page_idx": 16}, {"type": "text", "text": "D.1 Simulated Data ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We consider ordinary / linear ridge regression in $\\mathbb{R}^{d}$ , for $d=300$ and different structures for the covariance matrix $\\Sigma$ of the inputs: isotropic (i.e $\\Sigma=I_{d},$ ) and power-law (23), with $(\\beta,r)=(2,0.375)$ . For each value of $n$ (the generation index), the fake data-generator is constructed according to the process described in (4). Then, for different values of $T$ (between 1 and 1000, 000), a sample of size $T$ is drawn from this fake data-generator and then a downstream ridge model (7) is ftited. The test set consists of 100, 000 clean pairs $(x,y)$ form the true data distribution $P_{\\Sigma,w_{0},\\sigma^{2}}$ . This experiment is repeated 10 times to generate error bars. The results for the isotropic setting are shown in Figure 1a and the results for the power-law setting are shown in Figure 1b. Figure 2 shows the over-parametrized setting. ", "page_idx": 16}, {"type": "text", "text": "D.2 Real Data: Kernel Ridge Regression on MNIST ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "As in Cui et al. [14], Wei et al. [48] we consider a distribution on MNIST [16], a popular dataset in the ML community. The classification dataset contains 60, 000 training and 10, 000 test data points (handwritten), with labels from 0 to 9 inclusive. Like in Cui et al. [14], we convert the labels into real numbers (i.e a regression problem) as follows: $y=$ label mod $2+$ noise , where the variance of the noise is $\\sigma^{2}={\\bar{1}}$ (for simplicity, we also set $\\sigma_{0}^{2}=1$ ). The test set consists of 10, 000 pairs $(x,y)$ , with the labels $y$ constructed as described in the previous sentence. The fake data used for training is generated as in the previous experiment, but via kernel ridge regression (instead of least squares) with the RBF kernel (bandwidth $\\bar{\\mathbf{\\Gamma}}=10^{-4}$ ) and the polynomial kernel (degree $=5$ , bandwidth $\\,=\\,10^{-3}.$ ). Note that it was empirically shown in Cui et al. [14] that these datasets verify (23) with $(\\beta,r)\\approx(1.65,0.097)$ in the case of the aforementioned RBF kernel, and $(\\beta,r)\\approx(1.2,0.15)$ in the case of the polynomial kernel. Then, for different values of $T$ (between 1 and 1000), a sample of size $T$ is drawn from this fake data-generator and then a downstream kernel ridge model is ftited. Each of these experiments are repeated 10 times to generate error bars (due to different realizations of label noise). The results are shown in Figure 4. ", "page_idx": 16}, {"type": "text", "text": "D.3 Neural Networks on MNIST ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We now further examine model collapse in two-layer neural networks on the MNIST dataset, beyond the linear setting and Gaussian data. We consider two scenarios: ", "page_idx": 16}, {"type": "text", "text": "\u2022 learning with a random features (RF) model, where the first layer is fixed randomly, and only the second layer is trained, and   \n\u2022 learning with a fully trainable neural network. ", "page_idx": 16}, {"type": "text", "text": "For the two-layer network with the first layer fixed, our theory predicts a linear increase in test error as a function of the number of iterations $n$ . This is because such models belong to the linearized regimes as finite-width random feature models and can be approximated by kernel regression [25, 45]. For fully-trained neural networks, our theory does not directly apply. However, we anticipate that the general trends uncovered by our asymptotic theory will hold true \u2014 for example, more parameters are expected to lead to greater model collapse, as shown in Theorem 4.1. ", "page_idx": 16}, {"type": "text", "text": "Specifically, the models were trained using stochastic gradient descent (SGD) with a batch size of 128 and a learning rate of 0.1. We employed a regression setting where labels were converted to one-hot vectors, and the model was trained using mean squared error for 200 epochs to convergence. When generating the synthetic data, Gaussian label noise with a standard deviation of 0.1 is added. The test error is consistently evaluated on the test set using clean labels. ", "page_idx": 16}, {"type": "image", "img_path": "bioHNTRnQk/tmp/b7ae70aeb2b275c09e68f4a9f8f20737471babd630efde592f7c8a78d0a89ce9.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "", "img_caption": ["(b) Polynomial kernel (degree $=5$ , bandwidth $=10^{-3}$ ) "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Figure 4: Model collapse in kernel ridge regression (power-law covariance spectrum) on MNIST. Here, we use adaptive regularization $\\breve{T}^{-\\ell}$ for different values of the exponent $\\ell\\geq0$ (see Section D for full experimental setup). Top row: RBF kernel. Bottom row: polynomial kernel. In each plot, we show test error curves as a function of sample size $T$ , from different generations $(n)$ of fake data. The broken vertical line corresponds to $T=T_{0}$ , where $T_{0}$ is the number of samples (from the true data distribution) which was used to train the label faker. The value of the exponent regularization $\\ell=\\ell_{\\star}$ (broken curves) is the optimal value in the presence of iterative data relabeling, while $\\ell=\\ell_{c r i t}$ (solid curves) corresponds to the optimal value without iterative re-labelling (i.e $n=0$ ) proposed in Cui et al. [14] (see (26)). Specifically, we take $\\ell_{\\star}=(b-a)\\ell_{c i r t}=b\\ell_{c r i t}$ , where $b=\\log\\bar{T_{0}}/\\log T$ (so that $T_{0}=T^{b}$ ), as proposed in Theorem 5.1, formula (28). Notice how the effect of fake data makes the test error become non decreasing in sample size $T$ . This is effectively a collapse of the learned model. ", "page_idx": 17}, {"type": "text", "text": "The results for RF models of width (i.e number of hidden dimensions) $k$ of 20,000 are presented in Figure 5. We observe that, with the exception of the first two generations, the decay in MSE loss generally follows a linear trend, which is consistent with the predictions of our theory. ", "page_idx": 17}, {"type": "text", "text": "Next, we consider the scenario of training the entire neural network. By varying the width $k$ , we adjust the number of parameters to further explore the theoretical predictions on how the number of parameters influences model collapse. ", "page_idx": 17}, {"type": "text", "text": "Observations. From Figure 6, we can observe that ", "page_idx": 17}, {"type": "text", "text": "\u2022 More parameters (wider neural networks, i.e large $k$ ) lead to increased model collapse. This observation is consistent with our results proved in the linear regime (Theorem 4.1). For linear models, the number of parameters is proportional to $d$ (the input dimension), whereas in two-layer neural networks, the number of parameters is of order $k d$ (i.e proportional to the width $k$ ).   \n\u2022 The dependence of model collapse on the number of iterations $n$ is linear for small values of $n$ (with $n\\leq4$ in our experiments), and becomes superlinear (possibly quadratic) for larger values of $n$ (with $n\\geq4$ ). Recall that $n=0$ corresponds to training on clean data from the data distribution. Thus, possibly, model collapse neural networks appears to be even more severe than in linear regression. ", "page_idx": 17}, {"type": "image", "img_path": "bioHNTRnQk/tmp/687217f6cc06dab912e8866f6c28c8d29e2b5b5cd249c5626850b6b26e9e643c.jpg", "img_caption": ["Figure 5: Performance of RF model on MNIST, Figure 6: The performance of two-layer neural with one-hidden layer NN (width $k=20,000)$ . network on MNIST with varying hidden dimenStandard deviation is calculated using 10 seeds. sions. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "E Notations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The set of integers from 1 through $d$ is denoted $[d]$ . Given a variable $z$ (which can be the input dimension $d$ or the sample size $T$ , etc.) the notation $f\\!\\left(z\\right)\\lesssim g\\!\\left(z\\right)$ means that $f(z)\\leq C g(z)$ for sufficiently large $z$ and an absolute constant $C$ , while $f\\!\\left(z\\right)\\asymp g(z)$ means $f(z)\\lesssim g(z)\\lesssim f(z)$ . Further, $\\bar{f}(z)\\simeq g(z)$ means $f(z)=(1+o(1))g(z)$ , where $o(1)$ stands for a quantity which tends to zero in the limit $z\\rightarrow\\infty$ . We denote with $A^{\\dagger}$ the Moore-Penrose pseudo-inverse any matrix $A$ , and by $\\|A\\|_{o p}$ is operator norm, while the trace of a square matrix $A$ is denoted $\\operatorname{tr}A$ . Finally, $\\Vert u\\Vert_{\\Sigma}:=\\sqrt{u^{\\top}\\Sigma u}$ is the Mahalanobis norm induced by a positive-definite matrix $\\Sigma$ . ", "page_idx": 18}, {"type": "text", "text": "F Exact Characterization of Test Error Under Model Collapse ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "F.1 A General Formula for Test Error ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We now consider the case of general ridge penalty $\\lambda>0$ , and drop the requirements $T\\geq d+2$ and $T_{0}\\geq d+2$ . Recall the definitions of $X,Y,E$ and the random matrices $R$ and $\\widehat{\\Sigma}$ appearing in (7). For later reference, define ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{B i a s:=\\mathbb{E}\\left\\|\\widehat{\\Sigma}R w_{0}-w_{0}\\right\\|_{\\Sigma}^{2},}\\\\ &{V a r=\\mathbb{E}\\left\\|R X^{\\top}E/T\\right\\|_{\\Sigma}^{2}=\\sigma^{2}\\frac{1}{T}\\operatorname{tr}\\Sigma R^{2}\\widehat{\\Sigma}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "These are respectively the bias and variance terms in the classical bias-variance decomposition ", "page_idx": 18}, {"type": "equation", "text": "$$\nE_{t e s t}^{c l e a n}:=B i a s+V a r,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for standard ridge regression fitted on clean data from the true data distribution $P_{\\Sigma,w_{0},\\sigma^{2}}$ (e.g., see Hastie et al. [22]). ", "page_idx": 18}, {"type": "text", "text": "Theorem F.1. For an $n$ -fold fake data generation process, the test error of a ridge predictor $\\widehat{w}_{n}^{p r e d}$ based on a sample of size $T\\geq1$ with regularization parameter $\\lambda$ is given by ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{c}{E_{t e s t}(\\widehat{w}_{n}^{p r e d})=\\widehat{B i a s}+V a r+n\\sigma_{0}^{2}\\rho,}\\\\ {\\widehat{B i a s}=\\mathbb{E}\\left\\|\\widehat{\\Sigma}R Q_{n-1}w_{0}-w_{0}\\right\\|_{\\Sigma}^{2},}\\\\ {\\rho:=\\displaystyle\\frac{1}{n}\\sum_{m=0}^{n-1}\\mathbb{E}\\mathrm{\\tr}\\,C_{n-1,m}\\widehat{\\Sigma}R\\Sigma R\\widehat{\\Sigma},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where V ar is as given in (30) and $C_{k,m}\\;:=\\;\\overline{{Q}}_{k,m}\\overline{{Q}}_{k,m}^{\\top}$ for $\\overline{{{Q}}}_{k,m}~=~Q_{k,m}X_{m}^{\\dagger}$ , $Q_{k,m}~:=$ $P_{k}P_{k-1}\\dots P_{m},$ , $Q_{k}\\,:=\\,Q_{k,0}\\,=\\,P_{k}P_{k-1}\\,.\\,.\\,.\\,P_{0}$ , with $P_{m}\\,=\\,X_{m}^{\\dagger}X_{m}$ being the orthogonal projection matrix onto the subspace of $\\mathbb{R}^{d}$ spanned by the rows of $X_{m}$ . ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left.\\begin{array}{c}{{E_{t e s t}(\\widehat{w}_{n}^{p r e d})\\simeq E_{t e s t}^{c l e a n}+n\\sigma_{0}^{2}\\rho,}}\\\\ {{\\rho=\\displaystyle\\frac{1}{T_{0}-d-1}\\mathbb{E}\\,\\mathrm{tr}\\,{\\Sigma^{-1}}\\widehat{\\Sigma}R{\\Sigma}\\widehat{\\Sigma}R.}}\\end{array}\\right\\}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "In the second part of the theorem, the term $E_{t e s t}^{c l e a n}$ (introduced earlier in (31)) corresponds to the usual test error when the downstream model is trained on real (not fake) data, for which well-known formulae exist in a variety of scenarios (see Proposition 4.2). ", "page_idx": 19}, {"type": "text", "text": "Remark F.2. We show in Theorem 4.3 that $\\widetilde{B i a s}\\,\\geq\\,B i a s+\\Delta B i a s$ , where $\\Delta B i a s\\,\\geq\\,0$ in the appropriate asymptotic limit, with equality $f T_{0}\\geq d+2$ (the under-parametrized regime). Thus, apart from the variance term, an over-parametrized $(T_{0}<d+2)$ synthetic data-generator harms the bias term of the test error of downstream models. In contrast, an under-parametrized synthetic data-generator $(T_{0}\\,\\geq\\,d+2,$ only harms the variance. The increase in bias suffered in the overparametrized regime is precisely quantified in Section 4.5, and shown to be an increasing function of the number of generations $n$ . ", "page_idx": 19}, {"type": "text", "text": "The test error decomposition in Theorem F.1 is thus of the promised form (1). This additional term means that there is competition between the usual test error $E_{t e s t}^{c l e a n}$ and the additional term induced by the fake labeling process. Understanding the interaction of these two terms is key to demystifying the origins of model collapse. ", "page_idx": 19}, {"type": "text", "text": "Low-Dimensional Limit. Observe that if $d$ is fixed and $T\\rightarrow\\infty$ , then the empirical covariance matrix $\\widehat{\\Sigma}$ converges ${\\mathfrak{t o}}^{2}$ its population version $\\Sigma$ , and so for $T_{0}\\geq d+2$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\rho\\simeq\\frac{\\mathrm{tr}\\,\\Sigma^{2}(\\Sigma+\\lambda I_{d})^{-2}}{T_{0}-d}=\\frac{\\mathrm{d}\\mathrm{f}_{2}(\\lambda)}{T_{0}-d},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where for any $\\lambda\\geq0$ and $m\\in\\mathbb{N}_{\\star},\\mathrm{df}_{m}(\\lambda)$ is the $m$ th order \"degree of freedom\" of the covariance matrix $\\Sigma$ is given by ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathrm{f}_{m}(\\lambda)=\\mathrm{d}\\mathrm{f}_{m}(\\lambda;\\Sigma):=\\mathrm{tr}\\,\\Sigma^{m}(\\Sigma+\\lambda I_{d})^{-m}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Note that $\\mathrm{df}_{m}(\\lambda)\\leq d$ always. In the high-dimensional setting (where $d$ can grow beyond $T_{0}$ ), the precise analysis of $\\rho$ will be carried out via random matrix theory (RMT). ", "page_idx": 19}, {"type": "text", "text": "F.2 Proof of Theorem 4.1 (Rigeless Regression) ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The proof is by induction on the number of generations $n$ of fake data. For $n=0$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{E_{t e s t}(\\widehat{w}_{0}^{p r e d})=\\mathbb{E}\\,\\|\\widehat{w}_{0}^{p r e d}-w_{0}\\|_{\\Sigma}^{2}=\\mathbb{E}\\,\\|\\widehat{w}_{0}^{p r e d}-\\widehat{w}_{0}\\|_{2}^{2}=\\mathbb{E}\\|(X_{0}^{\\top}X_{0})^{-1}X_{0}^{\\top}E_{0}\\|_{2}^{2}}\\\\ &{\\qquad\\qquad\\qquad=\\sigma^{2}\\mathbb{E}\\,\\mathrm{tr}(X_{0}^{\\top}X_{0})^{-1}=\\sigma^{2}\\frac{d}{T-d-1}\\simeq\\frac{\\sigma^{2}\\phi}{1-\\phi},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\phi:=d/T\\in(0,1)$ and the last step has made use of Lemma F.3 below. This is a well-known result for the test error of linear regression in the under-parametrized regime, without any AI pollution (fake $/$ synthesized training data). ", "page_idx": 19}, {"type": "text", "text": "Analogously, for $n=1$ one computes the test error after the first generation of fake data as follows ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{E_{t e s t}(\\hat{w}_{1}^{p r e d})=\\mathbb{E}\\|\\hat{w}_{1}^{p r e d}-w_{0}\\|_{\\Sigma}^{2}=\\mathbb{E}\\|\\hat{w}_{1}^{p r e d}-\\hat{w}_{0}\\|_{2}^{2}}&{}\\\\ {=\\mathbb{E}\\|\\hat{w}_{1}^{p r e d}-\\hat{w}_{1}+\\hat{w}_{1}-\\hat{w}_{0}\\|_{2}^{2}}&{}\\\\ {=\\mathbb{E}\\|(X_{1}^{\\top}X_{1})^{-1}X_{1}^{\\top}E_{1}+\\hat{w}_{0}^{p r e d}-\\hat{w}_{0}\\|_{2}^{2}}&{}\\\\ {=\\mathbb{E}\\|w_{0}-\\hat{w}_{0}^{p r e d}\\|_{2}^{2}+\\mathbb{E}\\|(X_{1}^{\\top}X_{1})^{-1}X_{1}^{\\top}E_{1}\\|_{2}^{2}}&{}\\\\ {=E_{t e s t}(\\hat{w}_{0}^{p r e d})+\\frac{\\sigma_{1}^{2}d}{T_{1}-d-1}}&{}\\\\ {=E_{t e s t}(\\hat{w}_{0}^{p r e d})+\\frac{\\sigma_{0}^{2}d}{T_{0}-d-1}}&{}\\\\ {\\simeq\\frac{\\sigma^{2}\\phi}{1-\\phi}+\\frac{\\sigma_{0}^{2}\\phi_{0}}{1-\\phi_{0}},}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\phi_{0}=d/T_{0}\\in(0,1)$ . Continuing the induction on $n$ , we obtain the result. ", "page_idx": 20}, {"type": "text", "text": "Lemma F.3. Let $X_{0}$ be an $T_{0}\\times d$ random matrix with iid rows from $N(0,\\Sigma)$ . I $f T_{0}\\geq d+2,$ , then the empirical covariance matrix $\\widehat{\\Sigma}_{0}:=X_{0}^{\\top}X_{0}/T_{0}$ is invertible a.s and ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\widehat{\\Sigma}_{0}^{-1}\\right]=\\frac{T_{0}}{T_{0}-d-1}\\Sigma^{-1}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "F.3 Proof of Theorem F.1 (Ridge Regression $^+$ General Covariance) ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "F.3.1 Representation of $\\widehat{w}_{n}$ and ${\\widehat{w}}_{n}^{p r e d}$ ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We first obtain explicit formulae for the labelling vectors $\\widehat{w}_{n}$ used in the fake-data generation process (5). For any integer $m\\geq0$ , define $P_{m}=X_{m}^{\\dagger}X_{m}^{\\bullet}$ , the orthogonal projection matrix onto the subspace of $\\mathbb{R}^{d}$ spanned by the rows of $X_{m}$ . Observe from (5) that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\widehat{w}_{n}=X_{n-1}^{\\dagger}\\overline{{Y}}_{n-1}=X_{n-1}^{\\dagger}(X_{n-1}\\widehat{w}_{n-1}+E_{n-1})=P_{n-1}\\widehat{w}_{n-1}+X_{n-1}^{\\dagger}E_{n-1}}\\\\ {\\ \\ \\ =P_{n-1}X_{n-2}^{\\dagger}(X_{n-2}\\widehat{w}_{n-2}+E_{n-2})+X_{n-1}^{\\dagger}E_{n-1}}\\\\ {\\ \\ \\ =P_{n-1}P_{n-2}\\widehat{w}_{n-2}+P_{n-1}X_{n-2}^{\\dagger}E_{n-2}+X_{n-1}^{\\dagger}E_{n-1}}\\\\ {\\ \\ \\ \\ \\vdots}\\\\ {\\ \\ \\ =P_{n-1}P_{n-2}\\ldots P_{0}w_{0}+P_{n-1}P_{n-2}\\ldots P_{1}X_{1}^{\\dagger}E_{1}+P_{n-1}P_{n-2}\\ldots P_{2}X_{2}^{\\dagger}E_{2}+\\ldots}\\\\ {\\ \\ \\ \\vdots}\\\\ {\\ \\ \\ =P_{n-1}P_{n-2}\\ldots P_{0}w_{0}+\\displaystyle\\sum_{m=0}^{n-1}P_{n-1}P_{n-2}\\ldots P_{m}X_{m}^{\\dagger}E_{m}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We get the following result. ", "page_idx": 20}, {"type": "text", "text": "Lemma F.4. For any $n\\geq0$ , the following formula holds ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{w}_{n}=\\left\\{\\!\\!\\begin{array}{l l}{w_{0},\\quad}&{i f n=0,}\\\\ {Q_{n-1}w_{0}+\\sum_{m=0}^{n-1}\\overline{{Q}}_{n-1,m}E_{m},\\quad i f n\\geq1,}\\end{array}\\!\\!\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\overline{{Q}}_{k,m}:=Q_{k,m}X_{m}^{\\dag}$ , $Q_{k,m}:=P_{k}P_{k-1}\\dots P_{m}$ and $Q_{k}:=Q_{k,0}=P_{k}P_{k-1}\\dots P_{0}$ . Moreover, $\\widehat{w}_{n}\\in\\mathrm{Im}\\,P_{n-1}$ as soon as $n\\geq1$ . ", "page_idx": 20}, {"type": "text", "text": "In particular, under the simplifying condition (17), it holds that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{w}_{n}=\\left\\{w_{0},\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ i f n=0,\\right.}\\\\ {P_{0}w_{0}+X_{0}^{\\dagger}\\overline{{E}}_{n-1}\\in\\mathrm{Im}\\,P_{0},\\ \\ \\ i f n\\geq1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\textstyle{\\overline{{E}}}_{n-1}:=\\sum_{m=0}^{n-1}E_{m}$ , a random vector of length $T_{0}$ , with iid entries from $N(0,n\\sigma_{0}^{2})$ , and independent of $X_{0}$ . Moreover, ${\\widehat{w}}_{n}\\in\\operatorname{Im}P_{0}$ as soon as $n\\geq1$ . ", "page_idx": 20}, {"type": "text", "text": "Note that the second part of the result uses the elementary linear-algebraic fact that $P_{m}X_{m}^{\\dagger}=X_{m}^{\\dagger}$ In the special case where $T_{0}\\geq d$ , we have $P_{0}=I$ a.s., and so $\\widehat{w}_{n}=w_{0}+X_{0}^{\\dagger}\\overline{{E}}_{n-1}$ . Otherwise, even in the absence of generator noise $(\\sigma_{0}=0)$ ), the fake data labeller $\\widehat{w}_{n}=P_{0}w_{0}$ drifts away from the truth $w_{0}$ , into a subspace of $\\mathbb{R}^{d}$ spanned by the rows of $X_{0}$ . ", "page_idx": 21}, {"type": "text", "text": "Next, let us obtain a decomposition for the downstream predictor ${\\widehat{w}}_{n}^{p r e d}$ defined in (7). As usual, let $\\widehat{\\Sigma}:=X^{\\top}X/T$ be the empirical covariance matrix with resolvent $R=(\\widehat\\Sigma+\\lambda I)^{-1}$ , and observe that the downstream model writes ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{w}_{n}^{p r e d}=R X^{\\top}\\overline{{Y}}_{n}(X)/T=R X^{\\top}(X\\widehat{w}_{n}+E)/T}\\\\ &{\\qquad=R X^{\\top}(X Q_{n-1}w_{0}+X\\displaystyle\\sum_{m=0}^{n-1}\\overline{{Q}}_{n-1,m}E_{m}+E)/T}\\\\ &{\\qquad=R\\widehat{\\Sigma}Q_{n-1}w_{0}+R X^{\\top}E/T+R\\widehat{\\Sigma}\\displaystyle\\sum_{m=0}^{n-1}\\overline{{Q}}_{n-1,m}E_{m}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "F.3.2 Proof of Theorem F.1 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Using the decomposition (38) for the downstream model ${\\widehat{w}}_{n}^{p r e d}$ , we deduce that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{E_{t e s t}(\\hat{w}_{n}^{p r e d})=\\mathbb{E}\\|\\hat{w}_{n}^{p r e d}-w_{0}\\|_{\\Sigma}^{2}}\\\\ &{\\qquad\\qquad\\qquad=\\mathbb{E}\\|R\\hat{\\Sigma}P_{0}w_{0}+R X^{\\top}E/T+R\\hat{\\Sigma}\\displaystyle\\sum_{m=0}^{n-1}\\overline{{Q}}_{n-1,m}E_{m}-w_{0}\\|_{\\Sigma}^{2}}\\\\ &{\\qquad\\qquad\\qquad=\\mathbb{E}\\|R\\hat{\\Sigma}P_{0}w_{0}-w_{0}+R X^{\\top}E/T+R\\hat{\\Sigma}\\displaystyle\\sum_{m=0}^{n-1}\\overline{{Q}}_{n-1,m}E_{m}\\|_{\\Sigma}^{2}}\\\\ &{\\qquad\\qquad\\qquad=\\mathbb{E}\\|R\\hat{\\Sigma}P_{0}w_{0}-w_{0}\\|_{\\Sigma}^{2}+\\mathbb{E}\\|R X^{\\top}E/T\\|_{\\Sigma}^{2}+\\mathbb{E}\\left\\|R\\hat{\\Sigma}\\displaystyle\\sum_{m=0}^{n-1}\\overline{{Q}}_{n-1,m}E_{m}\\right\\|_{\\Sigma}^{2}}\\\\ &{\\qquad\\qquad=\\widetilde{B}\\widetilde{i a s}+V a r+n\\sigma_{0}^{2}\\rho,}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\widehat{\\Sigma}:=X^{\\top}X/T$ , $\\widetilde{B i a s}$ , $V a r$ , and $\\rho$ are as given in the theorem. On the second line, we have used independence (of $X$ $\\operatorname{\\check{C}},\\,X_{0},\\,E$ , and ${\\overline{{E}}}_{n-1}\\rangle$ ) and the fact that $E$ and ${\\overline{{E}}}_{n-1}$ are centered Gaussian random vectors, with iid components of variances $\\sigma^{2}$ and $n\\sigma_{0}^{2}$ respectively. \u53e3 ", "page_idx": 21}, {"type": "text", "text": "F.4 Proof of Theorem 4.3 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Analysis of Bias-like Term. An exact analysis of the Bi as term appearing in Theorems F.1 and 4.3 is presumably a treacherous enterprise given dependency on $X$ (via $R$ and $\\widehat{\\Sigma}^{\\prime}$ ) and $X_{0}$ (via $P_{0.}$ ). In place of such an analysis, we shall settle for the following result which gives an instructive lower-bound. ", "page_idx": 21}, {"type": "text", "text": "Proposition F.5. In the RMT limit (12), it holds that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{lim}\\widetilde{B i a s}-\\operatorname*{lim}B i a s\\geq\\operatorname*{lim}\\mathbb{E}\\left\\|R\\widehat{\\Sigma}P_{0}w_{0}-R\\widehat{\\Sigma}w_{0}\\right\\|_{\\Sigma}^{2}\\geq0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Thus, training on fake / synthesized data increases the bias term of the downstream model\u2019s test error! ", "page_idx": 21}, {"type": "text", "text": "Proof. Letting A := R\u03a3, one computes ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{B i a s}-B i a s=\\Vert A P_{0}w_{0}-w_{0}\\Vert_{\\Sigma}^{2}-\\Vert A w_{0}-w_{0}\\Vert_{\\Sigma}^{2}}\\\\ &{\\qquad\\qquad=\\Vert A P_{0}w_{0}-A w_{0}+A w-w\\Vert_{\\Sigma}^{2}-\\Vert A w_{0}-w_{0}\\Vert_{\\Sigma}^{2}}\\\\ &{\\qquad\\quad=\\Vert A P_{0}w_{0}-A w_{0}\\Vert_{\\Sigma}^{2}+2w_{0}^{\\top}(A-P_{0}A)\\Sigma(I-A)w_{0}}\\\\ &{\\qquad\\quad=\\Vert A P_{0}w_{0}-A w_{0}\\Vert_{\\Sigma}^{2}+2w_{0}^{\\top}(I-P_{0})A\\Sigma(I-A)w_{0}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "It then suffices to observe that, in the RMT limit (12), it holds that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{lim}\\mathbb{E}w_{0}^{\\top}(I-P_{0})A\\Sigma(I-A)w_{0}\\geq0,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "as can be seen from repeated application of Propositions 1 and 2 of Bach [4]. ", "page_idx": 21}, {"type": "text", "text": "Analysis of $\\rho$ Term. Define a $d\\times d$ random psd matrix $\\boldsymbol{H}:=\\widehat{\\Sigma}\\boldsymbol{R}\\boldsymbol{\\Sigma}\\widehat{\\boldsymbol{\\Sigma}}\\boldsymbol{R}$ . Under the simplifying assumption (17), the matrices $\\overline{{Q}}_{k,m}$ defined in the theorem all equal $\\overline{{Q}}_{0,0}=X_{0}^{\\dagger}$ . It follows that the $\\rho$ -term in (32) then writes ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\rho=\\frac{1}{n}\\sum_{m=0}^{n-1}\\mathbb{E}\\left[\\overline{{Q}}_{n-1,m}\\overline{{Q}}_{n-1,m}^{\\top}H\\right]=\\mathbb{E}\\left[\\mathrm{tr}\\,X_{0}^{\\dagger}(X_{0}^{\\dagger})^{\\top}H\\right]=\\mathbb{E}_{H}\\mathbb{E}\\left[\\mathrm{tr}\\,X_{0}^{\\dagger}(X_{0}^{\\dagger})^{\\top}H\\mid H\\right].\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Now, one computes the conditional expectation as follows ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\mathrm{tr}\\,X_{0}^{\\dagger}(X_{0}^{\\dagger})^{\\top}H\\mid H\\right]=\\mathbb{E}\\left[\\mathrm{tr}\\,X_{0}^{\\top}(X_{0}X_{0}^{\\top})^{-2}X_{0}H\\mid H\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\quad=\\underset{\\lambda_{0}\\to0^{+}}{\\operatorname*{lim}}\\frac{1}{T_{0}}\\frac{\\partial}{\\partial\\lambda_{0}}\\mathbb{E}\\left[\\mathrm{tr}\\,X_{0}^{\\top}(X_{0}X_{0}^{\\top}+\\lambda_{0}T_{0}I)^{-1}X_{0}H\\mid H\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Furthermore, defining $A:=\\Sigma^{1/2}H\\Sigma^{1/2}$ and $Z_{0}=X_{0}\\Sigma^{-1/2}$ , we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname{tr}X_{0}^{\\top}(X_{0}X_{0}^{\\top}+\\lambda_{0}T_{0}I)^{-1}X_{0}H=\\operatorname{tr}\\Sigma^{1/2}Z_{0}^{\\top}(Z_{0}\\Sigma Z_{0}^{\\top}+\\lambda_{0}T_{0}I)^{-1}Z_{0}\\Sigma^{1/2}H}\\\\ {=\\operatorname{tr}A Z_{0}^{\\top}(Z_{0}\\Sigma Z_{0}^{\\top}+\\lambda_{0}T_{0}I)^{-1}Z_{0},\\qquad\\qquad\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We deduce from Proposition 2 of Bach [4] that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathbb{E}\\left[\\mathrm{tr}\\,X_{0}^{\\top}(X_{0}X_{0}^{\\top}+\\lambda_{0}T_{0}I)^{-1}X_{0}H\\mid H\\right]\\simeq\\mathrm{tr}\\,A(\\Sigma+\\kappa(\\lambda_{0},T_{0})I)^{-1}}&{}&\\\\ {=\\mathrm{tr}\\,H(\\Sigma+\\kappa(\\lambda_{0},T_{0})I)^{-1}\\Sigma.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Differentiating w.r.t. $\\lambda_{0}$ and letting this parameter tend to zero from above gives ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\mathrm{tr}\\,X_{0}^{\\dagger}(X_{0}^{\\dagger})^{\\top}H\\mid H\\right]=-\\frac{1}{T_{0}}\\operatorname*{lim}_{\\lambda_{0}\\to0^{+}}\\frac{\\partial}{\\partial\\lambda_{0}}\\mathbb{E}\\left[\\mathrm{tr}\\,X_{0}^{\\top}(X_{0}X_{0}^{\\top}+\\lambda_{0}T_{0}I)^{-1}X_{0}H\\mid H\\right]}\\\\ &{~~~~~~~~~~~~~~~~~~~~~~~\\simeq-\\frac{1}{T_{0}}\\operatorname*{lim}_{\\lambda_{0}\\to0^{+}}\\frac{\\partial\\kappa(\\lambda_{0},T_{0})}{\\partial\\lambda_{0}}\\cdot\\frac{\\partial}{\\partial t}\\,\\mathrm{tr}\\,H(\\Sigma+t I)^{-1}\\Sigma\\Bigg|_{t=\\kappa(\\lambda_{0},T_{0})}}\\\\ &{~~~~~~~~~~~~~~~~~~~~\\simeq\\frac{\\mathrm{tr}\\,H(\\Sigma+\\kappa_{0}I)^{-2}\\Sigma}{T_{0}-\\mathrm{df}_{2}(\\kappa_{0})},}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\kappa_{0}=\\kappa(0,T_{0})$ , and we have made use of Lemma I.2. Combining with (40) and then applying Proposition 1 of Bach [4] to compute $\\mathbb{E}_{H}\\,\\operatorname{tr}H(\\Sigma+\\kappa_{0}I)^{-2}\\Sigma=\\mathbb{E}_{X}\\,\\operatorname{tr}{\\widehat{\\Sigma}}R\\Sigma\\widehat{\\Sigma}R(\\Sigma+\\kappa_{0}I)^{-2}\\Sigma$ gives the following result. ", "page_idx": 22}, {"type": "text", "text": "Proposition F.6. In the RMT limit (12), it holds for any $\\lambda>0$ that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\rho=\\frac{\\mathrm{tr}\\,\\Sigma^{4}(\\Sigma+\\kappa_{0}I)^{-2}(\\Sigma+\\kappa I)^{-2}}{T_{0}-\\mathrm{df}_{2}(\\kappa_{0})}+\\frac{\\kappa^{2}\\,\\mathrm{tr}\\,\\Sigma^{2}(\\Sigma+\\kappa_{0}I)^{-2}(\\Sigma+\\kappa I)^{-2}}{T_{0}-\\mathrm{df}_{2}(\\kappa_{0})}\\cdot\\frac{\\mathrm{d}\\mathrm{f}_{2}(\\kappa)}{T-\\mathrm{df}_{2}(\\kappa)},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\kappa_{0}:=\\kappa(\\lambda_{0},T_{0})$ and $\\kappa=\\kappa(\\lambda,T)$ . ", "page_idx": 22}, {"type": "text", "text": "In particular, i ${}^{*}T_{0}\\geq d,$ , then ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\rho\\simeq{\\frac{\\mathrm{d}\\mathrm{f}_{2}(\\kappa)}{T-\\mathrm{d}\\mathrm{f}_{2}(\\kappa)}}\\left(1+{\\frac{\\kappa^{2}\\,\\mathrm{tr}(\\Sigma+\\kappa I)^{-2}}{T_{0}-\\mathrm{d}\\mathrm{f}_{2}(\\kappa_{0})}}\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "This result completes the proof of Theorem 4.3. ", "page_idx": 22}, {"type": "text", "text": "F.5 Proof of Corollary 4.4 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "For the first part, we know from Theorem F.1 that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{E_{t e s t}(\\widehat{w}_{n}^{p r e d})=E_{t e s t}(\\widehat{w}_{0}^{p r e d})+n\\sigma_{0}^{2}\\rho,\\ \\mathrm{with}}\\\\ &{\\quad\\rho:=\\frac{\\mathbb{E}\\ \\mathrm{tr}\\,\\Sigma^{-1}\\widehat{\\Sigma}(\\widehat{\\Sigma}+\\lambda I)^{-1}\\Sigma(\\widehat{\\Sigma}+\\lambda I)^{-1}\\widehat{\\Sigma}}{T_{0}-d}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The $E_{t e s t}(\\widehat{w}_{0}^{p r e d})$ term is taken care of by Proposition 4.2, since this corresponds to generalization error on cl e an training data. For the $\\rho$ term, we use Proposition 1 of Bach [4] with $\\bar{A}=\\Sigma^{-1}$ and $B=\\Sigma$ to get ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\rho\\simeq\\frac{\\mathrm{tr}(\\Sigma+\\kappa I)^{-2}\\Sigma^{2}}{T_{0}-d}+\\frac{\\kappa^{2}\\,\\mathrm{tr}(\\Sigma+\\kappa I)^{-2})}{T_{0}-d}\\frac{\\mathrm{tr}(\\Sigma+\\kappa I)^{-2}\\Sigma^{2}}{I-\\mathrm{df}_{2}(\\kappa)}}\\\\ {\\displaystyle\\quad=\\frac{\\mathrm{df}_{2}(\\kappa)}{T_{0}-d}+\\frac{\\kappa^{2}\\,\\mathrm{tr}(\\Sigma+\\kappa I)^{-2}}{T_{0}-d}\\frac{\\mathrm{df}_{2}(\\kappa)}{T-\\mathrm{df}_{2}(\\kappa)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "which proves the first part of the result. ", "page_idx": 23}, {"type": "text", "text": "For the second part, note that $\\mathrm{df}_{2}(\\kappa)=d/(1+\\kappa)^{2}$ when $\\Sigma=I$ , (10) holds, and so ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{(1-1/\\phi_{0})\\rho\\simeq\\frac{1}{(1+\\kappa)^{2}}+\\frac{\\kappa^{2}}{(1+\\kappa)^{4}}\\frac{d}{T-d/(1+\\kappa)^{2}}}}\\\\ &{\\simeq\\frac{1}{(1+\\kappa)^{2}}+\\frac{\\kappa^{2}}{(1+\\kappa)^{4}}\\frac{\\phi}{1-\\phi/(1+\\kappa)^{2}}}\\\\ &{=\\frac{1}{(1+\\kappa)^{2}}+\\frac{1}{(1+\\kappa)^{2}}\\frac{\\phi\\kappa^{2}}{(1+\\kappa)^{2}-\\phi},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and the result follows. ", "page_idx": 23}, {"type": "text", "text": "F.6 A Note on Proposition 4.2 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "As mentioned in the main text, the result is classical Richards et al. [41], Hastie et al. [22], Bach [4]). Only the second part needs a comment which we now provide. Indeed, the second part of the result follows from the first as we now see. Indeed, $w_{0}^{\\top}\\Sigma(\\Sigma+\\kappa I)^{-2}w_{0}\\,=\\,r^{2}/(1+\\stackrel{\\cdot}{\\kappa})^{2}$ , $\\mathrm{df}_{2}(\\kappa)=d/(1+\\kappa)^{2}$ and so we deduce from the first part that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle V a r\\simeq\\sigma^{2}\\phi\\frac{1}{(1+\\kappa)^{2}}\\frac{1}{1-\\phi/(1+\\kappa)^{2}}=\\frac{\\sigma^{2}\\phi}{(1+\\kappa)^{2}-\\phi}},}\\\\ {{\\displaystyle B i a s\\simeq\\kappa^{2}\\|w_{0}\\|_{2}^{2}\\frac{1}{(1+\\kappa)^{2}}\\frac{1}{1-\\phi/(1+\\kappa)^{2}}=\\frac{\\kappa^{2}\\|w_{0}\\|_{2}^{2}}{(1+\\kappa)^{2}-\\phi},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "from which the result follows. ", "page_idx": 23}, {"type": "text", "text": "We now need to estimate $\\delta^{\\top}H\\delta$ for a deterministic psd matrix $H$ . Observe that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\delta^{\\top}H\\delta=(Q_{n-1}w_{0}-w_{0})^{\\top}H(Q_{n-1}w_{0}-w_{0})}\\\\ &{\\qquad\\qquad=w_{0}^{\\top}Q_{n-1}^{\\top}H Q_{n-1}w_{0}-2w_{0}^{\\top}Q_{n-1}^{\\top}H w_{0}+w_{0}^{\\top}H w_{0}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "F.7 Proof of Theorem 4.5 and Theorem 4.6 (Model Collapse in the Absence of Label Noise) ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We first prove Theorem 4.6. Note that since we are in the isotropic case, \u2206Bias defined in (15) is now given by $\\Delta B i a s:=\\mathbb{E}\\,\\|\\widehat{\\Sigma}R(Q_{n-1}w_{0}-w_{0})\\|^{2}$ , where $Q_{n-1}:=P_{n-1}P_{n-1}\\,.\\,.\\,.\\,P_{0}$ . Moreover, since $T>d$ and $\\lambda=0$ by assumption, we have $\\Sigma R\\,=\\,I_{d}$ , and so we further have $\\Delta B i a s:=$ $\\mathbb{E}\\,\\|Q_{n-1}w_{0}-w_{0}\\|^{2}$ . Now, one computes ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|Q_{n-1}w_{0}-w_{0}\\|^{2}=\\|w_{0}\\|^{2}-2w_{0}^{\\top}Q_{n-1}w_{0}+w_{0}^{\\top}Q_{n-1}^{\\top}Q_{n-1}w_{0}}\\\\ &{\\qquad\\qquad\\qquad\\quad=\\|w_{0}\\|^{2}-w_{0}^{\\top}Q_{n-1}w_{0}}\\\\ &{\\qquad\\qquad\\quad\\quad\\simeq\\|w_{0}\\|^{2}-w_{0}^{\\top}\\left(\\displaystyle\\prod_{m=0}^{n-1}(I+\\kappa_{m}I)^{-1}\\right)w_{0}}\\\\ &{\\qquad\\qquad\\qquad\\quad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\|w_{0}\\|^{2}-\\|w_{0}\\|^{2}\\displaystyle\\prod_{m=0}^{n-1}\\operatorname*{min}(1/\\phi_{m},1),}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where on the 2nd line we have used the fact that $Q_{n-1}^{\\top}Q_{n-1}\\ =\\ Q_{n-1}$ because the $P_{m}$ \u2019s are projections; on the 3rd line we have used Lemma F.7 with $\\Sigma=I$ and $u=v=w_{0}$ ; on the 4th line we have used the fact that $\\kappa_{m}:=\\kappa(0,T_{m})=\\operatorname*{max}(\\phi_{m}-1,0)=\\operatorname*{max}(\\phi_{m},1)-1$ . This completes the proof of Theorem 4.6. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "The proof of Theorem 4.5 is completely analogous, with $Q_{n-1}$ replaced with $Q_{0}$ . ", "page_idx": 24}, {"type": "text", "text": "Lemma F.7. Let $X_{0},\\ldots,X_{n-1}$ be independent random matrices of shapes $T_{m}\\,\\times\\,d$ for $m=$ $0,\\ldots,n-1$ , with rows iid from $N(0,\\Sigma)$ , and let $Q_{n-1}:=P_{n-1}P_{n-2}\\dots P_{0}$ , where $P_{m}=X_{m}^{\\dagger}X_{m}$ is the orthogonal projection onto the subspace of $\\mathbb{R}^{d}$ spanned by the rows of $X_{m}$ . Then, in the limit $d,T_{0},\\dots,T_{n-1}\\to\\infty$ such that $d/T_{0}\\,\\to\\,\\phi_{0}\\,\\in\\,(0,\\infty),\\ldots,d/T_{n-1}\\,\\to\\,\\phi_{n-1}\\,\\in\\,(0,\\infty)$ with $\\|\\Sigma\\|_{o p},\\|\\Sigma^{-1}\\|_{o p}=O(1)$ , it holds that for deterministic $L_{2}$ -bounded sequences of vectors $u$ and $v$ ", "page_idx": 24}, {"type": "equation", "text": "$$\nu^{\\top}Q_{n-1}v\\simeq u^{\\top}\\left(\\prod_{m=0}^{n-1}(\\Sigma+\\kappa_{m}I)^{-1}\\right)v,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\kappa_{m}=\\kappa(0,T_{m})$ is as defined in (9). ", "page_idx": 24}, {"type": "text", "text": "Proof. The prof is by induction on $n\\geq1$ . For $n=1$ , we have $Q_{n-1}=Q_{0}=P_{0}$ . Thus, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\boldsymbol{u}^{\\top}\\boldsymbol{Q}_{0}\\boldsymbol{v}=\\boldsymbol{u}^{\\top}\\boldsymbol{P}_{0}\\boldsymbol{v}=\\operatorname*{lim}_{t\\rightarrow0^{+}}\\boldsymbol{u}^{\\top}\\boldsymbol{X}_{0}^{\\top}(\\boldsymbol{X}_{0}\\boldsymbol{X}_{0}^{\\top}+t\\boldsymbol{I})^{-1}\\boldsymbol{X}_{0}\\boldsymbol{v}}\\\\ &{\\qquad\\quad\\simeq\\operatorname*{lim}_{t\\rightarrow0^{+}}\\boldsymbol{u}^{\\top}(\\Sigma+\\kappa(t,T)\\boldsymbol{I})^{-1}\\boldsymbol{v}=\\boldsymbol{u}^{\\top}(\\Sigma+\\kappa_{0}\\boldsymbol{I})^{-1}\\boldsymbol{v},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\kappa_{0}:=\\kappa(0,T)$ and we used Proposition 2 of Bach [4] at the beginning of the 2nd line. Now, suppose the claim holds for $n$ , and let\u2019s prove that it holds for $n+1$ . Indeed, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\boldsymbol{u}^{\\top}\\boldsymbol{Q}_{n}\\boldsymbol{v}=\\boldsymbol{u}^{\\top}P_{n}\\boldsymbol{Q}_{n-1}\\boldsymbol{v}\\simeq\\boldsymbol{u}^{\\top}P_{n-1}\\prod_{m=0}^{n-1}(\\Sigma+\\kappa_{m})^{-1}\\boldsymbol{v}\\simeq\\boldsymbol{u}^{\\top}\\prod_{m=0}^{n}(\\Sigma+\\kappa_{m})^{-1}\\boldsymbol{v},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the second step is an application of the induction hypothesis with $P_{n}u$ in place of $u$ . ", "page_idx": 24}, {"type": "text", "text": "The following lemma can be used to compute $\\|Q_{n-1}w_{0}-w_{0}\\|_{\\Sigma}^{2}$ in the case of anisotropic $\\Sigma$ . Lemma F.8. Under the hypothesis of Lemma $F.7_{s}$ , it holds that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle{u^{\\top}Q_{n-1}v\\simeq u^{\\top}\\Sigma^{n}\\left(\\prod_{m=0}^{n-1}(\\Sigma+\\kappa_{m}I)^{-1}\\right)v,}}}\\\\ {{\\mathrm{}}}\\\\ {{\\displaystyle{u^{\\top}Q_{n-1}^{\\top}\\Sigma Q_{n-1}v\\simeq u^{\\top}\\Sigma^{n}\\left(\\prod_{m=0}^{n-1}A_{m}\\right)v,}}}\\\\ {{\\displaystyle{w i t h\\;A_{m}:=(\\Sigma+\\kappa_{m}I)^{-2}\\left(\\Sigma^{2}+\\frac{\\kappa_{m}^{2}\\mathrm{df}_{2}\\left(\\kappa_{m}\\right)}{T-\\mathrm{df}_{2}\\left(\\kappa_{m}\\right)}I\\right),}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\kappa_{m}:=\\kappa(0,T_{m})$ as defined in (9) ", "page_idx": 24}, {"type": "text", "text": "Proof. The first formula follows directly from Lemma F.7 with $u$ replaced with $\\Sigma u$ . For the second formula, we can write ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{u^{\\top}Q_{n-1}^{\\top}M Q_{n-1}v=u^{\\top}P_{0}P_{1}\\ldots P_{n-2}P_{n-1}M P_{n-1}P_{n-2}\\ldots.\\ldots P_{0}P_{1}v}}\\\\ {{=\\widetilde{u}_{n-1}^{\\top}P_{n-1}M P_{n-1}\\widetilde{v}_{n-1},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\widetilde{u}_{n-1}:=P_{n-2}\\dots{}P_{0}u$ . So we really only need to prove the result for $n\\,=\\,1$ ; the general case will follow by induction and due to multiplicativity. Indeed, defining $A\\,=\\,\\Sigma^{1/2}u v^{\\top}\\Sigma^{1/2}$ , $B=\\Sigma^{1/2}M\\Sigma^{1/2}$ , and $Z_{0}=X_{0}\\Sigma^{-1/2}$ , we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\iota^{\\top}P_{0}M P_{0}v=\\underset{t\\to0^{+}}{\\operatorname*{lim}}u^{\\top}X_{0}^{\\top}(X_{0}X_{0}^{\\top}+t I)^{-1}X_{0}M X_{0}^{\\top}(X_{0}X_{0}^{\\top}+t I)^{-1}X_{0}v}\\\\ &{\\qquad\\qquad=\\underset{t\\to0^{+}}{\\operatorname*{lim}}\\,\\mathrm{tr}\\,A Z_{0}(Z_{0}\\Sigma Z_{0}^{\\top}+t I)^{-1}Z_{0}B Z_{0}^{\\top}(Z_{0}Z_{0}^{\\top}+t I)^{-1}Z_{0}}\\\\ &{\\qquad\\qquad\\simeq\\mathrm{tr}\\,A(\\Sigma+\\kappa_{0}I)^{-1}B(\\Sigma+\\kappa_{0}I)^{-1}+\\kappa_{0}^{2}\\operatorname{tr}A(\\Sigma+\\kappa_{0}I)^{-2}\\cdot\\frac{\\mathrm{tr}\\,B(\\Sigma+\\kappa_{0}I)^{-2}}{T-\\mathrm{df}_{2}(\\kappa_{0})}}\\\\ &{\\qquad\\qquad=u^{\\top}(\\Sigma+\\kappa_{0}I)^{-1}\\Sigma M\\Sigma(\\Sigma+\\kappa_{0}I)^{-1}v+\\kappa_{0}^{2}u^{\\top}\\Sigma(\\Sigma+\\kappa_{0}I)^{-2}v\\cdot\\frac{\\mathrm{tr}\\,M\\Sigma(\\Sigma+\\kappa_{0})^{-2}}{T-\\mathrm{df}_{2}(\\kappa_{0})}}\\\\ &{\\qquad\\qquad=u^{\\top}\\Sigma A_{0}v,\\,\\,\\mathrm{for}\\,M=\\Sigma,}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the 3rd line is an application of Proposition 2 of Bach [4]. ", "page_idx": 24}, {"type": "text", "text": "G The Noisy Regime for Power Law Spectra ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Here we discuss the consequences of Theorem 5.1 for the noisy regime. ", "page_idx": 25}, {"type": "text", "text": "Now fix $\\sigma^{2}~\\ne~0$ and $\\phi_{0}~\\in~(0,1)$ . In this regime, Theorem 5.1 predicts that consistency (i.e. $E_{t e s t}(\\widehat{w}_{n}^{p r e d})\\overset{T\\rightarrow\\infty}{\\longrightarrow}0)$ is only possible if $\\ell\\leq\\ell_{\\star}$ . First consider values of $\\ell$ for which the clean varian c e $\\sigma^{2}T^{-(1-\\ell/\\beta)}$ is less than the clean bias $T^{-2\\underline{{r}}\\ell}$ in (27) i.e. $0\\le\\ell\\le\\ell_{c r i t}$ . We get ", "page_idx": 25}, {"type": "equation", "text": "$$\nE_{t e s t}(\\widehat{w}_{n}^{p r e d})\\asymp T^{-2\\ell\\underline{{r}}}+T^{-(b-a-\\ell/\\beta)},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "which is minimized by taking $\\ell=\\operatorname*{min}(\\ell_{\\star},\\ell_{c r i t})$ . For other values of $\\ell$ , variance dominates, giving ", "page_idx": 25}, {"type": "equation", "text": "$$\nE_{t e s t}(\\hat{w}_{n}^{p r e d})\\asymp T^{-(1-\\ell/\\beta)}+T^{-(b-\\ell/\\beta-a)}\\asymp T^{-(\\gamma-\\ell/\\beta)},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $\\gamma:=\\operatorname*{min}(1,b-a)$ . This is minimized by taking $\\ell=\\ell_{c r i t}$ , leading to ", "page_idx": 25}, {"type": "equation", "text": "$$\nE_{t e s t}(\\widehat{w}_{n}^{p r e d})\\asymp T^{-(\\gamma-1/(2\\beta\\underline{{r}}+1))}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "This tends to zero with $T\\rightarrow\\infty$ only if $b>a+1/(2\\beta\\underline{{r}}+1)$ . ", "page_idx": 25}, {"type": "text", "text": "H Proof of Results for Power-Law Covariance Spectrum ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "H.1 Proof of Theorem 5.1 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "From Theorem F.1, we need to analyze the quantity ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\rho\\simeq\\frac{\\mathrm{d}\\mathrm{f}_{2}(\\kappa(\\lambda))}{T_{0}-d}+\\frac{\\kappa(\\lambda)^{2}\\operatorname{tr}\\left(\\Sigma+\\kappa(\\lambda)I_{d}\\right)^{-2}}{T_{0}-d}\\cdot\\frac{\\mathrm{d}\\mathrm{f}_{2}(\\kappa(\\lambda))}{T-\\mathrm{d}\\mathrm{f}_{2}(\\kappa(\\lambda))}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Now, for small $\\lambda$ , $,\\kappa:=\\kappa(\\lambda)$ is small and one can compute ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathrm{f}_{m}(\\kappa)\\asymp\\sum_{i}\\frac{\\lambda_{i}^{m}}{(\\lambda_{i}+\\kappa)^{m}}=\\kappa^{-m}\\sum_{i}\\frac{\\lambda_{i}^{m}}{(1+\\kappa^{-1}\\lambda_{i})^{m}}\\asymp\\kappa^{-m}\\kappa^{(m-1/\\beta)}=\\kappa^{-1/\\beta},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where we have used Lemma I.1 with $D=\\kappa^{-1}$ and $n=m$ in the last step. On the other hand, we can use some of the results of Appendix A (Section 3) of [14] to do the following. It can be shown (see aforementioned paper) that ", "page_idx": 25}, {"type": "text", "text": "For $\\ell<\\beta$ , plugging this into (52) gives ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\rho\\asymp\\displaystyle\\frac{T^{\\ell/\\beta}}{T_{0}-d}+\\frac{d}{T_{0}-d}\\frac{T^{\\ell/\\beta}}{T-T^{\\ell/\\beta}}\\asymp T_{0}^{-1}T^{\\ell/\\beta}+\\frac{\\phi_{0}}{1-\\phi_{0}}T^{-(1-\\ell/\\beta)}}\\\\ &{\\quad\\asymp\\displaystyle\\frac{1}{1-\\phi_{0}}\\operatorname*{max}\\left(T/T_{0},\\phi_{0}\\right)T^{-(1-\\ell/\\beta)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $\\phi_{0}:=d/T_{0}$ . Combining our Theorem F.1 with (57), we get the claimed result. ", "page_idx": 25}, {"type": "text", "text": "H.2 Representation of Clean Test Error ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We make a small digression to present the following curiosity: with a slight leap of faith, the main results of [14] can be obtained in a few lines from the tools developed in [4], namely Proposition 4.2. This is significant, because the computations in [14] were done via methods of statistical physics (replica trick), while [4] is based on RMT. ", "page_idx": 25}, {"type": "text", "text": "Indeed, for regularization parameter $\\lambda\\asymp T^{-\\ell}$ given in (25), we have $\\kappa=\\kappa(\\lambda)\\simeq\\lambda$ . Thus ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\kappa\\asymp T^{-\\ell},\\,\\mathrm{df}_{2}(\\kappa)\\asymp\\kappa^{-1/\\beta}\\asymp T^{\\ell/\\beta}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Now, since $\\lambda_{i}\\asymp i^{-\\beta}$ (capacity condition) and $(w_{0}^{\\top}v_{i})^{2}=c_{i}^{2}\\asymp i^{-\\delta}$ (source condition), we deduce ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\kappa^{2}w_{0}^{\\top}\\Sigma(\\Sigma+\\kappa I)^{-2}w_{0}\\asymp w_{0}^{\\top}\\left(\\sum_{i}\\frac{\\lambda_{i}}{(\\lambda_{i}+\\kappa^{-1}\\lambda_{i})^{2}}v_{i}v_{i}^{\\top}\\right)w_{0}=\\sum_{i}\\frac{c_{i}^{2}\\lambda_{i}}{(\\lambda_{i}+\\kappa^{-1}\\lambda_{i})^{2}}\\qquad\\qquad}\\\\ {=\\displaystyle\\sum_{i}\\frac{c_{i}^{2}\\lambda_{i}}{(\\lambda_{i}+\\kappa^{-1}\\lambda_{i})^{2}}\\asymp\\displaystyle\\sum_{i}\\frac{\\lambda_{i}^{1+\\delta/\\beta}}{(\\lambda_{i}+\\kappa^{-1}\\lambda_{i})^{2}}\\asymp\\kappa^{-\\gamma}\\asymp T^{-\\ell\\gamma},}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $\\gamma=\\operatorname*{min}(2,1+\\delta/\\beta-1/\\beta)=\\operatorname*{min}(2,2r)=2\\underline{{r}}$ , with $\\underline{{r}}:=\\operatorname*{min}(r,1)$ . The exponent is so because $\\delta=1+\\beta(2r-1)$ , and so $\\delta/\\beta=1/\\beta+2r-1$ by construction. The estimation of the last sum in (54) is thanks to Lemma I.1 applied with $D=\\kappa^{-\\bar{1}}$ , $n=1+\\delta/\\beta$ , and $m=2$ . Therefore, invoking Proposition 4.2 gives ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle B i a s\\simeq\\frac{\\kappa^{2}w_{0}^{\\top}\\Sigma(\\Sigma+\\kappa)^{-2}w_{0}}{1-\\mathrm{df}_{2}(\\kappa)/T}\\asymp\\frac{T^{-\\ell\\gamma}}{1-T^{-(1-\\ell/\\beta)}}\\asymp T^{-\\ell\\gamma}=T^{-2\\ell\\underline{{r}}}}}\\\\ {{\\displaystyle V a r\\simeq\\sigma^{2}\\frac{\\mathrm{df}_{2}(\\kappa)}{T}\\cdot\\frac{1}{1-\\mathrm{df}_{2}(\\kappa)/T}\\asymp\\sigma^{2}\\frac{T^{\\ell/\\beta}}{T}\\frac{1}{1-o(1)}\\asymp\\sigma^{2}T^{-(1-\\ell/\\beta)}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We deduce the scaling law ", "page_idx": 26}, {"type": "equation", "text": "$$\nE_{t e s t}\\simeq B i a s+V a r\\asymp T^{-2\\ell r}+\\sigma^{2}T^{-(1-\\ell/\\beta)}\\asymp\\operatorname*{max}(\\sigma^{2},T^{1-2\\ell_{2}-\\ell/\\beta)})T^{-(1-\\ell/\\beta)},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "which is precisely the main result of [14]. ", "page_idx": 26}, {"type": "text", "text": "Low-Noise Regime. In the low noise regime where $\\sigma^{2}=O(T^{-2\\beta\\underline{{r}}})$ , one may take $\\ell=\\beta$ ; the variance is then much smaller than the bias, and one has the fast rate ", "page_idx": 26}, {"type": "equation", "text": "$$\nE_{t e s t}\\asymp T^{-2\\beta\\underline{{r}}}\\,.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "High-Noise Regime. Now, consider the case where $\\sigma^{2}=\\Theta(1)$ . Setting $2\\ell r=1-\\ell/\\beta$ to balance out the bias and variance gives $\\ell=\\ell_{c r i t}$ , where ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\ell_{c r i t}:=\\frac{\\beta}{2\\beta\\underline{{r}}+1}\\in(0,\\beta).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "With this value of the exponent $\\ell$ , we get the error rate ", "page_idx": 26}, {"type": "equation", "text": "$$\nE_{t e s t}\\asymp T^{-2\\ell_{c r i t}\\cdot\\underline{{r}}}=T^{-c},\\ \\mathrm{with}\\ c:=\\frac{2\\beta\\underline{{r}}}{2\\beta\\underline{{r}}+1},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "which is precisely the main result of [14], known to be minimax optimal (de Vito [11], etc.) ! ", "page_idx": 26}, {"type": "text", "text": "I Auxiliary Lemmas ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Lemma I.1. Let the sequence $(\\lambda_{k})_{k\\geq1}$ of positive numbers be such that $\\lambda_{k}\\asymp k^{-\\beta}$ for some constant $\\beta>0$ , and let $m,n\\geq0$ with $n\\beta>1$ . Then, for $D\\gg1$ , it holds that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{\\infty}\\frac{\\lambda_{k}^{n}}{(1+D\\lambda_{k})^{m}}\\asymp D^{-c}\\left\\{\\log D,\\right.\\quad i f m=n-1/\\beta,\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $c:=\\operatorname*{min}(m,n-1/\\beta)\\geq0$ . ", "page_idx": 26}, {"type": "text", "text": "Proof. First observe that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\lambda_{k}^{n}/(1+D\\lambda_{k})^{m}\\asymp\\lambda_{k}^{n}\\operatorname*{min}(1,(D\\lambda_{k})^{-m})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\left\\{\\lambda_{k}^{n}=k^{-n\\beta},\\qquad\\qquad\\qquad\\qquad\\qquad\\mathrm{if}\\ D\\lambda_{k}<1,\\mathrm{~i.e~if}\\ k>D^{1/\\beta},}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=D^{-m}\\lambda_{k}^{-(m-n)}=D^{-m}k^{(m-n)\\beta},\\quad\\mathrm{~else.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We deduce that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{\\infty}\\frac{\\lambda_{k}^{n}}{(1+D\\lambda_{k})^{m}}\\asymp D^{-m}\\sum_{1\\leq k\\leq D^{1/\\beta}}k^{(m-n)\\beta}+\\sum_{k>D^{1/\\beta}}k^{-n\\beta}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "By comparing with the corresponding integral, one can write the first sum in (62) as ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{D^{-m}\\displaystyle\\sum_{1\\leq k\\leq D^{1/\\beta}}k^{(m-n)\\beta}\\asymp D^{-m}\\int_{1}^{D^{1/\\beta}}u^{(m-n)\\beta}\\mathrm{d}u}\\\\ {\\qquad\\qquad\\times\\mathcal{D}^{-m}\\left\\{\\log D,\\vphantom{\\frac{1}{\\log}}\\right.}&{\\mathrm{if}\\left.n-1/\\beta<m,\\vphantom{\\frac{1}{\\log}}\\right.}\\\\ {\\qquad\\qquad\\qquad\\left.1,}&{\\mathrm{else}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $c\\geq0$ is as given in the lemma. ", "page_idx": 27}, {"type": "text", "text": "Analogously, one can write the second sum in (62) as ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\sum_{k>D^{1/\\beta}}k^{-n\\beta}\\asymp\\int_{D^{1/\\beta}}^{\\infty}u^{-n\\beta}\\mathrm{d}u\\asymp(D^{1/\\beta})^{1-n\\beta}=D^{-(n-1/\\beta)},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and the result follows upon putting things together. ", "page_idx": 27}, {"type": "text", "text": "Lemma I.2. For $\\kappa=\\kappa(\\lambda,T)$ defined as in (9), it holds that ", "page_idx": 27}, {"type": "equation", "text": "$$\n{\\frac{\\partial\\kappa}{\\partial\\lambda}}={\\frac{1}{1-\\mathrm{df}_{2}(\\kappa)/T}}\\geq1.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The formula given in the above lemma is useful because it can be combined with the identities ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle B i a s=w_{0}^{\\top}\\Sigma(\\Sigma+\\kappa I)^{-2}w_{0}\\frac{\\partial\\kappa}{\\partial\\lambda}},}\\\\ {{\\displaystyle V a r=\\sigma^{2}\\frac{\\mathrm{df}_{2}(\\kappa)}{T}\\frac{\\partial\\kappa}{\\partial\\lambda}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The RHS of (64) is usually referred to as the omniscient risk Hastie et al. [22], Cui et al. [13], Wei et al. [48]. ", "page_idx": 27}, {"type": "text", "text": "Proof of Lemma I.2. By definition of $\\kappa$ , we know that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\kappa-\\lambda=\\kappa\\mathrm{d}\\mathrm{f}_{1}(\\kappa)/T=\\kappa\\,\\mathrm{tr}\\,\\Sigma(\\Sigma+\\kappa I)^{-1}/T.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Let $\\begin{array}{r}{\\kappa:=\\frac{\\partial\\kappa}{\\partial\\lambda}}\\end{array}$ . Differentiating the above identity w.r.t. $\\lambda$ gives ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\kappa^{\\prime}-1=\\kappa^{\\prime}(\\operatorname{tr}\\Sigma(\\Sigma+\\kappa I)^{-1}-\\kappa\\operatorname{tr}\\Sigma(\\Sigma+\\kappa)^{-2})/T=\\kappa^{\\prime}\\operatorname{tr}\\Sigma^{2}(\\Sigma+\\kappa I)^{-2}/T=\\kappa^{\\prime}\\operatorname{df}_{2}(\\kappa)/T,\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and the result follows upon rearranging. Note that we have used the identity ", "page_idx": 27}, {"type": "equation", "text": "$$\nI-\\kappa(\\Sigma+\\kappa I)^{-1}=\\Sigma(\\Sigma+\\kappa I)^{-1},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "to rewrite $\\Sigma(\\Sigma+\\kappa I)^{-1}-\\kappa\\Sigma(\\Sigma+\\kappa I)^{-2}=\\Sigma^{2}(\\Sigma+\\kappa I)^{-2}.$ ", "page_idx": 27}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We explicitly describe the setting, as well as the conclusions of our theory in various regimes. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 28}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: We clearly state that our theory applies to the case of kernel ridge regression. All conclusions to different models are by analogy only, as is always done when this type of model is studied. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 28}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: All assumptions are clearly stated in the main text, and all proofs can be found in the Appendix. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 29}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We clearly give all experimental details, including parameters. These are sufficient to reproduce all our experiments. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 29}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 30}, {"type": "text", "text": "Answer: [No] ", "page_idx": 30}, {"type": "text", "text": "Justification: We only use one publicly available dataset, MNIST, and no idiosyncratic model. Thus, we provide neither dataset nor code, as the dataset is publicly available, and the experiments are easy to reproduce from their description. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 30}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We precisely describe all our (simple) linear or kernel methods in sufficient detail. They can easily be reproduced from these. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 30}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Error bars are given and we describe the number of trials over which they are obtained. We believe these are chosen suitably. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 30}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 31}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Compared to most contributions, our experiments, which accompany our theory, do not require any significant compute whatsoever. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 31}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: our research does not involve human participants, and never uses private data or models. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 31}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Our paper provides toy theory that underlines the point that retraining on data generated from currently available models might lead to a vicious loop of model collapse, which might have societal impact. We call for attention to this matter. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 32}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: We do not release any models or new data. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 32}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The only public dataset we use is MNIST [16], which we properly credit. The MNIST dataset is made available under the terms of the Creative Commons AttributionShare Alike 3.0 license. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets. \u2022 The authors should cite the original paper that produced the code package or dataset. \u2022 The authors should state which version of the asset is used and, if possible, include a URL. ", "page_idx": 32}, {"type": "text", "text": "\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 33}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: We do not release any new assets. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 33}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: We do not involve crowdsourcing nor research on humon subjects. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 33}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: We do not involve crowdsourcing nor research on humon subjects. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 33}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 34}]