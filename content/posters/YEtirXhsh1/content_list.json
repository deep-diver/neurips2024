[{"type": "text", "text": "Towards Learning Group-Equivariant Features for Domain Adaptive 3D Detection ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Sangyun Shin\u2020 Yuhang He\u2020 Madhu Vankadari\u2020 Ta-Ying Cheng Qian Xie\u2021 Andrew Markham\u2020 Niki Trigoni\u2020 ", "page_idx": 0}, {"type": "text", "text": "\u2020Department of Computer Science, University of Oxford, United Kingdom \u2021School of Computer Science, University of Leeds, United Kingdom ", "page_idx": 0}, {"type": "text", "text": "https://github.com/yunshin/GroupExp-DA.git sangyun.shin@cs.ox.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The performance of 3D object detection in large outdoor point clouds deteriorates significantly in an unseen environment due to the inter-domain gap. To address these challenges, most existing methods for domain adaptation harness self-training schemes and attempt to bridge the gap by focusing on a single factor that causes the inter-domain gap, such as objects\u2019 sizes, shapes, and foreground density variation. However, the resulting adaptations suggest that there is still a substantial inter-domain gap left to be minimized. We argue that this is due to two limitations: 1) Biased pseudo-label collection from self-training. 2) Multiple factors jointly contributing to how the object is perceived in the unseen target domain. In this work, we propose a grouping-exploration strategy framework, Group Explorer Domain Adaptation (GroupEXP-DA), to addresses those two issues. Specifically, our grouping divides the available label sets into multiple clusters and ensures all of them have equal learning attention with the group-equivariant spatial feature, avoiding dominant types of objects causing imbalance problems. Moreover, grouping learns to divide objects by considering inherent factors in a data-driven manner, without considering each factor separately as existing works. On top of the group-equivariant spatial feature that selectively detects objects similar to the input group, we additionally introduce an explorative group update strategy that reduces the false negative detection in the target domain, further reducing the inter-domain gap. During inference, only the learned group features are necessary for making the group-equivariant spatial feature, placing our method as a simple add-on that can be applicable to most existing detectors. We show how each module contributes to substantially bridging the inter-domain gaps compared to existing works across large urban outdoor datasets such as NuScenes, Waymo, and KITTI. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Learning 3D object detection [42, 12, 27] and segmentation [50, 11, 32, 10] in large outdoor point cloud scenes is of increasing importance due to its wide range of applications, such as autonomous driving [19] and Augmented Reality (AR) [14]. However, annotating such 3D data is expensive and labor-intensive. This limits the applicability and generalization of off-the-shelf models to diverse real-world applications. To mitigate the dependency on large-scale datasets with labels, studies for Domain Adaptation (DA) have gained considerable attention from the community [38, 25, 41, 46, 44, 43, 20, 17, 40, 8, 3, 31, 7]. In the context of 3D detection, DA is motivated by utilizing knowledge learned from a source domain to adapt to a target domain using a pseudo-label set. ", "page_idx": 0}, {"type": "image", "img_path": "YEtirXhsh1/tmp/dacb5ad4f2d0e05c2c0853f686fe115f4f9a08ce67464ba3171ac37999bbcda8.jpg", "img_caption": ["Figure 1: Several factors causing the inter-domain gap, such as the point density and object volume in NuScenes (Target) and Waymo (Source) datasets are illustrated with the ftited multivariate Gaussian distribution (left). The baseline [43] adaptation primarily detects objects having features near the mean of the distributions indicated by red circles. On the other hand, the proposed adaptation first groups objects and explores the target domain to reduce the false negative. The heatmaps show average recall (right). Objects with extreme sparsity are excluded for clear visualization. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Typically, in the widely used self-training scheme [44, 43], a detection model is pre-trained on the source domain and constructs an initial pseudo-label set for re-training in the target domain. The pseudo-label set progressively expands as more pseudo-labels are collected after each retraining. Here, the pseudo-label set consists of detections with high confidence scores. Based on the principle of self-training, recent existing works focus on specific factors that cause the inter-domain gap. These can be broadly categorized into domain variations in object sizes [38], density [8], or geometric structure [17, 20]. Despite good progress in bridging the inter-domain gap, two challenges remain unsolved: 1) Biased pseudo-labels due to the conservative collection strategy of the self-training scheme 2) Strict separation of multiple factors that jointly contribute to the creation of inter-domain gap. Typically, in one domain, a group of objects sharing common features outnumber other objects having different features, as addressed in existing works [38, 8, 17, 20]. While this may not cause a significant performance deterioration in the source domain, where the environment is similar, it could cause a bias under the self-training scheme. For example, detection with high confidence scores typically comes from objects belonging to dominant groups, as they are the ones that the detector learns the most of in the source domain. As shown in Figure 1 (a), the recall of objects in the target domain is significantly higher when the objects contain similar features as the dominant objects in the source domain, ignoring other objects. As a partial solution to this problem, existing works focus on a single factor only to address the inter-domain gap e.g. size or point cloud density. However, as Figure 1 shows, a single factor cannot explain all the domain variation as an object\u2019s appearance is a result of multiple factors jointly influencing the foreground points. ", "page_idx": 1}, {"type": "text", "text": "In this work, we address the aforementioned two issues of the current domain adaptive 3D detection. Our core intuition comes from the fact that the factors causing inter-domain gaps often already exist in the source domain to an extent. For example, the density of points often varies due to the distance and viewing angle of the sensor even inside a domain. Object sizes and shapes would also vary even in one single domain. Based on this observation, we aim to reduce the bias of the detection model to dominant objects by finding groups and evenly distributing the detector\u2019s attention during learning to all groups that would be otherwise largely neglected due to less dominant features. Nevertheless, finding optimal groups that represent the intra-domain gap is not straightforward because multiple factors jointly contribute to variations in objects\u2019 appearances. To address this issue, we introduce a data-driven grouping method that finds object groups with different characteristics. The groups are then progressively updated for adaptation, redistributing the available labels according to each group to learn the different characteristics found for each group in the target domain. Our contributions can be summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "1. We introduce a new alternative approach for domain adaptation, Group Explorer Domain Adaptation, (GroupExp-DA), which reflects on the available labels in order to understand the target domain, bridging the inter-domain gap. 2. To ensure each object group receives equal attention for learning, we introduce the GroupEquivariant spatial feature, which is learned for selectively detecting objects similar to the input group, preventing dominant types of objects from causing imbalance problems. ", "page_idx": 1}, {"type": "text", "text": "3. To make the best use of the Group-Equivariant feature\u2019s selective detection ability depending on the input group, we propose an exploration strategy that encourages the groups to reflect the target domain by redistributing available labels, leading to fewer false negatives for the adaptation. 4. Extensive adaptation experiments on KITTI, Waymo, and NuScenes datasets show the effectiveness of our approach for bridging inter-domain gaps. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Point Cloud-based 3D Detection for Outdoor Scene. Existing methods for 3D point cloud-based object detection can be divided into two main categories: point-based and voxel-based. Based on PointNet series [21, 24], point-based methods [23, 29, 29, 22, 45] propose to extract features from unordered and unstructured raw point-cloud directly. Voxel-based methods divide the unstructured point clouds into regular voxel grids and are fed into encoders either based on sparse convolution [42, 51, 12, 30, 26, 2, 9, 48] or Transformer [34, 16, 5, 37, 15]. The encoded spatial features, which are also called Bird\u2019s Eyes View (BEV) features, are then fed into Regional Proposal Networks (RPN) for the final detection. There are also a few methods that combine point- and voxel-based methods [27, 28, 39]. Currently, in terms of performance, voxel-based detectors dominate the point-cloud-based 3D detection for outdoor scenes. Therefore, we employ Second-IoU [42] and PointPillars [12] as the base detectors for our experiments, as they are most-widely used detectors that existing works are built on. ", "page_idx": 2}, {"type": "text", "text": "Domain Adaptive Detection. The target of domain adaptive 3D detection is to mitigate the interdomain gap between the source and the target by focusing on several factors, such as object size [38, 25], point cloud deterioration [41], and the encoder separation for domains [46]. ST3D series [44, 43] propose a self-training scheme that progressively collects pseudo-label sets in the target domain for retraining. Built on self-training scheme, DA for 3D detection has been extensively studied to acquire higher quality pseudo-labels by addressing the inter-domain gap caused by geometric shape with prototype learning [20, 17], dense-to-sparse density variation using knowledge distillation [40], a general density variation with beam augmentation and knowledge distillation [8], cross-domain examination to measure the consistency of pseudo-labels [3] and focusing on specific architecture [49]. A considerable number of existing works focus on a single factor, such as density, shape, and size, to bridge the inter-domain gap. Instead, we attempt to see the inter-domain gap as a result of multiple factors combined and bridge the gap by finding inherent groups. ", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Framework Overview ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Following the self-training-based unsupervised domain adaptation scheme [44, 43] for 3D detection, we are given point clouds $X=X_{s}\\cup X_{t}$ and labels $Y=Y_{s}\\cup Y_{t}$ as the initial set. Here, $X_{s}$ and $Y_{s}$ are point could and box labels of the known source domain. On the other hand, $X_{t}$ and $Y_{t}$ are the point cloud and initial pseudo label set in the unlabeled target domain. $Y_{t}$ is collected by the detector trained only on the source domain using $X_{s}$ and $Y_{s}$ . A label in $Y$ consists of seven parameters defining a 3D box with three parameters for center $(x,y,z)$ , three parameters for size $(l,w,h)$ , and one parameter for vertical rotation $\\theta$ . Our goal is to improve the detector\u2019s inter-domain adaptation by focusing on the pseudo-label collection. In particular, instead of treating all available objects in $Y$ equivalently, we aim to understand objects in $Y$ better by grouping. ", "page_idx": 2}, {"type": "text", "text": "Specifically, as depicted in Figure 2 and Alg. 1, we first extract foreground points from available boxes and learn to encode objects into the object descriptor from the points (Sec.3.2). The descriptors are then used to progressively group objects (Sec. 3.3). Our Group-Region Correlation (Sec. 3.4) takes the group features as input and fuses with RPN to selectively detect objects similar to the individual group. ", "page_idx": 2}, {"type": "text", "text": "3.2 Object Descriptor Extraction ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Given a pair of point cloud $\\boldsymbol{x}\\,\\in\\,\\mathbb{R}^{n_{p}\\times3}$ and a label set $y\\,\\in\\,\\mathbb{R}^{n_{\\mathrm{b}}\\times\\tau}$ consisting of $n_{p}$ points and $n_{b}$ boxes from $X$ and $Y$ , respectively, this module aims to produce a learnable object descriptor $F_{o b j}\\in\\mathbb{R}^{n_{\\mathrm{b}}\\times d_{o b j}}$ during training. First, foreground points, $\\dot{P_{o b j}}=\\{p_{o b j}^{k}\\}_{k=1}^{n_{\\mathrm{b}}}$ are extracted using $y$ from the input point cloud $x$ . Our object descriptor extraction module then takes $P_{o b j}$ as input and outputs object descriptors $F_{o b j}$ using neural networks consisting of MLP and global max pooling, which are adopted from [21, 13]. The motivation behind the architectural choice is two-fold: (1) An arbitrary number of object points, $p_{o b j}^{k}$ , can be processed efficiently without involving comparably slow sampling techniques. (2) Global max pooling offers permutation invariant features, which helps $F_{o b j}$ less prone to overftiting by certain permutations from viewing angles, etc. During training, $F_{o b j}$ serves as the input for the progressive grouping process and is not used during inference. ", "page_idx": 2}, {"type": "image", "img_path": "YEtirXhsh1/tmp/095c4fb479d8fe8b5425fd8ef2872c3345dbc314794475523567f4698fca1752.jpg", "img_caption": ["Figure 2: Overall pipeline of our proposed method. During training, we extract foreground points from existing 3D box labels and feed them to the Object Descriptor Extraction module to acquire object descriptors. The descriptors are used for grouping & exploration and fed into the GroupCorrelation module to generate RPN to detect objects similar to each group. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "3.3 Progressive Grouping ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Determining similarities between objects is a complex problem as intra-object variations are created by various factors, such as density, shape, size, etc. To find groups that, when combined, explain this intra-object variation, we utilize Gaussian Mixture Model (GMM) based grouping for the following advantages: (1) GMM better captures the heterogeneity of data using only a few more parameters, such as covariance and weights, compared to the proximity-based methods. (2) The parameters that define groups can be efficiently updated with weighted linear combinations and are also differentiable for learning the groups in a data-driven manner. In the following sections, we explain the details of how the groups are initialized, determined for each sample, and updated. ", "page_idx": 3}, {"type": "text", "text": "Initialization Given all available pairs of scans and labels, $\\boldsymbol{\\mathrm{X}}$ and Y, we first extract $F_{o b j}$ from each pair and stack them. All of the stacked $\\{F_{o b j}^{o}\\}_{o=1}^{n_{t o t a l}}$ are then used for the initialization. Here, $n_{t o t a l}$ s  sctlaunsdtes rfionrg  tohne ,r  wofh epraei resa icnh $\\Chi$ uasntde r Yf.o rSmpes cai fgicroalulpy.,  wAfet eirn itthiiasl,i zMe $n_{\\mathrm{g}}$ mgruomu pLsi kuesliinhgo oKd$\\{F_{o b j}^{o}\\}_{o=1}^{n_{t o t a l}}$   \nEstimation is performed for each group to acquire parameters, such as mean $\\mu\\in\\mathbb{R}^{n_{\\mathrm{g}}\\times d_{o b j}}$ , covariance $\\sigma\\,\\in\\,\\mathbb{R}^{n_{\\mathrm{g}}\\times d_{o b j}\\times d_{o b j}}$ , and weight $\\phi\\,\\in\\,\\mathbb{R}^{n_{\\mathrm{g}}}$ that define a GMM-based group. This initialization is required only once before the training. In the following, all procedures are based on a single pair of scan and label, $x$ and $y$ , which can be extended to batch-wise operation. ", "page_idx": 3}, {"type": "text", "text": "Determining Group for New Sample The probability $P^{k\\to i}$ of $k$ -th sample belonging to $i$ -th group is estimated as : ", "page_idx": 3}, {"type": "equation", "text": "$$\nP^{k\\to i}=\\frac{\\phi^{i}\\mathcal{N}(F_{o b j}^{k}|\\mu^{i},\\phi^{i})}{\\sum_{t=1}^{n_{\\mathrm{g}}}\\phi^{t}\\mathcal{N}(F_{o b j}^{k}|\\mu^{t},\\phi^{t})},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where ${\\mathcal{N}}(|,)$ is the probability density function of multivariate Gaussian distribution that outputs the likelihood of a sample $F_{o b j}^{k}$ given mean $\\mu$ and covariance $\\phi$ . The group labels $G\\in\\mathbb{R}^{n_{b}}$ for all the $n_{b}$ samples in $F_{o b j}$ are then acquired using $P$ . That is, the group label $G^{k}$ for $k$ -th sample is specifically determined as: ", "page_idx": 3}, {"type": "equation", "text": "$$\nG^{k}=\\operatorname*{arg\\,max}_{i}P^{k\\rightarrow i}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Explorative Update during Training Typically, pseudo-label sets from the target domain are incorporated into the existing label set from the source domain and considered as the same label set ", "page_idx": 3}, {"type": "image", "img_path": "YEtirXhsh1/tmp/ecf3f62f4ffbcde78c78402666d98a387992574e9b52df578b8c2c1dca23172e.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 3: Conceptual diagram comparing pseudo-label generation processes of (a) baseline [44, 43] with our (b) grouping followed by (c) the explorative update using pseudo-labels. ", "page_idx": 4}, {"type": "text", "text": "for training [44, 43], as shown in Figure 3 (a). However, we argue that for the domain adaptation, the pseudo-label set should be given more weight on its usage than the source label sets because they contain inherent features of the target domain, which can be used to understand the target domain in terms of objects\u2019 appearances. To address this, we introduce an explorative group update strategy using pseudo-labels. For $i$ -th group, its mean $\\hat{\\mu}^{i}\\in\\mathbb{R}^{d_{o b j}}$ , covariance $\\bar{\\sigma}^{i}\\in\\mathbb{R}^{d_{o b j}\\times d_{o b j}^{\\star}}$ , and the group weight $\\hat{\\phi}^{i}$ are acquired as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{\\mu}^{i}=\\frac{1}{n_{b,i}}\\sum_{k=1}^{n_{b,i}}F_{b,i}^{k},\\quad\\hat{\\sigma}^{i}=\\frac{1}{n_{b,i}}\\sum_{k=1}^{n_{b,i}}(F_{o b j,i}^{k}-\\hat{\\mu}^{i})(F_{o b j,i}^{k}-\\hat{\\mu}^{i})^{T},\\quad\\hat{\\phi}^{i}=\\frac{n_{b,i}}{n_{b}},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $F_{o b j,i}$ is subset of $F_{o b j}$ that belong to $i$ -th group using $G$ in Eqn. 2 as: ", "page_idx": 4}, {"type": "equation", "text": "$$\nF_{o b j,i}=\\{F_{o b j}^{k}|G^{k}=i\\},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "and $n_{b,i}$ is the number of samples in $F_{o b j,i}$ . Similar to the update rule for prototype learning [20], each group parameters are updated with linear combination as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mu=\\alpha\\mu+(1-\\alpha)\\hat{\\mu},\\quad\\sigma=\\alpha\\sigma+(1-\\alpha)\\hat{\\sigma},\\quad\\phi=\\alpha\\phi+(1-\\alpha)\\hat{\\phi},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\alpha$ is a coefficient that affects how conservatively each parameter is updated. After the update, $\\mu^{i}$ is considered as a representative of $i$ -th group and utilized as a query input for generating groupequivariant spatial features. Accordingly, during training, the samples that belong to the $i$ -th group are used as the foreground boxes. Intuitively, this process distributes source labels according to the groups found in the target domain so that the source labels are used for learning to find similar objects in the target domain, as shown in Figure 3 (b) and (c). ", "page_idx": 4}, {"type": "text", "text": "For training, to ensure that the groups learn to be distinctive enough, we adopt inter-group repel [17] based on the contrastive loss. ", "page_idx": 4}, {"type": "equation", "text": "$$\nL_{r e p}=\\sum_{i=1}^{n_{g}-1}\\sum_{j=i+1}^{n_{g}}m a x(0,c o s(\\mu^{i},\\mu^{j})),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $c o s(,)$ is a function that calculates cosine-similarity between two inputs. In addition, to encourage similar features for group cohesion, intra-group attraction loss, $L_{a t t}$ is used: ", "page_idx": 4}, {"type": "equation", "text": "$$\nL_{a t t}=\\sum_{i=1}^{n_{g}}\\sum_{k=1}^{n_{b}}(1-c o s(F_{o b j}^{k},\\mu^{i}))\\mathbb{1}[G^{k}=i],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathbb{I}[G^{k}=i]$ is an indicator function that is 1 if $G^{k}=i$ and 0 otherwise. During the inference, only the learned $\\mu$ is necessary for the following Group-Region Correlation. ", "page_idx": 4}, {"type": "text", "text": "3.4 Group-Region Correlation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In a typical voxel-based 3D object detector\u2019s pipeline, voxelized points are fed into the backbone, which outputs the spatial feature $F_{b e v}\\in\\mathbb{R}^{H\\times W\\times d_{b e v}}$ , as shown in Figure 2. $F_{b e v}$ is then fed into RPN to make final box predictions. Given input group queries $\\mu=\\{\\bar{\\mu}^{i}\\}_{i=1}^{n_{g}},\\mu^{i}\\in\\mathbb{R}^{d_{o b j}}$ and $F_{b e v}$ , Our Group-Region Correlation aims at producing the spatial features $F_{c b e v}\\in\\mathbb{R}^{n_{g}\\times H\\times W\\times d_{b e v}}$ that ", "page_idx": 4}, {"type": "text", "text": "Require: a point-cloud $x$ , (pseudo) labels $y$ , Group parameters $G$ ", "page_idx": 5}, {"type": "text", "text": "1: Extract obj. descriptors $F_{o b j}$ using $x$ and $y$ (Sec 3.2)   \n2: Determine groups of $F_{o b j}$ and update $G$ (Sec 3.3)   \n3: Update $G$ using $F_{o b j}$ according to the determined groups (Sec 3.3)   \n4: Calculate $L_{r e p}$ and $\\bar{L}_{a t t}$ as in $\\csc3.3$   \n5: Extract spatial feature $F_{b e v}$ from the backbone (Sec 3.4)   \n6: Make group equivariant features for each group using $\\mathrm{G}$ and $F_{b e v}$ (Sec 3.4)   \n7: Predict boxes using shared RPN for each group equivariant feature   \n8: Calculate $L_{d e t}$ as in Eq. 10.   \n9: Calculate gradients using $L_{r e p}$ , $L_{a t t}$ , and $L_{d e t}$ .   \n10: Update all modules\u2019 weights using the gradients ", "page_idx": 5}, {"type": "text", "text": "are equivariant to the group query so that $F_{c b e v}$ provide selective features for detecting objects similar to each group. In the following sections, we explain how each group and spatial feature are correlated and then used by RPN to detect the corresponding objects. ", "page_idx": 5}, {"type": "text", "text": "Group Equivariant Spatial Feature The aim of Group-Region Correlation module is to make spatial features $F_{b e v}$ selectively attend to objects that are similar to the query group. The following RPN then detects only certain objects that are similar to the query group. To achieve this, we utilize cross-attention with $\\mu$ as query and $F_{b e v}$ as key and value to encourage features from $\\mu$ and $F_{b e v}$ to cross-attend to generate necessary features. Intuitively, $\\mu$ is compared to $F_{b e v}$ to find the object that is similar to each group. For $i$ -th group, the attended group-equivariant spatial features $F_{c b e v}^{\\tilde{i}}\\in\\mathbb{R}^{H\\times W\\times d_{b e v}}$ are acquired as: ", "page_idx": 5}, {"type": "equation", "text": "$$\nF_{c b e v}^{i}=c A t t n(\\mu^{i},F_{b e v},F_{b e v}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $c A t t n(.)$ refers to the cross attention [36] that takes query, key, and value as input and outputs the cross-attended feature. Here, $\\mu^{i}$ is the $i$ -th query in $\\mu$ . The group-equivariant spatial features $F_{c b e v}$ can then be fed into any existing RPN strcutures [42, 4, 12] to detect foreground objects for each group. The ground-truth boxes in $y$ that belong to the $i$ -th group are utilized as the foreground boxes for $\\mathbf{\\bar{\\boldsymbol{F}}}_{c b e v}^{i}$ to train the follwing RPN. ", "page_idx": 5}, {"type": "text", "text": "Regional Proposal Network (RPN) Following the general architecture of RPN [42, 12], our objectness and box regression heads take $F_{c b e v}\\in\\bar{\\mathbb{R}}^{n_{g}\\times\\check{H}\\times W\\times D}$ as input and predict objectness scores $F_{c l s}\\,\\in\\,\\mathbb{R}^{n_{g}\\times H\\times W\\times1}$ and box parameters $F_{b o x}\\;\\in\\;\\mathbb{R}^{n_{g}\\times H\\times W\\times\\bar{7}}$ to form 3D boxes on the dense spatial grid corresponding to $F_{c b e v}$ . ", "page_idx": 5}, {"type": "text", "text": "For the training of $i$ -th group, standard training losses for RPN based detection, $L_{d e t}^{i}$ , are applied as existing works [42, 12, 30, 26, 2, 9, 48]. Specifically, given the box labels $y^{i},F_{c l s}^{i}$ and F biox are used for calculating first-stage box detection training loss, $L_{d e t1}$ , as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n{\\cal L}_{d e t1}^{i}={\\cal L}_{f o c a l}^{i}+{\\cal L}_{b o x}^{i},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\boldsymbol{L}_{f o c a l}$ stands for Focal Loss [18] and $L_{b o x}$ is box regression loss. Here, the foreground labels and regression targets for $\\boldsymbol{L}_{f o c a l}$ and $L_{b o x}$ are calculated using $y^{i}$ depending on the individual base detectors\u2019 configurations. Similarly, for the second-stage box refinement training, $F_{c b e v}$ is used with $F_{c l s}$ and $F_{b o x}$ for RoI Pooling. Then, the pooled features are fed into classification and box regression head for refinement with architectures depending on the detectors to calculate the second stage loss $L_{d e t2}^{i}$ . The detection loss for all groups, $L_{d e t}$ , is then acquired by iterating over all groups as: ", "page_idx": 5}, {"type": "equation", "text": "$$\nL_{d e t}=\\frac{1}{n_{g}}\\sum_{i=1}^{n_{g}}{L_{d e t1}^{i}+L_{d e t2}^{i}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "3.5 Overall Training ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Apart from $L_{r e p}$ and $L_{a t t}$ for grouping, our overall training losses are defined the same as the general RPN learning for detection. ", "page_idx": 5}, {"type": "equation", "text": "$$\nL=\\lambda_{1}L_{r e p}+\\lambda_{2}L_{a t t}+\\lambda_{3}L_{d e t}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Using $L$ we train our system following the self-training scheme [44, 43]. Further details can be found in Sec:B.1. ", "page_idx": 5}, {"type": "text", "text": "4 Experiment ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.1 Datasets ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We evaluate our methods against various baselines across three different datasets, such as KITTI [6], NuScenes [1], and Waymo [33]. KITTI contains 7481 frames of point clouds for training and validation, and all the data is collected with 64-beam Velodyne LiDAR. NuScenes dataset contains 28130 training and 6019 validation point clouds collected with a 32-beam roof LiDAR. Waymo dataset contains 122000 training and 30407 validation frames of point clouds collected with five LiDAR sensors, i.e., one 64-beam LiDAR and four 200-beam LiDAR. ", "page_idx": 6}, {"type": "text", "text": "4.2 Implementation Details ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "For a fair comparison with existing domain adaptive 3D detection methods, we build our model on two base detectors, Second IoU [42] and PointPillars [12], following [44, 43, 17, 8], that are widely used and applicable to most recent detectors with the implementation based on OpenPCDet [35] and parameters from ST3D [44]. Following [17], we first train each detector for 50 epochs with batch-size 8 as a pretraining step using a single NVIDIA A10 GPU. In the self-training stage, we train 30 more epochs for the tuning to adapt to the target domain. The learning rate is set to $\\mathrm{1\\times\\bar{1}5^{-4}}$ using Adam optimizer with Cosine annealing [47] for scheduling the learning rate. In order to improve the learning stability, the baseline RPN is also learned in addition to the group learning. While the group equivariant RPNs are used for the pseudo label collection during self-training, only the baseline RPN is used for the final testing, ensuring the same execution speed as the existing pipeline. The feature dimensions $d_{b e v}$ and $d_{o b j}$ for $F_{b e v}$ and $\\mu$ are both set to 512 for $c A t t n$ . $\\alpha$ for updating the group parameters is empirically set to 0.8. $\\lambda_{1}$ , $\\lambda_{2}$ , and $\\lambda_{3}$ are set to 0.5, 0.5, and 1.0, respectively. ", "page_idx": 6}, {"type": "text", "text": "4.3 Comparing Methods ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We compare recent existing 3D domain adaptive detection methods, such as SN [38], 3D-CoCo [46], ST3D [44, 43], GPA-3D [17] and DTS [8] with our proposed method. As our method is based on the self-training, we set ST3D [44] as our baseline and show experimental results by comparing with more recent methods. Additionally, we also illustrate the performance of the oracle models, which refer to a fully-supervised model on the target domain directly as an upper bound. Following the most recent works [17, 8], all methods are compared in three adaptation scenarios focusing on \"car\" class: (1) Waymo $\\rightarrow$ NuScenes (2) NuScenes $\\rightarrow$ KITTI (3) Waymo $\\rightarrow$ KITTI. ", "page_idx": 6}, {"type": "text", "text": "4.4 Evaluation Metric ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Following [44, 43, 17, 8], we adopt Average Precision(AP) as our primary evaluation metric and evaluate our model on Bird-Eyes-View (BEV) IoU, $A P_{\\mathrm{BEV}}$ , and 3D Box IoU, $A P_{3\\mathrm{D}}$ , with 40 varying recall points and 0.7 as the IoU threshold. ", "page_idx": 6}, {"type": "text", "text": "4.4.1 Quantitative Result ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Waymo $\\rightarrow$ KITTI Table 1 (first task) shows the quantitative results of 3D detection in $A P_{\\mathrm{BEV}}$ and $A P_{3\\mathrm{D}}$ . When using Second-IoU as detector [42], our proposed method outperforms the baseline ST3D series for 4.75/11.88 in $A P_{\\mathrm{BEV}}/A P_{3\\mathrm{D}}$ , respectively. Compared with the SOTA method, DTS [8], 1.14/2.21 improvements are made. When using PointPillars as the base detector [12], our approach gains $2.34/3.91$ improvements compared to the best performing method, GPA-3D [17]. ", "page_idx": 6}, {"type": "text", "text": "NuScenes $\\rightarrow$ KITTI As shown in Table 1 (second task), our approach shows 0.97/5.80 performance gains in terms of $A P_{\\mathrm{BEV}}/A P_{3\\mathrm{D}}$ with Second-IoU [42] as the base detector. Compared with SOTA DTS [8], $0.07/1.60$ improvements are acquired. With PointPillars [12] as the base detector, our approach exceeds the baseline and DTS by 21.49/41.74 and 2.39/1.04, respectively. ", "page_idx": 6}, {"type": "text", "text": "Waymo $\\rightarrow$ NuScenes Table 1 (third task) illustrates the adaptation results. Our approach outperforms the baseline and the best-performing method by 8.55/5.72 and 3.27/2.91 in $A P_{\\mathrm{BEV}}/A P_{3\\mathrm{D}}$ , respectively, with Second-IoU as the detector. Similarly, with PointPillars as the detector, 14.89/7.84 and $32.8/1.94\\$ improvements are gained compared with the baseline and SOTA in terms of $A P_{\\mathrm{BEV}}/A P_{3\\mathrm{D}}$ . ", "page_idx": 6}, {"type": "table", "img_path": "YEtirXhsh1/tmp/0040faf392fda72bc9a56a5c02d5129730d32f990da31195775b381f6dd8d1ee.jpg", "table_caption": ["Table 1: Quantitative comparisons of the recent domain adaptive 3D detection methods on three adaptation scenarios. The top-3 performing methods are labeled in different colors. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.4.2 Qualitative Result ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Figure 4 compares the 3D detection results of the baseline [44], DTS [8] and our methods. Due to the conservative pseudo-label selection policy and absence of methods addressing variations in object size or foreground density, the baseline struggles to detect objects with comparably less common sizes or further away with different densities (red circles in 1st row (b)). Moreover, some dominant object shapes make the detector overfit, leading to false positive detection of road structure (red circles in 2nd row (b)). DTS [8] improves the inter-domain density variance problem presented in the baseline. However, it still encounters the overfitting problem to certain geometric shapes in the source domain, leading to the same false positive detection of the road structure as the baseline (red circles in 2nd row (c)). Moreover, despite the training for foreground density invariant, the objects appearing sparse due to the distance remain false negative (red circles in 1st row (c)) in DTS. The observation suggests that another factor, in addition to the foreground density, causes the inter-domain gap. On the other hand, our proposed method improves both false positive and negative detections, demonstrating a more robust adaptation ability than DTS, which is the best-performing method. ", "page_idx": 7}, {"type": "table", "img_path": "YEtirXhsh1/tmp/1006a7454269f6d64a4f696431ce3cc294c5d56ca575b9efed49f852b4e75206.jpg", "table_caption": ["Table 2: Impact of each component in $A P_{\\mathrm{BEV}}$ and $A P_{3\\mathrm{D}}$ on Waymo to NuScenes adaptation. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 3: Impact of grouping methods and $\\alpha$ on NuScenes to KITTI adaptation. ", "page_idx": 8}, {"type": "table", "img_path": "YEtirXhsh1/tmp/2ee31f711a73b8e35773202c6b9f737304d6291186b44d083be4ec78599f05e4.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "YEtirXhsh1/tmp/65481457a8ceedf476f6268292b63e143cb2bde491b19ea32f712ed02625fb46.jpg", "img_caption": ["Figure 5: Impact of $n_{\\mathrm{g}}$ on three adaptation scenarios in $A P_{3\\mathrm{D}}$ . Here $W\\dot{\\rightarrow}K$ , $N\\rightarrow K$ , and $W\\rightarrow$ $N$ refer to Waymo $\\rightarrow$ KITTI, NuScenes $\\mathrel{\\mathop:}\\to\\!\\mathrm{KITII}$ , and Waymo $\\rightarrow$ NuScenes adaptations. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "YEtirXhsh1/tmp/f21e0b687aed6958bc9ef5a4c5094b424305fc41da18d25cf616436eec4fb979.jpg", "img_caption": ["Figure 6: Comparison of DTS [8] (left), ours without explorative update (middle), and ours with explorative update (right) in Waymo $\\rightarrow$ NuScenes with t-SNE visualization. Here, the foreground features are extracted using ground-truth boxes using $F_{\\mathrm{bev}}$ for DTS and $F_{\\mathrm{cbev}}$ for ours. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.5 Ablations ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Impact of Each Component Table 2 shows the impact of each component in Waymo $\\rightarrow$ NuScenes adaptation using Second IoU [42] in terms of $A P_{\\mathrm{BEV}}/A P_{3\\mathrm{D}}$ . For this experiment, we progressively add each core component to the baseline (a) to see how they affect the performance. As grouping ensures every group has similar attention during training and prevents only a few dominant object types from having high confidence scores, it significantly improves the performance $9.91\\%/12.\\dot{2}3\\%$ from the baseline. From this result, $L_{a t t}$ (c) and $L_{r e p e l}$ (d) improve $3.3\\bar{9}\\%/2.11\\%$ and $4.20\\%/4.19\\%$ respectively, suggesting that making the groups distinctive has more impact than making the intragroup cohesive. This is expected because, during the group-equivariant RPN training, samples inside each group are already learned to be cohesive against background and samples from other groups, indirectly supervised to be close to each other. Nevertheless, when combined in (e), $L_{a t t}$ and $L_{r e p e l}$ improves $15\\bar{.}92\\%/21.59\\%$ from the baseline, demonstrating the synergy of the two losses. On top of this, the explorative update considerably boosts the performance by $\\mathrm{23.80\\%/28.33\\%}$ , proving that using samples found in target (pseudo-labels) for the update further reduces the inter-domain gap, as can also be seen in Figure 6 (a group of FN in the middle disappears in the right figure). ", "page_idx": 8}, {"type": "text", "text": "Impact of $n_{g}$ is illustrated in Figure 5 in $A P_{3\\mathrm{D}}$ for all adaptation scenarios presented in the main experiments. For this experiment, we include all components with the same hyper-parameters and only change $n_{g}$ to see the impact. As can be seen, increasing $n_{g}$ improves the performance, reaching the top around when $n_{g}$ is set to 4. In principle, having many groups would make the group-equivariant RPN discover more object types while ensuring the same attention for each group during learning. However, in practice, the performances start decreasing when $n_{g}$ is set around 6, suggesting that having too many groups increases the risks of overfti problems due to a small number of objects in each group or underfti problems due to high complexity for learning $F_{c b e v}$ . We also find that the training result is most stable when $n_{g}$ is set to 3. ", "page_idx": 8}, {"type": "text", "text": "Effect of GMM based Grouping and $\\alpha$ are illustrated in Table 3 to compare: (1) Proximity-based grouping and (2) GMM-based grouping with varying $\\alpha$ . For the proximity-based grouping, we discard $\\sigma$ and $\\phi$ from GMM, and determine the group of each sample as the closest $\\mu$ from the samples using $L2$ distance while all other configurations stay the same. As can be seen, GMM-based grouping constantly shows better performances in both $A P_{\\mathrm{BEV}}/A P_{3\\mathrm{D}}$ in all $\\alpha$ . This is due to $\\sigma$ and $\\phi$ that preserve the characteristics of each group more compared to only $\\mu$ based on previously seen samples, improving the stability during learning with similar objects grouped together. Additionally, setting $\\alpha$ too large or small degrades the performances, as small $\\alpha$ discourages the explorative update, while too large $\\alpha$ could result in instability as the input $\\mu$ for training RPN constantly changes more. Also, setting $\\alpha=0.6$ results in $0.49/0.66$ higher in $A P_{\\mathrm{BEV}}/A P_{3\\mathrm{D}}$ compared to setting $\\alpha=0.99$ , suggesting the advantage from exploration surpasses the stability. Interestingly, unlike GMM-based grouping, setting higher $\\alpha$ constantly shows increasingly better performance with the proximity-based grouping, proving inherent instability without $\\sigma$ and $\\phi$ . Nevertheless, in nearly all the configurations of $\\alpha$ , the GMM-based grouping persistently outperforms the best-performing method, DTS [8]. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "5 Conclusion and Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we present GroupExp-DA that learns object groups, which can be used to bridge the inter-domain gap with (1) less bias by the dominant objects in the available label sets and (2) consideration of multiple factors for creating inter-domain gaps in a data-driven manner. This is achieved by utilizing the group equivariant spatial features that connect the group feature and spatial features to be learned together with the existing detection loss function. Nevertheless, all methods, including ours, struggle to detect objects with extremely sparse foreground points (black circles in 2nd row), as shown in Figure 4, because those objects do not contain distinctive features to being well-learned as groups due to extreme sparsity, which remains as one of the challenges to be explored. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by Innovate UK Grant No: 10107189 \u201cScanSpot: 3D-Modelling \u201cDigital Twin\u201d Data For New Insurance Products\u201d. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, A. Krishnan, Y. Pan, G. Baldan, and O. Beijbom, \u201cnuscenes: A multimodal dataset for autonomous driving,\u201d in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2020, pp. 11 621\u201311 631. 7   \n[2] Y. Chen, J. Liu, X. Zhang, X. Qi, and J. Jia, \u201cLargekernel3d: Scaling up kernels in 3d sparse cnns,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 13 488\u201313 498. 3, 6   \n[3] Z. Chen, Y. Luo, Z. Wang, M. Baktashmotlagh, and Z. Huang, \u201cRevisiting domain-adaptive 3d object detection by reliable, diverse and class-balanced pseudo-labeling,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 3714\u20133726. 1, 3, 15   \n[4] K. Duan, S. Bai, L. Xie, H. Qi, Q. Huang, and Q. Tian, \u201cCenterNet: Keypoint Triplets for Object Detection,\u201d in IEEE/CVF International Conference on Computer Vision (ICCV), 2019. 6   \n[5] L. Fan, Z. Pang, T. Zhang, Y.-X. Wang, H. Zhao, F. Wang, N. Wang, and Z. Zhang, \u201cEmbracing single stride 3d object detector with sparse transformer,\u201d in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022, pp. 8458\u20138468. 3   \n[6] A. Geiger, P. Lenz, and R. Urtasun, \u201cAre we ready for Autonomous Driving? The KITTI Vision Benchmark Suite,\u201d in CVPR, 2012. 7   \n[7] A. W. Harley, Y. Zuo\\*, J. Wen\\*, A. Mangal, S. Potdar, R. Chaudhry, and K. Fragkiadaki, \u201cTrack, Check, Repeat: An EM Approach to Unsupervised Tracking,\u201d in CVPR, 2021. 1   \n[8] Q. Hu, D. Liu, and W. Hu, \u201cDensity-Insensitive Unsupervised Domain Adaption on 3D Object Detection,\u201d in IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 1, 2, 3, 7, 8, 9, 10, 14   \n[9] X. Jin, K. Liu, C. Ma, R. Yang, F. Hui, and W. Wu, \u201cSwiftpillars: High-efficiency pillar encoder for lidar-based 3d detection,\u201d in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 38, no. 3, 2024, pp. 2625\u20132633. 3, 6   \n[10] M. Kolodiazhnyi, A. Vorontsova, A. Konushin, and D. Rukhovich, \u201cOneformer3d: One transformer for unified point cloud segmentation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 1   \n[11] X. Lai, Y. Yuan, R. Chu, Y. Chen, H. Hu, and J. Jia, \u201cMask-attention-free transformer for 3d instance segmentation,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2023. 1   \n[12] A. H. Lang, S. Vora, H. Caesar, L. Zhou, J. Yang, and O. Beijbom, \u201cPointpillars: Fast encoders for object detection from point clouds,\u201d in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2019, pp. 12 697\u201312 705. 1, 3, 6, 7   \n[13] I. Lang, A. Manor, and S. Avidan, \u201cSamplenet: Differentiable point cloud sampling,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 7578\u20137588. 4   \n[14] V. V. Lehtola, H. Kaartinen, A. N\u00fcchter, R. Kaijaluoto, A. Kukko, P. Litkey, E. Honkavaara, T. Rosnell, M. T. Vaaja, J.-P. Virtanen, et al., \u201cComparison of the selected state-of-the-art 3d indoor scanning and point cloud generation methods,\u201d Remote sensing, vol. 9, no. 8, p. 796, 2017. 1   \n[15] J. Li, S. Dong, L. Ding, and T. Xu, \u201cMssvt++: Mixed-scale sparse voxel transformer with center voting for 3d object detection,\u201d IEEE transactions on pattern analysis and machine intelligence, 2023. 3   \n[16] Y. Li, Y. Chen, X. Qi, Z. Li, J. Sun, and J. Jia, \u201cUnifying voxel-based representation with transformer for 3d object detection,\u201d Advances in Neural Information Processing Systems, vol. 35, pp. 18 442\u201318 455, 2022. 3   \n[17] Z. Li, J. Guo, T. Cao, B. Liu, and W. Yang, \u201cGPA-3D: Geometry-aware Prototype Alignment for Unsupervised Domain Adaptive 3D Object Detection from Point Clouds,\u201d in International Conference on Computer Vision (ICCV), 2023. 1, 2, 3, 5, 7, 8   \n[18] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Doll\u00e1r, \u201cFocal loss for dense object detection,\u201d in Proceedings of the IEEE international conference on computer vision, 2017, pp. 2980\u20132988. 6   \n[19] K.-B. Park, M. Kim, S. H. Choi, and J. Y. Lee, \u201cDeep learning-based smart task assistance in wearable augmented reality,\u201d Robotics and Computer-Integrated Manufacturing, vol. 63, p. 101887, 2020. 1   \n[20] X. Peng, X. Zhu, and Y. Ma, \u201cCL3D: Unsupervised Domain Adaptation for Cross-Lidar 3D Detection,\u201d in Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), 2023. 1, 2, 3, 5   \n[21] C. Qi, H. Su, K. Mo, and L. J. Guibas, \u201cPointnet: Deep Learning on Point Sets for 3D Classification and Segmentation,\u201d Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2017. 3, 4   \n[22] C. R. Qi, O. Litany, K. He, and L. J. Guibas, \u201cDeep hough voting for 3d object detection in point clouds,\u201d in proceedings of the IEEE/CVF International Conference on Computer Vision, 2019, pp. 9277\u20139286. 3   \n[23] C. R. Qi, W. Liu, C. Wu, H. Su, and L. J. Guibas, \u201cFrustum pointnets for 3d object detection from rgb-d data,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 918\u2013927. 3   \n[24] C. R. Qi, L. Yi, H. Su, and L. J. Guibas, \u201cPointnet+ $^+$ : Deep hierarchical feature learning on point sets in a metric space,\u201d Advances in Neural Information Processing Systems (NeurIPS), vol. 30, 2017. 3   \n[25] C. Saltori, S. Lathuili\u00e9re, N. Sebe, E. Ricci, and F. Galasso, \u201cSf-uda 3d: Source-free unsupervised domain adaptation for lidar-based 3d object detection,\u201d in 2020 International Conference on 3D Vision (3DV). IEEE, 2020, pp. 771\u2013780. 1, 3   \n[26] G. Shi, R. Li, and C. Ma, \u201cPillarnet: Real-time and high-performance pillar-based 3d object detection,\u201d in European Conference on Computer Vision. Springer, 2022, pp. 35\u201352. 3, 6   \n[27] S. Shi, C. Guo, L. Jiang, Z. Wang, J. Shi, X. Wang, and H. Li, \u201cPv-rcnn: Point-voxel feature set abstraction for 3d object detection,\u201d in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2020, pp. 10 529\u201310 538. 1, 3   \n[28] S. Shi, L. Jiang, J. Deng, Z. Wang, C. Guo, J. Shi, X. Wang, and H. Li, \u201cPV-RCNN $^{++}$ : Point-Voxel Feature Set Abstraction With Local Vector Representation for 3D Object Detection,\u201d arXiv:2102.00463, 2021. 3   \n[29] S. Shi, X. Wang, and H. Li, \u201cPointrcnn: 3d object proposal generation and detection from point cloud,\u201d in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2019, pp. 770\u2013779. 3   \n[30] S. Shi, Z. Wang, J. Shi, X. Wang, and H. Li, \u201cFrom points to parts: 3d object detection from point cloud with part-aware and part-aggregation network,\u201d IEEE transactions on pattern analysis and machine intelligence, vol. 43, no. 8, pp. 2647\u20132664, 2020. 3, 6   \n[31] S. Shin, S. Golodetz, M. Vankadari, K. Zhou, A. Markham, and N. Trigoni, \u201cSample, crop, track: Selfsupervised mobile 3d object detection for urban driving lidar,\u201d in 2023 IEEE International Conference on Robotics and Automation (ICRA), 2023. 1   \n[32] S. Shin, K. Zhou, M. Vankadari, A. Markham, and N. Trigoni, \u201cSpherical mask: Coarse-to-fine 3d point cloud instance segmentation with spherical representation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 1   \n[33] P. Sun, H. Kretzschmar, X. Dotiwalla, A. Chouard, V. Patnaik, P. Tsui, J. Guo, Y. Zhou, Y. Chai, B. Caine, V. Vasudevan, W. Han, J. Ngiam, H. Zhao, A. Timofeev, S. Ettinger, M. Krivokon, A. Gao, A. Joshi, Y. Zhang, J. Shlens, Z. Chen, and D. Anguelov, \u201cScalability in perception for autonomous driving: Waymo open dataset,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020. 7   \n[34] P. Sun, M. Tan, W. Wang, C. Liu, F. Xia, Z. Leng, and D. Anguelov, \u201cSwformer: Sparse window transformer for 3d object detection in point clouds,\u201d in European Conference on Computer Vision. Springer, 2022, pp. 426\u2013442. 3   \n[35] O. D. Team, \u201cOpenpcdet: An open-source toolbox for 3d object detection from point clouds,\u201d https: //github.com/open-mmlab/OpenPCDet, 2020. 7   \n[36] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin, \u201cAttention is all you need,\u201d Advances in neural information processing systems, vol. 30, 2017. 6   \n[37] H. Wang, C. Shi, S. Shi, M. Lei, S. Wang, D. He, B. Schiele, and L. Wang, \u201cDsvt: Dynamic sparse voxel transformer with rotated sets,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 13 520\u201313 529. 3   \n[38] Y. Wang, X. Chen, Y. You, L. E. Li, B. Hariharan, M. Campbell, K. Q. Weinberger, and W.-L. Chao, \u201cTrain in germany, test in the usa: Making 3d object detectors generalize,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 11 713\u201311 723. 1, 2, 3, 7, 8   \n[39] Z. Wang, Y.-L. Li, X. Chen, H. Zhao, and S. Wang, \u201cUni3detr: Unified 3d detection transformer,\u201d Advances in Neural Information Processing Systems, vol. 36, 2024. 3   \n[40] Y. Wei, Z. Wei, Y. Rao, J. Li, J. Zhou, and J. Lu, \u201cLidar distillation: Bridging the beam-induced domain gap for 3d object detection,\u201d in European Conference on Computer Vision. Springer, 2022, pp. 179\u2013195. 1, 3   \n[41] Q. Xu, Y. Zhou, W. Wang, C. R. Qi, and D. Anguelov, \u201cSpg: Unsupervised domain adaptation for 3d object detection via semantic point generation,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 15 446\u201315 456. 1, 3   \n[42] Y. Yan, Y. Mao, and B. Li, \u201cSecond: Sparsely embedded convolutional detection,\u201d Sensors, vol. 18, no. 10, p. 3337, 2018. 1, 3, 6, 7, 9   \n[43] J. Yang, S. Shi, Z. Wang, H. Li, and X. Qi, \u201cSt3d $^{++}$ : Denoised self-training for unsupervised domain adaptation on 3d object detection,\u201d arXiv preprint arXiv:2108.06682, 2021. 1, 2, 3, 5, 6, 7, 8   \n[44] J. Yang, S. Shi, Z. Wang, and X. Qi, \u201cSt3d: Self-training for unsupervised domain adaptation on 3d object detection,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021. 1, 2, 3, 5, 6, 7, 8, 14, 15   \n[45] Z. Yang, Y. Sun, S. Liu, and J. Jia, \u201c3dssd: Point-based 3d single stage object detector,\u201d in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2020, pp. 11 040\u201311 048. 3   \n[46] Z. Yihan, C. Wang, Y. Wang, H. Xu, C. Ye, Z. Yang, and C. Ma, \u201cLearning transferable features for point cloud detection via 3d contrastive co-training,\u201d Advances in Neural Information Processing Systems, vol. 34, pp. 21 493\u201321 504, 2021. 1, 3, 7, 8   \n[47] B. Zhang and P. Wonka, \u201cPoint cloud instance segmentation using probabilistic embeddings,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 8883\u20138892. 7   \n[48] G. Zhang, C. Junnan, G. Gao, J. Li, and X. Hu, \u201cHednet: A hierarchical encoder-decoder network for 3d object detection in point clouds,\u201d Advances in Neural Information Processing Systems, vol. 36, 2024. 3, 6   \n[49] J. Zhang, J. Huang, Z. Luo, G. Zhang, X. Zhang, and S. Lu, \u201cDa-detr: Domain adaptive detection transformer with information fusion,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 23 787\u201323 798. 3   \n[50] W. Zhao, Y. Yan, C. Yang, J. Ye, X. Yang, and K. Huang, \u201cDivide and conquer: 3d point cloud instance segmentation with point-wise binarization,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2023. 1   \n[51] Y. Zhou and O. Tuzel, \u201cVoxelnet: End-to-end learning for point cloud based 3d object detection,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 4490\u20134499. 3 ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "B Supplementary Material ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this supplementary material, we provide the following additional information: ", "page_idx": 13}, {"type": "text", "text": "\u2022 Details of Self-Training Implementation \u2022 Additional Qualitative Results \u2022 Multi-Class Adaptation Results ", "page_idx": 13}, {"type": "text", "text": "B.1 Details of Self-Training Implementation ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In the self-training framework [44], a batch consists of an identical number of point cloud scans and labels from the source and target domains to train the base 3D detector. For training, the ground-truths from source domain and pseudo-labels from the target domain are considered the same. For each cycle of the self-training, we train our system for 2 epochs using $L$ in Eqn. 11 and collect the pseudo-label sets to merge into $Y$ using the model\u2019s inference. Here, the collection of the pseudo-labels takes place in the training set of the target domain. Utilizing the new $Y$ , we progressively train the detector for $n_{s e}$ epochs again and iterate the same process. The confidence score of the detection to be considered as the pseudo label is set to 0.5, and $n_{s e}$ is set to 2 epochs. ", "page_idx": 13}, {"type": "text", "text": "B.2 Additional Qualitative Results ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Figure II shows additional qualitative results on Waymo $\\rightarrow$ KITTI and Waymo $\\rightarrow$ NuScenes. For all the qualitative results of previous works, we only include the results that reproduce the reported results on their papers based on the published source codes for each dataset at the time of submission ", "page_idx": 13}, {"type": "image", "img_path": "YEtirXhsh1/tmp/221bfeb86d4d1ebf8fdf109f9e61e23dcaeec0865198b9a2a169fbee29b4e563.jpg", "img_caption": [], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "Figure II: Qualitative comparison of Baseline ST3D [44], DTS [8] and ours on Waymo to KITTI adaptation scenario (top) and Waymo to NuScenes(bottom) adaptation scenarios. ", "page_idx": 13}, {"type": "text", "text": "B.3 Multi-class Adaptation ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Table II shows the result of multi-class adaptation. As our method is designed as a simple add-on that can be applied to general existing pipelines, we apply our pipeline on top of the multi-class adaption approach, REDB [3]. As can be seen, our GroupEXP-DA and REDB show synergy, improving the performance further. ", "page_idx": 14}, {"type": "table", "img_path": "YEtirXhsh1/tmp/a2176a4a11e71e696597b57f9398959f349d383060b72efd19585432b3459358.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "Table II: Comparison with multi-class adaptation setting for NuScenes $\\rightarrow$ KITTI adaptation task. Here, Ours (SA) refers to the naive extension of single-class adaptation to multi-class adaptation, and Ours $^+$ ReDB stands for the proposed method added on top of ReDB. Here, all of ours use three groups. ", "page_idx": 14}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: The contribution is accurately reflected in the Abstract section. ", "page_idx": 15}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: We involved a section at the end of the paper discussing the limitation. ", "page_idx": 15}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 15}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 15}, {"type": "text", "text": "Justification: does not include theoretical results. ", "page_idx": 15}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: We have implementation detail section discussing the reproducibility in supp.   \nmaterial. ", "page_idx": 15}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: We just use open-sourced data and provide implementation detail. ", "page_idx": 15}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: Train/Test details are given in implementation details section in supp. material ", "page_idx": 15}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 15}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 15}, {"type": "text", "text": "Justification: We do not need to report the error bar. ", "page_idx": 15}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: We include the hardware information for training the model in supp. material. ", "page_idx": 15}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] We follow the Code of Ethics. ", "page_idx": 16}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative   \nsocietal impacts of the work performed?   \nAnswer: [NA] ", "page_idx": 16}, {"type": "text", "text": "Justification: No societal impact involved. ", "page_idx": 16}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 16}, {"type": "text", "text": "Answer: [NA] Justification: No involvement of safegards. ", "page_idx": 16}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] Justification: We run experiment on open-released data. ", "page_idx": 16}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 16}, {"type": "text", "text": "Answer: [NA] Justification: No new Asset ", "page_idx": 16}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 16}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 16}, {"type": "text", "text": "Justification: No Such thing. ", "page_idx": 16}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 16}, {"type": "text", "text": "Answer: [NA] Justification: No such thing. ", "page_idx": 16}]