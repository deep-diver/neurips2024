[{"figure_path": "eM5d7ZmekA/figures/figures_1_1.jpg", "caption": "Figure 1: Image to 3D using GeoLRM. Initially, a 3D-aware diffusion model, specifically SV3D [60], transforms an input image into multiple views. Subsequently, these views are processed by our GeoLRM to generate detailed 3D assets. Unlike other LRM-based approaches, GeoLRM notably improves as the number of input views increases.", "description": "This figure shows the results of converting a single input image into a 3D model using GeoLRM.  The process begins with a 3D-aware diffusion model (SV3D) generating multiple views from the input image. GeoLRM then processes these views to create a high-quality 3D asset. The figure demonstrates how the quality of the 3D model improves with an increasing number of input views, highlighting GeoLRM's advantage over other similar methods.", "section": "1 Introduction"}, {"figure_path": "eM5d7ZmekA/figures/figures_3_1.jpg", "caption": "Figure 2: Pipeline of the proposed GeoLRM, a geometry-powered method for efficient image to 3D reconstruction. The process begins with the transformation of dense tokens into an occupancy grid via a Proposal Transformer, which captures spatial occupancy from hierarchical image features extracted using a combination of a convolutional layer and DINOv2 [38]. Sparse tokens representing occupied voxels are further processed through a Reconstruction Transformer that employs self-attention and deformable cross-attention mechanisms to refine geometry and retrieve texture details with 3D to 2D projection. Finally, the refined 3D tokens are converted into 3D Gaussians for real-time rendering.", "description": "This figure illustrates the pipeline of the GeoLRM model, showing how it efficiently converts images into 3D Gaussians.  It uses a two-stage process: a proposal transformer creates a sparse occupancy grid from input images, and a reconstruction transformer refines this geometry and extracts texture details using deformable cross-attention. The final output is a set of 3D Gaussians suitable for real-time rendering.", "section": "3.2 Model Architecture"}, {"figure_path": "eM5d7ZmekA/figures/figures_7_1.jpg", "caption": "Figure 3: Qualitative comparisons of different image-3D methods. Better viewed when zoomed in.", "description": "This figure presents a qualitative comparison of several image-to-3D reconstruction methods.  Four different objects (a girl, a rabbit, a hammer, and a rabbit on a bicycle) are shown, each rendered using different methods including TripoSR, LGM, CRM, InstantMesh and the proposed GeoLRM.  The results demonstrate the relative visual quality and level of detail achievable by each method. The figure highlights the superior rendering quality of the GeoLRM, particularly noticeable upon zooming in.", "section": "4.4 Qualitative Results"}, {"figure_path": "eM5d7ZmekA/figures/figures_8_1.jpg", "caption": "Figure 4: Qualitative comparison concerning scalability in input views.", "description": "This figure compares the 3D reconstruction results of different methods using varying numbers of input views.  The input image shows a small potted plant.  InstantMesh (using Zero123++) produces a blurry and incomplete model. InstantMesh (using SV3D) provides a slightly better reconstruction, but still suffers from artifacts and a lack of detail. In contrast, the \"Ours (with SV3D)\" reconstruction is significantly more detailed and accurate, demonstrating improved scalability and performance with increased input views.", "section": "4.4 Qualitative Results"}, {"figure_path": "eM5d7ZmekA/figures/figures_8_2.jpg", "caption": "Figure 5: Effects of excluding high-level and low-level features in the image encoder.", "description": "This ablation study in the paper demonstrates the importance of using both high-level (semantic information like object identity and arrangement) and low-level (texture details such as surface patterns and colors) image features for accurate 3D reconstruction.  Excluding high-level features results in model instability, while omitting low-level features leads to a loss of textural detail.  The figure shows a qualitative comparison of the model's reconstruction with both features included, and with each feature type removed individually.", "section": "4.5 Ablation Study"}, {"figure_path": "eM5d7ZmekA/figures/figures_15_1.jpg", "caption": "Figure A: Image-to-3D generation with mesh extraction results.", "description": "This figure showcases the results of image-to-3D generation using mesh extraction.  It presents four different objects (a mushroom cluster, a phoenix, a robot, and a Na'vi-like character). For each object, the figure displays the original input image, four rendered views from different angles, and the final extracted mesh. The rendered views demonstrate the quality of the 3D model generated from the input image, highlighting its visual realism and detail. The mesh representation provides a geometric understanding of the 3D structure and serves as the foundation for creating various visual representations of the object.", "section": "A.1 Occupancy Ground Truth"}, {"figure_path": "eM5d7ZmekA/figures/figures_16_1.jpg", "caption": "Figure B: Comparison of the generated meshes.", "description": "This figure shows a comparison of 3D mesh reconstruction results from different methods (LGM, CRM, InstantMesh, and Ours) on three example objects: mushrooms, teapots, and robots.  Each row represents a different method, showing the generated meshes from multiple viewpoints. The figure aims to visually demonstrate the relative quality and detail of the meshes generated by each method.", "section": "A.2 Mesh Extraction from 3D Gaussians"}]