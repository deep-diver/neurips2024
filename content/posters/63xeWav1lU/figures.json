[{"figure_path": "63xeWav1lU/figures/figures_1_1.jpg", "caption": "Figure 1: Illustration of (a) self-conflict that exists in conventional pixel-to-point contrastive distillation and (b) our weakly supervised contrastive distillation.", "description": "This figure illustrates the \"self-conflict\" problem in traditional contrastive image-to-LiDAR methods (a) where pixels of the same semantic class but not directly corresponding to an anchor point are treated as negative samples. This neglects the semantic relationships, hindering effective learning. In contrast, the proposed method (b) leverages Visual Foundation Models (VFMs) to generate weak semantic labels. These labels guide the contrastive learning process, ensuring that pixels sharing the same semantic class with the anchor point are treated as positive samples, overcoming the self-conflict issue. This leads to improved 3D representation learning by preserving semantic integrity.", "section": "1 Introduction"}, {"figure_path": "63xeWav1lU/figures/figures_2_1.jpg", "caption": "Figure 2: The overall pipeline of our proposed OLIVINE. The pipeline starts with feature extraction via a trainable 3D backbone and a pre-trained 2D backbone, followed by feature alignment in a common space. The learning is driven by weakly-supervised contrastive distillation with coarse semantic labels, self-supervised distillation of randomly sampled point-pixel pairs, and semantic consistency regularization through the von Mises-Fisher distribution. Besides, our approach is also characterized by the novel sampling strategy of point-pixel pairs addressing spatial and category distribution imbalances.", "description": "This figure presents a detailed overview of the OLIVINE pipeline, which consists of multiple stages. It begins with feature extraction from both point cloud and image inputs using separate backbones. These features are then aligned using projection heads. The learning process involves three key components: weakly supervised contrastive distillation using semantic labels from a Visual Foundation Model, self-supervised contrastive distillation with randomly sampled point-pixel pairs, and semantic consistency regularization with von Mises-Fisher distributions. A novel sampling strategy is implemented to address imbalances in spatial and category distributions.", "section": "3 Proposed Method"}, {"figure_path": "63xeWav1lU/figures/figures_6_1.jpg", "caption": "Figure 8: Qualitative results of fine-tuning on 1% of the nuScenes-lidarseg dataset with different pre-training strategies. Note that the results are shown as error maps on the right, where red points indicate incorrect predictions. Best viewed in color and zoom in for more details.", "description": "This figure displays a comparison of qualitative results from fine-tuning on 1% of the nuScenes-lidarseg dataset using different pre-training strategies (Random, PPKT, SLIDR, and the proposed OLIVINE method).  The results are presented as point clouds, with color-coding indicating the predicted semantic classes. For each method, the ground truth semantic segmentation is shown alongside the predictions, and an error map highlights the differences between prediction and ground truth.  The error map shows incorrect predictions in red, giving a visual representation of the accuracy of each method. The figure shows two distinct examples to better demonstrate the results in different scenes.", "section": "4.1 Transfer on Semantic Segmentation"}, {"figure_path": "63xeWav1lU/figures/figures_9_1.jpg", "caption": "Figure 3: The visual results of various point cloud pretraining strategies, pre-trained on nuScenes and fine-tuned using merely 1% of annotated data, are displayed. To illustrate the distinctions, we mark correctly predicted areas in gray color and incorrect ones in red.", "description": "This figure compares the performance of different point cloud pre-training methods on semantic segmentation using only 1% of the annotated data from the nuScenes dataset.  The results are visualized using ground truth (a), random initialization (b), PPKT (c), SLidR (d) and the proposed OLIVINE method (e).  Correctly predicted areas are shown in gray, while incorrectly predicted areas are in red, illustrating the differences in accuracy across different pre-training techniques.", "section": "4.1 Transfer on Semantic Segmentation"}, {"figure_path": "63xeWav1lU/figures/figures_19_1.jpg", "caption": "Figure 5: Class distribution at the pixel level for nuScenes dataset.", "description": "This figure shows the class distribution of the nuScenes dataset at the pixel level.  The x-axis represents the different semantic categories present in the dataset, and the y-axis shows the count of pixels belonging to each category. The bar chart visually represents the class imbalance in the dataset, where some categories have significantly more pixel instances than others.  This imbalance is a common characteristic in real-world datasets and can impact the performance of machine learning models that are trained using such data.", "section": "A.3 Experimental Setup of 3D Pretraining"}, {"figure_path": "63xeWav1lU/figures/figures_20_1.jpg", "caption": "Figure 5: Class distribution at the pixel level for nuScenes dataset.", "description": "This bar chart visualizes the frequency distribution of different semantic classes within the nuScenes dataset at the pixel level.  The x-axis lists the various semantic categories present in the dataset, while the y-axis represents the count of pixels belonging to each category.  The chart clearly shows an imbalanced class distribution, with some categories having significantly more pixels than others. This highlights the challenge of class imbalance in the dataset, which the paper addresses with its proposed density and category-aware sampling strategy.", "section": "A.3 Experimental Setup of 3D Pretraining"}, {"figure_path": "63xeWav1lU/figures/figures_23_1.jpg", "caption": "Figure 7: Visualization of the similarities between image and point cloud feature. In the first column, we show the raw image and the projection of anchor point in the image. In second columns, we illustrate the similarities between 3D query and 2D features extracted by the conventional projection heads hp and hD for point-pixel level contrastive distillation. In third columns, we illustrate the similarities between 3D query and 2D features extracted by the extra projection heads him and hom for weakly-supervised (category-aware) contrastive distillation.", "description": "This figure visualizes the similarities between image and point cloud features using different projection heads. The first column shows the raw image and the location of an anchor point. The second column displays feature similarities from conventional projection heads used for point-pixel contrastive distillation (only directly corresponding pixels show high similarity). The third column shows feature similarities from additional projection heads designed for weakly-supervised (category-aware) contrastive distillation, where points and pixels of the same semantic category exhibit higher similarity.", "section": "4.5 Visualization and Analysis"}, {"figure_path": "63xeWav1lU/figures/figures_26_1.jpg", "caption": "Figure 8: Qualitative results of fine-tuning on 1% of the nuScenes-lidarseg dataset with different pre-training strategies. Note that the results are shown as error maps on the right, where red points indicate incorrect predictions. Best viewed in color and zoom in for more details.", "description": "This figure presents a qualitative comparison of the results obtained from fine-tuning on 1% of the nuScenes-lidarseg dataset using different pre-training methods.  The image shows the ground truth, the model predictions, and an error map highlighting incorrect predictions (red). The comparison includes results using random initialization, the SLidR method, and the proposed OLIVINE method. The results are best viewed in color and zoomed in for more details.", "section": "4.1 Transfer on Semantic Segmentation"}, {"figure_path": "63xeWav1lU/figures/figures_27_1.jpg", "caption": "Figure 8: Qualitative results of fine-tuning on 1% of the nuScenes-lidarseg dataset with different pre-training strategies. Note that the results are shown as error maps on the right, where red points indicate incorrect predictions. Best viewed in color and zoom in for more details.", "description": "This figure presents a qualitative comparison of the semantic segmentation results obtained using three different pre-training strategies (Random, SLidR, and OLIVINE) on the nuScenes-lidarseg dataset. Only 1% of the annotated data was used for fine-tuning. The results are visualized using error maps, where red points highlight incorrect predictions. The figure showcases the improved accuracy of the OLIVINE method compared to the baseline methods. ", "section": "4.1 Transfer on Semantic Segmentation"}, {"figure_path": "63xeWav1lU/figures/figures_28_1.jpg", "caption": "Figure 8: Qualitative results of fine-tuning on 1% of the nuScenes-lidarseg dataset with different pre-training strategies. Note that the results are shown as error maps on the right, where red points indicate incorrect predictions. Best viewed in color and zoom in for more details.", "description": "This figure shows a qualitative comparison of semantic segmentation results on the nuScenes-lidarseg dataset using different pre-training strategies (Random, SLidR, and OLIVINE).  The results are displayed in three columns: ground truth, prediction, and an error map.  The error map highlights incorrect predictions in red, allowing for a visual assessment of the effectiveness of each pre-training method. The image is best viewed in color and zoomed in for detailed examination.", "section": "4.1 Transfer on Semantic Segmentation"}, {"figure_path": "63xeWav1lU/figures/figures_29_1.jpg", "caption": "Figure 8: Qualitative results of fine-tuning on 1% of the nuScenes-lidarseg dataset with different pre-training strategies. Note that the results are shown as error maps on the right, where red points indicate incorrect predictions. Best viewed in color and zoom in for more details.", "description": "This figure presents a qualitative comparison of semantic segmentation results on the nuScenes-lidarseg dataset using different pre-training methods. The top row shows the ground truth, the middle row shows the predictions, and the bottom row shows the error maps. The error maps highlight the incorrect predictions in red.  The comparison showcases the improvements achieved by the proposed OLIVINE method compared to baseline methods (Random and SLidR) when fine-tuned with limited data (1%).", "section": "4.1 Transfer on Semantic Segmentation"}, {"figure_path": "63xeWav1lU/figures/figures_30_1.jpg", "caption": "Figure 12: Illustration of the weak semantic labels predicted by Grounded-SAM. The top half of the figure displays the raw RGB images and LiDAR point clouds, while the bottom half presents the corresponding weak semantic labels applied to both images and point clouds, aligned using camera parameters. Each distinct segment is represented by a unique color. Best viewed in color.", "description": "This figure shows a comparison between raw data (RGB images and LiDAR point clouds) and the semantic segmentation results produced by the Grounded-SAM model. The top half displays the original data, while the bottom half shows the semantic segmentation, where each segment is color-coded for easy identification.  The alignment of the labels highlights how the model associates image regions with corresponding LiDAR points.", "section": "4.5 Visualization and Analysis"}, {"figure_path": "63xeWav1lU/figures/figures_30_2.jpg", "caption": "Figure 12: Illustration of the weak semantic labels predicted by Grounded SAM. The top half of the figure displays the raw RGB images and LiDAR point clouds, while the bottom half presents the corresponding weak semantic labels applied to both images and point clouds, aligned using camera parameters. Each distinct segment is represented by a unique color. Best viewed in color.", "description": "This figure shows the weak semantic labels generated by the Grounded SAM model applied to both RGB images and LiDAR point clouds.  The top half displays the original data; the bottom half shows the corresponding semantic segmentation, where each segment is color-coded for easy identification. The alignment of the labels with the images and point clouds is based on camera parameters.", "section": "4.5 Visualization and Analysis"}]