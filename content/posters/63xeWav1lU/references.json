{"references": [{"fullname_first_author": "Alexander Kirillov", "paper_title": "Segment Anything", "publication_date": "2023-10-26", "reason": "This paper introduces the Segment Anything Model (SAM), a foundation model for image segmentation that is crucial to the proposed method's weakly-supervised contrastive distillation approach."}, {"fullname_first_author": "Yueh-Cheng Liu", "paper_title": "Learning from 2D: Contrastive pixel-to-point knowledge transfer for 3D pretraining", "publication_date": "2021-04-13", "reason": "This is the baseline method for the paper, providing the foundation for contrastive image-to-LiDAR knowledge transfer that is improved upon in the proposed work."}, {"fullname_first_author": "Corentin Sautier", "paper_title": "Image-to-lidar self-supervised distillation for autonomous driving data", "publication_date": "2022-06-20", "reason": "This paper proposes superpixel-based contrastive loss for image-to-LiDAR distillation, which is a relevant prior work the proposed method builds on and improves."}, {"fullname_first_author": "Anas Mahmoud", "paper_title": "Self-supervised image-to-point distillation via semantically tolerant contrastive loss", "publication_date": "2023-06-20", "reason": "This paper addresses the self-conflict issue in contrastive distillation by introducing a semantically tolerant loss, a problem directly addressed in the proposed method."}, {"fullname_first_author": "Youquan Liu", "paper_title": "Segment Any Point Cloud Sequences by Distilling Vision Foundation Models", "publication_date": "2024-01-01", "reason": "This paper, also published in NeurIPS 2024, uses visual foundation models for point cloud segmentation, offering a similar approach to the proposed method, providing a relevant comparison."}]}