[{"figure_path": "twpPD9UMUN/figures/figures_1_1.jpg", "caption": "Figure 1: The question in current AVQA datasets is generated by a limited set of predefined templates, which may not be in line with the real-world scenario. Our findings indicate that existing methods [5, 1] such as STG [4] are not robust, which may be attributed to excessive bias learning, such as memorizing statistical regularities between critical question words and answers.", "description": "This figure shows that current AVQA datasets use a limited set of predefined question templates.  This can lead to models overfitting to these templates and not generalizing well to real-world scenarios. The example shows that two questions that differ only by one word (\u201cukulele\u201d vs. \u201cflute\u201d) produce different answers from the STG method, while the proposed \u201cOurs\u201d method handles both correctly, highlighting the issue of bias learning in existing AVQA approaches.", "section": "1 Introduction"}, {"figure_path": "twpPD9UMUN/figures/figures_3_1.jpg", "caption": "Figure 2: Distribution of rephrasing questions based on the first three words.", "description": "This figure shows a circular visualization representing the distribution of rephrased questions in the MUSIC-AVQA-R dataset.  The distribution is categorized and displayed based on the first three words of each question. The size of each segment corresponds to the number of questions starting with that particular three-word sequence.  This visualization effectively demonstrates the diversity and variety of question phrasings achieved through the rephrasing process, showing that MUSIC-AVQA-R is not limited to the predefined templates of the original MUSIC-AVQA dataset.", "section": "3 Dataset Creation and Analysis"}, {"figure_path": "twpPD9UMUN/figures/figures_4_1.jpg", "caption": "Figure 3: Statistics visualization for MUSIC-AVQA-R. \u03bc(\u03b1) is the average number of answers in a group. The dark color on the right denotes the number of head samples, while the light-colored area denotes that of tail samples.", "description": "This figure visualizes the answer distribution in the MUSIC-AVQA-R dataset.  Panel (a) shows the distribution of answers for \"Temporal\" questions, highlighting the long-tailed nature of the distribution. Panel (b) presents a comparison of the number of head (frequent) and tail (rare) samples across different question types (Existential, Location, Counting, Comparative, Temporal). The dark and light colors represent the head and tail samples respectively. The figure demonstrates the dataset's ability to capture both frequent and rare answer distributions, making it suitable for evaluating the robustness of AVQA models.", "section": "3 Dataset Creation and Analysis"}, {"figure_path": "twpPD9UMUN/figures/figures_5_1.jpg", "caption": "Figure 4: Robust AVQA architecture to overcome bias learning. Our MCCD strategy is plug-and-play, allowing seamless integration with other AVQA methods.", "description": "This figure illustrates the architecture of a robust Audio-Visual Question Answering (AVQA) system designed to mitigate bias learning.  The architecture consists of several key components. First, it performs unimodal embedding, creating separate representations for the audio, video, and text components of the input.  Next, it generates uni-modal and multi-modal representations using a pre-trained model (likely a Vision-Language Transformer).  Then, it employs unimodal bias learning to capture biases specific to each modality (audio, video, and text). Finally, it uses a collaborative debiasing strategy, namely Multifaceted Cycle Collaborative Debiasing (MCCD), to address these biases.  MCCD works by enlarging the dissimilarity between uni-modal and multi-modal logits while employing cycle guidance to ensure the similarity of uni-modal logit distributions. The final output is the AVQA answer.  The architecture is designed to be plug-and-play, meaning it can be easily integrated with other AVQA methods.", "section": "4 Method"}, {"figure_path": "twpPD9UMUN/figures/figures_9_1.jpg", "caption": "Figure 5: Sensitivity and qualitative analysis. \u03b1 and \u03b2 are the weight-controlling factors in the MCCD strategy. We visualize attention weights on the uniformly sampled audio and video frames.", "description": "This figure shows the results of a sensitivity analysis and a qualitative analysis performed to evaluate the effectiveness of the proposed Multifaceted Cycle Collaborative Debiasing (MCCD) strategy. The sensitivity analysis assesses the impact of varying the weight-controlling factors (\u03b1 and \u03b2) on the MCCD strategy's performance. The qualitative analysis visualizes the attention weights assigned to uniformly sampled audio and video frames to understand the method's ability to focus on relevant parts of the input when making predictions. The visualization helps in understanding how the MCCD method uses the audio and visual cues effectively in the audio-visual question answering task.", "section": "5.7 Sensitivity and Qualitative Analysis"}, {"figure_path": "twpPD9UMUN/figures/figures_15_1.jpg", "caption": "Figure 6: Distribution visualization of questions based on the first three words.", "description": "This figure compares the distribution of the first three words in questions from the MUSIC-AVQA-R and MUSIC-AVQA datasets.  The MUSIC-AVQA-R dataset shows a much wider variety of question beginnings, indicating greater diversity and a more realistic representation of natural language questions compared to the original MUSIC-AVQA dataset. This visualization highlights one aspect of how the MUSIC-AVQA-R dataset was designed to be more robust and less prone to bias.", "section": "3 Dataset Creation and Analysis"}, {"figure_path": "twpPD9UMUN/figures/figures_15_2.jpg", "caption": "Figure 3: Statistics visualization for MUSIC-AVQA-R. \u03bc(\u03b1) is the average number of answers in a group. The dark color on the right denotes the number of head samples, while the light-colored area denotes that of tail samples.", "description": "This figure visualizes the statistics of the MUSIC-AVQA-R dataset, showing the distribution of answers for different question types.  Panel (a) shows the answer distribution for Temporal questions, illustrating a long-tailed distribution. Panel (b) shows the statistics of head (frequent) and tail (rare) samples across different question types, indicating the distribution shift introduced in the dataset for robustness evaluation. The dark and light colors represent the head and tail samples respectively. \u03bc(\u03b1) represents the average number of answers within each group.", "section": "3 Dataset Creation and Analysis"}, {"figure_path": "twpPD9UMUN/figures/figures_16_1.jpg", "caption": "Figure 3: Statistics visualization for MUSIC-AVQA-R. \u03bc(\u03b1) is the average number of answers in a group. The dark color on the right denotes the number of head samples, while the light-colored area denotes that of tail samples.", "description": "This figure visualizes the answer distribution statistics in MUSIC-AVQA-R dataset.  Panel (a) shows the answer distribution for \"Temporal\" questions, highlighting the long-tailed nature of the distribution. Panel (b) presents a comparison of head and tail sample statistics across various question types (\"Counting\", \"Comparative\", \"Existential\", \"Location\", \"Multi-modal Scene\"). The dark color represents the number of head (frequent) samples, while the lighter color shows the number of tail (rare) samples. The average number of answers per group is denoted by \u03bc(\u03b1). This figure is crucial for understanding the distribution shift introduced in the dataset for evaluating model robustness.", "section": "3 Dataset Creation and Analysis"}, {"figure_path": "twpPD9UMUN/figures/figures_16_2.jpg", "caption": "Figure 3: Statistics visualization for MUSIC-AVQA-R. \u03bc(\u03b1) is the average number of answers in a group. The dark color on the right denotes the number of head samples, while the light-colored area denotes that of tail samples.", "description": "This figure visualizes the statistics of the MUSIC-AVQA-R dataset, showing the distribution of answers for different question types.  Panel (a) displays the answer distribution for \"Temporal\" questions, illustrating a long-tailed distribution where some answers are much more frequent than others. Panel (b) provides a summary of the head and tail samples for various question types (Existential, Location, Counting, Comparative, and Temporal). The dark color represents head samples (frequent answers), and the light color represents tail samples (rare answers).  The y-axis shows the number of samples, and the x-axis shows the different answer categories or question types.", "section": "3 Dataset Creation and Analysis"}, {"figure_path": "twpPD9UMUN/figures/figures_18_1.jpg", "caption": "Figure 1: The question in current AVQA datasets is generated by a limited set of predefined templates, which may not be in line with the real-world scenario. Our findings indicate that existing methods [5, 1] such as STG [4] are not robust, which may be attributed to excessive bias learning, such as memorizing statistical regularities between critical question words and answers.", "description": "This figure illustrates a limitation of existing Audio-Visual Question Answering (AVQA) datasets.  Current datasets use a limited number of predefined question templates.  This leads to a lack of diversity in the questions and causes models to overlearn biases, affecting their robustness in real-world scenarios.  The figure shows examples of questions from a standard AVQA dataset (STG) compared to a more naturally phrased question. The standard questions rely on templates such as \"Is the <Object> in the video always playing?\" This limited variety leads to bias in trained models.", "section": "1 Introduction"}, {"figure_path": "twpPD9UMUN/figures/figures_20_1.jpg", "caption": "Figure 3: Statistics visualization for MUSIC-AVQA-R. \u03bc(\u03b1) is the average number of answers in a group. The dark color on the right denotes the number of head samples, while the light-colored area denotes that of tail samples.", "description": "This figure visualizes the statistics of the MUSIC-AVQA-R dataset, showing the distribution of answers for different question types.  The left panel (a) displays the distribution of answers for \"Temporal\" questions, highlighting the long-tailed nature of the data. The right panel (b) shows the number of head (frequent) and tail (rare) samples for various question types, indicating a distribution shift introduced for robustness evaluation.", "section": "3 Dataset Creation and Analysis"}]