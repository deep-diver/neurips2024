[{"type": "text", "text": "TrajCLIP: Pedestrian Trajectory Prediction Method Using Contrastive Learning and Idempotent Networks ", "text_level": 1, "page_idx": 0}, {"type": "image", "img_path": "fUBFy8tb3z/tmp/4378f3aa3e40cb3e14d77544af9c880c8c28cc8f9b9931c7cbe4d449b614fe4e.jpg", "img_caption": [], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "1Institute of Computing Technology, University of Chinese Academy of Sciences 2Beijing University of Posts and Telecommunications 3SenseTime 4Beijing Key Laboratory of Mobile Computing and Pervasive Device ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The distribution of pedestrian trajectories is highly complex and influenced by the scene, nearby pedestrians, and subjective intentions. This complexity presents challenges for modeling and generalizing trajectory prediction. Previous methods modeled the feature space of future trajectories based on the high-dimensional feature space of historical trajectories, but this approach is suboptimal because it overlooks the similarity between historical and future trajectories. Our proposed method, TrajCLIP, utilizes contrastive learning and idempotent generative networks to address this issue. By pairing historical and future trajectories and applying contrastive learning on the encoded feature space, we enforce same-space consistency constraints. To manage complex distributions, we use idempotent loss and tightness loss to control over-expansion in the latent space. Additionally, we have developed a trajectory interpolation algorithm and synthetic trajectory data to enhance model capacity and improve generalization. Experimental results on public datasets demonstrate that TrajCLIP achieves state-of-the-art performance and excels in scene-to-scene transfer, few-shot transfer, and online learning tasks. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Pedestrian trajectory prediction involves predicting future paths based on observed historical trajectories. This task is crucial in real-world applications such as autonomous driving [4, 13], and robotics [11]. It is challenging to model diverse trajectory distributions across different scenes (e.g., roundabouts and intersections), various patterns (e.g., staggering and strolling), and different interactions (e.g., walking together or avoiding strangers) due to their complexity. ", "page_idx": 0}, {"type": "text", "text": "The existing methods [31, 21, 6] assume that the training and test motions follow the same distribution to effectively model complex distributions within a specific range. These approaches focus on modeling the high-dimensional feature space distribution of trajectories within a particular dataset. They use generative models, such as Conditional Variational AutoEncoder [23, 24] and diffusion, to change the feature space of past trajectories into the feature space of future trajectories. This process aids in generating predicted trajectories and ftiting the distribution of pedestrian trajectories to specific datasets. However, as shown in Fig. 1, this approach has limitations. It involves reducing the feature space to match the distribution of a specific dataset to improve prediction accuracy. Nonetheless, this projection operation creates a gap in the feature space between the historical and future trajectories, limiting the model\u2019s ability to make predictions across different scenarios and to adapt to new scenarios. ", "page_idx": 0}, {"type": "image", "img_path": "fUBFy8tb3z/tmp/9b00224ca4d31c9a1a47c2850bc047a17d4dc8a9d15faf534433085b88a83731.jpg", "img_caption": ["Figure 1: The existing methods use an encoder to sample from a normal distribution in order to generate predicted trajectories. However, this approach leads to different feature spaces for historical and future trajectories, causing problems during sampling, such as sharp turns or predictions of positions far exceeding normal speeds. Our method ensures consistency between the two feature spaces, enabling position prediction through affine transformations and preserving spatio-temporal continuity. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "We believe that predicting future trajectories in trajectory prediction tasks is significantly different from tasks such as classification [32] and tracking [30], where the input and output are inconsistent. In trajectory prediction, the feature space of the input and output remains consistent. Therefore, mapping historical trajectory features to future trajectory features in the same feature space can effectively bridge the feature space gap and model complex feature space distribution more accurately. ", "page_idx": 1}, {"type": "text", "text": "We have developed a method called TrajCLIP for predicting trajectories, which is based on contrastive learning and idempotent neural networks. This method was inspired by the approach used in the paper by [18], which involves limiting the feature space of heterogeneous data. To achieve this, we created a trajectory feature encoder that utilizes frequency domain additivity and Fourier transforms reversibility to combine trajectory Interaction Features (STIF) in the time domain and Interaction Features (SAIF) between agents in the frequency domain. We utilized cross-entropy loss for constructive learning between historical trajectory and future trajectory encoders to ensure that the feature spaces of the two encoders are the same. Subsequently, we employed an idempotent generation network (IGN) as a global feature mapping framework to map the same set of features from past trajectory features to future trajectory features. Additionally, we constrained the growth of the specific distribution to allow for more general usage. Finally, the mapped future trajectory features were decoded into predicted trajectories. We also introduced a neighbor-based interpolation algorithm and used a trajectory generation model to create a synthetic trajectory to enhance the model\u2019s ability to describe complex distributions. Furthermore, we trained a lightweight version (TrajCLIP-tiny) without these augmentations. We conducted experimental comparisons with existing trajectory prediction methods on various tasks and datasets. The experiments demonstrated that our method achieves state-of-the-art accuracy and outperforms existing methods in cross-scene generalization, small sample transfer, and online tasks. ", "page_idx": 1}, {"type": "text", "text": "The contributions of our work are summarized as follows: (1) In this study, TrajCLIP is presented as a proposed method. The model effectively represents and generalizes complex feature distributions by integrating the feature spaces of historical and future trajectories. An idempotent neural network with global feature mapping capabilities is utilized, enabling strong cross-scene generalization abilities. (2) We proposed a novel trajectory feature encoder combines features from both the time domain and the frequency domain using STIF and SAIF to model pedestrian trajectories. (3) We have created a novel method for measuring trajectory similarity, which we use as a label for contrastive learning. This helps to narrow down the feature space of past and future trajectories. To our knowledge, this is the first application of contrastive learning to trajectory prediction. This is the first work that considers the feature space alignment between the history and future trajectory. Our experiments demonstrate that TrajCLIP performs exceptionally well across different tasks and datasets. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Pedestrian Trajectory Prediction ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Pedestrian trajectory prediction is approached as a sequence-to-sequence challenge. Social-LSTM [1] first uses LSTM to predict trajectories deterministically. Social-GAN [7] uses a Generative Adversarial Network (GAN) to make many possible future predictions for an input trajectory while taking into account the inherent multimodality of human motion. Trajectron $^{++}$ [21] takes in motion trajectories and heterogeneous data, and it handles multi-modality by using CVAE to sample latent variables in high-dimensional space. AgentFormer [31] proposes agent-aware attention to simultaneously capture the spatiotemporal features of trajectories and improves the sampling method of the CVAE framework to model more intricate trajectory modalities. MID [6] was the first to use diffusion, which gradually gets rid of uncertainty in all walkable areas by using a high-dimensional denoiser until the desired path is reached. A new model for predicting trajectories called Flow-Chain [14] uses normalizing flow to quickly figure out the probability distribution for each point on the trajectory. TUTR [22] employs a unified transformer to model trajectories, directly providing the predicted trajectory and the probability of each mode, which effectively eliminates the need for post-processing. Long-tail analysis [15] uses Kalman filter to directly predict output trajectory errors as a basis, distinguishing difficult and easy samples. It then conducts contrastive learning on difficult (long-tail part) historical trajectories to enhance the feature representation capabilities of the long-tail part of the dataset. FEND [25] uses VAE to extract features from the entire trajectory and performs offline clustering on the features to obtain labels; during training, it uses offilne labels for contrastive learning on historical trajectories to guide the feature generation of historical trajectories and enhancing the prediction of the long-tail part. However, projecting past trajectory features into separate feature spaces creates a feature space gap that restricts the model\u2019s capacity to generalize and accurately represent complex trajectory distributions. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2.2 Generative Framework ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Compared with the deterministic framework, the generative framework focuses on modeling data distribution in the dataset and is well-suited for tasks involving one input and multiple outputs. GAN [5] introduces a generative adversarial network, allowing the generator to model the real data distribution through adversarial learning with the discriminator. VAE [12] introduces latent variables, uses the encoder to construct the latent feature space representation of the data, and uses the decoder to restore and reconstruct the feature tensor. CVAE [24] adds a control signal on top of VAE [12]. This signal changes the variational lower bound in the conditional probability state, which lets the model control the generated data distribution based on the signal that goes into it. Diffusion [9] models the data distribution by simulating the diffusion process in high-dimensional space. Adding high-dimensional denoisers gradually lowers uncertainty about the data distribution, leading to samples that are similar to the training data. ", "page_idx": 2}, {"type": "text", "text": "In these methods, there is a separation between the feature spaces of the control signal and reconstructed data. However, future and historical trajectory features should be in the same feature space, and modeling these independently will lead to a natural gap. IGN [23] aims to model global feature mapping, which constrains the distribution of specific datasets on the expected manifold through idempotent loss and compaction loss. This approach prevents the model from being overftited to the distribution of specific datasets and enables data generation within the same feature space. DALLE [19] uses CLIP [18] to align the different feature spaces of text and images. It then uses the encoded feature vector of the text as a control signal to make different kinds of images using GLIDE [17]. We utilize contrastive learning to ensure consistency between the feature spaces of historical trajectory and future trajectory, based on the concept of aligning feature spaces. Subsequently, we employ IGN mapping to generate trajectories. ", "page_idx": 2}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Problem Formulation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We generalize the trajectory prediction task as follows. Given a scenario comprising $N$ agents, the historical information for each agent includes the two-dimensional coordinate position $p_{t}^{n}=\\left(x_{t}^{n},y_{t}^{n}\\right)$ of agent n at time t. The trajectory prediction task is to predict future position sequence p\u02c6tn0+1\u223ct0+Tf based on the observed historical agent position sequence ptn0\u2212Th+1\u223ct0. ", "page_idx": 2}, {"type": "text", "text": "3.2 Overall Framework ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We propose a trajectory prediction method called TrajCLIP, which uses contrastive learning and and idempotent generative network, as illustrated in Fig. 2. Our TrajCLIP comprises four parts: (1). A trajectory encoder that models interactions between agents, scene-agent interactions in the frequency domain, and trajectory spatiotemporal characteristics in the time domain. (2). A CLIP pairing learning module that uses cross-entropy loss to maintain feature space consistency between past and future trajectories. (3). A feature manifold predictor that employs idempotent and tightness loss to train an IGN for mapping past trajectory features into future trajectory features in the same feature space. (4). A trajectory manifold decoder that uses a pre-trained task decoder from the trajectory encoder to generate predicted trajectories. We will provide detailed introductions to each part in the subsequent sections. Specific implementation details can be found in Sec. 4.2. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "3.3 Data Augmentation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Various data augmentation techniques are used in current methods to enhance the performance of pedestrian trajectory prediction models, including rotation for robustness, scale alignment for training simplicity, and trajectory origin alignment for addressing absolute position ambiguity. In our research, we incorporate minor perturbations (as discussed in [20]) into trajectories to generate more reliable results. We achieve this by simulating scenarios using available datasets, as demonstrated in the study conducted by [33]. This approach allows us to enhance the existing dataset. Additionally, we have developed an interpolation technique that increases the model capacity by integrating neighboring trajectory sample points. ", "page_idx": 3}, {"type": "text", "text": "We formulate the adjacent point interpolation algorithm as illustrated in Fig. 4. If the vectors $\\overrightarrow{p_{t-1}p_{t}^{\\prime}}$ and $\\overrightarrow{p_{t+1}p_{t+2}}$ are on the same side of $\\overrightarrow{p_{t}p_{t+1}}^{\\rangle}$ for four consecutive positions $p_{t-1\\sim t+2}$ , we can calculate the acute angle between the two adjacent trajectories. To derive the interpolated trajectory, the data is divided into three equal segments. Then, we find the intersection point of the third line and connect them. When the vectors are on opposite sides or at the beginning and end of the sequence, the trisect points are chosen as the interpolation points. In the rest of this paper, we will use the symbol ptn\u2212Th+1\u223ct to represent the interpolated trajectory, for simplicity. Despite the potential decrease in accuracy, the generated and interpolated data can still be used to improve the model\u2019s capabilities and capture more complex distributions, considering the multimodal nature of the pedestrian trajectory prediction task. ", "page_idx": 3}, {"type": "text", "text": "3.4 Trajectory Encoder ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In our approach, we use two structurally identical encoders, as shown in Fig. 3. These encoders represent the historical trajectory $H$ and the future trajectory $F$ . We utilize the agent-aware attention $\\varphi_{A t t}$ from Agentformer [31] to extract the STIF, and employ a transformer based on Fourier Transform to extract the SAIF. By fusing these features, we train both encoders to decode future trajectories. Additionally, the historical trajectory encoder aims to accurately predict the subsequent frame position to maintain spatio-temporal continuity. ", "page_idx": 3}, {"type": "text", "text": "We define the time-series set $\\mathcal{T}_{H}=\\{t_{0}-T_{h}+1,...,t_{0}-1,t_{0}\\}$ for the historical trajectory encoder, and $\\mathcal{T}_{F}=\\{t_{0}-1,t_{0},t_{0}+1,...,t_{0}+\\dot{T}_{f}\\}$ for future trajectory encoders. To obtain the spatio-temporal interaction feature (STIF), we arrange the trajectories in a scene into $\\mathbf{X}_{t}=\\left(\\left(p_{t}^{n}\\mid n=1,...,N\\right)\\mid t\\in\\right.$ $\\tau$ ), where time is used as the index. At each time step, we use a straightforward linear mapping to tokenize the $X$ matrix and include positional encoding to maintain the temporal information of the trajectory token sequence. We employ agent-aware attention to extract both the temporal features of each individual trajectory and the interaction features between different trajectories simultaneously. The feature is denoted as $z_{t}$ and can be formally described as follows: ", "page_idx": 3}, {"type": "image", "img_path": "fUBFy8tb3z/tmp/7b606ddc4c1452baed6059664b0e098a9e7bb8e9b4bf06a30ddee1e54baac534.jpg", "img_caption": ["Figure 2: Overall framework of our proposed TrajCLIP. Introduction to each part can be found in Section 3.2, and specific implementation details can be found in Sec. 4.2. "], "img_footnote": [], "page_idx": 3}, {"type": "image", "img_path": "fUBFy8tb3z/tmp/e62d2c6da587961014578d07e7b704b484574c09a4797ded5fac2517b68fc358.jpg", "img_caption": ["Figure 3: The left side shows the Architecture of Spatio-Temporal Interaction Feature, while the right side shows the Scene-Agent Interaction Feature. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\nf:\\mathbf{M}\\rightarrow\\varphi_{A t t}(\\mathbf{W}_{2}(\\mathbf{W}_{1}^{T}\\mathbf{M}\\oplus\\Gamma))\\quad z_{t}=M L P[f(\\mathbf{X}_{t})+f(\\mathbf{V}_{t})+f(\\mathbf{A}_{t})]\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathbf{W}_{1}^{T}$ and $\\mathbf{W}_{2}$ represent trainable matrices, $\\varphi_{A t t}$ denotes agent-aware attention, $\\Gamma$ denotes sine and cosine position encoding [31], and $M L P$ denotes a multi-layer perceptron. We can easily derive the velocity $\\mathbf{V}_{t}$ and acceleration $\\mathbf{A}_{t}$ of the sequence using $\\mathbf{X}_{t}$ . Similarly, we employ equation 1 to extract features for both of these sequences. Parameters are not shared among different sequences. ", "page_idx": 4}, {"type": "text", "text": "The Scene-Agent Interaction Feature (SAIF) characterizes the shared representation of the scene by the agents. We believe that the interactions between agents and the scene are implicit and shared. These interaction features should be represented uniformly, without including the individual characteristics of the agents. As there is no direct way to merge the agents\u2019 trajectories in the time domain, we leverage the additivity in the frequency domain to eliminate the fine-grained features of the agents\u2019 trajectories, thereby emphasizing the interaction information of the scene. ", "page_idx": 4}, {"type": "text", "text": "We organize the trajectories in a scene into $\\mathbf{X}^{n}=((p_{t}^{n}\\mid t\\in\\mathcal{T})\\mid n=1,...,N)$ with agent as the index. We apply a Fast Fourier Transform (FFT) to each agent\u2019s trajectory based on Equation 2. Utilizing conjugate symmetry, we can derive the cosine expression for both amplitude and phase. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\tilde{p}_{k}^{n}=\\sum_{i\\in\\mathcal{T}}^{t}p_{i}^{n}e^{-j\\frac{2\\pi}{T_{h}}t k}=\\sum_{i\\in\\mathcal{T}}^{t}A_{i}^{n}c o s(\\frac{2k\\pi}{T_{h}}+\\phi_{i}^{n})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We employ a Butterworth filter to separate high-frequency information $(\\tilde{p}_{k}^{n},\\,A_{i}^{n},\\,\\phi_{i}^{n})$ to enhance modeling efficiency. Then we superimpose and concatenate the low-frequency signals $\\mathbf{X}^{n l}$ and high-frequency signals $\\mathbf{X}^{n h}$ of all agents in the scene respectively, and employ self-attention to extract feature. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\boldsymbol{\\tilde{\\mathbf{X}}}_{n}^{g}=\\mathbf{W}_{4}(\\mathbf{W}_{3}^{T}\\mathbf{X}^{n g}\\oplus\\Gamma),g\\in\\{h,l\\}\\quad z_{a}=M L P[I F F T(\\varphi_{a t t}^{h}(\\boldsymbol{\\tilde{\\mathbf{X}}}^{n h})+\\varphi_{a t t}^{l}(\\boldsymbol{\\tilde{\\mathbf{X}}}^{n l}))]\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathbf{W}_{3}$ and $\\mathbf{W}_{4}$ denote trainable matrices, $\\varphi_{a t t}^{g}$ denotes multi-head self-attention, and $I F F T$ denotes Inverse Fast Fourier Transform (IFFT). IFFT is a lossless and reversible transformation, so the extracted features can be inversely transformed into time-domain features without any information loss and fused with STIF in the same feature space. The final output of the trajectory encoder is the result of a linear transformation of the sum of $z_{t}$ and $z_{a}$ . ", "page_idx": 4}, {"type": "text", "text": "3.5 CLIP Pairing Learning Module ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We aim to transform the historical trajectory features into estimated future trajectory features, displaying temporal continuity to aid in trajectory prediction. It is important to maintain consistency between the feature spaces of past and future trajectories to satisfy spatio-temporal constraints. Therefore, it is necessary to retrain the historical trajectory encoder to align its feature space with that of the future trajectory encoder. We utilize CLIP [18], a method for aligning feature spaces of different types of information representation, such as text and images, using contrastive learning. We apply this concept to ensure consistency between the feature spaces in the past and future. The specific algorithm is outlined in Algorithm 1. The similarity matrix within each training batch of trajectories is calculated using Equation 4. The mean included angle cosine of the velocity direction vectors between trajectories $\\bar{i},j\\in\\{1,2,...,b a t c h\\_s i z e\\}$ is computed at each corresponding sample point in both the historical and future data. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\ns i m(i,j)=\\frac{1}{T_{f}+T_{h}}\\sum_{k=t_{0}-T_{h}+2}^{t_{0}+T_{f}}\\frac{\\overrightarrow{p_{k}^{i}p_{k-1}^{i}p_{k}^{j}p_{k-1}^{j}}}{||\\overrightarrow{p_{k}^{i}p_{k-1}^{i}}||||\\overrightarrow{p_{k}^{j}p_{k-1}^{j}}||}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Simultaneously, we calculate features $z_{H}^{i}$ and $z_{F}^{i}$ for each agent using the historical trajectory encoder and future trajectory encoder, and then combine the corresponding encoder features. Next, we align the dimensions using a trainable matrix W. Finally, we use the dot product to obtain the similarity scores and optimize them using the cross-entropy loss with the similarity matrix. Contrastive learning ensures that the feature space from the two encoders is consistent, which makes subsequent feature space mapping possible. ", "page_idx": 5}, {"type": "image", "img_path": "fUBFy8tb3z/tmp/184b5764c390f3ef2aa840ed3c196a667cc4f60fa90e971a151b4b91dbe15c7d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "image", "img_path": "fUBFy8tb3z/tmp/1ffbe31076659620816ccc288e382b4e70a3175eb8662fe07446ce89eb9ea6b7.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "3.6 Trajectory Manifold Predictor ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Following the CLIP pairing learning module, the historical and future trajectory embeddings are constrained to share the same feature space. Future trajectory features can be derived by applying a linear transformation to the historical trajectory features. Because there is inherent uncertainty surrounding the intentions of agents, the model\u2019s projected future trajectory should be represented as a probability distribution of the agents\u2019 positions. Therefore, it is necessary to estimate a manifold for the distribution of future trajectory features. ", "page_idx": 5}, {"type": "text", "text": "We have noticed that there is an idempotent phenomenon in this consistent feature space. Specifically, the training instances should have fixed points in the feature space as their future trajectories. Our goal is to create an estimated manifold within which the future historical trajectory will be located. This statement aligns with the fact that the distribution of predicted future trajectories, sampled in real space, includes the original trajectory. Therefore, the loss function used to train the feature transformation network should encompass both the reconstruction loss and the idempotent loss, as shown in the equation. ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{\\mathbf{Z}}_{F}=M L P[\\mathbf{Z}_{H};\\epsilon],\\;\\epsilon\\sim\\mathcal{N}(0,1)}\\\\ &{\\quad\\mathcal{L}_{r e c}=||\\widehat{\\mathbf{Z}}_{F}-\\mathbf{Z}_{F}||_{2}}\\\\ &{\\mathcal{L}_{i d e m}=||M L P_{\\theta^{\\prime}}[M L P_{\\theta}[\\mathbf{Z}_{F}]]-M L P_{\\theta}[\\mathbf{Z}_{F}]||_{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We additionally incorporate Gaussian noise sampling into the input to model the agents\u2019 intentions. Additionally, since the feature space is subject to consistency constraints, trajectories from multiple source domains correspond to different manifolds within this space. If, on the other hand, the model runs into complex distributions, the idempotent loss can cause representation collapse, also known as overftiting [23]. To mitigate this, a tightness loss is also needed to constrain the manifold expansion. ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{t i g h t}=-||M L P_{\\theta}[M L P_{\\theta^{\\prime}}[\\mathbf{Z}_{F}]]-M L P_{\\theta^{\\prime}}[\\mathbf{Z}_{F}]||_{2}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\theta$ denotes the MLP parameter, and $\\theta^{\\prime}$ denotes freezing the MLP parameter at training. The detailed reason and approach for adopting this training strategy can be found in the [23]. ", "page_idx": 5}, {"type": "text", "text": "3.7 Trajectory Manifold Decoder ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We employ an auto-regressive decoder with the standard Transformer architecture. No additional training tasks are required at this stage. During the trajectory encoding phase, both the historical trajectory feature encoder and the future trajectory feature encoder aim to output position coordinates for the period from $t-1$ to $t+T_{f}$ . Two additional frames of historical trajectory are included to better model spatiotemporal continuity. The former is similar to a prediction task, while the latter resembles a reconstruction task. The future trajectory feature decoder, trained during the encoder phase, is directly used for trajectory decoding at this stage. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{z=M L P(z_{a}+z_{t})}}\\\\ {{\\hat{p}_{t-1\\sim t+T_{f}}^{n}=D e c o d e r(z)}}\\\\ {{{\\mathcal{L}}=\\displaystyle\\frac{1}{N\\times(T_{f}+2)}\\sum_{n=1}^{N}\\sum_{i=t-1}^{t+T_{f}}||\\hat{p}_{i}^{n}-p_{i}^{n}||_{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.1 Datasets And Metrics ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Datasets. We evaluate our method using three popular trajectory prediction datasets: These include the ETH-UCY dataset, with its five scenes named ETH, HOTEL, UNIV, ZARA1, and ZARA2, which is considered the standard for predicting the paths of pedestrians. Additionally, we used the SDD dataset, the first large-scale dataset that collects agent trajectories in eight different outdoor scenes, and the SNU dataset, which includes different patterns such as walking straight, strolling, and staggering in indoor scenes. ", "page_idx": 6}, {"type": "text", "text": "Metrics. We use a standard evaluation approach for pedestrian trajectory prediction. We observe 8 frames of trajectories to predict 12 frames of future trajectories. Each time, we sample 20 predicted trajectories and compare the prediction results closest to the ground truth [31]. Our evaluation metrics of choice are Average Displacement Error (ADE) and Final Displacement Error (FDE). ", "page_idx": 6}, {"type": "equation", "text": "$$\nA D E=\\frac{1}{N\\times T_{f}}\\sum_{n=1}^{N}\\sum_{i=t+1}^{t+T_{f}}||\\hat{p}_{i}^{n}-p_{i}^{n}||_{2}\\quad F D E=\\frac{1}{N}\\sum_{n=1}^{N}||\\hat{p}_{t+T_{f}}^{n}-p_{t+T_{f}}^{n}||_{2}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "4.2 Implementation Detail ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "For training process, our batch size is set to 64, epochs to 100, and the learning rate is 0.01, which is half-formed every 25 epochs. We use the Adam optimizer. Our model is trained on an RTX 3090, with the encoder and contrastive learning pre-training requiring approximately 8 GPU hours, and the full framework training taking about 9.6 GPU hours. Our method is divided into three stages: offilne training, inference, and online adaptation. ", "page_idx": 6}, {"type": "text", "text": "Offline Training. First, we apply data augmentation to process trajectory data. Next, we pre-train historical and future trajectory encoders. Once these two encoders have reached convergence in their training, we use contrastive learning to impose constraints on their feature spaces for spatio-temporal constraints. During contrastive learning, we fine-tune the future trajectory decoders using pre-training tasks. It\u2019s important to note that during this step, the backpropagation gradient from the pre-train task is truncated only in the decoder section to adapt the future trajectory decoder to changes in the future trajectory encoder and maintain its decoding capabilities. Finally, we train the trajectory manifold predictor. The gradient for this training task is also truncated, meaning the parameters of the encoder are no longer updated. ", "page_idx": 6}, {"type": "text", "text": "Inference. During the inference stage, we do not engage in comparative learning or future trajectory encoding, unlike the offline training stage. Instead, we conduct interpolation and alignment on the input historical pedestrian trajectory that needs to be predicted. We then utilize the historical trajectory encoder and trajectory manifold predictor to extract future trajectory features. Finally, we use the trajectory manifold decoder to generate the predicted trajectory. ", "page_idx": 6}, {"type": "text", "text": "Online Adaption. During the online adaptation stage, we use a lightweight version of our model (TrajCLIP-tiny) for real-time inference. We freeze the trajectory encoder and manifold decoder and only fine-tune the parameters of the updated trajectory manifold predictor with a small learning rate based on newly observed trajectories. By adjusting the mapping between historical trajectory and future trajectory, the manifold predictor can map the same historical trajectories into future trajectories adapted to the current environment. ", "page_idx": 6}, {"type": "text", "text": "4.3 Experiment and Analysis ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We conduct comparative experiments with state-of-the-art methods on four common trajectory prediction tasks: standard trajectory prediction, scene-to-scene transfer, few-shot transfer, and online adaptation. These experiments provide a comprehensive evaluation of our method\u2019s prediction accuracy, generalization ability across different scenarios, learning capability with few samples, and online learning ability in dynamic environments. Additionally, we perform ablation studies on our proposed data augmentation method and various modules to verify the effectiveness of our approach. ", "page_idx": 6}, {"type": "table", "img_path": "fUBFy8tb3z/tmp/7bdd1b0d7d7aca14f66c15c82b8924d9686931535e5883e4b5ec29d91fe2a57b.jpg", "table_caption": [], "table_footnote": ["Table 1: Prediction performance on the ETH-UCY, SDD, SNU dataset using ADE/FDE. We sample 20 possible trajectories and calculate metrics based on the trajectory that is closest to the ground truth. The bold/underlined font denotes the best/second-best result. "], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Standard Trajectory Prediction Task. Follows the standard trajectory prediction setting, we compare our model, TrajCLIP with six recent models with the best performance (1) $\\mathrm{Traj++}$ [21], (2) AgentFormer [31], (3) MID [6], (4) GP-Graph [2], (5) EqMotion [27] (6) TUTR [22] and (7) SingularTrajectory [3] on the three datasets ETH-UCY, SDD, and SNU in Table. 1. From the table, our TrajCLIP model outperforms all other models, achieving the smallest ADE/FDE value and reducing the prediction error by up to $17\\%$ . Additionally, we develop TrajCLIP-tiny, which eliminates trajectory interpolation and reduces the attention matrix in the model, resulting in a $76.92\\%$ reduction in parameters. This allows TrajCLIP-tiny to adapt to various trade-offs between computational cost and prediction performance in real-world application scenarios. Moreover, TrajCLIP-tiny demonstrates competitive or superior performance in most of the dataset evaluation scenes. ", "page_idx": 7}, {"type": "text", "text": "Scene-to-Scene Transfer Task. In this experiment, our goal is to assess the model\u2019s ability to generalize across different scenes by training and evaluating it on different scenes within the ETHUCY dataset. Following the approach in [29], we train the model on one scene and test it on another. To ensure a fair comparison with existing baselines in this new setup, we train the baselines using the training data from the source scene and the validation data from the target scene. For example, taking column A2B from right part of Table 2, we train the baselines using the training data from dataset A and the validation data from dataset B, and then evaluate the model\u2019s performance using ADE/FDE on the test data from dataset B. It\u2019s important to note that the test and evaluation sets are independent, so the model only has access to trajectories from the validation set during evaluation. ", "page_idx": 7}, {"type": "text", "text": "We compare them with the existing trajectory prediction methods: the transfer learning work T-GNN [29] based on domain adaptation, the meta-learning work K0 [10] using ALPaCA [8] adaptation, and tra2tra [28] which performs better in transfer learning. The experiment result is shown in right part of Table 2. We discover that TrajCLIP-tiny exhibits competitive performance in the majority of scenes thanks to strong generalization capability from manifold predictor. In certain situations, it even outperforms trajectory prediction methods specifically designed for transfer learning. Moreover, TrajCLIP has achieved optimal or sub-optimal accuracy in the five scene-to-scene transfer tasks. ", "page_idx": 7}, {"type": "image", "img_path": "fUBFy8tb3z/tmp/b3dfe8262ca40f317fe9b3c7bc795a7d7e4bc9761ea81cbdb3758a3a662305f0.jpg", "img_caption": ["Table 2: Left is [ETH-UCY $\\rightarrow$ DeathCircle_0 in SDD] Comparison of ADE reduction of each method for updating models based on online observed data. Right is prediction performance in scene-to-scene transfer experiment setting on ETH-UCY using ADE and FDE. A, B, C, D, E denote ETH, Hotel, Univ, Zara1, Zara2. A2B experiment denotes a training model with a training set from A as well as the validation set from B, and reports test performance on a test set from B. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Few-shot Transfer Task. In this experiment, different from the scene-to-scene transfer, we train the model with the entire ETH-UCY dataset and four trajectories in the object scene in SNU dataset and evaluate with the other trajectories from the object scene. This experimental setting is called 4-shot transfer learning. We compare our TrajCLIP and TrajCLIP-tiny with several models: MemoNet [26], performs trajectory prediction with a memory-driven mechanism; Social-STGCNN [16], which claims data efficiency and performs well with $20\\%$ data in a dataset. T-GNN [29] and K0 [10] are models designed especially for the transfer learning task. The experiment results on ADE/FDE are shown in right part of Table 3. Only K0 outperforms our TrajCLIP-tiny $2.5\\%$ , but TrajCLIP-tiny performance remains competitive with other methods. With the ability to model more complex distributions, the TrajCLIP method outperforms all other methods on each SNU scene $8.7\\%$ to $40.6\\%$ . ", "page_idx": 8}, {"type": "image", "img_path": "fUBFy8tb3z/tmp/81e41bcab3d746469b2dccbb6727c4312ba48211db07b9ee8ccc8e5662ca345e.jpg", "img_caption": ["Table 3: Left is qualitative results of prediction results on UNIV scene (top) of ETH-UCY dataset, little_3 scene (left) and coupa scene (right) of SDD dataset. Right is ADE/FDE comparison of 4-shot transfer learning performance on each SNU scene. ", "Table 4: Ablation study on each key module of our TrajCLIP model (left). Ablation study on each data augmentation (right). SP denotes Small Perturbations, API denotes Adjacent Point Interpolation, PTG denotes Plausible Trajectories Generation. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Online Adaption Task. We evaluate the model performance of the online adaptation task. All methods are trained on the ETH-UCY dataset initially, with a batch size of 1 to simulate an online situation where data are observed sequentially. Then, all models are online adapted and tested. Our model adopts the online adaptation in Sec. 4.2 and compares it with the meta-learning-based K0 with/without additional online fine-tuning, as well as other methods that have better performance in online tasks. As shown in the left figure of Table 2, we take DeathCircle_0 in SDD as a new scenario as an example to give our comparison results on the online adaptation task. We select DeathCircle_0 in SDD as the target scene and compare the online adaptation effects of each model in the left figure of Table 2. The term \"base\" in the figure denotes the results of offline training for each model on DeathCircle_0. Using this prediction error as a benchmark, we calculate the ADE reduction rate of each model during the online adaptation process. Our model performance decreases the least at first and is improving fastest as the training step increases. After 100 steps, our model performs $20\\%$ better than the base model, which proves it could adapt to predict trajectories in new scenes quickly. ", "page_idx": 8}, {"type": "table", "img_path": "fUBFy8tb3z/tmp/cfaf37d2e0411ca0cb47081ccddb9d37dab78e183f3354140615cf3b07ccea01.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Ablation Studies. In left part of Table 4, we present the performance contributions of our proposed key modules: the SAIF, the CLIP pairing learning, and the manifold predictor (IGN generation module). The results show that combining the CLIP pairing method and the IGN module significantly enhances prediction performance, surpassing all other generation methods. This is because the CVAE framework involves an encoder obtaining a feature and then randomly sampling to get the mean and variance, which are used as inputs for the decoder. The diffusion method starts with a random initialization of an input and then gradually denoises. This randomness makes alignment work less effective (that is, even if aligned, the presence of random sampling still makes the generated results uncontrollable). The comparative experiments (c)(d) and (e)(f) illustrate that the two random methods of CVAE and diffusion methods make the improvement effect of alignment work not significant. On the ETH_UCY dataset, the addition of CLIP only improved ADE/FDE by 0.01/0.03 and $0.01/0.01$ . However, the network structure of IGN is an MLP, which is an affine transformation and does not have this issue; hence, as in our method, alignment is significantly effective on IGN. When CLIP is removed, as shown in experiments (b)(c)(e), since the two feature spaces cannot be connected through IGN without alignment, the performance drops by 0.06/0.17 compared to when CLIP is added. ", "page_idx": 8}, {"type": "text", "text": "We also conduct experiments to test different data augmentation methods and find that our method\u2019s improved performance is due to TrajCLIP\u2019s strong modeling ability to represent complex trajectory distributions, which shown in right part of Table 4. The results of applying the data augmentation methods are shown in the rightmost column of Table. When we train the latest state-of-the-art methods EqMotion and TUTR with our proposed data augmentation methods, we observe minimal changes in their performance, whereas our model\u2019s performance improved significantly. Additionally, experiments on each of the three core data augmentation methods demonstrate their effectiveness. Among these methods, Adjacent Point Interpolation (API) is proved to be the most effective in enhancing the model\u2019s prediction accuracy. ", "page_idx": 9}, {"type": "table", "img_path": "fUBFy8tb3z/tmp/a5d353ad39b1553e212b297a599a3df731b76f030dcfefb98550adc22e71c1e7.jpg", "table_caption": [], "table_footnote": ["Table 5: Comparison of our method with other methods in terms of model size, computational complexity, and infer speed. Infer speed refers to the time required to predict the next 12 frames using 8 frames. "], "page_idx": 9}, {"type": "text", "text": "Infer Speed and Computation Consumption. As the table 5 illustrates, we have contrasted our approach with alternative techniques in terms of model size, computational complexity, and inference speed. Our medium-sized model meets the requirements for a real-time prediction task in terms of both inference speed and model size, as it can predict trajectories in 0.0615 seconds. Additionally, our lightweight model is only 3.45MB in size, and its computational complexity is relatively low compared to its model size, making it deployable on most hardware platforms. ", "page_idx": 9}, {"type": "text", "text": "Qualitative Results. The left figure of Table 3 shows the visualization results of trajectory prediction for complex history trajectory using TrajCLIP and other state-of-the-art methods EqMotion and MID on UNIV scene of ETH-UCY dataset. Two trajectories in the figure include interference of surrounding pedestrians and scene environment and several turnings. We see that our TrajCLIP could predict these unusual turns, and the other two methods only predict a common smooth trajectory pattern, which shows a better ability to model complex trajectories. ", "page_idx": 9}, {"type": "image", "img_path": "fUBFy8tb3z/tmp/8dc4ef8e2237d54794ddbdbc4818ec45cf6dbca82309f0a6af00ac969377a3f2.jpg", "img_caption": ["Figure 5: Visualization of failure cases on the ETH/UCY dataset. Given the observed historical trajectories (yellow), our method predicts future trajectories (green). In complex interaction scenarios, our method occasionally exhibits failures such as deviating from the intended endpoint and colliding (red) with other agents. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Limitation Analysis. as illustrated in Figure 5, our method falls short in ensuring collision avoidance in complex, high-density environments, necessitating further research. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In our work, we introduce TrajCLIP, a method for predicting pedestrian trajectories. We use contrastive learning and an idempotent generative network to model a complex trajectory feature space. TrajCLIP offers five key contributions: the ability to model intricate trajectory distributions, CLIP pairing learning to combine the feature spaces of historical and future trajectories, a trajectory feature encoder based on the fusion of time domain and frequency domain, an idempotent generative network for mapping the same feature space, and an adjacent point interpolation algorithm to enhance the model\u2019s expression ability. We evaluate TrajCLIP on four prediction tasks, including standard trajectory prediction, scene-to-scene transfer, few-shot transfer, and online adaption, using three datasets. Our method achieves state-of-the-art prediction performance. ", "page_idx": 9}, {"type": "text", "text": "6 Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by the National Natural Science Foundation of China under Grant 62002345. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Alexandre Alahi, Kratarth Goel, Vignesh Ramanathan, Alexandre Robicquet, Li Fei-Fei, and Silvio Savarese. Social lstm: Human trajectory prediction in crowded spaces. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 961\u2013971, 2016.   \n[2] Inhwan Bae, Jin-Hwi Park, and Hae-Gon Jeon. Learning pedestrian group representations for multi-modal trajectory prediction. In European Conference on Computer Vision, pages 270\u2013289. Springer, 2022. [3] Inhwan Bae, Young-Jae Park, and Hae-Gon Jeon. Singulartrajectory: Universal trajectory predictor using diffusion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17890\u201317901, 2024.   \n[4] Haoyu Bai, Shaojun Cai, Nan Ye, David Hsu, and Wee Sun Lee. Intention-aware online pomdp planning for autonomous driving in a crowd. In 2015 ieee international conference on robotics and automation (icra), pages 454\u2013460. IEEE, 2015.   \n[5] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014. [6] Tianpei Gu, Guangyi Chen, Junlong Li, Chunze Lin, Yongming Rao, Jie Zhou, and Jiwen Lu. Stochastic trajectory prediction via motion indeterminacy diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17113\u201317122, 2022.   \n[7] Agrim Gupta, Justin Johnson, Li Fei-Fei, Silvio Savarese, and Alexandre Alahi. Social gan: Socially acceptable trajectories with generative adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2255\u20132264, 2018. [8] James Harrison, Apoorva Sharma, and Marco Pavone. Meta-learning priors for efficient online bayesian regression. In Algorithmic Foundations of Robotics XIII: Proceedings of the 13th Workshop on the Algorithmic Foundations of Robotics 13, pages 318\u2013337. Springer, 2020. [9] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840\u20136851, 2020.   \n[10] Boris Ivanovic, James Harrison, and Marco Pavone. Expanding the deployment envelope of behavior prediction via adaptive meta-learning. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 7786\u20137793. IEEE, 2023.   \n[11] Takayuki Kanda, Hiroshi Ishiguro, Tetsuo Ono, Michita Imai, and Ryohei Nakatsu. Development and evaluation of an interactive humanoid robot\" robovie\". In Proceedings 2002 IEEE international conference on robotics and automation (Cat. No. 02CH37292), volume 2, pages 1848\u20131855. IEEE, 2002.   \n[12] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.   \n[13] Yuanfu Luo, Panpan Cai, Aniket Bera, David Hsu, Wee Sun Lee, and Dinesh Manocha. Porca: Modeling and planning for autonomous driving among many pedestrians. IEEE Robotics and Automation Letters, 3(4):3418\u20133425, 2018.   \n[14] Takahiro Maeda and Norimichi Ukita. Fast inference and update of probabilistic density estimation on trajectory prediction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9795\u20139805, 2023.   \n[15] Osama Makansi, \u00d6zg\u00fcn \u00c7i\u00e7ek, Yassine Marrakchi, and Thomas Brox. On exposing the challenging long tail in future prediction of traffic actors. In 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 13127\u201313137, 2021.   \n[16] Abduallah Mohamed, Kun Qian, Mohamed Elhoseiny, and Christian Claudel. Social-stgcnn: A social spatio-temporal graph convolutional neural network for human trajectory prediction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14424\u201314432, 2020.   \n[17] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021.   \n[18] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.   \n[19] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022.   \n[20] Saeed Saadatnejad, Mohammadhossein Bahari, Pedram Khorsandi, Mohammad Saneian, SeyedMohsen Moosavi-Dezfooli, and Alexandre Alahi. Are socially-aware trajectory prediction models really socially-aware? Transportation research part C: emerging technologies, 141:103705, 2022.   \n[21] Tim Salzmann, Boris Ivanovic, Punarjay Chakravarty, and Marco Pavone. Trajectron $^{++}$ : Dynamically-feasible trajectory forecasting with heterogeneous data. In European Conference on Computer Vision, pages 683\u2013700. Springer, 2020.   \n[22] Liushuai Shi, Le Wang, Sanping Zhou, and Gang Hua. Trajectory unified transformer for pedestrian trajectory prediction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9675\u20139684, 2023.   \n[23] Assaf Shocher, Amil Dravid, Yossi Gandelsman, Inbar Mosseri, Michael Rubinstein, and Alexei A Efros. Idempotent generative network. arXiv preprint arXiv:2311.01462, 2023.   \n[24] Kihyuk Sohn, Honglak Lee, and Xinchen Yan. Learning structured output representation using deep conditional generative models. Advances in neural information processing systems, 28, 2015.   \n[25] Yuning Wang, Pu Zhang, Lei Bai, and Jianru Xue. Fend: A future enhanced distributionaware contrastive learning framework for long-tail trajectory prediction. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1400\u20131409, 2023.   \n[26] Chenxin Xu, Weibo Mao, Wenjun Zhang, and Siheng Chen. Remember intentions: Retrospective-memory-based trajectory prediction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6488\u20136497, 2022.   \n[27] Chenxin Xu, Robby T Tan, Yuhong Tan, Siheng Chen, Yu Guang Wang, Xinchao Wang, and Yanfeng Wang. Eqmotion: Equivariant multi-agent motion prediction with invariant interaction reasoning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1410\u20131420, 2023.   \n[28] Yi Xu, Dongchun Ren, Mingxia Li, Yuehai Chen, Mingyu Fan, and Huaxia Xia. Tra2tra: Trajectory-to-trajectory prediction with a global social spatial-temporal attentive neural network. IEEE Robotics and Automation Letters, 6(2):1574\u20131581, 2021.   \n[29] Yi Xu, Lichen Wang, Yizhou Wang, and Yun Fu. Adaptive trajectory prediction via transferable gnn. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6520\u20136531, 2022.   \n[30] Alper Yilmaz, Omar Javed, and Mubarak Shah. Object tracking: A survey. Acm computing surveys (CSUR), 38(4):13\u2013es, 2006.   \n[31] Ye Yuan, Xinshuo Weng, Yanglan Ou, and Kris M Kitani. Agentformer: Agent-aware transformers for socio-temporal multi-agent forecasting. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9813\u20139823, 2021.   \n[32] Guoqiang Peter Zhang. Neural networks for classification: a survey. IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews), 30(4):451\u2013462, 2000.   \n[33] Ziyuan Zhong, Davis Rempe, Danfei Xu, Yuxiao Chen, Sushant Veer, Tong Che, Baishakhi Ray, and Marco Pavone. Guided conditional diffusion for controllable traffic simulation. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 3560\u20133566. IEEE, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 13}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 13}, {"type": "text", "text": "Justification: The thesis proposition has been elaborated in the abstract and introduction. ", "page_idx": 13}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 13}, {"type": "text", "text": "Justification: We discussed in the experiment the disadvantages of our proposed TrajCLIP in terms of computational speed. ", "page_idx": 13}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 13}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 13}, {"type": "text", "text": "Justification: The theoretical results involved in the paper have all provided relevant references. ", "page_idx": 13}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 13}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 13}, {"type": "text", "text": "Justification: We are in methodology and Sec. 4.2 complete training process and detailed training details are provided in ref implement, and the code will be open source in the future. ", "page_idx": 13}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 13}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 13}, {"type": "text", "text": "Justification: Although we did not submit the code in the submission materials, we are confident that the description in the article is sufficiently reproducible, and we will open source the code and dataset on GitHub in the future. ", "page_idx": 13}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 13}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 13}, {"type": "text", "text": "Justification: We have elaborated in detail on the reasons, methods, and results of each experiment in the experimental section. ", "page_idx": 13}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 13}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 13}, {"type": "text", "text": "Justification: Our experiments were randomly initialized multiple times to avoid the impact of random seeds on the robustness of the experiment. ", "page_idx": 13}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 13}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 14}, {"type": "text", "text": "Justification: We have provided the relevant details of our training. ", "page_idx": 14}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] ", "page_idx": 14}, {"type": "text", "text": "Justification: Our research complies with relevant ethical guidelines. ", "page_idx": 14}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 14}, {"type": "text", "text": "Justification: Our research provides a new perspective for modeling abstract and complex pedestrian trajectories, and its generalization ability is beneficial for the online deployment of downstream planning tasks. ", "page_idx": 14}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 14}, {"type": "text", "text": "Justification: The data we use are all commonly used benchmark trajectory datasets and do not involve any safety risks. ", "page_idx": 14}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 14}, {"type": "text", "text": "Justification: The dataset used in the paper and other studies have provided references. ", "page_idx": 14}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 14}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 14}, {"type": "text", "text": "Justification: The new model mentioned in the article is not yet open source, and we will open source the code on GitHub as soon as possible. ", "page_idx": 14}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 14}, {"type": "text", "text": "Answer: [NA] Justification: not involve. ", "page_idx": 14}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 14}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 14}, {"type": "text", "text": "Answer: [NA] Justification: not invlove. ", "page_idx": 14}]