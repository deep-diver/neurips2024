[{"figure_path": "fUBFy8tb3z/figures/figures_0_1.jpg", "caption": "Figure 1: The existing methods use an encoder to sample from a normal distribution in order to generate predicted trajectories. However, this approach leads to different feature spaces for historical and future trajectories, causing problems during sampling, such as sharp turns or predictions of positions far exceeding normal speeds. Our method ensures consistency between the two feature spaces, enabling position prediction through affine transformations and preserving spatio-temporal continuity.", "description": "This figure illustrates the difference between existing trajectory prediction methods and the proposed TrajCLIP method. Existing methods encode historical and future trajectories into separate feature spaces, leading to inconsistencies and inaccuracies in predictions.  TrajCLIP, on the other hand, maintains consistency between these feature spaces by using affine transformations, resulting in smoother and more realistic trajectory predictions.", "section": "1 Introduction"}, {"figure_path": "fUBFy8tb3z/figures/figures_1_1.jpg", "caption": "Figure 1: The existing methods use an encoder to sample from a normal distribution in order to generate predicted trajectories. However, this approach leads to different feature spaces for historical and future trajectories, causing problems during sampling, such as sharp turns or predictions of positions far exceeding normal speeds. Our method ensures consistency between the two feature spaces, enabling position prediction through affine transformations and preserving spatio-temporal continuity.", "description": "This figure illustrates the difference between existing trajectory prediction methods and the proposed TrajCLIP method. Existing methods map historical and future trajectories into different feature spaces, leading to discontinuities in predicted trajectories. In contrast, TrajCLIP maps them into the same feature space, ensuring spatio-temporal continuity and smoother trajectory predictions.", "section": "1 Introduction"}, {"figure_path": "fUBFy8tb3z/figures/figures_3_1.jpg", "caption": "Figure 2: Overall framework of our proposed TrajCLIP. Introduction to each part can be found in Section 3.2, and specific implementation details can be found in Sec. 4.2.", "description": "This figure presents the overall architecture of the TrajCLIP model, highlighting its four main components: the Trajectory Encoder, the CLIP Pairing Learning module, the Trajectory Manifold Predictor, and the Trajectory Manifold Decoder.  The Trajectory Encoder processes both historical and future trajectories, extracting Spatio-Temporal Interaction Features (STIF) and Scene-Agent Interaction Features (SAIF). The CLIP Pairing Learning module uses contrastive learning to ensure consistency between the feature spaces of historical and future trajectories. The Trajectory Manifold Predictor employs idempotent and tightness loss functions to map historical trajectory features onto a manifold representing the distribution of future trajectories. Finally, the Trajectory Manifold Decoder generates the predicted future trajectory based on this manifold. The figure illustrates the data flow and the interactions between these components, clearly showing how historical and future trajectory features are processed and used to predict future trajectories.", "section": "3.2 Overall Framework"}, {"figure_path": "fUBFy8tb3z/figures/figures_4_1.jpg", "caption": "Figure 3: The left side shows the Architecture of Spatio-Temporal Interaction Feature, while the right side shows the Scene-Agent Interaction Feature.", "description": "This figure illustrates the architecture of two feature extractors used in the TrajCLIP model: Spatio-Temporal Interaction Feature (STIF) and Scene-Agent Interaction Feature (SAIF).  The STIF processes trajectory data in the time domain, incorporating position, velocity, acceleration, and agent sequences.  Agent-aware attention mechanisms are used to capture interactions between agents. The SAIF operates in the frequency domain, using Fast Fourier Transforms (FFT) to extract scene-agent interaction information. A Butterworth filter is applied to separate high and low-frequency components before inverse FFT (IFFT) to obtain the final features. Self-attention is used to aggregate agent information. Both feature extractors aim to provide comprehensive representations of trajectory dynamics.", "section": "3.4 Trajectory Encoder"}, {"figure_path": "fUBFy8tb3z/figures/figures_5_1.jpg", "caption": "Figure 4: Schematic diagram of Adjacent Point Interpolation.", "description": "This figure illustrates the adjacent point interpolation algorithm used for data augmentation.  It shows how new data points (blue circles) are interpolated between existing trajectory points (black circles) based on the vectors formed by consecutive points. The algorithm considers the relative positions of these vectors to determine the appropriate interpolation method, resulting in a more representative and complex distribution of trajectories for model training.", "section": "3.3 Data Augmentation"}, {"figure_path": "fUBFy8tb3z/figures/figures_5_2.jpg", "caption": "Figure 2: Overall framework of our proposed TrajCLIP. Introduction to each part can be found in Section 3.2, and specific implementation details can be found in Sec. 4.2.", "description": "This figure shows the overall framework of the TrajCLIP model, which consists of four main parts: a trajectory encoder, a CLIP pairing learning module, a trajectory manifold predictor, and a trajectory manifold decoder.  The trajectory encoder processes both historical and future trajectories, extracting spatio-temporal and scene-agent interaction features. The CLIP pairing learning module enforces consistency between the feature spaces of historical and future trajectories using contrastive learning. The trajectory manifold predictor maps the historical trajectory features to future trajectory features in the same feature space using an idempotent generative network. Finally, the trajectory manifold decoder generates the predicted future trajectories.  Sections 3.2 and 4.2 of the paper provide further details on each component and implementation.", "section": "3.2 Overall Framework"}, {"figure_path": "fUBFy8tb3z/figures/figures_7_1.jpg", "caption": "Figure 2: Overall framework of our proposed TrajCLIP. Introduction to each part can be found in Section 3.2, and specific implementation details can be found in Sec. 4.2.", "description": "This figure shows the overall framework of the TrajCLIP model, which is composed of four main parts: a trajectory encoder, a CLIP pairing learning module, a trajectory manifold predictor, and a trajectory manifold decoder. The trajectory encoder encodes the historical and future trajectories, the CLIP pairing learning module ensures consistency between the feature spaces of past and future trajectories, the trajectory manifold predictor maps past trajectory features into future trajectory features in the same feature space, and the trajectory manifold decoder generates predicted trajectories. Specific details about each component are available in Sections 3.2 and 4.2 of the paper.", "section": "3 Methodology"}, {"figure_path": "fUBFy8tb3z/figures/figures_8_1.jpg", "caption": "Figure 3: The left side shows the Architecture of Spatio-Temporal Interaction Feature, while the right side shows the Scene-Agent Interaction Feature.", "description": "This figure illustrates the architecture of two key components in the TrajCLIP model: the Spatio-Temporal Interaction Feature (STIF) and the Scene-Agent Interaction Feature (SAIF).  STIF processes trajectories to extract temporal features (position, velocity, acceleration) using agent-aware attention.  SAIF uses Fourier transforms to extract interaction features between agents and the scene, emphasizing shared information and eliminating agent-specific details. The figure visually depicts the data flow and processing steps within each component.", "section": "3.4 Trajectory Encoder"}, {"figure_path": "fUBFy8tb3z/figures/figures_9_1.jpg", "caption": "Figure 1: The existing methods use an encoder to sample from a normal distribution in order to generate predicted trajectories. However, this approach leads to different feature spaces for historical and future trajectories, causing problems during sampling, such as sharp turns or predictions of positions far exceeding normal speeds. Our method ensures consistency between the two feature spaces, enabling position prediction through affine transformations and preserving spatio-temporal continuity.", "description": "This figure illustrates the difference between existing trajectory prediction methods and the proposed TrajCLIP method. Existing methods project historical and future trajectories into different feature spaces, leading to inconsistencies and difficulties in prediction, especially in terms of spatio-temporal continuity. In contrast, TrajCLIP maintains consistency between the feature spaces, enabling smooth and accurate prediction through affine transformations.", "section": "1 Introduction"}]