[{"figure_path": "INAeUQ04lT/figures/figures_1_1.jpg", "caption": "Figure 1: Left: The forecasting with exogenous variables paradigm includes inputs from multiple external variables as auxiliary information without the need for forecasting. Right: Model performance comparison on existing electricity price forecasting with exogenous variables benchmarks.", "description": "The figure on the left illustrates the concept of time series forecasting with exogenous variables. It shows how multiple external variables provide additional information to improve the accuracy of predicting the target variable (endogenous variable). The figure on the right presents a comparison of the performance of different time series forecasting models (TimeXer, Crossformer, DLinear, TIDE, iTransformer, PatchTST) on several real-world electricity price forecasting benchmarks, demonstrating the effectiveness of TimeXer.", "section": "1 Introduction"}, {"figure_path": "INAeUQ04lT/figures/figures_4_1.jpg", "caption": "Figure 2: The schematic of TimeXer, which empowers time series forecasting with exogenous variables. (a) The endogenous embedding module yields multiple temporal token embeddings and one global token embedding for the endogenous variable. (b) The exogenous embedding module yields a variate token embedding for each exogenous variable. (c) Self-attention is applied simultaneously over the endogenous temporal tokens and the global token to capture patch-wise dependencies. (d) Cross-attention is applied over endogenous and exogenous variables to integrate external information.", "description": "This figure illustrates the architecture of the TimeXer model.  It shows how the model processes both endogenous (target) and exogenous (auxiliary) time series data.  The endogenous series is first embedded into patch-wise temporal tokens and a global token.  The exogenous series are embedded into variate-wise tokens.  Self-attention is applied within the endogenous tokens to capture temporal dependencies.  Cross-attention is then used to integrate the exogenous information with the endogenous information, facilitated by the global token. This combined information is then fed into subsequent layers for prediction.", "section": "3 TimeXer"}, {"figure_path": "INAeUQ04lT/figures/figures_7_1.jpg", "caption": "Figure 2: The schematic of TimeXer, which empowers time series forecasting with exogenous variables. (a) The endogenous embedding module yields multiple temporal token embeddings and one global token embedding for the endogenous variable. (b) The exogenous embedding module yields a variate token embedding for each exogenous variable. (c) Self-attention is applied simultaneously over the endogenous temporal tokens and the global token to capture patch-wise dependencies. (d) Cross-attention is applied over endogenous and exogenous variables to integrate external information.", "description": "This figure illustrates the TimeXer model architecture, showing how it processes endogenous and exogenous time series data.  The endogenous series is split into patches, each represented by a token, and a global token summarizes the entire series.  Exogenous series are each represented by a single variate token. Self-attention operates within the endogenous tokens, and cross-attention integrates information between the endogenous and exogenous tokens. The global endogenous token acts as a bridge between these two types of information.", "section": "3 TimeXer"}, {"figure_path": "INAeUQ04lT/figures/figures_8_1.jpg", "caption": "Figure 4: Forecasting performance on large-scale time series datasets. Left: Illustration of the forecasting scenario. The endogenous is the temperature collected from weather stations, and the exogenous variables are meteorological indicators from the surrounding 3x3 grids including the weather station. Each area contains four types of information, namely, temperature, pressure, u- and v- components of wind. Right: TimeXer outperforms other advanced forecasters.", "description": "This figure demonstrates TimeXer's performance on a large-scale weather forecasting task.  The left panel shows a world map highlighting the locations of weather stations (endogenous variable) and their surrounding 3x3 grid areas (exogenous variables). Each grid provides four meteorological features (temperature, pressure, u-component of wind, v-component of wind). The right panel presents a bar chart comparing the Mean Squared Error (MSE) achieved by TimeXer against several other state-of-the-art forecasting models, illustrating TimeXer's superior performance.", "section": "4.3 TimeXer Generality"}, {"figure_path": "INAeUQ04lT/figures/figures_9_1.jpg", "caption": "Figure 2: The schematic of TimeXer, which empowers time series forecasting with exogenous variables. (a) The endogenous embedding module yields multiple temporal token embeddings and one global token embedding for the endogenous variable. (b) The exogenous embedding module yields a variate token embedding for each exogenous variable. (c) Self-attention is applied simultaneously over the endogenous temporal tokens and the global token to capture patch-wise dependencies. (d) Cross-attention is applied over endogenous and exogenous variables to integrate external information.", "description": "This figure illustrates the architecture of TimeXer, a novel approach for time series forecasting with exogenous variables.  It shows how TimeXer uses different embedding strategies for endogenous and exogenous variables, employing patch-wise self-attention and variate-wise cross-attention mechanisms.  The global endogenous token acts as a bridge, integrating exogenous information into the endogenous temporal patches. The figure is divided into four parts, showing (a) endogenous embedding, (b) exogenous embedding, (c) endogenous self-attention, and (d) exogenous-to-endogenous cross-attention.", "section": "3 TimeXer"}, {"figure_path": "INAeUQ04lT/figures/figures_14_1.jpg", "caption": "Figure 2: The schematic of TimeXer, which empowers time series forecasting with exogenous variables. (a) The endogenous embedding module yields multiple temporal token embeddings and one global token embedding for the endogenous variable. (b) The exogenous embedding module yields a variate token embedding for each exogenous variable. (c) Self-attention is applied simultaneously over the endogenous temporal tokens and the global token to capture patch-wise dependencies. (d) Cross-attention is applied over endogenous and exogenous variables to integrate external information.", "description": "This figure illustrates the architecture of the TimeXer model.  It breaks down the process into four key stages:\n(a) **Endogenous Embedding:** The model processes the endogenous (target) time series by dividing it into patches and creating a token representation for each patch.  A separate global token is also created to represent the entire endogenous series.\n(b) **Exogenous Embedding:** Exogenous (external) time series are processed, each series creating a single variate-level token representation. \n(c) **Endogenous Self-Attention:** Self-attention mechanisms operate on the patch tokens and the global token to capture temporal dependencies within the target time series.\n(d) **Exogenous-to-Endogenous Cross-Attention:** Cross-attention links the endogenous tokens with the exogenous tokens, allowing the model to integrate external information into the forecasting process.", "section": "3 TimeXer"}, {"figure_path": "INAeUQ04lT/figures/figures_15_1.jpg", "caption": "Figure 7: Forecasting performance with the masked exogenous series on three EPF datasets, simulating the missing values scenario.", "description": "This figure showcases the impact of missing exogenous data on forecasting accuracy.  It compares TimeXer, iTransformer, and PatchTST across three datasets (NP, BE, DE) at various levels of missing data (mask ratios).  The results show the robustness or sensitivity of each model to missing exogenous information.  The x-axis represents the percentage of missing exogenous data, while the y-axis shows the Mean Squared Error (MSE), a measure of prediction accuracy.", "section": "4.3 TimeXer Generality"}, {"figure_path": "INAeUQ04lT/figures/figures_16_1.jpg", "caption": "Figure 2: The schematic of TimeXer, which empowers time series forecasting with exogenous variables. (a) The endogenous embedding module yields multiple temporal token embeddings and one global token embedding for the endogenous variable. (b) The exogenous embedding module yields a variate token embedding for each exogenous variable. (c) Self-attention is applied simultaneously over the endogenous temporal tokens and the global token to capture patch-wise dependencies. (d) Cross-attention is applied over endogenous and exogenous variables to integrate external information.", "description": "This figure illustrates the architecture of the TimeXer model, highlighting its key components and how they interact.  It shows how endogenous and exogenous variables are processed separately and combined to enhance forecasting accuracy.  The endogenous variable is split into patches which are processed via self-attention.  Exogenous variables are represented by variate tokens. Global tokens bridge the information between the exogenous and endogenous components. Finally, cross-attention is used to integrate information between exogenous and endogenous series.", "section": "3 TimeXer"}, {"figure_path": "INAeUQ04lT/figures/figures_17_1.jpg", "caption": "Figure 2: The schematic of TimeXer, which empowers time series forecasting with exogenous variables. (a) The endogenous embedding module yields multiple temporal token embeddings and one global token embedding for the endogenous variable. (b) The exogenous embedding module yields a variate token embedding for each exogenous variable. (c) Self-attention is applied simultaneously over the endogenous temporal tokens and the global token to capture patch-wise dependencies. (d) Cross-attention is applied over endogenous and exogenous variables to integrate external information.", "description": "This figure illustrates the TimeXer model architecture, showcasing how it handles endogenous and exogenous variables.  The endogenous variable is processed into multiple temporal tokens (patches) and a single global token which then undergoes self-attention.  Each exogenous variable is represented as a variate token.  Cross-attention combines the endogenous and exogenous information to improve forecasting accuracy.", "section": "3 TimeXer"}, {"figure_path": "INAeUQ04lT/figures/figures_18_1.jpg", "caption": "Figure 2: The schematic of TimeXer, which empowers time series forecasting with exogenous variables. (a) The endogenous embedding module yields multiple temporal token embeddings and one global token embedding for the endogenous variable. (b) The exogenous embedding module yields a variate token embedding for each exogenous variable. (c) Self-attention is applied simultaneously over the endogenous temporal tokens and the global token to capture patch-wise dependencies. (d) Cross-attention is applied over endogenous and exogenous variables to integrate external information.", "description": "This figure illustrates the architecture of the TimeXer model, highlighting the different embedding strategies and attention mechanisms used for endogenous and exogenous variables.  It shows how the model processes the input time series data:  (a) Endogenous data is split into patches, which are then embedded into temporal tokens, with a separate global token learned for the entire series. (b) Exogenous variables are represented by variate tokens. (c) Self-attention operates within the endogenous series to capture temporal relationships. (d) Cross-attention combines the endogenous and exogenous information, enabling the model to leverage exogenous information for improved prediction of the endogenous variable.", "section": "3 TimeXer"}, {"figure_path": "INAeUQ04lT/figures/figures_19_1.jpg", "caption": "Figure 2: The schematic of TimeXer, which empowers time series forecasting with exogenous variables. (a) The endogenous embedding module yields multiple temporal token embeddings and one global token embedding for the endogenous variable. (b) The exogenous embedding module yields a variate token embedding for each exogenous variable. (c) Self-attention is applied simultaneously over the endogenous temporal tokens and the global token to capture patch-wise dependencies. (d) Cross-attention is applied over endogenous and exogenous variables to integrate external information.", "description": "This figure illustrates the architecture of TimeXer, a novel approach for time series forecasting with exogenous variables.  It shows the different embedding strategies used for endogenous (patch-wise) and exogenous (variate-wise) variables, and how self-attention and cross-attention mechanisms are used to capture dependencies within and between these variables.  A key component is the inclusion of a learnable global token to bridge between endogenous and exogenous information.", "section": "3 TimeXer"}, {"figure_path": "INAeUQ04lT/figures/figures_20_1.jpg", "caption": "Figure 2: The schematic of TimeXer, which empowers time series forecasting with exogenous variables. (a) The endogenous embedding module yields multiple temporal token embeddings and one global token embedding for the endogenous variable. (b) The exogenous embedding module yields a variate token embedding for each exogenous variable. (c) Self-attention is applied simultaneously over the endogenous temporal tokens and the global token to capture patch-wise dependencies. (d) Cross-attention is applied over endogenous and exogenous variables to integrate external information.", "description": "This figure illustrates the TimeXer model architecture, highlighting the different embedding strategies used for endogenous and exogenous variables.  Endogenous variables are processed using patch-wise self-attention and global endogenous tokens to capture temporal dependencies. Exogenous variables are processed using variate-wise cross-attention with the global endogenous tokens. This design allows TimeXer to effectively integrate both endogenous and exogenous information to enhance forecasting accuracy.", "section": "3 TimeXer"}, {"figure_path": "INAeUQ04lT/figures/figures_20_2.jpg", "caption": "Figure 2: The schematic of TimeXer, which empowers time series forecasting with exogenous variables. (a) The endogenous embedding module yields multiple temporal token embeddings and one global token embedding for the endogenous variable. (b) The exogenous embedding module yields a variate token embedding for each exogenous variable. (c) Self-attention is applied simultaneously over the endogenous temporal tokens and the global token to capture patch-wise dependencies. (d) Cross-attention is applied over endogenous and exogenous variables to integrate external information.", "description": "This figure illustrates the architecture of the TimeXer model.  It shows how the model processes both endogenous (target) and exogenous (external) variables.  The endogenous variables are embedded into multiple temporal tokens and a global token, enabling the capture of temporal dependencies using self-attention. Exogenous variables are represented as variate tokens that interact with the endogenous tokens and global token via cross-attention to incorporate external information. This combined approach allows the model to handle both internal temporal dynamics and the influence of external factors.", "section": "3 TimeXer"}, {"figure_path": "INAeUQ04lT/figures/figures_20_3.jpg", "caption": "Figure 2: The schematic of TimeXer, which empowers time series forecasting with exogenous variables. (a) The endogenous embedding module yields multiple temporal token embeddings and one global token embedding for the endogenous variable. (b) The exogenous embedding module yields a variate token embedding for each exogenous variable. (c) Self-attention is applied simultaneously over the endogenous temporal tokens and the global token to capture patch-wise dependencies. (d) Cross-attention is applied over endogenous and exogenous variables to integrate external information.", "description": "This figure illustrates the architecture of the TimeXer model, highlighting its key components.  The model takes both endogenous (target) and exogenous (auxiliary) time series as input. Endogenous time series are embedded into multiple temporal tokens representing different segments, along with a global token to represent overall series information. Exogenous variables are each embedded into a single variate token. The model uses self-attention within endogenous tokens (temporal dependencies) and cross-attention between endogenous and exogenous tokens (integrating external information).", "section": "3 TimeXer"}]