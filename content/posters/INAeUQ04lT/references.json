{"references": [{"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-12-01", "reason": "This paper introduced the Transformer architecture, which is foundational to TimeXer and many other modern time series forecasting models."}, {"fullname_first_author": "Jacob Devlin", "paper_title": "Bert: Pre-training of deep bidirectional transformers for language understanding", "publication_date": "2018-10-01", "reason": "BERT's pre-training methodology has been highly influential in the field, and this is referenced here as a related work in the application of Transformers to time series."}, {"fullname_first_author": "Colin Lea", "paper_title": "Temporal convolutional networks for action segmentation and detection", "publication_date": "2017-07-01", "reason": "This paper is cited as a related work demonstrating the application of convolutional neural networks to time series data, an area TimeXer advances upon."}, {"fullname_first_author": "Haixu Wu", "paper_title": "Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting", "publication_date": "2021-12-01", "reason": "Autoformer is a highly cited state-of-the-art Transformer-based time series model; this reference provides context for TimeXer's approach to modeling long-term dependencies."}, {"fullname_first_author": "Bryan Lim", "paper_title": "Temporal fusion transformers for interpretable multi-horizon time series forecasting", "publication_date": "2021-01-01", "reason": "This paper is another highly-regarded state-of-the-art Transformer-based time series model focusing on interpretability, which provides context for TimeXer's improvements."}]}