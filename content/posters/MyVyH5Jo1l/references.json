{"references": [{"fullname_first_author": "Collin Burns", "paper_title": "Weak-to-strong generalization: Eliciting strong capabilities with weak supervision", "publication_date": "2023-12-09", "reason": "This paper introduces the concept of weak-to-strong generalization, which is the central focus of the current paper, providing empirical evidence for the phenomenon."}, {"fullname_first_author": "Jacob Devlin", "paper_title": "Bert: Pre-training of deep bidirectional transformers for language understanding", "publication_date": "2018-10-04", "reason": "The BERT model is used as a strong model in the experiments, highlighting the significance of this pre-trained language representation model in modern NLP research."}, {"fullname_first_author": "Yoav Freund", "paper_title": "Boosting a weak learning algorithm by majority", "publication_date": "1995-00-00", "reason": "This paper is foundational to the concept of boosting weak learners to create a stronger learner, which is theoretically related to the idea of weak-to-strong generalization."}, {"fullname_first_author": "Geoffrey Hinton", "paper_title": "Distilling the knowledge in a neural network", "publication_date": "2015-03-02", "reason": "Knowledge distillation, where a smaller student model learns from a larger teacher model, provides a similar context to weak-to-strong generalization and is discussed as related work in the paper."}, {"fullname_first_author": "Nilesh Tripuraneni", "paper_title": "On the theory of transfer learning: The importance of task diversity", "publication_date": "2020-00-00", "reason": "This paper provides a theoretical foundation for transfer learning, which is relevant to the concept of leveraging weak models to improve the performance of stronger models."}]}