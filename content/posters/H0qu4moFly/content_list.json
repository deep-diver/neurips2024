[{"type": "text", "text": "Embedding Dimension of Contrastive Learning and $k$ -Nearest Neighbors ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Dmitrii Avdiukhin ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Vaggos Chatziafratis ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Computer Science Department Northwestern University Evanston, IL 60657, USA dmitrii.avdiukhin@northwestern.edu ", "page_idx": 0}, {"type": "text", "text": "Computer Science and Engineering Department University of California at Santa Cruz Santa Cruz, CA 95064, USA vaggos@ucsc.edu ", "page_idx": 0}, {"type": "text", "text": "Orr Fischer   \nComputer Science Department   \nBar-Ilan University   \nRamat-Gan, Israel   \nfischeo@biu.ac.il   \nGrigory Yaroslavtsev   \nComputer Science Department   \nGeorge Mason University   \nFairfax, VA 22030, USA   \ngrigory@gmu.edu ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We study the embedding dimension of distance comparison data in two settings: contrastive learning and $k$ -nearest neighbors ( $\\boldsymbol{k}$ -NN). Our goal is to find the smallest dimension $d$ of an $\\ell_{p}$ -space in which a given dataset can be represented. We show that the arboricity of the associated graphs plays a key role in designing embeddings. For the most popular $\\ell_{2}$ -space, we get tight bounds in both settings. In contrastive learning, we are given $m$ labeled samples $(x_{i},y_{i}^{+},z_{i}^{-})$ representing the fact that the positive example $y_{i}$ is closer to the anchor $x_{i}$ than the negative example $z_{i}$ (we also give results for $t$ negatives). For representing such dataset in: ", "page_idx": 0}, {"type": "text", "text": "\u2022 $\\ell_{2}$ : $d=\\Theta({\\sqrt{m}})$ is necessary and sufficient, consistent with our experiments.   \n\u2022 $\\ell_{p}$ for $p\\geq1$ : $d=O(m)$ is sufficient and $d=\\tilde{\\Omega}(\\sqrt{m})$ is necessary.   \n\u2022 $\\ell_{\\infty}\\colon d=O(m^{2/3})$ is sufficient and $d=\\tilde{\\Omega}(\\sqrt{m})$ is necessary. ", "page_idx": 0}, {"type": "text", "text": "In $k$ -NN, for each of the $n$ data points we are given an ordered set of the closest $k$ points. We show that for preserving the ordering of the $k$ -NN for every point in: ", "page_idx": 0}, {"type": "text", "text": "\u2022 $\\ell_{2}$ : $d=\\Theta(k)$ is necessary and sufficient.   \n\u2022 $\\ell_{p}$ for $p\\geq1$ : $d=\\tilde{O}(k^{2})$ is sufficient and $d=\\tilde{\\Omega}(\\boldsymbol{k})$ is necessary.   \n\u2022 $\\bar{\\ell_{\\infty}}:d=\\tilde{\\Omega}(k)$ is necessary. ", "page_idx": 0}, {"type": "text", "text": "Furthermore, if the goal is to not just preserve the ordering of the $k$ -NN but also keep them as the nearest neighbors, then $d=\\tilde{O}(\\mathrm{poly}(k))$ suffices in $\\ell_{p}$ for $p\\geq1$ . ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Embedding vectors play an important role in machine learning, with the embedding dimension being a key parameter of interest when choosing a deep learning architecture. In this paper, we ask the following question: given a dataset labeled with distance relationships between its points, what is the smallest embedding dimension required to represent it? We answer this question for two types of distance comparison data: contrastive labels and $k$ -NN. ", "page_idx": 0}, {"type": "text", "text": "Contrastive Learning Contrastive learning [GH10] has recently become a popular technique for learning representations, see e.g. [SE05, MCCD13, DSRB14, SKP15a, WG15, WXYL18, ", "page_idx": 0}, {"type": "text", "text": "LL18, $\\mathrm{HFL}^{+}19$ , $\\mathrm{HFW}^{+}20$ , TKI20, CKNH20, CH21, GYC21, CLL21]. Recent interest in theoretical foundations of contrastive learning has resulted in extensive research focusing on generalization $[\\mathrm{AAE^{+}}24]$ , design of specific loss functions [HWGM21], transfer learning $\\bar{[\\mathrm{SPA}^{\\bar{+}}19}$ , $\\mathrm{CRL}^{+}20]$ , multi-view redundancy [TKH21], inductive biases $[\\mathbf{S}\\mathbf{A}\\mathbf{G}^{+}22$ , HM23], the role of negative samples [AGKM22, ADK22], mutual information [vdOLV18, $\\mathrm{HFL}^{+}19$ , BHB19, $\\mathrm{TDR}^{+}20\\bar{]}$ , and other topics [WI20, TWSM21, $Z\\!S\\!S^{+}21$ , $\\mathrm{vKSG}^{+}21$ , $\\mathbf{M}\\mathbf{M}\\mathbf{W}^{+}21$ , WL21]. ", "page_idx": 1}, {"type": "text", "text": "In on of the most common forms of contrastive learning, we are given $m$ labeled data points $\\{(x_{i},y_{i}^{+},z_{i}^{-})\\}_{i=1}^{m}$ (or more generally, $\\{(x_{i},y_{i}^{+},z_{i,1}^{-},z_{i,2}^{-},\\bar{.}\\cdot.\\cdot,z_{i,t}^{-})\\}_{i=1}^{m})$ over a dataset of size $n$ . Each point represents the fact that the distance between the anchor $x_{i}$ and the positive example $y_{i}$ is smaller than the distance between $x_{i}$ and the negative example $z_{i}$ (or, more generally, $t$ negative examples $z_{i,1},\\dotsc,z_{i,t})$ . We study the problem of embedding such data into $\\ell_{p}$ -spaces, i.e., constructing an embedding $F\\colon V\\to\\mathbb{R}^{d}$ such that $\\|F(x_{i})-F(y_{i})\\|_{p}\\,<\\,\\|F(x_{i})-F(z_{i})\\|_{p}$ for all $i$ (more generally, $\\|F(\\bar{x_{i}})-F(y_{i})\\|_{p}<\\|F(x_{i})\\stackrel{\\cdot\\cdot}{-F}(z_{i,j})\\|_{p}$ for all $i,j)$ . In particular, we focus on the embedding dimension: ", "page_idx": 1}, {"type": "text", "text": "Given a collection of m triplet comparisons of the form \u201c $x_{i}$ is closer to $y_{i}$ than to $z_{i}\\,^{,,}$ , what is the smallest dimension $d$ of an $\\ell_{p}$ -space in which the relative order of distances can be preserved? ", "page_idx": 1}, {"type": "text", "text": "$k$ -NNs We also study a similar question for $k$ -Nearest Neighbor $k$ -NN) data, which has major applications in machine learning since the seminal work of [CH67]. In this setting, we are given a set of $n$ items and the information about the $k$ -NN of each item $\\{(x_{i},\\pi_{1}(x_{i}),...\\,,\\pi_{k}(x_{i}))\\}_{i=1}^{n}$ where $\\pi_{1}(x_{i}),\\ldots,\\pi_{k}(x_{i})$ are the $k$ -NN of $x_{i}$ ordered by their distance from $x_{i}$ . Since $k$ -NN classifiers are extremely popular in deep learning pipelines, understanding the embedding dimension required for preserving $k$ -NN is a question of fundamental importance. In particular: ", "page_idx": 1}, {"type": "text", "text": "Given n items and their $k$ -NN, what is the smallest dimension $d$ of an $\\ell_{p}$ -space in which the ordering of the $k$ -NN can be preserved? What if the $k$ -NN have to remain $k$ -NN in the $\\ell_{p}$ -space? ", "page_idx": 1}, {"type": "text", "text": "1.1 Our Results and Techniques ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Let $V$ be the set of $n$ points. Our goal is to construct an embedding $F\\colon V\\to\\mathbb{R}^{d}$ . For an integer $n$ , we let $[n]=\\{1,2,\\ldots,n\\}$ . For a vector $\\boldsymbol{v}\\in\\mathbb{R}^{d}$ , let $v[i]$ be the $i^{{\\breve{t}}h}$ coordinate of $v$ . For vectors $v_{1},v_{2}$ , we denote their concatenation as $(v_{1},v_{2})$ . In a graph, denote by $N(x)$ the neighbors of vertex $x$ . For standard definitions (e.g. metric and norm) and basic facts see Appendix $\\mathbf{B}$ . ", "page_idx": 1}, {"type": "text", "text": "Contrastive Learning For a set of samples $Q\\,=\\,\\{(x_{1},y_{1}^{+},z_{1}^{-}),\\ldots,(x_{m},y_{m}^{+},z_{m}^{-})\\}$ , we call an embedding $F$ consistent with $Q$ if $\\|F(x_{i})-F(y_{i})\\|_{p}\\dot{<}\\|F(\\bar{x}_{i})\\dot{-}F(z_{i})\\|_{p}$ for all $i$ . W.l.o.g., we can assume1 that $m\\leq n^{2}$ . We call a set of samples non-contradictory if one can\u2019t derive a contradiction from the inequalities between the distances. In particular, this implies the existence of a metric $\\rho$ which is consistent with $Q$ (Fact 25). ", "page_idx": 1}, {"type": "text", "text": "We prove the following theorems in Section 2, Appendix D.2, and Appendix D respectively. ", "page_idx": 1}, {"type": "text", "text": "Theorem 1 (Embedding in $\\ell_{2}$ ). Let $Q$ be a set of m non-contradictory triplet samples on a set $V$ .   \nThere is an embedding of $V$ into $\\ell_{2}$ -space $\\mathbb{R}^{O(m^{1/2})}$ which is consistent with $Q$ . Theorem 2 (Embedding in $\\ell_{\\infty}$ ). Let $Q$ be a set of m non-contradictory triplet samples on a set $V$ .   \nThere is an embedding of $V$ into $\\ell_{\\infty}$ -space $\\mathbb{R}^{O(m^{2/3})}$ which is consistent with $Q$ . Theorem 3 (Embedding in $\\ell_{p}$ ). Let $Q$ be a set of m non-contradictory triplet samples on a set $V$ .   \nFor any integer $p\\geq1$ , there is an embedding of $V$ into $\\ell_{p}$ -space $\\mathbb{R}^{O(m)}$ which is consistent with $Q$ . ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "The lower bounds are shown in Appendix E and experimental results are in Section 4. Our results for the more general version of the problem with $t$ negatives and the lower bounds are given in Table 1. ", "page_idx": 1}, {"type": "text", "text": "In Appendix $\\boldsymbol{\\mathrm F}$ we give additional results, including an extension to $t$ -negatives, NP-hardness of fining an embeddding in the minimum dimension needed to satisfy a set of contrastive constraints, and results for an approximate setting in which we only need to satisfy a fraction of the constraints. ", "page_idx": 1}, {"type": "table", "img_path": "H0qu4moFly/tmp/6a5b74e6a96d85f1e790d3d9e92ecbf04181c5e8020beae410f66a947d3eca83.jpg", "table_caption": ["Table 1: Our results for contrastive learning "], "table_footnote": [], "page_idx": 2}, {"type": "table", "img_path": "H0qu4moFly/tmp/497234faff07d2bb3e88103a88e2d21502df00c88183321dc6aa336ffd044f0c.jpg", "table_caption": ["$k$ -NN In the $k$ -NN setting, we are given the following information for each data point. ", "Table 2: Our results for $k$ -NN "], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "Definition 4 ( $k$ -NN). For a distance function $\\delta\\colon V\\times V\\ \\to\\mathbb{R}_{\\geq0},$ , let $\\pi_{1}(x),...\\,,\\pi_{n-1}(x)$ be an ordering of $V\\setminus\\{x\\}$ such that $\\delta(x,\\pi_{1}(x))\\ <\\ \\delta(x,\\pi_{2}(x))\\ <\\ {\\bar{\\cdots}}\\ <\\ \\delta(x,\\pi_{n-1}(x))$ . We define $\\mathrm{k}.\\mathrm{NN}_{\\delta}(x)=(\\pi_{1}(x),\\ldots,\\pi_{k}(x))$ as the ordered set of $k$ closest points to $x$ . ", "page_idx": 2}, {"type": "text", "text": "For a function $F\\colon V\\,\\rightarrow\\,\\mathbb{R}^{d}$ , we denote by $\\mathrm{k-NN}_{F}$ the $k$ -nearest neighbors in the $\\ell_{p}$ -space corresponding to the image of $F$ . We prove the following theorem in Section 3. ", "page_idx": 2}, {"type": "text", "text": "Theorem 5. Let $\\delta\\colon V\\times V\\rightarrow\\mathbb{R}_{\\geq0}$ be a distance function, and let $p\\,\\geq\\,1$ be a constant. There exists an embedding $F\\colon V\\to\\mathbb{R}^{d}$ of $V$ into an $\\ell_{p}$ -space of dimension $d=O(k^{10}\\log^{10}n)$ such that $\\mathrm{k}\\mathrm{-}\\mathrm{NN}_{\\delta}(x)=\\mathrm{k}\\mathrm{-}\\mathrm{NN}_{F}(x),$ , i.e. the embedding $F$ preserves the ordered set of $k$ -nearest neighbors of any point $x\\in V$ under the distance function $\\delta$ .2 ", "page_idx": 2}, {"type": "text", "text": "We note that the above result is very surprising: $k$ -NN graph in fact corresponds to $n(n-1)$ triplet constraints \u2013 for each anchor, $k-1$ comparisons between its $k$ -NN and $n-k$ comparisons between the $k$ \u2019th nearest neighbor and the rest of the points \u2013 and Theorem 1 provides only an $O(n)$ upper bound on dimension for the $\\ell_{2}$ case. Nevertheless, we are able to exploit the structure of the contrastive constraints to avoid polynomial dependence on $n$ . ", "page_idx": 2}, {"type": "text", "text": "The following theorem addresses the setting when only the ordering of the $k$ -NN has to be preserved.   \nThis, as well as other results for $k$ -NN, are presented in Table 2. ", "page_idx": 2}, {"type": "text", "text": "Theorem 6. There is an embedding of $V$ into $\\ell_{2}$ -space $\\mathbb{R}^{O(k)}$ that preserves the $k$ -NN ordering. ", "page_idx": 2}, {"type": "text", "text": "Our Techniques The key tool in our results is the notion of graph arboricity [NW61, NW64] applied to the associated constraint graph. Arboricity of an undirected graph is the minimum number of forests in which its edges can be partitioned. More intuitively, arboricity measures the \u201cdensity\u201d of the graph: sparse graphs have low arboricity, while graphs with dense subgraphs \u2013 such as cliques \u2013 have high arboricity. ", "page_idx": 2}, {"type": "text", "text": "Fact 7 (Folklore; see\u221a e.g. [BE13, DHS91] and Appendix B.2). The arboricity r of a graph $G$ with m edges is at most $\\lceil\\sqrt{m}/2\\rceil$ . Moreover, if graph $G$ has arboricity $r$ , then the following hold. ", "page_idx": 2}, {"type": "text", "text": "(a) There is an ordering $x_{1},\\ldots,x_{n}$ of $V$ such that $|N^{-}(x_{i})|\\leq2r-1$ for each $1\\leq i\\leq n$ , where $N^{-}(x_{i})=\\{x_{j}\\in N(x_{i})\\,|\\,j<i\\}$ is the set of neighbors of $x_{i}$ in $G$ preceding $x_{i}$ in the ordering. ", "page_idx": 2}, {"type": "text", "text": "(b) $G$ is $2r$ -vertex colorable. ", "page_idx": 2}, {"type": "text", "text": "Definition 8 (Constraint graph). In contrastive learning, for a set $Q$ of samples on $V$ , we define the constraint graph $G=(V,E)$ as follows: for each sample $(x_{i},y_{i}^{+},z_{i}^{-})\\in Q$ , we add two edges $\\{x_{i},y_{j}\\}$ and $\\{x_{i},z_{i}\\}$ to $E$ , unless they already exist. In the $k$ -NN setting, for each $x$ and its nearest neighbors $\\pi_{1}(x),\\ldots,\\pi_{k}(x)$ , we add edges $\\{\\dot{x},\\pi_{i}(x)\\}\\,f o r\\,1\\leq i\\leq k$ . ", "page_idx": 2}, {"type": "text", "text": "Note that by Fact 7 the arboricity of the constraint graph resulting from $m$ samples is at most $\\sqrt{m}$ . The arboricity of the $k$ -NN constraint graph is at most $k+1$ (See Lemma 27). We show bounds on the embedding dimension in terms of arboricity, e.g. for $\\ell_{2}$ we prove the following in Section 2. ", "page_idx": 3}, {"type": "text", "text": "Theorem 9. Given a set of non-contradictory inequalities among pairwise distances in $V$ whose constraint graph has arboricity $r$ , there exists an embedding of $V$ into $\\ell_{2}$ -space $\\mathbb{R}^{4r}$ which satisfies all these inequalities. ", "page_idx": 3}, {"type": "text", "text": "Theorem 1 follows from Theorem 9 by using $r\\leq\\lceil{\\sqrt{m}}/2\\rceil$ (Fact 7). Moreover, since the arboricity of the constraint graph for $\\mathrm{k\\Omega}$ -NN at most $k+1$ (Lemma 27), Theorem 9 shows that preserving the ordering of the $k$ -NN in $\\ell_{2}$ requires $O(k)$ dimension. Furthermore, the following theorem, proven in Section 3.1, implies that ${\\tilde{O}}(k^{2})$ dimension suffices to preserve orderings of the $\\mathrm{k\\Omega}$ -NNs in $\\ell_{p}$ . ", "page_idx": 3}, {"type": "text", "text": "Theorem 10. Given a set of non-contradictory inequalities among pairwise distances in $V$ whose constraint graph has arboricity $r$ , for any real $p\\geq1$ , there exists an embedding of $V$ into $\\ell_{p}$ -space RO(r2log3n) which satisfies all these inequalities. ", "page_idx": 3}, {"type": "text", "text": "While the above constructions suffice for the contrastive learning case and for preserving the ordering of the $k$ -NN, the set of the nearest neighbors can change under the embeddings above. Hence, in order to preserve the $k$ -NN, we increase the dimension to separate neighbors from non-neighbors. In particular, we construct the extended part of the embedding randomly, using a sampling scheme which is guaranteed to embed neighbors much closer than non-neighbors. See Section 3.2 for more details and a proof of Theorem 5. ", "page_idx": 3}, {"type": "text", "text": "For $\\ell_{\\infty}$ , instead of arboricity, we use a related fact: by removing a set $V_{\\mathrm{high}}$ of $O(m^{2/3})$ high-degree vertices, we reduce the maximum degree of the remaining graph (i.e. $\\bar{V_{\\mathrm{low}}}=V\\setminus V_{\\mathrm{high}})$ to at most $O(m^{1/3})$ . We handle each set differently (points in $V_{\\mathrm{low}}$ using graph colorings, and points in $V_{\\mathrm{high}}$ using a Freche\u00b4t-like embedding). See Appendix D.2 for the details and the proof of Theorem 2. ", "page_idx": 3}, {"type": "text", "text": "1.2 Previous Work ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Understanding the underlying geometry of a given set of $n$ points based only on comparisons between pairs of distances is a basic question studied in the literature of non-metric embeddings (also known as ordinal embeddings or monotone maps). In a wide range of applications such as ranking, crowdsourcing, nearest-neighbor search, ad placement, recommendation systems, etc., the exact distances are not as important as their relative order. In fact, some of the early results in the field were motivated by applications in mathematical psychology [Tor52, She62, She74, CS74, Kru64a, Kru64b], and since then ordinal information and embeddings have been used in ranking [OG08, Ail12, WJJ13], metric learning $[\\mathrm{CHX^{+}19}]$ , clustering [VD16, GPvL19, KVH16], crowdsourcing $[\\boldsymbol{\\mathrm{TLB}}^{+}\\boldsymbol{1}\\boldsymbol{1}$ , JN11a, JN11b] and modeling human perception [ML09]. Note that the goal in ordinal embeddings is quite different from the vast literature on metric embeddings (e.g., see [Mat13, IMS17]) where the goal is to approximately preserve the numerical values of distances. ", "page_idx": 3}, {"type": "text", "text": "We study the question of finding the smallest dimension $d$ required to represent a given set of $n$ points such that a given set of $m$ distance comparisons are preserved. Related questions have been studied under statistical assumptions and it is known [KL14, TL14, GCY19] that for the large $n$ regime, upon knowledge of the ordinal relationships, the set of points can be approximately recovered (up to certain transformations). This serves as further motivation for studying ordinal information as it highlights its power in recovering the underlying geometry of the data points. ", "page_idx": 3}, {"type": "text", "text": "However, determining the exact relationships between the dimension $d$ , the number of points $n$ and the number of given constraints $m$ has been elusive. Most papers assume that all $\\Theta(n^{4})$ distance comparisons $\\delta(x_{i},x_{j})\\ \\lesssim\\ \\delta(x_{k},x_{l})$ among the pairwise distances are known. In [BL05, $\\mathrm{ABD^{+}08}$ , $\\mathrm{BDH^{+}08]}$ , for example, lower bounds are given for the dimension needed to preserve these comparisons. However, having access to such a large number of comparisons is prohibitive in practice. We only assume access to a set of $m$ distance comparisons and hence these lower bounds do not apply. ", "page_idx": 3}, {"type": "text", "text": "Contrastive learning has been studied for $d=1$ (embedding on the line) by $[\\mathrm{FIM}^{+}20]$ for dense instances, i.e. $m={\\Theta}(n^{3})$ . For higher dimensions, [CI24] gives an $\\Omega\\left(n\\right)$ lower bound on the smallest dimension (only for $\\ell_{2}$ ) that preserves all $\\Theta(n^{3})$ triplet comparisons. Our Theorem 1 improves this bound for the general case when $m$ triplet samples are given, without density assumptions. Then, our Theorems 2 and 3 go beyond $\\ell_{2}$ other $\\ell_{p}$ -norms. Our results can also be seen as the reverse direction of the recent work by $[\\mathrm{AAE^{+}}24]$ . In $[\\mathrm{AAE^{+}}24]$ , the central question is quantifying the amount of data required for generalization in contrastive learning, assuming that the data can be embedded into an $\\ell_{p}$ -space of fixed dimension. Here we assume that the data is fixed instead and study the embedding dimension. Combined with $[\\mathrm{AAE^{+}}24]$ , this completes the picture of the relationship between the size of data, its embedding dimension and generalization. ", "page_idx": 3}, {"type": "image", "img_path": "H0qu4moFly/tmp/c05ea24aa78e1151730133ab6f80c3583688a56489b4d88d4938b6092cd71ec7.jpg", "img_caption": ["Figure $1\\colon{\\hat{x}}$ is chosen so that if $w(x,y)>w(x^{\\prime},y^{\\prime})$ , then $\\left\\langle{\\hat{x}},{\\hat{y}}\\right\\rangle<\\left\\langle{\\hat{x}}^{\\prime},{\\hat{y}}^{\\prime}\\right\\rangle$ . $\\mathring{x}$ ensures that all vectors have the same norm, i.e. $\\|\\mathring{x}\\|_{2}^{2}=\\overset{\\cdot}{W}$ for all $x\\in V$ . ", "Figure 2: Example construction of $\\hat{x}$ . The embedding $\\hat{x}_{4}$ is computed based on the embeddings of its already processed neighbors $\\hat{x}_{1}$ , $\\hat{x}_{2}$ , $\\hat{x}_{3}$ . We find the solution $\\hat{x}_{4}$ to the linear system so that, for each edge to a preceding neighbor, the inner product equals the rank of the edge. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Our second setting ( $k$ -NNs) was also studied in [CI24] who showed a lower bound of $d=\\Omega(k)$ for preserving the ordering of the neighbors (again in $\\ell_{2}$ ). To the best of our knowledge, prior to our work, there was no known upper bound for the smallest dimension and here we provide a matching upper bound. Furthermore, we provide new results for $k$ -NNs embeddings (both upper and lower bounds) under various $\\ell_{p}$ metrics and results for the stronger setting when not just the ordering of the neighbors but also their status as $k$ -NN has to be preserved. ", "page_idx": 4}, {"type": "text", "text": "2 Contrastive Learning in $\\ell_{2}$ Norm ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we prove Theorem 9 \u2013 that contrastive queries with the constraint graph $G=(V,E)$ (Definition 8) of arboricity $r$ are preserved when the points are embedded into $\\ell_{2}$ space of dimension $4r$ \u2013 from which Theorem 1 and Theorem 6 follow. Fix a distance function $\\delta\\colon V\\times V\\rightarrow\\mathbb{R}_{\\geq0}$ that satisfies the given set of inequalities (such a function exists by Fact 25). We order all pairs of neighboring vertices by the distance function $\\delta$ in descending order, and let $w(x,y)=i$ if $\\bar{\\{x,y\\}}$ is the $i$ -th pair in the ranking. Recall that $\\left\\|F(x)-F(y)\\right\\|^{2}=\\left\\|F(x)\\right\\|^{2}+\\left\\|F(y)\\right\\|^{2}-2\\left\\langle F(x),F(\\bar{y})\\right\\rangle.$ In our construction, all embeddings have the same norm, and hence the distances depend only on the inner products between the embeddings. ", "page_idx": 4}, {"type": "text", "text": "We split the embedding $F\\colon V\\,\\rightarrow\\,\\mathbb{R}^{4r}$ into two parts, i.e. for a point $x$ let $\\boldsymbol{F}(\\boldsymbol{x})\\,=\\,(\\hat{x},\\overset{\\circ}{x})$ , where $\\hat{x}\\in\\dot{\\mathbb{R}}^{2r}$ and $\\mathring{x}\\in\\mathbb{R}^{2r}$ . For neighboring points $x$ and $y$ , our choices of $\\hat{x}$ and $\\hat{y}$ ensure that $\\left\\langle\\hat{x},\\hat{y}\\right\\rangle\\approx$ $w(x,y)$ . We embed the points one by one into $\\mathbb{R}^{h}$ in the arboricity ordering $x_{1},\\ldots,x_{n}$ , which by Fact 7 ensures that for every vertex, the number of neighbors with smaller indices is at most $h$ . When embedding $x_{i}$ , we make sure that for any neighbor $x_{j}\\in N^{-}(x_{i})$ (i.e. a neighbor $x_{j}$ of $x_{i}$ such that $j<i)$ it holds that $\\langle\\hat{x}_{i},\\hat{x}_{j}\\rangle\\approx w(x_{i},x_{j})$ . This requires solving a linear system over $\\hat{x}_{i}$ with at most $h$ equations, and hence with $h$ variables, with slight perturbations, the solution always exists. ", "page_idx": 4}, {"type": "text", "text": "The choices of $\\mathring{x}_{i}$ ensure that all vectors have the same norm while preserving the inner products. This is done by coloring the vertices of the constraint graph in $h$ colors using Fact 7 and assigning each color to a unique basis vector, which is scaled to equalize the norms. Since these basis vectors are orthogonal, the inner product between any two neighboring points $x_{i}$ and $x_{j}$ is $\\langle\\hat{x}_{i},\\hat{x}_{j}\\rangle$ . ", "page_idx": 4}, {"type": "text", "text": "Construction of ${\\hat{x}}_{i}$ Assume $\\hat{x}_{1},\\dotsc,\\hat{x}_{i-1}$ have already been chosen. Let $N^{-}(x_{i})=\\{x_{j}\\in N(x_{i})\\mid\\;$ $j<i\\}$ be the set of preceding neighbors of $x_{i}$ in $G$ . For each $x_{j}\\,\\in\\,N^{-}(x_{i})$ , let a linear equation $P(i,j)$ be $\\langle\\hat{x}_{i},\\hat{x}_{j}\\rangle=w(x_{i},x_{j})$ , where we consider the coordinates of ${\\hat{x}}_{i}$ as variables (recall that $\\hat{x}_{j}$ is already set for all $x_{j}\\in N^{-}(x_{i}))$ . In Appendix $\\mathbf{C}$ we show the following. ", "page_idx": 4}, {"type": "text", "text": "By Lemma 11, the system of linear equations $P_{i}\\;=\\;\\{P(i,j)\\;\\mid\\;x_{j}\\;\\in\\;N^{-}(x_{i})\\}$ has a solution $\\boldsymbol{v}\\in\\mathbb{R}^{2r}$ . Let $B(v)$ be a ball centered at $v$ with sufficiently small radius such that for any $v^{\\prime}\\in B(v)$ it holds that $|\\left\\langle v^{\\prime},\\hat{x}_{j}\\right\\rangle-w(x_{i},x_{j})|\\,<\\,1/3$ for all $x_{j}\\,\\in\\,N^{-}(x_{i})$ . Choose a point $v^{\\prime}$ uniformly at random from $B(v)$ , and set $\\hat{x}_{i}\\,=\\,v^{\\prime}$ : this random perturbation guarantees that, with probability 1, Lemma 11 holds in future iterations. By construction, the following property holds. ", "page_idx": 5}, {"type": "text", "text": "Proposition 12. For any $x$ and any $y\\in N^{-}(x)$ , we have $|\\left\\langle\\hat{x},\\hat{y}\\right\\rangle-w(x,y)|<1/3.$ ", "page_idx": 5}, {"type": "text", "text": "Construction of $\\mathring{x}_{i}$ Let $W=2\\operatorname*{max}_{x\\in V}\\|{\\hat{x}}\\|_{2}^{2}$ . By Fact 7, there exists vertex coloring $C\\colon V\\to[h]$ of $G$ , such that $C(x)\\neq C(y)$ for any pair $\\{x,y\\}\\in E$ . Set $\\mathring{x}=\\alpha_{x}e_{C(x)}$ , where $e_{C(x)}$ is the standard basis vector in the $C(x)$ -th coordinate, and $\\alpha_{x}$ is chosen so that $\\|F(x)\\|_{2}^{2}\\,=\\,\\|\\hat{x}\\|_{2}^{2}+\\|\\mathring{x}\\|_{2}^{2}\\,=\\,W$ (note that $\\alpha_{x}$ exists because $\\|\\hat{x}\\|_{2}^{2}\\leq W)$ . By construction, the following property holds. ", "page_idx": 5}, {"type": "text", "text": "Proposition 13. For any edge $\\{x,y\\}\\in E$ , we have $\\langle\\mathring{x},\\mathring{y}\\rangle=0$ . ", "page_idx": 5}, {"type": "text", "text": "Proof of Theorem $^{\\,g}$ (sufficient dimension for $\\ell_{2}$ embeddings). For any edge $\\{u,v\\}\\in E$ it holds that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\boldsymbol{F}(\\boldsymbol{u})-\\boldsymbol{F}(\\boldsymbol{v})\\|_{2}^{2}=\\|\\boldsymbol{F}(\\boldsymbol{u})\\|_{2}^{2}+\\|\\boldsymbol{F}(\\boldsymbol{v})\\|_{2}^{2}-2\\left<\\hat{\\boldsymbol{u}},\\hat{\\boldsymbol{v}}\\right>-2\\left<\\hat{\\boldsymbol{u}},\\hat{\\boldsymbol{v}}\\right>.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "By the choice of $\\mathring{u}$ and $\\mathring{v}$ , we have $\\|\\boldsymbol{F}(\\boldsymbol{u})\\|_{2}^{2}=\\|\\boldsymbol{F}(\\boldsymbol{v})\\|_{2}^{2}=W$ . By Proposition 13, $\\langle\\mathring{u},\\mathring{v}\\rangle=0$ , and hence the distance depends only on $\\langle\\hat{u},\\hat{v}\\rangle$ . For any $(x,y^{+},z^{-})\\in Q$ , we have $\\|F(\\boldsymbol{\\lambda})-\\dot{F}(\\boldsymbol{y})\\|_{2}^{2}<$ $\\|F(x)-F(z)\\|_{2}^{2}$ iff $\\bar{\\langle x,\\hat{y}\\rangle}\\,>\\,\\bar{\\langle x,\\hat{z}\\rangle}$ . By Proposition 12, for any edge $\\{x,y\\}$ in $G$ it holds that $|\\left\\langle\\hat{x},\\hat{y}\\right\\rangle-w(x,y)|<1/3$ . Since the function $w$ assigns only integer values, it holds that $\\langle\\hat{x},\\hat{y}\\rangle>$ $\\langle\\hat{x},\\hat{z}\\rangle$ if and only if $w(\\dot{x},y)<w(x,z)$ , hence preserving the ranking of the edges. \u53e3 ", "page_idx": 5}, {"type": "text", "text": "3 Preserving $k$ Nearest Neighbors ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we focus on $k$ nearest neighbors, and namely we prove Theorems 5 and 10. Let $G=(V,E)$ be the constraint graph (Definition 8) for given $k$ -NN input. In Section 3.1, we show how to preserve the order between the neighbors in this graph, and in Section 3.2 we show how to separate neighbors from non-neighbors. Combined, these results fully preserve the $k$ -NNs. ", "page_idx": 5}, {"type": "text", "text": "To simplify the presentation, we focus on the case $p=1-$ the construction for other $p$ is identical, with the change being that each embedding coordinate value $c$ should be replaced with $c^{1/p}$ . In this section, let $\\delta(u,\\bar{v})\\,=\\,\\delta_{\\ell_{1}}(u,v)$ . For a non-contradictory set of samples $Q$ , by Fact 25 there exists a metric $\\delta^{\\prime}$ consistent with $Q$ . We order all pairs of neighboring vertices by the value of $\\delta^{\\prime}$ in descending order, and let $w(x,y)=t$ if $(x,y)$ is the $t$ -th pair in the ranking. Given an embedding $F$ , let $\\alpha F$ be a re-scaling of the embedding by a factor of $\\alpha$ , i.e. multiplying each coordinate by $\\alpha$ . ", "page_idx": 5}, {"type": "text", "text": "3.1 Preserving the Ordering of the $\\mathrm{k\\Omega}$ -NN ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we show Theorem 10. This embedding is also used as a part of Theorem 5, shown in Section 3.2. Our embedding uses a new coloring scheme we call Neighbor-Collection Coloring. Let $x_{1},\\ldots,x_{n}$ be the arboricity ordering (Fact 7) and $N^{-}(x_{i})=\\{x_{j}\\mid\\{x_{i},x_{j}\\}\\in E,j<i\\}$ be the set of neighbors of $x_{i}$ preceding $x_{i}$ in the ordering. ", "page_idx": 5}, {"type": "text", "text": "Definition 14 (NCC Scheme). A neighbor-collection coloring scheme is a set of $K=\\Theta(r\\log n)$ vertex colorings $C^{(1)},\\ldots,C^{(K)}$ , where $C_{x}^{(j)}\\in[r]$ for any $x\\in V$ and $j\\,\\in\\,[K]$ , such that for any $x\\in V$ the following holds: ", "page_idx": 5}, {"type": "text", "text": "\u2022 (Collection) for any $y\\,\\in\\,N^{-}(x),$ , there exists a coloring $j~\\in~[K]$ such that $C_{x}^{(j)}\\,=\\,C_{y}^{(j)}$ , and $C_{z}^{(j)}\\neq C_{x}^{(j)}$ for any $z\\in N^{-}(x)\\setminus\\{y\\}$ . ", "page_idx": 5}, {"type": "text", "text": "\u2022 (Load) for any $j\\in[K]$ , the number of prior neighbors with $j$ -th color being the same as ${C_{x}^{j}}^{\\rangle}$ is small: $|\\{y\\in N^{-}(x)\\,|\\,\\bar{\\,}C_{x}^{(j)}=C_{y}^{(j)}\\}|=O(\\log n)$ . ", "page_idx": 5}, {"type": "text", "text": "Intuitively, each coloring corresponds to a part of the embedding. When the colors $C_{x}^{(j)},C_{y}^{(j)}$ are different, the $j$ \u2019th part of the embedding always contributes 2 to the distance between $x$ and $y$ . Otherwise, we can select the $j$ \u2019th part so that it contributes either 2 or 0, and the collection property guarantees that for any $y\\,\\in\\,N^{-}(x)$ such a part exists. The load property guarantees that for each part we always have enough choices to get distance 2. Finally, we represent $w(x,y)$ in binary format for all $x,y$ , and, using an NCC scheme, we recover $w(x,y)$ bit-by-bit. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Lemma 15. There exists an NCC scheme for the constraint graph $G$ . ", "page_idx": 6}, {"type": "text", "text": "Proof. For each $x\\in V$ and $j\\in[K]$ , we choose $C_{x}^{(j)}$ i.i.d. uniformly at random from $[r]$ . First, note that the load property holds: for any $j\\,\\in\\,[K]$ and $y\\,\\in\\,N^{-}(x)$ , we have $\\mathbb{P}\\left[C_{x}^{(j)}=C_{y}^{(j)}\\right]=1/r$ . By Fact 7, we have $|N^{-}(x)|\\,\\leq\\,2r$ , and by the Chernoff bound, color $C_{x}^{(j)}$ occurs no more than $O(\\log n)$ times in $N^{-}(x)$ w.h.p. By the union bound, the load property holds w.h.p. for all $j$ . Next, for any fixed $x\\in V$ , $y\\,\\in\\,N^{-}(x)$ , and $j\\in[K]$ , let $A^{(j)}(x,y)$ be the event that $y$ is the only point in N \u2212(x) such that Cx(j) $C_{x}^{(j)}=C_{y}^{(j)}$ . Since the colorings are selected uniformly at random, we have $\\mathbb{P}\\left[A^{(j)}(x,y)\\right]\\,=\\,\\Omega(1/r)$ . Since $K=O(r\\log n)$ , by Chernoff, w.h.p. there exists $j\\,\\in\\,[K]$ such that $A^{(j)}(x,y)$ occurs. By the union bound, the collection property holds w.h.p. \u53e3 ", "page_idx": 6}, {"type": "text", "text": "Definition 16 (NCC-Embedding). Given a graph $G$ and an NCC scheme, an NCC-embedding is an embedding of dimension $O(r^{2}\\log^{2}n)$ of the following form. Associate each color $i\\,\\in\\,[r]$ with $M=O(\\log n)$ unique basis vectors $\\mathcal{B}(i)=\\left\\{e_{(i-1)M+1},e_{(i-1)M+2},...\\,,e_{i M}\\right\\}$ . The embedding of point $x$ is comprised of $K$ parts ${\\mathring{x}}^{(1)},\\ldots,{\\mathring{x}}^{(K)}$ , where each part is a basis vector $\\mathring{x}^{(j)}\\in B(C_{x}^{(j)})$ , i.e. $\\mathring{x}^{(j)}$ is one of the basis vectors associated with color $C_{x}^{(j)}$ . ", "page_idx": 6}, {"type": "text", "text": "Lemma 17. Let $D\\colon E\\to\\{0,1\\}$ be a mapping of each edge, with 1 meaning \u201cclose\u201d and 0 meaning \u201cfar\u201d. For each $x\\in V$ there exists embedding $\\mathring{x}$ into $O(r^{2}\\log^{2}n)$ dimensions such that for any $\\{x,y\\}\\in E,$ , it holds that $\\delta(\\mathring{x},\\mathring{y})=K-D(x,y)$ . ", "page_idx": 6}, {"type": "text", "text": "Proof. Let $(C^{(1)},\\ldots,C^{(K)})$ be an NCC scheme of $G$ . We embed the points one by one according to the arboricity ordering $x_{1},\\ldots,x_{n}$ as in Fact 7. We assume by induction that all nodes $x_{1},\\ldots,x_{i-1}$ are embedded using an NCC-embedding. For each $y\\in N^{-}\\dot{(x)}$ , fix one index $j(y)$ such that under $C^{(j(y))}$ the points $x,y$ have the same color, which is different from colors of other points from $N^{-}(x)$ (such $j(y)$ exists by the collection property). Let $J=\\{j(y)\\mid y\\in N^{-}(x)\\}$ , and, since for any two points in $\\ensuremath{\\boldsymbol{N}}^{-}(\\ensuremath{\\boldsymbol{{x}}})$ the chosen index is distinct, $|J|=|N^{-}(x)|$ . ", "page_idx": 6}, {"type": "text", "text": "For each part $j\\in[K]\\backslash[J]$ , we choose $x^{(j)}$ to be a basis vector from $B(C_{x}^{(j)})$ that is different from all basis vectors $\\{\\mathring{y}^{(j)}\\mid y\\in N^{-}(x)\\}$ . This can be done, since, on the one hand, for each $C_{x}^{(j)}\\neq C_{y}^{(j)}$ , all basis vectors of $B(C_{x}^{(j)})$ are different from $\\mathring{y}^{(j)}$ , and, on the other hand, by the load property there are less than $O(\\log n)$ points $y\\in N^{-}(x)$ such that $C_{x}^{(j)}=C_{y}^{(j)}$ . Therefore, we can choose a basis vector that is different from any taken by these $O(\\log n)$ points. ", "page_idx": 6}, {"type": "text", "text": "For each part $j(y)\\,\\in\\,[J]$ , we select the basis vector based on $D$ . If $D(x,y)\\,=\\,1$ , then we take $\\mathring{x}^{(j(y))}=\\mathring{y}^{(j(y))}$ . Otherwise, we pick a basis vector $\\mathring{x}^{(j(y))}\\in B(C_{x}^{(j(y))})$ such that $\\mathring{x}^{(j(y))}\\neq\\mathring{y}^{(j(y))}$ . ", "page_idx": 6}, {"type": "text", "text": "We now show that distance between embeddings is $2(K-1)$ if the points are close, and is $2K$ otherwise. The result follows by scaling the embedding. Let $\\{x,y\\}\\,\\in\\,E$ such that $y\\,\\in\\,N^{-}(x)$ . Let $I^{(j)}(x,y)\\,=\\,1$ if $\\mathring{x}^{(j)}\\neq\\mathring{y}^{(j)}$ , and $I^{(j)}(x,y)=0$ otherwise. Since each part is a basis vector, $\\begin{array}{r}{\\delta(\\mathring{x},\\mathring{y})=2\\sum_{j\\in[K]}I^{(j)}(x,y)}\\end{array}$ . By construction, for any $j\\in[J]\\backslash\\{j(y)\\}$ it holds that $I^{(j)}(x,y)=1$ . For $j(y)$ we have $\\dot{I}^{(j(y))}(x,y)\\,=\\,1\\,-\\,D(x,y)$ , i.e. $\\delta(\\mathring{x},\\mathring{y})\\,=\\,2(K\\,-\\,D(x,y))$ . Rescaling the embedding vectors by a factor of $1/2$ completes the proof. \u53e3 ", "page_idx": 6}, {"type": "text", "text": "Corollary 18. Let $a^{\\prime}$ be a power of 2 such that for all $\\{x,y\\}\\in E$ we have $a_{x,y}\\in\\{0,\\ldots,a^{\\prime}\\}$ . Then there exists an embedding $F$ of $V$ into $O(r^{2}\\log^{2}n\\log a^{\\prime})$ dimensions such that for any $\\{x,y\\}\\in E$ , we have \u03b4(F(x), F(y)) = K(a\u2032 \u22121) \u2212ax,y. ", "page_idx": 6}, {"type": "text", "text": "Proof. Let $\\mathrm{Bin}^{(i)}(x,y)$ be the $i$ \u2019th bit of the binary encoding of $a_{x,y}$ using a string of size $\\log_{2}a^{\\prime}$ bits. Let $F_{1},\\ldots,F_{\\log_{2}a^{\\prime}}$ be embeddings as in Lemma 17, where for each $F_{i}$ we choose $D_{i}\\ =$ $\\mathrm{Bin}^{(i)}(x,y)$ . For embedding $F(x)=\\left(F_{1}(x),2F_{2}(x),\\dots,2^{i}F_{i}(x),\\dots,(a^{\\prime}/2)F_{\\log_{2}a^{\\prime}}(x)\\right)$ we have ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\delta(F(x),F(y))=\\displaystyle\\sum_{i=1}^{\\log_{2}a^{\\prime}}\\Big(K-\\mathrm{Bin}^{(i)}(x,y)\\Big)\\cdot2^{i-1}}\\\\ &{\\qquad\\qquad=K\\displaystyle\\sum_{i=1}^{\\log_{2}a^{\\prime}}2^{i-1}-\\displaystyle\\sum_{i=1}^{\\log_{2}a^{\\prime}}\\mathrm{Bin}^{(i)}(x,y)\\cdot2^{i-1}=K(a^{\\prime}-1)-a_{x,y}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Theorem 10 follows immediately from Corollary 18 by taking $a_{x,y}=w(x,y)$ and $a^{\\prime}\\geq m^{\\prime}$ . ", "page_idx": 7}, {"type": "text", "text": "3.2 Fully Preserving k-NN ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we prove Theorem 5, which states the existence of an embedding with dimension $d=O(k^{10}\\log^{10}n)$ that preserves the $k$ -NN. Our approach can be summarized as follows: for each $x\\in V$ , the final embedding is $F(x)\\,=\\,(2m\\hat{x},\\mathring{x})$ (Figure 3). The goal of $\\hat{x}$ is to have all nonneighbors $\\{x^{\\prime},y^{\\prime}\\}\\notin E$ be at a larger distance than any neighbors $\\{x,y\\}\\in E$ , i.e. for some large $W$ it holds that $\\delta(\\hat{x},\\hat{y})+W<\\delta(\\hat{x^{\\prime}},\\hat{y}^{\\prime})$ . The goal of $\\mathring{x}$ is to order the distances of neighboring pairs $\\{x,y\\}\\in E$ according to their rank, while still keeping non-neighbors further away than neighbors. ", "page_idx": 7}, {"type": "text", "text": "We choose $\\hat{x}^{(j)}$ via a random process, so that for any two neighbors $\\{x,y\\}\\in E$ we have $\\hat{x}^{(j)}=\\hat{y}^{(j)}$ with some probability $p_{1}$ (and otherwise they have substantial distance), while for non-neighbors $\\{x,y\\}\\not\\in E$ , we have $\\bar{\\hat{x}^{\\left(j\\right)}}=\\hat{y}^{\\left(j\\right)}$ with much smaller probability $p_{2}\\ll p_{1}$ . Repeating this process, we get a separation in distances between neighbors and non-neighbors. ", "page_idx": 7}, {"type": "table", "img_path": "H0qu4moFly/tmp/ba049d5252c028c132645f46dfc7b310c0f58f6a75638a6935cb5eddbc663e3e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 3: Structure of embedding for fully preserving $\\mathrm{k\\Omega}$ -NN. $\\hat{x}$ guarantees that non-edges have very large distance, i.e. if $\\{x,y\\}\\in E$ and $\\{x^{\\prime},y^{\\prime}\\}\\notin E$ , then $\\delta(\\hat{x},\\hat{y})\\ll\\delta(\\hat{x}^{\\prime},\\hat{y}^{\\prime})$ . $\\mathring{x}$ orders the edges. ", "page_idx": 7}, {"type": "text", "text": "Choosing $\\hat{x}$ : The embedding $\\hat{x}$ is comprised of $L=\\Theta(r^{4}\\log^{4}n)$ parts, i.e. $\\hat{x}=(\\hat{x}^{(1)},\\ldots,\\hat{x}^{(L)})$ . We take each part $\\hat{x}^{(j)}$ to be a vector from a design [DKS12] \u2013 a large family of vectors which are approximately equidistant. ", "page_idx": 7}, {"type": "text", "text": "Definition 19 $\\left(\\alpha,R\\right)$ -design). For integer $R$ and value $0<\\alpha<1$ , an $(\\alpha,R)$ -design is a family of sets $\\tau$ , such that (a) for each $S_{i}\\in{\\mathcal{T}}$ , $\\bar{S}_{i}\\subseteq[R^{2}]$ , $(b)$ for each $S_{i}\\in{\\mathcal{T}}$ , $|S_{i}|=R$ , and (c) for each two distinct sets $S_{i},S_{j}\\in\\mathcal{T}$ , $|S_{i}\\cap S_{j}|\\le\\alpha R$ . ", "page_idx": 7}, {"type": "text", "text": "Lemma 20 (Lemma 1, [DKS12]). For any sufficiently large integer $R$ and any value $0\\textless\\alpha\\textless1$ , there exists an $(\\alpha,R)$ -design $\\tau$ of size at least $2^{\\alpha R\\log_{2}^{\\cdot}R}$ . ", "page_idx": 7}, {"type": "text", "text": "Let $\\tau$ be a $(\\alpha,R)$ -design for $R\\,=\\,\\Theta(r^{3}\\log^{3}n)$ and $\\alpha\\,=\\,\\Theta(\\log n/R)$ , where $r$ is arboricity of the constraint graph (constants specified below). We associate $S\\in\\tau$ with a binary vector $I(\\dot{S})\\in$ $\\{0,1\\}^{R^{2}}$ as an indicator vector of the set $S$ , i.e. for $i\\in[R^{2}]$ we have $I(S)[i]=1$ iff $i\\in S$ . For each $x\\in V$ , we choose unique sets $S_{x}$ , $,S_{x}^{\\prime}\\in\\mathcal{T}$ and denote $\\bar{I}_{x}\\doteq I(S_{x}),I_{x}^{\\prime}=\\bar{I}(S_{x}^{\\prime})$ . By Lemma 20, the number of sets is $2^{\\alpha R\\log_{2}R}=2^{\\Omega(\\log n)}$ , exceeding $2n$ for appropriate choices of constants. ", "page_idx": 7}, {"type": "text", "text": "We choose each part $\\hat{x}^{\\left(j\\right)}$ independently of the rest as follows. For $p\\;=\\;O(1/(r\\log n))$ , with probability $1-p$ , we choose $\\hat{x}^{(j)}\\,=\\,I_{x}$ , and otherwise choose a uniformly random $i\\,\\in\\,[2r]$ . If $i\\,\\,\\le\\,\\,|N^{-}(x)|$ , set ${\\hat{x}}^{(j)}\\,=\\,I_{y}$ , where $y\\;\\in\\;N^{-}(x)$ is the $i'$ th point in $N^{-}(x)$ according to some ordering, and set ${\\hat{x}}^{(j)}=I_{x}^{\\prime}$ otherwise. Let $\\begin{array}{r}{\\gamma={\\frac{(1-p)p}{2r}}}\\end{array}$ (1\u22122rp)pbe the probability that x and y choose Iy. ", "page_idx": 7}, {"type": "text", "text": "Importantly, in this construction, neighbors are significantly more likely to sample the same vector compared with non-neighbors. Moreover, sampling the same vector contributes 0 to the distance be$K$ eies nd eefminbeedd daisn ign,  wDehifline istiaomn p1li4n, gl edti $c=\\operatorname*{max}(\\frac{r\\log n}{100K},\\frac{1}{100})$ u tbees  aa tc loenasstta $(2\\!-\\!\\alpha)R$ e tt $\\begin{array}{r}{\\alpha\\leq\\frac{c\\gamma}{8r\\log_{2}n}}\\end{array}$ aFnodr $R=\\lceil\\log_{2}n/\\alpha\\rceil$ . In Appendix C.1 we justify these choices of parameters and show the following. ", "page_idx": 7}, {"type": "text", "text": "Lemma 21. With high probability, the following bounds hold. ", "page_idx": 8}, {"type": "text", "text": "\u2022 If $\\mathbf{:}\\{x,y\\}\\notin E,$ , then $\\begin{array}{r}{\\vert\\delta(\\hat{x},\\hat{y})-2R L\\vert\\leq\\frac{c}{r\\log_{2}n}\\gamma R L}\\end{array}$ \u2022 I $f\\left\\{x,y\\right\\}\\in E$ , then $\\begin{array}{r}{\\vert\\delta(\\hat{x},\\hat{y})-2(1-\\gamma)R L\\vert\\leq\\frac{c}{r\\log_{2}n}\\gamma R L.}\\end{array}$ ", "page_idx": 8}, {"type": "text", "text": "That is, according to the embedding, the gap between neighbors\u2019 distances and non-neighbor\u2019 distances is larger than the maximum difference between neighbors\u2019 distances. ", "page_idx": 8}, {"type": "text", "text": "The final dimension is $O(r^{10}\\log^{10}n)$ : $L=\\Theta(r^{4}\\log^{4}n)$ parts of dimension $R^{2}\\,=\\,\\Theta(r^{6}\\log^{6}n)$ .   \nSince $r=O(k)$ (Lemma 27), it follows that the dimension is bounded by $O(k^{10}\\log^{10}n)$ . ", "page_idx": 8}, {"type": "text", "text": "Final Embedding Let ${\\mathring{x}}_{1},\\dots,{\\mathring{x}}_{n}$ be the embeddings from Corollary 18 with $a^{\\prime}$ being the closest power of two from above of the expression r5 lmogc \u03b3nRL. These embeddings have dimension at most $O(r^{2}\\log^{2}n\\log a^{\\prime})\\;\\;=\\;\\;O(r^{2}\\log^{3}n)$ . For $\\{x,y\\}\\;\\;\\in\\;\\;E$ , let $\\begin{array}{r l}{\\Delta(x,y)}&{{}=}\\end{array}$ $\\begin{array}{r}{\\left\\lceil2m\\left(\\delta(\\hat{x},\\hat{y})-2\\left(1-\\gamma-\\frac{\\gamma}{100}\\right)R L\\right)\\right\\rceil}\\end{array}$ . Set $a_{x,y}=\\Delta(x,y)+w(x,y)$ , where $w(x,y)$ is the ranking of edge $\\{x,y\\}$ if the edges are sorted by the decreasing order of distances. By Lemma 21, we have $\\begin{array}{r}{0\\le\\Delta(x,y)\\le\\frac{4m c\\gamma}{r\\log n}R L}\\end{array}$ r4 lmogc \u03b3nRL w.h.p., and hence ax,y \u2264r $\\begin{array}{r}{a_{x,y}\\leq\\frac{4m c\\gamma}{r\\log n}R L+m\\leq a^{\\prime}}\\end{array}$ . Finally, $F(x)=(2m{\\hat{x}},{\\overset{\\circ}{x}})$ . ", "page_idx": 8}, {"type": "text", "text": "Proof of Theorem 5. For each $x\\in V$ , let $F(x)=(2m{\\hat{x}},{\\stackrel{\\circ}{x}})$ . It suffices to show the following. ", "page_idx": 8}, {"type": "text", "text": "(a) For any $\\{x,y\\}\\ \\in\\ E$ and $\\{x^{\\prime},y^{\\prime}\\}\\ \\in\\ E$ , it holds that $w(x,y)~<~w(x^{\\prime},y^{\\prime})$ if and only if $\\delta(F(x),F(y))>\\delta(F(x^{\\prime}),F(y^{\\prime}))$ . ", "page_idx": 8}, {"type": "text", "text": "(b) For any $\\{x,y\\}\\in E$ and $\\{x^{\\prime},y^{\\prime}\\}\\notin E$ , it holds that $\\delta(F(x),F(y))<\\delta(F(x^{\\prime}),F(y^{\\prime})).$ . ", "page_idx": 8}, {"type": "text", "text": "By Corollary 18, for any $\\{x,y\\}\\in E$ : ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\delta(F(x),F(y))=K(a^{\\prime}-1)-\\left[2m\\left(\\delta(\\hat{x},\\hat{y})-2\\left(1-\\gamma-\\frac{\\gamma}{100}\\right)R L\\right)\\right]-w(x,y)+2m\\delta(\\hat{x},\\hat{y})\\ \\ }\\\\ &{}&{=K(a^{\\prime}-1)+4m\\left(1-\\gamma-\\frac{\\gamma}{100}\\right)R L-w(x,y)-\\varepsilon_{x,y},\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (1.25\\left(x+1.2\\right)\\ \\ \\ \\ \\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\varepsilon_{x,y}\\,\\in\\,[0,1)$ is the rounding error. Hence, property (a) holds: if $w(x,y)\\,<\\,w(x^{\\prime},y^{\\prime})$ then ${\\delta(F(x),\\mathring{F}(y))}^{\\cdot}>\\dot{\\delta}(F(x^{\\prime}),F(y^{\\prime}))$ , and vice versa, since the comparison is defined by ranking. The property (b) holds since for any $\\{x^{\\prime},y^{\\prime}\\}\\notin E$ and $\\{x,y\\}\\in E$ : ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\delta(F(x^{\\prime}),F(y^{\\prime}))\\geq\\delta(2m\\hat{x}^{\\prime},2m\\hat{y}^{\\prime})\\geq4m\\left(1-\\frac{\\gamma}{100}\\right)R L}}\\\\ {{\\qquad\\qquad\\qquad\\geq K(a^{\\prime}-1)+4m\\left(1-\\gamma-\\frac{\\gamma}{100}\\right)R L>\\delta(F(x),F(y)),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where the second inequality follows from Lemma 21, and the third inequality follows from $K(a^{\\prime}-$ 1) \u22644\u03b3mRL, which holds: since a\u2032 \u22121 \u2264r l1o0gc n\u03b3 , it suffices to have $\\begin{array}{r}{\\dot{K}\\leq\\frac{4}{10c}r\\log n}\\end{array}$ , which indeed holds for our choice of $c=\\operatorname*{max}(\\frac{r\\log n}{100K},\\frac{1}{100})$ . \u53e3 ", "page_idx": 8}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We perform experiments on CIFAR-10 and CIFAR-100 image datasets [KH09] (we show additional experiments in Appendix A). We define the ground-truth distance between points as the distance between their embedding vectors produced by a pretrained ResNet-18 neural network. Let $Q$ be contrastive triplets sampled uniformly at random from all possible triplets of images, labeled based on the ground-truth distance. Then, we train a different ResNet-18 model from scratch, where we control the embedding dimension by replacing the last fully-connected layer with a fully-connected layer with the chosen output dimension. We train the model for 50 epochs on a single NVIDIA A100 GPU using triplet loss [SKP15b]: $\\mathcal{L}_{F}(x,y^{+},z^{-})=\\|F(x)-F(y)\\|^{2}-\\|F(x)-F(z)\\|^{2}\\!+\\!1$ . Since our goal is to find an embedding of this set of queries, we evaluate the accuracy as the fraction of satisfied contrastive samples. ", "page_idx": 8}, {"type": "text", "text": "We present our results in Figure 4. In experiments, we vary the number of samples (Figure\u221as 4a and 4b) and the dimension (Figures $4c$ and 4d). Figures 4a and 4b show that, while $d\\stackrel{*}{{}\\geq}\\sqrt{m}$ , ", "page_idx": 8}, {"type": "image", "img_path": "H0qu4moFly/tmp/7b26ecc295e9ccf95ea667ff3f042515685619ca670c8d8ccd3486ca40895301.jpg", "img_caption": ["(a) CIFAR-10: the fraction of unsatisfied samples for various choices of the number of samples $m$ . The embedding dimension is $d=128$ . "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "H0qu4moFly/tmp/9a41c6eaab3a6718cf99352959b7c8daf4c27e86316c0c6daf667255a3209498.jpg", "img_caption": ["(c) CIFAR-10: the fraction of unsatisfied samples for various choices of the embedding dimension $d$ . The number of samples $m=10^{5}$ . "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "H0qu4moFly/tmp/dcaad4e4c09fbdb525fea4b133fcd32bd8344c8b716952e010e50597e85abb8d.jpg", "img_caption": ["(b) CIFAR-100: the fraction of unsatisfied samples for various choices of the number of samples $m$ . The embedding dimension is $d=128$ . "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "H0qu4moFly/tmp/af0aa15dd5af63851b90ada8f22befabd6d56af4a585d734becf0d91d15ad304.jpg", "img_caption": ["(d) CIFAR-100: the fraction of unsatisfied samples for various choices of the embedding dimension $d$ . The number of samples $m=10^{5}$ . "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Figure 4: Experiments on CIFAR-10 (left) and CIFAR-100 (right). The data points show the average over 5 runs, and the shaded area shows the minimum and the maximum values over the runs ", "page_idx": 9}, {"type": "text", "text": "the resulting embedding is consistent with \u221aalmost all $(\\geq\\,99\\%)$ ) triplets. On the other hand, for $m\\in\\{10^{5},\\bar{1}0^{6}\\}$ , $d$ is substantially less than $\\sqrt{m}$ , and the number of satisfied samples sharply drops from $99\\%$ to $93\\%$ . This is consistent with our theoretical results in Theorem 1. ", "page_idx": 9}, {"type": "text", "text": "Not surprisingly, Figures $4c$ and 4d show that, when the embedding dimension increases, so does the accuracy, i.e. the number\u221a of satisfied triplets. But the accuracy stops increasing when the dimension reaches approximately ${\\sqrt{m}}\\,\\approx\\,316\\;.$ \u2013 while there is a $2\\%$ accuracy increase when the dimension changes from 64 to 256, there is no accuracy increase when the dimension changes from 256 to 1024. This again conforms with our result from Theorem 1. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we provide bounds on the necessary and sufficient dimension to represent a collection of contrastive constraints of the form \u201cdistance from $x$ to $y$ is smaller than distance from $x$ to $z^{\\bullet}$ . This is a fundamental question in machine learning theory, since it educates the choice of deep learning architectures by providing guidance for the size of the embedding layer. Our experiments illustrate the predictive power of our theoretical findings in the context of deep learning. We also believe that it gives rise to many interesting directions for future work depending on the exact desiderata: approximate versions, different choices of normed spaces, bi-criteria algorithms, agnostic settings. ", "page_idx": 9}, {"type": "text", "text": "While the considered distance comparison settings play a central role in contrastive learning and nearest neighbor search, so far there has been no theoretical studies of their embedding dimension. Our work is the first to present a series of such upper and lower bounds in a variety of settings via a novel connection to the notion of arboricity from graph theory. As a follow-up, one can consider an improved embedding construction for $\\mathrm{k\\Omega}$ -NN: in the upped bound from Section 3, the dependence on both $\\log n$ and $k$ can likely can be improved. Another interesting direction is tighter data-dependent bounds on dimension: while we provide fine-grained bounds in terms of arboricity \u2013 which are potentially much stronger than bounds in terms of the number of edges \u2013 they don\u2019t necessary capture properties of dataset which can lead to sharper bounds. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We would like to thank Michael Barash for several very helpful suggestions. Work by Orr Fischer was partially supported by the Israel Science Foundation (grant No. 1042/22 and 800/22). Work by Vaggos Chatziafratis was partially supported by Hellman\u2019s fellowship and startup grant at UC Santa Cruz. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "$[\\mathrm{AAE^{+}}24]$ Noga Alon, Dmitrii Avdiukhin, Dor Elboim, Orr Fischer, and Grigory Yaroslavtsev. Optimal sample complexity of contrastive learning. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024.   \n$[\\mathrm{ABD^{+}08}]$ Noga Alon, Mihai Ba\u02d8doiu, Erik D Demaine, Martin Farach-Colton, MohammadTaghi Hajiaghayi, and Anastasios Sidiropoulos. Ordinal embeddings of minimum relaxation: general properties, trees, and ultrametrics. ACM Transactions on Algorithms (TALG), 4(4):1\u201321, 2008. [ADK22] Pranjal Awasthi, Nishanth Dikkala, and Pritish Kamath. Do more negative samples necessarily hurt in contrastive learning? In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesva\u00b4ri, Gang Niu, and Sivan Sabato, editors, International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pages 1101\u20131116. PMLR, 2022.   \n[AGKM22] Jordan T. Ash, Surbhi Goel, Akshay Krishnamurthy, and Dipendra Misra. Investigating the role of negatives in contrastive representation learning. In Gustau Camps-Valls, Francisco J. R. Ruiz, and Isabel Valera, editors, International Conference on Artificial Intelligence and Statistics, AISTATS 2022, 28-30 March 2022, Virtual Event, volume 151 of Proceedings of Machine Learning Research, pages 7187\u20137209. PMLR, 2022. [Ail12] Nir Ailon. An active learning algorithm for ranking from pairwise preferences with an almost optimal query complexity. Journal of Machine Learning Research, 13(1), 2012. [AMR92] Noga Alon, Colin McDiarmid, and Bruce A. Reed. Star arboricity. Comb., 12(4):375\u2013 380, 1992.   \n$[\\mathbf{B}\\mathbf{D}\\mathbf{H}^{+}08]$ Mihai Ba\u02d8doiu, Erik D Demaine, MohammadTaghi Hajiaghayi, Anastasios Sidiropoulos, and Morteza Zadimoghaddam. Ordinal embedding: Approximation algorithms and dimensionality reduction. In International Workshop on Approximation Algorithms for Combinatorial Optimization, pages 21\u201334. Springer, 2008. [BE13] Leonid Barenboim and Michael Elkin. Distributed Graph Coloring: Fundamentals and Recent Developments. Synthesis Lectures on Distributed Computing Theory. Springer International Publishing, Cham, 2013. [BHB19] Philip Bachman, R. Devon Hjelm, and William Buchwalter. Learning representations by maximizing mutual information across views. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d\u2019Alche\u00b4-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 15509\u201315519, 2019. [BL05] Yonatan Bilu and Nati Linial. Monotone maps, sphericity and bounded second eigenvalue. Journal of Combinatorial Theory, Series B, 95(2):283\u2013299, 2005. [CH67] T. Cover and P. Hart. Nearest neighbor pattern classification. IEEE Transactions on Information Theory, 13(1):21\u201327, 1967. [CH21] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021, pages 15750\u201315758. Computer Vision Foundation / IEEE, 2021.   \n$[\\mathrm{CHX^{+}19}]$ Fatih Cakir, Kun He, Xide Xia, Brian Kulis, and Stan Sclaroff. Deep metric learning to rank. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1861\u20131870, 2019. [CI24] Vaggos Chatziafratis and Piotr Indyk. Dimension-Accuracy Tradeoffs in Contrastive Embeddings for Triplets, Terminals & Top-k Nearest Neighbors. In 2024 Symposium on Simplicity in Algorithms (SOSA), Proceedings, pages 230\u2013243. Society for Industrial and Applied Mathematics, January 2024.   \n[CKNH20] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework for contrastive learning of visual representations. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 1597\u20131607. PMLR, 2020. [CLL21] Ting Chen, Calvin Luo, and Lala Li. Intriguing properties of contrastive losses. In Marc\u2019Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 11834\u201311845, 2021.   \n$[\\mathrm{CRL}^{+}20]$ Ching-Yao Chuang, Joshua Robinson, Yen-Chen Lin, Antonio Torralba, and Stefanie Jegelka. Debiased contrastive learning. In Hugo Larochelle, Marc\u2019Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. [CS74] James P Cunningham and Roger N Shepard. Monotone mapping of similarities into a general metric space. Journal of Mathematical Psychology, 11(4):335\u2013363, 1974. [DHS91] Alice Dean, Joan Hutchinson, and Edward Scheinerman. On the thickness and arboricity of a graph. Journal of Combinatorial Theory, Series B, 52(1):147\u2013151, 1991.   \n[DKS12] Anirban Dasgupta, Ravi Kumar, and D. Sivakumar. Sparse and lopsided set disjointness via information theory. In Anupam Gupta, Klaus Jansen, Jose\u00b4 D. P. Rolim, and Rocco A. Servedio, editors, Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques - 15th International Workshop, APPROX 2012, and 16th International Workshop, RANDOM 2012, Cambridge, MA, USA, August 15- 17, 2012. Proceedings, volume 7408 of Lecture Notes in Computer Science, pages 517\u2013528. Springer, 2012. [DL97] Michel Marie Deza and Monique Laurent. Geometry of cuts and metrics, volume 15 of Algorithms and combinatorics. Springer, 1997.   \n[DSRB14] Alexey Dosovitskiy, Jost Tobias Springenberg, Martin A. Riedmiller, and Thomas Brox. Discriminative unsupervised feature learning with convolutional neural networks. In Zoubin Ghahramani, Max Welling, Corinna Cortes, Neil D. Lawrence, and Kilian Q. Weinberger, editors, Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, pages 766\u2013774, 2014.   \n$[\\mathrm{FIM}^{+}20]$ Bohan Fan, Diego Ihara, Neshat Mohammadi, Francesco Sgherzi, Anastasios Sidiropoulos, and Mina Valizadeh. Learning lines with ordinal constraints. In Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques (APPROX/RANDOM 2020). Schloss Dagstuhl-Leibniz-Zentrum fu\u00a8r Informatik, 2020. [Fre\u00b410] M. Fre\u00b4chet. Les dimensions d\u2019un ensemble abstrait. Mathematische Annalen, 68:145\u2013 168, 1910.   \n[GCY19] Nikhil Ghosh, Yuxin Chen, and Yisong Yue. Landmark ordinal embedding. Advances in Neural Information Processing Systems, 32, 2019. [GH10] Michael Gutmann and Aapo Hyva\u00a8rinen. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In Yee Whye Teh and D. Mike Titterington, editors, Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, AISTATS 2010, Chia Laguna Resort, Sardinia, Italy, May 13-15, 2010, volume 9 of JMLR Proceedings, pages 297\u2013304. JMLR.org, 2010. [Goe06] Michel Goemans. Topics in tcs: Embeddings of finite metric spaces, lecture 1, 2006. [GPvL19] Debarghya Ghoshdastidar, Michae\u00a8l Perrot, and Ulrike von Luxburg. Foundations of comparison-based hierarchical clustering. Advances in neural information processing systems, 32, 2019. [GYC21] Tianyu Gao, Xingcheng Yao, and Danqi Chen. Simcse: Simple contrastive learning of sentence embeddings. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pages 6894\u20136910. Association for Computational Linguistics, 2021. $[\\mathrm{HFL^{+}19}]$ R. Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Philip Bachman, Adam Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation and maximization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.   \n$[\\mathrm{HFW}^{+}20]$ Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross B. Girshick. Momentum contrast for unsupervised visual representation learning. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 9726\u20139735. Computer Vision Foundation / IEEE, 2020. [HM23] Jeff Z. HaoChen and Tengyu Ma. A theoretical study of inductive biases in contrastive learning. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023.   \n[HWGM21] Jeff Z. HaoChen, Colin Wei, Adrien Gaidon, and Tengyu Ma. Provable guarantees for self-supervised deep learning with spectral contrastive loss. In Marc\u2019Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 5000\u20135011, 2021. [IMS17] Piotr Indyk, Jivr\u00b4\u0131 Matouvsek, and Anastasios Sidiropoulos. 8: low-distortion embeddings of finite metric spaces. In Handbook of discrete and computational geometry, pages 211\u2013231. Chapman and Hall/CRC, 2017. [JN11a] Kevin G Jamieson and Robert Nowak. Active ranking using pairwise comparisons. Advances in neural information processing systems, 24, 2011. [JN11b] Kevin G Jamieson and Robert D Nowak. Low-dimensional embedding using adaptively selected ordinal data. In 2011 49th Annual Allerton Conference on Communication, Control, and Computing (Allerton), pages 1077\u20131084. IEEE, 2011. [KH09] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical Report, University of Toronto, 2009. [KL14] Mattha\u00a8us Kleindessner and Ulrike Luxburg. Uniqueness of ordinal embedding. In Conference on Learning Theory, pages 40\u201367. PMLR, 2014. [Kru64a] Joseph B Kruskal. Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis. Psychometrika, 29(1):1\u201327, 1964. [Kru64b] Joseph B Kruskal. Nonmetric multidimensional scaling: a numerical method. Psychometrika, 29(2):115\u2013129, 1964.   \n[KVH16] Ramya Korlakai Vinayak and Babak Hassibi. Crowdsourced clustering: Querying edges vs triangles. Advances in Neural Information Processing Systems, 29, 2016. [LL18] Lajanugen Logeswaran and Honglak Lee. An efficient framework for learning sentence representations. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. [Mat13] Jir\u0131 Matouvsek. Lecture notes on metric embeddings. Technical report, Technical report, ETH Zu\u00a8rich, 2013.   \n[MCCD13] Toma\u00b4s Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. In Yoshua Bengio and Yann LeCun, editors, 1st International Conference on Learning Representations, ICLR 2013, Scottsdale, Arizona, USA, May 2-4, 2013, Workshop Track Proceedings, 2013. [ML09] Brian McFee and Gert Lanckriet. Partial order embedding with multiple kernels. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 721\u2013728, 2009.   \n[MMW+21] Jovana Mitrovic, Brian McWilliams, Jacob C. Walker, Lars Holger Buesing, and Charles Blundell. Representation learning via invariant causal mechanisms. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. [NW61] C. St.J. A. Nash-Williams. Edge-disjoint spanning trees of finite graphs. Journal of the London Mathematical Society, s1-36(1):445\u2013450, 1961. [NW64] C. St.J. A. Nash-Williams. Decomposition of finite graphs into forests. Journal of the London Mathematical Society, s1-39(1):12\u201312, 1964. [OG08] Hua Ouyang and Alex Gray. Learning dissimilarities by ranking: from sdp to qp. In Proceedings of the 25th international conference on Machine learning, pages 728\u2013 735, 2008. [Opa79] Jaroslav Opatrny. Total ordering problem. SIAM Journal on Computing, 8(1):111\u2013 114, 1979.   \n$[\\mathbf{S}\\mathbf{A}\\mathbf{G}^{+}22]$ Nikunj Saunshi, Jordan T. Ash, Surbhi Goel, Dipendra Misra, Cyril Zhang, Sanjeev Arora, Sham M. Kakade, and Akshay Krishnamurthy. Understanding contrastive learning requires incorporating inductive biases. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesva\u00b4ri, Gang Niu, and Sivan Sabato, editors, International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pages 19250\u201319286. PMLR, 2022. [SE05] Noah A. Smith and Jason Eisner. Contrastive estimation: Training log-linear models on unlabeled data. In Kevin Knight, Hwee Tou $\\mathrm{Ng}$ , and Kemal Oflazer, editors, ACL 2005, 43rd Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference, 25-30 June 2005, University of Michigan, USA, pages 354\u2013362. The Association for Computer Linguistics, 2005. [She62] Roger N Shepard. The analysis of proximities: multidimensional scaling with an unknown distance function. i. Psychometrika, 27(2):125\u2013140, 1962. [She74] Roger N Shepard. Representation of structure in similarity data: Problems and prospects. Psychometrika, 39(4):373\u2013421, 1974.   \n[SKP15a] Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A unified embedding for face recognition and clustering. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA, USA, June 7-12, 2015, pages 815\u2013823. IEEE Computer Society, 2015. [SKP15b] Florian Schroff, Dmitry Kalenichenko, and James Philbin. FaceNet: A unified embedding for face recognition and clustering. In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 815\u2013823, June 2015. $[\\mathrm{SPA^{+}19}]$ Nikunj Saunshi, Orestis Plevrakis, Sanjeev Arora, Mikhail Khodak, and Hrishikesh Khandeparkar. A theoretical analysis of contrastive unsupervised representation learning. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pages 5628\u20135637. PMLR, 2019.   \n$[\\mathrm{TDR}^{+}20]$ Michael Tschannen, Josip Djolonga, Paul K. Rubenstein, Sylvain Gelly, and Mario Lucic. On mutual information maximization for representation learning. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. [TJ06] MTCAJ Thomas and A Thomas Joy. Elements of information theory. WileyInterscience, 2006. [TKH21] Christopher Tosh, Akshay Krishnamurthy, and Daniel Hsu. Contrastive learning, multi-view redundancy, and linear models. In Vitaly Feldman, Katrina Ligett, and Sivan Sabato, editors, Algorithmic Learning Theory, 16-19 March 2021, Virtual Conference, Worldwide, volume 132 of Proceedings of Machine Learning Research, pages 1179\u20131206. PMLR, 2021. [TKI20] Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm, editors, Computer Vision - ECCV 2020 - 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XI, volume 12356 of Lecture Notes in Computer Science, pages 776\u2013794. Springer, 2020. [TL14] Yoshikazu Terada and Ulrike Luxburg. Local ordinal embedding. In International Conference on Machine Learning, pages 847\u2013855. PMLR, 2014.   \n$[\\mathrm{TLB^{+}}11]$ Omer Tamuz, Ce Liu, Serge Belongie, Ohad Shamir, and Adam Tauman Kalai. Adaptively learning the crowd kernel. In Proceedings of the 28th International Conference on International Conference on Machine Learning, pages 673\u2013680, 2011. [Tor52] Warren S Torgerson. Multidimensional scaling: I. theory and method. Psychometrika, 17(4):401\u2013419, 1952.   \n[TWSM21] Yao-Hung Hubert Tsai, Yue Wu, Ruslan Salakhutdinov, and Louis-Philippe Morency. Self-supervised learning from a multi-view perspective. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. [VD16] Sharad Vikram and Sanjoy Dasgupta. Interactive bayesian hierarchical clustering. In International Conference on Machine Learning, pages 2081\u20132090. PMLR, 2016.   \n[vdOLV18] Aa\u00a8ron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. CoRR, abs/1807.03748, 2018.   \n$[\\mathrm{vKSG^{+}}21]$ Julius von K\u00a8ugelgen, Yash Sharma, Luigi Gresele, Wieland Brendel, Bernhard Scho\u00a8lkopf, Michel Besserve, and Francesco Locatello. Self-supervised learning with data augmentations provably isolates content from style. In Marc\u2019Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 16451\u201316467, 2021. [War68] Hugh E. Warren. Lower bounds for approximation by nonlinear manifolds. Transactions of the American Mathematical Society, 133(1):167\u2013178, 1968. [WG15] Xiaolong Wang and Abhinav Gupta. Unsupervised learning of visual representations using videos. In 2015 IEEE International Conference on Computer Vision, ICCV 2015, Santiago, Chile, December 7-13, 2015, pages 2794\u20132802. IEEE Computer Society, 2015. [WI20] Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 9929\u20139939. PMLR, 2020.   \n[WJJ13] Fabian Wauthier, Michael Jordan, and Nebojsa Jojic. Efficient ranking from pairwise comparisons. In International Conference on Machine Learning, pages 109\u2013117. PMLR, 2013. [WL21] Zixin Wen and Yuanzhi Li. Toward understanding the feature learning process of selfsupervised contrastive learning. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 11112\u201311122. PMLR, 2021.   \nWXYL18] Zhirong Wu, Yuanjun Xiong, Stella X. Yu, and Dahua Lin. Unsupervised feature learning via non-parametric instance discrimination. In 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pages 3733\u20133742. Computer Vision Foundation / IEEE Computer Society, 2018.   \n$[Z\\mathrm{SS}^{+}21]$ Roland S. Zimmermann, Yash Sharma, Steffen Schneider, Matthias Bethge, and Wieland Brendel. Contrastive learning inverts the data generating process. CoRR, abs/2102.08850, 2021. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "table", "img_path": "H0qu4moFly/tmp/63a56b8d5ecd1781eafa4488f7cbcf3db7b7aafedd6ace5f9a114131e2a102fd.jpg", "table_caption": ["A Additional experiments "], "table_footnote": ["Table 3: Embedding dimension based on construction from Section 2. For each pair of $n$ and $m$ , we show the minimum and the maximum dimensions obtained over 10 runs (we show a single number when the minimum and the maximum are equal). "], "page_idx": 16}, {"type": "image", "img_path": "H0qu4moFly/tmp/c76d7a9110d833f22bb6cbd7082eb319581b274e9265fc6bbf4845dd230fe400.jpg", "img_caption": ["Figure 5: CIFAR-100: the fraction of unsatisfied samples for various choices of the number of samples $m$ . The embedding dimension is $d=128$ . "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "In this section, we present additional experiments. ", "page_idx": 16}, {"type": "text", "text": "Contrastive Samples In Table 3, for various values of $n$ and $m$ , we show the dimensions of the embeddings constructed according to Section 2. We sample $m$ random triplets from the CIFAR100 dataset, and label the triplets based on a ground-truth embedding generated using a pretrained ResNet18 network. Note that the embedding dimension is always at most $2n$ , which corresponds to the case when the constraint graph is a clique. Moreover, in agreement with our theory, when $m<n^{2}$ , increasing $n$ decreases the required dimension: the constraint graph becomes more sparse, which decreases the arboricity. ", "page_idx": 16}, {"type": "text", "text": "In Figure 5, similarly to Figure 4b, we show training accuracy on CIFAR-100 dataset for various values of $m$ . In this figure, we focus on the setting when $m$ is close to $d^{2}=16384$ . While for $m\\leq$ $d^{2}/2$ the accuracy is close to perfect $(99\\%)$ , the accuracy decreases starting from this point. This supports our theoretical result that $d=\\Theta({\\sqrt{m}})$ dimensions are required to preserve the contrastive samples. ", "page_idx": 16}, {"type": "text", "text": "$k$ -NN In Table 4, we present results for $k$ -NN settings for $d=128$ and for various choices of $n$ and $k$ . We sample $n$ points from the CIFAR-10 dataset, and generate $k$ -NN based on a ground-truth embedding generated using a pretrained ResNet18 network. For each element $x$ , let $\\pi_{1}^{*}(x),...,\\pi_{n-1}^{*}(x)$ be the ordering of other elements according to the ground-truth embedding. For each and $i\\in[k]$ and each $j>i$ , we generate contrastive samples $(x,\\bar{\\pi_{i}^{*}}(x)^{+},\\pi_{j}^{*}(x)^{-})$ , and we train the neural network on this set of samples similarly to Section 4. ", "page_idx": 16}, {"type": "text", "text": "For each $n$ and $k$ , we report the loss function measuring the quality of preserving the $k$ -NNs, defined as follows. For each vertex $x$ and each $i\\in[k]$ , we compute the change of rank of the $\\ddot{\\iota}$ th nearest neighbor of $x$ in the new embeddings. Formally, we find $j$ such that the $i$ \u2019th nearest neighbor of $x$ according to the ground-truth embedding is the $j^{!}$ \u2019th nearest neighbor according to the trained embedding, contributing $|i-j|$ to the loss. Finally, we define the final loss as the average loss over all $x\\in V$ and $i\\in[k]$ . ", "page_idx": 16}, {"type": "table", "img_path": "H0qu4moFly/tmp/0f79601bd6993adafed187fc7a53017b21517b232e81f4597f2cc8f40c259575.jpg", "table_caption": [], "table_footnote": ["Table 4: Training loss for preserving $\\overline{{k}}$ -NNs for various values of $n$ and $\\overline{{k}}$ . For each pair of $n$ and $k$ , we show the minimum and the maximum dimensions obtained over 10 runs (we show a single number when the minimum and the maximum are equal). "], "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "Table 4 shows that the loss increases with both $k$ and $n$ . However, dependence on $n$ is much lower than dependence on $k$ , supporting our theoretical result which shows polynomial dependence on $k$ and only polylogarithmic dependence on $n$ . ", "page_idx": 17}, {"type": "text", "text": "B Preliminaries ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Definition 22 (Metric, semimetric). $A n$ metric space is an ordered pair $(X,\\delta_{X})$ consisting of a set $X$ and a map $\\delta_{X}\\colon X\\times X\\to[0,\\infty]$ such that $\\delta_{X}$ satisfies: ", "page_idx": 17}, {"type": "text", "text": "1. $\\delta_{X}(x,y)=0\\iff x=y;$ ;   \n2. $\\delta_{X}(x,y)=\\delta_{X}(y,x),$ , for all $x,y\\in X$ ;   \n3. $\\delta_{X}(x,y)+\\delta_{X}(y,z)\\leq\\delta_{X}(x,z),$ , for all $x,y,z\\in X$ ", "page_idx": 17}, {"type": "text", "text": "If $\\delta_{X}$ satisfies the last two properties but only $\\delta_{X}(x,x)=0$ for all $x\\in X$ instead of the first one then it is called a semimetric. ", "page_idx": 17}, {"type": "text", "text": "We note that the triangle inequality doesn\u2019t affect our results. Intuitively, our goal is to preserve the ranking of distances, and adding a sufficiently large constant to distances preserves the ranking while also satisfying the triangle inequality. ", "page_idx": 17}, {"type": "text", "text": "Definition 23 ( $\\ell_{p}$ norm, $\\ell_{p}^{p}$ distance function, $\\ell_{\\infty}$ norm). Given vectors $v,v^{\\prime}\\in\\mathbb R^{d}$ and $p\\geq1,$ , the distance between $v$ and $v^{\\prime}$ under the $\\ell_{p}$ norm is ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\delta_{\\ell_{p}}(v,v^{\\prime})=\\left(\\sum_{i=1}^{d}\\lvert v[i]-v^{\\prime}[i]\\rvert^{p}\\right)^{1/p},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where v[i] is the i\u2019th coordinate of vector $v$ . The distance between $v$ and $v^{\\prime}$ under $\\ell_{p}^{p}$ is ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\delta_{\\ell_{p}^{p}}(v,v^{\\prime})=\\sum_{i=1}^{d}\\lvert v[i]-v^{\\prime}[i]\\rvert^{p}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The distance between v and $v^{\\prime}$ under the $\\ell_{\\infty}$ norm is ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\delta_{\\ell_{\\infty}}(v,v^{\\prime})=\\operatorname*{max}_{i\\in[d]}\\lvert v[i]-v^{\\prime}[i]\\rvert.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "For $p\\geq1$ or $p=\\infty$ , the norm of $v$ is defined as $\\|v\\|_{p}=\\delta_{\\ell_{p}}(v,v)$ . ", "page_idx": 17}, {"type": "text", "text": "Fact 24 (Chernoff bound). Let $X_{1},\\ldots,X_{r}\\,\\in\\,\\{0,1\\}$ be mutually independent random variables. Denote by $\\begin{array}{r}{\\mu=\\mathbb{E}\\left[\\sum_{i=1}^{r}X_{i}\\right]}\\end{array}$ , the expectation of the sum of variables. Then for any $0<\\gamma<1$ it holds that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[|\\sum_{i=1}^{r}X_{i}-\\mu|\\geq\\gamma\\mu\\right]\\leq2\\exp\\left(-\\frac{\\gamma^{2}\\mu}{3}\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Additionally, for any $\\gamma>0$ it holds that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\sum_{i=1}^{r}X_{i}\\geq(1+\\gamma)\\mu\\right]\\leq\\exp\\left(-\\frac{\\gamma^{2}\\mu}{2+\\gamma}\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "B.1 Ordinal Embeddings ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Fact 25 ([BL05]). Given a set of non-contradictory inequalities among pairwise distances on $V$ , there exists a metric $\\delta\\colon V\\times V\\rightarrow\\mathbb{R}_{\\geq0}$ which satisfies all the inequalities. ", "page_idx": 18}, {"type": "text", "text": "Proof. Consider a graph whose vertices are $V\\times V$ and create a directed edge between two vertices if they participate in some inequality. Since the inequalities are non-contradictory, there are no cycles in this graph. Consider any topological ordering of this graph and define $w_{i j}$ to be the index of each pair in the topological ordering. Let $\\delta=W\\!+\\!w_{i j}$ where $W=|V|^{2}$ . Note that $\\delta$ satisfies the triangle inequality. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "B.2 Arboricity ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this subsection, we present basic facts about arboricity, and analyze the arboricity of the constraint graph in our various settings. ", "page_idx": 18}, {"type": "text", "text": "For a directed graph, we say that the out-degree of a vertex $x$ is $R$ , for some integer $R$ , if $x$ has $R$ incident edges oriented towards $x$ . ", "page_idx": 18}, {"type": "text", "text": "Fact 26 ([AMR92], Lemma 2.2). If the edges of $G$ can be oriented such that each vertex has indegree at most $R$ for some integer $R$ , then $r\\leq R+1$ . ", "page_idx": 18}, {"type": "text", "text": "Lemma 27. The constraint graph in the $\\mathrm{k\\Omega}$ -NN setting has arboricity at most $k+1$ . ", "page_idx": 18}, {"type": "text", "text": "Proof. In the constraint graph of a $\\mathrm{k\\Omega}$ -NN instance, we have an edge for each pair $(x,\\pi_{i}(x))$ for $1\\leq i\\leq k,x\\in V$ , where $\\pi_{i}$ is the $\\ddot{\\iota}$ \u2019th nearest neighbor of $x$ . If for each such pair, we orient the edge inwards to $x$ , we obtain a directed graph with in-degree at most $k$ . Therefore, by Fact 26, the constraint graph $G$ has arboricity at most $k+1$ . \u53e3 ", "page_idx": 18}, {"type": "text", "text": "Finally, the following fact relates the number of edges $m$ and the arboricity of the graph. ", "page_idx": 18}, {"type": "text", "text": "Lemma 28 ([DHS91] Theorem 2). Any graph $G$ with m edges has arboricity $r\\leq\\lceil\\sqrt{m/2}\\rceil$ . ", "page_idx": 18}, {"type": "text", "text": "C Missing Proofs From Sections 2 and 3 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proof of Lemma $_{l l}$ . Since $|N^{-}(x)|\\,\\leq\\,h$ for all $x$ , it suffices to show that with probability 1, any subset $A\\subseteq\\{\\hat{x}_{1},\\ldots,\\hat{x}_{n}\\}$ of size $|A|\\leq h$ is linearly independent. We prove it by induction on $|A|$ , and the base case $|A|=0$ trivially holds. ", "page_idx": 18}, {"type": "text", "text": "For the induction step, let $\\hat{x}$ be the last point in $A$ . By the induction hypothesis, $A\\setminus\\{\\hat{x}\\}$ are linearly independent. Let $H\\stackrel{-}{=}\\mathrm{Span}(A\\setminus\\{\\hat{x}\\})$ , and let $B$ the ball where $\\hat{x}$ is sampled from. Since $\\operatorname{Vol}(H)=$ 0, and $\\mathrm{Vol}(B)>0$ , we have $\\mathbb{P}\\left[\\hat{x}\\in H\\right]=0$ , meaning that $A$ are linearly independent. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "C.1 Proof of Lemma 21 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section, $\\delta(x,y)=\\Vert x-y\\Vert_{1}$ . For other $\\ell_{p}$ for $p\\in[1,+\\infty)$ , the construction is the same by replacing each coordinate value $c$ with $c^{1/p}$ . ", "page_idx": 18}, {"type": "text", "text": "Agreement sets Before proving Lemma 21, we define the following sets: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Agr}(x,y)=\\{j\\in[L]\\mid\\hat{x}^{(j)}=\\hat{y}^{(j)}\\},}\\\\ &{\\mathrm{Agr}_{\\mathrm{D}}(x,y)=\\{j\\mid\\hat{x}^{(j)}=\\hat{y}^{(j)}=I_{x}\\mathrm{~or~}\\hat{x}^{(j)}=\\hat{y}^{(j)}=I_{y}\\},}\\\\ &{\\mathrm{Agr}_{\\mathrm{N}}(x,y)=\\{j\\mid\\exists z\\in N^{-}(x)\\cap N^{-}(y)\\mathrm{~such~that~}\\hat{x}^{(j)}=\\hat{y}^{(j)}=I_{z}\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The idea is to measure on which indices the points agree and to differentiate sources of agreements. ", "page_idx": 18}, {"type": "text", "text": "\u2022 ${\\mathrm{Agr}}(x,y)$ is the set of indices on which $x$ and $y$ agree, i.e. choose the same vector. \u2022 ${\\mathrm{Agr}}_{\\mathrm{D}}(x,y)$ is the set of indices where $x$ and $y$ choose the same vector by a direct connection: $x$ chooses its own set $I_{x}$ and $y$ chooses $x$ \u2019s set $I_{x}$ (or reverse). ", "page_idx": 18}, {"type": "text", "text": "\u2022 $\\mathrm{Agr_{N}}(x,y)$ is the set of indices where $x$ and $y$ choose the same vector by an indirect connection: $x$ and $y$ share a common neighbor $z$ , and both choose $z$ \u2019s set $I_{z}$ . ", "page_idx": 19}, {"type": "text", "text": "Note that $\\mathrm{Agr\\mathrm{}=\\mathrm{Agr_{D}\\cup A g r_{N}}}$ . When $x$ and $y$ are neighbors and $x^{\\prime}$ and $y$ are not, we will show that $|\\mathrm{Agr}(x,\\bar{y})|\\approx|\\bar{\\mathrm{Agr}}_{\\mathrm{D}}(x,\\bar{y})|\\gg|\\mathrm{Agr}(x^{\\prime},y^{\\prime})|$ . ", "page_idx": 19}, {"type": "text", "text": "Choice of parameters We next remind the choice of parameters. ", "page_idx": 19}, {"type": "text", "text": "Graph arboricity $\\begin{array}{r l}&{r\\leq2k}\\\\ &{R=\\Theta(r^{3}\\log^{3}n)}\\\\ &{p=O(1/(r\\log n))}\\\\ &{\\gamma=\\frac{p(1-p)}{2r}=\\Theta(\\frac{1}{r^{2}\\log n})}\\\\ &{\\alpha=\\Theta(\\frac{1}{r^{3}\\log^{2}n})}\\\\ &{L=\\Theta(r^{4}\\log^{4}n)}\\end{array}$ The size of the design   \nProbability that an element doesn\u2019t choose its own design vector   \nProbability that neighbors choose the same design vector   \nFraction of intersecting elements between sets in the design The number of blocks corresponding to designs ", "page_idx": 19}, {"type": "text", "text": "We next justify the choice of the parameters. ", "page_idx": 19}, {"type": "text", "text": "\u2022 In the proof of Theorem 5, to counter the $K=O(r\\log n)$ term from Section 3.1, for $\\{x,y\\}\\in E$ we need to bound the spread of $|\\mathrm{Agr}|$ for neighbors as ${\\frac{|\\operatorname{Agr}|}{r\\log n}}$ . To achieve that, we need to bound the spread of both $|\\mathrm{Agr_{N}}|$ and $|\\mathrm{Agr_{D}}|$ . ", "page_idx": 19}, {"type": "text", "text": "\u2022 First, we need to guarantee that $\\begin{array}{r}{\\left|\\mathrm{Agr}_{\\mathrm{N}}\\right|\\ =\\ O\\left(\\frac{|\\mathrm{Agr}_{\\mathrm{D}}|}{r\\log n}\\right)}\\end{array}$ . In Proposition 30, we require that $\\begin{array}{r}{2r\\left(\\frac{p}{2r}\\right)^{2}\\leq\\frac{c\\gamma}{4r\\log n}}\\end{array}$ (. wThhiicsh i sc osiunnctse $\\textstyle2r\\left({\\frac{p}{2r}}\\right)^{2}$ )d, s wthhiel ep ibsa tbhilei tpyr tohbaat btilwitoy  ptohiantt sewliellc tc thhoeo ssae $\\mathrm{Agr_{N}}.$ $\\gamma$ $x$ $I_{y}$ for $y\\in N^{-}(x)$ (which counts towards $\\mathrm{Agr_{D}}$ ). Since $\\begin{array}{r}{\\gamma=\\frac{p(1-p)}{2r}}\\end{array}$ , this bounds $p=O(1/(r\\log n))$ and $\\gamma=O(1/(r^{2}\\log n))$ . ", "page_idx": 19}, {"type": "text", "text": "\u2022 To bound the spread of $|\\mathrm{Agr_{D}}|$ , note that $\\mathbb{E}\\left[|\\mathrm{Agr}_{\\mathrm{D}}|=\\gamma L\\right]$ . To bound the spread as $\\left.{\\frac{|\\mathbf{Agr}_{\\mathrm{D}}|}{r\\log n}}\\right.$ , by Chernoff we must have $\\gamma L=r^{2}\\log^{3}n$ , meaning $L=r^{4}\\log^{4}n$ . ", "page_idx": 19}, {"type": "text", "text": "\u2022 Different sets from the design (Definition 19) intersect by at most $\\alpha R$ elements. When two points sample different sets, the distance between their embeddings increases by the number of elements outside of their intersection, which is at least $2R-\\alpha R$ . Note that the distance between neighbors will be approximately ", "page_idx": 19}, {"type": "equation", "text": "$$\n(2R-\\alpha R)(L-|\\mathrm{Agr}_{\\mathrm{D}}|)\\approx2R L(1-\\frac{\\alpha}{2}-\\gamma(2-\\alpha))\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Similarly to the above, we want to bound the deviation due to the $\\alpha/2$ term, and by the same logic we choose $\\alpha=O(\\gamma/(r\\log n))=O(1/(r^{3}\\log^{2}n))$ . ", "page_idx": 19}, {"type": "text", "text": "\u2022 We need to choose $2n$ sets from the design. Since by Lemma 20 the design has $2^{\\alpha R\\log R}$ sets, to guarantee that this value is at least $2n$ , we take $\\alpha$ and $R$ so that $\\alpha R\\,=\\,\\Omega(\\log n)$ , meaning $R=\\Theta(r^{3}\\log^{3}{n})$ . ", "page_idx": 19}, {"type": "text", "text": "Proofs The next statement shows concentration of $\\mathrm{Agr_{D}}$ for neighbors and non-neighbors. ", "page_idx": 19}, {"type": "text", "text": "Proposition 29. For any $x,y\\in V$ , $i f\\left\\{x,y\\right\\}\\,\\notin\\,E$ then $|\\mathrm{Agr}_{\\mathrm{D}}(x,y)|=0,$ , and if $\\{x,y\\}\\in E$ , then|AgrD(x, y)| \u2212\u03b3L  \u22644rc l\u03b3oLg n .", "page_idx": 19}, {"type": "text", "text": "Proof of Proposition 29. If $\\{x,y\\}\\not\\in E$ , then $x\\not\\in N^{-}(y)$ and $y\\notin N^{-}(x)$ , i.e. for every $j\\,\\in\\,[L]$ , we have ${\\hat{x}}^{(j)}\\neq I_{y}$ and ${\\hat{y}}^{(j)}\\neq I_{x}$ and hence $|\\mathrm{Agr}_{\\mathrm{D}}(x,y)|=0$ . ", "page_idx": 19}, {"type": "text", "text": "If $\\{x,y\\}\\in E$ , assume w.l.o.g. that $y\\,\\in\\,N^{-}(x)$ . Therefore, $j\\,\\in\\,\\mathrm{Agr}_{\\mathrm{D}}(x,y)$ if and only if we set ${\\hat{y}}^{(j)}=I_{y}$ and ${\\hat{x}}^{(j)}=I_{y}$ . Recall that $\\mathbb{P}\\left[\\hat{y}^{(j)}=I_{y}\\right]=1-p$ and $\\mathbb{P}\\left[\\hat{x}^{(j)}=I_{y}\\right]=p/(2r)$ . ", "page_idx": 19}, {"type": "text", "text": "Therefore, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[j\\in\\mathrm{Agr}_{\\mathrm{D}}(x,y)\\right]=\\frac{p(1-p)}{2r}=\\gamma,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "i.e. E $\\langle\\vert\\mathrm{Agr_{D}}(x,y)\\vert]=\\gamma L$ . Since $\\gamma L=\\Omega(r^{2}\\log^{3}n)$ , by Chernoff, w.h.p. we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left||\\mathrm{Agr}_{\\mathrm{D}}(x,y)|-\\gamma L\\right|\\leq\\frac{c\\gamma L}{4r\\log n}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We next show concentration for $\\operatorname{Agr}_{\\operatorname{N}}$ . Note that $\\mathrm{Agr_{D}}$ for neighbors is much larger than $\\operatorname{Agr}_{\\operatorname{N}}$ for both neighbors and non-neighbors. ", "page_idx": 20}, {"type": "text", "text": "Proposition 30. For any $x,y\\in V$ , we have $\\begin{array}{r}{0\\leq|\\mathrm{Agr}_{\\mathrm{N}}(x,y)|\\leq\\frac{c\\gamma L}{4r\\log n}\\ w.h.p.}\\end{array}$ ", "page_idx": 20}, {"type": "text", "text": "Proof. We bound the expectation of $\\vert\\mathrm{Agr}_{\\mathrm{N}}(x,y)\\vert$ . Recall that $j\\in\\mathrm{Agr}_{\\mathrm{N}}(x,y)$ if and only if there exists a point $z\\,\\in\\,N^{-}(x)\\cap N^{-}(y)$ such that $\\hat{x}^{(j)}\\,=\\,I_{z}$ and $\\hat{y}^{(j)}\\,=\\,I_{z}$ . Moreover, the events $\\hat{x}^{(j)}=I_{z}$ and $\\hat{y}^{(j)}=I_{z}$ are independent, each occurring with probability $p/2r$ . ", "page_idx": 20}, {"type": "text", "text": "Since $|N^{-}(x)\\cap N^{-}(y)|\\leq|N^{-}(x)|\\leq2r$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[|\\mathrm{Agr}_{\\mathrm{N}}(x,y)|\\right]\\leq|N^{-}(x)|\\cdot L\\left(\\frac{p}{2r}\\right)^{2}=\\frac{L p^{2}}{2r}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Finally, we note that ${\\frac{L p^{2}}{2r}}=\\Omega(\\log n)$ , and by Chernoff, w.h.p.: ", "page_idx": 20}, {"type": "equation", "text": "$$\n|\\operatorname{Agr}(x,y)|\\leq{\\frac{4}{3}}\\mathbb{E}\\left[|\\operatorname{Agr}_{\\operatorname{N}}(x,y)|\\right]={\\frac{4}{3}}\\cdot{\\frac{L p^{2}}{2r}}\\leq{\\frac{c\\gamma L}{4r\\log n}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof of Lemma $2l$ . Recall that $\\begin{array}{r}{\\delta(\\hat{x},\\hat{y})\\,=\\,\\sum_{j=1}^{L}\\delta(\\hat{x}^{(j)},\\hat{y}^{(j)})}\\end{array}$ . For each $j\\,\\in\\,\\mathrm{Agr}(x,y)$ , we have $\\delta(\\hat{x}^{(j)},\\hat{y}^{(j)})=0$ , and for $j\\not\\in\\mathrm{Agr}(x,y)$ , due to the property of the $(\\alpha,R)$ -design, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n|\\delta(\\hat{x}^{(j)},\\hat{y}^{(j)})-2R|\\leq2\\alpha R.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Summing over all $j\\not\\in\\mathrm{Agr}(x,y)$ , we get ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left|\\delta({\\hat{x}},{\\hat{y}})-2(L-|\\mathrm{Agr}(x,y)|)R\\right|\\leq2\\alpha R L\\leq{\\frac{c\\gamma}{4r\\log n}}R L,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where we used $\\begin{array}{r}{\\alpha\\leq\\frac{c\\gamma}{8r\\log n}}\\end{array}$ ", "page_idx": 20}, {"type": "text", "text": "Non-neighbors If {x, y} \u2208/E, then by Propositions 29 and 30 we have 0 \u2264|Agr(x, y)| \u22644rc l\u03b3oLg n. By Equation (2) we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left|\\delta(\\hat{x},\\hat{y})-2\\left(1-\\frac{c\\gamma}{4r\\log n}\\right)R L\\right|\\leq\\frac{c\\gamma}{4r\\log n}R L\\quad\\implies\\quad|\\delta(\\hat{x},\\hat{y})-2R L|\\leq\\frac{c\\gamma R L}{r\\log n}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Neighbors If $\\{x,y\\}\\in E$ , then by Propositions 29 and 30, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\vert\\vert\\mathrm{Agr}(x,y)\\vert-\\gamma L\\vert\\leq\\frac{2c\\gamma L}{4r\\log n}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and by Equation (2) it follows that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left|\\delta(\\hat{x},\\hat{y})-2\\left(1-\\gamma\\right)R L\\right|\\leq2\\left(\\frac{2c\\gamma}{4r\\log n}\\right)R L\\leq\\frac{c\\gamma R L}{r\\log n}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "D Contrastive Queries in $\\ell_{p}$ Norm ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section, we show upper bounds for dimensions for embedding into space with $\\ell_{p}$ -norms or $\\ell_{\\infty}$ -norm. ", "page_idx": 20}, {"type": "text", "text": "D.1 Contrastive Queries for Finite $p$ ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section, we prove Theorem 3, which provides an upper bound of $m+1$ on the embedding dimension in $\\ell_{p}$ for integer $p\\geq1$ . ", "page_idx": 20}, {"type": "text", "text": "We say that a set $S\\subseteq V\\times V$ is symmetric if $(x,y)\\in S\\Leftrightarrow(y,x)\\in S$ . ", "page_idx": 20}, {"type": "text", "text": "Due to the symmetry, with a slight abuse of notation, we define the cardinality $|S|$ for symmetric sets to be equal to the number of distinct unordered pairs in $S$ . ", "page_idx": 20}, {"type": "text", "text": "Definition 31 (Partial semimetric). An ordered triple $(V,S,\\delta_{S})$ consisting of a set $V$ , a symmetric set $S\\subseteq V\\times V$ and a map $\\delta_{S}\\colon S\\rightarrow[0,\\infty)$ is $a$ partial semimetric space $i f\\delta_{S}$ satisfies the following: ", "page_idx": 21}, {"type": "text", "text": "1. For all $x\\in V$ , $i f(x,x)\\in S$ then $\\delta_{S}(x,x)=0$ ", "page_idx": 21}, {"type": "text", "text": "2. $\\delta_{S}(x,y)=\\delta_{S}(y,x).$ for all $(x,y)\\in S$ ,   \n3. $\\delta_{S}(x,y)+\\delta_{S}(y,z)\\leq\\delta_{S}(x,z)$ for all $(x,y),(x,z),(y,z)\\in S$ . ", "page_idx": 21}, {"type": "text", "text": "Definition 32 (Partial embedding). We say that a partial semimetric $(V,S,\\delta_{S})$ partially embeds into a metric space $(Y,\\delta_{Y})$ if there exists a map $F\\colon V\\rightarrow Y$ such that $\\delta_{S}(x,y)=\\delta_{Y}(F(x),F(y))$ for all $(x,y)\\in S$ . ", "page_idx": 21}, {"type": "text", "text": "The following lemma is an extension of the standard embedding result into $\\ell_{p}$ (see e.g. [DL97]). Lemma 33. Let $\\pmb{S}=(V,S,\\delta_{S})$ be a partial semimetric on $V$ and let $m=|S|$ . If S partially embeds into an $\\ell_{p}^{p}$ -space with finite dimension, then it embeds into $(\\ell_{p}^{p})^{m+1}$ . ", "page_idx": 21}, {"type": "text", "text": "Proof. Let $\\{\\{x_{i},y_{i}\\}\\}_{i=1}^{m}$ be the unordered pairs of $S$ . We assign every partial semimetric $(V,S,\\delta)$ on $S$ an $m$ -dimensional vector $v_{\\delta}$ , where $\\bar{v_{\\delta}[i]}=\\delta(x_{i},y_{i})$ . We call $v_{\\delta}$ the representation vector of $(V,S,\\delta)$ . Define $\\mathrm{NOR}^{S}$ to be the set of representations of all partial semimetrics on $S$ which can be partially embedded into $\\ell_{p}^{p}$ , i.e. ", "page_idx": 21}, {"type": "text", "text": "Note that $\\mathrm{{NOR}}^{S}$ is a cone: ", "page_idx": 21}, {"type": "text", "text": "1. If $\\boldsymbol{v}_{\\delta}\\in\\mathrm{NOR}^{S}$ then $\\alpha v_{\\delta}\\in\\mathrm{NOR}^{S}$ for all $\\alpha\\geq0$ .   \n2. If $v_{\\delta},v_{\\delta^{\\prime}}\\in\\mathrm{NOR}^{S}$ then $v_{\\delta}+v_{\\delta^{\\prime}}\\in\\mathrm{NOR}^{S}$ . ", "page_idx": 21}, {"type": "text", "text": "An extreme ray is a point $v_{\\delta}\\in\\mathrm{NOR}^{S}$ such that if $v_{\\delta}=v_{\\delta_{1}}+v_{\\delta_{2}}$ for ${v_{\\delta_{1}}},{v_{\\delta_{2}}}\\in\\mathrm{NOR}^{S}$ then it has to be that ${v}_{\\delta_{1}}=\\alpha{v}_{\\delta}$ and $v_{\\delta_{2}}=(1-\\alpha)v_{\\delta}$ for some $\\alpha\\in[0,1]$ . ", "page_idx": 21}, {"type": "text", "text": "Next, we show that any extreme ray of $\\mathrm{NOR}^{S}$ has a partial embedding into the one-dimensional space $(\\mathbb{R},\\ell_{p}^{p})$ . Indeed, let $v_{\\delta}$ be an extreme ray, and let $d$ be the minimum dimension for which $(V,S,\\delta)$ partially embeds to $(\\mathbb{R}^{d},\\ell_{p}^{p})$ . If $d\\,=\\,1$ , then we are done; otherwise, assume by contradiction that $d>1$ . Let $F:V\\to\\mathbb{R}^{d}$ such that $\\delta(x,y)\\,=\\,\\delta_{p}^{p}(F(x),F(y))$ for all $(x,y)\\in S$ . Let $F_{1}:V\\to\\mathbb{R},F_{2}:V\\to\\mathbb{R}^{d-1}$ such that ", "page_idx": 21}, {"type": "equation", "text": "$$\nF_{1}(x)=F(x)[1]{\\mathrm{~and~}}F_{2}(x)=(F(x)[2],\\dots,F(x)[d]),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "i.e. $F_{1}$ is the embedding $F$ restricted to the first dimension, and $F_{2}$ is $F$ restricted to the remaining $d-1$ dimensions. We notice that for each $(x,y)\\,\\in\\,V\\,\\times\\,V$ , $\\delta_{p}^{p}(F(x,y))\\,=\\,\\delta_{p}^{p}(F_{1}(x,y))\\,+$ $\\delta_{p}^{p}(F_{2}(x,y))$ . ", "page_idx": 21}, {"type": "text", "text": "Define $\\rho_{1},\\rho_{2}:S\\rightarrow\\mathbb{R}$ such that $\\rho_{1}(x,y)=\\delta_{p}^{p}(F_{1}(x),F_{1}(y))$ , and $\\rho_{2}(x,y)\\,=\\,\\delta_{p}^{p}(F_{2}(x),F_{2}(y))$ . Therefore, $v_{\\delta}~=~v_{\\rho_{1}}+v_{\\rho_{2}}$ . Since $v_{\\delta}$ is an extreme ray, then there exists $\\alpha\\,\\in\\,[0,1]$ such that $v_{\\delta}=\\alpha v_{\\rho_{1}}$ . In particular, $\\delta$ can be partially embedded into one dimension, by taking the embedding $\\alpha F_{1}(x)$ , contradicting minimality of $d$ . We conclude that $d=1$ . ", "page_idx": 21}, {"type": "text", "text": "Finally, let $v_{\\mathbf{S}}$ be the representation vector of S. By Caratheodory\u2019s theorem, since $v_{\\mathbf{S}}\\,\\in\\,\\mathrm{NOR}^{S}$ , there exists $m\\!+\\!1$ extreme rays $v_{\\delta_{1}},\\ldots,v_{\\delta_{m+1}}\\in\\mathrm{NOR}^{S}$ such that $\\begin{array}{r}{v_{\\mathbf{S}}=\\sum_{i=1}^{m+1}v_{\\delta_{i}}}\\end{array}$ . We have shown that for each $i\\in[m+1]$ , the partial semi-metric $(X,S,\\delta_{i})$ has a partial embedding $F^{(i)}:V\\rightarrow\\mathbb{R}$ into the one dimensional space $(\\mathbb{R},\\ell_{p}^{p})$ . It follows that the embedding $F=(F^{(1)},\\dots,F^{(m+1)})$ is a partial embedding of S into $(\\mathbb{R}^{m+1},\\ell_{p}^{p})$ , and the claim follows. \u53e3 ", "page_idx": 21}, {"type": "text", "text": "Proof of Theorem 3. If $Q$ is a set of non-contradictory constraints, then we can embed it into $\\ell_{2}$ using Theorem 1. We can then embed it isometrically into $\\ell_{p}$ (see Chapter 1.5 from [Mat13] and Theorem 5 from $[\\mathrm{Goe}06];$ . By using the same points, the relationships between distances are also preserved in $\\ell_{p}^{p}$ . Let $S$ be the set of all edges in the constraint graph $G$ . Then we have a partial semimetric $(V,\\bar{S},\\delta_{S})$ which is partially embedded into $\\ell_{p}^{p}$ . By Lemma 33 it partially embeds isometrically into $(\\ell_{p}^{p})^{|S|+1}$ . For the same embedding, the relationships between distances are also preserved in $\\ell_{p}$ . ", "page_idx": 21}, {"type": "text", "text": "D.2 Contrastive Queries in $\\ell_{\\infty}$ Norm ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this section, we prove Theorem 2, which states that dimension $O(m^{2/3})$ suffices to satisfy any set of $m$ non-contradictory contrastive queries $Q$ in the $\\ell_{\\infty}$ norm. ", "page_idx": 22}, {"type": "text", "text": "Let $G\\,=\\,(V,E)$ be the constraint graph, where $E$ is the edge set. We arbitrarily assign a unique identifier $\\mathrm{id}(x)\\in[n]$ for each $x\\in V$ . Let $V_{\\mathrm{high}}\\subseteq V$ be the set of points with degree with at least $m^{1/3}$ in $G$ . Let $V_{\\mathrm{low}}=V\\setminus V_{\\mathrm{high}}$ . ", "page_idx": 22}, {"type": "text", "text": "Our embedding is a concatenation of two embeddings $F_{1}$ and $F_{2}$ , which intuitively \u201chandle\u201d $V_{\\mathrm{low}}$ and $V_{\\mathrm{high}}$ respectively. In the sub-embedding $F_{1}$ , we use the fact that the graph induced by $V_{\\mathrm{low}}$ has low degree to argue that it has a proper distance-2-edge coloring with $O(m^{2/3})$ colors, i.e. we can color the edges of the graph such that no two edges at distance at most 2 share the same color. We use this coloring to obtain an embedding $F_{1}\\colon V_{\\mathrm{low}}\\to\\mathbb{R}^{O(m^{2/3})}$ which satisfies certain distance properties between any pair of neighbors in $V_{\\mathrm{low}}$ . We then extend $F_{1}$ to an embedding $F\\colon V\\rightarrow$ RO(m2/3) which is consistent with Q. This extension draws inspiration from the seminal Fre\u00b4chet embedding $\\mathrm{[Fre10]}$ : for each point in $x_{i}\\in V_{\\mathrm{high}}$ we add a single distinct dimension $i$ , in which we intuitively set this coordinate for each point $x\\in V$ as distance from $x_{i}$ in $F^{\\prime}$ . In actuality, we set these coordinates slightly differently, in order to combine correctly with the sub-embedding $F_{1}$ , and obtain an embedding which is consistent with $Q$ . By Lemma 34, the size $|V_{\\mathrm{high}}|=O(m^{2/3})$ , which implies that together the dimension of $F$ is $O(m^{2/3})$ . ", "page_idx": 22}, {"type": "text", "text": "Lemma 34. Let $Q$ be the set of m contrastive queries. Let $V_{\\mathrm{high}}$ be the set of points with degree at least $m^{1/3}$ in the constraint graph. Then $|V_{\\mathrm{high}}|=O(m^{2/3})$ . ", "page_idx": 22}, {"type": "text", "text": "Proof. Recall that each query $(x,y^{+},z^{-})\\,\\in\\,Q$ is associated with two edges, $\\{x,y\\},\\{x,z\\}\\in E$ . Hence, the total number of edges in $G$ is at most $2|Q|=2m$ . This implies ", "page_idx": 22}, {"type": "equation", "text": "$$\nm^{1/3}\\vert V_{\\mathrm{high}}\\vert\\leq\\sum_{x\\in V_{\\mathrm{high}}}\\deg(x)\\leq\\sum_{x\\in V}\\deg(x)=2\\vert E\\vert=4m.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "By rearrangement, we obtain that $\\vert V_{\\mathrm{high}}\\vert\\le4m^{2/3}$ . ", "page_idx": 22}, {"type": "text", "text": "Since $Q$ is non-contradictory, by Fact 25 there exists a metric $\\delta$ consistent with $Q$ . Using the Freche\u00b4t embedding [Fre\u00b410], any metric on $n$ points may be isometrically embedded into $\\mathbb{R}^{n-1}$ under the $\\ell_{\\infty}$ norm. ", "page_idx": 22}, {"type": "text", "text": "Definition 35 (Scaled Freche\u00b4t embedding $F^{\\prime}$ ). Let $F^{\\prime}\\colon V\\to\\mathbb{R}^{n-1}$ be an embedding of $\\delta$ into the cube $[0,1/2]^{n-1}$ under the $\\ell_{\\infty}$ norm, obtained by scaling and shifting (i.e. multiplying or adding some value to all coordinates, respectively) the Frech\u00b4et embedding of $\\delta$ . ", "page_idx": 22}, {"type": "text", "text": "We note that scaling and shifting do not affect whether a contrastive query is satisfied, therefore $F^{\\prime}$ is consistent with $Q$ as well. ", "page_idx": 22}, {"type": "text", "text": "Lemma 36. There is an embedding $F_{1}$ of $V_{\\mathrm{low}}$ into $\\mathbb{R}^{O(m^{2/3})}$ such that the following hold: ", "page_idx": 22}, {"type": "text", "text": "(a) for each $x\\in V_{\\mathrm{low}}$ and $i\\in\\mathbb{N}$ , it holds that $F_{1}(x)[i]\\in[0,1].$ ;   \n$(b)$ for each $x,y\\in V_{\\mathrm{low}}$ such that $\\{x,y\\}\\in E$ , it holds that $\\|F_{1}(x)-F_{1}(y)\\|_{\\infty}=1/2+\\|F^{\\prime}(x)-$ $F^{\\prime}(y)\\Vert_{\\infty}$ . ", "page_idx": 22}, {"type": "text", "text": "Proof. By definition, each $x\\in V_{\\mathrm{low}}$ has degree at most $\\Delta=O(m^{1/3})$ . Therefore, there is an edge coloring $C\\colon E\\to[\\Delta^{2}+2]$ of $G\\,=\\,(V,\\bar{E})$ , in which (a) every vertex has at most one incident edge of any color, and (b) any two adjacent vertices $x,y$ share exactly one edge color \u2013 the one of their shared edge $C(x,y)$ . We remark that this coloring is called in the literature distance-2-edge coloring. Such a coloring can be found using a greedy approach, where we color the edges one by one, where for each edge $\\{x,y\\}$ we choose a color that is not taken by previous edges of $x,y$ or by edges of any neighbor $z\\in N({\\dot{x}})\\cup N(y)$ . In other words, let $K(x,y)$ be the set of colors taken by any edge incident to any vertex in $\\{x,y\\}\\cup N(x)\\cup N(y)$ . Since $|K(x,y)|\\leq2\\Delta^{2}+1$ , then we can always choose from $\\{x,y\\}$ a color different from all colors of $K(x,y)$ . ", "page_idx": 22}, {"type": "text", "text": "We define the embedding $F_{1}$ as follows: assume the color of the edge of $\\{x,y\\}$ is $C(x,y)\\in[\\Delta^{2}\\!+\\!2]$ . Let $c=C(x,y)$ and assume w.l.o.g. that $\\operatorname{id}(x)<\\operatorname{id}(y)$ . We define $F_{1}(x)[c]=0$ and $F_{1}(y)[c]\\stackrel{}{=}$ $1/2+\\|F^{\\prime}(x)-F^{\\prime}(y)\\|_{\\infty}$ . For any $x\\in V$ , if a coordinate $i$ is not set in this process, we set $F_{1}(x)[i]=1/2$ . We note this is well-defined since the edge coloring is proper (i.e. no vertex has two edges of the same color), so no coordinate is set twice. This concludes the description of the embedding. ", "page_idx": 23}, {"type": "text", "text": "Next, we show that properties (a) and (b) hold. Recall that we consider distances over $\\ell_{\\infty}$ , hence for each pair $x,y\\ \\in\\ V_{\\mathrm{low}}$ there is a coordinate $i(x,y)~\\in~\\mathbb{N}$ for which $\\|F^{\\prime}(x)\\,-\\,F^{\\prime}(y)\\|_{\\infty}\\,=$ $\\|F^{\\prime}(x)[i(\\bar{x},y)]-F^{\\prime}(y)[i(x,y)]\\|$ . ", "page_idx": 23}, {"type": "text", "text": "For property (a), we note that every coordinate $i$ is either set to $F(y)[i]\\;=\\;0$ , or to $F(y)[i]\\;=\\;$ $1/2\\,\\bar{+}\\,\\|F^{\\prime}\\bar{(}x)\\,-\\,F^{\\prime}(y)\\|_{\\infty}$ for some $x\\in V$ . Since $\\|F^{\\prime}(x)\\,-\\,F^{\\prime}(y)\\|_{\\infty}^{\\cdot\\cdot}\\,\\in\\,\\left[0,1/2\\right]$ , and hence $\\|F(x)-F(y)\\|_{\\infty}=1/2+\\|F^{\\prime}(x)-F^{\\prime}(y)\\|_{\\infty}\\in[0,1]$ , property (a) follows. ", "page_idx": 23}, {"type": "text", "text": "Next, we show that property (b) holds. Denoting $c=C(x,y)$ , for each edge $\\{x,y\\}\\in E$ such that $\\operatorname{id}(x)<\\operatorname{id}(y)$ , it holds that $\\dot{F}^{\\prime}(x)[c]=0$ and $F^{\\prime}(y)[c]=1/2\\!+\\!\\|F(x)\\!-\\!F(y)\\|_{\\infty}$ . Second, since $x,y$ share only one edge color, in each other coordinate $j\\neq c$ , either $F_{1}(x)[j]=1/2$ or $F_{1}(y)[j]=1/2$ , meaning that $c$ is the coordinate with the maximum difference, i.e. $\\|F_{1}(x)-F_{1}(y)\\|_{\\infty}=1/2+$ $\\|F^{\\prime}(x)-F^{\\prime}(y)\\|_{\\infty}$ . \u53e3 ", "page_idx": 23}, {"type": "text", "text": "The Overall Embedding: ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Lemma 37. Let $F_{1}\\colon V_{\\mathrm{low}}\\to\\mathbb{R}^{O(m^{2/3})}$ be the embedding described in Lemma 36. Then there exists an embedding $F\\colon V\\to\\mathbb{R}^{O(m^{2/3})}$ such that for any $\\{x,z\\}\\in E$ it holds that $\\|F(x)-F(z)\\|_{\\infty}=$ $1/2+\\|F^{\\prime}(x)-F^{\\prime}(z)\\|_{\\infty}$ . ", "page_idx": 23}, {"type": "text", "text": "Proof. Let $d\\,=\\,O(m^{2/3})$ be the dimension of $F_{1}$ (i.e. $F_{1}\\colon V_{\\mathrm{low}}\\ \\to\\ \\mathbb{R}^{d})$ ), and $r\\;=\\;|V_{\\mathrm{high}}|\\;=$ $O(m^{2/3})$ . Let $V_{\\mathrm{high}}~=~\\{y_{1},\\ldots,y_{r}\\}$ . We define $F(x)$ as follows: for $y_{i}~\\in~V_{\\mathrm{high}}$ , we set all coordinates for $1\\leq j\\leq(d\\!+\\!i-\\!1)$ to $F(y_{i})[j]=1/2$ , the $(d\\!+\\!i)$ \u2019th coordinate as $F(\\stackrel{\\smile}{y}_{i})[d+i]=0.$ , and set any coordinate $d+i+1\\,\\leq\\,j\\,\\leq\\,d+r$ to $F(y_{i})[j]\\;=\\;1/2+\\|F^{\\prime}(y_{i})\\,-\\,\\dot{F^{\\prime}}(y_{j-d})\\|_{\\infty}\\;$ . For $x\\,\\in\\,V_{\\mathrm{low}}$ , we set the first $d$ coordinates to be as in $F_{1}(x)$ . The remaining $r$ coordinates, i.e. $d+1\\,\\leq\\,j\\,\\leq\\,d+r$ , we define as $F(x)[j]\\,=\\,1/2\\,+\\,\\|F^{\\prime}\\(\\dot{x})\\,-\\,F^{\\prime}(y_{j})\\|_{\\infty}$ . This concludes the description of the embedding. ", "page_idx": 23}, {"type": "text", "text": "First, we show that for any $x,z\\,\\in\\,V_{\\mathrm{low}}$ such that $\\{x,z\\}\\,\\in\\,E$ , it holds that $\\|F(x)-F(z)\\|_{\\infty}=$ $1/2+\\|F^{\\prime}(x)-F^{\\prime}(z)\\|_{\\infty}$ . This indeed holds by Lemma 36, and by the fact that in all the $r$ last coordinates are set to a value in $[1/2,1]$ , i.e. the difference on any of these coordinates is at most $1/2$ . ", "page_idx": 23}, {"type": "text", "text": "Next, we show that for any $x\\in V_{\\mathrm{low}}$ and $y_{i}\\in V_{\\mathrm{high}}$ such that $\\{x,y_{i}\\}\\in E$ , it holds that $\\left\\|F(x)-\\right\\|$ $F(y_{i})\\|_{\\infty}=1/2+\\|F^{\\prime}(x)-F^{\\prime}(y_{i})\\|_{\\infty},$ . Indeed, in any coordinate $j\\neq(d+i),F(y_{i})[j]\\geq1/2$ , and hence $|F(y_{i})[j]-F(x)[j]|\\leq1/2$ . On the other hand, in the $(d+i)^{\\cdot}$ \u2019th coordinate $|F(y_{i})[d+i]-$ $F(x)[d+i]|=1/2+||F^{\\prime}(x)-F^{\\prime}(y_{i})||_{\\infty}>1/2.$ . ", "page_idx": 23}, {"type": "text", "text": "Finally, we consider the case where $y_{i},y_{j}\\in V_{\\mathrm{high}}$ such that $\\{y_{i},y_{j}\\}\\in E$ and $i<j$ . For the first $d$ coordinates, both vectors are set to $1/2$ , in all coordinates between $d+1,\\dotsc,d+j-1$ the vector $y_{j}$ is set to $1/2$ , and therefore they differ by at most $1/2$ in these coordinates. For the $(d+j)$ \u2019th coordinate, $y_{j}$ is set to zero, and $y_{i}$ is set to $1/2+\\|F^{\\prime}(\\dot{y_{i}})-F^{\\prime}(y_{j})\\|_{\\infty}$ . For higher coordinates, both $y_{i},y_{j}$ are set to values at least $1/2$ . Therefore, $\\|F(\\dot{y}_{i})-\\dot{F}(y_{j})\\|_{\\infty}^{\\infty}=1/2+\\|F^{\\prime}(y_{i})-F^{\\prime}(y_{j})\\|_{\\infty}$ . ", "page_idx": 23}, {"type": "text", "text": "Finally, we show that $F$ is consistent with $Q$ . ", "page_idx": 23}, {"type": "text", "text": "Lemma 38. The embedding $F\\colon V\\to\\mathbb{R}^{O(m^{2/3})}$ is consistent with $Q$ . ", "page_idx": 23}, {"type": "text", "text": "Proof. For any $(x,y^{+},z^{-})\\ \\in\\ Q$ , it holds that $\\|F^{\\prime}(x)\\,-\\,F^{\\prime}(y)\\|_{\\infty}\\,<\\,\\|F^{\\prime}(x)\\,-\\,F^{\\prime}(z)\\|_{\\infty}$ , and therefore ", "page_idx": 23}, {"type": "text", "text": "\u2225 $F(x)-F(z)\\|_{\\infty}=1/2+\\|F^{\\prime}(x)-F^{\\prime}(z)\\|_{\\infty}>1/2+\\|F^{\\prime}(x)-F^{\\prime}(y)\\|_{\\infty}=\\|F(x)-F(y)\\|_{\\infty}.$ And since $\\|F(x)-F(y)\\|_{\\infty}<\\|F(x)-F(z)\\|_{\\infty}$ , the query $(x,y^{+},z^{-})$ is satisfied. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "Theorem 2 follows directly from Lemma 38. ", "page_idx": 23}, {"type": "text", "text": "E Lower Bounds ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In this section, we prove lower bounds for all our settings. Before presenting the main theorem of this section, we formally introduce the notion of ordinally embedding a metric $\\delta$ into $\\ell_{p}$ space. ", "page_idx": 24}, {"type": "text", "text": "Recall that for $x\\in V$ , we denote $\\pi_{1}(x),\\ldots,\\pi_{n-1}(x)$ to be the points in $V\\setminus\\{x\\}$ ordered by their distance from $x$ . ", "page_idx": 24}, {"type": "text", "text": "Definition 39 (Ordinal Embedding). Given a metric $\\delta$ , the full ordinal sample set $Q(\\delta)$ is the following set of samples: $Q(\\delta)\\,=\\,\\{(x,\\pi_{i}^{+}(x),\\pi_{i+1}^{-}(x))\\mid x\\,\\in\\,V,i\\,\\in\\,[n\\,-\\,2]\\}$ . We say that $\\delta$ can be ordinally embedded in $\\ell_{p}$ space in dimension $d$ if its full ordinal sample set $Q$ is consistent with some embedding in $\\ell_{p}$ space with dimension $d$ . ", "page_idx": 24}, {"type": "text", "text": "Next, we present the main theorem of this section, from which we can obtain lower bounds for all our settings: ", "page_idx": 24}, {"type": "text", "text": "Theorem 40. For $p\\in\\mathbb{N}\\cup\\{\\infty\\}$ , there exists a metric $\\delta$ on n points which can only be ordinally embedded in $\\ell_{p}$ -space using $\\dot{d}=\\Omega(n)$ dimensions if $p$ is a constant even integer $p\\,\\geq\\,2,$ , or $d=$ $\\Omega(n/\\log n)$ if $p$ is a constant odd integer $p\\geq1$ or $p=\\infty$ . ", "page_idx": 24}, {"type": "text", "text": "We remark that the special case of $p=2$ was previously proven in [CI24]. To prove Theorem 40, we need several propositions. ", "page_idx": 24}, {"type": "text", "text": "For a set of unlabeled triplets $C$ , we say that a set of samples $Q$ is a labeling of $C$ if $Q$ has exactly one labeling for each unlabeled triplet of $C$ (and no other sample). We next show that there exists a set of $\\Theta(n^{\\overleftarrow{2}})$ triplets so that any its labeling is valid. ", "page_idx": 24}, {"type": "text", "text": "Lemma 41 $[\\mathrm{AAE^{+}}24]$ ). For $V\\,=\\,\\{x_{1},\\ldots,x_{n}\\},$ , let $C\\,=\\,\\{(x_{i},x_{j},x_{j+1})\\}_{1\\leq i<j<n}$ be the set of unlabeled triplets, whose labeling compares distances between $(x_{i},x_{j})$ and $\\left(x_{i},x_{j+1}\\right)$ . Then for any labeling $Q$ of $C$ , there is a metric $\\delta_{Q}$ consistent with $Q$ . ", "page_idx": 24}, {"type": "text", "text": "Proof. Let $Q$ be a labeling of $C$ . Fix anchor $x_{i}$ and consider a graph where we create a directed edge $x_{j}\\,\\to\\,x_{j+1}$ when $(\\bar{x_{i}},x_{j}^{+},x_{j+1}^{-})\\,\\in\\,Q$ , and an edge $x_{j+1}\\to x_{j}$ when $(x_{i},x_{j+1}^{+},x_{j}^{-})\\,\\in\\,Q$ . Note that for any $Q$ this graph is acyclic (since the corresponding undirected edges form a path), and hence there exists a topological sort $p_{i}$ on $x_{i+1},\\ldots,x_{n}$ . We define a metric $\\delta$ so that $\\delta(x_{i},x_{j})=$ $\\delta(x_{j},x_{i})=n+p_{i}(x_{j}\\bar{)}$ for $i<j$ and $\\delta(x_{i},x_{i})=0$ for all $i$ . ", "page_idx": 24}, {"type": "text", "text": "Note that $\\delta$ is a metric: by construction, $\\delta$ is symmetric and $\\delta(x,x)=0$ for all $x$ , and the triangle inequality is satisfied since all distances are between $n$ and $2n$ . Finally, note that $\\delta$ satisfies all samples from $Q$ . \u53e3 ", "page_idx": 24}, {"type": "text", "text": "Next, we use a claim proven in $[\\mathrm{AAE^{+}}24]$ , showing that any sufficiently large set of unlabeled triplets has an labeling which does not have a $d\\!.$ -dimensional $\\ell_{p}$ space embedding consistent with it (where the size of the unlabeled set is at least some function of $n,d,p)$ . ", "page_idx": 24}, {"type": "text", "text": "Fact 42 $[\\mathrm{AAE^{+}}24]$ , Reformulated). Let d be an integer, $V$ be a set of n points, and $p\\in\\{1,2,\\ldots\\}\\cup$ $\\{\\infty\\}$ be constant. Then there exists a constant $c_{p}\\;>\\;0$ such that for any sufficiently large n the following hold. ", "page_idx": 24}, {"type": "text", "text": "\u2022 If $p$ is odd or $p=\\infty$ , then for any set of triplets $C$ of size at least $c_{p}n d\\log n$ on $V$ , there exists a labeling of $C$ which is not consistent with any $d$ -dimensional $\\ell_{p}$ space.   \n\u2022 If $p$ is even, then for any set of triplets $C$ of size at least $c_{p}n d$ on $V$ , there exists a labeling which is not consistent with any $d$ -dimensional $\\ell_{p}$ space. ", "page_idx": 24}, {"type": "text", "text": "Proof of Theorem 40. We consider the case of even $p$ \u2013 cases of odd and infinite $p$ are analogous. By Lemma 41, for some constant $c>0$ there exists a set of triplets $C$ of cardinality at least $c n^{\\bar{2}}$ so that any labeling of $C$ is realizable by some metric. On the other hand, by Fact 42, when $|C|>c_{p}n d_{\\mathrm{:}}$ , there exists a labeling $Q$ of $C$ which is not consistent with any $d$ -dimensional $\\ell_{p}$ space metric. Solving for $d$ , unless $d>n c/c_{p}$ , there exists a labeling which is not realizable in the $d_{\\cdot}$ -dimensional $\\ell_{p}$ space. Hence, $d=\\Omega(n)$ for even $p$ . \u53e3 ", "page_idx": 24}, {"type": "text", "text": "Next, we show lower bounds for our settings, namely for contrastive learning and k-NN, and for the extended settings of $t$ -negatives and $t$ -orderings. All lower bounds follow as immediate corollaries of Theorem 40. ", "page_idx": 25}, {"type": "text", "text": "Theorem 43. Let p be a positive even integer. ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "1. (Contrastive triplets) There exists a set of non-contradictory triplet sample\u221as $Q$ of size $|Q|=m$ for which any embedding in $\\ell_{p}$ space consistent with $Q$ must have $d={\\bar{\\Omega}}({\\sqrt{m}})$ . 2. (t-negatives) There exists a set of non-contradictory $t$ -negatives sa\u221amples $Q$ of size $|Q|=m$ such that any embedding in $\\ell_{p}$ space in d dimensions requires $d=\\Omega({\\bar{\\sqrt{m}}})$ . 3. $t$ -orderings) There exists a set of non-contradictory $t$ -ordering sa\u221amples $Q$ of size $|Q|=m$ such that any embedding in $\\ell_{p}$ space in d dimensions requires $d=\\Omega({\\sqrt{m t}})$ . 4. (k-NN) There exists a metric $\\delta$ on n points such that any embedding in $\\ell_{p}$ space which preserves the $k$ -NN ordering of $\\delta$ must have $d=\\Omega(k)$ dimensions. ", "page_idx": 25}, {"type": "text", "text": "When $p$ is a positive odd integer or when $\\textit{p}=\\infty,$ \u221a, the lower bounds decrease \u221aby a logarithmic fact\u221aor, that is the lower bounds are respectively $\\Omega({\\sqrt{m}}/\\log m)$ , $\\Omega(k/\\log k)$ , $\\Omega(\\sqrt{m}/\\log m)$ , and $\\Omega({\\sqrt{m t}}/\\log(m t))$ . ", "page_idx": 25}, {"type": "text", "text": "Note that in the above statements, $V$ can be arbitrarily large: in the proofs below, we can choose subsets of required size inducing all the samples. ", "page_idx": 25}, {"type": "text", "text": "Proof. We consider the case of positive even $p$ . The cases of positive odd or infinite $p$ are analogous. ", "page_idx": 25}, {"type": "text", "text": "1. Choose an arbitrary set $V$ of size $\\sqrt{m}$ . By Theorem 40, there exists a non-contradictory sample set $Q$ of size $\\Theta(m)$ on point set $V$ such that any embedding into $\\ell_{p}$ space which is consistent with $Q$ must have dimension $\\Omega({\\sqrt{m}})$ . ", "page_idx": 25}, {"type": "text", "text": "2. L\u221aet $V$ be an arbitrary set of size ${\\sqrt{m}}+\\left(t-1\\right)$ , and $V^{\\prime}$ be an arbitrary subset of $V$ of size $\\sqrt{m}$ . By the previous item, there e\u221axists a non-contradictory sample set $Q^{\\prime}$ of size $\\Theta(m)$ on a set $V^{\\prime}$ that requires dimension $\\Omega({\\sqrt{m}})$ dimensions. Let $V\\setminus V^{\\prime}\\,=\\,\\{v_{1},\\dots,v_{t-1}\\}$ . For each $s^{\\prime}=(x,y^{+},z^{-})\\in Q^{\\prime}$ , define $s$ to be the $(t+1)$ -tuple sample $s=(x,y^{+},z^{-},v_{1}^{-},\\ldots,v_{t-1}^{-})$ . Let $Q$ be the set of all such $(t+1)$ -tuple samples. Next, we prove that $Q$ is non-contradictory. Since $Q^{\\prime}$ is non-contradictory, there is a metric $\\delta^{\\prime}$ on $V^{\\prime}$ which is consistent with $Q$ . Consider the following metric $\\delta$ on $V$ : for $x,y\\in V^{\\prime}$ , we set $\\delta(x,y)=\\delta^{\\prime}(x,y)$ , and otherwise $\\delta(x,y)=D$ , where $D=2\\operatorname*{max}_{x,y\\in V^{\\prime}}\\delta(x,y)$ . It is easy to see $\\delta$ satisfies triangle inequality, and is consistent with $Q$ . Since every constraint in $Q^{\\prime}$ is imp\u221alied by some constraint in $Q$ , embedding preserving $Q$ must also preserve $Q^{\\prime}$ , requiring $\\Omega({\\bar{\\sqrt{m}}})$ dimension.   \n3. Choose an arbitrary set $V$ of size $\\sqrt{m t}$ . By the first item, ther\u221ae exists a non-contradictory sample set $Q^{\\prime}$ of size $O(m t)$ on a set $V$ that requires dimension $\\Omega({\\sqrt{m t}})$ . It suffices to show that there is a set of non-contradictory $Q$ of size $\\bar{O}(m)$ of $(t+1)$ -tuple samples that imply all inequalities ", "page_idx": 25}, {"type": "text", "text": "of $Q^{\\prime}$ . ", "page_idx": 25}, {"type": "text", "text": "Consider a metric $\\delta$ on $V$ consistent with $Q^{\\prime}$ . Denoting the $j$ \u2019th nearest neighbor of $x$ according to $\\delta$ as $\\pi_{j}(x)$ , let ", "page_idx": 25}, {"type": "equation", "text": "$$\nQ=\\cup_{x\\in V}\\{(x,\\pi_{1}(x),\\ldots,\\pi_{t}(x)),(x,\\pi_{t}(x),\\ldots,\\pi_{2t-1}(x)),\\ldots\\},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the adjacent samples share one item. We note that $Q$ is consistent with $\\delta$ , hence is noncontradictory. Finally, every inequality in $Q^{\\prime}$ is implied by the inequalities of $Q$ : this is due to the fact that $\\delta$ is consistent with $Q^{\\prime}$ , and $Q$ implies all ordinal constraints of $\\delta$ (as it implies the order of distances between each point and all its neighbors). ", "page_idx": 25}, {"type": "text", "text": "4. Choose an arbitrary set $V$ of size $k+1$ . By Theorem 40, there exists a non-contradictory sample set $Q$ on point set $V$ such that any embedding into $\\ell_{p}$ space consistent with $Q$ must have dimension $\\Omega(k)$ . Consider a metric $\\delta$ on $V$ consistent with $Q$ . Since $|V|=k+1$ , k-NNs preserve all triplet comparisons of $\\delta$ , and therefore, any embedding of $V$ preserving the $\\mathrm{k\\Omega}$ -NN ordering has to be consistent with $Q$ , hence requiring dimension $\\Omega(k)$ . \u53e3 ", "page_idx": 25}, {"type": "text", "text": "F Other Results ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In this section, we first extend our results to contrastive queries with more than two candidates. Then, we show that the problem of actually constructing the embedding consistent with given contrastive samples is NP-hard. Finally, we consider an approximate setting for contrastive learning, in which we only need to satisfy an $\\alpha$ -fraction of the constraints. We show that there exists an instance for which satisfying $\\alpha\\approx0.77$ fraction of the constraints requires roughly the same number of dimensions as satisfying all constraints. On the other hand, we show that for $\\alpha\\leq1/2$ , one dimension always suffices. ", "page_idx": 26}, {"type": "text", "text": "F.1 Upper Bound for $t$ -Negatives and $t$ -Ordering Samples in $\\ell_{2}$ -norm ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In this section, we consider two additional settings, in which each sample contains ordinal information about the distance between an anchor point and multiple (i.e. more than two other) points. ", "page_idx": 26}, {"type": "text", "text": "In the first setting $t$ -negatives), we are given a set $Q$ of $m$ samples, where each sample $s$ is a $(t+2)$ -tuple $\\bar{s}\\;\\bar{=}\\;(x,\\bar{y^{+}},z_{1}^{-},\\ldots,z_{t}^{-})$ . We say sample $s$ is satisfied by distance function $\\delta$ if ${\\delta(x,y)>\\delta(x,z_{i})}$ for all $1\\leq i\\leq t$ . ", "page_idx": 26}, {"type": "text", "text": "In the second setting ( $t$ -ordering), we are given a set $Q$ of $m$ samples, where each sample $s$ is a $t$ -tuple $s\\,=\\,(x,y_{1},\\ldots,y_{t})$ , and we say sample $s$ is satisfied by distance function $\\delta$ if $\\delta(x,y_{1})~<$ $\\delta(x,y_{2})<\\dots<\\delta(x,y_{t})$ for all $1\\leq i\\leq t$ . ", "page_idx": 26}, {"type": "text", "text": "Theorem 44 ( $t$ -orderings, $t$ -negatives). Let $Q$ be a set of m non-contradictory $t$ -orde\u221aring samples (resp. $t$ -negative samples) on a set $V$ . There is an embedding of $V$ into $\\ell_{2}$ -space $\\mathbb{R}^{O(\\sqrt{m t})}$ which is consistent with $Q$ . ", "page_idx": 26}, {"type": "text", "text": "Proof. For a set of $(t+2)$ -tuple samples $Q$ on $V$ of size $m$ , we define the constraint graph $G=$ $(V,E)$ as follows: for each sample $(x_{1},\\ldots,x_{t+2})\\in Q$ , we add $t{+}1$ edges $\\{x_{1},x_{2}\\},\\dots,\\{x_{1},x_{t+2}\\}$ to $E$ (if they don\u2019t already exist). ", "page_idx": 26}, {"type": "text", "text": "First, we note that the constraint graph of $t$ -orderings and $t$ -negatives has arboricity $O({\\sqrt{m t}})$ . Indeed, we add for each sample at most $O(t)$ edges \u221ato $G$ , hence the total number of edges is at most $O(m t)$ . By Fact 7, the arboricity of $G$ is $r=O({\\sqrt{m t}})$ . By Theorem 9 there exists an embedding into $\\ell_{2}$ -space with dimension $r=O({\\sqrt{m t}})$ that satisfies the corresponding inequalities. \u53e3 ", "page_idx": 26}, {"type": "text", "text": "F.2 NP-Hardness for $d=1$ ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In this section, we show that, empirical risk minimization for embedding into an $\\ell_{p}$ space is NPhard. Even in the realizable case and even for $d=1$ , finding an embedding satisfying constraints is NP-hard, by the reduction from the betweenness problem. ", "page_idx": 26}, {"type": "text", "text": "Definition 45 (Betweenness). You are given a set of items $X$ of cardinality n and a set of triplets $\\{(a_{1},b_{1},c_{1}),\\ldots,(a_{m},b_{m},c_{m})\\}.$ , such that $a_{i},b_{i},c_{i}\\;\\in\\;X$ for all $i$ . The goal of the betweenness problem is to find an order of items on $X$ so that for each i, $b_{i}$ is located between $a_{i}$ and $c_{i}$ . That is, the goal is to find a bijection $r\\colon X\\to\\{1,\\ldots,n\\}$ so that for each $i$ either $r(a_{i})<r(b_{i})<r(c_{i})$ or $r(c_{i})<r(b_{i})<r(a_{i})$ hold. ", "page_idx": 26}, {"type": "text", "text": "[Opa79] shows that the decision version of the betweenness problem \u2013 i.e. checking whether such an ordering exists \u2013 is NP-hard. ", "page_idx": 26}, {"type": "text", "text": "Theorem 46. Unless $P=N P$ , there is no polynomial algorithm for finding an embedding into $\\ell_{2}$ space for $d=1$ in the realizable case. ", "page_idx": 26}, {"type": "text", "text": "Proof. Let $A$ be an algorithm for finding an $\\ell_{2}$ embedding for $d\\,=\\,1$ , which accepts the set of contrastive queries as an input. For contradiction, assume that in the realizable case the algorithm finds an embedding in time at most $T(n)=\\mathrm{poly}(n)$ , where $n$ is the number of points. ", "page_idx": 26}, {"type": "text", "text": "Let $A^{\\prime}$ be the algorithm which executes $A$ for at most $T(n)=\\mathrm{poly}(n)$ iterations. This way, $A^{\\prime}$ runs on all inputs in time at most $T(n)$ and outputs an embedding satisfying the input constraints iff such an embedding exists. ", "page_idx": 26}, {"type": "text", "text": "We complete the proof by reduction from the betweenness problem. Let $\\{(a_{1},b_{1},c_{1}^{-}),\\ldots,(a_{m},\\bar{b_{m}},c_{m})\\}$ be the input for the betweenness problem. Then, we can represent constraint $\\mathbf{\\nabla}^{*}b_{i}$ is between $a_{i}$ and $c_{i}^{\\,,}{}^{,}$ using two contrastive constraints $(a_{i},b_{i}^{+},c_{i}^{-})$ and $(c_{i},b_{i}^{+},a_{i}^{-})$ . For example, if $r(b_{i})\\,<\\,r(a_{i})\\,<\\,r(c_{i})$ , then the constraint $(c_{i},b_{i}^{+},a_{i}^{-})$ is violated; other cases are similar. ", "page_idx": 27}, {"type": "text", "text": "We execute $A^{\\prime}$ on this set of contrastive constraints. Since the algorithm finds a satisfying embedding iff such an embedding exists, we can check whether the contrastive constraints \u2013 and hence the original betweenness constraints \u2013 are satisfiable by checking the output of the algorithm. Hence, we can verify whether the set of betweenness constraints is satisfiable in the polynomial time, which contradicts NP-hardness of the problem and assumption that $P\\neq N P$ . \u53e3 ", "page_idx": 27}, {"type": "text", "text": "F.3 Satisfying a Fraction of Constraints ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In this section, we consider the settings when the embedding doesn\u2019t have to satisfy all the constraints. Instead, for some constant $\\alpha$ , we want to satisfy at least an $\\alpha$ -fraction constraints. We show the following separation in the $\\ell_{p}$ case for any integer $p$ . ", "page_idx": 27}, {"type": "text", "text": "Theorem 47. For the embedding into $\\ell_{p}$ space for $p\\in\\{1,2,3,\\ldots\\}$ , the following hold. ", "page_idx": 27}, {"type": "text", "text": "\u2022 For any $\\alpha\\,\\leq\\,1/2,$ , for any set of m constraints, for any $d\\geq1$ there exists an embedding with dimension $d$ satisfying at least \u03b1m constraints.   \n\u2022 Let $\\alpha^{*}\\approx0.77$ be the root of equation $H(x)=x$ , where $H$ is the binary entropy function. Then for any $\\alpha>\\alpha^{*}$ , there exists a set of m non-con\u221atradictory constraints so that sa\u221atisfying at least \u03b1m constraints requires dimension at least $\\Omega({\\sqrt{m}})$ for even $p$ and at least $\\Omega({\\sqrt{m}}/\\log m)$ for odd $p$ . ", "page_idx": 27}, {"type": "text", "text": "Notes The theorem shows that for $\\alpha\\leq1/2$ , the problem trivializes, while for $\\alpha>\\alpha^{*}$ , the problem is asymptotically as hard as in the case when we have to satisfy all constraints (up to $\\log{m}$ factor for odd $p$ ). There is a gap between $1/2$ and $\\alpha^{*}\\approx0.77$ , and we hypothesize that $\\alpha^{*}$ bound is the most likely one to be improved, due to the union bound used in the proof below. ", "page_idx": 27}, {"type": "text", "text": "Proof. The case $\\alpha\\leq1/2$ follows by the probabilistic argument, using the observation that a random one-dimensional embedding satisfies half of the constraints in the expectation. It remains to handle the case $\\alpha>\\alpha^{*}$ . For that, we construct a set of $m$ triplets, and, for a random labeling of $m$ triplets, we look at the induced labeling of each subset of \u03b1m triplets. For each individual subset, we will show the probability that its induced labeling is achievable is less than $1/\\binom{m}{\\alpha m}$ . By the union bound, the probability that any of the induced labelings is achievable is less than 1, implying that for at least one labeling, none of the induced labelings is achievable ", "page_idx": 27}, {"type": "text", "text": "$\\ell_{2}$ distance We first consider the $\\ell_{2}$ -case, and below we describe how to handle $\\ell_{p}$ distance for outnhlearb eilnetde gterri $p$ .et sB syu cLhe mthmata  a4n1y,  itthse rlea bfeolri nagn yis  sreet $V$ aobf liet.e mFso,r t ha esrue fefxiciisetnst lay  sleatr $C$ , $m\\,=\\,{\\binom{n-1}{2}}$ $n$ $d\\,<\\,c n$ for some constant $c$ (depending on $\\alpha$ and to be specified later). We will show that for $\\alpha\\;>\\;\\alpha^{*}$ , there exists no subset of $C$ of size \u03b1m so that every its labeling is realizable by some embedding into a $d_{\\cdot}$ -dimensional space. For that, we will use the following fact. ", "page_idx": 27}, {"type": "text", "text": "Fact 48 ([War68]). Let $m\\;\\geq\\;t\\;\\geq\\;2$ be integers, and let $P_{1},\\allowbreak\\dots,P_{m}$ be real polynomials on $t$ variables of degree at most $s$ . Let ", "page_idx": 27}, {"type": "equation", "text": "$$\nU(P_{1},\\dots,P_{m})=\\left\\{\\mathbf{x}\\in\\mathbb{R}^{t}\\mid P_{i}(\\mathbf{x})\\neq0\\,f o r\\,a l l\\,i\\in[m]\\right\\}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "be the set of points $\\mathbf{x}\\in\\mathbb{R}^{t}$ which are non-zero in all polynomials. Then the number of connected components in $U(P_{1},\\dots,P_{m})$ is at most $(4e s m/t)^{t}$ . ", "page_idx": 27}, {"type": "text", "text": "Similarly to $[\\mathrm{AAE^{+}}24]$ , we apply this fact to the following polynomials: for each triplet $\\left(x,y,z\\right)$ , for a fixed embedding function $F$ , we define a polynomial ", "page_idx": 27}, {"type": "equation", "text": "$$\nP_{x y z}=\\|F(x)-F(y)\\|_{2}^{2}-\\|F(x)-F(z)\\|_{2}^{2}=\\sum_{i=1}^{d}(F_{i}(x)-F_{i}(y))^{2}-\\sum_{i=1}^{d}(F_{i}(x)-F_{i}(z))^{2}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Denoting $V~=~\\{x_{1},\\ldots,x_{n}\\}$ , all $P_{x y z}$ for $(x,y,z)\\ \\in\\ C$ are polynomials over $^{n d}$ variables $F_{1}(x_{1}),\\ldots,F_{d}(x_{1}),\\ldots,F_{1}(x_{n}),\\ldots,F_{d}(x_{n})$ . ", "page_idx": 28}, {"type": "text", "text": "Importantly, when $(x,y^{+},z^{-})$ is satisfied by $F$ , the polynomial is negative, while, when $(x,z^{+},y^{-})$ is satisfied by $F$ , the polynomial is negative. Hence, different choices of labels of $C$ must correspond to the different sign combinations of polynomials. Fact 48 shows that the number of sign combinations of the polynomials \u2013 and hence the amount of possible labelings \u2013 is bounded by $(8e m/n d)^{n d}\\leq(4e n/d)^{n d}$ , where we used $\\textstyle m={\\binom{n-1}{2}}<{\\frac{n^{2}}{2}}$ ", "page_idx": 28}, {"type": "text", "text": "For any subset of \u03b1m constraints, there are $2^{\\alpha m}$ possible induced labelings. On the other hand, as shown above, only $(4e n/d)^{n d}$ of the labelings are achievable. Taking the ratio of these values, we get that the probability that an induced labeling is realizable is at most ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\frac{(4e n/d)^{n d}}{2^{\\alpha m}}=2^{n d\\log_{2}(4e n/d)-\\alpha m}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "As outlined above, since there are at most $\\bigl(}{}_{\\alpha m}^{m}\\bigr)$ subset of \u03b1m constraints, we want this ratio to be at most $1/\\binom{m}{\\alpha m}$ . By a well-known fact [TJ06], $\\dot{\\binom{m}{\\alpha m}}\\leq2^{H(\\alpha)m}$ , where $H$ is a binary entropy function. Hence, the probability that any subset of $\\alpha m$ induced constraints is satisfiable is at most ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\frac{(4e n/d)^{n d}\\binom{m}{\\alpha m}}{2^{\\alpha(n-1)^{2}/2}}\\leq2^{n d\\log_{2}(4e n/d)-\\alpha m+H(\\alpha)m}=2^{m(H(\\alpha)-\\alpha+(n d/m)\\log_{2}(4e n/d))}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Since $m\\geq(n-1)^{2}/2$ , for a sufficiently large $n$ we have $n d/m<3d/n$ . Consider the case when $d<c n$ for some constant $c$ . When $c<4$ , the last term $(3d/n)\\log_{2}(4e n/d)$ monotonically increases in $d$ , and hence we have ", "page_idx": 28}, {"type": "equation", "text": "$$\nH(\\alpha)-\\alpha+(n d/m)\\log_{2}(4e n/d)<H(\\alpha)-\\alpha+3c\\log_{2}(4e/c)\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "When $\\alpha\\,>\\,\\alpha^{*}$ , where $\\alpha^{*}\\approx0.77$ satisfies $\\alpha^{*}=H(\\alpha^{*})$ , we have $0\\,>\\,H(\\alpha)-\\alpha$ . Since $f(c)=$ $3c\\log_{2}(4e/c)$ is continuous and strictly monotone for $c\\in[0,4]$ and $f(0)=0$ , there exists $c^{\\prime}>0$ such that $\\dot{H}(\\alpha)\\,-\\,\\alpha\\,+\\,3c^{\\prime}\\log_{2}(4e/c^{\\prime})\\,<\\,0$ . Hence, when $d<c^{\\prime}n$ , there exists a labeling of $m$ triplets, so that no subset of \u03b1m triplets is satisfiable. ", "page_idx": 28}, {"type": "text", "text": "$\\ell_{p}$ distances for positive integer $p$ When $p$ is even, the above argument doesn\u2019t change. When $p$ is odd, we encounter the issue that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\|F(x)-F(y)\\|_{p}^{p}-\\|F(x)-F(z)\\|_{p}^{p}=\\sum_{i=1}^{d}|F_{i}(x)-F_{i}(y)|^{p}-\\sum_{i=1}^{d}|F_{i}(x)-F_{i}(z)|^{p}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "is not a polynomial. We address this issue similarly to $[\\mathrm{AAE^{+}}24]$ : for each coordinate $i$ , we guess the order of points with respect to this coordinate. This introduces an additional factor of $(n!)^{d}=$ $2^{O(n d\\log n)}$ in the number of possible sign combinations. The derivation is similar to the above, but we instead want the following inequality: ", "page_idx": 28}, {"type": "equation", "text": "$$\nH(\\alpha)-\\alpha+(n d/m)\\log_{2}(4e n/d)+O((n d/m)\\log n)<0,\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "which holds when $d<c n/\\log n$ for some constant $c$ . ", "page_idx": 28}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 29}, {"type": "text", "text": "Justification: The abstract and introduction clearly state all claims and contributions. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 29}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: All limitations of the work are discussed. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \u201dLimitations\u201d section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model wellspecification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 29}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 29}, {"type": "text", "text": "Justification: Paper contains full proofs for all claims. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 30}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 30}, {"type": "text", "text": "Justification: Paper discloses all information needed for reproducing the experimental results. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 30}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] . ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: code added to supplementary material. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code. ", "page_idx": 30}, {"type": "text", "text": "\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 31}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] . ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: Full information and code are available. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.   \n\u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 31}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] . ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: All experiments have error bars and other nessecary information. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \u201dYes\u201d if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 31}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 32}, {"type": "text", "text": "Justification: The paper provide full information on the resources used in the experiments. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 32}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 32}, {"type": "text", "text": "Justification: Paper fully conforms to the NeurIPS Code of Ethics. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 32}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] . ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: The paper studies fundamentals of embedding theory, and it does not contain any subjects that might introduce any direct societal impacts. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. ", "page_idx": 32}, {"type": "text", "text": "\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 33}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 33}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 33}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 33}, {"type": "text", "text": "Justification: All datasets are properly cited and credited. We are not aware of any standard license mentioned the dataset\u2019s creator\u2019s paper or website, but it does include guidelines on how to properly use and cite their asset, which we followed. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 33}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 33}, {"type": "text", "text": "Justification: The code used for experiments is contained in the supplementary material. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets. \u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. ", "page_idx": 33}, {"type": "text", "text": "\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 34}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 34}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 34}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] . ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 34}]