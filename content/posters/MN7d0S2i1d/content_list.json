[{"type": "text", "text": "Sample and Computationally Efficient Robust Learning of Gaussian Single-Index Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Puqian Wang   \nDepartment of Computer Science   \nUniversity of Wisconsin, Madison pwang333@wisc.edu Nikos Zarifis   \nDepartment of Computer Science   \nUniversity of Wisconsin, Madison zarifis@wisc.edu Ilias Diakonikolas   \nDepartment of Computer Science   \nUniversity of Wisconsin, Madison ilias@cs.wisc.edu Jelena Diakonikolas   \nDepartment of Computer Science   \nUniversity of Wisconsin, Madison jelena@cs.wisc.edu ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "A single-index model (SIM) is a function of the form $\\sigma(\\mathbf{w}^{*}\\cdot\\mathbf{x})$ , where $\\sigma:\\mathbb{R}\\rightarrow\\mathbb{R}$ is a known link function and $\\mathbf{w}^{*}$ is a hidden unit vector. We study the task of learning SIMs in the agnostic (a.k.a. adversarial label noise) model with respect to the $\\bar{L}_{2}^{2}$ -loss under the Gaussian distribution. Our main result is a sample and computationally efficient agnostic proper learner that attains $L_{2}^{2}$ -error of $O(\\bar{\\mathrm{OPT}})+$ $\\epsilon$ , where OPT is the optimal loss. The sample complexity of our algorithm is $\\tilde{O}(d^{\\lceil k^{*}/2\\rceil}+d/\\epsilon)$ , where $k^{*}$ is the information-exponent of $\\sigma$ corresponding to the degree of its first non-zero Hermite coefficient. This sample bound nearly matches known CSQ lower bounds, even in the realizable setting. Prior algorithmic work in this setting had focused on learning in the realizable case or in the presence of semi-random noise. Prior computationally efficient robust learners required significantly stronger assumptions on the link function. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Single-index models (SIMs) [Ich93, HJS01, ${\\mathrm{HMS}}^{+}04$ , DJS08, KS09, KKSK11, DH18, $\\mathrm{DGK}^{+}20$ , DKTZ22, WZDD23, DNGL23] are a classical supervised learning model characterized by hidden low-dimensional structure. The term SIM refers to any function $f$ of the form $f(\\mathbf{x})=\\sigma(\\mathbf{w}\\cdot\\mathbf{x})$ , where $\\sigma:\\mathbb{R}\\rightarrow\\mathbb{R}$ is the link (or activation) function and $\\textbf{w}\\in\\mathbb{R}^{d}$ is the hidden vector. In most settings of interest, the link function is assumed to satisfy certain regularity properties. Indeed, for an arbitrary function, learnability is information-theoretically impossible. ", "page_idx": 0}, {"type": "text", "text": "The efficient learnability of SIMs has been the focus of extensive investigation for several decades. For example, the special case where $\\sigma$ is the sign function corresponds to Linear Threshold Functions whose study goes back to the Perceptron algorithm [Ros58]. Classical early works [KS09, KKSK11] studied the efficient learnability of SIMs for monotone and Lipschitz link functions. They showed that a gradient method efficiently learns SIMs for any distribution on the unit ball. More recently, there has been a resurgence of research on the topic with a focus on first-order methods. Indeed, the non-convex optimization landscape of SIMs exhibits rich structure and has become a useful testbed for the analysis of such methods. [Sol17] showed that SGD efficiently learns SIMs for the case that $\\sigma$ is the ReLU activation and the distribution is Gaussian. [CLS15, CCFM19, SQW18] showed that gradient descent succeeds for the phase retrieval problem, corresponding to $\\sigma(t)=t^{2}$ or $\\sigma(t)=|t|$ . ", "page_idx": 0}, {"type": "text", "text": "More recently, a line of work [DH18, BAGJ21, DNGL23, DPVLB24] studied the efficient learnability of SIMs going significantly beyond the monotonicity assumption. Specifically, [BAGJ21, DNGL23] developed efficient gradient-based SIM learners for a general class of \u2014 not necessarily monotone \u2014 link functions under the Gaussian distribution. Roughly speaking, these works show that the complexity of learning SIMs is intimately related to the Hermite structure of the link function (roughly, the smallest degree non-zero Hermite coefficient). The results of the current paper are most closely related to the aforementioned works. ", "page_idx": 1}, {"type": "text", "text": "All the aforementioned algorithmic results succeed in the realizable model (i.e., with clean labels) or in a few cases in the presence of random label noise. The focus of this work is on learning SIMs in the challenging agnostic (or adversarial label noise) model [Hau92, KSS94]. In the agnostic model, no assumptions are made on the observed labels and the goal is to compute a hypothesis that is competitive with the best-fti function in the class. The algorithmic study of agnostically learning SIMs is not new. A recent line of work $[{\\mathrm{DGK}}^{+}20$ , DKTZ22, ATV23, WZDD23, GGKS23, ZWDD24] has given efficient agnostic SIM learners (typically based on first-order methods) with near-optimal error guarantees under natural distributional assumptions. The key difference between prior work and the results of the current paper is in the assumptions on the link function. Specifically, prior robust learners succeed for (a subclass of) monotone and Lipschitz link functions. In contrast, this work develops robust learners in the more general setting of [BAGJ21, DNGL23]. ", "page_idx": 1}, {"type": "text", "text": "In order to precisely describe our contributions, we require the definition of the agnostic learning problem for Gaussian SIMs. Let $\\mathcal{D}$ be a distribution of labeled examples $(\\mathbf{x},y)\\in\\overline{{\\mathbb{R}^{d}}}\\times\\mathbb{R}$ whose $\\mathbf{x}$ -marginal is the standard Gaussian, and let $\\begin{array}{r}{\\mathcal{L}_{2}^{\\sigma}(\\mathbf{w}):=\\mathbf{E}_{(\\mathbf{x},y)\\sim\\mathcal{D}}[(\\sigma(\\mathbf{\\dot{w}}\\cdot\\mathbf{\\dot{x}})-y)^{2}]}\\end{array}$ be the $L_{2}^{2}$ (or squared) loss of the hypothesis $h(\\mathbf{x})=\\sigma(\\mathbf{w}\\cdot\\mathbf{x})$ with respect to $\\mathcal{D}$ . Given i.i.d. samples from $\\mathcal{D}$ , the goal is to compute a hypothesis with squared error competitive with OPT, where OPT is the best attainable $L_{2}^{2}$ -error by any function in the target class. ", "page_idx": 1}, {"type": "text", "text": "Problem 1.1 (Robustly Learning Gaussian SIMs). Let $\\mathcal{D}$ be a distribution of labeled examples $(\\mathbf{x},y)\\in\\mathbb{R}^{d}\\times\\bar{\\mathbb{R}}$ whose x-marginal is $\\mathcal{D}_{\\mathbf{x}}=\\mathcal{N}(0,\\mathbf{I}_{d})$ and $y$ is arbitrary. We say that an algorithm is a $C$ -approximate proper SIM learner, for some $C\\geq1$ , $i f$ given $\\epsilon>0$ and i.i.d. samples from $\\mathcal{D}$ , the algorithm outputs a vector $\\widehat{\\mathbf{w}}\\in\\mathbb{S}^{d-1}$ such that with high probability it holds $\\mathcal{L}_{2}^{\\sigma}(\\widehat{\\mathbf{w}})\\leq C\\,\\mathrm{OPT}+\\epsilon,$ where OPT : $\\bar{\\mathbf{\\rho}}=\\mathcal{L}_{2}^{\\sigma}(\\mathbf{w}^{*})$ a n d $\\mathbf{w}^{\\ast}\\in\\mathrm{argmin}_{\\mathbf{w}\\in\\mathbb{S}^{d-1}}\\,\\mathcal{L}_{2}^{\\sigma}(\\mathbf{w})$ . ", "page_idx": 1}, {"type": "text", "text": "First, note that Problem 1.1 does not make realizability assumptions on the distribution $\\mathcal{D}$ . That is, the labels are allowed to be arbitrary and the goal is to be competitive against the best-fit function in the class. Second, our focus is on obtaining efficient learners that achieve a constant factor approximation to the optimum loss \u2014 independent of the dimension $d$ . The reason we require a constant factor approximation, instead of optimal error of $\\mathrm{OPT}+\\epsilon.$ , is the existence of computational hardness results ruling out this possibility. Specifically, even if the link function is the ReLU, there is strong evidence that any algorithm achieving error $\\mathrm{OPT}+\\epsilon$ in the above setting requires $d^{\\mathrm{poly}(1/\\epsilon)}$ time [DKZ20, GGK20, DKPZ21, DKR23]. ", "page_idx": 1}, {"type": "text", "text": "Recent work $[{\\mathrm{DGK}}^{+}20$ , DKTZ22, ATV23, WZDD23] gave efficient, constant-factor robust learners, for the special case of Problem 1.1 where the link function lies in a proper subclass of monotone and Lipschitz functions. In this work, we obtain a broad generalization of these results to a much more general class of link functions, defined below. ", "page_idx": 1}, {"type": "text", "text": "We now proceed to formalize the assumptions on the link function. Let $\\sigma:\\mathbb{R}\\rightarrow\\mathbb{R}$ be a realvalued function that admits the Hermite decomposition $\\begin{array}{r}{\\sigma(z)~=~\\sum_{k\\geq0}c_{k}\\mathrm{he}_{k}(z)}\\end{array}$ , where $c_{k}=$ $\\mathbf{E}_{z\\sim\\mathcal{N}(0,1)}[\\sigma(z)\\mathrm{he}_{k}(z)]$ and $\\mathrm{he}_{k}$ is the normalized probabilist\u2019s Hermite polynomial, defined by ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathrm{he}_{k}(z)=\\frac{(-1)^{k}}{\\sqrt{k!}}\\exp\\big(\\frac{z^{2}}{2}\\big)\\frac{\\mathrm{d}^{k}}{\\mathrm{d}z^{k}}\\exp\\big(-\\frac{z^{2}}{2}\\big).\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "We make the following assumptions. ", "page_idx": 1}, {"type": "text", "text": "Assumption 1 (Family of Link Functions). Suppose that $\\sigma$ is normalized, namely $\\begin{array}{r}{\\mathbf{E}_{z\\sim\\mathcal{N}(0,1)}[\\sigma^{2}(z)]=\\sum_{k\\geq0}c_{k}^{2}=1}\\end{array}$ . We assume the following: ", "page_idx": 1}, {"type": "text", "text": "$(i)$ The first non-zero Hermite coefficient has degree $k^{*}$ and is prominent: $c_{k^{*}}$ is an absolute constant that is bounded away from zero. ", "page_idx": 1}, {"type": "text", "text": "(ii) The fourth moment of $\\sigma(z)$ is bounded: ${\\bf E}_{z\\sim\\mathcal{N}(0,1)}[\\sigma^{4}(z)]\\leq B_{4}<\\infty$ . ", "page_idx": 1}, {"type": "text", "text": "(iii) The second moment of the derivative of $\\sigma(z)$ is bounded: $\\begin{array}{r}{\\mathbf{E}_{z\\sim\\mathcal{N}(0,1)}[(\\sigma^{\\prime}(z))^{2}]=\\sum_{k\\geq k^{*}}k c_{k}^{2}\\leq}\\end{array}$ $C_{k^{*}}$ , where $C_{k^{*}}$ is an absolute constant whenever $k^{*}$ is an absolute constant. ", "page_idx": 2}, {"type": "text", "text": "The parameter $k^{*}$ is known as the information exponent of $\\sigma$ . ", "page_idx": 2}, {"type": "text", "text": "The information exponent $k^{*}$ can be viewed as a complexity measure of the link function. Specifically, ReLU activations correspond to $k^{*}\\,=\\,1$ . The same holds for the class of bounded activations considered in [DKTZ22, WZDD23]. The link functions used in phase retrieval have $k^{*}=2$ . ", "page_idx": 2}, {"type": "text", "text": "We note that Assumption 1 is (at least) as general as those used in [BAGJ21, DNGL23] \u2014 which focused on the realizable setting. Comparing against previous constant-factor agnostic learners, Assumption 1 strongly subsumes the class of \u201cbounded activations\u201d [DKTZ22, WZDD23]. In particular, it is easy to see that there exist functions satisfying Assumption 1 with constant $k^{*}$ that are far from monotone. See Appendix A for a detailed justification. ", "page_idx": 2}, {"type": "text", "text": "In prior work, [DNGL23], building on [BAGJ21], gave a sample-efficient gradient method for learning SIMs under Assumption 1 in the realizable setting. The sample complexity of their method is $\\tilde{O}(d^{\\bar{k}^{*}/2}+d/\\epsilon)$ . This sample upper bound essentially matches known lower bounds in the Correlational Statistical Query (CSQ) model [DLS22, AAM23]. ", "page_idx": 2}, {"type": "text", "text": "This discussion leads to the following question, which served as the motivation for the current work: ", "page_idx": 2}, {"type": "text", "text": "Is there an efficient constant-factor agnostic learner for Gaussian SIMs under Assumption 1? ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "As our main contribution, we answer this question in the affirmative. Interestingly, our algorithm also relies on a gradient-method (Riemannian optimization over the sphere) following a non-trivial initialization step. We emphasize that this is the first polynomial-time constant-factor agnostic learner for this task under Assumption 1. ", "page_idx": 2}, {"type": "text", "text": "Specifically, we establish the following result (see Theorem 3.5 for a more detailed statement). ", "page_idx": 2}, {"type": "text", "text": "Theorem 1.2 (Main Result, Informal). There exists an algorithm that draws $n=\\tilde{\\Theta}_{k^{*}}(d^{\\lceil k^{*}/2\\rceil}+d/\\epsilon)$ labeled samples, runs in poly $(n,d)$ time, and outputs a weight vector $\\widehat{\\mathbf{w}}\\,\\in\\,\\mathbb{S}^{d-1}$ that with high probability satisfies $\\mathcal{L}_{2}^{\\sigma}(\\widehat{\\mathbf{w}})\\leq\\dot{C}\\,\\mathrm{OPT}+\\epsilon,$ , where $\\bar{C}=O(C_{k^{*}})$ . ", "page_idx": 2}, {"type": "text", "text": "Theorem 1.2 gives the first sample and computationally efficient robust learner for Gaussian SIMs under Assumption 1. This generalizes the algorithm of [DNGL23] to the agnostic setting and nearly matches the aforementioned CSQ lower bounds (our algorithm ftis the CSQ framework). It is worth pointing out that, while more efficient (non-CSQ) algorithms have been developed for the realizable case [CM20], these algorithms provably fail in the agnostic regime. Finally, we remark that very recent work [DPVLB24] developed an efficient SIM learner and a nearly matching SQ lower bound in a model that allows for some forms of label noise. Importantly, their model does not capture the adversarial label noise studied here. More specifically, the algorithms developed in this prior work [DNGL23, DPVLB24] fail in the agnostic setting. See Appendix B for a detailed discussion. ", "page_idx": 2}, {"type": "text", "text": "1.1 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "For $n\\in\\mathbb{Z}_{+}$ , let $[n]:=\\{1,\\ldots,n\\}$ . We use lowercase bold characters to denote vectors and uppercase bold characters for matrices and tensors. For $\\mathbf{x}\\in\\mathbb{R}^{d}$ and $i\\in[d]$ , $\\mathbf{x}_{i}$ denotes the $i$ -th coordinate of $\\mathbf{x}$ , and $\\begin{array}{r}{\\|\\mathbf{x}\\|_{2}:=(\\sum_{i=1}^{d}\\mathbf{x}_{i}^{2})^{1/2}}\\end{array}$ denotes the $\\ell_{2}$ -norm of $\\mathbf{x}$ . We use $\\mathbf x\\cdot\\mathbf y$ for the inner product of $\\mathbf{x},\\mathbf{y}\\in\\mathbb{R}^{d}$ and $\\theta(\\mathbf{x},\\mathbf{y})$ for the angle between x, y. We slightly abuse notation and denote by ${\\bf{e}}_{i}$ the $i^{\\mathrm{th}}$ standard basis vector in $\\mathbb{R}^{d}$ . We use $\\mathbb{1}\\{\\mathcal{E}\\}$ to denote the indicator of a statement $\\mathcal{E}$ . We use $\\mathcal{N}_{d}$ to denote the $d$ -dimensional standard Gaussian distribution, i.e., $\\mathcal{N}_{d}=\\mathcal{N}(0,\\mathbf{I})$ . We use $\\mathbb{B}_{d}$ to denote the centered unit ball in $\\mathbb{R}^{d}$ and denote the unit sphere in $\\mathbb{R}^{d}$ by $\\mathbb{S}^{d-1}$ . We use $\\mathrm{proj}_{\\mathbb{B}_{d}}(\\cdot)$ to denote the projection operator that projects a vector to the unit ball. ", "page_idx": 2}, {"type": "text", "text": "Given $\\mathbf{M}\\in\\mathbb{R}^{d_{1}\\times d_{2}}$ and a left singular vector $\\mathbf{v}\\in\\mathbb{R}^{d_{1}}$ of $\\mathbf{M}$ , we denote the corresponding singular value of $\\mathbf{v}$ by $\\rho(\\mathbf{v})$ . In addition, we use $\\rho_{1}\\geq\\rho_{2}\\geq\\cdots\\geq\\rho_{\\operatorname*{min}\\{d_{1},d_{2}\\}}$ to denote the singular values of a matrix $\\mathbf{M}\\in\\mathbb{R}^{d_{1}\\times d_{2}}$ . We use $\\mathcal{S}_{k}$ to denote the set of all possible permutations of $k$ distinct objects. Given a unit vector w, we define $\\mathbf{P}_{\\mathbf{w}^{\\perp}}:=\\mathbf{I}-\\mathbf{w}\\mathbf{w}^{\\top}$ to be the projection matrix that maps a vector $\\mathbf{v}$ to its component that is orthogonal to w, i.e., $\\mathbf{P_{w^{\\perp}}v}=\\mathbf{v^{\\perp}w}$ . ", "page_idx": 2}, {"type": "text", "text": "Given a vector $\\mathbf{x}\\in\\mathbb{R}^{d}$ , the (normalized) Hermite multivariate tensor is defined by [Rah17]: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{He}_{k}(\\mathbf{x}))_{i_{1},\\ldots,i_{k}}:=\\left({\\frac{\\alpha_{1}!\\cdot\\cdot\\cdot\\alpha_{d}!}{k!}}\\right)^{1/2}\\operatorname{he}_{\\alpha_{1}}(\\mathbf{x}_{1})\\,\\ldots\\operatorname{he}_{\\alpha_{d}}(\\mathbf{x}_{d}),{\\mathrm{~where~}}\\alpha_{j}=\\sum_{l=1}^{k}\\mathbf{1}\\left\\{i_{l}=j\\right\\},\\forall j\\in[d].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We use the standard $O(\\cdot),\\Theta(\\cdot),\\Omega(\\cdot)$ asymptotic notation. We use $\\widetilde O(\\cdot)$ to omit polylogarithmic factors in the argument. We write $E\\gtrsim F$ for two non-negative expressions $E$ and $F$ to denote that there exists some positive universal constant $c>0$ such that $E\\ge c F$ . ", "page_idx": 3}, {"type": "text", "text": "1.2 Technical Overview ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Our technical approach consists of two main parts: (1) new results for tensor PCA, which allow us to obtain an initial parameter vector $\\mathbf{w}^{0}$ that is nontrivially aligned with the target $\\mathbf{w}^{*}$ and (2) a structural \u201calignment sharpness\u201d result, which we use to argue that a variant of Riemannian minibatch stochastic gradient descent on a sphere applied to a \u201ctruncated square loss\u201d (defined below) converges geometrically fast. In proving these results, we review elementary tensor algebra and basic properties of Hermite polynomials, and prove several structural results for Hermite polynomials that are instructive and may be useful to non-experts entering this area. ", "page_idx": 3}, {"type": "text", "text": "We now highlight some of the key ideas used in our work. ", "page_idx": 3}, {"type": "text", "text": "Initialization via Tensor PCA For our optimization algorithm to work, a warm start ensuring nontrivial alignment between the initial vector $\\mathbf{w}^{0}$ and the target vector $\\mathbf{w}^{*}$ , as measured by $\\mathbf{w}^{0}\\cdot\\mathbf{w}^{*}$ , is essential. In particular, a consequence of our results in Claim 3.1 and Lemma 3.2 is that $\\mathbf{w}^{0}\\cdot\\mathbf{w}^{*}=$ $\\Omega(1)$ is required to deal with the highly corruptive agnostic noise. Unfortunately, if we were to select $\\mathbf{w}_{\\mathrm{0}}$ by drawing uniformly random samples from the sphere, exponentially many in $d$ such samples would be needed to ensure that with constant probability at least one of the sampled vectors $\\mathbf{w}_{\\mathrm{0}}$ is such that $\\mathbf{w}^{0}\\cdot\\mathbf{w}^{*}=\\Omega(1)$ , due to standard results on concentration of measure on the (unit) sphere. ", "page_idx": 3}, {"type": "text", "text": "Perhaps surprisingly, we prove that the tensor PCA method developed in [RM14] when applied to our problem with $\\bar{O}(d^{\\lceil k^{*}/2\\rceil})$ samples1 ensures that $\\mathbf{w}^{0}\\cdot\\mathbf{w}^{*}\\geq1-\\operatorname*{min}\\{1/k^{*},1/2\\}$ . The reason that this result is surprising is that the method in [RM14] was developed to solve the following problem: given a $k$ -tensor of the form ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{T}=\\tau\\mathbf{v}^{\\otimes k}+\\mathbf{A},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathbf{A}$ is a $k$ -tensor with i.i.d. standard Gaussian entries and $\\tau>0$ a \u201csignal strength\u201d parameter, recover the planted (signal) vector $\\mathbf{v}$ . This \u2018single-observation\u2019 model is equivalent (in law) to the followin\u221ag \u2018multi-observation\u2019 model ([BAGJ20]): given $n$ i.i.d. copies $\\mathbf{T}^{(i)}=\\tau^{\\prime}\\mathbf{v}^{\\otimes k}+\\mathbf{A}^{(i)}$ with $\\tau^{\\prime}=\\tau/\\bar{\\sqrt{n}}$ , recover $\\mathbf{v}$ using the empirical estimation: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\widehat{\\mathbf{T}}=\\tau^{\\prime}\\mathbf{v}^{\\otimes k}+(1/n)\\sum_{i=1}^{n}\\mathbf{A}^{(i)}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In our setting, we wish to recover a vector $\\mathbf{w}^{*}$ (up to some constant alignment error) for the $k$ -Chow tensor $\\mathbf{C}_{k}=\\mathbf{E}_{(\\mathbf{x},y)\\sim\\mathcal{D}}[y\\mathbf{H}\\mathbf{e}_{k}(\\mathbf{x})]$ , which can be decomposed as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{C}_{k}=\\underset{\\mathbf{x}\\sim\\mathcal{N}_{d}}{\\mathbf{E}}\\big[\\sum_{j\\geq k^{*}}c_{j}\\langle\\mathbf{He}_{j}(\\mathbf{x}),\\mathbf{w}^{*\\otimes j}\\rangle\\mathbf{He}_{k}(\\mathbf{x})\\big]+\\underset{(\\mathbf{x},y)\\sim\\mathcal{D}}{\\mathbf{E}}[(y-\\sigma(\\mathbf{w}^{*}\\cdot\\mathbf{x}))\\mathbf{He}_{k}(\\mathbf{x})].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The first term in this decomposition can be viewed as the \u201csignal\u201d $k$ -tensor. The second term represents noise, which, due to $y$ being potentially arbitrary, cannot be assumed to be Gaussian. Thus, previously developed techniques for tensor PCA, which crucially rely on the \u201cGaussianity\u201d of the noise term, do not apply here. ", "page_idx": 3}, {"type": "text", "text": "To obtain our result, we first argue that for any $k\\geq k^{*}$ , the top left singular vector $\\mathbf{v}^{*}$ of the $k$ -Chow tensor $\\mathbf{C}_{k}$ unfolded into a matrix of roughly equal dimensions carries a \u201cstron\u221ag signal\u201d about the target vector $\\mathbf{w}^{*}$ : its associated singular value scales with $c_{k}$ whenever $c_{k}=\\Omega(\\sqrt{\\mathrm{OPT}})$ and it has a nontrivial alignment with the vectorized version of the $l_{\\cdot}$ -tensor $\\mathbf{w}^{\\ast\\otimes l}$ for $l=\\lfloor k/2\\rfloor$ (Lemma 2.2). ", "page_idx": 3}, {"type": "text", "text": "To prove the desired alignment result using the empirical estimate of the matrix-unfolded $k$ -Chow tensor $\\mathbf{C}_{k}$ , we rely on the application of a matrix concentration inequality obtained very recently in $[\\mathrm{BvH}22]$ . This requires a rather technical argument utilizing Gaussian hypercontractivity of multivariate polynomials of bounded degree, which we show characterizes the different \u201cvariancelike\u201d quantities associated with the empirical estimate of (the matrix-unfolded) $\\mathbf{C}_{k}$ , for which we apply the aforementioned matrix concentration (see Lemma 2.4 and its proof). ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Anothe\u221ar intriguing aspect of our initialization result is that it is possible to use it directly to obtain an $O(\\sqrt{\\mathrm{OPT}}+\\epsilon)$ -error solution. In particular, in the realizable case studied in [DNGL23], where $\\mathrm{OPT}=0$ , this result directly leads to error $O(\\epsilon)$ in a sample and computationally efficient manner, with a rather simple algorithm and sample complexity comparable to [DNGL23]. ", "page_idx": 4}, {"type": "text", "text": "Optimization on a Sphere The second key ingredient in our work is a structural result, stated in Lemma 3.3, which ensures that the gradient field (Riemannian gradient of a truncated loss) guiding the steps of our algorithm (which can be interpreted as a Riemmanian minibatch SGD on a sphere) negatively correlates with $\\mathbf{w}^{*}$ to a significant extent. This property can be viewed as the considered gradient field, associated with the $L_{2}^{\\tilde{2}}$ loss truncated to only contain the first nonzero term in the Hermite expansion of the activation function, containing a strong \u201csignal\u201d that can \u201cpull\u201d the algorithm iterates towards the target $O(\\mathrm{OPT})+\\epsilon$ solutions. We rely on this property to argue that as long as our algorithm (initialized using the tensor PCA method described above) does not have as its iterate a vector with $O(\\mathrm{OPT})+\\epsilon$ loss, the distance between the iterate vector and the target vector must contract. As a consequence, this algorithm converges in ${\\cal O}(\\log(1/\\epsilon))$ iterations. ", "page_idx": 4}, {"type": "text", "text": "This argument parallels the line of work [MBM18, $\\mathrm{DGK}^{+}20$ , WZDD23, ZWDD24] on addressing learning of single-index models by proving structural, optimization-theory local error bounds that bound below the growth of a loss function outside the set of target solutions. Conceptually, the local error bounds from this line of work all have an intuitive interpretation as showing existence of a strong \u201csignal\u201d in the problem that can be used to guide algorithm updates towards target solutions. However, the methodology by which our structural result is obtained is entirely different, as it crucially relies on properties of Hermite polynomials, which were not considered in this past work. ", "page_idx": 4}, {"type": "text", "text": "2 Initialization Procedure ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we show how to get an initial parameter vector $\\mathbf{w}^{0}$ such that $\\mathbf{w}^{0}\\cdot\\mathbf{w}^{*}\\,=\\,1\\,-\\,\\epsilon_{0}$ for some small constant $\\epsilon_{0}$ . The main technique is a tensor PCA algorithm that finds the principal component of a noisy degree- $k$ -Chow tensor for any $k\\geq k^{*}$ , as long as $\\mathrm{OPT}\\lesssim c_{k}^{2}$ . Such a degree- $k$ Chow tensor is defined by $\\mathbf{C}_{k}=\\mathbf{E}_{(\\mathbf{x},y)\\sim\\mathcal{D}}[y\\mathbf{H}\\mathbf{e}_{k}(\\mathbf{x})]$ , and we denote its noiseless counterpart by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{C}_{k}^{*}=\\underset{\\mathbf{x}\\sim\\mathcal{N}_{d}}{\\mathbf{E}}[\\sigma(\\mathbf{w}^{*}\\cdot\\mathbf{x})\\mathbf{H}\\mathbf{e}_{k}(\\mathbf{x})]=\\underset{\\mathbf{x}\\sim\\mathcal{N}_{d}}{\\mathbf{E}}\\big[\\sum_{j\\ge k^{*}}c_{j}\\langle\\mathbf{H}\\mathbf{e}_{j}(\\mathbf{x}),\\mathbf{w}^{*\\otimes j}\\rangle\\mathbf{H}\\mathbf{e}_{k}(\\mathbf{x})\\big].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Furthermore, let us denote the difference between $\\mathbf{C}_{k}$ and $\\mathbf{C}_{k}^{*}$ by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{H}_{k}:=\\mathbf{C}_{k}-\\mathbf{C}_{k}^{*}=\\underset{(\\mathbf{x},y)\\sim\\mathcal{D}}{\\mathbf{E}}[(y-\\sigma(\\mathbf{w}^{*}\\cdot\\mathbf{x}))\\mathbf{H}\\mathbf{e}_{k}(\\mathbf{x})].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Note that since $\\mathbf{He}_{k}(\\mathbf{x})$ is a symmetric tensor for any $\\mathbf{x}$ , all $\\mathbf{C}_{k},\\mathbf{C}_{k}^{*}$ , and ${\\mathbf{H}}_{k}$ are symmetric tensors. We use the following matrix unfolding operator that maps a $k$ -tensor $\\mathbf{T}$ to a matrix in $\\mathbb{R}^{d^{l}\\times d^{k-l}}$ :2 ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{M}\\mathsf{a t}_{(l,k-l)}(\\mathbf{T})_{i_{1}+(i_{2}-1)d+\\cdots+(i_{l}-1)d^{l-1},j_{1}+\\cdots+(j_{k-l}-1)d^{k-l-1}}:=(\\mathbf{T})_{i_{1},i_{2},\\ldots,i_{l},j_{1},\\ldots,j_{k-l-1}}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "for all $i_{1},\\ldots,i_{l},j_{1},\\ldots,j_{k-l}\\in[d]$ . We also define the \u2018vectorize\u2019 operator and \u2018tensorize\u2019 operators, which map a vector $\\mathbf{v}\\in\\mathbb{R}^{d^{l}}$ to an $l$ -tensor for any integer $l$ , and vice versa. In detail, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathsf{T e n s o r}(\\mathbf v)_{i_{1},\\ldots,i_{l}}:=\\mathbf v_{i_{1}+(i_{2}-1)d+\\cdots+(i_{l}-1)d^{l-1}},\\;\\forall i_{1},\\ldots,i_{l}\\in[d],}\\\\ &{\\mathsf{V e c}(\\mathbf v^{\\otimes l})_{i_{1}+(i_{2}-1)d+\\cdots+(i_{l}-1)d^{l-1}}:=\\mathbf v_{i_{1}}\\mathbf v_{i_{2}}\\ldots\\mathbf v_{i_{l}},\\;\\forall i_{1},\\ldots,i_{l}\\in[d].}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Finally, given a vector $\\mathbf{v}\\in\\mathbb{R}^{d}$ , we can also convert this vector to a matrix of size $\\mathbb{R}^{d\\times d^{l-1}}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb M\\mathsf{a t}_{(1,l-1)}(\\mathbf v)_{i,j_{1},\\ldots,j_{l-1}}=\\mathbf v_{i+(j_{1}-1)d+\\cdots+(j_{l-1}-1)d^{l-1}},\\;\\forall i,j_{1},\\ldots,j_{l-1}\\in[d].}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Throughout this section, we take $l:=\\lfloor k/2\\rfloor$ . We leverage the tensor unfolding algorithm proposed in [RM14], which can succinctly be described as follows. First we unfold the degree- $k$ Chow tensor ", "page_idx": 4}, {"type": "text", "text": "1: Input: Parameters $\\epsilon,k,\\epsilon_{0},c_{k},B_{4}>0$ ; Sample access to $\\mathcal{D}$   \n2: Let $l=\\lfloor k/2\\rfloor$   \n3: Draw $n=\\Theta(e^{k}\\log^{k}(B_{4}/\\epsilon)d^{k-l}/(\\epsilon_{0}^{2})+1/\\epsilon)$ samples $\\{(\\mathbf{x}^{(i)},y^{(i)})\\}_{i=1}^{n}$ from $\\mathcal{D}$   \n4: Construct $\\begin{array}{r}{\\widehat{\\mathbf{M}}:=(1/n)\\sum_{i=1}^{n}\\mathsf{M}\\mathsf{a t}_{(l,k-l)}(y^{(i)}\\mathbf{H}\\mathbf{e}_{k}(\\mathbf{x}^{(i)}))}\\end{array}$ ; compute its top left singular vector $\\widehat{\\mathbf{v}}^{*}$   \n5: Compute the top-left singular vector $\\widehat{\\bf{u}}$ of the matrix $\\mathsf{M a t_{1,}}_{l-1}(\\widehat{\\mathbf{v}}^{*})$   \n6: Return:u ", "page_idx": 5}, {"type": "text", "text": "to a matrix in $\\mathbb{R}^{d^{l}\\times d^{k-l}}$ and find its top-left singular vector $\\mathbf{v}\\in\\mathbb{R}^{d}$ . Then, we calculate the matrix $\\mathsf{M a t}_{(1,l-1)}(\\mathbf{v})$ , and output its top left singular vector $\\mathbf{u}$ . ", "page_idx": 5}, {"type": "text", "text": "Our main result for initialization is that the output of Algorithm 1 significantly correlates with $\\mathbf{w}^{*}$ . ", "page_idx": 5}, {"type": "text", "text": "Proposition 2.1 (Initialization). Suppose Assumption 1 holds. Assume that $\\mathrm{OPT}\\le c_{k^{*}}^{2}/(64k^{*})^{2},$ , and let $\\epsilon_{0}\\,=\\,c_{k^{*}}/(256k^{*})$ . Then, Algorithm $^{\\,l}$ applied to Problem 1.1 with $k\\;=\\;k^{*}$ uses $n=$ $\\Theta((k^{*})^{2}e^{k^{*}}\\log^{k^{*}}(B_{4}/\\epsilon)d^{\\lceil k^{*}/2\\rceil}/(c_{k^{*}}^{2})+1/\\epsilon)$ samples, runs in polynomial time, and outputs $a$ vector $\\mathbf{w}^{0}\\in\\mathbb{S}^{d-1}$ such that $\\mathbf{w}^{0}\\cdot\\mathbf{w}^{*}\\geq1-\\operatorname*{min}\\{1/k^{*},1/2\\}$ . ", "page_idx": 5}, {"type": "text", "text": "We remark here that Algorithm 1 can also be used to find an approximate solution of our agnostic learning problem; however the error dependence on OPT is suboptimal, scaling with its square-root. For full details of this argument, included for completeness, see Proposition D.3 in Appendix D. ", "page_idx": 5}, {"type": "text", "text": "In the remainder of this section, we sketch the proof of Proposition 2.1, which relies on two main ingredients: (1) alignment of the left singular vectors $\\mathbf{v}$ of matrix-unfolded $k$ -Chow tensor and the target vector $\\mathbf{w}^{*}$ , which can be interpreted as the $k$ -Chow tensor containing a strong \u201csignal\u201d about the target vector $\\mathbf{w}^{*}$ , and (2) matrix concentration for the unfolded tensor, so that we can translate \u201cpopulation\u201d properties of the $k$ -Chow tensor to computable empirical quantities. ", "page_idx": 5}, {"type": "text", "text": "Signal in the $k$ -Chow Tensor Our first observation is that for any left singular vector $\\mathbf{v}$ of $\\mathsf{M a t}_{(l,k-l)}(\\mathbf{C}_{k})$ , the singular value $\\rho(\\mathbf{v})$ is close to the inner product between $\\mathbf{v}$ and $\\mathsf{V e c}(\\mathbf{w}^{*\\otimes l})$ , where $l=\\lfloor k/2\\rfloor$ . Concretely, we have: ", "page_idx": 5}, {"type": "text", "text": "L\u221aemma 2.2. Let v be any left singular vector of $\\mathsf{M a t}_{(l,k-l)}\\big(\\mathbf{C}_{k}\\big)$ . Then, $|\\rho(\\mathbf{v})\\!-\\!c_{k}(\\mathsf{V e c}(\\mathbf{w}^{*\\otimes l})\\!\\cdot\\!\\mathbf{v})|\\leq$ $\\sqrt{\\mathrm{OPT}}$ . ", "page_idx": 5}, {"type": "text", "text": "Proof Sketch of Lemma 2.2. Recall that the singular value of the left singular vector $\\mathbf{v}$ satisfies ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\rho(\\mathbf{v})=\\operatorname*{max}_{\\mathbf{r}\\in\\mathbb{R}^{d^{k-l}},\\|\\mathbf{r}\\|_{2}=1}\\mathbf{v}^{\\top}\\mathbb{M}\\mathsf{a t}_{(l,k-l)}(\\mathbf{C}_{k})\\mathbf{r}\\overset{(i)}{=}\\operatorname*{max}_{\\mathbf{r}\\in\\mathbb{R}^{k-l},\\|\\mathbf{r}\\|_{2}=1}\\langle\\mathbf{C}_{k},\\mathsf{T e n s o r}(\\mathbf{v})\\otimes\\mathsf{T e n s o r}(\\mathbf{r})\\rangle,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where we used Fact C.1(2) in $(i)$ . Since $\\mathbf{C}_{k}=\\mathbf{C}_{k}^{*}+\\mathbf{H}_{k}$ , we further have ", "page_idx": 5}, {"type": "text", "text": "$\\langle\\mathbf{C}_{k},\\mathsf{T e n s o r}(\\mathbf{v})\\otimes\\mathsf{T e n s o r}(\\mathbf{r})\\rangle=\\langle\\mathbf{C}_{k}^{*},\\mathsf{T e n s o r}(\\mathbf{v})\\otimes\\mathsf{T e n s o r}(\\mathbf{r})\\rangle+\\langle\\mathbf{H}_{k},\\mathsf{T e n s o r}(\\mathbf{v})\\otimes\\mathsf{T e n s o r}(\\mathbf{r})\\rangle.$ We bound both terms above respectively. For the first term, plugging in the definition of $\\mathbf{C}_{k}^{*}$ and using the orthonormality property of Hermite tensors (Fact C.3) and basic tensor algebraic calculations, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\langle\\mathbf{C}_{k}^{*},\\mathsf{T e n s o r}(\\mathbf{v})\\otimes\\mathsf{T e n s o r}(\\mathbf{r})\\rangle=c_{k}(\\mathsf{V e c}(\\mathbf{w}^{*\\otimes l})\\cdot\\mathbf{v})(\\mathsf{V e c}(\\mathbf{w}^{*\\otimes k-l})\\cdot\\mathbf{r}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Next, for the second term, after applying Cauchy-Schwarz inequality, one can show that it holds ", "page_idx": 5}, {"type": "equation", "text": "$$\n|\\langle\\mathbf{H}_{k},\\mathsf{T e n s o r}(\\mathbf{v})\\otimes\\mathsf{T e n s o r}(\\mathbf{r})\\rangle|\\leq\\sqrt{\\mathrm{OPT}}\\|\\mathrm{Sym}(\\mathsf{T e n s o r}(\\mathbf{v})\\otimes\\mathsf{T e n s o r}(\\mathbf{r}))\\|_{F}\\leq\\sqrt{\\mathrm{OPT}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Combining Equation (2) and Equation (3), we get that the singular value of $\\mathbf{v}$ must satisfy ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\rho(\\mathbf v)\\leq\\underset{\\mathbf r\\in\\mathbb R^{d^{k-l}},\\|\\mathbf r\\|_{2}=1}{\\operatorname*{max}}c_{k}(\\mathsf{V e c}(\\mathbf w^{*\\otimes l})\\cdot\\mathbf v)(\\mathsf{V e c}(\\mathbf w^{*\\otimes k-l})\\cdot\\mathbf r)+\\sqrt{\\mathrm{OPT}}}\\\\ &{\\qquad=c_{k}(\\mathsf{V e c}(\\mathbf w^{*\\otimes l})\\cdot\\mathbf v)+\\sqrt{\\mathrm{OPT}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where in the equation above, we used the observation that as $\\|\\mathsf{V e c}(\\mathbf{w}^{*\\otimes k-l})\\|_{2}=\\|\\mathbf{w}^{*\\otimes k-l}\\|_{F}=1$ , it holds $\\begin{array}{r}{\\operatorname*{max}_{\\mathbf{r}\\in\\mathbb{R}^{d^{k-l}},\\|\\mathbf{r}\\|_{2}=1}(\\mathsf{V e c}(\\mathbf{w}^{*\\otimes k-l})\\cdot\\mathbf{r})=\\|\\mathsf{V e c}(\\mathbf{w}^{*\\otimes k-l})\\|_{2}=1}\\end{array}$ . Since Equation (3) i\u221amplies $\\langle\\mathbf{H}_{k},\\mathsf{T e n s o r}(\\mathbf{v})\\otimes\\mathsf{T e n s o r}(\\mathbf{r})\\rangle\\geq-\\sqrt{\\mathrm{OPT}}$ , similarly we have $\\rho(\\mathbf{v})\\geq c_{k}(\\mathsf{V e c}(\\mathbf{w}^{*\\otimes l})\\cdot\\mathbf{v})-\\sqrt{\\mathrm{OPT}},$ , completing the proof of Lemma 2.2. \u53e3 ", "page_idx": 5}, {"type": "text", "text": "Lemma 2.2 implies that the top left singular vector $\\mathbf{v}^{*}$ aligns well with $\\mathsf{V e c}(\\mathbf{w}^{*\\otimes l})$ . The full version of Corollary 2.3 is deferred to Corollary D.5. ", "page_idx": 6}, {"type": "text", "text": "Corollary 2.3. The top left\u221a singular vector $\\mathbf{v}^{\\ast}\\in\\mathbb{R}^{d^{l}}$ of the unfolded tensor $\\mathsf{M a t}_{(l,k-l)}\\big(\\mathbf{C}_{k}\\big)$ satisfies $\\mathbf{v}^{*}\\cdot\\mathsf{V e c}(\\mathbf{w}^{*\\otimes l})\\geq1-(2\\sqrt{\\mathrm{OPT}})/c_{k}$ . ", "page_idx": 6}, {"type": "text", "text": "Concentration of the Unfolded Tensor Matrix Let us denote $\\mathbf{M}^{(i)}=\\mathsf{M a t}_{(l,k-l)}(y^{(i)}\\mathbf{He}_{k}(\\mathbf{x}^{(i)}))$ , for $i\\in[n]$ and $\\begin{array}{r}{\\widehat{\\mathbf{M}}=\\frac{1}{n}\\sum_{i=1}^{n}\\mathbf{M}^{(i)}}\\end{array}$ , which is the empirical approximation of $\\mathbf{M}=\\mathsf{M a t}_{(l,k-l)}(\\mathbf{C}_{k})=$ $\\mathsf{M a t}_{(l,k-l)}\\big(\\mathbf{E}_{(\\mathbf{x},y)\\sim\\mathcal{D}}\\big[y\\mathbf{H}\\mathbf{e}_{k}(\\mathbf{x})\\big]\\big)$ . Though we showed in Corollary 2.3 that the top left singular vector $\\mathbf{v}^{*}$ of the population M strongly correlates with the signal $\\mathsf{V e c}(\\mathbf{w}^{*\\otimes l})$ , we only have access to the empirical estimate M and its corresponding top left singular vector $\\widehat{\\mathbf{v}}^{*}$ . Thus, to guarantee that $\\widehat{\\mathbf{v}}^{*}$ correlates significa ntly with $\\mathsf{V e c}(\\mathbf{w}^{*\\stackrel{\\cdot}{\\otimes}l})$ as well, we need to show tha t the angle between the $\\mathbf{v}^{*}$ a nd $\\widehat{\\mathbf{v}}^{*}$ is sufficiently small as long as we use sufficiently many samples. To this aim, we apply Wedin\u2019s theorem (Fact D.6). Wedin\u2019s theorem states that $\\sin\\!{\\big(}\\theta({\\mathbf{v}^{*}},{\\widehat{\\mathbf{v}}^{*}}){\\big)}$ can be bounded above by: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\sin(\\theta(\\mathbf{v}^{*},\\widehat{\\mathbf{v}}^{*}))\\leq\\|\\mathbf{M}-\\widehat{\\mathbf{M}}\\|_{2}/(\\rho_{1}-\\rho_{2}-\\|\\mathbf{M}-\\widehat{\\mathbf{M}}\\|_{2}),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\rho_{1}$ and $\\rho_{2}$ are the top 2 singular values of $\\mathbf{M}$ . We prove in Claim D.7 that $\\rho_{1}\\mathrm{~-~}\\rho_{2}\\mathrm{~\\geq~}$ $(c_{k}-8\\sqrt{\\mathrm{OPT}})/2\\gtrsim c_{k}$ under the assumption that $\\sqrt{\\mathrm{OPT}}\\lesssim c_{k}$ , hence $\\rho_{1}-\\rho_{2}$ is bounded below by a constant. Thus, our remaining goal is to bound the operator norm such that $\\|\\mathbf{M}-\\widehat{\\mathbf{M}}\\|_{2}\\le\\epsilon_{0}$ where $\\epsilon_{0}>0$ is a small constant. This can be accomplished by applying a recently obtained matrix concentration inequality from [BvH22, DPVLB24] (stated in Fact D.8), with additional technical arguments. Plugging the lower bound on the singular gap $\\rho_{1}-\\rho_{2}$ and the upper bound on the operator norm $\\lVert\\mathbf{M}-\\widehat{\\mathbf{M}}\\rVert_{2}$ back into Wedin\u2019s theorem (Fact D.6), we obtain the following main technical lemma of this subsection, whose proof can be found in Appendix D: ", "page_idx": 6}, {"type": "text", "text": "Lemma 2.4 (Sample Complexity for Estimating the Unfolded Tensor Matrix). Let $\\epsilon,\\epsilon_{0}>0$ . Consider the unfolded matrix $\\mathbf{M}=\\mathsf{M a t}_{(l,k-l)}\\bigl(\\mathbf{E}_{(\\mathbf{x},y)\\sim\\mathcal{D}}\\bigl[y\\mathbf{H}\\mathbf{e}_{k}(\\mathbf{x})\\bigr]\\bigr)$ and its empirical estimate $\\widehat{\\mathbf{M}}:=$ $\\begin{array}{r}{(1/n)\\sum_{i=1}^{n}\\mathsf{M a t}_{(l,k-l)}(y^{(i)}\\mathbf{H}\\mathbf{e}_{k}(\\mathbf{x}^{(i)}))}\\end{array}$ , where $\\{(\\mathbf{x}^{(i)},y^{(i)})\\}_{i=1}^{n}$ are $n=\\Theta(e^{k}\\mathrm{log}^{k}(B_{4}/\\epsilon)d^{k/2}/\\epsilon_{0}^{2}+$ $1/\\epsilon)$ i.i.d. samples from $\\mathcal{D}$ . Then, with probability at least $1-\\exp(-d^{1/2})$ , $\\|\\widehat{\\mathbf{M}}-\\mathbf{M}\\|_{2}\\,\\leq\\,\\epsilon_{0}$ . Moreover, $i f\\widehat{\\mathbf{v}}^{*}$ is the top left singular vector of $\\widehat{\\bf M}$ , then with probability at least $1-\\exp(-d^{1/2})$ , ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\widehat{\\mathbf{v}}^{*}\\cdot\\mathsf{V e c}(\\mathbf{w}^{*\\otimes l})\\geq1-\\frac{2}{c_{k}}\\sqrt{\\mathsf{O P T}}-\\frac{2\\epsilon_{0}}{(c_{k}/2-4\\sqrt{\\mathsf{O P T}})-\\epsilon_{0}}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "After getting an approximate top left singular vector $\\widehat{\\mathbf{v}}^{*}\\in\\mathbb{R}^{d}$ of $\\mathsf{M a t}_{(l,k-l)}\\bigl(\\mathbf{E}_{(\\mathbf{x},y)\\sim\\mathcal{D}}\\bigl[y\\mathbf{H}\\mathbf{e}_{k}(\\mathbf{x})\\bigr]\\bigr)$ , the final piece of the argument is that finding the top left singular vector of the matrix $\\mathsf{M a t}_{(1,l-1)}(\\widehat{\\mathbf{v}}^{*})$ completes the task of computing a vector u that correlates strongly with $\\mathbf{w}^{*}$ . Concretely, we have: ", "page_idx": 6}, {"type": "text", "text": "Lemma 2.5. Suppose that $\\widehat{\\mathbf{v}}^{*}\\cdot\\mathsf{V e c}(\\mathbf{w}^{*\\otimes l})\\geq1-\\epsilon_{1}$ for some $\\epsilon_{1}\\,\\in\\,(0,1/16]$ . Then, the top left singular vector $\\mathbf u\\in\\mathbb R$ of $\\mathsf{M a t}_{(1,l-1)}(\\widehat{\\mathbf{v}}^{*})$ satisfies $\\mathbf{u}\\cdot\\mathbf{w}^{*}\\geq1-2\\epsilon_{1}$ . ", "page_idx": 6}, {"type": "text", "text": "Proof of Proposition 2.1 Since $\\sqrt{\\mathrm{OPT}}\\le c_{k^{*}}/(64k^{*})\\le c_{k^{*}}/64$ , choosing $\\epsilon_{0}={c_{k^{*}}}/{(256k^{*})}\\leq$ $c_{k^{*}}/256$ in Lemma 2.4, we obtain that using $n=\\Theta((k^{*})^{2}e^{k^{*}}\\log^{k^{*}}(B_{4}/\\epsilon)d^{[k^{*}/2]}/(c_{k^{*}}^{2})+1/\\epsilon)$ , it holds with probability at least $1-\\exp(-d^{1/2})$ that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\widehat{\\mathbf{v}}^{*}\\cdot\\mathsf{V e c}(\\mathbf{w}^{*\\otimes l})\\ge1-\\frac{2}{c_{k}}\\sqrt{\\mathrm{OPT}}-\\frac{2\\epsilon_{0}}{(c_{k}/2-4\\sqrt{\\mathrm{OPT}})-\\epsilon_{0}}\\ge1-\\frac{1}{16k^{*}}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Then applying Lemma 2.5 with $\\epsilon_{1}\\leq1/(16k^{*})\\leq1/16$ , we get that the output $\\mathbf{u}$ of Algorithm 1 satisfies $\\mathbf{u}\\cdot\\mathbf{w}^{*}\\geq1-2\\epsilon_{1}\\geq1-1/(8k^{*})\\geq1-\\operatorname*{min}\\{1/k^{*},1/2\\}$ , completing the proof. \u53e3 ", "page_idx": 6}, {"type": "text", "text": "3 Optimization via Riemannian Gradient Descent ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "After obtaining $\\mathbf{w}^{0}$ from Algorithm 1, we run Riemannian minibatch SGD Algorithm 2 on the \u2018truncated loss\u2019 (see definition in Equation (5)). In the rest of the section, we first present the definition of the truncated $L_{2}^{2}$ loss $\\mathcal{L}_{2}^{\\phi}$ and its Riemannian gradient and then proceed to proving that Algorithm 2 converges to a constant approximate solution in ${\\cal O}(\\log(1/\\epsilon))$ iterations. Due to space constraints, omitted proofs are provided in Appendix E. ", "page_idx": 6}, {"type": "text", "text": "Algorithm 2 Riemannian GD with Warm-start ", "page_idx": 7}, {"type": "text", "text": "1: Input: Parameters $\\epsilon$ , $k^{*},c_{k^{*}},B_{4}>0;T,\\eta$ ; Sample access to $\\mathcal{D}$ .   \n2: $\\boldsymbol{\\kappa}^{0}=\\mathbf{Initialization}[\\epsilon,k^{*},c_{k^{*}},B_{4},\\epsilon_{0}=c_{k^{*}}/(256k^{*})]$ (Algorithm 1).   \n3: for $t=0,\\dots,T-1$ do   \n4: Draw $n=\\Theta(C_{k^{*}}d e^{k^{*}}\\log^{k^{*}+1}(B_{4}/\\epsilon)/(\\epsilon\\delta))$ samples from $\\mathcal{D}$   \n5: $\\begin{array}{r}{\\widehat{\\mathbf{g}}(\\mathbf{w}^{t})=\\frac{1}{n}\\sum_{i=1}^{n}k^{*}c_{k^{*}}y^{(i)}(\\mathbf{I}-\\mathbf{w}^{t}(\\mathbf{w}^{t})^{\\top})\\langle\\mathbf{H}\\mathbf e_{k^{*}}(\\mathbf{x}^{(i)}),(\\mathbf{w}^{t})^{\\otimes k^{*}-1}\\rangle.}\\end{array}$   \n6: $\\mathbf{w}^{t+1}=\\big(\\mathbf{w}^{t}-\\eta\\widehat{\\mathbf{g}}(\\mathbf{w}^{t})\\big)/\\|\\mathbf{w}^{t}-\\eta\\widehat{\\mathbf{g}}(\\mathbf{w}^{t})\\|_{2}$ .   \n7: Return: wT ", "page_idx": 7}, {"type": "text", "text": "3.1 Truncated Loss and the Sharpness property of the Riemannian Gradient ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Instead of directly minimizing the $L_{2}^{2}$ loss $\\mathcal{L}_{2}^{\\sigma}$ , we work with the following truncated loss that drops all the terms higher than $k^{*}$ in the polynomial expansion of $\\sigma$ : ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{2}^{\\phi}(\\mathbf{w}):=2\\big(1-\\underset{(\\mathbf{x},y)\\sim\\mathcal{D}}{\\mathbf{E}}[y\\phi(\\mathbf{w}\\cdot\\mathbf{x})]\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Similarly, the noiseless surrogate loss is defined as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathcal{L}_{2}^{*\\phi}(\\mathbf{w}):=2\\bigl(1-\\underset{(\\mathbf{x},y)\\sim\\mathcal{D}}{\\mathbf{E}}[\\sigma(\\mathbf{w}^{*}\\cdot\\mathbf{x})\\phi(\\mathbf{w}\\cdot\\mathbf{x})]\\bigr)=2\\bigl(1-c_{k^{*}}(\\mathbf{w}\\cdot\\mathbf{w}^{*})^{k^{*}}\\bigr).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "It can be shown (using Fact C.1(2)) that the gradient of the truncated $L_{2}^{2}$ loss equals: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\nabla\\mathcal{L}_{2}^{\\phi}(\\mathbf{w})=-2\\underset{(\\mathbf{x},y)\\sim\\mathcal{D}}{\\mathbf{E}}[\\nabla\\phi(\\mathbf{w}\\cdot\\mathbf{x})y]=-2\\underset{(\\mathbf{x},y)\\sim\\mathcal{D}}{\\mathbf{E}}\\left[k^{*}c_{k^{*}}y\\langle\\mathbf{H}\\mathbf{e}_{k^{*}}(\\mathbf{x}),\\mathbf{w}^{\\otimes k^{*}-1}\\rangle\\right],\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "while for the gradient of the noiseless $L_{2}^{2}$ loss we have ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nabla\\mathcal{L}_{2}^{*\\phi}(\\mathbf{w})=-2\\underset{(\\mathbf{x},y)\\sim\\mathcal{D}}{\\mathbf{E}}\\left[k^{*}c_{k^{*}}\\sigma(\\mathbf{w}^{*}\\cdot\\mathbf{x})\\langle\\mathbf{H}\\mathbf{e}_{k^{*}}(\\mathbf{x}),\\mathbf{w}^{\\otimes k^{*}-1}\\rangle\\right]\\mathrm{.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Recall that $\\mathbf{P}_{\\mathbf{w}^{\\perp}}:=\\mathbf{I}-\\mathbf{w}\\mathbf{w}^{\\top}$ . Then the Riemannian gradient of the $L_{2}^{2}$ loss $\\mathcal{L}_{2}^{\\phi}$ , denoted by $\\mathbf{g}(\\mathbf{w})$ is ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbf{g}(\\mathbf{w}):=\\mathbf{P}_{\\mathbf{w}^{\\perp}}(\\nabla\\mathcal{L}_{2}^{\\phi}(\\mathbf{w}))=-2\\sum_{(\\mathbf{x},\\mathbf{y})\\sim\\mathcal{D}}\\left[k^{*}y\\mathbf{P_{w^{\\perp}}}\\langle\\mathbf{H}\\mathbf{e}_{k^{*}}(\\mathbf{x}),\\mathbf{w}^{\\otimes k^{*}-1}\\rangle\\right].\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Similarly, the Riemannian gradient of the noiseless $L_{2}^{2}$ loss $\\boldsymbol{\\mathcal{L}}_{2}^{*\\phi}$ is defined by ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{g}^{*}(\\mathbf{w}):=\\mathbf{P}_{\\mathbf{w}^{\\perp}}(\\nabla\\mathcal{L}_{2}^{*\\phi}(\\mathbf{w}))=-2\\underset{(\\mathbf{x},y)\\sim\\mathcal{D}}{\\mathbf{E}}\\left[k^{*}\\sigma(\\mathbf{w}^{*}\\cdot\\mathbf{x})\\mathbf{P_{w^{\\perp}}}\\langle\\mathbf{H}\\mathbf{e}_{k^{*}}(\\mathbf{x}),\\mathbf{w}^{\\otimes k^{*}-1}\\rangle\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "We first show that $\\mathbf{g}^{\\ast}(\\mathbf{w})$ carries information about the alignment between vectors w and $\\mathbf{w}^{*}$ ", "page_idx": 7}, {"type": "text", "text": "Claim 3.1. For any $\\mathbf{w}\\in\\mathbb{S}^{d-1}$ , we have $\\begin{array}{r}{\\mathbf{g}^{*}(\\mathbf{w})=-2k^{*}c_{k^{*}}(\\mathbf{w}\\cdot\\mathbf{w}^{*})^{k^{*}-1}(\\mathbf{w}^{*})^{\\perp_{\\mathbf{w}}}.}\\end{array}$ ", "page_idx": 7}, {"type": "text", "text": "Let us denote the difference between the noisy and the noiseless Riemannian gradient by $\\xi(\\mathbf{w})$ , i.e., $\\begin{array}{r}{\\xi(\\mathbf{w}):=\\mathbf{g}(\\mathbf{w})-\\mathbf{g}^{*}(\\mathbf{w})=-2\\,\\mathbf{E}_{(\\mathbf{x},\\mathbf{y})\\sim\\mathcal{D}}\\big[(y-\\sigma(\\mathbf{w}^{*}\\cdot\\mathbf{x}))\\mathbf{P}_{\\mathbf{w}^{\\perp}}\\nabla\\phi(\\mathbf{w}\\cdot\\mathbf{x})\\big].}\\end{array}$ We next show that the norm of $\\xi(\\mathbf{w})$ and the inner product between $\\xi(\\mathbf{w})$ and $\\mathbf{w}^{*}$ are both bounded. ", "page_idx": 7}, {"type": "text", "text": "Lemma 3.2. Let $\\xi(\\mathbf{w})=\\mathbf{g}(\\mathbf{w})-\\mathbf{g}^{*}(\\mathbf{w})$ as defined above. Then, $\\|\\xi(\\mathbf{w})\\|_{2}\\leq2k^{*}c_{k^{*}}\\sqrt{\\mathrm{OPT}}$ and $|\\xi(\\mathbf{w})\\cdot\\mathbf{w}^{*}|\\leq2k^{*}c_{k^{*}}\\sqrt{\\mathrm{OPT}}\\|(\\mathbf{w}^{*})^{\\perp_{\\mathbf{w}}}\\|_{2}$ . ", "page_idx": 7}, {"type": "text", "text": "We are now ready to present the main structural result of this section. ", "page_idx": 7}, {"type": "text", "text": "Lemma 3.3 (Sharpness). Assume $\\mathrm{OPT}\\,\\leq\\,c/(4e)^{2}$ for some small absolute constant $c<1$ . Let $\\mathbf{w}\\in\\mathbb{S}^{d-1}$ and suppose that $\\mathbf{w}\\cdot\\mathbf{w}^{*}\\geq1-1/k^{*}$ . Let $\\boldsymbol{\\theta}:=\\boldsymbol{\\theta}(\\mathbf{w},\\mathbf{w}^{*})$ . $I\\!f\\sin\\theta\\geq4e{\\sqrt{\\mathrm{OPT}}}$ , then $\\mathbf{g}(\\mathbf{w})\\cdot\\mathbf{w}^{*}\\leq-(1/2)\\|\\mathbf{g}^{*}(\\mathbf{w})\\|_{2}\\sin\\theta$ . ", "page_idx": 7}, {"type": "text", "text": "Proof. We start by noticing that by Claim 3.1, the noiseless gradient satisfies the following property: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbf{g}^{*}(\\mathbf{w})\\cdot\\mathbf{w}^{*}=-2k^{*}c_{k^{*}}(\\mathbf{w}\\cdot\\mathbf{w}^{*})^{k^{*}-1}\\lVert(\\mathbf{w}^{*})^{\\perp_{\\mathbf{w}}}\\rVert_{2}^{2}=-\\lVert\\mathbf{g}^{*}(\\mathbf{w})\\rVert_{2}\\sin\\theta,\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where we used that since $\\|\\mathbf{w}\\|_{2}=\\|\\mathbf{w}^{*}\\|_{2}=1$ , we have $\\|(\\mathbf{w}^{*})^{\\perp_{\\mathbf{w}}}\\|_{2}=\\sin\\theta_{*}$ . Furthermore, applying Lemma 3.2 we have the following sharpness property with respect to the $L_{2}^{2}$ loss: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{g}(\\mathbf{w})\\cdot\\mathbf{w}^{*}=\\mathbf{g}^{*}(\\mathbf{w})\\cdot\\mathbf{w}^{*}+\\xi(\\mathbf{w})\\cdot\\mathbf{w}^{*}\\le-(\\|\\mathbf{g}^{*}(\\mathbf{w})\\|_{2}-2k^{*}c_{k^{*}}\\sqrt{\\mathrm{OPT}})\\sin\\theta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Observe th ", "page_idx": 7}, {"type": "text", "text": "$(1-1/t)^{t-1}\\geq1/e$ for all $t\\geq1$ . Therefore, when $\\mathbf{w}\\cdot\\mathbf{w}^{*}\\geq1-1/k^{*}$ , we have ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\mathbf{g}^{*}(\\mathbf{w})\\|_{2}=2k^{*}c_{k^{*}}\\big(\\mathbf{w}\\cdot\\mathbf{w}^{*}\\big)^{k^{*}-1}\\sin\\theta\\geq2k^{*}c_{k^{*}}\\big(1-1/k^{*}\\big)^{k^{*}-1}\\sin\\theta\\geq e^{-1}k^{*}c_{k^{*}}\\sin\\theta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Hence, when $\\sin\\theta\\geq4e{\\sqrt{\\mathrm{{OPT}}}}$ and $\\mathbf{w}\\cdot\\mathbf{w}^{*}\\geq1-1/k^{*}$ , we have $\\|\\mathbf{g}^{*}(\\mathbf{w})\\|_{2}\\,\\geq\\,4k^{*}c_{k^{*}}\\sqrt{\\mathrm{OPT}}$ . Thus, as long as $\\sin\\theta\\geq4e{\\sqrt{\\mathrm{{OPT}}}}$ , we have that $\\begin{array}{r}{\\mathbf{g}(\\mathbf{w})\\cdot\\mathbf{w}^{*}\\leq-\\frac{1}{2}\\|\\mathbf{g}^{*}(\\mathbf{w})\\|_{2}\\sin\\theta}\\end{array}$ . \u53e3 ", "page_idx": 7}, {"type": "text", "text": "3.2 Concentration of Gradients ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Define the empirical estimate of $\\mathbf{g}(\\mathbf{w})$ as $\\begin{array}{r}{\\widehat{\\mathbf{g}}(\\mathbf{w}):=(1/n)\\sum_{i=1}^{n}k^{*}c_{k^{*}}y^{(i)}\\mathbf{P_{w^{\\perp}}}\\langle\\mathbf{H}\\mathbf e_{k^{*}}(\\mathbf x^{(i)}),\\mathbf w^{\\otimes k^{*}-1}\\rangle}\\end{array}$ . The following lemma provides the uppe r  bounds on the  number of samples required to approximate the Riemannian gradient $\\mathbf{g}(\\mathbf{w})$ by $\\widehat{\\bf g}({\\bf w})$ . ", "page_idx": 8}, {"type": "text", "text": "Lemma 3.4 (Concentration of Gradients). Let w\u2217, $\\mathbf{w}\\in\\mathbb{S}^{d-1}$ . Let $\\widehat{\\bf g}({\\bf w})$ be the empirical estimate of the Riemannian gradient. Furthermore, denote the angle between w and $\\mathbf{w}^{*}$ by $\\theta$ , and denote $\\kappa=(k^{*}c_{k^{*}})^{2}e^{k^{*}}\\log^{k^{*}}(B_{4}/\\epsilon)$ . Then, with probability at least $1-\\delta$ , it holds $\\|\\widehat{\\mathbf{g}}(\\mathbf{w})-\\mathbf{g}(\\mathbf{w})\\|_{2}\\lesssim$ $\\sqrt{d\\kappa/(n\\delta)}$ , and $({\\widehat{\\mathbf{g}}}(\\mathbf{w})-\\mathbf{g}(\\mathbf{w}))\\cdot\\mathbf{w}^{*}\\lesssim{\\sqrt{\\kappa/(n\\delta)}}\\sin^{2}\\theta.$ . ", "page_idx": 8}, {"type": "text", "text": "3.3 Proof of Main Theorem ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We proceed to the main theorem of this paper. It shows that using at most $\\tilde{\\Theta}(d^{[k^{*}/2]}\\,+\\,d/\\epsilon)$ samples, Algorithm 2 (with initialization from Algorithm 1) generates a vector $\\widehat{\\bf w}$ such that $\\mathcal{L}_{2}^{\\sigma}(\\widehat{\\mathbf{w}})=$ $O(\\mathrm{OPT})+\\epsilon$ within ${\\cal O}(\\log(1/\\epsilon))$ iterations. ", "page_idx": 8}, {"type": "text", "text": "Theorem 3.5. Suppose Assumption $^{\\,l}$ holds. Choose the batch size of Algorithm 2 to be $n\\,=\\,\\Theta(C_{k^{*}}d e^{k^{*}}\\,\\bar{\\log^{k^{*}+1}(B_{4}/\\epsilon)}/(\\epsilon\\delta))$ , and choose the step size $\\eta\\;=\\;9/\\big(40e k^{*}c_{k^{*}}\\big)$ . Then, after $T~=~{\\cal O}(\\log(C_{k^{*}}/\\epsilon))$ iterations, with probability at least $1\\,-\\,\\delta$ , Algorithm 2 outputs $\\mathbf{w}^{\\check{T}}$ with $\\mathcal{L}_{2}^{\\sigma}(\\mathbf{w}^{T})\\;\\;=\\;\\;O(C_{k^{*}}\\mathrm{OPT})\\;+\\;\\epsilon$ . The total sample complexity of Algorithm 2 is ${\\cal N}\\quad=$ $\\begin{array}{r}{\\Theta(({k^{*}}/{c_{k^{*}}})^{2}e^{k^{*}}\\log^{k^{*}}(B_{4}/\\epsilon)d^{\\lceil k^{*}/2\\rceil}+(C_{k^{*}}e^{k^{*}}\\log^{k^{*}+2}(B_{4}/\\epsilon))\\frac{d}{\\epsilon\\delta})}\\end{array}$ . ", "page_idx": 8}, {"type": "text", "text": "Proof Sketch of Theorem 3.5. Suppose first that $\\mathrm{OPT}\\geq(c_{k^{*}}/64k^{*})^{2}$ , then by Claim E.7 we know that any unit vector (e.g., $\\widehat{\\mathbf{w}}=\\mathbf{e}_{1}\\,,$ ) is a constant approximate solution with $\\mathcal{L}_{2}^{\\sigma}(\\widehat{\\mathbf{w}})=O(\\mathrm{OPT})$ . Now suppose that $\\mathrm{OPT}\\leq(c_{k^{*}}/64k^{*})^{2}$ . Consider the distance between $\\mathbf{w}^{t}$ and $\\mathbf{w}^{*}$ after each update of Algorithm 2. By the non-expansive property of projection operators, we have ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\mathbf{w}^{t+1}-\\mathbf{w}^{*}\\|_{2}^{2}=\\|\\mathrm{proj}_{\\mathbb{B}_{d}}(\\mathbf{\\bar{w}}^{t}-\\eta\\widehat{\\mathbf{g}}(\\mathbf{w}^{t}))-\\mathbf{w}^{*}\\|_{2}^{2}\\leq\\|\\mathbf{w}^{t}-\\eta\\widehat{\\mathbf{g}}(\\mathbf{w}^{t})-\\mathbf{w}^{*}\\|_{2}^{2}}\\\\ {=\\|\\mathbf{w}^{t}-\\mathbf{w}^{*}\\|_{2}^{2}+2\\eta\\widehat{\\mathbf{g}}(\\mathbf{w}^{t})\\cdot(\\mathbf{w}^{*}-\\mathbf{w}^{t})+\\eta^{2}\\|\\widehat{\\mathbf{g}}(\\mathbf{w}^{t})\\|_{2}^{2}.\\quad\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Let $\\theta_{t}=\\theta(\\mathbf{w}^{t},\\mathbf{w}^{*})$ . For the chosen batch size, Lemma 3.4 implies ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\|\\widehat{\\mathbf{g}}(\\mathbf{w}^{t})-\\mathbf{g}(\\mathbf{w}^{t})\\|_{2}^{2}\\leq(k^{*}c_{k^{*}})^{2}\\epsilon,\\quad(\\widehat{\\mathbf{g}}(\\mathbf{w}^{t})-\\mathbf{g}(\\mathbf{w}^{t}))\\cdot\\mathbf{w}^{*}\\leq\\sqrt{\\epsilon/d}\\sin^{2}\\theta_{t}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Assume for now that $\\sin\\theta_{t}\\geq4e{\\sqrt{\\mathrm{OPT}}}+{\\sqrt{\\epsilon}}$ , hence Lemma 3.3 applies. As $\\widehat{\\mathbf{g}}(\\mathbf{w}^{t})\\cdot(\\mathbf{w}^{*}-\\mathbf{w}^{t})=$ $(\\widehat{\\mathbf{g}}(\\mathbf{w}^{t})-\\mathbf{g}(\\mathbf{w}^{t}))\\cdot\\mathbf{w}^{*}+\\mathbf{g}(\\mathbf{w}^{t})\\cdot\\mathbf{w}^{*}$ , using Lemma 3.3, we get ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\widehat{\\mathbf{g}}(\\mathbf{w}^{t})\\cdot(\\mathbf{w}^{*}-\\mathbf{w}^{t})\\leq\\sqrt{\\epsilon/d}\\sin\\theta_{t}-(1/2)\\|\\mathbf{g}^{*}(\\mathbf{w}^{*})\\|_{2}\\sin\\theta_{t}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "On the other hand, the squared norm term $\\|\\widehat{\\mathbf{g}}(\\mathbf{w}^{t})\\|_{2}^{2}$ from Equation (12) can be bounded above by ", "page_idx": 8}, {"type": "text", "text": "$\\begin{array}{r}{||\\widehat{\\mathbf{g}}(\\mathbf{w}^{t})||_{2}^{2}\\leq2||\\widehat{\\mathbf{g}}(\\mathbf{w}^{t})-\\mathbf{g}(\\mathbf{w}^{t})||_{2}^{2}+2||\\mathbf{g}(\\mathbf{w}^{t})||_{2}^{2}\\leq(k^{*}c_{k^{*}})^{2}(\\mathrm{OPT}+\\epsilon)+||\\mathbf{g}^{*}(\\mathbf{w})||_{2}^{2}.}\\end{array}$ (14) Plugging Equation (13) and Equation (14) back into Equation (12), we get that w.p. at least $1-\\delta$ , ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\mathbf{w}^{t+1}-\\mathbf{w}^{*}\\|_{2}^{2}\\leq\\|\\mathbf{w}^{t}-\\mathbf{w}^{*}\\|_{2}^{2}+2\\eta(\\sqrt{\\epsilon/d}-\\|\\mathbf{g}^{*}(\\mathbf{w}^{t})\\|_{2}/2)\\sin\\theta_{t}}\\\\ {+\\;\\eta^{2}((k^{*}c_{k^{*}})^{2}(\\mathrm{OPT}+\\epsilon)+\\|\\mathbf{g}^{*}(\\mathbf{w})\\|_{2}^{2}).\\quad\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Let us assume first that $\\theta_{t}\\leq\\theta_{t-1}\\leq\\dots\\leq\\theta_{0}$ and $\\sin\\theta_{t}\\geq4e{\\sqrt{\\mathrm{OPT}}}+{\\sqrt{\\epsilon}}$ , then we show that it holds $\\theta_{t+1}\\leq\\theta_{t}$ . Recall in Claim 3.1 it was shown that $\\|\\mathbf{g}^{*}(\\mathbf{w}^{t})\\|_{2}=2k^{*}c_{k^{*}}(\\mathbf{w}^{t}\\cdot\\mathbf{w}^{*})^{k^{*}-1}\\sin\\theta_{t}$ . Since $\\mathbf{w}^{0}$ is the initial parameter vector that satisfies $\\mathbf{w}^{0}\\cdot\\mathbf{w}^{*}\\geq1-1/k^{*}$ , by the inductive hypothesis it holds $\\mathbf{w}^{t}\\cdot\\mathbf{w}^{*}\\geq1-1/k^{*}$ and hence $1\\geq(\\mathbf{w}^{t}\\cdot\\mathbf{w}^{*})^{k^{*}-1}\\geq1/e$ . Therefore, we further obtain $(2k^{*}c_{k^{*}}/e)\\sin\\theta_{t}\\,\\le\\,\\|\\mathbf{g}^{*}(\\mathbf{w}^{t})\\|_{2}\\,\\le\\,2k^{*}c_{k^{*}}\\sin\\theta_{t}$ . Therefore, using the inductive assumption that $\\sin{\\theta_{t}}\\,\\geq\\,4e{\\sqrt{\\mathrm{{OPT}}}}+{\\sqrt{\\epsilon}}$ and further noticing that $\\|\\mathbf{w}^{t}\\-\\mathbf{w}^{*}\\|_{2}\\;=\\;2\\sin(\\theta_{t}/2)$ , with step size $\\eta=9/(40e k^{*}c_{k^{*}})$ we can further bound $\\|\\mathbf{w}^{t+\\widetilde{1}}-\\mathbf{w}^{*}\\|_{2}^{2}$ in Equation (15) as: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\|\\mathbf{w}^{t+1}-\\mathbf{w}^{*}\\|_{2}^{2}\\leq(1-(81/(320e^{2})))\\|\\mathbf{w}^{t}-\\mathbf{w}^{*}\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "This shows that $\\theta_{t+1}\\leq\\theta_{t}$ , hence completing the inductive argument. Furthermore, E\u221aquation (16) implies that after at most $T=O(\\log(1/\\epsilon))$ iterations it must hold that $\\sin\\theta_{T}\\leq4e\\sqrt{\\mathrm{OPT}}+\\sqrt{\\epsilon}$ , therefore, we can end the algorithm after at most ${\\cal O}(\\log(1/\\epsilon))$ iterations. Applying Claim E.7 we know that this final vector $\\mathbf{w}^{T}$ has error bound $\\mathcal{L}_{2}^{\\sigma}(\\mathbf{w}^{\\bar{T}})=\\bar{O}(C_{k^{*}}(\\mathrm{OPT}\\bar{+\\epsilon}\\bar{\\epsilon}))$ . Thus, choosing $\\epsilon^{\\prime}=C_{k^{*}}\\epsilon$ , $\\delta^{\\prime}=\\delta T$ , where $T=O(\\log(C_{k^{*}}/\\epsilon^{\\prime}))$ , Algorithm 2 outputs a parameter $\\mathbf{w}^{T}$ such that with probability at least $1-\\delta^{\\prime}$ , $\\mathcal{L}_{2}^{\\sigma}(\\mathbf{w}^{T})=O(C_{k^{*}}\\mathrm{OPT})+\\epsilon^{\\prime}$ , with batch size $n=\\tilde{\\Theta}(d/(\\epsilon^{\\prime}\\delta^{\\prime}))$ . In summary, the total number of samples required for Algorithm 2 is $N=\\tilde{\\Theta}(d^{[k^{*}/2]}+d/(\\epsilon^{\\prime}\\delta^{\\prime}))$ . ", "page_idx": 8}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "PW was supported in part by NSF Award DMS-2023239. NZ was supported in part by NSF Medium Award CCF-2107079. ID was supported in part by NSF Medium Award CCF-2107079 and an H.I. Romnes Faculty Fellowship. JD was supported in part by the Air Force Office of Scientific Research under award number FA9550-24-1-0076 and by the U.S. Office of Naval Research under contract number N00014-22-1-2348. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the U.S. Department of Defense. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[AAM23] E. Abbe, E. B. Adsera, and T. Misiakiewicz. Sgd learning on neural networks: leap complexity and saddle-to-saddle dynamics. In The Thirty Sixth Annual Conference on Learning Theory, pages 2552\u20132623. PMLR, 2023.   \n[ADGM17] A. Anandkumar, Y. Deng, R. Ge, and H. Mobahi. Homotopy analysis for tensor pca. In Conference on Learning Theory, pages 79\u2013104. PMLR, 2017. [AS68] M. Abramowitz and I. A. Stegun. Handbook of mathematical functions with formulas, graphs, and mathematical tables, volume 55. US Government printing office, 1968. [ATV23] P. Awasthi, A. Tang, and A. Vijayaraghavan. Agnostic learning of general ReLU activation using gradient descent. In The Eleventh International Conference on Learning Representations, ICLR, 2023. [BAGJ20] G. Ben Arous, R. Gheissari, and A. Jagannath. Algorithmic thresholds for tensor pca. The Annals of Probability, 48(4):2052\u20132087, 2020.   \n[BAGJ21] G. Ben Arous, R. Gheissari, and A. Jagannath. Online stochastic gradient descent on non-convex losses from high-dimensional inference. Journal of Machine Learning Research, 22(106):1\u201351, 2021. [BvH22] T. Brailovskaya and R. van Handel. Universality and sharp matrix concentration inequalities. arXiv preprint arXiv:2201.05142, 2022.   \n[CCFM19] Y. Chen, Y. Chi, J. Fan, and C. Ma. Gradient descent with random initialization: Fast global convergence for nonconvex phase retrieval. Mathematical Programming, 176:5\u201337, 2019. [CLS15] E. J. Candes, X. Li, and M. Soltanolkotabi. Phase retrieval via wirtinger flow: Theory and algorithms. IEEE Transactions on Information Theory, 61(4):1985\u20132007, 2015. [CM20] S. Chen and R. Meka. Learning polynomials in few relevant dimensions. In Conference on Learning Theory, pages 1161\u20131227. PMLR, 2020.   \n$[\\mathrm{DGK}^{+}20]$ I. Diakonikolas, S. Goel, S. Karmalkar, A. R. Klivans, and M. Soltanolkotabi. Approximation schemes for ReLU regression. In Conference on Learning Theory, COLT, volume 125 of Proceedings of Machine Learning Research, pages 1452\u20131485. PMLR, 2020. [DH18] R. Dudeja and D. Hsu. Learning single-index models in Gaussian space. In Conference on Learning Theory, COLT, volume 75 of Proceedings of Machine Learning Research, pages 1887\u20131930. PMLR, 2018. [DH21] R. Dudeja and D. Hsu. Statistical query lower bounds for tensor pca. Journal of Machine Learning Research, 22(83):1\u201351, 2021. [DJS08] A. S. Dalalyan, A. Juditsky, and V. Spokoiny. A new algorithm for estimating the effective dimension-reduction subspace. The Journal of Machine Learning Research, 9:1647\u20131678, 2008.   \n[DKPZ21] I. Diakonikolas, D. M. Kane, T. Pittas, and N. Zarifis. The optimality of polynomial regression for agnostic learning under Gaussian marginals in the SQ model. In Proceedings of The $3\\overline{{{\\mathcal{4}}}}^{t h}$ Conference on Learning Theory, COLT, 2021. [DKR23] I. Diakonikolas, D. M. Kane, and L. Ren. Near-optimal cryptographic hardness of agnostically learning halfspaces and ReLU regression under Gaussian marginals. In ICML, 2023.   \n[DKTZ22] I. Diakonikolas, V. Kontonis, C. Tzamos, and N. Zarifis. Learning a single neuron with adversarial label noise via gradient descent. In Conference on Learning Theory (COLT), pages 4313\u20134361, 2022. [DKZ20] I. Diakonikolas, D. M. Kane, and N. Zarifis. Near-optimal SQ lower bounds for agnostically learning halfspaces and ReLUs under Gaussian marginals. In Advances in Neural Information Processing Systems, NeurIPS, 2020. [DLS22] A. Damian, J. Lee, and M. Soltanolkotabi. Neural networks can learn representations with gradient descent. In Conference on Learning Theory, pages 5413\u20135452. PMLR, 2022.   \n[DNGL23] A. Damian, E. Nichani, R. Ge, and J. D. Lee. Smoothing the landscape boosts the signal for sgd: Optimal sample complexity for learning single index models. Advances in Neural Information Processing Systems, 36, 2023.   \n[DPVLB24] A. Damian, L. Pillaud-Vivien, J. D. Lee, and J. Bruna. The computational complexity of learning gaussian single-index models. arXiv preprint arXiv:2403.05529, 2024. [FCG20] S. Frei, Y. Cao, and Q. Gu. Agnostic learning of a single neuron with gradient descent. In Advances in Neural Information Processing Systems, NeurIPS, 2020.   \n[FFRS21] A. Fiorenza, M. R. Formica, T. G. Roskovec, and F. Soudsky\\`. Detailed proof of classical gagliardo\u2013nirenberg interpolation inequality with historical remarks. Zeitschrift f\u00fcr Analysis und ihre Anwendungen, 40(2):217\u2013236, 2021. [GGK20] S. Goel, A. Gollakota, and A. R. Klivans. Statistical-query lower bounds via functional gradients. In Advances in Neural Information Processing Systems, NeurIPS, 2020.   \n[GGKS23] A. Gollakota, P. Gopalan, A. R. Klivans, and K. Stavropoulos. Agnostically learning single-index models using omnipredictors. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. [Hau92] D. Haussler. Decision theoretic generalizations of the PAC model for neural net and other learning applications. Information and Computation, 100:78\u2013150, 1992. [HJS01] M. Hristache, A. Juditsky, and V. Spokoiny. Direct estimation of the index coefficient in a single-index model. Annals of Statistics, pages 595\u2013623, 2001.   \n$[\\mathrm{HMS^{+}04}]$ W. H\u00e4rdle, M. M\u00fcller, S. Sperlich, A. Werwatz, et al. Nonparametric and semiparametric models, volume 1. Springer, 2004. [HSS15] S. B. Hopkins, J. Shi, and D. Steurer. Tensor principal component analysis via sum-ofsquare proofs. In Conference on Learning Theory, pages 956\u20131006. PMLR, 2015.   \n[HSSS16] S. B. Hopkins, T. Schramm, J. Shi, and D. Steurer. Fast spectral algorithms from sumof-squares proofs: tensor decomposition and planted sparse vectors. In Proceedings of the forty-eighth annual ACM symposium on Theory of Computing, pages 178\u2013191, 2016. [Ich93] H. Ichimura. Semiparametric least squares (SLS) and weighted SLS estimation of single-index models. Journal of econometrics, 58(1-2):71\u2013120, 1993.   \n[KKSK11] S. M Kakade, V. Kanade, O. Shamir, and A. Kalai. Efficient learning of generalized linear and single index models with isotonic regression. Advances in Neural Information Processing Systems, 24, 2011. [KS09] A. T. Kalai and R. Sastry. The isotron algorithm: High-dimensional isotonic regression. In COLT, 2009. [KSS94] M. Kearns, R. Schapire, and L. Sellie. Toward efficient agnostic learning. Machine Learning, 17(2/3):115\u2013141, 1994.   \n[MBM18] S. Mei, Y. Bai, and A. Montanari. The landscape of empirical risk for nonconvex losses. The Annals of Statistics, 46(6A):2747\u20132774, 2018. [Rah17] S. Rahman. Wiener\u2013hermite polynomial expansion for multivariate gaussian probability measures. Journal of Mathematical Analysis and Applications, 454(1):303\u2013334, 2017. [RM14] E. Richard and A. Montanari. A statistical model for tensor PCA. Advances in neural information processing systems, 27, 2014. [Ros58] F. Rosenblatt. The perceptron: A probabilistic model for information storage and organization in the brain. Psychological Review, 65(6):386\u2013408, 1958. [Sol17] M. Soltanolkotabi. Learning ReLUs via gradient descent. In Advances in neural information processing systems, pages 2007\u20132017, 2017.   \n[SQW18] J. Sun, Q. Qu, and J. Wright. A geometric analysis of phase retrieval. Foundations of Computational Mathematics, 18:1131\u20131198, 2018.   \n[WZDD23] P. Wang, N. Zarifis, I. Diakonikolas, and J. Diakonikolas. Robustly learning a single neuron via sharpness. 40th International Conference on Machine Learning, 2023.   \n[ZWDD24] N. Zarifis, P. Wang, I. Diakonikolas, and J. Diakonikolas. Robustly learning singleindex models via alignment sharpness. 41th International Conference on Machine Learning, arXiv preprint: 2402.17756, 2024. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Supplementary Material ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Organization The appendix is organized as follows. In Appendix A, we provide a detailed discussion about our assumptions on the activation functions. In Appendix B we compare our results with the most related prior works. In Appendix C, we provide additional preliminaries on basic tensor algebra as well as Hermite polynomials and Hermite tensors. In Appendix D and Appendix E we provide full versions of Section 2 and Section 3 respectively, with omitted lemmas and proofs. ", "page_idx": 12}, {"type": "text", "text": "A Remarks on the Assumptions ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Assumption $1(i)$ appears in the same form in [BAGJ21, DNGL23]. The remaining two assumptions are implied by $|\\sigma^{\\prime}(z)|\\leq A|z|^{q}+B$ for constants $A,B,q.$ , assumed in these works. In detail, Assumption $1(i i)$ can be implied from $|\\sigma^{\\prime}(z)|\\leq A|z|^{q}+B$ using the Gagliardo\u2013Nirenberg inequality [FFRS21]; Assumption $1(i i i)$ follows from direct calculations as $\\mathbf{E}_{z\\sim\\mathcal{N}_{1}}[(A|z|^{q}+B)^{2}]$ is finite. Hence Assumption 1 is no stronger than the assumptions made in prior work, which considered the less challenging realizable and zero-mean label noise settings. ", "page_idx": 12}, {"type": "text", "text": "Some activations satisfying Assumption 1 are: ", "page_idx": 12}, {"type": "text", "text": "1. All \u2018 $(a,b)$ -well-behaved\u2019 activations $[\\mathrm{DGK}^{+}20$ , DKTZ22, WZDD23] that are non-decreasing, zero at the origin, $b$ -Lipschitz and $\\sigma^{\\prime}(z)\\geq a$ when $z\\in[0,R]$ satisfy Assumption 1 with $k^{*}=1$ (see Claim A.1). This includes ReLU, $\\dot{\\sigma}(z)=\\operatorname*{max}\\{0,z\\}$ and sigmoid, $\\sigma(z)\\ '=e^{z}/(1+e^{z})$ . 2. Activations for phase-retrieval [CLS15, CCFM19, SQW18]: $\\sigma(z)\\,=\\,z^{2}$ and $\\sigma(z)=|z|$ have information component $k^{*}=2$ . One can verify that they satisfy Assumption 1 after normalization. ", "page_idx": 12}, {"type": "text", "text": "Claim A.1. Let $a,b,R>0$ be some absolute constants. Let $\\sigma:\\mathbb{R}\\rightarrow\\mathbb{R}$ be an activation such that it is non-decreasing, $b$ -Lipschitz, and satisfies $\\sigma(0)=0$ and $\\sigma^{\\prime}(z)\\geq a$ for all $z\\in[0,R]$ . Then $\\sigma$ satisfies Assumption $^{\\,l}$ with information component $k^{*}=1$ . ", "page_idx": 12}, {"type": "text", "text": "Proof. We calculate the Hermite coefficient $c_{1}$ of $\\sigma$ : ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{z\\sim N_{1}}{\\mathbf{E}}[\\sigma(z)z]=\\underset{z\\sim N_{1}}{\\mathbf{E}}[\\sigma(z)z\\mathbb{1}\\{z\\geq0\\}]+\\underset{z\\sim N_{1}}{\\mathbf{E}}[\\sigma(z)z\\mathbb{1}\\{z\\leq0\\}]}\\\\ &{\\qquad\\qquad\\overset{(i)}{\\geq}\\underset{z\\sim N_{1}}{\\mathbf{E}}[\\sigma(z)z\\mathbb{1}\\{z\\geq0\\}]\\geq\\underset{z\\sim N_{1}}{\\mathbf{E}}[a z^{2}\\mathbb{1}\\{z\\in[0,R]\\}]\\gtrsim a R^{3}\\exp(-R^{2}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where in $(i)$ we used the monotonicity property of $\\sigma$ and that $\\sigma(0)=0$ . Thus we get that all wellbehaved activations as $k^{*}=1$ . In addition, the $\\mathbf{E}_{z\\sim\\mathcal{N}_{1}}[\\sigma(z)^{2}]\\,\\le\\,\\mathbf{E}_{z\\sim\\mathcal{N}_{1}}[b^{2}z^{2}]\\le b^{2}$ , hence after normalization, we have $c_{1}\\gtrsim a R^{3}\\exp(-R^{2})/(2b^{2})$ , which is an absolute constant bounded away from 0. Finally, we have ${\\bf E}_{z\\sim\\mathcal{N}_{1}}[\\sigma(z)^{4}]\\,\\leq\\,3b^{4}$ and $\\mathbf{E}_{z\\sim\\mathcal{N}_{1}}[(\\sigma^{\\prime}(z))^{2}]\\,\\le\\,b^{2}$ since $\\sigma$ is $b$ -Lipschitz. Thus, we have that all well-behaved activations satisfy Assumption 1. \u53e3 ", "page_idx": 12}, {"type": "text", "text": "B Comparison with Prior Work ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "B.1 Comparison with Prior Works on Agnostically Learning SIMs ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A long thread of research has been focusing on agnostic learning of single index models, including [FCG20, $\\mathrm{DGK}^{+}20$ , DKTZ22, WZDD23, ZWDD24]. A common assumption on the activation function $\\sigma$ used in these works is the so-called \u201cwell-behaved\u201d property, namely that $\\sigma$ is nondecreasing, zero at the origin ${\\boldsymbol{\\sigma}}(0)=0)$ ), $b$ -Lipshitz, and $\\sigma^{\\prime}(z)\\geq\\bar{a}$ when $z\\in[0,\\dot{R}]$ . In particular, ReLUs and sigmoids are well-behaved activations. For well-behaved activations, we have shown in Claim A.1 that they have $k^{*}=1$ and satisfy Assumption 1. Therefore, we conclude that our assumption Assumption 1 is indeed milder compared to prior works. ", "page_idx": 12}, {"type": "text", "text": "We also note that Assumption 1 allows for non-monotonic activations as well, for example $\\sigma(z)=z^{2}$ and $\\sigma(z)=|z|$ satisfy Assumption 1 with $k^{*}=2$ . ", "page_idx": 12}, {"type": "text", "text": "At the level of techniques, the algorithmic approaches in the aforementioned prior works on agnostically learning SIMs [FCG20, $\\mathrm{DGK}^{+}20$ , WZDD23] inherently fail in our more general activation setting. The main reason is that the underlying algorithms only exploit the information in the degree ", "page_idx": 12}, {"type": "text", "text": "1-Chow parameters. However, under Assumption 1 with $k^{*}\\geq2$ , the \u221ainner product between degree 1-Chow vector and any unit vector $\\mathbf{v}$ equals $|\\,\\mathbf{E}_{(\\mathbf{x},y)\\sim\\mathcal{D}}[y\\mathbf{x}]\\cdot\\mathbf{v}|\\leq\\sqrt{\\mathrm{OPT}}$ \u2014 which provides no information about the hidden vector $\\mathbf{w}^{*}$ . That is, in our more general setting, considering Chow vectors of higher degree appears necessary for any optimization method to succeed. ", "page_idx": 13}, {"type": "text", "text": "On the other hand, we remark that the algorithms in these works succed under more general distributions (including the Gaussian distribution), such as log-concave distributions. ", "page_idx": 13}, {"type": "text", "text": "B.2 Comparison with [DNGL23] ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The work by [DNGL23] applied a smoothing operator to the $L_{2}^{2}$ loss inspired by [ADGM17]. For any function $f:\\mathbb{R}\\rightarrow\\mathbb{R}$ , let the smoothing operator be ", "page_idx": 13}, {"type": "equation", "text": "$$\ng_{\\lambda}(f(\\mathbf{w}\\cdot\\mathbf{x})):=\\mathbf{E}_{\\mathbf{w}}\\left[f{\\left({\\frac{\\mathbf{w}+\\lambda\\mathbf{z}}{\\|\\mathbf{w}+\\lambda\\mathbf{z}\\|_{2}}}\\cdot\\mathbf{x}\\right)}\\right]},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\mathbf{z}\\sim\\mu_{\\mathbf{w}}$ is the uniform distribution on the sphere $\\mathbb{S}^{d-2}$ that is orthogonal to $\\mathbf{w}$ , and $\\lambda$ is the \u2018smoothing strength\u2019. Applying the smoothing operator to the loss we get ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\lambda}(\\mathbf{w}):=1-\\underset{\\mu_{\\mathbf{w}}}{\\mathbf{E}}\\left[\\mathbf{\\underset{(\\mathbf{x},y)\\sim\\mathcal{D}}{E}}\\left[y\\sigma\\left(\\frac{\\mathbf{w}+\\lambda\\mathbf{z}}{\\lVert\\mathbf{w}+\\lambda\\mathbf{z}\\rVert_{2}}\\cdot\\mathbf{x}\\right)\\right]\\right]\\mathrm{,}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "and ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\ell_{\\lambda}(\\mathbf{w};\\mathbf{x},y):=1-\\sum_{\\mu_{\\mathbf{w}}}\\bigg[y\\sigma\\bigg(\\frac{\\mathbf{w}+\\lambda\\mathbf{z}}{\\|\\mathbf{w}+\\lambda\\mathbf{z}\\|_{2}}\\cdot\\mathbf{x}\\bigg)\\bigg].\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The main algorithm of [DNGL23] is an online Riemannian gradient descent algorithm on the smoothed loss $\\mathcal{L}_{\\lambda}(\\mathbf{w})$ . ", "page_idx": 13}, {"type": "text", "text": "The dynamics of the online SGD algorithm in [DNGL23] can be split into three stages: In the first stage, starting from a randomly initialized vector $\\mathbf{w}^{0}$ such that $\\mathbf{w}^{0}\\cdot\\mathbf{w}^{*}=\\Theta(d^{-1/2})$ , the algorithm used a large smoothing operator with $\\lambda=d^{1/4}$ . Then, after $\\tilde{O}(d^{k^{*}/2-1})$ iterations the algorithm converges to a parameter $\\mathbf{w}^{1}$ such that $\\mathbf{w}^{1}\\cdot\\mathbf{w}^{*}\\,\\geq\\,d^{-1/4}$ . Then, in the second stage, with zerosmoothing $\\lambda=0$ , they run the algorithm for another $\\tilde{O}(d^{k^{*}/2-1})$ iterations and get a parameter $\\mathbf{w}^{2}$ such that $\\mathbf{w}^{2}\\cdot\\mathbf{w}^{*}\\geq1-d^{-1/4}$ . In the third stage, online SGD converges to $\\mathbf{w}^{3}$ with $\\mathbf{w}^{3}\\cdot\\mathbf{w}^{*}\\geq1-\\epsilon$ in $d/\\epsilon$ iterations. ", "page_idx": 13}, {"type": "text", "text": "However, the smoothing technique does not work in the agnostic setting. The main reason is that the agnostic noise buries the signal of the gradient. To see this, note that in Lemma 11 of [DNGL23], they showed that $g_{\\lambda}(\\mathrm{he}_{k}(\\mathbf{w}\\cdot\\mathbf{x}))=\\langle\\mathbf{H}\\mathbf{\\dot{e}}_{k}(\\mathbf{x}),\\mathbf{T}_{k}(\\mathbf{w})\\rangle$ , where ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbf{T}_{k}(\\mathbf{w}):=\\frac{1}{(1+\\lambda^{2})^{k/2}}\\sum_{j=0}^{\\lfloor k/2\\rfloor}\\binom{k}{2j}\\mathrm{Sym}(\\mathbf{w}^{\\otimes k-2j}\\otimes(\\mathbf{P}_{\\mathbf{w}^{\\perp}}^{\\otimes j}))\\lambda^{2j}\\nu_{j}^{(d-1)},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "and $\\nu_{j}^{(d-1)}=\\mathbf{E}_{\\mathbf{z}\\sim\\mathbb{S}^{d-2}}[\\mathbf{z}_{1}^{2j}]=\\Theta((d-1)^{-j})=\\Theta(d^{-j})$ . Note since the smoothing operator is a linear opera $\\begin{array}{r}{\\mathrm{or},\\mathrm{it}\\,\\mathrm{holds}\\,\\mathrm{that}\\,\\mathcal{L}_{\\lambda}(\\mathbf{w})=1-\\mathbf{E}_{(\\mathbf{x},\\mathbf{y})\\sim\\mathcal{D}}[y g_{\\lambda}(\\sigma(\\mathbf{w}\\cdot\\mathbf{x}))]=1-\\mathbf{E}_{(\\mathbf{x},\\mathbf{y})\\sim\\mathcal{D}}[y\\sum_{k>k^{*}}c_{k}g_{\\lambda}(\\mathrm{he}_{k}(\\mathbf{w}\\cdot\\mathbf{x}))],}\\end{array}$ $\\begin{array}{r}{\\mathbf{x}))\\bigr]\\,=\\,1\\,-\\,\\mathbf{E}_{(\\mathbf{x},y)\\sim\\mathcal{D}}[y\\sum_{k\\geq k^{*}}c_{k}\\langle\\mathbf{H}\\mathbf{e}_{k}(\\mathbf{x}),\\mathbf{T}_{k}(\\mathbf{w})\\rangle]}\\end{array}$ . Let $\\mathbf{g}_{\\lambda}(\\mathbf{w})\\,:=\\,\\mathbf{P}_{\\mathbf{w}^{\\perp}}\\nabla\\mathcal{L}_{\\lambda}(\\mathbf{w})$ denote the Riemannian gradient of $\\mathcal{L}_{\\lambda}(\\mathbf{w})$ , we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{g}_{\\lambda}(\\mathbf{w})\\cdot\\mathbf{w}^{*}=-\\underset{(\\mathbf{x},y)\\sim\\mathcal{D}}{\\mathbf{E}}\\left[y\\underset{k\\geq k^{*}}{\\sum}c_{k}\\langle\\mathbf{He}_{k}(\\mathbf{x}),\\nabla\\mathbf{T}_{k}(\\mathbf{w})\\otimes(\\mathbf{w}^{*})^{\\perp}\\mathbf{\\cdot}\\mathbf{w}\\rangle\\right]}\\\\ &{=\\underbrace{-\\underset{(\\mathbf{x},y)\\sim\\mathcal{D}}{\\mathbf{E}}\\left[\\sigma(\\mathbf{w}^{*}\\cdot\\mathbf{x})\\underset{k\\geq k^{*}}{\\sum}c_{k}\\langle\\mathbf{He}_{k}(\\mathbf{x}),\\nabla\\mathbf{T}_{k}(\\mathbf{w})\\otimes(\\mathbf{w}^{*})^{\\perp,\\mathbf{\\hat{w}}}\\rangle\\right]}_{\\mathcal{I}_{1}}}\\\\ &{\\qquad\\underbrace{-\\underset{(\\mathbf{x},y)\\sim\\mathcal{D}}{\\mathbf{E}}\\left[(y-\\sigma(\\mathbf{w}^{*}\\cdot\\mathbf{x}))\\sum_{k\\geq k^{*}}c_{k}\\langle\\mathbf{He}_{k}(\\mathbf{x}),\\nabla\\mathbf{T}_{k}(\\mathbf{w})\\otimes(\\mathbf{w}^{*})^{\\perp,\\mathbf{\\hat{w}}}\\rangle\\right]}_{\\mathcal{I}_{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We show that the noise term $\\mathcal{T}_{2}$ kills the signal term $\\mathcal{T}_{1}$ in the beginning stage when $\\mathbf{w}\\!\\cdot\\!\\mathbf{w}^{*}=\\Theta(1/\\sqrt{d})$ and $\\lambda=d^{1/4}$ . ", "page_idx": 13}, {"type": "text", "text": "Claim B.1. When $\\mathbf{w}\\cdot\\mathbf{w}^{*}=\\Theta(1/\\sqrt{d})$ and $\\lambda=d^{1/4}$ , it holds ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left|\\mathcal{T}_{1}\\right|\\lesssim d^{-k^{*}/2},\\ \\left|\\mathcal{T}_{2}\\right|\\lesssim\\sqrt{\\mathrm{OPT}}d^{-k^{*}/4}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Thus, in order for the signal to overcome the noise, one requires $\\mathrm{OPT}\\lesssim d^{-k^{*}/4}$ , which is too strict to hold in reality. ", "page_idx": 14}, {"type": "text", "text": "Proof of Claim B.1. Following the steps in Lemma 12 in [DNGL23], $\\nabla\\mathbf{T}_{k}(\\mathbf{w})\\otimes(\\mathbf{w}^{*})^{\\perp}\\mathbf{w}$ equals: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\nabla\\mathbf{T}_{k}(\\mathbf{w})\\otimes(\\mathbf{w}^{*})^{\\perp_{\\mathbf{w}}}}\\\\ &{=\\frac{k}{(1+\\lambda^{2})^{k/2}}\\sum_{j=0}^{[\\frac{k-1}{2}]}\\binom{k-1}{2j}\\mathrm{Sym}(\\mathbf{w}^{\\otimes k-2j-1}\\otimes\\mathbf{P}_{\\mathbf{w}^{\\perp}}^{\\otimes j})\\otimes(\\mathbf{w}^{*})^{\\perp_{\\mathbf{w}}}\\lambda^{2j}\\nu_{j}^{(d-1)}}\\\\ &{\\quad-\\,\\frac{\\lambda^{2}k(k-1)}{(d-1)(1+\\lambda^{2})^{k/2}}\\sum_{j=0}^{\\lfloor\\frac{k-2}{2}\\rfloor}\\binom{k-1}{2j}\\mathrm{Sym}(\\mathbf{w}^{\\otimes k-2j-1}\\otimes\\mathbf{P}_{\\mathbf{w}^{\\perp}}^{\\otimes j})\\otimes(\\mathbf{w}^{*})^{\\perp_{\\mathbf{w}}}\\lambda^{2j}\\nu_{j}^{(d+1)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Plugging the equation above back into Equation (17), we study each term $\\lvert\\mathcal{T}_{1}\\rvert$ and $|{\\mathcal{T}}_{2}|$ respectively in the beginning phase when $\\mathbf{w}\\cdot\\mathbf{w}^{*}=O(d^{-1/2})$ . ", "page_idx": 14}, {"type": "text", "text": "Using Fact C.3, and recall that in [DNGL23] $\\lambda$ is chosen to be $\\lambda=d^{1/4}$ and $\\nu_{j}^{(d-1)}=\\Theta(d^{-j})$ , \u03bdj(d+1)= \u0398(d\u2212j) by definition, I1 equals: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left(-\\sum_{k\\geq0}^{\\infty}\\sqrt{\\operatorname*{sup}(\\mathbf{v}^{\\alpha}(\\mathbf{v}^{\\beta})},\\mathcal{S}_{m}(\\mathbb{T}\\mathbf{T}\\mathbf{S}_{0}\\left(\\mathbf{w}^{\\gamma}\\right)^{-\\alpha})\\right)}\\\\ &{\\ \\leq\\sum_{k\\geq0}^{\\infty}\\frac{c_{k}^{2}}{(k+1)^{2}(k)^{2}}\\frac{\\|\\mathbf{v}^{\\alpha}\\|_{\\mathcal{H}}^{2}}{\\sum_{k\\geq0}^{\\infty}\\left(\\frac{b_{k}^{2}}{2}\\right)(k+1)(k^{2})^{2}}\\exp\\left(-\\omega\\frac{c_{k}^{2}}{2}(k-1)(-\\omega\\frac{c_{k}^{2}}{2})^{\\gamma}\\|\\mathbf{(v}^{\\alpha})^{-1}\\|_{\\mathcal{H}}^{\\beta}\\right)}\\\\ &{\\ \\ \\ +\\sum_{k\\geq0}^{\\infty}\\frac{c_{k}^{2}\\|\\mathbf{\\hat{H}}(k-1)\\|_{\\mathcal{H}}^{2}}{(k-1)^{2}(k-1)(1+\\lambda^{2})^{2}}\\frac{\\|\\mathbf{v}^{\\alpha}\\|_{\\mathcal{H}}^{2}}{\\sum_{k\\geq0}^{\\infty}\\left(\\frac{b_{k}^{2}}{2}\\right)(k-1)^{2-k/2}(1-(\\mathbf{w}^{\\gamma})^{2})\\|\\mathbf{\\hat{v}}\\|^{\\alpha})^{k-1}\\frac{\\|\\mathbf{v}\\|_{\\mathcal{H}}^{2}}{\\hat{L}^{2}}}\\\\ &{\\ \\ \\ \\ +\\frac{\\|\\mathbf{v}^{\\alpha}\\|_{\\mathcal{H}}^{2}}{\\sum_{k\\geq0}^{\\infty}\\sqrt{\\operatorname*{sup}(\\mathbf{v}^{\\alpha}(\\mathbf{v}^{\\beta})}}\\left(-\\sum_{k\\geq0}^{\\infty}\\vert c_{k}^{-2-k/2}\\vert\\right)\\frac{\\|\\mathbf{v}\\|_{\\mathcal{H}}^{2}}{\\sum_{k\\geq0}^{\\infty}\\left(\\frac{b_{k}^{2}}{2}\\right)(k-1)^{2}}}\\\\ &{\\ \\ \\ \\ +\\sum_{k\\geq0}^{\\infty}\\frac{c_{k}^{2}\\|\\mathbf{\\hat{H}}(k-1)\\|_{\\mathcal{H}}^{2}}{\\sqrt{\\operatorname*\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where in $(i)$ \u221awe plugged in the value of $\\lambda$ and $\\nu_{j}^{(d-1)}$ , $\\nu_{j}^{(d+1)}$ ; in $(i i)$ we plugged in the value of $\\mathbf{w}\\cdot\\mathbf{w}^{*}=1/\\sqrt{d}$ ; and in $(i i i)$ we used the fact that ${\\binom{k}{j}}\\leq2^{k}$ . However, the magnitude of the signal from $\\lvert\\mathcal{T}_{1}\\rvert$ is much smaller compared to the strength of the noise, $\\left|{\\mathcal{T}}_{2}\\right|$ , as by Cauchy-Schwarz, we ", "page_idx": 14}, {"type": "text", "text": "have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|Z_{2}|\\overset{(i v)}{\\leq}\\bigg(\\underset{(\\mathbf{x},y)\\sim\\mathcal{D}}{\\mathbf{E}}[(y-\\sigma(\\mathbf{w}^{*}\\cdot\\mathbf{x}))^{2}]\\underset{\\mathbf{x}\\sim\\mathcal{N}_{d}}{\\mathbf{E}}\\bigg[\\bigg(\\sum_{k\\geq k^{*}}c_{k}\\langle\\mathbf{H}\\mathbf{e}_{k}(\\mathbf{x}),\\nabla\\mathbf{T}_{k}(\\mathbf{w})\\otimes(\\mathbf{w}^{*})^{\\perp_{\\mathbf{w}}}\\rangle\\bigg)^{2}\\bigg]\\bigg)^{\\frac{1}{2}}}\\\\ &{\\qquad\\leq\\sqrt{\\mathrm{OPT}}\\bigg(\\underset{k\\geq k^{*}}{\\sum}c_{k}^{2}\\|\\nabla\\mathbf{T}_{k}(\\mathbf{w})\\otimes(\\mathbf{w}^{*})^{\\perp_{\\mathbf{w}}}\\|_{F}^{2}\\bigg)^{\\frac{1}{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Note that in Lemma 12 of [DNGL23], it was proved that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\|\\nabla\\mathbf{T}_{k}(\\mathbf{w})\\otimes(\\mathbf{w}^{*})^{\\perp_{\\mathbf{w}}}\\|_{F}^{2}}\\\\ &{\\lesssim\\frac{k^{2}}{(1+\\lambda^{2})^{k}}\\displaystyle\\sum_{j=0}^{\\lfloor\\frac{k-1}{2}\\rfloor}\\binom{k-1}{2j}\\lambda^{4j}\\nu_{j}^{(d-1)}+\\frac{\\lambda^{4}k^{4}}{d^{2}(1+\\lambda^{2})^{k}}\\displaystyle\\sum_{j=0}^{\\lfloor\\frac{k-2}{2}\\rfloor}\\binom{k-2}{2j}\\lambda^{4j}\\nu_{j}^{(d+1)}}\\\\ &{\\stackrel{(v)}{\\lesssim}\\frac{k^{2}2^{k}}{d^{\\frac{k}{2}}}+\\frac{k^{4}2^{k}}{d^{\\frac{k}{2}+1}}\\lesssim k^{4}2^{k}d^{-k/2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where in $(v)$ we plugged in the value of $\\lambda$ and $\\nu_{j}^{(d-1)},\\nu_{j}^{(d+1)}$ . Thus, it holds that ", "page_idx": 15}, {"type": "equation", "text": "$$\n|\\mathcal{Z}_{2}|\\lesssim\\sqrt{\\mathrm{OPT}}\\bigg(\\sum_{k\\ge k^{*}}c_{k}^{2}4^{k}d^{-\\frac{k}{2}}\\bigg)^{1/2}\\lesssim\\sqrt{\\mathrm{OPT}}2^{k^{*}}d^{-k^{*}/4}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Finally, we remark that the equality holds at $(i v)$ when ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{\\Psi}_{t}=\\sigma(\\mathbf{w}^{*}\\cdot\\mathbf{x})+\\frac{\\sqrt{\\mathrm{OPT}}}{\\sqrt{\\sum_{k\\geq k^{*}}c_{k}^{2}\\Vert\\nabla\\mathbf{T}_{k}(\\mathbf{w})\\otimes(\\mathbf{w}^{*})^{\\perp_{\\infty}}\\Vert_{F}^{2}}}\\bigg(\\sum_{k\\geq k^{*}}c_{k}\\langle\\mathbf{H}\\mathbf{e}_{k}(\\mathbf{x}),\\nabla\\mathbf{T}_{k}(\\mathbf{w})\\otimes(\\mathbf{w}^{*})^{\\perp_{\\infty}}\\rangle\\bigg).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "B.3 Comparison with [DPVLB24] ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In [DPVLB24], the authors studied a milder noise model than the agnostic setting. In their model, the joint distribution on $(\\mathbf{x},y)$ is defined as $\\mathbf{E}_{y}[y\\mid\\mathbf{x}]=\\sigma(\\mathbf{w}^{*}\\cdot\\mathbf{x})+\\mathbf{\\bar{\\xi}}\\mathbf{\\bar{\\xi}}(\\mathbf{w}^{*}\\cdot\\mathbf{x})$ , where $\\xi:\\mathbb{R}\\mapsto\\mathbb{R}$ is assumed to be known to the learner. Note that in this model the labels $y$ are independent of all the directions orthogonal to $\\mathbf{w}^{*}$ , i.e., the random vector $\\mathbf{x}^{\\perp}\\mathbf{w}^{*}$ is independent of $y$ . In comparison, in the agnostic setting, the distribution of the labels is $\\mathbf{E}_{y}[y\\mid\\mathbf{x}]=\\sigma(\\bar{\\mathbf{w}^{*}}\\cdot\\mathbf{x})+\\xi^{\\prime}(\\mathbf{x})$ , where $\\xi^{\\prime}:\\mathbb{R}^{d}\\mapsto\\mathbb{R}$ is unknown and can depend arbitrarily on $\\mathbf{x}$ . In particular, the aforementioned independence with the directions orthogonal to $\\mathbf{w}^{*}$ does not hold. As a result of their milder noise model, the authors can utilize information on the joint distribution to mitigate the corruption of the noise. This assumption is significantly weaker than agnostic noise. ", "page_idx": 15}, {"type": "text", "text": "In addition, instead of studying the information component $k^{*}$ defined as the first non-zero Hermite coefficient of the activation $\\sigma$ , [DPVLB24] considered the generative component of the label $y$ , which is defined as $\\bar{k}^{*}:=\\operatorname*{min}_{k\\geq0}\\{\\lambda_{k}>0\\}$ , where $\\lambda_{k}:=\\sqrt{\\mathbf{E}_{y}[\\zeta_{k}^{2}]}$ , and $\\zeta_{k}(y):=\\mathbf{E}_{z}[\\mathrm{he}_{k}(z)|y]$ . The main results in [DPVLB24] are twofold. First, they proved an SQ lower bound showing that under the setting aforementioned, any polynomial time SQ algorithm requires at least $O(d^{\\bar{k}^{\\ast}/2})$ samples to learn the hidden direction $\\mathbf{w}^{*}$ . Then, they provided an SQ algorithm using partial-trace operators that returns a vector $\\widehat{\\bf w}$ such that $(\\widehat{\\mathbf{w}}\\cdot\\mathbf{w}^{*})^{2}\\geq1-\\epsilon^{2}$ with $O(d^{\\bar{k}^{*}/2}+d/\\epsilon^{2})$ samples, matching the SQ lower bound. ", "page_idx": 15}, {"type": "text", "text": "The algorithm proposed in [DPVLB24] can be described in short as follows: given joint distribution $\\mathsf{P}$ , calculate $\\zeta_{\\bar{k}^{*}}(y)$ and transform the label $y$ to $\\zeta_{\\bar{k}^{*}}(y)$ . Then, they applied tensor PCA on $\\zeta_{\\bar{k}^{*}}(y)\\mathbf{H}\\mathbf{e}_{k}(\\mathbf{x})$ , using the partial trace operator and tensor power iteration. ", "page_idx": 15}, {"type": "text", "text": "We note that the partial trace method for tensor PCA fails to find an initialization vector $\\mathbf{w}^{0}$ such that $\\mathbf{w}^{0}\\cdot\\mathbf{w}^{*}\\geq c$ for some positive absolute constant $c$ under agnostic noise. We explain this briefly below. For simplicity, let us consider $k$ being even. Note that for a $k$ -tensor $\\mathbf{T}$ with even $k$ , the partial trace operator is defined by $\\mathsf{P T}(\\mathbf{T})=\\langle\\mathbf{T},\\mathbf{I}^{\\otimes(k-2)/2}\\rangle$ , where $\\mathbf{I}$ is the identity matrix in $\\mathbb{R}^{d\\times d}$ . The idea in [DPVLB24] and [ADGM17] is to use the top eigenvector $\\mathbf{v}_{1}\\in\\mathbb{R}^{d}$ of $\\mathsf{P T}(\\mathbf{E}_{(\\mathbf{x},y)\\sim\\mathcal{D}}[y\\mathbf{H}\\mathbf{e}_{k}(\\mathbf{x})])$ as a warm-start. However, we show that under agnostic noise, this eigenvector $\\mathbf{v}_{1}$ does not contain a strong enough signal to provide information of $\\mathbf{w}^{*}$ . Note that by definition of the partial trace operator, for any $\\mathbf{v}\\in\\mathbb{B}_{d}$ we have ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad{\\mathbf{v}}^{\\top}\\mathsf{P T}(\\mathsf{\\mathbf{\\Omega}}_{(\\mathbf{x},y)\\sim\\mathcal{D}}^{\\mathbf{\\Gamma}}[y\\mathbf{He}_{k}(\\mathbf{x})]){\\mathbf{v}}=\\underset{(\\mathbf{x},y)\\sim\\mathcal{D}}{\\mathbf{E}}[y{\\mathbf{v}}^{\\top}\\langle\\mathbf{He}_{k}(\\mathbf{x}),\\mathbf{I}^{\\otimes(k-2)/2}\\rangle{\\mathbf{v}}]}\\\\ &{=\\underset{(\\mathbf{x},y)\\sim\\mathcal{D}}{\\mathbf{E}}[y\\langle\\mathbf{He}_{k}(\\mathbf{x}),\\mathbf{I}^{\\otimes(k-2)/2}\\otimes\\mathbf{v}^{\\otimes2}\\rangle]}\\\\ &{=\\underset{\\mathbf{x}\\sim\\mathcal{N}_{d}}{\\mathbf{E}}[\\sigma(\\mathbf{w}^{*}\\cdot\\mathbf{x})\\langle\\mathbf{He}_{k}(\\mathbf{x}),\\mathbf{I}^{\\otimes(k-2)/2}\\otimes\\mathbf{v}^{\\otimes2}\\rangle]}\\\\ &{\\quad+\\underset{(\\mathbf{x},y)\\sim\\mathcal{D}}{\\mathbf{E}}[(y-\\sigma(\\mathbf{w}^{*}\\cdot\\mathbf{x}))\\langle\\mathbf{He}_{k}(\\mathbf{x}),\\mathbf{I}^{\\otimes(k-2)/2}\\otimes\\mathbf{v}^{\\otimes2}\\rangle]}\\\\ &{=c_{k}\\langle\\mathbf{w}^{*\\otimes k},\\mathbf{I}^{\\otimes(k-2)/2}\\otimes\\mathbf{v}^{\\otimes2}\\rangle+\\underset{(\\mathbf{x},y)\\sim\\mathcal{D}}{\\mathbf{E}}[(y-\\sigma(\\mathbf{w}^{*}\\cdot\\mathbf{x}))\\langle\\mathbf{He}_{k}(\\mathbf{x}),\\mathbf{I}^{\\otimes(k-2)/2}\\otimes\\mathbf{v}^{\\otimes2}\\rangle].}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The first term in the equality above equals $c_{k}\\big\\langle\\mathbf{w}^{*\\otimes k},\\mathbf{I}^{\\otimes(k-2)/2}\\otimes\\mathbf{v}^{\\otimes2}\\big\\rangle=c_{k}(\\mathbf{w}^{*}\\cdot\\mathbf{v})^{2}$ , however, the second term can be as large as (by Cauchy-Schwarz): ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\underset{(\\mathbf{x},y)\\sim\\mathcal{D}}{\\mathbf{E}}[(y-\\sigma(\\mathbf{w}^{*}\\cdot\\mathbf{x}))\\langle\\mathbf{He}_{k}(\\mathbf{x}),\\mathbf{I}^{\\otimes(k-2)/2}\\otimes\\mathbf{v}^{\\otimes2}\\rangle]\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\overset{(i)}{\\leq}\\sqrt{\\underset{(\\mathbf{x},y)\\sim\\mathcal{D}}{\\mathbf{E}}[(y-\\sigma(\\mathbf{w}^{*}\\cdot\\mathbf{x}))^{2}]}\\underset{\\mathbf{x}\\sim\\mathcal{N}_{d}}{\\mathbf{E}}[\\langle\\mathbf{H}\\mathbf{e}_{k}(\\mathbf{x}),\\mathbf{I}^{\\otimes(k-2)/2}\\otimes\\mathbf{v}^{\\otimes2}\\rangle^{2}]}\\\\ &{\\leq\\sqrt{\\mathrm{OPT}}\\lVert\\mathbf{I}^{\\otimes(k-2)/2}\\otimes\\mathbf{v}^{\\otimes2}\\rVert_{F}=\\sqrt{\\mathrm{OPT}}d^{(k-2)/4}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "As suggested by inequality $(i)$ above, let $\\mathbf{v}$ be any unit vector orthogonal to $\\mathbf{w}^{*}$ , consider an agnostic noise model ", "page_idx": 16}, {"type": "equation", "text": "$$\ny=\\sigma(\\mathbf{w}^{*}\\cdot\\mathbf{x})+(\\sqrt{\\mathrm{OPT}}/d^{(k-2)/4})\\langle\\mathbf{He}_{k}(\\mathbf{x}),\\mathbf{I}^{\\otimes(k-2)/2}\\otimes\\mathbf{v}^{\\otimes2}\\rangle,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "then $\\mathbf{E}_{(\\mathbf{x},y)\\sim\\mathcal{D}}[(y-\\sigma(\\mathbf{w}^{*}\\cdot\\mathbf{x}))^{2}]=\\mathrm{OPT}.$ . However, as we can see from Equation (18), it holds that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad(\\mathbf{w}^{*})^{\\top}\\mathsf{P T}(\\mathbf{\\Omega}_{(\\mathbf{x},\\mathbf{y})\\sim\\mathcal{D}}[y\\mathbf{He}_{k}(\\mathbf{x})])\\mathbf{w}^{*}}\\\\ &{=c_{k}+\\frac{\\sqrt{\\mathrm{OPT}}}{d^{(k-2)/4}}\\mathbf{\\Omega}_{(\\mathbf{x},\\mathbf{y})\\sim\\mathcal{D}}[\\mathbf{\\Omega}\\mathbf{He}_{k}(\\mathbf{x}),\\mathbf{I}^{\\otimes(k-2)/2}\\otimes\\mathbf{v}^{\\otimes2}\\rangle\\langle\\mathbf{He}_{k}(\\mathbf{x}),\\mathbf{I}^{\\otimes(k-2)/2}\\otimes(\\mathbf{w}^{*})^{\\otimes2}\\rangle]}\\\\ &{=c_{k}+\\frac{\\sqrt{\\mathrm{OPT}}}{d^{(k-2)/4}}\\langle\\mathrm{Sym}(\\mathbf{I}^{\\otimes(k-2)/2}\\otimes\\mathbf{v}^{\\otimes2}),\\mathrm{Sym}(\\mathbf{I}^{\\otimes(k-2)/2}\\otimes(\\mathbf{w}^{*})^{\\otimes2})\\rangle}\\\\ &{\\stackrel{(i i)}{\\lesssim}c_{k}+\\frac{\\sqrt{\\mathrm{OPT}}}{d^{(k-2)/4}}d^{(k-4)/2}=c_{k}+\\sqrt{\\mathrm{OPT}}d^{k/4-3/2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $(i i)$ comes from the fact that for any permutation $\\pi$ such that $\\{\\pi(1),\\pi(2)\\}\\cap\\{1,2\\}\\neq\\emptyset$ , since $\\mathbf{v}\\perp\\mathbf{w}^{*}$ , it holds ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{i_{1},\\ldots,i_{k}}\\mathbf{v}_{i_{1}}\\mathbf{v}_{i_{2}}\\mathbf{w}_{i_{\\pi(1)}}^{*}\\mathbf{w}_{i_{\\pi(2)}}^{*}\\mathbf{I}_{i_{3},i_{4}}\\,\\ldots\\mathbf{I}_{i_{k-1},i_{k}}\\mathbf{I}_{i_{\\pi(3)},i_{\\pi(4)}}\\,\\ldots\\mathbf{I}_{i_{\\pi(k-1)},i_{\\pi(k)}}=0;\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and on the other hand, when $\\{\\pi(1),\\pi(2)\\}\\cap\\{1,2\\}=\\mathcal{O}$ , then ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{i_{1},\\ldots,i_{k}=1}^{d}\\mathbf{v}_{i_{1}}\\mathbf{v}_{i_{2}}\\mathbf{w}_{i_{\\pi(1)}}^{*}\\mathbf{w}_{i_{\\pi(2)}}^{*}\\mathbf{I}_{i_{3},i_{4}}\\ldots\\mathbf{I}_{i_{k-1},i_{k}}\\mathbf{I}_{i_{\\pi(3)},i_{\\pi(4)}}\\ldots\\mathbf{I}_{i_{\\pi(k-1)},i_{\\pi(k)}}\\le d^{(k-4)/2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "To see this, note that there exists a chain of identity matrices ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{I}_{i_{\\pi(a_{1})},i_{\\pi(a_{1}+1)}}\\mathbf{I}_{i_{j_{1}},i_{j_{1}+1}}\\mathbf{I}_{i_{\\pi(a_{2})},i_{\\pi(a_{2}+1)}}\\mathbf{I}_{i_{j_{2}},i_{j_{2}+1}}\\cdot\\cdot\\cdot\\left\\{\\mathbf{I}_{i_{\\pi(a_{m})},i_{\\pi(a_{m}+1)}}\\mathbf{I}_{i_{\\pi(a_{2}+1)},i_{\\pi(a_{2}+1)}}\\mathbf{I}_{i_{\\pi(a_{1}+1)},i_{\\pi(a_{m}+1)}}\\mathbf{I}_{i_{\\pi(a_{1}+1)},i_{\\pi(a_{m}+1)}}\\mathbf{I}_{i_{\\pi(a_{m}+1)},i_{\\pi(a_{m}+1)}}\\mathbf{I}_{i_{\\pi(a_{m}+1)},i_{\\pi(a_{m}+1)}}\\mathbf{I}_{i_{\\pi(a_{m}+1)}}\\mathbf{I}_{i_{\\pi(a_{m}+1)}}\\mathbf{I}_{i_{\\pi(a_{m}+1)}}\\mathbf{I}_{i_{\\pi(a_{m}+1)}}\\mathbf{I}_{i_{\\pi(a_{m}+1)}}\\mathbf{I}_{i_{\\pi(a_{m}+1)}}\\mathbf{I}_{i_{\\pi(a_{m}+1)}}\\mathbf{I}_{i_{\\pi(a_{m}+1)}}\\mathbf{I}_{i_{\\pi(a_{m}+1)}}\\mathbf{I}_{i_{\\pi(a_{m}+1)}}\\mathbf{I}_{i_{\\pi(a_{1}+1)}}\\mathbf{I}_{i_{\\pi(a_{2}+1)}}\\mathbf{I}_{i_{\\pi(a_{1}+1)}}\\mathbf{I}_{i_{\\pi(a_{2}+1)}}\\mathbf{I}_{i_{\\pi(a_{1}+1)}}\\mathbf{I}_{i_{\\pi(a_{2}+1)}}\\mathbf{I_{i_{\\pi(a_{m}+1)}}}\\mathbf{I_{i_{\\pi(a_{m1}+1)}}\\mathbf{I}\\mathbf{I_{i_(a_{1}+1)}}\\mathbf{I_{}}\\mathbf{I_{i\\pi(a_(a_{1)}})}\\mathbf{I\\right)\\mathbf{I(a_{I_(a_1}{})}\\mathbf{I}\\mathbf{I\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "such that $\\pi(a_{1})=1,\\pi(a_{1}+1)=j_{1},j_{1}+1=\\pi(a_{2}),\\pi(a_{2}+1)=j_{2}\\ldots$ . until we have $i_{\\pi(a_{m}+1)}\\in$ $\\{i_{1},i_{2}\\}$ or $i_{j_{m}+1}\\in\\{i_{\\pi(1)},i_{\\pi(2)}\\}$ . For the latter case, it implies that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum\\mathbf{v}_{i_{1}}(\\mathbf{w^{*}})_{i_{j_{m+1}}}\\mathbf{I}_{i_{\\pi(a_{1})},i_{\\pi(a_{1}+1)}}\\mathbf{I}_{i_{j_{1}},i_{j_{1}+1}}\\mathbf{I}_{i_{\\pi(a_{2})},i_{\\pi(a_{2}+1)}}\\mathbf{I}_{i_{j_{2}},i_{j_{2}+1}}\\cdot\\cdot\\mathbf{I}_{i_{j_{m}},i_{j_{m+1}}}=\\sum_{i_{1}}^{d}\\mathbf{v}_{i_{1}}(\\mathbf{w^{*}})_{i_{1}}=0.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore, only the first case is interesting. Since $\\pi(a_{1})=1$ , and $\\pi$ is a bijection, hence $\\pi(a_{m}\\!+\\!1)\\neq1$ therefore the only possible case would be $i_{\\pi(a_{m}+1)}=i_{2}$ , then we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum\\mathbf{v}_{i_{1}}\\mathbf{v}_{i_{\\pi(a_{m}+1)}}\\mathbf{I}_{i_{\\pi(a_{1})},i_{\\pi(a_{1}+1)}}\\mathbf{I}_{i_{j_{1}},i_{j_{1}+1}}\\mathbf{I}_{i_{\\pi(a_{2})},i_{\\pi(a_{2}+1)}}\\mathbf{I}_{i_{j_{2}},i_{j_{2}+1}}\\cdot\\cdot\\cdot\\mathbf{I}_{i_{\\pi(a_{m})},i_{\\pi(a_{m}+1)}}=\\sum_{i_{1}}^{d}\\mathbf{v}_{i_{1}}^{2}=1.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Continue the discussion for vi2, wi\u2217\u03c0(1) and $\\mathbf{w}_{i_{\\pi(2)}}^{*}$ , and let $\\left\\{j_{5},j_{6},\\ldots,j_{k-1},j_{k}\\right\\}=\\left\\{i_{1},\\ldots,i_{k}\\right\\}-$ $\\left\\{i_{1},i_{2},i_{\\pi(1)},i_{\\pi(2)}\\right\\}$ , we get that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{1,\\ldots,i_{k}=1}^{d}\\mathbf{v}_{i_{1}}\\mathbf{v}_{i_{2}}\\mathbf{w}_{i_{\\pi(1)}}^{*}\\mathbf{w}_{i_{\\pi(2)}}^{*}\\mathbf{I}_{i_{3},i_{4}}\\cdot\\cdot\\cdot\\mathbf{I}_{i_{k-1},i_{k}}\\mathbf{I}_{i_{\\pi(3)},i_{\\pi(4)}}\\cdot\\cdot\\cdot\\mathbf{I}_{i_{\\pi(k-1)},i_{\\pi(k)}}\\le\\displaystyle\\sum_{j_{5},\\ldots,j_{k}=1}^{d}\\mathbf{I}_{j_{5},j_{6}}^{2}\\cdot\\cdot\\cdot\\mathbf{I}_{j_{k-1},j_{k}}^{2}}&{}\\\\ {\\displaystyle=d^{(k-4)/2}.}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Similarly, for any unit vector u $\\mathbf{\\Sigma}.\\perp\\mathbf{v}$ and $\\mathbf{u}\\perp\\mathbf{w}^{*}$ , it holds ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbf{u}^{\\top}\\mathsf{P T}(\\mathbf{\\Pi}_{(\\mathbf{x},y)\\sim\\mathcal{D}}^{\\mathbf{E}}[y\\mathbf{H}\\mathbf{e}_{k}(\\mathbf{x})])\\mathbf{u}\\lesssim\\sqrt{\\mathrm{OPT}}d^{k/4-3/2}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "But one can also show that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{{\\mathbf v}^{\\top}{\\mathsf P}{\\mathsf T}\\big(\\underset{({\\mathbf x},{\\boldsymbol y})\\sim{\\mathcal D}}{\\mathbf E}[y{\\mathbf H}{\\mathbf e}_{k}({\\mathbf x})]\\big){\\mathbf v}=\\frac{\\sqrt{\\mathrm{OPT}}}{d^{(k-2)/4}}\\underset{({\\mathbf x},{\\boldsymbol y})\\sim{\\mathcal D}}{\\mathbf E}[\\langle{\\mathbf H}{\\mathbf e}_{k}({\\mathbf x}),{\\mathbf I}^{\\otimes(k-2)/2}\\otimes{\\mathbf v}^{\\otimes2}\\rangle^{2}]}\\\\ &{\\qquad\\qquad\\qquad=\\frac{\\sqrt{\\mathrm{OPT}}}{d^{(k-2)/4}}\\|\\mathrm{Sym}({\\mathbf I}^{\\otimes(k-2)/2}\\otimes{\\mathbf v}^{\\otimes2})\\|_{F}^{2}}\\\\ &{\\approx\\sqrt{\\mathrm{OPT}}d^{k/4-1/2};}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This implies that to guarantee that $\\mathbf{w}^{*}$ is the unique top eigenvector, it has to be that OPT $\\lesssim$ $c_{k}^{2}d^{-(k-2)/2}$ . Therefore, the noise term completely buries the signal $(\\mathbf{w}^{*}\\cdot\\mathbf{v})^{2}$ unless OPT $\\lesssim$ d\u2212(k\u22122)/2, which is unrealistic to assume. ", "page_idx": 17}, {"type": "text", "text": "B.4 Remarks on Tensor PCA ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We summarize some historical bits of Tensor PCA. [RM14] proposed the following \u2018spiked\u2019 tensor PCA problem: given a $k$ -tensor of the form3 ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbf{T}=\\tau\\mathbf{v}^{\\otimes k}+\\mathbf{A},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\mathbf{A}$ is a $k$ -tensor with i.i.d. standard Gaussian entries, recover the planted vector v. The \u2018singleobservation\u2019 model is equivalent (in law) to the following \u2018multi-observation\u2019 model([BAGJ20]): given $n$ i.i.d. copies $\\mathbf{T}^{(i)}\\overset{\\cdot}{=}\\tau^{\\prime}\\mathbf{v}^{\\otimes k}+\\mathbf{A}^{(i)}$ with $\\tau^{\\prime}=\\tau/\\sqrt{n}$ , recover $\\mathbf{v}$ using the empirical estimation: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\widehat{\\mathbf{T}}=\\tau^{\\prime}\\mathbf{v}^{\\otimes k}+\\frac{1}{n}\\sum_{i=1}^{n}\\mathbf{A}^{(i)}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "In [RM14], it has been shown that for\u221a model (PCA-S), it is information-theoretically impossible to recover $\\mathbf{v}$ when $\\tau\\,<\\,(\\beta_{k}^{*}-o_{d}(1))\\sqrt{d}$ , for some real constant $\\beta_{k}^{*}$ ; however, there also exists\u221a a constant $\\beta_{k}^{\\prime}$ such that the information-theoretic optimal threshold for $\\tau$ is $\\tau\\,>\\,(\\beta_{k}^{\\prime}+o_{d}(1))\\sqrt{d}$ (see also [DH21]). However, there is a huge statistical-computational gap for solving tensor PCA problems and it is conjectured impossible to solve (PCA-S) when $\\tau\\lesssim d^{k/4}$ for $k\\geq3$ [RM14, DH21]. For multi-observation model (PCA-M), this thresholds translates to a sample complexity of $n\\gtrsim d^{k/2}$ when $\\tau^{\\prime}=O(1)$ . ", "page_idx": 17}, {"type": "text", "text": "[RM14] proposed the tensor unfolding algorithm that recovers $\\mathbf{v}$ in (PCA-S) when $\\tau\\gtrsim d^{\\lceil k/2\\rceil/2}$ , i.e., for (PCA-M), the required sample complexity is $\\Omega(d^{\\lceil k/2\\rceil})$ . However, it is conjectured in [RM14] that the tensor unfolding algorithm can actually deal with $\\tau\\gtrsim d^{k/4}$ . ", "page_idx": 17}, {"type": "text", "text": "Note that the unfolding algorithm requires $\\tau\\gtrsim d$ when $k\\,=\\,3$ . Many papers are devoted to improving from $\\tau\\gtrsim d$ to $\\tau\\gtrsim\\,d^{3/4}$ and reducing the runtime and memory cost. To name a few, [HSS15, HSSS16] used Sum-of-Squares algorithms with partial trace operators to achieve the goal within $O(d^{3})$ runtime; [ADGM17] also used partial trace operators, but instead of using SOS algorithms, they injected noise to smooth the landscape of the loss. ", "page_idx": 18}, {"type": "text", "text": "Perhaps an interesting aspect of our paper is that the unfolding algorithm can deal with stronger noise (compared to the Gaussian noise A, our noise is very heavy-tailed) and is more robust to the noise, as partial trace operator does not work under our agnostic noise. However, this might be attributed to the special structure of the noisy chow tensor. ", "page_idx": 18}, {"type": "text", "text": "C Additional Preliminaries ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "C.1 Elementary Tensor Algebra ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We review basic definitions and elementary tensor algebra used throughout the paper. For any positive integer $k$ , a $k$ -tensor $\\mathbf{T}$ is defined as a multilinear real function that maps $k$ vectors to a real number, i.e., $\\mathbf{T}:\\mathbb{R}^{d}\\times\\cdots\\times\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ . A $k$ -tensor $\\mathbf{T}$ can also be viewed as a multidimensional array, where each entry is associated with $k$ indices $i_{1},\\ldots,i_{k}$ in $[d]$ and equals $\\mathbf{T}_{i_{1},\\dots,i_{k}}=\\mathbf{T}(\\mathbf{e}_{i_{1}},\\dots,\\mathbf{e}_{i_{k}})$ . ", "page_idx": 18}, {"type": "text", "text": "Given a vector $\\mathbf{w}\\in\\mathbb{R}^{d}$ , $\\mathbf{w}=(\\mathbf{w}_{1},\\hdots,\\mathbf{w}_{d})^{\\top}$ , the $k$ -product-tensor $\\mathbf{w}^{\\otimes k}$ is defined by ", "page_idx": 18}, {"type": "equation", "text": "$$\n(\\mathbf{w}^{\\otimes k})_{i_{1},\\dots,i_{k}}:=\\mathbf{w}_{i_{1}}\\mathbf{w}_{i_{2}}\\cdot\\cdot\\cdot\\mathbf{w}_{i_{k}},\\;\\forall i_{1},\\dots,i_{k}\\in[d].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Given a $k$ -tensor $\\mathbf{T}$ and an $l$ -tensor $\\mathbf{T}^{\\prime}$ (with $l\\geq k_{\\mathrm{.}}$ ), the inner product (or contraction) between T and T\u2032 is defined by (\u27e8T, T\u2032\u27e9)ik+1,...,il :=  id1,i2,...,ik(T)i1,i2,...,ik(T\u2032)i1,i2,...,ik,ik+1,...,il. In other words, the inner product of a -tensor and an -tensor yields an -tensor. When , the inner product between $\\mathbf{T}$ and $\\mathbf{T}^{\\prime}$ is a real number. In particular, for vectors $\\mathbf{w},\\mathbf{v}\\in\\mathbb{R}^{d}$ , and any $k\\geq1$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\langle\\mathbf{w}^{\\otimes k},\\mathbf{v}^{\\otimes k}\\rangle=(\\mathbf{w}\\cdot\\mathbf{v})^{k},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\mathbf{w}\\cdot\\mathbf{v}$ is the standard inner product between vectors. ", "page_idx": 18}, {"type": "text", "text": "A tensor $\\mathbf{T}$ is symmetric if $(\\mathbf{T})_{...,i,...,j,...}\\,=\\,(\\mathbf{T})_{...,j,...,i,...}$ . We can symmetrize a $k$ -tensor $\\mathbf{T}$ by summing up all its copies with permuted indices $i_{1},\\ldots,i_{k}$ and dividing the sum by $k!$ , which is the total number of permutations. We define the symmetrization operator of a $k$ -tensor $\\mathbf{T}$ by ", "page_idx": 18}, {"type": "equation", "text": "$$\n(\\mathrm{Sym}(\\mathbf{T}))_{i_{1},\\dots,i_{k}}=\\frac{1}{k!}\\sum_{\\pi\\in S_{k}}(\\mathbf{T})_{i_{\\pi(1)},\\dots,i_{\\pi(k)}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "When $k=2$ , this reduces to the symmetrization of a square matrix: $\\mathrm{Sym}(\\mathbf{T})=(1/2)(\\mathbf{T}+\\mathbf{T}^{\\top})$ ", "page_idx": 18}, {"type": "text", "text": "Let us provide some useful observations about tensor algebra. ", "page_idx": 18}, {"type": "text", "text": "Fact C.1. Let w $\\in\\mathbb{R}^{d}$ and let $\\mathbf{T}$ be any $k$ -tensor. Then, ", "page_idx": 18}, {"type": "text", "text": "(1) The inner product between $\\mathbf{w}^{\\otimes k}$ and $\\mathrm{Sym}(\\mathbf{T})$ is equal to the inner product between $\\mathbf{w}^{\\otimes k}$ and $\\mathbf{T}$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\langle\\mathbf{w}^{\\otimes k},\\operatorname{Sym}(\\mathbf{T})\\rangle=\\langle\\mathbf{w}^{\\otimes k},\\mathbf{T}\\rangle.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "(2) If T is a symmetric tensor, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\nabla\\big(\\langle\\mathbf{T},\\mathbf{w}^{\\otimes k}\\rangle\\big)=k\\langle\\mathbf{T},\\mathbf{w}^{\\otimes k-1}\\rangle\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. To prove the first statement in Fact C.1, note that direct calculation yields: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\mathbf{w}^{\\otimes k},\\mathrm{Sym}(\\mathbf{T})\\rangle=\\displaystyle\\sum_{i_{1},\\dots,i_{k}}\\mathbf{w}_{i_{1}}\\cdot\\cdot\\cdot\\mathbf{w}_{i_{k}}\\frac{1}{k!}\\sum_{\\pi\\in S}(\\mathbf{T})_{i_{\\pi(1)},\\dots,i_{\\pi(k)}}}\\\\ &{\\qquad\\qquad\\qquad=\\displaystyle\\frac{1}{k!}\\sum_{\\pi\\in S}\\sum_{i_{1},\\dots,i_{k}}\\mathbf{w}_{i_{\\pi(1)}}\\cdot\\cdot\\cdot\\mathbf{w}_{i_{\\pi(k)}}(\\mathbf{T})_{i_{\\pi(1)},\\dots,i_{\\pi(k)}}}\\\\ &{\\qquad\\qquad=\\displaystyle\\sum_{i_{1},\\dots,i_{k}}\\mathbf{w}_{i_{1}}\\cdot\\cdot\\cdot\\mathbf{w}_{i_{k}}(\\mathbf{T})_{i_{1},\\dots,i_{k}}=\\langle\\mathbf{w}^{\\otimes k},\\mathbf{T}\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Next for the second statement, let $f(\\mathbf{w})\\;=\\;\\langle\\mathbf{T},\\mathbf{w}^{\\otimes k}\\rangle$ where $\\mathbf{T}$ is a $k$ -tensor. Then, the partial derivative of $f$ w.r.t. ${\\bf w}_{j}$ is ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\frac{\\partial f(\\mathbf{w})}{\\partial\\mathbf{w}_{j}}=\\frac{\\partial}{\\partial\\mathbf{w}_{j}}\\langle\\mathbf{T},\\mathbf{w}^{\\otimes k}\\rangle=\\frac{\\partial}{\\partial\\mathbf{w}_{j}}\\bigg(\\sum_{i_{1},\\dots,i_{k}}(\\mathbf{T})_{i_{1},\\dots,i_{k}}\\mathbf{w}_{i_{1}}\\cdot\\cdot\\cdot\\mathbf{w}_{i_{k}}\\bigg)}\\\\ {\\displaystyle}&{=\\sum_{i_{2},\\dots,i_{k}}(\\mathbf{T})_{j,i_{2},\\dots,i_{k}}\\mathbf{w}_{i_{2}}\\cdot\\cdot\\cdot\\mathbf{w}_{i_{k}}+\\sum_{i_{1},i_{3}\\dots,i_{k}}(\\mathbf{T})_{i_{1},j,i_{3},\\dots,i_{k}}\\mathbf{w}_{i_{1}}\\mathbf{w}_{i_{3}}\\cdot\\cdot\\cdot\\mathbf{w}_{i_{k}}+}\\\\ {\\displaystyle}&{\\qquad\\cdot\\cdot+\\sum_{i_{1},\\dots,i_{k-1}}(\\mathbf{T})_{i_{1},\\dots,i_{k-1},j}\\mathbf{w}_{i_{1}}\\cdot\\cdot\\cdot\\mathbf{w}_{i_{k-1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Thus if $\\mathbf{T}$ is symmetric, then $\\nabla\\big(\\langle\\mathbf{T},\\mathbf{w}^{\\otimes k}\\rangle\\big)=k\\langle\\mathbf{T},\\mathbf{w}^{\\otimes k-1}\\rangle$ . ", "page_idx": 19}, {"type": "text", "text": "C.2 Hermite Polynomials and Hermite Tensors ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We make use of the normalized probabilist\u2019s Hermite polynomial, defined by ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{he}_{k}(z)=\\frac{(-1)^{k}}{\\sqrt{k!}}\\exp\\bigg(\\frac{z^{2}}{2}\\bigg)\\frac{\\mathrm{d}^{k}}{\\mathrm{d}z^{k}}\\exp\\bigg(-\\frac{z^{2}}{2}\\bigg).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We will heavily use the following properties of the normalized Hermite polynomials [AS68]: ", "page_idx": 19}, {"type": "text", "text": "Fact C.2. Hermite polynomials satisfy the following properties: ", "page_idx": 19}, {"type": "text", "text": "1. (Orthonormality) ${\\bf E}_{z\\sim\\mathcal{N}(0,1)}[\\mathrm{he}_{k}(z)\\mathrm{he}_{j}(z)]=\\mathbb{1}\\{k=j\\}.$ ", "page_idx": 19}, {"type": "text", "text": "Given a vector $\\mathbf{x}\\in\\mathbb{R}^{d}$ , we can then define the (normalized) Hermite multivariate tensor by [Rah17]: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbf{He}_{k}(\\mathbf{x}))_{i_{1},\\ldots,i_{k}}:=\\left({\\frac{\\alpha_{1}!\\cdot\\cdot\\cdot\\alpha_{d}!}{k!}}\\right)^{1/2}\\operatorname{he}_{\\alpha_{1}}(\\mathbf{x}_{1})\\dots\\operatorname{he}_{\\alpha_{d}}(\\mathbf{x}_{d}),{\\mathrm{~where~}}\\alpha_{j}=\\sum_{l=1}^{k}\\mathbb{1}\\{i_{l}=j\\},\\forall j\\in[d].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "For Hermite tensors, we have the following facts: ", "page_idx": 19}, {"type": "text", "text": "Fact C.3. Let x be a $d_{\\cdot}$ -dimensional standard Gaussian random vector. ", "page_idx": 19}, {"type": "text", "text": "1. For any $k$ -tensor A and $j$ -tensor $\\mathbf{B}$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{\\mathbf{x}\\sim\\mathcal{N}_{d}}{\\mathbf{E}}[\\langle\\mathbf{He}_{k}(\\mathbf{x}),\\mathbf{A}\\rangle\\langle\\mathbf{He}_{j}(\\mathbf{x}),\\mathbf{B}\\rangle]=\\mathbb{1}\\{k=j\\}\\langle\\mathrm{Sym}(\\mathbf{A}),\\mathrm{Sym}(\\mathbf{B})\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "2. For any $\\mathbf{w}\\in\\mathbb{R}^{d}$ such that $\\begin{array}{r}{\\mathbf{w}\\|_{2}=1,\\,\\mathrm{he}_{k}(\\mathbf{w}\\cdot\\mathbf{x})=\\langle\\mathbf{He}(\\mathbf{x}),\\mathbf{w}^{\\otimes k}\\rangle.}\\end{array}$ . ", "page_idx": 19}, {"type": "text", "text": "C.3 Loss and Gradients ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We consider the $L_{2}^{2}$ (or square) loss, defined by ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{2}^{\\sigma}(\\mathbf{w}):=\\underset{(\\mathbf{x},y)\\sim\\mathcal{D}}{\\mathbf{E}}[(\\sigma(\\mathbf{w}\\cdot\\mathbf{x})-y)^{2}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Let $\\begin{array}{r}{\\mathbf{w}^{*}\\ \\in\\ \\mathrm{argmin}_{\\mathbf{w}\\in\\mathbb{S}^{d-1}}\\mathbf{\\mathcal{L}}_{2}^{\\sigma}(\\mathbf{w})}\\end{array}$ , and denote the minimum value of the $L_{2}^{2}$ loss by $\\mathrm{OPT:=}$ $\\mathrm{min}_{\\mathbf{w}\\in\\mathbb{S}^{d-1}}\\,\\mathcal{L}_{2}^{\\sigma}(\\mathbf{w})$ . Furthermore, let us define the \u201cnoiseless\u201d $L_{2}^{2}$ loss by ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{L}_{2}^{*\\sigma}(\\mathbf{w}):=\\underset{\\mathbf{x}\\sim\\mathcal{N}_{d}}{\\mathbf{E}}[(\\sigma(\\mathbf{w}\\cdot\\mathbf{x})-\\sigma(\\mathbf{w}^{*}\\cdot\\mathbf{x}))^{2}].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We observe that the $L_{2}^{2}$ loss is determined by the inner product between w and $\\mathbf{w}^{*}$ , therefore, to obtain error $O(\\mathrm{OPT})\\,\\bar{+}\\,\\epsilon$ , it suffices to minimize the angle between w and $\\mathbf{w}^{*}$ . Concretely, we have: ", "page_idx": 19}, {"type": "text", "text": "Claim C.4. Let $\\mathbf{w}\\in\\mathbb{R}^{d}$ be a unit vector. Then, the $L_{2}^{2}$ loss $\\mathcal{L}_{2}^{\\sigma}(\\mathbf{w})$ satisfies: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{L}_{2}^{\\sigma}(\\mathbf{w})\\leq2\\mathrm{OPT}+4\\bigg(1-\\sum_{k\\geq k^{*}}c_{k}^{2}(\\mathbf{w}\\cdot\\mathbf{w}^{*})^{k}\\bigg).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. Recalling that the activation $\\sigma$ is normalized so that $\\mathbf{E}_{\\mathbf{x}\\sim\\mathcal{N}_{d}}[\\sigma^{2}(\\mathbf{w}\\cdot\\mathbf{x})]=\\mathbf{E}_{\\mathbf{x}\\sim\\mathcal{N}_{d}}[\\sigma^{2}(\\mathbf{w}^{*}$ \u00b7 $\\mathbf{x})]=1$ , we can simplify the $L_{2}^{2}$ loss to ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{2}^{\\sigma}(\\mathbf{w})=2\\Big(1-\\underset{(\\mathbf{x},y)\\sim\\mathcal{D}}{\\mathbf{E}}[y\\sigma(\\mathbf{w}\\cdot\\mathbf{x})]\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The noiseless $L_{2}^{2}$ loss admits the following decomposition: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{2}^{*\\sigma}(\\mathbf{w})=2\\Big(1-\\underset{\\mathbf{x}\\sim\\mathcal{N}_{d}}{\\mathbf{E}}[\\sigma(\\mathbf{w}\\cdot\\mathbf{x})\\sigma(\\mathbf{w}^{*}\\cdot\\mathbf{x})]\\Big)}\\\\ &{\\quad\\quad\\quad=2\\bigg(1-\\underset{\\mathbf{x}\\sim\\mathcal{N}_{d}}{\\mathbf{E}}\\left[\\sum_{k\\geq k^{*}}c_{k}\\langle\\mathbf{He}_{k}(\\mathbf{x}),\\mathbf{w}^{\\otimes k}\\rangle\\sum_{k^{\\prime}\\geq k^{*}}c_{k^{\\prime}}\\langle\\mathbf{He}_{k^{\\prime}}(\\mathbf{x}),\\mathbf{w}^{*\\otimes k^{\\prime}}\\rangle\\right]\\bigg)}\\\\ &{\\quad\\quad\\quad=2\\bigg(1-\\underset{k\\geq k^{*}}{\\sum}c_{k}^{2}(\\mathbf{w}\\cdot\\mathbf{w}^{*})^{k}\\bigg),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where in the last equality we used the orthonormality property of Hermite tensors (Fact C.3). Further, using Young\u2019s inequality and the definitions of OPT and $\\mathcal{L}_{2}^{*\\sigma}(\\mathbf{w})$ , we also have $\\mathcal{L}_{2}^{\\sigma}(\\mathbf{w})\\leq2\\mathrm{OPT}+$ $2\\mathcal{L}_{2}^{*\\bar{\\sigma}}(\\mathbf{w})$ , which combined with Equation (24) leads to ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{L}_{2}^{\\sigma}(\\mathbf{w})\\leq2\\mathrm{OPT}+4\\bigg(1-\\sum_{k\\geq k^{*}}c_{k}^{2}(\\mathbf{w}\\cdot\\mathbf{w}^{*})^{k}\\bigg).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Remark C.5. Equation (24) suggests that even in the realizable case, some assumption on boundedness of $\\sum_{k\\ge k^{*}}\\bar{k}c_{k}^{2}$ (see Assumption $I(i i i))$ may be necessary to have a nontrivial bound on the $L_{2}^{2}$ loss. Consider an algorithm that outputs a vector w such that w $\\mathbf{\\nabla}\\cdot\\mathbf{w}^{*}=1-\\alpha$ for some $\\alpha\\in(0,1)$ (if $\\alpha=0$ , $\\mathbf{w}=\\mathbf{w}^{*}$ since both vectors are on the unit sphere). Since $\\begin{array}{r}{\\sum_{k\\geq k^{*}}c_{k}^{2}=1}\\end{array}$ , we can also write $\\begin{array}{r}{\\mathcal L_{2}^{*\\sigma}(\\mathbf w)=2\\sum_{k\\geq k^{*}}c_{k}^{2}(1-(1-\\alpha)^{k})}\\end{array}$ . For $k=\\Omega(1/\\alpha),1-(1-\\alpha)^{k}\\approx k\\alpha$ . Thus, $i f$ we want an algorithm that works generically for any target accuracy, $\\sum_{k\\geq k^{*}}k c_{k}^{2}$ ought to be bounded. ", "page_idx": 20}, {"type": "text", "text": "Remark C.6. Even though for $\\|\\mathbf{w}\\|_{2}=1$ we have $\\begin{array}{r}{\\mathrm{he}_{k}(\\mathbf{w}\\cdot\\mathbf{x})=\\langle\\mathbf{H}\\mathbf{e}_{k}(\\mathbf{x}),\\mathbf{w}^{\\otimes k}\\rangle.}\\end{array}$ , the gradients with respect to w of these two functions are different in general. For example, for $k=2$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\langle\\mathbf{He}_{2}(\\mathbf{x}),\\mathbf{w}^{\\otimes2}\\rangle=(1/\\sqrt{2})((\\mathbf{w}\\cdot\\mathbf{x})^{2}-\\|\\mathbf{w}\\|_{2}^{2})\\ \\ a n d\\ \\ \\mathrm{he}_{2}(\\mathbf{w}\\cdot\\mathbf{x})=(1/\\sqrt{2})((\\mathbf{w}\\cdot\\mathbf{x})^{2}-1),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which are equal in function value, but ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla\\langle{\\mathbf H}{\\mathbf e}_{2}({\\mathbf x}),{\\mathbf w}^{\\otimes2}\\rangle=\\sqrt{2}(({\\mathbf w}\\cdot{\\mathbf x}){\\mathbf x}-{\\mathbf w})=2\\langle{\\mathbf H}{\\mathbf e}_{2}({\\mathbf x}),{\\mathbf w}\\rangle,}\\\\ &{\\qquad\\quad\\nabla\\mathrm{h}{\\mathbf e}_{2}({\\mathbf x})=\\sqrt{2}({\\mathbf w}\\cdot{\\mathbf x}){\\mathbf x}=\\sqrt{2}\\mathrm{h}{\\mathbf e}_{1}({\\mathbf w}\\cdot{\\mathbf x}){\\mathbf x},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "are different. In particular, for the derivative of the left-hand side of Equation (24) to be equal to the derivative of its right-hand side, we need to use the tensor form of Hermite polynomials, because to ensure interchangeability of differentiation and summation, the sequence needs to be uniformly convergent. Note that $\\begin{array}{r}{\\mathbf{E}_{\\mathbf{x}\\sim\\mathcal{N}_{d}}[\\sum_{k\\geq k^{*}}c_{k}\\langle\\mathbf{He}_{k}(\\mathbf{x}),\\mathbf{w}^{\\otimes k}\\rangle\\sum_{k^{\\prime}\\geq k^{*}}c_{k^{\\prime}}\\langle\\mathbf{He}_{k^{\\prime}}(\\mathbf{x}),\\mathbf{w}^{*\\otimes k^{\\prime}}\\rangle]}\\end{array}$ converges to $\\begin{array}{r}{\\sum_{k\\geq k^{*}}c_{k}^{2}(\\mathbf{w}\\cdot\\mathbf{w})^{k}}\\end{array}$ uniformly for all w $\\in\\ \\mathbb{R}^{d}$ , but the sequence $\\begin{array}{r}{\\mathbf{E}_{\\mathbf{x}\\sim\\mathcal{N}_{d}}\\sum_{k\\geq k^{*}}c_{k}\\mathrm{he}_{k}(\\mathbf{w}\\ .}\\end{array}$ $\\begin{array}{r}{\\mathbf{x})\\sum_{k^{\\prime}\\geq k^{*}}c_{k^{\\prime}}\\mathrm{he}_{k^{\\prime}}(\\mathbf{w}^{*}\\cdot\\mathbf{x})\\big]}\\end{array}$ converges to $\\begin{array}{r}{\\sum_{k}c_{k}^{2}(\\mathbf{w}\\cdot\\mathbf{w})^{k}}\\end{array}$ only when $\\|\\mathbf{w}\\|_{2}=1$ , since it requires $\\mathbf{w}\\cdot\\mathbf{x}\\sim{\\mathcal{N}}(0,1)$ to ensure that $\\mathbf{E}_{\\mathbf{x}\\sim\\mathcal{N}_{d}}[\\mathrm{he}_{k}(\\mathbf{w}\\cdot\\mathbf{x})\\mathrm{he}_{j}(\\mathbf{w}^{*}\\cdot\\mathbf{x})]=\\mathbb{1}\\{k=j\\}(\\mathbf{w}\\cdot\\mathbf{w}^{*})^{k}$ . ", "page_idx": 20}, {"type": "text", "text": "As observed in Remark C.6, the gradients of $\\operatorname{he}_{k}(\\mathbf{w}\\cdot\\mathbf{x})$ and $\\langle\\mathbf{H}\\mathbf{e}_{k}(\\mathbf{x}),\\mathbf{w}^{\\otimes k}\\rangle$ are different in general. Throughout the paper, we will be taking the gradient with respect to the tensor form of $\\sigma(\\mathbf{w}\\cdot\\mathbf{x})$ ; in other words, $\\begin{array}{r}{\\nabla\\sigma(\\mathbf{w}\\cdot\\mathbf{x})=\\nabla(\\sum_{k\\geq k^{*}}c_{k}\\langle\\mathbf{H}\\mathbf{e}_{k}(\\mathbf{x}),\\mathbf{w}^{\\otimes k}\\rangle)}\\end{array}$ . ", "page_idx": 20}, {"type": "text", "text": "D Full Version of Section 2 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section, we show how to get an initial parameter vector $\\mathbf{w}^{0}$ such that $\\mathbf{w}^{0}\\cdot\\mathbf{w}^{*}\\,=\\,1\\,-\\,\\epsilon_{0}$ for some small constant $\\epsilon_{0}$ . The main technique is a tensor PCA algorithm that finds the principal ", "page_idx": 20}, {"type": "text", "text": "component of a noisy degree- $k$ -Chow tensor for any $k\\geq k^{*}$ , as long as $\\mathrm{OPT}\\lesssim c_{k}^{2}$ . Such a degree- $k$ Chow tensor is defined by $\\mathbf{C}_{k}=\\mathbf{E}_{(\\mathbf{x},y)\\sim\\mathcal{D}}[y\\mathbf{H}\\mathbf{e}_{k}(\\mathbf{x})]$ , and we denote its noiseless counterpart by ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbf{C}_{k}^{*}=\\underset{\\mathbf{x}\\sim\\mathcal{N}_{d}}{\\mathbf{E}}[\\sigma(\\mathbf{w}^{*}\\cdot\\mathbf{x})\\mathbf{H}\\mathbf{e}_{k}(\\mathbf{x})]=\\underset{\\mathbf{x}\\sim\\mathcal{N}_{d}}{\\mathbf{E}}\\left[\\sum_{j\\geq k^{*}}c_{j}\\langle\\mathbf{H}\\mathbf{e}_{j}(\\mathbf{x}),\\mathbf{w}^{*\\otimes j}\\rangle\\mathbf{H}\\mathbf{e}_{k}(\\mathbf{x})\\right].\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Furthermore, let us denote the difference between $\\mathbf{C}_{k}$ and $\\mathbf{C}_{k}^{*}$ by ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbf{H}_{k}:=\\mathbf{C}_{k}-\\mathbf{C}_{k}^{*}=\\underset{(\\mathbf{x},y)\\sim\\mathcal{D}}{\\mathbf{E}}[(y-\\sigma(\\mathbf{w}^{*}\\cdot\\mathbf{x}))\\mathbf{H}\\mathbf{e}_{k}(\\mathbf{x})].\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Note that since $\\mathbf{H}\\mathbf{e}_{k}(\\mathbf{x})$ is a symmetric tensor for any $\\mathbf{x}$ , all $\\mathbf{C}_{k},\\mathbf{C}_{k}^{*}$ and ${\\bf{H}}_{k}$ are symmetric tensors. We use the following matrix unfolding operator that maps a k-tensor to a matrix in Rdl\u00d7dk\u2212l. Concretely, given a -tensor , we define: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{M}\\mathsf{a t}_{(l,k-l)}(\\mathbf{T})_{i_{1}+(i_{2}-1)d+\\cdots+(i_{l}-1)d^{l-1},j_{1}+\\cdots+(j_{k-l}-1)d^{k-l-1}}:=(\\mathbf{T})_{i_{1},i_{2},\\ldots,i_{l},j_{1},\\ldots,j_{k-l-1}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "for all $i_{1},\\ldots,i_{l},j_{1},\\ldots,j_{k-l}\\in[d]$ . ", "page_idx": 21}, {"type": "text", "text": "For notational convenience, we also define the \u2018vectorize\u2019 operator and \u2018tensorize\u2019 operator, which map a vector $\\mathbf{v}\\in\\mathbb{R}^{d}$ to an $l_{\\cdot}$ -tensor for any integer $l$ , and vice versa. In detail, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathsf{T e n s o r}(\\mathbf{v})_{i_{1},\\ldots,i_{l}}:=\\mathbf{v}_{i_{1}+(i_{2}-1)d+\\cdots+(i_{l}-1)d^{l-1}},\\;\\forall i_{1},\\ldots,i_{l}\\in[d];\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and conversely, we define ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathsf{V e c}(\\mathbf{v}^{\\otimes l})_{i_{1}+(i_{2}-1)d+\\cdots+(i_{l}-1)d^{l-1}}:=\\mathbf{v}_{i_{1}}\\mathbf{v}_{i_{2}}\\ldots\\mathbf{v}_{i_{l}},\\;\\forall i_{1},\\ldots,i_{l}\\in[d].\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Finally, given a vector $\\mathbf{v}\\in\\mathbb{R}^{d}$ , we can also convert this vector to a matrix of size $\\mathbb{R}^{d\\times d^{l-1}}$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb M\\mathsf{a t}_{(1,l-1)}(\\mathbf v)_{i,j_{1},\\ldots,j_{l-1}}=\\mathbf v_{i+(j_{1}-1)d+\\cdots+(j_{l-1}-1)d^{l-1}},\\;\\forall i,j_{1},\\ldots,j_{l-1}\\in[d].}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Some simple facts on the algebra of the unfolded matrix are in order. ", "page_idx": 21}, {"type": "text", "text": "Fact D.1. Let $\\mathbf{T}$ be a symmetric $k$ -tensor, and let $\\mathbf{r}\\in\\mathbb{R}^{d^{k-l}}$ , $\\mathbf{v}\\in\\mathbb{R}^{d}$ . Then ", "page_idx": 21}, {"type": "text", "text": "1. For any index $i\\in[d^{l}]$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n(\\mathsf{M}\\mathsf{a t}_{(l,k-l)}(\\mathbf{T})\\mathbf{r})_{i}=\\bigg\\langle\\mathbf{T},\\mathbf{e}_{i_{1}^{\\prime}}\\otimes\\ldots\\otimes\\mathbf{e}_{i_{l}^{\\prime}}\\otimes\\mathsf{T e n s o r}(\\mathbf{r})\\bigg\\rangle,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "2. For any index $j\\in[d^{k-l}]$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n(\\mathsf{M}\\mathsf{a t}_{(l,k-l)}(\\mathbf{T})^{\\top}\\mathbf{v})_{j}=\\bigg\\langle\\mathbf{T},\\mathsf{T e n s o r}(\\mathbf{v})\\otimes\\mathbf{e}_{j_{1}^{\\prime}}\\otimes\\ldots\\otimes\\mathbf{e}_{j_{k-l}^{\\prime}}\\bigg\\rangle,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "3. Finally, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbf{v}^{\\top}\\mathsf{M a t}_{(l,k-l)}(\\mathbf{T})\\mathbf{r}=\\langle\\mathbf{T},\\mathsf{T e n s o r}(\\mathbf{v})\\otimes\\mathsf{T e n s o r}(\\mathbf{r})\\rangle.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. First we show that for a symmetric tensor $\\mathbf{T}$ , the linear transformation $\\mathsf{M a t}_{(l,k-l)}(\\mathbf{T})\\mathbf{r}$ of vector $\\mathbf{r}\\in\\mathbb{R}^{d^{k-l}}$ is equal to the tensor inner product $\\langle\\mathbf{T},\\mathsf{T e n s o r}(\\mathbf{r})\\rangle$ . This can be proved by direct calculations that for any $i\\in[d^{l}]$ : ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{(\\mathsf{M a t}_{(l,k-l)}(\\mathbf{T})\\mathbf{r})_{i}=\\sum_{j=1}^{d^{(k-l)}}\\mathsf{M a t}_{(l,k-l)}(\\mathbf{T})_{i,j}\\mathbf{r}_{j}}}\\\\ &{=\\sum_{j_{1},\\dots,j_{k-l}\\in[d]}\\mathsf{M a t}_{(l,k-l)}(\\mathbf{T})_{i,j_{1}+\\dots+(j_{k-l}-1)d^{k-l-1}}\\mathbf{r}_{j_{1}+\\dots+(j_{k-l}-1)d^{k-l-1}}}\\\\ &{=\\sum_{j_{1},\\dots,j_{k-l}\\in[d]}(\\mathbf{T})_{i_{1}^{\\prime},\\dots,i_{l}^{\\prime},j_{1},\\dots,j_{k-l}}\\mathsf{T e n s o r}(\\mathbf{r})_{j_{1},\\dots,j_{k-l}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $i_{1}^{\\prime},\\ldots,i_{j}^{\\prime}\\in[d]$ satisfies $i=i_{1}^{\\prime}+(i_{2}^{\\prime}-1)d+\\cdot\\cdot\\cdot+(i_{l}^{\\prime}-1)d^{l-1}$ . Observe that the summation above further equals ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad(\\mathsf{M a t}_{(l,k-l)}(\\mathbf{T})\\mathbf{r})_{i}}\\\\ &{=\\displaystyle\\sum_{i_{1},\\dots,i_{l}\\in[d]}\\sum_{j_{1},\\dots,j_{k-l}\\in[d]}(\\mathbf{T})_{i_{1},\\dots,i_{l},j_{1},\\dots,j_{k-l}}\\mathsf{T e n s o r}(\\mathbf{r})_{j_{1},\\dots,j_{k-l}}\\mathbb{I}\\left\\{i_{1}=i_{1}^{\\prime}\\right\\}\\dots\\mathbb{1}\\left\\{i_{l}=i_{l}^{\\prime}\\right\\}}\\\\ &{=\\displaystyle\\sum_{i_{1},\\dots,i_{l},j_{1},\\dots,j_{k-l}\\in[d]}(\\mathbf{T})_{i_{1},\\dots,i_{l},j_{1},\\dots,j_{k-l}}(\\mathbf{e}_{i_{1}^{\\prime}}\\otimes\\dots\\otimes\\mathbf{e}_{i_{l}^{\\prime}}\\otimes\\mathsf{T e n s o r}(\\mathbf{r}))_{i_{1},\\dots,i_{l},j_{1},\\dots,j_{k-l}}}\\\\ &{=\\left\\langle\\mathbf{T},\\mathbf{e}_{i_{1}^{\\prime}}\\otimes\\dots\\otimes\\mathbf{e}_{i_{l}^{\\prime}}\\otimes\\mathsf{T e n s o r}(\\mathbf{r})\\right\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Similarly, for a symmetric tensor $\\mathbf{T}$ and any vector $\\mathbf{v}\\in\\mathbb{R}^{d^{l}}$ , and any index $j\\in[d^{k-l}]$ , it holds ", "page_idx": 22}, {"type": "equation", "text": "$$\n(\\mathsf{M}\\mathsf{a t}_{(l,k-l)}(\\mathbf{T})^{\\top}\\mathbf{v})_{j}=\\bigg\\langle\\mathbf{T},\\mathsf{T e n s o r}(\\mathbf{v})\\otimes\\mathbf{e}_{j_{1}^{\\prime}}\\otimes\\ldots\\otimes\\mathbf{e}_{j_{k-l}^{\\prime}}\\bigg\\rangle,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $j_{1}^{\\prime}+\\cdot\\cdot\\cdot+(j_{k-l}^{\\prime}-1)d^{k-l-1}=j$ ", "page_idx": 22}, {"type": "text", "text": "Finally, combining Equation (26) and Equation (25) we get that for any $\\mathbf{v}\\in\\mathbb{R}^{d^{l}},\\mathbf{r}\\in\\mathbb{R}^{d^{k-l}}$ , the quadratic form $\\mathbf{v}^{\\top}\\mathsf{M a t}_{(l,k-l)}(\\mathbf{T})\\mathbf{r}$ equals $\\mathbf{\\dot{v}}^{\\top}\\mathbf{Mat}_{(l,k-l)}(\\bar{\\mathbf{T})}\\mathbf{r}=\\langle\\mathbf{T}$ , Tensor $\\mathbf{\\Psi}(\\mathbf{v})\\otimes$ Tensor $(\\mathbf{r})\\rangle$ . ", "page_idx": 22}, {"type": "text", "text": "Throughout this section, we define $l=k/2$ when $k$ is even, and $l=(k-1)/2$ when $k$ is odd. In other words, $l=\\lfloor k/2\\rfloor$ . We leverage the tensor unfolding algorithm proposed in [RM14], which can be described in short as follows. First we unfold the degree-k Chow tensor to a matrix in Rdl\u00d7dk\u2212l, and find its top-left singular vector $\\mathbf{v}\\in\\mathbb{R}^{d}$ . Then, we calculate the matrix $\\mathsf{M a t}_{(1,l-1)}(\\mathbf{v})$ , and find its top left singular vector u. One can show that this eigenvector u correlates with $\\mathbf{w}^{*}$ significantly. ", "page_idx": 22}, {"type": "text", "text": "Algorithm 3 $k$ -Chow Tensor PCA ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1: Input: Parameters $\\epsilon,k,\\epsilon_{0},c_{k},B_{4}>0$ ; Sample access to $\\mathcal{D}$   \n2: Let $l=\\lfloor k/2\\rfloor$   \n3: Draw $n\\stackrel{*}{=}\\Theta(e^{k}\\log^{k}(B_{4}/\\epsilon)d^{k-l}/(\\epsilon_{0}^{2})+1/\\epsilon)$ samples $\\{(\\mathbf{x}^{(i)},y^{(i)})\\}_{i=1}^{n}$ from $\\mathcal{D}$   \n4: Construct $\\begin{array}{r}{\\widehat{\\mathbf{M}}:=(1/n)\\sum_{i=1}^{n}\\mathsf{M}\\mathsf{a t}_{(l,k-l)}(y^{(i)}\\mathbf{H}\\mathbf{e}_{k}(\\mathbf{x}^{(i)}))}\\end{array}$ ; compute its top left singular vector $\\widehat{\\mathbf{v}}^{*}$ ", "page_idx": 22}, {"type": "text", "text": "5: Compute the top-left singular vector $\\widehat{\\bf{u}}$ of the matrix $\\mathsf{M a t}_{1,l-1}(\\widehat{\\mathbf{v}}^{*})$ ", "page_idx": 22}, {"type": "text", "text": "6: Return: $\\widehat{\\bf{u}}$ ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Our main result for initialization is the following: ", "page_idx": 22}, {"type": "text", "text": "Proposition D.2 (Initialization). Suppose Assumption $^{\\,l}$ holds. Assume that $\\mathrm{OPT}\\leq c_{k^{*}}^{2}/(64k^{*})^{2}$ , and let $\\epsilon_{0}={c_{k^{*}}}/({256k^{*}})$ . Then, Algorithm $^{\\,l}$ applied to Problem 1.1 with $k=k^{*}$ uses ", "page_idx": 22}, {"type": "equation", "text": "$$\nn=\\Theta((k^{*})^{2}e^{k^{*}}\\log^{k^{*}}(B_{4}/\\epsilon)d^{[k^{*}/2]}/(c_{k^{*}}^{2})+1/\\epsilon)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "samples, runs in polynomial time, and outputs a vector $\\mathbf{w}^{0}\\,\\in\\,\\mathbb{S}^{d-1}$ such that $\\mathbf{w}^{0}\\cdot\\mathbf{w}^{*}\\,\\geq\\,1\\,-$ $\\operatorname*{min}\\{1/k^{*},1/2\\}$ . ", "page_idx": 22}, {"type": "text", "text": "We remark here that Algorithm 3 can also be used to find an approximate solution of our agnostic learning problem; however the dependence on the value of OPT is suboptimal, scaling with its square-root. In particular, we have the following proposition: ", "page_idx": 22}, {"type": "text", "text": "Proposition D.3 (Solving the Agnostic Learning Problem Using Tensor PCA). Suppose Assumption $^{\\,l}$ holds. Assume that $\\bar{\\mathrm{OPT}}\\,\\leq\\,\\bar{c_{k^{*}}^{2}}/(64k^{*})^{2}$ and $\\epsilon\\leq1/64$ . Let $\\epsilon_{0}\\,=\\,c_{k^{*}}\\epsilon/16$ . Then, Algorithm $^{\\,l}$ applied to Problem 1.1 with $k=k^{*}$ uses $n=\\Theta(e^{k^{*}}\\log^{k^{*}}(B_{4}/\\epsilon)d^{lceil k^{*}/2\\rceil}/(c_{k^{*}}^{2}\\epsilon^{2})+1/\\epsilon)$ samples, runs in polynomial time, and outputs a vector $\\mathbf{w}^{0}\\in\\mathbb{S}^{d-1}$ such that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbf{w}^{0}\\cdot\\mathbf{w}^{*}\\ge1-\\frac{4}{c_{k^{*}}}\\sqrt{\\mathrm{OPT}}-2\\epsilon/3.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Furthermore, the $L_{2}^{2}$ error of $\\mathbf{w}^{0}$ is at most ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathcal{L}_{2}^{\\sigma}(\\mathbf{w}^{0})=O\\bigg(C_{k^{*}}\\bigg(\\frac{1}{c_{k^{*}}}\\sqrt{\\mathrm{OPT}}+\\epsilon\\bigg)\\bigg).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Thus, when $\\mathrm{OPT}\\,=\\,0$ (i.e., in the realizable cases), applying Algorithm 3 with $O(d^{\\lceil k^{*}/2\\rceil}/\\epsilon^{2})$ samples recovers the hidden vector $\\mathbf{w}^{*}$ . ", "page_idx": 23}, {"type": "text", "text": "Roadmap To prove Proposition D.2 and Proposition D.3 we need three main ingredients. First, we will show (in Lemma D.4 and its corollary Corollary D.5) that the top-left singular vector $\\mathbf{v}^{*}$ of the unfolded matrix $\\mathbf{M}:=\\mathsf{M a t}_{(l,k-l)}(\\mathbf{E}_{(\\mathbf{x},y)\\sim\\mathcal{D}}[y\\mathbf{H}\\mathbf{e}_{k}(\\mathbf{\\dot{x}})])$ correlates significantly with the vectorized $l$ -product tensor, $\\mathsf{V e c}(\\mathbf{w}^{*\\otimes l})$ . This indicates that $\\mathbf{v}^{*}$ contains rich information about the direction of $\\mathbf{w}^{*}$ . However, since we only have access to $\\widehat{\\bf M}$ , the empirical estimation of M, we need to ensure that the top-left singular vector of $\\widehat{\\bf M}$ , denoted by ${\\widehat{\\mathbf{v}}}^{*}$ , is close to $\\mathbf{v}^{*}$ . This is proved in Lemma D.13 using sophisticated matrix concentr ation bounds. In   particular, in Equation (35) we guarantee that the angle between $\\widehat{\\mathbf{v}}^{*}$ and $\\mathbf{v}^{*}$ is bounded by $O(\\epsilon_{0}/c_{k})$ for any small constant $\\epsilon_{0}>0$ , provided that we take $\\tilde{\\Theta}(d^{[k/2]}/\\epsilon_{0}^{2})$ samples and assume tha\u221at $\\mathrm{OPT}\\lesssim c_{k}^{2}$ . The inner product between $\\widehat{\\mathbf{v}}^{*}$ and $\\mathsf{V e c}(\\mathbf{w}^{*\\otimes l})$ can then be bounded below by $1-O(({\\sqrt{\\mathrm{OPT}}}+\\epsilon_{0})/c_{k})$ . Combining with Corollary D.5, this implies that $\\widehat{\\mathbf{v}}^{*}$ correlates with $\\mathsf{V e c}(\\mathbf{w}^{*\\otimes l})$ significantly. Finally, in Lemma D.17 we show that after unfolding the $\\mathbb{R}^{d^{l}}$ vector $\\widehat{\\mathbf{v}}^{*}$ to an $\\mathbb{R}^{d\\times d^{l-1}}$ matrix, its top-left singular vector u correlates with $\\mathbf{w}^{*}$ significantly; it particular, w  e have $\\mathbf{w}^{*}\\cdot\\mathbf{u}\\gtrsim1-c\\epsilon_{0}$ for some absolute constant $c>0$ . Combining these results and choosing $\\epsilon_{0}\\approx c_{k^{*}}/k^{*}$ , we get Proposition D.2, and choosing $\\epsilon_{0}\\approx c_{k^{*}}\\epsilon$ yields Proposition D.3. ", "page_idx": 23}, {"type": "text", "text": "D.1 Signal in the $k$ -Chow Tensor ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Our first observation is that for any left singular vector $\\mathbf{v}$ of $\\mathsf{M a t}_{(l,k-l)}\\big(\\mathbf{C}_{k}\\big)$ , the singular value $\\rho(\\mathbf{v})$ is close to the inner product between $\\mathbf{v}$ and $\\mathsf{V e c}(\\mathbf{w}^{*\\otimes l})$ , where $l=\\lceil k/2\\rceil$ . Concretely, we have: ", "page_idx": 23}, {"type": "text", "text": "Lemma D.4. Let v be any left singular vector of $\\mathsf{M a t}_{(l,k-l)}(\\mathbf{C}_{k})$ . Then, ", "page_idx": 23}, {"type": "equation", "text": "$$\n|\\rho(\\mathbf{v})-c_{k}(\\mathsf{V e c}(\\mathbf{w}^{*\\otimes l})\\cdot\\mathbf{v})|\\leq\\sqrt{\\mathrm{OPT}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. Recall that the singular value of the left singular vector $\\mathbf{v}$ satisfies ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\rho(\\mathbf{v})=\\operatorname*{max}_{\\mathbf{r}\\in\\mathbb{R}^{d^{k-l}},\\|\\mathbf{r}\\|_{2}=1}\\mathbf{v}^{\\top}\\mathbb{M}\\mathsf{a t}_{(l,k-l)}(\\mathbf{C}_{k})\\mathbf{r}\\overset{(i)}{=}\\operatorname*{max}_{\\mathbf{r}\\in\\mathbb{R}^{k-l},\\|\\mathbf{r}\\|_{2}=1}\\langle\\mathbf{C}_{k},\\mathsf{T e n s o r}(\\mathbf{v})\\otimes\\mathsf{T e n s o r}(\\mathbf{r})\\rangle,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where we used Equation (27) in $(i)$ . Since $\\mathbf{C}_{k}=\\mathbf{C}_{k}^{*}+\\mathbf{H}_{k}$ , we further have ", "page_idx": 23}, {"type": "text", "text": "$\\langle\\mathbf{C}_{k},\\mathsf{T e n s o r}(\\mathbf{v})\\otimes\\mathsf{T e n s o r}(\\mathbf{r})\\rangle=\\langle\\mathbf{C}_{k}^{*},\\mathsf{T e n s o r}(\\mathbf{v})\\otimes\\mathsf{T e n s o r}(\\mathbf{r})\\rangle+\\langle\\mathbf{H}_{k},\\mathsf{T e n s o r}(\\mathbf{v})\\otimes\\mathsf{T e n s o r}(\\mathbf{r})\\rangle.$ We bound both terms above respectively. For the first term, plugging in the definition of $\\mathbf{C}_{k}^{*}$ and using Fact C.3, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\langle\\mathbf{C}_{k}^{*},\\mathsf{T e n s o r}(\\mathbf{v})\\otimes\\mathsf{T e n s o r}(\\mathbf{r})\\rangle}\\\\ &{=\\underset{\\mathbf{x}\\sim\\mathcal{N}_{d}}{\\mathbf{E}}\\Bigg[\\underset{j\\geq k^{*}}{\\sum_{j}}c_{j}\\langle\\mathbf{He}_{j}(\\mathbf{x}),\\mathbf{w}^{*\\otimes j}\\rangle\\langle\\mathbf{He}_{k}(\\mathbf{x}),\\mathsf{T e n s o r}(\\mathbf{v})\\otimes\\mathsf{T e n s o r}(\\mathbf{r})\\rangle\\Bigg]}\\\\ &{\\stackrel{(i)}{=}c_{k}\\bigg\\langle\\mathbf{w}^{*\\otimes k},\\mathrm{Sym}(\\mathsf{T e n s o r}(\\mathbf{v})\\otimes\\mathsf{T e n s o r}(\\mathbf{r}))\\bigg\\rangle}\\\\ &{\\stackrel{(i i)}{=}c_{k}\\underset{i_{1},\\ldots,i_{k}}{\\sum}\\mathbf{w}_{i_{1}}^{*}\\cdots\\mathbf{w}_{i_{l}}^{*}\\mathsf{T e n s o r}(\\mathbf{v})_{i_{1},\\ldots,i_{l}}\\mathbf{w}_{i_{l+1}}^{*}\\cdots\\mathbf{w}_{i_{k}}^{*}\\mathsf{T e n s o r}(\\mathbf{r})_{i_{l+1},\\ldots,i_{k}}}\\\\ &{=c_{k}(\\mathsf{V e c}(\\mathbf{w}^{*\\otimes l})\\cdot\\mathbf{v})(\\mathsf{V e c}(\\mathbf{w}^{*\\otimes k-l})\\cdot\\mathbf{r}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "note that we applied Fact C.3 in equation $(i)$ and Fact C.1(1) in $(i i)$ . Next, for the second term, after applying Cauchy-Schwarz inequality, it holds ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left|\\langle\\mathbf{H}_{k},\\mathsf{T e n s o r}(\\mathbf{v})\\otimes\\mathsf{T e n s o r}(\\mathbf{r})\\rangle\\right|}\\\\ &{=\\bigg|\\underbrace{\\mathbf{E}}_{(\\mathbf{x},\\mathbf{y})\\sim\\mathcal{D}}\\bigg[(y-\\sigma(\\mathbf{w}^{*}\\cdot\\mathbf{x}))\\langle\\mathbf{He}_{k}(\\mathbf{x}),\\mathsf{T e n s o r}(\\mathbf{v})\\otimes\\mathsf{T e n s o r}(\\mathbf{r})\\rangle\\bigg]\\bigg|}\\\\ &{\\leq\\sqrt{\\underset{(\\mathbf{x},\\mathbf{y})\\sim\\mathcal{D}}{\\mathbf{E}}[(y-\\sigma(\\mathbf{w}^{*}\\cdot\\mathbf{x}))^{2}]}\\sqrt{\\underset{\\mathbf{x}\\sim\\mathcal{N}_{d}}{\\mathbf{E}}\\left[(\\langle\\mathbf{He}_{k}(\\mathbf{x}),\\mathsf{T e n s o r}(\\mathbf{v})\\otimes\\mathsf{T e n s o r}(\\mathbf{r})\\rangle)^{2}\\right]}}\\\\ &{=\\sqrt{\\mathrm{OPT}}\\|\\mathrm{Sym}(\\mathsf{T e n s o r}(\\mathbf{v})\\otimes\\mathsf{T e n s o r}(\\mathbf{r}))\\|_{F}\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Since for any $k$ -tensor $A$ we have $\\|\\mathrm{Sym}(A)\\|_{F}\\leq\\|A\\|_{F}$ , and in addition, observe that as $\\|\\mathbf{v}\\|_{2}=$ $\\|\\mathbf{r}\\|_{2}=1$ it holds ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathsf{T e n s o r}(\\mathbf v)\\otimes\\mathsf{T e n s o r}(\\mathbf r)||_{F}^{2}=\\sum_{\\overset{i_{1},\\ldots,i_{l}}{i_{l+1},\\ldots,i_{k}}}\\left(\\mathsf{T e n s o r}(\\mathbf v)\\right)_{i_{1},\\ldots,i_{l}}^{2}\\left(\\mathsf{T e n s o r}(\\mathbf r)\\right)_{i_{l+1},\\ldots,i_{k}}^{2}=\\sum_{i=1}^{d^{l}}\\sum_{j=1}^{d^{k-l}}\\mathbf v_{i}^{2}\\mathbf r_{j}^{2}=1,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "we finally have ", "page_idx": 24}, {"type": "text", "text": "$|\\langle\\mathbf{H}_{k},\\mathsf{T e n s o r}(\\mathbf{v})\\otimes\\mathsf{T e n s o r}(\\mathbf{r})\\rangle|\\leq\\sqrt{\\mathrm{OPT}}\\|\\mathsf{T e n s o r}(\\mathbf{v})\\otimes\\mathsf{T e n s o r}(\\mathbf{r})\\|_{F}=\\sqrt{\\mathrm{OPT}}.$ (30) Combining Equation (28) and Equation (30), we get that for any $\\mathbf{v}\\,\\in\\,\\mathbb{R}^{d^{l}},\\mathbf{r}\\,\\in\\,\\mathbb{R}^{d^{k-l}}$ such that $\\|\\mathbf{v}\\|_{2}=\\|\\bar{\\mathbf{r}}\\|_{2}\\bar{=}1$ , it holds ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbf{v}^{\\top}\\mathsf{M a t}_{(l,k-l)}(\\mathbf{C}_{k})\\mathbf{r}\\leq c_{k}(\\mathsf{V e c}(\\mathbf{w}^{*\\otimes l})\\cdot\\mathbf{v})(\\mathsf{V e c}(\\mathbf{w}^{*\\otimes k-l})\\cdot\\mathbf{r})+\\sqrt{\\mathrm{OPT}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Therefore, the singular value of $\\mathbf{v}$ must satisfy ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\rho(\\mathbf v)\\leq\\underset{\\mathbf r\\in\\mathbb R^{d^{k-l}},\\|\\mathbf r\\|_{2}=1}{\\operatorname*{max}}c_{k}(\\mathsf{V e c}(\\mathbf w^{*\\otimes l})\\cdot\\mathbf v)(\\mathsf{V e c}(\\mathbf w^{*\\otimes k-l})\\cdot\\mathbf r)+\\sqrt{\\mathrm{OPT}}}\\\\ &{\\qquad=c_{k}(\\mathsf{V e c}(\\mathbf w^{*\\otimes l})\\cdot\\mathbf v)+\\sqrt{\\mathrm{OPT}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where in the equation above, we used the observation that as $\\|\\mathsf{V e c}(\\mathbf{w}^{*\\otimes k-l})\\|_{2}=\\|\\mathbf{w}^{*\\otimes k-l}\\|_{F}=1$ , it holds $\\begin{array}{r}{\\operatorname*{max}_{\\mathbf{r}\\in\\mathbb{R}^{d^{k-l}},\\|\\mathbf{r}\\|_{2}=1}(\\mathsf{V e c}(\\mathbf{w}^{*\\otimes k-l})\\cdot\\mathbf{r})=\\|\\mathsf{V e c}(\\mathbf{w}^{*\\otimes k-l})\\|_{2}=1}\\end{array}$ . ", "page_idx": 24}, {"type": "text", "text": "Similarly, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\rho(\\mathbf v)=\\underset{\\mathbf r\\in\\mathbb R^{d^{k-l}},\\|\\mathbf r\\|_{2}=1}{\\operatorname*{max}}\\mathbf v^{\\top}\\mathbb{M}\\mathbf a\\mathbf t_{(l,k-l)}(\\mathbf C_{k})\\mathbf r=\\underset{\\mathbf r\\in\\mathbb R^{d^{k-l}},\\|\\mathbf r\\|_{2}=1}{\\operatorname*{max}}\\langle\\mathbf C_{k},\\mathsf{T e n s o r}(\\mathbf v)\\otimes\\mathsf T\\mathrm{ensor}(\\mathbf r)\\rangle}\\\\ &{\\qquad=\\underset{\\mathbf r\\in\\mathbb R^{d^{k-l}},\\|\\mathbf r\\|_{2}=1}{\\operatorname*{max}}c_{k}(\\mathsf{V e c}(\\mathbf w^{*\\otimes l})\\cdot\\mathbf v)(\\mathsf{V e c}(\\mathbf w^{*\\otimes k-l})\\cdot\\mathbf r)+\\langle\\mathbf H_{k},\\mathsf T\\mathrm{ensor}(\\mathbf v)\\otimes\\mathsf T\\mathrm{ensor}(\\mathbf r)\\rangle}\\\\ &{\\qquad\\geq\\underset{\\mathbf r\\in\\mathbb R^{d^{k-l}},\\|\\mathbf r\\|_{2}=1}{\\operatorname*{max}}c_{k}(\\mathsf{V e c}(\\mathbf w^{*\\otimes l})\\cdot\\mathbf v)(\\mathsf{V e c}(\\mathbf w^{*\\otimes k-l})\\cdot\\mathbf r)-\\sqrt{\\mathrm{OPT}}}\\\\ &{\\qquad=c_{k}(\\mathsf{V e c}(\\mathbf w^{*\\otimes l})\\cdot\\mathbf v)-\\sqrt{\\mathrm{OPT}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "completing the proof of Lemma D.4. ", "page_idx": 24}, {"type": "text", "text": "A direct application of Lemma \u221aD.4 is that the top-left singular vector $\\mathbf{v}^{\\ast}\\in\\mathbb{R}^{d^{l}}$ of $\\mathsf{M a t}_{(l,k-l)}\\big(\\mathbf{C}_{k}\\big)$ has singular value at least $c_{k}-\\sqrt{\\mathrm{OPT}}$ , and in addition, $\\mathbf{v}^{*}$ aligns well with $\\mathsf{V e c}(\\mathbf{w}^{*\\otimes l})$ . ", "page_idx": 24}, {"type": "text", "text": "Corollary D.5. The top-left singular vector $\\mathbf{v}^{*}\\in\\mathbb{R}^{d^{l}}$ of the unfolded tensor $\\mathsf{M a t}_{(l,k-l)}\\big(\\mathbf{C}_{k}\\big)$ has corresp\u221aonding singular value $\\rho(\\mathbf{v}^{*})\\ge c_{k}-\\sqrt{\\mathrm{OPT}}$ . In addition, it holds that $\\mathbf{v}^{*}\\cdot\\mathsf{V e c}(\\mathbf{w}^{*\\otimes l})\\geq$ $1-(2\\sqrt{\\mathrm{OPT}})/c_{k}$ . ", "page_idx": 24}, {"type": "text", "text": "Proof. Plugging in $\\mathbf{v}=\\mathsf{V e c}(\\mathbf{w}^{*\\otimes l})$ to Lemma 2.\u221a2, we get that $\\rho(\\mathsf{V e c}(\\mathbf{w}^{*\\otimes l}))\\geq c_{k}-\\sqrt{\\mathrm{OPT}}$ . Thus, the top singular value must satisfy $\\rho_{1}\\ge c_{k}-\\sqrt{\\mathrm{OPT}}$ . Recall again that as proved\u221a in Lemma 2.2, it holds $\\rho(\\mathbf{v}^{*})\\leq c_{k}\\mathbf{v}^{*}\\cdot\\mathsf{V e c}(\\mathbf{w}^{*\\otimes l})+\\sqrt{\\mathrm{OPT}}$ . Thus, since $\\rho(\\mathsf{V e c}(\\mathbf{w}^{*\\otimes l}))\\geq c_{k}-\\sqrt{\\mathrm{OPT}}$ we have $\\mathbf{v}^{*}\\cdot\\mathsf{V e c}(\\mathbf{w}^{*\\otimes l})\\geq1-(2\\sqrt{\\mathrm{OPT}})/c_{k}$ . \u53e3 ", "page_idx": 24}, {"type": "text", "text": "D.2 Concentration of the Unfolded Tensor Matrix ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We start with some notations. Let us denote $\\mathbf{M}^{(i)}\\ =\\ \\mathsf{M a t}_{(l,k-l)}(y^{(i)}\\mathbf{He}_{k}(\\mathbf{x}^{(i)}))$ for $i~\\in~[n]$ and $\\begin{array}{r}{\\widehat{\\bf M}\\;=\\;\\frac{1}{n}\\sum_{i=1}^{n}{\\bf M}^{(i)}}\\end{array}$ , which is the empirical approximation of ${\\bf M}\\ =\\ \\mathsf{M a t}_{(l,k-l)}({\\bf C}_{k})\\ =$ $\\mathsf{M a t}_{(l,k-l)}\\big(\\mathbf{E}_{(\\mathbf{x},y)\\sim\\mathcal{D}}\\big[y\\mathbf{H}\\mathbf{e}_{k}(\\mathbf{x})\\big]\\big)$ . We will use Wedin\u2019s theorem to bound the distance between the top left singular vector $\\mathbf{v}^{*}$ of $\\mathbf{M}$ and the top singular vector ${\\widehat{\\mathbf{v}}}^{*}$ of the empirical M. ", "page_idx": 24}, {"type": "text", "text": "Fact D.6 (Wedin\u2019s theorem). Let $\\theta(\\mathbf{v}^{*},\\widehat{\\mathbf{v}}^{*})$ be the angle between the top left singular vectors $\\mathbf{v}^{\\ast}\\in\\mathbb{R}^{d^{l}}$ and $\\widehat{\\mathbf{v}}^{*}\\in\\mathbb{R}^{d^{l}}$ of M and $\\widehat{\\bf M}$ respectively. Let $\\rho_{1}$ and $\\rho_{2}$ be the first 2 singular values of $\\mathbf{M}$ Then, it holds that: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sin(\\theta(\\mathbf{v}^{*},\\widehat{\\mathbf{v}}^{*}))\\leq\\frac{\\|\\mathbf{M}-\\widehat{\\mathbf{M}}\\|_{2}}{\\rho_{1}-\\rho_{2}-\\|\\mathbf{M}-\\widehat{\\mathbf{M}}\\|_{2}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We first observe that M admits a large gap between the first and second singular values. ", "page_idx": 25}, {"type": "text", "text": "Claim D.7 (Singular Gap of Unfolded Tensor M\u221aatrix). Let $\\rho_{1},\\rho_{2}$ be the top two singular values of $\\mathbf{M}=\\mathsf{M a t}_{(l,k-l)}\\big(\\mathbf{C}_{k}\\big)$ . Then $\\rho_{1}-\\rho_{2}\\ge(c_{k}-8\\sqrt{\\mathrm{OPT}})/2$ . ", "page_idx": 25}, {"type": "text", "text": "Proof. Recall that in Corollary D.5 we showed $\\rho_{1}=\\rho(\\mathbf{v}^{*})\\geq c_{k}-\\sqrt{\\mathrm{OPT}}$ and $\\mathbf{v}^{*}\\cdot\\mathsf{V e c}(\\mathbf{w}^{*\\otimes l})\\geq$ $1-(2\\sqrt{\\mathrm{OPT}})/c_{k}$ . Now let $\\mathbf{v}\\in\\mathbb{R}^{d^{l}}$ be any left singular vector of $\\mathbf{M}$ that is orthogonal to $\\mathbf{v}^{*}$ . We can decompose $\\mathsf{V e c}(\\mathbf{w}^{*\\otimes l})$ into $\\mathsf{V e c}(\\mathbf{w}^{*\\otimes l})=a\\mathbf{v}^{*}+b\\mathbf{v}+\\mathbf{v}^{\\prime}$ where $\\mathbf{v}^{\\prime}$ is orthogonal to both $\\mathbf{v}^{*}$ and $\\mathbf{v}$ , and $a^{2}+b^{2}\\,\\leq\\,1\\,$ . Then, since $\\mathsf{V e c}(\\mathbf{w}^{*\\otimes l})\\cdot\\mathbf{v}^{*}\\,=\\,a\\,\\geq\\,1\\,-\\,(2\\sqrt{\\mathrm{OPT}})/c_{k}$ , we thus have $\\mathsf{V e c}(\\mathbf{w}^{*\\otimes l})\\cdot\\mathbf{v}=b\\leq\\sqrt{1-a^{2}}\\leq1-a^{2}/2$ . This implies that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\rho_{1}-\\rho(\\mathbf v)\\geq c_{k}-\\sqrt{\\mathrm{OPT}}-(c_{k}(\\mathsf{V e c}(\\mathbf w^{*\\otimes l})\\cdot\\mathbf v)+\\sqrt{\\mathrm{OPT}})}\\\\ &{\\qquad\\qquad\\geq c_{k}(1-b)-2\\sqrt{\\mathrm{OPT}}\\geq c_{k}(1-(1-a^{2}/2))-2\\sqrt{\\mathrm{OPT}}\\geq c_{k}/2-4\\sqrt{\\mathrm{OPT}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and hence we get $\\rho_{1}-\\rho_{2}\\ge(c_{k}-8\\sqrt{\\mathrm{OPT}})/2$ , completing the proof of Claim D.7. ", "page_idx": 25}, {"type": "text", "text": "Thus, our remaining goal is to bound the operator norm of $\\mathbf{M}-{\\widehat{\\mathbf{M}}}$ . For this purpose, we use the following matrix concentration inequality from [DPVLB24] (also Theorem 2.7 in [BvH22]). ", "page_idx": 25}, {"type": "text", "text": "Fact D.8 (Lemma I.5 [DPVLB24]). Let $\\mathbf{Z}^{(i)},i~\\in~[n]$ , be independent, mean-zero, self-adjoint matrices. Define: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathrm{\\boldmath~\\sigma~}_{i}^{\\prime}:=\\left\\|{\\bf E}\\left[\\left(\\sum_{i=1}^{n}{\\bf Z}^{(i)}\\right)^{2}\\right]\\right\\|_{2},\\quad\\gamma_{*}^{2}:=\\operatorname*{sup}_{\\|{\\bf v}\\|_{2}=\\|{\\bf r}\\|_{2}=1}{\\bf E}\\left[\\left(\\sum_{i=1}^{n}{\\bf v}^{\\top}{\\bf Z}^{(i)}{\\bf r}\\right)^{2}\\right],\\;\\bar{R}^{2}:={\\bf E}\\left[\\operatorname*{max}_{i\\in[n]}\\|{\\bf Z}^{(i)}\\|_{2}^{2}\\right].\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Then, for any $R\\geq\\bar{R}^{1/2}\\gamma^{1/2}+\\sqrt{2}\\bar{R},$ , and any $t\\geq0$ , $i f\\delta=\\mathbf{Pr}[\\operatorname*{max}_{i\\in[n]}\\|\\mathbf{Z}^{(i)}\\|_{2}\\geq R],$ , then with probability at least $1-\\delta-d e^{-t}$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\bigg\\|\\sum_{i=1}^{n}\\mathbf{Z}^{(i)}\\bigg\\|_{2}-2\\gamma\\lesssim\\gamma_{*}t^{1/2}+R^{1/3}\\gamma^{2/3}t^{2/3}+R t.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "However, note that $\\widehat{\\bf M}$ and $\\mathbf{M}$ are not symmetric matrices, hence to apply matrix concentration inequalities we will be working on the symmetrization of $\\widehat{\\bf M}$ and $\\mathbf{M}$ for simplicity, which we will denote by $\\widehat{\\mathbf{P}}$ and $\\mathbf{P}$ : ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\widehat{\\mathbf{P}}=\\frac{1}{n}\\sum_{i=1}^{n}\\mathbf{P}^{(i)}=\\frac{1}{n}\\sum_{i=1}^{n}\\left[\\mathbf{M}^{(i)\\top}\\mathbf{\\Lambda}\\ \\ \\mathbf{0}\\right]=\\left[\\widehat{\\mathbf{M}}^{(i)}\\right]=\\left[\\widehat{\\mathbf{M}}^{\\top}\\mathbf{\\Lambda}\\ \\mathbf{\\widehat{M}}\\right];\\quad\\mathbf{P}=\\left[\\mathbf{M}^{\\top}\\mathbf{\\Lambda}\\ \\mathbf{\\widehat{\\mathbf{0}}}\\right].\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Before we prove the main theorem of this subsection, we introduce two final pieces of tools that will be used later in the proof. The first one is Gaussian hypercontractivity. ", "page_idx": 25}, {"type": "text", "text": "Fact D.9 (Gaussian Hypercontractivity). Let $f(\\mathbf{x}):\\mathbb{R}^{d}\\to\\mathbb{R}$ be a multivariate polynomial of degree at most k. Let x be a standard Gaussian random variable of $\\mathbb{R}^{d}$ . Then, for any $p\\geq1$ it holds ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\|f(\\mathbf{x})\\|_{L^{p}}\\leq(p-1)^{k/2}\\|f(\\mathbf{x})\\|_{L^{2}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Gaussian hypercontractivity controls the moments of a polynomial $f(\\mathbf{x})$ . To utilize the bound on these moments, we make use of the following inequality from [DNGL23]. ", "page_idx": 25}, {"type": "text", "text": "Fact D.10 (Lemma 23 [DNGL23]). Let $A,B$ be random variables such that $\\|B\\|_{L^{p}}\\leq\\sigma_{B}p^{C}$ for all $p\\geq1$ and some positive real numbers $\\sigma_{B},C$ . Then, ", "page_idx": 25}, {"type": "equation", "text": "$$\n{\\bf E}[A B]\\leq{\\bf E}[\\vert A\\vert]\\sigma_{B}(2e)^{C}\\bigg(\\operatorname*{max}\\bigg\\{1,\\frac{1}{C}\\log\\bigg(\\frac{({\\bf E}[A^{2}])^{1/2}}{{\\bf E}[\\vert A\\vert]}\\bigg)\\bigg\\}\\bigg)^{C}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We also make use of the following lemma that bounds the magnitude of label $y$ without loss of generality. ", "page_idx": 25}, {"type": "text", "text": "Lemma D.11 (Bound on Labels). Let $P_{B_{y}}(z):\\mathbb{R}\\rightarrow\\mathbb{R}$ be a function that truncates the value of $z$ to the threshold $B_{y}$ : $P_{B_{y}}(z)=z\\mathbb{1}\\{|z|\\le\\bar{B}_{y}\\}+B_{y}\\mathbb{1}\\{|z|\\ge B_{y}\\}$ . Assume that Assumption 1 holds. Then choosing $B_{y}:=\\sqrt{4B_{4}/\\epsilon}$ , it holds that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\underset{(\\mathbf{x},y)\\sim\\mathcal{D}}{\\mathbf{E}}[(P_{B_{y}}(y)-\\sigma(\\mathbf{w}^{*}\\cdot\\mathbf{x}))^{2}]\\leq\\mathrm{OPT}+\\epsilon.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Therefore, it is without loss of generality to assume that $\\lvert y\\rvert\\le B_{y}$ . ", "page_idx": 26}, {"type": "text", "text": "Proof. After truncating the label $y$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad_{(\\mathbf{x},y)\\sim\\mathcal{D}}[(P_{t}(y)-\\sigma(\\mathbf{w}^{*}\\cdot\\mathbf{x}))^{2}]}\\\\ &{=\\underset{(\\mathbf{x},y)\\sim\\mathcal{D}}{\\mathbf{E}}[(P_{t}(y)-\\sigma(\\mathbf{w}^{*}\\cdot\\mathbf{x}))^{2}\\mathbb{1}\\{\\sigma(\\mathbf{w}^{*}\\cdot\\mathbf{x})\\le t\\}]+\\underset{(\\mathbf{x},y)\\sim\\mathcal{D}}{\\mathbf{E}}[(P_{t}(y)-\\sigma(\\mathbf{w}^{*}\\cdot\\mathbf{x}))^{2}\\mathbb{1}\\{\\sigma(\\mathbf{w}^{*}\\cdot\\mathbf{x})\\ge}]}\\\\ &{\\le\\underset{(\\mathbf{x},y)\\sim\\mathcal{D}}{\\mathbf{E}}[(y-\\sigma(\\mathbf{w}^{*}\\cdot\\mathbf{x}))^{2}]+\\underset{(\\mathbf{x},y)\\sim\\mathcal{D}}{\\mathbf{E}}[(P_{t}(y)-\\sigma(\\mathbf{w}^{*}\\cdot\\mathbf{x}))^{2}\\mathbb{1}\\{\\sigma(\\mathbf{w}^{*}\\cdot\\mathbf{x})\\ge t\\}]}\\\\ &{\\le\\mathrm{OPT}+2\\underset{(\\mathbf{x},y)\\sim\\mathcal{D}}{\\mathbf{E}}[(t^{2}+\\sigma^{2}(\\mathbf{w}^{*}\\cdot\\mathbf{x}))\\mathbb{1}\\{\\sigma(\\mathbf{w}^{*}\\cdot\\mathbf{x})\\ge t\\}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Since $\\begin{array}{r}{\\mathbf{E}_{\\mathbf{x}\\sim\\mathcal{N}_{d}}[\\sigma^{4}(\\mathbf{w}^{\\ast}\\cdot\\mathbf{x})]\\le B_{4}}\\end{array}$ by assumption, we have by Markov\u2019s inequality that $\\mathbf{Pr}[\\sigma(\\mathbf{w}^{*}\\cdot\\mathbf{x})\\geq$ $t]\\le B_{4}/t^{4}$ . Therefore, we can further bound $\\mathbf{E}_{(\\mathbf{x},y)\\sim\\mathcal{D}}[(\\dot{P}_{t}(y)-\\sigma(\\mathbf{w}^{*}\\cdot\\mathbf{\\dot{x}}))^{2}]$ from above by ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{(\\mathbf{x},\\mathbf{y})\\sim\\mathcal{D}}{\\mathbf{E}}[(P_{t}(y)-\\sigma(\\mathbf{w}^{*}\\cdot\\mathbf{x}))^{2}]\\leq\\mathrm{OPT}+\\frac{2B_{4}}{t^{2}}+2\\sqrt{\\underset{\\mathbf{x}\\sim\\mathcal{N}_{d}}{\\mathbf{E}}[\\sigma^{4}(\\mathbf{w}^{*}\\cdot\\mathbf{x})]\\,\\mathbf{Pr}[\\sigma(\\mathbf{w}^{*}\\cdot\\mathbf{x})\\geq t]}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\mathrm{OPT}+\\frac{4B_{4}}{t^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Thus, choosing $t=\\sqrt{4B_{4}/\\epsilon}$ we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\underset{(\\mathbf{x},y)\\sim\\mathcal{D}}{\\mathbf{E}}[(P_{t}(y)-\\sigma(\\mathbf{w}^{*}\\cdot\\mathbf{x}))^{2}]\\leq\\mathrm{OPT}+\\epsilon,\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "indicating that we can assume without loss of generality that $|y|\\le B_{y}:=\\sqrt{4B_{4}/\\epsilon}$ , completing the proof of Lemma D.11. \u53e3 ", "page_idx": 26}, {"type": "text", "text": "After assuming that $y$ is bounded by $B_{y}$ without loss of generality, we can then bound the $2^{\\mathrm{nd}}$ and $4^{\\mathrm{th}}$ moments of $y$ . These bounds on the moments of the label $y$ will be used when we implement Fact D.10 to get finer bounds compared to what we would get from a simple application of Cauchy-Schwarz. In particular, we use Fact D.10 to derive upper bounds on expectations like $\\mathbf{E}_{(\\mathbf{x},y)\\sim\\mathcal{D}}[y^{2}f^{2}(\\mathbf{x})]$ , where $f(\\mathbf{x})$ is a polynomial of $\\mathbf{x}$ , as we have control on the $p^{\\mathrm{th}}$ moments of $f(\\mathbf{x})$ using Gaussian hypercontractivity Fact D.9. ", "page_idx": 26}, {"type": "text", "text": "Lemma D.12 (Moments of Labels). $I f\\,\\mathrm{OPT}\\leq1/16,$ , then $1/2\\le\\mathbf{E}_{y}[y^{2}]\\le2$ and ${\\bf E}_{y}[y^{4}]\\le8B_{4}/\\epsilon$ . ", "page_idx": 26}, {"type": "text", "text": "Proof. We first bound the $2^{\\mathrm{nd}}$ moment of the label $y$ . Note that since $\\sigma(\\mathbf{w}^{*}\\cdot\\mathbf{x})$ is normalized such that $\\mathbf{\\dot{E}}_{\\mathbf{x}\\sim\\mathcal{N}_{d}}[(\\sigma(\\mathbf{w}^{\\ast}\\cdot\\mathbf{x}))^{2}]=1$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{\\underline{{E}}}[y^{2}]=\\underset{(\\mathbf{x},y)\\sim\\mathcal{D}}{\\mathbf{E}}[(y-\\sigma(\\mathbf{w}^{*}\\cdot\\mathbf{x})+\\sigma(\\mathbf{w}^{*}\\cdot\\mathbf{x}))^{2}]}\\\\ &{\\qquad\\overset{(i)}{\\leq}(1+1/a)\\underset{(\\mathbf{x},y)\\sim\\mathcal{D}}{\\mathbf{E}}[(y-\\sigma(\\mathbf{w}^{*}\\cdot\\mathbf{x}))^{2}]+(1+a)\\underset{\\mathbf{x}\\sim\\mathcal{N}_{d}}{\\mathbf{E}}[(\\sigma(\\mathbf{w}^{*}\\cdot\\mathbf{x}))^{2}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We used Young\u2019s inequality in $(i)$ . Choosing $a=1/8$ and since we assumed $\\mathrm{OPT}\\leq1/16$ , it holds $\\mathbf{E}_{y}[y^{2}]\\leq9/1\\bar{6}+9/\\bar{8}\\leq2$ . In addition, using Cauchy-Schwarz inequality, we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{E}[y^{2}]=\\underset{(\\mathbf{x},y)\\sim\\mathcal{D}}{\\mathbf{E}}[(y-\\sigma(\\mathbf{w}^{*}\\cdot\\mathbf{x})+\\sigma(\\mathbf{w}^{*}\\cdot\\mathbf{x}))^{2}]}\\\\ &{\\qquad=\\underset{(\\mathbf{x},y)\\sim\\mathcal{D}}{\\mathbf{E}}[(y-\\sigma(\\mathbf{w}^{*}\\cdot\\mathbf{x}))^{2}]+\\underset{\\mathbf{x}\\sim\\mathcal{N}_{d}}{\\mathbf{E}}[(\\sigma(\\mathbf{w}^{*}\\cdot\\mathbf{x}))^{2}]+2\\underset{(\\mathbf{x},y)\\sim\\mathcal{D}}{\\mathbf{E}}[(y-\\sigma(\\mathbf{w}^{*}\\cdot\\mathbf{x}))\\sigma(\\mathbf{w}^{*}\\cdot\\mathbf{x})]}\\\\ &{\\qquad\\geq1-2\\sqrt{\\underset{(\\mathbf{x},y)\\sim\\mathcal{D}}{\\mathbf{E}}[(y-\\sigma(\\mathbf{w}^{*}\\cdot\\mathbf{x}))^{2}]}\\underset{\\mathbf{x}\\sim\\mathcal{N}_{d}}{\\mathbf{E}}[(\\sigma(\\mathbf{w}^{*}\\cdot\\mathbf{x}))^{2}]\\geq1/2.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "This yields the first statement of the lemma. For the remaining statement, notice that since $y\\leq B_{y}$ , we have $\\mathbf{E}_{y}[y^{4}]\\le B_{y}^{2}\\,\\mathbf{E}_{y}[y^{2}]\\le2B_{y}^{2}=8B_{4}/{\\epsilon}$ . \u53e3 ", "page_idx": 26}, {"type": "text", "text": "We now proceed to bound the sample complexity of Algorithm 3, the argument for which relies on applying Fact D.8 to $\\begin{array}{r}{\\mathbf Z^{(i)}=\\frac{1}{n}(\\mathbf P^{(i)}-\\mathbf P)}\\end{array}$ and is summarized in the following lemma. ", "page_idx": 27}, {"type": "text", "text": "Lemma D.13 (Sample Complexity for Estimating the Unfolded Tensor Matrix). Let \u03f5, $\\epsilon_{0}>0.$ . Consider the unfolded matrix $\\mathbf{M}=\\mathsf{M a t}_{(l,k-l)}\\bigl(\\mathbf{E}_{(\\mathbf{x},y)\\sim\\mathcal{D}}\\bigl[y\\mathbf{H}\\mathbf{e}_{k}(\\mathbf{x})\\bigr]\\bigr)$ and its empirical estimate $\\widehat{\\mathbf{M}}:=$ $\\begin{array}{r}{(1/n)\\sum_{i=1}^{n}\\mathsf{M a t}_{(l,k-l)}(y^{(i)}\\mathbf{He}_{k}(\\mathbf{x}^{(i)}))}\\end{array}$ , where $\\{(\\mathbf{x}^{(i)},y^{(i)})\\}_{i=1}^{n}$ are $n=\\Theta(e^{k}\\mathrm{log}^{k}(B_{4}/\\epsilon)d^{k/2}/\\epsilon_{0}^{2}+$ $1/\\epsilon)$ i.i.d. samples from $\\mathcal{D}$ . Then, with probability at least $1-\\exp(-d^{1/2})$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\widehat{\\mathbf{M}}-\\mathbf{M}\\|_{2}\\leq\\epsilon_{0}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Moreover, $i f\\widehat{\\mathbf{v}}^{*}$ is the top left-singular vector of $\\widehat{\\bf M}_{}$ , then with probability at least $1-\\exp(-d^{1/2})$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\widehat{\\mathbf{v}}^{*}\\cdot\\mathsf{V e c}(\\mathbf{w}^{*\\otimes l})\\geq1-\\frac{2}{c_{k}}\\sqrt{\\mathrm{OPT}}-\\frac{2\\epsilon_{0}}{(c_{k}/2-4\\sqrt{\\mathrm{OPT}})-\\epsilon_{0}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. The proof hinges on applying Fact D.8, for which we need to bound above the parameters $\\gamma$ $\\gamma_{*}$ , and $\\bar{R}$ defined in the same fact. We do so in three separate claims, as follows. ", "page_idx": 27}, {"type": "text", "text": "Claim D.14. $\\begin{array}{r}{\\gamma\\lesssim\\frac{d^{(k-l)/2}e^{k}\\log^{k/2}(B_{4}/\\epsilon)}{\\sqrt{n}}=\\sqrt{\\frac{d^{k-l}e^{k}\\log^{k}(B_{4}/\\epsilon)}{n}}.}\\end{array}$ ", "page_idx": 27}, {"type": "text", "text": "Proof. By the definition of $\\gamma$ , we have: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\gamma^{2}=\\cfrac{\\|\\mathbf{E}\\left[\\bigg(\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\mathbf{P}^{(i)}-\\mathbf{P}\\bigg)^{2}\\right]\\bigg\\|_{2}\\leq\\frac{1}{n}\\|\\mathbf{E}[(\\mathbf{P}^{(i)}-\\mathbf{P})^{2}]\\|_{2}\\leq\\frac{1}{n}\\|\\mathbf{E}[(\\mathbf{P}^{(i)})^{2}]\\|_{2}}\\\\ &{\\quad=\\cfrac{1}{n}\\operatorname*{max}_{\\mathbf{v}\\in\\mathbb{R}^{d^{\\prime}},\\mathbf{r}\\in\\mathbb{R}^{d^{k-l}}}\\mathbf{E}[\\mathbf{v}^{\\top}\\mathbf{M}^{(i)}(\\mathbf{M}^{(i)})^{\\top}\\mathbf{v}+\\mathbf{r}^{\\top}(\\mathbf{M}^{(i)})^{\\top}\\mathbf{M}^{(i)}\\mathbf{r}],}\\\\ &{\\qquad\\|\\mathbf{v}\\|_{2}^{2}{+}\\|\\mathbf{r}\\|_{2}^{2}{=}1}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Observe that $\\begin{array}{r}{\\mathbf{v}^{\\top}\\mathbf{M}^{(i)}(\\mathbf{M}^{(i)})^{\\top}\\mathbf{v}\\,=\\,\\|\\mathbf{v}^{\\top}\\mathbf{M}^{(i)}\\|_{2}^{2}\\,=\\,\\sum_{j=1}^{d^{k-l}}(\\mathbf{v}^{\\top}\\mathbf{M}^{(i)})_{j}^{2}}\\end{array}$ , and notice that by definition $\\mathbf{M}^{(i)}\\,=\\,\\mathsf{M a t}_{(l,k-l)}\\bigl(y^{(i)}\\mathbf{He}_{k}\\bigl(\\mathbf{x}^{(i)}\\bigr)\\bigr)$ where $\\boldsymbol y^{(i)}\\mathbf{H}\\mathbf{e}_{k}(\\mathbf{x}^{(i)})$ is a symmetric tensor, hence using Equation (26) we get ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbf{v}^{\\top}\\mathbf{M}^{(i)}(\\mathbf{M}^{(i)})^{\\top}\\mathbf{v}=\\sum_{(j_{1},j_{2},\\ldots,j_{l-k})\\in[d]^{l-k}}\\left\\langle y^{(i)}\\mathbf{H}\\mathbf{e}_{k}(\\mathbf{x}^{(i)}),\\mathsf{T e n s o r}(\\mathbf{v})\\otimes\\mathbf{e}_{j_{1}}\\otimes\\ldots\\otimes\\mathbf{e}_{j_{k-l}}\\right\\rangle^{2}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "As $(\\mathbf{x}^{(i)},y^{(i)})$ are i.i.d. copies of $(\\mathbf{x},y)$ , using the linearity of expectation, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{E}[\\mathbf{v}^{\\top}\\mathbf{M}^{(i)}(\\mathbf{M}^{(i)})^{\\top}\\mathbf{v}]}\\\\ &{\\qquad\\qquad=\\displaystyle\\sum_{j_{1},\\dots,j_{k-l}}\\ \\underset{(\\mathbf{x},y)\\sim\\mathcal{D}}{\\mathbf{E}}\\left[y^{2}\\biggl\\langle\\mathbf{H}\\mathbf{e}_{k}(\\mathbf{x}),\\mathsf{T e n s o r}(\\mathbf{v})\\otimes\\mathbf{e}_{j_{1}}\\otimes\\dots\\otimes\\mathbf{e}_{j_{k-l}}\\biggr\\rangle^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Now given any indices $j_{1},\\ldots,j_{k-l}\\,\\in\\,[d]$ , observe that $f_{j_{1},\\dots,j_{k-l}}(\\mathbf{x})\\,:=\\,\\langle\\mathbf{H}\\mathbf{e}_{k}(\\mathbf{x})$ , Tensor $\\mathbf{\\Psi}(\\mathbf{v})\\otimes$ $\\mathbf{e}_{j_{1}}\\otimes\\ldots\\otimes\\mathbf{e}_{j_{k-l}}\\rangle$ is a polynomial of $\\mathbf{x}$ of degree at most $k$ , and note that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\mathbf{x}\\sim\\mathcal{N}_{d}}{\\mathbf{E}}[f_{j_{1},\\dots,j_{k-l}}(\\mathbf{x})^{2}]=\\underset{\\mathbf{x}\\sim\\mathcal{N}_{d}}{\\mathbf{E}}\\left[\\Bigl\\langle\\mathbf{H}\\mathbf{e}_{k}(\\mathbf{x}),\\mathsf{T e n s o r}(\\mathbf{v})\\otimes\\mathbf{e}_{j_{1}}\\otimes\\dots\\otimes\\mathbf{e}_{j_{k-l}}\\Bigr\\rangle^{2}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\left\\lVert\\mathrm{Sym}(\\mathsf{T e n s o r}(\\mathbf{v})\\otimes\\mathbf{e}_{j_{1}}\\otimes\\dots\\otimes\\mathbf{e}_{j_{k-l}})\\right\\rVert_{F}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\quad\\leq\\left\\lVert\\mathsf{T e n s o r}(\\mathbf{v})\\otimes\\mathbf{e}_{j_{1}}\\otimes\\dots\\otimes\\mathbf{e}_{j_{k-l}}\\right\\rVert_{F}^{2}\\leq1,}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where the second line is by Fact C.3. Our goal is to apply Fact D.10 with $\\textit{A}=\\textit{y}^{2}$ and $B\\,=\\,f_{j_{1},\\dots,j_{k-l}}(\\mathbf{x})^{2}$ . To this aim, we need to bound above the $L_{p}$ -norm of $f_{j_{1},\\dots,j_{k-l}}(\\mathbf{x})^{2}$ , i.e., $\\mathbf{E}_{\\mathbf{x}\\sim\\mathcal{N}_{d}}[(f_{j_{1},\\dots,j_{k-l}}(\\mathbf{x})^{2})^{p}]^{1/p}$ , which can be done using Fact D.9: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\big(\\mathop{\\mathbf{\\phi}}_{\\mathbf{x}\\sim\\mathcal{N}_{d}}^{\\mathbf{\\phi}}[(f_{j_{1},\\dots,j_{k-l}}(\\mathbf{x})^{2})^{p}]\\big)^{1/(2p)}\\le(2p-1)^{k/2}\\mathop{\\mathbf{\\phi}}_{\\mathbf{x}\\sim\\mathcal{N}_{d}}^{\\mathbf{E}}[f_{j_{1},\\dots,j_{k-l}}(\\mathbf{x}^{(i)})^{2}]\\le(2p)^{k/2}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "This implies that $\\|f_{j_{1},\\dots,j_{k-l}}(\\mathbf{x})^{2}\\|_{L^{p}}\\;\\leq\\;2^{k}p^{k}$ . Thus, using Fact D.10 with $A\\,=\\,y^{2}$ and $B\\,=$ $f_{j_{1},\\dots,j_{k-l}}(\\mathbf{x})^{2}$ , we get ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{\\mathbf{x},\\ y\\sim\\mathcal{D}}{\\mathbf{E}}\\left[y^{2}\\biggl\\langle\\mathbf{He}_{k}(\\mathbf{x}),\\mathsf{T e n s o r}(\\mathbf{v})\\otimes\\mathbf{e}_{j_{1}}\\otimes\\ldots\\otimes\\mathbf{e}_{j_{k-l}}\\biggr\\rangle^{2}\\right]\\leq\\underset{y}{\\mathbf{E}}[y^{2}](4e)^{k}\\biggl\\{1,\\frac{1}{k}\\log\\left(\\frac{\\mathbf{E}_{y}[y^{4}]^{1/2}}{\\mathbf{E}_{y}[y^{2}]}\\right)\\biggr\\}^{k}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Finally, using the bound on the moments of the labels as we proved in Lemma D.12, it holds that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\underset{(\\mathbf{x},y)\\sim\\mathcal{D}}{\\mathbf{E}}\\left[y^{2}\\biggl\\langle\\mathbf{He}_{k}(\\mathbf{x}),\\mathsf{T e n s o r}(\\mathbf{v})\\otimes\\mathbf{e}_{j_{1}}\\otimes...\\otimes\\mathbf{e}_{j_{k-l}}\\biggr\\rangle^{2}\\right]\\lesssim e^{k}\\log^{k}(B_{4}/\\epsilon).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Plugging the bound above back into Equation (33), we obtain: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\mathbf{x},\\mathbf{y}\\sim\\mathcal{D}}{\\mathbf{E}}[\\mathbf{v}^{\\top}\\mathbf{M}^{(i)}(\\mathbf{M}^{(i)})^{\\top}\\mathbf{v}]=\\underset{j_{1},\\ldots,j_{k-l}\\in[d]}{\\sum}\\underset{(\\mathbf{x},\\mathbf{y})\\sim\\mathcal{D}}{\\mathbf{E}}\\left[y^{2}\\Big\\langle\\mathbf{H}\\mathbf{e}_{k}(\\mathbf{x}),\\mathsf{T e n s o r}(\\mathbf{v})\\otimes\\mathbf{e}_{j_{1}}\\otimes\\ldots\\otimes\\mathbf{e}_{j_{k-l}}\\Big\\rangle^{2}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq e^{k}d^{k-l}\\log^{k}(B_{4}/\\epsilon).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We now proceed to bound above the second term $\\mathbf{E}[\\mathbf{r}^{\\top}(\\mathbf{M}^{(i)})^{\\top}\\mathbf{M}^{(i)}\\mathbf{r}]$ . Using Equation (25), similar calculations yield that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad{\\bf E}[{\\bf r}^{\\top}({\\bf M}^{(i)})^{\\top}{\\bf M}^{(i)}{\\bf r}]={\\bf E}[\\|{\\bf M}^{(i)}{\\bf r}\\|_{2}^{2}]}\\\\ &{=\\displaystyle\\sum_{i_{1},\\ldots,i_{l}}\\ {\\bf E}_{\\sim\\times N_{d}}\\left[(y)^{2}\\bigg\\langle{\\bf H}{\\bf e}_{k}({\\bf x}),{\\bf e}_{i_{1}}\\otimes\\ldots\\otimes{\\bf e}_{i_{l}}\\otimes{\\bf T}\\mathsf{e}_{\\mathsf{n}\\mathsf{s o r}}({\\bf r})\\bigg\\rangle^{2}\\right]}\\\\ &{\\leq e^{k}d^{l}\\log^{k}(B_{4}/\\epsilon)\\leq e^{k}d^{k-l}\\log^{k}(B_{4}/\\epsilon).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Thus, plugging in the value of $B_{y}=\\sqrt{B_{4}/\\epsilon}$ from Lemma D.11, the variance $\\gamma$ can be bounded by $\\begin{array}{r}{\\gamma\\lesssim\\frac{d^{(k-l)/2}e^{k}\\log^{k/2}(B_{4}/\\epsilon)}{\\sqrt{n}}=\\sqrt{\\frac{d^{k-l}e^{k}\\log^{k}(B_{4}/\\epsilon)}{n}}}\\end{array}$ \u53e3 ", "page_idx": 28}, {"type": "text", "text": "Next, we bound the operator norm $\\gamma_{*}$ from above. ", "page_idx": 28}, {"type": "text", "text": "Claim D.15. \u03b3\u2217\u2272ek/2 log\u221ak/n2(B4/\u03f5). ", "page_idx": 28}, {"type": "text", "text": "Proof. By the definition of $\\gamma_{*}$ , ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\gamma_{*}^{2}=\\underset{\\|\\widetilde{\\mathbf{v}}\\|_{2}=\\|\\widetilde{\\mathbf{r}}\\|_{2}=1}{\\operatorname*{sup}}\\mathbf{E}\\left[\\left(\\frac{1}{n}\\sum_{i=1}^{n}\\widetilde{\\mathbf{v}}^{\\top}(\\mathbf{P}^{(i)}-\\mathbf{P})\\widetilde{\\mathbf{r}}\\right)^{2}\\right]}\\\\ &{\\quad\\le\\underset{\\|\\widetilde{\\mathbf{v}}\\|_{2}=\\|\\widetilde{\\mathbf{r}}\\|_{2}=1}{\\operatorname*{sup}}\\frac{1}{n}\\,\\mathbf{E}\\left[\\left(\\widetilde{\\mathbf{v}}^{\\top}(\\mathbf{P}^{(i)}-\\mathbf{P})\\widetilde{\\mathbf{r}}\\right)^{2}\\right]}\\\\ &{\\quad\\le\\underset{\\|\\widetilde{\\mathbf{v}}\\|_{2}=\\|\\widetilde{\\mathbf{r}}\\|_{2}=1}{\\operatorname*{sup}}\\frac{1}{n}\\,\\mathbf{E}\\left[\\left(\\widetilde{\\mathbf{v}}^{\\top}\\mathbf{P}^{(i)}\\widetilde{\\mathbf{r}}\\right)^{2}\\right]\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Decompose $\\widetilde{{\\mathbf v}}$ into $\\tilde{\\mathbf{v}}^{\\top}\\,=\\,[(\\tilde{\\mathbf{v}}^{(1)})^{\\top},(\\tilde{\\mathbf{v}}^{(2)})^{\\top}]$ , where $\\tilde{\\mathbf{v}}^{(1)}\\,\\in\\,\\mathbb{R}^{d}$ and $\\tilde{\\mathbf{v}}^{(2)}\\,\\in\\,\\mathbb{R}^{d^{k-l}}$ . Similarly, we can decompose $\\tilde{\\mathbf{r}}$ into $\\tilde{\\mathbf{r}}^{\\top}\\,=\\,[(\\tilde{\\mathbf{r}}^{(1)})^{\\top},(\\tilde{\\mathbf{r}}^{(2)})^{\\top}]$ with the same structure. Then, $\\tilde{\\mathbf{v}}^{\\top}\\mathbf{P}^{(i)}\\tilde{\\mathbf{r}}\\;=$ $y_{\\cdot}^{(i)}(\\tilde{\\mathbf{v}}^{(1)\\top}\\mathbf{M}^{(i)}\\tilde{\\mathbf{r}}^{(2)}+\\tilde{\\mathbf{r}}^{(1)\\top}\\mathbf{M}^{(i)}\\tilde{\\mathbf{v}}^{(\\tilde{2})})$ . Thus, as $(\\mathbf{x}^{(i)},y^{(i)})$ are i.i.d. samples, we can further bound $\\gamma_{*}^{2}$ by ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\gamma_{*}^{2}\\lesssim\\underset{\\mathbf{v}\\in\\mathbb{R}^{d^{k-l}},\\|\\mathbf{v}\\|_{2}=1}{\\operatorname*{sup}}\\ \\frac{1}{n}\\underset{(\\mathbf{x},y)\\sim\\mathcal{D}}{\\mathbf{E}}\\bigg[y^{2}\\big(\\mathbf{v}^{\\top}\\mathbb{M}\\mathsf{a}\\mathbf{t}_{(l,k-l)}(\\mathbf{H}\\mathbf{e}_{k}(\\mathbf{x}))\\mathbf{r}\\big)^{2}\\bigg]}\\\\ &{\\qquad\\mathbf{r}\\mathrm{\\in}\\mathbb{R}^{d^{k-l}},\\|\\mathbf{r}\\|_{2}=1}\\\\ &{=\\frac{1}{n}\\underset{\\mathbf{v}\\in\\mathbb{R}^{d^{l}},\\|\\mathbf{v}\\|_{2}=1}{\\operatorname*{sup}}\\ \\underset{(\\mathbf{x},y)\\sim\\mathcal{D}}{\\mathbf{E}}\\bigg[y^{2}\\bigg\\langle\\mathbf{H}\\mathbf{e}_{k}(\\mathbf{x}),\\mathsf{T e n s o r}(\\mathbf{v})\\otimes\\mathsf{T e n s o r}(\\mathbf{r})\\bigg\\rangle^{2}\\bigg],}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where in the last equality we used Equation (27). ", "page_idx": 29}, {"type": "text", "text": "Now for any $\\|\\mathbf{u}\\|_{2}=\\|\\mathbf{v}\\|_{2}=1$ , define ", "page_idx": 29}, {"type": "equation", "text": "$$\nf_{(\\mathbf{v},\\mathbf{u})}(\\mathbf{x}):=\\bigg\\langle\\mathbf{He}_{k}(\\mathbf{x}),\\mathsf{T e n s o r}(\\mathbf{v})\\otimes\\mathsf{T e n s o r}(\\mathbf{r})\\bigg\\rangle,\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $\\mathbf{v}\\in\\mathbb{R}^{d^{l}},\\mathbf{r}\\in\\mathbb{R}^{d^{k-l}}$ , and $f_{(\\mathbf{v},\\mathbf{u})}$ is a polynomial of $(\\mathbf{x}_{1},\\ldots,\\mathbf{x}_{d})$ of degree at most $k$ . Note that the polynomial $f_{(\\mathbf{v},\\mathbf{u})}(\\mathbf{x})$ satisfies $f_{(\\mathbf{v},\\mathbf{u})}(\\mathbf{x})\\,\\geq\\,0$ and $\\mathbf{E}_{\\mathbf{x}\\sim\\mathcal{N}_{d}}[(f_{(\\mathbf{v},\\mathbf{u})}(\\mathbf{x}))^{2}]\\,=\\,\\|\\mathsf{T e n s o r}(\\mathbf{v})\\otimes$ $\\mathsf{T e n s o r}(\\mathbf{r})\\|_{F}^{2}=1$ . Similarly to the upper bound on $\\gamma$ , we apply Fact D.10 with $A$ being $y^{2}$ and $B$ being $f_{(\\mathbf{v},\\mathbf{u})}(\\mathbf{x})^{2}$ , which yields that for any $\\|\\mathbf{v}\\|_{2}=1$ , $\\|\\mathbf{u}\\|_{2}=1$ , ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{(\\mathbf{x},y)\\sim\\mathcal{D}}{\\mathbf{E}}\\bigg[y^{2}\\bigg\\langle\\mathbf{He}_{k}(\\mathbf{x}),\\mathsf{T e n s o r}(\\mathbf{v})\\otimes\\mathsf{T e n s o r}(\\mathbf{r})\\bigg\\rangle^{2}\\bigg]\\leq e^{k}\\log^{k}(B_{4}/\\epsilon).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Thus, plugging this inequality back into the upper bound on $\\gamma_{*}$ above, we obtain ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\gamma_{*}^{2}\\lesssim\\frac{e^{k}\\log^{k}(B_{4}/\\epsilon)}{n}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Taking the square root on both sides completes the proof of Claim D.15. ", "page_idx": 29}, {"type": "text", "text": "To apply Fact D.8, it remains to bound $\\bar{R}$ , which we do in the following claim. ", "page_idx": 29}, {"type": "text", "text": "Proof. By the definition of $\\ell_{2}$ norm, we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\bar{R}^{2}=\\mathbf{E}\\left[\\operatorname*{max}_{i\\in[n]}\\operatorname*{sup}_{\\|\\bar{\\mathbf{v}}\\|_{2}=\\|\\bar{\\mathbf{r}}\\|_{2}=1}\\frac{1}{n^{2}}(\\tilde{\\mathbf{v}}^{\\top}(\\mathbf{P}^{(i)}-\\mathbf{P})\\tilde{\\mathbf{r}})^{2}\\right]\\lesssim\\frac{1}{n^{2}}\\,\\mathbf{E}\\left[\\operatorname*{max}_{i\\in[n]}\\operatorname*{sup}_{\\|\\bar{\\mathbf{v}}\\|_{2}=\\|\\tilde{\\mathbf{r}}\\|_{2}=1}(\\tilde{\\mathbf{v}}^{\\top}\\mathbf{P}^{(i)}\\tilde{\\mathbf{r}})^{2}\\right].\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Let us define ", "page_idx": 29}, {"type": "equation", "text": "$$\nf_{i}(\\mathbf{x}^{(i)}):=\\|\\mathsf{M a t}_{(l,k-l)}(\\mathbf{He}_{k}(\\mathbf{x}^{(i)}))\\|_{2}=\\operatorname*{sup}_{\\|\\mathbf{v}\\|_{2}=\\|\\mathbf{r}\\|_{2}=1}\\biggl\\langle\\mathbf{He}_{k}(\\mathbf{x}^{(i)}),\\mathsf{T e n s o r}(\\mathbf{v})\\otimes\\mathsf{T e n s o r}(\\mathbf{r})\\biggr\\rangle,\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $\\mathbf{v}\\,\\in\\,\\mathbb{R}^{d^{l}},\\mathbf{r}\\,\\in\\,\\mathbb{R}^{d^{k-l}}$ , and $f_{i}(\\mathbf{x}^{(i)})$ is a polynomial of $(\\mathbf{x}_{1}^{(i)},\\ldots,\\mathbf{x}_{d}^{(i)})$ of degree at most $k$ . Using the decomposition of $\\tilde{\\mathbf{v}}^{\\top}=[(\\tilde{\\mathbf{v}}^{(1)})^{\\top},(\\tilde{\\mathbf{v}}^{(2)})^{\\top}]$ and $\\tilde{\\mathbf{r}}^{\\top}=[(\\tilde{\\mathbf{r}}^{(1)})^{\\top},(\\tilde{\\mathbf{r}}^{(2)})^{\\top}]$ again, we get ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{R}^{2}\\lesssim\\frac{1}{n^{2}}\\,\\mathbf{E}\\left[\\underset{i\\in[n]}{\\operatorname*{max}}\\ \\underset{\\mathbf{v}\\in\\mathbb{B}_{d^{l}},\\mathbf{r}\\in\\mathbb{B}_{d^{k-l}}}{\\operatorname*{sup}}(y^{(i)})^{2}\\langle\\mathbf{He}_{k}(\\mathbf{x}^{(i)}),\\mathsf{T e n s o r}(\\mathbf{v})\\otimes\\mathsf{T e n s o r}(\\mathbf{r})\\rangle^{2}\\right]}\\\\ &{\\quad\\leq\\frac{1}{n^{2}}\\,\\mathbf{E}\\left[\\underset{i\\in[n]}{\\operatorname*{max}}(y^{(i)})^{2}(f_{i}(\\mathbf{x}^{(i)}))^{2}\\right]\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Note that the polynomial $f_{i}(\\mathbf{x}^{(i)})$ satisfies $f_{i}(\\mathbf{x}^{(i)})\\geq0$ and $\\mathbf{E}_{\\mathbf{x}^{(i)}\\sim\\mathcal{N}_{d}}[(f_{i}(\\mathbf{x}^{(i)}))^{2}]=\\|\\mathsf{T e n s o r}(\\mathbf{v})\\otimes$ Tensor $(\\mathbf{r})\\|_{F}^{2}\\leq1$ . Note that $\\begin{array}{r}{\\mathbf{E}[\\operatorname*{max}_{i\\in[n]}Z_{i}]\\leq\\sum_{i=1}^{n}\\mathbf{E}[Z_{i}]}\\end{array}$ , thus using Fact D.10 we get: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\bar{R}^{2}\\leq\\frac{1}{n^{2}}\\sum_{i=1}^{n}\\underset{(\\mathbf{x}^{(i)},\\mathbf{y}^{(i)})\\sim\\mathcal{D}}{\\mathbf{E}}[(y^{(i)})^{2}(f_{i}(\\mathbf{x}^{(i)}))^{2}]\\leq\\frac{e^{k}\\log^{k}(B_{4}/\\epsilon)}{n}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Taking the square root on both sides completes the proof. ", "page_idx": 29}, {"type": "text", "text": "To apply Fact D.8, we need to choose the parameter $R$ such that $\\delta=\\mathbf{Pr}[\\operatorname*{max}_{i\\in[n]}(1/n)\\|\\mathbf{P}^{(i)}-$ $\\mathbf{P}\\|_{2}\\geq R]$ is sufficiently small. Consider choosing $R$ such that ", "page_idx": 29}, {"type": "equation", "text": "$$\nR\\gtrsim\\frac{e^{k}\\log^{k}(B_{4}/\\epsilon)d^{(k-l)/4}}{\\sqrt{n}}\\geq\\bar{R}^{1/2}\\gamma^{1/2}+\\sqrt{2}\\bar{R}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "To determine $\\delta$ , recall that from Fact D.9, we have (using Markov\u2019s inequality): ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbf{Pr}[|f_{i}(\\mathbf{x}^{(i)})|\\ge t]\\le\\frac{\\mathbf{E}_{\\mathbf{x}\\sim\\mathcal{N}_{d}}[|f_{i}(\\mathbf{x}^{(i)})|^{p}]}{t^{p}}\\le\\frac{p^{k p/2}}{t^{p}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Note that $\\|\\mathbf{P}^{(i)}-\\mathbf{P}\\|_{2}=|y^{(i)}|f_{i}(\\mathbf{x}^{(i)})$ , hence ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\delta=\\mathbf{Pr}\\left[\\displaystyle\\operatorname*{max}_{i\\in[n]}\\frac{1}{n}\\|\\mathbf{P}^{(i)}-\\mathbf{P}\\|_{2}\\geq R\\right]\\leq\\mathbf{Pr}\\left[\\displaystyle\\operatorname*{max}_{i\\in[n]}B_{y}f_{i}(\\mathbf{x}^{(i)})\\geq n R\\right]}\\\\ &{\\ \\ \\overset{(i)}{\\leq}n\\,\\mathbf{Pr}\\left[B_{y}f_{i}(\\mathbf{x}^{(i)})\\geq n R\\right]\\overset{(i i)}{\\leq}n\\bigg(\\frac{p^{k/2}}{n R/B_{y}}\\bigg)^{p},}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where in $(i)$ we used a union bound and in $(i i)$ we used Equation (34) with $t=n R/B_{y}$ . Now setting $p^{k/2}=n R/(e B_{y})$ , we get ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\delta\\leq\\exp(-(n R/(B_{y}e))^{2/k}+\\log(n))\\lesssim\\exp(-\\log^{2}(B_{4}/\\epsilon)(\\epsilon n)^{1/k}d^{1/4}).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "In summary, applying Fact D.8 with the bound on $\\delta$ and $\\gamma$ (Claim D.14), $\\gamma_{*}$ (Claim D.15), $\\bar{R}$ (Claim D.16), and choosing $t=d^{k/4}$ in Equation (32), we finally get that with probability at least $1-\\exp(-\\log^{2}(1/\\epsilon)(\\epsilon n)^{1/k}d^{1/4})-d\\exp\\bar{(}-d^{k/4})$ , it holds ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\|\\mathbf{M}-\\widehat{\\mathbf{M}}\\|_{2}=\\|\\widehat{\\mathbf{P}}-\\mathbf{P}\\|_{2}\\lesssim2\\gamma+\\gamma_{*}t^{1/2}+R^{1/3}\\gamma^{2/3}t^{2/3}+R t\\lesssim\\frac{\\log^{k/2}(B_{4}/\\epsilon)d^{(k-l)/2}}{\\sqrt{n}}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Therefore, choosing ", "page_idx": 30}, {"type": "equation", "text": "$$\nn=\\Theta\\bigg(\\frac{e^{k}\\log^{k}(B_{4}/\\epsilon)d^{k-l}}{\\epsilon_{0}^{2}}+\\frac{1}{\\epsilon}\\bigg),\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "we have $\\|\\mathbf{M}-\\widehat{\\mathbf{M}}\\|_{2}\\le\\epsilon_{0}$ , with probability at least $1-\\exp(-d^{1/2})$ . ", "page_idx": 30}, {"type": "text", "text": "To complete the proof of Lemma D.13, we apply Wedin\u2019s theorem (Fact D.6) and Claim D.7, which together imply that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\sin(\\theta(\\mathbf{v}^{*},\\widehat{\\mathbf{v}}^{*}))\\leq\\frac{\\epsilon_{0}}{(c_{k}/2-4\\sqrt{\\mathrm{OPT}})-\\epsilon_{0}}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "We then decompose $\\widehat{\\mathbf{v}}^{*}$ into $\\widehat{\\mathbf{v}}^{*}=a\\mathbf{v}^{*}+b\\mathbf{r}$ , where $\\mathbf{r}\\in\\mathbb{R}^{d^{l}}$ such that r $\\perp\\mathbf{v}^{*}$ and $\\|\\mathbf{r}\\|_{2}=1$ , and $a^{2}+b^{2}=1$ . Since $b=\\sin(\\theta(\\mathbf{v}^{*},\\widehat{\\mathbf{v}}^{*}))$ , applying Corollary D.5 we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{\\mathbf{v}}^{*}\\cdot\\mathsf{V e c}(\\mathbf{w}^{*\\otimes l})=a\\mathbf{v}^{*}\\cdot\\mathsf{V e c}(\\mathbf{w}^{*\\otimes l})+b\\mathbf{r}\\cdot\\mathsf{V e c}(\\mathbf{w}^{*\\otimes l})}\\\\ &{\\qquad\\qquad\\geq\\sqrt{1-b^{2}}(1-2\\sqrt{\\mathrm{OPT}}/c_{k})-b\\geq(1-2\\sqrt{\\mathrm{OPT}}/c_{k})-(2-2\\sqrt{\\mathrm{OPT}}/c_{k})b}\\\\ &{\\qquad\\qquad\\geq1-\\frac{2}{c_{k}}\\sqrt{\\mathrm{OPT}}-\\frac{2\\epsilon_{0}}{(c_{k}/2-4\\sqrt{\\mathrm{OPT}})-\\epsilon_{0}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "This completes the proof of Lemma D.13. ", "page_idx": 30}, {"type": "text", "text": "After getting an approximate top-left singular vector of $\\mathsf{M a t}_{(l,k-l)}\\bigl(\\mathbf{E}_{(\\mathbf{x},y)\\sim\\mathcal{D}}\\bigl[y\\mathbf{H}\\mathbf{e}_{k}(\\mathbf{x})\\bigr]\\bigr)$ , $\\widehat{\\mathbf{v}}^{*}\\in\\mathbb{R}^{d}$ , we show that finding the top-left singular vector of the matrix $\\mathsf{M a t}_{(1,l-1)}(\\widehat{\\mathbf{v}}^{*})$ completes the task of computing a vector $\\mathbf{u}$ that correlates strongly with $\\mathbf{w}^{*}$ . ", "page_idx": 30}, {"type": "text", "text": "Lemma D.17. Suppose that $\\widehat{\\mathbf{v}}^{*}\\cdot\\mathsf{V e c}(\\mathbf{w}^{*\\otimes l})\\geq1-\\epsilon_{1}$ for some $\\epsilon_{1}\\in(0,1/16]$ . Then, the top-left singular vector $\\mathbf u\\in\\mathbb R$ of $\\mathsf{M a t}_{(1,l-1)}(\\widehat{\\mathbf{v}}^{*})$ satisfies $\\mathbf{u}\\cdot\\mathbf{w}^{*}\\geq1-2\\epsilon_{1}$ . ", "page_idx": 30}, {"type": "text", "text": "Proof. Consider the SVD of $\\mathsf{M a t}_{(1,l-1)}(\\widehat{\\mathbf{v}}^{*})\\in\\mathbb{R}^{d\\times d^{l-1}}$ ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathsf{M a t}_{(1,l-1)}\\big(\\widehat{\\mathbf{v}}^{*}\\big)=\\sum_{i=1}^{d}\\rho_{i}\\mathbf{u}^{(i)}(\\mathbf{r}^{(i)})^{\\top},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $\\mathbf{u}^{(i)}\\,\\in\\,\\mathbb{R}^{d}$ , $i\\,\\in\\,[d]$ , and $\\mathbf{r}^{(i)}\\,\\in\\,\\mathbb{R}^{d^{l-1}}$ , $i\\,\\in\\,[d]$ , are two sets of orthonormal vectors. Note that $\\{\\mathbf{r}^{(1)},\\ldots,\\mathbf{r}^{(d)}\\}$ is a subset of $\\{\\mathbf{r}^{(1)},\\ldots,\\mathbf{r}^{(d^{l})}\\}$ , which is an orthonormal basis of $\\mathbb{R}^{d^{l}}$ . Since $(\\mathbf{w}^{*})^{\\top}\\mathsf{M a t}_{(1,l-1)}(\\widehat{\\mathbf{v}}^{*})\\mathsf{V e c}(\\mathbf{w}^{*\\otimes l-1})=\\widehat{\\mathbf{v}}^{*}\\cdot\\mathsf{V e c}(\\mathbf{w}^{*\\otimes l})\\geq1-\\epsilon_{1}$ , we have $\\rho_{1}\\geq1-\\epsilon_{1}$ , and thus ", "page_idx": 30}, {"type": "equation", "text": "$$\n(\\mathbf{w}^{*})^{\\top}\\mathsf{M a t}_{(1,l-1)}(\\widehat{\\mathbf{v}}^{*})\\mathsf{V e c}(\\mathbf{w}^{*\\otimes l-1})=\\sum_{i=1}^{d}\\rho_{i}(\\mathbf{w}^{*}\\cdot\\mathbf{u}^{(i)})(\\mathsf{V e c}(\\mathbf{w}^{*\\otimes l-1})\\cdot\\mathbf{r}^{(i)})\\ge1-\\epsilon_{1}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Since $\\mathbf{u}^{(i)}$ , $i\\in[d]$ and $\\mathbf{r}^{(i)}$ , $i\\in[d^{l-1}]$ form orthonormal bases of $\\mathbb{R}^{d}$ and $\\mathbb{R}^{d^{l}}$ respectively, we can decompose $\\mathbf{w}^{*}$ and $\\mathsf{V e c}(\\mathbf{w}^{*\\otimes l-1})$ in these bases respectively: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbf{w}^{*}=\\sum_{i=1}^{d}a_{i}\\mathbf{u}^{(i)},\\quad\\mathsf{V e c}(\\mathbf{w}^{*\\otimes l-1})=\\sum_{i=1}^{d^{l-1}}b_{i}\\mathbf{r}^{(i)}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Note that since $\\|\\mathbf{w}^{*}\\|_{2}\\,=\\,1$ and $\\|\\mathsf{V e c}(\\mathbf{w}^{*\\otimes l-1})\\|_{2}^{2}=\\|\\mathbf{w}^{*\\otimes l-1}\\|_{F}^{2}=1$ , we have $\\textstyle\\sum_{i}a_{i}^{2}\\,=\\,1$ and $\\sum_{i}b_{i}^{2}=1$ . In addition, since $\\|\\widehat{\\mathbf{v}}^{*}\\|_{2}=1$ , we have $\\begin{array}{r}{\\|\\mathsf{M}\\mathsf{a t}_{(1,l-1)}(\\widehat{\\mathbf{v}}^{*})\\|_{F}^{2}=\\sum_{i=1}^{d}\\rho_{i}^{2}=1}\\end{array}$ . Therefore, plugging the decomposition abo ve back into Equation (36), we g et ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{1-\\epsilon_{1}\\leq\\displaystyle\\sum_{i=1}^{d}\\rho_{i}a_{i}b_{i}\\leq\\rho_{1}a_{1}b_{1}+\\rho_{2}\\sqrt{\\displaystyle\\sum_{i=2}^{d}a_{i}^{2}}\\sqrt{\\displaystyle\\sum_{i=2}^{d}b_{i}^{2}}}}\\\\ {{\\leq\\rho_{1}a_{1}b_{1}+\\sqrt{1-\\rho_{1}^{2}}\\sqrt{1-a_{1}^{2}}\\sqrt{1-b_{1}^{2}}\\leq\\rho_{1}a_{1}b_{1}+\\sqrt{1-\\rho_{1}^{2}}(1-a_{1}b_{1}).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "When $\\rho_{1}\\geq1-\\epsilon_{1}\\geq\\sqrt{2}/2$ , we have $\\rho_{1}-\\sqrt{1-\\rho_{1}^{2}}\\ge0$ and then it holds that ", "page_idx": 31}, {"type": "equation", "text": "$$\na_{1}b_{1}\\geq\\frac{1-\\sqrt{1-\\rho_{1}^{2}}-\\epsilon_{1}}{\\rho_{1}-\\sqrt{1-\\rho_{1}^{2}}}:=g(\\rho_{1}).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We show that when $0\\,\\leq\\,\\epsilon_{1}\\,\\leq\\,1/16$ (which implies that $15/16\\le\\rho_{1}\\le1\\rangle$ ), it holds that $g(\\rho_{1})\\geq$ $1-2\\epsilon_{1}$ . By the definition of $g(\\rho_{1})$ , it suffices to argue that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{1-\\rho_{1}\\geq\\epsilon_{1}\\big(1-2\\rho_{1}+2\\sqrt{1-\\rho_{1}^{2}}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "This follows by direct calculations as when $\\rho_{1}\\in[15/16,1],1-2\\rho_{1}+2\\sqrt{1-\\rho_{1}^{2}}\\leq0.$ ", "page_idx": 31}, {"type": "text", "text": "Therefore, when $\\epsilon_{1}\\leq1/16$ , it holds that $a_{1}b_{1}\\geq1-2\\epsilon_{1}$ . Since $0\\leq a_{1},b_{1}\\leq1$ , it must be that $a_{1}\\geq1-2\\epsilon_{1}$ ; this further implies that $\\mathbf{w}^{*}\\cdot\\mathbf{u}\\geq1-2\\epsilon_{1}$ . \u53e3 ", "page_idx": 31}, {"type": "text", "text": "D.3 Proof of Proposition D.2 ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Proof of Proposition $D.2$ . Since $\\sqrt{\\mathrm{OPT}}\\,\\leq\\,c_{k^{*}}/(64k^{*})\\,\\leq\\,c_{k^{*}}/64$ , choosing $\\epsilon_{0}={c_{k^{*}}}/({256k^{*}})\\leq$ $c_{k^{*}}/256$ in Lemma D.13, we obtain that using $n=\\Theta((k^{*})^{2}e^{k^{*}}\\log^{k^{*}}(B_{4}/\\epsilon)d^{[k^{*}/2]}/(c_{k^{*}}^{2})+1/\\epsilon)$ , it holds with probability at least $1-\\exp(-d^{1/2})$ that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{\\mathbf{v}}^{*}\\cdot\\mathsf{V e c}(\\mathbf{w}^{*\\otimes l})\\ge1-\\displaystyle\\frac{2}{c_{k}}\\sqrt{\\mathrm{OPT}}-\\frac{2\\epsilon_{0}}{\\left(c_{k}/2-4\\sqrt{\\mathrm{OPT}}\\right)-\\epsilon_{0}}}\\\\ &{\\qquad\\qquad\\qquad\\ge1-\\displaystyle\\frac{1}{32k^{*}}-\\frac{c_{k^{*}}/(128k^{*})}{c_{k^{*}}/2-c_{k^{*}}/16-c_{k^{*}}/256}\\ge1-\\displaystyle\\frac{1}{16k^{*}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Then applying Lemma D.17 with $\\epsilon_{1}\\leq1/(16k^{*})\\leq1/16$ we get that the output $\\mathbf{u}$ of Algorithm 3 satisfies $\\mathbf{u}\\cdot\\mathbf{w}^{*}\\geq1-2\\epsilon_{1}\\geq1-1/(8k^{*})\\geq1-\\operatorname*{min}\\{1/k^{*},1/2\\}$ , completing the proof. \u53e3 ", "page_idx": 31}, {"type": "text", "text": "D.4 Proof of Proposition D.3 ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Proof of Proposition D.3. Since $\\sqrt{\\mathrm{OPT}}~\\leq~c_{k^{*}}/64$ and $\\epsilon\\ \\leq\\ 1/64$ , choosing $\\epsilon_{0}~=~c_{k^{*}}\\epsilon/16$ in Lemma D.13, we obtain that using $n=\\Theta(e^{k^{*}}\\log^{k^{*}}(B_{4}/\\epsilon)d^{lceil k^{*}/2\\rceil}/(c_{k^{*}}^{2}\\epsilon^{2})+1/\\epsilon)$ samples, it holds with probability at least $1-\\exp(-d^{1/2})$ that $\\widehat{\\mathbf{v}}^{*}\\cdot\\mathsf{V e c}(\\mathbf{w}^{*\\otimes l})\\geq1-(2/c_{k^{*}})\\sqrt{\\mathrm{OPT}}+\\epsilon/3(\\geq15/16)$ . Then applying Lemma D.17 with $\\epsilon_{1}=(2/c_{k^{*}})\\sqrt{\\mathrm{OPT}}-\\epsilon/3$ , we get that the output $\\mathbf{w}^{0}$ of Algorithm 3 satisfies $\\mathbf{w}^{0}\\cdot\\mathbf{w}^{*}\\ge1-2((2/c_{k^{*}})\\sqrt{\\mathrm{OPT}}+\\epsilon/3)$ . ", "page_idx": 31}, {"type": "text", "text": "Finally, to show the upper bound on the $L_{2}^{2}$ loss of $\\mathbf{w}^{0}$ , we bring in the definition of the $L_{2}^{2}$ loss $\\mathcal{L}_{2}^{\\sigma}(\\mathbf{w}^{\\mathbf{\\bar{0}}})$ , which yields ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{2}^{\\sigma}(\\mathbf{w}^{0})\\leq2\\mathrm{OPT}+2\\mathcal{L}_{2}^{*\\sigma}(\\mathbf{w}^{0})=2\\mathrm{OPT}+2\\bigg(1-\\displaystyle\\sum_{k\\geq k^{*}}c_{k}^{2}(\\mathbf{w}^{0}\\cdot\\mathbf{w}^{*})^{k}\\bigg)}\\\\ &{\\phantom{=}=2\\mathrm{OPT}+2\\bigg(\\displaystyle\\sum_{k\\geq k^{*}}c_{k}^{2}(1-(\\mathbf{w}^{0}\\cdot\\mathbf{w}^{*})^{k})\\bigg)}\\\\ &{\\phantom{=}=2\\mathrm{OPT}+2\\bigg(\\displaystyle\\sum_{k\\geq k^{*}}c_{k}^{2}(1-(\\mathbf{w}^{0}\\cdot\\mathbf{w}^{*}))(1+(\\mathbf{w}^{0}\\cdot\\mathbf{w}^{*})+\\cdots+(\\mathbf{w}^{0}\\cdot\\mathbf{w}^{*})^{k-1})\\bigg)}\\\\ &{\\phantom{=}\\leq2\\mathrm{OPT}+2\\bigg(\\displaystyle\\sum_{k\\geq k^{*}}k c_{k}^{2}(1-(\\mathbf{w}^{0}\\cdot\\mathbf{w}^{*}))\\bigg)\\lesssim\\bigg(\\displaystyle\\sum_{k\\geq k^{*}}k c_{k}^{2}\\bigg)\\bigg(\\displaystyle\\frac{4}{c_{k^{*}}}\\sqrt{\\mathrm{OPT}}+\\epsilon/3\\bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Since $\\begin{array}{r}{\\sum_{k\\geq k^{*}}k c_{k}^{2}\\leq C_{k^{*}}}\\end{array}$ by Assumption $1(i i i)$ , this completes the proof of Proposition D.3. ", "page_idx": 32}, {"type": "text", "text": "E Full Version of Section 3 ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "After getting an initialized vector $\\mathbf{w}^{0}$ using Algorithm 1, we run Riemannian minibatch SGD Algorithm 2 on the \u2018truncated loss\u2019. In the following sections, we will first present the definition of the truncated $L_{2}^{2}$ loss $\\mathcal{L}_{2}^{\\phi}$ and its Riemannian gradient, then we will proceed to show that Algorithm 2 converges to a constant approximate solution in ${\\cal O}(\\log(1/\\epsilon))$ iterations. ", "page_idx": 32}, {"type": "text", "text": "Algorithm 4 Riemannian GD with Warm-start   \n1: Input: Parameters $\\epsilon$ , $k^{*}$ , $c_{k^{*}},B_{4}>0;T,\\eta$ ; Sample access to $\\mathcal{D}$ .   \n2: $\\mathbf{w}^{(\\hat{0}}=$ Initialization $[\\epsilon,k^{*},c_{k^{*}},B_{4},\\epsilon_{0}=c_{k^{*}}/(256k^{*})]$ .   \n3: for $t=0,\\dots,T-1$ do   \n4: Draw $n=\\Theta(C_{k^{*}}d e^{k^{*}}\\log^{k^{*}+1}(B_{4}/\\epsilon)/(\\epsilon\\delta))$ samples from $\\mathcal{D}$ and compute $\\widehat{\\mathbf{g}}(\\mathbf{w}^{t})=\\frac{1}{n}\\sum_{i=1}^{n}k^{*}c_{k^{*}}y^{(i)}(\\mathbf{I}-\\mathbf{w}^{t}(\\mathbf{w}^{t})^{\\top})\\langle\\mathbf{H}\\mathbf e_{k^{*}}(\\mathbf{x}^{(i)}),(\\mathbf{w}^{t})^{\\otimes k^{*}-1}\\rangle.$ 5: $\\mathbf{w}^{t+1}=\\underset{\\star}{\\big(}\\mathbf{w}^{t}-\\eta\\widehat{\\mathbf{g}}(\\mathbf{w}^{t})\\big)/\\|\\mathbf{w}^{t}-\\eta\\widehat{\\mathbf{g}}(\\mathbf{w}^{t})\\|_{2}.$   \n6: Return: wT . ", "page_idx": 32}, {"type": "text", "text": "E.1 Truncated Loss and the Sharpness property of the Riemannian Gradient ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Instead of directly minimizing the $L_{2}^{2}$ loss $\\mathcal{L}_{2}^{\\sigma}$ , we work with the following truncated loss that drops all the terms higher than $k^{*}$ in the polynomial expansion of $\\sigma$ : ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{2}^{\\phi}(\\mathbf{w}):=2\\bigl(1-\\underset{(\\mathbf{x},y)\\sim\\mathcal{D}}{\\mathbf{E}}[y\\phi(\\mathbf{w}\\cdot\\mathbf{x})]\\bigr),\\ \\mathrm{where}\\ \\phi(\\mathbf{w}\\cdot\\mathbf{x})=\\langle\\mathbf{He}_{k^{*}}(\\mathbf{x}),\\mathbf{w}^{\\otimes k^{*}}\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Similarly, the noiseless surrogate loss is defined as ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathcal{L}_{2}^{*\\phi}(\\mathbf{w}):=2\\bigl(1-\\underset{(\\mathbf{x},y)\\sim\\mathcal{D}}{\\mathbf{E}}[\\sigma(\\mathbf{w}^{*}\\cdot\\mathbf{x})\\phi(\\mathbf{w}\\cdot\\mathbf{x})]\\bigr)=2\\bigl(1-c_{k^{*}}(\\mathbf{w}\\cdot\\mathbf{w}^{*})^{k^{*}}\\bigr).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Using Fact C.1(2), the gradient of the truncated $L_{2}^{2}$ loss equals: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\nabla\\mathcal{L}_{2}^{\\phi}(\\mathbf{w})=-2\\underset{(\\mathbf{x},y)\\sim\\mathcal{D}}{\\mathbf{E}}[\\nabla\\phi(\\mathbf{w}\\cdot\\mathbf{x})y]=-2\\underset{(\\mathbf{x},y)\\sim\\mathcal{D}}{\\mathbf{E}}\\left[k^{*}c_{k^{*}}y\\langle\\mathbf{H}\\mathbf{e}_{k^{*}}(\\mathbf{x}),\\mathbf{w}^{\\otimes k^{*}-1}\\rangle\\right],\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "while for the gradient of the noiseless $L_{2}^{2}$ loss we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\nabla\\mathcal{L}_{2}^{*\\phi}(\\mathbf{w})=-2\\sum_{(\\mathbf{x},y)\\sim\\mathcal{D}}\\left[k^{*}c_{k^{*}}\\sigma(\\mathbf{w}^{*}\\cdot\\mathbf{x})\\langle\\mathbf{H}\\mathbf{e}_{k^{*}}(\\mathbf{x}),\\mathbf{w}^{\\otimes k^{*}-1}\\rangle\\right]\\,.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Recall that $\\mathbf{P}_{\\mathbf{w}^{\\perp}}:=\\mathbf{I}-\\mathbf{w}\\mathbf{w}^{\\top}$ . Then the Riemannian gradient of the $L_{2}^{2}$ loss $\\mathcal{L}_{2}^{\\phi}$ , denoted by $\\mathbf{g}(\\mathbf{w})$ is ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbf{g}(\\mathbf{w}):=\\mathbf{P}_{\\mathbf{w}^{\\perp}}(\\nabla\\mathcal{L}_{2}^{\\phi}(\\mathbf{w}))=-2\\sum_{(\\mathbf{x},\\mathbf{y})\\sim\\mathcal{D}}\\left[k^{*}y\\mathbf{P_{w^{\\perp}}}\\langle\\mathbf{H}\\mathbf{e}_{k^{*}}(\\mathbf{x}),\\mathbf{w}^{\\otimes k^{*}-1}\\rangle\\right].\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Similarly, the Riemannian gradient of the noiseless $L_{2}^{2}$ loss $\\boldsymbol{\\mathcal{L}}_{2}^{*\\phi}$ is defined by ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{g}^{*}(\\mathbf{w}):=\\mathbf{P}_{\\mathbf{w}^{\\perp}}(\\nabla\\mathcal{L}_{2}^{*\\phi}(\\mathbf{w}))=-2\\underset{(\\mathbf{x},y)\\sim\\mathcal{D}}{\\mathbf{E}}\\left[k^{*}\\sigma(\\mathbf{w}^{*}\\cdot\\mathbf{x})\\mathbf{P_{w^{\\perp}}}\\langle\\mathbf{H}\\mathbf{e}_{k^{*}}(\\mathbf{x}),\\mathbf{w}^{\\otimes k^{*}-1}\\rangle\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "The following claim establishes that $\\mathbf{g}^{*}(\\mathbf{w})$ carries information about the alignment between vectors w and $\\mathbf{w}^{*}$ . ", "page_idx": 33}, {"type": "text", "text": "Claim E.1. For any $\\mathbf{w}\\in\\mathbb{S}^{d-1}$ , we have $\\begin{array}{r}{\\mathbf{g}^{*}(\\mathbf{w})=-2k^{*}c_{k^{*}}(\\mathbf{w}\\cdot\\mathbf{w}^{*})^{k^{*}-1}(\\mathbf{w}^{*})^{\\perp_{\\mathbf{w}}}.}\\end{array}$ ", "page_idx": 33}, {"type": "text", "text": "Proof. Using the definition of $\\mathbf{g}^{\\ast}(\\mathbf{w})$ from Equation (42), a direct calculation shows that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ \\ \\mathbf{g}^{*}(\\mathbf{w})\\cdot\\frac{\\left(\\mathbf{w}^{*}\\right)^{\\perp_{\\mathbf{w}}}}{\\left\\Vert\\left(\\mathbf{w}^{*}\\right)^{\\perp_{\\mathbf{w}}}\\right\\Vert_{2}}}\\\\ &{\\overset{(i)}{=}\\ -2k^{*}\\underset{\\mathbf{x}\\sim\\mathcal{N}_{d}}{\\mathbf{E}}\\left[\\underset{k\\geq k^{*}}{\\sum_{k^{*}}}c_{k}\\Bigg\\langle\\mathbf{He}_{k}(\\mathbf{x}),\\mathbf{w}^{*\\otimes k}\\Bigg\\rangle\\cdot\\left\\langle\\mathbf{He}_{k^{*}}(\\mathbf{x}),\\mathbf{w}^{\\otimes k^{*}-1}\\otimes\\frac{\\left(\\mathbf{w}^{*}\\right)^{\\perp_{\\mathbf{w}}}}{\\left\\Vert\\left(\\mathbf{w}^{*}\\right)^{\\perp_{\\mathbf{w}}}\\right\\Vert_{2}}\\Bigg\\rangle\\right]}\\\\ &{\\overset{(i i)}{=}\\ -2k^{*}c_{k^{*}}\\Bigg\\langle\\mathrm{Sym}(\\mathbf{w}^{*\\otimes k^{*}}),\\mathrm{Sym}\\Bigg(\\mathbf{w}^{\\otimes k^{*}-1}\\otimes\\frac{\\left(\\mathbf{w}^{*}\\right)^{\\perp_{\\mathbf{w}}}}{\\left\\Vert\\left(\\mathbf{w}^{*}\\right)^{\\perp_{\\mathbf{w}}}\\right\\Vert_{2}}\\Bigg)\\Bigg\\rangle}\\\\ &{\\overset{(i i i)}{=}\\ -2k^{*}c_{k^{*}}\\big(\\mathbf{w}\\cdot\\mathbf{w}^{*}\\big)^{k^{*}-1}\\big\\Vert\\big(\\mathbf{w}^{*}\\big)^{\\perp_{\\mathbf{w}}}\\big\\Vert_{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $(i)$ is by the definition of $\\mathbf{g}^{\\ast}(\\mathbf{w})$ and $\\sigma(\\mathbf{w}^{*}\\cdot\\mathbf{x})$ , $(i i)$ is by Fact C.3, and $(i i i)$ is by Fact C.1(1), Equation (20), and $\\|\\mathbf{w}^{*}\\|_{2}=1.$ Let $\\mathbf{v}\\in\\mathbb{R}^{d}$ be any unit vector that is orthogonal to $(\\mathbf{w}^{*})^{\\perp}\\mathbf{w}$ . Observe that $\\mathbf{v}^{\\perp\\mathbf{w}}\\cdot\\mathbf{w}^{*}=\\mathbf{v}\\cdot(\\mathbf{w}^{*})^{\\perp\\mathbf{w}}=0$ . Thus, we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{g}^{*}(\\mathbf{w})\\cdot\\mathbf{v}=-2k^{*}c_{k^{*}}\\Big\\langle\\mathrm{Sym}(\\mathbf{v}^{\\perp_{\\mathbf{w}}}\\otimes\\mathbf{w}^{\\otimes k^{*}-1}),\\mathrm{Sym}(\\mathbf{w}^{*\\otimes k^{*}})\\Big\\rangle}\\\\ &{\\qquad\\qquad=-2k^{*}c_{k^{*}}(\\mathbf{w}\\cdot\\mathbf{w}^{*})^{k^{*}-1}(\\mathbf{v}^{\\perp_{\\mathbf{w}}}\\cdot\\mathbf{w}^{*})=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "This implies that $\\mathbf{g}^{*}(\\mathbf{w})$ is parallel to $(\\mathbf{w}^{*})^{\\perp}\\mathbf{w}$ and thus $\\mathbf{g}^{*}(\\mathbf{w})=-2k^{*}c_{k^{*}}(\\mathbf{w}\\!\\cdot\\!\\mathbf{w}^{*})^{k^{*}-1}(\\mathbf{w}^{*})^{\\perp}\\mathbf{w}$ . L ", "page_idx": 33}, {"type": "text", "text": "Let us denote the difference between the noisy and the noiseless Riemannian gradient by $\\xi(\\mathbf{w})$ : ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\xi(\\mathbf{w}):=\\mathbf{g}(\\mathbf{w})-\\mathbf{g}^{*}(\\mathbf{w})=-2\\sum_{(\\mathbf{x},y)\\sim\\mathcal{D}}[(y-\\sigma(\\mathbf{w}^{*}\\cdot\\mathbf{x}))\\mathbf{P}_{\\mathbf{w}^{\\perp}}\\nabla\\phi(\\mathbf{w}\\cdot\\mathbf{x})]\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "We next show that the norm of $\\xi(\\mathbf{w})$ and the inner product between $\\xi(\\mathbf{w})$ and $\\mathbf{w}^{*}$ are both bounded: Lemma E.2. Let $\\xi(\\mathbf{w})=\\mathbf{g}(\\mathbf{w})-\\mathbf{g}^{*}(\\mathbf{w})$ as defined above. Then, ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\xi(\\mathbf{w})\\|_{2}\\leq2k^{*}c_{k^{*}}\\sqrt{\\mathrm{OPT}}\\quad a n d\\quad|\\xi(\\mathbf{w})\\cdot\\mathbf{w}^{*}|\\leq2k^{*}c_{k^{*}}\\sqrt{\\mathrm{OPT}}\\|(\\mathbf{w}^{*})^{\\perp_{\\mathbf{w}}}\\|_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Proof. Using the definition of $\\xi(\\mathbf{w})$ and the definition of the 2-norm, ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\xi(\\mathbf{w})|\\|_{2}=2k^{*}c_{k^{*}}\\underset{\\mathbf{v}\\in\\mathbb{S}^{d-1}}{\\operatorname*{max}}\\underset{\\mathbf{\\psi}\\in\\mathbb{S}^{d-1}}{\\mathbf{E}}\\left[(y-\\sigma(\\mathbf{w}^{*}\\cdot\\mathbf{x}))\\mathbf{P}_{\\mathbf{w}^{\\perp}}\\left\\langle\\mathbf{H}\\mathbf{e}_{k^{*}}(\\mathbf{x}),\\mathbf{w}^{\\otimes k^{*}-1}\\right\\rangle\\cdot\\mathbf{v}\\right]}\\\\ &{\\qquad=2k^{*}c_{k^{*}}\\underset{\\mathbf{v}\\in\\mathbb{S}^{d-1}}{\\operatorname*{max}}\\underset{\\mathbf{\\psi}\\in\\mathbb{S}^{d-1}}{\\mathbf{E}}\\left[(y-\\sigma(\\mathbf{w}^{*}\\cdot\\mathbf{x}))\\langle\\mathbf{H}\\mathbf{e}_{k^{*}}(\\mathbf{x}),\\mathbf{v}^{\\perp_{\\mathbf{w}}}\\otimes\\mathbf{w}^{\\otimes k^{*}-1}\\rangle\\right]}\\\\ &{\\qquad\\leq2k^{*}c_{k^{*}}\\underset{\\mathbf{v}\\in\\mathbb{S}^{d-1}}{\\operatorname*{max}}\\sqrt{\\underset{(\\mathbf{v},y)\\sim\\mathcal{D}}{\\mathbf{E}}[(y-\\sigma(\\mathbf{w}^{*}\\cdot\\mathbf{x}))^{2}]\\underset{\\mathbf{\\psi}\\sim\\mathcal{N}_{d}}{\\mathbf{E}}\\left[\\left(\\langle\\mathbf{H}\\mathbf{e}_{k^{*}}(\\mathbf{x}),\\mathbf{v}^{\\perp_{\\mathbf{w}}}\\otimes\\mathbf{w}^{\\otimes k^{*}-1}\\rangle\\right)^{2}\\right]}}\\\\ &{\\qquad=2k^{*}c_{k^{*}}\\underset{\\mathbf{v}\\in\\mathbb{S}^{d-1}}{\\operatorname*{max}}\\sqrt{\\mathrm{OPT}}\\|\\mathrm{Sym}(\\mathbf{v}^{\\perp_{\\mathbf{w}}}\\otimes\\mathbf{w}^{\\otimes k^{*}-1})\\|_{F},}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where the (only) inequality is by Cauchy-Schwarz, and the last equality is by Fact C.3. As a tensor $\\mathbf{A}$ , it holds $\\|\\mathrm{Sym}(\\mathbf{A})\\|_{F}\\leq\\|\\mathbf{A}\\|_{F}$ , we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\|\\mathrm{Sym}(\\mathbf{v}^{\\perp_{\\mathbf{w}}}\\otimes\\mathbf{w}^{\\otimes k^{*}-1})\\|_{F}^{2}\\leq\\|\\mathbf{v}^{\\perp_{\\mathbf{w}}}\\otimes\\mathbf{w}^{\\otimes k^{*}-1}\\|_{F}^{2}=\\|\\mathbf{v}^{\\perp_{\\mathbf{w}}}\\|_{2}^{2}\\leq1,\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "which then implies that4 ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\|\\xi(\\mathbf{w})\\|_{2}\\leq2k^{*}c_{k^{*}}\\sqrt{\\mathrm{OPT}}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Following the same line of argument as above, we also get ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\xi(\\mathbf{w})\\cdot\\mathbf{w}^{*}|\\leq2\\sqrt{\\mathrm{OPT}}\\sqrt{(k^{*}c_{k^{*}})^{2}\\|(\\mathbf{w}^{*})^{\\perp_{\\mathbf{w}}}\\otimes\\mathbf{w}^{\\otimes k^{*}-1}\\|_{F}^{2}}=2k^{*}c_{k^{*}}\\sqrt{\\mathrm{OPT}}\\|(\\mathbf{w}^{*})^{\\perp_{\\mathbf{w}}}\\|_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "This completes the proof of Lemma E.2. ", "page_idx": 34}, {"type": "text", "text": "As a direct corollary of Lemma E.2, we now show that the norm of the noisy gradient $\\|\\mathbf{g}(\\mathbf{x})\\|_{2}$ is close to the norm of the noiseless gradient $\\lVert\\mathbf{g}^{*}(\\mathbf{w})\\rVert_{2}$ . ", "page_idx": 34}, {"type": "text", "text": "Corollary E.3. For any w $\\cdot\\in\\mathbb{S}^{d-1}$ , $\\|\\mathbf{g}(\\mathbf{w})\\|_{2}\\leq2k^{*}c_{k^{*}}\\sqrt{\\mathrm{OPT}}+\\|\\mathbf{g}^{*}(\\mathbf{w})\\|_{2}.$ ", "page_idx": 34}, {"type": "text", "text": "Proof. Follows by the triangle inequality, as $\\|\\mathbf{g}(\\mathbf{w})\\|_{2}\\leq\\|\\xi(\\mathbf{w})\\|_{2}+\\|\\mathbf{g}^{*}(\\mathbf{w})\\|_{2}$ . ", "page_idx": 34}, {"type": "text", "text": "We are now ready to present the main structural result of this section. ", "page_idx": 34}, {"type": "text", "text": "Lemma E.4 (Sharpness). Assume $\\mathrm{OPT}\\,\\leq\\,c/(4e)^{2}$ for some small absolute constant $c<1$ . Let $\\textbf{w}\\in\\mathbb{R}^{d}$ s\u221auch that $\\|\\mathbf{w}\\|_{2}\\,=\\,1$ and suppose that $\\mathbf{w}\\cdot\\mathbf{w}^{*}\\;\\geq\\;1\\,-\\,1/k^{*}$ . Let $\\theta\\;:=\\;\\theta(\\mathbf{w},\\mathbf{w}^{*})$ . If $\\sin\\theta\\geq4e{\\sqrt{\\mathrm{{OPT}}}}$ , then ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbf{g}(\\mathbf{w})\\cdot\\mathbf{w}^{*}\\leq-\\frac{1}{2}\\|\\mathbf{g}^{*}(\\mathbf{w})\\|_{2}\\sin\\theta.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Proof. We start by noticing that by Claim E.1, the noiseless gradient satisfies the following property: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbf{g}^{*}(\\mathbf{w})\\cdot\\mathbf{w}^{*}=-2k^{*}c_{k^{*}}(\\mathbf{w}\\cdot\\mathbf{w}^{*})^{k^{*}-1}\\lVert(\\mathbf{w}^{*})^{\\perp_{\\mathbf{w}}}\\rVert_{2}^{2}=-\\lVert\\mathbf{g}^{*}(\\mathbf{w})\\rVert_{2}\\sin\\theta,\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where we used that since $\\|\\mathbf{w}\\|_{2}=\\|\\mathbf{w}^{*}\\|_{2}=1$ , we have $\\|(\\mathbf{w}^{*})^{\\perp_{\\mathbf{w}}}\\|_{2}=\\sin\\theta_{}$ . Furthermore, applying Lemma E.2 we have the following sharpness property with respect to the $L_{2}^{2}$ loss: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{g}(\\mathbf{w})\\cdot\\mathbf{w}^{*}=\\mathbf{g}^{*}(\\mathbf{w})\\cdot\\mathbf{w}^{*}+\\xi(\\mathbf{w})\\cdot\\mathbf{w}^{*}\\le-(\\|\\mathbf{g}^{*}(\\mathbf{w})\\|_{2}-2k^{*}c_{k^{*}}\\sqrt{\\mathrm{OPT}})\\sin\\theta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Observe that $(1-1/t)^{t-1}\\geq1/e$ for all $t\\geq1$ . Therefore, when $\\mathbf{w}\\cdot\\mathbf{w}^{*}\\geq1-1/k^{*}$ , the norm of the gradient vector $\\mathbf{g}^{\\ast}$ satisfies ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\mathbf{g}^{*}(\\mathbf{w})\\|_{2}=2k^{*}c_{k^{*}}\\big(\\mathbf{w}\\cdot\\mathbf{w}^{*}\\big)^{k^{*}-1}\\sin\\theta\\geq2k^{*}c_{k^{*}}(1-1/k^{*})^{k^{*}-1}\\sin\\theta\\geq e^{-1}k^{*}c_{k^{*}}\\sin\\theta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "therefore, when $\\sin\\theta\\geq4e{\\sqrt{\\mathrm{OPT}}}$ and $\\mathbf{w}\\cdot\\mathbf{w}^{*}\\geq1-1/k^{*}$ , we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\mathbf{g}^{*}(\\mathbf{w})\\|_{2}\\geq4k^{*}c_{k^{*}}\\sqrt{\\mathrm{OPT}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Thus, as long as $\\sin\\theta\\geq4e{\\sqrt{\\mathrm{OPT}}}$ , we have that $\\begin{array}{r}{\\mathbf{g}(\\mathbf{w})\\cdot\\mathbf{w}^{*}\\leq-\\frac{1}{2}\\|\\mathbf{g}^{*}(\\mathbf{w})\\|_{2}\\sin\\theta}\\end{array}$ . ", "page_idx": 34}, {"type": "text", "text": "E.2 Concentration of Gradients ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "For notational simplicity, define: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{g}(\\mathbf{w};\\mathbf{x}^{(i)},y^{(i)}):=k^{*}c_{k^{*}}y^{(i)}\\mathbf{P_{w^{\\perp}}}\\langle\\mathbf{H}\\mathbf{e}_{k^{*}}(\\mathbf{x}^{(i)}),\\mathbf{w}^{\\otimes k^{*}-1}\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Then the empirical estimate of $\\mathbf{g}(\\mathbf{w})$ is ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\widehat{\\mathbf{g}}(\\mathbf{w}):=\\frac{1}{n}\\sum_{i=1}^{n}\\mathbf{g}(\\mathbf{w};\\mathbf{x}^{(i)},y^{(i)})=\\frac{1}{n}\\sum_{i=1}^{n}k^{*}c_{k^{*}}y^{(i)}\\mathbf{P_{w^{\\perp}}}\\big\\langle\\mathbf{H}\\mathbf{e}_{k^{*}}\\big(\\mathbf{x}^{(i)}\\big),\\mathbf{w}^{\\otimes k^{*}-1}\\big\\rangle.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "The following lemma provides the upper bounds on the number of samples required to approximate the Riemannian gradient $\\mathbf{g}(\\mathbf{w})$ by $\\widehat{\\bf g}({\\bf w})$ . ", "page_idx": 34}, {"type": "text", "text": "Lemma E.5 (Concentration of Gradients). Let w\u2217, $\\mathbf{w}\\in\\mathbb{S}^{d-1}$ . Let $\\widehat{\\bf g}({\\bf w})$ be the empirical estimate of the Riemannian gradient. Furthermore, denote the angle betwe en w and $\\mathbf{w}^{*}$ by $\\theta$ . Then, with probability at least $1-\\delta$ it holds ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\widehat{\\mathbf{g}}(\\mathbf{w})-\\mathbf{g}(\\mathbf{w})\\|_{2}\\lesssim\\sqrt{\\frac{d\\left(k^{*}c_{k^{*}}\\right)^{2}e^{k^{*}}\\log^{k^{*}}\\left(B_{4}/\\epsilon\\right)}{n\\delta}};}\\\\ &{(\\widehat{\\mathbf{g}}(\\mathbf{w})-\\mathbf{g}(\\mathbf{w}))\\cdot\\mathbf{w}^{*}\\lesssim\\sqrt{\\frac{\\left(k^{*}c_{k^{*}}\\right)^{2}e^{k^{*}}\\log^{k^{*}}\\left(B_{4}/\\epsilon\\right)\\sin^{4}(\\theta)}{n\\delta}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Proof. By Chebyshev\u2019s inequality, we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{Pr}\\left[\\left\\lVert\\frac{1}{n}\\sum_{i=1}^{n}\\mathbf{g}(\\mathbf{w};\\mathbf{x}^{(i)},y^{(i)})-\\mathbf{g}(\\mathbf{w})\\right\\rVert_{2}\\geq t\\right]\\leq\\frac{1}{t^{2}}\\underset{(\\mathbf{x},y)\\sim\\mathcal{D}}{\\mathbf{E}}\\left[\\left\\lVert\\frac{1}{n}\\sum_{i=1}^{n}\\mathbf{g}(\\mathbf{w};\\mathbf{x}^{(i)},y^{(i)})-\\mathbf{g}(\\mathbf{w})\\right\\rVert_{2}^{2}\\right]}\\\\ {\\leq\\frac{1}{n t^{2}}\\underset{(\\mathbf{x},y)\\sim\\mathcal{D}}{\\mathbf{E}}\\left[\\left\\lVert\\mathbf{g}(\\mathbf{w};\\mathbf{x},y)-\\mathbf{g}(\\mathbf{w})\\right\\rVert_{2}^{2}\\right]\\cdot\\quad\\quad(46)\\leq2\\frac{1}{n}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Let ${\\bf{e}}_{j}$ be the $j^{\\mathrm{th}}$ basis of $\\mathbb{R}^{d}$ , we have $\\begin{array}{r}{\\|\\mathbf{g}(\\mathbf{w};\\mathbf{x},y)-\\mathbf{g}(\\mathbf{w})\\|_{2}^{2}=\\sum_{j=1}^{d}(\\mathbf{g}(\\mathbf{w};\\mathbf{x},y)\\cdot\\mathbf{e}_{j}-\\mathbf{g}(\\mathbf{w})\\cdot\\mathbf{e}_{j})^{2}}\\end{array}$ . Thus, it suffices to bound the expectation of each summand $(\\mathbf{g}(\\mathbf{w};\\mathbf{x},y)\\cdot\\mathbf{e}_{j}-\\mathbf{g}(\\mathbf{w})\\cdot\\mathbf{e}_{j})^{2}$ , for $j\\in[d]$ . Note first that since $\\mathbf{g}(\\mathbf{w})=\\bar{\\mathbf{E}_{(\\mathbf{x},y)\\sim\\mathcal{D}}}[\\mathbf{g}(\\mathbf{w};\\mathbf{x},y)]$ , we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\mathbf{x},y)\\sim\\mathcal{D}}{\\mathbf{E}}[(\\mathbf{g}(\\mathbf{w};\\mathbf{x},y)\\cdot\\mathbf{e}_{j}-\\mathbf{g}(\\mathbf{w})\\cdot\\mathbf{e}_{j})^{2}]\\leq\\underset{(\\mathbf{x},y)\\sim\\mathcal{D}}{\\mathbf{E}}[(\\mathbf{g}(\\mathbf{w};\\mathbf{x},y)\\cdot\\mathbf{e}_{j})^{2}]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=4(k^{*}c_{k^{*}})^{2}\\underset{(\\mathbf{x},y)\\sim\\mathcal{D}}{\\mathbf{E}}\\left[y^{2}\\bigg\\langle\\mathbf{H}\\mathbf{e}_{k^{*}}(\\mathbf{x}),\\mathbf{e}_{j}^{\\perp_{\\infty}}\\otimes\\mathbf{w}^{\\otimes k^{*}-1}\\bigg\\rangle^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Denote $f_{j}(\\mathbf{x}):=\\langle\\mathbf{H}\\mathbf{e}_{k^{*}}(\\mathbf{x}),\\mathbf{e}_{j}^{\\perp\\mathbf{w}}\\otimes\\mathbf{w}^{\\otimes k^{*}-1}\\rangle$ , which is a polynomial of $\\mathbf{x}$ of degree $k^{*}$ . In addition, note that $\\mathbf{E}_{\\mathbf{x}\\sim\\mathcal{N}_{d}}[f_{j}(\\mathbf{x})^{2}]=\\|\\mathbf{e}_{j}^{\\perp\\mathbf{w}}\\otimes\\mathbf{w}^{\\otimes k^{*}-1}\\|_{F}^{2}\\leq\\|\\mathbf{e}_{j}^{\\perp\\mathbf{w}}\\|_{2}^{2}\\leq1.$ Therefore, applying Fact D.9 and Fact D.10 with $A=y^{2}$ and $B=f_{j}(\\mathbf{x})^{2}$ , we get ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{(\\mathbf{x},y)\\sim\\mathcal{D}}{\\mathbf{E}}\\left[y^{2}\\bigg\\langle\\mathbf{He}_{k^{*}}(\\mathbf{x}),\\mathbf{e}_{j}^{\\perp_{\\mathbf{w}}}\\otimes\\mathbf{w}^{\\otimes k^{*}-1}\\bigg\\rangle^{2}\\right]\\leq\\underset{(\\mathbf{x},y)\\sim\\mathcal{D}}{\\mathbf{E}}[y^{2}](4e)^{k^{*}}\\operatorname*{max}\\left\\{1,\\frac{1}{k^{*}}\\log(B_{4}/\\epsilon)\\right\\}^{k^{*}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\lesssim e^{k^{*}}\\log^{k^{*}}(B_{4}/\\epsilon).}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Thus, it holds that $\\mathbf{E}_{(\\mathbf{x},y)\\sim\\mathcal{D}}[(\\mathbf{g}(\\mathbf{w};\\mathbf{x},y)\\cdot\\mathbf{e}_{j}-\\mathbf{g}(\\mathbf{w})\\cdot\\mathbf{e}_{j})^{2}]\\lesssim(k^{*}c_{k^{*}})^{2}e^{k^{*}}\\log^{k^{*}}(B_{4}/\\epsilon)$ , which further implies that the expectation of $\\|\\mathbf{g}(\\mathbf{w};\\mathbf{x},y)-\\mathbf{g}(\\mathbf{w})\\|_{2}^{2}$ is bounded above by ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{(\\mathbf{x},y)\\sim\\mathcal{D}}{\\mathbf{E}}[||\\mathbf{g}(\\mathbf{w};\\mathbf{x},y)-\\mathbf{g}(\\mathbf{w})||_{2}^{2}]\\lesssim d(k^{*}c_{k^{*}})^{2}e^{k^{*}}\\log^{k^{*}}(B_{4}/\\epsilon).}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Plugging this back into the Chebyshev\u2019s bound Equation (46), we obtain ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathbf{Pr}[\\|\\widehat{\\mathbf{g}}(\\mathbf{w})-\\mathbf{g}(\\mathbf{w})\\|_{2}\\ge t]\\le\\frac{d(k^{*}c_{k^{*}})^{2}e^{k^{*}}\\log^{k^{*}}(B_{4}/\\epsilon)}{n t^{2}}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Therefore, with probability at least $1-\\delta$ , it holds ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\|\\widehat{\\mathbf{g}}(\\mathbf{w})-\\mathbf{g}(\\mathbf{w})\\|_{2}\\le\\sqrt{\\frac{d(k^{*}c_{k^{*}})^{2}e^{k^{*}}\\log^{k^{*}}(B_{4}/\\epsilon)}{n\\delta}}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Now for the second statement of Lemma E.5, we similarly apply Chebyshev\u2019s inequality, which yields ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbf{Pr}\\left[\\left|\\displaystyle\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\mathbf{g}(\\mathbf{w};\\mathbf{x}^{(i)},y^{(i)})\\cdot\\mathbf{w}^{*}-\\mathbf{g}(\\mathbf{w})\\cdot\\mathbf{w}^{*}\\right|\\geq t\\right]}\\\\ &{\\leq\\displaystyle\\frac{1}{t^{2}}\\displaystyle\\sum_{(\\mathbf{x},y)\\sim\\mathcal{D}}\\left[\\left|\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\mathbf{g}(\\mathbf{w};\\mathbf{x}^{(i)},y^{(i)})\\cdot\\mathbf{w}^{*}-\\mathbf{g}(\\mathbf{w})\\cdot\\mathbf{w}^{*}\\right|^{2}\\right]}\\\\ &{\\leq\\displaystyle\\frac{1}{n t^{2}}\\displaystyle\\sum_{(\\mathbf{x},y)\\sim\\mathcal{D}}\\big[(\\mathbf{g}(\\mathbf{w};\\mathbf{x},y)\\cdot\\mathbf{w}^{*})^{2}\\big]}\\\\ &{=\\displaystyle\\frac{4(k^{*}c_{k^{*}})^{2}}{n t^{2}}\\displaystyle\\sum_{(\\mathbf{x},y)\\sim\\mathcal{D}}\\big[(y\\langle\\mathbf{H}\\mathbf{e}_{k^{*}}(\\mathbf{x}),\\mathbf{w}^{\\otimes k^{*}-1}\\otimes(\\mathbf{w}^{*})^{\\perp\\infty}\\rangle)^{2}\\big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Let $f_{\\mathbf{w}^{\\ast}}(\\mathbf{x})\\ :=\\ \\langle\\mathbf{H}\\mathbf{e}_{k^{\\ast}}(\\mathbf{x}),\\mathbf{w}^{\\otimes k^{\\ast}-1}\\otimes(\\mathbf{w}^{\\ast})^{\\perp\\mathbf{w}}\\rangle$ . Note that $\\mathbf{E}_{\\mathbf{x}\\sim\\mathcal{N}_{d}}\\big[f_{\\mathbf{w}^{\\ast}}(\\mathbf{x})^{2}\\big]\\ =\\ \\|\\mathbf{w}^{\\otimes k^{\\ast}-1}\\ \\otimes$ $(\\mathbf{w}^{\\ast})^{\\perp_{\\mathbf{w}}}\\|_{F}^{2}=\\|(\\mathbf{w}^{\\ast})^{\\perp_{\\mathbf{w}}}\\|_{2}^{2}$ . Since $\\mathbf{w}^{*}$ , $\\mathbf{w}\\in\\mathbb{S}^{d-1}$ , we have $\\|(\\mathbf{w}^{*})^{\\perp_{\\mathbf{w}}}\\|_{2}=\\sin{\\theta}$ . Thus, by Fact D.9, \u2225 $f_{\\mathbf{w}^{*}}^{2}(\\mathbf{x})\\Vert_{L^{p}}=\\left(\\underset{\\mathbf{x}\\sim N_{d}}{\\mathbf{E}}[(f_{\\mathbf{w}^{*}}(\\mathbf{x}))^{2p}]\\right)^{2/(2p)}\\le\\left((2p-1)^{k^{*}/2}\\underset{\\mathbf{x}\\sim N_{d}}{\\mathbf{E}}[(f_{\\mathbf{w}^{*}}(\\mathbf{x}))^{2}]\\right)^{2}\\le(2p)^{k^{*}}\\sin^{4}\\theta.$ Hence, applying Fact D.10 with $A=y^{2},B=f_{\\mathbf{w}^{*}}^{2}(\\mathbf{x}),\\sigma_{B}=\\sin^{4}\\theta.$ , and $C=k^{*}$ , we obtain ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\underset{(\\mathbf{x},y)\\sim\\mathcal{D}}{\\mathbf{E}}[(y\\langle\\mathbf{H}\\mathbf{e}_{k^{*}}(\\mathbf{x}),\\mathbf{w}^{\\otimes k^{*}-1}\\otimes(\\mathbf{w}^{*})^{\\perp\\mathbf{w}}\\rangle)^{2}]\\lesssim\\sin^{4}{\\theta}e^{k^{*}}\\log^{k^{*}}(B_{4}/\\epsilon).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Therefore, it holds that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathbf{Pr}[|(\\widehat{\\mathbf{g}}(\\mathbf{w})-\\mathbf{g}(\\mathbf{w}))\\cdot\\mathbf{w}^{*}|\\ge t]\\lesssim\\frac{(k^{*}c_{k^{*}})^{2}e^{k^{*}}\\log^{k^{*}}(B_{4}/\\epsilon)\\sin^{4}(\\theta)}{n t^{2}},\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "which implies that with probability at least $1-\\delta$ it holds ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(\\widehat{\\mathbf{g}}(\\mathbf{w})-\\mathbf{g}(\\mathbf{w}))\\cdot\\mathbf{w}^{*}\\lesssim\\sqrt{\\frac{(k^{*}c_{k^{*}})^{2}e^{k^{*}}\\log^{k^{*}}(B_{4}/\\epsilon)\\sin^{4}(\\theta)}{n\\delta}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "We proceed to the main theorem of this paper. It shows that using at most $\\tilde{\\Theta}(d^{\\lceil k/2\\rceil}+d/\\epsilon)$ samples, Algorithm 2 (with initialization subroutineAlgorithm 1) generates a vector $\\widehat{\\bf w}$ such that $\\widehat{\\cal L}_{2}^{\\sigma}(\\widehat{\\bf w})=$ $O(\\mathrm{OPT})+\\epsilon$ within ${\\cal O}(\\log(1/\\epsilon))$ iterations. ", "page_idx": 36}, {"type": "text", "text": "E.3 Proof of Main Theorem ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Theorem E.6. Suppose that Assumption $^{\\,l}$ holds. Choose the batch size of Algorithm $^{4}$ to be $n\\,=\\,\\Theta(C_{k^{*}}d e^{k^{*}}\\log^{k^{*}+1}(B_{4}/\\epsilon)/(\\epsilon\\delta))$ , and choose the step size $\\eta\\,=\\,9/\\bigl(40e{k^{*}}{c_{k^{*}}}\\bigr)$ . Then, after $T\\underline{{\\upsilon}}=O(\\log(C_{k^{*}}/\\epsilon))$ iterations, with probability at least $1-\\delta$ , Algorithm $^{4}$ generates a parameter $\\mathbf{w}^{T}$ that satisfies $\\begin{array}{r}{\\mathcal{L}_{2}^{\\sigma}(\\mathbf{w}^{T})=O(C_{k^{*}}\\dot{\\mathrm{OPT}})+\\epsilon}\\end{array}$ . The total number of samples required for Algorithm 4 is ", "page_idx": 36}, {"type": "equation", "text": "$$\nN=\\Theta\\bigg((k^{*}/c_{k^{*}})^{2}e^{k^{*}}\\log^{k^{*}}(B_{4}/\\epsilon)d^{\\lceil k^{*}/2\\rceil}+\\bigg(e^{k^{*}}\\log^{k^{*}+2}(B_{4}/\\epsilon)C_{k^{*}}\\bigg)\\frac{d}{\\epsilon\\delta}\\bigg).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Proof. Suppose first that $\\mathrm{OPT}\\geq(c_{k^{*}}/(64k^{*}))^{2}$ , i.e., OPT is of constant value. Then by Claim E.7 we know that for any unit vector $\\widehat{\\bf w}$ (e.g., $\\widehat{\\mathbf{w}}=\\mathbf{e}_{1}$ ) it holds ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathcal{L}_{2}^{\\sigma}(\\widehat{\\mathbf{w}})\\leq20\\mathrm{PT}+4\\bigg(\\sum_{k\\geq k^{*}}k c_{k}^{2}(1-(\\mathbf{w}\\cdot\\mathbf{w}^{*}))\\bigg)\\leq2\\mathrm{OPT}+4C_{k^{*}}=O(\\mathrm{OPT}).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Hence $\\widehat{\\bf w}$ is an approximate solution of the agnostic learning problem. ", "page_idx": 36}, {"type": "text", "text": "Now suppose $\\mathrm{OPT}\\le(c_{k^{*}}/(64k^{*}))^{2}$ , then the assumption in Proposition D.2 is satisfied and Algorithm 3 can be applied. Consider the distance between $\\mathbf{w}^{t}$ and $\\mathbf{w}^{*}$ after each update of Algorithm 4. By the non-expansive property of projection operators, we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathbf{w}^{t+1}-\\mathbf{w}^{*}\\|_{2}^{2}=\\|\\mathrm{proj}_{\\mathbb{B}_{d}}(\\mathbf{w}^{t}-\\eta\\widehat{\\mathbf{g}}(\\mathbf{w}^{t}))-\\mathbf{w}^{*}\\|_{2}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\|\\mathbf{w}^{t}-\\eta\\widehat{\\mathbf{g}}(\\mathbf{w}^{t})-\\mathbf{w}^{*}\\|_{2}^{2}}\\\\ &{\\qquad\\qquad=\\|\\mathbf{w}^{t}-\\mathbf{w}^{*}\\|_{2}^{2}+2\\eta\\widehat{\\mathbf{g}}(\\mathbf{w}^{t})\\cdot(\\mathbf{w}^{*}-\\mathbf{w}^{t})+\\eta^{2}\\|\\widehat{\\mathbf{g}}(\\mathbf{w}^{t})\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Let us denote the an\u221agle between $\\mathbf{w}^{t}$ and $\\mathbf{w}^{*}$ by $\\theta_{t}$ . Furthermore, let us assume for now that $\\theta_{t}$ satisfies $\\sin\\theta_{t}\\geq4e{\\sqrt{\\mathrm{OPT}}}+{\\sqrt{\\epsilon}}$ , hence the condition for Lemma E.4 is satisfied. Note by definition $\\widehat{\\mathbf{g}}(\\mathbf{w}^{t})\\perp\\mathbf{w}^{t}$ , hence using Lemma E.4 and Lemma E.5 we have that with probability at least $1-\\delta$ , the inner product term in Equation (47) is bounded above by: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{\\mathbf{g}}(\\mathbf{w}^{t})\\cdot(\\mathbf{w}^{*}-\\mathbf{w}^{t})=\\widehat{\\mathbf{g}}(\\mathbf{w}^{t})\\cdot\\mathbf{w}^{*}=(\\widehat{\\mathbf{g}}(\\mathbf{w}^{t})-\\mathbf{g}(\\mathbf{w}^{t}))\\cdot\\mathbf{w}^{*}+\\mathbf{g}(\\mathbf{w}^{t})\\cdot\\mathbf{w}^{*}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\frac{C_{1}k^{*}c_{k^{*}}e^{k^{*}/2}\\log^{k^{*}/2}(B_{4}/\\epsilon)}{\\sqrt{n\\delta}}\\sin^{2}(\\theta_{t})-\\frac{1}{2}\\|\\mathbf{g}^{*}(\\mathbf{w}^{*})\\|_{2}\\sin\\theta_{t},}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where $C_{1}$ is a sufficiently large absolute constant. On the other hand, the squared norm term $\\|\\widehat{\\mathbf{g}}(\\mathbf{w}^{t})\\|_{2}^{2}$ from Equation (47) can be bounded above using Lemma E.5 and Corollary E.3: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\widehat{\\mathbf{g}}(\\mathbf{w}^{t})\\|_{2}^{2}=\\|(\\widehat{\\mathbf{g}}(\\mathbf{w}^{t})-\\mathbf{g}(\\mathbf{w}^{t}))+\\mathbf{g}(\\mathbf{w}^{t})\\|_{2}^{2}}\\\\ &{\\qquad\\qquad\\leq2\\|\\widehat{\\mathbf{g}}(\\mathbf{w}^{t})-\\mathbf{g}(\\mathbf{w}^{t})\\|_{2}^{2}+2\\|\\mathbf{g}(\\mathbf{w}^{t})\\|_{2}^{2}}\\\\ &{\\qquad\\qquad\\leq\\frac{C_{2}d(k^{*}c_{k^{*}})^{2}e^{k^{*}}\\log^{k^{*}}(B_{4}/\\epsilon)}{n\\delta}+(k^{*}c_{k^{*}})^{2}0\\mathrm{PT}+\\|\\mathbf{g}^{*}(\\mathbf{w})\\|_{2}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "for a sufficiently large absolute constant $C_{2}$ . Plugging Equation (48) and Equation (49) back into Equation (47), and denoting $\\kappa:=\\operatorname*{max}\\{C_{1},C_{2}\\}k^{*}c_{k^{*}}e^{k^{*}/2}\\log^{k^{*}/2}(B_{4}/\\epsilon)$ , we get that with probability at least $1-\\delta$ , ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathbf{w}^{t+1}-\\mathbf{w}^{*}\\|_{2}^{2}\\leq\\|\\mathbf{w}^{t}-\\mathbf{w}^{*}\\|_{2}^{2}+\\frac{\\eta\\kappa}{\\sqrt{n\\delta}}\\sin^{2}\\theta_{t}-\\frac{\\eta}{2}\\|\\mathbf{g}^{*}(\\mathbf{w}^{t})\\|_{2}\\sin\\theta_{t}}\\\\ &{\\qquad\\qquad\\qquad+\\eta^{2}\\bigg(\\frac{d\\kappa^{2}}{n\\delta}+(k^{*}c_{k^{*}})^{2}\\mathrm{OPT}+\\|\\mathbf{g}^{*}(\\mathbf{w}^{t})\\|_{2}^{2}\\bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Let us assume first that $\\theta_{t}\\,\\leq\\,\\theta_{t-1}\\,\\leq\\,\\cdot\\,\\cdot\\,\\leq\\,\\theta_{0}$ and $\\theta_{t}$ satisfies $\\sin\\theta_{t}\\geq4e{\\sqrt{\\mathrm{OPT}}}+{\\sqrt{\\epsilon}}$ . We will argue that in this case $\\theta_{t+1}\\leq\\theta_{t}$ (in fact, that it contracts by a constant factor). Then, by an inductive argument, we imme\u221adiately know that the assumption is valid and that $\\theta_{t}$ is a decreasing sequence (as long as $\\sin\\theta_{t}\\geq4e\\sqrt{\\mathrm{OPT}}+\\epsilon)$ . To prove $\\theta_{t+1}\\leq\\theta_{t}$ , recall that in Claim E.1 it was shown that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\|\\mathbf{g}^{*}(\\mathbf{w}^{t})\\|_{2}=2k^{*}c_{k^{*}}(\\mathbf{w}^{t}\\cdot\\mathbf{w}^{*})\\|(\\mathbf{w}^{*})^{\\perp_{\\mathbf{w}^{t}}}\\|_{2}=2k^{*}c_{k^{*}}(\\mathbf{w}^{t}\\cdot\\mathbf{w}^{*})^{k^{*}-1}\\sin\\theta_{t}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Recall that $\\mathbf{w}^{0}$ is the initial parameter vector that satisfies $\\mathbf{w}^{0}\\cdot\\mathbf{w}^{*}\\geq1-1/k^{*}$ . By the inductive hypothesis it holds $\\theta_{t}\\leq\\,\\theta_{0}$ , hence $\\mathbf{w}^{t}\\cdot\\mathbf{w}^{*}\\ge\\mathbf{w}^{0}\\cdot\\mathbf{w}^{*}\\ge1-1/k^{*}$ . Furthermore, as we have $(1-1/t)^{t-1}\\geq1/e$ for $t\\geq1$ , it holds $1\\geq(\\mathbf{w}^{t}\\cdot\\mathbf{w}^{*})^{k^{*}-1}\\geq1/e$ . Therefore, we further obtain ", "page_idx": 37}, {"type": "equation", "text": "$$\n(2k^{*}c_{k^{*}}/e)\\sin\\theta_{t}\\le\\|\\mathbf{g}^{*}(\\mathbf{w}^{t})\\|_{2}\\le2k^{*}c_{k^{*}}\\sin\\theta_{t}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Now choosing $n\\gtrsim d\\kappa/((k^{*}c_{k^{*}})^{2}\\epsilon\\delta)$ , and recalling that we have assumed $\\sin\\theta_{t}\\geq4e{\\sqrt{\\mathrm{OPT}}}+{\\sqrt{\\epsilon}}$ by the induction hypothesis, we can further bound $\\lVert\\mathbf{w}^{t+1}-\\mathbf{w}^{*}\\rVert_{2}^{2}$ above as: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathbf{w}^{t+1}-\\mathbf{w}^{*}\\|_{2}^{2}\\leq\\|\\mathbf{w}^{t}-\\mathbf{w}^{*}\\|_{2}^{2}+\\eta((\\epsilon/d)^{1/2}-(4k^{*}c_{k^{*}}/e))\\sin^{2}\\theta_{t}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\,\\eta^{2}(k^{*}c_{k^{*}})^{2}(\\epsilon+\\mathrm{OPT}+4\\sin^{2}\\theta_{t})}\\\\ &{\\qquad\\qquad\\qquad\\leq\\|\\mathbf{w}^{t}-\\mathbf{w}^{*}\\|_{2}^{2}-(3k^{*}c_{k^{*}}/e)\\eta\\sin^{2}\\theta_{t}+5\\eta^{2}(k^{*}c_{k^{*}})^{2}\\sin^{2}\\theta_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Observe that since $\\theta_{t}\\ \\leq\\ \\theta_{0}$ \u221a and by assumption $\\mathbf{w}^{0}\\cdot\\mathbf{w}^{*}\\;=\\;\\cos\\theta_{0}\\;\\geq\\;1\\,-\\,\\operatorname*{min}\\{1/k^{*},1/2\\}\\;\\geq$ $1/2$ , we have $\\cos(\\theta_{t}/2)\\,\\geq\\,\\sqrt{3}/2$ and thus it further holds that $(\\sqrt{3}/2)(2\\sin(\\theta_{t}/2))\\;\\leq\\;\\sin\\theta_{t}\\;\\leq\\;$ $2\\sin(\\theta_{t}/2)$ . Since $\\|\\mathbf{w}^{t}-\\mathbf{w}^{*}\\|_{2}=2\\sin(\\theta_{t}/2)$ follows from $\\mathbf{w}^{t}$ , $\\mathbf{w}^{\\ast}\\in\\mathbb{S}^{d-1}$ , we finally obtain that, with probability at least $1-\\delta$ , ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\|\\mathbf{w}^{t+1}-\\mathbf{w}^{*}\\|_{2}^{2}\\leq(1-(9k^{*}c_{k^{*}}/(4e))\\eta+5(k^{*}c_{k^{*}})^{2}\\eta^{2})\\|\\mathbf{w}^{t}-\\mathbf{w}^{*}\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Choosing $\\eta=9/(40e k^{*}c_{k^{*}})$ yields (with probability at least $1-\\delta)$ : ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{4\\sin^{2}(\\theta_{t+1}/2)=\\|\\mathbf{w}^{t+1}-\\mathbf{w}^{*}\\|_{2}^{2}}\\\\ &{\\phantom{=}\\leq(1-(81/(320e^{2})))\\|\\mathbf{w}^{t+1}-\\mathbf{w}^{*}\\|_{2}^{2}}\\\\ &{\\phantom{=}=(1-(81/(320e^{2})))(4\\sin^{2}(\\theta_{t}/2)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "This shows that $\\theta_{t+1}\\leq\\theta_{t}$ , hence completing the inductive argument. Furthermore, E\u221aquation (5\u221a1) implies that after at most $T=O(\\log(1/\\epsilon))$ iterations it must hold that $\\sin\\theta_{T}\\leq4e\\sqrt{\\mathrm{OPT}}+\\sqrt{\\epsilon}$ , therefore, we can end the algorithm after at \u221amost ${\\cal O}(\\log(1/\\epsilon))$ iterations. Though the contraction Equation (51) only holds when $\\sin\\theta_{T}\\geq4e{\\sqrt{\\mathrm{OPT}}}+{\\sqrt{\\epsilon}}$ , we can further \u221ashow that if after some iteration $t^{*}$ we have $\\sin\\theta_{t^{*}}\\leq4e\\sqrt{\\mathrm{OPT}}+\\sqrt{\\epsilon}$ , then $\\sin\\theta_{t^{*}+1}$ is still of order $\\scriptstyle{\\sqrt{\\mathrm{OPT}}}+{\\sqrt{\\epsilon}}$ . Concretely, if there exists some step $t^{*}\\leq T$ such that $\\sin(\\theta_{t^{*}})\\le4e\\sqrt{\\mathrm{OPT}}+\\sqrt{\\epsilon}$ , then at step $t^{*}+1$ it must hold (by Equation (50)): ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\sin(\\theta_{t^{*}+1})\\leq\\sqrt{1+8\\eta^{2}(k^{*}c_{k^{*}})^{2}}\\sin(\\theta_{t^{*}})\\leq3\\sin\\theta_{t^{*}}\\leq3(4e\\sqrt{\\mathrm{OPT}}+\\sqrt{\\epsilon}).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "In other words, for all steps $t^{*}\\leq t\\leq T$ , it holds that $\\sin\\theta_{t}\\le30(\\sqrt{\\mathrm{OPT}}+\\sqrt{\\epsilon})$ . Thus\u221a, in summa\u221ary, choosing $T=O(\\log(1/\\epsilon))$ , we get that with probability at least $1-\\delta T$ , $\\sin\\theta_{T}\\lesssim\\sqrt{\\mathrm{OPT}}+\\sqrt{\\epsilon}$ , and applying Claim E.7 we get: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathcal{L}_{2}^{\\sigma}(\\mathbf{w}^{T})=O\\bigg(\\bigg(\\sum_{k\\ge k^{*}}k c_{k}^{2}\\bigg)(\\mathrm{OPT}+\\epsilon)\\bigg)=O(C_{k^{*}}\\mathrm{OPT})+\\epsilon^{\\prime},\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where we set $\\begin{array}{r}{\\epsilon^{\\prime}=\\epsilon/C_{k^{*}}\\leq\\epsilon/(\\sum_{k\\geq k^{*}}k c_{k}^{2})}\\end{array}$ , and used Assumption $1(i i i)$ that $\\begin{array}{r}{\\sum_{k\\geq k^{*}}k c_{k}^{2}\\leq C_{k^{*}}}\\end{array}$ . ", "page_idx": 38}, {"type": "text", "text": "Thus, choosing $\\delta^{\\prime}=\\delta T$ , where $T=O(\\log(C_{k^{*}}/\\epsilon^{\\prime}))$ , Algorithm 4 outputs a parameter $\\mathbf{w}^{T}$ such that with probability at least $1-\\delta^{\\prime}$ , $\\mathcal{L}_{2}^{\\sigma}(\\dot{\\mathbf{w}}^{T})=O(C_{k^{*}}\\mathrm{OPT})+\\epsilon^{\\prime}$ , with batch size ", "page_idx": 38}, {"type": "equation", "text": "$$\nn=\\Theta\\bigg(\\frac{d C_{k^{*}}e^{k^{*}}\\log^{k^{*}+1}(B_{4}/\\epsilon^{\\prime})}{\\epsilon^{\\prime}\\delta^{\\prime}}\\bigg).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "In summary, the total number of samples required for Algorithm 4 is ", "page_idx": 38}, {"type": "equation", "text": "$$\nN=\\Theta\\bigg(\\frac{(k^{*})^{2}e^{k^{*}}\\log^{k^{*}}(B_{4}/\\epsilon^{\\prime})d^{\\lceil k^{*}/2\\rceil}}{c_{k^{*}}^{2}}+\\frac{C_{k^{*}}d e^{k^{*}}\\log^{k^{*}+2}(B_{4}/\\epsilon^{\\prime})}{\\epsilon^{\\prime}\\delta^{\\prime}}\\bigg).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "The final claim shows that if $\\sin(\\theta(\\mathbf{w},\\mathbf{w}^{*}))\\lesssim\\sqrt{\\mathrm{OPT}}+\\sqrt{\\epsilon}$ , then $\\mathcal{L}_{2}^{\\sigma}(\\mathbf{w})\\lesssim C_{k^{*}}(\\mathrm{OPT}+\\epsilon)$ . ", "page_idx": 38}, {"type": "text", "text": "Cla\u221aim E.7. Let w $\\in\\ \\mathbb{S}^{d}$ and denote the angle between w and $\\mathbf{w}^{*}$ by $\\theta$ . If $\\theta$ satisfies $\\sin\\theta\\,\\leq$ $C({\\sqrt{\\mathrm{OPT}}}+{\\sqrt{\\epsilon}})$ for some absolute constant $C$ , then we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathcal{L}_{2}^{\\sigma}(\\mathbf{w})\\lesssim\\bigg(\\sum_{k\\geq k^{*}}k c_{k}^{2}\\bigg)(\\mathrm{OPT}+\\epsilon).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Proof. Since $\\mathbf{w}\\cdot\\mathbf{w}^{*}=\\cos\\theta\\geq1-\\sin^{2}\\theta.$ , according to Claim C.4 the $L_{2}^{2}$ loss $\\mathcal{L}_{2}^{\\sigma}(\\mathbf{w})$ can be upper bounded by ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{C_{2}^{\\prime\\prime}(\\mathbf w)\\leq20\\mathrm{PT}+4\\bigg(1-\\displaystyle\\sum_{k\\geq k^{\\prime}}c_{k}^{2}(\\mathbf w\\cdot\\mathbf w^{*})^{k}\\bigg)}\\\\ &{\\qquad=20\\mathrm{PT}+4\\bigg(\\displaystyle\\sum_{k\\geq k^{\\prime}}c_{k}^{2}(1-(\\mathbf w\\cdot\\mathbf w^{*})^{k})\\bigg)}\\\\ &{\\qquad=20\\mathrm{PT}+4\\bigg(\\displaystyle\\sum_{k\\geq k^{\\prime}}c_{k}^{2}(1-(\\mathbf w\\cdot\\mathbf w^{*}))(1+(\\mathbf w\\cdot\\mathbf w^{*})+\\cdots+(\\mathbf w\\cdot\\mathbf w^{*})^{k-1})\\bigg)}\\\\ &{\\qquad\\leq20\\mathrm{PT}+4\\bigg(\\displaystyle\\sum_{k\\geq k^{\\prime}}k c_{k}^{2}(1-(\\mathbf w\\cdot\\mathbf w^{*}))\\bigg)}\\\\ &{\\qquad\\leq20\\mathrm{PT}+4\\displaystyle\\sum_{k\\geq k^{\\prime}}k c_{k}^{2}\\sin^{2}\\theta}\\\\ &{\\qquad\\leq20\\mathrm{PT}+4\\bigg(\\displaystyle\\sum_{k\\geq k^{\\prime}}k c_{k}^{2}\\bigg)C^{2}(0\\mathrm{PT}+\\epsilon)\\lesssim\\bigg(\\displaystyle\\sum_{k\\geq k^{\\prime}}k c_{k}^{2}\\bigg)(0\\mathrm{PT}+\\epsilon).}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Justification: The abstract summarizes the main contribution of our paper provided in Theorem 3.5, that is, we provide the first algorithm for agnostically learning SIMs under a broad class of activations. In the introduction, we summarize the motivation of our work, provide a detailed discussion of our results, and compare our work with prior literature. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 40}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Justification: The limitations of our work are discussed in the introduction, and are clearly stated in the statements of the theorems. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 40}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Justification: Our assumptions on the activations are summarized in Assumption 1. Complete proofs are provided in the main body and the appendix (Appendix D and Appendix E). ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 41}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: The paper is theoretical in nature and does not include experiments. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 41}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: The paper is theoretical in nature and does not include experiments. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 42}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: The paper is theoretical in nature and does not include experiments. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 42}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: The paper is theoretical in nature and does not include experiments. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 42}, {"type": "text", "text": "", "page_idx": 43}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 43}, {"type": "text", "text": "Justification: The paper is theoretical in nature and does not include experiments. Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 43}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: The research conducted in the paper conforms with the NeurIPS Code of Ethics. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 43}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 43}, {"type": "text", "text": "Justification: The paper is theoretical in nature, and we do not see any major or immediate implications for society. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 43}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 44}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 44}, {"type": "text", "text": "Justification: The paper is theoretical and does not use data or models that have a high risk for misuse. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 44}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 44}, {"type": "text", "text": "Justification: This work does not use any assets. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 44}, {"type": "text", "text": "", "page_idx": 45}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 45}, {"type": "text", "text": "Justification: This work does not introduce any new assets. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 45}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 45}, {"type": "text", "text": "Justification: This work does not involve any crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 45}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 45}, {"type": "text", "text": "Justification: This work does not involve research with human subjects. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 45}, {"type": "text", "text": "", "page_idx": 46}]