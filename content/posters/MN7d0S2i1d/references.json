{"references": [{"fullname_first_author": "G. Ben Arous", "paper_title": "Algorithmic thresholds for tensor PCA", "publication_date": "2020-00-00", "reason": "This paper established algorithmic thresholds for tensor PCA, which is a fundamental problem in high-dimensional data analysis and directly relevant to the initialization step of the proposed algorithm."}, {"fullname_first_author": "G. Ben Arous", "paper_title": "Online stochastic gradient descent on non-convex losses from high-dimensional inference", "publication_date": "2021-00-00", "reason": "This paper extends the results of tensor PCA to online settings, which is crucial for the analysis of the gradient-based optimization steps of the proposed algorithm."}, {"fullname_first_author": "I. Diakonikolas", "paper_title": "Approximation schemes for ReLU regression", "publication_date": "2020-00-00", "reason": "This paper studies the problem of robustly learning ReLU regression, a closely related problem to the single-index model (SIM) learning considered in the current paper, and provides efficient agnostic learners."}, {"fullname_first_author": "I. Diakonikolas", "paper_title": "Agnostic learning of general ReLU activation using gradient descent", "publication_date": "2023-00-00", "reason": "This paper studies the problem of agnostically learning general ReLU activation functions, providing efficient algorithms and near-optimal error guarantees, and showing that gradient descent can succeed under adversarial label noise."}, {"fullname_first_author": "A. Damian", "paper_title": "Smoothing the landscape boosts the signal for SGD: Optimal sample complexity for learning single index models", "publication_date": "2023-00-00", "reason": "This paper studies the efficient learnability of SIMs under the Gaussian distribution, giving sample-efficient gradient methods and matching lower bounds; the current paper builds upon their result to handle the agnostic noise model."}]}