[{"figure_path": "6qJKrOulTr/figures/figures_5_1.jpg", "caption": "Figure 1: Example output from Algorithm 3 applied to digit classification. These two instances are predicted as 3 (left) and 4 (right). The brightness of the color indicates the eigenvalue's magnitude. The brighter the color, the more sensitive the patch. This indicates that changes in the values of these sensitive patches are likely to have a greater impact on the prediction probabilities. Each patch in the heatmap corresponds to a 2 \u00d7 2 square pixel.", "description": "This figure shows the results of applying Algorithm 3, a feature importance analysis method, to two handwritten digit images (a 3 and a 4 from the MNIST dataset).  Each image is displayed alongside a heatmap representing the eigenvalues obtained from the pullback metric. Brighter colors in the heatmap indicate larger eigenvalues, signifying higher sensitivity of the corresponding image patch to changes in the prediction.  The heatmaps highlight the areas that have the greatest influence on the model's classification decision.", "section": "3.2 Interpretability"}, {"figure_path": "6qJKrOulTr/figures/figures_5_2.jpg", "caption": "Figure 2: Example outputs from Algorithm 3. The darker the color, the higher the token's eigenvalue. Left: The sentence analysed is classified as \u201coffensive\u201d by the BERT for hate speech detection, with significant contributions from tokens [CLS], politicians, corrupt, and ##eit (part of the word deceitful). Right: Example instance processed by a BERT model for masked language modeling. [MASK] is predicted as \u201cham\u201d, with the most influential tokens being pizza and cheese.", "description": "This figure shows two examples of Algorithm 3's output which visualizes feature importance based on eigenvalues from the pullback metric. The left example uses a BERT model trained for hate speech detection, highlighting tokens like \"politicians\", \"corrupt\", and parts of \"deceitful\" as significant contributors to the classification. The right example uses a BERT model for masked language modeling, showing that the tokens \"pizza\" and \"cheese\" strongly influence predicting the masked token as \"ham\". The darker color indicates a higher eigenvalue and thus more importance.", "section": "3.2 Interpretability"}, {"figure_path": "6qJKrOulTr/figures/figures_6_1.jpg", "caption": "Figure 3: Example of SiMEC and SiMExp output interpretation for ViT digit classification. Left: Original MNIST image of an \u201c8\u201d. Center: Interpretation of a p1000 from a SiMEC experiment, where p1000 is predicted as \u201c8\u201d. Right: Interpretation of a p1000 from a SiMExp experiment, where p1000 is predicted as \u201c4\u201d. All patches are subject to SiMEC and SiMExp updates.", "description": "This figure shows the results of applying the SiMEC and SiMExp algorithms to a single MNIST digit image.  The original image is an 8.  The center image shows the result of applying the SiMEC algorithm for 1000 iterations, resulting in an image that is still classified as an 8, indicating it remains within the same equivalence class.  The right-hand image shows the result of applying the SiMExp algorithm for 1000 iterations; the resulting image is classified as a 4, showing a shift to a different equivalence class.", "section": "3.2 Interpretability"}, {"figure_path": "6qJKrOulTr/figures/figures_7_1.jpg", "caption": "Figure 4: Analysis involving results SiMEC and SiMExp applied to BERT for hate speech detection. Left: Prediction values for i* for each y \u2208 Yc. Right: Prediction values for y \u2208 Ys.", "description": "This figure displays the results of applying SiMEC and SiMExp algorithms to a BERT model for hate speech detection.  The left panel shows the probability of the original prediction (i*) remaining consistent across iterations for each alternative prompt generated (y \u2208 Yc). The right panel shows the probability of the original prediction changing (y \u2208 Ys). The shaded areas represent the standard deviation.  The results illustrate how the algorithms affect prediction probabilities over iterations, comparing their tendency to preserve or change the initial prediction.", "section": "4 Experiments"}]