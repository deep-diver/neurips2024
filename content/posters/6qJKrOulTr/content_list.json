[{"type": "text", "text": "Unveiling Transformer Perception by Exploring Input Manifolds ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 This paper introduces a general method for the exploration of equivalence classes in   \n2 the input space of Transformer models. The proposed approach is based on sound   \n3 mathematical theory which describes the internal layers of a Transformer architec  \n4 ture as sequential deformations of the input manifold. Using eigendecomposition   \n5 of the pullback of the distance metric defined on the output space through the   \n6 Jacobian of the model, we are able to reconstruct equivalence classes in the input   \n7 space and navigate across them. We illustrate how this method can be used as a   \n8 powerful tool for investigating how a Transformer sees the input space, facilitating   \n9 local and task-agnostic explainability in Computer Vision and Natural Language   \n10 Processing tasks. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "12 In this paper, we propose a method for exploring the input space of Transformer models by identifying   \n13 equivalence classes with respect to their predictions. We define an equivalence class of a Transformer   \n14 model as the set of vectors in the embedding space whose outcomes under the Transformer process   \n15 are the same. The study of the input manifold on which the inverse image of models lies provides   \n16 insights for both explainability and sensitivity analyses. Existing methods aiming at the exploration   \n17 of the input space of Deep Neural Networks and Transformers either rely on perturbations of input   \n18 data using heuristic or gradient-based criteria [16, 22, 17, 14], or they analyze specific properties of   \n19 the embedding space [5].   \n20 Our approach is based on sound mathematical theory which describes the internal layers of a   \n21 Transformer architecture as sequential deformations of the input manifold. Using eigendecomposition   \n22 of the pullback of the distance metric defined on the output space through the Jacobian of the model,   \n23 we are able to reconstruct equivalence classes in the input space and navigate across them. In the   \n24 XAI scenario, our framework can facilitate local and task-agnostic explainability methods applicable   \n25 to Computer Vision (CV) and Natural Language Processing (NLP) tasks, among others.   \n26 In Section 2, we summarise the preliminaries of the mathematical foundations of our approach.   \n27 In Section 3, we present our method for the exploration of equivalence classes in the input of the   \n28 Transformer models. In Section 4, we perform a preliminary investigation of some applicability   \n29 options of our method on textual and visual data. In Section 5, we discuss the relevant literature about   \n30 embedding space exploration and feature importance. Finally, in Section 6, we give our concluding   \n31 remarks1. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "32 2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "33 In this Section, we provide the theoretical foundation of the proposed approach, namely the Geometric   \n34 Deep Learning framework based on Riemannian Geometry [2].   \n35 A neural network is considered as a sequence of maps, the layers of the network, between manifolds,   \n36 and the latter are the spaces where the input and the outputs of the layers belong to.   \n37 Definition 1 (Neural Network). A neural network is a sequence of $\\mathcal{C}^{1}$ maps $\\Lambda_{i}$ between manifolds of   \n38 the form: ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "equation", "text": "$$\nM_{0}\\ {\\xrightarrow{\\Lambda_{1}}}\\ M_{1}\\ {\\xrightarrow{\\Lambda_{2}}}\\ M_{2}\\ {\\xrightarrow{\\Lambda_{4}}}\\ \\cdots\\ {\\xrightarrow{\\Lambda_{n-1}}}\\ M_{n-1}\\ {\\xrightarrow{\\Lambda_{n}}}\\ M_{n}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "39 We call $M_{0}$ the input manifold and $M_{n}$ the output manifold. All the other manifolds of the sequence   \n40 are called representation manifolds. The maps $\\Lambda_{i}$ are the layers of the neural network. We denote   \n41 with ${\\mathcal{N}}_{(i)}=\\Lambda_{n}\\circ\\cdot\\cdot\\cdot\\circ\\Lambda_{i}:M_{i}\\to M_{n}$ the mapping from the $i$ -th representation layer to the output   \n42 layer.   \n43 As an example, consider a shallow network with just one layer, the composition of a linear operator   \n44 $A\\cdot+b$ with a sigmoid function $\\sigma$ , where $A\\in\\mathbb{R}^{m\\times n}$ and $b\\in\\mathbb{R}^{m}$ : then, the input manifold $M_{0}$ and   \n45 the output manifold $M_{1}$ shall be $\\mathbb{R}^{n}$ and $\\mathbb{R}^{m}$ , respectively, and the map $\\Lambda_{1}{\\hat{(}}\\cdot)\\stackrel{}{=}\\sigma(A\\cdot+b)$ . We   \n46 generalize this observation into the following definition.   \n47 Definition 2 (Smooth layer). A map $\\Lambda_{i}:M_{i-1}\\to M_{i}$ is called a smooth layer if it is the restriction   \n48 to $M_{i-1}$ of a function $\\overline{{\\Lambda}}^{(i)}(x):\\mathbb{R}^{d_{i-1}}\\rightarrow\\mathbb{R}^{d_{i}}$ of the form ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "equation", "text": "$$\n\\overline{{{\\Lambda}}}_{\\alpha}^{(i)}(x)=F_{\\alpha}^{(i)}\\left(\\sum_{\\beta}A_{\\alpha\\beta}^{(i)}x_{\\beta}+b_{\\alpha}^{(i)}\\right)\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "49 for $i=1,\\cdots,n,\\,x\\in\\mathbb{R}^{d_{i}},\\,b^{(i)}\\in\\mathbb{R}^{d}.$ and $A^{(i)}\\in\\mathbb{R}^{d_{i}\\times d_{i-1}}$ , with $F^{(i)}:\\mathbb{R}^{d_{i}}\\,\\rightarrow\\,\\mathbb{R}^{d_{i}}$ a diffeomor  \n50 phism.   \n51 Remark 1. Transformers implicitly apply for this framework, since their modules are smooth   \n52 functions, such as fully connected layers, GeLU and sigmoid activations.   \n53 Our aim is to transport the geometric information on the data lying in the output manifold to the   \n54 input manifold: this allows us to obtain insight on how the network \"sees\" the input space, how it   \n55 manipulates it for reaching its final conclusion. For fulfilling this objective, we need several tools   \n56 from differential geometry. The first key ingredient is the notion of singular Riemannian metric,   \n57 which has the intuitive meaning of a degenerate scalar product which changes point to point.   \n58 Definition 3 (Singular Riemannian metric). Let $M=\\mathbb{R}^{n}$ or an open subset of $\\mathbb{R}^{n}$ . A singular   \n59 Riemannian metric g over $M$ is a map $g:M\\to B i l(\\mathbb{R}^{n}\\times\\mathbb{R}^{n})$ that associates to each point p a   \n60 positive semidefinite symmetric bilinear form $g_{p}:\\mathbb{R}^{n}\\times\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ in a smooth way.   \n61 Without loss of generality, we can assume the following hypotheses on the sequence (1): $i)$ The   \n62 manifolds $M_{i}$ are open and path-connected sets of dimension $\\dim M_{i}=d_{i}$ . ii) The maps $\\Lambda_{i}$ are $\\mathcal{C}^{1}$   \n63 submersions. iii) $\\bar{\\Lambda_{i}}(M_{i-1})\\bar{=}M_{i}$ for every $i=1,\\cdot\\cdot\\cdot,n.\\ i\\nu)$ The manifold $M_{n}$ is equipped with   \n64 the structure of Riemannian manifold, with metric $g^{(n)}$ . Definition 3 naturally leads to the definition   \n65 of the pseudolength and of energy of a curve.   \n66 Definition 4 (Pseudolength and energy of a curve). Let $\\gamma:[a,b]\\,\\rightarrow\\,\\mathbb{R}^{n}$ a curve defined on the   \n67 interval $[a,b]\\subset\\mathbb{R}$ and $\\|v\\|_{p}\\,=\\,\\sqrt{g_{p}(v,v)}$ the pseudo\u2013norm induced by the pseudo\u2013metric $g_{p}$ at   \n68 point $p$ . Then the pseudolength of $\\gamma$ and its energy are defined as ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "equation", "text": "$$\nP l(\\gamma)=\\int_{a}^{b}\\|\\dot{\\gamma}(s)\\|_{\\gamma(s)}d s=\\int_{a}^{b}\\sqrt{g_{\\gamma(s)}(\\dot{\\gamma}(s),\\dot{\\gamma}(s))}d s,\\qquad E(\\gamma)=\\int_{a}^{b}\\|\\dot{\\gamma}(s)\\|_{\\gamma(s)}^{2}d s\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "69 The notion of pseudolength leads naturally to define the distance between two points. ", "page_idx": 1}, {"type": "text", "text": "70 Definition 5 (Pseudodistance). Let $x,y\\in M=\\mathbb{R}^{n}$ . The pseudodistance between x and $y$ is then ", "page_idx": 1}, {"type": "equation", "text": "$$\nP d(x,y)=\\operatorname*{inf}\\{P l(\\gamma)\\mid\\gamma:[0,1]\\rightarrow M,\\,\\gamma\\in\\mathcal{C}^{1}([0,1]),\\,\\gamma(0)=x,\\,\\gamma(1)=y\\}.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "71 One can observe that endowing the space $\\mathbb{R}^{n}$ with a singular Riemannian metric leads to have   \n72 non trivial curves whose length is zero. A straightforward consequence is that there are distinct   \n73 points whose pseudodistance is therefore zero: a natural equivalence relation arises, i.e. $x\\sim y\\Leftrightarrow$   \n74 $P d(x,y)=0$ , obtaining thus a metric space $(\\mathbb{R}^{n}/\\sim,P d)$ .   \n75 The second crucial tool is the notion of pullback of a function. Let $f$ be a function from $\\mathbb{R}^{p}$ to $\\mathbb{R}^{q}$ ,   \n76 and fix the coordinate systems $\\boldsymbol{x}=(x_{1},\\ldots,x_{p})$ and $\\boldsymbol{y}=(y_{1},\\dots,y_{q})$ on $\\mathbb{R}^{p}$ and on $\\mathbb{R}^{q}$ , respectively.   \n77 Moreover, we endow $\\mathbb{R}^{q}$ with the standard Euclidean metric $g$ , whose associated matrix is the identity.   \n78 The space $\\mathbb{R}^{p}$ can be equipped with the pullback metric $f^{\\ast}g$ whose representation matrix reads as ", "page_idx": 2}, {"type": "equation", "text": "$$\n(f^{*}g)_{i j}=\\sum_{h,k=1}^{q}\\left({\\frac{\\partial f_{h}}{\\partial x_{i}}}\\right)g_{h k}\\left({\\frac{\\partial f_{k}}{\\partial x_{j}}}\\right).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "79 The sequence (1) shows that a neural network can be considered simply as a function, a composition   \n80 of maps: hence, taking $f=\\Lambda_{n}\\circ\\Lambda_{n-1}\\circ\\cdot\\cdot\\cdot\\circ\\Lambda_{1}$ and supposing that $M_{0}=\\mathbb{R}^{p},M_{n}=\\mathbb{R}^{q}$ , the   \n81 generalization of (5) applied to (1) provides with the pullback of a generic neural network.   \n82 Hereafter, we consider in (1) the case $M_{n}=\\mathbb{R}^{q}$ , equipped with the trivial metric $g^{(n)}=I_{q}$ , i.e.,   \n83 the identity. Each manifold $M_{i}$ of the sequence (1) is equipped with a Riemannian singular metric,   \n84 denoted with $g^{(i)}$ , obtained via the pullback of $\\mathcal{N}_{(i)}$ . The pseudolength of a curve $\\gamma$ on the $i$ -th   \n85 manifold, namely $P l_{i}(\\gamma)$ , is computed via the relative metric $g^{(i)}$ via (3). ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "86 2.1 General results ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "87 We depict hereafter the theoretical bases of our approach. We denote with ${\\mathcal{N}}_{i}$ the submap $\\Lambda_{i}\\circ\\cdots\\circ\\Lambda_{n}:$   \n88 $M_{i}\\to M_{n}$ , and with ${\\mathcal N}\\equiv{\\mathcal N}_{0}$ the map describing the action of the complete network. The starting   \n89 point is to consider the pair $(M_{i},P d_{i})$ : this is a pseudometric space, which can be turned into a   \n90 full-fledged metric space $M_{i}/\\sim_{i}$ by the metric identification $x\\stackrel{\\bar{}}{\\sim}_{i}y\\Leftrightarrow P d_{i}(x,y)=0$ . The first   \n91 result states that the length of a curve on the $i$ -th manifold is preserved among the mapping on the   \n92 subsequent manifolds.   \n93 Proposition 1. Let $\\gamma:[0,1]\\rightarrow M_{i}$ be a piecewise $\\mathcal{C}^{1}$ curve. Let $k\\in\\{i,i+1,\\cdots,n\\}$ and consider   \n94 the curve $\\gamma_{k}=\\Lambda_{k}\\circ\\cdot\\cdot\\cdot\\circ\\Lambda_{i}\\circ\\gamma$ on $M_{k}$ . Then $P l_{i}(\\gamma)=P l_{k}(\\gamma_{k})$ .   \n95 In particular this is true when $k=n$ , i.e., the length of a curve is preserved in the last manifold. This   \n96 result leads naturally to claim that if two points are in the same class of equivalence, then they are   \n97 mapped into the same point under the action of the neural network. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "98 Proposition 2. If two points $p,q\\in M_{i}$ are in the same class of equivalence, then $\\mathcal{N}_{i}(p)=\\mathcal{N}_{i}(q)$ . ", "page_idx": 2}, {"type": "text", "text": "99 The next step is to prove that the sets $M_{i}/\\sim_{i}$ are actually smooth manifolds: to this aim, we introduce   \n100 another equivalence relation: $x\\sim_{N_{i}}y$ if and only if there exists a piecewise $\\gamma:[0,1]\\rightarrow M_{i}$ such   \n101 that $\\gamma(0)=x,\\gamma(1)=y$ and $\\mathcal{N}_{i}\\circ\\dot{\\gamma_{}}(s)=\\mathcal{N}_{i}(x)\\;\\forall s\\in[0,1]$ . The introduction of this equivalence   \n102 relation allows us to easily state the following proposition. ", "page_idx": 2}, {"type": "text", "text": "103 Proposition 3. Let $x,y\\in M_{i},$ , then $x\\sim_{i}$ y if and only if $x\\sim_{N_{i}}y$ . ", "page_idx": 2}, {"type": "text", "text": "104 The following corollary contains the natural consequences of the previous result; the second point of   \n105 the claim below is the counterpart of Proposition 2.   \n106 Corollary 1. Under the hypothesis of Proposition 3, one has that $M_{i}/{\\sim}_{i}=M_{i}/{\\sim}{\\scriptstyle{\\mathcal{N}}_{i+1}}$ . Moreover,   \n107 if two points $p$ , $q\\in M_{i}$ are connected by a $\\mathcal{C}^{1}$ curve $\\gamma:[0,1]\\rightarrow M_{i}$ satisfying $\\mathcal{N}_{i}(p)=\\mathcal{N}_{i}\\circ\\gamma(s)$   \n108 for every $s\\in[0,1]$ , then they lie in the same class of equivalence.   \n109 Making use of the Godement\u2019s criterion, we are now able to prove that the set $M_{i}/\\sim_{i}$ is a smooth   \n110 manifold, together with its dimension. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "111 Proposition 4. $\\frac{M_{i}}{\\sim_{i}}$ is a smooth manifold of dimension $d i m(\\mathcal{N}(M_{0}))$ . ", "page_idx": 2}, {"type": "text", "text": "112 This last achievement provides practical insights about the projection $\\pi_{i}$ on the quotient space, that   \n113 consists the building block of the algorithms used for recovering and exploring the equivalence   \n114 classes of a neural network.   \n115 Proposition 5. $\\pi_{i}:M_{i}\\,\\to\\,M_{i}/\\,\\sim_{i}$ is a smooth fiber bundle, with $K e r(d\\pi_{i})\\,=\\,\\nu M_{i}$ , which is   \n116 therefore an integrable distribution. $\\mathcal{V}M_{i}$ is the vertical bundle of $M_{i}$ . Every class of equivalence   \n117 $[p]$ is a path-connected submanifold of $M_{i}$ and coincide with the fiber of the bundle over the point   \n118 $p\\in M_{i}$ . ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "119 3 Methodology ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "120 The results depicted in Section 2.1 provide powerful tools for investigating how a neural network   \n121 sees the input space starting from a point $x$ . In particular we point out the following remarks: i) If   \n122 two points $x,y$ belonging to the input manifold $M_{0}$ are are such that $x\\sim_{0}y$ , then $\\mathcal{N}(x)=\\mathcal{N}(y)$ ; ii)   \n123 given a point $p\\in M_{n}$ , the counterimage $\\mathcal{N}^{-1}(p)$ is a smooth manifold, whose connected components   \n124 are classes of equivalences in $M_{0}$ with respect to $\\sim\\!0$ . A necessary condition for two points $x,y\\in M_{0}$   \n125 to be in the same class of equivalence is that $\\mathcal{N}(x)=\\mathcal{N}(y)$ ; iii) any class of equivalence $[x]$ , $x\\in M_{0}$ ,   \n126 is a maximal integral submanifold of $\\mathcal{V}M_{0}$ . The above observations directly provide with a strategy   \n127 to build up the equivalence class of an input point $x\\in M_{0}$ . Proposition 5 tells us that $\\mathcal{V}M_{0}$ is an   \n128 integrable distribution, with dimension equal to the dimension of the kernel of $g^{(0)}$ : we can hence find   \n129 $d i m\\bar{(}K e r(g^{(0)}))$ vector fields which are a base for the tangent space of $M_{0}$ . This means that we can   \n130 compute the eigenvalue decomposition of $g_{x}^{(0)}$ and consider the $L$ linearly independent eigenvectors,   \n131 namely $\\{v_{l}\\}_{l=1,\\ldots,L}$ , associated to the null eigenvalue: these eigenvectors depend smoothly on the   \n132 point, a fact that is not trivial when the matrix associated to the metric depends on several parameters   \n133 [15]. We can build then all the null curves by randomly selecting one eigenvector $\\tilde{v}\\in\\{v_{l}\\}$ and then   \n134 reconstruct the curve along the direction $\\tilde{v}$ from the starting point $x$ . From a practical point of view,   \n135 one is led to solve the Cauchy problem, a first order differential equation, with $\\dot{\\gamma}=\\tilde{v}$ and initial   \n136 condition $\\gamma(0)=x$ . ", "page_idx": 3}, {"type": "text", "text": "137 3.1 Input Space Exploration ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "138 This whole procedure is coded in the Singular Metric Equivalence Class (SiMEC) and the Singular   \n139 Metric Exploration (SiMExp) algorithms, whose general schemes are depicted in Algorithms 1 and 2.   \n140 SiMEC reconstructs the class of equivalence of the input via the exploration of the input space by   \n141 randomly selecting one of the eigenvectors related to the zero eigenvalue. On the opposite, in SiMExp,   \n142 in order to move from a class of equivalence to another we consider the eigenvectors relative to the   \n143 nonzero eigenvalues. This requires the slight difference in lines 5 to 7 between Algorithm 1 and   \n144 Algorithm 2.   \n146 There are some remarks to point out. From a numerical point of view, the diagonalization of the   \n147 pullback may lead to have even negative eigenvalues: hence one may use the notion of energy of   \n148 a curve, related to the pseudolength. The update rule for the new point (line 8) amounts to solve   \n149 the differential problem via the Euler method: for a reliable solution, we suggest to choose a small   \n150 step-length $\\delta$ . On the other hand, if the value of $\\delta$ is too small more iterations are needed to move   \n151 away from the starting point sensibly. Therefore there is a trade-off between the reliability of the   \n152 solution and the exploration pace. The proof of the well-posedness theorem for Cauchy problems,   \n153 cf. [18, Theorem 2.1], yields some insights, suggesting to set $\\delta$ equal to the inverse of the Lipschitz   \n154 constant of the map $\\mathcal{N}$ \u2013 which in practice we can estimate with the inverse of the square root of the   \n155 largest eigenvalue $\\lambda_{M}$ of the pullback metric $g_{p_{k}}^{0}$ . This is our default choice for Algorithm 1. We also   \n156 note that Algorithm 1 is more sensitive to the choice of the parameter $\\delta$ compared to Algorithm 2.   \n157 To build points in the same equivalence class Algorithm 1 needs to follow a null curve closely with   \n158 as little approximations as possible, namely with a small $\\delta$ . In contrast Algorithm 2, whose goal is   \n159 to change the equivalence class from one iteration to the next, does not have the same problem and   \n160 larger \u03b4 are allowed. Out default choice is therefore to set \u03b4 = 2\u03bb\u2212M1/2f or Algorithm 2. As for the   \n161 computational complexity of the two algorithms, the most demanding step is the computation of the   \n162 eigenvalues and eigenvectors, which is $\\textstyle\\operatorname{\\omega}(n^{3})$ , with $n$ the dimension of the square matrix $g_{p_{k}}^{0}$ [20].   \n163 Since all the other operations are either $O(n)$ or $O(n^{2})$ , we conclude that the complexity of both   \n164 Algorithms 1 and 2 is $O(n^{3})$ . ", "page_idx": 3}, {"type": "table", "img_path": "6qJKrOulTr/tmp/dec94248e533181f20e5b71e02a91c034a3ff4137c87370c0576624e8ddcd994.jpg", "table_caption": [], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "165 3.2 Interpretability ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "166 Algorithms 1 and 2 allow for the exploration of the equivalence classes in the input space of a Trans  \n167 former model. However, the points explored by these algorithms may not be directly interpretable   \n168 by a human perspective. For instance, an image or a piece of text may need to be decoded to be   \n169 \u201creadable\u201d by a human observer. Furthermore, we present an interpretation of the eigenvalues of the   \n170 pullback metric which allows us to define a feature importance metric. We present two interpretability   \n171 methods for Transformers based on input space exploration. Both methods are then demonstrated on   \n172 a Vision Transformer (ViT) trained for digit classification [8], and two BERT models, one trained for   \n173 hate speech classification and the other trained for MLM [7, 19].   \n175 Feature importance. Consider a Transformer model $T$ whose architecture includes a tokenizer $t_{T}$   \n176 (or patcher for images) that segments the input so that each segment can be converted into a continuous   \n177 representation by an embedding layer $e_{T}$ . This results in a matrix of dimensions $n_{s}\\times h$ , where $n_{s}$   \n178 represents the number of segments, and $h$ denotes the hidden size of the model\u2019s embeddings. The   \n179 eigenvalues of the pullback metric can be used to deduce the importance of each embedding and, by   \n180 extension, the significance of the segments they represent, with respect to the final prediction. The   \n181 process for determining the importance of textual tokens or image patches is outlined in Algorithm 3.   \n182 The appearance of the resulting heatmaps varies according to the type of input used. An example   \n183 of experiments with ViT on the MNIST dataset [12] is shown in Figure 1 that depicts heatmaps for   \n184 two MNIST instances. Figure 2, on the left, illustrates two experiment using Algorithm 3 on both a   \n185 BERT model for hate speech detection and a BERT model for MLM.   \n186 Interpretation of input space exploration. Using SiMEC and SiMExp to explore the embedding   \n187 space reveals how Transformer models perceive equivalence among different data points. Specifically,   \n188 these methodologies facilitate the sequential acquisition of embedding matrices $p_{0}\\cdot\\cdot\\cdot p_{K}$ at each   \n189 iteration, as detailed in Algorithms 1 and 2. Algorithm 4 implements a practical application of the   \n190 SiMEC/SiMExp approach with Transformer models. A key feature of this method is its ability   \n191 to selectively update specific tokens (for text inputs) or patches (for image inputs) during each   \n192 iteration. This selective updating allows us to explore targeted modifications that prompt the model   \n193 to either categorize different inputs as the same class or recognize them as distinct. Unlike traditional   \n194 approaches where modifications are predetermined, this method lets the model itself guide us to   \n195 understand which data points belong to specific equivalence classes. To interpret embeddings resulted   \n196 from the exploration process, they must be mapped back into a human-understandable form, such as   \n197 text or images. The interpretation of an embedding vector depends on the operations performed by   \n198 the Transformer\u2019s embedding module $e_{T}$ . If $e_{T}$ consists only of invertible operations, it is feasible to   \n199 construct a layer that performs the inverse operation relative to $e_{T}$ . The output can then be visualized   \n200 and directly interpreted by humans, allowing for a comparison with the original input to discern   \n201 how differences in embeddings reflect differences in their representations (e.g., text, images). If the   \n202 operations in $e_{T}$ are non-invertible, a trained decoder is required to reconstruct an interpretable output   \n203 from each embedding matrix $p_{0}\\ldots p_{K}$ . When using a BERT model, it is feasible to utilize layers   \n204 that are specialized for the masked language modeling (MLM) task to map input embeddings back to   \n205 tokens. This approach is effective whether the BERT model in question is specifically designed for   \n206 MLM or for sentence classification. In the case of sentence classification models, it is necessary to   \n207 select a corresponding MLM BERT model that shares the same internal architecture, including the   \n208 number of layers and embedding size.   \n209 Algorithm 5 depicts the process of interpreting Algorithm 4 outputs for both ViT and BERT experi  \n210 ments. After initializing the decoder according to the model type, the embeddings $p_{0}\\cdot\\cdot\\cdot p_{K}$ need to   \n211 be constrained to a feasible region. This region is defined by the distribution of embeddings derived   \n212 from the original input instances. Next, the embeddings are decoded, and the selected segments   \n213 for exploration are extracted. These segments are then used to replace the corresponding parts of   \n214 the original input instance. Figure 3 depicts an example outcome of Algorithm 5 applied on a ViT   \n215 exploration experiment. Given that the interpretation process includes both a capping step and a   \n216 decoding step (lines 10 and 11 of Algorithm 5), it\u2019s important to note that there isn\u2019t a direct 1:1   \n217 correspondence between each iteration\u2019s update and the interpretation outcomes. Our primary focus   \n218 is on exploring the input embedding space, rather than the input image or input sentence spaces.   \n219 For further investigation, we provide a detailed discussion on considering interpretation outputs as   \n220 alternative prompts in Section 4. ", "page_idx": 4}, {"type": "table", "img_path": "6qJKrOulTr/tmp/cc72dcd54e0860b5ae6ff15dc1bc885a2300facccc9e85fda439aa5c3d86d867.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "image", "img_path": "6qJKrOulTr/tmp/f18f5d7c5cc61daa70d852611c443658f4f2136439843ba3fe821406082eca35.jpg", "img_caption": ["Figure 1: Example output from Algorithm 3 applied to digit classification. These two instances are predicted as 3 (left) and 4 (right). The brightness of the color indicates the eigenvalue\u2019s magnitude. The brighter the color, the more sensitive the patch. This indicates that changes in the values of these sensitive patches are likely to have a greater impact on the prediction probabilities. Each patch in the heatmap corresponds to a $2\\times2$ square pixel. "], "img_footnote": [], "page_idx": 5}, {"type": "image", "img_path": "6qJKrOulTr/tmp/d72707acc240e7d5e8365ec7fd94643b23039882c0c645e70a2d22a611a894ef.jpg", "img_caption": ["Figure 2: Example outputs from Algorithm 3. The darker the color, the higher the token\u2019s eigenvalue. Left: The sentence analysed is classified as \u201coffensive\u201d by the BERT for hate speech detection, with significant contributions from tokens [CLS], politicians, corrupt, and ##eit (part of the word deceitful). Right: Example instance processed by a BERT model for masked language modeling. [MASK] is predicted as \u201cham\u201d, with the most influential tokens being pizza and cheese. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "table", "img_path": "6qJKrOulTr/tmp/b7f69c9b8a2a11723e2d61d32290f6577696244c738119cbd36d0ae6b4768b7d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "6qJKrOulTr/tmp/dc6977be089b156a2a5f5ed07e64038efefc58038ec3b635b912d6f0bf8ff1e1.jpg", "img_caption": ["Figure 3: Example of SiMEC and SiMExp output interpretation for ViT digit classification. Left: Original MNIST image of an $\"8\"$ . Center: Interpretation of a $p_{1000}$ from a SiMEC experiment, where $p_{1000}$ is predicted as \u201c8\u201d. Right: Interpretation of a $p_{1000}$ from a SiMExp experiment, where $p_{1000}$ is predicted as \u201c4\u201d. All patches are subject to SiMEC and SiMExp updates. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "221 4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "222 Experiments are conducted on textual and visual data. We aim to perform a preliminary investigation   \n223 of 3 features of our approach: (i) how the class probability changes on the decoded output of   \n224 SiMEC/SiMExp, (ii) what is the trade-off between the quantity and the quality of the output, and (iii)   \n225 how our method can be used to extract feature importance-based explanations.   \n226 In the textual case, we experiment with hate speech classification datasets: we use HateXplain2 [13],   \n227 which provides a ground truth for feature importance, plus a sample of 100 hate speech sentences   \n228 generated by prompting ChatGPT3, which serve purposes (i) and (ii). In the visual case, we perform   \n229 experiments on MNIST [12] dataset.   \n230 Using interpretation outputs as alternative prompts An interesting investigation is to determine   \n231 if our interpretation algorithm (Algorithm 5) can generate alternative prompts that stay in the same   \n232 equivalence class as the original input data or move to a different one, based on SiMEC and SiMExp   \n233 explorations. We test how the probability assigned to the original equivalence class by the Transformer   \n234 model changes as the SiMEC and SiMExp algorithms explore the input embedding manifold.   \n235 For BERT experiments we generate prompts to inspect the probability distribution over the vocabulary   \n236 for tokens updated by Algorithms 1 and 2. We decode the updated $p_{0}\\ldots p_{K}$ using Algorithm 5,   \n237 focusing on tokens updated through the iterations. For each of these decoded tokens, we extract   \n238 the top-5 scores to obtain 5 alternative tokens to replace the original ones, creating 5 alternate   \n239 prompts. We then extract the prediction $i^{*}=\\arg\\operatorname*{max}_{i}y_{i}$ for the original sentence, which represents   \n240 the output whose equivalence class we aim to explore. Finally, we classify the new prompts,   \n241 obtaining the corresponding predictions $Y\\;=\\;\\mathbf{y}^{(0)}\\,.\\,.\\,.\\,\\mathbf{y}^{(K)}$ , where each $\\mathbf{y}^{(\\bar{k})}\\,\\in\\,\\mathbb{R}^{N}$ , $N$ being   \n242 the number of prediction classes. We visualize the prediction trend for the $i^{*}$ th value in every   \n243 $\\mathbf{y}^{(0)}\\dots..\\mathbf{y}^{(K)}$ categorizing the images into two subsets: those that lead to a change in prediction   \n244 $Y_{c}=\\{\\mathbf{\\dot{y}}^{(k)}\\in Y\\,\\,\\vert\\,\\mathop{\\mathrm{arg\\,max}}_{i}\\,y_{i}^{(k)}\\neq\\dot{\\iota}^{*}\\}$ and those that don\u2019t $Y_{s}=\\{\\mathbf{y}_{i}\\in Y\\mid\\arg\\operatorname*{max}_{i}y_{i}^{(\\bar{k})}=i^{*}\\}$ .   \n245 Sentence classification experiments4 involved 1000 iterations from both SiMEC and SiMExp, applied   \n246 to a subset of 8 sentences from the ChatGPT hate speech dataset. The plot on the left side of Figure 4   \n247 illustrates that, as the original embeddings are increasingly modified, SiMExp tends to produce   \n248 alternatives with lower prediction values for $i^{*}$ compared to SiMEC. Thus, even if predictions change   \n249 in SiMEC experiments, the equivalence class prediction value remains approximately constant and   \n250 higher than in SiMExp. Considering the plot on the right side of Figure 4, SiMExp identifies prompts   \n251 that lower the prediction value for $i^{*}$ . ViT and MLM experiments are detailed in the Supplementary   \n252 Materials.   \n253 Input space exploration We measure the time required to explore the input space of a ViT with   \n254 the SiMEC algorithm and compare it with a perturbation-based method. The perturbation-based   \n255 method mimics a trial-and-error approach as it takes an input image and, at each iteration, perturbs   \n256 it by a semi-random vector $\\mathbf{v}_{t+1}=a_{t}\\mathbf{v}_{t}+\\eta\\epsilon$ , where $a_{t}=1$ if $y_{t}=y_{t-1}$ , $a_{t}=-1$ otherwise, $\\epsilon$ is   \n257 an orthogonal random vector from a standard normal distribution and $\\eta$ is the step length. With the   \n258 perturbation, we obtain a new image, then check whether the model yields the same label for the   \n259 new image. The perturbation vector is re-initialized at random from a normal distribution $20\\%$ of the   \n260 times to allow for exploration. We construct this method to have a direct comparison with ours in the   \n261 absence of a consolidated literature about the task.   \n262 We train a ViT model having 4 layers and 4 heads per layer on the MNIST dataset5. The SiMEC   \n263 algorithm is run for 1000 iterations, so that it can generate 1000 examples starting from a single   \n264 image. In a sample of 100 images, the average time is approximately 339 seconds.6 In the same   \n265 time, the perturbation-based algorithm can produce up to 36000 images. However, we notice that   \n266 the perturbation-based algorithm ends up producing monochrome (pixel color has zero variance) or   \n267 totally noisy images, which provide little information about the behavior of the model. Excluding   \n268 only the images with low color variance $(<0.01)$ , we are left, on average, with 19 images (standard   \n269 deviation 13.9). SiMEC, in contrast, doesn\u2019t present this behavior, as all 1000 images have high   \n270 enough intensity variance and are thus useful for explainability purposes.   \n271 As BERT has many more parameters with respect to our ViT model, processing textual data takes   \n272 longer. Specifically, in a sample of 16 sentences, the average time needed to run 1000 iterations on a   \n273 sentence is 7089 seconds, taking into account both MLM and classification experiments.   \n274 Feature importance-based explanations We compare our method against Attention Rollout   \n275 (AR) [1] and the Relevancy method proposed by Chefer et al. [6]. In the textual case, we provide a   \n276 quantitative evaluation using the HateXplain dataset, which contains 20147 sentences (of which 1924   \n277 in the test set) annotated with normal, offensive and hate speech labels as well as the positions of   \n278 words that support the label decision. We then measure the cosine similarity between the importance   \n279 assigned by each method to each word in a sentence and the ground truth. Notice that, since the   \n280 dataset contains multiple annotations, the ground truth $y$ for each word $w$ is obtained as the average   \n281 of the binary labels assigned by each annotator, and therefore $y(w)\\in[0;1]$ . We also normalize all   \n282 scores in [0; 1] so to have them on the same scale. The average similarity achieved by our method is   \n283 0.707 (standard deviation $\\sigma=0.302)$ ), against 0.7 $\\sigma=0.315)$ for Relevancy and 0.583 $\\sigma=0.318)$ for   \n284 AR. This proves our method to be more effective in finding the most sensitive tokens for classification.   \n285 We provide an example on image classification in the Supplementary Materials. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "image", "img_path": "6qJKrOulTr/tmp/66e1f0f3d802d39462996ed2c887f0195e97069373ab0900a33572f78bd459a5.jpg", "img_caption": ["Figure 4: Analysis involving results SiMEC and SiMExp applied to BERT for hate speech detection. Left: Prediction values for $i^{*}$ for each $\\mathbf{y}\\in Y_{c}$ . Right: Prediction values for $\\mathbf{y}\\in Y_{s}$ . "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "286 5 Related work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "287 Our work relates to embedding space exploration literature, and has at least one collateral applications   \n288 in the XAI domain, namely producing feature importance-based explanations.   \n289 Embedding space exploration. Works dealing with embedding space exploration mostly focus   \n290 on the study of specific properties of the embedding space of Transformers, especially in NLP. For   \n291 instance, Cai et al. [5] challenge the idea that the embedding space is inherently anisotropic [10]   \n292 discovering local isotropy, and find low-dimensional manifold structures in the embedding space   \n293 of GPT and BERT. Bi\u00b4s et al. [3] argue that the anisotropy of the embedding space derives from   \n294 embeddings shifting in common directions during training. In the field of CV, Vilas et al. [21] map   \n295 internal representations of a ViT onto the output class manifold, enabling the early identification of   \n296 class-related patches and the computation of saliency maps on the input image for each layer and   \n297 head. Applying Singular Value Decomposition to the Jacobian matrix of a ViT, Salman et al. [17]   \n298 treat the input space as the union of two subspaces: one in which image embedding doesn\u2019t change,   \n299 and another one for which it changes. Except for the last one, all the aforementioned approaches rely   \n300 on data samples. By studying the inverse image of the model, instead, we can do away with data   \n301 samples.   \n302 Feature importance-based explanations. Feature importance is a measure of the contribution of   \n303 each data feature to a model prediction. In the context of Computer Vision and Natural Language   \n304 Processing, it amounts to giving a weight to pixels (or patches of pixels) in an image and tokens   \n305 in a piece of text, respectively. In recent years, much research has focused on Transformers in   \n306 both CV and NLP. Most approaches are based on the attention mechanism of the Transformer   \n307 architecture. Abnar and Zuidema [1] quantify the overall attention of the output on the input by   \n308 computing a linear combination of layer attentions (Attention Rollout) or applying a maximum   \n309 flow algorithm (Attention Flow). To overcome the limitations [4] of attention-based methods, Hao   \n310 et al. [11] use the concept of attribution, which is obtained by multiplying attention matrices by   \n311 the integrated gradient of the model with respect to them. Chefer et al. [6] propose the Relevancy   \n312 metric to generalize attribution to bi-modal and encoder-decoder architectures. Other methods are   \n313 perturbation-based, where perturbations of input data are used to record any change in the output and   \n314 draw a saliency map on the input. In order to overcome the main issue with such methods, i.e. the   \n315 generation of outlier inputs, Englebert et al. [9] apply perturbations after the position encoding of the   \n316 patches. In contrast with these methods, ours does not need arbitrary perturbations of inputs, and   \n317 considers all parameters of the model, not only the attention query and key matrices. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "318 6 Conclusions ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "319 Our exploration of the Transformer architecture through a theoretical framework grounded in Rie  \n320 mannian Geometry led to the application of our two algorithms, SiMEC and SiMExp, for examining   \n321 equivalence classes in the Transformers\u2019 input space. We demonstrated how the results of these explo  \n322 ration methods can be interpreted in a human-readable form and conducted preliminary investigations   \n323 into their potential applications. Notably, our methods show promise for ranking feature importance   \n324 and generating alternative prompts within the same or different equivalence classes.   \n325 Future research directions include expanding our experimental results and delving deeper into the po  \n326 tential of our framework for controlled input generation within an equivalence class. This application   \n327 holds significant promise for enhancing the explainability of Transformer models\u2019 decisions and for   \n328 addressing issues related to bias and hallucinations. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "329 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "330 [1] S. Abnar and W. Zuidema. Quantifying attention flow in transformers. 2020. arXiv:2005.00928.   \n331 [2] A. Benfenati and A. Marta. A singular Riemannian geometry approach to Deep Neural Networks   \n332 I. Theoretical foundations. Neural Networks, 158:331\u2013343, 2023.   \n333 [3] D. Bi\u00b4s, M. Podkorytov, and X. Liu. Too much in common: Shifting of embeddings in trans  \n334 former language models and its implications. In Proceedings of the 2021 conference of the   \n335 North American chapter of the Association for Computational Linguistics: Human Language   \n336 Technologies, pages 5117\u20135130, 2021.   \n337 [4] G. Brunner, Y. Liu, D. Pascual, O. Richter, M. Ciaramita, and R. Wattenhofer. On identifiability   \n338 in transformers. International Conference on Learning Representations, 2019.   \n339 [5] X. Cai, J. Huang, Y. Bian, and K. Church. Isotropy in the contextual embedding space: Clusters   \n340 and manifolds. In International conference on learning representations, 2020.   \n341 [6] H. Chefer, S. Gur, and L. Wolf. Generic attention-model explainability for interpreting bi-modal   \n342 and encoder-decoder transformers. In Proceedings of the IEEE/CVF International Conference   \n343 on Computer Vision, pages 397\u2013406, 2021.   \n344 [7] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: Pre-training of deep bidirectional   \n345 transformers for language understanding. In J. Burstein, C. Doran, and T. Solorio, editors,   \n346 Proceedings of the 2019 Conference of the North American Chapter of the Association for   \n347 Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Pa  \n348 pers), pages 4171\u20134186, Minneapolis, Minnesota, June 2019. Association for Computational   \n349 Linguistics. doi: 10.18653/v1/N19-1423. URL https://aclanthology.org/N19-1423.   \n350 [8] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani,   \n351 M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby. An image is worth 16x16   \n352 words: Transformers for image recognition at scale, 2021. arXiv:2010.11929.   \n353 [9] A. Englebert, S. Stassin, G. Nanfack, S. A. Mahmoudi, X. Siebert, O. Cornu, and   \n354 C. De Vleeschouwer. Explaining through transformer input sampling. In Proceedings of   \n355 the IEEE/CVF International Conference on Computer Vision, pages 806\u2013815, 2023.   \n356 [10] J. Gao, D. He, X. Tan, T. Qin, L. Wang, and T.-Y. Liu. Representation degeneration problem   \n357 in training natural language generation models. In International conference on learning rep  \n358 resentations, volume abs/1907.12009, 2019. URL https://api.semanticscholar.org/   \n359 CorpusID:59317065.   \n360 [11] Y. Hao, L. Dong, F. Wei, and K. Xu. Self-attention attribution: Interpreting information   \n361 interactions inside transformer. In Proceedings of the AAAI Conference on Artificial Intelligence,   \n362 volume 35, pages 12963\u201312971, 2021.   \n363 [12] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document   \n364 recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998. doi: 10.1109/5.726791.   \n365 [13] B. Mathew, P. Saha, S. M. Yimam, C. Biemann, P. Goyal, and A. Mukherjee. Hatexplain: A   \n366 benchmark dataset for explainable hate speech detection. In Proceedings of the AAAI conference   \n367 on artificial intelligence, volume 35, pages 14867\u201314875, 2021.   \n368 [14] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, and A. Swami. The Limitations   \n369 of Deep Learning in Adversarial Settings. In 2016 IEEE European Symposium on Security and   \n370 Privacy (EuroS&P), pages 372\u2013387, Mar. 2016. doi: 10.1109/EuroSP.2016.36.   \n371 [15] F. Rellich and J. Berkowitz. Perturbation Theory of Eigenvalue Problems. New York University.   \n372 Institute of Mathematical Sciences. Gordon and Breach, 1969. ISBN 9780677006802.   \n373 [16] M. T. Ribeiro, S. Singh, and C. Guestrin. \" why should i trust you?\" explaining the predictions of   \n374 any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge   \n375 discovery and data mining, pages 1135\u20131144, 2016.   \n376 [17] S. Salman, M. M. B. Shams, and X. Liu. Intriguing equivalence structures of the embedding   \n377 space of vision transformers, 2024. arXiv:2401.15568.   \n378 [18] M. E. Taylor. Partial differential equations. I: Basic theory, volume 115 of Appl. Math. Sci.   \n379 Cham: Springer, 3rd corrected and expanded edition edition, 2023. ISBN 978-3-031-33858-8;   \n380 978-3-031-33861-8; 978-3-031-33859-5. doi: 10.1007/978-3-031-33859-5.   \n381 [19] C. Toraman, F. \u00b8Sahinu\u00e7, and E. H. Yilmaz. Large-scale hate speech detection with cross  \n382 domain transfer. In Proceedings of the Language Resources and Evaluation Conference, pages   \n383 2215\u20132225, Marseille, France, June 2022. European Language Resources Association. URL   \n384 https://aclanthology.org/2022.lrec-1.238.   \n385 [20] L. N. Trefethen and D. I. Bau. Numerical linear algebra. Twenty-fifth anniversary edition,   \n386 volume 181 of Other Titles Appl. Math. Philadelphia, PA: Society for Industrial and Applied   \n387 Mathematics (SIAM), 2022. ISBN 978-1-61197-715-8.   \n388 [21] M. G. Vilas, T. Schauml\u00f6ffel, and G. Roig. Analyzing vision transformers for image classi  \n389 fication in class embedding space. Advances in Neural Information Processing Systems, 36,   \n390 2024.   \n391 [22] M. Wu, H. Wu, and C. Barrett. Verix: Towards verified explainability of deep neural networks.   \n392 Advances in neural information processing systems, 36, 2024. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "393 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "95 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n96 paper\u2019s contributions and scope?   \n97 Answer: [Yes]   \n98 Justification: In the abstract and introduction we claim that we present a method for the   \n99 exploration of equivalence classes in the input space of Transformer models, which is   \n00 analyzed in depth in Section 3. The mathematical theory we refer to is deepened in Section   \n01 2.   \n02 Guidelines:   \n03 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n04 made in the paper.   \n05 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n06 contributions made in the paper and important assumptions and limitations. A No or   \n07 NA answer to this question will not be perceived well by the reviewers.   \n08 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n09 much the results can be expected to generalize to other settings.   \n10 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n11 are not attained by the paper. ", "page_idx": 11}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 11}, {"type": "text", "text": "Justification: We discuss the limitations and tradeoff given by numeric integration in Subsection 3.1 and theoretical assumptions are enumerated in Section 2. Computational efficiency of our algorithms is discussed in Subsection 3.1. We conducted experiments on 3 datasets only, one of which of small dimensions since our main focus is on the mathematical theory grounding the application of the method to Transformers, as stated at the beginning of Section 4. Other limitations are mentioned throughout Section 4, including the fact that our investigations in the human-readable scenario are at a preliminary stage. ", "page_idx": 11}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best ", "page_idx": 11}, {"type": "text", "text": "judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 12}, {"type": "text", "text": "47   \n48   \n49 3. Theory Assumptions and Proofs   \n450 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n451 a complete (and correct) proof?   \n452 Answer: [Yes]   \n453 Justification: The full proofs are part of two previously published papers which we cannot   \n454 disclose for anonymity requirements. We replicate the relevant proofs in the supplementary   \n55 material, part of which will be removed from the final version of the paper, referencing to   \n456 the other papers.   \n457 Guidelines:   \n58 \u2022 The answer NA means that the paper does not include theoretical results.   \n459 \u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross  \n60 referenced.   \n461 \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n62 \u2022 The proofs can either appear in the main paper or the supplemental material, but if   \n63 they appear in the supplemental material, the authors are encouraged to provide a short   \n64 proof sketch to provide intuition.   \n65 \u2022 Inversely, any informal proof provided in the core of the paper should be complemented   \n66 by formal proofs provided in appendix or supplemental material.   \n467 \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced.   \n468 4. Experimental Result Reproducibility   \n469 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n70 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n71 of the paper (regardless of whether the code and data are provided or not)?   \n72 Answer: [Yes]   \n73 Justification: Pseudo-code of the proposed algorithms is reported in Subsections 3.1 and 3.2   \n74 so to make the algorithms reproducible, plus our implementation is made available in the   \n75 supplementary material. Experiments, including the complete setting, and the respective   \n76 baselines are described in Section 4.   \n77 Guidelines:   \n78 \u2022 The answer NA means that the paper does not include experiments.   \n79 \u2022 If the paper includes experiments, a No answer to this question will not be perceived   \n480 well by the reviewers: Making the paper reproducible is important, regardless of   \n481 whether the code and data are provided or not. ", "page_idx": 12}, {"type": "text", "text": "\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. ", "page_idx": 12}, {"type": "text", "text": "500 (c) If the contribution is a new model (e.g., a large language model), then there should   \n501 either be a way to access this model for reproducing the results or a way to reproduce   \n502 the model (e.g., with an open-source dataset or instructions for how to construct   \n503 the dataset).   \n504 (d) We recognize that reproducibility may be tricky in some cases, in which case   \n505 authors are welcome to describe the particular way they provide for reproducibility.   \n506 In the case of closed-source models, it may be that access to the model is limited in   \n507 some way (e.g., to registered users), but it should be possible for other researchers   \n508 to have some path to reproducing or verifying the results. ", "page_idx": 13}, {"type": "text", "text": "509 5. Open access to data and code ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "510 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n511 tions to faithfully reproduce the main experimental results, as described in supplemental   \n512 material? ", "page_idx": 13}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 13}, {"type": "text", "text": "Justification: All experiments are made reproducible through scripts provided as supplementary material. ", "page_idx": 13}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 13}, {"type": "text", "text": "536 6. Experimental Setting/Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "37 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n38 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n39 results? ", "page_idx": 13}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 13}, {"type": "text", "text": "Justification: All details are provided in Section 4: details of the analyzed architectures, number of iterations of the SiMEC/SiMExp algorithms, technical infrastructure on which the experiments were performed, amount of data the experiments were performed on. Guidelines: ", "page_idx": 13}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 13}, {"type": "text", "text": "550 7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "551 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n552 information about the statistical significance of the experiments?   \n553 Answer: [Yes]   \n554 Justification: The standard deviation is reported for the experiment that supports the claims   \n555 of the paper, i.e. the one on feature importance-based explanations, in Section 4. Stan  \n556 dard deviation is also reported for the number of uninformative images produced by the   \n557 perturbation-based baseline method in the Input space exploration experiment.   \n558 Guidelines:   \n559 \u2022 The answer NA means that the paper does not include experiments.   \n560 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n561 dence intervals, or statistical significance tests, at least for the experiments that support   \n562 the main claims of the paper.   \n563 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n564 example, train/test split, initialization, random drawing of some parameter, or overall   \n565 run with given experimental conditions).   \n566 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n567 call to a library function, bootstrap, etc.)   \n568 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n569 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n570 of the mean.   \n571 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n572 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n573 of Normality of errors is not verified.   \n574 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n575 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n576 error rates).   \n577 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n578 they were calculated and reference the corresponding figures or tables in the text.   \n579 8. Experiments Compute Resources   \n580 Question: For each experiment, does the paper provide sufficient information on the com  \n581 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n582 the experiments?   \n583 Answer: [Yes]   \n584 Justification: All the experiments were performed on the same infrastructure, which is   \n585 reported in a footnote in Section 4. Time of execution is one of the key indicators reported   \n586 for the Input space exploration experiments. More computing power would be required for   \n587 experiments on bigger Transformer models.   \n588 Guidelines:   \n589 \u2022 The answer NA means that the paper does not include experiments.   \n590 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n591 or cloud provider, including relevant memory and storage.   \n592 \u2022 The paper should provide the amount of compute required for each of the individual   \n593 experimental runs as well as estimate the total compute.   \n594 \u2022 The paper should disclose whether the full research project required more compute   \n595 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n596 didn\u2019t make it into the paper).   \n597 9. Code Of Ethics   \n598 Question: Does the research conducted in the paper conform, in every respect, with the   \n599 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n600 Answer: [Yes]   \n601 Justification: We comply with the terms of use of the datasets employed in the experiments,   \n602 and we deem our work has no potentially harmful effect on people safety, security, discrimi  \n603 nation, surveillance, harassment, nor on human rights. Our proposal does not contribute to   \n604 spread bias and unfairness towards certain groups of people nor to harm the environment. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 15}, {"type": "text", "text": "611 10. Broader Impacts ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 15}, {"type": "text", "text": "Justification: Although the impacts of XAI on society is broad and deep, in this paper we focus only on the technical problem of exploring the equivalence classes in the input space of Transformers, which doesn\u2019t add any specific impact to the discussion about XAI in general. ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 15}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "43 Question: Does the paper describe safeguards that have been put in place for responsible   \n44 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n45 image generators, or scraped datasets)? ", "page_idx": 15}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 15}, {"type": "text", "text": "647 Justification: The paper poses no such risks.   \n648 Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 15}, {"type": "text", "text": "659 12. Licenses for existing assets   \n660 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n661 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n662 properly respected?   \n663 Answer: [Yes]   \n664 Justification: The datasets used in the paper are explicitely mentioned in the references, as   \n665 required by the terms of use. Where applicable, the license is also reported.   \n666 Guidelines:   \n667 \u2022 The answer NA means that the paper does not use existing assets.   \n668 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n669 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n670 URL.   \n671 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n672 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n673 service of that source should be provided.   \n674 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n675 package should be provided. For popular datasets, paperswithcode.com/datasets   \n676 has curated licenses for some datasets. Their licensing guide can help determine the   \n677 license of a dataset.   \n678 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n679 the derived asset (if it has changed) should be provided.   \n680 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n681 the asset\u2019s creators.   \n682 13. New Assets   \n683 Question: Are new assets introduced in the paper well documented and is the documentation   \n684 provided alongside the assets?   \n685 Answer: [NA]   \n686 Justification: The paper does not release new assets.   \n687 Guidelines:   \n688 \u2022 The answer NA means that the paper does not release new assets.   \n689 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \nsubmissions via structured templates. This includes details about training, license,   \n691 limitations, etc.   \n692 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n693 asset is used.   \n694 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n695 create an anonymized URL or include an anonymized zip file.   \n696 14. Crowdsourcing and Research with Human Subjects   \n697 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n698 include the full text of instructions given to participants and screenshots, if applicable, as   \n699 well as details about compensation (if any)?   \n700 Answer: [NA]   \n701 Justification: Our work doesn\u2019t include crowdsourcing nor research with human subjects.   \n702 Guidelines:   \n703 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n704 human subjects.   \n705 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n706 tion of the paper involves human subjects, then as much detail as possible should be   \n707 included in the main paper.   \n708 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n709 or other labor should be paid at least the minimum wage in the country of the data   \n710 collector.   \n711 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n712 Subjects   \n713 Question: Does the paper describe potential risks incurred by study participants, whether   \n714 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n715 approvals (or an equivalent approval/review based on the requirements of your country or   \n716 institution) were obtained?   \n717 Answer: [NA]   \n718 Justification: Our work does not involve crowdsourcing nor research with human subjects.   \n719 Guidelines:   \n720 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n721 human subjects.   \n722 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)   \n723 may be required for any human subjects research. If you obtained IRB approval, you   \n724 should clearly state this in the paper.   \n725 \u2022 We recognize that the procedures for this may vary significantly between institutions   \n726 and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the   \n727 guidelines for their institution.   \n728 \u2022 For initial submissions, do not include any information that would break anonymity (if   \n729 applicable), such as the institution conducting the review. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}]