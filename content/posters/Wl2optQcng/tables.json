[{"figure_path": "Wl2optQcng/tables/tables_3_1.jpg", "caption": "Table 1: Average (standard deviation) test accuracy on CIFAR10/100-S for varying proportions of training data.", "description": "This table presents the average test accuracy and standard deviation achieved by different federated learning methods on CIFAR10-S and CIFAR100-S datasets under varying proportions of training data (100%, 75%, 50%, 25%).  It demonstrates the performance of these methods under conditions of covariate shift (simulated image corruptions) and data scarcity. The results highlight the relative performance gains of pFedFDA, especially in data-scarce settings.", "section": "5.2 Numerical Results"}, {"figure_path": "Wl2optQcng/tables/tables_7_1.jpg", "caption": "Table 1: Average (standard deviation) test accuracy on CIFAR10/100-S for varying proportions of training data.", "description": "This table presents the average and standard deviation of test accuracy achieved by various federated learning methods on CIFAR10-S and CIFAR100-S datasets.  The results are broken down by the percentage of training data used (100%, 75%, 50%, 25%) to show the impact of data scarcity on model performance.  The table allows for comparison of different algorithms under varying data constraints and highlights the relative performance in data-scarce scenarios.", "section": "5.2 Numerical Results"}, {"figure_path": "Wl2optQcng/tables/tables_7_2.jpg", "caption": "Table 1: Average (standard deviation) test accuracy on CIFAR10/100-S for varying proportions of training data.", "description": "This table presents the average test accuracy and standard deviation achieved by various federated learning methods on the CIFAR10 and CIFAR100 datasets under different data scarcity conditions (100%, 75%, 50%, and 25% of training data). The datasets are modified to simulate covariate shift by introducing image corruptions.  The results show how different methods perform with varying amounts of available data under conditions of data heterogeneity.", "section": "5.2 Numerical Results"}, {"figure_path": "Wl2optQcng/tables/tables_8_1.jpg", "caption": "Table 1: Average (standard deviation) test accuracy on CIFAR10/100-S for varying proportions of training data.", "description": "This table presents the average test accuracy and its standard deviation for CIFAR10 and CIFAR100 datasets with covariate shift, using different percentages of training data (100%, 75%, 50%, 25%). The results are shown for various federated learning algorithms, including the proposed pFedFDA, allowing comparison of their performance under different data scarcity levels.", "section": "5.2 Numerical Results"}, {"figure_path": "Wl2optQcng/tables/tables_8_2.jpg", "caption": "Table 4: Evaluation of new-client generalization on CIFAR10 Dir(0.5).", "description": "This table presents the results of evaluating the model's ability to generalize to new clients not seen during training.  It shows the average test accuracy for several federated learning methods on CIFAR10 dataset with Dir(0.5) partitioning, across both original clients and new clients subjected to various image corruptions.  The results highlight the performance of different models on unseen data and with different data quality issues.", "section": "5.3 Ablation of Method Components"}, {"figure_path": "Wl2optQcng/tables/tables_8_3.jpg", "caption": "Table 5: Ablation study on CIFAR100 with Dir(0.1) partition. NB denotes clients using only local data to estimate their feature distribution (\u03b2i = 1). SB denotes each client estimating a single \u03b2i for both the means and covariance, MB denotes clients computing \u03b2i terms for the means and covariance separately. We show the average computational overhead across all settings.", "description": "This table presents the ablation study of the pFedFDA algorithm, specifically focusing on the impact of different strategies for estimating the interpolation coefficient \u03b2i which balances local and global knowledge for feature distribution adaptation. It compares three methods: using only local data (NB), using a single \u03b2i for both mean and covariance (SB), and using separate \u03b2is for mean and covariance (MB). The results are shown for different data scarcity scenarios (CIFAR100, CIFAR100-25%, CIFAR100-S) under two different data distribution settings (Dir(0.1) and Dir(0.5)). The table also includes the computation overhead for each method.", "section": "5.3 Ablation of Method Components"}, {"figure_path": "Wl2optQcng/tables/tables_9_1.jpg", "caption": "Table 6: Comparison of system runtime on the CIFAR10 dataset.", "description": "This table compares the local training time (client-side computation) and total runtime of pFedFDA to baseline methods on CIFAR10. It shows that pFedFDA has a slightly increased training time compared to FedAvg, mainly due to the cost of learning the parameter interpolation coefficient. However, this increase is comparable to other methods and is lower than representation-learning methods.", "section": "5.4 Communication and Computation"}, {"figure_path": "Wl2optQcng/tables/tables_15_1.jpg", "caption": "Table 7: Results on multi-domain DIGIT-5 benchmark for varying data volumes.", "description": "This table presents the results of the multi-domain DIGIT-5 experiment with varying data volumes.  The DIGIT-5 dataset is a domain generalization benchmark where data is drawn from five different datasets. The experiment uses 20 clients with full participation, 4 assigned to each domain.  The Dirichlet(0.5) partitioning strategy is used to distribute data to each client. The table shows the average test accuracy and improvement for each method (Local, FedAvg, FedAvgFT, Ditto, FedPAC, and pFedFDA) at 25%, 50%, 75%, and 100% data volume.", "section": "D.1 Multi-Domain FL"}, {"figure_path": "Wl2optQcng/tables/tables_16_1.jpg", "caption": "Table 8: Comparison of communication load (parameters/iter.) between our Gaussian distribution parameters (\u03bc, \u03a3) and standard linear classifiers.", "description": "This table compares the number of parameters required for the Gaussian generative classifiers used in pFedFDA against the parameter count of a standard linear classifier. It demonstrates the relative overhead introduced by the generative model for different datasets and network architectures.  A negative overhead indicates that the generative classifier uses fewer parameters than the standard linear classifier.", "section": "D.3 Communication Load Examples"}, {"figure_path": "Wl2optQcng/tables/tables_16_2.jpg", "caption": "Table 9: Percentage (average (std)) of the local training time associated each component of our algorithm.", "description": "This table shows the proportion of the local training time spent on different parts of the pFedFDA algorithm. It breaks down the time spent on network passes (training the shared feature extractor), mean/covariance estimation (estimating local feature distribution parameters), and interpolation optimization (optimizing the interpolation coefficient beta).  The percentages are shown for CIFAR10, CIFAR100, and TinyImageNet datasets.", "section": "D.4 Runtime of Method Components"}]