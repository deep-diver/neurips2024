[{"type": "text", "text": "Personalized Federated Learning via Feature Distribution Adaptation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Connor J. McLaughlin, Lili Su Northeastern University, Boston, MA 02115 {mclaughlin.co,l.su}@northeastern.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Federated learning (FL) is a distributed learning framework that leverages commonalities between distributed client datasets to train a global model. Under heterogeneous clients, however, FL can fail to produce stable training results. Personalized federated learning (PFL) seeks to address this by learning individual models tailored to each client. One approach is to decompose model training into shared representation learning and personalized classifier training. Nonetheless, previous works struggle to navigate the bias-variance trade-off in classifier learning, relying solely on limited local datasets or introducing costly techniques to improve generalization. In this work, we frame representation learning as a generative modeling task, where representations are trained with a classifier based on the global feature distribution. We then propose an algorithm, pFedFDA, that efficiently generates personalized models by adapting global generative classifiers to their local feature distributions. Through extensive computer vision benchmarks, we demonstrate that our method can adjust to complex distribution shifts with significant improvements over current state-of-the-art in data-scarce settings. Our source code is available on GitHub1. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The success of deep learning models relies heavily on access to large, diverse, and comprehensive training data. However, communication constraints, user privacy concerns, and government regulations on centralized data collection often pose significant challenges to this requirement [31, 37, 18]. To address these issues, Federated Learning (FL) [34] has gained considerable attention as a distributed learning framework, especially for its privacy-preserving properties and efficiency in training deep networks. ", "page_idx": 0}, {"type": "text", "text": "The FedAvg algorithm, introduced in the seminal work [34], remains one of the most widely adopted algorithms in FL applications [32, 45, 38, 49, 40, 7]. It utilizes a parameter server to maintain a global model, trained through iterative rounds of distributed client local updates and server aggregation of client models. While effective under independent and identically distributed (i.i.d.) client data, its performance deteriorates as client datasets become more heterogeneous (non-i.i.d.). Data heterogeneity leads to the well-documented phenomenon of client drift [19], where distinct local objectives cause the model to diverge from the global optimum, resulting in slow convergence [20, 28] and suboptimal local client performance [42]. Despite extensive efforts [27, 19, 46, 6] to enhance FedAvg for non-i.i.d. clients, the use of a single global model remains too restrictive for many FL applications. ", "page_idx": 0}, {"type": "text", "text": "Personalized federated learning (PFL) has emerged as an alternative framework that produces separate models tailored to each client. The success of personalization techniques depends on balancing the bias introduced by using global knowledge that may not generalize to individual clients, and the variance inherent in learning from limited local datasets. Popular PFL techniques include regularized local objectives [41, 26], local-global parameter interpolation [6], meta-learning [9, 16], and representation learning [35, 5, 48, 29]. While these techniques have shown significant improvements for clients under limited types of synthetic data heterogeneity (e.g., imbalanced partitioning of an otherwise i.i.d. dataset), we find that current methods still struggle to navigate the bias-variance trade-off with the additional challenge of feature distribution shift and data scarcity, conditions commonly encountered in cross-device FL. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "As such, we look to design a method capable of handling real-world distribution shifts, e.g., covariate shift caused by weather conditions or poor camera calibration, (see clients 1 and 2 in Fig. 1) with limited local datasets. To this end, we approach PFL through shared representation learning guided by a global, low-variance generative classifier. Specifically, we select a probability density $p$ with desirable statistical properties (e.g., one that admits an efficient Bayesian classifier) and iteratively estimate the global parameters of this distribution and representation layers to produce features from the estimated distribution (Fig. 1a). ", "page_idx": 1}, {"type": "text", "text": "To further navigate the bias-variance trade-off, we introduce a local-global interpolation method to adapt the global estimate to the distribution of each client. At inference time, clients use their adaptive local distribution estimate in a personalized Bayesian classifier (Fig. 1b). ", "page_idx": 1}, {"type": "text", "text": "Contributions. We propose a novel Personalized Federated Learning method based on Feature Distribution Adaptation (pFedFDA). We contextualize our algorithm using a class-conditional multivariate Gaussian model of the feature space in a variety of computer vision benchmarks. Our empirical evaluation demonstrates that our proposed method consistently improves average model accuracy in benchmarks with covariate shift or client data scarcity, obtaining over $6\\%$ in multiple settings. At the same time, our method remains competitive with current state-of-the-art (often within $1\\%$ ) on more general benchmarks with more moderate data heterogeneity. To summarize, our contributions are three-fold: ", "page_idx": 1}, {"type": "text", "text": "\u2022 A novel generative modeling perspective for federated representation learning is proposed to enable a new bias-variance trade-off for client classifier learning. \u2022 We propose a personalized federated learning method, pFedFDA, which leverages awareness of latent data distributions to guide representation learning and client personalization. \u2022 Extensive experiments on image classification datasets with varying levels of natural data heterogeneity and data availability demonstrate the advantages of pFedFDA in challenging settings. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Federated Learning with Non-i.i.d. Data. Various studies have worked to understand and improve the ability of FL to serve heterogeneous clients. In non-i.i.d. scenarios, the traditional FedAvg method [34] is susceptible to client drift [19], resulting in slow convergence and poor local client accuracy [28, 27]. To tackle this challenge, [27, 1, 21] proposed the use of regularized local objectives to reduce the bias on the global model after local training. Another approach focuses on rectifying the bias of local updates [19, 10] through techniques such as control variates. Other strategies include loss-balancing [15, 47, 3], knowledge distillation [30, 54], prototype learning [43], and contrastive learning [25]. Despite promising results on non-i.i.d. data, their reliance on a single global model poses limitations for highly heterogeneous clients [17]. ", "page_idx": 1}, {"type": "text", "text": "Personalized Federated Learning. In response to the limitations of a single global model, PFL seeks to overcome heterogeneity by learning models tailored to each client. In this framework, methods attempt to strike a balance between being flexible enough to fit the local distribution and relying on global knowledge to prevent over-ftiting on small local datasets. Popular strategies include meta-learning an initialization for client adaptation [16, 9], multi-task learning with local model regularization [41, 26], local and global model interpolation [6], personalized model aggregation [51, 50], client clustering [39, 8], and decoupled representation and classifier learning [5, 29, 35, 48, 3]. Our work focuses on this latter approach, in which the neural network is typically decomposed into the first $L-1$ layers used for feature extraction, and the final classification layer. ", "page_idx": 1}, {"type": "text", "text": "Existing works in this category share feature extraction parameters between clients and rely on client classifiers for personalization. These approaches differ primarily in the acquisition of client classifiers during training, which influences representation learning. For example, FedRep [5] sequentially trains a strong local classifier while holding the representation fixed, then updates the representation under the fixed classifier. FedBABU [35] proposes to use fixed dummy classifiers to align client objectives, only fine-tuning the classifier layers after the representation parameters have converged. Similarly, FedRoD [3] aims to train a generic representation model and classifier in tandem via balanced softmax loss, later obtaining personalized classifiers through fine-tuning or hypernetworks. FedPAC [48] adopts the learning algorithm of FedRep, but additionally regularizes the feature space to be similar across clients, before learning a personalized combination of classifiers across clients to improve generalization. However, this collaboration comes with an additional computational overhead that scales with the number of active clients. pFedGP [2] leverages a shared feature extractor as a kernel for client Gaussian processes. Although this approach offers improved sample efficiency, it comes at the cost of increased computational complexity and reliance on an inducing points set. ", "page_idx": 1}, {"type": "image", "img_path": "Wl2optQcng/tmp/06225e5b5d3254053d74fd332f79a3e217bae2d112811bed517cbc8dc2991294.jpg", "img_caption": ["Figure 1: Overview of pFedFDA. (Left) Heterogeneous clients collaboratively train representation parameters under a generative classifier derived from a global estimate of class feature distributions. (Right) At test time, clients adapt the generative classifier to their feature distributions to obtain personalized classifiers. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "In a similar spirit to our method, FedEM [33] estimates the latent data distribution of clients in parallel to training classification models. FedEM estimates each client data distribution as a mixture of latent distributions, where personalized models are a weighted average of mixture-specific models. Notably, this introduces a significant overhead in both communication and computation as separate models are trained for each mixture. In contrast, our work estimates the distribution of client features in parallel to training a global representation model. ", "page_idx": 2}, {"type": "text", "text": "3 Problem Formulation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "FL System and Objective. We consider an $\\mathrm{FL}$ system where a parameter server coordinates with $M$ clients to train personalized models $\\theta_{i}$ , $i=1,2,\\cdots,M$ . Each client $i$ has a local training dataset $\\mathcal{D}_{i}=\\{(x_{i}^{j},y_{i}^{j})\\}_{j=1}^{n_{i}}$ , where $x\\in\\mathbb{R}^{m}$ and $y\\in\\{1,\\cdot\\cdot\\cdot,C\\}$ . The model training objective in PFL is: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta_{1},\\dots,\\theta_{M}\\in\\mathcal{Q}}\\;f(\\theta_{1},...,\\theta_{M}):=\\frac{1}{M}\\sum_{i=1}^{M}F_{i}(\\theta_{i}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathcal{Q}$ is feasible set of model parameters, $F_{i}(\\theta_{i})=\\mathbb{E}_{(x,y)\\sim\\mathcal{D}_{i}}[L(\\theta_{i}(x),y)]$ is the empirical risk of dataset $\\mathcal{D}_{i}$ , and $L$ is a loss function of the prediction errors (e.g., cross-entropy). The client population $M$ in FL can be large, resulting in partial client participation [17]. Let $q$ denote the participation rate, meaning that in each round, a client participates in model training with probability $q$ . ", "page_idx": 3}, {"type": "text", "text": "Following [5, 48], we approach this as a problem of global representation learning and local classification, in which each $\\theta_{i}$ consists of a shared backbone, $\\phi$ , responsible for extracting low-level features $\\mathit{\\check{z}}\\in\\mathbb{R}^{d}=\\phi(\\mathit{x}))$ , and a local classifier, $h_{i}$ , for learning a client-specific mapping between features and labels. Considering this decomposition of parameters $\\theta_{i}=(h_{i}\\circ\\phi)$ , we can rewrite the original PFL objective as the following: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\phi\\in\\Phi}\\frac{1}{M}\\sum_{i=1}^{M}\\operatorname*{min}_{h_{i}\\in\\mathcal{H}}F_{i}(h_{i}\\circ\\phi),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\Phi$ and $\\mathcal{H}$ are the feasible sets of neural network and classifier parameters, respectively. ", "page_idx": 3}, {"type": "text", "text": "In our generative modeling framework, we consider $\\mathcal{H}$ to be the probability simplex over $\\{1,\\cdot\\cdot\\cdot,C\\}$ , and our algorithm uses approximations of the posterior distributions as classifiers $h_{i}$ . However, for fair comparison with existing work (as well as other nice properties, discussed in Section 4.1), we select a generative model of the feature space such that $h_{i}$ can be represented with an equivalent linear layer. ", "page_idx": 3}, {"type": "text", "text": "Data Heterogeneity. The data distribution of each client $i$ is a joint distribution on $\\mathcal X\\times\\mathcal X$ , which can be written as $p_{i}(x,y)$ , $p_{i}(y)p_{i}(x|y)$ , or $p_{i}(x)p_{i}(y|x)$ . Using the terminology of [17], we refer to each case of data heterogeneity as follows: prior probability shift $(p_{i}(y)\\,\\neq\\,p_{i^{\\prime}}(y))$ , concept drift $(p_{i}(x|y)\\neq p_{i^{\\prime}}(x|y))$ , covariate shift $(p_{i}(x)\\neq p_{i^{\\prime}}(x))$ , and concept shift $(p_{i}(y|x)\\neq p_{i^{\\prime}}(y|x))$ . Furthermore, the local dataset volumes $\\mathcal{D}_{i}$ may have quantity skew, i.e., $n_{i}\\neq n_{i^{\\prime}}$ . ", "page_idx": 3}, {"type": "text", "text": "4 Methodology ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we introduce pFedFDA, a personalized federated learning method that utilizes a generative modeling approach to guide global representation learning and adapt to local client distributions. We present our method using a classconditional Gaussian model of the feature space, with additional discussion of the selected probability density in Section 4.1. ", "page_idx": 3}, {"type": "text", "text": "Algorithm 1 describes the workflow of pFedFDA. ", "page_idx": 3}, {"type": "text", "text": "Our algorithm begins with a careful initialization of parameters for the feature extractor $\\phi$ , Gaussian means $\\pmb{\\mu}=\\{\\mu^{c}\\}_{c=1}^{C}$ , and covariance $\\Sigma$ (Lines 1-2). We initialize $\\phi$ with established techniques (e.g., [12]) such that the output features follow a Gaussian distribution with controlled variance. We similarly use a spherical Gaussian to ensure a stable initialization of the corresponding generative classifier (see Section 4.1). ", "page_idx": 3}, {"type": "table", "img_path": "Wl2optQcng/tmp/a915c22647d20c1e33f72e0408f76c57f3c2da85cf44e270b62858b0914c4a90.jpg", "table_caption": [], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "At the start of each $\\mathrm{FL}$ round $r$ , the server broadcasts the current $\\phi_{g}^{r},\\pmb{\\mu}_{g}^{r},\\Sigma_{g}^{r}$ to each participating client. The local training of each client consists of two key components: (1) global representation learning, in which clients train $\\phi$ to maximize the likelihood of local features under the global feature distribution $\\pmb{\\mu}_{g}^{r}$ , $\\Sigma_{g}^{r}$ (Line 7); (2) local distribution adaptation, in which clients obtain robust estimates of their local feature distribution $\\pmb{\\mu}_{i}^{r},\\Sigma_{i}^{r}$ , using techniques for efficient low-sample Gaussian estimation (Line 8) and local-global parameter interpolation (Lines 9-11). After local training, clients send their $\\phi_{i}^{r},\\pmb{\\mu}_{i}^{r},\\Sigma_{i}^{r}$ to the parameter server for aggregation (Line 14). ", "page_idx": 3}, {"type": "text", "text": "In the following sections, we provide detailed explanations of each algorithmic component. In Section 4.1 we discuss the beneftis of a generative modeling framework and provide the justification for our selected class-conditional Gaussian model. We outline how the resulting generative classifier can be used to guide representation learning in Section 4.2 and describe how we obtain personalized generative classifiers in Section 4.3. ", "page_idx": 4}, {"type": "text", "text": "4.1 Generative Model of Feature Distributions ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Motivation for Generative Classifiers. A central theme in FL is exploiting inter-client knowledge to train more generalizable models than any client could attain using only their local dataset. This presents an important bias-variance trade-off, as incorporating global knowledge naively can introduce significant bias. Fortunately, under a generative modeling approach, this bias can be naturally handled, enabling efficient inter-client collaboration. ", "page_idx": 4}, {"type": "text", "text": "First note that local class priors $p_{i}(y)$ can be approximated with local counts: $p_{i}(y\\;=\\;c)\\;\\approx\\;$ $\\frac{n_{i}^{c}}{\\sum_{c^{\\prime}\\in C}n_{i}^{c^{\\prime}}}\\,:=\\,\\pi_{i}^{c}$ , where $n_{i}^{c}$ is the number of local samples whose labels are $c$ . This leaves the primary source of bias to the mismatch between local and global feature distributions $p_{g}(z|y)$ and $p_{i}(z|y)$ . Crucially, it turns out that this bias is controllable due to the dependence of $z$ on global representation parameters $\\phi$ . Consequentially, we propose to minimize this bias through our classification objective, which we discuss further in Section 4.2. ", "page_idx": 4}, {"type": "text", "text": "Class-Conditional Gaussian Model. In this work we approximate the distribution of latent representations using a class-conditional Gaussian with tied covariance, i.e., $p_{i}(z|y=c)=\\mathcal{N}(z|\\mu_{i}^{c},\\bar{\\Sigma}_{i})$ . We show the resulting generative classifier under this model in Eq. 4. Note that it has a closed form and results in a decision boundary that is linear in $z$ . I.e., if we know the underlying local feature distribution mean and covariance, we can efficiently compute the optimal header parameters $h_{i}$ for the inner objective in Eq. 2. ", "page_idx": 4}, {"type": "text", "text": "In addition to the convenient form of the Bayes classifier, we select this distribution as the Gaussianity of latent representations is likely to hold in practice. Notably, by adopting the common technique of Gaussian weight initialization (e.g., [12]), the resulting feature space is highly Gaussian at the start of training. It has also been observed that the standard supervised training of neural networks with cross-entropy objectives results in a feature space that is well approximated by a class-conditional Gaussian distribution [24], i.e., the corresponding generative classifier Eq. 4 has equal accuracy to the learned discriminative classifier. We provide a further discussion of this modeling assumption in Appendix A. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\quad p(y=c|z)=\\cfrac{\\mathcal{N}(z|\\mu^{c},\\Sigma)p(y=c)}{\\sum_{c^{\\prime}\\in\\mathcal{C}}\\mathcal{N}(z|\\mu^{c^{\\prime}},\\Sigma)p(y=c^{\\prime})}\\mathrm{,}}&\\\\ &{\\log p(y=c|z)\\propto z^{\\top}\\Sigma^{-1}\\mu^{c}-\\cfrac{1}{2}(\\mu^{c})^{\\top}\\Sigma^{-1}\\mu^{c}+\\log p(y=c).}&\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "4.2 Global Representation Learning ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Next, we describe our process for training the shared feature extractor $\\phi$ . Similar to existing works [5, 48], our local training consists of training $\\phi$ via gradient descent to minimize the cross-entropy loss of predictions from fixed client classifiers. We obtain our client classifiers through Eq. 4, using global estimates of $\\pmb{\\mu}_{g},\\pmb{\\Sigma}_{g}$ and local estimated priors $\\pi_{i}$ . For computational efficiency, we avoid inverting the covariance matrix by estimating $\\Sigma^{-1}\\mu^{c}$ with the least-squares solution $w=\\mathrm{min}_{w^{\\prime}}\\,\\|\\Sigma w^{\\prime}-\\mu^{c}\\|$ . ", "page_idx": 4}, {"type": "text", "text": "The loss of client $i$ for an individual training sample $(x,y)$ is provided in Eq. 5. ", "page_idx": 4}, {"type": "equation", "text": "$$\nL(x,y;\\phi,\\pmb{\\mu},\\Sigma,\\pi)=\\sum_{c=1}^{C}y^{c}\\log p(y^{c}|\\phi(x),\\mu^{c},\\Sigma,\\pi).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Note that for a spherical Gaussian $\\Sigma=\\mathbf{I}$ and uniform prior $\\pi$ , we recover a nearest-mean classifier under Euclidean distance. This resembles the established global prototype regularization [43], which minimizes the Euclidean distance of features from their corresponding global class prototypes. Notably, FedPAC [48] uses this prototype loss to align client features. However, this implicitly assumes that all feature dimensions have equal variance, and additionally requires a hyperparameter $\\lambda$ to balance the amount of regularization with the primary objective. In contrast, our generative classifier naturally aligns the distribution of client features by training $\\phi$ with our global generative classifier. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "4.3 Local Distribution Adaptation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Local Estimation. A key component of pFedFDA is the estimation of local feature distribution parameters, used both for model personalization and for updating the global distribution for representation learning. ", "page_idx": 5}, {"type": "text", "text": "Given a set of $n$ extracted features $Z$ with $n^{c}$ examples per class $c$ , a maximum likelihood estimate of the class means and an unbiased estimator of the covariance, respectively, are given by: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\widehat{\\mu}^{c}=\\frac{1}{n^{c}}\\sum_{j=1}^{n}\\mathbf{1}_{\\left\\{y_{j}=c\\right\\}}z_{j}\\qquad\\qquad(6)\\qquad\\qquad\\qquad\\widehat{\\Sigma}=\\frac{1}{n-1}\\bar{Z}^{\\top}\\bar{Z},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where, with slight abuse of notation, $\\bar{Z}\\in\\mathbb{R}^{n\\times d}$ denotes the matrix of centered features with rows corresponding to each original feature $z_{j}$ centered by their respective means, i.e., ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\bar{z}_{j}=z_{j}-\\sum_{c\\in C}\\mathbf{1}_{\\left\\{y_{j}=c\\right\\}}\\widehat{\\mu}^{c}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Estimators Eq. 6 and Eq. 7 may be noisy on clients with limited local data. To illustrate this, consider the common practical scenario where $n_{i}\\ll d$ . The feature covariance matrix $\\Sigma_{i}$ at client $i$ will be degenerate; in fact, it will have a multitude of zero eigenvalues. In these cases, we can add a small diagonal $\\epsilon\\mathbf{I}$ to $\\Sigma$ , and replace the non-positive-definite matrices with the nearest positive definite matrix with identical variance. This can be efficiently computed by clipping eigenvalues in the corresponding correlation matrix and followed by converting it back to a covariance matrix with normalization to maintain the initial variance. We refer readers to [11] for a review of low-sample covariance estimation. ", "page_idx": 5}, {"type": "text", "text": "Local-Global Interpolation. We introduce this fusion because even with the aforementioned correction to ill-defined covariances, the variance of the local estimates remains highly noisy, indicating the necessity of leveraging global knowledge. It is essential to consider that in the presence of data heterogeneity, clients with differing local data distributions and dataset sizes have varying requirements for global knowledge. ", "page_idx": 5}, {"type": "text", "text": "For our Gaussian parameters $\\pmb{\\mu},\\pmb{\\Sigma}$ , we consider the introduction of global knowledge through a personalized interpolation between local and global estimates, which can be viewed as a form of prior. We provide an analysis of the high-probability bound on estimation error for an interpolated mean estimate in simple settings in Theorem 1. The full derivation is deferred to Appendix E. ", "page_idx": 5}, {"type": "text", "text": "Theorem 1 (Bias-Variance Trade-Off). Let $C=1$ . Define $\\mu_{i}$ as the sample mean of client i\u2019s local features $\\begin{array}{r}{\\mu_{i}:={\\frac{1}{n_{i}}}\\sum_{j=1}^{n_{i}}z_{i}^{j}}\\end{array}$ , and $\\mu_{g}$ as the global sample mean using all $N$ samples across $M$ clients: $\\begin{array}{r}{\\mu_{g}:=\\frac{1}{N}\\sum_{i=1}^{M}\\sum_{j=1}^{n_{i}}z_{i}^{j}}\\end{array}$ . Assume client features are independent and distributed as $z_{i}\\sim\\mathcal{N}(\\theta_{i},\\Sigma_{i})$ with true global feature distribution ${\\cal N}(\\theta_{g},\\Sigma_{g})$ . We consider the use of global knowledge at client $i$ through an interpolated estimate: $\\widehat{\\mu}_{i}:=\\bar{\\beta}\\mu_{i}+(1-\\beta)\\mu_{g}.$ , where $\\beta\\in[0,1]$ . For any $\\delta\\in(0,1)$ , with probability at least $1-\\delta$ , it holds   that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\Vert\\widehat{\\mu}_{i}-\\theta_{i}\\right\\Vert_{2}^{2}\\leq\\left(1-\\beta\\right)^{2}\\left\\Vert\\theta_{g}-\\theta_{i}\\right\\Vert_{2}^{2}}\\\\ &{\\qquad\\qquad+\\left[1+4\\left(\\sqrt{\\frac{\\log{1/\\delta}}{c}}+\\frac{\\log{1/\\delta}}{c}\\right)\\right]\\left(\\frac{2\\beta}{n_{i}}\\operatorname{Tr}(\\Sigma_{i})+\\frac{(1-\\beta)^{2}}{N}\\operatorname{Tr}(\\Sigma_{g})\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $c>0$ is an absolute constant. ", "page_idx": 5}, {"type": "text", "text": "Intuitively, the estimation error and optimal $\\beta$ depend on the bias introduced by using global knowledge $\\bar{\\lVert{\\boldsymbol{\\theta}_{g}}-{\\boldsymbol{\\theta}_{i}}\\rVert_{2}^{2}}$ , the variance of local and global features, and the respective data volumes. ", "page_idx": 5}, {"type": "text", "text": "We formulate this as an optimization problem, in which clients estimate interpolation coefficients $\\beta_{i}$ to combine local and global estimates of $(\\pmb{\\mu},\\Sigma)$ with minimal $k$ -fold validation loss: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\beta_{i}\\in\\operatorname*{min}_{0\\leq\\beta\\leq1}\\;\\;\\frac{1}{k}\\sum_{k}\\sum_{\\substack{(x,y)\\in\\mathcal{D}_{k}}}L(x,y,\\phi,\\beta^{\\prime}\\widehat{\\mu}_{k}+(1-\\beta^{\\prime})\\mu_{g},\\beta^{\\prime}\\widehat{\\Sigma}_{k}+(1-\\beta^{\\prime})\\Sigma_{g},\\pi_{i}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathcal{D}_{k}$ is the dataset consisting of the validation samples for the $k$ -th fold, and $(\\widehat{\\mu}_{k},\\widehat{\\Sigma}_{k})$ are the local distribution estimates Eq. 6 and Eq. 7 estimated using the training samples fro m t he $k$ -th fold. In our experiments, we avoid additional forward passes on the local dataset by preemptively storing the feature-label pairs obtained over the latest round of training. ", "page_idx": 6}, {"type": "text", "text": "We solve Eq. 9 using off-the-shelf quasi-newton methods (e.g., L-BFGS-B). We additionally explore using separate $\\beta$ terms for the means and covariance (Section 5.3) and recommend the use of a single $\\beta$ term for most applications. ", "page_idx": 6}, {"type": "text", "text": "After obtaining $\\beta$ , we set our local estimates of $\\pmb{\\mu}_{i},\\pmb{\\Sigma}_{i}$ to their interpolated versions. These estimates are then sent to the server for aggregation. Notably, the server update rule can be viewed as a moving average [52] between the previous round estimate and the client average scaled by $\\beta$ , reducing the influence of local noise in the global distribution estimate. At test time, clients use their local distribution estimates for inference through the classification rule in Eq. 4. ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "5.1 Experimental Setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Datasets, Tasks, and Models: We consider image classification tasks and evaluate our method on four popular datasets. The EMNIST [4] dataset is for 62-class handwriting image classification. The CIFAR10/CIFAR100 [22] datasets are for 10 and 10-class color image classification. The TinyImageNet [23] dataset is for 200-class natural image classification. For EMNIST and CIFAR10/100 datasets, we adopt the 4-layer and 5-layer CNNs used in [48]. On the larger TinyImageNet dataset, we use the ResNet18 [13] architecture. Notably, the feature dimension $d$ for EMNIST/CIFAR CNNs is 128, and 512 for ResNet. We provide additional details in Appendix C.1. ", "page_idx": 6}, {"type": "text", "text": "Clients and Dataset Partitions: The EMNIST dataset has inherent covariate shifts due to the individual styles of each writer. We partition the dataset by writer following [6], and train with $M=1000$ total clients (writers), participating with rate $q=0.03$ . On CIFAR and TinyImageNet datasets, we simulate prior probability shift and quantity skew by partitioning the dataset according to a Dirichlet distribution with parameters $\\alpha\\in(0.1,0.5)$ , where lower $\\alpha$ indicates higher levels of heterogeneity. On these datasets, we use $M=100$ clients with participation rate $q=0.3$ . Additional details of the partitioning strategy are provided in Appendix C.1.2. ", "page_idx": 6}, {"type": "text", "text": "We split each client\u2019s data partition $80{-}20\\%$ between training and testing. ", "page_idx": 6}, {"type": "text", "text": "Covariate Shift and Data Scarcity: We introduce two modifications to client partitions to simulate the challenges of real-world cross-device FL. We first consider common sources of input noise for natural images, which may result from the qualities of the measuring devices (e.g., camera calibration, lens blur) or environmental factors (e.g., weather, lighting). To simulate this, we select ten image corruptions at five levels of severity defined in [14], and corrupt the training and testing samples of the first 50 clients in CIFAR10/100 with unique corruption-severity pairs. We leave the remaining 50 client datasets unchanged. We refer to these datasets with natural covariate shifts as CIFAR10-S/CIFAR100-S and detail the specific corruptions in Appendix C.1.1. ", "page_idx": 6}, {"type": "text", "text": "Second, we perform uniform subsampling of client training sets, leaving them with $75\\%$ , $50\\%$ , or $25\\%$ ) of their original samples. These low-sample settings are more realistic for cross-device FL, where clients rely more on knowledge sharing. ", "page_idx": 6}, {"type": "text", "text": "Baselines and Metrics: We compare pFedFDA to the following baselines: Local, in which each client trains its model in isolation; FedAvg [34] and FedAvg with fine-tuning (FedAvgFT); APFL [6]; Ditto [26]; pFedMe [41]; FedRoD [3]; FedBABU [35]; FedPAC [48]; FedRep [5]; and LG-FedAvg [29]. We report the average and standard deviation of client test accuracies. ", "page_idx": 6}, {"type": "text", "text": "Model Training: We train all algorithms with mini-batch SGD for $E=5$ local epochs and $R=200$ global rounds. We apply no data augmentation besides normalization into the range $[-1,1]$ . For pFedFDA, we use $k=2$ cross-validation folds to estimate a single $\\beta_{i}$ term for each client. Additional training details and hyperparameters for each baseline method are provided in Appendix C.2. ", "page_idx": 6}, {"type": "text", "text": "5.2 Numerical Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Performance under covariate shift and data scarcity. We first present our evaluation under natural client covariate shift with varying data scarcity in Table 1. In all experiments, pFedFDA outperforms the other methods in test accuracy, demonstrating the effectiveness of our method in adapting to heterogeneous client distributions. Additionally, pFedFDA has an increasing benefti relative to other methods in data-scarce settings: on CIFAR10, we improve $4.2\\%$ over the second-best method with $100\\%$ of training samples and $6.9\\%$ with $25\\%$ . On CIFAR100, the same improvements range from $0.1\\%$ to $6.5\\%$ . This indicates the success of our method in navigating the bias-variance trade-off. ", "page_idx": 6}, {"type": "table", "img_path": "Wl2optQcng/tmp/4747274fde7e5277e02501e37a212640bd0401d372e49dd10f9894dc23a9a9d0.jpg", "table_caption": ["Table 1: Average (standard deviation) test accuracy on CIFAR10/100-S for varying proportions of training data. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "Wl2optQcng/tmp/e7e4095f3f2fdeae7fac5b81463c71f3a0918736b2760bc3919f0d3914a4cbd2.jpg", "table_caption": ["Table 2: Average (standard deviation) test accuracy on multiple datasets. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Evaluation in more moderate scenarios. Our evaluation of all four datasets in the traditional setting (no added covariate shift, full training data) is presented in Table 2. We note that: (1) our method is still competitive, always ranking within the top 3 methods, and (2) the gap between top methods is smaller than in the previous experimental setting. For example, on EMNIST/CIFAR10, we see that FedAvgFT, FedPAC, and pFedFDA are within ${\\sim}1\\%$ accuracy. We observe larger performance gaps for CIFAR100, with FedPAC and pFedFDA having the best results. ", "page_idx": 7}, {"type": "text", "text": "Results under extreme data scarcity. We present additional results at the limits of data scarcity on CIFAR10/100 datasets in Table 3, where we assign a single mini-batch (50) of training examples to each client. Notably, even as $n_{i}\\ll d$ , which poses a challenge to local covariance estimation, pFedFDA clients obtain the best test accuracy, indicating the robustness of our local-global adaptation. ", "page_idx": 7}, {"type": "text", "text": "Generalization to new clients. We further analyze the ability of our generative classifiers to generalize on clients unseen at training time. To simulate this setting, we first train the server model model using half of the client population. We then evaluate each method on the set of clients not encountered throughout training, using their original input data, as well as their dataset transformed using each corruption from CIFAR-S. Further benchmark details, including fine-tuning (personalization) procedures, are provided in Appendix C.3. As demonstrated in Table 4, our method generalizes well even on clients with covariate shifts not encountered at training time. Moreover, observe that pFedFDA has the highest accuracy on the original clients, highlighting the efficacy of structured generative classifiers when less training data is available (i.e., having 50 rather than 100 clients). ", "page_idx": 7}, {"type": "table", "img_path": "Wl2optQcng/tmp/975f3fb1af6de8c7d05b8f548ab44163489b73eadde0147fb9f07f00c2407fec.jpg", "table_caption": ["Table 3: Results under extreme data scarcity on CIFAR10/CIFAR100 Dir(0.5). "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "Wl2optQcng/tmp/aac67ed8039cd6e3d4ad0bb172bb5294b8ec06e20f20d1625e29ef9df96b0c39.jpg", "table_caption": ["Table 4: Evaluation of new-client generalization on CIFAR10 Dir(0.5). "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "5.3 Ablation of Method Components ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We conduct two studies to verify the efficacy of our local-global interpolation method. In Table 5, we see that our interpolated estimates always perform better than using only local data, indicating the benefits of harnessing global knowledge. Learning separate $\\beta$ terms for the means and covariance may be beneficial in low-sample or covariate-shift settings when the local distribution estimate may fluctuate further from the global estimate. However, using a single scalar $\\beta$ appears sufficient and comes with the lowest computational cost (associated with the time to solve Eq. 9). ", "page_idx": 8}, {"type": "text", "text": "Table 5: Ablation study on CIFAR100 with $\\operatorname{Dir}(0.1)$ partition. NB denotes clients using only local data to estimate their feature distribution $\\begin{array}{r}{\\dot{\\rho}_{i}=1\\ }\\end{array}$ ). SB denotes each client estimating a single $\\beta_{i}$ for both the means and covariance, MB denotes clients computing $\\beta_{i}$ terms for the means and covariance separately. We show the average computational overhead across all settings. ", "page_idx": 8}, {"type": "table", "img_path": "Wl2optQcng/tmp/5ff54d79a28fed60cdb323dd487957c7aa7ee76279600a21a2d7b5b1dec69f38.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "We additionally visualize the spread of learned $\\beta$ across clients as a function of their dataset corruption in Fig. 2. As expected, clients with clean datasets rely more on global knowledge (smaller $\\beta$ values) than corrupted clients. Moreover, corruptions with higher $\\beta$ values (e.g., contrast) often align with the more difficult corruptions encountered in Table 4. ", "page_idx": 8}, {"type": "text", "text": "5.4 Communication and Computation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The parameter count and relative communication load of our generative classifiers compared to a simple linear classifier varies depending on class count $C$ and feature dimension $d$ . In our experimental configurations (datasets, architectures), the overhead in total parameter count ranges from $1.1\\%$ to $6.8\\%$ . See Appendix D.3 for additional details. ", "page_idx": 8}, {"type": "text", "text": "In Table 6, we compare the local training time (client-side computation) and total runtime of pFedFDA to baseline methods on CIFAR10. We observe a slight increase in training time relative to FedAvg, which can be attributed primarily to cost of learning our parameter interpolation coefficient $\\beta$ . However, this increase is comparable to the existing methods and is lower than representationlearning methods FedRep and FedPAC. This demonstrates the relative efficiency of our generative classifier formulation in comparison to classifiers obtained through local fine-tuning. ", "page_idx": 8}, {"type": "image", "img_path": "Wl2optQcng/tmp/fa1efc94e5179cc3ab90e8c7d7430bc0da642b0f3e836690793e0e362323fcfd.jpg", "img_caption": ["Figure 2: Comparison of client $\\beta$ and local dataset corruption on CIFAR10-S. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "table", "img_path": "Wl2optQcng/tmp/e8b87c70564444bd1a0c5af36a316024edec702c934c8d4c32a3db32fb0daef9.jpg", "table_caption": ["Table 6: Comparison of system runtime on the CIFAR10 dataset. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Balancing local model flexibility and generalization remains a central challenge in personalized federated learning (PFL). This paper introduces pFedFDA, a novel approach that addresses the biasvariance trade-off in client personalization through representation learning with generative classifiers. Our extensive evaluation on computer vision tasks demonstrates that pFedFDA significantly outperforms current state-of-the-art methods in challenging settings characterized by covariate shift and data scarcity. Furthermore, our approach remains competitive in more general settings, showcasing its robustness and adaptability. The promising results underline the potential of our method to improve personalized model performance in real-world federated learning applications. Future work will focus on exploring the scalability of pFedFDA and its application to other domains. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We gratefully acknowledge the support from the National Science Foundation CAREER award under Grant No. 2340482, the Army Research Laboratory under Cooperative Agreement Number W911NF-23-2-0014, the Sony Faculty Innovation Award, and the National Defense & Engineering Graduate (NDSEG) Fellowship Program. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Army Research Laboratory, the National Science Foundation, or the U.S. government. The U.S. government is authorized to reproduce and distribute reprints for government purposes notwithstanding any copyright notation herein. We also thank Ming Xiang for valuable discussions and feedback on this work. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Durmus Alp Emre Acar, Yue Zhao, Ramon Matas, Matthew Mattina, Paul Whatmough, and Venkatesh Saligrama. Federated learning based on dynamic regularization. In International Conference on Learning Representations, 2021. ", "page_idx": 9}, {"type": "text", "text": "[2] Idan Achituve, Aviv Shamsian, Aviv Navon, Gal Chechik, and Ethan Fetaya. Personalized federated learning with gaussian processes. Advances in Neural Information Processing Systems, 34:8392\u20138406, 2021.   \n[3] Hong-You Chen and Wei-Lun Chao. On bridging generic and personalized federated learning for image classification. In International Conference on Learning Representations, 2022.   \n[4] Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andr\u00e9 van Schaik. Emnist: an extension of mnist to handwritten letters, 2017.   \n[5] Liam Collins, Hamed Hassani, Aryan Mokhtari, and Sanjay Shakkottai. Exploiting shared representations for personalized federated learning. In International Conference on Machine Learning, pages 2089\u20132099. PMLR, 2021.   \n[6] Yuyang Deng, Mohammad Mahdi Kamani, and Mehrdad Mahdavi. Adaptive personalized federated learning. arXiv preprint arXiv:2003.13461, 2020.   \n[7] Zhaoyang Du, Celimuge Wu, Tsutomu Yoshinaga, Kok-Lim Alvin Yau, Yusheng Ji, and Jie Li. Federated learning for vehicular internet of things: Recent advances and open issues. IEEE Open Journal of the Computer Society, 1:45\u201361, 2020.   \n[8] Moming Duan, Duo Liu, Xinyuan Ji, Yu Wu, Liang Liang, Xianzhang Chen, Yujuan Tan, and Ao Ren. Flexible clustered federated learning for client-level data distribution shift. IEEE Transactions on Parallel & Distributed Systems, 33(11):2661\u20132674, November 2022.   \n[9] Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar. Personalized federated learning with theoretical guarantees: A model-agnostic meta-learning approach. Advances in Neural Information Processing Systems, 33:3557\u20133568, 2020.   \n[10] Liang Gao, Huazhu Fu, Li Li, Yingwen Chen, Ming Xu, and Cheng-Zhong Xu. Feddc: Federated learning with non-iid data via local drift decoupling and correction. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10112\u201310121, 2022.   \n[11] Yaqian Guo, Trevor Hastie, and Robert Tibshirani. Regularized linear discriminant analysis and its application in microarrays. Biostatistics (Oxford, England), 8:86\u2013100, 02 2007.   \n[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE International Conference on Computer Vision, pages 1026\u20131034, 2015.   \n[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770\u2013778, 2016.   \n[14] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. In International Conference on Learning Representations, 2019.   \n[15] Tzu-Ming Harry Hsu, Hang Qi, and Matthew Brown. Federated visual classification with real-world data distribution. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part X 16, pages 76\u201392. Springer, 2020.   \n[16] Yihan Jiang, Jakub Konec\u02c7ny\\`, Keith Rush, and Sreeram Kannan. Improving federated learning personalization via model agnostic meta learning. arXiv preprint arXiv:1909.12488, 2019.   \n[17] Peter Kairouz, H Brendan McMahan, Brendan Avent, Aur\u00e9lien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and open problems in federated learning. Foundations and Trends\u00ae in Machine Learning, 14(1\u20132):1\u2013210, 2021.   \n[18] Georgios Kaissis, Alexander Ziller, Jonathan Passerat-Palmbach, Th\u00e9o Ryffel, Dmitrii Usynin, Andrew Trask, Ion\u00e9sio Lima, Jason Mancuso, Friederike Jungmann, Marc-Matthias Steinborn, Andreas Saleh, Marcus Makowski, Daniel Rueckert, and Rickmer Braren. End-to-end privacy preserving deep learning on multi-institutional medical imaging. Nature Machine Intelligence, 3(6):473\u2013484, Jun 2021.   \n[19] Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In International Conference on Machine Learning, pages 5132\u20135143. PMLR, 2020.   \n[20] Ahmed Khaled, Konstantin Mishchenko, and Peter Richt\u00e1rik. Tighter theory for local sgd on identical and heterogeneous data. In International Conference on Artificial Intelligence and Statistics, pages 4519\u20134529. PMLR, 2020.   \n[21] Jinkyu Kim, Geeho Kim, and Bohyung Han. Multi-level branched regularization for federated learning. In International Conference on Machine Learning, pages 11058\u201311073. PMLR, 2022.   \n[22] Alex Krizhevsky. Learning multiple layers of features from tiny images. University of Toronto, 05 2012.   \n[23] Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge. CS 231N, 7(7):3, 2015.   \n[24] Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework for detecting out-of-distribution samples and adversarial attacks. Advances in Neural Information Processing Systems, 31, 2018.   \n[25] Qinbin Li, Bingsheng He, and Dawn Song. Model-contrastive federated learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10713\u201310722, 2021.   \n[26] Tian Li, Shengyuan Hu, Ahmad Beirami, and Virginia Smith. Ditto: Fair and robust federated learning through personalization. In International Conference on Machine Learning, pages 6357\u20136368. PMLR, 2021.   \n[27] Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated optimization in heterogeneous networks. Proceedings of Machine learning and systems, 2:429\u2013450, 2020.   \n[28] Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the convergence of fedavg on non-iid data. In International Conference on Learning Representations, 2020.   \n[29] Paul Pu Liang, Terrance Liu, Liu Ziyin, Nicholas B Allen, Randy P Auerbach, David Brent, Ruslan Salakhutdinov, and Louis-Philippe Morency. Think locally, act globally: Federated learning with local and global representations. arXiv preprint arXiv:2001.01523, 2020.   \n[30] Tao Lin, Lingjing Kong, Sebastian U Stich, and Martin Jaggi. Ensemble distillation for robust model fusion in federated learning. Advances in Neural Information Processing Systems, 33:2351\u20132363, 2020.   \n[31] Ximeng Liu, Lehui Xie, Yaopeng Wang, Jian Zou, Jinbo Xiong, Zuobin Ying, and Athanasios V Vasilakos. Privacy and security issues in deep learning: A survey. IEEE Access, 9:4566\u20134593, 2020.   \n[32] Nathalie Majcherczyk, Nishan Srishankar, and Carlo Pinciroli. Flow-f:l Data-driven federated learning for spatio-temporal predictions in multi-robot systems. In 2021 IEEE International Conference on Robotics and Automation (ICRA), pages 8836\u20138842. IEEE, 2021.   \n[33] Othmane Marfoq, Giovanni Neglia, Aur\u00e9lien Bellet, Laetitia Kameni, and Richard Vidal. Federated multi-task learning under a mixture of distributions. Advances in Neural Information Processing Systems, 34:15434\u201315447, 2021.   \n[34] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In Artificial intelligence and statistics, pages 1273\u20131282. PMLR, 2017.   \n[35] Jaehoon Oh, SangMook Kim, and Se-Young Yun. FedBABU: Toward enhanced representation for federated image classification. In International Conference on Learning Representations, 2022.   \n[36] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in Neural Information Processing Systems, 32, 2019.   \n[37] Kilian Pfeiffer, Martin Rapp, Ramin Khalili, and J\u00f6rg Henkel. Federated learning for computationally constrained heterogeneous devices: A survey. ACM Computing Surveys, 55(14s):1\u201327, July 2023.   \n[38] Swaroop Ramaswamy, Rajiv Mathews, Kanishka Rao, and Fran\u00e7oise Beaufays. Federated learning for emoji prediction in a mobile keyboard. arXiv preprint arXiv:1906.04329, 2019.   \n[39] Felix Sattler, Klaus-Robert M\u00fcller, and Wojciech Samek. Clustered federated learning: Modelagnostic distributed multitask optimization under privacy constraints. IEEE Transactions on Neural Networks and Learning Systems, 32(8):3710\u20133722, 2020.   \n[40] Micah J Sheller, G Anthony Reina, Brandon Edwards, Jason Martin, and Spyridon Bakas. Multi-institutional deep learning modeling without sharing patient data: A feasibility study on brain tumor segmentation. In Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries: 4th International Workshop, BrainLes 2018, Held in Conjunction with MICCAI 2018, Granada, Spain, September 16, 2018, Revised Selected Papers, Part I 4, pages 92\u2013104. Springer, 2019.   \n[41] Canh T Dinh, Nguyen Tran, and Josh Nguyen. Personalized federated learning with moreau envelopes. Advances in Neural Information Processing Systems, 33:21394\u201321405, 2020.   \n[42] Alysa Ziying Tan, Han Yu, Lizhen Cui, and Qiang Yang. Towards personalized federated learning. IEEE Transactions on Neural Networks and Learning Systems, 2022.   \n[43] Yue Tan, Guodong Long, Lu Liu, Tianyi Zhou, Qinghua Lu, Jing Jiang, and Chengqi Zhang. Fedproto: Federated prototype learning across heterogeneous clients. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 8432\u20138440, 2022.   \n[44] Roman Vershynin. High-dimensional probability: An introduction with applications in data science, volume 47. Cambridge university press, 2018.   \n[45] Chunnan Wang, Xiang Chen, Junzhe Wang, and Hongzhi Wang. Atpfl: Automatic trajectory prediction model design under federated learning framework. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 6563\u20136572, June 2022.   \n[46] Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H Vincent Poor. Tackling the objective inconsistency problem in heterogeneous federated optimization. Advances in Neural Information Processing Systems, 33:7611\u20137623, 2020.   \n[47] Lixu Wang, Shichao Xu, Xiao Wang, and Qi Zhu. Addressing class imbalance in federated learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 10165\u201310173, 2021.   \n[48] Jian Xu, Xinyi Tong, and Shao-Lun Huang. Personalized federated learning with feature alignment and classifier collaboration. In The Eleventh International Conference on Learning Representations, 2023.   \n[49] Timothy Yang, Galen Andrew, Hubert Eichner, Haicheng Sun, Wei Li, Nicholas Kong, Daniel Ramage, and Fran\u00e7oise Beaufays. Applied federated learning: Improving google keyboard query suggestions. arXiv preprint arXiv:1812.02903, 2018.   \n[50] Jianqing Zhang, Yang Hua, Hao Wang, Tao Song, Zhengui Xue, Ruhui Ma, and Haibing Guan. Fedala: Adaptive local aggregation for personalized federated learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 11237\u201311244, 2023.   \n[51] Michael Zhang, Karan Sapra, Sanja Fidler, Serena Yeung, and Jose M. Alvarez. Personalized federated learning with first order model optimization. In International Conference on Learning Representations, 2021. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "[52] Sixin Zhang, Anna Choromanska, and Yann LeCun. Deep learning with elastic averaging sgd, 2015. ", "page_idx": 13}, {"type": "text", "text": "[53] Kaiyang Zhou, Yongxin Yang, Timothy Hospedales, and Tao Xiang. Learning to generate novel domains for domain generalization. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XVI 16, pages 561\u2013578. Springer, 2020.   \n[54] Zhuangdi Zhu, Junyuan Hong, and Jiayu Zhou. Data-free knowledge distillation for heterogeneous federated learning. In International Conference on Machine Learning, pages 12878\u201312889. PMLR, 2021. ", "page_idx": 13}, {"type": "text", "text": "A Limitations ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "\u2022 The selected class-conditional Gaussian distribution may not work well for all neural network architectures. For example, if the output features are the result of an activation such as ReLU, a truncated Gaussian distribution may be a better model. Future work can look to exploit knowledge of the neural network architecture to improve the accuracy of the feature distribution estimate. \u2022 In this work, we leverage the insights from a fusion of global and local feature space. As in many applications there is often an underlying cluster structure between clients datasets, future works may explore the identification and efficient estimation of feature distributions of client clusters, in order to reduce the degree of bias introduced in client collaboration. ", "page_idx": 14}, {"type": "text", "text": "B Broader Impacts ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Federated learning has become the main trend for distributed learning in recent years and has deployed in many popular consumer devices such as Apple\u2019s Siri, Google\u2019s GBoard, and Amazon\u2019s Alexa. Our paper addresses the practical limitations of personalization methods in adapting to clients with covariate shifts and/or limited local data, which is a central issue in cross-device FL applications. We are unaware of any potential negative social impacts of our work. ", "page_idx": 14}, {"type": "text", "text": "C Details of Experimental Setup ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "All experiments are implemented in PyTorch 2.1 [36] and were each trained with a single NVIDIA A100 GPU. Compute time per experiment ranges from approximately 2 hours for CIFAR10/100 and 20 hours for TinyImageNet. Code for re-implementing our method is provided at the following GitHub URL: https://github.com/cj-mclaughlin/pFedFDA. ", "page_idx": 14}, {"type": "text", "text": "C.1 Dataset Description ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The EMNIST [4] dataset contains over $730\\small{,}000\\ 28\\!\\times\\!28$ grayscale images of 62 classes of handwritten characters. The CIFAR10/CIFAR100 [22] datasets contain $60\\substack{,}000\\,\\,32\\,{\\times}\\,32$ color images in 10 and 100 different classes of natural images, respectively. TinyImageNet [23] contains $120\\small{,}000\\ 64\\!\\times\\!64$ color images of natural images. ", "page_idx": 14}, {"type": "text", "text": "C.1.1 CIFAR-S Generation. ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We implement the following 10 common image corruptions at 5 levels of severity as described in [14]: Gaussian noise, shot (Poisson) noise, impulse noise, defocus blur, motion blur, fog, brightness, contrast, frost, JPEG compression. We apply a unique corruption-severity pair to all samples of the first 50 clients. ", "page_idx": 14}, {"type": "text", "text": "C.1.2 Non-i.i.d. Partitioning. ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "On CIFAR and TinyImageNet datasets, we distribute the proportion of samples of class $C$ across $M$ clients according to a Dirichlet distribution: $q_{c},m\\sim\\mathsf{D i r}_{M}(\\alpha)$ , where we consider $\\alpha\\in(0.1,0.5)$ as in [30]. ", "page_idx": 14}, {"type": "text", "text": "We provide a visualization of Dirichlet partitioning strategies on CIFAR10 below. The size of each point represents the number of allocated samples. Notably, as $\\alpha$ increases, $\\operatorname{Dir}(\\alpha)$ becomes less heterogeneous. ", "page_idx": 14}, {"type": "text", "text": "C.2 Training Settings ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "All methods are trained using mini-batch SGD for 200 global rounds with 5 local epochs of training. We use a fixed learning rate of 0.01, momentum of 0.5, and weight decay of 5e-4. The batch size is set to 50 for all experiments, except for EMNIST, where we use a batch size of 16. We sample the set of active clients uniformly with probability $\\scriptstyle{\\mathrm{q}}=0.3$ for CIFAR and TinyImageNet and $\\scriptstyle{\\ q=0.03}$ for EMNIST. The last global round of training employs full client participation. We split the data of each client $80{-}20\\%$ between training and testing. ", "page_idx": 14}, {"type": "image", "img_path": "Wl2optQcng/tmp/5239dfe5ba875d1fff6f2b97fc6a197bb53db67d93ceec7894e39b33e02401b7.jpg", "img_caption": ["Figure 3: Comparison of Dirichlet Partitions on CIFAR10. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Hyper-parameters. For APFL, we tune $\\alpha$ over [0.25, 0.5, 0.75, 1.0], and set $\\alpha=0.25$ . For pFedMe, we tune $\\lambda$ over [1.0, 5.0, 10.0, 15.0] and set $\\lambda\\,=\\,5.0$ . For Ditto, we use five local epochs for personalization and tune $\\mu$ over [0.05, 0.1, 0.5, 1.0, 2.0] and set $\\mu=1.0$ . For FedRep and FedBABU, we use five local epochs for training the head parameters. For FedPAC, we tune $\\lambda$ over [0.1, 0.5, 1.0, 5.0, 10.0], and set $\\lambda=1.0$ . FedPAC uses one local epoch for training head parameters with a higher learning rate of 0.1, following the original implementation. ", "page_idx": 15}, {"type": "text", "text": "C.3 Evaluation on New Clients ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Our fine-tuning procedure on new clients largely follows the methodology above. For FedAvgFT, we fine-tune the global model for five local epochs. For FedBABU and FedPAC, we personalize the model in 2 different ways and report the best result: (1) fine-tuning only the head for 5 local epochs, and (2) fine-tuning both the body and head for 5 local epochs. For pFedFDA, each new client estimates their local interpolated statistics (i.e., lines 8-11 of Algorithm 1) to obtain a personalized generative classifier. ", "page_idx": 15}, {"type": "text", "text": "For our covariate shift evaluation, we apply a medium severity corruption (level 3) to all samples. ", "page_idx": 15}, {"type": "text", "text": "D Additional Results ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "D.1 Multi-Domain FL ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In Table 7, we present results on the DIGIT-5 domain generalization benchmark [53]. This presents an alternate form of covariate shift, as the data from each client is drawn from one of 5 datasets (SVHN, USPS, SynthDigits, MNIST-M, and MNIST). In particular, we use 20 clients trained with full participation, and assign 4 clients to each domain. Within each domain, we use the Dirichlet(0.5) partitioning strategy to assign data to each client. We observe that pFedFDA is effective in all settings, but has the most significant benefits over prior work in the low-data regime. ", "page_idx": 15}, {"type": "table", "img_path": "Wl2optQcng/tmp/24a32fed0e8e1b9235318458ee49cb2a5a6e3f20007d82714876f5bee3fc6c36.jpg", "table_caption": ["Table 7: Results on multi-domain DIGIT-5 benchmark for varying data volumes. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "D.2 Effect of Local Epochs ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In many FL settings, we would like clients to perform more local training between rounds to reduce communication costs. However, too much local training can cause the model to diverge. In Fig. 4, we compare the effect of the local amount of epochs for CIFAR100 and CIFAR100-S- $.25\\%$ sample datasets. We observe that (1) pFedFDA outperforms FedAvgFT at all equivalent budgets of $E$ , (2) ", "page_idx": 15}, {"type": "text", "text": "both methods follow exhibit a general plateau in accuracy after $E=5$ , and (3) pFedFDA learns much faster than FedAvgFT, with significantly higher accuracy for $E=1$ . ", "page_idx": 16}, {"type": "image", "img_path": "Wl2optQcng/tmp/c608836f1297fff3c7712ac0aeaeb10465d48d782ed6e0e164fe3fdd8287a225.jpg", "img_caption": ["Figure 4: Comparison of average test accuracy with varying local epochs on CIFAR100. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "D.3 Communication Load Examples ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In Table 8, we compare the number of distinct parameters in our Gaussian estimates to that of a typical linear classifier for the models and datasets used in this paper, along with some additional examples. We display the resulting overhead relative to the base parameter count of the shared representation backbone. ", "page_idx": 16}, {"type": "table", "img_path": "Wl2optQcng/tmp/895e46d4d5092f70a47b3b8e9c5bf9ce0259d6b9e0c3f8f9314ff0e6540e07a1.jpg", "table_caption": ["Table 8: Comparison of communication load (parameters/iter.) between our Gaussian distribution parameters $(\\mu,\\Sigma)$ and standard linear classifiers. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "D.4 Runtime of Method Components ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In Table 9, we evaluate the proportion of each local iteration of pFedFDA associated with each line of our algorithm. Network Passes refers to the time taken to train the base network parameters $\\phi$ (Line 7 of Alg. 1). Mean/Covariance Est. refers to the time taken to estimate the local mean and covariance from features extracted during model training (Line 8 of Alg. 1). Interpolation Optimization refers to the time taken to optimize the local coefficient $\\beta$ (Line 9 of Alg. 1). Overall, we find that the majority of the overhead of our method comes from estimating the interpolation parameter $\\beta$ . ", "page_idx": 16}, {"type": "table", "img_path": "Wl2optQcng/tmp/2cc21142b611cc39bb3767896f2ed5ba2b74f647ef6817506eb4a0c31f12a1a6.jpg", "table_caption": ["Table 9: Percentage (average (std)) of the local training time associated each component of our algorithm. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "E On the Bias-Variance Tradeoff ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "This section justifies the bias-variance tradeoff under some simplified technical assumptions. For simplicity, we assume that at any given round, the extracted feature vectors for a class are independent. We illustrate the bias-variance tradeoff in estimating the mean feature of a given class $c$ at round $t$ . ", "page_idx": 17}, {"type": "text", "text": "Proof of Theorem 1. For ease of exposition, we drop the time index and class index. ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Let $i$ be an arbitrary client with local dataset size $n_{i}$ of class $c$ . Let $N$ be the total data volume of class $c$ over the entire FL system. Assuming that the distribution of client $i$ \u2019s features $z$ follow a multivariate Gaussian distribution ${\\mathcal{N}}(\\theta_{i},\\Sigma_{i})$ , and the global feature distribution follows ${\\mathcal{N}}(\\theta,\\Sigma)$ where $\\begin{array}{r}{\\theta_{g}\\,:=\\,\\sum_{i=1}^{M}n_{i}\\theta_{i}\\big/(\\sum_{i\\in[M]}n_{i})}\\end{array}$ , $\\begin{array}{r}{\\Sigma_{g}\\,:=\\,\\sum_{i=1}^{M}n_{i}^{2}\\Sigma_{i}/(\\sum_{i\\in[M]}n_{i})^{2}}\\end{array}$ . Note $\\theta_{i},\\theta_{g}$ are deterministic parameters. ", "page_idx": 17}, {"type": "text", "text": "We denote the local and global mean estimates as: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mu_{i}:=\\frac{1}{n_{i}}\\sum_{j=1}^{n_{i}}z_{i}^{j},\\;\\;\\mathrm{and}\\;\\;\\mu_{g}:=\\frac{1}{N}\\sum_{i=1}^{M}\\sum_{j=1}^{n_{i}}z_{i}^{j}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Let $\\widehat{\\mu}_{i}$ be the local estimate that interpolates between local and global knowledge, defined as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\widehat{\\mu}_{i}:=\\beta\\mu_{i}+(1-\\beta)\\mu_{g}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We will focus on bounding the high probability local estimation error $\\|\\widehat{\\mu}_{i}-\\mathbb{E}\\left[\\mu_{i}\\right]\\|_{2}$ . Note that Eq.(10) can be further expanded as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{\\mu}_{i}=\\beta\\mu_{i}+(1-\\beta)\\left(\\displaystyle\\frac{n_{i}}{N}\\mu_{i}+\\sum_{i^{\\prime}\\neq i}\\displaystyle\\frac{n_{i^{\\prime}}}{N}\\mu_{i^{\\prime}}\\right)}\\\\ &{\\quad=(\\beta+(1-\\beta)\\displaystyle\\frac{n_{i}}{N})\\mu_{i}+(1-\\beta)\\sum_{i^{\\prime}\\neq i}\\mu_{i^{\\prime}}}\\\\ &{\\quad=\\gamma\\mu_{i}+(1-\\beta)\\bar{\\mu},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\begin{array}{r}{\\gamma:=\\beta+(1-\\beta)\\frac{n_{i}}{N}}\\end{array}$ , and $\\begin{array}{r}{\\bar{\\mu}:=\\frac{1}{N}\\sum_{i^{\\prime}\\neq i}\\sum_{j=1}^{n_{i^{\\prime}}}\\mu_{i^{\\prime}}^{j}}\\end{array}$ . ", "page_idx": 17}, {"type": "text", "text": "Thus $\\mu_{i}\\sim{\\mathcal{N}}(\\theta_{i},{\\frac{1}{n_{i}}}\\Sigma_{i})$ and $\\begin{array}{r}{\\bar{\\mu}\\sim\\mathcal{N}(\\frac{N\\theta_{g}-n_{i}\\theta_{i}}{N},\\frac{N\\Sigma_{g}-n_{i}\\Sigma_{i}}{N^{2}})}\\end{array}$ . Since $\\mu_{i}$ and $\\bar{\\mu}$ are independent, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\widehat{\\mu}_{i}-\\theta_{i}\\;\\sim\\;{\\cal N}\\left((1-\\beta)(\\theta_{g}-\\theta_{i}),\\;\\gamma^{2}\\frac{1}{n_{i}}\\Sigma_{i}+(1-\\beta)^{2}\\frac{N\\Sigma_{g}-n_{i}\\Sigma_{i}}{N^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Let $\\pmb{g}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})$ . We have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\widehat{\\mu}_{i}-\\theta_{i}=(1-\\beta)(\\theta_{g}-\\theta_{i})+\\bar{\\Sigma}^{1/2}{\\pmb g},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\widehat{\\Sigma}^{1/2}$ is the square root matrix of $\\begin{array}{r}{\\widehat{\\Sigma}:=\\gamma^{2}\\frac{1}{n_{i}}\\Sigma_{i}+(1-\\beta)^{2}\\frac{N\\Sigma_{g}-n_{i}\\Sigma_{i}}{N^{2}}}\\end{array}$ . It holds that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\widehat{\\mu}_{i}-\\theta_{i}\\right\\|_{2}^{2}=(1-\\beta)^{2}\\left\\|\\theta_{g}-\\theta_{i}\\right\\|_{2}^{2}+2(1-\\beta)\\left\\langle\\theta_{g}-\\theta_{i},\\widehat{\\Sigma}^{1/2}g\\right\\rangle}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad+\\left\\langle\\widehat{\\Sigma}^{1/2}g,\\widehat{\\Sigma}^{1/2}g\\right\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Taking the expectation with respect to the randomness in the Gaussian random variable $\\textbf{\\textit{g}}$ and by the law of total expectation, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[2(1-\\beta)\\left\\langle\\theta_{g}-\\theta_{i},\\widehat{\\Sigma}^{1/2}g\\right\\rangle\\right]=2(1-\\beta)\\left\\langle\\theta_{g}-\\theta_{i},\\widehat{\\Sigma}^{1/2}\\mathbb{E}\\left[g\\right]\\right\\rangle=0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\langle\\widehat{\\Sigma}^{1/2}\\pmb{g},\\widehat{\\Sigma}^{1/2}\\pmb{g}\\right\\rangle\\right]\\overset{(a)}{=}\\mathbb{E}\\left[\\pmb{g}^{\\top}\\widehat{\\Sigma}\\pmb{g}\\right]}\\\\ &{\\qquad\\qquad\\qquad=\\mathbb{E}\\left[\\pmb{g}^{\\top}\\gamma^{2}\\frac{1}{n_{i}}\\Sigma_{i}\\pmb{g}\\right]+\\mathbb{E}\\left[\\pmb{g}^{\\top}(1-\\beta)^{2}\\frac{N\\Sigma_{g}-n_{i}\\Sigma_{i}}{N^{2}}\\pmb{g}\\right]}\\\\ &{\\qquad\\qquad\\qquad=\\gamma^{2}\\frac{1}{n_{i}}\\operatorname{Tr}(\\Sigma_{i})+(1-\\beta)^{2}\\frac{N\\operatorname{Tr}(\\Sigma_{g})-n_{i}\\operatorname{Tr}(\\Sigma_{i})}{N^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where equality $(a)$ holds because $(\\widehat{\\Sigma}^{1/2})^{\\top}(\\widehat{\\Sigma}^{1/2})=\\widehat{\\Sigma}$ as $\\widehat{\\Sigma}^{1/2}$ is symmetric. ", "page_idx": 18}, {"type": "text", "text": "By Hanson-Wright inequality [44, Theorem 6.2], we conclude that with probability at least $1-\\delta$ (for any given $\\delta\\in(0,1)$ ), ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left\\{\\left.\\begin{array}{l l}{-\\beta^{-1}(\\mu_{t}-\\delta_{t})^{2}}&{=\\beta}\\\\ {+\\frac{\\alpha^{2}\\mu_{t}}{\\gamma}\\frac{1}{\\mu_{t}(1-\\delta_{t})}\\eta\\nabla\\alpha(\\mathbf{x})+(-\\delta_{t})^{2}\\frac{1}{\\mu_{t}}\\nabla\\alpha(\\mathbf{x})\\right\\}}\\\\ &{+\\mathbb{E}\\left\\{\\frac{\\mu_{t}}{\\mu_{t}}\\frac{\\partial_{t}(\\mathbf{x})}{\\partial x}-\\frac{\\alpha^{2}\\mu_{t}}{\\gamma}\\right\\}\\eta\\nabla\\alpha(\\mathbf{x})+(-\\delta_{t})^{2}\\frac{1}{\\mu_{t}}\\nabla\\alpha(\\mathbf{x})\\Bigg\\}=\\Bigg\\{\\sqrt{\\frac{\\mu_{t}(\\delta_{t})}{\\gamma}}\\frac{1}{\\mu_{t}(1-\\delta_{t})}\\frac{1}{\\mu_{t}(1-\\delta_{t})}}\\\\ &{+\\mathbb{E}\\left\\{\\frac{\\mu_{t}}{\\mu_{t}}\\frac{\\partial_{t}(\\mathbf{x})}{\\partial x}-\\frac{\\alpha^{2}\\mu_{t}}{\\gamma}\\right\\}\\eta\\nabla\\alpha(\\mathbf{x})}\\\\ &{+\\frac{\\beta^{-1}}{\\mu_{t}}\\partial_{t}\\mathbf{x}-\\frac{\\alpha^{2}\\mu_{t}}{\\gamma}\\eta\\nabla\\alpha(\\mathbf{x})+\\frac{(1-\\delta_{t})^{2}}{\\mu_{t}}\\nabla\\alpha(\\mathbf{x})}\\\\ &{+\\frac{4}{\\mu_{t}}\\Bigg\\}\\Bigg\\{\\frac{\\delta_{t}}{\\mu_{t}}\\frac{\\partial_{t}(\\mathbf{x})}{\\partial x}+(1-\\delta_{t})^{2}\\frac{1}{\\mu_{t}(1-\\delta_{t})}\\Bigg\\}=\\Bigg\\{\\frac{\\sqrt{\\mu_{t}(\\delta_{t})}}{\\gamma}\\frac{1}{\\mu_{t}(1-\\delta_{t})}\\frac{1}{\\mu_{t}(1-\\delta_{t})}}\\\\ &{\\Bigg\\}=\\Bigg\\{1/\\frac{\\delta_{t}}{\\mu_{t}}-\\delta_{t}\\Bigg\\}^{2\\mu_{t}}\\nabla\\alpha(\\mathbf{x})+\\frac{(1-\\delta_{t})^{2}}{\\mu_{t}}\\nabla\\alpha(\\mathbf{x})\\Bigg\\}=\\Bigg\\{\\frac{\\sqrt{\\mu_{t}(\\delta_{t} \n$$is some absolute constant, inequality holds as , and , inequality ", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $c>0$   \n$(c)$ holds because of triangular inequality $\\|A+B\\|_{\\mathrm{F}}\\,\\le\\,\\|A\\|_{\\mathrm{F}}+\\|B\\|_{\\mathrm{F}}$ , that $\\|A\\|_{\\mathrm{F}}\\,=\\,\\sqrt{\\|A\\|_{\\mathrm{F}}^{2}}\\,=$ ${\\sqrt{\\operatorname{Tr}\\left(A^{\\top}A\\right)}}={\\sqrt{\\operatorname{Tr}(A^{2})}}$ if matrix $A$ is symmetric, and that $\\operatorname*{max}\\{a,b\\}\\leq a+b$ , inequality $(d)$ holds because $\\operatorname{Tr}(A^{2})\\leq(\\operatorname{Tr}(A))^{2}$ for positive semidefinite matrix $A$ and that $\\operatorname{Tr}(\\Sigma_{i})$ , $\\beta^{2}\\geq0$ , and $\\mathrm{Tr}(\\Sigma_{g})$ are by definition non-negative. ", "page_idx": 18}, {"type": "text", "text": "The first term $\\left(1-\\beta\\right)^{2}\\left\\|\\theta_{g}-\\theta_{i}\\right\\|_{2}^{2}$ is the bias introduced when client $i$ uses global knowledge; the smaller the $\\beta$ , the more bias introduced. The last term reveals the interaction of $\\beta$ and the tradeoff between local and global variance. When $\\beta$ approaches 0, we have the global feature variance $\\operatorname{Tr}(\\Sigma)$ reduced by the average of $N$ global samples. When $\\beta$ approaches 1, we have local feature variance $\\mathrm{Tr}(\\Sigma_{i})$ reduced by the average of only $n_{i}$ local data. Thus the bias-variance tradeoff on client $i$ crucially depends on the degree of local-global distribution shift, $\\lVert{\\boldsymbol{\\theta}}_{g}-{\\boldsymbol{\\theta}}_{i}\\rVert_{2}^{2}$ , the local data volume $n_{i}$ and its quality (i.e., $\\Sigma_{i}$ ), and the volume and quality of the data across clients $N,\\Sigma_{g}$ . ", "page_idx": 18}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: The scope of the paper is on an important topic of client model personalization in federated learning. We faithfully state our contributions in both the abstract and introduction. ", "page_idx": 19}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We discuss the limitations and considerations of our Gaussian modelling of the feature space both in the main text and with additional notes in the appendix. While the focus of the paper is in improving client personalization in the challenging setting of data scarcity and client distribution shift, we additionally benchmark our method in more general settings to demonstrate the widespread applicability of our work. Finally, we provide an assessment of the communication and computation overhead of our method compared to state-of-art approaches. ", "page_idx": 19}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We provide the key assumptions in the main text. The missing proof is deferred to Appendix E. ", "page_idx": 19}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We provide detailed experimental setups and review the hyperparameters in Section 5.4 and Appendix C.2. We additionally provide code and instructions to train our method. ", "page_idx": 19}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: Our evaluations are based on open-accessed datasets that are publically available. An official implementation code is provided in the supplementary materials. ", "page_idx": 19}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We provide detailed experimental setups and review the hyperparameters in Section 5.4 and Appendix C.2. We additionally provide code and instructions to train our method. ", "page_idx": 19}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: For our main experiments we include the standard deviation of client accuracies, and include std error bars in our ablation visualization of the method components. ", "page_idx": 20}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: Please find the software/hardware specifications in Appendix C. ", "page_idx": 20}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The NeurIPS code of ethics is strictly enforced throughout our research. ", "page_idx": 20}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We have discussed the broader impacts of our work in Appendix B. Please find details therein. ", "page_idx": 20}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: The paper poses no such risks ", "page_idx": 20}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The existing assets used in this paper has been adequately cited or credited to. ", "page_idx": 20}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: Our attached code is well documented and comes with a README file indicating how reviewers may set up our experiments and train the proposed method. ", "page_idx": 20}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 20}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 20}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects ", "page_idx": 21}]