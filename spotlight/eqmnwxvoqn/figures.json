[{"figure_path": "eqMNwXvOqn/figures/figures_1_1.jpg", "caption": "Figure 1: A workflow of MKGL (from bottom to top). The instruction to the LLM includes a dictionary exemplifying the entity ei and relation rk. The task is to construct new KG sentences initialized with eirk. The tokenizer first tokenizes the input text, where the entities and relations are represented as special tokens out of the original vocabulary. (a) To process these special tokens, MKGL collects the embeddings of their constituting text tokens; (b) Then, a retriever performs a 4-step process to aggregate textual and relational information into KGL token embeddings. The first and the last steps are LoRA-like down-scaling and up-scaling operations [12]; (c) The output is assigned as the embeddings of these special KGL tokens; (d) Similar to the context retriever, we design a score retriever to retriever the score information. (f) The output is in a form of probability distribution among candidate entities.", "description": "This figure illustrates the workflow of the proposed MKGL method.  It starts from the bottom and goes upward.  The input is an instruction to the LLM that includes a dictionary defining entity and relation tokens in the KGL language. The goal is to generate new KG sentences starting with an entity and relation. The process involves tokenization, embedding aggregation (using a retriever for textual and relational information, including LoRA-based scaling), and score retrieval to produce a probability distribution of candidate entities for completing the three-word sentence. ", "section": "3 Mastery of KG Language"}, {"figure_path": "eqMNwXvOqn/figures/figures_4_1.jpg", "caption": "Figure 2: Illustration of LoRA-based KGL Context Retriever. (a) The token embeddings are first scaled down to lower-dimensional vectors; (b) For each input KGL token, their constituting textual token embeddings are aggregated by a PNA encoder; (c) The output embeddings are further aggregated by multi-layered PNA encoders to retrieve neighboring information within KG; (e) The final embeddings are assigned to the KGL tokens.", "description": "This figure illustrates the LoRA-based KGL Context Retriever, a crucial component of the MKGL model. It shows a step-by-step process of how textual and relational information from a Knowledge Graph (KG) is incorporated into the KGL token embeddings. First, the dimensionality of the LLM token embeddings is reduced using a down-scaling operation.  Then, for each KGL token, its constituent textual embeddings are aggregated using a PNA (Principal Neighbourhood Aggregation) encoder. Next, a multi-layered PNA encoder aggregates the KG's relational information and the previously generated embedding to improve the embedding's contextual understanding. Finally, the resulting embedding is assigned to the KGL token. This process efficiently leverages information from the KG to enhance the LLM's understanding of KGL tokens.", "section": "3.4 LORA-based KGL Context Retriever"}, {"figure_path": "eqMNwXvOqn/figures/figures_7_1.jpg", "caption": "Figure 1: A workflow of MKGL (from bottom to top). The instruction to the LLM includes a dictionary exemplifying the entity ei and relation rk. The task is to construct new KG sentences initialized with eirk. The tokenizer first tokenizes the input text, where the entities and relations are represented as special tokens out of the original vocabulary. (a) To process these special tokens, MKGL collects the embeddings of their constituting text tokens; (b) Then, a retriever performs a 4-step process to aggregate textual and relational information into KGL token embeddings. The first and the last steps are LoRA-like down-scaling and up-scaling operations [12]; (c) The output is assigned as the embeddings of these special KGL tokens; (d) Similar to the context retriever, we design a score retriever to retriever the score information. (f) The output is in a form of probability distribution among candidate entities.", "description": "This figure illustrates the workflow of the proposed MKGL model.  It shows how the model takes an initial entity and relation as input, uses a tokenizer and retriever to generate KGL token embeddings, and then uses a score retriever to produce a probability distribution over candidate entities to complete the three-word sentence. The process includes steps for collecting text tokens, retrieving context vectors, assigning KGL token embeddings, and retrieving score estimations.", "section": "3 Mastery of KG Language"}, {"figure_path": "eqMNwXvOqn/figures/figures_18_1.jpg", "caption": "Figure 2: Illustration of LoRA-based KGL Context Retriever. (a) The token embeddings are first scaled down to lower-dimensional vectors; (b) For each input KGL token, their constituting textual token embeddings are aggregated by a PNA encoder; (c) The output embeddings are further aggregated by multi-layered PNA encoders to retrieve neighboring information within KG; (e) The final embeddings are assigned to the KGL tokens.", "description": "This figure illustrates the LoRA-based KGL Context Retriever, which aggregates textual and KG information into KGL token embeddings. It involves four steps: (a) scaling down token embeddings, (b) aggregating textual embeddings using a PNA encoder, (c) further aggregation of output embeddings using multi-layered PNA encoders for retrieving KG information, and (e) assigning final embeddings to KGL tokens.", "section": "3.4 LORA-based KGL Context Retriever"}, {"figure_path": "eqMNwXvOqn/figures/figures_19_1.jpg", "caption": "Figure 6: The performance of MKGL on FB15k-237 and WN18RR, with respect to the layer number of the retrievers.", "description": "This figure shows the performance of the proposed MKGL model on two benchmark datasets, FB15k-237 and WN18RR, when varying the number of layers in the KGL retrievers. The x-axis represents the number of layers, while the y-axis shows the performance metrics: MRR, Hits@1, and Hits@10.  The results indicate how the model's performance changes with different depths of the retrieval modules.  The plot helps in understanding the optimal number of layers for balancing performance and computational cost.", "section": "F.3 Different Layer Numbers"}, {"figure_path": "eqMNwXvOqn/figures/figures_19_2.jpg", "caption": "Figure 7: The performance of MKGL on FB15k-237 and WN18RR, with respect to different encoders in the retrievers.", "description": "This figure compares the performance of the MKGL model using three different encoders (GAT, Mean, and PNA) in its retrievers on two benchmark knowledge graph completion datasets: FB15k-237 and WN18RR.  The results are shown for three evaluation metrics: MRR (Mean Reciprocal Rank), Hits@1 (percentage of correctly ranked top-1 entities), and Hits@10 (percentage of correctly ranked top-10 entities).  The figure illustrates the relative effectiveness of each encoder in aggregating textual and relational information for the KGL token embeddings within the MKGL framework.", "section": "F.4 Different Encoders"}]