[{"figure_path": "Pox8jNQOo5/figures/figures_5_1.jpg", "caption": "Figure 1: Sparsely supervised Lorenz attractor. (A) In each trial, a 32-step snippet (colored line) of the Lorenz manifold attractor (grey) is chosen at random. The RNN is initialized in the starting state (blue dot), and the final state (green dot) is the only supervision label provided during training. (B) Learning curves for Adam, SOFO, FGD [4] and a first-order version of SOFO that uses the same number of tangents [28]. It is possible to get a smoother learning curve for Adam but at the cost of much slower convergence. (C) Example trajectory produced by the RNN after training with SOFO. In this task, SOFO is only 35% slower than Adam in wallclock time per iteration (not shown).", "description": "This figure shows the results of training a recurrent neural network (RNN) to learn the dynamics of a Lorenz attractor using only sparse supervision. Panel A illustrates the training paradigm, where the RNN is initialized at a random point on the Lorenz manifold and only the final state of a 32-step trajectory is used as a target. Panel B compares the learning curves of Adam, SOFO, a first-order gradient method (FGD), and a first-order version of SOFO on this task. SOFO shows faster convergence and a lower minimum loss than Adam. Panel C presents an example trajectory generated by the trained RNN, demonstrating its ability to accurately reproduce the Lorenz attractor dynamics.", "section": "4.1 Inferring dynamics from sporadic observations"}, {"figure_path": "Pox8jNQOo5/figures/figures_6_1.jpg", "caption": "Figure 2: Learning an adaptive Kalman filter (KF). (A) Task structure, where xt denotes the latent state and yt its momentary observation. Each trial (T = 5000 steps) is randomly partitioned into successive contexts (of potentially different durations; Appendix E.2). Each context is characterized by a different, randomly parameterized LDS, which produces latent state trajectories noisily observed by the RNN; context switches are uncued. At each step, the RNN must predict the current state. (B) Training curves for Adam and SOFO, compared to the MSE noise floor provided by KF knowing the current context (green, average over 1000 trials), and a baseline showing KF performance given random (and thus wrong) LDS parameters sampled from the task distribution (Kalman random; orange, average over 1000 trials). (C) Within-trial evolution of the mean-squared prediction error for the current context in the trained models, as a function of time elapsed since the last context switch, averaged over 1500 contexts (barely visible shaded areas = \u00b12 s.e.m.). Green and orange baselines: same as in (B).", "description": "This figure shows the results of training RNNs to perform adaptive Kalman filtering in a non-stationary environment.  Panel A illustrates the task structure, showing an RNN receiving noisy observations from a linear dynamical system (LDS) whose parameters change over time. Panel B compares the training loss curves for Adam and SOFO optimizers against baselines. Panel C shows the mean squared prediction error over time after a context switch, demonstrating that SOFO leads to faster learning and better performance.", "section": "4.2 Learning an adaptive Kalman filter"}, {"figure_path": "Pox8jNQOo5/figures/figures_6_2.jpg", "caption": "Figure 3: 3-bit flip-flop task. (A) Network architecture for the classical 3-bit flip-flop task: three input channels sporadically provide random bits (\u00b11) at random time intervals and each corresponding output channel must hold its activity at the value given by the last provided bit (see B). The network feeds back its own output via random feedback weights (grey). Only the output weights (black) are trained. (B) Example outputs for the network trained with SOFO (dashed lines); tight match to target outputs (solid pale) shows successful training. Solid dark lines show the corresponding input bits. (C) Evaluation loss for Adam (black), SOFO (red) and FORCE (green), for a network of size 1000 and for SOFO small (purple) where the network size is reduced to 128 but the all sets of weights (i.e. recurrent weights, biases etc) are trainable. (D) Same as C, for varying number of neurons from 1000 to 8000 (dark to pale).", "description": "Figure 3 demonstrates the performance of SOFO on the 3-bit flip-flop task, comparing it against Adam and FORCE.  Panel A shows the network architecture. Panel B shows example outputs illustrating successful training with SOFO.  Panels C and D display loss curves across various network sizes, demonstrating SOFO's superior performance and ability to handle smaller networks.", "section": "4.3 3-bit flip-flop task"}, {"figure_path": "Pox8jNQOo5/figures/figures_7_1.jpg", "caption": "Figure 4: Memory-guided single-reach task. (A) System schematics and task structure (see text). (B) Training curves for Adam and SOFO (mean \u00b1 2 s.e.m. over 3 independent runs). (C) Wallclock time per training iteration. (D-F) Example single-neuron firing rates (D), single-trial hand trajectories (E) and shoulder torques (F) for eight selected reach conditions (color-coded). Gray areas denote preparation epochs.", "description": "This figure shows the results of applying SOFO and Adam to a single-reach task.  Panel A provides a schematic of the experimental setup, illustrating a recurrent neural network controlling a simulated two-jointed arm.  Panel B displays learning curves, demonstrating that SOFO converges significantly faster than Adam. Panel C shows the wall-clock time per iteration, highlighting SOFO's efficiency. Panels D-F present example data from a successful SOFO training run: (D) shows single-neuron peristimulus time histograms (PSTHs) for eight different reach conditions. (E) displays single-trial hand trajectories, and (F) shows the shoulder torques during a single trial.  Gray shaded areas in (D) and (F) indicate the preparation phase, while the reach phase is marked in (E).", "section": "4.4 Motor tasks"}, {"figure_path": "Pox8jNQOo5/figures/figures_8_1.jpg", "caption": "Figure 5: Memory-guided double-reaching task. (A) Task structure (see text). (B) Training curves for Adam and SOFO. (C) Example single-trial hand trajectories for selected pairs of consecutive reach targets. (D) Example time series of shoulder torques.", "description": "This figure demonstrates the results of applying SOFO and Adam to a memory-guided double-reaching task. Panel A shows a schematic of the task setup, where a robotic arm must reach two targets in sequence after a preparation period. Panel B presents the training curves showing SOFO's superior performance over Adam. Panel C shows example hand trajectories generated by both optimizers highlighting SOFO's better accuracy and smoother movements. Finally, panel D displays shoulder torques to illustrate the difference in motor control strategies.", "section": "4.4 Motor tasks"}, {"figure_path": "Pox8jNQOo5/figures/figures_9_1.jpg", "caption": "Figure 6: Compute time and memory profiling. (A) GPU memory usage as a function of time horizon used in the Kalman filter task (Section 4.2). Gray shading indicate the 12GB memory limit of an RTX 2080 Ti. (B) Wallclock time per iteration (mean \u00b1 s.e.m.), for K = 256, T = 5000, and a batch size of 256. GPU memory usage is taken from nvtop.", "description": "This figure shows the results of an experiment comparing the memory usage and wall-clock time per iteration of the Adam and SOFO optimizers. The experiment used the Kalman filter task from Section 4.2 of the paper with different time horizons (T). The figure shows that the SOFO optimizer is more memory-efficient than Adam and has comparable wall-clock time per iteration.", "section": "4 Results"}, {"figure_path": "Pox8jNQOo5/figures/figures_13_1.jpg", "caption": "Figure 7: One-shot classification task. (A) Task structure (see text). (B) One-shot classification accuracy during the course of training.", "description": "Figure 7 shows the results of a one-shot classification task. Panel A illustrates the task structure, which involves a learning phase where the network is exposed to a set of three input/output pairs, followed by an exploitation phase where the network is tested on three new inputs. Panel B displays the learning curve of one-shot classification accuracy for both Adam and SOFO optimizers during training, demonstrating SOFO's superior performance.", "section": "A Additional Experiments"}, {"figure_path": "Pox8jNQOo5/figures/figures_15_1.jpg", "caption": "Figure 9: MNIST classification task. Training and test performance (see titles) for Adam (black) and SOFO (colored) with different numbers of tangents K. Percentages refer to the K/P ratio, i.e. number of tangents K to total parameter count P.", "description": "This figure shows the training and testing performance of Adam and SOFO optimizers on the MNIST classification task.  Different lines represent SOFO with varying numbers of tangents (K), expressed as a percentage of the total number of parameters (P).  The plots show training and testing loss, as well as training and testing accuracy over 200 training iterations.  The figure demonstrates that SOFO, even with a relatively small number of tangents, can achieve comparable or better performance than Adam on this classification task.", "section": "D Additional profiling experiments"}, {"figure_path": "Pox8jNQOo5/figures/figures_16_1.jpg", "caption": "Figure 9: MNIST classification task. Training and test performance (see titles) for Adam (black) and SOFO (colored) with different numbers of tangents K. Percentages refer to the K/P ratio, i.e. number of tangents K to total parameter count P.", "description": "The figure shows the training and test performance of Adam and SOFO on the MNIST classification task with different numbers of tangents K.  The left two panels display training and test loss, while the right two panels show training and test accuracy.  The results show that SOFO outperforms Adam, particularly as the number of tangents (K) increases. The K/P ratio (number of tangents K to total parameter count P) is also shown in the legend, indicating the fraction of parameters updated at each step. This experiment demonstrates the impact of the number of tangents on the performance of SOFO.", "section": "D Additional profiling experiments"}]