[{"type": "text", "text": "Learning Generalized Linear Programming Value Functions ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Tu Anh-Nguyen Google Research and Rice University Houston, TX tu.na@rice.edu ", "page_idx": 0}, {"type": "text", "text": "Joey Huchette   \nGoogle Research   \nCambridge, MA   \njhuchette@google.com ", "page_idx": 0}, {"type": "text", "text": "Christian Tjandraatmadja Google Research Cambridge, MA ctjandra@google.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We develop a theoretically-grounded learning method for the Generalized Linear Programming Value Function (GVF), which models the optimal value of a linear programming (LP) problem as its objective and constraint bounds vary. This function plays a fundamental role in algorithmic techniques for large-scale optimization, particularly in decomposition for two-stage mixed-integer linear programs (MILPs). This paper establishes a structural characterization of the GVF that enables it to be modeled as a particular neural network architecture, which we then use to learn the GVF in a way that benefits from three notable properties. First, our method produces a true under-approximation of the value function with respect to the constraint bounds. Second, the model is input-convex in the constraint bounds, which not only matches the structure of the GVF but also enables the trained model to be efficiently optimized over using LP. Finally, our learning method is unsupervised, meaning that training data generation does not require computing LP optimal values, which can be prohibitively expensive at large scales. We numerically show that our method can approximate the GVF well, even when compared to supervised methods that collect training data by solving an LP for each data point. Furthermore, as an application of our framework, we develop a fast heuristic method for large-scale two-stage MILPs with continuous second-stage variables, via a compact reformulation that can be solved faster than the full model linear relaxation at large scales and orders of magnitude faster than the original model. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The linear programming (LP) value function models the optimal value of an LP as problem data in that problem varies. Value functions are a fundamental abstraction used in many algorithms for largescale optimization. More concretely, many problems where decisions are made sequentially\u2013e.g., two-stage stochastic programs [46], facility location problem [17], multi-commodity problems [19], or network interdiction problems [49]\u2013can be modeled as two-stage mixed-integer linear programs (MILPs). To pick one common technique as a motivating example: Benders\u2019 decomposition is an algorithmic technique that decomposes such a large problem into many smaller ones [21, 42, 51]. At a high level, Benders\u2019 decomposition abstracts each LP subproblem away by replacing it with a value function. If we somehow had a good approximation of these value functions that we could efficiently optimize over, this reformulation would be straightforward to approximately solve. In Benders\u2019 decomposition however, we do not have such a representation a priori, and thus we iteratively construct an approximation of it via cutting planes; this is often the most computationally expensive part of the algorithm [36, 57]. In this paper, we focus on learning such representations. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Separately, it is well-known that neural networks (NNs) are \u201cuniversal approximators\u201d in theory [4, 23, 26, 35] and incredibly adept at modeling complex behaviors in practice [1, 7, 9, 37]. Taken together, we can state two natural questions that motivate this work: ", "page_idx": 1}, {"type": "text", "text": "\u2022 What are the meaningful structural properties of a value function, and what are suitable neural network architectures for encoding these properties?   \n\u2022 How good are these approximations in practice, and how can we leverage them to solve real-world problems? ", "page_idx": 1}, {"type": "text", "text": "Contributions. Our work studies the Generalized Linear Programming Value Function (GVF), defined as the function that models an LP\u2019s optimal value as both its objective and its constraint bounds vary, and shows how machine learning techniques can be used to build practical approximations of this function. In particular, our contributions are as follows. ", "page_idx": 1}, {"type": "text", "text": "1. A GVF Representation theorem. We study the structure of the GVF and show that it can be exactly modeled as a maximum of bilinear functions, where each function is the dot product of two piecewise linear functions that depends only the objective coefficients or constraint bounds, respectively.   \n2. A theoretically-grounded NN architecture for GVF. We present the Dual-Stack Model, a neural network architecture which mimics the structural property of the GVF exposed by our representation theorem.   \n3. An unsupervised learning approach. We show that the GVF can be written as the unique optimal solution of a constrained optimization problem that does not require solving any LPs to write down. We use this as inspiration for an unsupervised learning method that can be implemented using standard NN training constructs and libraries.   \n4. Empirical justification. We present a computational study showing that our unsupervised training approach can perform comparably with supervised training in terms of approximating a GVF, without the expensive data generation phase that supervised training requires.   \n5. A fast heuristic for large-scale two-stage MILPs. Due to the properties of the Dual-Stack Model, we can easily embed it as an LP within a larger optimization problem. As an application of our framework, we leverage this fact to produce a heuristic for two-stage MILPs with continuous second-stage variables, which includes a provable duality gap. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "A Linear Program (LP) is a mathematical optimization problem of the form: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\operatorname*{min}\\{c\\cdot x\\mid A x\\leq b,x\\geq0,x\\in\\mathbb{R}^{n}\\},\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $x$ are the decision variables, $c\\in\\mathbb{R}^{n}$ is the vector of objective coefficients, $A\\in\\mathbb{R}^{m\\times n}$ is the constraint matrix, and $b\\in\\mathbb{R}^{m}$ is the vector of constraint bounds (often called the \u201cright-hand side\" in such a representation). ", "page_idx": 1}, {"type": "text", "text": "Duality is a fundamental concept in linear programming that establishes a relationship between the primal (original) linear program and its dual (related) linear program. This relationship provides insights into the optimal solutions, and it is valuable for both theoretical understanding and practical applications. The dual problem of (1) is ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\{b\\cdot y\\mid A^{T}y\\leq c,y\\leq0,y\\in\\mathbb{R}^{m}\\}.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "The typical LP Value Function (LPVF) is $\\begin{array}{r}{h_{A,c}(\\beta):=\\operatorname*{min}_{x}\\big\\{c\\cdot x\\mid A x\\le\\beta,x\\ge0,x\\in\\mathbb{R}^{n}\\big\\}.}\\end{array}$ . As a corollary of strong duality, the LPVF is a piecewise linear convex function, which is the maximum of a finite number of affine functions. Note that the LPVF only considers varying constraint bounds; if we also permit the objective coefficients to vary, we obtain the Generalized LP Value Function (GVF) [52]. Formally, we define a GVF associated with a fixed constraint matrix $A\\in\\mathbb{R}^{m\\times n}$ as: ", "page_idx": 1}, {"type": "equation", "text": "$$\nh_{A}(\\gamma,\\beta):=\\operatorname*{min}_{x}\\left\\{\\,\\gamma\\cdot x\\mid A x\\leq\\beta,x\\geq0\\right\\}.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Many typical decomposition methods only need to consider LPVFs of fixed objective vectors $c$ . However, learning the entire GVF at once means that we can reuse the same learned model for many different objectives, potentially saving computation and allowing for a broader generalization. ", "page_idx": 2}, {"type": "text", "text": "We use $L P(\\gamma,\\beta)$ to denote the linear program in (3) for fixed values of $\\gamma$ and $\\beta$ . Conventionally, when $L P(\\gamma,\\beta)$ is infeasible, $h(\\gamma,\\beta)=+\\infty$ , and when $L P(\\gamma,\\beta)$ is unbounded from below, we have $h(\\gamma,\\beta)=-\\infty$ . Let $B:=\\{\\beta\\in\\mathbb{R}^{m}\\mid\\exists x\\in\\mathbb{R}^{n}$ s.t $A x\\leq\\beta,x\\geq0\\}$ and ${\\mathcal{C}}:=\\{{\\boldsymbol{\\gamma}}\\in\\mathbb{R}^{n}~|~\\exists y\\in$ $\\mathbb{R}^{m}$ s.t $A^{T}y\\leq\\gamma,y\\leq0\\}$ . By strong duality and the definitions of $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ and $\\mathcal{C}$ : $h(\\gamma,\\beta)$ is finite if and only if $\\gamma\\in{\\mathcal{C}}$ and $\\beta\\in\\mathcal{B}$ . We define $X(\\beta):=\\{x\\in\\mathbb{R}^{m}\\mid A x\\leq\\beta,x\\geq0\\}$ as the set of feasible solutions of $L P(\\cdot,\\beta)$ for a fixed $\\beta\\in\\mathcal B$ . ", "page_idx": 2}, {"type": "text", "text": "In this work, we will consider two-stage MILPs with continuous second-stage variables, where fixing the first-stage variables results in the problem decomposing into $K$ independent LPs. In particular, these have the form: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x^{1},x^{2}}\\left\\{c\\cdot x^{1}+\\sum_{k\\in\\left[K\\right]}d^{k}\\cdot x^{2,k}\\mid x^{1}\\in\\chi,\\,T^{k}x^{1}+A x^{2,k}\\leq b^{k},\\,x^{2,k}\\geq0\\,\\forall k\\in\\left[K\\right]\\right\\},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\boldsymbol{x}^{1}\\in\\mathbb{R}^{n_{1}}$ are the first-stage variables, $\\chi$ is the first-stage feasible set, $K\\in\\mathbb{Z}^{+}$ is the number of second-stage subproblems, and $[\\![K]\\!]$ denotes the set $\\{1,\\ldots,K\\}$ . For example, in the context of stochastic programming, $K$ is the   num ber of scenarios, or in a facility location problem, $K$ is the number of customers. Each subproblem $k$ is associated with corresponding continuous second-stage variables $\\boldsymbol{x}^{2,k}\\in\\mathbb{R}^{n_{2}}$ . We assume that the second-stage constraint matrices are the same, denoted as $A\\in\\mathbb{R}^{m_{2}\\times n_{2}}$ while the constraint matrices of first-stage and the constraint bounds constraints vector can vary among second-stages, denoted as $T^{k}$ and $b^{k}$ , respectively. We can rewrite (4) using GVF as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x^{1}}\\left\\{c\\cdot x^{1}+\\sum_{k\\in\\left\\|K\\right\\|}h_{A}(d^{k},b^{k}-T^{k}x^{1})\\mid x^{1}\\in X\\right\\}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Within the context of GVFs, we can reformulate equation (3) to elide the requirement for second-stage variables. The large number of these variables can impede computational efficiency [18], which motivates a compact representation of GVFs. ", "page_idx": 2}, {"type": "text", "text": "3 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Value function learning for multi-stage problems ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Neural networks are well-known to be powerful \u201cuniversal approximators\u201d [35]. This has motivated a line of research focused on learning value functions, particularly those based on constraint bounds such as LPVF and their MILP analogues, with the main goal of improving methods for two-stage or multi-stage optimization problems. Dai et al. [13], Lee et al. [34], and Bae et al. [5] propose various methods to learn the LPVF with the aim to solve multi-stage stochastic programming problems more quickly. Similar to our work, they use models that are convex on constraint bounds to match the structure of the LPVF. Beyond the LP value function, neural networks have also been used to learn IP value functions to improve the integer L-shaped method [33]. Moreover, to tackle difficult mixed-integer problems with a large number of scenarios, Dumouchelle et al. [14, 15] devise NN architectures that learn MILP value functions of constraint bounds and scenarios. ", "page_idx": 2}, {"type": "text", "text": "Our method differs from the above in that we directly learn the GVF rather than the LPVF (though [14] learns the GVF indirectly), we do not require solving optimization subproblems (LPs in our case) to obtain training data. Furthermore, we aim to not only generalize across second-stage subproblems (scenarios), but also across instances. In particular, by allowing the objective coefficients to vary, we learn a single value function that encompasses all subproblems, rather than learning one per subproblem. Of course, learning a single GVF is generally harder than learning a single LPVF, but a core thesis of this work is that there is underlying structure tying together those many related LPVFs that we can exploit when learning the GVF. ", "page_idx": 2}, {"type": "text", "text": "3.2 Other learning-based approaches ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "A related research direction in learning for stochastic optimization is scenario reduction, which seeks a smaller set of \u201crepresentative scenarios\u201d. Many of these approaches perform some form of clustering to reduce the number of scenarios and then solve a smaller surrogate problem with these scenarios [10, 16, 30, 41, 44]. Wu et al. [55] uses a conditional variational autoencoder to learn scenario embeddings and cluster them. Bengio et al. [8] predicts a representative scenario for a smaller surrogate problem, but it relies on problem structure to build scenarios for training. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Other learning-based methods for tackling two-stage stochastic problems include reinforcement learning for local search [38] and Benders cut classification [28]. More generally, ML-based approaches have also been applied for bilevel optimization [29, 47, 48, 56] where value functions are relevant, though unlike in our case, these only have a single inner optimization subproblem. Finally, there is an extensive stream of work focusing on applying ML to support decisions within MILP solvers, such as branching and cutting plane decisions (e.g., [2, 24, 39, 50]). ", "page_idx": 3}, {"type": "text", "text": "3.3 Computing value functions ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "LP value functions are well-studied (e.g., see [45, Chapter 19]), as they play crucial roles in sensitivity analysis and Benders\u2019 decomposition. On the other hand, GVFs are considerably less well-studied. While much is known about its structure [22, 27], to the best of our knowledge our method is the first that aims to learn it directly based on its theoretical properties. The computation of value functions has also been studied for ILPs and MILPs using superadditive duality [32, 43, 52, 53]. However, they are less tractable to compute and thus more difficult to leverage into a practical algorithm. ", "page_idx": 3}, {"type": "text", "text": "4 A Neural Network Representation for Generalized Linear Programming Value Functions ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we will develop a characterization of the GVF that, in the sequel, we will use as inspiration for a neural network architecture that is well-suited to approximate the GVF. ", "page_idx": 3}, {"type": "text", "text": "4.1 A Characterization of Generalized Linear Programming Value Function ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "It is known that ${\\boldsymbol{\\mathbf{\\mathit{1}}}}\\times{\\boldsymbol{\\mathcal{\\mathit{\\sigma}}}}$ can be partitioned into distinct invariancy regions, within each of which the GVF is bilinear with respect to $\\beta$ and $\\gamma$ [27]. This can be reformulated as the following proposition. ", "page_idx": 3}, {"type": "text", "text": "Proposition 1. Fix a matrix $A\\in\\mathbb{R}^{m\\times n}$ , and define $S(\\beta)$ as the set of all bases of $A$ which are feasible with respect to fixed constraint bounds $\\beta\\,\\in\\,B$ . Then, $\\begin{array}{r}{h_{A}(\\gamma,\\beta)\\,=\\,\\operatorname*{min}_{B\\in S(\\beta)}\\gamma_{B}B^{-1}\\beta}\\end{array}$ . Furthermore, $h_{A}(\\cdot,b)$ is piecewise linear concave for every fixed $b\\in\\mathcal{B}$ and $h_{A}(c,\\cdot)$ is piecewise linear convex for every fixed $c\\in{\\mathcal{C}}$ . ", "page_idx": 3}, {"type": "text", "text": "While Proposition 1 tells us that each invariancy region defined by $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ can be decomposed into a product of functions that depend only on $\\gamma$ and $\\beta$ , it does not provide us with a global decomposition that is valid across all invariancy regions. We now show that there does indeed exist a structured, global decomposition of a GVF in terms of piecewise linear functions that consider either $\\gamma$ or $\\beta$ , but not both. ", "page_idx": 3}, {"type": "text", "text": "Theorem 2. (GVF Representation Theorem) For a fixed matrix $A\\,\\in\\,\\mathbb{R}^{m\\times n}$ , there exists a set of $p$ piecewise linear functions $\\{F_{p}\\,:\\,\\mathbb{R}^{n}\\,\\rightarrow\\,\\mathbb{R}^{K}\\}_{p=1}^{\\dot{P}}$ and a piecewise linear convex function $G:\\mathbb{R}^{m}\\rightarrow\\mathbb{R}^{K}$ such that ", "page_idx": 3}, {"type": "equation", "text": "$$\nh_{A}(\\gamma,\\beta)=\\operatorname*{max}_{p\\in\\mathbb{I}^{P}}\\{F_{p}(\\gamma)^{T}G(\\beta)\\}\\quad\\forall\\gamma\\in\\mathcal{C},\\beta\\in\\mathcal{B}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We refer the reader to Appendix A for a proof of this result. ", "page_idx": 3}, {"type": "text", "text": "4.2 The Dual-Stack Model ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We now use Theorem 2 as inspiration for a neural network architecture that we dub the Dual-Stack Model (DSM). For simplicity, in the remainder we will consider LPs written in the form of (1); see Appendix F for analogous models for other LP representations. ", "page_idx": 3}, {"type": "text", "text": "The architecture of a DSM is depicted in Figure 1. It consists of two stacks of feedforward fullyconnected neural networks of depth $N$ and $M$ corresponding to the objective vector $\\gamma$ and the constraint bounds vector $\\beta$ ; we name them the $\\gamma$ -stack and the $\\beta$ -stack, respectively. Each layer has a piecewise linear activation function to ensure that the entire stack itself is piecewise linear; either ReLU or Max-pooling is a suitable choice. We denote the output matrix of the $\\gamma.$ -stack as $\\Phi$ and the output vector of the $\\beta.$ -stack as $\\Psi$ , respectively. To model the outer maximization in (6), the output of the model is the maximum element of the dot product between $\\Phi$ and $\\Psi$ . Finally, the $\\beta$ -stack is constrained so that the first layer has non-positive weights and each subsequent layer has non-negative weights; this enforces the desired properties of a GVP listed in Theorem 3, such as tchoen vfuexnicttyi oonn $\\beta$ f[r3o].m I nT hgeeonreeraml,  2t.he $\\gamma.$ -stack represents the functions $\\{F_{p}\\}_{p=1}^{Q}$ and the $\\beta.$ -stack models $G$ ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "We can summarize the properties of DSM as follows. Let $a^{i}$ the $i$ -th column of the matrix $A$ . Then we may define $\\mathcal{H}(A):=\\left\\{\\eta_{A}(\\cdot)\\mid\\eta_{A}\\right.$ is a DSM and $\\eta_{A}(\\gamma,a^{i})\\leq\\dot{\\gamma}_{i}\\,\\forall\\gamma\\in$ $c,i\\,\\in\\,[n]\\}$ to be the class of functions that can be represente d b y a DSM, subject to what we might call dual feasibility constraints on their outputs. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3. Any function $\\eta_{A}\\in{\\mathcal{H}}(A)$ has the following properties: ", "page_idx": 4}, {"type": "text", "text": "1. $\\eta_{A}(\\gamma,\\cdot)$ is piecewise linear, convex, and monotonically decreasing for every fixed $\\gamma\\in{\\mathcal{C}}$ .   \n2. $\\eta_{A}(\\cdot,\\beta)$ is piecewise linear for every fixed $\\beta\\in$ $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ .   \n3. $\\eta_{A}(\\gamma,\\beta)\\leq h_{A}(\\gamma,\\beta)$ for every fixed $\\beta\\in\\mathcal{B}$ and $\\gamma\\in{\\mathcal{C}}$ . ", "page_idx": 4}, {"type": "image", "img_path": "Vxijl0IOId/tmp/9b6efd2387d056da48c10522d4b5aa286f2327ed0b9b8f1d1ee68ee4b142b8c9.jpg", "img_caption": ["Figure 1: Dual-Stack Model (DSM) "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "This result shows that, for fixed inputs, $h_{A}$ is upper-bounded by the true GVF. In fact, we can show something even stronger. ", "page_idx": 4}, {"type": "text", "text": "Theorem 4. For any fixed $A\\in\\mathbb{R}^{m\\times n}$ , $h_{A}\\in{\\mathcal{H}}(A)$ , and moreover $h_{A}$ is pointwise larger than all other elements of $\\mathcal{H}(A)$ . ", "page_idx": 4}, {"type": "text", "text": "One way to interpret Theorem 4 is that there exists some DSM architecture whereby we can recover the GVF by setting the weights in such a way that we recover the pointwise maximum across infinitely many points in ${\\boldsymbol{\\mathbf{\\mathit{1}}}}\\times{\\boldsymbol{\\mathcal{\\mathit{\\sigma}}}}$ . We can now sharpen this result to show that it suffices to restrict attention to some finite subset of these points. For any given $\\mathbf{\\mathcal{M}}\\in\\mathbb{Z}_{\\geq0}^{M}$ and $\\mathcal{N}\\in\\mathbb{Z}_{\\ge0}^{N}$ , define $\\mathsf{D S M}(\\mathcal{M},\\mathcal{N})$ to be the class of functions represented by a DSM whose $\\gamma$ -stack and $\\beta$ -stack have layers with $\\mathcal{M}$ and $\\mathcal{N}$ neurons each, respectively. ", "page_idx": 4}, {"type": "text", "text": "Theorem 5. There exists some $\\mathcal{M}\\in\\mathbb{Z}_{+}^{M}$ , some ${\\mathcal N}\\in\\ensuremath{\\mathbb{Z}_{+}^{N}}$ , $a$ finite set $\\bar{\\mathcal{C}}\\subsetneq\\mathcal{C}$ , a finite set ${\\bar{\\boldsymbol{B}}}\\subsetneq{\\boldsymbol{B}}$ , and some $\\eta^{\\prime}\\in D S M(\\mathcal{M},\\mathcal{N})$ such that $\\eta^{\\prime}(\\gamma,\\beta)=h_{A}(\\gamma,\\beta)$ for all $\\beta\\in\\bar{\\mathcal{B}},\\gamma\\in\\bar{\\mathcal{C}}$ . Moreover, this same $\\eta^{\\prime}$ necessarily satisfies $\\eta^{\\prime}(\\gamma,\\beta)=h_{A}(\\gamma,\\beta)$ for all $\\beta\\in\\mathcal B$ and $\\gamma\\in{\\mathcal{C}}$ . ", "page_idx": 4}, {"type": "text", "text": "Finally, we reframe this existential result as an optimization problem; this will form the basis for the unsupervised training framework we develop in Section 5. ", "page_idx": 4}, {"type": "text", "text": "Corollary 6. Take the ${\\mathcal{M}},{\\mathcal{N}},{\\bar{B}},$ , and $\\bar{\\mathcal{C}}$ that Theorem 5 guarantees must exist. Denote the parameters of a DSM model with $\\theta$ . Then, $h_{A}$ is the unique solution of ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{\\substack{\\eta^{\\theta}\\in D S\\scriptscriptstyle{H}(\\mathcal{M},\\mathcal{N})}}{\\operatorname*{max}}\\,\\sum_{\\substack{\\gamma\\in\\bar{\\mathcal{C}}\\,\\beta\\in\\bar{\\mathcal{B}}}}\\eta^{\\theta}(\\gamma,\\beta)}&{}\\\\ {\\qquad\\qquad\\qquad s.t.~\\eta^{\\theta}(\\gamma,a^{i})\\le\\gamma_{i}}&{\\forall\\gamma\\in\\bar{\\mathcal{C}},i\\in[\\![n]\\!].}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We refer the reader to Figure 2 for an illustration of how, taken together, the results of this section permit us to learn a good approximation of the GVP. In addition, we highlight that typical \u201cuniversal approximation theorems\u201d [35] apply over bounded input domains, whereas here $\\mathcal{C}$ and $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ may be unbounded. However, the above approach shows that we can attain $h_{A}$ without this assumption. ", "page_idx": 4}, {"type": "text", "text": "5 Learning Generalized Linear Programming Value Functions ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "For context, we begin by describing a standard supervised training method to approximate the function $h_{A}$ using a neural network with parameters $\\theta$ . First, we generate some training data set $\\mathcal{D}:=\\{(\\gamma^{i},\\beta^{i};h_{A}(\\gamma^{i},\\beta^{i}))\\}_{i=1}^{|\\mathcal{D}|}.$ . Then, we optimize our parameters $\\theta$ by minimizing a loss function (such as an $\\ell_{2}$ -distance) between the output of the NN with our data. Notably, this is an unconstrained optimization problem. However, data generation may be expensive: in general, we must solve an linear programming problem, $L P(\\gamma^{i},\\breve{\\beta^{i}})$ , for every training data point $i\\in[|\\mathcal{D}|]$ . ", "page_idx": 4}, {"type": "image", "img_path": "Vxijl0IOId/tmp/9eadf56ac7d7004a54d5741dc92aab00724f05cf2fedd27d45eec59aa5bc8d26.jpg", "img_caption": ["Figure 2: An illustration of how we learn a GVF, looking at slices along the constraint bounds (Top) and objective coefficients (Bottom). Given only data points $\\bar{\\boldsymbol{B}}\\times\\bar{\\boldsymbol{\\mathcal{C}}}$ and no constraints, either (7b) or weight-sign, maximizing $\\textstyle\\sum_{\\gamma\\in{\\bar{c}}}\\sum_{\\beta\\in{\\bar{B}}}\\eta^{\\theta}(\\gamma,\\stackrel{\\cdot}{\\beta})$ will yield a poor initial approximation. Adding constraints (7b) will force the function to be smaller than $h_{A}$ at certain \u201canchor\u201d points which are distinct from the input data points, at which the function will still tend to be large. Adding the weight-sign constraints will force the function to be convex in terms of the constraint bounds, and will therefore produce an approximation that lower bounds $h_{A}$ . Theorem 5 then tells us that, with sufficiently many data points to start, we will eventually recover $h_{A}$ directly. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "5.1 An Unsupervised Learning Approach ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Corollary 6 tells that we do not actually need labeled data to learn the GVF. Of course, there is a trade-off here: directly applying the result requires us to somehow compute the sets $\\bar{\\boldsymbol{B}}$ and $\\bar{\\mathcal{C}}$ , and also requires solving a constrained optimization problem in (7) (as opposed to a typical unconstrained learning problem). To address the first issue, we propose using two subsets $\\mathcal{D}_{b}\\subsetneq\\mathcal{B}$ and $\\mathcal{D}_{c}\\subsetneq\\mathcal{C}$ to use in lieu of $\\bar{\\boldsymbol{B}}$ and $\\bar{\\mathcal{C}}$ respectively, which may come from training data or be randomly generated; this is further detailed for a specific application in Section 7. To address the second issue, we introduce a penalty term to the objective to model the constraints (7b) in a \u201csoft\" manner. This leaves us with the following unconstrained, unsupervised learning problem: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\eta^{\\theta}\\in\\operatorname{\\mathbb{DSM}}(\\mathcal{M},\\mathcal{N})}\\sum_{\\gamma\\in\\mathcal{D}_{c}}\\sum_{\\beta\\in\\mathcal{D}_{b}}-\\eta^{\\theta}(\\gamma,\\beta)+\\mu\\sum_{\\gamma\\in\\mathcal{D}_{c}}\\sum_{i\\in\\left\\|n\\right\\|}\\operatorname*{max}\\{\\eta^{\\theta}(\\gamma,a^{i})-\\gamma_{i},0\\},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mu\\in\\mathbb{R}_{+}$ is the penalty coefficient. We can motivate both of the alterations introduced above with the following corollary to Theorem 5. To enforce the constraint $\\eta^{\\theta}\\in\\mathsf{D S M}(\\mathcal{M},\\mathcal{N})$ , we take the positive (respectively, negative) absolute value of the weights for a nonnegative (resp. nonpositive) weight-sign constraint. ", "page_idx": 5}, {"type": "text", "text": "Corollary 7. Given any nonempty subsets $\\mathcal{D}_{b}\\subseteq\\mathcal{B}$ and $\\mathcal{D}_{c}\\subseteq\\mathcal{C}_{s}$ , there exists a sufficient large $\\mu$ such that $h_{A}$ is an optimal solution of (8). ", "page_idx": 5}, {"type": "text", "text": "Note that, while (7) has $h_{A}$ as its unique optimal solution, in general we cannot guarantee that (8) has an unique optimal solution, meaning that we may recover an optimal solution that deviates from $h_{A}$ on points outside the training set. However, Corollary 7 ensures that any solution of (8) is at least as good as $h_{A}$ at $(\\gamma,\\beta)\\in\\bar{\\mathcal{D}_{c}}\\times\\mathcal{D}_{b}{}^{1}$ . Moreover, in Section 5.2 we will provide computational evidence that a near-optimal solution of (8) is empirically a good approximation of $h_{A}$ . Corollary 7 also justifies why we elect to use an $\\ell^{1}$ penalty term in (8), rather than a smooth penalty or Lagrangian multipliers. If we were to modify (8) to use an $\\ell^{2}$ penalty term, for example, an analogous version of Corollary 7 need not hold: roughly speaking, $h_{A}$ is the function that satisfies most of (7b) at equality, whereas an $\\ell^{2}$ penalty term would tend to push optimal solutions away from the boundary of the feasible region of (7). We refer the interested reader to [40, Chapter 17] for a detailed discussion of exact-inexact or smooth-nonsmooth penalty terms. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "5.2 Penalty Coefficient Update Strategies ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Now, we use standard techniques from nonlinear optimization [40, Chapter 17] to develop a heuristic scheme for updating our penalty term $\\mu$ , depicted in Algorithm 1. In practice, the optimization problem (7) might be numerically unstable. For example, if we initialize $\\mu_{0}$ to a small value, then the first term $\\dot{\\Sigma_{\\gamma\\in{\\cal D}_{c},\\beta\\in{\\cal D}_{b}}}\\,\\eta_{A}^{\\theta}(\\gamma,\\stackrel{\\cdot}{\\beta})$ is weighed far more than the penalty term, and so the optimal solution will tend towards $-\\infty$ . We resolve this by using upper bounds, $u$ , on $h_{A}$ as \u201ctether\u201d points. For each training data point, we choose a simple upper bound of the optimal objective (see Section 7 and Appendix $\\boldsymbol{\\mathrm E}$ for details). ", "page_idx": 6}, {"type": "text", "text": "We parameterize our scheme on the penalty update strategy, which is perhaps the most important factor. A common choice would be the linear update strategy, which means $\\mu$ is scaled up by a constant factor $\\nu$ at each step, i.e., update $(\\mu)=\\nu\\mu$ . However, we propose a more computationally effective \u201cadaptive update\u201d strategy where we update $\\mu$ based on how many constraints (7b) the current solution $\\eta^{t}$ satisfies. In particular, update $:\\!(\\mu)=(2-$ percentage_cons_satisfied) $\\cdot\\,\\mu$ . ", "page_idx": 6}, {"type": "table", "img_path": "Vxijl0IOId/tmp/78cfe252bd6fef40ac4e9937843effd89f187b4b5cc549521091f064030151d9.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Note that, since the minimization problem in line 4 of Algorithm 1 is non-convex, we cannot guarantee a globally optimal solution $\\eta^{t+1}$ . Therefore, in practice, we solve the minimization subproblem until some criterion is met, e.g., the gradient is sufficiently small or we reach a prescribed iteration limit. ", "page_idx": 6}, {"type": "text", "text": "5.3 Guaranteeing an Under-Approximation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "By Theorem 3, any feasible solution for (7) lower bounds the GVF we are attempting to learn. However, our methods laid out in this section do not guarantee this property for two reasons. First, the penalty method treats the constraints (7b) as soft constraints rather than hard constraints, and thus they may not be fully satisfied. Second, we are approximating the set $\\bar{\\mathcal{C}}$ from Corollary 6 with a training set $\\mathcal{D}_{c}$ , and therefore our solution may not provide a lower bound at other objective vectors $\\gamma$ . To resolve this, we can scale the function down as much as needed to guarantee the constraints (7b), with the expectation that our learning method produces a solution that is not too far off from being dual feasible. However, we can do better if we only need to provide a valid lower bound for a single objective $\\gamma$ , which is often the case (see Section 6 below). In this way, we can train a single DSM approximation and reuse it across many objectives by suitably postprocessing it for each. We describe this procedure in Algorithm 2. ", "page_idx": 6}, {"type": "table", "img_path": "Vxijl0IOId/tmp/933b23263661232f1fd6e7226cb44c49dd6ddc89123e89b8445e97b18a380cbf.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "6 A GVF-Based Heuristic for Two-Stage MILPs ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "As an application of the learning method we developed in Section 5, we propose a heuristic method for two-stage MILPs with continuous second-stage variables (4). The main idea is to replace each of the second-stage subproblems with the corresponding LPVFs from our learned function: that is, we use our approximation to represent $h_{A}(d^{k},b^{k^{\\ast}}-T^{k}\\check{x}^{1})$ in (5) for each $k$ . This yields a fast heuristic for two reasons. First, our learned approximation is piecewise linear convex when restricted to a fixed objective, meaning that it can be efficiently modeled inside a larger optimization problem as an LP [3]. Second, the number of variables of this LP scales with the number of neurons in the DSM, typically much smaller than a second-stage subproblem LP. In practice, this enables the heuristic to run faster than solving the LP relaxation of (5), despite maintaining integrality of the first-stage variables. ", "page_idx": 7}, {"type": "text", "text": "Given a learned DSM representing the function $\\eta_{A}^{\\theta}$ , denote by $W_{\\Psi}^{0},W_{\\Psi}^{1},\\dots,W_{\\Psi}^{M}$ the weights of the constraint bounds stack, where $W_{\\Psi}^{0}\\le0$ and $W_{\\Psi}^{1},\\dots,W_{\\Psi}^{M}\\geq0$ . For a fixed objective coefficient $c\\in{\\mathcal{C}}$ and the output of the objective-stack $\\Phi^{c}$ (post-processed as in Section 5.3), we can model the set $\\{\\zeta\\mid\\zeta\\geq\\eta_{A}^{\\theta}({\\dot{c_{,}}}\\beta)\\}$ as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathsf{2S M}_{\\theta}(c,\\beta)=\\left\\{\\zeta\\,\\bigg|\\,\\exists z\\ \\mathrm{s.t.}\\ z_{1}\\ge\\sigma(W_{\\Psi}^{0}\\beta),\\ z_{i+1}\\ge\\sigma(W_{\\Psi}^{i}z_{i})\\,\\forall i\\in\\left[\\![M]\\!],\\,\\zeta\\ge\\Phi_{i}^{c}\\cdot z_{M}\\,\\forall i\\in\\left[\\![p]\\!]\\right]\\!\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Our heuristic is then to solve the following problem to obtain a solution for the first-stage variables: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x^{1},\\zeta^{1},\\dots,\\zeta^{K}}\\left\\{c\\cdot x^{1}+\\sum_{k\\in\\ensuremath{[\\![K]\\!]}}\\zeta^{k}\\Bigg|\\;\\zeta^{k}\\in\\ensuremath{\\operatorname{DSM}}_{\\theta}(d^{k},b^{k}-T^{k}x^{1})\\;\\forall k\\in\\ensuremath{[\\![K]\\!]},x\\in X\\right\\}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Note that (9) is simply (5) with the LPVFs replaced by our learned model. Since $\\eta_{A}^{\\theta}(c,\\beta)\\leq h_{A}(c,\\beta)$ for all $c\\in\\mathcal{C},\\beta\\in\\mathcal{C}$ , the objective value of (9) is a dual bound for the original problem. Once we compute optimal values of the first-stage variables $x^{*}$ , we can then recover the second-stage variable values by solving each of the second-stage LP subproblems independently with fixed $x^{*}$ , yielding the full solution. Algorithm 3 describes the full method. ", "page_idx": 7}, {"type": "table", "img_path": "Vxijl0IOId/tmp/d2e7ce655411abeabb860ef8fae9a8044039d3809294c8a2f04f3dfdbbf94eb8.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "7 Computational Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we computationally evaluate2 both the approximation quality of the learning method described in Section 5 and the effectiveness of the heuristic for two-stage problems from Section 6. ", "page_idx": 7}, {"type": "text", "text": "We evaluate these methods on the uncapacitated facility location (UFL) [54]. This is a deterministic two-stage problem, in which we first select $n_{f}$ facilities to open, and allocate each of $n_{c}$ customers to an open facilities. We consider two classes of instances, Euclidean and $K G$ , both with $n_{c}=n_{f}$ . In both cases, we take a set of objective and right-hand side vectors from one instance for training and a second, different, set from five instances for testing (more details are provided in Appendix C). ", "page_idx": 7}, {"type": "text", "text": "To produce the training data, we take all or some customer allocation costs from the UFL training instances as our objective coefficient dataset $\\mathcal{D}_{c}$ . Then, for each such cost vector, we generate $\\lfloor n_{f}/10\\rfloor$ points uniformly at random between $[0,1]$ for our constraint bound dataset $\\mathcal{D}_{b}$ . Thus, the total size of the training data is $\\left\\lfloor\\left\\lvert D_{c}\\right\\rvert\\cdot n_{f}/10\\right\\rfloor$ . If all customer costs are selected for $\\mathcal{D}_{c}$ , this is $\\lfloor|{\\mathcal{Z}}|\\cdot n_{c}\\cdot n_{f}/10\\rfloor$ where $\\mathcal{T}$ is the set of training instances. Note that we do not use the facility costs at all for training. To improve training stability, we normalize the customer allocation costs by their mean in the training data. For the training dataset, we choose an upper bound of the GVF to be 2, which is an upper bound for the largest possible assignment cost in the training dataset after normalization. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "To learn each GVF, we run a total of $T=40$ iterations in Algorithm 1, at each iteration, we solve (8) by performing 100 steps of the Adam algorithm [31]. For DSM, we select the model within the $T$ iterations that satisfies at least $98\\%$ of the constraints (7b) from the training dataset with lowest training objective function (8). Details of hyperparameter tuning for DSMs and DenseNets are provided in Appendix E. In addition to the numerical study for UFL, we include experiments on the Stochastic Capacitated Facility Location (SCFL) in Appendix D. ", "page_idx": 8}, {"type": "text", "text": "7.1 Learning Method ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Arguably, we would expect that the lack of supervised data would make the Dual-Stack Model more difficult to train than a standard supervised learning approach, especially considering that it also has training constraints and convexity requirements. On the other hand, our theoretical results suggest that these same requirements can help the model take the general shape of a GVF. Indeed, we observe in Table 1 that our approach produces a model that is comparable or better than a standard ReLU network (see Appendix E for DenseNet) and Random Forest Regressors in terms of how well it approximates the GVF. Unlike with DSM, these baselines require solving an LP for each training point to produce labels. The number of LPs can grow large, though they can be solved in an embarassingly parallel manner, and in the case of UFL we can use an efficient greedy algorithm given that the LP reduces to fractional knapsack. We report an a posteriori metric, the True Relative Error, defined as the gap between the model and the GVF, i.e., $|\\eta(\\gamma,\\beta)-h_{A}(\\gamma,\\beta)|/\\operatorname*{max}\\{\\eta(\\gamma,\\beta),h_{A}(\\gamma,\\beta)\\}$ , averaged across all $\\gamma,\\beta$ in the test set. The Lower Bound in Table 1 shows the percentage of constraints (7b) satisfied for either the training or test set. ", "page_idx": 8}, {"type": "text", "text": "We see that, on all but one instance family, we are able to train a DSM that attains a lower True Relative Error than both DenseNet and Random Forest. Moreover, the training times between DSM and DenseNet are roughly comparable. We also highlight that the Euclidean instances are relatively harder to learn than the KG instances; this is observable for all models. ", "page_idx": 8}, {"type": "table", "img_path": "Vxijl0IOId/tmp/dba913fa18f4949e3473b8917031cb8149d6526b1c2c25e29a4f06af306d1e2a.jpg", "table_caption": ["Table 1: Comparison between Dual-Stack Model and DenseNet in Learning GVF. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 1 examines the capacity of the DSM model to learn GVFs. To illustrate the scalability of DSMs, we train the model on LPs of various sizes. We use the allocation cost vectors of all customers for $\\mathcal{D}_{c}$ in all cases, except for KG 750 where we select 200 customers for $\\mathcal{D}_{c}$ due to memory limitations. Although training time naturally increases with instance size, both the True Relative Error and Lower Bounds remain stable for KG instances and exhibit only a slight increase in the Euclidean cases. This observation supports the scalability of the DSM approach. ", "page_idx": 8}, {"type": "text", "text": "7.2 Heuristic for Two-Stage Problems ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We evaluate our heuristic on our generated UFL instances by comparing its performance against that of a state-of-the-art open-source MILP solver, SCIP [20]. We compare with the best feasible solution found by the solver within the time limit specified in the column \u201cMIP Solve Time (s)\u201d in Table 2 for KG, or the optimal solution for Euclidean instances. For KG, even solving the LP relaxation for the full model can take a few minutes. On the other hand, despite being an MILP, solving the model (9) is very fast, often taking less than a second, particularly because we only require binary constraints on the first-stage variables. After solving this MILP, the second-stage solutions are recovered by simply taking the closest open facility. Denoting the objective value of the heuristic feasible solution by $v^{*}$ and the optimal value of (9) by $V$ \u2014which is a dual bound to the original problem due to our under-approximation guarantee\u2014the Provable Gap is computed as $(v^{*}-\\bar{V})/v^{*}$ , which is an upper bound of the true gap. The Gap to MILP is computed as $(v^{*}-\\bar{v})/v^{*}$ , where $\\bar{v}$ is the objective value of the MILP baseline. A negative value of Gap to MILP means that the solution returned by our heuristic is better than the solution returned by SCIP within the time limit. We solve 5 instances from each class and report the mean and standard deviation for each metric across these selected instances in each class. When reporting time, we use $\\mathbf{\\hat{\\rho}}\\ y_{}=\\mathbf{\\hat{\\rho}}$ to signal the solver reaches a time limit. We also compare our method with a heuristic based on Benders Decomposition, which mimics the DSM heuristic except that we use Benders cuts instead of the GVF. For each instance, we first iteratively generate a number of Benders cuts for all subproblems, and then solve one MILP with the inclusion of all generated optimality cuts. For each UFL instance, we set the maximum number of Benders LP iterations to match the total number of facilities, while the time limit for solving the MILPs with cuts is fixed at 1 minute. In the Euclidean case, the Benders heuristic was able to produce an optimal solution for all instances tested. We observe from Table 2 that our approach produces significantly better solutions than our full model baseline for large KG instances, though not for Euclidean ones. This may be because the Euclidean instances are more difficult to learn as observed in Table 1, both for DSM and the dense model. We also note that the Euclidean instances are much easier to solve to optimality than the KG instances, meaning that a fast heuristic for them is of relatively lesser value. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "table", "img_path": "Vxijl0IOId/tmp/868841a9494cca191a0f4e032177a35082ba62d8f0904f4c52220167ce0166da.jpg", "table_caption": ["Table 2: DSM Heuristic Solver on UFL Instances. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "8 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this study, we provide a structural characterization of the GVF, inspiring an NN architecture and unsupervised method that approximate it well. Additionally, we utilize this framework to develop a fast heuristic method for two-stage MILPs with continuous second-stage variables, effective for some large-scale instances. We will conclude this paper by highlighting two areas for future work, where we believe that the techniques presented in this paper could be sharpened or otherwise improved. ", "page_idx": 9}, {"type": "text", "text": "First, we believe that our training objective could be further improved. During training, we must balance two terms in our loss function: one that rewards fitting the data well, and another one that (softly) constrains the function to be below the true value function. Finding a stable balance between these two terms appears to be one of the most challenging parts of training. We overcome this limitation by proposing an adaptive update method for the penalty, tuning the initial hyperparameter well, and proposing a good stopping criterion. However, even with all these measures, we still observe occasional instability and believe that there is room for improvement, such as developing an adaptive stopping rule. ", "page_idx": 9}, {"type": "text", "text": "A second direction for future work is to improve the generalization of our method to objectives not seen in the training set. While our theoretical results guarantee that the constraints in (7) are sufficient to produce a function that lower bounds the GVF, this may require an exponential number of constraints. In practice, we only enforce this constraint for those objective vectors appearing in the training data, which does give us a lower bounding guarantee for arbitrary right-hand sides and objective vectors from outside the training set. We observe empirically that with sufficient samples, our method performs well on out-of-sample objectives, but we do note that our Test Lower Bound column in Table 1 differs from the Train Lower Bound column by a significant amount. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] A.A.M. Al-Saffar, H. Tao, and M.A. Talab. Review of deep convolution neural network in image classification. In 2017 International conference on radar, antenna, microwave, electronics, and telecommunications (ICRAMET), pages 26\u201331. IEEE, 2017.   \n[2] A. M. Alvarez, Q. Louveaux, and L. Wehenkel. A machine learning-based approximation of strong branching. INFORMS Journal on Computing, 29(1):185\u2013195, 2017.   \n[3] B. Amos, L. Xu, and J. Z. Kolter. Input convex neural networks. In International Conference on Machine Learning, pages 146\u2013155. PMLR, 2017.   \n[4] R. Arora, A. Basu, P. Mianjy, and A. Mukherjee. Understanding deep neural networks with rectified linear units. In International Conference on Learning Representations, 2018.   \n[5] H. Bae, J. Lee, W. C. Kim, and Y. Lee. Deep value function networks for large-scale multistage stochastic programs. In International Conference on Artificial Intelligence and Statistics, pages 11267\u201311287. PMLR, 2023.   \n[6] J. E. Beasley. OR-Library. URL: https://people.brunel.ac.uk/ mastjjb/jeb/info.html, Accessed in May 2024.   \n[7] C. Beck, M. Hutzenthaler, A. Jentzen, and B. Kuckuck. An overview on deep learning-based approximation methods for partial differential equations. Discrete and Continuous Dynamical Systems - B, 28(6):3697\u20133746, 2023.   \n[8] Y. Bengio, E. Frejinger, A. Lodi, R. Patel, and S. Sankaranarayanan. A learning-based algorithm to quickly compute good primal solutions for stochastic integer programs. In International Conference on Integration of Constraint Programming, Artificial Intelligence, and Operations Research (CPAIOR), pages 99\u2013111. Springer, 2020.   \n[9] Y. Bengio, A. Lodi, and A. Prouvost. Machine learning for combinatorial optimization: A methodological tour d\u2019horizon. European Journal of Operational Research, 290(2):405\u2013421, 2021.   \n[10] P. Beraldi and M. E. Bruni. A clustering approach for scenario tree reduction: an application to a stochastic programming portfolio optimization problem. TOP, 22:934\u2013949, 2014.   \n[11] C. E. Blair and R. G. Jeroslow. The value function of an integer program. Mathematical Programming, 23(1):237\u2013273, 1982.   \n[12] W. D. Cook and R. J. Webster. Caratheodory\u2019s Theorem. Canadian Mathematical Bulletin, 15 (2):293\u2013293, 1972.   \n[13] H. Dai, Y. Xue, Z. Syed, D. Schuurmans, and B. Dai. Neural stochastic dual dynamic programming. In International Conference on Learning Representations, 2022.   \n[14] J. Dumouchelle, R. Patel, E. B. Khalil, and M. Bodur. Neur2SP: Neural two-stage stochastic programming. Advances in Neural Information Processing Systems, 35:23992\u201324005, 2022.   \n[15] J. Dumouchelle, E. Julien, J. Kurtz, and E. B. Khalil. Neur2RO: Neural two-stage robust optimization. In International Conference on Learning Representations, 2024.   \n[16] J. Dupac\u02c7ov\u00e1, N. Gr\u00f6we-Kuska, and W. R\u00f6misch. Scenario reduction in stochastic programming. Mathematical programming, 95:493\u2013511, 2003.   \n[17] R. Z. Farahani, M. SteadieSeif,i and N. Asgari. Multiple criteria facility location problems: A survey. Applied Mathematical Modelling, 34(7):1689\u20131709, 2010.   \n[18] M. Fischetti, I. Ljubic\u00b4, and M. Sinnl. Redesigning benders decomposition for large-scale facility location. Management Science, 63(7):2146\u20132162, 2017.   \n[19] L. R. Foulds. A multi-commodity flow network design problem. Transportation Research Part B: Methodological, 15(4):273\u2013283, 1981.   \n[20] G. Gamrath, D. Anderson, K. Bestuzheva, W.-K. Chen, L. Eifler, M. Gasse, P. Gemander, A. Gleixner, L. Gottwald, K. Halbig, G. Hendel, C. Hojny, T. Koch, P. Le Bodic, S. J. Maher, F. Matter, M. Miltenberger, E. M\u00fchmer, B. M\u00fcller, M. E. Pfetsch, F. Schl\u00f6sser, F. Serrano, Y. Shinano, C. Tawfik, S. Vigerske, F. Wegscheider, D. Weninger, and J. Witzig. The SCIP Optimization Suite 7.0. Technical report, Optimization Online, March 2020. URL http: //www.optimization-online.org/DB_HTML/2020/03/7705.html.   \n[21] A. M. Geoffrion. Generalized Benders\u2019 decomposition. Journal of Optimization Theory and Applications, 10:237\u2013260, 1972.   \n[22] A. Ghaffari-Hadigheh, H. Ghaffari-Hadigheh, and T. Terlaky. Bi-parametric optimal partition invariancy sensitivity analysis in linear optimization. Central European Journal of Operations Research, 16:215\u2013238, 2008.   \n[23] B. Hanin. Universal function approximation by deep neural nets with bounded width and ReLU activations. Mathematics, 7(10):992, 2019.   \n[24] H. He, H. Daume III, and J. M. Eisner. Learning to search in branch and bound algorithms. Advances in Neural Information Processing Systems, 27, 2014.   \n[25] M. Hoefer. UflLib: A collection of benchmark instances for the uncapacitated facility location problem. URL: http://resources.mpi-inf.mpg.de/departments/d1/projects/benchmarks/UflLib, Accessed in May 2024.   \n[26] K. Hornik, M. Stinchcombe, and H. White. Multilayer feedforward networks are universal approximators. Neural Networks, 2(5):359\u2013366, 1989.   \n[27] B. Jansen, C. Roos, and T. Terlaky. An interior point approach to postoptimal and parametric analysis in linear programming. Delft University of Technology, Faculty of Technical Mathematics and Informatics, 1992.   \n[28] H. Jia and S. Shen. Benders cut classification via support vector machines for solving two-stage stochastic programs. INFORMS Journal on Optimization, 3(3):278\u2013297, 2021.   \n[29] H. Jiang, Z. Chen, Y. Shi, B. Dai, and T. Zhao. Learning to defend by learning to attack. In International Conference on Artificial Intelligence and Statistics, pages 577\u2013585. PMLR, 2021.   \n[30] K. Julien, O. Janosch, and R. Walter. Problem-driven scenario clustering in stochastic optimization. Computational Management Science, 20(1), 2023.   \n[31] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations, 2015.   \n[32] N. Kong, A. J. Schaefer, and B. Hunsaker. Two-stage integer programs with stochastic righthand sides: a superadditive dual approach. Mathematical Programming, 108(2):275\u2013296, 2006.   \n[33] E. Larsen, E. Frejinger, B. Gendron, and A. Lodi. Fast continuous and integer L-shaped heuristics through supervised learning. INFORMS Journal on Computing, 36(1):203\u2013223, 2024.   \n[34] J. Lee, S. Bae, W. C. Kim, and Y. Lee. Value function gradient learning for large-scale multistage stochastic programming problems. European Journal of Operational Research, 308(1):321\u2013335, 2023.   \n[35] M. Leshno, V. Y. Lin, A. Pinkus, and S. Schocken. Multilayer feedforward networks with a nonpolynomial activation function can approximate any function. Neural Networks, 6(6): 861\u2013867, 1993.   \n[36] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement and model selection criteria. Operations Research, 29(3):464\u2013484, 1981.   \n[37] T. P. Nagarhalli, V. Vaze, and N. K. Rana. Impact of machine learning in natural language processing: A review. In International Conference on Intelligent Communication Technologies and Virtual Mobile Networks (ICICV), pages 1529\u20131534. IEEE, 2021.   \n[38] V. Nair, D. Dvijotham, I. Dunning, and O. Vinyals. Learning fast optimizers for contextual stochastic integer programs. In UAI, pages 591\u2013600, 2018.   \n[39] V. Nair, S. Bartunov, F. Gimeno, I. Von Glehn, P. Lichocki, I. Lobov, B. O\u2019Donoghue, N. Sonnerat, C. Tjandraatmadja, P. Wang, et al. Solving mixed integer programs using neural networks. arXiv preprint arXiv:2012.13349, 2020.   \n[40] J. Nocedal and S. J. Wright. Numerical Optimization. Springer, New York, NY, USA, 2nd edition, 2006.   \n[41] V. Prochazka and S. W. Wallace. Scenario tree construction driven by heuristic solutions of the optimization problem. Computational Management Science, 17(2):277\u2013307, 2020.   \n[42] R. Rahmaniani, T. G. Crainic, M. Gendreau, and W. Rei. The Benders\u2019 decomposition algorithm: A literature review. European Journal of Operational Research, 259(3):801\u2013817, 2017.   \n[43] T. K. Ralphs and A. Hassanzadeh. On the value function of a mixed integer linear optimization problem and an algorithm for its construction. COR@L Technical Report 14T\u2013004, 2014.   \n[44] W. R\u00f6misch. Scenario reduction techniques in stochastic programming. In International Symposium on Stochastic Algorithms, pages 1\u201314. Springer, 2009.   \n[45] C. Roos, T. Terlaky, and J.-P. Vial. Interior point methods for linear optimization. Springer Science & Business Media, 2005.   \n[46] R. Schultz, L. Stougie, and M. H. Van Der Vlerk. Two-stage stochastic integer programming: A survey. Statistica Neerlandica, 50(3):404\u2013416, 1996.   \n[47] Z. Shao, J. Yang, C. Shen, and S. Ren. Learning for robust combinatorial optimization: Algorithm and application. In IEEE INFOCOM 2022-IEEE Conference on Computer Communications, pages 930\u2013939, 2022.   \n[48] J. Shen, X. Chen, H. Heaton, T. Chen, J. Liu, W. Yin, and Z. Wang. Learning a minimax optimizer: A pilot study. In International Conference on Learning Representations, 2020.   \n[49] J. C. Smith and Y. Song. A survey of network interdiction models and algorithms. European Journal of Operational Research, 283(3):797\u2013811, 2020.   \n[50] Y. Tang, S. Agrawal, and Y. Faenza. Reinforcement learning for integer programming: Learning to cut. In International Conference on Machine Learning, pages 9367\u20139376. PMLR, 2020.   \n[51] Z. C. Task\u0131n. Benders\u2019 decomposition. Wiley Encyclopedia of Operations Research and Management Science. John Wiley & Sons, 2010.   \n[52] O. Tavasl\u0131o\u02d8glu, O. A. Prokopyev, and A. J. Schaefer. Solving stochastic and bilevel mixedinteger programs via a generalized value function. Operations Research, 67(6):1659\u20131677, 2019.   \n[53] A. C Trapp, O. A. Prokopyev, and A. J. Schaefer. On a level-set characterization of the value function of an integer program and its application to stochastic programming. Operations Research, 61(2):498\u2013511, 2013.   \n[54] V. Verter. Uncapacitated and capacitated facility location problems. Foundations of location analysis, pages 25\u201337, 2011.   \n[55] Y. Wu, W. Song, Z. Cao, and J. Zhang. Learning scenario representation for solving two-stage stochastic integer programs. In International Conference on Learning Representations, 2022.   \n[56] Y. Xiong and C.-J. Hsieh. Improved adversarial training via learned optimizer. In Computer Vision\u2013ECCV 2020: 16th European Conference, pages 85\u2013100. Springer, 2020.   \n[57] S. Zaourar and J. Malick. Quadratic stabilization of benders decomposition. 2014. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Complete Proof for GVF Representation Theorem ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we provide the complete proof for the GVF Representation Theorem (Theorem 2). The following lemmas are inspired by [11, Lemma 6.1, Theorem 6.2 and Theorem 6.3]. These results from Blair and Jeroslow are originally stated for the value functions of integer programs. We borrow their techniques and extend these results for the GVFs. In the following proofs, we use English letters, e.g., $b$ or $c$ , to indicate a fixed vector, while Greek letters, e.g., $\\beta$ or $\\gamma$ , are used to signal inputs of some functions and can vary. ", "page_idx": 13}, {"type": "text", "text": "A.1 Lemma 8 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Lemma 8 states that there is a finite set $D$ of improving directions: given a suboptimal feasible point $x\\in X(\\beta)$ , we can improve $x$ by moving along some $r\\in D$ . Notably, the set $D$ depends only on $A$ and not on $\\gamma$ or $\\beta$ , thus setting a basis for constructing the function $h_{A}$ . ", "page_idx": 13}, {"type": "text", "text": "Lemma 8. There exists a finite set $D\\subset\\mathbb{R}^{n}$ such that, for any $\\beta\\in B,\\,\\gamma\\in{\\mathcal{C}}$ , and $x\\in X(\\beta)$ , exactly one of the following is true: ", "page_idx": 13}, {"type": "text", "text": "$I.\\,\\,\\,\\gamma\\cdot x=h_{A}(\\gamma,\\beta),\\,o r$ 2. \u2203\u03f5 > 0 and $r\\in D$ such that $x+\\epsilon r\\in X(\\beta)$ and $\\gamma\\cdot r<0$ . ", "page_idx": 13}, {"type": "text", "text": "Proof. Let $\\{B_{1},B_{2},\\ldots,B_{k}\\}\\subset\\mathbb{R}^{(n-1)\\times n}$ be the set of all full rank submatrix of $[A^{T}\\;I_{n}]^{T}$ . Since $B_{i}$ is full rank, there exists $r_{i}\\in\\mathbb{R}^{n}$ and $\\|r_{i}\\|_{2}=1$ such that for every $t\\in\\mathbb{R},t r_{i}$ is a solution of the system $B_{i}r=0$ . Let $D:=\\{\\pm r_{i}|\\;\\forall i\\in[\\![k]\\!]\\}$ . We show that the finite set $D$ satisfies the condition of Lemma 8. ", "page_idx": 13}, {"type": "text", "text": "We first show that, for any $\\beta\\in\\mathcal B$ , every extreme ray of $X(\\beta)$ (normalized so that its $\\ell^{2}$ -norm is one) belongs to $D$ . Let $v$ be an extreme ray of $X(\\beta)$ . Then, by definition, $v$ must satisfy at equality exactly $n-1$ inequalities from $A v\\le0$ and $v\\geq0$ . Therefore, there exists a matrix $B_{i}$ such that $B_{i}v=0$ and thus $v\\in D$ . ", "page_idx": 13}, {"type": "text", "text": "Consider $x\\ \\in\\ X(\\beta)$ a feasible solution of $L P(\\gamma,\\beta)$ . Let $P$ and $V$ denote the set of extreme points and extreme rays of $X(\\beta)$ respectively. By Carath\u00e9odory\u2019s Theorem [12], for any feasible point $x\\,\\in\\,X(\\beta)$ , there exists $\\{\\}_{p}\\dag_{p}\\geq\\dag0\\}_{p\\in P}$ where $\\textstyle\\sum_{p\\in P}\\lambda_{p}\\ {\\stackrel{\\cdot}{=}}\\ 1$ and $\\{\\rho_{v}~\\geq~0\\}_{v\\in V}$ such that $\\begin{array}{r}{x\\,=\\,\\sum_{p\\in P}\\lambda_{p}p+\\sum_{v\\in V}\\rho_{v}v}\\end{array}$ . If there exists $v^{\\prime}\\in V$ such that $\\rho_{v^{\\prime}}>0$ and $\\gamma\\cdot\\rho_{v^{\\prime}}>0$ , then $x$ cannot be a optimal solution of $L P(\\gamma,\\beta)$ because $x-\\rho_{v^{\\prime}}v^{\\prime}$ has a smaller objective and $-\\boldsymbol{v}^{\\prime}\\in D$ satisfies the second condition of Lemma 8. Without loss of generality, from now on, we assume that $\\begin{array}{r}{x=\\sum_{p\\in P}{\\lambda_{p}p}}\\end{array}$ . ", "page_idx": 13}, {"type": "text", "text": "Since $x$ is a feasible solution, $x$ is either a optimal solution of $L P(\\gamma,\\beta)$ or not. If $x$ is the optimal solution, then $\\gamma\\cdot x=h(\\gamma,\\beta)$ . Otherwise, by definition of $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ and $\\mathcal{C}$ , $L P(\\gamma,\\beta)$ has an basic optimal solution $p^{*}\\neq x$ such that $\\gamma\\cdot p^{*}<\\gamma\\cdot x$ . Since $\\begin{array}{r}{p^{*}-x=\\sum_{p\\neq p^{*}}\\lambda_{p}p^{*}+(1-\\lambda_{p^{*}})p^{*}}\\end{array}$ , we have $p^{*}-x\\in\\operatorname{span}(D)$ . By our choice of $D$ , if $r\\in D$ then $-r\\in D$ , thus there exists $r\\in D$ such that $\\gamma\\cdot r<0$ and $\\exists\\epsilon>0$ and $r\\in D$ such that $x+\\epsilon r\\in X(\\beta)$ . \u53e3 ", "page_idx": 13}, {"type": "text", "text": "A.2 Lemma 9 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Using the improving directions from Lemma 8, Lemma 9 produces a finite set of extreme rays that generate the objective coefficient space $\\mathcal{C}$ . By this lemma, the value of $h_{A}(\\gamma,\\beta)$ can always be written as a conic combination of $\\{\\bar{h}_{A}(c_{q},\\beta)\\}_{q\\in\\mathbb{I}\\not\\in\\mathbb{I}}$ . ", "page_idx": 13}, {"type": "text", "text": "Lemma 9. There exists a finite set $C:=\\{c_{1},c_{2},\\dots,c_{Q}\\}\\subset\\mathcal{C}$ such that for any $c\\in{\\mathcal{C}},\\,b\\in B$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\binom{\\gamma}{h_{A}(c,b)}\\in c o n e\\left(\\binom{c_{1}}{h_{A}(c_{1},b)},\\ldots,\\binom{c_{Q}}{h_{A}(c_{Q},b)}\\right)\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof. Let $D$ be the finite set satisfying the condition in Lemma 8. For every subset $R\\subset D$ , we define ", "page_idx": 13}, {"type": "equation", "text": "$$\nC(R)=\\{c\\in{\\mathcal{C}}|\\;c\\cdot r\\geq0\\,\\forall r\\in R\\}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Since $C(R)$ is a polyhedral cone for every $R\\subset D$ , we denote $E(R)$ to be the finite set of extreme rays of $C(R)$ . We show that $C:=\\cup_{R\\subset D}E(R)$ and denote its elements as $\\{c_{1},c_{2},\\ldots,c_{Q}\\}$ satisfies the condition stated in Lemma 9. ", "page_idx": 14}, {"type": "text", "text": "For any $\\gamma\\in{\\mathcal{C}}$ and $\\beta\\in\\mathcal B$ , since $L P(\\gamma,\\beta)$ has a finite optimal solution, there must exists $x^{*}$ such that $h(\\gamma,\\beta)=\\gamma\\cdot x^{*}$ . Let $R^{*}:=\\left\\{r\\in D\\right|\\exists\\epsilon>0$ , $x^{*}+\\epsilon r\\,\\!\\in\\boldsymbol{X}(\\beta)\\}$ be the set of all feasible improving directions at $x^{*}$ . ", "page_idx": 14}, {"type": "text", "text": "Claim 1: By our choice of $R^{*}$ , we must have $\\gamma\\in C(R^{*})$ . By contradiction, suppose that $\\gamma\\notin C(R^{*})$ , then there must exist $r^{*}\\,\\in\\,R^{*}$ such that $\\gamma\\cdot r^{*}<0$ . However, by definition of $R^{*}$ , there exists $\\epsilon>0$ such that $x^{*}+\\epsilon r^{*}$ is a feasible solution of $L P(\\gamma,\\beta)$ . In addition, we have $\\gamma\\cdot(x^{*}+\\epsilon r^{*})=$ $\\gamma\\cdot x^{*}+\\epsilon\\gamma\\cdot r^{*}<\\gamma\\cdot x^{*}$ , which contradicts the fact that $x^{*}$ is an optimal solution of $L P(\\gamma,\\beta)$ . Hence, we derive that $\\gamma\\in C(R^{*})$ . Moreover, since $C(R^{*})$ is a polyhedral cone whose extreme rays $E(R^{*})$ belongs to C by our construction of C, we have there exists \u03b1 \u22650 such that q\u2208Q \u03b1qcq = \u03b3. ", "page_idx": 14}, {"type": "text", "text": "Claim 2: For every $c\\in C(R^{*})$ , we have $h(c,\\beta)=c\\!\\cdot\\!x^{*}$ . Since $x^{*}\\in X(\\beta)$ , we have $h(c,\\beta)\\leq c\\!\\cdot\\!x^{*}$ . Moreover, because $x^{*}$ is optimal solution of $L P(\\gamma,\\beta)$ , we also have $\\gamma\\cdot r\\geq0$ for every $r\\in R^{*}$ . By contradiction, suppose there exists $c\\in C(R^{*})$ such that $h(c,\\beta)<c\\cdot x^{*}$ . By Lemma 8 and $x^{*}$ is not an optimal solution of $L P(\\gamma,\\beta)$ , there exists $\\epsilon>0$ and $r\\in D$ such that $x^{*}+\\epsilon r\\in X(\\beta)$ and $c\\cdot r<0$ . However, since $x^{*}+\\epsilon r\\in X(\\beta)$ , we must have $r\\in R^{*}$ , and by definition of $C(R^{*})$ , every $c\\in C(R^{*})$ must satisfy $c\\cdot r\\geq0$ , which contradicts with our conclusion that $c\\cdot r<0$ . Therefore, $h(c,\\beta)=c\\cdot x^{*}$ for every $c\\in C(R^{*})$ . ", "page_idx": 14}, {"type": "text", "text": "Finally, we have that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\sum_{q\\in[Q]}\\alpha_{q}h(c_{q},\\beta)=\\sum_{q\\in[Q]}\\alpha_{q}c_{q}\\cdot x^{*}=\\left(\\sum_{q\\in[Q]}\\alpha_{i}c_{q}\\right)\\cdot x^{*}=\\gamma\\cdot x^{*}=h(\\gamma,\\beta).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "This is because from Claim 1, $\\gamma\\in C(R^{*})$ , without loss of generality, we assume that $\\alpha_{q}=0$ for every $c_{q}\\not\\in E(R^{*})$ . Therefore, we either have $\\alpha_{q}=0$ or $h(c_{q},\\beta)=c_{q}\\cdot x^{*}$ . Thus, $\\begin{array}{r l}{\\sum_{q\\in[\\![Q]\\!]}\\alpha_{i}h(c_{q},\\beta)=}&{{}}\\end{array}$ $\\textstyle\\sum_{q\\in[{Q}]}\\alpha_{i}c_{q}\\cdot x^{*}$ . \u53e3 ", "page_idx": 14}, {"type": "text", "text": "A.3 Lemma 10 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "For ease of notation, we denote $g_{q}(\\beta):=h_{A}(c_{q},\\beta)$ for every $q\\in[\\![Q]\\!]$ . A function $g_{q}$ is obtained from $h_{A}$ by fixing the objective coefficients to $c_{q}$ . Therefore, $g_{q}(\\beta)$ is  an LPVF (parameterized by constraint bounds), and thus a convex piecewise linear function. ", "page_idx": 14}, {"type": "text", "text": "While Lemma 9 shows that $h_{A}$ can be represented by a finite set of LPVFs $\\{g_{q}\\}_{q\\in\\mathbb{I}Q\\mathbb{I}}$ , the linear combination coefficients of such representation depend on $b\\in\\mathcal{B}$ and $c\\in{\\mathcal{C}}$ and are u nc lear how to obtain. The following Lemma 10 gives a more explicit characterization of the linear combination coefficients. ", "page_idx": 14}, {"type": "text", "text": "Lemma 10. For every $\\gamma\\in{\\mathcal{C}}$ and $\\beta\\in\\mathcal B$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{h_{A}(\\gamma,\\beta)=\\underset{\\alpha}{\\operatorname*{max}}\\ \\displaystyle\\sum_{q\\in[\\![Q]\\!]}\\alpha_{q}g_{q}(\\beta)}\\\\ &{\\qquad\\qquad\\qquad s.t\\ \\displaystyle\\sum_{q\\in[\\![Q]\\!]}\\alpha_{q}c_{q}=\\gamma}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\alpha\\ge0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. Let $x^{*}$ be the optimal solution of $L P(\\gamma,\\beta)$ and $x^{i}$ be the optimal solution of $L P(c_{i},\\beta)$ for every $i\\in[N]$ . Since $x^{*}\\in X(\\beta)$ is always a feasible solution of $L P(c_{q},\\beta)$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\nc_{i}\\cdot x^{*}\\geq c_{i}\\cdot x^{i}\\,\\forall q\\in\\mathbb{I}Q\\mathbb{I}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Therefore, for every $\\alpha$ such that $\\textstyle\\sum_{q\\in[{Q}]}\\alpha_{q}c_{q}=\\gamma$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\nh(\\gamma,\\beta)=\\sum_{i\\in\\mathbb{I}^{N}\\mathbb{I}}\\alpha_{i}c_{i}x^{*}\\geq\\sum_{i\\in\\mathbb{I}^{N}\\mathbb{I}}\\alpha_{i}\\cdot c_{i}\\cdot x^{i}=\\sum_{i\\in\\mathbb{I}^{N}\\mathbb{I}}\\alpha_{i}g_{i}(\\beta),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the first equality follows Claim 2 of Lemma 9. Combined with the existence of $\\alpha$ in Lemma 9, we derive that $\\begin{array}{r}{\\mathring{h_{A}}(\\gamma,\\mathring{\\beta})=\\operatorname*{max}_{\\alpha\\geq0}\\{\\sum_{q\\in\\mathbb{I}Q\\mathbb{J}}\\alpha_{q}g_{q}(\\beta)\\;|\\;\\sum_{q\\in\\mathbb{I}Q\\mathbb{J}}\\alpha_{q}c_{q}=\\gamma\\}.}\\end{array}$ \u53e3 ", "page_idx": 14}, {"type": "text", "text": "Since $(L P_{\\alpha}(\\gamma,\\beta))$ is a linear program, for every $\\gamma,\\beta$ , there must exists at least one basic feasible point that is an optimal solution. Hence, there exists a maximizer $\\alpha$ of $(L P_{\\alpha}(\\gamma,\\beta))$ that has at least $N-n$ elements equal to 0 (non-basic variables), and at most $n$ non-zero elements (basic variables). The vectors of $\\mathcal{C}$ corresponding to the basic variables generates a cone. Another way to view Lemma 10 is that, to find $h(\\gamma,\\beta)$ , we need to find the cone containing $\\gamma$ that maximizes $\\dot{\\sum}_{i\\in[N]}\\,\\alpha_{i}g_{i}(\\beta)$ . ", "page_idx": 15}, {"type": "text", "text": "A.4 Lemma 11 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The next lemma shows that the optimal basis of $(L P_{\\alpha}(\\gamma,\\beta))$ remains the same when $\\gamma$ varies within the cone defined by that basis. ", "page_idx": 15}, {"type": "text", "text": "Lemma 11. Let $b\\in{\\mathfrak{B}}$ and $c\\in{\\mathcal{C}}$ be fixed. Without loss of generality, we assume that $\\textstyle c=\\sum_{q=1}^{n}\\alpha_{q}^{*}c_{q}$ where $\\alpha_{q}^{*}>0$ and the corresponding $C^{*}:=[c_{1},c_{2},...,c_{n}]$ is the optimal basis of $L P_{\\alpha}(c,\\bar{b})$ . Then for any $\\alpha_{1},\\ldots,\\alpha_{n}\\geq0$ and $\\textstyle\\gamma=\\sum_{q=1}^{n}\\alpha_{q}c_{q}$ , the basis $C^{*}$ is also the optimal basis of $L P_{\\alpha}(\\gamma,b)$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. By the hypothesis of the lemma and by Lemma 10, we have: ", "page_idx": 15}, {"type": "equation", "text": "$$\nh_{A}(c,b)=\\sum_{i=1}^{n}\\alpha_{i}^{*}h_{A}(c_{i},b).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Since $h_{A}(\\gamma,b)$ is a concave piecewise linear function and $\\textstyle\\gamma=\\sum_{i=1}^{n}\\alpha_{i}c_{i}$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\nh_{A}(\\gamma,b)=\\sum_{q=1}^{n}\\|\\alpha\\|_{1}h_{A}\\left(\\sum\\frac{\\alpha_{q}}{\\|\\alpha\\|_{1}}c_{q},b\\right)\\geq\\sum_{q=1}^{n}\\alpha_{q}g_{q}(b).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We will show that $\\begin{array}{r}{h(\\gamma,b)=\\sum_{q=1}^{n}\\alpha_{q}g_{q}(b)}\\end{array}$ for every $\\gamma\\in\\mathsf{c o n e}(C)$ . Since $h_{A}$ is linear with respect to $\\gamma$ , we only need to prove that $\\begin{array}{r}{h(\\gamma,b)=\\sum_{q=1}^{n}\\lambda_{q}g_{q}(b)}\\end{array}$ where $\\textstyle\\gamma:=\\sum_{q=1}^{n}\\lambda_{q}c_{q}\\in\\operatorname{conv}(C)$ . By contradiction, suppose that there exists $\\gamma^{*}\\in\\mathop{\\mathrm{argmax}}_{\\gamma\\in\\operatorname{conv}(C)}h_{A}(\\gamma,b)$ and $\\begin{array}{r}{h_{A}(\\gamma^{*},b)>\\sum_{q=1}^{n}\\lambda_{q}^{*}c_{q}}\\end{array}$ . Let $c^{\\prime}:=\\frac{c}{\\|\\alpha^{*}\\|_{1}}$ \u2225\u03b1c\u2217\u22251 so that c\u2032 \u2208conv(C). By contradiction hypothesis, we have hA(\u03b3\u2217, b)  > hA(c\u2032, b). Let $d:=\\gamma^{*}-c^{\\prime}$ , and consider the function $l(t)=h_{A}(c^{\\prime}\\!+\\!t d,b)$ . Since $l(t)$ is concave and $l(1)>l(t)$ for $t\\in[0,1)$ , $l(t)$ is a linear function on $[0,1]$ . However, because $c^{\\prime}$ is an interior point of $\\operatorname{conv}(C)$ , there exist $\\epsilon>0$ such that $c^{\\prime}-\\epsilon d\\in\\mathrm{conv}(C)$ and $l(-\\epsilon)\\leq l(0)-\\epsilon(l(1)-l(0))$ , which contradict the concavity of $h_{A}(\\gamma,b)$ . Therefore, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\nh(\\gamma,b)=\\sum_{q=1}^{n}\\alpha_{q}g_{q}(b).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Moreover, because the optimal solution of $L P_{\\alpha}(c_{q},b)$ is $\\alpha_{q}=1$ , $\\alpha_{j}=0\\;\\forall j\\neq q,$ $C^{*}$ is also the optimal basis of $L P_{\\alpha}(c_{q},b)$ . Thus, $C^{*}$ is the optimal basis $L P_{\\alpha}(\\gamma,b)$ for every $\\alpha_{i}\\geq0,i\\in[n]$ . ", "page_idx": 15}, {"type": "text", "text": "A.5 Theorem 2 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "sTuhbes eptrse voifo suisz eL ,m dmena o1t1e di $b$ uwceh  tchaant partition the finite set $C$ of Lemma 9 into $n$ $\\dot{\\mathcal{P}^{b}}:=\\{C_{1}^{b},\\ldots,C_{v_{b}}^{b}\\}$ ", "page_idx": 15}, {"type": "text", "text": "1. $\\mathcal{C}=\\cup_{v=1}^{v_{b}}\\mathrm{cone}(C_{v}^{b})$ , where the interiors of cone $\\left(C_{v}^{b}\\right)$ are mutually disjoint, and   \n2. If $\\gamma\\in\\mathrm{cone}(C_{v}^{b})$ , then $C_{v}^{b}$ is the optimal basis of $L P_{\\alpha}(\\gamma,b)$ . ", "page_idx": 15}, {"type": "text", "text": "Since $C$ is finite, there can only be a finite number of such partitions. Thus, we can associate each $\\beta\\in\\mathcal B$ with a partition of $C$ and group the vectors of $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ into groups of constraint bounds vectors with the same partition. ", "page_idx": 15}, {"type": "text", "text": "GVF Representation Theorem: For a fixed matrix $A\\in\\mathbb{R}^{m\\times n}$ , there exists a set of $P$ piecewise linear functions $\\{F_{p}:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}^{Q}\\}_{p=1}^{P}$ and a piecewise linear convex function $G:\\mathbb{R}^{\\dot{m}}\\rightarrow\\mathbb{R}^{Q}$ such that: ", "page_idx": 15}, {"type": "equation", "text": "$$\nh_{A}(\\gamma,\\beta)=\\operatorname*{max}_{p\\in\\mathbb{I}^{P}}\\{F_{p}(\\gamma)^{T}G(\\beta)\\}\\;\\forall\\gamma\\in\\mathcal{C},\\beta\\in\\mathcal{B}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. We will construct the function $h_{A}(\\gamma,\\beta)$ as follows. Let $\\{B_{1},\\dotsc,B_{P}\\}$ be the partition of $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ such that, for every $p\\in[\\![P]\\!]$ , $B_{p}$ contains every constraint bounds vector $b$ with the same partition of $C$ from Lemma 11, i. e., $\\mathcal{P}^{b^{1}}=\\mathcal{P}^{b^{2}}$ if and only if $b^{1},b^{2}\\in B_{k}$ for some $k$ . Note that $P$ is finite because $\\mathcal{C}$ is finite. Furthermore, we denote $\\mathcal{P}^{p}:=\\{C_{1}^{p},...\\,,C_{v_{p}}^{p}\\}$ to be the partition of $C$ associated with the set $B_{p}$ . ", "page_idx": 16}, {"type": "text", "text": "Note that $C_{v}^{p}$ is invertible since it comes from the basis of $L P_{\\alpha}(\\gamma,b_{p})$ . Let $F_{p}:\\mathbb{R}^{n}\\to\\mathbb{R}^{Q}$ be such that ", "page_idx": 16}, {"type": "equation", "text": "$$\n[F_{p}(\\gamma)]_{q}=\\left\\{\\!\\!\\begin{array}{l l}{[(C_{v}^{p})^{-1}\\gamma]_{q}}&{\\mathrm{~if~}c_{q},\\gamma\\in C_{v}^{p}}\\\\ {0}&{\\mathrm{~otherwise.}}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "In addition, let $G:\\mathbb{R}^{m}\\rightarrow\\mathbb{R}^{Q}$ be defined as ", "page_idx": 16}, {"type": "equation", "text": "$$\nG(\\beta)=[g_{1}(\\beta),g_{2}(\\beta),\\ldots,g_{Q}(\\beta)],\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which is convex piecewise linear since each $g_{i}$ is an LP value function. By our choice of $F_{p}$ , for any $\\gamma$ , we have $F(p)(\\gamma)\\geq0$ . Furthermore, let $\\alpha=F_{p}(\\gamma)$ , we also have q=1 cq\u03b1q = \u03b3. Thus, Fp(\u03b3) is a feasible solution of $L P_{\\alpha}(\\gamma,\\beta)$ for every $\\gamma\\in{\\mathcal{C}}$ , $\\beta\\in\\mathcal B$ , and $p\\in[\\![P]\\!]$ . ", "page_idx": 16}, {"type": "text", "text": "By Lemma 10, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\nh(\\gamma,\\beta)=\\operatorname*{max}_{p\\in[P]}\\{F_{p}(\\gamma)^{T}G(\\beta)\\}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "B Complete Proofs of the GVF Unsupervised Learning Theory ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "B.1 Proof of Theorem 3 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Theorem 3. Any function $\\eta_{A}\\in{\\mathcal{H}}(A)$ has the following properties: ", "page_idx": 16}, {"type": "text", "text": "1. $\\eta_{A}(\\gamma,\\cdot)$ is piecewise linear, convex, and monotonically decreasing for every fixed $\\gamma\\in{\\mathcal{C}}$ .   \n2. $\\eta_{A}(\\cdot,\\beta)$ is piecewise linear for every fixed $\\beta\\in\\mathcal B$ .   \n3. $\\eta_{A}(\\gamma,\\beta)\\leq h_{A}(\\gamma,\\beta)$ for every fixed $\\beta\\in\\mathcal B$ and $\\gamma\\in{\\mathcal{C}}$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. The properties of a function $\\eta_{A}$ follows by the our choice for activation functions, weightsigned and output constraints. ", "page_idx": 16}, {"type": "text", "text": "1. $\\eta_{A}(c,\\cdot)$ is linear since the at every layer of the constraint bounds stack, we apply a linear transformation and a activation that is also pieceiwse linear. Since maximum or non-negative linear combination of convex functions is convex, $\\eta_{A}(c,\\cdot)$ is convex with respect to the constraints bound. In addition, the function is monotonically decreasing since the first layer of the constraint bounds stack has non-positive weights. ", "page_idx": 16}, {"type": "text", "text": "2. Similarly, because of linear transformations at every layer in the objective stack, the function $\\eta_{A}(\\cdot,b)$ with a fixed $b$ is piecewise linear. ", "page_idx": 16}, {"type": "text", "text": "3. For every fixed $c\\in{\\mathcal{C}}$ , $\\eta_{A}(c,\\cdot)$ is a convex piecewise linear function. Therefore, $\\eta_{A}(c,\\beta)$ can be written as maximum of a finite number of linear function. Let $\\eta_{A}(c,\\beta)\\;:=\\;$ $\\operatorname*{max}_{l\\in\\mathbb{I}}\\{\\beta^{T}(y^{l})\\}$ . Moreover, by the weight-singed constraint of the first of layer of the constraint bounds stack, we have $y^{l}\\le0$ . In addition, since $\\eta_{A}\\in{\\mathcal{H}}(A)$ , we have that $\\operatorname*{max}_{l\\in\\mathbb{I}\\mathbb{L}}\\{(a^{i})^{T}(y^{l})\\}\\leq c_{i}$ for every $i\\in[n]$ . Therefore, each $y^{l}$ is a feasible solution of (2). By  l inear program weak duality, we  ha ve $\\eta_{A}(c,\\beta)\\leq\\,h_{A}(c,\\beta)$ . Since the argument applies for every $c\\in{\\mathcal{C}}$ , we derive $\\eta_{A}(\\gamma,\\beta)\\leq h_{A}(\\gamma,\\beta)$ for every $\\beta\\in\\mathcal B$ and $\\gamma\\in{\\mathcal{C}}$ . ", "page_idx": 16}, {"type": "text", "text": "B.2 Proof of Theorem 4 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Property 3 of Theorem 3 relies the linear program weak duality. We will show that the inequality in Property 3 (Theorem 3) is indeed tight. ", "page_idx": 17}, {"type": "text", "text": "Theorem 4. For any fixed $A\\in\\mathbb{R}^{m\\times n}$ , $h_{A}\\in{\\mathcal{H}}(A)$ , and moreover $h_{A}$ is pointwise larger than all other elements of $\\mathcal{H}(A)$ . ", "page_idx": 17}, {"type": "text", "text": "Proof. By the GVF Representation Theorem (Theorem 2), there exists continuous piecewise linear functions $\\{{F}_{i}\\}_{i=1}^{p}$ and convex piecewise linear function $G$ such that $\\begin{array}{r l}{h_{A}(\\gamma,\\beta)}&{{}=}\\end{array}$ $\\mathrm{max}_{i\\in\\mathbb{I}^{\\Vert}}\\{F_{i}(\\gamma)^{T}G(\\beta)\\}$ . The objective stack can model exactly any continuous piecewise linear fun ct ion. Therefore, with sufficient layers and neurons, it can model the functions $\\{F_{i}\\}_{i=1}^{p}$ exactly. Moreover, as shown in Theorem 2, each component of the function $G$ is a LPVF, and thus is a piecewise linear convex function with nonpositive slope. On the other hand, the constraint bounds stack with weight-sign constraint can model precisely any convex piecewise with nonpositive slope. Hence, with sufficient layers and neurons, it can also model the function $G$ exactly. Thus, $\\overline{{h_{A}}}\\in\\mathcal{H}(A)$ . In addition, by Property 3 of Theorem 3, every function in $\\boldsymbol{\\mathcal{A}}$ is upper bounded by hA. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "B.3 Proof of Theorem 5 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Theorem 5. There exists some $\\mathcal{M}\\in\\mathbb{Z}_{+}^{M}$ , some $\\mathcal{N}\\in\\mathbb{Z}_{+}^{N}$ , a finite set $\\bar{\\mathcal{C}}\\subsetneq\\mathcal{C}$ , a finite set ${\\bar{\\beta}}\\subsetneq B$ , and some $\\eta^{\\prime}\\in D S M(\\mathcal{M},\\mathcal{N})$ such that $\\eta^{\\prime}(\\gamma,\\beta)=h_{A}(\\gamma,\\beta)$ for all $\\beta\\in\\bar{\\mathcal{B}},\\gamma\\in\\bar{\\mathcal{C}}$ . Moreover, this same $\\eta^{\\prime}$ necessarily satisfies $\\eta^{\\prime}(\\gamma,\\beta)=h_{A}(\\gamma,\\beta)$ for all $\\beta\\in\\mathcal B$ and $\\gamma\\in{\\mathcal{C}}$ . ", "page_idx": 17}, {"type": "text", "text": "Proof. Let $C\\in\\mathcal{C}=\\{c_{1},\\ldots,c_{Q}\\}$ be the set describe in Lemma 9. Then for every cone generated by $n$ distinct vectors in $C$ , we pick an arbitrary interior point, and denote the set containing every such interior points as $C^{\\prime}$ . Then, we construct a finite set ${\\dot{\\bar{c}}}:=C\\cup C^{\\prime}$ as the union of $C$ and $C^{\\prime}$ . We construct $\\bar{\\boldsymbol{B}}$ in a similar way. For every cone generated by $m$ distinct vectors in $\\{a^{1},\\ldots,a^{n},e^{1},\\ldots,e^{m}\\}$ , where $e^{1},\\ldots,e^{m}\\,\\in\\,\\ensuremath{\\mathbb{R}}^{m}$ denotes the unit vector, we pick an arbitrary point, and denote the set containing every such interior points as $B^{\\prime}$ . We then derive $\\bar{\\mathcal{B}}:=\\{a^{\\flat},\\dot{\\ldots},a^{n},e^{1},\\ldots,e^{m}\\}\\cup B^{\\prime}$ . The GVF Representation Theorem (Theorem 2) proves existence of functions {Fp}pP=1 and G to model $h_{A}$ . Since these functions are piecewise linear, we denote $f_{p}$ as the number of pieces of $F_{p}$ for $p\\in[\\![P]\\!]$ and $g$ as number of pieces of $G$ . We then pick features $\\mathcal{N}$ such that the $\\gamma$ -stack of the DSM ca n re present every piecewise linear function with at most $f_{1},\\ldots,f_{P}$ pieces. Similarly, for the $\\beta$ -stack, we pick features $\\mathcal{M}$ such that the stack can represent every convex piecewise linear function with at most $g$ pieces. 3 ", "page_idx": 17}, {"type": "text", "text": "Let $\\eta^{\\prime}~\\in~\\mathsf{D S M}({\\cal M},{\\cal N})$ and $\\eta^{\\prime}(\\gamma,\\beta)~=~h_{A}(\\gamma,\\beta)~\\forall\\beta~\\in~\\bar{\\mathcal{B}},\\gamma~\\in~\\bar{\\mathcal{C}}$ , we show that $\\eta^{\\prime}(\\gamma,\\beta)\\;=$ $h_{A}(\\gamma,\\beta)\\ \\forall\\beta\\,\\in\\,B,\\gamma\\,\\in\\,\\mathcal{C}$ . For a fixed $c\\in{\\bar{\\mathcal{C}}}$ , the function $\\eta^{\\prime}(c,\\beta)$ , by Property 2 of Theorem 3, is a convex piecewise linear function. Moreover, $\\eta^{\\prime}(c,\\beta)=h_{A}(\\dot{c},\\beta)$ for every $\\beta\\in{\\bar{\\mathcal{B}}}$ . By our choice of $\\bar{\\boldsymbol{B}}$ , $\\dot{\\eta^{\\prime}}(c,\\beta)=h_{A}(c,\\beta)$ at more point than the number of piece that $h_{A}(c,\\beta)$ can have. Since $\\eta^{\\prime}(c,\\beta)\\leq h_{A}(c,\\beta)\\,\\forall\\beta\\in\\mathcal{B}$ and $\\eta^{\\prime}(c,\\beta)$ is convex, we have $\\eta^{\\prime}(c,\\beta)=h_{A}(c,\\stackrel{\\cdot}{\\beta})$ . Hence, we have $\\eta^{\\prime}(c_{q},\\beta)=g_{q}(\\beta)$ for every $q\\in[\\![Q]\\!]$ , $\\beta\\in\\mathcal B$ . ", "page_idx": 17}, {"type": "text", "text": "By Lemma 9, $h_{A}$ can be represented by a conic combination of $\\{g_{q}(\\beta)\\}_{q=1}^{Q}$ . By our choice $\\bar{\\mathcal{C}}$ . The function $\\eta^{\\prime}$ equal $h_{A}$ at more points than the number of pieces it can have. Thus $\\eta^{\\prime}(\\gamma,\\beta)=h_{A}(\\gamma,\\beta)$ for every $\\beta\\in\\mathcal B$ and $\\gamma\\in{\\mathcal{C}}$ . \u53e3 ", "page_idx": 17}, {"type": "text", "text": "B.4 Proof of Corollary 6 and 7 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Corollary 6 Take the ${\\mathcal{M}},{\\mathcal{N}},{\\bar{B}},$ , and $\\bar{\\mathcal{C}}$ that Theorem 5 guarantees must exist. Then, $h_{A}$ is the unique solution of (7) ", "page_idx": 17}, {"type": "text", "text": "Proof. Theorem 5 implies $h_{A}\\in\\mathsf{D S M}(\\mathcal{M},\\mathcal{N})$ . By Theorem 3, we have that $h_{A}(c,\\beta)\\ge\\eta(c_{\\underline{{{2}}}}\\beta)$ for every $\\eta$ satisfies (7). Finally, Theorem 5 shows that we can represent $h_{A}$ through set of point $\\bar{\\boldsymbol{B}}$ and $\\bar{\\mathcal{C}}$ . Therefore, $h_{A}$ is the optimal solution of (7). \u53e3 ", "page_idx": 17}, {"type": "text", "text": "Corollary 7. Given any nonempty subsets $\\mathcal{D}_{b}\\in\\mathcal{B}$ and $\\mathcal{D}_{c}\\in\\mathcal{C}$ , there exists a sufficient large \u00b5 such that $h_{A}$ is an optimal solution of (8). ", "page_idx": 18}, {"type": "text", "text": "Proof. The Corollary follows from [40, Theorem 17.3] on local minimizer of nonlinear program with non-smooth penalty function and the fact that $h_{A}$ is the optimal solution of (7). \u53e3 ", "page_idx": 18}, {"type": "text", "text": "C Uncapacitated Facility Location ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Uncapacitated facility location (UFL) [54] is a classical, widely-applicable problem with the goal of deciding which of $n_{f}$ \u201cfacilities\u201d to open (warehouses, plants, equipment, etc.) while taking into account a set of $n_{c}$ \u201ccustomers\u201c that are allocated to the open facilities. This can be interpreted as a deterministic two-stage problem: in the first stage, we select a subset of facility locations to open, and in the second stage, we assign each customer to a single open facility, while minimizing the costs of opening facilities plus customer allocation. Formally, the problem is as follows: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\operatorname*{min}_{x,y}}&{\\displaystyle\\sum_{i\\in[n_{f}]}f_{i}y_{i}+\\displaystyle\\sum_{i\\in[n_{f}]}\\displaystyle\\sum_{j\\in[n_{c}]}c_{i j}x_{i j}}\\\\ &{\\displaystyle\\sum_{i=1}^{n_{f}}x_{i j}=1}\\\\ &{x_{i j}\\leq y_{i}}\\\\ &{x_{i j}\\geq0}\\\\ &{y_{i}\\in\\{0,1\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\forall j\\in\\ensuremath{[\\![n_{c}]\\!]}}\\\\ &{\\forall i\\in\\ensuremath{[\\![n_{f}]\\!]},j\\in\\ensuremath{[\\![n_{c}]\\!]}}\\\\ &{\\forall i\\in\\ensuremath{[\\![n_{f}]\\!]},j\\in\\ensuremath{[\\![n_{c}]\\!]}}\\\\ &{\\forall i\\in\\ensuremath{[\\![n_{f}]\\!]}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $n_{f}$ and $n_{c}$ are the number of facilities and customers respectively. Note that the second-stage variables are implied integer variables, i.e. if $y^{\\ast}$ is integer, then there exists an optimal solution $(x^{\\ast},y^{\\ast})$ such that $x^{*}$ is integer. Therefore, we can effectively treat the second-stage variables $x$ as continuous variables. ", "page_idx": 18}, {"type": "text", "text": "We consider two sets of benchmark instances: ", "page_idx": 18}, {"type": "text", "text": "\u2022 Euclidean, in which we generate facilities and customers uniformly at random in a box $[0,1]^{2}$ , using Euclidean distances as customer costs, with sizes $n_{f}=\\dot{n}_{c}\\in\\{100,200,300\\}$ ; and   \n\u2022 $K G$ , which are the symmetric Koerkel-Ghosh instances from the UFLLIB benchmark set [25], with sizes $n_{f}\\dot{=}n_{c}\\in\\{250,500,750\\}$ . ", "page_idx": 18}, {"type": "text", "text": "For Euclidean UFL instances, we generate one instance of Euclidean for training and five instances for testing. In the KG set from UFLLIB, we use first instance from the class A instances (a1) for training and test on the class B instances (classes A and B differ in how the costs were generated). ", "page_idx": 18}, {"type": "text", "text": "D Stochastic Capacitated Facility Location ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The Stochastic Capacitated Facility Location (SCFL) problem is a variant of the traditional facility location problem where decision-making occurs under uncertainty regarding demand or other parameters, and facilities have limited capacities. SCFL problems are often modeled as two-stage stochastic programs. In the first stage, we make decisions involve selecting the facilities to open, committing to fixed setup costs. In the second stage, given the realized demand, the model assigns customers to open facilities, ensuring capacity constraints are respected while minimizing assignment costs. The problem can be formulated as follows: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{\\operatorname*{min}_{\\boldsymbol{x},\\boldsymbol{y}}\\displaystyle\\sum_{i\\in[n_{f}]}f_{i}y_{i}+\\sum_{\\boldsymbol{s}\\in S}p_{s}\\sum_{i\\in[n_{f}]}\\sum_{j\\in[n_{c}]}c_{i j}^{s}x_{i j}^{s}}&&{}\\\\ &{\\qquad\\displaystyle\\sum_{i\\in[n_{f}]}x_{i j}^{s}\\geq d_{j}^{s},}&&{\\quad\\forall j\\in[n_{c}],\\forall\\boldsymbol{s}\\in S}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{\\displaystyle\\sum_{j\\in[n_{f}]}x_{i j}^{s}\\leq u_{i}y_{i},}&&{\\qquad\\qquad\\qquad}&&{\\forall i\\in[\\![n_{f}]\\!],\\forall s\\in S}\\\\ &{x_{i j}^{s}\\geq0,}&&{\\qquad\\qquad\\qquad}&&{\\forall i\\in[\\![n_{f}]\\!],\\forall j\\in[\\![n_{c}]\\!],\\forall s\\in S}\\\\ &{y_{i}\\in\\{0,1\\},}&&{\\forall i\\in[\\![n_{f}]\\!],}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where, similar to the UFL case, $n_{f}$ and $n_{c}$ denotes the number of facilities and customers, respectively. In addition, $S$ is the set of all scenarios and $p_{s}$ denote the probability of the scenario $s$ . Moreover, we denote $f_{i}$ as the fixed cost of opening facility $i$ , $c_{i j}^{s}$ as the assignment cost per unit from facility $i$ to customer $j$ in scenario $s$ , $d_{j}^{s}$ as the demand of customer $j$ in scenario $s$ , and $u_{i}$ as the capacity of facility $i$ . ", "page_idx": 19}, {"type": "text", "text": "A major practical difference from UFL is that we can no longer decompose the problem over customers, due to capacity constraints that link all customers. However, this stochastic problem has scenarios that we decompose over, where both the demand and assignment costs can vary. Here, we have $n_{c}\\cdot n_{f}$ variables in each subproblem, whereas in UFL we had $n_{f}$ variables per subproblem. ", "page_idx": 19}, {"type": "text", "text": "To generate SCFL instances, we take deterministic instances from the OR-Library [6], and generate 50 scenarios by randomly modifying both the demands and unit costs to be $\\mathcal{N}(\\bar{\\mu},\\sigma)$ where $\\mu$ is the original value and $\\sigma$ is drawn from $\\mathcal{U}(0.1\\mu,0.2\\mu)$ , resampling any negative costs. We consider all scenarios to be equally likely, i.e., ps = |S1|. We use 30 samples for $\\mathcal{D}_{b}$ . We perform the experiment of learning the GVF function for SCFL in Table 3. We conduct the training on cap61 and testing on cap62, with 16 customers and 50 facilities. ", "page_idx": 19}, {"type": "text", "text": "Table 3: Comparison between Dual-Stack Model and DenseNet in Learning GVF. ", "page_idx": 19}, {"type": "table", "img_path": "Vxijl0IOId/tmp/c6ddd8a31c564a5da8e6cc9fe9fb63f0b8cec1fdca405c1525d68533ddbe52a6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "E Details on Numerical Experiments ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "For comparison of the performance between DSM and DenseNet in learning GVF, we perform a parameter tuning for both models in terms of model architecture, dropout and learning rate. We train both of these models on a single GPU. With DSM, for the $\\beta$ -stack we perform training with $1,2,3$ layers, each with 32, 64 neurons. The activation of every layer is max-pooling with a window of size 5. For the $\\gamma.$ -stack, we also train with 2, 3 layers each with 32, 64 neurons, except the size of last layer is 128, 256 respectively. In addition, to avoid vanishing gradients, we use a composition of GeLU and ReLU during training only. The final layer of $\\gamma.$ -stack is then reshaped to $4\\times32$ or $4\\times64$ to match the shape for the output of $\\beta$ -stack. The dropout is only added in the last layer of the DSM and its parameter is chosen among $\\{0.02,0.03,0.04\\}$ . With DenseNet, we also train the model with 1, 2, 3 layers, each with 32, 64 neurons. Dropout is added in every layer of the DenseNet and its parameter is chosen among $\\{0.1,0.2,0.3\\}$ . The learning rate of Adam for both model is selected from $\\{10^{-2},10^{-3},10^{-4}\\}$ . Table 4 summarizes the best configuration for each models. ", "page_idx": 19}, {"type": "text", "text": "Another aspect in learning GVF using our unsupervised framework is the choice of penalty $\\mu$ update function (see Algorithm 1). We also want to answer how the quality of the upper bound at every training data point affect the learned function. As we can observe in Table 6 and 5 that the quality of the learned function depends more on how we update the penalty term $\\mu$ more than how good the upper bound is. ", "page_idx": 19}, {"type": "table", "img_path": "Vxijl0IOId/tmp/94b3cffbd9e75f902e3742e4ce697f5140a7543cb0159360cf2a6f483e860efa.jpg", "table_caption": ["Table 4: NN Architectures and Learning Parameters for DSM and DenseNet "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "Vxijl0IOId/tmp/35de0ffae62480c4ce3ef4a71d337c737c85544be73d69f5cc610fbf4f0b5b87.jpg", "table_caption": ["Table 5: Comparison of update methods and upper bounds for GVF Learning on KG-sym 250 "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "Vxijl0IOId/tmp/0c2c9e5a1554bc7ae01f85fa93807cdcc95be0253abb56cfe0edad00d329773a.jpg", "table_caption": ["Table 6: Comparison of update methods and upper bounds for GVF Learning on Euclidean 100 "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "F Dual-Stack Model for Non-Standard LP Formulations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Table 7 describes the necessary changes to the DSM in the case that the LP is not in standard form.   \nAlternatively, one may simply convert the LP to standard form. ", "page_idx": 20}, {"type": "table", "img_path": "Vxijl0IOId/tmp/ef09cb56acedafe161902b8caa28b5655b763a91d9e0711f0086c4b918b8d478.jpg", "table_caption": ["Table 7: Alterations to the architecture for LPs not in standard form "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We would like to refer to Section 4, 5, 6, and 7. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 21}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: We would like to refer to Section 7. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 21}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We would like to refer to Section 4, Section 5, Appendix A and Appendix B. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 22}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We would like to refer to Section 7 and Appendix E. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 22}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 22}, {"type": "text", "text": "Answer: [No] ", "page_idx": 23}, {"type": "text", "text": "Justification: The main contribution of the paper lies in the theory and methodology for learning GVF. All algorithms and details on experiments are presented in the paper. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 23}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We would like to refer to Section 7 and Appendix E. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 23}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We report standard deviations in our results. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We would like to refer to Appendix E. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 24}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We have verified that we follow all items in the NeurIPS Code of Ethics. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 24}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper focus on theoretical properties and learning a mathematical object (the generalized LP value function), which does not have a direct societal impact. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: We study the theory of learning GVF and conduct the experiments on synthetic data, which poses no risks for misuse. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 25}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The only existing asset that we use are benchmark instances from UFLLIB, which are properly cited in the paper. These assets are not re-released with the paper. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: We do not release new assets. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 26}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: Our study does not involve crowdsourcing. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 26}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: Our study does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 26}]