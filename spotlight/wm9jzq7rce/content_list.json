[{"type": "text", "text": "An Analysis of Tokenization: Transformers under Markov Data ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Nived Rajaraman Jiantao Jiao Kannan Ramchandran UC Berkeley UC Berkeley UC Berkeley nived.rajaraman@berkeley.edu jiantao@berkeley.edu jiantao@berkeley.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "While there has been a large body of research attempting to circumvent tokenization for language modeling (Clark et al., 2022; Xue et al., 2022), the current consensus is that it is a necessary initial step for designing state-of-the-art performant language models. In this paper, we investigate tokenization from a theoretical point of view by studying the behavior of transformers on simple data generating processes. When trained on data drawn from certain simple $k^{\\mathrm{th}}$ -order Markov processes for $k>1$ , transformers exhibit a surprising phenomenon - in the absence of tokenization, they empirically are incredibly slow or fail to learn the right distribution and predict characters according to a unigram model (Makkuva et al., 2024). With the addition of tokenization, however, we empirically observe that transformers break through this barrier and are able to model the probabilities of sequences drawn from the source near-optimally, achieving small cross-entropy loss. With this observation as starting point, we study the end-to-end cross-entropy loss achieved by transformers with and without tokenization. With the appropriate tokenization, we show that even the simplest unigram models (over tokens) learnt by transformers are able to model the probability of sequences drawn from $k^{\\mathrm{th}}.$ - order Markov sources near optimally. Our analysis provides a justification for the use of tokenization in practice through studying the behavior of transformers on Markovian data. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The training of language models is typically not an end-to-end process. Language models are often composed of a \u201ctokenizer\u201d, which encodes a sequence of characters into a sequence of token ids, which map to substrings. The subsequent language modeling task is carried out by a neural network or transformer, which is pre-trained and fine-tuned on large datasets. The ideal goal is to jointly train the tokenizer and transformer with end-to-end accuracy as the objective. This is a challenging problem to solve efficiently, and thus, the tokenizer is generally adapted on a portion of the training dataset and frozen before the transformer is trained. In practice, byte-level/character level models such as ByT5 (Xue et al., 2022) and CANINE (Clark et al., 2022) which avoid tokenization often perform worse for the reason that semantic relationships can be harder to capture at the character level (Libovicky\\` et al., 2021; Itzhak and Levy, 2021). ", "page_idx": 0}, {"type": "text", "text": "Though used most commonly, tokenization at the subword level often has sharp edges. Test sequences may contain rare tokens which were never seen in the training dataset. The presence of such tokens may induce undesirable behavior in the outputs of models (Rumbelow and Watkins, 2023; Kharitonov et al., 2021; Yu et al., 2021) and present an attack surface for bad actors. Moreover, tokenized models struggle on tasks that involve manipulation at the character level, such as spelling out words or reversing sentences. For similar reasons, LLMs with standard tokenizers also struggle to carry out basic arithmetic (Golkar et al., 2023). Despite this brittleness, tokenization is used in nearly all state-of-the-art LLM architectures. ", "page_idx": 0}, {"type": "text", "text": "In this paper, we introduce a statistical formulation for tokenization for next-word-prediction. We study the class of models transformers are observed to express empirically under simple data generating processes, which often can have simpler descriptions. Taking a step back, rather than focusing on proxy evaluation metrics, which lead to an ever-changing goalpost, we focus on understanding the behavior of the end-to-end cross-entropy loss, $\\mathcal{L}(\\cdot)$ . In this paper, we study a simplification of real world data generating processes and study the case where data sources are $k^{\\mathrm{th}}$ -order Markov processes. Within this framework we can compare tokenizers against each other, and in the process capture several interesting phenomena. Our main results are as follows, ", "page_idx": 1}, {"type": "text", "text": "1. There are very simple $k^{\\mathrm{th}}$ -order Markov processes such that in the absence of any tokenization, transformers trained on data drawn this source empirically predict characters according to a unigram model. This phenomenon is observed under a wide variety of hyperparameter choices. This is problematic because unigram models such as that induced by the stationary distribution are poor at modeling Markovian data and suffer from a high cross-entropy loss. This phenomenon was also recently observed in Makkuva et al. (2024).   \n2. When trained with tokenization, transformers are empirically observed to break through this barrier and are able to capture the probability of sequences under the Markov distribution near-optimally. In other words, in the presence of tokenization, transformers appear to achieve near-optimal cross-entropy loss. This phenomenon is observed with a multitude of tokenizers used commonly in practice.   \n3. We analyze a toy tokenizer which adds all length- $k$ sequences into the dictionary and show that as dictionary size grows, unigram models trained on the tokens get better at modeling the probabilities of sequences drawn from Markov sources. We then theoretically prove that tokenizers used in practice, such as the LZW tokenizer (Zouhar et al., 2023a) and a variant of the BPE tokenizer (Gage, 1994; Sennrich et al., 2016) which are learnt from data also satisfy this property but require much smaller dictionaries to achieve any target cross-entropy loss. ", "page_idx": 1}, {"type": "text", "text": "In our framework, the most challenging hurdle and the biggest departure from previous work such as (Zouhar et al., 2023b) is the element of generalization - understanding how a tokenizer performs on new sequences that it was not trained on. This generalization turns out to be a delicate phenomenon - we show in Appendix D that there exist tokenizers which generalize poorly in the sense that they may compress the dataset they are trained on into a short sequence of tokens, but fail to generalize to new sequences. In Appendix E we show that there exist dictionaries which generalize well (in the sense of having low cross-entropy loss) to new sequences under one encoding algorithm, but completely fail to generalize under another. ", "page_idx": 1}, {"type": "text", "text": "1.1 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Tokenization has a long history of empirical study in natural language processing. In the literature, a number of tokenizers have been developed for various domains such as math (Singh and Strouse, 2024), code (Zheng et al., 2023; Parr, 2013) and morphology-aware tokenizers for different languages like Japanese (Tolmachev et al., 2018; Den et al., 2007) and Arabic (Alyafeai et al., 2023) among many others. In modern LLMs, the most commonly used tokenizers are variants of BPE (Gage, 1994), Wordpiece (Schuster and Nakajima, 2012) and the Unigram tokenizer (Kudo, 2018) which learn a dictionary from data, rather than hard-coding language dependent rules. There has been a long line of work interpreting tokenization from various lenses (Grefenstette and Tapanainen, 1994; Palmer, 2000; Zouhar et al., 2023b). ", "page_idx": 1}, {"type": "text", "text": "The theoretical study of transformers has also received much attention recently. We discuss the closest relatives to our work below. Edelman et al. (2024) study the learning trajectory of transformers trained on data drawn from $1^{\\mathrm{st}}$ -order Markov chains. While the authors empirically observe that the models eventually learn to predict tokens correctly according to the Markov kernel, simplicity bias slows down optimization - the models initially predict tokens according to a unigram model (in context unigrams), which delays learning the optimal solution. This phenomenon was also observed in Makkuva et al. (2024). On the positive side, Nichani et al. (2024) study an in-context causal learning task that generalizes learning in-context bigrams for $1^{\\mathrm{st}}$ -order Markov processes and analyze the trajectory of gradient descent. ", "page_idx": 1}, {"type": "image", "img_path": "wm9JZq7RCe/tmp/108354a837701e4f9ff3e6fcdde86401a54f28b869b4bdedb524959b479c9470.jpg", "img_caption": [], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Figure 1: 2-state switching process. The above state diagram describes the distribution of $X_{n}$ conditioned on $X_{n-1}$ . $k^{t h}$ -order extension: the conditional probability of $X_{n}$ only depends on $X_{n-k}$ through the kernel, $\\operatorname*{Pr}(X_{n}=1|X_{n-k}=0)=p$ and $\\operatorname*{Pr}(X_{n}=0|X_{n-k}=1)=q$ . ", "page_idx": 2}, {"type": "text", "text": "Notation. All logarithms are base $e$ , unless specified otherwise. The Shannon entropy $H(X)$ of a categorical random variable $X$ is $\\begin{array}{r}{-\\sum_{x\\in\\mathrm{supp}(X)}p(x)\\log p(x).\\ H_{\\mathrm{BER}}(}\\end{array}$ captures the entropy of a Bernoulli random variable with parameter $p$ . The notation $O_{p,q,r}(f(n))$ (likewise $\\Omega_{\\{\\cdot\\}}$ and $\\Theta_{\\{\\cdot\\}})$ indicate that the underlying constant depends polynomially on the parameters $p,q$ and $r$ and $\\widetilde{O}(f(n))$ (likewise, $\\widetilde{\\Theta}$ and $\\widetilde{\\Omega}$ ) ignores polylog $(n)$ terms. For a set $S$ , $S^{\\star}=\\cup_{k=1}^{\\infty}S^{k}$ , the set of all sequences with elem ents dr awn from $S$ . For a sequence $\\pmb{t}$ , $\\pmb{t}_{i:j}=(\\pmb{t}_{i},\\pmb{t}_{i+1},\\cdot\\cdot\\cdot\\dot{\\textbf{,}t_{j}})$ returns a slice. ", "page_idx": 2}, {"type": "text", "text": "2 Formulation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We consider a setting where the learner\u2019s objective is to learn a language model which models probabilities of sequences over an input alphabet $\\boldsymbol{\\mathcal{A}}$ . The data to be modeled is generated according to an unknown probability model $P\\,:\\,A^{\\star}\\,\\rightarrow\\,[0,1]$ over strings. A tokenizer is a tuple $\\tau=$ ( $\\mathsf{D i c t},\\mathsf{e n c}(\\cdot),\\mathsf{d e c}(\\cdot))$ . Here Dict is a collection of tokens The encoding function $\\mathsf{e n c}(\\cdot):\\mathcal{A}^{\\star}\\rightarrow$ Dict\u22c6, maps strings of characters to a sequence of tokens, and likewise, the decoding function ${\\mathsf{d e c}}(\\cdot):{\\bar{\\mathsf{D i c t}}}^{\\star}\\to A^{\\star}$ maps a sequence of tokens to a string of characters. We assume that the tokenizer is \u201cconsistent\u201d, namely, $\\mathsf{d e c}(\\mathsf{e n c}(\\cdot))$ is the identity function. ", "page_idx": 2}, {"type": "text", "text": "We consider a setting where the learner has access to a training dataset which is a sequence of length $n$ sampled from a data source1. We study the likelihood maximization problem, where the objective of the learner is to learn an end to end model such that the cross-entropy loss is minimized. In the presence of tokenization, we have a model of the form $Q_{\\mathrm{end}}=Q\\circ\\mathsf{e n c}(\\cdot)$ where $Q$ is a joint distribution across sequences of tokens when the tokenizer corresponding to $\\mathsf{e n c}(\\cdot)$ is used. The cross-entropy loss, i.e. the log-perplexity, can be written down as, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{m}(Q_{\\mathrm{end}})\\triangleq-\\mathbb{E}[\\log Q(\\mathsf{e n c}(s))],}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "with the objective to minimize it. Here, the expectation is over $\\pmb{s}$ , a fresh test sequence of length $m$ sampled from the data generating process. Fixing a tokenizer, let $\\mathcal{Q}$ denote a family of joint distributions over tokens (i.e. likelihood models). The objective is to jointly design a tokenizer (with encoding function enc $(\\cdot)$ ) and likelihood model $Q\\in\\mathcal{Q}$ with small test loss $\\mathscr{L}_{m}\\bar{(Q\\circ\\mathsf{e n c}(\\cdot))}$ . ", "page_idx": 2}, {"type": "text", "text": "Finally, for a dictionary Dict, the unigram family of models, $\\mathcal{Q}_{1-\\mathrm{gram}}$ , is defined as below: $Q\\in\\mathcal{Q}_{1}$ -gram associates probability $\\begin{array}{r}{Q(t_{1},t_{2},\\cdot\\cdot\\cdot,t_{j})=Q_{\\#}(j)\\prod_{i=1}^{j}Q_{\\mathrm{tok}}(t_{i})}\\end{array}$ to the sequence of tokens $\\pmb{t}_{1},\\cdots,\\pmb{t}_{j}$ for measures $Q_{\\#}$ and $Q_{\\mathrm{tok}}$ supported on $\\mathbb{N}$ and D ict respectively. ", "page_idx": 2}, {"type": "text", "text": "2.1 Data generating process ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this paper, we consider a simplification of real-world data generating processes by considering the case where the data generating distribution is a $k^{\\mathrm{th}}$ -order Markov process over characters. Studying the behavior of transformers trained on Markov data was the subject of the works Makkuva et al. (2024) and Edelman et al. (2024), where a number of interesting phenomena were unearthed. When a transformer is trained on data from certain simple Markov processes like the one considered in Figure 1, a very peculiar phenomenon occurs - within a reasonably large number of iterations, the transformer fails to improve beyond the loss incurred by the best unigram model. This phenomenon is reproducible across a wide number of hyperparameters, including the number of feed-forward layers in the model, the embedding dimension, and the number of attention heads. In Figure 3a this is made clearer - the transformer fails to improve its test loss beyond that of the best unigram model. ", "page_idx": 2}, {"type": "image", "img_path": "wm9JZq7RCe/tmp/c6304b34907659d8e9829380bce4ebc322f51e74a949f0f0ddef6f16609121fe.jpg", "img_caption": ["Figure 2: Token distribution returned by the transformer tokenized by a learnt BPE encoder with a dictionary size of 20. A test sequence is generated from the stochastic source and encoded into a token sequence $\\pmb{t}$ . Each narrow vertical column represents the distribution over next tokens returned by the transformer when the first $x$ tokens of $\\pmb{t}$ are fed into the model, where $x$ is varied from 0 to the length of $\\pmb{t}$ . For most values of $x$ , the model appears to predict the same distribution over the next token. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "How bad can a unigram model be? It turns out that the gap between the cross-entropy of the best unigram model and that of the optimal model can be characterized precisely. ", "page_idx": 3}, {"type": "text", "text": "Theorem 2.1. Consider any ergodic data source with stationary distribution over characters $\\pi$ . The unconstrained optimal likelihood model achieves cross-entropy loss, $\\mathrm{min}_{Q}\\,\\mathcal{L}_{m}(Q)\\,=\\,H(P)$ . In contrast, the cross-entropy loss under any unigram model $Q\\in\\mathcal{Q}_{1-g r a m}$ satisfies, $\\mathcal{L}_{m}(Q)\\ge m H(\\pi)$ . ", "page_idx": 3}, {"type": "text", "text": "The ratio of the optimal loss $H(P)$ , and the optimal unigram loss, $m H(\\pi)$ can be arbitrarily large. In particular, for the switching chain in Figure 1, as $p,q\\to0$ , the ratio diverges to $\\infty$ . ", "page_idx": 3}, {"type": "text", "text": "While transformers are a powerful class of models, it is concerning that they fail to learn very simple distributions such as $k^{\\mathrm{th}}$ -order Markov processes. Why do they work so well in practice if they can be so slow to learn Markovian data? It turns out that there is a simple missing ingredient in all the architectures considered so far: tokenization. All the models trained in Figure 3a operate on raw character sequences drawn from the stochastic source. To understand the role of tokenization, we run another experiment and train the transformer on sequences generated from the stochastic source which are encoded into tokens by a BPE tokenizer learnt from data. The transformer now operates on sequences of tokens, rather than sequences of individual symbols. In Figure 3b we plot the results of this experiment - in the presence of tokenization, the cross-entropy loss of the end-to-end model breaks past the unigram barrier and approaches the optimal bound within a small number of iterations. ", "page_idx": 3}, {"type": "text", "text": "Let\u2019s peek into the model a bit more and understand its behavior. In Figure 2 we run the following experiment: we sample a random sequence of length 2000 from a Markov chain and feed it into the transformer after tokenization, resulting in $\\approx500$ tokens. We plot the next-token distribution predicted by the transformer at every single position in the input, generated by autoregressive masking. In Figure 2 we stitch together these next-token distributions, each of which is a narrow column heatmap. Visually, we observe that the plot is approximately homogeneous along the $x$ -axis, implying that the next-token distribution learned does not depend strongly on the prefix at that position. Thus the transformer learns what is essentially a unigram model. ", "page_idx": 3}, {"type": "text", "text": "Thus, we come to a surprising conclusion: the behavior of the transformer on the $k^{\\mathrm{th}}$ -order switching source in Figure 1 with and without tokenization is essentially the same. In both cases, the model learns a unigram model over the tokens - in the absence of tokenization this unigram model is in fact the stationary distribution induced by the source. If the transformer learns a unigram model in both cases, how come there is such a large gap in performance between the two? To understand this in more detail, we analyze a toy tokenizer. As a simplification, we will analyze the behavior of an arbitrary, but exact unigram model under this tokenizer. ", "page_idx": 3}, {"type": "text", "text": "3 Unigram models under tokenization ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Let\u2019s consider a toy tokenizer which assigns all possible substrings of length $r$ as tokens in the dictionary and study what happens when a unigram model is trained on the tokenized sequences. The ", "page_idx": 3}, {"type": "image", "img_path": "wm9JZq7RCe/tmp/f8f483d5361eef5293339ecc8a0825c24d7da9eeee3034ff6cf0d19d2ebb3577.jpg", "img_caption": ["(a) The loss of the transformer fails to converge to the (b) In the presence of tokenization, the test loss of th optimal cross-entropy loss (dashed line) and instead model approaches the optimal bound (dashed line). It converges to that of the best unigram model (dotted is worth noting that the models trained here are sigline). The shaded blue region captures how the test nificantly smaller than those considered in Figure 3a, loss curves vary as hyperparameters (number of layers, having up to $70\\times$ fewer parameters and yet are able to embedding dimension etc.) are changed. achieve the optimal cross-entropy loss. "], "img_footnote": [], "page_idx": 4}, {"type": "image", "img_path": "wm9JZq7RCe/tmp/4d3fe37f9554cf86229b7b6f4472bc14f0b017a4235c0e12a813a85939f46ddc.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 3: Transformers trained on the order-2 switching Markov process (Figure 1) with $p=q=0.8$ On the left we have the model trained without tokenization and on the right the model uses BPE with a dictionary of size 10 learnt from data. ", "page_idx": 4}, {"type": "text", "text": "total dictionary size $d=2^{r}$ . A sequence of characters is mapped to a sequence of tokens by simply chunking it into a sequences of $r$ characters which are replaced by the corresponding token index2. The resulting stochastic process on the tokens is still Markovian, but over a state space of size $2^{r}$ . For any unigram model $Q$ on the tokens, the cross-entropy loss can be written down as, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{m}(Q\\circ\\mathsf{e n c}(\\cdot))=\\mathbb{E}\\left[\\sum_{t\\in\\mathsf{e n c}(s)}\\log(1/Q_{\\mathsf{t o k}}(t))\\right]+\\Theta(\\log(m)),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where we choose $Q_{\\#}=\\mathrm{Unif}([m])$ , which contributes an additive $\\log(m)$ to the loss. Choosing $\\begin{array}{r}{Q_{\\mathrm{tok}}(t)=\\pi(t_{1})\\prod_{i=1}^{r-1}P(t_{i+1}|t_{i})}\\end{array}$ as the stationary probability the Markov process associates with $\\pmb{t}$ , ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{m}\\mathcal L_{m}(Q\\circ\\mathsf{e n c}(\\cdot))\\approx-\\frac{1}{m}\\mathbb E\\left[\\log(P(s)+\\sum_{i=0}^{m/k-1}\\log\\left(\\frac{\\pi(s_{k i+1})}{P(s_{k i+1}|s_{k i})}\\right)\\right]}\\\\ &{\\phantom{\\frac{1}{m}\\mathcal L_{m}(Q\\circ\\mathsf{e n c}(\\cdot))}\\Tilde{\\approx}\\frac{1}{m}H(P)+\\frac{1}{m k}\\left(m H(\\pi)-H(P)\\right)}\\\\ &{\\phantom{\\frac{1}{m}\\mathcal L_{m}(Q\\circ\\mathsf{e n c}(\\cdot))}=\\frac{H(P)}{m}\\left(1-\\frac{1}{\\log_{2}(d)}\\right)+\\frac{H(\\pi)}{\\log_{2}(d)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "the approximation in $(i)$ uses the fact that as $m$ grows large, $\\begin{array}{r}{\\frac{1}{m}\\sum_{i=0}^{m/k}\\log(P(s_{k i+\\ell+1}|s_{k i+\\ell})}\\end{array}$ approaches H(kP ). With d = 2 (i.e., r = 1), we recover the performance of the character tokenizer in Theorem 2.1. An immediate implication of this simple calculation is that as $m\\rightarrow\\infty$ , there is a unigram model which is nearly optimal as the dictionary size grows to $\\infty$ . ", "page_idx": 4}, {"type": "text", "text": "While this toy tokenizer allows us to glean this intuition behind why tokenization allows unigram models to be near-optimal, there are some obvious issues. One, the tokenizer does not adapt to the distribution of the data. Indeed, for the switching Markov source in Figure 1, as $p=q=\\delta\\to0$ , the source contains increasingly longer sequences of contiguous $0\\,\\mathrm{\\dot{s}}$ and 1\u2019s. In this case, it makes since to have a dictionary containing such sequences, rather than all possible length- $^r$ sequences, many of which would be seen very few times (if at all) in a test sequence. At a more technical level, in eq. (2), to get to a cross-entropy loss of $2H(P)$ , the size of the dictionary required by the toy tokenizer is $e^{m H(\\pi)/H(P)}$ . As discussed in Example A.1 for the switching Markov process with $p=q=\\delta$ , this dictionary size can be extremely large and scales exponentially (in $1/\\delta$ ) as $e^{1/\\delta\\log\\stackrel{\\textstyle{\\cdot}}{(1/\\delta)}}$ when $\\delta$ is small. In general, on stochastic sources on a much larger alphabet, such as English/ASCII, this toy tokenizer would result in a prohibitively large dictionary. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Larger dictionaries are usually correlated with the presence of rare tokens which appear infrequently at training time. This presents a problem in practice - a lot more data is often required to see enough examples of such tokens to learn good embeddings for them. More importantly, in the absence of this volume of data, rare tokens present an attack surface to elicit undesirable behavior in the model (Rumbelow and Watkins, 2023). In practice, this issue present with the toy tokenizer is, to an extent, resolved by using tokenization algorithms such as BPE or Wordpiece, which learn dictionaries from data. In the process, they are able to avoid learning extremely rare tokens, by enforcing a lower bound on the number of their occurrences in the training data to be allocated as a token. By minimizing the number of such rare tokens, the model is able to utilize its token budget in a more efficient manner. ", "page_idx": 5}, {"type": "text", "text": "We now introduce the main theoretical result of this paper, showing that with the appropriate tokenization algorithm with a token budget of $d$ , a unigram model is not only asymptotically able to achieve the optimal cross-entropy loss, but also requires far smaller dictionaries to match the performance of the toy tokenizer considered earlier. In order to avoid dealing with the transient characteristics of the source, we consider the cross-entropy loss in eq. (1) under the assumption that the test sequences $\\pmb{s}$ are of length $m\\rightarrow\\infty$ . Namely, define the normalized loss, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\cdot)=\\operatorname*{lim}_{m\\to\\infty}\\frac{1}{m}\\mathcal{L}_{m}(\\cdot)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Theorem 3.1. Consider a Markov data generating process which satisfies Assumption 3.2. Let d denote a budget on the size of the dictionary. Then, there exists a tokenizer with at most $d$ tokens and encoding function $e n c(\\cdot)$ , such that, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{Q\\in\\mathcal{Q}_{1-g r a m}}\\mathcal{L}(Q\\circ e n c(\\cdot))\\leq\\frac{1}{1-\\varepsilon}\\operatorname*{min}_{Q^{\\prime}}\\mathcal{L}(Q^{\\prime})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\varepsilon$ is $\\log(1/\\delta)/0.99\\log(d)^{3}$ . Furthermore, a tokenizer satisfying eq. (3) with probability $\\geq1-d^{-\\Omega_{\\delta}(\\log(d))}$ can be learnt from a dataset of $\\widetilde{O}_{\\delta}(d)$ characters. ", "page_idx": 5}, {"type": "text", "text": "The tokenizers considered in this theorem are far more efficient with their token budget than the toy tokenizer - to achieve a cross entropy loss within a factor 2 of optimal, the dictionary size required by these tokenizer is $d\\approx1/\\delta^{2}$ on any source satisfying Assumption 3.2. In comparison, the toy tokenizer requires a dictionary size of $e^{1/\\delta\\log(1/\\delta)}$ to achieve the same error. We show that the LZW tokenizer proposed in (Zouhar et al., 2023a) achieves the upper bound in eq. (3) when trained on a dataset of size ${\\widetilde{O}}(d)$ . Likewise, we also show that a sequential variant of BPE achieves the upper bound in eq. (3)  up to a factor of 2 and with a worse dependency in $\\varepsilon$ when trained on a dataset of size $\\widetilde O(d^{2})$ . What is interesting is that neither of these algorithms explicitly learn a unigram likelihood model, $Q$ , while constructing the dictionary. Yet they are able to perform as well as the tokenizers which are jointly optimized with a likelihood model, such as the Unigram tokenizer (Kudo, 2018). ", "page_idx": 5}, {"type": "text", "text": "Key insight. While the toy tokenizer provides a high level intuition as to why tokenization might enable unigram models to model Markov sources well, here we present a different explanation which captures tokenization from an operational viewpoint. Tokenizers which do a good job at learning patterns in the data and assigning these frequent patterns as tokens in the dictionary are compatible with an i.i.d. model over tokens. A hypothetical example motivating this point: consider a tokenizer such that the distribution of tokens in the encoding of a fresh string sampled from the source is distributed i.i.d., except that whenever the token $t^{\\prime}$ appears, it is always followed by $t^{\\prime\\prime}$ . An i.i.d. model on the tokens is a poor approximation since $P(\\bar{t}^{\\prime}\\bar{t}^{\\prime\\prime})\\gg P(\\mathbf{t}^{\\prime})P(\\bar{t}^{\\prime\\prime})$ . However, by merging $t^{\\prime}$ and $t^{\\prime\\prime}$ into a new token $\\pmb{t}$ and adding this to the dictionary, the new distribution over tokens is i.i.d. In general, this motivates why it is desirable for a tokenizer to allocate new tokens to substrings which appear next to each other frequently, i.e. a pattern in the data. As more tokens are added to the dictionary, one might expect the cross-entropy loss incurred by the best unigram model to improve. ", "page_idx": 5}, {"type": "text", "text": "3.1 Learning patterns in the source ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The main result of this section is a generic reduction: dictionaries which typically encode new strings into a few long tokens (defined in a formal sense in Theorem 3.4), result in tokenizers achieving near-optimal cross-entropy loss. We prove this result for Markovian sources under a regularity assumption, which is that the associated connectivity graph of the chain is complete. The analogous assumption for $k^{\\mathrm{th}}$ -order sources is that the transition kernel is entry-wise bounded away from 0. This assumption is satisfied by all the sources considered in the paper thus far, such as the $k^{\\mathrm{th}}$ -order switching processes in Figure 1. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Assumption 3.2 (Data generating process). Assume that the data source is an ergodic Markov process with transition $P(\\cdot|\\cdot)$ and stationary distribution $\\pi$ . Assume that $\\mathrm{min}_{a,a^{\\prime}\\in A}\\,P(a^{\\prime}|a)\\triangleq\\delta>0$ . ", "page_idx": 6}, {"type": "text", "text": "Remark 3.3. Assumption 3.2 (and its $k^{\\mathrm{th}}$ -order extension) impose that there is a small but nonzero probability of observing any particular symbol after any preceding sequence. This limits the applicability of these processes in real-world scenarios where such a phenomenon may not occur. However, our motivation for this assumption is different: $\\delta$ allows parameterizing the Markov process in a way which interpolates between i.i.d. $(\\delta=1/|A|)$ and highly non-i.i.d. $\\left[\\delta\\rightarrow0\\right]$ ). ", "page_idx": 6}, {"type": "text", "text": "For a substring $\\pmb{s}$ and a character $a$ , define $\\begin{array}{r}{P(\\pmb{s}|\\boldsymbol{a})=P(\\pmb{s}_{1}|\\boldsymbol{a})\\prod_{i=2}^{|\\pmb{s}|}P(\\pmb{s}_{i}|\\pmb{s}_{i-1})}\\end{array}$ denote the conditional probability of the substring $\\pmb{s}$ . We now state the main res ult of this section. ", "page_idx": 6}, {"type": "text", "text": "Theorem 3.4 (Bound on cross-entropy loss of dictionaries under greedy encoder). Consider a source satisfying Assumption 3.2 and any tokenizer $\\tau$ equipped with the greedy encoder, $e n c_{g r e}(\\cdot)$ with finitely long tokens. Define, $P(t)=\\mathbb{E}_{a\\sim\\pi}[P(t|a)]$ and suppose $\\begin{array}{r}{H(Q_{M L E},P)\\geq\\frac{1}{\\varepsilon}\\log(1/\\delta).}\\end{array}$ for some $\\varepsilon<1$ . Then, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{Q\\in\\mathcal{Q}_{1-g r a m}}\\mathcal{L}(Q\\circ e n c_{g r e}(\\cdot))\\leq\\frac{\\operatorname*{min}_{Q}\\mathcal{L}(Q)}{1-\\varepsilon}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Interpretation. $H(Q_{\\mathrm{MLE}},P)=\\mathbb{E}_{t\\sim Q_{\\mathrm{MLE}}}[\\log(1/P(t))]$ is large when the encoder places higher mass (i.e. larger values of $Q_{\\mathrm{MLE}}(\\cdot))$ on tokens which have low probability under $P$ , i.e. which correspond to longer substrings. Intuitively, this metric is higher for tokenizers which typically use long tokens (i.e. low $P(\\cdot),$ ) to encode new strings. ", "page_idx": 6}, {"type": "text", "text": "3.2 LZW tokenizer ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section we study the Lempel-Ziv-Welch (LZW) based tokenization scheme introduced by Zouhar et al. (2023a) and establish guarantees of the form of Theorem 3.1 for this tokenizer. ", "page_idx": 6}, {"type": "text", "text": "Definition 3.5 (LZW tokenizer). Iterating from left to right, the shortest prefix of the training dataset which does not already exist as a token is assigned as the next token in the dictionary. This substring is removed and the process is iterated on the remainder of the dataset. The tokenizer uses the greedy encoding algorithm (Definition A.3) to encode new strings into tokens. ", "page_idx": 6}, {"type": "text", "text": "An example of the LZW tokenizer: For the dataset 0100111, the dictionary created is $\\{0,1,00,11\\}$ . ", "page_idx": 6}, {"type": "text", "text": "The LZW tokenizer is based on the LZW algorithm for compression (Ziv and Lempel, 1978; Welch, 1984). The dictionary satisfies the property that if some substring $s^{\\prime}$ exists as a token in the dictionary, then all of its prefixes must also belong to the dictionary. In the next theorem, we show that the LZW tokenizer approximately achieves the optimal cross-entropy loss. ", "page_idx": 6}, {"type": "text", "text": "Theorem 3.6. Suppose the LZW tokenizer is trained on a dataset of length at most $d$ (thereby learning a dictionary with at most d tokens). For Markov sources satisfying Assumption 3.2, with probability $\\geq1-d^{-\\check{O_{\\delta}}(\\log(d))}$ , the resulting tokenizer satisfies, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{Q\\in\\mathcal{Q}_{1-g r a m}}\\mathcal{L}(Q\\cdot e n c_{g r e}(\\cdot))\\leq\\frac{\\operatorname*{min}_{Q}\\mathcal{L}(Q)}{1-\\varepsilon}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where \u03b5 = $\\begin{array}{r}{\\varepsilon=\\frac{\\log(1/\\delta)}{0.99\\log(d)}^{4}}\\end{array}$ ", "page_idx": 6}, {"type": "text", "text": "The proof of this result considers all substrings $\\pmb{t}$ with $P(t)\\;\\geq\\;1/d^{0.99}$ . These substrings are reasonably high probability and observed many times in a dataset of $\\widetilde\\Omega(d)$ characters. We show that with high probability, the LZW tokenizer learns all of these substri ngs as tokens in the dictionary. Now, when processing a new string, since the greedy algorithm only emits the longest substring ", "page_idx": 6}, {"type": "image", "img_path": "wm9JZq7RCe/tmp/8dc44dde538d6638669883bacd48789734518c24797720b80f8895bb88c661ad.jpg", "img_caption": ["(a) Convergence rate of smallest model which is within (b) Convergence rate of models with the same embed$10\\%$ of the optimal-cross entropy by 300 epochs. The ding dimension (20), number of heads (1) and layers smallest untokenized model has 9010 parameters (3 (3) with and without tokenization. The model with tolayers, embedding dimension $=10$ ). The smallest to- kenization (dictionary size of 20) appears to converge kenized model with a dictionary size of 10 has 17880 more quickly, but the error floor is subtly higher comparameters (3 layers, embedding dimension $=~20$ ). pared to the model without tokenization. Both models The tokenized model has more parameters but the wall- are trained on input sequences of length 512. The width clock time taken to reach any loss value is smaller. of the tokenized model is smaller (145). "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "wm9JZq7RCe/tmp/746d8740d19d3b51783264ffff7254ce928056cec2dd4d3185cc70ac5a373f0e.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 4: Test loss vs. wall-clock time for the tokenized and untokenized models when trained on the order-1 switching Markov chain (Figure 1) with $p=q=0.8$ . The tokenizer used is BPE. ", "page_idx": 7}, {"type": "text", "text": "which matches a token, every token allocated must fall on the \u201cboundary\u201d of this set, having $P(t)\\leq$ $O(1/d^{0.99})$ . By definition, this means that $H(Q_{\\mathrm{MLE}},P)=\\mathbb{E}_{t\\sim Q_{\\mathrm{MLE}}}[\\log(1/P(t))]=0.\\bar{9}9\\log(d)$ . Combining this with Theorem 3.4 completes the proof. At a high level, on the infinite tree of substrings $\\mathcal{A}^{\\star}$ we study which nodes are populated as tokens by LZW. This structure forms a Digital Search Tree (DST) and prior work analyzes the mean and variance of the profile of the DST under various source processes (Jacquet et al., 2001; Drmota and Szpankowski, 2011; Hun and Vall\u00e9e, 2014; Drmota et al., 2021). A detailed proof of Theorem 3.6 is provided in Appendix A.6. ", "page_idx": 7}, {"type": "text", "text": "4 Experimental Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Experiment 1 (Figures 4a and 4b) In this experiment we study the order-1 switching Markov chain. Transformers without tokenization empirically achieve a small cross-entropy on this learning task as seen in Figure 4a and earlier in Makkuva et al. (2024). We vary hyperparameters to find the smallest untokenized model which achieves a loss within $10\\%$ of the optimal-cross entropy within 300 epochs. Fixing a token dictionary size of 20, we also find the smallest tokenized model which achieves the same loss. Although the smallest model with tokenization is larger than the smallest model without tokenization in terms of the number of parameters, the wall-clock time taken to optimize the model to any target test loss is observed to be smaller. Thus, tokenization appears to reduce the compute time required to train the model to a target test loss in the toy example we consider. In Figure 4b we compare models with the same architecture trained with and without tokenization5. The model with tokenization appears to converge more quickly, although the limiting error achieved is subtly higher in comparison with the model without tokenization. ", "page_idx": 7}, {"type": "text", "text": "Experiment 1 (Figure 5). In this experiment, we train tokenizers on the Wikitext-103-raw-v1 dataset (Merity et al., 2016) and compare the performance of unigram models trained on the GLUE dataset as the model size scales. Since the character-level tokenizer operates on a fixed vocabulary, in order to compare with the other tokenizers, we plot the number of unique $k$ -grams observed in the training dataset along the $x$ -axis. While this is not an apples-to-apples comparison, we use the number of unique $k$ -grams in the dataset as a proxy for the complexity of the likelihood model trained. ", "page_idx": 7}, {"type": "text", "text": "One may also use the total number of possible $k$ -grams as a proxy; however a large fraction of these $k$ -grams would likely never be observed in a real dataset (especially as $k$ grows). ", "page_idx": 8}, {"type": "text", "text": "Experiment 2 (Table 1). In this experiment, we compare the cross entropy loss of the best unigram model trained on pre-trained tokenizers on an array of datasets. All the considered tokenizers have dictionary sizes in the range 31K-51K. The best bigram model under the character tokenizer is consistently outperformed by the best unigram likelihood model trained under a number of pretrained tokenizers on a variety of datasets: Rotten Tomatoes (8.5K sequences), GLUE (105K), Yelp review (650K) and Wikitext-103-v1 (1.8M). ", "page_idx": 8}, {"type": "image", "img_path": "wm9JZq7RCe/tmp/1412b432394daa8ca0c63000279bc060520b4032fdae7f5b50c55bf524363f7d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "wm9JZq7RCe/tmp/8c01db63f039b44d85968e142689d2240befd8b563ae5b514864fe3b25501f85.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 5: Performance vs. dictionary size. Tokenizers are trained on the Wikitext-103 dataset. For all other tokenizers we train unigram models while for the the character-level tokenizer, we train $k$ -gram models for $k\\in\\{1,2,3,4\\}$ . Likelihood models are trained on the GLUE dataset. The parentheses indicates the number of distinct observed $k$ -grams, which lower bounds the $k$ -gram model complexity. ", "page_idx": 8}, {"type": "text", "text": "Table 1: Cross-entropy loss estimates (using eq. (55)) of unigram models trained on pretrained tokenizers under a number of datasets. The last row (blue) is the character level tokenizer, on which a more powerful bigram model is trained. BERT is based on Wordpiece, and the remaining tokenizers are BPE based. The character-level tokenizer we use is ByT5. ", "page_idx": 8}, {"type": "text", "text": "4.1 Additional theoretical results ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We present some additional theoretical results in the appendix which we discuss briefly below. In Appendix B, we do a theoretical study of the cross-entropy loss achieved by the popular BPE tokenizer. We show that a variant of BPE achieves the upper bound on the RHS of eq. (3) (Theorem 3.1) up to a factor approaching 2 as the dictionary size grows. It is an interesting question for future research to understand whether this factor of 2 can be removed, since transformers are observed to achieve the near-optimal cross-entropy loss as the dictionary size grows (cf. Figure 3b). In Appendix C, we prove finite sample bounds on the end-to-end model under the LZW tokenizer with a smoothed empirical estimator as the unigram model. This analysis reveals that there is a sweet spot for the dictionary size - too small a dictionary, and the statistical error floor is significant, too large a dictionary, and the statistical error incurred by the likelihood model dominates the overall loss. We also take a closer look into the aspect of generalization for tokenizers, which arises from the fact that the tokenizer is evaluated on data that it was not trained on. Prior work such as Zouhar et al. (2023b) show that BPE is an approximation algorithm for finding the sequence of merges which minimizes the size of the compressed dataset. This does not imply any guarantees on the end-to-end performance, or even compression power of the tokenizer on new sequences. In particular, in Appendix D we show that there exist tokenizers which compress the dataset into a short sequence of tokens, but do so in a way which fails to generalize to new sequences. Thus measuring the performance of a tokenizer necessitates understanding its behavior on data it was not trained on. In Appendix E, we show a different kind of intricacy - there exist tokenizers under which the best unigram model achieves low cross-entropy loss. However, the same dictionary under a different encoding algorithm performs nearly as poorly as the character-level tokenizer. The interaction between the dictionary and encoding algorithm is a poorly studied subject in the tokenization literature; this result emphasizes the importance of understanding this relationship. ", "page_idx": 8}, {"type": "text", "text": "5 Open questions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this section, we discuss some limitations of our work and open questions stemming from them. We show that when transformers are trained with or without tokenization, they learn to approximately represent $k$ -gram models for different values of $k$ . Transformers are capable of representing far more complex behavior, which are elicited under more complex data generating processes. Extending our formulation to these settings presents an avenue to develop an even better understanding of tokenization, and would allow finer-grained comparisons between tokenizers. The behavior and role of tokenizers may be very different in these contexts. Below we discuss some concrete questions. ", "page_idx": 9}, {"type": "text", "text": "Our theory assumes that the underlying Markov chain has every transition occurring with non-zero probability, which is a limitation. However, the analysis for the toy tokenizer in eq. (2) shows that when the dictionary size scales as $\\exp(m H(\\pi)/H(\\bar{P}))$ , even in the absence of Assumption 3.2, the tokenizer achieves the optimal cross-entropy to within a factor of 2. This leads to the following conjecture. ", "page_idx": 9}, {"type": "text", "text": "Conjecture 1. In the spirit of eliminating Assumption 3.2, is it possible to establish a version of Theorem 3.1 applicable to data drawn from any Markov chain, where $\\varepsilon=\\log(1/\\delta)/0.99\\log(d)$ is replaced by $\\varepsilon\\stackrel{}{=}\\log(m H(\\pi)/H(P))/0.99\\log(d)$ . ", "page_idx": 9}, {"type": "text", "text": "In Appendix B, we analyze a variant of the BPE tokenizer, which carries out a version of sample splitting, and establish a weaker variant of Theorem 3.1 for this tokenizer. This is to simplify the statistical dependencies arising from the fact that while learning its dictionary, BPE makes a run over the entire training dataset each time a new token is added. It remains an open question to analyze and establish a variant of Theorem 3.1 for the standard BPE tokenizer. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We present a theoretical framework to compare and analyze different tokenization algorithms. We study the end-to-end cross-entropy loss of the tokenizer $^+$ likelihood model, and focus on the case where the data generating process is Markovian. We empirically observe that transformers with tokenization are drastically more efficient at learning $k^{\\mathrm{th}^{-}}$ -order Markov processes, compared to without tokenization. We prove that algorithms such as LZW and a sequential variant of BPE learn tokenizers such that the best unigram likelihood model trained on them approaches the cross-entropy loss of the optimal likelihood model, as the vocabulary size $d$ grows. ", "page_idx": 9}, {"type": "text", "text": "7 Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "JJ and NR were partially supported by NSF Grants IIS-1901252 and CCF-2211209. KR was partially supported by NSF Grant CCF-2211209. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Zaid Alyafeai, Maged S Al-shaibani, Mustafa Ghaleb, and Irfan Ahmad. Evaluating various tokenizers for arabic text classification. Neural Processing Letters, 55(3):2911\u20132933, 2023. ", "page_idx": 9}, {"type": "text", "text": "Dietrich Braess and Thomas Sauer. Bernstein polynomials and learning theory. Journal of Approximation Theory, 128(2):187\u2013206, 2004. ", "page_idx": 9}, {"type": "text", "text": "Yen-Chi Chen. Stochastic modeling of scientific data, Autumn 2018. ", "page_idx": 9}, {"type": "text", "text": "Jonathan H Clark, Dan Garrette, Iulia Turc, and John Wieting. Canine: Pre-training an efficient tokenization-free encoder for language representation. Transactions of the Association for Computational Linguistics, 10:73\u201391, 2022. ", "page_idx": 9}, {"type": "text", "text": "Yasuharu Den, Toshinobu Ogiso, Hideki Ogura, Atsushi Yamada, Nobuaki Minematsu, Kiyotaka Uchimoto, and Hanae Koiso. The development of an electronic dictionary for morphological analysis and its application to japanese corpus linguistics, Oct 2007. URL https://repository. ninjal.ac.jp/api/records/2201. ", "page_idx": 9}, {"type": "text", "text": "Michael Drmota and Wojciech Szpankowski. The expected proflie of digital search trees. Journal of Combinatorial Theory, Series A, 118(7):1939\u20131965, 2011.   \nMichael Drmota, Michael Fuchs, Hsien-Kuei Hwang, and Ralph Neininger. Node profiles of symmetric digital search trees: Concentration properties. Random Structures & Algorithms, 58(3): 430\u2013467, 2021.   \nBenjamin L Edelman, Ezra Edelman, Surbhi Goel, Eran Malach, and Nikolaos Tsilivis. The evolution of statistical induction heads: In-context learning markov chains. arXiv preprint arXiv:2402.11004, 2024.   \nTanja Eisner, B\u00e1lint Farkas, Markus Haase, and Rainer Nagel. Operator theoretic aspects of ergodic theory, volume 272. Springer, 2015.   \nPhilip Gage. A new algorithm for data compression. C Users Journal, 12(2):23\u201338, 1994.   \nSiavash Golkar, Mariel Pettee, Michael Eickenberg, Alberto Bietti, Miles Cranmer, Geraud Krawezik, Francois Lanusse, Michael McCabe, Ruben Ohana, Liam Parker, et al. xval: A continuous number encoding for large language models. arXiv preprint arXiv:2310.02989, 2023.   \nRobert M Gray and RM Gray. Probability, random processes, and ergodic properties, volume 1. Springer, 2009.   \nGregory Grefenstette and Pasi Tapanainen. What is a word, what is a sentence?: problems of tokenisation. 1994.   \nYanjun Han, Soham Jana, and Yihong Wu. Optimal prediction of markov chains with and without spectral gap. Advances in Neural Information Processing Systems, 34:11233\u201311246, 2021.   \nKanal Hun and Brigitte Vall\u00e9e. Typical depth of a digital search tree built on a general source. In 2014 Proceedings of the Eleventh Workshop on Analytic Algorithmics and Combinatorics (ANALCO), pages 1\u201315. SIAM, 2014.   \nItay Itzhak and Omer Levy. Models in a spelling bee: Language models implicitly learn the character composition of tokens. arXiv preprint arXiv:2108.11193, 2021.   \nPhilippe Jacquet, Wojciech Szpankowski, and Jing Tang. Average profile of the lempel-ziv parsing scheme for a markovian source. Algorithmica, 31:318\u2013360, 2001.   \nEugene Kharitonov, Marco Baroni, and Dieuwke Hupkes. How bpe affects memorization in transformers. arXiv preprint arXiv:2110.02782, 2021.   \nTaku Kudo. Subword regularization: Improving neural network translation models with multiple subword candidates. arXiv preprint arXiv:1804.10959, 2018.   \nN Jesper Larsson and Alistair Moffat. Off-line dictionary-based compression. Proceedings of the IEEE, 88(11):1722\u20131732, 2000.   \nJind\u02c7rich Libovick\\`y, Helmut Schmid, and Alexander Fraser. Why don\u2019t people use character-level machine translation? arXiv preprint arXiv:2110.08191, 2021.   \nAshok Vardhan Makkuva, Marco Bondaschi, Adway Girish, Alliot Nagle, Martin Jaggi, Hyeji Kim, and Michael Gastpar. Attention with markov: A framework for principled analysis of transformers via markov chains. arXiv preprint arXiv:2402.04161, 2024.   \nBen Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.   \nStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016.   \nJaouad Mourtada and St\u00e9phane Ga\u00efffas. An improper estimator with optimal excess risk in misspecified density estimation and logistic regression. The Journal of Machine Learning Research, 23(1): 1384\u20131432, 2022.   \nAssaf Naor, Shravas Rao, and Oded Regev. Concentration of markov chains with bounded moments. 2020.   \nGonzalo Navarro and Lu\u00eds MS Russo. Re-pair achieves high-order entropy. In DCC, page 537. Citeseer, 2008.   \nEshaan Nichani, Alex Damian, and Jason D Lee. How transformers learn causal structure with gradient descent. arXiv preprint arXiv:2402.14735, 2024.   \nDavid D Palmer. Tokenisation and sentence segmentation. Handbook of natural language processing, pages 11\u201335, 2000.   \nTerence Parr. The Definitive ANTLR 4 Reference. Pragmatic Bookshelf, Raleigh, NC, 2 edition, 2013. ISBN 978-1-93435-699-9. URL https://www.safaribooksonline.com/library/ view/the-definitive-antlr/9781941222621/.   \nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.   \nJessica Rumbelow and Matthew Watkins. Solidgoldmagikarp. https://www.alignmentforum. org/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation, 2023.   \nMike Schuster and Kaisuke Nakajima. Japanese and korean voice search. In 2012 IEEE international conference on acoustics, speech and signal processing (ICASSP), pages 5149\u20135152. IEEE, 2012.   \nRico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. In Katrin Erk and Noah A. Smith, editors, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715\u20131725, Berlin, Germany, August 2016. Association for Computational Linguistics. doi: 10.18653/v1/P16-1162. URL https://aclanthology.org/P16-1162.   \nAaditya K Singh and DJ Strouse. Tokenization counts: the impact of tokenization on arithmetic in frontier llms. arXiv preprint arXiv:2402.14903, 2024.   \nArseny Tolmachev, Daisuke Kawahara, and Sadao Kurohashi. Juman $^{++}$ : A morphological analysis toolkit for scriptio continua. In Eduardo Blanco and Wei Lu, editors, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 54\u201359, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-2010. URL https://aclanthology.org/D18-2010.   \nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. 2019. In the Proceedings of ICLR.   \nTerry A. Welch. A technique for high-performance data compression. Computer, 17(06):8\u201319, 1984.   \nLinting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, and Colin Raffel. Byt5: Towards a token-free future with pre-trained byte-to-byte models. Transactions of the Association for Computational Linguistics, 10:291\u2013306, 2022.   \nSangwon Yu, Jongyoon Song, Heeseung Kim, Seong-min Lee, Woo-Jong Ryu, and Sungroh Yoon. Rare tokens degenerate all tokens: Improving neural text generation via adaptive gradient gating for rare token embeddings. arXiv preprint arXiv:2109.03127, 2021.   \nWenqing Zheng, SP Sharan, Ajay Kumar Jaiswal, Kevin Wang, Yihan Xi, Dejia Xu, and Zhangyang Wang. Outline, then details: Syntactically guided coarse-to-fine code generation. In International Conference on Machine Learning, pages 42403\u201342419. PMLR, 2023.   \nJacob Ziv and Abraham Lempel. Compression of individual sequences via variable-rate coding. IEEE transactions on Information Theory, 24(5):530\u2013536, 1978. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Vil\u00e9m Zouhar, Clara Meister, Juan Gastaldi, Li Du, Mrinmaya Sachan, and Ryan Cotterell. Tokenization and the noiseless channel. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5184\u20135207, Toronto, Canada, July 2023a. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.284. URL https: //aclanthology.org/2023.acl-long.284. ", "page_idx": 12}, {"type": "text", "text": "Vil\u00e9m Zouhar, Clara Meister, Juan Luis Gastaldi, Li Du, Tim Vieira, Mrinmaya Sachan, and Ryan Cotterell. A formal perspective on byte-pair encoding. arXiv preprint arXiv:2306.16837, 2023b. ", "page_idx": 12}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Contents ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A Analysis of LZW: Proofs of Theorems 3.4 and 3.6 14 ", "page_idx": 13}, {"type": "text", "text": "A.1 Notation and definitions 14   \nA.2 A basic result about the optimal achievable cross-entropy loss . . 14   \nA.3 Proof of Theorem 2.1 . . 15   \nA.4 Maximum likelihood unigram model . . . . 15   \nA.5 Proof of Theorem 3.4 . . . 16   \nA.6 Heavy-hitter dictionaries and a proof of Theorem 3.6 17 ", "page_idx": 13}, {"type": "text", "text": "B Additional Theoretical Results I: A sequential variant of BPE 22 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "B.1 Analysis of Algorithm 1 . . 25   \nB.2 Analysis for the large dictionary case: $|\\mathsf{D i c t}|>d_{0}$ . . . 25   \nB.3 Analysis in the small dictionary case . 32 ", "page_idx": 13}, {"type": "text", "text": "C Additional Theoretical Results II: Learning the likelihood model 35 ", "page_idx": 13}, {"type": "text", "text": "C.1 Proof of Theorem C.1 . 36 ", "page_idx": 13}, {"type": "text", "text": "D Additional Theoretical Results III: The generalization ability of tokenizers 40 ", "page_idx": 13}, {"type": "text", "text": "E Additional Theoretical Results IV: Interaction between the dictionary and encoding algorithm 41 ", "page_idx": 13}, {"type": "text", "text": "E.1 Stochastic source and dictionary. . . 42   \nE.2 Minimal encoder achieves the optimal cross-entropy loss up to a constant. 42   \nE.3 Greedy-encoder achieves poor cross-entropy loss 44 ", "page_idx": 13}, {"type": "text", "text": "F Experiment details 46 ", "page_idx": 13}, {"type": "text", "text": "G NeurIPS Paper Checklist 48 ", "page_idx": 13}, {"type": "text", "text": "A Analysis of LZW: Proofs of Theorems 3.4 and 3.6 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Notation and definitions ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "For each character $a\\in{\\mathcal{A}}$ let $\\mathcal{T}_{a}^{\\star}$ denote an infinite tree, with root vertex $\\varnothing$ , and subsequent vertices labelled by strings $\\pmb{t}\\in\\mathcal{A}^{\\star}$ . The edge from a parent vertex $\\pmb{t}$ to any child $t a^{\\prime}$ is labelled with the probability $P(\\pmb{t}a^{\\prime}|\\pmb{t})$ unless $\\pmb{t}=\\emptyset$ , in which case the edge probability is $P(a^{\\prime}|a)$ . An infinite trajectory sampled on the tree $\\mathcal{T}_{a}^{\\star}$ corresponds to an infinite string sampled from the stochastic source conditioned on the first character of the string being $a$ . In this paper we only consider ergodic sources (Gray and Gray, 2009) for which we can define the \u201centropy rate\u201d. The entropy rate fundamentally captures the compressibility of the source, and can be defined as $\\begin{array}{r}{H_{\\infty}\\triangleq\\operatorname*{lim}_{m\\rightarrow\\infty}\\frac{1}{m}H(P)}\\end{array}$ m1H(P) where s is a length $m$ string drawn from the source. By Theorem 2.1, $H_{\\infty}$ , captures $\\operatorname*{min}_{Q}\\tilde{\\mathcal{L}}(Q)$ . ", "page_idx": 13}, {"type": "text", "text": "A.2 A basic result about the optimal achievable cross-entropy loss ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The ratio of $H(P)$ and $m H(\\pi)$ can be made arbitrarily large for the switching Markov chains in Figure 1 as the switching probabilities $p$ and $q$ approach 0 or 1. See Example A.1 for more details. Example A.1. Consider the switching Markov process in Figure 1 on $\\{0,1\\}$ with $p=q=1-\\delta$ . For this process, limm\u2192\u221em1 $\\begin{array}{r}{\\operatorname*{lim}_{m\\rightarrow\\infty}\\frac{1}{m}H(P)\\stackrel{*}{=}H_{\\mathsf{B e r}}(\\delta)\\stackrel{*}{=}\\delta\\log(1/\\delta)+(1-\\delta)\\log(1/(1-\\delta))}\\end{array}$ , but $\\pi=$ {1/2, 1/2} and so H(\u03c0) = HBer(1/2) = log(2). The ratio limm\u2192\u221emHH(P( \u03c0)) goes to $\\infty$ as $\\delta\\rightarrow0$ . ", "page_idx": 13}, {"type": "text", "text": "A.3 Proof of Theorem 2.1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We first characterize the minimum achievable cross-entropy loss ${\\mathcal{L}}_{m}(Q)$ without any restrictions on the likelihood model class $\\mathcal{Q}$ . Choosing $Q(\\mathtt{e n c}(s))\\overset{\\mathtt{=}}{=}Q(s)\\,=\\,\\dot{P}(\\dot{s})$ , the true probability of the sequence $\\pmb{s}$ , we have $\\begin{array}{r}{\\mathcal{L}_{m}(Q\\circ\\mathsf{e n c}(\\cdot))\\,=\\,H(\\pmb{s})}\\end{array}$ where $H(\\cdot)$ is the entropy function. It is not that difficult to see that this is also the minimum cross-entropy loss that can be achieved. For any distribution $Q$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathcal{L}_{m}(Q)=\\mathbb{E}[\\log(1/Q(s)]}&{}\\\\ {=\\mathbb{E}[\\log(P(s)/Q(s)]+\\mathbb{E}[\\log(1/P(s))]}\\\\ {=H(P)+D_{\\mathrm{KL}}(P\\|Q).}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "On the other hand, the cross-entropy loss under any unigram model $Q\\in\\mathcal{Q}_{1-\\mathrm{gram}}$ satisfies, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{m}\\mathcal{L}_{m}(Q\\circ\\mathsf{e n c}(\\cdot))\\overset{(i)}{=}-\\cfrac{1}{m}\\displaystyle\\sum_{i=1}^{m}\\mathbb{E}[\\log Q_{\\mathrm{tok}}(t_{i})]-\\frac{1}{m}\\mathbb{E}[\\log Q_{\\#}(m)]}\\\\ &{\\qquad\\qquad\\qquad\\overset{(i i)}{\\geq}-\\displaystyle\\sum_{a\\in A}\\pi(a)\\log Q_{\\mathrm{tok}}(a)}\\\\ &{\\qquad\\qquad\\geq H(\\pi)}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where in $(i)$ , we use the definition of the unigram model $Q$ , and in $(i i)$ , $\\pi$ is the stationary distribution over characters induced by the stochastic source, and the ergodicity of the source is used. The last equation lower bounds $H(X,Y)\\geq H(X)$ . ", "page_idx": 14}, {"type": "text", "text": "A.4 Maximum likelihood unigram model ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A number of our results (Theorems 3.4 and 3.6 to name a few) are related to bounding $\\begin{array}{r}{\\operatorname*{min}_{Q\\in{\\mathcal Q}_{1\\mathrm{-gram}}}{\\mathcal L}(Q\\circ\\mathsf{e n c}(\\cdot))}\\end{array}$ for some tokenizer $\\tau$ . In this section we introduce the maximum likelihood unigram model which captures the optimizer over $Q$ for any given tokenizer. ", "page_idx": 14}, {"type": "text", "text": "For the character level tokenizer, an examination of Theorem 2.1 shows that the optimal unigram likelihood model associates probability $Q_{\\mathrm{tok}}(a)\\;=\\;\\pi(a)$ , i.e. the limiting fraction of times the character $a$ is observed in the sequence. More generally, for a non-trivial tokenizer, the corresponding optimal unigram model $Q_{\\mathrm{tok}}^{\\star}(t)$ ends up being the limiting expected fraction of times $\\pmb{t}$ is observed in an encoding of a sequence. This is the maximum likelihood unigram model, which we formally define below. The unigram MLE likelihood model associates probability, ", "page_idx": 14}, {"type": "equation", "text": "$$\nQ_{\\mathrm{MLE}}(t)\\gets\\operatorname*{lim}_{m\\rightarrow\\infty}\\mathbb{E}\\left[\\frac{n_{t}}{\\sum_{t}n_{t}}\\right]\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "to each token, where $n_{t}$ is the random variable capturing the number of occurrences of the token $\\pmb{t}$ in the encoding of the length- $\\cdot m$ string $\\pmb{s}$ . Restricting the class of likelihood models to the unigram models, $\\mathcal{Q}_{1-\\mathrm{gram}}$ , $Q_{\\mathrm{MLE}}$ captures the model which minimizes eq. (1). ", "page_idx": 14}, {"type": "text", "text": "The unigram MLE model cannot be computed without an infinite amount of data, but can be approximated well with a finite amount of data, which forms the basis for Theorem C.1. For certain encoding algorithms, we can show that the quantity $n_{t}/\\sum_{t}n_{t}$ asymptotically converges to its expectation (Lemma A.4). This is the reason the unigram  model in eq. (4) is referred to as a \u201cmaximum likelihood\u201d model, since $\\begin{array}{r}{\\operatorname*{lim}_{m\\to\\infty}n_{t}/\\sum_{t}n_{t}}\\end{array}$ is the limit as $|s|=m\\rightarrow\\infty$ of the solution to the following likelihood maximization problem: given a sequence $\\pmb{s}$ , find the distribution over tokens, $Q$ , which maximizes ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\prod_{t\\in\\mathsf{e n c}(s)}Q(t)\\equiv\\prod_{t\\in\\mathsf{D i c t}}\\left(Q(t)\\right)^{n_{t}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "As discussed previously, the unigram MLE model over tokens in eq. (4) induces a joint distribution over sequences of tokens by looking at the product of the marginal probabilities of the composed tokens; in particular, ", "page_idx": 14}, {"type": "equation", "text": "$$\nQ_{\\mathrm{MLE}}(t_{1},\\cdot\\cdot\\cdot\\cdot,t_{j})=Q_{\\mathrm{MLE}}(j)\\prod_{i=1}^{j}Q_{\\mathrm{MLE}}(t_{i}),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $Q_{\\mathrm{MLE}}(j)$ is a distribution on the total number of tokens generated and is instantiated as $\\mathrm{Unif}([m])$ . ", "page_idx": 15}, {"type": "text", "text": "Remark A.2. Note that the unigram MLE model specifies a distribution over tokens which is a function of the underlying encoding algorithm, enc(\u00b7). Different encoders result in different population level distributions over tokens, and consequently different unigram MLE models. ", "page_idx": 15}, {"type": "text", "text": "Definition A.3 (greedy encoder). Given a dictionary Dict, the greedy encoder $\\mathsf{e n c}_{\\mathrm{gre}}(s)$ encodes a string $\\pmb{s}$ into tokens by greedily matching from left to right, the largest substring that exists as a token in Dict. This substring is then removed and the process iterated on the remainder of $\\pmb{s}$ . The greedy decoder $\\mathsf{d e c}_{\\mathrm{gre}}(\\cdot)$ is a lookup table - a sequence of tokens is decoded by replacing each occurrence of a token by the corresponding substring it maps to in Dict. ", "page_idx": 15}, {"type": "text", "text": "Lemma A.4. limm\u2192\u221e $\\begin{array}{r}{\\operatorname*{lim}_{m\\to\\infty}\\frac{n_{t}}{\\sum_{t^{\\prime}}n_{t^{\\prime}}}\\overset{a.s.}{=}\\operatorname*{lim}_{m\\to\\infty}\\mathbb{E}\\left[\\frac{n_{t}}{\\sum_{t^{\\prime}}n_{t^{\\prime}}}\\right]}\\end{array}$ for any tokenizer having a finite vocabulary and finitely long tokens, using the greedy encoder. ", "page_idx": 15}, {"type": "text", "text": "Proof. This result is essentially true because under the greedy encoder, the tokens in an encoding of a fresh string $\\pmb{t}$ may be generated by an $r^{t h}$ -order Markov process for some $r$ . For such processes, the Ces\u00e0ro average of the state distributions converges to a stationary distribution of the process (i.e., the Krylov\u2013Bogolyubov argument). ", "page_idx": 15}, {"type": "text", "text": "Tokens are generated as follows. Suppose the previous tokens generated were $t_{1},t_{2},\\cdots\\,,t_{i}$ . The next token $\\pmb{t}_{i+1}$ is sampled by drawing an infinite trajectory from $\\mathcal{T}_{a}^{\\star}$ for $a\\sim P(\\cdot|t_{i})$ and returning the longest prefix $\\pmb{t}$ of this trajectory which is a token in Dict, conditional on satisfying the conditions, $t_{j}t_{j+1}\\cdot\\cdot\\cdot t_{i}t\\notin{\\sf D i}$ ct for all $\\dot{j}\\in\\{1,2,\\cdots\\,,i\\}$ . This process is repeated sequentially to generate all the tokens. ", "page_idx": 15}, {"type": "text", "text": "Suppose the length of the longest token in the dictionary is $\\ell_{\\mathrm{max}}$ . Then, the distribution from a which a token is sampled depends on at most the previous $\\ell_{\\mathrm{max}}$ tokens. The reason for this is that the dependency of the $(i+1)^{t h}$ token, $\\pmb{t}_{i+1}$ , on the previously sampled tokens emerges in the constraint $t_{j}t_{j+1}\\cdot\\cdot\\cdot t_{i}t_{i+1}\\notin\\mathsf{D i c t}$ , satisfied by any candidate $\\pmb{t}_{i+1}$ . Since each token is of length at least one, this condition is vacuously satisfied if $j<i-\\ell_{\\mathrm{max}}$ . ", "page_idx": 15}, {"type": "text", "text": "With this view, the evolution of the state, defined as $\\mathsf{s t a t e}_{r}\\,=\\,\\bigl(t_{r\\ell_{\\mathrm{max}}},t_{r\\ell_{\\mathrm{max}}-1},\\cdot\\cdot\\cdot\\,,t_{(r-1)\\ell_{\\mathrm{max}}}\\bigr)$ evolves in a Markovian fashion. By the Krylov\u2013Bogolyubov argument (cf. Proposition 4.2 in Chen (2018)), the time averaged visitation frequencies of a Markov chain coordinate-wise asymptotically converges to its expectation, almost surely. This expectation exists by Theorems 8.5 and 8.22 of Eisner et al. (2015) which shows that for a matrix $A$ such that $\\operatorname*{sup}_{t\\in\\mathbb{N}}\\|A^{t}\\|_{\\mathrm{op}}<\\infty$ the limit $\\begin{array}{r}{\\operatorname*{lim}_{t\\to\\infty}\\frac{1}{t}\\sum_{i=1}^{t}A^{i}}\\end{array}$ exists. For the finite-state Markov transition $A$ which captures the token generation process, condition $\\begin{array}{r}{\\operatorname*{sup}_{t\\in\\mathbb{N}}\\|A^{t}\\|_{\\mathrm{op}}\\;\\leq\\;|\\mathsf{D i c t}|^{\\ell_{\\operatorname*{max}}}\\;<\\;\\infty}\\end{array}$ . This means that the limit of the time averaged state distribution exists. Moreover, for any initial distribution $\\pi_{0}$ over tokens, $\\begin{array}{r}{\\pi=\\operatorname*{lim}_{t\\to\\infty}\\frac{1}{t}\\sum_{i=1}^{t}\\pi_{0}A^{i}}\\end{array}$ satisfies the condition $\\pi A=\\pi$ , implying that the limiting time-averaged state distribution is a stationary distribution of $A$ . Since the limiting time-averaged measure on the state $\\mathbf{state}_{r}=(t_{r\\ell_{\\mathrm{max}}},\\cdot\\cdot\\cdot\\cdot,t_{r\\ell_{\\mathrm{max}}-1},\\cdot\\cdot\\cdot\\cdot,t_{(r-1)\\ell_{\\mathrm{max}}})$ exists, this implies that the limiting timeaveraged measure of $t_{r\\ell_{\\mathrm{max}}-r^{\\prime}}$ for each $r^{\\prime}\\in\\{0,1,\\cdots\\,,\\ell_{\\mathrm{max}}\\}$ exists. By taking the uniform average over $r^{\\prime}$ and $r$ , the limiting time-averaged measure of $\\pmb{t}_{i}$ over $i\\in\\mathbb N$ exists. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "A.5 Proof of Theorem 3.4 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Consider a string $\\pmb{s}$ of length $m\\ \\rightarrow\\ \\infty$ which is encoded into a sequence of tokens $(t_{i}~:~i~\\in$ $[|\\mathsf{e n c}_{\\mathrm{gre}}(\\pmb{s})|])$ . By the Asymptotic Equipartition Property (AEP) for ergodic sources, i.e. the Shannon\u2013McMillan\u2013Breiman theorem, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathrm{Pr}\\left(\\operatorname*{lim}_{m\\rightarrow\\infty}-\\frac{1}{m}\\log P(\\pmb{s})=H_{\\infty}\\right)=1.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Here limm\u2192\u221e $\\begin{array}{r}{\\operatorname*{lim}_{m\\to\\infty}\\frac{H(P)}{m}}\\end{array}$ H(mP )also happens to be the entropy rate of the source. We use this property to bound the length of the greedy encoding, $|\\mathsf{e n c}_{\\mathrm{gre}}(s)|$ . Indeed, the probability of may be decomposed as, ", "page_idx": 15}, {"type": "equation", "text": "$$\nP(s)=P(t_{1})\\prod_{i=2}^{|\\mathsf{e n c}_{\\mathsf{g r e}}(s)|}P\\big(t_{i}|t_{i-1}\\big)\\leq\\prod_{i=1}^{|\\mathsf{e n c}_{\\mathsf{g r e}}(s)|}\\operatorname*{max}_{a\\in\\mathcal{A}}P\\big(t_{i}|a\\big).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Noting that $\\delta\\operatorname*{min}_{a}P(\\mathbf{t}|a)\\geq\\operatorname*{max}_{a}P(\\mathbf{t}|a)$ , up to a $\\delta$ factor we may replace the max over $a$ by an expectation over $a\\sim\\pi$ where $\\pi$ is the stationary distribution of the stochastic source. In particular, ", "page_idx": 16}, {"type": "equation", "text": "$$\nP(s)\\leq\\prod_{i=1}^{|\\mathsf{e n c}_{\\mathrm{gre}}(s)|}P(\\pmb{t}_{i})/\\delta.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "By invoking the AEP, eq. (5), ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{m\\to\\infty}\\frac{1}{m}\\sum_{i=1}^{|\\mathsf{e n c}_{\\mathrm{gre}}(s)|}-\\log\\left(P(t_{i})\\right)-\\log(1/\\delta)\\right)\\overset{\\mathrm{a.s.}}{\\leq}H_{\\infty}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Recall that the greedy encoder satisfies Lemma A.4 and for any t \u2208Dict, limm\u2192\u221e|encngrte(s)| $Q_{\\mathrm{MLE}}(t)$ . Furthermore, note that for any token $\\pmb{t}\\in$ Dict, $P(t)\\,>\\,\\delta^{|t|}\\,>\\,0$ , and $|\\mathtt{e n c}_{\\mathtt{g r e}}(s)|\\,\\leq\\,m$ surely. By almost sure convergence, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{m\\rightarrow\\infty}{\\operatorname*{lim}}\\frac{\\vert\\mathsf{e n c}_{\\mathrm{gre}}(s)\\vert}{m}\\sum_{t\\in\\mathsf{D i c t}}-\\frac{n_{t}}{\\vert\\mathsf{e n c}_{\\mathrm{gre}}(s)\\vert}\\left(\\log\\left(P(t)-\\log(1/\\delta)\\right)\\right)}\\\\ &{\\qquad\\overset{\\mathrm{a.s.}}{=}\\underset{m\\rightarrow\\infty}{\\operatorname*{lim}}\\frac{\\vert\\mathsf{e n c}_{\\mathrm{gre}}(s)\\vert}{m}\\left(H(Q_{\\mathrm{MLE}},P)-\\log(1/\\delta)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Furthermore, utilizing the assumption that $\\varepsilon H(Q_{\\mathrm{MLE}},P)\\geq\\log(1/\\delta)$ satisfied by the tokenizer, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{m\\to\\infty}\\frac{(1-\\varepsilon)|\\mathsf{e n c}_{\\mathrm{gre}}(\\mathbf{s})|\\left(H(Q_{\\mathrm{MLE}},P)\\right)}{m}\\overset{\\mathrm{a.s.}}{\\leq}H_{\\infty}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Now we are ready to bound the expected cross-entropy loss of the tokenizer. Define the unigram model $\\begin{array}{r}{P_{\\pi}(t_{1},t_{2},\\cdot\\cdot\\cdot\\cdot,\\dot{t_{j}})=P_{\\mathrm{unif}}(j)\\prod_{i=1}^{\\bar{j}}P(t_{i})}\\end{array}$ where $P_{\\mathrm{unif}}$ is the uniform measure over $[m]$ . Note that we have the inequality $\\begin{array}{r}{\\operatorname*{min}_{Q\\in\\mathscr{Q}_{1:\\mathrm{gam}}}\\operatorname*{lim}_{m\\rightarrow\\infty}\\frac{1}{m}\\mathscr{L}_{m}(Q\\circ\\mathsf{e n c}_{\\mathsf{g r e}}(\\cdot))\\leq\\operatorname*{lim}_{m\\rightarrow\\infty}\\frac{1}{m}\\mathscr{L}_{m}(P_{\\pi}\\circ\\mathsf{e n c}_{\\mathsf{g r e}}(\\cdot))}\\end{array}$ and therefore, it suffices to upper bound the RHS. In particular, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{m}(P_{\\pi}\\circ\\mathsf{e n c}_{\\mathsf{g r e}}(\\cdot))=-\\mathbb{E}[\\log P_{\\mathsf{u n i f}}(|\\mathsf{e n c}_{\\mathsf{g r e}}(s)|)]-\\mathbb{E}\\left[\\sum_{t\\in\\mathsf{e n c}_{\\mathsf{g r e}}(s)}\\log\\left(P(t)\\right)\\right]}\\\\ &{\\qquad\\qquad\\qquad\\leq\\log(m)-\\mathbb{E}\\left[\\sum_{t\\in\\mathsf{e n c}_{\\mathsf{g r e}}(s)}\\log\\left(P(t)\\right)\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the last inequality uses the fact that $P_{\\mathrm{unif}}(|\\mathsf{e n c}_{\\mathrm{gre}}(s)|)\\,=\\,1/m$ . Note that as $m\\rightarrow\\infty$ , by assumption on the tokenizer, the fraction of times the token $\\pmb{t}$ appears in the encoding of $\\pmb{s}$ converges almost surely converges to $Q_{\\mathrm{MLE}}(t)$ . Since $|\\mathtt{e n c}_{\\mathrm{gre}}(s)|\\,\\leq\\,m$ surely and $P(t)\\,>\\,{\\bar{\\delta}}^{|t|}\\,>\\,0$ , by an application of the Dominated Convergence Theorem, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{-\\operatorname*{lim}_{m\\rightarrow\\infty}\\frac{1}{m}\\mathbb{E}\\left[\\sum_{t\\in\\mathsf{e n c}_{p c}(s)}\\log\\left(P(t)\\right)\\right]=-\\operatorname*{lim}_{m\\rightarrow\\infty}\\frac{1}{m}\\mathbb{E}\\left[\\mathsf{e n c}_{\\mathsf{g r e}}(s)\\vert\\cdot\\sum_{t\\in\\mathsf{D i c}}Q_{\\mathsf{M L E}}(t)\\log\\left(P(t)\\right)\\right]}}\\\\ &{}&{=\\operatorname*{lim}_{m\\rightarrow\\infty}\\frac{1}{m}\\mathbb{E}\\left[\\mathsf{l e n c}_{\\mathsf{g r e}}(s)\\vert H(Q_{\\mathsf{M L E}},P)\\right]\\qquad\\qquad\\qquad(8)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Combining eq. (7) with eq. (8) and setting $\\begin{array}{r}{\\operatorname*{lim}_{m\\rightarrow\\infty}\\log(m)/m=0}\\end{array}$ , and invoking eq. (6), ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{Q\\in\\mathcal{Q}_{1:\\mathrm{grom}}}{\\operatorname*{min}}\\underset{m\\rightarrow\\infty}{\\operatorname*{lim}}\\frac{1}{m}\\mathcal{L}_{m}(Q_{\\mathrm{MLE}}\\circ\\mathsf{e n c}_{\\mathrm{gre}}(\\cdot))=\\underset{m\\rightarrow\\infty}{\\operatorname*{lim}}\\frac{1}{m}\\mathbb{E}\\left[|\\mathsf{e n c}_{\\mathrm{gre}}(s)|H(Q_{\\mathrm{MLE}},P)\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\frac{H_{\\infty}}{1-\\varepsilon}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "minQ limm\u2192\u221em1 uBsye sT thheeo rfeacmt  t2.h1a,t  twhee  hsaovuer tchea it $\\begin{array}{r}{\\operatorname*{min}_{Q}\\operatorname*{lim}_{m\\rightarrow\\infty}\\frac{1}{m}\\mathcal{L}_{m}(Q\\circ\\mathsf{e n c}_{\\mathsf{g r e}}(\\cdot))=\\operatorname*{lim}_{m\\rightarrow\\infty}\\frac{H(P)}{m_{\\cdot}}=H_{\\infty}}\\end{array}$ , which ", "page_idx": 16}, {"type": "text", "text": "A.6 Heavy-hitter dictionaries and a proof of Theorem 3.6 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section we prove Theorem 3.6 and introduce the notion of a heavy-hitting dictionary. At a high level, these dictionaries contain all the substrings which have reasonably high probability of being observed many times in a dataset of size $n=\\overbar{\\Omega}_{\\delta}(d)$ . We first show in Lemma A.6 that heavy hitting dictionaries generalize well in the sense of having $H(Q_{\\mathrm{MLE}},P)$ being large (in conjunction with Theorem 3.4 this implies an upper bound on the cross-entropy loss of the best unigram model). Next, we will prove that the LZW algorithm (Definition 3.5) results in a heavy hitting dictionary with high probability. ", "page_idx": 16}, {"type": "image", "img_path": "wm9JZq7RCe/tmp/5787bb815c0c9f67f76186192ea1d81fa4100705491d520b0372e2a6be24b545.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Figure 6: The circled nodes indicates substrings which are tokens in Dict. Red nodes indicate the set of \u201cmaximal tokens\u201d, which are the set of tokens which the greedy encoder assigns, leaving out those which can only be assigned as the last token of some string. Tokens like $\\mathit{b}\\mathit{\\Omega}^{,,}$ are never assigned by the greedy encoder (save as the last token of the encoding of a string) since any sufficiently long trajectory starting with $b$ must have a longer prefix which is also a token, namely, one of ba, bc, bba, $b b b$ or $b b c$ . The vertices of the tree which are assigned by the greedy encoder as tokens (together with all their prefixes) forms a cut of the tree, which marks the dotted red line. The heavy hitting property asserts that this cut is uniformly far away from the root node $\\varnothing$ , and that every vertex $\\pmb{s}$ marked red has $P(s)\\leq1/d^{\\beta}$ . ", "page_idx": 17}, {"type": "text", "text": "Definition A.5 $\\beta$ -heavy-hitting dictionary). A token $\\pmb{t}$ of a dictionary is said to be maximal if there exists an arbitrary substring containing $\\pmb{t}$ as a strict prefix, and in addition, $\\pmb{t}$ is also the largest prefix of the substring which is a token. A dictionary Dict is said to be $\\beta$ -heavy hitting if the set of maximal tokens is a subset of $\\{s^{\\prime}:\\operatorname*{max}_{a\\in\\mathcal{A}}P(s^{\\prime}|a)\\overset{\\cdot}{\\leq}1/d^{\\beta}\\}$ . ", "page_idx": 17}, {"type": "text", "text": "A pictorial depiction of the heavy hitting property is illustrated in Figure 6. ", "page_idx": 17}, {"type": "text", "text": "Lemma A.6. For a $\\beta$ -heavy-hitting dictionary, with the greedy encoder, $H(Q_{M L E},P)\\geq\\beta\\log(d)$ . ", "page_idx": 17}, {"type": "text", "text": "Proof. Note that the greedy encoder assigns tokens only among the set of maximal substrings (save for potentially the last token). If every maximal substring has $\\begin{array}{r}{\\operatorname*{max}_{a\\in\\mathcal{A}}P(\\pmb{\\mathscr{s}}|a)\\leq1/d^{\\beta}}\\end{array}$ , by the heavy-hitting property, for any token $\\pmb{t}$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\nP(t)\\leq\\operatorname*{max}_{a\\in\\mathcal{A}}P(\\pmb{s}^{\\prime}|a)\\leq1/d^{\\beta}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, ", "page_idx": 17}, {"type": "equation", "text": "$$\nH(Q_{\\mathrm{MLE}},P)=\\mathbb{E}_{t\\sim Q_{\\mathrm{MLE}}}[\\log(1/P(t))]\\geq\\beta\\log(d).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Define $\\mathcal{M}_{\\beta}=\\{t:\\operatorname*{max}_{a\\in\\mathcal{A}}P(t|a)\\geq\\delta/d^{\\beta}\\}$ . These are the set of \u201chigh-probability\u201d substrings under the stochastic source. We will show that for $\\beta$ bounded away from 1, with high probability, every substring in $\\mathcal{M}_{\\beta}$ is added as a token to the dictionary in a run of the LZW tokenizer (Definition 3.5). Note that if every substring in $\\mathcal{M}_{\\beta}$ is assigned as a token by LZW, then the algorithm must be $\\beta$ -heavy hitting since there always exists a maximal token on the \u201cboundary\u201d of the set $\\mathcal{M}_{\\beta}$ which is strictly contained in $\\{s^{\\prime}:\\operatorname*{max}_{a\\in A}P(s^{\\prime}|a)\\leq1/d^{\\beta}\\}$ . ", "page_idx": 17}, {"type": "text", "text": "Proof. Note that $\\mathrm{min}_{a,a^{\\prime}\\in{\\cal A}}\\,P(a|a^{\\prime})=\\delta$ , which implies that the probability of any transition must be bounded away from 1, i.e., $\\mathrm{max}_{a,a^{\\prime}\\in A}\\,P(a|a^{\\prime})\\leq1-\\delta$ . This implies that, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{a\\in\\mathcal{A}}P(t|a)\\leq(1-\\delta)^{|t|}\\leq e^{-\\delta|t|}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "By definition, for any substring $\\pmb{t}\\in\\mathcal{M}_{\\beta}$ , $\\mathrm{max}_{a\\in\\mathcal{A}}\\,P(t|a)\\geq\\delta/d^{\\beta}$ . In conjunction with eq. (10), this implies the statement of the lemma. ", "page_idx": 18}, {"type": "text", "text": "In the remainder of this section, let $n$ be the size of the dataset on which LZW is run. We show that the number of tokens added to the dictionary by LZW, $d$ , is $\\widetilde{\\Theta}_{\\delta}(n)$ . Rather than running the algorithm with early stopping (i.e., ceasing to add new tokens once t he budget is hit), instead, we assume that the algorithm runs on a prefix of the dataset of length $d$ . The number of tokens added this way cannot exceed $d$ . ", "page_idx": 18}, {"type": "text", "text": "Lemma A.8. With probability $\\geq1-d^{-\\Omega(\\log(d/\\delta)/\\delta)}$ , in a run of the LZW algorithm, no substring t added as a token to the dictionary satisfies $|t|\\geq\\ell_{\\mathrm{max}}\\triangleq4\\log(d|A|)/\\delta$ . ", "page_idx": 18}, {"type": "text", "text": "Proof. Consider any $s\\in\\mathbb N$ and any substring $\\pmb{t}$ of length $s$ . In order for $\\pmb{t}$ to be assigned as a token, each of its prefixes must disjointly appear at least once in the string. Since there are at most $d$ tokens, we can upper bound the probability that $\\pmb{t}$ is assigned as a token as, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{P(t\\mathrm{~is~assigned~as~a~token})\\le\\binom{d}{s}\\prod_{i=1}^{s}\\operatorname*{max}_{a\\in\\mathcal{A}}P(t_{1:i}|a)}}\\\\ &{}&{\\stackrel{(i)}{\\le}\\binom{d}{s}(1-\\delta)^{s(s-1)/2}}\\\\ &{}&{\\leq e^{s\\log(d)-\\delta s(s-1)/2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $(i)$ uses the fact that $\\begin{array}{r}{\\operatorname*{max}_{a\\in{\\cal{A}}}P(t_{1:i})\\;\\leq\\;\\prod_{j=1}^{i}\\operatorname*{max}_{a\\in{\\cal{A}}}P(t_{j}|a)\\;\\leq\\;(1\\,-\\,\\delta)^{i}\\;}\\end{array}$ . By union bounding across the $|\\mathcal{A}|^{s}$ strings of length $s$ , ", "page_idx": 18}, {"type": "text", "text": "When $s=4\\log(d\\vert A\\vert)/\\delta\\!+\\!1\\triangleq\\ell_{\\mathrm{max}}+1$ , the RHS is upper bounded by $e^{-\\delta\\ell_{\\mathrm{max}}^{2}/4}\\leq d^{-\\Omega(\\log(d/\\delta)/\\delta)}$ . With the same small probability, no substring of length $s^{\\prime}>s$ can become a token, since their length- $s$ prefixes are never assigned as tokens. ", "page_idx": 18}, {"type": "text", "text": "Corollary A.9. With probability $\\geq1-d^{-\\Omega_{\\delta}(\\log(d))}$ , learns a dictionary with at least $d^{\\star}=d/\\ell_{\\mathrm{max}}$ tokens when run on a training sequence of length $n$ drawn from a stochastic source satisfying Assumption 3.2. ", "page_idx": 18}, {"type": "text", "text": "Lemma A.10. For any constant $\\beta<1$ , with probability $\\geq1-d^{-\\Omega(\\log(d/\\delta)/\\delta)}-\\exp(-\\widetilde{\\Omega}_{\\delta}(d^{1-\\beta}))$ over the source dataset, every substring in $\\mathcal{M}_{\\beta}$ is added as a token to the dictionary in a run of the LZW algorithm. In other words, with the same probability, the LZW tokenizer results in a $\\beta$ -heavy hitting dictionary. ", "page_idx": 18}, {"type": "text", "text": "By Corollary A.9, note that with high probability the LZW tokenizer adds at least $d^{\\star}$ tokens to the dictionary when processing a length $d$ training sequence in entirety. In this proof, instead of generating $d$ samples, we sequentially sample $d^{\\star}$ tokens from their joint distribution, and generate a dictionary from these samples. From Corollary A.9, with high probability this results in at most $d$ samples being generated, implying that the dictionary generated by sampling $d^{\\star}$ tokens is a subset of the dictionary generated by a full run of the LZW tokenizer. Here, we use the fact that the LZW tokenizer adds tokens to the dictionary in a left to right fashion, and therefore a subset of the dictionary learnt by the LZW tokenizer can be generated by processing a portion of the dataset. ", "page_idx": 18}, {"type": "text", "text": "Next we consider a joint view for generating the dataset from the stochastic source and the dictionary learnt by LZW simultaneously. The stochastic source is sampled as a sequence of tokens. Suppose the last character of the previous token was $a^{\\prime}$ . Sample a character $a\\,\\sim\\,P(\\cdot|a^{\\prime})$ and an infinite trajectory on the tree $\\mathcal{T}_{a}^{\\star}$ . Consider the first node visited in this trajectory which does not already exist as a token in the dictionary. The substring corresponding to this node is added as a token in the dictionary. By repeating this process, the dictionary and the source dataset are constructed sequentially and simultaneously. As alluded to before, we truncate this token sampling process to repeat at most $d^{\\star}$ times, which results in a subset of the dictionary output by the LZW algorithm with high probability (Corollary A.9). This is simply a variant of the \u201cPoissonization\u201d trick to avoid statistical dependencies across tokens. Denote the set of infinite trajectories generated on the forest $\\{T_{a}^{\\star}:a\\in\\mathcal{A}\\}$ as $\\{\\mathsf{t r a j}_{i}:i\\in[d^{\\star}]\\}$ . ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "With this view of the sampling process, observe that if the substring $\\pmb{t}$ sampled was a prefix of $\\mathsf{t r a j}_{i}$ at least $\\left|t\\right|$ times across different values of $i$ , then $\\pmb{t}$ must be assigned as a token. In particular, in each of these $|t|$ trajectories, each of the prefixes of $\\pmb{t}$ is assigned as a token. With this observation, the event that $\\pmb{t}$ is not assigned as a token is contained in the event that $\\pmb{t}$ is visited at most $|t|-1$ times across the $d^{\\star}$ trajectories. Observe that, ", "page_idx": 19}, {"type": "equation", "text": "$$\nP(t\\mathrm{~is~not~assigned~as~a~token})\\leq\\sum_{i=0}^{|t|-1}\\binom{d^{\\star}}{i}\\operatorname*{max}_{a\\in A}(P(t|a))^{i}(1-P(t|a))^{d^{\\star}-i}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Since we aim to upper bound this probability across the substrings in $\\pmb{t}~\\in~\\mathcal{M}_{\\beta}$ , note that $(i)$ $\\begin{array}{r}{\\operatorname*{max}_{a\\in\\mathcal{A}}P(t|a)\\geq\\delta/d^{\\beta}}\\end{array}$ , and $(i i)$ tokens in $\\mathcal{M}_{\\beta}$ have length at most $\\ell_{\\star}=\\delta^{-1}(\\beta\\log(d)+\\log(1/\\delta))$ (Lemma A.7), implying there are at most $2|\\mathcal{A}|^{\\ell_{\\star}}$ substrings in this set. By union bounding, ", "page_idx": 19}, {"type": "equation", "text": "$$\nP(\\exists t\\in\\mathcal{M}_{\\beta}\\mathrm{~not~assigned~as~a~token})\\leq2|A|^{\\ell_{\\star}}\\sum_{i=0}^{\\ell_{\\star}-1}{\\binom{d^{\\star}}{i}}\\operatorname*{max}_{x\\geq\\delta/d^{\\beta}}x^{i}(1-x)^{d^{\\star}-i}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Case I. For $i\\leq\\ell_{\\star}$ and $x\\geq1/2$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|A|^{\\ell_{\\star}}\\binom{d^{\\star}}{i}x^{i}(1-x)^{d^{\\star}-i}\\leq|A|^{\\ell_{\\star}}\\frac{(d^{\\star})^{\\ell_{\\star}}}{2^{d^{\\star}/2}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq2^{\\ell_{\\star}\\log(d^{\\star}|A|)-d^{\\star}/2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq2^{-\\Omega_{\\beta,\\delta}(d^{\\star})},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the last inequality uses the fact that $\\ell_{\\star}=O_{\\beta,\\delta}(\\log(d))$ . ", "page_idx": 19}, {"type": "text", "text": "Case II. For $i\\leq\\ell_{\\star}$ and $\\delta/d^{\\beta}\\leq x\\leq1/2$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{|A|^{\\ell_{\\star}}\\binom{d^{\\star}}{i}x^{i}(1-x)^{d^{\\star}-i}\\le|A|^{\\ell_{\\star}}\\binom{d^{\\star}}{i}(1-x)^{d^{\\star}}}&{}\\\\ &{\\le|A|^{\\ell_{\\star}}(d^{\\star})^{\\ell_{\\star}}e^{-d^{\\star}x}}\\\\ &{\\le e^{\\ell_{\\star}\\log(|A|)+\\ell_{\\star}\\log(d^{\\star})-d^{\\star}x}}\\\\ &{\\le e^{-\\Omega(\\delta^{2}n/d^{\\beta}/\\log(d/\\delta))}}\\\\ &{\\le e^{-\\Omega(\\delta^{2}d^{1-\\beta}/\\log(d/\\delta))},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the last inequality uses the fact that $\\ell_{\\star}=O(\\log(d)),\\,x\\geq\\delta/d^{\\beta},\\,d^{\\star}=\\Omega(d\\delta/\\log(d/\\delta))$ . By combining eq. (12) and eq. (13) with eq. (11) completes the proof, as long as $\\beta$ is a constant bounded away from 1. ", "page_idx": 19}, {"type": "text", "text": "Lemma A.11. Fix a constant $\\gamma\\,>\\,0$ . Then, with probability $\\geq1-d^{-\\Omega_{\\gamma,\\delta}(\\log(d))}$ , none of the substrings in the set $\\begin{array}{r}{\\mathcal{N}_{\\gamma}\\,=\\,\\{\\mathbf{s}^{\\prime}:\\operatorname*{max}_{a\\in\\mathcal{A}}P(\\mathbf{s}^{\\prime}|a)\\overset{.}{\\leq}\\delta/d^{1+\\gamma}\\}}\\end{array}$ are assigned as tokens in a run of LZW. ", "page_idx": 19}, {"type": "text", "text": "Proof. Define the following set of substrings, ", "page_idx": 19}, {"type": "equation", "text": "$$\nS_{\\gamma}=\\left\\{{\\bf t}:\\delta/d^{1+\\gamma/2}\\leq\\operatorname*{max}_{a\\in\\mathcal{A}}P({\\bf t}|a)\\leq1/d^{1+\\gamma/2}\\right\\}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Since the width of this band is sufficiently large, by Assumption 3.2 every substring $\\pmb{t}$ such that $\\begin{array}{r}{\\operatorname*{max}_{a\\in{\\cal{A}}}P(t|a)\\le\\delta/d^{1+\\gamma/2}}\\end{array}$ has at least one prefix which falls in $S_{\\gamma}$ , and denote the longest such prefix $\\pmb{t}_{\\gamma}$ . Define $T_{\\gamma}=\\{t_{\\gamma}:t\\in\\mathcal{N}_{\\gamma}\\}$ as the set of longest prefixes in $S_{\\gamma}$ . Intuitively, if we think of the strings in $S_{\\gamma}$ (or $T_{\\gamma}$ ) as being intermediate in length, the strings in $\\mathcal{N}_{\\gamma}$ can be thought of as being particularly long: the value of $\\bar{\\operatorname*{max}}_{a\\in A}\\bar{P}(\\pmb{t}|a)$ for any $t\\in T_{\\gamma}$ and for any $t\\in\\mathcal{N}_{\\gamma}$ are separated by a factor of at least $1/d^{\\gamma/2}$ . In particular, since the probability of any character is lower bounded by $\\delta$ , each substring in $t\\in\\mathcal{N}_{\\gamma}$ must be at least 2 \u03b3l olgo(g1(/d\u03b4)) symbols longer than its corresponding longest prefix in $T_{\\gamma}$ , $\\pmb{t}_{\\gamma}$ . An implication of this is that for $\\mathbf{t}$ to be assigned as a token, $\\pmb{t}_{\\gamma}$ must be observed at least $\\Delta+1$ times disjointly in $\\pmb{s}$ . However, note that $\\pmb{t}_{\\gamma}$ already has low marginal probability to begin with $(\\ll1/d)$ so the odds of seeing this substring so many times disjointly is very small. Furthermore, note that $T_{\\gamma}$ has at most $d^{1+\\gamma/2}/\\delta$ substrings, which allows the probability of this event occurring simultaneously across all substrings in $T_{\\gamma}$ to be controlled by union bound. Under this condition, none of the substrings in $\\mathcal{N}_{\\gamma}$ are made into tokens. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "In order to argue that the dictionary does not contain certain tokens, we may argue this property about any superset of the dictionary. In contrast, in Lemma A.10, we construct a subset of the dictionary by running LZW on the concatenation of $d^{\\star}$ tokens sampled from their joint distribution. The superset we consider here is just to sample $d$ tokens from their joint distribution and concatenate them together to result in a string of length $\\geq d$ , and running LZW on this sequence (which simply would result in these $d$ tokens). As in Lemma A.10, let $\\{\\mathsf{t r a j}_{i}:i\\in[d]\\}$ denote the infinite trajectories generated from the Markov chain which are truncated to result in tokens. A sufficient condition for the event that no substring $t\\in\\mathcal{N}_{\\gamma}$ is assigned as a token by LZW is to the event that every substring $t^{\\prime}\\in T_{\\gamma}$ is observed as a prefix of $\\mathsf{i r a j}_{i}$ for $\\Delta$ or fewer choices of $i\\in[d]$ . To this end define $\\mathcal{E}(t^{\\prime})$ as the event that $|i\\in[d]:\\bar{t^{\\prime}}$ is a prefix of $\\operatorname{traj}_{i}|\\leq\\Delta$ . Then, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathrm{Pr}(\\mathcal{E}(\\pmb{t}^{\\prime}))\\leq\\binom{n}{\\Delta}(\\underset{a\\in\\mathcal{A}}{\\operatorname*{max}}P(\\pmb{t}^{\\prime}|a))^{\\Delta}}\\\\ &{}&{\\stackrel{(i)}{\\leq}e^{\\Delta\\log(n)}\\left(\\frac{1}{d^{1+\\gamma/2}}\\right)^{\\Delta}}\\\\ &{}&{\\leq e^{-\\frac{\\gamma}{2}\\Delta\\log(d)},\\ \\ \\ \\ \\ }\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $(i)$ uses the fact that $\\begin{array}{r}{\\operatorname*{max}_{a\\in{\\cal{A}}}P(t^{\\prime}|a)\\le1/d^{1+\\gamma/2}}\\end{array}$ since the substring $t^{\\prime}$ belongs to $T_{\\gamma}$ . ", "page_idx": 20}, {"type": "text", "text": "Note that the number of substrings in $S_{\\gamma}$ (and by extension, $T_{\\gamma}$ ) is at most $O_{\\delta}(d^{1+\\gamma/2})$ . Recall that these substrings satisfy the condition $\\begin{array}{r}{\\operatorname*{max}_{a\\in{\\cal{A}}}P(t|a)\\geq\\delta/d^{1+\\gamma/2}}\\end{array}$ . Observe that, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\delta|S_{\\gamma}|}{d^{1+\\gamma/2}}\\leq\\sum_{t\\in S_{\\gamma}}\\operatorname*{max}_{a\\in A}P(t|a)}\\\\ {\\displaystyle\\leq\\sum_{t\\in S_{\\gamma}}\\sum_{a\\in A}P(t|a)\\leq|A|\\leq\\frac{1}{\\delta}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "This implies that there are at most $d^{1+\\gamma/2}/\\delta^{2}$ substrings in $S_{\\gamma}$ . Finally, in conjunction with eq. (14), ", "page_idx": 20}, {"type": "equation", "text": "$$\nP(\\exists\\,t^{\\prime}\\in S_{\\gamma}:\\mathcal{E}(t^{\\prime}))\\leq\\frac{d^{1+\\gamma/2}}{\\delta^{2}}e^{-\\frac{\\gamma}{2}\\Delta\\log(d)}=d^{-\\Omega_{\\gamma,\\delta}(\\log(d))},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which implies that with high probability, no token in $S_{\\gamma}$ is observed as a prefix of $s^{i}$ for more than $\\Delta$ choices of the index $i\\in[d]$ . Under this event, no substring in $\\mathcal{N}_{\\gamma}$ is assigned as a token. \u53e3 ", "page_idx": 20}, {"type": "text", "text": "A.6.1 Proof of Theorem 3.6 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Choosing $\\beta=0.99$ in Lemma A.10, with probability $\\geq1-d^{-\\Omega_{\\delta}(\\log(d))}$ , the LZW tokenizer results in a 0.99-heavy-hitting dictionary. As a consequence of Lemma A.6, this implies that under the same event, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{H(Q_{\\mathrm{MLE}},P)\\geq0.99\\log(d).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Finally, combining with Theorem 3.4 completes the proof. ", "page_idx": 20}, {"type": "text", "text": "B Additional Theoretical Results I: A sequential variant of BPE ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "While the main results in the paper focused on understanding the limits of tokenization under a bound on the dictionary size, in this section we take a more practical look and try to analyze tokenizers used commonly in practice. The Byte-Pair-Encoding (BPE) algorithm (Gage, 1994; Sennrich et al., 2016), discovered in the compression literature as REPAIR (Larsson and Moffat, 2000; Navarro and Russo, 2008) was proposed as a faster alternative to LZW. It remains as one of the most commonly implemented tokenizers in natural language processing for various downstream tasks (Radford et al., 2019; Mann et al., 2020; Touvron et al., 2023). A large proportion of open source and commercial LLMs currently use BPE as the tokenization algorithm of choice, such as GPT-2/3, Llama 1/2 and Mistral-7B to name a few. ", "page_idx": 21}, {"type": "text", "text": "The BPE algorithm is based on constructing the dictionary iteratively by merging pairs of tokens to result in a tokens. In each iteration, the pair of tokens which appear most frequently next to each other are merged together into a single token. Subsequently, every occurrence of the pair of tokens are replaced by the newly added token, breaking ties arbitrarily. The dictionary is thus an ordered mapping of the form $t\\gets(t^{\\prime},t^{\\prime\\prime})$ . To encode a new string, the BPE encoder iterates through the dictionary and for each rule $t\\gets(t^{\\prime},t^{\\prime\\prime})$ replaces every consecutive occurrence of $t^{\\prime}$ and $t^{\\prime\\prime}$ by the token $\\pmb{t}$ breaking ties arbitrarily. ", "page_idx": 21}, {"type": "text", "text": "To warm up our main results, it is worth understanding the behavior of the BPE tokenizer in a bit more detail. Unlike the toy tokenizer, it is a priori unclear whether unigram models trained on sequences tokenized by BPE even asymptotically (in the dictionary size) achieve the optimal cross-entropy loss. Indeed, for $\\delta>0$ , consider a training sequence of length $m$ of the form, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{s=\\underbrace{\\left(\\underbrace{01\\cdot\\cdot\\cdot01}_{2/\\delta}\\underbrace{10\\cdot\\cdot\\cdot10}_{2/\\delta}\\right)}_{\\times\\,\\frac{m\\delta}{4}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The probability that this sequence is generated by the order-2 switching Markov source with $p=q=$ $\\delta$ is, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\approx(1-\\delta)^{\\frac{m\\delta}{4}\\times\\frac{4}{\\delta}\\times(1-\\delta)}(\\delta)^{\\frac{m\\delta}{4}\\times4}=e^{-H(P)},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "which uses the fact that $H(P)=m\\delta\\log(1/\\delta)+m(1-\\delta)\\log(1/(1-\\delta))$ . This implies that even though the string has exponentially small probability, it is one of the typical sequences for this order-2 Markov source. Let\u2019s understand what happens when the BPE tokenizer is trained on this dataset. Assuming that ties are broken arbitrarily, consider the run of the BPE algorithm detailed in Table 2. Here, we assume that $1/\\delta-1$ is a power of 2 and denote $r\\,=\\,\\log_{2}^{\\bar{(1/\\delta-1)}}$ . The algorithm first merges 0 and 1 into a single token $\\pmb{t}_{1}$ , which results in a long sequence of the form $\\pmb{t}_{1}\\cdot\\cdot\\cdot\\cdot\\cdot\\cdot\\pmb{t}_{1}\\pmb{1}t_{1}\\cdot\\cdot\\cdot\\cdot\\cdot\\cdot t_{1}0$ repeated $m\\delta/4$ times. In subsequent rounds, the tokens $(t_{1},t_{1})$ is merged into $t_{2}$ , then $(t_{2},t_{2})$ is merged into $\\pmb{t}_{3}$ and so on, until is no longer possible. Finally, the resulting sequence is a repeating sequence of 5 tokens where within each sequence, no pair of tokens appears more than once next to each other. Eventually these 5 tokens are merged into a single token labelled $\\pmb{t}_{r+4}$ , and in subsequent rounds the tokens $(t_{r+4},t_{r+4})$ are merged into $\\pmb{t}_{r+5}$ , $(t_{r+5},t_{r+5})$ is merged into $\\pmb{t}_{r+6}$ and so on, until is no longer possible. ", "page_idx": 21}, {"type": "text", "text": "Observe that in the initial training dataset the substrings 0000 and 1111 never appears as a contiguous sequence. However, in a test sequence of length $m$ sampled from the $2^{\\mathrm{nd}}$ -order Markov source, with high probability these substrings disjointly occur $\\Theta(m)$ times each. The learnt dictionary associates each such disjoint occurrence of these substrings with at least 1 token, for 0000, the $3^{\\mathrm{rd}}$ 0 must necessarily be tokenized as the token $\\bullet\\!\\cdot\\!\\,-$ . Likewise, in 1111, the $3^{\\mathrm{rd}}$ 1 must necessarily be tokenized as the token \u201c1\u201d. Therefore, when a new test string of length $m$ is tokenized, with high probability the tokens $\\bullet\\!\\cdot\\!\\,\\neg$ and \u201c1\u201d form a constant fraction of the total collection of tokens. ", "page_idx": 21}, {"type": "text", "text": "Thus on freshly sampled test sequences, the BPE tokenizer appears to behave like the character-level tokenizer on a constant fraction of the input sequence. In particular, a simple calculation shows that the cross-entropy loss of any unigram model trained on this tokenizer must be far from the optimal ", "page_idx": 21}, {"type": "table", "img_path": "wm9JZq7RCe/tmp/99a67e592dfdac061b7253c44dd582eaa5b98d938c51bb2878b0dba0d8f47393.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "Table 2: A representation of the behavior of BPE when trained on the dataset in eq. (15). We assume that $1/\\delta-1$ is a power of 2 and define $r=\\log_{2}(1/\\delta-1)$ . ", "page_idx": 22}, {"type": "text", "text": "bound of $m H_{\\mathrm{BER}}(\\delta)$ especially as $\\delta$ becomes smaller, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{Q\\in\\mathcal{Q}_{1:\\mathrm{gram}}}{\\mathrm{min}}\\mathcal{L}_{m}(Q\\circ\\mathsf{e n c}(\\cdot))}\\\\ &{\\geq\\underset{Q\\in\\mathcal{Q}_{1:\\mathrm{gram}}}{\\mathrm{min}}\\,\\mathbb{E}\\left[n_{0}\\log(1/Q_{\\mathrm{tok}}(0)+n_{1}\\log(1/Q_{\\mathrm{tok}}(1)\\right]}\\\\ &{\\overset{(i)}{\\geq}\\Omega(m)\\cdot\\underset{Q\\in\\mathcal{Q}_{1:\\mathrm{gram}}}{\\mathrm{min}}\\left(\\log(1/P_{\\mathrm{tok}}(0)+\\log(1/Q_{\\mathrm{tok}}(1)\\right)}\\\\ &{\\geq\\Omega(m).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $(i)$ uses the fact that $\\mathbb{E}[n_{0}]$ , $\\mathbb{E}[n_{1}]\\in\\Omega(m)$ and the last inequality uses $P_{\\mathrm{tok}}(0)P_{\\mathrm{tok}}(1)\\le1/4$ (AM-GM inequality) since they sum up to at most 1. The purpose of this example is to show that there exist pathological training datasets which appear to be drawn from a stochastic source, but on which BPE fails to learn a good dictionary for the source. Thus proving a result such as Theorem 3.1 for BPE would require arguing that training datasets such as that in eq. (15) are unlikely to be seen. ", "page_idx": 22}, {"type": "text", "text": "The analysis of the standard variant of BPE turns out to be complicated for other reasons too. After every token is added the training dataset becomes a mix of all the previously added tokens, and arguing about the statistics of which pair of tokens appears most frequently for the next iteration becomes involved. For instance, adding 00 as a token may reduce the frequency of occurrence of the substring 01, but will not affect 11. Thus, even though 01 may a priori have been seen more frequently, it may not be chosen by BPE as the next token after 00. ", "page_idx": 22}, {"type": "text", "text": "To avoid dealing with these issues, we consider a sequential/sample-splitting variant of BPE. At a high level, the algorithm breaks down a dataset of size $\\Theta(d^{2})$ into $d$ chunks and learns at most 1 token from each chunk. The algorithm iterates over the chunks and finding the pair of tokens which appear most frequently next to each other in each chunk and adding it to the dictionary if it appears more than $\\log(d)$ times. Every consecutive occurrence of the pair of tokens is replaced by the newly assigned token in the dataset. Thus, in each iteration $i$ , at most 1 token is added, depending on the statistics of the $i^{\\mathrm{th}}$ chunk and the tokens added so far to the dictionary. Based on the final size of the dictionary a different encoder/decoder pair is used - if the algorithm adds sufficiently many tokens to the dictionary, the greedy encoder is used, and if not, a parallel implementation of BPE\u2019s encoding algorithm is used (Definition B.1). A formal description of the algorithm is in Algorithm 1. ", "page_idx": 22}, {"type": "text", "text": "Definition B.1 (BPE.split encoder). The BPE.split encoder parses a new string into tokens as follows. The algorithm partitions the string into contiguous chunks of length $d$ . Then, BPE\u2019s encoder is applied on each chunk, which iterates through DS and replaces $\\pmb{t}^{\\prime}\\pmb{t}^{\\prime\\prime}$ by $\\pmb{t}$ for every rule $t\\gets(t^{\\prime},t^{\\prime\\prime})$ in DS, breaking ties arbitrarily. The individual token sequences are finally spliced together and returned. ", "page_idx": 22}, {"type": "text", "text": "The main result of this section is that up to a small additive error, Algorithm 1 approaches a 2- approximation to the optimal cross-entropy loss. ", "page_idx": 22}, {"type": "text", "text": "Input: $\\epsilon\\in(0,1)$ ; a dataset of size $n=\\Theta(d^{2})$ , split into $d$ contiguous texts $\\{\\mathbf{te}\\times\\mathbf{t}_{1},\\cdot\\cdot\\cdot,\\mathbf{te}\\times\\mathbf{t}_{d}\\}$ of   \nlength $\\Theta(d)$ each.   \nOutput: A tokenizer $\\tau$ .   \n// Generate Dictionary   \nfor $i=1,\\cdots,d$ do if $\\exists$ a pair of tokens/characters $(t^{\\prime},t^{\\prime\\prime})$ appearing $\\geq\\log(d)$ times consecutively in $\\mathsf{t e x t}_{i}$ then Append the rule $t\\gets(t^{\\prime},t^{\\prime\\prime})$ to DS for $j=i+1,\\cdot\\cdot\\cdot,d$ do $\\mathsf{t e x t}_{j}\\gets\\mathsf{A P P L Y}_{t\\gets(t^{\\prime},t^{\\prime\\prime})}(\\mathsf{t e x t}_{j})$ ;   \n// Can be implemented in parallel end for end if   \nend for   \n// Encoder and Decoder   \nif $|\\mathsf{D i c t}|<d_{0}\\triangleq\\epsilon d/2\\log(4|\\mathcal{A}|)$ then $\\tau\\leftarrow$ (Dict, DS, encBPE.split(\u00b7), decBPE.split(\u00b7))   \nelse $T\\gets(\\mathsf{D i c t},\\emptyset,\\mathsf{e n c}_{\\mathrm{gre}}(\\cdot),\\mathsf{d e c}_{\\mathrm{gre}}(\\cdot))$   \nend if   \ndef $\\mathsf{A P P L Y}_{t\\gets(t_{1},t_{2})}(\\mathsf{t e x t})$ :   \nReplace every consecutive occurrence of $(t^{\\prime},t^{\\prime\\prime})$ in text by $\\pmb{t}$ , breaking ties arbitrarily. ", "page_idx": 23}, {"type": "text", "text": "Theorem B.2. For any $\\epsilon\\in(0,1)$ , run Algorithm $^{\\,l}$ on a dataset of $n=\\Theta(d^{2})$ characters to learn $a$ dictionary with at most d tokens. The resulting tokenizer $\\tau$ satisfies with probability $\\geq1-e^{-\\Omega(d\\epsilon^{2})}$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{Q\\in\\mathcal{Q}_{1\\cdot g r a m}}\\mathcal{L}(Q\\circ\\pmb{e n c}(\\cdot))\\leq(2+\\varepsilon)\\operatorname*{min}_{Q}\\mathcal{L}(Q)+\\epsilon\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "While the guarantees established for the sequential BPE tokenizer are weaker than those in Theorem 3.1, the analysis turns out to be quite involved. Theorem B.2 implies that unigram models trained on the sequential BPE tokenizer asymptotically approach the optimal cross-entropy loss up to a factor of 2. ", "page_idx": 23}, {"type": "text", "text": "The formal proof of this result is presented in Appendix B. What is the intuition behind using a different encoder in Algorithm 1 depending on the number of tokens in the dictionary? When the number of tokens in the dictionary is smaller than $d_{0}$ , we know that on a $1-d_{0}/d$ fraction of the iterations of Algorithm 1, a token is not added to the dictionary, i.e. every pair of tokens already appears at most $\\log(d)$ times together. This is a datapoint of \u201cevidence\u201d that under the dictionary in that iteration, the BPE encoder is already good at encoding new strings (of length $\\Theta(d))$ in a way where pairs of tokens do not appear consecutively with high frequency. Since future dictionaries only have more rules appended to them, dictionaries only get better at encoding new strings into tokens where pairs do not frequently appear consecutively. In other words, the BPE encoder satisfies a monotonicity property. It remains to show that dictionaries which encode new sequences in a way where no pair of tokens appear too frequently have large $H(Q_{\\mathrm{MLE}},P)$ (to invoke Theorem 3.4). This follows from ideas introduced in (Navarro and Russo, 2008). ", "page_idx": 23}, {"type": "text", "text": "The case where the number of tokens is large $(\\geq\\ d_{0})$ turns out to present significant technical challenges for analyzing the BPE encoder. There is no longer much \u201cevidence\u201d that the dictionary in each iteration is good at encoding strings since in a large number of iterations a pair of tokens appear consecutively with high frequency. Analyzing the greedy encoder also presents its own challenges - although the algorithm has allocated a large number of tokens, it is possible that there are short tokens $\\pmb{t}$ which are maximal (i.e. they are not prefixes of other tokens). This is similar to the problem encountered by BPE when trained on the dataset in eq. (15) - although the algorithm has allocated a large number of tokens, the token 1 is maximal since every other token begins with the character 0. ", "page_idx": 23}, {"type": "text", "text": "However, it turns out that such tokens, although present in the dictionary, are not observed frequently while encoding a fresh string drawn from the source. ", "page_idx": 24}, {"type": "text", "text": "B.1 Analysis of Algorithm 1 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In this section we prove a rephrased version of Theorem B.2 which implies the statement in the main paper. Define d0 = 2 log\u03f5(d4|A|). ", "page_idx": 24}, {"type": "text", "text": "Theorem B.3 (Rephrased Theorem B.2). Run Algorithm 1 on a dataset of $n=\\Theta(d^{2})$ characters to learn a dictionary with at most $d$ tokens. The resulting tokenizer $\\tau$ satisfies one of the following 3 conditions, ", "page_idx": 24}, {"type": "text", "text": "1. Either, $|D i c t|>d_{0}$ , and, ", "page_idx": 24}, {"type": "text", "text": "$\\operatorname*{min}_{Q\\in\\mathscr{Q}_{1-g r a m}}\\mathscr{L}(Q\\circ e n c(\\cdot))\\leq\\frac{H_{\\infty}}{1-\\varepsilon}.$ Here, $\\begin{array}{r}{\\varepsilon=O\\left(\\frac{\\log^{3}(1/\\delta)\\log(1/\\epsilon)}{\\epsilon\\delta^{9}\\log(d)}\\right)}\\end{array}$   \n2. $\\mathrm{Pr}(|D i c t|<d_{0})=e^{-\\Omega(\\epsilon^{2}d/\\log^{2}(1/\\delta))},\\,o r;$   \n3. Conditional on $|D i c t|<d_{0}$ , with probability $\\geq1-e^{-\\Omega(\\epsilon^{2}d/\\log^{2}(1/\\delta))},$ , $\\operatorname*{min}_{Q\\in Q_{1:g r o m}}\\mathcal{L}(Q\\circ e n c(\\cdot))\\leq\\left(1-\\frac{2d_{0}}{d}\\right)\\left(2H_{\\infty}+O\\left(\\frac{1}{\\log(d)}\\right)\\right)+\\frac{2d_{0}}{d}\\log(4|\\mathcal{A}|).$ With the choice of $d_{0}=\\epsilon d/2\\log(4|\\mathcal{A}|)$ we get the statement of Theorem B.2. ", "page_idx": 24}, {"type": "text", "text": "B.2 Analysis for the large dictionary case: $|\\mathsf{D i c t}|>d_{0}$ ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In the large dictionary case, Algorithm 1 uses the greedy encoder/decoder pair in conjunction with the dictionary. The proof of Theorem B.2 relies on establishing that the cross-entropy $H(Q_{\\mathrm{MLE}},P)$ of the tokenizer is large. Namely, we prove that, ", "page_idx": 24}, {"type": "text", "text": "Lemma B.4. In Algorithm $^{\\,l}$ , assuming at least $d_{0}$ tokens are allocated, ", "page_idx": 24}, {"type": "equation", "text": "$$\nH(Q_{M L E},P)=\\Omega\\left(\\frac{\\epsilon\\delta^{9}\\log(d)}{\\log(1/\\epsilon)\\log^{3}(1/\\delta)}\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "To show this, it suffices to argue that conditioned on any previous set of tokens, with nontrivial probability over the underlying string generated from the stochastic source, the next token is long (i.e. having conditional probability at most $O(1/\\sqrt{d}))$ ). ", "page_idx": 24}, {"type": "text", "text": "Lemma B.5. Suppose that in a run of Algorithm $^{\\,I}$ , at least $d_{0}$ tokens are allocated. Suppose a set of tokens $\\pmb{t}_{1},\\cdots,\\pmb{t}_{k}$ have been sampled so far by the greedy encoder. Let $T_{i+1}$ be the random variable which denotes the next token returned by the greedy encoder, where the randomness comes from the underlying string being tokenized. Then, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left(P(T_{i+1}|t_{i})\\leq1/{\\sqrt{C\\delta d}}{\\Big|}t_{1},\\cdot\\cdot\\cdot,t_{i}\\right)\\geq{\\frac{d_{0}\\delta^{6}(1-\\delta)^{2}}{8C d\\Delta|A|\\log(2|A|)n_{D}}}=\\Omega\\left({\\frac{\\epsilon\\delta^{9}}{\\log^{3}(1/\\delta)\\log(1/\\epsilon)}}\\right)\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof sketch of Lemma B.5. The proof will proceed in 2 parts. We first show in Lemma B.9 that there is a set $D_{\\mathrm{valid}}$ of $\\Omega(d)$ tokens in the dictionary which are neither prefixes nor suffixes of any other token in Dict. The reason for considering this set of tokens is twofold, ", "page_idx": 24}, {"type": "text", "text": "1. Irrespective of what the previous set of tokens were, it is legal for a token $D_{\\mathrm{valid}}$ to be sampled in the current step by the greedy encoder, since for any candidate $t\\in D_{\\mathrm{valid}}$ , by definition, $\\pmb{t}_{j}\\cdot\\cdot\\cdot\\pmb{t}_{i}\\pmb{t}\\notin\\sf{D i c t}$ for every $j\\leq i$ . ", "page_idx": 24}, {"type": "text", "text": "2. Suppose a sequence of tokens $t_{1},\\cdot\\cdot\\cdot\\,,t_{i}$ have already been sampled, ending with the character $a$ . Then, we may sample the next token using rejection sampling. Sample $a^{\\prime}\\sim P(\\cdot|a)$ and an infinitely long trajectory on $\\mathcal{T}_{a^{\\prime}}^{\\star}$ . Return the last token on this trajectory which belongs to Dict, and if it so happens that $\\breve{\\exists}j\\in[i]$ such that $\\pmb{t}_{j}\\cdot\\cdot\\cdot\\pmb{t}_{i}\\pmb{t}\\in\\mathsf{D i c t}$ , then reject this trajectory and repeat. Since all the tokens in $D_{\\mathrm{valid}}$ are not prefixes of another token, any trajectory which reaches a token in $D_{\\mathrm{valid}}$ must terminate the rejection sampling process. ", "page_idx": 25}, {"type": "text", "text": "Next, in Lemma B.10, we show that since the number of tokens in $D_{\\mathrm{valid}}$ is sufficiently large, $\\Omega(d)$ , with constant (in $d_{,}$ ) probability, a trajectory rolled out in the first round of the rejection sampling process will reach a token $t\\in D_{\\mathrm{valid}}$ which has small probability, i.e. $\\operatorname*{max}_{a\\in A}P(t|a)\\leq1/\\mathsf{p o l y}(d)$ . By the previous arguments, this must mean that the rejection sampling process terminates on this \u201clow probability\u201d token, resulting in the statement of the lemma. \u53e3 ", "page_idx": 25}, {"type": "image", "img_path": "wm9JZq7RCe/tmp/3a6e57136443b9db921a06eaf07504278bb6ef214086523c6bf11491f6f19170.jpg", "img_caption": [], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "Figure 7: Jointly generating a sequence and its greedy encoding: In this example we use the greedy encoder under the dictionary composed of all the substrings shadowed red. The first character $(a)$ is sampled from the stationary distribution. Then an infinite string is sampled on the tree with $a$ as root (green path). The last substring on this path which is a token $\\pmb{t}_{1}=a b b)$ ) is returned by the greedy encoder. Then the next character $x=b$ is sampled from the source conditioned on the previous character $(b)$ and further conditioned on $\\pmb{t}_{1}x\\notin\\sf D i c t$ . Finally, another infinite string is sampled on the tree with $x=b$ as root (purple path) and the last substring on this path which is a token $\\mathbf{\\nabla}t_{2}=b a)$ is returned by the greedy encoder. Repeating this process, we can generate a string, here, abbba \u00b7 \u00b7 \u00b7 , as well as its greedy encoding, $(a b b,b a,\\cdots)$ . ", "page_idx": 25}, {"type": "text", "text": "Proof of Theorem B.3.1 and Lemma B.4 It is easy to see why Lemma B.5 implies a lower bound on the cross entropy $H(Q_{\\mathrm{MLE}},P)$ of the tokenizer. By Lemma A.4 for the greedy encoder, ", "text_level": 1, "page_idx": 25}, {"type": "equation", "text": "$$\nQ_{\\mathrm{MLE}}(t)=\\operatorname*{lim}_{m\\to\\infty}\\mathbb{E}\\left[\\frac{n_{t}}{\\sum_{t^{\\prime}}n_{t^{\\prime}}}\\right]\\stackrel{\\mathrm{a.s.}}{=}\\operatorname*{lim}_{m\\to\\infty}\\frac{n_{t}}{\\sum_{t^{\\prime}}n_{t^{\\prime}}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Since the limit $m\\rightarrow\\infty$ of the RHS exists by Lemma A.4, we may let $m\\rightarrow\\infty$ in any way we like, and in particular we may simply sample $i^{\\star}$ tokens, $\\pmb{t}_{1},\\cdots\\cdot\\pmb{,t}_{i^{\\star}}$ sequentially according to the process in Figure 7. Here, the first token sampled is returned by generating an infinitely long string on $\\mathcal{T}_{a}^{\\star}$ where $a\\sim\\pi$ and then truncating this trajectory to the longest token which belongs to Dict. Subsequently for every $i>1$ , $\\pmb{t}_{i}$ is generated by sampling a fresh infinitely long string from $\\mathcal{T}_{a}^{\\star}$ where $a$ is sampled from the $P(\\cdot|a^{\\prime})$ where $a^{\\prime}$ is the last character $\\pmb{t}_{i-1}$ and then returning the largest prefix of this string which is a token in Dict, conditioned on $\\pmb{t}_{j}\\cdot\\cdot\\cdot\\pmb{t}_{i-1}\\pmb{t}_{i}\\notin\\mathsf{D i c}$ t for any $j<i$ . ", "page_idx": 25}, {"type": "text", "text": "and concatenate the corresponding substrings to get an , we must have $m\\rightarrow\\infty$ surely since $m\\geq i^{\\star}$ $\\begin{array}{r}{m=\\sum_{i=1}^{i^{\\star}}|t_{i}|}\\end{array}$ . In this view, eq. (16) can be rewritten as, length character string. Letting ", "page_idx": 26}, {"type": "equation", "text": "$$\nQ_{\\mathrm{MLE}}(t)=\\operatorname*{lim}_{i^{\\star}\\to\\infty}\\frac{n_{t}}{i^{\\star}}=\\operatorname*{lim}_{i^{\\star}\\to\\infty}\\frac{1}{i^{\\star}}\\sum_{i=1}^{i^{\\star}}\\mathbb{I}(t_{i}=t)\\stackrel{\\mathrm{a.s.}}{=}\\operatorname*{lim}_{i^{\\star}\\to\\infty}\\frac{1}{i^{\\star}}\\sum_{i=1}^{i^{\\star}}\\mathbb{E}\\big[\\mathbb{I}(t_{i}=t)|t_{1},\\cdots,t_{i-1}\\big]\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the last inequality follows by the sequential nature of the token sampling process and a martingale argument. Consider the set of tokens $T$ such that $\\pmb{t}\\in T$ satisfies $\\operatorname*{max}_{a\\in\\mathcal{A}}P({\\pmb t}|a)\\leq$ $\\sqrt{1/C\\delta^{3}d}$ . From eq. (17), summing across $\\pmb{t}\\in T$ , we have that, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\sum_{t\\in T}Q_{\\mathrm{MLE}}(t)\\stackrel{\\mathrm{a.s.}}{=}\\operatorname*{lim}_{i^{\\star}\\to\\infty}\\frac{1}{i^{\\star}}\\sum_{i=1}^{i^{\\star}}\\operatorname*{Pr}\\left(t_{i}\\in T|t_{1},\\cdots,t_{i-1}\\right)=\\Omega\\left(\\frac{\\epsilon\\delta^{9}}{\\log^{3}(1/\\delta)\\log(1/\\epsilon)}\\right)\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where in the last inequality, we use Lemma B.5 and the fact that $\\delta\\operatorname*{max}_{a\\in{\\mathcal{A}}}P(t|a)\\geq\\operatorname*{min}_{a\\in{\\mathcal{A}}}P(t|a)$ Therefore, ", "page_idx": 26}, {"type": "equation", "text": "$$\nH(Q_{\\mathrm{MLE}},P)\\geq\\sum_{t\\in T}Q_{\\mathrm{MLE}}(t)\\log(1/P(t))\\geq\\sum_{t\\in T}Q_{\\mathrm{MLE}}(t)\\log(\\sqrt{C\\delta^{3}d})\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where in $(i)$ we use the fact that for $\\pmb{t}\\in\\cal T$ , $\\mathrm{max}_{a\\in{\\cal{A}}}\\,P({\\boldsymbol t}|{\\boldsymbol a})\\,\\le\\,1/\\sqrt{C\\delta^{3}d}$ , which implies that $P(t)\\leq1/\\sqrt{C\\delta^{3}d}$ . Finally, combining with eq. (18) completes the proof of Lemma B.4. Furthermore, since the cross-entropy $\\dot{H}(Q_{\\mathrm{MLE}},\\bar{P)}$ was established to be large, by invoking the reduction in Theorem 3.4, we complete the proof of Theorem B.3.1. ", "page_idx": 26}, {"type": "image", "img_path": "wm9JZq7RCe/tmp/65d38a07f9d4164b27909b41192e01aa0968bf14df39580f7392c7ec55d888ec.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Figure 8: The circled nodes indicate substrings which are tokens in Dict. The red boundary is the set of substrings $\\pmb{t}$ such that $\\operatorname*{max}_{a\\in A}$ $P({\\mathbf{t}}|a)\\geq1/C d$ . By Lemma B.8, none of the nodes which fall outside this boundary are assigned as tokens in a run of Algorithm 1. The set of circled substrings are the set of tokens in Dict. Among them, the ones circled green are the tokens in $D_{\\mathrm{valid}}$ , which are not prefixes or suffixes of any other tokens in Dict. Substrings such as $c b$ or $b a$ which are tokens in Dict do not belong to $D_{\\mathrm{valid}}$ because they are prefixes of longer tokens (in this case, $c b b$ and bab respectively). On the other hand, substrings like $a b$ do not belong to $D_{\\mathrm{valid}}$ since they are suffixes of tokens in Dict, in this case, $b a b$ . Lemma B.9 asserts that the number of tokens in $D_{\\mathrm{valid}}$ are $\\Omega(d)$ in number, assuming that Dict has $\\Omega(d)$ tokens to begin with. ", "page_idx": 26}, {"type": "text", "text": "Notation. For each $a\\in A$ and $j\\in\\mathbb N\\cup\\{0\\}$ , define a level set of substrings, ", "page_idx": 26}, {"type": "equation", "text": "$$\nS_{j}^{a}=\\Big\\{(1-\\delta)^{j+1}<P(\\pmb{t}|\\pmb{t}_{1}=a)\\leq(1-\\delta)^{j}\\Big\\}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "image", "img_path": "wm9JZq7RCe/tmp/8ef2abf87079e75887f94d491f078be514af472bc29a68c360d2c2057c7b9f19.jpg", "img_caption": ["Figure 9: A pictorial representation of the proof of Lemma B.6 "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "where $\\pmb{t}_{1}$ denotes the first character of $\\pmb{t}$ . And likewise, define the sets $S_{j}=\\cup_{a\\in\\mathcal{A}}S_{j}^{a}$ , $S_{\\leq j}^{a}$ and $S_{\\geq j}^{a}$ as the union of $S_{j^{\\prime}}^{a}$ over $j^{\\prime}\\geq j,\\,j^{\\prime}\\leq j$ and $S_{\\le j}$ and $S_{\\geq j}$ as the union of $S_{\\leq j}^{a}$ and $S_{\\geq j}^{a}$ over $a\\in A$ . Furthermore for a large universal constant $C>0$ , define parameters, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\Delta=\\frac{\\log(\\delta)}{\\log(1-\\delta)}\\asymp\\Theta\\left(\\frac{\\log(1/\\delta)}{\\delta}\\right);\\quad n_{D}=1-\\frac{2\\log(4C d/\\delta d_{0})}{\\log(1-\\delta)}\\asymp\\Theta\\left(\\frac{\\log(1/\\epsilon\\delta)}{\\delta}\\right).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We first begin by stating a folklore result: every pair of tokens assigned by a merging-based dictionary generation algorithm have distinct character representations. ", "page_idx": 27}, {"type": "text", "text": "Lemma B.6. If Algorithm 1 assigns a new token in some round, it\u2019s character representation must be distinct from that of all previously assigned tokens. ", "page_idx": 27}, {"type": "text", "text": "Proof. A pictorial proof is in Figure 9. We will prove this result by contradiction. Suppose $\\pmb{t}$ and $t^{\\prime}$ are tokens which decode to the same character substring, ${\\bf{s}}^{\\prime}$ . Consider all occurrences of ${\\bf{s}}^{\\prime}$ in the dataset which in some iteration encode into $t^{\\prime}$ or $t^{\\prime\\prime}$ , and denote these disjoint locations $\\boldsymbol{S}$ . Recall that at these locations, $\\mathbf{s}^{\\prime}$ eventually is assumed to map to a singular token $t^{\\prime}$ or $t^{\\prime\\prime}$ . Therefore, at every step in the merging process these occurrences of $\\bar{\\mathbf{s^{\\prime}}}$ must perfectly map to a sequence of tokens. ", "page_idx": 27}, {"type": "text", "text": "Now consider the merging process at the first time before any of the rules corresponding to tokens in $t^{\\prime}$ or $t^{\\prime\\prime}$ are implemented. Prior to this time, all the occurrences of $\\mathbf{s}^{\\prime}$ corresponding to the locations in $\\boldsymbol{S}$ have not been tokenized yet. When the first rule corresponding to one of the tokens in $\\{t^{\\prime},t^{\\prime\\prime}\\}$ is implemented, all the strings in $\\boldsymbol{S}$ must be modified identically. This uses the fact that we can isolate each of these occurrences of $\\mathbf{s}^{\\prime}$ while carrying out the merging process, since each location must be distinct. At every step, the encodings of these copies of $s^{\\prime}$ must be the same, and therefore $t^{\\prime}$ and $t^{\\prime\\prime}$ cannot be two distinct tokens. \u53e3 ", "page_idx": 27}, {"type": "text", "text": "Lemma B.7. The size of the level set $S_{j}^{a}$ is bounded by $(1-\\delta)^{-(j+1)}$ . ", "page_idx": 27}, {"type": "text", "text": "Proof. Since the probability of any transition is at most $1-\\delta$ , this implies that any infinite trajectory on the tree $\\mathcal{T}_{a}^{\\star}$ can intersect at most one vertex in $S_{j}^{a}$ . Therefore, $\\begin{array}{r}{\\dot{\\Sigma_{t\\in S_{j}^{a}}}\\,P(t|t_{1}=a)\\leq1}\\end{array}$ . By the lower bound on $P(t|t_{1}=a)$ for $t\\in S_{j}^{a}$ , this implies the statement of the lemma. \u53e3 ", "page_idx": 27}, {"type": "text", "text": "Next we show that with high probability none of the substrings $\\pmb{t}$ having probability mass (under $P$ ) of at most $\\delta/C d$ conditioned on the first character, are assigned as tokens by Algorithm 1. ", "page_idx": 28}, {"type": "text", "text": "Lemma B.8. In a run of Algorithm $^{\\,l}$ , for a sufficiently large constant $C>0$ , with probability $d^{-\\Omega(1)}p o I y(1/\\delta)$ all assigned tokens $\\pmb{t}\\in$ Dict satisfy $\\begin{array}{r}{\\operatorname*{max}_{a\\in\\mathcal{A}}\\bar{P}({\\mathbf t}|a)\\geq1/C d.}\\end{array}$ In other words, none of the substrings in $S_{\\geq j^{\\star}}$ are added as tokens to the dictionary in a run of Algorithm $^{\\,I}$ , where, ", "page_idx": 28}, {"type": "equation", "text": "$$\nj^{\\star}\\triangleq\\log(\\delta/C d)/\\log(1-\\delta)\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. Consider some $j\\geq j^{\\star}$ and $a\\in A$ and substring $t\\in S_{j}^{a}$ . In the $i^{t h}$ stage of the algorithm where $\\mathsf{t e x t}_{i}$ is being processed, for $\\pmb{t}$ to be assigned as a token, at the very least, $\\pmb{t}$ must appear at least $\\log(d)$ times disjointly in $\\mathsf{t e x t}_{i}$ . Therefore, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{P(t\\in S_{j}^{a}\\mathrm{~is~assigned~as~a~token~in~text_i)\\le\\left(\\displaystyle\\log(d)\\right)\\left(\\displaystyle\\operatorname*{max}_{a\\in A}P(t|a)\\right)^{\\log(d)}}&{}\\\\ {\\le d^{\\log(d)}\\left(\\displaystyle\\frac{1}{C d}(1-\\delta)^{j-j^{\\star}}\\right)^{\\log(d)}}&{}\\\\ {\\le d^{-\\log(C)}(1-\\delta)^{(j-j^{\\star})\\log(d)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Union bounding over $S_{j}^{a}$ over $j\\geq j^{\\star}$ using the bound on $|S_{j}^{a}|$ in Lemma B.7, and over $a\\in{\\mathcal{A}}$ and $i\\in[d]$ results in the bound, ", "page_idx": 28}, {"type": "equation", "text": "$$\n{\\mathfrak{H}}_{\\geq j^{\\star}}\\;{\\mathrm{is~assigned~as~a~token~in~step~}}i{\\mathrm{~for~some~}}i\\in[d]\\}\\leq d^{-\\Omega(1)}\\sum_{j\\geq j^{\\star}}{\\frac{(1-\\delta)^{(j-j^{\\star})\\log(d)}}{(1-\\delta)^{j+1}}}\\leq{\\frac{d^{-\\Omega(1)}}{\\delta(1-\\delta)}}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Lemma B.9. Consider the set of tokens $D_{\\nu a l i d}$ which are not a prefix or a suffix of any other token in Dict. That is, $D_{v a l i d}=\\{t\\in D i c t:\\exists s:s t\\in D i c t\\}\\cap\\{t\\in D i c t:\\exists s:t s\\in D i c t\\}$ . If $|D i c t|\\geq d_{0}$ , then, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\left|D_{\\nu a l i d}\\right|\\geq\\frac{d_{0}}{4n_{D}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $n_{D}$ is defined in eq. (19). ", "page_idx": 28}, {"type": "text", "text": "Proof. For any token $t\\in D_{\\mathrm{valid}}$ , there may be at most $2|t|$ tokens which are suffixes or prefixes of it and belong to Dict. More importantly, every token in Dict not belonging to $D_{\\mathrm{valid}}$ must either be a prefix or a suffix of some token in $D_{\\mathrm{valid}}$ . Split the suffixes and prefixes of the tokens in $D_{\\mathrm{valid}}$ into four sets, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{S_{\\mathrm{suff,min}}=\\bigcup_{t\\in D_{\\mathrm{valid}}}\\{t^{\\prime}\\in\\mathsf{D i c t}:t^{\\prime}\\in\\mathsf{s u f f}(t),\\ |t^{\\prime}|\\le|t|-n_{D}\\},}\\\\ &{S_{\\mathrm{suff,max}}=\\bigcup_{t\\in D_{\\mathrm{valid}}}\\{t^{\\prime}\\in\\mathsf{D i c t}:t^{\\prime}\\in\\mathsf{s u f f}(t),\\ |t^{\\prime}|>|t|-n_{D}\\},}\\\\ &{S_{\\mathrm{pre,min}}=\\bigcup_{t\\in D_{\\mathrm{valid}}}\\{t^{\\prime}\\in\\mathsf{D i c t}:t^{\\prime}\\in\\mathsf{p r e}(t),\\ |t^{\\prime}|\\le|t|-n_{D}\\},}\\\\ &{S_{\\mathrm{pre,max}}=\\bigcup_{t\\in D_{\\mathrm{valid}}}\\{t^{\\prime}\\in\\mathsf{D i c t}:t^{\\prime}\\in\\mathsf{p r e}(t),\\ |t^{\\prime}|>|t|-n_{D}\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $n_{D}$ is defined in eq. (19). Note from Lemma B.8 that all the tokens $\\pmb{t}~\\in$ Dict all satisfy $\\begin{array}{r}{\\operatorname*{max}_{a\\in{\\cal{A}}}P(t|a)\\ \\geq^{-}1/C d}\\end{array}$ . Therefore, the tokens in $S_{\\mathrm{pre,min}}$ and $S_{\\mathrm{suff,min}}$ all satisfy, $\\begin{array}{r}{\\operatorname*{max}_{a\\in{\\cal{A}}}P(t|a)\\;\\geq\\;d/C(1\\,-\\,\\delta)^{n_{D}}}\\end{array}$ . By summing Lemma B.7 over appropriate $j$ , we get that $|S_{\\mathrm{pre,min}}|+|S_{\\mathrm{suff,min}}|\\leq2C d(1-\\delta)^{n_{D}-1}/\\delta.$ . ", "page_idx": 28}, {"type": "text", "text": "On the other hand, corresponding to any $t\\in D_{\\mathrm{valid}}$ , there are at most $n_{D}$ tokens in $S_{\\mathrm{pre,max}}$ or $S_{\\mathrm{suff,max}}$ and and therefore $|S_{\\mathrm{pre,max}}|,|S_{\\mathrm{suff,max}}|\\leq n_{D}\\cdot|D_{\\mathrm{valid}}|$ . Since every token in Dict either belongs to $D_{\\mathrm{valid}}$ or is a suffix of some token in $D_{\\mathrm{valid}}$ , $S_{\\mathrm{pre,min}}\\cup S_{\\mathrm{pre,max}}\\cup S_{\\mathrm{suff,min}}\\cup S_{\\mathrm{suff,max}}=|\\mathsf{D i c t}|$ and, ", "page_idx": 28}, {"type": "equation", "text": "$$\n2n_{D}\\cdot|D_{\\mathrm{valid}}|+{\\frac{2C(1-\\delta)^{n_{D}-1}d}{\\delta}}\\geq d_{0}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Recalling the choice of $\\begin{array}{r}{n_{D}=1-\\frac{2\\log(4C d/\\delta d_{0})}{\\log(1-\\delta)}}\\end{array}$ , we get that, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\left|D_{\\mathrm{valid}}\\right|\\geq{\\frac{d_{0}}{4n_{D}}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Lemma B.10. Suppose Algorithm $^{\\,I}$ assigns at least $d_{0}$ tokens. For any character $a\\in{\\mathcal{A}}$ , sample an $a^{\\prime}\\sim P(\\cdot|a)$ and an infinite trajectory on the tree $\\mathcal{T}_{a^{\\prime}}^{\\star}$ , denoted traj. Then, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbb{E}_{a^{\\prime}\\sim P(\\cdot|a)}\\left[\\operatorname*{Pr}_{t r a j\\sim\\mathcal{T}_{a^{\\prime}}^{\\star}}\\left(\\operatorname*{min}_{t\\in t r a j\\cap D_{v a l i d}}P(t|a)\\leq\\sqrt{\\delta/C d}\\bigg|a^{\\prime}\\right)\\right]\\geq\\frac{d_{0}\\delta^{6}(1-\\delta)^{2}}{8C d\\Delta|A|n_{D}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where the notation $\\mathcal{T}_{a^{\\prime}}^{\\star}$ is used to overload the distribution over infinite trajectories on $\\mathcal{T}_{a^{\\prime}}^{\\star}$ . The parameters $n_{D}$ and $\\Delta$ are defined in eq. (19). ", "page_idx": 29}, {"type": "text", "text": "Proof. By Lemma B.8, recall that the $\\geq d_{0}$ tokens assigned in a run of Algorithm 1, with high probability, are substrings in $S_{\\leq j^{\\star}}$ . For any $a\\in A$ , the total number of substrings in $S_{\\leq j^{\\star}}$ can be bounded as, ", "page_idx": 29}, {"type": "equation", "text": "$$\n|S_{\\leq j^{\\star}}|\\leq\\sum_{a\\in A}\\sum_{j=0}^{j^{\\star}}|S_{j}^{a}|\\leq\\sum_{a\\in A}\\sum_{j=0}^{j^{\\star}}\\frac{1}{(1-\\delta)^{j+1}}\\leq\\frac{C|A|d}{\\delta(1-\\delta)}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "In order to prove this result, we use a counting argument and the fact that no tokens in $S_{>j^{\\star}}$ are assigned. Consider some character $a$ and all the leaves in the forest $S_{\\leq j^{\\star}}$ . Since every transition has $\\geq\\delta$ probability of occurring, across all leaf nodes $\\pmb{t}\\in S_{\\leq j^{\\star}}$ , $P(\\pmb{t}|\\alpha^{\\prime})$ are within a $\\delta^{2}(1-\\delta)$ factor of each other across different $a^{\\prime}\\in\\mathcal{A}$ . In particular, by counting the number of paths in $\\mathcal{T}^{\\star}$ (i.e. paths in $\\mathcal{T}_{a}^{\\star}$ from $\\varnothing$ to leaf nodes in $S_{\\le j^{\\star}}^{a}$ across $a\\in A$ ) along which a token in Dict exists in $S_{\\geq j^{\\star}/2}$ , we can also compute the probability mass across such trajectories up to a factor of $\\delta^{2}(1-\\delta)$ . ", "page_idx": 29}, {"type": "text", "text": "Taking the union across $a\\in\\mathcal{A}$ , consider the paths in $\\mathcal{T}_{a}^{\\star}$ from $\\varnothing$ to leaf nodes in $S_{\\le j^{\\star}}^{a}$ . From Lemma B.9, $\\begin{array}{r}{\\sum_{j\\le j^{\\star}}|D_{\\mathrm{valid}}\\cap S_{j}|\\ge d_{0}/4n_{D}}\\end{array}$ , where $n_{D}=1-2\\log(4C d/\\delta d_{0})/\\log(1\\stackrel{-\\circ}{-}\\delta)$ . Note that for sufficiently large $d=\\Omega(\\log(1/\\epsilon\\delta)/\\delta^{5})$ , by Lemma B.7, $\\begin{array}{r}{\\sum_{j\\le j^{\\star}/2}|S_{j}|=\\sqrt{C d/\\delta}/\\delta(1{-}\\delta)\\le}\\end{array}$ $d_{0}/8n_{D}$ . Therefore, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\sum_{j^{\\star}/2<j\\leq j^{\\star}}|D_{\\mathrm{valid}}\\cap S_{j}|\\geq\\frac{d_{0}}{8n_{D}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Define $\\Delta=\\log(\\delta)/\\log(1-\\delta)$ . Combining eq. (21) with eq. (20) and applying the probabilistic method, there exists an $i^{\\star}\\geq j^{\\star}/2$ such that, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\frac{|D_{\\mathrm{valid}}\\cap(S_{i^{\\star}+1}\\cup\\cdots\\cup S_{i^{\\star}+\\Delta})|}{|S_{i^{\\star}+1}\\cup\\cdots\\cup S_{i^{\\star}+\\Delta}|}\\geq\\frac{\\delta(1-\\delta)d_{0}}{8C d|A|n_{D}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Note that $\\Delta$ is chosen to be sufficiently large, so that every infinite trajectory on $\\mathcal{T}_{a^{\\prime}}^{\\star}$ must intersect at least once with the band of vertices $S_{i^{\\star}+\\Delta+1}^{a^{\\prime}}\\cup\\cdot\\cdot\\cdot\\cup S_{i+2\\Delta}^{a^{\\prime}}$ . Note that this band is different from the one considered in eq. (22). Define $L_{a^{\\prime}}$ as the set of longest prefixes across infinite trajectories in $\\mathcal{T}_{a^{\\prime}}^{\\star}$ which belong to $S_{i^{\\star}+\\Delta+1}^{a^{\\prime}}\\cup\\cdot\\cdot\\cdot\\cup S_{i+2\\Delta}^{a^{\\prime}}$ . ", "page_idx": 29}, {"type": "text", "text": "Note that our objective is to show that an infinite trajectory sampled on $\\mathcal{T}_{a^{\\prime}}^{\\star}$ where $a^{\\prime}\\sim P(\\cdot|a)$ , has a long prefix in Dict. We can truncate this trajectory to lower bound this probability, and therefore, we assume that the infinite trajectories on $\\mathcal{T}_{a^{\\prime}}^{\\star}$ terminate once they reach a substring in $L_{a^{\\prime}}$ . Furthermore, note that although $\\Delta$ is large, it is still a constant depending on $\\delta$ . Therefore, the band of states $S_{i^{\\star}+\\Delta+1}^{a^{\\prime}}\\cup\\cdot\\cdot\\cdot\\cup S_{i^{\\star}+2\\Delta}^{a^{\\prime}}$ is not too wide, and all the substrings in $L_{a^{\\prime}}$ have approximately similar probabilities to each other. In particular, for any character $a\\in A$ , and for any $a^{\\prime}\\in\\mathcal{A}$ and $\\pmb{t}\\in L_{a^{\\prime}}$ , decomposing $P(\\pmb{t}|\\alpha)$ as $P(t|t_{1}=a^{\\prime})P(a^{\\prime}|a)$ , ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\delta^{2}(1-\\delta)\\cdot(1-\\delta)^{i+\\Delta}\\stackrel{(i)}{\\leq}P(t|a)\\stackrel{(i i)}{\\leq}(1-\\delta)^{i+\\Delta}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Inequality $(i)$ follows from the fact that all transition probabilities are at least $\\delta$ , so every leaf node in $L_{a^{\\prime}}$ must have $P(t|t_{1}=a^{\\prime})\\,\\geq\\,(1-\\delta)^{i+2\\Delta+1}$ , and the fact that $P(a^{\\prime}|a)\\geq\\delta$ . Inequality $(i i)$ ", "page_idx": 29}, {"type": "text", "text": "follows similarly from the fact that $\\pmb{t}$ is a leaf node of $L_{a^{\\prime}}$ and therefore $P({\\boldsymbol t}|{\\boldsymbol t}_{1}=a^{\\prime})\\leq(1-\\delta)^{i+\\Delta}$ . Therefore, instead of bounding the probability of any event under the distribution over substrings in $L_{a^{\\prime}}$ induced by truncating the infinite strings sampled on $\\mathcal{T}_{a^{\\prime}}^{\\star}$ , it suffices to count the fraction of substrings in $L_{a^{\\prime}}$ satisfying the event (which are equivalent up to a $\\delta(1-\\delta)$ factor). Define, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathsf{p r e}(t)=(t_{1},t_{1:2},t_{1:3},\\cdot\\cdot\\cdot\\,,t_{1:|t|})\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "As the set of prefixes of $\\pmb{t}$ (including $\\pmb{t}$ ). Note that at most $\\Delta$ of the prefixes of any substring $\\pmb{t}$ can intersect with $S_{i^{\\star}+1}^{a}\\cup\\cdot\\cdot\\cdot\\cup S_{i^{\\star}+\\Delta}^{a}$ . Therefore, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{\\sigma^{\\prime}\\in A\\ t\\in L_{a}^{\\prime}}1(\\mathtt{p r e}(t)\\cap D_{\\mathrm{vald}}\\cap(S_{i+1}^{\\prime}\\cup\\cdots\\cup S_{i^{+}+\\Delta}^{\\sigma^{\\prime}})\\neq\\emptyset)}\\\\ &{\\ge\\displaystyle\\sum_{\\alpha^{\\prime}\\in A\\ t\\in L_{a}^{\\prime}}\\frac{|\\mathtt{p r e}(t)\\cap D_{\\mathrm{vald}}\\cap(S_{i+1}^{\\prime}\\cup\\cdots\\cup S_{i^{+}+\\Delta}^{\\sigma^{\\prime}})|}{\\Delta}}\\\\ &{\\overset{(i)}{\\ge}\\displaystyle\\sum_{\\alpha^{\\prime}\\in A}\\frac{|D_{\\mathrm{vald}}\\cap(S_{i}^{\\prime}+1\\cup\\cdots\\cup S_{i^{+}+\\Delta}^{\\sigma^{\\prime}})|}{\\Delta}}\\\\ &{\\overset{(i i)}{\\ge}\\frac{\\delta d_{0}(1-\\delta)}{8C d\\Delta|A|D_{D}}\\displaystyle\\sum_{\\alpha^{\\prime}\\in A}|S_{i+1}^{\\prime}\\cup\\cdots\\cup S_{i^{+}+\\Delta}^{\\sigma^{\\prime}}|}\\\\ &{\\overset{(i i i)}{\\ge}\\frac{\\delta^{3}d_{0}(1-\\delta)}{8C d\\Delta|A|D_{D}}\\displaystyle\\sum_{\\alpha^{\\prime}\\in A}|L_{\\alpha^{\\prime}}|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $(i)$ uses the fact that the prefixes of $\\pmb{t}\\in L_{a^{\\prime}}$ cover all the substrings in Sa\u2264i\u22c6+\u2206, and therefore $\\cup_{t\\in L_{a^{\\prime}}}\\mathsf{p r e}(t)\\supset S_{i^{\\star}+1}^{a^{\\prime}}\\cup\\cdots\\cup S_{i^{\\star}+\\Delta}^{a^{\\prime}}$ , and $(i i)$ uses eq. (22). Finally, $(i i i)$ uses the fact that $\\Delta$ is not too large, and therefore, for any substring $\\pmb{t}^{\\prime}\\in S_{i^{\\star}+1}^{a^{\\prime}}\\cup\\cdot\\cdot\\cdot\\cup S_{i^{\\star}+\\Delta}^{a^{\\prime}}$ , there are at most $1/(1-\\delta)^{2\\Delta}=$ $1/\\delta^{2}$ substrings $\\pmb{t}\\in L_{a^{\\prime}}$ which contain it as a prefix. This means, $|L_{a^{\\prime}}|\\leq|S_{i^{\\star}+1}^{a^{\\prime}}\\cup\\cdot\\cdot\\cdot\\cup S_{i^{\\star}+\\Delta}^{a^{\\prime}}|/\\delta^{2}$ . After dividing by $\\sum_{a^{\\prime}\\in\\mathcal{A}}|L_{a^{\\prime}}|$ on both sides, this implies, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbb{E}_{a^{\\prime}\\sim\\mathrm{Unif}}(A)\\left[\\operatorname*{Pr}_{t\\sim\\mathrm{Unif}(L_{a^{\\prime}})}\\left(\\mathsf{p r e}(t)\\cap D_{\\mathrm{valid}}\\cap(S_{i^{\\star}+1}^{a^{\\prime}}\\cup\\cdots\\cup S_{i^{\\star}+\\Delta}^{a^{\\prime}})\\neq\\emptyset\\right|a^{\\prime}\\right)\\right]\\geq\\frac{\\delta^{3}d_{0}(1-\\delta)}{8C d\\Delta|A|n_{D}}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "The event inside the inner probability term is the event that an infinitely long string (truncated at $L_{a^{\\prime}}$ ) has a prefix which lies in $D_{\\mathrm{valid}}$ and which intersects with $S_{i^{\\star}+1}^{a^{\\prime}}\\cup\\cdot\\cdot\\cdot\\bigcup S_{i^{\\star}+\\Delta}^{a^{\\bar{\\prime}}}$ , which implies that it has probability $P(t|a)\\leq\\sqrt{\\delta/C d}.$ Therefore, we have that for any $a\\in A$ , sampling an $a^{\\prime}\\sim P(\\cdot|a)$ and an infinite trajectory traj $\\sim T_{a^{\\prime}}^{\\star}$ , ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\alpha^{\\prime}\\sim P^{*}(\\cdot)}\\left[\\underset{\\operatorname{trim}}{\\operatorname*{Pr}}\\;\\middle(\\operatorname*{trim}_{i\\in\\mathbb{N}/D_{i}\\ne i}P(t|\\alpha)\\leq\\sqrt{\\delta/C^{d}}\\middle|\\alpha^{i})\\right]}\\\\ &{\\qquad\\overset{(i i)}{\\geq}\\delta^{2}(1-\\delta)\\cdot\\mathbb{E}_{\\alpha^{\\prime}\\sim P^{*}(\\cdot)}\\left[\\underset{\\ell^{\\prime}\\sim\\operatorname{tnif}\\{L_{\\alpha^{\\prime}}\\}}{\\sum_{\\ell^{\\prime}\\sim\\operatorname{tnin}}}\\left(\\underset{\\ell\\in\\mathbb{R}^{d}(\\cdot)\\cap D_{\\ell}}{\\operatorname*{min}}D(t|\\alpha)\\leq\\sqrt{\\delta/C^{d}}\\middle|\\alpha^{i})\\right]}\\\\ &{\\qquad\\overset{(i i)}{\\geq}\\delta^{2}(1-\\delta)\\cdot\\mathbb{E}_{\\alpha^{\\prime}\\sim P^{*}(\\cdot)}\\left[\\underset{\\ell^{\\prime}\\sim\\operatorname{tnin}\\{L_{\\alpha^{\\prime}}\\}}{\\sum_{\\ell^{\\prime}\\sim\\operatorname{tnin}}}\\left(\\mathrm{pre}(t^{\\prime})\\cap D_{\\mathrm{vali}}\\cap\\left(S_{i^{+}+1}^{\\prime}\\cup\\cdots\\cup S_{i^{+}+\\Delta}^{\\prime}\\right)\\neq\\varnothing\\right|\\alpha^{i}\\right)\\right]}\\\\ &{\\qquad\\overset{(i i i)}{\\geq}\\delta^{3}(1-\\delta)\\cdot\\mathbb{E}_{\\alpha^{\\prime}\\sim\\operatorname{trim}\\{\\alpha,i\\}}\\left[\\underset{\\ell^{\\prime}\\sim\\operatorname{tnin}\\{L_{\\alpha^{\\prime}}\\}}{\\sum_{\\ell^{\\prime}\\sim\\operatorname{tnin}\\{L_{\\alpha^{\\prime}}\\}}}\\left(\\mathrm{pre}(t^{\\prime})\\cap D_{\\mathrm{vali}}\\cap\\left(S_{i^{+}+1}^{\\prime}\\cup\\cdots\\cup S_{i^{+}+\\Delta}^{\\prime}\\right)\\neq\\varnothing\\right|\\alpha^{i}\\right)\\right]}\\\\ &{\\qquad\\geq\\delta^{3}(1-\\delta)\\cdot\\frac{\\delta^{3}\\delta_{0}}{8C}d_{\\Delta}(1-\\delta)}\\\\ &{\\qquad\\geq\\delta^{3}(1-\\delta)\\cdot\\frac{1}{8C d_{\\Delta}(\\lambda)\\cdot4|\\lambda|\\alpha}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Here $(i)$ follows by truncating the trajectory traj to terminate at a node in $\\cup_{a^{\\prime}\\in\\mathcal{A}}L_{a^{\\prime}}$ and from eq. (23), $(i i)$ follows by arguing that $i^{\\star}\\le j^{\\star}/2$ and therefore if a prefix of $t^{\\prime}$ lies in $S_{i^{\\star}+1}^{a^{\\prime}}\\cup\\cdot\\cdot\\cdot\\cup S_{i^{\\star}+\\Delta}^{a^{\\prime}}$ , then it must have $P(t|a)\\leq\\sqrt{\\delta/C d}$ . Inequality $(i i i)$ follows by noting that all the transitions $P(a^{\\prime}|a)$ have probability $\\geq\\delta$ , and the last inequality follows from eq. (24). \u53e3 ", "page_idx": 30}, {"type": "text", "text": "Proof of Lemma B.5 ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Lemma B.10 concludes that given any previous sequence of tokens terminating in a character $a$ , with constant probability, an infinite trajectory sampled from $\\mathcal{T}_{a^{\\prime}}^{\\star}$ with $a^{\\prime}\\sim P(\\cdot|a)$ has as prefix, a substring $\\pmb{t}$ , which not only has low probability, with $P(t|a)\\leq\\sqrt{\\delta/C d}$ , but also belongs to the subset of tokens $D_{\\mathrm{valid}}$ . Note that regardless of the previously sampled tokens, it is legal to sample any token in $D_{\\mathrm{valid}}$ as the current token, since by definition, these tokens are not the suffixes of any other tokens in Dict. Moreover, if any trajectory on $\\mathcal{T}_{a^{\\prime}}^{\\star}$ reaches a token in $D_{\\mathrm{valid}}$ , then it must be largest token along that trajectory, since none of the tokens in $D_{\\mathrm{valid}}$ are prefixes of another token in Dict. ", "page_idx": 31}, {"type": "text", "text": "Consider generating a new token by rejection sampling. Suppose the set of previous tokens $\\pmb{t}_{1},\\cdots,\\pmb{t}_{i}$ end in some character $a$ . Sample the next character $a^{\\prime}\\sim P(\\cdot|a)$ and an infinite trajectory on $\\mathcal{T}_{a^{\\prime}}^{\\star}$ . If it reaches an illegal token $\\pmb{t}$ such that $\\pmb{t}_{j}\\pmb{t}_{j+1}\\cdot\\cdot\\cdot\\pmb{t}_{i}\\pmb{t}$ already exists in Dict, this token is rejected and the trajectory is resampled. By the prefix-free property of these tokens, if this trajectory visits a token in $D_{\\mathrm{valid}}$ , it must immediately be output as the next token. Note that this probability is lower bounded by, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbb{E}_{a^{\\prime}\\sim P(\\cdot|a)}\\left[\\operatorname*{Pr}_{\\mathfrak{t r a j}\\sim\\mathcal{T}_{a^{\\prime}}^{\\star}}\\left(\\operatorname*{min}_{\\substack{t\\in\\mathfrak{t r a j}\\cap D_{\\mathrm{vaid}}}}P(t|a)\\leq\\sqrt{\\delta/C d}\\bigg|a^{\\prime}\\right)\\right]\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "which is lower bounded by poly $(\\epsilon,\\delta)$ , the subject of Lemma B.10. Therefore with this probability, the process terminates in the first step with a token in $D_{\\mathrm{valid}}$ being sampled. ", "page_idx": 31}, {"type": "text", "text": "B.3 Analysis in the small dictionary case ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "In this section, we will prove Theorem B.3.2 and Theorem B.3.3. In particular we show that, either, ", "page_idx": 31}, {"type": "text", "text": "1. The dictionary is small with low probability. i.e., $\\mathrm{Pr}(|\\mathsf{D i c t}|<d_{0})=e^{-\\Omega(\\epsilon^{2}d/\\log^{2}(1/\\delta))}$ , or, 2. Or conditioned on the dictionary being small, $|\\mathsf{D i c t}|<d_{0}$ , with high probability $\\ge1-$ $e^{-\\Omega(\\epsilon^{2}d/\\log^{2}(1/\\delta))}$ , ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{Q\\in\\mathbb{Q}_{1\\cdot\\operatorname{gram}}}\\mathcal{L}(Q\\circ\\mathsf{e n c}(\\cdot))\\leq4\\left(1-\\frac{2d_{0}}{d}+O\\left(\\frac{1}{\\log(d)}\\right)\\right)H_{\\infty}+\\frac{2d_{0}}{d}\\cdot\\log(2|\\mathcal{A}|).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "For $i\\in[d]$ , define the indicator random variable, ", "page_idx": 31}, {"type": "text", "text": "X(s\u2032, Dict) = 1(\u2203a pair of tokens in $\\mathsf{e n c}_{\\mathrm{BPE}}(s^{\\prime})$ under Dict appears at least $\\log(d)$ times). ", "page_idx": 31}, {"type": "text", "text": "which captures the event that the string $s^{\\prime}$ is compressed well by the dictionary Dict under the sequential encoder. ", "page_idx": 31}, {"type": "text", "text": "Let $\\sf D i c t_{i}$ denote the dictionary stored by Algorithm 1 right after $\\mathsf{t e x t}_{i}$ is processed. The key insight behind this lemma is the following statement, asserting that the sequential encoder satisfies a \u201cmonotonicity\u201d property: for any $j$ and string $s^{\\prime}$ , if there exists a pair of tokens appearing more than $\\log(d)$ times consecutively in the sequential encoding of $s^{\\prime}$ under $\\mathsf{D i c t}_{j}$ , then there must exist a pair of tokens appearing more than $\\log(d)$ times consecutively in the greedy encoding of $s^{\\prime}$ under $\\sf{D i c t}_{i}$ for any $i<j$ . This implies that $X(\\pmb{\\mathscr{s}}^{\\prime},\\mathsf{D i c t}_{j})\\leq X(\\pmb{\\mathscr{s}}^{\\prime},\\mathsf{D i c t}_{i})$ if $i<j$ for any string $s^{\\prime}$ . This monotonicity property implies that the last dictionary output by the learner, $\\mathsf{D i c t}_{d}$ sequentially encodes a $1-\\epsilon$ fraction of the previously seen texts, $\\mathsf{t e x t}_{i}$ in a way where every pair of tokens appears at most $\\log(d)$ times. While $\\mathsf{D i c t}_{d}$ is correlated with these texts, we can circumvent this correlation by using a martingale argument to prove the statement of the lemma. ", "page_idx": 31}, {"type": "text", "text": "Lemma B.11. Let Dict be the dictionary returned by Algorithm 1. Then, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\operatorname*{min}\\left\\{\\operatorname*{Pr}\\left(\\mathbb{E}\\big[X\\left(s^{\\prime},D i c t\\right)\\big|D i c t\\big]\\geq2d_{0}/d\\Big||D i c t|<d_{0}\\right),\\operatorname*{Pr}\\left(|D i c t|<d_{0}\\right)\\right\\}\\leq e^{-\\epsilon^{2}d/8\\log^{2}(1/\\delta)}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $s^{\\prime}$ is a fresh substring of length d sampled from the stochastic source. ", "page_idx": 31}, {"type": "text", "text": "Proof. Let $\\mathsf{D i c t}_{i}$ denote the state of dictionary returned by Algorithm 1 right after $\\mathsf{t e x t}_{i}$ is processed. Then, $\\mathsf{D i c t}_{d}$ is the final dictionary returned by Algorithm 1. Suppose $\\mathbb{E}\\big[X\\big(\\bar{\\bullet}^{\\prime},\\mathsf{D i c t}_{d}\\big)\\big|\\,\\mathsf{D i c t}_{d}\\big]^{-}\\geq2d_{0}/d$ , ", "page_idx": 31}, {"type": "text", "text": "where $s^{\\prime}$ is a fresh substring of length $d$ sampled from the stochastic source. Using monotonicity of the sequential encoder, almost surely for any string $s^{\\prime}$ , $X(s^{\\prime},\\mathsf{D i c t}_{i})\\leq X(s^{\\prime},\\mathsf{D i c t}_{j})$ for any $j>i$ . Therefore, ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\big[X\\big({\\boldsymbol s}^{\\prime},\\mathsf{D i c t}_{d}\\big)\\big|\\mathsf{D i c t}_{d}\\big]\\geq2d_{0}/d\\implies\\sum_{i=1}^{d-1}\\mathbb{E}\\big[X\\big({\\boldsymbol s}^{\\prime},\\mathsf{D i c t}_{i}\\big)\\big|\\mathsf{D i c t}_{i}\\big]\\geq2d_{0}\\cdot\\frac{d-1}{d}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Note in this expectation, $s^{\\prime}$ is an independent string of length $d$ sampled from the stochastic source. Since $\\sf{D i c t}_{i}$ and $\\mathsf{t e x t}_{i+1}$ are independent, we may instead write, ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sum_{i=1}^{d-1}\\mathbb{E}\\big[X\\big(\\mathbf{text}_{i+1},\\mathsf{D i c t}_{i}\\big)\\big|\\mathsf{D i c t}_{i},\\mathsf{t e x t}_{i},\\mathsf{D i c t}_{i-1},\\cdots,\\mathsf{D i c t}_{1},\\mathsf{t e x t}_{1}\\big]\\ge2d_{0}\\cdot\\frac{d-1}{d}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "For brevity, denote $\\begin{array}{r l r}{X_{i}}&{{}=}&{X(\\mathsf{t e x t}_{i+1},\\mathsf{D i c t}_{i})}\\end{array}$ and define the filtration $\\begin{array}{r l}{\\mathcal{F}_{i}}&{{}=}\\end{array}$ $\\sigma(\\{{\\sf t e x t}_{1},{\\sf D i c t}_{1},\\cdots,{\\sf t e x t}_{i},{\\sf D i c t}_{i}\\})$ . Note that $\\begin{array}{r}{\\sum_{j=1}^{i}X_{j}~-~\\mathbb{E}[X_{j}|\\mathcal{F}_{i}]}\\end{array}$ forms a martingale sequence under the filtration $\\{{\\mathcal{F}}_{i}:i\\in[d]\\}$ . Therefore, by the Azuma-Hoeffding inequality, for any $\\eta>0$ , ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left(\\sum_{i=1}^{d-1}\\mathbb{E}[X_{i}|\\mathcal{F}_{i}]-X_{i}\\leq-\\eta\\right)\\leq e^{-\\eta^{2}}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Under Case I, we have that $\\textstyle\\sum_{i=1}^{d}X_{i}\\leq d_{0}$ . Therefore, from eq. (25) and eq. (26), ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{v}_{\\boldsymbol{\\Upsilon}}\\Big(|{\\sf D i c t}|<d_{0};\\;\\mathbb{E}\\left[X(s^{\\prime},\\sf D i c t)|\\boldsymbol{\\mathrm{Dict}}\\right]\\geq2d_{0}/d\\right)\\leq\\operatorname*{Pr}\\left(\\displaystyle\\sum_{i=1}^{d-1}X_{i}<d_{0};\\,\\displaystyle\\sum_{i=1}^{d-1}\\mathbb{E}\\left[X_{i}|\\mathcal{F}_{i}\\right]\\geq2d_{0}\\cdot\\frac{d-1}{d}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\operatorname*{Pr}\\left(\\displaystyle\\sum_{i=1}^{d-1}\\mathbb{E}\\left[X_{i}|\\mathcal{F}_{i}\\right]-X_{i}\\geq d_{0}\\cdot\\frac{d-2}{d}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq e^{-d_{0}^{2}(1-2/d)^{2}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq e^{-d_{0}^{2}/2}=e^{-\\epsilon^{2}d/8\\log^{2}(1/\\delta)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Finally, using the inequality $\\operatorname*{Pr}(A,B)=\\operatorname*{Pr}(A|B)\\operatorname*{Pr}(B)\\geq(\\operatorname*{min}\\{\\operatorname*{Pr}(A),\\operatorname*{Pr}(B)\\})^{2}$ completes the proof. \u53e3 ", "page_idx": 32}, {"type": "text", "text": "Proofs of Theorem B.3.2 and Theorem B.3.3 If $\\mathrm{Pr}(|\\mathrm{Dict}|<d_{0})\\leq\\epsilon^{-\\epsilon^{2}d/8\\log^{2}(1/\\delta)}$ the proof of Theorem B.3.2 concludes. Otherwise, consider the case $\\mathrm{Pr}(|\\mathrm{Dict}|\\,<\\,d_{0})\\,>\\,\\epsilon^{-\\epsilon^{2}d/8\\log^{2}(1/\\delta)}$ , whereby, $\\mathbb{E}[X(s^{\\prime},\\mathsf{D i c t})|\\mathsf{D i c t}]\\,\\leq\\,2d_{0}/d$ with probability $\\geq1-e^{-\\epsilon^{2}d/8\\log^{2}(1/\\delta)}$ conditioned on $|\\mathsf{D i c t}|<d_{0}$ by Lemma B.11. Recall that when $|\\mathsf{D i c t}|<d_{0}$ , Algorithm 1 uses a parallel implementation of the sequential encoder which chunks a new string into pieces of length $d$ , denoted $\\{\\mathsf{c h u n k}_{i}:$ $i\\in[d]\\}$ and uses the sequential encoder under $\\mathsf{D i c t}_{d}$ to tokenize each chunk. Note that since the source is Markovian, the chunked process $\\{{\\mathsf{c h u n k}}_{i}=(X_{i d+1},X_{i d+2},\\cdot\\cdot\\cdot\\,,X_{(i+1)d}):i=1,2,\\cdot\\cdot\\cdot\\,\\}$ is also Markovian and ergodic. Therefore, by a similar limiting argument as in Lemma A.4, using the Krylov\u2013Bogolyubov argument (cf. Proposition 4.2 in Chen (2018)) for Markov processes, we have that, ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{\\ell\\to\\infty}\\frac{\\sum_{i=1}^{\\ell}X(\\mathsf{c h u n k}_{i},\\mathsf{D i c t})}{\\ell}=\\mathbb{E}[X(s^{\\prime},\\mathsf{D i c t})]\\leq\\frac{2d_{0}}{d}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $s^{\\prime}$ is a fresh string of length $d$ sampled with initial state distribution as the stationary measure of the stochastic source. On the remaining (limiting) $1-2d_{0}/d$ fraction of the chunks, their sequential encodings have every pair of tokens appearing at most $\\log(d)$ times consecutively. Using Theorem 1 of Navarro and Russo (2008), the number of tokens in the encoding of each of these chunks cannot be too large, and satisfies, ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\ |\\mathsf{e n c}_{\\mathsf{B P E}}(\\mathsf{c h u n k}_{i})|\\cdot\\log\\vert\\mathsf{e n c}_{\\mathsf{B P E}}(\\mathsf{c h u n k}_{i})\\vert\\le2d H_{\\infty}+O(d/\\log(d))}\\\\ &{\\Longrightarrow\\vert\\mathsf{e n c}_{\\mathsf{B P E}}(\\mathsf{c h u n k}_{i})\\vert\\cdot\\log d\\le2d H_{\\infty}+O(d/\\log(d))}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "For the (limiting) $2d_{0}/d$ fraction of the \u201cbad\u201d chunks, their sequential encodings may have one or more pairs of tokens which appear more than $\\log(d)$ times consecutively. ", "page_idx": 32}, {"type": "text", "text": "Define $\\mathcal{E}_{i}=\\{X(\\mathsf{c h u n k}_{i},\\mathsf{D i c t})=1\\}$ where $\\mathsf{D i c t}=\\mathsf{D i c t}_{d}$ is the dictionary returned by Algorithm 1 and consider the unigram model $\\begin{array}{r}{Q_{\\mathrm{uni}}(t)=\\frac{1}{2}Q_{1}(t)+\\frac{1}{2}Q_{2}(t)}\\end{array}$ , which is the uniform mixture of two models, ", "page_idx": 33}, {"type": "equation", "text": "$$\nQ_{1}(t)\\propto\\frac{1}{(2|A|)^{|t|}},\\quad\\mathrm{~and~}\\quad Q_{2}(t)=\\mathbb{E}\\left[\\frac{n_{t}^{1}}{|\\mathsf{e n c}_{\\mathrm{BPE.spit}}(\\mathsf{c h u n k}_{1})|}\\bigg|\\mathcal{E}_{1}^{c}\\right],\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "and let $\\begin{array}{r}{Q_{\\mathrm{uni}}(t_{1},\\cdot\\cdot\\cdot,t_{i})=Q_{\\#}(j)\\prod_{j=1}^{i}Q_{\\mathrm{uni}}(t_{i})}\\end{array}$ for some distribution $Q_{\\#}(i)$ over the number of tokens to be chosen later. We will analyze the case where the total number of chunks $\\ell$ is finite and take the limit $m\\rightarrow\\infty$ later. Then, the overall loss of the algorithm is, ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{C_{m}(Q_{\\mathrm{un}}\\circ\\mathsf{e n c}(\\cdot))}\\\\ &{\\,=\\,-\\displaystyle\\sum_{t\\in\\mathbb{D}}\\mathbb{E}\\bigg[\\log\\mathsf{Q}_{\\mathrm{un}}(\\Theta\\mathsf{r}\\mathbf{g}_{\\mathrm{anF},\\mathrm{spin}}(s))\\bigg]}\\\\ &{\\,=\\,-\\displaystyle\\sum_{t\\in\\mathbb{D}}\\mathbb{E}\\bigg[n_{t}\\log Q_{\\mathrm{un}}(t)+\\log Q_{\\mathrm{un}}(|\\mathsf{e n c}_{\\mathrm{BF},\\mathrm{spin}}(s)|)\\bigg]}\\\\ &{\\,\\overset{(i)}{=}\\displaystyle-\\sum_{i=1}^{t}\\mathbb{E}\\left[\\sum_{t\\in\\mathbb{D}\\backslash\\mathrm{e}}n_{t}^{i}\\log Q_{\\mathrm{un}}(t)\\right]+\\log(m)}\\\\ &{\\,=\\,-\\displaystyle\\sum_{i=1}^{t}\\mathbb{E}\\left[\\sum_{t\\in\\mathbb{D}\\backslash\\mathrm{e}}n_{t}^{i}\\log Q_{\\mathrm{un}}(t)\\bigg|\\mathcal{E}_{i}\\right]\\operatorname*{Pr}(\\mathcal{E}_{i})+\\mathbb{E}\\left[\\sum_{t\\in\\mathbb{D}\\backslash\\mathrm{e}}n_{t}^{i}\\log Q_{\\mathrm{un}}(t)\\bigg|\\mathcal{E}_{i}^{c}\\right]\\operatorname*{Pr}(\\mathcal{E}_{i}^{c})+\\log(m).}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $n_{t}^{i}$ is the number of times $\\pmb{t}$ is observed in the BPE encoding of chunki and $(i)$ uses the fact that $|\\mathsf{e n c}_{\\mathrm{BPE.split}}(s)|$ follows some distribution supported on $[m]$ , which implies its entropy is upper bounded by $\\log(m)$ . First observe that, ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{\\ell}\\mathbb{E}\\left[\\sum_{t\\in\\mathsf{D i c t}}n_{t}^{i}\\log Q_{\\mathsf{m i}}(t)\\bigg|\\mathcal{E}_{i}^{c}\\right]\\leq\\sum_{i=1}^{\\ell}\\mathbb{E}\\left[\\mathsf{e n c}_{\\mathsf{B P E}}(\\mathsf{c h u n k}_{i})\\big|\\cdot\\sum_{t\\in\\mathsf{D i c t}}\\frac{n_{t}^{i}}{|\\mathsf{e n c}_{\\mathsf{B P E}}(\\mathsf{c h u n k}_{i})|}\\log Q_{\\mathsf{u n i}}(t)\\right]}\\\\ {\\overset{(i)}{\\leq}\\ell\\left(\\frac{2d H_{\\infty}+O(d/\\log(d))}{\\log(d)}\\right)\\sum_{t\\in\\mathsf{D i c t}}Q_{2}(t)\\log Q_{\\mathsf{u n i}}(t)}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $(i)$ uses the upper bound on $|\\mathsf{e n c}_{\\mathrm{BPE.split}}(\\mathsf{c h u n k}_{i})|$ under the event $\\mathcal{E}_{i}^{c}$ (eq. (27)). Since $\\begin{array}{r}{Q_{\\mathrm{uni}}(t)=\\frac{1}{2}Q_{1}(t)+\\frac{1}{2}Q_{2}(t)\\geq\\frac{1}{2}Q_{2}(t)}\\end{array}$ and $Q_{2}$ is a distribution supported on at most $d$ tokens, this term results in the upper bound, ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{\\ell}\\mathbb{E}\\left[\\sum_{t\\in\\mathrm{Dict}}n_{t}^{i}\\log Q_{\\mathrm{uni}}(t)\\bigg|\\mathcal{E}_{i}^{c}\\right]\\leq\\ell\\left(\\frac{2d H_{\\infty}+O(d/\\log(d))}{\\log(d)}\\right)\\log(2d).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "On the other hand, since $\\begin{array}{r}{Q_{\\mathrm{uni}}(t)\\ge\\frac{1}{2}Q_{1}(t)}\\end{array}$ , ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{i=1}^{\\ell}\\mathbb{E}\\left[\\sum_{t\\in\\mathbb{D}\\mathrm{ict}}n_{t}^{i}\\log(1/Q_{\\mathrm{uni}}(t))\\Bigg|\\mathcal{E}_{i}\\right]\\leq\\displaystyle\\sum_{i=1}^{\\ell}\\mathbb{E}\\left[\\sum_{t\\in\\mathbb{D}\\mathrm{ict}}n_{t}^{i}\\log(2/Q_{1}(t))\\Bigg|\\mathcal{E}_{i}\\right]}&{}\\\\ {\\displaystyle\\leq\\displaystyle\\sum_{i=1}^{\\ell}\\mathbb{E}\\left[\\sum_{t\\in\\mathbb{D}\\mathrm{ict}}n_{t}^{i}\\left(\\log(2)+|t|\\log(2|A|)\\right)\\Bigg|\\mathcal{E}_{i}\\right]}&{}\\\\ {\\displaystyle}&{\\leq\\ell d\\log(2)+\\ell d\\log(2|A|)}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where the last inequality uses the fact that $\\textstyle\\sum_{t\\in{\\mathsf{D i c t}}}n_{t}^{i}\\leq d$ and $\\begin{array}{r}{\\sum_{t\\in\\mathsf{D i c t}}|t|n_{t}^{i}=d}\\end{array}$ computes the length of chunki. ", "page_idx": 33}, {"type": "text", "text": "Overall, since $\\begin{array}{r}{\\sum_{i=1}^{\\ell}\\operatorname*{Pr}({\\mathcal E}_{i})\\leq2d_{0}/d}\\end{array}$ by eq. (27), combining this with eqs. (28) to (30), ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathcal{L}_{m}(Q_{\\mathrm{uni}}\\circ\\mathsf{e n c}(\\cdot))\\leq\\left(1-\\frac{2d_{0}}{d}\\right)\\ell\\left(\\frac{2d H_{\\infty}+O(d/\\log(d))}{\\log(d)}\\right)\\log(2d)+\\frac{2d_{0}}{d}\\ell d\\log(4|\\mathcal{A}|).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Dividing throughout by the length of the character sequence $m\\in[d(\\ell-1),d\\ell]$ and letting $\\ell\\to\\infty$ , ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{2\\in Q_{1:p a n}}\\mathcal{L}(Q\\circ\\mathsf{e n c}(\\cdot))\\leq\\mathcal{L}(Q_{\\mathsf{u n i}}\\circ\\mathsf{e n c}(\\cdot))\\leq\\left(1-\\frac{2d_{0}}{d}\\right)\\left(2H_{\\infty}+O\\left(\\frac{1}{\\log(d)}\\right)\\right)+\\frac{2d_{0}}{d}\\log(4|A|).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "C Additional Theoretical Results II: Learning the likelihood model ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "The guarantees we prove in Theorems 3.1, 3.6 and B.2 on various tokenizers assume that the downstream model is trained optimally. In practice, these models are trained from a finite dataset and the sample complexity of learning this likelihood model scales with the number of tokens in the dictionary. In this section, we step away from the transformer architecture and focus on analyzing the performance of a simple estimator for the unigram model based on Laplace smoothing. We leave the problem of analyzing the finite-sample statistical error of simple transformer models trained with gradient descent as an interesting open direction for future research. ", "page_idx": 34}, {"type": "text", "text": "The result of Theorem 3.1 establishes that under appropriate assumptions on the Markov source, there exists a tokenizer $\\tau$ and a unigram model over tokens $Q^{\\star}\\in\\mathcal{Q}_{1-\\mathrm{gram}}$ such that, ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{m\\rightarrow\\infty}{\\operatorname*{lim}}\\frac{1}{m}\\mathbb{E}\\left[\\log(1/Q^{\\star}(\\mathsf{e n c}(s))\\right]}&{}\\\\ {\\quad\\leq(1+\\varepsilon)\\cdot\\underset{m\\rightarrow\\infty}{\\operatorname*{lim}}\\frac{1}{m}\\mathbb{E}\\left[\\log(1/P(s))\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Or in other words, ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{m\\to\\infty}\\frac{1}{m}{\\mathsf{K L}}(P,Q^{\\star}(\\mathsf{e n c}(\\cdot)))\\leq\\varepsilon\\cdot\\operatorname*{lim}_{m\\to\\infty}\\frac{1}{m}\\mathbb{E}\\left[\\log(1/P(s))\\right].\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "This implies that with the appropriate tokenization, the measure associated to the string by the best unigram model over tokens is close to that induced by the true Markov distribution over characters in KL divergence. In this section, we establish finite-sample guarantees on learning $Q^{\\star}$ specifically for the LZW tokenizer. The approach we consider for distribution learning is a smoothed Laplace estimator described in more detail in Algorithm 2. ", "page_idx": 34}, {"type": "text", "text": "For any constant $\\theta\\in(0,1)$ , define $\\mathcal{E}_{\\theta}$ as the event that every maximal token $\\pmb{t}$ (Definition A.5) in the LZW dictionary satisfies $1/d^{1-\\theta}\\geq\\operatorname*{max}_{a}P({\\boldsymbol{t}}|a)\\geq\\delta/d^{1+\\theta}$ . By Lemmas A.10 and A.11 when the LZW tokenizer is trained on a dataset of size $\\widetilde{\\Omega}_{\\delta}(d)$ drawn from a stochastic source satisfying Assumption 3.2, $\\mathcal{E}_{\\theta}$ occurs with probability $\\geq1-d^{-\\Omega_{\\theta,\\delta}(\\log(d))}$ . ", "page_idx": 34}, {"type": "text", "text": "Theorem C.1. Consider any constant $\\theta\\in(0,1)$ , failure probability $\\eta\\in(0,1)$ and approximation error $\\xi\\in(0,1)$ . Assume that the learnt LZW tokenizer $\\mathcal{T}_{L Z W}$ satisfies the event $\\mathcal{E}_{\\theta}$ , which occurs with probability $\\geq1-d^{-\\Omega_{\\theta,\\delta}(\\log(d))}$ . Assume that $d^{1-3\\theta}\\geq1+\\delta^{-2}$ and that the stochastic source satisfies Assumption 3.2. For an absolute constant $C>0$ , assume that the size of the training dataset is at least ${n}_{l m}^{\\star}(\\xi)$ , where, ", "page_idx": 34}, {"type": "equation", "text": "$$\nn_{l m}^{\\star}\\triangleq\\frac{C d^{1+\\theta}\\log^{3}(d/\\eta\\delta)\\log\\log(d/\\eta)}{\\delta\\xi^{2}}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Then, Algorithm 2 learns a unigram model $\\widehat{Q}$ such that, ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\widehat{Q}\\circ\\pmb{\\mathscr{e}}n c_{g r e}(\\cdot))\\leq(1+\\xi)\\operatorname*{min}_{Q\\in\\mathcal{Q}_{1\\cdot g r a m}}\\mathcal{L}(Q\\circ\\pmb{\\mathscr{e}}n c_{g r e}(\\cdot))\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "with probability $\\geq1-\\eta$ . ", "page_idx": 34}, {"type": "text", "text": "In conjunction with Theorem 3.6, this gives end-to-end guarantees on the cross-entropy loss of the LZW tokenizer (with vocabulary size $\\leq d)$ with the Laplace estimator as the downstream unigram model. We instantiate this result choosing $\\theta=0.01$ in Theorem C.1. ", "page_idx": 34}, {"type": "text", "text": "Corollary C.2. Choose any $\\xi\\in(0,1)$ . Suppose the data source satisfies Assumption 3.2. On a dataset of size $\\widetilde{\\Omega}_{\\delta}(d)$ drawn from the source, train an LZW tokenizer $\\mathcal{T}_{L Z W}$ with d tokens. Subsequently, using Algorithm 2, learn a unigram model $\\widehat{Q}$ using a dataset of size at least $\\widetilde\\Omega(d^{1.01}/\\delta\\xi^{2})$ drawn from the source. Then, with probability $\\geq1-d^{-\\Omega_{\\delta}(\\log(d))}$ , ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\widehat{Q}\\circ\\pmb{e}n c_{g r e}(\\cdot))\\leq\\frac{1+\\xi}{1-\\varepsilon}\\operatorname*{min}_{Q}\\mathcal{L}(Q),\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where $\\begin{array}{r}{\\varepsilon=\\frac{\\log(1/\\delta)}{0.99\\log(d)}}\\end{array}$ . ", "page_idx": 34}, {"type": "text", "text": "The analysis of Theorem C.1 relies on showing that the distribution over tokens induced when a string sampled from the data source is encoded into tokens by the greedy encoder and the LZW dictionary is a Markov process. In general, given a set of previously sampled tokens $\\pmb{t}_{1},\\cdots\\pmb{\\mathscr{s}}_{},\\pmb{t}_{i}$ , the next token $\\pmb{t}_{i+1}$ is sampled from the distribution $P(\\mathbf{t}_{i+1}|t_{i};\\forall j\\in[i]$ , $t_{i-j+1}\\cdot\\cdot\\cdot t_{i}t_{i+1}\\not\\in\\mathsf{D i c t})$ . The conditioning is to simply guarantee that the previous tokens which were sampled were indeed maximal, since if $\\pmb{t}_{i}\\pmb{t}_{i+1}\\in\\mathsf{D i c t}$ , then the previous token returned would in fact have been this longer token and not $\\pmb{t}_{i}$ (and likewise for $\\pmb{t}_{i-1}\\pmb{t}_{i}\\pmb{t}_{i+1}$ and so on). While in general, this process is complicated and depends on all the previous tokens sampled, for the LZW dictionary, we show that the conditioning $\\{\\bar{\\forall}j\\in[i]$ , $t_{i-j+1}\\cdot\\cdot\\cdot t_{i}t_{i+1}\\notin\\mathsf{D i c t}\\}$ can be removed, thereby resulting in a simple Markov process over tokens. ", "page_idx": 35}, {"type": "text", "text": "Furthermore, we establish that this Markov process has a relatively large spectral gap. The optimal unigram model ends up being the stationary distribution over tokens induced by greedy encoder. Given the large spectral gap of the Markov process over tokens, estimating the stationary distribution of this process in KL divergence ends up being closely related to estimating a distribution from i.i.d. samples in the same metric. For this problem, the de-facto choice of estimator is the Laplace estimator, and several existing results provide finite-sample bounds on the KL divergence (Braess and Sauer, 2004; Han et al., 2021; Mourtada and Ga\u00efffas, 2022). The Laplace estimator (Line 6 of Algorithm 2) is simply a smoothed empirical estimate to account for the degeneracy of the KL divergence in its second argument as any coordinate approaches 0. The non-i.i.d.ness of the Markov process is circumvented by using concentration inequalities which are a function of the spectral gap (Naor et al., 2020). ", "page_idx": 35}, {"type": "text", "text": "Algorithm 2 Training likelihood model on tokens ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Input: A training dataset of size $n_{\\mathrm{lm}}$ , likelihood model class $\\mathcal{Q}$ , likelihood model training algorithm TrainLM Output: Likelihood model $Q\\in\\mathcal{Q}$ .   \n1: Tokenize the training dataset into a sequence of tokens $\\mathcal{T}=(t_{1},\\cdot\\cdot\\cdot\\,,t_{i})$ .   \n2: Train a likelihood model $Q$ on the tokenized dataset $\\tau$ using the TrainLM $(\\tau,\\mathcal{Q})$ subroutine. // In the case of $\\mathcal{Q}=\\mathcal{Q}_{\\mathrm{1-gram}}$ use the Laplace estimator def TrainLM $(\\mathcal{T},\\mathcal{Q}_{1-\\mathrm{gram}})$ :   \n3: Truncate the dataset to the first $n^{\\prime}=\\lfloor n_{\\mathrm{lm}}/\\ell_{\\mathrm{max}}\\rfloor$ tokens where $\\ell_{\\mathrm{max}}=4\\log(d|A|)/\\delta$ . Let the truncated dataset be $\\mathcal{T}_{\\mathrm{trunc}}$   \n4: Construct the unigram modelQ  withQ # = Unif([m]) andQ tok(t) =ntn+t|+Di1ct // $n_{t}$ is the number of times $^t$ appears in $\\mathcal{T}_{\\mathrm{trunc}}$ . // Test sequences are assumed to be of length $m$ . ", "page_idx": 35}, {"type": "text", "text": "C.1 Proof of Theorem C.1 ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Since $\\mathcal{T}_{\\mathrm{LZW}}$ uses the greedy encoder, the cross-entropy loss of the unigram model learnt by Algorithm 2 is, ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}(\\hat{Q}\\circ\\mathsf{e n c}_{\\mathsf{g r e}}(\\cdot))-\\underset{Q\\in\\mathbb{Q}_{1:\\mathsf{g r e}}}{\\operatorname*{min}}\\mathcal{L}(Q\\circ\\mathsf{e n c}_{\\mathsf{g r e}}(\\cdot))}\\\\ &{=\\underset{Q\\in\\mathbb{Q}_{1:\\mathsf{g r e}}}{\\operatorname*{max}}\\underset{m\\to\\infty}{\\operatorname*{lim}}\\frac{1}{m}\\mathbb{E}[\\log(Q(\\mathsf{e n c}_{\\mathsf{g r e}}(s))/\\hat{Q}(\\mathsf{e n c}_{\\mathsf{g r e}}(s)))]}\\\\ &{\\overset{(i)}{=}\\underset{Q\\in\\mathbb{Q}_{1:\\mathsf{g r e}}}{\\operatorname*{max}}\\underset{m\\to\\infty}{\\operatorname*{lim}}\\frac{1}{m}\\mathbb{E}\\left[\\left|\\mathsf{e n c}_{\\mathsf{g r e}}(s)\\right|\\underset{t\\in\\mathbb{R}\\backslash\\mathsf{G r e}}{\\sum}\\frac{n_{t}}{|\\mathsf{e n c}_{\\mathsf{g r e}}(s)|}\\log(Q_{\\mathsf{t o k}}(t)/\\hat{Q}_{\\mathsf{t o k}}(t))\\right]+\\frac{\\log(m)}{m}}\\\\ &{\\overset{(i i)}{\\leq}\\underset{m\\to\\infty}{\\operatorname*{lim}}\\frac{1}{m}\\mathbb{E}\\left[\\left|\\mathsf{e n c}_{\\mathsf{g r e}}(s)\\right|\\underset{t\\in\\mathbb{R}\\backslash\\mathsf{G r e}}{\\sum}\\frac{n_{t}}{|\\mathsf{e n c}_{\\mathsf{g r e}}(s)|}\\log\\left(\\frac{n_{t}/|\\mathsf{e n c}_{\\mathsf{g r e}}(s)|}{\\hat{Q}_{\\mathsf{t o k}}(t)}\\right)\\right]+\\frac{\\log(m)}{m}}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where in $(i)$ we use the fact that $\\widehat{Q}_{\\#}=\\operatorname{Unif}([m])$ and in $(i i)$ we take the $\\operatorname*{max}\\{\\cdot\\}$ inside the limit and the expectation (Fatou\u2019s lemma  and Jensen\u2019s inequality) and plug in the maximizer of the negative cross-entropy, Qtok(t) = |encngrte(s)|. Note that limm\u2192\u221e|encngrte(s)| $\\begin{array}{r}{\\operatorname*{lim}_{m\\rightarrow\\infty}\\frac{n_{t}}{|\\mathsf{e n c}_{\\mathrm{gre}}(s)|}\\ \\overset{\\mathrm{a.s.}}{=}\\ Q_{\\mathrm{MLE}}(t)}\\end{array}$ by Lemma A.4. ", "page_idx": 35}, {"type": "text", "text": "Moreover, since $|\\mathsf{e n c}(s)|/m\\leq1$ and $\\widehat{Q}_{\\mathrm{tok}}(t)>0$ surely, by the Dominated Convergence Theorem, ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\widehat{Q}\\circ\\mathsf{e n c}_{\\mathrm{gre}}(\\cdot))-\\operatorname*{min}_{Q\\in\\mathscr{Q}_{1}\\mathrm{,~}\\mathtt{p a m}}\\mathcal{L}(Q\\circ\\mathsf{e n c}_{\\mathrm{gre}}(\\cdot))\\leq\\operatorname*{lim}_{m\\to\\infty}\\frac{1}{m}\\mathbb{E}[|\\mathsf{e n c}_{\\mathrm{gre}}(s)|]\\cdot\\mathsf{K L}(Q_{\\mathrm{MLE}},\\widehat{Q}_{\\mathrm{tok}})\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "By eq. (6), we have that for any tokenizer using the greedy encoder, ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{m\\to\\infty}\\frac{|{\\mathsf{e n c}}_{\\mathrm{gre}}({\\mathsf{s}})|\\left(H(Q_{\\mathrm{MLE}},P)-\\log(1/\\delta)\\right)}{m}\\overset{{\\mathrm{a.s.}}}{\\leq}H_{\\infty}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Furthermore under the event $\\mathcal{E}_{\\theta}$ which implies that the learnt dictionary is $(1-\\theta)$ -heavy hitting (cf. Definition A.5), which implies that, ", "page_idx": 36}, {"type": "equation", "text": "$$\nH(Q_{\\mathrm{MLE}},P)\\geq(1-\\theta)\\log(d).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Therefore, by almost sure boundedness, we have that, ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{m\\to\\infty}\\frac{1}{m}\\mathbb{E}\\left[\\mathsf{l e n c}_{\\mathsf{g r e}}(s)\\vert\\right]\\leq\\frac{H_{\\infty}}{(1-\\theta)\\log(d)-\\log(1/\\delta)}\\leq\\frac{\\operatorname*{min}_{Q\\in\\mathcal{Q}_{1:\\mathsf{g r a n}}}\\mathcal{L}(Q\\circ\\mathsf{e n c}(\\cdot))}{(1-\\theta)\\log(d)-\\log(1/\\delta)}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Putting this together with eq. (31), we have that, ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\widehat{Q}\\circ\\mathsf{e n c}_{\\mathtt{g r e}}(\\cdot))\\leq\\Big(1+\\mathsf{K L}(Q_{\\mathtt{M L E}},\\widehat{Q}_{\\mathtt{t o k}})\\Big)\\operatorname*{min}_{Q\\in\\mathcal{Q}_{1\\cdot\\mathtt{g r e}}}\\mathcal{L}(Q\\circ\\mathsf{e n c}_{\\mathtt{g r e}}(\\cdot)),\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "which uses the assumption $(1-\\theta)\\log(d)\\geq1+\\log(1/\\delta)$ . In the remainder of the proof we upper bound the KL term. ", "page_idx": 36}, {"type": "text", "text": "By the law of large numbers established in eq. (34) and the fact that $\\frac{n_{t}}{\\sum_{t^{\\prime}}n_{t^{\\prime}}}\\in[0,1]$ , we have that, ", "page_idx": 36}, {"type": "equation", "text": "$$\nQ_{\\mathrm{MLE}}(t)=\\operatorname*{lim}_{m\\to\\infty}\\mathbb{E}\\left[\\frac{n_{t}}{\\sum_{t^{\\prime}}n_{t^{\\prime}}}\\right]=\\operatorname*{lim}_{m\\to\\infty}\\frac{\\mathbb{E}\\left[n_{t}\\right]}{\\mathbb{E}\\left[\\sum_{t^{\\prime}}n_{t^{\\prime}}\\right]}=\\pi(t),\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where $\\pi(t)$ denote the stationary distribution over tokens induced by the greedy encoding process, which exists for the LZW tokenizer. This distribution is in fact an ergodic Markov process, as we discuss next. ", "page_idx": 36}, {"type": "text", "text": "By Lemmas A.10 and A.11, for any constant $\\theta\\in(0,1)$ , with probability $\\geq1-d^{-\\Omega_{\\theta,\\delta}(\\log(d))}$ , every maximal token in the the LZW dictionary satisfies $1/d^{1-\\theta}\\,\\geq\\,\\operatorname*{max}_{a}P(\\pmb{t}|a)\\,\\geq\\,\\delta/d^{1+\\theta}$ . Let $S_{\\mathrm{gre}}$ denote the set of tokens which have a non-zero probability (over a string drawn from the Markov source) of being chosen by the greedy encoder while encoding the string. More importantly, note that for any sequence of tokens $\\pmb{t}_{1},\\cdots,\\pmb{t}_{i}$ , the next token is necessarily in $S_{\\mathrm{gre}}$ and can be any token in this set. The reason for this is that for any $\\pmb{t}_{i},\\pmb{t}\\in S_{\\mathrm{gre}}$ , the concatenation $\\pmb{t}_{i}\\pmb{t}\\not\\in S_{\\mathrm{gre}}$ since $\\begin{array}{r}{\\operatorname*{max}_{a\\in\\mathcal{A}}P(t_{i}t|a)\\leq1/\\delta d^{2(1-\\theta)}}\\end{array}$ , which is smaller than the $\\begin{array}{r}{\\operatorname*{max}_{a\\in\\mathcal{A}}P({\\pmb t}^{\\prime}|a)\\geq\\delta/d^{1+\\theta}}\\end{array}$ for any token $t^{\\prime}\\in S_{\\mathrm{gre}}$ as long as $d^{1-3\\theta}\\geq1/\\delta^{2}$ . This constraint implies that in the sampling procedure in Figure 7, it suffices to drop the conditioning on the event $\\pmb{t}_{j}\\pmb{t}_{j+1}\\cdot\\cdot\\cdot\\pmb{t}_{i}\\pmb{t}\\notin$ Dict while sampling the next token $\\pmb{t}$ . This condition automatically implies that the sequence of tokens conditionally follows a Markov process with $\\operatorname*{Pr}(t_{i+1}=t|t_{1},\\cdot\\cdot\\cdot\\cdot,\\bar{t}_{i})=P(t||\\mathsf{a s t}(\\bar{t}_{i}))$ . Since the probability of every transition is lower bounded, this means that the Markov chain is ergodic. Moreover, the pseudo-spectral gap (Naor et al., 2020), $1-\\lambda$ can be lower bounded by the Dobrushin contraction coefficient, $\\kappa$ , ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{1-\\lambda\\leq\\kappa\\triangleq\\displaystyle\\operatorname*{max}_{(t,t^{\\prime})\\in\\mathrm{Dict}^{2}}\\|\\operatorname*{Pr}(\\cdot|t)-\\operatorname*{Pr}(\\cdot|t^{\\prime})\\|_{\\mathrm{TV}}}\\\\ &{\\qquad\\qquad=\\displaystyle\\operatorname*{max}_{(t,t^{\\prime})\\in\\mathrm{Dict}^{2}}1-\\displaystyle\\sum_{t^{\\prime\\prime}\\in\\mathrm{Dict}}\\operatorname*{min}\\{\\operatorname*{Pr}(t^{\\prime\\prime}|t),\\operatorname*{Pr}(t^{\\prime\\prime}|t^{\\prime})\\}}\\\\ &{\\qquad\\qquad\\leq1-\\delta d/d^{1+\\theta}}\\\\ &{\\qquad=1-\\delta d^{-\\theta}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Recall that the learner is given a training dataset of $n_{\\mathrm{lm}}$ characters to train the likelihood model. By Lemma A.8, with probability $\\geq1-d^{-\\Omega(\\log(d/\\delta)/\\delta)}$ , in the run of the LZW tokenization algorithm, every token in the dictionary has length at most $\\ell_{\\mathrm{max}}=4\\log(d|A|)/\\delta$ . Therefore, suppose the learner ", "page_idx": 36}, {"type": "text", "text": "always truncates the dataset to the first $n^{\\prime}=\\lfloor n_{\\mathrm{lm}}/\\ell_{\\mathrm{max}}\\rfloor$ tokens and runs the Laplace estimator on this truncated dataset. With this, we move onto upper bounding, ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathsf{K L}(Q_{\\mathrm{MLE}},\\widehat{Q}_{\\mathrm{tok}})=\\sum_{t\\in\\mathrm{Dict}}\\pi(t)\\log\\Big(\\pi(t)/\\widehat{Q}_{\\mathrm{tok}}(t)\\Big)\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "which necessitates lower bounding $\\widehat{Q}_{\\mathrm{tok}}(t)$ for every $\\pmb{t}$ . Recall that the learner\u2019s estimate $\\widehat Q(t)$ in Algorithm 2 is the Laplac e estimator , $\\frac{n_{t}+1}{\\sum_{t^{\\prime}}n_{t^{\\prime}}+\\mathsf{D i c t}}$ , where $\\{n_{t}:t\\in\\mathsf{D i c t}\\}$ is computed by tru ncating the dataset to the first tokens. Firstly, by invoking Corollary 1.3 of Naor et al. (2020) for the function $\\begin{array}{r}{n_{t}=\\sum_{i=1}^{n^{\\prime}}\\mathbb{I}(t_{i}=t)}\\end{array}$ , ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left(\\left|n_{t}-\\mathbb{E}[n_{t}]\\right|\\geq c{\\sqrt{\\frac{\\mathbb{E}[n_{t}]}{1-\\lambda}\\cdot\\log(1/\\eta)}}\\right)\\leq\\eta\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "for a universal constant $c>0$ . In particular, this implies that with probability $\\geq1-\\eta$ , simultaneously for all $\\pmb{t}$ , ", "page_idx": 37}, {"type": "equation", "text": "$$\n|n_{t}-\\mathbb{E}[n_{t}]|\\leq\\Delta_{t}\\triangleq\\sqrt{\\frac{d^{\\theta}}{\\delta}\\mathbb{E}[n_{t}]\\cdot\\log(|\\mathsf{D i c t}|/\\eta)},\\,\\mathrm{and},\\,\\mathbb{E}[n_{t}]-n_{t}\\geq\\mathbb{E}[n_{t}].\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Under this event, for any $\\pmb{t}$ , the estimate is lower bounded by, ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{Q}_{\\mathrm{tok}}(t)=\\displaystyle\\frac{n_{t}+1}{n^{\\prime}+|\\mathrm{Dict}|}\\geq\\frac{\\mathbb{E}[n_{t}]+1-\\operatorname*{min}\\{\\mathbb{E}[n_{t}],\\Delta_{t}\\}}{n^{\\prime}+|\\mathrm{Dict}|}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\geq\\operatorname*{max}\\left\\{\\pi(t)-\\displaystyle\\frac{(\\Delta_{t}-1)\\,n^{\\prime}+|\\mathrm{Dict}|\\mathbb{E}[n_{t}]}{(n^{\\prime})^{2}+n^{\\prime}|\\mathrm{Dict}|},\\,\\frac{1}{n^{\\prime}+|\\mathrm{Dict}|}\\right\\}}\\\\ &{\\qquad\\qquad\\qquad\\geq\\operatorname*{max}\\left\\{\\pi(t)-\\displaystyle\\frac{\\Delta_{t}n^{\\prime}+|\\mathrm{Dict}|\\mathbb{E}[n_{t}]}{(n^{\\prime})^{2}},\\,\\frac{1}{n^{\\prime}+|\\mathrm{Dict}|}\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Suppose the following condition is satisfied, ", "page_idx": 37}, {"type": "equation", "text": "$$\nn^{\\prime}=\\frac{4r d^{\\theta}|\\mathsf{D i c t}|\\log(|\\mathsf{D i c t}|/\\eta)}{\\delta}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "for some $r\\geq4$ . Under this condition, we have that $n^{\\prime}\\ge2\\sqrt{r}\\Delta$ and $n^{\\prime}\\geq4r|\\mathsf{D i c t}|$ . ", "page_idx": 37}, {"type": "text", "text": "In this case, we have the upper bound, ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{Q}_{\\mathrm{tok}}(t)\\geq\\operatorname*{max}\\left\\{\\pi(t)-\\frac{2\\Delta_{t}}{n^{\\prime}},\\;\\frac{1}{n^{\\prime}+|\\mathrm{Dict}|}\\right\\}}\\\\ &{\\qquad=\\operatorname*{max}\\left\\{\\pi(t)-2\\frac{\\sqrt{\\frac{\\pi^{\\theta}}{\\delta}\\mathbb{E}[n_{t}]\\cdot\\log(|\\mathrm{Dict}|/\\eta)}}{n^{\\prime}},\\;\\frac{1}{n^{\\prime}+|\\mathrm{Dict}|}\\right\\}}\\\\ &{\\qquad\\geq\\operatorname*{max}\\left\\{\\pi(t)-\\sqrt{\\frac{\\pi(t)}{r|\\mathrm{Dict}|}},\\;\\frac{1}{2n^{\\prime}}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where the last inequality uses eq. (C1). ", "page_idx": 37}, {"type": "text", "text": "Consider two sub-cases, ", "page_idx": 37}, {"type": "text", "text": "Sub-case I. $\\pi(t)\\geq2/r|\\mathsf{D i c t}|$ . Define this event $\\mathcal{C}_{\\bf l}$ . ", "page_idx": 37}, {"type": "text", "text": "Here, ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\pi(t)\\log(\\pi(t)/\\widehat{Q}_{\\mathrm{tok}}(t))\\leq-\\pi(t)\\log\\left(1-\\sqrt{\\frac{1}{\\pi(t)r|\\mathsf{D i c t}|}}\\right)\\leq\\frac{3}{2}\\sqrt{\\frac{\\pi(t)}{r|\\mathsf{D i c t}|}}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Sub-case II. $\\pi(t)\\leq2/r|\\mathsf{D i c t}|$ . Define this event $\\mathcal{C}_{||}$ . ", "page_idx": 38}, {"type": "text", "text": "Here, ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pi(t)\\log(\\pi(t)/\\widehat{Q}_{\\mathrm{tok}}(t))\\leq\\pi(t)\\log\\left(2n^{\\prime}\\pi(t)\\right)\\leq\\operatorname*{max}\\left\\{0,\\frac{2}{r|{\\sf D i c t}|}\\log\\left(\\frac{4n^{\\prime}}{r|{\\sf D i c t}|}\\right)\\right\\}}\\\\ {\\leq\\frac{2}{r|{\\sf D i c t}|}\\log\\left(16d^{\\theta}\\log(|{\\sf D i c t}|/\\eta)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Case II. $\\Delta_{t}n^{\\prime}<|\\mathsf{D i c t}|\\mathbb{E}[n_{t}]$ . Define this event $\\mathcal{C}_{|||}$ . ", "page_idx": 38}, {"type": "text", "text": "In this case we have the upper bound, ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\widehat{Q}_{\\mathrm{tok}}(t)\\geq\\pi(t)-\\frac{2|\\mathsf{D i c t}|\\mathbb{E}[n_{t}]}{(n^{\\prime})^{2}}\\geq\\pi(t)-\\frac{\\pi(t)}{2r}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where the last inequality follows from eq. (C1). This implies that, ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\pi(t)\\log(\\pi(t)/\\widehat{Q}_{\\mathrm{tok}}(t))\\leq-\\pi(t)\\log(1-1/2r)\\leq\\frac{\\pi(t)}{r}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "By using the geometric ergodicity of this Markov process (eq. (33)), when $n^{\\prime}$ tokens are sampled from an arbitrary initial distribution, ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\left(1-\\kappa^{n^{\\prime}}\\right)\\pi(t)\\leq{\\frac{\\mathbb{E}[n_{t}]}{n^{\\prime}}}\\leq\\kappa^{n^{\\prime}}+\\left(1-\\kappa^{n^{\\prime}}\\right)\\pi(t)\\implies\\pi(t)\\leq{\\frac{{\\widehat Q}_{\\mathrm{tok}}(t)}{1-e^{-4r|\\mathbb{D}|\\mathrm{to}}(|\\mathbb{D}\\mathrm{te}|/\\eta)}}={\\frac{{\\widehat Q}_{\\mathrm{tok}}(t)}{1-d^{-r}}}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where in the implication, we use the condition on $n^{\\prime}$ in eq. (C1) and the bound on the contraction coefficient $\\kappa$ in eq. (33). ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{M}(\\mathcal{Q}_{\\mathbf{q},\\mathbf{k}},\\mathcal{Q}_{\\mathbf{q},\\mathbf{k}})}\\\\ &{=\\displaystyle-\\sum_{i\\in\\mathcal{I}(\\mathcal{I})}\\pi(\\varepsilon(t)/\\hat{Q}_{\\mathbf{k}}(t))}\\\\ &{\\phantom{\\le}\\displaystyle\\sum_{i\\in\\mathcal{I}(\\mathcal{I})}\\frac{\\pi(t)}{1-d^{-1}}\\log(\\pi(t)/\\hat{Q}_{\\mathbf{k}}(t))-\\log(1-d^{-\\gamma})}\\\\ &{\\le\\displaystyle\\sum_{i=1}^{1}\\sum_{r\\le j\\in\\mathcal{I}(\\mathcal{A})}\\left\\{\\pi(\\sum_{i}\\varrho(t)\\log(\\pi(t)/\\hat{Q}_{\\mathbf{k}}(t))+1(C_{1})\\pi)(\\pi(t)\\log(\\pi(t)/\\hat{Q}_{\\mathbf{k}}(t))+1(C_{1})\\pi)(t)\\log(\\pi(t)\\log(1-}\\\\ &{\\le\\displaystyle\\frac{1}{1-d^{-\\gamma}}\\sum_{i\\in\\mathcal{I}(\\mathcal{I})}\\log(\\mathbb{C}_{j}^{(\\mathbb{I})})\\left(\\frac{1(C_{1})}{r^{\\mathbb{D}}(\\mathbb{R}(t)\\right)}+\\frac{2}{\\mathbb{M}^{\\gamma}}\\frac{2}{\\mathbb{P}(\\mathbb{R}(t)\\log(1-))}\\log\\left(16\\ell^{3}\\log(|\\mathbb{D}\\mathbb{E}||\\mathcal{Q}_{t})\\right)+1(\\gamma_{\\mathbf{q},\\mathbf{1}}\\frac{\\pi(t)}{r}+2d^{-\\gamma}}\\\\ &{\\le\\displaystyle\\frac{1}{1-d^{-\\gamma}}\\left(\\frac{3}{2}\\sqrt{\\frac{\\mathbb{D}\\mathbb{E}}{\\mathbb{D}\\mathbb{E}}}\\right)+\\frac{2}{r}\\log(16\\ell^{3}\\log(|\\mathbb{D}\\mathbb{E}||\\mathcal{Q}_{t})+\\frac{1}{r}\\right)+2d^{-r}}\\\\ &{\\le\\displaystyle\\frac{1}{1-d^{-\\gamma}}\\left(\\frac{3}{2}\\sqrt{\\frac{\\mathbb{D}\\mathbb{E}}{\\mathbb{D}\\mathbb{E}}}\\right)}\\\\ &{\\le\\displaystyle\\sum_{i=1}^{\\frac{\\gamma}{\\gamma}}\\log(16\\ell^{\\alpha}|\\mathbb{C}||\\mathcal{Q}_{\\mathbf{k}}||),}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Combining with eq. (32), we get the bound, ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}(\\widehat{Q}\\circ\\mathsf{e n c}_{\\mathrm{gre}}(\\cdot))\\leq\\Big(1+\\mathsf{K L}(Q_{\\mathsf{M L E}},\\widehat{Q})\\Big)\\underset{Q\\in\\mathcal{Q}_{1:\\mathsf{g r a m}}}{\\operatorname*{min}}\\mathcal{L}(Q\\circ\\mathsf{e n c}_{\\mathrm{gre}}(\\cdot))}\\\\ &{\\qquad\\qquad\\qquad\\leq\\left(1+\\frac{5}{\\sqrt{r}}\\log(16d^{\\theta}\\log(d/\\eta))\\right)\\underset{Q\\in\\mathcal{Q}_{1:\\mathsf{g r a m}}}{\\operatorname*{min}}\\mathcal{L}(Q\\circ\\mathsf{e n c}_{\\mathrm{gre}}(\\cdot)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Rescaling $r$ to be $r(\\log(16d^{\\theta}\\log(d/\\eta)))^{2}$ completes the proof. ", "page_idx": 38}, {"type": "text", "text": "The proofs of the upper bounds in the paper (Theorems 3.6 and B.2) relied on showing that the entropy $H(Q_{\\mathrm{MLE}},P)$ is large, or in other words, the algorithm typically encodes new strings into long length (i.e. low probability under $P$ ) tokens. This statement about generalization to new strings is fundamentally different from having a tokenizer which compresses the training dataset well. In other words, consider the following modification: the measure $Q_{\\mathrm{MLE}}$ is defined as the expected empirical distribution over tokens when a new string is encoded into tokens, and not on the source dataset used to construct the dictionary. Suppose the definition of $Q_{\\mathrm{MLE}}$ is changed to the empirical distribution over tokens in the source dataset. Under this new definition of the MLE unigram model, the largeness of the $H(Q_{\\mathrm{MLE}},P)$ metric, in a sense, captures compressing the source dataset well. However, we show that in general, this does not result in good tokenizers that minimize the population cross-entropy loss, suffering from $\\begin{array}{r}{\\operatorname*{min}_{Q\\in\\mathcal{Q}_{\\mathrm{1-gram}}}\\mathcal{L}\\big(Q\\circ\\mathsf{e n c}(\\cdot)\\big)\\approx H(\\pi)\\gg H_{\\infty}}\\end{array}$ . ", "page_idx": 39}, {"type": "text", "text": "Theorem D.1. Consider the stochastic source in example A.1 having entropy rate $H_{\\infty}=\\delta\\log(1/\\delta)+$ $(1-\\delta)\\log(1/(1-\\delta))$ . Consider a training dataset of size $n$ . For a dictionary Dict and $t\\in D i c t,$ , define $\\begin{array}{r}{\\widehat{Q}_{M L E}(t)=\\frac{n_{t}(s_{s r c})}{|e n c(s_{s r c})|}}\\end{array}$ as the empirical distribution over tokens induced by the greedy encoder when encoding the training dataset, $s_{s r c}$ . There exists a dictionary Dict such that with probability $\\geq1-e^{-\\Omega(\\sqrt{n})}$ over the training dataset, ", "page_idx": 39}, {"type": "equation", "text": "$$\nH(\\widehat{Q}_{M L E},P_{\\gamma})\\geq n H_{\\infty}(1-O(n^{-1/4}))\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "is large. However, for this dictionary, for any encoding algorithm (including the greedy encoder), the resulting tokenizer $\\mathcal{T}=(D i c t,\\emptyset,e\\dot{n}\\dot{c}(\\cdot),d\\acute{e}c(\\cdot))$ satisfies, ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{Q\\in\\mathcal{Q}_{1\\cdot g r a m}}\\mathcal{L}(Q\\circ\\pmb{e n c(\\cdot)})\\geq(1-\\varepsilon)H(\\pi)\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where \u03b5 = 2ne\u2212nH\u221e(1\u2212O(n\u22121/4)). ", "page_idx": 39}, {"type": "text", "text": "Proof. Suppose the entire training dataset was compressed into a single token, $\\pmb{t}_{\\mathrm{src}}$ . The dictionary is $A\\cup t_{\\mathrm{src}}$ . In the following argument, we show that the number of occurrences, $n_{t_{\\mathrm{src}}}$ , of the entire training dataset $\\pmb{t}_{\\mathrm{src}}$ in a new string of length $m$ generated from the stochastic source, $\\pmb{s}$ , converges to its expectation as $m\\rightarrow\\infty$ . Let $\\pi_{n}^{(i)}$ denote the stationary distribution of the Markov process induced by the stochastic source over length- ${\\cdot n}$ strings with a shift of $i$ from the starting position, and let $n_{t}^{(i)}$ denote the number of times appears in the training dataset starting at the position for some $r>0$ . Then, ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{m\\rightarrow\\infty}\\frac{n_{t_{\\mathrm{sc}}}}{m}=\\frac{1}{n}\\operatorname*{lim}_{m\\rightarrow\\infty}\\sum_{i=0}^{n-1}\\frac{n_{t_{\\mathrm{sc}}}^{(i)}}{m/n}\\overset{\\mathrm{1}}{=}\\frac{1}{n}\\sum_{i=0}^{n-1}\\mathbb{E}_{t^{\\prime}\\sim\\pi_{n}^{(i)}}[P(t_{\\mathrm{src}}|t^{\\prime})]\\leq\\operatorname*{max}_{a\\in\\mathcal{A}}P(t_{\\mathrm{src}}|a).\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "The second equation follows by considering the Markov process induced over length $n$ strings and applying the Krylov\u2013Bogolyubov argument for ergodic and homogeneous Markov processes. ", "page_idx": 39}, {"type": "text", "text": "In Lemma D.2, we show that with probability $\\geq1-e^{-\\Omega({\\sqrt{n}})}$ , the token $\\pmb{t}_{\\mathrm{src}}$ constructed from the source dataset satisfies, $\\begin{array}{r}{\\operatorname*{max}_{a\\in\\mathcal{A}}P(t|a)\\le e^{-n H_{\\infty}(1-O(n^{-1/4}))}}\\end{array}$ . In other words, the source stri\u221ang has exponentially small probability. Combining this with eq. (38), with probability $\\geq1-e^{-\\Omega({\\sqrt{n}})}$ over the source dataset, the number of occurrences of the substring $\\pmb{t}_{\\mathrm{src}}$ in a new string $\\pmb{s}$ is upper bounded by, ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{m\\to\\infty}\\frac{n_{t_{\\mathrm{src}}}}{m}\\overset{\\mathrm{a.s.}}{\\leq}e^{-n H_{\\infty}(1-O(n^{-1/4}))}\\triangleq\\varepsilon/2n.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "By the Krylov\u2013Bogolyubov argument, for each $a\\,\\in\\,{\\mathcal{A}}\\,=\\,\\{0,1\\}$ , $\\begin{array}{r}{\\operatorname*{lim}_{m\\to\\infty}\\frac{n_{a}}{m_{.}}\\ \\overset{\\mathrm{a.s.}}{=}\\ \\pi(a)}\\end{array}$ . More importantly, the number of times $a$ is made as a token is upper bounded by $n_{a}$ and lower bounded by $n_{a}-n n_{t_{\\mathrm{src}}}$ . Therefore, ", "page_idx": 39}, {"type": "equation", "text": "$$\n(1-\\varepsilon)\\pi(a)=\\pi(a)-{\\frac{\\varepsilon}{2}}\\leq\\operatorname*{lim}_{m\\rightarrow\\infty}{\\frac{n_{a}}{m}}\\leq\\pi(a)={\\frac{1}{2}}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Finally, putting everything together, ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\operatorname*{min}_{\\substack{2\\in Q_{1;\\mathtt{g r a m}}\\,m\\to\\infty}}\\operatorname*{lim}_{m}\\underline{{1}}_{m}\\mathcal{L}_{m}(Q\\circ\\mathsf{e n c}(\\cdot))=\\displaystyle\\operatorname*{min}_{Q\\in Q_{1;\\mathtt{g r a m}}\\,m\\to\\infty}-\\frac{1}{m}\\mathbb{E}\\left[\\log(Q_{\\#}(|\\mathsf{e n c}(s)|)+\\sum_{t\\in\\mathtt{D i c t}}n_{t}\\log Q_{\\mathsf{t o t}}(|s)|)\\right]}&{}\\\\ {\\displaystyle\\ge\\operatorname*{min}_{Q\\in Q_{1;\\mathtt{g r a m}}\\,m\\to\\infty}-\\frac{1}{m}\\mathbb{E}\\left[\\sum_{a\\in A}n_{a}\\log Q_{\\mathsf{t o b}}(a)\\right]}&{}\\\\ {\\displaystyle\\overset{(i)}{\\ge}\\operatorname*{min}_{Q\\in\\mathcal{Q}_{1;\\mathtt{g r a m}}}-(1-\\varepsilon)\\sum_{a\\in A}\\pi(a)\\log Q_{\\mathsf{t o b}}(a)}&{}\\\\ {\\displaystyle\\ge(1-\\varepsilon)H(\\pi).}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where $(i)$ follows from the lower bound on $n_{a}/m$ in eq. (39). This completes the proof. ", "page_idx": 40}, {"type": "text", "text": "Lemma D.2. With probability $\\geq1-e^{-\\Omega({\\sqrt{n}})}$ over the source dataset, ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{a\\in\\mathcal{A}}P(t_{s r c}|a)\\leq e^{-n H(\\delta)(1-O(n^{-1/4}))}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Proof. Let $X$ denote the number of $i\\in[n-1]$ such that $\\pmb{s}_{i}\\neq\\pmb{s}_{i+1}$ in $\\pmb{s}$ , the stochastic source. Since the transition of the Markov process only depends on whether the next character is the same as the previous character, we can write down, ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{a\\in A}\\log P(t_{\\mathrm{src}}|a)=-(X+1)\\log(\\delta)-(n-1-X)\\log(1-\\delta).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Note that $X$ is a sum of $n-1$ i.i.d. random variables, since $\\mathbb{I}(\\pmb{\\mathscr{s}}_{i}\\neq\\pmb{\\mathscr{s}}_{i+1})\\sim\\mathsf{B e r}(\\delta)$ does not depend on whether $\\pmb{s}_{i}=0\\;\\mathrm{or}=1$ . In particular, by Hoeffding\u2019s inequality, we have that with probability \u22651 \u2212e\u2212\u2126( n), ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\left|\\frac{1}{n}\\operatorname*{max}_{a\\in\\mathcal{A}}\\log P(t_{\\mathrm{src}}|a)-H(\\delta)\\right|\\leq O\\left(n^{-1/4}\\right),\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "which uses the fact that $\\mathbb{E}[X]=\\delta(n-1)$ and $H_{\\infty}=\\delta\\log(1/\\delta)+(1-\\delta)\\log(1/(1-\\delta))$ . Taking an exponential on both sides proves the statement of the lemma. \u53e3 ", "page_idx": 40}, {"type": "text", "text": "E Additional Theoretical Results IV: Interaction between the dictionary and encoding algorithm ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "In this section, we show another kind of barrier to generalization, which brings out the relationship between the encoding algorithm and the dictionary. We show that there exist dictionaries which generalize under the minimal encoder, i.e. the encoding algorithm which encodes a string into the shortest number of possible tokens, but at the same time, completely fail to generalize under the greedy encoder. This means that in the process of constructing good tokenizers, it does not suffice to think about the dictionary in isolation. Its interaction with the encoding algorithm is pertinent. ", "page_idx": 40}, {"type": "text", "text": "Definition E.1 (minimal encoder). The minimal encoder parses a new string into the fewest possible number of tokens from the dictionary as possible. Ties are broken arbitrarily. ", "page_idx": 40}, {"type": "text", "text": "Theorem E.2. There exists a stochastic source parameterized by $\\delta~\\in~(0,0.5)$ and a dictionary Dict such that under the minimal encoder/decoder pair, the resulting tokenizer, $\\tau\\,=$ $(D i c t,\\emptyset,e n c_{m i n}(\\cdot),d e c_{m i n}(\\cdot))$ generalizes near-optimally, ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{Q\\in\\mathcal{Q}_{1\\cdot g r a m}}\\mathcal{L}(Q\\circ e n c_{m i n}(\\cdot))\\leq1.273H_{\\infty}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Here the entropy rate of the source, $H_{\\infty}$ , is $\\delta\\log(\\sqrt{2}/\\delta)+(1-\\delta)\\log(1/(1-\\delta))$ . However, the same dictionary Dict under the greedy encoder/decoder pair, i.e. $T^{\\prime}=(D i c t,\\emptyset,e n c_{g r e}(\\cdot),d e c_{g r e}(\\cdot))$ , generalizes poorly, suffering from cross-entropy scaling as, ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{Q\\in\\mathscr{Q}_{1-g r a m}}\\mathcal{L}(Q\\circ e n c_{g r e}(\\cdot))\\geq\\frac{1-o_{\\delta}(1)}{3}H(\\pi).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where the entropy of the stationary distribution of the source is $\\begin{array}{r}{H(\\pi)=\\frac{1}{2}\\log(8)}\\end{array}$ and the $1-o\\delta(1)$ term is $(1-\\delta)^{2}(1+\\delta)^{-1}$ . ", "page_idx": 40}, {"type": "image", "img_path": "wm9JZq7RCe/tmp/0694326ce9c1af03321355316a1ade4ed39e3abc2dad9b5bbf12a67d0f784ad4.jpg", "img_caption": ["Figure 10: order-1 Markov source used in the proof of Theorem E.2 "], "img_footnote": [], "page_idx": 41}, {"type": "text", "text": "This means that the greedy encoder is not really compatible with the dictionary in the sense that the cross-entropy loss of the tokenizer is a constant multiple away from that achieved by the characterlevel tokenizer. The separation between eq. (40), and eq. (41) only manifests as $\\delta$ becomes smaller and smaller. ", "page_idx": 41}, {"type": "text", "text": "In this section, we prove that generalization of a dictionary is a function of the underlying tokenization algorithm used. In particular, the greedy encoder is not universal, and there exists dictionaries under the minimum-length encoder/decoder which achieve small cross-entropy loss, which do not generalize under the greedy encoder/decoder. ", "page_idx": 41}, {"type": "text", "text": "We split the proof of Theorem E.2 into two parts. We first define the stochastic source and dictionary we consider. Then we show that under the minimum-length encoder, the asymptotic cross-entropy loss is upper bounded by $H_{\\infty}$ up to a constant. Finally, we show that under the greedy-encoder, the same dictionary suffers from high cross-entropy loss, which is a constant factor away from that of the character encoder. ", "page_idx": 41}, {"type": "text", "text": "E.1 Stochastic source and dictionary. ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Consider an extension of the switching Markov source in example A.1 to $\\mathcal{A}=\\{0,1,2\\}$ . The Markov chain is described in Figure 10. The transition of the Markov chain is $P(0|0)=P(1|1)=P(2|2)=$ $1-\\delta$ , and $P(1|0)=\\bar{P(2|1)}=\\delta$ and $P(2|1)=P(0|1)=\\delta/2$ , with the remaining transitions being 0-probability. For a parameter $\\ell>0$ to be instantiated later, define $S_{1}$ (resp. $S_{\\mathbf{0}},S_{\\mathbf{2}},$ ) as the set of all-1 (resp. all-0, all-2) strings of length $\\le\\ell-1$ , including the empty string. Consider a dictionary composed of the following set of tokens, $\\{1s:s\\in S_{\\mathbf{0}}\\cup S_{\\mathbf{1}}\\cup S_{\\mathbf{2}}\\}$ . Therefore, the tokens follow the template $10\\cdot\\cdot\\cdot0,11\\cdot\\cdot\\cdot1$ or $12\\cdot\\cdot\\cdot2$ and are of length at most $\\ell$ . $\\ell$ is chosen to be $1+2\\log(1/\\delta)/\\delta$ . ", "page_idx": 41}, {"type": "text", "text": "Although we use the minimal encoder in the statement of Theorem E.2, for the purpose of analysis, define the following encoding algorithm: if the new string is prefixed by $10\\cdot\\cdot\\cdot0$ or $12\\cdot\\cdot\\cdot2$ , select the largest prefix which exists in dictionary and assign it as a token. If the new string starts with a sequence $11\\cdots1$ of length $x$ , consider the first $\\operatorname*{max}\\{\\bar{\\ell},x-1\\}$ length prefix and assign it as a token. Finally, if the string starts with 0 or 2, assign that character as token. Once the first token has been assigned, remove it and repeat. ", "page_idx": 41}, {"type": "text", "text": "E.2 Minimal encoder achieves the optimal cross-entropy loss up to a constant. ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "First consider a simplification of the overall cross-entropy loss, ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{Q\\in\\mathcal{Q}_{1,\\mathrm{grom}}}{\\mathrm{min}}\\underset{m\\to\\infty}{\\mathrm{lim}}\\frac{1}{m}\\mathcal{L}_{m}(Q\\circ\\mathsf{e n c}(\\cdot))}\\\\ &{\\qquad=\\underset{Q\\in\\mathcal{Q}_{1,\\mathrm{grom}}}{\\mathrm{min}}\\underset{m\\to\\infty}{\\mathrm{lim}}-\\frac{1}{m}\\mathbb{E}\\left[\\log Q_{\\#}(|\\mathsf{e n c}_{\\mathrm{min}}(s)|)+\\sum_{t\\in\\mathsf{D i c}}n_{t}\\log Q_{\\mathsf{t o k}}(t)\\right]}\\\\ &{\\qquad\\leq\\underset{m\\to\\infty}{\\mathrm{lim}}\\frac{1}{m}\\mathbb{E}\\left[\\log(m)+|\\mathsf{e n c}_{\\mathrm{min}}(s)|\\log|\\mathsf{D i c t}|\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where in the last inequality we upper bound by choosing $Q_{\\#}=\\mathrm{Unif}([m])$ and $Q_{\\mathrm{tok}}(t)=1/|\\mathsf{D i c t}|$ Note that $|\\mathsf{D i c t}|\\leq2\\ell+1$ and letting $\\begin{array}{r}{\\operatorname*{lim}_{m\\rightarrow\\infty}\\mathrm{{log}}(m)/\\bar{m}=0}\\end{array}$ , ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\displaystyle\\operatorname*{min}_{Q\\in\\mathscr{Q}_{1:\\mathtt{g r a m}}}\\operatorname*{lim}_{m\\to\\infty}\\displaystyle\\frac{1}{m}\\mathscr{L}_{m}(Q\\circ\\mathsf{e n c}(\\cdot))\\leq\\displaystyle\\operatorname*{lim}_{m\\to\\infty}\\frac{1}{m}\\mathbb{E}[|\\mathsf{e n c}_{\\mathsf{m i n}}(s)|\\log(2\\ell+1)]}\\\\ {\\leq\\displaystyle\\operatorname*{lim}_{m\\to\\infty}\\frac{1}{m}\\mathbb{E}\\left[|\\mathsf{e n c}(s)|\\log(2\\ell+1)\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where in $(i)$ , we replace $|\\mathsf{e n c}_{\\mathrm{min}}(s)|$ by $|\\mathsf{e n c}(s)|$ , which is the encoder we define in Appendix E.1. By definition of the minimal encoder, $|\\mathsf{e n c}_{\\mathrm{min}}(s)|\\leq|\\mathsf{e n c}(s)|$ surely. Recall that the encoder enc(\u00b7) processes strings in a sequential (left-to-right) manner. In particular, by a similar argument as Lemma A.4, we can show that under this encoder, the limit $n_{t}/\\sum_{t^{\\prime}}n_{t^{\\prime}}$ almost surely converges to its expectation. More importantly, since, $\\textstyle\\sum_{t\\in{\\mathsf{D i c t}}}|t|n_{t}=m$ , we  have that, ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{m\\to\\infty}\\frac{|\\mathsf{e n c}(s)|}{m}\\overset{\\mathrm{a.s.}}{=}\\frac{1}{\\mathbb{E}_{t\\sim Q_{\\mathrm{MLE}}}[|t|]}.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "converges to some limit almost surely. Therefore, from eq. (44), ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{Q\\in\\mathcal{Q}_{1:\\mathbb{g r a n}}}\\operatorname*{lim}_{m\\to\\infty}\\frac{1}{m}\\mathcal{L}_{m}(Q\\circ\\mathsf{e n c}(\\cdot))\\leq\\mathsf{e s s\\operatorname*{limsup}_{m\\to\\infty}}\\frac{|\\mathsf{e n c}(s)|}{m}\\log(2\\ell+1).\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where the essential lim-sup captures the almost sure limit $1/\\mathbb{E}_{t\\sim Q_{\\mathrm{MLE}}}[|t|]$ . The almost sure convergence of $|\\mathsf{e n c}(s)|/m$ also implies that we can let the limit $m$ go to $\\infty$ in any manner, and the limit will remain the same. In particular, consider a process parameterized by $i^{\\star}$ for generating the source string, such that surely $m\\geq i^{\\star}$ , where the total number of characters, $m$ , is a random variable. As $i^{\\star}\\rightarrow\\infty$ , we will also have $m\\rightarrow\\infty$ surely, and so the limit of $|\\mathsf{e n c}(s)|/m$ under this modified stochastic process should also converge to the same limit. ", "page_idx": 42}, {"type": "text", "text": "Rather than sampling a string of a fixed length $m$ from the source, consider the following sampling model: for $i^{\\star}\\to\\infty$ , sample $i^{\\star}$ geometric random variables $X_{1},\\cdot\\cdot\\cdot,X_{i^{\\star}}\\overset{\\mathrm{i.i.d.}}{\\sim}{\\sf G e o}(\\delta)$ and construct the source string as the concatenation of $i^{\\star}$ strings alternating between successive 1\u2019s and successive $0$ \u2019s or 2\u2019s (with the choice between the two made uniformly at random), with the $i^{t h}$ string of length $X_{i}+1$ . The overall number of characters sampled, $m$ , is surely at least $i^{\\star}$ . ", "page_idx": 42}, {"type": "text", "text": "Under this stochastic process, the size of the encoding of the string is upper bounded by, ", "page_idx": 42}, {"type": "equation", "text": "$$\n|\\mathsf{e n c}(s)|\\leq|X_{1}+1|+\\sum_{i=2}^{i^{\\star}}\\left(1+\\left(X_{i}+1-\\ell\\right)_{+}\\right)\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "This bound follows from the fact that in any substring $s^{\\prime}$ of successive 1\u2019s followed by a substring $s^{\\prime\\prime}$ of successive 0\u2019s or $2\\,\\mathrm{\\Omega}_{\\mathrm{S}}$ , the encoder tokenizes the first $\\operatorname*{nax}\\{\\ell,|s^{\\prime}|-1\\}$ length prefix of $s^{\\prime}$ as a token, and the remaining characters in $s^{\\prime}$ into individual tokens except the last. Then, the last character of $s^{\\prime}$ and the first $\\operatorname*{max}\\bar{\\{\\ell-1,|s^{\\prime\\prime}|\\}}$ characters of $s^{\\prime\\prime}$ are assigned as token. The remainder of $s^{\\prime\\prime}$ is assigned as individual tokens. Each of $s^{\\prime}$ or $s^{\\prime\\prime}$ of length $x$ , is allocated into at most $1+(x+1-\\ell)_{+}$ tokens. For any $i$ , $\\operatorname*{Pr}(X_{i}\\geq u)=(1\\!-\\!\\delta)^{u}$ , and therefore, summing over $u\\geq\\ell$ , we get that $\\mathbb{E}[(X_{i}{+}1{-}\\ell)_{+}]=$ $\\frac{(1\\!-\\!\\delta)^{\\ell-1}}{\\delta}$ . With $\\ell=1+2\\log(1/\\delta)/\\delta$ , this expectation is upper bounded by $\\delta$ . Therefore, ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{i^{\\star}\\rightarrow\\infty}\\frac{\\mathbb{E}[\\left|\\mathsf{e n c}(s)\\right|]}{i^{\\star}}\\leq\\operatorname*{lim}_{i^{\\star}\\rightarrow\\infty}\\frac{1}{i^{\\star}}\\mathbb{E}\\left[|X_{1}|+\\sum_{i=2}^{i^{\\star}}\\left(1+(X_{i}+1-\\ell)_{+}\\right)\\right]\\leq1+\\delta\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "More importantly, by the strong law of large numbers for a sum of independent random variables, $\\begin{array}{r}{\\big(|X_{1}+1|+\\sum_{i=2}^{i^{\\star}}(1+(X_{i}+1-\\ell)_{+})\\big)/i^{\\star}}\\end{array}$ , and therefore $|\\mathsf{e n c}(s)|/i^{\\star}$ is asymptotically almost surely upper bounded as, ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{i^{\\star}\\to\\infty}{\\frac{|\\mathsf{e n c}(s)|}{i^{\\star}}}\\leq1+\\delta,\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "On the other hand, the number of characters generated, $m$ , equals $\\textstyle\\sum_{i=1}^{i^{\\star}}(X_{i}+1)$ , and satisfies, $\\begin{array}{r}{\\operatorname*{lim}_{i^{\\star}\\to\\infty}\\mathbb{E}[m]/i^{\\star}=1+\\delta^{-1}}\\end{array}$ . By another application of the strong law of large numbers for a sum of independent random variables, ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{i^{\\star}\\to\\infty}{\\frac{m}{i^{\\star}}}\\ {\\stackrel{\\mathrm{a.s.}}{=}}\\ 1+\\delta^{-1}.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "By combining eqs. (46) and (47), we have that, ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{i\\star\\rightarrow\\infty}{\\frac{|\\mathsf{e n c}(s)|}{m}}\\overset{\\mathrm{a.s.}}{\\leq}\\frac{1+\\delta}{1+\\delta^{-1}}=\\frac{1}{\\delta}.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Finally, combining with eq. (45) and the ensuing discussion, we may upper bound the limiting cross-entropy loss by, ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{Q\\in Q_{1:\\mathtt{g r a m}}}\\operatorname*{lim}_{m\\to\\infty}\\frac{1}{m}\\mathcal L_{m}(Q\\circ\\mathtt{e n c}(\\cdot))\\leq\\delta\\log(2\\ell+1)=\\delta\\log(3+4\\log(1/\\delta)/\\delta).\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Note for this Markovian source, it is a short calculation to see that, ", "page_idx": 43}, {"type": "equation", "text": "$$\nH_{\\infty}=\\mathbb{E}_{x\\sim\\pi}[H(P(\\cdot|x))]=\\delta\\log(\\sqrt{2}/\\delta)+(1-\\delta)\\log(1/(1-\\delta))\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Note that for any $\\delta\\leq1/2$ , numerical evaluation gives the inequality, ", "page_idx": 43}, {"type": "equation", "text": "$$\n1\\leq\\frac{\\delta\\log(3+4\\log(1/\\delta)/\\delta)}{H_{\\infty}}\\leq1.273\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "with the approximation factor improving as $\\delta$ becomes smaller. Therefore, this tokenizer achieves a normalized cross-entropy loss which asymptotically scales as a constant multiple of the entropy rate of the source. ", "page_idx": 43}, {"type": "text", "text": "E.3 Greedy-encoder achieves poor cross-entropy loss ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Note that the greedy encoder picks the largest prefix of the string which is a token, assigns and removes it, and iterates on the rest of the string. The greedy encoder\u2019s behavior is easy to analyze - every string of consecutive 1\u2019s in the new string is broken into chunks of length $\\ell$ (save potentially the last chunk) and each chunk is assigned as a token in $\\{1s:s\\in S_{1}\\}\\subset\\mathsf{D i d}$ t. If the length of this substring of successive 1\u2019s is not $1,\\ell+1,2\\ell+1,\\cdot\\cdot\\cdot$ , or in general, $\\equiv1$ mod $\\ell$ , every character in the next sequence, composed of 0\u2019s or 2\u2019s is tokenized into individual characters. ", "page_idx": 43}, {"type": "text", "text": "Similar to eq. (42) to eq. (43), consider a simplification of the overall cross-entropy loss, ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{Q\\in Q_{1:g=m}}{\\operatorname*{min}}\\underset{m\\to\\infty}{\\operatorname*{lim}}\\frac{1}{m}\\mathcal L_{m}(Q\\circ\\mathsf{e n c}_{\\mathrm{gre}}(\\cdot))}\\\\ &{=\\underset{Q\\in Q_{1:g=m}}{\\operatorname*{min}}\\underset{m\\to\\infty}{\\operatorname*{lim}}-\\frac{1}{m}\\mathbb E\\left[\\log Q_{\\#}(|\\mathsf{e n c}_{\\mathrm{gre}}(s)|)+|\\mathsf{e n c}_{\\mathrm{gre}}(s)|\\sum_{t\\in\\mathsf{D i c}}\\frac{n_{t}}{|\\mathsf{e n c}_{\\mathrm{gre}}(s)|}\\log Q_{\\mathrm{tak}}(t)\\right]}\\\\ &{\\geq\\underset{Q\\in Q_{1:g=m}}{\\operatorname*{min}}\\underset{m\\to\\infty}{\\operatorname*{lim}}-\\frac{1}{m}\\mathbb E\\left[|\\mathsf{e n c}_{\\mathrm{gre}}(s)|\\sum_{Q_{\\mathrm{suk}}^{t}(t)\\sim0}Q_{\\mathrm{MLE}}(t)\\log Q_{\\mathrm{tak}}(t)\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where the last equation uses the fact that by Lemma A.4, for the greedy en$\\begin{array}{r l r}{\\operatorname*{lim}_{m\\rightarrow\\infty}\\frac{n_{t}}{|\\mathsf{e n c}_{\\mathrm{min}}(s)|}}&{{}\\stackrel{\\mathrm{a.s.}}{=}}&{Q_{\\mathrm{MLE}}(t)}\\end{array}$ $\\begin{array}{r}{\\sum_{t\\in\\mathsf{D i c t}:Q_{\\mathrm{MLE}}(t)>0}Q_{\\mathrm{tok}}(t)\\leq1}\\end{array}$ is $Q_{\\mathrm{tok}}(t)=Q_{\\mathrm{MLE}}(t)$ resulting in the inequality, ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{Q\\in\\mathcal{Q}_{1:\\mathtt{g r o m}}}\\operatorname*{lim}_{m\\to\\infty}\\frac{1}{m}\\mathcal{L}_{m}(Q\\circ\\mathsf{e n c}_{\\mathtt{g r e}}(\\cdot))\\geq\\operatorname*{lim}_{m\\to\\infty}\\frac{1}{m}\\mathbb{E}\\left[|\\mathsf{e n c}_{\\mathtt{g r e}}(s)|H(Q_{\\mathtt{M L E}})\\right],\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where we use the convention $0\\log(1/0)\\triangleq\\operatorname*{lim}_{P\\to0}P\\log(1/P)=0$ and therefore we may sum over tokens such that $Q_{\\mathrm{MLE}}(t)=0$ for free. ", "page_idx": 43}, {"type": "text", "text": "Considering the same geometric sampling model as in Appendix E.2, and Lemma A.4, we may study the almost sure limit $\\begin{array}{r}{\\bar{Q}_{\\mathrm{MLE}}(t)=\\operatorname*{lim}_{m\\rightarrow\\infty}n_{t}/|\\mathsf{e n c}_{\\mathrm{gre}}(\\bar{s})|}\\end{array}$ by computing $\\begin{array}{r}{\\operatorname*{lim}_{i^{\\star}\\to\\infty}n_{t}/|\\mathsf{e n}\\bar{\\mathsf{c}}_{\\mathrm{gre}}(s)|}\\end{array}$ under the geometric sampling model since the almost sure limit exists. Recall that in the geometric sampling model, we generate the overall source string by concatenating $i^{\\star}$ strings of length $X_{1}+$ $1,\\cdot\\cdot\\cdot,X_{i^{\\star}}+1$ where $X_{i}\\;\\sim\\;{\\sf G e o}(\\delta)$ , with the strings alternating between successive 1\u2019s and successive $0\\,\\mathrm{\\dot{s}}$ or 2\u2019s (with the choice between the two made by the flip of a fair coin). For $x\\in$ $\\{0,1,2\\}$ , let ${\\mathcal{E}}_{i}(x)$ denote the event that $X_{i}$ is a string composed only of all $x$ \u2019s. The length of the greedy encoding of $\\pmb{s}$ is lower bounded by, ", "page_idx": 43}, {"type": "equation", "text": "$$\n|\\mathsf{e n c}_{\\mathrm{gre}}(s)|\\geq\\sum_{i=1}^{i^{\\star}}X_{i}\\cdot\\mathbb{I}(X_{i-1}\\not\\equiv1\\mod\\ell)\\mathbb{I}(\\mathcal{E}_{i}(0)\\cup\\mathcal{E}_{i}(2)).\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Which captures for the fact that all 0\u2019s and 2\u2019s are encoded into singular tokens unless the previous string of 1\u2019s was of length $\\equiv1$ mod $\\ell$ . By the law of large numbers of the RHS of eq. (49), the following a.a.s. lower bound is satisfied, ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{i^{\\star}\\to\\infty}\\frac{|\\mathsf{e n c}_{\\mathsf{g r e}}(s)|\\,\\log}{i^{\\star}}\\ge\\frac{1}{2\\delta}\\left(1-\\sum_{u=0}^{\\infty}\\delta(1-\\delta)^{\\ell u+1}\\right)=\\frac{1}{2\\delta}\\left(1-\\frac{\\delta(1-\\delta)}{1-(1-\\delta)^{\\ell}}\\right)\\ge\\frac{1-\\delta}{2\\delta},\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "where the last inequality uses the fact that $\\ell=1\\!+\\!2\\log(1/\\delta)/\\delta$ . Likewise, observe that, $|\\mathsf{e n c}_{\\mathrm{gre}}(s)|\\leq$ $m$ surely, and following the analysis in Appendix E.2 of eq. (47), we have that, ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{i^{\\star}\\to\\infty}\\frac{|{\\sf e n c}_{\\mathrm{gre}}(s)|}{i^{\\star}}\\leq\\operatorname*{lim}_{i^{\\star}\\to\\infty}\\frac{m}{i^{\\star}}\\overset{\\mathrm{a.s.}}{=}1+\\delta^{-1}.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "For $x\\in\\{0,2\\}$ , observe that the expected number of times the token $x$ is observed in the encoding of s, $n_{x}$ can be written as, ", "page_idx": 44}, {"type": "equation", "text": "$$\nn_{x}\\geq\\sum_{i=1}^{i^{\\star}}\\left((X_{i}+1)\\cdot\\mathbb{I}(X_{i-1}\\not\\in1\\mod\\ell)\\right)\\mathbb{I}(\\mathcal{E}_{i}(x)).\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "In particular, taking the expectation of eq. (52), ", "page_idx": 44}, {"type": "equation", "text": "$$\n^{-}[n_{x}|\\mathcal{E}_{1}(0)\\cup\\mathcal{E}_{1}(2)],\\ \\mathbb{E}[n_{x}|\\mathcal{E}_{1}(1)]\\geq\\frac{i^{\\star}-1}{4}(1+\\delta^{-1})\\left(1-\\sum_{u=0}^{\\infty}\\delta(1-\\delta)^{\\ell u+1}\\right)\\geq\\frac{i^{\\star}-1}{4}\\cdot\\frac{1-\\delta^{2}}{\\delta}.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Note that in any realization of the geometric sampling process, in eq. (52), either the odd indexed substrings are all-1\u2019s or the even indexed substrings are all-1\u2019s. Therefore, surely, all the non-zero terms in the above summation are of the same parity. Moreover, since the $i^{t h}$ term in the sum only depends on $X_{i}$ and $X_{i-1}$ , conditioned on whether the non-zero parities are even or odd, $n_{x}$ can be written as a sum of $\\approx i^{\\star}/2$ mutually independent terms. By the strong law of large numbers on each of the conditional processes, eqs. (52) and (53) implies that for $x\\in\\{0,2\\}$ , ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{i^{\\star}\\rightarrow\\infty}\\frac{n_{x}}{i^{\\star}}\\overset{\\mathrm{a.s.}}{\\geq}\\frac{1-\\delta^{2}}{4\\delta}.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "To upper bound $n_{x}$ , note that it is upper bounded by the number of times the character $x$ appears in the source string, which by the strong law of large numbers a.a.s (after normalizing by $i^{\\star}$ ), scales as $1/4\\delta$ . Finally, to bound $Q_{\\mathrm{MLE}}(t)$ which is the sequential nature of the encoder, using a similar proof as Lemma A.4, we can show that $n_{t}/\\sum_{t^{\\prime}}n_{t^{\\prime}}$ converges to the unigram MLE model for this tokenizer. For the token $x\\in\\{0,2\\}$ , ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{i^{\\star}\\to\\infty}\\frac{n_{x}}{|\\mathsf{e n c}(s)|}=Q_{\\mathrm{MLE}}(x)\\leq\\mathbb{E}\\left[\\operatorname*{lim}_{i^{\\star}\\to\\infty}\\frac{n_{x}}{n_{2}+n_{0}}\\right]\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Using the a.a.s. upper and lower bounds on $|\\mathsf{e n c}(s)|$ , $n_{0}$ and $n_{2}$ derived in eqs. (51) and (54), we arrive at lower and upper bounds on $Q_{\\mathrm{MLE}}(x)$ for $x\\in\\{0,2\\}$ , ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\frac{1}{4}\\approx\\frac{1-\\delta}{4}=\\frac{(1-\\delta^{2})}{4\\delta(1+\\delta^{-1})}\\le Q_{\\mathrm{MLE}}(x)\\le\\frac{1}{2(1-\\delta^{2})}\\approx\\frac{1}{2}.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Since there are at least two tokens having probability bounded away from 0 and 1 by a constant under the MLE unigram model, the entropy of $Q_{\\mathrm{MLE}}$ must also be lower bounded by a constant. Indeed, ", "page_idx": 44}, {"type": "equation", "text": "$$\nH(Q_{\\mathrm{MLE}})\\geq2\\operatorname*{min}_{\\frac{1-\\delta}{4}\\leq y\\leq\\frac{1}{2(1-\\delta^{2})}}y\\log(1/y).\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "It is easy to verify that for $\\delta\\leq0.5$ , the minimizer is achieved at $\\begin{array}{r}{y=\\frac{1-\\delta}{4}}\\end{array}$ , which leads to the lower bound, ", "page_idx": 44}, {"type": "equation", "text": "$$\nH(Q_{\\mathrm{MLE}})\\geq\\left({\\frac{1-\\delta}{2}}\\right)\\log\\left({\\frac{4}{1-\\delta}}\\right)\n$$", "text_format": "latex", "page_idx": 44}, {"type": "table", "img_path": "wm9JZq7RCe/tmp/a6d25fb150ab91f117c5330a8bdd6bd543d18f756c06680dad20d4fcc54c1c79.jpg", "table_caption": ["Table 3: Hyperparameter choices "], "table_footnote": [], "page_idx": 45}, {"type": "text", "text": "Finally, combining this lower bound on $H(Q_{\\mathrm{MLE}})$ with eq. (48), we have that, ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{Q\\in Q_{1:g r u m}}{\\operatorname*{min}}\\underset{m\\rightarrow\\infty}{\\operatorname*{lim}}\\frac{1}{m}\\mathcal{L}_{m}(Q\\circ\\mathsf{e n c}(\\cdot))=\\underset{i\\^{*}\\rightarrow\\infty}{\\operatorname*{lim}}\\mathbb{E}\\left[\\frac{\\left|\\mathsf{e n c}_{\\mathsf{g r e}}(s)\\right|}{m}H(Q_{\\mathsf{M L E}})\\right]}&{}\\\\ &{\\geq\\underset{i^{*}\\rightarrow\\infty}{\\operatorname*{lim}}\\mathbb{E}\\left[\\frac{\\left|\\mathsf{e n c}_{\\mathsf{g r e}}(s)\\right|}{m}\\right]\\cdot\\left(\\frac{1-\\delta}{2}\\right)\\log\\left(\\frac{4}{1-\\delta}\\right)}&{}\\\\ &{\\overset{(i)}{\\geq}\\frac{1-\\delta}{2\\delta\\left(1+\\delta^{-1}\\right)}\\cdot\\left(\\frac{1-\\delta}{2}\\right)\\log\\left(\\frac{4}{1-\\delta}\\right)}&{}\\\\ &{\\geq\\frac{\\left(1-\\delta\\right)^{2}}{3\\left(1+\\delta\\right)}H(\\pi)}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "where $(i)$ follows from the lower bound on $|\\mathsf{e n c}_{\\mathrm{gre}}(s)|$ in eq. (50) with the almost sure limit of $m$ in eq. (47) and noting that $|\\mathsf{e n c}_{\\mathrm{gre}}(s)|/m\\leq1$ surely. The last inequality follows by simplifying using $\\pi=(1/4,1/2,1/4)$ and $\\begin{array}{r}{H(\\pi)=\\frac{1}{2}\\log(8)}\\end{array}$ . ", "page_idx": 45}, {"type": "text", "text": "F Experiment details ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Experiment 1 (Figures 4a and 4b). In this and previous experiments (Figures 2, 3a and 3b), we train the transformers on a single GPU on an $8\\times\\mathrm{Al00}$ node. The wall-clock time measured does not count time spent in validation loss evaluations. The hyperparameter choices are listed in Table 3. ", "page_idx": 45}, {"type": "text", "text": "Experiment 2 (Table 1). We evaluate pre-trained tokenizers on various datasets. In this experiment, we do not evaluate the likelihood model on test sequences, rather, we estimate the cross-entropy of the best unigram model by using the approximation, ", "page_idx": 45}, {"type": "equation", "text": "$$\n-\\,\\mathbb{E}\\left[\\sum_{t\\in{\\mathrm{Dict}}}n_{t}\\log Q_{{\\mathrm{MLE}}}(t)\\right]\\approx-\\sum_{t\\in{\\mathrm{Dict}}}{\\widehat{n}}_{t}\\log({\\widehat{Q}}(t))\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "whereQ (t) = n tnt is the MLE unigram model learnt from a finite dataset, which we choose here as GLUE (Wang et a l., 2019), and $\\widehat{n}_{t}$ is the number of times the token $\\pmb{t}$ is observed in the encoding of the dataset. This approximation  a llows us to separate the error stemming from learning a suboptimal likelihood model which tends to have higher sample complexity requirements and focus on the asymptotic error of the tokenizer. ", "page_idx": 45}, {"type": "text", "text": "We use Monte-carlo sampling to approximate the cross-entropy loss estimator in eq. (55). These approximations tends to underestimate the true cross-entropy loss due to the concavity of $x\\log(1/x)$ close to 0. In general, the gap between the approximation and the true error is expected to grow with $k$ . Therefore, the true difference between the estimate of the best unigram model on a tokenizer and the best $k$ -gram model for $k\\geq2$ on the character level tokenizer is likely to be larger than the reported figures. ", "page_idx": 46}, {"type": "text", "text": "Experiment 3 (Figure 5). We train the LZW, BPE, Unigram and Wordpiece tokenizers with dictionary sizes $\\{5000,6000,8000,12000,20000,32000,50000,80000\\}$ . The cross-entropy loss incurred by the best 1-gram model is estimated using eq. (55) while for $k$ -gram models for $k\\geq2$ , we use Monte-carlo sampling to estimate the cross-entropy of the empirical $k$ -gram model computed using the GLUE dataset. For the $k$ -gram models trained on the character level tokenizer, since the vocabulary size is fixed, we instead plot the number of distinct $k$ -grams on the $x$ -axis. While this is not a true measure of the number of parameters in the underlying $k$ -gram model, we use this as a proxy for the same. ", "page_idx": 46}, {"type": "text", "text": "G NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and precede the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 47}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 47}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] . ", "page_idx": 47}, {"type": "text", "text": "\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available. \u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 47}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Justification: The paper lists an empirical phenomenon (justified in Fig. 2) and theoretical contributions justified in Theorems 3.1, 3.3 and 3.5 ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 47}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Justification: Remark 3.3 ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 47}, {"type": "text", "text": "", "page_idx": 48}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 48}, {"type": "text", "text": "Justification: Assumption 3.2 ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 48}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 48}, {"type": "text", "text": "Justification: The code has been released along with the rest of the submission. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.   \n(b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 48}, {"type": "text", "text": "", "page_idx": 49}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 49}, {"type": "text", "text": "Justification: Instructions provided in the jupyter notebook. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 49}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 49}, {"type": "text", "text": "Justification: Table 3 ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 49}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 50}, {"type": "text", "text": "Justification: All plots which allow for it, contain standard error bars. Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 50}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 50}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 50}, {"type": "text", "text": "Justification: Appendix F contains this information. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 50}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 50}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 50}, {"type": "text", "text": "Justification: No NeurIPS code of ethics were violated. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 50}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 51}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 51}, {"type": "text", "text": "Justification: This work is a primarily theoretical study on the behavior of tokenization on toy problems (learning Markov chains). The societal impact of this research is not likely to be significant. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 51}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 51}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Justification: No models with a high risk for misuse were trained or released. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 51}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 51}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 51}, {"type": "text", "text": "Justification: Code has been properly credited, via citing the relevant paper. Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 52}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 52}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 52}, {"type": "text", "text": "Justification: No new assets released. ", "page_idx": 52}, {"type": "text", "text": "Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 52}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 52}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 52}, {"type": "text", "text": "Justification: No crowdsourcing or research with human subjects. ", "page_idx": 52}, {"type": "text", "text": "Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 52}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 52}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 52}, {"type": "text", "text": "Answer: [NA] Justification: No crowdsourcing or research with human subjects. ", "page_idx": 52}, {"type": "text", "text": "Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 53}]