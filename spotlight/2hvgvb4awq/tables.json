[{"figure_path": "2HvgvB4aWq/tables/tables_6_1.jpg", "caption": "Table 1: Task graph generation results on CaptainCook4D. Best results are in bold, second best results are underlined, best results among competitors are highlighted. Confidence interval bounds computed at 90% conf. for 5 runs.", "description": "This table presents the performance of different methods for task graph generation on the CaptainCook4D dataset.  The metrics used are Precision, Recall, and F1-score, which are standard evaluation metrics for evaluating the quality of predicted task graphs. The table compares the proposed methods (TGT-text and DO) against existing approaches like MSGI, LLM, Count-Based, and MSG2. The best results for each metric are highlighted in bold, while the second-best results are underlined.  Confidence intervals (at 90% confidence, based on 5 runs) are also provided to show the stability and reliability of the results.  The \"Improvement\" row shows the percentage improvement of the proposed methods compared to the best-performing existing method (MSG2).", "section": "4.1 Graph Generation"}, {"figure_path": "2HvgvB4aWq/tables/tables_6_2.jpg", "caption": "Table 2: We compare the abilities of our TGT model trained on visual features to generalize to two fundamental video understanding tasks, i.e., pairwise ordering and future prediction. Despite not being explicitly trained for these tasks, our model exhibits video understanding abilities, surpassing the baseline.", "description": "This table compares the performance of the TGT model (trained on visual features) on two video understanding tasks: pairwise ordering and future prediction.  It shows the model's accuracy on these tasks, along with the improvement over a random baseline. The results demonstrate that the model, despite not being explicitly trained for these tasks, exhibits video understanding abilities, surpassing the baseline.", "section": "4.1 Graph Generation"}, {"figure_path": "2HvgvB4aWq/tables/tables_8_1.jpg", "caption": "Table 3: Online mistake detection results. Results obtained with ground truth action sequences are denoted with *, while results obtained on predicted action sequences are denoted with +.", "description": "This table presents the performance of different methods on the online mistake detection task using two datasets (Assembly101-O and EPIC-Tent-O).  The results are broken down into average F1 scores, and further subdivided by correct and mistake predictions, reporting precision and recall for each.  Results are shown for both scenarios where ground truth action sequences are used and where predicted action sequences are used. The \"Improvement\" row shows the improvement in average F1 score over the PREGO baseline method. The table highlights the effectiveness of the proposed method, DO, in comparison to the other approaches in terms of accuracy and robustness in the online mistake detection task.", "section": "4.2 Online Mistake Detection"}, {"figure_path": "2HvgvB4aWq/tables/tables_15_1.jpg", "caption": "Table 1: Task graph generation results on CaptainCook4D. Best results are in bold, second best results are underlined, best results among competitors are highlighted. Confidence interval bounds computed at 90% conf. for 5 runs.", "description": "This table presents the performance of different methods for task graph generation on the CaptainCook4D dataset.  The metrics used are precision, recall, and F1-score, which are common metrics for evaluating the accuracy of classification tasks. The table highlights the best performing method among all the compared approaches and indicates the confidence intervals for the results, which helps to assess the reliability of the performance measurements.", "section": "4.1 Graph Generation"}, {"figure_path": "2HvgvB4aWq/tables/tables_15_2.jpg", "caption": "Table 5: List of hyper-parameters used in the models training process for task graph generation using Assembly101-O and EPIC-Tent-O.", "description": "This table lists the hyperparameters used for training the Direct Optimization (DO) and Task Graph Transformer (TGT) models on the Assembly101-O and EPIC-Tent-O datasets.  It shows the learning rate, maximum number of training epochs, optimizer used (Adam), beta (\u03b2) parameter for the TGML loss, and dropout rate (only applicable to the TGT model). Note that the beta parameter is linearly annealed from 1.0 to 0.55 during training for the TGT model.", "section": "3.2 Models"}, {"figure_path": "2HvgvB4aWq/tables/tables_16_1.jpg", "caption": "Table 1: Task graph generation results on CaptainCook4D. Best results are in bold, second best results are underlined, best results among competitors are highlighted. Confidence interval bounds computed at 90% conf. for 5 runs.", "description": "This table presents the performance comparison of different methods for task graph generation on the CaptainCook4D dataset.  The metrics used for evaluation are precision, recall, and F1-score.  The table highlights the best performing method, indicating the improvements achieved over existing approaches. Confidence intervals are also provided to demonstrate the reliability and statistical significance of the results. ", "section": "4.1 Graph Generation"}, {"figure_path": "2HvgvB4aWq/tables/tables_17_1.jpg", "caption": "Table 7: Performance comparison between the single TGT-text model trained across all Captain-Cook4D procedures and the unified model. The confidence intervals in the single models indicate that the unified method performs comparably to training individual models for each procedure.", "description": "This table compares the performance of two different approaches for training a Task Graph Transformer (TGT) model for task graph generation on the CaptainCook4D dataset. The first approach trained a separate TGT model for each of the 24 procedures in the dataset. The second approach trained a single unified TGT model across all 24 procedures. The table shows that while the unified model shows slightly lower precision, recall, and F1 scores than the average of the individual models, the confidence intervals indicate that the performance difference is not statistically significant.", "section": "4.1 Graph Generation"}, {"figure_path": "2HvgvB4aWq/tables/tables_17_2.jpg", "caption": "Table 8: We followed a \u201cleave-one-out\u201d scheme in which we trained the TGT on all procedures except one and then fine-tuned the model on sequences for the held-out procedure (hence a 5-shot regime). The table shows that our approach greatly improves over competitors which are unable to leverage transfer learning.", "description": "This table compares the performance of the proposed TGT model with other state-of-the-art methods on the task of task graph generation using a leave-one-out cross-validation approach. It demonstrates the effectiveness of the transfer learning capability of the TGT model by showing significant improvement in performance compared to methods that don't utilize transfer learning.", "section": "4.1 Graph Generation"}]