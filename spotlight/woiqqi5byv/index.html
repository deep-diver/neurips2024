<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><title>Interpret Your Decision: Logical Reasoning Regularization for Generalization in Visual Classification &#183; NeurIPS 2024</title>
<meta name=title content="Interpret Your Decision: Logical Reasoning Regularization for Generalization in Visual Classification &#183; NeurIPS 2024"><meta name=description content="This paper introduces L-Reg, a novel logical regularization technique, to improve generalization in visual classification. L-Reg effectively reduces model complexity and improves interpretability by f..."><meta name=keywords content="Computer Vision,Image Classification,üè¢ Xi'an-Jiaotong Liverpool University,"><link rel=canonical href=https://deep-diver.github.io/neurips2024/spotlight/woiqqi5byv/><link type=text/css rel=stylesheet href=/neurips2024/css/main.bundle.min.595affd4445a931ea6d6e3a5a3c709930fa52a60be10b21c6f81fdb8fecaacea33aacedf80cdc88be45f189be14ed4ce53ea74a1e1406fad9cbf90c5ed409173.css integrity="sha512-WVr/1ERakx6m1uOlo8cJkw+lKmC+ELIcb4H9uP7KrOozqs7fgM3Ii+RfGJvhTtTOU+p0oeFAb62cv5DF7UCRcw=="><script type=text/javascript src=/neurips2024/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj+e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script><script defer type=text/javascript id=script-bundle src=/neurips2024/js/main.bundle.min.efbf3b6b987689fffaf2d7b73173d2690c0279a04d444b0537a77d7f4ff6e6d493445400cb0cf56bc0f0f123e19f15394e63cae34e67f069bd013dd5c73df56e.js integrity="sha512-7787a5h2if/68te3MXPSaQwCeaBNREsFN6d9f0/25tSTRFQAywz1a8Dw8SPhnxU5TmPK405n8Gm9AT3Vxz31bg==" data-copy data-copied></script><script src=/neurips2024/lib/zoom/zoom.min.37d2094687372da3f7343a221a470f6b8806f7891aa46a5a03966af7f0ebd38b9fe536cb154e6ad28f006d184b294525a7c4054b6bbb4be62d8b453b42db99bd.js integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ=="></script><link rel=apple-touch-icon sizes=180x180 href=/neurips2024/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/neurips2024/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/neurips2024/favicon-16x16.png><link rel=manifest href=/neurips2024/site.webmanifest><meta property="og:url" content="https://deep-diver.github.io/neurips2024/spotlight/woiqqi5byv/"><meta property="og:site_name" content="NeurIPS 2024"><meta property="og:title" content="Interpret Your Decision: Logical Reasoning Regularization for Generalization in Visual Classification"><meta property="og:description" content="This paper introduces L-Reg, a novel logical regularization technique, to improve generalization in visual classification. L-Reg effectively reduces model complexity and improves interpretability by f‚Ä¶"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="spotlight"><meta property="article:published_time" content="2024-09-26T00:00:00+00:00"><meta property="article:modified_time" content="2024-09-26T00:00:00+00:00"><meta property="article:tag" content="Computer Vision"><meta property="article:tag" content="Image Classification"><meta property="article:tag" content="üè¢ Xi'an-Jiaotong Liverpool University"><meta property="og:image" content="https://deep-diver.github.io/neurips2024/spotlight/woiqqi5byv/cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://deep-diver.github.io/neurips2024/spotlight/woiqqi5byv/cover.png"><meta name=twitter:title content="Interpret Your Decision: Logical Reasoning Regularization for Generalization in Visual Classification"><meta name=twitter:description content="This paper introduces L-Reg, a novel logical regularization technique, to improve generalization in visual classification. L-Reg effectively reduces model complexity and improves interpretability by f‚Ä¶"><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"Spotlights","name":"Interpret Your Decision: Logical Reasoning Regularization for Generalization in Visual Classification","headline":"Interpret Your Decision: Logical Reasoning Regularization for Generalization in Visual Classification","abstract":"This paper introduces L-Reg, a novel logical regularization technique, to improve generalization in visual classification. L-Reg effectively reduces model complexity and improves interpretability by f\u0026hellip;","inLanguage":"en","url":"https:\/\/deep-diver.github.io\/neurips2024\/spotlight\/woiqqi5byv\/","author":{"@type":"Person","name":"AI Paper Reviewer"},"copyrightYear":"2024","dateCreated":"2024-09-26T00:00:00\u002b00:00","datePublished":"2024-09-26T00:00:00\u002b00:00","dateModified":"2024-09-26T00:00:00\u002b00:00","keywords":["Computer Vision","Image Classification","üè¢ Xi'an-Jiaotong Liverpool University"],"mainEntityOfPage":"true","wordCount":"3712"}]</script><meta name=author content="AI Paper Reviewer"><link href=https://neurips.cc/ rel=me><link href=https://x.com/NeurIPSConf rel=me><link href rel=me><link href=https://github.com/deep-diver/paper-reviewer/ rel=me><link href=https://x.com/algo_diver/ rel=me><script src=/neurips2024/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj+KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script><script defer src=/neurips2024/lib/typeit/typeit.umd.1b3200cb448f5cd1f548f2781452643d3511a43584b377b82c03a58055da4fdb7bc8f6c6c2ce846480c7677ff25bfd0d75f15823c09443ab18e0fd2cad792587.js integrity="sha512-GzIAy0SPXNH1SPJ4FFJkPTURpDWEs3e4LAOlgFXaT9t7yPbGws6EZIDHZ3/yW/0NdfFYI8CUQ6sY4P0srXklhw=="></script><script defer src=/neurips2024/lib/packery/packery.pkgd.min.js integrity></script><script type=text/javascript src=/neurips2024/js/shortcodes/gallery.min.9b4cb28f931ed922c26fb9b2510c2debb370f6a63305050c2af81740b2919883715e24efbbdf3a081496718ec751df3a72729d4d0bc71d6071297563a97ce1ee.js integrity="sha512-m0yyj5Me2SLCb7myUQwt67Nw9qYzBQUMKvgXQLKRmINxXiTvu986CBSWcY7HUd86cnKdTQvHHWBxKXVjqXzh7g=="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-KX0S6Q55Y7"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-KX0S6Q55Y7")</script><meta name=theme-color></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32 scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a></div><div class=min-h-[148px]></div><div class="fixed inset-x-0 pl-[24px] pr-[24px]" style=z-index:100><div id=menu-blur class="absolute opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl shadow-2xl"></div><div class="relative max-w-[64rem] ml-auto mr-auto"><div style=padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start space-x-3"><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/neurips2024/ class="text-base font-medium text-gray-500 hover:text-gray-900">NeurIPS 2024</a></nav><nav class="hidden md:flex items-center space-x-5 md:ml-12 h-12"><a href=/neurips2024/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>About</p></a><a href=/neurips2024/oral/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Oral</p></a><a href=/neurips2024/spotlight/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Spotlight</p></a><a href=/neurips2024/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Tags</p></a><button id=search-button aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button><div class="ltr:mr-14 rtl:ml-14 flex items-center"><button id=appearance-switcher aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></nav><div class="flex md:hidden items-center space-x-5 md:ml-12 h-12"><span></span>
<button id=search-button-mobile aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></button>
<button id=appearance-switcher-mobile aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400" style=margin-right:5px><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div><div class="-my-2 -mr-2 md:hidden"><label id=menu-button class=block><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper style=padding-top:5px class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li id=menu-close-button><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/neurips2024/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>About</p></a></li><li class=mt-1><a href=/neurips2024/oral/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Oral</p></a></li><li class=mt-1><a href=/neurips2024/spotlight/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Spotlight</p></a></li><li class=mt-1><a href=/neurips2024/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Tags</p></a></li></ul></div></label></div></div></div></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("menu-blur");n.style.opacity=t/300})</script><div class="relative flex flex-col grow"><main id=main-content class=grow><article><div id=hero class="h-[150px] md:h-[200px]"></div><div class="fixed inset-x-0 top-0 h-[800px] single_hero_background nozoom" style=background-image:url(/neurips2024/spotlight/woiqqi5byv/cover_hu11137013539814643551.png)><div class="absolute inset-0 bg-gradient-to-t from-neutral dark:from-neutral-800 to-transparent mix-blend-normal"></div><div class="absolute inset-0 opacity-60 bg-gradient-to-t from-neutral dark:from-neutral-800 to-neutral-100 dark:to-neutral-800 mix-blend-normal"></div></div><div id=background-blur class="fixed opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl"></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("background-blur");n.style.opacity=t/300})</script><header id=single_header class="mt-5 max-w-prose"><ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden"><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/neurips2024/>NeurIPS 2024</a><span class="px-1 text-primary-500">/</span></li><li class=inline><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/neurips2024/spotlight/>Spotlights</a><span class="px-1 text-primary-500">/</span></li><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/neurips2024/spotlight/woiqqi5byv/>Interpret Your Decision: Logical Reasoning Regularization for Generalization in Visual Classification</a><span class="px-1 text-primary-500">/</span></li></ol><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">Interpret Your Decision: Logical Reasoning Regularization for Generalization in Visual Classification</h1><div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime=2024-09-26T00:00:00+00:00>26 September 2024</time><span class="px-2 text-primary-500">&#183;</span><span>3712 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">18 mins</span></div><div class="flex flex-row flex-wrap items-center"><span style=margin-top:.5rem class=mr-2 onclick='window.open("/neurips2024/tags/computer-vision/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Computer Vision
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/neurips2024/tags/image-classification/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Image Classification
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/neurips2024/tags/-xian-jiaotong-liverpool-university/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">üè¢ Xi'an-Jiaotong Liverpool University</span></span></span></div></div><div class="flex author"><img class="!mt-0 !mb-0 h-24 w-24 rounded-full ltr:mr-4 rtl:ml-4" width=96 height=96 alt="AI Paper Reviewer" src=/neurips2024/img/avatar_hu3675860218824636004.png><div class=place-self-center><div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">Author</div><div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">AI Paper Reviewer</div><div class="text-sm text-neutral-700 dark:text-neutral-400">As an AI, I specialize in crafting insightful blog content about cutting-edge research in the field of artificial intelligence</div><div class="text-2xl sm:text-lg"><div class="flex flex-wrap text-neutral-400 dark:text-neutral-500"><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://neurips.cc/ target=_blank aria-label=Homepage rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg fill="currentcolor" height="800" width="800" id="Capa_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 491.398 491.398"><g><g id="Icons_19_"><path d="M481.765 220.422 276.474 15.123c-16.967-16.918-44.557-16.942-61.559.023L9.626 220.422c-12.835 12.833-12.835 33.65.0 46.483 12.843 12.842 33.646 12.842 46.487.0l27.828-27.832v214.872c0 19.343 15.682 35.024 35.027 35.024h74.826v-97.62c0-7.584 6.146-13.741 13.743-13.741h76.352c7.59.0 13.739 6.157 13.739 13.741v97.621h74.813c19.346.0 35.027-15.681 35.027-35.024V239.091l27.812 27.815c6.425 6.421 14.833 9.63 23.243 9.63 8.408.0 16.819-3.209 23.242-9.63 12.844-12.834 12.844-33.65.0-46.484z"/></g></g></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://x.com/NeurIPSConf target=_blank aria-label=X-Twitter rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href target=_blank aria-label=Line rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg id="Capa_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 14.707 14.707"><g><rect x="6.275" y="0" style="fill:currentColor" width="2.158" height="14.707"/></g></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://github.com/deep-diver/paper-reviewer/ target=_blank aria-label=Github rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://x.com/algo_diver/ target=_blank aria-label=X-Twitter rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg></span></span></a></div></div></div></div><div class=mb-5></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="order-first lg:ml-auto px-0 lg:order-last ltr:lg:pl-8 rtl:lg:pr-8"><div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-[140px]"><details open id=TOCView class="toc-right mt-0 overflow-y-scroll overscroll-contain scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600 rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 hidden lg:block"><summary class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="min-w-[220px] py-2 border-dotted ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#logical-reg-benefits>Logical Reg. Benefits</a></li><li><a href=#generalization-across-tasks>Generalization Across Tasks</a></li><li><a href=#l-reg-interpretability>L-Reg Interpretability</a></li><li><a href=#future-research>Future Research</a></li><li><a href=#l-reg-limitations>L-Reg Limitations</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><details class="toc-inside mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 lg:hidden"><summary class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#logical-reg-benefits>Logical Reg. Benefits</a></li><li><a href=#generalization-across-tasks>Generalization Across Tasks</a></li><li><a href=#l-reg-interpretability>L-Reg Interpretability</a></li><li><a href=#future-research>Future Research</a></li><li><a href=#l-reg-limitations>L-Reg Limitations</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><script>var margin=200,marginError=50;(function(){var t=$(window),e=$("#TOCView"),s=e.height();function n(){var n=t.height()-margin;s>=n?(e.css("overflow-y","scroll"),e.css("max-height",n+marginError+"px")):(e.css("overflow-y","hidden"),e.css("max-height","9999999px"))}t.on("resize",n),$(document).ready(n)})()</script></div></div><div class="min-w-0 min-h-0 max-w-fit"><div class="article-content max-w-prose mb-20"><br><div class="flex flex-row flex-wrap items-center space-x-2"><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 48 48" fill="none"><rect width="48" height="48" fill="#fff" fill-opacity=".01"/><path d="M18 43V22c0-3.3137 2.6863-6 6-6s6 2.6863 6 6V43" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M12 40V22c0-6.6274 5.3726-12 12-12s12 5.3726 12 12V40" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M6 35V22C6 12.0589 14.0589 4 24 4s18 8.0589 18 18V35" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 44V31" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 24.625v-2.75" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/></svg>
</span></span><span>Woiqqi5bYV</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg fill="#000" height="800" width="800" id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 511.999 511.999"><g><g><path d="M421.578 190.264l-99.847-99.847c-2.439-2.439-6.391-2.439-8.829.0L82.824 320.495c-2.439 2.439-2.439 6.392.0 8.829l99.847 99.847c2.439 2.439 6.391 2.439 8.829.0l230.078-230.078C424.017 196.655 424.017 192.703 421.578 190.264z"/></g></g><g><g><path d="M506.511 87.672 424.323 5.484c-7.308-7.31-19.175-7.315-26.488.0L348.219 55.1c-2.439 2.439-2.439 6.391.0 8.829l99.847 99.847c2.439 2.437 6.391 2.437 8.829.0l49.616-49.616C513.826 106.847 513.826 94.987 506.511 87.672z"/></g></g><g><g><path d="M508.133 491.11c-1.054-9.556-9.489-16.599-19.104-16.599H111.633l36.058-15.163c4.088-1.719 5.131-7.034 1.994-10.17l-86.854-86.854c-3.137-3.135-8.451-2.094-10.17 1.994C52.224 365.359 2.052 484.66 1.627 485.707c-5.815 13.208 4.855 27.01 18.107 26.263H489.52C500.566 511.97 509.379 502.408 508.133 491.11z"/></g></g></svg>
</span></span><span>Zhaorui Tan et el.</span></span></span></div></div><p><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href="https://openreview.net/forum?id=Woiqqi5bYV" target=_self role=button>‚Üó OpenReview
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://neurips.cc/virtual/2024/poster/94812 target=_self role=button>‚Üó NeurIPS Homepage</a></p><audio controls><source src=https://ai-paper-reviewer.com/Woiqqi5bYV/podcast.wav type=audio/wav>Your browser does not support the audio element.</audio><h3 class="relative group">TL;DR<div id=tldr class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#tldr aria-label=Anchor>#</a></span></h3><div class="lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl"><p>Visual classification models struggle to generalize to unseen data and discover novel categories. This paper explores the relationship between logical reasoning and deep learning generalization. The core problem addressed is the difficulty of generalizing deep learning models to unseen data and categories, a significant limitation hindering their broader applicability. Existing regularization techniques like L2 regularization offer limited interpretability and often don&rsquo;t sufficiently address this issue.</p><p>To tackle this, the authors propose L-Reg, a novel logical regularization term. L-Reg bridges logical analysis to image classification, reducing model complexity by balancing feature distribution and filtering redundant information. Experiments show that L-Reg consistently improves generalization across multi-domain scenarios and generalized category discovery. This demonstrates the effectiveness of logical reasoning for enhancing generalization and interpretability in visual classification, highlighting L-Reg&rsquo;s practical utility and potential for future improvements.</p></div><h4 class="relative group">Key Takeaways<div id=key-takeaways class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#key-takeaways aria-label=Anchor>#</a></span></h4><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-aeb9f80f71bf9a9374442c677729e997></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-aeb9f80f71bf9a9374442c677729e997",{strings:[" L-Reg, a novel logical regularization method, improves generalization in visual classification. "],speed:10,lifeLike:!0,startDelay:0,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-070679a40184966708ee7b4bce0154ae></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-070679a40184966708ee7b4bce0154ae",{strings:[" L-Reg reduces model complexity and enhances interpretability by focusing on salient features. "],speed:10,lifeLike:!0,startDelay:1e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-8c4b118395d05cfbecb8779dc84e5256></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-8c4b118395d05cfbecb8779dc84e5256",{strings:[" L-Reg consistently improves generalization across various scenarios, particularly in complex real-world settings with unseen domains and unknown classes. "],speed:10,lifeLike:!0,startDelay:2e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><h4 class="relative group">Why does it matter?<div id=why-does-it-matter class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#why-does-it-matter aria-label=Anchor>#</a></span></h4><p>This paper is <strong>highly important</strong> for researchers working on visual classification and generalization. It introduces a novel approach that <strong>bridges logical reasoning and deep learning</strong>, leading to improved model interpretability and generalization capabilities. This <strong>addresses a critical challenge</strong> in the field, paving the way for more robust and reliable visual AI systems. The proposed L-Reg method is <strong>easy to implement</strong> and shows strong results, making it directly applicable to many existing frameworks. Its use in real-world scenarios with complex, unlabeled data is particularly relevant, creating <strong>new avenues for practical application</strong> and further research.</p><hr><h4 class="relative group">Visual Insights<div id=visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#visual-insights aria-label=Anchor>#</a></span></h4><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Woiqqi5bYV/figures_0_1.jpg alt></figure></p><blockquote><p>The figure displays GradCAM visualizations, highlighting activated regions in a model trained for visual classification. It compares the results of a model trained with standard L2 regularization versus one with the proposed L-Reg (logical regularization). The visualizations are shown for both seen (training) and unseen (testing) domains. Focus is on the classification of the &lsquo;person&rsquo; class, comparing how salient features (e.g., faces) are detected with and without the use of L-Reg. The implication is that L-Reg leads to more focused and interpretable attention on relevant features, improving generalization.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Woiqqi5bYV/tables_6_1.jpg alt></figure></p><blockquote><p>This table presents the results of multi-domain generalization (mDG) experiments on several benchmark datasets. It compares the performance of the proposed method (GMDG + L-Reg) against various existing mDG methods, both non-ensemble and ensemble-based. The best performing method for each dataset is shown in bold, and improvements or decreases in performance compared to the GMDG baseline are highlighted in red. The average performance across all datasets is also provided.</p></blockquote><h3 class="relative group">In-depth insights<div id=in-depth-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#in-depth-insights aria-label=Anchor>#</a></span></h3><h4 class="relative group">Logical Reg. Benefits<div id=logical-reg-benefits class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#logical-reg-benefits aria-label=Anchor>#</a></span></h4><p>The heading &lsquo;Logical Reg. Benefits&rsquo; suggests an exploration of the advantages of using logical reasoning within a regularization framework for a machine learning model, likely in the context of image classification. <strong>Improved generalization</strong> is a key expected benefit, allowing the model to perform better on unseen data or novel categories. This likely stems from the regularization&rsquo;s ability to <strong>reduce model complexity</strong>, focusing on salient features and filtering out noise. Another benefit would be <strong>enhanced interpretability</strong>, making the model&rsquo;s decision-making process more transparent and understandable by highlighting the specific features driving classification. The theoretical analysis likely supports these claims by demonstrating how the logical regularization leads to a more balanced feature distribution and reduces the number of extreme weights. This approach contrasts with traditional regularization methods (e.g., L2) which may lack such benefits. Overall, the anticipated benefits suggest a superior approach to model training, offering more robust and transparent performance.</p><h4 class="relative group">Generalization Across Tasks<div id=generalization-across-tasks class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#generalization-across-tasks aria-label=Anchor>#</a></span></h4><p>Generalization across tasks examines a model&rsquo;s ability to apply knowledge learned from one task to perform well on another, related but distinct task. This is crucial for building truly intelligent systems that aren&rsquo;t limited to narrow, specialized functions. <strong>Strong generalization implies the model has learned underlying principles or representations transferable to different contexts</strong>, not simply memorizing task-specific details. Factors influencing this include the <strong>similarity between tasks</strong> (e.g., shared data representations or underlying structures), the <strong>model&rsquo;s architecture</strong> (capable of representing high-level abstractions), and the <strong>training methodology</strong> (e.g., multi-task learning or transfer learning techniques). Measuring generalization across tasks can involve various metrics, assessing performance on unseen tasks, evaluating the efficiency of knowledge transfer, and analyzing the model&rsquo;s ability to adapt to new task distributions. <strong>Research in this area seeks to develop methods that promote better generalization</strong>, leading to more robust and flexible AI applications that can easily adapt and learn from new experiences without extensive retraining.</p><h4 class="relative group">L-Reg Interpretability<div id=l-reg-interpretability class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#l-reg-interpretability aria-label=Anchor>#</a></span></h4><p>The concept of &lsquo;L-Reg Interpretability&rsquo; centers on the capacity of the proposed Logical Regularization (L-Reg) method to enhance the transparency and understandability of deep learning models in visual classification tasks. <strong>L-Reg achieves this by promoting a balanced feature distribution and reducing the complexity of classifier weights.</strong> This reduction isn&rsquo;t merely a decrease in the number of weights but a refinement, effectively removing redundant or less relevant features. The resulting models exhibit a focus on salient, meaningful features, making the decision-making process of the classifier more interpretable. <strong>Visualizations such as GradCAM are used to demonstrate that L-Reg guides the model toward identifying crucial, class-specific features</strong> ‚Äî for example, faces when classifying humans. This interpretability is particularly valuable in generalization scenarios (e.g., multi-domain and generalized category discovery) where understanding the model&rsquo;s decision-making process for unseen data is vital. The improved interpretability is not only insightful but also directly contributes to the enhanced generalization performance of L-Reg, solidifying its practical efficacy.</p><h4 class="relative group">Future Research<div id=future-research class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#future-research aria-label=Anchor>#</a></span></h4><p>Future research directions stemming from this work on logical reasoning regularization (L-Reg) for visual classification could explore several promising avenues. <strong>Extending L-Reg&rsquo;s applicability to other visual tasks</strong> beyond image classification, such as object detection or semantic segmentation, is a key area. Investigating <strong>the impact of different architectural choices</strong> on L-Reg&rsquo;s effectiveness, including the depth at which it&rsquo;s applied and the interaction with various backbone networks, would provide valuable insights. A deeper dive into <strong>theoretical analysis to better understand the relationship between logical reasoning and generalization</strong> could offer a stronger foundation for future improvements. Furthermore, <strong>empirical studies on larger, more diverse datasets</strong> are crucial to confirm the robustness and generalizability of L-Reg across a wide range of real-world scenarios. Finally, exploring <strong>methods to automatically learn or optimize the semantic supports</strong> used by L-Reg, rather than relying on hand-crafted features, could significantly improve its scalability and applicability.</p><h4 class="relative group">L-Reg Limitations<div id=l-reg-limitations class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#l-reg-limitations aria-label=Anchor>#</a></span></h4><p>The core limitation of L-Reg stems from its reliance on the assumption that each dimension of the latent feature vector Z represents an independent semantic. <strong>This independence is not always guaranteed</strong>, particularly in shallower layers of deep neural networks. When this assumption is violated, L-Reg&rsquo;s effectiveness in improving generalization can be compromised. Specifically, it might cause a slight degradation in performance on known classes while improving on unknown classes. <strong>This trade-off highlights the need for careful consideration of the layer from which the semantic features are extracted</strong>; applying L-Reg to deeper layers, where semantic independence is more likely, is crucial for maximizing the benefits. The paper acknowledges this limitation and suggests further research to explore methods for explicitly enforcing semantic independence in Z, potentially using techniques such as orthogonality regularization to enhance the performance of L-Reg across all classes.</p><h3 class="relative group">More visual insights<div id=more-visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#more-visual-insights aria-label=Anchor>#</a></span></h3><details><summary>More on figures</summary><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Woiqqi5bYV/figures_1_1.jpg alt></figure></p><blockquote><p>This figure shows GradCAM visualizations for the &lsquo;person&rsquo; class using the GMDG baseline model with and without L-Reg. The visualizations highlight which parts of the image the model focuses on to identify the &lsquo;person&rsquo; class in seen and unseen domains. The key difference shown is that with L-Reg, the model focuses more on facial features, indicating improved interpretability and generalization.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Woiqqi5bYV/figures_2_1.jpg alt></figure></p><blockquote><p>The figure visualizes the effects of L-Reg on the classifier&rsquo;s weights in a multi-domain generalization plus generalized category discovery setting using the PACS dataset. Subfigure (a) shows heatmaps of the classifier weights, revealing a more balanced distribution and fewer extreme values with L-Reg. Subfigure (b) presents the distribution of classifier weight values for each class, demonstrating that L-Reg leads to simpler classifiers with reduced complexity.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Woiqqi5bYV/figures_3_1.jpg alt></figure></p><blockquote><p>This figure visualizes the distribution of latent features from models trained with and without L-Reg on the PACS dataset under the multi-domain generalization and generalized category discovery setting. It shows that L-Reg leads to a more balanced distribution of features, reducing complexity and improving generalization.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Woiqqi5bYV/figures_8_1.jpg alt></figure></p><blockquote><p>This figure shows GradCAM visualizations comparing a model trained with L2 regularization only and a model trained with both L2 and L-Reg. The visualizations highlight the model&rsquo;s attention when classifying the &lsquo;person&rsquo; category across images from both seen and unseen domains. The L-Reg model demonstrates a focus on facial features even when presented with diverse image styles.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Woiqqi5bYV/figures_17_1.jpg alt></figure></p><blockquote><p>This figure compares the prediction visualizations of a Multilayer Perceptron (MLP) model trained with different regularization techniques. The ground truth (GT) is shown alongside results from a base model, models regularized with L1 and L2, and a model using the proposed L-Reg. The visualizations highlight the differences in how each regularization method affects the model&rsquo;s ability to learn and generalize from the training data. The visualizations show the model&rsquo;s output across the entire input space and is a contour plot showing the model&rsquo;s prediction values. The differences in the contour plots suggest that L-Reg might lead to better generalization performance than the other methods.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Woiqqi5bYV/figures_26_1.jpg alt></figure></p><blockquote><p>This figure shows GradCAM visualizations, highlighting the model&rsquo;s attention during classification. The top row illustrates a model trained without L-Reg (logical reasoning regularization), showcasing ambiguous attention across both seen and unseen domains when classifying a person. The bottom row shows a model trained with L-Reg, demonstrating focused attention on facial features‚Äîa key characteristic for identifying a person‚Äîregardless of domain. The comparison highlights L-Reg&rsquo;s ability to improve model interpretability and generalization.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Woiqqi5bYV/figures_27_1.jpg alt></figure></p><blockquote><p>This figure shows GradCAM visualizations for the known class &rsquo;elephant&rsquo; using GMDG with and without L-Reg. The results demonstrate that L-Reg improves generalization across seen and unseen domains by focusing on salient features (long noses, teeth, and big ears). However, it also highlights a limitation where this approach may compromise performance in domains with less distinctive features (e.g., sketch).</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Woiqqi5bYV/figures_28_1.jpg alt></figure></p><blockquote><p>This figure shows GradCAM visualizations for the classification of giraffes in the PACS dataset. The model was trained using the GMDG method, both with and without L-Reg. The visualizations highlight the areas of the images that are most important for classification. In the model trained with L-Reg, the visualizations clearly focus on the long necks of the giraffes, regardless of whether the images are from the seen or unseen domains. This illustrates the model&rsquo;s improved ability to generalize to unseen data when using L-Reg.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Woiqqi5bYV/figures_29_1.jpg alt></figure></p><blockquote><p>The figure shows GradCAM visualizations for the &lsquo;person&rsquo; class in seen and unseen domains. The visualizations compare models trained with and without the proposed Logical Reasoning Regularization (L-Reg). The goal is to illustrate how L-Reg improves the model&rsquo;s ability to identify salient features (such as faces) for classifying the &lsquo;person&rsquo; class, even in unseen domains, leading to better generalization.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Woiqqi5bYV/figures_30_1.jpg alt></figure></p><blockquote><p>This figure uses GradCAM to visualize the features used by a model for classifying the &lsquo;person&rsquo; class. It shows visualizations for images from both seen and unseen domains. The left column shows the model trained with only L2 regularization; the right column shows the model trained with both L2 and the proposed L-Reg. The visualization highlights the difference in attention: the L-Reg model focuses more on salient features like faces, showcasing improved interpretability and generalization.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Woiqqi5bYV/figures_31_1.jpg alt></figure></p><blockquote><p>This figure shows GradCAM visualizations, highlighting the model&rsquo;s attention areas when classifying images. The visualizations are separated into two groups: images trained without L-Reg and images trained with L-Reg. Each group shows a comparison across four different domains (art painting, photo, sketch, and cartoon). The visualizations reveal that images with L-Reg consistently focuses on human faces as salient features, improving the model&rsquo;s interpretability and generalization to unseen data, such as images from unseen domains or novel classes.</p></blockquote></details><details><summary>More on tables</summary><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Woiqqi5bYV/tables_7_1.jpg alt></figure></p><blockquote><p>This table shows the average accuracy results across six image datasets (CIFAR10, CIFAR100, ImageNet-100, CUB, Stanford Cars, and Herbarium19) for the Generalized Category Discovery (GCD) task. The results are broken down by the model&rsquo;s performance on all classes, known classes, and unknown classes. The table compares the performance of the PIM model (a baseline method for GCD) with and without the addition of the L-Reg (Logical Reasoning Regularization). Improvements from the PIM model are highlighted in red, and degradations are shown in blue. The results demonstrate that L-Reg improves overall performance, particularly for unknown classes, despite slightly impacting the accuracy of known classes in some cases.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Woiqqi5bYV/tables_7_2.jpg alt></figure></p><blockquote><p>This table presents the average accuracy results across five benchmark datasets (PACS, VLCS, OfficeHome, TerraIncognita, and DomainNet) for the multi-domain generalization plus generalized category discovery (mDG+GCD) task. It compares the performance of several methods (ERM, PIM, MIRO, GMDG) with and without the proposed L-Reg. The accuracy is broken down into three categories: all classes, known classes, and unknown classes. The &lsquo;Domain gap&rsquo; column indicates whether the domain gap was minimized or not sufficiently minimized during training. The improvements or degradations brought by L-Reg compared to each baseline model are highlighted in red and blue, respectively. The table shows that in the minimized domain gap settings, the addition of L-Reg consistently improved performance on all three metrics.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Woiqqi5bYV/tables_8_1.jpg alt></figure></p><blockquote><p>This table presents the average results of applying the L-Reg (Logic Regularization) technique to different layers of a deep learning model for the task of domain generalization on the PACS dataset. It shows the impact of applying L-Reg to only the deep layers versus applying it to both earlier and deeper layers of the model. The results are compared against a baseline GMDG (Generalized Multi-Domain Generalization) model without L-Reg. The metrics reported are overall accuracy, accuracy on known classes, and accuracy on unknown classes.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Woiqqi5bYV/tables_9_1.jpg alt></figure></p><blockquote><p>This table compares the proposed method&rsquo;s multi-domain generalization performance against several existing methods across five datasets (PACS, VLCS, OfficeHome, TerraIncognita, and DomainNet). The table shows the average accuracy for each method on each dataset. The best performing method in each dataset group (non-ensemble and ensemble) is highlighted in bold. The table also highlights in red whether the proposed method improves or degrades the performance compared to the GMDG baseline.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Woiqqi5bYV/tables_18_1.jpg alt></figure></p><blockquote><p>This table presents the average accuracy results across various datasets for the Generalized Category Discovery (GCD) task using the PIM model with and without L-Reg. The accuracy is broken down into three categories: overall, known classes, and unknown classes. Positive improvements from using L-Reg are highlighted in red, while negative impacts are shown in blue. This gives a concise overview of the performance gains or losses from applying L-Reg to the PIM model in different datasets and classes.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Woiqqi5bYV/tables_18_2.jpg alt></figure></p><blockquote><p>This table compares the performance of the proposed L-Reg method with various existing multi-domain generalization (mDG) methods across five benchmark datasets (PACS, VLCS, OfficeHome, TerraIncognita, and DomainNet). The best-performing method in each dataset and each group (ensemble vs. non-ensemble) is highlighted in bold. The table shows the average accuracy across different test domains for each method, and importantly, highlights in red where the proposed method improves or degrades upon the existing state-of-the-art method GMDG.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Woiqqi5bYV/tables_19_1.jpg alt></figure></p><blockquote><p>This table presents the results of multi-domain generalization experiments using the ERM (Empirical Risk Minimization) baseline on the TerraIncognita dataset. It compares the performance of ERM alone against ERM with L-Reg (Logical Regularization), ERM with Ortho-Reg (Orthogonality Regularization), and ERM with both L-Reg and Ortho-Reg. The goal is to demonstrate the effectiveness of L-Reg in improving generalization performance, even when compared to other regularization techniques.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Woiqqi5bYV/tables_19_2.jpg alt></figure></p><blockquote><p>This table presents a comparison of the proposed method&rsquo;s performance against existing state-of-the-art multi-domain generalization (mDG) methods. The comparison includes both non-ensemble and ensemble methods. The best performing method in each category is highlighted in bold, and improvements or degradations relative to the GMDG baseline are indicated in red.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Woiqqi5bYV/tables_22_1.jpg alt></figure></p><blockquote><p>This table compares the proposed method&rsquo;s performance on multi-domain generalization (mDG) tasks against several state-of-the-art baselines across five benchmark datasets (PACS, VLCS, OfficeHome, TerraIncognita, and DomainNet). The table shows the average accuracy for each method on each dataset, highlighting the best-performing method in each group. Improvements or reductions compared to the GMDG baseline are indicated in red. This provides a quantitative evaluation of the proposed method&rsquo;s ability to generalize across different domains.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Woiqqi5bYV/tables_22_2.jpg alt></figure></p><blockquote><p>This table presents the results of multi-domain generalization (mDG) experiments on five benchmark datasets (PACS, VLCS, OfficeHome, TerraIncognita, and DomainNet). It compares the performance of the proposed method (GMDG + L-Reg) against several state-of-the-art mDG methods, both ensemble and non-ensemble. The table shows the average accuracy across different test domains for each method, highlighting the best performance in each group and indicating improvements or degradations compared to the GMDG baseline.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Woiqqi5bYV/tables_22_3.jpg alt></figure></p><blockquote><p>This table presents a comparison of the proposed method&rsquo;s performance on multi-domain generalization (mDG) tasks against various existing non-ensemble and ensemble methods. The results are shown for five real-world benchmark datasets: PACS, VLCS, OfficeHome, TerraIncognita, and DomainNet. The best performing method for each dataset and method category (non-ensemble, ensemble) is highlighted in bold. The table also highlights (in red) whether the proposed method shows improvement or degradation in comparison to the GMDG (generalized multi-domain generalization) baseline. This allows for easy assessment of the relative performance gains or losses achieved by the proposed approach across different datasets and in comparison to state-of-the-art techniques.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Woiqqi5bYV/tables_22_4.jpg alt></figure></p><blockquote><p>This table compares the proposed method&rsquo;s multi-domain generalization (mDG) performance against various existing mDG methods across five benchmark datasets (PACS, VLCS, OfficeHome, TerraIncognita, and DomainNet). The results are presented as average accuracy with standard deviation across three trials. The table highlights the best-performing method in each dataset and shows the improvement or degradation relative to the GMDG baseline when L-Reg is added. This provides a quantitative assessment of L-Reg&rsquo;s impact on mDG performance.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Woiqqi5bYV/tables_22_5.jpg alt></figure></p><blockquote><p>This table compares the performance of the proposed method (GMDG + L-Reg) against several existing multi-domain generalization (mDG) methods on five benchmark datasets (PACS, VLCS, OfficeHome, TerraIncognita, and DomainNet). The table shows the average accuracy across different test domains for each method, highlighting the best-performing method in each group. Improvements or degradations relative to the GMDG baseline are indicated. The results demonstrate the effectiveness of the proposed L-Reg in enhancing the generalization performance of GMDG, especially in scenarios where the GMDG baseline achieves relatively low accuracy.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Woiqqi5bYV/tables_23_1.jpg alt></figure></p><blockquote><p>This table compares the proposed method&rsquo;s performance on multi-domain generalization (mDG) tasks against various other existing methods. The results are presented across five different benchmark datasets (PACS, VLCS, OfficeHome, TerraIncognita, and DomainNet). The table highlights the best performing methods for each dataset and indicates improvements or reductions in performance when using the proposed approach compared to a state-of-the-art GMDG baseline. The best results for each dataset are indicated in bold, and improvements or degradations from the GMDG baseline are highlighted in red.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Woiqqi5bYV/tables_23_2.jpg alt></figure></p><blockquote><p>This table presents a comparison of the proposed method&rsquo;s performance on multi-domain generalization (mDG) tasks against several existing non-ensemble and ensemble methods. The results are shown across five benchmark datasets (PACS, VLCS, OfficeHome, TerraIncognita, and DomainNet). The best performing method in each dataset group is highlighted in bold, and improvements or degradations relative to the GMDG baseline (a state-of-the-art method) are indicated in red.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Woiqqi5bYV/tables_23_3.jpg alt></figure></p><blockquote><p>This table compares the performance of the proposed L-Reg method with other state-of-the-art multi-domain generalization (mDG) methods across five benchmark datasets (PACS, VLCS, OfficeHome, TerraIncognita, and DomainNet). The table shows the average accuracy for each method on each dataset, highlighting the best-performing method in bold and indicating improvements or degradations compared to the GMDG baseline (with L-Reg). This provides a quantitative comparison to demonstrate the effectiveness of L-Reg in improving generalization performance in mDG tasks.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Woiqqi5bYV/tables_23_4.jpg alt></figure></p><blockquote><p>This table compares the proposed method&rsquo;s performance on multi-domain generalization (mDG) tasks against various existing non-ensemble and ensemble methods across five benchmark datasets (PACS, VLCS, OfficeHome, TerraIncognita, and DomainNet). The best performing method in each dataset group is highlighted in bold, illustrating the relative improvement or decline introduced by the proposed method compared to a strong baseline (GMDG). Red highlighting indicates performance changes relative to the GMDG baseline.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Woiqqi5bYV/tables_24_1.jpg alt></figure></p><blockquote><p>This table compares the performance of the proposed method (GMDG + L-Reg) with several other multi-domain generalization (mDG) methods on five benchmark datasets (PACS, VLCS, OfficeHome, TerraIncognita, and DomainNet). The results show the average accuracy across all test domains for each method and highlight the best performance within each group of methods. The table also indicates improvements or degradations compared to the GMDG baseline.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Woiqqi5bYV/tables_24_2.jpg alt></figure></p><blockquote><p>This table presents a comparison of the proposed method&rsquo;s performance on multi-domain generalization (MDG) tasks against several existing MDG methods. The comparison is made across five benchmark datasets (PACS, VLCS, OfficeHome, TerraIncognita, and DomainNet). For each dataset and method, the average accuracy across different domains is reported. The best-performing method in each group (non-ensemble, ensemble) is highlighted in bold, and improvements or degradations relative to the GMDG baseline (a state-of-the-art method) are highlighted in red. This allows for a direct assessment of the effectiveness of the proposed method in comparison to existing approaches.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Woiqqi5bYV/tables_24_3.jpg alt></figure></p><blockquote><p>This table compares the performance of the proposed method (GMDG + L-Reg) against various existing multi-domain generalization (mDG) methods on five benchmark datasets (PACS, VLCS, OfficeHome, TerraIncognita, and DomainNet). The table shows the average accuracy across different test domains for each method. Improvements and degradations relative to the GMDG baseline are highlighted to show the effectiveness of the proposed method.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Woiqqi5bYV/tables_24_4.jpg alt></figure></p><blockquote><p>This table compares the performance of the proposed method (GMDG + L-Reg) with several other state-of-the-art multi-domain generalization (mDG) methods on five benchmark datasets (PACS, VLCS, OfficeHome, TerraIncognita, and DomainNet). The results are presented as the average accuracy across different test domains, with the best results for each group of methods highlighted in bold. Improvements or degradations compared to the GMDG baseline are indicated in red.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Woiqqi5bYV/tables_24_5.jpg alt></figure></p><blockquote><p>This table compares the performance of the proposed method (GMDG + L-Reg) with several existing multi-domain generalization (mDG) methods on five benchmark datasets (PACS, VLCS, OfficeHome, TerraIncognita, and DomainNet). The table shows the average accuracy across different test domains for each method and highlights the best performing method in each category. Improvements or degradations compared to the GMDG baseline are indicated in red.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Woiqqi5bYV/tables_25_1.jpg alt></figure></p><blockquote><p>This table presents a comparison of the proposed method&rsquo;s performance on multi-domain generalization (mDG) tasks against several existing non-ensemble and ensemble methods across five benchmark datasets (PACS, VLCS, OfficeHome, TerraIncognita, and DomainNet). The best performing method for each dataset is highlighted in bold, and improvements or degradations compared to the GMDG baseline (a state-of-the-art method) are indicated in red. The average accuracy across all datasets is also provided for each method.</p></blockquote></details><h3 class="relative group">Full paper<div id=full-paper class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#full-paper aria-label=Anchor>#</a></span></h3><div id=gallery-b6bbc4b5d72b9e61251ad3543ca5043c class=gallery><img src=https://ai-paper-reviewer.com/Woiqqi5bYV/1.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/Woiqqi5bYV/2.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/Woiqqi5bYV/3.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/Woiqqi5bYV/4.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/Woiqqi5bYV/5.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/Woiqqi5bYV/6.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/Woiqqi5bYV/7.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/Woiqqi5bYV/8.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/Woiqqi5bYV/9.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/Woiqqi5bYV/10.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/Woiqqi5bYV/11.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/Woiqqi5bYV/12.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/Woiqqi5bYV/13.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/Woiqqi5bYV/14.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/Woiqqi5bYV/15.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/Woiqqi5bYV/16.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/Woiqqi5bYV/17.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/Woiqqi5bYV/18.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/Woiqqi5bYV/19.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/Woiqqi5bYV/20.png class="grid-w50 md:grid-w33 xl:grid-w25"></div></div><section class="flex flex-row flex-wrap justify-center pt-4 text-xl"><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://deep-diver.github.io/neurips2024/spotlight/woiqqi5byv/&amp;title=Interpret%20Your%20Decision:%20Logical%20Reasoning%20Regularization%20for%20Generalization%20in%20Visual%20Classification" title="Share on LinkedIn" aria-label="Share on LinkedIn"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://twitter.com/intent/tweet/?url=https://deep-diver.github.io/neurips2024/spotlight/woiqqi5byv/&amp;text=Interpret%20Your%20Decision:%20Logical%20Reasoning%20Regularization%20for%20Generalization%20in%20Visual%20Classification" title="Tweet on Twitter" aria-label="Tweet on Twitter"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="mailto:?body=https://deep-diver.github.io/neurips2024/spotlight/woiqqi5byv/&amp;subject=Interpret%20Your%20Decision:%20Logical%20Reasoning%20Regularization%20for%20Generalization%20in%20Visual%20Classification" title="Send via email" aria-label="Send via email"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1-27.64 140.9 68.65 266.2 199.1 285.1 19.01 2.888 36.17-12.26 36.17-31.49l1e-4-.6631c0-15.74-11.44-28.88-26.84-31.24-84.35-12.98-149.2-86.13-149.2-174.2.0-102.9 88.61-185.5 193.4-175.4 91.54 8.869 158.6 91.25 158.6 183.2v16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98.0036c-7.299.0-13.2 4.992-15.12 11.68-24.85-12.15-54.24-16.38-86.06-5.106-38.75 13.73-68.12 48.91-73.72 89.64-9.483 69.01 43.81 128 110.9 128 26.44.0 50.43-9.544 69.59-24.88 24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3c0-149.2-133.9-265.632-287.3-235.57zM239.1 304.3c-26.47.0-48-21.56-48-48.05s21.53-48.05 48-48.05 48 21.56 48 48.05-20.6 48.05-48 48.05z"/></svg></span></a></section></div><script>var oid="views_spotlight/Woiqqi5bYV/index.md",oid_likes="likes_spotlight/Woiqqi5bYV/index.md"</script><script type=text/javascript src=/neurips2024/js/page.min.0860cf4e04fa2d72cc33ddba263083464d48f67de06114529043cb4623319efed4f484fd7f1730df5abea0e2da6f3538855634081d02f2d6e920b956f063e823.js integrity="sha512-CGDPTgT6LXLMM926JjCDRk1I9n3gYRRSkEPLRiMxnv7U9IT9fxcw31q+oOLabzU4hVY0CB0C8tbpILlW8GPoIw=="></script></section><footer class="pt-8 max-w-prose print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="flex group mr-3" href=/neurips2024/spotlight/8kpyjm4gt5/><span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&larr;</span>
<span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Is Behavior Cloning All You Need? Understanding Horizon in Imitation Learning</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2024-09-26T00:00:00+00:00>26 September 2024</time>
</span></span></a></span><span><a class="flex text-right group ml-3" href=/neurips2024/spotlight/kxkrlsr4aj/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Input-to-State Stable Coupled Oscillator Networks for Closed-form Model-based Control in Latent Space</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2024-09-26T00:00:00+00:00>26 September 2024</time>
</span></span><span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&larr;</span></a></span></div></div><div class=pt-3><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class=pt-3><script src=https://utteranc.es/client.js repo=pmnxis/pmnxis.github.io issue-term=pathname label=Comment theme=dark-blue crossorigin=anonymous async></script></div></div></footer></article><div id=top-scroller class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer id=site-footer class="py-10 print:hidden"><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2024
AI Paper Reviewer</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://blowfish.page/ target=_blank rel="noopener noreferrer">Blowfish</a></p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/neurips2024/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh+sCQ0E53ghYrxgYqw+0GCRyIEpA=="></script></footer><div id=search-wrapper class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://deep-diver.github.io/neurips2024/ style=z-index:500><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body><script data-name=BMC-Widget data-cfasync=false src=https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js data-id=chansung data-description="Support me on Buy me a coffee!" data-message data-color=#FFDD00 data-position=Left data-x_margin=18 data-y_margin=18></script></html>