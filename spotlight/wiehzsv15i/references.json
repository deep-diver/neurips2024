{"references": [{"fullname_first_author": "Vaswani, A.", "paper_title": "Attention is all you need", "publication_date": "2017-00-00", "reason": "This paper introduced the Transformer architecture, which is foundational to many modern LTSF models and is extensively discussed in relation to the proposed model."}, {"fullname_first_author": "Zhou, H.", "paper_title": "Autoformer: Decomposition transformers with autocorrelation for long-term series forecasting", "publication_date": "2021-00-00", "reason": "This paper proposed Autoformer, a significant contribution to LTSF that is directly compared against the proposed model, highlighting its importance in the field."}, {"fullname_first_author": "Nie, Y.", "paper_title": "A time series is worth 64 words: Long-term forecasting with transformers", "publication_date": "2023-00-00", "reason": "This paper introduced PatchTST, a state-of-the-art LTSF model that is used as a benchmark for comparison against the proposed model, showcasing its significant influence."}, {"fullname_first_author": "Zeng, A.", "paper_title": "Are transformers effective for time series forecasting?", "publication_date": "2023-00-00", "reason": "This paper, DLinear, challenges the dominance of Transformers in LTSF and is compared against the proposed method, underscoring its importance in the field's ongoing evolution."}, {"fullname_first_author": "Deng, J.", "paper_title": "Disentangling structured components: Towards adaptive, interpretable and scalable time series forecasting", "publication_date": "2024-00-00", "reason": "This paper is another work by the authors and serves as a direct comparison point for the proposed SSCNN model, demonstrating the evolution of their research in this area."}]}