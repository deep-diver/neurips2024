[{"figure_path": "YCKuXkw6UL/figures/figures_1_1.jpg", "caption": "Figure 1: Left: From observations of the sound emitted by a speaker, our model constructs an impulse response field that can synthesize observations at novel listener positions. Right: Visualization of spatial variation of impulse responses on MeshRIR[20]. The synthesized impulse responses at different locations are transformed into the frequency domain, where we visualize phase and amplitude distributions at a specific wavelength (1m).", "description": "The figure shows the system's pipeline. The left part shows the top-down view of a room with a speaker and two microphones, indicating impulse responses are captured. The right part visualizes the spatial variation of impulse responses. The synthesized impulse responses at different listener locations are transformed into the frequency domain, showing phase and amplitude distributions.", "section": "1 Introduction"}, {"figure_path": "YCKuXkw6UL/figures/figures_1_2.jpg", "caption": "Figure 2: AcoustiX for improved acoustic simulation. Time-of-flight indicates how long it takes for an emitted sound to reach a listener. With sound traveling at a constant speed, the time-of-arrival should be proportional to the emitter-listener distance. While SoundSpace 2.0 simulations show significant time-of-flight errors, particularly at short emitter-listener distances, AcoustiX produces more accurate arrival times. All simulations are performed in the Gibson Montreal room [56] with direct line-of-sight between emitter and listener.", "description": "This figure compares the accuracy of time-of-flight estimations between SoundSpaces 2.0 and the proposed AcoustiX simulator.  It plots simulated time-of-flight against the ground truth time-of-flight for varying emitter-listener distances.  The results show that SoundSpaces 2.0 has significant errors, especially at shorter distances, whereas AcoustiX provides far more accurate estimations, demonstrating its improved simulation capabilities.", "section": "1 Introduction"}, {"figure_path": "YCKuXkw6UL/figures/figures_4_1.jpg", "caption": "Figure 3: Acoustic Rendering pipeline. We sample points along the ray that is shot from the microphone and query the network to obtain signals s(t) and density \u03c3. Time delay (d/v) is applied to account for the wave propagation. After that, we combine signals and densities to perform acoustic volume rendering for each ray to get the directional signal (hdir(t)). We integrate along the sphere to combine signals from all possible directions with gain pattern G(\u03c9) to obtain the final rendered impulse response h(t).", "description": "This figure illustrates the pipeline of the proposed Acoustic Volume Rendering (AVR) method. It shows how the method works step by step, starting from sampling points along a ray from the microphone to obtaining the final rendered impulse response. The key steps include querying a neural network to get signals and density, applying time delay to account for wave propagation, performing acoustic volume rendering for each ray, and integrating signals from all directions using a gain pattern to get the final impulse response. The figure provides a clear visualization of the entire process, making it easier to understand the core idea of the AVR method.", "section": "3 Method"}, {"figure_path": "YCKuXkw6UL/figures/figures_6_1.jpg", "caption": "Figure 4: Visualization of spatial signal distributions. We compare the spatial signal distributions between ground truth and various methods on the MeshRIR dataset and two simulated environments. While NAF and INRAS fail to capture the signal distributions, our model can estimate amplitude and phase distributions accurately.", "description": "This figure compares the spatial distribution of signals (both amplitude and phase) generated by different methods against the ground truth. The comparison is done across three different datasets: MeshRIR and two simulated environments. The visualization shows that the proposed method (AVR) accurately captures the signal distribution, unlike existing methods (NAF and INRAS) that fail to capture the detailed characteristics.", "section": "4.1 Results on Real World Datasets"}, {"figure_path": "YCKuXkw6UL/figures/figures_7_1.jpg", "caption": "Figure 1: Left: From observations of the sound emitted by a speaker, our model constructs an impulse response field that can synthesize observations at novel listener positions. Right: Visualization of spatial variation of impulse responses on MeshRIR[20]. The synthesized impulse responses at different locations are transformed into the frequency domain, where we visualize phase and amplitude distributions at a specific wavelength (1m).", "description": "The figure shows the model's ability to synthesize impulse responses at novel listener positions based on observations from a speaker.  The left panel depicts the general process, where observations from different positions are used to construct an impulse response field. The right panel shows a visualization of the spatial variation of impulse responses, which is visualized in the frequency domain to highlight phase and amplitude at specific wavelengths. The visualization helps demonstrate how the proposed method can accurately capture spatial variation of impulse responses. ", "section": "1 Introduction"}, {"figure_path": "YCKuXkw6UL/figures/figures_7_2.jpg", "caption": "Figure 4: Visualization of spatial signal distributions. We compare the spatial signal distributions between ground truth and various methods on the MeshRIR dataset and two simulated environments. While NAF and INRAS fail to capture the signal distributions, our model can estimate amplitude and phase distributions accurately.", "description": "This figure compares the spatial distribution of signals from different methods (NAF, INRAS, AVR, and Ground Truth) on the MeshRIR dataset and two simulated environments.  It visualizes the amplitude and phase of the impulse responses across different spatial locations. The comparison highlights that AVR accurately captures the detailed signal characteristics, unlike NAF and INRAS which struggle to represent the spatial signal variations properly. This demonstrates the superior performance of AVR in modeling the complex spatial variations of impulse responses.", "section": "4.1 Results on Real World Datasets"}, {"figure_path": "YCKuXkw6UL/figures/figures_15_1.jpg", "caption": "Figure 3: Acoustic Rendering pipeline. We sample points along the ray that is shot from the microphone and query the network to obtain signals s(t) and density \u03c3. Time delay ( ) is applied to account for the wave propagation. After that, we combine signals and densities to perform acoustic volume rendering for each ray to get the directional signal (hdir(t)). We integrate along the sphere to combine signals from all possible directions with gain pattern G(w) to obtain the final rendered impulse response h(t).", "description": "This figure illustrates the Acoustic Volume Rendering pipeline proposed by the authors. It depicts the process of generating acoustic impulse responses by sampling points along rays, applying time delays for wave propagation, performing volume rendering, and integrating signals from all directions using spherical integration.", "section": "3 Method"}, {"figure_path": "YCKuXkw6UL/figures/figures_15_2.jpg", "caption": "Figure 4: Visualization of spatial signal distributions. We compare the spatial signal distributions between ground truth and various methods on the MeshRIR dataset and two simulated environments. While NAF and INRAS fail to capture the signal distributions, our model can estimate amplitude and phase distributions accurately.", "description": "This figure compares the spatial distribution of acoustic signals generated by different methods (NAF, INRAS, AV-NeRF, and the proposed method) with ground truth on MeshRIR and simulated environments. It visualizes both amplitude and phase distributions in the frequency domain, highlighting the superior accuracy and detail of the proposed method in capturing the complex spatial variations of acoustic signals.", "section": "4.1 Results on Real World Datasets"}, {"figure_path": "YCKuXkw6UL/figures/figures_16_1.jpg", "caption": "Figure 9: Example of a simulated impulse response", "description": "This figure shows an example of a simulated impulse response generated by AcoustiX, the acoustic simulation platform developed by the authors.  The waveform depicts the time-varying signal received at a listener's position, showcasing the complex interactions of sound with the environment (reflections, scattering, diffraction). The initial, strong peak represents the direct sound arrival from the source, while the subsequent oscillations and decay illustrate the effects of environmental reflections and reverberation.  The figure highlights the ability of AcoustiX to generate realistic and detailed impulse responses.", "section": "D.2 Acoustic Ray Tracing"}]