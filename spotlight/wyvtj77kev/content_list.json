[{"type": "text", "text": "Generalized Protein Pocket Generation with Prior-Informed Flow Matching ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Zaixi Zhang1,2, Marinka Zitnik3\u2217, Qi Liu1,2,\u2217 ", "page_idx": 0}, {"type": "text", "text": "1: School of Computer Science and Technology, University of Science and Technology of China 2:State Key Laboratory of Cognitive Intelligence, Hefei, Anhui, China 3:Harvard University zaixi@mail.ustc.edu.cn, marinka@hms.harvard.edu, qiliuql@ustc.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Designing ligand-binding proteins, such as enzymes and biosensors, is essential in bioengineering and protein biology. One critical step in this process involves designing protein pockets, the protein interface binding with the ligand. Current approaches to pocket generation often suffer from time-intensive physical computations or template-based methods, as well as compromised generation quality due to the overlooking of domain knowledge. To tackle these challenges, we propose PocketFlow, a generative model that incorporates protein-ligand interaction priors based on flow matching. During training, PocketFlow learns to model key types of protein-ligand interactions, such as hydrogen bonds. In the sampling, PocketFlow leverages multi-granularity guidance (overall binding affinity and interaction geometry constraints) to facilitate generating high-affinity and valid pockets. Extensive experiments show that PocketFlow outperforms baselines on multiple benchmarks, e.g., achieving an average improvement of 1.29 in Vina Score and 0.05 in scRMSD. Moreover, modeling interactions make PocketFlow a generalized generative model across multiple ligand modalities, including small molecules, peptides, and RNA. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Proteins are the fundamental building blocks of living organisms, often interacting with ligands (e.g., small molecules, nucleic acids, and peptides) to execute their functions. Recently, computational methods have played critical roles in designing functional proteins binding with ligands with broad applications in bio-engineering and therapeutics [76, 52, 50, 51, 67, 8, 58]. For example, Polizzi et al., [65] leverage template-matching methods to design de novo proteins binding with the drug apixaban [15]; Yeh et al., [83] use deep learning methods to generate efficient light-emitting enzyme luciferases with selective substrate catalysis capabilities. To design such ligand-binding proteins, an essential step is to design protein pockets, the protein interface interacting with binding ligands [68, 39, 12, 28]. However, the complexity of ligand-protein interactions, the variability of protein sidechains, and sequence-structure relationships pose great challenges for pocket design [25, 51, 48]. ", "page_idx": 0}, {"type": "text", "text": "Traditional methods for pocket design mainly focus on physics modeling or template-matching [12, 28, 65, 19, 62]. However, the involved physical energy calculation or substructure enumeration could be quite time-consuming. Recent advancements in pocket design have beneftied a lot from deep learningbased approaches [73, 92, 83, 47, 51, 48]. However, these innovative approaches often overlook essential domain knowledge, such as the protein-ligand interactions and the geometric constraints governing them. Though they can efficiently generate many candidates, further screening/optimization is required to get valid and high-affinity pockets. Moreover, most methods are restricted to small molecule ligands, omitting other important ligand types such as nucleic acids [8] and peptides [53]. ", "page_idx": 0}, {"type": "text", "text": "To tackle the aforementioned challenges, we propose PocketFlow, a protein-ligand interaction priorinformed flow matching model for protein pocket generation. Firstly, we define conditional flows for diverse data modalities in the protein-ligand complex including backbone frames, sidechain torsions, and residue/interaction types. We choose flow matching as the generative framework because of its efficiency and flexibility [17, 13, 57, 57]. Furthermore, PocketFlow explicitly learns the dominant protein-ligand interaction types including hydrogen bonds [35], salt bridges [24], hydrophobic interactions [61], and $\\pi-\\pi$ stacking [36], which are crucial for strong binding stability and affinity of protein-ligand pairs [2]. In the sampling process, binding affinity and interaction geometry guidance are adopted to encourage generating pockets with high affinity and validity. Specifically, we leverage a lightweight binding affinity predictor to predict the affinity of the generated complex and apply distance and angle constraints to promote desirable protein-ligand interactions. To circumvent the non-differentiability issues associated with residue type sampling, we employ a novel sidechain ensemble method for interaction geometry calculations. Extensive experiments show that PocketFlow provides a generalized framework for high-quality protein pocket generation across various ligand modalities (small molecules, RNA, peptides, etc.,). The code is provided at https://github.com/zaixizhang/PocketFlow. Our main contributions are summarized as: ", "page_idx": 1}, {"type": "text", "text": "\u2022 Generalized tasks: Our study broadens the scope of protein pocket generation tasks to include various ligand modalities such as small molecules, nucleic acids, and peptides. \u2022 Novel method: PocketFlow combines the recent progress of flow-matching-based generative models and physical/chemical interaction priors (affinity guidance and interaction geometry guidance) to generate protein pockets with enhanced affinity and structural validity. \u2022 Strong performance: PocketFlow outperforms existing methods on various benchmarks of pocket generation, producing an average improvement of 1.29 in Vina score and 0.05 in scRMSD. Further interaction analysis highlights the model\u2019s ability to foster beneficial protein-ligand interactions, e.g., an average of 4.12 hydrogen bonds, while markedly reducing steric clashes (an average of 1.21 in generated pockets v.s. 4.59 in the test set). ", "page_idx": 1}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Generative Models for Protein Generation ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Recent advancements in deep generative models have significantly advanced the field of de novo protein structure generation, enabling researchers to create proteins with specific desired properties [81, 37, 84, 86, 13, 95, 94, 91]. For example, RFDiffusion [81] employs denoising diffusion probabilistic models [33] in conjunction with RoseTTAFold [7] for de novo protein structure generation. It achieved notable success by generating proteins validated in wet lab experiments. Chroma [37] leverages a similar diffusion process with efficient neural architecture for molecular systems that enables long-range reasoning with sub-quadratic scaling. It also demonstrates strong capabilities to satisfy constraints including symmetries, substructure, shape, semantics, and simple naturallanguage prompts. Recently, models leveraging flow matching frameworks have shown promising results on protein generation [86, 13, 85, 40, 53]. For example, FoldFlow [13] proposed a series of flow-matching-based generative models for protein backbones with improved training stability and efficiency than diffusion-based models. FrameFlow [84, 85] further shows sampling efficiency and achieves success on motif-scaffolding tasks with flow matching. However, these protein generation methods are not directly applicable to protein pocket generation that requires protein-ligand interaction modeling. ", "page_idx": 1}, {"type": "text", "text": "2.2 Protein Pocket Generation ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Protein pockets are the protein interface where the ligand binds to the protein and pocket design is a critical task for bioengineering [68, 39, 12, 28]. Traditional methods for pocket design focus on physics modeling or template-matching [12, 28, 65, 19, 62, 93]. For example, PocketOptimizer [62] predicts mutations in protein pockets to increase binding affinity based on physical energy calculation, which may bring a large time burden. The recent progress in protein pocket design has been facilitated by deep generative models [73, 92, 83, 93, 48]. For instance, FAIR [92] co-designs pocket structures and sequences using a two-stage coarse-to-fine refinement approach. RFDiffusion All-Atom [48] extends RFDiffusion for joint modeling of protein and ligand structure to generate ligand-binding protein and further leverages ProteinMPNN[21]/LigandMPNN[22] for sequence design. However, deep-learning methods lacking physical/chemical prior guidance may be less accurate and generalizable. In PocketFlow, we aim to design prior-guided pocket generative models. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "image", "img_path": "WyVTj77KEV/tmp/bcfbebac58a5b4af0b68728e04fd5520dd45131d0f5949a8fcaf526b1d176a8d.jpg", "img_caption": ["3.1 Notations and Problem Formulation "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Figure 1: (a) Parameterization of protein-ligand complex. (b) Illustration of PocketFlow forward process. The affinity and interaction geometry guidance are proposed to improve affinity and structural validity. The red/blue lines denote the unconditional/guidance paths respectively. ", "page_idx": 2}, {"type": "text", "text": "Notations. As shown in Figure 1(a), we model protein-ligand complex as $\\mathcal{C}=\\{\\mathcal{P},\\mathcal{G}\\}$ consisting of protein $\\mathcal{P}$ and ligand $\\mathcal{G}$ (small molecule as an example). Protein $\\mathcal{P}$ is composed of a sequence of residues (amino acids) with residue types denoted $\\mathbf{c}^{(i)}\\in\\mathbb{R}^{20}$ . Consistent with [92, 87], the protein pocket $\\mathcal{R}\\subset\\mathcal{P}$ is defined as the subset of residues closest to the ligand atoms under a threshold $\\delta$ (e.g., $\\bar{3}.5\\,\\mathring{\\mathrm{A}}\\$ ). In a residue, the backbone structure (consisting of $C_{\\alpha},N,C,O)$ is parameterized with $C_{\\alpha}$ position $\\pmb{x}^{(i)}\\in\\mathbb{R}^{3}$ and a frame orientation matrix $O^{(i)}\\in S O(3)$ following [43, 84]. The sidechain is parameterized with maximal 4 torsion angles $\\mathcal{X}^{(i)}=\\{\\chi^{i1},\\chi^{i2},\\chi^{i3},\\chi^{i4}\\}\\in[0,2\\pi)^{4}$ . Given these key parameters, the full atom protein structure can be derived with the ideal frame coordinates and the sidechain bond length/angles [43]. The protein-ligand interaction type for each residue is marked as $I^{(i)}\\in\\mathbb{R}^{5}$ (Hydrogen bond, Salt bridge, Hydrophobic, $\\pi{-}\\pi$ stacking, no interaction). A pocket with $N_{r}$ residues can be compactly represented as $\\mathcal{R}=\\{\\pmb{c}^{(i)},\\pmb{x}^{(i)},\\pmb{O}^{(i)},\\pmb{\\chi}^{(i)},I^{(i)}\\}_{i=1}^{N_{r}}$ . As for the ligand, we use a generalized atom-level representation that accommodates various modalities including small molecules, peptides, and RNA. The atom types and bonding information between atoms are given and PocketFlow predicts the $N_{l}$ ligand atom coordinates (also denoted as $\\pmb{x}^{(i)}$ for conciseness). ", "page_idx": 2}, {"type": "text", "text": "Problem Formulation. PocketFlow co-designs residue types and 3D structures of the protein pocket conditioned on the ligand (could be small molecules, nucleic acids, peptides, etc.) and protein scaffold (the other parts of protein besides the pocket region, i.e., $\\mathcal P\\setminus\\mathcal R)$ . The ligand structure $\\mathcal{G}$ is also predicted. Formally, PocketFlow aims to learn a conditional generative model $p_{\\theta}(\\mathcal{R},\\mathcal{G}|\\mathcal{P}\\setminus\\mathcal{R})$ . ", "page_idx": 2}, {"type": "text", "text": "3.2 Preliminaries on Flow Matching ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Flow matching (FM) [57] is a simulation-free method for learning continuous normalizing flows (CNFs) [18] that generates data by integrating an ordinary differential equation (ODE) over a learned vector field. Here, we first give an overview of Riemannian flow matching [17]. On a manifold $\\mathcal{M}$ , the CNF $\\psi_{t}(\\cdot)\\colon\\mathcal{M}\\rightarrow\\mathcal{M}$ is defined by integrating along a time-dependent vector field $u_{t}(x)\\in\\mathcal{T}_{x}\\mathcal{M}$ where $\\mathcal{T}_{x}\\mathcal{M}$ denotes the tangent space of the manifold at $x\\in{\\mathcal{M}}$ $\\begin{array}{r}{1\\colon\\frac{d}{d t}\\psi_{t}(x)=u_{t}(\\psi_{t}(x)),\\psi_{0}(x)=}\\end{array}$ $x,t\\in[0,1]$ . The flow transforms a simple distribution $p_{0}$ towards the data distribution $p_{1}$ . In FM, the target is to learn a neural network $v_{\\theta}(x,t)$ that approximates $u_{t}(x)$ , while the vanilla regression loss: $\\mathcal{L}_{F M}(\\theta)=\\mathbb{E}_{t\\sim\\mathcal{U}[0,1],p_{t}(x)}\\|v_{\\theta}(x,t)-u_{t}(x)\\|_{g}^{2}$ is hard to compute in practice. Here, $\\mathcal{U}[0,1]$ is the uniform distribution between 0 and 1, and ${\\|\\cdot\\|_{g}^{2}}$ is the norm induced by the Riemannian metric $g$ . Instead, it is tractable to define conditional vector field $\\boldsymbol{u}_{t}(\\boldsymbol{x}|\\boldsymbol{x}_{1})$ and obtain the conditional FM objective: $\\mathcal{L}_{C F M}(\\theta)=\\mathbb{E}_{t\\sim\\mathcal{U}[0,1],p_{1}(x_{1}),p_{t}(x|x_{1})}\\|v_{\\theta}(x,t)-u_{t}(x|x_{1})\\|_{g}^{2}.$ . It has been proved that $\\nabla_{\\boldsymbol{\\theta}}\\mathcal{L}_{F M}(\\boldsymbol{\\theta})=\\nabla_{\\boldsymbol{\\theta}}\\mathcal{L}_{C F M}(\\boldsymbol{\\theta})$ [57, 17]. In the inference, ODE solvers are applied to solve the ODE, e.g., $\\boldsymbol{x}_{1}=\\mathrm{ODESolve}(x_{0},v_{\\theta},0,1)$ where $x_{0}$ is the initialized data and $x_{1}$ is the generated data. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "4 PocketFlow ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "PocketFlow is an interaction prior-informed flow-matching model for pocket design. In this section, we first define PocketFlow for different components in the protein-ligand complex (backbone in Sec. 4.1, sidechain in Sec. 4.2, and residue/interaction types in Sec.4.3). Then we show the prior-informed training and sampling in Sec. 4.4 and 4.5. ", "page_idx": 3}, {"type": "text", "text": "4.1 PocketFlow on SE(3) ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "As introduced in Sec. 3.1, each residue frame can be parameterized by a rigid transformation $T=({\\pmb x}^{(i)},{\\pmb O}^{(i)})$ within SE(3) space. The backbone with $N_{r}$ residues can thus be described by a set of transformations $[T^{(1)},\\dots,T^{(N_{r})}]$ belonging to $\\mathrm{SE}(3)^{N_{r}}$ and constitutes a product space. The following deduction focuses on a single frame but can be generalized to the whole protein backbone. The $C_{\\alpha}$ coordinates $\\pmb{x}^{(i)}$ are initialized with linear interpolation and extrapolation based on the coordinates of neighboring scaffold residues following [92]. The prior distribution of $O^{(i)}$ is chosen as the uniform distribution on $\\mathrm{SO}(3)$ . Following previous works [17, 84], the conditional flow for $\\pmb{x}^{(i)}$ and $O^{(i)}$ are defined as $\\pmb{x}_{t}^{(i)}=(1-t)\\pmb{x}_{0}^{(i)}+t\\pmb{x}_{1}^{(i)}$ and $O_{t}^{(i)}=\\exp_{O_{0}^{(i)}}(t\\log_{O_{0}^{(i)}}(O_{1}^{(i)}))$ respectively, which are geodesic paths in $\\mathbb{R}^{3}$ and SO(3). The exponential map $\\exp_{\\mathrm{o}}$ can be computed using Rodrigues\u2019 formula and the logarithmic map $\\log_{O_{0}}$ is similarly easy to compute with its Lie algebra $\\mathfrak{s o}(3)$ [84]. The loss function of PocketFlow on SE(3) is the summation of the two losses below: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{c o o r d}(\\theta)=\\mathbb{E}_{t,p_{1}(x_{1}),p_{0}(x_{0}),p_{t}(x_{t}|x_{0},x_{1})}\\frac{1}{N_{r}+N_{l}}\\sum_{i=1}^{N_{r}+N_{l}}\\left\\|v_{\\theta}^{(i)}(x_{t}^{(i)},t)-x_{1}^{(i)}+x_{0}^{(i)}\\right\\|_{2}^{2},}\\\\ &{\\mathcal{L}_{o r i}(\\theta)=\\mathbb{E}_{t,p_{1}(O_{1}),p_{0}(O_{0}),p_{t}(O_{t}|O_{0},O_{1})}\\frac{1}{N_{r}}\\sum_{i=1}^{N_{r}}\\left\\|v_{\\theta}^{(i)}(O_{t}^{(i)},t)-\\frac{\\log_{O_{t}^{(i)}}(O_{1}^{(i)})}{1-t}\\right\\|_{\\mathrm{so}(3)}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where we additionally consider $N_{l}$ ligand atom coordinates in $\\mathcal{L}_{c o o r d}(\\theta)$ , for which we use Gaussian distribution at the center of ligand mass as the prior distribution. ", "page_idx": 3}, {"type": "text", "text": "4.2 PocketFlow on Torus ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "As described in Sec. 3.1, the sidechain conformation of each residue can be represented as maximally four torsion angles $\\mathcal{X}^{(i)}\\;=\\;\\{\\chi^{i1},\\chi^{i2},\\chi^{i3},\\chi_{.}^{i4}\\}\\;\\in\\;[0,2\\pi)^{4}$ . In a pocket with $N_{r}$ residues, the sidechain torsion angles form a hypertorus $\\mathbb{T}^{4N_{r}}$ , which is the quotient space $\\mathbb{R}^{4N_{r}}/2\\pi\\mathbb{Z}^{4N_{r}}$ with the equivalence relation: $\\chi=(\\chi^{1},\\dot{\\ldots},\\chi^{4N_{r}})\\sim(\\chi^{1}+2\\pi,\\ldots,\\chi^{\\dot{4}N_{r}})\\sim(\\dot{\\chi}^{1},\\cdot\\cdot\\cdot,\\chi_{\\ldots}^{4N_{r}}+2\\pi)$ [41, 90]. Following [42], the prior distribution is chosen as a uniform distribution over $\\mathbb{T}^{4N_{r}}$ . We regard the torsion angles as mutually independent and use interpolation paths as: $\\chi_{t}=(1-t)\\chi_{0}+t(\\chi_{1}^{\\prime}-\\chi_{0})$ where $\\chi_{1}^{\\prime}=\\left(\\chi_{1}-\\chi_{0}+\\pi\\right)$ mod $(2\\pi)-\\pi+\\chi_{0}$ . The loss for the torsion angles is defined as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{t o r}(\\theta)=\\mathbb{E}_{t,p_{1}(\\chi_{1}),p_{0}(\\chi_{0}),p_{t}(\\chi_{t}|\\chi_{0},\\chi_{1})}\\frac{1}{N_{r}}\\sum_{i=1}^{N_{r}}\\left\\|v_{\\theta}^{(i)}(\\chi_{t}^{(i)},t)-\\chi_{1}^{\\prime(i)}+\\chi_{0}^{(i)}\\right\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "4.3 PocketFlow on Residue Types and Interaction Types ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Each residue is assigned a probability vector with 20 dimensions: $\\pmb{c}^{(i)}\\in\\mathbb{R}^{20}$ . The prior distribution is set as the uniform distribution and the conditional flow is defined as the Euclidean interpolation between $c_{0}$ and $c_{1}$ (one hot vector indicating residue type). $\\mathbf{}c_{t}$ is a probability vector because its summation over all types equals 1. We leverage the cross-entropy loss $\\operatorname{CE}(\\cdot,\\cdot)$ following [53, 73, 16]: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{r e s}=\\mathbb{E}_{t\\sim\\mathcal{U}(0,1),p_{1}(c_{1}),p_{0}(c_{0}),p_{t}(c|c_{0},c_{1})}\\sum_{i=1}^{N_{r}}\\mathrm{CE}\\left(c_{t}^{(i)}+(1-t)v_{\\theta}^{(i)}(c_{t}^{(i)},t),c_{1}^{(i)}\\right),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "which measures the difference between the true probability and the inferred one $\\hat{c}_{1}^{(i)}={c}_{t}^{(i)}+(1-$ $t)v_{\\theta}^{(i)}(c_{t}^{(i)},t)$ . We also note the recent progress of the sequential flow matching methods [74, 16], which can be seamlessly integrated into PocketFlow and are left for future works. ", "page_idx": 4}, {"type": "text", "text": "It has been shown that modeling Protein-ligand interactions explicitly in biomolecular generative models can effectively enhance the generalizability [89, 97]. We used the protein\u2013ligand interaction proflier (PLIP) [69] to detect and annotate the protein-ligand interactions for each residue by analyzing their binding structure. Following [97], 4 dominant interactions are considered including salt bridges, $\\pi{-}\\pi$ stacking, hydrogen bonds, and hydrophobic interactions. For simplicity, if a residue has more than one interaction, we take the one with the highest rank, which considers both the contribution to the binding affinity and the frequencies (see Appendix. B). Similar to residue types, interactions are modeled as category data: $\\bar{I}=\\{I^{(i)}\\}_{i=1}^{N_{r}}$ . Besides the 4 interaction types, we also consider an unknown/none type. Similar to Equ. 4, we have the interaction loss: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{i n t e r}=\\mathbb{E}_{t\\sim\\mathcal{U}(0,1),p_{1}(I_{1}),p_{0}(I_{0}),p_{t}(I|I_{0},I_{1})}\\sum_{i=1}^{N_{r}}\\mathrm{CE}\\left(I_{t}^{(i)}+(1-t)v_{\\theta}^{(i)}(I_{t}^{(i)},t),I_{1}^{(i)}\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "4.4 Model Training ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Network Architecture. To design the binding protein pocket $\\mathcal{R}=\\{\\pmb{c}^{(i)},\\pmb{x}^{(i)},\\pmb{O}^{(i)},\\pmb{\\chi}^{(i)},I^{(i)}\\}_{i=1}^{N_{r}}$ and update the binding ligand coordinates $\\{\\pmb{x}^{(i)}\\}_{i=1}^{N_{l}}$ )}iN=l1, we utilize an architecture modified from the FrameDiff [86] which incorporates Invariant Point Attention (IPA) from AF2 [43] to encode spatial features combined with transformer layers [79] to encode sequence-level features. To achieve a unified representation of both protein residues and ligand atoms, we follow the approach used in RoseTTAFold All-Atom [49], where each ligand atom is treated as an individual residue. Initial representations are based on atom element type embeddings, and the frame orientations are set as identity matrices. To further model the covalent bonding information (single bond, double bond, triple bond, or aromatic bond), we also add the bond embeddings to the 2D track. We use additional MLPs based on the residue embeddings to predict the residue types, interaction types, and sidechain torsion angles. Instead of directly predicting the vector field, we let the model predict the final structure at $t=1$ and interpolate to obtain the vector field. More details are introduced in the Appendix. C. ", "page_idx": 4}, {"type": "text", "text": "Overall Training Loss. The overall training loss of PocketFlow is the summation of Equ. 1, 2, 3, 4, and 5. To fully utilize the protein-ligand context information, we use the whole protein-ligand complex structure at $t$ , i.e., $\\mathcal{C}_{t}=\\mathcal{P}_{t}\\cup\\mathcal{G}_{t}$ as the inputs of $v_{\\theta}(\\cdot,t)$ . ", "page_idx": 4}, {"type": "text", "text": "Equivariance. Following [84, 53], we perform all training and sampling within the zero center of mass $(\\mathrm{CoM})$ subspace by subtracting the CoM of the scaffold from the initialized structure. PocketFlow has the ideal SE(3)-equivariance property of geometric generative models: ", "page_idx": 4}, {"type": "text", "text": "Theorem 1. Denote the $S E(3)$ -transformation as $T_{g}$ , PocketFLow $p_{\\theta}(\\mathcal{R},\\mathcal{G}|\\mathcal{P}\\setminus\\mathcal{R})$ is $S E(3)$ equivariant i.e., $p_{\\theta}(T_{g}(\\mathcal{R},\\mathcal{G})|T_{g}(\\mathcal{P}\\setminus\\mathcal{R}))=p_{\\theta}(\\mathcal{R},\\mathcal{G}|\\mathcal{P}\\setminus\\check{\\mathcal{R}})$ , where $\\mathcal{R}$ denotes the designed pocket, $\\mathcal{G}$ is the binding ligand, and $\\mathcal{P}\\setminus\\mathcal{R}$ is the protein scaffold. ", "page_idx": 4}, {"type": "text", "text": "The main idea is that the SE(3)-invariant prior and SE(3)-equivariant neural network lead to an SE(3)-equivariant generative process of PocketFlow. We give the full proof in the Appendix. D. ", "page_idx": 4}, {"type": "text", "text": "4.5 Pocket Sampling with Prior Guidance ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To improve the binding affinity and structural validity of the generated protein pocket, we proposed a novel domain-knowledge-guided sampling scheme. Generally, we use classifier-guided sampling [23] and consider overall binding affinity guidance and interaction geometry guidance. To encourage the generated protein-ligand complex to satisfy a specific condition $y$ , we apply the Bayes rule [23, 30]: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\nabla_{\\boldsymbol{c}_{t}}\\log{p(\\boldsymbol{\\mathcal{C}}_{t}|\\boldsymbol{y})}=\\nabla_{\\boldsymbol{\\mathcal{C}}_{t}}\\log{p(\\boldsymbol{\\mathcal{C}}_{t})}+\\nabla_{\\boldsymbol{\\mathcal{C}}_{t}}\\log{p(\\boldsymbol{y}|\\boldsymbol{\\mathcal{C}}_{t})},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\nabla_{\\mathcal{C}_{t}}\\log{p(\\mathcal{C}_{t})}$ is the unconditional vector field $v_{\\theta}(\\mathcal{C}_{t},t)$ and $\\nabla_{\\mathcal{C}_{t}}\\log{p(\\boldsymbol{y}|\\mathcal{C}_{t})}$ is the guidance term to constrain the generated complex in a specific condition $y$ . ", "page_idx": 4}, {"type": "text", "text": "Binding Affinity Guidance. To generate protein pockets with higher binding affinity to the target ligand, we train a separate lightweight affinity predictor for guidance (More details of the predictor in Appendix. E.1). Specifically, the data points in the training set are annotated 1 if their affinity is ", "page_idx": 4}, {"type": "text", "text": "higher than the average score of the dataset, otherwise 0 [66]. Because the intermediate structure is noisy, we take the expected structure at $t=1$ , i.e., $\\hat{\\mathcal{C}}_{1}(\\mathcal{C}_{t})$ from the model output and feed it into the predictor. Then we have the classifier-guided velocity field $\\tilde{v}_{\\theta}(\\mathcal{C}_{t},t)$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\tilde{v}_{\\theta}(\\mathcal{C}_{t},t)=v_{\\theta}(\\mathcal{C}_{t},t)-\\gamma\\nabla_{\\mathcal{C}_{t}}\\log p_{\\theta}(v_{b}=1|\\hat{\\mathcal{C}}_{1}(\\mathcal{C}_{t})),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where we add a scaling factor $\\gamma>0$ that controls the gradient strength. $p_{\\theta}$ is the affinity predictor and $v_{b}\\in\\{0,1\\}$ is the binary label of binding affinity. ", "page_idx": 5}, {"type": "text", "text": "Interaction Geometry Guidance. Inspired by [97, 89], we considered 4 dominant non-covalent interaction types in PocketFlow, including salt bridges, $\\pi{-}\\pi$ stacking, hydrogen bonds, and hydrophobic interactions. The local geometries need to satisfy a series of distance/angle constraints to form strong interactions [69]. For example, for hydrogen bonds, the distances between donor and acceptor atoms need to be less than $4.1\\textup{\\AA}$ and larger than $2\\,\\mathring\\mathrm{A}$ to reduce steric clashes [35]. The following inequality is a necessary condition for residues in $\\hat{\\mathcal{C}}_{1}(\\mathcal{C}_{t})$ with predicted interaction label $\\hat{I}_{1}$ as hydrogen bond: ", "page_idx": 5}, {"type": "equation", "text": "$$\nl_{\\operatorname*{min}}\\le\\operatorname*{min}_{\\substack{i\\in A_{h b o n d}^{(k)},j\\in\\mathcal{G}}}\\left\\|\\pmb{x}^{(i)}-\\pmb{x}^{(j)}\\right\\|_{2}\\le l_{\\operatorname*{max}},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $l_{\\mathrm{min}}$ and $l_{\\mathrm{max}}$ are distance constraints; $\\mathcal{A}_{h b o n d}^{(k)}$ denote the $k$ -th residue in the set of residues with predicted hydrogen bonds. With a little abuse of notations, $\\pmb{x}^{(i)}$ and $\\pmb{x}^{(j)}$ denote the candidate atom coordinates in the residue and ligand respectively. The distance guidance can be derived as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n-\\nabla_{\\mathcal{C}_{t}}\\sum_{k=1}^{\\left|\\mathcal{A}_{h b o n d}\\right|}\\left[\\xi_{1}\\operatorname*{max}\\left(0,d^{\\left(k\\right)}-l_{\\operatorname*{max}}\\right)+\\xi_{2}\\operatorname*{max}\\left(0,l_{\\operatorname*{min}}-d^{\\left(k\\right)}\\right)\\right],\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\begin{array}{r}{d^{(k)}=\\operatorname*{min}_{i\\in{A_{h b o n d}^{(k)},j\\in\\mathcal{G}}}\\left\\|\\pmb{x}^{(i)}-\\pmb{x}^{(j)}\\right\\|_{2}}\\end{array}$ and $\\xi_{1},\\xi_{2}>0$ are constant coefficients that control distance constraint, the hydrogen bond needs to satisfy the acceptor/donor angle constraint [69], e.g., the donor/acceptor angle needs to be larger than $100^{\\circ}$ . The angle guidance is presented as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n-\\xi_{3}\\nabla c_{t}\\sum_{k=1}^{|\\mathcal{A}_{h b o n d}|}\\operatorname*{max}(0,\\alpha_{\\operatorname*{min}}-\\phi^{(k)}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where \u03d5(k) = maxi\u2208A(hkb)ond,j\u2208G $\\phi^{(k)}\\,=\\,\\mathrm{max}_{i\\in A_{h b o n d}^{(k)},j\\in\\mathcal{G}}\\,\\mathrm{hangle}(x^{(i)},\\pmb{x}^{(j)})$ and $\\mathrm{{hangle}(\\cdot,\\cdot)}$ calculates the acceptor/donor angle in Figure. 4. $\\xi_{3}>0$ is the guidance coefficient. The guidance for the other interactions is discussed in Appendix. E. We note that the residue type/side chain structure of the pocket is not determined during the sampling. Directly sampling from the residue type distribution makes the model not differentiable [38]. We propose the Sidechain Ensemble for the interaction geometry calculation, i.e., the weighted sum of geometric guidance with respect to residue types (Figure. 6). ", "page_idx": 5}, {"type": "text", "text": "Sampling. With the initialized data, the sampling process is the integration of the ODE $\\begin{array}{r}{\\frac{d\\mathcal{C}_{t}}{d t}\\,=\\,}\\end{array}$ $v_{\\theta}(\\mathcal{C}_{t},t)$ from $t=0$ to $t=1$ with an Euler solver [14]. $\\gamma,\\xi_{1},\\xi_{2}$ , and $\\xi_{3}$ are set as 1 in the default setting. To apply the guidance, we use $\\tilde{v}_{\\theta}$ which is $v_{\\theta}$ plus guidance terms (Equ. 7, 9, and 10): ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\boldsymbol{\\chi}_{t+\\Delta t}^{(i)}=\\mathrm{reg}\\left(\\boldsymbol{\\chi}_{t}^{(i)}+\\tilde{v}_{\\theta}(\\boldsymbol{\\chi}_{t}^{(i)},t)\\Delta t\\right);}\\\\ &{\\boldsymbol{x}_{t+\\Delta t}^{(i)}=\\boldsymbol{x}_{t}^{(i)}+\\tilde{v}_{\\theta}(\\boldsymbol{x}_{t}^{(i)},t)\\Delta t;\\quad\\boldsymbol{O}_{t+\\Delta t}^{(i)}=\\boldsymbol{O}_{t}^{(i)}\\exp\\left(\\tilde{v}_{\\theta}(\\boldsymbol{O}_{t}^{(i)},t)\\Delta t\\right);}\\\\ &{\\boldsymbol{c}_{t+\\Delta t}^{(i)}=\\mathrm{norm}\\left(\\boldsymbol{c}_{t}^{(i)}+\\tilde{v}_{\\theta}(\\boldsymbol{c}_{t}^{(i)},t)\\Delta t\\right);\\quad\\boldsymbol{I}_{t+\\Delta t}^{(i)}=\\mathrm{norm}\\left(\\boldsymbol{I}_{t}^{(i)}+\\tilde{v}_{\\theta}(\\boldsymbol{I}_{t}^{(i)},t)\\Delta t\\right);}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\Delta t$ is the time step; $v_{\\theta}(\\cdot;t)$ denotes the subcomponent of the vector field for different variables. $\\operatorname{norm}(\\cdot)$ means normalizing the vector to a probability vector such that the summation is 1, and $\\mathrm{reg}(\\cdot)$ means regularizing the torsion angles by $\\mathrm{reg}(\\tau)=(\\tau+\\pi)$ mod $(2\\pi)-\\pi$ . ", "page_idx": 5}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "5.1 Experimental Settings ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Datasets. Following previous works [29, 71, 92] we consider two widely used protein-small molecule binding datasets for experimental evaluations: CrossDocked dataset [27] is generated through crossdocking and is split with mmseqs2 [75] at $30\\%$ sequence identity, leading to train/val/test set of ", "page_idx": 5}, {"type": "image", "img_path": "WyVTj77KEV/tmp/be0daee54f88d9fc6165fadb23e1e5c27104613e0038cd736d02f52fd143e94c.jpg", "img_caption": ["Figure 2: Case studies of small-molecule-binding protein pocket design. We show the reference and designed structures/sequences of two protein pockets from the CrossDocked (PDB ID: 1NJE) and Binding MOAD (PDB ID: 2PQL) datasets respectively. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "$100\\mathrm{k}/100/100$ complexes. Binding MOAD dataset [34] contains experimentally determined proteinsmall molecule complexes and is split based on the proteins\u2019 enzyme commission number [9], resulting in $40\\mathrm{k}$ protein-small molecule pairs for training, 100 pairs for validation, and 100 pairs for testing. To test the generalizability of PocketFlow to other ligand modalities, we further consider PPDBench [3], which contains 133 non-redundant complexes of protein-peptides and PDBBind RNA [80], which contains 56 protein-RNA pairs by flitering the PDBBind nucleic acid subset. More details of data preprocessing are included in the Appendix. A. Considering the distance ranges of protein-ligand interactions [60], we redesign all the protein residues that contain atoms within $3.5\\,\\mathring\\mathrm{A}$ of any binding ligand atoms, i.e., the pocket area. The number of designed residues is set the same as the reference pocket. We sample 100 pockets for each complex in the test set for evaluation. ", "page_idx": 6}, {"type": "text", "text": "Baselines and Implementation. PocketFlow is compared with four state-of-the-art representative baseline methods. DEPACT [19] is a template-matching method [88] for pocket design by searching and grafting residues. dyMEAN [47] and FAIR [92] are deep-learning methods based on equivariant translation and iterative refinement. RFDiffusionAA [48] is the latest version of RFDiffusion [81], which can directly generate protein pocket structures conditioned on the ligand. Different from the sequence-structure co-design scheme in dyMEAN and FAIR, the residue types and sidechain structures in RFDiffusionAA are determined with LigandMPNN [22] in a post-hoc manner. ", "page_idx": 6}, {"type": "text", "text": "For experiments on PPDBench and PDBBind RNA, we pretrain PocketFlow and baseline models on the combination of the CrossDocked and Binding MOAD dataset, in which we carefully eliminate all complexes with the same PDB IDs to avoid potential data leakage. Then we represent the peptide and RNA similar to small molecules (atom coordinates/types and covalent bonds) and input to PocketFlow for sampling without fine-tuning. For simplicity, the structure of peptide/RNA is set fixed. All the baselines are run on the same Tesla A100 GPU. ", "page_idx": 6}, {"type": "text", "text": "Performance Metrics. We employ the following metrics to comprehensively evaluate the validity of the generated pocket sequence and structure. Amino Acid Recovery (AAR) is defined as the overlapping ratio between the predicted and ground truth residue types. In bioengineering, mutating too many residues may lead to instability or failure to fold [4]. Therefore, a larger AAR is more favorable. scRMSD refers to self-consistency Root Mean Squared Deviation between the generated and the predicted pocket\u2019s backbone atoms to reflect structural validity. Following established pipelines ", "page_idx": 6}, {"type": "text", "text": "Table 2: Evaluation of different approaches on the peptide and RNA datasets. DEPACT is not reported here because it is specially designed for small molecules. dyMEAN, FAIR, and PocketFlow are pretrained on protein-small molecule datasets and we use the checkpoint of RFDiffusionAA [1]. ", "page_idx": 7}, {"type": "table", "img_path": "WyVTj77KEV/tmp/b9daa4f3a1026d0cff9fdf775e5371dd65081b646313895426dc6999155d04b9.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "WyVTj77KEV/tmp/f72ed67afb21d55916939e557c5f97b98c8343953c4a2bbf9ca44a7fa75545c8.jpg", "img_caption": ["Figure 3: Case studies of peptide/RNA-binding protein pocket design. We show the reference and designed structures/sequences of two protein pockets from PPDBench (PDB ID: 1NQ7) and PDBBind RNA (PDB ID: 5DNO) datasets respectively. The ligand structures (orange) are set fixed. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "[77, 54], for each generated protein structure, eight sequences are firstly derived by ProteinMPNN [21] and the folded to structures with ESMFold [56]. we report the minimum scRMSD for the predicted structures. To measure the binding affinity for protein-small molecule pairs, we calculate Vina Score with AutoDock Vina [78] following [64, 92]. For protein-peptide and protein-RNA pairs, we calculate Rosetta $\\Delta\\Delta G$ [5] and Rosetta-Vienna RNP- $\\Delta\\Delta G$ [44] respectively that measure the binding affinity change. The unit is kcal/mol and a lower Vina score $^{\\prime}\\Delta\\Delta G$ indicates higher affinity. ", "page_idx": 7}, {"type": "text", "text": "5.2 Small-molecule-binding Pocket Design ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Table 1 shows the results of different methods on the CrossDocked and Binding MOAD dataset for small-molecule-binding pocket design. We can observe that PocketFlow overperforms baseline models with a clear margin on AAR, scRMSD, and Vina scores, demonstrating the strong ability of PocketFlow to design pockets with high validity and affinity. The average improvements over the RFDiffusionAA on AAR, scRMSD, and Vina Score are $3.3\\%$ , 0.05, and 1.29 respectively. PocketFlow also predicts more aligned sidechain angles with ground truth as evidenced in Table. 5. Compared with baselines, PocketFlow enjoys the advantage of powerful flow-matching architecture and effective physical/chemical prior guidance. Different from the post-hoc manner of deriving sequences in RFDiffusionAA [48], the co-design scheme also encourages sequence-structure consistency. We also compare the generation efficiency of different models in Figure. 7. Considering the pocket quality improvement brought by PocketFlow, the time overhead is acceptable. ", "page_idx": 7}, {"type": "text", "text": "We also perform ablation studies in Table. 1, where w/o Aff Guide, w/o Geo Guide, and w/o Geo & Aff Guide indicate generating pockets without Affinity Guidance, Interaction Geometry Guidance, and all Guidance respectively. In w/o Inter Learning, we retrain a model without learning interaction types and generate pockets without Interaction Geometry Guidance as well. We can observe that the Affinity and Geometry Guidance indeed play critical roles in enhancing binding affinity and structural validity. For example, the Vina score drops to -7.135 without guidance from -8.236 on the CrossDocked dataset. We also note Affinity Guidance may have slight side effects on scRMSD and we need to balance the strength of unconditional and guidance terms (more results in Appendix. F). ", "page_idx": 7}, {"type": "table", "img_path": "WyVTj77KEV/tmp/e3641c8808082505259bf61acddd82651192e2ee57d06367759c3e9f965b5c91.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 3: Interaction analysis of the generated protein pockets on the CrossDocked dataset. We measure the average number of steric clashes (Clash), hydrogen bonds $\\mathbf{\\sigma}(\\mathbf{H}\\mathbf{B})$ , salt bridges (Salt), hydrophobic interactions (Hydro), and $\\pi{-}\\pi$ stacking $(\\pi-\\pi)$ per protein-ligand complex. More results on the variants of PocketFlow are included in Appendix F. ", "page_idx": 8}, {"type": "text", "text": "5.3 Generalization to Other Ligand Domains ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Besides small molecules, the binding of protein with other ligand modalities such as peptides and nucleic acids play critical roles in biomedicine [82, 8]. However, the available dataset size compared with small molecules-protein complexes is quite limited (e.g., $\\sim100$ in PPDBench v.s. over $100\\mathrm{k}$ in CrossDocked). Here, we explore whether the pretrained PocketFlow on the combination of CrossDocked and Binding MOAD can generalize to peptide and RNA-binding pocket design in Table. 2. The peptide and RNA ligands are represented as molecules (atoms and covalent bonds) to fti into the pretrained models. We have observed that PocketFlow achieves performance comparable to the state-of-the-art baseline, RFDiffusionAA, with prior guidance significantly enhancing its generalizability. Our hypothesis is that the protein-ligand interactions and fundamental physical laws learned by PocketFlow are applicable universally across various biomolecular domains [89, 97]. By explicitly incorporating physical and chemical priors into the generative model, PocketFlow not only aligns with these universal principles but also gains a marked advantage of generalizability. ", "page_idx": 8}, {"type": "text", "text": "5.4 Interaction Analysis and Case Studies ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We adopt PLIP [69] and posecheck [31] to detect the protein-ligand interactions in the generated pockets. In Table. 3, we show the average number of steric clashes, hydrogen bond donors, acceptors, and hydrophobic interactions (without redocking). We observe that PocketFlow can generate pockets with fewer clashes and more favorable interactions. For example, the average steric clashes for RFDiffusionAA and PocketFlow are 3.58 and 1.21 respectively. The average number of Hydrogen Bonds for RFDiffusionAA and PocketFlow are 3.76 and 4.12 respectively. These improvements can be attributed to the model\u2019s affinity/geometry guidance and its enhanced modeling of pocket/ligand flexibility, both of which promote the formation of advantageous protein-ligand interactions while minimizing clashes. Some interaction types such as $\\pi{-}\\pi$ stacking in PocketFlow are a little less than the reference, which may be due to the low frequency of these interactions in the dataset. ", "page_idx": 8}, {"type": "text", "text": "Figure. 2 and 3 show examples of the generated pockets for small molecules, peptides, and RNA. PocketFlow recovers most residue types and changes several key residues to achieve higher binding affinity. The overall structure of the pocket, including the sidechains, is generally well-maintained. ", "page_idx": 8}, {"type": "text", "text": "5.5 Limitations and Broader Impacts ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "While PocketFlow is a powerful generative method for pocket generation, we find the following limitations for further improvement. First, PocketFlow is only trained on protein-small molecule datasets in the paper. In the future, incorporating protein-peptides/nucleic acids/metal datasets, even the generated data from AlphaFold3 [2] would be promising directions. Second, the integration of pretrained protein language models [55] and structure models [96] could significantly enhance PocketFlow\u2019s performance. Additionally, wet lab experiments to verify PocketFlow\u2019s efficacy are planned. Potential negative impacts may include the misuse of PocketFlow for creating harmful biomolecules [32]. Rigorous oversight and screening access to the model should be considered. ", "page_idx": 8}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this paper, we proposed PocketFlow, a protein-ligand interaction prior-informed flow matching model for protein pocket generation. We define multimodal flow matching for protein backbone frames, sidechain torsion angles, and residue/interaction types to appropriately represent the proteinligand complex. The binding affinity and interaction geometry guidance effectively improve the validity and affinity of the generated pockets. Moreover, PocketFlow offers a unified framework covering small-molecule, nucleic acids, and peptides-binding protein pocket generation. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "7 Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This research was supported by grants from the National Natural Science Foundation of China (Grant No. 623B2095) and the Fundamental Research Funds for the Central Universities. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] https://github.com/baker-laboratory/rf_diffusion_all_atom. 2024. [2] Josh Abramson, Jonas Adler, Jack Dunger, Richard Evans, Tim Green, Alexander Pritzel, Olaf Ronneberger, Lindsay Willmore, Andrew J. Ballard, Joshua Bambrick, et al. Accurate structure prediction of biomolecular interactions with alphafold 3. Nature, 2024. [3] Piyush Agrawal, Harinder Singh, Hemant Kumar Srivastava, Sandeep Singh, Gaurav Kishore, and Gajendra PS Raghava. Benchmarking of different molecular docking methods for proteinpeptide docking. BMC bioinformatics, 19:105\u2013124, 2019. [4] Matteo Aldeghi, Vytautas Gapsys, and Bert L de Groot. Accurate estimation of ligand binding affinity changes upon protein mutation. ACS central science, 4(12):1708\u20131718, 2018. [5] Rebecca F Alford, Andrew Leaver-Fay, Jeliazko R Jeliazkov, Matthew J O\u2019Meara, Frank P DiMaio, Hahnbeom Park, Maxim V Shapovalov, P Douglas Renfrew, Vikram K Mulligan, Kalli Kappel, et al. The rosetta all-atom energy function for macromolecular modeling and design. Journal of chemical theory and computation, 13(6):3031\u20133048, 2017. [6] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.   \n[7] Minkyung Baek, Frank DiMaio, Ivan Anishchenko, Justas Dauparas, Sergey Ovchinnikov, Gyu Rie Lee, Jue Wang, Qian Cong, Lisa N Kinch, R Dustin Schaeffer, et al. Accurate prediction of protein structures and interactions using a three-track neural network. Science, 373(6557):871\u2013876, 2021. [8] Minkyung Baek, Ryan McHugh, Ivan Anishchenko, Hanlun Jiang, David Baker, and Frank DiMaio. Accurate prediction of protein\u2013nucleic acid complexes using rosettafoldna. Nature Methods, 21(1):117\u2013121, 2024. [9] Amos Bairoch. The enzyme database in 2000. Nucleic acids research, 28(1):304\u2013305, 2000.   \n[10] David J Barlow and JM Thornton. Ion-pairs in proteins. Journal of molecular biology, 168(4):867\u2013885, 1983.   \n[11] Arieh Y Ben-Naim. Hydrophobic interactions. Springer Science & Business Media, 2012.   \n[12] Matthew J Bick, Per J Greisen, Kevin J Morey, Mauricio S Antunes, David La, Banumathi Sankaran, Luc Reymond, Kai Johnsson, June I Medford, and David Baker. Computational design of environmental sensors for the potent opioid fentanyl. Elife, 6:e28909, 2017.   \n[13] Avishek Joey Bose, Tara Akhound-Sadegh, Kilian Fatras, Guillaume Huguet, Jarrid RectorBrooks, Cheng-Hao Liu, Andrei Cristian Nica, Maksym Korablyov, Michael Bronstein, and Alexander Tong. Se (3)-stochastic flow matching for protein backbone generation. ICLR, 2024.   \n[14] Kathryn Eleda Brenan, Stephen L Campbell, and Linda Ruth Petzold. Numerical solution of initial-value problems in differential-algebraic equations. SIAM, 1995.   \n[15] Wonkyung Byon, Samira Garonzik, Rebecca A Boyd, and Charles E Frost. Apixaban: a clinical pharmacokinetic and pharmacodynamic review. Clinical pharmacokinetics, 58:1265\u20131279, 2019.   \n[16] Andrew Campbell, Jason Yim, Regina Barzilay, Tom Rainforth, and Tommi Jaakkola. Generative flows on discrete state-spaces: Enabling multimodal flows with applications to protein co-design. arXiv preprint arXiv:2402.04997, 2024.   \n[17] Ricky TQ Chen and Yaron Lipman. Riemannian flow matching on general geometries. arXiv preprint arXiv:2302.03660, 2023.   \n[18] Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differential equations. Advances in neural information processing systems, 31, 2018.   \n[19] Yaoxi Chen, Quan Chen, and Haiyan Liu. Depact and pacmatch: A workflow of designing de novo protein pockets to bind small molecules. Journal of Chemical Information and Modeling, 62(4):971\u2013985, 2022.   \n[20] Quan Dao, Hao Phung, Binh Nguyen, and Anh Tran. Flow matching in latent space. arXiv preprint arXiv:2307.08698, 2023.   \n[21] Justas Dauparas, Ivan Anishchenko, Nathaniel Bennett, Hua Bai, Robert J Ragotte, Lukas F Milles, Basile IM Wicky, Alexis Courbet, Rob J de Haas, Neville Bethel, et al. Robust deep learning\u2013based protein sequence design using proteinmpnn. Science, 378(6615):49\u201356, 2022.   \n[22] Justas Dauparas, Gyu Rie Lee, Robert Pecoraro, Linna An, Ivan Anishchenko, Cameron Glasscock, and David Baker. Atomic context-conditioned protein sequence design using ligandmpnn. Biorxiv, pages 2023\u201312, 2023.   \n[23] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:8780\u20138794, 2021.   \n[24] Jason E Donald, Daniel W Kulp, and William F DeGrado. Salt bridges: Geometrically specific, designable interactions. Proteins: Structure, Function, and Bioinformatics, 79(3):898\u2013915, 2011.   \n[25] Jiayi Dou, Lindsey Doyle, Per Jr Greisen, Alberto Schena, Hahnbeom Park, Kai Johnsson, Barry L Stoddard, and David Baker. Sampling and energy evaluation challenges in ligand binding protein design. Protein Science, 26(12):2426\u20132437, 2017.   \n[26] Dennis A Dougherty. Cation- $\\pi$ interactions in chemistry and biology: a new view of benzene, phe, tyr, and trp. Science, 271(5246):163\u2013168, 1996.   \n[27] Paul G Francoeur, Tomohide Masuda, Jocelyn Sunseri, Andrew Jia, Richard B Iovanisci, Ian Snyder, and David R Koes. Three-dimensional convolutional neural networks and a crossdocked data set for structure-based drug design. Journal of chemical information and modeling, 60(9):4200\u20134215, 2020.   \n[28] Anum A Glasgow, Yao-Ming Huang, Daniel J Mandell, Michael Thompson, Ryan Ritterson, Amanda L Loshbaugh, Jenna Pellegrino, Cody Krivacic, Roland A Pache, Kyle A Barlow, et al. Computational design of a modular protein sense-response system. Science, 366(6468):1024\u2013 1028, 2019.   \n[29] Jiaqi Guan, Wesley Wei Qian, Xingang Peng, Yufeng Su, Jian Peng, and Jianzhu Ma. 3d equivariant diffusion for target-aware molecule generation and affinity prediction. ICLR, 2023.   \n[30] Jiaqi Guan, Xiangxin Zhou, Yuwei Yang, Yu Bao, Jian Peng, Jianzhu Ma, Qiang Liu, Liang Wang, and Quanquan Gu. Decompdiff: Diffusion models with decomposed priors for structurebased drug design. ICML, 2023.   \n[31] Charles Harris, Kieran Didi, Arian Jamasb, Chaitanya Joshi, Simon Mathis, Pietro Lio, and Tom Blundell. Posecheck: Generative models for 3d structure-based drug design produce unrealistic poses. In NeurIPS 2023 Workshop on New Frontiers of AI for Drug Discovery and Development, 2023.   \n[32] Jiyan He, Weitao Feng, Yaosen Min, Jingwei Yi, Kunsheng Tang, Shuai Li, Jie Zhang, Kejiang Chen, Wenbo Zhou, Xing Xie, et al. Control risk for potential misuse of artificial intelligence in science. arXiv preprint arXiv:2312.06632, 2023.   \n[33] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840\u20136851, 2020.   \n[34] Liegi Hu, Mark L Benson, Richard D Smith, Michael G Lerner, and Heather A Carlson. Binding moad (mother of all databases). Proteins: Structure, Function, and Bioinformatics, 60(3):333\u2013340, 2005.   \n[35] Roderick E Hubbard and Muhammad Kamran Haider. Hydrogen bonds in proteins: role and strength. eLS, 2010.   \n[36] Christopher A Hunter and Jeremy KM Sanders. The nature of. pi.-. pi. interactions. Journal of the American Chemical Society, 112(14):5525\u20135534, 1990.   \n[37] John B Ingraham, Max Baranov, Zak Costello, Karl W Barber, Wujie Wang, Ahmed Ismail, Vincent Frappier, Dana M Lord, Christopher Ng-Thow-Hing, Erik R Van Vlack, et al. Illuminating protein space with a programmable generative model. Nature, pages 1\u20139, 2023.   \n[38] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv preprint arXiv:1611.01144, 2016.   \n[39] Lin Jiang, Eric A. Althoff, Fernando R. Clemente, Lindsey Doyle, Daniela R\u00f6thlisberger, Alexandre Zanghellini, Jasmine L. Gallaher, Jamie L. Betker, Fujie Tanaka, Carlos F. Barbas, Donald Hilvert, Kendall N. Houk, Barry L. Stoddard, and David Baker. De novo computational design of retro-aldol enzymes. Science, 319(5868):1387\u20131391, 2008.   \n[40] Bowen Jing, Bonnie Berger, and Tommi Jaakkola. Alphafold meets flow matching for generating protein ensembles. arXiv preprint arXiv:2402.04845, 2024.   \n[41] Bowen Jing, Gabriele Corso, Jeffrey Chang, Regina Barzilay, and Tommi Jaakkola. Torsional diffusion for molecular conformer generation. NeurIPS, 2022.   \n[42] Bowen Jing, Stephan Eismann, Pratham N Soni, and Ron O Dror. Equivariant graph neural networks for 3d macromolecular structure. ICML, 2021.   \n[43] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin \u017d\u00eddek, Anna Potapenko, et al. Highly accurate protein structure prediction with alphafold. Nature, 596(7873):583\u2013589, 2021.   \n[44] Kalli Kappel, Inga Jarmoskaite, Pavanapuresan P Vaidyanathan, William J Greenleaf, Daniel Herschlag, and Rhiju Das. Blind tests of rna\u2013protein binding affinity prediction. Proceedings of the National Academy of Sciences, 116(17):8336\u20138341, 2019.   \n[45] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.   \n[46] Xiangzhe Kong, Wenbing Huang, and Yang Liu. Conditional antibody design as 3d equivariant graph translation. ICLR, 2023.   \n[47] Xiangzhe Kong, Wenbing Huang, and Yang Liu. End-to-end full-atom antibody design. arXiv preprint arXiv:2302.00203, 2023.   \n[48] Rohith Krishna, Jue Wang, Woody Ahern, Pascal Sturmfels, Preetham Venkatesh, Indrek Kalvet, Gyu Rie Lee, Felix S Morey-Burrows, Ivan Anishchenko, Ian R Humphreys, et al. Generalized biomolecular modeling and design with rosettafold all-atom. Science, page eadl2528, 2024.   \n[49] Rohith Krishna, Jue Wang, Woody Ahern, Pascal Sturmfels, Preetham Venkatesh, Indrek Kalvet, Gyu Rie Lee, Felix S Morey-Burrows, Ivan Anishchenko, Ian R Humphreys, et al. Generalized biomolecular modeling and design with rosettafold all-atom. Science, page eadl2528, 2024.   \n[50] Alexander Kroll, Sahasra Ranjan, Martin KM Engqvist, and Martin J Lercher. A general model to predict small molecule substrates of enzymes based on machine and deep learning. Nature Communications, 14(1):2787, 2023.   \n[51] Gyu Rie Lee, Samuel J Pellock, Christoffer Norn, Doug Tischer, Justas Dauparas, Ivan Anishchenko, Jaron AM Mercer, Alex Kang, Asim Bera, Hannah Nguyen, et al. Small-molecule binding and sensing with a designed protein family. bioRxiv, pages 2023\u201311, 2023.   \n[52] Yipin Lei, Shuya Li, Ziyi Liu, Fangping Wan, Tingzhong Tian, Shao Li, Dan Zhao, and Jianyang Zeng. A deep-learning framework for multi-level peptide\u2013protein interaction prediction. Nature communications, 12(1):5465, 2021.   \n[53] Haitao Lin, Odin Zhang, Huifeng Zhao, Lirong Wu, Dejun Jiang, Zicheng Liu, Yufei Huang, and Stan Z Li. Ppflow: Target-aware peptide design with torsional flow matching. bioRxiv, pages 2024\u201303, 2024.   \n[54] Yeqing Lin and Mohammed AlQuraishi. Generating novel, designable, and diverse protein structures by equivariantly diffusing oriented residue clouds. ICML, 2023.   \n[55] Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Allan dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Sal Candido, et al. Language models of protein sequences at the scale of evolution enable accurate structure prediction. BioRxiv, 2022:500902, 2022.   \n[56] Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, et al. Evolutionary-scale prediction of atomic-level protein structure with a language model. Science, 379(6637):1123\u20131130, 2023.   \n[57] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022.   \n[58] Lei Lu, Xuxu Gou, Sophia K Tan, Samuel I Mann, Hyunjun Yang, Xiaofang Zhong, Dimitrios Gazgalis, Jes\u00fas Valdiviezo, Hyunil Jo, Yibing Wu, et al. De novo design of drug-binding proteins with predictable binding energy and specificity. Science, 384(6691):106\u2013112, 2024.   \n[59] Shitong Luo, Jiaqi Guan, Jianzhu Ma, and Jian Peng. A 3d generative model for structure-based drug design. NeurIPS, 34:6229\u20136239, 2021.   \n[60] Gilles Marcou and Didier Rognan. Optimizing fragment and scaffold docking by use of molecular interaction fingerprints. Journal of chemical information and modeling, 47(1):195\u2013 207, 2007.   \n[61] Emily E Meyer, Kenneth J Rosenberg, and Jacob Israelachvili. Recent progress in understanding hydrophobic interactions. Proceedings of the National Academy of Sciences, 103(43):15739\u2013 15746, 2006.   \n[62] Jakob Noske, Josef Paul Kynast, Dominik Lemm, Steffen Schmidt, and Birte H\u00f6cker. Pocketoptimizer 2.0: A modular framework for computer-aided ligand-binding design. Protein Science, 32(1):e4516, 2023.   \n[63] Noel M O\u2019Boyle, Michael Banck, Craig A James, Chris Morley, Tim Vandermeersch, and Geoffrey R Hutchison. Open babel: An open chemical toolbox. Journal of cheminformatics, 3:1\u201314, 2011.   \n[64] Xingang Peng, Shitong Luo, Jiaqi Guan, Qi Xie, Jian Peng, and Jianzhu Ma. Pocket2mol: Efficient molecular sampling based on 3d protein pockets. ICML, 2022.   \n[65] Nicholas F Polizzi and William F DeGrado. A defined structural unit enables de novo design of small-molecule\u2013binding proteins. Science, 369(6508):1227\u20131233, 2020.   \n[66] Hao Qian, Wenjing Huang, Shikui Tu, and Lei Xu. Kgdiff: towards explainable target-aware molecule generation with knowledge guidance. Briefings in Bioinformatics, 25(1):bbad435, 2024.   \n[67] Zhuoran Qiao, Weili Nie, Arash Vahdat, Thomas F Miller III, and Animashree Anandkumar. State-specific protein\u2013ligand complex structure prediction with a multiscale deep generative model. Nature Machine Intelligence, pages 1\u201314, 2024.   \n[68] Daniela R\u00f6thlisberger, Olga Khersonsky, Andrew M Wollacott, Lin Jiang, Jason DeChancie, Jamie Betker, Jasmine L Gallaher, Eric A Althoff, Alexandre Zanghellini, Orly Dym, et al. Kemp elimination catalysts by computational enzyme design. Nature, 453(7192):190\u2013195, 2008.   \n[69] Sebastian Salentin, Sven Schreiber, V Joachim Haupt, Melissa F Adasme, and Michael Schroeder. Plip: fully automated protein\u2013ligand interaction profiler. Nucleic acids research, 43(W1):W443\u2013W447, 2015.   \n[70] V\u0131ctor Garcia Satorras, Emiel Hoogeboom, and Max Welling. E (n) equivariant graph neural networks. In International conference on machine learning, pages 9323\u20139332. PMLR, 2021.   \n[71] Arne Schneuing, Yuanqi Du, Charles Harris, Arian Jamasb, Ilia Igashov, Weitao Du, Tom Blundell, Pietro Li\u00f3, Carla Gomes, Max Welling, et al. Structure-based drug design with equivariant diffusion models. arXiv preprint arXiv:2210.13695, 2022.   \n[72] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2020.   \n[73] Hannes St\u00e4rk, Bowen Jing, Regina Barzilay, and Tommi Jaakkola. Harmonic selfconditioned flow matching for multi-ligand docking and binding site design. arXiv preprint arXiv:2310.05764, 2023.   \n[74] Hannes Stark, Bowen Jing, Chenyu Wang, Gabriele Corso, Bonnie Berger, Regina Barzilay, and Tommi Jaakkola. Dirichlet flow matching with applications to dna sequence design. arXiv preprint arXiv:2402.05841, 2024.   \n[75] Martin Steinegger and Johannes S\u00f6ding. Mmseqs2 enables sensitive protein sequence searching for the analysis of massive data sets. Nature biotechnology, 35(11):1026\u20131028, 2017.   \n[76] Christine E Tinberg, Sagar D Khare, Jiayi Dou, Lindsey Doyle, Jorgen W Nelson, Alberto Schena, Wojciech Jankowski, Charalampos G Kalodimos, Kai Johnsson, Barry L Stoddard, et al. Computational design of ligand-binding proteins with high affinity and selectivity. Nature, 501(7466):212\u2013216, 2013.   \n[77] Brian L. Trippe, Jason Yim, Doug Tischer, David Baker, Tamara Broderick, Regina Barzilay, and Tommi S. Jaakkola. Diffusion probabilistic modeling of protein backbones in 3d for the motifscaffolding problem. In The Eleventh International Conference on Learning Representations, 2023.   \n[78] Oleg Trott and Arthur J Olson. Autodock vina: improving the speed and accuracy of docking with a new scoring function, efficient optimization, and multithreading. Journal of computational chemistry, 31(2):455\u2013461, 2010.   \n[79] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \n[80] Renxiao Wang, Xueliang Fang, Yipin Lu, Chao-Yie Yang, and Shaomeng Wang. The pdbbind database: methodologies and updates. Journal of medicinal chemistry, 48(12):4111\u20134119, 2005.   \n[81] Joseph L Watson, David Juergens, Nathaniel R Bennett, Brian L Trippe, Jason Yim, Helen E Eisenach, Woody Ahern, Andrew J Borst, Robert J Ragotte, Lukas F Milles, et al. De novo design of protein structure and function with rfdiffusion. Nature, 620(7976):1089\u20131100, 2023.   \n[82] Kejia Wu, Hua Bai, Ya-Ting Chang, Rachel Redler, Kerrie E McNally, William Sheffler, TJ Brunette, Derrick R Hicks, Tomos E Morgan, Tim J Stevens, et al. De novo design of modular peptide-binding proteins by superhelical matching. Nature, 616(7957):581\u2013589, 2023.   \n[83] Andy Hsien-Wei Yeh, Christoffer Norn, Yakov Kipnis, Doug Tischer, Samuel J Pellock, Declan Evans, Pengchen Ma, Gyu Rie Lee, Jason Z Zhang, Ivan Anishchenko, et al. De novo design of luciferases using deep learning. Nature, 614(7949):774\u2013780, 2023.   \n[84] Jason Yim, Andrew Campbell, Andrew YK Foong, Michael Gastegger, Jos\u00e9 Jim\u00e9nez-Luna, Sarah Lewis, Victor Garcia Satorras, Bastiaan S Veeling, Regina Barzilay, Tommi Jaakkola, et al. Fast protein backbone generation with se (3) flow matching. arXiv preprint arXiv:2310.05297, 2023.   \n[85] Jason Yim, Andrew Campbell, Emile Mathieu, Andrew YK Foong, Michael Gastegger, Jos\u00e9 Jim\u00e9nez-Luna, Sarah Lewis, Victor Garcia Satorras, Bastiaan S Veeling, Frank No\u00e9, et al. Improved motif-scaffolding with se (3) flow matching. arXiv preprint arXiv:2401.04082, 2024.   \n[86] Jason Yim, Brian L Trippe, Valentin De Bortoli, Emile Mathieu, Arnaud Doucet, Regina Barzilay, and Tommi Jaakkola. Se (3) diffusion model with application to protein backbone generation. ICML, 2023.   \n[87] Zhang Zaixi, Wanxiang Shen, Qi Liu, and Marinka Zitnik. Pocketgen: Generating full-atom ligand-binding protein pockets. bioRxiv, pages 2024\u201302, 2024.   \n[88] Alexandre Zanghellini, Lin Jiang, Andrew M Wollacott, Gong Cheng, Jens Meiler, Eric A Althoff, Daniela R\u00f6thlisberger, and David Baker. New algorithms and an in silico benchmark for computational enzyme design. Protein Science, 15(12):2785\u20132794, 2006.   \n[89] Odin Zhang, Tianyue Wang, Gaoqi Weng, Dejun Jiang, Ning Wang, Xiaorui Wang, Huifeng Zhao, Jialu Wu, Ercheng Wang, Guangyong Chen, et al. Learning on topological surface and geometric structure for 3d molecular generation. Nature Computational Science, 3(10):849\u2013859, 2023.   \n[90] Yangtian Zhang, Zuobai Zhang, Bozitao Zhong, Sanchit Misra, and Jian Tang. Diffpack: A torsional diffusion model for autoregressive protein side-chain packing. arXiv preprint arXiv:2306.01794, 2023.   \n[91] Zaixi Zhang and Qi Liu. Learning subpocket prototypes for generalizable structure-based drug design. ICML, 2023.   \n[92] Zaixi Zhang, Zepu Lu, Hao Zhongkai, Marinka Zitnik, and Qi Liu. Full-atom protein pocket design via iterative refinement. Advances in Neural Information Processing Systems, 36:16816\u2013 16836, 2023.   \n[93] Zaixi Zhang, Wanxiang Shen, Qi Liu, and Marinka Zitnik. Pocketgen: Generating full-atom ligand-binding protein pockets. bioRxiv, pages 2024\u201302, 2024.   \n[94] Zaixi Zhang, Mengdi Wang, and Qi Liu. Flexsbdd: Structure-based drug design with flexible protein modeling. Advances in Neural Information Processing Systems, 2024.   \n[95] Zaixi Zhang, Jiaxian Yan, Qi Liu, Enhong Chen, and Marinka Zitnik. A systematic survey in geometric deep learning for structure-based drug design. arXiv preprint arXiv:2306.11768, 2023.   \n[96] Zuobai Zhang, Minghao Xu, Arian Jamasb, Vijil Chenthamarakshan, Aurelie Lozano, Payel Das, and Jian Tang. Protein representation learning by geometric structure pretraining. arXiv preprint arXiv:2203.06125, 2022.   \n[97] Wonho Zhung, Hyeongwoo Kim, and Woo Youn Kim. 3d molecular generative framework for interaction-guided drug design. Nature Communications, 15(1):2688, 2024. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Dataset Preprocessing ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We consider two widely used datasets for benchmark evaluation: CrossDocked dataset [27] contains 22.5 million protein-molecule pairs generated through cross-docking. Following previous works [59, 64, 92], we fliter out data points with binding pose RMSD greater than $1\\,\\mathring\\mathrm{A}$ , leading to a refined subset with around 180k data points. For data splitting, we use mmseqs2 [75] to cluster data at $30\\%$ sequence identity, and randomly draw 100k protein-ligand structure pairs for training and 100 pairs from the remaining clusters for testing and validation, respectively; Binding MOAD dataset [34] contains around 41k experimentally determined protein-ligand complexes. Following previous work [71], we keep pockets with valid and moderately \u2018drug-like\u2019 ligands with QED score $\\geq0.3$ . We further filter the dataset to discard molecules containing atom types $\\notin\\{C,N,O,S,B,B r,C l,P,I,F\\}$ as well as binding pockets with non-standard amino acids. Then, we randomly sample and split the flitered dataset based on the Enzyme Commission Number (EC Number) [9] to ensure different sets do not contain proteins from the same EC Number main class. Finally, we have 40k protein-ligand pairs for training, 100 pairs for validation, and 100 pairs for testing. For all the benchmark tasks in this paper, PocketFlow and all the other baseline methods are trained with the same data split for a fair comparison. ", "page_idx": 15}, {"type": "text", "text": "To test the generalizability of PocketFlow to other ligand modalities, we further consider PPDBench [3], which contains 133 non-redundant complexes of protein-peptides and PDBBind RNA [80], which contains 56 protein-RNA pairs by filtering the PDBBind nucleic acid subset with RNA sequence lengths longer than 5 and less than 15. ", "page_idx": 15}, {"type": "text", "text": "B Considered Protein-ligand Interactions ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Table 4: Key geometric constraints to define protein-ligand interactions [69]. Angles in degree and distances in \u00c5ngstr\u00f6m. ", "page_idx": 15}, {"type": "table", "img_path": "WyVTj77KEV/tmp/fd840c629948fdd1f4a28ef1c9cb796c840c6b45c35ec650679da12be47da2f4.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Following [97], we considered 4 dominant non-covalent interaction types in PocketFlow, including salt bridges, $\\pi{-}\\pi$ stacking, hydrogen bonds, and hydrophobic interactions (ranked based on their contribution to affinity and reversed frequency). The frequency statistics are listed in Table.3. ", "page_idx": 15}, {"type": "text", "text": "\u2022 Salt bridges [10], which are electrostatic interactions between oppositely charged centers, are often considered among the strongest interactions in protein structures and other biomolecular complexes. They can significantly contribute to stability and binding affinity due to their strong electrostatic nature. To form salt bridges, two centers of opposite charges need to be below a distance of SALTBRIDGE_DIST_MAX. ", "page_idx": 15}, {"type": "text", "text": "\u2022 Hydrogen bonds [35] occur between a hydrogen atom covalently bonded to a more electronegative atom (like oxygen or nitrogen) and another electronegative atom. Their strength is less than that of salt bridges but is significant in biological contexts. ", "page_idx": 15}, {"type": "text", "text": "A hydrogen bond is established between a hydrogen bond donor and acceptor (OpenBabel [63] is used to detect hydrogen bond donor/acceptor). The distance between the donor and acceptor needs to be less than HBOND_DIST_MAX. The donor and acceptor angle needs to be larger than HBOND_DON_ANGLE_MIN and HBOND_ACC_ANGLE_MIN respectively. Since PocketFlow only considers heavy atoms (no hydrogen atoms), we consider the geometry of hydrogen bonds without protonation [35] (see Figure. 4). For simplicity, we do not differentiate donor/acceptor in the interaction geometry guidance. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "\u2022 $\\pi{-}\\pi$ stacking [69] involve the stacking of aromatic rings (like those found in phenylalanine, tyrosine, or tryptophan) due to favorable van der Waals forces and sometimes electrostatic interactions. $\\pi{-}\\pi$ stacking is crucial in the structure of nucleic acids and proteins, especially in the active sites of many enzymes, although they are generally weaker than hydrogen bonds and salt bridges. ", "page_idx": 16}, {"type": "text", "text": "To form $\\pi{-}\\pi$ stacking, we first need two aromatic rings (OpenBabel [63] is used to detect aromatic rings). The distance between the two ring centers needs to be below PISTACK_DIST_MAX. The angle between two normal vectors of ring planes needs to be below PISTACK_ANG_DEV. Additionally, each ring center is projected onto the opposite ring plane. The distance between the other ring center and the projected point (i.e., the offset) has to be less than PISTACK_OFFSET_MAX. Figure. 5 shows the illustration. ", "page_idx": 16}, {"type": "text", "text": "\u2022 Hydrophobic Interactions [11] are caused by the tendency of hydrophobic side chains to avoid contact with water, leading them to aggregate. While these are not strong interactions on their own, they play a crucial role in the folding and stability of proteins by driving the burial of nonpolar groups away from the aqueous environment, thereby contributing significantly to the overall stability. To form hydrophobic interactions, the atom distance needs to be less than HYDROPH_DIST_MAX. ", "page_idx": 16}, {"type": "image", "img_path": "WyVTj77KEV/tmp/e7a57b3fc8ecc9bc047a41f9f02adfd507d9da3d1d4d50c633fc6e2d3fbdb65d.jpg", "img_caption": ["Figure 4: Schematic representation of the geometry of a hydrogen bond (without protonation). D and A denote the hydrogen bond donor and acceptor respectively. X1 and X2 are the neighboring atoms of donor and acceptor. The hydrogen bond distance as well as donor/acceptor angles are illustrated. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "WyVTj77KEV/tmp/385663b48e52aed335412e854ca7f525022761d63816a9857a33477acc7ef40b.jpg", "img_caption": ["Figure 5: Schematic representation of the geometry of $\\pi{-}\\pi$ stacking. To form a $\\pi{-}\\pi$ stacking, we need two aromatic rings. The distance of ring centers, the angle between normal vectors, and the projection offset of the ring centers need to satisfy a set of geometry constraints. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "C Model Details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In PocketFlow, we adopt a neural network architecture modified from the FrameDiff [86]. This architecture consists of Invariant Point Attention from AlphaFold2 [43] and transformer blocks [79]. In this section, we use superscripts to refer to the network layer and subscripts to indexes or variables. In the network, each residue/ligand atom is represented by one embedding $h\\in\\mathbb{R}^{D_{x}}$ and a frame $T\\,\\in\\,S E(3)$ . For the ligand atoms, the orientation matrix of frame is set as identity matrices. Overall, at the $\\ell$ -th layer of the network, $\\pmb{h}^{\\ell}=[h_{1}^{\\ell},\\pmb{\\mathscr{s}}...,h_{N}^{\\ell}]\\in\\mathbb{R}^{N\\times D_{x}}$ are all the node embeddings where $h_{i}^{\\ell}$ is the embedding for the $i$ -th node and $N=N_{p}+N_{l}$ is the total number of nodes; $\\mathbf{\\cal{T}}^{\\ell}=[T_{1}^{\\ell},\\dots,T_{N}^{\\ell}]\\in S E(3)^{N}$ is the frames of every node at the $\\ell_{}$ -th layer; $\\pmb{z}^{\\ell}\\in\\mathbb{R}^{N\\times N\\times D_{z}}$ are edge embeddings with $z_{i j}^{\\ell}$ being the embedding of the edge between residues $i$ and $j$ . In the following paragraphs, we introduce the details of feature initialization, node/edge update, backbone update, and residue type/interaction type/sidechain torsion angle predictions. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "Feature initialization. Following [86], node embeddings are initialized with residue indices and timestep while edge embeddings additionally get relative sequence distances. Initial embeddings at layer 0 for residues $i,j$ are obtained with an MLP and sinusoidal embeddings $\\phi(\\cdot)$ [79] over the features. Following [86], we additionally include self-conditioning of predicted $C_{\\alpha}$ displacements. Let $\\tilde{x}$ be the $C_{\\alpha}$ coordinates predicted during self-conditioning. $50\\%$ of the time we set $\\tilde{x}=0$ . The binned displacement of two $C_{\\alpha}$ is given as, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathrm{disp}_{i j}=\\sum_{k=1}^{N_{\\mathrm{bins}}}1\\{\\tilde{x}^{i}-\\tilde{x}^{j}<d_{k}\\},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $d_{1},\\ldots,d_{N_{\\mathrm{bins}}}$ are linspace $(0,20)$ are equally spaced bins between 0 and 20 angstroms. In our experiments we set $N_{\\mathrm{bins}}=22$ . The initial embeddings can be expressed as ", "page_idx": 17}, {"type": "equation", "text": "$$\nh_{i}^{0}=\\mathbf{M}\\mathbf{L}\\mathbf{P}(\\phi(i),\\phi(t)),\\quad h_{i}^{0}\\in\\mathbb{R}^{D_{h}},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\nz_{i j}^{0}=\\mathbf{MLP}(\\phi(i),\\phi(j),\\phi(j-i),\\phi(t),\\phi(\\mathbf{disp}_{i j})),\\quad z_{i j}^{0}\\in\\mathbb{R}^{D_{z}},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $D_{h},D_{z}$ are node and edge embedding dimensions. For the initialization of $C_{\\alpha}$ coordinates, we use the interpolation and extrapolation strategy of FAIR [92]. ", "page_idx": 17}, {"type": "text", "text": "Node update. The process of node update is shown below. Invariant Point Attention (IPA) is from [43]. No weight sharing is performed across layers. We use the vanilla Transformer from [79]. We use Multi-Layer Perceptrons (MLP) with 3 Linear layers, ReLU activation, and LayerNorm [6] after the final layer. ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{h_{\\mathrm{ipa}}=\\mathrm{LayerNorm}(\\mathrm{IPA}(h^{\\ell},z^{\\ell},\\mathbf{T}^{\\ell})+h^{\\ell}),\\quad h_{\\mathrm{ipa}}\\in\\mathbb{R}^{N\\times D_{h}}}\\\\ &{h_{\\mathrm{skip}}=\\mathrm{Linear}(h_{0}),\\quad h_{\\mathrm{skip}}\\in\\mathbb{R}^{N\\times D_{\\mathrm{skip}}}}\\\\ &{\\quad h_{\\mathrm{in}}=\\mathrm{concat}(h_{\\mathrm{ipa}},h_{\\mathrm{skip}}),\\quad h_{\\mathrm{in}}\\in\\mathbb{R}^{N\\times(D_{h}+D_{\\mathrm{skip}})}}\\\\ &{h_{\\mathrm{trans}}=\\mathrm{Transformer}(h_{\\mathrm{in}}),\\quad h_{\\mathrm{trans}}\\in\\mathbb{R}^{N\\times(D_{h}+D_{\\mathrm{skip}})}}\\\\ &{\\quad h_{\\mathrm{out}}=\\mathrm{Linear}(h_{\\mathrm{trans}})+h_{\\ell},\\quad h_{\\mathrm{out}}\\in\\mathbb{R}^{N\\times D_{h}}}\\\\ &{h^{\\ell+1}=\\mathrm{MLP}(h_{\\mathrm{out}}),\\quad h_{\\ell+1}\\in\\mathbb{R}^{N\\times D_{h}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Edge update. Each edge is updated with a MLP over the current edge and source and target node embeddings. In the first line, node embeddings are first projected down to half the dimension. ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{h_{\\mathrm{down}}^{\\ell+1}=\\operatorname{Linear}(h^{\\ell+1}),\\quad h_{\\mathrm{down}}^{\\ell+1}\\in\\mathbb{R}^{N\\times D_{h}/2}}\\\\ &{\\quad z_{i j}^{\\prime}=\\mathrm{concat}(h_{\\mathrm{down},i}^{\\ell+1},h_{\\mathrm{down},j}^{\\ell+1},z_{i j}^{\\ell}),\\quad z_{i j}^{\\prime}\\in\\mathbb{R}^{N\\times(2D_{h}+D_{z})}}\\\\ &{\\boldsymbol{z}^{\\ell+1}=\\operatorname{LayerNorm}(\\operatorname{MLP}(\\boldsymbol{z}^{\\prime})),\\quad z^{\\ell+1}\\in\\mathbb{R}^{N\\times N\\times D_{z}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Backbone update. Our frame updates follow the BackboneUpdate algorithm in AlphaFold2 [43]. We write the algorithm here with our notation, ", "page_idx": 17}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n(b_{i},c_{i},d_{i},x_{i}^{\\mathrm{update}})=\\mathrm{Linear}(h_{i}^{L}),\n$$$$\n\\begin{array}{r l r}&{}&{\\{b_{i},c_{i},d_{i},{x_{i}}^{\\prime}~~~~\\}=\\mathrm{Lmear}(h_{i}^{-}),}\\\\ &{}&{\\quad\\quad(a_{i},b_{i},c_{i},d_{i})=\\left(1,b_{i},c_{i},d_{i}\\right)/\\sqrt{1+b_{i}^{2}+c_{i}^{2}+d_{i}^{2}},\\quad\\quad\\quad}\\\\ &{}&{\\quad\\quad R_{i}^{\\mathrm{update}}=\\left(\\begin{array}{c c c}{a_{i}^{2}+b_{i}^{2}-c_{i}^{2}-d_{i}^{2}}&{2b_{i}c_{i}-2a_{i}d_{i}}&{2b_{i}d_{i}+2a_{i}c_{i}}\\\\ {2b_{i}c_{i}+2a_{i}d_{i}}&{a_{i}^{2}-b_{i}^{2}+c_{i}^{2}-d_{i}^{2}}&{2c_{i}d_{i}-2a_{i}b_{i}}\\\\ {2b_{i}d_{i}-2a_{i}c_{i}}&{2c_{i}d_{i}+2a_{i}b_{i}}&{a_{i}^{2}-b_{i}^{2}-c_{i}^{2}+d_{i}^{2}}\\end{array}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\nT_{i}^{\\mathrm{update}}=(R_{i}^{\\mathrm{update}},x_{i}^{\\mathrm{update}}),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\nT_{i}^{\\ell+1}=T_{i}^{\\ell}\\cdot T_{i}^{\\mathrm{update}},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $b_{i},c_{i},d_{i}\\,\\in\\,\\mathbb{R},x_{i}^{\\mathrm{update}}\\,\\in\\,\\mathbb{R}^{3}$ . Equ. 27 constructs a normalized quaternion which is then converted into a valid rotation matrix in Equ. 28. Following [84, 81], we use the planar geometry of the backbone to impute the oxygen atoms. Note that we only update the pocket and ligand nodes in PocketFlow while setting the scaffold nodes fixed. ", "page_idx": 18}, {"type": "text", "text": "Residue/Interaction Type and Torsion angle Prediction. We predict the residue/interaction types and sidechain torsion angles based on node embeddings. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{h_{c}=\\mathrm{MLP}(h^{L}),\\quad h_{I}=\\mathrm{MLP}(h^{L}),\\quad h_{\\chi}=\\mathrm{MLP}(h^{L}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\nc=\\operatorname{softmax}(\\operatorname{Linear}(h_{c}+h^{L})),I=\\operatorname{softmax}(\\operatorname{Linear}(h_{\\psi}+h^{L})),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\chi=\\mathrm{Linear}(h_{\\chi}+h^{L})\\mathrm{mod}2\\pi\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\pmb{c}\\in\\mathbb{R}^{N\\times{20}}$ , $I\\in\\mathbb{R}^{N,5}$ , and $x\\in[0,2\\pi)^{4N}$ . In PocketFlow, the number of network blocks is set to 8, the number of transformer layers within each block is set to 4, and the number of hidden channels used in the IPA calculation is set to 16. The node embedding size $D_{h}$ and the edge embedding size $D_{z}$ are set as 128. We removed skip connections and psi-angle prediction. For model training, we use Adam [45] optimizer with learning rate 0.0001, $\\beta_{1}=0.9$ , $\\beta_{2}=0.999$ . We train on a Tesla A100 GPU for 20 epochs. In the sampling process, the total number of steps $T$ is set as 50. ", "page_idx": 18}, {"type": "text", "text": "D Proof of Equivariance ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Theorem 1. Denote the $S E(3)$ -transformation as $T_{g}$ , PocketFLow $p_{\\theta}(\\mathcal{R},\\mathcal{G}|\\mathcal{P}\\setminus\\mathcal{R})$ is $S E(3)$ equivariant i.e., $p_{\\theta}(T_{g}(\\mathcal{R},\\mathcal{G})|T_{g}(\\mathcal{P}\\setminus\\mathcal{R}))=p_{\\theta}(\\mathcal{R},\\mathcal{G}|\\mathcal{P}\\setminus\\check{\\mathcal{R}})$ , where $\\mathcal{R}$ denotes the designed pocket, $\\mathcal{G}$ is the binding ligand, and $\\mathcal{P}\\setminus\\mathcal{R}$ is the protein scaffold. ", "page_idx": 18}, {"type": "text", "text": "Proof. The main idea is that the SE(3)-invariant prior and SE(3)-equivariant neural network lead to an SE(3)-equivariant generative process of PocketFlow. By subtracting the CoM of the scaffold from the initialized structure, we obtain an SE(3)-invariant prior distribution similar to [86, 29]. Moreover, the neural network for structure update as shown in Appendix $\\mathbf{C}$ is SE(3)-equivariant. Formally, the two conditions to guarantee an invariant likelihood $p_{\\theta}(\\mathcal{R}_{1},\\mathcal{G}_{1}\\vert\\mathcal{P}\\setminus\\mathcal{R})$ are as follows (we use subscripts to denote the time steps from $t=0$ to $t=1$ ): ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{ior:}}&{\\,p(\\mathcal{R}_{0},\\mathcal{G}_{0},\\mathcal{P}\\setminus\\mathcal{R})=p(T_{g}(\\mathcal{R}_{0},\\mathcal{G}_{0},\\mathcal{P}\\setminus\\mathcal{R})),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Equivariant Transition: $\\begin{array}{r}{p_{\\theta}(\\mathcal{R}_{t+\\Delta t},\\mathcal{G}_{t+\\Delta t}|\\mathcal{R}_{t},\\mathcal{G}_{t},\\mathcal{P}\\backslash\\mathcal{R})=p_{\\theta}\\big(T_{g}(\\mathcal{R}_{t+\\Delta t},\\mathcal{G}_{t+\\Delta t})\\big|T_{g}(\\mathcal{R}_{t},\\mathcal{G}_{t},\\mathcal{P}\\backslash\\mathcal{R})\\big),}\\end{array}$ ", "page_idx": 18}, {"type": "text", "text": "We can obtain the conclusion as follows: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathrm{\\partial}_{\\varphi}^{r}(\\mathcal{R}_{1},\\mathcal{G}_{1})\\vert T_{g}(\\mathcal{P}\\setminus\\mathcal{R}))=\\int p(T_{g}(\\mathcal{R}_{0},\\mathcal{G}_{0},\\mathcal{P}\\setminus\\mathcal{R}))\\prod_{s=0}^{T-1}p_{\\theta}(T_{g}(\\mathcal{R}_{(s+1)\\Delta t},\\mathcal{G}_{(s+1)\\Delta t})\\vert T_{g}(\\mathcal{R}_{s\\Delta t},\\mathcal{G}_{s\\Delta t},\\mathcal{G}_{s\\Delta t},\\mathcal{G}_{s\\Delta t}))}}\\\\ &{}&{=\\int p(\\mathcal{R}_{0},\\mathcal{G}_{0},\\mathcal{P}\\setminus\\mathcal{R})\\prod_{s=0}^{T-1}p_{\\theta}(T_{g}(\\mathcal{R}_{(s+1)\\Delta t},\\mathcal{G}_{(s+1)\\Delta t})\\vert T_{g}(\\mathcal{R}_{s\\Delta t},\\mathcal{G}_{s\\Delta t},\\mathcal{P}\\setminus\\mathcal{R}))}\\\\ &{}&{=\\int p(\\mathcal{R}_{0},\\mathcal{G}_{0},\\mathcal{P}\\setminus\\mathcal{R})\\prod_{s=0}^{T-1}p_{\\theta}(\\mathcal{R}_{(s+1)\\Delta t},\\mathcal{G}_{(s+1)\\Delta t}\\vert\\mathcal{R}_{s\\Delta t},\\mathcal{G}_{s\\Delta t},\\mathcal{P}\\setminus\\mathcal{R})}\\\\ &{}&{=p_{\\theta}(\\mathcal{R}_{1},\\mathcal{G}_{1}\\vert\\mathcal{P}\\setminus\\mathcal{R}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $T$ is the total number of steps. We apply the invariant prior and equivariant transition conditions in the derivation. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "E Classifier-guided Flow Matching ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Here, we present the Bayesian approach to guide the flow matching with the affinity predictor. The key insight comes from connecting flow matching to diffusion models to which affinity guidance can be applied. Sampling a data point from the prior distribution $p_{0}$ , we have the following ordinary differential equation (ODE) [72] that pushes it to data distribution: ", "page_idx": 19}, {"type": "equation", "text": "$$\nd\\mathcal{C}_{t}=v(\\mathcal{C}_{t},t)d t=\\left[f(\\mathcal{C}_{t},t)-\\frac{1}{2}g(t)^{2}\\nabla\\log p_{t}(\\mathcal{C}_{t})\\right]d t,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\nabla\\log{p_{t}(\\mathcal{C}_{t})}$ is the score function, $f(\\mathcal{C}_{t},t)$ and $g(t)$ are the drift and diffusion coefficients respectively. We modify Equ. 36 to be conditioned on the affinity label ${v}_{b}=1$ ) followed by an application of Bayes rule, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d\\mathcal{C}_{t}=\\bigg[f(\\mathcal{C}_{t},t)-\\frac{1}{2}g(t)^{2}\\nabla\\log p_{t}(\\mathcal{C}_{t}|v_{b}=1)\\bigg]\\,d t}\\\\ &{\\quad\\quad=\\bigg[f(\\mathcal{C}_{t},t)-\\frac{1}{2}g(t)^{2}\\left(\\nabla\\log p_{t}(\\mathcal{C}_{t})+\\nabla\\log p_{t}(v_{b}=1|\\mathcal{C}_{t})\\right)\\bigg]\\,d t}\\\\ &{\\quad\\quad=\\bigg[v(\\mathcal{C}_{t},t)-\\frac{1}{2}g(t)^{2}\\nabla\\log p_{t}(v_{b}=1|\\mathcal{C}_{t})\\bigg]\\,d t,}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the first term is the unconditional vector field and the second term is the affinity guidance term. In practice, we do not directly predict the affinity label based on $\\ensuremath{\\mathcal{C}}_{t}$ because the intermediate structure is noisy. We use the following transformation: ", "page_idx": 19}, {"type": "equation", "text": "$$\np_{t}(v_{b}=1|\\mathcal{C}_{t})=\\int p(v_{b}=1|\\mathcal{C}_{1})p(\\mathcal{C}_{1}|\\mathcal{C}_{t})d\\mathcal{C}_{t}\\approx p(v_{b}=1|\\hat{\\mathcal{C}}_{1}(\\mathcal{C}_{t})),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\hat{\\mathcal{C}}_{1}(\\mathcal{C}_{t})$ is the expected denoised protein-ligand complex structure based on $\\ensuremath{\\mathcal{C}}_{t}$ . Details of the affinity predictor is introduced in the Appendix. E.1. We need to choose $g(t)$ such that it matches the learned probability path. Previous works [20, 86] showed $\\begin{array}{r}{g(t)^{2}=\\frac{t}{1-t}}\\end{array}$ in the Euclidean setting. For simplicity, we set $g(t)$ as constant 1 and observe good performance in experiments. ", "page_idx": 19}, {"type": "text", "text": "E.1 Binding Affinity Predictor ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In PocketFlow, we leverage a binding affinity predictor $p_{\\theta}(v_{b}|\\hat{\\mathcal{C}}_{1}(\\mathcal{C}_{t}))$ to guide the denoising process, where $v_{b}\\in\\{0,1\\}$ is the binary label of binding affinity and $\\hat{\\mathcal{C}}_{1}(\\mathcal{C}_{t})$ is the expected protein-ligand structure at $t=1$ . Following [66, 29], we leverage a 3-layer EGNN [70] with the node initialized embeddings and residue/ligand atom coordinates from Appendix C. Specifically, we take the $C_{\\alpha}$ coordinates for the residues and ligand atom coordinates and construct $k$ -NN graphs ( $k$ set as 9). Let $h_{i}^{\\ell}$ and $x_{i}^{\\ell}$ be denote the node representations and coordinates at the $\\ell-$ th layer. The $(\\ell+1)-$ th layer is computed as follows: ", "page_idx": 19}, {"type": "equation", "text": "$$\nh_{i}^{\\ell+1}=h_{i}^{\\ell}+\\sum_{j\\in\\mathcal{N},i\\neq j}f_{h}(d_{i j}^{\\ell},h_{i}^{\\ell},h_{j}^{\\ell},e_{i j}),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\nx_{i}^{\\ell+1}=x_{i}^{\\ell}+\\sum_{j\\in\\mathcal{N},i\\neq j}(x_{i}^{\\ell}-x_{j}^{\\ell})f_{x}(d_{i j}^{\\ell},x_{i}^{\\ell},x_{j}^{\\ell},e_{i j}),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $d_{i j}^{\\ell}=\\|x_{i}^{\\ell}-x_{j}^{\\ell}\\|$ represents the Euclidean distance between node $i$ and node $j$ at the $\\ell_{}$ -th layer, $\\mathcal{N}$ denotes the $k$ -NN neighbors, and $e_{i j}$ indicates the direction of message-passing, including from protein to protein, from protein to ligand, from ligand to protein, and from ligand to ligand. The functions $f_{h}$ and $f_{x}$ are graph attention networks. Finally, we append an average pooling, one linear layer, and softmax operation at the end to predict the binary label of affinity. ", "page_idx": 19}, {"type": "text", "text": "To train the binding affinity predictor, we first annotate the data points in the corresponding training set: data points are annotated 1 if their affinity is higher than the average score of the dataset, otherwise 0. We train the predictor separately instead of joint training with flow matching because we find it can converge more quickly than the flow matching losses. We did not train the predictor on the intermediate structures as we find they are noisy and deteriorate the predictor and PocketFlow\u2019s overall performance. In experiments, we use the Adam optimizer and train for 10 epochs. ", "page_idx": 19}, {"type": "text", "text": "E.2 Geometry Guidance ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Distance Guidance. For hydrogen bonds, the distances between donor and acceptor atoms need to be less than $4.1\\textup{\\AA}$ and larger than $2\\mathrm{~\\AA~}$ to reduce steric clashes [35]. The following inequality is a necessary condition for residues in $\\hat{\\mathcal{C}}_{1}(\\mathcal{C}_{t})$ with predicted interaction label $\\hat{I}_{1}$ as hydrogen bond: ", "page_idx": 20}, {"type": "equation", "text": "$$\nl_{\\operatorname*{min}}\\le\\operatorname*{min}_{\\substack{i\\in A_{h b o n d}^{(k)},j\\in\\mathcal{G}}}\\left\\|\\pmb{x}^{(i)}-\\pmb{x}^{(j)}\\right\\|_{2}\\le l_{\\operatorname*{max}},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $l_{\\mathrm{min}}$ and $l_{\\mathrm{max}}$ are distance constraints; $\\mathcal{A}_{h b o n d}^{(k)}$ denote the $k$ -th residue in the set of pocket residues with predicted hydrogen bonds. With a little abuse of notations, $\\pmb{x}^{(i)}$ and $\\pmb{x}^{(j)}$ denote the atom coordinates in the residue and ligand respectively. We use the following derivations to obtain the guidance term for the distance constraints: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{T}_{c_{t}}\\log P(\\{l_{\\operatorname*{min}}\\leq\\underset{i\\in\\mathcal{A}_{h b o n d}^{(k)},j\\in\\mathcal{G}}{\\operatorname*{min}}\\|x^{(i)}-x^{(j)}\\|_{2}\\leq l_{\\operatorname*{max}},k=1:|A_{h b o n d}|\\})}\\\\ &{=\\nabla_{c_{t}}\\overset{|A_{h b o n d}|}{\\underset{k=1}{\\sum}}\\log P(l_{\\operatorname*{min}}\\leq\\underset{i\\in\\mathcal{A}_{h b o n d}^{(k)},j\\in\\mathcal{G}}{\\operatorname*{min}}||x^{(i)}-x^{(j)}\\|_{2}\\leq l_{\\operatorname*{max}})}\\\\ &{=\\underset{k=1}{\\overset{|A_{h b o n d}|}{\\sum}}\\frac{\\nabla_{C_{t}}[P(-\\operatorname*{min}_{i\\in\\mathcal{A}_{h b o n d}^{(k)},j\\in\\mathcal{G}}||x^{(i)}-x^{(j)}||_{2}\\leq-l_{\\operatorname*{min}})\\cdot P(\\operatorname*{min}_{i\\in\\mathcal{A}_{h b o n d}^{(k)},j\\in\\mathcal{G}}||x^{(i)}-x^{(j)}||_{2}}{P(l_{\\operatorname*{min}}\\leq\\operatorname*{min}_{i\\in\\mathcal{A}_{h b o n d}^{(k)},j\\in\\mathcal{G}}||x^{(i)}-x^{(j)}||\\leq l_{\\operatorname*{max}})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\n=\\sum_{k=1}^{|A_{h b o n d}|}\\xi_{1}\\nabla_{C_{t}}P(\\operatorname*{min}_{\\substack{i\\in A_{h b o n d}^{(k)},j\\in\\mathcal{G}}}\\|x^{(i)}-x^{(j)}\\|_{2}\\leq l_{\\operatorname*{max}})+\\xi_{2}\\nabla_{x_{t}}P(-\\operatorname*{min}_{\\substack{i\\in A_{h b o n d}^{(k)},j\\in\\mathcal{G}}}\\|x^{(i)}-x^{(j)}\\|_{2}\\leq l_{\\operatorname*{max}})\\,.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\n=\\sum_{k=1}^{|A_{h b o n d}|}\\xi_{1}\\nabla_{C_{t}}\\mathbb{I}(\\operatorname*{min}_{i\\in A_{h b o n d}^{(k)},j\\in\\mathcal{G}}\\|x^{(i)}-x^{(j)}\\|_{2}\\leq l_{\\operatorname*{max}})+\\xi_{2}\\nabla_{C_{t}}\\mathbb{I}(-\\operatorname*{min}_{i\\in A_{h b o n d}^{(k)},j\\in\\mathcal{G}}\\|x^{(i)}-x^{(j)}\\|_{2}\\leq C_{t}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\begin{array}{r l r}{\\xi_{1}}&{{}~=~}&{1/\\nabla_{\\mathcal{C}_{t}}P(\\operatorname*{min}_{i\\in A_{h b o n d}^{(k)},j\\in\\mathcal{G}}\\|x^{(i)}\\-\\ -\\ x^{(j)}\\|_{2}\\quad\\leq\\quad l_{\\operatorname*{max}})}\\end{array}$ and $\\xi_{2}\\quad=$ $\\begin{array}{r}{1/P(-\\operatorname*{min}_{i\\in{\\mathcal A}_{h b o n d}^{(k)},j\\in{\\mathcal G}}\\|x^{(i)}\\-x^{(j)}\\|_{2}\\ \\leq\\ -l_{\\operatorname*{min}})}\\end{array}$ . Due to the discontinuity of the indicator function $\\mathbb{I}(\\cdot)$ that is incompatible with the gradient, we apply $\\xi-\\operatorname*{max}(0,\\xi-y)$ as a surrogate of $\\mathbb{I}(y<\\xi)$ in the above equation. Although $\\xi_{1}$ and $\\xi_{2}$ are dependent on $\\ensuremath{\\mathcal{C}}_{t}$ , we find setting them as constant still works well in experiments. With these approximations, we can derive guidance term for hydrogen bond distance constraints: ", "page_idx": 20}, {"type": "equation", "text": "$$\n-\\nabla_{\\mathcal{C}_{t}}\\sum_{k=1}^{\\left|\\mathcal{A}_{h b o n d}\\right|}\\left[\\xi_{1}\\operatorname*{max}\\left(0,d^{\\left(k\\right)}-l_{\\operatorname*{max}}\\right)+\\xi_{2}\\operatorname*{max}\\left(0,l_{\\operatorname*{min}}-d^{\\left(k\\right)}\\right)\\right],\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "iwntheerraec $\\begin{array}{r}{d^{(k)}\\,=\\,\\operatorname*{min}_{i\\in{\\mathcal A}_{h b o n d}^{(k)},j\\in{\\mathcal G}}\\left\\|{\\pmb x}^{(i)}-{\\pmb x}^{(j)}\\right\\|_{2}}\\end{array}$ r.e  sSiumcihla rd.i sTtahne cdei ffgeuriednacnec ies  tteor rmesp lfaocre owbitihc $\\pi-\\pi$ $A_{h b o n d}$ $\\mathcal{A}_{h y d r o}$ , $A_{s a l t}$ , and $A_{\\pi}$ that denotes the residue sets with corresponding interactions. We modify the functions in plip 2 for the ease of detecting interaction atom pair candidates. In practice, $\\nabla_{\\boldsymbol{\\mathcal{C}}_{t}}$ takes gradients with each component in $\\ensuremath{\\mathcal{C}}_{t}$ , including $\\boldsymbol{\\chi}_{t},\\boldsymbol{x}_{t},O_{t},c_{t}$ , and $I_{t}$ . ", "page_idx": 20}, {"type": "text", "text": "Angle Guidance. Besides the distance constraint, the hydrogen bond needs to satisfy the acceptor/donor angle constraint [69], e.g., the donor/acceptor angle needs to be larger than $100^{\\circ}$ . hangle $\\bar{(\\cdot,\\cdot)}$ ", "page_idx": 20}, {"type": "text", "text": "calculates the acceptor/donor angle in Figure. 4. ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{c_{t}}\\log P(\\{\\alpha_{\\operatorname*{min}}\\leq\\underset{i\\in A_{h b o n d}^{(k)}\\not\\in\\mathcal{G}}{\\operatorname*{max}}\\mathrm{hangle}(x^{(i)},x^{(j)}),k=1:|A_{h b o n d}|\\})}\\\\ &{=\\nabla_{c_{t}}\\underset{k=1}{\\overset{|A_{h b o n d}|}{\\sum}}\\log P(\\alpha_{\\operatorname*{min}}\\leq\\underset{i\\in A_{h b o n d}^{(k)}\\not\\in\\mathcal{G}}{\\operatorname*{max}}{\\operatorname*{hangle}}(x^{(i)},x^{(j)}))}\\\\ &{=\\underset{k=1}{\\overset{|A_{h b o n d}|}{\\sum}}\\frac{\\nabla_{c_{t}}P(\\alpha_{\\operatorname*{min}}\\leq\\operatorname*{max}_{i\\in A_{h b o n d}^{(k)}\\not\\in\\mathcal{G}}{\\operatorname*{hangle}}(x^{(i)},x^{(j)}))}{P(\\alpha_{\\operatorname*{min}}\\leq\\operatorname*{max}_{i\\in A_{h b o n d}^{(k)}\\not\\in\\mathcal{G}}{\\operatorname*{hangle}}(x^{(i)},x^{(j)}))}}\\\\ &{=\\underset{k=1}{\\overset{|A_{h b o n d}|}{\\sum}}\\xi_{3}\\nabla_{c_{t}}P(\\alpha_{\\operatorname*{min}}\\leq\\underset{i\\in A_{h b o n d}^{(k)}\\not\\in\\mathcal{G}}{\\operatorname*{max}}{\\operatorname*{hangle}}(x^{(i)},x^{(j)})),}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "$\\begin{array}{r}{\\xi_{3}=1/P(\\alpha_{\\mathrm{min}}\\le\\operatorname*{max}_{i\\in{A_{h b o n d}^{(k)},j\\in\\mathcal{G}}}\\mathrm{hangle}(x^{(i)},x^{(j)}))}\\end{array}$ . The final guidance term is: ", "page_idx": 21}, {"type": "equation", "text": "$$\n-\\xi_{3}\\nabla_{x_{t}}\\sum_{k=1}^{|\\mathcal{A}_{h b o n d}|}\\operatorname*{max}(0,\\alpha_{\\operatorname*{min}}-\\phi^{(k)}),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where \u03d5(k) = maxi\u2208A(hkb)ond,j\u2208G $\\phi^{(k)}=\\operatorname*{max}_{i\\in A_{h b o n d}^{(k)},j\\in\\mathcal{G}}\\mathrm{hangle}(\\pmb{x}^{(i)},\\pmb{x}^{(j)})$ . The angle constraint is similar for the $\\pi-\\pi$ stacking and the final guidance term is: ", "page_idx": 21}, {"type": "equation", "text": "$$\n-\\xi_{4}\\nabla_{\\mathcal{C}_{t}}\\sum_{k=1}^{|\\mathcal{A}_{\\pi}|}\\operatorname*{max}(0,\\phi_{\\pi}^{(k)}-\\alpha_{\\operatorname*{max}}),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "iwn hFeirge $\\phi_{\\pi}^{(k)}=\\operatorname*{min}_{i\\in\\mathcal{A}_{\\pi}^{(k)},j\\in\\mathcal{G}}$ npsi aanngdl ec(axl(ciu)l, axti(oj)n)s  aunsde dp iian nggeleo $(\\cdot,\\cdot)$ yc aglcuiudlaatnecs et haer $\\pi-\\pi$ sdtaifcfkeirnegn taianbgllee and can be plugged into the sampling process of PocketFlow. ", "page_idx": 21}, {"type": "image", "img_path": "WyVTj77KEV/tmp/5a325d4871dffd1fd6ea10960d9efe30cb6c50f3213de88dcbeb651a2233f681.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Figure 6: Superposition of 20 residue-type sidechains. When calculating the geometry guidance, we use the expected sidechain conformations with respect to the estimated residue type probability $\\hat{\\pmb{c}}_{1}^{(i)}$ to avoid the non-differentiability issue of residue type sampling. ", "page_idx": 21}, {"type": "text", "text": "Sidechain Ensemble. PocketFlow takes the co-design scheme, where the residue type/side chain structure of the pocket is not determined during sampling. Directly sampling from the residue type distribution makes the model not differentiable [38]. We propose to use the sidechain ensemble for the interaction geometry calculation, i.e., the weighted sum of geometric guidance with respect to residue types. For example, for Equ. 54, we have: ", "page_idx": 21}, {"type": "equation", "text": "$$\n-\\xi_{3}\\nabla_{\\mathcal{C}_{t}}\\sum_{k=1}^{|\\mathcal{A}_{h b o n d}|}\\sum_{n=1}^{20}\\hat{c}_{1}^{(i)}[n]\\cdot\\operatorname*{max}(0,\\alpha_{\\operatorname*{min}}-\\phi^{(k)}),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\hat{\\pmb{c}}_{1}^{(i)}[n]$ denote the $n$ -th residue type probability and $\\phi^{(k)}$ calculates the angle with the $n$ -th type residue side chain. ", "page_idx": 21}, {"type": "image", "img_path": "WyVTj77KEV/tmp/d1cb7713f609e9a3924a123a2b74d1c9d06fd6af980e7d7514d1b5a8605248a9.jpg", "img_caption": ["Figure 7: Average Generation time for 100 pockets by different models on CrossDocked (the error bars show the standard deviations over different runs). "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "WyVTj77KEV/tmp/26e079ac5cd940ffd6bf9fb8d8983c164e22cb805bba20bee11f59fe71631821.jpg", "img_caption": ["Figure 8: The influence of Affinity Guidance Strength $\\gamma$ on the pocket metrics. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "F More Results ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Here, we show additional results on efficiency analysis (Figure. 7), hyperparameter analysis of $\\gamma$ (Figure. 8), and ablation studies on the interaction analysis (Table. 6). ", "page_idx": 22}, {"type": "text", "text": "Figure. 7 shows that the PocketFlow is much more efficient than stat-of-the-art diffusion-based models such as RFDiffusionAA. Considering the high quality of the generated pockets, the slight time overhead over models based on iterative refinement (e.g., dyMEAN and FAIR) is acceptable. We find that Affinity and Interaction Geometry Guidance do not add much overhead to the generation process. Therefore, these prior guidance are efficient tools for pocket optimization. ", "page_idx": 22}, {"type": "text", "text": "In Figure. 8, we explore the impact of Affinity Guidance Strength $(\\gamma)$ on various generation metrics. As $\\gamma$ is scaled up, the Vina Score significantly improves and quickly stabilizes; AAR initially increases before gradually decreasing; scRMSD, on the other hand, increases with higher $\\gamma$ . These observations underscore the importance of selecting an appropriate $\\gamma$ to effectively balance the guidance and unconditional terms. While Affinity Guidance promotes the generation of high-affinity pockets, an excessively high $\\gamma$ can result in less valid pocket sequences or structures. In the default configuration, $\\gamma$ is set to 1. ", "page_idx": 22}, {"type": "text", "text": "To evaluate the validity of the generated sidechain structure, we compute the Mean Absolute Error (MAE) of sidechain angles (degrees) following [90] in Table. 5. We mainly compare PocketFlow with RFDiffusionAA [48] $^+$ LigandMPNN [22] on the recovered residues. In the table, we report the average MAE and can observe that PocketFlow achieves better performance in generating valid sidechain structures. ", "page_idx": 22}, {"type": "table", "img_path": "WyVTj77KEV/tmp/4fefdec1ceaa2889eebcb6746fe0d5e7e4aa6ac1f69933a44144d8c8d9266857.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "Table 5: The MAE of RFDiffusionAA $^+$ LigandMPNN and PocketFlow on sidechain torsion angles(degrees). ", "page_idx": 22}, {"type": "text", "text": "In Table. 6, we supplement further results of interaction analysis (Table. 3 in the main paper). We can observe that the guidance terms effectively improve the number of favorable interactions while reducing steric clashes, which lay the foundation for generating high-affinity pockets. ", "page_idx": 23}, {"type": "table", "img_path": "WyVTj77KEV/tmp/8cec0f36aad75d9d955939b87a243d9e5ccfee499d49e71693eb976e81b7cfb1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "Table 6: Ablation studies on the interaction analysis. The best results are bolded and the runner-up is underlined. ", "page_idx": 23}, {"type": "text", "text": "G Baseline Implementation ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "DEPACT [19] 3 is a template-matching method that follows a two-step strategy for pocket design. Firstly, it searches the protein-ligand complexes in the template database with similar ligand fragments and constructs a cluster model (a set of pocket residues). The template databases are constructed based on the corresponding training datasets for fair comparisons. Secondly, it grafts the cluster model into the protein pocket with PACMatch. It works by placing residues from the cluster model on protein scaffolds by matching the atoms of residues with atoms of the protein scaffold. The backbone coordinates of the pocket residues are also modified in the process. The qualities of the generated pockets are evaluated and ranked based on a statistical scoring function. We take the top 100 designed pockets for evaluation. The output of DEPACT $^{\\ast}$ PACMatch is complete protein structures with redesigned pockets. In the paper, we only use DEPACT to represent the whole method of DEPACT $^+.$ PACMatch for conciseness. ", "page_idx": 24}, {"type": "text", "text": "RFDiffusionAA [48] 4 is the latest version of RFDiffusion which combines a residue-based representation of amino acids and atomic representations of all other groups to model protein-small molecules/metals/nucleic acids/covalent modification complexes. Starting from random distributions of amino acid residues surrounding target small molecules, RFDiffusionAA can directly generate the small molecule binding protein backbone. Furthermore, with LigandMPNN [22], the latest version of ProteinMPNN[21], we can assign residue types and predict sidechain conformations considering the protein-ligand interactions. Experiments in RFDiffusionAA [48] show that the generated protein by RFDiffusionAA has better binding affinity than those obtained by RFDiffusion with auxiliary potential. We use the provided checkpoints of RFDiffusionAA for all the experiments since the training code is unavailable. ", "page_idx": 24}, {"type": "text", "text": "dyMEAN [47] 5 is an end-to-end full-atom model for E(3)-equivariant antibody design given the epitope and the incomplete sequence of the antibody. Its previous version, MEAN [46], only considers the backbone atoms, while dyMEAN considers the complete atom structure and performs better on downstream tasks. Generally, dyMEAN co-designs antibody sequence and structure via a multi-round progressive full-shot refinement manner, which is more efficient than auto-regressive or diffusionbased approaches. An adaptive multi-channel equivariant encoder is used in dyMEAN, which can process protein residues of variable sizes when considering full atoms. To adapt dyMEAN to our pocket design task, we replace the antigen with the target ligand molecule to provide the context information for pocket generation. We set the hidden size as 128, the number of layers as 3, and the number of iterations for decoding as 3. ", "page_idx": 24}, {"type": "text", "text": "FAIR [92] 6 is our previous method for full atom pocket sequence-structure co-design. FAIR operates in two steps, proceeding in a coarse-to-fine manner (backbone refinement to full atoms refinement, including side chains) for full-atom generation. In FAIR, residue types and atom coordinates are updated using a hierarchical graph transformer composed of a residue-level and atom-level encoder. The number of layers for the atom and residue-level encoder are 6 and 2, respectively. $K_{a}$ and $K_{r}$ are set as 24 and 8 respectively. The number of attention heads is set as 4; The hidden dimension $d$ is set as 128. ", "page_idx": 24}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: In the abstract and introduction, we clearly state the contributions of our paper. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 25}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: In Sec.5.5, we clearly describe the limitations of the work and the potential ways to reduce the limitations in future works. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 25}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The proofs of the theorems are included in the paper. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 26}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: The code will be included in https://github.com/zaixizhang/ PocketFlow. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 26}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We clearly discussed the training datasets and other details. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 27}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper specify all the training and test details. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 27}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: We provided the standard deviations of the results in the paper. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper provides sufficient information on the computer resources. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 28}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The research in this paper conforms in every respect with the NeurIPS Code of Ethics. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 28}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper discussed potential societal impacts. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 29}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: The existing assets are properly cited and credited. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 29}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 30}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 30}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 30}]