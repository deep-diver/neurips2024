[{"type": "text", "text": "Unified Gradient-Based Machine Unlearning with Remain Geometry Enhancement ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Zhehao Huang, Xinwen Cheng, JingHao Zheng, Haoran Wang, Zhengbao He, Tao Li, Xiaolin Huang ", "page_idx": 0}, {"type": "text", "text": "Shanghai Jiao Tong University [kinght_H, xinwencheng, zjh20030406, haoran_whynot, lstefanie, li.tao, xiaolinhuang]@sjtu.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Machine unlearning (MU) has emerged to enhance the privacy and trustworthiness of deep neural networks. Approximate MU is a practical method for large-scale models. Our investigation into approximate MU starts with identifying the steepest descent direction, minimizing the output Kullback-Leibler divergence to exact MU inside a parameters\u2019 neighborhood. This probed direction decomposes into three components: weighted forgetting gradient ascent, fine-tuning retaining gradient descent, and a weight saliency matrix. Such decomposition derived from Euclidean metric encompasses most existing gradient-based MU methods. Nevertheless, adhering to Euclidean space may result in sub-optimal iterative trajectories due to the overlooked geometric structure of the output probability space. We suggest embedding the unlearning update into a manifold rendered by the remaining geometry, incorporating second-order Hessian from the remaining data. It helps prevent effective unlearning from interfering with the retained performance. However, computing the second-order Hessian for large-scale models is intractable. To efficiently leverage the beneftis of Hessian modulation, we propose a fast-slow parameter update strategy to implicitly approximate the up-to-date salient unlearning direction. Free from specific modal constraints, our approach is adaptable across computer vision unlearning tasks, including classification and generation. Extensive experiments validate our efficacy and efficiency. Notably, our method successfully performs class-forgetting on ImageNet using DiT and forgets a class on CIFAR-10 using DDPM in just 50 steps, compared to thousands of steps required by previous methods. Code is available at Unified-Unlearning-w-Remain-Geometry. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Machine Unlearning (MU) [1\u20133] aims to remove the influence of samples from a pre-trained model, ensuring the model behaves as if it has never encountered those samples. The significance of MU research has grown following data protection regulations [4, 5]. It has rapidly developed in recent years, becoming an important means to help pre-trained large-scale models adapt to various trustworthy challenges [6] in computer vision (CV). In general, MU aids in purging outdated knowledge [7, 8], mitigating biases [9, 10], and preventing large text-to-image models from generating not-safe-for-work (NSFW) images [11\u201314]. ", "page_idx": 0}, {"type": "text", "text": "Existing MU methods are mainly divided into two categories: exact (or certified) MU [15] and approximate MU [16]. For deep neural networks, achieving exact MU necessitates retraining on a dataset excluding forgetting samples. However, retraining is computationally prohibitive for recent large-scale deep networks. Thus, in MU for deep networks, this retrained model only serves as an aspirational standard to be approached [17]. We primarily focus on more efficient approximate MU. ", "page_idx": 0}, {"type": "text", "text": "Approximate MU strives to align the output distribution of unlearned models with that of retrained models. We initially explore the vanilla gradient descent of minimizing the output Kullback-Leibler (KL) divergence between the current iteration and the retrained model. Our deduction reveals that this direction consists of three parts: a weighted gradient ascent to eliminate the influence of forgetting samples, a descent gradient for fine-tuning the remaining set, and a weight saliency matrix modulating the unlearning direction. Such decomposition provides a novel perspective that unifies previous MU approaches proposed in recent years [18\u201325], most of which only focus on one or two of these components. For example, SalUn [26] insightfully proposes saliency-based unlearning, which only optimizes the parameters important to forget, but lacks theoretical support. Our analysis flils this gap and provides new directions for further improvement. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In fact, the vanilla gradient descent for approximate MU actually pursues the steepest descent under Euclidean distance [27\u201330]. However, constraining parameter updates within an Euclidean region is arbitrary, as it treats the importance of all parameters equally. Recent research indicates deep models\u2019 parameters and training processes are embedded in a low-dimensional manifold [31, 32]. Thus, there exist manifolds where changes in Euclidean between parameters are drastic, yet the output space remains unchanged. In MU, it is evident that the importance of model parameters varies for forgetting and remaining [33, 26], prompting the following question. Can model parameters be embedded in a manifold that allows effective forgetting while efficiently maintaining remaining performance? ", "page_idx": 1}, {"type": "text", "text": "To achieve this goal, we propose to discover the descent direction under the KL divergence on the remaining output distribution. Using such a manifold metric, the forgetting direction can be amended by a second-order Hessian on the remaining set to prevent forgetting loss from harming retained performance. Such iterative direction is dominated by the unlearning function, allowing the optimization process to focus on efficient forgetting. However, computing the secondorder Hessian for large-scale models is computationally intensive, contradicting the need for efficiency in unlearning. Existing methods for estimating the second-order Hessian rely ", "page_idx": 1}, {"type": "image", "img_path": "dheDf5EpBT/tmp/1962b80bd03fd9e1e3a6583dafa72ad4fa08988f8485b8a8b1479464d7e1ea54.jpg", "img_caption": ["Figure 1: Overview of our proposal vs. previous unlearning methods on erasing concept \u2018nudity\u2019 in diffusion models [11, 12]. Conventional methods seek the steepest descent within an Euclidean ball, often compromising general capabilities. In contrast, we reach the region around retraining along a remain-preserving manifold. To address the large cost of Hessian, we implicitly approximate the up-to-date salient unlearning direction. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "on initial parameters and keep them fixed thereafter [34, 35]. Therefore, we propose a fast-slow weight [36, 37] method (Fig. 1) to implicitly and dynamically approximate the salient forgetting direction with Hessian modulation, forming a unified MU approach for CV unlearning tasks including image classification and image generation. Key contributions of this paper include: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We provide a novel perspective to unify previous approaches by decomposing the vanilla gradient descent direction of approximate MU into three components: weighted forgetting gradient ascent, remaining gradient descent, and a weight saliency matrix. ", "page_idx": 1}, {"type": "text", "text": "\u2022 We derive the steepest descent direction for approximate MU on the remain-preserved manifold. ", "page_idx": 1}, {"type": "text", "text": "\u2022 We propose a fast-slow weight method to implicitly approximate online Hessian-modulated salient forgetting updates. ", "page_idx": 1}, {"type": "text", "text": "\u2022 We conduct experiments on a wide range of CV unlearning tasks across multiple datasets and models of different architectures, verifying the effectiveness and efficiency of our method. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminary ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Problem Setup. MU aims to help a well-trained model eliminate the influence of specific data points, categories, or high-level concepts and patterns [1, 38]. Let $\\mathcal{D}\\,=\\,\\{z_{i}\\}_{i=1}^{N}$ represent a pretraining dataset of $N$ data points, including $z_{i}\\,=\\,(x_{i},y_{i})$ features and labels in supervised learning. The for getting datraset, Df = {zif }iN=f1 \u2282 D, is a subset of the pretrained dataset. Its complement, $\\mathcal{D}^{r}=\\{z_{i}^{r}\\}_{i=1}^{N^{r}}=\\mathcal{D}\\backslash\\mathcal{D}^{f}$ , is the remaining dataset that we wish to retain. The learner is a model parameterized by $\\theta$ . $\\dot{p_{z}}(\\theta)=p(z;\\theta)$ represents the model output probability. The pre-trained model is obtained by empirical risk minimization, i.e., $\\begin{array}{r}{\\theta_{0}=\\arg\\operatorname*{min}_{\\theta}\\mathcal{L}(\\theta;\\mathcal{D})=\\arg\\operatorname*{min}_{\\theta}\\sum_{i\\in\\mathcal{D}}\\ell(\\theta;z_{i})}\\end{array}$ , as Pretrain. This empirical risk can be divided into two parts based on the forgetting and retaining datasets. $\\begin{array}{r}{\\mathcal{L}(\\boldsymbol{\\theta};\\mathcal{D})=\\sum_{i\\in\\mathcal{D}^{f}}\\varepsilon_{i}\\ell(\\boldsymbol{\\theta};\\boldsymbol{z}_{i}^{f})+\\sum_{j\\in\\mathcal{D}^{r}}\\ell(\\boldsymbol{\\theta};\\boldsymbol{z}_{i}^{r})=\\mathcal{L}^{f}(\\boldsymbol{\\theta};\\boldsymbol{\\varepsilon})+\\mathcal{L}^{r}(\\boldsymbol{\\theta})}\\end{array}$ , where $\\varepsilon=\\{\\varepsilon_{i}\\}_{i=1}^{N_{f}}$ weight the former part [39, 38]. For the pre-trained model, the coefficients $\\varepsilon_{0}=1$ are all ones. The following are the instantiations in CV unlearning tasks. In classification (Cls) problems, models output the class posterior probability $p(z_{\\mathrm{Cls}};\\theta)=p(\\bar{y}|x;\\theta)$ and the empirical risk for each sample is the cross-entropy (CE) loss $\\ell_{\\mathrm{Cls}}(\\boldsymbol{\\theta};z)=\\ell_{\\mathrm{CE}}(\\boldsymbol{\\theta};x,y)$ . In the image generation (Gen) task of conditional diffusion models [40\u201342], the output is the conditional sampling probability $p\\big(z_{\\mathrm{Gen}};\\theta\\big)=p(x|y;\\theta)$ , and the average loss function for each sample is the mean squared error (MSE) loss: $\\begin{array}{r}{\\ell_{\\mathrm{Gen}}(\\theta;z)=}\\end{array}$ $\\ell_{\\mathrm{MSE}}(\\theta;x,y)\\ {=}\\,\\mathbb{E}_{t,\\epsilon\\sim\\mathcal{N}(0,1)}[\\|\\epsilon-\\epsilon_{\\theta}(x_{t}|y)\\|_{2}^{2}]$ , where $t$ represents the diffusion step, $\\epsilon$ is the random noise sampled from a Gaussian distribution ${\\mathcal{N}}(0,1)$ , $\\epsilon_{\\theta}$ is the conditional denoising model, and $x_{t}$ is the noisy version of the input image. For a more detailed introduction to image generation using conditional diffusion models and latent diffusion models, please refer to Appendix B.3. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Exact and Approximate Machine Unlearning. Exact MU [15, 43, 1] ensures that the parameter distribution of the unlearned model is identical to that of a model trained from scratch without seeing the forgetting samples. For large-scale models, exact unlearning can only be achieved by retraining $(\\mathbf{R}\\mathbf{T})$ on the remaining dataset $\\theta_{*}=\\arg\\operatorname*{min}_{\\theta}\\mathcal{L}^{r}(\\theta)$ . However, the computational cost of retraining in response to every forgetting request is prohibitive. Therefore, we regard RT as a gold standard to approximate rather than a competitor. A more practical approach is to guide the unlearned model output distribution to approximate the output distribution of RT, known as approximate $M U$ [16]. If we use KL divergence to measure the difference in output distributions, the objective of approximate unlearning can be expressed as: min\u03b8 $\\begin{array}{r}{D_{\\mathrm{KL}}(p_{z}(\\theta_{*})||\\dot{p_{z}}(\\theta))=\\operatorname*{min}_{\\theta}\\int p_{z}(\\theta_{*})\\mathrm{\\dot{\\log}}[p_{z}(\\theta_{*})\\dot{/}\\dot{p}_{z}(\\theta)]\\mathrm{d}\\mathcal{D},}\\end{array}$ starting from $\\theta_{0}$ . Therefore, a straight metric to approximate unlearning is the KL divergence from the retrained model\u2019s output distribution. In addition, we also investigate metrics related to forgetting efficacy, retained performance, and privacy protection for evaluation in image classification and generation tasks. For details of evaluation, please refer to Sec. 5 and Appendix D. ", "page_idx": 2}, {"type": "text", "text": "Steepest Descent. Approximate MU methods are usually based on gradient updates to obtain the unlearned model [18, 21]. Let\u2019s first reinterpret the gradient with steepest descent [29, 27, 30]. The goal of steepest descent is to find the direction $\\delta\\theta=\\theta_{t+1}-\\theta_{t}$ that drives the objective function $F(\\theta)$ descent fastest within a $\\xi$ -neighborhood of the current parameters $\\theta_{t}$ . This can be formulated as the following optimization problem. (See Appendix A.1 for proof.) ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\overset{\\sim}{\\delta}\\overset{\\cdot}{:}=\\underset{\\rho(\\theta_{t},\\theta_{t}+\\delta\\theta)\\leq\\xi}{\\arg\\operatorname*{min}}F(\\theta_{t}+\\delta\\theta)\\overset{\\star\\::}{\\Rightarrow}\\theta_{t+1}:=\\underset{\\theta_{t+1}}{\\arg\\operatorname*{min}}F(\\theta_{t+1})+\\frac{1}{\\alpha_{t}(\\xi,\\theta_{t})}\\rho(\\theta_{t},\\theta_{t+1}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\rho(\\cdot,\\cdot)$ represents the manifold metric that renders the geometry of the parameter\u2019s neighborhood. To simplify the derivation, we rewrite it to the form on the right, where $\\alpha_{t}(\\xi,\\theta_{t})$ represents the learning rate required to move the distance $\\xi$ . In the following, we fix a small learning rate $\\alpha_{t}$ to approximate a search within a local neighborhood. The characterization $\\rho$ of the underlying coordinate space of the neighborhood will determine update directions, optimization paths, and the flatness of the minimum. Vanilla gradient descent is obtained using the Euclidean metric, Newton\u2019s direction is measured by the second-order expansion of the objective function [44], and the KL divergence in the output space induces a natural gradient [30, 45]. Next, we will probe approximate MU through vanilla gradient descent and attempt to benefit from improved manifold metrics. ", "page_idx": 2}, {"type": "text", "text": "3 Approximate MU from Perspective of Steepest Descent ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Revisit Approximate MU Methods via Vanilla Gradient Descent. We begin with the vanilla gradient descent direction to address approximate MU. This involves finding the steepest descent direction that minimizes the KL divergence with the retrained output within the vicinity of the current model $\\theta_{t}$ . The optimization problem can be formalized as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\theta_{t+1}=\\underset{\\theta_{t+1}}{\\arg\\operatorname*{min}}\\,D_{\\mathrm{KL}}\\left(p_{z}\\left(\\theta_{*}\\right)\\right||p_{z}\\left(\\theta_{t+1}\\right)\\right)+\\frac{1}{\\alpha_{t}}\\rho(\\theta_{t},\\theta_{t+1})}\\\\ {=\\underset{\\theta_{t+1}}{\\arg\\operatorname*{min}}\\,\\underbrace{D_{\\mathrm{KL}}\\left(p_{z^{f}}\\left(\\theta_{*}\\right)||p_{z^{f}}\\left(\\theta_{t+1}\\right)\\right)}_{(a)}p^{f}+\\underbrace{D_{\\mathrm{KL}}\\left(p_{z^{r}}(\\theta_{*})||p_{z^{r}}(\\theta_{t+1})\\right)}_{(b)}p^{r}+\\frac{1}{\\alpha_{t}}\\underbrace{\\rho(\\theta_{t},\\theta_{t+1})}_{(c)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $p^{f}=p(D^{f}|D)$ and $p^{r}=p(\\mathcal{D}^{r}|\\mathcal{D})=1-p_{f}$ denote the partition of forgetting and remaining dataset, respectively. Analyzing (2), $(a)$ seeks to eliminate the influence of the target forgetting samples, $(b)$ aims to maintain the performance on the remaining samples, and (c) employs the metric $\\rho$ to constrain the magnitude of each update, thereby identifying the direction of steepest descent on the manifold. To solve the optimization challenge outlined in (2), we posit that for the current model \u03b8t, there exists a set of coefficients \u03b5t = {\u03b5t,i}iN=f1 t hat weights the forgetting loss, positioning $\\theta_{t}$ as the minimizer of the weighted loss for the original training set. The unlearning process necessitates adaptations in coefficients of forgetting loss. Then, we can determine the vanilla gradient descent for approximate MU by using Euclidean distance $\\ell_{2}$ as the manifold metric, as stated in Prop. 1. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Proposition 1. Under the Euclidean manifold metric, $\\begin{array}{r}{\\rho(\\theta_{t},\\theta_{t+1})=\\frac{1}{2}||\\theta_{t}-\\theta_{t+1}||^{2}}\\end{array}$ . Assuming that the current model $\\theta_{t}=\\arg\\operatorname*{min}_{\\theta}\\mathcal{L}^{f}(\\theta;\\varepsilon_{t})+\\mathcal{L}^{r}(\\theta)$ . Let $H_{*}^{f}=\\nabla^{2}\\mathcal{L}^{f}(\\theta_{*};\\mathbf{1})\\boldsymbol{\\epsilon}$ and $H_{*}^{r}=\\nabla^{2}\\mathcal{L}^{r}(\\theta_{*})$ denote the Hessian of the retrained model on the forgetting set and the remaining set, respectively. Then, the steepest descent direction that minimizes (2) is approximately: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\theta_{t+1}-\\theta_{t}:\\approx-\\alpha_{t}[\\underbrace{H_{*}^{f}(H_{*}^{r})^{-1}}_{(S)}\\underbrace{[-\\dot{\\nabla}\\dot{\\mathcal{L}}^{f}(\\theta_{t}^{'};\\varepsilon_{t})]}_{(F)}p^{f}+\\underbrace{\\dot{\\nabla}\\mathcal{L}^{r}(\\theta_{t})}_{(R)}p^{r}].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The proof can be found in Appendix A.2. To elucidate the effectiveness of gradient-based unlearning methods, we decompose the vanilla gradient descent direction in (3) into three components: (F), (R), and (S). (F) represents the gradient ascent direction of the weighted forgetting loss, which directs the model to discard the information of the forgetting samples. Fine-tuning (FT) [22, 19, 38] fails to guarantee MU due to the absence of (F). Current approximate MU methods such as Random Labeling (RL) [20] and BadTeacher knowledge distilla", "page_idx": 3}, {"type": "text", "text": "Table 1: Comparison of approximate MU methods. We decompose the steepest descent direction into three parts: the weight saliency matrix (S), the forgetting part (F), and the remaining part (R) as in (3) and (4). Only SA and our method consider the remain-preserving manifold, and we further approximate up-to-date Hessian. ", "page_idx": 3}, {"type": "table", "img_path": "dheDf5EpBT/tmp/28aca36a9abab09608ed15d8907f0fc6e8fc448d78f44d89bce7c873fcea2bc0.jpg", "table_caption": [], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "tion (BT) [23] are akin to weighted forgetting   \nloss gradient ascent, uniformly leading to an increase in the loss on forgetting samples. The unlearning process often causes catastrophic forgetting of the retained knowledge. Thus, it is common to integrate (R) fine-tuning on the remaining set to sustain the model\u2019s general capabilities. Unlearned models via Gradient Ascent (GA) [20, 21] usually lose usability without (R). Furthermore, (S), ignored in most of the previous literature, involves two Hessian modulation parts. $H_{*}^{f}$ amplifies the parameter updates crucial for forgetting, while $(H_{*}^{r})^{-1}$ dampens those important for maintaining. The notion of (S) closely mirrors the Weight Saliency introduced in SalUn [26]. We provide theoretical support for this notion. Importantly, our framework makes no assumption regarding input modalities, allowing its flexible application across various CV unlearning tasks. ", "page_idx": 3}, {"type": "text", "text": "Approximate MU in Remain-preserving Manifold. In fact, employing Euclidean distance as the manifold metric for parameter updates is arbitrary. It treats all coordinates as equally important because the local second-order expansion is identical $\\nabla_{\\theta_{t}}^{2}\\big(\\frac12\\|\\theta_{t}-\\theta_{t+1}\\|_{.}^{2}\\big)\\,=\\,\\dot{I}$ . This uniform treatment overlooks the varying parameter significance for forgetting and remaining. Moreover, certain manifolds of parameter space can exhibit substantial variations in Euclidean metric, yet the induced model output remains almost unchanged [46]. Since the retrained model performance on forgetting is unpredictable, it is pragmatic to introduce manifolds related to the remaining. Therefore, a practical objective is to constrain parameter updates during unlearning within a manifold that minimally impacts the retained performance. An empirical characterization of such a manifold could be the $K L$ divergence on the output distribution of the remaining set, $D_{\\mathrm{KL}}^{r}$ . Given that the original well-trained and the retrained model output closely match the ground-truth remaining distribution, $\\nabla\\mathcal{L}^{r}(\\theta_{0})\\approx\\nabla\\mathcal{L}^{r}(\\theta_{*})\\approx0$ . By starting with $\\theta_{0}$ and limiting updates to this manifold, the maintained output distribution remains almost consistent throughout the unlearning iterations, $\\nabla{\\mathcal{L}}^{r}(\\theta_{t+1})\\approx\\nabla{\\mathcal{L}}^{r}(\\theta_{t})\\approx\\nabla{\\mathcal{L}}^{r}(\\theta_{0})\\approx0$ . This consistency permits a second-order Taylor expansion at $\\theta_{t}$ to terms (b) and (c) in (2), providing crucial curvature information for unlearning to prevent deviations in the model output on the remaining set, leading to Prop. 2. ", "page_idx": 3}, {"type": "text", "text": "Proposition 2. Using the model output $K L$ divergence on the remaining set as the manifold metric, $\\bar{\\rho(\\theta_{t},\\theta_{t+1})}=D_{K L}\\,\\bar{(p_{z^{r}}(\\theta_{t})||p_{z^{r}}(\\theta_{t+1}))}\\bar{\\,}$ . Assuming that the current model $\\theta_{t}=\\arg\\operatorname*{min}_{\\theta}\\mathcal{L}^{r}(\\theta)+$ $\\mathcal{L}^{f}(\\boldsymbol{\\theta};\\boldsymbol{\\varepsilon}_{t})$ . Let $\\tilde{\\alpha}_{t}=\\alpha_{t}p^{f}/(\\alpha_{t}p^{r}+1)$ , and $H_{t}^{r}=\\nabla^{2}\\mathcal{L}^{r}(\\theta_{t})$ represent the Hessian w.r.t. $\\theta_{t}$ on the remaining set, then the steepest descent direction that minimizes (2) is approximately: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\theta_{t+1}\\stackrel{!}{-}\\theta_{t}:\\approx-\\tilde{\\alpha}_{t}\\underbrace{(H_{t}^{r})^{-1}}_{(R)}[\\underbrace{H_{*}^{f}(H_{*}^{r})^{-1}}_{(S)}\\underbrace{[-\\nabla\\mathcal{L}^{f}(\\theta_{t};\\dot{\\varepsilon}_{t})]}_{(F)}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We defer the proof to Appendix A.3. The unlearning updates in (4) incorporated second-order Hessian concerning remaining to guide the optimization direction. Specifically, the large curvature direction of $H_{t}^{r}$ corresponds with the weights that encapsulate remaining knowledge, while the small curvature direction encourages model updates for effective unlearning. Furthermore, the update direction in (4) primarily follows the weighted gradient ascent (F) modulated by weight saliency (S). Such unlearning iterative updates in remain-preserving manifold focus on diminishing the distributional discrepancy with exact MU concerning forgetting output. ", "page_idx": 4}, {"type": "text", "text": "Challenges in Hessian Approximation. To exploit the benefits of unlearning updates within the remain-preserving manifold, the key point is $(\\dot{H}_{t}^{r})^{-1}$ . However, calculating the Hessian and its reverse for large-scale models is computationally demanding [47]. Consequently, many methods have been developed to estimate the Hessian, such as Fisher information [34], Fisher diagonals [48], and Kronecker factored Laplace approximation [49]. Regarding unlearning, Selective Amnesia (SA) [35] employs the initial model\u2019s remaining Fisher diagonals as the second-order Hessian constraint on parameter updates. However, the fixed Hessian in SA leads to progressively increasing estimation biases, exacerbated by cumulative errors in Taylor expansion, which harms the retained performance during unlearning. To address this issue, the subsequent Sec. 4 introduces an fast-slow weight update method that implicitly approximates the direction adjusted by the up-to-date Hessian. ", "page_idx": 4}, {"type": "text", "text": "4 Proposed Method ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Implicit Online Hessian Approximation (R-on). Given that computing the inverse of even wellapproximated Hessian demands substantial computational resources, it is more practical to estimate the unlearning direction post-Hessian inversion modulation directly. Inspired by recent insights into the connection between Meta-Continual Learning and Hessian approximation [50], we propose a fast-slow weight [36, 37] method for implicitly approximating the desired updates. The optimization problem for fast weight updates is formulated as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta_{t}^{f}}\\mathcal{L}^{r}(\\theta_{t}^{f})\\quad\\mathrm{s.t.}\\quad\\theta_{t}^{f}=\\theta_{t}-\\beta_{t}\\nabla\\mathcal{L}^{u}(\\theta_{t}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathcal{L}^{u}$ represents an arbitrary forgetting loss and $\\beta_{t}$ is its learning rate. The iterative process is depicted in Fig. 1. A step of forgetting is taken at the current model, resulting in $\\theta_{t}^{f}$ . Several gradient descent updates on the remaining set follow to obtain the minimum point $\\theta_{t}^{r}$ . This fine-tuning ensures that the updated model adheres to the remain-preserving manifold. The slow weight updates leverage the underlying connection between $\\theta_{t}$ and $\\theta_{t}^{r}$ , as stated in Prop. 3. ", "page_idx": 4}, {"type": "text", "text": "Proposition 3. For implicit online Hessian approximation in (5), suppose $\\beta_{t},\\delta_{t}$ is small, $\\beta_{t}\\,<$ $\\sqrt{\\delta_{t}/|\\nabla\\mathcal{L}^{r}({\\boldsymbol{\\theta}}_{t})-[\\nabla\\mathcal{L}^{r}({\\boldsymbol{\\theta}}_{t})]^{2}|}$ , $\\mathcal{L}^{r}$ is $\\mu$ -smooth, i.e., $\\lVert\\nabla\\mathcal{L}^{r}({\\boldsymbol{\\theta}})\\,-\\,\\nabla\\mathcal{L}^{r}({\\boldsymbol{\\theta}}^{\\prime})\\rVert_{2}\\,\\leq\\,\\mu\\lVert{\\boldsymbol{\\theta}}-{\\boldsymbol{\\theta}}^{\\prime}\\rVert_{2}$ , and there exist an $\\zeta_{t}$ -neighborhood $\\mathcal{N}(\\theta_{t}^{r},\\zeta_{t})$ of the optimal model parameter $\\begin{array}{r}{\\theta_{t}^{r}=\\arg\\operatorname*{min}_{\\theta_{t}^{f}}\\mathcal{L}^{r}(\\theta_{t}^{f}),}\\end{array}$ , which includes $\\theta_{t}$ and $\\theta_{t}^{f}$ . Then, the iterative update term approximately is, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\theta_{t}-\\theta_{t}^{r}:\\approx\\beta_{t}^{2}\\left[\\nabla^{2}\\mathcal{L}^{r}(\\theta_{t})\\right]^{-1}\\nabla\\mathcal{L}^{u}(\\theta_{t})=\\beta_{t}^{2}(H_{t}^{r})^{-1}\\nabla\\mathcal{L}^{u}(\\theta_{t}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The proof is in Appendix A.4. Prop. 3 indicates that the model $\\theta_{t}^{r}$ , obtained after fine-tuning using the process described in (5), is approximately equivalent to updating the current model $\\theta_{t}$ by one step in the Hessian-adjusted unlearning direction. We use this direction to update the outer loop. ", "page_idx": 4}, {"type": "text", "text": "Comparison with the joint loss $(\\mathbf{R})$ . We investigate the differences in the updates between our optimization in (5) $(\\mathbf{R}{\\bullet}\\mathbf{o}\\mathbf{n})$ and the joint optimization of forgetting and remaining losses $(\\bf R)$ [23, 25]. We take the checkpoint after the first step of fine-tuning the remaining set as an example and ignore the step size. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\boldsymbol{\\mathcal{L}}^{\\mathrm{R}}(\\boldsymbol{\\theta}_{t})=\\boldsymbol{\\mathcal{L}}^{u}(\\boldsymbol{\\theta}_{t})+\\boldsymbol{\\mathcal{L}}^{r}(\\boldsymbol{\\theta}_{t}),\\quad\\boldsymbol{\\Delta}^{\\mathrm{R}}=\\nabla\\boldsymbol{\\mathcal{L}}^{u}(\\boldsymbol{\\theta}_{t})+\\nabla\\boldsymbol{\\mathcal{L}}^{r}(\\boldsymbol{\\theta}_{t}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Delta^{\\mathrm{R-on}}\\approx\\nabla\\mathcal{L}^{u}(\\theta_{t})+\\nabla\\mathcal{L}^{r}(\\theta_{t})+\\nabla^{2}\\mathcal{L}^{r}(\\theta_{t})(\\theta_{t}^{f}-\\theta_{t})=(I-H_{t}^{r})\\nabla\\mathcal{L}^{u}(\\theta_{t})+\\nabla\\mathcal{L}^{r}(\\theta_{t}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Comparison of the updates in (7) and (8) reveals that the remaining gradient is the same. Our forgetting update in fast weight is adjusted by an additional term $-H_{t}^{r}$ , which is absent in joint optimization. This modification weakens the directions that significantly impact the remain, thereby mitigating the damage of forgetting loss on the retained performance. Furthermore, certain methods [24] suggest two-stage unlearning that first impairs the model and then repairs it, actually paralleling a single fast-slow weight update of our method. ", "page_idx": 4}, {"type": "text", "text": "Sample-wise Adaptive Coefficient for Gradient Ascent (F). Despite a variety of forgetting losses introduced in previous literature, we stick to our theoretical result in (4) and adopt the weighted forgetting loss. Due to intractable challenges in solving the inverse problem associated with the arg min condition for the gradient ascent coefficients, we explore the properties of these coefficients and propose a heuristic estimation. $\\varepsilon_{t,i}$ satisfies: $\\textcircled{1}$ For the pretrained model, $\\varepsilon_{0,i}=1$ . $\\circledcirc$ For the retrained model, $\\varepsilon_{T^{*},i}=1$ , where $T^{*}$ is the optimal step to obtain RT. $\\circled{3}$ Assuming homogeneity of samples, if $\\ell(\\theta;z_{i}^{f})\\,>\\,\\ell(\\theta;z_{j}^{f})$ , then $\\varepsilon_{t,i}\\,<\\,\\varepsilon_{t,j}$ [51]. Moreover, considering the continuity in the model parameter space implies the continuity of $\\varepsilon_{t,i}$ in the function space. Thus, our heuristic estimation consists of decreasing numerical values across steps and sample-wise adaptation based on loss magnitude, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\tilde{\\varepsilon}_{t,i}=(1-\\frac{t}{T})\\frac{1/[\\ell(\\theta_{t};z_{i}^{f})]_{\\mathrm{detach}}^{\\lambda}}{\\sum_{z_{j}^{f}\\in\\mathcal{D}^{f}}1/[\\ell(\\theta_{t};z_{j}^{f})]_{\\mathrm{detach}}^{\\lambda}}\\times N^{f},\\,1\\leq i\\leq N^{f},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $T$ represents the outer loop iteration, $\\lambda$ is the temperature scalar to control the smoothness of coefficients, and $[\\cdot]_{\\mathrm{detach}}$ denotes the operation to detach a tensor from the computational graph, which means $\\ell(\\theta_{t};z_{i}^{f})$ in (9) only serves for weighting coefficients and does not contribute to the gradient. $\\tilde{\\varepsilon}_{t,i}$ is employed to modulate the gradient ascent loss for forgetting samples, thereby preventing model explosion and reducing the contribution to updates from samples whose influence has already been ablated, while prioritizing those whose losses remain minimal and are not yet adequately forgotten. However, relying solely on empirical loss as an evaluation metric for sample contribution is limited and potentially biased. We believe that enhanced designs for coefficient estimation in future research could yield more accurate unlearning results. ", "page_idx": 5}, {"type": "text", "text": "Forget-Remain Balanced Weight Saliency (S). Recall in the theoretical framework of (4), the steepest descent leverages $H_{*}^{f}(H_{*}^{r})^{-1}$ as weight saliency to modify the forgetting gradient. However, computing both Hessians at RT is impractical, necessitating an estimation of weight saliency for both forgetting and remaining. Previous work like SalUn [26] considers only parameters that significantly affect the forgetting set, whereas these parameters might also critically impact the retained performance. To address this and align with our theoretical insights, we adopt techniques from SSD [33] to approximate $H_{*}^{f}(H_{*}^{r})^{-1}$ using the diagonal of the initial model\u2019s Fisher information matrix. Through a hard thresholding operation, we obtain the weight saliency map: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{m}=\\mathbf{I}\\left[F_{\\mathrm{diag}}^{f}(F_{\\mathrm{diag}}^{r})^{-1}\\geq\\gamma\\right],\\;\\mathrm{where}\\;F_{\\mathrm{diag}}^{f}=[\\nabla\\mathcal{L}^{f}(\\theta_{0})]^{2},F_{\\mathrm{diag}}^{r}=[\\nabla\\mathcal{L}^{r}(\\theta_{0})]^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "$\\mathbf{I}[\\cdot]$ is an element-wise indicator function, and $\\gamma>0$ is a hard threshold used to control the ForgetRemain balance in selecting parameters. Guided by (4), we apply the weight saliency map exclusively to forgetting, in contrast to SalUn [26] which applies to both forgetting and remaining. The saliency map can enhance the unlearning process by directing updates to focus on the parameters that are crucial for erasing specific samples or concepts. More advanced weight saliency estimation is expected to improve outcomes in future work. ", "page_idx": 5}, {"type": "text", "text": "Integrated Fast-slow Weight Update. By integrating the three designs into a single update scheme towards the Saliency Forgetting in the Remain-preserving manifold online, we develop our SFR-on method. Specifically, in the inner loop for fast weights, we use adaptive coefficients in (9) to weight the forgetting gradient ascent with the weight saliency map from (10) to serve as the unlearning update in (5). Slow weights in outer loops update by linearly interpolating the fine-tuned $\\theta_{t}^{r}$ and $\\theta_{t}$ in weight space, achieving an estimated steepest descent for approximate MU under the remaining output constraint in (4). Then, we have the overall fast-slow weight update rule: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathrm{Inner\\:Loop:}\\:\\operatorname*{min}_{\\theta_{t}^{f}}\\mathcal{L}^{r}(\\theta_{t}^{r})\\quad\\mathrm{s.t.}\\;\\theta_{t}^{f}=\\theta_{t}-\\beta_{t}[\\mathbf{m}\\odot(-\\nabla\\mathcal{L}^{f}(\\theta_{t};\\tilde{\\varepsilon}_{t}))],\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\alpha_{t}$ represents the slow weights learning rate. Worth mentioning that our SFR-on does not require adaptation to specific application tasks, nor does it necessitate modifications to the task\u2019s inherent loss. Consequently, our approach can be seamlessly applied to various CV unlearning scenarios by simply substituting the loss in the weighted gradient ascent with either CE loss for image classification or MSE loss for image generation. Considering that calculating fine-tuning or the Fisher diagonal on the complete remaining dataset for large-scale image generation tasks, we randomly select an equivalent number of samples from the remaining as in the forgetting set for computation [35, 26]. We find that our method remains effective under such an inadequate setup, as detailed in Sec. 5. The complete algorithm is placed in Appendix C. ", "page_idx": 5}, {"type": "text", "text": "Table 2: Performance summary of MU methods for image classification (including RT, six baselines, our proposed SFR-on, and ablations on our designed components), assessing unlearning $10\\%$ random subset of CIFAR-10 using ResNet-18 and TinyImageNet using Swin-T. All results are presented as mean and standard deviation across 10 independent trials. Performance discrepancies from RT are indicated with (\u2022), highlighting that more effective unlearning is reflected by performance closer to RT. The \u2018Averaging Disparity\u2019 (Avg.D) metric is calculated by the average of the gaps measured in accuracy-related metrics, including FA, RA, TA, and MIA. $D_{\\mathrm{KL}}$ denotes the KL divergence to RT. RTE is recorded in minutes. ", "page_idx": 6}, {"type": "table", "img_path": "dheDf5EpBT/tmp/d56735737650444ce45e0b20a2260c79021ac4a30fed6aa413cd30526edbce31.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Datasets, Models, and Settings. In image classification, we primarily focus on the random subset unlearning task. Evaluations are conducted using ResNet-18 [52] on CIFAR10 [53] and SwinT [54] on TinyImageNet [55], with additional tests on random subset and class-wise forgetting tasks involving CIFAR100 [53] and SVHN [56], detailed in Appendix F.2. In image generation, our main interest lies in class-wise forgetting tasks. Following [35, 26], we unlearn conditional DDPM [40] with the UNet architecture [57] on CIFAR10. Moreover, for the first time, we explore the latent diffusion model [42] equipped with Diffusion Transformer (DiT) [58] on ImageNet [59], which demonstrates superior scalability in learning large-scale data generation tasks. Finally, we perform concept forgetting tasks using the open-source Stable Diffusion (SD) V1.4 [42] to inhibit the generation of NSFW content, specifically by targeting the prevention of nude images. Further details on unlearning setups and training are available in Appendix E. ", "page_idx": 6}, {"type": "text", "text": "Baselines and Evaluation. We regard RT as an oracle of approximate MU and compare our proposal with eight MU methods, including six gradient-based MU approaches outlined in Sec. 3: FT [19], GA [21], BT [23], RL [20], SalUn [26], and SA [35]. We also consider SCRUB [25], an enhanced variant of BT, for image classification, and ESD [11] for removing concepts in SD. For the evaluation of random subset unlearning tasks, we measure the output KL divergence $D_{\\mathrm{KL}}$ between the unlearned and retrained models, which is the direct target of approximate MU. Besides, we assess the accuracy on the forgetting set (FA) for unlearning efficacy, and the accuracy on the remaining (RA) and test (TA) sets for preserved generalization ability. We also consider the success rate of membership inference attack (MIA) [60, 38] on the forgetting set as a privacy metric. Note that the smaller the disparity in these metrics against RT, the more effective the unlearning. Run-time efficiency (RTE) is also reported. For class-wise forgetting tasks in image generation, we evaluate the accuracy of the unlearned model\u2019s generated images on forgetting classes (FA) by a pre-trained classifier. The Fr\u00e9chet Inception Distance (FID) [61] metric assesses the retained generative capability for remaining classes. For ablating the \u2018nudity\u2019 concept in SD, we employ the NudeNet [62] detector to identify and count nude body parts in generated NSFW images. For further introduction to the baselines and detailed evaluation metrics, please refer to Appendix E.1 and D. ", "page_idx": 6}, {"type": "text", "text": "To assess the unlearning effectiveness and efficiency of SFR-on, we perform comprehensive experiments and conduct ablation studies to address the following four key questions: ", "page_idx": 6}, {"type": "text", "text": "Q1: How does SFR-on perform on unlearning in image classification? We first evaluate the performance of our method, SFR-on, against existing gradient-based MU methods on the image classification random subset unlearning task. In this scenario, the forgetting set, remaining, and test sets all originate from the same distribution. Consequently, even if the model undergoes unlearning on the random subset, it may still generalize to these samples. To avoid potential biases from only using FA as an unlearning metric, we incorporate MIA to assess the privacy retention of the forgetting set, enhancing the robustness of assessments. As detailed in Tab. 2, for forgetting $10\\%$ random subset on CIFAR-10 and TinyImageNet, SFR-on not only most closely aligns with RT in the averaging metric disparity but also exhibits the smallest output KL divergences w.r.t. RT. This performance underscores our effectiveness and efficiency in achieving the objective of approximate MU. The results of the increased $50\\%$ random subset unlearning task are included in Appendix F.2. ", "page_idx": 6}, {"type": "table", "img_path": "dheDf5EpBT/tmp/cd37fe6abcb1ed45b8c5e8dfe9b496a7f3ef5e791a13540ff4e6800e9cdfcacb.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "dheDf5EpBT/tmp/231e282e2ab9ad23184b732ed1a2126cad36a370ac2f30808ba40f83b2e0c3ab.jpg", "img_caption": ["Figure 2: Image generations for class-wise forgetting tasks on CIFAR-10 using DDPM by baselines and our proposed SFR-on along with ablation variants. The forgetting class is \u2018cat\u2019, \u2018I\u2019 refers to the generated image sample from this class, and \u2018C\u2019 denotes the remaining class name. More results can be found in Appendix F.7. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Q2: How does SFR-on perform on class-forgetting in image generation? Another recent focus in MU involves the targeted removal of specific knowledge from image generation models, which are currently categorized into conditional and latent diffusion models. For the former, we investigated class-wise forgetting on CIFAR-10 using DDPM. As illustrated in Fig. 2, RT continues to generate high-quality images devoid of the semantic content of \u2018cat.\u2019 Contrarily, previous approaches like SA resulted in random noise for the forgetting class, whereas SalUn created images belonging to alternate classes, both differ from RT output. Our SFR-on (depicted in the last row of Fig. 2) effectively removes the \u2018cat\u2019 class by yielding high-quality pictures without discernible semantics, and maintains the high fidelity of images across non-forgetting classes, most matching RT. Furthermore, we extend the image generation unlearning task to DiT, a recently proposed model promising for realistic image generation. Due to computational resource constraints, RT for DiT is unfeasible. Instead, we simulate RT by substituting the DiT output of the forgetting class with initial random latent embeddings. These embeddings are then processed by the pre-trained autoencoder to reconstruct the corresponding images, referred to as $\\mathbf{R}\\mathbf{T}^{\\dagger}$ in Fig. 3. Previous methods, such as SA and SalUn, fail to completely reduce the forgetting class to noise and compromise the fidelity of non-forgetting class images. In contrast, our SFR-on successfully achieves results comparable to $\\mathbf{R}\\mathbf{T}^{\\dagger}$ , effectively forgetting the target class without degrading the general generative capability. Details on the accuracy of forgetting class samples by various unlearned DiT and FID of remaining classes are presented in Tab. 3. ", "page_idx": 7}, {"type": "text", "text": "Q3: What is the impact of each component of SFR-on on forgetting performance? To investigate the efficacy of each component we developed for the decomposition of approximate MU, we perform ablation studies. We build on the joint training of GA and FT as our baseline (R) and incorporate our proposed implicit online hessian approximation (R-on), adaptive coefficients (F), and forget-retain balanced weight saliency (S). As demonstrated in the last four rows of Tab. 2, the addition of our (R-on) allows models to effectively forget while sustaining performance on the remaining set. Both (F) and (S), crafted to approach the steepest descent for approximate MU, enhance the unlearning efficacy. In image generation, as depicted in the last three rows of Fig. 2, although the joint training (R) is capable of completely forgetting targeted classes, it results in distorted images for remaining. Replacing (R) with our (R-on) remarkably improves the image fidelity of the remaining classes, but the forgetting class images still show low-quality textures. Further, our (F) and (S) effectively direct the unlearning process towards the approximate MU, ensuring that the performance of the unlearned models closely mirrors that of RT. More ablations on hyperparameters are provided in Appendix F.1. ", "page_idx": 7}, {"type": "image", "img_path": "dheDf5EpBT/tmp/8790c9ce4caa9018d90c863ee27d2d2e6119701b8ace92b3e41ef7ed8673f99c.jpg", "img_caption": ["Figure 3: Class-wise forgetting of \u2018golden retriever\u2019 in image generations of ImageNet with DiT, comparing baselines and our proposed SFR-on. $\\mathbf{R}\\mathbf{T}^{\\dagger}$ feeds autoencoder with random latent embeddings for the forgetting class, due to the computational constraints, rather than full RT. \u2018I\u2019 denotes image samples from forgetting, and \u2018C\u2019 refers to other remaining class name, e.g. \u2018Cacatua galerita\u2019 (C1). More results can be found in Appendix F.7. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Q4: How effective is SFR-on in NSFW content removal for SD? We finally assess the efficacy of our method in removing the \u2018nudity\u2019 concept from the open-source SD to prevent the generation of NSFW content. Given that SD V1.4 is trained on the LAION dataset [63], purging all images depicting nudity and retraining the model would be prohibitively time-consuming and resource-intensive. In this unlearning task, ", "page_idx": 8}, {"type": "text", "text": "Table 4: Unlearning performance in erasing \u2018Nudity\u2019 concept from SD by the original SD V1.4, ESD, SalUn, and our SFR-on, measuring the number of total 9406 NSFW images generated from I2P prompts across nudity categories. The prefixes \u2018F-\u2019 and $\\mathbf{\\omega}^{\\bullet}\\mathbf{M}-\\mathbf{\\omega}^{\\bullet}$ denote \u2018Female-\u2019 and \u2018Male-\u2019 respectively. ", "page_idx": 8}, {"type": "table", "img_path": "dheDf5EpBT/tmp/2a779bb53f2120a497c1da4db407d4f218fd280210727eeb92d9df055d4f6bdc.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "we designate \u2018nudity\u2019 as the forgetting set and generate a set of clothed individuals as remaining to preserve SD\u2019s generalization capability across non-nudity themes. We utilize the inappropriate image prompts (I2P) [12] to query potentially NSFW content from the unlearned SD and employ NudeNet to detect exposed body parts in these images. The results in Tab. 4 demonstrate that our method significantly prevents the generation of culturally sensitive body parts, such as breasts and genitalia, highlighting our strength to enhance the trustworthiness of machine learning applications. ", "page_idx": 8}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "This paper revisits gradient-based approximate MU methods from the perspective of the steepest descent. The descent direction under an Euclidean manifold metric can be divided into three integral components: weighted forgetting gradient ascent, fine-tuning remaining gradient descent, and weight saliency matrix. Our approach advances beyond the Euclidean constraints by embedding the unlearning update within a remain-preserving manifold. This novel strategy incorporates the second-order Hessian of the current model on remaining, safeguarding against detrimental impacts on retained performance. To circumvent the prohibitive computational demands of the Hessian in large-scale models, we introduce an efficient fast-slow weight update method to approximate the Hessian-adjusted direction. Furthermore, our innovative adaptive coefficient for weight forgetting loss and a forget-remain balanced weight saliency map facilitate near-retraining unlearning. Our method can be applied to popular CV unlearning tasks with empirically verified unlearning efficacy. ", "page_idx": 8}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The authors would like to thank the anonymous reviewers for their insightful comments. ", "page_idx": 9}, {"type": "text", "text": "The research leading to these results has received funding from National Key Research Development Project (2023YFF1104202), National Natural Science Foundation of China (62376155, Shanghai Municipal Science and Technology Research Program Major Project (2021SHZDZX0102). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] L. Bourtoule, V. Chandrasekaran, C. A. Choquette-Choo, H. Jia, A. Travers, B. Zhang, D. Lie, and N. Papernot, \u201cMachine unlearning,\u201d 2021 IEEE Symposium on Security and Privacy (SP), pp. 141\u2013159, 2019. 1, 2, 3, 19   \n[2] T. Shaik, X. Tao, H. Xie, L. Li, X. Zhu, and Q. Li, \u201cExploring the landscape of machine unlearning: A survey and taxonomy,\u201d arXiv preprint arXiv:2305.06360, 2023. 19   \n[3] J. Xu, Z. Wu, C. Wang, and X. Jia, \u201cMachine unlearning: Solutions and challenges,\u201d IEEE Transactions on Emerging Topics in Computational Intelligence, 2024. 1, 19   \n[4] G. D. P. R. GDPR, \u201cGeneral data protection regulation,\u201d URL: https://gdpr-info. eu/[accessed 2020-11-21], 2018. 1   \n[5] E. Illman and P. Temple, \u201cCalifornia consumer privacy act,\u201d The Business Lawyer, vol. 75, no. 1, pp. 1637\u20131646, 2019. 1 [6] J. Rando, D. Paleka, D. Lindner, L. Heim, and F. Tram\u00e8r, \u201cRed-teaming the stable diffusion safety filter,\u201d arXiv preprint arXiv:2210.04610, 2022. 1, 19   \n[7] N. De Cao, W. Aziz, and I. Titov, \u201cEditing factual knowledge in language models,\u201d Empirical Methods in Natural Language Processing (EMNLP), 2021. 1   \n[8] K. Meng, D. Bau, A. Andonian, and Y. Belinkov, \u201cLocating and editing factual associations in gpt,\u201d Advances in Neural Information Processing Systems (NIPS), vol. 35, pp. 17 359\u201317 372, 2022. 1   \n[9] A. Oesterling, J. Ma, F. Calmon, and H. Lakkaraju, \u201cFair machine unlearning: Data removal while mitigating disparities,\u201d in International Conference on Artificial Intelligence and Statistics. PMLR, 2024, pp. 3736\u20133744. 1   \n[10] R. Chen, J. Yang, H. Xiong, J. Bai, T. Hu, J. Hao, Y. Feng, J. T. Zhou, J. Wu, and Z. Liu, \u201cFast model debias with machine unlearning,\u201d Advances in Neural Information Processing Systems (NIPS), vol. 36, 2024. 1   \n[11] R. Gandikota, J. Materzynska, J. Fiotto-Kaufman, and D. Bau, \u201cErasing concepts from diffusion models,\u201d in IEEE/CVF International Conference on Computer Vision (ICCV), 2023, pp. 2426\u20132436. 1, 2, 7, 19, 22   \n[12] P. Schramowski, M. Brack, B. Deiseroth, and K. Kersting, \u201cSafe latent diffusion: Mitigating inappropriate degeneration in diffusion models,\u201d in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023, pp. 22 522\u201322 531. 2, 9, 19   \n[13] N. Kumari, B. Zhang, S.-Y. Wang, E. Shechtman, R. Zhang, and J.-Y. Zhu, \u201cAblating concepts in text-toimage diffusion models,\u201d in IEEE/CVF International Conference on Computer Vision (ICCV), 2023, pp. 22 691\u201322 702. 19   \n[14] E. Zhang, K. Wang, X. Xu, Z. Wang, and H. Shi, \u201cForget-me-not: Learning to forget in text-to-image diffusion models,\u201d arXiv preprint arXiv:2303.17591, 2023. 1, 19   \n[15] C. Guo, T. Goldstein, A. Y. Hannun, and L. van der Maaten, \u201cCertified data removal from machine learning models,\u201d International Conference on Machine Learning (ICML), 2020. 1, 3, 19   \n[16] Z. Izzo, M. A. Smart, K. Chaudhuri, and J. Y. Zou, \u201cApproximate data deletion from machine learning models: Algorithms and evaluations,\u201d ArXiv, vol. abs/2002.10077, 2020. 1, 3, 19   \n[17] A. Thudi, H. Jia, I. Shumailov, and N. Papernot, \u201cOn the necessity of auditable algorithmic definitions for machine unlearning,\u201d in USENIX Security Symposium, 2021. 1   \n[18] S. Neel, A. Roth, and S. Sharifi-Malvajerdi, \u201cDescent-to-delete: Gradient-based methods for machine unlearning,\u201d Algorithmic Learning Theory, 2021. 2, 3, 19   \n[19] A. Warnecke, L. Pirch, C. Wressnegger, and K. Rieck, \u201cMachine unlearning of features and labels,\u201d Annual Network and Distributed System Security Symposium, 2023. 4, 7, 19, 22   \n[20] L. Graves, V. Nagisetty, and V. Ganesh, \u201cAmnesiac machine learning,\u201d in AAAI Conference on Artificial Intelligence (AAAI), 2021. 4, 7, 19, 22   \n[21] A. Thudi, G. Deza, V. Chandrasekaran, and N. Papernot, \u201cUnrolling sgd: Understanding factors influencing machine unlearning,\u201d European Symposium on Security and Privacy (EuroS&P), pp. 303\u2013319, 2022. 3, 4, 7, 19, 22   \n[22] A. Golatkar, A. Achille, and S. Soatto, \u201cEternal sunshine of the spotless net: Selective forgetting in deep networks,\u201d IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 9301\u20139309, 2019. 4, 19, 22   \n[23] V. S. Chundawat, A. K. Tarun, M. Mandal, and M. S. Kankanhalli, \u201cCan bad teaching induce forgetting? unlearning in deep networks using an incompetent teacher,\u201d AAAI Conference on Artificial Intelligence (AAAI), 2023. 4, 5, 7, 19, 22   \n[24] A. K. Tarun, V. S. Chundawat, M. Mandal, and M. S. Kankanhalli, \u201cFast yet effective machine unlearning,\u201d IEEE transactions on neural networks and learning systems, vol. PP, 2021. 5, 19   \n[25] M. Kurmanji, P. Triantafillou, and E. Triantafillou, \u201cTowards unbounded machine unlearning,\u201d Towards Unbounded Machine Unlearning (NIPS), 2023. 2, 5, 7, 19, 22   \n[26] C. Fan, J. Liu, Y. Zhang, D. Wei, E. Wong, and S. Liu, \u201cSalun: Empowering machine unlearning via gradient-based weight saliency in both image classification and generation,\u201d International Conference on Learning Representations (ICLR), 2023. 2, 4, 6, 7, 19, 22, 24   \n[27] M. Kim, D. Li, S. X. Hu, and T. M. Hospedales, \u201cFisher sam: Information geometry and sharpness aware minimisation,\u201d in International Conference on Machine Learning (ICML), 2022. 2, 3, 19   \n[28] T.-C. Kao, K. T. Jensen, G. M. van de Ven, A. Bernacchia, and G. Hennequin, \u201cNatural continual learning: success is a journey, not (just) a destination,\u201d Advances in Neural Information Processing Systems (NIPS), 2021. 19   \n[29] T. E. Abrudan, J. Eriksson, and V. Koivunen, \u201cSteepest descent algorithms for optimization under unitary matrix constraint,\u201d IEEE Transactions on Signal Processing, vol. 56, pp. 1134\u20131147, 2008. 3, 19   \n[30] J. Martens, \u201cNew insights and perspectives on the natural gradient method,\u201d Journal of Machine Learning Research, vol. 21, pp. 146:1\u2013146:76, 2014. 2, 3, 19   \n[31] T. Li, L. Tan, Z. Huang, Q. Tao, Y. Liu, and X. Huang, \u201cLow dimensional trajectory hypothesis is true: Dnns can be trained in tiny subspaces,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 45, pp. 3411\u20133420, 2022. 2   \n[32] J. E. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, and W. Chen, \u201cLora: Low-rank adaptation of large language models,\u201d International Conference on Learning Representations (ICLR), 2022. 2   \n[33] J. Foster, S. Schoepf, and A. Brintrup, \u201cFast machine unlearning without retraining through selective synaptic dampening,\u201d AAAI Conference on Artificial Intelligence (AAAI), 2024. 2, 6, 20   \n[34] J. Kirkpatrick, R. Pascanu, N. C. Rabinowitz, J. Veness, G. Desjardins, A. A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska, D. Hassabis, C. Clopath, D. Kumaran, and R. Hadsell, \u201cOvercoming catastrophic forgetting in neural networks,\u201d Proceedings of the National Academy of Sciences, vol. 114, pp. 3521 \u2013 3526, 2016. 2, 5, 19   \n[35] A. Heng and H. Soh, \u201cSelective amnesia: A continual learning approach to forgetting in deep generative models,\u201d Advances in Neural Information Processing Systems (NIPS), 2023. 2, 4, 5, 6, 7, 21, 22, 23   \n[36] M. R. Zhang, J. Lucas, G. E. Hinton, and J. Ba, \u201cLookahead optimizer: k steps forward, 1 step back,\u201d Advances in Neural Information Processing Systems (NIPS), 2019. 2, 5   \n[37] A. Nichol, J. Achiam, and J. Schulman, \u201cOn first-order meta-learning algorithms,\u201d ArXiv, vol. abs/1803.02999, 2018. 2, 5   \n[38] J. Jia, J. Liu, P. Ram, Y. Yao, G. Liu, Y. Liu, P. Sharma, and S. Liu, \u201cModel sparsity can simplify machine unlearning,\u201d in Neural Information Processing Systems (NIPS), 2023. 2, 3, 4, 7, 19, 22   \n[39] P. W. Koh and P. Liang, \u201cUnderstanding black-box predictions via influence functions,\u201d in International Conference on Machine Learning (ICML), 2017. 3, 19 ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "[40] J. Ho, A. Jain, and P. Abbeel, \u201cDenoising diffusion probabilistic models,\u201d Advances in Neural Information Processing Systems (NIPS), 2020. 3, 7, 19, 20 ", "page_idx": 11}, {"type": "text", "text": "[41] J. Ho, \u201cClassifier-free diffusion guidance,\u201d ArXiv, vol. abs/2207.12598, 2022. 19, 20   \n[42] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, \u201cHigh-resolution image synthesis with latent diffusion models,\u201d IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 10 674\u201310 685, 2022. 3, 7, 19, 20   \n[43] A. Mahadevan and M. Mathioudakis, \u201cCertifiable machine unlearning for linear models,\u201d $A r X i\\nu$ , vol. abs/2106.15093, 2021. 3   \n[44] D. Kovalev, K. Mishchenko, and P. Richt\u00e1rik, \u201cStochastic newton and cubic newton methods with simple local linear-quadratic rates,\u201d ArXiv, vol. abs/1912.01597, 2019. 3   \n[45] O. Calin and C. Udris\u00b8te, \u201cGeometric modeling in probability and statistics,\u201d 2014. 3   \n[46] T. Hoang, S. Rana, S. Gupta, and S. Venkatesh, \u201cLearn to unlearn for deep neural networks: Minimizing unlearning interference with gradient projection,\u201d IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 2023. 4   \n[47] M. Elsayed and A. R. Mahmood, \u201cHesscale: Scalable computation of hessian diagonals,\u201d 2022. 5   \n[48] F. Husz\u00e1r, \u201cNote on the quadratic penalties in elastic weight consolidation,\u201d Proceedings of the National Academy of Sciences, vol. 115, no. 11, Feb. 2018. [Online]. Available: http://dx.doi.org/10.1073/pnas.1717042115 5   \n[49] H. Ritter, A. Botev, and D. Barber, \u201cOnline structured laplace approximations for overcoming catastrophic forgetting,\u201d Advances in Neural Information Processing Systems (NIPS), 2018. 5   \n[50] Y. Wu, L.-K. Huang, R. Wang, D. Meng, and Y. Wei, \u201cMeta continual learning revisited: Implicitly enhancing online hessian approximation via variance reduction,\u201d in International Conference on Learning Representations (ICLR), 2024. [Online]. Available: https://openreview.net/forum?id=TpD2aG1h0D 5, 19   \n[51] B. Rozemberczki, L. Watson, P. Bayer, H.-T. Yang, O. Kiss, S. Nilsson, and R. Sarkar, \u201cThe shapley value in machine learning,\u201d International Joint Conference on Artificial Intelligence (IJCAI), 2022. 6   \n[52] K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep residual learning for image recognition,\u201d IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770\u2013778, 2015. 7   \n[53] A. Krizhevsky, \u201cLearning multiple layers of features from tiny images,\u201d 2009. 7   \n[54] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo, \u201cSwin transformer: Hierarchical vision transformer using shifted windows,\u201d IEEE/CVF International Conference on Computer Vision (ICCV), pp. 9992\u201310 002, 2021. 7   \n[55] Y. Le and X. S. Yang, \u201cTiny imagenet visual recognition challenge,\u201d 2015. 7   \n[56] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Ng, \u201cReading digits in natural images with unsupervised feature learning,\u201d 2011. 7   \n[57] O. Ronneberger, P. Fischer, and T. Brox, \u201cU-net: Convolutional networks for biomedical image segmentation,\u201d Medical Image Computing and Computer-Assisted Intervention (MICCAI), 2015. 7   \n[58] W. S. Peebles and S. Xie, \u201cScalable diffusion models with transformers,\u201d IEEE/CVF International Conference on Computer Vision (ICCV), pp. 4172\u20134182, 2023. 7, 19, 20   \n[59] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, \u201cImagenet: A large-scale hierarchical image database,\u201d IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 248\u2013255, 2009. 7   \n[60] L. Song and P. Mittal, \u201cSystematic evaluation of privacy risks of machine learning models,\u201d in USENIX Security Symposium, 2020. 7   \n[61] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter, \u201cGans trained by a two time-scale update rule converge to a local nash equilibrium,\u201d 2018. 7   \n[62] P. Bedapudi, \u201cNudenet: Neural nets for nudity classification, detection and selective censoring,\u201d 2019. 7   \n[63] C. Schuhmann, R. Vencu, R. Beaumont, R. Kaczmarczyk, C. Mullis, A. Katta, T. Coombes, J. Jitsev, and A. Komatsuzaki, \u201cLaion-400m: Open dataset of clip-filtered 400 million image-text pairs,\u201d ArXiv, vol. abs/2111.02114, 2021. 9   \n[64] A. Sekhari, J. Acharya, G. Kamath, and A. T. Suresh, \u201cRemember what you want to forget: Algorithms for machine unlearning,\u201d Advances in Neural Information Processing Systems (NIPS), 2021. 19   \n[65] A. A. Ginart, M. Y. Guan, G. Valiant, and J. Y. Zou, \u201cMaking ai forget you: Data deletion in machine learning,\u201d Advances in Neural Information Processing Systems (NIPS), 2019. 19   \n[66] E. Ullah, T. Mai, A. B. Rao, R. A. Rossi, and R. Arora, \u201cMachine unlearning via algorithmic stability,\u201d in Annual Conference Computational Learning Theory, 2021. 19   \n[67] R. Giordano, W. T. Stephenson, R. Liu, M. I. Jordan, and T. Broderick, \u201cA swiss army infinitesimal jackknife,\u201d in International Conference on Artificial Intelligence and Statistics, 2018. 19   \n[68] M. Chen, Z. Zhang, T. Wang, M. Backes, M. Humbert, and Y. Zhang, \u201cWhen machine unlearning jeopardizes privacy,\u201d Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security, 2020. 19   \n[69] Z. Li and Y. Zhang, \u201cMembership leakage in label-only exposures,\u201d Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security, 2020.   \n[70] L. Song, R. Shokri, and P. Mittal, \u201cPrivacy risks of securing machine learning models against adversarial examples,\u201d Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security, 2019.   \n[71] S. Yeom, I. Giacomelli, M. Fredrikson, and S. Jha, \u201cPrivacy risk in machine learning: Analyzing the connection to overftiting,\u201d 2018 IEEE 31st Computer Security Foundations Symposium (CSF), pp. 268\u2013282, 2017.   \n[72] N. Carlini, S. Chien, M. Nasr, S. Song, A. Terzis, and F. Tramer, \u201cMembership inference attacks from first principles,\u201d 2022. 19   \n[73] X. Cheng, Z. Huang, and X. Huang, \u201cMachine unlearning by suppressing sample contribution,\u201d 2024. 19   \n[74] H. Hu, Z. A. Salcic, L. Sun, G. Dobbie, P. Yu, and X. Zhang, \u201cMembership inference attacks on machine learning: A survey,\u201d ACM Computing Surveys (CSUR), vol. 54, pp. 1 \u2013 37, 2021. 19   \n[75] M. Fredrikson, S. Jha, and T. Ristenpart, \u201cModel inversion attacks that exploit confidence information and basic countermeasures,\u201d Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security, 2015. 19   \n[76] B. Balle, G. Cherubin, and J. Hayes, \u201cReconstructing training data with informed adversaries,\u201d 2022 IEEE Symposium on Security and Privacy (SP), pp. 1138\u20131156, 2022. 19   \n[77] M. McCloskey and N. J. Cohen, \u201cCatastrophic interference in connectionist networks: The sequential learning problem,\u201d Psychology of Learning and Motivation, vol. 24, pp. 109\u2013165, 1989. 19   \n[78] R. Ratcliff, \u201cConnectionist models of recognition memory: constraints imposed by learning and forgetting functions.\u201d Psychological review, vol. 97 2, pp. 285\u2013308, 1990.   \n[79] L. Wang, X. Zhang, H. Su, and J. Zhu, \u201cA comprehensive survey of continual learning: Theory, method and application,\u201d IEEE transactions on pattern analysis and machine intelligence, vol. PP, 2023. 19   \n[80] N. Carlini, J. Hayes, M. Nasr, M. Jagielski, V. Sehwag, F. Tram\u00e8r, B. Balle, D. Ippolito, and E. Wallace, \u201cExtracting training data from diffusion models,\u201d USENIX Security Symposium, 2023. 19   \n[81] G. E. Hinton, \u201cDeterministic boltzmann learning performs steepest descent in weight-space,\u201d Neural Computation, vol. 1, pp. 143\u2013150, 1989. 19   \n[82] L. Wu, M. Ye, Q. Lei, J. D. Lee, and Q. Liu, \u201cSteepest descent neural architecture optimization: Escaping local optimum with signed neural splitting,\u201d 2021. 19   \n[83] S. Amari, \u201cNatural gradient works efficiently in learning,\u201d Neural Computation, vol. 10, pp. 251\u2013276, 1998. 19   \n[84] R. Shrestha, \u201cNatural gradient methods: Perspectives, efficient-scalable approximations, and analysis,\u201d 2023. 19   \n[85] T. Hartland, G. Stadler, M. Perego, K. Liegeois, and N. Petra, \u201cHierarchical off-diagonal low-rank approximation of hessians in inverse problems, with application to ice sheet model initialization,\u201d Inverse Problems, vol. 39, 2023. 19   \n[86] J. Martens and R. B. Grosse, \u201cOptimizing neural networks with kronecker-factored approximate curvature,\u201d in International Conference on Machine Learning (ICML), 2015. 19   \n[87] Y. Wang, W. Deng, and G. Lin, \u201cAn adaptive hessian approximated stochastic gradient mcmc method,\u201d Journal of Computational Physics, vol. 432, p. 110150, 2020. 19   \n[88] L. Liu, X. Liu, C.-J. Hsieh, and D. Tao, \u201cStochastic optimization for non-convex problem with inexact hessian matrix, gradient, and function,\u201d IEEE transactions on neural networks and learning systems, vol. PP, 2023.   \n[89] X.-T. Yuan and P. Li, \u201cOn convergence of distributed approximate newton methods: Globalization, sharper bounds and beyond,\u201d Journal of Machine Learning Research, 2019. 19   \n[90] E. Berglund and M. Johansson, \u201cNovel limited memory quasi-newton methods based on optimal matrix approximation,\u201d 2024. 19   \n[91] S. J. Wright, \u201cConvergence of projected hessian approximations in quasi-newton methods for the nonlinear programming problem,\u201d Ima Journal of Numerical Analysis, vol. 6, pp. 463\u2013474, 1986. 19   \n[92] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby, \u201cAn image is worth 16x16 words: Transformers for image recognition at scale,\u201d 2021. 21   \n[93] I. Loshchilov and F. Hutter, \u201cDecoupled weight decay regularization,\u201d 2019. 23   \n[94] P. Maini, Z. Feng, A. Schwarzschild, Z. C. Lipton, and J. Z. Kolter, \u201cTofu: A task of fictitious unlearning for llms,\u201d arXiv preprint arXiv:2401.06121, 2024. 25   \n[95] Y. Li, S. Bubeck, R. Eldan, A. Del Giorno, S. Gunasekar, and Y. T. Lee, \u201cTextbooks are all you need ii: phi-1.5 technical report,\u201d arXiv preprint arXiv:2309.05463, 2023. 25   \n[96] B. Liu, Q. Liu, and P. Stone, \u201cContinual learning and private unlearning,\u201d in Conference on Lifelong Learning Agents. PMLR, 2022, pp. 243\u2013254. 25   \n[97] R. Zhang, L. Lin, Y. Bai, and S. Mei, \u201cNegative preference optimization: From catastrophic collapse to effective unlearning,\u201d arXiv preprint arXiv:2404.05868, 2024. 25 ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A Detailed Proof ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Proof of Equation (1) ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Proof. We form the following optimization problem for the steepest descent of approximate MU, which is to find the direction $\\delta\\theta=\\theta_{t+1}-\\theta_{t}$ that drives the objective function $F(\\theta)$ descent fastest within a $\\xi$ -neighborhood of the current parameters $\\theta_{t}$ , and the $\\xi$ -neighborhood is rendered by the manifold metric $\\rho(\\cdot,\\cdot)$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\delta\\theta=\\operatorname*{arg\\,min}_{\\delta\\theta}F(\\theta_{t}+\\delta\\theta)\\quad\\mathrm{s.t.}\\quad\\rho(\\theta_{t},\\theta_{t}+\\delta\\theta)\\leq\\xi.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We introduce a Lagrange multiplier $\\eta\\:\\geq\\:0$ to construct the Lagrangian $\\tilde{\\mathcal{L}}$ for this optimization problem: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\tilde{\\mathcal{L}}(\\delta\\theta,\\eta)=F(\\theta_{t}+\\delta\\theta)+\\eta(\\rho(\\theta_{t},\\theta_{t}+\\delta\\theta)-\\xi).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Using the Karush-Kuhn-Tucker (KKT) theorem, we can take the derivative of $\\tilde{\\mathcal{L}}\\;w.r.t.\\;\\delta\\theta$ and set it to zero: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\nabla_{\\delta\\theta}\\tilde{\\mathcal{L}}(\\delta\\theta,\\eta)=\\nabla_{\\delta\\theta}F(\\theta_{t}+\\delta\\theta)+\\eta\\nabla_{\\delta\\theta}\\rho(\\theta_{t},\\theta_{t}+\\delta\\theta)=0,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\eta$ depends on $\\xi$ and $\\theta_{t}$ implicitly. We can rewrite (A3) by variable substitution $\\theta_{t+1}=\\theta_{t}+\\delta\\theta$ and $\\eta=1/\\alpha_{t}(\\xi,\\theta_{t})$ . ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\nabla_{\\theta_{t+1}}\\tilde{\\mathcal{L}}(\\theta_{t+1},\\eta)=\\nabla_{\\theta_{t+1}}F(\\theta_{t+1})+\\frac{1}{\\alpha_{t}(\\xi,\\theta_{t})}\\nabla_{\\theta_{t+1}}\\rho(\\theta_{t},\\theta_{t+1})=0.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Thus, the original problem is transformed into an unconstrained optimization problem $w$ .r.t. $\\theta_{t+1}$ , where the neighborhood size is implicitly given by $\\alpha_{t}$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\theta_{t+1}=\\operatorname*{arg\\,min}_{\\theta_{t+1}}F(\\theta_{t+1})+\\frac{1}{\\alpha_{t}(\\xi,\\theta_{t})}\\rho(\\theta_{t},\\theta_{t+1}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "A.2 Proof of Proposition 1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The optimization problem of Prop. 1 is to find the steepest descent direction that minimizes the KL divergence with the retrained output within the vicinity of the current model $\\theta_{t}$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\theta_{t+1}=\\arg\\operatorname*{min}_{\\theta_{t+1}}D_{\\mathrm{KL}}\\left(p_{z}(\\theta_{*})||p_{z}(\\theta_{t+1})\\right)+\\frac{1}{\\alpha_{t}}\\rho(\\theta_{t},\\theta_{t+1})\\,}\\\\ {\\,}\\\\ {\\,=\\displaystyle\\arg\\operatorname*{min}_{\\theta_{t+1}}D_{\\mathrm{KL}}\\left(p_{z^{f}}(\\theta_{*})||p_{z^{f}}(\\theta_{t+1})\\right)p^{f}+D_{\\mathrm{KL}}\\left(p_{z^{r}}(\\theta_{*})||p_{z^{r}}(\\theta_{t+1})\\right)p^{r}+\\frac{1}{\\alpha_{t}}\\rho(\\theta_{t},\\theta_{t+1})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proposition 1. Under the Euclidean manifold metric, $\\begin{array}{r}{\\rho(\\theta_{t},\\theta_{t+1})\\,=\\,\\frac{1}{2}\\|\\theta_{t}\\,-\\theta_{t+1}\\|^{2}}\\end{array}$ . Assuming that the current model $\\theta_{t}\\;=\\;\\arg\\operatorname*{min}_{\\theta}\\mathcal{L}^{r}(\\theta)\\,+\\,\\mathcal{L}^{f}(\\theta;\\varepsilon_{t})$ . Let $H_{*}^{f}\\ =\\ \\nabla^{2}\\mathcal{L}^{f}(\\theta_{*};{\\bf1})a n d\\ H_{*}^{r}\\ =$ $\\nabla^{2}\\mathcal{L}^{r}(\\theta_{*})$ denote the Hessian matrix of the retrained model on the forgetting set and the remaining set, respectively. Then, the direction of the steepest gradient descent that minimizes the $K L$ divergence between the output of the current model and the retrained model is approximately: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\theta_{t+1}-\\theta_{t}:\\approx-\\alpha_{t}[H_{*}^{f}(H_{*}^{r})^{-1}[-\\nabla\\mathcal{L}^{f}(\\theta_{t};\\varepsilon_{t})]p^{f}+\\nabla\\mathcal{L}^{r}(\\theta_{t})p^{r}]\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. We can decompose the original optimization problem into three parts: $F(\\theta_{t+1}),R(\\theta_{t+1})$ , and $C(\\theta_{t+1})$ . ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{t=1}^{H}=\\operatorname*{max}_{\\mathbb{R}\\neq\\infty}D_{\\mathbf{x}}\\left(\\rho_{\\mathbb{R}}(\\varepsilon)|\\mathbf{J}_{\\mathbb{R}}(\\theta_{t})|\\mathrm{J}_{\\mathbb{R}}(\\theta_{t+1})\\right)+\\frac{1}{2}\\sum_{t=1}^{H}\\|\\theta_{t}-\\theta_{t+1}\\|^{2}}}\\\\ &{=\\frac{\\alpha_{\\mathrm{pram~in}}}{\\varepsilon_{\\mathrm{pram~in}}}\\int_{\\mathbb{R}^{3}(\\varepsilon)}\\mathrm{d}\\varphi_{\\mathbb{R}}(\\varepsilon),\\quad\\log(\\varepsilon(\\theta_{t+1}))\\|\\tilde{\\mathbf{d}}+\\frac{1}{2\\alpha_{\\mathrm{p}}(\\varepsilon)}|\\mathbf{J}_{\\mathbb{R}}(\\theta_{t}-\\theta_{+1})|}\\\\ &{=\\alpha_{\\mathrm{EqM}}\\int_{\\mathbb{R}^{3}(\\varepsilon)}[\\rho_{\\mathbb{R}}(\\varepsilon)|\\mathrm{J}_{\\mathbb{R}}(\\theta_{t})|\\mathrm{J}_{\\mathbb{R}}(\\theta_{t})-|\\log|\\mathcal{E}_{t}(\\theta_{t+1})|\\mathrm{d}\\varphi_{t}]\\mathrm{d}\\varphi(\\mathbb{R}^{7}|\\mathbb{I})}\\\\ &{\\quad+\\int_{\\mathbb{R}^{3}(\\varepsilon)}\\int_{0}^{\\varepsilon}\\sum_{t=1}^{H}\\log|\\mathcal{E}_{t}(\\theta_{t})|\\mathrm{J}_{\\mathbb{R}}(\\theta_{t})|\\mathrm{d}\\varphi_{t}}\\\\ &{\\quad+\\frac{1}{2}\\sum_{t=1}^{H}|\\theta_{t}-\\theta_{t+1}|^{2}}\\\\ &{=\\alpha_{\\mathrm{EqM}}\\sum_{\\ell\\in\\{t\\}_{0}}\\left[\\log(\\varepsilon^{2}/6)-\\log(\\varepsilon^{2}/6)+\\log(\\varepsilon^{2}/6)\\right]\\exp(|\\mathcal{E}_{t}|\\mathcal{J}|)}\\\\ &{\\quad+\\frac{1}{2\\alpha_{\\mathrm{p}}(\\varepsilon)}\\sum_{t=1}^{H}\\log(\\varepsilon^{2}/6)-\\log(\\varepsilon^{2}/6)+|\\gamma|\\mathcal{E}_{t}|\\mathcal{J}|}\\\\ &{\\quad+\\frac{1}{2\\alpha_{\\mathrm{Eq}}(\\varepsilon)}\\sum_{t=1}^{H}[\\theta_{t}-\\theta_{t+1}]^{2 \n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "(Part I) First, we solve the forgetting part $F(\\theta_{t+1})$ by taking the first-order approximation at $\\theta_{t}$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\nF(\\theta_{t+1})=F(\\theta_{t})+\\nabla F(\\theta_{t})^{\\top}(\\theta_{t+1}-\\theta_{t}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We denote $\\nabla F(\\theta_{t})=-\\mathbb{E}_{p(z^{f};\\theta_{*})}\\left[\\nabla\\log p(z^{f};\\theta_{t})\\right]=G(\\theta_{t}).$ , and then expand $G(\\theta_{t})$ at $\\theta_{*}$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{G(\\theta_{t})=G(\\theta_{*})+\\nabla G(\\theta_{*})(\\theta_{t}-\\theta_{*})}\\\\ &{\\quad\\quad=-\\mathbb{E}_{p(z^{f};\\theta_{*})}\\left[\\nabla\\log p(z^{f};\\theta_{*})\\right]-\\mathbb{E}_{p(z^{f};\\theta_{*})}\\left[\\nabla^{2}\\log p(z^{f};\\theta_{*})\\right]\\Delta_{t}}\\\\ &{\\quad\\quad=0+H_{*}^{f}\\Delta_{t},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $H_{\\ast}^{f}=-\\mathbb{E}_{p(z_{\\ast}^{f};\\theta_{\\ast})}\\left[\\nabla^{2}\\log p(z^{f};\\theta_{\\ast})\\right]$ is the Hessian w.r.t. the retrained model $\\theta_{*}$ at forgetting set, and $\\Delta_{t}=\\theta_{t}\\,\\dot{-}\\,\\theta_{*}$ is the difference between the current model $\\theta_{t}$ and the retrained model $\\boldsymbol{\\theta}_{*}$ . We cannot directly obtain the parameter difference $\\Delta_{t}$ and need to estimate it. Recalling the assumption that $\\theta_{t}=\\arg\\operatorname*{lim}_{\\theta}\\mathcal{L}^{r}(\\theta)\\dot{+}\\mathcal{L}^{f}(\\theta;\\varepsilon_{t})$ and $\\begin{array}{r}{\\theta_{*}=\\arg\\operatorname*{min}_{\\theta}\\mathcal{L}^{r}(\\theta)}\\end{array}$ . We can utilize the optimality of $\\theta_{t}$ on the weighted function to take the derivative w.r.t. $\\theta_{t}$ and set it to zero: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{0=\\nabla\\mathcal{L}^{r}(\\theta_{t})+\\nabla\\mathcal{L}^{f}(\\theta_{t};\\varepsilon_{t})}\\\\ &{\\quad=\\big[\\nabla\\mathcal{L}^{r}(\\theta_{*})+\\nabla^{2}L^{r}(\\theta_{*})\\Delta_{t}+o(\\Delta_{t})\\big]+\\nabla\\mathcal{L}^{f}(\\theta_{t};\\varepsilon_{t})}\\\\ &{\\quad\\approx0+\\nabla^{2}\\mathcal{L}^{r}(\\theta_{*})\\Delta_{t}+\\nabla\\mathcal{L}^{f}(\\theta_{t};\\varepsilon_{t}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Since $\\theta_{*}$ minimizes ${\\mathcal{L}}^{r}$ , $\\nabla{\\mathcal{L}}^{r}(\\theta_{*})=0$ . By performing the Taylor expansion and dropping $o(\\Delta_{t})$ terms, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\Rightarrow\\Delta_{t}\\approx-\\left[\\boldsymbol{\\nabla}^{2}\\mathcal{L}^{r}(\\theta_{*})\\right]^{-1}\\boldsymbol{\\nabla}\\mathcal{L}^{f}(\\theta_{t};\\varepsilon_{t})=-\\left(\\boldsymbol{H}_{*}^{r}\\right)^{-1}\\boldsymbol{\\nabla}\\mathcal{L}^{f}(\\theta_{t};\\varepsilon_{t}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "By plugging (A12) into (A10), we can get ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\nabla F(\\theta_{t})=G(\\theta_{t})\\approx-H_{*}^{f}\\left(H_{*}^{r}\\right)^{-1}\\nabla\\mathcal{L}^{f}(\\theta_{t};\\varepsilon_{t}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Bringing (A13) into (A9), we can get ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{F(\\theta_{t+1})\\approx F(\\theta_{t})-\\left[H_{*}^{f}\\left(H_{*}^{r}\\right)^{-1}\\nabla\\mathcal{L}^{f}(\\theta_{t};\\varepsilon_{t})\\right]^{\\top}(\\theta_{t+1}-\\theta_{t}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "(Part II) Next, we solve the remaining part $R(\\theta_{t+1})$ with similar pipeline in solving $F(\\theta_{t+1})$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R(\\theta_{t+1})=R(\\theta_{t})+\\nabla R(\\theta_{t})^{\\top}(\\theta_{t+1}-\\theta_{t})}\\\\ &{\\qquad\\quad=R(\\theta_{t})-\\mathbb{E}_{p(z^{r};\\theta_{*})}\\left[\\nabla\\log p(z^{r};\\theta_{t})\\right]^{\\top}(\\theta_{t+1}-\\theta_{t})}\\\\ &{\\qquad\\quad=R(\\theta_{t})+\\left[\\nabla{\\mathcal{L}^{r}}(\\theta_{t})\\right]^{\\top}(\\theta_{t+1}-\\theta_{t}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "(Part III) Finally, we derive the constraint $C(\\theta_{t+1})$ as follows, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{C(\\boldsymbol{\\theta}_{t+1})=\\!C(\\boldsymbol{\\theta}_{t})+\\nabla C(\\boldsymbol{\\theta}_{t})^{\\top}(\\boldsymbol{\\theta}_{t+1}-\\boldsymbol{\\theta}_{t})}\\\\ &{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!=C(\\boldsymbol{\\theta}_{t})+2(\\boldsymbol{\\theta}_{t+1}-\\boldsymbol{\\theta}_{t})^{\\top}(\\boldsymbol{\\theta}_{t+1}-\\boldsymbol{\\theta}_{t}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Substituting (A14), (A15), and (A16) to each part in (A8), and take the derivative w.r.t. $\\theta_{t+1}$ of the minimization problem to derive the optimal solution, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle0=\\nabla F(\\theta_{t+1})p^{f}+\\nabla R(\\theta_{t+1})p^{r}+\\frac{1}{2\\alpha_{t}}\\nabla C(\\theta_{t+1})}\\\\ {\\displaystyle\\approx H_{*}^{f}\\left(H_{*}^{r}\\right)^{-1}\\left[-\\nabla\\mathcal{L}^{f}(\\theta_{t};\\varepsilon_{t})\\right]p^{f}+\\nabla\\mathcal{L}^{r}(\\theta_{t})p^{r}+\\frac{1}{\\alpha_{t}}(\\theta_{t+1}-\\theta_{t}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We thus conclude that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Rightarrow\\theta_{t+1}-\\theta_{t}\\approx-\\alpha_{t}\\left[H_{*}^{f}(H_{*}^{r})^{-1}[-\\nabla\\mathcal{L}^{f}(\\theta_{t};\\varepsilon_{t})]p^{f}+\\nabla\\mathcal{L}^{r}(\\theta_{t})p^{r}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "A.3 Proof of Proposition 2 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proposition 2. Using the model output $K L$ divergence on the remaining set as the manifold metric, $\\rho(\\theta_{t},\\theta_{t+1})=D_{K L}\\left(p_{z^{r}}(\\theta_{t})||p_{z^{r}}(\\theta_{t+1})\\right);$ . Assuming that the current model $\\theta_{t}=\\arg\\operatorname*{min}_{\\theta}\\mathcal{L}^{r}(\\theta)+$ $\\mathcal{L}^{f}(\\boldsymbol{\\theta};\\boldsymbol{\\varepsilon}_{t})$ . Let $\\tilde{\\alpha}_{t}={\\alpha_{t}p^{f}}/{(\\alpha_{t}p^{r}+1)},$ , and $H_{t}^{r}=\\nabla^{2}\\mathcal{L}^{r}(\\theta_{t})$ represents the Hessian matrix of the current model on the remaining set, then the direction of the steepest gradient descent that minimizes the $K L$ divergence between the output of the current model and the retrained model is approximately: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\theta_{t+1}-\\theta_{t}:\\approx-\\tilde{\\alpha}_{t}(H_{t}^{r})^{-1}\\left[H_{*}^{f}(H_{*}^{r})^{-1}[-\\nabla\\mathcal{L}^{f}(\\theta_{t};\\varepsilon_{t})]\\right]\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. Now, the steepest descent optimization problem for approximate MU is as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\theta_{t+1}=\\underset{\\theta_{t+1}}{\\arg\\operatorname*{min}}\\,D_{\\mathrm{KL}}\\left(p_{z}(\\theta_{*})||p_{z}(\\theta_{t+1})\\right))+\\frac{1}{\\alpha_{t}}D_{\\mathrm{KL}}\\left(p_{z^{r}}(\\theta_{t})||p_{z^{r}}(\\theta_{t+1})\\right))}\\\\ &{\\quad\\quad=\\underset{\\theta_{t+1}}{\\arg\\operatorname*{min}}\\underbrace{D_{\\mathrm{KL}}\\left(p_{z^{f}}(\\theta_{*})||p_{z^{f}}(\\theta_{t+1})\\right)}_{(F(\\theta_{t+1}))}p^{f}+\\underbrace{D_{\\mathrm{KL}}\\left(p_{z^{r}}(\\theta_{*})||p_{z^{r}}(\\theta_{t+1})\\right)}_{(R(\\theta_{t+1}))}p^{r}}\\\\ &{\\quad\\quad\\quad+\\,\\frac{1}{\\alpha_{t}}\\underbrace{D_{\\mathrm{KL}}\\left(p_{z^{r}}(\\theta_{t})||p_{z^{r}}(\\theta_{t+1})\\right)}_{(C(\\theta_{t+1}))}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The result of forgetting part $F(\\theta_{t+1})$ is the same as that in the Euclidean distance metric. And the remaining part $R(\\theta_{t+1})$ and the constraint $C(\\theta_{t+1})$ vary due to the output KL divergence metric $D_{\\mathrm{KL}}^{r}$ . Note that $\\nabla{\\mathcal{L}}^{r}(\\theta_{t+1})\\approx\\nabla{\\mathcal{L}}^{r}(\\theta_{t})\\approx\\nabla{\\mathcal{L}}^{r}(\\theta_{0})\\approx0$ . This enables us to take the second-order Taylor expansion at $\\theta_{t}$ for the remaining part and the constraint. ", "page_idx": 16}, {"type": "equation", "text": "$$\nR(\\theta_{t+1})=R(\\theta_{t})+\\nabla R(\\theta_{t})^{\\top}(\\theta_{t+1}-\\theta_{t})+\\frac{1}{2}(\\theta_{t+1}-\\theta_{t})^{\\top}\\nabla^{2}R(\\theta_{t})(\\theta_{t+1}-\\theta_{t})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\nabla R(\\theta_{t})=-\\mathbb{E}_{p(z^{r};\\theta_{*})}\\left[\\nabla\\log p(z^{r};\\theta_{t})\\right]=\\nabla\\mathcal{L}^{r}(\\theta_{t})\\approx0\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\nabla^{2}R(\\theta_{t})=-\\mathbb{E}_{p(z^{r};\\theta_{*})}\\left[\\nabla^{2}\\log p(z^{r};\\theta_{t})\\right]=\\nabla^{2}\\mathcal{L}^{r}(\\theta_{t})=H_{t}^{r}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Substituting (A22) and (A23) into (A21), we can derive the remaining part: ", "page_idx": 16}, {"type": "equation", "text": "$$\nR(\\theta_{t+1})\\approx R(\\theta_{t})+\\frac{1}{2}(\\theta_{t+1}-\\theta_{t})^{\\top}H_{t}^{r}(\\theta_{t+1}-\\theta_{t})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "As for the constraint, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{c}{C(\\theta_{t+1})=C(\\theta_{t})+\\nabla C(\\theta_{t})^{\\top}(\\theta_{t+1}-\\theta_{t})+\\frac{1}{2}(\\theta_{t+1}-\\theta_{t})^{\\top}\\nabla^{2}C(\\theta_{t})(\\theta_{t+1}-\\theta_{t})}\\\\ {\\nabla C(\\theta_{t})=-\\mathbb{E}_{p(z^{r};\\theta_{t})}\\left[\\nabla\\log p(z^{r};\\theta_{t})\\right]=0}\\\\ {\\nabla^{2}C(\\theta_{t})=-\\mathbb{E}_{p(z^{r};\\theta_{t})}\\left[\\nabla^{2}\\log p(z^{r};\\theta_{t})\\right]=F_{t}^{r}\\approx H_{t}^{r}=\\nabla^{2}\\mathcal{L}^{r}(\\theta_{t})}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Substituting (A26)and (A27) into (A25), we get the constraint ", "page_idx": 17}, {"type": "equation", "text": "$$\nC(\\theta_{t+1})\\approx C(\\theta_{t})+\\frac{1}{2}(\\theta_{t+1}-\\theta_{t})^{\\top}H_{t}^{r}(\\theta_{t+1}-\\theta_{t}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Bringing (A14), (A24), and (A28) into (A20), and take the derivative w.r.t. $\\theta_{t+1}$ of the minimization problem to derive the optimal solution, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle0=\\nabla F(\\theta_{t+1})p^{f}+\\nabla R(\\theta_{t+1})p^{r}+\\frac{1}{\\alpha_{t}}\\nabla C(\\theta_{t+1})}\\\\ {\\displaystyle\\approx H_{*}^{f}\\left(H_{*}^{r}\\right)^{-1}\\left[-\\nabla\\mathcal{L}^{f}(\\theta_{t};\\varepsilon_{t})\\right]p^{f}+H_{t}^{r}(\\theta_{t+1}-\\theta_{t})p^{r}+\\frac{1}{\\alpha_{t}}H_{t}^{r}(\\theta_{t+1}-\\theta_{t}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By rearranging the terms, we get ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Rightarrow\\theta_{t+1}-\\theta_{t}:\\approx-\\tilde{\\alpha}_{t}(H_{t}^{r})^{-1}\\left[H_{*}^{f}(H_{*}^{r})^{-1}[-\\nabla\\mathcal{L}^{f}(\\theta_{t};\\varepsilon_{t})]\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "A.4 Proof of Proposition 3 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Proposition 3. For implicit Hessian approximation in (5), suppose (A1) $\\beta_{t},\\delta_{t}$ is small, $\\beta_{t}\\ <$ $\\sqrt{\\delta_{t}/|\\nabla\\mathcal{L}^{r}({\\boldsymbol{\\theta}}_{t})-[\\nabla\\mathcal{L}^{r}({\\boldsymbol{\\theta}}_{t})]^{2}|}$ , (A2) ${\\mathcal{L}}^{r}$ is $\\mu$ -smooth, i.e., $\\|\\nabla\\mathcal{L}^{r}(\\theta)-\\nabla\\mathcal{L}^{r}(\\theta^{\\prime})\\|_{2}\\,\\leq\\,\\mu\\|\\theta-\\theta^{\\prime}\\|_{2}$ , and (A3) there exist an $\\zeta_{t}$ -neighborhood $\\mathcal{N}(\\theta_{t}^{r},\\zeta_{t})$ of the optimal model parameter $\\theta_{t}^{r}\\ =$ ar $\\begin{array}{r}{\\mathrm{`g}\\operatorname*{min}_{\\theta_{t}^{f}}\\mathcal{L}^{r}(\\theta_{t}^{f}),}\\end{array}$ , which includes $\\theta_{t}$ and $\\theta_{t}^{f}$ . Then, the iterative update term approximately is, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\theta_{t}-\\theta_{t}^{r}:\\approx\\beta_{t}^{2}\\left[\\nabla^{2}\\mathcal{L}^{r}(\\theta_{t})\\right]^{-1}\\nabla\\mathcal{L}^{u}(\\theta_{t})=\\beta_{t}^{2}(H_{t}^{r})^{-1}\\nabla\\mathcal{L}^{u}(\\theta_{t})\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. The objective function of implicit Hessian approximation can be formulated as: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta_{t}^{f}}\\mathcal{L}^{r}(\\theta_{t}^{r})\\quad\\mathrm{s.t.}\\quad\\theta_{t}^{f}=\\theta_{t}-\\beta_{t}\\nabla\\mathcal{L}^{u}(\\theta_{t}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We need to get the optimal parameter $\\theta_{t}^{r}$ that minimizes (A33), which means $\\begin{array}{r}{0=\\frac{\\partial\\mathcal{L}^{r}(\\theta_{t}^{r})}{\\partial\\theta_{t}^{r}}}\\end{array}$ . We can take the Taylor expansion at $\\theta_{t}$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n0=\\frac{\\partial\\mathcal{L}^{r}(\\theta_{t}^{r})}{\\partial\\theta_{t}^{r}}=\\nabla\\mathcal{L}^{r}(\\theta_{t}^{r})=\\nabla\\mathcal{L}^{r}(\\theta_{t})+H_{t}^{r}(\\theta_{t}^{r}-\\theta_{t})+(\\theta_{t}^{r}-\\theta_{t})^{\\top}\\otimes\\mathbf{T}\\otimes(\\theta_{t}^{r}-\\theta_{t})+o(\\zeta_{t})\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $H_{t}^{r}=\\nabla^{2}\\mathcal{L}^{r}(\\theta_{t})$ and $\\mathbf{T}$ represent the Hessian matrix and the third-order symmetric tensor on the remaining set, respectively, and $\\otimes$ denotes the Kronecker product. ", "page_idx": 17}, {"type": "text", "text": "From (A2) and (A3), we can reduce the first-order term to $o(\\mu\\zeta_{t})$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\lVert\\nabla\\mathcal{L}^{r}({\\boldsymbol{\\theta}}_{t}^{r})-\\nabla\\mathcal{L}^{r}({\\boldsymbol{\\theta}}_{t})\\rVert_{2}\\leq\\mu\\lVert{\\boldsymbol{\\theta}}_{t}^{r}-{\\boldsymbol{\\theta}}_{t}\\rVert_{2}\\leq\\mu\\zeta_{t}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "To simplify the second-order term with (A1), we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad(\\theta_{t}^{r}-\\theta_{t})^{\\top}\\otimes\\mathbf{T}\\otimes(\\theta_{t}^{r}-\\theta_{t})}\\\\ &{=(\\theta_{t}^{f}-\\theta_{t})^{\\top}\\otimes\\mathbf{T}\\otimes(\\theta_{t}^{f}-\\theta_{t})+o(\\epsilon_{t})}\\\\ &{=\\mathbf{C}\\odot(\\theta_{t}^{f}-\\theta_{t})^{2}+o(\\epsilon_{t})}\\\\ &{\\approx\\beta^{2}\\left(\\nabla\\mathcal{L}^{u}(\\theta_{t})\\right)^{2}+o(\\epsilon_{t})}\\\\ &{=\\beta^{2}\\nabla\\mathcal{L}^{u}(\\theta_{t})+o(\\delta_{t})+o(\\epsilon_{t})}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Bringing (A35) and (A36) into (A34), we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{0=\\nabla\\mathcal{L}^{r}(\\theta_{t}^{r})\\approx H_{t}^{r}(\\theta_{t}^{r}-\\theta_{t})+\\beta^{2}\\nabla\\mathcal{L}^{u}(\\theta_{t})+o(\\delta_{t})+o(\\epsilon_{t})+o(\\mu\\epsilon_{t})+o(\\epsilon_{t}^{2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n\\theta_{t}-\\theta_{t}^{r}\\approx\\beta_{t}^{2}(H_{t}^{r})^{-1}\\nabla\\mathcal{L}^{u}(\\theta_{t}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "B Related Works ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "B.1 Related Works on Machine Unlearning ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Data Forgetting. MU is driven by the imperative to remove the influence of specific data from pre-trained models [2, 3, 18, 64], intrinsically linked to differential privacy [18, 64, 65, 15, 66], which aims to enhance the privacy protection of training data. Exact MU, which is approached from a parameter probability perspective, has been thoroughly explored within convex optimization problems and linear models [18, 15, 39, 67, 16]. These studies have established methods allowing models to forget data exactly while adhering to a specified privacy budget, thus significantly reducing the risk of privacy attacks [68\u201372]. However, in deep learning models, exact data forgetting typically requires retraining from scratch [1], a process whose computational intensity makes it impractical for routine application. This challenge underscores an urgent need for developing more efficient unlearning techniques that do not compromise the model\u2019s utility. ", "page_idx": 18}, {"type": "text", "text": "Gradient-based Approximate Machine Unlearning. To enhance data forgetting efficiency, research has honed in on aligning the forgetting objective with the model\u2019s output probability distribution, termed approximate MU [3, 21]. Numerous studies [22, 19, 38, 20, 21, 23, 26, 73] have developed specialized loss functions to prompt the model to expunge specific data, aiming to mitigate the risks of privacy breaches such as membership inference [74, 68\u201372] and data reconstruction attacks [75, 76]. Echoing the phenomenon of catastrophic forgetting observed in continual learning [77\u201379], training on forgetting data has precipitated significant declines in overall performance on remaining data. Various strategies [26, 24, 25] have emerged to uphold the model\u2019s original generalization capabilities, predominantly through fine-tuning the remaining set. Our approach provides a comprehensive analysis of iterative strategies employed in gradient-based approximate MU methods and introduces curvature information from the remaining set to better preserve the model\u2019s generalization capabilities. ", "page_idx": 18}, {"type": "text", "text": "Machine Unlearning for Generative Models. Recent advancements in text-conditional image generation models have demonstrated remarkable capability in producing images that accurately reflect textual descriptions [40, 42, 41, 58]. Despite these achievements, extensive research [6, 12, 80] has underscored significant security and privacy concerns associated with these technologies. The mechanisms underlying these issues are not yet fully understood. In response, there is a critical demand for the development of MU methods to bolster the trustworthiness of these models, facilitating their wider adoption. While pioneering studies [12, 14, 11, 13] have begun to address concept deletion within diffusion models, the dual objectives of maintaining generalization and ensuring efficient data forgetting continue to pose significant challenges. ", "page_idx": 18}, {"type": "text", "text": "B.2 Related Works on Steepest Descent in Optimization ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Steepest Descent and Natural Gradient. Steepest Descent [81, 29, 27, 30, 82] is one of the foundational algorithms in optimization, particularly in the context of machine learning and neural networks. Despite its simplicity and widespread use, Steepest Descent can suffer from slow convergence, especially in ill-conditioned problems where the objective function\u2019s curvature varies significantly across different dimensions [29]. Natural Gradient [83, 30, 84] is proposed as an enhancement of the standard gradient descent that addresses some of its limitations by considering the underlying geometry of the parameter space. Natural Gradient employs the Fisher Information Matrix to scale the gradient adaptively, leading to more efficient optimization. Natural Gradient has been shown to significantly accelerate convergence and improve optimization performance in various applications, including deep learning [30, 27] and continual learning [28]. We get inspiration from these techniques and try to improve MU update direction with preserved set curvature information. ", "page_idx": 18}, {"type": "text", "text": "Hessian Approximation. Computing the exact Hessian matrix is often impractical for large-scale problems due to its computational and memory requirements. To address this, various Hessian Approximation methods have been developed. Notable approaches include Diagonal and Low-rank Approximations [34, 85, 86], Stochastic Approximation [87\u201389], and Quasi-Newton Methods [90, 91]. [50] reveals that the regularization-based method is an explicit approximation of the Hessian, while the meta-learning method is an implicit estimation of it. By leveraging second-order information in a computationally feasible manner, these techniques strike a balance between accuracy and efficiency, facilitating the optimization of large-scale problems. ", "page_idx": 18}, {"type": "text", "text": "B.3 Preliminary on Diffusion Model ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Conditional Diffusion Models and Classifier-free guidance. Diffusion models [40] have gained prominence in the field of generative modeling, particularly for their effectiveness in generating high-quality images. A sample $x_{T}$ is sampled from a Gaussian distribution and gradually denoised for $T$ time steps, finally recovering a clean sample $x_{0}$ . Conditional Diffusion Model [40] is a variant that conditions the generation process on additional information $c$ such as class labels, text descriptions, or other modalities. In practice, the conditional diffusion model is trained to predict the noise $\\epsilon_{\\theta}(x_{t}|\\boldsymbol{c})$ to form the denoising process $p_{\\theta}(x_{t-1}|x_{t},c)$ , where $x_{t}$ is a noisy version of the input $x$ . The corresponding objective of the conditional diffusion model is typically formulated as: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\ell_{\\mathrm{MSE}}(\\theta;x,c)=\\mathbb{E}_{t,\\epsilon\\sim\\mathcal{N}(0,1)}\\left[\\|\\epsilon-\\epsilon_{\\theta}(x_{t}|c)\\|_{2}^{2}\\right],\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "with $t$ uniformly sampled from $\\{1,\\ldots,T\\}$ . In this setting, classifier-free guidance [41] is proposed to encourage the sampling procedure to find $x$ with high $\\operatorname{\\bar{log}}p(c|x)$ . Then diffusion process is given by $\\hat{\\epsilon}_{\\theta}(x_{t}|c)=(1-\\bar{w^{\\prime}})\\epsilon_{\\theta}(\\bar{x_{t}}|\\theta)+w\\epsilon_{\\theta}(x_{t}|c)$ , where $\\hat{\\epsilon}_{\\theta}(x_{t}|\\boldsymbol{c})$ represents the noise estimation obtained by the conditional diffusion model given $c$ , $w\\in[0,1]$ is the guidance weight, $\\varnothing$ denotes the \u2018null\u2019 condition. The generation process initiates with Gaussian noise $\\hat{x}_{T}\\sim\\mathcal{N}(0,1)$ and repeats denoising the data by $\\hat{\\epsilon}_{\\theta}(\\hat{x}_{t}|\\boldsymbol{c})$ to obtain $\\hat{x}_{t-1}$ until $t=0$ , producing the authentic data conditioned on $c$ . ", "page_idx": 19}, {"type": "text", "text": "Latent Diffusion Models. Direct training of conditional diffusion models in high-resolution pixel space is often computationally prohibitive. Latent diffusion models (LDMs) [42, 58] address this challenge with an image compression approach. Specifically, an autoencoder is trained using perceptual loss and a patch-based adversarial objective to master the process of perceptual image compression. The input images can be embedded into smaller latent representations with the learned encoder $E$ . The trained autoencoder facilitates the transition to a low-dimensional latent space where the diffusion model is more efficiently trained on the representations $z=E(x)$ rather than on high-resolution images $x$ . Authentic images can then be generated by sampling a representation $z$ from the diffusion model and subsequently reconstructed into an image with the learned decoder $x=D(z)$ . ", "page_idx": 19}, {"type": "text", "text": "C Algorithm ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We present the algorithm of our proposed SFR-on in Alg. A1. In steps 3-4, we first calculate the forget-remain balanced weight saliency mask. In steps 6-8, we compute the adaptive coefficients to weight the forgetting gradient ascent, followed by one step of forgetting update within the inner loop. In steps 9-13, we fine-tune the model on the remaining set. In step 14, we perform the slow weight update for the model in the outer loop. ", "page_idx": 19}, {"type": "text", "text": "D Evaluation Metrics ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "D.1 Evaluation Metrics for Image Classification ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "\u2022 Forgetting accuracy (FA): FA is the accuracy of the unlearned model on the forgetting dataset $\\mathcal{D}^{f}$ .   \nA favorable approximate MU method should reduce the disparity of FA with the retrained model. \u2022 Remaining accuracy (RA): RA is the accuracy of the unlearned model on the remaining dataset $\\mathcal{D}^{r}$ .   \nA favorable approximate MU method should reduce the disparity of RA with the retrained model. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "\u2022 Testing accuracy (TA): TA is the accuracy of the unlearned model on the testing dataset $\\mathcal{D}^{t}$ . $\\mathcal{D}^{t}$ in random subset forgetting is sampled from the same distribution as the remaining dataset, while in class-wise forgetting $\\mathcal{D}^{t}$ excludes the samples from the forgetting class. A favorable approximate MU method should reduce the disparity of TA with the retrained model. ", "page_idx": 19}, {"type": "text", "text": "\u2022 Membership inference attack success rate on $\\mathcal{D}^{f}$ (MIA): Following [33], we use a prediction entropy-based membership inference attack to evaluate the privacy preservation of the unlearned model. We first need to train an adversarial classifier to predict whether or not a particular example was contained in the training dataset. The prediction of the remaining dataset and the testing dataset by the unlearned model is collected to compute the label-agnostic prediction entropy for the attack classifier training. Specifically, a Logistic Regression classifier is trained on the remaining prediction entropy labeled as \u20181\u2019 and the testing prediction entropy labeled as $\\acute{\\bullet}$ . Then, this attack classifier is ", "page_idx": 19}, {"type": "text", "text": "1: Input: Forgetting set $\\mathcal{D}^{f}$ , remaining set $\\mathcal{D}^{r}$ , pre-trained model weights $\\theta$ , outer loop learning rate   \n$\\alpha$ , inner loop iteration number $T_{\\mathrm{in}}$ , outer loop iteration number $T_{\\mathrm{out}}$ , initial inner loop learning   \nrate for forgetting $\\beta^{f}$ , initial inner loop learning rate for remaining $\\beta^{r}$ , temperature scalar $\\lambda$ ,   \nweight saliency mask threshold $\\gamma$   \n2: Initialize: $\\theta_{0}=\\theta$ .   \n3: Compute $F_{\\mathrm{diag}}^{r},F_{\\mathrm{diag}}^{f}$ by (10).   \n4: Compute weight saliency mask by $\\mathbf{m}=\\mathbf{I}[F_{\\mathrm{diag}}^{f}(F_{\\mathrm{diag}}^{r})^{-1}\\geq\\gamma]$ .   \n5: for $t=1$ to $T_{\\mathrm{out}}$ do   \n6: Sample forgetting sample batch $B_{t}^{f}\\,\\mathbf{from}\\,D^{f}$   \n7: Compute adaptive coefficients $\\tilde{\\varepsilon}_{t-1}$ by (9).   \n8: $\\theta_{t-1}^{f}=\\theta_{t-1}-\\beta^{f}\\nabla\\mathcal{L}^{f}(\\theta_{t-1};\\tilde{\\varepsilon}_{t-1})$   \n9: $\\theta_{0}^{r}=\\theta_{t-1}^{f}$   \n10: for $t^{\\prime}=1$ to $T_{\\mathrm{in}}$ do   \n11: Sample remaining sample batch $B_{t}^{r}$ from $\\mathcal{D}^{r}$   \n12: $\\theta_{t^{\\prime}_{-}}^{r}\\bar{=\\theta_{t^{\\prime}-1}^{r}}-\\beta^{r}\\bar{\\nabla}\\mathcal{L}^{r}(\\bar{\\theta}_{t^{\\prime}-1}^{r})$   \n13: end for   \n14: $\\theta_{t}=\\theta_{t-1}-\\alpha(\\theta_{t-1}-\\theta_{T_{\\mathrm{in}}}^{r})$   \n15: end for ", "page_idx": 20}, {"type": "text", "text": "applied to the forgetting prediction entropy to predict the membership of these forgetting samples. The success rate of membership inference attack on $\\mathcal{D}^{f}$ is quantified by the true positive rate predicted by our classifier, $\\mathrm{MIA}=T P\\dot{/}N^{f}$ , where $T P$ represents the count of forgetting samples still identified as training samples and $N^{f}$ is the size of the forgetting dataset. Due to the limitations of the membership inference attack, the attack based on a linear classifier is weak and fails to distinguish all forgetting samples predicted by the retrained model as test samples. The more advanced shadow model-based sample-wise attack is too time-consuming to evaluate the MU method. Therefore, we only regard MIA as a readout function of the forgetting effect and advocate for the development of more precise and efficient membership inference attack techniques to enhance MU method evaluation. Note that $a$ favorable approximate MU method also should reduce the disparity of MIA with the retrained model. ", "page_idx": 20}, {"type": "text", "text": "\u2022 Output KL divergence with the retrained model $(D_{\\mathrm{KL}})$ : Given that it is impractical to traverse all sample spaces, we actually calculate the empirical output KL divergence with RT $\\boldsymbol{\\theta}_{*}$ . We first collect the predicted class probabilities from both the unlearned and retrained models across the remaining and forgetting datasets. Then, we empirically compute the output KL divergence as follows: ", "page_idx": 20}, {"type": "equation", "text": "$$\nD_{\\mathrm{KL}}=\\frac{1}{N^{r}+N^{f}}\\left(\\sum_{i\\in\\mathcal{D}^{r}}\\sum_{c\\in\\mathcal{C}}p(z_{i,c}^{r};\\theta_{*})\\log\\frac{p(z_{i,c}^{r};\\theta_{*})}{p(z_{i,c}^{r};\\theta_{u})}+\\sum_{j\\in\\mathcal{D}^{f}}\\sum_{c\\in\\mathcal{C}}p(z_{j,c}^{f};\\theta_{*})\\log\\frac{p(z_{j,c}^{f};\\theta_{*})}{p(z_{j,c}^{f};\\theta_{u})}\\right),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\mathcal{C}$ denotes the set of the prediction classes, $\\theta_{u}$ is the unlearned model parameter, and $z_{i,c}^{r}$ and zf $z_{j,c}^{f}$ represent the $i$ -th remaining sample posterior of the class $c$ and the $j$ -th forgetting sample posterior of the class $c$ , respectively. Note that even the output KL divergence between retrained models is not zero due to the randomness of the training algorithm. A favorable approximate $M U$ method is hoped to display as low output $K L$ divergence with RT as possible. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "\u2022 Run-time efficiency (RTE): This measures the computation efficiency of an MU method. We record RTE in minutes. MU methods are more efficient the lower their RTE are. Clearly, RT is not an efficient MU method. ", "page_idx": 20}, {"type": "text", "text": "D.2 Evaluation Metrics for Image Generation ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "\u2022 Forgetting accuracy (FA): We additionally train models to classify the images generated by the unlearned generative model. For CIFAR-10, following [35], we use ResNet-34 from torchvision pre-trained on ImageNet and fine-tune it on CIFAR-10 for 20 epochs. For ImageNet, we directly use pre-trained ViT-L [92] from torchvision as the classifier. We compute the classification accuracy of 500 images generated by the unlearned model on each forgetting category as FA. A favorable approximate MU method is hoped to exhibit as low FA as possible. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "\u2022 Fr\u00e9chet Inception Distance (FID): We evaluate the remaining image fidelity of the unlearned model by assessing the standard image generation metrics FID. The model performed class-wise forgetting of CIFAR-10 generates 1000 images for each remaining class for FID. We sample 10000 images from the unlearned DiT on ImageNet by randomly selecting the remaining classes to calculate FID. A favorable approximate MU method is expected to achieve as high FID as possible. ", "page_idx": 21}, {"type": "text", "text": "E Implementation Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "E.1 Baselines ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "MU methods for image classification: ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "\u2022 FT [22, 19, 38]: It fine-tunes the pre-trained model only on the remaining dataset to obtain the unlearned model. The code source is https://github.com/OPTML-Group/ Unlearn-Sparse.   \n\u2022 GA [20, 21]: It conducts gradient ascent for the pre-trained model only on the forgetting dataset to obtain the unlearned model. The code source is https://github.com/ OPTML-Group/Unlearn-Sparse.   \n\u2022 RL [20]: It replaces the forgotten set label with a random label that is not the same as the original label. Then, the modified forgetting set and the retained set are combined for fine-tuning the pre-trained model with the cross-entropy loss. The code source is https://github.com/OPTML-Group/Unlearn-Saliency.   \n\u2022 SalUn [26]: It first obtained the top- $k\\%$ large salient weight mask sorted by the absolute value of parameter gradient to the forgetting set loss. The pre-trained model is then finetuned using the same pipeline as for RL, and the gradient is modified using the weight saliency mask so that only the top- $k\\%$ significant parameters are optimized. The code source is https://github.com/OPTML-Group/Unlearn-Saliency.   \n\u2022 BT [23]: It stores pre-trained and randomly initialized models as competent and incompetent teachers, respectively. The unlearned model is acquired by minimizing the output KL divergence of the pre-trained model with the incompetent teacher on the forgetting set and with the competent teacher on the remaining set. The code source is https://github. com/vikram2000b/bad-teaching-unlearning.   \n\u2022 SCRUB [25]: It only saves the pre-trained model as the teacher model. Then, it optimizes the pre-trained model to minimize the KL divergence with the teacher on the remaining set and maximize the KL divergence with the teacher on the forgetting set. The cross-entropy loss of the remaining set is added to further maintain the performance. The code source is https://github.com/meghdadk/SCRUB. ", "page_idx": 21}, {"type": "text", "text": "MU methods for image generation: ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "\u2022 SA [35]: It needs to generate replaying remaining samples using pre-trained models in advance and calculate fisher diagonal on the replaying remaining samples for modulating parameter regularization terms. Its unlearning loss is composed of three components: the MSE loss to make the output generated by the pre-trained model for forgetting classes or concepts close to random noise, the generative model MSE loss optimized on the replaying remaining samples, and a parameter regularization term to maintain the pre-trained model parameters with second-order curvature modulation of Fisher diagonal. The code source is https://github.com/clear-nus/selective-amnesia. \u2022 ESD [11]: It saves a frozen SD as a copy. The SD model is then optimized so as to push the condition score for the erasing concept far away from the corresponding condition score generated by the frozen SD, and align with the unconditioned score generated by the copy. The code source is https://github.com/rohitgandikota/erasing. ", "page_idx": 21}, {"type": "table", "img_path": "dheDf5EpBT/tmp/c51c7f6603a0ab68161efabb6cd9e9b5530ef1603de6f4efd71f00d19b1b34b5.jpg", "table_caption": ["Table A1: Summary of hyperparameters for each method on unlearning $10\\%$ random subset of CIFAR-10 in Tab. 2. \u2018lr\u2019 is short for learning rate. "], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "dheDf5EpBT/tmp/66f5b60a963ed532a47598f8d6620fd7f4b73af3ca53c06cd9e6a66977b8519d.jpg", "table_caption": ["Table A2: Summary of hyperparameters for each method on unlearning $10\\%$ random subset of TinyImageNet in Tab. 2. \u2018lr\u2019 is short for learning rate. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "E.2 Training Details for Image Classification ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "For CIFAR-10, CIFAR-100, and SVHN using ResNet-18, all methods use the SGD optimizer with momentum of 0.9, weight decay of $5\\times10^{-4}$ , and batch size of 128. Our SFR-on train 1500 steps with the constant outer loop learning rate of $\\alpha=1.0$ , inner loop iteration number $T_{\\mathrm{in}}=5$ . SFR-on search inner loop learning rate for forgetting in range [0.1, 0.5] and for remaining in range $[10^{-3},10^{-2}]$ , temperature scalar $\\lambda$ in range $[0.0,2.0]$ , and threshold $\\gamma$ in list [0.3, 1.0, 3.0, 10.0]. Experiments are run on 1 RTX 4090. A summary of the hyperparameters for each method is shown in Tab. A1. ", "page_idx": 22}, {"type": "text", "text": "For TinyImageNet, Swin-T is initialized from torchvision weight pre-trained on ImageNet. All methods use the AdamW optimizer [93] with weight decay of 0.05 and batch size of 128. Our SFR-on train 500 steps with the constant outer loop learning rate $\\alpha\\,=\\,1.0$ , inner loop iteration number $T_{\\mathrm{in}}\\,=\\,1$ . SFR-on search inner loop learning rate for forgetting in range $[0.001,0.1]$ and for remaining in range $[10^{-5},10^{-4}]$ , temperature scalar $\\lambda$ in range [0.0, 2.0], and threshold $\\gamma$ in list [0.3, 1.0, 3.0, 10.0]. Experiments are run on 1 RTX 4090. A summary of the hyperparameters for each method is shown in Tab. A2. ", "page_idx": 22}, {"type": "text", "text": "E.3 Training Details for Image Generation ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "For CIFAR-10, following [35], we use DDPM1 based on U-Net architecture with 1000 timesteps for linear $\\beta$ schedule. All methods use Adam optimizer with a constant learning rate of $10^{-4}$ and batch size of 128. Pretrain and RT train for 800K steps. SA generates 5000 images as the remaining set for replaying and calculating the Fisher diagonal, and trains for 20K steps with $\\lambda\\,=\\,10$ for the regularization term. SalUn obtains top- $50\\%$ salient weight mask and trains for 1K steps with $\\alpha=1\\bar{0}^{-3}$ to balance the optimization of forgetting and remaining. Our SFR-on trains for 50 steps ", "page_idx": 22}, {"type": "image", "img_path": "dheDf5EpBT/tmp/11eea9047b183b606b47cda9bd4836f353d533710f9919906ef35e8b213ff857.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Figure A1: Performance of SFR-on with different $\\lambda$ in adaptive coefficients $v s$ RT on CIFAR-10 using ResNet18. The settings and metrics follow Tab. 2. The points closer to RT and with lower $D_{\\mathrm{KL}}$ are better. ", "page_idx": 23}, {"type": "image", "img_path": "dheDf5EpBT/tmp/70a6c0b25b708257cef95fabf5190e1d66e65eb4190d628ef7ebad7f582aed81.jpg", "img_caption": ["Figure A2: Performance of SFR-on with different $\\gamma$ in weight saliency mask vs RT on CIFAR-10 using ResNet18. The settings and metrics follow Tab. 2. The points closer to RT and with lower $D_{\\mathrm{KL}}$ are better. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "with $T_{\\mathrm{in}}=1,\\alpha=1.0,\\beta^{f}=10^{-3},\\beta^{r}=10^{-4},\\lambda=0.5,\\gamma=3.$ . Experiments are run on 2 RTX 4090s. ", "page_idx": 23}, {"type": "text", "text": "For ImageNet, we use pre-trained DiT- $\\mathbf{XL}/2^{2}$ with $256\\times256$ resolution. All methods use AdamW optimizer with a constant learning rate of $10^{-4}$ and batch size of 1. SA calculates the Fisher diagonal on randomly sampled 2000 remaining data, and trains for 10K steps with $\\lambda\\,=\\,10$ for the regularization term. SalUn obtains top- $50\\%$ salient weight mask and trains for 10K steps with $\\alpha=10^{-3}$ to balance the optimization of forgetting and remaining. Our SFR-on trains for 500 steps with $T_{\\mathrm{in}}=1,\\alpha=1.0,\\beta^{f}=10^{-7},\\beta^{r}=\\check{10}^{-4},\\check{\\gamma}=3$ . Since the batch size is 1, we ignore $\\lambda$ in adaptive coefficients. Experiments are run on 1 RTX 4090. ", "page_idx": 23}, {"type": "text", "text": "E.4 Training Details for Concept Forgetting ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "For concept forgetting of \u2018nudity\u2019, following [26], we use SD $\\mathbf{V1.4}^{3}$ to generate 1K images with the prompt \u2018a photo of a nude person\u2019 as the forgetting set and additional 1K images with the prompt \u2018a photo of a person wearing clothes\u2019 as the remaining set. All methods use Adam optimizer with a constant learning rate of $1\\bar{0}^{-5}$ and batch size of 1. ESD trains for 1K steps with negative guidance of 1.0. SalUn trains for 1K steps with $50\\%$ sparsity weight saliency and $\\alpha=0.1$ . Our SFR-on trains for 200 steps with $T_{\\mathrm{in}}=1,\\alpha=1.0,\\beta^{f}=10^{-6},\\beta^{r}=\\bar{10}^{-5},\\gamma=\\bar{3}.$ . Since the batch size is 1, we ignore $\\lambda$ in adaptive coefficients. Experiments are run on 1 RTX 4090. ", "page_idx": 23}, {"type": "text", "text": "F Additional Experimental Results ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "F.1 Ablation Study on Temperature Scalar $\\lambda$ and Threshold $\\gamma$ ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In our SFR-on method, we investigate the impact of two key hyperparameters, temperature scalar $\\lambda$ for adaptive coefficients and threshold $\\gamma$ for the weight saliency mask, on the unlearning performance in image classification. We vary $\\lambda$ within the range $[0,1]$ and $\\gamma$ within [0, 2]. The results in Fig. A1 and A2 indicate that increasing $\\gamma$ and $\\lambda$ enhances model retention capabilities, but it could adversely affect both the forgetting performance and privacy protection. Therefore, we select the hyperparameters for our method based on the goal of minimizing the output KL divergence from Retrain, aligning with the objectives of approximate MU. ", "page_idx": 23}, {"type": "table", "img_path": "dheDf5EpBT/tmp/bc50bcb51931f400d9607337b2c2ca291fb5f0cc84dda5d1b8d1adf74dfba306.jpg", "table_caption": ["Table A3: MU performance for unlearning $50\\%$ random subset of CIFAR-10 using ResNet-18. The content format follows Tab. 2. "], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "dheDf5EpBT/tmp/9fa1adf6b85820e076c95249cb77580e417ec10a4f4111832961fd21db662e38.jpg", "table_caption": ["Table A4: MU performance for unlearning $10\\%$ random subset of CIFAR-100 using ResNet-18. The content format follows Tab. 2. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "F.2 Additional Results for Image Classification ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We conducted experiments to assess the performance of unlearning a $50\\%$ random subset of CIFAR10 and a $10\\%$ random subset on two additional datasets, CIFAR-100 and SVHN. The results in Tab. A3, A4, and A5 demonstrate that our SFR-on method consistently achieves the closest average metric disparity and the smallest KL divergence relative to Retrain across all three scenarios. These findings emphasize the broad unlearning efficacy of our approach. ", "page_idx": 24}, {"type": "text", "text": "F.3 Results for Natural Language Processing ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "The input modality of the model does not constrain our analysis or methods. Therefore, our method can seamlessly extend to other modalities beyond images, such as natural language processing using large language models (LLMs), to achieve efficient forgetting. We conduct experiments using the recently proposed benchmark of TOFU [94] fine-tuned Phi-1.5 [95] to evaluate the effectiveness of our method in the LLM unlearning task, compared with four LLM unlearning baselines: gradient descent(GA), gradient difference(GradDiff [96]), negative preference optimization(NPO [97]), and its enhanced version. The TOFU dataset comprises fictional author biographies, along with questions and answers related to the authors\u2019 attributes, which helps assess methods of data forgetting on fine-tuned LLMs. ", "page_idx": 24}, {"type": "text", "text": "As shown in Tab. A6, our method achieves superior forgetting quality, making the unlearned model almost indistinguishable from the retrained model based on the Truth Ratio distribution of the forget set. Additionally, our method efficiently preserves model utility. ", "page_idx": 24}, {"type": "text", "text": "F.4 Ablation Study on SFR-on without Repairing ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We conduct ablation experiments to assess the performance of our \u2018Sample-wise Adaptive Coefficient for Gradient Ascent (F)\u2019 and \u2018Forget-Remain Balanced Weight Saliency (S)\u2019 in the absence of repairing with the remaining set. The results in Tab. A7 affirm that these components, when used independently, can still enhance baseline performance. ", "page_idx": 24}, {"type": "text", "text": "F.5 Results for Removing Influence of A Single Data Point ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Our method can be directly applied to the task of forgetting a single data point without additional adaptation, as we impose no assumptions to constrain the size of the forgetting set. We conduct experiments under two setups to validate the effectiveness of our method in unlearning either a single ", "page_idx": 24}, {"type": "table", "img_path": "dheDf5EpBT/tmp/c1e1c9cbe018b7d6f12cdc431c05b6c6d9003c1f88cc22a8b71278b50733eace.jpg", "table_caption": ["Table A5: MU performance for unlearning $10\\%$ random subset of SVHN using ResNet-18. The content format follows Tab. 2. "], "table_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "dheDf5EpBT/tmp/90c01b87c5a6aca6437f0890e536f00a9f5555a2a35ced2d691cf1302c5cfffe.jpg", "table_caption": ["Table A6: Unlearning performance of our method and three LLM unlearning baselines on forgetting $5\\%$ of authors or one author under the TOFU fine-tuned Phi-1.5 benchmark. \u2018Forget Quality\u2019 measures the KS-Test value of the Truth Ratio between the unlearned model and the retrained model after the target information is removed. \u2018Model Utility\u2019 evaluates the general performance retained by the unlearned model. RTE is recorded in minutes. "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "data point or a task with suitable granularity: (1) randomly forgetting one sample in the classification task on CIFAR-10 and (2) forgetting the relevant information of one author in TOFU benchmark. ", "page_idx": 25}, {"type": "text", "text": "The results in Tab. A8 and A6 indicate that, under these two settings, our method and all baselines achieve effective forgetting while fully retaining general performance. ", "page_idx": 25}, {"type": "text", "text": "F.6 Verification for Approximation in Practical implementations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Given that the approximation in the theoretical analysis may not hold in practical implementation, verifying the approximation of the formula in the actual operation is necessary. In the proofs of Prop. 1 and 2, considering the approximations of $\\nabla{\\mathcal{L}}^{r}(\\theta_{0})$ , $\\nabla{\\mathcal{L}}^{r}(\\theta_{*})$ , and $\\nabla R(\\theta_{t})$ , we demonstrate in Fig. A3 their gradient norms under practical unlearning settings, which are consistent with our theoretical assumptions. In our method, $\\nabla{\\mathcal{L}}^{r}(\\theta_{0})\\,\\approx\\,\\nabla{\\mathcal{\\dot{L}}}^{r}(\\theta_{*})\\,\\widetilde{\\,}\\approx\\,\\nabla R(\\theta_{t})\\,\\approx\\,0$ , allowing us to incorporate second-order information on the retain set. ", "page_idx": 25}, {"type": "text", "text": "F.7 Additional Results for Image Generation ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "The additional results of class-wise forgetting across all CIFAR-10 classes are presented in Fig. A4, A5, and A6. Our method successfully removes the semantics of the target classes without reducing the forgetting images to low-quality noise. Additionally, SFR-on maintains the generation fidelity of the remaining categories. ", "page_idx": 25}, {"type": "text", "text": "Fig. A7 and A8 display the additional outcomes of class-wise forgetting on ImageNet using DiT by our SFR-on method. The diagonal images within these figures depict the targeted forgetting classes, demonstrating the unlearning efficacy of our method. Conversely, the images off the diagonal represent various non-forgetting categories, showcasing the unlearned DiT\u2019s generalization capability preserved across extensive class conditions. ", "page_idx": 25}, {"type": "text", "text": "G Broader Impacts and Limitations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Broader Impacts. SFR-on can enhance trustworthy deep learning and is broadly applicable across various scenarios, aligning machine learning applications with ethical human standards. SFR-on can potentially increase the fairness of machine learning systems, mitigate biases against minority groups, and promote equitable decision-making processes. Moreover, SFR-on bolsters privacy protections within deep models and fortifies the security of these systems against potential privacy attacks. When deployed on public networks, our approach facilitates the continuous update of knowledge, enabling models to catch new developments without losing performance. In the case of generative models, implementing SFR-on reduces the likelihood of generating improper content and limits the possibility of infringing on intellectual property rights. This helps to enhance public confidence in machine learning technologies and encourages their broader acceptance and use. ", "page_idx": 25}, {"type": "table", "img_path": "dheDf5EpBT/tmp/f4541463d122f821467b31a16d9f08f78def3a8a9e8e4c1da2b70943fd483a27.jpg", "table_caption": ["Table A7: Performance of GA, our SFR-on, and ablations without using the remaining dataset, assessing unlearning $10\\%$ random subset of CIFAR-10 using ResNet-18. "], "table_footnote": [], "page_idx": 26}, {"type": "table", "img_path": "dheDf5EpBT/tmp/ca44e1016ccbee6c2bbbea37d87e7a657323516222edc2767e644e76ff566b47.jpg", "table_caption": ["Table A8: Performance of our method and four baselines in unlearning one sample on CIFAR-10 using ResNet-18. "], "table_footnote": [""], "page_idx": 26}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "Limitations. We acknowledge the limitations of our study and encourage further exploration. While the theoretical framework of SFR-on accommodates various input modalities, this paper does not extend evaluations to include language models or graph neural networks. Consequently, we cannot ascertain the direct applicability of our method to these modalities without adaptation. Furthermore, the absence of an asymptotic analysis for the steepest descent in machine unlearning hinders our ability to determine the disparity between our approximation method and the optimal direction, which is crucial for refining the approach. We advocate for research to address these gaps in the future. ", "page_idx": 26}, {"type": "image", "img_path": "dheDf5EpBT/tmp/078cf040fda0302d3ae4284e2e09d1676901f6bd64515a3030f20098cc37e544.jpg", "img_caption": ["Figure A3: Gradients norm of Pretrain $(\\nabla{\\mathcal{L}}^{r}(\\theta_{0}))$ , Retrain $(\\nabla{\\mathcal{L}}^{r}(\\theta_{*}))$ , Joint $(\\nabla R(\\theta_{t})$ in the proof of Prop. 1), and SFR-on $(\\nabla R(\\theta_{t})$ in the proof of Prop. 2), evaluating forgetting $10\\%$ random subset of CIFAR-10 using ResNet18. "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "dheDf5EpBT/tmp/0cba0ef2ff20639ecc7c896b6b451c93f60f94324a5cbdf80b46032b879a68cf.jpg", "img_caption": ["Figure A4: More class-wise unlearning results on classifier-free guidance DDPM on CIFAR-10. The forgetting class is marked with a red color. (More results will be shown in Fig. A5 and Fig. A6) "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "dheDf5EpBT/tmp/138ad9a7a4172b1c25dcc0432fb1dc6082a4494177154e264324aa6906b4a0a0.jpg", "img_caption": ["Figure A5: More class-wise unlearning results on classifier-free guidance DDPM on CIFAR-10. The forgetting class is marked with a red color (Extended results from Fig. A4). "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "dheDf5EpBT/tmp/24c0e16374edc37f0fe5733f6acc59d681215cb76c85c1062e4c8dbeb9668e16.jpg", "img_caption": ["Figure A6: More class-wise unlearning results on classifier-free guidance DDPM on CIFAR-10. The forgetting class is marked with a red color (Extended results from Fig. A4). "], "img_footnote": [], "page_idx": 29}, {"type": "table", "img_path": "dheDf5EpBT/tmp/5b4f33566ec0327bb0230cac4c6335e889ac4bf07cb48bcc4f6309a0d523eedd.jpg", "table_caption": [], "table_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "dheDf5EpBT/tmp/66666b44e5336145ecdd2ee8df4f65fe78dc91d1f6976a4674bc2bb6dbcc5caa.jpg", "img_caption": ["Figure A7: Additional results of generated images using SFR-on. From the rows below, diagonal images represent the forgetting class, while non-diagonal images represent the remaining class. (More results will be shown in Fig. A8) "], "img_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "dheDf5EpBT/tmp/85e0c7f8f3360215d473c622a37259ab9f04beecf7ed27cd7a4f66123b4feef8.jpg", "img_caption": ["Figure A8: Additional results of generated images using SFR-on. From the rows below, diagonal images represent the forgetting class, while non-diagonal images represent the remaining class. (Extended results from Fig. A7) "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We briefly describe the scope and contributions of this paper in the abstract, detail the areas of focus in the introduction, and list the main contributions at the end of the introduction. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 31}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: We discuss the limitations of our work in Appendix G. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 31}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: All formulas appearing in this paper are reasonably numbered and crossreferenced. We all clarify the theory assumptions in detail and provide complete proof in Appendix A. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 32}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We provide the experimental setup, training details, and hyperparameters in Appendix E to ensure reproducibility. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 32}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: The code is provided in supplemental material. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 33}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: We provide detailed experimental setup, training details, and hyperparameters in Appendix E. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 33}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: This paper provides the mean and variance for most of the experiments on image classification. However, due to the limitation of computing resources, most experiments on image generation have only been carried out once, which may not guarantee the statistically significant results of these experiments. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 33}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 34}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: The computational resources used in the experiments are indicated in Appendix E and the computational time overhead is given. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 34}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: Our submission preserves anonymity. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 34}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We discuss the broader impacts in Appendix G. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 34}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 35}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We have safeguarded the potentially unsafe images appearing in the paper. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 35}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: All original papers and codebases are appropriately cited in this paper. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 36}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 36}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 36}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 36}, {"type": "text", "text": "", "page_idx": 37}]