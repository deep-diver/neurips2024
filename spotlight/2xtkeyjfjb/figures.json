[{"figure_path": "2xTkeyJFJb/figures/figures_4_1.jpg", "caption": "Figure 1: A Seq2Seq encoder-decoder architecture is used to consume queries and produce relevant docids for GR. We employ a multi-graded constrained contrastive loss (Section 4.2) to characterize the relationships among relevance labels based on the relevant and distinct docids (Section 4.1).", "description": "This figure illustrates the overall architecture of the proposed GR2 framework for generative retrieval.  It uses a sequence-to-sequence (Seq2Seq) encoder-decoder model. The encoder processes queries and the decoder generates relevant document identifiers (docids).  The key innovation is the use of a multi-graded constrained contrastive loss function to better handle multi-graded relevance judgments, where documents can have varying degrees of relevance to a given query.  The figure also shows how the framework generates relevant and distinct docids, crucial for effective retrieval, and how the contrastive loss pulls representations of queries and relevant docids closer together based on their relevance grades while pushing away representations of irrelevant docids.", "section": "4 Methodology"}, {"figure_path": "2xTkeyJFJb/figures/figures_8_1.jpg", "caption": "Figure 1: A Seq2Seq encoder-decoder architecture is used to consume queries and produce relevant docids for GR. We employ a multi-graded constrained contrastive loss (Section 4.2) to characterize the relationships among relevance labels based on the relevant and distinct docids (Section 4.1).", "description": "This figure illustrates the overall architecture of the GR2 model proposed in the paper.  It shows a sequence-to-sequence (Seq2Seq) encoder-decoder model that takes a query as input and outputs a ranked list of relevant document identifiers (docIDs). The key innovation is the use of a multi-graded constrained contrastive loss function to model the relationships between different grades of relevance for the docIDs.  The figure also highlights how relevant and distinct docIDs are generated using a regularized fusion approach.  The model pulls the query representation closer to the representations of its relevant docIDs and pushes it away from the irrelevant ones, with the strength of the pull/push determined by the relevance grades.", "section": "4 Methodology"}, {"figure_path": "2xTkeyJFJb/figures/figures_8_2.jpg", "caption": "Figure 2: Ablation analysis. (Left) Supervised learning; (Right) Pre-training and fine-tuning.", "description": "This ablation study analyzes the impact of different components of the GR2 model on its performance. The left panel shows the results for supervised learning, while the right panel presents the results for pre-training and fine-tuning.  The figure compares various configurations of the GR2 model against baselines on different datasets (Gov 500K, MS 500K, etc.) and metrics (nDCG@20, P@20, MRR@3, Hits@10). By systematically removing or modifying specific parts of the GR2 model, such as the regularized fusion approach, the MGCC loss, or the pre-training stage, the authors evaluate the contribution of each component to the overall performance. This helps to demonstrate the effectiveness and robustness of each component within the GR2 framework and highlights the impact of using both supervised learning and pre-training.", "section": "5.2.2 Model ablation"}, {"figure_path": "2xTkeyJFJb/figures/figures_9_1.jpg", "caption": "Figure 4: t-SNE plots of query and document representations for GR2P (left), RIPOR (mid) and NCI (right).", "description": "This figure visualizes the query and document representations learned by three different models: GR2P, RIPOR, and NCI.  t-SNE is used to reduce the dimensionality of the data for visualization. Each point represents a document, colored according to its relevance grade to the query (red: query, green: 2-grade relevant, blue: 1-grade relevant, gray: irrelevant). GR2P shows clear clustering of documents by relevance grade, with higher-grade documents closer to the query. In contrast, RIPOR and NCI show less clear separation, indicating that GR2P better captures the relationships between query and documents across different relevance grades. ", "section": "5.2.4 Visual analysis"}, {"figure_path": "2xTkeyJFJb/figures/figures_17_1.jpg", "caption": "Figure 5: The regularized fusion approach to generate relevant and distinct docids.", "description": "This figure illustrates the architecture of the regularized fusion approach used for docid design. It consists of two main components: a query generation (QG) model and an autoencoder (AE) model. Both models employ an encoder-decoder structure, sharing the same decoder. The QG model takes a document (d) as input, generates a pseudo-query (e<sup>QG</sup>), and reconstructs the original document (d) using the decoder. The AE model takes a query (q) as input, generates a query embedding (e<sup>AE</sup>), and reconstructs the query (q) via the decoder. The training process jointly optimizes both the relevance and distinctness of generated docids by leveraging two regularization terms: L<sub>Rel</sub> which encourages the representation of the document and pseudo-query to be close to each other and L<sub>Div</sub> which pushes away the representations of different documents in the document space and different docids in the docid space.", "section": "4.1 Docid design: regularized fusion approach"}]