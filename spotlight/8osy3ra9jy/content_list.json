[{"type": "text", "text": "Finding Transformer Circuits with Edge Pruning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Adithya Bhaskar Alexander Wettig Dan Friedman Danqi Chen ", "page_idx": 0}, {"type": "text", "text": "Princeton Language and Intelligence (PLI), Princeton University adithyab@princeton.edu {awettig, dfriedman, danqic}cs.@princeton.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The path to interpreting a language model often proceeds via analysis of circuits\u2014 sparse computational subgraphs of the model that capture specific aspects of its behavior. Recent work has automated the task of discovering circuits. Yet, these methods have practical limitations, as they rely either on inefficient search algorithms or inaccurate approximations. In this paper, we frame automated circuit discovery as an optimization problem and propose Edge Pruning as an effective and scalable solution. Edge Pruning leverages gradient-based pruning techniques, but instead of removing neurons or components, it prunes the edges between components. Our method finds circuits in GPT-2 that use less than half the number of edges compared to circuits found by previous methods while being equally faithful to the full model predictions on standard circuit-finding tasks. Edge Pruning is efficient even with as many as 100K examples, outperforming previous methods in speed and producing substantially better circuits. It also perfectly recovers the ground-truth circuits in two models compiled with Tracr. Thanks to its efficiency, we scale Edge Pruning to CodeLlama-13B, a model over $100\\times$ the scale that prior methods operate on. We use this setting for a case study comparing the mechanisms behind instruction prompting and in-context learning. We find two circuits with more than $99.96\\%$ sparsity that match the performance of the full model and reveal that the mechanisms in the two settings overlap substantially. Our case study shows that Edge Pruning is a practical and scalable tool for interpretability and sheds light on behaviors that only emerge in large models.1 ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Mechanistic interpretability strives to understand models via bottom-up descriptions of their components (e.g., attention heads and MLPs in Transformers [Vaswani et al., 2017]). This typically proceeds via the identification and analysis of a circuit [Olah et al., 2020, Elhage et al., 2021]\u2014a sparse computational subgraph of the model that captures the aspects of its behavior we wish to study. The arduous process of identifying circuits (e.g., Wang et al. [2023]) was recently automated by ACDC [Conmy et al., 2023] and EAP [Syed et al., 2023]. However, ACDC uses an expensive greedy search that ablates each edge to estimate its importance. It cannot scale to datasets beyond a few hundred examples or to billion-parameter models. EAP, on the other hand, uses gradient-based linear approximations of activation patching to estimate the importance of all edges simultaneously. While fast, these first-order approximations often sacrifice faithfulness to the full model. Besides, this approach ignores the impact of the presence/absence of other edges on the score. ", "page_idx": 0}, {"type": "text", "text": "In this paper, we frame circuit discovery as an optimization problem and tackle it via gradient-based pruning, rather than discrete search or first-order approximations. As such, we adapt pruning for the goal of circuit discovery instead of model compression. Rather than components, we prune the edges between components and replace missing edges with counterfactual activations from corrupted examples. We enable this by replacing the residual stream of a Transformer (Figure 1a) with a disentangled residual stream [Lindner et al., 2023, Friedman et al., 2023], which retains a list of all previous activations. This allows us to introduce edge masks that determine from which components to read. We then leverage discrete optimization techniques such as $L_{0}$ regularization [Louizos et al., 2018] to optimize these edge masks and produce sparse circuits (Figure 1c). ", "page_idx": 0}, {"type": "image", "img_path": "8oSY3rA9jY/tmp/0a73e9560966c3934bb0a7440ce5d69384ab14ac9bc3961dda7cd605af4fee3f.jpg", "img_caption": ["Figure 1: Edge Pruning disentangles the residual stream and optimizes continuous masks on the read operations via gradient descent. Discretizing the masks to $\\{0,1\\}$ yields the final circuit. The full model corresponds to the case where all masks equal 1. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "We evaluate our approach, Edge Pruning, on four fronts: (1) we measure how faithfully the discovered circuits describe the behavior of the full model, (2) we verify if it can recover ground-truth circuits in Tracr models [Lindner et al., 2023] compiled from known program descriptions, (3) we evaluate how the method scales to more examples and (4) we assess its ability to find extremely sparse circuits in multi-billion parameter models. On four standard circuit-finding tasks, Edge Pruning finds circuits in GPT-2 Small [Radford et al., 2019] that are consistently more faithful to the full model and have better task performance than circuits found by prior methods. The gap is especially pronounced on more complex tasks like multi-template IOI [Wang et al., 2023], where we find circuits that have $2.65\\times$ fewer edges but describe model outputs just as faithfully as the circuit found by the next-best method. We show that Edge Pruning scales effectively to a version of IOI with 100K examples, where it outperforms prior methods in terms of speed and performance. Edge Pruning also perfectly recovers ground-truth circuits in two models compiled from known program descriptions with Tracr. ", "page_idx": 1}, {"type": "text", "text": "Finally, we establish that Edge Pruning scales to CodeLlama-13B [Rozi\u00e8re et al., 2024]\u2014 $.100\\times$ the size of models typically tackled by automated circuit discovery methods\u2014in a case study. Specifically, we compare the mechanisms behind instruction-prompting and in-context learning [Brown et al., 2020] on Boolean Expressions\u2014a task adapted from the BBH [Suzgun et al., 2022] benchmark. Edge Pruning finds circuits with just $0.04\\%$ of model edges that match the model\u2019s performance in either setting. Interestingly, the few-shot circuit performs well when instruction-prompted, and vice versa. The two circuits also have a substantial overlap $62.7\\%$ edges of the sparser circuit), and the circuit formed by this intersection also performs significantly above chance on the task. We infer that the model relies on shared mechanisms in the two settings. This case study demonstrates how Edge Pruning can inform the analysis of phenomena that only emerge in large models. ", "page_idx": 1}, {"type": "text", "text": "In summary, our contributions are as follows: ", "page_idx": 1}, {"type": "text", "text": "1. We propose Edge Pruning, an effective and scalable method for automated circuit finding. ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2. We demonstrate that Edge Pruning is competitive with or better than state-of-the-art methods on simple tasks, and significantly superior on more complex ones, in terms of faithfulness and performance. Edge Pruning also scales well with more examples. Further, it perfectly recovers ground-truth circuits in two Transformers compiled by Tracr. ", "page_idx": 1}, {"type": "text", "text": "3. We scale Edge Pruning to CodeLlama-13B\u2014a model over $100\\times$ larger than GPT-2 Small\u2014 on a task adapted from BBH. Our case study finds that mechanisms underlying in-context learning and instruction-prompting in CodeLlama-13B for this task overlap significantly. ", "page_idx": 2}, {"type": "text", "text": "2 Background: Circuit Discovery ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The goal of circuit discovery is to facilitate a mechanistic understanding of Transformers by identifying the subset of a model\u2019s computational graph that is most relevant to a particular model behavior. In this section, we define the computational graph of a Transformer, formalize the objective for circuit discovery, and discuss the approaches of previous work. ", "page_idx": 2}, {"type": "text", "text": "The computational graph of Transformers. The Transformer architecture consists of a sequence of layers, namely attention layers and MLPs, which operate on the residual stream (Figure 1a) [Elhage et al., 2021]. The $i^{!}$ \u2019th layer $f_{i}$ reads the current state of the residual stream $h_{i}$ , computes its activations $y_{i}=f_{i}(h_{i})$ , and applies it as an additive update to the residual stream $h_{i+1}=h_{i}+y_{i}$ . We can expand this recurrence to make the dependence on prior outputs explicit: ", "page_idx": 2}, {"type": "equation", "text": "$$\ny_{i}=f_{i}\\left(y_{0}+\\sum_{j=1}^{i-1}y_{j}\\right),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $y_{0}$ is the initialization of the residual stream with the input embeddings. We can represent the dependencies between layers as directed edges in a computational graph, where the edge $j\\rightarrow i$ denotes the connection between the output of layer $j$ to the input of layer $i$ . Note that the computational graph may be defined at a more granular level. For instance, Conmy et al. [2023] split attention layers into multiple parallel attention heads, and represents each head by four interconnected nodes. The query/key/value nodes receive separate input edges from previous layers, and the output node has outbound edges to downstream layers. We also follow this convention. ", "page_idx": 2}, {"type": "text", "text": "Circuits as subgraphs. A circuit is a computational subgraph $\\mathcal{C}\\subset\\mathcal{G}$ , where $\\mathcal{C}$ and $\\mathcal{G}$ denote the set of edges in the circuit and full model, respectively [Olah et al., 2020]. How do we model a Transformer with a missing edge $j\\rightarrow i?$ Instead of simply removing the term $y_{i}$ from the sum of inputs to node $i$ , we adopt the approach of interchange ablation [Geiger et al., 2020, Zhang and Nanda, 2024]. For each example $x$ , the user provides a corrupted example $\\tilde{x}$ , which should consist of a small change to $x$ that would result in a different label in the task. We use $\\tilde{x}$ as input to the full model to compute corrupted activations $\\Tilde{y}_{j}$ for all nodes. When an edge $j\\rightarrow i$ is removed from a circuit, we replace the contribution of $y_{j}$ at the input of node $i$ with the corrupted activation $\\Tilde{y}_{j}$ . This ensures that the summed activations remain in-distribution [Zhang and Nanda, 2024] and it frames the decision to remove an edge as a counterfactual intervention [Vig et al., 2020]. ", "page_idx": 2}, {"type": "text", "text": "Circuit discovery. The goal of circuit discovery [Olah et al., 2020] is to find a sparse subgraph that describes the behavior of the full model on a particular task. We use $p c(y\\mid x,\\tilde{x})$ to denote the output of the Transformer circuit given original and corrupted examples $x,\\tilde{x}$ , and denote the output of the full model as $p_{\\mathcal{G}}(y\\mid x)$ as the output of the full model. Formally, circuit discovery has the objective, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\arg\\operatorname*{min}_{\\mathcal{C}}\\mathbb{E}_{(x,\\tilde{x})\\in\\mathcal{T}}\\left[D\\!\\left(p_{\\mathcal{G}}(y\\mid x)\\mid\\mid p_{\\mathcal{C}}(y\\mid x,\\tilde{x})\\right)\\right],\\quad\\mathrm{subject\\;to\\;}1-|\\mathcal{C}|/|\\mathcal{G}|\\ge c\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where the constraint enforces a target sparsity of the circuit. $\\tau$ denotes the task distribution of interest, for which the user curates pairs of clean and corrupted examples $(x,\\tilde{x})$ that differ in crucial task features. The loss function $D$ should capture the discrepancy between the outputs of the full model and the circuit; for language models, a natural choice is the KL divergence between token predictions. ", "page_idx": 2}, {"type": "text", "text": "Previous approaches. We now discuss how previous methods approximate this combinatorial optimization problem and the limitations of their approaches. ", "page_idx": 2}, {"type": "text", "text": "1. ACDC [Conmy et al., 2023] proposes to solve the above objective using greedy search\u2014at each iteration, ACDC evaluates the effect of removing each edge individually, and removes any edge whose effect on the target metric is less than a specified threshold. This fails to capture the relative importance of edges and their interaction. Furthermore, the number of steps of the algorithm scales linearly with the number of edges, which is prohibitive at larger model sizes (e.g., CodeLlama-13B with 3.88M edges). ", "page_idx": 2}, {"type": "text", "text": "2. Edge Attribution Patching (EAP) [Syed et al., 2023] makes a linear (first-order) approximation of activation patching to assign an importance score to each edge. This defines a ranking over edges, from which the top- $k$ edges are used to form a circuit of a specific sparsity. While the linear approximation can compute the importance scores efficiently in a single step, it is likely to find suboptimal solutions to the circuit discovery problem. ", "page_idx": 3}, {"type": "text", "text": "3. Conmy et al. [2023] compare to two pruning-based approaches. These either (1) prune attention heads based on estimated importance scores [Michel et al., 2019], or (2) perform structured pruning of nodes to identify the most important nodes [Cao et al., 2021]. These approaches perform worse than ACDC [Conmy et al., 2023]. Our approach differs in that we prune edges instead of neurons or nodes. This allows us to optimize at a finer granularity but introduces an additional challenge as we will discuss in Section 3. ", "page_idx": 3}, {"type": "text", "text": "3 Method: Edge Pruning ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In structured pruning [Wang et al., 2020, Xia et al., 2022], components such as layers and attention heads are removed to increase the inference efficiency of models. The removal of a component can be modeled by a binary mask, which is relaxed to a continuous parameter to be trainable with gradient-based optimization. While structured pruning produces subgraphs with fewer nodes, they are typically too coarse-grained to help with the mechanistic interpretability of a model\u2019s computations. ", "page_idx": 3}, {"type": "text", "text": "We propose Edge Pruning, where we define masks not over nodes but over the edges connecting them. Specifically, we freeze the original model weights and introduce new trainable parameters $z\\in[0,\\bar{1}]^{|\\mathcal{G}|}$ , where $|g|$ is the number of edges in the Transformer, and the parameter $z_{j i}$ is a relaxed binary mask for the edge $j\\rightarrow i$ . In other words, the pruning mask indicates whether an edge is included $(z_{j i}=1)$ ) or removed $\\langle z_{j i}=0$ ) from the computational graph of a circuit. This formulation allows us to find subgraphs with greater granularity and precision compared to structured pruning, as the number of edges scales quadratically with the number of nodes in a model\u2019s computational graph. ", "page_idx": 3}, {"type": "text", "text": "While structured pruning discards pruned nodes by setting their activation to 0, the application to interpretability calls for more careful treatment of missing nodes and edges. Specifically, the activation of a removed edge $j\\rightarrow i$ should be replaced by the interchange activation obtained from the corrupted version of the example (Section 2). To allow gradient-based optimization, we model the process as the masks continuously interpolating between the clean and corrupted activation. Specifically, we parameterize the $i^{:}$ \u2019th component as, ", "page_idx": 3}, {"type": "equation", "text": "$$\ny_{i}=f_{i}\\left(\\underset{j\\mathrm{~upst}\\;\\mathrm{+}\\;(1-z_{0i})\\tilde{y}_{0}\\mathrm{~+}\\;\\mathrm{~\\sum_{\\scriptstyle~1\\leq~j<i}~}\\;\\mathrm{(}z_{j i}y_{j}\\mathrm{~+~}(1-z_{j i})\\,\\tilde{y}_{j}\\mathrm{)}}{\\mathrm{~}}\\right),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\{\\Tilde{y}_{j}\\}$ denote the corrupted activations corresponding to $\\tilde{x}$ . ", "page_idx": 3}, {"type": "text", "text": "Our formulation has a key challenge. Each node sees a different combination of activations depending on incoming edges, and thus a different residual stream. Thus, we can no longer add the activations $y_{i}$ immediately to the residual stream, i.e. $h_{i+1}=h_{i}+y_{i}$ , as shown in Figure 1a. Instead, we modify the Transformer architecture to retain a so-called disentangled residual stream [Friedman et al., 2023], in which the activations $y_{i}$ are concatenated to a list of all previous activations $(y_{0},y_{1},\\dotsc,y_{i-1})$ . Then, we dynamically aggregate these activations at the input of each node (Equation 3 and Figure 1b). ", "page_idx": 3}, {"type": "text", "text": "In practice, concatenation increases the GPU memory footprint during training compared to regular structured pruning (Appendix A), but it is necessary for optimizing over edges between nodes that are separated by many layers. Despite the memory overhead, we demonstrate in Section 5 that we can scale our method to large models by parallelizing training over multiple GPUs. ", "page_idx": 3}, {"type": "text", "text": "We directly optimize the objective in (2) by performing stochastic gradient descent with respect to the edge weights $_{\\textit{z}}$ . The target sparsity is enforced via $L_{0}$ regularization with a Lagrangian term. We leverage the formulation of Louizos et al. [2018] to model the masks as hard concrete parameters and to circumvent the non-differentiability of the L0 term. At the end of the training, the edge weights are converted to binary masks based on a threshold (e.g., 0.5), which uniquely determines the produced circuit (Figure 1c). We now describe this process in more detail. ", "page_idx": 3}, {"type": "text", "text": "Details of the Edge Pruning process Our formulation of pruning is based on that used by CoFi Pruning [Xia et al., 2022]. Specifically, we model the masks $_{z}$ based on the hard concrete distribution as done by Louizos et al. [2018]: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{u}\\sim\\mathrm{Uniform}(\\epsilon,1-\\epsilon)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{s}=\\sigma\\left(\\frac{1}{\\beta}\\cdot\\frac{\\mathbf{u}}{1-\\mathbf{u}}+\\log\\alpha\\right)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "equation", "text": "$$\n\\tilde{\\mathbf{s}}=\\mathbf{s}\\times(r-l)+l\n$$", "text_format": "latex", "page_idx": 4}, {"type": "equation", "text": "$$\n{\\bf z}=\\operatorname*{min}(1,\\operatorname*{max}(0,\\tilde{{\\bf s}}))\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\sigma$ refers to the sigmoid function, $\\epsilon=10^{-6}$ , and $\\log\\alpha$ indicates that the logarithm is applied element-wise. We fix the temperature $\\textstyle{\\frac{1}{\\beta}}={\\frac{2}{3}}$ . The last two lines stretch the distribution to $\\left[l,r\\right]=$ $[-0.1,1.1]$ and accumulate the \u201cexcess\u201d probability on either side to 0 and 1, respectively. The log alphas $\\log\\alpha$ are the learnable parameters in this formulation. ", "page_idx": 4}, {"type": "text", "text": "Following, Wang et al. [2020], a target sparsity is enforced via a Lagrangian term [Louizos et al., 2018]. If the current sparsity is $s$ , the term, parametrized by a reference value $t$ is ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{s}=\\lambda_{1}\\cdot(t-s)+\\lambda_{2}\\cdot(t-s)^{2}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "$\\lambda_{1}$ and $\\lambda_{2}$ are also updated during training via gradient ascent to keep the regularization tight. We vary the value of $t$ throughout training, linearly increasing it from 0 to a target value, as outlined in Appendix A. Although it may be useful to think of $t$ as a \u201ctarget\u201d sparsity, it is only a number. The runs usually converge to a value slightly below $t$ , so it is prudent to set it to a value greater than 1\u2014although $s$ can then never reach the target value, it will be pushed to higher sparsities. ", "page_idx": 4}, {"type": "text", "text": "We have two sets of masks $z$ . The first set associates a $0-1$ value $z_{e}$ with each edge $e\\equiv(n_{1},n_{2})$ in the computational graph. The second set tags each node of the graph $n$ with a $0-1$ value $z_{n}$ . The latter specifies whether a node is \u201cactive\u201d, i.e., producing output. In effect, the presence of an edge $e\\equiv(n_{1}^{\\bar{}},n_{2})$ is determined by the binary mask ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\tilde{z}_{(n_{1},n_{2})}=z_{(n_{1},n_{2})}\\times z_{n_{1}}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We initially only used edge masks but found that the method would have difficulty converging to high sparsities (i.e., end up at low sparsities). Introducing a second set of masks allows the process to eliminate many edges quickly, accelerating the removal of unimportant components. However, the lagrangian above only applies to the edge masks. This is fine since the node masks can only remove further edges, not introduce new ones on top of those chosen by the edge masks. The final loss is ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\mathcal{L}_{\\mathrm{KL}}+\\mathcal{L}_{\\mathrm{edge},s}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "4.1 Experimental Setup ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Methods. We compare Edge Pruning with a KL loss to ACDC and EAP in our experiments. Both are outlined in Section 2. We do not compare to other pruning-based methods, as Conmy et al. [2023] found them to perform much worse than ACDC. We list the hyperparameters used in Appendix A. The experiments in this section are all performed on GPT-2 Small (117M). ", "page_idx": 4}, {"type": "text", "text": "Tasks. Prior works evaluate their methods on the same examples used to find circuits. In a departure from this convention, we separate each dataset into train, validation, and test splits, to avoid artifacts caused by overfitting. We use the following tasks. ", "page_idx": 4}, {"type": "text", "text": "\u2022 Indirect Object Identification (IOI-t1 and IOI) [Wang et al., 2023] is a task with instances of the format \u201cFriends Juana and Kristi found a mango at the bar. Kristi gave it $t o\\rightarrow$ Juana\u201d. Conmy et al. [2023] use a version with a single template, which we refer to as IOI-t1\u2014this version has 50 examples in each split. We also compare the methods on a variant (IOI) with 30 templates found on HuggingFace2. We randomly select 200 examples each for the train and validation splits, and 36, 084 examples for the test split. ", "page_idx": 4}, {"type": "image", "img_path": "8oSY3rA9jY/tmp/d54a2c96cbc8347eaad5930f1515f62ec09b7f2e38391980ea0b8f9292ce57dd.jpg", "img_caption": ["Figure 2: The faithfulness of the methods, given the KL divergence between the model and obtained circuits (lower is better). On IOI-t1 and GP, Edge Pruning is competitive at low sparsities and better at high sparsities. It outperforms both ACDC and EAP by a significant margin on IOI and GT. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "\u2022 Greater Than (GT) [Hanna et al., 2023] consists of examples of the format \u201cThe war lasted from the year 1743 to $17\\rightarrow x y^{*}$ . The objective of the task is to place a greater probability on the continuations 44, $45,\\ldots,99$ than $00,01,\\ldots,42$ . Our dataset spans 5 templates, 120 choices for nouns, and the years 1100 through 2199. It has 150 examples in the train and validation splits, and 12, 240 examples in the test split. ", "page_idx": 5}, {"type": "text", "text": "\u2022 Gendered Pronoun (GP) [Athwin et al., 2023] consists of statements of the form \u201cSo Evan is a really great friend, isn\u2019t $\\rightarrow\\mathrm{he^{\\circ}}$ . We use the templates from the original Colab notebook used by Athwin et al. [2023], but generate more examples as they only work with 5. We use the top 1, 000 most popular baby names for boys and girls each in the year $2000^{3}$ to generate a dataset with 150 train and validation examples each, and 378 test examples. ", "page_idx": 5}, {"type": "text", "text": "\u2022 Tracr [Lindner et al., 2023] compiles programs written in the RASP [Weiss et al., 2021] programming language into few-layer Transformers. We evaluate Edge Pruning on how well it recovers ground-truth circuits for two Tracr programs\u2014xproportion (proportion of ${\\tt x}$ \u2019s in the prefix) and reverse (reversing a list). Both tasks were discussed in Weiss et al. [2021] and used by Conmy et al. [2023] in their evaluation. ", "page_idx": 5}, {"type": "text", "text": "Evaluation. A circuit is faithful to model behavior on a task if we can corrupt all model edges outside the circuit while retaining the model\u2019s outputs [Hanna et al., 2024]. We corrupt non-circuit edges with interchange ablation and evaluate the methods\u2019 faithfulness as the KL divergence between model and circuit outputs. Specifically, we corrupt an example by swapping the placeholder value in the same template with a random example from the dataset. We appraise the circuits\u2019 performance on IOI-t1, IOI, and GP via the Logit Difference $\\log P({\\mathrm{correct}})\\,-\\,\\log P($ (misleading) between the correct and misleading name/pronoun. For GT, we evaluate the Probability Difference $P(y y+1:99)-P(00:y y-1)$ between the correct and incorrect ranges. All metrics on GT work with predictions restricted to the set $\\{00,01,\\ldots,99\\}$ . We always take unrestricted predictions over the entire model vocabulary on other tasks. All non-Tracr experiments use a GPT-2 Small model. Appendix B evaluates additional metrics\u2014including circuit overlap with manually found circuits. ", "page_idx": 5}, {"type": "image", "img_path": "8oSY3rA9jY/tmp/bb3e7419f0b9841ad398937d5415915d70ec9ed1fa1485e0d35f0a980d83ae4e.jpg", "img_caption": ["Figure 3: Comparison of circuit performance between methods. We report the Logit Difference $\\log P({\\mathrm{correct}})-\\log P$ (misleading) for IOI-t1, IOI and GP, and the probability difference $P(y y+1:$ $99)-P(00:y y-1)$ for GT. Higher is better for all plots. Edge Pruning finds better-performing circuits on all four tasks. The dashed line indicates the performance of the full model. "], "img_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "8oSY3rA9jY/tmp/0f406a98064bbef0d4ba8b7ef157c4b17377aad60433d4af0a3efe20392705ec.jpg", "table_caption": ["Table 1: Scaling to a larger IOI dataset: ACDC improves with more examples but its runtime scales prohibitively. EAP is fast but cannot perform as well. Edge Pruning scales effectively to 100K examples, where it is the fastest and most faithful method. All runs use one NVIDIA H100 GPU. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "4.2 Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "This section compares the three methods on our primary faithfulness and performance metrics. We report additional metrics in Appendix B, and Appendix F shows some circuits found by Edge Pruning. ", "page_idx": 6}, {"type": "text", "text": "Edge Pruning outperforms prior methods on more complex tasks. Edge Pruning is competitive on IOI-t1 and GP in terms of faithfulness at low sparsities, and slightly better at higher sparsities (Figure 2). It is considerably more faithful on IOI and GT than both ACDC and EAP, especially at higher sparsities. In particular, ACDC does worse than randomly choosing between the two names (KL divergence 0.69) at high sparsities on IOI, whereas Edge Pruning remains better. We hypothesize that the relative simplicity of IOI-t1 and GP\u2014one template or small output space (he/she)\u2014renders local (ACDC) or first-order (EAP) approximations good proxies, potentially explaining the edge of Edge Pruning on IOI and GT. A similar trend is seen in performance (Figure 3): Edge Pruning finds better-performing circuits on all four tasks. Specifically, on IOI, Edge Pruning finds a circuit of $\\bar{9}8.8\\%$ sparsity that is as faithful and performs as well as the one found by ACDC at $96.8\\%$ sparsity\u2014using over $2.65\\times$ fewer edges. Interestingly, EAP scales better to higher sparsities than ACDC on GT, delivering respectable performance even at $99.5\\%$ sparsity. ", "page_idx": 6}, {"type": "text", "text": "Edge Pruning can scale to 100K examples. We investigate how the methods scale to more examples at representative sparsities. To this end, we create a large version of the IOI dataset\u2019s train split with 100K examples. We hold the number of gradient descent steps for Edge Pruning fixed (Appendix A). Although its runtime would scale linearly with more epochs, at 100K examples all approaches see almost all examples once.4 Thus, the time reported in Table 1 represents the relative overhead of each method. ACDC shows clear improvements with more examples, but cannot scale well due to prohibitive runtime. EAP, on the other hand, is fast even with more examples. However, it underperforms the other two methods significantly. Edge Pruning efficiently uses more examples and demonstrates both the least runtime and the highest faithfulness by far with 100k examples. We therefore conclude that Edge Pruning is a good fit for complex or mixture distributions where more examples may be needed to specify model behavior. ", "page_idx": 6}, {"type": "image", "img_path": "8oSY3rA9jY/tmp/c7622435ee54bdd91fc17824c4ea768813597ed7391c5726e781f81cf013841f.jpg", "img_caption": ["Figure 4: The canonical ground-truth circuits for the Tracr-compiled xproportion and reverse programs. Edge Pruning recovers both circuits perfectly. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Edge Pruning finds ground-truth circuits in Tracr programs. To check if Edge Pruning can find the ground-truth circuits, we use Tracr [Lindner et al., 2023] to compile two example programs\u2014 xproportion and reverse\u2014as Transformers. The former yields a 2-layer Transformer that outputs, at each position, the fraction of x\u2019s seen so far. The latter yields a 3-layer Transformer that can reverse lists. We use zero ablation following Conmy et al. [2023] (more details in Appendix A). Edge Pruning achieves perfect reconstruction of both circuits (Figure 4). ", "page_idx": 7}, {"type": "text", "text": "Edge Pruning is robust to variance in random initialization Appendix D finds that both the resulting sparsity and the faithfulness of the circuits found by Edge Pruning are remarkably consistent across different random initializations of masks. We also investigate there the question of whether multiple different circuits can exist for a given task, and if Edge Pruning can find them. ", "page_idx": 7}, {"type": "text", "text": "5 Case Study: Scaling to 13B Parameters ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We have seen that Edge Pruning can scale efficiently with more examples. We next investigate if it can scale with model size. This is increasingly important, given the recent interest in interpreting multi-billion parameter models [Lieberum et al., 2023, Prakash et al., 2024]. Current methods used to interpret such models, while undeniably indispensable, have limitations: path patching [GoldowskyDill et al., 2023] identifies important subsets of components but falls short of producing edge-level circuits. Distributed Alignment Search [Geiger et al., 2024, Wu et al., 2023] can verify proposed symbolic execution graphs and align them with the model but requires prior knowledge of the correct symbolic graph, which is nontrivial to obtain. ", "page_idx": 7}, {"type": "text", "text": "On the other hand, pruning can scale to large models using model parallelism [Xia et al., 2024]. We thus apply Edge Pruning to a case study on CodeLlama-13B [Rozi\u00e8re et al., 2024]\u2014a model over $100\\times$ larger than GPT-2\u2014with a real task. We are inspired by Prakash et al. [2024], who compare base and fine-tuned LMs and find that finetuning enhances existing mechanisms. Instead of comparing base and fine-tuned models, we compare mechanisms in the same model with different prompting schemes. Specifically, we ask whether the same mechanisms underlie (zero-shot) instruction prompted and few-shot behavior for the task-model pair we study. This case study serves a dual purpose. It demonstrates the scalability of Edge Pruning as a method. It also illustrates how circuit-finding methods may fti into the interpretability arsenal. We are interested in three research questions: (RQ1) Can Edge Pruning find edge-sparse circuits in a 13B model? (RQ2) To what extent do the circuits for instruction and few-shot prompting share the same edges? (RQ3) Does the instruction-prompted circuit perform well when used in a few-shot manner, and vice versa? ", "page_idx": 7}, {"type": "table", "img_path": "8oSY3rA9jY/tmp/440aa47110608f50f4a4e0724495f1d3fa86426f06e4218403c51496ff9a5319.jpg", "table_caption": ["Table 2: Edge pruning finds circuits with $0.03{-}0.04\\%$ of the edges in CodeLlama-13B that match the performance of the full model. The circuits perform well in cross-evaluation and overlap highly, hinting that the same mechanisms explain large parts of instruction-prompted and few-shot behavior. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Task and model setup. We work with the task Boolean Expressions from the BBH [Suzgun et al., 2022] benchmark suite. This task consists of instances of the form \u201c((not False) and False) or (False and True) $i s\\rightarrow F a l s e^{*}$ . The original dataset only has 250 examples, so we programmatically generate an in-house version of the task. Our dataset has 3840, 767, and 3070 examples in the train, validation, and test splits respectively. Each instance has between 3 and 6 literals, with a maximum nesting depth of 3 and at most 2 consecutive nots. We use 3 demonstrations for the few-shot setting. The prompts used for the instruction-prompted and few-shot settings are provided in Appendix E. Our model is the instruction-finetuned version of CodeLlama-13B.5 It achieves accuracies of $82\\%$ and $89.25\\%$ in the instruction-prompted (IP) and few-shot (FS) settings, respectively. ", "page_idx": 8}, {"type": "text", "text": "(RQ1) Edge Pruning produces extremely sparse circuits. We next apply Edge Pruning to the described settings. We isolate one circuit when instruction prompting and one with the few-shot prompt (hyperparameters in Appendix A, which also highlights other optimizations like distributed training and gradient checkpointing). The circuit discovered in the $\\mathrm{IP}$ setting has $1,041$ edges, corresponding to a $99.97\\%$ edge sparsity. That discovered in the FS setting has $1,464$ edges, equivalent to $\\bar{9}9.96\\%$ edge sparsity. The discovered circuits are evaluated in Table 2. Despite using less than $0.04\\%$ of the edges, the circuits closely match the performance of the full model\u2014the few-shot circuit achieves an accuracy of $87.25\\%$ and performs within $2\\%$ of the full model (when prompted few-shot). The instruction-prompted circuit is accurate within $2.75\\%$ of the full model. ", "page_idx": 8}, {"type": "text", "text": "(RQ2) The circuits have a high overlap, and their intersection performs well. We appraise the intersection of the IP and FS circuits next. The two circuits share 653 edges, accounting for $62.7\\%$ of the edges of the sparser (instruction prompted) circuit\u2014this corresponds to an intersection over 1, $700\\times$ larger than expected by random chance. We further evaluate the circuit formed by this intersection in the instruction prompted and few-shot settings (Table 2). It performs well in the instruction prompted setting, and worse than the model (but still significantly above chance) when prompted few-shot. ", "page_idx": 8}, {"type": "text", "text": "(RQ3) The circuits demonstrate strong performance in cross-evaluation. We note from Table 2 that the circuit found with few-shot prompting shows strong performance even when instruction prompted. Analogously, the instruction-prompted circuit also performs well in the fewshot setting. ", "page_idx": 8}, {"type": "text", "text": "Our case study suggests that the same mechanism (as represented by the intersection above) explains a large part of the performance in both settings\u2014i.e., they do not proceed via disjoint mechanisms. However, the performance gap between the FS and IP \u2229FS circuits is still sizable. Further, we see modest drops in cross-evaluation\u2014e.g., from $87.25\\%$ when evaluating the FS circuit few-shot to $75.75\\%$ in the instruction prompted setting. This suggests that additional components are needed to complete the picture. A complete mechanistic description of the components in the two circuits is an exciting avenue for future work, but beyond the scope of this case study. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Manual analysis of the CodeLlama-13B circuit. Interpreting a circuit in such a large model\u2014even if very sparse\u2014 remains a challenging task. We isolate a small region of the circuit and identify curious behavior in it in Appendix F, leading to an intriguing conjecture. Nonetheless, we believe that a thorough study requires more analysis, which is beyond the scope of this paper (but makes for exciting future work). ", "page_idx": 9}, {"type": "text", "text": "6 Related Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Circuits. By reducing a large model to a sparse subgraph, circuits help interpret internal model computations [Olah et al., 2020, Elhage et al., 2021], and several visualization tools have been developed to aid this process [Sakarvadia et al., 2023, Katz and Belinkov, 2023, Tufanov et al., 2024]. Circuits were originally found manually [Hanna et al., 2023, Athwin et al., 2023], but this has recently been automated by tools like ACDC [Conmy et al., 2023]. ACDC uses activation patching [Vig et al., 2020] to knock out unimportant edges. Other approaches instead estimate the importance of each edge via attribution scores [Nanda, 2022]; this approach was used by EAP [Syed et al., 2023]. Ferrando and Voita [2024] use attribution patching to identify domain-specific model components in Llama-2-7B. Kram\u00e1r et al. [2024] note that attribution patching may lead to incorrect approximations, and propose a variant with reduced error. In concurrent work, Hanna et al. [2024] argue that faithfulness metrics are better for evaluating circuits than measuring overlap with manual circuits. Recent work has explored other notions of a circuit. Inspired by the fact that Sparse Autoencoders (SAEs) can find human-interpretable features in LM activations [Cunningham et al., 2023], Marks et al. [2024] find circuits over these features. Wu et al. [2023] align computation in Alpaca [Taori et al., 2023] with a proposed symbolic algorithm [Geiger et al., 2024]. Our method is orthogonal to these developments. ", "page_idx": 9}, {"type": "text", "text": "Pruning. Pruning [LeCun et al., 1989] drops parameters or layers of a language model for space efficiency and potential speedups. Structured pruning [Wang et al., 2020, Xia et al., 2022] imposes some regularity on the resulting subnetworks, such as an equal fraction of preserved parameters in each layer. Doing so allows it to achieve substantial speedups on GPU hardware at the cost of lower compression. In contrast, unstructured pruning [LeCun et al., 1989, Hassibi and Stork, 1992] does not impose such constraints. Channel pruning [He et al., 2017] is a form of structured pruning that prunes input channels in vision models, which has been adapted for neural architecture search [e.g. Li et al., 2022]. Pruning has occasionally been used as part of an interpretability effort, but mostly at the level of neurons [Michel et al., 2019, Jain et al., 2023], or less commonly, attention heads/MLPs [Cao et al., 2021]. Our work finds circuits by pruning the edges between components instead. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we introduce Edge Pruning to find circuits by pruning edges between components. We find that it discovers sparse, faithful circuits, and we demonstrate its scalability to large datasets and large models. We close by discussing its limitations, and how future work may address them. ", "page_idx": 9}, {"type": "text", "text": "Limitations. We acknowledge that with small datasets, approximation-based approaches like EAP are faster than Edge Pruning. Circuit discovery with Edge Pruning may also require more GPU memory than these methods\u2014especially at scale\u2014where we use $32\\,\\mathrm{Hl00}$ GPUs for CodeLlama-13B (Appendix A). Future work may precede Edge Pruning with a fast, approximate method like EAP to balance efficiency and performance. We note that even at very high sparsities, circuits for large models can still have hundreds of edges, and their full interpretation remains challenging. Further automating interpretability [Bills et al., 2023] is a compelling avenue for future research. Finally, we note that even with perfect faithfulness to the model outputs, a circuit can misrepresent the necessary computations in the full model, thus leading to interpretability illusion [Makelov et al., 2024]. Better metrics are needed to reveal these possibilities in practice. ", "page_idx": 9}, {"type": "text", "text": "Societal and ethical impact. Our work aims to facilitate the process of understanding and explaining large foundation models, which is crucial for their continued safe development and deployment. We do not foresee Edge Pruning being used towards adverse societal or ethical ends. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We are thankful to Tianyu Gao, Zirui Wang, and Mengzhou Xia for their helpful discussions regarding the experiments. Input by Abhishek Panigrahi and Carlos Jiminez was also instrumental to this project. We also thank Howard Chen and Tianyu Gao for their help in proofreading and improving the writing in this paper. AB gratefully acknowledges the support of a Hisashi and Masae Kobayashi $^{\\ast}67$ Fellowship. This research is also funded by the National Science Foundation (IIS-2211779) and a Sloan Research Fellowship. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Chris Athwin, Guillaume Corlouer, Esben Kran, Fazl Barez, and Neel Nanda. Identifying a preliminary circuit for predicting gendered pronouns in GPT-2 small, 2023. URL https: //cmathw.itch.io/identifying-a-preliminary-circuit-for-predicting-gen dered-pronouns-in-gpt-2-smal/. ", "page_idx": 10}, {"type": "text", "text": "Steven Bills, Nick Cammarata, Dan Mossing, Henk Tillman, Leo Gao, Gabriel Goh, Ilya Sutskever, Jan Leike, Jeff Wu, and William Saunders. Language models can explain neurons in language models. https://openaipublic.blob.core.windows.net/neuron-explainer/paper/i ndex.html, 2023. ", "page_idx": 10}, {"type": "text", "text": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 1877\u20131901. Curran Associates, Inc., 2020. URL https://proceedings.ne urips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Pap er.pdf. ", "page_idx": 10}, {"type": "text", "text": "Steven Cao, Victor Sanh, and Alexander Rush. Low-complexity probing via finding subnetworks. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou, editors, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 960\u2013966, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.74. URL https://aclanthology.org/2021. naacl-main.74. ", "page_idx": 10}, {"type": "text", "text": "Arthur Conmy, Augustine N. Mavor-Parker, Aengus Lynch, Stefan Heimersheim, and Adri\u00e0 GarrigaAlonso. Towards automated circuit discovery for mechanistic interpretability. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net /forum?id $\\equiv$ 89ia77nZ8u. ", "page_idx": 10}, {"type": "text", "text": "Hoagy Cunningham, Aidan Ewart, Logan Riggs, Robert Huben, and Lee Sharkey. Sparse autoencoders find highly interpretable features in language models, 2023. ", "page_idx": 10}, {"type": "text", "text": "Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. A mathematical framework for Transformer circuits. Transformer Circuits Thread, 2021. https://transformer-circuits.pub/2021/framework/index.html. ", "page_idx": 10}, {"type": "text", "text": "Javier Ferrando and Elena Voita. Information flow routes: Automatically interpreting language models at scale, 2024. ", "page_idx": 10}, {"type": "text", "text": "Dan Friedman, Alexander Wettig, and Danqi Chen. Learning Transformer programs. In Thirtyseventh Conference on Neural Information Processing Systems, 2023. URL https://openrevi ew.net/forum?id=Pe9WxkN8Ff. ", "page_idx": 10}, {"type": "text", "text": "Atticus Geiger, Kyle Richardson, and Christopher Potts. Neural natural language inference models partially embed theories of lexical entailment and negation. In Afra Alishahi, Yonatan Belinkov, Grzegorz Chrupa\u0142a, Dieuwke Hupkes, Yuval Pinter, and Hassan Sajjad, editors, Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 163\u2013173, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/ 2020.blackboxnlp-1.16. URL https://aclanthology.org/2020.blackboxnlp-1.16.   \nAtticus Geiger, Zhengxuan Wu, Christopher Potts, Thomas Icard, and Noah Goodman. Finding alignments between interpretable causal variables and distributed neural representations. In Francesco Locatello and Vanessa Didelez, editors, Proceedings of the Third Conference on Causal Learning and Reasoning, volume 236 of Proceedings of Machine Learning Research, pages 160\u2013 187. PMLR, 01\u201303 Apr 2024. URL https://proceedings.mlr.press/v236/geiger24a.h tml.   \nNicholas Goldowsky-Dill, Chris MacLeod, Lucas Sato, and Aryaman Arora. Localizing model behavior with path patching, 2023.   \nMichael Hanna, Ollie Liu, and Alexandre Variengien. How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?i d=p4PckNQR8k.   \nMichael Hanna, Sandro Pezzelle, and Yonatan Belinkov. Have faith in faithfulness: Going beyond circuit overlap when finding model mechanisms, 2024.   \nBabak Hassibi and David Stork. Second order derivatives for network pruning: Optimal brain surgeon. In S. Hanson, J. Cowan, and C. Giles, editors, Advances in Neural Information Processing Systems (NeurIPS), volume 5. Morgan-Kaufmann, 1992. URL https://proceedings.neurips.cc/p aper_files/paper/1992/file/303ed4c69846ab36c2904d3ba8573050-Paper.pdf.   \nYihui He, Xiangyu Zhang, and Jian Sun. Channel pruning for accelerating very deep neural networks. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2017.   \nSamyak Jain, Robert Kirk, Ekdeep Singh Lubana, Robert P. Dick, Hidenori Tanaka, Edward Grefenstette, Tim Rockt\u00e4schel, and David Scott Krueger. Mechanistically analyzing the effects of fine-tuning on procedurally defined tasks, 2023.   \nShahar Katz and Yonatan Belinkov. VISIT: Visualizing and interpreting the semantic information flow of Transformers, 2023.   \nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann LeCun, editors, ICLR (Poster), 2015. URL http://dblp.uni-trier.de/db/conf/ iclr/iclr2015.html#KingmaB14.   \nJ\u00e1nos Kram\u00e1r, Tom Lieberum, Rohin Shah, and Neel Nanda. AtP\\*: An efficient and scalable method for localizing LLM behaviour to components, 2024.   \nYann LeCun, John Denker, and Sara Solla. Optimal brain damage. In D. Touretzky, editor, Advances in Neural Information Processing Systems (NeurIPS), volume 2. Morgan-Kaufmann, 1989. URL https://proceedings.neurips.cc/paper_files/paper/1989/file/6c9882bbac1c7 093bd25041881277658-Paper.pdf.   \nYanyu Li, Pu Zhao, Geng Yuan, Xue Lin, Yanzhi Wang, and Xin Chen. Pruning-as-Search: Efficient neural architecture search via channel pruning and structural reparameterization. In Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22, 2022.   \nTom Lieberum, Matthew Rahtz, J\u00e1nos Kram\u00e1r, Neel Nanda, Geoffrey Irving, Rohin Shah, and Vladimir Mikulik. Does circuit analysis interpretability scale? Evidence from multiple choice capabilities in Chinchilla, 2023.   \nDavid Lindner, Janos Kramar, Sebastian Farquhar, Matthew Rahtz, Thomas McGrath, and Vladimir Mikulik. Tracr: Compiled Transformers as a laboratory for interpretability. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net /forum?id $\\equiv$ tbbId8u7nP.   \nChristos Louizos, Max Welling, and Diederik P. Kingma. Learning sparse neural networks through l_0 regularization. In International Conference on Learning Representations, 2018. URL https: //openreview.net/forum?id=H1Y8hhg0b.   \nAleksandar Makelov, Georg Lange, Atticus Geiger, and Neel Nanda. Is this the subspace you are looking for? An interpretability illusion for subspace activation patching. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net /forum?id $\\equiv$ Ebt7JgMHv1.   \nSamuel Marks, Can Rager, Eric J. Michaud, Yonatan Belinkov, David Bau, and Aaron Mueller. Sparse feature circuits: Discovering and editing interpretable causal graphs in language models, 2024.   \nThomas McGrath, Matthew Rahtz, Janos Kramar, Vladimir Mikulik, and Shane Legg. The hydra effect: Emergent self-repair in language model computations, 2023.   \nPaul Michel, Omer Levy, and Graham Neubig. Are sixteen heads really better than one? In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper_files/paper/2019/file/2c601ad9d 2ff9bc8b282670cdd54f69f-Paper.pdf.   \nNeel Nanda. Attribution patching: Activation patching at industrial scale, 2022. URL https: //www.neelnanda.io/mechanistic-interpretability/attribution-patching.   \nChris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, and Shan Carter. Zoom in: An introduction to circuits. Distill, 2020. doi: 10.23915/distill.00024.001. URL https://distill.pub/2020/circuits/zoom-in.   \nNikhil Prakash, Tamar Rott Shaham, Tal Haklay, Yonatan Belinkov, and David Bau. Fine-tuning enhances existing mechanisms: A case study on entity tracking, 2024.   \nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. arXiv, 2019.   \nBaptiste Rozi\u00e8re, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, J\u00e9r\u00e9my Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre D\u00e9fossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. Code Llama: Open foundation models for code, 2024.   \nMansi Sakarvadia, Arham Khan, Aswathy Ajith, Daniel Grzenda, Nathaniel Hudson, Andr\u00e9 Bauer, Kyle Chard, and Ian Foster. Attention Lens: A tool for mechanistically interpreting the attention head information retrieval mechanism. In NeurIPS Workshop on Attributing Model Behavior at Scale, 2023. URL https://openreview.net/forum?id $\\equiv$ 5CDRc8VMhS.   \nMirac Suzgun, Nathan Scales, Nathanael Sch\u00e4rli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V. Le, Ed H. Chi, Denny Zhou, and Jason Wei. Challenging BIG-bench tasks and whether chain-of-thought can solve them, 2022.   \nAaquib Syed, Can Rager, and Arthur Conmy. Attribution patching outperforms automated circuit discovery. In NeurIPS Workshop on Attributing Model Behavior at Scale, 2023. URL https: //openreview.net/forum?id $=$ tiLbFR4bJW.   \nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023.   \nIgor Tufanov, Karen Hambardzumyan, Javier Ferrando, and Elena Voita. LM transparency tool: Interactive tool for analyzing Transformer language models, 2024. ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.ne urips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Pap er.pdf. ", "page_idx": 13}, {"type": "text", "text": "Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Yaron Singer, and Stuart Shieber. Investigating gender bias in language models using causal mediation analysis. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 12388\u201312401. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/92650b2e9 2217715fe312e6fa7b90d82-Paper.pdf. ", "page_idx": 13}, {"type": "text", "text": "Kevin Ro Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt. Interpretability in the wild: A circuit for indirect object identification in GPT-2 small. In The Eleventh International Conference on Learning Representations, 2023. URL https://openrevi ew.net/forum?id=NpsVSN6o4ul. ", "page_idx": 13}, {"type": "text", "text": "Ziheng Wang, Jeremy Wohlwend, and Tao Lei. Structured pruning of large language models. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6151\u20136162, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emn lp-main.496. URL https://aclanthology.org/2020.emnlp-main.496. ", "page_idx": 13}, {"type": "text", "text": "Gail Weiss, Yoav Goldberg, and Eran Yahav. Thinking like Transformers. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 11080\u201311090. PMLR, 18\u201324 Jul 2021. URL https://proceedings.mlr.press/v139/weiss21a.html. ", "page_idx": 13}, {"type": "text", "text": "Zhengxuan Wu, Atticus Geiger, Christopher Potts, and Noah Goodman. Interpretability at scale: Identifying causal mechanisms in alpaca. arXiv, 2023. ", "page_idx": 13}, {"type": "text", "text": "Mengzhou Xia, Zexuan Zhong, and Danqi Chen. Structured pruning learns compact and accurate models. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1513\u20131528, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.107. URL https://aclanthology.org/2022.acl-long.107. ", "page_idx": 13}, {"type": "text", "text": "Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi Chen. Sheared LLaMA: Accelerating language model pre-training via structured pruning. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id $\\equiv$ 09iOdaeOzp. ", "page_idx": 13}, {"type": "text", "text": "Fred Zhang and Neel Nanda. Towards best practices of activation patching in language models: Metrics and methods. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id $\\mathbf{\\mu=}$ Hf17y6u9BC. ", "page_idx": 13}, {"type": "text", "text": "Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, Alban Desmaison, Can Balioglu, Pritam Damania, Bernard Nguyen, Geeta Chauhan, Yuchen Hao, Ajit Mathews, and Shen Li. Pytorch FSDP: Experiences on scaling fully sharded data parallel, 2023. ", "page_idx": 13}, {"type": "image", "img_path": "8oSY3rA9jY/tmp/498e6068bdd27b81d44eab5d51526501ade4df197eddced37302b718864b1f04.jpg", "img_caption": ["Figure 5: Our secondary metric for measuring faithfulness is the Exact Match percentage between the model and circuit predictions on IOI-t1, IOI, and GP. On GT, we use the Kendall\u2019s Tau score between the model and circuit rankings of $00,01,\\dots,99$ as the secondary metric. Edge Pruning is the most faithful method on all four tasks, with the difference being especially pronounced for IOI. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "A Hyperparameters and Computational Details for Edge Pruning ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this appendix, we list the hyperparameters used for the various experiments in the main text of the paper. All of our runs use the Adam [Kingma and Ba, 2015] optimizer with $\\epsilon\\,=\\,10^{-8}$ and $(\\beta_{1},\\bar{\\beta_{2})}=(0.9,0.999)$ . ", "page_idx": 14}, {"type": "text", "text": "GPT-2 experiments For all tasks, we used a sequence length of 64 tokens with padding. A batch size of 32 was adopted, and the learning rate for both the edge and node masks, as well as for the lagrangians $\\lambda$ for both, was set to 0.8. IOI-t1 was an exception: here, we set all the above learning rates to 1 for all runs. The total number of optimization steps was 3000, and the target edge and node sparsities were linearly increased starting from 0 over the first 2500 steps. Evaluation and checkpointing were performed every 64 steps but we always used the final checkpoint to report results. To produce the scatterplots, we varied the edge target up to 1.1 but held the node target largely fixed for each task. These values were 0.72 for IOI-t1 and IOI, 0.68 for GT and 0.69 for GP. These values were chosen based on a small number of pilot runs, and we expect that a grid search can improve results further. ", "page_idx": 14}, {"type": "text", "text": "We also wish to make several remarks about our implementation. We turned off dropout for all runs since it made the optimization noisy. Our threshold for the final rounding is not a pre-determined value. Instead, we compute the average value of all entries of all masks, and brand that the desired sparsity. Then, we perform a binary search for a threshold such that the fraction of entries rounded to 1 equals this desired sparsity. The thresholds found this way usually fell between 0.2 and 0.8. This also allows the user to achieve exactly the desired sparsity by setting a different threshold. We implement all of our code by implementing modified versions of the HuggingFace model classes, as it allows us to use the HuggingFace Trainer and its optimizations out of the box. Our code also natively supports Flash Attention, though none of our results use it. Finally, we note that the role of $\\lambda_{1}$ in the lagrangian term is to allow (and indeed, encourage), \u201cshooting past\u201d $t$ when optimizing $s$ due to momentum. This prevents the model sparsities from \u201csettling into\u201d a mode where they lag behind the targets by a constant but non-zero amount throughout pruning. ", "page_idx": 14}, {"type": "text", "text": "Tracr experiments For both programs, we fix the $\\lambda_{1}$ values to 0 and only optimize $\\lambda_{2}$ , as described in Section 3. For the xproportion program, we use an edge target of 0.92 and a node target of 0.4. The edge and node mask learning rates were 1, and that for the lambdas was 0.0001. A total of 720 optimization steps were performed with a batch size of 16, of which 640 was used for target warmup. The learning rates were warmed up linearly over the first 96 steps. A sequence length of 5 was used. ", "page_idx": 15}, {"type": "text", "text": "Initially, for reverse, setting the regularization learning rate was tricky\u2014it was easy to end up not regularizing enough or overdoing it. Thankfully, an easy remedy was to increase the number of steps to 6000 (of which the first 5900 warmed up the edge and node targets, and the first 1500 warmed up the learning rates). This allowed us to set a relatively higher learning rate for the lambdas (0.001), along with an aggressive edge target of 1.02. The node target was set to 0.1. The learning rates of the log alphas and lambdas were 0.03 and 0.001, respectively. Despite using 6000 steps, the run took under 5 minutes on one NVIDIA A100. ", "page_idx": 15}, {"type": "text", "text": "CodeLLama-13B experiments For our CodeLlama-13B experiments, we use a learning rate of 0.8 for both the edge masks and the node masks. In a departure from the choice of Section 3, we also include a separate lagrangian term over node masks: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{node},s}=\\lambda_{1,\\mathrm{node}}\\cdot(t_{\\mathrm{node}}-s_{\\mathrm{node}})+\\lambda_{2,\\mathrm{node}}\\cdot(t_{\\mathrm{node}}-s_{\\mathrm{node}})^{2}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The reason for this choice was that, in our preliminary runs with small Sheared Llama [Xia et al., 2024], we found that this would achieve higher sparsities. We use a learning rate of 0.4 for all of the lambdas. The target edge and node sparsities are set to 1.2 and 0.7, respectively. We use 6000 steps with a batch size of 1. The first 200 steps linearly warm up the learning rate, while the target sparsities are linearly increased over the first 5500 steps. We enable gradient checkpointing, as well as FSDP [Zhao et al., 2023] with full sharding in BF16 precision. The maximum sequence lengths for the instruction-prompted and few-shot settings were 64 and 72, respectively. ", "page_idx": 15}, {"type": "text", "text": "We also comment here on the computational resources used for the runs. ", "page_idx": 15}, {"type": "text", "text": "Computational details. The Tracr experiments use one NVIDIA A100 with 80 GB of memory. The GPT-2 experiments use either one NVIDIA A100 or one H100 (both 80 GB) each. The experiments of Table 1 all use one NVIDIA H100 for a fair runtime comparison. Each CodeLlama-13B run utilizes 32 H100 GPUs and 600 gigabytes of CPU memory. The typical runtime of a GPT-2 pruning run was about 45 minutes, and that of a Tracr run was under 5 minutes. The CodeLlama runs each took around 35 hours. We estimate the total computational budget to be around 5000 GPU hours. ", "page_idx": 15}, {"type": "text", "text": "B More results ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We show more results on faithfulness and performance metrics in this appendix. Specifically, we evaluate on one alternate faithfulness (agreement) metric and one additional performance metric. For the former, we choose Exact Match percentage as the agreement metric on IOI-t1, IOI and GP. For GT, we instead report the Kendall\u2019s Tau score over the rankings of $00,01,02,\\ldots,99$ as induced by the output logits of the model and circuit, which is then averaged across examples. Figure 5 plot these metrics for the three approaches. We see that Edge Pruning is consistently the most faithful method on all four tasks, with the gap to the next-best method being large for IOI. ", "page_idx": 15}, {"type": "text", "text": "Our choice of the performance metric is Accuracy for IOI-t1, IOI and GP. For GT, we instead compute a variant of Probability Difference called Probability Difference 10, given by $P(y y+1:y y+1{\\bar{0}})-$ $P(y y-10:y y-1)$ . Note that the original probability difference, $P(y y+1:99)-P(00:y y-1)$ can be gamed by always predicting 99. The new variant overcomes this obstacle by measuring the sharpness of the cutoff. The results, shown in Figure 6, echo the results of the main text: edge pruning is competitive on GP, and outperforms the other methods in IOI-t1, IOI and GT. ", "page_idx": 15}, {"type": "text", "text": "We also compare Edge Pruning to ACDC in terms of circuit overlap with manually reverse-engineered circuits. Since the manual circuits only identified important components and not the edges between them, we plot node (component) ROC curves in Figure 7, where we consider a node included in a circuit if at least one edge incident to it is included. Note that the IOI manual circuit only studied attention heads, so we ignore MLP nodes in the corresponding circuits. The results show that Edge Pruning is competitive with ACDC on circuit overlap metrics. Nevertheless, we emphasize that manually reverse-engineered circuits are not guaranteed to be optimal since they also investigate one ablation at a time without considering interactions between ablations. As such, we echo Hanna et al. [2024]\u2019s suggestions of using circuit faithfulness metrics over circuit overlap. ", "page_idx": 15}, {"type": "image", "img_path": "8oSY3rA9jY/tmp/9b4523420453e7ee916b787d4c568383caa9c403ebec02c2f903b67640ee3abf.jpg", "img_caption": ["Figure 6: Comparison of the various methods on our secondary performance metric\u2014accuracy in the case of IOI-t1, IOI and GP, and Probability Difference 10 for GT (given by $P(y y+1:$ $y y+10)-P(y y-10:y y-1))$ . Once again, Edge Pruning is competitive on GP, and outperforms other methods on IOI-t1, IOI and GT. The dashed lines indicate full model performance. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "8oSY3rA9jY/tmp/b72871d21e2b291b4eaf4c4c1d4b62cf0095bf1d42373571c41d556cc7c6e053.jpg", "img_caption": ["Figure 7: ROC curves against manual circuits for Edge Pruning and ACDC. The AUC is slightly higher for Edge Pruning on IOI, and slightly lower on GT. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "C Edge Faithfulness ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In other sections and appendices, we have taken up the output faithfulness of Edge Pruning, i.e., whether the output distribution of the circuits matches that of the model. Here, we consider another edge faithfulness\u2014an edge important for the model should also be important for the circuit. Concretely, given a circuit $C$ of a model $M$ , we measure for each edge $e\\in C$ , $m_{e}\\equiv\\mathrm{KL}(M||M\\setminus\\{e\\})$ and $\\dot{c_{e}}\\equiv\\mathrm{KL}(M||C\\setminus\\{e\\})$ , i.e., how much removing the edge from the circuit or model affects its output distribution. For a method to be faithful, we expect to see a strong positive correlation between the two values, especially for edges where $m_{e}$ is large. We plot the two values against each other on the four tasks for four representative circuits found by Edge Pruning in Figure 8. The figure also provides the sparsities of each circuit. On all four tasks, whenever an edge is important to the model, it is also important to the circuit. Thus, studying the circuit to infer the role/importance of the components is a good proxy for the full model. On the other hand, we note that some edges are completely unimportant for the model, but ablating which perturbs the circuit KL by a small amount. This perturbation is much smaller than the ones seen in the former case above, but still non-negligible. This is not surprising, as circuit-finding methods may miss backup components that are deemed unnecessary for performance, and therefore be more sensitive to edge ablations. Alternatively, models may display behavior such as the Hydra effect [McGrath et al., 2023], whereas a circuit may not. Nonetheless, we suggest that practitioners verify any insights obtained from circuits on the full models wherever possible, regardless of the method used. ", "page_idx": 16}, {"type": "image", "img_path": "8oSY3rA9jY/tmp/ad8e3a2c40622f307bd1088617623953b11bd12d2c3237bb5b687aadf9b7b62f.jpg", "img_caption": ["Figure 8: The KL divergences of the model and circuit, upon ablating individual circuit edges from each, measured against the full model. We see that all components important to the model are also important to the circuits, with an almost linear correlation between the two quantities. The circuits shown here have sparsities of $97.23\\%$ , $96.44\\%$ , $98.59\\%$ , and $98.77\\%$ , respectively. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "D How consistent are the circuits found by Edge Pruning? ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this appendix, we evaluate if Edge Pruning can consistently find (i) good circuits, and (ii) consistent circuits in terms of chosen edges across different random initializations. To this end, we choose representative target sparsities $\\bar{(97.5\\%}$ for IOI, $99.0\\%$ for GT, and $97.0\\%$ for GP) and prune a GPT-2 small model with 12 different random seeds with these targets (and other hyperparameters as in Appendix A). As Figures 9 and 10 show, the resulting sparsities and faithfulness of these circuits are remarkably consistent across the 12 seeds, demonstrating that Edge Pruning is robust to different initializations. It is also interesting to ask whether multiple circuits exist for performing a task (and whether Edge Pruning finds them)\u2014Figure 11 investigates this question in this same setting by plotting the distribution of all pairwise IoUs (Intersection-over-Union) in terms of chosen edges, across the ${\\binom{12}{2}}=66$ pairs of circuits. We observe that the IoU values are generally high (0.5-0.7), but still far from 1. This suggests that while some components may be vital, others might be redundant. ", "page_idx": 17}, {"type": "image", "img_path": "8oSY3rA9jY/tmp/e09e12b4ecf83e70e99a65ca1774dc414c11f4e8e751afdde252e53d337cac59.jpg", "img_caption": ["Figure 9: The sparsities of obtained circuits are remarkably consistent across 12 seeds. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "8oSY3rA9jY/tmp/73d4984e7b53475edbff0d24d24f84c0330c377d3f02bd5ad6b75a3a2f0a0219.jpg", "img_caption": ["Figure 10: The KL divergences of obtained circuits are consistent across 12 seeds. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "This question can be further investigated in future work, especially in how we should define circuits in the face of redundancy. ", "page_idx": 18}, {"type": "image", "img_path": "8oSY3rA9jY/tmp/448c64eaacb581eee16aeb380a4e0b1af418988115c4759fcf67f5bb62281e47.jpg", "img_caption": ["Figure 11: The pairwise Intersection-over-Union over 12 seeds is usually high, but far from 1. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "E Prompt formats for Boolean Expressions ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We show the prompts used for the instruction-prompted and few-shot settings in the CodeLlama-13B case study in Figure 14. ", "page_idx": 19}, {"type": "text", "text": "F Circuits found with Edge Pruning ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we show example circuit diagrams of the circuits found by Edge Pruning. However, these come with one caveat. Since the typical circuit we found still had too many edges to present in a reasonably sized figure, we only provide figures here for GT and GP, where sparsities ove $99.5\\%$ still performed well. Despite this, the circuits here are among the sparsest ones we obtained for each task and therefore perform worse than those at lower sparsities (such as those reported in Figure 2). ", "page_idx": 19}, {"type": "text", "text": "The GT circuit is shown in Figure 15, which also reports the faithfulness and performance metrics for it. Similarly, Figure 16 shows a circuit for GP with $99.79\\%$ sparsity found by Edge Pruning. Note that the latter, due to the extremely high sparsity, does not perform that well. Nonetheless, the denser circuits compared in prior plots are too unwieldy to show here. ", "page_idx": 19}, {"type": "text", "text": "Interpretation of the CodeLlama-13B circuit. Interpreting circuits with $>1000$ edges remains difficult, but we have made progress in understanding parts of the circuit. For example, we have found the following sub-circuit of two composed heads (refer to Figure 17 for a snippet of this region): L8.H16 attends from operations (and/or) to the previous token (i.e. from op to a in a op b). L10.H24 attends from an operand to a previous operation (i.e. from b to op in a op b) and read the results from L8.H16. This suggests that this duo computes the value of the expression. Interestingly, the attention pattern also holds when a is not a literal like True but an arbitrarily nested subexpression\u2014e.g., attending from or to ( in \u201c((True or False) and True) or False\u201d. A hypothesis here is that the model could deal with arbitrary depth expressions by guessing the value of a\u2014allowing it to proceed with the second step\u2014and later verifying the guess. This would also allow the model to parallelize a sequential computation by doing both steps of expression resolution in parallel. Nonetheless, further study and careful interventions are required to verify this hypothesis. ", "page_idx": 19}, {"type": "text", "text": "Figure 12: Intruction prompt [INST] $\\tt{e s Y S\\gg}$ Evaluate the following boolean expression as either \u2018True\u2019 or \u2018False\u2019. $\\tt{e s Y S\\gg}$ ((not not True) and False) or True [/INST] ", "page_idx": 19}, {"type": "text", "text": "Figure 13: Few-shot prompt [INST] (True and False) or (False and True) is [/INST] False</s><s> [INST] (True or (not True)) and False is [/INST] False</s><s> [INST] (not (True and False)) or (False and True) is [/INST] True</s><s> ((not not True) and False) or True is [/INST] ", "page_idx": 19}, {"type": "text", "text": "Figure 14: The prompt used to elicit responses from the CodeLlama-13B model in the instruction prompted and few-shot settings, respectively. The test instance is underlined. ", "page_idx": 19}, {"type": "image", "img_path": "8oSY3rA9jY/tmp/743f0f722fcf4e0fa6da656b215ad6296bcdf4bc3599c888ea0d0b7899b50712.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Figure 15: A circuit for GT with $99.77\\%$ sparsity, found by Edge Pruning. This circuit obtains a KL divergence of 0.3987 and a Kendall\u2019s Tau of 0.7062. The corresponding values for Probability Difference and Probability Difference 10 are 0.4367 and 0.2478, respectively. ", "page_idx": 20}, {"type": "image", "img_path": "8oSY3rA9jY/tmp/14f7189b9ce2def609b5a5b5fbf18d7fe03671a8412edf80f8923a56ab7543ee.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Figure 16: A circuit for GP with $99.79\\%$ sparsity, found by Edge Pruning. It obtains a KL divergence of 0.4920, an accuracy of $55.03\\%$ , a Logit Difference of 0.9701, and an Exact Match of $64.02\\%$ . Note that this circuit does not perform as well as the less sparse ones (see Figure 6). However, we choose to show this circuit here as the denser ones have more edges and are unwieldy to plot. ", "page_idx": 21}, {"type": "image", "img_path": "8oSY3rA9jY/tmp/753f2e3083256a09c5ec59bd8b7ef2a2ff334fa1549434d8ff9bbf244d3879ee.jpg", "img_caption": ["Figure 17: A snippet of the CodeLlama-13B few-shot circuit. The entire circuit is too unwieldy to plot, but this snippet shows a densely connected region. Though a bit hard to make out, a8.h16 connects to a10.h24.v. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 22}, {"type": "text", "text": "Justification: Our abstract and introduction accurately reflect the ideas, findings, and implications of our work. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] . ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: We acknowledge assumptions and limitations in our paper where applicable.   \nWe also discuss the limitations of our method and point to future work in Section 7. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 22}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 23}, {"type": "text", "text": "Justification: Our paper does not include any theoretical results. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 23}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 23}, {"type": "text", "text": "Justification: We provide a complete description of our method in Appendix 3 and provide all hyperparameters and computational details in Appendix A. We also provide all prompt formats used in Appendix E. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 23}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 24}, {"type": "text", "text": "Justification: We will make our code and datasets publicly available. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/pu blic/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 24}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 24}, {"type": "text", "text": "We provide all details of how the data was chosen, and implementational nuances in Section 4 and Appendices 3. We list the hyperparameters used in A. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 24}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [No] . ", "page_idx": 24}, {"type": "text", "text": "Justification: In our comparisons, the independent variable, sparsity, can only be controlled with an approximate target sparsity and varies by model run. Therefore, we cannot measure the variance in performance of multiple circuits at exactly the same sparsity, but we run a large grid of experiments using different hyperparameters and report a scatterplot of the distribution of circuit performance with sparsity (Figures 2, 3, 5 and 6). For our scaling study (involving no comparisons, Section 5), we run our experiments with a single seed due to computational constraints. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 24}, {"type": "text", "text": "\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 25}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 25}, {"type": "text", "text": "Justification: We provide the runtime of all three approaches compared in Table 1. We provide other computational details, such as GPU configurations and compute budgets, in Appendix A. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 25}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper strictly follows the full Code of Ethics from NeurIPS. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 25}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 25}, {"type": "text", "text": "Justification: We discuss possible impacts of our work in Section 7. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 26}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: We do not work with any high risk datasets or models in this work. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 26}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 26}, {"type": "text", "text": "Justification: All assets and related work are properly cited in the paper. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 27}, {"type": "text", "text": "Justification: In our experiments, we largely repurpose publicly available datasets. The in-house version of Boolean Expressions (Section 5) is generated programmatically. All details relating to its generation are discussed in Section 5. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 27}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: We do not crowdsource or research with human subjects. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 27}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: Our experiments do not involve crowdsourcing or research with human subjects. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 28}]