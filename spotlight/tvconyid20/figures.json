[{"figure_path": "tVConYid20/figures/figures_4_1.jpg", "caption": "Figure 1: Pingpong scheduling for 2 warpgroups to overlap softmax and GEMMs: the softmax of one warpgroup should be scheduled when the GEMMs of another warpgroup are running. The same color denotes the same iteration.", "description": "This figure illustrates the ping-pong scheduling strategy used in FlashAttention-3 to overlap computation. Two warpgroups (sets of threads) work in parallel. While one warpgroup performs GEMM operations (matrix multiplication), the other performs softmax calculations. Then, the roles switch, creating an overlap and hiding latency.", "section": "3 FlashAttention-3: Algorithm"}, {"figure_path": "tVConYid20/figures/figures_5_1.jpg", "caption": "Figure 2: 2-stage WGMMA-softmax pipelining", "description": "This figure illustrates the 2-stage pipelining scheme used to overlap GEMMs and softmax computations.  The horizontal axis represents time, and the vertical axis shows the different stages of the algorithm (WGMMA0, Softmax, WGMMA1). The colored blocks represent the execution of different operations.  The figure shows how the softmax operations of one iteration are overlapped with the GEMM operations of the next iteration, effectively hiding the latency of the softmax operations and improving efficiency. Note the pipelining effect between consecutive iterations.", "section": "3.2 Intra-warpgroup overlapping GEMMs and softmax"}, {"figure_path": "tVConYid20/figures/figures_8_1.jpg", "caption": "Figure 5: Attention forward speed (BF16) on H100 GPU", "description": "This figure compares the forward pass speed of different attention methods on an NVIDIA H100 GPU for various sequence lengths and head dimensions.  The methods compared include standard attention, FlashAttention-2, FlashAttention-2 implemented in Triton, cuDNN's optimized implementation, and FlashAttention-3. The results are shown separately for different causal mask settings (with and without) and head dimensions (64 and 128).  FlashAttention-3 demonstrates significant speed improvements over other methods, especially at longer sequence lengths.", "section": "4.1 Benchmarking Attention"}, {"figure_path": "tVConYid20/figures/figures_8_2.jpg", "caption": "Figure 5: Attention forward speed (BF16) on H100 GPU", "description": "This figure presents a comparison of the forward pass speed (in TFLOPs/s) of different attention mechanisms on an NVIDIA H100 GPU using BF16 precision.  The sequence length varies from 512 to 16k, and different head dimensions (64, 128, and 256) are considered, both with and without causal masking.  The compared methods are standard attention, FlashAttention-2, FlashAttention-2 implemented in Triton, cuDNN's optimized attention, and FlashAttention-3.  FlashAttention-3 consistently demonstrates significant performance gains across all configurations.", "section": "4.1 Benchmarking Attention"}, {"figure_path": "tVConYid20/figures/figures_8_3.jpg", "caption": "Figure 5: Attention forward speed (BF16) on H100 GPU", "description": "This figure shows the results of benchmarking attention forward speed using BF16 precision on an NVIDIA H100 80GB SXM5 GPU.  It compares the performance of FlashAttention-3 against standard attention, FlashAttention-2, its Triton implementation, and cuDNN.  The benchmark is performed across different sequence lengths and with or without causal masking, and for head dimensions of 64, 128, and 256. The results demonstrate that FlashAttention-3 significantly outperforms other methods, especially for longer sequences.", "section": "4.1 Benchmarking Attention"}, {"figure_path": "tVConYid20/figures/figures_8_4.jpg", "caption": "Figure 5: Attention forward speed (BF16) on H100 GPU", "description": "This figure presents a comparison of the forward pass speed of different attention methods (Standard Attention, FlashAttention-2, Triton, cuDNN, and FlashAttention-3) using BF16 precision on an NVIDIA H100 80GB SXM5 GPU. The comparison is made across varying sequence lengths (512, 1k, 2k, 4k, 8k, 16k) and with different head dimensions (64 and 128), both with and without causal masking.  The results illustrate the performance improvements achieved by FlashAttention-3, particularly for longer sequences.", "section": "4.1 Benchmarking Attention"}, {"figure_path": "tVConYid20/figures/figures_8_5.jpg", "caption": "Figure 5: Attention forward speed (BF16) on H100 GPU", "description": "This figure presents the forward pass speed of different attention methods using BF16 precision on an NVIDIA H100 GPU. The sequence length varies from 512 to 16k, and the head dimension is either 64 or 128, both with and without causal masking.  The graph shows that FlashAttention-3 consistently outperforms other methods, including a standard attention implementation, FlashAttention-2, and optimized versions from Triton and cuDNN, especially as sequence length increases.", "section": "4.1 Benchmarking Attention"}, {"figure_path": "tVConYid20/figures/figures_8_6.jpg", "caption": "Figure 5: Attention forward speed (BF16) on H100 GPU", "description": "This figure shows the speed of different attention methods in terms of TFLOPS/s on an NVIDIA H100 GPU using BF16 precision. The sequence length varies from 512 to 16k, and different head dimensions (64, 128, and 256) are also tested with and without causal masks.  The figure compares the performance of FlashAttention-3 with FlashAttention-2, Triton, and cuDNN implementations. FlashAttention-3 shows significantly faster performance than the others.", "section": "4.1 Benchmarking Attention"}, {"figure_path": "tVConYid20/figures/figures_9_1.jpg", "caption": "Figure 6: Attention backward speed (BF16) on H100 GPU", "description": "This figure presents the results of benchmarking the backward pass of attention using BF16 precision on an NVIDIA H100 80GB SXM5 GPU.  The benchmark compares the speed (in TFLOPs/s) of four different methods: standard attention, FlashAttention-2, cuDNN, and FlashAttention-3. The results are shown for various sequence lengths (512, 1k, 2k, 4k, 8k, 16k) and a fixed head dimension of 64.  It demonstrates the speed improvements achieved by FlashAttention-3 over existing methods.", "section": "4.1 Benchmarking Attention"}, {"figure_path": "tVConYid20/figures/figures_9_2.jpg", "caption": "Figure 6: Attention backward speed (BF16) on H100 GPU", "description": "This figure compares the backward pass speed of different attention methods (Standard attention, FlashAttention-2, cuDNN, and FlashAttention-3) on the H100 GPU using BF16 precision.  The x-axis shows the sequence length, and the y-axis represents the speed in TFLOPS/s. The results are shown for different head dimensions (64 and 128).  It demonstrates that FlashAttention-3 outperforms other methods across various sequence lengths.", "section": "4.1 Benchmarking Attention"}, {"figure_path": "tVConYid20/figures/figures_9_3.jpg", "caption": "Figure 5: Attention forward speed (BF16) on H100 GPU", "description": "This figure presents the forward pass speed of different attention methods (Standard Attention, FlashAttention-2, FlashAttention-2 in Triton, cuDNN, and FlashAttention-3) using BF16 precision on an NVIDIA H100 80GB SXM5 GPU.  The speeds are shown for varying sequence lengths (512, 1k, 2k, 4k, 8k, 16k) and head dimensions (64 and 128), with and without causal masking.  It demonstrates the speedup achieved by FlashAttention-3 compared to other methods.", "section": "4.1 Benchmarking Attention"}, {"figure_path": "tVConYid20/figures/figures_9_4.jpg", "caption": "Figure 5: Attention forward speed (BF16) on H100 GPU", "description": "This figure presents a comparison of the forward pass speed (in TFLOPs/s) of different attention mechanisms on an NVIDIA H100 80GB SXM5 GPU.  The comparison includes standard attention, FlashAttention-2, FlashAttention-2 implemented using Triton, cuDNN's optimized implementation of FlashAttention-2, and FlashAttention-3. The results are shown for various sequence lengths and head dimensions (64, 128, and 256), with and without causal masking.  FlashAttention-3 consistently demonstrates superior performance across all tested configurations.", "section": "4.1 Benchmarking Attention"}, {"figure_path": "tVConYid20/figures/figures_17_1.jpg", "caption": "Figure 2: 2-stage WGMMA-softmax pipelining", "description": "This figure illustrates the 2-stage WGMMA-softmax pipelining technique.  It shows how the softmax computation for one iteration can overlap with the GEMM (WGMMA) computations for the next iteration, improving performance by hiding the latency of the softmax operation. The diagram depicts the pipelined execution of GEMM0, softmax, and GEMM1 operations across multiple iterations (0 to N-1).", "section": "3.2 Intra-warpgroup overlapping GEMMs and softmax"}, {"figure_path": "tVConYid20/figures/figures_18_1.jpg", "caption": "Figure 1: Pingpong scheduling for 2 warpgroups to overlap softmax and GEMMs: the softmax of one warpgroup should be scheduled when the GEMMs of another warpgroup are running. The same color denotes the same iteration.", "description": "This figure illustrates the ping-pong scheduling mechanism used in FlashAttention-3 to overlap softmax and GEMM operations. Two warpgroups alternate between performing GEMM and softmax calculations, maximizing hardware utilization.  The colors represent the same iteration in different warpgroups, showing how the operations are interleaved.", "section": "3 FlashAttention-3: Algorithm"}, {"figure_path": "tVConYid20/figures/figures_21_1.jpg", "caption": "Figure 5: Attention forward speed (BF16) on H100 GPU", "description": "This figure presents the results of benchmarking the forward pass of attention using BF16 precision on an NVIDIA H100 80GB SXM5 GPU.  It compares the speed (in TFLOPS/s) of four different methods across varying sequence lengths and head dimensions: Standard Attention, FlashAttention-2, FlashAttention-2 in Triton (optimized for H100 using specific instructions), and FlashAttention-3.  The results show that FlashAttention-3 consistently outperforms the other methods, demonstrating a significant speedup. The chart also considers the effect of causal masking (a technique used in certain types of sequence modeling).", "section": "4.1 Benchmarking Attention"}, {"figure_path": "tVConYid20/figures/figures_21_2.jpg", "caption": "Figure 5: Attention forward speed (BF16) on H100 GPU", "description": "This figure compares the forward pass speed of different attention methods (standard attention, FlashAttention-2, FlashAttention-2 in Triton, cuDNN, and FlashAttention-3) on an NVIDIA H100 GPU with different sequence lengths (512, 1k, 2k, 4k, 8k, 16k) and head dimensions (64 and 128). It shows the speed in TFLOPS/s for both causal and non-causal settings.", "section": "4.1 Benchmarking Attention"}, {"figure_path": "tVConYid20/figures/figures_21_3.jpg", "caption": "Figure 5: Attention forward speed (BF16) on H100 GPU", "description": "This figure compares the forward pass speed of four different attention implementations (Standard Attention, FlashAttention-2, FlashAttention-2 in Triton, cuDNN, and FlashAttention-3) across various sequence lengths (512, 1k, 2k, 4k, 8k, 16k) and head dimensions (64, 128, 256).  Both causal and non-causal mask settings are shown, providing a comprehensive performance comparison under different conditions.  The speed is measured in TFLOPs/s (Tera Floating Point Operations per second), a common metric for GPU performance.  FlashAttention-3 consistently demonstrates superior performance across all scenarios.", "section": "4.1 Benchmarking Attention"}, {"figure_path": "tVConYid20/figures/figures_21_4.jpg", "caption": "Figure 5: Attention forward speed (BF16) on H100 GPU", "description": "This figure presents a comparison of the forward pass speed (in TFLOPs/s) of different attention methods on an NVIDIA H100 GPU using BF16 precision.  The comparison includes Standard Attention, FlashAttention-2, FlashAttention-2 (Triton), cuDNN, and FlashAttention-3.  The speed is measured across various sequence lengths (512, 1k, 2k, 4k, 8k, 16k) and with two head dimensions (64 and 128), both with and without causal masking.  The results demonstrate the performance improvements achieved by FlashAttention-3.", "section": "4.1 Benchmarking Attention"}, {"figure_path": "tVConYid20/figures/figures_21_5.jpg", "caption": "Figure 5: Attention forward speed (BF16) on H100 GPU", "description": "This figure shows the forward pass speed of different attention methods (Standard Attention, FlashAttention-2, FlashAttention-2 in Triton, cuDNN, and FlashAttention-3) on an NVIDIA H100 GPU with BF16 precision. The speed is measured in TFLOPS/s and is plotted against the sequence length.  The experiments were performed with both causal and non-causal masking options for head dimensions of 64, 128, and 256.", "section": "4.1 Benchmarking Attention"}, {"figure_path": "tVConYid20/figures/figures_21_6.jpg", "caption": "Figure 5: Attention forward speed (BF16) on H100 GPU", "description": "This figure shows the speed of the forward pass of attention using BF16 precision on an NVIDIA H100 GPU, comparing different methods: Standard Attention, FlashAttention-2, FlashAttention-2 in Triton, cuDNN (NVIDIA's library), and FlashAttention-3.  The results are shown for various sequence lengths and head dimensions (64, 128, and 256), with and without causal masking.  FlashAttention-3 demonstrates significantly faster speeds compared to other methods, especially as sequence length increases.", "section": "4.1 Benchmarking Attention"}]