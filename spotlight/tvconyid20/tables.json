[{"figure_path": "tVConYid20/tables/tables_2_1.jpg", "caption": "Table 1: Thread-Memory hierarchy for the NVIDIA Hopper H100 SXM5 GPU.", "description": "This table shows the memory hierarchy of the NVIDIA Hopper H100 SXM5 GPU, including the capacity and bandwidth of each level: Global Memory (GMEM), L2 Cache, Shared Memory (SMEM), and Registers (RMEM).  It also specifies the parallel agent associated with each memory level, showing the relationship between the hardware architecture and the parallel programming model.", "section": "2.2 GPU hardware characteristics and execution model"}, {"figure_path": "tVConYid20/tables/tables_8_1.jpg", "caption": "Table 2: Pipelining ablation measurements", "description": "This table presents the results of an ablation study to evaluate the impact of two key techniques, GEMM-Softmax pipelining and warp specialization, on the performance of FLASHATTENTION-3. It shows the time taken and the TFLOPs/s achieved for three configurations: FLASHATTENTION-3 with both techniques, FLASHATTENTION-3 without GEMM-Softmax pipelining, and FLASHATTENTION-3 without warp specialization.  The results demonstrate the individual and combined contributions of these optimization strategies.", "section": "4.2 Ablation Study: 2-Stage Pipelining Experiments"}, {"figure_path": "tVConYid20/tables/tables_9_1.jpg", "caption": "Table 3: Numerical error comparisons in FP16 and FP8 (e4m3).", "description": "This table presents the root mean squared error (RMSE) for different attention methods using FP16 and FP8 precision.  The baseline FP16 represents a standard attention implementation. FLASHATTENTION-2 and FLASHATTENTION-3 are improved attention methods.  The table demonstrates the error reduction achieved by FLASHATTENTION-3 with FP16, and also shows the effects of block quantization and incoherent processing on the accuracy of the FP8 version of FLASHATTENTION-3.", "section": "4.3 Numerical Error Validation"}]