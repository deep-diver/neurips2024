[{"figure_path": "fykjplMc0V/tables/tables_6_1.jpg", "caption": "Table 1: Accuracy comparison of LLaMA-1 7B/13B, Llama-2 7B and Llama-3 8B against existing PEFT methods on eight commonsense reasoning datasets. *Performance results of all baseline methods are taken from Liu et al. [2024c]. We report averaged performance of three runs with distinct random seeds for our method. For our methods, Param. (%) is calculated by dividing the number of trainable parameters by the number of parameters of the base LM.", "description": "This table compares the performance of Low-rank Linear Subspace ReFT (LoReFT) and its ablation (DiReFT) against other parameter-efficient fine-tuning (PEFT) methods on eight commonsense reasoning datasets.  It shows the accuracy achieved by each method, along with the percentage of parameters trained relative to the base language model.  The results demonstrate LoReFT's superior performance and efficiency compared to existing PEFTs.", "section": "4 Commonsense reasoning"}, {"figure_path": "fykjplMc0V/tables/tables_7_1.jpg", "caption": "Table 2: Accuracy comparison of LLaMA-1 7B/13B against existing PEFT methods on four arithmetic reasoning datasets. *Performance results of all baseline methods are taken from Hu et al. [2023]. We report averaged performance of three runs with distinct random seeds for our method.", "description": "This table compares the performance of different parameter-efficient fine-tuning (PEFT) methods, including LoReFT and DiReFT (the methods introduced in the paper), against existing PEFTs on four arithmetic reasoning datasets using LLaMA-1 7B and 13B models.  The results show accuracy and the percentage of parameters trained for each method. The performance is averaged over three runs with different random seeds, highlighting the consistency and reliability of LoReFT and DiReFT. The baseline results are taken from a previous study by Hu et al. (2023).", "section": "4.3 Arithmetic reasoning"}, {"figure_path": "fykjplMc0V/tables/tables_7_2.jpg", "caption": "Table 3: Instruction tuning evaluation results for instruction-tuned Llama-2 7B with Alpaca-Eval v1.0. We report averaged performance of two runs with distinct random seeds for our method. half denotes our runs with half of the rank; IK denotes our runs with a low-resource setting where there is only 1K training examples. Performance results of baseline methods are taken from Li et al. [2023]. *Performance results of baseline methods are taken from Wu et al. [2024a]. It takes 18 minutes to train our Llama-2 Chat 7B on 1K examples using a single A100 40G GPU with \u22481MB parameters on disk.", "description": "This table compares the performance of different parameter-efficient fine-tuning (PEFT) methods and full finetuning (FT) on an instruction-following task using the Llama-2 7B language model.  The win-rate is measured using Alpaca-Eval v1.0, comparing against text-davinci-003 using GPT-4 as the annotator.  The table also shows the number of parameters used for each method.  Ablation studies are included with LoReFT using half the rank and only 1K training examples.", "section": "4.4 Instruction-following"}, {"figure_path": "fykjplMc0V/tables/tables_8_1.jpg", "caption": "Table 1: Accuracy comparison of LLaMA-1 7B/13B, Llama-2 7B and Llama-3 8B against existing PEFT methods on eight commonsense reasoning datasets. *Performance results of all baseline methods are taken from Liu et al. [2024c]. We report averaged performance of three runs with distinct random seeds for our method. For our methods, Param. (%) is calculated by dividing the number of trainable parameters by the number of parameters of the base LM.", "description": "This table compares the performance of LoReFT and other parameter-efficient fine-tuning (PEFT) methods on eight commonsense reasoning datasets using three different LLaMA models (LLaMA-1 7B/13B, Llama-2 7B, and Llama-3 8B).  It shows the accuracy achieved by each method, along with the percentage of parameters trained relative to the base model size.  The results highlight LoReFT's ability to achieve competitive or superior performance while using significantly fewer parameters than other PEFTs.", "section": "4 Commonsense reasoning"}, {"figure_path": "fykjplMc0V/tables/tables_22_1.jpg", "caption": "Table 1: Accuracy comparison of LLaMA-1 7B/13B, Llama-2 7B and Llama-3 8B against existing PEFT methods on eight commonsense reasoning datasets. *Performance results of all baseline methods are taken from Liu et al. [2024c]. We report averaged performance of three runs with distinct random seeds for our method. For our methods, Param. (%) is calculated by dividing the number of trainable parameters by the number of parameters of the base LM.", "description": "This table compares the performance of LoReFT and other parameter-efficient fine-tuning (PEFT) methods on eight commonsense reasoning datasets using three different LLaMA models (7B, 13B, and 8B).  It shows accuracy scores for each method, along with the percentage of parameters trained relative to the total number of parameters in the base model. The results from Liu et al. (2024c) are included for comparison.", "section": "4 Commonsense reasoning"}, {"figure_path": "fykjplMc0V/tables/tables_22_2.jpg", "caption": "Table 1: Accuracy comparison of LLaMA-1 7B/13B, Llama-2 7B and Llama-3 8B against existing PEFT methods on eight commonsense reasoning datasets. *Performance results of all baseline methods are taken from Liu et al. [2024c]. We report averaged performance of three runs with distinct random seeds for our method. For our methods, Param. (%) is calculated by dividing the number of trainable parameters by the number of parameters of the base LM.", "description": "This table compares the performance of different parameter-efficient fine-tuning (PEFT) methods, including the proposed Low-rank Linear Subspace ReFT (LoReFT) and its ablation DiReFT, on eight commonsense reasoning datasets using three different LLaMA models (7B, 13B, and 8B).  It shows the accuracy achieved by each PEFT method relative to the number of parameters used (as a percentage of the total model parameters). The results highlight LoReFT's superior performance and efficiency compared to existing PEFTs.", "section": "4 Commonsense reasoning"}, {"figure_path": "fykjplMc0V/tables/tables_23_1.jpg", "caption": "Table 1: Accuracy comparison of LLaMA-1 7B/13B, Llama-2 7B and Llama-3 8B against existing PEFT methods on eight commonsense reasoning datasets. *Performance results of all baseline methods are taken from Liu et al. [2024c]. We report averaged performance of three runs with distinct random seeds for our method. For our methods, Param. (%) is calculated by dividing the number of trainable parameters by the number of parameters of the base LM.", "description": "This table compares the performance of the proposed LoReFT and DiReFT methods against other parameter-efficient fine-tuning (PEFT) methods on eight commonsense reasoning datasets using different LLaMA models.  The table shows accuracy, the percentage of parameters trained, and highlights that LoReFT and DiReFT achieve state-of-the-art performance with significantly fewer parameters compared to existing PEFTs. The results from Liu et al. (2024c) are used as baselines for other PEFT methods.", "section": "4 Commonsense reasoning"}, {"figure_path": "fykjplMc0V/tables/tables_23_2.jpg", "caption": "Table 1: Accuracy comparison of LLaMA-1 7B/13B, Llama-2 7B and Llama-3 8B against existing PEFT methods on eight commonsense reasoning datasets. *Performance results of all baseline methods are taken from Liu et al. [2024c]. We report averaged performance of three runs with distinct random seeds for our method. For our methods, Param. (%) is calculated by dividing the number of trainable parameters by the number of parameters of the base LM.", "description": "This table compares the performance of LoReFT and other parameter-efficient fine-tuning (PEFT) methods on eight commonsense reasoning datasets using various LLaMA models.  It shows the accuracy achieved by each method, along with the percentage of parameters trained relative to the base model size.  The results from Liu et al. (2024c) are included for comparison.  The table highlights LoReFT's performance gains and parameter efficiency.", "section": "4 Commonsense reasoning"}, {"figure_path": "fykjplMc0V/tables/tables_24_1.jpg", "caption": "Table 1: Accuracy comparison of LLaMA-1 7B/13B, Llama-2 7B and Llama-3 8B against existing PEFT methods on eight commonsense reasoning datasets. *Performance results of all baseline methods are taken from Liu et al. [2024c]. We report averaged performance of three runs with distinct random seeds for our method. For our methods, Param. (%) is calculated by dividing the number of trainable parameters by the number of parameters of the base LM.", "description": "This table compares the performance of LoReFT and other parameter-efficient fine-tuning (PEFT) methods on eight commonsense reasoning datasets using different LLaMA models.  It shows accuracy scores for each dataset, along with the percentage of trainable parameters relative to the total number of parameters in the base LLaMA models.  The results from Liu et al. (2024c) are included for comparison.", "section": "4 Commonsense reasoning"}, {"figure_path": "fykjplMc0V/tables/tables_24_2.jpg", "caption": "Table 1: Accuracy comparison of LLaMA-1 7B/13B, Llama-2 7B and Llama-3 8B against existing PEFT methods on eight commonsense reasoning datasets. *Performance results of all baseline methods are taken from Liu et al. [2024c]. We report averaged performance of three runs with distinct random seeds for our method. For our methods, Param. (%) is calculated by dividing the number of trainable parameters by the number of parameters of the base LM.", "description": "This table compares the performance of LoReFT and other parameter-efficient fine-tuning (PEFT) methods on eight commonsense reasoning datasets using different LLaMA models.  It shows accuracy results for each dataset, along with the percentage of parameters trained for each method.  The results highlight LoReFT's performance compared to existing PEFTs such as Prefix-tuning, Adapters, LoRA, and DoRA.", "section": "4 Commonsense reasoning"}, {"figure_path": "fykjplMc0V/tables/tables_25_1.jpg", "caption": "Table 1: Accuracy comparison of LLaMA-1 7B/13B, Llama-2 7B and Llama-3 8B against existing PEFT methods on eight commonsense reasoning datasets. *Performance results of all baseline methods are taken from Liu et al. [2024c]. We report averaged performance of three runs with distinct random seeds for our method. For our methods, Param. (%) is calculated by dividing the number of trainable parameters by the number of parameters of the base LM.", "description": "This table compares the performance of the proposed Low-rank Linear Subspace ReFT (LoReFT) method and its variant, DiReFT, against other parameter-efficient fine-tuning (PEFT) methods on eight commonsense reasoning datasets using three different LLaMA models.  It shows the accuracy achieved by each method, along with the percentage of parameters trained relative to the full model size. The results highlight LoReFT's effectiveness with fewer parameters.", "section": "4 Commonsense reasoning"}, {"figure_path": "fykjplMc0V/tables/tables_25_2.jpg", "caption": "Table 1: Accuracy comparison of LLaMA-1 7B/13B, Llama-2 7B and Llama-3 8B against existing PEFT methods on eight commonsense reasoning datasets. *Performance results of all baseline methods are taken from Liu et al. [2024c]. We report averaged performance of three runs with distinct random seeds for our method. For our methods, Param. (%) is calculated by dividing the number of trainable parameters by the number of parameters of the base LM.", "description": "This table compares the accuracy of LLaMA-1 7B/13B, Llama-2 7B, and Llama-3 8B language models when fine-tuned using LoReFT against existing parameter-efficient fine-tuning (PEFT) methods across eight commonsense reasoning datasets.  It shows the accuracy achieved by each method, along with the percentage of parameters trained relative to the total number of parameters in the base language model.  The results from existing PEFT methods are taken from a previous study by Liu et al. (2024c).  The table highlights LoReFT's performance in comparison to other PEFTs, showing it achieves competitive results while using significantly fewer parameters.", "section": "4 Commonsense reasoning"}, {"figure_path": "fykjplMc0V/tables/tables_26_1.jpg", "caption": "Table 1: Accuracy comparison of LLaMA-1 7B/13B, Llama-2 7B and Llama-3 8B against existing PEFT methods on eight commonsense reasoning datasets. *Performance results of all baseline methods are taken from Liu et al. [2024c]. We report averaged performance of three runs with distinct random seeds for our method. For our methods, Param. (%) is calculated by dividing the number of trainable parameters by the number of parameters of the base LM.", "description": "This table compares the performance of LoReFT and DiReFT against other parameter-efficient fine-tuning (PEFT) methods on eight commonsense reasoning datasets using LLaMA 1 7B/13B, Llama-2 7B and Llama-3 8B language models.  It shows accuracy scores and the percentage of parameters trained for each method.  The results highlight LoReFT's superior performance and efficiency compared to existing PEFTs.", "section": "4 Commonsense reasoning"}, {"figure_path": "fykjplMc0V/tables/tables_28_1.jpg", "caption": "Table 1: Accuracy comparison of LLaMA-1 7B/13B, Llama-2 7B and Llama-3 8B against existing PEFT methods on eight commonsense reasoning datasets. *Performance results of all baseline methods are taken from Liu et al. [2024c]. We report averaged performance of three runs with distinct random seeds for our method. For our methods, Param. (%) is calculated by dividing the number of trainable parameters by the number of parameters of the base LM.", "description": "This table compares the performance of LoReFT and other parameter-efficient fine-tuning (PEFT) methods on eight commonsense reasoning datasets using three different LLaMA models (7B, 13B, and 8B).  It shows the accuracy achieved by each method and its relative parameter efficiency (percentage of parameters trained relative to the base model).  The results highlight LoReFT's superior performance and efficiency compared to other PEFTs. Note that the baseline results for other methods are taken from a different source, Liu et al. [2024c].", "section": "4 Commonsense reasoning"}, {"figure_path": "fykjplMc0V/tables/tables_28_2.jpg", "caption": "Table 1: Accuracy comparison of LLaMA-1 7B/13B, Llama-2 7B and Llama-3 8B against existing PEFT methods on eight commonsense reasoning datasets. *Performance results of all baseline methods are taken from Liu et al. [2024c]. We report averaged performance of three runs with distinct random seeds for our method. For our methods, Param. (%) is calculated by dividing the number of trainable parameters by the number of parameters of the base LM.", "description": "This table compares the performance of LoReFT and DiReFT against other Parameter-Efficient Fine-Tuning (PEFT) methods on eight commonsense reasoning datasets using three different LLaMA models (LLaMA-1 7B, LLaMA-1 13B, Llama-2 7B, and Llama-3 8B).  It shows the accuracy achieved by each method and the percentage of parameters trained relative to the full model size. The results from other PEFT methods are cited from Liu et al. (2024c).", "section": "4 Commonsense reasoning"}, {"figure_path": "fykjplMc0V/tables/tables_29_1.jpg", "caption": "Table 1: Accuracy comparison of LLaMA-1 7B/13B, Llama-2 7B and Llama-3 8B against existing PEFT methods on eight commonsense reasoning datasets. *Performance results of all baseline methods are taken from Liu et al. [2024c]. We report averaged performance of three runs with distinct random seeds for our method. For our methods, Param. (%) is calculated by dividing the number of trainable parameters by the number of parameters of the base LM.", "description": "This table compares the performance of LoReFT and DiReFT against other parameter-efficient fine-tuning (PEFT) methods on eight commonsense reasoning datasets using three different LLaMA models (LLaMA-1 7B, LLaMA-1 13B, Llama-2 7B, and Llama-3 8B).  It shows the accuracy achieved by each method and the percentage of parameters trained relative to the full model size. The results from other PEFT methods are taken from Liu et al. (2024c).", "section": "4 Commonsense reasoning"}, {"figure_path": "fykjplMc0V/tables/tables_30_1.jpg", "caption": "Table 1: Accuracy comparison of LLaMA-1 7B/13B, Llama-2 7B and Llama-3 8B against existing PEFT methods on eight commonsense reasoning datasets. *Performance results of all baseline methods are taken from Liu et al. [2024c]. We report averaged performance of three runs with distinct random seeds for our method. For our methods, Param. (%) is calculated by dividing the number of trainable parameters by the number of parameters of the base LM.", "description": "This table compares the performance of LLaMA 1 7B/13B, Llama-2 7B, and Llama-3 8B language models fine-tuned using LoReFT against other parameter-efficient finetuning (PEFT) methods on eight commonsense reasoning datasets.  It shows accuracy results, along with the percentage of parameters trained for each method.  The results from other PEFT methods are sourced from Liu et al., 2024c.  The study used three different random seeds for LoReFT and reports the average performance.", "section": "4 Commonsense reasoning"}, {"figure_path": "fykjplMc0V/tables/tables_33_1.jpg", "caption": "Table 1: Accuracy comparison of LLaMA-1 7B/13B, Llama-2 7B and Llama-3 8B against existing PEFT methods on eight commonsense reasoning datasets. *Performance results of all baseline methods are taken from Liu et al. [2024c]. We report averaged performance of three runs with distinct random seeds for our method. For our methods, Param. (%) is calculated by dividing the number of trainable parameters by the number of parameters of the base LM.", "description": "This table compares the performance of different parameter-efficient fine-tuning (PEFT) methods and the proposed Representation Finetuning (ReFT) methods on eight commonsense reasoning datasets using LLaMA-1 7B/13B, Llama-2 7B and Llama-3 8B language models.  It shows accuracy scores for each dataset and method, along with the percentage of trainable parameters relative to the base model size.  The results highlight the efficiency and performance of the ReFT methods.", "section": "4 Commonsense reasoning"}, {"figure_path": "fykjplMc0V/tables/tables_33_2.jpg", "caption": "Table 1: Accuracy comparison of LLaMA-1 7B/13B, Llama-2 7B and Llama-3 8B against existing PEFT methods on eight commonsense reasoning datasets. *Performance results of all baseline methods are taken from Liu et al. [2024c]. We report averaged performance of three runs with distinct random seeds for our method. For our methods, Param. (%) is calculated by dividing the number of trainable parameters by the number of parameters of the base LM.", "description": "This table compares the performance of LoReFT and other parameter-efficient fine-tuning (PEFT) methods on eight commonsense reasoning datasets using three different LLaMA models (LLaMA-1 7B, LLaMA-1 13B, Llama-2 7B, and Llama-3 8B).  The table shows accuracy scores for each dataset and method, along with the percentage of trainable parameters relative to the base model's total parameters.  It highlights the performance and parameter efficiency of LoReFT compared to other PEFTs.", "section": "4 Commonsense reasoning"}, {"figure_path": "fykjplMc0V/tables/tables_36_1.jpg", "caption": "Table 1: Accuracy comparison of LLaMA-1 7B/13B, Llama-2 7B and Llama-3 8B against existing PEFT methods on eight commonsense reasoning datasets. *Performance results of all baseline methods are taken from Liu et al. [2024c]. We report averaged performance of three runs with distinct random seeds for our method. For our methods, Param. (%) is calculated by dividing the number of trainable parameters by the number of parameters of the base LM.", "description": "This table compares the performance of LLaMA-1 7B/13B, Llama-2 7B, and Llama-3 8B language models fine-tuned using LoReFT against other parameter-efficient fine-tuning (PEFT) methods on eight commonsense reasoning datasets.  The table shows accuracy scores for each model and method, along with the percentage of parameters trained relative to the base model's total parameters.  Results from Liu et al. (2024c) are used as a baseline for comparison.  LoReFT's performance is averaged over three runs with different random seeds to ensure reliability.", "section": "4 Commonsense reasoning"}]