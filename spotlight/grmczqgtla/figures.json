[{"figure_path": "GrMczQGTlA/figures/figures_1_1.jpg", "caption": "Figure 1: A humanoid that walks in San Francisco. We deploy our model to the robot and test it in various locations in San Francisco over the course of one week. Please see our project page for videos.", "description": "This figure shows a teal-colored humanoid robot walking in various outdoor locations throughout San Francisco.  The photos illustrate the robot's ability to navigate different terrains and environments, showcasing its zero-shot deployment in a real-world, complex urban setting.  The full video can be found on the project's webpage.", "section": "1 Introduction"}, {"figure_path": "GrMczQGTlA/figures/figures_3_1.jpg", "caption": "Figure 2: Humanoid locomotion as next token prediction. We collect a dataset on trajectories from various sources, such as from neural network policies, model-based controllers, human motion capture, and YouTube videos of humans. Then we use this dataset to train a transformer policy by autoregressive modeling of observations and actions. Our transformer allows a humanoid to walk zero-shot on various terrains around San Francisco. Please see our project page for video results.", "description": "This figure illustrates the overall process of the proposed method in the paper.  It starts by showing the various data sources used for training: neural network policies, model-based controllers, motion capture data, and videos from YouTube. All of this data is fed into a transformer model for training via autoregressive prediction. The trained model is then deployed on a real humanoid robot to perform zero-shot locomotion in various locations in San Francisco. ", "section": "3 Approach"}, {"figure_path": "GrMczQGTlA/figures/figures_4_1.jpg", "caption": "Figure 3: A general framework for training with different data sources. Our data modeling allows us to train our transformer with multiple modes of training. In the case of observation-action pairs being available, we train our transformer to predict the next pair of observation-action. When there is no action data available, with MoCap and internet data, we only train our transformer to predict the next observations by masking the actions with a mask token. These two models of training allow our model to utilize both types of data, and this enables us to scale our training in terms of data.", "description": "This figure illustrates the two training scenarios of the transformer model. The left side shows training with complete data (observation-action pairs), while the right side demonstrates training with missing data (only observations from MoCap and internet videos, actions are masked).  The model's ability to handle both types of data is highlighted, emphasizing its scalability.", "section": "3 Approach"}, {"figure_path": "GrMczQGTlA/figures/figures_5_1.jpg", "caption": "Figure 4: Training dataset. To train our model, we construct a dataset of trajectories coming from four different sources. (i) neural network policy: provides trajectories with complete observations and actions. (ii) model-based controller: produces trajectories from the same robot morphology but without actions. (iii) motion capture of humans: does not contain actions and is approximately retargeted onto the robot. (iv) internet videos of humans: sequences of human poses are first reconstructed via computer vision techniques and then approximately retargeted onto the robot.", "description": "This figure shows the four data sources used to train the humanoid locomotion model.  The neural network policy provides complete trajectories with observations and actions. The model-based controller offers trajectories with observations only.  Motion capture data includes human movement retargeted to the robot, again without actions, and finally, internet videos of humans provide data processed through computer vision to extract pose information, then retargeted to the robot, also lacking action data.", "section": "4 Dataset"}, {"figure_path": "GrMczQGTlA/figures/figures_7_1.jpg", "caption": "Figure 5: Comparison to state of the art, trajectory adherence. The robot is commanded to walk starting from the origin with a fixed heading command of 0.5 m/s and varying yaw commands in [-0.4, 0.4] rad/s. We plot the desired (dotted) and actual (solid) trajectories for our policy and a reinforcement-learning trained policy (RL).", "description": "This figure compares the trajectory adherence of the proposed method to a reinforcement learning baseline.  The robot is given a forward walking command with varying yaw commands. The plots show the desired trajectory (dotted lines) and the actual trajectories (solid lines) for both the proposed method and the reinforcement learning baseline.  The plot demonstrates that the proposed method follows the desired trajectories more accurately than the reinforcement learning baseline.", "section": "5.4 Comparison to the state of the art"}, {"figure_path": "GrMczQGTlA/figures/figures_8_1.jpg", "caption": "Figure 7: Scaling studies. We find that our approach has favorable scaling properties, and that the performance improves with the dataset size (left), context length (middle), and model size (right).", "description": "This figure shows the results of scaling studies performed on the model.  Three subplots illustrate how performance (measured by position tracking error) changes with increases in (left) training dataset size (number of trajectories), (middle) context length (number of timesteps considered by the model), and (right) model size (number of parameters). In all cases, performance improves with increased scale, indicating favorable scaling properties.", "section": "5.6 Scaling studies"}, {"figure_path": "GrMczQGTlA/figures/figures_8_2.jpg", "caption": "Figure 8: Prediction error correlation.", "description": "This figure shows the correlation between prediction error and position tracking error in the simulation experiments.  The scatter plot displays the position tracking error (y-axis) against the prediction loss (x-axis) for fourteen different models trained with varied training methods, architectures, and data sizes. The strong positive correlation (r=0.87) indicates that models with lower prediction errors generally exhibit better position tracking accuracy.", "section": "5.7 Prediction error correlates with performance"}]