{"references": [{"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-MM-DD", "reason": "This paper is foundational for the field of large language models and directly inspired the approach of using autoregressive prediction for humanoid control."}, {"fullname_first_author": "Alec Radford", "paper_title": "Improving language understanding by generative pre-training", "publication_date": "2018-MM-DD", "reason": "This paper introduced GPT, a highly influential model that demonstrated the power of autoregressive language modeling, which the current paper adapts to the problem of humanoid control."}, {"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-MM-DD", "reason": "This paper introduced the transformer architecture, a key component of the model used in the current work, showcasing its effectiveness in processing sequential data."}, {"fullname_first_author": "Ilija Radosavovic", "paper_title": "Real-world humanoid locomotion with reinforcement learning", "publication_date": "2024-MM-DD", "reason": "This paper, also by the current authors, establishes a strong baseline for humanoid locomotion using reinforcement learning, which the current work seeks to improve upon using generative modeling."}, {"fullname_first_author": "Ilija Radosavovic", "paper_title": "Robot learning with sensorimotor pre-training", "publication_date": "2023-MM-DD", "reason": "This paper, also by the current authors, explores the use of sensorimotor pretraining, providing a foundation for the current work's approach to leveraging diverse and incomplete data."}]}