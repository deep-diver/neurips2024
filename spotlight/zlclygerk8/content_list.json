[{"type": "text", "text": "Logarithmic Smoothing for Pessimistic Off-Policy Evaluation, Selection and Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Otmane Sakhi Criteo AI Lab, Paris, France o.sakhi@criteo.com ", "page_idx": 0}, {"type": "text", "text": "Imad Aouali CREST, ENSAE Criteo AI Lab, Paris, France i.aouali@criteo.com ", "page_idx": 0}, {"type": "text", "text": "Pierre Alquier ESSEC Business School, Singapore alquier@essec.edu ", "page_idx": 0}, {"type": "text", "text": "Nicolas Chopin CREST, ENSAE nicolas.chopin@ensae.fr ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "This work investigates the offline formulation of the contextual bandit problem, where the goal is to leverage past interactions collected under a behavior policy to evaluate, select, and learn new, potentially better-performing, policies. Motivated by critical applications, we move beyond point estimators. Instead, we adopt the principle of pessimism where we construct upper bounds that assess a policy\u2019s worstcase performance, enabling us to confidently select and learn improved policies. Precisely, we introduce novel, fully empirical concentration bounds for a broad class of importance weighting risk estimators. These bounds are general enough to cover most existing estimators and pave the way for the development of new ones. In particular, our pursuit of the tightest bound within this class motivates a novel estimator (LS), that logarithmically smooths large importance weights. The bound for LS is provably tighter than its competitors, and naturally results in improved policy selection and learning strategies. Extensive policy evaluation, selection, and learning experiments highlight the versatility and favorable performance of LS. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In decision-making under uncertainty, offline contextual bandit [16] presents a practical framework for leveraging past interactions with an environment to optimize future decisions. This comes into play when we possess logged data summarizing an agent\u2019s past interactions [10]. These interactions, typically captured as context-action-reward tuples, hold valuable insights into the underlying dynamics of the environment. Each tuple represents a single round of interaction, where the agent observes a context (including relevant features), takes an action according to its current policy, often called behavior policy, and receives a reward that depends on both the observed context and the taken action. This framework is prevalent in interactive systems like online advertising, music streaming, and video recommendation. In online advertising, for instance, the user\u2019s profile is the context, the recommended product is the action, and the click-through rate (CTR) is the expected reward. By learning from past interactions, the recommender system tailors product suggestions to individual preferences, maximizing engagement and ultimately, business success. ", "page_idx": 0}, {"type": "text", "text": "To optimize future decisions without requiring real-time deployments, this framework presents us with three tasks: off-policy evaluation (OPE) [16], off-policy selection (OPS) [32], and off-policy learning (OPL) [55]. OPE estimates the risk: the negative of expected reward that a target policy would achieve, essentially predicting its performance if deployed. OPS selects the best-performing ", "page_idx": 0}, {"type": "text", "text": "policy from a finite set of options, and OPL finds the optimal policy within an infinite class of policies.   \nIn general, OPE is an intermediary step for OPS and OPL since its primary goal is policy comparison. ", "page_idx": 1}, {"type": "text", "text": "A significant amount of research in OPE has centered around Inverse Propensity Scoring (IPS) estimators [24, 16\u201318, 60, 19, 54, 38, 32, 45]. These estimators rely on importance weighting to address the discrepancy between the target and behavior policies. While unbiased under some conditions, IPS induces high variance. To mitigate this, regularization techniques have been proposed for IPS [10, 38, 54, 5, 21] trading some bias for reduced variance. However, these estimators can still deviate from the true risk, undermining their reliability for decision-making, especially in critical applications. In such scenarios, practitioners need estimates that cover the true risk with high confidence. To address this, several approaches focused on constructing either asymptotic [10, 48, 15] or finite sample [32, 21], high probability, empirical upper bounds on the risk. These bounds evaluate the performance of a policy in the worst-case scenario, adopting the principle of pessimism [27]. ", "page_idx": 1}, {"type": "text", "text": "If this principle is used in OPE, it is central in OPS and OPL, where strategies are inspired by, or directly derived from, upper bounds on the risk [55, 35, 32, 49, 5, 59, 21]. Examples for OPS include Kuzborskij et al. [32] who employed an Efron-Stein bound for self-normalized IPS, or Gabbianelli et al. [21] that based their analysis on an upper bound constructed with the Implicit Exploration estimator. Focusing on OPL, Swaminathan and Joachims [55] exploited the empirical Bernstein bound [36] alongside the Clipping estimator to motivate sample variance penalization. This work was recently improved by either modifying the penalization [59] or analyzing the problem from the PAC-Bayesian lens [35]. The latter direction was further explored by Sakhi et al. [49], Aouali et al. [5, 7], Gabbianelli et al. [21] resulting in tight PAC-Bayesian bounds that can be directly optimized. ", "page_idx": 1}, {"type": "text", "text": "Existing pessimistic OPE, OPS, and OPL approaches involve analyzing the concentration properties of a pre-defined risk estimator, often chosen to simplify the analysis. We propose a different approach: we derive general concentration bounds applicable to a broad class of regularized IPS estimators and then identify the estimator within this class that achieves the tightest concentration bound. This leads to a tailored estimator, named Logarithmic Smoothing (LS). LS enjoys several desirable properties. It concentrates at a sub-Gaussian rate, and has a finite variance without being necessarily bounded. Its concentration upper bound allows us to evaluate the worst-case risk of any policy, enables us to derive a simple OPS strategy that directly minimizes our estimator akin to Gabbianelli et al. [21], and achieves state-of-the-art learning guarantees for OPL when analyzed within the PAC-Bayesian framework akin to [35, 49, 5, 7, 21]. ", "page_idx": 1}, {"type": "text", "text": "This paper is structured as follows. Section 2 introduces the necessary background. In Section 3, we provide unified risk bounds for a broad class of regularized IPS estimators, for which LS enjoys the tightest upper bound. In Section 4, we analyze LS for OPS and OPL, and we further extend the analysis within the PAC-Bayesian framework. Extensive experiments in Section 5 highlight the favorable performance of LS, and Section 6 provides concluding remarks. ", "page_idx": 1}, {"type": "text", "text": "2 Setting and background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Offline contextual bandit. Let $\\mathcal{X}\\subset\\mathbb{R}^{d}$ be the context space, which is a compact subset of $\\mathbb{R}^{d}$ , and let $\\boldsymbol{A}=[K]$ be a finite action set. An agent\u2019s actions are guided by a stochastic and stationary policy $\\pi\\in\\Pi$ within a policy space $\\Pi$ . Given a context $x\\in\\mathscr{X}$ , $\\pi(\\cdot|x)$ is a probability distribution over the action set $\\boldsymbol{\\mathcal{A}}$ ; $\\pi(a|x)$ is the probability that the agent selects action $a$ in context $x$ . Then, an agent interacts with a contextual bandit over $n$ rounds. In round $i\\,\\in\\,[n]$ , the agent observes a context $x_{i}\\sim\\nu$ where $\\nu$ is a distribution with support $\\mathcal{X}$ . After this, the agent selects an action $a_{i}\\sim\\pi_{0}(\\cdot|x_{i})$ , where $\\pi_{0}$ is the behavior policy of the agent. Finally, the agent receives a stochastic cost $c_{i}\\in[-1,0]$ that depends on the observed context $x_{i}$ and the taken action $a_{i}$ . This cost $c_{i}$ is sampled from a cost distribution $p(\\cdot|x_{i},a_{i})$ . This leads to $n$ -sized logged data, $D_{n}=(x_{i},a_{i},c_{i})_{i\\in[n]},$ , where tuples $(x_{i},a_{i},c_{i})$ for $i\\in[n]$ are i.i.d. The expected cost of taking action $a$ in context $x$ is $c(x,a)=\\underline{{\\mathbb{E}}}_{c\\sim p(\\cdot|x,a)}\\left[c\\right]$ , and the costs are negative because they are interpreted as the negative of rewards. The performance of a policy $\\pi\\in\\Pi$ is evaluated through its $r i s k$ , which aggregates the expected costs $c(x,a)$ over all possible contexts $x\\in\\mathscr{X}$ and taken actions $a\\in A$ by policy $\\pi$ , such as ", "page_idx": 1}, {"type": "equation", "text": "$$\nR(\\pi)=\\mathbb{E}_{x\\sim\\nu,a\\sim\\pi(\\cdot|x),c\\sim p(\\cdot|x,a)}\\left[c\\right]=\\mathbb{E}_{x\\sim\\nu,a\\sim\\pi(\\cdot|x)}\\left[c(x,a)\\right]\\,.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "The main goal is to use logged dataset $\\mathcal{D}_{n}$ to enhance future decision-making without necessitating live deployments. This often entails three tasks: OPE, OPS, and OPL. First, OPE is concerned with constructing an estimator $\\hat{R}_{n}(\\pi)$ of the risk $R(\\pi)$ of a fixed target policy $\\pi$ and study its deviation, aspiring for $\\hat{R}_{n}(\\pi)$ to concentrate well around $R(\\pi)$ . Second, OPS focuses on selecting the best performing policy $\\hat{\\pi}_{n}^{\\mathrm{s}}$ from a predefined and finite collection of target policies $\\{\\pi_{1},\\dots,\\pi_{m}\\}$ , effectively seeking to determine $\\mathrm{argmin}_{k\\in[m]}\\,R(\\pi_{k})$ . Third, OPL aims to find a policy $\\hat{\\pi}_{n}^{\\mathrm{L}}$ within the potentially infinite policy space $\\Pi$ that achieves the lowest risk, essentially aiming to find $\\textstyle\\operatorname{argmin}_{\\pi\\in\\Pi}R(\\pi)$ . In general, both OPS and OPL rely on OPE\u2019s initial estimation of the risk. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Regularized IPS. Our work focuses on the inverse propensity scoring (IPS) estimator [24]. IPS approximates the risk of a policy $\\pi$ , $R(\\pi)$ , by adjusting the contribution of each sample in logged data according to its importance weight (IW), which is the ratio of the probability of an action under the target policy $\\pi$ to its probability under the behavior policy $\\pi_{0}$ , ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\hat{R}_{n}(\\pi)=\\frac{1}{n}\\sum_{i=1}^{n}w_{\\pi}(x_{i},a_{i})c_{i}\\,,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where for any $(x,a)\\,\\in\\,\\mathcal{X}\\times\\mathcal{A}\\,,w_{\\pi}(x,a)\\,=\\,\\pi(a|x)/\\pi_{0}(a|x)$ are the IWs. IPS is unbiased under the coverage assumption (see for example Owen [39, Chapter 9]). However, it can suffer high variance, which tends to scale linearly with IWs [57]. This issue becomes pronounced when there is a significant discrepancy between the target policy $\\pi$ and the behavior policy $\\pi_{0}$ . To mitigate this, a common strategy consists in applying a regularization function $h:[\\bar{0},1]^{2}\\times[-1,0]\\to\\bar{(}-\\infty,0]$ to $\\pi(a|x)$ , $\\pi_{0}(a|x)$ and $c$ . This function is designed to reduce the estimator\u2019s variance at the cost of introducing some bias. Formally, the function $h$ needs to satisfy the condition (C1), that is defined by ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\iff\\forall(p,q,c)\\in[0,1]^{2}\\times[-1,0],\\quad p c/q\\leq h(p,q,c)\\leq0.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "With such function $h$ , the regularized IPS estimator reads ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\hat{R}_{n}^{h}(\\pi)=\\frac{1}{n}\\sum_{i=1}^{n}h\\left(\\pi(a_{i}|x_{i}),\\pi_{0}(a_{i}|x_{i}),c_{i}\\right)=\\frac{1}{n}\\sum_{i=1}^{n}h_{i}\\,,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $h_{i}\\,=\\,h\\left(\\pi(a_{i}|x_{i}),\\pi_{0}(a_{i}|x_{i}),c_{i}\\right)$ . We recover standard IPS in (2) when $h(p,q,c)\\,=\\,p c/q$ . Numerous regularization functions $h$ were studied in the literature. For example, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{h(p,q,c)=\\operatorname*{min}(p/q,M)c\\,,M\\in\\mathbb{R}^{+}\\implies\\mathrm{Clipping~[10]}~,}\\\\ &{h(p,q,c)=p c/q^{\\alpha}\\,,\\alpha\\in[0,1]\\implies\\mathrm{Exponential~Smoothing~[5]}\\,,}\\\\ &{h(p,q,c)=p c/(q+\\gamma)\\,,\\gamma\\ge0\\implies\\mathrm{Implicit~Exploration~[21]}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Other IW regularizations include Harmonic [38] and Shrinkage [54]. With $h$ satisfying (C1), we can derive our core result: a family of high-probability bounds that hold for regularized IPS. ", "page_idx": 2}, {"type": "text", "text": "3 Pessimistic off-policy evaluation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Standard OPE directly uses estimates of the risk, without capturing their associated uncertainty. This limits its effectiveness in critical applications. Pessimistic OPE addresses this issue by relying on finite sample, high-probability upper bounds to assess any policy\u2019s worst-case risk [15, 32]. This section contributes to this effort and focuses on providing novel, finite sample, tight upper bounds on the risk. This is achieved by deriving general bounds applicable to regularized IPS in (3). ", "page_idx": 2}, {"type": "text", "text": "3.1 Preliminaries and unified risk bounds ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Let $\\lambda>0,\\pi\\in\\Pi$ , and $h$ satisfying (C1), we define ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\hat{\\mathcal{M}}_{n}^{h,\\ell}(\\pi)=\\frac{1}{n}\\sum_{i=1}^{n}h_{i}^{\\ell}\\,,\\qquad\\qquad\\mathrm{and}\\quad\\psi_{\\lambda}(x)=\\frac{1}{\\lambda}\\left(1-\\exp(-\\lambda x)\\right),\\,\\forall x\\in\\mathbb{R}\\,,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\hat{\\mathcal{M}}_{n}^{h,\\ell}(\\pi)$ is the empirical $\\ell_{}$ -th moment of regularized IPS $\\hat{R}_{n}^{h}(\\pi)$ , and $\\psi_{\\lambda}\\,:\\,\\mathbb{R}\\,\\rightarrow\\,\\mathbb{R}$ is a contraction function satisfying $\\psi_{\\lambda}(x)\\leq x$ for any $x\\in\\mathbb R$ . Then, we state our first result. ", "page_idx": 2}, {"type": "text", "text": "Proposition 1 (Empirical moments risk bound). Let $\\pi\\,\\in\\,\\Pi$ , $L\\geq1$ , $\\delta\\,\\in\\,(0,1],\\;\\lambda\\,>\\,0,$ , and $h$ satisfying (C1). Then it holds with probability at least $1-\\delta$ that ", "page_idx": 2}, {"type": "equation", "text": "$$\nR(\\pi)\\leq U_{L}^{\\lambda,h}(\\pi)\\,,\\quad w i t h\\quad U_{L}^{\\lambda,h}(\\pi)=\\psi_{\\lambda}\\Bigl(\\hat{R}_{n}^{h}(\\pi)+\\sum_{\\ell=2}^{2L}\\frac{\\lambda^{\\ell-1}}{\\ell}\\hat{\\mathcal{M}}_{n}^{h,\\ell}(\\pi)+\\frac{\\ln(1/\\delta)}{\\lambda n}\\Bigr)\\,,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\psi_{\\lambda}$ and $\\hat{\\mathcal{M}}_{n}^{h,\\ell}(\\pi)$ are both defined in (5), and recall that $\\psi_{\\lambda}(x)\\leq x$ . ", "page_idx": 2}, {"type": "text", "text": "In Appendix F.1, we provide detailed proof, leveraging Chernoff bounds with a careful analysis of the moment-generating function. This results in the first empirical, high-order moment bound for offilne contextual bandits, with several advantages. First, the bound applies to any regularization function $h$ that satisfies the mild condition (C1), enabling the design of a tailored $h$ that minimizes the bound. Second, it relies solely on empirical moments, without assuming the existence of theoretical moments. Third, the bound is fully empirical and tractable, facilitating efficient implementation of pessimism. Lastly, the parameter $L$ controls the number of moments used, allowing a balance between bound tightness and computational cost. Specifically, for sufficiently small values of $\\lambda$ , higher values of $L$ yield tighter bounds, though potentially at the cost of increased computational complexity as we would need to compute higher order moments. This is formally stated as follows. ", "page_idx": 3}, {"type": "text", "text": "Proposition 2 (Impact of $L$ ). Let $\\pi\\in\\Pi$ , $\\delta\\in(0,1]$ , $\\lambda>0,$ , $L\\geq1$ , and $h$ satisfying (C1). Then, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\lambda\\leq\\operatorname*{min}_{i\\in[n]}\\left\\{\\frac{2L+2}{(2L+1)|h_{i}|}\\right\\}\\implies U_{L+1}^{\\lambda,h}(\\pi)\\leq U_{L}^{\\lambda,h}(\\pi)\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "From (7), the bound $U_{L}^{\\lambda,h}(\\pi)$ in (6) becomes a decreasing function of $L$ when $\\begin{array}{r}{\\lambda\\leq\\operatorname*{min}_{i\\in[n]}(1/|h_{i}|)}\\end{array}$ suggesting that for sufficiently small $\\lambda$ , the tightest bound is achieved as $L\\rightarrow\\infty$ . This condition on $\\lambda$ also depends on the values of $h$ , highlighting the importance of the regularizer choice $h$ . In fact, once we evaluate our bounds at their optimal regularizer function $h$ , this condition on $\\lambda$ becomes unnecessary when comparing some of the optimal bounds. Specifically, we demonstrate in the following proposition that the bound with $L=1$ can be always improved by increasing $L$ . ", "page_idx": 3}, {"type": "text", "text": "Proposition 3 (Comparison of our bounds). Let $\\pi\\in\\Pi$ , and $\\lambda>0$ , we define ", "page_idx": 3}, {"type": "equation", "text": "$$\nU_{L}^{\\lambda}(\\pi)=\\operatorname*{min}_{h}U_{L}^{\\lambda,h}(\\pi)\\,,\\quad a n d\\quad h_{*,L}=\\operatorname*{argmin}_{h}U_{L}^{\\lambda,h}(\\pi)\\,,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "with the minimum taken over $h$ satisfying (C1). Then, for any $\\lambda>0$ , it holds that for any $L>1$ , $U_{L}^{\\lambda}(\\pi)\\leq U_{1}^{\\lambda}(\\pi)$ . In particular, for any $\\lambda>0$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\nU_{\\infty}^{\\lambda}(\\pi)\\leq U_{1}^{\\lambda}(\\pi)\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Proposition 3 shows that, irrespective of the value of $\\lambda$ , the bound with $L=1$ can be always improved by bounds of increased moment order $L$ , evaluated at their optimal regularizer $h_{*,L}$ . This result encourages us to study bounds with high moment order $L$ , especially if we can derive their optimal regularizers $h_{*,L}$ . To this end, we examine two cases: $L=1$ , which results in an empirical secondmoment bound, and $L\\to\\infty$ , yielding a tight bound that does not require computing high-order moments. For each case, we identify the function $h$ that minimizes the bound. If the minimizer for $L=1$ is a variant of the clipping estimator [10], minimizing $L\\rightarrow\\infty$ motivates a novel logarithmic smoothing estimator. We begin by analyzing our empirical moment risk bound at $L=1$ . ", "page_idx": 3}, {"type": "text", "text": "3.2 Global clipping ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Corollary 4 (Empirical second-moment risk bound with $L=1$ ). Let $\\pi\\in\\Pi_{i}$ , $\\delta\\in(0,1]$ , $\\lambda>0$ , and $h$ satisfying (C1). Then it holds with probability at least $1-\\delta$ that ", "page_idx": 3}, {"type": "equation", "text": "$$\nR(\\pi)\\leq\\psi_{\\lambda}\\Bigl(\\hat{R}_{n}^{h}(\\pi)+\\frac{\\lambda}{2}\\hat{\\mathcal{M}}_{n}^{h,2}(\\pi)+\\frac{\\ln(1/\\delta)}{\\lambda n}\\Bigr)\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "This is a direct consequence of (6) when $L=1$ . The bound holds for any $h$ satisfying (C1). Thus we search for a function $h_{*,1}$ that minimizes bound in (10). This function $h_{*,1}$ writes ", "page_idx": 3}, {"type": "equation", "text": "$$\nh_{*,1}(p,q,c)=-\\operatorname*{min}(p|c|/q,1/\\lambda)\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In particular, if we assume that costs are binary, $c\\in\\{-1,0\\}$ , then $h_{*,1}$ corresponds to clipping in (4) with parameter $M=1/\\lambda$ . This is because $-\\operatorname*{min}(|c|p/q,1/\\lambda)=\\operatorname*{min}\\left(p/q,{\\textstyle{\\frac{1}{\\lambda}}}\\right)c$ when $c$ is binary. This motivates the widely used clipping estimator [10]. However, this also suggests that the standard way of clipping (as in (4)) is only optimal1 for binary costs. \u221aIn general, the cost should also be clipped (as in (11)). Finally, with a suitable choice of $\\lambda={\\mathcal{O}}(1/{\\sqrt{n}})$ , our bound in Corollary 4, using clipping (i.e., $h=h_{*,1}$ ), outperforms the existing empirical Bernstein bound [55], which was specifically derived for clipping. This confirms the strength of our general bound, as minimizing it results in a bound with tighter concentration than specialized bounds. Appendix F.4 gives the the proof to find $h_{*,1}$ and formal comparisons with empirical Bernstein are provided in Appendix F.5. In the next section, we study our general bound when we set $L\\rightarrow\\infty$ . ", "page_idx": 3}, {"type": "text", "text": "3.3 Logarithmic smoothing ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Corollary 5 (Empirical infinite-moment bound with $L\\to\\infty$ ). Let $\\pi\\in\\Pi$ , $\\delta\\in(0,1]$ , $\\lambda>0$ , and $h$ satisfying (C1). Then it holds with probability at least $1-\\delta$ that ", "page_idx": 4}, {"type": "equation", "text": "$$\nR(\\pi)\\le\\psi_{\\lambda}\\Bigl(-\\,\\frac{1}{n}\\sum_{i=1}^{n}\\frac{1}{\\lambda}\\log\\left(1-\\lambda h_{i}\\right)+\\frac{\\ln(1/\\delta)}{\\lambda n}\\Bigr)\\,.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Appendix F.6 provides detailed proof. Setting $L\\rightarrow\\infty$ in (6) results in the bound in Corollary 5, which has different properties than Corollary 4. The resulting bound has a simple expression that does not require computing high order moments. This means that we can obtain the best of both worlds, a tight concentration bound with no additional computational complexity. As the bound is increasing in $h$ , the function $h_{*,\\infty}$ that minimizes this bound is $h_{*,\\infty}(p,q,\\bar{c})=\\dot{p c}/q$ . This corresponds to the standard IPS in (2). This differs from the $L=1$ bound in Corollary 4 that favored clipping. This shows the impact of the moment order $L$ on the optimal function $h$ . For any $\\pi\\in\\Pi$ , applying the bound in Corollary 5 with the optimal $h_{*,\\infty}$ leads to $U_{\\infty}^{\\lambda}(\\pi)$ , of the following expression: ", "page_idx": 4}, {"type": "equation", "text": "$$\nU_{\\infty}^{\\lambda}(\\pi)=\\psi_{\\lambda}\\Bigl(\\hat{R}_{n}^{\\lambda}(\\pi)+\\frac{\\ln(1/\\delta)}{\\lambda n}\\Bigr)\\,.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Even if we set $h_{*,\\infty}(p,q,c)=p c/q$ (without IW regularization), $U_{\\infty}^{\\lambda}(\\pi)$ can be seen as a risk upper bound of a novel regularized IPS estimator (satisfying (C1)), called Logarithmic Smoothing (LS): ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{R}_{n}^{\\lambda}(\\pi)=-\\frac{1}{n}\\sum_{i=1}^{n}\\frac{1}{\\lambda}\\log\\left(1-\\lambda w_{\\pi}(x_{i},a_{i})c_{i}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The LS estimator in (14) is defined for any non-negative $\\lambda\\geq0$ , with its bound in (13) holding for any positive $\\lambda\\,>\\,0$ . Notably, $\\lambda=0$ retrieves the standard IPS estimator in (2), while $\\lambda>0$ introduces a bias-variance trade-off by logarithmically smoothing the IWs (Figure 1). This estimator acts as a soft, differentiable variant of clipping with parameter $1/\\lambda$ . A Taylor expansion of our estimator around $\\lambda=0$ yields ", "page_idx": 4}, {"type": "image", "img_path": "zLClygeRK8/tmp/619a3b05a455efd1e9cc600515650e91a2b2af2a9632fa041db3f406a904606a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{R}_{n}^{\\lambda}(\\pi)=\\hat{R}_{n}(\\pi)+\\sum_{\\ell=2}^{\\infty}\\frac{\\lambda^{\\ell-1}}{\\ell}\\Big(\\frac1n\\sum_{i=1}^{n}\\left(w_{\\pi}(x_{i},a_{i})c_{i}\\right)^{\\ell}\\Big)\\,.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Thus, LS is a pessimistic estimator by design, implicitly im- Figure 1: LS with different $\\lambda s$ . plementing a form of Sample All Moments Penalization, which generalizes the Sample Variance Penalization [55]. To examine the statistical properties of our estimator, we introduce ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\displaystyle{S_{\\lambda}(\\pi)=\\mathbb{E}\\left[\\frac{(w_{\\pi}(x,a)c)^{2}}{(1-\\lambda w_{\\pi}(x,a)c)}\\right],}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "which quantifies the discrepancy between $\\pi$ and $\\pi_{0}$ . Notably, ${\\mathcal{S}}_{\\lambda}(\\pi)$ is always smaller than the second moment of the IW, effectively interpolating between a weighted first moment $(\\lambda\\gg1)$ ) and the second moment $\\lambda=0$ ) of IPS. This quantity $\\mathcal{S}_{\\lambda}$ characterizes the concentration properties of the LS estimator akin to the coverage ratio for IX estimator [21]. With $\\mathcal{S}_{\\lambda}$ defined, we proceed by bounding the mean squared error (MSE) of our estimator, specifically bounding its bias and variance. ", "page_idx": 4}, {"type": "text", "text": "Proposition 6 (Bias-variance trade-off). Let $\\pi\\in\\Pi$ and $\\lambda\\geq0$ . Let $B^{\\lambda}(\\pi)$ and $\\mathcal{V}^{\\lambda}(\\pi)$ be respectively the bias and the variance of the LS estimator. Then we have that ", "page_idx": 4}, {"type": "equation", "text": "$$\n0\\leq\\beta^{\\lambda}(\\pi)\\leq\\lambda S_{\\lambda}(\\pi)\\,,\\quad a n d\\quad\\gamma^{\\lambda}(\\pi)\\leq\\frac{S_{\\lambda}(\\pi)}{n}\\,.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Moreover, it holds that for any $\\lambda>0$ , the variance is finite as $\\nu^{\\lambda}(\\pi)\\leq|R(\\pi)|/\\lambda n\\leq1/\\lambda n.$ ", "page_idx": 4}, {"type": "text", "text": "We observe that both the bias and variance are controlled by ${\\mathcal{S}}_{\\lambda}(\\pi)$ . Particularly, $\\lambda=0$ recovers the IPS estimator in (2), with zero bias and a variance bounded by $\\mathbb{E}\\left[w^{2}(x,a)c^{2}\\right]/n$ . When $\\lambda>0$ , a bias-variance trade-off emerges. The bias is always non-negative and is capped at $\\lambda S_{\\lambda}(\\pi)$ , which diminishes to zero when $\\lambda$ is small and goes to $|R(\\pi)|$ as $\\lambda$ increases. Conversely, the variance decreases with a higher $\\lambda$ . Notably, $\\lambda\\:>\\:0$ ensures finite variance bounded by $1/\\lambda n$ , despite the estimator being unbounded. This is different from previous estimators that relied on boun\u221aded functions to ensure finite variance. We also prove in the following that a good choice of $\\lambda={\\mathcal{O}}(1/{\\sqrt{n}})$ ensures that our LS estimator enjoys a sub-Gaussian concentration [38]. ", "page_idx": 4}, {"type": "text", "text": "Proposition 7 (Sub-Gaussianity and comparison with Metelli et al. [38]). Let $\\pi\\in\\Pi$ , $\\delta\\in(0,1]$ and $\\lambda>0$ . Then the following inequalities holds with probability at least $1-\\delta$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\nR(\\pi)-\\hat{R}_{n}^{\\lambda}(\\pi)\\leq\\frac{\\ln(2/\\delta)}{\\lambda n}\\,,\\qquad a n d\\qquad\\hat{R}_{n}^{\\lambda}(\\pi)-R(\\pi)\\leq\\lambda S_{\\lambda}(\\pi)+\\frac{\\ln(2/\\delta)}{\\lambda n}\\,.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In particular, setting $\\lambda=\\lambda_{*}=\\sqrt{\\ln(2/\\delta)/n\\mathbb{E}\\left[w_{\\pi}(x,a)^{2}c^{2}\\right]}$ yields that ", "page_idx": 5}, {"type": "equation", "text": "$$\n|R(\\pi)-\\hat{R}_{n}^{\\lambda_{*}}(\\pi)|\\leq\\sqrt{2\\sigma^{2}\\ln(2/\\delta)}\\,,\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\,w h e r e\\ \\sigma^{2}=2\\mathbb{E}\\left[w_{\\pi}(x,a)^{2}c^{2}\\right]/n\\,.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Thus, a particular choice of $\\lambda^{*}$ ensures that $\\hat{R}_{n}^{\\lambda_{*}}(\\pi)$ is sub-Gaussian, with a variance proxy $\\sigma^{2}$ that improves on that obtained for the Harmonic estimator of Metelli et al. [38]. We refer the interested reader to Appendix E.2 for further discussions and proofs. ", "page_idx": 5}, {"type": "text", "text": "Next, we focus on the tightness of the LS upper bound in (13) as it will motivate our selection and learning strategies. Proposition 3 already showed that $U_{\\infty}^{\\lambda}(\\pi)$ , the bound of LS is tighter than $U_{1}^{\\lambda}(\\pi)$ , the bound in Corollary 4 evaluated at the Global clipping function $h_{*,1}$ . In this section, we compare the LS bound to the already tight IX bound presented by Gabbianelli et al. [21] and demonstrate in the following that the LS bound dominates it in all scenarios. ", "page_idx": 5}, {"type": "text", "text": "Proposition 8 (Comparison with IX of Gabbianelli et al. [21]). Let $\\pi\\in\\Pi$ , $\\delta\\in]0,1]$ and $\\lambda>0$ , the IX bound from $I2I J$ states that we have with probability at least $1-\\delta$ ", "page_idx": 5}, {"type": "equation", "text": "$$\nR(\\pi)\\le\\hat{R}_{n}^{\\lambda\\mathrm{-}\\mathrm{IX}}(\\pi)+\\frac{\\ln(1/\\delta)}{\\lambda n}\\,,\\quad w i t h\\quad\\hat{R}_{n}^{\\lambda\\mathrm{-}\\mathrm{IX}}(\\pi)=\\frac{1}{n}\\sum_{i=1}^{n}\\frac{\\pi(a_{i}|x_{i})}{\\pi_{0}(a_{i}|x_{i})+\\lambda/2}c_{i}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Let $U_{\\mathrm{IX}}^{\\lambda}(\\pi)$ be the upper bound of (17), we have for any $\\lambda>0$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\nU_{\\infty}^{\\lambda}(\\pi)\\leq U_{\\mathrm{IX}}^{\\lambda}(\\pi)\\,.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "This result states that no matter the scenario, for any evaluated policy $\\pi$ , and any chosen $\\lambda>0$ , the LS bound will be always tighter than IX. The gap between the LS and IX bounds increases when $n$ is small, or when the evaluated policy $\\pi$ is stochastic, as demonstrated and developed in Appendix F.8. These findings further validate the effectiveness of our approach, enabling us to identify the LS estimator, with an empirical bound that improves upon the tightest existing bounds. Consequently, we leverage the LS bound in the next section to derive our pessimistic OPS and OPL strategies. ", "page_idx": 5}, {"type": "text", "text": "4 Off-policy selection and learning ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1 Off-policy selection ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Let $\\Pi_{s}=\\{\\pi_{1},...,\\pi_{m}\\}$ be a finite set of policies. In OPS, the goal is to find $\\pi_{*}^{\\mathrm{{s}}}\\in\\Pi_{\\mathrm{s}}$ that satisfies ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\pi_{*}^{\\mathrm{s}}=\\operatorname*{argmin}_{\\pi\\in\\Pi_{s}}R(\\pi)=\\operatorname*{argmin}_{k\\in[m]}R(\\pi_{k})\\,.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "As we do not have access to the true risk, we use a data-driven selection strategy that guarantees the identification of policies of performance close to that of $\\pi_{*}^{\\mathrm{s}}$ . Precisely, for $\\lambda>0$ , we search for ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\hat{\\pi}_{n}^{\\mathrm{s}}=\\underset{\\pi\\in\\Pi_{\\mathrm{s}}}{\\operatorname{argmin}}\\,\\hat{R}_{n}^{\\lambda}(\\pi)=\\underset{k\\in[m]}{\\operatorname{argmin}}\\,\\hat{R}_{n}^{\\lambda}(\\pi_{k})\\,.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "To derive our strategy in (20), we minimize the bound of LS in (13), employing pessimism [27]. Fortunately, in our case, this boils down to minimizing $\\hat{R}_{n}^{\\lambda}(\\pi)$ , since the other terms in the bound are independent of the target policy $\\pi$ . This allows us to avoid computing complex statistics [55, 32] and does not require access to the behavior policy $\\pi_{0}$ . As we show next, it also ensures low suboptimality. ", "page_idx": 5}, {"type": "text", "text": "Proposition 9 (Suboptimality of our selection strategy in (20)). Let $\\lambda>0$ and $\\delta\\in(0,1]$ . Then, it holds with probability at least $1-\\delta$ that ", "page_idx": 5}, {"type": "equation", "text": "$$\n0\\leq R(\\hat{\\pi}_{n}^{s})-R(\\pi_{*}^{s})\\leq\\lambda S_{\\lambda}(\\pi_{*}^{s})+\\frac{2\\ln(2|\\Pi_{s}|/\\delta)}{\\lambda n}\\,,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $S_{\\lambda}(\\pi),\\,\\pi_{*}^{\\mathrm{s}}$ and $\\hat{\\pi}_{n}^{\\mathrm{s}}$ are defined in (15), (19) and (20). ", "page_idx": 5}, {"type": "text", "text": "The derived suboptimality bound only requires coverage of the optimal actions (support of the optimal policy $\\pi_{*}^{s}$ ), and improves on IX suboptimality [21], matching the minimax suboptimality lower bound of pessimistic methods [34, 27, 28]. Appendix G.1 provides proof of this suboptimality bound, and we discuss how this suboptimality improves upon existing strategies in Appendix E.3. By selecting $\\lambda_{n}^{s}=\\sqrt{2\\ln(2\\vert\\Pi_{\\mathrm{s}}\\vert/\\delta)/n}$ for LS, we achieve a suboptimality scaling of $\\mathcal{O}(1/\\sqrt{n})$ , ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{0\\leq R(\\hat{\\pi}_{n}^{\\mathrm{s}})-R(\\pi_{*}^{\\mathrm{s}})\\leq\\left(1+\\mathcal{S}_{\\lambda_{n}^{s}}(\\pi_{*}^{\\mathrm{s}})\\right)\\sqrt{2\\ln(2|\\Pi_{\\mathrm{s}}|/\\delta)/n},}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "which ensures finding the optimal policy with sufficient samples. Additionally, the multiplicative constant is smaller when $\\pi_{0}$ is close to $\\pi_{*}^{\\mathrm{s}}$ , confirming the known observation that it is easier to identify the best policy if it is similar to the behavior policy $\\pi_{0}$ . ", "page_idx": 6}, {"type": "text", "text": "4.2 Off-policy learning ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Similar to how we extended the evaluation bound in Corollary 5 (which applies to a single fixed target policy) to OPS (where it applies to a finite set of target policies), we can further derive bounds for an infinite policy class $\\Pi$ , enabling OPL. Several approaches have been proposed in previous work, primarily based on replacing the finite union bound over policies with more sophisticated uniform-convergence arguments. This was used by [55], which derived a variance-sensitive bound scaling with the covering number [61]. Since these approaches incorporate a complexity term that depends only on the policy class, the resulting pessimistic learning strategy (which minimizes the upper bound) would be similar to the selection strategy adopted earlier, leading, for a fixed $\\lambda$ , to ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\hat{\\pi}_{n}^{\\mathrm{L}}=\\operatorname*{argmin}_{\\pi\\in\\Pi}\\hat{R}_{n}^{\\lambda}(\\pi)+\\frac{\\mathcal{C}(\\Pi)}{\\lambda n}=\\operatorname*{argmin}_{\\pi\\in\\Pi}\\hat{R}_{n}^{\\lambda}(\\pi).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where ${\\mathcal{C}}(\\Pi)$ is a complexity measure [61]. This learning strategy is straightforward because it involves a smooth estimator that can be optimized using first-order methods and does not require second-order statistics. However, analyzing this approach is more challenging because the complexity measure ${\\mathcal{C}}(\\Pi)$ varies depending on the policy class considered, is often intractable [49] and can only be upper bounded with problem dependent constants [28]. ", "page_idx": 6}, {"type": "text", "text": "Instead of the method described above, we derive PAC-Bayesian generalization bounds [37, 11] that apply to arbitrary policy classes. This framework has been shown to provide strong performance guarantees for OPL in practical scenarios [49, 5]. The PAC-Bayesian framework analyzes the performance of policies by viewing them as randomized predictors [35]. Specifically, let ${\\mathcal{F}}(\\Theta)=$ $\\ \\dot{\\left\\{f_{\\theta}:\\mathcal X\\rightarrow[K],\\dot{\\theta}\\in\\Theta\\right\\}}$ be a set of parameterized predictors that associate the context $x$ with the action $f_{\\theta}(x)\\in[K]$ . Let $\\mathcal{P}(\\Theta)$ be the set of all probability distributions on $\\Theta$ . Each distribution $Q\\in\\mathcal{P}(\\Theta)$ defines a policy $\\pi_{Q}$ by setting the probability of action $a$ given context $x$ as the probability that a random predictor $f_{\\theta}\\sim\\mathrm{\\mathit{Q}}$ maps $x$ to action $a$ , that is, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\pi_{Q}(a|x)=\\mathbb{E}_{\\theta\\sim Q}\\left[\\mathbb{1}\\left[f_{\\theta}(x)=a\\right]\\right]\\,,\\qquad\\qquad\\qquad\\forall(x,a)\\in\\mathcal{X}\\times\\mathcal{A}\\,.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "This characterization is not restrictive as any policy can be represented in this form [49]. Deriving PAC-Bayesian generalization bounds with this policy definition requires the regularized IPS to be linear in the target policy $\\pi$ [35, 5, 21]. Our estimator LS in (14) is non-linear in $\\pi$ . Therefore, for this PAC-Bayesian analysis, we introduce a linearized variant of LS, called LS-LIN, and defined as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\hat{R}_{n}^{\\lambda\\mathrm{{-LIN}}}(\\pi)=-\\frac{1}{n}\\sum_{i=1}^{n}\\frac{\\pi(a_{i}|x_{i})}{\\lambda}\\log\\left(1-\\frac{\\lambda c_{i}}{\\pi_{0}(a_{i}|x_{i})}\\right),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "which smooths the impact of the behavior propensity $\\pi_{0}$ instead of the IWs $\\pi/\\pi_{0}$ . We provide in the following a core result of this section, the PAC-Bayesian bound that defines our learning strategy. ", "page_idx": 6}, {"type": "text", "text": "Proposition 10 (PAC-Bayes learning bound for $\\hat{R}_{n}^{\\lambda}$ -LIN). Given a prior $P\\in\\mathcal P(\\Theta)$ , $\\delta\\in(0,1]$ and $\\lambda>0$ , the following holds with probability at least $1-\\delta$ : ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\forall Q\\in\\mathcal{P}(\\Theta),\\quad R(\\pi_{Q})\\leq\\psi_{\\lambda}\\Big(\\hat{R}_{n}^{\\lambda\\mathrm{-}\\mathrm{LIN}}(\\pi_{Q})+\\frac{\\mathcal{K L}(Q||P)+\\ln\\frac{1}{\\delta}}{\\lambda n}\\Big)\\,,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\kappa\\mathcal{L}(Q||P)$ is the Kullback-Leibler divergence from $P$ to $Q$ . ", "page_idx": 6}, {"type": "text", "text": "PAC-Bayes bounds hold uniformly for all distributions $Q\\ \\in\\ \\mathcal{P}(\\Theta)$ and replace the complexity measure ${\\mathcal{C}}(\\Pi)$ with the divergence $\\kappa\\mathcal{L}(Q||P)$ from a reference prior distribution $P$ . Extensive research focuses on identifying the best strategies for choosing this prior $P$ [40]. While these bounds hold for any fixed prior $P$ , in practice, it is typically set to the distribution inducing the behavior policy $\\pi_{0}$ , meaning $P$ satisfies $\\pi_{0}=\\pi_{P}$ . This leads to an intuitive learning principle: by minimizing the upper bound, we seek policies with good empirical risk that do not deviate significantly from $\\pi_{0}$ . ", "page_idx": 7}, {"type": "text", "text": "Our bound can also be obtained using the truncation method from Alquier [1, Corollary 2.5]. This bound surpasses the already tight PAC-Bayesian bounds derived for Clipping [49], Exponential Smoothing [5], and Implicit Exploration [21], resulting in the tightest known generalization bound in OPL. Appendix G.2 gives formal proof of this bound and comparisons with existing PAC-Bayesian bounds can be found in Appendix E.4. For a fixed $\\lambda$ and a fixed prior $P$ , we derive a learning strategy that minimizes the upper bound for a subset ${\\mathcal{L}}(\\Theta)\\subseteq{\\mathcal{P}}(\\Theta)$ of distributions, seeking ", "page_idx": 7}, {"type": "equation", "text": "$$\nQ_{n}=\\operatorname*{argmin}_{Q\\in\\mathcal{L}(\\Theta)}\\left\\{\\hat{R}_{n}^{\\lambda\\mathrm{{LIN}}}(\\pi_{Q})+\\frac{K\\mathcal{L}(Q||P)}{\\lambda n}\\right\\}\\,,\\quad\\mathrm{and\\;setting}\\;\\hat{\\pi}_{n}^{\\mathrm{L}}=\\pi_{Q_{n}}\\,.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "(27) is tractable and can be efficiently optimized for various policy classes [49, 5]. Below, we analyze its suboptimality compared to the best policy in the chosen class, $\\pi_{Q^{*}}=\\operatorname{argmin}_{Q\\in{\\mathcal{L}}(\\Theta)}R(\\pi_{Q})$ . ", "page_idx": 7}, {"type": "text", "text": "Proposition 11 (Suboptimality of the learning strategy in (27)). Let $\\lambda>0$ , $P\\in{\\mathcal{L}}(\\Theta)$ and $\\delta\\in(0,1]$ . Then, it holds with probability at least $1-\\delta$ that ", "page_idx": 7}, {"type": "equation", "text": "$$\n0\\leq R(\\hat{\\pi}_{n}^{\\mathrm{L}})-R(\\pi_{Q^{*}})\\leq\\lambda S_{\\lambda}^{\\mathrm{LIN}}(\\pi_{Q^{*}})+\\frac{2\\left(K\\mathcal{L}(Q^{*}||P)+\\ln(2/\\delta)\\right)}{\\lambda n},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Our suboptimality bound only requires coverage of the support of the optimal policy $\\pi_{Q_{*}}$ . This bound matches the minimax suboptimality lower bound of pessimistic learning with deterministic policies [28]. Appendix G.3 provides a proof of Proposition 11, while Appendix E.5 discusses the suboptimality bou\u221and further and proves that it improves on the IX learning \u221astrategy of [21, Section 5]. Setting $\\lambda_{n}^{l}\\doteq2/\\sqrt{n}$ guarantees us a suboptimality that scales with $\\mathcal{O}(1/\\sqrt{n})$ as ", "page_idx": 7}, {"type": "equation", "text": "$$\n0\\leq R(\\hat{\\pi}_{n}^{\\mathrm{L}})-R(\\pi_{Q^{*}})\\leq(2S_{\\lambda_{n}^{l}}^{\\mathrm{LN}}(\\pi_{Q^{*}})+K\\mathcal{L}(Q^{*}||P)+\\ln(2/\\delta))/\\sqrt{n}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "By setting the reference $P$ to the distribution inducing $\\pi_{0}$ , we find that the learning suboptimality is reduced when the behavior policy $\\pi_{0}$ is close to the optimal policy $\\pi_{Q^{*}}$ . This is similar to the suboptimality for our selection strategy. The suboptimality upper bound reflects a common intuition in the OPL literature: pessimistic learning algorithms converge faster when $\\pi_{0}$ is close to $\\pi_{Q^{*}}$ . ", "page_idx": 7}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Our experimental setup follows the standard multiclass-to-bandit conversion used in prior studies [18, 55]. Each multi-class dataset has features and labels and we convert it to contextual bandit problems where contexts correspond to features and actions to labels. Precisely, the reward $r$ for taking action (label) $a$ with context (features) $x$ is modeled as Bernoulli with probability $p_{x}=$ $\\epsilon+\\mathbb{1}^{-}[a=\\rho(x)]\\left(1-2\\epsilon\\right)$ , where $\\rho(x)$ be the true label of features $x$ , and $\\epsilon$ is a noise parameter. In particular, the true label $\\rho(x)$ represents the action with the highest average reward for context $x$ . This setup ensures an average reward of $1-\\epsilon$ for the optimal action $\\rho(x)$ and $\\epsilon$ for all others, constructing a logged bandit feedback dataset in the form $\\{x_{i},a_{i},c_{i}\\}_{i\\in[n]}$ , where $c_{i}=-r_{i}$ is the associated cost. ", "page_idx": 7}, {"type": "text", "text": "5.1 Off-policy evaluation and selection experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "For both evaluation and selection, we adopt the same experimental design as [32] to facilitate the comparison. We consider exponential target policies $\\begin{array}{r}{\\pi(a|\\dot{\\boldsymbol{x}})\\propto\\exp(\\frac{1}{\\tau}f(a,\\boldsymbol{\\bar{x}}))}\\end{array}$ , with $\\tau$ a temperature controlling the policy\u2019s entropy and $f(a,x)$ the score of the item $a$ for the context $x$ . We use this to define ideal policies as $\\pi^{\\pm{\\mathsf{d e a l}}}(a|x)\\propto\\exp(\\frac{1}{\\tau}\\mathbb{I}\\{\\rho(x)=a\\})$ , and also create faulty, mismatching policies for which the peak is shifted to another, wrong action for a set of faulty actions $F\\subset[K]$ . To recreate real world scenarios, we also consider policies directly learned from logged bandit feedback, of the form $\\begin{array}{r}{\\pi_{\\theta^{\\mathrm{IPS}}}(a|x)\\propto\\exp(\\frac{1}{\\tau}x^{t}\\theta_{a}^{\\mathrm{IPS}})}\\end{array}$ and $\\begin{array}{r}{\\pi_{\\theta^{\\mathrm{ss}}}(a|x)\\propto\\exp(\\frac{1}{\\tau}x^{t}\\theta_{a}^{\\mathrm{sN}})}\\end{array}$ , with their parameters learned by respectively minimizing the IPS [24] and SN [56] empirical risks. More details on the definition of the different policies are given in Appendix H. Finally, 11 real multiclass classification datasets are chosen from the UCI ML Repository [8] (See Table 3 in Appendix H.1.1) with various number of samples, dimensions and action space sizes to conduct our experiments2. ", "page_idx": 7}, {"type": "table", "img_path": "zLClygeRK8/tmp/b6be351a524990297f38917effc4509a4a77aac7ab27e2870be0ae46d122c1fa.jpg", "table_caption": ["Table 1: Bound\u2019s tightness $(|U(\\pi)/R(\\pi)-1|)$ with varying number of samples of the kropt dataset. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "(OPE) Tightness of the bounds. Evaluating the worst case performance of a policy is done through evaluating risk upper bounds [10, 32]. This means that a better evaluation will solely depend on the tightness of the bounds used. To this end, given a policy $\\pi$ , we are interested in bounds $U(\\pi)$ with a small relative radius $|U(\\pi)/R(\\pi)-1|$ . We compare our newly derived bounds $({\\mathsf{c I P S}}{\\mathsf{-L}}{\\mathsf{=}}1$ for $U_{1}^{\\lambda}$ and LS for $U_{\\infty}^{\\lambda}$ both with $\\lambda=1/\\sqrt{n})$ to empirical evaluation bounds of the literature: SN-ES: the Efron Stein bound for Self Normalized IPS [32], cIPS-EB: Empirical Bernstein for Clipping [55] and the recent IX: Implicit Exploration bound [21]. The first experiment uses the kropt dataset with $\\epsilon\\,=\\,0.2$ , collects bandit feedback with faulty behavior policy (with $\\tau\\,=\\,0.25)$ ) to evaluate an ideal policy $(\\tau=0.1)$ ), and explores how the relative radiuses of the considered bounds shrink while varying the number of datapoints. Table 1 compiles the results of the experiments and suggest that the LS bound is tighter than its competitors no matter the size of the feedback collected. The second experiments uses all 11 datasets, with different behavior policies $(\\tau_{0}\\,\\in\\,\\{0.2,0.25,0.3\\})$ and different noise levels $(\\epsilon\\in\\{0.,0.1,0.2\\})$ to evaluate ideal policies with different temperatures $(\\tau\\in\\{0.1,0.2,0.3,0.4,0.5\\})$ , defining $\\sim500$ different scenarios to validate our findings. We plot in Figure 2 the cumulative distribution of the relative radius of the considered bounds. We observe that while ${\\tt c I P S-L=1}$ and IX can be comparable, the LS bound is tighter than all its competitors. We also provide detailed results in Appendix H.1.2 that further confirm the superiority of the LS bound. ", "page_idx": 8}, {"type": "text", "text": "(OPS) Find the best, avoid the worst policy. Policy selection aims at identifying the best policy among a set of finite candidates. In practice, we are interested in finding policies that improve on $\\pi_{0}$ and avoid policies that perform worse than $\\pi_{0}$ . To replicate real world scenarios, we design an experiment where $\\pi_{0}$ is a faulty policy $\\left(\\tau\\right.=\\left.0.2\\right)$ ), that collects noisy $(\\epsilon\\,=\\,0.2)$ ) interaction data, some of which is used to learn $\\pi_{\\theta^{\\tt I P S}},\\pi_{\\theta^{\\mathrm{SI}}}$ , and that we add to our discrete set of policies $\\Pi_{k=4}\\,=\\,\\{\\pi_{0},\\pi^{\\mathrm{ideal}},\\pi_{\\theta^{\\mathrm{IPS}}},\\pi_{\\theta^{\\mathrm{sN}}}\\}$ . The goal is to measure the ability of our selection strategies to choose from $\\Pi_{k=4}$ , better performing policies than $\\pi_{0}$ . We thus define three possible outcomes: a strategy can select worse performing policies, better performing or the best policy. Our goal in these experiments is to empirically validate the pitfalls of point estimators while confirming the benefits of using the pessimism principle. To this end, we compare pessimistic selection strategies to policy selection using the classical point estimators IPS [24] and SN [56]. The comparison is conducted on the 11 UCI datasets with 10 different seeds resulting in 110 scenarios. We plot in Figure 2 the percentage of time each method selected the best policy, a better or a worse policy than $\\pi_{0}$ . While risk estimators can identify the best policy, they are unreliable as they can choose worse performing policies than $\\pi_{0}$ , a catastrophic outcome in critical applications. Pessimistic selection is more conservative, as it avoids poor performing policies completely and empirically confirms that tighter upper bounds result in better selection strategies: LS upper bound is less conservative and finds best policies the most (comparable to SN) while never selecting poor performing policies. Fine grained results (for each dataset) can be found in Appendix H.1.3. ", "page_idx": 8}, {"type": "text", "text": "5.2 Off-policy learning experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We follow the successful off policy learning paradigm based on directly minimizing PAC-Bayesian risk generalization bounds [49, 5] as it comes with guarantees of improvement and avoids hyperparameter tuning. For comparable results, we use the same 4 datasets (described in Appendix H.2, Table 7) as in [49, 5] and adopt the LGP: Linear Gaussian Policies [49] as our class of parametrized policies. For each dataset, we use behavior policies trained on a small fraction of the data in a supervised fashion, combined with different inverse temperature parameters $\\alpha\\in\\{0.1,0.3,0.5,0.7,\\mathrm{{\\bar{1}}.\\}}$ to cover cases of diffused and peaked behavior policies. These policies generate for 10 different seeds, 10 logged bandit feedback datasets resulting in 200 different scenarios to test our learning approaches. In the PAC-Bayesian OPL paradigm, we minimize the empirical upper bounds $U(\\pi)$ directly and obtain the learned policy as the bound\u2019s minimizer $\\hat{\\pi}_{n}^{\\mathrm{L}}$ (as in (27)). With $\\hat{\\pi}_{n}^{\\mathrm{L}}$ obtained, we are interested in two quantities: The guaranteed risk by the bound, which is the value of the bound $U(\\hat{\\pi}_{n}^{\\mathrm{L}})$ at its minimizer. This quantity reflects the worst case performance of the learned policy, a lower value implies stronger performance guarantees. We are also interested in the true risk of the minimizer of the bound $R(\\hat{\\pi}_{n}^{\\mathrm{L}})$ as it translates the performance of the obtained policy acting on unseen data. As this learning paradigm is based on optimizing tractable, generalization bounds, we only compare our approach to methods that provide them. Precisely, we compare our LS-LIN learning strategy in (27) to strategies based on minimizing off-policy PAC Bayesian bounds from the literature: clipped IPS (cIPS) and Control Variate clipped IPS (cvcIPS) [49], Exponential Smoothing (ES) [5] and Implicit Exploration (IX) [21]. The results are summarized in Table 2 where we compute: ", "page_idx": 8}, {"type": "image", "img_path": "zLClygeRK8/tmp/4df30f3d88f3c0baef7c2d965fa2256af1b4c04ae88ba5edc2778e5e9cc44f4e.jpg", "img_caption": ["Table 2: OPL: Relative Improvement of guaranteed risk and true risk averaged over 200 scenarios. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "equation", "text": "$$\nr I(x)=(R(\\pi_{0})-x)/(R(\\pi_{0})-R(\\pi^{*}))=(R(\\pi_{0})-x)/(R(\\pi_{0})+1)\\,,\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "the improvement over $R(\\pi_{0})$ achieved by minimizing the different bounds in terms of $x\\in\\{U,R\\}$ (guaranteed risk and true risk respectively), relative to an ideal improvement. This metric helps us normalize the results, and we report its average over 200 different scenarios, with results in bold being significantly better. Fine grained results can be found in Appendix H.2.4. We observe that the LS-LIN PAC-Bayesian bound improves substantially on its competitors in terms of the guaranteed risk, and also obtains the best performing policies (on par with the IX PAC-Bayesian bound). ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Motivated by the pessimism principle, we have derived novel, empirical risk upper bounds tailored for the regularized IPS family of estimators. Minimizing these bounds within this family unveiled Logarithmic Smoothing, a simple estimator with good concentration properties. With its tight upper bound, LS confidently evaluates a policy, and shows provably better guarantees for both selecting and learning policies than all competitors. Our upper bounds remain broadly applicable, only requiring negative costs. While this condition does not impact importance weighting estimators, it does not hold for doubly robust estimators. Extending our approach to derive empirical bounds for this type of estimators presents a nontrivial, yet interesting task to explore in future work. Another potential extension would be to relax the i.i.d. assumption of the contextual bandit problem to address, the general offline Reinforcement Learning setting. This direction will introduce a more challenging estimation task and requires developing new concentration bounds. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Pierre Alquier. Transductive and Inductive Adaptative Inference for Regression and Density Estimation. Theses, ENSAE ParisTech, December 2006. URL https://pastel.hal.science/ tel-00119593.   \n[2] Pierre Alquier. User-friendly introduction to PAC-Bayes bounds. Foundations and Trends\u00ae in Machine Learning, 17(2), 2024.   \n[3] Imad Aouali, Amine Benhalloum, Martin Bompaire, Achraf Ait Sidi Hammou, Sergey Ivanov, Benjamin Heymann, David Rohde, Otmane Sakhi, Flavian Vasile, and Maxime Vono. Reward Optimizing Recommendation using Deep Learning and Fast Maximum Inner Product Search. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD $^{\\circ2}$ , page 4772\u20134773, New York, NY, USA, 2022. Association for Computing Machinery. ISBN 9781450393850. doi: 10.1145/3534678.3542622. URL https://doi.org/10.1145/ 3534678.3542622.   \n[4] Imad Aouali, Achraf Ait Sidi Hammou, Sergey Ivanov, Otmane Sakhi, David Rohde, and Flavian Vasile. Probabilistic Rank and Reward: A Scalable Model for Slate Recommendation, 2022.   \n[5] Imad Aouali, Victor-Emmanuel Brunel, David Rohde, and Anna Korba. Exponential Smoothing for Off-Policy Learning. In Proceedings of the 40th International Conference on Machine Learning, pages 984\u20131017. PMLR, 2023.   \n[6] Imad Aouali, Victor-Emmanuel Brunel, David Rohde, and Anna Korba. Bayesian off-policy evaluation and learning for large action spaces. arXiv preprint arXiv:2402.14664, 2024.   \n[7] Imad Aouali, Victor-Emmanuel Brunel, David Rohde, and Anna Korba. Unified PAC-Bayesian Study of Pessimism for Offilne Policy Learning with Regularized Importance Sampling. In The 40th Conference on Uncertainty in Artificial Intelligence, 2024. URL https://openreview. net/forum?id=d7W4H0sTXU.   \n[8] A. Asuncion and D. J. Newman. UCI machine learning repository, 2007. URL http://www. ics.uci.edu/\\$\\sim\\$mlearn/{MLR}epository.html.   \n[9] Heejung Bang and James M Robins. Doubly robust estimation in missing data and causal inference models. Biometrics, 61(4):962\u2013973, 2005.   \n[10] L\u00e9on Bottou, Jonas Peters, Joaquin Qui\u00f1onero-Candela, Denis X Charles, D Max Chickering, Elon Portugaly, Dipankar Ray, Patrice Simard, and Ed Snelson. Counterfactual reasoning and learning systems: The example of computational advertising. Journal of Machine Learning Research, 14(11), 2013.   \n[11] Olivier Catoni. PAC-Bayesian supervised classification: The thermodynamics of statistical learning. IMS Lecture Notes Monograph Series, page 1\u2013163, 2007. ISSN 0749-2170. doi: 10. 1214/074921707000000391. URL http://dx.doi.org/10.1214/074921707000000391.   \n[12] Minmin Chen, Alex Beutel, Paul Covington, Sagar Jain, Francois Belletti, and Ed H. Chi. Top-K Off-Policy Correction for a REINFORCE Recommender System. In Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining, WSDM \u201919, page 456\u2013464, New York, NY, USA, 2019. Association for Computing Machinery. ISBN 9781450359405. doi: 10.1145/3289600.3290999. URL https://doi.org/10.1145/3289600.3290999.   \n[13] Victor Chernozhukov, Mert Demirer, Greg Lewis, and Vasilis Syrgkanis. Semi-parametric efficient policy learning with continuous actions. Advances in Neural Information Processing Systems, 32, 2019.   \n[14] Matej Cief, Jacek Golebiowski, Philipp Schmidt, Ziawasch Abedjan, and Artur Bekasov. Learning action embeddings for off-policy evaluation. In European Conference on Information Retrieval, pages 108\u2013122. Springer, 2024.   \n[15] Bo Dai, Ofir Nachum, Yinlam Chow, Lihong Li, Csaba Szepesvari, and Dale Schuurmans. Coindice: Off-policy confidence interval estimation. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 9398\u20139411. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/ 6aaba9a124857622930ca4e50f5afed2-Paper.pdf.   \n[16] Miroslav Dud\u00edk, John Langford, and Lihong Li. Doubly robust policy evaluation and learning. In Proceedings of the 28th International Conference on International Conference on Machine Learning, ICML\u201911, page 1097\u20131104, 2011.   \n[17] Miroslav Dud\u00edk, Dumitru Erhan, John Langford, and Lihong Li. Sample-efficient nonstationary policy evaluation for contextual bandits. In Proceedings of the Twenty-Eighth Conference on Uncertainty in Artificial Intelligence, UAI\u201912, page 247\u2013254, Arlington, Virginia, USA, 2012. AUAI Press.   \n[18] Miroslav Dudik, Dumitru Erhan, John Langford, and Lihong Li. Doubly robust policy evaluation and optimization. Statistical Science, 29(4):485\u2013511, 2014.   \n[19] Mehrdad Farajtabar, Yinlam Chow, and Mohammad Ghavamzadeh. More robust doubly robust off-policy evaluation. In International Conference on Machine Learning, pages 1447\u20131456. PMLR, 2018.   \n[20] Hamish Flynn, David Reeb, Melih Kandemir, and Jan Peters. PAC-Bayes Bounds for Bandit Problems: A Survey and Experimental Comparison. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(12):15308\u201315327, 2023. doi: 10.1109/TPAMI.2023.3305381.   \n[21] Germano Gabbianelli, Gergely Neu, and Matteo Papini. Importance-weighted offline learning done right. In Proceedings of The 35th International Conference on Algorithmic Learning Theory, volume 237 of Proceedings of Machine Learning Research, pages 614\u2013634. PMLR, 25\u201328 Feb 2024. URL https://proceedings.mlr.press/v237/gabbianelli24a.html.   \n[22] Alexandre Gilotte, Cl\u00e9ment Calauz\u00e8nes, Thomas Nedelec, Alexandre Abraham, and Simon Doll\u00e9. Offline A/B testing for recommender systems. In Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining, pages 198\u2013206, 2018.   \n[23] Torben Hagerup and Christine R\u00fcb. A Guided Tour of Chernoff Bounds. Inf. Process. Lett., 33 (6):305\u2013308, 1990. URL http://dblp.uni-trier.de/db/journals/ipl/ipl33.html# HagerupR90.   \n[24] Daniel G Horvitz and Donovan J Thompson. A generalization of sampling without replacement from a finite universe. Journal of the American statistical Association, 47(260):663\u2013685, 1952.   \n[25] Edward L Ionides. Truncated importance sampling. Journal of Computational and Graphical Statistics, 17(2):295\u2013311, 2008.   \n[26] Olivier Jeunen and Bart Goethals. Pessimistic reward models for off-policy learning in recommendation. In Fifteenth ACM Conference on Recommender Systems, pages 63\u201374, 2021.   \n[27] Ying Jin, Zhuoran Yang, and Zhaoran Wang. Is pessimism provably efficient for offline rl? In International Conference on Machine Learning, pages 5084\u20135096. PMLR, 2021.   \n[28] Ying Jin, Zhimei Ren, Zhuoran Yang, and Zhaoran Wang. Policy learning \"without\u201d overlap: Pessimism and generalized empirical Bernstein\u2019s inequality, 2023.   \n[29] Nathan Kallus and Angela Zhou. Policy evaluation and optimization with continuous treatments. In International conference on artificial intelligence and statistics, pages 1243\u20131251. PMLR, 2018.   \n[30] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.   \n[31] Akshay Krishnamurthy, John Langford, Aleksandrs Slivkins, and Chicheng Zhang. Contextual bandits with continuous actions: Smoothing, zooming, and adapting. Journal of Machine Learning Research, 21(137):1\u201345, 2020. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "[32] Ilja Kuzborskij, Claire Vernade, Andras Gyorgy, and Csaba Szepesv\u00e1ri. Confident off-policy evaluation and selection through self-normalized importance weighting. In International Conference on Artificial Intelligence and Statistics, pages 640\u2013648. PMLR, 2021. ", "page_idx": 12}, {"type": "text", "text": "[33] Tor Lattimore and Csaba Szepesvari. Bandit Algorithms. Cambridge University Press, 2019.   \n[34] Lihong Li, Remi Munos, and Csaba Szepesvari. Toward Minimax Off-policy Value Estimation. In Guy Lebanon and S. V. N. Vishwanathan, editors, Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics, volume 38 of Proceedings of Machine Learning Research, pages 608\u2013616, San Diego, California, USA, 09\u201312 May 2015. PMLR. URL https://proceedings.mlr.press/v38/li15b.html.   \n[35] Ben London and Ted Sandler. Bayesian counterfactual risk minimization. In International Conference on Machine Learning, pages 4125\u20134133. PMLR, 2019.   \n[36] Andreas Maurer and Massimiliano Pontil. Empirical Bernstein bounds and sample variance penalization. arXiv preprint arXiv:0907.3740, 2009.   \n[37] David A. McAllester. Some PAC-Bayesian theorems. In Proceedings of the Eleventh Annual Conference on Computational Learning Theory, COLT\u2019 98, page 230\u2013234, New York, NY, USA, 1998. Association for Computing Machinery. ISBN 1581130570. doi: 10.1145/279943.279989. URL https://doi.org/10.1145/279943.279989.   \n[38] Alberto Maria Metelli, Alessio Russo, and Marcello Restelli. Subgaussian and differentiable importance sampling for off-policy evaluation and learning. Advances in Neural Information Processing Systems, 34:8119\u20138132, 2021.   \n[39] Art B. Owen. Monte Carlo theory, methods and examples. https://artowen.su.domains/ mc/, 2013.   \n[40] Emilio Parrado-Hern\u00e1ndez, Amiran Ambroladze, John Shawe-Taylor, and Shiliang Sun. PACBayes Bounds with Data Dependent Priors. Journal of Machine Learning Research, 13(112): 3507\u20133531, 2012. URL http://jmlr.org/papers/v13/parrado12a.html.   \n[41] Jie Peng, Hao Zou, Jiashuo Liu, Shaoming Li, Yibao Jiang, Jian Pei, and Peng Cui. Offline policy evaluation in large action spaces via outcome-oriented action grouping. In Proceedings of the ACM Web Conference 2023, pages 1220\u20131230, 2023.   \n[42] James M Robins and Andrea Rotnitzky. Semiparametric efficiency in multivariate regression models with missing data. Journal of the American Statistical Association, 90(429):122\u2013129, 1995.   \n[43] Noveen Sachdeva, Yi Su, and Thorsten Joachims. Off-policy bandits with deficient support. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 965\u2013975, 2020.   \n[44] Noveen Sachdeva, Lequn Wang, Dawen Liang, Nathan Kallus, and Julian McAuley. Offpolicy evaluation for large action spaces via policy convolution. In Proceedings of the ACM Web Conference 2024, WWW \u201924, page 3576\u20133585, New York, NY, USA, 2024. Association for Computing Machinery. ISBN 9798400701719. doi: 10.1145/3589334.3645501. URL https://doi.org/10.1145/3589334.3645501.   \n[45] Yuta Saito and Thorsten Joachims. Off-policy evaluation for large action spaces via embeddings. In Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 19089\u201319122. PMLR, 17\u201323 Jul 2022. URL https://proceedings.mlr.press/v162/saito22a.html.   \n[46] Yuta Saito, Qingyang Ren, and Thorsten Joachims. Off-policy evaluation for large action spaces via conjunct effect modeling. In international conference on Machine learning, pages 29734\u201329759. PMLR, 2023.   \n[47] Otmane Sakhi, Stephen Bonner, David Rohde, and Flavian Vasile. BLOB: A Probabilistic model for recommendation that combines organic and bandit signals. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 783\u2013793, 2020.   \n[48] Otmane Sakhi, Louis Faury, and Flavian Vasile. Improving Offline Contextual Bandits with Distributional Robustness, 2020.   \n[49] Otmane Sakhi, Pierre Alquier, and Nicolas Chopin. PAC-Bayesian Offilne Contextual Bandits with Guarantees. In International Conference on Machine Learning, pages 29777\u201329799. PMLR, 2023.   \n[50] Otmane Sakhi, David Rohde, and Nicolas Chopin. Fast Slate Policy Optimization: Going Beyond Plackett-Luce. Transactions on Machine Learning Research, 2023. ISSN 2835-8856. URL https://openreview.net/forum?id=f7a8XCRtUu.   \n[51] Otmane Sakhi, David Rohde, and Alexandre Gilotte. Fast Offilne Policy Optimization for Large Scale Recommendation. Proceedings of the AAAI Conference on Artificial Intelligence, 37 (8):9686\u20139694, Jun. 2023. doi: 10.1609/aaai.v37i8.26158. URL https://ojs.aaai.org/ index.php/AAAI/article/view/26158.   \n[52] Yevgeny Seldin, Nicol\u00f2 Cesa-Bianchi, Peter Auer, Fran\u00e7ois Laviolette, and John Shawe-Taylor. PAC-Bayes-Bernstein Inequality for Martingales and its Application to Multiarmed Bandits. In Dorota Glowacka, Louis Dorard, and John Shawe-Taylor, editors, Proceedings of the Workshop on On-line Trading of Exploration and Exploitation 2, volume 26 of Proceedings of Machine Learning Research, pages 98\u2013111, Bellevue, Washington, USA, 02 Jul 2012. PMLR. URL https://proceedings.mlr.press/v26/seldin12a.html.   \n[53] Anshumali Shrivastava and Ping Li. Asymmetric LSH (ALSH) for Sublinear Time Maximum Inner Product Search (MIPS). In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems, volume 27. Curran Associates, Inc., 2014. URL https://proceedings.neurips.cc/paper_files/paper/ 2014/file/310ce61c90f3a46e340ee8257bc70e93-Paper.pdf.   \n[54] Yi Su, Maria Dimakopoulou, Akshay Krishnamurthy, and Miroslav Dud\u00edk. Doubly robust off-policy evaluation with shrinkage. In International Conference on Machine Learning, pages 9167\u20139176. PMLR, 2020.   \n[55] Adith Swaminathan and Thorsten Joachims. Batch learning from logged bandit feedback through counterfactual risk minimization. The Journal of Machine Learning Research, 16(1): 1731\u20131755, 2015.   \n[56] Adith Swaminathan and Thorsten Joachims. The self-normalized estimator for counterfactual learning. advances in neural information processing systems, 28, 2015.   \n[57] Adith Swaminathan, Akshay Krishnamurthy, Alekh Agarwal, Miro Dudik, John Langford, Damien Jose, and Imed Zitouni. Off-policy evaluation for slate recommendation. Advances in Neural Information Processing Systems, 30, 2017.   \n[58] Muhammad Faaiz Taufiq, Arnaud Doucet, Rob Cornish, and Jean-Francois Ton. Marginal density ratio for off-policy evaluation in contextual bandits. Advances in Neural Information Processing Systems, 36, 2024.   \n[59] Lequn Wang, Akshay Krishnamurthy, and Aleksandrs Slivkins. Oracle-efficient pessimism: Offline policy optimization in contextual bandits. arXiv preprint arXiv:2306.07923, 2023.   \n[60] Yu-Xiang Wang, Alekh Agarwal, and Miroslav Dud\u0131k. Optimal and adaptive off-policy evaluation in contextual bandits. In International Conference on Machine Learning, pages 3589\u20133597. PMLR, 2017.   \n[61] Ding-Xuan Zhou. The covering number in learning theory. J. Complex., 18(3):739\u2013767, sep 2002. ISSN 0885-064X. doi: 10.1006/jcom.2002.0635. URL https://doi.org/10.1006/ jcom.2002.0635. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Table of Contents for Supplementary Material ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A Limitations 16 ", "page_idx": 14}, {"type": "text", "text": "B Broader impact 16 ", "page_idx": 14}, {"type": "text", "text": "C Extended related work 16 ", "page_idx": 14}, {"type": "text", "text": "D Useful lemmas 18 ", "page_idx": 14}, {"type": "text", "text": "E Additional results and discussions 19 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "E.1 Plots of the empirical moments bounds (Proposition 1) 19   \nE.2 The study of Logarithmic Smoothing estimator and proofs 19   \nE.3 OPS: Formal comparison with IX suboptimality . . . . . 23   \nE.4 OPL: Formal comparison of PAC-Bayesian bounds . . . . 24   \nE.5 OPL: Formal comparison with IX PAC-Bayesian learning suboptimality 27   \nF.1 Proof of high order empirical moments bound (Proposition 1) . . . 29   \nF.2 Proof of the impact of $L$ on the bound\u2019s tightness (Proposition 2) . . . 31   \nF.3 Comparisons of the bounds $U_{L}^{\\lambda}$ (Proposition 3) 31   \nF.4 Proof of the optimality of global clipping for Corollary 4 . 32   \nF.5 Comparison with empirical Bernstein . . 34   \nF.6 Proof of the $L\\rightarrow\\infty$ bound (Corollary 5) 35   \nF.7 Proof of the optimality of IPS for Corollary 5 36   \nF.8 Comparison with the IX bound (Proposition 8) 36 ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "G Proofs of OPS and OPL 37 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "G.1 OPS: Proof of suboptimality bound (Proposition 9) 37   \nG.2 OPL: Proof of PAC-Bayesian LS-LIN bound (Proposition 10) 38   \nG.3 OPL: Proof of PAC-Bayesian suboptimality bound (Proposition 11) 39 ", "page_idx": 14}, {"type": "text", "text": "H Experimental design and detailed experiments 40 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "H.1 Off-policy evaluation and selection . 40   \nH.1.1 Datasets . . 40   \nH.1.2 (OPE) Tightness of the bounds . . 40   \nH.1.3 (OPS) Find the best, avoid the worst policy 41   \nH.2 Off-policy learning 42   \nH.2.1 Datasets . . 42   \nH.2.2 Policy class . . 42   \nH.2.3 Detailed hyperparameters 42   \nH.2.4 Detailed results 43 ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Limitations ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "This work develops theoretically grounded and practical pessimistic approaches for the offline contextual bandit setting. Even if the proposed algorithms are general, and provably better than competitors, they still suffer from the intrinsic limitations of importance weighting estimators. Specifically, our method, as presented, will perform poorly in extremely large action spaces. However, these limitations can be mitigated by incorporating additional structure as in Saito and Joachims [45], Saito et al. [46]. Another limitation arises from the offilne contextual bandit setting itself, which assumes i.i.d. observations. While this assumption is valid in simple scenarios, it becomes unsuitable once we want to capture the long term effect of interventions. Extending our results to the more general, reinforcement learning setting would be an interesting research direction as it comes with a challenging estimation task and will require developing new concentration bounds. ", "page_idx": 15}, {"type": "text", "text": "B Broader impact ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Our work contributes to the development of theoretically grounded and practical pessimistic approaches for the offilne contextual bandit setting. The derived algorithms can improve the robustness of decision-making processes by prioritizing safety and minimizing uncertainty associated risks. By leveraging pessimistic strategies, we ensure that decisions are made with a conservative bias, thereby potentially improving outcomes in high-stakes environments where the cost of errors is substantial. Although our framework and algorithms have broad, potentially good applications, their specific social impacts will solely depend on the chosen application domain. ", "page_idx": 15}, {"type": "text", "text": "C Extended related work ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Offline contextual bandits. Contextual bandit is a widely adopted framework for online learning in uncertain environments [33]. However, some real-world applications present challenges for existing online algorithms, and thus offilne methods that leverage historical data to optimize decisionmaking have gained traction [10]. Fortunately, large datasets summarizing past interactions are often available, allowing agents to improve their policies offline [55]. Our work explores this offline approach, known as offline (or off-policy) contextual bandits [16]. In this setting, off-policy evaluation (OPE) estimates policy performance using historical data, mimicking real-time evaluations. Depending on the application, the goal might be to find the best policy within a predefined finite set (off-policy selection (OPS)) or the optimal policy overall (off-policy learning (OPL)). ", "page_idx": 15}, {"type": "text", "text": "Off-policy evaluation. In recent years, OPE has experienced a noticeable surge of interest, with numerous significant contributions [16\u201318, 60, 19, 54, 38, 32, 45, 47, 26]. The literature on OPE can be broadly classified into three primary approaches. The first, referred to as the direct method (DM) [26, 6], involves the development of a model designed to approximate expected costs for any context-action pair. This model is subsequently employed to estimate the performance of the policies. This approach is often designed for specific applications such as large-scale recommender systems [47, 26, 4]. The second approach, known as inverse propensity scoring (IPS) [24, 17], aims to estimate the costs associated with the evaluated policies by correcting for the inherent preference bias of the behavior policy within the dataset. While IPS maintains its unbiased nature when operating under the assumption that the evaluation policy is absolutely continuous with respect to the behavior policy, it can be susceptible to high variance and substantial bias when this assumption is violated [43]. In response to the variance issue, various techniques have been introduced, including clipping [25, 10], shrinkage [54], power-mean correction [38], implicit exploration [21], self-normalization [56], among others [22]. The third approach, known as doubly robust (DR) [42, 9, 16, 18, 19], combines elements from both the direct method (DM) and inverse propensity scoring (IPS). This work focuses on regularized IPS. ", "page_idx": 15}, {"type": "text", "text": "Off-policy selection and learning. as in OPE, three key approaches dominate: DM, IPS and DR in OPS and OPL. In OPS, all these methods share the same core objective: identifying the policy with the highest estimated reward from a finite set of candidates. However, they differ in their reward estimation techniques, as discussed in the OPE section above. In contrast, in OPL, DM either deterministically selects the action with the highest estimated reward or constructs a distribution based on these estimates. IPS and DR, on the other hand, employ gradient descent for policy learning [55], updating a parameterized policy denoted by $\\pi_{\\theta}$ as $\\bar{\\theta_{t+1}}\\gets\\bar{\\theta_{t}}-\\nabla_{\\theta}R(\\pi_{\\theta})$ for each iteration $t$ . ", "page_idx": 15}, {"type": "text", "text": "Since the true risk $R$ is unknown, $\\nabla_{\\boldsymbol{\\theta}}R(\\pi_{\\boldsymbol{\\theta}})$ is unknown and needs to be estimated using techniques like IPS or DR. ", "page_idx": 16}, {"type": "text", "text": "Pessimism in offline contextual bandits. Most OPE studies directly use their point estimators of the risk in OPE, OPS and OPL. However, point estimators can deviate from the true value of the risk, rendering them unreliable for decision-making. Therefore, and to increase safety, alternative approaches focus on constructing bounds on the risk. These bounds, either asymptotic [10, 48, 15] or finite sample [32, 21], aim to evaluate a policy\u2019s worst-case performance, adhering to the principle of pessimism in face of uncertainty [27]. The principle of pessimism transcends OPE, influencing both OPS and OPL. In these domains, strategies are predominantly inspired by, or directly derived from, upper bounds on the true risk [55, 35, 32, 49, 5, 59]. Consider OPS: [32] leveraged an EfronStein bound for the self-normalized IPS estimator, while [21] anchored their analysis on a bound constructed with the Implicit Exploration estimator. Shifting focus to OPL, [55] combined the empirical Bernstein bound [36] with the clipping estimator, motivating sample variance penalization for policy learning. Recent advancements include modifications to the penalization term [59] to be scalable and efficient. ", "page_idx": 16}, {"type": "text", "text": "PAC-Bayes extension. The PAC-Bayesian paradigm [37, 11] (see Alquier [2] for a recent introduction) provides a rich set of tools to prove generalization bounds for different statistical learning problems. The classical (online) contextual bandit problem received a lot of attention from the PAC-Bayesian community with the seminal work of Seldin et al. [52]. It is just recently that these tools were adapted to the offline contextual bandit setting, with [35] that introduced a clean and scalable PAC-Bayesian perspective to OPL. This perspective was further explored by [20, 49, 5, 7, 21], leading to the development of tight, tractable PAC-Bayesian bounds suitable for direct optimization. ", "page_idx": 16}, {"type": "text", "text": "Large action space extension. While regularization techniques can improve IPS properties, they often fall short when dealing with extremely large action spaces. Additional assumptions regarding the structure of the contextual bandit problem become necessary. For example, Saito and Joachims [45] introduced the Marginalized IPS (MIPS) framework and estimator. MIPS leverages auxiliary information about the actions in the form of action embeddings. Roughly speaking, MIPS assumes access to embeddings $e_{i}$ within logged data and defines the risk estimator as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\hat{R}_{n}^{\\mathrm{MPS}}(\\pi)=\\frac{1}{n}\\sum_{i=1}^{n}\\frac{\\pi\\left(e_{i}\\mid x_{i}\\right)}{\\pi_{0}\\left(e_{i}\\mid x_{i}\\right)}c_{i}=\\frac{1}{n}\\sum_{i=1}^{n}w\\left(x_{i},e_{i}\\right)c_{i}\\,,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the logged data $\\mathcal{D}_{n}=\\left\\{(x_{i},a_{i},e_{i},r_{i})\\right\\}_{i=1}^{n}$ now includes action embeddings for each data point. The marginal importance weight ", "page_idx": 16}, {"type": "equation", "text": "$$\nw(x,e)={\\frac{\\pi(e\\mid x)}{\\pi_{0}(e\\mid x)}}={\\frac{\\sum_{a}p(e\\mid x,a)\\pi(a\\mid x)}{\\sum_{a}p(e\\mid x,a)\\pi_{0}(a\\mid x)}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "is a key component of this approach. Compared to IPS and DR, MIPS achieves significantly lower variance in large action spaces [45] while maintaining unbiasedness if the action embeddings directly influence costs $c$ . This necessitates informative embeddings that capture the causal effects of actions on costs. However, high-dimensional embeddings can still lead to high variance for MIPS, similar to IPS. Additionally, high bias can arise if the direct effect assumption is violated and embeddings fail to capture these causal effects. This bias is particularly present when performing action feature selection for dimensionality reduction. Recent work proposes learning such embeddings directly from logged data [41, 44, 14], or loosen this assumption [58, 46]. Our proposed importance weight regularization can be potentially combined with these estimators under their respective assumptions on the underlying structure of the contextual bandit problem, extending our approach to large action spaces, and we posit that this will be beneficial when, for example, the action embedding dimension is high. Another line of research in large action spaces is more interested with the learning problem, precisely solving the optimization issues arising from policies defined on large action spaces. Indeed, naive optimization tends to be slow and scales linearly with the number of actions $K$ [12]. Recent work [51, 50] solve this by leveraging fast maximum inner product search [53, 3] in the training loop, reducing the optimization complexity to logarithmic in the action space size. These methods however require a linear objective on the target policy. Luckily, our PAC-Bayesian learning objective is linear in the policy and its optimization is amenable to such acceleration. ", "page_idx": 16}, {"type": "text", "text": "Continuous action space extension. While research has predominantly focused on discrete action spaces, a limited number of studies have tackled the continuous case [29, 13, 59]. For example, [29] explored non-parametric evaluation and learning of continuous action policies using kernel smoothing, while [13] investigated the semi-parametric setting. Recently, [59] leveraged the smoothing approach from [31] to extend their discrete OPL method to continuous actions. Our work can either use the densities directly, or be similarly extended to continuous actions through a well-defined discretization of the space. Imagine a scenario with infinitely many actions, where policies are defined by density functions. For any context $x$ , $\\pi(a\\mid x)$ represents the density function that maps actions $a$ to probabilities. The discretization process transforms the original contextual bandit problem characterized by the density-based policy class $\\Pi$ into an OPL problem defined by a discrete, mass-based policy class $\\Pi_{K}$ (for a finite number of actions $K$ ). Each policy within $\\Pi_{K}$ approximates a policy in $\\Pi$ through a smoothing process. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "D Useful lemmas ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In the following, and for any quantity $Z$ , all expectations are computed w.r.t to the distribution of the data when playing actions under the behaviour policy $\\pi_{0}$ , as in: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[Z\\right]=\\mathbb{E}_{x\\sim\\nu,a\\sim\\pi_{0}(\\cdot|x),c\\sim p(\\cdot|x,a)}\\left[Z\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "A lot of the results derived in the paper are based on the use of the well known Chernoff Inequality, that we state below for a sum of i.i.d. random variables: ", "page_idx": 17}, {"type": "text", "text": "Lemma 12 (Chernoff Inequality for a sum of i.i.d. random variables.). Let $a\\in\\mathbb{R},$ , $n\\in\\mathbb{N}^{*}$ and $\\{X_{i},i\\in[n]\\}$ a collection of n i.i.d. random variables. The following concentration bounds on the right tail of $\\textstyle\\sum_{i\\in[n]}X_{i}$ hold for any $\\lambda\\geq0$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\nP\\left(\\sum_{i\\in[n]}X_{i}>a\\right)\\leq\\left(\\mathbb{E}\\left[\\exp\\left(\\lambda X_{1}\\right)\\right]\\right)^{n}\\exp(-\\lambda a)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This result is classical in the literature [23] and we omit its proof. We will also need the following lemma, that states the monotonous nature of a key function in our analysis, and that we take the time to prove. ", "page_idx": 17}, {"type": "text", "text": "Lemma 13. Let $L\\geq1$ and $f_{L}$ be the following function: ", "page_idx": 17}, {"type": "equation", "text": "$$\nf_{L}(x)={\\frac{\\log(1+x)-\\sum_{\\ell=1}^{L}{\\frac{(-1)^{\\ell-1}}{\\ell}}x^{\\ell}}{(-1)^{L}x^{L+1}}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We have that $f_{L}$ is a decreasing function in $\\mathbb{R}^{+}$ for all $L\\in\\mathbb{N}^{*}$ ", "page_idx": 17}, {"type": "text", "text": "Proof. Let $L\\geq1$ and $f_{L}$ be the following function: ", "page_idx": 17}, {"type": "equation", "text": "$$\nf_{L}(x)={\\frac{\\log(1+x)-\\sum_{\\ell=1}^{L}{\\frac{(-1)^{\\ell-1}}{\\ell}}x^{\\ell}}{(-1)^{L}x^{L+1}}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Let $x\\in\\mathbb{R}^{+}$ , we have the following identity holding $\\forall t>0$ and $\\forall n\\geq0$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\n{\\frac{1+(-1)^{n}t^{n+1}}{1+t}}=\\sum_{k=0}^{n}(-1)^{k}t^{k}\\iff{\\frac{1}{1+t}}=\\sum_{k=0}^{n}(-1)^{k}t^{k}+{\\frac{(-1)^{n+1}t^{n+1}}{1+t}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Recall the integral form of the log function: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\log(1+x)=\\int_{0}^{x}{\\frac{1}{1+t}}d t.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We integrate both sides of the Equality (29) and show that the numerator of $f_{L}(x)$ is equal to: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\log(1+x)-\\sum_{k=1}^{K}{\\frac{(-1)^{k-1}}{k}}x^{k}=(-1)^{K}\\int_{0}^{x}{\\frac{t^{K}}{1+t}}d t.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This result enables us to rewrite the function $f_{L}$ as: ", "page_idx": 18}, {"type": "equation", "text": "$$\nf_{L}(x)=\\frac{1}{x^{L+1}}\\int_{0}^{x}\\frac{t^{L}}{1+t}d t.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Using the change of variable $t=u x$ , we obtain: ", "page_idx": 18}, {"type": "equation", "text": "$$\nf_{L}(x)=\\int_{0}^{1}\\frac{u^{L}}{1+x u}d t\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which is clearly decreasing for in $\\mathbb{R}^{+}$ . This ends the proof. ", "page_idx": 18}, {"type": "text", "text": "Finally, we also state the important change of measure lemma: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\theta\\sim Q}[g(\\theta,\\mathcal{D}_{n})]\\leq\\mathcal{K L}(Q||P)+\\ln\\frac{\\Psi_{g}}{\\delta}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Lemma 14 is the backbone of a multitude of PAC-Bayesian bounds. It is proven in many references, see for example [2] or Lemma 1.1.3 in [11]. With this result, the recipe of constructing a generalization bound reduces to choosing an adequate function $g$ for which we can control $\\Psi_{g}$ . ", "page_idx": 18}, {"type": "text", "text": "E Additional results and discussions ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "E.1 Plots of the empirical moments bounds (Proposition 1) ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "For any $\\pi\\in\\Pi$ , let $U_{L}^{\\lambda,h}(\\pi)$ be the upper bound of Proposition 1: ", "page_idx": 18}, {"type": "equation", "text": "$$\nU_{L}^{\\lambda,h}(\\pi)=\\psi_{\\lambda}\\left(\\hat{R}_{n}^{h}(\\pi)+\\frac{\\ln(1/\\delta)}{\\lambda n}+\\sum_{\\ell=2}^{2L}\\frac{\\lambda^{\\ell-1}}{\\ell}\\hat{\\mathcal{M}}_{n}^{h,\\ell}(\\pi)\\right).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "One can observe that the bound $U_{L}^{\\lambda,h}$ depends on three parameters, the regularized IPS function $h$ the free parameter $\\lambda$ and the moment order $L$ . We choose a dataset (balance-scale) with $n=612$ , and evaluate a policy $\\pi$ with $R(\\pi)=-0.93$ to evaluate our bound for different parameters. We fix $\\lambda=$ $\\sqrt{1/n}$ and plot the value of $U_{L}^{\\lambda,h}$ for different values of the moment order\u221a $L\\in\\{1,2,3,4,6,8,\\infty\\}$ and for 4 different regularization functions, namely IPS, clipped IPS $\\left\\langle M={\\sqrt{n}}\\right\\rangle$ , Implicit Exploration (IX) $(\\lambda=\\sqrt{1/n})$ and Exponential Smoothing (ES) $(\\alpha=1-\\sqrt{1/n})$ . The results are shown in Figure 3. One can observe from the plot that The decreasing nature of $U_{L}^{\\lambda,h}$ depends on $\\lambda$ and the regularization function $h$ . Indeed, Proposition 2 states that $\\begin{array}{r}{\\lambda\\,<\\,\\operatorname*{min}_{i\\in[n]}1/|h_{i}|}\\end{array}$ implies that the bound is decreasing w.r.t $L$ . Which means that once this condition is not verified, we do not know if the bound will keep decreasing with $L$ . If the bound seems decreasing for CIPS and IX, One can observe that for both IPS and ES, the bound increased from $L=4$ to $L=8$ , but achieved its minimum at $L=\\infty$ , with IPS being optimal for this value. This highlights the connection between $L$ , the value of $\\lambda$ and the regularizer $h$ . ", "page_idx": 18}, {"type": "text", "text": "E.2 The study of Logarithmic Smoothing estimator and proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Recall the form of the Logarithmic Smoothing estimator, defined for any $\\lambda\\geq0$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\hat{R}_{n}^{\\lambda}(\\pi)=-\\frac{1}{n}\\sum_{i=1}^{n}\\frac{1}{\\lambda}\\log\\left(1-\\lambda w_{\\pi}(x_{i},a_{i})c_{i}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Our estimator $\\hat{R}_{n}^{\\lambda}(\\pi)$ , is defined for a non-negative $\\lambda\\geq0$ . In particular, $\\lambda=0$ recovers the unbiased IPS estimator in (2) and $\\lambda>0$ introduces a bias variance trade-off. This estimator can be interpreted ", "page_idx": 18}, {"type": "image", "img_path": "zLClygeRK8/tmp/ef2a8802565303ff1767ac39d33548b90f23bc49bde7ba5e012245c5faba5202.jpg", "img_caption": ["Figure 3: Proposition 1 for different values of $L$ and with different regularized IPS $h$ . "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "as Logarithmic Soft Clipping, and have a similar behavior than Clipping of Bottou et al. [10]. Indeed, $1/\\lambda$ plays a similar role to the clipping parameter $M$ , as for any $i\\in[n]$ , we have: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{w_{\\pi}(x_{i},a_{i})c_{i}\\ll\\frac{1}{\\lambda}\\implies-\\frac{1}{\\lambda}\\log\\left(1-\\lambda w_{\\pi}(x_{i},a_{i})c_{i}\\right)\\approx w_{\\pi}(x_{i},a_{i})c_{i}.}\\\\ &{w_{\\pi}(x_{i},a_{i})c_{i}<M\\implies\\operatorname*{min}\\left(w_{\\pi}(x_{i},a_{i}),M\\right)c_{i}=w_{\\pi}(x_{i},a_{i})c_{i}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "LS can be seen as a smooth, differentiable version of clipping. We plot the graph of the two functions in Figure 4. One can observe that once $\\lambda>0$ , LS exhibits a bias-variance trade-off, with a declining bias with $\\lambda\\,\\rightarrow\\,0$ . This is different than Clipping as no bias is suffered once $M$ is bigger than the support of $w_{\\pi}$ , this comes however with the price of suffering the full variance of IPS. In the following, we study the bias-variance trade-off that emerges with the new Logarithmic Smoothing estimator. ", "page_idx": 19}, {"type": "image", "img_path": "zLClygeRK8/tmp/3d70ddfed3536cae60184b4e0574588bad91203d7d8afb44869058f71298d9c8.jpg", "img_caption": ["Figure 4: Comparison of Logarithmic Smoothing and Clipping. ", "Importance Weight $w_{\\pi}$ "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "We begin by defining the bias and variance of $\\hat{R}_{n}^{\\lambda}(\\pi)$ : ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{B}^{\\lambda}(\\pi)=\\mathbb{E}\\left[\\hat{R}_{n}^{\\lambda}(\\pi)\\right]-R(\\pi)\\,,\\qquad\\quad\\nu^{\\lambda}(\\pi)=\\mathbb{E}\\left[\\left(\\hat{R}_{n}^{\\lambda}(\\pi)-\\mathbb{E}\\left[\\hat{R}_{n}^{\\lambda}(\\pi)\\right]\\right)^{2}\\right]\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Moreover, for any $\\lambda\\geq0$ , we define the following quantity ", "page_idx": 19}, {"type": "equation", "text": "$$\nS_{\\lambda}(\\pi)=\\mathbb{E}\\left[\\frac{w_{\\pi}(x,a)^{2}c^{2}}{1-\\lambda w_{\\pi}(x,a)c}\\right]\\,,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "that will be essential in studying the properties of this estimator akin to the coverage ratio used for the IX-estimator [21]. In the following, we study the properties of our estimator $\\hat{R}_{n}^{\\lambda}(\\pi)$ in (14). We start with bounding its mean squared error (MSE), which involves bounding its bias and variance. ", "page_idx": 19}, {"type": "text", "text": "Proposition (Bias-variance trade-off). Let $\\pi\\in\\Pi$ and $\\lambda\\geq0$ . Then we have that $0\\leq\\beta^{\\lambda}(\\pi)\\leq\\lambda S_{\\lambda}(\\pi)\\,,\\quad a n d\\quad\\nu^{\\lambda}(\\pi)\\leq S_{\\lambda}(\\pi)/n\\,.$   \nMoreover, it holds that for any $\\lambda>0$ : $\\nu^{\\lambda}(\\pi)\\leq\\frac{|R(\\pi)|}{n\\lambda}\\leq\\frac{1}{n\\lambda}.$ ", "page_idx": 20}, {"type": "text", "text": "Proof. Let us start with bounding the bias. We have for any $\\lambda\\geq0$ : ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{B^{\\lambda}(\\pi)=\\mathbb{E}\\left[\\hat{R}_{n}^{\\lambda}(\\pi)\\right]-R(\\pi)}\\\\ &{\\qquad\\quad=\\mathbb{E}\\left[-\\frac{1}{\\lambda}\\log(1-\\lambda w_{\\pi}(x,a)c)-w_{\\pi}(x,a)c\\right]\\quad\\mathrm{(IPS~is~unbiased).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Using $\\log(1+x)\\leq x$ for any $x\\geq0$ proves that the bias is positive. For its upper bound, we use the following inequality log(1 + x) \u22651+xx holding for $x\\geq0$ : ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{B}^{\\lambda}(\\pi)=\\mathbb{E}\\left[-\\displaystyle\\frac{1}{\\lambda}\\log(1-\\lambda w_{\\pi}(x,a)c)-w_{\\pi}(x,a)c\\right]}\\\\ &{\\qquad\\quad\\le\\mathbb{E}\\left[\\displaystyle\\frac{w_{\\pi}(x,a)c}{1-\\lambda w_{\\pi}(x,a)c}-w_{\\pi}(x,a)c\\right]=\\lambda\\mathbb{E}\\left[\\displaystyle\\frac{(w_{\\pi}(x,a)c)^{2}}{1-\\lambda w_{\\pi}(x,a)c}\\right]=\\lambda S_{\\lambda}(\\pi).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Now focusing on the variance, we have: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\gamma^{\\lambda}(\\pi)=\\mathbb{E}\\left[\\left(\\hat{R}_{n}^{\\lambda}(\\pi)-\\mathbb{E}\\left[\\hat{R}_{n}^{\\lambda}(\\pi)\\right]\\right)^{2}\\right]}\\\\ &{\\qquad\\qquad\\leq\\displaystyle\\frac{1}{n\\lambda^{2}}\\mathbb{E}\\left[\\log(1-\\lambda w_{\\pi}(x,a)c)^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We use the following inequality $\\log(1+x)\\leq x/{\\sqrt{x+1}}$ holding for $x\\geq0$ to obtain our result: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{V}^{\\lambda}(\\pi)\\leq\\frac{1}{n}S_{\\lambda}(\\pi).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Notice that once $\\lambda>0$ , we have: ", "page_idx": 20}, {"type": "equation", "text": "$$\nS_{\\lambda}(\\pi)=\\mathbb{E}\\left[\\frac{w_{\\pi}(x,a)^{2}c^{2}}{1-\\lambda w_{\\pi}(x,a)c}\\right]\\leq\\frac{1}{\\lambda}\\mathbb{E}\\left[w_{\\pi}(x,a)|c|\\right]=\\frac{|R(\\pi)|}{\\lambda},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "resulting in a finite variance whenever $\\lambda>0$ : ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\nu^{\\lambda}(\\pi)\\leq\\frac{|R(\\pi)|}{n\\lambda}\\leq\\frac{1}{n\\lambda}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "$\\lambda=0$ recovers the IPS estimator in (2), with zero bias and variance bounded by $\\mathbb{E}\\left[w^{2}(x,a)c^{2}\\right]/n$ . When $\\lambda>0$ , a bias-variance trade-off emerges. The bias is always non-negative as we still recover an estimator that verifies (C1). The bias is capped at $\\lambda S_{\\lambda}(\\pi)$ , which diminishes to zero when $\\lambda$ is small and goes to $|R(\\pi)|$ as $\\lambda$ increases. Conversely, the variance decreases with a higher $\\lambda$ . Notably, $\\lambda>0$ ensures finite variance bounded by $1/\\lambda n$ , despite the estimator being unbounded. This is different from previous regularizations that relied on bounded functions to ensure finite variance. ", "page_idx": 20}, {"type": "text", "text": "While prior evaluations of estimators often relied on bias and variance analysis, Metelli et al. [38] argued for studying the non-asymptotic concentration rate of the estimators, advocating for subGaussianity as a desired property. Even if our estimator is not bounded, we prove in the following that it is sub-Gaussian. ", "page_idx": 20}, {"type": "text", "text": "Proposition (Sub-Gaussianity). Let $\\pi\\,\\in\\,\\Pi$ , $\\delta\\,\\in\\,(0,1]$ and $\\lambda\\:>\\:0,$ . Then the following   \ninequalities holds with probability at least $1-\\delta$ : $R(\\pi)-\\hat{R}_{n}^{\\lambda}(\\pi)\\leq\\frac{\\ln(2/\\delta)}{\\lambda n}\\,,\\qquad a n d\\qquad\\hat{R}_{n}^{\\lambda}(\\pi)-R(\\pi)\\leq\\lambda S_{\\lambda}(\\pi)+\\frac{\\ln(2/\\delta)}{\\lambda n}\\,.$   \nIn particular, setting $\\lambda=\\lambda_{*}=\\sqrt{\\ln(2/\\delta)/n\\mathbb{E}\\left[w_{\\pi}(x,a)^{2}c^{2}\\right]}$ yields that $|R(\\pi)-\\hat{R}_{n}^{\\lambda_{*}}(\\pi)|\\leq\\sqrt{2\\sigma^{2}\\ln(2/\\delta)}\\,,\\qquad w h e r e\\ \\sigma^{2}=2\\mathbb{E}\\left[w_{\\pi}(x,a)^{2}c^{2}\\right]/n\\,.$ (34) ", "page_idx": 21}, {"type": "text", "text": "Proof. Let $\\pi\\in\\Pi$ , $\\lambda>0$ and $\\delta>0$ . To prove sub-Gaussianity, we need both upper bounds and lower bounds on $R(\\pi)$ using $\\hat{R}_{n}^{\\lambda}(\\pi)$ . For the upper bound, we can use the bound of Corollary 5, and recall that $\\psi_{\\lambda}(x)\\leq x$ for all $x$ . We then obtain with a probability $1-\\delta$ : ", "page_idx": 21}, {"type": "equation", "text": "$$\nR(\\pi)\\leq\\psi_{\\lambda}\\left(\\hat{R}_{n}^{\\lambda}(\\pi)+\\frac{\\ln(1/\\delta)}{\\lambda n}\\right)\\implies R(\\pi)-\\hat{R}_{n}^{\\lambda}(\\pi)\\leq\\frac{\\ln(1/\\delta)}{\\lambda n}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For the lower bound on the risk, we go back to our Chernoff Lemma 12, and use the collection of i.i.d. random variable, that for any $i\\in[n]$ , are defined as: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\bar{X}_{i}=-\\frac{1}{\\lambda}\\log\\left(1-\\lambda w_{\\pi}(x_{i},a_{i})c_{i}\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "This gives for $a\\in\\mathbb{R}$ : ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P\\left(\\displaystyle\\sum_{i\\in[n]}\\bar{X}_{i}>a\\right)\\leq\\left(\\mathbb{E}\\left[\\exp\\left(\\lambda\\bar{X}_{1}\\right)\\right]\\right)^{n}\\exp(-\\lambda a)}\\\\ &{P\\left(\\displaystyle\\sum_{i\\in[n]}\\bar{X}_{i}>a\\right)\\leq\\left(\\mathbb{E}\\left[\\frac{1}{1-\\lambda w_{\\pi}(x,a)c}\\right]\\right)^{n}\\exp(-\\lambda a)}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Solving for $\\begin{array}{r}{\\delta=\\left(\\mathbb{E}\\left[\\frac{1}{1-\\lambda w_{\\pi}(x,a)c}\\right]\\right)^{n}\\exp(-\\lambda a)}\\end{array}$ , we get: ", "page_idx": 21}, {"type": "equation", "text": "$$\nP\\left(\\frac{1}{n}\\sum_{i\\in[n]}\\bar{X}_{i}>\\frac{1}{\\lambda}\\log\\left(\\mathbb{E}\\left[\\frac{1}{1-\\lambda w_{\\pi}(x,a)c}\\right]\\right)+\\frac{\\ln(1/\\delta)}{\\lambda n}\\right)\\leq\\delta\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The complementary event holds with at least probability $1-\\delta$ : ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\hat{R}_{n}^{\\lambda}(\\pi)\\leq\\frac{1}{\\lambda}\\log\\left(\\mathbb{E}\\left[\\frac{1}{1-\\lambda w_{\\pi}(x,a)c}\\right]\\right)+\\frac{\\ln(1/\\delta)}{\\lambda n},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "which implies using the inequality $\\log(x)\\leq x-1$ for all $x>0$ : ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{R}_{n}^{\\lambda}(\\pi)-R(\\pi)\\leq\\displaystyle\\frac{1}{\\lambda}\\log\\left(\\mathbb{E}\\left[\\frac{1}{1-\\lambda w_{\\pi}(x,a)c}\\right]\\right)-R(\\pi)+\\frac{\\ln(1/\\delta)}{\\lambda n}}\\\\ &{\\phantom{\\hat{R}_{n}^{\\lambda}(\\pi)}\\leq\\displaystyle\\frac{1}{\\lambda}\\left(\\mathbb{E}\\left[\\frac{1}{1-\\lambda w_{\\pi}(x,a)c}\\right]-1\\right)-R(\\pi)+\\frac{\\ln(1/\\delta)}{\\lambda n}}\\\\ &{\\phantom{\\hat{R}_{n}^{\\lambda}(\\pi)}\\leq\\mathbb{E}\\left[\\frac{w_{\\pi}(x,a)c}{1-\\lambda w_{\\pi}(x,a)c}-w_{\\pi}(x,a)c\\right]+\\frac{\\ln(1/\\delta)}{\\lambda n}}\\\\ &{\\phantom{\\hat{R}_{n}^{\\lambda}(\\pi)}\\leq\\lambda\\mathbb{E}\\left[\\frac{w_{\\pi}(x,a)^{2}c^{2}}{1-\\lambda w_{\\pi}(x,a)c}\\right]+\\frac{\\ln(1/\\delta)}{\\lambda n}=\\lambda S_{\\lambda}(\\pi)+\\frac{\\ln(1/\\delta)}{\\lambda n},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "which proves the lower bound on the risk. As both results hold with high probability, we use a union argument to have them both holding for probability at least $1-\\delta$ : ", "page_idx": 21}, {"type": "equation", "text": "$$\nR(\\pi)-\\hat{R}_{n}^{\\lambda}(\\pi)\\leq\\frac{\\ln(2/\\delta)}{\\lambda n}\\,,\\qquad\\mathrm{and}\\qquad\\hat{R}_{n}^{\\lambda}(\\pi)-R(\\pi)\\leq\\lambda S_{\\lambda}(\\pi)+\\frac{\\ln(2/\\delta)}{\\lambda n}\\,,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "which implies that: ", "page_idx": 22}, {"type": "equation", "text": "$$\n|R(\\pi)-\\hat{R}_{n}^{\\lambda}(\\pi)|\\leq\\lambda S_{\\lambda}(\\pi)+\\frac{\\ln(2/\\delta)}{\\lambda n}\\,\\leq\\lambda\\mathbb{E}\\left[w_{\\pi}(x,a)^{2}c^{2}\\right]+\\frac{\\ln(2/\\delta)}{\\lambda n}\\,.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "This means that setting $\\lambda=\\lambda_{*}=\\sqrt{\\ln(2/\\delta)/n\\mathbb{E}\\left[w_{\\pi}(x,a)^{2}c^{2}\\right]}$ yields a sub-Gaussian concentration: ", "page_idx": 22}, {"type": "equation", "text": "$$\n|R(\\pi)-\\hat{R}_{n}^{\\lambda_{*}}(\\pi)|\\leq2\\sqrt{\\frac{\\mathbb{E}\\left[w_{\\pi}(x,a)^{2}c^{2}\\right]\\ln(2/\\delta)}{n}}\\,.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "This ends the proof. ", "page_idx": 22}, {"type": "text", "text": "From (34), $\\hat{R}_{n}^{\\lambda_{*}}(\\pi)$ is sub-Gaussian with variance proxy $\\sigma^{2}=2\\mathbb{E}\\left[\\omega(x,a)^{2}c^{2}\\right]/n$ , which is lower that the variance proxy of the Harmonic estimator of Metelli et al. [38]. Indeed, the Harmonic estimator has a slightly worse variance proxy of $\\begin{array}{r}{\\sigma_{H}^{2}=\\frac{(2+\\sqrt{3})^{2}}{3}\\mathbb{E}\\left[\\omega(x,a)^{2}c^{2}\\right]/n}\\end{array}$ , giving $\\sigma^{2}<\\sigma_{H}^{2}$ . ", "page_idx": 22}, {"type": "text", "text": "E.3 OPS: Formal comparison with IX suboptimality ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Let us begin by stating results from the IX work [21]. Recall that the IX estimator is defined for any $\\lambda>0$ , by: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\hat{R}_{n}^{\\lambda\\mathrm{-IX}}(\\pi)=\\frac{1}{n}\\sum_{i=1}^{n}\\frac{\\pi(a_{i}|x_{i})}{\\pi_{0}(a_{i}|x_{i})+\\lambda/2}c_{i}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Let $\\Pi_{s}=\\{\\pi_{1},...,\\pi_{m}\\}$ be a finite set of predefined policies. In OPS, the goal is to find $\\pi_{*}^{\\mathrm{s}}\\in\\Pi_{\\mathrm{s}}$ that satisfies ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pi_{*}^{s}=\\operatorname*{argmin}_{\\pi\\in\\Pi_{s}}R(\\pi)=\\operatorname*{argmin}_{k\\in[m]}R(\\pi_{k})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "for $\\lambda>0$ , the selection strategy suggested in Gabbianelli et al. [21] was to search for: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\hat{\\pi}_{n}^{\\mathrm{S,\\tiny~IX}}=\\underset{\\pi\\in\\Pi_{\\mathrm{s}}}{\\mathrm{argmin}}\\,\\hat{R}_{n}^{\\lambda^{-\\mathrm{IX}}}(\\pi)=\\underset{k\\in[m]}{\\mathrm{argmin}}\\,\\hat{R}_{n}^{\\lambda^{-\\mathrm{IX}}}(\\pi)\\,.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proposition 15 (Suboptimality of the IX selection strategy). Let $\\lambda>0$ and $\\delta\\in(0,1]$ . Then, it holds with probability at least $1-\\delta$ that ", "page_idx": 22}, {"type": "equation", "text": "$$\n0\\le R(\\hat{\\pi}_{n}^{\\mathrm{s,\\,IX}})-R(\\pi_{*}^{\\mathrm{s}})\\le\\lambda\\mathcal{C}_{\\lambda/2}(\\pi_{*}^{\\mathrm{s}})+\\frac{2\\ln(2|\\Pi_{\\mathrm{s}}|/\\delta)}{\\lambda n}\\,,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathcal{C}_{\\lambda}(\\pi)=\\mathbb{E}\\left[\\frac{\\pi(a|x)}{\\pi_{0}^{2}(a|x)+\\lambda\\pi_{0}(a|x)}|c|\\right].\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Both suboptimalities (LS and IX) have the same form, they only depend on two different quantities $\\boldsymbol{S}_{\\lambda}$ and $\\mathcal{C}_{\\lambda}$ respectively). For a $\\pi\\in\\Pi$ and $\\lambda>0$ , If we can identify when $S_{\\lambda}(\\pi)\\le\\mathcal{C}_{\\lambda/2}\\dot{(\\pi)}$ , then we can prove that the sub-optimality of LS selection strategy is better than the one of IX. Luckily, this is always the case, and it is stated formally below. ", "page_idx": 22}, {"type": "text", "text": "Proposition 16. Let $\\pi\\in\\Pi$ and $\\lambda>0$ . We have: ", "page_idx": 22}, {"type": "equation", "text": "$$\nS_{\\lambda}(\\pi)\\le{\\mathcal C}_{\\lambda/2}(\\pi).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. Let $\\pi\\in\\Pi$ and $\\lambda>0$ , we have: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{C}_{\\lambda/2}(\\pi)-\\mathcal{S}_{\\lambda}(\\pi)=\\mathbb{E}\\left[\\frac{\\pi(a|x)}{\\pi_{0}^{2}(a|x)+\\frac{\\lambda}{2}\\pi_{0}(a|x)}|c|-\\frac{w_{\\pi}(x,a)^{2}c^{2}}{1-\\lambda w_{\\pi}(x,a)c}\\right]}\\\\ &{\\phantom{\\quad\\quad}=\\mathbb{E}\\left[\\frac{\\pi(a|x)}{\\pi_{0}^{2}(a|x)+\\frac{\\lambda}{2}\\pi_{0}(a|x)}|c|-\\frac{\\pi(a|x)^{2}c^{2}}{\\pi_{0}^{2}(a|x)-\\lambda\\pi_{0}(a|x)\\pi(a|x)c}\\right]}\\\\ &{\\phantom{\\quad\\quad}=\\mathbb{E}\\left[\\pi(a|x)|c\\left(\\frac{1}{\\pi_{0}^{2}(a|x)+\\frac{\\lambda}{2}\\pi_{0}(a|x)}-\\frac{\\pi(a|x)|c|}{\\pi_{0}^{2}(a|x)+\\lambda\\pi_{0}(a|x)\\pi(a|x)|c|}\\right)\\right]}\\\\ &{\\phantom{\\quad\\quad}=\\mathbb{E}\\left[\\pi(a|x)|c|\\left(\\frac{\\pi_{0}^{2}(a|x)(1-\\pi(a|x)|c|)+\\frac{\\lambda}{2}\\pi_{0}(a|x)\\pi(a|x)|c|}{(\\pi_{0}^{2}(a|x)+\\frac{\\lambda}{2}\\pi_{0}(a|x))(\\pi_{0}^{2}(a|x)+\\lambda\\pi_{0}(a|x)\\pi(a|x)|c|)}\\right)\\right]}\\\\ &{\\phantom{\\quad\\quad}>0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "This means that the suboptimality of LS selection strategy is better bounded than the one of IX. Our experiments confirm that the LS selection strategy is better than IX in practical scenarios. ", "page_idx": 23}, {"type": "text", "text": "Minimax optimality of our selection strategy. As discussed in Gabbianelli et al. [21], pessimistic algorithms tend to have the property that their regret scales with the minimax sample complexity of estimating the value of the optimal policy [27]. For the case of multi-armed bandit (one context $x$ ), this estimation minimax sample complexity is proved by Li et al. [34] and is of the rate $O(\\mathbb{E}[w_{\\pi^{*}}(x,a)^{2}c^{2}])$ , with $\\pi^{*}$ being the optimal policy. Our bound matches the lower bound proved by Li et al. [34], as: ", "page_idx": 23}, {"type": "equation", "text": "$$\nS_{\\lambda}(\\pi^{*})=\\mathbb{E}\\left[\\frac{w_{\\pi^{*}}(x,a)^{2}c^{2}}{1-\\lambda w_{\\pi^{*}}(x,a)c}\\right]\\le\\mathbb{E}\\left[w_{\\pi^{*}}(x,a)^{2}c^{2}\\right],\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "which is not the case for the suboptimality of IX, that only matches it in the deterministic setting with binary costs, as: ", "page_idx": 23}, {"type": "equation", "text": "$$\n{\\mathcal{C}}_{\\lambda}(\\pi^{*})=\\mathbb{E}\\left[{\\frac{\\pi^{*}(a|x)}{\\pi_{0}^{2}(a|x)+\\lambda\\pi_{0}(a|x)}}|c|\\right]\\leq\\mathbb{E}\\left[{\\frac{\\pi^{*}(a|x)}{\\pi_{0}^{2}(a|x)}}|c|\\right]=\\mathbb{E}\\left[\\left({\\frac{\\pi^{*}(a|x)}{\\pi_{0}(a|x)}}\\right)^{2}c^{2}\\right],\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "with the last inequality only holding when $\\pi^{*}$ is deterministic and the costs are binary. For deterministic policies and the general contextual bandit, we invite the reader to see a formal proof of the minimax lower bound of pessimism in Jin et al. [28, Theorem 4.4], matched for both IX and LS. ", "page_idx": 23}, {"type": "text", "text": "E.4 OPL: Formal comparison of PAC-Bayesian bounds ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "As it is easier to work with linear estimators within the PAC-Bayesian framework, we define the following estimator of the risk $\\hat{R}_{n}^{p-\\mathrm{LIN}}(\\pi)$ , with the help of a function $p:\\mathbb{R}\\rightarrow\\mathbb{R}$ as: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\hat{R}_{n}^{p-\\mathrm{LIN}}(\\pi)=\\frac{1}{n}\\sum_{i=1}^{n}\\frac{\\pi(a_{i}|x_{i})}{p(\\pi_{0}(a_{i}|x_{i}))}c_{i}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "with the only condition on $p$ to be $\\{C_{1}^{\\mathrm{LIN}}\\,:\\,\\forall x,p(x)\\,\\geq\\,x\\}$ . This condition helps us control the impact of actions with low probabilities under $\\pi_{0}$ . This risk estimator encompasses well known risk estimators depending on the choice of $p$ . ", "page_idx": 23}, {"type": "text", "text": "Now that we defined the family of estimators covered by our analysis, we attack the problem of deriving generalization bounds. We derive our empirical high order bound expressed in the following: ", "page_idx": 23}, {"type": "equation", "text": "$$\nR(\\pi_{Q})\\leq\\psi_{\\lambda}\\left(\\hat{R}_{n}^{p-\\mathrm{LIN}}(\\pi_{Q})+\\frac{K\\mathcal{L}(Q||P)+\\ln\\frac{1}{\\delta}}{\\lambda n}+\\sum_{\\ell=2}^{2L}\\frac{\\lambda^{\\ell-1}}{\\ell}\\hat{\\mathcal{M}}_{n}^{p-\\mathrm{LIN},\\ell}(\\pi_{Q})\\right)\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "with: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\hat{\\mathcal{M}}_{n}^{p-\\mathrm{LIN},\\ell}(\\pi_{Q})=\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\frac{\\pi_{Q}(a_{i}|x_{i})}{p(\\pi_{0}(a_{i}|x_{i}))^{\\ell}}c_{i}^{\\ell}}\\\\ &{}&{\\psi_{\\lambda}=x:\\rightarrow\\frac{1-\\exp(-\\lambda x)}{\\lambda}.\\ \\ \\ }\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. Let $L\\geq1$ , we have from Lemma 13, and for any positive random variable $X\\geq0$ and $\\lambda>0$ : ", "page_idx": 24}, {"type": "equation", "text": "$$\nf_{2L-1}(0)={\\frac{1}{2L}}\\geq f_{2L-1}(\\lambda X)=-{\\frac{\\log(1+\\lambda X)-\\sum_{\\ell=1}^{2L-1}{\\frac{(-1)^{\\ell-1}}{\\ell}}(\\lambda X)^{\\ell}}{(\\lambda X)^{2L}}}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "which is equivalent to: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i=1}^{2L}\\frac{(-1)^{\\ell-1}}{\\ell}(\\lambda X)^{\\ell}\\le\\log(1+\\lambda X)\\iff\\exp\\left(\\sum_{\\ell=1}^{2L}\\frac{(-1)^{\\ell-1}}{\\ell}(\\lambda X)^{\\ell}\\right)\\le1+\\lambda X}\\\\ &{\\implies\\mathbb{E}\\left[\\exp\\left(\\displaystyle\\sum_{\\ell=1}^{2L}\\frac{(-1)^{\\ell-1}}{\\ell}(\\lambda X)^{\\ell}\\right)\\right]\\le1+\\mathbb{E}\\left[\\lambda X\\right]}\\\\ &{\\implies\\mathbb{E}\\left[\\exp\\left(\\displaystyle\\sum_{\\ell=1}^{2L}\\frac{(-1)^{\\ell-1}}{\\ell}(\\lambda X)^{\\ell}\\right)\\right]\\le\\exp\\left(\\log(1+\\mathbb{E}\\left[\\lambda X\\right])\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "which implies that: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\exp\\left(\\lambda(X-\\frac{1}{\\lambda}\\log(1+\\mathbb{E}\\left[\\lambda X\\right]))+\\sum_{\\ell=2}^{2L}\\frac{(-1)^{\\ell-1}}{\\ell}(\\lambda X)^{\\ell}\\right)\\right]\\leq1.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "For any $X\\le0$ , we can inject $-X\\geq0$ to obtain: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\forall X\\leq0,\\quad\\mathbb{E}\\left[\\exp\\left(\\lambda\\left(-\\frac{1}{\\lambda}\\log(1+\\mathbb{E}\\left[\\lambda X\\right])-X\\right)-\\sum_{k=2}^{2K}\\frac{1}{k}(\\lambda X)^{k}\\right)\\right]\\leq1.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Let: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{d_{\\theta}(a|x)=\\mathbb{1}\\left[f_{\\theta}(x)=a\\right]\\,,\\forall(x,a)\\in\\mathcal{X}\\times\\mathcal{A}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "it means that: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\pi_{Q}(a|x)=\\mathbb{E}_{\\theta\\sim Q}\\left[d_{\\theta}(a|x)\\right]\\,,\\forall(x,a)\\in\\mathcal{X}\\times\\mathcal{A}\\,.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Let $\\lambda>0$ . The adequate function $g$ we are going to use in combination with Lemma 14 is: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\iota(\\theta,\\mathcal{D}_{n})=\\sum_{i=1}^{n}\\lambda\\left(-\\frac{1}{\\lambda}\\log(1+\\lambda R^{p-\\mathrm{LIN}}(d_{\\theta}))-\\frac{d_{\\theta}(a_{i}|x_{i})}{p(\\pi_{0}(a_{i}|x_{i}))}c_{i}\\right)-\\sum_{\\ell=2}^{2L}\\frac{1}{\\ell}\\left(\\lambda\\frac{d_{\\theta}(a_{i}|x_{i})}{p(\\pi_{0}(a_{i}|x_{i}))}c_{i}\\right)^{\\ell}}}\\\\ &{}&{=\\sum_{i=1}^{n}\\lambda\\left(-\\frac{1}{\\lambda}\\log(1+\\lambda R^{p-\\mathrm{LIN}}(d_{\\theta}))-\\frac{d_{\\theta}(a_{i}|x_{i})}{p(\\pi_{0}(a_{i}|x_{i}))}c_{i}\\right)-\\sum_{\\ell=2}^{2L}\\frac{d_{\\theta}(a_{i}|x_{i})}{\\ell}\\left(\\frac{\\lambda}{p(\\pi_{0}(a_{i}|x_{i}))}c_{i}\\right)^{\\ell}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "By exploiting the i.i.d. nature of the data and exchanging the order of expectations $P$ is independent of $\\mathcal{D}_{n}$ ), we can naturally prove using (39) that: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\Psi_{g}=\\mathbb{E}_{P}\\left[\\prod_{i=1}^{n}\\mathbb{E}\\left[\\exp\\left(\\lambda\\left(-\\frac{1}{\\lambda}\\log(1+\\lambda R^{p-\\mathrm{LIN}}(d_{\\theta}))-X_{i}(\\theta)\\right)-\\sum_{k=2}^{2K}\\frac{1}{k}\\left(\\lambda X_{i}(\\theta)\\right)^{k}\\right)\\right]\\right]\\leq1,\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "as we have : ", "page_idx": 25}, {"type": "equation", "text": "$$\nX_{i}(\\theta)=\\frac{d_{\\theta}(a_{i}|x_{i})}{p(\\pi_{0}(a_{i}|x_{i}))}c_{i}\\leq0\\quad\\forall i.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Injecting $\\Psi_{g}$ in Lemma 14, rearranging terms and using that $\\hat{R}_{n}^{p-\\mathrm{LIN}}(\\pi)$ has positive bias concludes the proof. \u53e3 ", "page_idx": 25}, {"type": "text", "text": "Similarly to the OPE section, we use this general bound to obtain a PAC-Bayesian Empirical Second Moment bound and the PAC-Bayesian LS-LIN bound. That we state directly below: ", "page_idx": 25}, {"type": "text", "text": "Empirical second moment bound. With $L=1$ , we obtain the following: ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Corollary 18 (Second Moment Upper bound). Given a prior $P$ on $\\mathcal{F}_{\\Theta}$ , $\\delta\\in(0,1]$ and $\\lambda>0$ . The following bound holds with probability at least $1-\\delta$ uniformly for all distribution $Q$ over $\\mathcal{F}_{\\Theta}$ : ", "page_idx": 25}, {"type": "equation", "text": "$$\nR(\\pi_{Q})\\leq\\psi_{\\lambda}\\left(\\hat{R}_{n}^{p}(\\pi_{Q})+\\frac{K\\mathcal{L}(Q||P)+\\ln\\frac{1}{\\delta}}{\\lambda n}+\\frac{\\lambda}{2}\\hat{\\mathcal{M}}_{n}^{p-\\mathrm{LIN},2}(\\pi_{Q})\\right).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Log Smoothing PAC-Bayesian Bound. With $L\\rightarrow\\infty$ , we obtain the following: ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Proposition 19 $(\\hat{R}_{n}^{\\lambda-\\mathrm{LIN}}$ PAC-Bayes bound). Given a prior $P$ on $\\mathcal{F}_{\\Theta}$ , $\\delta\\in(0,1]$ and $\\lambda>0$ , the following bound holds with probability at least $1-\\delta$ uniformly for all distribution $Q$ over $\\mathcal{F}_{\\Theta}$ : ", "page_idx": 25}, {"type": "equation", "text": "$$\nR(\\pi_{Q})\\leq\\psi_{\\lambda}\\left(\\hat{R}_{n}^{\\lambda-\\mathrm{LIN}}(\\pi_{Q})+\\frac{\\mathcal{K L}(Q||P)+\\ln\\frac{1}{\\delta}}{\\lambda n}\\right).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "with: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\hat{R}_{n}^{\\lambda\\mathrm{{-LIN}}}(\\pi)=-\\frac{1}{n}\\sum_{i=1}^{n}\\frac{\\pi(a_{i}|x_{i})}{\\lambda}\\log\\left(1-\\frac{\\lambda c_{i}}{\\pi_{0}(a_{i}|x_{i})}\\right).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Following the same proof schema as of the OPE section, we can demonstrate that the Log Smoothing PAC-Bayesian bound dominates the Empirical Second moment PAC-Bayesian bound $L=1$ . However, we use the bound of $L=1$ as an intermediary to state the dominance of the Log Smoothing PAC-Bayesian bound. ", "page_idx": 25}, {"type": "text", "text": "Indeed, we can easily compare the result obtained with $L=1$ to previously derived PAC-Bayesian bounds for off-policy learning. We start by writing down the conditional Bernstein bound of Sakhi et al. [49] holding for the (linear) cIPS $\\boldsymbol{p}:\\boldsymbol{x}\\to\\operatorname*{max}(\\boldsymbol{x},\\tau))$ . For a policy $\\pi_{Q}$ and a $\\lambda>0$ , we have: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R(\\pi_{Q})\\leq\\hat{R}_{n}^{\\tau}(\\pi_{Q})+\\sqrt{\\frac{K\\mathcal{L}(Q||P)+\\ln\\frac{4\\sqrt{n}}{\\delta}}{2n}}+\\frac{K\\mathcal{L}(Q||P)+\\ln\\frac{2}{\\delta}}{\\lambda n}+\\lambda g\\left(\\lambda/\\tau\\right)\\mathcal{V}_{n}^{\\tau}(\\pi_{Q}).}\\\\ &{R(\\pi_{Q})\\leq\\hat{R}_{n}^{\\tau}(\\pi_{Q})+\\frac{K\\mathcal{L}(Q||P)+\\ln\\frac{1}{\\delta}}{\\lambda n}+\\frac{\\lambda}{2}\\hat{S}_{n}^{\\tau}(\\pi_{Q}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "equation", "text": "$$\n(\\mathbf{L}=\\mathbf{1})\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We can observe that the previously derived conditional Bernstein bound has several terms that make it less tight: ", "page_idx": 25}, {"type": "text", "text": "\u2022 It has an additional, strictly positive square root KL divergence term. ", "page_idx": 25}, {"type": "text", "text": "\u2022 The multiplicative factor $g(\\lambda/\\tau)$ is always bigger than $1/2$ , and diverges when $\\tau\\rightarrow0$ . \u2022 With enough data $(n\\gg1)$ ), we also have: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\hat{S}_{n}^{\\tau}(\\pi_{Q})\\approx\\mathbb{E}\\left[\\frac{\\pi_{Q}(a|x)}{\\operatorname*{max}\\{\\pi_{0}(a|x),\\tau\\}^{2}}c(a,x)^{2}\\right]\\le\\mathbb{E}\\left[\\frac{\\pi_{Q}(a|x)}{\\operatorname*{max}\\{\\pi_{0}(a|x),\\tau\\}^{2}}\\right]\\approx\\mathcal{V}_{n}^{\\tau}(\\pi_{Q}).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "These observations confirm that the new bound derived with $L=1$ is tighter than what was previously proposed for cIPS, especially when $n\\gg1$ . As our bound can work for other estimators, we also compare it to a recently proposed PAC-Bayes bound in Aouali et al. [5] for the exponentially-smoothed estimator $(p:x\\rightarrow x^{\\alpha})$ ) with $\\alpha\\in[0,1]$ : ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{{\\cal R}(\\pi_{Q})\\le\\hat{\\cal R}_{n}^{\\alpha}(\\pi_{Q})+\\sqrt{\\frac{K{\\cal Z}(Q||P)+\\ln\\frac{4\\sqrt{n}}{\\delta}}{2n}}+\\frac{K{\\cal Z}(Q||P)+\\ln\\frac{2}{\\delta}}{\\lambda n}+\\frac{\\lambda}{2}\\left(\\mathcal{V}_{n}^{\\alpha}(\\pi_{Q})+\\hat{\\cal S}_{n}^{\\alpha}(\\pi_{Q})\\right)\\cdot}\\\\ &{{\\cal R}(\\pi_{Q})\\le\\hat{\\cal R}_{n}^{\\alpha}(\\pi_{Q})+\\frac{K{\\cal Z}(Q||P)+\\ln\\frac{1}{\\delta}}{\\lambda n}+\\frac{\\lambda}{2}\\hat{\\cal S}_{n}^{\\alpha}(\\pi_{Q}).\\eqno({\\bf L}={\\bf1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We can clearly see that the previously proposed bound for the exponentially smoothed estimator has two additional positive quantities that makes it less tight than our bound. In addition, computing our bound does not rely on expectations under $\\pi_{0}$ (contrary to the previous bounds that have $\\mathcal{V}_{n}$ ) which alleviates the need to access the logging policy and reduce the computations. ", "page_idx": 26}, {"type": "text", "text": "This demonstrates the superiority of $L=1$ compared to existing variance sensitive PAC-Bayesian bounds. It means that $L\\rightarrow\\infty$ is even better. We can also prove that the Log smoothing PAC-Bayesian Bound is better than the one of IX in Gabbianelli et al. [21]. Indeed, using $\\textstyle\\log(1+x)\\geq{\\frac{x}{1+x/2}}$ for all $x\\geq0$ , we have for any $P,Q\\in\\mathcal{P}(\\Theta)$ and $\\lambda>0$ : ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\dot{\\gamma}_{\\lambda}\\left(\\hat{R}_{n}^{\\lambda-\\mathrm{LIN}}(\\pi_{Q})+\\frac{K{\\mathcal L}(Q||P)+\\ln\\frac{1}{\\delta}}{\\lambda n}\\right)\\leq\\hat{R}_{n}^{\\lambda-\\mathrm{LIN}}(\\pi_{Q})+\\frac{K{\\mathcal L}(Q||P)+\\ln\\frac{1}{\\delta}}{\\lambda n}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq-\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\frac{\\pi_{Q}(a_{i}|x_{i})}{\\lambda}\\log\\left(1-\\frac{\\lambda c_{i}}{\\pi_{0}(a_{i}|x_{i})}\\right)+\\frac{K{\\mathcal L}(Q||P)}{\\lambda n}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\frac{\\pi_{Q}(a_{i}|x_{i})}{\\pi_{0}(a_{i}|x_{i})-\\lambda c_{i}/2}+\\frac{K{\\mathcal L}(Q||P)+\\ln\\frac{1}{\\delta}}{\\lambda n}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\hat{R}_{n}^{\\lambda-\\mathrm{IN}}(\\pi_{Q})+\\frac{K{\\mathcal L}(Q||P)+\\ln\\frac{1}{\\delta}}{\\lambda n},\\qquad\\quad\\mathrm{(IX-bound)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "with the last implication leveraging that the cost is always bigger than $-1/$ This proves that our bound is better than the IX bound. This means that our PAC-Bayesian bound is better than all existing PAC-Bayesian off-policy learning bounds. ", "page_idx": 26}, {"type": "text", "text": "E.5 OPL: Formal comparison with IX PAC-Bayesian learning suboptimality ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Let us begin by stating results from the IX work [21]. Recall that the IX estimator is defined for any $\\lambda>0$ , by: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\hat{R}_{n}^{\\mathrm{IX-}\\lambda}(\\pi)=\\frac{1}{n}\\sum_{i=1}^{n}\\frac{\\pi(a_{i}|x_{i})}{\\pi_{0}(a_{i}|x_{i})+\\lambda/2}c_{i},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "and that we used the linearized version of the LS estimator, LS-LIN defined as: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\hat{R}_{n}^{\\lambda\\mathrm{{-LIN}}}(\\pi)=-\\frac{1}{n}\\sum_{i=1}^{n}\\frac{\\pi(a_{i}|x_{i})}{\\lambda}\\log\\left(1-\\frac{\\lambda c_{i}}{\\pi_{0}(a_{i}|x_{i})}\\right).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Let $\\Theta$ be a parameter space and $\\mathcal{P}(\\Theta)$ be the set of all probability distribution on $\\Theta$ . Our goal is to find the best policy in a chosen class ${\\mathcal{L}}(\\Theta)\\subset{\\mathcal{P}}(\\Theta)$ : ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\pi_{Q^{*}}=\\operatorname*{argmin}_{Q\\in\\mathcal{L}(\\Theta)}R(\\pi_{Q}).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "For $\\lambda>0$ and a prior $P\\in\\mathcal P(\\Theta)$ , the PAC-Bayesian learning strategy suggested in Gabbianelli et al. [21] is to find in ${\\mathcal{L}}(\\Theta)\\subset{\\mathcal{P}}(\\Theta)$ : ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\hat{\\pi}_{Q_{n}}^{\\mathrm{IX}}=\\operatorname*{argmin}_{Q\\in\\mathcal{L}(\\Theta)}\\left\\{\\hat{R}_{n}^{\\mathrm{IX}-\\lambda}(\\pi_{Q})+\\frac{\\mathcal{K L}(Q||P)}{\\lambda n}\\right\\}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "This learning strategy suffers from a suboptimality bounded in the result below: ", "page_idx": 27}, {"type": "text", "text": "Proposition 20 (Suboptimality of the IX PAC-Bayesian learning strategy from [21]). Let $\\lambda>0$ and $\\delta\\in(0,1]$ . Then, it holds with probability at least $1-\\delta$ that ", "page_idx": 27}, {"type": "equation", "text": "$$\n0\\le R(\\hat{\\pi}_{Q_{n}}^{\\mathrm{IX}})-R(\\pi_{Q^{*}})\\le\\lambda\\mathcal{C}_{\\lambda/2}(\\pi_{Q^{*}})+\\frac{2\\left(K\\mathcal{L}(Q^{*}||P)+\\ln(2/\\delta)\\right)}{\\lambda n}\\,,\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathcal{C}_{\\lambda}(\\pi)=\\mathbb{E}\\left[\\frac{\\pi(a|x)}{\\pi_{0}^{2}(a|x)+\\lambda\\pi_{0}(a|x)}|c|\\right].\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Similarly for PAC-Bayesian learning, both suboptimalities (LS and IX) have the same form, they only depend on two different quantities $(S_{\\lambda}^{\\mathrm{LIN}}$ and $\\mathcal{C}_{\\lambda}$ respectively). For a $\\pi\\in\\Pi$ and $\\lambda>0$ , If we can identify when $S_{\\lambda}^{\\mathrm{LIN}}(\\pi)\\le{\\mathcal C}_{\\lambda/2}(\\pi)$ , then we can prove that the sub-optimality of LS PAC-Bayesian learning strategy is better than the one of IX in certain cases. Luckily, this is always the case, and it is stated formally below. ", "page_idx": 27}, {"type": "text", "text": "Proposition 21. Let $\\pi\\in\\Pi$ and $\\lambda>0$ . We have: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{S_{\\lambda}^{\\mathrm{LIN}}(\\pi)\\le{\\mathcal C}_{\\lambda/2}(\\pi).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. Let $\\pi\\in\\Pi$ and $\\lambda>0$ , and recall that: ", "page_idx": 27}, {"type": "equation", "text": "$$\nS_{\\lambda}^{\\mathrm{LIN}}(\\pi)=\\mathbb{E}\\left[\\frac{\\pi(a|x)c^{2}}{\\pi_{0}^{2}(a|x)-\\lambda\\pi_{0}(a|x)c}\\right].\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We have: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\mathcal C}_{\\lambda/2}(\\pi)-S_{\\lambda}^{\\mathrm{LIN}}(\\pi)=\\mathbb{E}\\left[\\frac{\\pi(a|x)}{\\pi_{0}^{2}(a|x)+\\frac{\\lambda}{2}\\pi_{0}(a|x)}|c|-\\frac{\\pi(a|x)c^{2}}{\\pi_{0}^{2}(a|x)-\\lambda\\pi_{0}(a|x)c}\\right]}\\\\ &{\\phantom{\\mathcal{C}_{\\lambda/2}(\\pi)-}=\\mathbb{E}\\left[\\pi(a|x)|c|\\left(\\frac{1}{\\pi_{0}^{2}(a|x)+\\frac{\\lambda}{2}\\pi_{0}(a|x)}-\\frac{|c|}{\\pi_{0}^{2}(a|x)+\\lambda\\pi_{0}(a|x)|c|}\\right)\\right]}\\\\ &{\\phantom{\\mathcal{C}_{\\lambda/2}(\\pi)-}=\\mathbb{E}\\left[\\pi(a|x)|c|\\left(\\frac{\\pi_{0}^{2}(a|x)\\left(1-|c|\\right)+\\frac{\\lambda}{2}\\pi_{0}(a|x)|c|}{(\\pi_{0}^{2}(a|x)+\\frac{\\lambda}{2}\\pi_{0}(a|x))(\\pi_{0}^{2}(a|x)+\\lambda\\pi_{0}(a|x)|c|)}\\right)\\right]}\\\\ &{\\phantom{\\mathcal{C}_{\\lambda/2}(\\pi)-}\\geq0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Similarly, this means that the suboptimality of LS-LIN PAC-Bayesian learning strategy is also, better bounded than the one of IX. ", "page_idx": 27}, {"type": "text", "text": "Minimax optimality of our learning strategy. From Jin et al. [28, Theorem 4.4] we can state that the mi\u221animax suboptimality lower bound, in the case of deterministic optimal policies is of the rate $O(1/\\sqrt{n C^{*}})$ with $\\mathrm{inf}_{x\\in{\\mathcal{X}}}\\,\\pi_{0}(\\pi^{*}(x)|x)>C^{*}$ . Our bound as well as IX bound match this minimax lower bound, as: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{S_{\\lambda}^{\\mathrm{LIN}}(\\pi^{*})=\\mathbb{E}_{x,c}\\left[\\frac{c^{2}}{\\pi_{0}(\\pi^{*}(x)|x)-\\lambda c}\\right]\\leq\\frac{1}{C^{*}}}\\\\ {\\mathcal{C}_{\\lambda}(\\pi^{*})=\\mathbb{E}_{x,c}\\left[\\frac{|c|}{\\pi_{0}(\\pi^{*}(x)|x)+\\lambda}\\right]\\leq\\frac{1}{C^{*}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "One can see that for both, selecting a ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\lambda^{*}=\\sqrt{\\frac{2\\left(K\\mathcal{L}(Q^{*}||P)+\\ln(2/\\delta)\\right)C^{*}}{n}},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "gets you the desired bound, matching this minimax rate. ", "page_idx": 28}, {"type": "text", "text": "F Proofs of OPE ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "F.1 Proof of high order empirical moments bound (Proposition 1) ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Proposition (Empirical moments risk bound). Let $\\pi\\in\\Pi$ , $L\\geq1$ , $\\delta\\in(0,1]$ , $\\lambda>0$ and $h$ satisfying (C1). Then it holds with probability at least $1-\\delta$ that $R(\\pi)\\leq\\psi_{\\lambda}\\Bigl(\\hat{R}_{n}^{h}(\\pi)+\\sum_{\\ell=2}^{2L}\\frac{\\lambda^{\\ell-1}}{\\ell}\\hat{\\mathcal{M}}_{n}^{h,\\ell}(\\pi)+\\frac{\\ln(1/\\delta)}{\\lambda n}\\Bigr)\\,,$ where $\\psi_{\\lambda}$ and $\\hat{\\mathcal{M}}_{n}^{h,\\ell}(\\pi)$ are defined in (5), respectively, and recall that $\\psi_{\\lambda}(x)\\leq x$ . ", "page_idx": 28}, {"type": "text", "text": "Proof. Let $L\\in\\mathbb{N}^{*}$ , $\\lambda>0$ and $X\\geq0$ a positive random variable. We have $2{\\cal L}-1\\geq1$ , and with the decreasing nature of $f_{(2L-1)}$ (Lemma 13), we also have: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{f_{(2L-1)}(0)\\geq f_{2L-1}(\\lambda X)\\iff\\frac{1}{2L}\\geq-\\frac{\\log(1+\\lambda X)-\\sum_{t=1}^{2L-1}\\frac{(-1)^{t-1-\\lambda}}{k}(\\lambda X)^{t}}{(\\lambda X)^{2}}\\qquad\\qquad\\qquad\\qquad}&{}\\\\ {\\iff\\sum_{t=1}^{2L}\\frac{(-1)^{t-1}}{k}(\\lambda X)^{t}\\leq\\log(1+\\lambda X)}\\\\ {\\iff\\exp\\left(\\frac{2L}{\\cosh}\\left(-1\\right)^{t-1}(\\lambda X)^{t}\\right)\\leq1+\\lambda X}\\\\ {\\implies\\mathbb{E}\\left[\\exp\\left(\\frac{2L}{\\cosh}\\frac{(-1)^{t-1}}{\\ell}(\\lambda X)^{t}\\right)\\right]\\leq1+\\lambda X\\left[X\\right]}\\\\ {\\implies\\mathbb{E}\\left[\\exp\\left(\\frac{2L}{\\cosh}\\frac{(-1)^{t-1}}{\\ell}(\\lambda X)^{t}\\right)\\right]\\leq\\exp\\left(\\log(1+\\lambda\\mathbb{E}\\left[X\\right]\\right)\\right)}\\\\ {\\implies\\mathbb{E}\\left[\\exp\\left(\\lambda X-\\frac{1}{\\lambda}\\log(1+\\lambda\\mathbb{E}\\left[X\\right]\\right)\\right)+\\sum_{t=1}^{2L}\\frac{(-1)^{t-1}}{\\ell}(\\lambda X)^{t}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "For any $X\\le0$ , we can inject $-X\\geq0$ to obtain: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\forall X\\leq0,\\quad\\mathbb{E}\\left[\\exp\\left(\\lambda\\left(-\\frac{1}{\\lambda}\\log\\left(1-\\lambda\\mathbb{E}\\left[X\\right]\\right)-X\\right)-\\sum_{\\ell=2}^{2L}\\frac{1}{\\ell}(\\lambda X)^{\\ell}\\right)\\right]\\leq1.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "The result in Equation (43) will be combined with Chernoff Inequality (Lemma 12) to finally prove our bound. Let $\\lambda>0$ , for our problem, we define the random variable $X_{i}$ to use in the Chernoff Inequality as: ", "page_idx": 28}, {"type": "equation", "text": "$$\nX_{i}=-\\frac{1}{\\lambda}\\log\\left({1-\\lambda\\mathbb{E}\\left[h\\right]}\\right)-h_{i}-\\sum_{\\ell=2}^{2L}\\frac{1}{\\ell}(\\lambda h_{i})^{\\ell}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "For any $a\\in\\mathbb{R}$ , this gives us the following: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P\\left(\\displaystyle\\sum_{i\\in[n]}X_{i}>a\\right)\\leq\\left(\\mathbb{E}\\left[\\exp{(\\lambda X_{1})}\\right]\\right)^{n}\\exp(-\\lambda a)}\\\\ &{P\\left(-\\displaystyle\\frac{n}{\\lambda}\\log\\left(1-\\lambda\\mathbb{E}\\left[h\\right]\\right)-\\displaystyle\\sum_{i\\in[n]}\\left(h_{i}+\\displaystyle\\sum_{\\ell=2}^{2L}\\frac{1}{\\ell}(\\lambda h_{i})^{\\ell}\\right)>a\\right)\\leq\\left(\\mathbb{E}\\left[\\exp{(\\lambda X_{1})}\\right]\\right)^{n}\\exp(-\\lambda a)}\\\\ &{P\\left(-\\displaystyle\\frac{n}{\\lambda}\\log\\left(1-\\lambda\\mathbb{E}\\left[h\\right]\\right)-\\displaystyle\\sum_{i\\in[n]}\\left(h_{i}+\\displaystyle\\sum_{\\ell=2}^{2L}\\frac{1}{\\ell}(\\lambda h_{i})^{\\ell}\\right)>a\\right)\\leq\\exp(-\\lambda a)\\quad\\left(\\mathrm{Use~of~Equation}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Solving for $\\delta=\\exp(-\\lambda a)$ , we get: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle P\\left(-\\frac{n}{\\lambda}\\log\\left(1-\\lambda\\mathbb{E}\\left[h\\right]\\right)-\\sum_{i\\in[n]}\\left(h_{i}+\\sum_{\\ell=2}^{2L}\\frac{1}{\\ell}(\\lambda h_{i})^{\\ell}\\right)>\\frac{\\ln(1/\\delta)}{\\lambda}\\right)\\leq\\delta}}\\\\ {{\\displaystyle P\\left(-\\frac{1}{\\lambda}\\log\\left(1-\\lambda\\mathbb{E}\\left[h\\right]\\right)-\\frac{1}{n}\\sum_{i\\in[n]}\\left(h_{i}+\\sum_{\\ell=2}^{2L}\\frac{\\lambda^{\\ell}}{\\ell}h_{i}^{\\ell}\\right)>\\frac{\\ln(1/\\delta)}{\\lambda n}\\right)\\leq\\delta}}\\\\ {{\\displaystyle P\\left(-\\frac{1}{\\lambda}\\log\\left(1-\\lambda\\mathbb{E}\\left[h\\right]\\right)-\\hat{R}_{n}^{h}(\\pi)-\\sum_{\\ell=2}^{2L}\\frac{\\lambda^{\\ell-1}}{\\ell}\\hat{A}_{n}^{h,\\ell}(\\pi)>\\frac{\\ln(1/\\delta)}{\\lambda n}\\right)\\leq\\delta}}\\\\ {{\\displaystyle P\\left(-\\frac{1}{\\lambda}\\log\\left(1-\\lambda\\mathbb{E}\\left[h\\right]\\right)>\\hat{R}_{n}^{h}(\\pi)+\\sum_{\\ell=2}^{2L}\\frac{\\lambda^{\\ell-1}}{\\ell}\\hat{A}_{n}^{h,\\ell}(\\pi)\\frac{\\ln(1/\\delta)}{\\lambda n}\\right)\\leq\\delta.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "This means that the following, complementary event will hold with probability at least $1-\\delta$ : ", "page_idx": 29}, {"type": "equation", "text": "$$\n-\\frac{1}{\\lambda}\\log\\left(1-\\lambda\\mathbb{E}\\left[h\\right]\\right)\\leq\\hat{R}_{n}^{h}(\\pi)+\\sum_{\\ell=2}^{2L}\\frac{\\lambda^{\\ell-1}}{\\ell}\\hat{\\mathcal{M}}_{n}^{h,\\ell}(\\pi)\\frac{\\ln(1/\\delta)}{\\lambda n}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "$\\psi_{\\lambda}$ being a non-decreasing function, applying it to the two sides of this inequality gives us: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[h\\right]\\leq\\psi_{\\lambda}\\Big(\\hat{R}_{n}^{h}(\\pi)+\\sum_{\\ell=2}^{2L}\\frac{\\lambda^{\\ell-1}}{\\ell}\\hat{\\mathcal{M}}_{n}^{h,\\ell}(\\pi)+\\frac{\\ln(1/\\delta)}{\\lambda n}\\Big).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Finally, $h$ satisfies (C1), this means that the bound is also an upper bound on the true risk, giving: ", "page_idx": 29}, {"type": "equation", "text": "$$\nR(\\pi)\\leq\\psi_{\\lambda}\\Bigl(\\hat{R}_{n}^{h}(\\pi)+\\sum_{\\ell=2}^{2L}\\frac{\\lambda^{\\ell-1}}{\\ell}\\hat{\\mathcal{M}}_{n}^{h,\\ell}(\\pi)+\\frac{\\ln(1/\\delta)}{\\lambda n}\\Bigr)\\,,\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "which concludes the proof. ", "page_idx": 29}, {"type": "text", "text": "F.2 Proof of the impact of $L$ on the bound\u2019s tightness (Proposition 2) ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Proposition (Impact of $L$ on the bound\u2019s tightness). Let $\\pi\\in\\Pi$ , $\\delta\\in(0,1]$ , \u03bb > 0, L \u22651 and $h$ satisfying (C1). Let ", "page_idx": 30}, {"type": "equation", "text": "$$\n{\\cal U}_{L}^{\\lambda,h}(\\pi)=\\psi_{\\lambda}\\left(\\hat{R}_{n}^{h}(\\pi)+\\frac{\\ln(1/\\delta)}{\\lambda n}+\\sum_{\\ell=2}^{2L}\\frac{\\lambda^{\\ell-1}}{\\ell}\\hat{\\mathcal{M}}_{n}^{h,\\ell}(\\pi)\\right)\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "be the upper bound in Equation (6). Then, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\lambda\\leq\\operatorname*{min}_{i\\in[n]}\\left\\{\\frac{2L+2}{(2L+1)|h_{i}|}\\right\\}\\implies U_{L+1}^{\\lambda,h}(\\pi)\\leq U_{L}^{\\lambda,h}(\\pi)\\,.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "which implies that: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\lambda\\leq\\operatorname*{min}_{i\\in[n]}\\left\\{{\\frac{1}{|h_{i}|}}\\right\\}\\implies U_{L}^{\\lambda,h}(\\pi)\\;i s\\;a\\;d e c r e a s i n g\\;f\\!u n c t i o n\\;w.r.t\\;L.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Proof. We want to prove the implication (44) from which the condition on the decreasing nature of our bound will follow. Indeed, Let us suppose that (44) is true, we have: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lambda\\leq\\underset{i\\in[n]}{\\operatorname*{min}}\\left\\lbrace\\frac{1}{|h_{i}|}\\right\\rbrace\\implies\\forall L\\geq1,}&{\\lambda\\leq\\underset{i\\in[n]}{\\operatorname*{min}}\\left\\lbrace\\frac{2L+2}{(2L+1)|h_{i}|}\\right\\rbrace}\\\\ {\\implies\\forall L\\geq1,}&{U_{L+1}^{\\lambda,h}(\\pi)\\leq U_{L}^{\\lambda,h}(\\pi)\\quad(\\mathrm{Using}\\ (44))}\\\\ {\\implies U_{L}^{\\lambda,h}(\\pi)\\~\\mathrm{is~a~decreasing}\\ \\mathrm{function}\\ \\mathrm{w.r.t}\\ L.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Now let us prove the implication in (44). We have for any $L\\geq1$ : ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{U_{L+1}^{\\lambda,h}(\\pi)\\leq U_{L}^{\\lambda,h}(\\pi)\\iff\\displaystyle\\sum_{\\ell=2L+1}^{2L+2}\\frac{\\lambda^{\\ell-1}}{\\ell}\\hat{\\mathcal{M}}_{n}^{h,\\ell}(\\pi)\\leq0}\\\\ &{\\iff\\displaystyle\\frac{\\lambda^{2L}}{n}\\sum_{i=1}^{n}h_{i}^{2L+1}\\left(\\frac{1}{2L+1}+\\frac{\\lambda h_{i}}{2L+2}\\right)\\leq0}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "As $h_{i}\\leq0$ , we can ensure this inequality by choosing a $\\lambda$ that verifies: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\forall i\\in[n],\\quad\\lambda\\leq\\left\\{{\\frac{2L+2}{(2L+1)|h_{i}|}}\\right\\}\\iff\\lambda\\leq\\operatorname*{min}_{i\\in[n]}\\left\\{{\\frac{2L+2}{(2L+1)|h_{i}|}}\\right\\}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "which concludes the proof. ", "page_idx": 30}, {"type": "text", "text": "F.3 Comparisons of the bounds $U_{L}^{\\lambda}$ (Proposition 3) ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "We compare the bounds evaluated in their optimal regularisation function $h$ . We start by stating the proposition and proving it. ", "page_idx": 30}, {"type": "text", "text": "Proposition. Let $\\pi\\in\\Pi$ , and $\\lambda>0$ , we define: ", "page_idx": 30}, {"type": "equation", "text": "$$\nU_{L}^{\\lambda}(\\pi)=\\operatorname*{min}_{h}U_{L}^{\\lambda,h}(\\pi).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Then, for any $\\lambda>0$ , it holds that for any $L>1$ : ", "page_idx": 30}, {"type": "equation", "text": "$$\nU_{L}^{\\lambda}(\\pi)\\leq U_{1}^{\\lambda}(\\pi).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "In particular, $\\forall\\lambda>0$ : ", "page_idx": 30}, {"type": "equation", "text": "$$\nU_{\\infty}^{\\lambda}(\\pi)\\leq U_{1}^{\\lambda}(\\pi)\\,,\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Proof. Let $\\pi\\in\\Pi$ , $\\lambda>0$ and ", "page_idx": 31}, {"type": "equation", "text": "$$\nU_{L}^{\\lambda}(\\pi)=\\operatorname*{min}_{h}U_{L}^{\\lambda,h}(\\pi).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We can prove (see Appendix F.4) that: ", "page_idx": 31}, {"type": "equation", "text": "$$\nU_{1}^{\\lambda}(\\pi)=U_{1}^{\\lambda,h_{*,1}}(\\pi)=\\psi_{\\lambda}\\Bigl(\\hat{R}_{n}^{h_{*,1}}(\\pi)+\\frac{\\lambda}{2}\\hat{\\mathcal{M}}_{n}^{h_{*,1},2}(\\pi)+\\frac{\\ln(1/\\delta)}{\\lambda n}\\Bigr)\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "with: ", "page_idx": 31}, {"type": "equation", "text": "$$\nh_{*,1}(p,q,c)=-\\operatorname*{min}(|c|p/q,1/\\lambda),\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "and that (see Appendix F.7): ", "page_idx": 31}, {"type": "equation", "text": "$$\nU_{\\infty}^{\\lambda}(\\pi)=\\psi_{\\lambda}\\Bigl(\\hat{R}_{n}^{\\lambda}(\\pi)+\\frac{\\ln(1/\\delta)}{\\lambda n}\\Bigr).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "From Proposition 2, we have that for any $h$ : ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\lambda\\leq\\operatorname*{min}_{i\\in[n]}\\left\\{\\frac{1}{|h_{i}|}\\right\\}\\implies U_{L}^{\\lambda,h}(\\pi)\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "It appears that the optimal function $h_{*,1}$ respects this condition, as by definition: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{i\\in[n]}\\left\\{\\frac{1}{|(h_{*,1})_{i}|}\\right\\}\\geq\\lambda,\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "meaning that: ", "page_idx": 31}, {"type": "text", "text": "This result suggests that the Empirical Second Moment bound, evaluated in its optimal function $h_{*,1}$ , is always bigger than bounds with additional moments (evaluated in the same $h_{*,1.}$ ). This leads us to the result wanted, as for any $L>1$ : ", "page_idx": 31}, {"type": "equation", "text": "$$\nU_{L}^{\\lambda}(\\pi)=\\operatorname*{min}_{h}U_{L}^{\\lambda,h}(\\pi)\\le U_{L}^{\\lambda,h_{*,1}}(\\pi)\\le U_{1}^{\\lambda,h_{*,1}}(\\pi)=U_{1}^{\\lambda}(\\pi).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "In particular, we get: ", "page_idx": 31}, {"type": "equation", "text": "$$\nU_{\\infty}^{\\lambda}(\\pi)\\leq U_{1}^{\\lambda}(\\pi),\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "which ends the proof. ", "page_idx": 31}, {"type": "text", "text": "This means that $U_{\\infty}^{\\lambda}$ is tighter than $U_{1}^{\\lambda}$ , and thus can also be tighter than empirical Bernstein. ", "page_idx": 31}, {"type": "text", "text": "F.4 Proof of the optimality of global clipping for Corollary 4 ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Proposition (Optimal $h$ for $L=1$ ). Let $\\lambda>0$ . The function h that minimizes the bound for   \n$L=1$ , giving the tightest result is: $\\forall i,\\quad h_{i}=h(\\pi(a_{i}|x_{i}),\\pi_{0}(a_{i}|x_{i}),c_{i}))=-\\operatorname*{min}\\left\\{\\frac{\\pi(a_{i}|x_{i})}{\\pi_{0}(a_{i}|x_{i})}|c_{i}|,\\frac{1}{\\lambda}\\right\\}$   \nThis means that when the costs are binary, we obtain the classical Clipping estimator of   \nparameter $1/\\lambda$ : $h_{i}=\\operatorname*{min}\\left\\{\\frac{\\pi(a_{i}|x_{i})}{\\pi_{0}(a_{i}|x_{i})},\\frac{1}{\\lambda}\\right\\}c_{i}.$ ", "page_idx": 31}, {"type": "text", "text": "Proof. We want to look for the value of $h$ that minimizes the bound. Formally, by fixing all variables of the bound, this problem reduces to: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\operatorname*{argmin}_{h\\in({\\bf C}{\\bf I})}\\hat{R}_{n}^{h}(\\pi)+\\frac{\\lambda}{2}\\hat{\\mathcal{M}}_{n}^{h,2}(\\pi)=\\operatorname*{argmin}_{h\\in({\\bf C}{\\bf I})}\\frac{1}{n}\\sum_{i=1}^{n}\\left(h_{i}+\\frac{\\lambda}{2}h_{i}^{2}\\right).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "The objective decomposes across data points, so we can solve it for every $h_{i}$ independently. Let us fix a $j\\in[n]$ , the following problem: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{h_{j}\\in\\mathbb{R}}{\\mathrm{argmin}}\\,\\hat{R}_{n}^{h}(\\pi)+\\frac{\\lambda}{2}\\hat{\\mathcal{M}}_{n}^{h,2}(\\pi)=\\underset{h_{j}\\in\\mathbb{R}}{\\mathrm{argmin}}\\left\\{h_{j}+\\frac{\\lambda}{2}h_{j}^{2}\\right\\}}\\\\ &{\\mathrm{subject}\\,\\,\\mathrm{to}\\quad h_{j}\\geq\\frac{\\pi\\left(a_{j}|x_{j}\\right)}{\\pi_{0}\\left(a_{j}|x_{j}\\right)}c_{j}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "is strongly convex in $h_{j}$ . We write the KKT conditions for $h_{j}$ to be optimal; there exists $\\alpha^{*}$ that verifies: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{1+\\lambda h_{j}-\\alpha^{*}=0}\\\\ &{\\alpha^{*}\\geq0}\\\\ &{\\alpha^{*}\\left(\\frac{\\pi(a_{j}|x_{j})}{\\pi_{0}(a_{j}|x_{j})}c_{j}-h_{j}\\right)=0}\\\\ &{h_{j}\\geq\\frac{\\pi(a_{j}|x_{j})}{\\pi_{0}(a_{j}|x_{j})}c_{j}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "We study the two following two cases: ", "page_idx": 32}, {"type": "text", "text": "Case 1: $h_{j}\\leq-{\\frac{1}{\\lambda}}$ : we have $\\alpha^{*}=1+\\lambda h_{j}\\leq0\\implies\\alpha^{*}=0$ , meaning that: ", "page_idx": 32}, {"type": "equation", "text": "$$\nh_{j}=-{\\frac{1}{\\lambda}}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Case 2: $h_{j}>-{\\textstyle{\\frac{1}{\\lambda}}}$ : ", "page_idx": 32}, {"type": "text", "text": "we have $\\alpha^{*}=1+\\lambda h_{j}>0$ , which combined to condition (36) gives: ", "page_idx": 32}, {"type": "equation", "text": "$$\nh_{j}={\\frac{\\pi(a_{j}|{\\boldsymbol{x}}_{j})}{\\pi_{0}(a_{j}|{\\boldsymbol{x}}_{j})}}c_{j}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "The two results combined mean that we always have: ", "page_idx": 32}, {"type": "equation", "text": "$$\nh_{j}\\geq-{\\frac{1}{\\lambda}},{\\mathrm{~and~whenever~}}h_{j}>-{\\frac{1}{\\lambda}}\\implies h_{j}={\\frac{\\pi(a_{j}|x_{j})}{\\pi_{0}(a_{j}|x_{j})}}c_{j}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "We deduce that $h_{j}$ has the following form: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{h_{j}=h(\\pi(a_{j}|x_{j}),\\pi_{0}(a_{j}|x_{j}),c_{j})=-\\operatorname*{min}\\left\\{\\frac{\\pi(a_{j}|x_{j})}{\\pi_{0}(a_{j}|x_{j})}\\left|c_{j}\\right|,\\frac{1}{\\lambda}\\right\\}}\\\\ &{\\alpha^{*}=1-\\lambda\\operatorname*{min}\\left\\{\\frac{\\pi(a_{j}|x_{j})}{\\pi_{0}(a_{j}|x_{j})}\\left|c_{j}\\right|,\\frac{1}{\\lambda}\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "These values verify the KKT conditions. As the problem is strongly convex, $h_{j}$ has a unique possible value and must be equal to equation (38). The form of $h_{j}$ is a global clipping that includes the cost in the function as well. In the case where the cost function $c$ is binary: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\forall i\\quad c_{i}\\in\\{-1,0\\},\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "we recover the classical Clipping with parameter $1/\\lambda$ as an optimal solution for $h$ : ", "page_idx": 32}, {"type": "equation", "text": "$$\nh_{j}=\\operatorname*{min}\\left\\{\\frac{\\pi(a_{j}|x_{j})}{\\pi_{0}(a_{j}|x_{j})},\\frac{1}{\\lambda}\\right\\}c_{j}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "F.5 Comparison with empirical Bernstein ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "We begin by comparing the Second Moment Bound with Swaminathan and Joachims [55]\u2019s bound as they both manipulate similar quantities. The bound of [55] uses the Empirical Bernstein bound of [36] applied to the Clipping Estimator. We recall its expression below for a parameter $M>0$ : ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\hat{R}_{n}^{M}(\\pi)=\\frac{1}{n}\\sum_{i=1}^{n}\\operatorname*{min}\\left\\{\\frac{\\pi(a_{i}|x_{i})}{\\pi_{0}(a_{i}|x_{i})},M\\right\\}c_{i}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "We also give below the Empirical Bernstein Bound applied to this estimator: ", "page_idx": 33}, {"type": "text", "text": "Proposition (Empirical Bernstein for Clipping of [55]). Let $\\pi\\in\\Pi_{s}$ , $\\delta\\in(0,1]$ and $M>0$ . Then it holds with probability at least $1-\\delta$ that ", "page_idx": 33}, {"type": "equation", "text": "$$\nR(\\pi)\\le\\hat{R}_{n}^{M}(\\pi)+\\sqrt{\\frac{2\\hat{V}_{n}^{M}(\\pi)\\ln(2/\\delta)}{n}}+\\frac{7M\\ln(2/\\delta)}{3(n-1)}\\,,\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "with $\\hat{V}_{n}^{M}(\\pi)$ the empirical variance of the clipping estimator. ", "page_idx": 33}, {"type": "text", "text": "We are usually interested in the case where $\\pi$ and $\\pi_{0}$ are different, leading to substantial importance weights. In this practical scenario, the variance and the second moment are of the same magnitude of $M$ . Indeed, one can see it from the following equality: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underbrace{\\hat{V}_{n}^{M}(\\pi)}_{\\mathcal{O}(M)}=\\underbrace{\\hat{M}_{n}^{M,2}(\\pi)}_{\\mathcal{O}(M)}-\\underbrace{\\left(\\hat{R}_{n}^{M}(\\pi)\\right)^{2}}_{\\mathcal{O}(\\bar{c}^{2})}}\\\\ &{\\qquad\\quad\\approx\\underbrace{\\hat{M}_{n}^{M,2}(\\pi)}_{\\mathcal{O}(M)}\\quad(M\\gg\\bar{c}^{2}=o(1).)}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "This means that in practical scenarios, the empirical variance and the empirical second moment are approximately the same. Recall that the Second Moment Bound works for any regularizer $h$ , As Clipping satisfies (C1), we give the Second Moment Upper of Corollary 4 with Clipping below: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\psi_{\\lambda}\\Big(\\hat{R}_{n}^{M}(\\pi)+\\displaystyle\\frac{\\lambda}{2}\\hat{M}_{n}^{M,2}(\\pi)+\\frac{\\ln(1/\\delta)}{\\lambda n}\\Big)\\!}&{\\le\\hat{R}_{n}^{M}(\\pi)+\\displaystyle\\frac{\\lambda}{2}\\hat{M}_{n}^{M,2}(\\pi)+\\frac{\\ln(1/\\delta)}{\\lambda n}\\quad(\\psi_{\\lambda}(x)\\le x,\\forall x)}\\\\ &{\\le\\hat{R}_{n}^{M}(\\pi)+\\displaystyle\\frac{\\lambda}{2}\\hat{M}_{n}^{M,2}(\\pi)+\\frac{\\ln(1/\\delta)}{\\lambda n}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Choosing a $\\lambda\\approx\\sqrt{2\\ln(1/\\delta)/(n\\hat{\\mathcal{M}}_{n}^{M,2}(\\pi))}$ gives us an upper bound that is close to: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\hat{R}_{n}^{M}(\\pi)+\\frac{\\lambda}{2}\\hat{\\mathcal{M}}_{n}^{M,2}(\\pi)+\\frac{\\ln(1/\\delta)}{\\lambda n}\\approx\\hat{R}_{n}^{M}(\\pi)+\\sqrt{\\frac{2\\hat{\\mathcal{M}}_{n}^{M,2}(\\pi)\\ln(1/\\delta)}{n}}}\\\\ &{\\approx\\hat{R}_{n}^{M}(\\pi)+\\sqrt{\\frac{2\\hat{V}_{n}^{M}(\\pi)\\ln(1/\\delta)}{n}}}\\\\ &{\\le\\hat{R}_{n}^{M}(\\pi)+\\sqrt{\\frac{2\\hat{V}_{n}^{M}(\\pi)\\ln(2/\\delta)}{n}}+\\frac{7M\\ln(2/\\delta)}{3(n-1)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "This means that in practical scenarios, and with a good choice of $\\lambda\\sim{\\mathcal{O}}(1/{\\sqrt{n}})$ , the Second Moment bound would be better than the Empirical Bernstein bound, and this difference will be even greater when $M\\gg1$ . This is aligned with our experiments, where we see that the new Second Moment bound is much tighter in practice. This also confirms that the Logarithmic smoothing bound is even tighter, because it is smaller than the Second Moment bound as stated in Proposition 3. ", "page_idx": 33}, {"type": "text", "text": "Proposition (Empirical Logarithmic Smoothing bound with $L\\rightarrow\\infty$ ). Let $\\pi\\in\\Pi$ , $\\delta\\in(0,1]$ and $\\lambda>0$ . Then it holds with probability at least $1-\\delta$ that ", "page_idx": 34}, {"type": "equation", "text": "$$\nR(\\pi)\\le\\psi_{\\lambda}\\Bigl(-\\,\\frac{1}{n}\\sum_{i=1}^{n}\\frac{1}{\\lambda}\\log\\left(1-\\lambda h_{i}\\right)+\\frac{\\ln(1/\\delta)}{\\lambda n}\\Bigr)\\,.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Taking the limit of $L$ naively recovers this form of the bound, but imposes a condition on $\\lambda$ for the bound to converge. We instead, take another path of proof that does not impose any condition on $\\lambda$ , developed below. The main idea is to take the limit of $L$ to recover the variable to use along Chernoff. ", "page_idx": 34}, {"type": "text", "text": "Proof. Recall that for the proof of the Empirical moments bounds, we used the following random variable defined with $\\lambda>0$ : ", "page_idx": 34}, {"type": "equation", "text": "$$\nX_{i}=-\\frac{1}{\\lambda}\\log\\left({1-\\lambda\\mathbb{E}\\left[h\\right]}\\right)-h_{i}-\\sum_{\\ell=2}^{2L}\\frac{1}{\\ell}(\\lambda h_{i})^{\\ell},\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "combined with Chernoff Inequality (Lemma 12) to prove our bound. If we take the limit $L\\rightarrow\\infty$ for our random variable, we obtain the following random variable: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\tilde{X}_{i}=-\\frac{1}{\\lambda}\\log\\left(1-\\lambda\\mathbb{E}\\left[h\\right]\\right)+\\frac{1}{\\lambda}\\log\\left(1-\\lambda h_{i}\\right)}}\\\\ {{\\displaystyle=\\frac{1}{\\lambda}\\log\\left(\\frac{1-\\lambda h_{i}}{1-\\lambda\\mathbb{E}\\left[h\\right]}\\right).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "We use the random variable $\\tilde{X}_{i}$ with the Chernoff Inequality. For any $a\\in\\mathbb{R}$ , we have: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P\\left(\\underset{i\\in[n]}{\\sum}\\tilde{X}_{i}>a\\right)\\leq\\left(\\mathbb{E}\\left[\\exp\\left(\\lambda\\tilde{X}_{1}\\right)\\right]\\right)^{n}\\exp(-\\lambda a)}\\\\ &{P\\left(-\\frac{n}{\\lambda}\\log\\left(1-\\lambda\\mathbb{E}\\left[h\\right]\\right)+\\underset{i\\in[n]}{\\sum}\\left(\\frac{1}{\\lambda}\\log\\left(1-\\lambda h_{i}\\right)\\right)>a\\right)\\leq\\left(\\mathbb{E}\\left[\\exp\\left(\\lambda\\tilde{X}_{1}\\right)\\right]\\right)^{n}\\exp(-\\lambda a)}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "On the other hand, we have: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\exp\\left(\\lambda\\tilde{X}_{1}\\right)\\right]=\\frac{\\mathbb{E}\\left[1-\\lambda h_{i}\\right]}{1-\\lambda\\mathbb{E}\\left[h\\right]}=1.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Using this equality and solving for $\\delta=\\exp(-\\lambda a)$ , we get: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{l}{P\\left(\\displaystyle-\\frac{n}{\\lambda}\\log\\left(1-\\lambda\\mathbb{E}\\left[h\\right]\\right)+\\displaystyle\\sum_{i\\in[n]}\\left(\\frac{1}{\\lambda}\\log\\left(1-\\lambda h_{i}\\right)\\right)>\\frac{\\ln(1/\\delta)}{\\lambda}\\right)\\leq\\delta}\\\\ {P\\left(\\displaystyle-\\frac{1}{\\lambda}\\log\\left(1-\\lambda\\mathbb{E}\\left[h\\right]\\right)+\\frac{1}{n}\\sum_{i\\in[n]}\\frac{1}{\\lambda}\\log\\left(1-\\lambda h_{i}\\right)>\\frac{\\ln\\left(1/\\delta\\right)}{\\lambda n}\\right)\\leq\\delta}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "This means that the following, complementary event will hold with probability at least $1-\\delta$ : ", "page_idx": 34}, {"type": "equation", "text": "$$\n-\\frac{1}{\\lambda}\\log\\left(1-\\lambda\\mathbb{E}\\left[h\\right]\\right)\\leq-\\frac{1}{n}\\sum_{i=1}^{n}\\frac{1}{\\lambda}\\log\\left(1-\\lambda h_{i}\\right)+\\frac{\\ln(1/\\delta)}{\\lambda n}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "$\\psi_{\\lambda}$ being a non-decreasing function, applying it to the two sides of this inequality gives us: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[h\\right]\\leq\\psi_{\\lambda}\\Big(-\\frac{1}{n}\\sum_{i=1}^{n}\\frac{1}{\\lambda}\\log\\left(1-\\lambda h_{i}\\right)+\\frac{\\ln(1/\\delta)}{\\lambda n}\\Big).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "As $h$ satisfies (C1), we obtain the required inequality: ", "page_idx": 34}, {"type": "equation", "text": "$$\nR(\\pi)\\leq\\psi_{\\lambda}{\\Big(}-{\\frac{1}{n}}\\sum_{i=1}^{n}{\\frac{1}{\\lambda}}\\log{(1-\\lambda h_{i})}+{\\frac{\\ln(1/\\delta)}{\\lambda n}}{\\Big)}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "and conclude the proof. ", "page_idx": 34}, {"type": "text", "text": "F.7 Proof of the optimality of IPS for Corollary 5 ", "text_level": 1, "page_idx": 35}, {"type": "table", "img_path": "zLClygeRK8/tmp/c8e9f365f0b1d0111fa0a7b3e9a5cdad4c077f0b6cf9982fda34262150389b1d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 35}, {"type": "text", "text": "Proof. The proof of this proposition is quite simple. The function: ", "page_idx": 35}, {"type": "equation", "text": "$$\nf(x)=-\\log\\left(1-\\lambda x\\right)\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "is increasing. This means that the lowest possible value of $h_{i}$ ensures the tightest result. As our variables $h_{i}$ verifies (C1), we recover IPS as an optimal choice for this bound. \u53e3 ", "page_idx": 35}, {"type": "text", "text": "F.8 Comparison with the IX bound (Proposition 8) ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "We now attack the recently derived IX bound in Gabbianelli et al. [21] and show that our newly proposed bound dominates it in all scenarios. ", "page_idx": 35}, {"type": "text", "text": "Proposition (Comparison with IX [21]). Let $\\pi\\in\\Pi$ , $\\delta\\in]0,1]$ and $\\lambda>0$ , the IX bound from [21] states that we have with at least probability $1-\\delta$ : ", "page_idx": 35}, {"type": "equation", "text": "$$\nR(\\pi)\\leq\\hat{R}_{n}^{\\lambda\\scriptscriptstyle{-}\\mathrm{IX}}(\\pi)+\\frac{\\ln(1/\\delta)}{\\lambda n}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "with: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\hat{R}_{n}^{\\lambda\\mathrm{-IX}}(\\pi)=\\frac{1}{n}\\sum_{i=1}^{n}\\frac{\\pi(a_{i}|x_{i})}{\\pi_{0}(a_{i}|x_{i})+\\lambda/2}c_{i}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Let $U_{\\mathrm{IX}}^{\\lambda}(\\pi)$ be the $I X$ upper bound defined above, we have for any $\\lambda>0$ : ", "page_idx": 35}, {"type": "equation", "text": "$$\nU_{\\infty}^{\\lambda}(\\pi)\\leq U_{\\mathrm{IX}}^{\\lambda}(\\pi)\\,.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Proof. Let $\\pi\\in\\Pi,\\delta\\in]0,1]$ and $\\lambda>0$ . Recall that $\\begin{array}{r}{U_{\\infty}^{\\lambda}(\\pi)=\\psi_{\\lambda}\\Big(\\hat{R}_{n}^{\\lambda}(\\pi)+\\frac{\\ln(1/\\delta)}{\\lambda n}\\Big)}\\end{array}$ . We have: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\psi_{\\lambda}\\Big(\\hat{R}_{n}^{\\lambda}(\\pi)+\\frac{\\ln(1/\\delta)}{\\lambda n}\\Big)\\leq\\hat{R}_{n}^{\\lambda}(\\pi)+\\frac{\\ln(1/\\delta)}{\\lambda n}\\quad(\\forall x,\\psi_{\\lambda}(x)\\leq x)}\\\\ {\\displaystyle\\qquad\\qquad\\qquad\\leq-\\frac{1}{n}\\sum_{i=1}^{n}\\frac{1}{\\lambda}\\log\\big(1-\\lambda w_{\\pi}(x_{i},a_{i})c_{i}\\big)+\\frac{\\ln(1/\\delta)}{\\lambda n}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Using the inequality log(1 + x) \u22651+xx/2 for all $x>0$ , we get: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{U_{\\infty}^{\\lambda}(\\pi)\\leq-\\frac{1}{n}\\sum_{i=1}^{n}\\frac{1}{\\lambda}\\log\\left(1-\\lambda w_{\\pi}(x_{i},a_{i})c_{i}\\right)+\\frac{\\ln(1/\\delta)}{\\lambda n}}}\\\\ &{\\leq\\frac{1}{n}\\sum_{i=1}^{n}\\frac{w_{\\pi}(x_{i},a_{i})}{1-\\lambda w_{\\pi}(x_{i},a_{i})c_{i}/2}c_{i}+\\frac{\\ln(1/\\delta)}{\\lambda n}\\;\\;\\;\\left(\\log(1+x)\\geq\\frac{x}{1+x/2}\\right)}\\\\ &{\\leq\\frac{1}{n}\\frac{n}{i-1}\\frac{\\pi}{\\pi_{0}(a_{i}|x_{i})-\\lambda\\pi(a_{i}|x_{i})c_{i}/2}c_{i}+\\frac{\\ln(1/\\delta)}{\\lambda n}}\\\\ &{\\leq\\frac{1}{n}\\sum_{i=1}^{n}\\frac{\\pi(a_{i}|x_{i})}{\\pi_{0}(a_{i}|x_{i})+\\lambda/2}c_{i}+\\frac{\\ln(1/\\delta)}{\\lambda n}\\;\\;\\;(-\\pi(a_{i}|x_{i})c_{i}\\leq1\\mathrm{~and~}c_{i}\\leq0)}\\\\ &{\\leq\\hat{R}_{n}^{I X-\\lambda}(\\pi)+\\frac{\\ln(1/\\delta)}{\\lambda n}=U_{L X}^{\\lambda}(\\pi),}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "which ends the proof. ", "page_idx": 35}, {"type": "text", "text": "The result states the dominance of the LS bound compared to IX. The proof of this result also gives us insight on when the LS bound will be much tighter than IX. Indeed, to obtain the IX bound, LS bound is loosened through 3 steps: ", "page_idx": 36}, {"type": "text", "text": "The two first inequalities are loose when $\\lambda\\sim1/\\sqrt{n}$ is not too small, which means that LS will be much better in problems with few samples. The third inequality is loose when $\\pi$ is not a peaked policy or the cost is way less than 1. Even if LS bound is always smaller than IX, LS will give way better result if the number of samples is small, and/or the policy evaluated is diffused. ", "page_idx": 36}, {"type": "text", "text": "G Proofs of OPS and OPL ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "G.1 OPS: Proof of suboptimality bound (Proposition 9) ", "text_level": 1, "page_idx": 36}, {"type": "equation", "text": "$$\n0\\leq R(\\hat{\\pi}_{n}^{s})-R(\\pi_{*}^{s})\\leq\\lambda S_{\\lambda}(\\pi_{*}^{s})+\\frac{2\\ln(2|\\Pi_{s}|/\\delta)}{\\lambda n}\\,,\n$$", "text_format": "latex", "page_idx": 36}, {"type": "equation", "text": "$$\nS_{\\lambda}(\\pi)=\\mathbb{E}\\left[(w_{\\pi}(x,a)c)^{2}/\\left(1-\\lambda w_{\\pi}(x,a)c\\right)\\right].\n$$", "text_format": "latex", "page_idx": 36}, {"type": "equation", "text": "$$\n\\lambda S_{\\lambda}(\\pi)=\\lambda\\mathbb{E}\\left[\\frac{(w_{\\pi}(x,a)c)^{2}}{1-\\lambda w_{\\pi}(x,a)c}\\right]\\leq\\operatorname*{min}\\left\\{|R(\\pi)|,\\lambda\\mathbb{E}\\left[(w_{\\pi}(x,a)c)^{2}\\right]\\right\\}\\leq|R(\\pi)|.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Proof. To prove this bound on the suboptimality of our selection method, we need both an upper bound and a lower bound on the true risk using the LS estimator. Luckily, we already have derived them in ?? . For a fixed $\\lambda$ , taking a union of the two bounds over the cardinal of the finite policy class $\\left|\\Pi_{s}\\right|$ , we get the following holding with probability at least $1-\\delta$ for all $\\pi\\in\\Pi_{s}$ : ", "page_idx": 36}, {"type": "equation", "text": "$$\nR(\\pi)-\\hat{R}_{n}^{\\lambda}(\\pi)\\leq{\\frac{\\ln(2|\\Pi_{s}|/\\delta)}{\\lambda n}}\\,,\\qquad{\\mathrm{and}}\\qquad\\hat{R}_{n}^{\\lambda}(\\pi)-R(\\pi)\\leq\\lambda S_{\\lambda}(\\pi)+{\\frac{\\ln(2|\\Pi_{s}|/\\delta)}{\\lambda n}}\\,.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "As $\\hat{\\pi}_{n}^{\\mathrm{s}}\\in\\Pi_{s}$ and by definition of $\\hat{\\pi}_{n}^{\\mathrm{s}}$ (minimizer of $\\hat{R}_{n}^{\\lambda}(\\pi)\\rangle$ ), we have: ", "page_idx": 36}, {"type": "equation", "text": "$$\nR(\\hat{\\pi}_{n}^{\\mathrm{s}})\\leq\\hat{R}_{n}^{\\lambda}(\\hat{\\pi}_{n}^{\\mathrm{s}})+\\frac{\\ln(2|\\Pi_{s}|/\\delta)}{\\lambda n}\\leq\\hat{R}_{n}^{\\lambda}(\\hat{\\pi}_{*}^{\\mathrm{s}})+\\frac{\\ln(2|\\Pi_{s}|/\\delta)}{\\lambda n}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Using the lower bound on the risk of $R(\\hat{\\pi}_{*}^{\\mathrm{s}})$ , we have: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R(\\hat{\\pi}_{n}^{\\mathrm{s}})\\leq\\hat{R}_{n}^{\\lambda}(\\hat{\\pi}_{*}^{\\mathrm{s}})+\\frac{\\ln(2|\\Pi_{s}|/\\delta)}{\\lambda n}}\\\\ &{\\qquad\\quad\\leq R(\\hat{\\pi}_{*}^{\\mathrm{s}})+\\lambda S_{\\lambda}(\\hat{\\pi}_{*}^{\\mathrm{s}})+\\frac{2\\ln(2|\\Pi_{s}|/\\delta)}{\\lambda n}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "which gives us the suboptimality upper bound: ", "page_idx": 36}, {"type": "equation", "text": "$$\n0\\leq R(\\hat{\\pi}_{n}^{s})-R(\\pi_{*}^{s})\\leq\\lambda S_{\\lambda}(\\pi_{*}^{s})+\\frac{2\\ln(2|\\Pi_{s}|/\\delta)}{\\lambda n}\\,.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Note that: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\lambda S_{\\lambda}(\\pi)=\\lambda\\mathbb{E}\\left[\\frac{(w_{\\pi}(x,a)c)^{2}}{1-\\lambda w_{\\pi}(x,a)c}\\right]\\leq\\operatorname*{min}\\left\\{|R(\\pi)|,\\lambda\\mathbb{E}\\left[(w_{\\pi}(x,a)c)^{2}\\right]\\right\\},\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "always ensuring a finite bound. ", "page_idx": 36}, {"type": "text", "text": "G.2 OPL: Proof of PAC-Bayesian LS-LIN bound (Proposition 10) ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Proposition (PAC-Bayes learning bound for $\\hat{R}_{n}^{\\lambda-\\mathrm{LIN}}.$ ). Given a prior $P\\in\\mathcal P(\\Theta)$ , $\\delta\\in(0,1]$ and $\\lambda>0$ , the following holds with probability at least $1-\\delta$ : ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\forall Q\\in\\mathcal{P}(\\Theta),\\quad R(\\pi_{Q})\\leq\\psi_{\\lambda}\\left(\\hat{R}_{n}^{\\lambda-\\mathrm{LIN}}(\\pi_{Q})+\\frac{K\\mathcal{L}(Q||P)+\\ln\\frac{1}{\\delta}}{\\lambda n}\\right)\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Proof. To prove this proposition, we can either take the path of High Order Empirical moments as for Pessimistic OPE, or we can prove it directly. We provide here a simple proof of this proposition using ideas from Alquier [1, Corollary 2.5]. Let: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r}{d_{\\theta}(a|x)=\\mathbb{1}\\left[f_{\\theta}(x)=a\\right]\\,,\\forall(x,a)\\in\\mathcal{X}\\times\\mathcal{A}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "it means that: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\pi_{Q}(a|x)=\\mathbb{E}_{\\theta\\sim Q}\\left[d_{\\theta}(a|x)\\right]\\,,\\forall(x,a)\\in\\mathcal{X}\\times\\mathcal{A}\\,.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Recall that to prove a PAC-Bayesian generalization bound, one can rely on the Change of measure Lemma (Lemma 14). For any $\\lambda>0$ , the adequate function $g$ to consider is: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{g(\\theta,\\mathcal{D}_{n})=\\displaystyle\\sum_{i=1}^{n}\\left(-\\log(1-\\lambda R(d_{\\theta}))+\\log\\left(1-\\lambda\\frac{d_{\\theta}(a_{i}|x_{i})c_{i}}{\\pi_{0}(a_{i}|x_{i})}\\right)\\right)}\\\\ &{\\quad\\quad\\quad=\\displaystyle\\sum_{i=1}^{n}\\log\\left(\\frac{1-\\lambda\\frac{d_{\\theta}(a_{i}|x_{i})c_{i}}{\\pi_{0}(a_{i}|x_{i})}}{1-\\lambda R(d_{\\theta})}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "By exploiting the i.i.d. nature of the data and exchanging the order of expectations $P$ is independent of $\\mathcal{D}_{n}$ ), we can naturally prove that: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Psi_{g}=\\mathbb{E}_{P}\\left[\\underset{i=1}{\\overset{n}{\\prod}}\\mathbb{E}\\left[\\exp\\left(\\log\\left(\\frac{1-\\lambda\\frac{d_{\\theta}\\left(a_{i}|x_{i}\\right)c_{i}}{\\pi_{0}\\left(a_{i}|x_{i}\\right)}}{1-\\lambda R\\left(d_{\\theta}\\right)}\\right)\\right)\\right]\\right]}\\\\ &{\\quad=\\mathbb{E}_{P}\\left[\\underset{i=1}{\\overset{n}{\\prod}}\\mathbb{E}\\left[\\frac{1-\\lambda\\frac{d_{\\theta}\\left(a_{i}|x_{i}\\right)c_{i}}{\\pi_{0}\\left(a_{i}|x_{i}\\right)}}{1-\\lambda R\\left(d_{\\theta}\\right)}\\right]\\right]}\\\\ &{\\quad=\\mathbb{E}_{P}\\left[\\underset{i=1}{\\overset{n}{\\prod}}\\frac{1-\\lambda R\\left(d_{\\theta}\\right)}{1-\\lambda R\\left(d_{\\theta}\\right)}\\right]=1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Injecting $\\Psi_{g}$ in Lemma 14, gives: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{L}_{\\theta\\sim Q}\\left[-\\log(1-\\lambda R(d_{\\theta})\\right]\\leq\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}_{\\theta\\sim Q}\\left[-\\log\\left(1-\\lambda\\frac{d_{\\theta}(a_{i}|x_{i})c_{i}}{\\pi_{0}(a_{i}|x_{i})}\\right)\\right]+\\frac{K\\mathcal{L}(Q||P)+\\ln\\frac{1}{\\delta}}{n}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}_{\\theta\\sim Q}\\left[-d_{\\theta}(a_{i}|x_{i})\\log\\left(1-\\lambda\\frac{c_{i}}{\\pi_{0}(a_{i}|x_{i})}\\right)\\right]+\\frac{K\\mathcal{L}(Q||P)+\\ln\\frac{1}{\\delta}}{n}}\\\\ &{\\qquad\\qquad\\qquad\\leq-\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\pi_{Q}(a_{i}|x_{i})\\log\\left(1-\\lambda\\frac{c_{i}}{\\pi_{0}(a_{i}|x_{i})}\\right)+\\frac{K\\mathcal{L}(Q||P)+\\ln\\frac{1}{\\delta}}{n}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\lambda\\hat{R}_{n}^{\\lambda-\\mathrm{LN}}(\\pi_{Q})+\\frac{K\\mathcal{L}(Q||P)+\\ln\\frac{1}{\\delta}}{n}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "From the convexity of $x\\to-\\log(1+x)$ , we have: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r}{-\\frac{1}{\\lambda}\\log\\left(1-\\lambda R(\\pi_{Q})\\right)\\leq\\frac{1}{\\lambda}\\mathbb{E}_{\\theta\\sim Q}\\left[-\\log(1-\\lambda R(d_{\\theta})\\right]\\leq\\hat{R}_{n}^{\\lambda-\\mathrm{LIN}}(\\pi_{Q})+\\frac{K\\mathcal{L}(Q||P)+\\ln\\frac{1}{\\delta}}{\\lambda n}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Applying the increasing function $\\psi_{\\lambda}$ of Equation (5) to both sides concludes the proof. ", "page_idx": 37}, {"type": "text", "text": "Proposition (Suboptimality of the learning strategy in (27)). Let $\\lambda>0$ , $P\\,\\in\\,{\\mathcal{L}}(\\Theta)$ and $\\delta\\in\\bar{(0,1]}$ . Then, it holds with probability at least $1-\\delta$ that ", "page_idx": 38}, {"type": "equation", "text": "$$\n0\\le R(\\hat{\\pi}_{Q_{n}})-R(\\pi_{Q^{*}})\\leq\\lambda S_{\\lambda}^{\\mathrm{LIN}}(\\pi_{Q^{*}})+\\frac{2\\left(K\\mathcal{L}(Q^{*}||P)+\\ln(2/\\delta)\\right)}{\\lambda n},\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where ", "page_idx": 38}, {"type": "equation", "text": "$$\nS_{\\lambda}^{\\mathrm{LIN}}(\\pi)=\\mathbb{E}\\left[\\frac{\\pi(a|x)c^{2}}{\\pi_{0}^{2}(a|x)-\\lambda\\pi_{0}(a|x)c}\\right].\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "In addition, our upper bound is always finite as: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\lambda S_{\\lambda}^{\\mathrm{LIN}}(\\pi)\\leq\\operatorname*{min}\\left\\{|R(\\pi)|,\\lambda{\\mathbb E}\\left[\\frac{\\pi(a|x)c^{2}}{\\pi_{0}^{2}(a|x)}\\right]\\right\\}\\leq|R(\\pi)|.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Proof. To prove this bound on the suboptimality of our learning strategy, we need both a PACBayesian upper bound and a lower bound on the true risk using the LS-LIN estimator. Luckily, we already have derived an upper bound in Proposition 10, that we linearize here as $\\psi_{\\lambda}(x)\\leq x$ : ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\forall Q\\in\\mathcal{P}(\\Theta),\\quad R(\\pi_{Q})\\leq\\hat{R}_{n}^{\\lambda-\\mathrm{LIN}}(\\pi_{Q})+\\frac{\\mathcal{K L}(Q||P)+\\ln\\frac{1}{\\delta}}{\\lambda n}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "For the lower bound, we rely a second time on the Change of measure Lemma (Lemma 14). For any $\\lambda>0$ , we choose the following function $g$ : ", "page_idx": 38}, {"type": "equation", "text": "$$\ng(\\theta,\\mathcal{D}_{n})=\\sum_{i=1}^{n}\\left(-\\frac{1}{\\lambda}\\log\\left(1-\\lambda\\frac{d_{\\theta}(a_{i}\\vert x_{i})c_{i}}{\\pi_{0}(a_{i}\\vert x_{i})}\\right)-R(d_{\\theta})-\\lambda S_{\\lambda}^{\\mathrm{LIN}}(d_{\\theta})\\right).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "By exploiting the i.i.d. nature of the data and exchanging the order of expectations $P$ is independent of $\\mathcal{D}_{n}$ ), we can prove that: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Psi_{\\vartheta}=\\mathbb{E}_{P}\\left[\\frac{\\tilde{\\Pi}}{\\prod_{t=1}^{n}}\\left(\\exp\\left(-\\lambda(R(d_{\\theta})+\\lambda S_{\\Lambda}^{\\mathrm{tl},\\mathrm{n}}(d_{\\theta}))\\right)\\mathbb{E}\\left[\\frac{1}{1-\\lambda\\frac{d_{\\theta}(d_{\\theta})^{\\top}}{\\theta_{0}(1)}}\\right]\\right)\\right]}\\\\ &{\\leq\\mathbb{E}_{P}\\left[\\frac{\\tilde{\\Pi}}{\\prod_{t=1}^{n}}\\left(\\exp\\left(-\\lambda(R(d_{\\theta})+\\lambda S_{\\Lambda}^{\\mathrm{tl},\\mathrm{n}}(d_{\\theta}))+\\mathbb{E}\\left[\\frac{1}{1-\\lambda\\frac{d_{\\theta}(d_{\\theta})^{\\top}}{\\theta_{0}(1)}}\\right]-1\\right)\\right)\\right]}\\\\ &{\\leq\\mathbb{E}_{P}\\left[\\frac{\\tilde{\\Pi}}{\\prod_{t=1}^{n}}\\left(\\exp\\left(-\\lambda(R(d_{\\theta})+\\lambda S_{\\Lambda}^{\\mathrm{tl},\\mathrm{n}}(d_{\\theta}))+\\mathbb{E}\\left[\\frac{M_{\\theta}(d_{\\theta})-\\lambda}{|U_{\\theta}|_{\\theta}|_{\\theta}^{\\top}}\\right]\\right)\\right)\\right]}\\\\ &{\\leq\\mathbb{E}_{P}\\left[\\frac{\\tilde{\\Pi}}{\\prod_{t=1}^{n}}\\left(\\exp\\left(-\\lambda(R(d_{\\theta})+\\lambda S_{\\Lambda}^{\\mathrm{tl},\\mathrm{n}}(d_{\\theta}))+\\mathbb{E}\\left[\\frac{M_{\\theta}(d_{\\theta})-\\lambda}{|U_{\\theta}|_{\\theta}|_{\\theta}^{\\top}}\\right]\\right)\\right)\\right]}\\\\ &{\\leq\\mathbb{E}_{P}\\left[\\frac{\\tilde{\\Pi}}{\\prod_{t=1}^{n}}\\left(\\exp\\left(-\\lambda(R(d_{\\theta})+\\lambda S_{\\Lambda}^{\\mathrm{tl},\\mathrm{n}}(d_{\\theta}))+\\mathbb{E}\\left[\\frac{M_{\\theta}(d_{\\theta})-\\lambda}{|U_{\\theta}|_{\\theta}|_{\\theta}^{\\top}}\\right]\\right)\\right)\\right]\\,\\mathrm{d}_{\\theta}\\,\\mathrm{binary},}\\\\ &{\\leq\\mathbb{E}_{P}\\left[\\frac{\\tilde{\\Pi}}{\\prod_{t=1}^{n}}\\left(\\exp\\left(-\\lambda^{2}S_{\\Lambda}^{\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "giving by rearranging terms, the following PAC-Bayesian bound: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\forall Q\\in\\mathcal{P}(\\Theta),\\quad\\hat{R}_{n}^{\\lambda-\\mathrm{LIN}}(\\pi_{Q})\\leq R(\\pi_{Q})+\\lambda S_{\\lambda}^{\\mathrm{LIN}}(\\pi_{Q})+\\frac{K\\mathcal{L}(Q||P)+\\ln(2/\\delta)}{\\lambda n}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Now we take a union of the the two bounds, for them to hold with probability at least $1-\\delta$ for all $Q$ . By definition of $\\hat{\\pi}_{Q_{n}}$ (minimizer of the upper bound), we have: ", "page_idx": 38}, {"type": "equation", "text": "$$\nR(\\hat{\\pi}_{Q_{n}})\\leq\\hat{R}_{n}^{\\lambda-\\mathrm{LIN}}(\\hat{\\pi}_{Q_{n}})+\\frac{K\\mathcal{L}(Q_{n}||P)+\\ln(2/\\delta)}{\\lambda n}\\leq\\hat{R}_{n}^{\\lambda-\\mathrm{LIN}}(\\pi_{Q^{*}})+\\frac{K\\mathcal{L}(Q^{*}||P)+\\ln(2/\\delta)}{\\lambda n}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Using the lower bound on the risk of $R(\\pi_{Q^{*}})$ , we have: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R(\\hat{\\pi}_{Q_{n}})\\leq\\hat{R}_{n}^{\\lambda-\\mathrm{LIN}}(\\pi_{Q^{*}})+\\frac{K\\mathcal{L}(Q^{*}||P)+\\ln(2/\\delta)}{\\lambda n}}\\\\ &{\\qquad\\qquad\\leq R(\\pi_{Q^{*}})+\\lambda S_{\\lambda}^{\\mathrm{LIN}}(\\pi_{Q^{*}})+\\frac{K\\mathcal{L}(Q^{*}||P)+\\ln(2/\\delta)}{\\lambda n}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "which gives us the PAC-Bayesian suboptimality upper bound: ", "page_idx": 39}, {"type": "equation", "text": "$$\n0\\le R(\\hat{\\pi}_{Q_{n}})-R(\\pi_{Q^{*}})\\leq\\lambda S_{\\lambda}^{\\mathrm{LIN}}(\\pi_{Q^{*}})+\\frac{2\\left(K\\mathcal{L}(Q^{*}||P)+\\ln(2/\\delta)\\right)}{\\lambda n}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Concluding the proof. ", "page_idx": 39}, {"type": "text", "text": "H Experimental design and detailed experiments ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "All our experiments were conducted on a machine with 16 CPUs. The PAC-Bayesian learning experiments require a moderate amount of computation due to the handling of medium-sized datasets. However, our experiments remain reproducible with minimal computational resources. ", "page_idx": 39}, {"type": "text", "text": "H.1 Off-policy evaluation and selection ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "H.1.1 Datasets ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "For both our OPE and OPS experiments, we use 11 UCI datasets with different sizes, action spaces and number of features. The statistics of all these datasets are described in Table 3. ", "page_idx": 39}, {"type": "table", "img_path": "zLClygeRK8/tmp/7cc2f7f89de188b30017cb89721af8b42955650c79cdd9aced4de529511f7423.jpg", "table_caption": ["Table 3: OPE and OPS: 11 Datasets used from OpenML [8]. "], "table_footnote": [], "page_idx": 39}, {"type": "text", "text": "H.1.2 (OPE) Tightness of the bounds ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Additional details. For these experiments, as we only use oracle policies (faulty policies to log data and we evaluate ideal policies), we use the full 11 datasets without splitting them. The faulty policies are defined exactly as described in the experiments of Kuzborskij et al. [32]. For each datapoint, the behavior (faulty) policy plays an action and we record a cost. The triplets datapoint, action and cost constitute our logged bandit dataset, with which we can compute our estimates and bounds. As we have access to the true label, the original dataset can be used to compute the true risk of any policy. ", "page_idx": 39}, {"type": "text", "text": "Detailed results. Evaluating the worst case performance of a policy is done through evaluating risk upper bounds [10, 32]. This means that a better evaluation will solely depend on the tightness of the bounds used. To this end, given a policy $\\pi$ , we are interested in bounds with a small relative radius $|U(\\pi)/R(\\pi)\\!-\\!1|$ . We compare our newly derived bounds $\\scriptstyle\\mathrm{cIPS-L=1}$ for $U_{1}^{\\lambda}$ and LS for $U_{\\infty}^{\\lambda}$ both with $\\lambda=1/\\sqrt{n})$ to SNIPS-ES: the Efron Stein bound for Self Normalized IPS [32], cIPS-EB: Empirical Bernstein for Clipping [55] and the recent IX: Implicit Exploration bound [21]. We use all 11 datasets, with different behavior policies $(\\tau_{0}\\in\\{0.2,0.25,0.3\\})$ and different noise levels $(\\epsilon\\in\\{0.,0.1,0.2\\})$ to evaluate ideal policies with different temperatures $(\\tau\\in\\{0.1,0.2,0.3,0.4,0.5\\})$ ), defining $\\sim500$ different scenarios to validate our findings. In addition to the cumulative distribution of the relative radius of the considered bounds of Figure 2. We give two tables in the following: the average relative radius of our bounds for each dataset, compiled in Table 4, and the average relative radius of our bounds for each policy evaluated, compiled in Table 5. One can observe that LS always gives the best results no matter the projection. However, the ${\\mathsf{c I P S-L}}{=}1$ bound is sometimes better than IX, especially when it comes to evaluating diffused policies, see Table 5. ", "page_idx": 39}, {"type": "table", "img_path": "zLClygeRK8/tmp/6abb577ddcf3d10d1eca41837dee9b02d7768dc4d004b3c6163f0d68df2ecaae.jpg", "table_caption": ["Table 4: OPE: Average relative radius for each datasets "], "table_footnote": [], "page_idx": 40}, {"type": "text", "text": "", "page_idx": 40}, {"type": "table", "img_path": "zLClygeRK8/tmp/7a376d29ecd93331ab5276312d7801d0062663ef13e476b8d2400ca053c63c85.jpg", "table_caption": ["Table 5: OPE: Average relative radiuses for each target policies (ideal policies with different $\\tau$ ) "], "table_footnote": [], "page_idx": 40}, {"type": "text", "text": "H.1.3 (OPS) Find the best, avoid the worst policy ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Policy selection aims at identifying the best policy among a set of finite candidates. In practice, we are interested in finding policies that improve on $\\pi_{0}$ and avoid policies that perform worse than $\\pi_{0}$ . To replicate real world scenarios, we design an experiment where $\\pi_{0}$ is a faulty policy $(\\tau=0.2)$ ), that collects noisy $(\\epsilon=0.2)$ ) interaction data, some of which is used to learn $\\pi_{\\theta^{\\mathrm{IPS}}}$ , $\\pi_{\\theta^{\\mathrm{SI}}}$ , and that we add to our discrete set of policies $\\Pi_{k=4}=\\{\\pi_{0},\\pi^{\\mathrm{ideal}},\\pi_{\\theta^{\\mathrm{IPS}}},\\pi_{\\theta^{\\mathrm{sn}}}\\}$ . The splits for these experiments are the following: $70\\%$ of the data is used to create bandit feedback ( $20\\%$ is used to train $\\pi_{\\theta^{\\mathrm{IPS}}}$ , $\\pi_{\\theta^{\\mathrm{SI}}}$ and $50\\%$ is used to evaluate policies based on estimators/upper bounds.) the rest is used to evaluate the true value of the policies. The goal is to measure the ability of our selection strategies to choose from $\\Pi_{k=4}$ , better performing policies than $\\pi_{0}$ . We thus define three possible outcomes: a strategy can select worse performing policies, better performing or the best policy. We compare selection strategies based on upper bounds to the commonly used estimator\u221as IPS and SNIPS. The hyperparameters of all bounds (the clipping parameter $M$ and $\\lambda$ ) are set to $1/\\sqrt{n}$ . The comparison is conducted on the 11 datasets with 10 different seeds resulting in 110 scenarios. In addition to the plot in Figure 2, we collect the number of times each method selected the best policy $(\\pi_{*}^{\\mathrm{s}})$ , a better $({\\bf{B}})$ or a worse (W) policy than $\\pi_{0}$ for all datasets in Table 6. We can see that risk estimators can be unreliable, especially in small sample datasets, as they can choose worse performing policies than $\\pi_{0}$ , a catastrophic outcome in highly sensitive applications. Selecting policies based on upper bounds is more conservative, as it avoids completely poor performing policies. In addition, the tighter the bound, the better its percentage of time it selects the best policy: LS upper bound is less conservative and can find best policies more than any other bound, while never selecting poor performing policies. ", "page_idx": 40}, {"type": "table", "img_path": "zLClygeRK8/tmp/a499585139aed8e7d30b9c29d14d78728797f530751a9ae97a36b7d727772ec3.jpg", "table_caption": ["Table 6: OPS: Number of times the worst, better or best policy was selected for each dataset. "], "table_footnote": [], "page_idx": 41}, {"type": "text", "text": "H.2 Off-policy learning ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "H.2.1 Datasets ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "As described in the experiments section, we follow exactly the experimental design of Sakhi et al. [49], Aouali et al. [5] to conduct our PAC-Bayesian Off-Policy learning experiments. We however take the time to explain it in details. In this procedure, we need three splits: $D_{l}$ (of size $n_{l}$ ) to train the logging policy $\\pi_{0}$ , another split $D_{c}$ (of size $n_{c.}$ ) to generate the logging feedback with $\\pi_{0}$ , and finally a test split $D_{t e s t}$ (of size $n_{t e s t}$ ) to compute the true risk $R(\\pi)$ of any policy $\\pi$ . In our experiments, we split the training split $D_{t r a i n}$ (of size $N$ ) of the four datasets considered into $D_{l}$ $[n_{l}=0.05N)$ and $D_{c}$ $\\ensuremath{n_{c}}=0.95\\ensuremath{N})$ and use their test split $D_{t e s t}$ . The detailed statistics of the different splits can be found in Table 7. Recall that $K$ is the number of actions and $p$ the number of features. ", "page_idx": 41}, {"type": "table", "img_path": "zLClygeRK8/tmp/76e5790c316d1780210fa12d2e2ffd59b675c1f5ef0145990f2ab159907f58b8.jpg", "table_caption": ["Table 7: OPL: Detailed statistics of the splits used. "], "table_footnote": [], "page_idx": 41}, {"type": "text", "text": "H.2.2 Policy class ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "In the PAC-Bayesian Learning paradigm, we are interested in the definition of policies as mixtures of decision rules: ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\pi_{Q}(a|x)=\\mathbb{E}_{f_{\\theta}\\sim Q}\\left[\\mathbb{1}\\left[f_{\\theta}(x)=a\\right]\\right]\\,,\\qquad\\qquad\\qquad\\forall(x,a)\\in\\mathcal{X}\\times\\mathcal{A}\\,.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "We use the Linear Gaussian Policy of Sakhi et al. [49]. To obtain these policies, we restrict $f_{\\theta}$ to: ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\forall x\\in\\mathcal{X},\\quad f_{\\theta}(x)=\\underset{a^{\\prime}\\in\\mathcal{A}}{\\mathrm{argmax}}\\left\\lbrace x^{t}\\theta_{a^{\\prime}}\\right\\rbrace\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "This results in a parameter $\\theta$ of dimension $d=p\\times K$ with $p$ the dimension of the features $\\phi(x)$ and $K$ the number of actions. We also restrict the family of distributions ${\\mathcal{Q}}_{d+1}\\;=\\;\\{Q_{\\mu,\\sigma}\\;=\\;\\}$ $\\mathcal{N}(\\pmb{\\mu},\\sigma^{2}I_{d}),\\pmb{\\mu}\\in\\mathbb{R}^{d},\\sigma>0\\}$ to independent Gaussians with shared scale. Estimating the propensity of $a$ given $x$ reduces the computation to a one dimensional integral: ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\pi_{\\mu,\\sigma}(a|x)=\\mathbb{E}_{\\epsilon\\sim\\mathcal{N}(0,1)}\\left[\\prod_{a^{\\prime}\\neq a}\\Phi\\left(\\epsilon+\\frac{\\phi(x)^{T}(\\mu_{a}-\\mu_{a^{\\prime}})}{\\sigma||\\phi(x)||}\\right)\\right]\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "with $\\Phi$ the cumulative distribution function of the standard normal. ", "page_idx": 41}, {"type": "text", "text": "H.2.3 Detailed hyperparameters ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Contrary to previous work, our method does not require tuning any loss function hyperparameter over a hold out set. We do however need to choose parameters to optimize the policies. ", "page_idx": 41}, {"type": "text", "text": "The logging policy $\\pi_{0}$ . $\\pi_{0}$ is trained on $D_{l}$ (supervised manner) with the following parameters: We use $L_{2}$ regularization of $10^{-4}$ . This is used to prevent the logging policy $\\pi_{0}$ from being close to deterministic, allowing efficient learning with importance sampling. We use Adam [30] with a learning rate of $10^{-1}$ for 10 epochs. ", "page_idx": 41}, {"type": "text", "text": "Parameters of the bounds. cIPS and cvcIPS: The clipping parameter $\\tau$ is fixed to $1/K$ with $K$ the action size of the dataset and cvcIPS is used with $\\xi=-0.5$ (the values used in Sakhi et al. [49]). ES: The exponential smoothing parameter $\\alpha$ is fixed to $1-1/K$ . ", "page_idx": 42}, {"type": "text", "text": "Optimizing the bounds. We use Adam [30] with a learning rate of $10^{-3}$ for 100 epochs. The gradient of LIG policies is a one dimensional integral, and is approximated using $S=32$ samples. ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pi_{\\mu,\\sigma}(a|x)=\\mathbb{E}_{\\epsilon\\sim\\mathcal{N}(0,1)}\\left[\\displaystyle\\prod_{a^{\\prime}\\neq a}\\Phi\\left(\\epsilon+\\frac{\\phi(x)^{T}(\\mu_{a}-\\mu_{a^{\\prime}})}{\\sigma||\\phi(x)||}\\right)\\right]}\\\\ &{\\qquad\\qquad\\approx\\displaystyle\\frac{1}{S}\\displaystyle\\sum_{s=1}^{S}\\prod_{a^{\\prime}\\neq a}\\Phi\\left(\\epsilon_{s}+\\frac{\\phi(x)^{T}(\\mu_{a}-\\mu_{a^{\\prime}})}{\\sigma||\\phi(x)||}\\right)\\quad\\epsilon_{1},...,\\epsilon_{S}\\sim\\mathcal{N}(0,1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "For all bounds, instead of fixing $\\lambda$ , we take a union bound over a discretized space of possible parameters $\\Lambda$ of size $n_{\\Lambda}=100$ and for each iteration $j$ of the optimization procedure, we take $\\lambda_{j}\\in\\Lambda$ that minimizes the estimated bound and proceed to compute the gradient w.r.t $\\mu$ and $\\sigma$ with $\\lambda_{j}$ . ", "page_idx": 42}, {"type": "text", "text": "H.2.4 Detailed results ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "In addition to the results of Table 2, we also provide a more detailed view of the results here. For each $\\alpha$ and dataset, we average both $\\{\\mathcal{G R},\\bar{R}\\}$ over the 10 seeds and plot them in Figure 6 and Figure 5. Note that the error bars are too small $\\sigma/\\sqrt{10}\\approx0.001$ and all our results in these graphs are significant. We observe that the LS PAC-Bayesian bound improves substantially on its competitors in terms of the guaranteed risk, especially on MNIST and FashionMNIST and also obtains the best performing policies, on par with the IX bound in the majority of scenarios. ", "page_idx": 42}, {"type": "image", "img_path": "zLClygeRK8/tmp/22ad520d4cbfb4df720c737d4182282d051f18eb23290f722e86e09453911d28.jpg", "img_caption": [], "img_footnote": [], "page_idx": 42}, {"type": "text", "text": "Figure 5: OPL: Guaranteed Risk given by the different bounds. We observe that our LS-LIN dominates all other bounds. IX comes close, especially on EMNIST and nuswide ", "page_idx": 42}, {"type": "image", "img_path": "zLClygeRK8/tmp/8ddef9d928accb921e525a480fadc535f5cd25d7042da1595c3cd269cb657ae8.jpg", "img_caption": ["Figure 6: OPL: True risk of obtained policies after minimizing the PAC-Bayesian bounds. We observe that LS-LIN and IX are hardly distinguishable, they both give the best policies in the majority of scenarios. "], "img_footnote": [], "page_idx": 42}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: The abstract and introduction scopes our work in the offilne contextual bandit setting, describes our method and claims its superiority compared to existing work. We provide both strong theoretical and empirical evidence in the paper to defend the claim. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 43}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: We provide the limitations of our method in Appendix A and discuss ways to mitigate them in future work. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 43}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 44}, {"type": "text", "text": "Justification: All our results are proven in the paper and all assumptions are discussed. Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 44}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 44}, {"type": "text", "text": "Justification: Our paper follows the classical off-policy experimental design, and all details to reproduce them are given in Appendix H. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 44}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: All data used is accessible UCI Repository, the code is also given in the supplementary material to reproduce all experimental results. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 45}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: The experimental settings are detailed in Section 5 and Appendix H. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 45}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: All our experiments are run with multiple seeds and for different scenarios and datasets. Some graphs do not need error bars (cumulative distributions or selection strategies), for the other results, we have very small error bars (Appendix H.2), making our results significant. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 45}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 46}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: Our experiments can be conducted in small machines and do not require heavy compute, this is detailed in Appendix H. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 46}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: Our paper is of theoretical nature, presents ideas to increase safety in decision making, uses publicly available data for the experiments and conforms to the NeurIPS Code of Ethics. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 46}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: The paper discusses both the positive and negative impacts in Appendix B. Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 47}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 47}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 47}, {"type": "text", "text": "Justification: Our paper tackles theoretical questions for decision-making, the data used for the experiments is openly accessible in UCI repository. We do not believe that our work poses such risks. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 47}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 47}, {"type": "text", "text": "Justification: Our experimental design is inspired from the code base of some papers that we cite, and all data used is openly accessible in UCI repository. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets. \u2022 The authors should cite the original paper that produced the code package or dataset. \u2022 The authors should state which version of the asset is used and, if possible, include a URL. ", "page_idx": 47}, {"type": "text", "text": "\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 48}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 48}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 48}, {"type": "text", "text": "Justification: We do not release new assests, but we give the code to reproduce the experiments in the supplementary material. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 48}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 48}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 48}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 48}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 48}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 48}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 48}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 49}]