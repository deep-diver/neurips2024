[{"type": "text", "text": "Is Behavior Cloning All You Need? Understanding Horizon in Imitation Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Dylan J. Foster Adam Block Microsoft Research Department of Mathematics dylanfoster@microsoft.com MIT ablock@mit.edu ", "page_idx": 0}, {"type": "text", "text": "Dipendra Misra Microsoft Research dipendrakumar.misra@databricks.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Imitation learning (IL) aims to mimic the behavior of an expert in a sequential decision making task by learning from demonstrations, and has been widely applied to robotics, autonomous driving, and autoregressive text generation. The simplest approach to IL, behavior cloning (BC), is thought to incur sample complexity with unfavorable quadratic dependence on the problem horizon, motivating a variety of different online algorithms that attain improved linear horizon dependence under stronger assumptions on the data and the learner\u2019s access to the expert. ", "page_idx": 0}, {"type": "text", "text": "We revisit the apparent gap between offilne and online IL from a learning-theoretic perspective, with a focus on the realizable/well-specified setting with general policy classes up to and including deep neural networks. Through a new analysis of behavior cloning with the logarithmic loss, we show that it is possible to achieve horizon-independent sample complexity in offline IL whenever (i) the range of the cumulative payoffs is controlled, and (ii) an appropriate notion of supervised learning complexity for the policy class is controlled. Specializing our results to deterministic, stationary policies, we show that the gap between offilne and online IL is smaller than previously thought: (i) it is possible to achieve linear dependence on horizon in offline IL under dense rewards (matching what was previously only known to be achievable in online IL); and (ii) without further assumptions on the policy class, online IL cannot improve over offline IL with the logarithmic loss, even in benign MDPs. We complement our theoretical results with experiments on standard RL tasks and autoregressive language generation to validate the practical relevance of our findings. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Imitation learning (IL) is the problem of emulating an expert policy for sequential decision making by learning from demonstrations. Compared to reinforcement learning (RL), the learner in IL does not observe reward-based feedback, and must imitate the expert\u2019s behavior based on demonstrations alone; their objective is to achieve performance close to that of the expert on an unobserved reward function. Imitation learning is motivated by the observation that in many domains, demonstrating the desired behavior for a task (e.g., robotic grasping) is simple, while designing a reward function to elicit the desired behavior can be challenging. IL is also often preferable to RL because it removes the need for exploration, leading to empirically reduced sample complexity and often much more stable training. Indeed, the relative ease of applying IL (over RL methods) has led to extensive adoption, ranging from classical applications in autonomous driving [63] and helicopter filght [1] to contemporary works that leverage deep learning to achieve state-of-the-art performance for self-driving vehicles [15, 6, 44], visuomotor control [30, 103], navigation [45], and game AI [46, 90]. Imitation learning also offers a conceptual framework through which to study autoregressive language modeling [21, 13], and a number of useful empirical insights have arisen as a result of this perspective. However, a central challenge limiting broader real-world deployment is to understand and improve the reliability and stability properties of algorithms that support general-purpose (deep/neural) function approximation. ", "page_idx": 0}, {"type": "image", "img_path": "8KPyJm4gt5/tmp/b7192f87a2f339e1564090b6864ac8a9d81aa75e2c73a0d45f42a9d619c8064d.jpg", "img_caption": ["Figure 1: Suboptimality of a policy learned with log-loss behavior cloning (LogLossBC) as a function of the number of expert trajectories, for varying values of horizon $H$ . In each environment, an imitator is trained according to LogLossBC and the regret with respect to the expert is reported, with reward normalized to be horizon-independent. (a) Continuous control with MuJoCo environment Walker2dv4. (b) Discrete control with Atari environment BeamRiderNoFrameskip-v4. For both environments, we find that the regret is independent of horizon (or in the case of Atari, slightly improving with horizon), as predicted by our theoretical results. Full experimental details are provided in Appendix C. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In more detail, imitation learning algorithms can be loosely grouped into offilne and online approaches. Offilne imitation learning algorithms only require access to a dataset of logged trajectories from the expert, making them broadly applicable. The most widely used approach, behavior cloning, reduces imitation learning to a standard supervised learning problem in which the learner attempts to predict the expert\u2019s actions from observations given the collected trajectories. The simplicity of this approach allows the learner to leverage the considerable machinery developed for supervised learning and readily incorporate complex function approximation with deep models [10, 70]. On the other hand, BC seemingly ignores the problem of distribution shift, wherein small deviations from the expert policy early in rollout lead the learner off-distribution to regions where they are less able to accurately imitate. This apparent error amplification phenomenon has been widely observed empirically [70, 54, 13], and motivates online or interactive approaches to imitation learning [70, 72, 71, 78], which avoid error amplification by interactively querying the expert and learning to correct mistakes on-policy. ", "page_idx": 1}, {"type": "text", "text": "In theory, online imitation learning enables sample complexity guarantees with improved (linear, as opposed to quadratic) dependence on horizon for favorable MDPs. Yet, while online imitation learning has found empirical success [73, 52, 38, 6, 26, 51, 7, 108, 59], online access to the expert can be costly or infeasible in many applications, and offline imitation learning remains a dominant empirical paradigm. Motivated by this disconnect between theory and practice, we aim to understand to what extent the apparent gap between offilne and online imitation learning is fundamental. We ask: ", "page_idx": 1}, {"type": "text", "text": "Is online imitation learning truly more sample-efficient than offilne imitation learning, or can existing algorithms or analyses be improved? ", "page_idx": 1}, {"type": "text", "text": "1.1 Background: Offline and Online Imitation Learning ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "To motivate our results, we begin by formally introducing the offline and online imitation learning frameworks, highlighting gaps in current sample complexity guarantees concerning horizon dependence. We take a learning-theoretic perspective, with a focus on general policy classes. ", "page_idx": 1}, {"type": "text", "text": "Markov decision processes. We study imitation learning in episodic Markov decision processes. Formally, a Markov decision process $\\dot{M}=(\\mathcal{X},\\mathcal{A},P,r,\\bar{H})$ consists of a (potentially large) state ", "page_idx": 1}, {"type": "text", "text": "space $\\mathcal{X}$ , action space $\\boldsymbol{\\mathcal{A}}$ , horizon $H$ , probability transition function $\\textit{P}=\\{P_{h}\\}_{h=0}^{H}$ , where $P_{h}:\\mathcal{X}\\times\\mathcal{A}\\to\\Delta(\\mathcal{X})$ , and reward function $r=\\{r_{h}\\}_{h=1}^{H}$ , where $r_{h}:\\mathcal{X}\\times\\mathcal{A}\\to\\mathbb{R}$ . A (randomized) ap oldiicstyr iibs uati osenq uoevnerc et roafj epcetro-rtiiems e $\\pi\\,=\\,\\left\\{\\pi_{h}:\\mathcal{X}\\to\\Delta(\\mathcal{A})\\right\\}_{h=1}^{H}$ .o wTinhge  pporloicceys si.nduTchees $(x_{1},a_{1},r_{1}),\\dots,(x_{H},a_{H},r_{H})$   \ninitial state is drawn via $x_{1}\\sim P_{0}(\\emptyset)$ ,1 then for $h\\,=\\,1,\\dots,H\\colon a_{h}\\,\\sim\\,\\pi(x_{h}),\\,r_{h}\\,=\\,r_{h}(x_{h},a_{h}),$ and $x_{h+1}\\,\\sim\\,P_{h}(x_{h},a_{h})$ . For notational convenience, we use $x_{H+1}$ to denote a deterministic terminal state with zero reward. We let $\\mathbb{E}^{\\pi}[\\cdot]$ and $\\mathbb{P}^{\\pi}[\\cdot]$ denote expectation and probability law for $(x_{1},a_{1}),\\dots,(x_{H},a_{H})$ under this process, respectively.2 ", "page_idx": 2}, {"type": "text", "text": "The expected reward for policy $\\pi$ is given by $\\begin{array}{r}{J(\\pi):=\\mathbb{E}^{\\pi}\\left[\\sum_{h=1}^{H}r_{h}\\right]}\\end{array}$ , and the value functions for $\\pi$ are given by $\\begin{array}{r}{V_{h}^{\\pi}(x):=\\mathbb{E}^{\\pi}\\left[\\sum_{h^{\\prime}=h}^{H}r_{h^{\\prime}}\\mid x_{h}=x\\right]}\\end{array}$ and $\\begin{array}{r}{Q_{h}^{\\pi}(x,a):=\\mathbb{E}^{\\pi}\\left[\\sum_{h^{\\prime}=h}^{H}r_{h^{\\prime}}\\mid x_{h}=x,a_{h}=a\\right]\\!.}\\end{array}$ . ", "page_idx": 2}, {"type": "text", "text": "Reward normalization. To study the role of horizon in imitation learning in a way that disentangles the effects of reward scaling from other factors, we assume that rewards are normalized such $\\textstyle\\sum_{h=1}^{H}r_{h}\\,\\in\\,[0,R]$ ,a  wpahricahm iest etrh $R>0$ [o4f7 ,m 9o4s,t  1p0ri4o, r 4w8]o.r kW [e7 r0,e f7e2r ,t o7 1t,h e6 7s\u2013et6t9i,n g8 0i]n,  awsh tichhe $r_{h}\\in[0,1]$ $h\\in[H]$ dense reward setting, which has $R\\leq H$ ; we will frequently specialize our results to this setting. ", "page_idx": 2}, {"type": "text", "text": "1.1.1 Offline Imitation Learning: Behavior Cloning ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Let $\\pi^{\\star}=\\left\\{\\pi_{h}^{\\star}:\\mathcal{X}\\rightarrow\\Delta(A)\\right\\}_{h=1}^{H}$ denote the expert policy. In the offline imitation learning setting, we receive a dataset $\\mathcal{D}=\\left\\{o^{i}\\right\\}_{i=1}^{n}$ of (reward-free) trajectories $o^{i}=(x_{1}^{i},a_{1}^{i}),\\dots,(x_{H}^{i},a_{H}^{i})$ obtained by executing $\\pi^{\\star}$ in the underlying MDP $M^{\\star}$ . Using these trajectories, our goal is to learn a policy $\\widehat{\\pi}$ such that the rollout regret $\\dot{J}(\\bar{\\pi^{\\star}})-J(\\widehat{\\pi})$ to $\\pi^{\\star}$ is as small as possible. We emphasize that $\\pi^{\\star}$ is an arbitrary policy, and is not assumed to be optimal. ", "page_idx": 2}, {"type": "text", "text": "Behavior cloning. Behavior cloning, which reduces the imitation learning problem to supervised prediction, is the dominant offline imitation learning paradigm. To describe the algorithm in its simplest form, consider the case where $\\pi^{\\star}:=\\,\\{\\pi_{h}^{\\star}:\\mathcal{X}\\to A\\}_{h=1}^{H}$ is deterministic. For a userspecified policy class $\\Pi\\subset\\{\\pi_{h}:\\mathcal{X}\\rightarrow\\Delta(\\mathcal{A})\\}_{h=1}^{H}$ , the most basic version of behavior cloning [70] solves the supervised classification problem $\\begin{array}{r}{\\hat{\\pi}=\\arg\\operatorname*{min}_{\\pi\\in\\Pi}\\sum_{i=1}^{n}\\frac{1}{H}\\sum_{h=1}^{H}\\mathbb{I}\\{\\pi_{h}(x_{h}^{i})\\neq a_{h}^{i}\\}=:}\\end{array}$ $\\widehat{L}_{\\mathfrak{b c}}(\\pi)$ . Naturally, other classification losses (e.g., square loss, logistic loss, or log loss) may be used in place of the indicator loss.3 To provide sample complexity bounds for this algorithm, we make a standard realizability assumption (e.g., Agarwal et al. [2], Foster and Rakhlin [33]). ", "page_idx": 2}, {"type": "text", "text": "Assumption 1.1 (Realizability). The policy class \u03a0 contains the expert policy, i.e. $\\pi^{\\star}\\in\\Pi$ . ", "page_idx": 2}, {"type": "text", "text": "This assumption asserts that $\\Pi$ is expressive enough to represent the expert policy. Depending on the application, $\\Pi$ might be parameterized by simple linear models, or by flexible models such as convolutional neural networks or transformers. We primarily restrict our attention to the realizable setting throughout the paper, as it is meaningful and non-trivial, yet not fully understood. Our main results extend to provide guarantees for the misspecified case, but a thorough study of the role of misspecification is beyond the scope of this work.To simplify presentation, we adopt a standard convention in RL theory and focus on finite classes with $|\\bar{\\Pi}|<\\bar{\\infty}$ [2, 33]. ", "page_idx": 2}, {"type": "text", "text": "To proceed with analyzing the behavior cloning algorithm, a standard uniform convergence argument implies that if we define $\\begin{array}{r}{\\bar{L}_{\\mathsf{b c}}(\\pi)=\\frac{1}{H}\\sum_{h=1}^{H}\\bar{\\mathbb{P}}^{\\pi^{\\star}}\\bar{[}\\pi_{h}(x_{h})\\neq\\pi_{h}^{\\star}(x_{h})]}\\end{array}$ , then with probability at least $1-\\delta$ , behavior cloning has ", "page_idx": 2}, {"type": "equation", "text": "$$\nL_{\\sf b c}(\\widehat{\\pi})\\lesssim\\frac{\\log(|\\Pi|\\delta^{-1})}{n}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Meanwhile, a standard error analysis for BC leads to the following bound on rollout performance: ", "page_idx": 2}, {"type": "equation", "text": "$$\nJ(\\pi^{\\star})-J(\\widehat\\pi)\\lesssim R H\\cdot L_{\\mathsf{b c}}(\\widehat\\pi).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Combining these bounds, we conclude that ", "page_idx": 3}, {"type": "equation", "text": "$$\nJ(\\pi^{\\star})-J(\\widehat{\\pi})\\lesssim R H\\cdot\\frac{\\log(|\\Pi|\\delta^{-1})}{n}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "For the dense reward setting where $R\\,=\\,H$ , this leads to quadratic dependence on horizon; that is, $\\Omega(H^{2})$ trajectories are required to achieve constant accuracy. Unfortunately, both steps in this argument are tight in general: ", "page_idx": 3}, {"type": "text", "text": "\u2022 The generalization bound $\\begin{array}{r}{L_{\\mathrm{bc}}(\\widehat{\\pi})\\lesssim\\frac{\\log(|\\Pi|\\delta^{-1})}{n}}\\end{array}$ is tight even when $|\\Pi|=2$ (this is true not just for the indicator loss, but for ot her standard losses such as square loss, absolute loss, and hinge loss). Since the amount of information in a trajectory grows with $H$ , one might hope a-priori that the generalization error would decrease with $H$ ; alas, this does not occur due to the dependence between samples in each trajectory. ", "page_idx": 3}, {"type": "text", "text": "\u2022 Ross and Bagnell [70] show that the inequality $J(\\pi^{\\star})-J(\\widehat{\\pi})\\lesssim R H\\cdot L_{\\mathsf{b c}}(\\widehat{\\pi})$ is tight for MDPs with 3 states; the quadratic scaling in $H$ this induces under  dense rewards i s often attributed to error amplification or distribution shift incurred by passing from error under the state distribution of $\\pi^{\\star}$ to the state distribution of $\\widehat{\\pi}$ . ", "page_idx": 3}, {"type": "text", "text": "Combining, these observations, it is natural to conclude that offline imitation learning is fundamentally harder than supervised classification, where linear dependence on horizon might be expected (e.g., with $H$ independent prediction tasks). ", "page_idx": 3}, {"type": "text", "text": "1.1.2 Online Imitation Learning and Recoverability ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The aforementioned limitations of behavior cloning have motivated online approaches to IL [70, 72, 71, 78]. In the online framework, the learner can interactively choose policies to roll out and query the expert for the action at each state in the trajectory (see Appendix E.2 for a formal description), representing a substantially stronger (and in some cases unrealistic) assumption on the learner\u2019s access both to the MDP and the expert than in the offilne setting. Online imitation learning can avoid error amplification and achieve improved dependence on horizon for MDPs that satisfy a recoverability condition [72, 68]. ", "page_idx": 3}, {"type": "text", "text": "Definition 1.1 (Recoverability parameter). The recoverability parameter for an MDP $M^{\\star}$ and expert $\\pi^{\\star}$ is given by4 ", "page_idx": 3}, {"type": "text", "text": "Under recoverability, the Dagger algorithm of Ross et al. [72] leverages online interaction by interactively querying the expert and learning to correct mistakes on-policy, leading to sample complexity ", "page_idx": 3}, {"type": "equation", "text": "$$\nJ(\\pi^{\\star})-J(\\widehat{\\pi})\\lesssim\\mu H\\cdot\\frac{\\log|\\Pi|}{n}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "for any finite class $\\Pi$ and deterministic expert policy $\\pi^{\\star}$ (for completeness, we include an analysis in Appendix E.2; see Propositions E.1 and E.2). For the dense reward setting where $R=H$ , we can have $\\mu=H$ in the worst case, in which case Eq. (3) matches the quadratic horizon dependence of behavior cloning, but when $\\mu=O(1)$ (informally, this means it is possible to \u201crecover\u201d from a bad action that deviates from $\\pi^{\\star}$ ), the bound in Eq. (3) achieves linear dependence on horizon. Other online $\\mathrm{IL}$ algorithms such as Forward, Smile [70], and Aggrevate [71] achieve similar guarantees (we are not aware of an approach that improves upon Eq. (3) for general finite classes). ", "page_idx": 3}, {"type": "text", "text": "The improvements of online IL notwithstanding, Eq. (2) is known to be tight for BC, but this is an algorithm-dependent (as opposed to information-theoretic) lower bound, and does not preclude the existence of more sample-efficient, purely offilne algorithms. In this context, our central question can be restated as: Can offilne imitation learning algorithms achieve sub-quadratic horizon dependence for general policy classes \u03a0? While prior work has investigated this question for tabular and linear policies [67\u201369], we approach the problem from a new (learning-theoretic) perspective by considering general policy classes. ", "page_idx": 3}, {"type": "text", "text": "1.2 Contributions ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We present several new results that clarify the role of horizon in offilne and online imitation learning. ", "page_idx": 3}, {"type": "table", "img_path": "8KPyJm4gt5/tmp/e5aaf686628e551d41cd032620159f0b6fd92f76d9472ee0da6edcc196d83459.jpg", "table_caption": [], "table_footnote": ["Table 1: Summary of upper bounds for deterministic experts; lower bounds are more nuanced, and discussed in Section 2.2. Each cell denotes the regret of a policy learned with log-loss behavior cloning (LogLossBC), which is optimal in each setting. Here, $\\Pi$ is the policy class, $R$ is the reward range, $H$ is the horizon, and $n$ is the number of expert trajectories. In the dense-reward setting, we set $R=H$ . "], "page_idx": 4}, {"type": "text", "text": "1. Horizon-independent analysis of log-loss behavior cloning. Through a new analysis of behavior cloning with the logarithmic loss (LogLossBC), we show that it is possible to achieve horizon-independent sample complexity [47, 94, 104, 105] in offline imitation learning whenever (i) the range of the cumulative payoffs is normalized, and (ii) an appropriate notion of supervised learning complexity for the policy class is controlled. Our result is facilitated by a novel information-theoretic analysis which controls policy behavior at the trajectory level, supporting both deterministic and stochastic expert policies. ", "page_idx": 4}, {"type": "text", "text": "2. Deterministic policies: Closing the gap between offilne and online IL. Specializing LogLossBC to deterministic stationary policies (more generally, policies with parameter sharing) and cumulative rewards in the range $[0,H]$ , we show that it is possible to achieve sample complexity with linear dependence on horizon in offilne $\\mathrm{IL}$ in arbitrary MDPs, matching was was previously only known of online IL. We complement this result with a lower bound showing that, without further structural assumptions on the policy class (e.g., no parameter sharing [67]), online IL cannot improve over offline IL with LogLossBC, even for benign MDPs. Our results are summarized in Table 1. Nonetheless, as observed in prior work [67], online imitation learning can still be beneficial for non-stationary policies. ", "page_idx": 4}, {"type": "text", "text": "3. Stochastic policies: Tight understanding of optimal sample complexity. For stochastic expert policies, our analysis of LogLossBC gives the first variance-dependent sample complexity bounds for imitation learning with general policy classes, which we prove to be tight in a problem-dependent and minimax sense. Using this result, we show that for stochastic stationary experts, (i) quadratic dependence on the horizon is necessary when cumulative rewards lie in the range $[0,H]$ , in contrast to the deterministic setting, but (ii) LogLossBC\u2014through our variance-dependent analysis\u2014can sidestep this hardness and achieve linear dependence on horizon under a recoverability-like condition. Finally, we show that\u2014as in the deterministic case\u2014online $\\mathrm{IL}$ cannot improve over offline IL with LogLossBC without further assumptions on the policy class. Our results are summarized in Table 2. ", "page_idx": 4}, {"type": "text", "text": "Toward a learning-theoretic understanding of imitation learning. Our findings highlight the need to develop a fine-grained, problem-dependent understanding of algorithms and complexity for IL. Instabilities of offline IL [60, 27, 13] and benefits of online IL [73, 52, 38, 6, 26, 51, 7, 108, 59] likely arise in practice, but existing assumptions in theoretical research are often too coarse to give insights into the true nature of these phenomena, leading to an important gap between theory and practice. As a first step in this direction, we highlight several under-explored mechanisms through which online IL can lead to improved sample complexity, including representational benefits and exploration (Appendix I). We also complement our theoretical results with empirical demonstrations of the phenomena we describe (Appendix C). ", "page_idx": 4}, {"type": "text", "text": "Experiments. In Appendix C (deferred to the appendix due to space constraints), we complement our theoretical results with an empirical demonstration of the horizon-independence of LogLossBC predicted by our theory (under parameter sharing and sparse rewards). We consider tasks where the horizon $H$ can be naturally scaled up and down\u2014for example, an agent walking for a set number of timesteps\u2014and use an expert trained according to RL to generate expert trajectories, before training a policy using LogLossBC. We consider both continuous action space (MuJoCo environment Walker2d) and discrete action space (Atari environment Beamrider) tasks to demonstrate the broad applicability of our theoretical results. As can be seen in Figure 1, the performance of the learned policy is independent or improving with horizon, consistent with our theoretical results. We also perform simplified experiments on autoregressive language generation with transformers. Here, we find that the performance of the imitator is largely independent of $H$ , as predicted by our results, though the results are more nuanced. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Notation. We use $\\mathbb{I}_{x}\\,\\in\\,\\Delta(\\mathcal{X})$ to denote the direct delta distribution, which places probability mass 1 on $x$ . We adopt standard big-oh notation, and write $f={\\widetilde{O}}(g)$ to denote that $f=O(g\\;\\cdot$ $\\operatorname*{max}\\{1,\\operatorname{polylog}(g)\\})$ and $a\\lesssim b$ as shorthand for $a=O(b)$ . ", "page_idx": 5}, {"type": "text", "text": "2 Horizon-Independent Analysis of Log-Loss Behavior Cloning ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "This section presents the first of our main results, a horizon-independent sample complexity analysis of log-loss behavior cloning for the case of deterministic experts. Our second main result, which handles the case of stochastic experts, builds on our results here and is presented in Section 3. ", "page_idx": 5}, {"type": "text", "text": "2.1 Log-Loss Behavior Cloning and Supervised Learning Guarantees ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The workhorse for all of our results (both for deterministic and stochastic experts) is the following simple modification to behavior cloning. For a class of (potentially stochastic) policies $\\Pi$ , we minimize the logarithmic loss: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\widehat{\\pi}=\\mathop{\\mathrm{arg\\,min}}_{\\pi\\in\\Pi}\\sum_{i=1}^{n}\\sum_{h=1}^{H}\\log\\left(\\frac{1}{\\pi_{h}(a_{h}^{i}\\mid x_{h}^{i})}\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "This scheme is ubiquitous in practice [45, 31], and forms the basis for autoregressive language modeling [64]; we refer to it as LogLossBC. We will show that this seemingly small change\u2014 moving from indicator loss to log loss\u2014has significant benefits. Following the classical tradition of imitation learning [70, 72, 71], our analysis proceeds via reduction to supervised learning. We first show that LogLossBC satisfies an appropriate supervised learning guarantee, then translate this into rollout performance. Our starting point is to observe that LogLossBC, via Eq. (4), can be interpreted as performing maximum likelihood estimation over the set $\\{\\mathbb{P}^{\\pi}\\}_{\\pi\\in\\Pi}$ in order to estimate the law $\\mathbb{P}^{\\pi^{\\star}}$ over trajectories under $\\pi^{\\star}$ (see Appendix E.1 for details). As a result, standard guarantees for maximum likelihood estimation [89, 102] imply convergence in distribution whenever $\\pi^{\\star}\\in\\Pi$ . To be precise, define the squared Hellinger distance for probability measures $\\mathbb{P}$ and $\\mathbb{Q}$ by $\\begin{array}{r}{D_{\\mathsf{H}}^{2}(\\mathbb{P},\\mathbb{Q})=\\int\\!\\left(\\sqrt{d\\mathbb{P}}\\!-\\!\\sqrt{d\\mathbb{Q}}\\right)^{2}}\\end{array}$ . Then for any finite policy class $\\Pi$ , we have the following guarantee.5 ", "page_idx": 5}, {"type": "text", "text": "Proposition 2.1 (Supervised learning guarantee for LogLossBC (special case of Theorem E.1)). For any (potentially stochastic) expert $\\pi^{\\star}\\in\\Pi$ , the LogLossBC algorithm ensures that with probability at least 1 \u2212\u03b4,D2H P\u03c0 , P\u03c0\u22c6 \u22642 log(|\u03a0n|\u03b4\u22121). ", "page_idx": 5}, {"type": "text", "text": "That is, by performing LogLossBC, we are implicitly estimating the law $\\mathbb{P}^{\\pi^{\\star}}$ ; note that this result holds even if $\\pi^{\\star}$ is stochastic, as long as $\\pi^{\\star}\\in\\Pi$ . We will focus on finite, realizable policy classes throughout this section to simplify presentation as much as possible, but guarantees for infinite classes under misspecification are given in Appendix E.1. ", "page_idx": 5}, {"type": "text", "text": "2.2 Horizon-Independent Analysis of LogLossBC for Deterministic Experts ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We first consider the case where the expert $\\pi^{\\star}$ is deterministic. Our main result is the following theorem, which translates the supervised learning error $D_{\\mathsf{H}}^{2}\\big(\\mathbb{P}^{\\widehat{\\pi}},\\mathbb{P}^{\\pi^{\\star}}\\big)$ into a bound on rollout performance in a horizon-independent fashion. ", "page_idx": 5}, {"type": "text", "text": "Theorem 2.1 (Horizon-independent regret decomposition (deterministic case)). For any deterministic policy $\\pi^{\\star}$ and potentially stochastic policy $\\widehat{\\pi}$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\nJ(\\pi^{\\star})-J(\\widehat\\pi)\\leq4R\\cdot D_{\\mathsf{H}}^{2}\\big(\\mathbb{P}^{\\widehat\\pi},\\mathbb{P}^{\\pi^{\\star}}\\big).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "This result shows that horizon-independent bounds on rollout performance are possible whenever (i) rewards are appropriately normalized, and (ii) the supervised learning error $D_{\\mathsf{H}}^{2}\\big(\\mathbb{P}^{\\widehat{\\pi}},\\mathbb{P}^{\\pi^{\\star}}\\big)$ is appropriately controlled. It is proven using novel trajectory-level control over deviations between $\\widehat{\\pi}$ and $\\pi^{\\star}$ ; we will elaborate upon this in the sequel. We emphasize that this result would be trivial if squared Hellinger distance were replaced by total variation distance in (5); that the bound scales with squared Hellinger distance is crucial for obtaining fast $1/n$ -type rates and linear horizon dependence. We further remark that this reduction is not specific to LogLossBC, and can be applied to any IL algorithm for which we can bound the Hellinger distance. Combining Theorem 2.1 with Proposition 2.1, we obtain the following guarantee for finite policy classes. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Corollary 2.1 (Regret of LogLossBC). For any deterministic expert $\\pi^{\\star}\\ \\in\\ \\Pi$ , the LogLossBC algorithm in Eq. (4) ensures that with probability at least $1-\\delta$ , $\\begin{array}{r}{J(\\pi^{\\star})-J(\\widehat{\\pi})\\le8R\\cdot\\frac{\\log(2|\\Pi|\\delta^{-1})}{n}}\\end{array}$ . ", "page_idx": 6}, {"type": "text", "text": "To the best of our knowledge, this is the tightest available sample complexity guarantee for offline imitation learning with general policy classes. This bound improves upon the guarantee for indicatorloss behavior cloning in Eq. (2) by an $O(H)$ factor, and improves upon the guarantee for Dagger in Eq. (3) (replacing $H$ with $R\\leq H$ under $r_{h}\\in[0,1])$ ) in the typical regime where $\\mu=\\Omega(1)$ . ", "page_idx": 6}, {"type": "text", "text": "2.3 Interpreting the Sample Complexity of LogLossBC ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To understand the behavior of the bound for LogLossBC in Corollary 2.1 in more detail, we consider two special cases (summarized in Table 1). ", "page_idx": 6}, {"type": "text", "text": "Stationary policies and parameter sharing. If $\\log\\lvert\\Pi\\rvert=O(1)$ , the bound in Corollary 2.1 is independent of horizon in the case of sparse rewards $\\ R=O(1)$ ), and linear in horizon in the case of dense rewards $\\begin{array}{r}{\\mathbf{\\nabla}R=O(H))}\\end{array}$ . In other words, our work establishes for the first time that: ", "page_idx": 6}, {"type": "text", "text": "$O(H)$ sample complexity can be achieved in offline $I L$ under dense rewards for general classes \u03a0, as long as $\\mathrm{log}|\\Pi$ is appropriately controlled and realizability holds. This runs somewhat counter to intuition expressed in prior work [70, 72, 71, 67\u201369, 80], but we will show in the sequel that there is no contradiction. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Generally speaking, we expect to have $\\log\\lvert\\Pi\\rvert=O(1)$ if $\\Pi$ consists of stationary policies or more broadly, policies with parameter sharing across steps $h\\in[H]$ (as is the case in transformers used for autoregressive text generation). As an example, for a tabular (finite state/action) MDP, if $\\Pi$ consists of all stationary policies, we have $\\log|\\Pi|\\,=\\,|\\chi|\\log|A|$ , so Corollary 2.1 gives $J(\\pi^{\\star})-$ $\\begin{array}{r}{J(\\widehat{\\pi})\\lesssim\\frac{R|\\mathcal{X}|\\log(|\\mathcal{A}|\\delta^{-1})}{n}}\\end{array}$ ; that is, stationary policies can be learned with horizon-independent samples co mplexity under sparse rewards and linear dependence on horizon under dense rewards. Similar behavior holds for non-stationary policies with parameter sharing (e.g., log-linear policies of the form $\\pi_{h}(a\\mid x)\\propto\\exp(\\langle\\phi_{h}(x,a),\\theta\\rangle)^{\\frac{1}{2}}$ ; see Appendix E.1 for details. ", "page_idx": 6}, {"type": "text", "text": "Non-stationary policies or no parameter sharing. For non-stationary policies or policies with no parameter sharing across steps $h$ (e.g., product classes where $\\Pi=\\Pi_{1}\\times\\Pi_{2}\\cdot\\cdot\\cdot\\times\\Pi_{H})$ , we expect $\\log\\left|\\Pi\\right|=O(H)$ (more generally, $D_{\\mathsf{H}}^{2}\\bigl(\\mathbb{P}^{\\tilde{\\pi}},\\mathbb{P}^{\\pi^{\\star}}\\bigr)=\\widetilde{O}(H/n))$ . For example, in a tabular MDP, if $\\Pi$ consists of all non-stationary policies, we have $\\log|\\Pi|=H|\\mathcal{X}|\\log|A|$ . In this case, Corollary 2.1 gives linear dependence on horizon for sparse rewards $\\begin{array}{r}{(J(\\pi^{\\star})-\\dot{J}(\\hat{\\pi})\\,\\lesssim\\,\\frac{R H|\\mathcal{X}|\\log(|\\mathcal{A}|\\delta^{-1})}{n})}\\end{array}$ and quadratic dependence on horizon for dense rewards (J(\u03c0\u22c6) \u2212J(\u03c0) \u2272H2|X| logn(|A|\u03b4\u22121)) . The latter bound is known to be optimal [67] for offline $\\mathrm{IL}$ . ", "page_idx": 6}, {"type": "text", "text": "2.4 Optimality and Consequences for Online versus Offline Imitation Learning ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We now investigate the optimality of Theorem 2.1 and discuss implications for online versus offilne imitation learning, as well as connections to prior work. Our main result here shows that in the dense-reward regime where $r_{h}~\\in~[0,1]$ and $R\\;=\\;H$ , Theorem 2.1 cannot be improved when $\\log\\lvert\\Pi\\rvert=O(1)$ \u2014even with online access, recoverability, and known dynamics. ", "page_idx": 6}, {"type": "text", "text": "Theorem 2.2 (Lower bound for deterministic experts). For any $n\\,\\in\\,\\mathbb{N}$ and $H\\in\\mathbb{N},$ , there exists a (reward-free) MDP $M^{\\star}$ with $|{\\mathcal{X}}|=|{\\mathcal{A}}|=2,$ , a class of reward functions $\\mathcal{R}$ with $|{\\mathcal{R}}|=2$ , and a class of deterministic policies \u03a0 with $|\\Pi|=2$ with the following property. For any (online or offline) imitation learning algorithm, there exists a deterministic reward function $r\\,=\\,\\{r_{h}\\}_{h=1}^{H}$ with $r_{h}\\in[0,1]$ (in particular, $R\\leq H$ ) and (optimal) expert policy $\\pi^{\\star}\\in\\Pi$ with $\\mu=1$ such that $\\begin{array}{r}{\\mathbb{E}[J(\\pi^{\\star})-J(\\widehat\\pi)]\\geq c\\cdot\\frac{H}{n}}\\end{array}$ for an absolute constant $c>0$ . In addition, the dynamics, rewards, and expert policies  are stationary. ", "page_idx": 6}, {"type": "text", "text": "Together, Theorems 2.1 and 2.2 show that without further assumptions on $\\Pi$ , online imitation learning cannot improve upon offilne imitation learning in the realizable setting. That is, even if recoverability is satisfied, there is no online imitation learning algorithm that improves upon Theorem 2.1 uniformly for all policy classes. See Appendix H.1 for further lower bounds. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Benefits of online IL for policies with no parameter sharing. How can we reconcile our results with prior work showing that that online IL improves the horizon dependence of offilne IL [70, 72, 71, 67\u201369, 80]? The important distinction here is that online $\\mathrm{IL}$ can still improve on a policy-class dependent basis. In particular, methods like Dagger can still lead to improved sample complexity for policy classes with no parameter sharing across steps $h\\in[H]$ . Let $\\Pi_{h}^{-}:=\\left\\{\\pi_{h}\\mid\\bar{\\pi}\\in\\Pi\\right\\}$ denote the projection of $\\Pi$ onto step $h$ . In Appendix E.2, we prove the following refined guarantee for a variant of Dagger based on the log-loss (LogLossDagger). ", "page_idx": 7}, {"type": "text", "text": "Proposition 2.2 (Special case of Proposition E.2). When $\\pi^{\\star}\\in\\Pi$ is deterministic, LogLossDagger ensures that with probability at least $1-\\delta$ $\\begin{array}{r}{\\stackrel{\\cdot}{,}J(\\pi^{\\star})-J(\\widehat{\\pi})\\lesssim\\mu\\cdot\\sum_{h=1}^{H}\\frac{\\log(|\\Pi_{h}|H\\delta^{-1})}{n}}\\end{array}$ . ", "page_idx": 7}, {"type": "text", "text": "For classes with no parameter sharing (i.e., product classes where $\\Pi=\\Pi_{1}\\times\\Pi_{2}\\cdot\\cdot\\cdot\\times\\Pi_{H})$ , we have $\\begin{array}{r}{\\sum_{h=1}^{H}\\log\\vert\\Pi_{h}\\vert=\\log\\vert\\Pi\\vert}\\end{array}$ . In this case, Proposition 2.2 scales as $\\begin{array}{r}{J(\\pi^{\\star})-J(\\widehat{\\pi})\\lesssim\\mu\\cdot\\frac{\\log(|\\Pi|H\\delta^{-1})}{n}}\\end{array}$ improving on the bound for LogLossBC in Theorem 2.1 by replacing $R$ with $\\mu\\le R$ . Thus, online IL can indeed improve over offline IL for classes with no parameter sharing. This is consistent with Rajaraman et al. [67, 68], who proved a $\\mu H$ vs. $H^{2}$ gap between online and offline $\\mathrm{IL}$ for the special case of non-stationary tabular policies (where $\\Pi$ is a product class with $\\log\\vert\\Pi\\vert\\propto H)$ under dense rewards. However, for classes with parameter sharing (i.e., where $\\log\\vert\\Pi_{h}\\vert\\propto\\log\\vert\\Pi\\vert)$ , the bound in Proposition 2.2 scales as $\\frac{\\mu H\\log|\\Pi|}{n}$ , which does not improve over Theorem 2.1 unless $\\mu\\ll1$ . Since virtually all empirical work on imitation learning uses parameter sharing across steps $h\\in[H]$ , we believe the finding that online $\\mathrm{IL}$ does not improve over offilne $\\mathrm{IL}$ in this regime is quite salient. Nevertheless, it is important to emphasize that there are various practical considerations (e.g., misspecification or geometric structure) which this result may not account for.6 ", "page_idx": 7}, {"type": "text", "text": "2.5 Proving Theorem 2.1: How Does LogLossBC Avoid Error Amplification? ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The central object in the proof of Theorem 2.1 is the following trajectory-level distance function between policies. For a pair of potentially stochastic policies $\\pi$ and $\\pi^{\\prime}$ , define ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\rho(\\pi\\parallel\\pi^{\\prime}):=\\mathbb{E}^{\\pi}\\,\\mathbb{E}_{a_{1:H}^{\\prime}\\sim\\pi^{\\prime}(x_{1:H})}[\\mathbb{I}\\{\\exists h:a_{h}\\neq a_{h}^{\\prime}\\}],\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where we use the shorthand $a_{1:H}^{\\prime}\\sim\\pi^{\\prime}(x_{1:H})$ to indicate that $a_{1}^{\\prime}\\sim\\pi^{\\prime}(x_{1}),\\ldots,a_{H}^{\\prime}\\sim\\pi^{\\prime}(x_{H})$ . We begin by showing (Lemma F.2) that for all (potentially stochastic) policies $\\pi^{\\star}$ and $\\widehat{\\pi}$ , $\\dot{\\boldsymbol J}(\\pi^{\\star})-\\boldsymbol J(\\widehat{\\pi})\\leq$ $R\\cdot\\rho(\\pi^{\\star}\\parallel\\widehat\\pi)$ . We then show (Lemma F.3) that whenever $\\pi^{\\star}$ is deterministic,   Hellinger dist ance satisfies7 $\\begin{array}{r}{D_{\\mathsf{H}}^{2}\\!\\left(\\mathbb{P}^{\\widehat{\\pi}},\\mathbb{P}^{\\pi^{\\star}}\\right)\\,\\geq\\,\\frac{1}{4}\\cdot\\rho(\\widehat{\\pi}\\parallel\\pi^{\\star})}\\end{array}$ . Finally, we show (Lemma F.1) that the trajectory-level distance is symmetric, i.e. $\\rho(\\widehat{\\pi}\\parallel\\pi^{\\star})\\;=\\;\\rho(\\pi^{\\star}\\parallel\\widehat{\\pi})$ . This step is perhaps the most critical: by considering trajectory-level err ors, we can switch f rom the state distribution induced by $\\widehat{\\pi}$ to that of $\\pi^{\\star}$ for free, without incurring error amplification or spurious horizon factors. Comb i ning the preceding inequalities yields Theorem 2.1; see Appendix $\\boldsymbol{\\mathrm{F}}$ for the full proof. ", "page_idx": 7}, {"type": "text", "text": "This analysis is closely related to a result in Rajaraman et al. [68]. For the special case of deterministic, linearly parameterized policies with parameter sharing, Rajaraman et al. [68] consider an algorithm that minimizes an empirical analogue of the trajectory-wise distance in Eq. (6), and show that it leads to a bound similar to Corollary 2.1 (i.e., linear-in- $\\mathcal{H}$ sample complexity under dense rewards). Relative to this work, our contributions are threefold: (i) we show that horizon-independent sample complexity can be achieved for arbitrary policy classes with parameter sharing, not just linear classes; (ii) we show that said guarantees can be achieved by a natural algorithm, LogLossBC, which is already widely used in practice; and (iii), by virtue of considering the log loss, our results readily generalize to encompass stochastic expert policies, as we will show in the sequel. ", "page_idx": 7}, {"type": "text", "text": "3 Horizon-Independent Analysis of LogLossBC for Stochastic Experts ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we turn out attention to the general setting in which the expert policy $\\pi^{\\star}$ is stochastic. Stochastic policies are widely used in practice, where they are useful for modeling multimodal behavior [76, 25, 14], but have received relatively little exploration in theory beyond the work of Rajaraman et al. [67] for tabular policies.8 Our main result for this section is a regret decomposition based on the supervised learning error $D_{\\mathsf{H}}^{2}\\big(\\mathbb{P}^{\\widehat{\\pi}},\\mathbb{P}^{\\pi^{\\star}}\\big)$ that is horizon-independent and variance-dependent [107, 106, 93]. To state the result, define $\\begin{array}{r}{\\sigma_{\\pi^{\\star}}^{2}:=\\,\\sum_{h=1}^{H}\\mathbb{E}^{\\pi^{\\star}}\\left[(Q_{h}^{\\pi^{\\star}}(x_{h},\\pi_{h}^{\\star}(x_{h}))-Q_{h}^{\\pi^{\\star}}(x_{h},a_{h}))^{2}\\right]}\\end{array}$ as the variance for the expert policy. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Theorem 3.1 (Horizon-independent regret decomposition). Assume $R\\geq1$ . For any pair of (potentially stochastic) policies $\\pi^{\\star}$ and $\\widehat{\\pi}$ and any $\\varepsilon\\in(\\bar{0},e^{-1})$ , ", "page_idx": 8}, {"type": "equation", "text": "$$\nJ(\\pi^{\\star})-J(\\widehat\\pi)\\leq\\sqrt{6\\sigma_{\\pi^{\\star}}^{2}\\cdot D_{\\mathsf{H}}^{2}\\big(\\mathbb{P}^{\\widehat\\pi},\\mathbb{P}^{\\pi^{\\star}}\\big)}+O\\big(R\\log(R\\varepsilon^{-1})\\big)\\cdot D_{\\mathsf{H}}^{2}\\big(\\mathbb{P}^{\\widehat\\pi},\\mathbb{P}^{\\pi^{\\star}}\\big)+\\varepsilon.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Applying this result with LogLossBC leads to the following guarantee. ", "page_idx": 8}, {"type": "text", "text": "Corollary 3.1 (Regret of LogLossBC). For any $\\pi^{\\star}\\in\\Pi$ , the LogLossBC algorithm in Eq. (4) ensures that with prob. at least 1 \u2212\u03b4, J(\u03c0\u22c6) \u2212J(\u03c0) \u2264O(1) \u00b7 \u03c32\u03c0\u22c6log(n|\u03a0|\u03b4\u22121) $\\begin{array}{r}{t\\,1-\\delta,\\,J(\\pi^{\\star})-J(\\widehat\\pi)\\leq O(1)\\cdot\\sqrt{\\frac{\\sigma_{\\pi^{\\star}}^{2}\\log(|\\Pi|\\delta^{-1})}{n}}+O(R\\log(n))\\cdot\\frac{\\log(|\\Pi|\\delta^{-1})}{n}.}\\end{array}$ ", "page_idx": 8}, {"type": "text", "text": "As we show, when the expert policy is stochastic, we can no longer hope for a \u201cfast\u201d $1/n$ -type rate, and must instead settle for a \u201cslow\u201d $1/\\sqrt{n}$ -type rate. The slow term in Corollary 3.1 is controlled by the variance $\\sigma_{\\pi^{\\star}}^{2}$ for the optimal policy. In particular, if $\\pi^{\\star}$ is deterministic, then $\\sigma_{\\pi^{\\star}}^{2}=0$ , and Corollary 3.1 recovers our bound for the deterministic setting in Corollary 2.1 up to a $\\log(n)$ factor. ", "page_idx": 8}, {"type": "text", "text": "3.1 Horizon-Independence and Optimality for Stochastic Experts ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To understand the dependence on horizon in Corollary 3.1, we restrict our attention to the \u201cparameter sharing\u201d case where $\\log\\lvert\\Pi\\rvert=O(1)$ , and separately discuss the sparse and dense reward settings (results summarized in Table 2). ", "page_idx": 8}, {"type": "text", "text": "Consider the sparse reward setting where $R\\,=\\,O(1)$ . Here, at first glance it would appear that the variance $\\sigma_{\\pi^{\\star}}^{\\bar{2}}$ should scale with the horizon. Fortunately, this is not the case: The following result\u2014via a law-of-total-variance-type argument [4]\u2014implies that Corollary 3.1 is fully horizonindependent, with no explicit dependence on horizon when $R=O(1)$ and $\\log\\lvert\\Pi\\rvert=O(1)$ . For a function $f(x_{1:H},a_{1:H})$ , let $\\operatorname{Var}^{\\pi}{\\bar{[}{f}]}$ denote the variance of $f$ under $(x_{1},a_{1}),\\ldots,(x_{H},a_{H})\\sim\\pi$ . ", "page_idx": 8}, {"type": "text", "text": "Proposition 3.1. We have that $\\begin{array}{r}{\\sigma_{\\pi^{\\star}}^{2}\\le\\mathrm{Var}^{\\pi^{\\star}}\\left[\\sum_{h=1}^{H}r_{h}\\right]\\le R^{2}.}\\end{array}$ . ", "page_idx": 8}, {"type": "text", "text": "For the dense-reward regime where $R=H$ , Proposition 3.1 gives $J(\\pi^{\\star})-J(\\widehat\\pi)\\lesssim H\\sqrt{\\log(|\\Pi|)/n}$ . This is somewhat disappointing, as we now require $\\Omega(H^{2})$ trajectories (quadratic sample complexity) to learn a non-trivial policy, even when $\\log\\lvert\\Pi\\rvert=O(1)$ . We show now that this quadratic lower bound is qualitatively tight: the slow $H/\\sqrt{n}$ rate for $\\dot{\\sigma}_{\\pi^{\\star}}^{2}\\,=\\,H^{2}$ is necessary in both offline and online IL. This reveals a fundamental difference between deterministic and stochastic experts, since $O(H)$ sample complexity is sufficient in the former case. ", "page_idx": 8}, {"type": "text", "text": "Theorem 3.2 (informal). For any $\\sigma^{2}\\in\\left[H,H^{2}\\right]$ , there exists $\\Pi$ with $|\\Pi|=2$ such that $\\sigma_{\\pi^{\\star}}^{2}\\leq\\sigma^{2}$ and any (offline or online) $I L$ algorithm must have $\\begin{array}{r}{J(\\pi^{\\star})-J(\\widehat{\\pi})\\gtrsim\\sqrt{\\frac{\\sigma^{2}}{n}}}\\end{array}$ with constant probability. ", "page_idx": 8}, {"type": "text", "text": "Nonetheless, it is possible to obtain linear-in- $H$ sample complexity for dense rewards under a recoverability-like condition. Let us define the signed recoverability constant via $\\begin{array}{r l}{\\widetilde{\\mu}}&{{}=}\\end{array}$ $\\begin{array}{r l}{\\operatorname*{max}_{\\boldsymbol{x}\\in\\mathcal{X},\\boldsymbol{a}\\in\\mathcal{A},\\boldsymbol{h}\\in[H]}\\big|\\big(Q_{\\boldsymbol{h}}^{\\pi^{\\star}}(\\boldsymbol{x},\\pi_{\\boldsymbol{h}}^{\\star}(\\boldsymbol{x}))-Q_{\\boldsymbol{h}}^{\\pi^{\\star}}(\\boldsymbol{x},\\boldsymbol{a})\\big|}&{{}}\\end{array}$ . Note that $\\widetilde{\\mu}\\in[0,R]$ , and that $\\widetilde{\\mu}\\geq\\mu$ , since this version counts actions $a$ that outperform $\\pi^{\\star}$ , not just those that underperform. It is immediate to see that $\\sigma_{\\pi^{\\star}}^{2}\\leq\\widetilde{\\mu}^{2}H$ . Hence, even if $R=H$ , as long as $\\widetilde{\\mu}=O(1)$ , Corollary 3.1 yields $J(\\pi^{\\star})-J(\\widehat\\pi)\\lesssim$ ${\\sqrt{H\\log(|\\Pi|)/n}}$ , so that $O\\big(\\frac{H\\log|\\Pi|}{\\varepsilon^{2}}\\big)$ trajectories su f fice to learn an $\\varepsilon$ -optimal policy.9 ", "page_idx": 8}, {"type": "text", "text": "See Appendix $_\\mathrm{H}$ for further results concerning tightness of Theorem 3.1, including instance-dependent lower bounds. ", "page_idx": 8}, {"type": "text", "text": "Consequences for online versus offline IL. The lower bound in Theorem G.1 holds even for online imitation learning algorithms. Thus, similar to the deterministic setting, there is no online IL algorithm that improves upon Theorem 3.1 uniformly for all policy classes. This means that even for stochastic experts, online imitation learning cannot improve upon offline imitation learning in the realizable setting without further assumptions (e.g., no parameter sharing) on the policy class under consideration. ", "page_idx": 9}, {"type": "text", "text": "Proof sketch for Theorem 3.1. When the expert is stochastic, the trajectory-wise distance in Eq. (6), is no longer useful (i.e., $\\rho(\\pi^{\\star}\\parallel\\pi^{\\star})\\neq0,$ , which necessitates a more information-theoretic analysis. Our starting point is the following scale-sensitive change-of-measure lemma for Hellinger distance. ", "page_idx": 9}, {"type": "text", "text": "Lemma 3.1 (Change-of-measure for Hellinger distance [34, 35]). Let $\\mathbb{P}$ and $\\mathbb{Q}$ be probability distributions over a measurable space $(\\mathcal{X},\\mathcal{F})$ . Then for all functions $h:\\mathcal{X}\\to\\mathbb{R}$ , ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\mathbb{E}_{\\mathbb{P}}[h(X)]-\\mathbb{E}_{\\mathbb{Q}}[h(X)]|\\leq\\sqrt{\\frac{1}{2}(\\mathbb{E}_{\\mathbb{P}}[h^{2}(X)]+\\mathbb{E}_{\\mathbb{Q}}[h^{2}(X)])\\cdot D_{\\mathbb{H}}^{2}(\\mathbb{P},\\mathbb{Q})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "In particular, i ${{\\u{r}}_{h}}\\in[0,R]$ almost surely, then $\\begin{array}{r}{\\mathbb{E}_{\\mathbb{P}}[h(X)]\\leq2\\,\\mathbb{E}_{\\mathbb{Q}}[h(X)]+R\\cdot D_{\\mathbb H}^{2}(\\mathbb{P},\\mathbb{Q}).}\\end{array}$ ", "page_idx": 9}, {"type": "text", "text": "We sketch how to use Lemma 3.1 to prove a weaker version of Theorem 3.1, and defer the full proof, which builds on this argument, to Appendix G.1. Define the sum of advantages for a trajectory $o=(x_{1},a_{1}),\\dots,(x_{H},a_{H})$ via $\\begin{array}{r}{\\Delta(o)=\\sum_{h=1}^{H}Q_{h}^{\\pi^{\\star}}(x_{h},\\pi_{h}^{\\star}(x_{h}))-Q_{h}^{\\pi^{\\star}}(x_{h},a_{h})}\\end{array}$ . By the performance difference lemma, we can write $J(\\pi^{\\star})-J(\\widehat\\pi)=\\mathbb{E}^{\\widehat\\pi}[\\Delta(o)]$ , so applying Eq. (8) yields ", "page_idx": 9}, {"type": "equation", "text": "$$\nJ(\\pi^{\\star})-J(\\widehat\\pi)=\\mathbb{E}^{\\widehat\\pi}[\\Delta(o)]\\lesssim\\mathbb{E}^{\\pi^{\\star}}[\\Delta(o)]+\\sqrt{(\\mathbb{E}^{\\widehat\\pi}[\\Delta^{2}(o)]+\\mathbb{E}^{\\pi^{\\star}}[\\Delta^{2}(o)])\\cdot D_{\\mathsf{H}}^{2}(\\mathbb{P}^{\\widehat\\pi},\\mathbb{P}^{\\pi^{\\star}})}.\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "From here, we observe that $\\mathbb{E}^{\\pi^{\\star}}[\\Delta(o)]=0$ and $\\mathbb{E}^{\\pi^{\\star}}\\left[\\Delta^{2}(o)\\right]=\\sigma_{\\pi^{\\star}}^{2}$ (this follows because advantages are a martingale difference sequence under $\\mathbb{P}^{\\pi^{\\star}}$ ), so all that remains is to bound the term $\\mathbb{E}^{\\widehat{\\pi}}\\left[\\Delta^{2}(o)\\right]$ . A crude approach is to observe that $|\\Delta(o)|\\leq\\widetilde{\\mu}H$ , so that applying Lemma 3.1 gives $\\lesssim\\mathbb{E}^{\\pi^{\\star}}\\left[\\Delta^{2}(o)\\right]+$ $(\\widetilde{\\mu}H)^{2}{\\cdot}D_{\\mathsf{H}}^{2}(\\mathbb{P}^{\\widehat{\\pi}},\\mathbb{P}^{\\pi^{\\star}})$ , and consequently $J(\\pi^{\\star})\\!-\\!J(\\widehat\\pi)\\lesssim\\sqrt{\\sigma_{\\pi^{\\star}}^{2}\\cdot D_{\\mathsf{H}}^{2}\\!\\left(\\mathbb{P}^{\\widehat\\pi},\\mathbb{P}^{\\pi^{\\star}}\\right)}\\!+\\!\\widetilde\\mu H\\!\\cdot\\!D_{\\mathsf{H}}^{2}\\!\\left(\\mathbb{P}^{\\widehat\\pi},\\mathbb{P}^{\\pi^{\\star}}\\right)$ . This falls short of Eq. (9) due to the suboptimal lower-order term, which does not recover Theorem 2.1 when $\\pi^{\\star}$ is deterministic $(\\sigma_{\\pi^{\\star}}^{2}=0)$ . The full proof in Appendix G.1 corrects this disparity using a subtle and significantly more involved argument based on stopping times and martingale concentration. ", "page_idx": 9}, {"type": "text", "text": "4 Discussion and Additional Results ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Our results clarify the role of horizon in offilne and online $\\mathrm{IL}$ , and show that\u2014at least under standard theoretical assumptions\u2014the gap between online and offline $\\mathrm{IL}$ is smaller than previously thought. ", "page_idx": 9}, {"type": "text", "text": "Benefits of online interaction Instabilities of offline IL [60, 27, 13] and benefits of online IL [73, 52, 38, 6, 26, 51, 7, 108, 59] likely arise in practice, but existing assumptions in theoretical research on imitation learning appear be too coarse to give insights into the true nature of these phenomena. Toward developing a fine-grained, problem-dependent understanding of algorithms and complexity for IL, in Appendix I, we highlight several special cases in which online interaction does lead to benefits over offline imitation learning, but in a policy class-dependent fashion not captured by existing theory. We identify three phenomena that can lead to improved sample complexity: (i) representational benefits; (ii) value-based feedback; and (iii) exploration. ", "page_idx": 9}, {"type": "text", "text": "Further directions. Additional directions for future research include (i) developing and analyzing imitation learning algorithms under control-theoretic assumptions that more directly capture practical notions of instability [61, 88, 13, 14], and (ii) developing a more refined theory in the context of language models, via the connection in Appendix B.3. For both settings, an important question is to understand whether the notion of supervised learning error $D_{\\mathsf{H}}^{2}\\big(\\mathbb{P}^{\\widehat{\\pi}},\\mathbb{P}^{\\pi^{\\star}}\\big)$ we consider is a suitable proxy for real-world performance, or whether more refined notions are required. ", "page_idx": 9}, {"type": "text", "text": "Additional results. Secondary results deferred to the appendix for space include (i) examples and additional guarantees for LogLossBC and LogLossDagger (Appendix E); and (ii) additional lower bounds and results concerning the tightness of Theorems 2.2 and 3.1 (Appendix H). ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We thank Jordan Ash, Audrey Huang, Akshay Krishnamurthy, Max Simchowitz, and Cyril Zhang for many helpful discussions. We thank Drew Bagnell for valuable comments and pointers to related work. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] P. Abbeel and A. Y. Ng. Apprenticeship learning via inverse reinforcement learning. In Proceedings of the twenty-first international conference on Machine learning, page 1, 2004.   \n[2] A. Agarwal, N. Jiang, and S. M. Kakade. Reinforcement learning: Theory and algorithms. 2019.   \n[3] A. Ayoub, K. Wang, V. Liu, S. Robertson, J. McInerney, D. Liang, N. Kallus, and C. Szepesv\u00e1ri. Switching the loss reduces the cost in batch reinforcement learning. arXiv preprint arXiv:2403.05385, 2024.   \n[4] M. G. Azar, I. Osband, and R. Munos. Minimax regret bounds for reinforcement learning. In International Conference on Machine Learning, pages 263\u2013272, 2017.   \n[5] G. Bachmann and V. Nagarajan. The pitfalls of next-token prediction. arXiv preprint arXiv:2403.06963, 2024.   \n[6] M. Bansal, A. Krizhevsky, and A. Ogale. Chauffeurnet: Learning to drive by imitating the best and synthesizing the worst. arXiv preprint arXiv:1812.03079, 2018.   \n[7] M. Barnes. World scale inverse reinforcement learning in Google Maps. https://research. google/blog/world-scale-inverse-reinforcement-learning-in-google-maps/, 2023. [Online; accessed 26-Oct-2024].   \n[8] P. L. Bartlett, D. J. Foster, and M. J. Telgarsky. Spectrally-normalized margin bounds for neural networks. In Advances in Neural Information Processing Systems, 2017.   \n[9] M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47: 253\u2013279, 2013.   \n[10] A. Beygelzimer, V. Dani, T. Hayes, J. Langford, and B. Zadrozny. Error limiting reductions between classification tasks. In Proceedings of the 22nd international conference on Machine learning, pages 49\u201356, 2005.   \n[11] A. Beygelzimer, J. Langford, L. Li, L. Reyzin, and R. Schapire. Contextual bandit algorithms with supervised learning guarantees. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, pages 19\u201326, 2011.   \n[12] S. Bhattamishra, K. Ahuja, and N. Goyal. On the ability and limitations of transformers to recognize formal languages. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7096\u20137116, 2020.   \n[13] A. Block, D. J. Foster, A. Krishnamurthy, M. Simchowitz, and C. Zhang. Butterfly effects of sgd noise: Error amplification in behavior cloning and autoregression. International Conference on Learning Representations (ICLR), 2024.   \n[14] A. Block, A. Jadbabaie, D. Pfrommer, M. Simchowitz, and R. Tedrake. Provable guarantees for generative behavior cloning: Bridging low-level stability and high-level behavior. Advances in Neural Information Processing Systems, 36, 2024.   \n[15] M. Bojarski, D. Del Testa, D. Dworakowski, B. Firner, B. Flepp, P. Goyal, L. D. Jackel, M. Monfort, U. Muller, and J. Zhang. End to end learning for self-driving cars. arXiv preprint arXiv:1604.07316, 2016.   \n[16] K. Brantley, W. Sun, and M. Henaff. Disagreement-regularized imitation learning. In International Conference on Learning Representations, 2019.   \n[17] M. Braverman, X. Chen, S. Kakade, K. Narasimhan, C. Zhang, and Y. Zhang. Calibration, entropy rates, and memory in language models. In International Conference on Machine Learning, pages 1089\u20131099. PMLR, 2020.   \n[18] C. L. Canonne. A short note on learning discrete distributions. arXiv preprint arXiv:2002.11457, 2020.   \n[19] N. Cesa-Bianchi and G. Lugosi. Prediction, Learning, and Games. Cambridge University Press, New York, NY, USA, 2006. ISBN 0521841089.   \n[20] J. D. Chang, M. Uehara, D. Sreenivas, R. Kidambi, and W. Sun. Mitigating covariate shift in imitation learning via offline data without great coverage. Advances in Neural Information Processing Systems, 2021.   \n[21] J. D. Chang, K. Brantley, R. Ramamurthy, D. Misra, and W. Sun. Learning to generate better than your llm. arXiv preprint arXiv:2306.11816, 2023.   \n[22] C.-A. Cheng and B. Boots. Convergence of value aggregation for imitation learning. In International Conference on Artificial Intelligence and Statistics, pages 1801\u20131809. PMLR, 2018.   \n[23] C.-A. Cheng, X. Yan, E. Theodorou, and B. Boots. Accelerating imitation learning with predictive models. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 3187\u20133196. PMLR, 2019.   \n[24] C.-A. Cheng, A. Kolobov, and A. Agarwal. Policy improvement via imitation of multiple oracles. Advances in Neural Information Processing Systems, 33:5587\u20135598, 2020.   \n[25] C. Chi, S. Feng, Y. Du, Z. Xu, E. Cousineau, B. Burchfiel, and S. Song. Diffusion policy: Visuomotor policy learning via action diffusion. arXiv preprint arXiv:2303.04137, 2023.   \n[26] S. Choudhury, M. Bhardwaj, S. Arora, A. Kapoor, G. Ranade, S. Scherer, and D. Dey. Datadriven planning via imitation learning. The International Journal of Robotics Research, 37 (13-14):1632\u20131672, 2018.   \n[27] P. De Haan, D. Jayaraman, and S. Levine. Causal confusion in imitation learning. Advances in neural information processing systems, 32, 2019.   \n[28] D. L. Donoho and R. C. Liu. Geometrizing rates of convergence, II. The Annals of Statistics, pages 633\u2013667, 1991.   \n[29] J. Farebrother, J. Orbay, Q. Vuong, A. A. Ta\u00efga, Y. Chebotar, T. Xiao, A. Irpan, S. Levine, P. S. Castro, and A. Faust. Stop regressing: Training value functions via classification for scalable deep rl. arXiv preprint arXiv:2403.03950, 2024.   \n[30] C. Finn, T. Yu, T. Zhang, P. Abbeel, and S. Levine. One-shot visual imitation learning via meta-learning. In Conference on robot learning, pages 357\u2013368. PMLR, 2017.   \n[31] P. Florence, C. Lynch, A. Zeng, O. A. Ramirez, A. Wahid, L. Downs, A. Wong, J. Lee, I. Mordatch, and J. Tompson. Implicit behavioral cloning. In Conference on Robot Learning, pages 158\u2013168. PMLR, 2022.   \n[32] D. J. Foster and A. Krishnamurthy. Efficient first-order contextual bandits: Prediction, allocation, and triangular discrimination. Neural Information Processing Systems (NeurIPS), 2021.   \n[33] D. J. Foster and A. Rakhlin. Foundations of reinforcement learning and interactive decision making. arXiv preprint arXiv:2312.16730, 2023.   \n[34] D. J. Foster, S. M. Kakade, J. Qian, and A. Rakhlin. The statistical complexity of interactive decision making. arXiv preprint arXiv:2112.13487, 2021.   \n[35] D. J. Foster, A. Rakhlin, A. Sekhari, and K. Sridharan. On the complexity of adversarial decision making. Advances in Neural Information Processing Systems, 35:35404\u201335417, 2022.   \n[36] D. J. Foster, Y. Han, J. Qian, and A. Rakhlin. Online estimation via offline estimation: An information-theoretic framework. arXiv preprint arXiv:2404.10122, 2024.   \n[37] N. Golowich, A. Rakhlin, and O. Shamir. Size-independent sample complexity of neural networks. In Conference On Learning Theory, pages 297\u2013299. PMLR, 2018.   \n[38] S. Gupta, J. Davidson, S. Levine, R. Sukthankar, and J. Malik. Cognitive mapping and planning for visual navigation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2616\u20132625, 2017.   \n[39] M. Hahn. Theoretical limitations of self-attention in neural sequence models. Transactions of the Association for Computational Linguistics, 8:156\u2013171, 2020.   \n[40] S. Hanneke. Theory of disagreement-based active learning. Foundations and Trends\u00ae in Machine Learning, 7(2-3):131\u2013309, 2014.   \n[41] A. Havens and B. Hu. On imitation learning of linear control policies: Enforcing stability and robustness constraints via lmi conditions. In 2021 American Control Conference (ACC), pages 882\u2013887. IEEE, 2021.   \n[42] J. Ho and S. Ermon. Generative adversarial imitation learning. Advances in neural information processing systems, 29, 2016.   \n[43] A. Holtzman, J. Buys, L. Du, M. Forbes, and Y. Choi. The curious case of neural text degeneration. arXiv preprint arXiv:1904.09751, 2019.   \n[44] A. Hussein, M. M. Gaber, E. Elyan, and C. Jayne. Imitation learning: A survey of learning methods. ACM Computing Surveys (CSUR), 50(2):1\u201335, 2017.   \n[45] A. Hussein, E. Elyan, M. M. Gaber, and C. Jayne. Deep imitation learning for 3d navigation tasks. Neural computing and applications, 29:389\u2013404, 2018.   \n[46] B. Ibarz, J. Leike, T. Pohlen, G. Irving, S. Legg, and D. Amodei. Reward learning from human preferences and demonstrations in atari. Advances in neural information processing systems, 31, 2018.   \n[47] N. Jiang and A. Agarwal. Open problem: The dependence of sample complexity lower bounds on planning horizon. In Conference On Learning Theory, pages 3395\u20133398. PMLR, 2018.   \n[48] C. Jin, Q. Liu, and S. Miryoosef.i Bellman eluder dimension: New rich classes of RL problems, and sample-efficient algorithms. Neural Information Processing Systems, 2021.   \n[49] L. Ke, S. Choudhury, M. Barnes, W. Sun, G. Lee, and S. Srinivasa. Imitation learning as f-divergence minimization. In Algorithmic Foundations of Robotics XIV: Proceedings of the Fourteenth Workshop on the Algorithmic Foundations of Robotics 14, pages 313\u2013329. Springer, 2021.   \n[50] L. Ke, J. Wang, T. Bhattacharjee, B. Boots, and S. Srinivasa. Grasping with chopsticks: Combating covariate shift in model-free imitation learning for fine manipulation. In 2021 IEEE International Conference on Robotics and Automation (ICRA), pages 6185\u20136191. IEEE, 2021.   \n[51] M. Kelly, C. Sidrane, K. Driggs-Campbell, and M. J. Kochenderfer. Hg-dagger: Interactive imitation learning with human experts. In 2019 International Conference on Robotics and Automation (ICRA), pages 8077\u20138083. IEEE, 2019.   \n[52] B. Kim, A.-m. Farahmand, J. Pineau, and D. Precup. Learning from limited demonstrations. Advances in Neural Information Processing Systems, 26, 2013.   \n[53] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. International Conference on Learning Representations, 2015.   \n[54] M. Laskey, J. Lee, R. Fox, A. Dragan, and K. Goldberg. Dart: Noise injection for robust imitation learning. In Conference on robot learning, pages 143\u2013156. PMLR, 2017.   \n[55] Y. LeCun. Do large language models need sensory grounding for meaning and understanding. In Workshop on Philosophy of Deep Learning, NYU Center for Mind, Brain, and Consciousness and the Columbia Center for Science and Society, 2023.   \n[56] Y. Li and C. Zhang. On efficient online imitation learning via classification. Advances in Neural Information Processing Systems, 35:32383\u201332397, 2022.   \n[57] Y. Li, J. Song, and S. Ermon. Infogail: Interpretable imitation learning from visual demonstrations. Advances in neural information processing systems, 30, 2017.   \n[58] B. Liu, J. T. Ash, S. Goel, A. Krishnamurthy, and C. Zhang. Transformers learn shortcuts to automata. In The Eleventh International Conference on Learning Representations, 2022.   \n[59] T. G. W. Lum, M. Matak, V. Makoviychuk, A. Handa, A. Allshire, T. Hermans, N. D. Ratliff, and K. Van Wyk. Dextrah-g: Pixels-to-action dexterous arm-hand grasping with geometric fabrics. arXiv preprint arXiv:2407.02274, 2024.   \n[60] U. Muller, J. Ben, E. Cosatto, B. Flepp, and Y. Cun. Off-road obstacle avoidance through end-to-end learning. Advances in neural information processing systems, 18, 2005.   \n[61] D. Pfrommer, T. Zhang, S. Tu, and N. Matni. Tasil: Taylor series imitation learning. Advances in Neural Information Processing Systems, 35:20162\u201320174, 2022.   \n[62] Y. Polyanskiy and Y. Wu. Lecture notes on information theory. 2014.   \n[63] D. A. Pomerleau. Alvinn: An autonomous land vehicle in a neural network. Advances in neural information processing systems, 1, 1988.   \n[64] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.   \n[65] A. Raffin. Rl baselines3 zoo. https://github.com/DLR-RM/rl-baselines3-zoo, 2020.   \n[66] A. Raffin, A. Hill, A. Gleave, A. Kanervisto, M. Ernestus, and N. Dormann. Stable-baselines3: Reliable reinforcement learning implementations. Journal of Machine Learning Research, 22 (268):1\u20138, 2021. URL http://jmlr.org/papers/v22/20-1364.html.   \n[67] N. Rajaraman, L. Yang, J. Jiao, and K. Ramchandran. Toward the fundamental limits of imitation learning. Advances in Neural Information Processing Systems, 33:2914\u20132924, 2020.   \n[68] N. Rajaraman, Y. Han, L. Yang, J. Liu, J. Jiao, and K. Ramchandran. On the value of interaction and function approximation in imitation learning. Advances in Neural Information Processing Systems, 34:1325\u20131336, 2021.   \n[69] N. Rajaraman, Y. Han, L. F. Yang, K. Ramchandran, and J. Jiao. Provably breaking the quadratic error compounding barrier in imitation learning, optimally. arXiv preprint arXiv:2102.12948, 2021.   \n[70] S. Ross and D. Bagnell. Efficient reductions for imitation learning. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 661\u2013668. JMLR Workshop and Conference Proceedings, 2010.   \n[71] S. Ross and J. A. Bagnell. Reinforcement and imitation learning via interactive no-regret learning. arXiv preprint arXiv:1406.5979, 2014.   \n[72] S. Ross, G. Gordon, and D. Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pages 627\u2013635. JMLR Workshop and Conference Proceedings, 2011.   \n[73] S. Ross, N. Melik-Barkhudarov, K. S. Shankar, A. Wendel, D. Dey, J. A. Bagnell, and M. Hebert. Learning monocular reactive uav control in cluttered natural environments. In 2013 IEEE international conference on robotics and automation, pages 1765\u20131772. IEEE, 2013.   \n[74] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.   \n[75] A. Sekhari, K. Sridharan, W. Sun, and R. Wu. Selective sampling and imitation learning via online regression. Advances in Neural Information Processing Systems, 36, 2024.   \n[76] N. M. Shafiullah, Z. Cui, A. A. Altanzaya, and L. Pinto. Behavior transformers: Cloning $k$ modes with one stone. Advances in neural information processing systems, 35:22955\u201322968, 2022.   \n[77] J. Spencer, S. Choudhury, A. Venkatraman, B. Ziebart, and J. A. Bagnell. Feedback in imitation learning: The three regimes of covariate shift. arXiv preprint arXiv:2102.02872, 2021.   \n[78] W. Sun, A. Venkatraman, G. J. Gordon, B. Boots, and J. A. Bagnell. Deeply aggrevated: Differentiable imitation learning for sequential prediction. In International conference on machine learning, pages 3309\u20133318. PMLR, 2017.   \n[79] G. Swamy, S. Choudhury, J. A. Bagnell, and S. Wu. Of moments and matching: A gametheoretic framework for closing the imitation gap. In International Conference on Machine Learning, pages 10022\u201310032. PMLR, 2021.   \n[80] G. Swamy, N. Rajaraman, M. Peng, S. Choudhury, J. Bagnell, S. Z. Wu, J. Jiao, and K. Ramchandran. Minimax optimal online imitation learning via replay estimation. Advances in Neural Information Processing Systems, 35:7077\u20137088, 2022.   \n[81] U. Syed and R. E. Schapire. A game-theoretic approach to apprenticeship learning. Advances in neural information processing systems, 20, 2007.   \n[82] U. Syed and R. E. Schapire. A reduction from apprenticeship learning to classification. Advances in neural information processing systems, 23, 2010.   \n[83] U. Syed, M. Bowling, and R. E. Schapire. Apprenticeship learning using linear programming. In Proceedings of the 25th international conference on Machine learning, pages 1032\u20131039, 2008.   \n[84] D. Tiapkin, D. Belomestny, D. Calandriello, E. Moulines, A. Naumov, P. Perrault, M. Valko, and P. Menard. Demonstration-regularized rl. International Conference on Learning Representations, 2024.   \n[85] E. Todorov, T. Erez, and Y. Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026\u20135033. IEEE, 2012. doi: 10.1109/IROS.2012.6386109.   \n[86] M. Towers, J. K. Terry, A. Kwiatkowski, J. U. Balis, G. d. Cola, T. Deleu, M. Goul\u00e3o, A. Kallinteris, A. KG, M. Krimmel, R. Perez-Vicente, A. Pierr\u00e9, S. Schulhoff, J. J. Tai, A. T. J. Shen, and O. G. Younis. Gymnasium, Mar. 2023. URL https://zenodo.org/record/ 8127025.   \n[87] S. Tu, R. Frostig, and M. Soltanolkotabi. Learning from many trajectories. arXiv preprint arXiv:2203.17193, 2022.   \n[88] S. Tu, A. Robey, T. Zhang, and N. Matni. On the sample complexity of stability constrained imitation learning. In Learning for Dynamics and Control Conference, pages 180\u2013191. PMLR, 2022.   \n[89] S. A. van de Geer. Empirical Processes in M-Estimation. Cambridge University Press, 2000.   \n[90] O. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Mathieu, A. Dudzik, J. Chung, D. H. Choi, R. Powell, T. Ewalds, P. Georgiev, et al. Grandmaster level in starcraft ii using multi-agent reinforcement learning. Nature, 575(7782):350\u2013354, 2019.   \n[91] M. J. Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cambridge University Press, 2019.   \n[92] K. Wang, K. Zhou, R. Wu, N. Kallus, and W. Sun. The benefits of being distributional: Small-loss bounds for reinforcement learning. Advances in Neural Information Processing Systems, 36, 2023.   \n[93] K. Wang, O. Oertell, A. Agarwal, N. Kallus, and W. Sun. More beneftis of being distributional: Second-order bounds for reinforcement learning. arXiv preprint arXiv:2402.07198, 2024.   \n[94] R. Wang, S. S. Du, L. F. Yang, and S. M. Kakade. Is long horizon reinforcement learning more difficult than short horizon reinforcement learning? Neural Information Processing Systems (NeurIPS), 2020.   \n[95] K. Wen, Y. Li, B. Liu, and A. Risteski. Transformers are uninterpretable with myopic methods: a case study with bounded dyck grammars. Advances in Neural Information Processing Systems, 36, 2024.   \n[96] D. Williams. Probability with martingales. Cambridge university press, 1991.   \n[97] W. H. Wong and X. Shen. Probability inequalities for likelihood ratios and convergence rates of sieve mles. The Annals of Statistics, pages 339\u2013362, 1995.   \n[98] T. Xu, Z. Li, Y. Yu, and Z.-Q. Luo. Understanding adversarial imitation learning in small sample regime: A stage-coupled analysis. arXiv preprint arXiv:2208.01899, 2022.   \n[99] X. Yan, B. Boots, and C.-A. Cheng. Explaining fast improvement in online imitation learning. In Uncertainty in Artificial Intelligence, pages 1874\u20131884. PMLR, 2021.   \n[100] S. Yao, B. Peng, C. Papadimitriou, and K. Narasimhan. Self-attention networks can process bounded hierarchical languages. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 3770\u20133785, 2021.   \n[101] A. Zanette and E. Brunskill. Tighter problem-dependent regret bounds in reinforcement learning without domain knowledge using value function bounds. In International Conference on Machine Learning, pages 7304\u20137312. PMLR, 2019.   \n[102] T. Zhang. From $\\epsilon$ -entropy to KL-entropy: Analysis of minimum information complexity density estimation. The Annals of Statistics, 34(5):2180\u20132210, 2006.   \n[103] T. Zhang, Z. McCarthy, O. Jow, D. Lee, X. Chen, K. Goldberg, and P. Abbeel. Deep imitation learning for complex manipulation tasks from virtual reality teleoperation. In 2018 IEEE international conference on robotics and automation (ICRA), pages 5628\u20135635. IEEE, 2018.   \n[104] Z. Zhang, X. Ji, and S. Du. Is reinforcement learning more difficult than bandits? a nearoptimal algorithm escaping the curse of horizon. In Conference on Learning Theory, pages 4528\u20134531. PMLR, 2021.   \n[105] Z. Zhang, X. Ji, and S. Du. Horizon-free reinforcement learning in polynomial time: the power of stationary policies. In Conference on Learning Theory, pages 3858\u20133904. PMLR, 2022.   \n[106] H. Zhao, J. He, D. Zhou, T. Zhang, and Q. Gu. Variance-dependent regret bounds for linear bandits and reinforcement learning: Adaptivity and computational efficiency. In The Thirty Sixth Annual Conference on Learning Theory, pages 4977\u20135020. PMLR, 2023.   \n[107] R. Zhou, Z. Zihan, and S. S. Du. Sharp variance-dependent bounds in reinforcement learning: Best of both worlds in stochastic and deterministic environments. In International Conference on Machine Learning, pages 42878\u201342914. PMLR, 2023.   \n[108] Z. Zhuang, Z. Fu, J. Wang, C. Atkeson, S. Schwertfeger, C. Finn, and H. Zhao. Robot parkour learning. arXiv preprint arXiv:2309.05665, 2023.   \n[109] B. D. Ziebart, A. L. Maas, J. A. Bagnell, and A. K. Dey. Maximum entropy inverse reinforcement learning. In Aaai, volume 8, pages 1433\u20131438. Chicago, IL, USA, 2008. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Contents of Appendix ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "A Omitted Tables 18 ", "page_idx": 16}, {"type": "text", "text": "B Additional Related Work 18 ", "page_idx": 16}, {"type": "text", "text": "B.1 Theory of Imitation Learning and Reinforcement Learning . . . 18   \nB.2 Empirical Research on Imitation Learning . . . . 19   \nB.3 Autoregressive Language Modeling . . 20 ", "page_idx": 16}, {"type": "text", "text": "C Experiments 20 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "C.1 Experimental Setup 20   \nC.2 Results . 23 ", "page_idx": 16}, {"type": "text", "text": "D Technical Tools 25 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "D.1 Tail Bounds 25   \nD.2 Information Theory . 26   \nD.3 Reinforcement Learning 26   \nD.4 Maximum Likelihood Estimation . 27 ", "page_idx": 16}, {"type": "text", "text": "I Proofs and Supporting Results 28 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "E Examples and Supporting Results from Section 2 and Section 3 28 ", "page_idx": 16}, {"type": "text", "text": "E.1 General Guarantees and Examples for Log-Loss Behavior Cloning . . 28   \nE.2 Online IL Framework and Sample Complexity Bounds for Log-Loss Dagger . . . . . 31   \nProofs from Section 2 34   \nF.1 Proof of Theorem 2.1 34   \nF.2 Proof of Theorem 2.2 36   \nG.1 Proof of Theorem 3.1 37   \nG.2 Formal Statement and Proof of Theorem G.1 44   \nG.3 Additional Proofs . 46 ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "II Additional Results 47 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "H Additional Lower Bounds 47 ", "page_idx": 16}, {"type": "text", "text": "H.1 Lower Bounds for Online Imitation Learning in Active Interaction Model 47   \nH.2 An Instance-Dependent Lower Bound for Stochastic Experts . . . . . 49   \nH.3 Tightness of the Hellinger Distance Reduction . . . 51 ", "page_idx": 16}, {"type": "text", "text": "I Benefits of Online Interaction 53 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "I.1 The Role of Misspecification . . 53   \nI.2 Representational Benefits 53   \nI.3 Benefits of Value-Based Feedback 54   \nI.4 Benefits from Exploration . 55   \nI.5 Proofs . 55 ", "page_idx": 16}, {"type": "table", "img_path": "8KPyJm4gt5/tmp/6ea5687bc5708bbc1a66aa71684d5659b69c52c8c5c460292bef8c1a9a27303f.jpg", "table_caption": [], "table_footnote": ["Table 2: Summary of upper bounds for stochastic experts (Corollary 3.1). Each cell denotes the expected regret of a policy learned with LogLossBC; lower bounds are more nuanced and discussed in Section 3. Here $\\Pi$ is the policy class, $R$ is the cumulative reward range, $H$ is the horizon, $n$ is the number of expert trajectories, $\\sigma_{\\pi^{\\star}}^{2}$ is the variance of the expert policy , and $\\widetilde{\\mu}$ is the signed recoverability parameter ; see Section 3 for definitions.10 "], "page_idx": 17}, {"type": "text", "text": "B Additional Related Work ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "B.1 Theory of Imitation Learning and Reinforcement Learning ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Classical theoretical works in imitation learning, beginning from the work of Ross and Bagnell [70] observes that behavior cloning (for the specific indicator loss in Section 1.1) can incur quadratic dependence on horizon, and shows that online interaction, via algorithms like Dagger and Aggrevate, can obtain improved sample complexity under recoverability-type conditions [70, 72, 71, 78]. Further works along this line include Cheng and Boots [22], Cheng et al. [24, 23], Yan et al. [99], Spencer et al. [77]. ", "page_idx": 17}, {"type": "text", "text": "These papers can be thought of as supervised learning reduction, in the sense that\u2014in the vein of Eq. (1)\u2014they guarantee that the imitation learning performance is controlled by an appropriate notion of supervised learning performance. Notably, this holds for any policy $\\widehat{\\pi}$ , which means that in practice, the rollout performance is good whenever supervised learning succeeds, even if we do not necessarily have a provable guarantee for the generalization of $\\widehat{\\pi}$ (e.g., for neural networks, where understanding generalization is an active area of research). How e ver, as noted throughout this paper and elsewhere [67\u201369], these works typically state regret guarantees in terms of different, often incomparable notions of supervised learning performance, and avoid giving concrete, end-to-end guarantees for specific policy classes of interest. This can make it challenging to objectively evaluate optimality, and to understand whether limitations of specific algorithms are due to suboptimal design choices versus information-theoretic limitations. For example, Li and Zhang [56] show that in some cases, supervised learning oracles that satisfy assumptions required by prior work do not actually exist. ", "page_idx": 17}, {"type": "text", "text": "Minimax sample complexity of imitation learning. More recently, a line of work beginning with Rajaraman et al. [67] revisits the minimax sample complexity of imitation learning, aiming to provide end-to-end sample complexity guarantees and lower bounds, but primarily focused on tabular MDPs and policies [67\u201369, 80]. Notably, Rajaraman et al. [67] show that when $\\Pi$ is the set of all non-stationary policies in a tabular MDP and $R=H$ , online $\\mathrm{IL}$ methods can achieve $O(\\mu H)$ sample complexity, while offline $\\mathrm{IL}$ methods must pay $\\Omega(H^{2})$ ; this is consistent with our findings in Section 2, as $\\dot{\\mathrm{log}}|\\Pi|=\\Omega(H)$ for this setting. Other interesting findings from this line of work include the observation that when the MDP dynamics are known, the sample complexity for offline IL with non-stationary tabular policies can be brought down to $O(H^{3/2})$ . As noted in Section 2, Rajaraman et al. [68] show that offilne IL methods can obtain $O(H)$ sample complexity for linearly parameterized policies under parameter sharing; our analysis of LogLossBC for the special case of deterministic policies shows that it can be viewed as implicitly minimizing the objective they consider. ", "page_idx": 17}, {"type": "text", "text": "Xu et al. [98] also consider the problem of horizon independence in IL. Their work focuses on tabular MDPs and policies, and shows with knowledge of the dynamics, it is possible to achieve horizon dependence for a restricted class of MDPs termed RBAS-MDPs. In contrast, our work achieves horizon independence for general MDPs, without knowledge of dynamics. ", "page_idx": 17}, {"type": "text", "text": "Compared to the works above, we focus on general finite classes \u03a0. Various works on theoretical reinforcement learning [2, 33] have observed that finite classes are a useful test case for general function approximation, because they are arguably the simplest type of policy class from a generalization perspective, yet do not have any additional structure (e.g., linearity) that could lead to spurious conclusions that do not extend to rich function classes like neural networks. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "Recent work of Tiapkin et al. [84] provides generalization guarantees for behavior cloning with the logarithmic loss, but their results scale linearly with the horizon, and thus cannot give tight guarantees for policy classes with parameter sharing. In addition, their results are stated in terms of KL-divergence and, as a consequence, require a lower bound on the action densities for the policy class under consideration. We expect that both of these limitations are inherent to KL divergence. Tiapkin et al. [84] also give variance-dependent bounds on rollout performance similar to Theorem 3.1, but their results require a bound on KL divergence (which is stronger than a bound on Hellinger distance), and thus are unlikely to meaningfully capture optimal horizon dependence. These bounds on rollout performance also do not recover the notion of variance in Theorem 3.1. ", "page_idx": 18}, {"type": "text", "text": "We also mention in passing Sekhari et al. [75], who consider active imitation learning algorithms, and focus on obtaining improved sample complexity with respect to dependence on the accuracy $\\varepsilon$ (as opposed to $H$ ), under strong distributional assumptions in the vein of active learning [40]. ", "page_idx": 18}, {"type": "text", "text": "Inverse reinforcement learning. A long line of research on inverse reinforcement learning and related techniques considers a setting in which either a) the dynamics of the MDP $M^{\\star}$ are known, or b) it is possible to interact with $M^{\\star}$ online (without expert feedback), with empirical [1, 109] and theoretical results [81, 83, 82, 16, 20]. This setting encompasses generative adversarial imitation learning and related moment matching methods [42, 57, 49, 79]. A detailed discussion is out of scope for the present work, but we believe this framework can improve over the sample complexity of offline IL in some but not all situations (e.g., Rajaraman et al. [67]). ", "page_idx": 18}, {"type": "text", "text": "Benefits of logarithmic loss. Our work draws inspiration from Foster and Krishnamurthy [32], who observed that the logarithmic loss can have benefits over square loss when outcomes are heteroskedastic, and used this observation to derive first-order regret bounds for contextual bandits. Subsequent works have extended their analysis technicals to derive first-order regret bounds in various reinforcement learning settings [92, 93, 3].11 To the best of our knowledge, our work is the first to uncover a decision making setting in which switching to the logarithmic loss is beneficial even in a minimax sense. We emphasize that while our analysis uses the information-theoretic machinery introduced in Foster and Krishnamurthy [32] and related work [34, 35], our results are quite specialized to structure of the imitation learning setting, and cannot directly be derived from any of the results in Foster and Krishnamurthy [32], Wang et al. [92, 93], Ayoub et al. [3]. ", "page_idx": 18}, {"type": "text", "text": "Horizon-free reinforcement learning. Our results also take inspiration from the line of research on horizon-independent sample complexity bounds for reinforcement learning [47, 101, 94, 104, 105], as well as a closely related line of research on variance-dependent regret bounds [107, 106, 93].12 These papers provide sample complexity bounds for reinforcement learning that have little or no explicit dependence on horizon whenever rewards are normalized such that h=1 rh \u2208[0, 1]. We consider a simpler setting (imitation learning), but provide guarantees that hold under general function approximation, while the works above are restricted to either tabular MDPs or MDPs with linear/lowrank structure. Nonetheless, our proof of Theorem 3.1 makes use of concentration arguments inspired by Zhang et al. [104, 105]. ", "page_idx": 18}, {"type": "text", "text": "B.2 Empirical Research on Imitation Learning ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Many empirical works have observed compounding error in behavior cloning. Outside of online imitation learning, mitigations include noise injection at data collection time [54, 50] or inverse RL methods that assume knowledge of system dynamics [109]. Other works take a control-theoretic perspective [88, 41, 61, 14], and augment behavior cloning with techniques designed to ensure incremental stability (or other control-theoretic notions of stability) of system. ", "page_idx": 18}, {"type": "text", "text": "Online imitation learning. Many empirical works have noted beneftis of online imitation learning methods like Dagger over classical behavior cloning [73, 52, 38, 6, 26, 51, 7, 108, 59]. These results are not in contradiction to our findings, as they typically do not ablate the effect of the loss function (e.g., [70] uses the squared hinge loss, Ross et al. [72] uses the hinge loss, and Ross et al. [73] uses the square loss). It is also possible that the perceived beneftis arise from factors beyond horizon (e.g., representational benefits), as discussed in Appendix I. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "B.3 Autoregressive Language Modeling ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Autoregressive language modeling with the standard next-token prediction objective [64] can be viewed as an instance of behavior cloning with the logarithmic loss. In this setting, $M^{\\star}$ corresponds to a token-level MDP. Here $\\boldsymbol{\\mathcal{A}}$ is a space or vocabulary of tokens The initial state is $x_{1}=z\\sim P_{0}$ , where $z$ is a prompt or context. Given the prompt, for each $h=1,\\ldots,H$ the action $a_{h}\\in A$ is a new token, which is concatenated to the state via the deterministic dynamics $x_{h+1}\\gets(z,a_{1:h})$ . Via Bayes\u2019 rule, an expert policy ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\pi^{\\star}(a_{1:H}\\mid z)=\\prod_{h=1}^{H}\\pi_{h}^{\\star}(a_{h}\\mid z,a_{1:h-1})=\\prod_{h=1}^{H}\\pi_{h}^{\\star}(a_{h}\\mid x_{h})\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "can represent an arbitrary conditional distribution over sequences, from which a training set ${\\mathcal{D}}=\\{o^{i}\\}$ with $o^{i}=\\left(z^{i},a_{1}^{i},\\cdot\\cdot\\cdot,a_{H}^{i}\\right)$ is generated. With this setup, log-loss behavior cloning ", "page_idx": 19}, {"type": "equation", "text": "$$\n{\\widehat{\\pi}}=\\operatorname{arg\\,max}_{\\pi\\in\\Pi}\\sum_{i=1}^{n}\\sum_{h=1}^{H}\\log(\\pi_{h}(a_{h}^{i}\\mid z^{i},a_{1:h-1}^{i}))\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "is equivalent to the standard next-token prediction objective for unsupervised language model pretraining [64], with the class $\\Pi$ parameterized by a transformer or a similar neural net architecture. In this context, long-range error amplification arising from the next-token prediction objective (often referred to as exposure bias) has been widely observed by prior work [43, 17, 13], and in some cases speculated to be a fundamental limitation [55, 5]. ", "page_idx": 19}, {"type": "text", "text": "Applying our results. To apply our results, consider a fixed reward function $r=\\{r_{h}\\}_{h=1}^{H}$ , which might measure performance for a particular task of interest (e.g., question answering or commonsense reasoning). Then, for a model $\\pi$ , $J(\\pi)$ corresponds to rollout performance at the task for an autoregressively generated sequence (i.e., given $z\\sim P_{0}$ , we sample $a_{h}\\sim\\pi_{h}(\\cdot\\mid z,a_{1:h-1})$ for all $h\\in[\\bar{H}])$ . For this setting, Theorem 3.1 states that ", "page_idx": 19}, {"type": "equation", "text": "$$\nJ(\\pi^{\\star})-J(\\widehat\\pi)\\leq\\widetilde O\\bigg(\\sqrt{\\sigma_{\\pi^{\\star}}^{2}\\cdot D_{\\mathsf{H}}^{2}\\big(\\mathbb{P}^{\\widehat\\pi},\\mathbb{P}^{\\pi^{\\star}}\\big)}+R\\cdot D_{\\mathsf{H}}^{2}\\big(\\mathbb{P}^{\\widehat\\pi},\\mathbb{P}^{\\pi^{\\star}}\\big)\\bigg),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\begin{array}{r}{\\sigma_{\\pi^{\\star}}^{2}=\\sum_{h=1}^{H}\\mathbb{E}^{\\pi^{\\star}}\\left[(Q_{h}^{\\pi^{\\star}}(z,a_{1:h})-V_{h}^{\\pi^{\\star}}(z,a_{1:h-1}))^{2}\\right]}\\end{array}$ . In particular, as long as the cumulative reward for the task is bounded by $R=O(1)$ (e.g., if we receive an episode-level reward $r_{H}=1$ if a question is answered correctly, and receive zero reward otherwise), the rollout performance has $n o$ explicit dependence on the sequence length, except through the generalization error $D_{\\mathsf{H}}^{2}\\big(\\mathbb{P}^{\\widehat{\\pi}},\\mathbb{P}^{\\pi^{\\star}}\\big)$ . In light of this result, we expect that error amplification observed in practice may arise from challenges in minimizing the generalization error $D_{\\mathsf{H}}^{2}\\big(\\mathbb{P}^{\\widehat{\\pi}},\\mathbb{P}^{\\pi^{\\star}}\\big)$ itself (e.g., architecture, data generation process, optimization [17, 13]), rather than fundamental limits of next-token prediction. ", "page_idx": 19}, {"type": "text", "text": "C Experiments ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we validate our theoretical results empirically. We first provide a detailed overview of our experimental setup, including the control and natural language tasks we consider, then present empirical results for each task individually. We ran all of our experiments on NVidia V100 GPUs. Training time for each experiment varied by environment, but all were less than 6 hours. ", "page_idx": 19}, {"type": "text", "text": "C.1 Experimental Setup ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We evaluate the effect of horizon on the performance of LogLossBC in three environments. We begin by describing our training and evaluation protocol (which is agnostic to the environment under consideration), then provide details for each environment. ", "page_idx": 19}, {"type": "text", "text": "In each experiment, we begin with an expert policy $\\pi^{\\star}$ (which is always a neural network; details below) and construct an offline dataset by rolling out with it $n$ times for $H$ timesteps per episode. To train the imitator policy $\\widehat{\\pi}$ , we use the same architecture as the expert, but randomly initialize the weights and use stochastic gradient descent with the Adam optimizer to minimize the LogLossBC objective for the offline dataset; this setup ensures that the realizability assumption used by our main results (Assumption 1.1) is satisfied. We repeat this entire process for varying values of $H$ . ", "page_idx": 19}, {"type": "image", "img_path": "8KPyJm4gt5/tmp/5008def55d7bcacba590a826ca23489c7e2e3238a1812a2de68dd407ff151cce.jpg", "img_caption": ["Figure 2: Dependence of expected regret on the horizon for multiple choices for the number of imitator trajectories $n$ . (a) Continuous control environment Walker2d-v4. (b) Discrete Atari environment BeamriderNoFrameskip- $\\cdot\\boldsymbol{\\mathsf{v4}}$ . For both environments, increasing the horizon does not lead to a significant increase in regret, as predicted by our theory. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "To evaluate the regret $J(\\pi^{\\star})-J(\\widehat\\pi)$ after training, we approximate the average reward of the imitator policy $\\widehat{\\pi}$ by selecting new random seeds and collecting $n$ trajectories of length $H$ by rolling out with $\\widehat{\\pi}$ ; we approximate the average reward of the expert $\\pi^{\\star}$ in the same fashion, and we also compute several auxiliary performance measures (details below) that aim to capture the distance between $\\widehat{\\pi}$ and $\\pi^{\\star}$ . In all environments, we normalize rewards so that the average reward of the expert is at most 1, in order to bring us to the sparse reward setting in Section 1.1 and keep the range of the possible rewards constant as a function of the (varying) horizon. ", "page_idx": 20}, {"type": "text", "text": "We consider four diverse environments, with the aim of evaluating LogLossBC in qualitatively different domains: (i) Walker2d, a classical continuous control task from MuJoCo [86, 85] where the learner attempts to make a stick figure-like agent walk to the right by controlling its joints; (ii) Beamrider, a standard discrete-action RL task from the Atari suite [9], where the learner attempts to play the game of Beamrider; (iii) Car, a top-down discrete car racing environment where the car has to avoid obstacles to reach a goal, and (iv) Dyck, an autoregressive language generation task where the agent is given a sequence of brackets in $\\{\\{,\\},[,],(,)\\}$ and has to close all open brackets in the correct order. ", "page_idx": 20}, {"type": "text", "text": "We emphasize diversity in task selection in order to demonstrate the generality of our results, covering discrete and continuous actions spaces, as well as both control and language generation. For some of the environment (Walker2d, Beamrider), the task is intended to be \u201cstateless\u201d, in the sense that varying the horizon $H$ does not change the difficulty of the task itself (e.g., complexity of the expert policy $\\pi^{\\star}$ ), allowing for an honest evaluation of the difficulty of the learning problem as we vary the horizon $H$ . For other domains, such as Dyck, horizon dependence is more nuanced, as here the capacity required to represent the expert grows as the horizon increases; this manifests itself in our theoretical results through the realizability condition (Assumption 1.1), which necessitates a more complex function class $\\Pi$ as $H$ increases. ", "page_idx": 20}, {"type": "text", "text": "We now provide details for our experimental setup for each environment. ", "page_idx": 20}, {"type": "text", "text": "Walker2d. We use the Gymnasium [86] environment Walker2d-v4, which has continuous state and action spaces of dimensions 17 and 6 respectively. The agent is rewarded for moving to the right and staying alive as well as being penalized for excessively forceful actions; because we vary the horizon $H$ , in order to make the comparison fair, we normalize the rewards so that our trained expert always has average reward 1. Our expert is a depth-2 MLP with width 64. We use the Stable-Baselines3 [66] implementation of the Proximal Policy Optimization (PPO) algorithm [74] with default settings to train the expert for 500K steps. The policy\u2019s action distribution is Gaussian, with the mean and covariance determined by the MLP; we use this for computation of the logarithmic loss. For data collection, we enforce a deterministic expert by always playing the mean of the Gaussian distribution produced by their policy. Our imitator policy uses the same architecture as the expert policy, with the weights re-initialized randomly. We train the imitator using the logarithmic loss by default, but as an ablation, we also evaluate the effect of training with the mean-squared-error loss on the Euclidean norm over the actions. We train using the Adam optimizer [53] with a learning rate of $10^{-3}$ and a batch size of 128. We stop training early based on the validation loss on a held out set of expert trajectories. Note that the expert and imitator policies above are both stationary policies. ", "page_idx": 20}, {"type": "image", "img_path": "8KPyJm4gt5/tmp/4a93026d6f4d1be9dbe03484eddf2cdb04cb8f85774b82495e8e7aa5bf2fc3a6.jpg", "img_caption": ["Figure 3: (a) Relationship between the number of expert trajectories and expected regret for the Dyck environment multiple choices of horizon $H$ . The expert is trained to produce valid Dyck words of length $H$ , and the imitator\u2019s ability to generate a valid word is evaluated. We find that regret increases as a function of $H$ . (b) Logarithm of the product of weight matrix norms for the expert policy network as a function of $H$ , for Dyck and Car environments. The log-product-norm acts as a proxy for complexity for the class $\\Pi$ ; we rescale such that log-product-norm at $H=10$ is 1.0 for both domains. For Dyck, we find that as $H$ increases, the complexity of $\\Pi$ required to represent the expert policy (as measured by the log-product-norm) also increases, explaining the increasing regret in (a). However, the gain in log-product-norm for the Car domain is much lower, which is in line with the fact that the regret for the Car domain exhibits only mild scaling with horizon. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "Beamrider. We use the Gymnasium environment BeamRiderNoFrameskip-v4, which has 9 discrete actions and a $210\\mathrm{x}160\\mathrm{x}3$ image as the state; the rewards are computed as a function of how many enemies are destroyed. As in the case of the previous setup, we account for the varying of $H$ by normalizing expert rewards to be 1. Here we do not train our expert ourselves, but instead use the trained PPO agent provided by Raffin [65], which is a convolutional neural network. We use the same architecture for our imitator policy, with the weights re-initialized randomly. Here, the expert (and imitator) policies map the observation to a point on the probability simplex over actions, and so logarithmic loss computation is immediate. Similar to the case of Walker2d, we enforce a deterministic expert for collecting trajectories by taking the action with maximal probability. We then train our imitators using the same setup as in the Walker2d environment. As with Walker2d, the expert and imitator here are both stationary policies. ", "page_idx": 21}, {"type": "text", "text": "Car. We introduce a simple top-down navigation task where the agent is a \u201ccar\u201d that always moves forward by one step, but can take actions to move left, right, or remain in its lane to avoid obstacles and reach the desired destination. There are $M$ possible lanes. At timestep $h\\in[H+1]$ , if the agent is in lane $i\\in[M]$ , then the agent\u2019s state is $(i,h)$ . We view the state space as a $\\dot{M}\\times(\\dot{H}+1)$ grid; a given point $(i,j)$ in the grid can be empty, or contain an obstacle, or contain the agent. The agent\u2019s action space consists of 3 possible actions: stay in the current lane $((i,h)\\mapsto(i,h^{+}1))$ , move one step left $((i,h)\\mapsto(i-1,\\bar{h}+1))$ , or move one step right $((i,h)\\mapsto(i+1,h+1))$ . If the agent\u2019s action causes it to collide with an obstacle or the boundary of the grid, it is sent to an absorbing state. The agent gets a reward of 1 for reaching the goal state for the first time, and a reward of 0 otherwise. When the agent occupies a state $(i,h)$ , it observes an image-based observation $x_{h}$ showing the state of all lanes for $V$ steps ahead where $V$ is the size of the viewing field. At the start of each episode, we randomly sample obstacles positions, the start position, and the goal position. The goal can be reached after $H$ actions, and it is always possible to reach the goal. ", "page_idx": 21}, {"type": "text", "text": "Dyck. In addition to the RL environments above, we evaluate LogLossBC for autoregressive language generation with transformers (cf. Appendix B.3), where the goal of the \u201cagent\u201d is to complete a valid word of a given length in a Dyck language; this has emerged as a popular sandbox for understanding the nuances of autoregressive text generation in theory [100, 39, 12] and empirically [58, 95]. We recall that a Dyck language $\\mathsf{D y c k}_{k}$ consists of $2k$ matched symbols thought of as open and closed parentheses, with concatenations being valid words if the parentheses are closed in the correct order. For example, if we define the space of characters as $\\cdot\\,\\cdot\\,[\\,]^{\\ast},\\,\\,^{\\ast}[\\,]^{\\ast}$ , and $\\mathbf{\\theta}^{\\bullet}\\{\\mathbf{\\phi}\\}^{\\bullet}$ , then $^{\\bullet}([()])\\{\\}^{\\bullet}$ is a valid word, whereas $^{\\cdot}([)]$ \u2019 and \u2018((({}\u2019 are not. ", "page_idx": 21}, {"type": "image", "img_path": "8KPyJm4gt5/tmp/e70c2cf5524bed7475179d5b6a40b53976aa6f1aa393e0ef98fa312c3e64889b.jpg", "img_caption": ["Figure 4: Dependence of expected regret on the number of expert trajectories for Car environment under varying values for horizon $H$ for log-loss (a) and mean-squared loss $({\\bf b})$ . The expert policy network is trained on a set of $2\\times10^{4}$ episodes generated by an optimal policy via behavior cloning. We use LogLossBC to train imitator policy for varying values of the horizon $H$ and number of trajectories $n$ . For both losses, we find that the expected regret goes down as the number of expert trajectories increases, but degrades slightly as a function of $H$ . "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "Our experiments use the Dyck language $\\mathsf{D y c k_{3}}$ . For our expert, we train an instance of GPT-2 small [64] with 6 layers, 3 heads, and 96 hidden dimensions from scratch to produce valid Dyck words. In particular, the training dataset consists of random Dyck prefixes that require exactly $H$ actions (symbols) to complete. To imitate this expert, we train a GPT-2 small model with the same architecture, but with randomly initialized weights on an offilne dataset of sequences generated by the expert. We assign a reward 1 to each trajectory if the generated word is valid, and assign reward 0 otherwise. We use Adam optimization for training, with our experts trained for 40K iterations in order to ensure their quality. Note that in this environment, the expert and imitator policies are non-stationary, but use parameter sharing via the transformer architecture. ", "page_idx": 22}, {"type": "text", "text": "C.2 Results ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We summarize our main findings below. ", "page_idx": 22}, {"type": "text", "text": "Effect of horizon on regret. Figures 1 and 2 plot the relationship between expected regret and the number of expert trajectories for the Walker2d (MuJoCo), and BeamriderNoFrameskip (Atari) environments, as the horizon $H$ is varied from 50 to 500. For both environments, we find regret is largely independent of the horizon, consistent with our theoretical results. In fact, in the case of BeamriderNoFrameskip, we find that increasing the horizon leads to better regret. To understand this, note that our theory provides horizon-agnostic upper bounds independent of the environment. Our lower bounds are constructed for specific worst-case environments, and not rule out the possibility of improved performance with longer horizons environments with favorable structure. We conjecture that this phenomenon is related to the fact that longer horizons yield fundamentally more data, as the total number of state-action pairs in the expert dataset is equal to $n H$ .13 ", "page_idx": 22}, {"type": "text", "text": "Figure 3(a) plots our findings for the Dyck environment. Here, we see that with the number of trajectories $n$ fixed, regret does increase with $H$ , which might appear to contradict our theory at first glance. However, we note that the policy class itself must become larger as $H$ increases, as the task itself becomes more difficult (equivalently, the supervised learning error $D_{\\mathsf{H}}^{2}\\big(\\mathbb{P}^{\\pi^{\\star}},\\mathbb{P}^{\\widehat\\pi}\\big)$ must grow with $H$ ). As a result, the regret is not expected to be independent of $H$ for this environment, in spit of parameter sharing. To verify whether supervised learning error is indeed the cause for horizon dependence for Dyck, Figure 3(b) plots the logarithm of the product of the Frobenius norms of the weight matrices of the expert for varying values of $H$ , as a proxy for supervised learning performance [8, 37].14 We find that the log-product-norms do in fact grow with $H$ , consistent with the fact that the regret grows with $H$ in this case. ", "page_idx": 22}, {"type": "image", "img_path": "8KPyJm4gt5/tmp/b16d519cdbbc20e382ba0f9aad69d6b80e42abd7f0d3581e605601fbd769564a.jpg", "img_caption": ["Figure 5: Dependence of expected regret on the number of expert trajectories for continuous control environment Walker2d-v4 under varying choices for horizon $H$ . (Left) Behavior cloning with logarithmic loss (LogLossBC); (Right) Behavior cloning with mean squared error (MSE) Loss. Both losses lead to similar performance for this environment, possibly due to Gaussian policy parameterization. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "For the Car environment, we observe similar behavior to the Dyck environment, visualized in Figure 4. We find that performance degrades slightly as a function of the horizon $H$ , but that this increase in regret can be explained by an increase in the log-product-norm (Figure 3(b)). However, the effect is mild compared to Dyck. ", "page_idx": 23}, {"type": "text", "text": "Comparison between log loss and square loss. As an ablation, Figures 4 and 5 compare LogLossBC to the original behavior cloning objective of Pomerleau [63], which uses the mean squared error (MSE) to regress expert actions to observations in the offilne dataset. Focusing on the Walker2d environment (Figure 5) and Car environment (Figure 4) (other environments presented difficulties in training15), we find that performance with the MSE loss is comparable to that of the logarithmic loss. For Walker2d, a possible explanation is that under the Gaussian policy parameterization we use, the MSE loss is the same as the logarithmic loss up to state-dependent heteroskedasticity.16 Another possible explanation is that this is an instance of the phenomenon described in ??. ", "page_idx": 23}, {"type": "text", "text": "Relationship between regret and Hellinger distance to expert. Finally, we directly evaluate the quality of (i) Hellinger distance $D_{\\mathsf{H}}^{2}\\big(\\bar{\\mathbb{P}^{\\pi^{\\star}}},\\mathbb{P}^{\\widehat\\pi}\\big)$ , and (ii) validation loss as proxies for rollout performance. We estimate the Hellinger distance using sample trajectories. Figure 6 displays our findings for Walker2d with $H=n=500$ , where we observe that both metrics, particularly the Hellinger distance, are well correlated with rollout performance, as measured by average reward. In Figure 6a, we see that under LogLossBC, Hellinger distance and validation loss are highly correlated with each other, and negatively correlated with expected reward, thereby acting as excellent proxies for rollout performance. Meanwhile, in Figure 6b, we find that under behavior cloning with the MSE loss, validation error is less well correlated with the expected reward of the imitator policy, as evinced by the cluster in the upper left corner, where there are policies with roughly the same validation loss, but variable expected reward. On the other hand, the Hellinger distance $\\dot{D}_{\\mathsf{H}}^{2}$ still appears to predict the performance of the policy well, as is consistent with our theoretical results. ", "page_idx": 23}, {"type": "image", "img_path": "8KPyJm4gt5/tmp/8fd67d641f6b8246861a1585be1b535e672dd59a39b47df17343a551f9b3cf3f.jpg", "img_caption": [""], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "8KPyJm4gt5/tmp/82758aed2ac789f1c8324969369a19b727448c745a073aac49b8badd5ef534f0.jpg", "img_caption": ["MuJoCo (MSE Loss) "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Figure 6: Evaluation of the quality of (i) Hellinger distance $D_{\\mathsf{H}}^{2}\\big(\\mathbb{P}^{\\pi^{\\star}},\\mathbb{P}^{\\widehat{\\pi}}\\big)$ , and (ii) validation loss as a proxy for rollout reward. We plot Hellinger distance and validation loss against mean reward for a over a single training run for Walker2d environment with $H=500$ and $n=500$ . (a) Results for LogLossBC, where the validation loss and Hellinger distance $D_{\\mathsf{H}}^{2}$ are highly correlated, and serve as good proxies for the expected reward of the policy. (b) Results for MSE loss, where the validation loss is less well correlated with the expected reward (note the cluster in the upper left hand corner), but the Hellinger distance $D_{\\mathsf{H}}^{2}$ remains a good proxy. ", "page_idx": 24}, {"type": "text", "text": "D Technical Tools ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "D.1 Tail Bounds ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Lemma D.1 (e.g., Foster et al. [34]). For any sequence of real-valued random variables $(X_{t})_{t\\leq T}$ adapted to a filtration $(\\mathcal{F}_{t})_{t\\leq T}$ , it holds that with probability at least $1-\\delta_{i}$ , for all $T^{\\prime}\\leq T$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T^{\\prime}}-\\log\\bigl(\\mathbb{E}_{t-1}\\big[e^{-X_{t}}\\big]\\bigr)\\leq\\sum_{t=1}^{T^{\\prime}}X_{t}+\\log(\\delta^{-1}).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Lemma D.2 (Time-uniform Freedman-type inequality). Let $(X_{t})_{t\\leq T}$ be a real-valued martingale difference sequence adapted to a filtration $(\\mathcal{F}_{t})_{t\\leq T}$ . If $|X_{t}|\\ \\leq\\ \\overline{{R}}$ almost surely, then for any $\\eta\\in(0,1/R)$ , with probability at least $1-\\delta_{i}$ , for all ${\\dot{T}}^{\\prime}\\leq T$ . ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T^{\\prime}}X_{t}\\leq\\eta\\sum_{t=1}^{T^{\\prime}}\\mathbb{E}_{t-1}\\big[X_{t}^{2}\\big]+\\frac{\\log(\\delta^{-1})}{\\eta}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof of Lemma D.2. Let $\\textstyle S_{t}=\\sum_{s=1}^{t}X_{t}$ and $\\begin{array}{r}{V_{t}=\\sum_{s=1}^{t}\\mathbb{E}_{t=1}\\big[X_{t}^{2}\\big]}\\end{array}$ . Let $Z_{t}=\\exp(\\eta S_{t}-\\eta^{2}V_{t})$ . As shown in Beygelzimer et al. [11] (see proof of Theorem 1), as long as $\\eta\\leq1/R$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}_{t-1}[\\exp(\\eta X_{t})]\\le\\,\\exp(\\eta^{2}\\,\\mathbb{E}_{t-1}\\big[X_{t}^{2}\\big]),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and so ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{t-1}[Z_{t}]=\\mathbb{E}_{t-1}\\big[\\mathrm{exp}\\big(\\eta X_{t}-\\eta^{2}\\,\\mathbb{E}_{t-1}\\big[X_{t}^{2}\\big]\\big)\\big]\\cdot Z_{t-1}\\leq Z_{t-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "It follows that $\\left(Z_{t}\\right)$ is a non-negative supermartingale. Hence, by Ville\u2019s inequality, for any $\\eta\\in$ $(0,1/R)$ , we have that for any $\\tau>0$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}[\\exists t:S_{t}-\\eta V_{t}\\ge\\tau]=\\mathbb{P}[\\exists t:Z_{t}\\ge e^{\\eta\\tau}]\\le e^{-\\eta\\tau}\\,\\mathbb{E}[Z_{T}]\\le e^{-\\eta\\tau}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We conclude by setting $\\tau=\\log(\\delta^{-1})/\\eta$ . ", "page_idx": 24}, {"type": "text", "text": "The following result is a standard consequence of Lemma D.2. ", "page_idx": 24}, {"type": "text", "text": "Lemma D.3. Let $(X_{t})_{t\\leq T}$ be a sequence of random variables adapted to a filtration $(\\mathcal{F}_{t})_{t\\leq T}$ . If $0\\le X_{t}\\le R$ almost surely, then with probability at least $1-\\delta,$ , for all $T^{\\prime}\\leq T$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T^{\\prime}}X_{t}\\leq{\\frac{3}{2}}\\sum_{t=1}^{T^{\\prime}}\\mathbb{E}_{t-1}[X_{t}]+4R\\log(2\\delta^{-1}),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T^{\\prime}}\\mathbb{E}_{t-1}[X_{t}]\\leq2\\sum_{t=1}^{T^{\\prime}}X_{t}+8R\\log(2\\delta^{-1}).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "D.2 Information Theory ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "For a pair of probability measures $\\mathbb{P}$ and $\\mathbb{Q}$ , we define the total variation distance as $D_{\\mathsf{T V}}(\\mathbb{P},\\mathbb{Q})=$ $\\textstyle{\\frac{1}{2}}\\int\\!|\\mathrm{d}\\mathbb{P}\\,-\\,\\mathrm{d}\\mathbb{Q}|$ , and define the $\\chi^{2}$ -divergence by $\\begin{array}{r}{D_{\\chi^{2}}(\\mathbb{P}\\parallel\\mathbb{Q})\\ =\\ \\int\\frac{(\\mathrm{d}\\mathbb{Q}-\\mathrm{d}\\mathbb{Q})^{2}}{\\mathrm{d}\\mathbb{Q}}}\\end{array}$ if $\\mathbb{P}\\ll\\mathbb{Q}$ and $D_{\\chi^{2}}(\\mathbb{P}\\parallel\\mathbb{Q})\\;=\\;+\\infty$ otherwise. We define KL divergence by $\\begin{array}{r}{D_{\\mathsf{K L}}(\\mathbb{P}\\,\\|\\,\\mathbb{Q})\\;=\\;\\int\\mathrm{d}\\mathbb{P}\\log\\bigl(\\frac{\\mathrm{d}\\mathbb{P}}{\\mathrm{d}\\mathbb{Q}}\\bigr)}\\end{array}$ if $\\mathbb{P}\\ll\\mathbb{Q}$ and $D_{\\mathsf{K L}}(\\mathbb{P}\\parallel\\mathbb{Q})=+\\infty$ otherwise. ", "page_idx": 25}, {"type": "text", "text": "The following lemma states some basic inequalities between divergences. ", "page_idx": 25}, {"type": "text", "text": "Lemma D.4 (e.g., [62]). The following inequalities hold: ", "page_idx": 25}, {"type": "text", "text": "\u2022 $D_{\\mathsf{T V}}^{2}(\\mathbb{P},\\mathbb{Q})\\leq D_{\\mathsf{H}}^{2}(\\mathbb{P},\\mathbb{Q})\\leq2D_{\\mathsf{T V}}(\\mathbb{P},\\mathbb{Q}).$ \u2022 $\\begin{array}{r}{\\frac{1}{6}D_{\\mathsf{H}}^{2}(\\mathbb{P},\\mathbb{Q})\\leq D_{\\chi^{2}}\\big(\\mathbb{P}\\parallel\\frac{1}{2}(\\mathbb{P}+\\mathbb{Q})\\big)\\leq D_{\\mathsf{H}}^{2}(\\mathbb{P},\\mathbb{Q}).}\\end{array}$ ", "page_idx": 25}, {"type": "text", "text": "D.3 Reinforcement Learning ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "The following lemma is a somewhat standard result; see, e.g., Lemma 15 in Zanette and Brunskill [101]. We include a proof for completeness. ", "page_idx": 25}, {"type": "text", "text": "Lemma D.5 (Law of total variance). For any (potentially stochastic) policy $\\pi$ , we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathrm{Var}^{\\pi}\\biggl[\\sum_{h=1}^{H}r_{h}\\biggr]=\\mathbb{E}^{\\pi}\\biggl[\\sum_{h=0}^{H}\\mathrm{Var}^{\\pi}\\bigl[r_{h}+V_{h+1}^{\\pi}(x_{h+1})\\mid x_{h}\\bigr]\\biggr],\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "with the convention that $x_{0}$ is a deterministic dummy state (so that $P_{0}(x_{1}=\\cdot\\mid x_{0},a=\\cdot)$ is the initial state distribution) and $r_{0}=0$ . ", "page_idx": 25}, {"type": "text", "text": "Proof of Lemma D.5. Let $h\\in\\{0,\\ldots,H\\}$ be fixed. We can expand ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\nabla\\mathrm{tar}^{n}\\bigg[\\underset{t\\rightarrow k}{\\underbrace{\\prod_{i=1}^{M}r_{i}}}\\,\\mathrm{t}_{X_{k}}\\bigg]=\\mathrm{E}^{n}\\Bigg[\\Bigg(\\underset{t\\rightarrow k+1}{\\underbrace{\\prod_{i=1}^{M}r_{i}-V_{k}^{n}(x_{k})}}\\Bigg)^{2}\\mid x_{k}\\Bigg]}\\\\ &{=\\mathrm{E}^{n}\\Bigg[\\Bigg(\\underset{t\\rightarrow k+1}{\\underbrace{\\prod_{i=1}^{M}r_{i}-V_{k}^{n}(x_{k+1})}}+\\big(r_{1}+V_{k+1}^{n}(x_{k+1})-V_{k}^{n}(x_{k})\\big)\\Bigg)^{2}\\mid x_{k}\\Bigg]}\\\\ &{=\\mathrm{E}^{n}\\Bigg[\\Bigg(\\underset{t\\rightarrow k+1}{\\underbrace{\\prod_{i=1}^{M}r_{i}-V_{k}^{n}(x_{k+1})}}\\Bigg)^{2}\\mid x_{k}\\Bigg]+\\mathrm{E}^{n}\\Bigg[\\big(r_{1}+V_{k+1}^{n}(x_{k+1})-V_{k}^{n}(x_{k})\\big)}\\\\ &{\\qquad+2\\mathrm{E}^{n}\\Bigg[\\bigg(\\underset{t\\rightarrow k+1}{\\underbrace{\\prod_{i=1}^{M}r_{i}-V_{k}^{n}(x_{k+1})}}\\bigg)\\left(r_{1}+V_{k+1}^{n}(x_{k+1})-V_{k}^{n}(x_{k})\\right)\\mid x_{k}\\Bigg]}\\\\ &{=\\mathrm{E}^{n}\\Bigg[\\Bigg(\\underset{t\\rightarrow k+1}{\\underbrace{\\prod_{i=1}^{M}r_{i}-V_{k}^{n}(x_{k+1})}}\\Bigg)^{2}\\mid x_{k}\\Bigg]+\\mathrm{E}^{n}\\Bigg[\\big(r_{1}+V_{k+1}^{n}(x_{k+1})-V_{k}^{n}(x_{k})}\\\\ &{\\qquad+\\mathrm{E}^{n}\\Bigg[\\underset{t\\rightarrow k+1}{\\underbrace{\\prod_{i=1}^{M}r_{i}-V_{k+1}^{n}(x_{k+1})}}\\Bigg)\\mid x_{k}\\Bigg]+\\mathrm{Var}^{n}\\big[\\big(r_{1}+V_{k+1}^{n}(x_{k+1})\\big)\\mid x_{k}\\big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We conclude inductively that for all $h\\in\\{0,\\ldots,H\\}$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathrm{Var}^{\\pi}\\left[\\sum_{\\ell=h}^{H}r_{\\ell}\\ |\\ x_{h}\\right]=\\sum_{\\ell=h}^{H}\\mathbb{E}^{\\pi}\\big[\\mathrm{Var}^{\\pi}\\big[(r_{\\ell}+V_{\\ell+1}^{\\pi}(x_{\\ell+1})\\ |\\ x_{\\ell}\\big]\\ |\\ x_{h}\\big].\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "To obtain the final expression, we note that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname{Var}^{\\pi}\\left[\\sum_{h=1}^{H}r_{h}\\right]=\\operatorname{Var}^{\\pi}\\left[\\sum_{h=0}^{H}r_{h}\\mid x_{0}\\right],\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "under the convention that $x_{0}$ is a deterministic dummy state (so that $P_{1}(x_{1}=\\cdot\\mid x_{0},a)$ is the initial state distribution) and $r_{0}=0$ . ", "page_idx": 25}, {"type": "text", "text": "D.4 Maximum Likelihood Estimation ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "This section presents a self-contained analysis of the maximum likelihood estimator (MLE) for density estimation. The results are somewhat standard (e.g., Wong and Shen [97], van de Geer [89], Zhang [102]), but we include proofs for completeness. ", "page_idx": 26}, {"type": "text", "text": "Consider a setting where we receive $\\{z^{i}\\}_{i=1}^{n}$ i.i.d. from $z\\sim g^{\\star}$ , where $g^{\\star}\\in\\Delta(\\mathcal{Z})$ . We have a class $\\mathcal{G}\\subseteq\\Delta(\\mathcal{Z})$ that may or may not contain $g^{\\star}$ . We analyze the following maximum likelihood estimator: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\widehat{g}=\\underset{g\\in\\mathcal{G}}{\\arg\\operatorname*{max}}\\sum_{i=1}^{n}\\log(g(z^{i})).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "To provide sample complexity guarantees that support infinite classes, we appeal to the following notion of covering number (e.g., Wong and Shen [97]), which tailored to the log-loss. ", "page_idx": 26}, {"type": "text", "text": "Definition D.1 (Covering number). For a class $\\mathcal{G}\\subset\\Delta(\\mathcal{Z})$ , we set that a class $\\mathcal{G}^{\\prime}\\subset\\Delta(\\mathcal{Z})$ is an $\\varepsilon$ -cover if for all $g\\in{\\mathcal{G}}$ , there exists $g^{\\prime}\\in\\mathcal{G}^{\\prime}$ such that for all $z\\in{\\mathcal{Z}}$ , $\\log(g(z)/g^{\\prime}(z))\\leq\\varepsilon$ . We denote the size of the smallest such cover by $\\mathcal{N}_{\\mathrm{log}}(\\mathcal{G},\\varepsilon)$ . ", "page_idx": 26}, {"type": "text", "text": "We also allow for optimization errors, and concretely assume that $\\widehat g$ satisfies ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}\\log(\\widehat{g}(z^{i}))\\geq\\operatorname*{max}_{g\\in\\mathcal{G}}\\sum_{i=1}^{n}\\log(g(z^{i}))-\\varepsilon_{\\mathsf{o p t}}\\cdot n\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "for a parameter $\\varepsilon_{\\mathsf{o p t}}\\geq0$ ; the case $\\varepsilon_{\\mathsf{o p t}}=0$ coincides with Eq. (10). Our main guarantee for MLE is as follows. ", "page_idx": 26}, {"type": "text", "text": "Proposition D.1. The maximum likelihood estimator in Eq. (10) has that with probability at least $1-\\delta$ , ", "page_idx": 26}, {"type": "equation", "text": "$$\nD_{\\mathsf{H}}^{2}(\\widehat{g},g^{\\star})\\leq\\operatorname*{inf}_{\\varepsilon>0}\\biggl\\{\\frac{6\\log(2\\mathcal{N}_{\\log}(\\mathcal{G},\\varepsilon)/\\delta^{-1})}{n}+4\\varepsilon\\biggr\\}+2\\operatorname*{inf}_{g\\in\\mathcal{G}}\\log(1+D_{\\chi^{2}}(g^{\\star}\\parallel g))+2\\varepsilon_{\\mathsf{o p t}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "In particular, if $\\mathcal{G}$ is finite, the maximum likelihood estimator satisfies ", "page_idx": 26}, {"type": "equation", "text": "$$\nD_{\\mathsf{H}}^{2}(\\widehat{g},g^{\\star})\\leq\\frac{6\\log(2|\\mathcal{G}|/\\delta^{-1})}{n}+2\\operatorname*{inf}_{g\\in\\mathcal{G}}\\log(1+D_{\\chi^{2}}(g^{\\star}\\parallel g))+2\\varepsilon_{\\mathsf{o p t}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Note that the term $\\begin{array}{r}{\\operatorname*{inf}_{g\\in{\\mathcal{G}}}\\log(1+D_{\\chi^{2}}(g^{\\star}\\parallel g))}\\end{array}$ corresponds to misspecification error, and is zero if $g^{\\star}\\in\\mathcal{G}$ . ", "page_idx": 26}, {"type": "text", "text": "Proof of Proposition D.1. Let $\\mathcal{G}_{\\varepsilon}$ denote a minimal $\\varepsilon$ -cover for $\\mathcal{G}$ , and let $\\widetilde{g}\\in\\mathcal{G}_{\\varepsilon}$ denote any element that covers $\\widehat g$ in the sense of Definition D.1. Going forward, we will use  t hat $\\widetilde g$ satisfies ", "page_idx": 26}, {"type": "equation", "text": "$$\nD_{\\mathsf{H}}^{2}(g^{\\star},\\widetilde{g})\\leq D_{\\mathsf{K L}}(g^{\\star}\\parallel\\widetilde{g})\\leq\\varepsilon.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Let $\\ell^{i}(g)=-\\log(g(z^{i}))$ , and set $\\begin{array}{r}{\\widehat{L}(g)=-\\sum_{i=1}^{n}\\log(g(z^{i}))}\\end{array}$ . Set $\\begin{array}{r}{X_{i}(g)=\\frac{1}{2}(\\ell^{i}(g)-\\ell^{i}(g^{\\star}))}\\end{array}$ . By applying Lemma D.1 with the seq uence $(X_{i}(g))_{i=1}^{n}$ for each $g\\in{\\mathcal{G}}_{\\varepsilon}$ and taking a union bound, we have that with probability at least $1-\\delta$ , for all $g\\in{\\mathcal{G}}_{\\varepsilon}$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n-n\\cdot\\log\\biggl(\\mathbb{E}_{z\\sim g^{\\star}}\\Bigl[e^{\\frac{1}{2}\\log(g(z)/g^{\\star}(z))}\\Bigr]\\biggr)\\leq\\frac{1}{2}\\Bigl(\\widehat{L}(g)-\\widehat{L}(g^{\\star})\\Bigr)+\\log(|\\mathcal{G}_{\\varepsilon}|\\delta^{-1}).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Using a standard argument [102], we have that ", "page_idx": 26}, {"type": "equation", "text": "$$\n-\\log\\biggl(\\mathbb{E}_{z\\sim g^{\\star}}\\Bigl[e^{\\frac{1}{2}\\log(g(z)/g^{\\star}(z))}\\Bigr]\\biggr)=-\\log\\biggl(1-\\frac{1}{2}D_{\\mathsf{H}}^{2}(g,g^{\\star})\\biggr)\\geq\\frac{1}{2}D_{\\mathsf{H}}^{2}(g,g^{\\star}).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "In particular, this implies that ", "page_idx": 26}, {"type": "equation", "text": "$$\nD_{\\mathsf{H}}^{2}(\\widetilde{g},g^{\\star})\\leq\\frac{2\\log(|\\mathcal{G}|/\\delta^{-1})}{n}+\\frac{1}{n}\\Big(\\widehat{L}(\\widetilde{g})-\\widehat{L}(g^{\\star})\\Big),\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "and so ", "page_idx": 26}, {"type": "equation", "text": "$$\nD_{\\mathsf{H}}^{2}(\\widehat{g},g^{\\star})\\leq2D_{\\mathsf{H}}^{2}(\\widehat{g},\\widetilde{g})+2D_{\\mathsf{H}}^{2}(\\widetilde{g},g^{\\star})\\leq\\frac{4\\log(|\\mathcal{G}|/\\delta^{-1})}{n}+\\frac{2}{n}\\Big(\\widehat{L}(\\widetilde{g})-\\widehat{L}(g^{\\star})\\Big)+2\\varepsilon,\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "by the triangle inequality for Hellinger distance and Eq. (11). ", "page_idx": 27}, {"type": "text", "text": "It remains to bound the right-hand-side. Let ${\\overline{{g}}}\\in{\\mathcal{G}}$ be arbitrary. We can bound $\\widehat{L}(\\widetilde{g})-\\widehat{L}(g^{\\star})\\leq\\widehat{L}(\\widetilde{g})-\\widehat{L}(\\widehat{g})+\\widehat{L}(\\widehat{g})-\\widehat{L}(g^{\\star})\\leq\\widehat{L}(\\widetilde{g})-\\widehat{L}(\\widehat{g})+\\widehat{L}(\\overline{{g}})-\\widehat{L}(g^{\\star})+\\varepsilon_{\\mathrm{opt}}n,$ (12) by the definition of the maximum likelihood estimator. For the first term in Eq. (12), we observe that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\widehat{L}(\\widetilde{\\boldsymbol{g}})-\\widehat{L}(\\boldsymbol{g}^{\\star})=\\sum_{i=1}^{n}\\log(\\boldsymbol{g}^{\\star}(\\boldsymbol{z}^{i})/\\widetilde{\\boldsymbol{g}}(\\boldsymbol{z}^{i}))\\leq\\varepsilon n,\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "by Definition D.1. ", "page_idx": 27}, {"type": "text", "text": "To bound the second term in Eq. (12), set $Y_{i}=-(\\ell^{t}(\\bar{g})-\\ell^{t}(g^{\\star}))$ . Applying Lemma D.1 with the sequence $(Y_{i})_{i=1}^{n}$ , we have that with probability at least $1-\\delta$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\widehat{L}(\\bar{g})-\\widehat{L}(g^{\\star})\\leq n\\cdot\\log\\biggr(\\mathbb{E}_{z\\sim g^{\\star}}\\Bigl[e^{\\log(g^{\\star}(z)/\\bar{g}(z))}\\Bigr]\\Bigr)+\\log(\\delta^{-1}).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Finally, note that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\log\\biggr(\\mathbb{E}_{z\\sim g^{\\star}}\\Bigl[e^{\\log(g^{\\star}(z)/\\bar{g}(z))}\\Bigr]\\biggr)=\\log\\biggr(\\mathbb{E}_{z\\sim g^{\\star}}\\biggl[\\frac{g^{\\star}(z)}{\\bar{g}(z)}\\biggr]\\biggr)=\\log(1+D_{\\chi^{2}}(g^{\\star}\\parallel\\bar{g})).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The result follows by choosing ${\\overline{{g}}}\\in{\\mathcal{G}}$ to minimize this quantity. ", "page_idx": 27}, {"type": "text", "text": "Part I ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Proofs and Supporting Results ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "E Examples and Supporting Results from Section 2 and Section 3 ", "page_idx": 27}, {"type": "text", "text": "This section contains supporting results from Sections 2 and 3: ", "page_idx": 27}, {"type": "text", "text": "\u2022 Appendix E.1 presents general sample complexity guarantees for log-loss behavior cloning that support infinite policy classes and misspecification, as well as concrete examples. \u2022 Appendix E.2 formally introduces the online imitation learning framework, and gives sample complexity guarantees for a log-loss variant of Dagger. ", "page_idx": 27}, {"type": "text", "text": "E.1 General Guarantees and Examples for Log-Loss Behavior Cloning ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In this section, we give bounds on the generalization error $D_{\\mathsf{H}}^{2}\\big(\\mathbb{P}^{\\widehat{\\pi}},\\mathbb{P}^{\\pi^{\\star}}\\big)$ for log-loss behavior cloning for concrete classes $\\Pi$ of interest. To do so, we observe that the log-loss behavior cloning objective ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\widehat{\\pi}=\\arg\\operatorname*{max}_{\\pi\\in\\Pi}\\sum_{i=1}^{n}\\sum_{h=1}^{H}\\log(\\pi_{h}(a_{h}^{i}\\mid x_{h}^{i})).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "is equivalent to performing maximum likelihood estimation over the density class ${\\mathcal{P}}=\\{\\mathbb{P}^{\\pi}\\}_{\\pi\\in\\Pi}$ . Indeed, for any $\\pi\\in\\Pi$ , we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i=1}^{n}\\log(\\mathbb{P}^{\\pi}(o^{i}))=\\displaystyle\\sum_{i=1}^{n}\\log\\left(P_{0}(x_{1}^{i})\\prod_{h=1}^{H}P_{h}(x_{h+1}^{i}\\mid x_{h}^{i},a_{h}^{i})\\pi_{h}(a_{h}^{i}\\mid x_{h}^{i})\\right)}\\\\ &{\\displaystyle=\\sum_{i=1}^{n}\\sum_{h=1}^{H}\\log(\\pi_{h}(a_{h}^{i}\\mid x_{h}^{i}))+C(\\mathcal{D}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $C(\\mathcal{D})$ is a constant that depends on the dataset $\\mathcal{D}$ but not on $\\pi$ . It follows that both objectives have the same maximizer. Consequently, we can prove sample complexity bounds for log-loss behavior cloning by specializing sample complexity bounds for MLE given in Appendix D.4. ", "page_idx": 27}, {"type": "text", "text": "To give guarantees that support infinite policy classes, we appeal to the following notion of covering number. ", "page_idx": 27}, {"type": "text", "text": "Definition E.1 (Policy covering number). For a class $\\Pi\\subset\\{\\pi_{h}:\\mathcal{X}\\rightarrow\\Delta(\\mathcal{A})\\}$ , we set that $\\Pi^{\\prime}~\\subset$ $\\{\\pi_{h}:\\mathcal{X}\\to\\Delta(\\mathcal{A})\\}$ is an $\\varepsilon$ -cover if for all $\\pi\\,\\in\\,\\Pi$ , there exists $\\pi^{\\prime}\\,\\in\\,\\Pi^{\\prime}$ such that for all $x\\in\\mathscr{X}$ , $a\\in{\\mathcal{A}}$ , and $\\in[{\\dot{H}}],\\log(\\pi_{h}(a\\mid x)/\\pi_{h}^{\\prime}(a\\mid x))\\leq\\varepsilon$ . We denote the size of the smallest such cover by $\\mathcal N_{\\mathrm{pol}}(\\Pi,\\varepsilon)$ . ", "page_idx": 28}, {"type": "text", "text": "In addition, to allow for optimization errors, we replace Eq. (4) with the assumption that $\\widehat{\\pi}$ satisfies ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}\\sum_{h=1}^{H}\\log(\\widehat{\\pi}_{h}(a_{h}^{i}\\mid x_{h}^{i}))\\geq\\operatorname*{max}_{\\pi\\in\\Pi}\\sum_{i=1}^{n}\\sum_{h=1}^{H}\\log(\\pi_{h}(a_{h}^{i}\\mid x_{h}^{i}))-\\varepsilon_{\\mathsf{o p t}}\\cdot n\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "for a parameter $\\varepsilon_{\\mathsf{o p t}}>0$ ; Eq. (4) is the special case in which $\\varepsilon_{\\mathsf{o p t}}\\,=\\,0$ . With these definitions, specializing Proposition D.1 leads to the following result. ", "page_idx": 28}, {"type": "text", "text": "Theorem E.1 (Generalization bound for LogLossBC). The LogLossBC policy in Eq. (13) has that with probability at least $1-\\delta$ , ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathcal{D}_{\\mathsf{H}}^{2}\\Big(\\mathbb{P}^{\\widehat{\\pi}},\\mathbb{P}^{\\pi^{*}}\\Big)\\leq\\operatorname*{inf}_{\\varepsilon>0}\\bigg\\{\\frac{6\\log(2\\ensuremath{\\mathcal{N}}_{\\mathrm{pol}}(\\Pi,\\varepsilon/H)\\delta^{-1})}{n}+4\\varepsilon\\bigg\\}+2\\operatorname*{inf}_{\\pi\\in\\Pi}\\log\\Big(1+D_{\\chi^{2}}\\big(\\mathbb{P}^{\\pi^{*}}\\parallel\\mathbb{P}^{\\pi}\\big)\\Big)+2\\varepsilon_{\\mathsf{o p t}}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "In particular, if \u03a0 is finite, the log-loss behavior cloning policy satisfies ", "page_idx": 28}, {"type": "equation", "text": "$$\nD_{\\mathsf{H}}^{2}\\Big(\\mathbb{P}^{\\widehat{\\pi}},\\mathbb{P}^{\\pi^{\\star}}\\Big)\\leq\\frac{6\\log(2|\\Pi|\\delta^{-1})}{n}+2\\operatorname*{inf}_{\\pi\\in\\Pi}\\log\\Big(1+D_{\\chi^{2}}\\big(\\mathbb{P}^{\\pi^{\\star}}\\mid\\mathbb{P}^{\\pi}\\big)\\Big)+2\\varepsilon_{\\mathsf{o p t}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Let us make two remarks. ", "page_idx": 28}, {"type": "text", "text": "\u2022 First, the only explicit dependence on the horizon $H$ is through the precision $\\varepsilon/H$ through which we evaluate the covering number: $\\ensuremath{\\mathcal{N}_{\\mathrm{pol}}}(\\Pi,\\varepsilon/H)$ . As a result, for parametric classes where $\\mathcal{N}_{\\mathrm{pol}}(\\Pi,\\varepsilon)\\;\\asymp\\;\\log(\\varepsilon^{-1})$ (we will give examples in the sequel), the result will scale at most logarithmically in $H$ , but for nonparametric classes the dependence can be polynomial. \u2022 Second, the remainder term $\\operatorname*{inf}_{\\pi\\in\\Pi}\\log(1+D_{\\chi^{2}}(\\mathbb{P}^{\\pi^{\\star}}\\parallel\\mathbb{P}^{\\pi}))$ corresponds to misspecification error, and is zero if $\\pi^{\\star}\\in\\Pi$ . We remark that . when $\\pi^{\\star}$ is deterministic, this expression can be simplified $\\begin{array}{r}{\\operatorname*{inf}_{\\pi\\in\\Pi}\\log\\left(\\mathbb{E}^{\\pi^{\\star}}\\left[\\frac{1}{\\prod_{h=1}^{H}\\pi_{h}\\left(a_{h}|x_{h}\\right)}\\right]\\right)}\\end{array}$ ", "page_idx": 28}, {"type": "text", "text": "Proof of Theorem E.1. This follows by applying Proposition D.1 with the class $\\{\\mathbb{P}^{\\pi}\\}_{\\pi\\in\\Pi}$ , and noting that if $\\pi^{\\prime}$ covers $\\pi$ in the sense of Definition E.1, then for all $o\\,\\in\\,(\\mathcal{X}\\times\\mathcal{A})^{H}$ , we have $\\log(\\mathbb{P}^{\\pi}(o)/\\mathbb{P}^{\\pi^{\\prime}}(o))\\leq\\varepsilon{\\cal H}$ , meaning that an $\\varepsilon$ -cover in the sense of Definition E.1 yields an $\\varepsilon H$ -cover in the sense of Definition D.1. ", "page_idx": 28}, {"type": "text", "text": "E.1.1 Example: Tabular Policies ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "We now instantiate Theorem E.1 to give generalization bounds for specific policy classes of interest. Consider a tabular MDP in which $|{\\mathcal{X}}|,|{\\mathcal{A}}|<\\infty$ are small and finite. Here, choosing $\\Pi$ to be the set of all stationary policies leads to a bound independent of $H$ . ", "page_idx": 28}, {"type": "text", "text": "Corollary E.1 (Stationary tabular policies). When $\\Pi$ is the set of all deterministic stationary policies, the log-loss behavior cloning policy Eq. (4) has that with probability at least $1-\\delta$ , ", "page_idx": 28}, {"type": "equation", "text": "$$\nD_{\\mathsf{H}}^{2}\\big(\\mathbb{P}^{\\widehat{\\pi}},\\mathbb{P}^{\\pi^{\\star}}\\big)\\leq O\\bigg(\\frac{|\\mathcal{X}|\\log(|A|\\delta^{-1})}{n}\\bigg).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Meanwhile, $i f\\Pi$ is the set of all stochastic stationary policies, the log-loss behavior cloning policy Eq. (4) has that with probability at least $1-\\delta$ , ", "page_idx": 28}, {"type": "equation", "text": "$$\nD_{\\mathsf{H}}^{2}\\big(\\mathbb{P}^{\\widehat{\\pi}},\\mathbb{P}^{\\pi^{\\star}}\\big)\\leq\\widetilde{O}\\bigg(\\frac{|\\mathcal{X}||\\mathcal{A}|\\log(H n\\delta^{-1})}{n}\\bigg).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof of Corollary E.1. This follows by noting that we have $\\log\\!|\\Pi|\\ \\leq\\ |\\chi|\\log|\\mathcal{A}|$ in the deterministic case and $\\log\\mathcal{N}_{\\mathrm{pol}}(\\Pi,\\varepsilon)\\,\\le\\,\\widetilde O\\bigl(|\\mathcal{X}||\\mathcal{A}|\\log(\\varepsilon^{-1})\\bigr)$ in the stochastic case (this follows ", "page_idx": 28}, {"type": "text", "text": "Naturally, we can also give generalization guarantees for non-stationary tabular policies, though the sample complexity will scale with $H$ in this case. ", "page_idx": 29}, {"type": "text", "text": "Corollary E.2 (Non-stationary tabular policies). When \u03a0 is the set of all deterministic non-stationary policies, the log-loss behavior cloning policy $E q.$ . (4) has that with probability at least $1-\\delta$ , ", "page_idx": 29}, {"type": "equation", "text": "$$\nD_{\\mathsf{H}}^{2}\\big(\\mathbb{P}^{\\widehat{\\pi}},\\mathbb{P}^{\\pi^{\\star}}\\big)\\leq O\\bigg(\\frac{H|\\chi|\\log(|\\mathcal{A}|\\delta^{-1})}{n}\\bigg).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Meanwhile, if $\\Pi$ is the set of all stochastic non-stationary policies, the log-loss behavior cloning policy Eq. (4) has that with probability at least $1-\\delta$ , ", "page_idx": 29}, {"type": "equation", "text": "$$\nD_{\\mathsf{H}}^{2}\\big(\\mathbb{P}^{\\widehat{\\pi}},\\mathbb{P}^{\\pi^{\\star}}\\big)\\leq\\widetilde{O}\\bigg(\\frac{H|\\chi||\\mathcal{A}|\\log(H n\\delta^{-1})}{n}\\bigg).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Proof of Corollary E.2. This follows because we have $\\log|\\Pi|\\leq H|{\\mathcal{X}}|\\log|A|$ in the deterministic case and $\\log\\mathcal{N}_{\\mathrm{pol}}(\\Pi,\\varepsilon)\\leq\\widetilde O\\big(H|\\mathcal{X}||\\mathcal{A}|\\log(\\varepsilon^{-1})\\big)$ in the stochastic case. \u53e3 ", "page_idx": 29}, {"type": "text", "text": "E.1.2 Example: Softmax Policies ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Next, we give an example of a general family of policy classes based on function approximation for which the sample complexity is at most polylogarithmic in $H$ . ", "page_idx": 29}, {"type": "text", "text": "For a vector $v\\in\\mathbb{R}^{A}$ , let $\\sigma:\\mathbb{R}^{A}\\rightarrow\\Delta(A)$ be the softmax function, which is given by ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\sigma_{a}(v)=\\frac{\\exp(v_{a})}{\\sum_{a^{\\prime}\\in\\mathcal{A}}\\exp(v_{a^{\\prime}})}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Let $\\mathcal{F}\\subset\\left\\{f_{h}:\\mathcal{X}\\times\\mathcal{A}\\rightarrow\\mathbb{R}\\right\\}_{h=1}^{H}$ be a class of value functions, and define the induced class of softmax policies via ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\Pi_{\\mathcal{F}}=\\{\\pi_{f}\\ |\\ f\\in\\mathcal{F}\\},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\pi_{f,h}(x):=\\sigma_{a}(f_{h}(x,a)).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "We give sample complexity guarantees based on covering numbers for the value function class $\\mathcal{F}$ . ", "page_idx": 29}, {"type": "text", "text": "Definition E.2 (Value function covering number). For a class ${\\mathcal{F}}\\subset\\{f_{h}:{\\mathcal{X}}\\times{\\mathcal{A}}\\rightarrow\\mathbb{R}\\}$ , we set that ${\\mathcal{F}}^{\\prime}\\subset\\{f_{h}:\\mathcal{X}\\times\\mathcal{A}\\rightarrow\\mathbb{R}\\}$ is an $\\varepsilon$ -cover if for all $f\\in{\\mathcal{F}}$ , there exists $f^{\\prime}\\in\\mathcal{F}^{\\prime}$ such that for all $x\\in\\mathscr{X}$ , $a\\in A$ , and $h\\in[H]$ , $|f_{h}(x,a)-f_{h}^{\\prime}(x,\\stackrel{\\cdot}{a})|\\leq\\varepsilon$ . We denote the size of the smallest such cover by ${\\mathcal N}_{\\mathrm{val}}(\\Pi,\\varepsilon)$ . ", "page_idx": 29}, {"type": "text", "text": "Corollary E.3 (Softmax policies). When $\\Pi=\\Pi_{\\mathcal{F}}$ is the softmax policy class for a value function class $\\mathcal{F}$ , the log-loss behavior cloning policy Eq. (4) has that with probability at least $1-\\delta$ , ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{D_{\\mathbf{H}}^{2}\\Big(\\mathbb{P}^{\\widehat{\\pi}},\\mathbb{P}^{\\pi^{*}}\\Big)\\leq O(1)\\cdot\\operatorname*{inf}_{\\varepsilon>0}\\Big\\{\\frac{\\log(\\mathcal{N}_{\\mathrm{val}}(\\mathcal{F},\\varepsilon/H)\\delta^{-1})}{n}+\\varepsilon\\Big\\}+2\\operatorname*{inf}_{\\pi\\in\\Pi_{\\mathcal{F}}}\\log\\Big(1+D_{\\chi^{2}}\\big(\\mathbb{P}^{\\pi^{*}}\\parallel\\mathbb{P}^{\\pi}\\big)\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Proof of Corollary E.3. Consider a pair of functions $f,f^{\\prime}$ with $|f_{h}(x,a)-f_{h}^{\\prime}(x,a)|\\leq\\varepsilon$ for all $x\\in{\\mathcal{X}},a\\in{\\mathcal{A}}$ , and $h\\in[H]$ . The induced softmax policies satisfy ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\log(\\pi_{f,h}(a\\mid x)/\\pi_{f^{\\prime},h}(a\\mid x))=f_{h}(x,a)-f_{h}^{\\prime}(x,a)+\\log\\left({\\frac{\\sum_{a^{\\prime}\\in A}\\exp(f_{h}^{\\prime}(x,a^{\\prime}))}{\\sum_{a\\in A}\\exp(f_{h}^{\\prime}(x,a^{\\prime}))}}\\right).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Clearly we have $f_{h}(x,a)-f_{h}^{\\prime}(x,a)\\leq\\varepsilon$ , and we can bound ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathrm{og}\\biggl(\\frac{\\sum_{a^{\\prime}\\in A}\\exp\\bigl(f_{h}^{\\prime}(x,a^{\\prime})\\bigr)}{\\sum_{a\\in A}\\exp\\bigl(f_{h}(x,a^{\\prime})\\bigr)}\\biggr)=\\log\\biggl(\\frac{\\sum_{a^{\\prime}\\in A}\\exp\\bigl(f_{h}(x,a^{\\prime})\\bigr)\\cdot\\,\\exp\\bigl(f_{h}^{\\prime}(x,a^{\\prime})-f_{h}(x,a^{\\prime})\\bigr)}{\\sum_{a\\in A}\\exp\\bigl(f_{h}(x,a^{\\prime})\\bigr)}\\biggr)}\\qquad}&{}\\\\ &{\\leq\\,\\log\\biggl(\\frac{\\sum_{a^{\\prime}\\in A}\\exp\\bigl(f_{h}(x,a^{\\prime})\\bigr)\\cdot\\,\\operatorname*{max}_{a^{\\prime\\prime}\\in A}\\exp\\bigl(f_{h}^{\\prime}(x,a^{\\prime\\prime})-f_{h}(x,a^{\\prime\\prime})\\bigr)}{\\sum_{a\\in A}\\exp\\bigl(f_{h}(x,a^{\\prime})\\bigr)}\\biggr)}\\\\ &{\\leq\\,\\underset{a^{\\prime\\prime}\\in A}{\\operatorname*{max}}\\{f_{h}^{\\prime}(x,a^{\\prime\\prime})-f_{h}(x,a^{\\prime\\prime})\\}\\leq\\varepsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Whenever $\\mathcal{F}$ is parametric in the sense that $\\log\\mathcal{N}_{\\mathrm{val}}(\\mathcal{F},\\varepsilon)\\;\\propto\\;\\log(\\varepsilon^{-1})$ , Corollary E.3 leads to polylogarithmic dependence on $H$ . The following result gives such an example. ", "page_idx": 30}, {"type": "text", "text": "Linear softmax policies. Consider the set of stationary linear softmax policies induced by the value function class ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathcal{F}=\\{(x,a,h)\\mapsto\\langle\\phi_{h}(x,a),\\theta\\rangle\\ |\\ \\|\\theta\\|_{2}\\leq B\\},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $\\phi_{h}(x,a)\\in\\mathbb{R}^{d}$ is a known feature map with $\\|\\phi_{h}(x,a)\\|\\le B$ . Here, we have $\\log\\mathcal{N}_{\\mathrm{val}}(\\mathcal{F},\\varepsilon)\\propto$ $d\\log(B\\varepsilon^{-1})$ (e.g., Wainwright [91]), which yields the following generalization guarantee. ", "page_idx": 30}, {"type": "text", "text": "Corollary E.4. When $\\Pi$ is the set of stationary linear softmax policies and $\\pi^{\\star}\\in\\Pi$ , the log-loss behavior cloning policy Eq. (4) has that with probability at least $1-\\delta$ , ", "page_idx": 30}, {"type": "equation", "text": "$$\nD_{\\mathsf{H}}^{2}\\big(\\mathbb{P}^{\\widehat{\\pi}},\\mathbb{P}^{\\pi^{\\star}}\\big)\\leq O\\bigg(\\frac{d\\log(B H n\\delta^{-1})}{n}\\bigg).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "E.2 Online IL Framework and Sample Complexity Bounds for Log-Loss Dagger ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "In this section, we give sample complexity bounds for a variant of the Dagger algorithm for online IL [72] that uses the logarithmic loss. The main purpose of including this result is to give end-to-end sample complexity guarantees for general policy classes, which we use in Sections 2 and 3 to compare the optimal rates for online and offline IL. For this comparison, we are be mainly interested in the case of deterministic expert policies, but our analysis supports stochastic policies, which may be of independent interest. ", "page_idx": 30}, {"type": "text", "text": "Online imitation learning framework. In the online imitation learning framework, learning proceeds in $n$ episodes in which the learner can directly interact with the underlying MDP $M^{\\star}$ $\\pi^{i}\\,=\\,\\left\\{\\pi_{h}^{i}:\\mathcal{X}\\to\\Delta(A)\\right\\}_{h=1}^{H}$ aCnodn rcerecteeilvye, sf oa rt reaajcehc teopriys $o^{t}\\,=\\,(x_{1}^{i},a_{1}^{i},a_{1}^{\\star,i}),\\ldots,(x_{H}^{i},a_{H}^{i},a_{H}^{\\star,i})$ $i\\,\\in\\,[n]$ l,i ciny which $a_{h}^{i}\\sim\\pi_{h}^{i}(x_{h}^{i})$ , $a_{h}^{\\star,i}\\sim\\pi^{\\star}(x_{h}^{t})$ , and $x_{h+1}^{i}\\sim P_{h}(x_{h}^{i},a_{h}^{i})$ ; in other words, the trajectory induced by the learner\u2019s policy is annotated by the expert\u2019s action $a_{h}^{\\star}\\sim\\pi_{h}^{\\star}(x_{h})$ at each state $x_{h}$ encountered. After all $n$ episodes conclude, they can use all of the data collected to produce a policy $\\widehat{\\pi}$ such that $J(\\pi^{\\star})-J(\\bar{\\widehat{\\pi}})$ is small. ", "page_idx": 30}, {"type": "text", "text": "Dagger algorithm. We consider a general version of the Dagger algorithm. The algorithm is parameterized by an online learning algorithm $\\bf A l g_{E s t}$ , which attempts to estimate the expert policy in a sequential fashion based on trajectories. ", "page_idx": 30}, {"type": "text", "text": "\u2022 Query online learning algorithm $\\bf A l g_{E s t}$ with $\\mathcal{D}^{i}$ and receive policy $\\widehat{\\pi}$ . ", "page_idx": 30}, {"type": "text", "text": "\u2022 Execute $\\widehat{\\pi}$ and observe $o^{i}=(x_{1}^{i},a_{1}^{i},a_{1}^{\\star,i}),\\dots,(x_{H}^{i},a_{H}^{i},a_{H}^{\\star,i}).$ .   \n\u2022 Update ${\\mathcal{D}}^{i+1}\\leftarrow{\\mathcal{D}}^{i}\\cup\\{o^{i}\\}$ . ", "page_idx": 30}, {"type": "text", "text": "At the end, we output ${\\widehat{\\pi}}=\\operatorname{unif}\\left(\\pi^{1},\\ldots,\\pi^{n}\\right)$ as the final policy. ", "page_idx": 30}, {"type": "text", "text": "To measure the performance of the estimation oracle, we define the online estimation error as: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbf{Est}_{\\mathsf{H}}^{\\mathsf{o n}}(n)=\\frac{1}{n}\\sum_{i=1}^{n}\\sum_{h=1}^{H}\\mathbb{E}^{\\hat{\\pi}^{i}}\\big[D_{\\mathsf{H}}^{2}(\\widehat{\\pi}_{h}^{i}(x_{h}),\\pi^{\\star}(x_{h}))\\big].\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "As we will show in a moment, this notion of estimation error is well-suited for online learning algorithms that estimate $\\pi^{\\star}$ using the logarithmic loss. ", "page_idx": 30}, {"type": "text", "text": "Our following result gives a general guarantee for Dagger that holds for any choice of online learning algorithm. To state the result, let $\\mathbb{P}^{\\pi^{\\star}|\\pi}$ denote the law of $o=(x_{1},a_{1},a_{1}^{\\star}),\\ldots,(x_{H},a_{H},a_{H}^{\\star})$ when $\\pi^{\\star}$ is the expert policy and we execute $\\pi$ . Let ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\sigma_{\\pi^{\\star}|\\pi}^{2}=\\sum_{h=1}^{H}\\mathbb{E}^{\\pi\\circ_{h}\\pi^{\\star}}\\Bigl[({Q_{h}^{\\pi^{\\star}}}(x_{h},a_{h})-{V_{h}^{\\pi^{\\star}}}(x_{h}))^{2}\\Bigr],\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "so that $\\sigma_{\\pi^{\\star}}^{2}=\\sigma_{\\pi^{\\star}|\\pi^{\\star}}^{2}$ and define $\\overline{{{\\sigma}}}_{\\pi^{\\star}}^{2}=\\operatorname*{sup}_{\\pi}\\sigma_{\\pi^{\\star}|\\pi}^{2}$ . Note that $\\overline{{\\sigma}}_{\\pi^{\\star}}^{2}=0$ whenever $\\pi^{\\star}$ is deterministic, but in general, \u03c32\u03c0\u22c6\u2265\u03c3\u03c02\u22c6. ", "page_idx": 31}, {"type": "text", "text": "Proposition E.1 (Regret for Dagger). For any MDP $M^{\\star}$ with signed recoverability parameter $\\widetilde{\\mu}$ and any online learning algorithm $\\bf A l g_{E s t}$ , Dagger ensures that ", "page_idx": 31}, {"type": "equation", "text": "$$\nJ(\\pi^{\\star})-J(\\widehat{\\pi})\\lesssim\\sqrt{\\overline{{\\sigma}}_{\\pi^{\\star}}^{2}\\cdot{\\bf E s t}_{\\mathsf{H}}^{\\mathsf{o n}}(n)}+\\widetilde{\\mu}\\cdot{\\bf E s t}_{\\mathsf{H}}^{\\mathsf{o n}}(n).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Furthermore, whenever $\\pi^{\\star}$ is deterministic, Dagger ensures that ", "page_idx": 31}, {"type": "equation", "text": "$$\nJ(\\pi^{\\star})-J(\\widehat\\pi)\\lesssim\\mu\\cdot{\\bf E s t}_{\\mathsf{H}}^{\\mathsf{o n}}(n).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "To instantiate the bound above, we choose $\\mathbf{Alg}_{\\mathsf{E s t}}$ by applying the exponential weights algorithm (e.g., Cesa-Bianchi and Lugosi [19]) with the logarithmic loss. Let $\\Pi_{h}\\ :=\\{\\pi_{h}\\ |\\ \\pi\\in\\mathrm{\\bar{H}}\\}$ denote the projection of $\\Pi$ onto step $h$ . The algorithm proceeds as follows. At step $i\\in[n]$ , given the dataset $\\mathcal{D}^{i}$ , for each layer $h\\in[H]$ we define a distribution $\\mu_{h}^{i}\\in\\Delta(\\Pi_{h})$ via ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mu_{h}^{i}(\\pi)\\propto\\exp\\left(\\sum_{j<j}\\log(\\pi_{h}(a_{h}^{\\star,j}\\mid x_{h}^{j}))\\right)=\\prod_{j<j}\\pi_{h}(a_{h}^{\\star,j}\\mid x_{h}^{j}).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We then set ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\widehat{\\pi}_{h}^{i}(a\\mid x)=\\mathbb{E}_{\\pi_{h}\\sim\\mu_{h}^{i}}[\\pi_{h}(a\\mid x)].\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We refer to the resulting algorithm as LogLossDagger. This leads to the following guarantee for finite classes. ", "page_idx": 31}, {"type": "text", "text": "Proposition E.2 (Regret for LogLossDagger). When $\\pi^{\\star}\\in\\Pi$ , the log-loss exponential weights algorithm ensures that with probability at least $1-\\delta$ , ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbf{Est}_{\\mathsf{H}}^{\\mathsf{o n}}(n)\\leq\\frac{2}{n}\\sum_{h=1}^{H}\\log(|\\Pi_{h}|H\\delta^{-1}).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Consequently, LogLossDagger ensures that with probability at least $1-\\delta$ , ", "page_idx": 31}, {"type": "equation", "text": "$$\nJ(\\pi^{\\star})-J(\\widehat\\pi)\\lesssim\\sqrt{\\overline{{\\sigma}}_{\\pi^{\\star}}^{2}\\cdot\\sum_{h=1}^{H}\\frac{\\log(|\\Pi_{h}|H\\delta^{-1})}{n}}+\\widetilde\\mu\\cdot\\sum_{h=1}^{H}\\frac{\\log(|\\Pi_{h}|H\\delta^{-1})}{n},\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "and when $\\pi^{\\star}$ is deterministic, ", "page_idx": 31}, {"type": "equation", "text": "$$\nJ(\\pi^{\\star})-J(\\widehat{\\pi})\\lesssim\\mu\\cdot\\sum_{h=1}^{H}\\frac{\\log(|\\Pi_{h}|H\\delta^{-1})}{n}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We note that for many parameter regimes, the sample complexity bound in Proposition E.1 can be worse than that of LogLossBC in Theorem 3.1 (for stationary policies, Proposition E.1 has spurious dependence on $H$ , and the variance-like quantity in the leading order term is weaker). It would be interesting to get the best of both worlds, though this may require changing the algorithm. ", "page_idx": 31}, {"type": "text", "text": "Proof of Proposition E.1. Consider an arbitrary policy $\\widehat{\\pi}$ . Begin by writing ", "page_idx": 31}, {"type": "equation", "text": "$$\nJ(\\pi^{\\star})-J(\\widehat\\pi)=\\sum_{h=1}^{H}\\mathbb{E}^{\\widehat\\pi|\\widehat\\pi}\\Big[Q_{h}^{\\pi^{\\star}}(x_{h},\\pi_{h}^{\\star}(x_{h}))-Q_{h}^{\\pi^{\\star}}(x_{h},a_{h})\\Big].\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Fix a layer $h$ . By Lemma 3.1, we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}^{\\widehat{\\pi}|\\widehat{\\pi}}\\left[Q_{h}^{\\pi^{\\star}}(x_{h},\\pi_{h}^{\\star}(x_{h}))-Q_{h}^{\\pi^{\\star}}(x_{h},a_{h})\\right]}\\\\ &{\\le\\mathbb{E}^{\\pi^{\\star}|\\widehat{\\pi}}\\left[Q_{h}^{\\pi^{\\star}}(x_{h},\\pi_{h}^{\\star}(x_{h}))-Q_{h}^{\\pi^{\\star}}(x_{h},a_{h})\\right]}\\\\ &{\\quad+\\sqrt{\\left(\\mathbb{E}^{\\widehat{\\pi}|\\widehat{\\pi}}[(Q_{h}^{\\pi^{\\star}}(x_{h},\\pi_{h}^{\\star}(x_{h}))-Q_{h}^{\\pi^{\\star}}(x_{h},a_{h}))^{2}]+\\mathbb{E}^{\\pi^{\\star}|\\widehat{\\pi}}[(Q_{h}^{\\pi^{\\star}}(x_{h},\\pi_{h}^{\\star}(x_{h}))-Q_{h}^{\\pi^{\\star}}(x_{h},a_{h}))^{2}]\\right)\\mathbb{E}^{\\widehat{\\pi}}[{\\cal D}_{h}^{2}(\\widehat{\\pi}_{h}(x_{h},a_{h}))]}\\\\ &{=\\sqrt{\\left(\\mathbb{E}^{\\widehat{\\pi}|\\widehat{\\pi}}[(Q_{h}^{\\pi^{\\star}}(x_{h},\\pi_{h}^{\\star}(x_{h}))-Q_{h}^{\\pi^{\\star}}(x_{h},a_{h}))^{2}]+\\mathbb{E}^{\\pi^{\\star}|\\widehat{\\pi}}[(Q_{h}^{\\pi^{\\star}}(x_{h},\\pi_{h}^{\\star}(x_{h}))-Q_{h}^{\\pi^{\\star}}(x_{h},a_{h}))^{2}]\\right)\\mathbb{E}^{\\widehat{\\pi}}[{\\cal D}_{h}^{2}(\\widehat{\\pi}_{h}(x_{h},a_{h}))]}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Furthermore, using Lemma 3.1, we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\mathbb{E}^{\\widehat\\pi|\\widehat\\pi}\\Big[(Q_{h}^{\\pi^{\\star}}(x_{h},\\pi_{h}^{\\star}(x_{h}))-Q_{h}^{\\pi^{\\star}}(x_{h},a_{h}))^{2}\\Big]}\\\\ &{\\displaystyle\\lesssim\\sum_{h=1}^{H}\\mathbb{E}^{\\pi^{\\star}|\\widehat\\pi}\\Big[(Q_{h}^{\\pi^{\\star}}(x_{h},\\pi_{h}^{\\star}(x_{h}))-Q_{h}^{\\pi^{\\star}}(x_{h},a_{h}))^{2}\\Big]+\\widetilde{\\mu}^{2}\\displaystyle\\sum_{h=1}^{H}\\mathbb{E}^{\\widehat\\pi}\\big[D_{\\mathsf{H}}^{2}(\\widehat\\pi_{h}(x_{h}),\\pi_{h}^{\\star}(x_{h}))\\big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "so that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\mathbb{E}^{\\widehat\\pi|\\widehat\\pi}\\Big[Q_{h}^{\\pi^{\\star}}(x_{h},\\pi_{h}^{\\star}(x_{h}))-Q_{h}^{\\pi^{\\star}}(x_{h},a_{h})\\Big]}&{\\quad{\\scriptstyle(15)}}\\\\ &{\\lesssim\\sqrt{\\mathbb{E}^{\\pi^{\\star}|\\widehat\\pi}[(Q_{h}^{\\pi^{\\star}}(x_{h},\\pi_{h}^{\\star}(x_{h}))-Q_{h}^{\\pi^{\\star}}(x_{h},a_{h}))^{2}]\\cdot\\mathbb{E}^{\\widehat\\pi}[D_{\\mathbb H}^{2}(\\widehat\\pi_{h}(x_{h}),\\pi_{h}^{\\star}(x_{h}))]}+\\widetilde\\mu\\cdot\\mathbb{E}^{\\widehat\\pi}\\big[D_{\\mathbb H}^{2}(\\widehat\\pi_{h}(x_{h}),\\pi_{h}^{\\star}(x_{h}))\\big]}&\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Recall that the Dagger policy satisfies ", "page_idx": 32}, {"type": "equation", "text": "$$\nJ(\\pi^{\\star})-J(\\widehat{\\pi})=\\frac{1}{n}\\sum_{i=1}^{n}J(\\pi^{\\star})-J(\\widehat{\\pi}^{i}).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Applying Eq. (15) to each policy ${\\widehat{\\pi}}^{i}$ , summing over all layer $h$ , and applying Cauchy-Schwarz yields ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{J(\\pi^{\\star})-J(\\widehat{\\pi})\\lesssim\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}\\sigma_{\\pi^{\\star}|\\pi^{i}}^{2}\\cdot\\mathbf{Est}_{\\mathsf{H}}^{\\mathsf{o n}}(n)}+\\widetilde{\\mu}\\cdot\\mathbf{Est}_{\\mathsf{H}}^{\\mathsf{o n}}(n)}}\\\\ &{}&{\\lesssim\\sqrt{\\overline{{\\sigma}}_{\\pi^{\\star}}^{2}\\cdot\\mathbf{Est}_{\\mathsf{H}}^{\\mathsf{o n}}(n)}+\\widetilde{\\mu}\\cdot\\mathbf{Est}_{\\mathsf{H}}^{\\mathsf{o n}}(n).}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "In the deterministic case, we tighten the argument above by applying the following improved changeof-measure argument based on Lemma 3.1: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb E^{\\pi^{\\star}\\mid\\widehat{\\pi}}\\Big[Q_{h}^{\\pi^{\\star}}(x_{h},\\pi_{h}^{\\star}(x_{h}))-Q_{h}^{\\pi^{\\star}}(x_{h},a_{h})\\Big]}\\\\ &{\\le\\mathbb E^{\\pi^{\\star}\\mid\\widehat{\\pi}}\\Big[(Q_{h}^{\\pi^{\\star}}(x_{h},\\pi_{h}^{\\star}(x_{h}))-Q_{h}^{\\pi^{\\star}}(x_{h},a_{h}))_{+}\\Big]}\\\\ &{\\le2\\mathbb E^{\\pi^{\\star}\\mid\\widehat{\\pi}}\\Big[(Q_{h}^{\\pi^{\\star}}(x_{h},\\pi_{h}^{\\star}(x_{h}))-Q_{h}^{\\pi^{\\star}}(x_{h},a_{h}))_{+}\\Big]+\\mu\\cdot\\mathbb E^{\\widehat{\\pi}}\\big[D_{\\mathsf{H}}^{2}(\\widehat{\\pi}_{h}(x_{h}),\\pi_{h}^{\\star}(x_{h}))\\big]}\\\\ &{=\\mu\\cdot\\mathbb E^{\\widehat{\\pi}}\\big[D_{\\mathsf{H}}^{2}(\\widehat{\\pi}_{h}(x_{h}),\\pi_{h}^{\\star}(x_{h}))\\big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "This leads to Eq. (14). ", "page_idx": 32}, {"type": "text", "text": "Proof of Proposition E.2. Since $\\pi^{\\star}\\in\\Pi$ , a standard guarantee for exponential weights with the log-loss (e.g., Cesa-Bianchi and Lugosi [19]) ensures that for all $h\\in[H]$ , the following bound holds almost surely: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}\\log(1/\\widehat{\\pi}_{h}^{i}(a_{h}^{\\star,i}\\mid x_{h}^{i}))\\leq\\sum_{i=1}^{n}\\log(1/\\pi_{h}^{\\star}(a_{h}^{\\star,i}\\mid x_{h}^{i}))+\\log|\\Pi_{h}|.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "From here, for each $h\\in[H]$ , Lemma A.14 of Foster et al. [34] implies that with probability at least $1-\\delta$ , ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}\\mathbb{E}^{\\widehat{\\pi}^{i}}\\left[D_{\\mathsf{H}}^{2}(\\widehat{\\pi}_{h}^{i}(x_{h}),\\pi^{\\star}(x_{h}))\\right]\\leq\\log\\lvert\\Pi_{h}\\rvert+2\\log(\\delta^{-1}).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "The result now follows by taking a union bound. ", "page_idx": 32}, {"type": "text", "text": "F Proofs from Section 2 ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "F.1 Proof of Theorem 2.1 ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Proof of Theorem 2.1. We begin by defining the following trajectory-wise semi-metric between policies. For a pair of potentially stochastic policies $\\pi$ and $\\pi^{\\prime}$ , define ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\rho(\\pi\\parallel\\pi^{\\prime}):=\\mathbb{E}^{\\pi}\\,\\mathbb{E}_{a_{1:H}^{\\prime}\\sim\\pi^{\\prime}(x_{1:H})}[\\mathbb{I}\\{\\exists h:a_{h}\\neq a_{h}^{\\prime}\\}],\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where we use the shorthand $a_{1:H}^{\\prime}\\,\\sim\\,\\pi^{\\prime}(x_{1:H})$ to indicate that $a_{1}^{\\prime}\\,\\sim\\,\\pi_{1}^{\\prime}(x_{1}),.\\..\\,.\\,,a_{H}^{\\prime}\\,\\sim\\,\\pi_{H}^{\\prime}(x_{H})$ . Despite being defined in an asymmetric fashion, the following lemma shows that the trajectory-wise distance $\\rho(\\cdot\\,\\|\\,\\cdot)$ is symmetric, from which it follows that it is indeed a semi-metric. ", "page_idx": 33}, {"type": "text", "text": "Lemma F.1. For all (potentially stochastic) policies $\\pi$ and $\\pi^{\\prime}$ , it holds that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\rho(\\pi\\parallel\\pi^{\\prime})=\\rho(\\pi^{\\prime}\\parallel\\pi).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Next, we show that it is possible to bound the difference in reward for any pair of policies in terms of the trajectory-wise distance $\\rho(\\cdot\\parallel\\cdot)$ . ", "page_idx": 33}, {"type": "text", "text": "Lemma F.2. For all (potentially stochastic) policies $\\pi$ and $\\pi^{\\prime}$ , it holds that ", "page_idx": 33}, {"type": "equation", "text": "$$\nJ(\\pi)-J(\\pi^{\\prime})\\leq R\\cdot\\rho(\\pi\\parallel\\pi^{\\prime}).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Finally, using Lemma F.1, we show that when one of the policies is deterministic, the trajectory-wise distance is equivalent to Hellinger distance up to an absolute constant. ", "page_idx": 33}, {"type": "text", "text": "Lemma F.3. Let $\\pi^{\\star}$ be a deterministic policy and $\\pi$ be an arbitrary stochastic policy. Then we have that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\frac{1}{4}\\cdot\\rho(\\pi^{\\star}\\parallel\\pi)\\le D_{\\mathsf{H}}^{2}\\Big(\\mathbb{P}^{\\pi},\\mathbb{P}^{\\pi^{\\star}}\\Big)\\le2\\cdot\\rho(\\pi^{\\star}\\parallel\\pi).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Combining Lemmas F.2 and F.3, we conclude that for any deterministic policy $\\pi^{\\star}$ and stochastic policy $\\widehat{\\pi}$ , ", "page_idx": 33}, {"type": "equation", "text": "$$\nJ(\\pi^{\\star})-J(\\widehat\\pi)\\leq4R\\cdot D_{\\mathsf{H}}^{2}\\big(\\mathbb{P}^{\\widehat\\pi},\\mathbb{P}^{\\pi^{\\star}}\\big).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Proof of Lemma F.1. This follows by noting that we can write ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\rho(\\pi\\parallel\\pi^{\\prime})=1-\\mathbb{E}^{\\pi}\\mathbb{E}_{a_{1:H}^{\\prime}\\sim\\pi^{\\prime}(x_{1:H})}[\\mathbb{I}\\{a_{h}=a_{h}^{\\prime}\\vee h\\}]}\\\\ &{\\qquad\\qquad=1-\\underset{x_{1:H},a_{1:H},a_{1:H}^{\\prime}}{\\sum}P_{0}(x_{1})\\underset{h=1}{\\overset{H}{\\prod}}P_{h}(x_{h+1}\\mid x_{h},a_{h})\\pi_{h}(a_{h}\\mid x_{h})\\pi_{h}^{\\prime}(a_{h}^{\\prime}\\mid x_{h})\\mathbb{I}\\{a_{h}=a_{h}^{\\prime}\\}}\\\\ &{\\qquad\\qquad=1-\\underset{x_{1:H},a_{1:H},a_{1:H}^{\\prime}}{\\sum}P_{0}(x_{1})\\underset{h=1}{\\overset{H}{\\prod}}P_{h}(x_{h+1}\\mid x_{h},a_{h}^{\\prime})\\pi_{h}(a_{h}\\mid x_{h})\\pi_{h}^{\\prime}(a_{h}^{\\prime}\\mid x_{h})\\mathbb{I}\\{a_{h}=a_{h}^{\\prime}\\}}\\\\ &{\\qquad\\qquad=1-\\mathbb{E}^{\\pi^{\\prime}}\\mathbb{E}_{a_{1:H}^{\\prime}\\sim\\pi(x_{1:H})}[\\mathbb{I}\\{a_{h}=a_{h}^{\\prime}\\vee h\\}]=\\rho(\\pi^{\\prime}\\parallel\\pi).}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Proof of Lemma F.2. Observe that since $\\textstyle\\sum_{h=1}^{H}r_{h}\\in[0,R]$ , we can bound the reward for $\\pi$ as ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I(\\pi)\\leq\\,\\mathbb{E}^{\\pi}\\left[\\left(\\displaystyle\\sum_{h=1}^{H}r_{h}\\right)\\mathbb{E}_{a_{1:H}^{\\prime}\\sim\\pi^{\\prime}(x_{1:H})}[\\mathbb{I}\\{a_{h}^{\\prime}=a_{h}\\,\\forall h\\}]\\right]+R\\cdot\\mathbb{E}^{\\pi}\\,\\mathbb{E}_{a_{1:H}^{\\prime}\\sim\\pi^{\\prime}(x_{1:H})}[\\mathbb{I}\\{\\exists h:\\,a_{h}^{\\prime}\\neq a_{h}\\}]}\\\\ &{\\quad=\\mathbb{E}^{\\pi}\\left[\\left(\\displaystyle\\sum_{h=1}^{H}r_{h}\\right)\\mathbb{E}_{a_{1:H}^{\\prime}\\sim\\pi^{\\prime}(x_{1:H})}[\\mathbb{I}\\{a_{h}^{\\prime}=a_{h}\\,\\forall h\\}]\\right]+R\\cdot\\rho(\\pi\\mid\\|\\,\\pi^{\\prime}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "We can bound the first term as ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}^{\\pi}\\Bigg[\\Bigg(\\displaystyle\\sum_{h=1}^{H}r_{h}\\Bigg)\\mathbb{E}_{a_{1:H}^{\\prime}\\sim\\pi^{\\prime}(x_{1:H})}[\\mathbb{I}\\{a_{h}^{\\prime}=a_{h}\\;\\forall h\\}]\\Bigg]}\\\\ &{=\\mathbb{E}^{\\pi}\\Big[f\\big(x_{1:H},a_{1:H}\\big)\\mathbb{E}_{a_{1:H}^{\\prime}\\sim\\pi^{\\prime}(x_{1:H})}[\\mathbb{I}\\{a_{h}^{\\prime}=a_{h}\\;\\forall h\\}]\\Big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where $\\begin{array}{r}{f(x_{1:H},a_{1:H}):=\\,\\sum_{h=1}^{H}\\mathbb{E}[r_{h}\\mid x_{h},a_{h}]}\\end{array}$ . We now observe that for any function $f$ ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}^{\\pi}\\Big[f\\big(x_{1:H},a_{1:H},a_{1:H}\\big)\\mathbb{E}_{a_{1:H}^{\\prime}\\sim\\pi^{\\prime}(x_{1:H})}[\\mathbb{I}\\{a_{h}^{\\prime}=a_{h}\\vee h\\}]\\Big]}\\\\ &{=\\displaystyle\\sum_{x_{1:H},a_{1:H},a_{1:H}^{\\prime},a_{1:H}^{\\prime}}f(x_{1:H},a_{1:H})\\cdot\\prod_{h=1}^{H}P_{h}\\big(x_{h+1}\\mid x_{h},a_{h}\\rangle\\pi_{h}(a_{h}\\mid x_{h})\\pi_{h}^{\\prime}(a_{h}^{\\prime}\\mid x_{h})\\mathbb{I}\\{a_{h}=a_{h}^{\\prime}\\}}\\\\ &{=\\displaystyle\\sum_{x_{1:H},a_{1:H},a_{1:H}^{\\prime},a_{1:H}^{\\prime}}f(x_{1:H},a_{1:H}^{\\prime})\\cdot P_{0}(x_{1})\\displaystyle\\prod_{h=1}^{H}P_{h}\\big(x_{h+1}\\mid x_{h},a_{h}^{\\prime}\\big)\\pi_{h}(a_{h}\\mid x_{h})\\pi_{h}^{\\prime}(a_{h}^{\\prime}\\mid x_{h})\\mathbb{I}\\{a_{h}=a_{h}^{\\prime}\\}}\\\\ &{\\le\\displaystyle\\sum_{x_{1:H},a_{1:H}^{\\prime}}f(x_{1:H},a_{1:H}^{\\prime})\\cdot P_{0}(x_{1})\\displaystyle\\prod_{h=1}^{H}P_{h}\\big(x_{h+1}\\mid x_{h},a_{h}^{\\prime}\\big)\\pi_{h}^{\\prime}(a_{h}^{\\prime}\\mid x_{h})}\\\\ &{=\\mathbb{E}^{\\pi^{\\prime}}\\big[f(x_{1:H},a_{1:H}^{\\prime})\\big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "We conclude that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbb{E}^{\\pi}\\Bigg[\\left(\\sum_{h=1}^{H}r_{h}\\right)\\mathbb{E}_{a_{1:H}^{\\prime}\\sim\\pi^{\\prime}(x_{1:H})}[\\mathbb{I}\\{a_{h}^{\\prime}=a_{h}\\vee h\\}]\\Bigg]\\leq J(\\pi^{\\prime}),\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "so that ", "page_idx": 34}, {"type": "equation", "text": "$$\nJ(\\pi)-J(\\pi^{\\prime})\\leq R\\cdot\\rho(\\pi\\parallel\\pi^{\\prime}).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Proof of Lemma F.3. Define the triangular discrimination via $\\begin{array}{r}{D_{\\Delta}(\\mathbb{P},\\mathbb{Q}):=\\int\\frac{\\left(d\\mathbb{P}-d\\mathbb{Q}\\right)^{2}}{d\\mathbb{P}+d\\mathbb{Q}}}\\end{array}$ dPP+dQQ , and recall that $\\textstyle\\frac12D_{\\Delta}(\\mathbb{P},\\mathbb{Q})\\leq D_{\\mathsf{H}}^{2}(\\mathbb{P},\\mathbb{Q})\\leq D_{\\Delta}(\\mathbb{P},\\mathbb{Q})$ (e.g., Foster and Krishnamurthy [32]). Next, define the shorthand $\\begin{array}{r}{P(x_{1:H}\\mid a_{1:H}):=\\prod_{h=0}^{H-1}P(x_{h+1}\\mid x_{h},a_{h})}\\end{array}$ and $\\begin{array}{r}{P^{\\pi}(a_{1:H}\\mid x_{1:H}):=\\prod_{h=1}^{H}\\pi_{h}(a_{h}\\mid x_{h})}\\end{array}$ (these quantities do not have an interpretation as conditional probability measures in the way the notation might suggest, but this will not be relevant to the proof). For any deterministic policy $\\pi^{\\star}$ , we can write ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D_{\\Delta}\\Big(\\mathbb{P}^{\\pi},\\mathbb{P}^{\\pi^{*}}\\Big)}\\\\ &{=\\displaystyle\\sum_{x_{1:H}}\\sum_{a_{1:H}}P(x_{1:H}\\mid a_{1:H-1})\\cdot\\frac{\\big(P^{\\pi}(a_{1:H}\\mid x_{1:H})-P^{\\pi^{*}}(a_{1:H}\\mid x_{1:H})\\big)^{2}}{P^{\\pi}(a_{1:H}\\mid x_{1:H})+P^{\\pi^{*}}(a_{1:H}\\mid x_{1:H})}}\\\\ &{=\\displaystyle\\sum_{x_{1:H}}\\sum_{a_{1:H}=\\pi^{*}(x_{1:H}\\mid a_{1:H-1})}P(x_{1:H}\\mid a_{1:H-1})\\cdot\\frac{\\big(P^{\\pi}(a_{1:H}\\mid x_{1:H})-P^{\\pi^{*}}(a_{1:H}\\mid x_{1:H})\\big)^{2}}{P^{\\pi}(a_{1:H}\\mid x_{1:H})+P^{\\pi^{*}}(a_{1:H}\\mid x_{1:H})}}\\\\ &{\\quad+\\displaystyle\\sum_{x_{1:H}}\\sum_{a_{1:H}=\\pi^{*}(x_{1:H}\\mid a_{1:H-1})}P(x_{1:H}\\mid a_{1:H-1})\\cdot\\frac{\\big(P^{\\pi}(a_{1:H}\\mid x_{1:H})-P^{\\pi^{*}}(a_{1:H}\\mid x_{1:H})\\big)^{2}}{P^{\\pi}(a_{1:H}\\mid x_{1:H})+P^{\\pi^{*}}(a_{1:H}\\mid x_{1:H})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Since $\\pi^{\\star}$ is deterministic, $P^{\\pi^{\\star}}(a_{1:H}\\mid x_{1:H})=1$ if $a_{1:H}=\\pi^{\\star}(x_{1:H})$ , and is $P^{\\pi^{\\star}}(a_{1:H}\\mid x_{1:H})=0$ otherwise. Using this, we can write the second term above as ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{x_{1:H}}\\sum_{a_{1:H}\\neq\\pi^{\\star}(x_{1:H})}P(x_{1:H}\\mid a_{1:H-1})\\cdot\\frac{\\big(P^{\\pi}(a_{1:H}\\mid x_{1:H})-0\\big)^{2}}{P^{\\pi}(a_{1:H}\\mid x_{1:H})+0}}\\\\ &{=\\displaystyle\\sum_{x_{1:H}}\\sum_{a_{1:H}\\neq\\pi^{\\star}(x_{1:H})}P(x_{1:H}\\mid a_{1:H-1})P^{\\pi}(a_{1:H}\\mid x_{1:H})}\\\\ &{=\\mathbb{P}^{\\pi}[\\exists h:a_{h}\\neq\\pi^{\\star}(x_{H})]=\\rho(\\pi\\mid\\mid\\pi^{\\star}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "This proves that $D_{\\Delta}\\bigl(\\mathbb{P}^{\\pi},\\mathbb{P}^{\\pi^{\\star}}\\bigr)\\geq\\rho(\\pi\\parallel\\pi^{\\star})$ . For the upper bound, we use that $\\pi^{\\star}$ is deterministic once more to write the first term above as ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{x_{1:H}}\\sum_{a_{1:H}=\\pi^{\\star}(x_{1:H})}P(x_{1:H}\\mid a_{1:H-1})\\cdot\\frac{\\big(P^{\\pi}(a_{1:H}\\mid x_{1:H})-1\\big)^{2}}{P^{\\pi}(a_{1:H}\\mid x_{1:H})+1}}\\\\ &{=\\mathbb{E}^{\\pi^{\\star}}\\left[\\frac{\\big(P^{\\pi}(a_{1:H}\\mid x_{1:H})-1\\big)^{2}}{P^{\\pi}(a_{1:H}\\mid x_{1:H})+1}\\right]\\leq\\mathbb{E}^{\\pi^{\\star}}\\left[\\left(P^{\\pi}(a_{1:H}\\mid x_{1:H})-1\\right)^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "We further note that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}^{\\pi^{\\star}}\\big[(P^{\\pi}(a_{1:H}\\mid x_{1:H})-1)^{2}\\big]}\\\\ &{=\\mathbb{E}^{\\pi^{\\star}}\\mathbb{E}_{a_{1:H}^{\\prime}\\sim\\pi(x_{1:H})}[1+\\big(P^{\\pi}(a_{1:H}^{\\prime}\\mid x_{1:H})-2\\big)\\mathbb{I}\\{a_{1:H}^{\\prime}=a_{1:H}\\}]}\\\\ &{\\le\\mathbb{E}^{\\pi^{\\star}}\\mathbb{E}_{a_{1:H}^{\\prime}\\sim\\pi(x_{1:H})}[1-\\mathbb{I}\\{a_{1:H}^{\\prime}=a_{1:H}\\}]}\\\\ &{=\\mathbb{E}^{\\pi^{\\star}}\\mathbb{E}_{a_{1:H}^{\\prime}\\sim\\pi(x_{1:H})}[\\mathbb{I}\\{\\exists h:a_{1:H}^{\\prime}\\neq a_{1:H}\\}]=\\rho(\\pi^{\\star}\\parallel\\pi).}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "By Lemma F.1, we conclude that $D_{\\Delta}\\bigl(\\mathbb{P}^{\\pi},\\mathbb{P}^{\\pi^{\\star}}\\bigr)\\leq\\rho(\\pi\\parallel\\pi^{\\star})+\\rho(\\pi^{\\star}\\parallel\\pi)=2\\rho(\\pi^{\\star}\\parallel\\pi).$ ", "page_idx": 35}, {"type": "text", "text": "F.2 Proof of Theorem 2.2 ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Proof of Theorem 2.2. For this proof, we consider a slightly more general online imitation learning model in which the learner is allowed to select $a_{h}^{i}$ based on the sequence $(x_{1}^{i},a_{1}^{i},a_{1}^{\\star,i}),\\ldots,(x_{h-1}^{i},a_{h-1}^{i},a_{h-1}^{\\star,i}),(x_{h}^{i},a_{h}^{\\star,i})$ at training time; this subsumes the offline imitation learning model. Let $n\\in\\mathbb N$ and $H\\in\\mathbb{N}$ be fixed. Let $\\Delta\\in(0,1/3)$ be a parameter whose value will be chosen later. ", "page_idx": 35}, {"type": "text", "text": "We first specify the dynamics for the reward-free MDP $M^{\\star}$ and the policy class \u03a0. Set $\\mathcal{X}=\\{\\mathfrak{x},\\mathfrak{y}\\}$ and $\\bar{\\mathcal{A}}=\\bar{\\{}\\mathfrak{a},\\bar{\\mathfrak{v}}\\}$ . The initial state distribution sets $P_{0}(\\pmb{\\xi})=1-\\Delta$ and $\\dot{P_{0}}(\\mathfrak{y})=\\Delta$ . The transition dynamics are $\\dot{P_{h}}(x^{\\prime}\\mid x,a)=\\mathbb{I}\\{x^{\\prime}=x\\}$ for all $h$ ; that is, $\\mathfrak{X},\\mathfrak{Y}$ are self-looping terminal states. We set $\\dot{\\Pi}=\\{\\pi^{\\mathfrak{a}},\\pi^{\\mathfrak{b}}\\}$ , where the expert policies are $\\pi^{\\mathfrak{a}}$ , which sets $\\pi_{h}^{\\mathfrak{a}}(x)=\\mathfrak{a}$ for all $h$ and $x$ , and $\\pi^{\\mathrm{{b}}}$ , which sets $\\pi_{h}^{\\mathfrak{h}}(\\mathfrak{x})=\\mathfrak{a}$ and sets $\\pi_{h}^{\\mathfrak{h}}(\\mathfrak{y})=\\mathfrak{b}$ . ", "page_idx": 35}, {"type": "text", "text": "Let a problem instance $\\mathcal{T}=(M^{\\star},r,\\pi^{\\star})$ refer to a tuple consisting of the reward-free MDP $M^{\\star}$ , a reward function $r\\,=\\,\\{r_{h}\\}_{h=1}^{H}$ , and an expert policy $\\pi^{\\star}$ . We consider two problem instances, $\\mathcal{T}^{\\mathrm{a}}=(M^{\\star},r^{\\mathrm{a}},\\pi^{\\mathrm{a}})$ and $T^{\\flat}=(\\overbar{M^{\\star}},r^{\\flat},\\pi^{\\flat})$ : ", "page_idx": 35}, {"type": "text", "text": "\u2022 For problem instance $\\mathcal{T}^{\\mathrm{a}}$ , the expert policy is $\\pi^{\\mathrm{a}}$ . We set $r_{h}^{\\mathfrak{a}}(\\mathfrak{x},\\cdot)=0$ , $r_{h}^{\\mathrm{a}}(\\mathfrak{y},a)=\\mathbb{I}\\{a=\\mathfrak{a}\\}$ for all $h$ .   \n\u2022 For problem instance $\\mathcal{T}^{\\mathrm{{b}}}$ , the expert policy is $\\pi^{\\mathrm{{b}}}$ . We set $r_{h}^{\\flat}({\\texttt x},\\cdot)=0$ , $r_{h}^{\\flat}({\\mathfrak{y}},a)=\\mathbb{I}\\{a={\\mathfrak{h}}\\}$ for all $h$ . ", "page_idx": 35}, {"type": "text", "text": "Note that both of these instances satisfy $\\mu\\,=\\,1$ , and that $\\pi^{\\mathrm{a}}$ and $\\pi^{\\mathrm{{b}}}$ are optimal policies for their respective instances. Let $J^{\\mathrm{{a}}}$ denote the expected reward function for instance $\\mathcal{T}^{\\mathrm{a}}$ , and likewise for $\\mathcal{T}^{\\mathrm{{b}}}$ . ", "page_idx": 35}, {"type": "text", "text": "Going forward, we fix the online imitation learning algorithm under consideration and let $\\mathbb{P}^{\\mathrm{a}}$ denote the law of $O^{1},\\ldots,O^{n}$ when we execute the algorithm on instance ${\\mathfrak a}$ , and likewise for $\\mathfrak{b}$ ; let $\\mathbb{E}^{\\mathfrak{a}}[\\cdot]$ and $\\mathbb{E}^{\\mathfrak{b}}[\\cdot]$ denote the corresponding expectations. In addition, for any policy $\\pi$ , let $\\mathbb{P}^{\\pi^{\\mathrm{a}}|\\pi}$ denote the law of $o\\bar{=^{\\prime}}(x_{1},a_{1},a_{1}^{\\star}),\\ldots,(x_{H},a_{H},a_{H}^{\\star})$ when we execute $\\pi$ in the online imitation learning framework and the expert policy is $\\pi^{\\star}=\\pi^{\\mathrm{a}}$ , and define $\\mathbb{P}^{\\pi^{\\flat}|\\pi}$ analogously. ", "page_idx": 35}, {"type": "text", "text": "We first observe that for any policy $\\widehat{\\pi}$ , ", "page_idx": 35}, {"type": "equation", "text": "$$\nJ^{\\mathfrak{a}}(\\pi^{\\mathfrak{a}})-J^{\\mathfrak{a}}(\\widehat{\\pi})=\\Delta\\cdot\\sum_{h=1}^{H}\\mathbb{E}_{a_{h}\\sim\\widehat{\\pi}_{h}(\\mathfrak{y})}[\\mathbb{I}\\{a_{h}\\neq\\pi_{h}^{\\mathfrak{a}}(\\mathfrak{y})\\}],\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "and that $\\begin{array}{r c l}{{\\cal J}^{\\mathrm{\\mathrm{\\scriptscriptstyle0}}}(\\pi^{\\mathrm{\\scriptscriptstyleb}})\\;-\\;{\\cal J}^{\\mathrm{\\scriptscriptstyleb}}(\\widehat{\\pi})}&{=}&{\\Delta\\,\\mathrm{\\boldmath~\\cdot~}\\,\\sum_{h=1}^{H}\\mathbb{E}_{a_{h}\\sim\\widehat{\\pi}_{h}(\\mathfrak{y})}[\\mathbb{I}\\{a_{h}\\neq\\pi_{h}^{\\mathrm{\\scriptscriptstyleb}}(\\mathfrak{y})\\}].}\\end{array}$ Defining $\\begin{array}{r l}{\\rho(\\pi,\\pi^{\\prime})}&{{}=}\\end{array}$ $\\begin{array}{r}{\\sum_{h=1}^{H}\\mathbb{E}_{a_{h}\\sim\\pi_{h}(\\mathbf{y}),a_{h}^{\\prime}\\sim\\pi_{h}^{\\prime}(\\mathbf{y})}\\mathbb{I}\\{a_{h}\\neq a_{h}^{\\prime}\\}}\\end{array}$ as a metric, we note that $\\rho(\\pi^{\\mathfrak{a}},\\pi^{\\mathfrak{b}})=H$ , and hence by the standard Le Cam two-point argument (e.g.,. Wainwright [91]), the algorithm must have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\{\\mathbb{E}^{*}[J^{\\mathrm{s}}(\\pi^{\\mathrm{s}})-J^{\\mathrm{s}}(\\widehat{\\pi})],\\mathbb{E}^{\\flat}[J^{\\mathrm{s}}(\\pi^{\\mathrm{b}})-J^{\\mathrm{b}}(\\widehat{\\pi})]\\}\\geq\\frac{\\Delta H}{4}(1-D_{\\mathsf{T V}}(\\mathbb{P}^{\\mathrm{s}},\\mathbb{P}^{\\mathrm{b}})),\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where $D_{\\mathsf{T V}}(\\cdot,\\cdot)$ denotes total variation distance. Next, using Lemma D.2 of Foster et al. [36], we can bound ", "page_idx": 36}, {"type": "equation", "text": "$$\nD_{\\mathsf{T V}}^{2}(\\mathbb{P}^{\\mathrm{a}},\\mathbb{P}^{\\mathrm{b}})\\leq D_{\\mathsf{H}}^{2}(\\mathbb{P}^{\\mathrm{a}},\\mathbb{P}^{\\mathrm{b}})\\leq7\\,\\mathbb{E}^{\\mathrm{a}}\\left[\\sum_{i=1}^{n}D_{\\mathsf{H}}^{2}\\Big(\\mathbb{P}^{\\pi^{\\mathrm{a}}|\\pi^{i}},\\mathbb{P}^{\\pi^{\\mathrm{b}}|\\pi^{i}}\\Big)\\right].\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Since, the feedback the learner receives for a given episode $i$ is identical under instances $\\mathcal{T}^{\\mathrm{a}}$ and $\\mathcal{T}^{\\mathrm{{b}}}$ unless $x_{1}=\\mathfrak{y}$ (regardless of how $\\pi^{i}$ is chosen), we can bound ", "page_idx": 36}, {"type": "equation", "text": "$$\nD_{\\mathsf{H}}^{2}\\Big(\\mathbb{P}^{\\pi^{a}|\\pi^{i}},\\mathbb{P}^{\\pi^{\\mathsf{b}}|\\pi^{i}}\\Big)\\leq2\\Delta,\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "and hence ", "page_idx": 36}, {"type": "equation", "text": "$$\nD_{\\mathsf{T V}}^{2}(\\mathbb{P}^{\\mathrm{a}},\\mathbb{P}^{\\mathrm{b}})\\leq14\\Delta n.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "We set $\\Delta=1/56n$ , and conclude that any algorithm must have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\{\\mathbb{E}^{{\\mathsf{a}}}[J^{\\mathrm{a}}(\\pi^{\\mathrm{a}})-J^{\\mathrm{a}}(\\widehat{\\pi})],\\mathbb{E}^{\\mathrm{b}}[J^{\\mathrm{b}}(\\pi^{\\mathrm{b}})-J^{\\mathrm{b}}(\\widehat{\\pi})]\\}\\ge\\frac{\\Delta H}{8}=c\\cdot\\frac{H}{n}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "for an absolute constant $c>0$ . ", "page_idx": 36}, {"type": "text", "text": "G Proofs from Section 3 ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "G.1 Proof of Theorem 3.1 ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Proof of Theorem 3.1. Assume without loss of generality that $\\textit{R}=\\textit{1}$ . Let $\\begin{array}{r l}{O}&{{}=}\\end{array}$ $(x_{1},a_{1}),\\dots,(x_{H},a_{H})$ , and for each $h\\in[H]$ , define the sum of advantages up to step $h$ via ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\Delta_{h}(o)=\\sum_{\\ell=1}^{h}\\Bigl(Q_{\\ell}^{\\pi^{\\star}}(x_{\\ell},\\pi_{\\ell}^{\\star}(x_{\\ell}))-Q_{\\ell}^{\\pi^{\\star}}(x_{\\ell},a_{\\ell})\\Bigr),\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "which has $|\\Delta(o)|\\leq H$ almost surely. Consider the filtration $\\mathcal{F}_{h}:=\\sigma(x_{1},a_{1},\\dots,x_{h},a_{h})$ . Fix a parameter $L\\geq1$ whose value will be chosen later, and define a random variable ", "page_idx": 36}, {"type": "equation", "text": "$$\nH^{\\star}:=\\,\\operatorname*{min}\\{h\\mid|\\Delta_{h}(o)|>L\\},\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "with $H^{\\star}:=\\,H+1$ if there is no $h$ such that $|\\Delta_{h}(o)|\\;>\\;L$ ; we will adopt the convention that $Q_{H+1}^{\\pi^{\\star}}=V_{H+1}^{\\pi^{\\star}}=0$ . ", "page_idx": 36}, {"type": "text", "text": "Lemma G.1. $H^{\\star}$ is a stopping time with respect $(\\mathcal{F}_{h})_{h\\geq1}$ ,17 and has $|\\Delta_{H^{\\star}}(o)|\\le L+1$ almost surely. ", "page_idx": 36}, {"type": "text", "text": "The following lemma, which is one of the central technical components of this proof, gives a bound on regret in terms of the expected advantage at the stopping time $H^{\\star}$ . We use the stopping time to keep the sum of advantages $\\Delta_{H^{\\star}}$ bounded, which facilitates a strong change-of-measure argument in the sequel. ", "page_idx": 36}, {"type": "text", "text": "Lemma G.2 (Regret decomposition for stopped advantages). If $r_{h}\\geq0$ and h=1 rh \u2208[0, R], then for all policies $\\widehat{\\pi}$ , we have that ", "page_idx": 36}, {"type": "equation", "text": "$$\nJ(\\pi^{\\star})-J(\\widehat{\\pi})\\leq\\mathbb{E}^{\\widehat{\\pi}}[\\Delta_{H^{\\star}}(o)]+R\\cdot\\mathbb{P}^{\\widehat{\\pi}}[H^{\\star}\\leq H].\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Note that even though we assume $R=1$ throughout this proof, we state this lemma for general $R$ for the sake of keeping it self-contained. ", "page_idx": 36}, {"type": "text", "text": "We proceed to bound the right-hand-side of Eq. (16) using change-of-measure based on Hellinger distance (Lemma 3.1). For the second term in Eq. (16), Lemma 3.1 gives ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}^{\\widehat\\pi}[H^{\\star}\\leq H]\\leq2\\mathbb{P}^{\\pi^{\\star}}[H^{\\star}\\leq H]+D_{\\mathsf{H}}^{2}\\Big(\\mathbb{P}^{\\widehat\\pi},\\mathbb{P}^{\\pi^{\\star}}\\Big)}\\\\ &{\\qquad\\qquad\\qquad=2\\mathbb{P}^{\\pi^{\\star}}[\\exists h:|\\Delta_{h}(o)|>L]+D_{\\mathsf{H}}^{2}\\Big(\\mathbb{P}^{\\widehat\\pi},\\mathbb{P}^{\\pi^{\\star}}\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "17That is, for all $h$ $,\\mathbb{I}\\{h=H^{\\star}\\}$ is a measurable function of $(x_{1},a_{1}),\\ldots,(x_{h},a_{h})$ . ", "page_idx": 36}, {"type": "text", "text": "For the first term in Eq. (16), Lemma 3.1, gives that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}^{\\widehat\\pi}[\\Delta_{H^{\\star}}(o)]\\leq\\mathbb{E}^{\\pi^{\\star}}[\\Delta_{H^{\\star}}(o)]+\\sqrt{\\frac{1}{2}\\Big(\\mathbb{E}^{\\widehat\\pi}[\\Delta_{H^{\\star}}^{2}(o)]+\\mathbb{E}^{\\pi^{\\star}}[\\Delta_{H^{\\star}}^{2}(o)]\\Big)\\cdot D_{\\mathbb{H}}^{2}(\\mathbb{P}^{\\widehat\\pi},\\mathbb{P}^{\\pi^{\\star}})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "To bound the first moment and second moment of $\\Delta_{H^{\\star}}(o)$ under $\\pi^{\\star}$ , we use the following lemma, which follows from elementary properties of stopped martingale difference sequences. ", "page_idx": 37}, {"type": "text", "text": "Lemma G.3. We have that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}^{\\pi^{\\star}}[\\Delta_{H^{\\star}}(o)]\\leq0,\\quad a n d\\quad\\mathbb{E}^{\\pi^{\\star}}[\\Delta_{H^{\\star}}^{2}(o)]\\leq4\\sigma_{\\pi^{\\star}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "It remains to bound the second moment under $\\widehat{\\pi}$ . Here, since $|\\Delta_{H^{\\star}}(o)|\\le L+1$ almost surely by Lemma G.1, we note that Lemma 3.1 gives ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}^{\\widehat{\\pi}}\\left[\\Delta_{H^{\\star}}^{2}(o)\\right]\\leq2\\,\\mathbb{E}^{\\pi^{\\star}}\\left[\\Delta_{H^{\\star}}^{2}(o)\\right]+(L+1)^{2}D_{\\mathsf{H}}^{2}\\Big(\\mathbb{P}^{\\widehat{\\pi}},\\mathbb{P}^{\\pi^{\\star}}\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Combining these developments, we have that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}^{\\widehat\\pi}[\\Delta_{H^{\\star}}(o)]\\leq\\sqrt{\\frac{3}{2}\\mathbb{E}^{\\pi^{\\star}}[\\Delta_{H^{\\star}}^{2}(o)]\\cdot D_{\\mathsf{H}}^{2}(\\mathbb{P}^{\\widehat\\pi},\\mathbb{P}^{\\pi^{\\star}})}+(L+1)D_{\\mathsf{H}}^{2}\\Big(\\mathbb{P}^{\\widehat\\pi},\\mathbb{P}^{\\pi^{\\star}}\\Big)}\\\\ &{\\qquad\\qquad\\leq\\sqrt{6\\sigma_{\\pi^{\\star}}^{2}\\cdot D_{\\mathsf{H}}^{2}(\\mathbb{P}^{\\widehat\\pi},\\mathbb{P}^{\\pi^{\\star}})}+(L+1)D_{\\mathsf{H}}^{2}\\Big(\\mathbb{P}^{\\widehat\\pi},\\mathbb{P}^{\\pi^{\\star}}\\Big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "and thus ", "page_idx": 37}, {"type": "equation", "text": "$$\nJ(\\pi^{\\star})-J(\\widehat\\pi)\\leq\\sqrt{6\\sigma_{\\pi^{\\star}}^{2}\\cdot D_{\\mathsf{H}}^{2}(\\mathbb{P}^{\\widehat\\pi},\\mathbb{P}^{\\pi^{\\star}})}+(L+2)D_{\\mathsf{H}}^{2}\\Big(\\mathbb{P}^{\\widehat\\pi},\\mathbb{P}^{\\pi^{\\star}}\\Big)+2\\mathbb{P}^{\\pi^{\\star}}[\\exists h:|\\Delta_{h}(o)|>L].\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "To wrap up, we appeal to the second of our main technical lemmas, Lemma G.4. ", "page_idx": 37}, {"type": "text", "text": "Lemma G.4 (Concentration for advantages). Assume that $r_{h}\\,\\geq\\,0$ and $\\textstyle\\sum_{h=1}^{H}r_{h}\\,\\in\\,[0,R]$ almost surely for some $R>0$ . Then for any (potentially stochastic) policy $\\pi$ , it holds that for all $\\delta\\in(0,e^{-1})$ , ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathbb{P}^{\\pi}\\left[\\exists H^{\\prime}:\\left|\\sum_{h=1}^{H^{\\prime}}Q_{h}^{\\pi}(x_{h},a_{h})-V_{h}^{\\pi}(x_{h})\\right|\\geq c\\cdot R\\log(\\delta^{-1})\\right]\\leq\\delta,\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "for an absolute constant $c>0$ . ", "page_idx": 37}, {"type": "text", "text": "Let $\\varepsilon\\in(0,e^{-1})$ be fixed. If we define ", "page_idx": 37}, {"type": "equation", "text": "$$\nL=c\\cdot\\log(\\varepsilon^{-1}),\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where $c>1$ is a sufficiently large absolute constant, then by Lemma G.4, we have that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathbb{P}^{\\pi^{\\star}}[\\exists h:|\\Delta_{h}(o)|>L]\\le\\varepsilon.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "This proves the result. ", "page_idx": 37}, {"type": "text", "text": "Proof of Lemma G.1. To prove that $H^{\\star}$ is a stopping time, we observe that for all $h\\leq H$ , we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathbb{I}\\{\\boldsymbol{h}=\\boldsymbol{H}^{\\star}\\}=\\mathbb{I}\\{|\\Delta_{\\boldsymbol{h}}(o)|>L,|\\Delta_{\\boldsymbol{h^{\\prime}}}(o)|\\le L\\,\\forall\\boldsymbol{h^{\\prime}}<\\boldsymbol{h}\\},\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "and $\\Delta_{h}(o)$ is a measurable function of $(x_{1},a_{1}),\\ldots,(x_{h},a_{h})$ . Likewise, we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathbb{I}\\{\\boldsymbol{h}=\\boldsymbol{H}^{\\star}+1\\}=\\mathbb{I}\\{|\\Delta_{\\boldsymbol{h}}(o)|\\leq\\boldsymbol{L}\\,\\forall\\boldsymbol{h}\\leq\\boldsymbol{H}\\},\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "which is a measurable function of $(x_{1},a_{1}),\\dots,(x_{H},a_{H})$ . ", "page_idx": 37}, {"type": "text", "text": "For the second claim, we observe that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\Delta_{H^{\\star}}(o)|\\leq|\\Delta_{H^{\\star}-1}(o)|+\\left|Q_{H^{\\star}}^{\\pi^{\\star}}(x_{H^{\\star}},\\pi_{H^{\\star}}^{\\star}(x_{H^{\\star}}))-Q_{H^{\\star}}^{\\pi^{\\star}}(x_{H^{\\star}},a_{H^{\\star}})\\right|}\\\\ &{\\qquad\\qquad\\leq L+1}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "almost surely. ", "page_idx": 37}, {"type": "text", "text": "Proof of Lemma G.3. Define $X_{h}\\ :=\\ Q_{h}^{\\pi^{\\star}}(x_{h},\\pi_{h}^{\\star}(x_{h}))\\ -\\ Q_{h}^{\\pi^{\\star}}(x_{h},a_{h})$ , and $\\mathcal{F}_{h}\\quad=\\quad$ $\\sigma(x_{1},a_{1},\\ldots,x_{h},a_{h})$ , with $X_{H+1}:=0$ . Since $H^{\\star}$ is a stopping time with respect to $\\left(\\mathcal{F}_{h}\\right)$ and $X_{h}$ is a martingale difference sequence (under $\\pi^{\\star}$ ), the optional stopping theorem (e.g., [96]) implies that18 ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathbb{E}^{\\pi^{\\star}}[\\Delta_{H^{\\star}}(o)]=\\mathbb{E}^{\\pi^{\\star}}\\left[\\sum_{h=1}^{H^{\\star}}X_{h}\\right]=0.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "We now bound the second moment. Recall Doob\u2019s maximal inequality (e.g., Williams [96]). ", "page_idx": 38}, {"type": "text", "text": "Lemma G.5. $I f(S_{h})_{h\\in[H]}$ is a non-negative submartingale, then ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathbb{E}\\!\\left[\\operatorname*{max}_{h\\in[H]}S_{h}^{2}\\right]\\leq4\\mathbb{E}\\!\\left[S_{H}^{2}\\right].\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "We claim that $|\\Delta_{h}(o)|$ is a submartingale, since a convex function of a martingale is a submartingale.19 As a result, Lemma G.5 gives that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathbb{E}\\big[\\Delta_{H^{\\star}}^{2}(o)\\big]\\leq\\mathbb{E}\\bigg[\\!\\operatorname*{max}_{h\\in[H]}\\Delta_{h}^{2}(o)\\!\\bigg]\\leq4\\mathbb{E}\\big[\\Delta_{H}^{2}(o)\\big].\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Finally, we note that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}^{\\pi^{\\star}}\\left[\\Delta_{H}^{2}(o)\\right]=\\mathbb{E}^{\\pi^{\\star}}\\left[\\left(\\displaystyle\\sum_{h=1}^{H}\\left(Q_{h}^{\\pi^{\\star}}(x_{h},\\pi_{h}^{\\star}(x_{h}))-Q_{h}^{\\pi^{\\star}}(x_{h},a_{h})\\right)\\right)^{2}\\right]}\\\\ &{\\qquad\\qquad\\qquad=\\displaystyle\\sum_{h=1}^{H}\\mathbb{E}^{\\pi^{\\star}}\\left[(Q_{h}^{\\pi^{\\star}}(x_{h},\\pi_{h}^{\\star}(x_{h}))-Q_{h}^{\\pi^{\\star}}(x_{h},a_{h}))^{2}\\right]=\\sigma_{\\pi^{\\star}}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where we have once more used that $X_{h}=Q_{h}^{\\pi^{\\star}}(x_{h},\\pi_{h}^{\\star}(x_{h}))\\!-\\!Q_{h}^{\\pi^{\\star}}(x_{h},a_{h})$ is a martingale difference sequence. ", "page_idx": 38}, {"type": "text", "text": "G.1.1 Proof of Lemma G.2 (Regret Decomposition for Stopped Advantages) Proof of Lemma G.2. Consider the following non-Markovian policy: ", "text_level": 1, "page_idx": 38}, {"type": "equation", "text": "$$\n\\widetilde{\\pi}_{h}(\\cdot\\mid x_{1:h},a_{1:h-1})=\\left\\{\\begin{array}{l l}{\\widehat{\\pi}_{h}(\\cdot\\mid x_{h})}&{h\\leq H^{\\star},}\\\\ {\\pi_{h}^{\\star}(\\cdot\\mid x_{h})}&{h>H^{\\star}.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "This is a well-defined policy, since we can write $\\mathbb{I}\\{h>H^{\\star}\\}\\ =\\ \\operatorname*{max}_{h^{\\prime}<h}\\mathbb{I}\\{h^{\\prime}=H^{\\star}\\}$ , and $\\mathbb{I}\\{h^{\\prime}=H^{\\star}\\}$ is a measurable function of $(x_{1},a_{1}),\\ldots,(x_{h^{\\prime}},a_{h^{\\prime}})\\,\\subset\\,(x_{1},a_{1}),\\ldots,(x_{h-1},a_{h-1})$ ) for $h^{\\prime}<h$ . ", "page_idx": 38}, {"type": "text", "text": "We begin by writing ", "page_idx": 38}, {"type": "equation", "text": "$$\nJ(\\pi^{\\star})-J(\\widehat\\pi)=J(\\pi^{\\star})-J(\\widetilde\\pi)+J(\\widetilde\\pi)-J(\\widehat\\pi).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "For the second pair of terms in Eq. (17), we use the following lemma. ", "page_idx": 38}, {"type": "text", "text": "Lemma G.6. Under the same assumptions as Lemma $G.2$ , it holds that ", "page_idx": 38}, {"type": "equation", "text": "$$\nJ(\\widetilde{\\pi})-J(\\widehat{\\pi})\\leq R\\cdot\\mathbb{P}^{\\widehat{\\pi}}[H^{\\star}\\leq H].\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "For the first pair of terms in Eq. (17), using the performance difference lemma, we can write20 ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{J(\\pi^{*})-J(\\Tilde{\\pi})=\\mathbb{E}^{\\pi}\\Bigg[\\frac{H}{\\displaystyle M}Q_{h}^{*}\\left(x_{h},\\pi_{h}^{*}(x_{h})\\right)-Q_{h}^{**}\\left(x_{h},a_{h}\\right)\\Bigg]}\\\\ &{\\quad\\quad\\quad=\\mathbb{E}^{\\pi}\\Bigg[\\frac{\\displaystyle M}{\\displaystyle M-1}\\mathbb{E}_{h-1}\\Big[Q_{h}^{**}\\left(x_{h},\\pi_{h}^{*}(x_{h})\\right)-Q_{h}^{**}\\left(x_{h},a_{h}\\right)\\Big]\\Bigg]}\\\\ &{\\quad\\quad\\quad=\\mathbb{E}^{\\pi}\\Bigg[\\frac{H}{\\displaystyle M-1}\\mathbb{E}_{h-1}\\Big[Q_{h}^{**}\\left(x_{h},\\pi_{h}^{*}(x_{h})\\right)-Q_{h}^{**}\\left(x_{h},a_{h}\\right)\\Big]\\mathbb{I}\\{h\\leq H^{*}\\}\\Bigg]}\\\\ &{\\quad\\quad\\quad=\\mathbb{E}^{\\pi}\\Bigg[\\frac{H}{\\displaystyle M-1}\\mathbb{E}_{h-1}\\Big[\\Big(Q_{h}^{**}\\left(x_{h},\\pi_{h}^{*}(x_{h})\\right)-Q_{h}^{**}\\left(x_{h},a_{h}\\right)\\Big)\\mathbb{I}\\{h\\leq H^{*}\\}\\Bigg]\\Bigg]}\\\\ &{\\quad\\quad\\quad=\\mathbb{E}^{\\pi}\\Bigg[\\sum_{h=1}^{H^{*}}Q_{h}^{**}\\left(x_{h},\\pi_{h}^{*}(x_{h})\\right)-Q_{h}^{**}\\left(x_{h},a_{h}\\right)\\Bigg]=\\mathbb{E}^{\\pi}[\\Delta_{H}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where the third equality uses that $\\widetilde{\\pi}_{h}(\\cdot\\ |\\ x_{1:h},a_{1:h-1})=\\pi_{h}^{\\star}(\\cdot\\ |\\ x_{h})$ for $h>H^{\\star}$ , and the fourth equality uses that $\\mathbb{I}\\{h\\le H^{\\star}\\}$ is $\\mathcal{F}_{h-1}$ -measurable. We now appeal to the following lemma, proven in the sequel. ", "page_idx": 39}, {"type": "text", "text": "Lemma G.7. Under the same assumptions as Lemma $G.2$ , it holds that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\mathbb{E}^{\\widetilde{\\pi}}[\\Delta_{H^{\\star}}(o)]=\\mathbb{E}^{\\widehat{\\pi}}[\\Delta_{H^{\\star}}(o)].\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Altogether, we conclude that ", "page_idx": 39}, {"type": "equation", "text": "$$\nJ(\\pi^{\\star})-J(\\widehat\\pi)\\leq\\mathbb{E}^{\\widehat\\pi}[\\Delta_{H^{\\star}}(o)]+R\\cdot\\mathbb{P}^{\\widehat\\pi}[H^{\\star}\\leq H].\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Proof of Lemma G.6. Let us define $\\begin{array}{r}{f(o)=\\sum_{h=1}^{H}\\mathbb{E}[r_{h}\\mid x_{h},a_{h}]}\\end{array}$ and $g(o)=\\mathbb{I}\\{H^{\\star}>H\\}$ ; note that $g(o)$ is indeed a measurable function of $o\\,=\\,(x_{1},a_{1}),\\ldots,(x_{H},a_{H})$ , since $\\mathbb{I}\\{H^{\\star}>H\\}\\,=$ $1\\,-\\,\\mathbb{I}\\{H^{\\star}\\leq H\\}$ , $\\{H^{\\star}\\leq H\\}\\;=\\;\\cup_{h\\leq H}\\{H^{\\star}=h\\}$ , and $\\{H^{\\star}=h\\}$ is a measurable function of $(x_{1},a_{1}),\\ldots,(x_{h},a_{h})$ . We can write ", "page_idx": 39}, {"type": "equation", "text": "$$\nJ(\\widetilde{\\pi})\\le\\mathbb{E}^{\\widetilde{\\pi}}\\left[\\left(\\sum_{h=1}^{H}r_{h}\\right)\\mathbb{I}\\{H^{\\star}>H\\}\\right]+R\\cdot\\mathbb{P}^{\\widetilde{\\pi}}[H^{\\star}\\le H].\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Let us adopt the shorthand $\\begin{array}{r}{P(x_{1:H}\\mid a_{1:H-1}):=\\prod_{h=0}^{H-1}P_{h}(x_{h+1}\\mid x_{h},a_{h})}\\end{array}$ . We can bound the first term in Eq. (18) via ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}^{\\overline{{\\pi}}}\\left[\\left(\\displaystyle\\sum_{h=1}^{H}r_{h}\\right)!\\mathbb{I}\\{H^{*}>H\\}\\right]=\\displaystyle\\sum_{o=x_{1},H,a_{1},H}f(o)g(o)P(x_{1:H}\\mid a_{1:H-1})\\prod_{h=1}^{H}\\widetilde{\\pi}_{h}(a_{h}\\mid x_{1:h},a_{1:h-1})}\\\\ &{\\quad\\quad\\quad\\quad=\\displaystyle\\sum_{o=x_{1},H,a_{1},H}f(o)g(o)P(x_{1:H}\\mid a_{1:H-1})\\prod_{h=1}^{H}\\widehat{\\pi}_{h}(a_{h}\\mid x_{h})}\\\\ &{\\quad\\quad\\quad\\quad\\leq\\displaystyle\\sum_{o=x_{1},H,a_{1:H}}f(o)P(x_{1:H}\\mid a_{1:H-1})\\prod_{h=1}^{H}\\widehat{\\pi}_{h}(a_{h}\\mid x_{h})}\\\\ &{\\quad\\quad\\quad\\quad=\\mathbb{E}^{\\overline{{\\pi}}}\\left[\\displaystyle\\sum_{h=1}^{H}r_{h}\\right]=J(\\widehat{\\pi}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "20Since $\\widetilde{\\pi}$ is non-Markovian, we need to expand the state space to $x_{h}^{\\prime}=x_{1:h},a_{1:h-1}$ to apply the performance difference  l emma, but since $\\pi^{\\star}$ itself is Markovian, this results in the claimed expression. ", "page_idx": 39}, {"type": "text", "text": "where the second equality uses that $\\widetilde{\\pi}(\\cdot~\\mid~x_{1:h},a_{1:h-1})\\,=\\,\\widehat{\\pi}(\\cdot~\\mid~x_{h})$ for all $\\textit{h}\\in[H]$ whenever $g(o)=1$ . ", "page_idx": 40}, {"type": "text", "text": "To bound the second term in Eq. (18), we can write ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\mathbb{P}^{\\tilde{\\pi}}[H^{\\star}\\leq H]=\\sum_{h=1}^{H}\\mathbb{P}^{\\tilde{\\pi}}[H^{\\star}=h].\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "For each $h$ , let $o_{h}\\;:=\\;(x_{1},a_{1}),\\ldots,(x_{h},a_{h})$ and $g_{h}(o_{h})\\,:=\\,\\mathbb{I}\\{H^{\\star}=h\\}$ (recall that $\\mathbb{I}\\{H^{\\star}=h\\}$ is a measurable function of $(x_{1},a_{1}),\\ldots,(x_{h},a_{h}))$ . Note that for each $h$ , if we define $P(\\boldsymbol{x}_{1:h}\\mid$ $\\begin{array}{r}{a_{1:h-1}):=\\prod_{h=0}^{h-1}P_{\\ell}(x_{\\ell+1}\\mid x_{\\ell},a_{\\ell})}\\end{array}$ , then ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}^{\\widetilde\\pi}[H^{\\star}=h]=\\displaystyle\\sum_{o_{h}=x_{1:h},a_{1:h}}g_{h}(o_{h})P(x_{1:h}\\mid a_{1:h-1})\\displaystyle\\prod_{\\ell=1}^{h}\\widetilde\\pi_{\\ell}(a_{\\ell}\\mid x_{1:\\ell},a_{1:\\ell-1})}\\\\ &{\\qquad\\quad=\\displaystyle\\sum_{o_{h}=x_{1:h},a_{1:h}}g_{h}(o_{h})P(x_{1:h}\\mid a_{1:h-1})\\displaystyle\\prod_{\\ell=1}^{h}\\widehat\\pi_{\\ell}(a_{\\ell}\\mid x_{\\ell})}\\\\ &{\\qquad=\\mathbb{P}^{\\widehat\\pi}[H^{\\star}=h],}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where the second inequality uses that $\\widetilde{\\pi}(\\cdot\\mid x_{1:\\ell},a_{1:\\ell-1})=\\widehat{\\pi}(\\cdot\\mid x_{\\ell})$ whenever $\\ell\\leq H^{\\star}$ . ", "page_idx": 40}, {"type": "text", "text": "Proof of Lemma G.7. We start by writing ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\mathbb{E}^{\\widetilde{\\pi}}[\\Delta_{H^{\\star}}(o)]=\\sum_{h=1}^{H+1}\\mathbb{E}^{\\widetilde{\\pi}}[\\mathbb{I}\\{H^{\\star}=h\\}\\Delta_{h}(o)].\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "For each $h\\,\\leq\\,H\\,+\\,1$ , let $o_{h}\\;:=\\;(x_{1},a_{1}),\\ldots,(x_{h},a_{h})$ and $g_{h}(o_{h})\\::=\\:\\mathbb{I}\\{H^{\\star}=h\\}$ (recall that $\\mathbb{I}\\{H^{\\star}=h\\}$ is a measurable function of $(x_{1},a_{1}),\\ldots,(x_{h},a_{h}))$ . For each $h\\leq H+1$ , if we define $\\begin{array}{r}{\\Bar{P}(x_{1:h}\\mid a_{1:h-1}):=\\prod_{h=0}^{h-1}P_{\\ell}(x_{\\ell+1}\\mid x_{\\ell},a_{\\ell})}\\end{array}$ , then ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}^{\\widetilde\\pi}[\\mathbb{I}\\{H^{\\star}=h\\}\\Delta_{h}(o)]=\\displaystyle\\sum_{o_{h}=x_{1:h},a_{1:h}}g_{h}(o_{h})\\Delta_{h}(o_{h})P(x_{1:h}\\mid a_{1:h-1})\\displaystyle\\prod_{\\ell=1}^{h}\\widetilde\\pi_{\\ell}(a_{\\ell}\\mid x_{1:\\ell},a_{1:\\ell-1})}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\sum_{o_{h}=x_{1:h},a_{1:h}}g_{h}(o_{h})\\Delta_{h}(o_{h})P(x_{1:h}\\mid a_{1:h-1})\\displaystyle\\prod_{\\ell=1}^{h}\\widehat\\pi_{\\ell}(a_{\\ell}\\mid x_{\\ell})}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\mathbb{E}^{\\widetilde\\pi}[\\mathbb{I}\\{H^{\\star}=h\\}\\Delta_{h}(o)],}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where the second inequality uses that $\\widetilde{\\pi}(\\cdot\\mid x_{1:\\ell},a_{1:\\ell-1})=\\widehat{\\pi}(\\cdot\\mid x_{\\ell})$ whenever $\\ell\\leq H^{\\star}$ . ", "page_idx": 40}, {"type": "text", "text": "G.1.2 Proof of Lemma G.4 (Concentration for Advantages) ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Lemma G.4 is proven using arguments similar to those in Zhang et al. [104, 105], but requires non-trivial modifications to accommodate the fact that $\\pi$ is an arbitrary, potentially suboptimal policy. ", "page_idx": 40}, {"type": "text", "text": "Proof of Lemma G.4. Let us abbreviate $Q=Q^{\\pi}$ and $V=V^{\\pi}$ . Assume without loss of generality that $R\\,=\\,1$ , and note that this implies that $r_{h}\\,\\in\\,[0,1]$ and $Q_{h},V_{h}\\ \\in\\ [0,1]$ , which we will use throughout the proof. ", "page_idx": 40}, {"type": "text", "text": "Define a filtration $\\mathcal{F}_{h-1}:=\\sigma((x_{1},a_{1},r_{1}),\\dots,(x_{h-1},a_{h-1},r_{h-1}),x_{h})$ . Since ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\mathbb{E}_{h-1}[Q_{h}(x_{h},a_{h})-V_{h}(x_{h})]=0,\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "two applications of Lemma D.2 and a union bound imply that with probability at least $1-\\delta$ , for all $H^{\\prime}\\in\\bar{[}H]$ ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\left\\vert\\sum_{h=1}^{H^{\\prime}}Q_{h}(x_{h},a_{h})-V_{h}(x_{h})\\right\\vert\\leq\\sum_{h=1}^{H^{\\prime}}\\mathbb{E}^{\\pi}\\left[(Q_{h}(x_{h},a_{h})-V_{h}(x_{h}))^{2}\\mid x_{h}\\right]+\\log(2\\delta^{-1}).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Since $\\mathbb{E}^{\\pi}[Q_{h}(x_{h},a_{h})\\mid x_{h}]=V_{h}(x_{h})$ , we can write ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{h=1}^{H^{\\prime}}\\mathbb{E}^{\\pi}\\big[(Q_{h}(x_{h},a_{h})-V_{h}(x_{h}))^{2}\\mid x_{h}\\big]=\\displaystyle\\sum_{h=1}^{H^{\\prime}}\\mathbb{E}^{\\pi}\\big[(Q_{h}^{2}(x_{h},a_{h})\\mid x_{h}\\big]-V_{h}^{2}(x_{h})}\\\\ &{=\\displaystyle\\sum_{h=1}^{H^{\\prime}}\\big(\\mathbb{E}^{\\pi}\\big[(Q_{h}^{2}(x_{h},a_{h})\\mid x_{h}\\big]-V_{h+1}^{2}(x_{h+1})\\big)+V_{H^{\\prime}+1}^{2}(x_{H^{\\prime}})\\big]}\\\\ &{\\le\\displaystyle\\sum_{h=1}^{H^{\\prime}}(\\mathbb{E}^{\\pi}\\big[(Q_{h}^{2}(x_{h},a_{h})\\mid x_{h}\\big]-V_{h+1}^{2}(x_{h+1})\\big)+1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Observe that by Jensen\u2019s inequality, we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}^{\\pi}\\big[(Q_{h}^{2}(x_{h},a_{h})\\mid x_{h}\\big]\\le\\mathbb{E}^{\\pi}\\big[(r_{h}+V_{h+1}(x_{h+1}))^{2}\\mid x_{h}\\big]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\mathbb{E}^{\\pi}\\big[V_{h+1}^{2}(x_{h+1})\\mid x_{h}\\big]+\\mathbb{E}^{\\pi}\\big[r_{h}^{2}\\mid x_{h}\\big]+2\\,\\mathbb{E}^{\\pi}\\big[r_{h}V_{h+1}(x_{h+1})\\mid x_{h}\\big]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\le\\,\\mathbb{E}^{\\pi}\\big[V_{h+1}^{2}(x_{h+1})\\mid x_{h}\\big]+3\\,\\mathbb{E}^{\\pi}\\big[r_{h}\\mid x_{h}\\big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "so that ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathbb{E}^{\\pi}\\left[(Q_{h}(x_{h},a_{h})-V_{h}(x_{h}))^{2}\\mid x_{h}\\right]\\leq\\sum_{h=1}^{H^{\\prime}}\\mathbb{E}^{\\pi}\\left[V_{h+1}^{2}(x_{h+1})\\mid x_{h}\\right]-V_{h+1}^{2}(x_{h+1})+3\\sum_{h=1}^{H^{\\prime}}\\mathbb{E}^{\\pi}[r_{h}\\mid x_{h}]\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "By Lemma D.3, we have that with probability at least $1-\\delta$ , for all $H^{\\prime}\\in[H]$ , ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{h=1}^{H^{\\prime}}\\mathbb{E}^{\\pi}[r_{h}\\mid x_{h}]\\le\\frac{3}{2}\\sum_{h=1}^{H^{\\prime}}r_{h}+4\\log(2\\delta^{-1})}}\\\\ &{}&{\\le\\frac{3}{2}+4\\log(2\\delta^{-1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Likewise, by Lemma D.2, we have that with probability at least $1-\\delta$ , for all $H^{\\prime}\\in[H]$ , ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{=1}^{H^{\\prime}}\\mathbb{E}^{\\pi}\\big[V_{h+1}^{2}(x_{h+1})\\mid x_{h}\\big]-V_{h+1}^{2}(x_{h+1})\\le\\displaystyle\\sum_{h=1}^{H^{\\prime}}\\mathbb{E}^{\\pi}\\Big[\\big(V_{h+1}^{2}(x_{h+1})-\\mathbb{E}^{\\pi}\\big[V_{h+1}^{2}(x_{h+1})\\mid x_{h}\\big]\\big)^{2}\\mid x_{h}\\Big]}\\\\ &{\\displaystyle=\\sum_{h=1}^{H^{\\prime}}\\mathrm{Var}^{\\pi}\\big[V_{h+1}^{2}(x_{h+1})\\mid x_{h}\\big]+\\log(\\delta^{-1})}\\\\ &{\\displaystyle\\le4\\displaystyle\\sum_{h=1}^{H^{\\prime}}\\mathrm{Var}^{\\pi}\\big[V_{h+1}(x_{h+1})\\mid x_{h}\\big]+\\log(\\delta^{-1}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where the last line uses the following lemma, proven in the sequel. ", "page_idx": 41}, {"type": "text", "text": "Lemma G.8. If $X$ is a random variable with $|X|\\leq1$ , then ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\operatorname{Var}(X^{2})\\leq4\\mathrm{Var}(X).\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "We now appeal to the following lemma, also proven in the sequel. ", "page_idx": 41}, {"type": "text", "text": "Lemma G.9. Under the same setting as Lemma G.4, we have that for any $\\delta\\in(0,1)$ , with probability at least $1-2\\delta$ , for all $H^{\\prime}\\in[H]$ , ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\sum_{h=1}^{H^{\\prime}}{\\mathrm{Var}}^{\\pi}\\left[V_{h+1}^{\\pi}(x_{h+1})\\mid x_{h}\\right]\\leq8+32\\log(2\\delta^{-1}).\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Putting together all of the developments so far, we have that with probability at least $1-5\\delta$ , for all $H^{\\prime}\\in\\bar{[}H]$ , ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\left\\vert\\sum_{h=1}^{H^{\\prime}}Q_{h}(x_{h},a_{h})-V_{h}(x_{h})\\right\\vert\\leq4\\displaystyle\\sum_{h=1}^{H^{\\prime}}\\mathrm{Var}^{\\pi}[V_{h+1}(x_{h+1})\\mid x_{h}]+6+14\\log(2\\delta^{-1})}&{}\\\\ {\\quad\\leq38+142\\log(2\\delta^{-1}).}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Proof of Lemma G.8. Note that we have ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\operatorname{Var}(X^{2})=\\mathbb{E}\\big[(X^{2}-\\mathbb{E}\\big[X^{2}\\big])^{2}\\big]\\leq\\mathbb{E}\\Big[(X^{2}-\\mathbb{E}[X]^{2})^{2}\\Big]\\leq4\\mathbb{E}\\big[(X-\\mathbb{E}[X])^{2}\\big],\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where the last line uses that $\\left|a^{2}-b^{2}\\right|\\leq2|a-b|$ for $a,b\\in[-1,1]$ . ", "page_idx": 42}, {"type": "text", "text": "Proof of Lemma G.9. Abbreviate $V\\equiv V^{\\pi}$ . By telescoping, we can write ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\zeta_{H^{\\prime}}:=\\displaystyle\\sum_{h=1}^{H^{\\prime}}\\mathrm{Var}^{\\pi}[V_{h+1}(x_{h+1})\\mid x_{h}]}&{}\\\\ &{=\\displaystyle\\sum_{h=1}^{H^{\\prime}}\\mathbb{E}^{\\pi}\\big[V_{h+1}^{2}(x_{h+1})\\mid x_{h}\\big]-\\big(\\mathbb{E}^{\\pi}[V_{h+1}(x_{h+1})\\mid x_{h}]\\big)^{2}}\\\\ &{=\\displaystyle\\sum_{h=1}^{H^{\\prime}}\\mathbb{E}^{\\pi}\\big[V_{h+1}^{2}(x_{h+1})\\mid x_{h}\\big]-V_{h+1}^{2}(x_{h+1})+\\displaystyle\\sum_{h=1}^{H^{\\prime}}V_{h}^{2}(x_{h})-\\big(\\mathbb{E}^{\\pi}[V_{h+1}(x_{h+1})\\mid x_{h}]\\big)^{2}+V_{h}^{\\prime}}\\\\ &{\\le\\displaystyle\\sum_{h=1}^{H^{\\prime}}\\mathbb{E}^{\\pi}\\big[V_{h+1}^{2}(x_{h+1})\\mid x_{h}\\big]-V_{h+1}^{2}(x_{h+1})+\\displaystyle\\sum_{h=1}^{H^{\\prime}}V_{h}^{2}(x_{h})-\\big(\\mathbb{E}^{\\pi}[V_{h+1}(x_{h+1})\\mid x_{h}]\\big)^{2}+1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "For the latter term, since $\\left|a^{2}-b^{2}\\right|\\leq2|a-b|$ for $a,b\\in[0,1]$ , we have that ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{h=1}^{H^{\\prime}}V_{h}^{2}(x_{h})-\\left(\\mathbb{E}^{\\pi}[V_{h+1}(x_{h+1})\\mid x_{h}]\\right)^{2}\\leq2\\sum_{h=1}^{H^{\\prime}}\\lvert V_{h}(x_{h})-\\mathbb{E}^{\\pi}[V_{h+1}(x_{h+1})\\mid x_{h}]\\rvert}}\\\\ &{}&{=2\\sum_{h=1}^{H^{\\prime}}\\lvert\\mathbb{E}^{\\pi}[r_{h}\\mid x_{h}]\\rvert\\leq2\\sum_{h=1}^{H^{\\prime}}\\mathbb{E}^{\\pi}[r_{h}\\mid x_{h}],}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "By Lemma D.3, we have that with probability at least $1-\\delta$ , for all $H^{\\prime}\\in[H]$ , ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\sum_{h=1}^{H^{\\prime}}\\mathbb{E}^{\\pi}[r_{h}\\mid x_{h}]\\leq{\\frac{3}{2}}\\sum_{h=1}^{H^{\\prime}}r_{h}+4\\log(2\\delta^{-1})\\leq{\\frac{3}{2}}+4\\log(2\\delta^{-1}).\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "For the first term, by Lemma D.2, we have that for all $\\eta\\in(0,1)$ , with probability at least $1-\\delta$ , for all $H^{\\prime}\\in[H]$ , ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{=1}^{H^{\\prime}}\\mathbb{E}^{\\pi}\\left[V_{h+1}^{2}(x_{h+1})\\mid x_{h}\\right]-V_{h+1}(x_{h+1})\\le\\eta\\displaystyle\\sum_{h=1}^{H^{\\prime}}\\mathbb{E}^{\\pi}\\Big[\\big(V_{h+1}^{2}(x_{h+1})-\\mathbb{E}^{\\pi}\\big[V_{h+1}^{2}(x_{h+1})\\mid x_{h}\\big]\\big)^{2}\\mid x_{h}\\Big]}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad=\\eta\\displaystyle\\sum_{h=1}^{H^{\\prime}}\\mathrm{Var}^{\\pi}\\big[V_{h+1}^{2}(x_{h+1})\\mid x_{h}\\big]+\\eta^{-1}\\log(\\delta^{-1})}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\le4\\eta\\displaystyle\\sum_{h=1}^{H^{\\prime}}\\mathrm{Var}^{\\pi}\\big[V_{h+1}(x_{h+1})\\mid x_{h}\\big]+\\eta^{-1}\\log(\\delta^{-1})}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad=4\\eta Z_{H^{\\prime}}+\\eta^{-1}\\log(\\delta^{-1}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where the last inequality uses Lemma G.8. Putting everything together and setting $\\eta=1/8$ , we conclude that with probability at least $1-2\\delta$ , for all $H^{\\prime}\\in[H]$ ", "page_idx": 42}, {"type": "equation", "text": "$$\nZ_{H^{\\prime}}\\leq\\frac{1}{2}Z_{H^{\\prime}}+16\\log(2\\delta^{-1})+4,\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "which yields the result after rearranging. ", "page_idx": 42}, {"type": "text", "text": "G.2 Formal Statement and Proof of Theorem G.1 ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "The following result shows that the dependence on the variance in Corollary 2.1 cannot be improved in general, which implies that the horizon-dependence in this regime is tight. ", "page_idx": 43}, {"type": "text", "text": "Theorem G.1 (Lower bound for stochastic experts). Consider the dense reward setting where $r_{h}\\,\\in\\,[0,1]$ and $R\\,=\\,H$ . For any $n\\,\\in\\,\\mathbb{N}$ , $H\\,\\in\\,\\mathbb{N}$ and $\\sigma^{2}\\,\\in\\,[H,H^{2}]$ , there exists a reward-free MDP $M^{\\star}$ with $|\\mathcal{X}|=3$ and $|{\\mathcal{A}}|=2$ , a class of reward functions $\\mathcal{R}$ with $|{\\mathcal{R}}|=2$ , and a class of policies $\\Pi$ with $|\\Pi|=2$ with the following property. For any (online or offline) imitation learning algorithm, there exists a deterministic reward function $r=\\{r_{h}\\}_{h=1}^{H}$ and expert policy $\\pi^{\\star}\\in\\Pi$ such that $\\sigma_{\\pi^{\\star}}^{2}\\leq\\sigma^{2}$ and $\\widetilde{\\mu}\\leq\\sigma^{2}/H$ , and for which ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\mathbb{P}\\!\\left(J(\\pi^{\\star})-J({\\widehat{\\pi}})\\geq c\\cdot{\\sqrt{\\frac{\\sigma^{2}}{n}}}\\right)\\geq{\\frac{1}{8}}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "for an absolute constant $c\\geq1$ . ", "page_idx": 43}, {"type": "text", "text": "Beyond showing that a slow $1/\\sqrt{n}$ rate is required for stochastic policies,21 ", "page_idx": 43}, {"type": "text", "text": "Proof of Theorem G.1. For this proof, we consider a slightly more general online imitation learning model in which the learner is allowed to select $a_{h}^{i}$ based on the sequence $(x_{1}^{i},a_{1}^{i},a_{1}^{\\star,i}),\\ldots,(x_{h-1}^{i},a_{h-1}^{i},a_{h-1}^{\\star,i}),(x_{h}^{i},a_{h}^{\\star,i})$ at training time; this subsumes the offline imitation learning model. Let $H\\in\\mathbb{N}$ , $n\\in\\mathbb N$ , and $\\sigma^{2}\\in[H,H^{2}]$ be given. Fix a parameter $K\\in\\mathbb N$ such that $H/K$ is an integer and a parameter $\\Delta\\in(0,1/2)$ be fixed; both parameters will be chosen at the end of the proof. ", "page_idx": 43}, {"type": "text", "text": "We first specify the dynamics for the reward-free MDP $M^{\\star}$ and the policy class $\\Pi$ . Let $\\mathcal{A}=\\{{\\mathfrak{a}},{\\mathfrak{b}}\\}$ , and let $\\bar{\\mathcal{X}}\\,=\\,\\bigl\\{\\mathfrak{s},\\mathfrak{a},\\mathfrak{h}\\bigr\\}$ . We consider the following (deterministic) dynamics. For $h\\ \\in\\ \\mathcal{H}\\ :=$ $[1,K+1,2K+1,.~.~.],$ , always the state is always $x_{h}=\\mathfrak{s}$ . For such a step $h\\in\\mathcal H$ , choosing $a_{h}=\\mathfrak{a}$ sets $x_{h}=\\mathfrak{a}$ for the next $K-1$ steps until returning to $\\mathfrak{s}$ at time $h+K$ , and choosing $a_{h}=\\mathfrak{h}$ sets $x_{h}=\\mathfrak{h}$ until returning to $\\mathfrak{s}$ at time $h+K$ (that is, the action has no effect for $h\\not\\in\\mathcal{H},$ . ", "page_idx": 43}, {"type": "text", "text": "We consider a class $\\Pi=\\{\\pi^{a},\\pi^{\\flat}\\}$ consisting of two experts $\\pi^{\\mathrm{a}}$ and $\\pi^{\\mathrm{{b}}}$ . $\\pi^{\\mathfrak{a}}$ sets $\\pi_{h}^{\\mathfrak{a}}(\\mathfrak{a}\\mid\\mathfrak{s})=\\frac{1}{2}+\\Delta$ for $h\\in\\mathcal H$ and sets $\\pi_{h}(x)=\\mathfrak{a}$ for all $h\\not\\in\\mathcal{H}$ and $x\\in\\mathscr{X}$ . Meanwhile, $\\pi^{\\mathrm{{b}}}$ sets $\\textstyle\\pi^{\\flat}({\\mathfrak{h}}\\mid{\\mathfrak{s}})={\\frac{1}{2}}+\\Delta$ for $h\\in\\mathcal H$ and sets $\\pi_{h}(x)=\\mathfrak{a}$ for all $h\\not\\in\\mathcal{H}$ and $x\\in\\mathscr{X}$ . ", "page_idx": 43}, {"type": "text", "text": "We consider two choices of reward function, $r^{\\alpha}$ and $r^{\\mathrm{p}}$ . $r^{\\mathrm{{a}}}$ sets $r_{h}^{\\mathfrak{a}}(\\mathfrak{s},\\mathfrak{a})=1$ and $r_{h}^{\\mathfrak{a}}(\\mathfrak{s},\\mathfrak{h})=0$ for $h\\in\\mathcal H$ , and sets $r_{h}^{\\mathfrak{a}}(\\mathfrak{a},\\cdot)\\,=\\,1$ and $r_{h}^{\\mathfrak{a}}(\\mathfrak{h},\\cdot)\\,=\\,0$ for $h\\not\\in\\mathcal{H}$ . Meanwhile, $r^{\\mathrm{p}}$ sets $r_{h}^{\\flat}(\\mathfrak{s},\\mathfrak{h})\\,=\\,1$ and $r_{h}^{\\flat}(\\mathfrak{s},\\mathfrak{a})=0$ for $h\\in\\mathcal H$ and sets $r_{h}^{\\flat}({\\mathfrak{a}},\\cdot)=0$ and $r_{h}^{\\flat}({\\mathfrak{h}},\\cdot)=1$ for $h\\not\\in\\mathcal{H}$ . ", "page_idx": 43}, {"type": "text", "text": "Let a problem instance $\\mathcal{T}=(M^{\\star},r,\\pi^{\\star})$ refer to a tuple consisting of the reward-free MDP $M^{\\star}$ , a reward function $r\\,=\\,\\{r_{h}\\}_{h=1}^{H}$ , and an expert policy $\\pi^{\\star}$ . We consider four problem instances altogether: $(M^{\\star},r^{a},\\pi^{a})$ , $(M^{\\star},r^{\\flat},\\pi^{\\ast})$ , $(M^{\\star},r^{\\mathrm{a}},\\pi^{\\flat})$ , and $(M^{\\star},r^{\\flat},\\pi^{\\flat})$ . ", "page_idx": 43}, {"type": "text", "text": "Let $\\mathbb{P}^{\\mathrm{a}}$ denote the law of $O^{1},\\ldots,O^{n}$ when a when we execute the algorithm on the underlying instance, and likewise for $\\mathfrak{h}$ (recall that the law does not depend on the choice of reward function, since this is not observed); let $\\mathbb{E}^{\\mathrm{a}}[\\cdot]$ and $\\mathbb{E}^{\\mathfrak{b}}[\\cdot]$ denote the corresponding expectations. In addition, for any policy $\\pi$ , let $\\mathbb{P}^{\\pi^{\\mathrm{a}}|\\pi}$ denote the law of $o=(x_{1},a_{1},a_{1}^{\\star}),\\ldots,(x_{H},a_{H},a_{H}^{\\star})$ when we execute $\\pi$ in the online imitation learning framework and the expert policy is $\\pi^{\\star}=\\pi^{\\mathfrak{a}}$ , and define $\\mathbb{P}^{\\pi^{\\flat}|\\pi}$ analogously. ", "page_idx": 43}, {"type": "text", "text": "We begin by lower bounding the regret. Consider a fixed policy $\\widehat{\\pi}=\\{\\widehat{\\pi}_{h}:\\mathcal{X}\\rightarrow\\Delta(\\mathcal{X})\\}$ , and let $\\begin{array}{r}{\\overline{{\\pi}}(a):=\\frac{1}{|\\mathcal{H}|}\\sum_{h\\in\\mathcal{H}}\\widehat{\\pi}_{h}(a\\mid\\mathfrak{s})}\\end{array}$ . Observe that for instance $(M^{\\star},r^{\\mathrm{a}},\\pi^{\\mathrm{a}})$ , we  have ", "page_idx": 43}, {"type": "equation", "text": "$$\nJ_{r^{3}}(\\pi^{\\mathrm{a}})-J_{r^{3}}(\\widehat{\\pi})=\\bigg(\\frac{1}{2}+\\Delta\\bigg)H-K\\sum_{h\\in\\mathcal{H}}\\widehat{\\pi}_{h}(\\mathfrak{a}\\mid\\mathfrak{s})=\\bigg(\\frac{1}{2}+\\Delta\\bigg)H-H\\overline{{\\pi}}(a)\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "and for instance $(M^{\\star},r^{\\flat},\\pi^{\\ast})$ , ", "page_idx": 44}, {"type": "equation", "text": "$$\nJ_{r^{\\mathfrak{b}}}(\\pi^{\\mathfrak{a}})-J_{r^{\\mathfrak{b}}}(\\widehat{\\pi})=\\bigg(\\frac{1}{2}-\\Delta\\bigg)H-K\\sum_{h\\in\\mathcal{H}}\\widehat{\\pi}_{h}(\\mathfrak{b}\\mid\\mathfrak{s})=H\\overline{{\\pi}}(a)-\\bigg(\\frac{1}{2}+\\Delta\\bigg)H.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Likewise, for instance $(M^{\\star},r^{\\flat},\\pi^{\\flat})$ , we have ", "page_idx": 44}, {"type": "equation", "text": "$$\nJ_{r^{\\mathfrak{b}}}(\\pi^{\\mathfrak{b}})-J_{r^{\\mathfrak{b}}}(\\widehat{\\pi})=\\bigg(\\frac{1}{2}+\\Delta\\bigg)H-K\\sum_{h\\in\\mathcal{H}}\\widehat{\\pi}_{h}(\\mathfrak{b}\\mid\\mathfrak{s})=\\overline{{\\pi}}(\\mathfrak{a})H-\\bigg(\\frac{1}{2}-\\Delta\\bigg)H\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "and for instance $(M^{\\star},r^{\\mathrm{a}},\\pi^{\\flat})$ , ", "page_idx": 44}, {"type": "equation", "text": "$$\nJ_{r^{\\mathrm{a}}}(\\pi^{\\mathfrak{b}})-J_{r^{\\mathrm{a}}}(\\widehat{\\pi})=\\bigg(\\frac{1}{2}-\\Delta\\bigg)H-K\\sum_{h\\in\\mathcal{H}}\\widehat{\\pi}_{h}(\\mathfrak{a}\\mid\\mathfrak{s})=\\bigg(\\frac{1}{2}-\\Delta\\bigg)H-\\overline{{\\pi}}(\\mathfrak{a})H.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "We conclude that for any $\\varepsilon>0$ , since the law of the dataset is independent of the choice of the reward function, ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{max}\\{\\mathbb P^{*}|J_{*}(\\pi^{*}(\\pi))-J_{*}(\\pi^{*})\\geq{t}H_{1}^{1},\\mathbb P^{*}|J_{*}(\\pi^{*})-J_{*}(\\pi)\\geq{\\varepsilon}H_{1}^{1}\\}\\mathbb P^{*}|J_{*}(\\pi^{*})-J_{*}(\\pi)\\geq{\\varepsilon}H_{1}^{1}\\},\\mathbb P^{*}|J_{*}(\\pi^{*})}\\\\ &{\\geq\\operatorname*{max}\\left\\{\\mathbb P^{*}\\Big[\\Big(\\frac12+\\Delta\\Big)H-\\pi(a)H\\geq\\varepsilon H\\Big],\\mathbb P^{*}\\Big[\\pi(a)H-\\Big(\\frac12+\\Delta\\Big)H\\geq\\varepsilon H\\Big],\\right\\}}\\\\ &{\\geq\\operatorname*{max}\\left\\{\\mathbb P^{*}\\Big[\\pi(a)H-\\Big(\\frac12-\\Delta\\Big)H\\geq\\varepsilon H\\Big],\\mathbb P^{*}\\Big[\\Big(\\frac12-\\Delta\\Big)H-\\pi(a)H\\geq\\varepsilon H\\Big]\\right\\}}\\\\ &{\\geq\\frac{1}{2}\\operatorname*{max}\\left\\{\\mathbb P^{*}\\Big[\\Big(\\Big|\\Big(\\frac12+\\Delta\\Big)-\\pi(a)\\Big|H\\geq\\varepsilon H\\Big),\\mathbb P^{*}\\Big[\\Big|\\pi(a)-\\Big(\\frac12-\\Delta\\Big)\\Big|H\\geq\\varepsilon H\\Big]\\right\\}}\\\\ &{=\\frac{1}{2}\\operatorname*{max}\\{\\mathbb P^{*}\\Big[\\Big(\\Big|\\Big(\\frac12+\\Delta\\Big)-\\pi(a)\\Big|\\geq\\varepsilon\\Big),\\mathbb P^{*}\\Big[\\Big|\\pi(a)-\\Big(\\frac12-\\Delta\\Big)\\Big|\\geq\\varepsilon\\Big]\\}}\\\\ &{\\geq\\frac{1}{4}\\Big(\\mathbb P^{*}\\Big[\\Big(\\frac12+\\Delta\\Big)-\\pi(a)\\Big|\\geq\\varepsilon\\Big]+\\mathbb P^{*}\\Big[\\Big|\\pi(a)-\\Big(\\frac12-\\Delta\\Big)\\Big|\\geq\\varepsilon\\Big]\\Big\\}}\\\\ &{\\geq\\frac{1}{4}\\Big(1-\\mathbb P^{*}\\Big[\\Big[\\Big(\\frac12+\\Delta\\Big)-\\pi(a)\\Big|\\leq\\varepsilon\\Big]+\\mathbb P^{*}\\Big[\\Big|\\pi(a)-\\Big(\\frac12-\\Delta\\Big)\\Big|\\geq\\varepsilon\\Big]\\Big)}\\\\ &{\\geq\\frac{1}{4}\\Big(1-\\mathbb P^{*}\\Big[\\Big(\\frac12-\\Delta\\Big)-\\pi(a)\\Big|\\geq\\varepsilon\\Big]+\\mathbb P^{*}\\Big[\\Big|\\pi(a\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "where the second inequality uses the union bound (i.e. $\\mathbb{P}[|x|\\geq\\varepsilon]=\\mathbb{P}[x\\geq\\varepsilon\\cup-x\\geq\\varepsilon]\\leq\\mathbb{P}[x\\geq$ $\\varepsilon\\!\\!\\right]+\\mathbb{P}[-x\\geq\\varepsilon],$ ), and the second-to-last inequality holds as long as $\\varepsilon<\\Delta$ . In particular, this implies that ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\left\\{\\mathbb{P}^{\\scriptscriptstyle3}\\bigg[J_{r^{3}}(\\pi^{\\scriptscriptstyle3})-J_{r^{3}}(\\widehat{\\pi})\\ge\\frac{\\Delta H}{2}\\bigg],\\mathbb{P}^{\\scriptscriptstyle3}\\bigg[J_{r^{5}}(\\pi^{\\scriptscriptstyle3})-J_{r^{8}}(\\widehat{\\pi})\\ge\\frac{\\Delta H}{2}\\bigg],\\right\\}\\ge\\frac{1}{4}(1-D_{\\mathsf{T V}}(\\mathbb{P}^{\\scriptscriptstyle3},\\mathbb{P}^{\\scriptscriptstyle3})).\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Next, using Lemma D.2 of Foster et al. [36], we can bound ", "page_idx": 44}, {"type": "equation", "text": "$$\nD_{\\mathsf{T V}}^{2}(\\mathbb{P}^{\\mathrm{a}},\\mathbb{P}^{\\mathrm{b}})\\leq D_{\\mathsf{H}}^{2}(\\mathbb{P}^{\\mathrm{a}},\\mathbb{P}^{\\mathrm{b}})\\leq7\\,\\mathbb{E}^{\\mathrm{a}}\\left[\\sum_{i=1}^{n}D_{\\mathsf{H}}^{2}\\Big(\\mathbb{P}^{\\pi^{\\mathrm{a}}|\\pi^{i}},\\mathbb{P}^{\\pi^{\\mathrm{b}}|\\pi^{i}}\\Big)\\right].\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Observe that for a given episode $i$ , regardless of how the policy $\\pi^{i}$ is selected: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The feedback for steps $h\\not\\in\\mathcal{H}$ is identical under $\\mathbb{P}^{\\alpha}$ and $\\mathbb{P}^{\\mathrm{b}}$ . ", "page_idx": 44}, {"type": "text", "text": "\u2022 The feedback at step $h\\in\\mathcal H$ differs only in the distribution of $a_{h}^{\\star}\\sim\\pi^{\\mathfrak{a}}(\\mathfrak{s})$ versus $a_{h}^{\\star}\\sim\\pi^{\\mathfrak{h}}(\\mathfrak{s})$ . This is equivalently to $\\mathrm{Ber}(^{1}/2+\\Delta)$ feedback versus $\\mathrm{Ber}(^{1}/2-\\Delta)$ feedback. ", "page_idx": 44}, {"type": "text", "text": "As a result, using Lemma D.2 of Foster et al. [36] once more, we have ", "page_idx": 45}, {"type": "equation", "text": "$$\nD_{\\mathsf{H}}^{2}\\Big(\\mathbb{P}^{\\pi^{\\mathrm{a}}|\\pi^{i}},\\mathbb{P}^{\\pi^{\\mathrm{b}}|\\pi^{i}}\\Big)\\le7\\sum_{h\\in\\mathcal{H}}D_{\\mathsf{H}}^{2}(\\mathrm{Ber}(1/2+\\Delta),\\mathrm{Ber}(1/2-\\Delta))\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Since $\\Delta\\in(0,1/2)$ , we have $D_{\\mathsf{H}}^{2}(\\mathrm{Ber}(1/2+\\Delta),\\mathrm{Ber}(1/2-\\Delta))\\,\\le\\,O(\\Delta^{2})$ (e.g., Foster et al. [34, Lemma A.7]). We conclude that ", "page_idx": 45}, {"type": "equation", "text": "$$\nD_{\\mathsf{T V}}^{2}(\\mathbb{P}^{\\mathrm{a}},\\mathbb{P}^{\\mathrm{b}})\\leq O\\big(n\\cdot|\\mathcal{H}|\\cdot\\Delta^{2}\\big)=O\\bigg(n\\cdot\\frac{H}{K}\\cdot\\Delta^{2}\\bigg)\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "We set \u22062 = c \u00b7HKn for $c>0$ sufficiently small so that $D_{\\mathsf{T V}}^{2}(\\mathbb{P}^{\\mathrm{a}},\\mathbb{P}^{\\mathrm{b}})\\le1/2$ , and conclude that on at least one of the four problem instances, the algorithm must have ", "page_idx": 45}, {"type": "equation", "text": "$$\nJ(\\pi^{\\star})-J(\\widehat{\\pi})\\geq\\Omega(\\Delta H)=\\Omega\\bigg(\\sqrt{\\frac{H K}{n}}\\bigg)\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "with probability at least $1/8$ . ", "page_idx": 45}, {"type": "text", "text": "Finally, we compute the variance and choose the parameter $K$ . Observe that for all of the choices of expert policy and reward function described above, we have $Q_{h}^{\\pi^{\\star}}(x_{h},\\pi^{\\star}(x_{h}))-Q_{h}^{\\pi^{\\star}}(x_{h},a)=0$ for $h\\not\\in\\mathcal{H}$ , while ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|Q_{h}^{\\pi^{\\star}}(x_{h},\\pi^{\\star}(x_{h}))-Q_{h}^{\\pi^{\\star}}(x_{h},a)\\right|\\le K}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "for $h\\in\\mathcal H$ , so we can take $\\widetilde{\\mu}\\leq K$ . Consequently, we have ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\mathsf{r}_{\\pi^{\\star}}^{2}=\\sum_{h=1}^{H}\\mathbb{E}^{\\pi^{\\star}}\\left[(Q_{h}^{\\pi^{\\star}}(x_{h},\\pi^{\\star}(x_{h}))-Q_{h}^{\\pi^{\\star}}(x_{h},a_{h}))^{2}\\right]\\le\\displaystyle\\sum_{h\\in\\mathcal{H}}\\mathbb{E}^{\\pi^{\\star}}\\left[(Q_{h}^{\\pi^{\\star}}(\\mathfrak{s},\\pi^{\\star}(\\mathfrak{s}))-Q_{h}^{\\pi^{\\star}}(\\mathfrak{s},a_{h}))^{2}\\right]}\\\\ {\\displaystyle\\le\\frac{H}{K}\\cdot K^{2}=H K.}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "We conclude by setting $K=\\sigma^{2}/H$ , which is admissible for $\\sigma^{2}\\in\\left[H,H^{2}\\right]$ (up to a loss in absolute constants, we can assume that $\\sigma^{2}/H$ is an integer without loss of generality). \u53e3 ", "page_idx": 45}, {"type": "text", "text": "G.3 Additional Proofs ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Proof of Proposition 3.1. We have ", "text_level": 1, "page_idx": 45}, {"type": "equation", "text": "$$\n\\sigma_{\\pi^{\\star}}^{2}=\\sum_{h=1}^{H}\\mathbb{E}^{\\pi^{\\star}}\\left[(Q_{h}^{\\pi^{\\star}}(x_{h},a_{h})-V_{h}^{\\pi^{\\star}}(x_{h}))^{2}\\right].\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Note that $Q_{h}^{\\pi^{\\star}}(x_{h},a_{h})=\\mathbb{E}\\big[r_{h}+V_{h}^{\\pi^{\\star}}(x_{h+1})\\mid x_{h},a_{h}\\big]$ . Hence, by Jensen\u2019s inequality we can bound ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb E^{\\pi^{\\star}}\\left[(Q_{h}^{\\pi^{\\star}}(x_{h},a_{h})-V_{h}^{\\pi^{\\star}}(x_{h}))^{2}\\right]\\le\\mathbb E^{\\pi^{\\star}}\\left[\\mathbb E\\Big[(r_{h}+V_{h+1}^{\\pi^{\\star}}(x_{h+1})-V_{h}^{\\pi^{\\star}}(x_{h}))^{2}\\mid x_{h},a_{h}\\Big]\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\mathbb E^{\\pi^{\\star}}\\left[\\mathbb E^{\\pi^{\\star}}\\left[(r_{h}+V_{h+1}^{\\pi^{\\star}}(x_{h+1})-V_{h}^{\\pi^{\\star}}(x_{h}))^{2}\\mid x_{h}\\right]\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\mathbb E^{\\pi^{\\star}}\\left[\\mathrm{Var}^{\\pi^{\\star}}\\left[r_{h}+V_{h+1}^{\\pi^{\\star}}(x_{h+1})\\mid x_{h}\\right]\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "so that ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sigma_{\\pi^{\\star}}^{2}\\le\\,\\mathbb{E}^{\\pi^{\\star}}\\left[\\sum_{h=1}^{H}\\mathrm{Var}^{\\pi^{\\star}}\\left[r_{h}+V_{h+1}^{\\pi^{\\star}}(x_{h+1})\\mid x_{h}\\right]\\right]}\\\\ {\\le\\,\\mathbb{E}^{\\pi^{\\star}}\\left[\\sum_{h=0}^{H}\\mathrm{Var}^{\\pi^{\\star}}\\left[r_{h}+V_{h+1}^{\\pi^{\\star}}(x_{h+1})\\mid x_{h}\\right]\\right]=\\mathrm{Var}^{\\pi^{\\star}}\\left[\\sum_{h=1}^{H}r_{h}\\right]\\le R^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "where the second to last inequality follows from Lemma D.5. ", "page_idx": 45}, {"type": "text", "text": "Part II Additional Results ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "H Additional Lower Bounds ", "page_idx": 46}, {"type": "text", "text": "This section contains additional lower bounds that complement the results in Sections 2 and 3: ", "page_idx": 46}, {"type": "text", "text": "\u2022 Appendix H.1 shows that the conclusion of Theorem H.1 continues to hold even for online imitation learning in an active sample complexity framework.   \n\u2022 Appendix H.2 presents an instance-dependent lower bound for stochastic experts, complementing the minimax lower bound in Theorem G.1.   \n\u2022 Appendix H.3 investigates the extent to which Theorems 2.1 and 3.1 are tight on a per-policy basis. ", "page_idx": 46}, {"type": "text", "text": "H.1 Lower Bounds for Online Imitation Learning in Active Interaction Model ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "For the online imitation learning setting introduced in Section 1.1, we measure sample complexity in terms of the total number of episodes of online interaction, and expert feedback is available in every episode. In this section, we consider a more permissive sample complexity framework inspired by active learning [40, 75]. Here, as in Section 1.1, the learner interacts with the underlying MDP $M^{\\star}$ through multiple episodes. At each episode $i\\in[n]$ the learner executes a policy $\\pi^{i}=$ $\\left\\{\\pi_{h}^{i}:\\mathcal{X}\\to\\Delta(\\mathcal{A})\\right\\}_{h=1}^{H}$ da ta tt haen yc sutrerpe $h$ isnt atthe .i soWdee ,s etth e icfi dthe e wlehaetrhneerr  tqo uqeurieersy  tthhee  eexxppeerrtt $a_{h}^{\\star}\\sim\\stackrel{\\cdot\\cdot}{\\pi}_{h}^{\\star}(x_{h})$ $x_{h}$ $M^{i}=1$   \nat any point during episode $i$ and set $M^{i}=0$ otherwise, and define the active sample complexity $\\textstyle M:=\\sum_{i=1}^{n}M^{i}$ as the total number of queries. ", "page_idx": 46}, {"type": "text", "text": "It is clear that the active sample complexity satisfies $m\\leq n$ , and in some cases we might hope for it to be much smaller than the total number of episodes, at least for a well-designed algorithm. While this can indeed be the case for MDPs that satisfies (fairly strong) distributional assumptions [75], we will show that the lower bound in Theorem 2.2 continues to hold in this framework (up to a logarithmic factor), meaning that online interaction in the active sample complexity framework cannot improve over LogLossBC in general. ", "page_idx": 46}, {"type": "text", "text": "Theorem H.1 (Lower bound for deterministic experts in active sample complexity framework). For any $m\\in\\mathbb{N}$ and $H\\in\\mathbb{N},$ , there exists a reward-free MDP $M^{\\star}$ with $|{\\mathcal{X}}|=|A|=m+1$ , a class of reward functions $\\mathcal{R}$ with $|{\\mathcal{R}}|=m+1$ , and a class of deterministic policies $\\Pi$ with $\\log\\lvert\\Pi\\rvert=\\log(m)$ with the following property. For any online imitation learning algorithm in the active sample complexity framework that has sample complexity $\\mathbb{E}[M]\\leq c\\cdot m$ for an absolute constant $c>0$ there exists a deterministic reward function $r=\\{r_{h}\\}_{h=1}^{H}$ with $r_{h}\\in[0,1]$ and (optimal) expert policy $\\pi^{\\star}\\in\\Pi$ with $\\mu=1$ such that the expected suboptimality is lower bounded as ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\mathbb{E}[J(\\pi^{\\star})-J(\\widehat\\pi)]\\geq c\\cdot\\frac{H}{m}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "for an absolute constant $c>0$ . In addition, the dynamics, rewards, and expert policies are all stationary. ", "page_idx": 46}, {"type": "text", "text": "Since this example has $\\log\\lvert\\Pi\\rvert=\\log(M)$ , it follows that the sample complexity bound for LogLossBC in Theorem 2.1 (which uses $M=n$ ) can be improved by no more than a $\\log(n)$ factor through online interaction in the active framework. ", "page_idx": 46}, {"type": "text", "text": "Proof of Theorem H.1. Let $m\\in\\mathbb{N}$ and $H\\in\\mathbb{N}$ be fixed. We first specify the dynamics for the reward-free MDP $M^{\\star}$ . Set $\\mathcal{X}\\,=\\,\\{\\mathfrak{x}_{1},\\dots,\\mathfrak{x}_{m}\\}$ and ${\\mathcal A}=\\{{\\mathfrak{a}},{\\mathfrak{d}}\\}$ . The initial state distribution is $P_{0}=\\operatorname{unif}(\\mathfrak{x}_{1},\\dots,\\mathfrak{x}_{m})$ . The transition dynamics are $P_{h}(x^{\\prime}\\mid x,a)=\\mathbb{I}\\{x^{\\prime}=x\\}$ for all $h$ ; that is, $\\mathfrak{x}_{1},\\hdots,\\mathfrak{x}_{m}$ are all self-looping terminal states. ", "page_idx": 46}, {"type": "text", "text": "Let a problem instance $\\mathcal{T}=(M^{\\star},r,\\pi^{\\star})$ refer to a tuple consisting of the reward-free MDP $M^{\\star}$ , a reward function $r=\\{r_{h}\\}_{h=1}^{H}$ , and an expert policy $\\pi^{\\star}$ . We consider $m+1$ problem instances $\\mathcal{T}^{0},\\dots,\\mathcal{T}^{m}$ parameterized by a collection of policies $\\Pi\\;=\\;\\{\\pi^{0},\\ldots,\\pi^{m}\\}$ and reward functions $\\mathcal{R}=\\{r^{0},\\ldots,r^{m}\\}$ . ", "page_idx": 47}, {"type": "text", "text": "\u2022 For problem instance $\\mathcal{T}^{0}\\,=\\,(M^{\\star},r^{0},\\pi^{0})$ , the expert policy is $\\pi^{0}$ , which sets $\\pi_{h}^{0}(x)\\,=\\,\\mathfrak{a}$ for all $x\\in\\mathscr{X}$ and $h\\in[H]$ . The reward function $r^{0}$ sets $\\overline{{r_{h}(x,a)}}\\overset{\\cdot}{=}\\mathbb{I}\\{a={a}\\}$ for all $x\\in\\mathscr{X}$ and $h\\in[H]$ . \u2022 For each problem instance ${\\cal T}^{j}\\,=\\,(M^{\\star},r^{j},\\pi^{j})$ , the expert policy is $\\pi^{j}$ , which for all $\\textit{h}\\in[H]$ sets $\\pi_{h}^{j}(x\\bar{)}\\;=\\;{\\mathfrak{a}}$ for $x\\neq\\,\\mathfrak{x}_{j}$ and sets $\\pi^{h}(\\mathfrak{x}_{j})\\;=\\;\\mathfrak{h}$ . The reward function $r^{j}$ sets $r_{h}(x,a)\\;=\\;$ $\\mathbb{I}\\{a=\\bar{\\mathfrak{a}},x\\neq\\mathfrak{x}_{j}\\}+\\mathbb{I}\\{a=\\mathfrak{h},x=\\mathfrak{x}_{j}\\}$ for all $\\bar{h}\\in[H]$ . ", "page_idx": 47}, {"type": "text", "text": "Let $J^{j}$ denote the expected reward under instance $j$ . Note that all instances satisfy $\\mu=1$ , and that $\\pi^{j}$ is an optimal policy for each instance $j$ . ", "page_idx": 47}, {"type": "text", "text": "Going forward, we fix the online imitation learning algorithm under consideration and let $\\mathbb{P}^{j}$ denote the law of $O^{1},\\ldots,O^{n}$ when $\\mathfrak{a}$ when we execute the algorithm on instance $\\mathcal{T}^{j}$ ; let $\\mathbb{E}^{j}\\left[\\cdot\\right]$ denote the corresponding expectation. In addition, for any policy $\\pi$ , let $\\mathbb{P}^{\\pi^{j}|\\pi}$ denote the law of $o=$ $(x_{1},a_{1},a_{1}^{\\star}),\\ldots,(x_{H},a_{H},a_{H}^{\\star})$ when we execute $\\pi$ in the online imitation learning framework when the underlying instance is $\\mathcal{T}^{j}$ , with the convention that $a_{h}^{\\star}=\\perp$ if the learner does not query the expert in episode $j$ . ", "page_idx": 47}, {"type": "text", "text": "Our aim is to lower bound ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{j\\in\\{0,...,m\\}}\\mathbb{E}^{j}[J^{j}(\\pi^{j})-J^{j}(\\widehat\\pi)]\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "To this end, define $\\begin{array}{r}{\\rho_{j}(\\pi,\\pi^{\\prime})=\\sum_{h=1}^{H}\\mathbb{E}_{a_{h}\\sim\\pi_{h}(\\ast_{j}),a_{h}^{\\prime}\\sim\\pi_{h}^{\\prime}(\\ast_{j})}\\mathbb{I}\\{a_{h}\\neq a_{h}^{\\prime}\\}}\\end{array}$ and $\\begin{array}{r}{\\rho(\\pi,\\pi^{\\prime})=\\frac{1}{m}\\rho_{j}(\\pi,\\pi^{\\prime})}\\end{array}$ , and observe that ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}^{0}[J^{0}(\\pi^{0})-J^{0}(\\widehat{\\pi})]=\\mathbb{E}^{0}\\Bigg[\\displaystyle\\frac{1}{m}\\sum_{j=1}^{m}\\sum_{h=1}^{H}\\mathbb{E}_{a_{h}\\sim\\widehat{\\pi}_{h}(\\boldsymbol{\\mathfrak{x}}_{j})}[\\mathbb{I}\\{a_{h}\\neq\\pi_{h}^{0}(\\boldsymbol{\\mathfrak{x}}_{j})\\}]\\Bigg]}\\\\ &{\\qquad\\qquad\\qquad=\\mathbb{E}^{0}[\\rho(\\widehat{\\pi},\\pi^{0})]\\geq\\displaystyle\\frac{H}{2m}\\cdot\\mathbb{P}^{0}\\bigg[\\rho(\\widehat{\\pi},\\pi^{0})\\geq\\displaystyle\\frac{H}{2m}\\bigg].}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Next, note that for any $i\\in[m]$ , if $\\begin{array}{r}{\\rho(\\widehat{\\pi},\\pi^{0})<\\frac{H}{2m}}\\end{array}$ , then $\\begin{array}{r}{\\rho_{j}(\\widehat{\\pi},\\pi^{0})<\\frac{H}{2}}\\end{array}$ , which means that $\\rho_{j}(\\widehat{\\pi},\\pi^{j})\\geq$ $\\textstyle{\\frac{H}{2}}$ . It follows that ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\mathbb{E}^{j}\\big[J^{j}(\\pi^{j})-J^{j}(\\widehat{\\pi})\\big]=\\mathbb{E}^{j}\\bigg[\\frac{1}{m}\\rho_{j}(\\widehat{\\pi},\\pi^{j})\\bigg]\\geq\\frac{H}{2m}\\mathbb{P}^{j}\\bigg[\\rho(\\widehat{\\pi},\\pi^{0})<\\frac{H}{2m}\\bigg],\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "and if we define $\\overline{{\\mathbb{P}}}=\\mathbb{E}_{j\\sim\\mathrm{unif}([m])}\\,\\mathbb{P}^{j}$ , then ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\mathbb{E}_{j\\sim\\mathrm{unif}([m])}\\mathbb{E}^{j}\\big[J^{j}\\big(\\pi^{j}\\big)-J^{j}(\\widehat\\pi)\\big]\\geq\\frac{H}{2m}\\overline{{\\mathbb{P}}}\\bigg[\\rho(\\widehat\\pi,\\pi^{0})<\\frac{H}{2m}\\bigg].\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Combining these observations, we find that ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\operatorname*{max}_{i\\in\\{0,\\ldots,m\\}}\\mathbb{E}^{j}\\big[J^{j}(\\pi^{j})-J^{j}(\\widehat{\\pi})\\big]\\geq\\displaystyle\\frac{H}{4m}\\Bigg(\\mathbb{P}^{\\scriptscriptstyle0}\\bigg[\\rho(\\widehat{\\pi},\\pi^{\\scriptscriptstyle0})\\geq\\displaystyle\\frac{H}{2m}\\bigg]+\\mathbb{P}\\bigg[\\rho(\\widehat{\\pi},\\pi^{\\scriptscriptstyle0})<\\displaystyle\\frac{H}{2m}\\bigg]\\bigg)}\\\\ {\\displaystyle\\geq\\displaystyle\\frac{H}{4m}(1-D_{\\mathsf{T V}}\\big(\\mathbb{P}^{\\scriptscriptstyle0},\\overline{{\\mathbb{P}}}\\big)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "It remains to bound the total variation distance. Next, using Lemma D.2 of Foster et al. [36], we can bound ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\mathsf{\\partial}_{\\mathsf{T V}}^{2}\\big(\\mathbb{P}^{\\scriptscriptstyle0},\\mathbb{\\bar{P}}\\big)\\leq D_{\\mathsf{H}}^{2}\\big(\\mathbb{P}^{\\scriptscriptstyle0},\\mathbb{\\bar{P}}\\big)\\leq\\mathbb{E}_{j\\sim\\operatorname{unif}[m]}\\big[D_{\\mathsf{H}}^{2}\\big(\\mathbb{P}^{\\scriptscriptstyle0},\\mathbb{P}^{\\scriptscriptstyle j}\\big)\\big]\\leq7\\,\\mathbb{E}_{j\\sim\\operatorname{unif}[m]}\\,\\mathbb{E}^{0}\\Bigg[\\sum_{t=1}^{n}D_{\\mathsf{H}}^{2}\\Big(\\mathbb{P}^{\\scriptscriptstyle0}|^{\\scriptscriptstyle0}\\,^{\\scriptscriptstyle0}|\\,\\mathbb{P}^{\\scriptscriptstyle\\sigma}|^{\\scriptscriptstyle\\sigma}\\Big)\\Bigg]\\,.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Since the feedback the learner receives for a given episode $t$ is identical under instances $\\mathcal{Z}^{0}$ and $\\mathcal{T}^{j}$ is identical unless i) $x_{1}=\\mathfrak{x}_{j}$ , and ii) the learner decides to query the expert for feedback (i.e., $M^{t}=1$ ), we can bound ", "page_idx": 47}, {"type": "equation", "text": "$$\nD_{\\mathsf{H}}^{2}\\Big(\\mathbb{P}^{\\pi^{0}|\\pi^{t}},\\mathbb{P}^{\\pi^{j}|\\pi^{0}}\\Big)\\le2\\mathbb{P}^{\\pi^{0}|\\pi^{t}}[x_{1}^{t}=\\mathfrak{x}_{j},M^{t}=1]\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "and hence ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}_{j\\sim\\mathrm{unif}[m]}\\mathbb{E}^{0}\\left[\\sum_{t=1}^{n}D_{\\mathsf{H}}^{2}\\Big(\\mathbb{P}^{\\pi^{0}|\\pi^{t}},\\mathbb{P}^{\\pi^{t}|\\pi^{t}}\\Big)\\right]\\le2\\,\\mathbb{E}_{j\\sim\\mathrm{unif}[m]}\\,\\mathbb{E}^{0}\\left[\\sum_{t=1}^{n}\\mathbb{P}^{\\pi^{0}|\\pi^{t}}[x_{1}^{t}=x_{j},M^{t}=1]\\right]}\\\\ {\\displaystyle=\\,\\frac{2}{m}\\,\\mathbb{E}^{0}\\left[\\sum_{t=1}^{n}\\sum_{j=1}^{m}\\mathbb{P}^{\\pi^{0}|\\pi^{t}}[x_{1}^{t}=x_{j},M^{t}=1]\\right]}\\\\ {\\displaystyle=\\,\\frac{2}{m}\\,\\mathbb{E}^{0}\\left[\\sum_{t=1}^{n}\\mathbb{P}^{\\pi^{0}|\\pi^{t}}[M^{t}=1]\\right]}\\\\ {\\displaystyle=\\,\\frac{2}{m}\\,\\mathbb{E}^{0}[M].}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "It follows that if $\\mathbb{E}^{0}[M]\\leq m/56$ , then $D_{\\mathsf{T V}}(\\mathbb{P}^{0},\\bar{\\mathbb{P}})\\leq1/2$ , so that the algorithm must have ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{i\\in\\{0,...,m\\}}\\mathbb{E}^{j}[J^{j}(\\pi^{j})-J^{j}(\\widehat\\pi)]\\geq\\frac{H}{8m}.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "H.2 An Instance-Dependent Lower Bound for Stochastic Experts ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "In this section, we further investigate the optimality of LogLossBC for stochastic experts (Theorem 3.1). Recall that when $\\log\\lvert\\Pi\\rvert=O(1)$ the leading-order term in Theorem 3.1 scales as roughly $\\sqrt{\\sigma_{\\pi^{\\star}}^{2}/n}$ , where the salient quantity is the variance ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\sigma_{\\pi^{\\star}}^{2}:=\\,\\sum_{h=1}^{H}\\mathbb{E}^{\\pi^{\\star}}\\Bigl[(Q_{h}^{\\pi^{\\star}}(x_{h},\\pi^{\\star}(x_{h}))-Q_{h}^{\\pi^{\\star}}(x_{h},a_{h}))^{2}\\Bigr]\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "for the expert policy $\\pi^{\\star}$ . Theorem G.1 shows that this is optimal qualitatively, in the sense that for any value $\\sigma^{2}$ , there exists a class of MDPs where the $\\sigma_{\\pi^{\\star}}^{2}\\leq\\sigma^{2}$ , and where the minimax rate is at least $\\sqrt{\\sigma^{2}/n}$ . ", "page_idx": 48}, {"type": "text", "text": "In what follows, we will prove that for the special case of autoregressive MDPs (that is, the special case of the imitation learning problem in which the state takes the form $x_{h}=a_{1:h-1}$ ; cf. Appendix B.3), Theorem G.1 is optimal on a per-policy basis. Concretely, we prove a local minimax lower bound [28] which states that for any policy $\\pi^{\\star}$ and any reward function $r^{\\star}$ , there exists a difficult \u201calternative\u201d policy $\\widetilde{\\pi}$ , such that in worst case over rewards $r\\in\\{-r^{\\star},+r^{\\star}\\}$ and expert policies $\\pi\\in\\{\\pi^{\\star},\\widetilde{\\pi}\\}$ , any algorithm must have regret at least $\\sqrt{\\sigma^{2}/n}$ . ", "page_idx": 48}, {"type": "text", "text": "Theorem H.2. Consider the offilne imitation learning setting, and let $M^{\\star}$ be an autoregressive MDP. Let a reward function $r^{\\star}$ with $\\textstyle\\sum_{h=1}^{H}r_{h}^{\\star}\\in[0,R]$ almost surely be fixed, and let an expert policy $\\pi^{\\star}$ be given. For any $n\\in\\mathbb{N}$ , there exists an alternative policy $\\widetilde{\\pi}$ such that ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathrm{Alg}}\\operatorname*{max}_{\\pi\\in\\{\\pi^{\\star},\\widetilde{\\pi}\\}}\\operatorname*{max}_{r\\in\\{r^{\\star},-r^{\\star}\\}}\\mathbb{P}\\Bigg[J(\\pi)-J(\\widehat\\pi)\\geq c\\cdot\\sqrt{\\frac{\\sigma_{\\pi^{\\star}}^{2}}{n}}\\Bigg]\\geq\\frac{1}{4}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "for all n \u2265c\u2032 \u00b7 \u03c3R22, where c, c\u2032 > 0 are absolute constants. ", "page_idx": 48}, {"type": "text", "text": "Theorem H.2 suggests that the leading term in Theorem 3.1 cannot be improved substantially without additional assumptions, on a (nearly) per-instance basis. The restriction to $\\begin{array}{r}{\\bar{n}\\geq c^{\\prime}\\cdot\\frac{R^{2}}{\\sigma_{\\pi^{\\star}}^{2}}}\\end{array}$ in Theorem H.2 is somewhat natural, as this corresponds to the regime in which the $\\sqrt{\\sigma_{\\pi^{\\star}}^{2}/n}$ term in Theorem 3.1 dominates the lower-order term. ", "page_idx": 48}, {"type": "text", "text": "Proof of Theorem H.2. We begin by observing that for any $\\Delta>0$ , ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbb{A}\\mathbb{g}}\\operatorname*{max}_{\\pi\\in\\{\\pi^{*},\\tilde{\\pi}\\}}\\operatorname*{max}_{r\\in\\{r^{*},-r^{*}\\}}\\mathbb{P}[J_{r}(\\pi)-J_{r}(\\widehat\\pi)\\geq\\Delta]\\geq\\operatorname*{min}_{\\mathbb{A}\\mathbb{g}}\\operatorname*{max}_{\\pi\\in\\{\\pi^{*},\\tilde{\\pi}\\}}\\mathbb{P}[\\left|J_{r^{*}}(\\pi)-J_{r^{*}}(\\widehat\\pi)\\right|\\geq\\Delta].\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "with the convention that $J_{r}(\\pi)$ denotes the expected reward under $r$ ; we abbreviate $J(\\pi)\\equiv J_{r^{\\star}}(\\pi)$ going forward. Let $\\mathbb{P}_{n}^{\\pi}$ denote the law of the offline imitation learning dataset under $\\pi$ . If we set $\\breve{\\Delta}=\\breve{|}J(\\pi^{\\star})-J(\\widetilde{\\pi})|/\\breve{2}$ , then by the standard Le Cam two-point argument, we have that ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{max}\\biggl\\{\\mathbb{P}_{n}^{\\pi^{\\star}}[\\vert J(\\pi^{\\star})-J(\\widehat\\pi)\\vert\\ge\\Delta],\\mathbb{P}_{n}^{\\widetilde\\pi}[\\vert J(\\widetilde\\pi)-J(\\widehat\\pi)\\vert\\ge\\Delta]\\biggr\\}}\\\\ &{\\ge\\frac{1}{2}\\Big(1-\\mathbb{P}_{n}^{\\pi^{\\star}}[\\vert J(\\pi^{\\star})-J(\\widehat\\pi)\\vert<\\Delta]+\\mathbb{P}_{n}^{\\widetilde\\pi}[\\vert J(\\widetilde\\pi)-J(\\widehat\\pi)\\vert\\ge\\Delta]\\Big)}\\\\ &{\\ge\\frac{1}{2}\\Big(1-\\mathbb{P}_{n}^{\\pi^{\\star}}[\\vert J(\\widetilde\\pi)-J(\\widehat\\pi)\\vert\\ge\\Delta]+\\mathbb{P}_{n}^{\\widetilde\\pi}[\\vert J(\\widetilde\\pi)-J(\\widehat\\pi)\\vert\\ge\\Delta]\\Big)}\\\\ &{\\ge\\frac{1}{2}\\Big(1-D_{\\mathsf{T V}}\\Big(\\mathbb{P}_{n}^{\\pi^{\\star}},\\mathbb{P}_{n}^{\\widetilde\\pi}\\Big)\\Big)\\ \\ge\\frac{1}{2}\\bigg(1-\\sqrt{n\\cdot D_{\\mathsf{H}}^{2}(\\mathbb{P}^{\\pi^{\\star}},\\mathbb{P}^{\\widetilde\\pi})}\\bigg),}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "where the final inequality uses the standard tensorization property for Hellinger distance (e.g., Wainwright [91]). ", "page_idx": 49}, {"type": "text", "text": "We will proceed by showing that ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\omega_{\\pi^{\\star}}(\\varepsilon):=\\operatorname*{sup}_{\\pi}\\Bigl\\{|J(\\pi)-J(\\pi^{\\star})|\\ |\\ D_{\\mathsf{H}}^{2}\\Bigl(\\mathbb{P}^{\\pi^{\\star}},\\mathbb{P}^{\\pi}\\Bigr)\\leq\\varepsilon^{2}\\Bigr\\}\\geq\\Omega(1)\\cdot\\sqrt{\\sigma_{\\pi^{\\star}}^{2}\\cdot\\varepsilon^{2}},\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "for any $\\varepsilon>0$ sufficiently small, from which the result will follow by setting $\\varepsilon^{2}\\propto1/n$ and ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\widetilde{\\pi}=\\arg\\operatorname*{max}_{\\pi}\\Bigl\\{|J(\\pi)-J(\\pi^{\\star})|\\;|\\;D_{\\mathsf{H}}^{2}\\Bigl(\\mathbb{P}^{\\pi^{\\star}},\\mathbb{P}^{\\pi}\\Bigr)\\le\\varepsilon^{2}\\Bigr\\}\\ge\\Omega(1)\\cdot\\sqrt{\\sigma_{\\pi^{\\star}}^{2}\\cdot\\varepsilon^{2}}.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "To prove this, we will appeal to the following technical lemma. ", "page_idx": 49}, {"type": "text", "text": "Lemma H.1. For any distribution $\\mathbb{Q}$ and function $h$ with $|h|\\leq R$ almost surely, it holds that for all $\\begin{array}{r}{0\\leq\\varepsilon^{2}\\leq\\frac{\\mathrm{Varg}[h]}{4R^{2}}}\\end{array}$ , there exists a distribution $\\mathbb{P}$ such that ", "page_idx": 49}, {"type": "equation", "text": "$I.\\ \\mathbb{E}_{\\mathbb{P}}[h]-\\mathbb{E}_{\\mathbb{Q}}[h]\\geq2^{-3}\\sqrt{\\mathrm{Var}_{\\mathbb{Q}}[h]\\cdot\\varepsilon^{2}}$ ", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Since stochastic policies $\\pi$ in the autoregressive MDP $M^{\\star}$ are equivalent to arbitrary joint laws over the sequence $a_{1:H}$ (via Bayes\u2019 rule) and $\\begin{array}{r}{{J}(\\pi)=\\mathbb{E}^{\\pi}\\left[\\sum_{h=1}^{H}{r}_{h}^{\\star}\\right]}\\end{array}$ , Lemma H.1 implies that for any $\\begin{array}{r}{\\varepsilon^{2}\\leq\\mathrm{Var}^{\\pi^{\\star}}\\left[\\sum_{h=1}^{H}r_{h}^{\\star}\\right]/4R^{2}}\\end{array}$ , there exists a policy $\\widetilde{\\pi}$ such that (i) $D_{\\mathsf{H}}^{2}(\\mathbb{P}^{\\pi^{\\star}},\\mathbb{P}^{\\widetilde\\pi})\\leq D_{\\mathsf{K L}}(\\mathbb{P}^{\\pi^{\\star}}\\parallel\\mathbb{P}^{\\widetilde\\pi})\\leq$ $\\varepsilon^{2}$ , and (ii) ", "page_idx": 49}, {"type": "equation", "text": "$$\nJ(\\widetilde{\\pi})-J(\\pi^{\\star})\\geq2^{-3}\\sqrt{\\mathrm{Var}^{\\pi^{\\star}}\\left[\\sum_{h=1}^{H}r_{h}^{\\star}\\right]\\cdot\\varepsilon^{2}}.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "This establishes Eq. (19). The result now follows by setting $\\textstyle\\varepsilon^{2}={\\frac{c}{n}}$ for an absolute constant $c>0$ so that $\\sqrt{n\\cdot D_{\\mathsf{H}}^{2}(\\mathbb{P}^{\\pi^{\\star}},\\mathbb{P}^{\\widetilde\\pi})}\\le1/2$ , which is admissible whenever $n\\ge c^{\\prime}\\cdot\\frac{R^{2}}{\\sigma_{\\pi^{\\star}}^{2}}$ . Finally, we observe that for any deterministic MDP, by Lemma D.5, ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\left.\\tau\\mathrm{ar}^{\\pi^{*}}\\left[\\sum_{h=1}^{H}r_{h}\\right]=\\mathbb{E}^{\\pi^{*}}\\left[\\sum_{h=1}^{H}\\mathrm{Var}^{\\pi^{*}}\\left[r_{h}+V_{h+1}^{\\pi^{*}}(x_{h+1})\\mid x_{h}\\right]\\right]=\\mathbb{E}^{\\pi^{*}}\\left[\\sum_{h=1}^{H}(Q_{h}^{\\pi^{*}}(x_{h},a_{h})-V_{h}^{\\pi^{*}}(x_{h}))^{2}\\right]~~~~.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "since deterministic MDPs satisfy ", "page_idx": 49}, {"type": "equation", "text": "$$\nQ_{h}^{\\pi^{\\star}}(x_{h},a_{h})=r_{h}(x_{h},a_{h})+V_{h+1}^{\\pi^{\\star}}(x_{h+1})\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "almost surely, and since $\\mathbb{E}^{\\pi^{\\star}}\\left[Q_{h}^{\\pi^{\\star}}(x_{h},a_{h})\\mid x_{h}\\right]=V_{h}^{\\pi^{\\star}}(x_{h}).$ ", "page_idx": 49}, {"type": "text", "text": "Proof of Lemma H.1. Recall that we assume the domain is countable, so that $\\mathbb{Q}$ admits a probability mass function $q$ . We will define $\\mathbb{P}$ via the probability mass function ", "page_idx": 49}, {"type": "equation", "text": "$$\np(x)=\\frac{q(x)e^{\\eta h(x)}}{\\sum_{x^{\\prime}}q(x^{\\prime})e^{\\eta h(x^{\\prime})}}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "for a parameter $\\eta>0$ . We begin by observing that ", "page_idx": 50}, {"type": "equation", "text": "$$\nD_{\\mathsf{K L}}(\\mathbb{Q}\\,\\|\\,\\mathbb{P})=\\log\\bigl(\\mathbb{E}_{\\mathbb{Q}}\\big[e^{\\eta h}\\big]\\bigr)-\\eta\\,\\mathbb{E}_{\\mathbb{Q}}[h]=\\log\\Bigl(\\mathbb{E}_{\\mathbb{Q}}\\Big[e^{\\eta(h-\\mathbb{E}_{\\mathbb{Q}}[h])}\\Big]\\Bigr).\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "We now use the following lemma. ", "page_idx": 50}, {"type": "text", "text": "Lemma H.2. For any random variable $X$ with $|X|\\leq R$ almost surely and any $\\eta\\in(0,(2R)^{-1})$ , ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\frac{\\eta^{2}}{8}\\mathrm{Var}[X]\\leq\\log\\Bigl(\\mathbb{E}\\Bigl[e^{\\eta(X-\\mathbb{E}[X])}\\Bigr]\\Bigr)\\leq\\eta^{2}\\mathrm{Var}[X].\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Hence, as long as $\\eta\\leq(2R)^{-1}$ , ", "page_idx": 50}, {"type": "equation", "text": "$$\nD_{\\mathsf{K L}}(\\mathbb{Q}\\,\\|\\,\\mathbb{P})\\leq\\eta^{2}\\mathrm{Var}_{\\mathbb{Q}}[h].\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "We set $\\begin{array}{r}{\\eta=\\operatorname*{min}\\biggr\\{\\sqrt{\\frac{\\varepsilon^{2}}{\\mathrm{Var}_{\\mathbb{Q}}[h]}},\\frac{1}{2R}\\biggr\\}}\\end{array}$ so that $D_{{\\sf K L}}(\\mathbb{Q}\\,\\|\\,\\mathbb{P})\\leq\\varepsilon^{2}$ . ", "page_idx": 50}, {"type": "text", "text": "Next, we compute that ", "page_idx": 50}, {"type": "equation", "text": "$$\n0\\leq D_{\\mathsf{K L}}(\\mathbb{P}\\,\\|\\,\\mathbb{Q})=\\eta\\,\\mathbb{E}_{\\mathbb{P}}[h]-\\log\\bigl(\\mathbb{E}_{\\mathbb{Q}}\\bigl[e^{\\eta h}\\bigr]\\bigr),\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "so that ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\mathbb{P}}[h]-\\mathbb{E}_{\\mathbb{Q}}[h]\\geq\\eta^{-1}\\log\\bigl(\\mathbb{E}_{\\mathbb{Q}}\\bigl[e^{\\eta h}\\bigr]\\bigr)-\\mathbb{E}_{\\mathbb{Q}}[h]=\\eta^{-1}\\log\\Bigl(\\mathbb{E}_{\\mathbb{Q}}\\Bigl[e^{\\eta(h-\\mathbb{E}_{\\mathbb{Q}}[h])}\\Bigr]\\Bigr).}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Since $\\eta\\leq(2R)^{-1}$ , Lemma H.2 yields ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbb{P}}[h]-\\mathbb{E}_{\\mathbb{Q}}[h]\\geq\\frac{\\eta}{8}\\mathrm{Var}_{\\mathbb{Q}}[h]=\\frac{1}{8}\\sqrt{\\mathrm{Var}_{\\mathbb{Q}}[h]\\cdot\\varepsilon^{2}}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "as long as $\\begin{array}{r}{\\varepsilon^{2}\\leq\\frac{\\mathrm{Var}_{\\mathbb{Q}}[h]}{4R^{2}}}\\end{array}$ ", "page_idx": 50}, {"type": "text", "text": "Proof of Lemma H.2. Note that $e^{x}\\leq1+x+(e-2)x^{2}\\leq1+x+x^{2}$ whenever $|x|\\le1$ , and similarly $\\begin{array}{r}{e^{x}\\geq1+x+\\frac{x^{2}}{4}}\\end{array}$ for $\\vert x\\vert\\le1$ . It follows that if $\\eta\\leq(2R)^{-1}$ , ", "page_idx": 50}, {"type": "equation", "text": "$$\n1+{\\frac{\\eta^{2}}{4}}\\mathrm{Var}(X)\\leq\\mathbb{E}{\\Big[}e^{\\eta(X-\\mathbb{E}[X])}{\\Big]}\\leq1+\\eta^{2}\\mathrm{Var}(X).\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "We conclude by using that ${\\frac{x}{2}}\\leq\\log(1+x)\\leq x$ for $x\\in[0,1]$ . ", "page_idx": 50}, {"type": "text", "text": "H.3 Tightness of the Hellinger Distance Reduction ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Theorem 2.1 and Theorem 3.1 are supervised learning reductions that bound the regret of any policy $\\widehat{\\pi}$ in terms of its Hellinger distance $D_{\\mathsf{H}}^{2}\\big(\\mathbb{P}^{\\widehat{\\pi}},\\mathbb{P}^{\\pi^{\\star}}\\big)$ to the expert policy $\\pi^{\\star}$ . The following result shows t hat these reductions are tight in a fairly strong instance-dependent sense: Namely, for any pair of policies $\\widehat{\\pi}$ and $\\pi^{\\star}$ , and for any reward-free MDP $M^{\\star}$ , it is possible to design a reward function $\\bar{r}=\\{r_{h}\\}_{h=1}^{H}$ for which each term in Eq. (9) of Theorem 3.1 is tight, and such that Theorem 2.1 is tight; the only caveat is that we require the reward function to be non-Markovian, in the sense that $r_{h}$ depends on the full history $x_{1:h}$ and $a_{1:h}$ . ", "page_idx": 50}, {"type": "text", "text": "Theorem H.3 (Converse to Theorems 2.1 and 3.1). Let a reward-free MDP $M^{\\star}$ and a pair of (potentially stochastic) policies $\\widehat{\\pi}$ and $\\pi^{\\star}$ be given. ", "page_idx": 50}, {"type": "text", "text": "1. For any $R>0$ , there exists a non-Markovian reward function $r=\\,\\{r_{h}\\}_{h=1}^{H}\\,w i t h\\,\\textstyle\\sum_{h=1}^{H}r_{h}\\,\\in$ $[0,R]$ such that ", "page_idx": 50}, {"type": "equation", "text": "$$\nJ(\\pi^{\\star})-J(\\widehat{\\pi})\\geq\\frac{R}{6}\\cdot D_{\\mathsf{H}}^{2}\\big(\\mathbb{P}^{\\widehat{\\pi}},\\mathbb{P}^{\\pi^{\\star}}\\big).\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "2. For any $\\sigma^{2}>0,$ , there exists a non-Markovian reward function $r=\\{r_{h}\\}_{h=1}^{H}$ for which $\\sigma_{\\pi^{\\star}}^{2}:=$ $\\begin{array}{r}{\\sum_{h=1}^{H}\\mathbb{E}^{\\pi^{\\star}}\\left[(Q_{h}^{\\pi^{\\star}}(x_{1:h},a_{1:h})-V_{h}^{\\pi^{\\star}}(x_{1:h},a_{1:h-1}))^{2}\\right]\\le\\sigma^{2},}\\end{array}$ , and such that22 ", "page_idx": 51}, {"type": "equation", "text": "$$\nJ(\\pi^{\\star})-J(\\widehat\\pi)\\geq\\frac{1}{9}\\sqrt{\\sigma^{2}\\cdot D_{\\mathsf{H}}^{2}(\\mathbb{P}^{\\widehat\\pi},\\mathbb{P}^{\\pi^{\\star}})}.\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "3. For any $R>0$ and $\\sigma^{2}>0$ , there exists a non-Markovian reward function $r=\\{r_{h}\\}_{h=1}^{H}$ with $\\textstyle\\sum_{h=1}^{H}r_{h}\\in[0,R]$ and $\\sigma_{\\pi^{\\star}}^{2}\\leq\\sigma^{2}$ simultaneously such that ", "page_idx": 51}, {"type": "equation", "text": "$$\nJ(\\pi^{\\star})-J(\\widehat\\pi)\\geq\\frac{1}{9}\\operatorname*{min}\\biggr\\{\\sqrt{\\sigma^{2}\\cdot D_{\\mathsf{H}}^{2}(\\mathbb{P}^{\\widehat\\pi},\\mathbb{P}^{\\pi^{\\star}})},R\\cdot D_{\\mathsf{H}}^{2}\\bigl(\\mathbb{P}^{\\widehat\\pi},\\mathbb{P}^{\\pi^{\\star}}\\bigr)\\biggr\\}.\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Eq. (20) shows that there exist reward functions with bounded range for which Theorem 2.1 and the lower-order term in Eq. (9) of Theorem 3.1 are tight, while Eq. (21) shows that there exist reward functions with bounded variance (but not necessarily bounded range) for which the leading term in Eq. (9) or Theorem 3.1 is tight. ", "page_idx": 51}, {"type": "text", "text": "Note that for some MDPs, the state $x_{h}$ already contains the full history $x_{1:h-1},a_{1:h-1}$ , so the assumption of non-Markovian rewards is without loss of generality. For MDPs that do not have this property, Theorem H.3 leaves open the possibility that Theorems 2.1 and 3.1 can be improved on a per-MDP basis. ", "page_idx": 51}, {"type": "text", "text": "Proof of Theorem H.3. Consider a pair of measures $\\mathbb{P}$ and $\\mathbb{Q}$ , and set $\\overline{{\\mathbb{P}}}:=\\frac{1}{2}(\\mathbb{P}+\\mathbb{Q})$ . Consider the function ", "page_idx": 51}, {"type": "equation", "text": "$$\nh=1-\\frac{1}{2}\\frac{\\mathbb{Q}}{\\overline{{{\\mathbb{P}}}}}\\in[0,1].\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Using Lemma D.4, we observe that ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbb{P}}[h]-\\mathbb{E}_{\\mathbb{Q}}[h]=2\\big(\\mathbb{E}_{\\mathbb{P}}[h]-\\mathbb{E}_{\\mathbb{Q}}[h]\\big)=\\mathbb{E}_{\\mathbb{Q}}\\left[\\frac{\\mathbb{Q}}{\\mathbb{P}}\\right]-\\mathbb{E}_{\\mathbb{F}}\\left[\\frac{\\mathbb{Q}}{\\mathbb{P}}\\right]=D_{\\chi^{2}}\\big(\\mathbb{Q}\\parallel\\mathbb{P}\\big)\\ge\\frac{1}{6}D_{\\mathbf{H}}^{2}(\\mathbb{Q},\\mathbb{P}).\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "We also observe that by concavity of variance, ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\frac{1}{2}(\\mathrm{Var}_{\\mathbb{P}}[h]+\\mathrm{Var}_{\\mathbb{Q}}[h])\\leq\\mathrm{Var}_{\\mathbb{P}}[h]=\\frac{1}{4}\\mathbb{E}_{\\mathbb{P}}\\bigg[\\bigg(\\frac{\\mathbb{Q}}{\\mathbb{P}}-\\mathbb{E}_{\\mathbb{P}}\\bigg[\\frac{\\mathbb{Q}}{\\mathbb{P}}\\bigg]\\bigg)\\bigg]^{2}=D_{\\chi^{2}}\\big(\\mathbb{Q}\\parallel\\Bar{\\mathbb{P}}\\big)\\leq D_{\\mathbb{H}}^{2}(\\mathbb{Q},\\mathbb{P}).\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "To apply this observation to the theorem at hand, let a parameter $B>0$ be given, let $\\overline{{\\mathbb{P}}}:=\\frac{1}{2}(\\mathbb{P}^{\\pi^{\\star}}\\!+\\!\\mathbb{P}^{\\widehat\\pi})$ , and consider the non-Markov reward function $r$ that sets $r_{1},\\dots,r_{h-1}=0$ and ", "page_idx": 51}, {"type": "equation", "text": "$$\nr_{H}(\\tau)=B\\cdot\\left(1-\\frac{1}{2}\\frac{\\mathbb{P}^{\\widehat{\\pi}}}{\\mathbb{P}}\\right)\\in[0,B].\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Then by Eq. (22), we have that ", "page_idx": 51}, {"type": "equation", "text": "$$\nJ(\\pi^{\\star})-J(\\widehat{\\pi})\\geq\\frac{B}{6}\\cdot D_{\\mathsf{H}}^{2}\\Big(\\mathbb{P}^{\\widehat{\\pi}},\\mathbb{P}^{\\pi^{\\star}}\\Big).\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "At the same time, by Eq. (23), we have that ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\operatorname{Var}^{\\pi^{\\star}}\\biggl[\\sum_{h=1}^{H}r_{h}\\biggr]=\\operatorname{Var}^{\\pi^{\\star}}[r_{H}]\\leq2B^{2}\\cdot D_{\\mathsf{H}}^{2}\\Bigl(\\mathbb{P}^{\\widehat{\\pi}},\\mathbb{P}^{\\pi^{\\star}}\\Bigr),\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "and by Proposition 3.1, ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\sigma_{\\pi^{\\star}}^{2}\\leq\\mathrm{Var}^{\\pi^{\\star}}\\left[\\sum_{h=1}^{H}r_{h}\\right].\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "To conclude, note that if we set $B^{2}=R^{2}$ , then $\\textstyle\\sum_{h=1}^{H}r_{h}\\in[0,R]$ and ", "page_idx": 52}, {"type": "equation", "text": "$$\nJ(\\pi^{\\star})-J(\\widehat{\\pi})\\geq\\frac{R}{6}\\cdot D_{\\mathsf{H}}^{2}\\Big(\\mathbb{P}^{\\widehat{\\pi}},\\mathbb{P}^{\\pi^{\\star}}\\Big).\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Meanwhile, if we set ", "page_idx": 52}, {"type": "equation", "text": "$$\nB^{2}=\\frac{\\sigma^{2}}{2D_{\\mathsf{H}}^{2}(\\mathbb{P}\\widehat{\\pi}_{,}\\mathbb{P}\\pi^{\\star})},\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "then $\\sigma_{\\pi^{\\star}}^{2}\\leq\\sigma^{2}$ and ", "page_idx": 52}, {"type": "equation", "text": "$$\nJ(\\pi^{\\star})-J(\\widehat\\pi)\\geq\\frac{1}{9}\\sqrt{\\sigma^{2}\\cdot D_{\\mathsf{H}}^{2}(\\mathbb{P}^{\\widehat\\pi},\\mathbb{P}^{\\pi^{\\star}})}.\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Finally, if we set ", "page_idx": 52}, {"type": "equation", "text": "$$\nB^{2}=\\frac{\\sigma^{2}}{2D_{\\mathsf{H}}^{2}(\\mathbb{P}\\widehat{\\pi}_{\\,},\\mathbb{P}\\pi^{\\star})}\\wedge R^{2}.\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Then $\\begin{array}{r}{\\sum_{h=1}^{H}r_{h}\\in[0,R],\\sigma_{\\pi^{\\star}}^{2}\\le\\sigma^{2}}\\end{array}$ , and ", "page_idx": 52}, {"type": "equation", "text": "$$\nJ(\\pi^{\\star})-J(\\widehat\\pi)\\geq\\frac{B}{6}\\cdot D_{\\mathsf{H}}^{2}\\Big(\\mathbb{P}^{\\widehat\\pi},\\mathbb{P}^{\\pi^{\\star}}\\Big)\\geq\\operatorname*{min}\\bigg\\{\\frac{1}{9}\\sqrt{\\sigma^{2}\\cdot D_{\\mathsf{H}}^{2}(\\mathbb{P}^{\\widehat\\pi},\\mathbb{P}^{\\pi^{\\star}})},\\frac{R}{6}\\cdot D_{\\mathsf{H}}^{2}\\Big(\\mathbb{P}^{\\widehat\\pi},\\mathbb{P}^{\\pi^{\\star}}\\Big)\\bigg\\}.\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "I Benefits of Online Interaction ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Our results in Sections 2 and 3 show that the benefits of online interaction in imitation learning\u2014to the extent that horizon is concerned\u2014are more limited than previously thought. We expect that in practice, online interaction will likely lead to beneftis, but only in a problem-dependent sense. To this end, we first discuss the role of misspecification and the realizability assumption used by our results, then highlight several special cases in which online interaction is indeed beneficial, but in a policy class-dependent fashion not captured by existing theory. In particular, we identify three phenomena which lead to improved sample complexity: (i) representational benefits; (ii) value-based feedback; and (iii) exploration. Our results in this section can serve as a starting point toward developing a more fine-grained understanding of algorithms and sample complexity of imitation learning. ", "page_idx": 52}, {"type": "text", "text": "I.1 The Role of Misspecification ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "This paper (for both deterministic and stochastic experts) focuses on the realizable setting in which $\\pi^{\\star}\\in\\Pi$ . It is natural to ask how the role of horizon in imitation learning changes under misspecification, and whether online interaction brings greater benefits in this case. This is a subtle issue, as there are various incomparable notions of misspecification error which can lead to different forms of horizon dependence. For example, for deterministic experts, if $\\Pi$ is misspecified in the sense that $\\operatorname*{inf}_{\\pi\\in\\Pi}L_{\\mathtt{b c}}(\\pi)\\ \\leq\\ \\varepsilon_{\\mathsf{a p x}}$ , the indicator-loss behavior cloning algorithm in ?? achieves $\\begin{array}{r}{J(\\pi^{\\star})-J(\\widehat{\\pi})\\lesssim R H\\cdot\\left(\\frac{\\log\\left(|\\Pi|\\delta^{-1}\\right)}{n}+\\varepsilon_{\\mathsf{a p x}}\\right)}\\end{array}$ , which is tight in general. In other words, the dependence on $\\varepsilon_{\\mathsf{a p x}}$ is not horizon-independent. On the other hand, as we show in Appendix E, if we assume that $\\operatorname*{inf}_{\\pi\\in\\Pi}D_{\\chi^{2}}(\\mathbb{P}^{\\pi^{\\star}}\\parallel\\mathbb{P}^{\\pi})\\,\\le\\,\\varepsilon_{\\mathsf{a p x}}$ , a stronger notion of misspecification error, then LogLossBC achieves a horizon-independent guarantee of the form $\\begin{array}{r}{J(\\pi^{\\star})-J(\\widehat{\\pi})\\lesssim R\\cdot\\left(\\frac{\\log(|\\Pi|\\delta^{-1})}{n}+\\varepsilon_{\\mathsf{a p x}}\\right)}\\end{array}$ . We leave a detailed investigation of tradeoffs between misspecification and horizon (as well as interplay with online versus offilne IL) for future work; by giving the first horizon-independent treatment for the realizable setting, we hope that our results can serve as a starting point. ", "page_idx": 52}, {"type": "text", "text": "I.2 Representational Benefits ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "The classical intuition behind algorithms like Dagger and Aggrevate (which Definition 1.1 attempts to quantify) is recoverability: through online access, we can learn to correct the mistakes of an imperfect policy. Our results in Sections 2 and 3 show that recoverability has limited benefits for stationary policy classes as far as horizon is concerned. In spite of this, the following proposition shows that recoverability can have pronounced benefits for representational reasons, even with constant horizon. ", "page_idx": 52}, {"type": "text", "text": "Proposition I.1 (Representational benefits of online IL). For any $N\\in\\mathbb{N},$ , there exists a class $\\mathcal{M}$ of MDPs with $H=2$ and a policy class \u03a0 with $\\log\\lvert\\Pi\\rvert=O(N)$ such that ", "page_idx": 53}, {"type": "text", "text": "\u2022 There is an online imitation learning algorithm that achieves $J(\\pi^{\\star})-J(\\widehat\\pi)=0$ with probability at least $1-\\delta$ using $O(\\log(\\delta^{-1}))$ episodes for any MDP $M^{\\star}\\in\\mathcal{M}$ and expert policy $\\pi^{\\star}\\in\\Pi$ . In particular, this can be achieved by Dagger.   \n\u2022 Any proper offline imitation learning algorithm requires $n=\\Omega(N)$ trajectories to learn a nontrivial policy with $J(\\pi^{\\star})-J(\\widehat\\pi)\\leq\\bar{c}$ for an absolute constant $c>0$ .23 ", "page_idx": 53}, {"type": "text", "text": "The idea behind this construction is as follows: The behavior of the (stochastic) expert policy at step $h=1$ is very complex, and learning to imitate it well in distribution (e.g., with respect to total variation or Hellinger distance) is a difficult representation learning problem (in the language of Section 2, e.g., Theorem 2.1, we must take $\\mathrm{log}|\\Pi_{1}|$ very large in order to realize $\\pi_{1}^{\\star}$ ). For offline imitation learning, we have no choice but to imitate $\\pi_{1}^{\\star}$ well at $h=1$ , leading to the lower bound in Proposition I.1. With online access though, we can give up on learning $\\pi_{1}^{\\star}$ well, and instead learn to correct our mistake at step $h\\,=\\,2$ . For the construction in Proposition I.1, this a much easier representation learning problem, and requires very low sample complexity (i.e., we can realize $\\pi_{2}^{\\star}$ with a class $\\Pi_{2}$ for which $\\mathrm{log}|\\Pi_{2}|$ is small. We conclude that Dagger can indeed lead to substantial benefits over offline $\\mathrm{IL}$ , but for representational reasons unrelated to horizon, and not captured by existing theory. While this example is somewhat contrived, it suggests that potential to develop a deeper understanding of representational beneftis in imitation learning, which we leave as a promising direction for future work. ", "page_idx": 53}, {"type": "text", "text": "I.3 Benefits of Value-Based Feedback ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Beginning with the work of Ross and Bagnell [71] on Aggrevate, many works (e.g., Sun et al. [78]) consider a value-based feedback variant of the online $\\mathrm{IL}$ framework (Section 1.1) where in addition to (or instead of) observing $a_{h}^{\\star}$ , the learner observes the expert\u2019s advantage function $A_{h}^{\\pi^{\\star}}(x_{h},\\cdot):=$ $Q_{h}^{\\pi^{\\star}}(x_{h},\\pi_{h}^{\\star}(x_{h}))-Q_{h}^{\\pi^{\\star}}(x_{h},\\cdot)$ or value function $Q_{h}^{\\pi^{\\star}}(x_{h},\\cdot)$ at every state visited by the learner (see Appendix I.5.2 for details, which are deferred to the appendix for space). While such feedback intuitively seems useful, existing theoretical guarantees\u2014to the best of our knowledge\u2014[71, 78] only show that algorithms like Aggrevate are no worse than non-value based methods like Dagger, and do not quantify situations in which value-based feedback actually leads to improvement.24 ", "page_idx": 53}, {"type": "text", "text": "The following result shows that i) value-based feedback can lead to arbitrarily large improvement over non-value based feedback for representational reasons similar to Proposition I.1 (that is for a complicated stochastic expert, learning to optimize a fixed value function can be much easier than learning to imitate the expert well in TV distance), but ii) it is only possible to exploit value-based feedback in this fashion under online interaction (that is, even if we annotate the trajectories for offline imitation learning with $A_{h}^{\\pi^{\\star}}(x_{h},\\cdot)$ for the visited states, this cannot lead to improvement in sample complexity). ", "page_idx": 53}, {"type": "text", "text": "Proposition I.2 (Benefits of value-based feedback (informal)). For any $N\\in\\mathbb{N},$ , there is a class of MDPs $\\mathcal{M}$ with $H=2$ and a policy class $\\Pi$ with $\\log\\lvert\\Pi\\rvert=O(N)$ such that ", "page_idx": 53}, {"type": "text", "text": "\u2022 There is an online imitation learning algorithm with value-based feedback that achieves $J(\\pi^{\\star})-$ $J(\\widehat{\\pi})=0$ with probability at least $1-\\delta$ using $O(\\log(\\delta^{-1}))$ episodes for every MDP $M^{\\star}\\in\\mathcal{M}$ and expert $\\pi^{\\star}\\in\\Pi.$ . In particular, this can be achieved by Aggrevate.   \n\u2022 Any proper offline imitation learning algorithm (with value-based feedback) or proper online imitation learning algorithm (without valued-based feedback) requires $n=\\Omega(N)$ trajectories to learn a non-trivial policy with $J(\\pi^{\\star})-J(\\widehat\\pi)\\leq c$ for an absolute constant $c>0$ .25   \nAs with Proposition I.1, this example calls for a fine-grained policy class-dependent theory, which we   \nhope to explore more deeply in future work. ", "page_idx": 53}, {"type": "text", "text": "I.4 Benefits from Exploration ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "A final potential benefit of online interaction arises in exploration. One might hope that with online access, we can directly guide the MDP to informative states that will help to identify the optimal policy faster. The following proposition gives an example in which deliberate exploration can lead to arbitrarily large improvement over offline imitation learning, as well as over naive online imitation learning algorithms like Dagger that do not deliberately explore. ", "page_idx": 54}, {"type": "text", "text": "Proposition I.3 (Benefits of exploration for online IL). For any $n\\in\\mathbb N$ and $H\\in\\mathbb{N}$ , there exists an MDP $M^{\\star}$ and a class of deterministic policies \u03a0 with $|\\Pi|=2$ with the following properties. ", "page_idx": 54}, {"type": "text", "text": "1. There exists an online imitation learning algorithm that returns a policy $\\widehat{\\pi}$ such that $J(\\pi^{\\star})-J(\\widehat\\pi)=0$ with probability at least $1-\\delta$ using $O(\\log(\\delta^{-1}))$ episodes, for all possible reward functions (i.e., even if $\\mu=H$ ). 2. For any offline imitation learning algorithm, there exists a deterministic reward function $r~=~\\{r_{h}\\}_{h=1}^{H}$ and expert policy $\\pi^{\\star}\\in\\ \\Pi$ with $\\mu\\:=\\:1\\:$ such that any algorithm must have $\\begin{array}{r}{\\mathbb{E}[J(\\pi^{\\star})-J(\\widehat\\pi)]\\geq\\Omega(1)\\cdot\\frac{H}{n}}\\end{array}$ . In addition, Dagger has regret $\\begin{array}{r}{\\mathbb{E}[J(\\pi^{\\star})-J(\\widehat\\pi)]\\geq\\Omega(1)\\cdot\\frac{H}{n}}\\end{array}$ . ", "page_idx": 54}, {"type": "text", "text": "The idea behind this construction is simple: We take the lower bound construction from Theorem 2.2 and augment it with a \u201crevealing\u201d which directly reveals the identity of the underlying expert. The true expert never visits this state, so offilne imitation learning algorithms cannot exploit it (standard online $\\mathrm{IL}$ algorithms like Dagger and relatives do not exploit the revealing state for the same reason),26 but a well-designed online $\\mathrm{IL}$ algorithm that deliberately navigates to the revealing state can use it to identify $\\pi^{\\star}$ extremely quickly. ", "page_idx": 54}, {"type": "text", "text": "As with the previous examples, this construction is somewhat contrived, but it suggests that directly maximizing information acquisition may be a useful algorithm design paradigm for online $\\mathrm{IL}$ , and we hope to explore this more deeply in future work. ", "page_idx": 54}, {"type": "text", "text": "I.5 Proofs ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "I.5.1 Proof of Proposition I.1 ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Proof of Proposition I.1. Let $N\\in\\mathbb{N}$ be given. We set $\\mathcal{X}\\,=\\,\\{{\\mathfrak{x}},{\\mathfrak{y}},{\\mathfrak{z}}\\}$ , $\\mathcal{A}=[N]\\cup\\{\\mathfrak{a},\\mathfrak{b}\\}$ , and $H=2$ . We consider a family of problem instances $\\{(M,\\pi^{\\star},r)\\}$ indexed by a subset $S\\subset[N]$ with $\\vert{\\cal S}\\vert=N/2$ and an action $a^{\\star}\\in\\{{\\mathfrak{a}},{\\mathfrak{b}}\\}$ as follows. For a given pair $(S,a^{\\star})$ : ", "page_idx": 54}, {"type": "text", "text": "\u2022 The dynamics are as follows. We have $x_{1}=\\mathfrak{X}$ deterministically. For simplicity, we assume that only actions in $[N]$ are available at step $h=1$ . If $a_{1}\\in S$ , then $x_{2}=\\mathfrak{y}$ , otherwise $x_{2}=\\Xi$ . \u2022 The reward at step 1 is $r_{1}(\\cdot,\\cdot)\\;=\\;0$ , and the reward at step 2 is given by $r_{2}(\\mathfrak{y},\\cdot)\\;=\\;1$ and $r_{2}(\\z,a)=\\mathbb{I}\\{a=\\bar{a}^{\\star}\\}$ . \u2022 The expert $\\pi^{\\star}$ sets $\\pi^{\\star}(\\pmb{\\mathfrak{x}})=\\operatorname{unif}(S),\\pi^{\\star}(\\mathfrak{y})=\\operatorname{unif}(\\{\\mathfrak{a},\\mathfrak{b}\\})$ , and $\\pi^{\\star}(\\z)=a^{\\star}$ . ", "page_idx": 54}, {"type": "text", "text": "Let us refer to the problem instance above as ${\\mathbb Z}_{S,a^{\\star}}\\,=\\,\\bigl\\{(M_{S,a^{\\star}},\\pi_{S,a^{\\star}}^{\\star},r_{S,a^{\\star}})\\bigr\\}$ , and let $J_{S,a^{\\star}}(\\pi)$ denote the expected reward under this instance. ", "page_idx": 54}, {"type": "text", "text": "Upper bound for online imitation learning. Consider the algorithm that sets ${\\widehat{\\pi}}_{1}^{i}={\\mathrm{unif}}([N])$ for each $i\\in[n]$ . If we play for $n=\\log_{2}(\\delta^{-1})$ episodes, we will see $x_{2}=\\Xi$ in at le a st one episode with probability at least $1-\\delta$ , at which point we will observe $a^{\\star}=\\pi^{\\star}(\\z)$ , and we can return the policy $\\widehat{\\pi}$ that sets ${\\dot{\\widehat{\\pi}}}_{1}({\\mathfrak{x}})=\\operatorname{unif}([N])$ and $\\widehat{\\pi}_{2}(\\cdot)=a^{\\star}$ ; this policy has zero regret. ", "page_idx": 54}, {"type": "text", "text": "Note that if we define $\\Pi=\\,\\bigl\\{\\pi_{S,a^{\\star}}^{\\star}\\bigr\\}_{|S|=N/2,a^{\\star}\\in\\{\\mathfrak{a},\\mathfrak{b}\\}}$ as the natural policy class for the family of instances above, then the algorithm above is equivalent to running Dagger with the online learning algorithm that, at iteration $i$ , sets ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\widehat{\\pi}_{h}^{i}=\\operatorname{unif}\\big(\\{\\pi\\in\\Pi_{h}\\ |\\ \\pi_{2}(\\tilde{z})=a_{2}^{\\star,j}\\ \\forall j<i:x_{2}^{j}=\\tilde{z}\\}\\big),\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "and choosing the final policy as $\\widehat{\\pi}=\\widehat{\\pi}^{i}$ for any iteration $i$ after $x_{2}=\\Xi$ is encountered. ", "page_idx": 54}, {"type": "text", "text": "Lower bound for offilne imitation learning. Consider the offilne imitation learning setting. When the underlying instance is $\\mathcal{T}_{S,a^{\\star}}$ , we observe a dataset $\\mathcal{D}$ consisting of $n$ trajectories generated by executing $\\pi_{S,a^{\\star}}^{\\star}$ in $M_{S,a^{\\star}}$ . The trajectories never visit the state $\\breve{z}$ , so $a^{\\star}$ is not identifiable, and we can do no better than guessing $a^{\\star}$ uniformly in this state. Letting $\\mathbb{E}_{S,a^{\\star}}$ denote the law of $\\mathcal{D}$ under instance $\\mathcal{T}_{S,a^{\\star}}$ , we have $J_{S,a^{\\star}}(\\widehat{\\pi})\\,=\\,\\widehat{\\pi}_{1}(S\\mathbin{\\overline{{\\mid}}}\\,\\mathfrak{x})+\\widehat{\\pi}_{1}(S^{c}\\mathbin{\\mid}\\mathfrak{x})\\widehat{\\pi}_{2}(a^{\\star}\\mathbin{\\mid}\\overline{{\\mathfrak{z}}})$ . It follows that for any $S$ since the law of $\\mathcal{D}$ does not depe nd o n $a^{\\star}$ , ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\underset{a^{\\star}\\in\\{a,\\mathfrak{b}\\}}{\\operatorname*{max}}\\;\\mathbb{E}_{S,a^{\\star}}\\bigl[J_{S,a^{\\star}}(\\pi_{S,a^{\\star}}^{\\star})-J_{S,a^{\\star}}(\\widehat{\\pi})\\bigr]\\geq\\mathbb{E}_{S,a}[1-\\widehat{\\pi}_{1}(S\\mid\\mathfrak{x})-\\widehat{\\pi}_{1}(S^{c}\\mid\\mathfrak{x})/2]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\frac{1}{2}\\mathbb{E}_{S,a}[1-\\widehat{\\pi}_{1}(S\\mid\\mathfrak{x})].}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Note that if $\\widehat{\\pi}$ is proper in the sense that $\\widehat\\pi_{1}(\\cdot{\\boldsymbol{\\mathfrak{x}}})=\\operatorname{unif}(\\widehat{S})$ for some $\\widehat{S}\\subset[N]$ with $|\\widehat{S}|=N/2$ , we have $\\begin{array}{r}{1-\\widehat{\\pi}_{1}(S\\mid\\mathfrak{x})=1-\\frac{2}{N}|\\widehat{S}\\cup S|}\\end{array}$ . We conclude that if $\\begin{array}{r}{\\mathbb{E}_{S,a^{\\star}}\\left[J_{S,a^{\\star}}(\\pi_{S,a^{\\star}}^{\\star})-J_{S,a^{\\star}}(\\widehat\\pi)\\right]\\leq\\frac{1}{8}}\\end{array}$ , then $\\begin{array}{r}{\\mathbb{E}_{S,a^{\\star}}\\left[|\\widehat{S}\\cap S|\\right]\\geq\\frac{3}{8}N}\\end{array}$ . From here, it follows from standard lower bounds for discrete distribution estimation (e.g., Canonne [18]) that any such estimator $\\widehat S$ requires $n~=~\\Omega(N)$ samples for a worst-case choice of $S$ . \u53e3 ", "page_idx": 55}, {"type": "text", "text": "I.5.2 Background and Proof for Proposition I.2 ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Before proving Proposition I.2, we first formally introduce the value-based feedback model we consider. ", "page_idx": 55}, {"type": "text", "text": "Background on value-based feedback. We can consider two models for imitation learning with value-based feedback, inspired by Ross and Bagnell [71], Sun et al. [78]. ", "page_idx": 55}, {"type": "text", "text": "\u2022 Offline setting. In the offline setting, we receive $n$ trajectories $(x_{1},a_{1}),\\dots,(x_{H},a_{H})$ generated by executing $\\pi^{\\star}$ in $M^{\\star}$ . For each state in each such trajectory, we observe $A_{h}^{\\pi^{\\star}}(x_{h},\\cdot)$ , where $A_{h}^{\\pi^{\\star}}(x,a)=Q_{h}^{\\pi^{\\star}}(x,\\pi^{\\star}(x))-Q_{h}^{\\pi^{\\star}}(x,a)$ is the advantage function for $\\pi^{\\star}$ .27 ", "page_idx": 55}, {"type": "text", "text": "\u2022 Online setting. The online setting is as follows. There are $n$ at episodes. For each episode $i$ , we execute a policy ${\\widehat{\\pi}}^{i}$ , and receive a \u201ctrajectory\u201d $o^{i}=(x_{1}^{i},a_{1}^{i},a_{1}^{\\star,\\bar{i}}),\\dots,(x_{H}^{i},a_{H}^{i},a_{H}^{\\star,i\\bar{}})$ , where $a_{h}^{i}\\sim\\widehat{\\pi}^{i}(x_{h}^{i})$ and $a_{h}^{\\star,i}\\sim\\pi^{\\star}(x_{h}^{i})$ . In addition, for each state in the trajectory, we observe $A_{h}^{\\pi^{\\star}}(x_{h},\\cdot)$ After the $n$ episodes conclude, we output a final policy $\\widehat{\\pi}$ on which performance is evaluated. ", "page_idx": 55}, {"type": "text", "text": "Proof of Proposition I.2. We only sketch the proof, as it is quite similar to Proposition I.1. Let $N\\in\\mathbb{N}$ be given. We set $\\boldsymbol{S}\\,=\\,\\{\\boldsymbol{{\\mathfrak{x}}},\\boldsymbol{\\mathfrak{y}},\\boldsymbol{\\mathfrak{z}}\\}$ , $\\boldsymbol{\\mathcal{A}}=[N]$ , and $H=2$ . We consider a class of problem instances $\\{(M,\\pi^{\\star},r)\\}$ indexed by sets $S_{1},S_{2}\\subset[N]$ with $|S_{1}|=|S_{2}|=N/2$ defined as follows. For a given pair $(S_{1},S_{2})$ : ", "page_idx": 55}, {"type": "text", "text": "\u2022 The dynamics are as follows. We have $x_{1}=\\mathfrak{x}$ deterministically. If $a_{1}\\in S_{1}$ , then $x_{2}=\\mathfrak{y}$ , otherwise $x_{2}=\\Xi$ . \u2022 The reward function sets $r_{1}({\\mathfrak{x}},\\cdot)=0$ , $r_{2}(\\mathfrak{y},\\cdot)=1$ , and $r_{2}(\\z,a)=\\mathbb{I}\\{a\\in S_{2}\\}$ . \u2022 The expert $\\pi^{\\star}$ sets $\\pi^{\\star}({\\mathfrak{x}})=\\operatorname{unif}(S_{1})$ , $\\pi^{\\star}(\\z)=\\operatorname{unif}\\left(S_{2}\\right)$ , and $\\pi^{\\star}({\\mathfrak{y}})=\\operatorname{unif}([N])$ ", "page_idx": 55}, {"type": "text", "text": "We refer to the problem instance above as $\\mathcal{Z}_{S_{1},S_{2}}\\,=\\,\\left(M_{S_{1},S_{2}},\\pi_{S_{1},S_{2}}^{\\star},r_{S_{1},S_{2}}\\right)$ , and let $J_{S_{1},S_{2}}(\\pi)$ denote the expected reward under this instance. ", "page_idx": 55}, {"type": "text", "text": "Upper bound for online imitation learning with value-based feedback. Consider an algorithm that sets ${\\widehat{\\pi}}_{1}^{i}={\\mathrm{unif}}([N])$ for each $i\\in[n]$ . If we play for $n=\\log_{2}(\\delta^{-1})$ episodes, we will see $x_{2}=\\Xi$ in at least one episode with probability at least $1-\\delta$ , at which point we will observe $A_{2}^{\\pi^{\\star}}(z,\\cdot)$ . We can pick an arbitrary action with $A_{2}^{\\pi^{\\star}}(\\z,\\cdot)=0$ and return the policy $\\widehat{\\pi}$ that sets $\\widehat{\\pi}_{1}(\\mathfrak{x})=\\operatorname{unif}([N])$ and $\\widehat{\\pi}_{2}(\\cdot)=a$ ; this policy has zero regret. ", "page_idx": 55}, {"type": "text", "text": "Note that if we define $\\Pi\\,=\\,\\left\\{\\pi_{S_{1},S_{2}}^{\\star}\\right\\}_{|S_{1}|=|S_{2}|=N/2}$ as the natural policy class for the family of instances above, then the algorithm above is equivalent to running Aggrevate with the online learning algorithm that, at iteration $i$ , sets ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\widehat{\\pi}_{h}^{i}=\\operatorname{unif}\\big(\\{\\pi\\in\\Pi_{h}\\mid\\pi_{2}(\\widetilde{z})\\in\\arg\\operatorname*{max}_{a}A_{2}^{\\pi^{\\star}}(x_{2}^{j},a)\\;\\forall j<i:x_{2}^{j}=\\widetilde{z}\\}\\big),\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "and choosing the final policy as $\\widehat{\\pi}=\\widehat{\\pi}^{i}$ for any iteration $i$ after $x_{2}=\\Xi$ is encountered. ", "page_idx": 56}, {"type": "text", "text": "Lower bound for offilne imitation learning. Consider the offilne imitation learning setting. When the underlying instance is $\\mathcal{T}_{S_{1},S_{1}}$ , we observe a dataset $\\mathcal{D}$ consisting of $n$ trajectories generated by executing $\\pi_{S_{1},S_{2}}^{\\star}$ in $M_{S_{1},S_{2}}$ . The trajectories never visit the state $\\breve{z}$ , so $S_{2}$ is not identifiable, and we can do no better than guessing uniformly in this state. Letting $\\mathbb{E}_{S_{1},S_{2}}$ denote the law of $\\mathcal{D}$ under instance $\\mathcal{T}_{S_{1},S_{2}}$ , we have $J_{S_{1},\\bar{S}_{2}}(\\widehat{\\pi})=\\widehat{\\pi}_{1}(S_{1}\\mid\\mathfrak{x})+\\widehat{\\pi}_{1}(S_{1}^{c}\\mid\\bar{\\mathfrak{x}})\\widehat{\\pi}_{2}(\\bar{S_{2}}\\mid\\mathfrak{z})$ . It follows that for any $(S_{1},S_{2})$ , since the law of $\\mathcal{D}$ does n ot de p end on $S_{2}$ , ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{S_{2}:\\vert S_{2}\\vert=N/2}{\\operatorname*{max}}\\mathbb{E}_{S_{1},S_{2}}\\left[J_{S_{1},S_{2}}(\\pi_{S_{1},S_{2}}^{\\star})-J_{S_{1},S_{2}}(\\widehat{\\pi})\\right]\\ge\\mathbb{E}_{S_{1}}[1-\\widehat{\\pi}_{1}(S_{1}\\mid\\varkappa)-\\widehat{\\pi}_{1}(S_{1}^{c}\\mid\\varkappa)/2]}\\\\ &{\\phantom{=}\\quad}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\frac{1}{2}\\mathbb{E}_{S_{1}}[1-\\widehat{\\pi}_{1}(S_{1}\\mid\\varkappa)],}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "with the convention that $\\mathbb{E}_{S_{1}}$ denotes the law of $\\mathcal{D}$ for an arbitrary choice of $S_{2}$ . If $\\widehat{\\pi}$ is proper in the sense that $\\widehat\\pi_{1}(\\cdot\\mathfrak{x})=\\operatorname{unif}(\\widehat{S_{1}})$ for some $\\widehat{S_{1}}\\subset[N]$ with $|\\widehat{S_{1}}|=N/2$ , we have $1-\\widehat{\\pi}_{1}(S_{1}\\mid\\mathfrak{x})=1-$ $\\frac{2}{N}|\\widehat{S_{1}}\\cup S_{1}|$ . We conclude that if $\\begin{array}{r}{\\mathbb{E}_{S_{1},S_{2}}\\left[J_{S_{1},S_{2}}(\\pi_{S_{1},S_{2}}^{\\star})-J_{S_{1},S_{2}}(\\widehat\\pi)\\right]\\le\\frac{1}{8}.}\\end{array}$ , then $\\mathbb{E}_{S_{1}},\\bigl[|\\widehat{S_{1}}\\cap S_{1}|\\bigr]\\geq$ $\\scriptstyle{\\frac{3}{8}}N$ . From here, it follows from standard lower bounds for discrete distribution estimation (e.g., Canonne [18]) that any such estimator $\\widehat S$ requires $n=\\Omega(N)$ samples for a worst-case choice of $S$ . ", "page_idx": 56}, {"type": "text", "text": "Lower bound for online imitation learning without value-based-feedback. Consider an online imitation learning algorithm that does not receive value-based feedback. We claim, via an argument similar to the one above, that if the algorithm that ensures ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{S_{1},S_{2}}\\left[J_{S_{1},S_{2}}(\\pi_{S_{1},S_{2}}^{\\star})-J_{S_{1},S_{2}}(\\widehat\\pi)\\right]\\leq c}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "on all instances for a sufficiently small absolute constant $c$ , then it can be used to produce estimators $\\widehat{S_{1}},\\widehat{S_{2}}\\subset[N]$ such that with constant probability, either $\\begin{array}{r}{\\left|\\widehat{S_{1}}\\cap S_{1}\\right|\\geq\\frac{3}{8}N}\\end{array}$ or $\\begin{array}{r}{\\left|\\widehat{S_{2}}\\cap S_{2}\\right|\\geq\\frac{3}{8}N}\\end{array}$ . From here, it should follow from standard arguments that this requires $n=\\Omega(N)$ samples for a worst-case choice of $S_{1}$ and $S_{2}$ . ", "page_idx": 56}, {"type": "text", "text": "I.5.3 Proof of Proposition I.3 ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Proof of Proposition I.3. We consider a slight variant of the construction from Theorem 2.2. Let $n$ and $H$ be given, and let $\\Delta\\in(0,1/3)$ be a parameter whose value will be chosen later. We first specify the dynamics for $M^{\\star}$ . Set $\\mathcal{X}=\\left\\{{\\mathfrak{x}},{\\mathfrak{y}},{\\mathfrak{z}}\\right\\}$ and $\\boldsymbol{\\mathcal{A}}=\\{\\boldsymbol{\\mathfrak{a}},\\mathfrak{b},\\boldsymbol{\\mathfrak{c}}\\}$ . The initial state distribution sets $P_{0}(\\pmb{\\xi})=1-\\Delta$ and $P_{0}(\\mathfrak{y})=\\Delta$ . The transition dynamics are: ", "page_idx": 56}, {"type": "text", "text": "\u2022 $P_{h}(x^{\\prime}=\\cdot\\mid x=\\mathfrak{x},a)=\\mathbb{I}_{\\mathfrak{x}}\\cdot\\mathbb{I}\\{a\\in\\{\\mathfrak{a},\\mathfrak{b}\\}\\}+\\mathbb{I}_{\\overline{{\\varepsilon}}}\\cdot\\mathbb{I}\\{a=\\mathfrak{c}\\}.$ \u2022 $P_{h}(x^{\\prime}\\mid x,a)=\\mathbb{I}\\{x^{\\prime}=x\\}{\\mathrm{~for~}}x\\in\\{\\mathfrak{y},\\mathfrak{z}\\}.$ ", "page_idx": 56}, {"type": "text", "text": "In other words, $\\mathfrak{y}$ and $\\breve{z}$ are terminal states. For state $\\pmb{\\xi}$ , actions $\\mathfrak{a}$ and $\\mathfrak{b}$ are self-loops, but action c transitions to $\\breve{z}$ . ", "page_idx": 56}, {"type": "text", "text": "The expert policies are $\\pi^{\\mathrm{a}}$ , which sets $\\pi_{h}^{\\mathfrak{a}}(x)=\\mathfrak{a}$ for all $h$ and $x\\in\\mathscr{X}$ , and $\\pi^{\\mathrm{{b}}}$ , which sets $\\pi_{h}^{\\mathfrak{h}}(\\mathfrak{x})=\\mathfrak{a}$ and sets $\\pi_{h}^{\\mathfrak{h}}(\\mathfrak{y})=\\pi_{h}^{\\mathfrak{h}}(\\mathfrak{z})=\\mathfrak{h}$ . We have $\\ddot{\\Pi}=\\{\\pi^{a},\\pi^{\\flat}\\}$ . ", "page_idx": 56}, {"type": "text", "text": "We consider two problem instances for the lower bound, $\\mathcal{T}^{a}=(M^{\\star},\\pi^{a},r^{a})$ , and $\\mathcal{T}^{\\flat}=(M^{\\star},\\pi^{\\flat},r^{\\flat})$ . For problem instance $\\mathcal{T}^{\\mathrm{a}}$ , the expert policy is $\\pi^{\\mathrm{a}}$ . We set $r_{h}^{\\mathfrak{a}}(\\mathfrak{x},\\cdot)=r_{h}^{\\mathfrak{a}}(\\mathfrak{z},\\cdot)=0$ , $r_{h}^{\\mathrm{a}}(\\mathfrak{y},a)=\\mathbb{I}\\{a=\\mathfrak{a}\\}$ for all $h$ . On the other hand, for problem instance $\\mathcal{T}^{\\mathrm{{b}}}$ , the expert policy is $\\pi^{\\mathrm{{b}}}$ . We set $r_{h}^{\\flat}({\\boldsymbol{\\mathfrak{x}}},\\cdot)=r_{h}^{\\flat}({\\boldsymbol{\\mathfrak{z}}},\\cdot)=0,$ , $r_{h}^{\\flat}({\\mathfrak{y}},a)=\\mathbb{I}\\{a={\\mathfrak{b}}\\}$ for all $h$ . Note that both of these choices for the reward function satisfy $\\mu=1$ , and that $\\pi^{\\mathrm{a}}$ and $\\pi^{\\mathrm{{b}}}$ are optimal policies for the respective instances. Let $J^{\\mathrm{{a}}}$ denote the expected reward function for instance $\\mathfrak{a}$ , and likewise for $\\mathfrak{b}$ . ", "page_idx": 56}, {"type": "text", "text": "Upper bound on online sample complexity. We consider the following online algorithm. For episodes $t=1,\\ldots$ ,: ", "page_idx": 56}, {"type": "text", "text": "\u2022 If $x_{1}\\neq\\mathfrak{x}$ , proceed to the next episode.   \n\u2022 If $x_{1}=\\mathfrak{X}$ , take action $\\mathfrak{C}$ , and observe $a_{2}=\\pi^{\\star}(\\Xi)$ . If $a_{2}=\\mathfrak{a}$ , return $\\widehat{\\pi}=\\pi^{a}$ , and if $a_{2}=\\mathfrak{h}$ , return $\\widehat{\\pi}=\\pi^{\\flat}$ . ", "page_idx": 56}, {"type": "text", "text": "For any $\\Delta\\leq e^{-1}$ , this algorithm will terminate after $\\log(1/\\delta)$ episodes with probability at least $1-\\delta$ , and whenever the algorithm terminates, it is clear that $\\widehat{\\pi}=\\pi^{\\star}$ . In particular, this leads to zero regret for any choice of reward function. ", "page_idx": 57}, {"type": "text", "text": "Lower bound on offilne sample complexity. By setting $\\Delta\\propto{\\textstyle{\\frac{1}{n}}}$ , an argument essentially identical to the proof of Theorem 2.2 shows that any offline imitation learning algorithm must have ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\{\\mathbb{E}^{{\\scriptscriptstyle a}}[J^{\\circ}(\\pi^{\\scriptscriptstyle a})-J^{\\circ}(\\widehat{\\pi})],\\mathbb{E}^{\\scriptscriptstyle b}[J^{\\flat}(\\pi^{\\flat})-J^{\\flat}(\\widehat{\\pi})]\\}\\gtrsim\\Delta H\\gtrsim\\frac{H}{n}.\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "For the sake of avoiding repetition, we omit the details. Finally, we observe that since neither policy in $\\Pi$ takes the action c, Dagger\u2014when equipped with any online learning algorithm that predicts from a mixture of policies in $\\Pi$ , such as in Proposition E.2)\u2014will never take the action c, and hence is subject to the $\\frac{H}{n}$ lower bound from Theorem 2.2 as well. ", "page_idx": 57}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 58}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 58}, {"type": "text", "text": "Justification: ", "page_idx": 58}, {"type": "text", "text": "Guidelines: ", "page_idx": 58}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 58}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 58}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 58}, {"type": "text", "text": "Justification: ", "page_idx": 58}, {"type": "text", "text": "Guidelines: ", "page_idx": 58}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. ", "page_idx": 58}, {"type": "text", "text": "\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 58}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 59}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "Justification: ", "page_idx": 59}, {"type": "text", "text": "Guidelines: ", "page_idx": 59}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 59}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 59}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 59}, {"type": "text", "text": "Justification: ", "page_idx": 59}, {"type": "text", "text": "Guidelines: ", "page_idx": 59}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 59}, {"type": "text", "text": "\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. ", "page_idx": 59}, {"type": "text", "text": "\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. ", "page_idx": 59}, {"type": "text", "text": "\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. ", "page_idx": 59}, {"type": "text", "text": "\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example ", "page_idx": 59}, {"type": "text", "text": "(a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.   \n(b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of ", "page_idx": 59}, {"type": "text", "text": "closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 60}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 60}, {"type": "text", "text": "Answer: [No]   \nJustification: This is a primarily theoretical work.   \nGuidelines:   \n\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 60}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 60}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 60}, {"type": "text", "text": "Justification: ", "page_idx": 60}, {"type": "text", "text": "Guidelines: ", "page_idx": 60}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 60}, {"type": "text", "text": "\u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 60}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 60}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 60}, {"type": "text", "text": "Justification: ", "page_idx": 60}, {"type": "text", "text": "Guidelines: ", "page_idx": 60}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 60}, {"type": "text", "text": "\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 61}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 61}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 61}, {"type": "text", "text": "Justification: ", "page_idx": 61}, {"type": "text", "text": "Guidelines: ", "page_idx": 61}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 61}, {"type": "text", "text": "\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. ", "page_idx": 61}, {"type": "text", "text": "\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. ", "page_idx": 61}, {"type": "text", "text": "\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 61}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 61}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 61}, {"type": "text", "text": "Justification: ", "page_idx": 61}, {"type": "text", "text": "Guidelines: ", "page_idx": 61}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. ", "page_idx": 61}, {"type": "text", "text": "\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. ", "page_idx": 61}, {"type": "text", "text": "\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 61}, {"type": "text", "text": "10. Broader Impacts ", "page_idx": 61}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 62}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 62}, {"type": "text", "text": "Justification: This is a primarily theoretical work. ", "page_idx": 62}, {"type": "text", "text": "Guidelines: ", "page_idx": 62}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 62}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 62}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. ", "page_idx": 62}, {"type": "text", "text": "\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 62}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. ", "page_idx": 62}, {"type": "text", "text": "\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 62}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 62}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 62}, {"type": "text", "text": "Justification: ", "page_idx": 62}, {"type": "text", "text": "Guidelines: ", "page_idx": 62}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 62}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. ", "page_idx": 62}, {"type": "text", "text": "\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. ", "page_idx": 62}, {"type": "text", "text": "\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 62}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 62}, {"type": "text", "text": "Answer: [Yes] Justification: ", "page_idx": 62}, {"type": "text", "text": "Guidelines: ", "page_idx": 63}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 63}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 63}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 63}, {"type": "text", "text": "Justification: ", "page_idx": 63}, {"type": "text", "text": "Guidelines: ", "page_idx": 63}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets. ", "page_idx": 63}, {"type": "text", "text": "\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. ", "page_idx": 63}, {"type": "text", "text": "\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used. ", "page_idx": 63}, {"type": "text", "text": "\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 63}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 63}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 63}, {"type": "text", "text": "Justification: ", "page_idx": 63}, {"type": "text", "text": "Guidelines: ", "page_idx": 63}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 63}, {"type": "text", "text": "\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. ", "page_idx": 63}, {"type": "text", "text": "\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 63}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 63}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 64}, {"type": "text", "text": "Answer: [NA]   \nJustification:   \nGuidelines:   \n\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 64}]