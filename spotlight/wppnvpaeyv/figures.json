[{"figure_path": "WpPNVPAEyv/figures/figures_2_1.jpg", "caption": "Figure 1: Illustration of our method: (a) Existing methods train for a specific long-tailed distribution but may fail on arbitrarily skewed test distributions. (b) Multi-expert learns different experts for different distributions from one training set but lacks flexibility for arbitrary distributions/preferences. (c) Our method samples preference vectors during training to simulate distributions, and can flexibly adjust the preference vector during testing for flexible long-tailed classification.", "description": "This figure compares three different approaches to long-tailed learning. (a) shows the traditional approach where a single model is trained on the imbalanced dataset, often resulting in poor performance on tail classes during testing with different distributions. (b) illustrates a multi-expert approach where multiple experts are trained on different distributions from the same dataset, but still lacks flexibility to adapt to arbitrary test-time distributions. (c) presents the proposed method which samples diverse preference vectors during training to generate a diverse set of expert models via hypernetworks to address any possible distribution scenarios. The ensemble model is optimized to flexibly adapt to any test distribution and output a dedicated model solution according to the user's preference.", "section": "Methodology"}, {"figure_path": "WpPNVPAEyv/figures/figures_5_1.jpg", "caption": "Figure 2: Mapping from preference to model properties.", "description": "This figure visualizes the relationship between user preferences and model performance. The three-dimensional coordinate system represents the performance on the forward50, uniform, and backward50 splits of the CIFAR100-LT dataset. The dark plane represents the performance on the three distributions for different preference vectors. Yellow dots represent the results of running SADE, while purple dots show our method's results. The figure shows that our method covers various distributions with a single training and outperforms SADE by satisfying different user preferences.", "section": "4.4 Preference-Controlled Trade-off"}, {"figure_path": "WpPNVPAEyv/figures/figures_6_1.jpg", "caption": "Figure 1: Illustration of our method: (a) Existing methods train for a specific long-tailed distribution but may fail on arbitrarily skewed test distributions. (b) Multi-expert learns different experts for different distributions from one training set but lacks flexibility for arbitrary distributions/preferences. (c) Our method samples preference vectors during training to simulate distributions, and can flexibly adjust the preference vector during testing for flexible long-tailed classification.", "description": "This figure compares three different approaches to long-tailed learning. (a) shows the traditional approach, where a single model is trained on a specific long-tailed distribution. (b) shows a multi-expert approach, where multiple experts are trained on different distributions. This approach is limited as the distribution of the data is pre-defined and cannot be changed. (c) shows the proposed method, which uses hypernetworks to generate diverse expert models that can adapt to any test distribution. The method allows for flexible adjustment of the head-tail trade-off according to user preferences. This approach is more flexible and robust, as it can handle various distribution scenarios and satisfy the user's requirements for performance on head and tail classes.", "section": "Methodology"}, {"figure_path": "WpPNVPAEyv/figures/figures_7_1.jpg", "caption": "Figure 2: Mapping from preference to model properties.", "description": "This figure visualizes the relationship between user preferences and model performance. The three-dimensional coordinate system represents the performance on forward50, uniform, and backward50 splits of the CIFAR100-LT dataset.  The dark plane shows the performance with no preference input. The red points indicate preference vectors that improve performance on the many-shot classes. The green points represent preference vectors that hurt performance on the many-shot classes. The figure demonstrates the flexibility of the proposed method in controlling performance trade-offs based on user preferences.", "section": "4.4 Preference-Controlled Trade-off"}, {"figure_path": "WpPNVPAEyv/figures/figures_8_1.jpg", "caption": "Figure 5: Ablation analysis, including the ablation of the hypernetwork and Chebyshev polynomials.", "description": "This figure presents the results of ablation studies conducted on the CIFAR100-LT dataset (IR100) to evaluate the impact of removing the hypernetwork (w.o. hnet) and the Chebyshev polynomial (w.o. stch) on model performance under various unknown test class distributions.  The x-axis shows different test distributions, ranging from those heavily biased toward many-shot classes (F-50) to those biased toward few-shot classes (B-50), with a uniform distribution in the middle. The y-axis represents the Top-1 accuracy achieved.  The four colored bars for each distribution show the performance of the full model ('ours'), the model without the stochastic convex ensemble ('ours w.o. stch'), the model without the hypernetwork ('ours w.o. hnet'), and the SADE baseline.", "section": "5.2 Comparative Evaluation on Standard and Test-Agnostic Long-Tailed Recognition"}]