[{"figure_path": "nY0BrZdqLt/tables/tables_3_1.jpg", "caption": "Table 1: Description of different TRLM model variants.", "description": "This table describes four variations of the Time Reversed Language Model (TRLM): TRLM-Ba, TRLM-Fo, TRLM-FoBa (Reverse), and TRLM-FoBa (Forward).  Each model variant is differentiated by its pre-training method (reverse token order, forward token order, or both) and how it scores and generates queries given responses. The table details the scoring and generation processes for each variant, highlighting their unique characteristics and behaviors in relation to forward language models.", "section": "3 TRLM - Time Reversed Language Models"}, {"figure_path": "nY0BrZdqLt/tables/tables_6_1.jpg", "caption": "Table 2: The best re-ranked response is compared with a single response of GPT4-1106-Preview. The setting is identical to the AlpacaEval Alp Leader board. TRLM-Fo, that scores in the backward direction, fares better than the conventional forward baseline. Scoring using TRLM-Ba (pretrained in reverse) gets even a higher (LC) win rate.", "description": "This table presents the results of the Alpaca Leaderboard evaluation. It compares the performance of different models, including various TRLM variants and baselines, in a best-of-N reranking task. The models' performance is evaluated based on win rates, using length-controlled win rates to account for the length bias that is otherwise preferred by GPT4-1106-Preview. The table shows that TRLM models, especially TRLM-Ba, outperform the baselines, demonstrating the effectiveness of time-reversed scoring for improving the quality of LLM generations.", "section": "5.1.1 Alpaca Leaderboard Evaluation"}, {"figure_path": "nY0BrZdqLt/tables/tables_6_2.jpg", "caption": "Table 3: Tabulates the citation Attribution results through Re-ranking on the CNN-Daily Mail dataset. A denotes article whereas S denotes the corresponding summary. The ease of scoring a summary given the article instead of reverse is clearly highlighted in all of the search methods.", "description": "This table presents the results of citation attribution experiments using different methods and scoring directions on the CNN Daily Mail dataset.  The goal is to identify which sentence(s) in a news article best support a given highlight summary.  The table compares the performance of various models (TRLM-Ba, TRLM-FoBa, TRLM-Fo, Forward Baseline, Backward Baseline) using different search algorithms (Linear Search, Binary Search, Exclusion Search) and evaluation metrics (Gecko cosine similarity, TF-IDF cosine similarity, ROUGE). The results demonstrate the effectiveness of TRLM-based reverse scoring in improving citation attribution accuracy.", "section": "5.2 Citation Attribution"}, {"figure_path": "nY0BrZdqLt/tables/tables_7_1.jpg", "caption": "Table 2: The best re-ranked response is compared with a single response of GPT4-1106-Preview. The setting is identical to the AlpacaEval Alp Leader board. TRLM-Fo, that scores in the backward direction, fares better than the conventional forward baseline. Scoring using TRLM-Ba (pretrained in reverse) gets even a higher (LC) win rate.", "description": "This table compares the performance of different models on the AlpacaEval Leaderboard, a benchmark for evaluating the quality of language models. The models used are TRLM-Ba, TRLM-Fo, TRLM-FoBa (forward), TRLM-FoBa (backward), One Generation, Self, and Forward Baseline.  The table shows the win rate, a measure of how often the model's response is better than a baseline response, along with standard, length-controlled, and discrete win rates.  The results demonstrate that TRLMs, particularly TRLM-Ba, significantly improve the performance of the base model compared to the conventional forward baseline, highlighting the benefits of scoring in the reverse direction.", "section": "5.1.1 Alpaca Leaderboard Evaluation"}, {"figure_path": "nY0BrZdqLt/tables/tables_7_2.jpg", "caption": "Table 5: Tabulates the results of various reranking algorithms with two inference directions. Q denotes Queries, while D denotes Documents. TRLM outperforms Forward Baseline and Backward Baseline significantly, which highlights the importance of inference direction in this task.", "description": "This table presents the performance of different reranking algorithms on two information retrieval datasets, MS-MARCO and NF-CORPUS.  The algorithms are categorized by inference direction (query to document or document to query) and model type (TRLM variants, Forward Baseline, Backward Baseline).  The results show precision, recall, and NDCG@k metrics for different values of k, demonstrating that TRLM models, especially when using a document-to-query approach, significantly outperform baselines. This highlights the importance of inference direction in these tasks.", "section": "5 Experimental Results"}, {"figure_path": "nY0BrZdqLt/tables/tables_8_1.jpg", "caption": "Table 6: Performance of the proposed defense strategies across different thresholds, evaluated on the human annotated and jailbreakbench toxic responses. TRLM-Ba achieves significant gains over all other approaches. Notations: PT [Pretrained], IT[Instruction-finetuned], FNR[False Negative Rate], FPR[False Positive Rate], new-HA [new HA Dataset], JBB[JBB Dataset], (H) [Hard], (E) [Easy]", "description": "This table presents the performance of different defense strategies against jailbreak attacks on various datasets. The strategies use different variants of the TRLM model (pre-trained and instruction-tuned), combined with an input toxicity filter. The table shows the False Negative Rate (FNR) and False Positive Rate (FPR) at different thresholds, indicating the effectiveness of the defense strategies in reducing toxic outputs while maintaining a low false positive rate.  The results demonstrate that TRLM-Ba, particularly the instruction-tuned variant, significantly outperforms other methods.", "section": "5.4 Defending against Jailbreak attacks"}, {"figure_path": "nY0BrZdqLt/tables/tables_15_1.jpg", "caption": "Table 2: The best re-ranked response is compared with a single response of GPT4-1106-Preview. The setting is identical to the AlpacaEval Alp Leader board. TRLM-Fo, that scores in the backward direction, fares better than the conventional forward baseline. Scoring using TRLM-Ba (pretrained in reverse) gets even a higher (LC) win rate.", "description": "This table presents the results of comparing different models' performance on the AlpacaEval Leaderboard.  It contrasts the win rates of various TRLM models (scoring in reverse) against a standard forward baseline and a self-scoring baseline. The table highlights that time-reversed scoring methods, particularly TRLM-Ba, achieve significantly higher win rates, demonstrating the effectiveness of time reversal for unsupervised feedback in LLMs.", "section": "5.1 Alpaca Leaderboard Evaluation"}, {"figure_path": "nY0BrZdqLt/tables/tables_15_2.jpg", "caption": "Table 2: The best re-ranked response is compared with a single response of GPT4-1106-Preview. The setting is identical to the AlpacaEval Alp Leader board. TRLM-Fo, that scores in the backward direction, fares better than the conventional forward baseline. Scoring using TRLM-Ba (pretrained in reverse) gets even a higher (LC) win rate.", "description": "This table compares the performance of different models on the AlpacaEval leaderboard, a benchmark for evaluating language models.  The models use different scoring and inference methods, including time-reversed language models (TRLM).  The table shows that TRLM models, particularly TRLM-Ba, achieve higher win rates compared to a forward baseline, demonstrating the effectiveness of using time-reversed scoring for reranking responses.", "section": "5.1.1 Alpaca Leaderboard Evaluation"}, {"figure_path": "nY0BrZdqLt/tables/tables_16_1.jpg", "caption": "Table 7: Per-Task Scoring and Conditioning Prompts", "description": "This table shows the scoring prompts and conditioning prompts used for different tasks in the experiments.  The prompts are tailored to each task (Best-of-N reranking, citation attribution, and passage retrieval) and to the direction of the language model (forward or backward). This table is essential for understanding how the model is prompted to score the different responses and generate different queries for each task.", "section": "5.1 Best-of-N reranking"}, {"figure_path": "nY0BrZdqLt/tables/tables_17_1.jpg", "caption": "Table 8: Mixtral 8x7B generations with TRLM/Forward reranking against Mixtral 8x22B reference as rated by a GPT4-1106-Preview annotator", "description": "This table presents the results of the Alpaca Leaderboard evaluation using Mixtral 8x7B model with different reranking methods including TRLM variants, Self and Forward Baselines. The evaluation is performed against a Mixtral 8x22B reference model, and the results are assessed by a GPT4-1106-Preview annotator.  Metrics shown include win rate (LC, Reg, Discrete), standard error, wins, losses and ties.", "section": "5.1.1 Alpaca Leaderboard Evaluation"}, {"figure_path": "nY0BrZdqLt/tables/tables_17_2.jpg", "caption": "Table 9: Mixtral 8x22B generations with TRLM/Forward reranking against GPT4-1106-Preview reference as rated by a GPT4-1106-Preview annotator", "description": "This table presents the results of an AlpacaEval leaderboard experiment using the Mixtral 8x22B language model.  Different reranking methods are compared: TRLM-Ba, TRLM-FoBa (Reverse), TRLM-FoBa (Forward), TRLM-Fo, Forward Baseline, and Self.  The table shows win rates (LC, Reg, Discrete), standard errors, and the counts of wins, losses, and ties for each method.  The goal is to evaluate the effectiveness of different TRLM variants in improving the model's responses compared to a strong baseline.", "section": "5.1.1 Alpaca Leaderboard Evaluation"}, {"figure_path": "nY0BrZdqLt/tables/tables_18_1.jpg", "caption": "Table 2: The best re-ranked response is compared with a single response of GPT4-1106-Preview. The setting is identical to the AlpacaEval Alp Leader board. TRLM-Fo, that scores in the backward direction, fares better than the conventional forward baseline. Scoring using TRLM-Ba (pretrained in reverse) gets even a higher (LC) win rate.", "description": "This table presents the results of comparing different models' performance on the AlpacaEval leaderboard. The models are evaluated based on their win rate, a metric representing the percentage of times the model generates a better response than a baseline model. The table showcases the improvement achieved by using Time-Reversed Language Models (TRLMs) for scoring in the reverse direction (Response \u2192 Query) compared to the conventional forward scoring (Query \u2192 Response). Specifically, it demonstrates that TRLM-Fo (which scores in reverse but uses a forward-trained model) outperforms the forward baseline, and TRLM-Ba (pre-trained in reverse) achieves even higher win rates.", "section": "5.1.1 Alpaca Leaderboard Evaluation"}, {"figure_path": "nY0BrZdqLt/tables/tables_19_1.jpg", "caption": "Table 2: The best re-ranked response is compared with a single response of GPT4-1106-Preview. The setting is identical to the AlpacaEval Alp Leader board. TRLM-Fo, that scores in the backward direction, fares better than the conventional forward baseline. Scoring using TRLM-Ba (pretrained in reverse) gets even a higher (LC) win rate.", "description": "This table presents the results of comparing different models' performance on the AlpacaEval leaderboard.  It shows the win rates (standard and length-controlled) achieved by several models, including various TRLM configurations (TRLM-Ba, TRLM-Fo, TRLM-FoBa), a self-scoring baseline, and a forward baseline. The table highlights the improvement in win rates obtained by using TRLMs for scoring in the reverse direction (response->query) compared to conventional forward scoring (query->response).  The length-controlled win rate metric is particularly emphasized, indicating that TRLMs are effective even when accounting for length bias. ", "section": "5.1.1 Alpaca Leaderboard Evaluation"}, {"figure_path": "nY0BrZdqLt/tables/tables_19_2.jpg", "caption": "Table 2: The best re-ranked response is compared with a single response of GPT4-1106-Preview. The setting is identical to the AlpacaEval Alp Leader board. TRLM-Fo, that scores in the backward direction, fares better than the conventional forward baseline. Scoring using TRLM-Ba (pretrained in reverse) gets even a higher (LC) win rate.", "description": "This table presents the results of reranking experiments on the AlpacaEval leaderboard. It compares the performance of different models, including TRLM-Ba, TRLM-Fo, TRLM-FoBa (forward and backward), Self, and Forward Baseline, in terms of win rate (standard and length-controlled) when using various scoring methods.  The results show that TRLM-Ba and TRLM-Fo, which score in the reverse direction (response to query), achieve higher win rates compared to the forward baseline, demonstrating the effectiveness of reverse scoring in improving LLM generations.", "section": "5.1.1 Alpaca Leaderboard Evaluation"}, {"figure_path": "nY0BrZdqLt/tables/tables_21_1.jpg", "caption": "Table 10: Comparison of various Input+Output Filter combinations on Human Annotated dataset on JailbreakBench. For the filter based on GPT-3.5 (version gpt-3.5-turbo-1106), we use the prompt from Llama-Guard [Inan et al., 2023]", "description": "This table compares the performance of different input and output filter combinations on a human-annotated dataset from JailbreakBench.  It shows the agreement rate, false positive rate, and false negative rate for each method. The GPT-3.5 input filter is a baseline, while the GPT-4 input+output filter represents a combined approach.  The numbers reflect the accuracy and error rates of the filter combinations in identifying toxic content.", "section": "F Details on our Defence Task: Defending against Jailbreak attacks"}]