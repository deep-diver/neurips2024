[{"figure_path": "zeYyq0GpXO/figures/figures_3_1.jpg", "caption": "Figure 1: PCA visualization of positional vectors from the 1-st and 7-th layers.", "description": "This figure uses principal component analysis (PCA) to visualize the positional vectors extracted from the first and seventh layers of two Transformer models: one without positional encodings (TL-NoPE) and one with rotary position embeddings (TL-ROPE).  The plots show how positional information is distributed across different positions in the sequence. In the first layer, initial tokens show distinctly different positional vectors, while later tokens have similar vectors. This highlights the role of the initial tokens in establishing positional information within the sequence. By the seventh layer, the positional vectors become more evenly distributed across all positions, indicating that the network has learned to represent positional information more comprehensively.", "section": "3.2 Formation and Effect of Positional Vectors within Context Window"}, {"figure_path": "zeYyq0GpXO/figures/figures_3_2.jpg", "caption": "Figure 2: Comparison of distinct positional vectors and theoretical receptive field.", "description": "This figure compares the number of distinct positional vectors observed in a Transformer model with windowed attention against the theoretical receptive field (TRF).  The TRF represents the maximum number of tokens a single token can theoretically attend to given the window size and number of layers.  The plot shows that while the TRF grows linearly with the layer number, the number of distinct positional vectors also increases but more gradually.  This suggests that even though the theoretical receptive field allows for larger context, the model does not fully utilize that capacity to capture distinct positional information at every layer. The difference likely reflects the effect of the attention mechanism's ability to focus the attention on relevant tokens across layers, leading to a less than maximal use of the theoretically available receptive field.", "section": "3.2 Formation and Effect of Positional Vectors within Context Window"}, {"figure_path": "zeYyq0GpXO/figures/figures_4_1.jpg", "caption": "Figure 3: Logarithmic attention maps of TL-ROPE, and TL-NOPE.", "description": "This figure visualizes the logarithmic attention maps for TL-ROPE and TL-NOPE models.  It shows the attention weights across different positions for different scenarios: the original models and variants where semantic vectors, positional vectors, or positional bases have been removed.  The heatmaps illustrate the impact of these different components on the overall attention distribution, particularly highlighting the formation of attention sinks and long-term decay properties. By comparing the original models to the modified versions, the figure demonstrates the critical role positional vectors and bases play in modulating attention scores and shaping the characteristic long-term decay patterns in LLMs.", "section": "3.2 Formation and Effect of Positional Vectors within Context Window"}, {"figure_path": "zeYyq0GpXO/figures/figures_5_1.jpg", "caption": "Figure 4: Left: The average PPL across positions during direct extrapolation. Right: The maximum cosine similarity between positional vectors within and beyond context window during extrapolation.", "description": "This figure shows the results of direct extrapolation experiments. The left panel displays the average perplexity (PPL) across different positions in sequences that exceed the context window.  The right panel shows the maximum cosine similarity between positional vectors within and beyond the context window.  The figure demonstrates that models with stable PPL scores also maintain high cosine similarity between positional vectors, both inside and outside the context window, highlighting the importance of positional vector consistency for successful length extrapolation.", "section": "3.3 Direct Extrapolation"}, {"figure_path": "zeYyq0GpXO/figures/figures_6_1.jpg", "caption": "Figure 5: Left: Attention map of TL-NoPE. Middle: Attention Scores between initial token and others in TL-NoPE. Right: Similarity of logits of positional vectors across positions in TL-NOPE.", "description": "This figure visualizes the attention mechanism and the impact of out-of-distribution (OOD) positional vectors in a decoder-only Transformer without positional encodings (TL-NoPE). The left panel shows the logarithmic attention map, illustrating the distribution of attention weights across different token positions. The middle panel plots the attention scores specifically on the initial token, highlighting its role as an 'attention sink'. The right panel displays the similarity of logits (output of the linear projection layer) across different positions, indicating the impact of OOD positional vectors on the model's prediction probability distribution.", "section": "3.3 Effect of Positional Vectors beyond Context Window"}, {"figure_path": "zeYyq0GpXO/figures/figures_6_2.jpg", "caption": "Figure 6: The average cosine similarity between the scaled and original positional vectors.", "description": "This figure displays two heatmaps visualizing the average cosine similarity between positional vectors before and after applying two different context window extension methods. The left heatmap shows the results for TL-ROPE with Dynamic NTK, and the right heatmap shows the results for TL-NoPE with Attention Scaling. Each heatmap compares the original positional vectors (y-axis) with the scaled positional vectors (x-axis) produced by the respective method. The color intensity represents the cosine similarity, with warmer colors indicating higher similarity and cooler colors indicating lower similarity. This figure helps in understanding the effectiveness of the context window extension methods by showing how well the new positional vectors generated by interpolation approximate the original ones.", "section": "3.3.2 Context Window Extension"}, {"figure_path": "zeYyq0GpXO/figures/figures_14_1.jpg", "caption": "Figure 7: The values of first elements of the output of single head attention due to attention preferences in Transformers with NoPE and ROPE.", "description": "The figure shows two line graphs, one for Transformers with NoPE and one for Transformers with RoPE.  The y-axis represents the \"First element of each hidden state after attention\", indicating the value of the first element of the hidden state vector after the attention mechanism has been applied. The x-axis represents the \"position id\", which corresponds to the position of the token within the input sequence.  Both graphs show a similar trend: the first element's value increases rapidly at the beginning of the sequence and then plateaus.  This demonstrates that the initial tokens in a sequence have a greater effect on the attention mechanism, as they are assigned greater attention weights, particularly evident in the NoPE model. This difference is likely due to the way positional information is handled in the models, with ROPE using rotary positional embeddings that might mitigate this initial token bias.", "section": "3.2 Formation and Effect of Positional Vectors within Context Window"}, {"figure_path": "zeYyq0GpXO/figures/figures_15_1.jpg", "caption": "Figure 8: Changes of PPL with replacement at different layers.", "description": "This figure shows the performance (logarithmic PPL score) of the positional vector replacement strategy applied at different layers of the TL-NoPE model. Two different interpolation ratios and expansion factors, (r=4, \u03b1=1) and (r=5, \u03b1=1.3), were used. The results show that replacing positional vectors in the 4th layer results in the lowest PPL, indicating this is the optimal layer for this operation.  The graph also includes baselines representing the original performance with 2K and 8K tokens.", "section": "D Analysis of Positional Vector Replacement"}, {"figure_path": "zeYyq0GpXO/figures/figures_15_2.jpg", "caption": "Figure 9: Effective scaling factors of positional vectors at each layer.", "description": "This figure shows how the effective interpolation ratio of positional vectors changes across different layers in a Transformer model.  The effective interpolation ratio is a measure of how well the positional vectors are interpolated when extending the context window.  The graph plots this ratio against the layer number for four different settings, each characterized by different interpolation ratios (r) and scaling factors (\u03b1). The figure demonstrates that as the layer number increases, the effective interpolation ratio decreases.  This decline is mitigated when the scaling factor (\u03b1) is increased, suggesting that a higher scaling factor improves the quality of interpolation.", "section": "D Analysis of Positional Vector Replacement"}, {"figure_path": "zeYyq0GpXO/figures/figures_17_1.jpg", "caption": "Figure 10: PCA visualization of positional vectors from the 1-st layer of Llama-3-8B, Qwen1.5-7B, Yi-9B, and TL-NoPE-new.", "description": "This figure shows the results of Principal Component Analysis (PCA) applied to the positional vectors extracted from the first layer of four different large language models (LLMs): Llama-3-8B, Qwen1.5-7B, Yi-9B, and a newly trained model TL-NoPE.  The PCA reduces the dimensionality of the positional vectors, allowing for visualization in a 2D space.  The plot helps to illustrate how the positional information is represented differently across these various models in the initial layer of the Transformer architecture.", "section": "F.1 Formation of Positional Vectors within Context Window"}, {"figure_path": "zeYyq0GpXO/figures/figures_18_1.jpg", "caption": "Figure 1: PCA visualization of positional vectors from the 1-st and 7-th layers.", "description": "This figure uses principal component analysis (PCA) to visualize the positional vectors extracted from the hidden states of a Transformer model.  The left column shows the positional vectors from the first layer, while the right column displays those from the seventh layer. The visualization helps to understand how positional information is represented and evolves across different layers of the model, showing a clear shift from less distinct positional vectors in the first layer to more distinct vectors in the deeper layer.", "section": "3.2 Formation and Effect of Positional Vectors within Context Window"}, {"figure_path": "zeYyq0GpXO/figures/figures_19_1.jpg", "caption": "Figure 12: PCA visualization of positional vectors at different layers of TL-NoPE.", "description": "This figure shows the results of Principal Component Analysis (PCA) applied to positional vectors extracted from different layers (1-21) of the TL-NoPE model.  Each subplot represents a different layer, visualizing the distribution of the positional vectors in a 2D space.  The color coding likely indicates the position of the token within the sequence.  The patterns reveal how the positional information evolves as the model processes the input sequence through its multiple layers.  The initial layers might show a more scattered or less structured pattern, while deeper layers often exhibit a more organized and distinct representation of positional information.", "section": "3.2 Formation and Effect of Positional Vectors within Context Window"}, {"figure_path": "zeYyq0GpXO/figures/figures_20_1.jpg", "caption": "Figure 1: PCA visualization of positional vectors from the 1-st and 7-th layers.", "description": "This figure shows the results of Principal Component Analysis (PCA) applied to positional vectors extracted from the first and seventh layers of a Transformer model.  The left column displays the PCA for the first layer, and the right column displays the PCA for the seventh layer. Each plot shows how positional information is distributed across different positions within a sequence. In the first layer, initial tokens (the first few tokens in a sequence) exhibit significantly distinct positional vectors. By layer seven, the positional vectors of all tokens are evenly distributed, indicating a more sophisticated encoding of positional information across the sequence.  The difference highlights how positional information evolves as the information passes through more layers of the Transformer model.", "section": "3.2 Formation and Effect of Positional Vectors within Context Window"}, {"figure_path": "zeYyq0GpXO/figures/figures_20_2.jpg", "caption": "Figure 1: PCA visualization of positional vectors from the 1-st and 7-th layers.", "description": "This figure visualizes positional vectors obtained through Principal Component Analysis (PCA) from the first and seventh layers of a Transformer model.  The left column shows the PCA results for the first layer, illustrating how initial tokens have significantly distinct positional vectors, while subsequent tokens exhibit similar vectors. The right column displays the PCA results for the seventh layer, demonstrating that after several layers, positional vectors become more evenly distributed across all positions. This visual representation helps to understand the formation and distribution of positional information within a Transformer model and how it evolves across different layers.", "section": "3.2 Formation and Effect of Positional Vectors within Context Window"}, {"figure_path": "zeYyq0GpXO/figures/figures_20_3.jpg", "caption": "Figure 1: PCA visualization of positional vectors from the 1-st and 7-th layers.", "description": "This figure uses Principal Component Analysis (PCA) to visualize the positional vectors extracted from the first and seventh layers of a Transformer model.  The left column shows the PCA visualization of the positional vectors from the first layer, demonstrating that the initial tokens show significantly distinct positional vectors, while subsequent tokens' vectors are more similar. The right column shows the PCA visualization of positional vectors from the seventh layer, indicating that positional vectors are more evenly distributed across all positions in later layers. This visual comparison helps illustrate how positional information is distributed and changes over the layers of the model.  This visualization supports the paper's claim that after the first layer, initial tokens form distinct positional vectors, serving as anchors to shape positional vectors in later tokens.", "section": "3.2 Formation and Effect of Positional Vectors within Context Window"}, {"figure_path": "zeYyq0GpXO/figures/figures_21_1.jpg", "caption": "Figure 10: PCA visualization of positional vectors from the 1-st layer of Llama-3-8B, Qwen1.5-7B, Yi-9B, and TL-NoPE-new.", "description": "This figure displays the Principal Component Analysis (PCA) of positional vectors extracted from the first layer of four different large language models (LLMs): Llama-3-8B, Qwen1.5-7B, Yi-9B, and a newly trained model TL-NoPE-new.  PCA is used to reduce the dimensionality of the positional vectors and visualize them in a 2D space.  The plot helps to understand how positional information is represented differently across these models in the initial layers, providing insights into the formation and distribution of positional information.", "section": "F.1 Formation of Positional Vectors within Context Window"}, {"figure_path": "zeYyq0GpXO/figures/figures_21_2.jpg", "caption": "Figure 1: PCA visualization of positional vectors from the 1-st and 7-th layers.", "description": "This figure uses Principal Component Analysis (PCA) to visualize the positional vectors extracted from the hidden states of a Transformer model at two different layers: the first and the seventh.  The visualizations show how the distribution of positional vectors changes as the model processes the input sequence. In the first layer, the initial tokens exhibit distinct positional vectors, while in the seventh layer, the vectors are more evenly distributed across all positions. This demonstrates how positional information is implicitly learned and encoded within the model's hidden states during the processing of the input.", "section": "3.2 Formation and Effect of Positional Vectors within Context Window"}]