[{"figure_path": "iW0wXE0VyR/tables/tables_8_1.jpg", "caption": "Table 1: Perplexity for an LSTM Language Model using the Penn TreeBank dataset. The numbers on None and KN Noising are from Xie et al. (2017) and can be replicated using their original code (we use the model with latent dimension 1500). Like the baseline, for each row, we report the best value across as many restarts.", "description": "This table shows the perplexity results for an LSTM language model trained on the Penn TreeBank dataset using three different methods:  None (only regular dropout), KN Noising (reproducible), and IMM with KN Bigram.  The perplexity is reported for both validation and test sets.  The results for None and KN Noising are taken from Xie et al. (2017) and are shown here for comparison. IMM, or Induced Model Matching, is a novel method introduced in the current paper.", "section": "7.2 Language Modeling Experiments"}, {"figure_path": "iW0wXE0VyR/tables/tables_8_2.jpg", "caption": "Table 2: Results on the BERTBASE Language Model. The baseline numbers can be replicated using the original BERT code by Google, as well as our provided repository. Matthew's Correlation Coefficient is used for COLA, F1 score for MRPC and Accuracy for QNLI and RTE. Like the baseline, reported numbers are averages across multiple restarts.", "description": "This table shows the results of experiments using the BERTBASE language model on several GLUE tasks.  Three configurations are compared: the baseline BERTBASE model, the model with the Masked Language Model (MLM) objective added, and the model with the Induced Model Matching (IMM) method added in addition to MLM.  The metrics used vary depending on the specific task (Matthew's Correlation Coefficient for COLA, F1 score for MRPC, and accuracy for QNLI and RTE). All results are averages across multiple restarts.", "section": "7.2 Language Modeling Experiments"}, {"figure_path": "iW0wXE0VyR/tables/tables_12_1.jpg", "caption": "Table 1: Perplexity for an LSTM Language Model using the Penn TreeBank dataset. The numbers on None and KN Noising are from Xie et al. (2017) and can be replicated using their original code (we use the model with latent dimension 1500). Like the baseline, for each row, we report the best value across as many restarts.", "description": "This table shows the perplexity results for an LSTM language model trained on the Penn TreeBank dataset using different methods: no regularization, KN noising (a data augmentation technique), and IMM with KN bigrams.  The table compares the validation and test perplexity scores achieved by each method and highlights the improvement achieved by IMM over other methods.  Lower perplexity indicates better performance.", "section": "7.2 Language Modeling Experiments"}, {"figure_path": "iW0wXE0VyR/tables/tables_15_1.jpg", "caption": "Table 4: Deterioration of noising vs. consistency of IMM when \u03bb = 1.5 is fixed in the logistic regression example.", "description": "This table presents the performance comparison between the baseline, noising, and IMM methods across different dataset sizes in a logistic regression experiment where the regularization parameter \u03bb is fixed at 1.5.  The \"IMM-Noising Gap\" column shows the difference in performance between IMM and noising, highlighting IMM's improvement.  The results demonstrate IMM's consistent superior performance compared to noising, even as the dataset size increases.", "section": "7.1 Starting Toy Example: Logistic Regression"}, {"figure_path": "iW0wXE0VyR/tables/tables_23_1.jpg", "caption": "Table 5: Restricted vs. Full Model on the Bigram Task (i.e., predicting the next word given only the previous one.) The small model is the Kneser-Ney bigram on the PTB training set. The full model is the 1,500-dimensional 2-layer LSTM from Xie et al. (2017), trained without noising on the PTB training set. To make the LSTM predict the next word based only on the previous one, we complete the history by averaging predictions over all extended contexts in the training set (full word history) that share that short context (same previous word). In the language of the paper, this is exactly the induced bigram of the learned LSTM. We report the cross-entropy and perplexity of each model on the PTB test set. Note that the small model clearly outperforms the LSTM on this restricted task.", "description": "This table compares the performance of a Kneser-Ney bigram model and an LSTM model (with and without IMM) on a bigram prediction task using the Penn TreeBank dataset.  It demonstrates that a simple bigram model outperforms the LSTM on this restricted task, highlighting the potential benefits of incorporating restricted model knowledge when training full-featured models. The table shows that IMM improves the LSTM's performance on this task.", "section": "7.2 Language Modeling Experiments"}]