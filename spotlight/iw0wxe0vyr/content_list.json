[{"type": "text", "text": "Induced Model Matching: Restricted Models Help Train Full-Featured Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Usama Muneeb Electrical and Computer Engineering University of Illinois Chicago umunee2@uic.edu ", "page_idx": 0}, {"type": "text", "text": "Mesrob I. Ohannessian Electrical and Computer Engineering University of Illinois Chicago mesrob@uic.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We consider scenarios where a very accurate (often small) predictive model using restricted features is available when training a full-featured (often larger) model. This restricted model may be thought of as \u201cside-information\u201d, and can come either from an auxiliary dataset or from the same dataset by forcing the restriction. How can the restricted model be useful to the full model? To answer this, we introduce a methodology called Induced Model Matching (IMM). IMM aligns the context-restricted, or induced, version of the large model with the restricted model. We relate IMM to approaches such as noising, which is implicit in addressing the problem, and reverse knowledge distillation from weak teachers, which is explicit but does not exploit restriction being the nature of the weakness. We show that these prior methods can be thought of as approximations to IMM and can be problematic in terms of consistency. Experimentally, we first motivate IMM using logistic regression as a toy example. We then explore it in language modeling, the application that initially inspired it, and demonstrate it on both LSTM and transformer full models, using bigrams as restricted models. We lastly give a simple RL example, which shows that POMDP policies can help learn better MDP policies. The IMM principle is thus generally applicable in common scenarios where restricted data is cheaper to collect or restricted models are easier to learn. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In many applications, it is both statistically and computationally easier to construct (often small) feature-restricted models. In this paper, we address the question of whether this could be beneficial in the construction of an (often larger) full-featured model. ", "page_idx": 0}, {"type": "text", "text": "To motivate, consider the following toy logistic regression problem, with further details in Section 7. Say we have three features $(x_{1},x_{2},x_{3}\\overline{{)}}\\in\\mathbb{R}^{3}$ and a binary label $y\\,\\in\\,\\{0,1\\}$ . Let features be generated from a distribution $\\pi$ and let labels be conditionally generated according to a logistic (full-featured) true model $P(y|x_{1},x_{2},x_{3})$ . Assume we have ample feature-restricted data with only one feature $\\left({x_{1},y}\\right)$ , which, by marginalization, are samples from the (feature-restricted) true induced model $\\begin{array}{r}{\\overline{{P}}(y|x_{1})=\\sum_{x_{2},x_{3}}\\pi(x_{2},x_{3}|x_{1})P(y|x_{1},x_{2},x_{3})}\\end{array}$ . By virtue of this data, we get a very good approximation $\\hat{P}$ of $\\overline{P}$ , which we can use as a target induced model. How do we use $\\hat{P}$ , along with full-featured data $(x_{1},x_{2},x_{3},y)$ to obtain a (full-featured) learned model $Q(y|x_{1},x_{2},x_{3})?$ A few possible approaches are as follows. We could ignore P\u02c6, and learn $Q$ simply by minimizing cross-entropy on the data. This is wasteful, because $\\hat{P}$ contains valuable information. Alternatively, we could learn $Q$ by minimizing cross-entropy in addition to a secondary loss that keeps $Q$ close to $\\hat{P}$ . This is reasonable \u2014 in Section 2 we relate this to reverse knowledge distillation and in Section 6 to noising. However, $\\hat{P}$ addresses a markedly different task than $Q$ , i.e., that of predicting with a restricted set of features. Instead, what this paper proposes is to equalize the field when comparing to $\\hat{P}$ , by inducing a restricted model $\\overline{{Q}}(y|x_{1})$ from $Q$ during training, and using a secondary loss to match $\\hat{P}$ to $\\overline{{Q}}$ , rather than to $Q$ . We call this induced model matching (IMM). In Figure 1 we show how this speeds up learning and reduces predictor variance. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "image", "img_path": "iW0wXE0VyR/tmp/b2a9d6024524616605fd52d75eb4ce73ce8b8e5e7dc4aceddd011393206f7743.jpg", "img_caption": ["Figure 1: Test accuracy of logistic model, with and without IMM. Bars are $10^{t h}$ to $90^{t h}$ percentiles of 300 runs. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "This example gives an overview of the entire process behind IMM. Most of the paper, however, is dedicated to language modeling, where very natural restricted models exist, namely $N$ -grams. The inspiration of this research is in fact rooted in certain language model data augmentation techniques that noise the data using $N$ -grams Xie et al. (2017). The fresh perspective that we offer here is that such noising is best understood as an attempt to incorporate a feature-restricted model\u2019s knowledge into the full-featured model being trained. This interpretation reveals the general fundamental question: If we are armed with a very accurate feature-restricted model, what is the right way to incorporate it into the training of a model with a larger feature set? ", "page_idx": 1}, {"type": "text", "text": "Our contributions and organization are as follows: ", "page_idx": 1}, {"type": "text", "text": "1. In Section 3, we rigorously frame this question through the notion of an induced model.   \n2. In Section 4, we use this framework to propose a strategy to incorporate the knowledge of the restricted model during learning. This consists of using a regularizer that matches the predictions of the induced learned model with that of the desired target restricted model. This is the language model instance of the induced model matching (IMM) methodology.   \n3. In Section 5, we propose computational approaches to make IMM practical and pave the way for further scaling IMM.   \n4. In Section 6, we loop back to relate IMM to the noising approach of Xie et al. (2017) and to reverse knowledge distillation. We share two key findings. First, these alternatives may be thought of as approximations to IMM. Second, they have a major caveat. They may not be consistent even in the ideal infinite-data regime, in contrast to IMM, which is consistent.   \n5. In Section 7, we experimentally demonstrate the effectiveness of IMM through: (1) details of the logistic regression example, (2) experiments on an LSTM RNN for language modeling and on BERT for classification, showing improvements on multiple tasks, and (3) a simple reinforcement learning example that illustrates the further potential of IMM. ", "page_idx": 1}, {"type": "text", "text": "In Section 8 we discuss limitations and applications beyond those illustrated in the paper. ", "page_idx": 1}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We review two lines of work that are closely related to IMM, noising and knowledge distillation, which are respectively implicit and explicit versions of the same idea. We also review key literature showing the continued merit of $N$ -grams as restricted models. ", "page_idx": 1}, {"type": "text", "text": "Noising and Data Augmentation in Language Modeling The noising methodology in language modeling was proposed initially in order to provide a Natural Language Processing (NLP) parallel to the many data augmentation techniques that existed in other machine learning applications (e.g., sampling translated and rotated variants, in image processing and computer vision). ", "page_idx": 1}, {"type": "text", "text": "Most data augmentation techniques in NLP have used noising or smoothing in one form or another. Feature noising by Wang et al. (2013) was one of the earliest attempts at noising when it came to structured prediction tasks. Later, Xie et al. (2017) successfully used restricted language models, namely unigrams and bigrams, to perform data augmentation through noising. By making a rough connection between noising and smoothing, Xie et al. (2017) were able to show a clear advantage, beyond the use of other regularization techniques such as dropout. Section 6 more closely examines the claims of that paper and connects it to the framework of the present one. ", "page_idx": 2}, {"type": "text", "text": "The current approach is not related to all data augmentation techniques, rather specifically to those that use noising with a restricted model. Indeed, it can complement other data augmentation techniques. For example, in our BERT experiments, this technique complements the MLM objective, where the masking itself can be thought of as data augmentation. Similarly, it is capable of complementing alternative masking and corruption techniques, such as EDA (Wei and Zou, 2019) and SSMBA $\\mathrm{Ng}$ et al., 2020), which are known to outperform BERT\u2019s default masking for the MLM objective. Similarly, IMM\u2019s gains are in addition to those from other regularization techniques, e.g., weight decay and dropout (Srivastava et al., 2014), as these can be simultaneously utilized. ", "page_idx": 2}, {"type": "text", "text": "$N$ -grams and their Merits $N$ -grams are based on co-occurrence counts, which makes them easy to learn but limits their usefulness for long histories, However, common techniques such as smoothing and backoff make it possible to build excellent short-context models such as bigrams and trigrams, which can rival or exceed neural models that use the same context size (Chelba et al., 2017). This hints at there being value in using these $N$ -gram models to improve long-context models. Some of the earliest attempts at this interpolate the output of the smaller model with modern models. The continued relevance of this approach is evidenced in a very recent paper (Liu et al., 2024) that proposes a special data structure to precompute $N$ -grams to arbitrary lengths and uses interpolation to improve larger language models (LMs). A key motivator of the current paper is the approach of Xie et al. (2017), which instead noises the training data of LMs using $N$ -grams, with improved outcomes over interpolation. Note that other approaches that take advantage of $N$ -grams also exist, such as that of Li et al. (2022), who let LMs learn what the $N$ -gram could not, i.e., the residual. ", "page_idx": 2}, {"type": "text", "text": "Knowledge Distillation Knowledge distillation (KD) is a paradigm first proposed to let a powerful teacher model help better train a weaker student model, by complementing the hard labels of existing data with soft labels (Hinton et al., 2015). In the present notation, the resulting average loss takes the following general form, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathsf{C r o s s-E n t r o p y}\\left(Q\\right)+\\lambda\\sum_{x}\\pi_{n}(x)\\mathsf{D}_{\\mathsf{K L}}\\left(P_{\\tau}^{\\mathsf{t e a c h e r}}(\\cdot|x)\\bigg|\\Big|Q_{\\tau}(\\cdot|x)\\right),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where Cross-Entropy uses hard data, $x$ are contexts, $\\pi_{n}$ is the empirical distribution of contexts, $Q$ is the learned/student prediction model, and $P^{\\tt t e a c h e r}$ is the teacher model, which can be thought of as providing soft predictions. $\\tau$ , the softmax temperature, is used for smoothing the outputs. ", "page_idx": 2}, {"type": "text", "text": "Most relevant to IMM is the recent discovery that, paradoxically, KD with a weak teacher can be helpful to a powerful student. This \u201creverse-KD\u201d or \u201cdistillation from weak teachers\u201d phenomenon was first demonstrated in vision by Yuan et al. (2020), who interpret and ascribe its performance to smoothing targets in a context-dependent way using the teacher, in contrast to typical label smoothing that can be interpreted as using a uniform distribution. Though this comes years after the noising papers, it parallels closely how Xie et al. (2017) transition from uniform noising to Kneser\u2013Ney noising. Later, reverse-KD was also shown to be effective in language models (Qin et al., 2021). ", "page_idx": 2}, {"type": "text", "text": "On the surface, IMM is similar to reverse-KD, as it uses a weak target teacher to regularize a powerful student\u2019s learning. A quick comparison to Eq. (14) when $P^{\\tt t e a c h e r}$ is the restricted model $\\overline{P}$ reveals that reverse-KD has more in common with noising, which we show can be sub-optimal. Indeed, the major difference between reverse-KD and IMM is the fact that the weakness of the teacher in this case is of a very particular nature \u2014 it stems from the reliance on a restricted context. Reverse-KD ignores this fact by comparing the student model $Q$ directly to the teacher model in the KL term of Eq. (1). In contrast, IMM harnesses this fact, by comparing the student indirectly to the teacher (target model) at its own level, i.e., using the induced model $\\overline{{Q}}$ . Very recent work by Lee et al. (2023) shows that reverse-KD in language, unlike in vision, can be harmful with very weak teachers, and thus it is not advisable to use reverse-KD with the simple restricted models, e.g., bigrams, that we consider here. Our work shows that they can be effectively incorporated, if we use IMM instead. ", "page_idx": 2}, {"type": "text", "text": "3 Problem Description ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Problem Setting We consider the problem of training a full model $Q(y_{t}|x_{t})$ that is able to predict a label of data point $t$ based on its full context. For example, in forward predictive models, $x_{t}$ is the sequence of past tokens and $y_{t}$ is the next token. Throughout the paper, $Q$ always denotes the full model and is deemed to belong to a reasonably large functional class. Given context-prediction data $\\left({{x}_{t}},{{y}_{t}}\\right)$ of size $n$ , possibly tokenized from a single text, let the primary loss used for training this model be the log-loss. Thus training minimizes the cross-entropy risk: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathsf{C r o s s\\!-\\!E n t r o p y}(Q)=-\\sum_{t}\\log Q(y_{t}|x_{t})\\equiv\\sum_{x}\\pi_{n}(x)\\sum_{y}P_{n}(y|x)\\log\\frac{1}{Q(y|x)},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\pi_{n}$ is the empirical distribution of the context and $P_{n}$ be the empirical distribution of the prediction given the context. Note that $(x_{t},y_{t})$ refer to tokens while $(x,y)$ refer to types. Minimizing this empirical risk can be seen as $Q$ striving to approach the \u201ctrue\u201d model $P(y_{t}|x_{t})$ generating the data, in average context-conditional KL divergence. The idealized risk is thus: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\sum_{x}\\pi(x)\\sum_{y}P(y|x)\\log{\\frac{P(y|x)}{Q(y|x)}}\\equiv\\sum_{x}\\pi(x)\\;\\mathsf{D}_{\\mathsf{K L}}\\left(P(\\cdot|x)\\|Q(\\cdot|x)\\right)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where now $\\pi$ is also the true (not empirical) context distribution. ", "page_idx": 3}, {"type": "text", "text": "Induced Models While $Q$ strives to approximate all of $P$ , we may have additional knowledge about $P$ that captures some of its properties. We consider in particular knowledge of the following form. Assume that the full context $x_{t}$ can be decomposed into a short context $\\overline{{x}}_{t}$ and an extended context $\\underline{{x}}_{t}$ , and that one has access to the \u201ctrue\u201d model that predicts $y_{t}$ based solely on the short context $\\overline{{x}}_{t}$ , i.e. ${\\overline{{P}}}(y|{\\overline{{x}}})$ . To make the notation easier to follow, we invite the reader to consult the glossary of notations in Appendix A. How is this restricted model related to $P$ and $\\pi?$ By a simple marginalization argument, we can see that this model is: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\overline{{P}}(y|\\overline{{x}})=\\!\\!\\sum_{\\underline{{x}}}\\mathbf{P}(y,\\underline{{x}}|\\overline{{x}})=\\!\\!\\sum_{\\underline{{x}}}\\pi(\\underline{{x}}|\\overline{{x}})\\;P(y|\\overline{{x}},\\underline{{x}})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We call $\\overline{P}$ the true induced model. It depends both on the context distribution $\\pi$ and the true model $P$ . Since we do not have the latter two, we cannot explicitly compute $\\overline{P}$ . What motivates us, however, is the possibility to learn it more accurately than $P$ , either by virtue of its smaller parametrization or thanks to cheaply procured auxiliary context-restricted data. ", "page_idx": 3}, {"type": "text", "text": "Problem Statement Given knowledge of the true induced model $\\overline{P}$ , or a very good approximation thereof, how can this information be incorporated to improve the learned model $Q$ ? ", "page_idx": 3}, {"type": "text", "text": "4 Induced Model Matching (IMM) ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Construction of the IMM risk To address the problem, we introduce a secondary loss that matches the learned model\u2019s prediction with that of the induced model, whence the name Induced Model Matching or IMM. The key insight here is not to match $\\overline{P}$ with $Q$ , but rather with $\\overline{{Q}}$ , the learned induced model that, just like the move from $P$ to $\\overline{P}$ in Eq. (3), specializes $Q$ to performing predictions with only the short context: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\overline{{{Q}}}(y|\\overline{{{x}}})=\\sum_{\\underline{{{x}}}}\\pi(\\underline{{{x}}}|\\overline{{{x}}})Q(y|\\overline{{{x}}},\\underline{{{x}}})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Let\u2019s first idealize and assume availability of $\\overline{P}$ and the context distribution $\\pi$ , required to compute $\\overline{{Q}}$ . Equipped with $\\overline{{Q}}$ , we can introduce the idealized induced model matching (IMM) risk, a secondary risk that is the average context-conditional KL divergence with the restricted context: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\sum_{x}\\pi(x)\\underbrace{\\sum_{y}\\overline{{P}}(y|\\overline{{x}})\\log\\frac{\\overline{{P}}(y|\\overline{{x}})}{\\overline{{Q}}(y|\\overline{{x}})}}_{{\\sf D}_{\\sf K L}(\\overline{{P}}(\\cdot|\\overline{{x}})\\|\\overline{{Q}}(\\cdot|\\overline{{x}}))}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Since $\\overline{P}$ and $\\pi$ are not available in practice, the idealized IMM risk cannot be computed. However, as the core motivation of using $\\overline{P}$ is the potential ability to learn it very accurately from data, we assume instead that we have access to a target induced model $\\hat{P}$ as a proxy to $\\overline{P}$ . As for calculating $\\overline{{Q}}$ , knowledge of $\\pi$ in Eq. (4) can be intuitively understood as a mechanism for fliling-in for the extended context, based on the short context. As such, we have the following natural empirical version which can be thought of as averaging $Q$ over all extended contexts in the dataset, while keeping the short context fixed: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{Q}(y|\\overline{{x}})\\propto\\sum_{t}\\mathbf{1}\\{\\overline{{x}}_{t}=\\overline{{x}}\\}Q(y|x_{t})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The proportionality constant is unimportant as, thanks to the logarithm, it contributes only a constant to the overall risk. By combining these empirical proxies, we obtain the empirical IMM risk: ", "page_idx": 4}, {"type": "equation", "text": "$$\n{\\sf I M M}(Q)=\\sum_{x}\\pi_{n}(x)\\sum_{y}\\hat{P}(y|\\overline{{{x}}})\\log\\frac{1}{\\hat{Q}(y|\\overline{{{x}}})}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This mirrors Eq. (5), up to $\\overline{P}$ inside the logarithm, which only contributes an additive entropy term that does not depend on $Q$ and is thus irrelevant to optimization. ", "page_idx": 4}, {"type": "text", "text": "IMM as a regularizer Given a reliable $\\hat{P}$ , we propose to incorporate this knowledge into the full model by using the IMM risk as a regularizer. Using a hyper-parameter $\\lambda$ that can be tuned, our induced model matching methodology consists of training the model $Q$ by minimizing ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathsf{C r o s s-E n t r o p y}(Q)+\\lambda\\,\\mathsf{I M M}(Q).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Separating IMM into components To weave IMM into existing ML optimization pipelines, it is useful to treat it as a separable risk. For this, we can rewrite Eq. (7) by expanding the empirical distribution $\\pi_{n}$ . Then, the components of this separable risk are given by the conditional crossentropies between $\\hat{P}$ and $\\hat{Q}$ , because we can write: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathsf{I M M}(Q)=-\\frac{1}{n}\\sum_{t}\\underbrace{\\left[\\sum_{y}\\hat{P}(y|\\overline{{x}}_{t})\\log\\hat{Q}(y|\\overline{{x}}_{t})\\right]}_{\\mathsf{I M M}_{t}(Q)}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The simplest version of $\\hat{P}$ could be based on empirical counts, which may be valid if the context space is discrete and small. In language modeling, smoothing methods can be used to generate better versions of $\\hat{P}$ , such as the modified Kneser\u2013Ney smoothed version of the bigram counts (Kneser and Ney, 1995; Chen and Goodman, 1999) that can be expressed as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{P}(y|\\overline{{x}})=\\frac{1}{n}\\sum_{t}(1-\\nu(\\overline{{x}}))\\mathbf{1}\\{y_{t}=y\\}+\\nu(\\overline{{x}})b(y),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\nu({\\overline{{x}}})$ is the missing mass for previous token $\\overline{{x}}$ and $b$ is the back-off distribution. For logistic regression in Section 7 the context space is continuous, and these components come from a separately trained one-feature predictor. For our RL example, they come from the optimal actions of a POMDP. ", "page_idx": 4}, {"type": "text", "text": "5 Computation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "A direct implementation of IMM is prohibitive, since evaluation of the risk, Eq. (8), requires making a secondary pass over the entire dataset for each $t$ , when inducing the model in Eq. (6). We address this with two approaches. The first approximates the objective by replacing this secondary pass with sampling whereas the second incorporates this secondary pass into the primary pass by serializing the gradient calculation. Sampled IMM has the advantage of low gradient variance at the expense of added computation. Serialized IMM has higher gradient variance but has only constant-factor computational overhead. All of our experiments use sampled IMM, with the exception of a demonstration of serialized IMM for logistic regression (in Appendix D.5), as a proof of concept for IMM\u2019s scalability. ", "page_idx": 4}, {"type": "text", "text": "Sampled IMM To alleviate the exact computation of IMM, we could maintain a dictionary/multiset of extended contexts for each short context1, and sweeping only across those. This amounts to rewriting Eq. (6) as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\hat{Q}(y|\\overline{{x}})\\propto\\sum_{t^{\\prime}:\\overline{{x}}_{t^{\\prime}}=\\overline{{x}}_{t}}Q(y|\\overline{{x}}_{t},\\underline{{x}}_{t^{\\prime}})=\\sum_{x\\in\\mathrm{extend}(\\overline{{x}}_{t})}Q(y|\\overline{{x}}_{t},x),\\mathrm{where~}\\mathsf{e x t e n d}(\\overline{{x}})=\\bigcup_{t:\\overline{{x}}_{t}=\\overline{{x}}}\\{\\underline{{\\{x}}}_{t}\\}\\!\\!\\!\\!\\!\\{\\mathrm{g}}_{t}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "is the multiset of extended contexts of the short context of $x$ . ", "page_idx": 5}, {"type": "text", "text": "Since a given short context may appear in a large number of extended contexts, the dictionary/multiset approach of Eq. (10) remains expensive. In particular, gradients need to be computed with each of these contexts. Instead, we propose an approximation based on writing the (normalized) sum as an expectation over samples from extend $\\left({\\overline{{x}}}_{t}\\right)$ , followed by approximating this expectation with $k$ samples. As a result, the IMM component at $t$ becomes: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle{|\\sf M M_{t}(\\boldsymbol{Q})}}&{=}&{\\displaystyle-\\sum_{y}\\hat{P}(y|\\overline{{x}})\\log\\left[\\mathbf{E}_{X\\sim\\mathrm{extend}(\\overline{{x}}_{t})}\\left[Q(y|\\overline{{x}}_{t},X)\\right]\\right]}\\\\ &&{\\approx}&{\\displaystyle-\\sum_{y}\\hat{P}(y|\\overline{{x}})\\log\\left[\\frac{1}{k}\\sum_{i=1}^{k}Q(y_{0}|\\overline{{x}}_{t},x_{i})\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $x_{i}\\sim\\mathsf{e x t e n d}(\\overline{{x}}_{t})$ are samples from the multiset of extended contexts. Implementation details are deferred to Appendix C, including Algorithms 1 and 2 that describe a typical training loop when sampled IMM or serialized IMM are incorporated into SGD respectively. ", "page_idx": 5}, {"type": "text", "text": "At first glance, it appears that sampled IMM requires maintaining $k$ copies of the model. However, this can be sequentialized, as explained Appendix C.2. This means that the space overhead during training is a factor of 2 compared to the baseline of no-IMM (i.e., a second set of gradients). However, the time overhead is generally a $k$ -fold increase over the baseline, but can be worse for recurrent models such as LSTMs. This is because we typically unroll the model over $L$ tokens and can perform forward/backward passes over this unroll, in $L$ steps. If we apply IMM at every unroll position, we need to restart the LSTM, which then incurs an $\\ensuremath{\\mathcal{O}}(k L)$ -fold increase. We partially mitigate this in our current implementation by applying IMM periodically (not at every iteration), see Appendix D.2.2. For example, if we apply it only every $\\Omega(1/L)$ iterations, the overhead factor remains $O(k)$ . ", "page_idx": 5}, {"type": "text", "text": "Serialized IMM Sampled IMM delivers the beneftis of IMM, but at a computational cost. To make IMM truly scalable, it is imperative to bring it to near-constant factor overhead to the baseline of no-IMM. The main bottleneck is the calculation of the learned induced model $\\hat{Q}$ . The following alternative approach bears a resemblance to the sequentialization aspect covered in Appendix C.2, Eq. (20). Up to a constant, we can write the idealized IMM risk of Eq. (5) as an averaging only over short contexts: ", "page_idx": 5}, {"type": "equation", "text": "$$\n-\\sum_{\\vec{x}}\\pi(\\overline{{x}})\\sum_{y}\\overline{{P}}(y|\\overline{{x}})\\log\\overline{{Q}}(y|\\overline{{x}}),\\mathrm{where~the~induced~model~is~}\\overline{{Q}}(y|\\overline{{x}})=\\sum_{\\underline{{x}}}\\pi(\\underline{{x}}|\\overline{{x}})Q(y|\\underline{{x}},\\overline{{x}}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The gradient of this IMM risk then becomes: ", "page_idx": 5}, {"type": "equation", "text": "$$\n-\\sum_{\\overline{{x}}}\\pi(\\overline{{x}})\\sum_{y}\\overline{{P}}(y|\\overline{{x}})\\frac{\\sum_{\\underline{{x}}}\\pi(\\underline{{x}}|\\overline{{x}})\\nabla Q(y|\\underline{{x}},\\overline{{x}})}{\\overline{{Q}}(y|\\overline{{x}})}=-\\sum_{\\overline{{x}}}\\sum_{\\underline{{x}}}\\pi(\\overline{{x}})\\pi(\\underline{{x}}|\\overline{{x}})\\sum_{y}\\overline{{P}}(y|\\overline{{x}})\\frac{\\nabla Q(y|\\underline{{x}},\\overline{{x}})}{\\overline{{Q}}(y|\\overline{{x}})}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathrm{~tne~empirrat~versuon~or~uns~graatent~s};\\quad\\,}\\\\ {\\displaystyle-\\frac{1}{n}\\sum_{t}\\sum_{y}\\hat{P}(y|\\overline{{x}}_{t})\\frac{\\nabla Q(y|\\underline{{x}}_{t},\\overline{{x}}_{t})}{\\hat{Q}(y|\\overline{{x}})}=\\frac{1}{n}\\sum_{t}\\nabla\\sqrt{\\displaystyle-\\sum_{y}\\overline{{P}}(y|\\overline{{x}}_{t})}\\underbrace{\\frac{Q(y|\\underline{{x}}_{t},\\overline{{x}}_{t})}{\\hat{Q}(y|\\overline{{x}}_{t})}\\log Q(y|\\underline{{x}}_{t},\\overline{{x}}_{t})}_{\\displaystyle\\hat{Q}(y|\\overline{{x}}_{t})}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The last expression in Eq. (13) is in the form of a \u201ccorrected\u201d cross-entropy between $\\hat{P}$ and $\\hat{Q}$ , which we denote by $\\widetilde{|\\mathsf{M M}_{t}}$ . The correction factor is not differentiated and is the ratio or Radon-Nykodim derivative of $Q$ relative to $\\hat{Q}$ . What this accomplishes is to delegate the averaging of the gradients to the primary pass over the dataset. If the correction term is given, computing this gradient costs the same as computing the baseline of no-IMM gradients. There are two caveats: first, because the averaging is now happening in the primary pass, the variance of this gradient is higher than sampled IMM and, second, the correction factor still requires knowing the learned induced model $\\hat{Q}$ . To address the higher variance of the gradients, techniques such as momentum approaches along with learning rate schedulers can be used. To address knowing $\\hat{Q}$ , we suggest the heuristic of updating $\\hat{Q}$ only periodically. Then, if model $Q_{\\dagger}$ (that eventually becomes stale) is used to calculate $\\bar{\\hat{Q}}_{\\dagger}$ , use the ratio $Q_{\\dagger}/\\hat{Q}_{\\dagger}$ for correction factor. Using the current $Q$ tends to cause instability, likely due to the correction no longer obeying expected constraints, e.g., mean 1 for every $y$ . By choosing the update period inversely proportionally to the cost of updating $\\hat{Q}_{\\dagger}$ and by keeping $Q_{\\dagger}$ in memory, we incur an $\\mathcal{O}(1)$ factor increase in time and space compared to no-IMM, which makes this variant of IMM highly scalable. Note that without the correction factor, IMM turns into reverse-KD and noising, a connection that we elaborate on in Section 6. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "6 Analysis ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In what follows, we assume to be in the realizable case, i.e., that the model space of $Q$ is expressive enough to contain the true model $P$ . We also take an infinite-data perspective and focus on consistency only, even though IMM also confers a finite-sample advantage (understanding this advantage analytically is worth studying in the future.) We show (1) that IMM is consistent, (2) that noising and reverse-KD are akin to single-sample IMM, (3) that this shows that they minimize an upper bound on the IMM risk, and finally (4) that this introduces sub-optimality, which could even threaten consistency. This gives basic analytical backing to why IMM is preferable. ", "page_idx": 6}, {"type": "text", "text": "Consistency of IMM Observe that if, in the main objective, Eq. (8), cross-entropy and IMM were replaced with their true counterparts, Eqs. (2) and (5) respectively, then $Q=P$ remains the minimizer of the objective. This observation shows that we recover the true model in the infinite-data regime, i.e., that IMM is consistent for all $\\lambda$ . We next aim to show that the key caveat of noising and reverse-KD is that they may be inconsistent, unless $\\lambda$ is made to vanish. ", "page_idx": 6}, {"type": "text", "text": "From single-sample IMM to noising and reverse-KD We first explain how IMM and noising are related. Experimentally, using a single sample $(k=1)$ ) in the IMM approximation of Eq. (12) produces perplexities that are near-identical to noising. We explain this phenomenon as follows. Say data point $t$ is considered during optimization, in an SGD mini-batch. When a single random extended context is sampled, it is equivalent to swapping that data point\u2019s context with another based on the multiset extend $\\left({\\overline{{x}}}_{t}\\right)$ . That context belongs to a different data point $t^{\\prime}$ . Since sampling is done uniformly from the multiset, this simply presents data to the SGD algorithm in a different random order, possibly with repetition. More pertinently to noising, the actual prediction $x_{t^{\\prime}}$ has no role in calculating the loss. Instead, the prediction is randomized through the sum over all possible $y_{0}$ in Eq. (9). Though not identical, this is a very close map to the noising-based data augmentation proposed by Xie et al. (2017), namely prediction noising (see Appendix B.1 for a review of this framework and why prediction noising is its main aspect.) It is in fact even closer to an alternative proposed later by Gao et al. (2019), which uses soft prediction vectors just like $\\hat{P}$ in this single-sample approximation of IMM. As a result of this argument, we can think of noising as minimizing the following idealized objective, which coincidentally is equivalent to the reverse-KD objective (Yuan et al., 2020; Qin et al., 2021) (see also Section 2) with $\\overline{P}$ as the teacher: ", "page_idx": 6}, {"type": "equation", "text": "$$\n{\\mathsf{C r o s s-E n t r o p y}}(Q)+\\lambda\\sum_{x}\\pi(x)\\sum_{y}{\\overline{{P}}}(y|\\overline{{x}})\\log\\frac{1}{\\underbrace{Q(y|x)}}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Inconsistency of noising and reverse-KD How is this single-sample IMM objective related to performing IMM? Recall that we can write a single component of the empirical IMM risk with sampling as in (12), which by Jensen\u2019s inequality can be upper bounded as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\n-\\sum_{y}\\hat{P}(y|\\overline{{x}})\\log\\left(\\mathbf{E}_{X}\\left[Q(y|\\overline{{x}}_{t},X)\\right]\\right)\\leq-\\sum_{y}\\hat{P}(y|\\overline{{x}})\\mathbf{E}_{X}\\left[\\log\\left(Q(y|\\overline{{x}}_{t},X)\\right)\\right]\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "A single-sample approximation of the expectation inside the log is in fact a biased estimator of the left-hand side. However, it is an unbiased estimator of the right-hand side, with the expectation outside of the log. Thus, noising and reverse-KD upper bound the IMM risk. Minimizing an upper bound instead of the desired risk could introduce suboptimality. The following proposition uses the difference between these methods, which pit the target model against the full learned model $Q$ and not the induced learned model $\\overline{{Q}}$ like IMM (contrast Eq. (14) and Eq. (5)), to show that this suboptimality can indeed occur. Even in the realizable case and with infinite data, IMM is always consistent but there exists a counterexample where noising and reverse-KD fail to consistently recover the true model. The proof is in Appendix B.2. ", "page_idx": 7}, {"type": "text", "text": "Proposition 6.1. Assume that we are optimizing the idealized noising objective of Eq. (14) \u2014 i.e., we are operating in the infinite-data regime \u2014 and let $Q^{\\star}$ be its global minimizer. Assume further that the model class for $Q$ contains the true model $P-i.e.$ ., we are in the realizable case. Then, there exists a choice of $\\pi$ and $P$ such that $Q^{\\star}\\neq P$ . ", "page_idx": 7}, {"type": "text", "text": "7 Experimental Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "7.1 Starting Toy Example: Logistic Regression ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Consider the logistic regression example from the introduction. The main results are given in Figure 1 and the full experimental details can be found in Appendix D.1. Here we highlight how the problem fits the IMM framework and where it deviates from language modeling. First, note that the context decomposition is $\\overline{{x}}=x_{1}$ and $\\underline{{x}}=\\left(x_{2},x_{3}\\right)$ . ", "page_idx": 7}, {"type": "text", "text": "Setting We sample features uniformly over a cube and assume we have ample data points of the form $(x_{1},y)$ . This allows us to build an excellent restricted model to predict the label based on just $x_{1}$ , call it $\\overline{{P}}(y|x_{1})$ , nearly close to the (restricted) Bayes predictor or true conditional probability. Just like in language modeling, to induce a model we need to draw $\\underline{{x}}$ \u2019s from its conditional distribution given $\\overline{{x}}$ . $\\overline{{Q}}(y|x_{1})$ , the induced model of $Q(y|x_{1},x_{2},x_{3})$ , can then be interpreted as the average of $Q$ \u2019s predictions, when $\\underline{{x}}_{t}=\\left(x_{2},x_{3}\\right)$ is drawn from its conditional distribution given $\\overline{{x}}_{t}=x_{1}$ . Since we typically don\u2019t have access to this distribution, we approximate it empirically. In language modeling, we could just sample from the empirical distribution of $\\underline{{x}}$ for a given $\\overline{{x}}$ . In logistic regression, this is not viable since $x_{1}$ is continuous and does not repeat. We rely instead on density estimation. We use a soft nearest-neighbor density estimate $\\begin{array}{r}{\\hat{f}(x_{2},x_{3}|x_{1})\\propto\\sum_{t=1}^{n}\\delta_{x_{2,t},x_{3,t}}(x_{2},x_{3})\\mathrm{e}^{-\\alpha|x_{1,t}-x_{1}|}}\\end{array}$ , where $1/\\alpha$ is the bandwidth of the Laplace kernel. (With cross-validation, we determine $\\alpha=1$ to be a good choice.) If we let $w_{t}(x_{1})=\\mathrm{e}^{-\\alpha|x_{1,t}-x_{1}|}$ , the resulting induced model by marginalization is: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\overline{{Q}}(y|\\overline{{x}})\\!=\\!\\int f(\\underline{{x}}|\\overline{{x}})Q(y|\\overline{{x}},\\underline{{x}})\\!\\approx\\!\\sum_{t=1}^{n}\\frac{w_{t}(\\overline{{x}})}{\\sum_{t=1}^{n}w_{t}(\\overline{{x}})}Q(y|\\overline{{x}},\\underline{{x}}_{t})\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "These equations are respectively equivalent to Eqs. (4) and (6). The IMM risk and the corresponding overall objective remain the same: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbb{M}\\mathbb{M}(Q)=\\sum_{t=1}^{n}\\sum_{y=0,1}\\hat{P}(y|x_{1,t})\\log\\frac{1}{\\overline{{Q}}(y|x_{1,t})},\\qquad\\mathsf{C r o s s-E n t r o p y}(Q)+\\lambda\\,\\mathbb{M}\\mathbb{M}(Q).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Results In Figure 1, we compare the performance of IMM-trained $Q$ (green) to that without IMM (maroon). We sweep a range of $n$ from 2 to 50, and use a cross-validation optimized $\\lambda$ for each (details in the Appendix D.1.1). The key observations are that: (1) IMM always improves on the baseline performance, (2) the variance of the outcomes is also typically diminished, (3) the improvement is greater with less data, but the gain across data sizes is equivalent to access to an average of $30\\%$ extra data. This and similar experiments suggest that gains are highest when the dataset size is comparable to the number of parameters. This simple scenario demonstrates how IMM effectively harnesses the benefit of accurate feature-restricted models when training full-featured models. For reference, we also include the results of noising and interpolation. IMM is always better than noising, but interestingly interpolation is better with very few samples, though much worse with more. We attribute this to the fact that with less data it is harder to obtain an accurate induced model. ", "page_idx": 7}, {"type": "text", "text": "Table 1: Perplexity for an LSTM Language Model using the Penn TreeBank dataset. The numbers on None and KN Noising are from Xie et al. (2017) and can be replicated using their original code (we use the model with latent dimension 1500). Like the baseline, for each row, we report the best value across as many restarts. ", "page_idx": 8}, {"type": "table", "img_path": "iW0wXE0VyR/tmp/8b510f8628a018e2298aecbf1455cada5ffd91a8726cd1c9394248893369c260.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "iW0wXE0VyR/tmp/efb1f569a12533076cbc73b2cd352b35b3b0a383d33c2fcf04b4e06f2ea8b57f.jpg", "table_caption": ["Table 2: Results on the BERTBASE Language Model. The baseline numbers can be replicated using the original BERT code by Google, as well as our provided repository. Matthew\u2019s Correlation Coefficient is used for CoLA, F1 score for MRPC and Accuracy for QNLI and RTE. Like the baseline, reported numbers are averages across multiple restarts. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "7.2 Language Modeling Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In these language modeling experiments, the restricted model we use is the modified Kneser\u2013Ney bigram (Kneser and Ney, 1995; Chen and Goodman, 1999) of the dataset in question. To see why this is a good choice, we refer the reader to benchmarking done by Chelba et al. (2017); neural models (single layer and 2-layer LSTM RNNs) could not improve upon the perplexity of an interpolated Kneser\u2013Ney $N$ -gram of a similar order. After the introduction of the attention mechanism (Bahdanau et al., 2014; Luong et al., 2015), better neural models now dominate language modeling (Vaswani et al., 2017; Devlin et al., 2018; Turc et al., 2019). We investigate how IMM could potentially improve even these more modern models, by using BERT\u2019s performance on some GLUE benchmarks as a proof of concept. (Appendix D.2.1 gives evidence that IMM indeed improves the full model\u2019s performance on the restricted task.) ", "page_idx": 8}, {"type": "text", "text": "LSTM RNN Experiments We build on top of the code provided by Xie et al. (2017) using the same topology for the LSTM RNN. The chief purpose of these experiments is to contrast directly with noising introduced in that paper. The PTB dataset is a document dataset and the LLM is solving a next word prediction task. The average cross-entropy of multiple unroll positions of the RNN, or exponentiated as perplexity, is used as the measure of performance. For training and measuring validation perplexity, $L=35$ unroll positions are used. During testing, only 1 unroll position is used. The IMM component is always calculated by running the LSTM in an evaluation mode (i.e., without any dropout). In addition, while regular evaluation updates the state in stateful models like the LSTM, the IMM branch never changes the state and uses the state set by the primary branch when traversing the dataset. In Table 1, we report perplexity values using $k$ -sampled IMM with $k=10$ . The table also includes the best noising results that we could reproduce based on the code of Xie et al. (2017), after communication with the authors (these are 0.5 more than the paper\u2019s numbers). ", "page_idx": 8}, {"type": "text", "text": "BERT Experiments We introduce the IMM objective in BERT\u2019s fine-tuning phase by reintroducing the Masked Language Model (MLM) objective that is originally only present during pre-training. In Google\u2019s original BERT code, MLM was present during pre-training but removed from fine-tuning, possibly because of minimal gain. For us, however, MLM is ideally suited to be augmented with the IMM objective because it is based on cross-entropy and we can similarly generate an induced bigram for predicting masked words in a sequence of tokens. We report numbers on these datasets in Table 2. The second column shows the numbers after adding back the MLM objective, which doesn\u2019t produce much gain on its own. The third column adds IMM within MLM, significantly boosting the gains. ", "page_idx": 8}, {"type": "text", "text": "Since some GLUE (Wang et al., 2018) datasets (used in the original BERT paper) are too large to be trained in an academic setting, we use a subset of the GLUE tasks (Warstadt et al., 2018; Dolan and Brockett, 2005; Rajpurkar et al., 2016; Dagan et al., 2005) to demonstrate the gains using IMM. For diversity, our selected GLUE tasks are a mixture of single-sentence and double-sentence tasks. Further experimental details are provided in Appendix D.2. ", "page_idx": 8}, {"type": "text", "text": "7.3 Reinforcement Learning: POMDPs helping MDPs ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "IMM is particularly appealing in situations where incomplete-feature data may be much more available, due to reasons like law, privacy, or limited sensing. If an autonomous driving system is developed with few sensors, and later a new system taking advantage of more sensors is to be designed, the older system may act as a restricted model helping the new design. If a diagnostic system is built based on a limited set of genetic markers, and later more markers are determined relevant, then the legacy system can be used without referring to the legacy data, which may need to be kept private. If a platform\u2019s recommendation and ad engine is trained from ample general-public data, and later a personalized engine is to be developed, then the older engine can inform the personalized one through IMM. ", "page_idx": 9}, {"type": "text", "text": "In stateful environments, problems like the latter often require a reinforcement-learning (RL) solution. If the personalized engine has full-featured data, it can use the data to train an MDP (Markov Decision Process) policy to optimize expected reward (Sutton and Barto, 2018). In contrast, the general-public data, despite being abundant, may lack in features and may only allow for solving a POMDP (Partially Observable Markov Decision Process). We can show that IMM can allow a good POMDP solution to significantly improve MDP training, by modifying policy-gradient methods such as REINFORCE (Williams, 1992). In Figure 2, we illustrate the reward achieved with and without the use of IMM, for learning policies for an agent on a toroidal $11\\times11$ grid, with reward peaking at its center. The POMDP only observes one coordinate, whereas the MDP observes both. ", "page_idx": 9}, {"type": "image", "img_path": "iW0wXE0VyR/tmp/463aef3afb96db7187d96cfb6db5790c9157cedadd3e6c0572d4161359c52f6c.jpg", "img_caption": ["Figure 2: Average reward of MDP trained without and with IMM incorporating POMDP. Details in Appendix D. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "8 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we addressed the question of how to incorporate accurate restricted models in the training of full-featured models using the principle of induced model matching (IMM). This was inspired by interpreting some noising-based data augmentation techniques in natural language noising, as an attempt to harness the knowledge of the restricted model used for noising. We showed that na\u0131\u00a8ve noising is not the best way to incorporate this knowledge, as it may fail to consistently recover the true model. IMM, on the other hand, directly aligns the learned model with the target model and is consistent. The results shows that IMM always outperforms noising, and improvements even decay gracefully with lower restricted model quality (see Appendix D.4). One limitation of our approach is that computing the induced model exactly is not always viable. To remedy this, we proposed sampled IMM, which yields accurate but somewhat computationally demanding learning, and serialized IMM, which is slightly less accurate but has a potential to be as efficient as the no-IMM baseline. We then experimentally demonstrated the gains that IMM can offer, in a logistic regression toy example, when training LSTM language models and fine-tuning pretrained transformers, and in a simple reinforcement learning scenario. We believe that scaling from feature-restricted to full-featured models is an important yet under-studied sub-problem of knowledge transfer. In addition to the proof-of-concept experiments in this paper, many others of particular contemporary relevance may be devised. For example, lengthening the context of large language models remains an open problem; we believe that IMM can be part of betters solutions, by correctly informing new longer-context LLMs using current shorter-context LLMs. The principle behind IMM is applicable very generally and we hope this work gives impetus to such explorations. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This paper is based upon work supported in part by the National Science Foundation, through the NSF CAREER Program under Award No. CCF-2146334 (From Rare Events to Competitive Learning Algorithms), the NSF HDR TRIPODS Phase II Program under Award No. ECCS-2217023 (IDEAL Institute), and the NSF TRIPODS Phase I Program under Award No. CCF-1934915 (UIC Foundations of Data Science Institute). Computational infrastructure was supported in part by the NSF MRI Program Award No. CNS-1828265 (COMPaaS DLV). ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Bahdanau, D., Cho, K., and Bengio, Y. (2014). Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473.   \nChelba, C., Norouzi, M., and Bengio, S. (2017). N-gram language modeling using recurrent neural network estimation. arXiv preprint arXiv:1703.10724.   \nChen, S. F. and Goodman, J. (1999). An empirical study of smoothing techniques for language modeling. Computer Speech & Language, 13(4):359\u2013394.   \nDagan, I., Glickman, O., and Magnini, B. (2005). The pascal recognising textual entailment challenge. In Machine Learning Challenges Workshop, pages 177\u2013190. Springer.   \nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.   \nDolan, W. B. and Brockett, C. (2005). Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005).   \nEgorov, M., Sunberg, Z. N., Balaban, E., Wheeler, T. A., Gupta, J. K., and Kochenderfer, M. J. (2017). POMDPs.jl: A framework for sequential decision making under uncertainty. Journal of Machine Learning Research, 18(26):1\u20135.   \nGao, F., Zhu, J., Wu, L., Xia, Y., Qin, T., Cheng, X., Zhou, W., and Liu, T.-Y. (2019). Soft contextual data augmentation for neural machine translation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5539\u20135544.   \nHinton, G., Vinyals, O., Dean, J., et al. (2015). Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2(7).   \nKaplun, G., Malach, E., Nakkiran, P., and Shalev-Shwartz, S. (2022). Knowledge distillation: Bad models can be good role models. Advances in Neural Information Processing Systems, 35:28683\u201328694.   \nKneser, R. and Ney, H. (1995). Improved backing-off for m-gram language modeling. In 1995 International Conference on Acoustics, Speech, and Signal Processing, volume 1, pages 181\u2013184. IEEE.   \nKochenderfer, M. J., Wheeler, T. A., and Wray, K. H. (2022). Algorithms for decision making. MIT press.   \nLee, H., Hou, R., Kim, J., Liang, D., Hwang, S. J., and Min, A. (2023). A study on knowledge distillation from weak teacher for scaling up pre-trained language models. arXiv preprint arXiv:2305.18239.   \nLi, H., Cai, D., Xu, J., and Watanabe, T. (2022). Residual learning of neural text generation with $n$ -gram language model. arXiv preprint arXiv:2210.14431.   \nLiu, J., Min, S., Zettlemoyer, L., Choi, Y., and Hajishirzi, H. (2024). Infini-gram: Scaling unbounded n-gram language models to a trillion tokens. arXiv preprint arXiv:2401.17377.   \nLuong, M.-T., Pham, H., and Manning, C. D. (2015). Effective approaches to attention-based neural machine translation. arXiv preprint arXiv:1508.04025.   \nNg, N., Cho, K., and Ghassemi, M. (2020). SSMBA: Self-supervised manifold based data augmentation for improving out-of-domain robustness. arXiv preprint arXiv:2009.10195.   \nPascanu, R., Mikolov, T., and Bengio, Y. (2013). On the difficulty of training recurrent neural networks. In International conference on machine learning, pages 1310\u20131318. PMLR.   \nQin, Y., Lin, Y., Yi, J., Zhang, J., Han, X., Zhang, Z., Su, Y., Liu, Z., Li, P., Sun, M., et al. (2021). Knowledge inheritance for pre-trained language models. arXiv preprint arXiv:2105.13880.   \nRajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P. (2016). Squad: $100{,}000{+}$ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250.   \nSrivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. (2014). Dropout: a simple way to prevent neural networks from overftiting. The journal of machine learning research, 15(1):1929\u20131958.   \nSutton, R. S. and Barto, A. G. (2018). Reinforcement learning: An introduction. MIT press.   \nTurc, I., Chang, M.-W., Lee, K., and Toutanova, K. (2019). Well-read students learn better: The impact of student initialization on knowledge distillation. arXiv preprint arXiv:1908.08962, 13.   \nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30.   \nWang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. R. (2018). Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461.   \nWang, S. I., Wang, M., Wagner, S., Liang, P., and Manning, C. D. (2013). Feature noising for log-linear structured prediction. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1170\u20131179.   \nWarstadt, A., Singh, A., and Bowman, S. R. (2018). Neural network acceptability judgments. arXiv preprint arXiv:1805.12471.   \nWei, J. and Zou, K. (2019). Eda: Easy data augmentation techniques for boosting performance on text classification tasks. arXiv preprint arXiv:1901.11196.   \nWilliams, R. J. (1992). Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8:229\u2013256.   \nXie, Z., Wang, S. I., Li, J., L\u00b4evy, D., Nie, A., Jurafsky, D., and Ng, A. Y. (2017). Data noising as smoothing in neural network language models. arXiv preprint arXiv:1703.02573.   \nYuan, L., Tay, F. E., Li, G., Wang, T., and Feng, J. (2020). Revisiting knowledge distillation via label smoothing regularization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 3903\u20133911. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Explanation of main notations ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Figure 3 gives a schematic overview of the process of IMM. ", "page_idx": 12}, {"type": "image", "img_path": "iW0wXE0VyR/tmp/c6686e0e832ea609c9969f118d63d8404a6259b85fbc736327bc8ba4239ef558.jpg", "img_caption": ["Figure 3: Schematic overview of IMM. "], "img_footnote": [], "page_idx": 12}, {"type": "table", "img_path": "iW0wXE0VyR/tmp/bbd94734e78f6fc630eed4cba59b3d0558ed79dd526e228e9ee04512e92adb45.jpg", "table_caption": ["Table 3: Glossary "], "table_footnote": [], "page_idx": 12}, {"type": "text", "text": "Table 3 compiles our main notation. We summarize the main formulation here and explain how it connects to its instances in the experiments. ", "page_idx": 12}, {"type": "text", "text": "\u2022 $Q$ is always the full model (LSTM or Transformer). $Q(y_{t}|x_{t})$ is thus a learned contextconditional model: takes the full context $\\left(\\boldsymbol{x}_{t}\\right)$ and outputs prediction probabilities (over $y_{t}$ ). In Eq. (4), $-\\log(Q)$ is the log-loss, whose average is the cross-entropy. \u2013 LSTM/PTB: context $=$ previous words, prediction $=$ next word, and cross-entropy $=$ main (next-word prediction) loss. ", "page_idx": 12}, {"type": "text", "text": "\u2013 BERT/GLUE: context $=$ unmasked words, prediction $=$ masked word, and cross-entropy $=\\mathbf{M}\\mathbf{L}\\mathbf{M}$ (masked word prediction) portion of the fine-tuning loss. \u2013 Note: KL divergence and cross-entropy are interchangeable, as their difference doesn\u2019t depend on $Q$ .   \nTrue context distribution $\\pi$ and true context-conditional model $P$ describe the unknown   \nprobability space. We assume tokenized data $(x_{t},y_{t})$ generated according to $\\pi(x_{t})$ and   \n$\\textstyle P(y_{t}|x_{t})$ . \u2013 The goal of learning $Q$ is to approximate $P$ .   \nEmpirical context distribution $\\pi_{n}$ and empirical context-conditional model $P_{n}$ are distribution) with sums (over the training set), e.g., Eq. $\\left(5\\right)\\rightarrow\\mathrm{Eq}$ . (9).   \nInduced model: specializes full context-conditional model $\\left(y_{t}|x_{t}\\right)$ to short context only   \n$(y_{t}|\\overline{{x}}_{t})$ , under a specific context distribution. \u2013 If the full context-conditional is $P$ and the context distribution is $\\pi$ , we get the true induced model $\\overline{P}$ . This is the ideal, or Bayes\u2019 optimal small model (which is not available in practice). In practice, we approximate it by a target induced model, $\\hat{P}$ (e.g., Kneser\u2013Ney bigram). \u2013 If the full context-conditional is $Q$ , and the context distribution is... \\* ... $\\pi$ , we get the learned induced model $\\overline{{Q}}$ . This is the best way $Q$ can predict based only on the short context. However, we cannot evaluate it without $\\pi$ . Instead, we use... \\* ... $\\pi_{n}$ , and get the empirical learned induced model $\\hat{Q}$ . This, we can evaluate and use in our training. In summary, the experiments use $\\hat{Q}$ given by Eq. (6), efficiently approximated via sampling in Eq.   \n(18). This is then plugged into the empirical IMM objective in Eq. (9), which pits $\\hat{Q}$ against $\\hat{P}$ . ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "B Caveats of prior methods: noising and reverse-KD ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "B.1 A quick review and analysis of noising-based data augmentation ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In Xie et al. (2017), noising was proposed as an approach to perform data augmentation in language and sequence-to-sequence models. Two types of noising were suggested: context noising and target noising. We first show that context noising is not justifiable except in select cases. To simplify, we consider only a bigram model, with counts $c(x,y)$ , where $x$ is the context and $y$ is the prediction. The general noising scheme in Xie et al. (2017) would, with some probability $\\gamma(x)$ that may depend on the context, substitute either the context, the prediction, or both with a sample from a distribution $q(\\cdot)$ . It is straightforward to show that, as a result, the expected counts presented to the learner are no longer $c(x,y)$ , but rather change to $\\tilde{c}(x,y)$ as follows. ", "page_idx": 13}, {"type": "text", "text": "(a) If only contexts are noised, then $\\tilde{c}(x,y)=$ ", "page_idx": 13}, {"type": "equation", "text": "$$\n[1-\\gamma(x)]c(x,y)+q(x)\\sum_{x^{\\prime}}\\gamma(x^{\\prime})c(x^{\\prime},y)\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "(b) If only the predictions are noised, then $\\tilde{c}(x,y)=$ ", "page_idx": 13}, {"type": "equation", "text": "$$\n[1-\\gamma(x)]c(x,y)+q(y)\\gamma(x)c(x)\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "(c) Lastly if both predictions and targets are noised, independently, then $\\tilde{c}(x,y)=$ ", "page_idx": 13}, {"type": "equation", "text": "$$\n[1-\\gamma(x)]c(x,y)+q(y)q(x)\\sum_{x^{\\prime},y^{\\prime}}\\gamma(x^{\\prime})c(x^{\\prime},y^{\\prime})\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "These noising schemes are primarily motivated (see Xie et al. (2017), Section 3.3) through the idea that noising leads to classical forms of smoothing, and which in turn may be a desirable property to inject into training more sophisticated language models. This is indeed true in case (a) when $\\gamma(x^{\\prime})=\\lambda$ is constant and $q(\\cdot)\\,{\\bar{=}}\\,c(\\cdot)/n$ is the unigram distribution, leading to simple interpolative smoothing. It immediately fails to be true when $q$ is any other distribution even if $\\bar{\\gamma(\\boldsymbol{x}^{\\prime})}=\\bar{\\lambda}$ , as one needs to normalize with $c(x)$ to get a conditional distribution. The failure of this interpretation is even more apparent in case (a), when $\\gamma(\\boldsymbol{x}^{\\prime})$ and $q(\\cdot)$ are determined via the missing mass and Kneser\u2013Ney backoff, failing to recreate any likeness of the Kneser\u2013Ney smoothed bigram. The situation is at least as bad in case (c). ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "The only instance that succeeds to uphold \u201cnoising as smoothing\u201d is case (b), which recreates both interpolative smoothing in the unigram case, and gives an output related to the Kneser\u2013Ney bigram with the choices of $\\gamma(x^{\\prime})$ and $q(\\cdot)$ from the paper. This last case is of particular interest to us, namely because even though it may appear that we recreate the Kneser\u2013Ney bigram itself, the choice of $\\gamma(x^{\\prime})\\,=\\,\\gamma_{0}N_{1+}(\\bar{x_{-1}},\\cdot)/c(\\bar{x})$ with $\\gamma_{0}\\,=\\,0.2$ or 0.6 (see Xie et al. (2017), Table 1 and Figures 1 and 2) makes it evident that this is under-weighed to represent the missing mass, which typically corresponds to larger discounts (0.75 or higher), due to the heavy Zipfian nature of language. What can we deduce from this? If $\\nu(x)$ is the true missing mass in the Kneser\u2013Ney model, then we can understand this choice as $\\gamma(x)=\\lambda\\nu(x)$ . As a result, we have: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\tilde{c}(x,y)=(1-\\lambda)c(x,y)+\\lambda\\left[(1-\\nu(x))c(x,y)+\\nu(x)q(y)c(x)\\right]\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Upon close examination, we identify this as an interpolation between the data on the left and the Kneser\u2013Ney bigram on the right, which suggests a form that is similar to IMM, in that the typical calculation of the log-loss is additionally joined by the log-loss pitted against a target model, the Kneser\u2013Ney bigram. ", "page_idx": 14}, {"type": "text", "text": "B.2 Proof of Proposition 6.1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Assume that we are optimizing the idealized noising objective of Eq. (14) \u2014 i.e., we are operating in the infinite-data regime \u2014 and let $Q^{\\star}$ be its global minimizer. Assume further that the model class for $Q$ contains the true model $P$ \u2014 i.e., we are in the realizable case. Then, there exists a choice of $\\pi$ and $P$ such that $Q^{\\star}\\neq P$ . ", "page_idx": 14}, {"type": "text", "text": "Proof. Let us first rewrite (14) below, with the addition of constants (an entropy to the first term and $P(\\boldsymbol{y}|\\boldsymbol{x})$ in the logarithm of te second term) that do not change the minimizer of this objective: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\underbrace{\\mathsf{D}(P||Q)}_{f(Q)}+\\lambda\\underbrace{\\sum_{x}\\pi(x)\\sum_{y}\\overline{{P}}(y|\\overline{{x}})\\log\\frac{P(y|x)}{Q(y|x)}}_{g(Q)}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "To simplify the notation, let $f$ refer to main objective term and $g$ refer to the noising regularization, which we have equivalently identified with the single-sample approximation of IMM. ", "page_idx": 14}, {"type": "text", "text": "We note that both $f$ and $g$ are convex in $Q$ and $f\\geq0$ . Since $f(Q)$ is minimized at $P$ , its gradient should be 0 at $P$ . $g(Q)$ is 0 at $P$ . ", "page_idx": 14}, {"type": "text", "text": "If $\\lambda g(Q^{\\dagger})$ is $-\\epsilon<0$ at some $Q^{\\dagger}$ , then along the line connecting $P$ to $Q^{\\dagger}$ , $\\lambda g(Q)$ is below the line 0 to $-\\epsilon$ . To compensate, $f$ would need to be above the line 0 to $+\\epsilon$ , which would violate the fact that the gradient of $f$ is 0 at $P$ . ", "page_idx": 14}, {"type": "text", "text": "We now numerically show that $g(Q)$ can indeed be negative at some $Q^{\\dagger}$ , for a given construction with specific choices of $\\pi$ and $P$ . ", "page_idx": 14}, {"type": "text", "text": "Consider a trigram scenario, with $\\boldsymbol{x}=(x_{-1},x_{-2})$ and where we understand the short context as ${\\overline{{x}}}=x_{-1}$ and the extended context as ${\\underline{{x}}}=x_{-2}$ . Let us rewrite the regularization term $g(Q)$ explicitly splitting the short and long contexts: ", "page_idx": 14}, {"type": "equation", "text": "$$\ng(Q)=\\sum_{x_{-1},x_{-2}}\\pi(x_{-1},x_{-2})\\sum_{y}{\\overline{{{P}}}}(y|x_{-1})\\log{\\frac{P(y|x_{-1},x_{-2})}{Q(y|x_{-1},x_{-2})}}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We search for distributions over the prediction $y$ , short context $\\overline{{x}}$ , and extended context $\\underline{{x}}$ . Consider the following resulting tensor: ", "page_idx": 15}, {"type": "equation", "text": "$$\nP(y,x_{-1},x_{-2})={\\left[\\begin{array}{l l}{\\left[0.396}&{0.003\\right]}\\\\ {0.1}&{0.05}\\end{array}\\right]}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Based on this, we get the context distribution $\\pi({\\overline{{x}}},{\\underline{{x}}})$ , the conditional $P(y\\mid x_{-1},x_{-2})$ , as well as the restricted model $\\overline{P}$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n{\\begin{array}{r}{\\pi(x_{-1},x_{-2})={\\left[\\!\\!0.4\\!\\!\\right]}\\quad\\cdot\\quad\\cdot\\quad\\pi.3{\\left[\\!\\!\\right]}}\\\\ {\\qquad\\pi\\left(x_{-1},x_{-2}\\!\\!\\right)={\\left[\\!\\!\\begin{array}{l l}{\\left[\\!\\!0.99\\!\\!\\!}&{\\left.0.01\\!\\!\\right]}\\\\ {\\left[\\!\\!0.5\\!\\!}&{\\left.0.5\\right]}\\end{array}\\!\\!\\!\\right]}}\\\\ {P(y\\mid x_{-1},x_{-2})={\\left[\\!\\!\\!\\begin{array}{l l}{\\left[\\!\\!0.01\\!\\!\\!}&{\\left.0.99\\!\\!\\right]}\\\\ {\\left[\\!\\!0.5\\!\\!}&{\\left.0.5\\right]}\\end{array}\\!\\!\\!\\right]}}\\end{array}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "These give us the following induced model: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\overline{{P}}(y\\mid x_{-1})={\\binom{0.57}{0.43}}\\begin{array}{c}{0.5}\\\\ {0.5}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Now consider the following choice of $Q^{\\dagger}$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\nQ^{\\dagger}(y\\mid x_{-1},x_{-2})={\\left[\\begin{array}{l l}{\\left[0.5}&{0.5}\\\\ {0.5}&{0.5}\\right]}\\\\ {\\left[0.5}&{0.5}\\\\ {0.5}&{0.5}\\right]}\\end{array}\\right]}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Plugging the above choice of $\\pi$ , $\\overline{P}$ , $P(y\\mid x_{-1},x_{-2})$ and $Q$ into Eq. (17) gives us a negative value of $g(Q^{\\dagger})=-1.1$ , thus completing the construction of the counterexample and the proof. \u53e3 ", "page_idx": 15}, {"type": "table", "img_path": "iW0wXE0VyR/tmp/5fedf260b36f31dd9ee481365913ed69b6588b66464bd5d6a6b8cb79b55d8c93.jpg", "table_caption": ["Table 4: Deterioration of noising vs. consistency of IMM when $\\lambda=1.5$ is fixed in the logistic regression example. This is a concrete manifestation of Propositon 6.1. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Experimental Evidence of Inconsistency One may dismiss Proposition 6.1 as being too specific of a counterexample. After all, in Figure 1, the gap between all methods does vanish. It is important therefore to emphasize that this only happens because we are being very favorable to noising. Specifically, we are decaying $\\lambda$ (the amount of noising) optimally with increasing data. This is necessary for noising, for the precise reason of Proposition 6.1: even if the target model is perfect, because noising incorrectly tracks the target model, without decaying its influence it will not only not narrow the gap, but would in fact derail the learned model. Decaying $\\lambda$ is also acknowledged as critical in the reverse knowledge-distillation literature (see for example Sec. 3 of Qin et al. (2021)). However, tuning $\\lambda$ is optional for IMM, thanks to $\\hat{Q}$ (with more extended context samples) accurately tracking the target (see the flat curves on the right column of Figure 6 in Appendix D.1.1). ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "To experimentally verify this, we ran the logistic regression example with fixed $\\lambda=1.5$ (optimal at data size 5). The results are below. IMM maintains performance comparable to Figure 1, whereas noising experiences a widening gap, and soon underperforms even the baseline. ", "page_idx": 16}, {"type": "text", "text": "B.3 Knowledge Distillation Theory ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We would like mention that recent theory elucidates how weak teachers can be useful in reverse KD (Kaplun et al., 2022). That work is specific to the classification setting and assumes that the teachers are good samplers, i.e., noisy versions of the Bayes decision, and shows that reverse-KD can effectively ensemble multiple teachers and remove the noise. This setting and assumptions do not directly apply here, however, a common insight may be that the process of inducing a model can also be thought of as ensembling, but over contexts rather than over independent resampling. Otherwise, considering that the implicit noising objective that we identify in Eq. (17) is equivalent to the explicit objective of reverse knowledge distillation, the arguments in this section equally apply as caveats for reverse-KD when the weak teacher is a restricted-context model. ", "page_idx": 16}, {"type": "text", "text": "C Implementation Details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Algorithm 1 Sampled IMM with SGD for a Model $Q$ with parameters $W$   \nInput: Tokenized data $\\left({{x}_{t}},{{y}_{t}}\\right)$ for $t=1,2,...,n$ , $k=$ sampling rate   \nOutput: IMM-trained model $Q$   \n1: repeat   \n2: $\\mathbf{\\dot{\\operatorname{\\calQ}}}(y_{t}|x_{t})\\gets\\operatorname{FEEDFORWARD}(Q,\\overline{{x}}_{t},\\underline{{x}}_{t})$   \n3: \u2207W Cross-Entropy $(Q)\\leftarrow$ BACKPROPAGATE(Q, Cross-Entropy(Q))   \n4: $\\hat{Q}(y_{t}|\\overline{{x}}_{t})\\gets0$   \n5: for all $\\boldsymbol{x}^{\\prime}\\in\\mathbf{Sample}(\\mathsf{e x t e n d}(\\overline{{\\boldsymbol{x}}}_{t}),\\boldsymbol{k})$ do   \n6: $\\hat{Q}(y_{t}|\\overline{{x}}_{t})\\gets\\hat{Q}(y_{t}|\\overline{{x}}_{t})+\\mathtt{F E E D F o R W A R D}(Q,\\overline{{x}}_{t},x^{\\prime})$   \n7: end for   \n8: $\\begin{array}{r}{\\mathsf{M}\\mathsf{M}_{t}(Q)=-\\sum_{y}\\hat{P}(y|\\overline{{x}}_{t})\\log\\hat{Q}(y|\\overline{{x}}_{t})}\\end{array}$   \n9: \u2207W IMMt(Q) \u2190BACKPROPAGATE(Q, IMMt(Q))   \n10: APPLYGRADIENTS( $\\nabla_{W}$ Cross-Entropy(Q) + \u03bb\u2207W IMMt(Q))   \n11: until convergence ", "page_idx": 16}, {"type": "text", "text": "C.1 $\\mathbf{k}$ -approximated IMM ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "For the approximation, a given short context $\\overline{{x}}_{t}$ , we take only a fixed number $k$ of samples $X_{1},\\cdot\\cdot\\cdot,X_{k}$ uniformly from extend $\\left({\\overline{{x}}}_{t}\\right)$ . The approximated $\\hat{Q}$ (Eq. (11)) can then be written as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\hat{Q}(y|\\overline{{x}})\\approx\\frac{1}{k}\\sum_{i=1}^{k}Q(y|\\overline{{x}}_{t},X_{i}\\sim\\mathsf{e x t e n d}(\\overline{{x}}_{t}))\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Indeed this is what is happening on lines 5-8 of Algorithm 1. Sampling $k$ extended contexts is only part of the solution. Another algorithmic innovation that we need is to address the task of computing the derivative of our main objective, Eq. (8), because it requires differentiating Eq. (7), where a na\u0131\u00a8ve implementation would need all $k$ instances of the LLM used to estimate $\\bar{Q}(\\bar{y}|\\overline{{x}})$ in the memory simultaneously. Fortunately, we provide a solution to this, explain in detail in Section C.2 below. ", "page_idx": 16}, {"type": "text", "text": "C.2 IMM Gradient Computation ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Eq. (7) is a cross-entropy and the $\\hat{Q}(y|\\overline{{x}})$ term that we approximated in Eq. (18) occurs inside the log term of the cross-entropy. Na\u0131\u00a8vely backpropagating through this term makes the memory complexity of backpropagation scale with the number of random samples $k$ used to approximate $\\hat{Q}(y|\\overline{{x}})$ . In ", "page_idx": 16}, {"type": "text", "text": "Input: Tokenized data $\\left({{x}_{t}},{{y}_{t}}\\right)$ for $t=1,2,...,n$ , $r=$ refresh frequency   \nOutput: IMM-trained model $Q$   \n1: repeat   \n2: $^{\\overline{{}}}Q(y_{t}|x_{t})\\gets\\operatorname{FEEDFORWARD}(Q,\\overline{{x}}_{t},\\underline{{x}}_{t})$   \n3: $\\nabla_{W}$ Cross-Entropy $(Q)\\leftarrow$ BACKPROPAGATE(Q, Cross-Entropy(Q))   \n4: if Repeats $\\%$ $r=0$ then   \n5: $\\boldsymbol{Q}_{\\dagger}^{-}\\gets\\boldsymbol{Q}$   \n6: Q\u02c6\u2020 0   \n7: for all $t=1,2,...,n$ do   \n8: for all $y$ do   \n9: $\\hat{Q}_{\\dagger}(\\bar{y}|\\overline{{x}}_{t})\\gets\\hat{Q}_{\\dagger}(y|\\overline{{x}}_{t})+\\mathrm{FEEDFORWARD}(Q_{\\dagger},x_{t})$   \n{Or use other induction mechanism, e.g., density estimator for logistic regression.}   \n10: end for   \n11: end for   \n12: NOGRADIENT $(Q_{\\dagger},{\\hat{Q}}_{\\dagger})$   \n1134:: $\\begin{array}{r}{\\widehat{|\\mathsf{M M}_{t}(\\boldsymbol{Q})}:=-\\sum_{y}\\frac{Q_{\\dagger}(y|x_{t})}{\\hat{Q}_{\\dagger}(y|\\overline{{x}}_{t})}\\hat{P}(y|\\overline{{x}}_{t})\\log Q(y|\\overline{{x}}_{t})}\\end{array}$   \n{This is not the IMM loss, but its gradient averages to the correct gradient, see Section 5.}   \n15: $\\begin{array}{r l}&{\\grave{\\nabla}_{W}|\\widetilde{\\mathsf{M M}}_{t}(Q)\\gets\\mathsf{B a c K P R o p a c a i r E}(Q,|\\widetilde{\\mathsf{M M}}_{t}(\\bar{Q}))}\\\\ &{\\underset{\\Delta_{\\gets}}{\\Delta}\\mathsf{P P L Y G R A D I E N T S}(\\nabla_{W}\\mathsf{C r o s s-E n t r o p y}(Q)+\\lambda\\nabla_{W}|\\widetilde{\\mathsf{M M}}_{t}(Q))}\\end{array}$   \n16:   \n17: until convergence ", "page_idx": 17}, {"type": "text", "text": "our experiments, it was problematic for the GPU (Nvidia V100 with 32 GB memory) to perform backpropagation for $k>6$ for the LSTM RNN. ", "page_idx": 17}, {"type": "text", "text": "Contemporary deep learning frameworks do not have the ability to sequentialize along $k$ the computation of the derivative of a log where the argument of the log is an average (or a sum) of $k$ entities. This is expected, because $\\log(A+B)$ cannot be decomposed, and therefore, sequentializing this by splitting the cost function (this cross-entropy term) is not possible. ", "page_idx": 17}, {"type": "text", "text": "Despite this, we propose a solution that may be of interest generally when such differentiation needs to be performed. This stems from the simple observation that the derivative of $\\log(f+g)$ can be represented as a weighted average of the derivatives of $\\log(f)$ and $\\log(g)$ , where the \u201ccrosstalk\u201d weights do not require differentiation: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\nabla\\log(f+g)={\\frac{\\nabla f+\\nabla g}{f+g}}={\\frac{f}{f+g}}\\nabla\\log f+{\\frac{g}{f+g}}\\nabla\\log g.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "In summary, this allows us to compute the derivatives separately for each random sample and accumulate them appropriately. ", "page_idx": 17}, {"type": "text", "text": "Sequentializing IMM gradient computations using crosstalk ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Recall that we can write an IMM component at data point $t$ (Eq. (12)) with $k$ randomly sampled extended contexts as: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathbb{M}\\mathsf{M}_{t}(Q)=\\mathsf{C r o s s-E n t r o p y}(\\hat{Q}(y|\\overline{{x}}),\\hat{P}(y|\\overline{{x}}))}}\\\\ &{=-\\displaystyle\\sum_{y}\\hat{P}(y|\\overline{{x}})\\log\\left[\\mathbf{E}_{X}\\left[Q(y_{0}|\\overline{{y}}_{-t},X)\\right]\\right]}\\\\ &{\\approx_{a}-\\displaystyle\\sum_{y}\\hat{P}(y|\\overline{{y}}_{-})\\log\\left[\\frac{1}{k}\\sum_{i=1}^{k}Q(y|\\overline{{x}}_{t},X_{i}\\sim\\mathsf{e x t e n d}(\\overline{{x}}_{t}))\\right]}\\\\ &{=-\\displaystyle\\sum_{y}\\hat{P}(y|\\overline{{x}})\\log\\left[\\frac{1}{k}\\sum_{i=1}^{k}Q(y|\\overline{{x}}_{t},x_{i})\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $a$ comes from Eq. (18). ", "page_idx": 18}, {"type": "text", "text": "The above entity cannot be directly decomposed into $k$ terms because it involves the log of a sum. Since it\u2019s the IMM component of the loss, during backpropagation, we will need its derivative with respect to the parameters of the neural model (LSTM RNN or BERT). In the next section, we see that the derivative can be decomposed into $k$ terms. ", "page_idx": 18}, {"type": "text", "text": "Denoting the set of parameters using $W$ , we can then write $\\nabla_{W}|\\mathsf{M}\\mathsf{M}(Q)$ as below. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\nabla_{x}\\operatorname{mb}(A(t))=-\\sum_{\\gamma}F_{\\gamma}(x_{0})\\nabla F_{\\kappa}\\kappa\\left[\\frac{1}{\\kappa}\\sum_{i=1}^{n}Q_{0}(x_{i},x_{i})\\right]}&{}\\\\ {\\quad-\\sum_{\\gamma}F_{\\kappa}(x_{0})\\frac{\\nabla F_{\\kappa}}{\\varepsilon}\\Bigg[\\sum_{i=1}^{n}Q_{0}(x_{i},x_{i})\\Bigg]}&{}\\\\ {\\quad-\\sum_{\\gamma}F_{\\kappa}(x_{0})\\frac{\\nabla F_{\\kappa}}{\\varepsilon}\\Bigg[\\frac{1}{\\varepsilon}\\sum_{i=1}^{n}Q_{0}(x_{i},x_{i})}\\\\ {\\quad}\\\\ {=-\\sum_{\\gamma}F_{\\kappa}(x_{0})\\frac{\\nabla F_{\\kappa}}{\\varepsilon}\\Bigg[\\sum_{i=1}^{n}\\nabla Q_{0}(x_{i},x_{i})}\\\\ {\\quad-\\sum_{i=1}^{n}Q_{0}(x_{i},x_{i})}\\\\ {\\quad}\\\\ {=-\\sum_{\\gamma}F_{\\kappa}(x_{0})\\Bigg[\\frac{1}{\\varepsilon^{2}}\\Bigg(\\frac{\\nabla F_{\\kappa}}{\\sqrt{1+\\frac{\\mathcal{N}_{\\delta}}{\\varepsilon}}}\\Bigg)\\nabla\\pi_{0}\\Bigg(g(x_{i},x_{i})\\Bigg)}\\\\ {\\quad-\\sum_{\\gamma}F_{\\kappa}(x_{0})\\Bigg[\\frac{1}{\\varepsilon^{3}}\\Bigg(\\frac{\\nabla G_{0}(x_{i},x_{i})}{\\sqrt{1+\\mathcal{N}_{\\delta}}(\\partial_{0}(x_{i},x_{i})})\\Bigg)\\nabla\\pi_{0}(g(x_{i},x_{i})\\Bigg)}\\\\ {\\quad}\\\\ {=-\\sum_{\\gamma}F_{\\kappa}(x_{0})\\Bigg[\\frac{1}{\\varepsilon^{3}}\\Bigg(\\frac{\\nabla G_{0}(x_{i},x_{i})}{\\sqrt{1+\\mathcal{N}_{\\delta}}(\\partial_{0}(x_{i},x_{i})})\\Bigg)\\nabla\\pi_{0}(g(x_{i},x_{i})\\Bigg)}\\\\ {\\quad-\\sum_{\\gamma}F_{\\kappa}(x_{0})\\Bigg[\\frac{1}{\\varepsilon^{3}}\\Bigg(\\frac{\\nabla G_{0}(x_{i},x_{i})}{\\sqrt{1+\\mathcal{N}_{\\delta}}(\\partial_{0}(x_{i\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We notationally move the derivative outside, but it is crucial that Ct,i(y) =  ik=Q1( yQ|x(ty,|xxit),xi) be treated as a constant with respect to the parameters of the LLM. We do this to show that we can effectively compute individual cross-entropies, with the $C_{t,i}(y)$ terms acting like a \u201ccrosstalk\u201d between the $k$ random samples. This exactly parallels the decomposition that we highlighted above for the case of $\\nabla\\log(f+g)$ . Note that this decomposition is not possible directly on the cross-entropy (that involves the log of a sum), but possible when we consider its gradient. ", "page_idx": 18}, {"type": "text", "text": "Overcoming limitations of contemporary frameworks to compute the crosstalk vector ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We cannot compute the above total gradient, i.e. $\\nabla_{W}|\\mathsf{M}\\mathsf{M}(Q)$ unless for every $t$ and $y$ we have $C_{t,i}(y)$ for $i\\in[1,k]$ . To this end, we implement this by partially running the feedforward computation graphs of all $k$ random samples only until the point which gives us $Q(y_{0}|\\overline{{\\boldsymbol{x}}}_{t},\\boldsymbol{x}_{i})$ for $i\\in[1,k]$ . Once we have all the $Q(y|\\overline{{x}}_{t},\\bar{x}_{i})$ , we can normalize all of them by their element-wise sum to get the weights $C_{t,i}(y)$ . These weights are then fed back into the graph, and the forward pass is completed to get a \u201ctampered\u201d cross-entropy for $i\\in[1,k]$ . The $k$ backward passes using these tampered cross-entropies give us the terms inside the sum in the last step of (20), which after summation produce $\\nabla_{W}||\\mathsf{M}\\mathsf{M}(Q))$ . ", "page_idx": 18}, {"type": "text", "text": "D Additional Experimental Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We now provide some additional detail of both the logistic regression and language modeling experiments. One minor difference between the two is that in the logistic regression case we interpolate the main loss $(1-\\lambda)$ and the IMM risk $(\\lambda)$ , to efficiently cross-validate for the choice of $\\lambda$ as we explain in Section D.1.1. In the language modeling experiments, we simply add the regularizer with the factor $\\lambda$ , while maintaining the main loss as is (factor 1). ", "page_idx": 19}, {"type": "text", "text": "D.1 Logistic Regression Toy Example ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We provide the complete details of the logistic regression toy example2. Consider a dataset with feature space $x=({\\bar{x_{1}}},x_{2},x_{3})\\in\\mathbb{R}^{3}$ containing two linearly separable classes, defined by a linear discriminant $g(x)=a x_{1}+b x_{2}+c x_{3}+d$ . We will think of features as decomposable into $\\overline{{x}}$ , the short context of restricted features, and $\\underline{{x}}$ , the extended context of remaining features. For this example, let $\\overline{{x}}=x_{1}$ and $\\underline{{x}}=\\left(x_{2},x_{3}\\right)$ . The choice of word \u201ccontext\u201d stems from features in language modeling. ", "page_idx": 19}, {"type": "text", "text": "Target restricted model Say features are sampled uniformly over a cube and that we have ample observation of the labels $y=\\mathbf{1}\\{g(x)>0\\}$ along only the short context, i.e., ample data points of the form $(x_{1},y)$ . This allows us to build an excellent restricted model to predict the label based on just $x_{1}$ , call it $\\overline{{P}}(y|x_{1})$ or ${\\overline{{P}}}(y|{\\overline{{x}}})$ . For the sake of this example, let us assume that this is the (restricted) Bayes predictor or true conditional probability. ", "page_idx": 19}, {"type": "text", "text": "Main question We then observe a few samples of all features along with labels, i.e., data points of the form $(x_{1},x_{2},x_{3},y)$ . How can we use the restricted model $\\hat{P}(y|x_{1})$ as part of the training of the full model $Q(y|x_{1},x_{2},x_{3})^{\\prime}$ ", "page_idx": 19}, {"type": "text", "text": "Induced model Say we have $n$ data points. Let us index them by $t=1,\\cdot\\cdot\\cdot,n$ , written either as $\\left({{x}_{t}},{{y}_{t}}\\right)$ or $(x_{1,t},x_{2,t},x_{3,t},y_{t})$ . The key concept of the method is that it is not $Q$ \u2019s behavior that should target that of $\\hat{P}(y|x_{1})$ , but rather the behavior of $Q$ if it were itself restricted. We call this restricted version $\\overline{{Q}}$ the induced model of $Q$ and, by marginalization, we interpret it as the average of $Q$ \u2019s predictions, when $\\underline{{x}}_{t}$ is drawn from its conditional distribution given ${\\overline{{x}}}_{t}$ . Since we typically don\u2019t have access to this distribution, we approximate it empirically. In language modeling, we could just sample from the empirical distribution of $\\underline{{x}}$ for a given $\\overline{{x}}$ . In logistic regression, this is not viable since $x_{1}$ does not repeat. We instead use a soft nearest-neighbor density estimate $\\begin{array}{r}{\\hat{f}(x_{2},x_{3}|x_{1})\\,\\propto\\,\\sum_{t=1}^{n}\\delta_{x_{2,t},x_{3,t}}(x_{2},x_{3})\\mathrm{e}^{-\\alpha|x_{1,t}-x_{1}|}}\\end{array}$ , where $1/\\alpha$ is the bandwidth of the kernel. (With cross-validation, we determine $\\alpha=1$ to be a good choice in this example.) If we let $w_{t}=\\mathrm{e}^{-\\alpha|x_{1,t}-x_{1}|}$ , the resulting induced model by marginalization is: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\overline{{{Q}}}(y|\\overline{{{x}}})=\\displaystyle\\int f(\\underline{{{x}}}|\\overline{{{x}}})Q(y|\\overline{{{x}}},\\underline{{{x}}})}}\\\\ {{\\approx\\displaystyle\\sum_{t=1}^{n}\\frac{w_{t}(x)}{\\sum_{t=1}^{n}w_{t}(x)}Q(y|\\overline{{{x}}},\\underline{{{x}}}_{t})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Just like in language modeling, to induce a model we need to to draw $\\underline{{x}}$ \u2019s from its conditional distribution given $\\overline{{x}}$ . Unlike in the discrete case where we could directly approximate the conditional distribution using the empirical counts, we need to rely here on an estimated density. ", "page_idx": 19}, {"type": "text", "text": "Matching the target The main objective of logistic regression is to minimize $\\begin{array}{r}{-\\sum_{t=1}^{n}\\log Q(y_{t}|x_{1,t},x_{2,t},x_{3,t})}\\end{array}$ , which is equivalent to Cross-Entropy $(Q)$ versus the empirical distribution. We now additionally want the induced model $\\overline{{Q}}$ to match the target restricted model $\\hat{P}$ . This gives this method the name Induced Model Matching (IMM). This requirement can be captured through a KL-divergence, and introduced as a regularizer. The result is a secondary objective, which we call the IMM risk, expressed as: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathsf{I M M}(Q)=\\sum_{t=1}^{n}\\sum_{y=0,1}\\hat{P}(y|x_{1,t})\\log\\frac{1}{\\overline{{Q}}(y|x_{1,t})}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathsf{C r o s s-E n t r o p y}(Q)+\\lambda\\,\\mathsf{I M M}(Q),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\lambda$ is the regularization trade-off and can be determined via cross-validation (details in the Appendix D.1.1). ", "page_idx": 20}, {"type": "text", "text": "IMM improves restricted-context prediction In Figure 4, for $n=10$ and 30 Monte-Carlo runs, we show how the induced model of $Q$ (i.e. $\\overline{{Q}}_{.}$ ) fairs in its prediction, compared to the target model $\\hat{P}$ . Note that without IMM (i.e. with $\\lambda=0$ ), the secondary objective is large and thus $\\overline{{Q}}$ \u2019s performance is worse. With increasing $\\lambda$ , IMM improves this performance both in average and in variance. We deduce that there is information in the accurate restricted model ${\\overline{{P}}}(y|{\\overline{{x}}})$ that is not known to $Q$ , unless we explicitly incorporate it. This shows that $Q$ gets better at predicting from a restricted context, naturally bringing up the question: does it also get better at predicting from the full context, i.e., the main objective? ", "page_idx": 20}, {"type": "image", "img_path": "iW0wXE0VyR/tmp/808b8bbc6c188a85db62e052a8afb922be767b62aff25bdca39bdc5ad382537d.jpg", "img_caption": ["Figure 4: Performance on restricted task, i.e. $I M M(Q)$ measured on models $Q$ trained using Eq. (24) as the objective with varying $\\frac{\\lambda}{1+\\lambda}$ ratio (refer to Appendix D.1.1). We stop at $\\begin{array}{r}{\\frac{\\lambda}{1+\\lambda}=0.9}\\end{array}$ = 0.9 because1+\u03bb\u03bb $\\textstyle{\\frac{\\lambda}{1+\\lambda}}=1$ would zero out the contribution of the main objective (and replacing the main objective completely is never the intention). "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "IMM improves full-context prediction In Figure 1, we compare the performance of IMM-trained $Q$ (green) to that without IMM (maroon). We sweep a range of $n$ from 2 to 50, and use a crossvalidation optimized $\\lambda$ for each (details in the Appendix D.1.1). The key observations are that: (1) IMM always improves on the baseline performance, (2) the variance of the outcomes is also typically diminished, (3) the improvement is greater with less data, but the gain across data sizes is equivalent to access to an average of $30\\%$ extra data. This and similar experiments suggest that gains are highest when the dataset size is comparable to the number of parameters. This simple scenario demonstrates how IMM effectively harnesses the benefti of accurate feature-restricted models when training full-featured models. ", "page_idx": 20}, {"type": "text", "text": "Visualizing the effect of IMM In Figure 5, we illustrate the 3-dimensional logistic regression problem. The features are samples uniformly in this box. The Bayes-optimal restricted model only uses the $x_{1}$ -coordinate, and so assigns probabilities proportionally to the blue/red areas in the illustrated slice. IMM then encourages the full logistic model to be consistent with these weights, i.e., making sure the proportion of points labeled $\\pm$ agrees with these weights at each $x_{1}$ . Intuitively, this biases the separating plane to have the right inclination/alignment with the $x_{1}$ -axis, which subsequently speeds up the learning process. ", "page_idx": 20}, {"type": "text", "text": "D.1.1 Tuning Lambda ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "As mentioned in Section 7.1, IMM always improves baseline performance. The main parameters to choose are $\\alpha$ and $\\lambda$ . For $\\alpha$ , we performed a few experiments on held-out data and determined the value of 1 to be generally adequate. For $\\lambda$ , we saw that adapting to the dataset size made a more significant difference. This makes sense, as for smaller datasets, we want more IMM contribution to compensate for the lack of data (i.e., we want a larger $\\lambda$ ). ", "page_idx": 20}, {"type": "image", "img_path": "iW0wXE0VyR/tmp/23106defa5e2e6c2a2bba9a793e83d13d7f45e5f2d8230de0e2a368539a9acc8.jpg", "img_caption": ["Figure 5: A visualization of the inductive bias brought upon by IMM in the logistic regression example. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "To determine a good schedule for $\\lambda$ as a function of dataset size $n$ , we sweep the ratio $\\frac{\\lambda}{1+\\lambda}$ (which represents the IMM coefficient as a fraction of the combined IMM and regular loss coefficients) in the range $[0,1]$ . We repeat this for learning rates 0.1 as well as 1.0, for a range of dataset sizes, from a minimum of 2 to a maximum of 50. ", "page_idx": 21}, {"type": "text", "text": "We provide our $\\lambda$ tuning plots in Figure 6. Since a learning rate of 1.0 was found to give the best results, we choose it for all the reported experiments. Looking at our plots in Figure 6, we select $\\lambda/(1+\\lambda)$ of 0.8, 0.7, 0.6 for dataset sizes, $n$ of 2, 10 and 20 respectively. These suggest a linearly decaying optimal $\\lambda/(1+\\lambda)$ , in this range of dataset sizes. For this example, doing an interpolation fit, we create our automatic $\\lambda$ schedule rule to be $\\textstyle{\\frac{\\lambda}{1+\\lambda}}=-0.0111n+0.818$ . ", "page_idx": 21}, {"type": "text", "text": "D.2 Language Modeling Experiments ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Code is available at https://github.com/uicdice/imm-language-modeling ", "page_idx": 21}, {"type": "text", "text": "D.2.1 Evidence that IMM acts through improving restricted task ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We give an illustrative example that shows the advantage that a restricted model may have, and which we would like to incorporate into the full-featured model. Clearly, our approach is only valuable if this the case. The simple experiment that we propose is to compare how well a full-featured model (LSTM) performs a restricted task, namely bigram prediction, vs. a restricted model built for that task (Kneser\u2013Ney Bigram). ", "page_idx": 21}, {"type": "image", "img_path": "iW0wXE0VyR/tmp/aa51b8265475283a6a6351a05e64f1abcee2db515ba9ea66d10e02fd49a8f98a.jpg", "img_caption": ["Figure 6: Effect of varying $\\lambda/(1+\\lambda)$ on different dataset sizes and learning rates of 0.1 and 1.0. Every point is an average of $30$ runs and the error bars are computed using $10^{t h}$ and $90^{t h}$ percentiles. "], "img_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "iW0wXE0VyR/tmp/a88b3ef6b193513c7d9f89ddde9f265dda96b43849dcf9bc043bc2a775bb5efe.jpg", "table_caption": ["Table 5: Restricted vs. Full Model on the Bigram Task (i.e., predicting the next word given only the previous one.) The small model is the Kneser\u2013Ney bigram on the PTB training set. The full model is the 1,500-dimensional 2-layer LSTM from Xie et al. (2017), trained without noising on the PTB training set. To make the LSTM predict the next word based only on the previous one, we complete the history by averaging predictions over all extended contexts in the training set (full word history) that share that short context (same previous word). In the language of the paper, this is exactly the induced bigram of the learned LSTM. We report the cross-entropy and perplexity of each model on the PTB test set. Note that the small model clearly outperforms the LSTM on this restricted task. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "How can an LSTM perform bigram prediction when given only a single word? It can do so by averaging its output over all possible long histories consistent with that word, which is exactly the notion of induced model of Eq. (3). In Section 4, we show how to do this empirically. ", "page_idx": 23}, {"type": "text", "text": "When we do this comparison for the non-noised LSTM in (Xie et al., 2017), we obtain the results tabulated in Table 5. The Kneser\u2013Ney bigram (restricted model) outperforms the LSTM (full model) on the restricted task. This may appear surprising, however, had this not been the case, the improvements that Xie et al. (2017) obtained through noising would have been hard to explain! More importantly, using IMM improves the performance of the LSTM on this task, mirroring the logistic regression case \u2014 cf. Figure 4. ", "page_idx": 23}, {"type": "text", "text": "D.2.2 Parameter details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Choice of $\\lambda$ For Language Modeling experiments, due to computational cost, we did not perform an exhaustive search for the best $\\lambda$ . We tried a few values of $\\lambda$ and tested on held-out data, and determined the value of 0.2 to work well in most cases, and adhered to it in all of the experiments. ", "page_idx": 23}, {"type": "text", "text": "Restarts We would like to clarify that the baseline for LSTM RNN (Xie et al., 2017) reports the best perplexity across multiple restarts while the baseline for BERT (Devlin et al., 2018) reports the average across multiple restarts. We do likewise in our experiments. We have additionally included error bars for BERT experiments as that\u2019s where most of the gain is seen for BERT. ", "page_idx": 23}, {"type": "text", "text": "Clipping On the technical front, we note that in either LSTM or BERT, if gradient clipping (Pascanu et al., 2013) is used, then gradients should be clipped separately for the primary objective and the IMM objective (i.e. they should be clipped before they are added together). The reason is that they are obtained from two loss functions that have very different scales. ", "page_idx": 23}, {"type": "text", "text": "Choice of $k$ To reduce overhead for LSTM, we perform IMM only every $j^{!}$ \u2019th batch using $k$ -samples. Otherwise we use 1-sample IMM. For BERT, we perform $k$ -sample IMM every minibatch. For the selection of the parameter $k$ (number of long histories used to approximate the induced model), we experimented with $k\\in\\{1,5,10,20\\}$ . For the LSTM experiments, there was not much gain going from 10 to 20, so we settled for $k=10$ (and $j=5$ ) as a good compromise between the accuracy and time complexity of the induced model computation. For the BERT experiments, we used $k=5$ . ", "page_idx": 23}, {"type": "text", "text": "IMM through reintroduced MLM loss in BERT fine-tuning BERT on GLUE differs from LSTM on PTB in two ways: (1) it is not a next-word predictive model, and (2) its main loss during fine-tuning is the task (e.g., CLS) loss. We do not touch the latter. Instead, we reintroduce the MLM loss, to have a fine-tuning predictive task close to our formulation. Since MLM is based on cross-entropy, IMM can be directly added to it as a secondary loss. Additionally, only for the computation of the IMM component, at each location of a fine-tuning training sentence, we mask that word and all future words during the forward passes being done to compute the IMM component. This forces BERT to act like a causal model, i.e., it predicts the next word based only on the past, as in our main formulation. This is done to ensure that the induced model of BERT is the right induced model to match against an accurate causal model. Incidentally, for this accurate target model we used the Kneser\u2013Ney bigram constructed from the Wikipedia data, on which BERT is initially trained. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "In summary, in the BERT experiments we fine-tune with the sum of 3 losses: the task loss, the MLM cross-entropy loss, and the IMM risk for the MLM task. ", "page_idx": 24}, {"type": "text", "text": "D.3 RL Experiment ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We consider an $11\\times11$ toroidal grid in the $(x,y)$ plane with a predefined reward landscape that has a single peak at the center of the grid, see the heatmap in Figure 7. The toroidal configuration allows the agent to wrap around the grid in case it exceeds its confines, which allows us to rely on uniformity in the action space, in any state. ", "page_idx": 24}, {"type": "image", "img_path": "iW0wXE0VyR/tmp/79ae9c795b75a20999830475c37ae6cca1536ed58b040ce14c8465c53a152c12.jpg", "img_caption": ["Figure 7: Heat map representing the reward function, which depends on state only, on the $11\\times11$ toroidal grid of the RL experiment. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "We model this as both a Markov Decision Process (MDP) as well as a Partially Observable Markov Decision Process (POMDP). The MDP agent knows both the $x$ and the $y$ coordinates of position, while the POMDP agent knows only $x$ but maintains a uniform belief over the $y$ dimension. We first solve the POMDP using the POMDPs. $\\mathrm{j}2$ (Egorov et al., 2017) package package and the Fast Informed Bound (FIB) Solver (Kochenderfer et al., 2022) to obtain a very accurate policy based on partial information. (These POMDP policies are represented in the form of alpha vectors.) ", "page_idx": 24}, {"type": "text", "text": "MDP Training without IMM Our MDP training algorithm is REINFORCE (Williams, 1992). REINFORCE, at every epoch, samples a set of observations, which can also be called a batch. In REINFORCE, at every epoch we sample fresh data from the policy. We elect to limit the observation length at every epoch, to emulate limited exploration, and hence the number of epochs can be seen as the effective dataset size. ", "page_idx": 24}, {"type": "text", "text": "Adding IMM to REINFORCE For every observation, we consider its restricted context to be $x$ (the known dimension of the POMDP agent). Unlike language modeling experiments (where we randomly sample for the extended context), we use the POMDP belief. Here, we thus use a uniform distribution over the unknown dimension $y$ (the analog of the extended context). Using this distribution, we compute our policy\u2019s induced model. We then match this induced model against the POMDP agent\u2019s action, which is an action based solely on the knowledge of $x$ (and only a belief over $y\\}$ ). We manually set $\\lambda$ to 0.25. ", "page_idx": 24}, {"type": "text", "text": "Experimental Results We set the per epoch observation length to 50. Since then the number of epochs represents our dataset size control, we vary it and compare the performance of REINFORCE and IMM-augmented REINFORCE3 by evaluating their average reward over a given rollout horizon. ", "page_idx": 24}, {"type": "text", "text": "Similarly to the logistic regression experiments, we see that IMM is uniformly beneficial, with gains at their best when the dataset size is roughly in the regime of the number of parameters. We perform 30 Monte Carlo runs at each number of epochs, and report 10th and 90th percentiles. Here also, IMM reduces the variance of the reward. Results are reported in Figure 2. ", "page_idx": 25}, {"type": "text", "text": "D.4 Effect of Restricted Model Quality ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "While the paper is presented under the assumption that we have access to a very good target model $\\hat{P}$ , we also ran experiments to show the effect of model quality on IMM. Of course, for the language model, we don\u2019t have a gold reference and the fact that IMM achieves gains and improves on noising is evidence that even non-perfect models help. However, for the logistic regression and RL experiments we do have gold references, which we can weaken. We weaken the restricted Bayes-optimal predictor in the regression example by adding Bernoulli noise to the label (interpolate with a coin filp), $20\\%$ for the medium quality model and $50\\%$ for the low quality model. For the RL experiment, we replace the max on the POMDP utility with a softmax, and adjust the quality via the temperature of the softmax (small is higher quality). Even with reduced model quality, we find out that IMM always improves learning. However, for this to happen, we need to tune $\\lambda$ more carefully than when the model is perfect, leading to less reliance on the IMM risk when the dataset size increases (otherwise the noise in the lower quality models would offset the convergence). For logistic regression, this is illustrated in Figure 8. For RL, this is illustrated in Figure 9. ", "page_idx": 25}, {"type": "image", "img_path": "iW0wXE0VyR/tmp/7331c99d58d71758ce1926cd994eb3a34241b9938bde08ce07f833d89947ee56.jpg", "img_caption": ["Figure 8: Test accuracy of logistic regression model trained without IMM and with IMM using restricted model of varying quality levels, with \u03bb determined through dataset size (refer to Appendix D.1.1). At every dataset size, we perform 300 runs and plot the 10th and 90th percentiles for error bars. "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "iW0wXE0VyR/tmp/841ef9f5cfdf8931b6559d3285f446d7a1eb71d6f842c5002469a2961608e6dc.jpg", "img_caption": ["Figure 9: Average reward of MDP trained without and with IMM incorporating POMDP solutions of various \u201cqualities\u201d. Details in Appendix $D$ . "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "All experiments in this section use the Sampled IMM variant. In Appendix D.5, we show that Serialized IMM achieves competitive statistical performance with ", "page_idx": 25}, {"type": "text", "text": "D.5 Serialized IMM Performance ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "To demonstrate the validity of the Serialized IMM variant, which comes with computational advantages that can allow IMM to scale, we implemented it for the logistic regression example. However, we first elaborate on the details of the Sampled IMM, to show the dramatic computational advantage. By combining the density estimation approach for obtaining the induced model, as outlined in Section 7.1 with the sequentialization technique explained in Appendix C.2, we obtain the following gradient calculation for the IMM risk: ", "page_idx": 26}, {"type": "equation", "text": "$$\nJ=-\\frac{1}{n}\\sum_{t=1}^{n}\\sum_{y}\\overline{{P}}(y|\\overline{{x}}_{t})\\log\\hat{Q}(y|\\overline{{x}}_{t})\n$$", "text_format": "latex", "page_idx": 26}, {"type": "image", "img_path": "iW0wXE0VyR/tmp/314bbf959aa7ee3748fdd50ea82a1a5062b82507fd4b8f968af296ec2952cd0f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Here $w_{t,t^{\\prime}}$ uses the index of $x_{1}$ , rather than its value, as argument. By using a density estimator that looks at the entire dataset, we have a sampling scheme that effectively has set $k\\,=\\,n$ , costing an $n$ -fold computational time increase. While we can address this through shortcuts (e.g., sparsifying $w$ ), this extreme version of computing the induced model is revealing in terms of where the bottleneck is, i.e., computing $\\hat{Q}$ . ", "page_idx": 26}, {"type": "text", "text": "For the Serialized IMM variant, by comparing the above to Eq. (13), we see the large gain thanks to the double summation collapsing into a single summation. We still need to compute $\\hat{Q}$ for the correction factor, but we can do this every $n$ iterations. By incurring the ${\\mathcal{O}}(n)$ slowdown only $\\frac{1}{n}$ of the time, we go back to only a constant factor overhead. The additional noise in the gradient and the fact that $\\hat{Q}$ is stale between updates mean that Serialized IMM cannot be expected to perform at the same level as Sampled IMM. Despite this, we see in Figure 10 that Serialized IMM remains better than noising and often closely competes with Sampled IMM. Note that for ease of reproducibility, the same $\\lambda$ tuning was used for these results and no particular optimization tweaks were made to compensate for gradient noise, suggesting that performance could be improved by making such changes. ", "page_idx": 26}, {"type": "image", "img_path": "iW0wXE0VyR/tmp/4420fa739f7d67692e4fd174f61f9b7b45f15f27efa9f180c24f658f47fdc4ee.jpg", "img_caption": ["Figure 10: Serialized IMM vs. Sampled IMM and Noising. Serialized IMM provides $O(k)$ -times computational speedup over Sampled IMM (where $k$ is the number of samples and, in this case, $k=$ dataset size), while closely competing with it and always improving on noising. "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "E Potential Negative Societal Impact ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Improving language models can have negative societal impact, if these language models are fine-tuned for tasks that are not aligned with positive human values. We hope that progress achieved by new methodologies such as IMM will not be put into misuse of this kind. On the contrary, we expect that any improved statistical efficiency achieved by methods like IMM to bring value when data is not as abundant, thus helping make machine learning and language modeling more impactful in underserved domains, where data is more scarce. ", "page_idx": 27}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "IMM as a generalization of noising is analytically shown in the paper, along with noising\u2019s caveats that are solved by IMM. It\u2019s application is experimentally demonstrated in the paper, in Language Modeling and Reinforcement Learning experiments, as well as a proof of concept, using Logistic Regression. ", "page_idx": 28}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "IMM, in some applications like noising, requires random sampling, because exact evaluation is not feasible. This affects the accuracy of the induced model, but regardless, IMM is able to outperform the baseline (i.e. noising). ", "page_idx": 28}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "The only proof is the one that discusses the caveat of noising through a counter example.   \nAll assumptions and a complete proof is present. ", "page_idx": 28}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Code for all experiments has been anonymously released with clear instructions for reproducibility. A summary of relevant experimental details is also contained within the paper. ", "page_idx": 28}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Code for all experiments has been anonymously released with clear instructions for reproducibility. A summary of relevant experimental details is also contained within the paper. ", "page_idx": 28}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "A summary of relevant experimental details is also contained within the paper. ", "page_idx": 28}, {"type": "text", "text": "7. Experiment Statistical Significance ", "page_idx": 28}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Error bars have been provided along with the number of Monte Carlo runs performed to obtain each bar. ", "page_idx": 28}, {"type": "text", "text": "8. Experiments Compute Resources ", "page_idx": 29}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] We mention the GPU used (Nvidia V100 with 32 GB memory) ", "page_idx": 29}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Improving language models can have negative societal impact, if these language models are fine-tuned for tasks that are not aligned with positive human values. We hope that progress achieved by new methodologies such as IMM will not be put into misuse of this kind. ", "page_idx": 29}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Appendix E addresses this. ", "page_idx": 29}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "None of our datasets are scrapped. Pre-trained checkpoints are from reliable sources (Google trained checkpoint). ", "page_idx": 29}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "All our code (both original and derivative) is Apache 2.0 licensed and a copy of this license is included in all code repositories. Datasets are public domain. ", "page_idx": 29}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "The only new asset is code. Code for all experiments has been anonymously released with clear instructions for reproducibility. A summary of relevant experimental details is also contained within the paper. ", "page_idx": 29}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] No human subjects were involved. ", "page_idx": 29}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] .   \nNo human subjects were involved. ", "page_idx": 30}]