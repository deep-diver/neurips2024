[{"figure_path": "iW0wXE0VyR/figures/figures_1_1.jpg", "caption": "Figure 1: Test accuracy of logistic model, with and without IMM. Bars are 10th to 90th percentiles of 300 runs.", "description": "This figure shows the test accuracy results for a logistic regression model trained with and without Induced Model Matching (IMM).  The x-axis represents the size of the dataset used for training, and the y-axis represents the test accuracy.  Multiple runs (300) were conducted, and the bars in the graph represent the 10th to 90th percentiles of the accuracy across these runs, indicating the variance in performance.  The graph compares four different training methods: IMM with a target model, noising with a target model, interpolation with a target model, and a baseline method with no IMM, noising, or interpolation. The results show that IMM consistently improves test accuracy compared to the other methods, especially with smaller datasets, suggesting its effectiveness in leveraging information from a restricted model to enhance full-featured model training.", "section": "7.1 Starting Toy Example: Logistic Regression"}, {"figure_path": "iW0wXE0VyR/figures/figures_9_1.jpg", "caption": "Figure 2: Average reward of MDP trained without and with IMM incorporating POMDP. Details in Appendix D.", "description": "The figure shows the average reward achieved by an MDP (Markov Decision Process) agent trained with and without the IMM (Induced Model Matching) method.  The MDP agent is trained using REINFORCE (Reinforcement Learning algorithm). A POMDP (Partially Observable Markov Decision Process) agent, trained on a limited observation space, is used to provide side information via the IMM method for improving the MDP training. The graph plots the average reward against the number of training epochs. Error bars represent 10th and 90th percentiles from multiple runs. The results show that incorporating the POMDP information via IMM leads to higher average reward and reduced variance.", "section": "7.3 Reinforcement Learning: POMDPs helping MDPs"}, {"figure_path": "iW0wXE0VyR/figures/figures_12_1.jpg", "caption": "Figure 3: Schematic overview of IMM.", "description": "This figure shows a schematic overview of the Induced Model Matching (IMM) process.  It illustrates how a full-featured true predictive model P(y|x) and its associated data are used to create a feature-restricted induced model P(y|x\u0304). Simultaneously, a full-featured learned predictive model Q(y|x) and its associated data are used to create a learned feature-restricted induced model Q(y|x\u0304).  IMM then matches the proxy of the true feature-restricted induced model (P(y|x\u0304)) with the learned feature-restricted induced model (Q(y|x\u0304)). The true context distribution \u03c0 and empirical context distribution \u03c0n are also shown to highlight the relationship between the true and empirical models.", "section": "Explanation of main notations"}, {"figure_path": "iW0wXE0VyR/figures/figures_20_1.jpg", "caption": "Figure 4: Performance on restricted task, i.e., IMM(Q) measured on models Q trained using Eq. (24) as the objective with varying ratio \u03bb/(1+\u03bb) (refer to Appendix D.1.1). We stop at \u03bb/(1+\u03bb) = 0.9 because \u03bb/(1+\u03bb) = 1 would zero out the contribution of the main objective (and replacing the main objective completely is never the intention).", "description": "This figure shows the performance of the induced model Q on the restricted task (using only x1) as a function of the hyperparameter \u03bb in the objective function.  The y-axis represents the IMM(Q) value, and the x-axis shows the weight given to the IMM loss (\u03bb/(1+\u03bb)). It demonstrates that as the weight on the IMM loss increases, the performance of the induced model on the restricted task improves.  The error bars show the variability across multiple runs.", "section": "7.1 Starting Toy Example: Logistic Regression"}, {"figure_path": "iW0wXE0VyR/figures/figures_21_1.jpg", "caption": "Figure 5: A visualization of the inductive bias brought upon by IMM in the logistic regression example.", "description": "This figure visualizes the inductive bias introduced by Induced Model Matching (IMM) in a 3D logistic regression example.  The data points are uniformly sampled within a cube, and the Bayes-optimal restricted model uses only the x1 coordinate, assigning probabilities proportionally to the blue/red areas in the illustrated slice.  IMM encourages the full logistic model to align with these weights, biasing the separating plane towards the correct inclination relative to the x1-axis, which speeds up learning.", "section": "7.1 Starting Toy Example: Logistic Regression"}, {"figure_path": "iW0wXE0VyR/figures/figures_22_1.jpg", "caption": "Figure 1: Test accuracy of logistic model, with and without IMM. Bars are 10th to 90th percentiles of 300 runs.", "description": "This figure shows the test accuracy results of a logistic regression model trained with and without Induced Model Matching (IMM).  The x-axis represents the size of the dataset used for training. The y-axis represents the test accuracy.  Multiple runs (300) were performed, and the bars indicate the 10th to 90th percentiles of the accuracy across those runs.  The figure demonstrates that using IMM consistently leads to higher accuracy and lower variance in the test accuracy compared to training without IMM.", "section": "7.1 Starting Toy Example: Logistic Regression"}, {"figure_path": "iW0wXE0VyR/figures/figures_24_1.jpg", "caption": "Figure 7: Heat map representing the reward function, which depends on state only, on the 11 \u00d7 11 toroidal grid of the RL experiment.", "description": "This figure shows a heatmap representing the reward function used in the reinforcement learning experiment described in the paper.  The reward is defined on an 11x11 toroidal grid, meaning the grid wraps around at the edges. The heatmap illustrates that the reward is highest in the center of the grid and decreases as the distance from the center increases.", "section": "7.3 Reinforcement Learning: POMDPs helping MDPs"}, {"figure_path": "iW0wXE0VyR/figures/figures_25_1.jpg", "caption": "Figure 8: Test accuracy of logistic regression model trained without IMM and with IMM using restricted model of varying quality levels, with \u03bb determined through dataset size (refer to Appendix D.1.1). At every dataset size, we perform 300 runs and plot the 10th and 90th percentiles for error bars.", "description": "This figure shows the test accuracy of a logistic regression model trained with and without IMM, using restricted models of varying quality (high, medium, low).  The x-axis represents the dataset size, and the y-axis shows the test accuracy. Error bars represent the 10th and 90th percentiles from 300 runs at each data point. The figure demonstrates that IMM consistently improves accuracy compared to training without it, even when using lower-quality restricted models. The improvement is most pronounced for smaller datasets.", "section": "D.4 Effect of Restricted Model Quality"}, {"figure_path": "iW0wXE0VyR/figures/figures_25_2.jpg", "caption": "Figure 2: Average reward of MDP trained without and with IMM incorporating POMDP. Details in Appendix D.", "description": "The figure shows the average reward achieved by training an MDP policy with and without using IMM.  The MDP is trained using REINFORCE, where the restricted model is a POMDP that only observes one coordinate of the agent's position on an 11x11 toroidal grid.  The x-axis represents the number of epochs (effectively the dataset size), and the y-axis shows the average reward achieved during the rollout horizon. Different lines represent the MDP trained without IMM, and the MDP trained with IMM using either the maximal utility action from the POMDP or a softmaxed POMDP policy with different temperatures. Error bars show 10th and 90th percentiles from 30 Monte Carlo runs. The plot demonstrates that incorporating information from the POMDP using IMM significantly improves the performance of the MDP, particularly with smaller datasets.", "section": "7.3 Reinforcement Learning: POMDPs helping MDPs"}, {"figure_path": "iW0wXE0VyR/figures/figures_26_1.jpg", "caption": "Figure 1: Test accuracy of logistic model, with and without IMM. Bars are 10th to 90th percentiles of 300 runs.", "description": "This figure compares the test accuracy of a logistic regression model trained with and without the Induced Model Matching (IMM) method.  The x-axis represents the size of the dataset used for training, and the y-axis shows the accuracy.  The graph displays that IMM consistently improves the accuracy of the model, particularly with smaller datasets. Error bars showing 10th to 90th percentiles over 300 runs are included to show variability. The figure provides visual evidence supporting the claim that IMM enhances model performance.", "section": "7.1 Starting Toy Example: Logistic Regression"}, {"figure_path": "iW0wXE0VyR/figures/figures_27_1.jpg", "caption": "Figure 1: Test accuracy of logistic model, with and without IMM. Bars are 10th to 90th percentiles of 300 runs.", "description": "This figure shows the test accuracy results of a logistic regression model trained with and without Induced Model Matching (IMM).  The x-axis represents the size of the dataset used for training, and the y-axis shows the test accuracy.  Multiple runs (300) were conducted, and error bars represent the 10th to 90th percentiles of the accuracy results.  The graph demonstrates that IMM improves the accuracy of the logistic regression model, especially when the training dataset is relatively small.", "section": "7.1 Starting Toy Example: Logistic Regression"}]