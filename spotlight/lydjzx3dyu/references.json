{"references": [{"fullname_first_author": "A. Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-01", "reason": "This paper introduces CLIP, a highly influential model that bridges vision and language, forming the basis for many of the vision models merged in this paper."}, {"fullname_first_author": "A. Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2020-10-26", "reason": "This paper introduces Vision Transformers (ViTs), a groundbreaking architecture that significantly improves image recognition and is a core model type in the merging experiments."}, {"fullname_first_author": "Y. Liu", "paper_title": "Roberta: A robustly optimized bert pretraining approach", "publication_date": "2019-07-19", "reason": "RoBERTa is a prominent and widely-used language model, and its inclusion highlights the paper's broader applicability beyond vision models."}, {"fullname_first_author": "M. S. Matena", "paper_title": "Merging models with fisher-weighted averaging", "publication_date": "2022-12-01", "reason": "This paper presents a prominent model merging technique using Fisher information matrices, providing a strong baseline for comparison with the proposed method."}, {"fullname_first_author": "X. Jin", "paper_title": "Dataless knowledge fusion by merging weights of language models", "publication_date": "2022-04-01", "reason": "This work explores a dataless model merging approach, aligning with the core goal of the current paper to merge models without additional data or fine-tuning."}]}