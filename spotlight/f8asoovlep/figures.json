[{"figure_path": "F8aSOovlEP/figures/figures_1_1.jpg", "caption": "Figure 1: (a): Illustration of Multi-Event Causal Discovery Task, where the 3rd and 5th premise events account for the occurrence of the final event. The objective of our task is to determine whether a causal relation exists between events and outputs a structured causal diagram. (c): Example of causality confounding. (d)&(e): Illustration of illusory causality.", "description": "This figure illustrates the Multi-Event Causal Discovery (MECD) task.  Panel (a) shows an example video with multiple events and their corresponding visual segments and textual descriptions.  The goal is to identify causal relationships between events and represent them as a structured causal diagram, shown in panel (b). Panels (c), (d), and (e) depict three challenges in MECD: causality confounding (where an intermediary event obscures the true causal relationship), illusory temporal causality (where correlation is mistaken for causation due to temporal proximity), and illusory existence causality (where the presence of an object in an earlier event is wrongly interpreted as causation).", "section": "3 Benchmark"}, {"figure_path": "F8aSOovlEP/figures/figures_3_1.jpg", "caption": "Figure 2: Constitute of MECD dataset. In (a1), we present 5 main video categories of the dataset. The word cloud is also summarized for video types. In (a2), the left chart indicates the impact of positions of events on their causality where we find the second last event tends to be more significant; while the right chart plots the number of events in videos.", "description": "This figure shows the composition of the MECD dataset.  (a1) displays five main video categories (Sports & Competition, Making & Creating, Daily Activities, Performing & Displaying, Interaction & Socializing) and a word cloud summarizing the common activities within each category.  (a2) shows two graphs: the first graph shows the relationship between the position of an event and its likelihood of having a causal relation with the final event, with the second-to-last event being the most influential; and the second graph is a histogram illustrating the distribution of the number of events per video in the dataset.", "section": "3 Benchmark"}, {"figure_path": "F8aSOovlEP/figures/figures_4_1.jpg", "caption": "Figure 3: Video Granger Causality Model. Two data streams VP and V serve as input, video and text embeddings are concatenated after being separately embedded. The VGCM incorporates two classifiers, the caption head takes the unmasked stream to accomplish the event-predicting task, while the relation head discovers the causal relations with two embedding streams.", "description": "This figure illustrates the architecture of the Video Granger Causality Model (VGCM).  The model takes video and caption data as input, processes them through separate video and caption encoders. These encodings are then fed into a multi-modal decoder, which has two branches. One branch uses all premise events (unmasked) to predict the result event, while the other masks a single premise event at a time to assess its causal impact. The results are then compared by a relation head that determines whether a causal relation exists between the premise and result events, alongside a caption head to predict the textual description of the result event.", "section": "4 Methodology"}, {"figure_path": "F8aSOovlEP/figures/figures_6_1.jpg", "caption": "Figure 4: Causal Effect of the Adjacent Events and Causality Diagram. (a1) shows the causality of the third event analyzed, the red causal effect needs to be compensated while the blue needs to be mitigated. (a2) shows the causal inference methods corresponding to the two causal effects.", "description": "This figure illustrates how causality confounding and illusory causality affect causal relationships in multi-event videos.  Panel (a1) shows a simplified causal diagram with three events (ek-1, ek, ek+1) leading to a result event (eN).  When the middle event (ek) is masked (removed from consideration), the direct causal effect from ek-1 to eN is lost (blue dotted line), and a spurious causal link between ek+1 and eN might appear (red dotted line). Panel (a2) details the methods to handle this: Frontdoor adjustment addresses the missing link from ek-1 to eN; Counterfactual intervention removes the spurious link from ek+1 to eN; and an Existence-only path addresses the issue of illusory existence causality.", "section": "4.2 Causal Inference in VGCM"}, {"figure_path": "F8aSOovlEP/figures/figures_8_1.jpg", "caption": "Figure 5: Dataset robustness. Accuracy decreases slightly when increasing noise, and increases slowly when increasing the training data.", "description": "This figure shows the robustness of the MECD model to different factors such as noise in the data and the amount of training data. The blue line shows the accuracy when a random percentage of the causal relations are flipped, simulating noise in the data. The red line shows the accuracy as the amount of training data increases. As can be seen, the model is relatively robust to both noise and variations in the amount of training data.", "section": "5.3 Robustness Analysis"}, {"figure_path": "F8aSOovlEP/figures/figures_8_2.jpg", "caption": "Figure 6: Causality discovered analysis. The similarity of masking causal premise events is obviously lower through counterfactual intervention.", "description": "This figure shows the results of an ablation study evaluating the impact of counterfactual intervention on the similarity between the output features with and without masking a causal premise event. The x-axis represents the training epoch, and the y-axis represents the ratio of similarities between output features with and without masking. The blue line represents the results with counterfactual intervention, and the red line represents the results without it. The figure demonstrates that counterfactual intervention effectively reduces the similarity between the output features when masking a causal premise event, indicating its effectiveness in mitigating causality confounding and improving the accuracy of causal discovery.", "section": "5.2 Ablation Study"}, {"figure_path": "F8aSOovlEP/figures/figures_8_3.jpg", "caption": "Figure 7: Complete causal diagram.", "description": "This figure visualizes a complete causal diagram generated by the VGCM model.  The diagram shows multiple events chronologically arranged and connected by causal links.  These links indicate how premise events causally influence the result event.  The diagram effectively demonstrates the model's ability to represent complex multi-event causal relationships in a structured format.", "section": "5.1 Main results"}, {"figure_path": "F8aSOovlEP/figures/figures_9_1.jpg", "caption": "Figure 4: Causal Effect of the Adjacent Events and Causality Diagram. (a1) shows the causality of the third event analyzed, the red causal effect needs to be compensated while the blue needs to be mitigated. (a2) shows the causal inference methods corresponding to the two causal effects.", "description": "This figure illustrates the impact of masking an intermediate event (ek) on the causal relationships between adjacent events and the final event (en).  Panel (a1) shows how masking ek can lead to either a missing causal effect (red arrow) from the event before ek (ek-1) to en, or a redundant causal effect (blue arrow) from the event after ek (ek+1) to en.  Panel (a2) demonstrates how causal inference techniques, specifically front-door adjustment and counterfactual intervention, are used to address these issues of confounding.  Front-door adjustment is used to compensate for the missing causal effect while counterfactual intervention is used to remove the redundant causal effect.  The chain of thoughts and existence-only descriptions help to mitigate illusory causality.", "section": "4.2 Causal Inference in VGCM"}, {"figure_path": "F8aSOovlEP/figures/figures_15_1.jpg", "caption": "Figure 4: Causal Effect of the Adjacent Events and Causality Diagram. (a1) shows the causality of the third event analyzed, the red causal effect needs to be compensated while the blue needs to be mitigated. (a2) shows the causal inference methods corresponding to the two causal effects.", "description": "This figure illustrates the concept of causality confounding and illusory causality and how the proposed method addresses these challenges.  Panel (a1) shows a causal chain where the removal of an intermediate event (e<sub>k</sub>) affects the causality between other events, requiring compensation (red) and removal (blue) of causal effects. Panel (a2) details the causal inference techniques (front-door adjustment and counterfactual inference) used to address these issues, resulting in a more accurate representation of causality.", "section": "4.2 Causal Inference in VGCM"}, {"figure_path": "F8aSOovlEP/figures/figures_15_2.jpg", "caption": "Figure 7: Complete causal diagram.", "description": "This figure shows a complete causal diagram generated by the VGCM model.  It visually represents the causal relationships between multiple events in a video, illustrating how premise events contribute to the final result event. The diagram provides a comprehensive and structured overview of the causal chain, allowing for a clearer understanding of the complex interactions between various events.", "section": "5.1 Main results"}, {"figure_path": "F8aSOovlEP/figures/figures_15_3.jpg", "caption": "Figure 4: Causal Effect of the Adjacent Events and Causality Diagram. (a1) shows the causality of the third event analyzed, the red causal effect needs to be compensated while the blue needs to be mitigated. (a2) shows the causal inference methods corresponding to the two causal effects.", "description": "This figure illustrates the causal effects of adjacent events when a premise event (ek) is masked in the Video Granger Causality Model (VGCM).  Panel (a1) shows how masking ek disrupts the causal relationships, causing a confounding effect.  The red arrows represent causal effects that are lost (missing) because ek is masked, requiring compensation. The blue arrows represent causal effects that are redundant because ek is masked and can be removed. Panel (a2) depicts the causal inference methods, front-door adjustment and counterfactual inference, used by VGCM to compensate for the missing effects and remove redundant ones.  The introduction of chain of thoughts and existence-only descriptions are also highlighted to address illusory causality.", "section": "4.2 Causal Inference in VGCM"}, {"figure_path": "F8aSOovlEP/figures/figures_16_1.jpg", "caption": "Figure 10: Failure abduction examples of GPT-4. Many failure cases of GPT's causal reasoning are due to confusion with illusions and the conflation of subjective emotions with objective laws.", "description": "This figure showcases instances where GPT-4 incorrectly identifies causal relationships in videos. The errors stem from two main sources: confusing correlations with causality (illusory causality) and misinterpreting emotional expressions as causal factors.  The examples highlight how GPT-4 struggles to differentiate between objective causal links and subjective interpretations of events, leading to inaccurate causal inferences. ", "section": "B.2 Failure abduction examples of GPT-4"}, {"figure_path": "F8aSOovlEP/figures/figures_16_2.jpg", "caption": "Figure 1: (a): Illustration of Multi-Event Causal Discovery Task, where the 3rd and 5th premise events account for the occurrence of the final event. The objective of our task is to determine whether a causal relation exists between events and outputs a structured causal diagram. (c): Example of causality confounding. (d)&(e): Illustration of illusory causality.", "description": "This figure illustrates the Multi-Event Causal Discovery (MECD) task.  Panel (a) shows an example of the task, where multiple premise events (events leading up to the final event) are shown chronologically, along with their corresponding video segments and textual descriptions. The goal is to determine causal relationships between the premise events and the final result event, and represent these relationships in a causal diagram. Panels (c), (d), and (e) illustrate confounding and illusory causality, which are challenges the MECD task addresses.", "section": "3 Benchmark"}, {"figure_path": "F8aSOovlEP/figures/figures_17_1.jpg", "caption": "Figure 3: Video Granger Causality Model. Two data streams VP and V serve as input, video and text embeddings are concatenated after being separately embedded. The VGCM incorporates two classifiers, the caption head takes the unmasked stream to accomplish the event-predicting task, while the relation head discovers the causal relations with two embedding streams.", "description": "This figure illustrates the architecture of the Video Granger Causality Model (VGCM).  The model takes video and caption data as input, processes them through separate encoders, and then fuses the information in a multi-modal decoder. Two heads, a caption head and a relation head, are used for event prediction and causal relation discovery, respectively. The caption head predicts the result event based on the unmasked premise events, while the relation head identifies causal relationships by comparing predictions with and without masking premise events. This allows for the determination of causal links between events. ", "section": "4 Methodology"}, {"figure_path": "F8aSOovlEP/figures/figures_18_1.jpg", "caption": "Figure 13: The trend chart of inference accuracy as the number of examples changes under the In Context Learning paradigm. Accuracy increases slightly when increasing the number of few-shot examples, when the number of examples > 3, the accuracy tends to remain constant.", "description": "This figure shows how the accuracy of the model changes as the number of examples in few-shot learning increases.  The x-axis represents the number of examples used, and the y-axis represents the top-1 accuracy.  The accuracy shows a slight increase with an increasing number of examples until it plateaus after around 3 examples. This suggests that adding more examples beyond a certain point provides minimal additional benefit to the model's performance.", "section": "C.2 Adequacy of the prompts provided to GPT-4"}]