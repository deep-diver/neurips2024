[{"figure_path": "go4zzXBWVs/tables/tables_6_1.jpg", "caption": "Table 1: TransCLIP atop inductive vision-language zero-shot and popular few-shot methods.", "description": "This table presents the results of applying TransCLIP to several vision-language models.  It compares the performance of inductive zero-shot and few-shot methods (CLIP, CoOp, TIP-Adapter-F, PLOT, and TaskRes, ProGrad) with and without the TransCLIP module across eleven datasets. The table shows that TransCLIP consistently improves the top-1 accuracy of these methods, highlighting its effectiveness as a plug-and-play module for enhancing the performance of vision-language models.", "section": "4.1 Main results"}, {"figure_path": "go4zzXBWVs/tables/tables_6_2.jpg", "caption": "Table 1: TransCLIP atop inductive vision-language zero-shot and popular few-shot methods.", "description": "This table presents the results of applying TransCLIP (a novel transductive approach) on top of several existing inductive zero-shot and few-shot methods for vision-language models.  It shows the top-1 accuracy achieved on 11 datasets (ImageNet and 10 fine-grained datasets) when TransCLIP is used as a plug-and-play module.  The results are broken down by the base inductive method used (CLIP, COOP, TIP-Adapter-F, PLOT, and TaskRes) and the number of shots (0, 1, 4, 16) used in few-shot settings. The table demonstrates how TransCLIP improves the accuracy of existing methods across different datasets and shot settings.", "section": "4.1 Main results"}, {"figure_path": "go4zzXBWVs/tables/tables_7_1.jpg", "caption": "Table 1: TransCLIP atop inductive vision-language zero-shot and popular few-shot methods.", "description": "This table presents the results of using TransCLIP on top of various inductive vision-language models.  It shows the improvement in top-1 accuracy achieved by TransCLIP when added to different zero-shot and few-shot methods. The table compares performance across multiple datasets, using several different encoder sizes (ResNet-50, ResNet-101, ViT-B/32, ViT-B/16, EVA-CLIP 8B) and  different few-shot methods (CoOp prompt tuning, TIP-Adapter-F, PLOT, TaskRes, ProGrad). The 'Average' column provides the average accuracy across the eleven datasets shown.", "section": "4.1 Main results"}, {"figure_path": "go4zzXBWVs/tables/tables_7_2.jpg", "caption": "Table 1: TransCLIP atop inductive vision-language zero-shot and popular few-shot methods.", "description": "This table presents the results of applying TransCLIP to several popular inductive zero-shot and few-shot methods.  It demonstrates the improvement in top-1 accuracy achieved by adding TransCLIP as a plug-and-play module. The table shows results across multiple datasets and different encoder sizes, highlighting the consistent performance gains provided by the TransCLIP method.", "section": "4.1 Main results"}, {"figure_path": "go4zzXBWVs/tables/tables_8_1.jpg", "caption": "Table 5: Performance and runtime comparison between TransCLIP and prompt learning solutions on average over ImageNet and the 10 fine-grained classification datasets. UPL* is a transductive adaptation of the original unsupervised procedure in [25], more details in Appendices C.1 and C.5.", "description": "This table compares the performance and runtime of TransCLIP-ZS against UPL*, a transductive adaptation of an unsupervised prompt learning method, in a zero-shot setting.  It highlights TransCLIP-ZS's significant speed advantage while maintaining comparable performance.", "section": "4 Experiments"}, {"figure_path": "go4zzXBWVs/tables/tables_8_2.jpg", "caption": "Table 5: Performance and runtime comparison between TransCLIP and prompt learning solutions on average over ImageNet and the 10 fine-grained classification datasets. UPL* is a transductive adaptation of the original unsupervised procedure in [25], more details in Appendices C.1 and C.5.", "description": "This table compares the performance and runtime of TransCLIP-ZS and TransCLIP-FS against CoOp+UPL*.  TransCLIP shows significant performance gains with substantially faster runtime.  The comparison highlights the efficiency advantage of TransCLIP.", "section": "4 Experiments"}, {"figure_path": "go4zzXBWVs/tables/tables_8_3.jpg", "caption": "Table 1: TransCLIP atop inductive vision-language zero-shot and popular few-shot methods.", "description": "This table presents the results of applying TransCLIP, a novel transductive approach, on top of several inductive zero-shot and few-shot vision-language models.  It shows the top-1 accuracy achieved on 11 datasets (ImageNet, SUN397, Aircraft, EuroSAT, StanfordCars, Food101, Pets, Flower102, Caltech101, DTD, UCF101) for different vision encoders (ResNet-50, ResNet-101, ViT-B/32, ViT-B/16, EVA-CLIP) and different few-shot learning methods (zero-shot, CoOp, TIP-Adapter-F, PLOT, TaskRes, ProGrad). The table demonstrates the consistent improvements in accuracy obtained by adding TransCLIP as a plug-and-play module, showcasing its effectiveness in enhancing the performance of various VLMs.", "section": "4 Experiments"}, {"figure_path": "go4zzXBWVs/tables/tables_9_1.jpg", "caption": "Table 7: Performance of TransCLIP-ZS for increasingly large VLMs. Relative \u0394 is the improvement normalized by the zero-shot error: (ACCTransCLIP - ACCZERO-SHOT) / (100 - ACCZERO-SHOT).", "description": "This table shows the performance of TransCLIP-ZS on three different sizes of vision-language models (VLMs) on ImageNet and an average of 11 datasets.  It demonstrates the effectiveness of TransCLIP-ZS across various model scales by showing the increase in top-1 accuracy and the relative improvement compared to the original zero-shot performance.  Larger models generally exhibit a larger absolute improvement, but the relative improvement is roughly consistent.", "section": "4 Experiments"}, {"figure_path": "go4zzXBWVs/tables/tables_18_1.jpg", "caption": "Table 1: TransCLIP atop inductive vision-language zero-shot and popular few-shot methods.", "description": "This table presents the performance improvement achieved by applying TransCLIP (a novel transductive approach) on top of various inductive zero-shot and few-shot learning methods.  It showcases the improvements in Top-1 accuracy across 11 different datasets when TransCLIP is added as a plug-and-play module. The results are broken down by the number of shots used (zero-shot, 1-shot, 4-shot, and 16-shot) and the base inductive method used (CLIP with various vision encoders, COOP, TIP-Adapter-F, PLOT, and TaskRes). The table demonstrates the consistent improvement TransCLIP provides across different datasets and inductive methods.", "section": "4.1 Main results"}, {"figure_path": "go4zzXBWVs/tables/tables_19_1.jpg", "caption": "Table 1: TransCLIP atop inductive vision-language zero-shot and popular few-shot methods.", "description": "This table presents a comparison of the performance of TransCLIP when used in conjunction with several inductive vision-language models.  It shows the top-1 accuracy achieved by different zero-shot and few-shot methods (CLIP, CoOp, TIP-Adapter-F, PLOT, and TaskRes) on 11 image classification datasets, both with and without the addition of TransCLIP as a module. The results demonstrate the consistent improvement in accuracy that TransCLIP provides across various models and datasets.", "section": "4.1 Main results"}, {"figure_path": "go4zzXBWVs/tables/tables_19_2.jpg", "caption": "Table 1: TransCLIP atop inductive vision-language zero-shot and popular few-shot methods.", "description": "This table presents the results of using TransCLIP on top of several inductive vision-language models.  It compares the performance of zero-shot and few-shot (1, 4, and 16-shot) methods on ImageNet and ten other fine-grained datasets, showcasing the improvement achieved by adding TransCLIP. The methods compared are CLIP, CoOp, TIP-Adapter-F, PLOT, TaskRes, and ProGrad, highlighting TransCLIP's consistent improvement across various base models and settings.", "section": "4 Experiments"}, {"figure_path": "go4zzXBWVs/tables/tables_20_1.jpg", "caption": "Table 1: TransCLIP atop inductive vision-language zero-shot and popular few-shot methods.", "description": "This table presents the results of applying TransCLIP to various vision-language models, both zero-shot and few-shot learning methods, across 11 datasets.  It shows the improvement in top-1 accuracy achieved by adding TransCLIP as a plug-and-play module. The table compares the performance of several popular inductive models (CLIP, CoOp, TIP-Adapter-F, PLOT, TaskRes, and ProGrad) with and without TransCLIP, demonstrating consistent performance improvements across different model architectures and learning scenarios.", "section": "4.1 Main results"}, {"figure_path": "go4zzXBWVs/tables/tables_20_2.jpg", "caption": "Table 1: TransCLIP atop inductive vision-language zero-shot and popular few-shot methods.", "description": "This table presents the results of applying TransCLIP to various inductive zero-shot and few-shot vision-language models.  It shows the top-1 accuracy achieved on 11 different datasets (ImageNet, SUN397, Aircraft, EuroSAT, StanfordCars, Food101, Pets, Flower102, Caltech101, DTD, UCF101) for different model sizes (ViT-B/16) and different adaptation approaches (zero-shot, 1-shot, 4-shot). The table compares the performance of the base models with TransCLIP-ZS (zero-shot TransCLIP) applied to them.  This demonstrates the improvement TransCLIP provides.", "section": "4 Experiments"}, {"figure_path": "go4zzXBWVs/tables/tables_21_1.jpg", "caption": "Table 1: TransCLIP atop inductive vision-language zero-shot and popular few-shot methods.", "description": "This table presents the results of TransCLIP when used on top of several inductive zero-shot and few-shot methods.  It compares the top-1 accuracy across eleven datasets (ImageNet, SUN397, Aircraft, EuroSAT, Cars, Food101, Pets, Flowers101, Caltech101, DTD, UCF101) for various vision encoders (ResNet-50, ResNet-101, ViT-B/32, ViT-B/16, EVA-CLIP). The comparison is done for different shot settings (zero-shot, 1-shot, 4-shot, and 16-shot). Each row shows the baseline accuracy of a specific method and the improvement provided by adding TransCLIP. This allows evaluating the efficacy of TransCLIP in boosting various existing VLM methods.", "section": "4 Experiments"}, {"figure_path": "go4zzXBWVs/tables/tables_21_2.jpg", "caption": "Table 1: TransCLIP atop inductive vision-language zero-shot and popular few-shot methods.", "description": "This table presents the results of using TransCLIP (a novel transductive approach) on top of several inductive zero-shot and few-shot methods for vision-language models.  It shows the top-1 accuracy achieved on 11 different datasets, comparing the performance of the baseline methods alone and with the addition of TransCLIP-ZS. The baseline methods include CLIP (zero-shot), CoOp (1-shot and 4-shot), TIP-Adapter-F (1-shot and 4-shot), PLOT (1-shot and 4-shot), and TaskRes (1-shot and 4-shot) using several different vision encoders.", "section": "4.1 Main results"}, {"figure_path": "go4zzXBWVs/tables/tables_22_1.jpg", "caption": "Table 1: TransCLIP atop inductive vision-language zero-shot and popular few-shot methods.", "description": "This table presents the results of applying TransCLIP, a novel transductive approach, on top of various inductive zero-shot and few-shot methods for vision-language models.  It shows the top-1 accuracy achieved on 11 datasets (ImageNet and 10 fine-grained datasets) when TransCLIP is used as a plug-and-play module. The table compares the performance of different base models (CLIP, CoOp, TIP-Adapter-F, PLOT, TaskRes, and ProGrad) both with and without TransCLIP.  The results demonstrate the consistent performance improvements achieved by incorporating TransCLIP across different base models and datasets.", "section": "4.1 Main results"}, {"figure_path": "go4zzXBWVs/tables/tables_22_2.jpg", "caption": "Table 1: TransCLIP atop inductive vision-language zero-shot and popular few-shot methods.", "description": "This table presents the results of using TransCLIP on top of various inductive vision-language models for zero-shot and few-shot image classification. It demonstrates the improvement in top-1 accuracy achieved by TransCLIP across multiple datasets and different model architectures (CLIP with various vision encoders and several few-shot methods).  The table allows for a comparison of TransCLIP's performance enhancement in both zero-shot and few-shot scenarios, highlighting its effectiveness as a plug-and-play module.", "section": "4.1 Main results"}, {"figure_path": "go4zzXBWVs/tables/tables_23_1.jpg", "caption": "Table 1: TransCLIP atop inductive vision-language zero-shot and popular few-shot methods.", "description": "This table presents the results of using TransCLIP on top of several inductive vision-language models for zero-shot and few-shot learning. It shows the improvement in top-1 accuracy achieved by TransCLIP across multiple datasets and various model architectures (CLIP with different backbones and popular few-shot methods such as COOP, TIP-Adapter-F, PLOT, TaskRes, and ProGrad).  The table highlights the consistent improvement offered by TransCLIP, enhancing the performance of both zero-shot and few-shot settings.", "section": "4.1 Main results"}, {"figure_path": "go4zzXBWVs/tables/tables_23_2.jpg", "caption": "Table 1: TransCLIP atop inductive vision-language zero-shot and popular few-shot methods.", "description": "This table presents the results of applying TransCLIP, a novel transductive approach, to improve the performance of several inductive vision-language models on 11 datasets.  The table compares the top-1 accuracy of several base zero-shot and few-shot methods (CLIP, CoOp, TIP-Adapter-F, PLOT, TaskRes, and ProGrad) against the results obtained after incorporating TransCLIP.  The improvement in accuracy is shown for each method and dataset, demonstrating the effectiveness of the TransCLIP module in enhancing generalization capabilities across various models and tasks.", "section": "4.1 Main results"}, {"figure_path": "go4zzXBWVs/tables/tables_24_1.jpg", "caption": "Table 1: TransCLIP atop inductive vision-language zero-shot and popular few-shot methods.", "description": "This table presents the performance comparison of TransCLIP with various inductive zero-shot and few-shot methods across eleven image classification datasets.  The results are shown as top-1 accuracy and are broken down by the number of shots (0-shot, 1-shot, 4-shot, and 16-shot) used for the few-shot methods.  The table allows for a clear comparison of how the proposed TransCLIP method enhances the performance of existing inductive baselines across different scenarios.", "section": "4.1 Main results"}, {"figure_path": "go4zzXBWVs/tables/tables_24_2.jpg", "caption": "Table 1: TransCLIP atop inductive vision-language zero-shot and popular few-shot methods.", "description": "This table presents the results of applying TransCLIP (a novel transductive approach) on top of various inductive zero-shot and few-shot vision-language models.  It shows the improvements in top-1 accuracy achieved by TransCLIP across multiple datasets (ImageNet, SUN397, Aircraft, EuroSAT, StanfordCars, Food101, Pets, Flower102, Caltech101, DTD, UCF101) and various model architectures (CLIP with different vision encoders and popular few-shot adaptation methods like CoOp, TIP-Adapter-F, PLOT, TaskRes, and ProGrad).  The table highlights TransCLIP's consistent ability to boost the performance of existing methods, demonstrating its effectiveness as a plug-and-play module.", "section": "4.1 Main results"}, {"figure_path": "go4zzXBWVs/tables/tables_25_1.jpg", "caption": "Table 1: TransCLIP atop inductive vision-language zero-shot and popular few-shot methods.", "description": "This table presents the results of applying TransCLIP to improve the performance of several inductive vision-language models on eleven datasets.  The table compares zero-shot and few-shot methods (1-shot, 4-shot, and 16-shot) with and without TransCLIP. It demonstrates the consistent improvement in top-1 accuracy achieved by adding TransCLIP as a plug-and-play module on top of existing methods.  The various vision encoder sizes used are also shown.", "section": "4.1 Main results"}, {"figure_path": "go4zzXBWVs/tables/tables_25_2.jpg", "caption": "Table 1: TransCLIP atop inductive vision-language zero-shot and popular few-shot methods.", "description": "This table presents the results of TransCLIP applied on top of several inductive zero-shot and few-shot methods.  It compares the Top-1 accuracy across 11 datasets (ImageNet, SUN397, Aircraft, EuroSAT, StanfordCars, Food101, Pets, Flower102, Caltech101, DTD, UCF101) for each method with and without TransCLIP.  The methods considered include CLIP, COOP, TIP-Adapter-F, PLOT, and TaskRes. The results show how TransCLIP improves accuracy across these methods and different datasets.", "section": "4.1 Main results"}, {"figure_path": "go4zzXBWVs/tables/tables_26_1.jpg", "caption": "Table 23: UPL* top-1 accuracy on ImageNet for 8, 16 and 32 top-confidence pseudo-labels drawn from the test set.", "description": "This table shows the top-1 accuracy achieved by the unsupervised prompt learning method (UPL*) on the ImageNet dataset.  The results are presented for different numbers of top-confidence pseudo-labels (8, 16, and 32) drawn from the test set.  The table compares the performance of ResNet-50 and ViT-B/16 architectures.  The purpose is to evaluate the effect of the number of pseudo-labels on the accuracy of the UPL* method.", "section": "4 Experiments"}, {"figure_path": "go4zzXBWVs/tables/tables_26_2.jpg", "caption": "Table 1: TransCLIP atop inductive vision-language zero-shot and popular few-shot methods.", "description": "This table presents the results of applying TransCLIP (a novel transductive approach) on top of several popular inductive vision-language models, including zero-shot and few-shot methods.  It demonstrates the consistent improvement in top-1 accuracy achieved by TransCLIP across various models and shot settings, showcasing its effectiveness as a plug-and-play module for enhancing VLM performance.  The table shows top-1 accuracy for each dataset on different learning settings (Zero-shot, 1-shot, 4-shot, 16-shot).", "section": "4.1 Main results"}, {"figure_path": "go4zzXBWVs/tables/tables_26_3.jpg", "caption": "Table 1: TransCLIP atop inductive vision-language zero-shot and popular few-shot methods.", "description": "This table presents the results of TransCLIP when used on top of several inductive zero-shot and few-shot methods.  It shows the top-1 accuracy achieved on 11 different datasets (ImageNet, SUN397, Aircraft, EuroSAT, StanfordCars, Food101, Pets, Flower102, Caltech101, DTD, UCF101) for each method, both with and without the addition of TransCLIP.  The few-shot methods included are CoOp, TIP-Adapter-F, PLOT, TaskRes, and ProGrad, each tested with 0-shot, 1-shot, 4-shot, and 16-shot settings.  The table demonstrates TransCLIP's consistent improvement across different base methods and shot settings.", "section": "4.1 Main results"}]