[{"figure_path": "go4zzXBWVs/figures/figures_0_1.jpg", "caption": "Figure 1: TransCLIP improves significantly the averaged top-1 accuracy on 11 datasets when used on top of inductive zero-shot CLIP, 2-shot CoOp prompt tuning and 2-shot TaskRes adapter for various encoder sizes.", "description": "This figure shows a grouped bar chart visualizing the performance improvement achieved by TransCLIP across different vision-language model setups.  The chart compares the top-1 accuracy on 11 datasets for three baseline approaches: zero-shot CLIP, 2-shot prompt tuning using CoOp, and 2-shot adapter using TaskRes.  Each group of bars represents a different vision encoder size (ResNet-50, ResNet-101, ViT-B/32, ViT-B/16, ViT-L/14, and EVA-CLIP 8B). The green portion of each bar shows the improvement in accuracy obtained by adding TransCLIP to the baseline method.  The results demonstrate the consistent improvement provided by TransCLIP across various models and datasets, highlighting its effectiveness as a plug-and-play module.", "section": "1 Introduction"}, {"figure_path": "go4zzXBWVs/figures/figures_18_1.jpg", "caption": "Figure 1: TransCLIP improves significantly the averaged top-1 accuracy on 11 datasets when used on top of inductive zero-shot CLIP, 2-shot CoOp prompt tuning and 2-shot TaskRes adapter for various encoder sizes.", "description": "The figure shows bar charts comparing the performance improvements achieved by TransCLIP on 11 different datasets.  The charts are broken down by the base VLM used (zero-shot CLIP, 2-shot prompt tuning with CoOp, and 2-shot TaskRes adapter) and the encoder size (ResNet-50, ResNet-101, ViT-B/32, ViT-B/16, ViT-L/14, EVA-CLIP 8B). For each combination, the chart shows the baseline accuracy and the increase in accuracy obtained by adding TransCLIP.  The improvements are substantial across different datasets and base models, highlighting TransCLIP's effectiveness in enhancing VLM performance.", "section": "1 Introduction"}]