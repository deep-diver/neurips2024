[{"figure_path": "7sdkLVuYCU/tables/tables_4_1.jpg", "caption": "Table 1: QTIP's compute-based codes (1MAD, 3INST, HYB) achieve similar distortion rates as a pure-lookup random Gaussian trellis code (RPTC) when quantizing an i.i.d Gaussian source to 2 bits. All TCQ methods (L = 16) outperform SQ and VQ and are significantly closer to the infinite-length distortion rate DR, which lower bounds the distortion a k-bit quantizer can attain.", "description": "This table compares the mean squared error (MSE) achieved by different quantization techniques when applied to an independent and identically distributed (i.i.d.) Gaussian source.  The techniques include scalar quantization (SQ), vector quantization (VQ), and various trellis coded quantization (TCQ) methods.  The table shows that high-dimensional TCQ methods using compute-based codes achieve MSE values close to the theoretical lower bound, significantly outperforming SQ and VQ.", "section": "3 QTIP"}, {"figure_path": "7sdkLVuYCU/tables/tables_6_1.jpg", "caption": "Table 3: Wikitext2 and C4 perplexity (\u2193), ctx. 4096, QTIP with pure-computed codes. Even without fine-tuning, pure-computed QTIP outperforms QuIP# and AQLM, both of which use fine-tuning, at almost all models sizes.", "description": "This table presents the results of experiments evaluating the performance of QTIP with pure-computed codes on the Wikitext2 and C4 datasets.  The perplexity, a measure of how well a language model predicts a sample, is shown for different model sizes and bit depths (2-bit, 3-bit, and 4-bit). The results highlight that even without the fine-tuning process used in QuIP# and AQLM, QTIP still surpasses the performance of these state-of-the-art methods.", "section": "4 Experiments"}, {"figure_path": "7sdkLVuYCU/tables/tables_6_2.jpg", "caption": "Table 3: Wikitext2 and C4 perplexity (\u2193), ctx. 4096, QTIP with pure-computed codes. Even without fine-tuning, pure-computed QTIP outperforms QuIP# and AQLM, both of which use fine-tuning, at almost all models sizes.", "description": "This table presents the results of experiments evaluating the performance of QTIP with pure-computed codes on the Wikitext2 and C4 datasets.  It compares the perplexity achieved by QTIP (using 1MAD and 3INST codes) at different bitrates (2-bit, 3-bit, and 4-bit) against the results from QuIP# and AQLM, which are state-of-the-art methods that do employ fine-tuning. The key finding is that QTIP consistently outperforms QuIP# and AQLM even without fine-tuning, highlighting its effectiveness in achieving high-quality quantization.", "section": "4.1 Lookup-Free Computed Codes"}, {"figure_path": "7sdkLVuYCU/tables/tables_7_1.jpg", "caption": "Table 3: Wikitext2 and C4 perplexity (\u2193), ctx. 4096, QTIP with pure-computed codes. Even without fine-tuning, pure-computed QTIP outperforms QuIP# and AQLM, both of which use fine-tuning, at almost all models sizes.", "description": "This table shows the results of experiments conducted using pure-computed codes on Wikitext2 and C4 datasets. The perplexity metric, which measures the model's ability to predict the next word in a sequence, is used to evaluate the performance of different quantization methods (QTIP, QuIP#, AQLM). The results demonstrate that QTIP, even without fine-tuning, consistently outperforms both QuIP# and AQLM, which utilize fine-tuning. This highlights QTIP's effectiveness in achieving superior quantization quality.", "section": "4.1 Lookup-Free Computed Codes"}, {"figure_path": "7sdkLVuYCU/tables/tables_7_2.jpg", "caption": "Table 2: Quantizing 4K T = 256 i.i.d Gaussian seqs. with a tail-biting (12, k, 1) trellis.", "description": "This table presents the results of quantizing 4096 independent and identically distributed (i.i.d.) Gaussian sequences of length 256 using a tail-biting trellis with various values of k (number of bits).  It compares the Mean Squared Error (MSE) achieved by Algorithm 4 (a tail-biting trellis approximation algorithm) against the optimal MSE for each value of k. The results demonstrate the accuracy of Algorithm 4 in approximating the optimal MSE for tail-biting trellis quantization.", "section": "3 QTIP"}, {"figure_path": "7sdkLVuYCU/tables/tables_7_3.jpg", "caption": "Table 3: Wikitext2 and C4 perplexity (\u2193), ctx. 4096, QTIP with pure-computed codes. Even without fine-tuning, pure-computed QTIP outperforms QuIP# and AQLM, both of which use fine-tuning, at almost all models sizes.", "description": "This table shows the Wikitext2 and C4 perplexity scores for different Llama model sizes (2-7B, 2-13B, and 2-70B) and bit depths (2, 3, and 4 bits).  It compares the performance of QTIP's pure computed codes (1MAD and 3INST) against QuIP# and AQLM.  The results demonstrate that QTIP achieves lower perplexity scores (better performance) than QuIP# and AQLM even without fine-tuning, highlighting the effectiveness of QTIP's approach.", "section": "4.1 Lookup-Free Computed Codes"}, {"figure_path": "7sdkLVuYCU/tables/tables_7_4.jpg", "caption": "Table 4: Batch size 1 decoding throughput on a RTX 6000 Ada (960GB/s mem. BW).", "description": "This table shows the inference speed of QTIP, QuIP#, and AQLM on Llama 2 7B and 70B models with matrix fusion.  It demonstrates the significant speedups achieved by QTIP and QuIP# over FP16, highlighting QTIP's ability to match QuIP#'s throughput with a much higher effective dimension.", "section": "4.3 Inference Speed"}, {"figure_path": "7sdkLVuYCU/tables/tables_8_1.jpg", "caption": "Table 3: Wikitext2 and C4 perplexity (\u2193), ctx. 4096, QTIP with pure-computed codes. Even without fine-tuning, pure-computed QTIP outperforms QuIP# and AQLM, both of which use fine-tuning, at almost all models sizes.", "description": "This table compares the performance of QTIP using pure compute-based codes (1MAD and 3INST) against QuIP# and AQLM on the Wikitext2 and C4 datasets.  The results show that QTIP, even without the fine-tuning step used by QuIP# and AQLM, achieves significantly lower perplexity scores across various model sizes and bit depths (2-bit, 3-bit, and 4-bit). This demonstrates the effectiveness of QTIP's compute-based codes in improving quantization quality.", "section": "4.1 Lookup-Free Computed Codes"}, {"figure_path": "7sdkLVuYCU/tables/tables_8_2.jpg", "caption": "Table 3: Wikitext2 and C4 perplexity (\u2193), ctx. 4096, QTIP with pure-computed codes. Even without fine-tuning, pure-computed QTIP outperforms QuIP# and AQLM, both of which use fine-tuning, at almost all models sizes.", "description": "This table presents the results of experiments evaluating the performance of QTIP with pure-computed codes on the Wikitext2 and C4 datasets.  It compares QTIP's performance to QuIP# and AQLM, highlighting QTIP's superior performance even without the fine-tuning used by the other methods.  The results are broken down by model size and bit-depth.", "section": "4.1 Lookup-Free Computed Codes"}, {"figure_path": "7sdkLVuYCU/tables/tables_9_1.jpg", "caption": "Table 3: Wikitext2 and C4 perplexity (\u2193), ctx. 4096, QTIP with pure-computed codes. Even without fine-tuning, pure-computed QTIP outperforms QuIP# and AQLM, both of which use fine-tuning, at almost all models sizes.", "description": "This table shows the results of using pure computed codes in QTIP on Wikitext2 and C4 datasets with a context size of 4096.  The table compares the perplexity scores (lower is better) achieved by QTIP's 1MAD and 3INST methods against QuIP# and AQLM across different bitrates (2, 3, 4 bits).  The key finding is that QTIP significantly outperforms the other methods even without fine-tuning, showcasing the effectiveness of its compute-based codes.", "section": "4.1 Lookup-Free Computed Codes"}, {"figure_path": "7sdkLVuYCU/tables/tables_9_2.jpg", "caption": "Table 3: Wikitext2 and C4 perplexity (\u2193), ctx. 4096, QTIP with pure-computed codes. Even without fine-tuning, pure-computed QTIP outperforms QuIP# and AQLM, both of which use fine-tuning, at almost all models sizes.", "description": "This table presents the results of experiments on the Wikitext2 and C4 datasets using QTIP with pure-computed codes.  The results are compared to QuIP# and AQLM, which both use fine-tuning.  The table shows that even without fine-tuning, QTIP achieves better perplexity scores (lower is better) across various model sizes and bit depths (2-bit, 3-bit, 4-bit).  This demonstrates the effectiveness of QTIP's pure-computed codes compared to fine-tuned VQ methods.", "section": "4.1 Lookup-Free Computed Codes"}, {"figure_path": "7sdkLVuYCU/tables/tables_9_3.jpg", "caption": "Table 9: Llama 3.2 instruct-tuned results when quantizing to 4 bits (ctx. 8192 for perplexity). Even on extremely small models, QTIP is still able to achieve meaningful compression without sacrificing quality. This table uses the same LM Eval setup as Table 8.", "description": "This table shows the results of quantizing Llama 3.2 instruct-tuned models to 4 bits, focusing on perplexity and zeroshot accuracy across various metrics (W2, ARCC, ARCE, HSWAG, PIQA).  It highlights QTIP's ability to maintain good performance even on smaller models, demonstrating effective compression without significant quality loss compared to the baseline FP16.", "section": "4.2 Hybrid Lookup-Computed Codes"}, {"figure_path": "7sdkLVuYCU/tables/tables_13_1.jpg", "caption": "Table 10: Ablation on L when quantizing Llama 2 7B to 2 bits (K = 2 and V = 1).", "description": "This table shows the results of an ablation study on the hyperparameter L (the length of the trellis) when quantizing the Llama 2 7B model to 2 bits.  The experiment keeps K (the number of bits per edge) and V (the dimension of the vector quantized per trellis step) constant while varying L. The table shows the trellis size, codebook size, total size, and resulting perplexity on the Wikitext2 and C4 datasets for different values of L. This experiment helps to understand the impact of the trellis length on both model compression and performance.", "section": "A.1.1 Ablations on Trellis Size"}, {"figure_path": "7sdkLVuYCU/tables/tables_13_2.jpg", "caption": "Table 11: Ablation on V when quantizing Llama 2 7B to 2 bits (K = 2).", "description": "This table shows the results of an ablation study on the parameter V in the QTIP algorithm, while keeping other parameters constant (K=2, Llama 2 7B, 2-bit quantization). The study investigates how varying the length of subsequences (V) used in trellis-coded quantization affects the performance of the model, in terms of perplexity scores on Wikitext2 (W2) and C4 datasets. Different trellis lengths (L) are also considered.", "section": "3.1 \"Bitshift\" Trellis and Codebook Design"}, {"figure_path": "7sdkLVuYCU/tables/tables_13_3.jpg", "caption": "Table 3: Wikitext2 and C4 perplexity (\u2193), ctx. 4096, QTIP with pure-computed codes. Even without fine-tuning, pure-computed QTIP outperforms QuIP# and AQLM, both of which use fine-tuning, at almost all models sizes.", "description": "This table presents the results of experiments using pure-computed codes (1MAD and 3INST) in QTIP for quantizing the Wikitext2 and C4 datasets on Llama 2 models with context size of 4096. It compares QTIP's performance against QuIP# and AQLM, highlighting QTIP's superior performance even without fine-tuning, particularly at 2-bit quantization.", "section": "4.1 Lookup-Free Computed Codes"}, {"figure_path": "7sdkLVuYCU/tables/tables_14_1.jpg", "caption": "Table 13: Zeroshot results for the 3INST code.", "description": "This table presents the zeroshot accuracy results obtained using the 3INST code for various bit depths (2, 3, 4, and 16 bits) and sequence lengths (7, 13, 30, 65, 70). The results are reported for five different tasks: ArcC, ArcE, BoolQ, PiQA, and Wino.  The table shows how the performance of the 3INST code varies with the number of bits used and the length of the sequences, and also allows for comparison across different tasks.", "section": "4.1 Lookup-Free Computed Codes"}, {"figure_path": "7sdkLVuYCU/tables/tables_14_2.jpg", "caption": "Table 3: Wikitext2 and C4 perplexity (\u2193), ctx. 4096, QTIP with pure-computed codes. Even without fine-tuning, pure-computed QTIP outperforms QuIP# and AQLM, both of which use fine-tuning, at almost all models sizes.", "description": "This table shows the results of using pure-computed codes in QTIP for quantizing the Wikitext2 and C4 datasets with a context size of 4096.  It compares the perplexity achieved by QTIP against QuIP# and AQLM, highlighting QTIP's superior performance even without fine-tuning, which is a crucial step in other methods. The table demonstrates QTIP's effectiveness across different model sizes and bit depths.", "section": "4.1 Lookup-Free Computed Codes"}, {"figure_path": "7sdkLVuYCU/tables/tables_14_3.jpg", "caption": "Table 5: Wikitext2 and C4 perplexity (\u2193), QTIP with the hybrid-computed code. QTIP enables high-dimensional quantization and outperforms state-of-the-art vector quantization approaches.", "description": "This table presents the results of using QTIP with the hybrid lookup-computed code on Wikitext2 and C4 datasets for different bit depths (2, 3, and 4 bits).  It compares the perplexity achieved by QTIP against the baseline (FP16) and other state-of-the-art vector quantization methods, QuIP# and AQLM.  Lower perplexity indicates better performance.  The table demonstrates that QTIP consistently achieves lower perplexity scores than competing methods across various model sizes, showcasing its effectiveness in high-dimensional quantization.", "section": "4.2 Hybrid Lookup-Computed Codes"}, {"figure_path": "7sdkLVuYCU/tables/tables_15_1.jpg", "caption": "Table 16: Wikitext2 and C4 zeroshot accuracy (\u2191), QTIP with a size 2<sup>14</sup> LUT codebook. This codebook is too large (32KB) for current GPU L1 caches, but could fit on near-future hardware.", "description": "This table shows the zeroshot accuracy results on Wikitext2 and C4 datasets using QTIP with a 2<sup>14</sup> size LUT codebook.  The codebook size is too large for current GPU L1 caches, but it's suggested that this could be feasible with near-future hardware. The results are presented for different bit depths (2, 3, 4, and 16 bits) showing the performance of QTIP under these conditions and comparing it to other methods.", "section": "4.2 Hybrid Lookup-Computed Codes"}, {"figure_path": "7sdkLVuYCU/tables/tables_15_2.jpg", "caption": "Table 4: Batch size 1 decoding throughput on a RTX 6000 Ada (960GB/s mem. BW).", "description": "This table shows the decoding throughput for different Llama models (7B and 70B) quantized with different bit-widths (2, 3, and 4 bits) using various methods (AQLM, QuIP#, and QTIP). The throughput is measured in tokens per second (Tok/s) with batch size 1 on an RTX 6000 Ada GPU.  It demonstrates the speed improvements achieved by QTIP and QuIP# over FP16 and AQLM, showcasing QTIP's ability to maintain speed even with higher-dimensional quantization.", "section": "4.3 Inference Speed"}]