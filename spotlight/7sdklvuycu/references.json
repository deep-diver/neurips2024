{"references": [{"fullname_first_author": "Jerry Chee", "paper_title": "QuIP: 2-bit quantization of large language models with guarantees", "publication_date": "2023", "reason": "This paper introduces QuIP, a 2-bit quantization method that serves as a baseline for the proposed QTIP method, highlighting the importance of incoherence processing in improving LLM quantization."}, {"fullname_first_author": "Vage Egiazarian", "paper_title": "Extreme compression of large language models via additive quantization", "publication_date": "2024", "reason": "This paper introduces AQLM, another state-of-the-art LLM quantization technique that utilizes vector quantization, which is compared to and improved upon by the proposed QTIP method."}, {"fullname_first_author": "M.W. Marcellin", "paper_title": "Trellis coded quantization of memoryless and gauss-markov sources", "publication_date": "1990", "reason": "This foundational paper introduces trellis-coded quantization (TCQ), the core technique behind QTIP, establishing its theoretical basis and demonstrating its potential for efficient high-dimensional quantization."}, {"fullname_first_author": "Stuart Lloyd", "paper_title": "Least squares quantization in pcm", "publication_date": "1982", "reason": "This seminal paper introduces the Lloyd-Max quantizer, which is a fundamental building block for many quantization algorithms and provides a theoretical framework for evaluating QTIP's performance."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Llama: Open and efficient foundation language models", "publication_date": "2023", "reason": "This paper introduces the Llama family of language models, which are used as the experimental benchmark in the QTIP paper, providing the context for evaluating the effectiveness of QTIP's quantization techniques."}]}