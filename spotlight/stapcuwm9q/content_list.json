[{"type": "text", "text": "Diffusion Model with Cross Attention as an Inductive Bias for Disentanglement ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Tao Yang1\u2217, Cuiling Lan2\u2020, Yan Lu2 , Nanning Zheng1\u2020 ", "page_idx": 0}, {"type": "text", "text": "yt14212@stu.xjtu.edu.cn, {culan, yanlu}@microsoft.com, nnzheng@mail.xjtu.edu.cn,   \n1National Key Laboratory of Human-Machine Hybrid Augmented Intelligence,   \nNational Engineering Research Center for Visual Information and Applications, and Institute of Artificial Intelligence and Robotics, Xi\u2019an Jiaotong University, 2Microsoft Research Asia, https://github.com/thomasmry/EncDiff ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Disentangled representation learning strives to extract the intrinsic factors within observed data. Factorizing these representations in an unsupervised manner is notably challenging and usually requires tailored loss functions or specific structural designs. In this paper, we introduce a new perspective and framework, demonstrating that diffusion models with cross-attention itself can serve as a powerful inductive bias to facilitate the learning of disentangled representations. We propose to encode an image into a set of concept tokens and treat them as the condition of the latent diffusion model for image reconstruction, where cross-attention over the concept tokens is used to bridge the encoder and U-Net of diffusion model. We analyze that the diffusion process inherently possesses the time-varying information bottlenecks. Such an information bottlenecks and cross-attention act as strong inductive biases for promoting disentanglement. Without any regularization term in loss function, this framework achieves superior disentanglement performance on the benchmark datasets, surpassing all previous methods with intricate designs. We have conducted comprehensive ablation studies and visualization analyses, shedding a light on the functioning of this model. We anticipate that our findings will inspire more investigation on exploring diffusion model for disentangled representation learning towards more sophisticated data analysis and understanding. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Disentangled representation learning strive to uncover and understand the underlying causal factors of observed data [1, 13]. This is believed to possess immense potential to enhance a multitude of machine learning tasks, facilitating machines to attain better interpretability, superior generalizability, controlled generation, and robustness [33]. Over the years, the field of disentangled representation learning has attracted significant academic interest and many research contributions. Numerous methods, encompassing Variational Autoencoders (VAE) based techniques (such as $\\beta$ -VAE [14, 2], FactorVAE [19]), Generative Adversarial Networks (GAN) based approaches (such as InfoGAN [5], InfoGAN-CR [23]), along with others [37, 28], have have been proposed to advance this field further. ", "page_idx": 0}, {"type": "image", "img_path": "StapcUWm9q/tmp/6f64f781232b3caa8c43a83bc443a03b6a090841f7525188fcb6444c448bb606.jpg", "img_caption": ["Figure 1: Average attention map across all time steps in stable diffusion. We draw inspiration from the process of text-to-image generation using a diffusion model with cross-attention. Utilizing the highly \u2018disentangled\u2019 words as the condition for image generation, the cross-attention maps observed from the diffusion model exhibit a strong text semantic and spatial alignment, indicating the model is capable of incorporating each individual word into the generation process for a final semantic aligned generation. This leads us to question whether such a diffusion structure could be inductive to disentangled representation learning. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Originally, Variational Autoencoders (VAEs) are conceived as deep generative probabilistic models, primarily focusing on image generation tasks [20]. The core idea behind VAEs is to model data distributions from the perspective of maximizing likelihood using variational inference. Subsequent research has revealed that VAEs possess the potential to learn disentangled representations with appropriate regularizations on simple datasets. To enhance disentanglement, a range of regularization losses have been proposed and integrated within the VAE framework [14, 19, 21]. Similarly, GANs have incorporated regularizations to enable the learning of disentangled features [5, 23, 44]. Despite significant progress, the disentanglement capabilities of these models remain less than satisfactory, and the disentangled representation learning is still very challenging. Locatello et al. demonstrate that relying solely on regularizations is insufficient for achieving disentanglement [24]. They emphasize the necessity of inductive biases on both the models and the data for effective disentanglement. A fresh perspective is eagerly anticipated to shed light on this field. ", "page_idx": 1}, {"type": "text", "text": "Recently, diffusion models have surfaced as compelling generative models known for their high sample quality [36]. Drawing inspiration from the evolution of VAE-based disentanglement methods, we are intrigued by the question of whether diffusion models, also fundamentally designed as deep generative probabilistic models, possess the potential to learn disentangled representations. Obtaining a compact and disentangled representation for a given image from diffusion models is non-trivial. Diffusion Autoencoder (Diff-AE) [26] and PDAE [42] move a step forward towards using diffusion models for representation learning by encoding the image into a feature vector, incorporating this into the diffusion generation process. However, these representations have not exhibited disentanglement characteristics. What inductive biases are essential for the learning of disentangled representations? Could we have a diffusion-based framework possessing such inductive biases? ", "page_idx": 1}, {"type": "text", "text": "Notably, in text-to-image generation, a conditional diffusion model integrates the \u2018disentangled\u2019 text tokens through cross attention, demonstrating the ability to generate semantically aligned images [30, 36, 12]. Interestingly, the observed cross-attention map reveals that different words have their corresponding spatial regions of high affinities, exhibiting strong semantic and spatial alignment as illustrated in Figure 1. These disentangled representations of \u2018word\u2019s could potentially contribute to a more streamlined generation process. Inspired by this, we wonder whether such diffusion structure with cross attention can act as an inductive bias to facilitate the disentangled representation learning. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we endeavour to investigate this question and explore the potential of diffusion models in disentangled representation learning. We discover that the diffusion model with crossattention can serve as a strong inductive bias to drive disentangled representation learning, even without any additional regularization. As illustrated in Figure 2 (a), we employ an encoder to transform an image into a set of concept tokens, which we treat as \u2018word\u2019 tokens, acting as the conditional input to the latent diffusion model with cross attention. Here, cross-attention bridges the interaction between the diffusion network and the image encoder. We refer to this scheme as EncDiff. EncDiff is powered by two valuable inductive biases, i.e., information bottleneck in diffusion, and cross attention for fostering \u2018word\u2019 (concept token) and spatial alignment, contributing to the disentanglement. Experimental results on benchmark datasets demonstrate that EncDiff achieves excellent disentanglement performance, surpassing all the previous methods with elaborate designs. Comprehensive ablation studies show that the strong disentanglement capability is mainly attributed to 1) the diffusion modelling and 2) the cross-attention interaction. Visualization analysis provides insights into the effectiveness of the disentangled representations. ", "page_idx": 1}, {"type": "image", "img_path": "StapcUWm9q/tmp/a80a5376efee64f9e1e521feea88257619dc1a8590fc27414f02d4ad3357f932.jpg", "img_caption": ["Figure 2: (a) Illustration of our framework EncDiff. We employ an image encoder $\\tau_{\\phi}$ to transform an image $I$ into a set of disentangled representations, which we treat them as the conditional input to the latent diffusion model with cross attention. Here cross attention bridges the interaction between the diffusion network and the image encoder. For simplicity, we only briefly show the diffusion model which consists of an encoder $E$ , a denoising U-Net and a decoder $D$ that reconstructs the image from the latent $x_{t}$ . (b) Information bottleneck reflected by KL divergence in reverse diffusion process. The KL divergence between the data distribution $q(\\bar{x_{t-1}}|x_{t},x_{0})$ and the Gaussian prior distribution $\\mathcal{N}(0,\\mathbf{I})$ under four different variance $(\\beta)$ schedules: cosine, linear, sqrt linear and sqrt. The results have been normalized by the number of dimensions. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "We have four main contributions. ", "page_idx": 2}, {"type": "text", "text": "\u2022 We uncover that the diffusion model with cross-attention can serve as a strong inductive bias for enabling disentangled representation learning. \u2022 We introduce a simple yet effective framework, EncDiff, powered by a latent diffusion model with cross attention and an ordinary image encoder, for disentangled representation learning. \u2022 This framework inherently incorporates two valuable inductive biases: the information bottleneck in diffusion, and cross attention, fostering concept token and spatial alignment. We analyze that the diffusion process inherently possesses the time-varying information bottlenecks. \u2022 Without additional regularization or specific designs, our framework achieves state-of-the-art disentanglement performance, even outperforming the latest methods with more complex designs. ", "page_idx": 2}, {"type": "text", "text": "We anticipate the new perspective will illuminate the field of disentanglement and inspire deeper investigations, paving the way for future sophisticated data analysis, understanding, and generation. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Disentangled Representation Learning Disentangled Representation Learning endeavours to train a model proficient in disentangling the underlying factors of observed data [1, 13, 33]. A plethora of methods have been proposed to augment the generative models of VAEs and GANs, endowing them with disentanglement capability. These methods primarily rely on probability-based regularizations applied to the latent space. To improve disentanglement, most approaches focus on how to regularize the original VAE. For instance, this includes weighting the evidence lower bound (ELBO) as in $\\beta$ -VAE [14], or introducing different terms to the ELBO, such as mutual information (InfoVAE [43], InfoMax-VAE [29]), total correlation (Factor-VAE [19]), and covariance (DIP-VAE [21]). Locatello et al. [24] demonstrate that relying solely on these regularizations is insufficient for achieving disentanglement. DIP-VAE [21] introduces a regularizer on the expectation of the approximate posterior over observed data, by matching the moments of the distributions of latents. In this work, we analyze and identify two valuable inductive biases of diffusion that promote the disentangled representation learning in our framework. Inductive biases on both the models and the data are necessary. In this paper, we investigate the disentanglement capability of diffusion models and demonstrate that diffusion models with cross-attention can serve as a powerful inductive bias for disentanglement. ", "page_idx": 2}, {"type": "text", "text": "Diffusion Models Diffusion models have emerged as a powerful new family of deep probabilistic generative models [36], surpassing VAEs and GANs for image generation and many other tasks. ", "page_idx": 2}, {"type": "text", "text": "Diffusion models progressively perturb data by injecting noise and then learn to reverse this process for the generation. A question arises as to whether diffusion models can effectively serve as disentangled representation learners. It is challenging to obtain a compact yet disentangled representation of an image from a diffusion model. Diff-AE [26] and PDAE [42] investigate the possibility of using DPMs for representation learning, whereby an input image is autoencoded into a latent vector. However, these representations do not manifest disentangled characteristics. SlotDiffusion [35] and LSD [17] integrate diffusion models into object-centric learning, where diffusion acts as a improved slot-toimage decoder, and slot attention is still the key for promoting object-centric learning. Moreover, they aim to learn object-wise representation but still cannot disentangle the factors/attributes of an object/a scene. Limited research has explored disentangled representation learning by leveraging diffusion models. InfoDiffusion [34] encourages the disentanglement of the latent feature of Diff-AE [26] by introducing mutual information and prior regularization, similar to InfoVAE [43]. CL-Dis [18] introduces a VAE to guide diffusion model to learn disentangled representation. DisDiff [40] employs a pre-trained diffusion model for disentangled feature learning. DisDiff adopts an encoder to learn the disentangled representations and a decoder to learn the sub-gradient field for each disentangled factor. It requires multiple decoders to predict these sub-gradient fields for all the factors and complex disentanglement losses, resulting in a costly and intricate process. The recent work SODA [16] leverages a diffusion model to generate novel view of image for representation learning, revealing the capability of capturing visual semantics to diffusion model. Is it necessary to impose these complicated regularizations upon diffusion models as [40]? Does a strong inductive bias facilitating disentanglement already exist within diffusion models? In this paper, we endeavour to answer these questions and illustrate that a simple framework driven by a diffusion model without any additional regularization is capable of achieving superior disentanglement performance. ", "page_idx": 3}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We aim to investigate the potential of diffusion models in disentangled representation learning. We propose a simple yet effective framework, EncDiff, that exhibits strong disentanglement capabilities, even without additional regularizations. We analyze and identify two valuable inductive biases, i.e., information bottleneck in diffusion, and the cross attention for fostering \u2018word\u2019 (concept token) and spatial alignment, thereby promoting disentanglement. We elaborate on the framework design in subsection 3.1 and the analysis in subsection 3.2, respectively. ", "page_idx": 3}, {"type": "text", "text": "3.1 Framework of EncDiff ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Figure 2 (a) illustrates the flowchart. It consists of an image encoder that transforms an input image into a set of concept tokens and a diffusion model that serves as the decoder to reconstruct the image. Cross-attention is employed as the bridge for the diffusion network and the image encoder. ", "page_idx": 3}, {"type": "text", "text": "Image Encoder For a given input image $I$ , the image encoder $\\tau_{\\phi}$ aims to provide a set of concept tokens $S=\\{s_{1},\\cdots,s_{N}\\}$ , which act similarly to the word embeddings in the prompts for text-toimage generation in the latent diffusion models (LDMs) [30]. Without loss of generality, we use an ordinary CNN network as the image encoder to obtain concept tokens. Following the design of the encoder in VAE [20], we use a fully connected layer to transform the feature map into a feature vector. We treat each dimension of the encoded feature vector as a disentangled factor and map each factor to a vector (i.e., concept token) by non-shared MLP layers (as illustrated by Figure 3). ", "page_idx": 3}, {"type": "text", "text": "Diffusion Model with Cross Attention We follow LDM [30] to construct our diffusion model in the latent space, which demonstrates superior generation ability. LDM is one of the most popular diffusion models, proposing to conduct diffusion denoising in the latent space. To condition the concept tokens during image generation, cross-attention is used to map these tokens into the intermediate representations of the U-Net in the diffusion model. This is accomplished by using the cross-attention defined as $\\begin{array}{r}{A t t e n t i o n(Q,K,V)=\\mathrm{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d}}\\right)\\cdot V,}\\end{array}$ , where the spatial feature in the intermediate feature map in diffusion model serves as a query, the concept tokens act as keys and values. ", "page_idx": 3}, {"type": "text", "text": "End-to-End Training We conduct an end-to-end training of the encoder and the diffusion model, utilizing the optimization objective of reconstructing noise, which is a methodology aligned with that employed in LDM [30]. ", "page_idx": 3}, {"type": "text", "text": "3.2 Inductive Biases ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We analyze that there are two crucial inductive biases in diffusion models: the information bottleneck in diffusion, and the cross-attention interaction. We analyze that the diffusion process inherently possesses the time-varying information bottlenecks. ", "page_idx": 4}, {"type": "text", "text": "3.2.1 Information Bottleneck in Diffusion ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "$\\beta$ -VAE [14] and AnnealVAE [2] utilize the original Kullback\u2013Leibler (KL) divergence in VAEs to enhance the disentanglement capability, where the KL divergence constraint plays a role of an information bottleneck. Here, we analyze the presence of an information bottleneck mechanism that promotes the disentanglement in diffusion models. Without loss of generality, our analysis focuses on the diffusion model in image latent space [30]. The analysis also holds in pixel space. ", "page_idx": 4}, {"type": "text", "text": "Within the framework of the latent diffusion model, we add Gaussian noise to an image latent $x_{0}$ over $T$ steps according to a variance schedule $\\beta_{1},\\cdot\\cdot\\cdot,\\beta_{T}$ . This process yields a sequence of noisy samples x1, \u00b7 \u00b7 \u00b7 , xT , ", "page_idx": 4}, {"type": "equation", "text": "$$\nq(x_{t}|x_{t-1}):=\\mathcal{N}(x_{t};\\sqrt{1-\\beta_{t}}x_{t-1},\\beta_{t}\\mathbf{I}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Let $\\alpha_{t}=1-\\beta_{t}$ and $\\begin{array}{r}{\\bar{\\alpha}_{t}=\\prod_{i=1}^{t}\\alpha_{t}.}\\end{array}$ $x_{t}$ can be obtained using the following equation [15, 36]: ", "page_idx": 4}, {"type": "equation", "text": "$$\nx_{t}=\\sqrt{\\bar{\\alpha}_{t}}x_{0}+\\sqrt{1-\\bar{\\alpha}_{t}}\\epsilon,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the noise $\\epsilon$ is sampled from a Gaussian distribution $\\mathcal{N}(0,\\mathbf{I})$ . ", "page_idx": 4}, {"type": "text", "text": "The diffusion model optimizes a network (e.g., U-Net) $\\epsilon_{\\theta}$ to predict the noise from the noisy input $x_{t}$ and the conditioning input $\\boldsymbol{S}$ (concept tokens), with the loss function defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{r}=\\mathbb{E}_{x_{0},\\epsilon,t}\\lVert\\epsilon_{\\theta}(x_{t},t,S)-\\epsilon\\rVert.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Here, we omit the weighting terms of loss function for simplicity. The latent $x_{0}$ can be reconstructed based on the predicted noises. ", "page_idx": 4}, {"type": "text", "text": "Let\u2019s analyze the inherent information bottleneck at each time step $t$ in the reverse diffusion process. In the reverse diffusion process, the reverse conditional distribution is tractable as [15]: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{q(x_{t-1}|x_{t},x_{0})=\\mathcal{N}(x_{t-1}|\\tilde{\\mu}_{t},\\tilde{\\beta}_{t}\\mathbf{I}),}\\\\ &{\\quad\\mathrm{~where}\\quad\\tilde{\\mu}_{t}=\\frac{\\sqrt{\\bar{\\alpha}_{t-1}}\\beta_{t}}{1-\\bar{\\alpha}_{t}}x_{0}+\\frac{\\sqrt{\\alpha_{t}}\\left(1-\\bar{\\alpha}_{t-1}\\right)}{1-\\bar{\\alpha}_{t}}x_{t},\\quad\\tilde{\\beta}_{t}=\\frac{1-\\bar{\\alpha}_{t-1}}{1-\\bar{\\alpha}_{t}}\\beta_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We formulate the Kullback-Leibler (KL) divergence $C_{t}$ between $q(x_{t-1}|x_{t},x_{0})$ and the Gaussian prior distribution $p(x_{t-1})=\\mathcal{N}(0,\\mathbf{I})$ at step $t-1$ as: ", "page_idx": 4}, {"type": "equation", "text": "$$\nC_{t}=D_{K L}\\big(q(x_{t-1}|x_{t},x_{0})||p(x_{t-1})\\big)=\\frac{n}{2}(-\\log\\tilde{\\beta}_{t}-1-\\tilde{\\beta}_{t}+\\tilde{\\mu}_{t}^{T}\\tilde{\\mu}_{t}/n),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $n$ denotes the number of dimension of signal $x\\left(i.e.,x_{0},x_{t},x_{t-1}\\right)$ . ", "page_idx": 4}, {"type": "text", "text": "In Figure 2 (b), we present a plot illustrating the KL divergence $C_{t}$ under different variance $(\\beta)$ schedules, including linear, sqrt linear, cosine [25, 15], and sqrt schedules [6, 30]. We can see that as the time step $t$ decreases, the KL divergence $C_{t}$ increases, indicating the information carried by $x_{t-1}$ increases and leading to an increasingly looser information bottleneck over $x_{t-1}$ . According to [2, 14], such a time-varying information bottleneck may play an important role in promoting disentanglement. Different variance $(\\beta)$ schedules results in different KL divergence curves. We found that these different variance schedules lead to different disentanglement performance (see Subsection 4.4). ", "page_idx": 4}, {"type": "text", "text": "Actually, optimizing the loss of conditional diffusion model as in (3) is equivalent to push the reverse conditional distribution $p_{\\theta}(x_{t-1}|x_{t},S)$ to approach $q(x_{t-1}|x_{t},x_{0})$ (see the explanation in Appendix B). By this means, the information bottlenecks $\\tilde{C}_{t}=D_{K L}\\big(p_{\\theta}(x_{t-1}|x_{t},S)||p(x_{t-1})\\big)$ over $x_{t-1}$ tend to approach $C_{t}=D_{K L}\\big(q(x_{t-1}|x_{t},x_{0})||p(x_{t-1})\\big)$ in training for all the time steps. According to theorem in Appendix C, the information bottleneck over latent $x_{t-1}$ is transferred to the condition $\\boldsymbol{S}$ (i.e., concept tokens). Intuitively, this is because $x_{t-1}$ is controlled by the concept tokens and the network parameters $\\theta$ , as indicated by $p_{\\theta}(x_{t-1}|x_{t},S)$ . The concept token representations $\\boldsymbol{S}$ are learnable, and the information bottleneck is transferred to and imposed on $\\boldsymbol{S}$ . With time-varying information bottlenecks, the diffusion process encourages a range of different information capacities on $\\boldsymbol{S}$ during the diffusion process, promoting the disentanglement of concept tokens. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Discussion The information bottleneck described above shares a certain similarity with the optimization objective in AnnealVAE [2]. The minimizing objective of AnnealVAE is expressed as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\phi,\\varphi)=-\\;E_{q_{\\phi}(z|x)}[\\log p_{\\varphi}(x|z)]+\\gamma\\|D_{K L}(q_{\\phi}(z|x)||p(z))-C\\|,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\phi,\\varphi$ are the parameters of the encoder and decoder; the latent representation and data are denoted as $z,x$ , respectively. $C$ is a handcrafted constant used to control the information bottleneck of the latent space. Different factors to be disentangled may contain different amounts of information. Instead of using a constant during training, AnnealVAE dynamically allocates larger amount of information (larger $C$ ) to the latent units as the training iteration increases. So that different factors can be learned at various training stages. ", "page_idx": 5}, {"type": "text", "text": "In diffusion models, where the KL divergence characterizes the information amount, we observe that the information amount varies in reverse diffusion steps, see Figure 2 (b). ", "page_idx": 5}, {"type": "text", "text": "3.2.2 Cross-Attention for Interaction ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The information bottleneck sheds a light on disentangling, acting as an inductive bias for diffusion. However, using the information bottleneck still only has theoretical feasibility. Its effectiveness also relies on the structure design of diffusion model. We believe that the cross-attention design in conditional diffusion model is crucial for disentanglement, serving as another effective inductive bias. ", "page_idx": 5}, {"type": "text", "text": "Our objective is to train an encoder that obtains a set of concept tokens taking an image as input under the guidance of the diffusion model. We take the output of the encoder as the condition of the U-Net of diffusion model for image generation. We incorporate the concept tokens into diffusion model through cross-attention. Intuitively, a spatial position of an image is related to several concepts, e.g., object color and shape in Shapes3D. Each spatial feature is composed of several related concept-based representations. Interestingly, cross-attention in the U-Net play a similar role, where each spatial feature servers as the query, and the learned concept tokens are used as the keys and values to refine the query. In subsection 4.4, we validate the necessity of the two inductive biases leading to disentanglement. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1 Experimental Setup ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Implementation Details The trainable parts are the encoder and diffusion model. We employ the popular diffusion structure of latent diffusion [30] by default. Without loss of generality, following [30], we use the VQ-reg to avoid arbitrarily high-variance latent spaces and sample images in 200 steps. We adopt the cosine as the variance $(\\beta)$ schedule in the diffusion model by default. By default, we use a CNN encoder for the image encoder to obtain a set of disentangled concept tokens. We use a CNN encoder similar to that used in [40]. We denote our scheme as EncDiff. ", "page_idx": 5}, {"type": "text", "text": "Training Details During the training phase of EncDiff, we maintain a consistent batch size of 64 across all datasets. The learning rate is consistently set to $1\\times10^{-4}$ . We adopt the standard practice of employing an Exponential Moving Average (EMA) with a decay factor of 0.9999 for all model parameters. The training hyper-parameters follows DisDiff [40] and DisCo [28]. For each concept token, we follow DisDiff [40] to use a 32 dimensional representation vector. We train EncDiff on a single Tesla V100 16G GPU. A model takes about 1 day for training. ", "page_idx": 5}, {"type": "text", "text": "Datasets To evaluate the disentanglement performance, we utilize the commonly used benchmark datasets: Shapes3D [19], MPI3D [10] and Cars3D [27]. Shapes3D [19] consists of a collection of 3D shapes. MPI3D is a dataset of 3D objects created in a controlled setting. Cars3D is a dataset consisting of 3D-rendered cars. For real-world data, we conduct our experiments using CelebA, a dataset of celebrity faces with attributes. Our experiments are carried out at a $64\\!\\times\\!64$ image resolution, consistent with previous studies [19, 4, 28, 40]. ", "page_idx": 5}, {"type": "table", "img_path": "StapcUWm9q/tmp/8541a63a3b9e525032953c18e01397a8b47cc20a5a7092e698cb13ca80ecdca0.jpg", "table_caption": ["Table 1: Comparisons of disentanglement on the FactorVAE score and DCI disentanglement metrics (mean $\\pm$ std, higher is better). EncDiff outperforms the state-of-the-art methods with a large margin except on Cars3D. "], "table_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "StapcUWm9q/tmp/e8890b5976bd334363e9e0b89f12b92b55fe52a9ff94296ff71861ae9d985905.jpg", "img_caption": ["Figure 3: Illustration of the encoder $\\tau_{\\phi}$ , which transforms an image into a feature vector of dimension $N$ , with each dimension (scalar) encoding a disentangled factor. We then use nonshared three-layer MLP layers to map each scalar into a vector (concept token). The concept tokens will be treated as the conditional input to the latent diffusion model with cross attention. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Table 2: Comparisons of disentanglement performance and generation quality in terms of TAD and FID metrics (mean $\\pm$ std) on real-world dataset CelebA. EncDiff achieves the state-ofthe-art performance on both aspects compared to all baselines. ", "page_idx": 6}, {"type": "table", "img_path": "StapcUWm9q/tmp/8dd65d4c3f34114a06105e6fb6be1b342ce98fac50a6f653e8a31d6c05f56f8a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Baselines & Metrics We compare the performance of our method with VAE-based, GAN-based, and diffusion-based methods, following the experimental protocol as in DisCo [28]. The VAE-based models we use for comparison are FactorVAE [19] and $\\beta$ -TCVAE [4], while the GAN-based baselines include InfoGAN-CR [23], GANspace (GS) [11], LatentDiscovery (LD) [32] and DisCo [28]. Each method utilizes scalar-valued representations. DisDiff [40] uses vector-valued representations. EncDiff has two kinds of representations simultaneously. We focus on the scalar-valued in the main paper. We follow DisDiff to set $N$ to 20. For these vector-valued representations, we follow [7, 40, 38] to perform PCA as a post-processing on the representation before evaluation. To assess the potential variability in performance due to random seed selection, we have fifteen runs for each method for reliable evaluation, reporting the mean and variance. Regarding evaluation metrics, we adopt two representative metrics, the FactorVAE score [19] and the DCI [8]. ", "page_idx": 6}, {"type": "text", "text": "4.2 Comparison with the State-of-the-Arts ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We compare the disentanglement ability of our EncDiff with the state-of-the-art methods. Table 1 shows quantitative comparison results of disentanglement under different metrics. We can see that EncDiff achieves the best performance on all the datasets except Cars3D, showcasing the model\u2019s superior disentanglement ability. EncDiff achieves superior performance by leveraging the strong inductive bias from the diffusion model with cross-attention without using any additional regularization losses. EncDiff also outperforms InfoDiffusion [34] and DisDiff [40] by a significant marginal, even though DisDiff uses complex disentanglement loss and inference the decoder multiple times for prediction sub-gradient fields. On the Cars3D dataset, the quantitative evaluation is not so reliable because some factors, such as color and shape, are not included in the labels. From the visualization in Figure 6 of Appendix E, we can see that EncDiff achieves superior disentanglement compared to DisDiff despite the lower FactorVAE score. ", "page_idx": 6}, {"type": "image", "img_path": "StapcUWm9q/tmp/f9d93d25cc88151a5e04b85f438b2e21b91b96d635fe45e81343d28322a4d692.jpg", "img_caption": ["Figure 4: The qualitative results on Shapes3D and MPI3D. The source (SRC) images provide the representations of the generated image. The target (TRT) image provides the representation for swapping. Other images are generated by swapping the representation of the corresponding factor. For Shapes3D, the learned factors on Shapes3D are wall color (Wall), floor color (Floor), object color (Color), and object shape (Shape), orientation (Orien), scale. See Appendix E for more visualizations. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "StapcUWm9q/tmp/ee418739c447782b3b2c5d1f569f0be27c53a8beff16ca514b63f7e7a98d72eb.jpg", "img_caption": ["Figure 5: Visualization of the cross-attention maps on Shapes3D and MPI3D. The first column shows the original image while the other columns show the attention masks for different concept tokens. See Appendix F for more visualizations. \u201cPos\u201d represents \u201cPosition\u201d. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Moreover, we have conducted experiments on real-world dataset CelebA. Table 2 shows the comparisons of disentanglement performance and generation quality in terms of TAD and FID metrics (mean $\\pm$ std). EncDiff achieves the state-of-the-art performance on both aspects compared to all baselines. ", "page_idx": 7}, {"type": "text", "text": "In addition, our EncDiff achieves superior reconstruction quality (see Appendix G for more details). ", "page_idx": 7}, {"type": "text", "text": "4.3 Visualization ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Visualization Analysis on the Disentanglement We qualitatively examine the disentanglement properties of our proposed method. We interchange the concept tokens (factors) of the learned representation of two distinct images and observe the generated images conditioned on these exchanged representations. For illustration purposes, we focus on the widely used Shapes3D dataset from the disentanglement literature and show the results in Figure 4. We can see that our EncDiff successfully isolates factors. Notably, in comparison to VAE-based methods, EncDiff delivers superior image quality quantitatively (please refer to Appendix H). ", "page_idx": 7}, {"type": "text", "text": "Visualization of Learned Cross-Attention Maps As mentioned in Section 1, our model draws inspiration from the alignment between \u2019word\u2019 tokens (disentangled representations) and spatial features. The alignment is demonstrated by the learned cross-attention maps. We verify whether our learned concept tokens present the disentangled characteristics by visualizing the alignment of concept tokens with spatial positions through cross-attention maps. As depicted in Figure 5, the results of our model exhibit exemplary alignment between concept tokens and spatial positions. Distinct concept tokens are associated with varying attended spatial regions, corresponding to different semantics that are comprehensible by humans, such as the region of \u201cFloor\u201d and \u201cColor\u201d for the images from Shapes3D. ", "page_idx": 7}, {"type": "text", "text": "Table 3: Influence of the two inductive biases. For EncDec w/o Diff, we replace the diffusion model with a decoder while cross-attention is preserved. For EncDiff w/ AdaGN, we replace the cross-attention with AdaGN. ", "page_idx": 8}, {"type": "table", "img_path": "StapcUWm9q/tmp/766ec030112060ed01f6e3effb0ad67740d785a901d5a9e324c41cabe48637cf.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "StapcUWm9q/tmp/b29301acf62a174a84be253daf6cd52fcfd28a2acb39b69a57cd3629105dbd2e.jpg", "table_caption": ["Table 4: Ablation study on the influence of the variance $(\\beta)$ schedule. We use four kinds of variance schedules: sqrt, cosine, linear, and sqrt linear. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.4 Ablation Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In our framework, in order to analyze and understand the key factors contributing to the advancement of disentangled representation learning, we conduct ablation studies covering three aspects in the design: 1) whether to use diffusion as the decoder; 2) whether to use cross-attention as the bridge for interaction; 3) The influence of different variance $(\\beta)$ schedules. We conduct these ablation studies on the Shapes3D dataset. Please see Appendix H for more ablation studies. ", "page_idx": 8}, {"type": "text", "text": "Using Diffusion as Decoder or Not To validate whether the use of a diffusion model as an inductive bias for disentangled representation learning is crucial, we employ a network structure similar to the U-Net in our used diffusion model as the decoder, utilizing reconstruction $l_{2}$ loss to optimize the entire network. We designed a variant (EncDec w/o Diff) of EncDiff to have an autoencoder-like structure, by reusing the image encoder as encoder and the lower half of the U-Net structure as decoder for reconstruction. In contrast to EncDiff, we discard the multiple step diffusion process and only run once feedforward inference to get the reconstruction. If the autoencoder\u2019s performance drops significantly, this will provide evidence about the importance of the diffusion process instead of the U-Net architecture. Specifically, we remove the encoder part of the U-Net and the skip connection between it and the decoder part of the U-Net. We then feed the U-Net decoder with a randomly initialized learnable spatial tensor to maintain the structure of the decoder U-Net. Similarly to EncDiff, the encoded disentangled representations are input to the decoder through cross-attention (CA). We refer to this scheme as EncDec w/o Diff. Table 3 shows the results. The performance of EncDiff with diffusion significantly outperforms EncDec w/o Diff by 0.46 and 0.79 in terms of FactorVAE score and DCI, respectively. This indicates that inductive bias from diffusion modelling is crucial for achieving effective disentanglement. ", "page_idx": 8}, {"type": "text", "text": "Using Cross-Attention for Interaction To incorporate the image representation to the diffusion model as a condition, we use cross-attention by treating each disentangled representation as a conditional token (similar to the use of \u2018word\u2019 token in text-to-image generation in stable diffusion model [30]). As an alternative, similar to that in Diff-AE [26] and InfoDiffusion [34], we use adaptive group normalization (AdaGN) to incorporate the representation vector (by concatenating the concept tokens) to modulate the spatial features. We name this scheme as EncDiff w/ AdaGN. Table 3 presents the results. We can see that EncDiff w/ AdaGN is inferior to EncDiff, with a significant decrease of 0.33 in terms of DCI. Cross-attention facilitates the alignment of each concept token with the corresponding spatial features, akin to the alignment of the \u2018word\u2019 token to spatial features in the text-to-image generation. In contrast, AdaGN did not efficiently promote disentanglement. ", "page_idx": 8}, {"type": "text", "text": "Influence of Different Variance $(\\beta)$ Schedules We investigate the influence of the different variance $(\\beta)$ schedules, including including linear, sqrt linear, cosine [25, 15], sqrt schedules [6, 30], on the disentanglement performance. From Table 4, we can see that distinct schedules result in different performance, demonstrating the influence on disentanglement of different information bottleneck schedules. Note that the FactorVAE scores are all very high and cannot well reflect the performance. We prefer to use DCI metric here for evaluation. We can see that the cosine schedule performs the best and we adopt it by default. The linear schedule approaches that of the sqrt linear in terms of the curve shape, please see Figure 2 (b) and achieves the similar performance in terms of DCI. ", "page_idx": 8}, {"type": "text", "text": "Scalar-valued vs. Vector-valued Manners We treat each dimension of the encoded feature vector as a disentangled factor, followed by a mapping to concept token (vector) for each factor. Another design alternative is to directly split the feature vector into $N$ chunks, with each chunk being a concept token, similar to DisDiff [40]. We name this vector-valued design and refer to it by DisDiff-V. Table 5 shows that EncDiff outperforms EncDiff-V obviously. The intermediate scalar design in EncDiff may serve a bottleneck role and contribute to the disentanglement. We think that the vector-based representation potentially extracts more information and hence enforces a looser bottleneck than scalar-valued representation. Note that the more information encoded, there is a higher probability that the encoded information is correlated, which is contradictory for disentanglement. Therefore, the tighter bottleneck from scalar-valued representation leads to (a slightly) better performance. ", "page_idx": 8}, {"type": "table", "img_path": "StapcUWm9q/tmp/fdcbc3a1909fd1613dd4f6e895eb8455e386d580ba5788386fe983cf0550efe7.jpg", "table_caption": ["Table 5: Ablation study on the two design alternatives on obtaining the token representations. "], "table_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "StapcUWm9q/tmp/0aae02fe525518b8547fa50385d340fbe48069eaadd84692bf5e3df0595aebc4.jpg", "table_caption": ["Table 6: Ablation study on the space applying diffusion model. "], "table_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "StapcUWm9q/tmp/8d274dd5b8d46957be9f1080cfc63bfc62194c61de14e4a5c9e1bd828ed620f2.jpg", "table_caption": ["Table 7: Computational complexity comparison. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Results on Pixel Space As stated in Section 3.2.1, our analysis is applicable in pixel space as well. In order to verify this, we trained EncDiff directly in pixel space on the Shapes3D dataset, which we denote the scheme as EncDiff pixel. The results of the disentanglement analysis are presented in Table 6. The performance of our framework in pixel space remains robust, indicating that the operation in latent space is not a critical factor for achieving effective disentanglement. ", "page_idx": 9}, {"type": "text", "text": "4.5 Computational Complexity ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We compare the computational complexity of Diff-AE [26], DisDiff [40], and our EncDiff in terms of the parameters (Params.), floating-point operations (FLOPs), and inference time (seconds/sample) for sampling an image. As shown in Table 7, our EncDiff demonstrates much higher computational efficiency than Diff-AE and DisDiff. ", "page_idx": 9}, {"type": "text", "text": "5 Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Our method operates in a fully unsupervised manner and exhibits strong disentanglement capability on simple datasets. Similar to other disentanglement-based methods [14, 19, 5, 23, 37], obtaining satisfactory performance on complex data remains a challenge. As a diffusion-based method, the generation speed of EncDiff is faster than DisDiff [40]. However, it is still slower compared to VAE-based and GAN-based methods. More effective sampling strategies, as employed in DPM-based methods, could be utilized for accelerating in the future. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper unveils a fresh viewpoint, demonstrating that diffusion models with cross-attention can serve as a strong inductive bias to foster disentangled representation learning. Within our framework EncDiff, we reveal that the diffusion model structure with cross-attention can drive an image encoder to learn superior disentangled representations, even without any regularization. Our comprehensive ablation studies demonstrate that the strong capability is mainly attributed to diffusion modelling and cross-attention interaction. This work will inspire further investigations on diffusion for disentanglement, paving the way for sophisticated data analysis, understanding, and generation. ", "page_idx": 9}, {"type": "text", "text": "7 Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We thank all the anonymous reviewers for their constructive and helpful comments, which have significantly improved the quality of the paper. The work was partly supported by the National Natural Science Foundation of China (Grant No. 62088102) ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Yoshua Bengio, Aaron C. Courville, and Pascal Vincent. Representation learning: A review and new perspectives. PAMI, 2013.   \n[2] Christopher P Burgess, Irina Higgins, Arka Pal, Loic Matthey, Nick Watters, Guillaume Desjardins, and Alexander Lerchner. Understanding disentangling in beta-vae. arXiv:1804.03599, 2018.   \n[3] Jaehoon Cha and Jeyan Thiyagalingam. Orthogonality-enforced latent space in autoencoders: An approach to learning disentangled representations. In International Conference on Machine Learning, pages 3913\u20133948. PMLR, 2023.   \n[4] Ricky TQ Chen, Xuechen Li, Roger B Grosse, and David K Duvenaud. Isolating sources of disentanglement in variational autoencoders. In NeurPIS, 2018.   \n[5] Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. InfoGAN: Interpretable representation learning by information maximizing generative adversarial nets. Advances in neural information processing systems, 29, 2016.   \n[6] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in Neural Information Processing Systems, 34:8780\u20138794, 2021.   \n[7] Yilun Du, Shuang Li, Yash Sharma, Josh Tenenbaum, and Igor Mordatch. Unsupervised learning of compositional energy concepts. In Advances in Neural Information Processing Systems, volume 34, 2021.   \n[8] Cian Eastwood and Christopher K. I. Williams. A framework for the quantitative evaluation of disentangled representations. In ICLR, 2018.   \n[9] Marco Fumero, Florian Wenzel, Luca Zancato, Alessandro Achille, Emanuele Rodol\u00e0, Stefano Soatto, Bernhard Sch\u00f6lkopf, and Francesco Locatello. Leveraging sparse and shared feature activations for disentangled representation learning. Advances in Neural Information Processing Systems, 36, 2023.   \n[10] Muhammad Waleed Gondal, Manuel Wuthrich, Djordje Miladinovic, Francesco Locatello, Martin Breidt, Valentin Volchkov, Joel Akpo, Olivier Bachem, Bernhard Sch\u00f6lkopf, and Stefan Bauer. On the transfer of inductive bias from simulation to the real world: a new disentanglement dataset. In NeurIPS, 2019.   \n[11] Erik H\u00e4rk\u00f6nen, Aaron Hertzmann, Jaakko Lehtinen, and Sylvain Paris. Ganspace: Discovering interpretable GAN controls. In NeurIPS, 2020.   \n[12] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626, 2022.   \n[13] Irina Higgins, David Amos, David Pfau, Sebastien Racaniere, Loic Matthey, Danilo Rezende, and Alexander Lerchner. Towards a definition of disentangled representations. arXiv preprint arXiv:1812.02230, 2018.   \n[14] Irina Higgins, Lo\u00efc Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a constrained variational framework. In ICLR, 2017.   \n[15] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840\u20136851, 2020.   \n[16] Drew A Hudson, Daniel Zoran, Mateusz Malinowski, Andrew K Lampinen, Andrew Jaegle, James L McClelland, Loic Matthey, Felix Hill, and Alexander Lerchner. Soda: Bottleneck diffusion models for representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 23115\u201323127, 2024.   \n[17] Jindong Jiang, Fei Deng, Gautam Singh, and Sungjin Ahn. Object-centric slot diffusion. Advances in Neural Information Processing Systems, 2023. ", "page_idx": 10}, {"type": "text", "text": "[18] Xin Jin, Bohan Li, BAAO Xie, Wenyao Zhang, Jinming Liu, Ziqiang Li, Tao Yang, and Wenjun Zeng. Closed-loop unsupervised representation disentanglement with beta-vae distillation and diffusion probabilistic feedback. arXiv preprint arXiv:2402.02346, 2024. ", "page_idx": 11}, {"type": "text", "text": "[19] Hyunjik Kim and Andriy Mnih. Disentangling by factorising. In ICML, 2018.   \n[20] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.   \n[21] Abhishek Kumar, Prasanna Sattigeri, and Avinash Balakrishnan. Variational inference of disentangled latent concepts from unlabeled observations. arXiv preprint arXiv:1711.00848, 2017.   \n[22] Felix Leeb, Giulia Lanzillotta, Yashas Annadani, Michel Besserve, Stefan Bauer, and Bernhard Sch\u00f6lkopf. Structure by architecture: Structured representations without regularization. In The Eleventh International Conference on Learning Representations, 2022.   \n[23] Zinan Lin, Kiran Thekumparampil, Giulia Fanti, and Sewoong Oh. Infogan-cr and modelcentrality: Self-supervised model training and selection for disentangling gans. In ICML, 2020.   \n[24] Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Raetsch, Sylvain Gelly, Bernhard Sch\u00f6lkopf, and Olivier Bachem. Challenging common assumptions in the unsupervised learning of disentangled representations. In international conference on machine learning, pages 4114\u2013 4124. PMLR, 2019.   \n[25] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In International Conference on Machine Learning, pages 8162\u20138171. PMLR, 2021.   \n[26] Konpat Preechakul, Nattanat Chatthee, Suttisak Wizadwongsa, and Supasorn Suwajanakorn. Diffusion autoencoders: Toward a meaningful and decodable representation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.   \n[27] Scott E. Reed, Yi Zhang, Yuting Zhang, and Honglak Lee. Deep visual analogy-making. In NeurIPS, 2015.   \n[28] Xuanchi Ren, Tao Yang, Yuwang Wang, and Wenjun Zeng. Learning disentangled representation by exploiting pretrained generative models: A contrastive learning view. In International Conference on Learning Representations, 2021.   \n[29] Ali Lotf iRezaabad and Sriram Vishwanath. Learning representations by maximizing mutual information in variational autoencoders. In IEEE International Symposium on Information Theory (ISIT), pages 2729\u20132734, 2020.   \n[30] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. Highresolution image synthesis with latent diffusion models. In IEEE/CVF conference on computer vision and pattern recognition, pages 10684\u201310695, 2022.   \n[31] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 22500\u201322510, 2023.   \n[32] Andrey Voynov and Artem Babenko. Unsupervised discovery of interpretable directions in the GAN latent space. In ICML, 2020.   \n[33] Xin Wang, Hong Chen, Si\u2019ao Tang, Zihao Wu, and Wenwu Zhu. Disentangled representation learning. arXiv preprint arXiv:2211.11695, 2022.   \n[34] Yingheng Wang, Yair Schiff, Aaron Gokaslan, Weishen Pan, Fei Wang, Christopher De Sa, and Volodymyr Kuleshov. Infodiffusion: Representation learning using information maximizing diffusion models. International conference on machine learning, 2023.   \n[35] Ziyi Wu, Jingyu Hu, Wuyue Lu, Igor Gilitschenski, and Animesh Garg. Slotdiffusion: Objectcentric generative modeling with diffusion models. Advances in Neural Information Processing Systems, 2023.   \n[36] Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao, Wentao Zhang, Bin Cui, and Ming-Hsuan Yang. Diffusion models: A comprehensive survey of methods and applications. ACM Computing Surveys, 2022.   \n[37] Tao Yang, Xuanchi Ren, Yuwang Wang, Wenjun Zeng, and Nanning Zheng. Towards building a group-based unsupervised representation disentanglement framework. In International Conference on Learning Representations, 2021.   \n[38] Tao Yang, Yuwang Wang, Cuiling Lan, Yan Lu, and Nanning Zheng. Vector-based representation is the key: A study on disentanglement and compositional generalization. arXiv preprint arXiv:2305.18063, 2023.   \n[39] Tao Yang, Yuwang Wang, Yan Lu, and Nanning Zheng. Visual concepts tokenization. Advances in Neural Information Processing Systems, 35:31571\u201331582, 2022.   \n[40] Tao Yang, Yuwang Wang, Yan Lv, and Nanning Zh. Disdiff: Unsupervised disentanglement of diffusion probabilistic models. Advances in Neural Information Processing Systems, 2023.   \n[41] Tao Yang, Yuwang Wang, Yan Lv, and Nanning Zheng. Disdiff: Unsupervised disentanglement of diffusion probabilistic models, january 2023b. Advances in Neural Information Processing Systems, 2023.   \n[42] Zijian Zhang, Zhou Zhao, and Zhijie Lin. Unsupervised representation learning from pre-trained diffusion probabilistic models. NeurIPS, 2022.   \n[43] Shengjia Zhao, Jiaming Song, and Stefano Ermon. InfoVAE: Information maximizing variational autoencoders. arXiv preprint arXiv:1706.02262, 2017.   \n[44] Xinqi Zhu, Chang Xu, and Dacheng Tao. Where and what? examining interpretable disentangled representations. In IEEE Conference on Computer Vision and Pattern Recognition, pages 5861\u2013 5870, 2021. ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Impact Statements ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "This paper presents work whose goal is to advance the field of disentanglement learning. Our research is designed to be a positive force for innovation purpose. Viewed from a societal lens, the potential negative impacts is the malicious use of the models. This highlights the critical necessity of incorporating ethical considerations in the utilization of our method for responsible AI. ", "page_idx": 13}, {"type": "text", "text": "B Optimization of the Reverse Conditional Probability $p_{\\theta}(x_{t-1}|x_{t},S)$ ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Two distinct approaches exist for decomposing the loss function, Variational Lower Bound(VLB), of the diffusion model. The derivations presented herein closely follow the methodology outlined by Ho et al. [15]. Without loss of generality, we could incorporate the conditional input denoted by $\\boldsymbol{S}$ to the diffusion. ", "page_idx": 13}, {"type": "text", "text": "For the first decomposition alternative, we can derive the following equations: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}=\\mathbb{E}_{q}\\left[-\\log\\frac{p_{\\theta}\\left(x_{0:T}|S\\right)}{q\\left(x_{1:T}|x_{0}\\right)}\\right]}\\\\ &{\\quad=\\mathbb{E}_{q}\\left[\\log\\frac{\\prod_{t=1}^{T}q\\left(x_{t}|x_{t-1}\\right)}{p\\left(x_{T}\\right)\\prod_{t=1}^{T}p_{\\theta}\\left(x_{t-1}|x_{t},S\\right)}\\right]}\\\\ &{\\quad=\\mathbb{E}_{q}\\left[-\\log p(x_{T})+\\sum_{t\\ge1}\\log\\frac{q\\left(x_{t}|x_{t-1}\\right)}{p_{\\theta}\\left(x_{t-1}|x_{t},S\\right)}\\right]}\\\\ &{\\quad=\\mathbb{E}_{q}\\left[-\\log p(x_{T})-\\sum_{t>1}\\log\\frac{p_{\\theta}\\left(x_{t-1}|x_{t},S\\right)}{q\\left(x_{t}|x_{t-1}\\right)}-\\log\\frac{p_{\\theta}\\left(x_{0}|x_{1}\\right)}{q\\left(x_{1}|x_{0}\\right)}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "By applying Bayes\u2019 Rule and the Markov property of the diffusion process, we conclude that ", "page_idx": 13}, {"type": "equation", "text": "$$\nq(x_{t-1}|x_{t},x_{0})={\\frac{q(x_{t}|x_{t-1},x_{0})q(x_{t-1}|x_{0})}{q(x_{t}|x_{0})}}={\\frac{q(x_{t}|x_{t-1})q(x_{t-1}|x_{0})}{q(x_{t}|x_{0})}}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "This leads to ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}=\\mathbb{E}_{q}\\left[-\\log p(x_{T})-\\Sigma_{t>1}\\log\\frac{p_{\\theta}\\left(x_{t-1}|x_{t},S\\right)}{q\\left(x_{t-1}|x_{t},x_{0}\\right)}\\frac{q\\left(x_{t-1}|x_{0}\\right)}{q\\left(x_{t}|x_{0}\\right)}-\\log\\frac{p_{\\theta}\\left(x_{0}|x_{1}\\right)}{q\\left(x_{1}|x_{0}\\right)}\\right]}\\\\ &{\\quad=\\mathbb{E}_{q}\\left[-\\log\\frac{p\\left(x_{T}\\right)}{q\\left(x_{T}|x_{0}\\right)}-\\Sigma_{t>1}\\log\\frac{p_{\\theta}\\left(x_{t-1}|x_{t},S\\right)}{q\\left(x_{t-1}|x_{t},x_{0}\\right)}-\\log p_{\\theta}\\left(x_{0}|x_{1}\\right)\\right]}\\\\ &{\\quad=\\mathbb{E}_{q}\\left[D_{K L}(q(x_{T}|x_{0})||p(x_{T}))+\\Sigma_{t>1}D_{K L}(q(x_{t-1}|x_{t},x_{0})||p_{\\theta}(x_{t-1}|x_{t},S))-\\log p_{\\theta}(x_{0}|x_{1})\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "For the second alternative, the loss function is derived to be $\\mathbb{E}_{x_{0},\\epsilon,t}\\|\\epsilon_{\\theta}(x_{t},t,S)-\\epsilon\\|$ [15]. ", "page_idx": 13}, {"type": "text", "text": "The optimization of conditional diffusion model by predicting the noises through minimizing $\\mathbb{E}_{x_{0},\\epsilon,t}\\|\\epsilon_{\\theta}(x_{t},t,S)\\;-\\;\\epsilon\\|$ is thus equivalent to minimizing the second term of (9), i.e., $\\Sigma_{t>1}D_{K L}(q(x_{t-1}|x_{t},x_{0})||p_{\\theta}(x_{t-1}|x_{t},S))$ , which pushes the reverse conditional probability $p_{\\theta}(x_{t-1}|x_{t},S)$ to approach $q(x_{t-1}|x_{t},x_{0})$ . ", "page_idx": 13}, {"type": "text", "text": "C The Transfer of Information Bottleneck ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Theorem C.1. The Kullback-Leibler divergence is invariant under a differentiable mapping $f$ , i.e . ", "page_idx": 13}, {"type": "equation", "text": "$$\nD_{K L}(p(x)|q(x))=D_{K L}(p(S)|q(S))\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $x=f(S)$ is a differentiable function between $x$ and $S$ and $p(x)$ and $q(x)$ are the probability density functions of the probability distributions $P$ and $Q$ , respectively. ", "page_idx": 13}, {"type": "text", "text": "Proof: The Kullback-Leibler divergence (KL divergence) is defined as ", "page_idx": 13}, {"type": "equation", "text": "$$\nD_{K L}(p(x)|q(x))=\\int p(x)\\log\\frac{p(x)}{q(x)}d x\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "According to the change of variable theorem, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{p(x)=p(f(x))|f^{\\prime}(x)|=p(S)|f^{\\prime}(x)|}\\\\ {q(x)=q(f(x))|f^{\\prime}(x)|=q(S)|f^{\\prime}(x)|}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\left|f^{\\prime}(x)\\right|$ denotes the Jacobian of $f(x),\\,p(S)$ is the corresponding distribution of $p(x)$ under the mapping $f.\\ q(S)$ is the corresponding distribution of $q(x)$ under the mapping $f$ . Combine the two equations, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\nD_{K L}(p(x)|q(x))=\\int p(f(x))|f^{\\prime}(x)|\\log\\frac{p(f(x))|f^{\\prime}(x)|}{q(f(x))|f^{\\prime}(x)|}d x=\\int p(f(x))\\log\\frac{p(f(x))}{q(f(x))}|f^{\\prime}(x)|d x.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Considering the property of integral that for $S=f(x)$ we have $d S=|f^{\\prime}(x)|d x$ . We then have the following: ", "page_idx": 14}, {"type": "equation", "text": "$$\nD_{K L}(p(x)|q(x))=\\int p(S)\\log\\frac{p(S)}{q(S)}d S=D_{K L}(p(S)|q(S)).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "This indicates that the information bottleneck on $x$ is transferable to the input $S$ of the function. ", "page_idx": 14}, {"type": "text", "text": "D Implementation Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "For our EncDiff (scalar-valued), following the approach of [22], each dimension of the encoded feature vector is treated as a disentangled factor. Each factor is then mapped to a vector (i.e., concept token) using non-shared MLP layers, as illustrated in Figure 3. For vector-valued EncDiff (EncDiffV), inspired by DisDiff [40], we partition the feature vector into $N$ (e.g., 20) chunks, referred to as concept tokens, to encode different factors. ", "page_idx": 14}, {"type": "text", "text": "Image Encoder Architecture To ensure an equitable comparison, we employ the encoder architecture, consistent with DisCo [28] and DisDiff [40]. The encoder specifications are detailed in Table 8. ", "page_idx": 14}, {"type": "table", "img_path": "StapcUWm9q/tmp/e9eadc746fa73593ab7aa94108c3f6145d706d9c9642a8d51fd379b39561ce6c.jpg", "table_caption": ["Table 8: Encoder architecture used in EncDiff. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "Diffusion U-Net Architecture The diffusion architecture adheres to the design principles of latent diffusion [30] and DisDiff [42]. Table 9 provides a detailed overview of the network structure, similar to the structure of Latent Diffusion Probabilistic Model. ", "page_idx": 14}, {"type": "text", "text": "We pretrain VQ-VAE in diffusion. Then the diffusion network and our image encoder are jointly trained. ", "page_idx": 14}, {"type": "text", "text": "E More Visualizations ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We present qualitative results for Cars3D in Figure 6. Notably, on the synthetic dataset Cars3D, EncDiff demonstrates the acquisition of disentangled representations, presenting better disentanglement ability than DisDiff [40]). We can see that our EncDiff can capture the factors of \u201cColor\u201d, \u201cAzimuth\u201d, ", "page_idx": 14}, {"type": "table", "img_path": "StapcUWm9q/tmp/c562ba20879d0f0b9b356b70cfb21aca4652725f02df876f5d43830ed85eacd6.jpg", "table_caption": ["Table 9: U-Net architecture used in EncDiff. "], "table_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "StapcUWm9q/tmp/3a41811011e936cf6dc34cef81c935f6bd24772b14a34f0ac5bcdef7ab3c5838.jpg", "img_caption": ["Figure 6: The qualitative comparison on Cars3D. The source (SRC) images provide the representations of the generated image. The target (TRT) image provides the representation for swapping. Other images are generated by swapping the representation of the corresponding factor. \u201cOrien\u201d refers to Orientation. We can see that our EncDiff can capture the factors of \u201cColor\u201d, \u201cAzimuth\u201d, and \u201cShape\u201d while DisDiff failed to capturing them. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "and \u201cShape\u201d while DisDiff failed to capturing them. Furthermore, we include three rows of images demonstrating the manipulation of representations lacking informative content, denoted as \u201cNone\u201d. ", "page_idx": 15}, {"type": "text", "text": "We present the qualitative outcomes for MPI3D in Figure 7. Remarkably, on the challenging disentanglement dataset MPI3D, EncDiff showcases its ability to obtain disentangled representations. ", "page_idx": 15}, {"type": "text", "text": "F More Visualizations on Attention Maps ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We showcase the visualization of attention maps for our model\u2019s disentangled representations on the Cars3D dataset, as depicted in Figure 8. Similar to EncDiff, the attention maps provide insights into the acquisition of disentangled representations in the synthetic setting of Cars3D. Notably, our attention maps reveal distinct alignments between concept tokens and spatial features, demonstrating the disentangled characteristics learned by our model. ", "page_idx": 15}, {"type": "text", "text": "For the MPI3D dataset, Figure 8 demonstrate the qualitative outcomes of attention map visualization. In this challenging disentanglement scenario, EncDiff excels in acquiring disentangled representations. The attention maps further illustrate the alignment between concept tokens and spatial positions, affirming the model\u2019s ability to disentangle complex features in MPI3D. ", "page_idx": 15}, {"type": "image", "img_path": "StapcUWm9q/tmp/5777d56a28bcf3908569d718d53e57e1106b5ad2ec9d881ec7b940f106f4fcb9.jpg", "img_caption": ["Figure 7: The qualitative results on MPI3D. The source (SRC) images provide the representations of the generated image. The target (TRT) image provides the representation for swapping. Other images are generated by swapping the representation of the corresponding factor. The learned factors on MPI3D are thickness, BG (Background) color, object (OB) color, and object (OB) shape, orientation (Orien), Pos (Background bar position). "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "StapcUWm9q/tmp/faa9aab0279b3b2e87ad1f79a036a41adda90f3277c827a10084986803f966e2.jpg", "img_caption": ["Figure 8: Visualization of the cross-attention maps on MPI3D and Cars3D. The first column shows the original image, while the other columns show the attention masks for different concept tokens. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "G Autoencoding Reconstruction Quality ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Autoencoding Reconstruction Quality To investigate the autoencoding reconstruction quality of EncDiff, we conduct the same quantitative experiments with PDAE, DisDiff and Diff-AE. We follow them to evaluate the reconstruction quality using averaged SSIM, LPIPS, and MSE. The results are shown in Table 10. It is evident that EncDiff outperforms DisDiff on all metrics. EncDiff achieves the state-of-the-art performance of SSIM and LPIPS with a strong disentanglement capability. ", "page_idx": 16}, {"type": "table", "img_path": "StapcUWm9q/tmp/4ea597dc4f73717f49f7cb3569dfdac60e109941efcedb149d6fa8b2425ef8ad.jpg", "table_caption": ["Table 10: Reconstruction quality comparison on the Shape3D dataset. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "H More Ablation Related with EncDiff ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Measuring on Scalar-valued vs. Vector-valued Representations in our EncDiff For EncDiff, each dimension of the scalar-valued representation is mapped to a representation vector, resulting in two representations in EncDiff: a scalar-valued one and a mapped vector-valued one. From Table 11, we can see that these two representations have similar performance. ", "page_idx": 16}, {"type": "table", "img_path": "StapcUWm9q/tmp/7f2450c59e64659d9cfffe4f6fc9a0c920d6ea0738f0908d14d5bc23ad702269.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Influence of the Number of Concept Tokens Similar to other disentanglement methods, the number of disentangled latent units influences performance. To study such an effect on EncDiff, we train our model with the following number of tokens: 5, 10, 15, 20 and 30, respectively. As shown in Table 12, when the number of tokens is fewer than the number of ground truth factors, the performance significantly drops. With the use of more tokens, the performance improves. In accordance with the setup proposed by [39], EncDiff adopts the default setting of 20 tokens for a fair comparison with other methods [39, 40, 28]. ", "page_idx": 17}, {"type": "table", "img_path": "StapcUWm9q/tmp/c38719c42a4d040d451ce0eca56065917d4365a3b27120c4f390cb2f0ef9030f.jpg", "table_caption": ["Table 12: Influence of the number of concept tokens in EncDiff. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Efficacy of Additional Regularization We are wondering whether additional regularization can further promote the disentanglement. To validate this, we conducted the following experiments to investigate the effectiveness of incorporating two types of constraints: sparsity and orthogonality, respectively. ", "page_idx": 17}, {"type": "text", "text": "We explored the orthogonality constraint proposed in [3], which enforces orthogonality from a group theory perspective. We adapted their method of transforming representations using Euler encoding to enforce orthogonality within EncDiff. We adopted the official implementation on github to modify EncDiff, denoted as EncDiff with [3]. We integrated the sparsity regularization terms proposed in [9] into EncDiff to facilitate disentanglement, denoted as EncDiff with [9]. We take the techniques proposed in [41], which involve matrix decomposition and matrix exponentiation to construct orthogonal matrices. We replaced the scalar MLP mappings with a series of learnable orthogonal vectors to ensure orthogonality in the representations, denoted as EncDiff with [41].The results are shown in Table 13. We observed that the regularization can slightly improve the performance further on our EncDiff. For simplicity, we do not incorporate any regularization on all other results. ", "page_idx": 17}, {"type": "table", "img_path": "StapcUWm9q/tmp/8e5afc2c9e027224bac6f8c23f8f808da9749f0cd6c9ece865ef41952432f712.jpg", "table_caption": ["Table 13: Ablation study on the additional regularization over EncDiff. We use a CNN encoder by default. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Influence of the Image Encoder Architecture To investigate the influence of encoder design, we use a powerful encoder to replace the CNN encoder used in the EncDiff. We adopt a transformer encoder with a set of learnable tokens as the disentangled representations, as introduced in [39], which are refined through cross-attention. We refer to this scheme EncDiff w/Trans. For fair comparison, the model size of the encoders is similar. As shown in Table 14, EncDiff w/Trans is comparable to EncDiff. When considering the performance gap between DisDiff [40] and our EncDiff, the influence of encoder structures is small and is not the key factor for influencing disentangling capabilities. A similar phenomenon is observed in vector-valued one. ", "page_idx": 17}, {"type": "text", "text": "Table 14: Ablation study on image encoder. EncDiff w/Trans denotes the scheme in which we replace CNN encoder with a transformer encoder. ", "page_idx": 18}, {"type": "table", "img_path": "StapcUWm9q/tmp/0ae4b27673210a9f2306cd01e09467f2c0a6a1abe40e8309bed084c6eb82a9da.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "I More Ablation Study on EncDiff-V ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We also perform ablation study related with EncDiff-V to validate the effects of the two inductive bias, the inefficacy of additional regularization. Similar trends as EncDiff are observed. ", "page_idx": 18}, {"type": "text", "text": "Ablation on Two Inductive Bias of EncDiff-V In alignment with the main paper, we also conducted an experiment to assess the effectiveness of the two inductive biases in EncDiff-V. We adopt the same decoder used in Section 4.4 to substitute the diffusion in EncDiff-V. We denote this model as EncDec-V w/o Diff. On the other hand, to study the effectiveness of cross-attention, we use the same conditional decoder of EncDiff w/ AdaGN in EncDiff-V, denoted as EncDiff-V w/ AdaGN. The performance of both of these two models drops significantly, as indicated by the results in Table 15. ", "page_idx": 18}, {"type": "table", "img_path": "StapcUWm9q/tmp/7712fda305e8228ee2aba7098d52553eff75858b59cf3811a1de9bf56325157b.jpg", "table_caption": ["Table 15: Ablation study on the influence of the two different inductive biases of EncDiff-V. For EncDec-V w/o Diff, we replace the diffusion model with a decoder while cross attention is preserved for the interaction. For EncDiff- $V\\,w/A d a G N,$ , we replace the cross attention with AdaGN. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "J Ablation Study on MPI3D ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Besides Shapes3d, we conducted the main ablation study on another dataset MPI3D. The results are shown in Table 16 and 17 below. We can observe that the trends are consistent with that on Shapes3D (see Table 3). ", "page_idx": 18}, {"type": "text", "text": "Table 16: Influence of the two inductive biases on MPI3D. For EncDec w/o Diff, we replace the diffusion model with a decoder while cross-attention is preserved. For EncDiff w/ AdaGN, we replace the cross-attention with AdaGN. ", "page_idx": 18}, {"type": "table", "img_path": "StapcUWm9q/tmp/27ff42ee6c4fbbcc28bac5eb5a4380ceecddf22085ac462f7dbe5ccd893ea203.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "StapcUWm9q/tmp/d92b009170735346ee9f2bf714e8765d4f8358463e38592f3a55856359cfc32b.jpg", "table_caption": ["Table 17: Ablation study on the two design alternatives on obtaining the token representations. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "K Computational Complexity on More Methods ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The computational complexity of the VAE-based and GAN-based methods are also listed as shown in Table. The computational complexity and inference time VAEs and GANs still have strength, but diffusion has much better generation and disentangling ability. Among the diffusion models, our EncDiff has better generation and disentangling ability but less computational cost and inference time. ", "page_idx": 18}, {"type": "table", "img_path": "StapcUWm9q/tmp/1b8a07d01f3810da2ce89350108b37010183b46eaff329ac9162ba954237bd66.jpg", "table_caption": ["Table 18: Computational complexity comparison. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "L Comparison with Diff-AE ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Diff-AE shows disentanglement qualities. One maybe interested in a comparison of disentanglement between Diff-AE and EncDiff. The results of Diff-AE on Shapes3D as shown in Table 19, which is consistent with the trends on CelebA (see Table 2 in the main paper), our method outperforms Diff-AE with a large margin on Shapes3D. ", "page_idx": 19}, {"type": "table", "img_path": "StapcUWm9q/tmp/a912cc667b136eb64fe4e765d940be00ae7b22be8a97af8592c115d0e701c8bc.jpg", "table_caption": ["Table 19: Performance comparison with Diff-AE on Shapes3D. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "M EncDiff for Stable Diffusion, i.e., EncDiff (SD) ", "text_level": 1, "page_idx": 19}, {"type": "image", "img_path": "StapcUWm9q/tmp/c3180932cf9956e8babf29e25385e8965c0acc40b5029c7b3f0d62d18682d50a.jpg", "img_caption": ["Figure 9: Illustration of applying EncDiff for disentangling DreamBooth i.e., EncDiff (SD). "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "In order to discuss the disentanglement of EncDiff on real-world data, a newly designed architecture is introduced in Figure 9, named EncDiff(SD).In this architecture, we use a strong pretrained model (stable diffusion v1.4) to replace the original diffusion in EncDiff. Motivated by DreamBooth [31] for customized representation learning, we assign several latent units for each object for inverting and learning object semantic representation. As shown in Figure 9, in order to disentangle these latent units, we use a set of non-shared MLPs to map these latent units into concept tokens. Different from EncDiff, the instance for disentangling is not an image but the semantics of objects. The target is to disentangle concepts or properties (e.g., color, long-hair, big-eared) from the inverted objects (dog). ", "page_idx": 19}, {"type": "text", "text": "Figure 10 shows sampling process of EncDiff (SD), we denote the image on the left as image1 and image on the right as image2. We use a prompt \u201cA <token from image1, token from image $2>$ dog is playing a blue ball\u201d to sample image. The <token from image1> encodes the color of the dog. The <token from image $_{\\cdot2>}$ encodes the type of the dog. We can sample images of new objects that combine color of dog1 and type of dog2. ", "page_idx": 19}, {"type": "image", "img_path": "StapcUWm9q/tmp/0fabff54ad16688fc9ba7bb31c851b6bc93d5171892fa6bc368b76c4ce815240.jpg", "img_caption": ["Figure 10: Sampling of EncDiff(SD). We can combine different concepts by sampling EncDiff (SD) with combined tokens from different images. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "We take the new prompt as the condition of Stable Diffusion, so that we can use diffusion as an inductive bias for disentangling. The process above is the same as EncDiff. Firstly, to disentangle the objects from the background, we replace the Diffusion Loss with DisenBooth Loss. Secondly, to learn multiple concepts, we follow Custom Diffusion to finetune only the KV layer in cross attention. Lastly, for the ease of learning disentangled concept with a few images, we take an image from each object to construct a training batch. The results are shown in Figure 11, 12, 13. Our model also demonstrates the ability to disentangled semantic factors on real and complex data. ", "page_idx": 20}, {"type": "image", "img_path": "StapcUWm9q/tmp/ad9467d517c0bc29cc56f7a727959f9b79c1ab755714b6d3c09a6e64d837f30e.jpg", "img_caption": ["Figure 11: Sampling of EncDiff(SD). Prompt: \u201dA <new concept tokens> dog is playing a blue ball\u201c. The images of the first column provide source representation and the image (target image) in the first row provides the target representation. The concept write color is disentangled in representation of target image. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "StapcUWm9q/tmp/1d283b89a4def8e3e6512df6b08a834fcbaca984cfec8b62bd222bcc29be0800.jpg", "img_caption": ["Figure 12: Sampling of EncDiff(SD). We can combine different concept in images by sampling. Prompt \u201dA <new concept tokens> dog is playing a blue ball\u201c. The images of the first column provide source representation and the image (target image) in the first row provides the target representation. The concept long ear is disentangled in representation of target image. "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "StapcUWm9q/tmp/bf447724b8f944ea50a2294734d8e68dea4a9990be56307f8c86fb167e73aec2.jpg", "img_caption": ["Figure 13: Sampling of EncDiff(SD). We can combine different concept in images by sampling. Prompt \u201dA <new concept tokens> dog is playing a blue ball\u201c. The images of the first column provide source representation and the image (target image) in the first row provides the target representation. The concept long hair is disentangled in representation of target image. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We introduce a new perspective and framework, demonstrating that diffusion models with cross-attention can serve as a powerful inductive bias to facilitate the learning of disentangled representations. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: Please see Section 5 in the main paper. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 22}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper does not include theoretical results. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 23}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Section 3, 4, and Appendix D have presented the necessary information to reproduce the main experimental results of the paper. The used datasets are all public datasets. We will release the code. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 23}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 24}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We have described the necessary information in Section 3, Appendix D. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 24}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Following other papers in this field to report error bar in our tables. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We provide the information of computer resources used in Appendix D. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 25}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: We have read the NeurIPS Code of Ethics and confirm that we follow that. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 25}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: Please see appendix A. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: When we release our models, we ask the users to follow the user guidelines to assure the positive usage. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 26}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We have cited the original papers of the datasets. We have checked the licenses of used datasets and followed them. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 27}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 27}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 27}]