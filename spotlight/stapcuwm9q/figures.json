[{"figure_path": "StapcUWm9q/figures/figures_1_1.jpg", "caption": "Figure 1: Average attention map across all time steps in stable diffusion. We draw inspiration from the process of text-to-image generation using a diffusion model with cross-attention. Utilizing the highly 'disentangled' words as the condition for image generation, the cross-attention maps observed from the diffusion model exhibit a strong text semantic and spatial alignment, indicating the model is capable of incorporating each individual word into the generation process for a final semantic aligned generation. This leads us to question whether such a diffusion structure could be inductive to disentangled representation learning.", "description": "This figure shows average attention maps across all time steps during the stable diffusion process for text-to-image generation.  The input is a sentence with semantically disentangled words (e.g., \"dog\", \"red\", \"shirt\", \"blue\", \"bird\"). The attention maps demonstrate a strong correlation between these words and specific spatial regions in the generated image.  This suggests that the diffusion model's cross-attention mechanism effectively integrates individual words into the generation process, leading the authors to investigate its potential for disentangled representation learning.", "section": "1 Introduction"}, {"figure_path": "StapcUWm9q/figures/figures_2_1.jpg", "caption": "Figure 2: (a) Illustration of our framework EncDiff. We employ an image encoder T4 to transform an image I into a set of disentangled representations, which we treat them as the conditional input to the latent diffusion model with cross attention. Here cross attention bridges the interaction between the diffusion network and the image encoder. For simplicity, we only briefly show the diffusion model which consists of an encoder E, a denoising U-Net and a decoder D that reconstructs the image from the latent xt. (b) Information bottleneck reflected by KL divergence in reverse diffusion process. The KL divergence between the data distribution q(xt-1|Xt, xo) and the Gaussian prior distribution N(0, I) under four different variance (3) schedules: cosine, linear, sqrt linear and sqrt. The results have been normalized by the number of dimensions.", "description": "This figure illustrates the EncDiff framework, showing how an image encoder produces disentangled representations that condition a latent diffusion model with cross-attention for image reconstruction.  The cross-attention mechanism bridges the encoder and the U-Net within the diffusion model. The second part of the figure displays KL divergence curves demonstrating the time-varying information bottlenecks inherent in the reverse diffusion process under various variance schedules.", "section": "3.1 Framework of EncDiff"}, {"figure_path": "StapcUWm9q/figures/figures_6_1.jpg", "caption": "Figure 3: Illustration of the encoder T4, which transforms an image into a feature vector of dimension N, with each dimension (scalar) encoding a disentangled factor. We then use non-shared three-layer MLP layers to map each scalar into a vector (concept token). The concept tokens will be treated as the conditional input to the latent diffusion model with cross attention.", "description": "This figure illustrates the image encoder architecture used in the EncDiff framework. The encoder takes an image as input and transforms it into a feature vector of dimension N. Each dimension of this feature vector represents a disentangled factor.  The encoder uses separate, three-layer MLPs to map each scalar disentangled factor into a higher-dimensional vector, called a \"concept token\". These concept tokens are then used as the conditional input to the latent diffusion model, acting as a bridge between the encoder and the U-Net within the diffusion model.", "section": "3.1 Framework of EncDiff"}, {"figure_path": "StapcUWm9q/figures/figures_7_1.jpg", "caption": "Figure 4: The qualitative results on Shapes3D and MPI3D. The source (SRC) images provide the representations of the generated image. The target (TRT) image provides the representation for swapping. Other images are generated by swapping the representation of the corresponding factor. For Shapes3D, the learned factors are wall color (Wall), floor color (Floor), object color (Color), and object shape (Shape), orientation (Orien), scale. See Appendix E for more visualizations.", "description": "This figure demonstrates the disentanglement capability of the EncDiff model on the Shapes3D and MPI3D datasets.  It shows the results of swapping different factors (like color, shape, orientation, etc.) between source and target images. By changing a specific factor's representation,  the model generates new images with only that factor changed, demonstrating its ability to isolate and manipulate individual latent variables.", "section": "4 Experiments"}, {"figure_path": "StapcUWm9q/figures/figures_7_2.jpg", "caption": "Figure 5: Visualization of the cross-attention maps on Shapes3D and MPI3D. The first column shows the original image while the other columns show the attention masks for different concept tokens. See Appendix F for more visualizations. \u201cPos\u201d represents \u201cPosition\u201d.", "description": "This figure visualizes the cross-attention maps generated by the EncDiff model on the Shapes3D and MPI3D datasets.  Each row represents a different image. The first column shows the original image, while subsequent columns display attention maps for different concept tokens (e.g., Wall color, Floor color, Shape, Orientation, Scale, Position). The attention maps highlight which parts of the image are most relevant to each concept token, illustrating how the model disentangles different factors within the image.  Appendix F contains additional visualizations.", "section": "4 Experiments"}, {"figure_path": "StapcUWm9q/figures/figures_15_1.jpg", "caption": "Figure 6: The qualitative comparison on Cars3D. The source (SRC) images provide the representations of the generated image. The target (TRT) image provides the representation for swapping. Other images are generated by swapping the representation of the corresponding factor. \u201cOrien\u201d refers to Orientation. We can see that our EncDiff can capture the factors of \u201cColor\u201d, \u201cAzimuth\u201d, and \u201cShape\u201d while DisDiff failed to capturing them.", "description": "This figure compares the qualitative results of DisDiff and EncDiff on the Cars3D dataset.  It shows how manipulating the representation of different factors (color, azimuth, shape, orientation) affects the generated images.  EncDiff demonstrates a better ability to isolate and control these factors than DisDiff.", "section": "4.3 Visualization"}, {"figure_path": "StapcUWm9q/figures/figures_16_1.jpg", "caption": "Figure 4: The qualitative results on Shapes3D and MPI3D. The source (SRC) images provide the representations of the generated image. The target (TRT) image provides the representation for swapping. Other images are generated by swapping the representation of the corresponding factor. For Shapes3D, the learned factors are wall color (Wall), floor color (Floor), object color (Color), and object shape (Shape), orientation (Orien), scale. See Appendix E for more visualizations.", "description": "This figure shows the qualitative results of disentanglement on the Shapes3D and MPI3D datasets.  It demonstrates the ability of the EncDiff model to isolate and manipulate individual factors of the generated images.  By swapping the representation of a specific factor (e.g., color, shape, orientation) between two source images, the model generates new images reflecting the changes made to that specific factor, proving the disentanglement of these latent variables.", "section": "4 Experiments"}, {"figure_path": "StapcUWm9q/figures/figures_16_2.jpg", "caption": "Figure 5: Visualization of the cross-attention maps on Shapes3D and MPI3D. The first column shows the original image while the other columns show the attention masks for different concept tokens. See Appendix F for more visualizations. \u201cPos\u201d represents \u201cPosition\u201d.", "description": "This figure visualizes the cross-attention maps generated by the EncDiff model on the Shapes3D and MPI3D datasets. Each row represents a different image. The first column shows the original image. The subsequent columns display the attention masks for each concept token (e.g., Wall, Floor, Color, Shape, Orientation, Scale, Position, Thickness, BG Color, OB Color, Size). The heatmaps indicate the attention weights assigned to different spatial locations by the model for each concept token, highlighting the alignment between concept tokens and spatial regions. This demonstrates the model's ability to disentangle different factors by effectively bridging image features with concept tokens through cross-attention.  See Appendix F for additional examples.", "section": "4 Experiments"}, {"figure_path": "StapcUWm9q/figures/figures_19_1.jpg", "caption": "Figure 9: Illustration of applying EncDiff for disentangling DreamBooth i.e., EncDiff (SD).", "description": "This figure illustrates the architecture of EncDiff applied to DreamBooth, referred to as EncDiff(SD).  It shows how EncDiff is adapted to disentangle different concepts or properties (like color, long-hair, big-eared) from images of dogs.  Instead of using a complete image as input, the model takes semantic representations (text tokens) extracted from images. These tokens are processed through multiple MLP layers to create concept tokens that are then used as input for the cross-attention mechanism within the stable diffusion model.  The output is a disentangled representation, allowing for independent control over different image features during generation.", "section": "M EncDiff for Stable Diffusion, i.e., EncDiff (SD)"}, {"figure_path": "StapcUWm9q/figures/figures_20_1.jpg", "caption": "Figure 9: Illustration of applying EncDiff for disentangling DreamBooth i.e., EncDiff (SD).", "description": "This figure illustrates the architecture of EncDiff applied to DreamBooth, denoted as EncDiff(SD).  EncDiff(SD) uses a pre-trained Stable Diffusion model as its base.  To disentangle concepts within the images, it employs an image encoder that extracts features. These features are then passed through multiple MLP layers to produce concept tokens. These tokens are used as conditional inputs to the Stable Diffusion model.  Cross-attention mechanisms are utilized to integrate the concept tokens into the diffusion process for image generation, enabling the disentanglement of concepts during the process.  The example shows using a combination of different properties to create novel images using the disentangled features.", "section": "M EncDiff for Stable Diffusion, i.e., EncDiff (SD)"}, {"figure_path": "StapcUWm9q/figures/figures_20_2.jpg", "caption": "Figure 1: Average attention map across all time steps in stable diffusion. We draw inspiration from the process of text-to-image generation using a diffusion model with cross-attention. Utilizing the highly \u2018disentangled\u2019 words as the condition for image generation, the cross-attention maps observed from the diffusion model exhibit a strong text semantic and spatial alignment, indicating the model is capable of incorporating each individual word into the generation process for a final semantic aligned generation. This leads us to question whether such a diffusion structure could be inductive to disentangled representation learning.", "description": "This figure shows average attention maps across all time steps during stable diffusion's text-to-image generation process.  The use of disentangled words as input conditions highlights how cross-attention maps show strong semantic and spatial alignment. This suggests the model successfully integrates individual words, leading the authors to explore if this diffusion structure promotes disentangled representation learning.", "section": "1 Introduction"}, {"figure_path": "StapcUWm9q/figures/figures_21_1.jpg", "caption": "Figure 1: Average attention map across all time steps in stable diffusion. We draw inspiration from the process of text-to-image generation using a diffusion model with cross-attention. Utilizing the highly 'disentangled' words as the condition for image generation, the cross-attention maps observed from the diffusion model exhibit a strong text semantic and spatial alignment, indicating the model is capable of incorporating each individual word into the generation process for a final semantic aligned generation. This leads us to question whether such a diffusion structure could be inductive to disentangled representation learning.", "description": "This figure shows average attention maps across all time steps during the stable diffusion process. The authors used highly disentangled words as input conditions for image generation. The resulting cross-attention maps show a strong alignment between the text semantics and the spatial layout of the generated images. This observation leads the authors to hypothesize that the diffusion model's structure, with its cross-attention mechanism, could serve as an inductive bias for learning disentangled representations.", "section": "1 Introduction"}, {"figure_path": "StapcUWm9q/figures/figures_21_2.jpg", "caption": "Figure 1: Average attention map across all time steps in stable diffusion. We draw inspiration from the process of text-to-image generation using a diffusion model with cross-attention. Utilizing the highly 'disentangled' words as the condition for image generation, the cross-attention maps observed from the diffusion model exhibit a strong text semantic and spatial alignment, indicating the model is capable of incorporating each individual word into the generation process for a final semantic aligned generation. This leads us to question whether such a diffusion structure could be inductive to disentangled representation learning.", "description": "This figure shows average attention maps across all time steps during stable diffusion's text-to-image generation process.  The use of disentangled words as input conditions results in attention maps demonstrating a strong alignment between the word semantics and the spatial locations in the generated image. This observation suggests that the diffusion model's architecture, particularly the cross-attention mechanism, might inherently promote disentangled representation learning.", "section": "1 Introduction"}]