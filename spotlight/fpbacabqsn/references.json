{"references": [{"fullname_first_author": "Tri Dao", "paper_title": "FlashAttention-2: Faster attention with better parallelism and work partitioning", "publication_date": "2024-00-00", "reason": "This paper introduces FlashAttention-2, a highly optimized kernel used by MInference to significantly accelerate attention calculation."}, {"fullname_first_author": "Guangxuan Xiao", "paper_title": "Efficient streaming language models with attention sinks", "publication_date": "2024-00-00", "reason": "This paper presents StreamingLLM, a baseline method compared against in the MInference experiments, providing a context for evaluating the effectiveness of MInference."}, {"fullname_first_author": "Zichang Liu", "paper_title": "Dynamic sparse attention for scalable transformer acceleration", "publication_date": "2022-00-00", "reason": "This paper provides foundational research on dynamic sparse attention, a key concept upon which MInference builds."}, {"fullname_first_author": "Cheng-Ping Hsieh", "paper_title": "Ruler: What's the real context size of your long-context language models?", "publication_date": "2024-00-00", "reason": "This paper introduces the RULER benchmark, a crucial evaluation dataset for assessing MInference's performance across various long-context scenarios."}, {"fullname_first_author": "Xinrong Zhang", "paper_title": "Bench: Extending long context evaluation beyond 100K tokens", "publication_date": "2024-00-00", "reason": "This paper introduces the InfiniteBench benchmark, another essential evaluation dataset used to assess MInference's capabilities in handling various long-context tasks."}]}