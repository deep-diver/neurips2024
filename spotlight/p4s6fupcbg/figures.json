[{"figure_path": "P4s6FUpCbG/figures/figures_0_1.jpg", "caption": "Figure 1: The 3DGS-Enhancer improves 3D Gaussian splatting representations on unbounded scenes with sparse input views.", "description": "This figure shows a comparison of novel view synthesis results using 3DGS (3D Gaussian splatting) and the proposed 3DGS-Enhancer method.  The top row displays a scene with a plant and shelves, while the bottom row shows a scene with a circular structure.  In each case, the leftmost image shows the results using the standard 3DGS approach with sparse input views, exhibiting artifacts such as missing details and unrealistic shapes (as indicated by lower PSNR values).  The middle image shows the improved reconstruction achieved using 3DGS-Enhancer, demonstrating a significant improvement in quality (higher PSNR values) and visual fidelity. Finally, the rightmost images are the corresponding ground truth images.", "section": "Abstract"}, {"figure_path": "P4s6FUpCbG/figures/figures_3_1.jpg", "caption": "Figure 2: An overview of the proposed 3DGS-Enhancer framework for 3DGS representation enhancement. We learn 2D video diffusion priors on a large-scale novel view synthesis dataset to enhance the novel views rendered from the 3DGS model on a novel scene. Then, the enhanced views and input views jointly fine-tune the 3DGS model.", "description": "The figure illustrates the 3DGS-Enhancer framework's workflow.  It starts with input views and novel views rendered by a 3DGS model. These views are fed into a Video Diffusion Prior module, which leverages a temporal denoising U-Net and CLIP for enhancement. The enhanced views are then integrated with the original input views using a spatial-temporal decoder. Finally, the combined views fine-tune the 3DGS model, resulting in improved 3D representation and better novel view synthesis.", "section": "4 Method"}, {"figure_path": "P4s6FUpCbG/figures/figures_5_1.jpg", "caption": "Figure 4: A visual comparison of rendered images on scenes from DL3DV [21] test set with the 3-view setting.", "description": "This figure presents a visual comparison of novel view synthesis results using different methods on scenes from the DL3DV test set, specifically focusing on scenarios with only three input views. It compares the results of Mip-NeRF, FreeNeRF, 3DGS, DNGaussian, and the proposed 3DGS-Enhancer, showcasing the superior visual quality and detail preservation achieved by the 3DGS-Enhancer in comparison to existing methods.", "section": "5 Experiments"}, {"figure_path": "P4s6FUpCbG/figures/figures_7_1.jpg", "caption": "Figure 4: A visual comparison of rendered images on scenes from DL3DV [21] test set with the 3-view setting.", "description": "This figure visually compares the performance of different novel view synthesis methods on three example scenes from the DL3DV dataset, using only three input views.  It showcases the superior quality of the 3DGS-Enhancer method compared to baselines such as Mip-NeRF, FreeNeRF, 3DGS, and DN-Gaussian. The results highlight 3DGS-Enhancer's ability to generate sharper, more detailed, and visually more realistic novel views, especially in areas with high-frequency details or challenging viewing conditions.", "section": "5 Experiments"}, {"figure_path": "P4s6FUpCbG/figures/figures_8_1.jpg", "caption": "Figure 5: A visual comparison of cross-dataset generalization ability, where the methods are trained on the DL3DV-10K dataset [21] and tested on the Mip-NeRF360 dataset [2].", "description": "This figure shows a visual comparison of the results obtained by the proposed 3DGS-Enhancer method and other baselines on the Mip-NeRF360 dataset.  The key aspect highlighted is the cross-dataset generalization ability; the model trained on DL3DV-10K is used to generate novel views on unseen data. The results demonstrate how well the model generalizes to different datasets and scene types.", "section": "5 Experiments"}, {"figure_path": "P4s6FUpCbG/figures/figures_9_1.jpg", "caption": "Figure 6: An ablation study of the video diffusion model components in our 3DGS-Enhancer framework.", "description": "This figure shows the ablation study of the video diffusion model components in the 3DGS-Enhancer framework.  It visually compares the results of using only the video diffusion model, the spatial-temporal decoder (STD), both combined, and the ground truth. The input image shows significant artifacts. The video diffusion model improves the results but still shows some artifacts. The STD further enhances the image quality and reduces artifacts. The ground truth image is shown for comparison.", "section": "5.3 Ablation Study"}, {"figure_path": "P4s6FUpCbG/figures/figures_14_1.jpg", "caption": "Figure 7: The fitting trajectories under different number of input views.", "description": "This figure shows how the camera trajectories are fitted for different numbers of input views (3, 6, and 9).  The fitting process is crucial for generating smooth and consistent image sequences for the video diffusion model training.  The trajectories are fitted using either the high-quality input views (for simple trajectories) or the low-quality input views (for complex trajectories) to ensure reasonable artifact distributions in the rendered images.", "section": "8.1 Details of 3DGS Enhancement Dataset"}, {"figure_path": "P4s6FUpCbG/figures/figures_15_1.jpg", "caption": "Figure 4: A visual comparison of rendered images on scenes from DL3DV [21] test set with the 3-view setting.", "description": "This figure presents a visual comparison of novel view synthesis results generated by different methods on four scenes from the DL3DV dataset.  Each row shows a scene with three input views (leftmost column) and the results from Mip-NeRF, FreeNeRF, 3DGS, DNGaussian, and the proposed 3DGS-Enhancer method (from left to right, except for the ground truth which is in the rightmost column).  The goal is to showcase how the proposed method compares in visual quality to other state-of-the-art methods, especially in terms of resolving artifacts and maintaining high-fidelity details.", "section": "5 Experiments"}]