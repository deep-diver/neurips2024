{"references": [{"fullname_first_author": "A. Agarwal", "paper_title": "Meteor, M-BLEU and M-TER: Evaluation metrics for high-correlation with human rankings of machine translation output", "publication_date": "2008-06-01", "reason": "This paper introduces evaluation metrics crucial for assessing the performance of machine translation systems, directly relevant to the paper's NLP tasks."}, {"fullname_first_author": "P. Izmailov", "paper_title": "Averaging weights leads to wider optima and better generalization", "publication_date": "2018-00-00", "reason": "This foundational work on Stochastic Weight Averaging (SWA) is central to the paper's model fusion approach, demonstrating the benefits of averaging model weights for improved generalization."}, {"fullname_first_author": "E. Brochu", "paper_title": "A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning", "publication_date": "2010-12-01", "reason": "This paper provides a comprehensive tutorial on Bayesian Optimization (BO), a core methodology used in the paper for efficient hyperparameter tuning and model fusion."}, {"fullname_first_author": "M. Wortsman", "paper_title": "Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time", "publication_date": "2022-00-00", "reason": "This highly relevant paper introduces the concept of 'Model Soups,' a model fusion technique directly related to and compared with the proposed BOMF method."}, {"fullname_first_author": "M. T. M. Emmerich", "paper_title": "Single-and multiobjective evolutionary optimization assisted by Gaussian random field metamodels", "publication_date": "2006-00-00", "reason": "This paper details multi-objective evolutionary optimization, providing the theoretical foundation for the paper's multi-objective Bayesian optimization (MOBO) approach to model fusion."}]}