[{"figure_path": "LzLeAscHnj/tables/tables_3_1.jpg", "caption": "Table 1: Comparisons for various OFT-based adapters", "description": "This table compares three different orthogonal fine-tuning (OFT) based adapter methods: OFT [37], BOFT [34], and the proposed Householder Reflection Adaptation (HRA). For each method, it illustrates the implementation, the number of parameters, the computational complexity, and whether it can cover the whole set of orthogonal matrices (Odxd).  The table shows that HRA achieves comparable parameter efficiency and computational complexity to OFT and BOFT while offering a more flexible and potentially lower-complexity implementation.", "section": "3.2 Comparisons with Existing OFT Methods"}, {"figure_path": "LzLeAscHnj/tables/tables_5_1.jpg", "caption": "Table 2: Results (%) of various methods on GLUE development set. The best results on each dataset are shown in bold, and the second best results are shown in underline. We report the matched accuracy for MNLI, Matthew's correlation for CoLA and average correlation for STS-B.", "description": "This table presents the performance comparison of different parameter-efficient fine-tuning (PEFT) methods on the GLUE benchmark dataset.  It shows the average score achieved by each method across eight different GLUE tasks, along with the number of trainable parameters used.  The best and second-best results for each task are highlighted.", "section": "4.1 Natural Language Understanding"}, {"figure_path": "LzLeAscHnj/tables/tables_7_1.jpg", "caption": "Table 3: The results (%) of LLaMA2-7B on classic natural language processing tasks after fine-tuned on MATHQA by LoRA and HRA, respectively.", "description": "This table presents the performance of LLaMA2-7B on various natural language processing tasks after fine-tuning with LoRA and HRA methods.  The tasks include ARC, HellaSwag, MMLU, Winogrande, and HumanEval.  The \"Overall Impact\" column shows the percentage change in performance compared to the original LLaMA2-7B model.  The table highlights that HRA demonstrates less performance degradation after fine-tuning on MATHQA compared to LoRA, indicating better preservation of pre-training knowledge.", "section": "4.2 Mathematical Reasoning of LLM"}, {"figure_path": "LzLeAscHnj/tables/tables_7_2.jpg", "caption": "Table 1: Comparisons for various OFT-based adapters", "description": "This table compares three different orthogonal fine-tuning (OFT) based adapter methods: OFT, BOFT, and the proposed HRA method. For each method, it shows the implementation details (using illustrations), the number of trainable parameters, the computational complexity and the model capacity. The table helps to illustrate the differences and similarities between these OFT based methods in terms of their efficiency and capacity.", "section": "3 Proposed Method"}, {"figure_path": "LzLeAscHnj/tables/tables_15_1.jpg", "caption": "Table 5: The hyperparameters for DeBERTaV3-base on tasks included in the GLUE benchmark.", "description": "This table shows the hyperparameters used for training the DeBERTaV3-base model on the GLUE benchmark using the HRA method.  Specifically, it lists the number of epochs, learning rate, warm-up steps, and maximum sequence length for each of the eight GLUE tasks: MNLI, SST-2, CoLA, QQP, QNLI, RTE, MRPC, and STS-B.  Two different sets of hyperparameters are shown, representing variations of the HRA method with and without orthogonality regularization.", "section": "B Implementation Details"}, {"figure_path": "LzLeAscHnj/tables/tables_16_1.jpg", "caption": "Table 6: The comparison for various models on their computational efficiency.", "description": "This table compares the computational efficiency of different model adaptation methods, including LoRA, OFT, and HRA, when adapting the LLaMA2-7B model on the MetaMathQA dataset.  It shows the parameter ratio (percentage of trainable parameters relative to the original model size), training time (in hours), and peak memory usage (in GB). The results indicate the relative efficiency and memory requirements of each method.", "section": "B Comparisons with Existing OFT Methods"}, {"figure_path": "LzLeAscHnj/tables/tables_16_2.jpg", "caption": "Table 1: Comparisons for various OFT-based adapters", "description": "This table compares three different orthogonal fine-tuning (OFT)-based adapter methods: OFT [37], BOFT [34], and the proposed Householder Reflection Adaptation (HRA).  For each method, it shows the implementation details, the number of parameters, and the computational complexity. It highlights the differences in how these methods construct orthogonal matrices and their implications for model size and efficiency. The illustration section uses diagrams to visualize the structure of the matrices for each method.", "section": "3.2 Comparisons with Existing OFT Methods"}]