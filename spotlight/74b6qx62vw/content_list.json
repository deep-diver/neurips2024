[{"type": "text", "text": "Sample-Efficient Private Learning of Mixtures of Gaussians ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Hassan Ashtiani Mahbod Majid Shyam Narayanan McMaster University MIT Citadel Securities\u2217 zokaeiam@mcmaster.ca mahbod@mit.edu shyam.s.narayanan@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We study the problem of learning mixtures of Gaussians with approximate differential privacy. We prove that roughly $k d^{2}+k^{1.5}d^{1.75}+k^{2}d$ samples suffice to learn a mixture of $k$ arbitrary $d$ -dimensional Gaussians up to low total variation distance, with differential privacy. Our work improves over the previous best result [AAL24b] (which required roughly $k^{2}d^{4}$ samples) and is provably optimal when $d$ is much larger than $k^{2}$ . Moreover, we give the first optimal bound for privately learning mixtures of $k$ univariate (i.e., 1-dimensional) Gaussians. Importantly, we show that the sample complexity for learning mixtures of univariate Gaussians is linear in the number of components $k$ , whereas the previous best sample complexity [AAL21] was quadratic in $k$ . Our algorithms utilize various techniques, including the inverse sensitivity mechanism [AD20b, AD20a, HKMN23], sample compression for distributions $\\mathrm{ABDH}^{+}20]$ , and methods for bounding volumes of sumsets. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Learning Gaussian Mixture Models (GMMs) is one of the most fundamental problems in algorithmic statistics. Gaussianity is a common data assumption, and the setting of Gaussian mixture models is motivated by heterogeneous data that can be split into numerous clusters, where each cluster follows a Gaussian distribution. Learning mixture models is among the most important problems in machine learning [Bis06], and is at the heart of several unsupervised and semi-supervised machine learning models. The study of Gaussian mixture models has had numerous scientific applications dating back to the 1890s [Pea94], and is a crucial tool in modern data analysis techniques in a variety of fields, including bioinformatics [LKWB22], anomaly detection $[Z\\dot{\\mathrm{SM}}^{+}18]$ , and handwriting analysis [Bis06]. ", "page_idx": 0}, {"type": "text", "text": "In this work, we study the problem of learning a GMM from samples. We focus on the density estimation setting, where the goal is to learn the overall mixture distribution up to low total variation distance. Unlike the parameter estimation setting for GMMs, density estimation can be done even without any boundedness or separation assumptions on the parameters of the components. In fact, it is known that mixtures of $k$ Gaussians in $d_{\\cdot}$ -dimensions can be learned up to total variation distance $\\alpha$ using $\\widetilde{\\cal O}(k d^{2}/\\alpha^{2})$ samples $[\\mathrm{ABH^{+}18}]$ . ", "page_idx": 0}, {"type": "text", "text": "Ensuring data privacy has emerged as an increasingly important challenge in modern data analysis and statistics. Differential privacy (DP) [DMNS06] is a rigorous way of defining privacy, and is considered to be the gold standard both in theory and practice, with deployments by Apple [Tea17], Google [EPK14], Microsoft [DKY17], and the US Census Bureau $[\\mathrm{DLS}^{+}1\\dot{7}]$ . As is the case for many data analysis tasks, standard algorithms for learning GMMs leak potentially sensitive information about the individuals who contributed data. This raises the question of whether we can do density estimation for GMMs under the constraint of differential privacy. ", "page_idx": 0}, {"type": "text", "text": "Private density estimation for GMMs with unrestricted Gaussian components is a challenging task. In fact, privately learning a single unrestricted Gaussian has been the subject of multiple recent studies [AAK21, $\\mathrm{KMS}^{+}\\bar{2}2\\mathrm{b}$ , AL22, KMV22, $\\mathrm{AKT}^{+}23$ , HKMN23]. Private learning of GMMs is significantly more challenging, because even without privacy constraints, parameter estimation for GMMs requires exponentially many samples in terms of the number of components [MV10]. Therefore, it is not clear how to use the typical recipe of \u201cadding noise\u201d to the estimated parameters or \u201cprivately choosing\u201d from the finite-dimensional space of parameters. Consequently, the only known sample complexity bounds for privately learning unrestricted GMMs are loose [AAL24b, AAL21]. ", "page_idx": 1}, {"type": "text", "text": "Let us first formally define the problem of learning GMMs. We represent a GMM $\\mathcal{D}=$ $\\textstyle\\sum_{i=1}^{k}w_{i}{\\mathcal{N}}(\\mu_{i},\\Sigma_{i})$ by its parameters, namely $\\{(w_{i},\\mu_{i},\\Sigma_{i})\\}_{i=1}^{k}$ , where $w_{i}~\\geq~0$ , $\\textstyle\\sum_{i}w_{i}\\ =\\ 1$ , $\\mu_{i}\\,\\in\\,\\mathbb{R}^{d}$ , and $\\Sigma_{i}$ is a positive definite matrix. In the following, a GMM learning algorithm $\\boldsymbol{\\mathcal{A}}$ receives a set of data points in $\\mathbb{R}^{d}$ and outputs a (representation of) a GMM. The total variation distance between two distributions is $\\begin{array}{r}{\\mathrm{d}_{\\mathrm{TV}}(\\tilde{\\mathcal{D}},\\mathcal{D})=\\frac{1}{2}\\int_{\\mathbb{R}^{d}}|\\mathcal{D}(x)-\\tilde{\\mathcal{D}}(x)|d x^{2}}\\end{array}$ . ", "page_idx": 1}, {"type": "text", "text": "Definition 1.1 (Learning GMMs). For $\\alpha,\\beta\\in(0,1)$ , we say $\\boldsymbol{\\mathcal{A}}$ learns GMMs with $n$ samples up to accuracy $\\alpha$ and failure probability $\\beta$ if for every $\\mathrm{GMM}\\,\\mathcal{D}$ , given samples $X_{1},\\ldots,X_{n}\\stackrel{i.i.d.}{\\sim}\\mathcal{D},$ it outputs (a representation of) a GMM $\\tilde{\\mathcal{D}}$ such that $\\mathrm{d}_{\\mathrm{TV}}(\\tilde{\\mathcal{D}},\\mathcal{D})\\leq\\alpha$ with probability at least $1-\\beta$ . ", "page_idx": 1}, {"type": "text", "text": "$\\alpha$ and $\\beta$ are called the accuracy and failure probability, respectively. For clarity of presentation, we will typically fix the value of $\\beta$ (e.g., $\\beta=1/3$ ). The above definition does not enforce the constraint of differential privacy. The following definitions formalizes (approximate) differential privacy. ", "page_idx": 1}, {"type": "text", "text": "Definition 1.2 (Differential Privacy (DP) [DMNS06, $\\mathrm{DKM^{+}06l}]$ ). Let $\\varepsilon,\\delta~\\geq~0$ . A randomized algorithm $A:\\mathcal{X}^{n}\\to{\\mathcal{O}}$ is said to be $(\\varepsilon,\\delta)$ -differentially private ( $[\\varepsilon,\\delta)$ -DP) if for any two neighboring datasets X $\\!\\!\\!\\!\\!\\mathbf{\\Psi}\\!\\!\\!\\!\\!\\!\\!\\mathbf{x}\\!\\!\\!\\!\\!\\!\\!\\!^{\\prime}\\in\\mathcal{X}^{n}$ and any measurable subset $O\\subset\\mathcal{O}$ , ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathbb{P}[A(\\mathbf{X}^{\\prime})\\in O]\\leq e^{\\varepsilon}\\cdot\\mathbb{P}[A(\\mathbf{X})\\in O]+\\delta.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "If the GMM learner $\\boldsymbol{\\mathcal{A}}$ of Definition 1.1 is $(\\varepsilon,\\delta)$ -DP, we say that $\\boldsymbol{\\mathcal{A}}$ privately learns GMMs. Formally, we have the following definition. ", "page_idx": 1}, {"type": "text", "text": "Definition 1.3 (Privately learning GMMs). Fix the number of samples $n$ , dimension $d$ , and number of mixture components $k$ . For $\\alpha,\\beta\\in(0,1)$ and $\\varepsilon,\\delta\\ge0$ , a randomized algorithm $\\boldsymbol{\\mathcal{A}}$ , that takes as input $X_{1},\\ldots,X_{n}\\in\\mathbb{R}^{d}$ , $(\\varepsilon,\\delta)$ -privately learns GMMs up to accuracy $\\alpha$ and failure probability $\\beta$ , if: ", "page_idx": 1}, {"type": "text", "text": "1. For any GMM $\\mathcal{D}$ that is a mixture of up to $k$ Gaussians in $d$ dimensions, if $\\textbf{X}=$ $\\{X_{1},\\ldots,X_{n}\\}\\overset{i.i.d.}{\\sim}\\mathcal{D},\\,\\mathcal{A}(X_{1},\\ldots,X_{n})$ outputs a GMM $\\tilde{\\mathcal{D}}$ such that $\\mathrm{d}_{\\mathrm{TV}}(\\tilde{\\cal D},{\\cal D})\\,\\le\\,\\alpha$ with probability at least $1-\\beta$ (over the randomness of the data $\\mathbf{X}$ and the algorithm $\\boldsymbol{\\mathcal{A}}$ ). 2. For any neighboring datasets $\\mathbf{X},\\mathbf{X}^{\\prime}\\in(\\mathbb{R}^{d})^{n}$ (not necessarily drawn from any GMM) and any measurable subset $O\\subset{\\mathcal{O}}$ , $\\mathbb{P}[A(\\mathbf{X}^{\\prime})\\in O]\\leq e^{\\varepsilon}\\cdot\\mathbb{P}[A(\\dot{\\mathbf{X}})\\in O]+\\delta.$ . ", "page_idx": 1}, {"type": "text", "text": "Finally, we assume a default value for $\\beta$ of $1/3$ , meaning that if not stated, the failure probability $\\beta$ is assumed to equal $1/3$ . ", "page_idx": 1}, {"type": "text", "text": "Our main goal in this paper is to understand the number of samples (as a function of the dimension $d$ , the number of mixture components $k$ , the accuracy $\\alpha$ , and the privacy parameters $\\varepsilon,\\delta$ ) that are needed to privately and accurately learn the GMM up to low total variation distance. ", "page_idx": 1}, {"type": "text", "text": "1.1 Results ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this work, we provide improved sample complexity bounds for privately learning mixtures of arbitrary Gaussians, improving over previous work of [AAL21, AAL24b]. Moreover, our sample complexity bounds are optimal in certain regimes, when the dimension is either 1 or a sufficiently large polynomial in $k$ and $\\log{\\frac{1}{\\delta}}$ . For general dimension $d$ , we prove the following theorem. ", "page_idx": 1}, {"type": "text", "text": "Theorem 1.4. For any $\\alpha,\\varepsilon,\\delta\\in(0,1),k,d\\in\\mathbb{N},$ , there exists an inefficient $(\\varepsilon,\\delta)$ -DP algorithm that can learn a mixture of $k$ arbitrary full-dimensional Gaussians in $d$ dimensions up to accuracy $\\alpha$ , using the following number of samples: ", "page_idx": 1}, {"type": "equation", "text": "$$\nn=\\widetilde O\\left(\\frac{k d^{2}}{\\alpha^{2}}+\\frac{k d^{2}+d^{1.75}k^{1.5}\\log^{0.5}(1/\\delta)+k^{1.5}\\log^{1.5}(1/\\delta)}{\\alpha\\varepsilon}+\\frac{k^{2}d}{\\alpha}\\right).\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "2We are slightly abusing the notation and using ${\\mathcal{D}}(x)$ as the pdf of $\\mathcal{D}$ at points $_x$ . ", "page_idx": 1}, {"type": "text", "text": "Notably, the mixing weights and the means can be arbitrary and the covariances of the Gaussians can be arbitrarily poorly conditioned, as long as the covariances are non-singular3. ", "page_idx": 2}, {"type": "text", "text": "We remark that we omit the dependence on $\\beta$ (and assume by default a failure probability of $1/3)$ . However, it is well-known that one can obtain failure probability $\\beta$ with only a multiplicative $O(\\log{1/\\beta})$ blowup in sample complexity, in a black-box fashion4. In fact, our analysis can yield even better dependencies on $\\beta$ in some regimes, though to avoid too much complication, we do not analyze this. ", "page_idx": 2}, {"type": "text", "text": "For reasonably large dimension, i.e., $d\\geq k^{2}\\log^{2}(1/\\delta)$ , this can be simplified to $\\begin{array}{r}{\\tilde{O}\\left(\\frac{k d^{2}}{\\alpha^{2}}+\\frac{k d^{2}}{\\alpha\\varepsilon}\\right)}\\end{array}$ , which is in fact optimal (see Theorem 1.6). Hence, we obtain the optimal sample complexity for sufficiently large dimension. Theorem 1.4 also improves over the previous best sample complexity upper bound of [AAL24b], which uses ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\widetilde{\\cal O}\\left(\\frac{k^{2}d^{4}+k d^{2}\\log(1/\\delta)}{\\alpha^{2}\\varepsilon}+\\frac{k d\\log(1/\\delta)}{\\alpha^{3}\\varepsilon}+\\frac{k^{2}d^{2}}{\\alpha^{4}\\varepsilon}\\right)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "samples. Our results provide a polynomial improvement in all parameters, but to simplify the comparison, if we ignore dependencies in the error parameter $\\alpha$ and privacy parameters $\\varepsilon,\\delta$ , we improve the sample complexity from $k^{2}d^{4}$ to $k d^{2}+k^{\\dot{2}}d+k^{1.5}d^{1.75}$ : note that our result is quadratic in the dimension whereas [AAL24b] is quartic. ", "page_idx": 2}, {"type": "text", "text": "When the dimension is $d=1$ , we can provide an improved result, which is optimal for learning mixtures of univariate Gaussians (see Theorem 1.6 for a matching lower bound). ", "page_idx": 2}, {"type": "text", "text": "Theorem 1.5. For any $\\alpha,\\varepsilon,\\delta\\in(0,1),k\\in\\mathbb{N},$ , there exists an inefficient $(\\varepsilon,\\delta)$ -DP algorithm that can learn a mixture of $k$ arbitrary univariate Gaussians (of nonzero variance) up to accuracy $\\alpha$ , using the following number of samples: ", "page_idx": 2}, {"type": "equation", "text": "$$\nn=\\widetilde{O}\\left(\\frac{k}{\\alpha^{2}}+\\frac{k\\log(1/\\delta)}{\\alpha\\varepsilon}\\right).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "For privately learning mixtures of univariate Gaussians, the previous best-known result for arbitrary Gaussians required $\\tilde{\\widetilde{O}}\\left(\\frac{k^{2}\\log^{3/2}(1/\\delta)}{\\alpha^{2}\\varepsilon}\\right)$ samples [AAL21]. Importantly, we are the first paper to show that the sample complexity can be linear in the number of components. ", "page_idx": 2}, {"type": "text", "text": "Our work purely focuses on sample complexity, and as noted in Theorems 1.4 and 1.5, they do not have polynomial time algorithms. We note that the previous works of [AAL21, AAL24b] also do not run in polynomial time. Indeed, there is reason to believe that even non-privately, it is impossible to learn GMMs in polynomial time (in terms of the optimal sample complexity) [DKS17, BRST21, GVV22]. ", "page_idx": 2}, {"type": "text", "text": "Finally, we prove the following lower bound for learning GMMs in any fixed dimension $d$ . ", "page_idx": 2}, {"type": "text", "text": "Theorem 1.6. Fix any dimension $d\\geq1$ number of components $k\\geq2$ , any $\\alpha,\\varepsilon$ at most a sufficiently small constant $c^{*}$ , and $\\delta\\,\\le\\,(\\alpha\\varepsilon/d)^{\\overline{{O}}(1)}$ . Then, any $(\\varepsilon,\\delta)$ -DP algorithm that can learn a mixture of $k$ arbitrary full-dimensional Gaussians in $d$ dimensions up to total variation distance $\\alpha$ , with probability at least $2/3$ , requires at least the following number of samples: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\tilde{\\Omega}\\left(\\frac{k d^{2}}{\\alpha^{2}}+\\frac{k d^{2}}{\\alpha\\varepsilon}+\\frac{k\\log(1/\\delta)}{\\alpha\\varepsilon}\\right).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Note that for $d=1$ , this matches the upper bound of Theorem 1.5, thus showing that our univariate result is near-optimal in all parameters $\\alpha,\\varepsilon,\\delta$ . Moreover, our lower bound refutes the conjecture of [AAL21], which conjectures that only $\\begin{array}{r}{\\Theta\\left(\\frac{k}{\\alpha^{2}}+\\frac{k}{\\alpha\\varepsilon}+\\frac{\\log\\left(1/\\delta\\right)}{\\varepsilon}\\right)}\\end{array}$ samples are needed in the univariate case and \u0398 k\u03b1d22 + k\u03b1d\u03b52 + log(\u03b51/\u03b4) samples are needed in the $d$ -dimensional case. However, we note that our lower bound asymptotically differs from the conjectured bound in [AAL21] only when $\\delta$ is extremely small. ", "page_idx": 2}, {"type": "text", "text": "1.2 Related work ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In the non-private setting, the sample complexity of learning unrestricted GMMs with respect to total variation distance (a.k.a. density estimation) is known to be $\\widetilde{\\Theta}(k d^{2}/\\alpha^{2})$ [ABM18, $\\mathrm{ABH^{+}18}\\ensuremath{\\mathrm{18}}$ ], where the upper bound is obtained by the so-called distributional  compression schemes. ", "page_idx": 3}, {"type": "text", "text": "In the private setting, the only known sample complexity upper bound for unrestricted GMMs [AAL24b] is roughly $k^{2}d^{4}\\log(1/\\delta)/(\\alpha^{4}\\varepsilon)$ , which exhibits sub-optimal dependence on various parameters5. This bound is achieved by running multiple non-private list-decoders and then privately aggregating the results. For the special case of axis-aligned GMMs, an upper bound of $k^{2}d\\,\\dot{\\log(1/\\delta)^{3/2}}/(\\alpha^{2}\\varepsilon)$ is known [AAL21]. These are the only known results even for privately learning (unbounded) univariate GMMs. In other words, the best known upper bound for sample complexity of privately learning univariate GMMs has quadratic dependence on $k$ . ", "page_idx": 3}, {"type": "text", "text": "In the related public-private setting [BKS22, $\\mathbf{BBC}^{+}23^{-}$ ], it is assumed that the learner has access to some public data. In this setting, $[\\mathbf{BBC}^{+}23]$ show that unrestricted GMMs can be learned with a moderate amount of public and private data. ", "page_idx": 3}, {"type": "text", "text": "Assuming the parameters of the Gaussian components (and the condition numbers of the covariance matrices) are bounded, one can create a cover for GMMs and use private hypothesis selection [BSKW19] or the private minimum distance estimator [AAK21] to learn the GMM. On the filp side, [ASZ21] prove a lower bound on the sample complexity of learning GMMs, though their lower bound is weaker than ours and is only against pure-DP algorithms. ", "page_idx": 3}, {"type": "text", "text": "The focus of our work is on density estimation. A related problem is learning the parameters a GMM, which has received extensive attention in the (non-private) literature (e.g., [Das99, MV10, BS10, LM21, $\\mathbf{B}\\mathbf{D}\\mathbf{J}^{+}22$ , LL22] among many other papers). To avoid identifiability issues, one has to assume that the Gaussian components are sufficiently separated and have large-enough weights. In the private setting, the early work of [NRS07] demonstrated a privatized version of [VW04] for learning GMMs with fixed (known) covariance matrices. The strong separation assumption (of $\\Omega(k^{1/4}))$ between the Gaussian components in [NRS07] was later relaxed to a weaker separation assumption $[\\mathsf{C C A d}^{+}23]$ . A substantially more general result for privately learning GMMs with unknown covariance matrices was established in [KSSU19], based on a privatized version of [AM05]. Yet, this approach also requires a polynomial separation (in terms of $k$ ) between the components, as well as a bound on the spectrum of the covariance matrices. $[\\mathsf{C K M}^{+}21]$ weakened the separation assumption of [KSSU19] and improved over their sample complexity. This result is based on a generic method that learns a GMM using a private learner for Gaussians and a non-private clustering method for GMMs. Finally, [AAL23] designed an efficient reduction from private learning of GMMs to its non-private counterpart, removing the boundedness assumptions on the parameters and achieving minimal separation (e.g., by reducing to [MV10]). Nevertheless, unlike density estimation, parameter estimation for unrestricted GMMs requires exponentially many samples in terms of $k$ [MV10]. ", "page_idx": 3}, {"type": "text", "text": "A final important question is that of efficient algorithms for learning GMMs. Much of the work on learning GMM parameters focuses on computational efficiency (e.g,. [MV10, BS10, LM21, $\\mathbf{B}\\mathbf{D}\\mathbf{J}^{+}22$ , LL22]), as does some work on density estimation (e.g., [CDSS14, ADLS17]). However, under some standard hardness assumptions, it is known that even non-privately learning mixtures of $k\\;d$ -dimensional Gaussians with respect to total variation distance cannot be done in polynomial time as a function of $k$ and $d$ [DKS17, BRST21, GVV22]. ", "page_idx": 3}, {"type": "text", "text": "Addendum. In a concurrent submission, [AAL24a] extended the result of [AAL24b] for learning unrestricted GMMs to the agnostic (i.e., robust) setting. In contrast, our algorithm works only in the realizable (non-robust) setting. Moreover, [AAL24a] slightly improved the sample complexity result of [AAL24b] from $\\widetilde O(\\log(1/\\delta)k^{2}d^{4}/(\\varepsilon\\alpha^{4}))$ to $\\widetilde O(\\log(1/\\delta)k^{2}d^{4}/(\\varepsilon\\alpha^{2}))$ . The sample complexity of our approach is sti ll significantly better than [ AAL24a] in terms of all parameters\u2014similar to the way it improved over [AAL24b]. ", "page_idx": 3}, {"type": "text", "text": "2 Technical overview and roadmap ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We highlight some of our conceptual and technical contributions. We mainly focus on the highdimensional upper bound, and discuss the univariate upper bound at the end. ", "page_idx": 4}, {"type": "text", "text": "2.1 Reducing to crude approximation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Suppose we are promised a bound on the means and covariances of the Gaussian components, i.e., ${\\begin{array}{r}{{\\frac{1}{R}}\\cdot{\\hat{I}}\\prec\\Sigma_{i}\\prec{\\hat{R^{\\prime}}}.\\,I}\\end{array}}$ and $\\|\\mu_{i}\\|_{2}\\leq R$ for all $i\\in[k]$ . In this case, there is in fact a known algorithm, using private hypothesis selection [BSKW19, AAK21], that can privately learn the distribution using only $\\begin{array}{r}{{\\cal O}\\left(\\frac{k d^{2}\\log R}{\\alpha^{2}}+\\frac{k d^{2}\\log R}{\\alpha\\varepsilon}\\right)}\\end{array}$ samples. Moreover, with a more careful application of the hypothesis selection results (see Appendix D), we can prove that result holds even if $\\left(\\mu_{i},\\Sigma_{i}\\right)$ are possibly unbounded, but we have some very crude approximation. By this, we mean that if for each $i\\in[k]$ we know some $\\hat{\\Sigma}_{i}$ such that $\\begin{array}{r}{\\frac{1}{R}\\cdot\\Sigma_{i}\\prec\\hat{\\Sigma}_{i}\\preccurlyeq R\\cdot\\Sigma_{i}}\\end{array}$ , then it suffices to have $\\begin{array}{r}{n=O\\left(\\frac{k d^{2}\\log{R}}{\\alpha^{2}}+\\frac{k d^{2}\\log{R}}{\\alpha\\varepsilon}\\right)}\\end{array}$ samples to learn the full GMM in total variation distance. ", "page_idx": 4}, {"type": "text", "text": "Our main goal will be to learn every covariance $\\Sigma_{i}$ with such an approximation, for $R\\,\\,=$ poly $\\textstyle(k,d,{\\frac{1}{\\alpha}},{\\frac{1}{\\varepsilon}})$ , so that $\\log{R}$ can be hidden in the $\\tilde{O}$ factor. To explain why this goal is sufficient, suppose we can crudely learn every covariance $\\Sigma_{i}$ with approximation ratio $R$ , using $n^{\\prime}$ samples. We then need O kd2 lo2g R+ kd2 log R $\\begin{array}{r}{{\\cal O}\\left(\\frac{k d^{2}\\log R}{\\alpha^{2}}+\\frac{k d^{2}\\log R}{\\alpha\\varepsilon}\\right)=\\tilde{\\cal O}\\left(\\frac{k d^{2}}{\\alpha^{2}}+\\frac{k d^{2}}{\\alpha\\varepsilon}\\right)}\\end{array}$ additional samples to learn the full distribution using hypothesis selection, so the total sample complexity is O\u02dc n\u2032 + kd2 \u03b1lo2g R+ kd2 \u03b1lo\u03b5g R . Hence, we will aim for this easier goal of crudely learning each covariance, for both Theorem 1.4 and Theorem 1.5, using as few samples $n^{\\prime}$ as possible. We will also need to approximate each mean $\\mu_{i}$ , though for simplicity we will just focus on covariances in this section. ", "page_idx": 4}, {"type": "text", "text": "2.2 Overview of Theorem 1.5 for univariate GMMs ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The m\u221aain goal will be to simply provide a rough approximation to the set of standard deviations $\\sigma_{i}=\\sqrt{\\Sigma_{i}}$ , as we can finish the procedure with hypothesis selection, as discussed above. ", "page_idx": 4}, {"type": "text", "text": "Bird\u2019s eye view: Say we are given a dataset $\\mathbf{X}=\\{X_{1},\\ldots,X_{n}\\}$ : note that every $X_{j}\\in\\mathbb{R}$ since we are dealing with univariate Gaussians. The main insight is to sort the data in increasing order (i.e., reorder so that $X_{1}\\leq X_{2}\\leq\\cdots\\leq X_{n})$ and consider the unordered multiset of successive differences $\\{X_{2}-X_{1},X_{3}-X_{2},\\ldots,X_{n}-X_{n-1}\\}$ . One can show that if a single datapoint $X_{j}$ is changed, then the set of consecutive differences (up to permutation) does not change in more than 3 locations (see Lemma F.5 for a formal proof). ", "page_idx": 4}, {"type": "text", "text": "We then apply a standard private histogram approach. Namely, for each integer $a\\in\\mathbb{Z}$ , we create a corresponding bucket $B_{a}$ , and map each $X_{j+1}-X_{j}$ into $B_{a}$ if $2^{a}\\leq X_{j+1}-X_{j}<2^{a+1}$ . If some mixture component $i$ had variance $\\Sigma_{i}=\\sigma_{i}^{2}$ , we should expect a significant number of $X_{j+1}-X_{j}$ to at least be crudely close to $\\sigma_{i}$ , such as for the $X_{j}$ drawn from the $i^{\\mathrm{th}}$ mixture component. So, some corresponding bucket should be reasonably large. Finally, by adding noise to the count of each bucket and taking the largest noisy counts, we will successfully find an approximation to all variances. ", "page_idx": 4}, {"type": "text", "text": "In more detail: For each (possibly negative) integer $a$ let $c_{a}$ be the number of indices $i$ such that $2^{a}\\,\\leq\\,X_{j+1}-X_{j}\\,<\\,2^{a+1}$ . We will prove that, if the weight of the $i^{\\mathrm{th}}$ component in the mixture is $w_{i}$ and there are $n$ points, then we should expect at least $\\Omega(w_{i}\\cdot n)$ indices $j$ to be in a bucket $a$ with $2^{a}$ within a $\\mathrm{poly}(n)$ multiplicative factor of the standard deviation $\\sigma_{i}$ (see Lemma F.3). The point of this observation is that there are at most $O(\\log n)$ buckets $B_{a}$ with $2^{a}$ between $\\frac{\\sigma_{i}}{\\mathrm{poly}(n)}$ and $O(\\sigma_{i})$ , but there are at least $\\Omega(w_{i}\\cdot n)$ indices mapping to one of these buckets. So by the Pigeonhole principle, one of these buckets has at least $\\textstyle\\Omega\\left({\\frac{w_{i}\\cdot n}{\\log n}}\\right)$ indices, i.e., $\\begin{array}{r}{c_{a}\\geq\\Omega\\left(\\frac{w_{i}\\cdot n}{\\log n}\\right)}\\end{array}$ for some $a$ with $\\begin{array}{r}{\\frac{\\sigma_{i}}{\\mathrm{poly}(n)}\\le2^{a}\\le O(\\sigma_{i}).}\\end{array}$ ", "page_idx": 4}, {"type": "text", "text": "Moreover, we know that if we change a single data point $X_{j}$ , the set of consecutive differences $\\{X_{j+1}-X_{j}\\}$ after sorting changes by at most 3. So, if we change a single $X_{j}$ , at most 3 of the counts $c_{a}$ can change, each by at most 3. ", "page_idx": 4}, {"type": "text", "text": "Now, for every integer $a$ , draw a noise value from the Truncated Laplace distribution (see Definition A.2 and Lemma A.3), and add it to $c_{a}$ to get a noisy count $\\tilde{c}_{a}$ . The details of the noise distribution are not important, but the idea is that this distribution is always bounded by $\\begin{array}{r}{O\\left(\\frac{1}{\\varepsilon}\\log\\frac{1}{\\delta}\\right)}\\end{array}$ . Moreover, the Truncated Laplace distribution preserves $(\\varepsilon,\\delta)$ -DP. This means that the counts $\\{\\widetilde{c}_{a}\\}_{a\\in\\mathbb{Z}}$ will have $(O(\\varepsilon),O(\\delta))$ -DP, because the true counts $c_{a}$ only change minimally across adjacent datasets. ", "page_idx": 5}, {"type": "text", "text": "Our crude approximation to the set of standard deviations will be the set of $2^{a}$ such that $\\tilde{c}_{a}$ exceeds some threshold which is a large multiple of $\\frac{1}{\\varepsilon}\\log{\\frac{1}{\\delta}}$ . If $\\begin{array}{r}{n\\geq\\tilde{O}\\left(\\frac{k\\log\\left(1/\\delta\\right)}{\\alpha\\varepsilon}\\right)}\\end{array}$ and the weight $w_{i}\\geq\\alpha/k$ , it is not hard to verify that $\\frac{w_{i}\\!\\cdot\\!n}{\\log n}$ exceeds a large multiple  of $\\frac{1}{\\varepsilon}\\log\\frac{1}{\\delta}$ . So, for each $i\\leq k$ with weight at least $\\alpha/k$ , some corresponding $a$ with pol\u03c3yi(n) \u22642a \u2264O(\u03c3i) will have count ca significantly exceeding the threshold, and thus noisy count $\\tilde{c}_{a}$ exceeding the threshold. This will be enough to crudely approximate the values $\\sigma_{i}$ coming from large enough weight. We can ignore any component with weight less than $\\alpha/k$ , as even if all but one of components have such small weight, together they only contribute $\\alpha$ weight. So, we can ignore them and it will only cost us $\\alpha$ in total variation distance, which we can afford. ", "page_idx": 5}, {"type": "text", "text": "Putting everything together: In summary, we needed O\u02dc k log\u03b1(\u03b51/\u03b4) samples to approximate each covariance (of sufficient weight) up to a $\\mathrm{poly}(n)$ multiplicative factor. By setting $R=\\mathrm{poly}(n)$ and using the reduction described in Section 2.1, we then need an additional $\\begin{array}{r}{O\\left(\\frac{k\\log n}{\\alpha^{2}}+\\frac{k\\log n}{\\alpha\\varepsilon}\\right)}\\end{array}$ samples. If we set $\\begin{array}{r}{n=\\tilde{O}\\left(\\frac{k}{\\alpha^{2}}+\\frac{k\\log\\left(1/\\delta\\right)}{\\alpha\\varepsilon}\\right)}\\end{array}$ , we will obtain that $\\begin{array}{r}{n\\geq\\tilde{O}\\left(\\frac{k\\log(1/\\delta)}{\\alpha\\varepsilon}\\right)+O\\left(\\frac{k\\log n}{\\alpha^{2}}+\\frac{k\\log n}{\\alpha\\varepsilon}\\right)}\\end{array}$ which is sufficient to solve our desired problem in the univariate setting. ", "page_idx": 5}, {"type": "text", "text": "Note that this proof relies heavily on the use of private histograms and the order of the data points in the real line. Therefore, it cannot be extended to the high-dimensional setting. We will use a completely different approach to prove Theorem 1.4. ", "page_idx": 5}, {"type": "text", "text": "2.3 Overview of Theorem 1.4 for general GMMs ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "As in the univariate case, we only need rough approximations of the covariances. We will learn the covariances one at a time: in each iteration, we privately identify a single covariance $\\hat{\\Sigma}_{i}$ which crudely approximates some covariance $\\Sigma_{i}$ in the mixture, with $(\\varepsilon/\\sqrt{k\\log(1/\\delta)},\\delta/k)$ -DP. Using the well-known advanced composition theorem (see Theorem A.1), we will get an overall privacy guarantee of $(\\varepsilon,\\delta)$ -DP. However, to keep things simple, we will usually aim for $(\\varepsilon,\\delta)$ -DP when learning each covariance, and we can replace $\\varepsilon$ with $\\varepsilon/{\\sqrt{k\\log(1/\\delta)}}$ and $\\delta$ with $\\delta/k$ at the end. ", "page_idx": 5}, {"type": "text", "text": "A natural approach for parameter estimation, rather than learn $\\Sigma_{i}$ one at a time, is to learn all of the covariances together. However, we believe that this approach would cause the sample complexity to multiply by a factor of $k$ , compared to learning a single covariance. The advantage of learning the covariances one at a time is that we can apply advanced composition: this will cause the sample complexity to multiply by roughly $\\sqrt{k\\log(1/\\delta)}$ instead. ", "page_idx": 5}, {"type": "text", "text": "In the rest of this subsection, our main focus is to identify a single crude approximation $\\hat{\\Sigma}_{i}$ . ", "page_idx": 5}, {"type": "text", "text": "Applying robustness-to-privacy: The first main insight is to use the robustness-to-privacy conversion of Hopkins et al. [HKMN23] (see also [AUZ23]). Hopkins et al. prove a black-box (but not computationally efficient) approach that can convert robust algorithms into differentially private algorithms, using the exponential mechanism and a well-designed score function. This reduction only works for finite dimensional parameter estimation and therefore cannot be applied directly to density estimation. On the other hand, parameter estimation for arbitrary GMMs requires exponentially many samples in the number of components [MV10]. However, we will demonstrate that this lower bound does not hold when we only need a very crude estimation of the parameters. ", "page_idx": 5}, {"type": "text", "text": "The idea is the following. For a dataset $\\mathbf{X}=\\{X_{1},\\ldots,X_{n}\\}$ , let $S=S(\\widetilde{\\Sigma},{\\bf X})$ be a score function, which takes in a dataset $\\mathbf{X}$ of size $n$ and some candidate covariance $\\widetilde{\\Sigma}$ , and outputs some non-negative integer. At a high level, the score function $S(\\widetilde{\\Sigma},{\\mathbf{X}})$ will be the smallest number of data points $t$ that we should change in $\\mathbf{X}$ to get to some new data set $\\mathbf{X^{\\prime}}$ with a specific desired property: $\\mathbf{X^{\\prime}}$ should \u201clook like\u201d a sample generated from a mixture distribution with\u03a3 being the covariance of one of its components \u2013 namely, a component with a significant mixing weight. One can define \u201clooks like\u201d in different ways, and we will adjust the precise definition later. We remark that this notion of score roughly characterizes robustness, because if the samples in $\\mathbf{X}$ were truly drawn from a Gaussian mixture model with covariances $\\{\\Sigma_{i}\\}_{i=1}^{k}$ , we should expect ${\\cal S}(\\Sigma_{i},{\\bf X})$ to be 0 (since $\\mathbf{X}$ should already satisfy the desired property), but if we altered $k$ data points, the score should be at most $k$ . The high-level choice of score function is somewhat inspired by a version of the exponential mechanism called the inverse sensitivity mechanism [AD20b, AD20a], though the precise way of defining the score function requires significant care and is an important contribution of this paper. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "The robustness-to-privacy framework of [HKMN23], tailored to learning a single covariance, implies the following general result, which holds for any score function $\\boldsymbol{S}$ following the blueprint above. For now, we state an informal (and slightly incorrect) version. ", "page_idx": 6}, {"type": "text", "text": "Theorem 2.1 (informal - see Theorem C.3 for the formal statement). For any $\\eta\\in[0,1)$ , and any dataset $\\mathbf{X}$ of size $n$ , define the value $V_{\\eta}(\\mathbf{X})$ to be the volume (i.e., Lebesgue measure) of the set of covariance matrices\u03a3 (where the covariance can be viewed as a vector by flattening), such that $S(\\widetilde{\\Sigma},\\mathbf{X})\\leq\\eta\\cdot n$ . ", "page_idx": 6}, {"type": "text", "text": "Fix a parameter $\\eta<0.1$ , and suppose that for any dataset $\\mathbf{X}$ of size n such that $V_{\\eta/2}(\\mathbf{X})$ is strictly positive, the ratio of volumes $V_{\\eta}(\\mathbf{X})/V_{\\eta/2}(\\mathbf{X})$ is at most some $K$ (which doesn\u2019t depend on $\\mathbf{X}$ ). Then, $\\begin{array}{r}{i f{n}\\geq\\frac{\\log K}{\\varepsilon\\cdot\\eta}}\\end{array}$ , there is a differentially private algorithm that can find a covariance $\\widetilde{\\Sigma}$ of low score (i.e., where $S(\\overset{\\cdot}{\\Sigma},{\\bf X})\\leq\\eta\\cdot n)$ using n samples. ", "page_idx": 6}, {"type": "text", "text": "Note that Theorem 2.1 does not seem to say anything about whether $\\mathbf{X}$ comes from a mixture of Gaussians. However, we aim to instantiate this theorem with a score function that is carefully designed for GMMs. Recall that we want $S(\\widetilde{\\Sigma},{\\mathbf{X}})$ to capture the number of points in $\\mathbf{X}$ that need to be altered to make it look like a data set that was generated from a mixture, with\u03a3 being the covariance of one of the Gaussian components. In other words, $S(\\widetilde{\\Sigma},{\\mathbf{X}})$ should be small if (and hopefully only if) a \u201cmildly corrupted\u201d version of $\\mathbf{X}$ includes a subset of points that are generated from a Gaussian with covariance $\\widetilde{\\Sigma}$ . At the heart of designing such a score function, one needs to come up with a form of \u201crobust Gaussianity tester\u201d that tells whether a given set of data points are generated from a Gaussian distribution. Aside from this challenge, the volume ratio associated with the chosen score function needs to be small for every dataset $\\mathbf{X}$ otherwise the above theorem would require a large $n$ (i.e., number of samples). These two challenges are, however, related. If the robust Gaussianity tester has high specificity\u2014i.e., rejects most of the sets that are not generated from a (corrupted) Gaussian\u2014then the volume ratio is likely to be small (i.e., a smaller number of candidates $\\widetilde{\\Sigma}$ would receive a good/low score). ", "page_idx": 6}, {"type": "text", "text": "First Attempt: We first try an approach which resembles that of [HKMN23] for privately learning a single Gaussian. We \u201cdefine\u201d $S(\\widetilde{\\Sigma},{\\mathbf{X}})$ as the smallest integer $t$ satisfying the following property: there exists a subset $\\mathbf{Y}\\subset\\mathbf{X}$ of size $n/k$ , such that we can change $t$ data points from $\\mathbf{Y}$ to get to ${\\bf{Y}}^{\\prime}$ , where ${\\bf Y^{\\prime}}$ \u201clooks like\u201d i.i.d. samples from a Gaussian with some covariance $\\Sigma$ that is \u201csimilar to\u201d $\\widetilde{\\Sigma}$ . The choice of $\\mathbf{Y}$ having size $n/k$ is motivated by the fact that each mixture component, on avera ge, has $n/k$ data points in $\\mathbf{X}$ . ", "page_idx": 6}, {"type": "text", "text": "The notions of \u201clooks like\u201d and \u201csimilar to\u201d of course need to be formally defined. We say $\\Sigma$ is \u201csimilar to\u201d $\\widetilde{\\Sigma}$ (or $\\Sigma\\approx\\widetilde{\\Sigma},$ ) if they are spectrally close, i.e., $0.5\\Sigma\\prec\\widetilde{\\Sigma}\\preccurlyeq2\\Sigma$ . We say that ${\\bf{Y}}^{\\prime}$ \u201clooks like\u201d sampl es from a Gaussian with covariance $\\Sigma$ if some covariance estimation algorithm predicts that ${\\bf{Y}}^{\\prime}$ came from a Gaussian with covariance $\\Sigma$ . The choice of covariance estimation algorithm will be quite nontrivial and ends up being a key ingredient in proving Theorem 1.4. ", "page_idx": 6}, {"type": "text", "text": "To apply Theorem 2.1, we first set $\\eta=c/k$ for some small constant $c$ . We cannot set a larger value $\\eta$ , because if we change $t\\approx n/k$ data points, we could in fact create a totally arbitrary new Gaussian component with large weight. Since there is no bound on the eigenvalues of the covariance matrix, this could cause the volume $V_{\\eta}(\\mathbf{X})$ to be infinite. The main question we must answer is how to bound the volume ratio $V_{\\eta}(\\mathbf{X})/V_{\\eta/2}(\\mathbf{\\dot{X}})$ . To answer this question, we first need to understand what it means for $S(\\widetilde{\\Sigma},\\mathbf{X})\\leq\\eta\\cdot n$ . If $S(\\widetilde{\\Sigma},\\mathbf{X})\\leq\\eta\\cdot n$ , then there exists a corresponding set $\\mathbf{Y}\\subset\\mathbf{X}$ of size $n/k$ , and we can change $\\eta\\cdot n=c\\cdot|\\mathbf{Y}|$ points from $\\mathbf{Y}$ to get to some ${\\bf{Y}}^{\\prime}$ which looks like samples from a Gaussian with covariance $\\Sigma\\approx\\widetilde{\\Sigma}$ . Thus, $\\mathbf{Y}$ looks like $c$ -corrupted samples from such a Gaussian (i.e., a $c$ fraction of the data is corrupted). This motivates using a robust covariance estimation algorithm: indeed, robust algorithms can still approximately learn $\\Sigma$ even if a small constant fraction of data is corrupted, so for any $\\mathbf{Y}\\subset\\mathbf{X}$ , we expect that no matter how we change a $c$ fraction of $\\mathbf{Y}$ to obtain ${\\bf{Y}}^{\\prime}$ , the robust algorithm\u2019s covariance estimate should not change much. So, for any fixed $\\mathbf{Y}$ , the set of possible $\\Sigma$ , and thus the set of possible $\\widetilde{\\Sigma}$ , should not be that large. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "In summary, to bound $V_{\\eta}(\\mathbf{X})$ versus $V_{\\eta/2}(\\mathbf{X})$ , there are at most $\\scriptstyle{\\binom{n}{n/k}}$ choices for $\\mathbf{Y}\\subset\\mathbf{X}$ in the former case, and at least 1 choice in the latter case (since we assume $V_{\\eta/2}(\\mathbf{X})>0$ in Theorem 2.1). Moreover, for any such $\\mathbf{Y}$ , the volume of corresponding\u03a3 should be exponential in $d^{2}$ (either for $V_{\\eta}(\\mathbf{X})$ or $V_{\\eta/2}(\\mathbf{X}))$ ), since the dimensionality of the covariance is roughly $d^{2}$ . So, this suggests that the overall volume ratio is at most $\\binom{n}{n/k}\\cdot e^{O(d^{2})}$ . Since $\\log\\binom{n}{n/k}\\approx(n/k)\\cdot\\log k$ , if we plug into Theorem 2.1 it suffices to have (n/k\u03b5)\u00b7 (lco/g kk)+d2. Unfortunately this is impossible unless \u03b5 \u2265log k. These ideas will serve as a good starting point, though we need to improve the volume ratio analysis. To do so, we also modify the robust algorithm, by strengthening the assumptions on what it means for samples to \u201clook like\u201d they came from a Gaussian. ", "page_idx": 7}, {"type": "text", "text": "Improvement via Sample Compression: To improve the volume ratio, we draw inspiration from a technique called sample compression, which has been used in previous work on non-private density estimation for mixtures of Gaussians $[\\mathrm{ABH^{+}18}$ , $\\mathrm{ABDH^{+}20)}$ ]. The idea behind sample compression is that one does not need the full set $\\mathbf{Y}$ to do robust covariance estimation; instead, we look at a smaller set of samples. For instance, if $\\mathbf{Y}\\subset\\mathbf{X}$ looks like $c$ -corrupted samples from a Gaussian of covariance $\\Sigma\\approx\\widetilde{\\Sigma}$ , we expect that a random subset $\\mathbf{Z}$ of $\\mathbf{Y}$ also looks like $c$ -corrupted samples from such a Gaussian. Moreover, as long as one uses $m\\geq O(d)$ corrupted samples from a Gaussian, we can still (inefficiently) approximate the covariance. This motivates us to modify the robust algorithm as follows: rather than just checking whether $\\mathbf{Y}$ looks like $c$ -corrupted samples from a Gaussian of covariance roughly $\\widetilde{\\Sigma}$ , we also test whether an average subset $\\mathbf{Z}\\subset\\mathbf{Y}$ of size $m$ does as well. Therefore, if $\\widetilde{\\Sigma}$ has low score, there exists a corresponding set $\\mathbf{Z}\\subset\\mathbf{X}$ of size $m$ , and there are only ${\\binom{n}{m}}\\leq e^{m\\log n}$ choices for $\\mathbf{Z}$ . So, now it suffices to have $\\begin{array}{r}{n\\geq\\frac{m\\log n+d^{2}}{\\varepsilon\\cdot(c/k)}}\\end{array}$ m \u03b5l\u00b7o(gc /nk+)d , which will give us a bound of ${\\tilde{O}}(d^{2}k/\\varepsilon)$ , as long as $m\\leq O(d^{2})$ . Importantly, we still check the robust algorithm on $\\mathbf{Y}$ of size roughly $n/k$ , which allows us to keep the robustness threshold $\\eta$ at roughly $c/k$ . ", "page_idx": 7}, {"type": "text", "text": "There is one important caveat that for each subset $\\mathbf{Z}$ , there is a distinct corresponding covariance $\\Sigma$ , and the volume of $\\widetilde{\\Sigma}\\approx\\Sigma$ can change drastically as $\\Sigma$ changes. (For instance, the volume of $\\widetilde{\\Sigma}\\approx T\\cdot\\Sigma$ is $T^{\\Theta(d^{2})}$ times as large as the volume of $\\widetilde{\\Sigma}\\approx\\Sigma$ . Since we have no bounds on the possible covariances, $T$ could be unbounded.) For our v olume ratio to actually be bounded by about ${\\binom{n}{m}}\\cdot e^{O(d^{2})}$ , we want the volume of $\\widetilde{\\Sigma}\\approx\\Sigma$ to stay invariant with respect to $\\Sigma$ . This can be done by defining a \u201cnormalized volume\u201d whe re the normalization is inversely proportional to the determinant (see Appendix C.1 for more details). The robustness-to-privacy conversion (Theorem 2.1) will still hold. ", "page_idx": 7}, {"type": "text", "text": "While the bound of ${\\tilde{O}}(d^{2}k/\\varepsilon)$ seems good, we recall that this bound is merely the sample complexity for $(\\varepsilon,\\delta)$ -DP crude approximation of a single Gaussian component. As discussed at the beginning of this subsection, to learn all $k$ Gaussians, we actually need $(\\varepsilon/\\sqrt{k\\log(1/\\delta)},\\delta/k)$ -DP, rather than $(\\varepsilon,\\delta)$ -DP, when crudely approximating a single component. This will still end up leading to a significant improvement over previous work [AAL24b], but we can improve the volume ratio even further and thus obtain even better sample complexity. ", "page_idx": 7}, {"type": "text", "text": "Improving Dimension Dependence: Previously, we used the fact that the volume of candidate\u03a3 (corresponding to a fixed $\\mathbf{Z}$ ) was roughly exponential in $d^{2}$ for either $V_{\\eta/2}(\\mathbf{X})$ or $V_{\\eta}(\\mathbf{X})$ , so the rat io should also be roughly exponential in $d^{2}$ . Here, we improve this ratio, which will improve the overall volume ratio. ", "page_idx": 7}, {"type": "text", "text": "First, we return to understanding the guarantees of the robust algorithm. It is known that, given $m\\,\\geq\\,O(d)$ samples from a Gaussian of covariance $\\Sigma$ , we can provide an estimate $\\hat{\\Sigma}$ such that $(1-O(\\sqrt{d/m}))\\Sigma\\prec\\hat{\\Sigma}\\prec(1-O(\\sqrt{d/m}))\\Sigma$ . As above, we need to solve this even if a $c$ fraction of samples are corrupted. While this can cause the relative spectral error to increase from $1\\pm O(\\sqrt{d/m})$ to $1\\pm O(c+{\\sqrt{d/m}})$ , for now let us ignore the additional $c$ factor. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "If $V_{\\eta/2}(\\mathbf{X})>0$ , then there is some covariance $\\Sigma$ and some set $\\mathbf{Y}$ of size $n/k$ , where the robust algorithm thinks $\\mathbf{Y}$ looks like (possibly corrupted) Gaussian samples of covariance $\\Sigma$ . So, every\u03a3 such that $0.5\\Sigma\\prec\\widetilde{\\Sigma}\\preccurlyeq2\\Sigma$ has score of at most $\\eta/2\\cdot n$ . This gives us a lower bound on $V_{\\eta/2}(\\mathbf{X})$ . We now want to upper bound $V_{\\eta}(\\mathbf{X})$ . If $S(\\widetilde{\\Sigma},{\\mathbf{X}})<\\eta n$ , we still have that the robust algorithm thinks some $\\mathbf{Y}$ looks like Gaussian samples of covariance $\\Sigma$ , where $0.5\\Sigma\\prec\\widetilde{\\Sigma}\\preccurlyeq2\\Sigma$ . But now, we use the additional fact that for some $\\mathbf{Z}\\subset\\mathbf{Y}$ of size $m$ , the robust algorithm on $\\mathbf{Z}$ finds a covariance $\\hat{\\Sigma}$ . By the accuracy of the robust algorithm, $\\Sigma$ and $\\hat{\\Sigma}$ should be similar, i.e., $(1-O({\\sqrt{d/m}}))\\Sigma\\preccurlyeq\\hat{\\Sigma}\\preccurlyeq$ $(1-O({\\sqrt{d/m}}))\\Sigma$ (where we ignored the $c$ factor). Thus, there exists some $\\mathbf{Z}\\subset\\mathbf{X}$ of size $m$ and a $\\hat{\\Sigma}$ corresponding to $\\mathbf{Z}$ , such that $0.5(1-O(\\sqrt{d/m}))\\hat{\\Sigma}\\preccurlyeq\\widetilde\\Sigma\\preccurlyeq2(1+O(\\sqrt{d/m}))\\hat{\\Sigma}$ . ", "page_idx": 8}, {"type": "text", "text": "Therefore, from $\\eta/2$ to $\\eta$ , we have dilated the candidate set of $\\widetilde{\\Sigma}$ by a factor of $1+O({\\sqrt{d/m}})$ in the worst case, and we have at least 1 choice in the $\\eta/2$ case but at most $\\binom{n}{m}$ choices in the $\\eta$ case. Thus, the overall volume ratio is at most ${\\binom{n}{m}}\\cdot(1+O({\\sqrt{d/m}}))^{d^{2}}=e^{O(m\\log n+d^{2}\\cdot{\\sqrt{d/m}})}$ , since the dimension of $\\widetilde{\\Sigma}$ is roughly $d^{2}$ . Consequently, it now suffices to have $\\begin{array}{r}{n\\geq\\frac{m\\log n+d^{2}\\cdot\\sqrt{d/m}}{\\varepsilon\\cdot(c/k)}}\\end{array}$ : setting $m=d^{5/3}$ gives us an improved bound of ${\\tilde{O}}(d^{5/3}k/\\varepsilon)$ for learning a single $\\Sigma_{i}$ . ", "page_idx": 8}, {"type": "text", "text": "There are some issues with this approach: most notably, we ignored the fact that the spectral error is really $c+\\sqrt{d/m}$ rather than $\\sqrt{d/m}$ . However, the robust algorithm can do better than just estimating up to spectral error $c+\\sqrt{d/m}$ : it can also get an improved Frobenius error. While we will not formally state the guarantees on the robust algorithm here (see Theorem B.3 for the formal statement), the main high-level observation is that if the robust estimator \u03a3\u02c6 can be $1\\pm c$ times as large as the true covariance $\\Sigma$ in only $O(1)$ directions then for an average direction the ratio of $\\hat{\\Sigma}$ to $\\Sigma$ will be $1\\pm O(\\sqrt{d/m})$ . We can utilize this observation to bound the volume ratio, using some careful $\\varepsilon$ -net arguments (this is executed in Appendix C.3). Our dimension dependence of $d^{5/3}$ will increase to $d^{7/4}$ , though this still improves over the previous $d^{2}$ bound. ", "page_idx": 8}, {"type": "text", "text": "We will formally define the score function $S(\\widetilde{\\Sigma},{\\mathbf{X}})$ in Appendix E.1 and fully analyze the application of the robustness-to-privacy conversion, as outlined here, in Appendix E.2. ", "page_idx": 8}, {"type": "text", "text": "Accuracy: One important final step that we have so far neglected is ensuring that any\u03a3 of low score must be crudely close to some $\\Sigma_{i}$ , if $\\mathbf{X}$ is actually drawn from a GMM. We will j ust focus on the case where ${\\cal S}(\\widetilde{\\Sigma},{\\bf X})=0$ , so some $\\mathbf{Y}\\subset\\mathbf{X}$ of size $n/k$ looks like a set of samples from a Gaussian with covariance\u03a3. If the samples $\\mathbf{Y}$ all came from the $i^{\\th}$ -th component of the GMM, then it will not be difficult to show $\\widetilde{\\Sigma}$ is similar to $\\Sigma_{i}$ . The more difficult case is when data point in $\\mathbf{Y}$ are generated from several components. ", "page_idx": 8}, {"type": "text", "text": "However, if $n\\geq20k^{2}d.$ , then $|\\mathbf{Y}|\\geq20k d$ , which means that by the Pigeonhole Principle, at least $20d$ points in $\\mathbf{Y}$ come from the same mixture component $\\left(\\mu_{i},\\Sigma_{i}\\right)$ . We are able to prove that, with high probability over samples drawn from a single Gaussian component $\\mathcal{N}(\\mu_{i},\\Sigma_{i})$ , that the empirical covariance of any subset of size at least $20d$ is crudely close to $\\Sigma_{i}$ (see Corollary B.5). As a result, when verifying that a subset $\\mathbf{Y}$ \u201clooks like\u201d i.i.d. Gaussian samples with covariance $\\Sigma$ , we can ensure that the empirical covariance of every subset $\\mathbf{Z}\\subset\\mathbf{Y}$ of size $20d$ is crudely close to $\\Sigma$ . Thus, if the score ${\\cal S}(\\widetilde{\\Sigma},{\\bf X})=0.$ , $\\widetilde{\\Sigma}$ is close to $\\Sigma$ , which is crudely close to some $\\Sigma_{i}$ . ", "page_idx": 8}, {"type": "text", "text": "We also formally analyze the accuracy in Appendix E.2. ", "page_idx": 8}, {"type": "text", "text": "Putting everything together: To crudely learn a single Gaussian component with $(\\varepsilon,\\delta)$ -DP, we will need $n\\geq\\tilde{O}(d^{7/4}k/\\varepsilon)$ samples to find some covariance $\\widetilde{\\Sigma}$ with low score, and we also need $n\\geq O(k^{2}d)$ so that a covariance $\\widetilde{\\Sigma}$ of low score is actually a crude approximation to one of the real mixture components. To crudely  approximate all components, we learn each Gaussian component with $(\\varepsilon/\\sqrt{k\\log(1/\\delta)},\\delta/k)$ -DP. The advanced composition theorem will imply that repeating this procedure $k$ times on the same data (to learn all $k$ components) will be $(\\varepsilon,\\delta)$ -DP. Hence, by replacing $\\varepsilon$ with $\\varepsilon/{\\sqrt{k\\log(1/\\delta)}}$ , we get that it suffices for $\\begin{array}{r}{n\\geq\\tilde{O}\\left(\\frac{d^{7/4}k^{3/2}\\sqrt{\\log(1/\\delta)}}{\\varepsilon}+k^{2}d\\right)}\\end{array}$ , if we need to crudely learn all of the covariances. Finally, we can apply the private hypothesis selection technique (recall Section 2.1), which requires an additional $\\begin{array}{r}{\\tilde{O}\\left(\\frac{d^{2}k}{\\alpha^{2}}+\\frac{d^{2}k}{\\alpha\\varepsilon}\\right)}\\end{array}$ . Combining these terms will give the final sample complexity. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "We remark that the sample complexity obtained above is actually better than the complexity in Theorem 1.4. There are two reasons for this. The first is that we have been assuming each component has weight $1/k$ , meaning it contributes about $n/k$ data points. In reality, the weights may be arbitrary and thus some components may have much fewer data points. However, it turns out that one can actually ignore any component with less than $\\alpha/k$ weight, if we want to solve density estimatio\u221an up to total variation distance $\\alpha$ . This will multiply the sample complexity terms $\\tilde{O}\\left(\\frac{d^{7/4}k^{3/2}\\sqrt{\\log(1/\\delta)}}{\\varepsilon}+k^{2}d\\right)$ , needed for crude approximation, by a factor of $1/\\alpha$ . Finally, the iandfdoirnmg atlh eT haedodrietimo n2a.l1 t eisr msl iogfh $\\frac{k^{\\bar{3}/2}\\log^{3/2}(1/\\delta)}{\\alpha\\varepsilon}$ .  anAdl otnhge  awcitchu rtahtee  fvinerasl itoenr mo $\\begin{array}{r}{\\tilde{O}\\left(\\frac{d^{2}k}{\\alpha^{2}}+\\frac{d^{2}k}{\\alpha\\varepsilon}\\right)}\\end{array}$ ilflr oenmd  tuhpe private hypothesis selection, these terms will exactly match Theorem 1.4. ", "page_idx": 9}, {"type": "text", "text": "2.4 Roadmap ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In Appendix A, we note some general preliminary results. In Appendix B, we note some additional preliminaries on robust learning of a single Gaussian. In Appendix C, we discuss the robustnessto-privacy conversion and prove some volume arguments needed for Theorem 1.4. In Appendix D, we explain how to reduce to the crude approximation setting, using private hypothesis selection. In Appendix E, we design and analyze the algorithm for multivariate Gaussians, and prove Theorem 1.4. In Appendix F, we design and analyze the algorithm for univariate Gaussians, and prove Theorem 1.5. In Appendix G, we prove Theorem 1.6. Finally, Appendix H proves some auxiliary results that we state in Appendices B and C. ", "page_idx": 9}, {"type": "text", "text": "Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Our results are on theoretical guarantees on the sample complexity of privately learning Mixtures of Gaussians. We do not provide any efficient or practical algorithms, and focus on statistical guarantees. We also do not discuss how to set the parameters and accuracy guarantees for practical applications, this is a question best left to practitioners. Finally, we assume each sample is i.i.d. drawn from a Gaussian Mixture Model distribution, though we remark that we use a \u201crobustness-to-privacy\u201d framework that will automatically make our algorithm robust to a roughly $\\alpha/k$ fraction of corruptions. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements. Hassan Ashtiani was supported by an NSERC Discovery grant. Shyam Narayanan was supported by an NSF Graduate Fellowship and a Google Fellowship. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[AAK21] Ishaq Aden-Ali, Hassan Ashtiani, and Gautam Kamath. On the sample complexity of privately learning unbounded high-dimensional gaussians. In Algorithmic Learning Theory, pages 185\u2013216. PMLR, 2021.   \n[AAL21] Ishaq Aden-Ali, Hassan Ashtiani, and Christopher Liaw. Privately learning mixtures of axis-aligned gaussians. Advances in Neural Information Processing Systems, 34:3925\u2013 3938, 2021.   \n[AAL23] Jamil Arbas, Hassan Ashtiani, and Christopher Liaw. Polynomial time and private learning of unbounded gaussian mixture models. In International Conference on Machine Learning, pages 1018\u20131040. PMLR, 2023.   \n[AAL24a] Mohammad Afzali, Hassan Ashtiani, and Christopher Liaw. Agnostic private density estimation for gmms via list global stability. arXiv e-prints, pages arXiv\u20132407, 2024. ", "page_idx": 9}, {"type": "text", "text": "[AAL24b] Mohammad Afzali, Hassan Ashtiani, and Christopher Liaw. Mixtures of gaussians are privately learnable with a polynomial number of samples. In Algorithmic Learning Theory, pages 1\u201327. PMLR, 2024. $[\\mathrm{ABDH^{+}20}]$ Hassan Ashtiani, Shai Ben-David, Nicholas JA Harvey, Christopher Liaw, Abbas Mehrabian, and Yaniv Plan. Near-optimal sample complexity bounds for robust learning of gaussian mixtures via compression schemes. Journal of the ACM (JACM), 67(6):1\u201342, 2020. $[\\mathrm{ABH^{+}18}]$ Hassan Ashtiani, Shai Ben-David, Nicholas Harvey, Christopher Liaw, Abbas Mehrabian, and Yaniv Plan. Nearly tight sample complexity bounds for learning mixtures of gaussians via sample compression schemes. Advances in Neural Information Processing Systems, 31, 2018. [ABM18] Hassan Ashtiani, Shai Ben-David, and Abbas Mehrabian. Sample-efficient learning of mixtures. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, 2018. [AD20a] Hilal Asi and John C Duchi. Instance-optimality in differential privacy via approximate inverse sensitivity mechanisms. Advances in neural information processing systems, 33:14106\u201314117, 2020. [AD20b] Hilal Asi and John C Duchi. Near instance-optimality in differential privacy. CoRR, abs/2005.10630, 2020. [ADLS17] Jayadev Acharya, Ilias Diakonikolas, Jerry Li, and Ludwig Schmidt. Sample-optimal density estimation in nearly-linear time. In Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 1278\u20131289. SIAM, 2017. $[\\mathrm{AKT}^{+}23]$ Daniel Alabi, Pravesh K Kothari, Pranay Tankala, Prayaag Venkat, and Fred Zhang. Privately estimating a gaussian: Efficient, robust, and optimal. In Proceedings of the 55th Annual ACM Symposium on Theory of Computing, pages 483\u2013496, 2023. [AL22] Hassan Ashtiani and Christopher Liaw. Private and polynomial time algorithms for learning gaussians and beyond. In Conference on Learning Theory, pages 1075\u20131076. PMLR, 2022. [AM05] Dimitris Achlioptas and Frank McSherry. On spectral learning of mixtures of distributions. In International Conference on Computational Learning Theory, pages 458\u2013469. Springer, 2005. [ASZ21] Jayadev Acharya, Ziteng Sun, and Huanyu Zhang. Differentially private assouad, fano, and le cam. In Algorithmic Learning Theory, pages 48\u201378. PMLR, 2021. [AUZ23] Hilal Asi, Jonathan Ullman, and Lydia Zakynthinou. From robustness to privacy and back. In International Conference on Machine Learning, pages 1121\u20131146. PMLR, 2023. $[\\mathbf{BBC}^{+}23]$ Shai Ben-David, Alex Bie, Cl\u00e9ment L. Canonne, Gautam Kamath, and Vikrant Singhal. Private distribution learning with public data: The view from sample compression. In Advances in Neural Information Processing Systems (NeurIPS), 2023. $[\\mathbf{BDJ}^{+}22]$ Ainesh Bakshi, Ilias Diakonikolas, He Jia, Daniel M Kane, Pravesh K Kothari, and Santosh S Vempala. Robustly learning mixtures of k arbitrary gaussians. In Proceedings of the 54th Annual ACM SIGACT Symposium on Theory of Computing, pages 1234\u2013 1247, 2022. [Bis06] Christopher M. Bishop. Pattern Recognition and Machine Learning. Springer, 2006. [BKS22] Alex Bie, Gautam Kamath, and Vikrant Singhal. Private estimation with public data. Advances in Neural Information Processing Systems, 2022. [BRST21] Joan Bruna, Oded Regev, Min Jae Song, and Yi Tang. Continuous lwe. In Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing, pages 694\u2013707, 2021. ", "page_idx": 10}, {"type": "text", "text": "[BS10] Mikhail Belkin and Kaushik Sinha. Polynomial learning of distribution families. In 2010 IEEE 51st Annual Symposium on Foundations of Computer Science, pages 103\u2013112. IEEE, 2010.   \n[BSKW19] Mark Bun, Thomas Steinke, Gautam Kamath, and Zhiwei Steven Wu. Private hypothesis selection. Advances in Neural Information Processing Systems, 32, 2019. [BUV14] Mark Bun, Jonathan Ullman, and Salil Vadhan. Fingerprinting codes and the price of approximate differential privacy. In Proceedings of the forty-sixth annual ACM symposium on Theory of computing, pages 1\u201310, 2014.   \n$[\\mathsf{C C A d}^{+}23]$ Hongjie Chen, Vincent Cohen-Addad, Tommaso d\u2019Orsi, Alessandro Epasto, Jacob Imola, David Steurer, and Stefan Tiegel. Private estimation algorithms for stochastic block models and mixture models. arXiv preprint arXiv:2301.04822, 2023.   \n[CDSS14] Siu-On Chan, Ilias Diakonikolas, Rocco A Servedio, and Xiaorui Sun. Efficient density estimation via piecewise polynomial approximation. In Proceedings of the forty-sixth annual ACM symposium on Theory of computing, pages 604\u2013613, 2014.   \n$[\\mathsf{C K M}^{+}21]$ Edith Cohen, Haim Kaplan, Yishay Mansour, Uri Stemmer, and Eliad Tsfadia. Differentially-private clustering of easy instances. In International Conference on Machine Learning, pages 2049\u20132059. PMLR, 2021. [Das99] Sanjoy Dasgupta. Learning mixtures of gaussians. In 40th Annual Symposium on Foundations of Computer Science (Cat. No. 99CB37039), pages 634\u2013644. IEEE, 1999. [DK22] Ilias Diakonikolas and Daniel M. Kane. Algorithmic High-Dimensional Robust Statistics. Cambridge University Press, 2022.   \n$[\\mathrm{DKM}^{+}06]$ Cynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and Moni Naor. Our data, ourselves: Privacy via distributed noise generation. In Advances in Cryptology-EUROCRYPT 2006: 24th Annual International Conference on the Theory and Applications of Cryptographic Techniques, St. Petersburg, Russia, May 28-June 1, 2006. Proceedings 25, pages 486\u2013503. Springer, 2006. [DKS17] Ilias Diakonikolas, Daniel Kane, and Alistair Stewart. Statistical query lower bounds for robust estimation of high-dimensional gaussians and gaussian mixtures. In 58th IEEE Annual Symposium on Foundations of Computer Science, FOCS \u201817, pages 73\u201384, 2017. [DKY17] Bolin Ding, Janardhan Kulkarni, and Sergey Yekhanin. Collecting telemetry data privately. In Advances in Neural Information Processing Systems (NeurIPS), pages 3571\u20133580, 2017.   \n$[\\mathrm{DLS^{+}}17]$ Aref N. Dajani, Amy D. Lauger, Phyllis E. Singer, Daniel Kifer, Jerome P. Reiter, Ashwin Machanavajjhala, Simson L. Garfinkel, Scot A. Dahl, Matthew Graham, Vishesh Karwa, Hang Kim, Philip Lelerc, Ian M. Schmutte, William N. Sexton, Lars Vilhuber, and John M. Abowd. The modernization of statistical disclosure limitation at the u.s. Census Bureau, 2017. In the September 2017 meeting of the Census Scientific Advisory Committee, 2017.   \n[DMNS06] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. In Theory of Cryptography: Third Theory of Cryptography Conference, TCC 2006, New York, NY, USA, March 4-7, 2006. Proceedings 3, pages 265\u2013284. Springer, 2006. [DR14] Cynthia Dwork and Aaron Roth. The algorithmic foundations of differential privacy. Foundations and Trends\u00ae in Theoretical Computer Science, 9(3\u20134):211\u2013407, 2014. [EPK14] \u00dalfar Erlingsson, Vasyl Pihur, and Aleksandra Korolova. RAPPOR: randomized aggregatable privacy-preserving ordinal response. In Proceedings of the 2014 ACM SIGSAC Conference on Computer and Communications Security (CCS), pages 1054\u2013 1067, 2014.   \n[GDGK20] Quan Geng, Wei Ding, Ruiqi Guo, and Sanjiv Kumar. Tight analysis of privacy and utility tradeoff in approximate differential privacy. In International Conference on Artificial Intelligence and Statistics (AISTATS), volume 108 of Proceedings of Machine Learning Research, pages 89\u201399. PMLR, 2020. [GVV22] Aparna Gupte, Neekon Vafa, and Vinod Vaikuntanathan. Continuous LWE is as hard as LWE & applications to learning gaussian mixtures. In 63rd IEEE Annual Symposium on Foundations of Computer Science, FOCS 2022, Denver, CO, USA, October 31 - November 3, 2022, pages 1162\u20131173. IEEE, 2022.   \n[HKMN23] Samuel B Hopkins, Gautam Kamath, Mahbod Majid, and Shyam Narayanan. Robustness implies privacy in statistical estimation. In Symposium on Theory of Computing, pages 497\u2013506, 2023.   \n[KMS22a] Gautam Kamath, Argyris Mouzakis, and Vikrant Singhal. New lower bounds for private estimation and a generalized fingerprinting lemma. In Advances in Neural Information Processing Systems 35, NeurIPS \u201922, 2022.   \n$[\\mathrm{KMS}^{+}22\\mathrm{b}]$ Gautam Kamath, Argyris Mouzakis, Vikrant Singhal, Thomas Steinke, and Jonathan Ullman. A private and computationally-efficient estimator for unbounded gaussians. In Conference on Learning Theory, pages 544\u2013572. PMLR, 2022. [KMV22] Pravesh Kothari, Pasin Manurangsi, and Ameya Velingker. Private robust estimation by stabilizing convex relaxations. In Conference on Learning Theory, pages 723\u2013777. PMLR, 2022.   \n[KSSU19] Gautam Kamath, Or Sheffet, Vikrant Singhal, and Jonathan Ullman. Differentially private algorithms for learning mixtures of separated gaussians. Advances in Neural Information Processing Systems, 32, 2019. [KV18] Vishesh Karwa and Salil Vadhan. Finite sample differentially private confidence intervals. In Proceedings of the 9th Conference on Innovations in Theoretical Computer Science, ITCS \u201918, pages 44:1\u201344:9, Dagstuhl, Germany, 2018. Schloss Dagstuhl\u2013 Leibniz-Zentrum fuer Informatik.   \n[LKWB22] Ta-Chun Liu, Peter N Kalugin, Jennifer L Wilding, and Walter F Bodmer. Gmmchi: gene expression clustering using gaussian mixture modeling. BMC bioinformatics, 23(1):457, 2022. [LL22] Allen Liu and Jerry Li. Clustering mixtures with almost optimal separation in polynomial time. In Proceedings of the 54th Annual ACM SIGACT Symposium on Theory of Computing, pages 1248\u20131261, 2022. [LM21] Allen Liu and Ankur Moitra. Settling the robust learnability of mixtures of gaussians. In Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing, pages 518\u2013531, 2021. [MH08] Arakaparampil M Mathai and Hans J Haubold. Special functions for applied scientists, volume 4. Springer, 2008. [MV10] Ankur Moitra and Gregory Valiant. Settling the polynomial learnability of mixtures of gaussians. In 2010 IEEE 51st Annual Symposium on Foundations of Computer Science, pages 93\u2013102. IEEE, 2010. [Nar23] Shyam Narayanan. Better and simpler lower bounds for differentially private statistical estimation. arXiv preprint arXiv:2310.06289, 2023. [NRS07] Kobbi Nissim, Sofya Raskhodnikova, and Adam Smith. Smooth sensitivity and sampling in private data analysis. In Proceedings of the thirty-ninth annual ACM symposium on Theory of computing, pages 75\u201384, 2007. [Pea94] Karl Pearson. Contributions to the mathematical theory of evolution. Philosophical Transactions of the Royal Society of London, 1894. ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "[PH24] Victor S Portella and Nick Harvey. Lower bounds for private estimation of gaussian covariance matrices under all reasonable parameter regimes. arXiv preprint arXiv:2404.17714, 2024. [Tea17] Apple Differential Privacy Team. Learning with privacy at scale. Apple Machine Learning Journal, 1(8), 2017. [VW04] Santosh Vempala and Grant Wang. A spectral algorithm for learning mixture models. Journal of Computer and System Sciences, 68(4):841\u2013860, 2004. $[Z\\mathrm{SM}^{+}18]$ Bo Zong, Qi Song, Martin Renqiang Min, Wei Cheng, Cristian Lumezanu, Daeki Cho, and Haifeng Chen. Deep autoencoding gaussian mixture model for unsupervised anomaly detection. In International conference on learning representations, 2018. ", "page_idx": 13}, {"type": "text", "text": "A Preliminaries ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Differential Privacy ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We state the advanced composition theorem of differential privacy. ", "page_idx": 14}, {"type": "text", "text": "Theorem A.1 (Advanced Composition Theorem [DR14, Theorem 3.20]). Let \u03b5, \u03b4, $\\delta^{\\prime}\\geq0$ be arbitrary parameters. Let $\\mathcal{A}_{1},\\ldots,\\mathcal{A}_{k}$ be algorithms on a dataset $\\mathbf{X}$ , where each $\\boldsymbol{A}_{i}$ is $(\\varepsilon,\\delta)$ -DP. Then, the concatenation ${\\cal A}({\\bf X})=({\\cal A}_{1}({\\bf X}),\\ldots,{\\cal A}_{k}({\\bf X}))\\;i s\\left(\\sqrt{2k\\log\\frac{1}{\\delta^{\\prime}}}\\cdot\\varepsilon+k\\varepsilon\\cdot(e^{\\varepsilon}-1),k\\cdot\\delta+\\delta^{\\prime}\\right)\\!\\cdot{\\cal D}\\!P.$ ", "page_idx": 14}, {"type": "text", "text": "Moreover, this holds even if the algorithms $A_{i}$ are adaptive. By this, we mean that for all $i\\geq2$ the algorithm $\\mathcal{A}_{i}$ is allowed to depend on $\\mathcal{A}_{1}(\\mathbf{X}),\\ldots,\\mathcal{A}_{i-1}(\\mathbf{X}).$ . However, privacy must still hold for $A_{i}$ , conditioned on the previous outputs $\\mathcal{A}_{1}(\\mathbf{X}),\\ldots,\\mathcal{A}_{i-1}(\\mathbf{X})$ . ", "page_idx": 14}, {"type": "text", "text": "Next, we note the properties of the Truncated Laplace distribution and mechanism. ", "page_idx": 14}, {"type": "text", "text": "Definition A.2 (Truncated Laplace Distribution). $\\mathrm{For}\\,\\Delta,\\varepsilon,\\delta>0$ , the Truncated Laplace Distribution $\\mathrm{TLap}(\\Delta,\\varepsilon,\\delta)$ is the distribution with PDF proportional to $e^{-|x|\\cdot\\varepsilon/\\Delta}$ on the region $[-A,A]$ , where $\\begin{array}{r}{A=\\frac{\\Delta}{\\varepsilon}\\cdot\\log\\left(1+\\frac{e^{\\varepsilon}-1}{2\\delta}\\right)}\\end{array}$ , and PDF 0 outside the region $[-A,A]$ . ", "page_idx": 14}, {"type": "text", "text": "Lemma A.3 (Truncated Laplace Mechanism [GDGK20, Theorem 1]). Let $f:\\mathcal{X}^{n}\\to\\mathbb{R}$ be a realvalued function, and let $\\Delta>0$ , such that for any neighboring datasets $\\mathbf{X},\\mathbf{X}^{\\prime}$ , $|f(\\mathbf{X})-f(\\mathbf{X}^{\\prime})|\\leq\\Delta$ . Then, the mechanism $\\boldsymbol{\\mathcal{A}}$ that outputs $f(\\mathbf{X})+\\operatorname{TLap}(\\Delta,\\varepsilon,\\delta)$ is $(\\varepsilon,\\delta)$ -DP. ", "page_idx": 14}, {"type": "text", "text": "A.2 Matrix and Concentration Bounds ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we note some standard but useful lemmas. We first note the Courant-Fischer theorem. ", "page_idx": 14}, {"type": "text", "text": "Theorem A.4 (Courant-Fischer). Let $A\\,\\in\\,\\mathbb{R}^{d\\times d}$ be a real symmetric matrix, with eigenvalues $\\lambda_{1}\\geq\\lambda_{2}\\geq\\cdot\\cdot\\geq\\lambda_{d}$ . Then, for each $k$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\lambda_{k}=\\operatorname*{max}_{\\stackrel{V\\subset\\mathbb{R}^{d}}{\\mathrm{dim}(V)=k}}\\operatorname*{min}_{\\stackrel{x\\in V}{\\|x\\|_{2}=1}}x^{\\top}A x=\\operatorname*{min}_{\\stackrel{V\\subset\\mathbb{R}^{d}}{\\mathrm{dim}(V)=d-k+1}}\\operatorname*{max}_{x\\in V}x^{\\top}A x,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $V$ refers to a linear subspace of $\\mathbb{R}^{d}$ . ", "page_idx": 14}, {"type": "text", "text": "By setting $A=J^{\\top}J$ , we have the following corollary. ", "page_idx": 14}, {"type": "text", "text": "Corollary A.5. Let $J\\in\\mathbb{R}^{d\\times d}$ be a real (possibly asymmetric) matrix with singular values $\\sigma_{1}\\geq$ $\\sigma_{2}\\geq\\cdot\\cdot\\cdot\\geq\\sigma_{d}$ . Then, for each $k$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\sigma_{k}=\\operatorname*{max}_{\\stackrel{V\\subset\\mathbb{R}^{d}}{\\mathrm{dim}(V)=k}}\\operatorname*{min}_{\\stackrel{x\\in V}{\\mathrm{|}}|x||_{2}=1}\\|J x\\|_{2}=\\operatorname*{min}_{\\stackrel{V\\subset\\mathbb{R}^{d}}{\\mathrm{dim}(V)=d-k+1}}\\operatorname*{max}_{\\stackrel{x\\in V}{\\mathrm{|}}|x||_{2}=1}\\|J x\\|_{2}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Next, we note a basic proposition. ", "page_idx": 14}, {"type": "text", "text": "Proposition A.6. [HKMN23, Lemma $6.8J$ Let $\\boldsymbol{M}\\,\\in\\,\\mathbb{R}^{d\\times d}$ be a real symmetric matrix, and $J\\in\\mathbf{\\bar{R}}^{d\\times d}$ be any real-valued (but not necessarily symmetric) matrix such that $\\|J J^{\\top}\\!-\\!I\\|_{o p}\\leq\\phi\\leq1$ , for some parameter $\\phi$ . Then, $\\|J^{\\top}M J\\|_{F}^{2}\\leq(1+3\\phi)\\cdot\\|M\\|_{F}^{2}$ . ", "page_idx": 14}, {"type": "text", "text": "We also note the Hanson-Wright inequality. ", "page_idx": 14}, {"type": "text", "text": "Lemma A.7 (Hanson-Wright). Given a $d\\times d$ matrix $M\\in\\mathbb{R}^{d\\times d}$ and a $d$ -dimensional Gaussian vector $X\\sim{\\mathcal{N}}(0,I)\\,$ , for any $t\\geq0$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\left|X^{\\top}M X-\\mathbb{E}[X^{\\top}M X]\\right|\\geq t\\right)\\leq2\\exp\\left(-c\\operatorname*{min}\\left(\\frac{t^{2}}{\\|M\\|_{F}^{2}},\\frac{t}{\\|M\\|_{o p}}\\right)\\right),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "for some universal constant $c>0$ . ", "page_idx": 14}, {"type": "text", "text": "Finally, we note a folklore simple characterization of total variation distance (see also [AAL23, Theorem 1.8]). ", "page_idx": 14}, {"type": "text", "text": "Lemma A.8. For any $\\mu_{1},\\mu_{2}\\in\\mathbb{R}^{d}$ and positive definite $\\Sigma_{1},\\Sigma_{2}\\in\\mathbb{R}^{d\\times d}$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathrm{d}_{\\mathrm{TV}}(N(\\mu_{1},\\Sigma_{1}),N(\\mu_{2},\\Sigma_{2}))\\leq\\frac{1}{\\sqrt{2}}\\cdot\\operatorname*{max}\\left(\\|\\Sigma_{1}^{-1/2}\\Sigma_{2}\\Sigma_{1}^{-1/2}-I\\|_{F},\\|\\Sigma_{1}^{-1/2}(\\mu_{1}-\\mu_{2})\\|_{2}\\right).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "B Robust Estimation of a Single Gaussian ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we establish what a robust algorithm can do for learning a single high-dimensional Gaussian. We will state the accuracy guarantees in terms of the following definition, which combines Mahalanobis distance, spectral distance, and Frobenius distance. ", "page_idx": 15}, {"type": "text", "text": "Definition B.1. Given mean-covariance pairs $(\\mu,\\Sigma)$ and $(\\hat{\\mu},\\hat{\\Sigma})$ , and given parameters $\\gamma,\\rho,\\tau>0$ , we say that $(\\hat{\\mu},\\hat{\\Sigma})\\approx_{\\gamma,\\rho,\\tau}(\\mu,\\Sigma)$ if ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\Sigma^{-1/2}\\hat{\\Sigma}\\Sigma^{-1/2}-I\\|_{o p}\\leq\\gamma.\\mathrm{~(Spectral~distance)}}\\\\ &{\\|\\Sigma^{-1/2}\\hat{\\Sigma}\\Sigma^{-1/2}-I\\|_{F}\\leq\\rho.\\mathrm{~(Frobenius~distance)}}\\\\ &{\\|\\Sigma^{-1/2}(\\hat{\\mu}-\\mu)\\|_{2}\\leq\\tau.\\mathrm{~(Mahalanobis~distance)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We will also use $\\approx_{\\gamma,\\tau}$ as a shorthand for $\\approx_{\\gamma,\\sqrt{d}\\cdot\\gamma,\\tau}$ , i.e., there is no additional Frobenius norm condition beyond what is already imposed by the operator norm condition. ", "page_idx": 15}, {"type": "text", "text": "Although $\\approx_{\\gamma,\\rho,\\tau}$ is neither symmetric nor transitive, symmetry and transitivity happen in an approximate sense. Namely, the following result holds: we defer the proof to Appendix H. We remark that similar results are known (e.g., [AL22, Lemma 3.2], [AAL23, Lemma 4.1]). ", "page_idx": 15}, {"type": "text", "text": "Proposition B.2 (Approximate Symmetry and Transitivity). Fix any $\\gamma,\\rho,\\tau>0$ such that $\\gamma\\le0.1$ . Then, for any $\\begin{array}{r}{(\\mu_{1},\\Sigma_{1})\\approx_{\\gamma,\\rho,\\tau}\\ (\\mu_{2},\\Sigma_{2}).}\\end{array}$ , we have that $(\\mu_{2},\\Sigma_{2})\\approx_{2\\gamma,2\\rho,2\\tau}$ $\\left(\\mu_{1},\\Sigma_{1}\\right)$ , and for any $(\\mu_{1},\\Sigma_{1})\\approx_{\\gamma,\\rho,\\tau}$ $\\left(\\mu_{2},\\Sigma_{2}\\right)$ and $\\mathrm{(}\\mu_{2},\\Sigma_{2})\\approx_{\\gamma,\\rho,\\tau}\\mathrm{(}\\mu_{3},\\Sigma_{3}\\mathrm{)},$ , we have that $(\\mu_{1},\\Sigma_{1})\\approx_{4\\gamma,4\\rho,4\\tau}(\\mu_{3},\\Sigma_{3})$ . ", "page_idx": 15}, {"type": "text", "text": "We note the following theorem about the accuracy of robustly learning a single Gaussian. While this result will roughly follow from known results and techniques in the literature, we were unable to find a formal statement of the theorem, so we prove it in Appendix $\\mathrm{H}$ . ", "page_idx": 15}, {"type": "text", "text": "Theorem B.3. For some universal constants $c_{0}\\in(0,0.01)$ and $C_{0}>1$ , the following holds. Fix any $\\eta\\le\\gamma\\le c_{0}$ , $\\beta<1$ , and $\\rho$ such that $\\widetilde{\\cal O}(\\eta)\\le\\rho\\le c_{0}\\sqrt{d}$ . Let $\\begin{array}{r}{n\\ge\\widetilde O\\left(\\frac{d+\\log(1/\\beta)}{\\gamma^{2}}+\\frac{(d+\\log(1/\\beta))^{2}}{\\rho^{2}}\\right)}\\end{array}$ . Then, there exists an (inefficient, deterministic) algorithm $\\mathcal{A}_{\\mathrm{0}}$ with the following property. For any $\\mu,\\Sigma,$ , with probability at least $1-\\beta$ over $\\mathbf{X}=\\{X_{1},\\ldots,X_{n}\\}\\sim{\\mathcal{N}}({\\boldsymbol{\\mu}},{\\boldsymbol{\\Sigma}})$ , and for all datasets $\\mathbf{X^{\\prime}}$ with $\\operatorname{d}_{\\mathrm{H}}(\\mathbf{X},\\mathbf{X}^{\\prime})\\leq{\\boldsymbol{\\eta}}\\cdot{\\boldsymbol{n}}$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{A}_{0}(\\mathbf{X}^{\\prime})\\approx_{C_{0}\\gamma,C_{0}\\rho,C_{0}\\gamma}(\\mu,\\Sigma).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We remark that $\\mathcal{A}_{\\mathrm{0}}$ may have knowledge of $\\eta,\\gamma,\\rho,\\beta$ , but does not have knowledge of $\\mu,\\Sigma,$ , or the uncorrupted data $\\mathbf{X}$ . ", "page_idx": 15}, {"type": "text", "text": "B.1 Directional bounds ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we note that, when given i.i.d. samples from a Gaussian, with high probability one cannot choose a reasonably large subset with an extremely different empirical mean or covariance. ", "page_idx": 15}, {"type": "text", "text": "First, we note the following proposition, for which the proof follows by an $\\varepsilon$ -net type argument. ", "page_idx": 15}, {"type": "text", "text": "Proposition B.4. Let $n\\geq n^{\\prime}\\geq20d,$ , and $L\\geq C_{1}n^{2}$ for a sufficiently large constant $C_{1}$ . Suppose that some data points $\\mathbf{X}=\\{X_{1},\\ldots,X_{n}\\}$ are i.i.d. sampled from $\\mathcal{N}(\\pmb{\\theta},I)$ . Then, with probability at least $1-n^{-\\Omega(n^{\\prime})}$ , the following both hold. ", "page_idx": 15}, {"type": "text", "text": "1. For all $X_{i}\\in\\mathbf{X},\\,\\|X_{i}\\|_{2}\\leq L.$ .   \n2. For all subsets $\\mathbf{Y}\\subset\\mathbf{X}$ of size $n^{\\prime}$ , there does not exist any real number r and any unit vector $v$ such that $\\begin{array}{r}{|\\langle X_{i},v\\rangle-r|\\le\\frac{1}{L}}\\end{array}$ for all $X_{i}\\in\\mathbf{Y}$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. If $\\|X_{i}\\|_{2}~\\ge~L$ , then $X^{\\top}M X~\\geq~L^{2}$ for $M$ the $d\\,\\times\\,d$ identity matrix $I$ . However, $\\mathbb{E}[X^{\\top}M X]=d$ . So, by Lemma A.7, the probability of this event is at most $2e^{-c\\cdot\\operatorname*{min}(n^{4}/d,n^{2})}\\leq$ $2e^{-c\\cdot n^{2}}\\leq n^{-\\Omega(n^{\\prime})}$ . ", "page_idx": 15}, {"type": "text", "text": "Next, we bound the probability that the first item holds but the second item doesn\u2019t hold. First, we can create a $\\frac{1}{L^{2}}$ -net of unit vectors $v^{\\prime}$ , of size $(3L^{2})^{d}\\leq L^{3d}$ , and a $\\frac{1}{L}$ -net of real numbers $r^{\\prime}$ from $[-L,L]$ , of size $2L^{2}$ . Now, suppose that the first item holds, but there is some unit vector $v$ and real number $r$ with $\\begin{array}{r}{|\\langle X_{i},v\\rangle-\\dot{r}|\\,\\le\\,\\frac{1}{L}}\\end{array}$ for all $X_{i}\\,\\in\\,{\\bf Y}$ , for some $\\textbf{Y}\\subset\\textbf{X}$ of size $n^{\\prime}$ . In this case, $\\langle X_{i},v\\rangle\\,\\leq\\,\\|X_{i}\\|_{2}\\,\\leq\\,L$ for all $X_{i}\\,\\in\\,\\mathbf{X}$ , so we can assume that $|r|\\,\\le\\,L$ . Thus, if $v^{\\prime}$ is the closest unit vector to $v$ in the net and $r^{\\prime}$ is the closest real number to $r$ in the net, then $\\begin{array}{r}{|\\langle X_{i},v^{\\prime}\\rangle-r^{\\prime}|\\leq|\\langle X_{i},v\\rangle-r|+|\\langle X_{i},v^{\\prime}-v\\rangle|+|r-r^{\\prime}|\\leq\\frac{1}{L}+\\|X_{i}\\|_{2}\\cdot\\frac{1}{L^{2}}+\\frac{1}{L}\\leq\\frac{3}{L}}\\end{array}$ . ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "In other words, if the first event holds and the second does not, there exists $v^{\\prime},r^{\\prime}$ in the net and $\\mathbf{Y}\\subset\\mathbf{X}$ of size $n^{\\prime}$ , such that $\\begin{array}{r}{|\\langle X_{i},v^{\\prime}\\rangle-r^{\\prime}|\\leq\\frac{3}{L}}\\end{array}$ for all $X_{i}\\in\\mathbf{Y}$ . We can now bound this via a union bound. To perform the union bound, we first bound the probability of this event holding for some fixed $\\bar{v^{\\prime}},r^{\\prime},\\mathbf{\\bar{Y}}$ . ", "page_idx": 16}, {"type": "text", "text": "For any fixed $v^{\\prime},r^{\\prime},\\mathbf{Y}$ , note that $\\langle X_{i},v^{\\prime}\\rangle_{X_{i}\\in\\mathbf{Y}}$ is just $n^{\\prime}$ i.i.d. copies of $\\mathcal{N}(0,1)$ . Let\u2019s call these values $z_{1},\\dotsc,z_{n^{\\prime}}$ . If there is some $r^{\\prime}$ such that $\\begin{array}{r}{|z_{i}-r^{\\prime}|\\leq\\frac{3}{L}}\\end{array}$ for all $i$ , then $\\begin{array}{r}{|z_{i}-z_{1}|\\leq\\frac{6}{L}}\\end{array}$ for all $i$ . Since the PDF of a $\\mathcal{N}(0,1)$ is uniformly bounded by at most $\\frac{1}{2}$ , for any fixed $z_{1}$ , the probablity that any other $z_{i}$ is within $\\frac{6}{L}$ of $z_{1}$ is at most $\\frac{6}{L}$ , so the overall probability is at most $(6/L)^{n^{\\prime}-1}$ . ", "page_idx": 16}, {"type": "text", "text": "Now, the union bound is done over $\\textstyle{\\binom{n}{n^{\\prime}}}$ choices of $\\mathbf{Y}$ , at most $L^{3d}$ choices of $v^{\\prime}$ , and at most $2L^{2}$ choices of $\\mathbf{Y}$ . Thus, the overall probability is at most $(6/L)^{n^{\\prime}-1}\\,\\cdot\\,n^{n^{\\prime}}\\,\\cdot\\,L^{3d}\\,\\cdot\\,2L^{2}\\,\\,\\leq$ $(6n)^{n^{\\prime}}/L^{n^{\\prime}-3-3d}\\ \\geq\\ (6n/L^{0.7})^{n^{\\prime}}$ . Thus, as long as $L~\\ge~100n^{2}$ , this is much smaller than n\u2212\u2126(n\u2032). \u53e3 ", "page_idx": 16}, {"type": "text", "text": "By shifting and scaling appropriately, we have the following corollary. ", "page_idx": 16}, {"type": "text", "text": "Corollary B.5. Let $n\\geq n^{\\prime}\\geq20d,$ , and $L\\geq C_{1}n^{2}$ , for $C_{1}$ the same constant as in Proposition B.4. Suppose that some data points $\\mathbf{X}=\\{X_{1},\\ldots,X_{n}\\}$ are drawn from $\\mathcal{N}(\\mu,\\Sigma)$ . Then, with probability at least $1-n^{-\\Omega(n^{\\prime})}$ , the following both hold. ", "page_idx": 16}, {"type": "text", "text": "1. For all $X_{i}\\in{\\bf X},\\|\\Sigma^{-1/2}(X_{i}-\\mu)\\|_{2}\\le L.$   \n2. For all subsets $\\mathbf{Y}\\subset\\mathbf{X}$ of size $n^{\\prime}$ , there does not exist any real number r and any nonzero vector $v$ such that $\\begin{array}{r}{|\\langle X_{i},\\bar{v}\\rangle-r|\\leq\\frac{1}{L}\\|\\Sigma^{1/2}v\\|_{2}}\\end{array}$ for all $X_{i}\\in\\mathbf{Y}$ . ", "page_idx": 16}, {"type": "text", "text": "B.2 Modified robust algorithm ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We can modify the robust algorithm of Theorem B.3 to have the following guarantees. ", "page_idx": 16}, {"type": "text", "text": "Lemma B.6. Let $c_{0},C_{0}$ be as in Theorem B.3, and $C_{1}$ be as in Proposition B.4. Let $L:=C_{1}\\cdot n^{2}$ . Also, fix any $\\eta\\le\\gamma\\le c_{0}$ , any $e^{-d}\\leq\\beta<1$ , and and any $\\rho$ such that $\\widetilde{\\cal O}(\\eta)\\le\\rho\\le c_{0}\\sqrt{d}.$ . Finally, suppose $\\begin{array}{r}{n\\ge m\\ge\\widetilde O\\left(\\frac{d}{\\gamma^{2}}+\\frac{d^{2}}{\\rho^{2}}\\right)}\\end{array}$ . ", "page_idx": 16}, {"type": "text", "text": "Then, there exists an (inefficient, deterministic) algorithm $\\boldsymbol{\\mathcal{A}}$ that, on a dataset $\\mathbf{Y}$ of size $m$ , outputs either some $(\\hat{\\mu},\\hat{\\Sigma})$ or $\\perp$ , with the following properties. ", "page_idx": 16}, {"type": "text", "text": "1. For any datasets ${\\mathbf Y},{\\mathbf Y}^{\\prime}$ with $\\operatorname{d}_{\\mathrm{H}}(\\mathbf{Y},\\mathbf{Y}^{\\prime})\\,\\leq\\,\\eta\\cdot m,$ , if $\\boldsymbol{\\mathcal{A}}(\\mathbf{Y})=(\\hat{\\mu},\\boldsymbol{\\hat{\\Sigma}})\\neq\\perp$ and $\\mathbf{\\mathcal{A}}(\\mathbf{Y^{\\prime}})=$ $(\\hat{\\mu}^{\\prime},\\hat{\\Sigma}^{\\prime})\\neq\\perp$ , then ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(\\hat{\\mu}^{\\prime},\\hat{\\Sigma}^{\\prime})\\approx_{8C_{0}\\gamma,8C_{0}\\rho,8C_{0}\\gamma}(\\hat{\\mu},\\hat{\\Sigma}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "2. For any fixed $\\mu,\\Sigma$ , with probability at least $1\\,-\\,O(\\beta)$ over $Y_{1},\\ldots,Y_{m}\\;\\sim\\;{\\mathcal{N}}(\\mu,{\\Sigma}),$ , ${\\mathcal{A}}(\\mathbf{Y})\\neq\\bot$ and ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{A(\\mathbf{Y})\\approx_{C_{0}\\gamma,C_{0}\\rho,C_{0}\\gamma}(\\mu,\\Sigma).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "3. Fix an integer $k\\,\\geq\\,1$ , and additionally suppose that $m\\:\\geq\\:40d\\cdot k$ . Suppose that $\\mathbf{X}=$ $\\{X_{1},\\ldots,X_{n}\\}$ are drawn i.i.d. from some mixture of $k$ Gaussians $\\left(\\mu_{i},\\Sigma_{i}\\right)$ . Then, with probability at least $1-O(\\beta)$ over $\\mathbf{X}$ , for every ${\\bf Y^{\\prime}}\\subset\\bf X$ of size m and every $\\mathbf{Y}$ with $\\mathrm{d}_{\\mathrm{H}}(\\mathbf{Y},\\mathbf{Y}^{\\prime})\\leq m/2$ , either ${\\mathcal{A}}(\\mathbf{Y})=\\bot$ , or $\\boldsymbol{A}(\\mathbf{Y})=(\\hat{\\mu},\\hat{\\Sigma})$ , where there is some $i\\in[k]$ such that $\\begin{array}{r}{\\frac{1}{9L^{4}}\\cdot\\Sigma_{i}\\preccurlyeq\\hat{\\Sigma}\\preccurlyeq9L^{4}\\cdot\\Sigma_{i}}\\end{array}$ and $\\|\\Sigma_{i}^{-1/2}(\\hat{\\mu}-\\mu_{i})\\|_{2}\\leq10L^{3}$ . ", "page_idx": 16}, {"type": "text", "text": "As in Theorem B.3, $\\boldsymbol{\\mathcal{A}}$ has knowledge of $\\eta,\\gamma,\\rho,\\beta$ , but not $\\mu$ or $\\Sigma$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. The algorithm $\\boldsymbol{\\mathcal{A}}$ works as follows. Start off by computing $\\mathcal{A}_{0}(\\mathbf{Y})\\,=\\,(\\hat{\\mu},\\hat{\\Sigma})$ , where $\\mathcal{A}_{0}$ is the algorithm of Theorem B.3. We can run the algorithm on any arbitrary dataset, even if it didn\u2019t come from Gaussian samples, and we can assume that the algorithm $\\mathcal{A}_{\\mathrm{0}}$ always outputs some mean-covariance pair (for instance, by having it output $(\\mathbf{0},I)$ by default instead of $\\bot$ ). Next, $\\boldsymbol{\\mathcal{A}}$ , on a dataset $\\mathbf{Y}$ , checks if any of the following three conditions hold. ", "page_idx": 17}, {"type": "text", "text": "(b) There exists $Y_{i}\\in\\mathbf{Y}$ such that $\\|\\hat{\\Sigma}^{-1/2}(Y_{i}-\\hat{\\mu})\\|_{2}>3L.$ . ", "page_idx": 17}, {"type": "text", "text": "(c) There exists a subset $\\mathbf{Z}\\subset\\mathbf{Y}$ of size $20d$ , and a unit vector $v$ and real number $r$ , such that for all $Y_{i}\\in\\mathbf{Z}$ , $\\begin{array}{r}{|\\langle Y_{i},v\\rangle-r|<\\frac{1}{2L}\\cdot\\|\\hat{\\Sigma}^{1/2}v\\|_{2}}\\end{array}$ . ", "page_idx": 17}, {"type": "text", "text": "If any of these conditions hold, the output is ${\\mathcal{A}}(\\mathbf{Y})=\\bot$ . Otherwise, the output is $\\mathcal{A}(\\mathbf{Y})=\\mathcal{A}_{0}(\\mathbf{Y})$ . ", "page_idx": 17}, {"type": "text", "text": "We now verify that the algorithm satisfies the required properties. Property 1 clearly holds, because if there exist datasets ${\\mathbf Y},{\\mathbf Y}^{\\prime}$ with $\\operatorname{d}_{\\mathrm{H}}(\\mathbf{Y},\\mathbf{Y}^{\\prime})\\,\\leq\\,\\eta\\cdot m$ , $A(\\mathbf{Y})\\neq\\perp$ and $A(\\mathbf{Y}^{\\prime})\\;\\neq\\perp$ , and $A(\\mathbf{Y})\\approx_{8C_{0}\\gamma,8C_{0}\\rho,8C_{0}\\gamma}A(\\mathbf{Y}^{\\prime})$ , then we would have in fact set ${\\mathcal{A}}(\\mathbf{Y})$ to $\\perp$ . ", "page_idx": 17}, {"type": "text", "text": "For Property 2, note that with probability $1-O(\\beta)$ , ${\\mathcal{A}}_{0}(\\mathbf{Y})$ satisfies the desired property by Theorem B.3, since $\\beta\\geq e^{-d}$ so $d+\\log1/\\beta=O(d)$ . So, we just need to make sure that we don\u2019t set ${\\mathcal{A}}(\\mathbf{Y})$ to be $\\bot$ . Since $Y_{1},\\dots,Y_{m}\\sim{\\mathcal{N}}(\\mu,{\\Sigma})$ , then with probability $1-O(\\beta)$ , both $\\mathbf{Y}$ and any ${\\bf Y^{\\prime}}$ with $\\mathrm{d}_{\\mathrm{H}}(\\mathbf{Y},\\mathbf{Y}^{\\prime})\\leq\\boldsymbol{\\eta}\\cdot\\boldsymbol{n}$ satisfy the conditions of Theorem B.3. In other words, $\\mathcal{A}_{0}(\\mathbf{Y})\\approx_{C_{0}\\gamma,C_{0}\\rho,C_{0}\\gamma}$ and $\\mathcal{A}_{0}(\\mathbf{Y}^{\\prime})\\approx_{C_{0}\\gamma,C_{0}\\rho,C_{0}\\gamma}$ . So, by Proposition B.2, $A_{0}({\\bf Y}^{\\prime})\\approx_{8C_{0}\\gamma,8C_{0}\\rho,8C_{0}\\gamma}\\,\\,\\mathcal{A}_{0}({\\bf Y})$ for any ${\\bf Y^{\\prime}}$ with $\\mathrm{d}_{\\mathrm{H}}(\\mathbf{Y},\\mathbf{Y}^{\\prime})\\leq\\boldsymbol{\\eta}\\cdot\\boldsymbol{n}$ . Thus, condition a) is not met. ", "page_idx": 17}, {"type": "text", "text": "Moreover, by Corollary B.5, with failure probability at most $m^{-\\Omega(20d)}\\leq\\beta,\\|\\Sigma^{-1/2}(Y_{i}-\\mu)\\|_{2}\\leq L$ for all $Y_{i}\\in\\mathbf{Y}$ , and for all $\\mathbf{Z}\\subset\\mathbf{Y}$ of size $20d$ , there does not exist a real $r$ and a nonzero vector $v$ such that $\\begin{array}{r}{|\\langle\\dot{Y_{i}},v\\rangle-r|\\,\\le\\,\\frac{1}{L}\\,\\cdot\\,\\|\\Sigma^{1/2}v\\|_{2}}\\end{array}$ for all $Y_{i}\\,\\in\\,\\mathbf{Y}$ . Now, we know that by Theorem B.3, $(\\hat{\\mu},\\hat{\\Sigma})\\approx_{0.5,0.5}(\\mu,\\Sigma)$ . So, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\hat{\\Sigma}^{-1/2}(Y_{i}-\\hat{\\mu})\\|_{2}\\le2\\|\\Sigma^{-1/2}(Y_{i}-\\hat{\\mu})\\|_{2}\\le2(\\|\\Sigma^{-1/2}(Y_{i}-\\mu)\\|+0.5)=2L+1\\le3L,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and if $\\begin{array}{r}{|\\langle Y_{i},v\\rangle-r|\\,\\le\\,\\frac{1}{2L}\\,\\cdot\\,\\|\\hat{\\Sigma}^{1/2}v\\|_{2}}\\end{array}$ , then $\\begin{array}{r}{|\\langle Y_{i},v\\rangle-r|\\,\\le\\,\\frac{1}{L}\\,\\cdot\\,\\|\\Sigma^{1/2}v\\|_{2}}\\end{array}$ , for all $Y_{i}\\,\\in\\,\\mathbf{Y}$ . Thus, conditions b) and c) are also not met, so Property 2 holds. ", "page_idx": 17}, {"type": "text", "text": "For Property 3, note that if $m\\geq40d\\cdot k$ , then $|\\mathbf{Y}\\cap\\mathbf{X}|\\geq20d\\cdot k,$ , i.e., $\\mathbf{Y}$ contains at least $20d k$ uncorrupted points. So, by the Pigeonhole Principle, for every possible $\\mathbf{Y}$ , there is some index $i\\in[k]$ such that at least $20d$ points in $\\mathbf{Y}\\cap\\mathbf{X}$ come from the $i^{\\mathrm{th}}$ Gaussian component. ", "page_idx": 17}, {"type": "text", "text": "Now, let us condition on the event that Corollary B.5 holds for $\\mathbf{X}$ , where $n^{\\prime}=20d$ , which happens with at least $1-n^{-\\Omega(n^{\\prime})}\\geq1-\\beta$ probability. We claim that for every possible $\\mathbf{Y}$ , and for an $i\\in[k]$ such that $20d$ points $\\mathbf{Z}\\subset\\mathbf{Y}\\cap\\mathbf{X}$ came from the $i^{\\mathrm{th}}$ Gaussian component, then either ${\\mathcal{A}}(\\mathbf{Y})=\\bot$ or if $\\boldsymbol{A}(\\mathbf{Y})=(\\hat{\\mu},\\hat{\\Sigma})$ then $\\begin{array}{r}{\\frac{1}{9L^{4}}\\cdot\\Sigma_{i}\\preccurlyeq\\hat{\\Sigma}\\preccurlyeq9L^{4}\\cdot\\Sigma_{i}}\\end{array}$ and $\\|\\Sigma_{i}^{-1/2}(\\hat{\\mu}-\\mu_{i})\\|_{2}\\leq10L^{3}$ . ", "page_idx": 17}, {"type": "text", "text": "First, we verify that $\\begin{array}{r}{\\frac{1}{9L^{4}}\\cdot\\Sigma_{i}\\preccurlyeq\\hat{\\Sigma}\\preccurlyeq9L^{4}\\cdot\\Sigma_{i}}\\end{array}$ . Otherwise, there exists a unit vector $v$ such that either $\\boldsymbol{v}^{\\top}\\hat{\\Sigma}\\boldsymbol{v}\\geq9L^{4}\\cdot\\boldsymbol{v}^{\\top}\\Sigma_{i}\\boldsymbol{v}$ or $\\begin{array}{r}{v^{\\top}\\hat{\\Sigma}v\\leq\\frac{1}{9L^{4}}\\cdot v^{\\top}\\Sigma_{i}v}\\end{array}$ . In the former case, by Part 1 of Corollary B.5, for all $Y_{i}\\in\\mathbf{Z}$ , $\\|\\Sigma_{i}^{-1/2}(Y_{i}-\\mu_{i})\\|_{2}\\leq L$ , so ", "page_idx": 17}, {"type": "equation", "text": "$$\nY_{i}-\\mu_{i})|=|\\langle\\Sigma_{i}^{1/2}v,\\Sigma_{i}^{-1/2}(Y_{i}-\\mu_{i})\\rangle|\\leq L\\cdot\\|\\Sigma_{i}^{1/2}v\\|_{2}=L\\cdot\\sqrt{v^{\\top}\\Sigma_{i}v}\\leq L\\cdot\\sqrt{\\frac{v^{\\top}\\hat{\\Sigma}v}{9L^{4}}}=\\frac{1}{3L}\\cdot\\|\\hat{\\Sigma}^{1/2}v\\|_{2}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Thus, if we set $r\\,=\\,\\langle v,\\mu_{i}\\rangle$ , then $\\begin{array}{r}{|\\langle Y_{i},v\\rangle-r|<\\frac{1}{2L}\\cdot\\|\\hat{\\Sigma}^{1/2}v\\|_{2}}\\end{array}$ for all $Y_{i}\\,\\in\\,{\\bf Z}$ , so the algorithm $\\boldsymbol{\\mathcal{A}}$ would have output $\\perp$ due to condition c). Alternatively, if there exists a unit vector $v$ such that $\\begin{array}{r}{v^{\\top}\\hat{\\Sigma}v\\leq\\frac{1}{9L^{4}}\\cdot v^{\\top}\\bar{\\Sigma}_{i}v}\\end{array}$ , then by condition b), $\\|\\hat{\\Sigma}^{-1/2}(Y_{i}-\\hat{\\mu})\\|_{2}\\leq3L$ for all $Y_{i}\\in\\mathbf{Z}$ , so ", "page_idx": 17}, {"type": "equation", "text": "$$\n|\\langle v,Y_{i}-\\hat{\\mu}\\rangle|=|\\langle\\hat{\\Sigma}^{1/2}v,\\hat{\\Sigma}^{-1/2}(Y_{i}-\\mu)\\rangle|\\leq3L\\cdot\\|\\hat{\\Sigma}^{1/2}v\\|_{2}\\leq\\frac{1}{L}\\cdot\\|{\\Sigma}_{i}^{1/2}v\\|_{2}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "So, there exists $r=\\langle v,\\hat{\\mu}\\rangle$ such that $\\begin{array}{r}{|\\langle v,Y_{i}\\rangle-r|\\leq\\frac{1}{L}\\cdot\\sqrt{v^{\\top}\\Sigma_{i}v}}\\end{array}$ which is impossible by Part 2 of Corollary B.5 ", "page_idx": 17}, {"type": "text", "text": "Next, we verify that $\\|\\Sigma_{i}^{-1/2}(\\hat{\\mu}\\mathrm{~-~}\\mu_{i})\\|_{2}\\;\\leq\\;10L^{3}$ . By Triangle inequality, for every $Y_{i}~\\in~\\mathbf{Z}$ , $\\|\\Sigma_{i}^{-1/2}(Y_{i}-\\hat{\\mu})\\|_{2}+\\|\\Sigma_{i}^{-1/2}(Y_{i}-\\mu_{i})\\|_{2}\\;\\geq\\;\\|\\Sigma_{i}^{-1/2}(\\mu_{i}-\\hat{\\mu})\\|_{2}\\;$ . But, $\\|\\Sigma_{i}^{-1/2}(Y_{i}\\,-\\,\\mu_{i})\\|_{2}\\,\\leq\\,L$ by Part 1 of Corollary B.5, and $\\begin{array}{r}{\\|\\Sigma_{i}^{-1/2}(Y_{i}-\\hat{\\mu})\\|_{2}\\leq3L^{2}\\cdot\\|\\hat{\\Sigma}^{-1/2}(Y_{i}-\\hat{\\mu})\\|_{2}\\leq3L^{2}\\cdot3L\\leq9L^{3},}\\end{array}$ because we just proved that $9L^{4}\\Sigma_{i}\\succ\\hat{\\Sigma}$ and assuming we do not reject due to condition b). Thus, $\\|\\Sigma_{i}^{-1/2}(\\mu_{i}-\\hat{\\mu})\\|_{2}\\leq9L^{3}+L\\leq10L^{3}$ . \u53e3 ", "page_idx": 18}, {"type": "text", "text": "C Volume and the Robustness-to-Privacy Conversion ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section, we explain the robustness-to-privacy conversion [HKMN23] that we will utilize in proving Theorem 1.4. We will also need some relevant results about computing volume, which will be important in the robustness-to-privacy conversion. ", "page_idx": 18}, {"type": "text", "text": "C.1 Normalized Volume ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Given a pair $(\\mu,\\Sigma)$ , where $\\mu\\in\\mathbb{R}^{d}$ and $\\Sigma\\in\\mathbb{R}^{d\\times d}$ is positive definite, we will define $\\mathrm{Proj}(\\mu,\\Sigma)\\in$ $\\mathbb{R}^{d\\cdot(d+3)/2}$ to represent the coordinates of $\\mu$ along with the upper-diagonal coordinates of $\\Sigma$ . For any set $\\Omega$ of mean-covariance pairs $(\\mu,\\Sigma)$ , define $\\mathrm{Proj}(\\Omega):=\\{\\bar{\\mathrm{Proj}}(\\mu,\\bar{\\Sigma}):(\\mu,\\Sigma)\\in\\Omega\\}$ . Because $\\Sigma$ is symmetric, $\\mathrm{Proj}(\\mu,\\Sigma)$ fully encodes the information about $\\mu$ and $\\Sigma$ . We also define $\\mathrm{vol}(\\Omega)$ to be the Lebesgue measure of $\\mathrm{Proj}(\\Omega)$ , i.e., the $\\textstyle{\\frac{d(d+3)}{2}}$ -dimensional measure of all points $\\operatorname{Proj}(\\mu,\\Sigma)$ for $(\\mu,\\Sigma)\\in{\\bar{\\Omega}}$ . Next, we will define the normalized volume ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathrm{vol}_{\\mathrm{n}}(\\Omega):=\\int_{\\theta\\in\\mathrm{Proj}(\\Omega)}\\frac{1}{(\\operatorname*{det}\\Sigma)^{(d+2)/2}}d\\theta,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\theta=\\operatorname{Proj}(\\mu,\\Sigma)$ , and we take a Lebesgue integral over $\\theta\\in\\mathrm{Proj}(\\Omega)$ . ", "page_idx": 18}, {"type": "text", "text": "To motivate this choice of normalized volume, we will see that the volume is invariant under some basic transformations. ", "page_idx": 18}, {"type": "text", "text": "issy tmhem emtraitcr imx awtriitx $f:\\mathbb{R}^{D}\\to\\mathbb{R}^{D}$ $D\\geq1$ sn cat isoyn $J$ $x$ $\\begin{array}{r}{J_{i j}\\,=\\,\\frac{\\partial f_{i}}{\\partial x_{j}}(x)}\\end{array}$ $\\Sigma$ $A\\Sigma A^{\\top}$ $\\begin{array}{r}{D=\\frac{d(d+1)}{2}}\\end{array}$ $\\Sigma\\mapsto A\\Sigma A^{\\top}$ $f$   \n$\\mathbb{R}^{D}\\to\\mathbb{R}^{D}$ by taking the upper triangular part of both $\\Sigma$ and $A\\Sigma A^{\\top}$ . Note that $f$ is a linear function   \n(thus $f(x)=J\\cdot x$ where $\\bar{\\boldsymbol{J}}^{'}\\in\\mathbb{R}^{D\\times\\breve{D}}$ , and moreover, the following fact is well-known. ", "page_idx": 18}, {"type": "text", "text": "Lemma C.1. [MH08, Theorem 11.1.5] For $J$ as defined above, $\\operatorname*{det}(J)=\\operatorname*{det}(A)^{d+1}$ . ", "page_idx": 18}, {"type": "text", "text": "Now, for any fixed $\\mu~\\in~\\mathbb{R}^{d}$ and positive definite $\\Sigma~\\in~\\mathbb{R}^{d\\times d}$ , consider the transformation $(\\hat{\\mu},\\hat{\\Sigma})\\mapsto(\\Sigma^{1/2}\\hat{\\Sigma}\\Sigma^{1/2},\\Sigma^{1/2}\\hat{\\mu}+\\mu)$ , viewed as a linear map $g$ from $\\operatorname{Proj}(\\hat{\\mu},\\hat{\\Sigma})\\rightarrow\\operatorname{Proj}(\\Sigma^{1/2}\\hat{\\mu}+$ $\\mu,\\Sigma^{1/2}\\hat{\\Sigma}\\Sigma^{1/2})$ . By setting $A:=\\Sigma^{1/2}$ , note that the map $g$ behaves like $f$ on the last $\\textstyle{\\frac{d(d+1)}{2}}$ coordinates, and on the first $d$ coordinates, it is simply an affine map $\\hat{\\mu}\\mapsto\\Sigma^{1/2}\\hat{\\mu}+\\mu$ . Therefore, the overall linear map $g$ has determinant $\\operatorname*{det}({\\Sigma}^{1/2})\\cdot\\operatorname*{det}({\\Sigma}^{1/2})^{d+1}=(\\operatorname*{det}{\\Sigma})^{(d+2)/2}$ . ", "page_idx": 18}, {"type": "text", "text": "From this, we can infer the following. ", "page_idx": 18}, {"type": "text", "text": "Lemma C.2. Fix any $\\mu\\:\\in\\:\\mathbb{R}^{d}$ and positive definite $\\Sigma~\\in~\\mathbb{R}^{d\\times d}$ . Let h be the map $(\\hat{\\mu},\\hat{\\Sigma})\\;\\mapsto\\;$ $(\\Sigma^{1/2}\\hat{\\mu}+\\mu,\\Sigma^{1/2}\\hat{\\Sigma}\\Sigma^{1/2})$ . Then, for any set $S$ of mean-covariance pairs, the normalized volume of $S$ equals the normalized volume of $h(S)$ . ", "page_idx": 18}, {"type": "text", "text": "Proof. Let $g$ be the corresponding map from $\\mathrm{Proj}(\\hat{\\mu},\\hat{\\Sigma})$ to $\\operatorname{Proj}(\\Sigma^{1/2}\\hat{\\mu}+\\mu,\\Sigma^{1/2}\\hat{\\Sigma}\\Sigma^{1/2})$ . Using a simple integration by substitution, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{vol}_{\\mathrm{n}}(h(S))=\\int_{\\hat{\\theta}=\\mathrm{Proj}(\\hat{\\mu},\\hat{\\Sigma})\\in\\mathrm{Proj}(h(S))}\\frac{1}{(\\operatorname*{det}\\hat{\\Sigma})^{(d+2)/2}}d\\hat{\\theta}}\\\\ {=\\int_{\\hat{\\theta}\\in g(\\mathrm{Proj}(S))}\\frac{1}{(\\operatorname*{det}\\hat{\\Sigma})^{(d+2)/2}}d\\hat{\\theta}}\\\\ {=\\int_{\\hat{\\theta}=\\mathrm{Proj}(\\hat{\\mu},\\hat{\\Sigma})\\in\\mathrm{Proj}(S)}\\frac{1}{(\\operatorname*{det}(\\Sigma^{1/2}\\hat{\\Sigma}\\Sigma^{1/2}))^{(d+2)/2}}\\cdot(\\operatorname*{det}g)d\\hat{\\theta}}\\\\ {=\\int_{\\hat{\\theta}\\in\\mathrm{Proj}(S)}\\frac{1}{(\\operatorname*{det}\\hat{\\Sigma})^{(d+2)/2}}d\\hat{\\theta}=\\mathrm{vol}_{\\mathrm{n}}(S).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "C.2 Robustness to Privacy Conversion ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We note the following restatement of (the inefficient, approx-DP version of) the main robustness-toprivacy conversion of [HKMN23]. ", "page_idx": 19}, {"type": "text", "text": "In the following theorem, we will think of the parameter space as lying in $\\mathbb{R}^{D}$ , with some normalized volume $\\begin{array}{r}{\\operatorname{vol}_{\\mathrm{n}}(\\bar{\\Omega})=\\int_{\\theta\\in\\Omega}p(\\theta)d\\theta}\\end{array}$ , where $p\\geq0$ is some nonnegative Lebesgue-measurable function and the integration is Lebesgue integration. In our application, we will think of $\\theta=\\operatorname{Proj}(\\mu,\\Sigma)$ , and $p(\\theta)=(\\operatorname*{det}\\Sigma)^{-(d+2)/2}$ , to match with (1). ", "page_idx": 19}, {"type": "text", "text": "Theorem C.3. Restatement of [HKMN23, Lemma $4.2J$ Let $0\\,<\\,\\eta\\,<\\,0.1$ and $10\\eta\\leq\\eta^{*}\\leq1$ be fixed parameters. Also, fix privacy parameters $\\varepsilon_{0},\\delta_{0}<1$ and confidence parameter $\\beta_{0}<1$ . Let ${\\cal S}={\\cal S}(\\theta,{\\bf X})\\in\\mathbb{R}_{\\ge0}$ be a score function that takes as input a dataset $\\mathbf{X}=\\left\\{X_{1},\\ldots,X_{n}\\right\\}$ and $a$ parameter $\\theta\\in\\Theta\\subset\\mathbb{R}^{D}$ . For any dataset $\\mathbf{X}$ and any $0\\leq\\eta^{\\prime}\\leq1,$ , let $V_{\\eta^{\\prime}}(\\mathbf{X})$ be the $D$ -dimensional normalized volume of points $\\theta\\in\\Theta$ with score at most $\\eta^{\\prime}\\!\\cdot\\!n_{\\!;}$ , i.e., $V_{\\eta^{\\prime}}(\\mathbf{X})=\\operatorname{vol}_{\\mathbf{n}}(\\{\\theta\\in\\Theta:S(\\theta,\\mathbf{X})\\leq$ $\\eta^{\\prime}\\cdot n\\}$ ). ", "page_idx": 19}, {"type": "text", "text": "Suppose the following properties hold: ", "page_idx": 19}, {"type": "text", "text": "\u2022 (Bounded Sensitivity) For any two adjacent datasets $\\mathbf{X},\\mathbf{X}^{\\prime}$ and any $\\theta\\,\\in\\,\\Theta$ , $\\mathinner{|{S(\\theta,\\mathbf{X})}\\,\\mathinner{-}\\,\\rightleftharpoons}$ $S(\\theta,\\mathbf{X}^{\\prime})|\\leq1$ .   \n\u2022 (Volume) For some universal constant $C$ , and for any $\\mathbf{X}$ of size n such that there exists $\\theta$   \nwith S(\u03b8, X) \u22640.7\u03b7\u2217n, n \u2265C \u00b7 log(V\u03b7\u2217(X)/V0.8\u03b7\u00b7\u2217\u2217(X))+log(1/\u03b40). ", "page_idx": 19}, {"type": "text", "text": "Then, there exists an $(\\varepsilon_{0},\\delta_{0})$ -DP algorithm $\\mathcal{M}$ , that takes as input $\\mathbf{X}$ and outputs either some $\\theta\\in\\Theta$ or $\\perp$ , such that for any dataset $\\mathbf{X}$ , $i f n\\geq C\\cdot\\operatorname*{max}_{\\eta^{\\prime}:\\eta\\leq\\eta^{\\prime}\\leq\\eta^{*}}\\frac{\\log(V_{\\eta^{\\prime}}(\\mathbf{X})/V_{\\eta}(\\mathbf{X}))+\\log(1/(\\beta_{0}\\cdot\\eta))}{\\varepsilon_{0}\\cdot\\eta^{\\prime}}$ log(V\u03b7\u2032(X)/V\u03b7(X))+log(1/(\u03b20\u00b7\u03b7)), then M(X) outputs some $\\theta\\in\\Theta$ of score at most 2\u03b7n with probability $1-\\beta_{0}$ . ", "page_idx": 19}, {"type": "text", "text": "We remark that the algorithm $\\mathcal{M}$ is allowed prior knowledge of $n,D,\\eta,\\eta^{*},\\varepsilon_{0},\\delta_{0},\\beta_{0},$ , as well as the domain $\\Theta$ , function $p(\\theta)$ that dictates the normalized volume, and score function $\\boldsymbol{S}$ . ", "page_idx": 19}, {"type": "text", "text": "We remark that the original result in [HKMN23] assumes the volumes are unnormalized, but the result immediately generalizes to normalized volumes. To see why, consider a modified domain $\\Theta^{\\prime}\\in\\mathbb{R}^{D+1}$ , where $\\theta^{\\prime}\\,=\\,(\\theta,z)\\,\\in\\,\\Theta^{\\prime}$ if and only if $\\theta\\in\\Theta$ and $0\\ \\stackrel{.}{\\leq}\\ z\\leq p(\\theta)$ . Also, consider a modified score function $S^{\\prime}$ acting on $\\Theta^{\\prime}$ , where $S^{\\prime}((\\theta,z),\\mathbf{X}):=S(\\theta,\\mathbf{X})$ for for any $(\\theta,z)\\in\\Theta^{\\prime}$ . ", "page_idx": 19}, {"type": "text", "text": "Then, note that the unnormalized volume of $\\Theta^{\\prime}$ , by Fubini\u2019s theorem, is precisely $\\begin{array}{r}{\\int\\!\\!\\int_{(\\theta,z)\\in\\Theta^{\\prime}}1d z d\\theta=}\\end{array}$ $\\textstyle\\int_{\\theta\\in\\Theta_{\\cdot}}p(\\theta)d\\theta$ , which is precisely the normalized volume of corresponding to the new $\\Theta^{\\prime}$ precisely match the normalized volumes corresponding to the old $\\Theta$ . A similar calculation will give us that the unnormalized volume of points $(\\theta,z)$ in $\\Theta^{\\prime}$ with $S^{\\prime}((\\theta,z),\\mathbf{X})\\leq t$ equals the normalized volume of points $\\theta\\in\\Theta$ with $S(\\theta,\\mathbf{X})\\leq t$ , for any $t$ . ", "page_idx": 19}, {"type": "text", "text": "Thus, to privately find a parameter $\\theta\\in\\Theta$ of low score with respect to $\\boldsymbol{S}$ (assuming the normalized volume constraints hold), can create $\\Theta^{\\prime}$ and $S^{\\prime}$ , and apply the unnormalized version of Theorem C.3 to obtain some $(\\theta,z)$ . Also, if $S^{\\prime}((\\theta,z),\\mathbf{X})\\,\\leq\\,2\\eta n$ , then $S(\\theta)\\,=\\,S^{\\prime}((\\theta,z),\\mathbf{X})\\,\\leq\\,2\\eta n$ , and if outputting $(\\theta,z)$ is $(\\varepsilon,\\delta)$ -DP, then so is simply outputting $\\theta$ . ", "page_idx": 19}, {"type": "text", "text": "C.3 Computing Normalized Volume Ratios ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Lemma C.4. Let $M\\in\\mathbb{R}^{d\\times d}$ be symmetric with $\\|M\\|_{o p}\\,\\le\\,0.1$ , and let $1\\leq\\nu\\leq2$ be a scaling parameter. Then, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\nu^{-d}\\leq{\\frac{\\operatorname*{det}(I+\\nu M)}{\\operatorname*{det}(I+M)}}\\leq\\nu^{d}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. If the eigenvalues of $M$ are $\\lambda_{1},...\\,.\\,,\\lambda_{d}\\in[-0.1,0.1]$ , then ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{\\operatorname*{det}(I+\\nu M)}{\\operatorname*{det}(I+M)}=\\prod_{i=1}^{d}\\frac{1+\\nu\\cdot\\lambda_{i}}{1+\\lambda_{i}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Note that for $x\\in[-0.1,0.1]$ and $\\begin{array}{r}{\\nu\\geq1,\\,\\frac{1+\\nu\\cdot x}{1+x}}\\end{array}$ is an increasing function. Therefore, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\prod_{i=1}^{d}\\frac{1+\\nu\\cdot\\lambda_{i}}{1+\\lambda_{i}}\\leq\\prod_{i=1}^{d}\\frac{1+0.1\\nu}{1+0.1}\\leq\\prod_{i=1}^{d}\\nu=\\nu^{d}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\prod_{i=1}^{d}\\frac{1+\\nu\\cdot\\lambda_{i}}{1+\\lambda_{i}}\\geq\\prod_{i=1}^{d}\\frac{1-0.1\\nu}{1-0.1}\\geq\\prod_{i=1}^{d}\\frac{1}{\\nu}=\\nu^{-d},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where we used the fact that $1\\leq\\nu\\leq2$ . ", "page_idx": 20}, {"type": "text", "text": "We note an important lemmas about the normalized volumes of certain sets. ", "page_idx": 20}, {"type": "text", "text": "Lemma C.5. Fix some $\\gamma,\\tau\\leq0.1$ and some scaling parameters $1\\le\\nu_{1}\\le2$ and $1\\leq\\nu_{2}$ . Let $\\mathcal{R}_{1}$ be the set of $(\\mu,\\Sigma)\\approx_{\\gamma,\\tau}(\\pmb{\\theta},I)$ and $\\mathcal{R}_{2}$ be the set of $(\\mu,\\Sigma)\\approx_{\\nu_{1}\\cdot\\gamma,\\nu_{2}\\cdot\\tau}\\,(\\pmb{\\theta},I)$ . Then, the ratio of normalized volumes ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{vol_{n}}(\\mathcal{R}_{2})}{\\mathrm{vol_{n}}(\\mathcal{R}_{1})}\\leq\\nu_{1}^{2d^{2}}\\cdot\\nu_{2}^{d}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. By definition of normalized volume, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname{vol}_{\\mathrm{n}}(\\mathcal{R}_{2})=\\iint_{\\|M\\|_{o p}\\leq\\nu_{1}\\cdot\\gamma,\\|\\mu\\|_{2}\\leq\\nu_{2}\\cdot\\tau}\\frac{1}{\\operatorname*{det}(I+M)^{(d+2)/2}}d\\mu d M,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where we are slightly abusing notation because we are truly integrating along the upper-triangular part of $M$ . (Overall, the integral i s d(d2+3)-dimensional.) We have a similar expression for voln(R1). Let us consider the map sending $(\\mu,I+M)\\mapsto(\\nu_{2}\\cdot\\mu,I+\\nu_{1}\\cdot M)$ . This is a linear map, and for $\\|\\mu\\|_{2}\\leq\\gamma$ and symmetric $\\|M\\|_{o p}\\,\\le\\,\\tau$ , this is a bijective map from $\\mathcal{R}_{1}$ to $\\mathcal{R}_{2}$ . Therefore, by an integration by substitution, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathrm{vol}_{\\mathrm{n}}(\\mathcal{R}_{2})=\\iint_{\\|M\\|_{o p}\\leq\\gamma,|\\mu||_{2}\\leq\\tau}\\frac{1}{\\operatorname*{det}(I+\\nu_{1}\\cdot M)^{(d+2)/2}}\\cdot\\nu_{1}^{d(d+1)/2}\\nu_{2}^{d}d\\mu d M}}\\\\ &{\\leq\\nu_{1}^{d(d+1)/2}\\nu_{2}^{d}\\cdot(\\nu^{-d})^{-(d+2)/2}\\cdot\\iint_{\\|M\\|_{o p}\\leq\\gamma,|\\mu||_{2}\\leq\\tau}\\frac{1}{\\operatorname*{det}(I+M)^{(d+2)/2}}d\\mu d M}\\\\ &{=\\nu_{1}^{2d^{2}}\\nu_{2}^{d}\\cdot\\mathrm{vol}_{\\mathrm{n}}(\\mathcal{R}_{1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Above, the first line is integration by substitution, and the second line uses Lemma C.4. ", "page_idx": 20}, {"type": "text", "text": "Next, we note the following bound on the size of a net of matrices with small Frobenius norm. ", "page_idx": 20}, {"type": "text", "text": "Lemma C.6. Fix some $\\gamma\\le0.1$ and $\\textstyle{\\frac{1}{d}}\\leq\\rho\\leq0.1{\\sqrt{d}}$ . Let ${\\mathcal{R}}_{3}$ be the set of symmetric matrices $M\\in\\mathbb{R}^{d\\times d}$ with $\\|M\\|_{o p}\\leq\\gamma$ and $\\|M\\|_{F}\\leq\\rho.$ . Then, for any $1\\leq\\ell\\leq d,$ , there exists a net $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ of size at most $e^{O(\\ell\\cdot d\\log d)}$ such that for any $M\\in\\mathcal{R}_{3}$ , there exists $B\\in B$ such that $\\begin{array}{r}{\\|\\boldsymbol{M}-\\boldsymbol{B}\\|_{o p}\\leq\\frac{2\\rho}{\\sqrt{\\ell}}}\\end{array}$ . ", "page_idx": 20}, {"type": "text", "text": "Proof. First, note that the unit sphere in $\\mathbb{R}^{d}$ has a $\\frac{1}{d^{10}}$ -net (in Euclidean distance) of size $e^{O(d\\log d)}$ ", "page_idx": 21}, {"type": "text", "text": "Let $B_{0}$ be this net. The net $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ will be the set of matrices $\\textstyle\\sum_{i=1}^{\\ell}\\kappa_{i}w_{i}w_{i}^{\\top}$ , where every $w_{i}\\in B_{0}$ , every $\\kappa_{i}$ is an integral multiple of $\\frac{1}{d^{10}}$ , and every $|\\kappa_{i}|\\leq\\gamma$ . The cardinality of $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ is at most $|B_{0}|^{\\ell}\\cdot(d^{10})^{\\ell}\\leq$ $e^{O(\\ell\\cdot d\\log d)}$ . ", "page_idx": 21}, {"type": "text", "text": "Now, we show that $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ is actually a net. For any matrix $M\\in\\mathcal{R}_{3}$ , we can write $\\begin{array}{r}{M=\\sum_{i=1}^{d}\\lambda_{i}v_{i}v_{i}^{\\top}}\\end{array}$ , where $\\lambda_{i},v_{i}$ are the eigenvalues and eigenvectors, respectively, of $M$ . Assume the eigenvalues are sorted so that $\\left|\\lambda_{i}\\right|$ are in decreasing order. Now, since $\\sum_{i=1}^{d}\\lambda_{i}^{2}\\le\\rho^{2}$ , which means that $\\begin{array}{r}{|\\lambda_{i}|\\leq\\frac{\\rho}{\\sqrt{\\ell}}}\\end{array}$ for all $i>\\ell$ . ", "page_idx": 21}, {"type": "text", "text": "Now, let $\\begin{array}{r}{B_{1}=\\sum_{i>\\ell}\\lambda_{i}v_{i}v_{i}^{\\top};}\\end{array}$ : since the $v_{i}$ \u2019s are orthogonal and $\\begin{array}{r}{|\\lambda_{i}|\\leq\\frac{\\rho}{\\sqrt{\\ell}}}\\end{array}$ , this means that $\\|B_{1}\\|_{o p}\\leq$ $\\frac{\\alpha}{\\sqrt{\\ell}}$ . Also, note that $\\begin{array}{r}{M=B_{1}+\\sum_{i=1}^{\\ell}\\lambda_{i}v_{i}v_{i}^{\\top}}\\end{array}$ . Suppose we replace each $\\lambda_{i}$ with $\\kappa_{i}$ by rounding to the nearest multiple of $1/d^{10}$ , and replace each $v_{i}$ with $w_{i}\\in\\mathfrak{B}_{0}$ such that $\\|v_{i}-w_{i}\\|_{2}\\leq1/d^{10}$ . Then, $\\scriptstyle\\sum_{i=1}^{\\ell}\\kappa_{i}w_{i}w_{i}^{\\top}$ is in $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ , and by Triangle inequality, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left\\|\\sum_{i=1}^{\\ell}\\lambda_{i}v_{i}v_{i}^{\\top}-\\sum_{i=1}^{\\ell}\\kappa_{i}w_{i}w_{i}^{\\top}\\right\\|_{o p}\\leq\\ell\\cdot\\left(|\\lambda_{i}-\\kappa_{i}|+2\\cdot\\lambda_{i}\\cdot\\|v_{i}-w_{i}\\|_{2}\\right)\\leq O\\left(\\frac{1}{d^{9}}\\right)\\leq\\frac{\\rho}{\\sqrt{\\ell}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Thus, $\\begin{array}{r}{\\|M-\\sum_{i=1}^{\\ell}\\kappa_{i}w_{i}w_{i}^{\\top}\\|_{o p}\\leq2\\cdot\\frac{\\rho}{\\sqrt{\\ell}}}\\end{array}$ . ", "page_idx": 21}, {"type": "text", "text": "We also note the following lemma. We defer the proof to Appendix $_\\mathrm{H}$ . ", "page_idx": 21}, {"type": "text", "text": "Lemma C.7. Fix any $\\mu$ and positive definite $\\Sigma_{}^{},$ . Then, for any $\\mu_{1},\\mu_{2},\\Sigma_{1},\\Sigma_{2}$ , we have that $(\\mu_{1},\\Sigma_{1})~\\approx_{\\gamma,\\rho,\\tau}~\\stackrel{\\cdot}{(\\mu_{2},\\Sigma_{2})}$ if and only if $(\\Sigma^{1/2}\\mu_{1}\\,+\\,\\mu,\\dot{\\Sigma}^{1/2}\\dot{\\Sigma_{1}}\\dot{\\Sigma}^{1/2}\\dot{)}\\;\\;\\approx_{\\gamma,\\rho,\\tau}$ $(\\Sigma^{1/2}\\mu_{2}\\mathrm{~+~}$ $\\mu,\\Sigma^{1/2}\\Sigma_{2}\\Sigma^{1/2}\\rangle$ . ", "page_idx": 21}, {"type": "text", "text": "Our main lemma in this subsection is the following, which roughly bounds the normalized volume of a Frobenius norm ball \u201cfattened\u201d by an operator norm ball. ", "page_idx": 21}, {"type": "text", "text": "definite Lemma C.8. Let $\\Sigma$ $c_{1}\\in(0,0.01)$ parameters \u03b3 be a sufficiently small universal constant. Fix any $\\textstyle\\langle\\b{1},\\gamma_{2}\\in\\left({\\frac{1}{d}},c_{1}\\right)$ and $\\begin{array}{r}{\\rho_{2}\\in\\left(\\frac{1}{d},\\frac{\\gamma_{1}^{3}}{1000}\\cdot\\sqrt{d}\\right)}\\end{array}$ . Let $\\mathcal{T}_{1}(\\mu,\\Sigma)$ $\\mu$ and positive the set of $\\{(\\mu_{1},\\Sigma_{1})\\}$ such that $(\\mu_{1},\\Sigma_{1})\\approx_{\\gamma_{1},\\gamma_{1}}(\\mu,\\Sigma)$ . Let $\\ensuremath{\\mathcal{T}}_{2}\\ensuremath{\\vec{(}}\\mu,\\ensuremath{\\Sigma})$ represent the set of $\\{(\\mu_{2},\\Sigma_{2})\\}$ such that $\\begin{array}{r}{(\\mu_{2},\\Sigma_{2})\\approx_{\\gamma_{1},\\gamma_{1}}(\\mu_{1},\\Sigma_{1}).}\\end{array}$ for some $(\\mu_{1},\\Sigma_{1})\\approx_{\\gamma_{2},\\rho_{2},\\gamma_{2}}(\\mu,\\Sigma)$ . Then, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{vol}_{\\mathrm{n}}(\\mathcal T_{2}(\\mu,\\Sigma))}{\\mathrm{vol}_{\\mathrm{n}}(\\mathcal T_{1}(\\mu,\\Sigma))}\\leq e^{O(d^{5/3}\\log d\\cdot\\rho_{2}^{2/3}/\\gamma_{1}^{2})}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Furthermore, both $\\operatorname{vol}_{\\mathrm{n}}(\\mathcal{T}_{1}(\\mu,\\Sigma))$ and $\\operatorname{vol}_{\\mathrm{n}}(\\mathcal{T}_{2}(\\mu,\\Sigma))$ do not change even if we change $\\mu,\\Sigma$ . ", "page_idx": 21}, {"type": "text", "text": "Proof. For now, we also assume that $\\boldsymbol{\\mu}=\\mathbf{0}$ and $\\Sigma=I$ . ", "page_idx": 21}, {"type": "text", "text": "Let $1\\,\\leq\\,\\ell\\,\\leq\\,d$ be decided later, and let $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ be the net from Lemma C.6, where we set $\\gamma\\,=\\,\\gamma_{2}$ and $\\rho\\,=\\,\\rho_{2}$ . Let $\\boldsymbol{{\\beta}}_{1}$ be a $\\frac{1}{d^{10}}$ -net (in Euclidean distance) over the $d$ -dimensional unit ball, i.e., over $\\mu$ with $\\|\\mu\\|_{2}\\leq1$ . Note that $|B_{1}|\\;\\leq\\;e^{O(d\\log d)}$ , and recall that $|B|\\;\\leq\\;e^{O(\\ell\\cdot d\\log d)}$ . Now, for any $(\\mu_{2},\\Sigma_{2})\\,\\in\\,T_{2}(\\mathbf{0},I)$ , we can write $(\\mu_{2},\\Sigma_{2})\\,\\approx_{\\gamma_{1},\\gamma_{1}}\\,\\,(\\mu_{1},\\Sigma_{1})$ , where $\\|\\Sigma_{1}\\,-\\,I\\|_{o p}\\,\\leq\\,\\gamma_{2},$ , $\\|\\Sigma_{1}-I\\|_{F}\\le\\rho_{2}$ , and $\\|\\mu_{1}\\|_{2}\\leq\\gamma_{2}$ . Now, we can choose some $\\mu^{\\prime}\\in\\mathfrak{B}_{1}$ with $\\|\\mu_{1}-\\mu^{\\prime}\\|_{2}\\leq d^{-10}$ and $B\\in B$ such that $\\begin{array}{r}{\\|(\\Sigma_{1}-I)-B\\|_{o p}\\leq\\frac{2\\rho_{2}}{\\sqrt{\\ell}}}\\end{array}$ . ", "page_idx": 21}, {"type": "text", "text": "What can we say about the relationship between $\\left(\\mu_{2},\\Sigma_{2}\\right)$ and $(\\mu^{\\prime},I+B)?$ First, note that because $(\\mu_{2},\\Sigma_{2})\\approx_{\\gamma_{1},\\gamma_{1}}(\\mu_{1},\\Sigma_{1})$ , we have $(1\\!-\\!\\gamma_{1})\\!\\cdot\\!\\Sigma_{1}\\preccurlyeq\\Sigma_{2}\\preccurlyeq(1\\!+\\!\\gamma_{1})\\!\\cdot\\!\\Sigma_{1}$ . Next, since $\\|\\Sigma_{1}\\!-\\!(I\\!+\\!B)\\|_{o p}=$ $\\begin{array}{r}{\\|(\\Sigma_{1}-I)-B\\|_{o p}\\leq\\frac{2\\rho_{2}}{\\sqrt{\\ell}}}\\end{array}$ , and since $\\Sigma_{1}$ has all eigenvalues between $1-\\gamma_{2}\\geq\\frac{1}{2}$ and $1+\\gamma_{2}\\leq2$ this means $\\begin{array}{r}{\\left(1-\\frac{4\\rho_{2}}{\\sqrt{\\ell}}\\right)\\cdot\\dot{\\Sigma}_{1}\\preccurlyeq\\left(I+B\\right)\\preccurlyeq\\left(1+\\frac{4\\rho_{2}}{\\sqrt{\\ell}}\\right)\\cdot\\Sigma_{1}}\\end{array}$ . Thus, if $4\\rho<\\sqrt{\\ell}$ , we have that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{1-\\gamma_{1}}{1+(4\\rho_{2}/\\sqrt{\\ell})}\\cdot(I+B)\\prec\\Sigma_{1}\\preccurlyeq\\frac{1+\\gamma_{1}}{1-(4\\rho_{2}/\\sqrt{\\ell})}\\cdot(I+B).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Next, because $(\\mu_{2},\\Sigma_{2})\\approx_{\\gamma_{1},\\gamma_{1}}(\\mu_{1},\\Sigma_{1})$ , we have $\\|\\Sigma_{1}^{-1/2}(\\mu_{2}-\\mu_{1})\\|_{2}\\leq\\gamma_{1}$ , which means $\\lVert\\mu_{2}-$ $\\mu_{1}\\|_{2}\\leq2\\gamma_{1}$ . Thus, because $\\|\\mu_{1}-\\mu^{\\prime}\\|_{2}\\leq d^{-10}\\leq\\gamma_{1}$ , this means that $\\|\\mu_{2}-\\mu^{\\prime}\\|_{2}\\leq3\\gamma_{1}$ . Moreover, ", "page_idx": 21}, {"type": "text", "text": "assuming $10\\rho\\leq\\sqrt{\\ell}$ , the eigenvalues of $I+B$ are at least $\\begin{array}{r}{\\left(1-\\gamma_{1}\\right)\\cdot\\left(1-\\frac{4\\rho_{2}}{\\sqrt{\\ell}}\\right)\\geq\\frac{1}{4}}\\end{array}$ , which means that $\\|(I+B)^{-1/2}(\\mu_{2}-\\mu^{\\prime})\\|_{2}\\le6\\gamma_{1}.$ . ", "page_idx": 22}, {"type": "text", "text": "In summary, if $10\\rho\\leq\\sqrt{\\ell}$ , then ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{1-\\gamma_{1}}{1+(4\\rho_{2}/\\sqrt{\\ell})}\\cdot(I+B)\\prec\\Sigma_{2}\\prec\\frac{1+\\gamma_{1}}{1-(4\\rho_{2}/\\sqrt{\\ell})}\\cdot(I+B),\\quad\\|(I+B)^{-1/2}(\\mu_{2}-\\mu^{\\prime})\\|_{2}\\le6\\gamma_{1}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Note that if $\\gamma_{1}\\leq0.1$ and $10\\rho\\leq\\sqrt{\\ell}$ , then by a simple calculation, $\\begin{array}{r}{\\frac{1+\\gamma_{1}}{1-(4\\rho_{2}/\\sqrt{\\ell})}\\,\\leq\\,1+\\gamma_{1}+\\frac{10\\rho_{2}}{\\sqrt{\\ell}}}\\end{array}$ 1 $\\begin{array}{r}{\\frac{1-\\gamma_{1}}{1+(4\\rho_{2}/\\sqrt{\\ell})}\\geq1-\\gamma_{1}-\\frac{4\\rho_{2}}{\\sqrt{\\ell}}}\\end{array}$ . This implies that $(\\mu_{2},\\Sigma_{2})\\approx_{\\gamma_{1}+10\\rho_{2}/\\sqrt{\\ell},6\\gamma_{1}}(\\mu^{\\prime},I+B)$ , for some $\\mu^{\\prime}\\in\\mathcal{B}_{1}$ and $B\\in\\mathcal{B}$ . Note that this holds for any $(\\mu_{2},\\Sigma_{2})\\in T_{2}(\\mathbf{0},I)$ . ", "page_idx": 22}, {"type": "text", "text": "Therefore, recalling that $|B_{1}|\\,\\leq\\,e^{O(d\\log d)}$ , and $|B|\\,\\leq\\,e^{O(\\ell\\cdot d\\log d)}$ we can cover ${\\mathcal{T}}_{2}(\\mathbf{0},I)$ with at most $e^{O(\\ell\\cdot d\\log d)}$ regions, each of which is the set of $(\\mu_{2},\\Sigma_{2})\\,\\approx_{\\gamma_{1}+10\\rho_{2}/\\sqrt{\\ell},6\\gamma_{1}}\\ (\\mu^{\\prime},\\Sigma^{\\prime})$ . Now, by Lemma C.7, $(\\mu_{2},\\Sigma_{2})~\\approx_{\\gamma_{1}+10\\rho_{2}/\\sqrt{\\ell},6\\gamma_{1}}~~(\\mu^{\\prime},\\Sigma^{\\prime})$ if and only if $\\mu_{2}~=~\\Sigma^{\\prime1/2}\\mu_{0}\\,+\\,\\mu^{\\prime}$ and $\\Sigma_{2}\\,=\\,{\\Sigma^{\\prime}}^{1/2}\\Sigma_{0}\\Sigma^{\\prime}{}^{1/2}$ , where $(\\mu_{0},\\Sigma_{0})\\,\\approx_{\\gamma_{1}+10\\rho_{2}/\\sqrt{\\ell},6\\gamma_{1}}\\,\\,(\\mathbf{0},I).$ . By Lemma C.2, this means that the volume of $\\{(\\mu_{2},\\Sigma_{2})\\,:\\,(\\mu_{2},\\Sigma_{2})\\,\\approx_{\\gamma_{1}+10\\rho_{2}/\\sqrt{\\ell},6\\gamma_{1}}\\,\\,(\\mu^{\\prime},\\Sigma^{\\prime})\\}$ equals the volume of $(\\mu_{0},\\Sigma_{0})\\approx_{\\gamma_{1}+10\\rho_{2}/\\sqrt{\\ell},6\\gamma_{1}}(\\mathbf{0},I)\\}$ . ", "page_idx": 22}, {"type": "text", "text": "Thus, if $10\\rho\\leq\\sqrt{\\ell}$ , then ${\\mathcal{T}}_{2}(\\mathbf{0},I)$ can be covered by $e^{O(\\ell\\cdot d\\log d)}$ regions, each of which has the same normalized volume as the volume of $\\{(\\mu_{0},\\Sigma_{0}):(\\dot{\\mu}_{0},\\Sigma_{0})\\approx_{\\gamma_{1}+10\\dot{\\rho}_{2}/\\sqrt{\\ell},6\\gamma_{1}}(\\mathbf{0},I)\\}$ . Moreover, if we further have 10\u221a\u03c1\u21132 \u2264\u03b31, then by Lemma C.8, this is at most 1 + $\\begin{array}{r}{\\left(1+\\frac{10\\rho_{2}}{\\sqrt{\\ell}\\cdot\\gamma_{1}}\\right)^{2d^{2}}\\cdot6^{d}\\cdot\\mathrm{vol}_{\\mathrm{n}}(\\mathcal{T}_{1}(\\mathbf{0},I)).}\\end{array}$ . We will set \u2113 = 100(d\u03c122)2/3. Since $\\rho_{2}~\\geq~{\\frac{1}{d}}$ , $\\ell~\\geq~100$ as long as $\\gamma_{1}~\\leq~c_{1}~\\leq~1$ . Also, $\\ell\\,\\leq$ $\\begin{array}{r}{100\\cdot\\left(\\frac{\\gamma_{1}^{3}}{1000}\\cdot d^{3/2}\\right)^{2/3}\\cdot\\frac{1}{\\gamma_{1}^{2}}=d}\\end{array}$ . Finally, $\\begin{array}{r}{\\frac{10\\rho_{2}}{\\sqrt{\\ell}}=\\frac{\\gamma_{1}\\cdot\\rho_{2}^{2/3}}{d^{1/3}}\\leq\\gamma_{1}}\\end{array}$ , since $\\rho_{2}\\leq{\\sqrt{d}}$ . Overall, the ratio $\\frac{\\mathrm{vol}_{\\mathrm{n}}(\\mathcal{T}_{2}(\\mathbf{0},I))}{\\mathrm{vol}_{\\mathrm{n}}(\\mathcal{T}_{1}(\\mathbf{0},I))}\\le e^{O(\\ell\\cdot d\\log d)}\\cdot\\bigg(1+\\frac{10\\rho_{2}}{\\sqrt{\\ell}\\cdot\\gamma_{1}}\\bigg)^{2d^{2}}\\cdot6^{d}\\le e^{O(d^{5/3}\\log d\\cdot\\rho_{2}^{2/3}/\\gamma_{1}^{2})}.$ ", "page_idx": 22}, {"type": "text", "text": "Finally, we show how to remove the assumption that $\\boldsymbol{\\mu}\\mathbf{\\Lambda}=\\mathbf{0}$ and $\\Sigma\\,\\,=\\,\\,I$ . First, note that for any general $\\mu,\\Sigma,\\,(\\mu_{1},\\Sigma_{1})\\,\\in\\,T_{1}(\\mathbf{0},I)$ if and only if $(\\Sigma^{1/2}\\mu_{1}+\\mu,\\Sigma^{1/2}\\Sigma_{1}\\Sigma^{1/2})\\,\\in\\,\\mathcal{T}_{1}(\\mu,\\Sigma)$ , by Lemma C.7. So, by Lemma C.2, $\\mathrm{vol}_{\\mathrm{n}}(\\mathcal{T}_{1}(\\boldsymbol{\\mu},\\Sigma))=\\mathrm{vol}_{\\mathrm{n}}(\\mathcal{T}_{1}(\\mathbf{0},I))$ . Next, $(\\mu_{2},\\Sigma_{2})\\approx_{\\gamma_{1},\\gamma_{1}}(\\mu_{1},\\Sigma_{1})$ and (\u00b51, \u03a31) \u2248\u03b32,\u03c12,\u03b32 if and only if $(\\Sigma^{1/2}\\mu_{2}\\,+\\,\\mu,\\Sigma^{1/2}\\Sigma_{2}\\Sigma^{1/2})\\ \\approx_{\\gamma_{1},\\gamma_{1}}$ $(\\Sigma^{1/2}\\mu_{1}\\mathrm{~+~}$ $\\mu,\\Sigma^{1/2}\\Sigma_{1}\\Sigma^{1/2})$ and $\\begin{array}{r l r}{\\left(\\Sigma^{1/2}\\mu_{1}\\;+\\;\\mu,\\Sigma^{1/2}\\Sigma_{1}\\Sigma^{1/2}\\right)}&{\\approx_{\\gamma_{2},\\rho_{2},\\gamma_{2}}}&{(\\mu,\\Sigma).}\\end{array}$ , by two applications of Lemma C.7. So, by Lemma C.2, $\\mathrm{vol}_{\\mathrm{n}}(\\mathcal{T}_{2}(\\boldsymbol{\\mu},\\Sigma))=\\mathrm{vol}_{\\mathrm{n}}(\\mathcal{T}_{2}(\\mathbf{0},I))$ . Thus, the volume ratios stay the same as well. \u53e3 ", "page_idx": 22}, {"type": "text", "text": "D Fine Approximation via Hypothesis Selection ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this section, we prove that if we know a very crude approximation to all of the Gaussian mixture components with sufficiently large weight, we can privately learn the full density of the Gaussian mixture model up to low total variation distance. This will be useful for proving both Theorem 1.4 and Theorem 1.5. ", "page_idx": 22}, {"type": "text", "text": "We start with the following auxiliary lemma. ", "page_idx": 22}, {"type": "text", "text": "Lemma D.1. Let $G\\geq1$ and $\\zeta\\leq1$ be some parameters. For some $d\\geq1$ , let $\\hat{\\mu}\\in\\mathbb{R}^{d}$ , and let $\\hat{\\Sigma}\\in\\mathbb{R}^{d\\times d}$ be positive definite. Let $u_{\\hat{\\mu},\\hat{\\Sigma}}$ be the set of $(\\mu,\\Sigma)$ such\u221a that $\\textstyle{\\frac{1}{G}}\\cdot{\\hat{\\Sigma}}\\preccurlyeq\\Sigma\\preccurlyeq G\\cdot{\\hat{\\Sigma}}$ and $\\|\\hat{\\Sigma}^{-1/2}(\\mu-\\hat{\\mu})\\|_{2}\\leq G$ . Then, there exists a net $B_{\\hat{\\mu},\\hat{\\Sigma}}$ of size $O(G{\\sqrt{d}}/\\zeta)^{d(d+3)}$ such that for every $(\\mu,\\Sigma)\\in\\mathcal{U}_{\\hat{\\mu},\\hat{\\Sigma}};$ , there exists $(\\mu^{\\prime},\\Sigma^{\\prime})\\in B_{\\hat{\\mu},\\hat{\\Sigma}}$ such that $\\mathrm{d}_{\\mathrm{TV}}(\\mathcal{N}(\\mu,\\Sigma),\\mathcal{N}(\\mu^{\\prime},\\Sigma^{\\prime}))\\leq\\zeta$ . ", "page_idx": 22}, {"type": "text", "text": "Proof. First, assume that $\\hat{\\Sigma}=I$ and $\\hat{\\boldsymbol{\\mu}}=\\mathbf{0}$ . Now, let us consider the set ${\\mathcal{U}}:={\\mathcal{U}}_{\\mathbf{0},I}\\,=\\,\\{(\\mu,\\Sigma)\\,:\\,$ $\\|\\mu\\|_{2}\\leq G,{\\frac{1}{G}}\\cdot I\\prec\\Sigma\\prec G\\cdot I\\rbrace$ . Let $G^{\\prime}\\geq G$ be a parameter that we will set later. Now, we can look at the un-normalized volume of $\\boldsymbol{\\mathcal{U}}$ viewed in $\\mathbb{R}^{d(d+3)/2}$ (i.e., we are projecting to the upper-triangular part of $\\Sigma$ ). First, we claim (using a standard volume argument) that there is a cover $B\\subset\\mathcal{U}$ of size $(2G^{\\prime})^{d(d+3)}$ , such that for every $(\\mu,\\Sigma)\\in{\\mathcal{U}}$ , there is $(\\mu^{\\prime},\\Sigma^{\\prime})\\in B$ with $\\begin{array}{r}{\\|\\mu-\\mu^{\\prime}\\|_{2}\\leq\\frac{1}{G^{\\prime}}}\\end{array}$ and $\\begin{array}{r}{\\|\\Sigma-\\Sigma^{\\prime}\\|_{o p}\\leq\\frac{1}{G^{\\prime}}}\\end{array}$ . ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "To see why, consider a maximal packing of $(\\mu_{i}^{\\prime},\\Sigma_{i}^{\\prime})\\in\\mathcal{U}$ such that for every $i\\neq j$ , either $\\|\\mu_{i}^{\\prime}-\\mu_{j}^{\\prime}\\|_{2}>$ $\\frac{1}{G^{\\prime}}$ or $\\begin{array}{r}{\\|\\Sigma_{i}^{\\prime}{-}\\Sigma_{j}^{\\prime}\\|_{o p}>\\frac{1}{G^{\\prime}}.}\\end{array}$ . Then, this set is clearly a cover of $\\boldsymbol{\\mathcal{U}}$ (or else we could have increased the size of the packing, breaking maximality). Then, the sets $\\begin{array}{r}{S_{i}:=\\{(\\mu,\\Sigma):\\|\\mu-\\mu_{i}^{\\prime}\\|\\leq\\frac{1}{2G^{\\prime}},\\|\\Sigma-\\Sigma_{i}^{\\prime}\\|_{o p}\\leq}\\end{array}$ $\\frac{1}{2G^{\\prime}}\\}$ are disjoint, and are all contained in the set $S:=\\{(\\mu,\\Sigma):\\|\\mu\\|_{2}\\leq2G^{\\prime},\\|\\Sigma\\|_{o p}\\leq2G^{\\prime}\\}$ . $S$ is just a shifting and scaling of $S_{i}$ by a factor of $4(G^{\\prime})^{2}$ , and thus $\\operatorname{vol}(S)=(4(G^{\\prime})^{2})^{d(d+3)/2}\\cdot\\operatorname{vol}(S_{i})=$ $(2G^{\\prime})^{d(d+3)}\\cdot\\mathrm{vol}(S_{i})$ . Because every $S_{i}$ is disjoint and contained in $S$ , the number of such indices $i$ is at most (2G\u2032)d(d+3). ", "page_idx": 23}, {"type": "text", "text": "Next, consider any $(\\mu,\\Sigma)\\in\\mathcal{U}$ and $(\\mu^{\\prime},\\Sigma^{\\prime})\\in B$ with $\\begin{array}{r}{\\|\\mu-\\mu^{\\prime}\\|_{2}\\,\\le\\,\\frac{1}{G^{\\prime}},\\|\\Sigma-\\Sigma^{\\prime}\\|_{o p}\\,\\le\\,\\frac{1}{G^{\\prime}}}\\end{array}$ . Then, $\\|\\Sigma^{-1/2}\\Sigma^{\\prime}\\Sigma^{-1/2}\\,-\\,I\\|_{o p}\\,=\\,\\|\\Sigma^{-1/2}(\\Sigma^{\\prime}\\,-\\,\\Sigma)\\Sigma^{-1/2}\\|_{o p}\\,\\le\\,\\|\\Sigma^{-1}\\|_{o p}\\,\\cdot\\,\\|\\Sigma^{\\prime}\\,-\\,\\Sigma\\|_{o p}\\,\\le\\,\\frac{G^{\\prime}}{G^{\\prime}}$ . Also, $\\begin{array}{r}{\\|\\Sigma^{-1/2}(\\mu-\\mu^{\\prime})\\|_{2}\\leq\\|\\Sigma^{-1/2}\\|_{o p}\\cdot\\|\\mu-\\mu^{\\prime}\\|_{2}\\leq\\frac{\\sqrt{G}}{G^{\\prime}}.\\mathrm{~In~}}\\end{array}$ other words, we have found a net $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ of size $(2G^{\\prime})^{d(d+3)}$ such that for every $(\\mu,\\Sigma)\\in{\\mathcal{U}}$ , there exists $(\\mu^{\\prime},\\Sigma^{\\prime})\\in B$ with $(\\mu^{\\prime},\\Sigma^{\\prime})\\approx_{G/G^{\\prime},G/G^{\\prime}}$ $(\\mu,\\Sigma)$ . ", "page_idx": 23}, {"type": "text", "text": "Next, consider general $\\hat{\\mu},\\hat{\\Sigma}$ . Note that $(\\mu,\\Sigma)\\in\\mathcal{U}_{\\hat{\\mu},\\hat{\\Sigma}}$ is equivalent to $\\mu\\,=\\,\\hat{\\Sigma}^{1/2}\\mu_{0}+\\hat{\\mu}$ , where $\\|\\mu_{0}\\|_{2}\\leq\\ G$ , and $\\Sigma\\,=\\,\\hat{\\Sigma}^{1/2}\\Sigma_{0}\\hat{\\Sigma}^{1/2}$ , where ${\\frac{1}{G}}\\,\\cdot\\,I\\;\\prec\\;\\Sigma_{0}\\;\\prec\\;G\\,\\cdot\\,I$ . So, if we consider the map $f:(\\mu_{0},\\Sigma_{0})\\,\\mapsto\\,(\\hat{\\Sigma}^{1/2}\\mu_{0}+\\hat{\\mu},\\hat{\\Sigma}^{1/2}\\Sigma_{0}\\hat{\\Sigma}^{1/2})$ , this bijectively maps $\\boldsymbol{\\mathcal{U}}$ to $u_{\\hat{\\mu},\\hat{\\Sigma}}$ . We can also consider the cover $\\mathcal{B}_{\\hat{\\mu},\\hat{\\Sigma}}\\;=\\;f(\\boldsymbol{B})$ , where $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ is the cover constructed for $\\boldsymbol{\\mathcal{U}}$ . Then, by Lemma C.7, $(\\mu_{0}^{\\prime},\\Sigma_{0}^{\\prime})\\approx_{G/G^{\\prime},G/G^{\\prime}}$ $\\left(\\mu_{0},\\Sigma_{0}\\right)$ if and only if $f(\\mu_{0}^{\\prime},\\Sigma_{0}^{\\prime})\\approx_{G/G^{\\prime},G/G^{\\prime}}f(\\mu_{0},\\Sigma_{0})$ . ", "page_idx": 23}, {"type": "text", "text": "Hence, regardless of the choice of $\\hat{\\mu},\\hat{\\Sigma}$ , for every $(\\mu,\\Sigma)\\,\\in\\,\\mathcal{U}_{\\hat{\\mu},\\hat{\\Sigma}}$ , there exists $(\\mu^{\\prime},\\Sigma^{\\prime})\\;\\in\\;\\mathcal{B}_{\\hat{\\mu},\\hat{\\Sigma}}$ with $(\\mu^{\\prime},\\Sigma^{\\prime})~\\approx_{G/G^{\\prime},G/G^{\\prime}}$ $(\\mu,\\Sigma)$ . Moreover, $|{\\cal B}_{\\hat{\\mu},\\hat{\\Sigma}}|~=~|{\\cal B}|~\\le~(2G^{\\prime})^{d(d+3)}$ . Moreover, if $(\\mu^{\\prime},\\Sigma^{\\prime})~\\approx_{G/G^{\\prime},G/G^{\\prime}}$ $(\\mu,\\Sigma)$ , then $\\begin{array}{r l r}{\\|\\Sigma^{-1/2}(\\mu^{\\prime}\\,-\\,\\dot{\\mu})\\|_{2}\\!\\;\\leq\\!\\;\\frac{G}{G^{\\prime}}}\\end{array}$ and $\\|\\Sigma^{-1/2}\\Sigma\\Sigma^{-1/2}\\;-\\;I\\|_{F}\\;\\;\\leq$ $\\begin{array}{r}{\\sqrt{d}\\cdot\\|\\Sigma^{-1/2}\\Sigma\\Sigma^{-1/2}-I\\|_{o p}\\,\\leq\\,\\frac{\\sqrt{d}\\cdot G}{G^{\\prime}}}\\end{array}$ .e  Ssioz,e  boyf  Ltheem nmeat $\\begin{array}{r}{\\mathrm{d}_{\\mathrm{TV}}(N(\\mu,\\Sigma),\\mathcal{N}(\\mu^{\\prime},\\Sigma^{\\prime}))\\,\\leq\\,\\frac{\\sqrt{d}\\cdot G}{G^{\\prime}}}\\end{array}$ . $\\begin{array}{r}{G^{\\prime}\\,=\\,\\frac{G\\sqrt{d}}{\\zeta}}\\end{array}$ $B_{\\hat{\\mu},\\hat{\\Sigma}}$ $O(G{\\sqrt{d}}/\\zeta)^{d(d+3)}$ $(\\mu,\\Sigma)\\in\\mathcal{U}_{\\hat{\\mu},\\hat{\\Sigma}}$ , there exists $\\hat{(\\mu^{\\prime},\\Sigma^{\\prime})}\\in B_{\\hat{\\mu},\\hat{\\Sigma}}$ such that $\\mathrm{d}_{\\mathrm{TV}}(\\dot{\\mathcal{N}}(\\mu,\\Sigma),\\mathcal{N}(\\mu^{\\prime},\\Sigma^{\\prime}))\\leq\\zeta$ . \u53e3 ", "page_idx": 23}, {"type": "text", "text": "Next, we note the following result about differentially private hypothesis selection. ", "page_idx": 23}, {"type": "text", "text": "Theorem D.2. [BSKW19] Let ${\\mathcal{H}}\\,=\\,\\{H_{1},...\\,,H_{M}\\}$ be a set of probability distributions over some domain $D$ . There exists an $\\varepsilon$ -differentially private algorithm (with respect to a dataset $\\mathbf{X}=$ $\\{X_{1},\\ldots,X_{n}\\}.$ ) which has following guarantees. ", "page_idx": 23}, {"type": "text", "text": "Let $\\mathcal{D}$ be an unknown probability distribution over $D$ , and suppose there exists a distribution $H^{*}\\in{\\mathcal{H}}$ such that $\\mathrm{d}_{\\mathrm{TV}}({\\mathcal{D}},H^{*})\\leq\\alpha.$ . I $\\begin{array}{r}{{\\boldsymbol{r}}_{n}\\geq O\\left(\\frac{\\log M}{\\alpha^{2}}+\\frac{\\log M}{\\alpha\\varepsilon}\\right)}\\end{array}$ , and if $X_{1},\\ldots,X_{n}$ are samples drawn independently from $\\mathcal{D}$ , then the algorithm will output a distribution $\\hat{H}\\;\\in\\;\\mathcal{H}$ such that $\\mathrm{d}_{\\mathrm{TV}}(D,\\hat{H})\\leq4\\alpha$ with probability at least 9/10. ", "page_idx": 23}, {"type": "text", "text": "Given Lemma D.1 and Theorem D.2, we are in a position to convert any crude approximation into a fine approximation. Namely, we prove the following result. ", "page_idx": 23}, {"type": "text", "text": "Lemma D.3. Let $\\mathcal{D}$ represent an unknown GMM with representation $\\{(w_{i},\\mu_{i},\\Sigma_{i})\\}_{i=1}^{k}$ . Let $G~\\ge~1~$ be some fixed parameter. Then there exists an $\\varepsilon$ -differentially algorithm, on $n~\\ge$ $\\begin{array}{r}{{\\cal O}\\left(\\frac{d^{2}k\\cdot\\log(G\\cdot k\\cdot k^{\\prime}\\cdot\\sqrt{d}/\\alpha)}{\\alpha^{2}}+\\frac{d^{2}k\\cdot\\log(G\\cdot k\\cdot k^{\\prime}\\cdot\\sqrt{d}/\\alpha)}{\\alpha\\varepsilon}\\right)}\\end{array}$ samples, with the following property. ", "page_idx": 23}, {"type": "text", "text": "Suppose the algorithm is given as input a set $\\{(\\hat{\\mu}_{j},\\hat{\\Sigma}_{j})\\}_{j=1}^{k^{\\prime}}$ for some $k^{\\prime}\\geq1$ , such that for every $\\left(\\mu_{i},\\Sigma_{i}\\right)$ with weight $\\begin{array}{r}{w_{i}\\geq\\frac{\\alpha}{k}}\\end{array}$ , there exists $j\\le k^{\\prime}$ such that $\\begin{array}{r}{\\frac{1}{G}\\cdot\\hat{\\Sigma}_{j}\\prec\\Sigma_{i}\\preccurlyeq G\\cdot\\hat{\\Sigma}_{j}}\\end{array}$ and $\\|\\hat{\\Sigma}_{j}^{-1/2}(\\mu_{i}-$ $\\hat{\\mu}_{j})\\|_{2}\\leq G.$ . Then, if the samples are $X_{1},\\ldots,X_{n}\\stackrel{i.i.d.}{\\sim}\\mathcal{D},$ , then with probability at least $9/10,$ , the algorithm outputs a mixture of at most $k$ Gaussians $H$ , with $\\mathrm{d}_{\\mathrm{TV}}({\\mathcal{D}},H)\\leq O(\\alpha)$ . ", "page_idx": 23}, {"type": "text", "text": "Proof. Let $\\zeta=\\frac{\\alpha}{k}$ . For each $(\\hat{\\mu}_{j},\\hat{\\Sigma}_{j})$ , let $B_{\\hat{\\mu}_{j},\\hat{\\Sigma}_{j}}$ be as in Lemma D.1. We define $\\begin{array}{r}{\\hat{\\boldsymbol{B}}=\\bigcup_{j\\le k^{\\prime}}\\boldsymbol{B}_{\\hat{\\mu}_{j},\\hat{\\Sigma}_{j}}}\\end{array}$ . ", "page_idx": 24}, {"type": "text", "text": "Let $\\mathcal{H}$ be the set of hypotheses consisting of mixtures of up to $k$ Gaussians $\\mathcal{N}(\\mu_{i}^{\\prime},\\Sigma_{i}^{\\prime})$ with weights $w_{i}^{\\prime}$ , with every $(\\mu_{i}^{\\prime},\\Sigma_{i}^{\\prime})\\in\\hat{B}$ and with every $w_{i}^{\\prime}$ an integral mu\u221altiple of $\\zeta$ . Note that the number of hypothesis $M=|\\mathcal{H}|$ is at most $|\\hat{\\mathcal{B}}|^{O(k)}\\cdot(1/\\zeta)^{O(k)}=(k^{\\prime}\\cdot G\\cdot\\sqrt{d}/\\zeta)^{O(d^{2}\\cdot k)}$ . Since we set $\\zeta=\\alpha/k$ , $\\begin{array}{r}{M\\leq(G\\cdot\\frac{k k^{\\prime}\\sqrt{d}}{\\alpha})^{O(d^{2}\\cdot k)}}\\end{array}$ . ", "page_idx": 24}, {"type": "text", "text": "Next, let us consider the true GMM $\\mathcal{D}$ , with representation $\\{(w_{i},\\mu_{i},\\Sigma_{i})\\}_{i=1}^{k}$ . Because every $\\left(\\mu_{i},\\Sigma_{i}\\right)$ with $\\begin{array}{r}{w_{i}\\geq\\frac{\\alpha}{k}}\\end{array}$ satisfies $\\begin{array}{r}{\\frac{1}{G}\\cdot\\hat{\\Sigma}_{j}\\prec\\Sigma_{i}\\preccurlyeq G\\cdot\\hat{\\Sigma}_{j}}\\end{array}$ and $\\|\\hat{\\Sigma}_{j}^{-1/2}(\\mu_{i}-\\hat{\\mu}_{j})\\|_{2}\\leq G$ , by Lemma D.1, there exists $(\\mu_{i}^{\\prime},\\Sigma_{i}^{\\prime})\\in\\hat{B}$ such that $\\mathrm{d}_{\\mathrm{TV}}(\\mathcal{N}(\\mu_{i},\\Sigma_{i}),(\\mu_{i}^{\\prime},\\Sigma_{i}^{\\prime}))\\leq\\zeta$ . Moreover, we can round each $\\begin{array}{r}{w_{i}\\geq\\frac{\\alpha}{k}}\\end{array}$ to some $w_{i}^{\\prime}$ which is an integral multiple of $\\zeta$ , such that $|w_{i}-w_{i}^{\\prime}|\\leq\\zeta$ . Finally, for each $\\begin{array}{r}{w_{i}<\\frac{\\alpha}{k}}\\end{array}$ , we can choose an arbitrary Gaussian in $\\hat{\\boldsymbol{B}}$ and round $w_{i}$ to some $w_{i}^{\\prime}$ . Then, the total variation distance between $\\mathcal{D}$ and the GMM with representation $\\{(w_{i}^{\\prime},\\mu_{i}^{\\prime},\\Sigma_{i}^{\\prime})\\}_{i=1}^{k}$ is at most ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{\\substack{i\\leq k:w_{i}>\\alpha/k}}\\left(\\frac{\\alpha}{k}+|w_{i}-w_{i}^{\\prime}|\\right)+\\sum_{\\substack{i\\leq k:w_{i}\\geq\\alpha/k}}(\\zeta+|w_{i}-w_{i}^{\\prime}|)\\leq\\alpha+O(k\\cdot\\zeta).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Hence, if we set $\\zeta=\\frac{\\alpha}{k}$ , there exists a distribution $H^{*}\\in\\mathcal{H}$ with $\\mathrm{d}_{\\mathrm{TV}}({\\mathcal{D}},H^{*})\\leq O(\\alpha)$ . ", "page_idx": 24}, {"type": "text", "text": "Therefore, the algorithm of Theorem D.2, using $\\begin{array}{r}{n\\ge O\\left(\\frac{d^{2}k\\cdot\\log(G\\cdot k\\cdot k^{\\prime}\\cdot\\sqrt{d}/\\alpha)}{\\alpha^{2}}+\\frac{d^{2}k\\cdot\\log(G\\cdot k\\cdot k^{\\prime}\\cdot\\sqrt{d}/\\alpha)}{\\alpha\\varepsilon}\\right)}\\end{array}$ samples, will find $H\\in{\\mathcal{H}}$ such that $\\mathrm{d}_{\\mathrm{TV}}(\\mathcal{D},H)\\leq O(\\dot{\\alpha})$ , and is $\\varepsilon$ -differentially private. \u53e3 ", "page_idx": 24}, {"type": "text", "text": "Pseudocode: We give a simple pseudocode for the algorithm of Lemma D.3, in Algorithm 1. ", "page_idx": 24}, {"type": "text", "text": "$\\begin{array}{r}{\\mathbf{Algorithm\\textbf{1:}F I N E-E S T I M A T E}(X_{1},X_{2},\\ldots,X_{n}\\in\\mathbb{R}^{d},d,k,k^{\\prime},\\varepsilon,\\alpha,G,\\{(\\hat{\\mu}_{j},\\hat{\\Sigma}_{j})\\}_{j\\leq k^{\\prime}})}\\end{array}$ Input: Samples $X_{1},\\ldots,X_{n}$ , and crude predictions $(\\hat{\\mu}_{j},\\hat{\\Sigma}_{j})$ , where $1\\le j\\le k^{\\prime}$ for some $k^{\\prime}$ . Output: An $(\\varepsilon,0)$ -DP prediction $H$ , which is a mixture over at most $k$ Gaussians. 1 Set $\\zeta=\\alpha/k$ . 2 for $j=1$ to $k^{\\prime}$ do 3 Define the sets B\u00b5\u02c6j,\u03a3\u02c6j as in Lemma D.1, for parameters G, \u03b6. 4 end 5 B\u02c6 \u2190 j\u2264k\u2032 B\u00b5\u02c6j,\u03a3\u02c6j . 6 Let $\\mathcal{H}$ be the set of mixtures $\\{(w_{i}^{\\prime},\\mu_{i}^{\\prime},\\Sigma_{i}^{\\prime})\\}$ of $k$ or fewer components, where every $(\\mu_{i}^{\\prime},\\Sigma_{i}^{\\prime})\\in\\hat{B}$ and every $w_{i}^{\\prime}$ is an integral multiple of $\\zeta$ . 7 Run the $\\varepsilon$ -DP algorithm of Theorem D.2 on $X_{1},\\ldots,X_{n}$ with respect to $\\mathcal{H}$ . ", "page_idx": 24}, {"type": "text", "text": "E The High Dimensional Setting ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In this section, we provide the algorithm and analysis for Theorem 1.4. We first describe and provide pseudocode for the algorithm, and then we prove that it can privately learn mixtures of $d_{\\cdot}$ -dimensional Gaussians with low sample complexity. ", "page_idx": 24}, {"type": "text", "text": "E.1 Algorithm ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "High-Level Approach: Suppose that the unknown distribution is a GMM with representation $\\{(\\breve{w}_{i},\\mu_{i},\\Sigma_{i})\\}_{i=1}^{\\bar{k}}$ . ", "page_idx": 24}, {"type": "text", "text": "Define $\\Theta$ to be the set of all feasible mean-covariance pairs $(\\mu,\\Sigma)$ , i.e., where $\\mu\\in\\mathbb{R}^{d}$ and $\\Sigma\\in\\mathbb{R}^{d\\times d}$ is positive definite. We also start off with a set $\\Omega=\\Theta$ , which will roughly characterize the region of \u201cremaining\u201d mean-covariance pairs. ", "page_idx": 24}, {"type": "text", "text": "At a high level, our algorithm proceeds as follows. We will first learn a very crude approximation of each $\\left(\\mu_{i},\\Sigma_{i}\\right)$ , one at a time. Namely, using roughly $\\begin{array}{r}{\\left(\\frac{\\varepsilon}{\\sqrt{k\\log(1/\\delta)}},\\frac{\\delta}{k}\\right)}\\end{array}$ -DP, we will learn some $(\\hat{\\mu}_{1},\\hat{\\Sigma}_{1})\\in\\Omega$ which is \u201cvaguely close\u201d to some $\\left(\\mu_{i},\\Sigma_{i}\\right)$ . We will then remove every $(\\mu,\\Sigma)$ that is vaguely close to this $(\\hat{\\mu},\\hat{\\Sigma})$ from our $\\Omega$ , and then attempt to repeat this process. We will repeat it up to $k$ times, in hopes that every $(\\mu_{i},\\Sigma_{i})$ is vaguely close to some $(\\hat{\\mu}_{j},\\hat{\\Sigma}_{j})$ . By advanced composition, the full set of $\\{(\\hat{\\mu}_{j},\\hat{\\Sigma}_{j})\\}$ is still $(\\varepsilon,\\delta)$ -DP. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "At this point, the remainder of the algorithm is quite simple. Namely, we have some crude estimate for every $\\left(\\mu_{i},\\Sigma_{i}\\right)$ (it is \u201cvaguely close\u201d to some $(\\hat{\\mu}_{j},\\hat{\\Sigma}_{j}))$ ). Moreover, one can create a fine net of roughly $e^{\\tilde{O}(d^{2})}\\left(\\mu,\\Sigma\\right)$ -pairs that cover all mean-covariance pairs vaguely close to each $(\\hat{\\mu}_{j},\\hat{\\Sigma}_{j})$ , since the dimension of $(\\mu,\\Sigma)$ is $O(d^{2})$ . Moreover, the weight vector $(w_{1},\\hdots,w_{k})$ has a fine net of size roughly $e^{\\tilde{O}(k)}$ . As a result, we can reduce the problem to a small set of hypotheses: there are roughly $e^{\\tilde{O}(\\stackrel{-}{d}^{2})}$ choices for each $\\left(\\mu_{i},\\Sigma_{i}\\right)$ , and $e^{\\tilde{O}(k)}$ choices for $w$ , for a total of $e^{\\tilde{O}(k\\cdot d^{2})}$ choices for the mixture of Gaussians. We can then apply known results on private hypothesis selection [BSKW19], which will suffice. ", "page_idx": 25}, {"type": "text", "text": "The main difficulty in the algorithm is in privately learning some $(\\hat{\\mu},\\hat{\\Sigma})$ which is \u201cvaguely close\u201d to some $\\left(\\mu_{i},\\Sigma_{i}\\right)$ . We accomplish this task by applying the robustness-to-privacy conversion of [HKMN23], along with a carefully constructed score function, which we will describe later in this section. ", "page_idx": 25}, {"type": "text", "text": "Algorithm Description: Let $c_{0}\\leq0.01$ and $C_{0}>1$ be the constants of Lemma B.6, and $c_{1}\\leq0.01$ be the constant of Lemma C.8. Let $c_{2}\\,\\leq\\,0.1\\cdot\\operatorname*{min}(c_{0},c_{1})$ be a sufficiently small constant. We set parameters $\\begin{array}{r}{\\eta^{*}\\,=\\,\\frac{c_{2}\\cdot\\alpha}{8k},\\eta\\,=\\,\\frac{\\eta^{*}}{10},\\varepsilon_{0}\\,=\\,\\frac{\\varepsilon}{\\sqrt{4k\\log(1/\\delta)}},\\delta_{0}\\,=\\,\\frac{\\delta}{2k},\\beta_{0}\\,=\\,e^{-d}}\\end{array}$ . We also set $m=$ $\\begin{array}{r}{\\widetilde{O}(d^{1.75}),N=\\widetilde{O}\\left(m\\cdot\\frac{\\sqrt{k\\log(1/\\delta)}}{\\varepsilon}+\\frac{\\sqrt{k}\\cdot\\log^{3/2}(1/\\delta)}{\\varepsilon}+k d\\right)}\\end{array}$ , and $\\textstyle n=N\\cdot{\\frac{2k}{\\alpha}}$ . Finally, we define parameters $\\gamma\\,=\\,\\tau\\,=\\,c_{2}$ and $\\rho\\,=\\,c_{2}\\cdot d^{1/8}$ . (See Lines 1\u20134 of Algorithm 2 for a more precise description of some of the parameters.) ", "page_idx": 25}, {"type": "text", "text": "We now define the main score function. To do so, we first set some auxiliary parameter $\\begin{array}{r}{\\eta^{\\prime}=\\gamma^{\\prime}=\\frac{c_{2}}{8C_{0}}}\\end{array}$ and $\\begin{array}{r}{\\rho^{\\prime}=\\frac{\\rho}{8C_{0}}}\\end{array}$ . We will consider the (deterministic) algorithm $\\boldsymbol{\\mathcal{A}}$ of Lemma B.6, where $\\boldsymbol{\\mathcal{A}}$ is given parameters $\\eta^{\\prime},\\gamma^{\\prime},\\rho^{\\prime},\\beta_{0}$ , that acts on a dataset $\\mathbf{Z}$ and outputs either $\\perp$ or some $(\\hat{\\mu},\\hat{\\Sigma})$ . Next, for some domain $\\Omega\\subset\\Theta$ of \u201cfeasible\u201d mean-covariance pairs $(\\mu,\\Sigma)$ , we will define a function $f_{\\Omega}$ , which takes as input a dataset $\\mathbf{Y}$ of size $N$ and a mean-covariance pair $(\\mu,\\Sigma)\\in\\Theta$ , and outputs ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\widehat{\\mathbf{\\xi}}_{\\Omega}(\\mathbf{Y},\\mu,\\Sigma)=\\left\\{\\begin{array}{l l}{1}&{\\mathrm{if}\\,\\,\\left(\\mu,\\Sigma\\right)\\in\\Omega\\,,\\,\\,\\mathcal{A}(\\mathbf{Y})\\approx_{\\gamma,\\rho,\\tau}\\left(\\mu,\\Sigma\\right),\\,\\,\\,\\mathrm{and}\\,\\,\\,\\,\\underbrace{\\mathbb{P}}_{\\Sigma\\,\\Gamma}\\left[\\mathcal{A}(\\mathbf{Z})\\approx_{\\gamma,\\rho,\\tau}\\left(\\mu,\\Sigma\\right)\\right]\\geq\\frac{2}{3}}\\\\ {0}&{\\mathrm{else}\\,.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Finally, given a dataset $\\mathbf{X}$ of size $n$ and $(\\widetilde{\\mu},\\widetilde{\\Sigma})\\in\\Theta$ , we define the score function ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{S_{\\Omega}((\\widetilde{\\mu},\\widetilde{\\Sigma}),\\mathbf{X})=\\operatorname*{min}\\big\\{t:\\exists\\mathbf{X}^{\\prime},\\mathbf{Y}^{\\prime},\\mu,\\Sigma\\mathrm{~such~that~}}\\\\ &{\\qquad\\qquad\\quad\\mathrm{d}_{\\mathrm{H}}(\\mathbf{X},\\mathbf{X}^{\\prime})=t,\\mathbf{Y}^{\\prime}\\subset\\mathbf{X}^{\\prime},|\\mathbf{Y}^{\\prime}|=N,(\\widetilde{\\mu},\\widetilde{\\Sigma})\\approx_{\\gamma,\\tau}(\\mu,\\Sigma),f_{\\Omega}(\\mathbf{Y}^{\\prime},\\mu,\\Sigma)=1\\big\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Note that $(\\widetilde{\\mu},\\widetilde{\\Sigma})$ does not need to be in $\\Omega$ , but it must satisfy $(\\widetilde{\\mu},\\widetilde{\\Sigma})\\approx_{\\gamma,\\tau}(\\mu,\\Sigma)$ for some $(\\mu,\\Sigma)\\in\\Omega$ . Also, note that it is possible that no matter how one changes the data points, the conditions are never met (for instance if $(\\widetilde{\\mu},\\widetilde{\\Sigma})\\not\\approx_{\\gamma,\\tau}(\\mu,\\Sigma)$ for any $(\\mu,\\Sigma)\\in\\Omega)$ ). This is not a problem: we will simply set the score to be $+\\infty$ if this occurs. ", "page_idx": 25}, {"type": "text", "text": "With this definition of score function $S_{\\Omega}$ , we can let $\\mathcal{M}$ be an $(\\varepsilon_{0},\\delta_{0})$ -DP algorithm based on tThhaet .i3s,  vwieitwh etdh ea ss o-fd $n,\\eta,\\eta^{*},\\varepsilon_{0},\\delta_{0},\\beta_{0}$ ya sc oanbsoivdee,r ianngd  twhiet hu $\\begin{array}{r}{D=\\frac{n(n+3)}{2}}\\end{array}$ .a rH pearret,  owf .e cWalel $(\\tilde{\\mu},\\tilde{\\Sigma})$ $\\textstyle{\\frac{n(n+3)}{2}}$ $\\tilde{\\Sigma}$ also assume the domain is $\\bar{\\Theta}$ and the normalized volume is as in (1). ", "page_idx": 25}, {"type": "text", "text": "Given this algorithm $\\mathcal{M}$ (which implicitly depends on $\\Omega$ ), the algorithm works as follows. We will actually draw a total of $n+n^{\\prime}$ samples (for $n$ as above, and $n^{\\prime}$ to be defined later), though we start by just looking at the first $n$ samples $\\mathbf{X}=\\{X_{1},\\ldots,X_{n}\\}$ . We initially set $\\Omega=\\Theta$ , and run the following procedure up to $k$ times, or until the algorithm outputs $\\perp$ . For the $j^{\\mathrm{th}}$ iteration, we use $\\mathcal{M}(\\mathbf{X})$ to compute some prediction $(\\hat{\\mu}_{j},\\hat{\\Sigma}_{j})$ (or possibly we output $\\perp$ ). Assuming we do not output $\\bot$ , we remove from $\\Omega$ every $(\\mu,\\Sigma)$ such that $n^{-12}\\cdot\\hat{\\Sigma}_{j}\\precneq\\Sigma\\preccurlyeq n^{12}\\cdot\\hat{\\Sigma}_{j}$ and $\\|\\hat{\\Sigma}_{j}^{-1/2}(\\mu-\\hat{\\mu}_{j})\\|_{2}\\leq n^{12}$ . At the end, we have computed some $\\{(\\hat{\\mu}_{j},\\hat{\\Sigma}_{j})\\}_{j=1}^{k^{\\prime}}$ where $0\\leq k^{\\prime}\\leq k$ . If $k^{\\prime}=0$ we simply output $\\bot$ . Otherwise, we will draw $\\begin{array}{r}{n^{\\prime}=\\tilde{O}\\left(\\frac{d^{2}k}{\\alpha^{2}}+\\frac{d^{2}k}{\\alpha\\varepsilon}\\right)}\\end{array}$ fresh samples. We run the $\\varepsilon$ -DP hypothesis selection based algorithm, Algorithm 1 on the fresh samples, using parameters $G=n^{12}$ and $\\{(\\hat{\\mu}_{j},\\hat{\\Sigma}_{j})\\}_{j=1}^{k^{\\prime}}$ . ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "Pseudocode: We give pseudocode for the algorithm in Algorithm 2. ", "page_idx": 26}, {"type": "text", "text": "$\\mathbf{Algorithm\\left.2\\cdotESTIMATE\\left(}X_{1},X_{2},\\ldots,X_{n+n^{\\prime}}\\in\\mathbb{R}^{d},k,\\varepsilon,\\delta,\\alpha\\right\\rangle$ Input: Samples $X_{1},\\ldots,X_{n}$ , $X_{n+1},\\ldots,X_{n+n^{\\prime}}$ . Output: An $(\\varepsilon,\\delta)$ -DP prediction $\\{(\\widetilde{\\mu}_{i},\\widetilde{\\Sigma}_{i}),\\widetilde{w}_{i}\\}$ . $/*$ Set parameters $\\ast/$ 1 Let $c_{2}$ be a sufficiently small constant and $\\begin{array}{r}{K=\\mathrm{poly}\\log(d,k,\\frac{1}{\\alpha},\\frac{1}{\\varepsilon},\\log\\frac{1}{\\delta})}\\end{array}$ be sufficiently large. 2 $\\begin{array}{r}{\\eta^{*}\\leftarrow\\frac{c_{2}\\cdot\\alpha}{8k}}\\end{array}$ , $\\begin{array}{r}{\\eta\\leftarrow\\frac{\\eta^{*}}{10}}\\end{array}$ , $\\begin{array}{r}{\\varepsilon_{0}\\leftarrow\\frac{\\varepsilon}{\\sqrt{4k\\log(1/\\delta)}}}\\end{array}$ , $\\delta_{0}\\leftarrow\\frac{\\delta}{2k}$ , $\\beta_{0}\\gets e^{-d}$ . 3 $m=K\\cdot d^{1.75}$ , $\\begin{array}{r}{N\\leftarrow K\\cdot\\left(\\frac{m+\\log(1/\\delta_{0})}{\\varepsilon_{0}}+k d\\right)}\\end{array}$ , n \u2190N \u00b7 2k. 4 \u03b3, \u03c4 \u2190c2, \u03c1 \u219010 \u00b7 d1/8. 5 $\\Theta,\\Omega\\gets\\{(\\mu,\\Sigma):\\mu\\in\\mathbb{R}^{d}$ , $\\Sigma\\in\\mathbb{R}^{d\\times d}$ Symmetric, Positive Definite $\\}$ . /\\* Get X \\*/ 6 Obtain samples $\\mathbf{X}\\gets\\{X_{1},X_{2},\\ldots,X_{n}\\}$ . $/*$ Learn a crude approximation $(\\hat{\\mu}_{j},\\hat{\\Sigma}_{j})$ of the mean-covariance pairs, one at a time $\\ast/$ 7 for $j=1$ to $k$ do 8 Define $f_{\\Omega}({\\bf Y},\\mu,\\Sigma)$ and $S_{\\Omega}((\\mu,\\Sigma),{\\bf X})$ as in (2) and (3), respectively. 9 Let $\\mathcal{M}$ be the $(\\varepsilon_{0},\\delta_{0})$ -DP algorithm obtained by Theorem C.3 with the score function $S_{\\Omega}$ , with parameters $n,\\eta,\\eta^{*},\\varepsilon_{0},\\delta_{0},\\beta_{0}$ as defined above, $\\begin{array}{r}{D=\\frac{n(n+3)}{2}}\\end{array}$ , domain $\\Theta$ , and $p(\\mu,\\Sigma)=(\\operatorname*{det}\\Sigma)^{-(d+2)/2}$ . 10 $A\\leftarrow\\mathcal{M}(\\mathbf{X})$ 11 if $A\\neq\\perp$ then 12 \u00b5\u02c6j, \u03a3\u02c6j \u2190A 13 $\\begin{array}{r}{\\tilde{\\Omega}\\longleftarrow\\overleftarrow{\\Omega}\\backslash\\{(\\mu,\\Sigma):n^{-12}\\cdot\\hat{\\Sigma}_{j}\\preccurlyeq\\Sigma\\preccurlyeq n^{12}\\cdot\\hat{\\Sigma}_{j}\\mathrm{~and~}\\|\\hat{\\Sigma}_{j}^{-1/2}(\\mu-\\hat{\\mu}_{j})\\|_{2}\\leq n^{12}\\}}\\end{array}$ 14 else 15 Break // Break out of the for loop 16 end 17 end $/*$ Fine approximation, via private hypothesis selection \\*/ 18 Set $\\begin{array}{r}{n^{\\prime}\\leftarrow K\\cdot\\left(\\frac{d^{2}k}{\\alpha^{2}}+\\frac{d^{2}k}{\\alpha\\varepsilon}\\right)}\\end{array}$ . 19 Sample $X_{n+1},\\ldots,X_{n+n^{\\prime}}$ , and redefine $\\mathbf{X}\\gets\\{X_{n+1},\\ldots,X_{n+n^{\\prime}}\\}.$ ", "page_idx": 26}, {"type": "text", "text": "20 Run Algorithm 1 on $\\mathbf{X}$ , $\\{(\\hat{\\mu}_{j},\\hat{\\Sigma}_{j})\\}$ , with $k^{\\prime}=\\#\\{(\\hat{\\mu}_{j},\\hat{\\Sigma}_{j})\\}$ and $G=n^{12}$ ", "page_idx": 26}, {"type": "text", "text": "E.2 Analysis of Crude approximation ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In this section, we analyze Lines 7\u201317 of Algorithm 2. The main goal is to show that the algorithm is private, and that for samples drawn from a mixture of Gaussians, every component $\\left(\\mu_{i},\\Sigma_{i}\\right)$ with large enough weight $w_{i}$ is \u201cvaguely close\u201d to some $(\\hat{\\mu}_{j},\\hat{\\Sigma}_{j})$ computed by the algorithm. ", "page_idx": 26}, {"type": "text", "text": "First, we show that when the samples are actually drawn as a Gaussian mixture, then under some reasonable conditions, any $(\\mu,\\Sigma)$ close to a true mean-covariance pair has low score. ", "page_idx": 26}, {"type": "text", "text": "Proposition E.1. Suppose $\\mathbf{X}\\,=\\,\\{X_{1},\\ldots,X_{n}\\}$ is drawn from a Gaussian mixture model, with representation $\\{(w_{i},\\mu_{i},\\Sigma_{i})\\}_{i=1}^{k}$ . Then, with $1\\,-\\,O(k\\,\\cdot\\,\\beta_{0})$ probability over X, for any set $\\Omega$ of mean-covariance pairs, for all $i~\\in~[k]$ such that $(\\mu_{i},\\Sigma_{i})\\;\\in\\;\\Omega$ and $w_{i}~\\geq~\\alpha/k$ , and for all $(\\widetilde{\\mu},\\widetilde{\\Sigma})\\approx_{\\gamma,\\tau}(\\mu_{i},\\Sigma_{i}),\\widetilde{S}_{\\Omega}((\\widetilde{\\mu},\\widetilde{\\Sigma}),{\\bf X})=0$ . ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "Proof. Fix any $i\\in[k]$ with $w_{i}\\geq\\alpha/k$ . Set $\\mu=\\mu_{i}$ and $\\Sigma=\\Sigma_{i}$ . Also, let $\\mathbf{X}^{\\prime}=\\mathbf{X}$ , and $\\mathbf{Y}$ be a random subset of size $N$ among the samples in $\\mathbf{X}$ actually drawn from $\\mathcal{N}(\\mu_{i},\\Sigma_{i})$ . Note that the number of such samples has distribution $\\operatorname{Bin}(n,w_{i})$ , which for $\\begin{array}{r}{w_{i}\\geq\\frac{\\alpha}{k}}\\end{array}$ and $\\textstyle n={\\frac{2k}{\\alpha}}\\cdot N$ , is at least $N$ with $e^{-\\Omega(N)}\\leq\\beta_{0}$ failure probability. ", "page_idx": 27}, {"type": "text", "text": "Then, $\\mathbf{Y}$ is just $N$ i.i.d. samples from $\\mathcal{N}(\\mu_{i},\\Sigma_{i})$ . So, if we draw a random subset $\\mathbf{Z}$ of size $m$ of $\\mathbf{Y}$ , it has the same distribution as $m$ i.i.d. samples from $\\mathcal{N}(\\mu_{i},\\Sigma_{i})$ . We apply Part 2 of Lemma B.6 (where we use parameters $\\eta^{\\prime},\\gamma^{\\prime},\\rho^{\\prime}$ in the application). Note that $\\begin{array}{r}{m\\geq\\widetilde O\\left(\\frac{d}{\\gamma^{\\prime2}}+\\frac{d^{2}}{\\rho^{\\prime2}}\\right)}\\end{array}$ , so with probability at least $1-O(\\beta_{0})$ over Z, $A(\\mathbf{Z})\\approx_{8C_{0}\\gamma^{\\prime},8C_{0}\\rho^{\\prime},8C_{0}\\gamma^{\\prime}}\\left(\\mu_{i},\\Sigma_{i}\\right)$ . By our setting of $\\gamma^{\\prime},\\rho^{\\prime}$ , this means that ${\\cal A}({\\bf Z})\\approx_{\\gamma,\\rho,\\gamma}\\left(\\mu_{i},\\Sigma_{i}\\right)$ . ", "page_idx": 27}, {"type": "text", "text": "In other words, for each index $i\\;\\in\\;\\binom{N}{m}$ and corresponding subset $\\mathbf{Z}$ of $\\mathbf{Y}$ , if we let $W_{i}$ be the indicator that ${\\mathcal{A}}(\\mathbf{Z})\\not\\approx_{\\gamma,\\rho,\\gamma}\\;(\\mu_{i},\\Sigma_{i})$ , then $\\mathbb{P}(W_{i}\\,=\\,1)\\,\\leq\\,O(\\beta_{0})$ . While the values of $W_{i}$ are not necessarily independent, by linearity of expectation we have that $\\mathbb{E}[\\sum W_{i}]\\,\\le\\,\\binom{N}{m}\\cdot O(\\beta_{0})$ , so the probability that $\\mathbb{E}[\\sum W_{i}]\\,\\ge\\,\\frac{1}{3}\\,\\cdot\\,{\\binom{N}{m}}$ is at most $O(\\beta_{0})$ by Markov\u2019s inequality. Moreover, because $\\begin{array}{r}{N\\ge m\\ge\\widetilde O\\left(\\frac{d}{\\gamma^{\\prime2}}+\\frac{d^{2}}{\\rho^{\\prime2}}\\right)}\\end{array}$ , we can apply $\\boldsymbol{\\mathcal{A}}$ on $\\mathbf{Y}$ and we again obtain that with probability $1\\!-\\!O(\\beta_{0})$ $A(\\mathbf{Y})\\approx_{\\gamma,\\rho,\\gamma}\\left(\\mu_{i},\\Sigma_{i}\\right)$ . ", "page_idx": 27}, {"type": "text", "text": "In summary, for any fixed $i\\in[k]$ with $\\begin{array}{r}{w_{i}\\geq\\frac{\\alpha}{k}}\\end{array}$ , with probability at least $1-O(\\beta_{0})$ over $\\mathbf{X}$ , there exists $\\mathbf{Y}\\subset\\mathbf{X}$ of size $N$ , such that $A(\\mathbf{Y})\\approx_{\\gamma,\\tilde{\\rho},\\gamma}\\left(\\mu_{i},\\Sigma_{i}\\right)$ and at least $2/3$ of the subsets $\\mathbf{Z}\\subset\\mathbf{Y}$ of size $m$ have $\\mathscr{A}(\\mathbf{Z})\\approx_{\\gamma,\\rho,\\gamma}\\left(\\mu_{i},\\Sigma_{i}\\right)$ . Thus, for any set $\\Omega$ , if $(\\mu_{i},\\Sigma_{i})\\in\\Omega$ then $S_{\\Omega}((\\widetilde{\\mu},\\widetilde{\\Sigma}),{\\bf X})=0$ for all $(\\widetilde{\\mu},\\widetilde{\\Sigma})\\approx_{\\gamma,\\tau}(\\mu_{i},\\Sigma$ . The proof follows by a union bound over all $i\\in[k]$ . \u53e3 ", "page_idx": 27}, {"type": "text", "text": "Next, we show that for any dataset $\\mathbf{X}$ , if there is even a single $(\\mu^{*},\\Sigma^{*})$ with low score, there must be a region of $(\\widetilde{\\mu},\\widetilde{\\Sigma})$ which all has low score. ", "page_idx": 27}, {"type": "text", "text": "Proposition E.2. Suppose that $\\begin{array}{r}{t=\\operatorname*{min}_{\\mu^{*},\\Sigma^{*}}S_{\\Omega}((\\mu^{*},\\Sigma^{*}),\\mathbf{X})}\\end{array}$ . Then, there exists some $\\mu,\\Sigma$ such that for all $(\\widetilde{\\mu},\\widetilde{\\Sigma})\\approx_{\\gamma,\\tau}(\\mu,\\Sigma)$ , we have that $S_{\\Omega}((\\widetilde{\\mu},\\widetilde{\\Sigma}),{\\bf X})=t$ . ", "page_idx": 27}, {"type": "text", "text": "Proof. Fix $\\mu^{*},\\Sigma^{*}$ so that $S_{\\Omega}((\\mu^{*},\\Sigma^{*}),\\mathbf{X})=t$ . Then, there is some $(\\mu,\\Sigma)$ and some $\\mathbf{X}_{}^{\\prime},\\mathbf{Y}$ such that $\\mathrm{d_{H}}(\\mathbf{X},\\mathbf{X}^{\\prime})=t.$ , $\\mathbf{Y}\\subset\\mathbf{X}^{\\prime}$ , $|\\mathbf{Y}|=N$ , and $f_{\\Omega}(\\mathbf{Y},\\boldsymbol{\\mu},\\Sigma)=1$ . Thus, for any $(\\widetilde{\\mu},\\widetilde{\\Sigma})\\approx_{\\gamma,\\tau}\\left(\\mu,\\Sigma\\right)$ , by definition, we have that $S_{\\Omega}((\\widetilde{\\mu},\\widetilde{\\Sigma}),\\mathbf{X})\\leq t$ . But since $t$ is the minimum possible score, we in fact have equality: $S_{\\Omega}((\\widetilde{\\mu},\\widetilde{\\Sigma}),{\\bf X})=t$ for all such $(\\widetilde{\\mu},\\widetilde{\\Sigma})$ . \u53e3 ", "page_idx": 27}, {"type": "text", "text": "Next, we want to show that regardless of the dataset (even if not drawn from any Gaussian mixture distribution), the set of points of low score can\u2019t have that much volume. ", "page_idx": 27}, {"type": "text", "text": "Proposition E.3. Fix a dataset $\\mathbf{X}$ of size $n$ . Then, the set of $\\widetilde{\\mu},\\widetilde{\\Sigma}$ such that $S_{\\Omega}((\\widetilde{\\mu},\\widetilde{\\Sigma}),\\mathbf{X})\\leq\\eta^{*}\\cdot n$ can be partitioned into at most $\\binom{n}{m}$ regions $S_{i}$ , which is inde x ed  by some $(\\mu_{i}^{\\prime},\\Sigma_{i}^{\\prime})$ .  Moreover, for all $(\\widetilde{\\mu},\\widetilde{\\Sigma})\\in S_{i},$ , there exists $(\\mu,\\Sigma)$ such that $(\\widetilde{\\mu},\\widetilde{\\Sigma})\\approx_{\\gamma,\\tau}(\\mu,\\Sigma)$ and $\\begin{array}{r}{(\\mu,\\Sigma)\\approx_{8\\gamma,8\\rho,8\\tau}(\\mu_{i}^{\\prime},\\Sigma_{i}^{\\prime}).}\\end{array}$ . ", "page_idx": 27}, {"type": "text", "text": "Proof. Pick any $(\\widetilde{\\mu},\\widetilde{\\Sigma})$ with score at most $\\eta^{*}\\cdot n$ . Let $\\mathbf{X}^{\\prime},\\mathbf{Y}^{\\prime},\\mu,\\Sigma$ be such that $\\mathrm{d}_{\\mathrm{H}}(\\mathbf{X},\\mathbf{X}^{\\prime})\\leq\\boldsymbol{\\eta}^{*}\\cdot\\boldsymbol{n}$ , ${\\bf Y^{\\prime}}\\subset{\\bf X^{\\prime}}$ , $|\\mathbf{Y}^{\\prime}|=N$ , $(\\widetilde{\\mu},\\widetilde{\\Sigma})\\approx_{\\gamma,\\tau}(\\mu,\\Sigma)$ , and $f_{\\Omega}(\\mathbf{Y}^{\\prime},\\boldsymbol{\\mu},\\Sigma)=1$ . If we define $\\mathbf{Y}\\subset\\mathbf{X}$ of size $N$ to be the corresponding su b set as ${\\bf{Y}}^{\\prime}$ is to $\\mathbf{X^{\\prime}}$ , then $\\begin{array}{r}{\\mathrm{d}_{\\mathrm{H}}(\\mathbf{Y},\\mathbf{Y}^{\\prime})\\le\\boldsymbol{\\eta}^{*}\\cdot\\boldsymbol{n}=\\frac{c_{2}}{4}\\cdot N}\\end{array}$ . ", "page_idx": 27}, {"type": "text", "text": "Now, if we take a random subset $\\mathbf{Z}^{\\prime}$ of size $m$ in ${\\bf{Y}}^{\\prime}$ , and look at the corresponding subset $\\mathbf{Z}$ of size $m$ in $\\mathbf{Y}$ , by a Chernoff bound, with at least 0.99 probability $\\begin{array}{r}{\\mathrm{d}_{\\mathrm{H}}({\\bf Z},{\\bf Z}^{\\prime})\\le\\frac{c_{2}}{2}\\cdot m}\\end{array}$ . Moreover, with at least $2/3$ probability over the random subset $\\mathbf{Z}^{\\prime}$ , $\\mathcal{A}(\\mathbf{Z}^{\\prime})\\,\\approx_{\\gamma,\\rho,\\tau}\\,\\left(\\mu,\\Sigma\\right)$ . Therefore, there always exists a subset $\\mathbf{Z}\\subset\\mathbf{X}$ of size $m$ and a set $\\mathbf{Z}^{\\prime}$ of size $m$ such that $\\begin{array}{r}{\\mathrm{{d}_{H}(}\\pmb{\\mathrm{Z}},\\pmb{\\mathrm{Z}}^{\\prime})\\leq\\,\\frac{c_{2}}{2}\\cdot m}\\end{array}$ and $\\boldsymbol{\\mathcal{A}}(\\mathbf{Z}^{\\prime})\\approx_{\\gamma,\\rho,\\tau}\\left(\\mu,\\Sigma\\right)$ . ", "page_idx": 27}, {"type": "text", "text": "Now, for any fixed $\\mathbf{Z}\\subset\\mathbf{X}$ of size $m$ , if we look at any sets $\\mathbf{Z}^{\\prime},\\mathbf{Z}^{\\prime\\prime}$ of size $m$ and Hamming distance at most ${\\textstyle{\\frac{c_{2}}{2}}}\\cdot m$ from $\\mathbf{Z}$ , then $\\mathrm{d}_{\\mathrm{H}}(\\mathbf{Z}^{\\prime},\\mathbf{Z}^{\\prime\\prime})\\le c_{2}\\cdot m$ . So, by Property 1 of Lemma B.6, for every such $\\mathbf{Z}^{\\prime}$ and $\\mathbf{Z}^{\\prime\\prime}$ with $\\mathbf{\\mathcal{A}}(\\mathbf{Z}^{\\prime}),\\mathbf{\\mathcal{A}}(\\mathbf{Z}^{\\prime\\prime})\\neq\\perp$ , we must have $\\boldsymbol{\\mathcal{A}}(\\mathbf{Z}^{\\prime\\prime})\\approx_{\\gamma,\\rho,\\tau}\\boldsymbol{\\mathcal{A}}(\\mathbf{Z}^{\\prime})$ . ", "page_idx": 27}, {"type": "text", "text": "To complete the proof, we order the subsets $\\mathbf{Z}_{1},\\mathbf{Z}_{2},\\ldots,\\mathbf{Z}_{{\\binom{n}{m}}}$ of size $m$ in $\\mathbf{X}$ . For each $i\\leq\\binom{n}{m}$ , if there exists some $\\mathbf{Z}^{\\prime}$ such that $\\begin{array}{r}{\\mathrm{d}_{\\mathrm{H}}(\\mathbf{Z}_{i},\\mathbf{Z}^{\\prime})\\,\\le\\,\\frac{c_{2}}{2}\\,\\cdot\\,m}\\end{array}$ and $\\mathbf{\\mathcal{A}}(\\mathbf{Z}^{\\prime})\\neq\\perp$ , choose an arbitrary such $\\mathbf{Z}^{\\prime}$ and let $(\\mu_{i}^{\\prime},\\Sigma_{i}^{\\prime}):=\\mathcal{A}(\\mathbf{Z}^{\\prime})$ . Otherwise, we do not define $(\\mu_{i}^{\\prime},\\Sigma_{i}^{\\prime})$ . Then, for any $(\\widetilde{\\mu},\\widetilde{\\Sigma})$ of score at most $\\eta^{*}\\cdot n$ , there exists $(\\mu,\\Sigma)$ , a subset $\\mathbf{Z}_{i}\\subset\\mathbf{X}$ of size $m$ , and a set $\\mathbf{Z}_{i}^{\\prime}$ of size $m$ such that $\\begin{array}{r}{(\\widetilde{\\mu},\\widetilde{\\Sigma})\\approx_{\\gamma,\\tau}\\,(\\mu,\\Sigma),\\,\\mathrm{d}_{\\mathrm{H}}(\\mathbf{Z}_{i},\\mathbf{Z}_{i}^{\\prime})\\,\\leq\\,\\frac{c_{2}}{2}\\cdot m}\\end{array}$ , and $\\mathcal{A}(\\mathbf{Z}_{i}^{\\prime})\\approx_{\\gamma,\\rho,\\tau}\\left(\\mu,\\Sigma\\right)$ . Because $A(\\mathbf{Z}_{i}^{\\prime})\\neq\\perp$ and $\\mathbf{Z}_{i}^{\\prime}$ h as  Hamming distance at most ${\\frac{c_{2}}{2}}\\cdot m$ from $\\mathbf{Z}_{i}$ , this also implies that $\\mathscr{A}(\\mathbf{Z}_{i}^{\\prime})\\approx_{\\gamma,\\rho,\\tau}\\ (\\mu_{i}^{\\prime},\\Sigma_{i}^{\\prime})$ . By Proposition B.2, this means that $(\\bar{\\mu},\\Sigma)\\approx_{8\\gamma,8\\rho,8\\tau}\\,(\\mu_{i}^{\\prime},\\Sigma_{i}^{\\prime})$ , which completes the proof. \u53e3 ", "page_idx": 28}, {"type": "text", "text": "Proposition E.4. Suppose $\\mathbf{X}\\,=\\,\\{X_{1},\\ldots,X_{n}\\}$ is drawn from a Gaussian mixture model, with representation $\\{(w_{i},\\mu_{i},\\Sigma_{i})\\}_{i=1}^{k}$ . Then, with probability at least $1-O(\\beta_{0})$ over the randomness of $\\mathbf{X}$ , for any set $\\Omega$ of mean-covariance pairs, and for every $(\\widetilde{\\mu},\\widetilde{\\Sigma})$ with $\\begin{array}{r}{S_{\\Omega}((\\widetilde{\\mu},\\widetilde{\\Sigma}),{\\bf X})\\le\\frac{{\\cal N}}{2}}\\end{array}$ , there exists an index $i\\in[k]$ such that $\\begin{array}{r}{\\frac{1}{O(n^{8})}\\cdot\\Sigma_{i}\\prec\\widetilde\\Sigma\\prec O(n^{8})\\cdot\\Sigma_{i}}\\end{array}$ and $\\|\\Sigma_{i}^{-1/2}(\\widetilde{\\mu}-\\mu_{i})\\|_{2}\\leq O(n^{6})$ . ", "page_idx": 28}, {"type": "text", "text": "Proof. We apply Part 3 of Lemma B.6, though we use $N$ to replace the value $m$ in Lemma B.6. We are assuming $N\\geq40d\\cdot k$ . So, by definition of score, $(\\widetilde{\\mu},\\widetilde{\\Sigma})\\approx_{\\gamma,\\tau}(\\mu,\\Sigma)$ where $f_{\\Omega}(\\mathbf{Y}^{\\prime},{\\boldsymbol{\\mu}},\\Sigma)=1$ , for some ${\\bf Y^{\\prime}}$ which can be generated by taking a subset o f $\\mathbf{X}$ of size at most $N$ and altering at most $N/2$ elements. Since $f_{\\Omega}(\\mathbf{Y}^{\\prime},\\boldsymbol{\\mu},\\Sigma)=1$ , this means that if $(\\hat{\\mu},\\hat{\\Sigma})=A(\\mathbf{Y}^{\\prime})$ then $(\\hat{\\mu},\\hat{\\Sigma})\\approx_{\\gamma,\\rho,\\tau}(\\mu,\\Sigma)$ . So, by Proposition B.2, $(\\widetilde{\\mu},\\widetilde{\\Sigma})\\approx_{8\\gamma,8\\tau}\\,(\\hat{\\mu},\\hat{\\Sigma})$ . Assuming $\\gamma=\\tau=c_{2}$ is sufficiently small, this means that $(\\widetilde{\\mu},\\widetilde{\\Sigma})\\approx_{1/2,1/2}(\\hat{\\mu},\\hat{\\Sigma})$ . ", "page_idx": 28}, {"type": "text", "text": "By Part 3 of Lemma B.6, with probability at least $1\\!-\\!O(\\beta_{0})$ , there exists $i\\in[k]$ such that $\\frac{1}{O(n^{8})}\\cdot\\Sigma_{i}\\preccurlyeq$ $\\hat{\\Sigma}\\preccurlyeq O(n^{8})\\cdot\\Sigma_{i}$ and $\\|\\Sigma_{i}^{-1/2}(\\hat{\\mu}-\\mu_{i})\\|_{2}\\leq O(n^{6})$ . However, we know that ${\\textstyle\\frac{1}{2}}\\hat{\\Sigma}\\preccurlyeq\\widetilde{\\Sigma}\\preccurlyeq2\\hat{\\Sigma}$ , which means that ${\\frac{1}{O(n^{8})}}\\cdot\\Sigma_{i}\\prec\\widetilde\\Sigma\\prec O(n^{8})\\cdot\\Sigma_{i}$ . Moreover, $\\|\\hat{\\Sigma}^{-1/2}(\\hat{\\mu}\\!-\\!\\widetilde{\\mu})\\|_{2}\\leq0.5.$ , so $\\|{\\boldsymbol{\\Sigma}}_{i}^{-1/2}(\\hat{\\mu}\\!-\\!\\widetilde{\\mu})\\|_{2}\\leq$ $O(n^{4})$ . Thus, by triangle inequality, we have that $\\|\\Sigma_{i}^{-1/2}(\\widetilde{\\mu}-\\mu_{i})\\|_{2}\\leq O(n^{6})$ . \u53e3 ", "page_idx": 28}, {"type": "text", "text": "Now, given a set $\\mathbf{X}=\\{X_{1},\\ldots,X_{n}\\}$ drawn from a GMM with representation $\\{(w_{i},\\mu_{i},\\Sigma_{i})\\}_{i=1}^{k}$ , we say $\\mathbf{X}$ is regular if it satisfies Propositions E.1 and E.4. In other words: ", "page_idx": 28}, {"type": "text", "text": "1. for any set $\\Omega$ of mean-covariance pairs, for all $i\\in[k]$ such that $(\\mu_{i},\\Sigma_{i})\\in\\Omega$ and $w_{i}\\geq\\alpha/k$ , and for all $(\\widetilde{\\mu},\\widetilde{\\Sigma})\\approx_{\\gamma,\\tau}(\\mu_{i},\\Sigma_{i}),S_{\\Omega}((\\widetilde{\\mu},\\widetilde{\\Sigma}),{\\bf X})=0$ .   \n2. for any set $\\Omega$ of mean-covariance pairs, and for every $(\\widetilde{\\mu},\\widetilde{\\Sigma})$ with $\\begin{array}{r}{S_{\\Omega}((\\widetilde{\\mu},\\widetilde{\\Sigma}),\\mathbf{X})\\le\\frac{N}{2}}\\end{array}$ , there exists an index $i\\in[k]$ such that $\\begin{array}{r}{\\frac{1}{O(n^{8})}\\cdot\\Sigma_{i}\\prec\\widetilde\\Sigma\\prec O(n^{8})\\cdot\\Sigma_{i}}\\end{array}$ and $\\|{\\boldsymbol{\\Sigma}}_{i}^{-1/2}({\\widetilde{\\mu}}-{\\boldsymbol{\\mu}}_{i})\\|_{2}\\leq$ $O(n^{6})$ . ", "page_idx": 28}, {"type": "text", "text": "When we say $\\mathbf{X}$ is regular, the components $\\left(\\mu_{i},\\Sigma_{i}\\right)$ and weights $w_{i}$ are implicit. ", "page_idx": 28}, {"type": "text", "text": "We now show that every step of the crude approximation (Lines 8\u201316 in Algorithm 2) will, with $1\\!-\\!O(\\beta_{0})$ probability, find some crude approximation $(\\hat{\\mu}_{j},\\hat{\\Sigma}_{j})$ to some $\\left(\\mu_{i},\\Sigma_{i}\\right)$ , as long as $(\\mu_{i},\\Sigma_{i})\\in$ $\\Omega$ . ", "page_idx": 28}, {"type": "text", "text": "Lemma E.5. For any $\\Omega\\subset\\Theta$ , the corresponding algorithm $\\mathcal{M}$ (which depends on $\\Omega$ ) is $(\\varepsilon_{0},\\delta_{0}){\\cdot}D P$ on datasets $\\mathbf{X}$ . ", "page_idx": 28}, {"type": "text", "text": "Suppose additionally that $\\mathbf{X}$ is regular, and that there exists $i\\in[k]$ such that $\\begin{array}{r}{w_{i}\\geq\\frac{\\alpha}{k}}\\end{array}$ and $(\\mu_{i},\\Sigma_{i})\\in$ $\\Omega$ . Then, with $1-O(\\beta_{0})$ probability (just over the mechanism $\\mathcal{M}$ ), $\\mathcal{M}(\\mathbf{X})=(\\widetilde{\\mu},\\Sigma)$ and ", "page_idx": 28}, {"type": "text", "text": "\u2022 There exists $i\\in[k]$ such that ${\\frac{1}{O(n^{8})}}\\cdot\\Sigma_{i}\\prec\\widetilde\\Sigma\\prec O(n^{8})\\cdot\\Sigma_{i}$ and $\\|\\Sigma_{i}^{-1/2}(\\widetilde{\\mu}-\\mu_{i})\\|_{2}\\leq O(n^{6}).$ .   \n\u2022 There exists $(\\mu,\\Sigma)\\in\\Omega$ such that $(\\widetilde{\\mu},\\widetilde{\\Sigma})\\approx_{\\gamma,\\tau}(\\mu,\\Sigma)$ . ", "page_idx": 28}, {"type": "text", "text": "Proof. We will apply Theorem C.3. We first need to check the necessary conditions. It follows almost immediately from the definition of $S_{\\Omega}$ that $\\ensuremath{S_{\\Omega}}((\\widetilde{\\mu},\\widetilde{\\Sigma}),\\cdot)$ has the bounded sensitivity property with respect to neighboring datasets, for any fixed $\\Omega,{\\widetilde{\\mu}},{\\widetilde{\\Sigma}}$ . ", "page_idx": 28}, {"type": "text", "text": "To check the volume condition, note that for any dataset $\\mathbf{X}$ of size $n$ , if $\\mathrm{min}_{\\mu,\\Sigma}\\,S_{\\Omega}((\\mu,\\Sigma),{\\bf X})\\leq$ $0.7\\eta^{*}n$ , then by Proposition E.2, there exists $\\mu,\\Sigma$ such that $S_{\\Omega}((\\widetilde{\\mu},\\widetilde{\\Sigma}),{\\bf X})~\\le~0.8\\eta^{*}n$ for all $(\\widetilde{\\mu},\\widetilde{\\Sigma})\\approx_{\\gamma,\\tau}(\\mu,\\Sigma)$ . Conversely, for any $\\mathbf{X}$ , by Proposition E.3, the set of $(\\widetilde{\\mu},\\widetilde{\\Sigma})$ of score at most $\\eta^{*}n$ can be partitioned into $\\binom{n}{m}$ regions $S_{i}$ , indexed by $(\\mu_{i}^{\\prime},\\Sigma_{i}^{\\prime})$ . Moreover, each $S_{i}$ is contained in the set of $(\\widetilde{\\mu},\\widetilde{\\Sigma})$ such that there exists $(\\mu,\\Sigma)$ such that $\\begin{array}{r}{(\\widetilde{\\mu},\\widetilde{\\Sigma})\\approx_{\\gamma,\\tau}(\\mu,\\Sigma)\\approx_{8\\gamma,8\\rho,8\\tau}(\\mu_{i}^{\\prime},\\Sigma_{i}^{\\prime}).}\\end{array}$ ", "page_idx": 29}, {"type": "text", "text": "Let us use the notation of Lemma C.8, where $\\gamma_{1}=\\gamma$ , $\\gamma_{2}=8\\gamma$ , and $\\rho_{2}=8\\rho$ . Then, we have that the set of points with score at most $0.8\\eta^{*}n$ contains $\\mathcal{T}_{1}(\\mu,\\Sigma)$ for some $(\\mu,\\Sigma)$ , but the set of points with score at most $\\eta^{*}n$ is contained in the union of $\\mathcal{T}_{2}(\\mu_{i}^{\\prime},\\Sigma_{i}^{\\prime})$ for at most $\\binom{n}{m}$ choices of $(\\mu_{i}^{\\prime},\\Sigma_{i}^{\\prime})$ . So, as long as $\\begin{array}{r}{\\operatorname*{min}_{\\mu,\\Sigma}S_{\\Omega}((\\mu,\\Sigma),\\mathbf{X})\\leq0.7\\eta^{*}n}\\end{array}$ , we can apply Lemma C.8 to obtain ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\frac{V_{\\eta^{*}}(\\mathbf{X})}{V_{0.8\\eta^{*}}(\\mathbf{X})}\\leq{\\binom{n}{m}}\\cdot\\exp\\left(O\\left(\\frac{d^{5/3}\\cdot\\log d\\cdot\\rho_{2}^{2/3}}{\\gamma_{1}^{2}}\\right)\\right)=\\exp\\left(O\\left(d^{7/4}\\log d+m\\log n\\right)\\right),\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "since $\\rho_{2}=8\\rho=8c_{2}\\cdot d^{1/8}$ and $\\gamma_{1}=\\gamma=c_{2}$ , and since $c_{2}$ is a constant. Thus, if ", "page_idx": 29}, {"type": "equation", "text": "$$\nn\\geq C\\cdot\\frac{O(d^{7/4}\\log d+m\\log n)+\\log(1/\\delta_{0})}{\\varepsilon_{0}\\cdot\\eta^{*}},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "we have that $\\mathcal{M}$ is $(\\varepsilon_{0},\\delta_{0})$ -DP, by Theorem C.3. This holds by our parameter settings, assuming $K$ is sufficiently large. ", "page_idx": 29}, {"type": "text", "text": "Next, we prove accuracy. Assume that $\\mathbf{X}$ is regular. We again adopt the notation of Lemma C.8, (with $\\gamma_{1}=\\gamma$ , $\\gamma_{2}=8\\gamma$ , and $\\rho_{2}=8\\rho$ ). By the first condition of regularity, for all $i\\in[k]$ such that $(\\mu_{i},\\Sigma_{i})\\in\\Omega$ and $w_{i}\\geq\\alpha/k$ , every $(\\widetilde{\\mu},\\widetilde{\\Sigma})\\in\\mathcal{T}_{1}(\\mu_{i},\\Sigma_{i})$ satisfies $S_{\\Omega}((\\widetilde{\\mu},\\widetilde{\\Sigma}),{\\bf X})=0$ . We still have that the set of points of score at most $\\eta^{*}n$ is contained in the union of at most $\\binom{n}{m}$ sets $\\mathcal{T}_{2}(\\mu_{i}^{\\prime},\\Sigma_{i}^{\\prime})$ . Thus, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\frac{V_{\\eta^{*}}(\\mathbf{X})}{V_{\\eta}(\\mathbf{X})}\\leq\\exp\\left(O\\left(d^{7/4}\\log d+m\\log n\\right)\\right),\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where we recall that we set $\\begin{array}{r}{\\eta=\\frac{\\eta^{*}}{10}}\\end{array}$ . So, by Theorem C.3, as long as ", "page_idx": 29}, {"type": "equation", "text": "$$\nn\\geq C\\cdot\\frac{O(d^{7/4}\\log d+m\\log n)+\\log(1/(\\beta_{0}\\cdot\\eta))}{\\varepsilon_{0}\\cdot\\eta},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "which indeed holds, $\\mathcal{M}(\\mathbf{X})$ outputs some $(\\widetilde{\\mu},\\widetilde{\\Sigma})$ of score at most $2\\eta n$ , with $1-\\beta_{0}$ probability. By the second condition of regularity\u201e there exists an index $i\\in[k]$ such that $\\begin{array}{r}{\\frac{1}{O(n^{8})}\\cdot\\Sigma_{i}\\prec\\widetilde\\Sigma\\prec O(n^{8})\\cdot\\Sigma_{i}}\\end{array}$ and $\\|\\Sigma_{i}^{-1/2}(\\widetilde{\\mu}-\\mu_{i})\\|_{2}\\leq O(n^{6})$ . Moreover, because $(\\widetilde{\\mu},\\widetilde{\\Sigma})$ has finite score, $(\\widetilde{\\mu},\\widetilde{\\Sigma})\\approx_{\\gamma,\\tau}(\\mu,\\Sigma)$ for some $(\\mu,\\Sigma)\\in\\Omega$ . ", "page_idx": 29}, {"type": "text", "text": "Lemma E.6. Lines 7\u201317 of Algorithm 2 are $(\\varepsilon,\\delta)$ -DP. Moreover, for every regular set $\\mathbf{X}$ , with probability at least $1-O(k\\cdot\\beta_{0})$ over the randomness of Algorithm 2, for every $i\\in[k]$ such that $\\begin{array}{r}{w_{i}\\geq\\frac{\\alpha}{k}}\\end{array}$ , there exists $j$ such that $n^{-12}\\cdot\\hat{\\Sigma}_{j}\\precneq\\Sigma_{i}\\preccurlyeq n^{12}\\cdot\\hat{\\Sigma}_{j}$ and $\\|\\hat{\\Sigma}_{j}^{-1/2}(\\mu-\\hat{\\mu}_{j})\\|_{2}\\leq n^{12}$ . ", "page_idx": 29}, {"type": "text", "text": "Proof. The privacy guarantee is immediate by (adaptive) advanced composition. Indeed, the algorithm $\\mathcal{M}$ at each step is $(\\varepsilon_{0},\\delta_{0})$ -DP, and only depends on $\\mathbf{X}$ and $\\Omega$ , which is determined only by the output of all previous runs of $\\mathcal{M}$ . ", "page_idx": 29}, {"type": "text", "text": "Now, let\u2019s say that $\\mathbf{X}=\\{X_{1},\\ldots,X_{n}\\}$ is regular, and we run Algorithm 2 on $\\mathbf{X}$ . Now, after $j\\geq0$ steps of the loop, define $P_{j}$ to be the set of indices $i\\in[k]$ such that $\\begin{array}{r}{w_{i}\\geq\\frac{\\alpha}{k}}\\end{array}$ and $(\\mu_{i},\\Sigma_{i})\\in\\Omega_{j}$ (where $\\Omega_{j}$ refers to the set $\\Omega$ after $j$ steps of the loop are completed). Likewise, define $Q_{j}$ to be the set of indices $i$ such that there exists $(\\hat{\\mu},\\hat{\\Sigma})\\;\\in\\;\\Omega_{j}$ with $\\begin{array}{r}{\\frac{1}{n^{10}}\\,\\cdot\\,\\Sigma_{i}\\;\\prec\\;\\hat{\\Sigma}\\;\\prec\\;n^{10}\\,\\cdot\\,\\Sigma_{i}}\\end{array}$ and $\\|{\\boldsymbol{\\Sigma}}_{i}^{-1/2}(\\hat{\\mu}-\\mu_{i})\\|_{2}\\leq n^{10}$ . Note that $P_{0}\\subset Q_{0}=[k]$ , and that $P_{j}\\subset Q_{j}$ always. Moreover, because $\\Omega$ only shrinks, $P_{j+1}\\subset P_{j}$ and $Q_{j+1}\\subset Q_{j}$ always. ", "page_idx": 29}, {"type": "text", "text": "Now, after some step $j<k$ , we claim that if $P_{j}\\neq\\emptyset$ , then $|Q_{j+1}|\\leq|Q_{j}|-1$ with failure probability at most $O(\\beta_{0})$ , i.e., $Q$ decreases in size for the next step if $P$ isn\u2019t currently empty. The failure probability is only over the randomness of $\\mathcal{M}$ at each step, and holds for any regular dataset $\\mathbf{X}$ . A union bound says the total failure probability is $O(k\\cdot\\beta_{0})$ . ", "page_idx": 29}, {"type": "text", "text": "To see why this holds, note that if some index $i\\;\\in\\;P_{j}$ , then $(\\mu_{i},\\Sigma_{i})\\;\\in\\;\\Omega_{j}$ . So, we can apply Lemma E.5. At step $j+1$ , we find some $(\\hat{\\mu}_{j+1},\\hat{\\Sigma}_{j+1})$ , with the following properties. First, there exists an index $i^{\\prime}\\in[k]$ with $\\begin{array}{r}{\\frac{1}{O(n^{8})}\\cdot\\Sigma_{i^{\\prime}}\\prec\\hat{\\Sigma}_{j+1}\\preccurlyeq O(n^{8})\\cdot\\Sigma_{i^{\\prime}}}\\end{array}$ and $\\|\\Sigma_{i^{\\prime}}^{-1/2}(\\hat{\\mu}_{j+1}-\\mu_{i^{\\prime}})\\|_{2}\\leq O(n^{6})$ . Second, $\\begin{array}{r}{(\\hat{\\mu}_{j+1},\\hat{\\Sigma}_{j+1})\\approx_{\\gamma,\\tau}(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma})}\\end{array}$ , for some $(\\mu,\\Sigma)\\in\\Omega_{j}$ . ", "page_idx": 30}, {"type": "text", "text": "We claim this means $i^{\\prime}\\in Q_{j}$ . To see why, it suffices to show that $\\begin{array}{r}{\\frac{1}{n^{10}}\\cdot\\Sigma_{i^{\\prime}}\\preccurlyeq\\Sigma\\preccurlyeq n^{10}\\cdot\\Sigma_{i^{\\prime}}}\\end{array}$ and $\\|{\\boldsymbol{\\Sigma}}_{i^{\\prime}}^{-1/2}({\\boldsymbol{\\mu}}-{\\boldsymbol{\\mu}}_{i^{\\prime}})\\|_{2}\\leq n^{10}$ eafnindi $Q_{j}$ ,e $(\\mu,\\Sigma)\\in\\Omega_{j}$ s. . O(1n8) \u00b7\u03a3i\u2032 \u227c\u03a3\u02c6j+1 \u227cO(n8)\u00b7\u03a3i\u2032  12 \u03a3\u02c6j $\\begin{array}{r}{\\frac12\\hat{\\Sigma}_{j+1}\\preccurlyeq\\Sigma\\preccurlyeq2\\hat{\\Sigma}_{j+1}}\\end{array}$ ${\\frac{1}{n^{10}}}\\cdot\\Sigma_{i^{\\prime}}\\preccurlyeq\\Sigma\\preccurlyeq n^{10}\\cdot\\Sigma_{i^{\\prime}}$ Also, we know that $\\|\\Sigma_{i^{\\prime}}^{-1/2}(\\hat{\\mu}_{j+1}-\\mu_{i^{\\prime}})\\|_{2}\\leq O(n^{6})$ and $\\|\\Sigma^{-1/2}(\\mu-\\hat{\\mu}_{j+1})\\|_{2}\\leq\\tau\\leq1$ The latter inequality along with the fact that $\\Sigma\\preccurlyeq n^{10}\\cdot\\Sigma_{i^{\\prime}}$ implies that $\\|\\Sigma_{i^{\\prime}}^{-1/2}(\\mu-\\hat{\\mu}_{j+1})\\|_{2}\\leq n^{5}$ . So, by triangle inequality, $\\|\\Sigma_{i^{\\prime}}^{-1/2}(\\mu-\\mu_{i^{\\prime}})\\|_{2}\\leq O(n^{6})\\leq n^{10}$ . ", "page_idx": 30}, {"type": "text", "text": "Next, we claim that $i^{\\prime}\\notin P_{j+1}$ , $\\mathrm{so\\}i^{\\prime}\\not\\in Q_{j+1}$ . Indeed, we have that $\\begin{array}{r}{\\frac{1}{O(n^{8})}\\cdot\\Sigma_{i^{\\prime}}\\prec\\hat{\\Sigma}_{j+1}\\preccurlyeq O(n^{8})\\cdot\\Sigma_{i^{\\prime}}}\\end{array}$ , which immediately means $\\begin{array}{r}{\\frac{1}{n^{10}}\\cdot\\hat{\\Sigma}_{j+1}\\prec\\frac{1}{O(n^{8})}\\cdot\\hat{\\Sigma}_{j+1}\\prec\\Sigma_{i^{\\prime}}\\prec O(n^{8})\\cdot\\hat{\\Sigma}_{j+1}\\preccurlyeq n^{10}\\cdot\\hat{\\Sigma}_{j+1}}\\end{array}$ . Also, $\\|\\Sigma_{i^{\\prime}}^{-1/2}(\\hat{\\mu}_{j+1}-\\mu_{i^{\\prime}})\\|_{2}\\leq O(n^{6})$ , and since $\\Sigma_{i^{\\prime}}\\precsim n^{10}\\cdot\\hat{\\Sigma}_{j+1}$ , this means $\\|\\hat{\\Sigma}_{j+1}^{-1/2}(\\mu_{i^{\\prime}}-\\hat{\\mu}_{j+1})\\|_{2}\\leq$ $O(n^{11})\\leq n^{12}$ . Thus, the algorithm will remove $(\\mu_{i^{\\prime}},\\Sigma_{i^{\\prime}})$ from $\\Omega$ at step $j+1$ , so $i^{\\prime}\\not\\in Q_{j+1}$ . ", "page_idx": 30}, {"type": "text", "text": "Thus, either $P_{j}$ is empty, or $Q_{j}$ decreases in size to $Q_{j+1}$ for each $0\\leq j\\leq k+1$ . This implies that $P_{k}$ is empty, i.e., for every $i\\in[k]$ such that $\\begin{array}{r}{w_{i}\\geq\\frac{\\alpha}{k}}\\end{array}$ , $(\\mu_{i},\\Sigma_{i})\\not\\in\\Omega_{j}$ . This can only happen if each such $(\\mu_{i},\\Sigma_{i})$ was removed at some point. So, for every such $i$ , there was some index $j$ such that $n^{-12}\\cdot\\hat{\\Sigma}_{j}\\precneq\\Sigma_{i}\\preccurlyeq n^{12}\\cdot\\hat{\\Sigma}_{j}$ and $\\|\\hat{\\Sigma}_{j}^{-1/2}(\\mu-\\hat{\\mu}_{j})\\|_{2}\\leq n^{12}$ . \u53e3 ", "page_idx": 30}, {"type": "text", "text": "E.3 Summary and Completion of Analysis ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "We first quickly summarize how to put everything together to prove Theorem 1.4. To prove our main result, need to ensure that our algorithm is private, uses few enough samples, and accurately learns the mixture. Privacy will be simple, as we have shown in Lemma E.6 that the crude approximation is private, and the hypothesis selection is private as shown in Appendix D. The sample complexity will come from the settings of $n$ and $n^{\\prime}$ in lines 3 and 18 of the algorithm, and from our setting of parameters. Finally, we showed in Lemma E.6 that we have found a set of $(\\hat{\\mu}_{j},\\hat{\\Sigma}_{j})$ , of at most $k$ meancovariance pairs, such that every true $\\left(\\mu_{i},\\Sigma_{i}\\right)$ of sufficiently large weight is crudely approximated by some $(\\hat{\\mu}_{j},\\hat{\\Sigma}_{j})$ . Indeed, this is exactly the situation for which we can apply the result on private hypothesis selection (Lemma D.3, based on [BSKW19]). ", "page_idx": 30}, {"type": "text", "text": "We are now ready to complete the analysis and prove Theorem 1.4. ", "page_idx": 30}, {"type": "text", "text": "Proof of Theorem 1.4. By Lemma E.6, note that the algorithm up to Line 17 is $(\\varepsilon,\\delta)$ -DP with respect to $X_{1},\\ldots,X_{n}$ , and does not depend on $X_{n+1},\\ldots,X_{n+n^{\\prime}}$ . Assuming $\\{(\\hat{\\mu}_{j},\\hat{\\Sigma}_{j})\\}$ from these lines is fixed, Lines 18\u201320 are $(\\varepsilon,\\delta)$ -DP with respect to $X_{n+1},\\ldots,X_{n+n^{\\prime}}$ , by Lemma D.3, and do not depend on $X_{1},\\ldots,X_{n}$ . So, the overall algorithm is $(\\varepsilon,\\delta)$ -DP. ", "page_idx": 30}, {"type": "text", "text": "Next, we verify accuracy. Note that by Lemma E.6, and for $G=n^{12}$ , the sets $(\\hat{\\mu}_{j},\\hat{\\Sigma}_{j})$ that we find satisfy the conditions for Lemma D.3, with failure probability at most $O(k\\cdot\\beta_{0})$ . This probability is at most 0.1, assuming $e^{d}$ is significantly larger than $k$ . So, it su\u221affices for $n^{\\prime}$ , the n\u221aumber of samples used in Line 18 of Algorithm 2, to satisfy $\\begin{array}{r}{n^{\\prime}\\geq O\\left(\\frac{d^{2}k\\cdot\\log(G\\cdot k\\sqrt{d}/\\alpha)}{\\alpha^{2}}+\\frac{d^{2}k\\cdot\\log(G\\cdot k\\sqrt{d}/\\alpha)}{\\alpha\\varepsilon}\\right)=}\\end{array}$ $\\begin{array}{r}{\\widetilde{O}\\left(\\frac{d^{2}k}{\\alpha^{2}}+\\frac{d^{2}k}{\\alpha\\varepsilon}\\right)}\\end{array}$ , where the last part of the bound holds by our assumptions on $G$ and $n$ . Thus, the total sample complexity is ", "page_idx": 30}, {"type": "equation", "text": "$$\nn+n^{\\prime}=\\widetilde{O}\\left(\\frac{k d^{2}}{\\alpha^{2}}+\\frac{k d^{2}+d^{1.75}k^{1.5}\\log^{0.5}(1/\\delta)+k^{1.5}\\log^{1.5}(1/\\delta)}{\\alpha\\varepsilon}+\\frac{k^{2}d}{\\alpha}\\right).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "By Lemma D.3, with failure probability at most 0.1, Lines 18\u201320 of the algorithm will successfully output a hypothesis which is a mixture of $k$ Gaussians, with total variation distance at most $O(\\alpha)$ from the right answer. So, the overall success probability is at least 0.8. ", "page_idx": 30}, {"type": "text", "text": "Finally, we note that the assumption that $e^{d}$ is much larger than $k$ can be made WLOG, by padding $d$ to be a sufficiently large multiple of $\\log k$ . Namely, if given samples $X_{i}=(X_{i,1},\\ldots,\\dot{X_{i,d}})$ , we can add coordinates $X_{i,d+1},\\ldots,X_{O(\\log k)}\\sim{\\mathcal{N}}(0,1)$ . Then, if the original samples were a mixture of $k$ Gaussians $\\mathcal{N}(\\mu_{i},\\Sigma_{i})$ , the new distribution is a mixture of $k$ Gaussians $N\\left(\\left({\\mu_{i}\\atop0}\\right),\\left({\\Sigma_{i}\\atop0}\\quad0\\right)\\right)$ , with the same mixing weights. So, we can learn the mixture distribution up to total variation distance $\\alpha$ in the larger space, and then remove all except the first $d$ coordinates, to output our final answer. This will not affect the sample complexity by more than a poly $(\\log k)$ factor. \u53e3 ", "page_idx": 31}, {"type": "text", "text": "F The Univariate Setting ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "In this section, we provide the algorithm and analysis for Theorem 1.5. We first describe and provide pseudocode for the algorithm, and then we prove that it can privately learn mixtures of Gaussians with low sample complexity. ", "page_idx": 31}, {"type": "text", "text": "We will use $\\sigma_{i}:=\\sqrt{\\Sigma_{i}}$ to denote the standard deviation of a univariate Gaussian $\\mathcal{N}(\\mu_{i},\\Sigma_{i})$ . ", "page_idx": 31}, {"type": "text", "text": "F.1 Algorithm ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "As in the algorithm in Appendix $\\boldsymbol{\\mathrm E}$ , we will actually draw a total of $n+n^{\\prime}$ samples. We start off by only considering the first $n$ data points $\\mathbf{X}\\,=\\,\\{X_{1},\\ldots,X_{n}\\}$ , where each $X_{j}\\,\\in\\,\\mathbb{R}$ . Now, let $\\bar{\\mathbf Y}=\\{Y_{1},\\ldots,Y_{n}\\}$ be the sorted version of $\\mathbf{X}$ , i.e., $Y_{1}\\leq Y_{2}\\leq\\,\\cdot\\,\\cdot\\,\\leq\\,Y_{n}$ , and $({\\bar{X}}_{1},\\cdot\\cdot\\cdot,X_{n})$ and $\\left(Y_{1},\\ldots,Y_{n}\\right)$ are the same up to permutation. Finally, for each $j\\leq n-1$ , define $Z_{j}$ to be the ordered pair $(Y_{j},Y_{j+1}-Y_{j})$ , and define $\\mathbf{Z}\\,=\\,\\mathbf{Z}(\\mathbf{X})$ to be the unordered multiset of $Z_{j}$ \u2019s. (Note that $\\mathbf{Z}$ depends deterministically on $\\mathbf{X}$ .) ", "page_idx": 31}, {"type": "text", "text": "The algorithm now works as follows. Suppose we have data $\\mathbf{X}=\\{X_{1},\\ldots,X_{n}\\}$ . After sorting to obtain $Y_{1},\\ldots,Y_{n}$ , let $r_{j}=Y_{j},s_{j}=Y_{j+1}-Y_{j}$ , and $Z_{j}=(r_{j},s_{j})$ . So, $\\mathbf{Z}(\\mathbf{X})=\\{(r_{j},s_{j})\\}_{1\\leq j\\leq n-1}$ , viewed as an unordered set. Note that every $s_{j}\\geq0$ , and the $r_{j}$ \u2019s are in nondecreasing order. We will create a set of buckets that can be bijected onto $\\ensuremath{\\mathbb{Z}}^{2}$ . For each $Z_{j}=(r_{j},s_{j})$ , if $s_{j}>0$ we assign $Z_{j}$ to the bucket labeled $(a,b)\\in\\mathbb{Z}^{2}$ if $2^{a}\\leq s_{j}<2^{a+1}$ and $b\\cdot n^{5}\\cdot2^{a}\\leq r_{j}<(b+1)\\cdot n^{5}\\cdot2^{a}$ . If $s_{j}=0$ we do not assign $Z_{j}$ to any bucket. ", "page_idx": 31}, {"type": "text", "text": "For each element $e\\in\\mathbb{Z}^{2}$ , we keep track of the number of indices $j\\in[n-1]$ with $Z_{j}$ sent to $e$ . In other words, for $e=(a,b)$ , we define the count $c_{e}=\\#\\{j:2^{a}\\leq s_{j}<2^{a+1},b\\cdot n^{5}\\cdot2^{a}\\leq r_{j}<$ $(b+1)\\cdot n^{5}\\cdot2^{a}\\}$ . For each $c_{e}$ , we sample an independent draw $g_{e}\\sim\\mathrm{TLap}(1,\\varepsilon/10,\\delta/10)$ , and define $\\tilde{c}_{e}=c_{e}+g_{e}$ . Finally, we let ${\\cal S}=\\{(\\hat{\\mu}_{i},\\hat{\\Sigma}_{i})\\}$ be the the set of pairs $(b\\cdot n^{5}\\cdot2^{a},2^{2a})$ where $e=(a,b)$ satisfies $\\begin{array}{r}{\\tilde{c}_{e}>\\frac{100}{\\varepsilon}\\log\\frac{1}{\\delta}}\\end{array}$ . ", "page_idx": 31}, {"type": "text", "text": "Hence, we have computed some $\\{(\\hat{\\mu}_{i},\\hat{\\Sigma}_{i})\\}_{i=1}^{k^{\\prime}}$ , for some $k^{\\prime}\\ge0$ . (We will show in the analysis that $k^{\\prime}\\leq n-1$ always, so $k^{\\prime}$ is finite.) If $k^{\\prime}=0$ we simply output $\\bot$ . Otherwise, we will draw $\\tilde{O}\\left(\\frac{k}{\\alpha^{2}}+\\frac{k}{\\alpha\\varepsilon}\\right)$ fresh samples. We run the $\\varepsilon$ -DP hypothesis selection based algorithm, Algorithm 1 on the fresh samples, using parameters $G=n^{10}$ and $\\{(\\hat{\\mu}_{i},\\hat{\\Sigma}_{i})\\}_{i=1}^{k^{\\prime}}$ . ", "page_idx": 31}, {"type": "text", "text": "Pseudocode. We give pseudocode for the algorithm in Algorithm 3. ", "page_idx": 31}, {"type": "text", "text": "F.2 Analysis ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "We start by analyzing Lines 1\u201319, which generates the set ${\\cal S}\\;=\\;\\{(\\hat{\\mu}_{i},\\hat{\\Sigma}_{i})\\}$ of candidate meancovariance pairs. ", "page_idx": 31}, {"type": "text", "text": "First, we note a very basic proposition. ", "page_idx": 31}, {"type": "text", "text": "Proposition F.1. Let $\\varepsilon,\\delta\\leq1$ . Then, with probability 1, $\\mathrm{TLap}(1,\\varepsilon/10,\\delta/10)$ is at most $\\frac{100}{\\varepsilon}\\cdot\\log\\frac{1}{\\delta}$ in absolute value. ", "page_idx": 31}, {"type": "text", "text": "Pwritoho fp.r oBbya bdileitfyin i1ti. oFno ro $\\varepsilon,\\delta\\leq1$ ,t tehdi s Lias plleascs et,h $\\begin{array}{r}{|\\operatorname{TLap}(1,\\varepsilon/10,\\delta/10)|\\,\\le\\,\\frac{1}{(\\varepsilon/10)}\\cdot\\log\\Big(1+\\frac{e^{\\varepsilon/10}-1}{2\\delta/10}\\Big)}\\end{array}$ $\\frac{100}{\\varepsilon}\\cdot\\log\\frac{1}{\\delta}$ ", "page_idx": 31}, {"type": "text", "text": "Input: Samples $X_{1},\\ldots,X_{n}$ , $X_{n+1},\\ldots,X_{n+n^{\\prime}}$ . Output: An $(\\varepsilon,\\delta)$ -DP prediction $\\{(\\widetilde{\\mu}_{i},\\widetilde{\\Sigma}_{i}),\\widetilde{w}_{i}\\}$ . $/*$ Set parameters $\\ast/$ 1 Let $\\begin{array}{r}{K=\\mathrm{poly}\\log\\left(k,\\frac{1}{\\alpha},\\frac{1}{\\varepsilon},\\log\\frac{1}{\\delta}\\right)}\\end{array}$ be sufficiently large. 2 Set n \u2190K \u00b7 k log(1/\u03b4). $/*$ Get $\\textrm{\\textbf{X}}*/$ 3 Obtain samples $\\mathbf{X}\\gets\\{X_{1},X_{2},\\ldots,X_{n}\\}$ . $/*$ Obtain some candidate mean-covariance pairs $(\\hat{\\mu}_{i},\\hat{\\Sigma}_{i})\\;\\ast/\\,$ 4 Let $\\mathbf{Y}$ be $\\mathbf{X}$ in sorted (nondecreasing) order. 5 Set $c_{(a,b)}\\gets0$ for all $(a,b)\\in\\mathbb{Z}^{2}$ . 6 for $j=1$ to $n-1$ do 7 Set $r_{j}\\leftarrow Y_{j}$ $Y_{j},s_{j}\\gets Y_{j+1}-Y_{j},Z_{j}\\gets(r_{j},s_{j}).$ 8 if $s_{j}>0$ then 9 Let $\\begin{array}{r}{a\\gets\\lfloor\\log_{2}(r_{j})\\rfloor,b\\gets\\lfloor\\frac{r_{j}}{n^{5}\\cdot2^{a}}\\rfloor,}\\end{array}$ . 10 $c_{a,b}\\gets c_{a,b}+1$ . 11 end 12 end 13 $S\\gets\\emptyset$ . 14 for $(a,b)\\in\\mathbb{Z}^{2}$ do 15 Sample $\\tilde{c}_{a,b}\\gets c_{a,b}+\\mathrm{TLap}(1,\\varepsilon/10,\\delta/10).$ 16 if $\\begin{array}{r}{\\tilde{c}_{a,b}>\\frac{100}{\\varepsilon}\\log\\frac{1}{\\delta}}\\end{array}$ then 17 $S\\gets\\check{S}\\cup\\{(b\\cdot n^{5}\\cdot2^{a},2^{2a})\\}$ . 18 end ", "page_idx": 32}, {"type": "text", "text": "19 end ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "/\\* Fine approximation, via private hypothesis selection \\*/ ", "page_idx": 32}, {"type": "text", "text": "20 Set $\\begin{array}{r}{n^{\\prime}\\leftarrow K\\cdot\\left(\\frac{d^{2}k}{\\alpha^{2}}+\\frac{d^{2}k}{\\alpha\\varepsilon}\\right)}\\end{array}$ ", "page_idx": 32}, {"type": "text", "text": "21 Sample $X_{n+1},\\ldots,X_{n+n^{\\prime}}$ , and redefine $\\mathbf{X}\\gets\\{X_{n+1},\\ldots,X_{n+n^{\\prime}}\\}$ . ", "page_idx": 32}, {"type": "text", "text": "22 Run Algorithm 1 on $\\mathbf{X}$ , $S=\\{(\\hat{\\mu}_{i},\\hat{\\Sigma}_{i})\\}$ , with parameters $d=1$ , $\\boldsymbol{k}^{\\prime}=\\#\\{(\\hat{\\mu}_{i},\\hat{\\Sigma}_{i})\\}$ , and $G=n^{3}$ . ", "page_idx": 32}, {"type": "text", "text": "Next, we show that $S$ is not too large. ", "page_idx": 32}, {"type": "text", "text": "Lemma F.2. The number of $e\\,=\\,(a,b)\\,\\in\\,\\mathbb{Z}^{2}$ such that $\\begin{array}{r}{\\tilde{c}_{e}>\\frac{100}{\\varepsilon}\\cdot\\log\\frac{1}{\\delta}}\\end{array}$ is at most $n-1$ . Thus, $|S|\\leq n-1$ . ", "page_idx": 32}, {"type": "text", "text": "Proof. Note that every $Z_{j}$ only increases the count of a single $c_{e}$ , so it suffices to prove that $\\begin{array}{r}{\\tilde{c}_{e}>\\frac{100}{\\varepsilon}\\cdot\\log\\frac{1}{\\delta}}\\end{array}$ only if $c_{e}\\geq1$ . ", "page_idx": 32}, {"type": "text", "text": "To prove this, suppose that $c_{e}=0$ . Then, $\\begin{array}{r}{\\tilde{c}_{e}\\sim\\mathrm{TLap}(1,\\varepsilon/10,\\delta/10)\\leq\\frac{100}{\\varepsilon}\\cdot\\log\\frac{1}{\\delta}}\\end{array}$ , by Proposition F.1. Thus, $\\begin{array}{r}{\\tilde{c}_{e}>\\frac{100}{\\varepsilon}\\cdot\\log\\frac{1}{\\delta}}\\end{array}$ can only happen if $c_{e}>0$ , meaning $c_{e}\\geq1$ . \u53e3 ", "page_idx": 32}, {"type": "text", "text": "Next, we show that for every Gaussian component $(\\mu_{i},\\sigma_{i})$ of sufficiently large weight $w_{i}$ , there are several pairs $Z_{j}=(Y_{j},Y_{j+1}-Y_{j})$ that crudely approximate $(\\mu_{i},\\sigma_{i})$ . ", "page_idx": 32}, {"type": "text", "text": "Lemma F.3. Let $n\\geq K\\cdot{\\frac{k\\log(1/\\delta)}{\\alpha\\varepsilon}}$ , where $\\begin{array}{r}{K=\\mathrm{poly}\\log(k,\\frac{1}{\\alpha},\\frac{1}{\\varepsilon},\\log\\frac{1}{\\delta})}\\end{array}$ is sufficiently large. Let $\\mathbf{X}$ be n i.i.d. samples from a\u03b1 \u03b5GMM with represent\u03b1ation $\\{(w_{i},\\mu_{i},\\Sigma_{i})\\}_{i=1}^{k}$ . Then, with failure probability at least 0.99, for ev\u03c3ery with $\\begin{array}{r}{w_{i}\\geq\\frac{\\alpha}{k}}\\end{array}$ , there are at least ${\\frac{\\alpha}{8k}}\\cdot n$ indices such that $Y_{j}\\in[\\mu_{i}-\\sigma_{i},\\mu_{i}+\\sigma_{i}]$ and $\\begin{array}{r}{\\frac{\\sigma_{i}}{10^{4}\\cdot n^{4}}\\leq Y_{j+1}-Y_{j}\\leq2\\ddot{\\sigma}_{i}}\\end{array}$ . ", "page_idx": 32}, {"type": "text", "text": "Proof. Fix some $i\\in[k]$ such that $\\begin{array}{r}{w_{i}\\geq\\frac{\\alpha}{k}}\\end{array}$ . By a Chernoff bound, since $\\begin{array}{r}{w_{i}\\cdot n\\geq\\frac{\\alpha}{k}\\cdot n}\\end{array}$ is a sufficiently large multiple of $\\log k$ , with failure probability at most $\\frac{1}{1000k}$ , the number of data points $X_{j}$ from component $i$ is at least ${\\textstyle{\\frac{\\alpha}{2k}}}\\cdot n$ . Let us condition on $T_{i}\\ \\bar{\\subset}\\ [n]$ , the set of indices coming from the $i^{\\mathrm{th}}$ mixture component, and condition on $\\begin{array}{r}{|T_{i}|\\,\\geq\\,\\frac{\\alpha}{2k}\\,\\cdot n}\\end{array}$ . Then, by a Chernoff bound, with failure probability at most $\\frac{1}{1000k}$ , at least ${\\frac{\\alpha}{4k}}\\cdot n$ points $X_{r}:r\\in T_{i}$ are in the interval $[\\mu_{i}-\\sigma_{i},\\mu_{i}+\\sigma_{i}]$ Next, note that for any $X_{r},X_{r^{\\prime}}\\sim\\mathcal{N}(\\mu_{i},\\sigma_{i}^{2}),$ , $X_{r}-X_{r^{\\prime}}\\sim\\mathcal{N}(0,2\\sigma_{i}^{2})$ , and thus has magnitude at least $\\frac{\\sigma_{i}}{10^{4}n^{3}}$ with at most $\\frac{1}{1000n^{3}}$ failure probability. So, by a union bound over all $r\\neq r^{\\prime}\\in T_{i}$ , with at most10010 $\\begin{array}{r}{\\frac{1}{1000n}\\leq\\frac{1}{1000k}}\\end{array}$ failure probability, all data points $X_{r},X_{r^{\\prime}}$ for $r\\neq r^{\\prime}\\in T_{i}$ are separated by at least 104in3 . ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "So, with at least10030k failure probability, there is a subset $U_{i}\\subset T_{i}$ of size at least ${\\frac{\\alpha}{4k}}\\cdot n$ , such that for every $r,r^{\\prime}\\in U_{i}$ , $\\left|X_{r}-X_{r^{\\prime}}\\right|$ , and for every $r\\in U_{i}$ , $X_{r}\\in[\\mu_{i}-\\sigma_{i},\\mu_{i}+\\sigma_{i}]$ . By a union bound, this holds simultaneously for all $i\\in[k]$ with $\\begin{array}{r}{w_{j}\\geq\\frac{\\alpha}{k}}\\end{array}$ , with probability at least 0.997. ", "page_idx": 33}, {"type": "text", "text": "Conditioned on this event, if we consider the elements $\\mathbf{X}$ in sorted order (i.e., $\\mathbf{Y}_{.}$ ), between every consecutive pair $X_{r},X_{r^{\\prime}}:r,r^{\\prime}\\in U_{i}$ (after sorting), w\u03c3e know $\\begin{array}{r}{X_{r^{\\prime}}-X_{r}\\ge\\frac{\\sigma_{i}}{10^{4}\\cdot n^{3}}}\\end{array}$ . So, there exi\u03b1st some $X_{r}\\leq Y_{j},Y_{j+1}\\leq X_{r^{\\prime}}$ such that $\\begin{array}{r}{Y_{j+1}-Y_{j}\\ge\\frac{\\sigma_{i}}{10^{4}\\cdot n^{4}}}\\end{array}$ . Hence, for every $\\ddot{i}\\in[k]$ with $\\begin{array}{r}{w_{i}\\geq\\frac{\\alpha}{k}}\\end{array}$ there exist at least $\\begin{array}{r}{|U_{i}|-1\\geq\\frac{\\alpha}{4k}-1\\geq\\frac{\\alpha}{8k}}\\end{array}$ indices $j$ such that $\\begin{array}{r}{Y_{j+1}-Y_{j}\\ge\\frac{\\sigma_{i}}{10^{4}\\cdot n^{4}}}\\end{array}$ . Moreover, note that $Y_{j},Y_{j+1}\\in[\\mu_{i}-\\sigma_{i},\\mu_{i}+\\overline{{\\sigma_{i}}}]$ , so $Y_{j+1}-Y_{j}\\le2\\sigma_{i}$ . Hence, conditioned on the event from the previous paragraph, the lemma holds. \u53e3 ", "page_idx": 33}, {"type": "text", "text": "Given the above lemma, we can prove that for every $\\left(\\mu_{i},\\Sigma_{i}\\right)$ in the Gaussian mixture with at least $\\scriptstyle{\\frac{\\alpha}{k}}$ weight, there is some corresponding crude approximation $(\\hat{\\mu},\\hat{\\Sigma})\\in S$ . ", "page_idx": 33}, {"type": "text", "text": "Lemma F.4. Assume that the conditions and result of Lemma $F.3$ holds. Then, there exists $(\\hat{\\mu},\\hat{\\Sigma})\\in S$ such that $n^{-10}\\cdot\\hat{\\Sigma}\\precsim\\Sigma_{i}\\precsim n^{10}\\cdot\\hat{\\Sigma}$ and $\\|\\hat{\\Sigma}^{-1/2}(\\mu_{i}-\\hat{\\mu})\\|_{2}\\leq n^{6}$ . ", "page_idx": 33}, {"type": "text", "text": "Proof. Fix any $i$ with $\\begin{array}{r}{w_{i}\\geq\\frac{\\alpha}{k}}\\end{array}$ . Then, for any $j$ with $Y_{j},Y_{j+1}\\in[\\mu_{i}\\!-\\!\\sigma_{i},\\mu_{i}\\!+\\!\\sigma_{i}]$ and $\\begin{array}{r}{\\frac{\\sigma_{i}}{10^{4}\\cdot n^{4}}\\leq Y_{j+1}-}\\end{array}$ $Y_{j}\\leq2\\sigma_{i}$ , the index $i$ contributes to the count of some bucket $\\boldsymbol{e}=(a,b)$ , where $\\left\\lfloor\\log_{2}{\\frac{\\sigma_{i}}{10^{5}\\cdot n^{4}}}\\right\\rfloor\\,\\leq$ $a\\,\\leq\\,\\lfloor\\log_{2}(2\\sigma_{i})\\rfloor$ . Moreover, $Y_{j}\\;\\in\\;[b\\cdot n^{5}\\cdot2^{a},(b+1)\\cdot n^{5}\\cdot2^{a})$ . Therefore, if we consider the set $V_{i}$ of $\\textit{e}=\\,(a,b)$ such that $\\lfloor\\log_{2}{\\frac{\\sigma_{i}}{10^{5}\\cdot n^{4}}}\\rfloor\\ \\leq\\ a\\ \\leq\\ \\lfloor\\log_{2}(2\\sigma_{i})\\rfloor$ and $[b\\cdot n^{5}\\cdot2^{a},(b+1)\\cdot n^{5}$ \u00b7 $2^{a})\\cap[\\mu_{i}\\mathrm{~-~}\\sigma_{i},\\mu_{i}\\mathrm{~+~}\\sigma_{i}]$ is nonempty, the sum of the counts $c_{e}$ across such $e~\\in~V_{i}$ is at least ${\\frac{\\alpha}{8k}}\\,\\cdot\\,n$ . But there are at most ${\\cal O}(\\log n)$ choices of $a$ , and since $n^{5}\\cdot2^{a}\\,>\\,2\\sigma$ , there are at most two choices for $b$ for any fixed $a$ . So, $|V_{i}|\\,\\le\\,O(\\log n)$ , and $\\begin{array}{r}{\\sum_{e\\in V_{i}}c_{e}\\;\\geq\\;\\frac{\\alpha}{8k}\\,\\cdot\\,n}\\end{array}$ . Assuming that $\\begin{array}{r}{\\frac{\\alpha}{8k}\\cdot n\\geq O(\\log n\\cdot\\frac{1}{\\varepsilon}\\log\\frac{1}{\\delta})}\\end{array}$ , i.e., $n\\geq K\\cdot{\\frac{k\\log(1/\\delta)}{\\alpha\\varepsilon}}$ for a suf ficiently large polylogarithmic factor $\\begin{array}{r}{K=\\mathrm{poly}\\log\\left(k,\\frac{1}{\\alpha},\\frac{1}{\\varepsilon},\\log\\frac{1}{\\delta}\\right)}\\end{array}$ , one of these buckets $e=(a,b)\\in V_{j}$ must have $\\begin{array}{r}{c_{e}>\\frac{200}{\\varepsilon}\\log\\frac{1}{\\delta}}\\end{array}$ . In this case, $\\begin{array}{r}{\\tilde{c}_{e}=c_{e}\\bar{+}\\,\\mathrm{TLap}(1,\\varepsilon/10,\\delta/10)>\\frac{100}{\\varepsilon}\\log\\frac{1}{\\delta}}\\end{array}$ by Proposition F.1, so we include the pair $({\\hat{\\mu}},{\\hat{\\Sigma}}):=(b\\cdot n^{5}\\cdot2^{a},2^{2a})$ in $S$ . ", "page_idx": 33}, {"type": "text", "text": "Hence, there exists $(a,b)\\,\\in\\,V_{i}$ such that $({\\hat{\\mu}},{\\hat{\\Sigma}})\\,:=\\,(b\\cdot n^{5}\\cdot2^{a},2^{2a})$ in $S$ . By definition of $V_{i}$ , $\\begin{array}{r}{\\frac{\\sigma_{i}}{n^{5}}\\leq2^{a}\\leq2\\sigma_{i}}\\end{array}$ , so $\\begin{array}{r}{\\frac{1}{n^{10}}\\cdot\\Sigma_{i}\\le2^{2a}\\le4\\Sigma_{i}\\le n^{10}\\cdot\\Sigma_{i}}\\end{array}$ . Moreover, $|\\hat{\\Sigma}^{-1/2}(\\mu_{i}-\\hat{\\mu})|=2^{-a}\\cdot|\\mu_{i}-\\hat{\\mu}|$ But the definition of $V_{i}$ means ${\\hat{\\mu}}=b\\cdot n^{5}\\cdot2^{a}$ satisfies $\\hat{\\mu}\\leq\\mu_{i}+\\sigma_{i}$ and ${\\hat{\\mu}}\\geq\\mu_{i}-\\sigma_{i}-n^{5}\\cdot2^{a}\\geq$ $\\mu_{i}-(2n^{5}+1)\\cdot\\sigma_{i}$ . So, $|{\\hat{\\mu}}-\\mu_{i}|\\leq(2n^{5}+1)\\cdot\\sigma_{i}\\leq n^{6}\\cdot2^{a}$ . Thus, $\\vert\\hat{\\Sigma}^{-1/2}(\\hat{\\mu}-\\mu_{i})\\vert\\leq n^{6}$ \u53e3 ", "page_idx": 33}, {"type": "text", "text": "We now prove that the mechanism (until Line 19) is differentially private. First, we note the following auxiliary claim. ", "page_idx": 33}, {"type": "text", "text": "Lemma F.5. Let $\\mathbf{X},\\mathbf{X}^{\\prime}$ be adjacent datasets of size $n$ (i.e., only differing on a single element). Then, the corresponding sets $\\mathbf{Z}=\\mathbf{Z}(\\mathbf{X})$ and $\\mathbf{Z}^{\\prime}=\\mathbf{Z}(\\mathbf{X}^{\\prime})$ differ in distance at most 3, where by distance we mean that there exists a permutation of the elements in $\\mathbf{Z}$ and $\\mathbf{Z}^{\\prime}$ , respectively, such that at most three indices $i\\leq n-1$ satisfy $Z_{i}\\neq Z_{i}^{\\prime}$ . ", "page_idx": 33}, {"type": "text", "text": "Proof. Note that for the sorted versions ${\\bf Y},{\\bf Y^{\\prime}}$ of $\\mathbf{X},\\mathbf{X}^{\\prime}$ , respectively, we can convert from $\\mathbf{Y}$ to ${\\bf Y^{\\prime}}$ by removing one data point and adding one more data point, without affecting the order of any other data points. ", "page_idx": 33}, {"type": "text", "text": "Suppose we remove some $Y_{j}$ from $Y$ . If $j=1$ , then this just removes $Z_{1}$ , and if $j=n$ , then this just removes $Z_{n-1}$ . If $j\\geq2$ , this modifies $Z_{j-1}$ and removes $Z_{j}$ . Likewise, if we add some new $Y_{j^{\\prime}}$ , this will either add one new ordered pair to $\\mathbf{Z}$ (if $Y_{j^{\\prime}}$ is either the smallest or largest element), or replace one ordered pair in $\\mathbf{Z}$ with two new pairs. Therefore, if we remove a $Y_{j}$ and then add a $Y_{j^{\\prime}}$ , this will change at most 3 of the ordered pairs in $\\mathbf{Z}$ . \u53e3 ", "page_idx": 33}, {"type": "text", "text": "Lemma F.6. The set ${\\cal S}=\\{(\\hat{\\mu}_{j},\\hat{\\Sigma}_{j})\\}$ of candidate mean-covariance pairs is $(\\varepsilon,\\delta)$ -DP with respect to $\\mathbf{X}=\\{X_{1},\\ldots,X_{n}\\}$ . ", "page_idx": 34}, {"type": "text", "text": "Proof. Let $\\mathbf{X},\\mathbf{X}^{\\prime}$ be adjacent datasets of size $n$ . Then, the corresponding sets $\\mathbf{Z},\\mathbf{Z^{\\prime}}$ (after a possible permutation) differ in 3 elements. Therefore, if we let $\\{\\widetilde{c}_{e}\\}_{e\\in\\mathbb{Z}^{2}}$ be the counts for $\\mathbf{Z}$ and $\\{\\widetilde{c}_{e}^{\\prime}\\}_{e\\in\\mathbb{Z}^{2}}$ be the counts for $\\mathbf{Z}^{\\prime}$ , we have that $\\|\\tilde{c}_{e}-\\tilde{c}_{e}^{\\prime}\\|_{1}\\leq6$ . Because changing a single count $c_{e}$ by 1 leads to $(\\varepsilon/10,\\delta/10)$ -DP, overall, the counts $\\{\\tilde{c}_{e}\\}$ will satisfy $(\\varepsilon,\\delta)$ -DP. Finally, $S$ is a deterministic function of the noisy counts $\\tilde{c}_{e}$ , and therefore must also be $(\\varepsilon,\\delta)$ -DP. \u53e3 ", "page_idx": 34}, {"type": "text", "text": "Finally, we can incorporate the fine approximation (i.e., Lines 20\u201322 of Algorithm 3) and prove Theorem 1.5. ", "page_idx": 34}, {"type": "text", "text": "Proof of Theorem 1.5. By Lemma F.6, the algorithm up to Line 19 is $(\\varepsilon,\\delta)$ -DP with respect to $X_{1},\\ldots,X_{n}$ , and does not depend on $X_{n+1},\\ldots,X_{n+n^{\\prime}}$ . Assuming ${\\cal S}=\\{(\\hat{\\mu}_{i},\\hat{\\Sigma}_{i})\\}$ from these lines is fixed, Lines 20\u201322 are $(\\varepsilon,\\delta)$ -DP with respect to $X_{n+1},\\ldots,X_{n+n^{\\prime}}$ , by Lemma D.3, and do not depend on $X_{1},\\ldots,X_{n}$ . So, the overall algorithm is $(\\varepsilon,\\delta)$ -DP. ", "page_idx": 34}, {"type": "text", "text": "Next, we verify accuracy. Note that by Lemma F.4, and for $G=n^{10}$ , the sets $(\\hat{\\mu}_{i},\\hat{\\Sigma}_{i})$ that we find satisfy the conditions for Lemma D.3, with failure probability at most 0.01. Moreover, the size of ${\\cal S}=\\{(\\hat{\\mu}_{i},\\hat{\\Sigma}_{i})\\}$ is at most $n$ , by Lemma F.2. So, because $d=1$ , it suffices for $n^{\\prime}$ , the number of samples used in Line 20 of Algorithm 3, to satisfy n\u2032 \u2265O k\u00b7log(\u03b1G2\u00b7kn/\u03b1)+ k\u00b7log(G\u03b1\u00b7\u03b5kn/\u03b1) = $\\widetilde{\\cal O}\\left(\\frac{k}{\\alpha_{\\ast}^{2}}+\\frac{k}{\\alpha\\varepsilon}\\right)$ , where the last part of the bound holds by our assumptions on $G$ and $n$ . Thus, the total sample complexity is ", "page_idx": 34}, {"type": "equation", "text": "$$\nn+n^{\\prime}=\\widetilde O\\left(\\frac{k\\log(1/\\delta)}{\\alpha\\varepsilon}+\\frac{k}{\\alpha^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "By Lemma D.3, with failure probability at most 0.1, Lines 20\u201322 of the algorithm will successfully output a hypothesis which is a mixture of $k$ Gaussians, with total variation distance at most $O(\\bar{\\alpha})$ from the right answer. So, the overall success probability is at least 0.8. \u53e3 ", "page_idx": 34}, {"type": "text", "text": "G Lower Bound ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "In this section, we prove Theorem 1.6. The proof of the lower bound will, at a high level, follow from known lower bounds for privately learning a single Gaussian [KV18, KMS22a], which we now state. ", "page_idx": 34}, {"type": "text", "text": "Theorem G.1. [KV18] For some sufficiently small constant $c^{*}$ , $(\\varepsilon,\\delta)$ -privately learning an arbitrary univariate Gaussian N(\u00b5, \u03c32) up to total variation distance c\u2217requires \u2126 log(\u03b51/\u03b4) samples. Moreover, this lower bound holds even if we are promised that $|\\mu|\\leq(1/\\delta)^{C}$ for a sufficiently large constant $C$ , and $\\sigma=1$ . ", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "Note that this lower bound immediately implies the same lower bound for general dimension $d$ . ", "page_idx": 34}, {"type": "text", "text": "Theorem G.2. [KMS22a] Let $\\alpha$ be at most a sufficiently small constant, and let $\\begin{array}{r}{\\delta\\le\\left(\\frac{\\alpha\\varepsilon}{d}\\right)^{C}}\\end{array}$ for $a$ sufficiently large constant $C$ . Then, $(\\varepsilon,\\delta)$ -privately learning a $d$ -dimensional Gaussian $\\mathcal{N}(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma})\\;u p$ to total variation distance $\\alpha$ requires $\\tilde{\\Omega}\\left(\\frac{d^{2}}{\\alpha\\varepsilon}\\right)$ samples. ", "page_idx": 34}, {"type": "text", "text": "Moreover, this lower bound holds even if we are promised that $\\mu=\\pmb\\theta$ , and $I\\preccurlyeq\\Sigma\\preccurlyeq2I$ . ", "page_idx": 34}, {"type": "text", "text": "Note that a tighter version of the above theorem has been proved in [Nar23, PH24]; we also refer the reader to [BUV14]. ", "page_idx": 34}, {"type": "text", "text": "Proof Sketch of Theorem 1.6. First, the lower bound o f k\u03b1d22 is already known \u2013 see [ABH+18]. ", "page_idx": 34}, {"type": "text", "text": "The lower bound of $\\frac{k d^{2}}{\\alpha\\varepsilon}$ will follow from Theorem G.2. To explain how, we consider $k$ distinct Gaussians $\\mathcal{N}(\\mu_{i},\\Sigma_{i})$ , where the means $\\mu_{i}$ are known and very far away from each other, and $I\\preccurlyeq\\Sigma_{i}\\preccurlyeq2I$ are unknown. The overall mixture that we will try to learn is the uniform mixture over $\\mathcal{N}(\\mu_{i},\\Sigma_{i})$ , i.e., every weight $w_{i}=1/k$ . By making them very far away from each other, we are making learning the full mixture equivalent to learning each component (on average). Namely, even if we are given the information of which Gaussian each sample comes from, we will need to learn at least $2/3$ of the Gaussians up to total variation distance $O(\\alpha)$ , to learn the full mixture up to total variation distance $\\alpha$ . Hence, we will need at least $k$ times as many samples as for learning a single Gaussian, which means we need $\\begin{array}{r}{\\tilde{\\Omega}\\left(\\frac{k d^{2}}{\\alpha\\varepsilon}\\right)}\\end{array}$ total samples. ", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 35}, {"type": "text", "text": "bTohue nldo iwne rt hbeo uunnidv aorfi $\\frac{k\\log(1/\\delta)}{\\alpha\\varepsilon}$ Wwei lpl lfaonltl w d firstoinmc tT hGeaoursesima nGs. ,h awt hite rseu ftfhice oa rper voevrey t fhaer  laowwaeyr $k$ $\\mathcal{N}(\\mu_{i},1)$ $\\mu_{i}$ from each other, i.e., pairwise $|\\mu_{i}-\\mu_{j}|\\gg(1/\\delta)^{10C}$ . We also assume that $\\mu_{1}=0$ is known, and the remaining $\\mu_{i}$ are unknown but we are promised the value of each $\\mu_{i}$ up to error $(1/\\delta)^{C}$ . The overall mixture will have the first Gaussian $\\bar{\\mathcal{N}}(0,1)$ of weight $w_{1}=1-\\alpha/c^{*}$ , and the remaining Gaussians $\\mathcal{N}(\\mu_{i},1)$ each have weight $w_{i}=\\alpha/(c^{*}\\cdot(k-1))$ . Even if we are given the information of which Gaussian component each sample comes from, to learn the overall mixture up to error $\\alpha$ , we need to learn at least $2/3$ of the small-weight components, each up to total variation distance $O(c^{*})$ . Hence, we will need $\\frac{\\log(1/\\delta)}{\\varepsilon}$ samples from most of the small-weight components. Since the small weight components have weight $\\Theta(\\alpha/k)$ , we need $\\Omega\\left(\\frac{k\\log(1/\\delta)}{\\alpha\\varepsilon}\\right)$ total samples. ", "page_idx": 35}, {"type": "text", "text": "We now give a formal proof of Theorem 1.6. ", "page_idx": 35}, {"type": "text", "text": "G.1 Formal Proof of Theorem 1.6 ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "First, we note two lemmas that will be helpful in proving the Theorem. ", "page_idx": 35}, {"type": "text", "text": "Lemma G.3. Let $\\alpha\\in[0,1)$ . Let $f$ be a probability density function over $\\mathbb{R}^{d}$ . Assume $g:\\ensuremath{\\mathbb{R}^{d}}\\to\\ensuremath{\\mathbb{R}^{\\geq0}}$ exists such that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\int_{\\mathbb{R}^{d}}|f(x)-g(x)|\\,\\,\\mathrm{d}x\\leq\\alpha.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Then there exists $h$ such that $h$ is a probability density function over $\\mathbb{R}^{d}$ , and ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\int_{\\mathbb{R}^{d}}|f(x)-h(x)|\\ \\mathrm{d}x\\leq2\\alpha.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Proof. We know that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\left|\\int_{\\mathbb{R}^{d}}f(x)\\ \\mathrm{d}x-\\int_{\\mathbb{R}^{d}}g(x)\\ \\mathrm{d}x\\right|\\leq\\int_{\\mathbb{R}^{d}}|f(x)-g(x)|\\ \\mathrm{d}x\\leq\\alpha.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Therefore, $\\begin{array}{r}{G:=\\int_{\\mathbb{R}^{d}}g(x)\\;\\mathrm{d}x=1\\pm\\alpha}\\end{array}$ . ", "page_idx": 35}, {"type": "text", "text": "Now two cases are possible either $G<1$ , or $G>1$ , otherwise we are done. If $G<1$ , let $h(x)$ be the density function corresponding to the following distribution: with probability $G$ take a sample from $g(x)/G$ , and with probability $1-G$ select 0. Then ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\int_{\\mathbb{R}^{d}}\\left|f(x)-h(x)\\right|\\ \\mathrm{d}x\\leq1-G+\\int_{\\mathbb{R}^{d}}\\left|f(x)-g(x)\\right|\\ \\mathrm{d}x\\leq2\\alpha,\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "as desired. If $G\\,>\\,1$ , let $h(x)$ be a density function as follows: $\\forall x\\,:\\,0\\,\\leq\\,h(x)\\,\\leq\\,g(x)$ , and $\\begin{array}{r}{\\int_{\\mathbb{R}^{d}}h(x)\\ \\,\\mathrm{d}x\\ =\\ 1}\\end{array}$ . It is easy to see such an $h$ exists by greedily picking $h$ . Then we have $\\begin{array}{r}{\\int_{\\mathbb{R}^{d}}\\left|g(x)-h(x)\\right|\\;\\mathrm{d}x=\\int_{\\mathbb{R}^{d}}g(x)-h(x)\\;\\mathrm{d}x\\leq G-1\\leq\\alpha.}\\end{array}$ Therefore, we can write ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\int_{\\mathbb{R}^{d}}\\left|f(x)-h(x)\\right|\\ \\mathrm{d}x\\leq\\int_{\\mathbb{R}^{d}}\\left|f(x)-g(x)\\right|\\ \\mathrm{d}x+\\int_{\\mathbb{R}^{d}}\\left|g(x)-h(x)\\right|\\ \\mathrm{d}x\\leq2\\alpha,\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "as desired. ", "page_idx": 35}, {"type": "text", "text": "Lemma G.4. Suppose $w\\in(0,1)$ is fixed. Suppose an $(\\varepsilon,\\delta)$ differentially private algorithm exists that takes n samples from the mixture $D=w D_{1}+(1-w)D_{2}$ , and learns $D_{1}$ in total variation distance up to error $\\alpha$ , with success probability $1-\\beta$ . Moreover, assume while sampling it is known which component the sample is sampled from. Then there exists an $(\\varepsilon,\\delta)$ differentially private algorithm $\\boldsymbol{\\mathcal{A}}$ that takes nw $/\\gamma$ samples from $D_{1}$ and outputs an estimate of $D_{1}$ up to total variation distance $\\alpha$ , with success probability $1-\\beta-\\gamma$ . ", "page_idx": 35}, {"type": "text", "text": "Proof. We can view the sampling procedure of the mixture as sampling a random variable $t\\sim$ $\\mathrm{Bin}(n,w)$ , and taking $t$ samples from $D_{1}$ and $n-t$ samples from $D_{2}$ . In order to make an algorithm using $n w/\\gamma$ samples we can take that many samples from $D_{1}$ , and then sample $t$ from $\\mathrm{Bin}(n,w)$ , and run $\\boldsymbol{\\mathcal{A}}$ on $n$ data points constructed as follows: If $t$ is smaller than $n w/\\gamma$ , use $t$ of the samples taken from $D_{1}$ and set the rest to 0, If $t$ is larger than $n w/\\gamma$ , just run $\\boldsymbol{\\mathcal{A}}$ on all zeroes. Finally output the output of $\\boldsymbol{\\mathcal{A}}$ on this input. Clearly, this would be $(\\varepsilon,\\delta)$ differentially private. We know the Algorithm succeeds with probability $1-\\beta$ , over the random coins of the algorithm and the randomness of sampling, if the input is sampled from $w D_{1}+(1-w)D_{2}$ . From Markov\u2019s inequality, we know $\\mathbb{P}[t\\leq n\\bar{w}/\\gamma]\\geq1\\bar{-\\gamma}$ . Therefore, our constructed sample is drawn i.i.d from $w\\bar{D}_{1}+\\bar{(1-w)}D_{2}$ with probability at least $1-\\gamma.$ , where $D_{2}$ is the fixed 0 distribution. Therefore, there exists an $(\\varepsilon,\\delta)$ differentially private algorithm that takes $n w/\\gamma$ many samples from $D_{1}$ and outputs an estimate of $D_{1}$ up to total variation distance $\\alpha$ , with success probability $1-\\beta-\\gamma$ , as desired. \u53e3 ", "page_idx": 36}, {"type": "text", "text": "We now prove Theorem 1.6. ", "page_idx": 36}, {"type": "text", "text": "Proof. We prove the lower bound terms one by one. The first term $\\frac{k d^{2}}{\\alpha^{2}}$ (the non private term) is known by previous work $[\\mathrm{ABH^{+}18}]$ . We prove the lower bound for the second term and the last term here using Theorems G.1 and G.2. ", "page_idx": 36}, {"type": "text", "text": "Let\u2019s prove the second term $\\frac{k d^{2}}{\\alpha\\varepsilon}$ . We apply Theorem G.2, this theorem implies that for any $\\alpha$ smaller than a sufficiently small constant, and $\\begin{array}{r}{\\delta\\le\\left(\\frac{\\alpha\\varepsilon}{d}\\right)^{C}}\\end{array}$ for a sufficiently large $C$ , any $(\\varepsilon,\\delta)$ differentially private algorithm $\\boldsymbol{\\mathcal{A}}$ taking $n$ samples in $\\mathbb{R}^{d}$ , satisfying ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\textrm{t}I\\underset{\\leqslant}{\\longrightarrow}\\Sigma\\prec2I:\\mathbb{P}_{X\\sim\\mathcal{N}(0,\\Sigma)^{\\otimes n},\\;A^{\\varsigma}\\mathrm{~internal~random~bits}}[\\mathrm{d}_{\\mathrm{TV}}(A(X),\\mathcal{N}(0,\\Sigma))\\leq\\alpha]>0.6,}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "must also satisfy $\\begin{array}{r}{n=\\tilde{\\Omega}(\\frac{d^{2}}{\\alpha\\varepsilon})}\\end{array}$ . ", "page_idx": 36}, {"type": "text", "text": "Let $\\mu_{i}^{\\prime}s$ be $k$ distinct vectors in $\\mathbb{R}^{d}$ each having $\\ell_{2}$ distance $M$ from each other, for $M$ to be set later. To see why such a set exists, we can take $\\mu_{i}=M i e_{1}$ , where $e_{1}$ is the first unit vector. Now consider the following set of Gaussians: $D_{i}=\\mathcal{N}(\\mu_{i},\\Sigma_{i})$ , where $\\mu_{i}$ \u2019s are known and constructed as above and $\\Sigma_{i}$ unknown. Consider the uniform mixture $D$ over these Gaussians, with weights $w_{i}=1/k$ . We also assume that when sampling from this distribution we know that which component the samples came from. Consider an $(\\varepsilon,\\delta)$ differentially private algorithm $\\boldsymbol{\\mathcal{A}}$ that takes $n$ samples from $D$ and outputs a distribution $\\hat{D}$ such that $\\mathrm{d}_{\\mathrm{TV}}(D,\\hat{D})\\leq\\alpha$ with probability $2/3$ . ", "page_idx": 36}, {"type": "text", "text": "Now consider a sample from $D_{i}$ , from standard Gaussian tail bounds we know that at least $1\\:-$ $\\exp(-M^{2}/800)$ fraction of the mass of $D_{i}$ is contained within a ball of radius $M/10$ , around $\\mu_{i}$ . Let $B_{i}$ denote this ball, and note that $B_{i}$ \u2019s are disjoint. ", "page_idx": 36}, {"type": "text", "text": "Let $f,f_{i},{\\hat{f}}$ be the probability density functions corresponding to $D,D_{i},\\hat{D}$ respectively. Assuming, we are in the success regime, we can write ", "page_idx": 36}, {"type": "equation", "text": "$$\n2\\alpha\\geq2\\operatorname{d}_{\\operatorname{TV}}(D,{\\hat{D}})=\\int_{\\mathbb{R}^{d}}\\left|f(x)-{\\hat{f}}(x)\\right|\\,\\operatorname{d}\\!x\\geq\\sum_{i=1}^{k}\\int_{B_{i}}\\left|f(x)-{\\hat{f}}(x)\\right|\\,\\operatorname{d}\\!x.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Now let $B\\subseteq[k]$ be the set of indices $i$ such that $\\begin{array}{r}{\\int_{B_{i}}\\left|f(x)-\\hat{f}(x)\\right|\\ \\mathrm{d}x\\geq200\\alpha/k}\\end{array}$ , and $\\mathcal{G}$ be its complement. Then we have that $|\\boldsymbol{B}|\\leq\\,k/100$ . Therefore, there exists $\\mathcal{G}$ such that $|{\\mathcal{G}}|\\,\\geq\\,0.99k$ , and $\\begin{array}{r}{\\forall i\\,\\in\\,\\mathcal{G}\\,:\\,\\int_{B_{i}}\\left|f(x)-\\hat{f}(x)\\right|\\,\\mathrm{~d}x\\,\\le\\,200\\alpha/k}\\end{array}$ . Assume $i\\;\\in\\;{\\mathcal{G}}$ is one such index. Note that $\\begin{array}{r}{f(x)=\\sum_{j=1}^{k}f_{j}(x)/k}\\end{array}$ . Therefore, we can write ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\int_{B_{i}}\\Big|f_{i}(x)/k-\\hat{f}(x)\\Big|\\ \\mathrm{d}x\\le\\int_{B_{i}}\\Big|f(x)-\\hat{f}(x)\\Big|\\ \\mathrm{d}x+\\int_{B_{i}}|f_{i}(x)-f(x)|\\ \\mathrm{d}x}}\\\\ &{\\le200\\alpha/k+\\operatorname*{max}_{j\\ne i}\\mathbb{P}_{X\\sim D_{j}}\\left[X\\notin B_{j}\\right]}\\\\ &{\\le200\\alpha/k+\\exp(-M^{2}/800)}\\\\ &{\\le300\\alpha/k,}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where the last inequality comes from taking $M\\,\\geq\\,30{\\sqrt{\\log(k/100\\alpha)}}$ . We show that using the distribution $\\hat{D}$ , we can construct an answer to the problem of learning the Gaussian $D_{i}$ . To do so first take $g_{i}$ to be equal to $k{\\hat{f}}(x)$ over $B_{i}$ and 0 everywhere else. We have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\int_{\\mathbb{R}^{d}}|f_{i}(x)-g_{i}(x)|\\ \\mathrm{d}x=\\int_{B_{i}}\\left|f_{i}(x)-k\\hat{f}(x)\\right|\\ \\mathrm{d}x+\\int_{\\mathbb{R}^{d}\\backslash B_{i}}f_{i}(x)\\ \\mathrm{d}x\\leq300\\alpha+100\\alpha/k\\leq400\\alpha.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Now we may apply Lemma G.3, and deduce that given $\\hat{D}$ we can construct probability density functions and distributions $\\hat{D}_{i}$ \u2019s such that $\\mathrm{d}_{\\mathrm{TV}}(\\hat{D}_{i},D_{i})\\leq800\\alpha$ , for all $i\\in\\mathcal G$ . To recap, so far we have shown that given an $(\\varepsilon,\\delta)$ differentially private algorithm that takes as inputs samples from our constructed mixture of Gaussians and outputs a density $\\hat{D}$ that has total variation distance at most $\\alpha$ , from the ground truth distribution with success probability $2/3$ , we can use $\\hat{D}$ to construct densities $\\hat{D}_{i}$ such that $\\mathrm{d}_{\\mathrm{TV}}(\\hat{D}_{i},D_{i})\\leq800\\alpha.$ , for $0.99k$ of the indices $i$ . This implies that there exists a fixed index $i$ for which the component $D_{i}$ is learned up to error $800\\alpha$ with success probability $2/3-0.01\\geq0.65$ . Applying Lemma G.4, implies that there must exist an $(\\varepsilon,\\delta)$ differentially private algorithm that takes $100n/k$ samples from $D_{i}$ and estimates its density up to total variation distance $800\\alpha$ , with success probability at least 0.6. Therefore, applying Theorem G.2, we conclude that $\\begin{array}{r}{n=\\tilde{\\Omega}(\\frac{k d^{2}}{\\alpha\\varepsilon})}\\end{array}$ . ", "page_idx": 37}, {"type": "text", "text": "Now let\u2019s prove the last term $\\frac{k\\log(1/\\delta)}{\\alpha\\varepsilon}$ . We aim to apply Theorem G.1. Let $\\mu_{i}$ \u2019s be $k$ distinct values in $\\mathbb{R}$ , each having distance $M$ from each other, for $M\\gg(1/\\delta)^{10C}$ to be set later, where $\\mu_{1}=0$ . It is easy to see such a set exists. Now consider the following set of Gaussians: $D_{i}=\\mathcal{N}(\\mu_{i},1)$ , where $\\mu_{i}$ \u2019s are known up to $\\log(1/\\delta)^{C}$ , and $\\mu_{1}=0$ is also known. Consider the mixture $D$ over these Gaussians, with weights $w_{1}=1-\\alpha/c^{*}$ , and $w_{i}=\\alpha/(c^{*}(k-1))$ . We also assume that when sampling from the mixture we know which component each sample comes from. Consider an $(\\varepsilon,\\delta)$ differentially private algorithm $\\boldsymbol{\\mathcal{A}}$ that takes $n$ samples from $D$ and outputs a distribution $\\hat{D}$ such that $\\mathrm{d}_{\\mathrm{TV}}(D,\\hat{D})\\bar{\\leq}\\;\\alpha$ with probability $2/3$ . ", "page_idx": 37}, {"type": "text", "text": "Now consider a sample from $D_{i}$ , from standard Gaussian tail bounds we know that at least $1\\:-$ $\\exp(-M^{2}/200)$ fraction of the mass of $D_{i}$ is contained within a ball of radius $M/10$ , around $\\mu_{i}$ . Let $B_{i}$ denote this ball, and note that $B_{i}$ \u2019s are disjoint. ", "page_idx": 37}, {"type": "text", "text": "Let $f,f_{i},{\\hat{f}}$ be the probability density functions corresponding to $D,D_{i},\\hat{D}$ respectively. Assuming, we are in the success regime, similar to the proof of the previous term, we can show that there exists a set $\\mathcal{G}$ of indices such that $|\\mathcal{G}|\\ge0.99k$ , and $\\begin{array}{r}{\\forall i\\in\\mathcal{G}:\\int_{B_{i}}\\left|f(x)-\\hat{f}(x)\\right|\\ \\mathrm{d}x\\leq200\\alpha/k}\\end{array}$ . Moreover, with a similar argument as the previous term for $i\\in\\mathcal G$ we can say as long as $M\\geq20{\\sqrt{\\log(k/100\\alpha)}}$ , $\\begin{array}{r}{\\int_{B_{i}}\\left|w_{i}f_{i}(x)-\\hat{f}(x)\\right|~\\mathrm{d}x\\leq300\\alpha/k}\\end{array}$ . We show that using the distribution $\\hat{D}$ , we can construct an answer to the problem of learning the Gaussian $D_{i}$ , for $i\\neq1$ . To do so take $g_{i}$ to be equal to $\\hat{f}(x)/w_{i}$ , over $B_{i}$ and 0 everywhere else. We have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\int_{\\mathbb{R}^{d}}|f_{i}(x)-g_{i}(x)|\\ d x=\\int_{B_{i}}\\left|f_{i}(x)-\\hat{f}(x)/w_{i}\\right|\\ d x+\\int_{\\mathbb{R}^{d}\\setminus B_{i}}f_{i}(x)\\ \\mathrm{d}x\\le\\frac{300\\alpha}{k w_{i}}+\\frac{100\\alpha}{k}\\le400c^{*}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Now we may apply Lemma G.3, and deduce that given $\\hat{D}$ , we can construct probability density functions and distributions $\\hat{D}_{i}$ \u2019s such that $\\mathrm{d}_{\\mathrm{TV}}(\\hat{D}_{i},D_{i})\\leq800c^{*}$ , for all $i\\in\\mathcal G$ . To recap, so far we have shown that given an $(\\varepsilon,\\delta)$ differentially private algorithm that takes as inputs samples from our constructed mixture of Gaussians and outputs density D\u02c6 that has total variation distance at most $\\alpha$ from the ground truth distribution with success probability $2/3$ , we can use $\\hat{D}$ to construct densities $\\hat{D_{i}}$ such that $\\mathrm{d}_{\\mathrm{TV}}(\\hat{D}_{i},D_{i})\\leq800c^{*}$ , for $0.99k$ of the indices $i$ . This implies that there exists a fixed index $i\\neq1$ , for which the component $D_{i}$ is learned up to error $800c^{*}$ , with success probability $2/3\\textrm{--}0.01\\,\\geq\\,0.65$ . Applying Lemma G.4, implies that there must exist an $(\\varepsilon,\\delta)$ differentially private algorithm that takes $100n w_{i}$ samples from $D_{i}$ and estimates its density up to total variation distance $800c^{*}$ , with success probability 0.6. Therefore, applying Theorem G.1, and noting that $w_{i}=\\alpha/(c^{*}(k-1))$ we conclude that $\\begin{array}{r}{n=\\Omega(\\frac{k\\log(1/\\delta)}{\\alpha\\varepsilon})}\\end{array}$ . \u53e3 ", "page_idx": 37}, {"type": "text", "text": "H Omitted Proofs ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "In this section, we prove Proposition B.2, Theorem B.3, and Lemma C.7. ", "page_idx": 38}, {"type": "text", "text": "H.1 Proof of Proposition B.2 ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "First, we note a basic fact. ", "page_idx": 38}, {"type": "text", "text": "Fact H.1. For any $x,y\\in[0.9,1.1]$ , we have that $(x y-1)^{2}\\leq4((x-1)^{2}+(y-1)^{2})$ . ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{t}\\,|\\,x y-1|=|x-1+x(y-1)|\\leq|x-1|+|x|\\cdot|y-1|\\leq\\sqrt{2}\\cdot(|x-1|+|y-1|).}\\\\ &{:\\leq2(|x-1|+|y-1|)^{2}\\leq4((x-1)^{2}+(y-1)^{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "We now prove Proposition B.2. ", "page_idx": 38}, {"type": "text", "text": "First, assume $(\\mu_{1},\\Sigma_{1})\\approx_{\\gamma,\\rho,\\tau}(\\mu_{2},\\Sigma_{2})$ . This means $\\|J_{1}J_{1}^{\\top}-I\\|_{o p}\\leq\\gamma$ and $\\|J_{1}J_{1}^{\\top}-I\\|_{F}\\le\\rho.$ We then have that $\\Sigma_{1}^{-1/2}\\Sigma_{2}\\Sigma_{1}^{-1/2}=J_{1}^{-1}(J_{1}^{-1})^{\\top}=(J_{1}^{\\top}J_{1})^{-1}$ . Now, note that $J_{1}J_{1}^{\\top}$ and $J_{1}^{\\top}J_{1}$ are both symmetric and have the same eigenvalues. If we call these eigenvalues $\\lambda_{1},\\ldots,\\lambda_{d}$ , then the eigenvalues of \u03a31\u22121/2\u03a32\u03a31\u22121/2 are $\\lambda_{1}^{-1},\\ldots,\\lambda_{d}^{-1}$ . Now, our assumption $(\\mu_{1},\\Sigma_{1})\\approx_{\\gamma,\\rho,\\tau}\\left(\\mu_{2},\\Sigma_{2}\\right)$ implies that $1-\\gamma\\le\\lambda_{i}\\le1\\!+\\!\\gamma$ and $\\sum(1-\\lambda_{i})^{2}\\le\\rho^{2}$ . This means that, assuming $\\gamma\\leq0.1,1-2\\gamma\\leq$ $\\begin{array}{r}{\\frac{1}{1+\\gamma}\\leq\\lambda_{i}^{-1}\\leq\\frac{1}{1-\\gamma}\\leq1+2\\gamma}\\end{array}$ , and $\\begin{array}{r}{\\sum\\left(1-\\lambda_{i}^{-1}\\right)^{2}\\le\\sum(1-\\lambda_{i})^{2}\\cdot\\lambda_{i}^{-2}\\le2\\cdot\\sum(1-\\lambda_{i})^{2}=2\\rho^{2}.}\\end{array}$ This means that $\\|\\dot{\\Sigma}_{1}^{-1/2}\\Sigma_{2}\\Sigma_{1}^{-1/2}\\|_{o p}\\leq2\\gamma$ and $\\|\\Sigma_{1}^{-1/2}\\Sigma_{2}\\Sigma_{1}^{-1/2}\\|_{F}\\leq2\\rho$ . ", "page_idx": 38}, {"type": "text", "text": "Finally, $\\Sigma_{1}^{-1/2}(\\mu_{1}\\,-\\,\\mu_{2})\\,=\\,J_{1}^{-1}\\Sigma_{2}^{-1/2}(\\mu_{1}\\,-\\,\\mu_{2})$ . Because $\\Sigma_{2}^{-1/2}(\\mu_{1}\\mathrm{~-~}\\mu_{2})$ has magnitude at most $\\tau$ by our assumption, $J_{1}^{-1}\\Sigma_{2}^{-1/2}(\\mu_{1}\\textrm{--}\\mu_{2})$ has magnitude at most the maximum singular value of $J_{1}^{-1}$ times $\\tau$ . But every singular value of $J_{1}^{-1}$ is some $\\lambda_{i}^{-1/2}$ which is at most 2, so $\\begin{array}{r}{\\|\\Sigma_{1}^{-1/2}(\\mu_{2}-\\mu_{1})\\|_{2}=\\|J_{1}^{-1}\\Sigma_{2}^{-1/2}(\\mu_{1}-\\mu_{2})\\|_{2}\\leq2\\tau.}\\end{array}$ ", "page_idx": 38}, {"type": "text", "text": "Next, assume $\\begin{array}{r l}{(\\mu_{1},\\Sigma_{1})}&{{}\\approx_{\\gamma,\\rho,\\tau}\\quad(\\mu_{2},\\Sigma_{2})}\\end{array}$ and $\\begin{array}{r l}{(\\mu_{2},\\Sigma_{2})}&{{}\\approx_{\\gamma,\\rho,\\tau}\\quad(\\mu_{3},\\Sigma_{3})}\\end{array}$ . First, note that $\\Sigma_{3}^{-1/2}\\Sigma_{1}\\Sigma_{3}^{-1/2}\\,=\\,J_{2}J_{1}J_{1}^{\\top}J_{2}^{\\top}\\,=\\,(J_{2}J_{1})(J_{2}J_{1})^{\\top}$ . If the eigenvalues of\u221a $J_{1}J_{1}^{\\top}$ are $\\{\\lambda_{i}\\}$ and the eigenvalues of $J_{2}J_{2}^{\\top}$ are $\\{\\lambda_{i}^{\\prime}\\}$ , then the singular values of $J_{1}$ and $J_{2}$ are $\\{{\\sqrt{\\lambda_{i}}}\\}$ and $\\{\\sqrt{\\lambda_{i}^{\\prime}}\\}$ , respectively. By our assumption, $1\\!-\\!\\gamma\\le\\lambda_{i},\\lambda_{i}^{\\prime}\\le1\\!+\\!\\gamma$ , which means that ${\\sqrt{1-\\gamma}}\\leq{\\sqrt{\\lambda_{i}}}$ , $\\sqrt{\\lambda_{i}^{\\prime}}\\le\\sqrt{1+\\gamma}$ Thus, the singular values of $J_{2}J_{1}$ are between $1-\\gamma$ and $1+\\gamma$ , which means that the eigenvalues of $(J_{2}J_{1})(J_{2}J_{1})^{\\top}$ are between $(1-\\gamma)^{2}$ and $(1+\\gamma)^{2}$ . Hence, for $\\gamma\\le0.1$ , $\\lVert J_{2}J_{1}J_{1}^{\\top}J_{2}^{\\top}-I\\rVert_{o p}\\leq4\\gamma$ ", "page_idx": 38}, {"type": "text", "text": "Assume that $\\lambda_{i}$ , $\\lambda_{i}^{\\prime}$ are in decreasing order. We now consider the $k^{\\mathrm{th}}$ largest singular value of $J_{2}J_{1}$ . If $\\sigma_{k}:=\\sqrt{\\lambda_{k}}$ is the $k^{\\mathrm{th}}$ largest singular value of $J_{1}$ and $\\sigma_{k}^{\\prime}:=\\sqrt{\\lambda_{k}^{\\prime}}$ is the $k^{\\mathrm{th}}$ largest singular value of $J_{2}$ , by Corollary A.5 there exist subspaces $V_{k},V_{k}^{\\prime}$ of dimension $d-k+1$ such that $\\|J_{1}v\\|_{2}\\leq\\sigma_{k}\\|v\\|_{2}$ for all $v\\,\\in\\,V_{k}$ and $\\|J_{2}v\\|_{2}\\;\\leq\\;\\sigma_{k}^{\\prime}\\|v\\|_{2}$ for all $v\\,\\in\\,V_{k}^{\\prime}$ . Therefore, for every $v\\;\\in\\;V_{k}\\cap J_{1}^{-1}V_{k}^{\\prime}$ (note that $J_{1}$ is invertible since $J_{1}J_{1}^{\\top}$ has all eigenvalues between $1-\\gamma$ and $1+\\gamma)$ we have that $\\|J_{2}J_{1}v\\|_{2}\\le\\sigma_{k}^{\\prime}\\|J_{1}v\\|_{2}\\le\\sigma_{k}\\sigma_{k}^{\\prime}\\|v\\|_{2}$ . Because $V_{k}$ and $J_{1}^{-1}V_{k}^{\\prime}$ both have dimension $d-k+1$ , their intersection has dimension at least $d-2k+2$ . So, there is a subspace of dimension at least $d-2k+2$ such that every $v$ in the subspace has $\\|J_{2}J_{1}v\\|_{2}\\leq\\sigma_{k}\\sigma_{k}^{\\prime}\\cdot\\|v\\|_{2}$ . ", "page_idx": 38}, {"type": "text", "text": "Thus, the $(2k-1)^{\\mathrm{th}}$ largest singular value of $J_{2}J_{1}$ is at most $\\sigma_{k}\\sigma_{k}^{\\prime}$ , so the $(2k-1)^{\\mathrm{th}}$ largest eigenvalue of $J_{2}J_{1}J_{1}^{\\top}J_{2}^{\\top}$ is at most $\\lambda_{k}\\lambda_{k}^{\\prime}$ . The same argument, looking at the smallest singular values, tells us that the $(2k-1)^{\\mathrm{th}}$ smallest eigenvalue of $J_{2}J_{1}J_{1}^{\\top}J_{2}^{\\top}$ is at least $\\lambda_{d-k+1}\\lambda_{d-k+1}^{\\prime}$ . Thus, for any $t$ , the $t^{\\mathrm{th}}$ largest eigenvalue of $J_{2}J_{1}J_{1}^{\\top}J_{2}^{\\top}$ is at most $\\lambda_{\\lfloor(t+1)/2\\rfloor}\\lambda_{\\lfloor(t+1)/2\\rfloor}^{\\prime}$ and at least $\\lambda_{\\lceil(d+t)/2\\rceil}\\lambda_{\\lceil(d+t)/2\\rceil}^{\\prime}$ ", "page_idx": 38}, {"type": "text", "text": "Overall, this means that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|J_{2}J_{1}J_{1}^{\\top}J_{2}^{\\top}-I\\|_{F}^{2}\\leq\\displaystyle\\sum_{t=1}^{d}\\operatorname*{max}\\left((\\lambda_{\\lfloor(t+1)/2\\rfloor}\\lambda_{\\lfloor(t+1)/2\\rfloor}^{\\prime}-1)^{2},\\,(\\lambda_{\\lceil(d+t)/2\\rceil}\\lambda_{\\lceil(d+t)/2\\rceil}^{\\prime}-1)^{2}\\right)}\\\\ &{\\qquad\\qquad\\leq\\displaystyle\\sum_{t=1}^{d}\\bigg((\\lambda_{\\lfloor(t+1)/2\\rfloor}\\lambda_{\\lfloor(t+1)/2\\rfloor}^{\\prime}-1)^{2}+(\\lambda_{\\lceil(d+t)/2\\rceil}\\lambda_{\\lceil(d+t)/2\\rceil}^{\\prime}-1)^{2}\\bigg)}\\\\ &{\\qquad=2\\cdot\\displaystyle\\sum_{i=1}^{d}(\\lambda_{i}\\lambda_{i}^{\\prime}-1)^{2}}\\\\ &{\\qquad\\qquad\\leq8\\cdot\\displaystyle\\sum_{i=1}^{d}(\\lambda_{i}-1)^{2}+8\\cdot\\displaystyle\\sum_{i=1}^{d}(\\lambda_{i}^{\\prime}-1)^{2}}\\\\ &{\\qquad\\qquad\\leq8\\cdot(\\|J_{1}J_{1}^{\\top}-I\\|_{F}^{2}+\\|J_{2}J_{1}^{\\top}-I\\|_{F}^{2})\\leq16\\rho^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where the fourth line uses Fact H.1. Thus, $\\|\\Sigma_{3}^{-1/2}\\Sigma_{1}\\Sigma_{3}^{-1/2}-I\\|_{F}\\leq4\\rho.$ ", "page_idx": 39}, {"type": "text", "text": "Finally, note that $\\begin{array}{r}{\\|\\Sigma_{3}^{-1/2}(\\mu_{1}-\\mu_{3})\\|_{2}\\le\\|\\Sigma_{3}^{-1/2}(\\mu_{1}-\\mu_{2})\\|_{2}+\\|\\Sigma_{3}^{-1/2}(\\mu_{2}-\\mu_{3})\\|_{2}=\\|J_{2}\\Sigma_{2}^{-1/2}(\\mu_{1}-\\mu_{3})\\|_{2}.}\\end{array}$ $\\mu_{2})\\|_{2}+\\|\\Sigma_{3}^{-1/2}(\\mu_{2}-\\mu_{3})\\|_{2}$ . By our assumptions, both $\\|\\Sigma_{2}^{-1/2}(\\mu_{1}-\\mu_{2})\\|_{2}$ and $\\|\\Sigma_{3}^{-1/2}(\\mu_{2}-\\mu_{3})\\|_{2}$ are at most $\\tau$ , and $J_{2}$ has operator norm at most $1.1\\leq2$ , which means that $\\lVert\\Sigma_{3}^{-1/2}(\\mu_{1}-\\mu_{3})\\rVert_{2}\\leq$ $3\\tau$ . ", "page_idx": 39}, {"type": "text", "text": "H.2 Proof of Theorem B.3 ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "First, we note a series of known results that will be key to proving the theorem. We start with the bound for robust covariance estimation in spectral error. ", "page_idx": 39}, {"type": "text", "text": "Lemma H.2 (e.g., [DK22, Exercise 4.3]). Fix any $\\eta\\in(0,\\eta_{0})$ , where $\\eta_{0}<0.01$ is a small universal constant, and fix any $\\beta\\in(0,1)$ . There is a (deterministic, inefficient) algorithm $\\mathcal{A}_{1}$ with the following property. Let $\\Sigma\\in\\mathbb{R}^{d\\times d}$ be any covariance matrix, and let $\\mathbf{X}=\\{X_{1},\\ldots,X_{n}\\}\\sim{\\mathcal{N}}(0,{\\boldsymbol{\\Sigma}})$ , where $\\bar{n}\\bar{\\geq O}((d\\!+\\!\\log(1/\\beta))/\\eta^{2})$ . Then, with probability at least $1\\!-\\!\\beta$ over the randomness of $\\mathbf{X}$ , for any $\\eta$ - corruption $\\mathbf{X^{\\prime}}=\\{X_{1}^{\\prime},\\ldots,X_{n}^{\\prime}\\}$ of $\\mathbf{X}$ , $A_{1}(\\mathbf{X}^{\\prime})$ outputs $\\hat{\\Sigma}_{1}$ such that $\\|\\Sigma^{-1/2}\\hat{\\Sigma}_{1}\\Sigma^{-1/2}\\!-\\!I\\|_{o p}\\leq O(\\eta)$ . Importantly, $\\mathcal{A}_{1}$ may have knowledge of $\\eta$ and $\\beta$ , but does not have knowledge of $\\mathbf{X}$ or $\\Sigma$ . ", "page_idx": 39}, {"type": "text", "text": "Next, we prove how to robustly estimate the covariance up to Frobenius error. We start with the following structural lemma. ", "page_idx": 39}, {"type": "text", "text": "Lemma H.3. There exists a universal constant $c\\in(0,0.1)$ with the following property. Fix any $1\\leq k\\leq d,$ , and let $n\\geq{\\widetilde{O}}(d\\cdot k)$ be a sufficiently large (i.e., $n\\geq d k\\cdot(C_{3}\\log(d k))^{C_{4}}$ for some absolute constants $C_{3},C_{4})$ . If we sample i.i.d. $\\mathbf{X}=\\{X_{1},\\ldots,X_{n}\\}\\sim{\\mathcal{N}}(0,I)$ , then with probability at least $1-e^{-c n}$ over $\\mathbf{X}$ , for all symmetric matrices $P\\subset\\mathbb{R}^{d\\times d}$ of rank at most $k$ and Frobenius norm 1, and for all subsets $S\\subset[n]$ of size at least $(1-c)\\cdot n$ , $\\begin{array}{r}{|\\frac{1}{n}\\textstyle\\sum_{i\\in S}\\langle X_{i}X_{i}^{\\top}-I,P\\rangle|\\leq0.1}\\end{array}$ . ", "page_idx": 39}, {"type": "text", "text": "Proof. Consider any fixed $P\\subset\\mathbb{R}^{d\\times d}$ of rank $k$ and Frobenius norm 1, and fix any integer $m$ . For any data points $X_{1},\\ldots,X_{m}\\overset{i.i.d.}{\\sim}\\mathcal{N}(0,I)$ , let $X=(X_{1},\\ldots,X_{m})\\in\\mathbb{R}^{m\\cdot d}$ be the concatenation of $X_{1},\\ldots,X_{m}$ , and let $Q\\in\\mathbb{R}^{(m d)\\times(m d)}$ be the block matrix ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\left(\\begin{array}{c c c c}{{P}}&{{\\mathbf0}}&{{\\cdots}}&{{\\mathbf0}}\\\\ {{\\mathbf0}}&{{P}}&{{\\cdots}}&{{\\mathbf0}}\\\\ {{\\vdots}}&{{\\vdots}}&{{\\ddots}}&{{\\vdots}}\\\\ {{\\mathbf0}}&{{\\mathbf0}}&{{\\cdots}}&{{P}}\\end{array}\\right).\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Then, ${\\begin{array}{r}{{\\frac{1}{m}}\\sum_{i=1}^{m}\\langle X_{i}X_{i}^{\\top}-I,P\\rangle={\\frac{1}{m}}\\sum_{i=1}^{m}\\left(X_{i}^{\\top}P X_{i}-\\operatorname{Tr}(P)\\right)={\\frac{1}{m}}\\left(X^{\\top}Q X-\\operatorname{Tr}(Q)\\right)}\\end{array}}$ . By the Hanson-Wright inequality, we have that for any fixed $\\|P\\|_{F}\\leq1$ , and for any $t\\leq1$ , ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(\\left|\\displaystyle\\frac{1}{m}\\left(X^{\\top}Q X-\\operatorname{Tr}(Q)\\right)\\right|>t\\right)=\\mathbb{P}\\left(\\left|X^{\\top}Q X-\\operatorname{Tr}(Q)\\right|>m\\cdot t\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq2\\cdot\\exp\\left(-\\operatorname*{min}\\left(\\displaystyle\\frac{\\Omega(m t)^{2}}{m\\cdot\\|P\\|_{F}^{2}},\\displaystyle\\frac{\\Omega(m t)}{\\|P\\|_{o p}}\\right)\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq2\\cdot\\exp\\left(-\\Omega(m\\cdot t^{2})\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "By setting $t=0.01$ , we have that for some universal constant $c_{1}$ , ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\left|\\frac{1}{m}\\sum_{i=1}^{m}\\langle X_{i}X_{i}^{\\top}-I,P\\rangle\\right|>0.01\\right)\\leq2e^{-c_{1}m}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Now, we draw $X_{1},\\ldots,X_{n}\\sim{\\mathcal{N}}(0,I)$ , and take a union bound over all subsets $S\\subset[n]$ of size at least $(1-c)n$ (with $m=|S|,$ ) and a union bound over a net of possible matrices $P$ . The number of options for $S$ is at most $\\begin{array}{r}{\\sum_{i\\leq c n}{\\binom{n}{i}}\\leq(e/c)^{c n}}\\end{array}$ . For $P$ , we can choose a $1/n^{10}$ -sized net over the Frobenius norm metric (i.e., the distance between two matrices $P_{1},P_{2}$ is $\\|P_{1}-P_{2}\\|_{F})$ for each of the $k$ nonzero eigenvalues and eigenvectors in the unit $d_{\\cdot}$ -dimensional sphere, which has size at most $n^{100d\\cdot k}$ . Therefore, by a union bound, the probability that $\\begin{array}{r}{\\left|\\frac{1}{n}\\sum_{i\\in S}\\dot{\\langle X_{i}X_{i}^{\\top}-I,P^{\\prime}\\rangle}\\right|\\le0.01}\\end{array}$ for every $\\vert S\\vert\\geq(1-c)n$ and every $P^{\\prime}$ in the net is at least $1-(2e/c)^{c n}\\cdot n^{100d\\cdot k}\\cdot e^{-c_{1}n/2}$ . ", "page_idx": 40}, {"type": "text", "text": "Finally, we consider $P$ outside of the net. For any symmetric $P$ of rank $k$ and Frobenius norm 1, it has Frobenius distance at most $1/n^{8}$ from some $P^{\\prime}$ in the net. Let us consider the event that every $\\lVert X_{i}\\rVert_{2}^{2}\\leq10n$ , which for $n\\geq d$ occurs with failure probability at most $2n\\cdot e^{-c_{1}n}$ by \u221aHanson-Wright. Under this event, $\\langle X_{i}X_{i}^{\\top}-I,P-P^{\\prime}\\rangle\\leq\\|X_{i}X_{i}^{\\top}-I\\|_{F}\\cdot\\|P-P^{\\prime}\\|_{F}\\leq(10n+\\sqrt{d})/n^{8}\\leq0.01$ . As a result, with failure probability at most $(2e/c)^{c n}\\!\\cdot\\!n^{100d\\cdot k}\\!\\cdot\\!e^{-c_{1}n/2}\\!+\\!2n\\!\\cdot\\!e^{-c_{1}n}\\leq e^{-c n}$ (assuming $c$ is sufficiently small), we have both properties. Namely, $\\begin{array}{r}{\\vert\\frac{1}{n}\\sum_{i\\in S}\\langle X_{i}X_{i}^{\\top}-I,P^{\\prime}\\rangle\\vert\\leq0.01}\\end{array}$ for every $|S|\\geq(1-c)n$ and every $P^{\\prime}$ in the net, and for any $P$ , $\\langle X_{i}X_{i}^{\\top}-I,P-P^{\\prime}\\rangle{0.01}$ for the closest $P^{\\prime}$ in the net to $P$ . Overall, this means that for all such $P$ and $S$ , $\\begin{array}{r}{\\left|\\frac{1}{n}\\sum_{i\\in S}\\langle X_{i}X_{i}^{\\top}-I,P\\rangle\\right|\\leq0.1}\\end{array}$ . ", "page_idx": 40}, {"type": "text", "text": "We prove another lemma which contrasts with Lemma H.3. ", "page_idx": 40}, {"type": "text", "text": "Lemma H.4. Fix any $2\\leq\\rho\\leq{\\sqrt{d}}.$ . Let $k=4d/\\rho^{2}$ , and let $n\\geq{\\widetilde{O}}(d\\cdot k)$ and $c\\in(0,0.1)$ be as in Lemma H.3. Fix any covariance matrix $\\Sigma$ and let $\\mathbf{X}=\\{X_{1}\\ldots,X_{n}\\}\\sim{\\mathcal{N}}(0,{\\boldsymbol{\\Sigma}})$ . Then, with probability at least $1-e^{-c n}$ , for every $\\widetilde{\\Sigma}$ such that $0.95\\cdot\\Sigma\\prec\\widetilde{\\Sigma}\\preccurlyeq1.05\\cdot\\Sigma$ and $\\lVert\\Sigma^{-1/2}\\widetilde{\\Sigma}\\Sigma^{-1/2}\\rVert_{\\infty}$ $I\\|_{F}\\geq\\rho$ , for every symmetric matrix $P\\in\\mathbb{R}^{d\\times d}$ of rank at most $k$ and Frobenius norm 1, and for every $S\\subset[n]$ of size at least $(1-c)n,$ , ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\left|\\frac{1}{n}\\sum_{i\\in S}\\langle\\widetilde{\\Sigma}^{-1/2}X_{i}X_{i}^{\\top}\\widetilde{\\Sigma}^{-1/2}-I,P\\rangle\\right|\\geq0.7.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Proof. Let $J=\\widetilde{\\Sigma}^{-1/2}\\Sigma^{1/2}$ , and let $Y_{i}=\\Sigma^{-1/2}X_{i}$ . Then, $\\widetilde{\\Sigma}^{-1/2}X_{i}=J Y_{i}$ , which means that ", "page_idx": 40}, {"type": "text", "text": "From now on, we assum e that for all $S\\subset[n]$ of size at least $(1-c)\\cdot n$ , and for all symmetric matrices $P$ of rank $k$ and Frobenius norm 1, $\\begin{array}{r}{\\left|\\frac{1}{n}\\sum_{i\\in S}\\langle Y_{i}Y_{i}^{\\top}-I,P\\rangle\\right|\\leq0.1}\\end{array}$ . This happens with at least $e^{-c n}$ probability, by Lemma H.3. ", "page_idx": 40}, {"type": "text", "text": "Now, for any subset $S\\subset[n]$ , ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{n}\\sum_{i\\in S}\\langle\\widetilde{\\Sigma}^{-1/2}X_{i}X_{i}^{\\top}\\widetilde{\\Sigma}^{-1/2}-I,P\\rangle=\\displaystyle\\frac{1}{n}\\sum_{i\\in S}\\big(\\langle J(Y_{i}Y_{i}^{\\top}-I)J^{\\top},P\\rangle+\\langle J J^{\\top}-I,P\\rangle\\big)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad=\\displaystyle\\frac{1}{n}\\left(\\sum_{i\\in S}\\mathrm{Tr}(J(Y_{i}Y_{i}^{\\top}-I)J^{\\top}P)\\right)+\\displaystyle\\frac{|S|}{n}\\cdot\\langle J J^{\\top}-I,P\\rangle}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad=\\displaystyle\\frac{1}{n}\\left(\\sum_{i\\in S}\\langle Y_{i}Y_{i}^{\\top}-I,J^{\\top}P J\\rangle\\right)+\\displaystyle\\frac{|S|}{n}\\cdot\\langle J J^{\\top}-I,P\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Now, note that by our assumptions, $\\|\\Sigma^{-1/2}\\widetilde{\\Sigma}\\Sigma^{-1/2}-I\\|_{o p}\\leq0.05$ and $\\|\\Sigma^{-1/2}\\widetilde{\\Sigma}\\Sigma^{-1/2}-I\\|_{F}\\geq\\rho.$ Thus, by Proposition B.2, $\\|\\widetilde{\\Sigma}^{-1/2}\\Sigma\\widetilde{\\Sigma}^{-1/2}-I\\|_{o p}\\leq0.1$ , and by Proposition B.2 again, applied the reverse direction this time, $\\|\\widetilde{\\Sigma}^{-1/2}\\Sigma\\widetilde{\\Sigma}^{-1/2}-I\\|_{F}\\geq\\rho/2$ . We just showed $\\|J J^{\\top}-I\\|_{o p}\\leq0.1$ , so by Proposition A.6, $\\|J^{\\top}P J\\|_{F}\\leq2\\|P\\|_{F}\\leq2$ . Therefore, $\\begin{array}{r}{\\left|\\frac{1}{n}\\sum_{i\\in S}\\langle Y_{i}Y_{i}^{\\top}-I,J^{\\top}P J\\rangle\\right|\\leq0.2}\\end{array}$ . ", "page_idx": 41}, {"type": "text", "text": "Conversely, we just showed $\\|J J^{\\top}\\,-\\,I\\|_{F}\\;\\geq\\;\\rho/2$ . So, if we order the eigenvalues of $J J^{\\top}$ as $\\lambda_{1},\\lambda_{2},\\ldots,\\lambda_{d}$ (and the corresponding unit eigenvectors $v_{1},\\ldots,v_{d})$ such that $|\\lambda_{i}\\!-\\!1|$ are in decreasing order, then $\\begin{array}{r}{\\sum_{i=1}^{d}(\\lambda_{i}-1)^{2}\\geq\\rho^{2}/4}\\end{array}$ , which means that $\\begin{array}{r}{\\sum_{i=1}^{4d/\\rho^{2}}(\\lambda_{i}-1)^{2}\\geq1}\\end{array}$ . So, if we choose $P$ to be $\\begin{array}{r}{\\left(\\sum_{i=1}^{4d/\\rho^{2}}(\\lambda_{i}-1)^{2}\\right)^{-1/2}\\sum_{i=1}^{4d/\\rho^{2}}(\\lambda_{i}-1)v_{i}v_{i}^{\\top}}\\end{array}$ , we have that $\\|P\\|_{F}=1$ and ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\langle J J^{\\top}-I,P\\rangle=\\left(\\sum_{i=1}^{4d/\\rho^{2}}\\left(\\lambda_{i}-1\\right)^{2}\\right)^{-1/2}\\cdot\\sum_{i=1}^{4d/\\rho^{2}}\\left(\\lambda_{i}-1\\right)^{2}=\\left(\\sum_{i=1}^{4d/\\rho^{2}}\\left(\\lambda_{i}-1\\right)^{2}\\right)^{1/2}\\geq1.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Therefore, ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\frac{1}{n}\\sum_{i\\in S}\\langle\\widetilde{\\Sigma}^{-1/2}X_{i}X_{i}^{\\top}\\widetilde{\\Sigma}^{-1/2}-I,P\\rangle\\geq\\frac{|S|}{n}-0.2\\geq1-c-0.2\\geq0.7.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Now, we can show how to learn the covariance of $\\Sigma$ up to Frobenius error. ", "page_idx": 41}, {"type": "text", "text": "Lemma H.5. Fix any $\\eta\\in(0,\\eta_{0})$ , where $\\eta_{0}<0.01$ is a small universal constant, any $\\beta\\in(0,1)$ , and any $\\widetilde{\\cal O}(\\eta)\\le\\rho\\le\\sqrt{d}.$ . There is a (deterministic, possibly inefficient) algorithm $\\boldsymbol{A}_{2}$ with the following prop erty. Let $\\Sigma\\in\\mathbb{R}^{d\\times d}$ be any covariance matrix, and let $\\mathbf{X}=\\{X_{1},\\ldots,X_{n}\\}\\sim{\\mathcal{N}}(0,{\\boldsymbol{\\Sigma}})$ , where $\\begin{array}{r}{n\\geq O\\left(\\frac{d^{2}+\\log^{2}(1/\\beta)}{\\rho^{2}}+\\log(\\frac{1}{\\beta})\\right)}\\end{array}$ . Then, with probability at least $1-\\beta$ over the randomness of $\\mathbf{X}$ , for any $\\eta$ -corruption $\\mathbf{X^{\\prime}}=\\{X_{1}^{\\prime},\\ldots,X_{n}^{\\prime}\\}$ of $\\mathbf{X}$ , $A_{2}(\\mathbf{X}^{\\prime})$ outputs $\\hat{\\Sigma}_{2}$ such that $\\begin{array}{r}{\\|\\Sigma^{-1/2}\\hat{\\Sigma}_{2}\\Sigma^{-1/2}\\!-\\!I\\|_{F}\\leq}\\end{array}$ $O(\\rho)$ . Importantly, $\\boldsymbol{A}_{2}$ may have knowledge of $\\eta,\\,\\rho,$ , and $\\beta$ , but does not have knowledge of $\\mathbf{X}$ or $\\Sigma$ . ", "page_idx": 41}, {"type": "text", "text": "Proof. In the case that $\\rho\\leq2$ , the claim follows immediately from known results (for instance, it is implicit from [HKMN23]). ", "page_idx": 41}, {"type": "text", "text": "Alternatively, assume that $\\rho\\geq2$ . In this case, the algorithm works as follows. Assume $\\eta\\leq\\eta_{0}\\leq$ $c/2$ , where $c$ is the constant in Lemma H.3. First, compute $\\hat{\\Sigma}_{1}$ based on Lemma H.2. Note that $(\\mathrm{1}-O(\\eta))\\cdot\\Sigma\\,\\precnsim\\,\\hat{\\Sigma}_{1}\\,\\preccurlyeq\\,(1+O(\\eta))\\cdot\\Sigma$ with $1-\\beta$ probability, since the number of samples is sufficiently large. Now, find any positive definite $\\widetilde{\\Sigma}$ and a set $T\\subset[n]$ of size at least $(1-\\eta)n$ , such that: ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bullet\\,\\,(1-O(\\eta))\\cdot\\hat{\\Sigma}_{1}\\preccurlyeq\\widetilde{\\Sigma}\\preccurlyeq(1+O(\\eta))\\cdot\\hat{\\Sigma}_{1}.}\\\\ &{\\bullet\\,\\,\\mathrm{for~any}\\ S\\subset T\\mathrm{~of~size~at~least~}(1-2\\eta)n,\\,\\Big|\\frac{1}{n}\\sum_{i\\in S}\\langle\\widetilde{\\Sigma}^{-1/2}X_{i}X_{i}^{\\top}\\widetilde{\\Sigma}^{-1/2}-I,P\\rangle\\Big|\\le0.2.}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "First, we note that $\\widetilde{\\Sigma}=\\Sigma$ is a feasible choice of $\\widetilde{\\Sigma}$ . Indeed, the first condition trivially holds. For the second conditi on, let $T$ be the subset of unco rrupted data points (i.e., $X_{i}^{\\prime}=X_{i})$ ). Then, for any $S\\subset T$ , the data points $X_{i}^{\\prime}$ for $i\\in S$ are the same as $X_{i}$ , so by Lemma H.3, with $1-\\beta$ probability, for every such $S$ , $\\begin{array}{r}{\\left|\\frac{1}{n}\\sum_{i\\in S}\\langle\\Sigma^{-1/2}X_{i}X_{i}^{\\top}\\Sigma^{-1/2}-I,P\\rangle\\right|\\leq0.1}\\end{array}$ . ", "page_idx": 41}, {"type": "text", "text": "Next, we show that every $\\widetilde{\\Sigma}$ with $\\|\\Sigma^{-1/2}\\widetilde{\\Sigma}\\Sigma^{-1/2}-I\\|_{F}\\geq\\rho$ is infeasible. First, we may assume that $0.95\\Sigma\\prec\\widetilde{\\Sigma}\\preccurlyeq1.05\\Sigma$ , as otherwise, we cannot simultaneously satisfy $(1-O(\\eta))\\cdot\\hat{\\Sigma}_{1}\\preccurlyeq\\widetilde{\\Sigma}\\preccurlyeq$ $(1+O(\\eta))\\cdot\\hat{\\Sigma}_{1}$ and $\\left(1-O(\\eta)\\right)\\cdot\\Sigma\\prec\\hat{\\Sigma}_{1}\\preccurlyeq\\left(1+O(\\eta)\\right)\\cdot\\Sigma$ , assuming $\\eta\\le c/2$ is sufficiently small. Hence, we just have to verify the infeasibility for every $\\widetilde{\\Sigma}$ such that $\\|\\Sigma^{-1/2}\\widetilde{\\Sigma}\\Sigma^{-1/2}-I\\|_{F}\\geq\\rho$ and 0.95\u03a3 \u227c\u03a3  \u227c1.05\u03a3. Indeed, for any subset $T$ of size at least $(1-\\eta)n$ , let $S$ be the uncorrupted points in $T$ . Because there are at most \u03b7n uncorrupted points, $|S|\\ge(1-2\\eta)n$ . So by Lemma H.4, with $1-\\beta$ probability, for every such $S$ , $\\begin{array}{r}{\\left|\\frac{1}{n}\\sum_{i\\in S}\\langle\\widetilde{\\Sigma}^{-1/2}X_{i}X_{i}^{\\top}\\widetilde{\\Sigma}^{-1/2}-I,P\\rangle\\right|\\geq0.8}\\end{array}$ . ", "page_idx": 41}, {"type": "text", "text": "Therefore, with at most $O(\\beta)$ failure probability, some $\\hat{\\Sigma}_{2}\\;:=\\;\\widetilde{\\Sigma}$ is returned, and it satisfies $\\|\\Sigma^{-1/2}\\widetilde{\\Sigma}\\Sigma^{-1/2}-I\\|_{F}\\leq\\rho.$ . \u53e3 ", "page_idx": 42}, {"type": "text", "text": "Next, we note some results on robust mean estimation. We first note the definition of stability, and some properties. ", "page_idx": 42}, {"type": "text", "text": "Lemma H.6 ([DK22, Proposition 3.3]). Let $n~\\geq~O((d+\\log(1/\\beta))/\\alpha^{2})$ , for some $\\alpha\\ \\geq$ $O(\\eta\\sqrt{\\log{1/\\eta}})$ . Let $\\mathbf{X}\\ =\\ \\{X_{i}\\}_{i=1}^{n}\\stackrel{i.i.d.}{\\sim}\\mathcal{D},$ , where $\\mathcal{D}$ is a subgaussian random variable with mean $\\mu\\in\\mathbb{R}^{d}$ and covariance $I$ . Then, with probability $1-\\beta,$ , for all vectors $b\\in[0,1]^{n}$ such that $\\mathbb{E}_{i}b_{i}\\geq1-\\eta$ and all unit vectors $\\boldsymbol{v}\\in\\mathbb{R}^{d}$ , we have: ", "page_idx": 42}, {"type": "equation", "text": "$I.\\ |\\mathbb{E}_{i}b_{i}\\langle v,X_{i}-\\mu\\rangle|\\leq\\alpha.$ ", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Given a dataset $\\mathbf{X}$ with these properties, call it $(\\eta,\\alpha)$ -stable with respect to $\\mu$ . ", "page_idx": 42}, {"type": "text", "text": "Lemma H.7 (implicit from [DK22, Section 2]). Fix \u03b7 sufficiently small and $\\alpha=\\widetilde{O}(\\eta)$ . There is a deterministic algorithm $\\mathcal{A}_{3}$ that, on a dataset $\\mathbf{X^{\\prime}}$ , outputs $\\hat{\\mu}$ such that $\\|{\\hat{\\mu}}-\\mu\\|_{2}\\leq O(\\alpha),$ for any $\\eta$ -corruption $\\mathbf{X^{\\prime}}$ of any $\\mathbf{X}$ that is $(\\eta,\\alpha)$ -mean stable with respect to any $\\mu\\in\\mathbb{R}^{d}$ . Importantly, $\\mathcal{A}_{3}$ does not require knowledge of $\\mathbf{X}$ or $\\mu$ . ", "page_idx": 42}, {"type": "text", "text": "We now prove Theorem B.3. ", "page_idx": 42}, {"type": "text", "text": "Proof. We first show how to estimate the covariance $\\Sigma$ . First, we apply a \u201csample pairing\u201d trick (e.g., see [DK22, Section 4.4]). Namely, assume WLOG that $n$ is even, and define $\\tilde{\\mathbf{X}}$ to be the set $\\{(\\bar{X}_{2i-1}-X_{2i})/\\sqrt{2}\\}_{i=1}^{n/2}$ , and $\\tilde{\\mathbf{X}}^{\\prime}=\\{(X_{2i-1}^{\\prime}-X_{2i}^{\\prime})/\\sqrt{2}\\}_{i=1}^{n/2}$ . Note that $\\tilde{\\mathbf{X}}$ are i.i.d. samples from $\\mathcal{N}(0,\\Sigma)$ , and $\\tilde{\\mathbf{X}}^{\\prime}$ is at most $2\\eta$ -corrupted. ", "page_idx": 42}, {"type": "text", "text": "Now, because $\\eta\\le\\gamma$ , $\\tilde{\\mathbf{X}}^{\\prime}$ is at most $2\\gamma$ corrupted, so Lemma H.2 on $\\tilde{\\mathbf{X}}^{\\prime}$ (replacing $\\eta$ with $2\\gamma$ ) gives us some $\\hat{\\Sigma}_{1}$ such that $\\|\\Sigma^{-1/2}\\hat{\\Sigma}_{1}\\Sigma^{-1/2}-I\\|_{o p}\\leq O(\\gamma)$ , by our assumed bound on the number of samples. Next, Lemma H.5 on $\\tilde{\\mathbf{X}}^{\\prime}$ gives us some $\\hat{\\Sigma}_{2}$ such that $\\|\\Sigma^{-1/2}\\hat{\\Sigma}_{2}\\Sigma^{-1/2}-I\\|_{F}\\leq O(\\rho)$ , by our assumed bound on the number of samples. So, we can set $\\hat{\\Sigma}$ to be any covariance such that $\\|\\hat{\\Sigma}^{-1/2}\\hat{\\Sigma}_{1}\\hat{\\Sigma}^{-1/2}-I\\|_{o p}\\leq O(\\gamma)$ and $\\|\\hat{\\Sigma}^{-1/2}\\hat{\\Sigma}_{2}\\hat{\\Sigma}^{-1/2}-I\\|_{F}\\leq O(\\rho)$ . Note that $\\Sigma$ satisfies these properties, and any $\\hat{\\Sigma}$ that satisfies these properties must satisfy $\\|\\Sigma^{-1/2}\\hat{\\Sigma}\\Sigma^{-1/2}-I\\|_{F}\\leq O(\\gamma)$ and $\\|\\Sigma^{-1/2}\\hat{\\Sigma}\\Sigma^{-1/2}\\,-\\,I\\|_{F}\\;\\leq\\;O(\\rho)$ , by the approximate symmetry and transitivity properties (Proposition B.2). ", "page_idx": 42}, {"type": "text", "text": "Now, we estimate the mean $\\mu$ . Taking the original data $\\mathbf{X^{\\prime}}$ , we compute $\\{\\hat{\\Sigma}^{-1/2}X_{i}^{\\prime}\\}$ . By stability (Lemma H.7), we know that with $1-\\beta$ probability, $\\{\\Sigma^{-1/2}X_{i}\\}$ is $(\\eta,\\gamma)$ -stable with respect to $\\mu$ (where we are using the uncorrupted data and the true covariance $\\Sigma$ ). Letting $J=\\hat{\\Sigma}^{-1/2}\\Sigma^{1/2}$ , we know that $J$ has all singular values between $1-O(\\gamma)$ and $1+O(\\gamma)$ , and that $\\{J^{-1}\\cdot\\hat{\\Sigma}^{-1/2}X_{i}\\}$ is $(\\eta,\\gamma)$ -stable with respect to $\\mu$ . Moreover, note that we can write $\\langle v,\\hat{\\Sigma}^{-1/2}(X_{i}-\\mu)\\rangle=\\langle J v,J^{-1}$ \u00b7 $\\hat{\\Sigma}^{-1/2}(X_{i}\\!-\\!\\mu)\\rangle$ , and that $1\\!-\\!O(\\gamma)\\leq\\|J v\\|_{2}\\leq1\\!+\\!O(\\gamma)$ . Therefore, $\\{\\hat{\\Sigma}^{-1/2}X_{i}\\}$ is $(\\eta,O(\\gamma))$ -stable with respect to $\\hat{\\Sigma}^{-1/2}\\mu$ , which means that by Lemma H.7, $\\mathcal{A}_{3}$ on $\\{\\hat{\\Sigma}^{-1/2}X_{i}^{\\prime}\\}$ outputs some value $\\hat{\\nu}$ such that $\\|\\hat{\\nu}-\\hat{\\Sigma}^{-1/2}\\mu\\|_{2}\\leq O(\\gamma)$ . Thus, by setting $\\hat{\\mu}:=\\hat{\\Sigma}^{1/2}\\cdot\\hat{\\nu}$ , we have that $\\lVert\\hat{\\Sigma}^{-1/2}(\\hat{\\mu}-\\nu)\\rVert_{2}\\leq$ $O(\\gamma)$ , which means that $\\|\\Sigma^{-1/2}(\\hat{\\mu}-\\mu)\\|_{2}=\\|J^{-1}\\hat{\\Sigma}^{-1/2}(\\hat{\\mu}-\\mu)\\|_{2}\\leq(1+O(\\gamma))\\cdot\\|\\hat{\\Sigma}^{-1/2}(\\hat{\\mu}-\\mu)\\|_{2}.$ $\\mu)\\|_{2}\\leq O(\\gamma)$ . \u53e3 ", "page_idx": 42}, {"type": "text", "text": "H.3 Proof of Lemma C.7 ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "In this subsection, we prove Lemma C.7. ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Proof. First, note that for any positive definite matrices $\\Sigma_{1},\\Sigma_{2}$ , $\\begin{array}{r}{\\|\\Sigma_{2}^{-1/2}\\Sigma_{1}\\Sigma_{2}^{-1/2}-I\\|_{o p}\\leq\\gamma}\\end{array}$ is equivalent to $(1-\\gamma)I\\prec\\Sigma_{2}^{-1/2}\\Sigma_{1}\\Sigma_{2}^{-1/2}\\prec(1+\\gamma)I$ . Since $M$ being PSD implies $A M A^{\\top}$ is PSD ", "page_idx": 42}, {"type": "text", "text": "(and vice versa if $A$ is invertible), the previous statement is thus equivalent to $\\left(1-\\gamma\\right)\\cdot\\Sigma_{2}\\preccurlyeq\\Sigma_{1}\\preccurlyeq$ $(1+\\gamma)\\Sigma_{2}$ . Next, note that ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\Sigma_{2}^{-1/2}\\Sigma_{1}\\Sigma_{2}^{-1/2}-I\\|_{F}=\\sqrt{\\mathrm{Tr}\\left((\\Sigma_{2}^{-1/2}\\Sigma_{1}\\Sigma_{2}^{-1/2}-I)^{2}\\right)}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\sqrt{\\mathrm{Tr}\\left(\\Sigma_{2}^{-1/2}\\Sigma_{1}\\Sigma_{2}^{-1}\\Sigma_{1}\\Sigma_{2}^{-1/2}-2\\cdot\\Sigma_{2}^{-1/2}\\Sigma_{1}\\Sigma_{2}^{-1/2}+I\\right)}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\sqrt{\\mathrm{Tr}\\left(\\Sigma_{1}\\Sigma_{2}^{-1}\\Sigma_{1}\\Sigma_{2}^{-1}-2\\cdot\\Sigma_{1}\\Sigma_{2}^{-1}+I\\right)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where the first two lines are a straightforward expansion, and the final line simply uses the fact that $\\operatorname{Tr}(A B)=\\operatorname{Tr}(B A)$ for any matrices $A,B$ . Finally, note that ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\|\\Sigma_{2}^{-1/2}(\\mu_{1}-\\mu_{2})\\|_{2}=\\sqrt{(\\mu_{1}-\\mu_{2})^{\\top}\\Sigma_{2}^{-1}(\\mu_{1}-\\mu_{2})}.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Now, consider replacing $\\Sigma_{1}$ with $\\Sigma_{3}:=\\Sigma^{1/2}\\Sigma_{1}\\Sigma^{1/2}$ , $\\Sigma_{2}$ with $\\Sigma_{4}:=\\Sigma^{1/2}\\Sigma_{2}\\Sigma^{1/2}$ , $\\mu_{1}$ with $\\mu_{3}:=$ $\\Sigma^{1/2}\\mu_{1}+\\mu$ , and $\\mu_{2}$ with $\\mu_{4}:=\\Sigma^{1/2}\\mu_{2}+\\mu$ . Again, since $M$ being PSD implies $A M A^{\\top}$ is PSD (and vice versa), we have that $(1\\!-\\!\\gamma)\\!\\cdot\\!\\Sigma_{2}\\prec\\Sigma_{1}\\prec(1\\!+\\!\\gamma)\\!\\cdot\\!\\Sigma_{2}$ if and only if $(1\\!-\\!\\gamma)\\!\\cdot\\!\\Sigma_{4}\\prec\\Sigma_{3}\\prec(1\\!+\\!\\gamma)\\!\\cdot\\!\\Sigma_{4}$ . Moreover, note that ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\mathrm{Tr}(\\Sigma_{3}\\Sigma_{4}^{-1}\\Sigma_{3}\\Sigma_{4}^{-1})=\\mathrm{Tr}(\\Sigma^{1/2}\\Sigma_{1}\\Sigma_{2}^{-1}\\Sigma_{1}\\Sigma_{2}^{-1}\\Sigma^{-1/2})=\\mathrm{Tr}(\\Sigma_{1}\\Sigma_{2}^{-1}\\Sigma_{1}\\Sigma_{2}^{-1})\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "and ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{Tr}(\\Sigma_{3}\\Sigma_{4}^{-1})=\\mathrm{Tr}(\\Sigma^{1/2}\\Sigma_{1}\\Sigma_{2}^{-1}\\Sigma^{-1/2})=\\mathrm{Tr}(\\Sigma_{1}\\Sigma_{2}^{-1}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "which means that (4) would be the same if we replaced $\\Sigma_{1}$ with $\\Sigma_{3}$ and $\\Sigma_{2}$ with $\\Sigma_{4}$ . Finally, ", "page_idx": 43}, {"type": "text", "text": "( ${\\mu_{3}}-{\\mu_{4}})^{\\top}\\Sigma_{4}^{-1}({\\mu_{3}}-{\\mu_{4}})=({\\mu_{1}}-{\\mu_{2}})^{\\top}\\Sigma^{1/2}(\\Sigma^{-1/2}\\Sigma_{2}^{-1}\\Sigma^{-1/2})\\Sigma^{1/2}({\\mu_{1}}-{\\mu_{2}})=({\\mu_{1}}-{\\mu_{2}})^{\\top}\\Sigma_{2}^{-1}({\\mu_{1}}-{\\mu_{3}})$ \u00b52), which means that (4) would be the same if we replaced $\\mu_{1}$ with $\\mu_{3},\\mu_{2}$ with $\\mu_{4},\\Sigma_{1}$ with $\\Sigma_{3}$ , and $\\Sigma_{2}$ with $\\Sigma_{4}$ . ", "page_idx": 43}, {"type": "text", "text": "Overall, by the definition of $\\approx_{\\gamma,\\rho,\\tau}$ , we have that $(\\mu_{1},\\Sigma_{1})~\\approx_{\\gamma,\\rho,\\tau}~~(\\mu_{2},\\Sigma_{2})$ if and only if $\\left(\\mu_{3},\\Sigma_{3}\\right)\\approx_{\\gamma,\\rho,\\tau}\\left(\\mu_{4},\\Sigma_{4}\\right)$ . ", "page_idx": 43}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 44}, {"type": "text", "text": "Justification: We give complete proofs for all theorem statements. ", "page_idx": 44}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 44}, {"type": "text", "text": "Justification: We discuss limitations about the data assumptions and practical algorithms, at the end of the main body. ", "page_idx": 44}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 44}, {"type": "text", "text": "Justification: We give complete proofs for all theorems. ", "page_idx": 44}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] Justification: No experiments. ", "page_idx": 44}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 44}, {"type": "text", "text": "Justification: No experiments. ", "page_idx": 44}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 44}, {"type": "text", "text": "Justification: No experiments. ", "page_idx": 44}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 44}, {"type": "text", "text": "Justification: No experiments. ", "page_idx": 44}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 44}, {"type": "text", "text": "Justification: No experiments. ", "page_idx": 44}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: Our work is entirely theoretical, and we do not see any potential harms coming from our research. ", "page_idx": 45}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 45}, {"type": "text", "text": "Justification: Our work is entirely theoretical, and we do not see any potential negative societal impacts coming from our research. ", "page_idx": 45}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 45}, {"type": "text", "text": "Justification: Our work is entirely theoretical, and poses no such risks. ", "page_idx": 45}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 45}, {"type": "text", "text": "Justification: Our paper does not use any existing assets, such as code/data/models/etc. ", "page_idx": 45}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 45}, {"type": "text", "text": "Justification: Our paper does not release any new assets ", "page_idx": 45}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 45}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing or research with human subjects. ", "page_idx": 45}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 45}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 45}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing or research with human subjects. ", "page_idx": 45}]