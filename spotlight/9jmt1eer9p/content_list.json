[{"type": "text", "text": "Optimization Algorithm Design via Electric Circuits ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Stephen P. Boyd Tetiana Parshakova\u2217 Ernest K. Ryu Jaewook J. Suh\u2217 Stanford University Stanford University UCLA Rice University ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We present a novel methodology for convex optimization algorithm design using ideas from electric RLC circuits. Given an optimization problem, the first stage of the methodology is to design an appropriate electric circuit whose continuoustime dynamics converge to the solution of the optimization problem at hand. Then, the second stage is an automated, computer-assisted discretization of the continuous-time dynamics, yielding a provably convergent discrete-time algorithm. Our methodology recovers many classical (distributed) optimization algorithms and enables users to quickly design and explore a wide range of new algorithms with convergence guarantees. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In the classical literature of optimization theory, optimization algorithms are designed with the goal of establishing fast worst-case convergence guarantees. However, these methods, designed with the pessimistic framework of worst-case analysis, often exhibit slow practical performance. In the modern machine learning literature, optimizers are designed with the goal of obtaining fast empirical performance on a set of practical problems of interest. However, these methods, designed without consideration of the feasibility of a convergence analysis, tend to be much more difficult to analyze theoretically, and such methods sometimes even fail to converge under nice idealized assumptions such as convexity [92, 131]. ", "page_idx": 0}, {"type": "text", "text": "In this work, we present a novel methodology for convex optimization algorithm design using ideas from electric RLC (resistor-inductor-capacitor) circuits and the performance estimation problem [54, 149]. (To clarify, our proposal does not involve building a physical circuit.) Specifically, our methodology provides a quick and systematic recipe for designing new, provably convergent optimization algorithms, including distributed optimization algorithms. The ease of the methodology enables users to quickly explore a wide range of algorithms with convergence guarantees. ", "page_idx": 0}, {"type": "text", "text": "Optimization problem formulation. We consider the standard-form optimization problem ", "text_level": 1, "page_idx": 0}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\operatorname*{minimize}}&{{}f(\\boldsymbol{x})}\\\\ {\\mathrm{subject\\,to}}&{{}x\\in\\mathcal{R}(E^{\\intercal}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "where $x\\,\\in\\,\\mathbf{R}^{m}$ is the optimization variable, $f\\colon\\mathbf{R}^{m}\\rightarrow\\mathbf{R}\\cup\\{\\infty\\}$ is closed, convex, and proper, and $E\\,\\in\\,\\mathbf{R}^{n\\times m}$ . Assume we have $n$ nets $N_{1},\\ldots,N_{n}$ forming a partition of $\\{1,\\ldots,m\\}$ . More specifically, we let $E\\in\\mathbf{R}^{n\\times m}$ be a selection matrix defined as ", "page_idx": 0}, {"type": "equation", "text": "$$\nE_{i j}={\\left\\{\\begin{array}{l l}{+1}&{{\\mathrm{if~}}j\\in N_{i}}\\\\ {0}&{{\\mathrm{otherwise.}}}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "Our goal is to find a primal-dual solution satisfying the KKT conditions [126, Theorem 28.3] ", "page_idx": 0}, {"type": "equation", "text": "$$\ny\\in\\partial f(x),\\quad x\\in\\mathcal{R}(E^{\\intercal}),\\quad y\\in\\mathcal{N}(E).\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "As we show through examples, this standard-form problem (1) conveniently models many optimization problem setups of practical interest. ", "page_idx": 0}, {"type": "text", "text": "In the analysis and design of optimization algorithms, a standard approach is to consider a continuoustime model of a given algorithm, corresponding to the limit of small stepsizes [124, 79, 6, 155, 142, 93, 86, 136]. Our work is based on the key observation that such continuous-time models can be interpreted as RLC circuits connected to the subdifferential operator $\\partial f$ , which we interpret as a nonlinear resistor. We expand on this observation and propose a general methodology for designing optimization algorithms by designing RLC circuits that relax to the nets defined by $E$ . ", "page_idx": 1}, {"type": "text", "text": "Example. Problem (1) represents a general form of distributed optimization, where the constraints enforce consensus among the primal variables. An example is the so-called consensus problem [30, $\\S7]$ ", "page_idx": 1}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}{\\operatorname{minimize}\\ }&{f_{1}(x_{1})+\\cdot\\cdot\\cdot+f_{N}(x_{N})}\\\\ {x_{1},\\ldots,x_{N}\\!\\in\\!\\mathbf{R}^{m/N}}&{x_{1}=\\cdot\\cdot\\cdot=x_{N},}\\end{array}}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\boldsymbol{x}\\,=\\,(x_{1},\\ldots,x_{N})$ is the decision variable, the objective function $f(x)\\,=\\,f_{1}(x_{1})+\\cdots+$ $f_{N}(x_{N})$ is block-separable, and $E^{\\boldsymbol{\\mathsf{T}}}=(I,\\ldots,I)\\in\\mathbf{R}^{m\\times m/N}$ . Refer to sections $\\S E$ and $\\S\\mathrm{F}$ , for an overview of classical splitting methods and decentralized methods for solving (1). ", "page_idx": 1}, {"type": "text", "text": "1.1 Preliminaries and Notation ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We generally follow the standard definitions and notations of convex optimization [31, 118, 23, 119, 129]. Consider the extended-valued function $f\\colon\\mathbf{R}^{n}\\rightarrow\\mathbf{R}\\cup\\{\\infty\\}$ . We say $f$ is closed if its epigraph is closed set in $\\mathbf{R}^{n+1}$ and proper if its value is finite somewhere. We say $f$ is CCP if it is closed, convex, and proper. For $R>0$ , we say $f$ is $R$ -smooth if $f$ is finite and differentiable everywhere and $\\|\\nabla f(\\bar{x})\\,\\bar{-}\\,\\nabla f(y)\\|\\;\\leq\\;R\\|x-\\bar{y}\\|$ for all $x,y\\,\\in\\,\\mathbf{R}^{n}$ . For $\\mu\\,>\\,0$ , we say $f$ is $\\mu$ -strongly convex if $f(x)-(\\mu/2)\\|x\\|^{2}$ is convex. Let $f^{*}(y)=\\operatorname*{sup}_{x\\in\\mathbf{R}^{n}}\\left\\{\\langle y,x\\rangle-f(x)\\right\\}$ denote the Fenchel conjugate of $f$ . For $R\\,>\\,0$ and a CCP $f$ , define the $R$ -Moreau envelope of $f$ as $\\textstyle R f(x)=\\operatorname*{inf}_{z\\in\\mathbf{R}^{n}}$ $\\begin{array}{r}{\\{f(z)+\\frac{1}{2R}\\|z-x\\|^{2}\\}}\\end{array}$ . One can show [23, Proposition 13.24] that the $R$ -Moreau envelope is given by $\\begin{array}{r}{^{R}\\!f=\\left(f^{*}+\\frac{R}{2}\\|\\cdot\\|^{2}\\right)^{*}}\\end{array}$ . If $f$ is $1/R$ -smooth, we can define [23, Theorem 18.15] the $R$ -pre-Moreau envelope of $f$ as ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{f}=\\left(f^{*}-\\frac{R}{2}\\|\\cdot\\|^{2}\\right)^{*},}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "which is defined such that ${\\boldsymbol{R}}({\\boldsymbol{\\tilde{f}}})={\\boldsymbol{f}}$ . ", "page_idx": 1}, {"type": "text", "text": "Due to the limited space, we defer the review of prior works to $\\S\\mathrm{A}$ of the appendix. ", "page_idx": 1}, {"type": "text", "text": "1.2 Contributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Our work presents two technical novelties, one in continuous time and the other in discrete time. The first is the observation that many standard optimization algorithms can be interpreted as discretizations of electric RLC circuits connected to the subdifferential operator $\\partial f$ . The second is the use of the performance estimation problem to obtain an automated recipe for discretizing convergent continuoustime dynamics into convergent discrete-time algorithms, and we provide code implementing our automatic discretization methodology. ", "page_idx": 1}, {"type": "text", "text": "By combining these two insights, we provide a quick and systematic methodology for designing new, provably convergent optimization algorithms, including distributed optimization algorithms. We provide an open-source package that implements automatic discretization of our circuits: ", "page_idx": 1}, {"type": "text", "text": "2 Continuous-time optimization with circuits ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Interconnects ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We now describe two types of electric circuits that we call static and dynamic interconnects. Both interconnects have $m$ terminals, and we will later connect them to the $m$ inputs of $\\partial f$ . ", "page_idx": 1}, {"type": "text", "text": "Static interconnect. The static interconnect is a set of (ideal) wires connecting $m$ terminals and forming $n$ nets. See Figure 1 for an example. Let $\\boldsymbol{x}\\in\\mathbb{R}^{m}$ be a vector of terminal potentials and $y\\in$ $\\mathbf{R}^{m}$ be a vector of currents leaving the terminals. Using matrix $E\\in\\mathbf{R}^{n\\times m}$ as defined in (2), we can express Kirchhoff\u2019s voltage law (KVL) as $x\\in\\mathcal{R}(E^{\\intercal})$ and Kirchhoff\u2019s current law (KCL) as $y\\in\\mathcal{N}(E)$ . In other words, the static interconnect enforces the ", "page_idx": 2}, {"type": "text", "text": "V-I relationship ", "page_idx": 2}, {"type": "equation", "text": "$$\n(x,y)\\in\\mathcal{R}(E^{\\intercal})\\times\\mathcal{N}(E).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "image", "img_path": "9Jmt1eER9P/tmp/7c4c13dd9fc47b9306e38f834854aa6fb46b284e15e507486e02cdc410b6a64c.jpg", "img_caption": ["Static interconnect ", "Figure 1: Example of a static interconnect, $m=5$ , $N_{1}=\\{1,3\\}$ , $N_{2}=\\{2,4\\}$ , $N_{3}=\\{5\\}$ . "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Dynamic interconnect. The dynamic interconnect is an RLC circuit with $m$ terminals and 1 ground node. We assume all inductances and capacitances have values in $(0,\\infty)$ while the resistances have values in $[0,\\infty)$ . (A 0-ohm resistor is an ideal wire. We do not permit ideal wire loop.) Each RLC component has two (scalar-valued) terminals: the $^+$ and terminals. ", "page_idx": 2}, {"type": "text", "text": "Denote the number of nodes in the RLC circuit by $\\tau$ . Connect nodes $1,2,\\ldots,m$ to terminals $1,2,\\ldots,m$ , and let the last node, node $\\tau$ , be the ground node. (This implies $\\tau\\geq m+1.$ ) Denote the number of RLC components by $\\sigma$ . We describe the topology with a reduced node incidence matrix (with the bottom row corresponding to the ground node removed) $A\\in\\mathbf{R}^{(\\tau-1)\\times\\sigma}$ defined as ", "page_idx": 2}, {"type": "text", "text": "+1 if node $i$ connects to $^+$ terminal of component $j$ 1 if node $i$ connects to \u2212terminal of component $j$ 0 otherwise. ", "page_idx": 2}, {"type": "text", "text": "See Figure 2 for an example. ", "page_idx": 2}, {"type": "table", "img_path": "9Jmt1eER9P/tmp/953fa07150accef3bbc3d4a6a90a59375d5425ac99a0f16b04652ea232885c41.jpg", "table_caption": [], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "Figure 2: Example of a dynamic interconnect with $\\tau=8$ nodes, $\\sigma=7$ RLC components, $m=5$ terminals, and 1 ground node. Reduced node incidence matrix $A$ is provided. $R_{2}$ and $R_{3}$ are 0-ohm resistors.) This dynamic interconnect is admissible with respect to the static interconnect of Figure 1. ", "page_idx": 2}, {"type": "text", "text": "The ground node is designated to have 0 potential, and the potential of any node is the potential relative to ground. The voltage across a component is the difference of potentials between the $^+$ and \u2212terminals. The current through a component is defined as the current flowing from the $^+$ terminal to the \u2212terminal. ", "page_idx": 2}, {"type": "text", "text": "Let $\\boldsymbol{x}\\in\\mathbb{R}^{m}$ be the potentials at the $m$ terminals, which are connected to nodes $1,\\ldots,m$ , and $y\\in\\mathbf{R}^{m}$ be the currents leaving the terminals. Denote the node potential vector with the ground node excluded (since the potential at ground is 0) by ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\left[\\stackrel{x}{e}\\right]\\in\\mathbf{R}^{\\tau-1}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "So, $e\\in\\mathbf{R}^{\\tau-1-m}$ denotes the potentials at the non-terminal nodes. Denote the vector of voltages by $v\\in\\mathbb{R}^{\\sigma}$ and the vector of currents by $i\\in\\mathbf{R}^{\\sigma}$ . Then, the currents and voltages of the dynamic ", "page_idx": 2}, {"type": "text", "text": "interconnect satisfy the following V-I relations ", "page_idx": 3}, {"type": "text", "text": "(iii) ${\\begin{array}{r l}&{{\\mathrm{(i)}}\\ A i=\\left[{\\begin{array}{l l l}{-y}\\\\ {0}\\end{array}}\\right]\\quad{\\mathrm{(KCL)}}\\quad{\\mathrm{(ii)}}\\ v=A^{\\top}\\left[{\\underline{{x}}}\\right]\\quad{\\mathrm{(KVL)}}}\\\\ {\\qquad}&{{\\mathrm{(v)}}\\ v_{\\mathcal{R}}=D_{\\mathcal{R}}i_{\\mathcal{R}}\\quad{\\mathrm{(Resistor)}}\\quad\\quad{\\mathrm{(iv)}}\\ v_{\\mathcal{L}}=D_{\\mathcal{L}}{\\frac{d}{d t}}i_{\\mathcal{L}}\\quad{\\mathrm{(Inductor)}}\\quad\\quad{\\mathrm{(v)}}\\ i_{\\mathcal{C}}=D_{\\mathcal{C}}{\\frac{d}{d t}}v_{\\mathcal{C}}}\\end{array}}$ (Capacitor) where $D_{\\mathcal{R}},\\ D_{\\mathcal{L}}$ , and $D_{\\mathcal{C}}$ are diagonal matrices respectively with resistances, inductances, and capacitances values in the diagonals. ", "page_idx": 3}, {"type": "text", "text": "Admissibility. When an RLC circuit reaches equilibrium, voltages across inductors and currents through capacitors are 0. We say a dynamic interconnect is admissible if it relaxes to the static interconnect at equilibrium. Mathematically, this condition is expressed as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\left\\{(x,y)\\,{\\Big|}\\,A i={\\left[\\!\\!\\begin{array}{l}{-y}\\\\ {0}\\end{array}\\!\\!\\right]}\\,,v=A^{\\mathsf{T}}\\left[\\!\\!\\begin{array}{l}{x}\\\\ {e}\\end{array}\\!\\!\\right],v_{\\mathcal{R}}=D_{\\mathcal{R}}i_{\\mathcal{R}},v_{\\mathcal{L}}=0,i_{\\mathcal{C}}=0\\right\\}=\\mathcal{R}(E^{\\mathsf{T}})\\times\\mathcal{N}(E).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "As an example, the dynamic interconnect of Figure 2 is admissible with respect to the static interconnect of Figure 1. ", "page_idx": 3}, {"type": "text", "text": "2.2 Composing interconnects with $\\partial f$ ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We view the subdifferential operator $\\partial f$ as an $m$ -terminal electric device that is also grounded. Let $\\boldsymbol{x}\\in\\mathbb{R}^{m}$ be the potentials at the $m$ terminals (excluding ground) and $y\\in\\mathbf{R}^{m}$ be the currents flowing into the $m$ terminals. The $\\partial f$ operator enforces the V-I relation ", "page_idx": 3}, {"type": "equation", "text": "$$\ny\\in\\partial f(x).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We connect the $m$ terminals of $\\partial f$ to the $m$ terminals of the static and dynamic interconnects. Immediately, connecting the static interconnect with $\\partial f$ enforces the V-I relations (4) and $y\\in\\partial f(x)$ , which combine to be the optimality condition (3). Therefore, the potentials at the $m$ terminals as a vector in $\\mathbf{R}^{m}$ is an optimal $x^{\\star}\\in\\mathbf{R}^{m}$ solving (1). To clarify, connecting the static interconnect with $\\partial f$ leads to a static circuit in the sense that the potential $x$ and current $y$ do not depend on time. ", "page_idx": 3}, {"type": "table", "img_path": "9Jmt1eER9P/tmp/a717d0aa97b667e9ae07763e3994938f9fe5c83bfce686715d4fe368e4f96580.jpg", "table_caption": [], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 3: The static interconnect of Figure 1 connected with $\\partial f$ . The potentials at the $m$ terminals is an optimal $x^{\\star}\\in\\mathbf{R}^{m}$ solving (1). ", "page_idx": 3}, {"type": "text", "text": "Next, we compose (connect) the dynamic interconnect with $\\partial f$ . Due to capacitors and inductors, this circuit is dynamic in the sense that the voltages $v(t)$ and $x(t)$ and currents $i(t)$ and $y(t)$ depend on time, although we often omit explicitly writing the $t$ -dependence for notational convenience. Then, the V-I relations of the dynamic interconnect combined with $y\\in\\partial f(x)$ leads to the V-I relation ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\left\\lbrace(v,i)\\,\\middle\\vert\\,y\\in\\partial f(x),~A i=\\left[\\!\\!\\begin{array}{l}{-y}\\\\ {0}\\end{array}\\!\\!\\right],~v=A^{\\mathsf{T}}\\left[\\!\\!\\begin{array}{l}{x}\\\\ {e}\\end{array}\\!\\!\\right],~}}\\\\ &{}&{v_{\\mathcal{R}}=D_{\\mathcal{R}}i_{\\mathcal{R}},~v_{\\mathcal{L}}=D_{\\mathcal{L}}\\frac{d}{d t}i_{\\mathcal{L}},~i_{\\mathcal{C}}=D_{\\mathcal{C}}\\frac{d}{d t}v_{\\mathcal{C}},~t\\in(0,\\infty)\\right\\rbrace,}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $v(t)\\,=\\,(v_{\\mathcal{R}}(t),v_{\\mathcal{L}}(t),v_{\\mathcal{C}}(t))\\,\\in\\,{\\bf R}^{\\sigma},\\,i(t)\\,=\\,(i_{\\mathcal{R}}(t),i_{\\mathcal{L}}(t),i_{\\mathcal{C}}(t))\\,\\in\\,{\\bf R}^{\\sigma},\\,e(t)\\,\\in\\,{\\bf R}^{\\tau-m-1},$ , $\\boldsymbol{x}(t)\\in\\mathbb{R}^{m}$ , and $y(t)\\in\\mathbf{R}^{m}$ for $t\\in[0,\\infty)$ . ", "page_idx": 3}, {"type": "image", "img_path": "9Jmt1eER9P/tmp/f8d3230393a1317bbf5edd5e0580ead222a8b48ef45739807114e8baec6bfe16.jpg", "img_caption": ["Dynamic interconnect "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 4: The dynamic interconnect of Figure 2 connected with $\\partial f$ . The potentials at the $m$ terminals satisfy $x(t)\\to\\bar{x}^{\\star}$ for an optimal $x^{\\star}\\in\\mathbf{R}^{\\bar{m}}$ solving (1) under the conditions of Theorem 2.2. ", "page_idx": 4}, {"type": "text", "text": "Under appropriate conditions, the dynamics (5) is mathematically well-posed in the sense that there exist unique Lipschitz-continuous curves $v(t),i(t),x(t)$ , and $y(t)$ satisfying the V-I relation (5) as formalized in the following Theorem 2.1. The proof, which utilizes the machinery of monotone operator theory [21, 23, 129], is provided in $\\S B$ of the appendix. ", "page_idx": 4}, {"type": "text", "text": "Theorem 2.1. Assume $f$ is $\\mu$ -strongly convex and $M$ -smooth. Suppose $(v^{0},i^{0},x^{0},y^{0})$ satisfy ", "page_idx": 4}, {"type": "equation", "text": "$$\nA i^{0}=\\left[\\!\\!\\begin{array}{c}{{-y^{0}}}\\\\ {{0}}\\end{array}\\!\\!\\right],\\quad v^{0}=A^{\\mathsf{T}}\\left[\\!\\!\\begin{array}{c}{{x^{0}}}\\\\ {{e}}\\end{array}\\!\\!\\right],\\quad v_{\\mathcal{R}}^{0}=D_{\\mathcal{R}}i_{\\mathcal{R}}^{0},\\quad y^{0}=\\nabla f(x^{0}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Then there is a unique Lipschitz continuous curve $(v,i,x,y)\\colon[0,\\infty)\\to\\mathbf{R}^{\\sigma}\\times\\mathbf{R}^{\\sigma}\\times\\mathbf{R}^{m}\\times\\mathbf{R}^{m}$ satisfying the conditions in (5) and the initial condition $(v(0),i(0),x(0),y(0))=(v^{0},i^{0},x^{0},y^{0}).$ ", "page_idx": 4}, {"type": "text", "text": "Equillibrium yields a primal-dual solution. With the dynamic interconnect composed with $\\partial f$ , we generically expect the circuit state $(v(t),i(t),x(t),y(t))$ to converge (relax) to an equilibrium state. The admissibility condition ensures that at such an equilibrium, $(x,y)$ will be a primal-dual solution. We formally state this fact as Theorem C.2 of the Appendix. ", "page_idx": 4}, {"type": "text", "text": "2.3 Energy dissipation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Let $(v^{\\star},i^{\\star},x^{\\star},y^{\\star})$ be an equilibrium of an admissible dynamic interconnect composed with $\\partial f$ . Since the voltages across resistors and inductors and the currents through capacitors are zero under equilibrium, we have ", "page_idx": 4}, {"type": "equation", "text": "$$\nv^{\\star}=(v_{\\mathcal{R}}^{\\star},v_{\\mathcal{L}}^{\\star},v_{\\mathcal{C}}^{\\star})=(0,0,v_{\\mathcal{C}}^{\\star}),\\qquad i^{\\star}=(i_{\\mathcal{R}}^{\\star},i_{\\mathcal{L}}^{\\star},i_{\\mathcal{C}}^{\\star})=(0,i_{\\mathcal{L}}^{\\star},0).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "(We formally show this in Theorem C.2 of the appendix.) Define the energy of the circuit at time $t$ as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{E}(t)=\\frac{1}{2}\\|v_{C}(t)-v_{C}^{\\star}\\|_{D_{C}}^{2}+\\frac{1}{2}\\|i_{C}(t)-i_{C}^{\\star}\\|_{D_{C}}^{2},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "which is a dissipative (non-increasing) quantity: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{d}{d t}\\mathcal{E}=\\langle v_{\\mathcal{C}}-v_{\\mathcal{C}}^{\\star},i_{\\mathcal{C}}-i_{\\mathcal{C}}^{\\star}\\rangle+\\langle i_{\\mathcal{L}}-i_{\\mathcal{L}}^{\\star},v_{\\mathcal{L}}-v_{\\mathcal{L}}^{\\star}\\rangle}\\\\ {\\displaystyle=-\\|i_{\\mathcal{R}}\\|_{D_{\\mathcal{R}}}^{2}-\\langle x-x^{\\star},y-y^{\\star}\\rangle\\leq0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Here, we use $i_{\\mathcal{C}}^{\\star}=0$ and $v_{\\mathcal{L}}^{\\star}=0$ and the fact that the power dissipated by the resistors and $\\partial f$ must come from the energy stored in the capacitors and inductors. This dissipativity property leads to the following continuous-time convergence. ", "page_idx": 4}, {"type": "text", "text": "Theorem 2.2. Assume $f\\colon\\mathbf{R}^{m}\\rightarrow\\mathbf{R}$ is strongly convex and smooth. Assume the dynamic interconnect is admissible, and let $(x^{\\star},y^{\\star})$ be a primal-dual solution pair. Let $(v(t),i(t),x(t),y(t))$ be a curve satisfying (5). Then, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{t\\to\\infty}(x(t),y(t))=(x^{\\star},y^{\\star}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Theorem 2.2 largely follows as a corollary of Theorem 2.1. The formal proof is provided in $\\S D$ of the appendix. In $\\S4$ , we present a systematic framework for finding discretized versions of Theorem 2.2 the corresponding discretized algorithms. ", "page_idx": 4}, {"type": "text", "text": "3 Circuits for classical algorithms ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we present circuits recovering the classical Nesterov acceleration, decentralized ADMM, and PG-EXTRA. For additional examples and detailed derivations, refer to $\\S E$ and $\\S\\mathrm{F}$ of the appendix, where we provide circuits and analyses of classical algorithms such as gradient descent [35], proximal point method [125], proximal gradient method [42], primal decomposition [72, 139], dual decomposition [59, 96, 63, 141], Douglas\u2013Rachford splitting [123, 53, 101], Davis\u2013Yin splitting [46], decentralized gradient descent [116, 170], and diffusion [33, 34]. ", "page_idx": 5}, {"type": "text", "text": "Multi-wire notation. We start by quickly introducing the multiwire notation depicted in Figure 5. When optimizing $f\\!\\!\\!\\::\\!\\!\\mathbf{R}^{m}\\rightarrow\\mathbf{R}$ and using the $m$ -terminal device $\\partial f$ , we will often use dynamic interconnects that have the same RLC circuit across each net, i.e., the dynamic interconnect consists of $m$ identical copies of the same RLC circuit for the $m$ coordinates of $\\boldsymbol{x}\\in\\mathbb{R}^{m}$ . In this case, we use the diagonal-line notation depicted in Figure 5. ", "page_idx": 5}, {"type": "image", "img_path": "9Jmt1eER9P/tmp/679db39e333cbc27abbc35dc386cf3952b868870c651fadbc676fd6cae89b0b2.jpg", "img_caption": ["Figure 5: Multi-wire notation. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Moreau envelope. We use the following simple identity throughout this work: $\\partial f$ composed with a resistor is equivalent to $\\nabla^{R}f(x)$ . ", "page_idx": 5}, {"type": "image", "img_path": "9Jmt1eER9P/tmp/d2f4afbebc8ea34a6fe049437a3f6a94d404d167b59a315b2211087ab09d26cf.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "To clarify, the equivalence means the two circuits impose the same V-I relation on the $m$ pins of $x$ . To see this, note $\\begin{array}{r}{\\left[\\partial f(\\tilde{x})=\\frac{1}{R}(x-\\tilde{x})\\right]\\Leftrightarrow\\left[\\tilde{x}=\\mathbf{prox}_{R f}^{.}(x)\\right]}\\end{array}$ and use the identity for the gradient of the Moreau envelope to conclude ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\nabla^{R}f(x)={\\frac{1}{R}}(x-\\mathbf{prox}_{R f}(x))={\\frac{1}{R}}(x-{\\tilde{x}}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "See $\\S E.1$ of the appendix for further details. ", "page_idx": 5}, {"type": "text", "text": "3.1 Nesterov acceleration ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Let $f\\colon\\mathbf{R}^{m}\\rightarrow\\mathbf{R}$ be a $1/R$ -smooth convex function. Then, the circuit corresponding to the classical Nesterov acceleration is given below. ", "page_idx": 5}, {"type": "image", "img_path": "9Jmt1eER9P/tmp/54f32af567df00db5f718751964fc5f59d1ce91b60972a0e002e2174a993316d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "The use of a negative resistor $-R$ may seem unconventional, but the fact that this circuit is stable is easier to see if we consider the equivalent circuit with the pre-Moreau envelope $\\tilde{f}$ , i.e., $\\tilde{f}$ is the convex function such that $R\\tilde{f}=f$ . To clarify, negative resistors satisfy the same V-I relations of the standard resistors but with a negative slope. Negative resistors have also been considered in [152]. ", "page_idx": 5}, {"type": "text", "text": "The V-I relations of this circuit lead to the ODE ", "page_idx": 5}, {"type": "equation", "text": "$$\n{\\frac{d^{2}}{d t^{2}}}x+{\\frac{R}{L}}{\\frac{d}{d t}}x+\\left({\\frac{1}{C}}-{\\frac{R^{2}}{L}}\\right){\\frac{d}{d t}}\\nabla f(x)+{\\frac{R}{L C}}\\nabla f(x)=0.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "If we set $R=\\sqrt{L/C}$ , which can be interpreted as an instance of critical damping [163, 173, 40], $\\begin{array}{r}{L=\\frac{1}{8\\mu\\sqrt{\\mu}}}\\end{array}$ 8\u00b51\u221a\u00b5, and C = 2\u221a\u00b5, we recover the Nesterov ODE [161] ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\frac{d^{2}}{d t^{2}}x+2\\sqrt{\\mu}\\frac{d}{d t}x+\\nabla f(x)=0.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We also quickly point out that other choices of parameters lead to the high-resolution ODE introduced in [135]. See $\\S E.3$ of the appendix for further details. ", "page_idx": 5}, {"type": "text", "text": "3.2 Decentralized ADMM ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Let $f_{1},\\ldots,f_{N}\\colon\\mathbf{R}^{m}\\rightarrow\\mathbf{R}\\cup\\{\\infty\\}$ be CCP functions. Consider a decentralized optimization setup with graph $G$ . We provide the full description of the decentralized setup and notations in $\\S\\mathrm{F}$ of the appendix. Define $\\Gamma_{j}$ to be the neighbors of $j$ in graph $G$ . For simplicity, we only illustrate the circuit related to nodes $j$ and $l$ , where $j$ and $l$ are directly connected through an edge in the graph $G$ . ", "page_idx": 6}, {"type": "text", "text": "The circuit corresponding to decentralized ADMM [74, 71, 70, 156, 138] is given below. ", "page_idx": 6}, {"type": "image", "img_path": "9Jmt1eER9P/tmp/192ee015609571a038705c85261d47d11ef9ec80889b9915519d5172a5254771.jpg", "img_caption": [], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "In the following, the left column presents the dynamics of the continuous-time circuit and the right column presents the discretization with stepsize $L/R$ , recovering the standard decentralized ADMM: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{c c}{\\displaystyle a_{j}=\\frac{1}{|\\Gamma_{j}|}\\sum_{l\\in\\Gamma_{j}}(R i c_{j l}+e_{j l})}\\\\ {\\displaystyle x_{j}=\\mathbf{prox}_{(R/|\\Gamma_{j}|)f_{j}}\\left(a_{j}\\right)}\\\\ {\\displaystyle e_{j l}=\\frac{1}{2}(x_{j}+x_{l})}\\\\ {\\displaystyle\\frac{d}{d t}i_{\\mathcal{L}j l}=\\frac{1}{L}(e_{j l}-x_{j})}\\end{array}\\left|\\begin{array}{c c}{\\displaystyle a_{j}^{k+1}=\\frac{1}{|\\Gamma_{j}|}\\sum_{l\\in\\Gamma_{j}}(R i_{\\mathcal{L}j,l}^{k}+e_{j l}^{k})}\\\\ {\\displaystyle x_{j}^{k+1}=\\mathbf{prox}_{(R/|\\Gamma_{j}|)f_{j}}\\left(a_{j}^{k+1}\\right)}\\\\ {\\displaystyle e_{j l}^{k+1}=\\frac{1}{2}(x_{j}^{k+1}+x_{l}^{k+1})}\\\\ {\\displaystyle i_{\\mathcal{L}}^{k+1}{}_{j l}=i_{\\mathcal{L}j l}^{k}+\\frac{1}{R}(e_{j l}^{k+1}-x_{j}^{k+1})}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "for every node $j=1,\\ldots,N$ and every edge $(j,l)$ in graph $G$ . ", "page_idx": 6}, {"type": "text", "text": "3.3 PG-EXTRA ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Let $f_{1},\\ldots,f_{N}\\colon\\mathbf{R}^{m}\\rightarrow\\mathbf{R}\\cup\\{\\infty\\}$ be CCP functions and $h_{1},\\hdots,h_{N}\\colon\\mathbf{R}^{m}\\rightarrow\\mathbf{R}$ be convex $M$ -smooth functions. Consider a decentralized optimization setup with graph $G$ . The circuit corresponding to PG-EXTRA [137] is given below. ", "page_idx": 6}, {"type": "image", "img_path": "9Jmt1eER9P/tmp/b0c1f7bdedac16a53e57330fc71eaae9d227ef6bed0a21dc0017ccf77b6d8a69.jpg", "img_caption": [], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Define the mixing matrix $W\\in\\mathbf{R}^{N\\times N}$ with ", "page_idx": 6}, {"type": "equation", "text": "$$\nW_{j l}=\\left\\{\\begin{array}{l l}{1-\\sum_{l\\in\\Gamma_{j}}\\frac{R}{R_{j l}}}&{\\mathrm{if~}j=l}\\\\ {\\frac{R}{R_{j l}}}&{\\mathrm{if~}j\\neq l,\\quad l\\in\\Gamma_{j}}\\\\ {0}&{\\mathrm{otherwise}.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "In the following, the left column presents the V-I relations for the continuous-time circuit and the right column presents the discretization with stepsize $\\frac{1}{2}$ , recovering the standard PG-EXTRA: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle x_{j}=\\mathbf{prox}_{R f_{j}}\\left(\\sum_{l=1}^{N}W_{j l}x_{l}-R\\nabla h_{j}(x_{j})-w_{j}\\right)}&{\\left|\\displaystyle x_{j}^{k+1}=\\mathbf{prox}_{R f_{j}}\\left(\\sum_{l=1}^{N}W_{j l}x_{l}^{k}-R\\nabla h_{j}(x_{j}^{k})-w_{j}^{k}\\right)\\right|}\\\\ {\\displaystyle\\frac{d}{d t}w_{j}=x_{j}-\\sum_{l=1}^{N}W_{j l}x_{l}}&{w_{j}^{k+1}=w_{j}^{k}+\\frac{1}{2}(x_{j}^{k}-\\sum_{l=1}^{N}W_{j l}x_{l}^{k})}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "for every node $j=1,\\ldots,N$ and every edge $(j,l)$ in graph $G$ . ", "page_idx": 7}, {"type": "text", "text": "4 Automatic discretization ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We discretize the continuous-time dynamics given by the circuit with an admissible dynamic interconnect using a two-stage Runge\u2013Kutta method with parameters $\\alpha,\\beta$ and stepsize $h>0$ . The explicit form of the discretization is stated in $\\S\\mathrm{G}$ of the appendix. Let $\\{(v^{k},i^{k},x^{\\hat{k}},y^{k})\\}_{k=1}^{\\infty}$ be the iterates generated by the discretized algorithm. Then the energy stored in the circuit at time $t=k h$ is ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathcal{E}_{k}=\\frac{1}{2}\\|v_{\\mathcal{C}}^{k}-v_{\\mathcal{C}}^{\\star}\\|_{D_{\\mathcal{C}}}^{2}+\\frac{1}{2}\\|i_{\\mathcal{L}}^{k}-i_{\\mathcal{L}}^{\\star}\\|_{D_{\\mathcal{L}}}^{2}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "To guarantee convergence of the discretized algorithm, we search for discretization parameters that ensure the $\\mathcal{E}_{1},\\mathcal{E}_{2},\\ldots$ sequence is dissipative in the following sense. Specifically, we say the algorithm or the discretization is sufficiently dissipative if there is an $\\eta>0$ such that ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathcal{E}_{k+1}-\\mathcal{E}_{k}+\\eta\\langle x^{k}-x^{\\star},y^{k}-y^{\\star}\\rangle\\leq0,\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "holds for all $k=1,2,\\ldots$ This requirement is analogous to the \u201csufficient decrease\u201d conditions in optimization [31, 121]. The following Lemma 4.1, which proof we provide in $\\S\\mathrm{G}$ of the appendix, states that sufficient dissipativity ensures convergence under suitable conditions. ", "page_idx": 7}, {"type": "text", "text": "Lemma 4.1. Assume $f\\colon\\mathbf{R}^{m}\\rightarrow\\mathbf{R}\\cup\\{\\infty\\}$ is a strictly convex function and the dynamic interconnect is admissible. If the two-stage Runge\u2013Kutta discretization, as explicitly stated in $\\S G$ of the appendix, generates a discrete-time sequence $\\{(v^{k},i^{k},x^{k},y^{k})\\}_{k=1}^{\\infty}$ satisfying the sufficient dissipativity condition (8), then $x^{k}$ converges to a primal solution. ", "page_idx": 7}, {"type": "text", "text": "We find such a discretization with the following automated methodology. Given a discretization characterized by $(\\alpha,\\beta,h)$ , the dissipativity condition (8) for a given $\\eta>0$ is implied if the optimal value of the following optimization problem is non-positive: ", "page_idx": 7}, {"type": "text", "text": "subject to maximize $\\begin{array}{r l}&{\\mathcal{E}_{2}-\\mathcal{E}_{1}+\\eta\\langle x^{1}-x^{\\star},y^{1}-y^{\\star}\\rangle}\\\\ &{\\mathcal{E}_{s}=\\frac{1}{2}\\|v_{\\mathcal{C}}^{s}-v_{\\mathcal{C}}^{\\star}\\|_{D_{\\mathcal{C}}}^{2}+\\frac{1}{2}\\|i_{\\mathcal{L}}^{s}-i_{\\mathcal{L}}^{\\star}\\|_{D_{\\mathcal{L}}}^{2},\\quad s\\in\\{1,2\\}}\\end{array}$ $(v^{1},i^{1},x^{1},y^{1})$ iCs feasible inLitial pLoint $(v^{2},i^{2},x^{2},y^{2})$ is generated by discrete optimization method from initial point $f\\in\\mathcal F$ , ", "page_idx": 7}, {"type": "text", "text": "where $f,v^{1},i^{1},x^{1},y^{1},v^{\\star},i^{\\star},x^{\\star},y^{\\star}$ are the decision variables and $\\mathcal{F}$ is a family of functions (e.g., $L$ -smooth convex) that the algorithm is to be applied to. Here, we are using the fact that (8) is homogeneous with respect to $k$ (i.e., (8) essentially has no $k$ -dependence), and therefore it is sufficient to verify the condition for $k=1$ but for all feasible initial points $(v^{1},i^{1},x^{1},y^{1})$ . It turns out that (9) can be solved exactly as a semidefinite program (SDP) for many commonly considered function classes $\\mathcal{F}$ . This technique was initially proposed as the performance estimation problem (PEP) [54, 149], a computer-aided methodology for constructing convergence proofs of first-order optimization methods. See, e.g., PEPit [76] package that implements PEP in Python. ", "page_idx": 7}, {"type": "text", "text": "Further, (9) can be posed as a nonconvex quadratically constrained quadratic problem (QCQP) with only a few tens of variables and such problems can be solved exactly with spatial branch-and-bound algorithms [2, 102, 80, 98, 45]. ", "page_idx": 7}, {"type": "text", "text": "In conclusion, we can solve a non-convex QCQP to find a provably convergent discretization of the continuous-time circuit with an admissible dynamic interconnect. We use the Ipopt [154, 9] solver. Further details are provided in $\\S\\mathrm{G}$ of the appendix. ", "page_idx": 7}, {"type": "text", "text": "Example. Consider the following example circuit for the minimization of a convex function $f$ . Let $R_{1}=R_{2}=R_{3}=1$ , and $C_{1}=C_{2}=10$ . ", "page_idx": 8}, {"type": "image", "img_path": "9Jmt1eER9P/tmp/21a7a95b3e43877f2e9a486cd8a4f72103c0de03eb6cc8f8c4e90fc2de46a336.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "With our automatic discretization methodology, we find the sufficiently dissipative parameters ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\eta=6.66,\\qquad h=6.66,\\quad\\alpha=0,\\quad\\beta=1.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "The resulting provably convergent algorithm is ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{{x^{k}}}&{{=}}&{{\\mathbf{prox}_{(1/2)f}(z^{k}),\\quad y^{k}=2(z^{k}-x^{k})}}\\\\ {{w^{k+1}}}&{{=}}&{{w^{k}-0.33(y^{k}+3w^{k})}}\\\\ {{z^{k+1}}}&{{=}}&{{z^{k}-0.16(5y^{k}+3w^{k}).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "is provably convergent2 under the condition that $f$ is strictly convex, see $\\S\\mathrm{H}$ for details. ", "page_idx": 8}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we use our methodology to obtain a new algorithm and experiment with it on a specific problem instance. Consider a decentralized optimization problem with a communication graph $G$ with $N=6$ nodes and 7 edges, as shown in Figure 8. Specifically, we consider the optimization problem ", "page_idx": 8}, {"type": "image", "img_path": "9Jmt1eER9P/tmp/6444d1ec6ed0a2b70b7628fd71464996f4e0af2e3266600a4d3cdde0f1705003.jpg", "img_caption": ["Figure 8: Underlying graph $G$ . "], "img_footnote": [], "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{x\\in\\mathbb{R}^{100}}{\\mathrm{minimize}}}&{\\sum_{i\\in\\{4,5\\}}\\left(\\|x-b_{i}\\|_{2}+\\|x-b_{i}\\|_{2}^{2}\\right)+\\sum_{i\\notin\\{4,5\\}}\\|x-b_{i}\\|_{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where each agent $i\\in\\{1,\\ldots,6\\}$ holds the vector $b_{i}\\in{\\bf R}^{100}$ . To leverage the strong convexity of $f_{4}$ and $f_{5}$ , we propose a modification to the DADMM circuit described in $\\S\\mathrm{F}.3$ . Given that a circuit with a capacitor and inductor corresponds to a momentum method (see $\\S3.1\\rangle$ ), and momentum is known to accelerate convergence for strongly convex functions [124], we add a capacitor to $e_{45}$ to DADMM circuit as shown in the left column of Figure 9. We then discretize the circuit and refer the the resulting algorithm DADMM $\\pm C$ . We apply DADMM $+\\mathbf{C}$ to the decentralized optimization problem and observe a speedup as shown in the right columns of Figure 9. The relative error for DADMM $+\\mathbf{C}$ decreases to $10^{-1\\dot{0}}$ in 66 iterations, for DADMM in 87 iterations and for P-EXTRA in 294 iterations. For further details, see $\\S\\mathrm{I.}1$ of the appendix. ", "page_idx": 8}, {"type": "image", "img_path": "9Jmt1eER9P/tmp/401a25886b8fc49ab552d4d39b46474dbc8ab9cf392f97da77a6f183f0991299.jpg", "img_caption": ["Figure 9: (Left) Circuit of DADMM $+\\mathbf{C}$ . Compared to the DADMM circuit of $\\S\\mathrm{F}.3$ , the DADMM $+C$ circuit has an additional capacitor. (Right) Relative error $\\left|f(x^{k})-f^{\\star}\\right|/f^{\\star}$ vs. iteration count. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Further, we define a general version of the DADMM $+\\mathrm{C}$ method for any connected graph and establish a general convergence proof in Lemma I.1 of in $\\S\\mathrm{I.}1.1$ of the appendix. This convergence analysis demonstrates how to use our methodology to discover a new family of methods with a classical convergence proof. Finally, we provide another set of similar experiments in $\\S\\mathrm{I}.2$ of the appendix. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we present a novel approach to optimization algorithm design using ideas from electric RLC circuits. The continuous-time RLC circuit models combined with the automatic discretization method provide a foundation for designing algorithms that inherently possess convergence guarantees. Further, we provide code implementing the automatic discretization. Our framework opens the door to future research by applying this methodology to a broader range of optimization problems and extending the problem to other setups, such as the stochastic optimization setup. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by the Samsung Science and Technology Foundation (Project Number SSTF-BA2101-02), the National Research Foundation of Korea (NRF) grant funded by the Korean government (No.RS-2024-00421203, RS-2024-00406127), and the Oliger Memorial Fellowship. We thank Hangjun Cho for the helpful discussions on the continuous-time analysis. We also thank anonymous reviewers for the highly constructive feedback. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] B. Abbas and H. Attouch. Dynamical systems and forward\u2013backward algorithms associated with the sum of a convex subdifferential and a monotone cocoercive operator. Optimization. A Journal of Mathematical Programming and Operations Research, 64(10):2223\u20132252, 2015.   \n[2] T. Achterberg and E. Towle. Non-Convex Quadratic Optimization: Gurobi 9.0. 2020. https: //www.gurobi.com/resource/non-convex-quadratic-optimization/.   \n[3] S. Adly and H. Attouch. Finite convergence of proximal-gradient inertial algorithms combining dry friction with Hessian-driven damping. SIAM Journal on Optimization, 30(3):2134\u20132162, 2020.   \n[4] A. Agarwal, C. Fiscko, S. Kar, L. Pileggi, and B. Sinopoli. An equivalent circuit workflow for unconstrained optimization. arXiv preprint arXiv:2305.14061, 2023.   \n[5] A. Agarwal and L. Pileggi. An equivalent circuit approach to distributed optimization. arXiv preprint arXiv:2305.14607, 2023.   \n[6] F. Alvarez. On the minimizing property of a second order dissipative system in hilbert spaces. SIAM Journal on Control and Optimization, 38(4):1102\u20131119, 2000.   \n[7] F. Alvarez and H. Attouch. An inertial proximal method for maximal monotone operators via discretization of a nonlinear oscillator with damping. Set-Valued Analysis, 9(1):3\u201311, 2001.   \n[8] F. Alvarez, H. Attouch, J. Bolte, and P. Redont. A second-order gradient-like dissipative dynamical system with Hessian-driven damping : Application to optimization and mechanics. Journal de Math\u00e9matiques Pures et Appliqu\u00e9es, 81(8):747\u2013779, 2002.   \n[9] J. Andersson, J. Gillis, G. Horn, J. B. Rawlings, and M. Diehl. CasADi: a software framework for nonlinear optimization and optimal control. Mathematical Programming Computation, 11:1\u201336, 2019.   \n[10] V. Apidopoulos, J.-F. Aujol, and C. Dossal. The differential inclusion modeling FISTA algorithm and optimality of convergence rate in the case $b\\leq3$ . SIAM Journal on Optimization, 28(1):551\u2013574, 2018.   \n[11] H. Attouch and F. Alvarez. The heavy ball with friction dynamical system for convex constrained minimization problems. Belgian-French-German Conference on Optimization, 1998.   \n[12] H. Attouch, Z. Chbani, J. Fadili, and H. Riahi. First-order optimization algorithms via inertial systems with Hessian driven damping. Mathematical Programming, 2020.   \n[13] H. Attouch, Z. Chbani, J. Peypouquet, and P. Redont. Fast convergence of inertial dynamics and algorithms with asymptotic vanishing viscosity. Mathematical Programming, 168(1):123\u2013175, 2018.   \n[14] H. Attouch, Z. Chbani, and H. Riahi. Fast proximal methods via time scaling of damped inertial dynamics. SIAM Journal on Optimization, 29(3):2227\u20132256, 2019.   \n[15] H. Attouch, Z. Chbani, and H. Riahi. Rate of convergence of the Nesterov accelerated gradient method in the subcritical case $\\alpha\\leq3$ . ESAIM: Control, Optimisation and Calculus of Variations, 25:2, 2019.   \n[16] H. Attouch and M.-O. Czarnecki. Asymptotic control and stabilization of nonlinear oscillators with non-isolated equilibria. Journal of Differential Equations, 179(1):278\u2013310, 2002.   \n[17] H. Attouch, X. Goudou, and P. Redont. The heavy ball with friction method, I. The continuous dynamical system: Global exploration of the local minima of a real-valued function by asymptotic analysis of a dissipative dynamical system. Communications in Contemporary Mathematics, 02(01):1\u201334, 2000.   \n[18] H. Attouch and S. C. L\u00e1szl\u00f3. Newton-like inertial dynamics and proximal algorithms governed by maximally monotone operators. SIAM Journal on Optimization, 30(4):3252\u20133283, 2020.   \n[19] H. Attouch and J. Peypouquet. Convergence of inertial dynamics and proximal algorithms governed by maximally monotone operators. Mathematical Programming, 174(1):391\u2013432, 2019.   \n[20] H. Attouch, J. Peypouquet, and P. Redont. A dynamical approach to an inertial forwardbackward algorithm for convex minimization. SIAM Journal on Optimization, 24(1):232\u2013256, 2014.   \n[21] J.-P. Aubin and A. Cellina. Differential Inclusions: Set-Valued Maps and Viability Theory, volume 264. Springer Science & Business Media, 2012.   \n[22] M. Barr\u00e9, A. B. Taylor, and F. Bach. Principled analyses and design of first-order methods with inexact proximal operators. Mathematical Programming, 201(1):185\u2013230, 2023.   \n[23] H. Bauschke and P. Combettes. Convex Analysis and Monotone Operator Theory in Hilbert Spaces. Springer International Publishing, second edition, 2017.   \n[24] M. Betancourt, M. I. Jordan, and A. C. Wilson. On symplectic optimization. arXiv:1802.03653, 2018.   \n[25] R. I. Bot\u00b8 and E. R. Csetnek. Convergence rates for forward\u2013backward dynamical systems associated with strongly monotone inclusions. Journal of Mathematical Analysis and Applications, 457(2):1135\u20131152, 2018.   \n[26] R. I. Bot\u00b8, E. R. Csetnek, and D.-K. Nguyen. Fast Optimistic Gradient Descent Ascent (OGDA) method in continuous and discrete time. Foundations of Computational Mathematics, 2023.   \n[27] R. I. Bot\u00b8 and D. A. Hulett. Second order splitting dynamics with vanishing damping for additively structured monotone inclusions. Journal of Dynamics and Differential Equations, 2022.   \n[28] S. Boyd. Distributed optimization: Analysis and synthesis via circuits. Lecture Note EE364b, Stanford University, 2010.   \n[29] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends in Machine Learning, 3(1):1\u2013122, 2011.   \n[30] S. Boyd, N. Parikh, E. Chu, B. Peleato, J. Eckstein, et al. Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends\u00ae in Machine learning, 3(1):1\u2013122, 2011.   \n[31] S. P. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.   \n[32] R. E. Bruck. Asymptotic convergence of nonlinear contraction semigroups in Hilbert space. Journal of Functional Analysis, 18(1):15\u201326, 1975.   \n[33] F. S. Cattivelli, C. G. Lopes, and A. H. Sayed. A diffusion rls scheme for distributed estimation over adaptive networks. In Signal Processing Advances in Wireless Communications, 2007.   \n[34] F. S. Cattivelli and A. H. Sayed. Diffusion LMS strategies for distributed estimation. IEEE Transactions on Signal Processing, 58(3):1035\u20131048, 2010.   \n[35] A.-L. Cauchy. M\u00e9thode g\u00e9n\u00e9rale pour la r\u00e9solution des syst\u00e9mes d\u2019\u00e9quations simultan\u00e9es. Comptes Rendus Hebdomadaires des S\u00e9ances de l\u2019Acad\u00e9mie des Sciences, 25:536\u2013538, 1847.   \n[36] T. Chaffey, S. Banert, P. Giselsson, and R. Pates. Circuit analysis using monotone+skew splitting. European Journal of Control, 74:100854, 2023.   \n[37] T. Chaffey and A. Padoan. Circuit model reduction with scaled relative graphs. Conference on Decision and Control (CDC), pages 6530\u20136535, 2022.   \n[38] T. Chaffey and R. Sepulchre. Monotone RLC circuits. European Control Conference, 2021.   \n[39] T. Chaffey and R. Sepulchre. Monotone one-port circuits. IEEE Transactions on Automatic Control, 69(2):783\u2013796, 2024.   \n[40] S. Chen, B. Shi, and Y.-X. Yuan. On underdamped Nesterov\u2019s acceleration. arXiv:2304.14642, 2023.   \n[41] L. Chua and G.-N. Lin. Nonlinear programming without computation. IEEE Transactions on Circuits and Systems, 31(2):182\u2013188, 1984.   \n[42] P. L. Combettes and V. R. Wajs. Signal recovery by proximal forward-backward splitting. Multiscale Modeling and Simulation, 4(4):1168\u20131200, 2005.   \n[43] E. R. Csetnek, Y. Malitsky, and M. K. Tam. Shadow Douglas\u2013Rachford splitting for monotone inclusions. Applied Mathematics & Optimization, 80(3):665\u2013678, 2019.   \n[44] S. Cyrus, B. Hu, B. Van Scoy, and L. Lessard. A robust accelerated optimization algorithm for strongly convex functions. American Control Conference (ACC), pages 1376\u20131381, 2018.   \n[45] S. Das Gupta, B. P. G. Van Parys, and E. K. Ryu. Branch-and-bound performance estimation programming: A unified methodology for constructing optimal optimization methods. Mathematical Programming, 2023.   \n[46] D. Davis and W. Yin. A three-operator splitting scheme and its optimization applications. Set-Valued and Variational Analysis, 25(4):829\u2013858, 2017.   \n[47] G. B. De Luca and E. Silverstein. Born-infeld (BI) for AI: Energy-conserving descent (ECD) for optimization. International Conference on Machine Learning, 162, 2022.   \n[48] J. B. Dennis. Mathematical Programming and Electrical Networks. PhD thesis, Massachusetts Institute of Technology, 1959.   \n[49] C. A. Desoer and J. Katzenelson. Nonlinear RLC networks. Bell System Technical Journal, 44(1):161\u2013198, 1965.   \n[50] C. A. Desoer and E. S. Kuh. Basic Circuit Theory. Electronic Engineering. McGraw-Hill, 1969.   \n[51] C. A. Desoer and F. F. Wu. Nonlinear monotone networks. SIAM Journal on Applied Mathematics, 26(2):315\u2013333, 1974.   \n[52] J. Diakonikolas and M. I. Jordan. Generalized momentum-based methods: A Hamiltonian perspective. SIAM Journal on Optimization, 31(1):915\u2013944, 2021.   \n[53] J. Douglas and H. H. Rachford. On the numerical solution of heat conduction problems in two and three space variables. Transactions of the American Mathematical Society, 82(2):421\u2013439, 1956.   \n[54] Y. Drori and M. Teboulle. Performance of first-order methods for smooth convex minimization: A novel approach. Mathematical Programming, 145(1):451\u2013482, 2014.   \n[55] R. J. Duffin. Nonlinear networks. I. Bulletin of the American Mathematical Society, 52(10):833\u2013 838, 1946.   \n[56] J. Eckstein and D. P. Bertsekas. On the Douglas\u2014Rachford splitting method and the proximal point algorithm for maximal monotone operators. Mathematical Programming, 55(1):293\u2013318, 1992.   \n[57] L. Euler. Institutiones Calculi Differentialis. Petropolis, 1755.   \n[58] M. Even, R. Berthier, F. Bach, N. Flammarion, H. Hendrikx, P. Gaillard, L. Massouli\u00e9, and A. Taylor. Continuized accelerations of deterministic and stochastic gradient descents, and of gossip algorithms. Neural Information Processing Systems, 2021.   \n[59] H. Everett. Generalized Lagrange multiplier method for solving problems of optimum allocation of resources. Operations Research, 11(3):399\u2013417, 1963.   \n[60] M. Fazlyab, A. Ribeiro, M. Morari, and V. M. Preciado. Analysis of optimization algorithms via integral quadratic constraints: Nonstrongly convex problems. SIAM Journal on Optimization, 28(3):2654\u20132689, 2018.   \n[61] K. Feng. On difference schemes and symplectic geometry. Proceedings of the 5th International Symposium on Differential Geometry and Differential Equations, pages 42\u201358, 1984.   \n[62] S. Fiori. Quasi-geodesic neural learning algorithms over the orthogonal group: A tutorial. Journal of Machine Learning Research, 6(26):743\u2013781, 2005.   \n[63] M. L. Fisher. The Lagrangian relaxation method for solving integer programming problems. Management Science, 50(12_supplement):1861\u20131871, 2004.   \n[64] G. Fran\u00e7a, M. I. Jordan, and R. Vidal. On dissipative symplectic integration with applications to gradient-based optimization. Journal of Statistical Mechanics: Theory and Experiment, 2021(4):043402, 2021.   \n[65] G. Fran\u00e7a, D. Robinson, and R. Vidal. ADMM and accelerated ADMM as continuous dynamical systems. International Conference on Machine Learning, 2018.   \n[66] G. Fran\u00e7a, D. P. Robinson, and R. Vidal. Gradient flows and proximal splitting methods: A unified view on accelerated and stochastic optimization. Physical Review E: Statistical Physics, Plasmas, Fluids, and Related Interdisciplinary Topics, 103(5):053304, 2021.   \n[67] G. Fran\u00e7a, D. P. Robinson, and R. Vidal. A nonsmooth dynamical systems perspective on accelerated extensions of ADMM. IEEE Transactions on Automatic Control, 68(5):2966\u20132978, 2023.   \n[68] G. Fran\u00e7a, J. Sulam, D. Robinson, and R. Vidal. Conformal symplectic and relativistic optimization. Neural Information Processing Systems, 2020.   \n[69] G. Fran\u00e7a, J. Sulam, D. P. Robinson, and R. Vidal. Conformal symplectic and relativistic optimization. Journal of Statistical Mechanics: Theory and Experiment, 2020(12):124008, 2020.   \n[70] D. Gabay. Chapter IX applications of the method of multipliers to variational inequalities. In M. Fortin and R. Glowinski, editors, Augmented Lagrangian Methods: Applications to the Numerical Solution of Boundary-Value Problems, volume 15 of Studies in Mathematics and Its Applications, pages 299\u2013331. Elsevier, 1983.   \n[71] D. Gabay and B. Mercier. A dual algorithm for the solution of nonlinear variational problems via finite element approximation. Computers and Mathematics with Applications, 2(1):17\u201340, 1976.   \n[72] A. M. Geoffrion. Primal resource-directive approaches for optimizing nonlinear decomposable systems. Operations Research, 18(3):375\u2013403, 1970.   \n[73] B. Gharesifard and J. Cort\u00e9s. Distributed continuous-time convex optimization on weightbalanced digraphs. IEEE Transactions on Automatic Control, 59(3):781\u2013786, 2014.   \n[74] R. Glowinski and A. Marroco. Sur l\u2019approximation, par \u00e9l\u00e9ments finis d\u2019ordre un, et la r\u00e9solution, par p\u00e9nalisation-dualit\u00e9 d\u2019une classe de probl\u00e8mes de Dirichlet non lin\u00e9aires. Revue Fran\u00e7aise d\u2019Automatique, Informatique, Recherche Op\u00e9rationnelle. Analyse Num\u00e9rique, 9(2):41\u201376, 1975.   \n[75] E. Gorbunov, N. Loizou, and G. Gidel. Extragradient method: $O(1/K)$ last-iterate convergence for monotone variational inequalities and connections with cocoercivity. International Conference on Artificial Intelligence and Statistics, 2022.   \n[76] B. Goujaud, C. Moucer, F. Glineur, J. M. Hendrickx, A. B. Taylor, and A. Dieuleveut. PEPit: Computer-assisted worst-case analyses of first-order optimization methods in Python. Mathematical Programming Computation, 16(3):337\u2013367, 2024.   \n[77] E. Hairer, C. Lubich, and W. Gerhard. Geometric Numerical Integration: Structure-Preserving Algorithms for Ordinary Differential Equations. Springer, 2 edition, 2006.   \n[78] S. Hassan-Moghaddam and M. R. Jovanovic\u00b4. Proximal gradient flow and Douglas\u2013Rachford splitting dynamics: Global exponential stability via integral quadratic constraints. Automatica, 123:109311, 2021.   \n[79] U. Helmke and J. Moore. Optimization and dynamical systems. Proceedings of the IEEE, 84(6):907\u2013, 1996.   \n[80] R. Horst and H. Tuy. Global Optimization: Deterministic Approaches. Springer Science & Business Media, 2013.   \n[81] B. Hu and L. Lessard. Dissipativity theory for Nesterov\u2019s accelerated method. International Conference on Machine Learning, 2017.   \n[82] T. H. Hughes. Passivity and electric circuits: A behavioral approach. IFAC-PapersOnLine, 50(1):15500\u201315505, 2017.   \n[83] A. Iserles. A First Course in the Numerical Analysis of Differential Equations. Cambridge University Press, 2009.   \n[84] U. Jang, S. D. Gupta, and E. K. Ryu. Computer-assisted design of accelerated composite optimization methods: OptISTA. arXiv:2305.15704, 2023.   \n[85] H. K. Khalil. Nonlinear Systems. Pearson Education. Prentice Hall, 2002.   \n[86] S. S. Kia, J. Cort\u00e9s, and S. Mart\u00ednez. Distributed convex optimization via continuous-time coordination algorithms with discrete-time communication. Automatica, 55:254\u2013264, 2015.   \n[87] D. Kim. Accelerated proximal point method for maximally monotone operators. Mathematical Programming, 190(1\u20132):57\u201387, 2021.   \n[88] D. Kim and J. A. Fessler. Optimized first-order methods for smooth convex minimization. Mathematical Programming, 159(1-2):81\u2013107, 2016.   \n[89] D. Kim and J. A. Fessler. Optimizing the efficiency of first-order methods for decreasing the gradient of smooth convex functions. Journal of Optimization Theory and Applications, 188(1):192\u2013219, 2021.   \n[90] J. Kim and I. Yang. Convergence analysis of ODE models for accelerated first-order methods via positive semidefinite kernels. Neural Information Processing Systems, 2023.   \n[91] J. Kim and I. Yang. Unifying Nesterov\u2019s accelerated gradient methods for convex and strongly convex objective functions. International Conference on Machine Learning, 2023.   \n[92] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. International Conference on Learning Representations, 2015.   \n[93] W. Krichene, A. Bayen, and P. L. Bartlett. Accelerated mirror descent in continuous and discrete time. Neural Information Processing Systems, 2015.   \n[94] W. Kutta. Beitrag Zur N\u00e4herungsweisen Integration Totaler Differentialgleichungen. Teubner, 1901.   \n[95] S. Lee and D. Kim. Fast extra gradient methods for smooth structured nonconvex-nonconcave minimax problems. Neural Information Processing Systems, 2021.   \n[96] C. Lemar\u00e9chal. Lagrangian relaxation. In M. J\u00fcnger and D. Naddef, editors, Computational Combinatorial Optimization: Optimal or Provably Near-Optimal Solutions, Lecture Notes in Computer Science, pages 112\u2013156. Springer, Berlin, Heidelberg, 2001.   \n[97] L. Lessard, B. Recht, and A. Packard. Analysis and design of optimization algorithms via integral quadratic constraints. SIAM Journal on Optimization, 26(1):57\u201395, 2016.   \n[98] L. Liberti. Introduction to global optimization. Ecole Polytechnique, 2008.   \n[99] F. Lieder. On the convergence rate of the Halpern-iteration. Optimization Letters, 15(2):405\u2013 418, 2021.   \n[100] P. Lin, W. Ren, and J. A. Farrell. Distributed continuous-time optimization: Nonuniform gradient gains, finite-time convergence, and convex constraint set. IEEE Transactions on Automatic Control, 62(5):2239\u20132253, 2017.   \n[101] P. L. Lions and B. Mercier. Splitting algorithms for the sum of two nonlinear operators. SIAM Journal on Numerical Analysis, 16(6):964\u2013979, 1979.   \n[102] M. Locatelli and F. Schoen. Global Optimization: Theory, Algorithms, and Applications. SIAM, 2013.   \n[103] H. Lu. An $O(s^{r})$ -resolution ODE framework for understanding discrete-time algorithms and applications to the linear convergence of minimax problems. Mathematical Programming, 194(1):1061\u20131112, 2022.   \n[104] J. Lu and C. Y. Tang. Zero-gradient-sum algorithms for distributed convex optimization: The continuous-time case. IEEE Transactions on Automatic Control, 57(9):2348\u20132354, 2012.   \n[105] C. J. Maddison, D. Paulin, Y. W. Teh, B. O\u2019Donoghue, and A. Doucet. Hamiltonian descent methods. arXiv:1809.05042, 2018.   \n[106] J. C. Maxwell. A Treatise on Electricity and Magnetism, volume 1. Oxford: Clarendon Press, 1873.   \n[107] R. McLachlan and M. Perlmutter. Conformal Hamiltonian systems. Journal of Geometry and Physics, 39(4):276\u2013300, 2001.   \n[108] A. Megretski and A. Rantzer. System analysis via integral quadratic constraints. IEEE Transactions on Automatic Control, 42(6):819\u2013830, 1997.   \n[109] W. Millar. CXVI. Some general theorems for non-linear systems possessing resistance. Philosophical Magazine and Journal of Science, 42(333):1150\u20131160, 1951.   \n[110] G. Minty. Solving steady-state nonlinear networks of \u2018monotone\u2019 elements. IRE Transactions on Circuit Theory, 8(2):99\u2013104, 1961.   \n[111] G. J. Minty. Monotone networks. Proceedings of the Royal Society of London. Series A. Mathematical and Physical Sciences, 257(1289):194\u2013212, 1960.   \n[112] G. J. Minty. On the maximal domain of a \u201cmonotone\u201d function. The Michigan Mathematical Journal, 8(2):135\u2013137, 1961.   \n[113] C. Moucer, A. Taylor, and F. Bach. A systematic approach to Lyapunov analyses of continuoustime models in convex optimization. SIAM Journal on Optimization, 33(3):1558\u20131586, 2023.   \n[114] M. Muehlebach and M. Jordan. A dynamical systems perspective on Nesterov acceleration. International Conference on Machine Learning, 2019-06-09/2019-06-15.   \n[115] M. Muehlebach and M. I. Jordan. Optimization with momentum: Dynamical, control-theoretic, and symplectic perspectives. Journal of Machine Learning Research, 22(73):1\u201350, 2021.   \n[116] A. Nedic and A. Ozdaglar. Distributed Subgradient Methods for Multi-Agent Optimization. IEEE Transactions on Automatic Control, 54(1):48\u201361, 2009.   \n[117] Y. Nesterov. A method of solving a convex programming problem with convergence rate ${\\cal O}(1/k^{2})$ . Doklady Akademii Nauk SSSR, 269(3):543\u2013547, 1983.   \n[118] Y. Nesterov. Introductory Lectures on Convex Optimization: A Basic Course. Springer, 2004.   \n[119] Y. Nesterov. Lectures on Convex Optimization. Springer, 2 edition, 2018.   \n[120] R. Nishihara, L. Lessard, B. Recht, A. Packard, and M. Jordan. A general analysis of the convergence of ADMM. International Conference on Machine Learning, 2015.   \n[121] J. Nocedal and S. J. Wright. Numerical Optimization. Springer, 1999.   \n[122] J. Park and E. K. Ryu. Exact optimal accelerated complexity for fixed-point iterations. International Conference on Machine Learning, 2022.   \n[123] D. W. Peaceman and J. Rachford, H. H. The numerical solution of parabolic and elliptic differential equations. Journal of the Society for Industrial and Applied Mathematics, 3(1):28\u2013 41, 1955.   \n[124] B. T. Polyak. Some methods of speeding up the convergence of iteration methods. USSR Computational Mathematics and Mathematical Physics, 4(5):1\u201317, 1964.   \n[125] R. T. Rockafellar. Monotone operators and the proximal point algorithm. SIAM Journal on Control and Optimization, 14(5):877\u2013898, 1976.   \n[126] T. Rockafellar. Convex analysis, volume 11. Princeton University Press, 1997.   \n[127] C. Runge. Ueber die numerische Aufl\u00f6sung von Differentialgleichungen. Mathematische Annalen, 46(2):167\u2013178, 1895.   \n[128] R. D. Ruth. A canonical integration technique. IEEE Transactions on Nuclear Science, 30(4):2669\u20132671, 1983.   \n[129] E. Ryu and W. Yin. Large-Scale Convex Optimization via Monotone Operators. Cambridge University Press, 2022.   \n[130] E. K. Ryu, A. B. Taylor, C. Bergeling, and P. Giselsson. Operator splitting performance estimation: Tight contraction factors and optimal parameter selection. SIAM Journal on Optimization, 30(3):2251\u20132271, 2020.   \n[131] S. K. Sashank J. Reddi, Satyen Kale. On the convergence of Adam and beyond. International Conference on Learning Representations, 2018.   \n[132] J. Schropp and I. Singer. A dynamical systems approach to constrained minimization. Numerical Functional Analysis and Optimization, 21(3-4):537\u2013551, 2000.   \n[133] D. Scieur, V. Roulet, F. Bach, and A. d\u2019Aspremont. Integration methods and optimization algorithms. Neural Information Processing Systems, 2017.   \n[134] S. Seshu and M. B. Reed. Linear Graphs and Electrical Networks. Addison-Wesley Series in Behavioral Science: Quantitative Methods. Addison-Wesley Publishing Company, 1961.   \n[135] B. Shi, S. Du, W. Su, and M. Jordan. Acceleration via symplectic discretization of highresolution differential equations. Neural Information Processing Systems, 2019.   \n[136] B. Shi, S. S. Du, M. I. Jordan, and W. J. Su. Understanding the acceleration phenomenon via high-resolution differential equations. Mathematical Programming, 2021.   \n[137] W. Shi, Q. Ling, G. Wu, and W. Yin. A proximal gradient algorithm for decentralized composite optimization. IEEE Transactions on Signal Processing, 63(22):6013\u20136023, 2015.   \n[138] W. Shi, Q. Ling, K. Yuan, G. Wu, and W. Yin. On the linear convergence of the ADMM in decentralized consensus optimization. IEEE Transactions on Signal Processing, 62(7):1750\u2013 1761, 2014.   \n[139] G. J. Silverman. Primal decomposition of mathematical programs by resource allocation: I. Basic theory and a direction-finding procedure. Operations Research, 20(1):58\u201374, 1972.   \n[140] C. S. Simon Michalowsky and C. Ebenbauer. Robust and structure exploiting optimisation algorithms: An integral quadratic constraint approach. International Journal of Control, 94(11):2956\u20132979, 2021.   \n[141] D. Sontag, A. Globerson, and T. Jaakkola. Introduction to dual decomposition for inference. In S. Sra, S. Nowozin, and S. J. Wright, editors, Optimization for Machine Learning, pages 219\u2013254. The Massachusetts Institute of Technology Press, 2011.   \n[142] W. Su, S. Boyd, and E. J. Cand\u00e8s. A differential equation for modeling Nesterov\u2019s accelerated gradient method: Theory and insights. Neural Information Processing Systems, 2014.   \n[143] W. Su, S. Boyd, and E. J. Cand\u00e8s. A differential equation for modeling Nesterov\u2019s accelerated gradient method: Theory and insights. Journal of Machine Learning Research, 17(153):1\u201343, 2016.   \n[144] J. J. Suh, J. Park, and E. K. Ryu. Continuous-time analysis of anchor acceleration. Neural Information Processing Systems, 2023.   \n[145] J. J. Suh, G. Roh, and E. K. Ryu. Continuous-time analysis of AGM via conservation laws in dilated coordinate systems. International Conference on Machine Learning, 2022.   \n[146] A. Sundararajan, B. Van Scoy, and L. Lessard. Analysis and design of first-order distributed optimization algorithms over time-varying graphs. IEEE Transactions on Control of Network Systems, 7(4):1597\u20131608, 2020.   \n[147] A. Taylor and F. Bach. Stochastic first-order methods: Non-asymptotic and computer-aided analyses via potential functions. Conference on Learning Theory, 2019-06-25/2019-06-28.   \n[148] A. Taylor and Y. Drori. An optimal gradient method for smooth strongly convex minimization. Mathematical Programming, 199(1-2):557\u2013594, 2023.   \n[149] A. B. Taylor, J. M. Hendrickx, and F. Glineur. Smooth strongly convex interpolation and exact worst-case performance of first-order methods. Mathematical Programming, 161(1):307\u2013345, 2017.   \n[150] K. Ushiyama, S. Sato, and T. Matsuo. A unified discretization framework for differential equation approach with Lyapunov arguments for convex optimization. Neural Information Processing Systems, 2023.   \n[151] B. Van Scoy, R. A. Freeman, and K. M. Lynch. The fastest known globally convergent first-order method for minimizing strongly convex functions. IEEE Control Systems Letters, 2(1):49\u201354, 2018.   \n[152] S. Vichik and F. Borrelli. Solving linear and quadratic programs with an analog circuit. Computers & Chemical Engineering, 70:160\u2013171, 2014.   \n[153] R. D. Vogelaere. Methods of integration which preserve the contact transformation property of the hamilton equations. 1956.   \n[154] A. W\u00e4chter and L. T. Biegler. On the implementation of an interior-point filter line-search algorithm for large-scale nonlinear programming. Mathematical Programming, 106:25\u201357, 2006.   \n[155] J. Wang and N. Elia. Control approach to distributed optimization. Annual Allerton Conference on Communication, Control, and Computing, 2010.   \n[156] E. Wei and A. Ozdaglar. On the $O(1/k)$ convergence of asynchronous distributed alternating direction method of multipliers. Global Conference on Signal and Information Processing, 2013.   \n[157] A. Wibisono, A. C. Wilson, and M. I. Jordan. A variational perspective on accelerated methods in optimization. Proceedings of the National Academy of Sciences, 113(47):E7351\u2013E7358, 2016.   \n[158] J. C. Willems. The Generation of Lyapunov Functions for Input-Output Stable Systems. SIAM Journal on Control, 9(1):105\u2013134, 1971.   \n[159] J. C. Willems. Dissipative dynamical systems part I: General theory. Archive for Rational Mechanics and Analysis, 45(5):321\u2013351, 1972.   \n[160] A. C. Wilson, L. Mackey, and A. Wibisono. Accelerating rescaled gradient descent: Fast optimization of smooth functions. Neural Information Processing Systems, 2019.   \n[161] A. C. Wilson, B. Recht, and M. I. Jordan. A Lyapunov analysis of accelerated methods in optimization. Journal of Machine Learning Research, 22(113):1\u201334, 2021.   \n[162] G. Wilson. Quadratic programming analogs. IEEE Transactions on Circuits and Systems, 33(9):907\u2013911, 1986.   \n[163] L. Yang, R. Arora, V. braverman, and T. Zhao. The physical systems behind optimization algorithms. Neural Information Processing Systems, 31, 2018.   \n[164] T. Yang, X. Yi, J. Wu, Y. Yuan, D. Wu, Z. Meng, Y. Hong, H. Wang, Z. Lin, and K. H. Johansson. A survey of distributed optimization. Annual Reviews in Control, 47:278\u2013305, 2019.   \n[165] T. Yoon, J. Kim, J. J. Suh, and E. K. Ryu. Optimal acceleration for minimax and fixed-point problems is not unique. International Conference on Machine Learning, 2024.   \n[166] T. Yoon and E. K. Ryu. Accelerated algorithms for smooth convex-concave minimax problems with ${\\mathcal{O}}(1/k^{2})$ rate on squared gradient norm. International Conference on Machine Learning, 2021.   \n[167] Y. Yu and B. A\u00e7\u0131kme\u00b8se. RC circuits based distributed conditional gradient method. arXiv:2003.06949, 2020.   \n[168] Y. Yu and B. A\u00e7\u0131kme\u00b8se. RLC circuits-based distributed mirror descent method. IEEE Control Systems Letters, 4(3):548\u2013553, 2020.   \n[169] H. Yuan, Y. Zhou, C. J. Li, and Q. Sun. Differential inclusions for modeling nonsmooth ADMM variants: A continuous limit theory. International Conference on Machine Learning, 2019.   \n[170] K. Yuan, Q. Ling, and W. Yin. On the convergence of decentralized gradient descent. SIAM Journal on Optimization, 26(3):1835\u20131854, 2016.   \n[171] J. Zhang, A. Mokhtari, S. Sra, and A. Jadbabaie. Direct Runge\u2013Kutta discretization achieves acceleration. Neural Information Processing Systems, 2018.   \n[172] J. Zhang, S. Sra, and A. Jadbabaie. Acceleration in first order quasi-strongly convex optimization by ODE discretization. Conference on Decision and Control, 2019.   \n[173] P. Zhang, A. Orvieto, and H. Daneshmand. Rethinking the variational interpretation of accelerated optimization methods. Neural Information Processing Systems, 2021. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "A Prior works ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Distributed optimization as RLC circuits. This work started as a lecture for the Stanford University EE 364b class given in 2010 [28]. The lecture proposed the idea of relating distributed optimization algorithms to the dynamics of RLC circuits. Different from the prior studies [48, 162, 41, 152], that consider solving specific optimization problems through implementing physical circuits, our focus is on using insights from circuit theory to design new algorithms, without any consideration of implementing physical circuits. The follow-up works [167, 168, 4, 5], have built upon this setup [28]. ", "page_idx": 19}, {"type": "text", "text": "Optimization algorithms from continuous-time dynamics. Relating continuous-time dynamics described by ordinary differential equation (ODE) with optimization algorithm is a technique with a long history [32, 79, 6, 132, 62]. The continuous-time dynamics related to Polyak\u2019s heavy ball method [124] were studied by [11, 17, 8, 16]. The ODE model for Nesterov acceleration [117] was introduced by [142, 143], analyses for generalized cases were followed by [10, 13, 15], and the ODE model for Nesterov acceleration for strongly convex function (NAG-SC) was introduced in [161]. Together with [93], the studies by [142, 143] initiated continuous-time analyses of accelerated first-order methods and inspired much follow-up works such as [157, 19, 115, 52, 58, 161, 18, 27, 145, 91, 144, 26]. As a further refined continuous-time model preserving more information from the discretization, the high-resolution ODE for NAG-SC was introduced in [136], and was further developed by [103]. ", "page_idx": 19}, {"type": "text", "text": "In addition to accelerated methods, various topics and methods in optimization have been studied in a continuous-time framework. Continuous-time dynamics related to splitting methods were studied by [20, 1, 25, 43, 66, 78]. [65] studied continuous-time dynamics of ADMM [74, 71, 70, 56, 29], and provided an accelerated ADMM by discretizing the ODE model combined with [142]. The analyses were furthermore generalized to differential inclusions by [169, 67]. There are numerous works of continuous-time analyses for distributed optimization, [155, 104, 73, 86, 100] to name a few, and we refer the readers to the survey paper [164] for a comprehensive overview. ", "page_idx": 19}, {"type": "text", "text": "Computer-assisted analysis of optimization algorithms. There has been lines of work automating the analysis of optimization methods using semidefinite programs (SDP). One line of work is performance estimation problems (PEP) introduced by [54], which provides a systematic way to obtain worst-case performance guarantees of a given fixed-step first-order method. The range and technique of utilizing PEP have been further developed by [149, 147, 130, 113, 90], and many efficient algorithms with tight analyses utilizing PEP are discovered [88, 99, 87, 89, 166, 95, 122, 75, 148, 84, 22, 165]. ", "page_idx": 19}, {"type": "text", "text": "Another line of work is an approach adapting integral quadratic constraints (IQC) [108]. IQCs are a powerful analysis method in control theory for analyzing interconnected dynamical systems with nonlinear feedback. This approach was first adapted for analyzing first-order optimization algorithms by [97] and followed by [60]. Analyses based on IQC have lead to tight bounds for well-known algorithms [120, 81]. IQC has also been utilized to develop new fast algorithms with tight convergence rates [151, 44, 146, 140]. ", "page_idx": 19}, {"type": "text", "text": "Recently, an extension of PEP to leveraging quadratic constrained quadratic programs (QCQP) was introduced by [45]. Treating the step-sizes as optimization variables, this work furthermore provides systematic computer-assisted methodology to optimize the step-sizes. Our work adapts this approach to finding appropriate discretizations. To the best of our knowledge, our proposal is the first instance of using computer-assisted methodologies to find discretizations of continuous-time dynamics. ", "page_idx": 19}, {"type": "text", "text": "Physics-bases approaches to designing optimization algorithms. Optimization methods obtained by discretizing conformal Hamiltonian dynamics [107] were considered by [105]. Studying structure-preserving discretizations for conformal (dissipative) Hamiltonian systems, [68, 69] analyzed symplectic structure of Nesterov and heavy ball, and introduced Relativistic Gradient Descent (RGD) by adopting ideas from special relativity. Based on relativistic Born-Infeld (BI) dynamics, [47] considered a class of frictionless, energy-conserving system and introduced Bouncing BI (BBI) algorithm as a discretization. ", "page_idx": 19}, {"type": "text", "text": "Our work is based on nonlinear resistive electric circuits, the study of which dates back to [55]. The stationary condition for nonlinear networks were considered by [109], generalizing theorems of Maxwell [106] for linear networks. The study of nonlinear resistive networks influenced the refinement of the concept of maximal monotonicity [111, 112, 110], which is now a fundamental concept in convex optimization. Well-posedness of the solutions for nonlinear networks was studied by [49, 51], but only for one-descent nonlinear resistors. Recently, the study of nonlinear electrical circuits was revisited by [38, 37, 36, 39] using contemporary methods of convex optimization. However, their main focus was on circuits, not on designing new optimization algorithms. To the best of our knowledge, our work is the first to introduce a generalized framework for designing optimization methods based on electric circuits. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "Discretization. Continuous-time analyses of optimization algorithms must eventually contend with the issue of discretizing the dynamics into a discrete-time algorithm. Discretization of differential equations is a subject of numerical analysis, and it has a long history, even dating back to Euler [57]. Standard discretization schemes such as Euler, Runge\u2013Kutta [127, 94] and symplectic integrators [153, 128, 61], have a rich body of research analyzing their convergence [77, 83] for example. However, these theories in numerical analysis primarily focus on the convergence of the discretized sequence to the trajectory of the solution flow in differential equations throughout a finite time-interval, which differs from the focus of optimization. Therefore, directly applying standard discretization schemes from numerical analysis does not ensure convergence to the optimality criteria of interest in optimization, such as function value or optimal point convergence. ", "page_idx": 20}, {"type": "text", "text": "In optimization, the study of discretization can broadly be divided into two categories. One involves applying standard discretization schemes or their variants, and the other provides special rules tailored to the specific dynamics of interest. As previously discussed, the former cases can only guarantee the convergence involving certain errors [24, 64], or introduce specific and limited cases they can cover [133, 171, 135, 114, 172, 145, 150]. The latter type of works do provide discretization rules with analytic proofs for certain families of ODEs [7, 143, 157, 14, 160, 3, 12, 52, 26], but cannot be applied to general cases. Of course, both approaches have brought significant advances in obtaining new methods from continuous-time dynamics, however, it is still true that previous approaches cannot immediately applied the new ODEs that emerge from our framework. To the best of our knowledge, our work is the first to propose to automate the process of finding a discretized method from ODE using computer-assisted tools. ", "page_idx": 20}, {"type": "text", "text": "B Proof of Theorem 2.1 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "To prove Theorem 2.1, it is sufficient to consider the cases without 0-ohm resistors and furthermore all resistor, inductance, capacitance values are 1. We first state the theorem for such cases, which implies Theorem 2.1. ", "page_idx": 21}, {"type": "text", "text": "Theorem B.1. Let $f\\colon\\mathbf{R}^{m}\\rightarrow\\mathbf{R}^{m}$ be a $\\mu$ -strongly convex and $M$ -smooth function and $B\\colon\\mathbf{R}^{\\mathcal{I}}\\to\\mathbf{R}^{K}$ be a matrix. Suppose $(v^{0},i^{0},x^{0},y^{0})$ satisfy ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left[\\!\\!\\begin{array}{l}{i^{0}\\right]\\in\\mathcal{N}(B),\\quad\\left[\\!\\!\\begin{array}{l}{v^{0}\\right]\\in\\mathcal{R}(B^{\\intercal}),\\quad v_{\\mathcal{R}}^{0}=i_{\\mathcal{R}}^{0},\\quad y^{0}=\\nabla f(x^{0}).}\\end{array}\\!\\!\\right]\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then there is a uniquely determined Lipschitz continuous curve $(v,i,x,y)\\colon[0,\\infty)\\to\\mathbf{R}^{2\\mathcal{K}}$ satisfi $\\left[\\!\\!\\begin{array}{c}{i}\\\\ {y}\\end{array}\\!\\!\\right]\\in\\mathcal{N}(B),\\quad\\left[\\!\\!\\begin{array}{c}{v}\\\\ {x}\\end{array}\\!\\!\\right]\\in\\mathcal{R}(B^{\\top}),\\quad y=\\nabla f(x),\\quad v_{\\mathcal{R}}=i_{\\mathcal{R}},\\quad v_{\\mathcal{L}}=\\frac{d}{d t}i_{\\mathcal{L}},\\quad i_{\\mathcal{C}}=\\frac{d}{d t}v_{\\mathcal{C}},$ (1 ", "page_idx": 21}, {"type": "text", "text": "for all $t\\in(0,\\infty)$ and the initial condition $(v(0),i(0),x(0),y(0))=(v^{0},i^{0},x^{0},y^{0})$ . ", "page_idx": 21}, {"type": "text", "text": "Lemma B.2. Theorem $B.1$ implies Theorem 2.1. ", "page_idx": 21}, {"type": "text", "text": "Proof. (i) KCL, KVL and V-I relations for equivalent dynamics without 0-ohm resistors. ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We first consider the equivalent dynamic interconnect without 0-ohm resistors. As 0-ohm resistors are ideal wires, from basic circuit theory we know the nodes connected by 0-ohm resistors can be considered as a single node. We find the expression for KCL, KVL and V-I relations for the equivalent dynamic interconnect composed with $\\partial f$ . The equivalent expression for KCL and KVL can be considered as consequence of Tellegen\u2019s theorem in [50, $\\S10.2.3]$ , however, we write the detail here to make it self-contained. ", "page_idx": 21}, {"type": "text", "text": "Observe, KCL and KVL can be equivalently written as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left[A\\;\\middle|\\;\\stackrel{I_{m}}{0}\\right]\\left[\\!\\!\\!{\\binom{i}{y}}\\!\\!\\right]=0,\\qquad\\left[\\!\\!\\!{\\boldsymbol{v}}\\!\\!\\right]=\\left[\\!\\!\\!\\begin{array}{c}{A\\;\\middle|\\;\\stackrel{I_{m}}{0}\\right]^{\\intercal}\\left[\\!\\!\\!\\begin{array}{c}{x}\\!\\!\\right].\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We furthermore restrict the values to satisfy Ohm\u2019s law for 0-ohm resistors, i.e., the potential values of two nodes connected to a 0-ohm resistor is identical. ", "page_idx": 21}, {"type": "text", "text": "Let\u2019s first focus on KCL, the left equation. Suppose node $j$ and $l$ are connected with 0-ohm resistor named as $\\mathcal{R}_{j l}$ . Suppose the $k^{*}$ \u2019th column of $A$ corresponds to $\\mathcal{R}_{j l}$ . Eliminating $\\mathcal{R}_{j l}$ corresponds to eliminating the $k$ \u2019th column of $A$ and eliminating $i_{\\mathcal{R}_{j l}}$ from $i$ . However, if we just directly eliminate them, as $i_{\\mathcal{R}_{j l}}$ may not be zero, the equation will no longer be satisfied. We need to keep the information that currents flowing into node $j$ (except for $-i_{\\mathcal{R}_{j l}}$ ) flows to node $l$ . As we do not permit ideal wire loop, without loss of generality we may assume node $j$ is not the ground node. ", "page_idx": 21}, {"type": "text", "text": "To preserve the information, when node $l$ is not the ground node, we add the $j'$ \u2019th row of $\\left[A\\ \\middle|\\begin{array}{c}{{I_{m}}}\\\\ {{0}}\\end{array}\\right]$ to the $l^{\\star}$ th row. Then $k$ \u2019th component of the $l'$ th row becomes 0, thus the equation corresponding to the $l^{:}$ th row will still be satisfied after eliminating the $k$ \u2019th column and $\\mathcal{R}_{j l}$ . When node $l$ is the ground node, skip the row addition. Now eliminate the $j^{:}$ \u2019th row. Note that column is eliminated only from $A$ . We now move on to KVL. Eliminating a column of $A$ and a component in $i$ corresponds to eliminating a row of $A^{\\intercal}$ and a component in $v$ . This conserves the validity of the equation. Next, the row operation for $\\left[A\\ {\\stackrel{\\left.\\right|}{\\left.\\right|}}\\ {\\stackrel{\\left.I_{m}\\right]}{\\left.\\right|}}$ corresponds to column operation for $\\left[A\\ \\biggl|\\begin{array}{c}{{I_{m}}}\\\\ {{0}}\\end{array}\\biggr\\rangle^{\\top}=\\biggl[\\frac{A^{\\bar{\\top}}}{I_{m}\\ 0}\\biggr]\\right.$ . Recall we\u2019ve restricted the potential values of the nodes connected with 0-ohm resistor to be same, values in $\\Big[\\!\\!\\!\\begin{array}{c}{x}\\\\ {e}\\end{array}\\!\\!\\Big]$ corresponding to column $j$ and $l$ coincide. Thus when node $l$ is not the ground node, adding $j$ \u2019th column to the $l'$ th column and eliminating $j$ \u2019th component in $\\Big[\\!\\!\\!\\begin{array}{c}{x}\\\\ {e}\\end{array}\\!\\!\\Big]$ , will not change the values on the left hand side. When node $l$ is the ground node, the same argument holds by skipping the column addition. Repeat this process until there is no 0-ohm resistors. Name the reduced matrix as $\\tilde{B}$ and reduced current as $\\Tilde{i}$ . Then KCL reduces to $\\tilde{B}\\left[\\tilde{\\it{i}}\\right]=0$ and KVL reduces to $\\Big[_{x}^{v}\\Big]=\\tilde{B}^{\\intercal}\\left[_{\\tilde{e}}^{\\tilde{x}}\\right]$ , or equivalently $\\Big[\\tilde{\\boldsymbol{v}}\\Big]\\in\\mathcal{R}(\\tilde{B}^{\\intercal}).$ ", "page_idx": 21}, {"type": "text", "text": "Now name the reduced diagonal matrices ${\\tilde{D}}_{\\mathcal{R}}$ , ${\\tilde{D}}_{\\mathcal{L}}$ and ${\\tilde{D}}_{\\mathcal{C}}$ as the reduced matrices that without the entries corresponding to eliminated components. Note ${\\tilde{D}}_{\\mathcal{R}}$ has no zero diagonal entries. Then KCL, KVL and V-I relations for the equivalent dynamic interconnect composed with $\\partial f$ become as follows ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left[\\!\\!\\left[\\frac{\\tilde{i}}{y}\\right]\\!\\!\\right]\\in\\mathcal{N}(\\tilde{B}),\\quad\\left[\\!\\!\\left[\\tilde{v}\\right]\\!\\!\\right]\\in\\mathcal{R}(\\tilde{B}^{\\top}),\\quad y=\\nabla f(x),\\quad\\tilde{v}_{\\mathcal{R}}=\\tilde{D}_{\\mathcal{R}}\\tilde{i}_{\\mathcal{R}},\\quad\\tilde{v}_{\\mathcal{L}}=\\tilde{D}_{\\mathcal{L}}\\frac{d}{d t}\\tilde{i}_{\\mathcal{L}},\\quad\\tilde{i}_{\\mathcal{C}}=\\tilde{D}_{\\mathcal{C}}\\frac{d}{d t}\\tilde{v}_{\\mathcal{L}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "As an equivalent dynamics, it is enough to prove the curve that satisfies (12) and the initial condition $(\\tilde{v}(0),\\tilde{i}(0),x(0),\\dot{y_{}(0)})=(\\tilde{v}^{0},\\tilde{i}^{0},x^{0},\\dot{y_{}}^{0})$ with condition ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left[\\tilde{\\dot{y}}^{0}\\right]\\in\\mathcal{N}(\\tilde{B}),\\quad\\left[\\tilde{\\dot{v}}^{0}\\right]\\in\\mathcal{R}(\\tilde{B}^{\\intercal}),\\quad\\tilde{\\boldsymbol{v}}_{\\mathcal{R}}^{0}=\\tilde{D}_{\\mathcal{R}}\\tilde{\\dot{\\iota}}_{\\mathcal{R}}^{0},\\quad y^{0}=\\nabla f(x^{0})\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "is unique and Lispchitz continuous. ", "page_idx": 22}, {"type": "text", "text": "(ii) Sufficient to consider only the cases with ${\\tilde{D}}_{\\mathcal{R}}$ , ${\\tilde{D}}_{\\mathcal{L}}$ and ${\\tilde{D}}_{\\mathcal{C}}$ are identity matrices. For a dynamic interconnect composed with $\\partial f$ , consider the equivalent dynamics without 0-ohm resistors. Let $\\tilde{B}$ be the matrix in (12) for the dynamics, and let $\\kappa$ be the number of columns of $\\tilde{B}$ . Suppose $(\\tilde{v}^{0},\\tilde{i}^{0},x^{0},y^{0})$ satisfy (13). Define the diagonal matrix ", "page_idx": 22}, {"type": "equation", "text": "$$\nP=\\mathbf{diag}\\left(\\sqrt{\\tilde{D}_{\\mathcal{R}}^{-1}},\\sqrt{\\tilde{D}_{\\mathcal{L}}^{-1}},\\sqrt{\\tilde{D}_{\\mathcal{C}}},I_{m}\\right)\\!,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and define $B\\,=\\,\\tilde{B}P$ . Define $i^{0}$ and $v^{0}$ to satisfy $\\left[\\stackrel{\\cdot}{y}^{0}\\right]\\,=\\,P^{-1}\\left[\\stackrel{\\cdot}{\\iota}^{0}\\right]\\,$ and $\\begin{array}{r}{\\left[\\stackrel{v^{0}}{x}\\right]\\,=\\,P\\left[\\stackrel{\\tilde{v}^{0}}{x^{0}}\\right]}\\end{array}$ Then $(v^{0},i^{0},x^{0},y^{0})$ satisfies (10) since ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\tilde{B}\\left[\\!\\!\\begin{array}{l}{\\tilde{i}^{0}}\\\\ {y^{0}}\\end{array}\\!\\!\\right]=0}&{\\iff\\quad B\\left[\\!\\!\\begin{array}{l}{i^{0}\\atop y^{0}}\\right]=(\\tilde{B}P)\\left({P^{-1}\\left[\\!\\!\\begin{array}{l}{\\tilde{i}^{0}}\\\\ {y^{0}}\\end{array}\\!\\!\\right]}\\right)=0,}\\\\ {\\exists z^{0},\\ \\left[\\!\\!\\begin{array}{l}{\\tilde{v}^{0}}\\\\ {x^{0}}\\end{array}\\!\\!\\right]=\\tilde{B}^{\\intercal}z^{0}}&{\\iff\\quad\\exists z^{0},\\ \\left[\\!\\!\\begin{array}{l}{v^{0}}\\\\ {x^{0}}\\end{array}\\!\\!\\right]=P\\left[\\!\\!\\begin{array}{l}{\\tilde{v}^{0}}\\\\ {x^{0}}\\end{array}\\!\\!\\right]=P\\tilde{B}^{\\intercal}z=B z^{0},}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\tilde{v}_{\\mathcal{R}}^{0}=\\tilde{D}_{\\mathcal{R}}\\tilde{i}_{\\mathcal{R}}^{0}\\iff\\sqrt{\\tilde{D}_{\\mathcal{R}}}\\tilde{v}_{\\mathcal{R}}^{0}=\\sqrt{\\tilde{D}_{\\mathcal{R}}^{-1}}\\tilde{i}_{\\mathcal{R}}^{0}\\iff v_{\\mathcal{R}}^{0}=i_{\\mathcal{R}}^{0}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Then by Theorem B.1, there is a Lipschitz continuous curve $(v,i,x,y)\\colon[0,\\infty)\\to\\mathbf{R}^{2K}$ that satisfies (11) for all $t\\in(0,\\infty)$ and the initial condition $\\underline{{{(v(0),i(0),x(0),y(0))}}}=(v^{0},i^{0},x^{0},y^{0})$ . Define $\\Tilde{i}$ and $\\tilde{v}$ to satisfy $\\left[\\stackrel{\\cdot}{i}\\right]=P\\left[\\stackrel{\\cdot}{x}\\right]$ and $\\begin{array}{r}{\\Big[\\tilde{\\boldsymbol{v}}\\Big]=\\boldsymbol{P}^{-1}\\,\\Big[\\boldsymbol{v}\\Big]}\\end{array}$ . Then $\\widetilde{i}$ and $\\tilde{v}$ are Lipschitz continuous as well, as they are composition of linear operators and Lipschitz continuous curves. Furthermore, we can check (12) and the initial condition $(\\tilde{v}(0),\\tilde{i}(0),x(0),y(0))=(\\tilde{v}^{0},\\tilde{i}^{0},x^{0},y^{0})$ is satisfied, with the similar argument above. ", "page_idx": 22}, {"type": "text", "text": "Reversing the arguments, the uniqueness can be obtained since $P$ is invertible and thus $(v,i)\\mapsto(\\tilde{v},\\tilde{i})$ is bijective. This concludes the proof. ", "page_idx": 22}, {"type": "text", "text": "By Lemma B.2, our goal has reduced to Theorem B.1. We will establish the well-posedness for $v_{\\mathcal{C}}$ and $i_{\\mathcal{L}}$ first, then extended them to whole curve. The well-posedness of $v_{\\cal{C}},\\,i_{\\cal{L}}$ can be obtained by reducing the dynamics to a differential inclusion with a maximal monotone operator. We first restate the theorem in [21] and its immediate implication as a remark, which we use in the proof. ", "page_idx": 22}, {"type": "text", "text": "Theorem B.3. [21, Thm 3.2.1] Let $\\mathbb{M}\\colon\\mathbf{R}^{n}\\rightrightarrows\\mathbf{R}^{n}$ be a maximal monotone operator, consider the differential inclusion ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\dot{X}(t)\\in-\\mathsf{M}(X(t)),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "with initial condition $X(0)=X_{0}\\in\\mathbf{dom}\\mathbb{M}$ . Then there is a unique solution $X\\colon[0,\\infty)\\rightarrow\\mathbf{R}^{n}$ that is absolutely continuous and satisfies (14) for almost all $t$ . Moreover, $i f$ we denote $\\mathcal{T}=\\{t\\in[0,\\infty)$ | $X$ is differentiable at $t\\}$ , then followings are true. ", "page_idx": 22}, {"type": "text", "text": "(i) Let $X(\\cdot),Y(\\cdot)$ are the solutions issued from X0, Y0 \u2208 dom \ud835\udd44 respectively. Then $\\|X(t)-Y(t)\\|\\leq\\|X_{0}-Y_{0}\\|$ for all $t\\geq0$ .   \n(ii) For all $t\\geq0$ , $\\begin{array}{r}{\\dot{X}_{+}(t):=\\operatorname*{lim}_{h\\rightarrow0+}\\frac{X(t+h)-X(t)}{h}}\\end{array}$ X(t+hh)\u2212X(t)is well-defined and continuous from the right. Note, $\\dot{X}(t)=\\dot{X}_{+}(t)$ for all $t\\in\\mathcal T$ .   \n(iii) $t\\mapsto\\left\\|\\dot{X}_{+}(t)\\right\\|$ is nonincreasing.   \n(iv) $\\dot{X}_{+}(t)=-m(\\mathbb{M}(X(t)))$ holds for all $t\\geq0$ . Here $m(K)$ is the element of $K\\subset\\mathbf{R}^{n}$ with minimal norm, that is, $m(K)=\\Pi_{K}(0)=\\underset{k\\in K}{\\mathrm{argmin}}\\left\\|k\\right\\|$ . Therefore ${\\dot{X}}(t)=-m(\\mathbb{M}(X(t)))$ holds for all $t\\in\\mathcal T$ , and so (14) is satisfied almost everywhere. ", "page_idx": 23}, {"type": "text", "text": "Remark. From $(i i i)$ we have $\\left\\|\\dot{X}_{+}(t)\\right\\|\\;\\leq\\;\\left\\|\\dot{X}_{+}(0)\\right\\|\\;=\\;\\left\\|m(\\mathbb{M}(X_{0}))\\right\\|$ for all $t~\\geq~0,$ , thus for $t_{1},t_{2}\\geq0$ we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|X(t_{1})-X(t_{2})\\|=\\left\\|\\int_{t_{2}}^{t_{1}}\\dot{X}_{+}(s)d s\\right\\|\\leq\\int_{t_{2}}^{t_{1}}\\left\\|\\dot{X}_{+}(s)\\right\\|d s}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\int_{t_{2}}^{t_{1}}\\left\\|m(\\mathbf{M}(X_{0}))\\right\\|d s=|t_{1}-t_{2}|\\left\\|m(\\mathbf{M}(X_{0}))\\right\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Therefore the theorem implies that $X$ is Lipschitz-continuous, in particular with parameter $\\|m(\\mathbb{M}(X_{0}))\\|$ . ", "page_idx": 23}, {"type": "text", "text": "Thus our first goal is to prove the condition (11) can be equivalently written as ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{d}{d t}\\left[v_{\\mathcal{C}}\\right]\\in-\\mathbb{A}\\left[v_{\\mathcal{C}}\\right]\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "for some maximal monotone operator $\\mathbb{A}\\colon\\mathbb{R}^{|\\mathcal{C}|+|\\mathcal{L}|}\\implies\\mathbb{R}^{|\\mathcal{C}|+|\\mathcal{L}|}$ . We first establish an efficient reformulation of KCL and KVL. ", "page_idx": 23}, {"type": "text", "text": "Lemma B.4. There is a skew-symmetric matrix $\\hat{H}\\colon\\mathbf{R}^{\\sigma+m}\\rightarrow{\\mathbf{R}}^{\\sigma+m}$ and a corresponding diagonal matrix $J\\colon{\\mathbf{R}}^{\\sigma+m}\\rightarrow{\\mathbf{R}}^{\\sigma+m}$ with entries 0 of 1 that satisfies ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left[\\!\\!\\begin{array}{l}{i}\\\\ {y}\\end{array}\\!\\!\\right]\\in\\mathcal{N}(B),\\ \\left[\\!\\!\\begin{array}{l}{v}\\\\ {x}\\end{array}\\!\\!\\right]\\in\\mathcal{R}(B^{\\intercal})\\quad\\Longleftrightarrow\\quad\\hat{u}=\\hat{H}\\hat{w},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\hat{u}$ and w\u02c6 are defined as ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\hat{w}=[J\\quad I_{\\sigma+m}-J]\\left[\\!\\!\\begin{array}{l}{v}\\\\ {x}\\\\ {i}\\\\ {y}\\end{array}\\!\\!\\right],\\quad\\hat{u}=[I_{\\sigma+m}-J\\quad J]\\left[\\!\\!\\begin{array}{l}{v}\\\\ {x}\\\\ {i}\\\\ {y}\\end{array}\\!\\!\\right].\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Moreover, let $Q\\colon\\mathbf{R}^{\\sigma+m}\\rightarrow\\mathbf{R}^{\\sigma+m}$ be a permutation matrix, define $w=Q\\hat{w},\\,u=Q\\hat{u}.$ . Then there is a skew-symmetric matrix $H$ that satisfies ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left[\\!\\!{\\begin{array}{l}{i}\\\\ {y}\\end{array}}\\right]\\in{\\mathcal{N}}(B),\\ \\left[\\!\\!{\\boldsymbol{v}}\\right]\\in{\\mathcal{R}}(B^{\\intercal})\\quad\\Longleftrightarrow\\quad u=H w.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Remark. The diagonal matrix $J$ determines whether to select voltage or current for each component, to construct $\\hat{w}$ . To clarify, $w,u\\in\\mathbf{R}^{\\sigma+m}$ are the vectors that $\\{w_{l},u_{l}\\}$ becomes a current and voltage pair of a component for $l\\,=\\,1,2,\\ldots,\\sigma\\,+\\,m$ . Such partitions of current, voltages values $w,u$ and skew-symmetric matrix $H$ were also considered in [82] with different notation. However, we introduce our method of constructing them here, as we will consider $H$ with a special property in Corollary B.4.1 that plays a key role in the proof. ", "page_idx": 23}, {"type": "text", "text": "Proof. Define $N$ and $\\tilde{B}$ be matrices consisted with basis of ${\\mathcal{N}}(B)$ and $\\operatorname{Row}(B)$ respectively. Then KCL and KVL can be shortly rewritten as ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left[N\\quad0\\atop0\\right]\\left[{v\\atop i}\\right]=\\left[0\\atop0\\right].\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We now show there is a diagonal matrix $J\\colon{\\mathbf{R}}^{\\sigma+m}\\,\\rightarrow\\,{\\mathbf{R}}^{\\sigma+m}$ with entries 0 or 1, that makes the below square matrix invertible ", "page_idx": 24}, {"type": "equation", "text": "$$\nG=\\left[\\!\\!\\begin{array}{c c}{N}&{0}\\\\ {0}&{\\tilde{B}}\\\\ {J}&{I_{\\sigma+m}-J}\\end{array}\\!\\!\\right]\\in{\\bf R}^{2(\\sigma+m)\\times2(\\sigma+m)}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Name $N_{0}={\\binom{N}{0}}$ and $\\tilde{B}_{0}=\\binom{0}{\\tilde{B}}$ We will attach the standard basis vectors or 0 below, and increase the index with attached number of rows. We proceed induction on the index, until the index becomes $\\sigma+m$ . ", "page_idx": 24}, {"type": "text", "text": "Suppose, for $0\\leq k\\leq\\sigma+m-1$ , $N_{k}$ and ${\\tilde{B}}_{k}$ satisfy the form ", "page_idx": 24}, {"type": "equation", "text": "$$\nN_{k}=\\left[\\!\\!\\begin{array}{c}{{N}}\\\\ {{0}}\\\\ {{j_{1}\\mathbf{e}_{1}}}\\\\ {{\\vdots}}\\\\ {{j_{k}\\mathbf{e}_{k}}}\\end{array}\\!\\!\\right],\\quad\\tilde{B}_{k}=\\left[\\!\\!\\begin{array}{c}{{0}}\\\\ {{\\tilde{B}}}\\\\ {{(1-j_{1})\\mathbf{e}_{1}}}\\\\ {{\\vdots}}\\\\ {{(1-j_{k})\\mathbf{e}_{k}}}\\end{array}\\!\\!\\right],\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $j_{l}\\in\\{0,1\\}$ and ${\\bf e}_{l}\\in{\\bf R}^{\\sigma+m}$ is a standard basis (row) vector for $1\\leq l\\leq k$ . We claim either $\\mathbf{e}_{k+1}\\not\\in\\mathrm{Row}(N_{k})$ or $\\mathbf{e}_{k+1}\\not\\in\\mathrm{Row}(\\tilde{B}_{k})$ is true. ", "page_idx": 24}, {"type": "text", "text": "Proof by contradiction. Suppose not. That is, suppose $\\mathbf{e}_{k+1}\\in\\operatorname{Row}(N_{k})$ and $\\mathbf{e}_{k+1}\\in\\operatorname{Row}(\\tilde{B}_{k})$ Then there are $\\mathbf{n}=(n_{1},\\ldots,n_{\\sigma+m})\\in\\operatorname{Row}(N),\\,\\mathbf{r}=(r_{1},\\ldots,r_{\\sigma+m})\\in\\operatorname{Row}({\\tilde{B}})$ and coefficients $a_{l},b_{l}$ such that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbf{e}_{k+1}=\\mathbf{n}+\\sum_{l=1}^{k}a_{l}j_{l}\\mathbf{e}_{l}=\\mathbf{r}+\\sum_{l=1}^{k}b_{l}(1-j_{l})\\mathbf{e}_{l}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Taking inner product with ${\\bf e}_{p}$ , $1\\leq p\\leq\\sigma+m$ , we have ", "page_idx": 24}, {"type": "equation", "text": "$$\nn_{p}=\\left\\{\\begin{array}{l l}{-a_{p}}&{\\mathrm{if~}1\\leq p\\leq k,\\ j_{p}=1}\\\\ {0}&{\\mathrm{if~}1\\leq p\\leq k,\\ j_{p}=0}\\\\ {1}&{\\mathrm{if~}p=k+1}\\\\ {0}&{\\mathrm{if~}k+1<p\\leq\\sigma+m,}\\end{array}\\right.\\quad r_{p}=\\left\\{\\begin{array}{l l}{0}&{\\mathrm{if~}1\\leq p\\leq k,\\ j_{p}=1}\\\\ {-b_{p}}&{\\mathrm{if~}1\\leq p\\leq k,\\ j_{p}=0}\\\\ {1}&{\\mathrm{if~}p=k+1}\\\\ {0}&{\\mathrm{if~}k+1<p\\leq\\sigma+m.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Therefore ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\langle\\mathbf{n},\\mathbf{r}\\rangle=\\sum_{p=1}^{\\sigma+m}n_{p}r_{p}=n_{k+1}r_{k+1}=1.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "By the way, since $\\mathbf{n}\\in\\operatorname{Row}(N)=N(B)$ , $\\mathbf{r}\\in\\mathrm{Row}(\\tilde{B})=R(N\\mathsf{T})$ we have $\\mathbf{n}\\perp\\mathbf{r}$ and so $\\langle\\mathbf{n},\\mathbf{r}\\rangle=0$ .   \nA contradiction, we conclude either $\\mathbf{e}_{k+1}\\not\\in\\mathrm{Row}(N_{k})$ or $\\mathbf{e}_{k+1}\\notin\\operatorname{Row}(\\tilde{B}_{k})$ is true. ", "page_idx": 24}, {"type": "text", "text": "From the proved claim, we can extend $N_{0},\\tilde{B}_{0}$ to $N_{\\sigma+m}$ , $\\tilde{B}_{\\sigma+m}$ with keeping the form of (15) by repeating the process below. Recall, the desired form of the matrix was ", "page_idx": 24}, {"type": "equation", "text": "$$\nG=\\left[\\begin{array}{c c}{{N}}&{{0}}\\\\ {{0}}&{{\\tilde{B}}}\\\\ {{J}}&{{I_{\\sigma+m}-J}}\\end{array}\\right]\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "with diagonal matrix $J\\in\\mathbf{R}^{\\left(\\sigma+m\\right)\\times\\left(\\sigma+m\\right)}$ with entries 0 or 1. By the construction, we see matrix $\\left[N_{\\sigma+m}\\right~^{\\bullet}\\,\\tilde{B}_{\\sigma+m}\\right]$ satisfies the desired form. Moreover, we know the nonzero rows of $N_{\\sigma+m}$ and $\\tilde{B}_{\\sigma+m}$ are linearly independent respectively, by their construction. By the form of $G$ we see if $l$ -th row of $N_{\\sigma+m}$ is nonzero then $l_{\\cdot}$ -th row of $\\tilde{B}_{\\sigma+m}$ is zero and vice-versa, we conclude the rows of $G$ are linearly independent. Therefore, $G$ is invertible. ", "page_idx": 24}, {"type": "text", "text": "Observe ", "page_idx": 24}, {"type": "equation", "text": "$$\nG\\left[\\!\\!\\begin{array}{c}{v}\\\\ {x}\\\\ {\\dot{v}}\\\\ {y}\\end{array}\\!\\!\\right]=\\left[\\!\\!\\begin{array}{c c c}{N}&{0}\\\\ {0}&{\\tilde{B}}\\\\ {J}&{I_{\\sigma+m}-J}\\end{array}\\!\\!\\right]\\left[\\!\\!\\begin{array}{c}{v}\\\\ {x}\\\\ {\\dot{v}}\\\\ {y}\\end{array}\\!\\!\\right]=\\left[\\!\\!\\begin{array}{c c c}{0}\\\\ {\\hat{w}}\\end{array}\\!\\!\\right]\\quad\\Longrightarrow\\quad\\left[\\!\\!\\begin{array}{c}{v}\\\\ {x}\\\\ {\\dot{v}}\\\\ {y}\\end{array}\\!\\!\\right]=G^{-1}\\left[\\!\\!\\begin{array}{c}{0}\\\\ {\\hat{w}}\\\\ {\\hat{w}}\\end{array}\\!\\!\\right].\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We know above equation holds for arbitrarily chosen $(v,x),\\;(i,y)$ that satisfies KVL and KCL respectively. Observe $\\mathbf{dom}(G)=R(B^{\\intercal})\\times\\dot{N}(B)$ and from dimension theorem we know ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\dim(R(B^{\\boldsymbol{\\mathsf{T}}})\\times N(B))=\\dim(R(B^{\\boldsymbol{\\mathsf{T}}}))+\\dim(N(B))=\\sigma+m.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "As $G$ is invertible, we have $\\dim(R(G))=\\dim(\\mathbf{dom}(G))=\\sigma+m$ . Therefore the values of the components of $\\hat{w}$ can be arbitrary values in $\\mathbf{R}$ . ", "page_idx": 25}, {"type": "text", "text": "Rearranging the rows of $G^{-1}$ , from (16) we obtain $\\tilde{H}\\in\\mathbf{R}^{2(\\sigma+m)\\times2(\\sigma+m)}$ that satisfies ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left[\\hat{w}\\right]=\\tilde{H}\\left[\\r_{\\hat{w}}^{0}\\right]=\\left[\\tilde{H}_{0}^{w}\\ r_{\\ H_{w}}^{\\ H_{w}}\\right]\\left[\\r_{\\hat{w}}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the block matrices are in $\\mathbf{R}^{\\left(\\sigma+m\\right)\\times\\left(\\sigma+m\\right)}$ . Now, naming $\\hat{H}=\\tilde{H}_{w}^{u}$ we get ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\hat{u}=\\hat{H}\\hat{w}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Now to show $H$ is skew-symmetric, recall from $(v,x)\\,\\in\\,R(B^{\\boldsymbol{\\mathsf{r}}})$ and $(i,y)\\;\\in\\;N(B)$ we have $\\langle(v,x),(i,y)\\rangle=0$ . Thus for all $\\hat{w}\\in\\mathbf{R}^{\\sigma+m}$ , we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left\\langle\\hat{w},\\hat{H}\\hat{w}\\right\\rangle=\\left\\langle\\hat{w},\\hat{u}\\right\\rangle=\\left\\langle\\left(v,x\\right),(i,y)\\right\\rangle=0.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Therefore $\\hat{H}$ is skew-symmetric. ", "page_idx": 25}, {"type": "text", "text": "Finally, let $Q\\colon\\mathbf{R}^{\\sigma+m}\\rightarrow\\mathbf{R}^{\\sigma+m}$ be a permutation matrix. Define $H=Q\\hat{H}Q^{\\boldsymbol{\\mathsf{T}}}$ . Since ", "page_idx": 25}, {"type": "equation", "text": "$$\nH^{\\intercal}=Q\\hat{H}^{\\intercal}Q^{\\intercal}=Q(-\\hat{H})Q^{\\intercal}=-H,\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "$H$ is skew-symmetric. And from $Q^{\\top}Q=I_{\\sigma+m}$ we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\hat{u}=\\hat{H}\\hat{w}\\quad\\Longleftrightarrow\\quad u=Q\\hat{u}=Q\\hat{H}\\hat{w}=Q\\hat{H}Q^{\\top}Q\\hat{w}=H w,\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "we conclude the proof. ", "page_idx": 25}, {"type": "text", "text": "Corollary B.4.1. Recall $\\hat{w}$ is composed with voltage or current values of each component. Integrate the values of resistors and denote as $r$ , integrate $v_{\\cal{C}},i_{\\cal{L}}$ as $p$ and integrate $i_{\\mathcal{C}},v_{\\mathcal{L}}$ as $p_{*}$ . Then we may rearrange the elements of $\\hat{w}$ with certain permutation matrix $Q$ , that $w=Q\\hat{w}$ can be decomposed as following order ", "page_idx": 25}, {"type": "equation", "text": "$$\nw=\\left[\\begin{array}{l}{w_{p}}\\\\ {w_{p_{*}}}\\\\ {w_{r}}\\end{array}\\right],\\qquad w h e r e\\quad w_{p}=\\left[\\begin{array}{l}{w_{v_{c}}}\\\\ {w_{i_{c}}}\\end{array}\\right],\\quad w_{p^{*}}=\\left[\\begin{array}{l}{w_{i_{c}}}\\\\ {w_{v_{c}}}\\end{array}\\right],\\quad w_{r}=\\left[\\begin{array}{l}{w_{v_{r}}}\\\\ {w_{i_{r}}}\\\\ {w_{y}}\\end{array}\\right].\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Consider rewriting $u=H w$ in the decomposed way as ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left[\\!\\!\\begin{array}{c}{{u_{p_{*}}}}\\\\ {{u_{p}}}\\\\ {{u_{r}}}\\end{array}\\!\\!\\right]=\\left[\\!\\!\\begin{array}{c c c}{{H_{p}^{p_{*}}}}&{{H_{p_{*}}^{p_{*}}}}&{{H_{r}^{p_{*}}}}\\\\ {{H_{p}^{p}}}&{{H_{p_{*}}^{p}}}&{{H_{r}^{p}}}\\\\ {{H_{p}^{r}}}&{{H_{p_{*}}^{r}}}&{{H_{r}^{r}}}\\end{array}\\!\\!\\right]\\left[\\!\\!\\begin{array}{c}{{w_{p}}}\\\\ {{w_{p_{*}}}}\\\\ {{w_{r}}}\\end{array}\\!\\!\\right].\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Then there is a diagonal matrix $J$ satisfies the properties considered in Lemma B.4, that corresponding $H$ satisfies ", "page_idx": 25}, {"type": "equation", "text": "$$\nH_{p_{*}}^{r}=0,\\quad H_{p_{*}}^{p}=0,\\quad H_{r}^{p_{*}}=0.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. Name the indices as $\\mathcal{C}_{l},\\mathcal{L}_{k}\\,\\in\\,\\{1,\\dots,\\sigma+m\\}$ for $l\\,\\in\\,\\bigl\\{1,\\dots,|{\\mathcal{C}}|\\bigr\\},\\,k\\,\\in\\,\\bigl\\{1,\\dots,|{\\mathcal{L}}|\\bigr\\}$ that satisfy ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbf{e}_{\\mathcal{C}_{l}}\\left[v\\right]=v_{\\mathcal{C}_{l}},\\qquad\\mathbf{e}_{\\mathcal{L}_{k}}\\left[\\overset{\\cdot}{y}\\right]=i_{\\mathcal{L}_{k}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "First we put $\\mathbf{e}_{\\mathcal{C}_{l}}$ \u2019s and $\\mathbf{e}_{\\mathcal{L}_{k}}$ \u2019s in $J$ as many as possible. That is, determine the values of $j_{\\mathcal{C}_{l}}$ \u2019s and $j_{\\mathcal{L}_{k}}$ \u2019s to satisfy ", "page_idx": 25}, {"type": "text", "text": "\u2022 $\\{\\mathbf{e}_{C_{l}}\\mid j_{C_{l}}=1\\}$ is linearly independent to $\\mathrm{Row}(N)$ . ", "page_idx": 25}, {"type": "text", "text": "\u2022 $\\{\\mathbf{e}_{\\mathcal{C}_{l}}\\mid j_{\\mathcal{C}_{l}}=1\\}\\cup\\{\\mathbf{e}_{\\mathcal{C}_{l^{\\prime}}}\\}$ is linearly dependent to $\\mathrm{Row}(N)$ for any $l^{\\prime}\\in\\{j_{\\mathcal{C}_{l}}\\neq1\\}$ .   \n\u2022 $\\{\\mathbf{e}_{\\mathcal{L}_{s}}~|~j_{\\mathcal{L}_{k}}=0\\}$ is linearly independent to $\\operatorname{Row}(\\tilde{B})$ .   \n\u2022 $\\{\\mathbf{e}_{\\mathcal{L}_{s}}\\mid j_{\\mathcal{L}_{k}}=0\\}\\cup\\{\\mathbf{e}_{\\mathcal{L}_{s^{\\prime}}}\\}$ is linearly dependent to $\\operatorname{Row}(\\tilde{B})$ for any $s^{\\prime}\\in\\{j_{\\mathcal{L}_{s}}\\neq0\\}$ . ", "page_idx": 26}, {"type": "text", "text": "Next, fill the remaining $j$ \u2019s as we\u2019ve done in Lemma B.4. ", "page_idx": 26}, {"type": "text", "text": "Since the proof can be applied using the same argument to other cases, we will focus on a specific case. Focusing on the last row of (17), we can furthermore decompose and write as following ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\iota_{r}=\\left[H_{p}^{r}\\quad H_{p_{\\star}}^{r}\\quad H_{r}^{r}\\right]\\xrightarrow{W_{p}}\\quad\\Longleftrightarrow\\quad\\left[u_{v_{r}}\\right]=\\left[\\begin{array}{l l l l l}{0}&{H_{i_{c}}^{i_{r}}}&{H_{i_{c}}^{i_{r}}}&{0}&{0}&{H_{i_{r}}^{i_{r}}}\\\\ {U_{v_{c}}^{v_{r}}}&{0}&{0}&{H_{v_{c}}^{v_{r}}}&{H_{v_{r}}^{v_{r}}}&{0}\\end{array}\\right]\\xrightarrow{W_{b_{c}}}\\quad.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Note, since above equations origin from KCL and KVL (which are linear equations only with current values or voltage values), $H_{\\alpha}^{\\beta}=\\,0$ if $\\alpha$ is current and $\\beta$ is voltage, and vice-versa. Refer [134, Theorem 6.3]. ", "page_idx": 26}, {"type": "text", "text": "Observe $H_{p_{*}}^{r}=\\left[H_{i c}^{i_{r}}\\quad\\quad0\\right],$ , here we show $H_{i c}^{i_{r}}=0$ . Focusing on arbitrary $k^{*}$ \u2019th row of $H_{i c}^{i_{r}}$ , from above equality we get ", "page_idx": 26}, {"type": "equation", "text": "$$\nu_{i_{r_{k}}}=\\left[H_{i_{C}}^{i_{r}}\\quad H_{i_{C}}^{i_{r}}\\quad H_{i_{r}}^{i_{r}}\\right]_{k}\\left[\\stackrel{w_{i_{C}}}{w_{i_{r}}}\\right]\\quad\\Longleftrightarrow\\quad0=\\left[H_{i_{C}}^{i_{r}}\\quad H_{i_{C}}^{i_{r}}\\quad H_{i_{r}}^{i_{r}}\\right]_{k}\\left[\\stackrel{w_{i_{C}}}{w_{i_{r}}}\\right]-u_{i_{r_{k}}},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the subscript $k$ means the $k$ \u2019th row of the block matrix. As this is a linear equation of current values, it origins from KCL, thus there is a vector $\\mathbf{r}\\in\\mathrm{Row}(\\tilde{B})$ corresponding to this equation, i.e. ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbf{r}\\left[\\!\\!\\begin{array}{l}{i}\\\\ {y}\\end{array}\\!\\!\\right]=\\left[\\!\\!H_{i_{c}}^{i_{r}}\\quad H_{i_{c}}^{i_{r}}\\quad H_{i_{r}}^{i_{r}}\\right]_{k}\\left[\\!\\!\\begin{array}{l}{w_{i_{c}}}\\\\ {w_{i_{c}}}\\\\ {w_{i_{r}}}\\end{array}\\!\\!\\right]-u_{i_{r_{k}}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "On the other hand, as $w_{i\\varsigma},w_{i c},w_{i_{r}}$ are consisted with the components of $i,y$ that corresponds to $j_{l}=0$ , there are coefficient vectors $a\\in{\\mathbf{R}}^{|\\mathcal{L}|},b\\in{\\mathbf{R}}^{|\\mathcal{C}|},c\\in{\\mathbf{R}}^{|\\mathcal{R}|+m}$ that satisfies ", "page_idx": 26}, {"type": "equation", "text": "$$\nH_{i_{c}}^{i_{r}}\\quad H_{i_{c}}^{i_{r}}\\quad H_{i_{r}}^{i_{r}}\\Big|_{k}\\left[\\boldsymbol w_{i_{c}}\\right]-u_{i_{r_{k}}}=\\left(\\sum_{\\substack{s\\in\\{j_{c}=0\\}}}a_{s}\\mathbf{e}_{\\mathcal{L}_{s}}+\\sum_{\\substack{l\\in\\{j_{c}=0\\}}}b_{l}\\mathbf{e}_{\\mathcal{C}_{l}}+\\sum_{\\substack{q\\in\\{j_{r_{q}}=0\\}}}c_{q}\\mathbf{e}_{r_{q}}-\\mathbf{e}_{r_{k}}\\right)\\left[\\boldsymbol i\\right].\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Note $b_{l}$ \u2019s correspond to components of $\\left[H_{i c}^{i_{r}}\\right]_{k}$ . Organizing, we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbf{r}=\\sum_{s\\in\\{j_{\\mathcal{L}_{s}}=0\\}}a_{s}\\mathbf{e}_{\\mathcal{L}_{s}}+\\sum_{l\\in\\{j c_{l}=0\\}}b_{l}\\mathbf{e}_{\\mathcal{C}_{l}}+\\sum_{q\\in\\{j_{r_{q}}=0\\}}c_{q}\\mathbf{e}_{r_{q}}-\\mathbf{e}_{r_{k}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Observe that from right hand side, we can see $\\mathbf{r}$ is orthogonal to $\\{\\mathbf{e}_{C_{l^{\\prime}}}\\mid j_{C_{l^{\\prime}}}=1\\}$ ", "page_idx": 26}, {"type": "text", "text": "By the way, as $\\{\\mathbf{e}_{\\mathcal{C}_{l^{\\prime}}}\\mid j_{\\mathcal{C}_{l^{\\prime}}}=1\\}\\cup\\{\\mathbf{e}_{\\mathcal{C}_{l}}\\}$ is linearly dependent to $\\mathrm{Row}(N)$ for all $l\\in\\{j_{{\\mathcal{C}}_{l}}=0\\}$ , we see ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\sum_{l\\in\\{j_{\\mathcal{C}_{l}}=0\\}}b_{l}\\mathbf{e}_{\\mathcal{C}_{l}}\\in\\mathbf{span}\\left(\\{\\mathbf{e}_{\\mathcal{C}_{l^{\\prime}}}\\mid j_{\\mathcal{C}_{l^{\\prime}}}=1\\}\\cup\\mathrm{Row}(N)\\right),\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "so there is some coefficient vector $d\\in\\mathbf{R}^{|\\mathcal{C}|}$ and $\\mathbf{n}\\in\\mathrm{Row}(N)$ that satisfies ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\sum_{l\\in\\{j c_{l}=0\\}}b_{l}\\mathbf{e}_{\\mathcal{C}_{l}}=\\sum_{l^{\\prime}\\in\\{j c_{l^{\\prime}}=1\\}}d_{l^{\\prime}}\\mathbf{e}_{\\mathcal{C}_{l^{\\prime}}}+\\mathbf{n}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "However, as $\\textbf{r}\\in\\,\\mathrm{Row}(\\tilde{B})$ and $\\mathrm{Row}(\\tilde{B})\\,\\,\\perp\\,\\,\\mathrm{Row}(N).$ , we have $\\langle\\mathbf{r},\\mathbf{n}\\rangle\\;=\\;0$ . Moreover, as $\\mathbf{r}$ is orthogonal to $\\{\\mathbf{e}_{C_{l^{\\prime}}}\\mid j_{C_{l^{\\prime}}}=1\\}$ , we conclude ", "page_idx": 26}, {"type": "equation", "text": "$$\n0=\\left\\langle\\mathbf{r},\\sum_{\\substack{l^{\\prime}\\in\\{j c_{\\iota^{\\prime}}=1\\}}}d_{l^{\\prime}}\\mathbf{e}_{C_{l^{\\prime}}}+\\mathbf{n}\\right\\rangle=\\left\\langle\\mathbf{r},\\sum_{\\substack{l\\in\\{j c_{\\iota}=0\\}}}b_{l}\\mathbf{e}_{C_{l}}\\right\\rangle=\\left\\|\\sum_{l\\in\\{j c_{\\iota}=0\\}}b_{l}\\mathbf{e}_{C_{l}}\\right\\|^{2}=\\sum_{l\\in\\{j c_{\\iota}=0\\}}b_{l}^{2}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Therefore, as $b_{l}$ \u2019s corresponds to components of $\\left[H_{i c}^{i_{r}}\\right]_{k}$ , we conclude $\\left[H_{i c}^{i_{r}}\\right]_{k}=0$ . As $k$ was arbitrary, we get $H_{i c}^{i_{r}}=0$ . Similarly we can show $H_{v\\mathcal{L}}^{v_{r}}=0$ , and thus $H_{p_{*}}^{r}=0$ . Repeating the same argument, we can show $H_{p_{*}}^{p}=0$ . Finally, as $H$ is skew-symmetric, we have $\\begin{array}{r}{H_{r}^{p_{*}}=-(H_{p_{*}}^{r})^{\\intercal}=0}\\end{array}$ . \u53e3 ", "page_idx": 27}, {"type": "text", "text": "We now move on to V-I relations of resistors. To express V-I relations in terms of $w$ and $u$ , we adopt partial inverse. ", "page_idx": 27}, {"type": "text", "text": "Definition. [23, Definition 20.42] Let $\\mathbb{M}\\colon\\mathbf{R}^{d}\\rightrightarrows\\mathbf{R}^{d}$ be a set-valued operator and let $K$ be a closed linear subspace of $\\mathbf{R}^{d}$ . Denote $\\Pi_{K}\\colon\\ensuremath{\\mathbb{R}}^{d}\\to\\ensuremath{\\mathbb{R}}^{d}$ the projection onto $K$ as ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\Pi_{K}(z)=\\underset{k\\in K}{\\mathrm{argmin}}\\:\\|z-k\\|\\,.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The partial inverse of $\\mathbb{M}$ with respect to $K$ is the operator $\\mathbb{M}_{K}\\colon\\mathbb{R}^{d}\\rightrightarrows\\mathbb{R}^{d}$ defined by ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbf{\\dot{a}}\\,\\mathbf{M}_{K}=\\left\\{\\left(\\Pi_{K}\\mathbf{x}+\\Pi_{K^{\\perp}}\\mathbf{y},\\Pi_{K}\\mathbf{y}+\\Pi_{K^{\\perp}}\\mathbf{x}\\right)\\mid\\left(\\mathbf{x},\\mathbf{y}\\right)\\in\\operatorname{gra}\\mathbf{M}\\right\\},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "i.e., ", "page_idx": 27}, {"type": "text", "text": "We then prove important properties of the function related to V-I relations for resistors. ", "page_idx": 27}, {"type": "text", "text": "Lemma B.5. Suppose $f$ is $\\mu$ -strongly convex and $M$ -smooth function. Let $Q_{r},H_{r}^{r},J_{r}\\colon\\mathbf{R}^{|\\mathcal{R}|+m}\\rightarrow$ $\\mathbf{R}^{|\\mathcal{R}|+m}$ be a permutation matrix, a skew-symmetric matrix, a diagonal matrix with entries $1$ or 0 respectively and let $K=\\mathcal{R}(J_{r})$ . Define $F\\colon{\\bf R}^{|\\mathcal{R}|+m}\\rightarrow{\\bf R}$ as ", "page_idx": 27}, {"type": "equation", "text": "$$\nF(v_{\\mathcal{R}},x)=\\frac{1}{2}\\left\\|v_{\\mathcal{R}}\\right\\|^{2}+f(x).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Then the following holds. ", "page_idx": 27}, {"type": "text", "text": "(i) dom(Qr(\u2207F)KQr\u22ba \u2212Hrr )\u22121 = R|R|+m.   \n(ii) $(Q_{r}(\\nabla F)_{K}Q_{r}^{\\intercal}-H_{r}^{r})^{-1}$ is Lipschitz continuous monotone operator. ", "page_idx": 27}, {"type": "text", "text": "Proof. Take $(w_{r}^{l},u_{r}^{l})\\;\\in\\;{\\bf R}^{|\\mathcal{R}|+m}$ for $l\\;\\in\\;\\{1,2\\}$ , such that $u_{r}^{l}\\;\\in\\;(Q_{r}(\\nabla F)_{K}Q_{r}^{\\intercal})w_{r}^{l}$ . As $Q_{r}$ is permutation matrix, we know $(Q_{r})^{-1}=(Q_{r})^{\\top}$ , and thus $Q_{r}^{\\top}u_{r}^{l}\\in(\\nabla F)_{K}(Q_{r}^{\\top}w_{r}^{l})$ . Then there are $\\left[\\!\\!\\left[v_{\\mathcal{R}}^{l}\\right]\\!\\!\\right],\\left[i_{\\mathcal{R}}^{l}\\right]\\in\\mathbf{R}^{\\left|\\mathcal{R}\\right|+m}$ such that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\left[\\!\\!\\begin{array}{c}{^{i}\\!\\!\\slash_{\\mathfrak{R}}^{l}}\\\\ {y^{l}\\!\\!\\slash}\\end{array}\\!\\!\\right]=\\nabla F\\left[\\!\\!\\begin{array}{c}{v_{\\mathcal{R}}^{l}}\\\\ {x^{l}}\\end{array}\\!\\!\\right],\\quad\\left(Q_{r}^{\\intercal}w_{r}^{l},Q_{r}^{\\intercal}u_{r}^{l}\\right)=\\left(\\Pi_{K}\\left[\\!\\!\\begin{array}{c}{v_{\\mathcal{R}}^{l}}\\\\ {x^{l}}\\end{array}\\!\\!\\right]+\\Pi_{K^{\\perp}}\\left[\\!\\!\\begin{array}{c}{i_{\\mathcal{R}}^{l}}\\\\ {y^{l}}\\end{array}\\!\\!\\right],\\Pi_{K}\\left[\\!\\!\\begin{array}{c}{i_{\\mathcal{R}}^{l}}\\\\ {y^{l}}\\end{array}\\!\\!\\right]+\\Pi_{K^{\\perp}}\\left[\\!\\!\\begin{array}{c}{v_{\\mathcal{R}}^{l}}\\\\ {x^{l}}\\end{array}\\!\\!\\right]\\right).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "By [23, Proposition 20.44, (iii)], we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\left\\langle Q_{r}^{\\top}(w_{r}^{1}-w_{r}^{2}),Q_{r}^{\\top}(u_{r}^{1}-u_{r}^{2})\\right\\rangle=\\left\\langle\\left[v_{\\mathcal{R}}^{1}\\right]-\\left[v_{\\mathcal{R}}^{2}\\right],\\left[i_{\\mathcal{R}}^{1}\\right]-\\left[i_{\\mathcal{R}}^{2}\\right]\\right\\rangle.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Moreover, we can check ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\left[v_{R}^{1}\\right]-\\left[v_{R}^{2}\\right]\\right\\|^{2}+\\left\\|\\left[\\binom{i1}{R^{1}}-\\left[\\binom{i2}{R^{2}}\\right]\\right\\|^{2}}\\\\ &{=\\left\\|\\Pi_{K}\\left(\\left[v_{R}^{1}\\right]-\\left[v_{R}^{2}\\right]\\right)\\right\\|^{2}+\\left\\|\\Pi_{K^{\\perp}}\\left(\\left[v_{R}^{1}\\right]-\\left[v_{R}^{2}\\right]\\right)\\right\\|^{2}}\\\\ &{\\quad+\\left\\|\\Pi_{K}\\left(\\left[\\binom{i1}{y^{\\perp}}-\\left[\\binom{i2}{y^{2}}\\right]\\right)\\right\\|^{2}+\\left\\|\\Pi_{K^{\\perp}}\\left(\\left[\\binom{i1}{y^{\\perp}}-\\left[\\binom{i2}{y^{2}}\\right]\\right)\\right\\|^{2}=\\left\\|w_{r}^{1}-w_{r}^{2}\\right\\|^{2}+\\left\\|u_{r}^{1}-u_{r}^{2}\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Define $\\mu_{\\mathrm{min}}=\\{\\mu,1\\}$ and $M_{\\operatorname*{min}}=\\{M,1\\}$ . Then we can check $\\nabla F$ is $\\mu_{\\mathrm{min}}$ -strongly convex and $M_{\\mathrm{min}}$ -smooth, we see ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\left\\langle(i_{\\mathcal{R}}^{1},y^{1})-(i_{\\mathcal{R}}^{2},y^{2}),(v_{\\mathcal{R}}^{1},x^{1})-(v_{\\mathcal{R}}^{2},x^{2})\\right\\rangle}&{\\ge}&{\\mu_{\\mathrm{min}}\\left\\|(v_{\\mathcal{R}}^{1},x^{1})-(v_{\\mathcal{R}}^{2},x^{2})\\right\\|^{2},}\\\\ {\\left\\langle(i_{\\mathcal{R}}^{1},y^{1})-(i_{\\mathcal{R}}^{2},y^{2}),(v_{\\mathcal{R}}^{1},x^{1})-(v_{\\mathcal{R}}^{2},x^{2})\\right\\rangle}&{\\ge}&{M_{\\mathrm{min}}\\left\\|(i_{\\mathcal{R}}^{1},y^{1})-(i_{\\mathcal{R}}^{2},y^{2})\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Thus ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left\\langle w_{r}^{1}-w_{r}^{2},u_{r}^{1}-u_{r}^{2}\\right\\rangle}&{=}&{\\left\\langle Q_{r}^{\\top}(w_{r}^{1}-w_{r}^{2}),Q_{r}^{\\top}(u_{r}^{1}-u_{r}^{2})\\right\\rangle=\\left\\langle\\left[v_{R}^{1}\\right]-\\left[v_{R}^{2}\\right],\\left[\\frac{i_{R}^{1}}{y^{1}}\\right]-\\left[\\frac{i_{R}^{2}}{y^{2}}\\right]\\right\\rangle}\\\\ &{\\geq}&{\\frac{\\mu_{\\operatorname*{min}}+M_{\\operatorname*{min}}}{2}\\left(\\left\\|\\left[v_{r}^{1}\\right]-\\left[v_{R}^{2}\\right]\\right\\|^{2}+\\left\\|\\left[\\frac{i_{R}^{1}}{y^{1}}\\right]-\\left[\\frac{i_{R}^{2}}{y^{2}}\\right]\\right\\|^{2}\\right)}\\\\ &{=}&{\\frac{\\mu_{\\operatorname*{min}}+M_{\\operatorname*{min}}}{2}\\left(\\left\\|w_{r}^{1}-w_{r}^{2}\\right\\|^{2}+\\left\\|u_{r}^{1}-u_{r}^{2}\\right\\|^{2}\\right)}\\\\ &{\\geq}&{\\frac{\\mu_{\\operatorname*{min}}+M_{\\operatorname*{min}}}{2}\\left\\|w_{r}^{1}-w_{r}^{2}\\right\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "we see $Q_{r}(\\nabla F)_{K}Q_{r}^{\\intercal}$ is $\\frac{\\mu_{\\mathrm{min}}+M_{\\mathrm{min}}}{2}$ -strongly monotone. Note we can check $(\\nabla F)_{K}$ is also strongly monotone, by considering the special case $Q_{r}=I_{|r|+m}$ . Lastly, since $H_{r}^{r}$ is skew-symmetric, we know $\\langle H_{r}^{r}z,z\\rangle=0$ for all $z\\in\\mathbf{R}^{|\\mathcal{R}|+m}$ . Therefore for arbitrary $(\\tilde{w}_{r}^{l},\\tilde{u}_{r}^{l})\\in\\mathbf{R}^{|\\mathcal{R}|+m}$ with $l\\in\\{1,2\\}$ such that $\\tilde{u}_{r}^{l}\\in(Q_{r}(\\nabla F)_{K}Q_{r}^{\\intercal}-H_{r}^{r})\\,\\tilde{w}_{r}^{l}.$ , since $u_{r}^{l}=\\tilde{u}_{r}^{l}+H_{r}^{r}\\tilde{w}_{r}^{l}\\in(Q_{r}(\\nabla F)_{K}Q_{r}^{\\intercal})\\tilde{w}_{r}^{l}$ , ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\left\\langle\\tilde{w}_{r}^{1}-\\tilde{w}_{r}^{2},\\tilde{u}_{r}^{1}-\\tilde{u}_{r}^{2}\\right\\rangle}&{=}&{\\left\\langle\\tilde{w}_{r}^{1}-\\tilde{w}_{r}^{2},u_{r}^{1}-u_{r}^{2}-H_{r}^{r}(\\tilde{w}_{r}^{1}-\\tilde{w}_{r}^{2})\\right\\rangle}\\\\ &{=}&{\\left\\langle\\tilde{w}_{r}^{1}-\\tilde{w}_{r}^{2},u_{r}^{1}-u_{r}^{2}\\right\\rangle}\\\\ &{\\geq}&{\\displaystyle\\frac{\\mu_{\\operatorname*{min}}+M_{\\operatorname*{min}}}{2}\\left\\|w_{r}^{1}-w_{r}^{2}\\right\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "thus $\\big(Q_{r}(\\nabla F)_{K}Q_{r}^{\\intercal}-H_{r}^{r}\\big)$ is also $\\frac{\\mu_{\\mathrm{min}}\\!+\\!M_{\\mathrm{min}}}{2}$ -strongly monotone. ", "page_idx": 28}, {"type": "text", "text": "Now since $\\nabla F$ is maximal monotone, $(\\nabla F)_{K}$ is maximal monotone by [23, Proposition 20.44, (v)]. Since $(\\nabla F)_{K}$ is strongly monotone, we have $\\mathbf{dom}(\\nabla F)_{K}\\,=\\,\\mathbf{R}^{|\\mathcal{R}|+m}$ by [23, Proposition 22.11], and thus $\\boldsymbol{Q}_{r}(\\boldsymbol{\\nabla}\\boldsymbol{F})_{K}\\boldsymbol{Q}_{r}^{\\intercal}$ is maximal monotone by [129, Theorem 12]. Moreover, since both $Q_{r}(\\nabla F)_{K}Q_{r}^{\\intercal}$ and $-H_{r}^{r}$ have full domain, $\\big(Q_{r}(\\nabla F)_{K}{Q}_{r}^{\\intercal}-H_{r}^{r}\\big)$ is maximal monotone by [129, Theorem 10]. ", "page_idx": 28}, {"type": "text", "text": "Organizing, $\\big(Q_{r}(\\nabla F)_{K}Q_{r}^{\\intercal}-H_{r}^{r}\\big)$ is maximal monotone and strongly monotone. Therefore by [23, Proposition 22.11], we conclude (ii). Finally, observe ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{Q_{r}(\\nabla F)_{K}Q_{r}^{\\intercal}-H_{r}^{r})^{-1}}&{=}&{\\left(Q_{r}(\\nabla F)_{K}Q_{r}^{\\intercal}-H_{r}^{r}-\\frac{\\mu_{\\mathrm{min}}+M_{\\mathrm{min}}}{2}I_{|\\mathcal{R}|+m}+\\frac{\\mu_{\\mathrm{min}}+M_{\\mathrm{min}}}{2}I_{|\\mathcal{R}|+m}\\right.}\\\\ &&{=}&{\\left.\\mathbb{J}_{\\frac{2}{\\mu_{\\mathrm{min}}+M_{\\mathrm{min}}}}\\mathbb{M}^{\\circ}\\,\\frac{2}{\\mu_{\\mathrm{min}}+M_{\\mathrm{min}}}I_{|\\mathcal{R}|+m}.\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Since \ud835\udd44is monotone,\u00b5m in+2Mmin \ud835\udd44is also monotone, by [23, Corollary 23.9] we know \ud835\udd41\u00b5min+2Mmin \ud835\udd44 is 1-Lipschitz continuous. Therefore (Qr(\u2207F)KQr\u22ba \u2212Hrr )\u22121 is\u00b5min+2Mmin - Lipschitz continuous. Finally it is monotone as it is an inverse of a monotone operator, we conclude (ii). \u53e3 ", "page_idx": 28}, {"type": "text", "text": "Finally, we are ready to prove Theorem 2.1. ", "page_idx": 28}, {"type": "text", "text": "Proof of Theorem 2.1. By Lemma B.2 it is suffices to show Theorem B.1. ", "page_idx": 28}, {"type": "text", "text": "(i) Well-posedness and Lipschitz continuity of $v_{\\cal C},i_{\\cal C}$ . Existence of the whole curve $(v,x,i,y)$ . Define an operator $\\mathbb{A}\\colon\\mathbb{R}^{|\\mathcal{L}|+|\\mathcal{C}|}\\rightrightarrows\\mathbb{R}^{|\\mathcal{L}|+|\\mathcal{C}|}$ as ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\mathbb{A}=\\Bigg\\{\\left(\\left[\\underset{i\\,c}{\\left[\\begin{array}{l}{v_{c}}\\\\ {i\\,c}\\end{array}\\right]},\\,-\\left[\\underset{v_{c}}{i\\,c}\\right]\\right)\\ \\Big|\\ \\exists\\,v=(v_{\\mathcal{R}},v_{\\mathcal{L}},v_{c}),\\ i=(i_{\\mathcal{R}},i_{\\mathcal{L}},i_{c}),\\ (x,y)}&{\\quad(19)}\\\\ &{}&{\\mathrm{such~that~}\\left[\\underset{y}{i}\\right]\\in\\mathcal{N}(B),\\ \\left[\\underset{x}{v}\\right]\\in\\mathcal{R}(B^{\\top}),\\ y=\\nabla f(x),\\ v_{\\mathcal{R}}=i_{\\mathcal{R}}\\Bigg\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We prove $\\mathbb{A}$ is maximal monotone by providing an explicit expression of $\\mathbb{A}$ and apply Theorem B.3. ", "page_idx": 28}, {"type": "text", "text": "From Corollary B.4.1, we know there is a diagonal matrix $J\\colon{\\mathbf{R}}^{\\sigma+m}\\rightarrow{\\mathbf{R}}^{\\sigma+m}$ , a permutation matrix $Q\\colon\\mathbf{R}^{\\sigma+m}\\rightarrow\\mathbf{\\dot{R}}^{\\sigma+m}$ and a corresponding skew-symmetric matrix $H\\colon\\mathbf{R}^{\\sigma+m}\\rightarrow{\\dot{\\mathbf{R}}}^{\\sigma+m}$ that satisfies ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\left[\\!\\!{\\begin{array}{c}{i}\\\\ {y}\\end{array}}\\!\\!\\right]\\in{\\mathcal{N}}(B),\\ \\left[\\!\\!{\\boldsymbol{x}}\\right]\\in{\\mathcal{R}}(B^{\\intercal})\\iff\\left[\\!\\!{\\begin{array}{c}{u_{p_{*}}}\\\\ {u_{p}}\\\\ {u_{r}}\\end{array}}\\!\\!\\right]=\\left[\\!\\!{\\begin{array}{c c c}{H_{p}^{p_{*}}}&{H_{p_{*}}^{p_{*}}}&{H_{r}^{p_{*}}}\\\\ {H_{p}^{p}}&{0}&{0}\\\\ {H_{p}^{r}}&{0}&{H_{r}^{r}}\\end{array}}\\!\\!\\right]\\left[\\!\\!{\\begin{array}{c}{w_{p}}\\\\ {w_{p_{*}}}\\\\ {w_{r}}\\end{array}}\\!\\!\\right]\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $w$ and $u$ are defined as in Lemma B.4. ", "page_idx": 29}, {"type": "text", "text": "Define $F\\colon{\\bf R}^{|\\mathcal{R}|+m}\\rightarrow{\\bf R}$ as $\\begin{array}{r}{F(v_{\\mathcal{R}},x)=\\frac{1}{2}\\left\\|v_{\\mathcal{R}}\\right\\|^{2}+f(x)}\\end{array}$ . Then it is straight forward that ", "page_idx": 29}, {"type": "equation", "text": "$$\ny=\\nabla f(x),\\ v_{\\mathcal{R}}=i_{\\mathcal{R}}\\iff\\left[i_{\\mathcal{R}}\\right]=\\nabla F\\left[{v_{\\mathcal{R}}}\\right].\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "By the construction of $w,\\,u$ , there is a diagonal matrix $J_{r}\\colon{\\mathbf{R}}^{|\\mathcal{R}|+m}\\rightarrow{\\mathbf{R}}^{|\\mathcal{R}|+m}$ with entries 1 or 0 and a permutation matrix $Q_{r}\\colon\\mathbf{R}^{|\\mathcal{R}|+m}\\rightarrow\\mathbf{\\bar{R}}^{|\\mathcal{R}|+m}$ that satisfies ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\left[Q_{r}^{-1}w_{r}\\right]=\\left[Q_{r}^{\\mathsf{T}}w_{r}\\right]=\\left[I_{|\\mathcal{R}|+m}\\quad I_{|\\mathcal{R}|+m}\\right]\\left[\\!\\!\\begin{array}{c}{v_{\\mathcal{R}}}\\\\ {x}\\\\ {i_{\\mathcal{R}}}\\\\ {y}\\end{array}\\!\\!\\right].\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Define $K_{r}=\\mathcal{R}(J_{r})$ . Note, we can check $\\Pi_{K_{r}}(z)=J_{r}z$ and $\\Pi_{K_{r}^{\\perp}}(z)=(I_{|\\mathcal{R}|+m}-J_{r})z$ for all $z\\in\\mathbf{R}^{|\\mathcal{R}|+m}$ . Recalling (18) in Lemma B.5 we see ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\left[\\!\\!\\begin{array}{l}{i_{\\mathcal{R}}}\\\\ {y}\\end{array}\\!\\!\\right]=\\nabla F\\left[\\!\\!\\begin{array}{l}{v_{\\mathcal{R}}}\\\\ {x}\\end{array}\\!\\!\\right]\\iff u_{r}=(Q_{r}(\\nabla F)_{K_{r}}Q_{r}^{\\intercal})w_{r}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "From ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(Q_{r}(\\nabla F)_{K_{r}}Q_{r}^{\\intercal})w_{r}=u_{r}=H_{p}^{r}w_{p}+H_{r}^{r}w_{r},}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "we get expression for $w_{r}$ in terms of $w_{p}$ ", "page_idx": 29}, {"type": "equation", "text": "$$\nw_{r}=\\left(Q_{r}(\\nabla F)_{K_{r}}Q_{r}^{\\intercal}-H_{r}^{r}\\right)^{-1}H_{p}^{r}w_{p}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Now focusing on the expression for $\\left[\\begin{array}{l}{u_{p_{*}}}\\\\ {u_{p}}\\end{array}\\right]$ , since $H_{p}^{r}=-(H_{r}^{p_{*}})^{\\intercal}$ as $H$ is skew-symmetric, eliminating $w_{r}$ by applying (21) we see ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\left[\\!\\!\\begin{array}{c c}{u_{p_{*}}}\\\\ {u_{p_{*}}}\\end{array}\\!\\!\\right]}&{=}&{\\left[\\!\\!\\begin{array}{c c c}{H_{p_{*}}^{p_{*}}}&{H_{p_{*}}^{p_{*}}}&{H_{r^{*}}^{p_{*}}}\\\\ {H_{p_{*}}^{p}}&{0}&{0}\\end{array}\\!\\!\\right]\\left[\\!\\!\\begin{array}{c}{w_{p}}\\\\ {w_{p_{*}}}\\end{array}\\!\\!\\right]}\\\\ &{=}&{-\\underbrace{\\left(-\\left[\\!\\!\\begin{array}{c c}{H_{p_{*}}^{p_{*}}}&{H_{p_{*}}^{p_{*}}}\\\\ {H_{p}^{p}}&{0}\\end{array}\\!\\!\\right]+\\left[\\!\\!H_{p}^{r}\\&{0}\\right]^{\\mathsf{T}}\\left(\\!\\!Q_{r}(\\nabla F)_{K_{r}}Q_{r}^{\\mathsf{T}}-H_{r}^{r}\\right)^{-1}\\left[\\!\\!H_{p}^{r}\\&{0}\\!\\!\\right]\\!\\right)}_{\\left[\\!\\!w_{p_{*}}\\right]}\\left[\\!\\!\\begin{array}{c}{w_{p_{*}}}\\\\ {w_{p_{*}}}\\end{array}\\!\\!\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Since $H$ is skew-symmetric, its principal minors $-\\begin{array}{r l}{\\left[H_{p}^{p_{*}}\\right.}&{{}\\left.H_{p_{*}}^{p_{*}}\\right]}\\\\ {H_{p}^{p}}&{{}\\left.0\\right]}\\end{array}$ and $-H_{r}^{r}$ are skew-symmetric and so maximal monotone. Furthermore, by Lemma B.5 we have $(\\bar{Q}_{r}(\\nabla F)_{K_{r}}Q_{r}^{\\intercal}-H_{r}^{r})^{-1}$ is maximal monotone and $\\mathbf{dom}((\\nabla F)^{1,-1}-H_{r}^{r})^{-1}=\\mathbf{R}^{|\\mathcal{R}|+m}$ . Invoking [129, Theorem 11, 12], we conclude $\\mathbb{B}$ is maximal monotone. ", "page_idx": 29}, {"type": "text", "text": "Organizing, we see ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{\\phi}_{v,c}^{i}\\Big]\\in-\\mathbb{A}\\displaystyle\\left[\\!\\!\\begin{array}{c}{v_{c}}\\\\ {i_{c}}\\end{array}\\!\\!\\right]\\iff\\left[\\!\\!\\begin{array}{c}{i}\\\\ {y}\\end{array}\\!\\!\\right]\\in\\mathcal{N}(B),\\:\\:\\left[\\!\\!\\begin{array}{c}{v}\\\\ {x}\\end{array}\\!\\!\\right]\\in\\mathcal{R}(B^{\\top}),\\:\\:y=\\nabla f(x),\\:\\:v_{\\mathcal{R}}=i_{\\mathcal{R}}}\\\\ &{\\qquad\\qquad\\iff\\left[\\!\\!\\begin{array}{c}{u_{p_{*}}}\\\\ {u_{p}}\\end{array}\\!\\!\\right]=-\\mathbb{B}\\displaystyle\\left[\\!\\!\\begin{array}{c}{w_{p}}\\\\ {w_{p_{*}}}\\end{array}\\!\\!\\right],\\:\\mathrm{where}\\ \\displaystyle\\left[\\!\\!\\begin{array}{c}{w}\\\\ {u}\\end{array}\\!\\!\\right]=\\left[\\!\\!\\begin{array}{c c}{Q}&{0}\\\\ {0}&{Q}\\end{array}\\!\\!\\right]\\left[\\!\\!\\begin{array}{c c}{J}&{I_{\\sigma+m}-J}\\\\ {I_{\\sigma+m}-J}&{J}\\end{array}\\!\\!\\right]\\left[\\!\\!\\begin{array}{c}{v}\\\\ {\\vdots}\\\\ {v}\\end{array}\\!\\!\\!\\right]=\\mathbb{B}(B).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Therefore there is a diagonal matrix $J_{p,p^{*}}\\colon\\mathbf{R}^{|\\mathcal{L}|+|\\mathcal{C}|}\\to\\mathbf{R}^{|\\mathcal{L}|+|\\mathcal{C}|}$ with entries 1 or 0 and a permutation matrix $Q_{p,p^{*}}\\colon\\mathbf{R}^{|\\mathcal{L}|+|\\mathcal{C}|}\\to\\mathbf{R}^{|\\mathcal{L}|+|\\mathcal{C}|}$ that satisfies ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\left[\\!\\!\\begin{array}{l}{i c}\\\\ {v_{\\mathcal{L}}}\\end{array}\\!\\!\\right]\\in-\\mathbb{A}\\left[\\!\\!\\begin{array}{l}{v_{\\mathcal{C}}}\\\\ {i_{\\mathcal{L}}}\\end{array}\\!\\!\\right]\\iff\\left[\\!\\!\\begin{array}{l}{u_{p_{*}}}\\\\ {u_{p}}\\end{array}\\!\\!\\right]=-\\mathbb{B}\\left[\\!\\!\\begin{array}{l}{w_{p}}\\\\ {w_{p_{*}}}\\end{array}\\!\\!\\right],\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\left[Q_{p,p^{*}}^{\\mathsf{T}}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\! \n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Therefore, for $K_{p,p^{*}}=\\mathcal{R}(J_{p,p^{*}})$ we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbb{A}_{K_{p,p^{*}}}=Q_{p,p^{*}}^{\\boldsymbol{\\mathsf{T}}}\\mathbb{B}\\,Q_{p,p^{*}}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Since $\\mathbb{B}$ is a maximal monotone operator with dom $\\mathbb{B}=\\mathbf{R}^{\\vert\\mathcal{L}\\vert+\\vert\\mathcal{C}\\vert}$ , we have $Q_{p,p^{*}}^{\\mathsf{T}}\\mathbb{B}\\,Q_{p,p^{*}}$ is maximal monotone. Finally from [23, Proposition 20.44, (v)], we conclude is maximal monotone. ", "page_idx": 30}, {"type": "text", "text": "By applying Theorem B.3 and its remark, we know there is a unique Lipschitz continuous curve $(v_{\\mathcal{C}},i_{\\mathcal{L}})\\colon[0,\\infty)\\to\\mathbf{R}^{|\\mathcal{L}|}\\times\\mathbf{R}^{|\\mathcal{C}|}$ that satisfies ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\frac{d}{d t}\\left[v_{\\mathcal{C}}(t)\\right]=-m\\mathbb{A}\\left[v_{\\mathcal{C}}(t)\\right]\\in-\\mathbb{A}\\left[v_{\\mathcal{C}}(t)\\right]\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "for almost all $t~\\in~[0,\\infty)$ , where $m\\mathbb{A}$ is the minimum-norm selection of $\\mathbb{A}$ . Let $\\begin{array}{r l}{\\left[i c(t)\\right]}&{{}=}\\end{array}$ $-m\\mathbb{A}\\left[v_{\\mathcal{L}}(t)\\right].$ Moreover, the definition of $\\mathbb{A}$ implies the existence of accompanying curves $v_{\\mathcal{R}},i_{\\mathcal{R}}$ , $x$ and $y$ that satisfy KCL, KVL and V-I relations. This concludes the existence of the curve. ", "page_idx": 30}, {"type": "text", "text": "(ii) The whole flow $(v,x,i,y)$ is well-posed and Lipschitz continuous.   \nFinally, we show other curves besides $(v_{\\mathcal{L}},i_{\\mathcal{C}})$ are defined uniquely and Lipschitz continuous. To do so, we prove there is a Lipschitz continuous function $\\mathcal{G}\\colon\\mathbf{R}^{|\\mathcal{C}|}\\times\\mathbf{R}^{|\\mathcal{L}|}\\rightarrow\\mathbf{R}^{\\sigma}\\times\\mathbf{R}^{\\sigma}\\times\\mathbf{R}^{m}\\times\\mathbf{R}^{m}$ that satisfies ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathcal{G}(v\\mathcal{L},i c)=(v,i,x,y).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "We prove the claim by finding the explicit expression of the component functions of $G$ . We first show one key equation ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{w_{p_{*}}=-(H_{p_{*}}^{p_{*}})^{\\mathsf{T}}u_{p_{*}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "From (20) we have $H_{p_{*}}^{p_{*}}w_{p_{*}}=u_{p_{*}}$ . And as $H$ is skew-symmetric, we have $H_{p}^{p}=-(H_{p_{*}}^{p_{*}})^{\\intercal}$ . The core information we additionally use here, is the V-I relations ddtwp = up\u2217. As differentiation is a linear operation, we get (23) by following ", "page_idx": 30}, {"type": "equation", "text": "$$\n-\\left(H_{p_{*}}^{p_{*}}\\right)^{\\intercal}u_{p_{*}}=H_{p}^{p}u_{p_{*}}=H_{p}^{p}\\left(\\frac{d}{d t}w_{p}\\right)=\\frac{d}{d t}\\left(H_{p}^{p}w_{p}\\right)=\\frac{d}{d t}u_{p}=w_{p_{*}}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Recall, from (22) we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\boldsymbol{u}_{p_{*}}=\\left(H_{p}^{p_{*}}+H_{r}^{p_{*}}\\left(Q_{r}(\\nabla F)_{K_{r}}Q_{r}^{\\intercal}-H_{r}^{r}\\right)^{-1}H_{p}^{r}\\right)\\boldsymbol{w}_{p}+H_{p_{*}}^{p_{*}}\\boldsymbol{w}_{p_{*}}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Moving the last term of right hand side to left hand side, multiplying both sides by $-(H_{p_{*}}^{p_{*}})\\boldsymbol{\\mathsf{T}}$ and using (23), we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\big(\\mathbb I+(H_{p_{*}}^{p_{*}})^{\\top}H_{p_{*}}^{p_{*}}\\big)\\,w_{p_{*}}=-(H_{p_{*}}^{p_{*}})^{\\top}\\,\\Big(H_{p}^{p_{*}}+H_{r}^{p_{*}}\\,(Q_{r}(\\nabla F)_{K_{r}}Q_{r}^{\\top}-H_{r}^{r})^{-1}\\,H_{p}^{r}\\Big)\\,w_{p}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Since $\\mathbb{I}+(H_{p_{*}}^{p_{*}})^{\\top}H_{p_{*}}^{p_{*}}\\succ0$ its inverse exists, we conclude ", "page_idx": 30}, {"type": "equation", "text": "$$\nw_{p_{*}}=\\underbrace{-\\left(\\mathbb{I}+(H_{p_{*}}^{p_{*}})^{\\top}H_{p_{*}}^{p_{*}}\\right)^{-1}(H_{p_{*}}^{p_{*}})^{\\top}\\left(H_{p}^{p_{*}}+H_{r}^{p_{*}}\\left(Q_{r}(\\nabla F)_{K_{r}}Q_{r}^{\\top}-H_{r}^{r}\\right)^{-1}H_{p}^{r}\\right)}_{=:\\mathbf{C}}w_{p}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Organizing, we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\left[\\!\\!\\begin{array}{c}{w_{p}}\\\\ {w_{p_{*}}}\\\\ {w_{r}}\\end{array}\\!\\!\\right]=\\left(\\left[\\!\\!\\begin{array}{c}{\\mathbb{I}}\\\\ {0}\\\\ {0}\\end{array}\\!\\!\\right]+\\left[\\!\\!\\begin{array}{c}{0}\\\\ {\\mathbb{C}}\\\\ {0}\\end{array}\\!\\!\\right]+\\left[\\!\\!\\begin{array}{c}{0}\\\\ {0}\\\\ {\\left(Q_{r}(\\nabla F)_{K_{r}}Q_{r}^{\\intercal}-H_{r}^{r}\\right)^{-1}H_{p}^{r}\\!\\!\\right]\\!\\!\\right)w_{p}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "From Lemma B.5 we know $\\big(Q_{r}(\\nabla F)_{K_{r}}Q_{r}^{\\intercal}-H_{r}^{r}\\big)^{-1}$ is Lipschitz continuous, and clearly linear operators are Lipschitz continuous, so $\\mathbb{C}$ is Lipschitz continuous as it is composition and sum of ", "page_idx": 30}, {"type": "text", "text": "Lipschitz continuous functions. Therefore $w_{p}\\mapsto w$ is Lipschitz continuous. Finally since $u=H w$ and $H$ is indeed Lipschitz continuous as a linear operator, mapping $w_{p}\\mapsto u$ is also Lipschitz continuous. As $(w,u)$ is rearrangement of $(v,i,x,y)$ and $w_{v c},w_{i_{C}}$ are component functions of $v_{\\mathcal{C}}$ and $i_{\\mathcal{L}}$ , we get the desired result. ", "page_idx": 31}, {"type": "text", "text": "For $(v,i,x,y)$ that satisfies (11) with proper initial value, we know $(v_{\\cal C},i_{\\cal C})$ is uniquely defined by previous observation and $(v,i,x,y)\\overset{\\cdot}{=}\\bar{\\mathcal{G}}(v_{\\mathcal{C}},i_{\\mathcal{L}})$ should hold, we conclude $(v,i,x,y)$ is uniquely determined since $\\mathcal{G}$ is single valued. Furthermore, as $v_{c}(t)$ and $i_{\\mathcal{L}}(t)$ are Lipschitz continuous with respect to $t$ , we have $(v(\\bar{t}),i(t),x(t),y(t))=\\mathcal{G}(v_{\\mathcal{C}}(t),\\dot{i}_{\\mathcal{L}}(t))$ is also Lipschitz continuous as it is composition of Lipschitz continuous functions. This concludes the proof. ", "page_idx": 31}, {"type": "text", "text": "C Equilibrium condition ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Define the set of voltages and currents in the equilibrium of the interconnect ", "page_idx": 32}, {"type": "equation", "text": "$$\nD_{x,y}=\\left\\{(v,i)\\mid A i=\\left[{\\frac{-y}{0}}\\right],v=A^{\\mathsf{T}}\\left[{\\vphantom{\\int_{v}^{v}}}\\right],v_{\\mathcal{R}}=D_{\\mathcal{R}}i_{\\mathcal{R}},v_{\\mathcal{L}}=0,i_{\\mathcal{C}}=0\\right\\}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Lemma C.1. Assume the dynamic interconnect is admissible. Then for all $(v,i)\\in D_{x,y}$ we have ", "page_idx": 32}, {"type": "equation", "text": "$$\nv_{\\mathcal{R}}=i_{\\mathcal{R}}=0.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Proof. First, note that for all $(v,i)$ in the dynamic interconnect ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\langle v,i\\rangle=\\langle e,A i\\rangle=\\left\\langle\\left[x\\right],\\left[{-y\\atop0}\\right]\\right\\rangle=-\\langle x,y\\rangle.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Now suppose $(v,i)\\in D_{x,y}$ , then we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\langle v,i\\rangle=\\langle v_{\\mathcal{R}},i_{\\mathcal{R}}\\rangle=\\|i_{\\mathcal{R}}\\|_{D_{\\mathcal{R}}}^{2}=-\\langle x,y\\rangle.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "From the admissibility assumption we have $x\\in\\mathcal{R}(E^{\\intercal})$ and $y\\in\\mathcal{N}(E)$ . Thus ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\|i_{\\mathcal{R}}\\|_{D_{\\mathcal{R}}}^{2}=-\\langle x,y\\rangle=0,\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "which concludes the proof. ", "page_idx": 32}, {"type": "text", "text": "Theorem C.2. Assume the dynamic interconnect is admissible. If $(x^{\\star},y^{\\star})$ is a primal-dual solution pair (with zero duality gap) for the optimization problem, then there exist vC \u2208R|C| and iL \u2208R|L| such that ", "page_idx": 32}, {"type": "equation", "text": "$$\n((0,0,v_{}c{}),(0,i_{}c{},0){})\\in D_{x^{\\star},y^{\\star}}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Conversely, if $y\\in\\partial f(x)$ and ", "page_idx": 32}, {"type": "equation", "text": "$$\n((v_{\\mathcal{R}},v_{\\mathcal{L}},v_{\\mathcal{C}}),(i_{\\mathcal{R}},i_{\\mathcal{L}},i_{\\mathcal{C}}))\\in D_{x,y},\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "then $v_{\\mathcal{R}}=i_{\\mathcal{R}}=0$ and $(x,y)$ is a primal-dual solution pair (with zero-duality) for the optimization problem. ", "page_idx": 32}, {"type": "text", "text": "Proof. First, observe the admissibility assumption can be rewritten as ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\{(x,y)\\mid\\exists(v,i){\\mathrm{~such~that~}}(v,i)\\in D_{x,y}\\}=\\mathcal{R}(E^{\\intercal})\\times{\\mathcal{N}}(E).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Now suppose $(x^{\\star},y^{\\star})\\in X^{\\star}\\times Y^{\\star}$ . Then by Karush-Kuhn-Tucker (KKT) optimality conditions, we have $(x^{\\star},y^{\\star})\\in\\mathcal{R}(E^{\\sf T})\\times\\mathcal{N}(E)$ . Thus there exists $(v^{\\star},i^{\\star})$ such that ", "page_idx": 32}, {"type": "equation", "text": "$$\n(v^{\\star},i^{\\star})=((v_{\\mathcal{R}}^{\\star},0,v_{\\mathcal{C}}^{\\star}),(i_{\\mathcal{R}}^{\\star},i_{\\mathcal{L}}^{\\star},0))\\in D_{x^{\\star},y^{\\star}}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Furthermore from Lemma C.1 we have $v_{\\mathcal{R}}^{\\star}=i_{\\mathcal{R}}^{\\star}=0$ . Therefore $v_{c}^{\\star},i_{\\mathcal{L}}^{\\star}$ are the vectors that satisfiy the desired statement. ", "page_idx": 32}, {"type": "text", "text": "Conversely, suppose ", "page_idx": 32}, {"type": "equation", "text": "$$\n(v,i)=((v_{\\mathcal{R}},v_{\\mathcal{L}},v_{\\mathcal{C}}),(i_{\\mathcal{R}},i_{\\mathcal{L}},i_{\\mathcal{C}}))\\in D_{x,y}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "From Lemma C.1, we have $v_{\\mathcal{R}}=i_{\\mathcal{R}}=0$ . Moreover, since there exists $(v,i)$ such that $(v,i)\\in D_{x,y}$ , by admissibility assumption we have $(x,y)\\in\\mathcal{R}(E^{\\intercal})\\times\\mathcal{N}(E)$ . Finally, given the assumption that $\\bar{y}\\in\\partial f(x)$ , by using KKT optimality conditions, we conclude $(x,y)\\in X^{\\star}\\times Y^{\\star}$ . ", "page_idx": 32}, {"type": "text", "text": "D Energy dissipation analysis ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "In this section, we provide the proof of Theorem 2.2. The energy function (6) used in the proof is related to the Lyapunov function considered in [158]. However, the dissipativity theory presented in [158, 159] does not directly apply to our setup. In our setup, we allow cases where $v_{C},i_{L}$ oscillate, for example, a circuit with a disconnected $L-C$ loop. The proof is obtained by combining Barbalat\u2019s lemma [85, Lemma 8.2] with Theorem 2.1. ", "page_idx": 33}, {"type": "text", "text": "Proof of Theorem 2.2. Let $(x^{\\star},y^{\\star})$ be a primal-dual solution pair. Then by Theorem C.2, there is $(v^{\\star},i^{\\star})\\in D_{x^{\\star},y^{\\star}}$ that satisfies ", "page_idx": 33}, {"type": "equation", "text": "$$\n(v^{\\star},i^{\\star})=((0,0,v_{\\mathcal{C}}^{\\star}),(0,i_{\\mathcal{L}}^{\\star},0)).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "In particular, $i_{\\mathcal{C}}^{\\star}=0$ and $v_{\\mathcal{L}}^{\\star}=0$ . Define the total energy at time $t$ as ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathcal{E}(t)=\\frac{1}{2}\\|v_{\\mathcal{C}}-v_{\\mathcal{C}}^{\\star}\\|_{D_{\\mathcal{C}}}^{2}+\\frac{1}{2}\\|i_{\\mathcal{L}}-i_{\\mathcal{L}}^{\\star}\\|_{D_{\\mathcal{L}}}^{2}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Then the power at time $t$ is given by ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{l l l}{\\frac{d}{d t}\\mathcal{E}(t)}&{\\;=\\;}&{\\langle v_{C}-v_{C}^{\\star},D_{C}\\dot{v}_{C}\\rangle+\\langle i_{C}-i_{C}^{\\star},D_{C}\\dot{i}_{C}\\rangle}\\\\ &{\\;=\\;}&{\\langle v_{C}-v_{C}^{\\star},i_{C}-i_{C}^{\\star}\\rangle+\\langle i_{C}-i_{C}^{\\star},v_{C}-v_{C}^{\\star}\\rangle}\\\\ &{\\;=\\;}&{-\\langle v_{R}-y_{R}^{\\star},i_{R}-i_{R}^{\\star}\\rangle-\\langle x-x^{\\star},y-y^{\\star}\\rangle}\\\\ &{\\;=\\;}&{-\\|i_{R}\\|_{D_{R}}^{2}-\\underbrace{\\langle x-x^{\\star},y-y^{\\star}\\rangle}_{\\ge0}}\\\\ &{\\,\\le\\;0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where we used (24) and the monotonicity of $\\partial f$ . Therefore $\\begin{array}{r}{\\mathcal{E}(\\infty)\\,=\\,\\operatorname*{lim}_{t\\rightarrow\\infty}\\mathcal{E}(t)}\\end{array}$ exists. Now, integrating from 0 to $\\infty$ we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n0\\leq\\int_{0}^{\\infty}\\langle x(t)-x^{\\star},y(t)-y^{\\star}\\rangle d t\\leq\\mathcal{E}(0)-\\mathcal{E}(\\infty)<\\infty.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "From Theorem 2.1 we know the integrand is Lipschitz continuous, by Barbalat\u2019s lemma we get ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{t\\rightarrow\\infty}\\langle x(t)-x^{\\star},y(t)-y^{\\star}\\rangle=0.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Since $f$ is $\\mu$ -strongly convex and $M$ -smooth, $\\nabla f$ and $(\\nabla f)^{-1}$ are strictly monotone, we conclude ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{t\\to\\infty}x(t)=x^{\\star},\\quad\\operatorname*{lim}_{t\\to\\infty}y(t)=y^{\\star},\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "which is our desired result. ", "page_idx": 33}, {"type": "text", "text": "E Centralized classical algorithms ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "E.1 Resistors and Moreau envelope ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "For $R>0$ , define the Moreau envelope of $f\\colon\\mathbf{R}^{m}\\rightarrow\\mathbf{R}$ of parameter $R$ as ", "page_idx": 34}, {"type": "equation", "text": "$$\n{}^{R}f(x)=\\operatorname*{inf}_{z\\in\\mathbf{R}^{m}}\\left(f(z)+{\\frac{1}{2R}}\\|z-x\\|_{2}^{2}\\right).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Then $R_{f}$ is $1/R$ -smooth with gradient given by ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\nabla^{R}f(x)={\\frac{1}{R}}(x-\\mathbf{prox}_{R f}(x)).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "In this section we show that composing linear resistors with $\\partial f$ is equivalent to taking a Moreau envelope of $f$ . See two circuits below. ", "page_idx": 34}, {"type": "image", "img_path": "9Jmt1eER9P/tmp/65fb28ba3f2a39dd208343e83e2857d8e44d8277be5b5a06d2fef96870de93ff.jpg", "img_caption": [], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "By KCL and Ohm\u2019s law for the first circuit, we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\frac{1}{R}(x-\\tilde{x})=i\\in\\partial f(\\tilde{x}),\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "which is equivalent to $\\tilde{x}=\\mathbf{prox}_{R f}(x)$ . Using identity for the gradient of the Moreau envelope, we get ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\nabla^{R}f(x)={\\frac{1}{R}}{\\big(}x-\\mathbf{prox}_{R f}(x){\\big)}={\\frac{1}{R}}(x-{\\tilde{x}})=i.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Therefore, the V-I relation on $m$ pins of $x$ in both circuits is identical. ", "page_idx": 34}, {"type": "text", "text": "As a consequence, consider $f$ to be $1/R$ -smooth. Let $\\tilde{f}$ be pre-Moreau envelope of $f$ , i.e., $R\\:\\tilde{f}=f$ . Note that $\\tilde{f}$ is a convex function. Then from the series connection of the resistors ( $-R$ in series with $R$ is the same as 0-ohm resistor), we get the equivalence of the two circuits below. ", "page_idx": 34}, {"type": "image", "img_path": "9Jmt1eER9P/tmp/024ac82e201e07aafd012bb45ce420b6b2436ad6525e18311ada32c427cee1c7.jpg", "img_caption": [], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "Note that for this circuit $x=\\tilde{x}-R\\nabla f(\\tilde{x})$ . ", "page_idx": 34}, {"type": "text", "text": "E.2 Gradient flow ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Let $f\\colon\\mathbf{R}^{m}\\rightarrow\\mathbf{R}$ be a convex function. Consider the circuit below. ", "page_idx": 34}, {"type": "image", "img_path": "9Jmt1eER9P/tmp/bcb7fc6d6f7334aea309a65516f63fec98a72ccea9fcce626fe18fea2f071e74.jpg", "img_caption": [], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "Let $x$ be the potentials at $m$ pins of $\\partial f$ , and $y$ be the current entering those pins. Applying KCL and the V-I relations of the capacitor we get ", "page_idx": 35}, {"type": "equation", "text": "$$\nD c{\\frac{d}{d t}}v c=i c=-y\\in-\\partial f(x).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Since $e$ is connected to ground, we have $v_{\\mathcal{C}}=x-e=0$ . The resulting differential inclusion is ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\frac{d}{d t}x\\in-D_{\\mathcal{C}}^{-1}\\partial f(x).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "For the automatic discretization of the continuous-time dynamics of this circuit, we define the energy at time $k$ as ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathcal{E}_{k}=\\frac{1}{2}\\|\\boldsymbol{x}^{k}-\\boldsymbol{x}^{\\star}\\|_{D_{c}}^{2}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "E.3 Nesterov acceleration ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Let $f\\colon\\mathbf{R}^{m}\\rightarrow\\mathbf{R}$ be a $1/R$ -smooth convex function. Consider the circuit below. ", "page_idx": 35}, {"type": "text", "text": "Observe, by Ohm\u2019s law and $y=\\nabla f(x)$ we have ", "page_idx": 35}, {"type": "equation", "text": "$$\nx^{+}=x-R y=x-R\\nabla f(x).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "From KCL, KVL, and V-I relations we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{{\\frac{d}{d t}i_{\\mathcal{L}}}}&{{=}}&{{D_{\\mathcal{L}}^{-1}(v_{\\mathcal{C}}-x^{+})}}\\\\ {{}}&{{}}&{{}}\\\\ {{\\frac{d}{d t}v_{\\mathcal{C}}}}&{{=}}&{{-D_{\\mathcal{C}}^{-1}\\nabla f(x).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Applying KCL and Ohm\u2019s law at $x$ , it follows ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\nabla f(x)=i\\mathcal{L}+\\frac{1}{R}(v\\boldsymbol{c}-\\boldsymbol{x}^{+}),\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "which implies that ", "page_idx": 35}, {"type": "equation", "text": "$$\nx=v_{\\mathcal{C}}+R i_{\\mathcal{L}}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Differentiating the above equality twice and plugging in the V-I relations, we get ", "page_idx": 35}, {"type": "equation", "text": "$$\n{\\frac{d^{2}}{d t^{2}}}x=-R(D_{\\mathcal{L}}D_{\\mathcal{C}})^{-1}\\nabla f(x)-R D_{\\mathcal{L}}^{-1}{\\frac{d}{d t}}x-(D_{\\mathcal{C}}^{-1}-R^{2}D_{\\mathcal{L}}^{-1}){\\frac{d}{d t}}\\nabla f(x).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Reorganizing, we conclude ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\frac{d^{2}}{d t^{2}}x+R D_{\\mathscr{L}}^{-1}\\frac{d}{d t}x+\\big(D_{\\mathscr{C}}^{-1}-R^{2}D_{\\mathscr{L}}^{-1}\\big)\\frac{d}{d t}\\nabla f(x)+R(D_{\\mathscr{L}}D_{\\mathscr{C}})^{-1}\\nabla f(x)=0.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Under the proper selection of parameters for $\\mu$ -strongly convex and $L$ -smooth function $f$ , (28) corresponds to the high-resolution ODE for NAG-SC introduced in [135] ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\frac{d^{2}}{d t^{2}}x+2\\sqrt{\\mu}\\frac{d}{d t}x+\\sqrt{s}\\frac{d}{d t}\\nabla f(x)+(1+\\sqrt{\\mu s})\\nabla f(x)=0.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "As an immediate consequence, if we set $\\begin{array}{r}{R\\,=\\,\\frac{1}{4\\mu},\\,L_{i}\\,=\\,\\frac{1}{8\\mu\\sqrt{\\mu}},\\,C_{i}\\,=\\,2\\sqrt{\\mu}}\\end{array}$ , we recover the lowresolution ODE of NAG-SC ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\frac{d^{2}}{d t^{2}}x+2\\sqrt{\\mu}\\frac{d}{d t}x+\\nabla f(x)=0.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "For the automatic discretization of the continuous-time dynamics of this circuit, we define the energy at time $k$ as ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathcal{E}_{k}=\\frac{1}{2}\\|\\boldsymbol{v}_{\\mathcal{C}}^{k}-\\boldsymbol{x}^{\\star}\\|_{D_{\\mathcal{C}}}^{2}+\\frac{1}{2}\\|i_{\\mathcal{L}}^{k}-\\boldsymbol{y}^{\\star}\\|_{D_{\\mathcal{L}}}^{2}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "E.4 Proximal point method ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Consider the circuit below. ", "page_idx": 36}, {"type": "text", "text": "Then from the discussion in $\\S E.1$ , the above circuit is equivalent to the circuit below. ", "page_idx": 36}, {"type": "image", "img_path": "9Jmt1eER9P/tmp/ab58fd4dbd1bf1824e8548fcdebdeec21833e8dca86bedb81f68772a1ac3ae08.jpg", "img_caption": [], "img_footnote": [], "page_idx": 36}, {"type": "text", "text": "According to $\\S E.2$ , the ODE for the above circuit is ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\frac{d}{d t}x=-D_{\\mathcal{C}}^{-1}\\nabla^{R}f(x).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Since from (27) we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{prox}_{R f}(x)=x-R\\nabla^{R}f(x),}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "this circuit gives a continuous model for the proximal point method. ", "page_idx": 36}, {"type": "text", "text": "Applying Euler discretization to (29) with a stepsize of $C_{i}R$ for each $i$ th coordinate, we recover proximal point method ", "page_idx": 36}, {"type": "equation", "text": "$$\nx^{k+1}=x^{k}-R\\nabla^{R}f(x^{k})=x^{k}-(x^{k}-\\mathbf{prox}_{R f}(x^{k}))=\\mathbf{prox}_{R f}(x^{k}).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "For the automatic discretization of the continuous-time dynamics of this circuit, we define the energy at time $k$ as ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathcal{E}_{k}=\\frac{1}{2}\\|\\boldsymbol{x}^{k}-\\boldsymbol{x}^{\\star}\\|_{D_{c}}^{2}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "E.5 Proximal gradient method ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Let $f\\colon\\mathbf{R}^{m}\\rightarrow\\mathbf{R}$ be $1/R$ -smooth convex function, and $g\\colon\\mathbf{R}^{m}\\rightarrow\\mathbf{R}$ be convex function. Consider the circuit below. ", "page_idx": 36}, {"type": "image", "img_path": "9Jmt1eER9P/tmp/b2b55b1b7c7c658291bb219412c94c25ebd0b57ab73a62a692fc8984a9bfde5c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 36}, {"type": "text", "text": "Observe, by the Ohm\u2019s law $e=x-R\\nabla f(x)$ . Using KCL at $x$ and KVL at $e$ , we get ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{{i c}}&{{=}}&{{-\\nabla f(x)-\\nabla^{R}g(e)}}\\\\ {{v_{C}}}&{{=}}&{{x-R\\nabla f(x).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Applying V-I relation for the capacitor and eliminating $e$ gives ", "page_idx": 36}, {"type": "equation", "text": "$$\n{\\frac{d}{d t}}x-R{\\frac{d}{d t}}\\nabla f(x)={\\frac{d}{d t}}v_{C}=-{\\frac{1}{C}}\\left(\\nabla f(x)+\\nabla^{R}g(x-R\\nabla f(x))\\right).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Organizing ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\frac{d}{d t}x=-\\frac{1}{C}\\left(\\nabla^{R}g(I-R\\nabla f)+\\nabla f\\right)(x)+R\\frac{d}{d t}\\nabla f(x).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "We can show that $\\begin{array}{r}{R\\left\\|\\frac{d}{d t}\\nabla f(x)\\right\\|<M}\\end{array}$ for some $M>0$ , thus ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\frac{d}{d t}x=-\\frac{1}{C R}\\left(\\left(R\\nabla^{R}g(I-R\\nabla f)+R\\nabla f\\right)(x)+O\\left(M C R\\right)\\right).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Applying Euler discretization with stepsize $C R$ we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\frac{x^{k+1}-x^{k}}{C R}=-\\frac{1}{C R}\\left(\\left(R\\nabla^{R}g(I-R\\nabla f)+R\\nabla f\\right)(x^{k})+O\\left(M C R\\right)\\right).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Multiplying $C R$ on both sides and reorganizing gives ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{x^{k+1}}&{=}&{x^{k}-\\left(R\\nabla^{R}g(I-R\\nabla f)+R\\nabla f\\right)(x^{k})+O(M C R)}\\\\ &{=}&{\\left(\\mathbf{prox}_{R g}-I\\right)(I-R\\nabla f)(x^{k})+\\left(I-R\\nabla f\\right)(x^{k})+O(M C R)}\\\\ &{=}&{\\mathbf{prox}_{R g}(I-R\\nabla f)(x^{k})+O(M C R).}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "If we set $C\\ll R$ , we recover the proximal gradient method. ", "page_idx": 37}, {"type": "text", "text": "For the automatic discretization of the continuous-time dynamics of this circuit, we define the energy at time $k$ as ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathcal{E}_{k}=\\frac{C}{2}\\|e^{k}-e^{\\star}\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where $e^{\\star}=x^{\\star}-R\\nabla f(x^{\\star})$ . ", "page_idx": 37}, {"type": "text", "text": "E.6 Primal decomposition ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Let $f_{1},\\ldots,f_{N}\\colon\\mathbf{R}^{m}\\rightarrow\\mathbf{R}$ be convex functions. Consider the circuit below. ", "page_idx": 37}, {"type": "image", "img_path": "9Jmt1eER9P/tmp/8bfab9f8f9a30f8c015131525596ac2f2fed669824cfe8fc55543ca911fca5e8.jpg", "img_caption": [], "img_footnote": [], "page_idx": 37}, {"type": "text", "text": "Let $x_{1},\\ldots,x_{N}\\in\\mathbf{R}^{m}$ be vectors of potentials at pins of $\\partial f_{1},\\ldots,\\partial f_{N}$ respectively. From KVL, we have ", "page_idx": 37}, {"type": "equation", "text": "$$\ne=x_{1}=\\cdot\\cdot\\cdot=x_{N}=v_{\\mathcal{C}}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Using KCL at $e$ and the $\\mathrm{V}_{-1}$ relation of nonlinear resistors we get $\\begin{array}{r}{\\sum_{j=1}^{N}y_{j}\\,+\\,i c\\;=\\,0}\\end{array}$ , where $y_{j}\\in\\partial f_{j}(x_{j})$ . Using the V-I relation for capacitor we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\frac{d}{d t}e=-\\frac{1}{C}\\sum_{j=1}^{N}y_{j}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Discretizing above V-I relations, we recover primal decomposition ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle y_{j}^{k}}&{\\in}&{\\displaystyle\\partial f_{j}(\\boldsymbol{x}_{j}^{k})}\\\\ {\\displaystyle e^{k+1}}&{=}&{e^{k}-\\displaystyle\\frac{h}{C}\\sum_{j=1}^{N}y_{j}^{k}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "For the automatic discretization of the continuous-time dynamics of this circuit, we define the energy at time $k$ as ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathcal{E}_{k}=\\frac{C}{2}\\|e^{k}-x^{\\star}\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "E.7 Dual decomposition ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Let $f_{1},\\ldots,f_{N}\\colon\\mathbf{R}^{m}\\rightarrow\\mathbf{R}$ be convex functions. Consider the circuit below. ", "page_idx": 38}, {"type": "image", "img_path": "9Jmt1eER9P/tmp/cfaaa86e0ab2f275f0acf554cb5c869f95f41a85bcb89d7ee4eac92a554f445d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 38}, {"type": "text", "text": "Using KCL at $x_{j}$ and V-I relation for nonlinear resistors we get ", "page_idx": 38}, {"type": "equation", "text": "$$\nx_{j}\\in\\partial f_{j}^{*}(y_{j})=\\partial f_{j}^{*}(i_{\\mathcal{L}_{j}}).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Using KCL at $e$ yields $\\textstyle\\sum_{j=1}^{N}y_{j}=0$ . Using KVL and $\\mathrm{V}_{-1}$ relation for inductors we get ", "page_idx": 38}, {"type": "equation", "text": "$$\ne-x_{j}=v_{\\mathcal{L}_{j}}=L\\frac{d}{d t}i_{\\mathcal{L}_{j}}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Summing over $j=1,\\ldots,N$ gives ", "page_idx": 38}, {"type": "equation", "text": "$$\nN e-\\sum_{j=1}^{N}x_{j}=L\\sum_{j=1}^{N}\\frac{d}{d t}i\\mathcal{L}_{j}=L\\frac{d}{d t}\\sum_{j=1}^{N}y_{j}=0,\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "leading to $\\begin{array}{r}{e=(1/N)\\sum_{j=1}^{N}x_{j}}\\end{array}$ . Discretizing above V-I relations, we recover dual decomposition ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{{x_{j}^{k}}}&{{\\in}}&{{\\displaystyle\\partial f_{j}^{*}\\big(i_{\\mathcal{L}_{j}}^{k}\\big)}}\\\\ {{e^{k}}}&{{=}}&{{\\displaystyle\\frac{1}{N}\\sum_{j=1}^{N}x_{j}^{k}}}\\\\ {{i_{\\mathcal{L}_{j}}^{k+1}}}&{{=}}&{{\\displaystyle i_{\\mathcal{L}_{j}}^{k}-\\frac{h}{L}\\big(e^{k}-x_{j}^{k}\\big).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "For the automatic discretization of the continuous-time dynamics of this circuit, we define the energy at time $k$ as ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathcal{E}_{k}=\\sum_{j=1}^{N}\\frac{L}{2}\\|i_{\\mathcal{L}_{j}}^{k}-y_{j}^{\\star}\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "E.8 Proximal decomposition ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Let $f_{1},\\ldots,f_{N}\\colon\\mathbf{R}^{m}\\to\\mathbf{R}$ be convex functions. Consider the circuit below. ", "page_idx": 38}, {"type": "image", "img_path": "9Jmt1eER9P/tmp/e0f2620e7fb089e8322211798f989ac336b1401fcd47e3e4bb01190b56a3934a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 38}, {"type": "text", "text": "Let $x_{1},\\ldots,x_{N}\\in\\mathbf{R}^{m}$ be vectors of potentials at pins of $\\partial f_{1},\\ldots,\\partial f_{N}$ respectively. Define $e\\in\\mathbf{R}^{m}$ to be a vector of potentials on the bottom of the circuit. Observe, by Ohm\u2019s law and KCL we have ", "page_idx": 39}, {"type": "equation", "text": "$$\ny_{j}=i\\mathcal{L}_{j}+\\frac{1}{R}(e-x_{j})\\in\\partial f_{j}(x_{j}).\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "It implies that $x_{i}=\\mathbf{prox}_{R f_{i}}(e+R i\\mathscr{L}_{i})$ . The V-I relations for inductors are given by ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\frac{d}{d t}i_{\\mathcal{L}}=v_{\\mathcal{L}}/L=(E^{\\intercal}e-x)/L,\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where E\u22ba= (I, . . . , I) \u2208RNm\u00d7m. ", "page_idx": 39}, {"type": "text", "text": "Further, note that by KCL $\\begin{array}{r}{E y=\\sum_{j=1}^{N}y_{j}=0}\\end{array}$ , therefore $\\begin{array}{r}{E{\\frac{d}{d t}}y={\\frac{d}{d t}}E y=0}\\end{array}$ . Using the above V-I relations for $\\begin{array}{r}{g=N e-E x}\\end{array}$ we get the following ODE ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\dot{g}=\\frac{d}{d t}R E(y-i_{\\mathcal{L}})=-R E\\frac{d}{d t}i_{\\mathcal{L}}=-\\frac{R}{L}E(E^{\\intercal}e-x)=-\\frac{R}{L}g.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "We initialize circuit with $E i_{\\mathcal{L}}(0)=0$ and $E E^{\\intercal}=N I$ gives ", "page_idx": 39}, {"type": "equation", "text": "$$\n0=E y(0)=E(i_{\\mathcal{L}}(0)+(E^{\\top}e(0)-x(0))/R)=-\\frac{1}{R}g(0).\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Thus the solution to an ODE (31) is $g=0$ and we conclude $\\begin{array}{r}{e={\\frac{1}{N}}E x}\\end{array}$ . ", "page_idx": 39}, {"type": "text", "text": "The V-I relations for the circuit are ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{{x_{j}}}&{{=}}&{{{\\bf p r o x}_{R f_{j}}(e+R i\\varepsilon_{j}),\\quad j=1,\\ldots,N}}\\\\ {{}}&{{}}&{{}}\\\\ {{e}}&{{=}}&{{\\displaystyle\\frac{1}{N}E x}}\\\\ {{\\displaystyle\\frac{d}{d t}i\\varepsilon}}&{{=}}&{{(E^{\\top}e-x)/L.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Discretizing above V-I relations we recover proximal decomposition ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r c l r}{{x_{j}^{k+1}}}&{{=}}&{{\\mathbf{prox}_{R f_{j}}(e^{k}+R i_{\\mathcal{L}_{j}}^{k}),\\quad j=1,\\ldots,N}}\\\\ {{e^{k+1}}}&{{=}}&{{\\displaystyle\\frac{1}{N}E x^{k}}}\\\\ {{i_{\\mathcal{L}}^{k+1}}}&{{=}}&{{i_{\\mathcal{L}}^{k}+\\displaystyle\\frac{h}{L}(E^{\\top}e^{k+1}-x^{k+1}).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "For the automatic discretization of the continuous-time dynamics of this circuit, we define the energy at time $k$ as ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\mathcal{E}_{k}=\\sum_{j=1}^{N}\\frac{L}{2}\\|i_{\\mathcal{L}_{j}}^{k}-y_{j}^{\\star}\\|_{2}^{2}+\\gamma\\|e^{k}-x^{\\star}\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where $\\gamma$ is a parameter that is being optimized, see $\\S\\mathrm{G}$ ", "page_idx": 39}, {"type": "text", "text": "E.9 Douglas\u2013Rachford splitting ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Let $f,g\\colon\\mathbf{R}^{m}\\rightarrow\\mathbf{R}$ be convex functions. Consider the circuit below. ", "page_idx": 39}, {"type": "image", "img_path": "9Jmt1eER9P/tmp/054e733fdc1c1e2f70b4c9faadc8a62648abee80554002d82416878f81341bd6.jpg", "img_caption": [], "img_footnote": [], "page_idx": 39}, {"type": "text", "text": "Using KCL at $x_{1}$ and Ohm\u2019s law we get ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\frac{1}{R}(x_{2}-x_{1})+i\\mathcal{L}\\in\\partial g(x_{1}),\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "which implies $x_{1}=\\mathbf{prox}_{R g}(x_{2}+R i_{\\mathcal{L}})$ . Similarly, using KCL at $x_{2}$ and Ohm\u2019s law we get ", "page_idx": 40}, {"type": "equation", "text": "$$\n{\\frac{1}{R}}(x_{1}-x_{2})-i{\\underline{{c}}}\\in\\partial f(x_{2}),\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "which implies $x_{2}=\\mathbf{prox}_{R f}(x_{1}-R i_{\\mathcal{L}})$ . From KVL and V-I relation for inductors we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n{\\frac{d}{d t}}i_{\\mathcal{L}}={\\frac{1}{L}}(x_{2}-x_{1}).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Discretizing above V-I relations with $R=L=1$ and stepsize $h=1$ , we recover Douglas\u2013Rachford splitting ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{l c l}{{x_{1}^{k+1}}}&{{=}}&{{{\\bf p r o x}_{R g}(x_{2}^{k}+R i_{\\mathcal{L}}^{k})}}\\\\ {{x_{2}^{k+1}}}&{{=}}&{{{\\bf p r o x}_{R f}(x_{1}^{k+1}-R i_{\\mathcal{L}}^{k})}}\\\\ {{i_{\\mathcal{L}}^{k+1}}}&{{=}}&{{i_{\\mathcal{L}}^{k}+\\displaystyle\\frac{h}{L}(x_{2}^{k+1}-x_{1}^{k+1}).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "For the automatic discretization of the continuous-time dynamics of this circuit, we define the energy at time $k$ as ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\mathcal{E}_{k}=\\frac{L}{2}\\|i_{\\mathcal{L}}^{k}-y_{1}^{\\star}\\|_{2}^{2}+\\gamma\\|x_{2}^{k}-x^{\\star}\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where $\\gamma$ is a parameter that is being optimized, see $\\S\\mathrm{G}$ , and $y_{1}^{\\star}\\in\\partial g(x^{\\star})$ . ", "page_idx": 40}, {"type": "text", "text": "E.10 Davis-Yin splitting ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Let $f,g,h\\colon\\mathbf{R}^{m}\\,\\rightarrow\\,\\mathbf{R}$ be convex functions, with $h$ also being $1/S$ -smooth. Consider the circuit below. ", "page_idx": 40}, {"type": "image", "img_path": "9Jmt1eER9P/tmp/9116a338fe6e60abb502c702aaca6c05f1a40510a16790c5754b23cd5fefec96.jpg", "img_caption": [], "img_footnote": [], "page_idx": 40}, {"type": "text", "text": "Using KCL at $e$ and Ohm\u2019s law we have ", "page_idx": 40}, {"type": "equation", "text": "$$\nx_{1}=x_{2}-S i_{-S}+S i_{S}=x_{2}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Applying KCL at $x_{2}$ , we get ", "page_idx": 40}, {"type": "equation", "text": "$$\ni\\mathcal{L}=\\frac{e-x_{2}}{-S}-\\nabla h(x_{2}).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Using KCL at $x_{3}$ and (32) it follows ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\frac{x_{1}-x_{3}}{R}+i\\varsigma=\\frac{x_{1}-x_{3}}{R}+\\frac{x_{2}-e}{S}-\\nabla h(x_{2})\\in\\partial g(x_{3}).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Using KCL at $x_{1}$ , we get ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\frac{x_{3}-x_{1}}{R}+\\frac{e-x_{1}}{S}\\in\\partial f(x_{1}).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Organizing, and applying V-I relation for inductor ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{x_{3}}&{=}&{\\mathbf{prox}_{R g}\\left(\\left(1+\\displaystyle\\frac{R}{S}\\right)x_{1}-\\displaystyle\\frac{R}{S}e-R\\nabla h(x_{1})\\right)}\\\\ {x_{1}}&{=}&{\\mathbf{prox}_{R f}\\left(x_{3}+\\displaystyle\\frac{R}{S}(e-x_{1})\\right)}\\\\ {i_{\\mathcal{L}}}&{=}&{\\displaystyle\\frac{e-x_{1}}{-S}-\\nabla h(x_{1})}\\\\ {\\displaystyle\\frac{d}{d t}i_{\\mathcal{L}}}&{=}&{\\displaystyle\\frac{1}{L}(x_{1}-x_{3}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Now we eliminate the term $i_{\\mathcal{L}}$ . Differentiating (32), applying $\\begin{array}{r}{L\\frac{d}{d t}i_{\\mathcal{L}}=x_{1}-x_{3}}\\end{array}$ we get ", "page_idx": 41}, {"type": "equation", "text": "$$\n{\\frac{1}{L}}{\\big(}x_{1}-x_{3}{\\big)}={\\frac{d}{d t}}i\\varepsilon={\\frac{d}{d t}}{\\frac{x_{1}-e}{S}}-{\\frac{d}{d t}}\\nabla h(x_{1}).\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "In other words, ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\frac{d}{d t}e=\\frac{S}{L}(x_{3}-x_{1})+\\frac{d}{d t}x_{1}-S\\frac{d}{d t}\\nabla h(x_{1}).\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Using \u201calternating update\u201d and Euler discretization of $e$ and $x_{1}$ , we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{x_{3}^{k+1}}&{=}&{\\mathbf{prox}_{R g}\\left(\\left(1+\\displaystyle\\frac{R}{S}\\right)x_{1}^{k}-\\displaystyle\\frac{R}{S}e^{k}-R\\nabla h(x_{1}^{k})\\right)}\\\\ {x_{1}^{k+1}}&{=}&{\\mathbf{prox}_{R f}\\left(x_{3}^{k+1}+\\displaystyle\\frac{R}{S}(e^{k}-x_{1}^{k})\\right)}\\\\ {e^{k+1}}&{=}&{e^{k}+\\displaystyle\\frac{S h}{L}(x_{3}^{k+1}-x_{1}^{k+1})+x_{1}^{k+1}-x_{1}^{k}-S h\\displaystyle\\frac{d}{d t}\\nabla h(x_{1}^{k}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Set $R=S=h=\\alpha$ and $L=\\alpha^{2}$ , then the above can be rewritten as ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{x_{3}^{k+1}}&{=}&{\\mathbf{prox}_{\\alpha g}\\left(2x_{1}^{k}-e^{k}-\\alpha\\nabla h(x_{1}^{k})\\right)}\\\\ {x_{1}^{k+1}}&{=}&{\\mathbf{prox}_{\\alpha f}\\left(e^{k}+x_{3}^{k+1}-x_{1}^{k}\\right)}\\\\ {e^{k+1}}&{=}&{e^{k}+x_{3}^{k+1}-x_{1}^{k}-\\alpha^{2}\\displaystyle\\frac{d}{d t}\\nabla h(x_{1}^{k}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "aWnhd erne $\\left\\|\\frac{d}{d t}\\nabla h(x^{k})\\right\\|$ is bounded, then $\\begin{array}{r}{\\alpha^{2}\\frac{d}{d t}\\nabla h(x^{k})=O(\\alpha^{2})}\\end{array}$ . For small $\\alpha$ we may ignore this term ", "page_idx": 41}, {"type": "text", "text": "For the automatic discretization of the continuous-time dynamics of this circuit, we define the energy at time $k$ as ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathcal{E}_{k}=\\frac{L}{2}\\|i_{\\mathcal{L}}^{k}-y_{1}^{\\star}\\|_{2}^{2}+\\gamma\\|e_{1}^{k}-e^{\\star}\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where $\\gamma$ is a parameter that is being optimized, see $\\S\\mathrm{G}$ , and $e^{\\star}=x^{\\star}-R(y_{1}^{\\star}+y_{3}^{\\star}),y_{1}^{\\star}\\in\\partial f(x^{\\star})$ , $y_{3}^{\\star}\\in\\partial g(x^{\\star})$ . ", "page_idx": 41}, {"type": "text", "text": "F Decentralized classical algorithms ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "In a decentralized optimization setup, we are given a graph $G=(V,A)$ which defines the communication pattern between agents. This means that each agent is constrained to communicate only to its direct neighbors for the edges of the graph. ", "page_idx": 42}, {"type": "text", "text": "We define $\\Gamma_{j}$ as the neighbors of $j$ in graph $G$ . For simplicity, in each example we only illustrate the circuit between components indexed by $j$ and $l$ , where $j$ and $l$ are connected through an edge in the graph $G$ . ", "page_idx": 42}, {"type": "text", "text": "F.1 Decentralized gradient descent ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Let $f_{1},\\ldots,f_{N}\\colon\\mathbf{R}^{m}\\rightarrow\\mathbf{R}$ be differentiable convex functions. Decentralized gradient descent (DGD) is derived as a gradient descent of the appropriate penalty formulation of a decentralized problem. Similarly, to construct a DGD circuit we apply the gradient flow circuit of $\\S E.2$ to appropriate nonlinear resistors and arrive at the following circuit. ", "page_idx": 42}, {"type": "image", "img_path": "9Jmt1eER9P/tmp/9bd17682c209a8b9d2a9ba46c1fa5587f1e59c966c85736be44e1abda3bfbdb6.jpg", "img_caption": [], "img_footnote": [], "page_idx": 42}, {"type": "text", "text": "The right side of the circuit contains the graph with resistors $R_{j l}$ connecting vectors of potentials $\\boldsymbol{x}_{j}\\in\\bar{\\mathbb{R}}^{m}$ and $\\boldsymbol{x}_{l}\\in\\mathbb{R}^{m}$ for every neighbors $j$ and $l$ in the given graph $G$ . ", "page_idx": 42}, {"type": "text", "text": "Using the KCL at $x_{j}$ we get ", "page_idx": 42}, {"type": "equation", "text": "$$\n0=\\nabla f_{j}(x_{j})+\\sum_{l\\in\\Gamma_{j}}\\frac{x_{j}-x_{l}}{R_{j l}}+i c_{j}.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Applying the V-I relation for the capacitors we get ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\frac{d}{d t}x_{j}=\\frac{d}{d t}v_{\\mathcal{C}_{j}}=-\\frac{1}{C}\\left(\\nabla f_{j}(x_{j})+\\sum_{l\\in\\Gamma_{j}}(x_{j}-x_{l})/R_{j l}\\right).\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Euler discretization recovers the DGD ", "page_idx": 42}, {"type": "equation", "text": "$$\nx_{j}^{k+1}=\\left(1-\\sum_{l\\in\\Gamma_{j}}\\frac{h}{C R_{j l}}\\right)x_{j}^{k}+\\sum_{l\\in\\Gamma_{j}}\\frac{h}{C R_{j l}}x_{l}^{k}-\\frac{h}{C}\\nabla f_{j}(x_{j}^{k}),\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "with gradient stepsize $h/C$ and the mixing matrix ", "page_idx": 42}, {"type": "equation", "text": "$$\nW_{j l}=\\left\\{\\begin{array}{l l}{1-\\sum_{l\\in\\Gamma_{j}}\\frac{h}{C R_{j l}}}&{\\mathrm{if~}j=l}\\\\ {\\frac{h}{C R_{j l}}}&{\\mathrm{if~}j\\neq l,\\quad l\\in\\Gamma_{j}}\\\\ {0}&{\\mathrm{otherwise}.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "For the automatic discretization of the continuous-time dynamics of this circuit, we define the energy at time $k$ as ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathcal{E}_{k}=\\sum_{j=1}^{N}\\frac{C}{2}\\|x_{j}^{k}-x_{j}^{\\star}\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "F.2 Diffusion ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Let $f_{1},\\ldots,f_{N}\\colon\\mathbf{R}^{m}\\ \\to\\ \\mathbf{R}$ be $1/R$ -smooth convex functions. Decentralized gradient descent is derived as a forward-backward splitting fixed point iteration of the appropriate penalty formulation of a decentralized problem. Similarly, to construct a diffusion circuit we apply the proximal gradient circuit of $\\S E.5$ to appropriate nonlinear resistors and arrive at the following circuit. ", "page_idx": 43}, {"type": "image", "img_path": "9Jmt1eER9P/tmp/3423e0d65707590e9d7502daaf49829c8bba5a55bd60592d2813016200b21545.jpg", "img_caption": [], "img_footnote": [], "page_idx": 43}, {"type": "text", "text": "The right side of the circuit is the graph with linear resistors $R_{j l}$ connecting vectors of potentials $\\boldsymbol{e}_{j}\\in\\bar{\\bf R}^{m}$ and $e_{l}\\in\\mathbb{R}^{m}$ for every neighbors $j$ and $l$ in the given graph $G$ . ", "page_idx": 43}, {"type": "text", "text": "By Ohm\u2019s law we have $v_{C_{j}}=e_{j}=x_{j}-R\\nabla f_{j}(x_{j})$ . Using the KCL at $e_{j}$ we get ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\nabla f_{j}(x_{j})+\\sum_{l\\in\\Gamma_{j}}\\frac{e_{j}-e_{l}}{R_{j l}}+i c_{j}=0.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Applying the V-I relation for the capacitors we get ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\frac{d}{d t}e_{j}}&{=}&{\\displaystyle\\frac{d}{d t}x_{j}-R\\displaystyle\\frac{d}{d t}\\nabla f_{j}(x_{j})}\\\\ &{=}&{\\displaystyle-\\frac{1}{C}\\left(\\nabla f_{j}(x_{j})+\\sum_{l\\in\\Gamma_{j}}\\frac{(x_{j}-R\\nabla f_{j}(x_{j}))-(x_{l}-R\\nabla f_{l}(x_{l}))}{R_{j l}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "We can show that $\\begin{array}{r}{R\\left|\\right|\\frac{d}{d t}\\nabla f_{j}(x_{j})\\right|\\rvert<M}\\end{array}$ for some $M>0$ , thus ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\frac{d}{d t}x_{j}=-\\frac{1}{C}\\left(\\nabla f_{j}(x_{j})+\\sum_{l\\in\\Gamma_{j}}\\frac{(x_{j}-R\\nabla f_{j}(x_{j}))-(x_{l}-R\\nabla f_{l}(x_{l}))}{R_{j l}}+O(M C)\\right).\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Applying Euler discretization with stepsize $C R$ gives ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{x_{j}^{k+1}}&{=}&{\\displaystyle\\left(1-\\sum_{l\\in\\Gamma_{j}}\\frac{R}{R_{j l}}\\right)(x_{j}^{k}-R\\nabla f_{j}(x_{j}^{k}))}\\\\ &&{\\displaystyle+\\sum_{l\\in\\Gamma_{j}}\\frac{R}{R_{j l}}(x_{l}^{k}-R\\nabla f_{l}(x_{l}^{k}))+{\\cal O}(M C R),}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "with gradient stepsize $R$ and the mixing matrix ", "page_idx": 43}, {"type": "equation", "text": "$$\nW_{j l}=\\left\\{\\begin{array}{l l}{1-\\sum_{l\\in\\Gamma_{j}}\\frac{R}{R_{j l}}}&{\\mathrm{if~}j=l}\\\\ {\\frac{R}{R_{j l}}}&{\\mathrm{if~}j\\neq l,\\quad l\\in\\Gamma_{j}}\\\\ {0}&{\\mathrm{otherwise}.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "If we set $C\\ll R$ , we recover the diffusion method. ", "page_idx": 43}, {"type": "text", "text": "For the automatic discretization of the continuous-time dynamics of this circuit, we define the energy at time $k$ as ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\mathcal{E}_{k}=\\sum_{j=1}^{N}\\frac{C}{2}\\|e_{j}^{k}-e_{j}^{\\star}\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "where $e_{j}^{\\star}=x_{j}^{\\star}-R\\nabla f_{j}(x_{j}^{\\star})$ for all $j=1,\\ldots,N$ . ", "page_idx": 44}, {"type": "text", "text": "F.3 Decentralized ADMM ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Let $f_{1},\\ldots,f_{N}\\colon\\mathbf{R}^{m}\\to\\mathbf{R}$ be convex functions. Then the decentralized ADMM circuit is given below. ", "page_idx": 44}, {"type": "image", "img_path": "9Jmt1eER9P/tmp/b63604271087402b1accf5010d3ab85a35dc98048e569a058bcc171c0ee3c565.jpg", "img_caption": [], "img_footnote": [], "page_idx": 44}, {"type": "text", "text": "Note that this circuit is similar to the one in proximal decomposition in $\\S E.8$ with the difference that instead of a single net $e$ we now have a net $e_{j l}$ for each edge $(j,l)$ in graph $G$ . Denote currents on inductors to be $i_{\\mathcal{L}j l}\\in\\mathbb{R}^{m}$ and $i_{\\mathcal{L}l j}\\in\\mathbb{R}^{m}$ . ", "page_idx": 44}, {"type": "text", "text": "Using KCL at $x_{j}$ we get ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\sum_{l\\in\\Gamma_{j}}\\left(i_{\\mathcal{L}j l}+\\frac{e_{j l}-x_{j}}{R}\\right)\\in\\partial f_{j}(x_{j}).\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "We initialize the circuit such that $i_{\\mathcal{L}j l}(0)+i_{\\mathcal{L}l j}(0)=0$ for each edge $(j,l)$ in graph $G$ . Now consider KCL at ejl ", "page_idx": 44}, {"type": "equation", "text": "$$\ni_{\\mathcal{L}j l}+i_{\\mathcal{L}l j}=-\\frac{(e_{j l}-x_{j})}{R}-\\frac{(e_{j l}-x_{l})}{R}.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Using V-I relation for inductor we also have ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\frac{d}{d t}i\\mathcal{L}j l+\\frac{d}{d t}i\\mathcal{L}l j=\\frac{1}{L}\\big(2e_{j l}-x_{j}-x_{l}\\big).\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Combining the two equalities above we get an ODE ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\frac{d}{d t}\\left(i\\mathcal{L}_{j l}+i\\mathcal{L}_{l j}\\right)=-\\frac{R}{L}\\left(i\\mathcal{L}_{j l}+i\\mathcal{L}_{l j}\\right).\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Using initial conditions the solution of an ODE is $i_{\\mathcal{L}j l}+i_{\\mathcal{L}l j}=0$ . From (34) we conclude that $e_{j l}=\\textstyle{\\frac{1}{2}}(x_{j}+x_{l})$ . ", "page_idx": 44}, {"type": "text", "text": "Using (33), we get the V-I relations for the circuit ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{{x_{j}}}&{{=}}&{{\\displaystyle{\\bf p r o x}_{(R/|\\Gamma_{j}|)f_{j}}\\left(\\frac{1}{|\\Gamma_{j}|}\\sum_{l\\in\\Gamma_{j}}(R i_{\\angle j l}+e_{j l})\\right)}}\\\\ {{e_{j l}}}&{{=}}&{{\\displaystyle{\\frac{1}{2}}(x_{j}+x_{l})}}\\\\ {{\\displaystyle{\\frac{d}{d t}}i_{\\angle j l}}}&{{=}}&{{\\displaystyle{\\frac{1}{L}}(e_{j l}-x_{j}),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "for every $j=1,\\ldots,N$ and every edge $(j,l)$ in graph $G$ . Discretizing the V-I relations with stepsize $L/R$ , we recover decentralized ADMM, ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{{x_{j}^{k+1}}}&{{=}}&{{{\\displaystyle\\mathbf{prox}_{(R/|\\Gamma_{j}|)f_{j}}~\\left(\\frac{1}{|\\Gamma_{j}|}\\sum_{l\\in\\Gamma_{j}}(R i_{\\mathcal{L}j l}^{k}+e_{j l}^{k})\\right)}}}\\\\ {{}}&{{}}&{{}}\\\\ {{e_{j l}^{k+1}}}&{{=}}&{{\\displaystyle\\frac{1}{2}(x_{j}^{k+1}+x_{l}^{k+1})}}\\\\ {{i_{\\mathcal{L}j}{}^{k+1}}}&{{=}}&{{i_{\\mathcal{L}j l}^{k}+\\displaystyle\\frac{1}{R}(e_{j l}^{k+1}-x_{j}^{k+1}).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "For the automatic discretization of the continuous-time dynamics of this circuit, we define the energy at time $k$ as ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\mathcal{E}_{k}=\\sum_{\\mathrm{edge}\\ \\{j,l\\}}\\left(\\frac{L}{2}\\|i_{\\mathcal{L}j l}^{k}-i_{\\mathcal{L}j l}^{\\star}\\|_{2}^{2}+\\frac{L}{2}\\|i_{\\mathcal{L}l j}^{k}-i_{\\mathcal{L}l j}^{\\star}\\|_{2}^{2}+\\gamma\\|e_{j l}^{k}-x^{\\star}\\|_{2}^{2}\\right),\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "where $\\gamma$ is a parameter that is being optimized, see $\\S\\mathrm{G}$ , and $i_{\\mathcal{L}_{j l}}^{\\star}$ is the current through inductor at equilibrium. ", "page_idx": 45}, {"type": "text", "text": "F.4 PG-EXTRA ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Let $f_{1},\\ldots,f_{N}\\colon\\mathbf{R}^{m}\\to\\mathbf{R}$ be convex functions, and $h_{1},\\hdots,h_{N}\\colon\\mathbf{R}^{m}\\rightarrow\\mathbf{R}$ be convex $M$ -smooth functions. Then the PG-EXTRA circuit is given below. ", "page_idx": 45}, {"type": "image", "img_path": "9Jmt1eER9P/tmp/5d10e69d704f6955947dc6f27937d554ab45fe0031852dfd91782a792a8a6ec7.jpg", "img_caption": [], "img_footnote": [], "page_idx": 45}, {"type": "text", "text": "Denote current on inductor going from $x_{j}$ to $x_{l}$ to be $i_{\\mathcal{L}j l}\\in\\mathbf{R}^{m}$ . ", "page_idx": 45}, {"type": "text", "text": "Recall $\\S E.1$ and apply Ohm\u2019s law to get ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\frac{e_{j}-\\tilde{x}_{j}}{R}=\\frac{e_{j}-\\mathbf{p}\\mathbf{r}\\mathbf{o}\\mathbf{x}_{R f_{j}}(e_{j})}{R}=\\nabla^{R}f_{j}(e_{j})=\\frac{x_{j}-e_{j}}{-R}.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "This yields $x_{j}=e_{j}-R\\nabla^{R}f_{j}(e_{j})=\\mathbf{prox}_{R f_{j}}(e_{j})$ . Using KCL at $x_{j}$ we get ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\frac{e_{j}-x_{j}}{-R}=\\nabla h_{j}(x_{j})+\\sum_{l\\in\\Gamma_{j}}\\left(i_{\\mathcal{L}j l}+\\frac{x_{j}-x_{l}}{R_{j l}}\\right).\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Define the mixing matrix ", "page_idx": 45}, {"type": "equation", "text": "$$\nW_{j l}=\\left\\{\\begin{array}{l l}{1-\\sum_{l\\in\\Gamma_{j}}\\frac{R}{R_{j l}}}&{\\mathrm{if~}j=l}\\\\ {\\frac{R}{R_{j l}}}&{\\mathrm{if~}j\\neq l,\\quad l\\in\\Gamma_{j}}\\\\ {0}&{\\mathrm{otherwise}.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Rearranging the terms in (35) we get ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{{e_{j}}}&{{=}}&{{\\displaystyle x_{j}-R\\nabla h_{j}(x_{j})-\\sum_{l\\in\\Gamma_{j}}\\left(R i_{\\mathcal{L}j l}+\\frac{x_{j}-x_{l}}{R_{j l}/R}\\right)}}\\\\ {{}}&{{=}}&{{\\displaystyle x_{j}-\\sum_{l=1}^{N}W_{j l}(x_{j}-x_{l})-R\\nabla h_{j}(x_{j})-\\sum_{l\\in\\Gamma_{j}}R i_{\\mathcal{L}j l}}}\\\\ {{}}&{{=}}&{{\\displaystyle\\sum_{l=1}^{N}W_{j l}x_{l}-R\\nabla h_{j}(x_{j})-\\sum_{l\\in\\Gamma_{j}}R i_{\\mathcal{L}j l}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Using the V-I relation for inductor we also have ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\frac{d}{d t}_{i\\mathscr{L}_{j l}}=\\frac{1}{L_{j l}}(x_{j}-x_{l}).\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Set $L_{j l}=R_{j l}$ for every edge $(j,l)$ in graph $G$ . Define $\\begin{array}{r}{w_{j}=\\sum_{l\\in\\Gamma_{j}}R i_{\\mathcal{L}j l}}\\end{array}$ , then ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{l l l}{{\\displaystyle\\frac{d}{d t}w_{j}}}&{{=}}&{{\\displaystyle\\sum_{l\\in\\Gamma_{j}}\\frac{R}{L_{j l}}(x_{j}-x_{l})}}\\\\ {{\\displaystyle}}&{{=}}&{{\\displaystyle x_{j}-\\sum_{l=1}^{N}W_{j l}x_{l}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Combining the above, we get the V-I relations for the circuit ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle x_{j}}&{=}&{\\mathbf{prox}_{R f_{j}}\\left(\\sum_{l=1}^{N}W_{j l}x_{l}-R\\nabla h_{j}(x_{j})-w_{j}\\right)}\\\\ {\\displaystyle\\frac{d}{d t}w_{j}}&{=}&{x_{j}-\\sum_{l=1}^{N}W_{j l}x_{l},}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "for every $j=1,\\ldots,N$ and every edge $(j,l)$ in graph $G$ . Discretizing the above V-I relations with stepsize $1/2$ , and following the decentralized notation of [129, $\\S11.3]$ , we recover PG-EXTRA, ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{{x^{k+1}}}&{{=}}&{{\\mathbf{prox}_{R f}\\left(W x^{k}-R\\nabla h(x^{k})-w^{k}\\right)}}\\\\ {{w^{k+1}}}&{{=}}&{{w^{k}+\\displaystyle\\frac{1}{2}(I-W)x^{k}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "We can simplify the circuit by eliminating potentials $e_{j}$ as shown below. ", "page_idx": 46}, {"type": "image", "img_path": "9Jmt1eER9P/tmp/9d11cbf9af6a1029ad3b90d93ae1a29f6dbee5207f4ffdd34157023930762393.jpg", "img_caption": [], "img_footnote": [], "page_idx": 46}, {"type": "text", "text": "For the automatic discretization of the continuous-time dynamics of this circuit, we define the energy at time $k$ as ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\mathcal{E}_{k}=\\sum_{\\mathrm{{edge}}\\ \\{j,l\\}}\\frac{L_{j l}}{2}\\|i_{\\mathcal{L}_{j l}}^{k+1}-i_{\\mathcal{L}_{j l}}^{\\star}\\|_{2}^{2}+\\sum_{j=1}^{N}\\gamma\\|x_{j}^{k}-x^{\\star}\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "where $\\gamma$ is a parameter that is being optimized, see $\\S\\mathrm{G}$ , and $i_{\\mathcal{L}_{j l}}^{\\star}$ is the current through inductor at equilibrium. ", "page_idx": 46}, {"type": "text", "text": "G Automatic discretization ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "In this paper, we discretize admissible dynamic interconnects corresponding to the following decentralizes setup with graph consensus ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{x_{1},\\ldots,x_{N}\\in\\mathbf{R}^{m/N}}{\\mathrm{minimize}}}&{f_{1}(x_{1})+\\cdots+f_{N}(x_{N})}\\\\ {\\mathrm{subject\\,}\\,\\mathbf{to}}&{x_{j}=x_{l},\\quad j=1,\\ldots,N,\\quad l\\in\\Gamma_{j},}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "where $\\Gamma_{j}$ contains the neighbors of agent $j$ in the communication graph, see $\\S\\mathrm{F}$ . We assume that the communication graph is connected, ensuring that all agents can communicate with each other [129, $\\S11.2]$ . The static interconnect for this problem corresponds to the consensus problem ", "page_idx": 47}, {"type": "equation", "text": "$$\n{\\begin{array}{r l r l}&{\\;\\;{\\mathrm{minimize}}}&{f_{1}(x_{1})+\\cdot\\cdot\\cdot+f_{N}(x_{N})}\\\\ &{x_{1},\\ldots,x_{N}\\!\\in\\!\\mathbf{R}^{m/N}}&&{}\\\\ &{{\\mathrm{subject~to}}}&{x_{1}=\\cdot\\cdot\\cdot=x_{N},}\\end{array}}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "which is a special case of (1) where $E^{\\boldsymbol{\\uptau}}=(I,\\ldots,I)\\in\\mathbf{R}^{m\\times m/N}$ . Therefore, we have $n=m/N$ nets each of size $N$ with $x_{j}\\,\\in\\,\\mathbf{R}^{n}$ for all $j\\,=\\,1,\\ldots,N$ . This setup generalizes the setup of the classical methods discussed in $\\mathrm{\\SE}$ and $\\S\\mathrm{F}$ . ", "page_idx": 47}, {"type": "text", "text": "For automatic discretization, we focus on dynamic interconnects that have the same RLC circuit across each net, i.e., the dynamic interconnects represented with the multi-wire notation. ", "page_idx": 47}, {"type": "text", "text": "Runge\u2013Kutta method. The capacitor and inductor ODEs are of the form ", "page_idx": 47}, {"type": "equation", "text": "$$\n{\\frac{d}{d t}}x(t)=F(x(t)).\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "We discretize ODEs using the two-stage Runge\u2013Kutta method, with coefficients $\\alpha,\\beta$ , and stepsize $h$ : ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{x^{k+1/2}}&{=}&{x^{k}+\\alpha h F(x^{k})}\\\\ {x^{k+1}}&{=}&{x^{k}+\\beta h F(x^{k})+(1-\\beta)h F(x^{k+1/2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "We clarify that simpler one-stage discretization schemes can also be used. We chose two-stage Runge\u2013 Kutta to demonstrate that multi-stage discretization schemes are compatible with our automatic discretization methodology. ", "page_idx": 47}, {"type": "text", "text": "Energy descent. Let a discrete-time optimization algorithm generate a sequence $\\{(v^{k},\\bar{i}^{k},x^{k},y^{k})\\}_{k=1}^{\\infty}$ with $v^{k},i^{k}\\ \\ \\in\\ \\mathbf{R}^{\\sigma}$ (voltages across and currents through the branches of interconnect) and $x^{k},y^{k}\\,\\in\\,\\mathbf{R}^{m}$ (potentials at terminals and currents leaving terminals). Let the subscripts $\\mathcal{R},\\,\\mathcal{L}$ , and $\\mathcal{C}$ denote the components related to resistors, inductors, and capacitors, respectively. Then the energy stored in the circuit is given by ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\mathcal E_{k}}&{=}&{\\displaystyle\\frac{1}{2}\\|\\boldsymbol v_{C}^{k}-\\boldsymbol v_{C}^{\\star}\\|_{D_{C}}^{2}+\\frac{1}{2}\\|i_{\\mathcal L}^{k}-i_{\\mathcal L}^{\\star}\\|_{D_{C}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Lemma G.1. Assume $f\\colon\\mathbf{R}^{m}\\rightarrow\\mathbf{R}\\cup\\{\\infty\\}$ is a strictly convex function and the dynamic interconnect is admissible. Let a discrete-time optimization algorithm generate a sequence $\\{(v^{k},i^{k},x^{k},y^{k})\\}_{k=1}^{\\infty}$ . If there exists $\\eta>0$ such that for all $k=1,2,\\dots$ . the energy descent ", "page_idx": 47}, {"type": "equation", "text": "$$\nD_{k}=\\left(\\mathcal{E}_{k+1}+\\eta\\langle x^{k}-x^{\\star},y^{k}-y^{\\star}\\rangle\\right)-\\mathcal{E}_{k}\\leq0\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "holds, then $x^{k}$ converges to a primal solution. ", "page_idx": 47}, {"type": "text", "text": "Proof. Suppose there exists $\\eta>0$ for which (39) holds. Then we have ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{0}&{\\leq}&{{\\mathcal{E}}_{K+1}}\\\\ &{\\leq}&{{\\mathcal{E}}_{K}-\\eta\\langle x^{K}-x^{\\star},y^{K}-y^{\\star}\\rangle}\\\\ &{\\leq}&{{\\mathcal{E}}_{0}-\\displaystyle\\sum_{k=0}^{K}\\eta\\langle x^{k}-x^{\\star},y^{k}-y^{\\star}\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "By monotonicity of subdifferential operator $\\partial f$ , we have ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\langle x^{k}-x^{\\star},y^{k}-y^{\\star}\\rangle\\geq0,\\quad y^{k}\\in\\partial f(x^{k}).\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Thus rearranging the terms we get ", "page_idx": 48}, {"type": "equation", "text": "$$\n0\\leq\\sum_{k=0}^{K}\\eta\\langle x^{k}-x^{\\star},y^{k}-y^{\\star}\\rangle\\leq\\mathcal{E}_{0}.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Sending $K$ to infinity, by the summability argument it follows that ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\langle x^{k}-x^{\\star},y^{k}-y^{\\star}\\rangle\\to0.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Define the Lagrangian function ", "page_idx": 48}, {"type": "equation", "text": "$$\nL(x,z,y)=f(x)-y^{T}(x-E^{\\mathsf{T}}z).\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Since $x^{\\star}\\in\\mathcal{R}(E^{\\intercal})$ , there exists some $z^{\\star}$ such that $x^{\\star}=E^{\\intercal}z^{\\star}$ . Then for fixed $z^{\\star}$ and $y^{\\star}$ , function $L(x,z^{\\star},y^{\\star})$ is strictly convex. Its subgradient is given by $(y-y^{\\star})\\in\\partial_{x}L(x,z^{\\star},y^{\\star})$ for $y\\in\\partial f(x)$ , therefore, $\\dot{0}\\in L(x^{\\star},z^{\\star},y^{\\star})$ . Together with strict convexity this implies that $L(x,z^{\\star},y^{\\star})$ achieves a unique global minimum at $x^{\\star}$ with $L(x^{\\star},z^{\\star},y^{\\star})=f(x^{\\star})$ . By the subgradient inequality, we have ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\langle x^{k}-x^{\\star},y^{k}-y^{\\star}\\rangle\\geq L(x^{k},z^{\\star},y^{\\star})-f(x^{\\star})\\geq0.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Then the condition (40) implies $L(x^{k},z^{\\star},y^{\\star})\\rightarrow f(x^{\\star})$ . Therefore, $x^{k}\\to x^{\\star}$ which concludes the proof. \u53e3 ", "page_idx": 48}, {"type": "text", "text": "By the descent lemma G.1, the discretization is dissipative if there exist value $\\eta>0$ such that ", "page_idx": 48}, {"type": "equation", "text": "$$\nD_{k}=\\left(\\mathcal{E}_{k+1}+\\eta\\langle x^{k}-x^{\\star},y^{k}-y^{\\star}\\rangle\\right)-\\mathcal{E}_{k}\\leq0\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "for all $k\\,=\\,1,2,\\ldots$ Since the descent $D_{k}$ is defined using a one-step transition, without loss of generality, it suffices to consider $k=1$ . ", "page_idx": 48}, {"type": "text", "text": "Solver dissipative term. To provide more flexibility with the Ipopt [154, 9] solver, we also incorporate dissipation from the linear resistors as in the continuous-time energy dissipation (7), i.e., we try to establish ", "page_idx": 48}, {"type": "equation", "text": "$$\nD_{k}=\\left(\\mathcal{E}_{k+1}+\\eta\\langle x^{k}-x^{\\star},y^{k}-y^{\\star}\\rangle+\\rho R\\|i_{\\mathcal{R}}^{k}\\|_{2}^{2}\\right)-\\mathcal{E}_{k}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "with $\\eta>0$ and $\\rho\\geq0$ . Also see $\\S D$ . If there exist values $\\eta>0$ and $\\rho\\geq0$ such that $D_{k}\\leq0$ holds, then the discretization is sufficiently dissipative and Lemma G.1 applies. ", "page_idx": 48}, {"type": "text", "text": "G.1 Dissipative discretization ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "In this section, we fix $\\alpha,\\,\\beta,\\,h,\\,\\eta.$ , and $\\rho$ and describe a convex optimization problem that checks whether the discretization is dissipative. We focus on problem (38). ", "page_idx": 48}, {"type": "text", "text": "Worst-case optimization problem. To verify if the dissipativity condition $D_{k}\\leq0$ (41) is satisfied for a given discretization, we can alternatively solve a worst-case problem. Specifically, this entails determining if the optimal value of the following optimization problem is non-positive: ", "page_idx": 48}, {"type": "text", "text": "maximize $\\begin{array}{r l}&{\\mathcal{E}_{2}-\\mathcal{E}_{1}+\\eta\\langle x^{1}-x^{\\star},y^{1}-y^{\\star}\\rangle+\\rho R\\|i_{\\mathcal{R}}^{1}\\|_{2}^{2}}\\\\ &{\\mathcal{E}_{s}=\\frac{1}{2}\\|v_{\\mathcal{C}}^{s}-v_{\\mathcal{C}}^{\\star}\\|_{D_{\\mathcal{C}}}^{2}+\\frac{1}{2}\\|i_{\\mathcal{L}}^{s}-i_{\\mathcal{L}}^{\\star}\\|_{D_{\\mathcal{L}}}^{2},\\quad s\\in\\{1,2\\}}\\end{array}$   \nsubject to $(v^{1},i^{1},x^{1},y^{1})$ iCs feCasible inLit ial pLointL $(v^{2},i^{2},x^{2},y^{2})$ is generated by discrete optimization method from initial point $f\\in\\mathcal F$ , ", "page_idx": 48}, {"type": "text", "text": "where $f,v^{k},i^{k},x^{k},y^{k},v^{\\star},i^{\\star},x^{\\star},y^{\\star}$ are the decision variables and $\\mathcal{F}$ is a family of functions (e.g., $L$ -smooth convex) that the algorithm is to be applied to. ", "page_idx": 48}, {"type": "text", "text": "Reformulated worst-case optimization problem. Recall that we assume that the RLC circuit across each net is the same. Thus we can define an operator $\\operatorname*{mat}(z)$ that reshapes vector $z\\in\\mathbf{R}^{\\sigma}$ into a matrix of size $n\\times\\sigma/n$ , where each row contains information (voltage or current) of the electric components that belong to the same net. Define index sets ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{{I_{K}}}&{{=}}&{{\\{1,1.5,2,\\star\\},}}\\\\ {{I_{N}}}&{{=}}&{{\\{1,...\\,,N\\},}}\\\\ {{I_{K}\\times I_{N}}}&{{=}}&{{\\{(k,l)\\mid l\\in I_{N},k\\in I_{K}\\},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "and matrices ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{l l l}{{{\\cal H}}}&{{=}}&{{\\Bigl[\\mathrm{mat}(v^{1})\\quad\\mathrm{mat}(i^{1})\\quad\\left[y_{l}^{k}\\right]_{(k,l)\\in I_{K}\\times I_{N}}\\Bigr]\\in{\\bf R}^{n\\times(2\\sigma/n+|I_{K}|N)},}}\\\\ {{{\\cal G}}}&{{=}}&{{{\\cal H}^{T}{\\cal H}\\in{\\bf S}_{+}^{2\\sigma/n+|I_{K}|N},}}\\\\ {{{\\cal F}}}&{{=}}&{{\\left[f_{l}^{k}\\right]_{(k,l)\\in I_{K}\\times I_{N}}\\in{\\bf R}^{|I_{K}|N},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "where $y_{l}^{k}\\in\\partial f_{l}(x^{k})$ and $f_{l}^{k}=f_{l}(x^{k})$ for all $l\\in I_{N}$ . Note that we have ", "page_idx": 49}, {"type": "equation", "text": "$$\n|F|=|I_{K}|N,\\qquad|G|=(2\\sigma/n+|I_{K}|N)^{2},\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "and for $|I_{K}|=4$ this simplifies to ", "page_idx": 49}, {"type": "equation", "text": "$$\n|F|=4N,\\qquad|G|=(2\\sigma/n+4N)^{2}.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Recall the circuit ODEs are discretized with the two-stage Runge\u2013Kutta method, leading to the variables $\\operatorname{mat}(v^{k}),\\,\\operatorname{mat}(i^{k}),\\,x^{k},\\,y^{k}$ that are linear combinations of columns in $H$ . The coefficients of these linear combinations are polynomials in $\\alpha,\\,\\beta$ , and $h$ . In other words, there exist matrices $\\mathbf{v}^{k},\\mathbf{i}^{k},\\mathbf{x}^{k},\\mathbf{y}_{l}^{k}$ such that ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\operatorname{mat}(v^{k})=H\\mathbf{v}^{k},\\quad\\operatorname{mat}(i^{k})=H\\mathbf{i}^{k},\\quad x^{k}=H\\mathbf{x}^{k},\\quad y_{l}^{k}=H\\mathbf{y}_{l}^{k}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "for all $k\\in I_{K}$ , $l\\in I_{N}$ . Similarly, we can find $\\mathbf{f}_{l}^{k}$ such that $f_{l}^{k}=F\\mathbf{f}_{l}^{k}$ . ", "page_idx": 49}, {"type": "text", "text": "For fixed parameters $\\alpha,\\beta,h,\\eta$ , and $\\rho$ , the problem (42) can be reformulated as ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{f_{1},\\ldots,f_{N},H}{\\mathrm{maximize}}}&{\\mathcal{E}_{2}-\\mathcal{E}_{1}+\\eta\\langle x^{1}-x^{\\star},y^{1}-y^{\\star}\\rangle+\\rho R\\|i_{\\mathcal{R}}^{1}\\|_{2}^{2}}\\\\ {\\mathrm{subject~to}}&{\\mathcal{E}_{s}=\\frac{1}{2}\\|v_{\\mathcal{C}}^{s}-v_{\\mathcal{C}}^{\\star}\\|_{D_{\\mathcal{C}}}^{2}+\\frac{1}{2}\\|i_{\\mathcal{L}}^{s}-i_{\\mathcal{L}}^{\\star}\\|_{D_{\\mathcal{L}}}^{2},\\quad s\\in\\{1,2\\}}\\\\ &{\\mathrm{mat}(v^{k})=H\\mathbf{v}^{k},\\quad k\\in I_{K}}\\\\ &{\\mathrm{mat}(i^{k})=H\\mathbf{i}^{k},\\quad k\\in I_{K}}\\\\ &{x^{k}=H\\mathbf{x}^{k},\\quad k\\in I_{K}}\\\\ &{y_{l}^{k}=H\\mathbf{y}_{l}^{k},\\quad k\\in I_{K},\\quad l\\in I_{N}}\\\\ &{f_{l}\\in\\mathcal{F}_{\\mu_{l},M_{l}}(\\mathbf{R}^{n}),\\quad l\\in I_{N}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "By the interpolation lemma ([148], Theorem 2), $f_{l}\\in\\mathcal{F}_{\\mu_{l},M_{l}}(\\mathbf{R}^{n})$ if and only if ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{0}&{\\displaystyle\\geq}&{f_{l}^{j}-f_{l}^{i}+\\langle g_{l}^{j},x^{i}-x^{j}\\rangle+\\displaystyle\\frac{1}{2M_{l}}\\|g_{l}^{i}-g_{l}^{j}\\|_{2}^{2}}\\\\ &&{\\displaystyle+\\frac{\\mu_{l}}{2(1-\\mu_{l}/M_{l})}\\|x^{i}-x^{j}-1/M_{l}(g_{l}^{i}-g_{l}^{j})\\|_{2}^{2},\\quad i,j\\in I_{K}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Therefore, we can replace infinite dimensional decision variable $f_{l}\\in\\mathcal{F}_{\\mu_{l},M_{l}}(\\mathbf{R}^{n})$ with $|I_{K}|(|I_{K}|\\!-\\!1)$ inequalities. ", "page_idx": 49}, {"type": "text", "text": "Grammian formulation. Now using Grammian formulation, the problem of finding the worst-case energy difference over a given family of functions reduces to solving an SDP, similar to [149]. This SDP can be presented compactly as ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{G,F}{\\mathrm{maximize}}}&{[F^{T}\\mathrm{~}\\mathrm{vec}(G)^{T}]D p}\\\\ {\\mathrm{subject}\\mathrm{~to}}&{[F^{T}\\mathrm{~}\\mathrm{vec}(G)^{T}]S_{l i j}p\\leq0,\\quad l=1,\\dots,N,\\ i,j\\in I_{K}}\\\\ &{G\\succeq0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "where $p$ is a vector with dummy variables that encode the monomials of $\\alpha,\\beta,h,\\eta,\\rho$ , and $D\\ \\in$ $\\mathbf{R}^{(|F|+|G|)\\times|p|}$ and $S\\in\\mathbf{R}^{(|I_{K}|N)\\mathbf{\\bar{\\times}}(|F|+|G|)\\times|p|}$ are some matrices with constant coefficients. ", "page_idx": 49}, {"type": "text", "text": "Dualization. Define variables for the energy descent as ", "page_idx": 50}, {"type": "equation", "text": "$$\nV_{D}=[F^{T}\\,\\,\\operatorname{vec}(G)^{T}]D p,\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "and for interpolating inequality indexed by $l i j$ as ", "page_idx": 50}, {"type": "equation", "text": "$$\n(V_{S})_{l i j}=[F^{T}\\mathrm{~vec}(G)^{T}]S_{l i j}p.\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Let $Z$ and $\\lambda_{l i j}$ for all $i,j\\in I_{K}$ , $l=1,\\ldots,N$ be the dual variables for problem (43). Vertically stack $\\lambda_{l i j}$ and $(V_{S})_{l i j}$ to form vectors $\\lambda$ and $V_{S}$ respectively. The Lagrangian that generates primal problem (43) is ", "page_idx": 50}, {"type": "equation", "text": "$$\nL(G,f,Z,\\lambda)=V_{D}-\\lambda^{T}V_{S}+\\mathbf{Tr}(G Z),\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "and the dual problem is given by ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{Z,\\lambda}{\\mathrm{minimize}}}&{0}\\\\ {\\mathrm{subject\\to}}&{D_{F}p-{\\lambda}^{T}S_{F}p=0}\\\\ &{D_{G}p-{\\lambda}^{T}S_{G}p+Z=0}\\\\ &{Z\\succeq0}\\\\ &{\\lambda\\succeq0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "with d\u22c6= 0, then for all G \u2208S(+|C|+|L|)/n+|IK|Nand \u2264F \u2208R|IK|N it follows that $Z^{\\star}$ $\\lambda^{\\star}$ be optimal dual variables ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{L(G,F,Z^{\\star},\\lambda^{\\star})}&{=}&{F^{T}\\underbrace{\\left(D_{F}p-(\\lambda^{\\star})^{T}S_{F}p\\right)}_{=0}+\\mathbf{Tr}\\left(G\\underbrace{\\left(D_{G}p-(\\lambda^{\\star})^{T}S_{G}p+Z^{\\star}\\right)}_{=0}\\right)}\\\\ &{=}&{0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Therefore, having $Z^{\\star}$ and $\\lambda^{\\star}$ gives us an algebraic proof for the worst-case one step energy difference ", "page_idx": 50}, {"type": "equation", "text": "$$\nV_{D}=\\sum_{l,i,j}\\underbrace{\\lambda_{l i j}^{\\star}}_{\\geq0}\\underbrace{(V_{S})_{l i j}}_{\\leq0}-\\underbrace{\\mathbf{Tr}(G Z^{\\star})}_{\\geq0}\\leq0,\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "where $G\\succeq0$ because $G$ is a Gram matrix and $(V_{S})_{l i j}\\leq0$ for all $f_{l}\\in\\mathcal{F}_{\\mu_{l},L_{l}}$ . ", "page_idx": 50}, {"type": "text", "text": "G.2 Optimizing over discretizations ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "In this section we also optimize over the parameters $\\alpha,\\beta,h,\\eta$ , and $\\rho$ . ", "page_idx": 50}, {"type": "text", "text": "G.2.1 QCQP formulation ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "We can formulate the dual problem (44) as QCQP following [45], ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{p,\\lambda,P}{\\mathrm{minimize}}}&{\\,0}\\\\ {\\mathrm{subject\\,}}&{\\,D_{F}p-\\lambda^{T}S_{F}p=0}\\\\ &{\\,D_{G}p-\\lambda^{T}S_{G}p+P P^{T}=0}\\\\ &{\\,p^{T}Q_{e}p+a_{e}^{T}p=0,\\quad e=1,\\dots,|p|}\\\\ &{\\,P\\mathrm{\\,is\\,lower\\,triangular}}\\\\ &{\\,\\mathrm{diag}(P)\\geq0}\\\\ &{\\,\\lambda\\geq0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "where relations for dummy variables are specified using quadratic or bilinear constraints $\\boldsymbol{p}^{T}{\\boldsymbol{Q}}_{e}{\\boldsymbol{p}}+$ aeT p = 0. ", "page_idx": 50}, {"type": "text", "text": "G.2.2 Lifted nonconvex SDP ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Alternatively, we can formulate the dual problem (44) as lifted nonconvex semidefinite problem with respect to a variable $w=(p,\\lambda)\\in\\mathbf{R}^{|p|+|\\lambda|}$ . Specifically, we have ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{w,w,Z}{\\mathrm{minimize}}}&{0}\\\\ {\\mathrm{subject}\\ t o}&{\\overline{{D}}_{F}(i,:)w-\\mathbf{Tr}\\left(\\overline{{S}}_{F}(:,i,:)W\\right)=0,\\quad i=1,\\dots,|F|}\\\\ &{\\overline{{D}}_{G}(i,:)w-\\mathbf{Tr}\\left(\\overline{{S}}_{G}(:,i,:)W\\right)+Z(i,i)=0,\\quad i=1,\\dots,|G|}\\\\ &{\\mathbf{Tr}\\left(\\overline{{Q}}_{e}W\\right)+\\overline{{a}}_{e}^{T}w=0}\\\\ &{W=w w^{T}}\\\\ &{Z\\succeq0}\\\\ &{\\lambda\\succeq0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "where $\\begin{array}{r}{\\overline{{S}}_{F}=\\left[\\begin{array}{c c}{0}&{\\frac{1}{2}S_{F}^{T}}\\\\ {\\frac{1}{2}S_{F}}&{0}\\end{array}\\right],\\overline{{D}}_{F}=\\left[\\begin{array}{c c}{D_{F}}&{0}\\end{array}\\right],\\overline{{S}}_{G}=\\left[\\begin{array}{c c}{0}&{\\frac{1}{2}S_{G}^{T}}\\\\ {\\frac{1}{2}S_{G}}&{0}\\end{array}\\right],\\overline{{D}}_{G}=\\left[\\begin{array}{c c}{D_{G}}&{0}\\end{array}\\right],}\\end{array}$ $\\overline{{Q}}_{e}=\\left[\\begin{array}{c c}{{Q_{e}}}&{{\\bar{0}}}\\\\ {{0}}&{{0}}\\end{array}\\right]$ and $\\overline{{a}}_{e}=(a_{e},0)$ . In the above the transpose for the third order tensors $S_{F}$ and $S_{G}$ is obtained by transposing the first and third dimensions. Note that with the exception of the rank-1 constraint $\\stackrel{\\cdot}{W}=\\dot{w}w^{T}$ , the constraints define convex sets. ", "page_idx": 51}, {"type": "text", "text": "G.2.3 SDP relaxation ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "To find globally optimal solutions to the nonconvex optimization problem, methods like spacial branch-and-bound require good initial bounds on the variables. Following [45], an SDP relaxation of (45) is given by ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{w,w,z}{\\mathrm{minimize}}}&{0}\\\\ &{\\mathrm{subject~to}}&{\\overline{{D}}_{F}(i,:)w-\\mathbf{Tr}\\left(\\overline{{S}}_{F}(:,i,:)W\\right)=0,\\quad i=1,\\dots,|F|}\\\\ &{\\overline{{D}}_{G}(i,:)w-\\mathbf{Tr}\\left(\\overline{{S}}_{G}(:,i,:)W\\right)+Z(i,i)=0,\\quad i=1,\\dots,|G|}\\\\ &{\\mathbf{Tr}\\left(\\overline{{Q}}_{e}W\\right)+\\overline{{a}}_{e}^{T}w=0}\\\\ &{\\left[\\begin{array}{l l}{W}&{w}\\\\ {w^{T}}&{1}\\end{array}\\right]\\succeq0}\\\\ &{Z\\succeq0}\\\\ &{\\lambda\\succeq0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Problem (46) is now a convex optimization problem, since the rank-1 constraint $W\\,=\\,w w^{T}$ has been relaxed to $W\\succeq w w^{T}$ . This constraint in turn can be represented equivalently using the Schur complement. ", "page_idx": 51}, {"type": "text", "text": "H Package ciropt ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "In this section, we present a simple problem instance to demonstrate the step-by-step process of obtaining a discretized algorithm with our methodology. ", "page_idx": 52}, {"type": "text", "text": "Optimization problem. Consider a problem ", "page_idx": 52}, {"type": "text", "text": "where $f$ is a convex function. ", "page_idx": 52}, {"type": "text", "text": "Determine the static interconnect. Static interconnect is determined from the optimality conditions. The optimality condition for this problem is to find an $x$ such that $0\\in\\partial f(x)$ . The corresponding static interconnect for this condition provided below. ", "page_idx": 52}, {"type": "text", "text": "Admissible dynamic interconnect. An admissible dynamic interconnect with RLC components relaxes to the static interconnect in equilibrium. The following provides an example of such a dynamic interconnect. ", "page_idx": 52}, {"type": "image", "img_path": "9Jmt1eER9P/tmp/7665fee56f3765e23ecff3edc555d9c00a98ab432f980fc517633d8a060f3ac6.jpg", "img_caption": [], "img_footnote": [], "page_idx": 52}, {"type": "text", "text": "The V-I relations for the circuit (left column) and convergent discretized method found by our method (right) are displayed below. ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{c c}{\\displaystyle{\\mathbf{x}=\\mathbf{prox}_{(R/2)f}\\left(z\\right)}}&{x^{k}=\\mathbf{prox}_{(R/2)f}\\left(z^{k}\\right)}\\\\ {\\displaystyle y=\\frac{2}{R}(z-x)}&{y^{k}=\\frac{2}{R}(z^{k}-x^{k})}\\\\ {\\displaystyle\\frac{d}{d t}e_{2}=-\\frac{1}{2C R}(R y+3e_{2})}\\\\ {\\displaystyle\\frac{d}{d t}z=-\\frac{1}{4C R}(5R y+3e_{2})}\\end{array}\\right)\\left(\\begin{array}{c c}{\\displaystyle x^{k}+\\mathbf{prox}_{(R/2)f}\\left(z^{k}\\right)}&{x^{k}=0}\\\\ {\\displaystyle y^{k}+\\frac{2}{R}(z^{k}-x^{k})}&{x^{k}=0}\\end{array}\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Automatic discretization. Now we find a discretization parameters for this dynamic interconnect that guarantee algorithm convergence using ciropt package. ", "page_idx": 52}, {"type": "text", "text": "Step 1. Define a problem. ", "page_idx": 52}, {"type": "table", "img_path": "9Jmt1eER9P/tmp/915ce86bbf3a9fae16e1dd9f8b1f3d16a2a7a678e62b1102641c6786f442ae2f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 52}, {"type": "text", "text": "Step 2. Define function class, in this example $f$ is convex and nondifferentiable, i.e., $\\mu=0$ and $M=\\infty$ . ", "page_idx": 52}, {"type": "table", "img_path": "9Jmt1eER9P/tmp/11198f2d8412e7a4096f23b287b9d42f13453bd08940146e660d116ba70074b6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 52}, {"type": "text", "text": "Step 3. Define the optimal points. ", "page_idx": 53}, {"type": "table", "img_path": "9Jmt1eER9P/tmp/d6dc8024710f180bba671de5d9208adb7860c71dbee5795918566aabfa7c17c3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 53}, {"type": "text", "text": "Step 4. Define values for the RLC components and discretization parameters, here for simplicity we take $\\alpha=0$ and $\\beta=1$ . ", "page_idx": 53}, {"type": "table", "img_path": "9Jmt1eER9P/tmp/3c11579f07de6a64b7f7dd3af08e0daec5d5f77523eedae5a95519ca08e2a718.jpg", "table_caption": [], "table_footnote": [], "page_idx": 53}, {"type": "text", "text": "Step 5. Define the one step transition in the discretized V-I relations. ", "page_idx": 53}, {"type": "image", "img_path": "9Jmt1eER9P/tmp/a06c6d1ba5fe7b07262c4e38c5e8a998919614c7558daa8d4c7338a2b4490372.jpg", "img_caption": [], "img_footnote": [], "page_idx": 53}, {"type": "text", "text": "Step 6. Define the dissipative term ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\mathcal{E}_{2}-\\mathcal{E}_{1}+\\eta\\langle x^{1}-x^{\\star},y^{1}-y^{\\star}\\rangle.\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Solve the final problem. ", "page_idx": 53}, {"type": "table", "img_path": "9Jmt1eER9P/tmp/37ea550e8beea6423bd2b38bff190c6aa1e7b635e3f539408476165608081ac6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 53}, {"type": "text", "text": "This gives the disretization parameters ", "page_idx": 53}, {"type": "equation", "text": "$$\nb=6.66,\\qquad h=6.66.\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "The resulting provably convergent algorithm is ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{x^{k}}&{=}&{\\mathbf{prox}_{(1/2)f}(z^{k})}\\\\ {y^{k}}&{=}&{2(z^{k}-x^{k})}\\\\ {w^{k+1}}&{=}&{w^{k}-0.33(y^{k}+3w^{k})}\\\\ {z^{k+1}}&{=}&{z^{k}-0.16(5y^{k}+3w^{k}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "New algorithm. Solve your problem using new algorithm. Consider Huber penalty function $\\phi:{\\mathbf{R}}\\rightarrow{\\mathbf{R}}$ ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\phi(x)={\\binom{x^{2}}{2x-1}}\\quad|x|\\leq1\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "We consider the primal problem ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{minimize}}&{{}f(x)=\\sum_{i}\\phi(x_{i}-c_{i})}\\\\ {\\mathrm{subject\\;to}}&{{}A x=b,}\\end{array}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "where $A\\in\\mathbf{R}^{m\\times n}$ , $b\\in\\mathbf{R}^{m}$ and $c\\in\\mathbf{R}^{n}$ , and solve the dual problem ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\mathrm{maximize}\\quad g(y)=-f^{*}(-A^{\\mathsf{T}}y)-b^{\\mathsf{T}}y.\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "We apply our algorithm to solve the dual problem. Note that the proximal operator $\\mathbf{prox}_{\\alpha g}(\\tilde{y})$ is equivalent to ", "page_idx": 54}, {"type": "equation", "text": "$$\nx=\\underset{x}{\\mathrm{argmin}}\\left(f(x)+(\\alpha/2)\\|A x-b\\|_{2}^{2}+\\tilde{y}^{\\top}(A x-b)\\right),\\quad y=\\tilde{y}+\\alpha(A x-b).\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "Since $f$ is CCP and 2-smooth (as a Huber loss), $f^{*}$ is $1/2$ -strongly convex. We take $m\\,=\\,30$ , $n=100$ and sample entries of $A$ , $c$ and $b$ from i.i.d. Gaussian distribution. Finally we rescale the entries of $A$ by $\\lambda_{\\mathrm{min}}(A A^{\\top})$ to have $g$ that is $1/2$ -strongly convex. The following Figure 10 presents the results of the algorithm applied to a random problem instance. ", "page_idx": 54}, {"type": "image", "img_path": "9Jmt1eER9P/tmp/670c29b99679bd0beb83280a0a64d2ab372a7e57b03ff2b4cf5f4c6b7aba0001.jpg", "img_caption": ["Figure 10: Relative error across iterations when applying the new algorithm. "], "img_footnote": [], "page_idx": 54}, {"type": "text", "text": "I Numerical experiments ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "I.1 Decentralized ADMM+C ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Consider a decentralized optimization problem ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{x\\in\\mathbf{R}^{m}}{\\mathrm{minimize}}}&{{}\\sum_{i=1}^{N}f_{i}(x),}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "where $f_{1},\\ldots,f_{N}$ are CCP. Suppose furthermore we know some of the functions are strongly convex, that is, suppose there is a subset $S\\subset\\{1,2,\\ldots,N\\}$ such that $f_{j}$ are strongly convex for $j\\in S$ . We wish to find an efficient algorithm that fully exploits the additional information for $f_{j}$ \u2019s. ", "page_idx": 55}, {"type": "text", "text": "To solve the problem in a decentralized manner, define a primal variable $x_{i}\\in\\mathbf{R}^{m}$ for each agent function $f_{i}$ . To leverage the strong convexity of $f_{j}$ for each $j\\in S$ , we could consider implementing a specialized update rule for $x_{j}$ that is more effective for strongly convex functions. ", "page_idx": 55}, {"type": "text", "text": "We consider a modification of the DADMM circuit in $\\S\\mathrm{F}.3$ . Recall from $\\S3.1$ , a circuit with a capacitor and inductor corresponds to a method with momentum. It is known [124] that momentum accelerates the convergence of methods for strongly convex functions. Therefore, we propose to attach capacitors to the circuit in $\\S\\mathrm{F}.3$ , on the nets that are directly related to $x_{j}$ \u2019s in $j\\in S$ . We anticipate that the method derived by discretization of a new circuit (using our automatic discretization methodology) will outperform the DADMM. ", "page_idx": 55}, {"type": "text", "text": "Consider a modified decentralized geometric median problem from [137]. Suppose each agent $i\\in\\{1,\\ldots,N\\}$ holds vector $b_{i}\\in\\mathbf{R}^{m}$ , and consider the minimization problem ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{x\\in\\mathbf{R}^{m}}{\\mathrm{minimize}}}&{\\sum_{i\\in S}\\left(\\|x-b_{i}\\|_{2}+\\|x-b_{i}\\|_{2}^{2}\\right)+\\sum_{i\\notin S}\\|x-b_{i}\\|_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "The minimization subproblem has an explicit solution, i.e., ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\mathbf{prox}_{\\rho f_{i}}(z)=b_{i}-\\frac{b_{i}-\\tilde{z}}{\\Vert b_{i}-\\tilde{z}\\Vert_{2}}(\\Vert b_{i}-\\tilde{z}\\Vert_{2}-\\tilde{\\rho})_{+},\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "where ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{z}=\\left\\{\\underset{1+2\\rho}{z}(z+2\\rho b_{i})\\quad i\\notin S\\right.\\qquad\\qquad\\tilde{\\rho}=\\left\\{\\underset{1+2\\rho}{\\rho}\\quad\\r_{i}\\notin S\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "We set $m=100$ , $N=6$ , $S=\\{4,5\\}$ , and sample vectors $b_{i}\\in{\\bf R}^{100}$ from the uniform distribution over $[-100,100]^{100}$ . We use graph $G$ provided in Figure 9. We initialize iterates to $x_{i}^{0}=b_{i}$ for all $i$ . We use a modified DADMM circuit $\\S\\mathrm{F}.3$ for the graph in Figure 9. This modified version includes an extra capacitor connected at $e_{45}$ , to which we refer as DADMM $+\\mathrm{C}$ . ", "page_idx": 55}, {"type": "image", "img_path": "9Jmt1eER9P/tmp/42e3c95d8b539adda90cc3e47f852bbf4af20b565fbf5881308736bb8178af69.jpg", "img_caption": [""], "img_footnote": [], "page_idx": 55}, {"type": "text", "text": "Note when $N=1$ , the DADMM $+\\mathbf{C}$ circuit corresponds to the Nesterov acceleration circuit $\\S3.1$ Using KCL at $e_{45}$ , we have ", "page_idx": 56}, {"type": "equation", "text": "$$\ni_{\\mathcal{L}45}+i_{\\mathcal{L}54}=-\\frac{(e_{45}-x_{4})}{R}-\\frac{(e_{45}-x_{5})}{R}-C\\frac{d}{d t}e_{45}.\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "Thus for $\\{j,l\\}\\neq\\{4,5\\}$ the update rule of $e_{j l}$ is given by (34), while for $e_{45}$ we get ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\frac{d}{d t}e_{45}=-\\frac{1}{C}\\left(i_{\\mathcal{L}45}+i_{\\mathcal{L}54}+\\frac{1}{R}\\left(2e_{45}-x_{4}-x_{5}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "Other V-I relations remain unchanged as in $\\S\\mathrm{F}.3$ . The resulting algorithm becomes ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r c l l}{{x_{j}^{k+1}}}&{{=}}&{{{\\bf p r o x}_{(R/|\\Gamma_{j}|)f_{j}}\\left(\\displaystyle\\frac{1}{|\\Gamma_{j}|}\\sum_{l\\in\\Gamma_{j}}(R i_{\\mathcal{L}j l}^{k}+e_{j l}^{k})\\right)}}&{{}}\\\\ {{e_{j l}^{k+1}}}&{{=}}&{{\\displaystyle\\left\\{e_{45}^{k}-\\frac{h}{C R}\\left(R(i_{\\mathcal{L}45}^{k}+i_{\\mathcal{L}54}^{k})+2e_{45}^{k}-x_{4}^{k+1}-x_{5}^{k+1}\\right)\\quad}}&{{\\{j,l\\}=\\{4,5\\}}}\\\\ {{{i_{\\mathcal{L}j}^{k+1}}}}&{{=}}&{{\\displaystyle{i_{\\mathcal{L}j l}+\\frac{h}{L}(e_{j l}^{k+1}-x_{j}^{k+1})}}}&{{}}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "We consider the circuit with $R=0.8$ , $L=2$ and $C=15$ . To discretize the circuit, we take advantage of the fact that the strong convexity of $f_{i}$ is 2 for $i\\in S$ (47). Specifically, we apply our automatic discretization methodology to convex functions, setting $\\mu=0$ for $f_{i}$ with $i\\not\\in S$ and $\\mu=2$ for $f_{i}$ with $i\\in S$ , and using smoothness $M=100$ . The sufficiently dissipative parameters we find are ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\eta=3.70,\\quad h=3.52,\\quad\\rho=0,\\quad\\alpha=0,\\quad\\beta=1,\\quad\\gamma=4.48.\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "We compare DADMM $+\\mathbf{C}$ with DADMM and P-EXTRA. Based on grid search, we set $R=0.6$ for DADMM in $\\S\\mathrm{F}.3$ , and $R=1$ and $h_{1}=\\cdot\\cdot=h_{N}=0$ for PG-EXTRA in $\\S\\mathrm{F}.4$ to get P-EXTRA. Note that the parameters of the proximal operators for DADMM are scaled by $1/|\\Gamma_{j}|$ , in contrast to P-EXTRA, where $\\left|\\Gamma_{j}\\right|$ is generally not equal to 1. We use Metropolis mixing matrix for P-EXTRA, ", "page_idx": 56}, {"type": "equation", "text": "$$\nW_{i j}=\\left\\{\\frac{1}{\\operatorname*{max}\\{|\\Gamma_{i}|,|\\Gamma_{j}|\\}+1}\\right.\\quad\\mathrm{if}\\;i\\in\\Gamma_{j}}\\\\ {1-\\sum_{j\\in\\Gamma_{j}}W_{i j}\\quad\\mathrm{if}\\;i=j}\\\\ {0\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\mathrm{otherwise}.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "The numerical results are illustrated in Figure 11. The relative error for DADMM $+\\mathbf{C}$ decreases to $10^{-10}$ in 66 iterations, for DADMM in 87 iterations and for P-EXTRA in 294 iterations. ", "page_idx": 56}, {"type": "image", "img_path": "9Jmt1eER9P/tmp/0bac0f0f49ab0698335c7b9f09ce893ff54d1fa2b14214edc8189e4ffd707c34.jpg", "img_caption": ["Figure 11: (Left) Underlying graph $G$ . (Right) Relative error $\\left|f(x^{k})-f^{\\star}\\right|/f^{\\star}$ vs. $k$ . "], "img_footnote": [], "page_idx": 56}, {"type": "text", "text": "I.1.1 Convergence proof of decentralized ADMM $\\pm\\mathbf{C}$ ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "We first review the meaning of the numerical values in the previous section. Suppose $(x^{\\star},y^{\\star})$ be a primal-dual solution pair. Then by Theorem C.2, there is $(v^{\\star},i^{\\star})\\;\\in\\;D_{x^{\\star},y^{\\star}}$ that satisfies $(v^{\\star},i^{\\bar{\\star}})=((0,0,v_{\\mathcal{C}}^{\\star}),(0,i_{\\mathcal{L}}^{\\star},0))$ . The numerical values imply, for the energy function ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\mathcal{E}_{k}=\\sum_{(j,l)\\in A}\\left\\|i_{\\mathcal{L}_{j l}}^{k}-i_{\\mathcal{L}_{j l}}^{\\star}\\right\\|^{2}+\\sum_{\\substack{j<l,\\,\\{j,l\\}\\subset S}}\\frac{15}{2}\\left\\|e_{j l}^{k}-x_{j}^{\\star}\\right\\|^{2}+3.52\\sum_{\\substack{j<l,\\{j,l\\}\\subset S}}\\left\\|e_{j l}^{k}-x_{j}^{\\star}\\right\\|^{2},\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "following inequality is true up to certain numerical precision ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\left(\\mathcal{E}_{k+1}+3.7\\langle x^{k+1}-x^{\\star},y^{k+1}-y^{\\star}\\rangle\\right)-\\mathcal{E}_{k}\\leq0.\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "This inequality guarantees the convergence of the method we\u2019ve used in the experiment. Inspired form the numerical results, we could obtain an analytic proof for generalized cases as well. To clarify, $A$ is the set of edges introduced in $\\S\\mathrm{F}$ , and each edge is counted twice in the sum $\\sum_{(j,l)\\in A}$ . ", "page_idx": 57}, {"type": "text", "text": "Lemma I.1. Let $f_{j}:\\mathbf{R}^{m}\\rightarrow\\mathbf{R}\\cup\\{\\infty\\}$ are CCP functions for $j\\in\\{1,\\ldots,N\\}$ and $S\\subset\\{1,2,\\ldots,N\\}$ . Consider the generalized $D A D M M\\!+\\!C$ ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{{x_{j}^{k+1}}}&{{=}}&{{{\\displaystyle{\\bf p r o x}_{(R/|\\Gamma_{j}|)\\it f_{j}}\\left(\\frac{1}{|\\Gamma_{j}|}\\sum_{l\\in\\Gamma_{j}}(R i_{\\mathcal{L}j l}^{k}+e_{j l}^{k})\\right)}}}\\\\ {{e_{j l}^{k+1}}}&{{=}}&{{\\displaystyle{\\left\\{e_{j l}^{k}-\\frac{h}{C R}\\left(R(i_{\\mathcal{L}j l}^{k}+i_{\\mathcal{L}j l}^{k})+2e_{j l}^{k}-x_{j}^{k+1}-x_{l}^{k+1}\\right)\\right.}}}&{{\\left.\\{j,l\\}\\subset S\\right.}}\\\\ {{i_{\\mathcal{L}j l}^{k+1}}}&{{=}}&{{i_{\\mathcal{L}j l}^{k}+\\displaystyle{\\frac{h}{L}}(e_{j l}^{k+1}-x_{j}^{k+1}),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "with initilzation i0Ljl $i_{\\mathcal{L}_{j l}}^{0}=i_{\\mathcal{L}_{l j}}^{0}$ for all edge $(j,l)$ in $G.$ . Let $(x^{\\star},y^{\\star})$ be a primal-dual solution pair and $(v^{\\star},i^{\\star})\\in D_{x^{\\star},y^{\\star}}$ . Define the energy function as ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\mathcal{E}_{k}=\\sum_{(j,l)\\in A}\\frac{L}{2}\\left\\|i_{\\mathcal{L}_{j l}}^{k}-i_{\\mathcal{L}_{j l}}^{\\star}\\right\\|^{2}+\\sum_{j<l,\\{j,l\\}\\subset S}\\frac{C}{2}\\left\\|e_{j l}^{k}-x_{j}^{\\star}\\right\\|^{2}+\\sum_{j<l,\\{j,l\\}\\subset S}\\frac{h}{2R}\\left\\|e_{j l}^{k}-x_{j}^{\\star}\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "Then for all $R,L,C,h,\\tau>0$ that satisfy ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\left\\{1,\\frac{2h}{C R}\\right\\}\\leq\\tau^{2}\\leq2-\\frac{h R}{L},\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "following inequality is true ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\left(\\mathcal{E}_{k+1}+\\frac{h}{2R}\\left(2-\\frac{h R}{L}-\\tau^{2}\\right)\\sum_{(j,l)\\in A}\\left\\|\\boldsymbol{e}_{j l}^{k+1}-\\boldsymbol{x}_{j}^{k+1}\\right\\|^{2}+h\\langle\\boldsymbol{x}^{k+1}-\\boldsymbol{x}^{\\star},\\boldsymbol{y}^{k+1}-\\boldsymbol{y}^{\\star}\\rangle\\right)-\\mathcal{E}_{k}\\leq0.\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "Proof. For notation simplicity, define ", "page_idx": 57}, {"type": "equation", "text": "$$\ny_{j l}^{k+1}=i_{\\mathcal{L}_{j l}}^{k}+\\frac{1}{R}\\left(e_{j l}^{k}-x_{j}^{k+1}\\right).\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "Note, from the first line of the algorithm we have xjk+ $\\begin{array}{r}{x_{j}^{k+1}+\\frac{R}{|\\Gamma_{j}|}y_{j}^{k+1}=\\frac{1}{|\\Gamma_{j}|}\\sum_{l\\in\\Gamma_{j}}\\Big(R i_{\\mathcal{L}_{j l}}^{k}+e_{j l}^{k}\\Big),}\\end{array}$ and therefore ", "page_idx": 57}, {"type": "equation", "text": "$$\ny_{j}^{k+1}=\\sum_{l\\in\\Gamma_{j}}\\left(i_{\\mathcal{L}_{j l}}^{k}+\\frac{1}{R}\\left(e_{j l}^{k}-x_{j}^{k+1}\\right)\\right)=\\sum_{l\\in\\Gamma_{j}}y_{j l}^{k+1}.\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "(i) Difference of $\\begin{array}{r}{\\sum_{(j,l)\\in A}\\frac{L}{2}\\left\\|i_{\\mathcal{L}_{j l}}^{k}-i_{\\mathcal{L}j l}^{\\star}\\right\\|^{2}}\\end{array}$ . ", "page_idx": 57}, {"type": "text", "text": "Name ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\Delta_{L}=\\sum_{(j,l)\\in A}\\frac{L}{2}\\left\\Vert i_{\\mathcal{L}_{j l}}^{k+1}-i_{\\mathcal{L}_{j l}}^{\\star}\\right\\Vert^{2}-\\sum_{(j,l)\\in A}\\frac{L}{2}\\left\\Vert i_{\\mathcal{L}_{j l}}^{k}-i_{\\mathcal{L}_{j l}}^{\\star}\\right\\Vert^{2}.\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "Observe ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\frac{\\Delta_{L}}{h}}&{=\\phantom{-}\\frac{1}{h}\\left(\\phantom{|}\\sum_{\\ell(\\ell)\\in\\overline{{\\mathcal{A}}}}\\frac{L}{\\alpha_{\\ell}}\\bigg(\\frac{1}{\\ell}\\bigg)\\bigg(\\frac{1}{\\ell}\\bigg.\\mathrm{d}_{\\ell}-\\frac{1}{\\ell}\\bigg.+\\frac{h}{\\ell}\\bigg.\\mathrm{d}_{\\ell}\\bigg.+1\\bigg.\\mathrm{d}_{\\ell}\\bigg.\\mathrm{d}_{\\ell}^{\\star\\star1}-\\frac{1}{\\ell}\\bigg.+1\\bigg)\\right]^{2}-\\sum_{\\ell(\\ell)\\in\\overline{{\\mathcal{A}}}}\\frac{L}{\\alpha_{\\ell}}\\bigg(\\frac{1}{\\ell}\\bigg.\\mathrm{d}_{\\ell}-\\frac{1}{\\ell}\\bigg.\\bigg.\\bigg|^{2}\\right)}\\\\ &{=\\phantom{-}\\sum_{\\ell(\\ell)\\in\\overline{{\\mathcal{A}}}}\\left(\\dot{\\ell}_{\\ell}^{\\star\\star1}-x_{\\ell}^{\\star\\star1},\\dot{\\ell}_{\\ell}^{\\star\\star}-\\dot{\\ell}_{\\ell}^{\\star}\\right)+\\phantom{-}\\sum_{\\ell(\\ell)\\in\\overline{{\\mathcal{A}}}}\\frac{h}{\\alpha_{\\ell}\\beta_{\\ell}}\\bigg|\\dot{\\mathrm{d}}_{\\ell}^{\\star\\star1}-x_{\\ell}^{\\star\\star1}\\bigg|^{2}}\\\\ &{=\\phantom{-}\\sum_{\\ell(\\ell)\\in\\overline{{\\mathcal{A}}}}\\left\\langle\\dot{\\ell}_{\\ell}^{\\star\\star1}-x_{\\ell}^{\\star\\star1},\\dot{\\ell}_{\\ell}^{\\star\\star1}-\\frac{1}{\\ell}\\bigg.\\left(\\dot{e}_{\\ell}^{\\star}-x_{\\ell}^{\\star\\star1}\\right)-\\dot{\\ell}_{\\ell}^{\\star\\star}\\right\\rangle+\\phantom{-}\\sum_{\\ell(\\ell)\\in\\overline{{\\mathcal{A}}}}\\frac{h}{\\alpha_{\\ell}\\beta_{\\ell}}\\bigg|\\left[e_{\\ell}^{\\star\\star1}-x_{\\ell}^{\\star\\star1}\\right]^{2}}\\\\ &{=\\phantom{-}-\\frac{1}{R}\\phantom{\\bigg.}\\sum_{\\ell(\\ell)\\in\\overline{{\\mathcal{A}}}}\\left\\langle\\dot{\\ell}_{\\ell}^{\\star\\star1}-x_{\\ell}^{\\star\\star1},\\dot{\\ell}_{\\ell}^{\\star\\star}-x_{\\\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "On the other hand, from Theorem C.2 we know $i_{\\mathcal{R}}^{\\star}=0$ , by KCL at $x_{j}$ we have $\\begin{array}{r}{y_{j}^{\\star}=\\sum_{l\\in\\Gamma_{j}}i_{\\mathcal{L}_{j l}}^{\\star}}\\end{array}$ . Therefore ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\sum_{j,l)\\in A}\\left\\langle x_{j}^{k+1}-x_{j}^{\\star},y_{j l}^{k+1}-i\\hat{z}_{j l}\\right\\rangle=\\sum_{j=1}^{N}\\sum_{l\\in\\Gamma_{j}}\\left\\langle x_{j}^{k+1}-x_{j}^{\\star},y_{j l}^{k+1}-i\\hat{\\xi}_{j l}\\right\\rangle=\\sum_{j=1}^{N}\\left\\langle x_{j}^{k+1}-x_{j}^{\\star},y_{j}^{k+1}-i\\hat{\\xi}_{j l}\\right\\rangle=0.\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "Moreover, i\u22c6Ljl = \u2212i\u22c6Llj, ejkl+ $e_{j l}^{k+1}=e_{l j}^{k+1}$ holds by their definition, and $x_{j}^{\\star}=x_{l}^{\\star}$ as $x^{\\star}$ is the solution. Therefore we see ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\sum_{(j,l)\\in A}\\left\\langle e_{j l}^{k+1}-x_{j}^{\\star},i_{\\mathcal{L}_{j l}}^{\\star}\\right\\rangle=\\sum_{j<l,(j,l)\\in A}\\left\\langle e_{j l}^{k+1}-x_{j}^{\\star},i_{\\mathcal{L}_{j l}}^{\\star}+i_{\\mathcal{L}_{l j}}^{\\star}\\right\\rangle=0.\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "Lastly, following equality is true for $\\tau\\in(0,\\infty)$ ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{{\\left\\langle e_{j l}^{k+1}-x_{j}^{k+1},e_{j l}^{k}-e_{j l}^{k+1}\\right\\rangle}}&{{=}}&{{\\displaystyle\\frac{1}{2}\\left\\|\\tau(e_{j l}^{k+1}-x_{j}^{k+1})+\\frac{1}{\\tau}(e_{j l}^{k}-e_{j l}^{k+1})\\right\\|^{2}}}\\\\ {{}}&{{}}&{{\\displaystyle-\\frac{\\tau^{2}}{2}\\left\\|e_{j l}^{k+1}-x_{j}^{k+1}\\right\\|^{2}-\\frac{1}{2\\tau^{2}}\\left\\|e_{j l}^{k}-e_{j l}^{k+1}\\right\\|^{2}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "Finally, applying above observations we have ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\frac{\\Delta_{L}}{h}}&{=}&{\\displaystyle-\\left(\\frac{1}{R}-\\frac{h}{2L}-\\frac{\\tau^{2}}{2R}\\right)\\sum_{(j,l)\\in A}\\left\\|e_{j l}^{k+1}-x_{j}^{k+1}\\right\\|^{2}}\\\\ &&{\\displaystyle+\\frac{1}{2R\\tau^{2}}\\sum_{(j,l)\\in A}\\left\\|e_{j l}^{k}-e_{j l}^{k+1}\\right\\|^{2}-\\frac{1}{2R}\\sum_{(j,l)\\in A}\\left\\|\\tau(e_{j l}^{k+1}-x_{j}^{k+1})+\\frac{1}{\\tau}(e_{j l}^{k}-e_{j l}^{k+1})\\right\\|^{2}}\\\\ &&{\\displaystyle-\\sum_{j=1}^{N}\\left\\langle x_{j}^{k+1}-x_{j}^{k},y_{j}^{k+1}-y_{j}^{\\star}\\right\\rangle+\\sum_{(j,l)\\in A}\\left\\langle e_{j l}^{k+1}-x_{j}^{\\star},y_{j l}^{k+1}\\right\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "(ii) Difference of $\\begin{array}{r}{\\sum_{j<l,\\{j,l\\}\\subset{\\cal S}}\\frac{C}{2}\\left\\|e_{j l}^{k}-x_{j}^{\\star}\\right\\|^{2}\\!.}\\end{array}$ . ", "page_idx": 58}, {"type": "text", "text": "Name ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\Delta_{C}=\\sum_{j<l,\\left\\{j,l\\right\\}\\subset s}\\frac{C}{2}\\left\\|e_{j l}^{k+1}-x_{j}^{\\star}\\right\\|^{2}-\\sum_{j<l,\\left\\{j,l\\right\\}\\subset s}\\frac{C}{2}\\left\\|e_{j l}^{k}-x_{j}^{\\star}\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "Plugging the definition of the method, we have ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\frac{\\Delta_{C}}{h}}&{=}&{\\displaystyle\\frac{1}{h}\\left(\\displaystyle\\frac{1}{2}\\sum_{\\{j,l\\}\\subset{S}}\\frac{C}{2}\\left\\|e_{j l}^{k+1}-x_{j}^{\\star}\\right\\|^{2}-\\displaystyle\\frac{1}{2}\\sum_{\\{j,l\\}\\subset S}\\frac{C}{2}\\left\\|e_{j l}^{k+1}-x_{j}^{\\star}-\\left(e_{j l}^{k+1}-e_{j l}^{k}\\right)\\right\\|^{2}\\right)}\\\\ &{=}&{\\displaystyle\\frac{C}{2h}\\sum_{\\{j,l\\}\\subset S}\\left\\langle e_{j l}^{k+1}-e_{j l}^{k},e_{j l}^{k+1}-x_{j}^{\\star}\\right\\rangle-\\displaystyle\\frac{C}{4h}\\sum_{\\{j,l\\}\\subset S}\\left\\|e_{j l}^{k+1}-e_{j l}^{k}\\right\\|^{2}}\\\\ &{=}&{\\displaystyle-\\frac{1}{2}\\sum_{\\{j,l\\}\\subset S}\\left\\langle y_{j l}^{k+1}+y_{l j}^{k+1},e_{j l}^{k+1}-x_{j}^{\\star}\\right\\rangle-\\displaystyle\\frac{C}{4h}\\sum_{\\{j,l\\}\\subset S}\\left\\|e_{j l}^{k+1}-e_{j l}^{k}\\right\\|^{2}}\\\\ &{=}&{\\displaystyle-\\sum_{\\{j,l\\}\\subset S}\\left\\langle y_{j l}^{k+1},e_{j l}^{k+1}-x_{j}^{\\star}\\right\\rangle-\\displaystyle\\frac{C}{4h}\\sum_{\\{j,l\\}\\subset S}\\left\\|e_{j l}^{k+1}-e_{j l}^{k}\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "(iii) Difference of $\\begin{array}{r}{\\sum_{j<l,\\{j,l\\}\\not\\subset S}\\frac{h}{2R}\\left\\|e_{j l}^{k}-x_{j}^{\\star}\\right\\|^{2}}\\end{array}$ . ", "page_idx": 59}, {"type": "text", "text": "Name ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\Delta_{\\gamma}=\\sum_{j<l,\\{j,l\\}\\not\\in S}\\frac{h}{2R}\\left\\|e_{j l}^{k+1}-x_{j}^{\\star}\\right\\|^{2}-\\sum_{j<l,\\{j,l\\}\\not\\in S}\\frac{h}{2R}\\left\\|e_{j l}^{k}-x_{j}^{\\star}\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "For {j, l} \u0338\u2282S, from the initialization i0Ljl = \u2212i0Llj and from ejkl+1 $e_{j l}^{k+1}={\\textstyle\\frac{1}{2}}\\left(x_{j}^{k+1}+x_{l}^{k+1}\\right)$ , inductively we can check ", "page_idx": 59}, {"type": "equation", "text": "$$\ni_{\\mathcal{L}_{j l}}^{k+1}=i_{\\mathcal{L}_{j l}}^{k}+\\frac{h}{L}\\left(e_{j l}^{k+1}-x_{j}^{k+1}\\right)=-i_{\\mathcal{L}_{l j}}^{k}-\\frac{h}{L}\\left(e_{j l}^{k+1}-x_{l}^{k+1}\\right)=-i_{\\mathcal{L}_{l j}}^{k+1}.\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "Therefore ikjl $i_{\\mathcal{L}_{j l}}^{k}=-i_{\\mathcal{L}_{l j}}^{k}$ for all $k$ . And from the definition of $y_{l j}^{k+1}$ , we have ", "page_idx": 59}, {"type": "equation", "text": "$$\ny_{l j}^{k+1}=-i_{\\mathcal{L}_{j l}}^{k}-\\frac{1}{R}\\left(e_{j l}^{k}-x_{j}^{k+1}\\right)+\\frac{2}{R}\\left(e_{j l}^{k}-e_{j l}^{k+1}\\right)=-y_{j l}^{k+1}+\\frac{2}{R}\\left(e_{j l}^{k}-e_{j l}^{k+1}\\right).\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "And thus ", "page_idx": 59}, {"type": "equation", "text": "$$\ne_{j l}^{k+1}-e_{j l}^{k}=-\\frac{R}{2}\\left(y_{l j}^{k+1}+y_{l j}^{k+1}\\right).\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "Now proceeding the similar calculation and argument for $\\Delta_{C}$ , we have ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\frac{\\Delta_{\\gamma}}{h}}&{=}&{-\\displaystyle\\sum_{\\{j,l\\}\\not\\subset{\\boldsymbol{S}}}\\left\\langle y_{j l}^{k+1},e_{j l}^{k+1}-x_{j}^{\\star}\\right\\rangle-\\displaystyle\\sum_{j<l,\\{j,l\\}\\not\\in{\\boldsymbol{S}}}\\displaystyle\\frac{1}{2R}\\left\\|e_{j l}^{k+1}-e_{j l}^{k}\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "Finally, summing the calculations in (i), (ii), (iii), we have ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{1}{\\eta}\\left(\\mathcal E_{k+1}-\\mathcal E_{k}\\right)+\\sum_{j=1}^{N}\\left\\langle x_{j}^{k+1}-x_{j}^{*},y_{j}^{k+1}-y_{j}^{*}\\right\\rangle+\\left(\\frac1R-\\frac{h}{2L}-\\frac{\\tau^{2}}{2R}\\right)\\sum_{(j,l)\\in A}\\left\\|e_{j l}^{k+1}-x_{j}^{k+1}\\right\\|^{2}}\\\\ {\\displaystyle=\\frac1h\\left(\\Delta_{L}+\\Delta_{C}+\\Delta_{\\gamma}\\right)+\\sum_{j=1}^{N}\\left\\langle x_{j}^{k+1}-x_{j}^{*},y_{j}^{k+1}-y_{j}^{*}\\right\\rangle+\\frac1{2R}\\left(2-\\frac{h R}{L}-\\tau^{2}\\right)\\sum_{(j,l)\\in A}\\left\\|e_{j l}^{k+1}-x_{j}^{k+1}\\right\\|^{2}}\\\\ {\\displaystyle=-\\frac1R\\sum_{(j,l)\\in A}\\left\\|\\tau(e_{j l}^{k+1}-x_{j}^{k+1})+\\frac1{\\tau}(e_{j l}^{k}-e_{j l}^{k+1})\\right\\|^{2}}\\\\ {\\displaystyle~-\\,\\frac1{2R}\\left(\\frac{C R}{2h}-\\frac1{\\tau^{2}}\\right)\\sum_{\\xi_{j},l\\in S_{j}}\\left\\|e_{j l}^{k+1}-e_{j l}^{k}\\right\\|^{2}-\\frac1{2R}\\left(1-\\frac1{\\tau^{2}}\\right)\\sum_{(j,l)\\in S_{j}}\\left\\|e_{j l}^{k+1}-e_{j l}^{k}\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "Therefore, for all $R,L,C,h,\\tau>0$ that satisfy ", "page_idx": 59}, {"type": "equation", "text": "$$\n2-\\frac{h R}{L}-\\tau^{2}\\geq0,\\quad\\frac{C R}{2h}-\\frac{1}{\\tau^{2}}\\geq0,\\quad1-\\frac{1}{\\tau^{2}}\\geq0\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "or equivalently, ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\left\\{1,\\frac{2h}{C R}\\right\\}\\leq\\tau^{2}\\leq2-\\frac{h R}{L},\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "we conclude the desired inequality. ", "page_idx": 59}, {"type": "text", "text": "I.2 PG-EXTRA $^+$ Parallel C ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "In this section, we introduce an additional pipeline of designing new optimization algorithm via circuit. The previous automatized discretization pipeline has the advantage of guaranteeing convergence; however, it may provide a conservative step size since it considers all worst-case scenarios. As a result, it may eliminate the possibility of finding efficient step size that works for certain optimization problem in practice. ", "page_idx": 60}, {"type": "text", "text": "Our circuit-based approach has the advantage of designing a variant of the prior method quickly, that is likely to converge and possibly works better based on physical intuition. Furthermore, the variant method provides greater freedom in selecting parameters to tune. We provide an example of new optimization method obtained with exploiting these advantages, that outperforms PG-EXTRA for the problem considered in the paper introduced PG-EXTRA [137]. ", "page_idx": 60}, {"type": "text", "text": "We use a modified PG-EXTRA circuit $\\S\\mathrm{F}.4$ , that includes extra capacitors connected parallel to inductors. ", "page_idx": 60}, {"type": "image", "img_path": "9Jmt1eER9P/tmp/762a4c54622d37f495a2ecab0903ed3af94cc2b4cc3774a1baa12364c847f68c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 60}, {"type": "text", "text": "Recalling (6), we know the energy for this circuit is defined as below ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\mathcal{E}(t)=\\sum_{\\substack{j<l,(j,l)\\in A}}\\frac{C_{j l}}{2}\\left\\|v_{\\mathcal{C}_{j l}}(t)-v_{\\mathcal{C}_{j l}}^{\\star}\\right\\|^{2}+\\sum_{\\substack{j<l,(j,l)\\in A}}\\frac{L_{j l}}{2}\\left\\|i_{\\mathcal{L}_{j l}}(t)-i_{\\mathcal{L}_{j l}}^{\\star}\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "Note, compared to the energy of PG-EXTRA, we have additional energy terms for capacitors. Observe ", "page_idx": 60}, {"type": "equation", "text": "$$\nv_{{\\mathcal{C}}_{j l}}(t)-v_{{\\mathcal{C}}_{j l}}^{\\star}=x_{j}(t)-x_{l}(t)-(x_{j}^{\\star}-x_{l}^{\\star})=x_{j}(t)-x_{l}(t).\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "Previously, the energy dissipation by the resistors only reduced the values of $\\|i_{\\mathcal{L}_{j l}}(t)-i_{\\mathcal{L}_{j l}}^{\\star}\\|^{2}$ , but now it also reduces the values of $\\|x_{j}(t)-x_{l}(t)\\|^{2}$ for $(j,l)\\in A$ . Intuitively, we may hope that this dissipation accelerates the convergence $\\begin{array}{r}{\\operatorname*{lim}_{t\\to\\infty}(x_{j}(t)-x_{l}(t))=0}\\end{array}$ , thus eventually speed up the convergence to the optimal. This observation provides informal motivation for the method. ", "page_idx": 60}, {"type": "text", "text": "Following the arguments of $\\S\\mathrm{F}.4$ , and additionally defining $\\begin{array}{r}{u_{j}=\\sum_{l\\in\\Gamma_{j}}R i_{{\\mathcal C}_{j l}}}\\end{array}$ and setting $\\begin{array}{r}{C_{j l}=\\frac{C}{R_{j l}}}\\end{array}$ for $(j,l)\\in A$ with some constant $C>0$ , we derive the following method ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{x^{k+1}}&{=}&{{\\bf p r o x}_{R f}\\left(W x^{k}-R\\nabla h(x^{k})-w^{k}-u^{k}\\right)}\\\\ {w^{k+1}}&{=}&{w^{k}+s(I-W)x^{k}}\\\\ {u^{k+1}}&{=}&{\\displaystyle\\frac{C}{s}(I-W)(x^{k+1}-x^{k}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "We now consider the decentralized quadratic programming from [137]. Suppose each agent $j\\in$ $\\{1,\\ldots,N\\}$ holds a symmetric positive semidefinite matrix $\\mathbf{\\omega}_{Q_{j}}\\in\\mathbf{R}^{m\\times m}$ , vectors $a_{j},p_{j}\\in\\mathbf{R}^{m}$ and scalars $b_{j}\\in\\mathbf{R}$ . Consider the minimization problem ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{x\\in\\mathbf{R}^{m}}{\\mathrm{minimize}}}&{\\frac{1}{N}\\sum_{j=1}^{N}\\left(x^{\\mathsf{T}}Q_{j}x+p_{j}^{\\mathsf{T}}x\\right),}\\\\ {\\mathrm{subject\\,to}}&{a_{j}^{\\mathsf{T}}x\\leq b_{j},\\quad j=1,\\ldots,N.}\\end{array}\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "Set $f_{j}(x)=\\delta_{\\{z|a_{j}^{\\sf T}z\\leq b_{j}\\}}(x)$ and $h_{j}(\\boldsymbol{x})=x^{\\mathsf{T}}Q_{j}x+p_{j}^{\\mathsf{T}}x$ , where ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\delta_{\\{z|a_{j}^{\\sf T}z\\leq b_{j}\\}}(x)=\\left\\{\\!\\!\\begin{array}{l l}{{0}}&{{\\mathrm{if~}a_{j}^{\\sf T}x\\leq b_{j}}}\\\\ {{\\infty}}&{{\\mathrm{otherwise}}}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "is the indicator function. Then the given optimization problem recasts to ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{x\\in\\mathbf{R}^{m}}{\\mathrm{minimize}}}&{\\frac{1}{N}\\sum_{j=1}^{N}\\left(f_{j}(x)+h_{j}(x)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "Since the minimization subproblem has an explicit solution ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{prox}_{R f_{j}}(z)=\\left\\{\\vphantom{\\sum_{j}^{j}}z\\right.\\qquad\\qquad\\qquad\\qquad\\qquad\\mathrm{if}\\ \\ensuremath{a_{j}^{\\sf T}}z\\leq b_{j}}\\\\ {z+\\frac{b_{j}-a_{j}^{\\sf T}z}{\\|a_{j}\\|_{2}^{2}}a_{j}\\quad\\mathrm{otherwise},}\\end{array}\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "this problem can be solved by PG-EXTRA. ", "page_idx": 61}, {"type": "text", "text": "We follow the same setting of [137]. We set $m=50$ . Each $Q_{j}$ is generated by taking the product of ${\\tilde{Q}}_{j}$ and its transpose, where $\\tilde{Q}_{j}\\in\\mathbf{R}^{m\\times m}$ is a matrix with elements that follow an i.i.d. Gaussian distribution. Each $p_{j}$ is generated to follow an i.i.d. Gaussian distribution. Vectors $a_{j}$ and $b_{j}$ are also randomly generated, however, we conducted the experiment for the case that the solution of the constrained problem differs from that of the unconstrained problem. We use Metropolis mixing matrix as in $\\S\\mathrm{I}.1$ . ", "page_idx": 61}, {"type": "text", "text": "The numerical results are illustrated in Figure 12. We compare PG-EXTRA and the variant method (48) obtained from the modified circuit with additional parallel capacitors. We use $R\\:=\\:0.05$ , $R=0.07$ for PG-EXTRA and $R=0.07$ , $C=0.3$ and $s=0.8$ for (48). The parameters for PGEXTRA were obtained through a grid search. The parameters for (48) are hand-optimized starting from $C=0$ and $s=0.5$ , the parameter selection that makes (48) to coincide with PG-EXTRA when $u^{0}=0$ . The relative error for (48) decreases to $10^{-8}$ in 147 iterations, while for PG-EXTRA with $R=0.05$ in 214 iterations. ", "page_idx": 61}, {"type": "image", "img_path": "9Jmt1eER9P/tmp/a12b22662587d1a351341811988372333fc28c4bab8e55ed7b310c316a180c31.jpg", "img_caption": ["Figure 12: (Left) Underlying graph $G$ . (Right) Relative error $\\left|f(x^{k})-f^{\\star}\\right|/f^{\\star}$ vs. $k$ . "], "img_footnote": [], "page_idx": 61}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 62}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 62}, {"type": "text", "text": "Justification: We provide the methodology to design an appropriate electric circuit whose continuous-time dynamics converge to the solution of the optimization problem in $\\S2,\\,\\S3$ , and introduce the automated, computer-assisted discretization methodology in $\\S4,\\,\\S5$ . ", "page_idx": 62}, {"type": "text", "text": "Guidelines: ", "page_idx": 62}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 62}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [NA] ", "page_idx": 62}, {"type": "text", "text": "Justification: This work proposes a new methodology for optimization algorithm design.   \nThe theoretical nature makes the discussion of such limitations not strongly relevant. ", "page_idx": 62}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 62}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 62}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 63}, {"type": "text", "text": "Justification: Our paper provides the full set of assumptions and complete proofs for all theoretical results, with theorems and lemmas properly numbered and cross-referenced. ", "page_idx": 63}, {"type": "text", "text": "Guidelines: ", "page_idx": 63}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 63}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 63}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 63}, {"type": "text", "text": "Justification: We provide the code and thoroughly describe the details to ensure reproducibility of the experimental results. ", "page_idx": 63}, {"type": "text", "text": "Guidelines: ", "page_idx": 63}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 63}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 64}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 64}, {"type": "text", "text": "Justification: We provide our codes and the instructions for using them in the README.md file. ", "page_idx": 64}, {"type": "text", "text": "Guidelines: ", "page_idx": 64}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 64}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 64}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 64}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 64}, {"type": "text", "text": "Justification: We provide the necessary details and code to understand and reproduce the experiments. ", "page_idx": 64}, {"type": "text", "text": "Guidelines: ", "page_idx": 64}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 64}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 64}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 64}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 64}, {"type": "text", "text": "Justification: Our work is theoretical and includes non-statistical simple experiments with synthetic data. Therefore, the notion of statistical significance is not applicable to our work. Guidelines: ", "page_idx": 64}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 64}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 65}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 65}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 65}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 65}, {"type": "text", "text": "Justification: We use minimal CPU computation for toy experiments. ", "page_idx": 65}, {"type": "text", "text": "Guidelines: ", "page_idx": 65}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 65}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 65}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 65}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 65}, {"type": "text", "text": "Justification: We have reviewed the NeurIPS Code of Ethics and our paper conforms to it in every respect. ", "page_idx": 65}, {"type": "text", "text": "Guidelines: ", "page_idx": 65}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 65}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 65}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 65}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 65}, {"type": "text", "text": "Justification: We conduct theoretical research on convex optimization and do not anticipate any negative social impacts from our results. ", "page_idx": 65}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 65}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 66}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 66}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 66}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 66}, {"type": "text", "text": "Justification: Our theoretical research on convex optimization does not involve releasing data or models, hence no safeguards are needed. ", "page_idx": 66}, {"type": "text", "text": "Guidelines: ", "page_idx": 66}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 66}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 66}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 66}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 66}, {"type": "text", "text": "Justification: We explicitly mention the solvers and the packages we are referring to in the paper and the code. ", "page_idx": 66}, {"type": "text", "text": "Guidelines: ", "page_idx": 66}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 66}, {"type": "text", "text": "", "page_idx": 67}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 67}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 67}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 67}, {"type": "text", "text": "Justification: This paper does not release new datasets or assets. ", "page_idx": 67}, {"type": "text", "text": "Guidelines: ", "page_idx": 67}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 67}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 67}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 67}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 67}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 67}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 67}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 67}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 67}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 67}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 67}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 67}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 68}]