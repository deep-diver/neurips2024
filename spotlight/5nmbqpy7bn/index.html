<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><title>TOPA: Extending Large Language Models for Video Understanding via Text-Only Pre-Alignment &#183; NeurIPS 2024</title>
<meta name=title content="TOPA: Extending Large Language Models for Video Understanding via Text-Only Pre-Alignment &#183; NeurIPS 2024"><meta name=description content="TOPA: Extending LLMs for video understanding using only text data."><meta name=keywords content="Natural Language Processing,Large Language Models,üè¢ Zhejiang University,"><link rel=canonical href=https://deep-diver.github.io/neurips2024/spotlight/5nmbqpy7bn/><link type=text/css rel=stylesheet href=/neurips2024/css/main.bundle.min.595affd4445a931ea6d6e3a5a3c709930fa52a60be10b21c6f81fdb8fecaacea33aacedf80cdc88be45f189be14ed4ce53ea74a1e1406fad9cbf90c5ed409173.css integrity="sha512-WVr/1ERakx6m1uOlo8cJkw+lKmC+ELIcb4H9uP7KrOozqs7fgM3Ii+RfGJvhTtTOU+p0oeFAb62cv5DF7UCRcw=="><script type=text/javascript src=/neurips2024/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj+e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script><script defer type=text/javascript id=script-bundle src=/neurips2024/js/main.bundle.min.efbf3b6b987689fffaf2d7b73173d2690c0279a04d444b0537a77d7f4ff6e6d493445400cb0cf56bc0f0f123e19f15394e63cae34e67f069bd013dd5c73df56e.js integrity="sha512-7787a5h2if/68te3MXPSaQwCeaBNREsFN6d9f0/25tSTRFQAywz1a8Dw8SPhnxU5TmPK405n8Gm9AT3Vxz31bg==" data-copy data-copied></script><script src=/neurips2024/lib/zoom/zoom.min.37d2094687372da3f7343a221a470f6b8806f7891aa46a5a03966af7f0ebd38b9fe536cb154e6ad28f006d184b294525a7c4054b6bbb4be62d8b453b42db99bd.js integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ=="></script><link rel=apple-touch-icon sizes=180x180 href=/neurips2024/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/neurips2024/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/neurips2024/favicon-16x16.png><link rel=manifest href=/neurips2024/site.webmanifest><meta property="og:url" content="https://deep-diver.github.io/neurips2024/spotlight/5nmbqpy7bn/"><meta property="og:site_name" content="NeurIPS 2024"><meta property="og:title" content="TOPA: Extending Large Language Models for Video Understanding via Text-Only Pre-Alignment"><meta property="og:description" content="TOPA: Extending LLMs for video understanding using only text data."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="spotlight"><meta property="article:published_time" content="2024-09-26T00:00:00+00:00"><meta property="article:modified_time" content="2024-09-26T00:00:00+00:00"><meta property="article:tag" content="Natural Language Processing"><meta property="article:tag" content="Large Language Models"><meta property="article:tag" content="üè¢ Zhejiang University"><meta property="og:image" content="https://deep-diver.github.io/neurips2024/spotlight/5nmbqpy7bn/cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://deep-diver.github.io/neurips2024/spotlight/5nmbqpy7bn/cover.png"><meta name=twitter:title content="TOPA: Extending Large Language Models for Video Understanding via Text-Only Pre-Alignment"><meta name=twitter:description content="TOPA: Extending LLMs for video understanding using only text data."><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"Spotlights","name":"TOPA: Extending Large Language Models for Video Understanding via Text-Only Pre-Alignment","headline":"TOPA: Extending Large Language Models for Video Understanding via Text-Only Pre-Alignment","abstract":"TOPA: Extending LLMs for video understanding using only text data.","inLanguage":"en","url":"https:\/\/deep-diver.github.io\/neurips2024\/spotlight\/5nmbqpy7bn\/","author":{"@type":"Person","name":"AI Paper Reviewer"},"copyrightYear":"2024","dateCreated":"2024-09-26T00:00:00\u002b00:00","datePublished":"2024-09-26T00:00:00\u002b00:00","dateModified":"2024-09-26T00:00:00\u002b00:00","keywords":["Natural Language Processing","Large Language Models","üè¢ Zhejiang University"],"mainEntityOfPage":"true","wordCount":"4870"}]</script><meta name=author content="AI Paper Reviewer"><link href=https://neurips.cc/ rel=me><link href=https://x.com/NeurIPSConf rel=me><link href rel=me><link href=https://github.com/deep-diver/paper-reviewer/ rel=me><link href=https://x.com/algo_diver/ rel=me><script src=/neurips2024/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj+KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script><script defer src=/neurips2024/lib/typeit/typeit.umd.1b3200cb448f5cd1f548f2781452643d3511a43584b377b82c03a58055da4fdb7bc8f6c6c2ce846480c7677ff25bfd0d75f15823c09443ab18e0fd2cad792587.js integrity="sha512-GzIAy0SPXNH1SPJ4FFJkPTURpDWEs3e4LAOlgFXaT9t7yPbGws6EZIDHZ3/yW/0NdfFYI8CUQ6sY4P0srXklhw=="></script><script defer src=/neurips2024/lib/packery/packery.pkgd.min.js integrity></script><script type=text/javascript src=/neurips2024/js/shortcodes/gallery.min.9b4cb28f931ed922c26fb9b2510c2debb370f6a63305050c2af81740b2919883715e24efbbdf3a081496718ec751df3a72729d4d0bc71d6071297563a97ce1ee.js integrity="sha512-m0yyj5Me2SLCb7myUQwt67Nw9qYzBQUMKvgXQLKRmINxXiTvu986CBSWcY7HUd86cnKdTQvHHWBxKXVjqXzh7g=="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-KX0S6Q55Y7"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-KX0S6Q55Y7")</script><meta name=theme-color></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32 scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a></div><div class=min-h-[148px]></div><div class="fixed inset-x-0 pl-[24px] pr-[24px]" style=z-index:100><div id=menu-blur class="absolute opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl shadow-2xl"></div><div class="relative max-w-[64rem] ml-auto mr-auto"><div style=padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start space-x-3"><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/neurips2024/ class="text-base font-medium text-gray-500 hover:text-gray-900">NeurIPS 2024</a></nav><nav class="hidden md:flex items-center space-x-5 md:ml-12 h-12"><a href=/neurips2024/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>About</p></a><a href=/neurips2024/oral/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Oral</p></a><a href=/neurips2024/spotlight/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Spotlight</p></a><a href=/neurips2024/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Tags</p></a><button id=search-button aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button><div class="ltr:mr-14 rtl:ml-14 flex items-center"><button id=appearance-switcher aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></nav><div class="flex md:hidden items-center space-x-5 md:ml-12 h-12"><span></span>
<button id=search-button-mobile aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></button>
<button id=appearance-switcher-mobile aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400" style=margin-right:5px><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div><div class="-my-2 -mr-2 md:hidden"><label id=menu-button class=block><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper style=padding-top:5px class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li id=menu-close-button><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/neurips2024/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>About</p></a></li><li class=mt-1><a href=/neurips2024/oral/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Oral</p></a></li><li class=mt-1><a href=/neurips2024/spotlight/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Spotlight</p></a></li><li class=mt-1><a href=/neurips2024/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Tags</p></a></li></ul></div></label></div></div></div></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("menu-blur");n.style.opacity=t/300})</script><div class="relative flex flex-col grow"><main id=main-content class=grow><article><div id=hero class="h-[150px] md:h-[200px]"></div><div class="fixed inset-x-0 top-0 h-[800px] single_hero_background nozoom" style=background-image:url(/neurips2024/spotlight/5nmbqpy7bn/cover_hu14950235677658222681.png)><div class="absolute inset-0 bg-gradient-to-t from-neutral dark:from-neutral-800 to-transparent mix-blend-normal"></div><div class="absolute inset-0 opacity-60 bg-gradient-to-t from-neutral dark:from-neutral-800 to-neutral-100 dark:to-neutral-800 mix-blend-normal"></div></div><div id=background-blur class="fixed opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl"></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("background-blur");n.style.opacity=t/300})</script><header id=single_header class="mt-5 max-w-prose"><ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden"><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/neurips2024/>NeurIPS 2024</a><span class="px-1 text-primary-500">/</span></li><li class=inline><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/neurips2024/spotlight/>Spotlights</a><span class="px-1 text-primary-500">/</span></li><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/neurips2024/spotlight/5nmbqpy7bn/>TOPA: Extending Large Language Models for Video Understanding via Text-Only Pre-Alignment</a><span class="px-1 text-primary-500">/</span></li></ol><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">TOPA: Extending Large Language Models for Video Understanding via Text-Only Pre-Alignment</h1><div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime=2024-09-26T00:00:00+00:00>26 September 2024</time><span class="px-2 text-primary-500">&#183;</span><span>4870 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">23 mins</span></div><div class="flex flex-row flex-wrap items-center"><span style=margin-top:.5rem class=mr-2 onclick='window.open("/neurips2024/tags/natural-language-processing/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Natural Language Processing
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/neurips2024/tags/large-language-models/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Large Language Models
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/neurips2024/tags/-zhejiang-university/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">üè¢ Zhejiang University</span></span></span></div></div><div class="flex author"><img class="!mt-0 !mb-0 h-24 w-24 rounded-full ltr:mr-4 rtl:ml-4" width=96 height=96 alt="AI Paper Reviewer" src=/neurips2024/img/avatar_hu3675860218824636004.png><div class=place-self-center><div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">Author</div><div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">AI Paper Reviewer</div><div class="text-sm text-neutral-700 dark:text-neutral-400">As an AI, I specialize in crafting insightful blog content about cutting-edge research in the field of artificial intelligence</div><div class="text-2xl sm:text-lg"><div class="flex flex-wrap text-neutral-400 dark:text-neutral-500"><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://neurips.cc/ target=_blank aria-label=Homepage rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg fill="currentcolor" height="800" width="800" id="Capa_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 491.398 491.398"><g><g id="Icons_19_"><path d="M481.765 220.422 276.474 15.123c-16.967-16.918-44.557-16.942-61.559.023L9.626 220.422c-12.835 12.833-12.835 33.65.0 46.483 12.843 12.842 33.646 12.842 46.487.0l27.828-27.832v214.872c0 19.343 15.682 35.024 35.027 35.024h74.826v-97.62c0-7.584 6.146-13.741 13.743-13.741h76.352c7.59.0 13.739 6.157 13.739 13.741v97.621h74.813c19.346.0 35.027-15.681 35.027-35.024V239.091l27.812 27.815c6.425 6.421 14.833 9.63 23.243 9.63 8.408.0 16.819-3.209 23.242-9.63 12.844-12.834 12.844-33.65.0-46.484z"/></g></g></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://x.com/NeurIPSConf target=_blank aria-label=X-Twitter rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href target=_blank aria-label=Line rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg id="Capa_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 14.707 14.707"><g><rect x="6.275" y="0" style="fill:currentColor" width="2.158" height="14.707"/></g></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://github.com/deep-diver/paper-reviewer/ target=_blank aria-label=Github rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://x.com/algo_diver/ target=_blank aria-label=X-Twitter rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg></span></span></a></div></div></div></div><div class=mb-5></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="order-first lg:ml-auto px-0 lg:order-last ltr:lg:pl-8 rtl:lg:pr-8"><div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-[140px]"><details open id=TOCView class="toc-right mt-0 overflow-y-scroll overscroll-contain scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600 rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 hidden lg:block"><summary class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="min-w-[220px] py-2 border-dotted ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#topa-llm-video-alignment>TOPA: LLM Video Alignment</a></li><li><a href=#textual-video-dataset>Textual Video Dataset</a></li><li><a href=#zero-shot-video-tasks>Zero-Shot Video Tasks</a></li><li><a href=#topa-limitations>TOPA Limitations</a></li><li><a href=#future-research>Future Research</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><details class="toc-inside mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 lg:hidden"><summary class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#topa-llm-video-alignment>TOPA: LLM Video Alignment</a></li><li><a href=#textual-video-dataset>Textual Video Dataset</a></li><li><a href=#zero-shot-video-tasks>Zero-Shot Video Tasks</a></li><li><a href=#topa-limitations>TOPA Limitations</a></li><li><a href=#future-research>Future Research</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><script>var margin=200,marginError=50;(function(){var t=$(window),e=$("#TOCView"),s=e.height();function n(){var n=t.height()-margin;s>=n?(e.css("overflow-y","scroll"),e.css("max-height",n+marginError+"px")):(e.css("overflow-y","hidden"),e.css("max-height","9999999px"))}t.on("resize",n),$(document).ready(n)})()</script></div></div><div class="min-w-0 min-h-0 max-w-fit"><div class="article-content max-w-prose mb-20"><br><div class="flex flex-row flex-wrap items-center space-x-2"><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 48 48" fill="none"><rect width="48" height="48" fill="#fff" fill-opacity=".01"/><path d="M18 43V22c0-3.3137 2.6863-6 6-6s6 2.6863 6 6V43" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M12 40V22c0-6.6274 5.3726-12 12-12s12 5.3726 12 12V40" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M6 35V22C6 12.0589 14.0589 4 24 4s18 8.0589 18 18V35" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 44V31" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 24.625v-2.75" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/></svg>
</span></span><span>5NMbQPY7Bn</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg fill="#000" height="800" width="800" id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 511.999 511.999"><g><g><path d="M421.578 190.264l-99.847-99.847c-2.439-2.439-6.391-2.439-8.829.0L82.824 320.495c-2.439 2.439-2.439 6.392.0 8.829l99.847 99.847c2.439 2.439 6.391 2.439 8.829.0l230.078-230.078C424.017 196.655 424.017 192.703 421.578 190.264z"/></g></g><g><g><path d="M506.511 87.672 424.323 5.484c-7.308-7.31-19.175-7.315-26.488.0L348.219 55.1c-2.439 2.439-2.439 6.391.0 8.829l99.847 99.847c2.439 2.437 6.391 2.437 8.829.0l49.616-49.616C513.826 106.847 513.826 94.987 506.511 87.672z"/></g></g><g><g><path d="M508.133 491.11c-1.054-9.556-9.489-16.599-19.104-16.599H111.633l36.058-15.163c4.088-1.719 5.131-7.034 1.994-10.17l-86.854-86.854c-3.137-3.135-8.451-2.094-10.17 1.994C52.224 365.359 2.052 484.66 1.627 485.707c-5.815 13.208 4.855 27.01 18.107 26.263H489.52C500.566 511.97 509.379 502.408 508.133 491.11z"/></g></g></svg>
</span></span><span>Wei Li et el.</span></span></span></div></div><p><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href="https://openreview.net/forum?id=5NMbQPY7Bn" target=_self role=button>‚Üó OpenReview
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://neurips.cc/virtual/2024/poster/96589 target=_self role=button>‚Üó NeurIPS Homepage</a></p><audio controls><source src=https://ai-paper-reviewer.com/5NMbQPY7Bn/podcast.wav type=audio/wav>Your browser does not support the audio element.</audio><h3 class="relative group">TL;DR<div id=tldr class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#tldr aria-label=Anchor>#</a></span></h3><div class="lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl"><p>Current video understanding methods struggle with the complexity of videos and noisy web video-text data, requiring extensive pre-training on large-scale video data. This is computationally expensive and limits accessibility for researchers. This paper introduces a novel approach, TOPA, which tackles these challenges.</p><p>TOPA uses Large Language Models (LLMs) to automatically generate &ldquo;textual videos&rdquo; with corresponding annotations, simulating real video-text pairs. It then uses these simulated pairs for pre-alignment with the video modality via CLIP model. This method successfully extends LLMs to video understanding without pre-training on real video data and achieves state-of-the-art results on challenging video benchmarks like EgoSchema.</p></div><h4 class="relative group">Key Takeaways<div id=key-takeaways class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#key-takeaways aria-label=Anchor>#</a></span></h4><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-b86562ae6aa8907b9ae172741f1c0821></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-b86562ae6aa8907b9ae172741f1c0821",{strings:[" TOPA pre-trains large language models for video understanding using only text data, eliminating the need for real video data and reducing computational costs. "],speed:10,lifeLike:!0,startDelay:0,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-9c80428ed1da0f4ad86e75cdb7ebedf5></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-9c80428ed1da0f4ad86e75cdb7ebedf5",{strings:[" TOPA introduces a novel text-only pre-alignment framework using automatically generated textual videos to simulate real video-text pairs. "],speed:10,lifeLike:!0,startDelay:1e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-8d467aa62f4c1955125ebfb8316034f8></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-8d467aa62f4c1955125ebfb8316034f8",{strings:[" Experiments show TOPA outperforms existing video-text pre-training methods on challenging long-form video understanding benchmarks, even without training on real videos. "],speed:10,lifeLike:!0,startDelay:2e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><h4 class="relative group">Why does it matter?<div id=why-does-it-matter class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#why-does-it-matter aria-label=Anchor>#</a></span></h4><p>This paper is important because it presents a novel approach to video understanding that avoids the computationally expensive and data-hungry process of traditional video-text pre-training. By using large language models and a text-only pre-alignment technique, researchers can achieve promising results on challenging video understanding benchmarks without the need for extensive video data. This opens up new possibilities for efficient video understanding research, especially for those with limited computational resources or access to large video datasets. The approach also tackles the issues with inconsistent web-collected video-text data. The proposed method addresses current limitations in the field, paving the way for further advancements in video-language research.</p><hr><h4 class="relative group">Visual Insights<div id=visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#visual-insights aria-label=Anchor>#</a></span></h4><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5NMbQPY7Bn/figures_2_1.jpg alt></figure></p><blockquote><p>This figure illustrates the TOPA framework, showing the process of generating the TextVid dataset (left) and aligning the large language model (LLM) with video modality (right). The left side depicts how an LLM generates textual videos with annotations to simulate real video-text pairs. The right side shows how TOPA uses these textual videos to pre-align the LLM with the video modality using CLIP, bridging the gap between textual and real videos for zero-shot and supervised fine-tuning.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5NMbQPY7Bn/tables_3_1.jpg alt></figure></p><blockquote><p>This table presents the zero-shot accuracy results on the EgoSchema benchmark, comparing various video understanding approaches. It highlights the performance of TOPA against existing methods that use pre-trained models on web video-text data or adapt image-based Multimodal Large Language Models (MLLMs) to videos. The table shows the Top-1 accuracy for each method, categorized by whether they use pre-trained web video data, adapt image MLLMs, utilize LLM-based video agents, or employ TOPA&rsquo;s novel text-only pre-alignment method. Closed-source LLMs are indicated, as are models trained on in-domain egocentric videos.</p></blockquote><h3 class="relative group">In-depth insights<div id=in-depth-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#in-depth-insights aria-label=Anchor>#</a></span></h3><h4 class="relative group">TOPA: LLM Video Alignment<div id=topa-llm-video-alignment class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#topa-llm-video-alignment aria-label=Anchor>#</a></span></h4><p>The concept of &ldquo;TOPA: LLM Video Alignment&rdquo; presents a novel approach to bridging the gap between large language models (LLMs) and video understanding. Instead of relying on traditional video-text pre-training with real video data, <strong>TOPA leverages an LLM to generate synthetic &ldquo;Textual Videos,&rdquo; sequences of textual descriptions mimicking real video frames.</strong> This allows for efficient pre-alignment of the LLM to video modalities without the need for extensive video data. <strong>A key strength is the use of CLIP for feature extraction</strong>, aligning textual and visual representations. <strong>The effectiveness of TOPA is demonstrated through impressive zero-shot and fine-tuning results on challenging video understanding benchmarks</strong>, highlighting its potential as a cost-effective and efficient method for video-LLM alignment. However, <strong>limitations exist, including the modality gap between textual and visual CLIP features and challenges in capturing fine-grained visual details.</strong> Future work could explore methods to mitigate these limitations, such as incorporating video instruction tuning.</p><h4 class="relative group">Textual Video Dataset<div id=textual-video-dataset class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#textual-video-dataset aria-label=Anchor>#</a></span></h4><p>A textual video dataset is a novel approach to video understanding that sidesteps the complexities of directly using real video data. Instead of relying on video frames, <strong>it leverages large language models (LLMs) to generate sequences of textual descriptions, simulating the temporal dynamics of a video</strong>. This approach bypasses challenges associated with visual-textual misalignment and inefficient language supervision in web-collected video-text datasets. The dataset&rsquo;s effectiveness hinges on the LLM&rsquo;s ability to create rich and accurate textual representations, effectively capturing video content. <strong>High-quality annotations, including dense descriptions and question-answer pairs, are crucial for successful pre-alignment of LLMs with the video modality.</strong> The dataset provides a scalable and cost-effective alternative to traditional video-text datasets, potentially addressing data scarcity limitations inherent in current video understanding research. The generation process itself is also significant, highlighting the capabilities of LLMs in creating synthetic yet realistic multimodal data. The quality of this synthetic data will directly influence the effectiveness of any downstream task.</p><h4 class="relative group">Zero-Shot Video Tasks<div id=zero-shot-video-tasks class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#zero-shot-video-tasks aria-label=Anchor>#</a></span></h4><p>Zero-shot video task evaluation is crucial for assessing the generalization capabilities of video understanding models. It reveals how well a model can perform on unseen video categories or tasks without any prior training data for those specific categories. <strong>Strong zero-shot performance demonstrates a model&rsquo;s robust learning of underlying visual and semantic concepts</strong>, transferring knowledge effectively across different domains. However, <strong>the inherent complexity and variability of videos pose significant challenges for zero-shot learning</strong>. Success often relies on the model&rsquo;s ability to leverage pre-training on large, diverse video-text datasets. Furthermore, the design of appropriate zero-shot evaluation metrics that accurately capture model understanding is critical. <strong>Careful consideration of dataset bias and the definition of task success are needed</strong> to ensure meaningful and fair evaluation. Future research directions include improving the robustness of zero-shot methods against domain shift and noise, developing more comprehensive benchmarks that capture varied aspects of visual understanding, and investigating novel training strategies that specifically target zero-shot generalization.</p><h4 class="relative group">TOPA Limitations<div id=topa-limitations class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#topa-limitations aria-label=Anchor>#</a></span></h4><p>The TOPA framework, while demonstrating promising results in video-language understanding, is not without limitations. A crucial limitation stems from the inherent <strong>modality gap between CLIP&rsquo;s image and text encoders</strong>. TOPA&rsquo;s reliance on CLIP for feature extraction introduces a discrepancy between training (textual videos) and inference (real videos), hindering performance. The reliance on textual video representations also means TOPA may struggle with tasks needing fine-grained temporal understanding or fine-grained visual details that textual descriptions often miss. <strong>The inability to capture the nuanced visual dynamics of real videos</strong> is another key limitation, impacting performance on tasks requiring precise visual analysis. While TOPA mitigates the modality gap through a projection technique, this approach inherently limits its capability to handle highly detailed or subtle visual aspects. Therefore, while a significant advancement, TOPA&rsquo;s performance is dependent on the quality of the textual video representation and may not fully capture the complexity of real-world video understanding.</p><h4 class="relative group">Future Research<div id=future-research class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#future-research aria-label=Anchor>#</a></span></h4><p>Future research directions stemming from this TOPA framework could explore several promising avenues. <strong>Firstly, enhancing the model&rsquo;s ability to handle fine-grained visual details</strong> is crucial, perhaps through integrating more advanced visual feature extractors or incorporating mechanisms for explicit temporal modeling. <strong>Secondly, investigating different LLMs</strong> beyond Llama 2 could reveal the extent of TOPA&rsquo;s generalizability and potential for even greater performance gains. <strong>Thirdly, expanding the TextVid dataset</strong> to encompass a wider range of video styles and complexities will further enhance the robustness of the pre-alignment process. Finally, <strong>combining TOPA with video instruction tuning</strong> could unlock the next level of video understanding, bridging the gap between textual and visual representations in a more powerful way. This fusion of techniques holds significant potential for advancing video-language capabilities.</p><h3 class="relative group">More visual insights<div id=more-visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#more-visual-insights aria-label=Anchor>#</a></span></h3><details><summary>More on figures</summary><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5NMbQPY7Bn/figures_5_1.jpg alt></figure></p><blockquote><p>This figure illustrates the TOPA framework, showing the pipeline for generating the TextVid dataset (left) and the video-LLM alignment process (right). The TextVid dataset generation involves using an LLM to create textual videos mimicking real videos. The video-LLM alignment uses CLIP to extract features from both textual and real videos and aligns the LLM to the video modality through text-only pre-alignment. The framework also allows for zero-shot inference and supervised fine-tuning on downstream datasets.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5NMbQPY7Bn/figures_8_1.jpg alt></figure></p><blockquote><p>This figure illustrates the TOPA framework, which consists of two main parts: TextVid dataset generation and video-LLM alignment. The left side shows how the TextVid dataset is created using an LLM to generate textual videos with annotations. The right side shows how the LLM is aligned with the video modality using continuous CLIP text features (during training) and then adapts to real video data using projected CLIP visual features (during inference). The framework also supports fine-tuning with real video data to further enhance performance.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5NMbQPY7Bn/figures_19_1.jpg alt></figure></p><blockquote><p>This figure illustrates the TOPA framework, which consists of two main parts: TextVid dataset generation and video-LLM alignment. The left side shows how the TextVid dataset is created using an LLM to generate textual videos (sequences of textual frames mimicking real videos) and their corresponding annotations (dense descriptions and QA pairs). The right side details the video-LLM alignment process. TOPA pre-aligns the LLM with video modality using only text data from TextVids, leveraging CLIP for feature extraction to bridge the gap between text and image. Zero-shot inference and supervised fine-tuning are also shown as options for adapting the aligned model to real video understanding tasks.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5NMbQPY7Bn/figures_19_2.jpg alt></figure></p><blockquote><p>This figure illustrates the TOPA framework, which consists of two main parts: TextVid dataset generation and video-LLM alignment. The left side shows how TextVid is created using an LLM to generate textual videos and corresponding annotations. The right side shows how TOPA aligns LLMs with video modality using only text data. During text-only pre-alignment, the LLM processes continuous CLIP text features, which are analogous to continuous CLIP image features from real videos. Zero-shot inference uses projected CLIP visual features, and fine-tuning can be done on downstream datasets.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5NMbQPY7Bn/figures_19_3.jpg alt></figure></p><blockquote><p>This figure illustrates the TOPA framework, which consists of two main parts: TextVid dataset generation and video-LLM alignment. The left side shows how the TextVid dataset is generated using an LLM to create textual videos and their corresponding annotations. The right side illustrates the video-LLM alignment process. During text-only pre-alignment, the LLM processes continuous CLIP text features of textual videos, effectively aligning itself with the video modality. In zero-shot inference, CLIP image features from real videos are projected into the LLM&rsquo;s space, allowing for video understanding without training on real video data. Finally, supervised fine-tuning is supported to enhance performance further.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5NMbQPY7Bn/figures_19_4.jpg alt></figure></p><blockquote><p>This figure illustrates the TOPA framework, which consists of two main parts: TextVid dataset generation and video-LLM alignment. The left side shows how TextVids are created using an LLM to generate textual frames and annotations. The right side details how the TextVids are used for text-only pre-alignment of the LLM with video modality via CLIP features, enabling zero-shot inference and supervised fine-tuning on downstream video datasets.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5NMbQPY7Bn/figures_20_1.jpg alt></figure></p><blockquote><p>This figure illustrates the TOPA framework, showing the process of generating the TextVid dataset (left) and the video-LLM alignment process (right). The TextVid dataset is created by using an LLM to generate textual videos that simulate real videos. These textual videos are then used to pre-align the LLM with the video modality. During inference, CLIP is used to extract features from real videos, which are then projected into the LLM feature space. The LLM can then be fine-tuned on downstream video datasets to further improve performance.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5NMbQPY7Bn/figures_20_2.jpg alt></figure></p><blockquote><p>This figure illustrates the TOPA framework, showing the process of generating the TextVid dataset (left) and how the video-LLM alignment is performed (right). The TextVid dataset is created using an LLM to generate textual videos mimicking real video content. The right side shows how TOPA aligns LLMs with video modality by using continuous CLIP text features for pre-alignment and projected CLIP visual features for zero-shot inference. Supervised fine-tuning on downstream datasets is also supported to further enhance performance.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5NMbQPY7Bn/figures_20_3.jpg alt></figure></p><blockquote><p>This figure illustrates the TOPA framework, which consists of two main parts: TextVid dataset generation and video-LLM alignment. The left side shows how the TextVid dataset is created using an LLM to generate textual videos and annotations. The right side depicts the video-LLM alignment process, where a pre-trained LLM is aligned with video modality using textual video representations. The alignment allows for both zero-shot inference (using projected CLIP visual features) and supervised fine-tuning on downstream video datasets.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5NMbQPY7Bn/figures_21_1.jpg alt></figure></p><blockquote><p>This figure shows the overall architecture of the TOPA framework, which is divided into two main parts: TextVid dataset generation and video-LLM alignment. The left side illustrates the process of generating the TextVid dataset using an LLM to create textual videos and corresponding annotations. The right side shows how TOPA aligns the LLM with the video modality using CLIP features. During text-only pre-alignment, the LLM processes continuous text features, and then transitions to processing continuous image features for real video inference using a projection layer. Zero-shot inference is also supported, as is supervised finetuning on downstream video datasets to further improve performance.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5NMbQPY7Bn/figures_21_2.jpg alt></figure></p><blockquote><p>This figure illustrates the TOPA framework, which consists of two main parts: TextVid dataset generation and video-LLM alignment. The left side shows the process of generating the TextVid dataset using an LLM to create textual videos and their annotations. The right side shows how TOPA aligns LLMs with video modality using CLIP features and a pre-alignment step, enabling zero-shot inference and supervised fine-tuning.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5NMbQPY7Bn/figures_21_3.jpg alt></figure></p><blockquote><p>This figure illustrates the TOPA framework, which consists of two main parts: TextVid dataset generation and video-LLM alignment. The left side shows how the TextVid dataset is created using an LLM to generate textual videos and annotations. The right side details the video-LLM alignment process, demonstrating how TOPA pre-aligns LLMs with video modality using only text data and then adapts to real video understanding through zero-shot inference and supervised fine-tuning.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5NMbQPY7Bn/figures_21_4.jpg alt></figure></p><blockquote><p>This figure shows the overall architecture of the TOPA framework, which consists of two main parts: TextVid dataset generation and video-LLM alignment. The left side illustrates the process of generating the TextVid dataset using an LLM to create textual videos and their corresponding annotations. The right side shows the video-LLM alignment process, which involves pre-aligning the LLM with the video modality using the generated TextVid data and then using CLIP to align the text and image features. The framework supports both zero-shot inference and supervised fine-tuning on downstream video datasets.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5NMbQPY7Bn/figures_21_5.jpg alt></figure></p><blockquote><p>This figure shows a schematic overview of the TOPA framework. The left side illustrates the process of generating the TextVid dataset, which involves using a large language model (LLM) to create textual videos and their corresponding annotations. The right side shows the process of aligning a large language model (LLM) with the video modality. This alignment is performed using Text-Only Pre-Alignment (TOPA), which leverages continuous textual frames (analogous to continuous CLIP image features) to pre-align the LLM with the video modality. The figure also highlights the zero-shot inference and supervised fine-tuning capabilities of the TOPA framework.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5NMbQPY7Bn/figures_22_1.jpg alt></figure></p><blockquote><p>This figure illustrates the TOPA framework, which consists of two main parts: TextVid dataset generation and video-LLM alignment. The left side shows how textual videos are generated using an LLM, mimicking real video dynamics. The right side shows how these textual videos are used to pre-align LLMs with video modality using CLIP features, allowing for zero-shot and finetuned video understanding. The framework highlights the process of generating textual video data, aligning the LLM with the video modality using textual frames and CLIP, performing zero-shot inference using projected CLIP visual features, and the option for supervised finetuning on downstream datasets.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5NMbQPY7Bn/figures_22_2.jpg alt></figure></p><blockquote><p>This figure illustrates the TOPA framework, which consists of two main parts: TextVid dataset generation and video-LLM alignment. The left side shows how textual videos (Tideos) are generated using an LLM, mimicking real videos with continuous textual frames and annotations. The right side depicts the video-LLM alignment process, where a pre-trained LLM is aligned with the video modality using the generated Tideos. The process includes text-only pre-alignment, zero-shot inference (using projected CLIP visual features), and optional supervised fine-tuning on downstream video datasets.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5NMbQPY7Bn/figures_22_3.jpg alt></figure></p><blockquote><p>This figure illustrates the TOPA framework, showing the process of generating the TextVid dataset (left) and the video-LLM alignment process (right). The TextVid dataset generation uses an LLM to create textual videos simulating real videos. The alignment process uses CLIP to bridge textual and real video modalities, enabling the LLM to learn from textual videos and then adapt to real video data. Zero-shot inference and supervised fine-tuning options are also shown.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5NMbQPY7Bn/figures_22_4.jpg alt></figure></p><blockquote><p>This figure illustrates the TOPA framework, showing the TextVid dataset generation pipeline on the left and the video-LLM alignment framework on the right. The left side depicts how an LLM is used to create textual videos (TextVids) mimicking real videos, complete with annotations for training. The right side details how TOPA aligns an LLM with video modality by using CLIP to bridge textual and real video features. TOPA supports both zero-shot inference (using projected CLIP visual features) and supervised fine-tuning on downstream video datasets.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5NMbQPY7Bn/figures_22_5.jpg alt></figure></p><blockquote><p>This figure illustrates the TOPA framework, which consists of two main parts: TextVid dataset generation and video-LLM alignment. The left side shows how the TextVid dataset is created using an LLM to generate textual videos and annotations. The right side details how TOPA aligns the LLM with video modality using continuous CLIP text and image features, enabling zero-shot inference and supervised fine-tuning.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5NMbQPY7Bn/figures_23_1.jpg alt></figure></p><blockquote><p>This figure illustrates the TOPA framework, which consists of two main parts: TextVid dataset generation and video-LLM alignment. The left side shows how textual videos (Tideos) are created using an LLM. These Tideos mimic real videos with textual frames and annotations, creating simulated video-text pairs. The right side depicts the video-LLM alignment process, where the LLM is pre-aligned with video modality using the generated Tideos and CLIP features (both text and image). The framework supports both zero-shot inference (using projected CLIP visual features) and supervised fine-tuning on downstream video datasets.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5NMbQPY7Bn/figures_23_2.jpg alt></figure></p><blockquote><p>This figure illustrates the TOPA framework, showing the process of generating the TextVid dataset (left) and the video-LLM alignment (right). The left side depicts how an LLM generates textual videos and their annotations. The right side details how TOPA pre-aligns LLMs with video modality using textual video data and CLIP for feature extraction, enabling zero-shot inference and fine-tuning on real video data.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5NMbQPY7Bn/figures_23_3.jpg alt></figure></p><blockquote><p>This figure illustrates the TOPA framework, which consists of two main parts: TextVid dataset generation and video-LLM alignment. The left side shows how TextVid is created using an LLM to generate textual videos with annotations. The right side demonstrates how TOPA aligns LLMs with video modality using TextVid, CLIP, and an adapter. The framework supports both zero-shot inference and supervised fine-tuning.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5NMbQPY7Bn/figures_24_1.jpg alt></figure></p><blockquote><p>This figure illustrates the TOPA framework, which consists of two main parts: TextVid dataset generation and video-LLM alignment. The left side shows how the TextVid dataset is created using an LLM to generate textual videos and annotations. The right side details the video-LLM alignment process. During text-only pre-alignment, the LLM processes textual video features (analogous to real video features), enabling zero-shot inference with real video data using projected CLIP visual features. The framework also supports supervised fine-tuning for improved performance.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5NMbQPY7Bn/figures_24_2.jpg alt></figure></p><blockquote><p>This figure illustrates the TOPA framework, which consists of two main parts: TextVid dataset generation and video-LLM alignment. The left side shows how the TextVid dataset is created using an LLM to generate textual videos and their annotations. The right side details the video-LLM alignment process, showing how TOPA uses text-only pre-alignment to align LLMs with the video modality. It also highlights the zero-shot inference and supervised finetuning capabilities of the framework.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5NMbQPY7Bn/figures_24_3.jpg alt></figure></p><blockquote><p>This figure illustrates the TOPA framework, which consists of two main parts: TextVid dataset generation and video-LLM alignment. The left side shows the process of generating the TextVid dataset using an LLM to create textual videos and their annotations. The right side illustrates the video-LLM alignment process, where the LLM is pre-aligned with video modality using textual videos and CLIP features. Zero-shot inference and supervised fine-tuning are also shown as ways to use the aligned LLM for video understanding tasks.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5NMbQPY7Bn/figures_24_4.jpg alt></figure></p><blockquote><p>This figure illustrates the TOPA framework, showing the process of generating the TextVid dataset (left) and the video-LLM alignment process (right). The TextVid generation uses an LLM to create textual videos that mimic real videos. The alignment process involves pre-aligning an LLM with textual video representations using CLIP for feature extraction, enabling zero-shot inference and fine-tuning on real video datasets.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5NMbQPY7Bn/figures_24_5.jpg alt></figure></p><blockquote><p>This figure illustrates the TOPA framework, which consists of two main parts: TextVid dataset generation and video-LLM alignment. The left side shows how TextVids are created using an LLM, generating textual frames analogous to real video frames, complete with descriptions and question-answer pairs. The right side depicts how TOPA pre-aligns LLMs with video data using only text, bridging the gap between textual and real video representations via the CLIP model. Zero-shot inference and supervised fine-tuning are also shown as options for using the aligned model.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5NMbQPY7Bn/figures_26_1.jpg alt></figure></p><blockquote><p>This figure illustrates the TOPA framework, which consists of two main parts: TextVid dataset generation and video-LLM alignment. The left side shows how TextVid is created using an LLM to generate textual videos and their annotations. The right side details the video-LLM alignment process, including text-only pre-alignment using continuous CLIP text features and zero-shot inference using projected CLIP visual features. The framework also supports supervised fine-tuning for enhanced performance.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5NMbQPY7Bn/figures_26_2.jpg alt></figure></p><blockquote><p>This figure visualizes the Tideo features generated from different types of prompts, namely Howto100m, WebVid, Ego4D, and WordNet. Each point represents a Tideo feature vector, and the color indicates the type of prompt used to generate it. The figure shows that the Tideo features generated from different types of prompts are clustered together in different regions of the feature space, indicating that the features capture different aspects of the video content. This visualization helps to understand the diversity of the Tideo dataset and how different types of prompts contribute to this diversity. The plot appears to use a dimensionality reduction technique (like t-SNE or UMAP) to project the high-dimensional feature vectors into a 2D space for visualization.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5NMbQPY7Bn/figures_28_1.jpg alt></figure></p><blockquote><p>This figure illustrates the TOPA framework, which consists of two main parts: TextVid dataset generation and video-LLM alignment. The left side shows how the TextVid dataset is created using an LLM to generate textual videos and their annotations. The right side details the video-LLM alignment process, including text-only pre-alignment using continuous CLIP text features, zero-shot inference using projected CLIP visual features, and optional supervised fine-tuning on downstream video datasets to enhance performance.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5NMbQPY7Bn/figures_29_1.jpg alt></figure></p><blockquote><p>This figure illustrates the TOPA framework, which consists of two main stages: TextVid dataset generation and video-LLM alignment. The left side shows how the TextVid dataset is created using an LLM to generate textual videos (Tideos) mimicking real videos. The right side details the video-LLM alignment process, which involves text-only pre-alignment using the Tideos and CLIP features, followed by zero-shot inference and optional supervised fine-tuning on real video datasets.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5NMbQPY7Bn/figures_31_1.jpg alt></figure></p><blockquote><p>This figure illustrates the TOPA framework, showing the two main stages: TextVid dataset generation and video-LLM alignment. The left side depicts the process of creating the TextVid dataset using an LLM to generate textual videos and annotations. The right side details the alignment process, where a language model (LLM) is pre-aligned with video modality using textual videos, and how this alignment facilitates both zero-shot inference using CLIP visual features and supervised finetuning on real video datasets.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5NMbQPY7Bn/figures_33_1.jpg alt></figure></p><blockquote><p>This figure illustrates the TOPA framework, which consists of two main parts: TextVid dataset generation and video-LLM alignment. The left side shows how the TextVid dataset is created using an LLM to generate textual videos and their annotations. The right side shows how the pre-trained LLM is aligned with video modality using the generated TextVid dataset and CLIP features. TOPA supports both zero-shot inference using projected CLIP visual features and supervised fine-tuning on downstream video datasets for improved performance.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5NMbQPY7Bn/figures_34_1.jpg alt></figure></p><blockquote><p>This figure illustrates the TOPA framework&rsquo;s two main stages: TextVid dataset generation and video-LLM alignment. The left side shows how textual videos (Tideos) are created using an LLM, mimicking real videos with textual frames and annotations. The right side details the alignment process: during text-only pre-alignment, the LLM processes continuous CLIP text features from Tideos. For real video inference (zero-shot or fine-tuned), CLIP visual features are projected to align with the LLM&rsquo;s text feature space. Fine-tuning on downstream video datasets can further enhance performance.</p></blockquote></details><details><summary>More on tables</summary><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5NMbQPY7Bn/tables_6_1.jpg alt></figure></p><blockquote><p>This table presents the zero-shot performance of various video understanding approaches on the EgoSchema benchmark&rsquo;s full set. It compares different model types (pre-trained on web video-text data, adapting image MLLMs for video understanding, LLM-based video agents, and the proposed TOPA), highlighting their accuracy (Acc@1). Closed-source LLMs are indicated, along with notes on any in-domain training and subset results for clarity.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5NMbQPY7Bn/tables_7_1.jpg alt></figure></p><blockquote><p>This table presents the results of a zero-shot evaluation on the EgoSchema benchmark. It compares the performance of various video understanding models, including those using closed-source LLMs (marked in gray), models trained with in-domain data (denoted by ‚Ä†), and models evaluated on a subset of EgoSchema (denoted by *). The table highlights the Top-1 accuracy achieved by each model, providing a comparison across different video understanding approaches.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5NMbQPY7Bn/tables_7_2.jpg alt></figure></p><blockquote><p>This table presents the zero-shot results on the EgoSchema benchmark, comparing various video understanding approaches. It highlights the performance of different models, categorized by their underlying core visual language models (VLMs) and large language models (LLMs). The table distinguishes between models trained on web video-text data, those adapting image-based MLLMs for video, LLM-based video agents, and the proposed TOPA method. The results show TOPA&rsquo;s competitive performance, especially considering its text-only pre-alignment approach and lack of training on real video data.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5NMbQPY7Bn/tables_7_3.jpg alt></figure></p><blockquote><p>This table presents the results of a zero-shot evaluation on the EgoSchema benchmark, comparing various video understanding approaches. It highlights the performance (accuracy@1) of different models, categorized by their core visual language models (VLMs), core large language models (LLMs), or whether they employ image-based adaptation or video agents. The table also indicates if a model uses closed-source LLMs and trained with in-domain egocentric videos, providing a comprehensive comparison of approaches.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5NMbQPY7Bn/tables_8_1.jpg alt></figure></p><blockquote><p>This table presents the zero-shot performance of various video understanding models on the EgoSchema benchmark. It compares different approaches, categorizing them by their underlying methodology (e.g., web video pre-training, adapting image MLLMs, LLM-based video agents, and the proposed TOPA method). The table highlights the Top-1 accuracy achieved by each model, showing how TOPA compares to existing state-of-the-art methods, even without training on real video data. The use of closed-source LLMs is indicated, as is the use of in-domain training data.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5NMbQPY7Bn/tables_9_1.jpg alt></figure></p><blockquote><p>This table presents the zero-shot video captioning results on MSR-VTT and VATEX benchmarks, using CIDEr scores as the evaluation metric. It compares the performance of various models, categorized into those pre-trained on web video-text data and those using a text-only pre-training approach (TOPA). The table highlights the performance improvement achieved by TOPA compared to other text-only methods and even some video-text pre-training methods.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5NMbQPY7Bn/tables_9_2.jpg alt></figure></p><blockquote><p>This table presents the results of a blind test conducted on the EgoSchema benchmark. The &lsquo;Blind&rsquo; setting means that the model only received the questions and choices, but not the actual video. This tests the model&rsquo;s ability to answer questions based solely on its pre-existing knowledge and linguistic understanding. The table compares the performance of several LLMs (Large Language Models) under this blind condition. The performance of TOPA models (TOPA-Llama2-7B and TOPA-Llama2-13B) is also shown, demonstrating their ability to perform well even without access to the video.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5NMbQPY7Bn/tables_9_3.jpg alt></figure></p><blockquote><p>This table presents the ablation study on the number of video frames used as input to the TOPA model for the NeXT-QA and EgoSchema benchmarks. It shows the performance of Llama2-7B and Llama2-13B models with 1, 5, and 10 frames, demonstrating how the accuracy increases with the number of frames, indicating the model&rsquo;s ability to capture temporal dynamics from more video information.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5NMbQPY7Bn/tables_17_1.jpg alt></figure></p><blockquote><p>This table compares the performance of various methods on the EgoSchema benchmark&rsquo;s subset and full set for multi-choice video QA. The &lsquo;Gap&rsquo; column shows the difference in accuracy between the subset and full set, highlighting the challenge of generalizing to more complex and diverse video data. The methods include those using similarity-based approaches, LLM logits, and LLM selection. TOPA is shown with and without multi-choice training to demonstrate the impact of this type of training. This table demonstrates the impact of using different evaluation methods (LLM Logits vs LLM Selection) on the performance gap between the subset and full set of the EgoSchema dataset.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5NMbQPY7Bn/tables_18_1.jpg alt></figure></p><blockquote><p>This table presents the ablation study of the modality projection (Equation 2) in the TOPA framework. It shows the results on the EgoSchema full set for two different models, TOPA-LLama2-7B and TOPA-LLama2-13B, with and without the modality projection. The modality projection aims to bridge the gap between CLIP text features used in pre-training and CLIP image features used in inference, improving the model&rsquo;s performance.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5NMbQPY7Bn/tables_25_1.jpg alt></figure></p><blockquote><p>This table presents the zero-shot results on the EgoSchema benchmark. It compares various video understanding approaches, categorized by their method (web video pre-training, adapting image MLLMs, LLM-based video agents, and the proposed TOPA method). The results are shown as accuracy at Top-1 (Acc@1), highlighting the performance of each method on the full EgoSchema dataset and, in some cases, a subset. The table also notes which methods use closed-source LLMs and those trained on in-domain egocentric videos.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5NMbQPY7Bn/tables_26_1.jpg alt></figure></p><blockquote><p>This table presents the vocabulary size of textual videos (Tideos) generated using different prompts. The prompts are based on four different datasets: Howto100m, Ego4D, WebVid, and WordNet. To ensure a fair comparison, a random sample of 20,000 global captions was selected from each dataset&rsquo;s generated Tideos, and the resulting vocabulary size was then calculated and displayed in the table. This analysis helps to understand the diversity of the language used across different sources and their impact on the generated Tideos.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/5NMbQPY7Bn/tables_27_1.jpg alt></figure></p><blockquote><p>This table details the hyperparameters used for training the various models in the paper. It includes information about the model, training dataset, number of epochs, effective batch size, base learning rate, and the optimizer used. The table differentiates between the pre-training phase and the fine-tuning phase for the TOPA models, and also shows hyperparameters for the baseline models for comparison.</p></blockquote></details><h3 class="relative group">Full paper<div id=full-paper class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#full-paper aria-label=Anchor>#</a></span></h3><div id=gallery-e1067488471cffca5b87f1381a39a4eb class=gallery><img src=https://ai-paper-reviewer.com/5NMbQPY7Bn/1.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/5NMbQPY7Bn/2.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/5NMbQPY7Bn/3.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/5NMbQPY7Bn/4.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/5NMbQPY7Bn/5.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/5NMbQPY7Bn/6.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/5NMbQPY7Bn/7.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/5NMbQPY7Bn/8.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/5NMbQPY7Bn/9.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/5NMbQPY7Bn/10.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/5NMbQPY7Bn/11.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/5NMbQPY7Bn/12.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/5NMbQPY7Bn/13.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/5NMbQPY7Bn/14.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/5NMbQPY7Bn/15.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/5NMbQPY7Bn/16.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/5NMbQPY7Bn/17.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/5NMbQPY7Bn/18.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/5NMbQPY7Bn/19.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/5NMbQPY7Bn/20.png class="grid-w50 md:grid-w33 xl:grid-w25"></div></div><section class="flex flex-row flex-wrap justify-center pt-4 text-xl"><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://deep-diver.github.io/neurips2024/spotlight/5nmbqpy7bn/&amp;title=TOPA:%20Extending%20Large%20Language%20Models%20for%20Video%20Understanding%20via%20Text-Only%20Pre-Alignment" title="Share on LinkedIn" aria-label="Share on LinkedIn"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://twitter.com/intent/tweet/?url=https://deep-diver.github.io/neurips2024/spotlight/5nmbqpy7bn/&amp;text=TOPA:%20Extending%20Large%20Language%20Models%20for%20Video%20Understanding%20via%20Text-Only%20Pre-Alignment" title="Tweet on Twitter" aria-label="Tweet on Twitter"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="mailto:?body=https://deep-diver.github.io/neurips2024/spotlight/5nmbqpy7bn/&amp;subject=TOPA:%20Extending%20Large%20Language%20Models%20for%20Video%20Understanding%20via%20Text-Only%20Pre-Alignment" title="Send via email" aria-label="Send via email"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1-27.64 140.9 68.65 266.2 199.1 285.1 19.01 2.888 36.17-12.26 36.17-31.49l1e-4-.6631c0-15.74-11.44-28.88-26.84-31.24-84.35-12.98-149.2-86.13-149.2-174.2.0-102.9 88.61-185.5 193.4-175.4 91.54 8.869 158.6 91.25 158.6 183.2v16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98.0036c-7.299.0-13.2 4.992-15.12 11.68-24.85-12.15-54.24-16.38-86.06-5.106-38.75 13.73-68.12 48.91-73.72 89.64-9.483 69.01 43.81 128 110.9 128 26.44.0 50.43-9.544 69.59-24.88 24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3c0-149.2-133.9-265.632-287.3-235.57zM239.1 304.3c-26.47.0-48-21.56-48-48.05s21.53-48.05 48-48.05 48 21.56 48 48.05-20.6 48.05-48 48.05z"/></svg></span></a></section></div><script>var oid="views_spotlight/5NMbQPY7Bn/index.md",oid_likes="likes_spotlight/5NMbQPY7Bn/index.md"</script><script type=text/javascript src=/neurips2024/js/page.min.0860cf4e04fa2d72cc33ddba263083464d48f67de06114529043cb4623319efed4f484fd7f1730df5abea0e2da6f3538855634081d02f2d6e920b956f063e823.js integrity="sha512-CGDPTgT6LXLMM926JjCDRk1I9n3gYRRSkEPLRiMxnv7U9IT9fxcw31q+oOLabzU4hVY0CB0C8tbpILlW8GPoIw=="></script></section><footer class="pt-8 max-w-prose print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="flex group mr-3" href=/neurips2024/spotlight/qfuszvw9mx/><span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&larr;</span>
<span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Towards an Information Theoretic Framework of Context-Based Offline Meta-Reinforcement Learning</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2024-09-26T00:00:00+00:00>26 September 2024</time>
</span></span></a></span><span><a class="flex text-right group ml-3" href=/neurips2024/spotlight/lnnfwc2ah1/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Tolerant Algorithms for Learning with Arbitrary Covariate Shift</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2024-09-26T00:00:00+00:00>26 September 2024</time>
</span></span><span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&larr;</span></a></span></div></div><div class=pt-3><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class=pt-3><script src=https://utteranc.es/client.js repo=pmnxis/pmnxis.github.io issue-term=pathname label=Comment theme=dark-blue crossorigin=anonymous async></script></div></div></footer></article><div id=top-scroller class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer id=site-footer class="py-10 print:hidden"><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2024
AI Paper Reviewer</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://blowfish.page/ target=_blank rel="noopener noreferrer">Blowfish</a></p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/neurips2024/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh+sCQ0E53ghYrxgYqw+0GCRyIEpA=="></script></footer><div id=search-wrapper class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://deep-diver.github.io/neurips2024/ style=z-index:500><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body><script data-name=BMC-Widget data-cfasync=false src=https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js data-id=chansung data-description="Support me on Buy me a coffee!" data-message data-color=#FFDD00 data-position=Left data-x_margin=18 data-y_margin=18></script></html>