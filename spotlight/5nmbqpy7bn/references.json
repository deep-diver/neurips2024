{"references": [{"fullname_first_author": "A. Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-01", "reason": "CLIP, introduced in this paper, is a fundamental model used for feature extraction in TOPA, bridging the gap between textual and real video representations."}, {"fullname_first_author": "J.-B. Alayrac", "paper_title": "Flamingo: a visual language model for few-shot learning", "publication_date": "2022-12-01", "reason": "Flamingo is a state-of-the-art visual language model, used as a comparison point to highlight TOPA's performance in video understanding tasks."}, {"fullname_first_author": "K. Mangalam", "paper_title": "Egoschema: A diagnostic benchmark for very long-form video language understanding", "publication_date": "2023-12-01", "reason": "EgoSchema, introduced in this paper, is a challenging benchmark used to evaluate TOPA's performance on long-form video understanding, showcasing its capabilities on complex, temporal understanding tasks."}, {"fullname_first_author": "J. Li", "paper_title": "BLIP: Bootstrapping language-image pre-training for unified vision-language understanding and generation", "publication_date": "2022-07-01", "reason": "BLIP is a foundational vision-language model that shares similarities with TOPA's approach in aligning large language models with image and video modalities."}, {"fullname_first_author": "Y. Wang", "paper_title": "Internvideo: General video foundation models via generative and discriminative learning", "publication_date": "2022-12-01", "reason": "InternVideo is a strong baseline model for video understanding, highlighting the advancements achieved by TOPA, particularly in efficiency by avoiding large-scale video pre-training."}]}