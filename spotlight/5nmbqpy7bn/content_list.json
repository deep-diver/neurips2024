[{"type": "text", "text": "TOPA: Extending Large Language Models for Video Understanding via Text-Only Pre-Alignment ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Wei Li1,2 Hehe Fan1\u2217 Yongkang Wong3 Mohan Kankanhalli3 Yi Yang1,2 ", "page_idx": 0}, {"type": "text", "text": "1 ReLER Lab, CCAI, Zhejiang University, China 2 The State Key Laboratory of Brain-Machine Intelligence, Zhejiang University, China 3 School of Computing, National University of Singapore, Singapore {weili6,hehefan,yangyics}@zju.edu.cn yongkang.wong@nus.edu.sg mohan@comp.nus.edu.sg https://github.com/dhg-wei/TOPA ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recent advancements in image understanding have beneftied from the extensive use of web image-text pairs. However, video understanding remains a challenge despite the availability of substantial web video-text data. This dififculty primarily arises from the inherent complexity of videos and the inefifcient language supervision in recent web-collected video-text datasets. In this paper, we introduce Text-Only Pre-Alignment (TOPA), a novel approach to extend large language models (LLMs) for video understanding, without the need for pre-training on real video data. Specifically, we first employ an advanced LLM to automatically generate Textual Videos comprising continuous textual frames, along with corresponding annotations to simulate real video-text pairs. Then, these annotated textual videos are used to pre-align language-only LLMs with the video modality. To bridge the gap between textual and real videos, we employ the CLIP model as the feature extractor to align image and text modalities. During text-only pre-alignment, the continuous textual frames, encoded as a sequence of CLIP text features, are analogous to continuous CLIP image features, thus aligning the LLM with real video representation. Extensive experiments, including zero-shot evaluation and finetuning on various video understanding tasks, demonstrate that TOPA is an effective and efifcient framework for aligning video modality with LLMs. In particular, without training on any video data, the TOPA-Llama2-13B model achieves a Top-1 accuracy of $51.0\\%$ on the challenging long-form video understanding benchmark, EgoSchema. This performance surpasses previous video-text pre-training approaches and is competitive with recent GPT-3.5-based video agents. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Image-language understanding has made large advancements in both image-language alignment [26, 48] and Multimodal Large Language Models (MLLMs) [1, 27, 34, 89], aided by pre-training on large-scale noise-paired image-text data collected from the web [6, 19, 51, 53, 52]. This raises a question: Can we mirror this success in video-language understanding? Research [45, 64, 77, 86] has explored pretraining video-language models on millions of web video-text data [3, 40, 65], achieving promising results in basic video tasks such as video-text retrieval, video captioning, and video question answering across conventional video benchmarks. However, recent research reveals that these models struggle with a challenging long-form video understanding benchmark, i.e., EgoSchema [39], which requires intrinsic temporal understanding capabilities. This highlights the gap in adapting web video-text pretrained models to more comprehensive video understanding tasks. ", "page_idx": 0}, {"type": "text", "text": "We attribute this gap to two primary factors: 1) The intrinsic complexity of the video modality. Videos introduce intrinsic complexities in both spatial and temporal dimensions, which are not present in static images. These complexities require extensive training on larger-scale data to effectively capture video dynamics. Furthermore, representing videos typically involves processing multiple frames, significantly increasing computational demands compared to image modeling. The dual challenges of large-scale training and increased computational requirements make video-language modeling particularly challenging. 2) The limitations of web language supervision. The language supervision in recent web video-text datasets primarily comes from subtitles or descriptions associated with the videos [3, 40]. However, subtitles often suffer from the issues of visual-textual misalignment [33, 17]. Moreover, the form of descriptive supervision is inefifcient in building robust video reasoning capabilities, especially in terms of temporal reasoning. This mismatch between the complex video content and the limited supervision hinders effective video-language modeling. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we propose an innovative approach to develop video understanding capabilities by using LLMs to simulate and understand video dynamics. Instead of directly aligning LLMs with real video representation, we first introduce a textual video representation \u2014 a sequence of textual frames designed to mimic real visual dynamics. This textual video can be readily generated by advanced LLMs and effectively simulates various video dynamics by describing them in text. Specifically, we present a Textual Video (TextVid) dataset, automatically generated by LLMs. TextVid includes: 1) Textual videos (hereinafter referred to as \u201cTideo\u201d), which consist of a sequence of textual frames crafted to mimic the keyframes of real videos, and 2) Tideo annotations, including comprehensive Tideo-level dense descriptions and varied question-answer (QA) pairs. These annotations are of high quality and closely align with the Tideo content, by virtue of the powerful capability of LLM in language generation. ", "page_idx": 1}, {"type": "text", "text": "Building on the proposed TextVid dataset, we introduce the Text-Only Pre-Alignment (TOPA) framework, to effectively and efifciently pre-align LLMs with the video modality, reducing the need for costly video-text pre-training. We introduce three tasks for video-LLM pre-alignment: Tideo summarization, Tideo QA and multi-choice Tideo QA. To bridge the gap between textual Tideos and visual videos, we leverage the CLIP [48] model for feature extraction. Specifically, we employ the CLIP text encoder to extract frame-level representations for Tideos, and the CLIP visual encoder for real videos. During the text-only pre-alignment phase, the LLM learns to process continuous CLIP text features of Tideos. In the real video inference phase, it transitions to handling continuous CLIP image features of real video. Due to the aligned CLIP image-text feature space, the LLM can adapt to real video inputs despite being trained on textual representations. Our main contributions include: ", "page_idx": 1}, {"type": "text", "text": "(1) We propose a novel Text-Only Pre-Alignment (TOPA) framework to extend Large Language Models (LLMs) for video understanding. TOPA aligns LLMs with the video modality efifciently and effectively without the need for training on real videos, reducing the costs for video-text pre-training. ", "page_idx": 1}, {"type": "text", "text": "(2) We introduce TextVid, a textual video dataset automatically generated by advanced LLMs. TextVid dataset comprises 721K diverse Tideos along with associated high-quality annotations, which include detailed Tideo descriptions and a variety of question-answer pairs. ", "page_idx": 1}, {"type": "text", "text": "(3) Extensive experiments demonstrate TOPA\u2019s effectiveness across various video understanding tasks. Particularly, the TOPA-Llama2-13B model achieves $51.0\\%$ Top-1 accuracy in the challenging EgoSchema benchmark, outperforming previous video-text pretraining methods and competitive with recent GPT-3.5-based video agents. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Vision-language alignment. CLIP [48] aligns the vision and language modalities in a common feature space via contrastive learning with large-scale web image-text data. MLLMs [1, 27, 34, 89] align the visual model with LLM via training on image-caption pairs and interleaved image-text data. Video-LLMs [7, 23, 32, 83] explore modeling video sequences within LLM spaces, leveraging LLM for video-language understanding. In this paper, we focus on video-LLM alignment. Rather than using multimodal data for vision-language alignment, we introduce a novel text-only pre-alignment framework to extend LLMs for video understanding without pre-training on real video-text data. ", "page_idx": 1}, {"type": "text", "text": "LLMs for multimodal data augmentation. Recent research explores the use of LLMs to enhance the multimodal data. A line of work [5, 12, 34] use LLMs for refining captions or extending the image caption pairs to diverse visual tasks like visual conversation and image editing. Another line of work [28, 37, 38, 47] further employ advanced LLM to enrich web video supervision for video instruction tuning. In this paper, rather than enhancing multimodal datasets, we propose generating text-only data consisting of \"textual videos\" and diverse language supervision, which aims to simulate real videos and their corresponding annotations. ", "page_idx": 1}, {"type": "image", "img_path": "5NMbQPY7Bn/tmp/29be662fef6badbc32fd23b4fc706261a84a7b1c23b0b71aeb51abd48e6848e8.jpg", "img_caption": ["Figure 1: Overview of the proposed Text-Only Pre-Alignment (TOPA) framework. Left: The pipeline used for generating the TextVid dataset. Right: The video-LLM alignment framework. During text-only pre-alignment, the LLM learns to process continuous CLIP text features. In zero-shot inference, the LLM uses projected CLIP visual features as input. Additionally, TOPA supports supervised fine-tuning on downstream video datasets to further improve the performance. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Long-form video understanding. Long-form video understanding [39, 61, 71] presents significant challenges due to the intricate spatial and temporal dynamics. Conventional video-text pretraining approaches [4, 45, 65, 66, 77, 90] utilize extensive web video-caption data for video-language alignment. Recent research [28, 66, 83, 88] employ video instruction-tuning for video-LLM alignment to enhance video-language understanding. Another line of research [22, 54, 81, 49] seeks to adapt recent image MLLMs to video understanding. A parallel line of research [9, 42, 55, 60, 63, 13, 78, 82, 67, 21] combine the LLM with various VLM tools as video agents to perform video-understanding tasks. In this paper, we propose a novel text-only pre-alignment framework to efifciently and effectively align LLMs with videos without pre-training on real videos. ", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we detail the TOPA framework. We first introduce the data generation pipeline of TextVid Dataset (Section 3.1). Next, we describe how to align the Tideo representation with LLM (Section 3.2). Finally, we discuss adapting the text-only aligned video-LLM model for real video inference (Section 3.3). An overview is illustrated in Figure 1. ", "page_idx": 2}, {"type": "text", "text": "3.1 TextVid Dataset ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "This dataset, comprising textual videos (Tideos) and associated annotations, is generated by an advanced LLM (i.e., Gemini Pro 1.0 [56]). The data generation pipeline is detailed in Appendix D. Each Tideo is presented in a textual format and contains 5-15 sequential frames. Each frame includes a frame caption that describes the scene and multiple object captions. To enhance understanding and interaction with these Tideos, the dataset features a dense description summarizing the Tideo, as well as a set of multiple-choice questions and answers related to the Tideo content. The structure of each element is as follows: ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "table", "img_path": "5NMbQPY7Bn/tmp/0ae8ad730e949cce6a097c45934e1bfd384fd2c68d2a2e6c01ad30a24e1d3f37.jpg", "table_caption": [], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "There are two major advantages of the TextVid dataset. (1) The large-scale and diverse Tideos. As the dataset is text-only and fully generated by an LLM, the size of TextVid is practically unlimited. Moreover, the Tideos can cover a broad range of domains by simply prompting the language model with appropriate conditions. It is distinctly different from previous web video-text dataset like Howto100M [40] that are limited to specific human-centric instructional videos. In practice, we enhance the diversity of TextVid by randomly sampling video captions from WebVid-2M [3], video titles from Howto $100\\mathrm{m}$ [40], video tasks from Ego4D [15] and object names with descriptions from WordNet [41] as a condition of prompts. These varied prompts enable the language model to generate a diverse dataset. (2) The high-quality, consistent and free-form language supervision. The language supervisions are generated along with Tideos. The advanced capabilities of LLM ensure the quality of these supervisions, making them less noisy than web video-text data. Moreover, both the Tideo and the supervision are in textual format, making the supervision closely aligned with the Tideo\u2019s content. Additionally, the format of the language supervision is unrestricted. For example, we prompt the LLM to generate dense descriptions and multi-choice QA pairs as language supervision. ", "page_idx": 3}, {"type": "text", "text": "3.2 Text-Only Pre-Alignment ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Preliminary: Video-LLM alignment. The goal of video-LLM alignment is to extend pre-trained LLMs for processing video inputs. Given a video sampled with $n$ frames $\\{\\mathbf{I}_{1},\\mathbf{I}_{2},\\dotsc,\\mathbf{I}_{n}\\}$ , Recent work [23, 77] uses a frozen CLIP model to extract the frame-level visual feature, formulated as $\\mathbf{f}_{i}^{v}=E_{\\mathrm{image}}(\\mathbf{I}_{i})$ , where $E_{\\mathrm{image}}$ denotes CLIP image encoder. The CLIP features are then projected into the LLM space via a simple linear layer, denoted as $G(P(\\mathbf{f}_{1}^{v}),...,P(\\mathbf{f}_{n}^{v}))$ , where $G$ denotes a language model and $P$ denotes a projection layer that projects the CLIP feature to LLM space. ", "page_idx": 3}, {"type": "text", "text": "Tideo representation. In this work, we leverage Tideos ( $x f.$ Section 3.1) for video-LLM pre-alignment instead of training on real videos. Specifically, given the textual frame $T_{i}$ , we employ CLIP text encoder to extract the frame representation from frame caption $C_{i}$ and detailed object captions $D_{i}$ , represented as $\\mathbf{f}_{i}^{t}=F_{\\mathrm{fusion}}(E_{\\mathrm{text}}(C_{i}),E_{\\mathrm{text}}(D_{i,1}),...,E_{\\mathrm{text}}(\\bar{D}_{i,j}))$ , where $F_{\\mathrm{fusion}}$ is a fusion function such as simple average pooling, and $E_{\\mathrm{text}}$ denotes the CLIP text encoder. A Tideo with $n$ textual frames is represented as $\\mathbf{V}^{t}=\\{\\mathbf{f}_{1}^{t},...,\\mathbf{f}_{n}^{t}\\}$ . ", "page_idx": 3}, {"type": "text", "text": "Text-only pre-alignment. Given the Tideo $T$ , dense Tideo-level description $D_{V}$ , and QA pairs with multiple choices $\\{(Q_{k},O_{k},A_{k})\\}$ , we introduce the following tasks for Tideo-LLM alignment: (1) Tideo Summarization: Given the Tideo, generate a detailed description to summarize the Tideo; (2) Tideo QA: Given the Tideo and question, predict the answer; (3) Multi-choice Tideo QA: Given the Tideo, question and multiple choices, choose the correct answer from the candidates. We employ a unified auto-regressive Language Modeling (LM) objective for these three tasks: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{LM}}(\\theta_{G},\\theta_{P})=-\\frac{1}{|t|}\\sum_{i=1}^{|t|}\\log G\\big(t_{i}|P(\\mathbf{V^{t}}),Z,t_{<i}\\big),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathbf{V}^{\\mathbf{t}}$ denotes the Tideo representation, and $\\mathbf{V^{t}}=\\{\\mathbf{f}_{1}^{t},...,\\mathbf{f}_{n}^{t}\\}$ during the text-only training, $Z$ denotes the task specific condition tokens and $t_{i}$ denotes the $i_{t h}$ target token. $\\theta_{G}$ and $\\theta_{P}$ denote the learnable parameters of the LLM adapter and the projection layer $P$ , respectively. In practice, we use the following format as the LLM input: {Task Instruction}. Video: $\\{\\mathbf{f}_{1}^{t},...,\\mathbf{f}_{n}^{t}\\}$ . {Task Conditions}. Answer: {Predict Targets}. For the Tideo summarization task, the target is detailed Tideo descriptions. For Tideo QA task, the target is the answer and the condition is the question. For multi-choice Tideo ", "page_idx": 3}, {"type": "text", "text": "QA task, the target is the correct option and the condition consists with question and options. The details of the task-specific prompts are included in Appendix F.1. ", "page_idx": 4}, {"type": "text", "text": "3.3 Adapting to Real Video Understanding ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Section 3.2 introduces the text-only pre-alignment using the TextVid dataset. In this section, we detail how to adapt this text-only pre-aligned LLM for real video understanding. We introduce two approaches: one is zero-shot inference, which directly infers with real video data. And the other is supervised finetuning, where the pre-aligned model is further finetuned on downstream video data. ", "page_idx": 4}, {"type": "text", "text": "Zero-shot inference. During pre-alignment, we leverage the textual representation $\\mathbf{V}^{t}=\\{\\mathbf{f}_{1}^{t},...,\\mathbf{f}_{n}^{t}\\}$ as the Tideo representation. During inference, we take real videos features as input, i.e., ${\\bf V}^{v}\\,=$ $\\{\\mathbf{f}_{1}^{v},...,\\mathbf{f}_{n}^{v}\\}$ , where $\\mathbf{f}_{i}^{v}=E_{\\mathrm{image}}(\\mathbf{I}_{i})$ . These two modality features $\\mathbf{f}^{t}$ and $\\mathbf{f}^{v}$ that come from CLIP image encoder and CLIP text encoder are aligned via CLIP pre-training. This aligned image-text representation makes it possible to perform zero-shot inference without additional finetuning. However, the modality gap phenomenon [16, 30, 31, 44, 85], i.e., CLIP image feature and CLIP text feature are located in two completely separate regions of the feature space, prevents us from directly taking the visual feature $\\mathbf{f}^{v}$ as the textual feature $\\mathbf{f}^{t}$ . To bridge this modality gap, we follow DeCap [30] to employ a support memory to project the CLIP visual feature into the CLIP text feature space. This training-free projection process is formulated as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{f}^{v\\rightarrow t}=\\sum_{i=1}^{N}w_{i}*\\mathbf{m}_{i}=\\sum_{i=1}^{N}\\frac{\\exp((\\mathbf{m}_{i}^{\\top}\\mathbf{f}^{v})/\\tau)}{\\sum_{k=1}^{N}\\exp((\\mathbf{m}_{k}^{\\top}\\mathbf{f}^{v})/\\tau)}*\\mathbf{m}_{i},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where ${\\bf m}_{i}$ denotes CLIP text feature from a pre-constructed memory of size $N,\\mathbf{f}^{v}$ denotes input frame feature of real video and $\\mathbf{f}^{v\\rightarrow t}$ denotes the projected feature. During zero-shot inference, we take the $\\mathbf{V}^{v\\rightarrow t}=\\{\\mathbf{f}_{1}^{v\\rightarrow t},...,\\mathbf{f}_{n}^{v\\rightarrow t}\\}$ as the real video\u2019s representation. ", "page_idx": 4}, {"type": "text", "text": "Supervised finetuning. On the other hand, the text-only pre-alignment can be viewed as a pretraining stage. Following the pretraining-finetuning paradigm, the pre-aligned LLMs can then be fine-tuned on real video data for improved downstream task performance. The finetuning process is similar to the text-only pre-alignment as detailed in Section 3.2, except that the LLM receives a sequence of CLIP visual features as input instead of CLIP textual features. ", "page_idx": 4}, {"type": "text", "text": "3.4 Implementation Details ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We leverage Llama2-7B, Llama2-13B [57] and Llama3-8B as the LLM backbone. Additionally, we employ the Llama-adapter [84] with an adaptation embedding length of 50. We utilize CLIP-ViT-L as the multimodal encoder. We employ a simple linear layer to project the CLIP feature into the LLM feature space. During training, the CLIP model and LLM backbone are frozen. The projection layer and additional Llama-adapter are trainable. For text-only pre-alignment, we uniformly sample the Tideos into 10 frames. We train the model on a mixture of tasks comprising Tideo summarization, Tideo QA, multi-choice Tideo QA with the ratio of 1:1:2. For zero-shot inference, we construct a memory for cross-modal projection, consisting of 2M CLIP text features sampled from captions in the TextVid dataset. TOPA-Llama2-7B and TOPA-Llama3-8B are trained on four 40G-A100 GPUs in one day. TOPA-Llama2-13B is trained in two days. More training details of TOPA and baselines are included in Appendix E.2. ", "page_idx": 4}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "TOPA enables the LLM to perform various video understanding tasks as shown in Figure 2. In this section, we evaluate TOPA on multi-choice video QA and video captioning tasks. Section 4.1 evaluates TOPA on NeXT-QA [72], STAR [70], TVQA [24], recent challenging EgoSchema [39] and MVBench[29] benchmarks with the zero-shot setting. We further evaluate TOPA on multi-choice video QA with the finetuning setting (Section 4.2) and zero-shot video captioning task (Section 4.3). In Section 4.4, we conduct ablation study on the LLM prior and input video frames. We report Top-1 accuracy on multi-choice video QA benchmarks and CIDEr [58] score on video captioning benchmarks. We mainly compare TOPA with the following categories of video understanding approaches: ", "page_idx": 4}, {"type": "image", "img_path": "5NMbQPY7Bn/tmp/d668be81d0301f853ee748fb296e3c491e5ffb84489acbebc96469fd1d5bea95.jpg", "img_caption": ["Figure 2: Examples of TOPA-LLama2-13B for video-language understanding. Given a video, TOPA is able to summarize the video content and answer the questions. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "(1) Web video pre-training approaches [4, 45, 64, 66, 77]. This line of work aims to develop general video-language models by leveraging extensive web videos, using associated video captions or audio as weak supervision signals. ", "page_idx": 5}, {"type": "text", "text": "(2) Adapting image MLLMs for video understanding [22, 81, 49]. These approaches aim to extend the image understanding capabilities of recent vision-language models (VLMs) to video understanding. Specifically, SeViLa [81] utilizes BLIP-2 for localizing and understanding key frames of a video. IG-VLM [22] converts video into a composite image by arranging the video frames into a grid layout. ", "page_idx": 5}, {"type": "text", "text": "(3) LLM-based video agents [42, 59, 60, 63, 82, 13, 21, 67]. This line of work leverages LLMs like GPT-3.5 and GPT-4 as an agent to understand a video by designing and executing a series of actions. The language-only agents perceive visual information via recent foundation VLMs (e.g., CLIP [48], BLIP-2 [27], LaViLa [87] and PALI [8]). ", "page_idx": 5}, {"type": "text", "text": "(4) Our text-only pre-alignment. Different from the above works, TOPA leverages the proposed TextVid dataset for video-LLM pre-alignment, enabling the LLM to process continuous features. Thus, it can enable performing video understanding tasks. ", "page_idx": 5}, {"type": "text", "text": "4.1 Zero-Shot Evaluation on Multi-Choice Video QA ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1.1 Zero-shot Results on EgoSchema ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Table 1 shows the results on EgoSchema full set. We compare our method against a range of recent approaches in video understanding. Our proposed text-only pre-alignment framework, despite training without real videos, shows impressive results on the EgoSchema benchmark. TOPA outperforms previous image-based adaptation approach IG-VLM and video agents LLoVi and Vamos with the same scale LLM (Llama2-7B and Llama2-13B). Moreover, TOPA shows consistent improvements when scaled up with a larger LLM backbone, indicating the effectiveness of LLMs in complex video understanding tasks. ", "page_idx": 5}, {"type": "text", "text": "Discussion 1: The necessity of high-quality language supervision for video understanding. Recent video pre-training approaches like LongViVit [45] and InternVideo [64], despite training on million-level web video-text data, show inferior performance on EgoSchema evaluation. These results highlight the inefifcacy and inefifciency of conventional contrastive pre-training in understanding long-form videos, primarily due to noisy and simplistic language supervision. In contrast, our TOPA, trained on 721K Tideoswith high-quality language supervision, shows impressive results on EgoSchema. It indicates that, unlike image understanding which significantly beneftis from leveraging web language as supervision, video understanding may require more precise and accurate language supervision to better capture the complex visual dynamics. ", "page_idx": 5}, {"type": "table", "img_path": "5NMbQPY7Bn/tmp/4242cbecc40db85fe80b18216c74e1cb3f477582de417c38249a5f102c3b3d43.jpg", "table_caption": ["Table 1: Zero-shot results on EgoSchema [39] full set. Methods that leverage closed-source LLMs are marked in gray. $\\dagger$ denotes the model is trained with in-domain egocentric videos from Ego4D [15]. \u2217denotes results on EgoSchema subset. Results of InternVideo and FrozenBiLM are sourced from [39]. Results of SeViLA are sourced from [45]. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Discussion 2: Video agents versus end-to-end video-LLM modeling. Video agents have shown impressive results on the EgoSchema benchmark, aided by advanced LLMs and VLMs. However, a significant limitation of these approaches is their heavy reliance on the powerful LLMs. For example, the accuracy of Vamos drops by $-11.6\\%$ when the GPT-4 is replaced with Llama2-13B, largely falling behind the performance of the TOPA-Llama2-13B model. The reliance on powerful closed-source LLMs restricts its application fields and introduces external overheads. Moreover, video agents make decisions based on the language format clues collected by VLMs. Converting the video content into language clues may lead to a limited upper bound compared to end-to-end modeling. Additionally, the inference speed of these approaches is another concern, since it involves multiple interactions with both VLMs and LLMs. In contrast, end-to-end video-LLM models, which condense the video into a sequence of embeddings as the input of LLM, are more efifcient. ", "page_idx": 6}, {"type": "text", "text": "4.1.2 Zero-shot Results on NExT-QA, STAR and TVQA ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Table 2 shows the multi-choice video QA results across various benchmarks. TOPA achieves impressive performance on the TVQA and EgoSchema benchmarks, significantly outperforming previous video pre-training models and image-to-video adaptation approaches. This indicates that our TOPA framework effectively enables LLMs to handle video input, despite not being pre-trained on real videos. However, for the NeXT-QA and STAR benchmarks, TOPA underperforms compared to SeViLA and IG-VLM. A major reason is that these benchmarks involve many fine-grained visual questions, including those about object locations and relationships. SeViLA and IG-VLM, beneftiing from the advanced image-understanding capabilities of pre-trained VLMs such as LLaVA, excel in answering these fine-grained visual questions. In contrast, our TOPA framework primarily focuses on high-level semantic alignment. Moreover, during zero-shot inference, we project the visual features into the text feature space to bridge the modality gap, as described in Eq. 2. This cross-modal semantic projection process tends to overlook fine-grained visual details, such as object locations, which leads to inferior performance on the STAR benchmark. We provide extensive qualitative results to illustrate TOPA\u2019s advantages and limitations across various video understanding tasks in Appendix A.3. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "table", "img_path": "5NMbQPY7Bn/tmp/cf0d5b2c89388b9bff70d1f2cf019807f0d8ac19de04ed9729d99f4395b00950.jpg", "table_caption": ["Table 2: Zero-shot results on multi-choice video QA benchmarks. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.1.3 Results on MVBench ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "MVBench [29] is a recent video-language understanding benchmark that covers 20 challenging video tasks, regrouped from existing video-language benchmarks. Table 3 shows the results. TOPA demonstrates impressive results compared to previous image MLLM and video MLLM. It excels particularly in tasks requiring high-level video-language understanding, such as Scene Transition (ST), Episodic Reasoning (ER), and Unexpected Action (UA). TOPA Surprisingly excels in the Action Localization (AL) task, which requires identifying the moment an action occurs. This indicates that the text-only pre-alignment enables the LLM to understand temporal visual sequences. However, TOPA struggles with tasks that demand fine-grained visual understanding, such as Moving Direction (MR), Action Antonym (AA), and Object Shuflfe (OS). A common challenge in these tasks is the requirement for detailed visual understanding. For example, Action Antonym involves identifying the direction of an action, while Object Shuflfe involves locating objects. TOPA struggles in these fine-grained visual tasks since it is trained with CLIP text features. The modality gap between CLIP text features and image features hinders TOPA from capturing visual details. Further video instruction tuning might address this limitation, which we leave for future work. We provide qualitative results in Appendix A.3 to illustrate TOPA\u2019s advantages and limitations on various video understanding tasks. ", "page_idx": 7}, {"type": "text", "text": "Table 3: Evaluation results on MVBench. The results of other approaches are sourced from [29]. We gray out the results of VideoChat2 since it utilizes extensive annotated downstream video data. ", "page_idx": 7}, {"type": "table", "img_path": "5NMbQPY7Bn/tmp/d4af177623b8878c97bb7b93a1008c997951ac5730cb60314d3797536759081b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "5NMbQPY7Bn/tmp/21d499df72573a266c9d24d94bc81e87ab336153f4d3c48f6570bbb57a1b1f7d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.2 Supervised Finetuning ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we further finetune the pre-aligned TOPA models to study the benefits of TOPA for downstream supervised learning. During finetuning, TOPA directly takes the video feature as input without the cross-modal projection. More finetuning details for each dataset are provided in Appendix E.2. Table 4 shows the finetuning results on multi-choice video QA dataset. For comparison, we include baseline models without text-only pretraining. Our text-only pre-alignment consistently improves the performance across three benchmarks. Notably, TOPA-Llama2-7B achieves $67.1\\%$ accuracy on TVQA, outperforming other approaches by a large margin. These results suggest that our text-only pre-alignment, even without training on real videos, has a similar effect to conventional video-language pre-training. ", "page_idx": 8}, {"type": "table", "img_path": "5NMbQPY7Bn/tmp/051d9e5724216526c5ce2a29f1e74d76c8fa4a43f771383919050ee3cc37a070.jpg", "table_caption": ["Table 4: Finetuning results on NExT-QA, STAR and TVQA. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Data-efifcient finetuning. Figure 3 shows the results of finetuning LLMs with various ratios of training data. TOPA trained with $10\\%$ data achieves $64.7\\%$ Top 1 accuracy on NeXT-QA benchmark, significantly outperforming the baseline that without text-only pre-alignment. Besides, when trained with less than $20\\%$ data, the baseline model even performs worse than TOPA-zeroshot on NeXT-QA and TVQA, clearly demonstrating the effectiveness of TOPA in limited annotated data scenarios. ", "page_idx": 8}, {"type": "image", "img_path": "5NMbQPY7Bn/tmp/057473296f407ba2b3e18fe38d8db5b1b45837ce48c3c803e8f5f78ea91d58e7.jpg", "img_caption": ["Figure 3: Results of finetuning TOPA with various ratios of training data. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.3 Video Captioning ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Results on zero-shot video captioning. We further perform zero-shot video captioning on MSRVTT [75] and VATEX [62]. As shown in Table 5, TOPA largely outperforms previous text-only approaches like Decap which is trained on captions sourced from CC3M [53]. TOPA even outperforms the video-text pre-training approaches like VideoCoCa, which is pre-trained on millions of videos-text data, demonstrating that TOPA is an efifcient and effective framework for video-LLM alignment. ", "page_idx": 8}, {"type": "text", "text": "Table 5: Zero-shot video captioning results. We report CIDEr score for all benchmarks. VT denotes $\\langle{\\nu}i d e o\\;c l i p,t e x t\\rangle$ pairs, $I T$ denotes $\\langle i m a g e,t e x t\\rangle$ pairs, and $W P$ denotes webpages consisting of interleaved image and text data. ", "page_idx": 9}, {"type": "table", "img_path": "5NMbQPY7Bn/tmp/e596cafdc699dce6cad3a68665b3aa503095908f3b71c379bd6db573a99b47e2.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "4.4 Ablations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "LLM prior in video-language understanding. To investigate the impact of LLM prior in multichoice video QA, we conduct experiments on EgoSchema with the blind setting, where only the questions and choices are provided to the LLM. Table 6 shows the results. Bard and GPT-4-Turbo achieve $33.2\\%$ and $30.8\\%$ accuracy, respectively. Gemini-Pro-1.0 reaches $38.2\\%$ accuracy. These blind results of advanced LLMs suggest that in some video QA cases, LLMs can accurately choose the correct answer solely based on the question and choices, without visual input. However, the blind performance of Llama2-7B and Llama2-13B is inferior, potentially due to their smaller model size. After training on the TextVid dataset, TOPA-Llama2-13B achieves a blind accuracy of $37.5\\%$ (or $+11.7\\%)$ , closely approaching that of Gemini-Pro-1.0 model. These results suggest that text-only pre-alignment can effectively prepare LLMs for downstream video-language tasks by leveraging specialized text-only tasks, even in complex scenarios where the original LLMs are limited. ", "page_idx": 9}, {"type": "table", "img_path": "5NMbQPY7Bn/tmp/059284b5c8490cb252e83996f85a84bfc4a99200697643fe2f222a17f6153443.jpg", "table_caption": ["Table 6: Blind results on EgoSchema. $\\dagger$ denotes results sourced from [4]. "], "table_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "5NMbQPY7Bn/tmp/bee67e003d683f6924ac2f24046573ad51d25da34b8f787611893cb58895d1a4.jpg", "table_caption": ["Table 7: Ablation on video frames. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "The impact of video frames. To better investigate TOPA\u2019s capability in understanding temporal dynamics of real videos, we conduct experiments with different number of frames. Table 7 shows the results. Multiple frames input consistently enhances performance on NeXT-QA and EgoSchema for both TOPA-Llama2-7B and TOPA-Llama2-13B. This indicates that the text-only pre-alignment effectively enables the LLM to handle multiple video frames, despite not being trained on real videos. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we introduce TOPA, a text-only pre-alignment framework designed for aligning LLMs with video modality without requiring training on real videos. TOPA has demonstrated remarkable performance on the recent, challenging long-form video understanding benchmark, i.e., EgoSchema, showcasing that a text-only approach is effective in capturing the dynamics of long-form videos. Our approach, which includes data generation and text-only pre-alignment, has potential applications across various vision-language tasks where obtaining paired vision-language data is dififcult. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was supported by National Key R&D Program of China (No. 2023YFC3305600), the National Natural Science Foundation of China (U2336212), the Fundamental Research Funds for the Zhejiang Provincial Universities (226-2024-00208), Lu\u2019s Graduate Education International Exchange Foundation and the National Research Foundation, Singapore under its Strategic Capability Research Centres Funding Initiative. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not reflect the views of National Research Foundation, Singapore. The computational work was partially performed on resources of the National Supercomputing Centre, Singapore (https://www.nscc.sg). ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35:23716\u201323736, 2022.   \n[2] R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos, S. Shakeri, E. Taropa, P. Bailey, Z. Chen, et al. PaLM 2 technical report. arXiv preprint arXiv:2305.10403, 2023. [3] M. Bain, A. Nagrani, G. Varol, and A. Zisserman. Frozen in time: A joint video and image encoder for end-to-end retrieval. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1728\u20131738, 2021. [4] I. Balazevic, Y. Shi, P. Papalampidi, R. Chaabouni, S. Koppula, and O. J. Henaf.f Memory consolidation enables long-context video understanding. In Forty-first International Conference on Machine Learning, 2024. [5] T. Brooks, A. Holynski, and A. A. Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18392\u201318402, 2023. [6] S. Changpinyo, P. Sharma, N. Ding, and R. Soricut. Conceptual $12\\mathrm{m}$ : Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 3558\u20133568, 2021. [7] G. Chen, Y.-D. Zheng, J. Wang, J. Xu, Y. Huang, J. Pan, Y. Wang, Y. Wang, Y. Qiao, T. Lu, et al. VideoLLM: Modeling video sequence with large language models. arXiv preprint arXiv:2305.13292, 2023. [8] X. Chen, X. Wang, S. Changpinyo, A. Piergiovanni, P. Padlewski, D. Salz, S. Goodman, A. Grycner, B. Mustafa, L. Beyer, A. Kolesnikov, J. Puigcerver, N. Ding, K. Rong, H. Akbari, G. Mishra, L. Xue, A. V. Thapliyal, J. Bradbury, W. Kuo, M. Seyedhosseini, C. Jia, B. K. Ayan, C. R. Ruiz, A. P. Steiner, A. Angelova, X. Zhai, N. Houlsby, and R. Soricut. PaLI: A jointly-scaled multilingual language-image model. In The Eleventh International Conference on Learning Representations, 2023. [9] R. Choudhury, K. Niinuma, K. M. Kitani, and L. A. Jeni. Zero-shot video question answering with procedural programs. arXiv preprint arXiv:2312.00937, 2023.   \n[10] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, Y. Li, X. Wang, M. Dehghani, S. Brahma, et al. Scaling instruction-finetuned language models. Journal of Machine Learning Research, 25(70):1\u201353, 2024.   \n[11] W. Dai, J. Li, D. Li, A. M. H. Tiong, J. Zhao, W. Wang, B. Li, P. N. Fung, and S. Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning. Advances in Neural Information Processing Systems, 36, 2023.   \n[12] L. Fan, D. Krishnan, P. Isola, D. Katabi, and Y. Tian. Improving clip training with language rewrites. Advances in Neural Information Processing Systems, 36, 2023.   \n[13] Y. Fan, X. Ma, R. Wu, Y. Du, J. Li, Z. Gao, and Q. Li. Videoagent: A memory-augmented multimodal agent for video understanding. In European Conference on Computer Vision, volume 15080 of Lecture Notes in Computer Science, pages 75\u201392. Springer, 2025.   \n[14] J. Gao, C. Sun, Z. Yang, and R. Nevatia. Tall: Temporal activity localization via language query. In Proceedings of the IEEE international conference on computer vision, pages 5267\u20135275, 2017.   \n[15] K. Grauman, A. Westbury, E. Byrne, Z. Chavis, A. Furnari, R. Girdhar, J. Hamburger, H. Jiang, M. Liu, X. Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18995\u201319012, 2022.   \n[16] S. Gu, C. Clark, and A. Kembhavi. I can\u2019t believe there\u2019s no images! learning visual tasks using only language supervision. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2672\u20132683, 2023.   \n[17] T. Han, W. Xie, and A. Zisserman. Temporal alignment networks for long-term video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2906\u20132916, 2022.   \n[18] Q. Huang, Y. Xiong, A. Rao, J. Wang, and D. Lin. MovieNet: A holistic dataset for movie understanding. In European Conference Computer Vision, volume 12357 of Lecture Notes in Computer Science, pages 709\u2013727. Springer, 2020.   \n[19] C. Jia, Y. Yang, Y. Xia, Y.-T. Chen, Z. Parekh, H. Pham, Q. Le, Y.-H. Sung, Z. Li, and T. Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International conference on machine learning, pages 4904\u20134916. PMLR, 2021.   \n[20] A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bamford, D. S. Chaplot, D. d. l. Casas, E. B. Hanna, F. Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024.   \n[21] K. Kahatapitiya, K. Ranasinghe, J. Park, and M. S. Ryoo. Language repository for long video understanding. arXiv preprint arXiv:2403.14622, 2024.   \n[22] W. Kim, C. Choi, W. Lee, and W. Rhee. An image grid can be worth a video: Zero-shot video question answering using a vlm. arXiv preprint arXiv:2403.18406, 2024.   \n[23] D. Ko, J. S. Lee, W.-Y. Kang, B. Roh, and H. J. Kim. Large language models are temporal and causal reasoners for video question answering. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023.   \n[24] J. Lei, L. Yu, M. Bansal, and T. L. Berg. Tvqa: Localized, compositional video question answering. In EMNLP, 2018.   \n[25] B. Li, Y. Zhang, L. Chen, J. Wang, F. Pu, J. Yang, C. Li, and Z. Liu. Mimic-it: Multi-modal in-context instruction tuning. arXiv preprint arXiv:2306.05425, 2023.   \n[26] J. Li, D. Li, C. Xiong, and S. Hoi. BLIP: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International conference on machine learning, pages 12888\u201312900. PMLR, 2022.   \n[27] J. Li, D. Li, S. Savarese, and S. Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pages 19730\u201319742. PMLR, 2023.   \n[28] K. Li, Y. He, Y. Wang, Y. Li, W. Wang, P. Luo, Y. Wang, L. Wang, and Y. Qiao. VideoChat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355, 2023.   \n[29] K. Li, Y. Wang, Y. He, Y. Li, Y. Wang, Y. Liu, Z. Wang, J. Xu, G. Chen, P. Lou, L. Wang, and Y. Qiao. MVBench: A comprehensive multi-modal video understanding benchmark. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22195\u201322206, 2024.   \n[30] W. Li, L. Zhu, L. Wen, and Y. Yang. Decap: Decoding clip latents for zero-shot captioning via text-only training. In The Eleventh International Conference on Learning Representations, 2023.   \n[31] V. W. Liang, Y. Zhang, Y. Kwon, S. Yeung, and J. Y. Zou. Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning. Advances in Neural Information Processing Systems, 35:17612\u201317625, 2022.   \n[32] B. Lin, B. Zhu, Y. Ye, M. Ning, P. Jin, and L. Yuan. Video-llava: Learning united visual representation by alignment before projection. arXiv preprint arXiv:2311.10122, 2023.   \n[33] Y. Lin, J. Zhang, Z. Huang, J. Liu, Z. Wen, and X. Peng. Multi-granularity correspondence learning from long-term noisy videos. In Proceedings of the International Conference on Learning Representations, May 2024.   \n[34] H. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2023.   \n[35] J. Liu, A. Shahroudy, M. Perez, G. Wang, L.-Y. Duan, and A. C. Kot. Ntu rgb $^+$ d 120: A large-scale benchmark for 3d human activity understanding. IEEE transactions on pattern analysis and machine intelligence, 42(10):2684\u20132701, 2019.   \n[36] I. Loshchilov and F. Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019.   \n[37] R. Luo, Z. Zhao, M. Yang, J. Dong, M. Qiu, P. Lu, T. Wang, and Z. Wei. Valley: Video assistant with large language model enhanced ability. arXiv preprint arXiv:2306.07207, 2023.   \n[38] M. Maaz, H. Rasheed, S. Khan, and F. S. Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. arXiv preprint arXiv:2306.05424, 2023.   \n[39] K. Mangalam, R. Akshulakov, and J. Malik. Egoschema: A diagnostic benchmark for very long-form video language understanding. Advances in Neural Information Processing Systems, 36, 2023.   \n[40] A. Miech, D. Zhukov, J.-B. Alayrac, M. Tapaswi, I. Laptev, and J. Sivic. Howto100m: Learning a text-video embedding by watching hundred million narrated video clips. In Proceedings of the IEEE/CVF international conference on computer vision, pages 2630\u20132640, 2019.   \n[41] G. A. Miller. WordNet: a lexical database for english. Communications of the ACM, 1995.   \n[42] J. Min, S. Buch, A. Nagrani, M. Cho, and C. Schmid. Morevqa: Exploring modular reasoning models for video question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024.   \n[43] M. Monfort, A. Andonian, B. Zhou, K. Ramakrishnan, S. A. Bargal, T. Yan, L. Brown, Q. Fan, D. Gutfreund, C. Vondrick, et al. Moments in time dataset: one million videos for event understanding. IEEE transactions on pattern analysis and machine intelligence, 42(2):502\u2013508, 2019.   \n[44] D. Nukrai, R. Mokady, and A. Globerson. Text-only training for image captioning using noise-injected clip. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 4055\u20134063, 2022.   \n[45] P. Papalampidi, S. Koppula, S. Pathak, J. Chiu, J. Heyward, V. Patraucean, J. Shen, A. Miech, A. Zisserman, and A. Nematzdeh. A simple recipe for contrastively pre-training video-first encoders beyond 16 frames. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14386\u201314397, 2024.   \n[46] V. Patraucean, L. Smaira, A. Gupta, A. Recasens, L. Markeeva, D. Banarse, S. Koppula, M. Malinowski, Y. Yang, C. Doersch, et al. Perception test: A diagnostic benchmark for multimodal video models. Advances in Neural Information Processing Systems, 36, 2023.   \n[47] L. Qian, J. Li, Y. Wu, Y. Ye, H. Fei, T.-S. Chua, Y. Zhuang, and S. Tang. Momentor: Advancing video large language model with fine-grained temporal reasoning. In Forty-first International Conference on Machine Learning, 2024.   \n[48] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.   \n[49] K. Ranasinghe, X. Li, K. Kahatapitiya, and M. S. Ryoo. Understanding long videos in one multimodal language model pass. arXiv preprint arXiv:2403.16998, 2024.   \n[50] M. Reid, N. Savinov, D. Teplyashin, D. Lepikhin, T. Lillicrap, J.-b. Alayrac, R. Soricut, A. Lazaridou, O. Firat, J. Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024.   \n[51] C. Schuhmann, R. Kaczmarczyk, A. Komatsuzaki, A. Katta, R. Vencu, R. Beaumont, J. Jitsev, T. Coombes, and C. Mullis. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. In NeurIPS Workshop Datacentric AI, 2021.   \n[52] C. Schuhmann, R. Beaumont, R. Vencu, C. Gordon, R. Wightman, M. Cherti, T. Coombes, A. Katta, C. Mullis, M. Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35: 25278\u201325294, 2022.   \n[53] P. Sharma, N. Ding, S. Goodman, and R. Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2018.   \n[54] E. Song, W. Chai, G. Wang, Y. Zhang, H. Zhou, F. Wu, X. Guo, T. Ye, Y. Lu, J.-N. Hwang, et al. MovieChat: From dense token to sparse memory for long video understanding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 18221\u201318232, 2024.   \n[55] D. Sur\u00eds, S. Menon, and C. Vondrick. Vipergpt: Visual inference via python execution for reasoning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11888\u201311898, 2023.   \n[56] G. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.   \n[57] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \n[58] R. Vedantam, C. Lawrence Zitnick, and D. Parikh. Cider: Consensus-based image description evaluation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4566\u20134575, 2015.   \n[59] J. Wang, D. Chen, C. Luo, X. Dai, L. Yuan, Z. Wu, and Y.-G. Jiang. Chatvideo: A tracklet-centric multimodal and versatile video understanding system. arXiv preprint arXiv:2304.14407, 2023.   \n[60] S. Wang, Q. Zhao, M. Q. Do, N. Agarwal, K. Lee, and C. Sun. Vamos: Versatile action models for video understanding. arXiv preprint arXiv:2311.13627, 2023.   \n[61] W. Wang, Z. He, W. Hong, Y. Cheng, X. Zhang, J. Qi, S. Huang, B. Xu, Y. Dong, M. Ding, et al. Lvbench: An extreme long video understanding benchmark. arXiv preprint arXiv:2406.08035, 2024.   \n[62] X. Wang, J. Wu, J. Chen, L. Li, Y.-F. Wang, and W. Y. Wang. Vatex: A large-scale, highquality multilingual dataset for video-and-language research. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4581\u20134591, 2019.   \n[63] X. Wang, Y. Zhang, O. Zohar, and S. Yeung-Levy. Videoagent: Long-form video understanding with large language model as agent. In European Conference on Computer Vision (ECCV), 2024.   \n[64] Y. Wang, K. Li, Y. Li, Y. He, B. Huang, Z. Zhao, H. Zhang, J. Xu, Y. Liu, Z. Wang, et al. Internvideo: General video foundation models via generative and discriminative learning. arXiv preprint arXiv:2212.03191, 2022.   \n[65] Y. Wang, Y. He, Y. Li, K. Li, J. Yu, X. Ma, X. Li, G. Chen, X. Chen, Y. Wang, P. Luo, Z. Liu, Y. Wang, L. Wang, and Y. Qiao. Internvid: A large-scale video-text dataset for multimodal understanding and generation. In The Twelfth International Conference on Learning Representations, 2024.   \n[66] Y. Wang, K. Li, X. Li, J. Yu, Y. He, G. Chen, B. Pei, R. Zheng, J. Xu, Z. Wang, et al. Internvideo2: Scaling video foundation models for multimodal video understanding. CoRR, 2024.   \n[67] Y. Wang, Y. Yang, and M. Ren. Lifelongmemory: Leveraging llms for answering queries in long-form egocentric videos. arXiv preprint arXiv:2312.05269, 2024.   \n[68] Z. Wang, A. Blume, S. Li, G. Liu, J. Cho, Z. Tang, M. Bansal, and H. Ji. Paxion: Patching action knowledge in video-language foundation models. Advances in Neural Information Processing Systems, 36, 2023.   \n[69] Z. Wang, J. Li, Y. Hong, Y. Wang, Q. Wu, M. Bansal, S. Gould, H. Tan, and Y. Qiao. Scaling data generation in vision-and-language navigation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 12009\u201312020, 2023.   \n[70] B. Wu, S. Yu, Z. Chen, J. B. Tenenbaum, and C. Gan. Star: A benchmark for situated reasoning in real-world videos. In Thirty-ffith conference on neural information processing systems datasets and benchmarks track (Round 2), 2021.   \n[71] H. Wu, D. Li, B. Chen, and J. Li. Longvideobench: A benchmark for long-context interleaved video-language understanding, 2024.   \n[72] J. Xiao, X. Shang, A. Yao, and T.-S. Chua. NExT-QA: Next phase of question-answering to explaining temporal actions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9777\u20139786, 2021.   \n[73] B. Xie, S. Zhang, Z. Zhou, B. Li, Y. Zhang, J. Hessel, J. Yang, and Z. Liu. Funqa: Towards surprising video comprehension. In European Conference on Computer Vision, pages 39\u201357. Springer, 2025.   \n[74] H. Xu, Q. Ye, M. Yan, Y. Shi, J. Ye, Y. Xu, C. Li, B. Bi, Q. Qian, W. Wang, et al. mPLUG-2: A Modularized Multi-modal Foundation Model Across Text, Image and Video. In ICML, 2023.   \n[75] J. Xu, T. Mei, T. Yao, and Y. Rui. MSR-VTT: A large video description dataset for bridging video and language. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5288\u20135296, 2016.   \n[76] S. Yan, T. Zhu, Z. Wang, Y. Cao, M. Zhang, S. Ghosh, Y. Wu, and J. Yu. VideoCoCa: Video-text modeling with zero-shot transfer from contrastive captioners. arXiv preprint arXiv:2212.04979, 2022.   \n[77] A. Yang, A. Miech, J. Sivic, I. Laptev, and C. Schmid. Zero-shot video question answering via frozen bidirectional language models. In NeurIPS, 2022.   \n[78] Z. Yang, G. Chen, X. Li, W. Wang, and Y. Yang. Doraemongpt: Toward understanding dynamic scenes with large language models (exemplified as a video agent). In Forty-first International Conference on Machine Learning, 2024.   \n[79] Q. Ye, H. Xu, G. Xu, J. Ye, M. Yan, Y. Zhou, J. Wang, A. Hu, P. Shi, Y. Shi, et al. mplugowl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023.   \n[80] K. Yi, C. Gan, Y. Li, P. Kohli, J. Wu, A. Torralba, and J. B. Tenenbaum. Clevrer: Collision events for video representation and reasoning. In International Conference on Learning Representations, 2019.   \n[81] S. Yu, J. Cho, P. Yadav, and M. Bansal. Self-chained image-language model for video localization and question answering. Advances in Neural Information Processing Systems, 36, 2023.   \n[82] C. Zhang, T. Lu, M. M. Islam, Z. Wang, S. Yu, M. Bansal, and G. Bertasius. A simple llm framework for long-range video question-answering. arXiv preprint arXiv:2312.17235, 2023.   \n[83] H. Zhang, X. Li, and L. Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 543\u2013553, 2023.   \n[84] R. Zhang, J. Han, C. Liu, A. Zhou, P. Lu, Y. Qiao, H. Li, and P. Gao. LLaMA-adapter: Efifcient fine-tuning of large language models with zero-initialized attention. In The Twelfth International Conference on Learning Representations, 2024.   \n[85] Y. Zhang, E. Sui, and S. Yeung-Levy. Connect, collapse, corrupt: Learning cross-modal tasks with uni-modal data. In International Conference on Learning Representations (ICLR), 2024.   \n[86] L. Zhao, N. B. Gundavarapu, L. Yuan, H. Zhou, S. Yan, J. J. Sun, L. Friedman, R. Qian, T. Weyand, Y. Zhao, et al. Videoprism: A foundational visual encoder for video understanding. In Forty-first International Conference on Machine Learning, 2024.   \n[87] Y. Zhao, I. Misra, P. Kr\u00e4henb\u00fchl, and R. Girdhar. Learning video representations from large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6586\u20136597, 2023.   \n[88] Y. Zhao, L. Zhao, X. Zhou, J. Wu, C.-T. Chu, H. Miao, F. Schrof,f H. Adam, T. Liu, B. Gong, et al. Distilling vision-language models on millions of videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13106\u201313116, 2024.   \n[89] D. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. In The Twelfth International Conference on Learning Representations, 2023.   \n[90] L. Zhu and Y. Yang. Actbert: Learning global-local video-text representations. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8746\u20138755, 2020. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In Appendix A, we provide additional experiments and analysis. ", "page_idx": 16}, {"type": "text", "text": "\u2022 In Appendix A.1, we further discuss the Multi-choice QA task and study the impact of the multi-choice Tideo QA pre-training.   \n\u2022 In Appendix A.2, we study the impact of cross-modal projection (Eq. 2).   \n\u2022 In Appendix A.3, we provide extensive qualitative results to illustrate TOPA\u2019s advantages and limitations across various video understanding tasks. Appendix B: The limitations of TOPA.   \nAppendix C: The broader impact of TOPA.   \nAppendix D: The details of proposed TextVid dataset.   \nAppendix E.1, The details of benchmarks.   \nAppendix E.2, The training details of TOPA.   \nAppendix F: The prompts used in this paper.   \nAppendix G: The licenses of datasets, codes and models used in this paper.   \nAppendix H: Examples from TextVid. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "A Additional Experiments ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "A.1 Further Discussion on Multi-Choice Video QA Task ", "text_level": 1, "page_idx": 17}, {"type": "table", "img_path": "5NMbQPY7Bn/tmp/cd8ed2aa951dcd7579ef6fff40a0652e605a7a2a8170a038ca7b6da081006b98.jpg", "table_caption": ["Table 8: Multi-choice video QA on EgoSchema subset and full set. \u201cGap\u201d refers to the difference in performance between the subset and the full set "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "A significant advantage of the text-only framework is that we can utilize the LLM to automatically generate diverse language-based supervisions as needed, such as the multi-choice QA pairs. To explore the impact of the multi-choice QA training tasks, we conduct an ablation study as shown in Table 8. We would like to first introduce the different evaluation modes for multi-choice video QA tasks: (1) LLM Selection: Asking the LLM to predict the correct answer given the video-question-choices. (2) LLM Logits: Given the video and question as LLM context, we calculate the logits for each choice by averaging the logits of all words within the choice. The choice with higher logit tends to match the video-question context better and is thus selected as the predicted answer. (3) Similarity Comparison [45, 4]: Mapping the multiple question-choice pairs and video to a common feature space and calculating the similarity between the video and each question-choice. ", "page_idx": 17}, {"type": "text", "text": "The performance gap between the EgoSchema subset and full set. Previous work [4, 45] highlights a huge performance gap between the subset and the full set of EgoSchema as shown in Table 8. While concurrent work [21, 49] introduces log-likelihood based approaches for LLM inference, which significantly improve the performance on EgoSchema subset, the issue of the performance gap still persists. In this paper, we observe that such a performance gap phenomenon also occurs in approaches based on LLM logits. However, but it diminishes or even disappears in methods employing LLM selection. We find that this phenomenon may be attributed to differences in the linguistic structures of the choices, as shown below. The choices in the subset often differ in several key works like \u201ccreate\u201d, \u201crepair\u201d and \u201cclean\u201d. The similarity or logit can effectively identify this keyword-level difference to select a more appropriate choice. Conversely, the choices in the full set display more substantial linguistic differences. These variations introduce significant language biases, i.e., some sentences naturally receive higher logits in LLM, complicating the reliance on similarity or logit for choice selection. In contrast, LLM selection methods take all the choices within the context, allowing the LLM to leverage its robust contextual understanding to select the correct choice. ", "page_idx": 17}, {"type": "text", "text": "Question-Choices examples from subset: Q: Can you summarize the primary objective and the steps the person took throughout the video to achieve it? ensure your answer captures the essence of the video without listing all actions. A: The main aim of the person\u2019s primary objective was to create and build a new, sturdy wooden bench. B: The primary objective for the person was to thoroughly repair and restore the wooden bench. C: The person\u2019s primary objective was to thoroughly clean and sanitize the wooden bench\u2019s surface. ", "page_idx": 17}, {"type": "text", "text": "LLM for multi-choice QA. In Table 8, we observe a notable phenomenon where the TOPA models achieve impressive results on the subset with the logits evaluation mode. TOPA-LLama2-13B achieves $67.5\\%$ top1 accuracy, surpassing GPT-4-based video agents. However, when evaluated with the multi-choice selection mode, the performance of the subset declines to $51.2\\%$ , but the performance of the full set increases from $41.6\\%$ to $51.0\\%$ . These results suggest that while the LLM is capable of selecting the answer from multiple choices, it is less sensitive to the keywords within those choices. In contrast, the logit-based approach is sensitive to the keywords but has dififculty with complex sentence understanding. ", "page_idx": 18}, {"type": "text", "text": "The impact of the Multi-Choice Tideo QA pre-training. In Table 8, we report the results of TOPA without the multi-choice Tideo QA task, i.e., trained with Tideo summarization and Tideo QA tasks. In this case, we find that TOPA-LLama2-7B maintains similar performance when evaluated with the logit mode. However, there is a significant performance drop when evaluated with the multi-choice selection mode. This result suggests that while the LLM is adapted to process video inputs, its capability is somewhat constrained and can not extend to more complex video-language tasks beyond the pre-training tasks. This finding highlights the advantage of our text-only data generation and text-only pre-alignment framework, which enable us to develop a variety of pre-alignment tasks to better equip the LLM for general video-language tasks such as dense captioning, multi-choice video QA, and video chat. ", "page_idx": 18}, {"type": "text", "text": "A.2 The CLIP Modality Gap ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "TOPA is pretrained with CLIP text features while inferenced with CLIP image features. We employ a modality projection approach, i.e., Eq. 2, to bridge this CLIP modality gap during zero-shot inference. Table 9 shows the impact of Eq. 2. TOPA shows inferior results when directly taking the visual feature as input due to the modality gap problem. The projection approach effectively alleviates such a modality gap problem without additional training. ", "page_idx": 18}, {"type": "text", "text": "Table 9: Ablation on the modality projection (Eq. 2). Results on EgoSchema full set. ", "page_idx": 18}, {"type": "table", "img_path": "5NMbQPY7Bn/tmp/fb58a926db8340aa9c43382d4d4fb688932477c22f142aca59d559c10be2ca12.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "A.3 Qualitative Results and Analysis ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We present qualitative results to illustrate the capabilities and limitations of TOPA across various video understanding tasks. Figure 4 shows qualitative results on the NExT-QA validation set. Figure 5 shows qualitative results on the EgoSchema subset. Figure 6 - 9 shows qualitative results on 20 video understanding tasks from MVBench. ", "page_idx": 19}, {"type": "image", "img_path": "5NMbQPY7Bn/tmp/af3b946a79d5bd2e42eddfb03ee1dd3bba0de6618b8bbd7c22728a6662e159cd.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Question: How did the children fill up the bucket?   \nChoices:  (A) turn on the tap. (B) through straw. (C) person bringing food to baby mouth. (D) touch the top of bottle. (E) sit around on the floor. ", "page_idx": 19}, {"type": "text", "text": "Prediction: A Ground Truth: A ", "page_idx": 19}, {"type": "image", "img_path": "5NMbQPY7Bn/tmp/a36adf1e82aeef39d56d83753201e1ce3ffa852fb2f1c737f3aff2bb732d3450.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Question: What does the girl do as the dog sat next to her at the start?   \nChoices:  (A) look at the table.   (B) wave at it. (C) shave the dog. (D) play with toy. (E) caress the dog. Prediction: E Ground Truth: E Question: Why did the man lying down roll over on the floor near the end?   \nChoices:  (A) to move to the camera. (B) fall over. (C) laughing.   (D) baby crying. (E) let kids walk through. Prediction: A Ground Truth: A ", "page_idx": 19}, {"type": "image", "img_path": "5NMbQPY7Bn/tmp/acb33c4159c94236fed57f2594141b1a36b4f49453bc9eca89bfd7de25c8911d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "image", "img_path": "5NMbQPY7Bn/tmp/e23ba147f3821e33d1aed513be81e6c44d4270338f5370bd10d52f581282abd8.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Question: How many people are on stage? Choices:  (A) one.   (B) six.   (C) three.   (D) five.  (E) seven. Prediction: D Ground Truth: B ", "page_idx": 19}, {"type": "text", "text": "Figure 4: Qualitative results on NeXT-QA. TOPA effectively performs complex video understanding tasks. Additionally, a failure case is also shown in the figure, i.e., in the last sample, TOPA failed to accurately count the number of people. ", "page_idx": 19}, {"type": "image", "img_path": "5NMbQPY7Bn/tmp/5b54d5e688ecd3e000d25e171c2e6450783dcdfa148807faff54514934cf6e8c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Question: What is the overall process and purpose of the actions performed by the person in the video? ", "page_idx": 20}, {"type": "text", "text": "Choices: ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "(A) The person is molding bricks with mortar and wet clay.   \n(B) Currently, the person is diligently working on constructing a wall.   \n(C) Currently, artist the person is diligently working on creating a unique sculpture.   \n(D) Currently, young the person is actively playing and having fun with mud.   \n(E) The person is doing a science experiment. ", "page_idx": 20}, {"type": "text", "text": "Prediction: A Ground Truth: A ", "page_idx": 20}, {"type": "image", "img_path": "5NMbQPY7Bn/tmp/76e59aea5abde27addfc6a155011fc5342cd9ceef4d9cff25ca8daaf270bfe01.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Question: How does the person interact with the camera during the video and what might be the reason behind these adjustments? ", "page_idx": 20}, {"type": "text", "text": "Choices: ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "(A) Casually, the person adjusts the camera angle skillfully to achieve a more visually appealing view of herself.   \n(B) The person skillfully adjusts the camera angle to achieve a better, enhanced view of the entire room.   \n(C) Casually, the person adjusts the camera angle to get a more improved, better view of the entrance door.   \n(D) The person adjusts the camera to get a better view of the window.   \n(E) The person adjusts the camera to get a better view of the bed and the cloths. ", "page_idx": 20}, {"type": "text", "text": "Prediction: E Ground Truth: E ", "page_idx": 20}, {"type": "image", "img_path": "5NMbQPY7Bn/tmp/d72eb6e117ba70d626ff6ee37eb62324692083f631c8a92091dece90464affdc.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Question: Determine the primary purpose of the person's actions in the video, and explain the importance of the repetitive actions involved in this process. ", "page_idx": 20}, {"type": "text", "text": "Choices: ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "(A) In the kitchen, the person is diligently making a cup of coffee. the repetitive actions, or steps, are necessary to ensure that the coffee concoction is mixed evenly throughout.   \n(B) The person is stirring a pot of soup. the repetitive actions are necessary to ensure that the soup is cooked evenly. (C) The person is painting a piece of furniture. the repetitive actions are necessary to ensure that the paint is applied evenly. (D) The person is diligently washing dishes. the repetitive actions performed are extremely necessary to ensure that every single dish is thoroughly clean.   \n(E) Currently, the person is diligently brushing his teeth. these repetitive actions are crucial and necessary to effectively ensure that his teeth remain ultimately clean and healthy. ", "page_idx": 20}, {"type": "text", "text": "Prediction: C Ground Truth: C ", "page_idx": 20}, {"type": "text", "text": "Figure 5: EgoSchema presents unique challenges compared to previous video benchmarks. The questions in EgoSchema are complex and demand advanced video capabilities, encompassing both recognition and reasoning skills. ", "page_idx": 20}, {"type": "image", "img_path": "5NMbQPY7Bn/tmp/612741e1eee49ffbc8d454ae23789fe621afd94eadd818035182b3736c1d956d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Question: What happened after the person held the blanket? Choices: (A) Tidied up the table. (B) Took the dish. (C) Opened the window. (D) Took the pillow. ", "page_idx": 21}, {"type": "text", "text": "Prediction: A Ground Truth: A ", "page_idx": 21}, {"type": "text", "text": "Task: AS Action Sequence ", "page_idx": 21}, {"type": "image", "img_path": "5NMbQPY7Bn/tmp/25752e560c30f133bdcab783517b29cbb4a6819bb885c2438eb5effd0ab1deb6.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Question: What will the person do next? Choices: (A) Eat the medicine. (B) Take the shoe. (C) Take the clothes. (D) Throw the food. ", "page_idx": 21}, {"type": "text", "text": "Prediction: B Ground Truth: B ", "page_idx": 21}, {"type": "text", "text": "Task: AP Action Prediction ", "page_idx": 21}, {"type": "image", "img_path": "5NMbQPY7Bn/tmp/1c661bb7e7d2c03585704883aac2e0a32e34f4ac71fdf5568b3fb2e2b58b59d1.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Question: What activity does the video depict?   \nChoices:   \n(A) Catching something in the air and preventing it ascend (C) Throwing something in the air and letting it fall.   \nPrediction: A Ground Truth: C ", "page_idx": 21}, {"type": "text", "text": "Task: AA Action Antonym ", "page_idx": 21}, {"type": "image", "img_path": "5NMbQPY7Bn/tmp/25c43bfe4fdbfb162a0e246d958ac9d59c2d0fc74ff34ac6cfd630d1bcd6acba.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Question: Which one of these descriptions correctly matches the actions in the video? Choices: ", "page_idx": 21}, {"type": "text", "text": "(A) Descending (B) stacking (C) Flipping (D) loading Prediction: A Ground Truth: B ", "page_idx": 21}, {"type": "text", "text": "Task: FA Fine-grained Action ", "page_idx": 21}, {"type": "image", "img_path": "5NMbQPY7Bn/tmp/b9853f1f1089270ede3ceb577a0dc8dd6001ef47e42e4efa1498617b19eebe0d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Question: What unusual event takes place in the video during the magical segment? Choices: ", "page_idx": 21}, {"type": "text", "text": "(A) The basketball court transforms into a football field midway through the game.   \n(B) A small basketball transforms into a person when thrown at the hoop.   \n(C) A man playing basketball suddenly grows wings and starts flying.   \n(D) A gigantic basketball appears after a man throws a normal-sized one at the hoop.   \nPrediction: D Ground Truth: D ", "page_idx": 21}, {"type": "text", "text": "Task: UA Unexpected Action ", "page_idx": 21}, {"type": "text", "text": "Figure 6: Qualitative results on MVBench (Task 1-5). The tasks where TOPA performs well, average, or poorly are marked in green, blue, and red colors respectively. ", "page_idx": 21}, {"type": "image", "img_path": "5NMbQPY7Bn/tmp/acc427169412380e24fc1700f99561326d797c8a48d916158adf7cbc4492825c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Question: Are there any moving brown objects? Choices: (A) not sure (B) no (C) yes. Prediction: C Ground Truth: C ", "page_idx": 22}, {"type": "text", "text": "Task: OE Object Existence ", "page_idx": 22}, {"type": "image", "img_path": "5NMbQPY7Bn/tmp/62ac4f6f303e0da3b5126f796cdf14068a332c105afc6cb4ea5de4460f4aec90.jpg", "img_caption": [], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Question: Which object was put down by the person?   \nChoices:   \n(A) The laptop. (B) The dish. (C) The picture. (D) The food. Prediction: C Ground Truth: C ", "page_idx": 22}, {"type": "text", "text": "Task: OI Object Interaction ", "text_level": 1, "page_idx": 22}, {"type": "image", "img_path": "5NMbQPY7Bn/tmp/f4a51bbd5752fbe55eba7e4213173035246982f4a14f166708b06cb5c80fc9cb.jpg", "img_caption": [], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Question: The person uses multiple similar objects to play an occlusion game. Where is the hidden object at the end of the game from the person's point of view? ", "page_idx": 22}, {"type": "text", "text": "Choices: (A) Under the second object from the left. (B) Under the third object from the left. (C) Under the fourth object from the left. ", "page_idx": 22}, {"type": "text", "text": "Prediction: B Ground Truth: C ", "page_idx": 22}, {"type": "text", "text": "Task: OS Object Shuffle ", "page_idx": 22}, {"type": "image", "img_path": "5NMbQPY7Bn/tmp/b3a9b7b1ff8d8cf1f0f2078d927baa1fd555d6099d32147404506f49d02a45bf.jpg", "img_caption": [], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Question: Can you identify the direction of the red sphere's movement in the video? ", "page_idx": 22}, {"type": "text", "text": "Choices: (A) Down and to the right.   \n(B) Up and to the left.   \n(C) Up and to the right.   \n(D) The object is stationary. ", "page_idx": 22}, {"type": "text", "text": "Prediction: A Ground Truth: D ", "page_idx": 22}, {"type": "text", "text": "Task: MD Moving Direction ", "page_idx": 22}, {"type": "text", "text": "Question: During which part of the video does the action 'person drinking from the cup' occur? Choices: ", "page_idx": 22}, {"type": "image", "img_path": "5NMbQPY7Bn/tmp/5db2c5f3279a9b662400b7325d55d33b8141e7ab69c32aa4f0b7dbbfc81b5ebc.jpg", "img_caption": ["Figure 7: Qualitative results on MVBench (Task 6-10). "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "(A) In the middle of the video. (B) At the end of the video.   \n(C) At the beginning of the video. (D) Throughout the entire video. ", "page_idx": 22}, {"type": "text", "text": "Prediction: A Ground Truth: A ", "page_idx": 22}, {"type": "text", "text": "Action Localization ", "page_idx": 22}, {"type": "image", "img_path": "5NMbQPY7Bn/tmp/d1f4c892138e366651e7622e1a1bd564aa38e6c2d9a19c832892d55118a8c605.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Question: Which choice matches the scene changes in the video? Choices:   \n(A) From the grocery store to the park.   \n(B) From the classroom to the library.   \n(C) From the barbershop to the front of the glass window.   \n(D) From the office to the beach. ", "page_idx": 23}, {"type": "image", "img_path": "5NMbQPY7Bn/tmp/71f41b6a7b35d48e23e63b8482e25050c59549bf9930f53cd8571cf2f91b142b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "Prediction: C Ground Truth: C ", "page_idx": 23}, {"type": "text", "text": "Scene Transition ", "page_idx": 23}, {"type": "text", "text": "on at any point? ", "page_idx": 23}, {"type": "text", "text": "Choices:  (A) I don't know (B) yes (C) no ", "page_idx": 23}, {"type": "image", "img_path": "5NMbQPY7Bn/tmp/8e639a27d7be90303f51999e65b839d30659ffef22184899a6549998acc19aef.jpg", "img_caption": ["Figure 8: Qualitative results on MVBench (Task 11-15). "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Prediction: C Ground Truth: C ", "page_idx": 23}, {"type": "text", "text": "State Change ", "page_idx": 23}, {"type": "image", "img_path": "5NMbQPY7Bn/tmp/bddf87aa4bc8eb8a875c170979d035761a6ef90907207d1b69e8c81fc6d1996e.jpg", "img_caption": [], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Question: What is the action performed by the person in the video? Choices: (A) Pick up (B) drop (C) sit down (D) jump up ", "page_idx": 24}, {"type": "text", "text": "Prediction: A Ground Truth: A ", "page_idx": 24}, {"type": "text", "text": "Task: FP Fine-grained Pose ", "page_idx": 24}, {"type": "image", "img_path": "5NMbQPY7Bn/tmp/7f2cf782592c679e38ac74458cdf95a4d5821b15afbdd5a85cf25cb4c4ff5c89.jpg", "img_caption": [], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Question: What is the order of the letters at the end? ", "page_idx": 24}, {"type": "text", "text": "Choices: (A) kag (B) kav (C) bag ", "page_idx": 24}, {"type": "text", "text": "Prediction: A Ground Truth: C ", "page_idx": 24}, {"type": "text", "text": "Character Order ", "page_idx": 24}, {"type": "image", "img_path": "5NMbQPY7Bn/tmp/cc66b6d3abe7aaa52a3efe44779d2ab00bfdbd7cbb7eb644e8195715cd165532.jpg", "img_caption": [], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Question: This is a navigation video of an agent following instruction: \"Walk to the front of the fireplace.\" What is the next action it should take? ", "page_idx": 24}, {"type": "text", "text": "Choices: (A) Move forward (B) Turn left and move forward (C) Stop (D) Turn right and move forward Prediction: B Ground Truth: B ", "page_idx": 24}, {"type": "text", "text": "Task: EN Egocentric Navigation ", "page_idx": 24}, {"type": "image", "img_path": "5NMbQPY7Bn/tmp/c78d0594efe0c96706c1e03f00d161f76f0652cb68c1764006ce046ccb49c7f8.jpg", "img_caption": [], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Question: Why did Joey make a sign before Monica and Chandler are meant to come home? ", "page_idx": 24}, {"type": "text", "text": "Choices:   \n(A) To announce a sandwich eating contest (B) To welcome a new puppy   \n(C) To welcome the new baby (D) To welcome Phoebe   \n(E) To welcome Rachel   \nPrediction: C Ground Truth: C ", "page_idx": 24}, {"type": "text", "text": "Task: ER Episodic Reasoning ", "page_idx": 24}, {"type": "image", "img_path": "5NMbQPY7Bn/tmp/855c3711d8830d745f597edc2f6d865a964e85c6a142a3194b2f28ab09e14fdf.jpg", "img_caption": [], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Question: Without the gray object, which of the following will happen? Choices: ", "page_idx": 24}, {"type": "text", "text": "(A) The yellow cylinder collides with the blue object (B) The sphere collides with the blue object (C) The sphere and the rubber object collide ", "page_idx": 24}, {"type": "text", "text": "Task: CI Counterfactual Inference ", "page_idx": 24}, {"type": "text", "text": "Prediction: C Ground Truth: C ", "page_idx": 24}, {"type": "text", "text": "B Limitations. ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Modality gap in CLIP. Despite the fact that TOPA achieves impressive results, a significant limitation in TOPA is the gap between the CLIP text feature and CLIP image feature. On the one hand, we use the CLIP text feature for pre-alignment, while inference is with the CLIP visual feature. The modality gap makes the performance degrade, despite employing a modality projection mechanism to mitigate it. On the other hand, the CLIP text features cannot fully capture the fine-grained visual details present in actual images, such as object locations and relationships. This limitation causes TOPA to struggle in scenarios where questions involve detailed visual information, as shown in Appendix A.3. ", "page_idx": 25}, {"type": "text", "text": "Struggles in fine-grained visual understanding. In TOPA, we propose textual videos to mimic real videos. However, this approach primarily focuses on keyframes understanding, which is insufifcient for scenarios requiring the model to process hundreds of frames at high fps, such as action counting tasks. Besides, for the fine-grained action understanding scenarios, TOPA is unable to capture the fine-grained visual information. For example, in a scene where a person climbs a ladder, it is dififcult for TOPA to identify whether the person is going up or down due to the limited capability to capture detailed visual dynamics. Further enhancing TOPA with video instruction tuning might address these limitations which we leave for future work. ", "page_idx": 25}, {"type": "text", "text": "C Broader Impact ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Academic Impact. TOPA\u2019s methodology, which frees the need for costly video-text data collection and large-scale pre-training, lowers the barriers to entry for research and development in video-language understanding technologies. The text-only learning framework of TOPA may inspire researchers with limited resources to engage in cutting-edge multi-modal research, providing a more diverse range of perspectives and contributions to this field. ", "page_idx": 25}, {"type": "text", "text": "Social Impact. The ultimate objective of TOPA is to develop a general video-language understanding model. Its primary application enables users to extract information from long-form videos without the need for detailed viewing. Moreover, these capabilities for interpreting and managing video content could significantly enhance content moderation systems. Platforms hosting user-generated content could employ sophisticated video-language models to efifciently detect and mitigate the effects of inappropriate or harmful video content. ", "page_idx": 25}, {"type": "text", "text": "D The details of proposed TextVid dataset ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We utilize Gemini Pro 1.0 API for our data generation process. We prompt the LLM to create textual videos along with associated annotations. To ensure a diverse dataset that covers a wide of domains, we add condition prompts including different themes, video captions, video events, and the names of main objects. Specifically, we leverage video titles from Howto100M [40], video captions from WebVid2M [3], video events from Ego4D [15], and object from Wordnet [41] as conditions to generate diverse textual videos. For Ego4D condition, we ask the LLM to mimic an ego-centric video to further improve the diversity of the dataset. Table 10 compares vocabulary sizes. Figure 11 shows that Tideos generated under different conditions have different distributions. For each data generation, we prompt the LLM with the task prompt and one of the condition prompts as shown in Figure 13. The statistics TextVid are shown in Table 10. Additionally, we provide Wordcloud of TextVid in Figure 10. The examples of TextVid are shown in Appendix H. ", "page_idx": 25}, {"type": "table", "img_path": "5NMbQPY7Bn/tmp/90b1de177aff2a17803c51dc7110b0bac088127420e28fe5f7a6f9507ae74b50.jpg", "table_caption": ["Table 10: Statistics of TextVid. "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "Table 11: Vocabulary size of Tideos generated under different prompts. We randomly sampled 20,000 global captions from each type of Tideos for comparison. ", "page_idx": 26}, {"type": "table", "img_path": "5NMbQPY7Bn/tmp/4a1a5f8fc715d6fbda8dc440333b526e362bc5021e74151cd09933574f6b9b8e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "5NMbQPY7Bn/tmp/90d294396a8c7ad436fd2984f23a70dd896d12c9168a189daaa84b6ed8f11d2a.jpg", "img_caption": ["Figure 10: Wordcloud of TextVid. The frame caption (left) and the dense video caption (right). "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "5NMbQPY7Bn/tmp/16159a5f61ff8d5af9f0b0cf36f387001b37bcb48bbcc14432bf3bfe02bec9dd.jpg", "img_caption": ["Figure 11: Visualization of Tideo features generated from different type of prompts. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "E Experimental Setup ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "E.1 Benchmarks ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "EgoSchema [39] is a challenging long-form video understanding benchmark with 5000 multi-choice questions. The videos in EgoSchema are sourced from Ego-4D [15], with an average length of 3 minutes, distinct from previous benchmarks that focused on shorter, seconds-long videos. The ", "page_idx": 26}, {"type": "text", "text": "questions in EgoSchema are manually curated to demand long temporal reasoning. We report results on EgoSchema full set. ", "page_idx": 27}, {"type": "text", "text": "NExT-QA [72] is a multi-choice video QA benchmark for causal and temporal reasoning, including 5,440 natural videos. The average length of video is 44 seconds. We report results on NExT-QA validation set, which contains 570 videos and 5,000 multiple-choice questions. ", "page_idx": 27}, {"type": "text", "text": "STAR [70] is a benchmark for situated reasoning. It contains 22K video clips with an average length of 44 seconds. There are 4 different question types in STAR: Interaction (Int.), Sequence (Seq.), Prediction (Pre.), and Feasibility (Fea.). We report results on STAR validation set. ", "page_idx": 27}, {"type": "text", "text": "TVQA [24] is a benchmark containing 21k video clips with an average length of 76 seconds. We report results on TVQA validation set without subtitles. ", "page_idx": 27}, {"type": "text", "text": "MVbench [29] is a reorganized benchmark containing 20 video understanding tasks. These tasks are sourced from STAR [70], PAXION [68], MiT Vi [43], FunQA [73], Perception Test [46], Charades-STA [14], MovieNet [18], NTU RGB $+\\mathrm{D}$ [35], VLN-CE [69] and TVQA [24]. ", "page_idx": 27}, {"type": "text", "text": "E.2 The details of training and evaluation. ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "We leverage Llama2-7B, Llama2-13B [57] and Llama3-8B as the LLM backbone. Additionally, we employ the Llama-adapter [84] with an adaptation embedding length of 50 for efifcient finetuning. We utilize CLIP-ViT-L as the multimodal encoder. We employ a simple linear layer to project the CLIP feature into the LLM feature space. The CLIP model and LLM backbone are frozen. The projection layer and additional Llama-adapter are trainable. For text-only pre-alignment, we uniformly sample the Tideos into 10 frames. We train the model on a mixture of tasks comprising Tideo summarization, Tideo QA, multi-choice Tideo QA with the ratio of 1:1:2. TOPA-Llama2-7B and TOPA-Llama3-8B are trained on four 40G-A100 GPUs in one day. TOPA-Llama2-13B is trained in two days. For zero-shot inference, we construct a memory for cross-modal projection, consisting of 2M CLIP text features sampled from the frame captions in the TextVid dataset. We include the training details in Table 12. The actual learning rate is calculated by base lr $^*$ Effective Batchsize/256. ", "page_idx": 27}, {"type": "table", "img_path": "5NMbQPY7Bn/tmp/7d51b24bb6f7ab2e126d97fa5a1ab689ee3f9879b194250031663121fe6d4b60.jpg", "table_caption": ["Table 12: Training hyper-parameters. "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "F Prompts ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "F.1 Text-only Training Prompts. ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "We use the following prompts for Text-only training. The prompts are partially based on [23]. ", "page_idx": 28}, {"type": "text", "text": "Tideo Multi-choice QA:   \nInstruction: Choose the correct answer based on the video and question.   \nVideo: $\\{\\mathbf{f}_{1}^{t},...,\\mathbf{f}_{n}^{t}\\}$ .   \nQuestion: {Question}.   \nChoices:   \n(A): {Option A}. (B): {Option B}. (C): {Option C}. (D): {Option D}. (E): {Option E}. Answer: The correct choice is {Correct Choice}. Tideo QA: Instruction: Predict the answer based on the video and question.   \nVideo: $\\{\\mathbf{f}_{1}^{t},...,\\mathbf{f}_{n}^{t}\\}$ .   \nQuestion: {Question}.   \nAnswer: {Answer}. Tideo Description:   \nInstruction: Generate a dense description for the video. Video: $\\{\\mathbf{f}_{1}^{t},...,\\mathbf{f}_{n}^{t}\\}$ .   \nDescription: {Tideo Description}. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "F.2 Prompt for Gemini Blind Evaluation ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "In Table 6, we use the following prompt for the blind evaluation of Gemini-Pro-1.0 on EgoSchema.   \nThe prompt is based on [4]. ", "page_idx": 28}, {"type": "image", "img_path": "5NMbQPY7Bn/tmp/33e7152dd0b049ec4c48476f197abcfeaea0511fdb561a37de5993894534631f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "Figure 12: The multi-choice QA prompts used for the blind evaluation of Gemini-1.0-Pro. ", "page_idx": 28}, {"type": "text", "text": "F.3 Prompts for Data Generation ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "The prompts for TextVid generation are shown in Figure 13. ", "page_idx": 28}, {"type": "image", "img_path": "5NMbQPY7Bn/tmp/d21c33ea1ea849b572881a976b95aaa3a4f9242bd1f0a7a6152d5ba97cf12c78.jpg", "img_caption": ["Figure 13: The prompts used in TextVid generation. "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "G License ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "The code, model and proposed dataset will be publicly accessible. We use standard licenses from the community. We include the following licenses for the codes, datasets and models we used in this paper. ", "page_idx": 30}, {"type": "text", "text": "1. Benchmarks NExT-QA [72]: MIT STAR [70]: Apache TVQA [24]: MIT EgoSchema [39]: MIT MVBench [29]: MIT   \n2. Codes LLama-adapter [84] GNU General Public License v3.0 Flipped-VQA [23]: MIT   \n3. Models CLIP [48]: MIT LLama2 [57]: Llama 2 Community License Agreement LLama3: Meta Llama 3 Community License Agreement Gimini-API [56]: Gemini API Additional Terms of Service. ", "page_idx": 30}, {"type": "text", "text": "H Examples of TextVid ", "text_level": 1, "page_idx": 31}, {"type": "image", "img_path": "5NMbQPY7Bn/tmp/268bee0530866cfa1b6faf166fee78bd5aa3fca9adf5cba89323554d2a7e65e6.jpg", "img_caption": [], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "Textual Video ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Annotations ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Video Title: Make Dough Without Yeast ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Frame Caption 1: A kitchen counter with flour, salt, baking powder, and olive oil in bowls Flour: A bowl of flour is on the left side of the counter Salt: A bowl of salt is next to the bowl of flour. Baking powder: A bowl of baking powder is next to the bowl of salt. Salt: A bowl of salt is next to the bowl of flour. ", "page_idx": 32}, {"type": "text", "text": "Frame Caption 2: A woman is adding olive oil to the flour and salt in a bowl   \nWoman: A woman is standing behind the counter, adding olive oil to a bowl of flour and salt.   \nOlive oil: A cup of olive oil is in the woman's left hand.   \nBowl: A bowl of flour and salt is on the counter in front of the woman. ", "page_idx": 32}, {"type": "text", "text": "Frame Caption 3: The woman is stirring the ingredients in the bowl. Woman: The woman is standing behind the counter, stirring the ingredients in a bowl. Spoon: A spoon is in the woman's right hand Bowl: A bowl of ingredients is on the counter in front of the woman. ", "page_idx": 32}, {"type": "text", "text": "Frame Caption 4: The woman is adding water to the ingredients in the bowl.   \nWoman: The woman is standing behind the counter, adding water to a bowl of ingredients.   \nWater: A cup of water is in the woman's left hand.   \nBowl: A bowl of ingredients is on the counter in front of the woman. ", "page_idx": 32}, {"type": "text", "text": "Detailed Video Descriptions: This video shows how to make dough without yeast. The woman starts by adding olive oil to a bowl of flour and salt. She then stirs the ingredients together and adds water. The woman continues to stir the ingredients until they form a dough. She then kneads the dough on a floured surface and shapes it into a loaf. The woman places the loaf of dough in a baking pan and bakes it in the oven. Once the bread is baked, the woman removes it from the oven and lets it cool. ", "page_idx": 32}, {"type": "text", "text": "QA Pairs:   \nQuestion:  What is the woman adding to the flour and salt? Options:   \nA: Water.   \nB: Olive oil   \nC: Baking powder.   \nD: Sugar   \nE: Yeast   \nAnswer: B   \nQuestion:  What is the woman doing in frame   \nOptions:   \nA: Stirring the ingredients   \nB: Kneading the dough   \nC: Shaping the dough   \nD: Baking the bread   \nE: Removing the bread from the oven   \nAnswer: B fork.   \nWoman: The woman is standing behind the counter, stirring the ingredients in a bowl with a fork.   \nFork: A fork is in the woman's right hand.   \nBowl: A bowl of ingredients is on the counter in front of the woman. Frame Caption 6: The woman is kneading the dough on a floured surface.   \nWoman: The woman is standing behind the counter, kneading dough on a floured surface.   \nDough: A ball of dough is on the counter in front of the woman.   \nSurface: The counter is covered in flour. Frame Caption 7: The woman is shaping the dough into a loaf.   \nWoman: The woman is standing behind the counter, shaping dough into a loaf.   \nDough: A loaf of dough is on the counter in front of the woman.   \nHands: The woman's hands are on the dough.   \nA: In a bowl   \nB: On a baking sheet   \nC: In a baking pan   \nD: On a cutting board   \nE: In a plastic bag   \nAnswer: C   \nA: To combine them   \nB: To dissolve the salt   \nC: To activate the yeast   \nD: To make the dough smooth   \nE: To prevent the dough from sticking   \nAnswer: A ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "A: Dough B: Bread C: Pizza D: Pasta E: Cake Answer: B ", "page_idx": 32}, {"type": "image", "img_path": "5NMbQPY7Bn/tmp/f0f4fc069de18d22dc97bbc0a87bc962c06e2b381c7fdf3750fc45d0f88fbe8a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 33}, {"type": "image", "img_path": "5NMbQPY7Bn/tmp/71e222c9337d9398ab3ccb12c2253096d59722704375a038ec07923257aa83dc.jpg", "img_caption": [], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "Annotations ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Detailed Video Descriptions: The video shows Jose cooking an omelet. He first chops vegetables on a cutting board, then cracks an egg into a bowl. He whisks the egg and other ingredients in the bowl, then pours the mixture into a pan. He flips the omelet in the pan until it is cooked through. ", "page_idx": 35}, {"type": "text", "text": "Textual Video ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Video Recorder: Jose ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Frame Caption 1: A first-person perspective of Jose standing in the kitchen, holding a knife and chopping vegetables on a cutting board. Cutting board: A rectangular wooden cutting board is placed on the kitchen counter. Knife: Jose is holding a sharp, stainless steel knife in his right hand. ", "page_idx": 35}, {"type": "text", "text": "Question: What is Jose's primary goal in this video? Options:   \nA: To wash dishes.   \nB: To make an omelet.   \nC: To clean the kitchen   \nD: To eat dinner.   \nE: To chop vegetables. Frame Caption 2: Jose is cracking an egg into a bowl.   \nMan: The man is still sitting on the couch, his expression is still serious and his eyes are still closed.   \nBowl: A white ceramic bowl is placed on the kitchen counter.   \nEgg: Jose is holding a brown egg in his left hand. Question: What type of pan does Jose use to cook the omelet? Options:   \nA: A cast iron skillet.   \nB: A nonstick pan.   \nC: A Dutch oven   \nD: A saucepan.   \nE: A baking dish.   \nAnswer: B Frame Caption 4: Jose is pouring the egg mixture into a pan..   \nPan: A black nonstick pan is placed on the stovetop.   \nBowl: Jose is holding the white ceramic bowl in his left hand.   \nSpatula: Jose is holding a black spatula in his right hand. ", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 35}, {"type": "text", "text": "A: Salt and pepper B: Milk and cheese. C: Flour and water D: Meat and vegetables. E: Sugar and butter Answer: A ", "page_idx": 35}, {"type": "text", "text": ": He serves it with toast.   \n: He eats it with a fork.   \n: He puts it in a lunch box.   \nD: He gives it to his dog.   \nE: He throws it away.   \nnswer: B ", "page_idx": 35}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Justification: This paper aim to extend Large Language Models for video understanding via Text-Only Pre-Alignment, which does not need real videos for pre-training. Extensive experiments on video understanding benchmarks demonstrate the effectiveness of TOPA. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 36}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Justification: The limitations of TOPA are detailed in Appendix B. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efifciency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 36}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: The paper does not include theoretical results. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 37}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: The training details of TOPA are included in Appendix E.2. The dataset generation pipeline is detailed in Appendix D. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might sufifce, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 37}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufifcient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 38}, {"type": "text", "text": "Justification: The code and dataset is publicly accessible (https://github.com/dhg-wei/ TOPA). ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 38}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: The details of training and evaluation are included in Appendix E.2. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 38}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 38}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Justification: TOPA is a large-scale pre-training framework. Similar to previous related work [45, 81, 29], error bars are not reported because it would be computationally too expensive. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 38}, {"type": "text", "text": "", "page_idx": 39}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufifcient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: The details of compute resources are included in Section 3.4. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 39}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: We have reviewed the Code of Ethics and it conforms with the Code of Ethics. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 39}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: The details of broader impacts are included in Appendix C. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efifciency and accessibility of ML). ", "page_idx": 39}, {"type": "text", "text": "", "page_idx": 40}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: Our model and data focus on video-language understanding, with minimal risk of misuse. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 40}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: The licenses are mentioned in Appendix G. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 40}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 41}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: The details of the new dataset and model are detailed in this paper. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 41}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 41}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 41}]