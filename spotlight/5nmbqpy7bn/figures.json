[{"figure_path": "5NMbQPY7Bn/figures/figures_2_1.jpg", "caption": "Figure 1: Overview of the proposed Text-Only Pre-Alignment (TOPA) framework. Left: The pipeline used for generating the TextVid dataset. Right: The video-LLM alignment framework. During text-only pre-alignment, the LLM learns to process continuous CLIP text features. In zero-shot inference, the LLM uses projected CLIP visual features as input. Additionally, TOPA supports supervised fine-tuning on downstream video datasets to further improve the performance.", "description": "This figure illustrates the TOPA framework, showing the process of generating the TextVid dataset (left) and aligning the large language model (LLM) with video modality (right). The left side depicts how an LLM generates textual videos with annotations to simulate real video-text pairs. The right side shows how TOPA uses these textual videos to pre-align the LLM with the video modality using CLIP, bridging the gap between textual and real videos for zero-shot and supervised fine-tuning.", "section": "3 Method"}, {"figure_path": "5NMbQPY7Bn/figures/figures_5_1.jpg", "caption": "Figure 1: Overview of the proposed Text-Only Pre-Alignment (TOPA) framework. Left: The pipeline used for generating the TextVid dataset. Right: The video-LLM alignment framework. During text-only pre-alignment, the LLM learns to process continuous CLIP text features. In zero-shot inference, the LLM uses projected CLIP visual features as input. Additionally, TOPA supports supervised fine-tuning on downstream video datasets to further improve the performance.", "description": "This figure illustrates the TOPA framework, showing the pipeline for generating the TextVid dataset (left) and the video-LLM alignment process (right).  The TextVid dataset generation involves using an LLM to create textual videos mimicking real videos. The video-LLM alignment uses CLIP to extract features from both textual and real videos and aligns the LLM to the video modality through text-only pre-alignment.  The framework also allows for zero-shot inference and supervised fine-tuning on downstream datasets.", "section": "3 Method"}, {"figure_path": "5NMbQPY7Bn/figures/figures_8_1.jpg", "caption": "Figure 1: Overview of the proposed Text-Only Pre-Alignment (TOPA) framework. Left: The pipeline used for generating the TextVid dataset. Right: The video-LLM alignment framework. During text-only pre-alignment, the LLM learns to process continuous CLIP text features. In zero-shot inference, the LLM uses projected CLIP visual features as input. Additionally, TOPA supports supervised fine-tuning on downstream video datasets to further improve the performance.", "description": "This figure illustrates the TOPA framework, which consists of two main parts: TextVid dataset generation and video-LLM alignment.  The left side shows how the TextVid dataset is created using an LLM to generate textual videos with annotations. The right side shows how the LLM is aligned with the video modality using continuous CLIP text features (during training) and then adapts to real video data using projected CLIP visual features (during inference).  The framework also supports fine-tuning with real video data to further enhance performance.", "section": "3 Method"}, {"figure_path": "5NMbQPY7Bn/figures/figures_19_1.jpg", "caption": "Figure 1: Overview of the proposed Text-Only Pre-Alignment (TOPA) framework. Left: The pipeline used for generating the TextVid dataset. Right: The video-LLM alignment framework. During text-only pre-alignment, the LLM learns to process continuous CLIP text features. In zero-shot inference, the LLM uses projected CLIP visual features as input. Additionally, TOPA supports supervised fine-tuning on downstream video datasets to further improve the performance.", "description": "This figure illustrates the TOPA framework, which consists of two main parts: TextVid dataset generation and video-LLM alignment.  The left side shows how the TextVid dataset is created using an LLM to generate textual videos (sequences of textual frames mimicking real videos) and their corresponding annotations (dense descriptions and QA pairs). The right side details the video-LLM alignment process.  TOPA pre-aligns the LLM with video modality using only text data from TextVids, leveraging CLIP for feature extraction to bridge the gap between text and image.  Zero-shot inference and supervised fine-tuning are also shown as options for adapting the aligned model to real video understanding tasks.", "section": "3 Method"}, {"figure_path": "5NMbQPY7Bn/figures/figures_19_2.jpg", "caption": "Figure 1: Overview of the proposed Text-Only Pre-Alignment (TOPA) framework. Left: The pipeline used for generating the TextVid dataset. Right: The video-LLM alignment framework. During text-only pre-alignment, the LLM learns to process continuous CLIP text features. In zero-shot inference, the LLM uses projected CLIP visual features as input. Additionally, TOPA supports supervised fine-tuning on downstream video datasets to further improve the performance.", "description": "This figure illustrates the TOPA framework, which consists of two main parts: TextVid dataset generation and video-LLM alignment. The left side shows how TextVid is created using an LLM to generate textual videos and corresponding annotations. The right side shows how TOPA aligns LLMs with video modality using only text data. During text-only pre-alignment, the LLM processes continuous CLIP text features, which are analogous to continuous CLIP image features from real videos.  Zero-shot inference uses projected CLIP visual features, and fine-tuning can be done on downstream datasets.", "section": "3 Method"}, {"figure_path": "5NMbQPY7Bn/figures/figures_19_3.jpg", "caption": "Figure 1: Overview of the proposed Text-Only Pre-Alignment (TOPA) framework. Left: The pipeline used for generating the TextVid dataset. Right: The video-LLM alignment framework. During text-only pre-alignment, the LLM learns to process continuous CLIP text features. In zero-shot inference, the LLM uses projected CLIP visual features as input. Additionally, TOPA supports supervised fine-tuning on downstream video datasets to further improve the performance.", "description": "This figure illustrates the TOPA framework, which consists of two main parts: TextVid dataset generation and video-LLM alignment. The left side shows how the TextVid dataset is generated using an LLM to create textual videos and their corresponding annotations.  The right side illustrates the video-LLM alignment process. During text-only pre-alignment, the LLM processes continuous CLIP text features of textual videos, effectively aligning itself with the video modality.  In zero-shot inference, CLIP image features from real videos are projected into the LLM's space, allowing for video understanding without training on real video data.  Finally, supervised fine-tuning is supported to enhance performance further.", "section": "3 Method"}, {"figure_path": "5NMbQPY7Bn/figures/figures_19_4.jpg", "caption": "Figure 1: Overview of the proposed Text-Only Pre-Alignment (TOPA) framework. Left: The pipeline used for generating the TextVid dataset. Right: The video-LLM alignment framework. During text-only pre-alignment, the LLM learns to process continuous CLIP text features. In zero-shot inference, the LLM uses projected CLIP visual features as input. Additionally, TOPA supports supervised fine-tuning on downstream video datasets to further improve the performance.", "description": "This figure illustrates the TOPA framework, which consists of two main parts: TextVid dataset generation and video-LLM alignment. The left side shows how TextVids are created using an LLM to generate textual frames and annotations.  The right side details how the TextVids are used for text-only pre-alignment of the LLM with video modality via CLIP features, enabling zero-shot inference and supervised fine-tuning on downstream video datasets.", "section": "3 Method"}, {"figure_path": "5NMbQPY7Bn/figures/figures_20_1.jpg", "caption": "Figure 1: Overview of the proposed Text-Only Pre-Alignment (TOPA) framework. Left: The pipeline used for generating the TextVid dataset. Right: The video-LLM alignment framework. During text-only pre-alignment, the LLM learns to process continuous CLIP text features. In zero-shot inference, the LLM uses projected CLIP visual features as input. Additionally, TOPA supports supervised fine-tuning on downstream video datasets to further improve the performance.", "description": "This figure illustrates the TOPA framework, showing the process of generating the TextVid dataset (left) and the video-LLM alignment process (right). The TextVid dataset is created by using an LLM to generate textual videos that simulate real videos. These textual videos are then used to pre-align the LLM with the video modality. During inference, CLIP is used to extract features from real videos, which are then projected into the LLM feature space. The LLM can then be fine-tuned on downstream video datasets to further improve performance.", "section": "3 Method"}, {"figure_path": "5NMbQPY7Bn/figures/figures_20_2.jpg", "caption": "Figure 1: Overview of the proposed Text-Only Pre-Alignment (TOPA) framework. Left: The pipeline used for generating the TextVid dataset. Right: The video-LLM alignment framework. During text-only pre-alignment, the LLM learns to process continuous CLIP text features. In zero-shot inference, the LLM uses projected CLIP visual features as input. Additionally, TOPA supports supervised fine-tuning on downstream video datasets to further improve the performance.", "description": "This figure illustrates the TOPA framework, showing the process of generating the TextVid dataset (left) and how the video-LLM alignment is performed (right).  The TextVid dataset is created using an LLM to generate textual videos mimicking real video content. The right side shows how TOPA aligns LLMs with video modality by using continuous CLIP text features for pre-alignment and projected CLIP visual features for zero-shot inference.  Supervised fine-tuning on downstream datasets is also supported to further enhance performance.", "section": "3 Method"}, {"figure_path": "5NMbQPY7Bn/figures/figures_20_3.jpg", "caption": "Figure 1: Overview of the proposed Text-Only Pre-Alignment (TOPA) framework. Left: The pipeline used for generating the TextVid dataset. Right: The video-LLM alignment framework. During text-only pre-alignment, the LLM learns to process continuous CLIP text features. In zero-shot inference, the LLM uses projected CLIP visual features as input. Additionally, TOPA supports supervised fine-tuning on downstream video datasets to further improve the performance.", "description": "This figure illustrates the TOPA framework, which consists of two main parts: TextVid dataset generation and video-LLM alignment.  The left side shows how the TextVid dataset is created using an LLM to generate textual videos and annotations. The right side depicts the video-LLM alignment process, where a pre-trained LLM is aligned with video modality using textual video representations.  The alignment allows for both zero-shot inference (using projected CLIP visual features) and supervised fine-tuning on downstream video datasets.", "section": "3 Method"}, {"figure_path": "5NMbQPY7Bn/figures/figures_21_1.jpg", "caption": "Figure 1: Overview of the proposed Text-Only Pre-Alignment (TOPA) framework. Left: The pipeline used for generating the TextVid dataset. Right: The video-LLM alignment framework. During text-only pre-alignment, the LLM learns to process continuous CLIP text features. In zero-shot inference, the LLM uses projected CLIP visual features as input. Additionally, TOPA supports supervised fine-tuning on downstream video datasets to further improve the performance.", "description": "This figure shows the overall architecture of the TOPA framework, which is divided into two main parts: TextVid dataset generation and video-LLM alignment. The left side illustrates the process of generating the TextVid dataset using an LLM to create textual videos and corresponding annotations. The right side shows how TOPA aligns the LLM with the video modality using CLIP features.  During text-only pre-alignment, the LLM processes continuous text features, and then transitions to processing continuous image features for real video inference using a projection layer.  Zero-shot inference is also supported, as is supervised finetuning on downstream video datasets to further improve performance.", "section": "3 Method"}, {"figure_path": "5NMbQPY7Bn/figures/figures_21_2.jpg", "caption": "Figure 1: Overview of the proposed Text-Only Pre-Alignment (TOPA) framework. Left: The pipeline used for generating the TextVid dataset. Right: The video-LLM alignment framework. During text-only pre-alignment, the LLM learns to process continuous CLIP text features. In zero-shot inference, the LLM uses projected CLIP visual features as input. Additionally, TOPA supports supervised fine-tuning on downstream video datasets to further improve the performance.", "description": "This figure illustrates the TOPA framework, which consists of two main parts: TextVid dataset generation and video-LLM alignment. The left side shows the process of generating the TextVid dataset using an LLM to create textual videos and their annotations. The right side shows how TOPA aligns LLMs with video modality using CLIP features and a pre-alignment step, enabling zero-shot inference and supervised fine-tuning.", "section": "3 Method"}, {"figure_path": "5NMbQPY7Bn/figures/figures_21_3.jpg", "caption": "Figure 1: Overview of the proposed Text-Only Pre-Alignment (TOPA) framework. Left: The pipeline used for generating the TextVid dataset. Right: The video-LLM alignment framework. During text-only pre-alignment, the LLM learns to process continuous CLIP text features. In zero-shot inference, the LLM uses projected CLIP visual features as input. Additionally, TOPA supports supervised fine-tuning on downstream video datasets to further improve the performance.", "description": "This figure illustrates the TOPA framework, which consists of two main parts: TextVid dataset generation and video-LLM alignment.  The left side shows how the TextVid dataset is created using an LLM to generate textual videos and annotations. The right side details the video-LLM alignment process, demonstrating how TOPA pre-aligns LLMs with video modality using only text data and then adapts to real video understanding through zero-shot inference and supervised fine-tuning.", "section": "3 Method"}, {"figure_path": "5NMbQPY7Bn/figures/figures_21_4.jpg", "caption": "Figure 1: Overview of the proposed Text-Only Pre-Alignment (TOPA) framework. Left: The pipeline used for generating the TextVid dataset. Right: The video-LLM alignment framework. During text-only pre-alignment, the LLM learns to process continuous CLIP text features. In zero-shot inference, the LLM uses projected CLIP visual features as input. Additionally, TOPA supports supervised fine-tuning on downstream video datasets to further improve the performance.", "description": "This figure shows the overall architecture of the TOPA framework, which consists of two main parts: TextVid dataset generation and video-LLM alignment. The left side illustrates the process of generating the TextVid dataset using an LLM to create textual videos and their corresponding annotations.  The right side shows the video-LLM alignment process, which involves pre-aligning the LLM with the video modality using the generated TextVid data and then using CLIP to align the text and image features.  The framework supports both zero-shot inference and supervised fine-tuning on downstream video datasets.", "section": "3 Method"}, {"figure_path": "5NMbQPY7Bn/figures/figures_21_5.jpg", "caption": "Figure 1: Overview of the proposed Text-Only Pre-Alignment (TOPA) framework. Left: The pipeline used for generating the TextVid dataset. Right: The video-LLM alignment framework. During text-only pre-alignment, the LLM learns to process continuous CLIP text features. In zero-shot inference, the LLM uses projected CLIP visual features as input. Additionally, TOPA supports supervised fine-tuning on downstream video datasets to further improve the performance.", "description": "This figure shows a schematic overview of the TOPA framework. The left side illustrates the process of generating the TextVid dataset, which involves using a large language model (LLM) to create textual videos and their corresponding annotations. The right side shows the process of aligning a large language model (LLM) with the video modality. This alignment is performed using Text-Only Pre-Alignment (TOPA), which leverages continuous textual frames (analogous to continuous CLIP image features) to pre-align the LLM with the video modality.  The figure also highlights the zero-shot inference and supervised fine-tuning capabilities of the TOPA framework.", "section": "3 Method"}, {"figure_path": "5NMbQPY7Bn/figures/figures_22_1.jpg", "caption": "Figure 1: Overview of the proposed Text-Only Pre-Alignment (TOPA) framework. Left: The pipeline used for generating the TextVid dataset. Right: The video-LLM alignment framework. During text-only pre-alignment, the LLM learns to process continuous CLIP text features. In zero-shot inference, the LLM uses projected CLIP visual features as input. Additionally, TOPA supports supervised fine-tuning on downstream video datasets to further improve the performance.", "description": "This figure illustrates the TOPA framework, which consists of two main parts: TextVid dataset generation and video-LLM alignment. The left side shows how textual videos are generated using an LLM, mimicking real video dynamics. The right side shows how these textual videos are used to pre-align LLMs with video modality using CLIP features, allowing for zero-shot and finetuned video understanding.  The framework highlights the process of generating textual video data, aligning the LLM with the video modality using textual frames and CLIP, performing zero-shot inference using projected CLIP visual features, and the option for supervised finetuning on downstream datasets.", "section": "3 Method"}, {"figure_path": "5NMbQPY7Bn/figures/figures_22_2.jpg", "caption": "Figure 1: Overview of the proposed Text-Only Pre-Alignment (TOPA) framework. Left: The pipeline used for generating the TextVid dataset. Right: The video-LLM alignment framework. During text-only pre-alignment, the LLM learns to process continuous CLIP text features. In zero-shot inference, the LLM uses projected CLIP visual features as input. Additionally, TOPA supports supervised fine-tuning on downstream video datasets to further improve the performance.", "description": "This figure illustrates the TOPA framework, which consists of two main parts: TextVid dataset generation and video-LLM alignment. The left side shows how textual videos (Tideos) are generated using an LLM, mimicking real videos with continuous textual frames and annotations.  The right side depicts the video-LLM alignment process, where a pre-trained LLM is aligned with the video modality using the generated Tideos.  The process includes text-only pre-alignment, zero-shot inference (using projected CLIP visual features), and optional supervised fine-tuning on downstream video datasets.", "section": "3 Method"}, {"figure_path": "5NMbQPY7Bn/figures/figures_22_3.jpg", "caption": "Figure 1: Overview of the proposed Text-Only Pre-Alignment (TOPA) framework. Left: The pipeline used for generating the TextVid dataset. Right: The video-LLM alignment framework. During text-only pre-alignment, the LLM learns to process continuous CLIP text features. In zero-shot inference, the LLM uses projected CLIP visual features as input. Additionally, TOPA supports supervised fine-tuning on downstream video datasets to further improve the performance.", "description": "This figure illustrates the TOPA framework, showing the process of generating the TextVid dataset (left) and the video-LLM alignment process (right).  The TextVid dataset generation uses an LLM to create textual videos simulating real videos. The alignment process uses CLIP to bridge textual and real video modalities, enabling the LLM to learn from textual videos and then adapt to real video data.  Zero-shot inference and supervised fine-tuning options are also shown.", "section": "3 Method"}, {"figure_path": "5NMbQPY7Bn/figures/figures_22_4.jpg", "caption": "Figure 1: Overview of the proposed Text-Only Pre-Alignment (TOPA) framework. Left: The pipeline used for generating the TextVid dataset. Right: The video-LLM alignment framework. During text-only pre-alignment, the LLM learns to process continuous CLIP text features. In zero-shot inference, the LLM uses projected CLIP visual features as input. Additionally, TOPA supports supervised fine-tuning on downstream video datasets to further improve the performance.", "description": "This figure illustrates the TOPA framework, showing the TextVid dataset generation pipeline on the left and the video-LLM alignment framework on the right.  The left side depicts how an LLM is used to create textual videos (TextVids) mimicking real videos, complete with annotations for training. The right side details how TOPA aligns an LLM with video modality by using CLIP to bridge textual and real video features.  TOPA supports both zero-shot inference (using projected CLIP visual features) and supervised fine-tuning on downstream video datasets.", "section": "3 Method"}, {"figure_path": "5NMbQPY7Bn/figures/figures_22_5.jpg", "caption": "Figure 1: Overview of the proposed Text-Only Pre-Alignment (TOPA) framework. Left: The pipeline used for generating the TextVid dataset. Right: The video-LLM alignment framework. During text-only pre-alignment, the LLM learns to process continuous CLIP text features. In zero-shot inference, the LLM uses projected CLIP visual features as input. Additionally, TOPA supports supervised fine-tuning on downstream video datasets to further improve the performance.", "description": "This figure illustrates the TOPA framework, which consists of two main parts: TextVid dataset generation and video-LLM alignment.  The left side shows how the TextVid dataset is created using an LLM to generate textual videos and annotations. The right side details how TOPA aligns the LLM with video modality using continuous CLIP text and image features, enabling zero-shot inference and supervised fine-tuning.", "section": "3 Method"}, {"figure_path": "5NMbQPY7Bn/figures/figures_23_1.jpg", "caption": "Figure 1: Overview of the proposed Text-Only Pre-Alignment (TOPA) framework. Left: The pipeline used for generating the TextVid dataset. Right: The video-LLM alignment framework. During text-only pre-alignment, the LLM learns to process continuous CLIP text features. In zero-shot inference, the LLM uses projected CLIP visual features as input. Additionally, TOPA supports supervised fine-tuning on downstream video datasets to further improve the performance.", "description": "This figure illustrates the TOPA framework, which consists of two main parts: TextVid dataset generation and video-LLM alignment. The left side shows how textual videos (Tideos) are created using an LLM. These Tideos mimic real videos with textual frames and annotations, creating simulated video-text pairs. The right side depicts the video-LLM alignment process, where the LLM is pre-aligned with video modality using the generated Tideos and CLIP features (both text and image).  The framework supports both zero-shot inference (using projected CLIP visual features) and supervised fine-tuning on downstream video datasets.", "section": "3 Method"}, {"figure_path": "5NMbQPY7Bn/figures/figures_23_2.jpg", "caption": "Figure 1: Overview of the proposed Text-Only Pre-Alignment (TOPA) framework. Left: The pipeline used for generating the TextVid dataset. Right: The video-LLM alignment framework. During text-only pre-alignment, the LLM learns to process continuous CLIP text features. In zero-shot inference, the LLM uses projected CLIP visual features as input. Additionally, TOPA supports supervised fine-tuning on downstream video datasets to further improve the performance.", "description": "This figure illustrates the TOPA framework, showing the process of generating the TextVid dataset (left) and the video-LLM alignment (right).  The left side depicts how an LLM generates textual videos and their annotations. The right side details how TOPA pre-aligns LLMs with video modality using textual video data and CLIP for feature extraction, enabling zero-shot inference and fine-tuning on real video data.", "section": "3 Method"}, {"figure_path": "5NMbQPY7Bn/figures/figures_23_3.jpg", "caption": "Figure 1: Overview of the proposed Text-Only Pre-Alignment (TOPA) framework. Left: The pipeline used for generating the TextVid dataset. Right: The video-LLM alignment framework. During text-only pre-alignment, the LLM learns to process continuous CLIP text features. In zero-shot inference, the LLM uses projected CLIP visual features as input. Additionally, TOPA supports supervised fine-tuning on downstream video datasets to further improve the performance.", "description": "This figure illustrates the TOPA framework, which consists of two main parts: TextVid dataset generation and video-LLM alignment. The left side shows how TextVid is created using an LLM to generate textual videos with annotations. The right side demonstrates how TOPA aligns LLMs with video modality using TextVid, CLIP, and an adapter. The framework supports both zero-shot inference and supervised fine-tuning.", "section": "3 Method"}, {"figure_path": "5NMbQPY7Bn/figures/figures_24_1.jpg", "caption": "Figure 1: Overview of the proposed Text-Only Pre-Alignment (TOPA) framework. Left: The pipeline used for generating the TextVid dataset. Right: The video-LLM alignment framework. During text-only pre-alignment, the LLM learns to process continuous CLIP text features. In zero-shot inference, the LLM uses projected CLIP visual features as input. Additionally, TOPA supports supervised fine-tuning on downstream video datasets to further improve the performance.", "description": "This figure illustrates the TOPA framework, which consists of two main parts: TextVid dataset generation and video-LLM alignment. The left side shows how the TextVid dataset is created using an LLM to generate textual videos and annotations.  The right side details the video-LLM alignment process.  During text-only pre-alignment, the LLM processes textual video features (analogous to real video features), enabling zero-shot inference with real video data using projected CLIP visual features.  The framework also supports supervised fine-tuning for improved performance.", "section": "3 Method"}, {"figure_path": "5NMbQPY7Bn/figures/figures_24_2.jpg", "caption": "Figure 1: Overview of the proposed Text-Only Pre-Alignment (TOPA) framework. Left: The pipeline used for generating the TextVid dataset. Right: The video-LLM alignment framework. During text-only pre-alignment, the LLM learns to process continuous CLIP text features. In zero-shot inference, the LLM uses projected CLIP visual features as input. Additionally, TOPA supports supervised fine-tuning on downstream video datasets to further improve the performance.", "description": "This figure illustrates the TOPA framework, which consists of two main parts: TextVid dataset generation and video-LLM alignment.  The left side shows how the TextVid dataset is created using an LLM to generate textual videos and their annotations. The right side details the video-LLM alignment process, showing how TOPA uses text-only pre-alignment to align LLMs with the video modality.  It also highlights the zero-shot inference and supervised finetuning capabilities of the framework.", "section": "3 Method"}, {"figure_path": "5NMbQPY7Bn/figures/figures_24_3.jpg", "caption": "Figure 1: Overview of the proposed Text-Only Pre-Alignment (TOPA) framework. Left: The pipeline used for generating the TextVid dataset. Right: The video-LLM alignment framework. During text-only pre-alignment, the LLM learns to process continuous CLIP text features. In zero-shot inference, the LLM uses projected CLIP visual features as input. Additionally, TOPA supports supervised fine-tuning on downstream video datasets to further improve the performance.", "description": "This figure illustrates the TOPA framework, which consists of two main parts: TextVid dataset generation and video-LLM alignment. The left side shows the process of generating the TextVid dataset using an LLM to create textual videos and their annotations. The right side illustrates the video-LLM alignment process, where the LLM is pre-aligned with video modality using textual videos and CLIP features.  Zero-shot inference and supervised fine-tuning are also shown as ways to use the aligned LLM for video understanding tasks.", "section": "3 Method"}, {"figure_path": "5NMbQPY7Bn/figures/figures_24_4.jpg", "caption": "Figure 1: Overview of the proposed Text-Only Pre-Alignment (TOPA) framework. Left: The pipeline used for generating the TextVid dataset. Right: The video-LLM alignment framework. During text-only pre-alignment, the LLM learns to process continuous CLIP text features. In zero-shot inference, the LLM uses projected CLIP visual features as input. Additionally, TOPA supports supervised fine-tuning on downstream video datasets to further improve the performance.", "description": "This figure illustrates the TOPA framework, showing the process of generating the TextVid dataset (left) and the video-LLM alignment process (right).  The TextVid generation uses an LLM to create textual videos that mimic real videos. The alignment process involves pre-aligning an LLM with textual video representations using CLIP for feature extraction, enabling zero-shot inference and fine-tuning on real video datasets.", "section": "3 Method"}, {"figure_path": "5NMbQPY7Bn/figures/figures_24_5.jpg", "caption": "Figure 1: Overview of the proposed Text-Only Pre-Alignment (TOPA) framework. Left: The pipeline used for generating the TextVid dataset. Right: The video-LLM alignment framework. During text-only pre-alignment, the LLM learns to process continuous CLIP text features. In zero-shot inference, the LLM uses projected CLIP visual features as input. Additionally, TOPA supports supervised fine-tuning on downstream video datasets to further improve the performance.", "description": "This figure illustrates the TOPA framework, which consists of two main parts: TextVid dataset generation and video-LLM alignment.  The left side shows how TextVids are created using an LLM, generating textual frames analogous to real video frames, complete with descriptions and question-answer pairs. The right side depicts how TOPA pre-aligns LLMs with video data using only text, bridging the gap between textual and real video representations via the CLIP model.  Zero-shot inference and supervised fine-tuning are also shown as options for using the aligned model.", "section": "3 Method"}, {"figure_path": "5NMbQPY7Bn/figures/figures_26_1.jpg", "caption": "Figure 1: Overview of the proposed Text-Only Pre-Alignment (TOPA) framework. Left: The pipeline used for generating the TextVid dataset. Right: The video-LLM alignment framework. During text-only pre-alignment, the LLM learns to process continuous CLIP text features. In zero-shot inference, the LLM uses projected CLIP visual features as input. Additionally, TOPA supports supervised fine-tuning on downstream video datasets to further improve the performance.", "description": "This figure illustrates the TOPA framework, which consists of two main parts: TextVid dataset generation and video-LLM alignment. The left side shows how TextVid is created using an LLM to generate textual videos and their annotations. The right side details the video-LLM alignment process, including text-only pre-alignment using continuous CLIP text features and zero-shot inference using projected CLIP visual features.  The framework also supports supervised fine-tuning for enhanced performance.", "section": "3 Method"}, {"figure_path": "5NMbQPY7Bn/figures/figures_26_2.jpg", "caption": "Figure 11: Visualization of Tideo features generated from different type of prompts.", "description": "This figure visualizes the Tideo features generated from different types of prompts, namely Howto100m, WebVid, Ego4D, and WordNet. Each point represents a Tideo feature vector, and the color indicates the type of prompt used to generate it. The figure shows that the Tideo features generated from different types of prompts are clustered together in different regions of the feature space, indicating that the features capture different aspects of the video content. This visualization helps to understand the diversity of the Tideo dataset and how different types of prompts contribute to this diversity.  The plot appears to use a dimensionality reduction technique (like t-SNE or UMAP) to project the high-dimensional feature vectors into a 2D space for visualization.", "section": "E Experimental Setup"}, {"figure_path": "5NMbQPY7Bn/figures/figures_28_1.jpg", "caption": "Figure 1: Overview of the proposed Text-Only Pre-Alignment (TOPA) framework. Left: The pipeline used for generating the TextVid dataset. Right: The video-LLM alignment framework. During text-only pre-alignment, the LLM learns to process continuous CLIP text features. In zero-shot inference, the LLM uses projected CLIP visual features as input. Additionally, TOPA supports supervised fine-tuning on downstream video datasets to further improve the performance.", "description": "This figure illustrates the TOPA framework, which consists of two main parts: TextVid dataset generation and video-LLM alignment. The left side shows how the TextVid dataset is created using an LLM to generate textual videos and their annotations.  The right side details the video-LLM alignment process, including text-only pre-alignment using continuous CLIP text features, zero-shot inference using projected CLIP visual features, and optional supervised fine-tuning on downstream video datasets to enhance performance.", "section": "3 Method"}, {"figure_path": "5NMbQPY7Bn/figures/figures_29_1.jpg", "caption": "Figure 1: Overview of the proposed Text-Only Pre-Alignment (TOPA) framework. Left: The pipeline used for generating the TextVid dataset. Right: The video-LLM alignment framework. During text-only pre-alignment, the LLM learns to process continuous CLIP text features. In zero-shot inference, the LLM uses projected CLIP visual features as input. Additionally, TOPA supports supervised fine-tuning on downstream video datasets to further improve the performance.", "description": "This figure illustrates the TOPA framework, which consists of two main stages: TextVid dataset generation and video-LLM alignment.  The left side shows how the TextVid dataset is created using an LLM to generate textual videos (Tideos) mimicking real videos. The right side details the video-LLM alignment process, which involves text-only pre-alignment using the Tideos and CLIP features, followed by zero-shot inference and optional supervised fine-tuning on real video datasets.", "section": "3 Method"}, {"figure_path": "5NMbQPY7Bn/figures/figures_31_1.jpg", "caption": "Figure 1: Overview of the proposed Text-Only Pre-Alignment (TOPA) framework. Left: The pipeline used for generating the TextVid dataset. Right: The video-LLM alignment framework. During text-only pre-alignment, the LLM learns to process continuous CLIP text features. In zero-shot inference, the LLM uses projected CLIP visual features as input. Additionally, TOPA supports supervised fine-tuning on downstream video datasets to further improve the performance.", "description": "This figure illustrates the TOPA framework, showing the two main stages: TextVid dataset generation and video-LLM alignment. The left side depicts the process of creating the TextVid dataset using an LLM to generate textual videos and annotations. The right side details the alignment process, where a language model (LLM) is pre-aligned with video modality using textual videos, and how this alignment facilitates both zero-shot inference using CLIP visual features and supervised finetuning on real video datasets.", "section": "3 Method"}, {"figure_path": "5NMbQPY7Bn/figures/figures_33_1.jpg", "caption": "Figure 1: Overview of the proposed Text-Only Pre-Alignment (TOPA) framework. Left: The pipeline used for generating the TextVid dataset. Right: The video-LLM alignment framework. During text-only pre-alignment, the LLM learns to process continuous CLIP text features. In zero-shot inference, the LLM uses projected CLIP visual features as input. Additionally, TOPA supports supervised fine-tuning on downstream video datasets to further improve the performance.", "description": "This figure illustrates the TOPA framework, which consists of two main parts: TextVid dataset generation and video-LLM alignment. The left side shows how the TextVid dataset is created using an LLM to generate textual videos and their annotations. The right side shows how the pre-trained LLM is aligned with video modality using the generated TextVid dataset and CLIP features.  TOPA supports both zero-shot inference using projected CLIP visual features and supervised fine-tuning on downstream video datasets for improved performance.", "section": "3 Method"}, {"figure_path": "5NMbQPY7Bn/figures/figures_34_1.jpg", "caption": "Figure 1: Overview of the proposed Text-Only Pre-Alignment (TOPA) framework. Left: The pipeline used for generating the TextVid dataset. Right: The video-LLM alignment framework. During text-only pre-alignment, the LLM learns to process continuous CLIP text features. In zero-shot inference, the LLM uses projected CLIP visual features as input. Additionally, TOPA supports supervised fine-tuning on downstream video datasets to further improve the performance.", "description": "This figure illustrates the TOPA framework's two main stages: TextVid dataset generation and video-LLM alignment.  The left side shows how textual videos (Tideos) are created using an LLM, mimicking real videos with textual frames and annotations.  The right side details the alignment process: during text-only pre-alignment, the LLM processes continuous CLIP text features from Tideos.  For real video inference (zero-shot or fine-tuned), CLIP visual features are projected to align with the LLM's text feature space. Fine-tuning on downstream video datasets can further enhance performance.", "section": "3 Method"}]