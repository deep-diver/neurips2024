[{"type": "text", "text": "Scalable and Effective Arithmetic Tree Generation for Adder and Multiplier Designs ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yao Lai1, Jinxin Liu3, David Z. Pan2, Ping Luo1 1The University of Hong Kong, 2The University of Texas at Austin, 3Zhejiang University ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Across a wide range of hardware scenarios, the computational efficiency and physical size of the arithmetic units significantly influence the speed and footprint of the overall hardware system. Nevertheless, the effectiveness of prior arithmetic design techniques proves inadequate, as they do not sufficiently optimize speed and area, resulting in increased latency and larger module size. To boost computing performance, this work focuses on the two most common and fundamental arithmetic modules, adders and multipliers. We cast the design tasks as singleplayer tree generation games, leveraging reinforcement learning techniques to optimize their arithmetic tree structures. This tree generation formulation allows us to efficiently navigate the vast search space and discover superior arithmetic designs that improve computational efficiency and hardware size within just a few hours. Our proposed method, ArithTreeRL, achieves significant improvements for both adders and multipliers. For adders, our approach discovers designs of 128-bit adders that achieve Pareto optimality in theoretical metrics. Compared with PrefixRL, it reduces delay and size by up to $26\\%$ and $30\\%$ , respectively. For multipliers, compared to RL-MUL, our method enhances speed and reduces size by as much as $49\\%$ and $45\\%$ . Additionally, ArithTreeRL\u2019s flexibility and scalability enable seamless integration into $7\\mathrm{nm}$ technology. We believe our work will offer valuable insights into hardware design, further accelerating speed and reducing size through the refined search space and our tree generation methodologies. Codes are released at github.com/laiyao1/ArithmeticTree. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Since the inception of computers, researchers have striven to boost computing speed and decrease hardware size. High computing speed is essential for a wide range of real-world applications, such as artificial intelligence [1], high-performance computing [2], and high-frequency trading [3], particularly for the recent applications of large language models like GPT [4]. Concurrently, the demand for smaller hardware has escalated due to the growth of wearable devices and IoT technology [5]. ", "page_idx": 0}, {"type": "text", "text": "Hardware specialists have steadily miniaturized CMOS technology [6] to boost processor speeds and shrink chip sizes. However, as CMOS technology\u2019s scaling nears its fundamental physical limits [7], further miniaturization poses significant challenges. Therefore, exploring innovative circuit design has emerged as a vital alternative to drive performance enhancement and area reduction. Among the family of arithmetic modules for hardware architectures, adders and multipliers constitute two essential modules, playing a critical role in various computational operations. For example, basic addition and multiplication operations compute all convolution and fully connected layers of deep learning models. Performance analysis of the ResNet model [8] reveals that the convolution operation, consisting solely of addition and multiplication, constitutes $98.4\\%$ of the overall GPU execution time during model inference. Under Amdahl\u2019s Law [9], an enhancement of $30\\%$ in addition and multiplication operation speeds could result in a $29\\%$ improvement in inference speed. Intriguingly, this improvement is comparable to the speedup typically seen with a generational upgrade in semiconductor process technology [10, 11]. Thus, designing more efficient and compact adders and multipliers is crucial for the overall advancement of hardware design. ", "page_idx": 0}, {"type": "image", "img_path": "5pnhGedG98/tmp/953d75d5d99e631a3358174e5ff2309bab294b080a7ae8b91e713bb9c4f98025.jpg", "img_caption": ["Figure 1: (a) ArithTreeRL framework. Two agents optimize prefix and compressor trees, respectively, modeling the tasks as AddGame for adders and MultGame for multipliers. (b) Prefix tree. (c) Compressor tree. Different tree structures lead to different qualities of adder and multiplier designs. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Numerous arithmetic module design methods have been proposed in recent years. These techniques generally fall into one of three main categories: human-based [12, 13], optimization-based [14\u201316], and learning-based [17\u201319]. However, these methods either demand significant hardware expertise or get trapped in local optimal due to the vast design search space for adders and multipliers modules. For human-based methods, hardware experts have crafted a variety of arithmetic modules, such as the Sklansky adder [12] and the Wallace multiplier [13]. Nevertheless, designing new structures becomes increasingly challenging for humans as input bits increase. Optimization-based methods, like bottom-up enumerate search [14, 15] and integer linear programming [16], can enhance the quality of arithmetic designs by exploring a wider variety of structures. Despite their potential, the extensive search space poses a challenge, necessitating manually defined assumptions to limit the search scope for feasible computation. For example, Ma et al. [20] assumed the existence of semi-regular structures in adders, which may lead to locally optimal solutions. While learning-based approaches have emerged as a promising tool for automating hardware design in recent years [17\u2013 19], navigating the vast design space to find the optimal solution for arithmetic modules remains a formidable challenge. For example, the two primary components of an $N$ -bit multiplier, the compressor tree and the prefix tree, have approximately $O(2^{\\bar{N}^{2}})$ and $O(2^{4N^{2}})$ design space [17], respectively. Consequently, the search space of a simple 16-bit multiplier is already comparable to that of the Go game $\\mathrm{\\dot{(3^{361})}}$ [21]. Meanwhile, such learning-based approaches also fail to consider the joint optimization of different components within arithmetic hardware [17, 18], thus easily leading to degenerated hardware with undesired performance bottleneck. ", "page_idx": 1}, {"type": "text", "text": "To resolve the above limitations and boost the performance, we formulate the arithmetic adder and multiplier design problems as two single-player tree generation games, AddGame and MultGame, respectively, as shown in Fig. 1a. The key insight is that by reframing the design problems into interactive tree generation games, we harness the power of progressive optimization algorithms, allowing us to explore the intricate design space of arithmetic units dynamically. Starting from an initial prefix tree, the player in AddGame sequentially modifies cells in the prefix tree, in the same spirit as tactical movements in board games. Our MultGame contains two parts, specifically for designing the compressor tree and the prefix tree of multipliers. The compressor tree design involves the player compressing all partial products with different compressors, similar to a match game. In contrast, the prefix tree design follows the same rules as the AddGame. Unlike the default design process depicted in Fig. 2a, the tree structures discovered in games are converted into specific Verilog codes [22], as illustrated in Fig. 2b. We demonstrate that the delay and area of arithmetic modules can be largely decreased by substituting the default designs with our discovered tree structures. ", "page_idx": 1}, {"type": "image", "img_path": "5pnhGedG98/tmp/fd4d81f37961cc3f238282e9ee48c56d16cbf403bce6feaf91a8308f7fca3006.jpg", "img_caption": ["Figure 2: Comparison of design processes. (a) Default design process. The synthesis tool automatically generates a default multiplier when using multiplication commands $(\\mathbf{x}^{*}\\mathbf{y})$ in Verilog HDL code. (b) Enhanced design process in ArithTreeRL. ArithTreeRL discovers an optimized multiplier structure and generates specialized Verilog HDL code for this improved structure, reducing delay and area after synthesis. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "We propose ArithTreeRL (Arithmetic Tree Reinforcement Learning), a novel approach that utilizes customized reinforcement learning agents for optimizing arithmetic tree structures. In practical implementation, ArithTreeRL employs two distinct agents tailored to the specific characteristics of prefix and compressor tree optimization. For the prefix tree, appearing in both AddGame and MultGame, we utilize a Monte-Carlo Tree Search (MCTS) [23] agent to efficiently explore the large action space while preserving previous exploration experience. For the compressor tree, exclusive to MultGame, we take a Proximal Policy Optimization (PPO) [24] agent due to its superior exploration efficiency. To capture the global design for multiplier designs, we also designed an optimization curriculum as depicted in Fig. 1, iteratively running MCTS and PPO agents to refine the prefix and compressor trees. ", "page_idx": 2}, {"type": "text", "text": "This paper has three main contributions. Firstly, we model the arithmetic module design tasks as single-player tree generation games, i.e., AddGame and MultGame, which inherit the well-established RL capabilities for complex decision-making tasks (arithmetic tree optimization). Secondly, we propose a co-designed framework that integrates prefix and compressor tree modules, enabling the discovery of optimal combinations that lead to global optimal multipliers. Thirdly, our experiments reveal that our designed 128-bit Pareto-optimal adders outperform the latest theoretical designs. Also, our designed adders achieved up to $26\\%$ and $30\\%$ reductions in delay and area compared to PrefixRL [17], and multipliers offer $33\\%$ and $45\\%$ improvements over RL-MUL [18] in the same metrics. These designs are ready for direct integration into synthesis tools, offering significant industrial beneftis, and are flexible and scalable enough to be seamlessly adopted into $7\\mathrm{nm}$ technology. ", "page_idx": 2}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Adder Design. An $N$ -bit adder can be constructed by cascading $N$ 1-bit adders. However, this approach results in an $O(N)$ delay due to the sequential propagation of the carry signal from the lower bit to the higher bit. To address this issue, prefix adders have been proposed [25, 26]. Prefix adders are designed based on the principles of addition, with a focus on reusing and parallelizing intermediate signal bits. These signal bits can be divided into two categories: propagation bits $p_{i}=a_{i}\\oplus b_{i}$ and generation bits $g_{i}=a_{i}\\cdot b_{i}$ , where $a_{i},b_{i}\\in\\{0,1\\},i\\in\\{1,\\bar{2},...,\\bar{N^{\\}}\\}$ represent the addends at the $i$ -th bit, and $\\cdot\\Phi^{\\prime}$ and \u2018\u00b7\u2019 denote the logic XOR and AND operations, respectively [27]. These propagation and generation signals can be defined at both the individual bit level and across a range of bits. For an individual bit with index $i$ , they are denoted by $P_{i:i}=p_{i}$ and $G_{i:i}=g_{i}$ . When considering a range of bits, this range is treated as an interval identified by a tuple $(i,j)$ . Within each such interval, we have a swinhegrlee \u2019p raegparteisoenn stis gtnhael $\\begin{array}{r}{P_{i:j}=\\prod_{k=i}^{j}p_{k}}\\end{array}$ oann. d Na ostien tghlaet  gtehne ecroatmiopnu tsaitginoanl $\\begin{array}{r}{G_{i:j}=g_{j}+\\sum_{k=i}^{j-1}P_{k:j}\\cdot g_{k}}\\end{array}$ $^{\\bullet}+^{\\bullet}$ $P_{i;j}$ $G_{i:j}$ solely by the input bits from position $i$ to $j$ . The $(N+1)$ outputs of the adder can be calculated from the signal bits with the initial condition $G_{1:0}=0$ by $c_{N+1}=g_{N}+p_{N}\\cdot G_{1:N}$ and $s_{i}=p_{i}\\oplus G_{1:i-1}$ , where $c_{N+1}$ is the carry-out bit and $s_{i}$ is the $i$ -th sum bit. ", "page_idx": 2}, {"type": "text", "text": "The prefix adder design aims to optimize a hierarchical tree structure that generates all intervals $(1,i)$ from the initial intervals $(i,i)$ , as shown in Fig. 1b. Signal bits for two adjacent intervals, $(i,k)$ and $(k+1,j)$ , can be merged to form the larger interval $(i,j)$ by the computations $P_{i:j}=P_{i:k}\\cdot P_{k+1:j}$ and $G_{i:j}=G_{i:k}\\cdot P_{k+1:j}+G_{k+1:j}$ . This merging process generates a prefix tree where each cell represents an $(i,j)$ interval with two signal bits. If an interval results from merging two others, its corresponding cell is the child node in the tree, and the merged intervals are its parent node. For example, the $(5,8)$ cell is the child node of the $(5,6)$ and $(7,8)$ cells because it derived from them. A key advantage of this structure is that cells with no dependencies can be computed in parallel. Different tree structures can result in adders with varying delays and areas. When evaluating the theoretical quality of the prefix adder, We can use level (tree height) and size (number of cells) as theoretical metrics to substitute for practical metrics like delay and area. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Multiplier Design. An $N$ -bit multiplier carries out the multiplication of two $N$ -bit multiplicands, which can be regarded as the cumulative addition of $N$ addends, involving a total of $N^{2}$ bits. Each addend represents a partial product with different powers of two weights, illustrated in Fig. 4b. Multipliers can be easily achieved by cascading $(N-1)$ $N$ -bit adders or using a single $N$ -bit adder $(N-1)$ times. However, both result in a large area or high delay. To mitigate this, the $N^{2}$ bits in the partial products can be added simultaneously by 1-bit adders, which can also be seen as a bit compression process because the number of bits gradually decreases. The compression process halts when the number of bits for each binary digit is reduced to two or fewer before feeding into a downstream adder, as illustrated in Fig. 1c. The process generates a compressor tree, describing a compression mechanism that merges $N^{2}$ bits into fewer bits by compressors such as half and full adders. Introducing an additional carry-in input distinguishes a full adder from a half adder, as shown in Fig. 1b, which affects the latency and area. The difference is crucial when configuring the compressor tree in multipliers to optimize for delays and area requirements. Upon completing the compression, the remaining bits are processed by a $2N$ -bit prefix adder, designed to yield the globally optimal multiplier. In summary, adder and multiplier design tasks can be interpreted as a tree-based structural generation process to optimize hardware metrics while maintaining functionality. ", "page_idx": 3}, {"type": "text", "text": "3 Our Approach ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We use reinforcement learning to solve the tree generation for adder and multiplier designs. The environments are modeled as single-player tree generation games: AddGame for adder design and MultGame for multipliers, as illustrated in Fig. 1a. Considering differences among the games, such as action space, we propose two types of agents: one by MCTS [28] and another by PPO [24]. ", "page_idx": 3}, {"type": "text", "text": "3.1 AddGame ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "AddGame is modeled for designing prefix trees in adders and multipliers, as shown in Fig. 3. In this game, the player modifies the structures of given initial prefix trees by basic actions to optimize the adders\u2019 metrics. The state of the game is denoted as $s$ , corresponding to the current prefix tree. In our evaluation, each state $s$ is assessed on two theoretical metrics, level and size, and two practical metrics, delay and area. The player always chooses one action from two kinds of actions: (1) delete a cell $(i,j)$ , (2) add a cell $(i,j)$ , which $(i,j)$ is the cell index as shown in Fig. 1b. A cell $\\left(i,j\\right)\\left(i<j\\right)$ can be deleted if the prefix tree does not have the cell $(i,k)$ subject to $k>j$ and $i>1$ , and all deletable cells are marked in red in Fig. 3 and 5. A cell $(i,j)$ can be added if it does not exist in the prefix tree. All positions where cells can be added are marked with $\\mathbf{\\omega}^{\\star}\\times\\mathbf{\\omega}^{\\star}$ . A legalization operation [17] is always executed after one action to guarantee the feasibility of the prefix tree as Fig. 3. The game aims to maximize the performance score $R(s)$ of the adder $s$ . This score is determined by a weighted combination of delay and area (using level and size when optimizing theoretical metrics). ", "page_idx": 3}, {"type": "text", "text": "Given the large action space, the agent for playing AddGame is based on an improved MCTS method, which has demonstrated its effectiveness in numerous game tasks [21, 29, 30]. Starting from the prefix trees in human-designed adders, the MCTS agent continuously cycles through four phases: selection, expansion, simulation, and backpropagation, and gradually builds a search tree in this process. Each node in the search tree represents one prefix tree. ", "page_idx": 3}, {"type": "text", "text": "In the selection phase, the agent selects the child node with state $s$ that has the highest score $W(s)$ , continuing until it encounters a node that has not been fully expanded. The scores for evaluating nodes are computed by the Upper Confidence bounds applied to Trees (UCT) [31], keeping the balance between exploration and exploitation. In the search tree, each node with the state $s$ stores a visit count $N(s)$ and an action value $V(s)$ . The visit count $N(s)$ records the number of visits to the node $s$ . The action value $V(s)$ is the weighted sum of the best performance score max $R$ and average performance score $\\overline{{R}}$ of all its descendant nodes, which can be formalized as: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\nV(s)=(1-\\beta)\\sum_{s^{\\prime}\\in D(s)}R(s^{\\prime})/|D(s)|\\,+\\,\\beta\\,\\operatorname*{max}_{s^{\\prime}\\in D(s)}R(s^{\\prime})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $D(s)$ represents all descendant nodes of the node $s$ (including $s$ itself), i.e., all generated adders by a sequence of actions from adder $s.\\ |\\cdot|$ gives the number of nodes. $R(s^{\\prime})$ indicates the performance score of the adder of the state $s^{\\prime}$ , which is defined as \u2212Delay \u2212\u03b1Area or \u2212Size. $\\alpha$ and $\\beta$ are sum weights. ", "page_idx": 4}, {"type": "text", "text": "We define the node score $W(s)$ with the state $s$ as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\nW(s)={\\sqrt{\\frac{\\ln(P(s))}{N(s)}}}+c V(s)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $P(s)$ is the parent node of $s$ , $N(\\cdot)$ is visit count function, and $c$ is an adjustable parameter. ", "page_idx": 4}, {"type": "text", "text": "In the expansion phase, a random action is chosen from the unexplored actions available at the node identified in the selection phase and executed. It expands the search tree by adding a new node corresponding to the result after that action. In the simulation phase, a sequence of actions is taken until the performance scores of adders can no longer be improved (in theoretical metrics optimization) or the simulation exceeds the maximum steps (in practical metrics optimization). In the backpropagation phase, the last state $s$ reached in the simulation phase is evaluated to get a performance score $R(s)$ , which is then backpropagated to update the scores of all preceding nodes in the search tree. ", "page_idx": 4}, {"type": "text", "text": "Pruning. To enhance efficiency, we implement pruning techniques to avoid the exploration of unnecessary sub-trees. When optimizing theoretical metrics, we restrict modifications to delete cell ", "page_idx": 4}, {"type": "image", "img_path": "5pnhGedG98/tmp/237c006b10b56230c29d9d578ba10ae3515950102958bd644247d34d61fe5ed9.jpg", "img_caption": ["Figure 3: Method for designing prefix trees with MCTS. Four phases in the search process are executed iteratively, gradually building a search tree. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "actions, as adding cells does not improve the design outcome. Furthermore, we impose an upper limit on the level metric to prevent the creation of structures with excessively high complexity. This upper limit, denoted as $L$ , is set for each MCTS search and is gradually relaxed with each search iteration. ", "page_idx": 4}, {"type": "text", "text": "Two-level Retrieval. We adopt a two-level retrieval strategy to balance synthesis accuracy and computational efficiency. We divide the search into two stages because the full synthesis flow is highly accurate but time-consuming. A faster yet marginally less simulating accurate synthesis flow is employed in the first stage, eliminating the time-intensive steps such as routing. Only the top $K$ adders identified in the first stage undergo full synthesis in the second stage. ", "page_idx": 4}, {"type": "text", "text": "3.2 MultGame ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "MultGame consists of two parts for jointly designing compressor and prefix trees in multipliers, as shown in Fig. 1. The part focused on the prefix tree design is identical to that in AddGame. Meanwhile, the part focused on the compressor tree design involves continually merging bits in partial products through compression actions, as depicted in Fig. 4. This process is similar to some match games like \u20182048\u2019 [32], where items are merged in a specific way to achieve high scores. ", "page_idx": 4}, {"type": "text", "text": "The compressor tree is built from scratch instead of starting from existing solutions for more design flexibility. The game state $s_{t}$ at step $t$ is represented by a vector representing the current compressor tree status. The player chooses one of two actions: (1) using a half adder or (2) using a full adder to compress bits at the action digit, which is defined as the lowest digit containing more than two bits, as indicated in Fig. 4a. Half and full adders compress two or three bits in the $k$ -th digit and generate a carry-out bit in the $(k+1)$ -th digit and a sum bit in the $k$ -th digit. Rough delays for all bits are estimated, assuming a one-unit delay for all basic logic gates, as shown in the dots of Fig. 4a. To minimize the increase in total delay, the bits with the lowest estimated delays are selected as inputs for the adders. The game terminates at step $T$ when all digits have two or fewer bits. A reward $r_{T}$ is computed through the synthesis tools as the negative of the delay, denoted $r_{T}=-\\mathrm{delay}$ . Moreover, a penalty term $-p$ is also applied to $r_{t}$ if the action $a_{t-1}$ uses a half adder, where $1\\leq t\\leq T$ . This penalty reflects that a full adder accepts three input bits (two addend bits and a carry-in bit) and produces two output bits (a sum bit and a carry-out bit), effectively reducing the bit count. In contrast, a half adder only processes two addend bits and outputs two bits, thus not contributing to a reduction in bit count. A half adder\u2019s lack of bit count reduction can lead to more adder modules, increasing the overall module area. ", "page_idx": 4}, {"type": "image", "img_path": "5pnhGedG98/tmp/85ee0dd487cf64d54035d4eca94ed9fc1fb1d73cd5e23ccb7df1d1d43315e2a8.jpg", "img_caption": ["Figure 4: Designing compressor trees with PPO. Three representations are illustrated. (a) Dot notation. Each dot represents an output bit, with the number inside indicating the estimated delay for selecting adder input bits. The agent\u2019s actions involve adding full or half adders to compress the bits until each binary digit contains no more than two bits. The final reward, $r_{T}$ , is defined as the inverse of the delay, encouraging designs with lower delays. (b) Binary bit notation. 0/1 are values of bits for the example multiplication. (c) Logic gate notation. The actual logic gate circuit design for each state. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "We train an RL agent with policy and value networks using the PPO method. Both networks are built by multi-layer perceptions (MLPs) [33] with three layers. The inputs comprise pre-defined features as Table 1, including action digit, max delay, number of half adders, eligible action type, and the estimated delays of bits. The policy and value networks contain $(64,16,2)$ and $(64,8,1)$ neurons in each layer. The last layer of the policy network is connected to a Softmax activation function [34] for choosing actions. ", "page_idx": 5}, {"type": "table", "img_path": "5pnhGedG98/tmp/8438ce16c25c32bd417d9f31446cb7719b0dd5033217800f2d3ca7c851e76e36.jpg", "table_caption": ["Table 1: State features for policy and value network. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "When training, the objective function can be defined as follows for maximizing the game\u2019s cumulative reward: ", "page_idx": 5}, {"type": "equation", "text": "$$\nJ(\\theta)=\\mathbb{E}_{\\tau\\sim\\pi_{\\theta}}\\left[G_{T}\\right]\\;=\\mathbb{E}_{\\tau\\sim\\pi_{\\theta}}\\big[\\sum_{i=0}^{T}\\gamma^{i}r_{i}\\big]\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\tau=(s_{0},a_{0},s_{1},r_{1},a_{1},...,a_{T-1},s_{T},r_{T})$ is a trajectory from the game episode, and $\\pi_{\\theta}$ denotes the policy parameterized by $\\theta$ . $G_{T}$ refers to the cumulative discounted reward from step 0 to step $T$ . The discount factor $\\gamma$ adjusts the emphasis between immediate and future rewards. When implementing the PPO, the objective function for optimizing the policy network can be formalized as: ", "page_idx": 6}, {"type": "equation", "text": "$$\nL(\\theta)=\\hat{\\mathbb{E}}_{t}\\big[\\operatorname*{min}\\big(r_{t}(\\theta)\\hat{A}_{t},\\;\\mathrm{clip}(r_{t}(\\theta),1-\\epsilon,1+\\epsilon)\\hat{A}_{t}\\big)\\big]\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\hat{\\mathbb{E}}_{t}[\\cdot]$ indicates the empirical average over a finite batch of samples, and $r_{t}(\\theta)$ denotes the probability ratio \u03c0\u03b8(at|st) . Here, $\\theta_{\\mathrm{old}}$ is the policy network parameters before the update. $\\hat{A}_{t}=$ $G_{t}-\\hat{V}_{t}$ is an estimation of the advantage function at step $t$ , and $\\hat{V}_{t}$ is the value estimated by the value network. $\\mathrm{clip}(\\cdot,1-\\epsilon,1+\\epsilon)$ is the function restricting results to the interval $[1-\\epsilon,1+\\dot{\\epsilon}]$ . ", "page_idx": 6}, {"type": "text", "text": "Simultaneously, the value network with parameters $\\phi$ is updated by optimizing the following objective function $L(\\phi)\\overset{\\cdot}{=}\\hat{\\mathbb{E}}_{t}\\left[\\mathrm{smooth\\_L1}(G_{t},\\hat{V_{t}})\\right]$ , where smooth_L1 $(\\cdot)$ is the smooth L1 loss function [35]. ", "page_idx": 6}, {"type": "text", "text": "Synthesis Acceleration. In RL-MUL [18], running synthesis tools proved to be a bottleneck, especially for scaling to multipliers with higher bit-widths. To address this, our enhancements to the synthesis flow yield a $10\\times$ speedup in reward computation without sacrificing accuracy. These modifications facilitate the design of multipliers up to 64-bit, expanding from the 16-bit limit in RL-MUL. Enhancements include activating the fast mode in the logical synthesis script and adopting direct code template-based generation of Verilog HDL code from our search results, moving away from the time-consuming EasyMAC [36] tool. ", "page_idx": 6}, {"type": "text", "text": "Co-design Framework. As shown in Fig. 1, we developed a joint design approach to optimize the multiplier\u2019s two primary components: the prefix and compressor trees. Our method involves an iterative process where each round involves optimizing the compressor tree with a fixed prefix tree and searching for an ideal prefix tree that aligns with the optimized compressor. This alternating optimization continues until the computational iterations conclude. ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We use the logic synthesis tool Yosys 0.27 [37] and the physical synthesis tool OpenROAD 2.0 [38] with Nangate45 [39] and ASAP7 [40] libraries to implement experiments. Both synthesis tools are open-sourced for result reproduction. All experiments are run on one GeForce RTX 3090 GPU and one AMD Ryzen 9 5950X 16-core CPU. Detailed settings are in Appendix A.3 and A.5. All designed modules have successfully undergone functional verification. ", "page_idx": 6}, {"type": "text", "text": "4.1 Adder Design ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Theoretical Evaluation. As illustrated in Fig. 1b, prefix tree structures define the technologyindependent theoretical metrics of level and size. Empirically, optimizing the level usually presents more challenges than size. Therefore, we set the search objective when optimizing theoretical metrics to find the optimal size for each specified upper bound level $L$ . We begin our search with the Sklansky adder [12], which has a theoretical minimum level of $\\log_{2}N$ . Starting with $L$ set at this minimum, we incrementally increase it for each new iteration, using the smallest prefix tree identified in the previous round as the initial state. We limited the number of steps to $4^{\\circ}\\times10^{5}$ for each search iteration. For baselines, the results were obtained directly from the respective original publications. Table 2 shows that our method surpasses the state-of-the-art designs in [14]. Some discovered adder structures are presented in Fig. 5. Despite the exponentially growing search space, our MCTS method can enhance 128-bit adders, surpassing the designs from optimization-based methods. Notably, guided by Snir\u2019s theoretical lower bound for size at a given level [41], we were the first to discover an optimal 128-bit adder with 10 levels and a size of 244. ", "page_idx": 6}, {"type": "text", "text": "Practical Evaluation. Practical metrics, including the delay and area of hardware modules, are computed through synthesis tools for evaluation. We run 1000 full syntheses for adders in each method to ensure a fair comparison. Our ArithTreeRL method begins each search from one of three adders: Sklansky [12], Brent-Kung [43], and ripple-carry [44]. A two-level retrieval strategy is implemented by dividing the search into two stages: (1) 5000 fast syntheses. (2) 500 full syntheses with the top 500 adders selected from the first stage. Efficiency tests show that one full flow\u2019s computational load equals 10 fast flows. Thus, the proposed strategy achieves the same computational volume with 1000 full flows. The state-of-the-art method PrefixRL [17] is implemented with optimal settings. In our results in Fig. 6, each prefix adder is represented by a 2D point based on its delay and area. It shows the significant improvement achieved when our two-level retrieval strategy is used in the PrefixRL method due to efficiency improvements that facilitate exploring an expanded sample corpus. Moreover, employing the MCTS method can lead to the discovery of more superior adders because this method effectively navigates through problems with vast state spaces, utilizing information stored during the search process. Overall, our approach can reduce the delay or area of adders by up to $26\\%$ and $30\\%$ , respectively, compared with PrefixRL, while maintaining the computational amount. ", "page_idx": 6}, {"type": "table", "img_path": "5pnhGedG98/tmp/2bb267d27c75d7d976887b75f05456f7cc76f7c21612c69374b639c66e9cba3a.jpg", "table_caption": ["Table 2: Comparisons of discovered adders in size and area. Smaller sizes are preferable. "], "table_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "5pnhGedG98/tmp/8019b741a17c9cc4ef68722155a607af62455527d88362ce7bf999d88a4fae1b.jpg", "img_caption": ["Figure 5: Some first discovered prefix trees for 128-bit adders with the smallest sizes. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "5pnhGedG98/tmp/97c28cac7338d3fffd9964f249521c916e0616514d438cb4c5f6db00fd2f9607.jpg", "img_caption": ["Figure 6: Comparison of adders in delay and area. Each point represents one adder and line segments connect Pareto-optimal adders. \u2018PrefixRL (2-level retr.)\u2019 is the raw PrefixRL method improved by our two-level retrieval strategy. Sklansky, Brent-Kung, and Kogge-Stone refer to human-designed adders. ArithTreeRL can significantly improve the delay and area, particularly for high-bit adders. Furthermore, it can discover adders with minimal delays. Our two-level retrieval strategy can effectively find superior designs. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Visualization. The scores of the first actions after 400 search steps when optimizing the theoretical metrics are visualized as heatmaps in Fig. 7. In the selection phase, the action with the highest score is chosen. For example, the first action for the 8-bit adder is to delete the $(5,7)$ cell with the highest score because this reduces the size of the adder. On the contrary, the action with the lowest score is to add the $(4,7)$ cell because it augments both size and level. ", "page_idx": 7}, {"type": "image", "img_path": "5pnhGedG98/tmp/379306b7ec3ef85fb2308ccf10571d2e9e8cd1d6054005bc558b48fabeccf547.jpg", "img_caption": ["Figure 7: Heatmap for first action scores. The actions with the highest and lowest scores are marked. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Accuracy of Fast Flow. The time-consuming routing phase is removed in the fast flow of the two-level strategy. To evaluate the impact of this simplification, we tested the simulation accuracy of the fast flow against the full flow. The results in Table 3 indicate that the fast flow can still achieve an utterly accurate area estimation and over $95\\%$ accurate delay. Therefore, the fast synthesis flow can help improve efficiency without significantly losing accuracy. ", "page_idx": 8}, {"type": "table", "img_path": "5pnhGedG98/tmp/9a56accc0586fb3a82d0d79e0df1c60b3b1376e08042d8c91720606d10638bee.jpg", "table_caption": ["Table 3: Accuracy of fast synthesis flow. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.2 Multiplier Design ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Practical Evaluation. Given the lack of a commonly adopted theoretical metric for multipliers, we use practical metrics for evaluation. Our multiplier design utilizes a co-design framework with three iterative search rounds, incorporating 900 steps for the compressor tree and 100 for the prefix tree each round. This yields 3000 steps, consistent with the search steps of other baseline methods in our experiments. As shown in Fig. 8 and Appendix Fig. 14, we compared the effectiveness of our method with several baselines, including the human-designed Wallace multiplier [13], optimization-based methods including GOMIL [16] and SA [45], the default multiplier given in the synthesis tool, and the learning-based method RL-MUL [18]. In our evaluation, we assessed the multipliers\u2019 performance by adjusting the expected delay parameter in the synthesis process. Subsequently, the resulting areas of each multiplier at different delays are depicted as a segmented line. Consistent with the RLMUL [18] assessment approach, each method selects an optimal multiplier for comparison. Results for Wallace [13], GOMIL [16], SA [45], and RL-MUL [18] in 8/16 bits are referenced from the RL-MUL work. RL-MUL method is reproduced and tested in 32/64 bits. The results show that the codesign method, ArithTreeRL, outperforms the synthesis tool\u2019s baselines and default multipliers. This is because of the co-design framework, the restructured MultGame, and the improved synthesis flow. It can achieve second-best results even when only optimizing the compressor tree. Compared with the state-of-the-art RL-MUL method, our method can reduce the delay by up to $33\\%$ and the area by $45\\%$ . Furthermore, our method can reduce the delay of the default multipliers used in the Yosys tool [37] by up to $16\\%$ and the area by $35\\%$ . We also report the delays and areas in Table 4. Our method consistently achieves minimal delays for the delay minimization. When optimizing for a trade-off (delay $+\\ 0.001$ area), our approach achieves optimal or comparable results. Also, The multipliers designed by $45\\mathrm{nm}$ technology are compatible with the $7\\mathrm{nm}$ [40] without any modifications. ", "page_idx": 8}, {"type": "text", "text": "Efficiency. Due to the time-consuming nature of the full synthesis flow, we developed a synthesis flow that is over $10\\times$ faster while maintaining high simulation accuracy for adder design, as discussed in the method. The efficiency is shown in Fig. 9a. Additionally, we optimized the logic synthesis and HDL code generation processes in the synthesis flow for multiplier design. According to Fig. 9b, our improved fast flow can accelerate the process up to $20\\times$ . ", "page_idx": 8}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Designing adder and multiplier modules is a fundamental and crucial task in computer science. We first model this task as a tree-generation process, conceptualizing it as a sequential decision-making game. Then, we propose a reinforcement learning method to solve it, facilitating a scalable and efficient search for globally optimal designs. Through extensive experiments, our approach achieves state-of-the-art performance for adders and multipliers in terms of delay and area within the same computational resources. Moreover, our method has demonstrated transferability, as the designs we discovered can be applied to more advanced technology processes. This enhancement in basic arithmetic modules optimizes hardware performance and size, showing significant potential for boosting computationally intensive fields. ", "page_idx": 8}, {"type": "table", "img_path": "5pnhGedG98/tmp/4a0b6cf8db0215e3a450d839c5ff1ff0864fc32a254e3bdc5bafd511e4494235.jpg", "table_caption": ["Table 4: Numerical comparison of multipliers in delay (ns) and area $(\\upmu\\mathrm{m}^{2})$ . (45nm) "], "table_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "5pnhGedG98/tmp/aa8dc022f5f32f09de41ed099d62054919ac5f90e443c16314d1c325f3318e6a.jpg", "img_caption": ["Figure 8: Comparison of multipliers. The designs were tested in $45\\mathrm{nm}$ and $7\\mathrm{nm}$ . Each segmented line represents the performance of one multiplier under different timing constraints. \u2018Method (our flow)\u2019 are methods with our improved flow. The \u2018Default\u2019 multipliers are those generated by the synthesis tool by default. \u2018ArithTreeRL\u2019 is our co-design method combining PPO and MCTS, while \u2018PPO (our)\u2019 optimizes only the compressor tree. We apply $45\\mathrm{nm}$ designs to the $7\\mathrm{nm}$ library without modifications, showcasing the transferability. "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "5pnhGedG98/tmp/5f2e7d0f00797b68f2edd277cdca23c280eb62679c70f2081d035fd7df77e7f3.jpg", "img_caption": ["Figure 9: Design flow time consumption. (average of 1000 runs) "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Limitations. This paper focuses exclusively on designing and optimizing adder and multiplier modules, which are fundamental components in computational systems. It does not explore other basic elements, such as exponentiation or more complex arithmetic units. However, our method is naturally extendable to other arithmetic operations, such as exponentiation. Future research could explore these extensions to unlock further designs across various hardware components. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We gratefully acknowledge Ronghao Lin of Sun Yat-sen University for his assistance with the introduction video. This paper is partially supported by the National Key R&D Program of China No.2022ZD0161000 and the General Research Fund of Hong Kong No.17200622 and 17209324. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] P. Yao, H. Wu, B. Gao, J. Tang, Q. Zhang, W. Zhang, J. J. Yang, and H. Qian, \u201cFully hardware-implemented memristor convolutional neural network,\u201d Nature, vol. 577, no. 7792, pp. 641\u2013646, 2020.   \n[2] M. Haseeb and F. Saeed, \u201cHigh performance computing framework for tera-scale database search of mass spectrometry data,\u201d Nature computational science, vol. 1, no. 8, pp. 550\u2013561, 2021.   \n[3] R. Or\u00fas, S. Mugel, and E. Lizaso, \u201cQuantum computing for finance: Overview and prospects,\u201d Reviews in Physics, vol. 4, p. 100028, 2019.   \n[4] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray et al., \u201cTraining language models to follow instructions with human feedback,\u201d Advances in Neural Information Processing Systems (NeurIPS), vol. 35, pp. 27 730\u201327 744, 2022.   \n[5] J. Min, S. Demchyshyn, J. R. Sempionatto, Y. Song, B. Hailegnaw, C. Xu, Y. Yang, S. Solomon, C. Putz, L. E. Lehner et al., \u201cAn autonomous wearable biosensor powered by a perovskite solar cell,\u201d Nature Electronics, pp. 1\u201312, 2023.   \n[6] M. T. Bohr and I. A. Young, \u201cCmos scaling trends and beyond,\u201d IEEE Micro, vol. 37, no. 6, pp. 20\u201329, 2017.   \n[7] Y. Taur, \u201cCmos design near the limit of scaling,\u201d IBM Journal of Research and Development, vol. 46, no. 2.3, pp. 213\u2013222, 2002.   \n[8] K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep residual learning for image recognition,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR). IEEE, 2016, pp. 770\u2013778.   \n[9] D. P. Rodgers, \u201cImprovements in multiprocessor system design,\u201d ACM SIGARCH Computer Architecture News, vol. 13, no. 3, pp. 225\u2013231, 1985.   \n[10] T. Hiramoto, \u201cFive nanometre cmos technology,\u201d Nature Electronics, vol. 2, no. 12, pp. 557\u2013558, 2019.   \n[11] S. Salahuddin, K. Ni, and S. Datta, \u201cThe era of hyper-scaling in electronics,\u201d Nature Electronics, vol. 1, no. 8, pp. 442\u2013450, 2018.   \n[12] J. Sklansky, \u201cConditional-sum addition logic,\u201d IEEE Transactions on Electronic computers, pp. 226\u2013231, 1960.   \n[13] C. S. Wallace, \u201cA suggestion for a fast multiplier,\u201d IEEE Transactions on electronic Computers, pp. 14\u201317, 1964.   \n[14] S. Roy, M. Choudhury, R. Puri, and D. Z. Pan, \u201cTowards optimal performance-area trade-off in adders by synthesis of parallel prefix structures,\u201d in Proceedings of the Annual Design Automation Conference (DAC). ACM/IEEE, 2013, pp. 1\u20138.   \n[15] \u2014\u2014, \u201cTowards optimal performance-area trade-off in adders by synthesis of parallel prefix structures,\u201d IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems (TCAD), vol. 33, no. 10, p. 1517, 2014.   \n[16] W. Xiao, W. Qian, and W. Liu, \u201cGomil: Global optimization of multiplier by integer linear programming,\u201d in Design, Automation & Test in Europe Conference & Exhibition (DATE). IEEE, 2021, pp. 374\u2013379.   \n[17] R. Roy, J. Raiman, N. Kant, I. Elkin, R. Kirby, M. Siu, S. Oberman, S. Godil, and B. Catanzaro, \u201cPrefixrl: Optimization of parallel prefix circuits using deep reinforcement learning,\u201d in Proceedings of the Annual Design Automation Conference (DAC). ACM/IEEE, 2021, pp. 853\u2013858.   \n[18] D. Zuo, Y. Ouyang, and Y. Ma, \u201cRL-MUL: Multiplier design optimization with deep reinforcement learning,\u201d in Proceedings of the Annual Design Automation Conference (DAC). ACM/IEEE, 2023, pp. 1\u20138.   \n[19] H. Geng, Y. Ma, Q. Xu, J. Miao, S. Roy, and B. Yu, \u201cHigh-speed adder design space exploration via graph neural processes,\u201d IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems (TCAD), vol. 41, no. 8, pp. 2657\u20132670, 2021.   \n[20] Y. Ma, S. Roy, J. Miao, J. Chen, and B. Yu, \u201cCross-layer optimization for high speed adders: A pareto driven machine learning approach,\u201d IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems (TCAD), vol. 38, no. 12, pp. 2298\u20132311, 2018.   \n[21] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot et al., \u201cMastering the game of go with deep neural networks and tree search,\u201d nature, vol. 529, no. 7587, pp. 484\u2013489, 2016.   \n[22] S. Palnitkar, Verilog HDL: a guide to digital design and synthesis. Prentice Hall Professional, 2003, vol. 1.   \n[23] C. B. Browne, E. Powley, D. Whitehouse, S. M. Lucas, P. I. Cowling, P. Rohlfshagen, S. Tavener, D. Perez, S. Samothrakis, and S. Colton, \u201cA survey of monte carlo tree search methods,\u201d IEEE Transactions on Computational Intelligence and AI in games, vol. 4, no. 1, pp. 1\u201343, 2012.   \n[24] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, \u201cProximal policy optimization algorithms,\u201d arXiv preprint arXiv:1707.06347, 2017.   \n[25] P. M. Kogge and H. S. Stone, \u201cA parallel algorithm for the efficient solution of a general class of recurrence equations,\u201d IEEE Transactions on Computers (TC), vol. 100, no. 8, pp. 786\u2013793, 1973.   \n[26] R. E. Ladner and M. J. Fischer, \u201cParallel prefix computation,\u201d Journal of the ACM (JACM), vol. 27, no. 4, pp. 831\u2013838, 1980.   \n[27] C. H. Roth Jr, L. L. Kinney, and E. B. John, Fundamentals of logic design. Cengage Learning, 2020.   \n[28] M. \u00b4Swiechowski, K. Godlewski, B. Sawicki, and J. Ma\u00b4ndziuk, \u201cMonte carlo tree search: A review of recent modifications and applications,\u201d Artificial Intelligence Review, vol. 56, no. 3, pp. 2497\u20132562, 2023.   \n[29] O. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Mathieu, A. Dudzik, J. Chung, D. H. Choi, R. Powell, T. Ewalds, P. Georgiev et al., \u201cGrandmaster level in starcraft ii using multi-agent reinforcement learning,\u201d Nature, vol. 575, no. 7782, pp. 350\u2013354, 2019.   \n[30] A. Fawzi, M. Balog, A. Huang, T. Hubert, B. Romera-Paredes, M. Barekatain, A. Novikov, F. J. R Ruiz, J. Schrittwieser, G. Swirszcz et al., \u201cDiscovering faster matrix multiplication algorithms with reinforcement learning,\u201d Nature, vol. 610, no. 7930, pp. 47\u201353, 2022.   \n[31] L. Kocsis and C. Szepesv\u00e1ri, \u201cBandit based monte-carlo planning,\u201d in European conference on machine learning (ECML). Springer, 2006, pp. 282\u2013293.   \n[32] A. Dedieu and J. Amar, \u201cDeep reinforcement learning for 2048,\u201d in Conference on Neural Information Processing Systems (NeurIPS), 2017.   \n[33] F. Murtagh, \u201cMultilayer perceptrons for classification and regression,\u201d Neurocomputing, vol. 2, no. 5-6, pp. 183\u2013197, 1991.   \n[34] C. M. Bishop and N. M. Nasrabadi, Pattern recognition and machine learning. Springer, 2006, vol. 4.   \n[35] R. Girshick, \u201cFast r-cnn,\u201d in Proceedings of the IEEE international conference on computer vision (ICCV). IEEE, 2015, pp. 1440\u20131448.   \n[36] J. Zhang, Q. Gao, Y. Guo, B. Shi, and G. Luo, \u201cEasymac: design exploration-enabled multiplieraccumulator generator using a canonical architectural representation,\u201d in Proceedings of Asia and South Pacific Design Automation Conference (ASP-DAC). IEEE, 2022, pp. 647\u2013653.   \n[37] C. Wolf, \u201cYosys open synthesis suite,\u201d 2016.   \n[38] T. Ajayi and D. Blaauw, \u201cOpenroad: Toward a self-driving, open-source digital layout implementation tool chain,\u201d in Proceedings of Government Microcircuit Applications and Critical Technology Conference, 2019.   \n[39] I. NanGate, \u201cNanGate FreePDK45 open cell library,\u201d 2008.   \n[40] L. T. Clark, V. Vashishtha, L. Shifren, A. Gujja, S. Sinha, B. Cline, C. Ramamurthy, and G. Yeric, \u201cASAP7: A 7-nm FinFET predictive process design kit,\u201d Microelectronics Journal, vol. 53, 2016.   \n[41] M. Snir, \u201cDepth-size trade-offs for parallel prefix computation,\u201d Journal of Algorithms, vol. 7, no. 2, pp. 185\u2013201, 1986.   \n[42] T. Matsunaga and Y. Matsunaga, \u201cArea minimization algorithm for parallel prefix adders under bitwise delay constraints,\u201d in Proceedings of the 17th ACM Great Lakes symposium on VLSI, 2007, pp. 435\u2013440.   \n[43] Brent and Kung, \u201cA regular layout for parallel adders,\u201d IEEE Transactions on Computers (TC), vol. 100, no. 3, pp. 260\u2013264, 1982.   \n[44] P. Behrooz, \u201cComputer arithmetic: Algorithms and hardware designs,\u201d Oxford University Press, vol. 19, pp. 512 583\u2013512 585, 2000.   \n[45] P. J. Van Laarhoven, E. H. Aarts, P. J. van Laarhoven, and E. H. Aarts, Simulated annealing. Springer, 1987.   \n[46] J. K. Ousterhout et al., Tcl: An embeddable command language. University of California, Berkeley, Computer Science Division, 1989.   \n[47] S. D. Compiler, \u201cSynopsys design compiler,\u201d Pages/default. aspx, 2016.   \n[48] N. H. Weste and D. Harris, CMOS VLSI design: a circuits and systems perspective. Pearson Education India, 2015.   \n[49] T. Han, D. A. Carlson, and S. P. Levitan, VLSI DESIGN OF HIGH-SPEED, LOW-AREA ADDITION CIRCUITRY. IEEE, 1987.   \n[50] J. Liu, S. Zhou, H. Zhu, and C.-K. Cheng, \u201cAn algorithmic approach for generic parallel adders,\u201d in International Conference on Computer Aided Design (ICCAD). IEEE, 2003, pp. 734\u2013740.   \n[51] J. P. Fishburn, \u201cA depth-decreasing heuristic for combinational logic: or how to convert a ripple-carry adder into a carry-lookahead adder or anything in-between,\u201d in Proceedings of the Annual Design Automation Conference (DAC). ACM/IEEE, 1991, pp. 361\u2013364.   \n[52] R. Zimmermann, \u201cNon-heuristic optimization and synthesis of parallel-prefix adders,\u201d in proc. of IFIP workshop. Citeseer, 1996.   \n[53] H. Zhu, C.-K. Cheng, and R. Graham, \u201cConstructing zero-deficiency parallel prefix adder of minimum depth,\u201d in Proceedings of Asia and South Pacific Design Automation Conference (ASP-DAC), 2005, pp. 883\u2013888.   \n[54] F. S. Melo, \u201cConvergence of q-learning: A simple proof,\u201d Institute Of Systems and Robotics, Tech. Rep, pp. 1\u20134, 2001.   \n[55] L. Dadda, \u201cSome schemes for parallel multipliers,\u201d Alta frequenza, vol. 34, pp. 349\u2013356, 1965.   \n[56] W. J. Townsend, E. E. Swartzlander Jr, and J. A. Abraham, \u201cA comparison of dadda and wallace multiplier delays,\u201d in Advanced signal processing algorithms, architectures, and implementations XIII, vol. 5205. SPIE, 2003, pp. 552\u2013560.   \n[57] D. J. Mankowitz, A. Michi, A. Zhernov, M. Gelmi, M. Selvi, C. Paduraru, E. Leurent, S. Iqbal, J.-B. Lespiau, A. Ahern et al., \u201cFaster sorting algorithms discovered using deep reinforcement learning,\u201d Nature, vol. 618, no. 7964, pp. 257\u2013263, 2023.   \n[58] J. Schrittwieser, I. Antonoglou, T. Hubert, K. Simonyan, L. Sifre, S. Schmitt, A. Guez, E. Lockhart, D. Hassabis, T. Graepel et al., \u201cMastering atari, go, chess and shogi by planning with a learned model,\u201d Nature, vol. 588, no. 7839, pp. 604\u2013609, 2020.   \n[59] Z. Wang, J. Wang, Q. Zhou, B. Li, and H. Li, \u201cSample-efficient reinforcement learning via conservative model-based actor-critic,\u201d in Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), vol. 36, no. 8, 2022, pp. 8612\u20138620.   \n[60] M. Hessel, J. Modayil, H. Van Hasselt, T. Schaul, G. Ostrovski, W. Dabney, D. Horgan, B. Piot, M. Azar, and D. Silver, \u201cRainbow: Combining improvements in deep reinforcement learning,\u201d in Proceedings of the AAAI conference on artificial intelligence (AAAI), vol. 32, no. 1, 2018.   \n[61] R. Yang, J. Wang, Z. Geng, M. Ye, S. Ji, B. Li, and F. Wu, \u201cLearning task-relevant representations for generalization via characteristic functions of reward sequence distributions,\u201d in Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (SIGKDD), 2022, pp. 2242\u20132252.   \n[62] J. Wang, R. Yang, Z. Geng, Z. Shi, M. Ye, Q. Zhou, S. Ji, B. Li, Y. Zhang, and F. Wu, \u201cGeneralization in visual reinforcement learning with the reward sequence distribution,\u201d arXiv preprint arXiv:2302.09601, 2023.   \n[63] Z. Wang, T. Pan, Q. Zhou, and J. Wang, \u201cEfficient exploration in resource-restricted reinforcement learning,\u201d in Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), vol. 37, no. 8, 2023, pp. 10 279\u201310 287.   \n[64] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. Riedmiller, \u201cPlaying atari with deep reinforcement learning,\u201d arXiv preprint arXiv:1312.5602, 2013.   \n[65] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra, \u201cContinuous control with deep reinforcement learning,\u201d arXiv preprint arXiv:1509.02971, 2015.   \n[66] R. S. Sutton, D. McAllester, S. Singh, and Y. Mansour, \u201cPolicy gradient methods for reinforcement learning with function approximation,\u201d Conference on Neural Information Processing Systems (NeurIPS), vol. 12, 1999.   \n[67] A. Hosny, S. Hashemi, M. Shalan, and S. Reda, \u201cDrills: Deep reinforcement learning for logic synthesis,\u201d in 2020 25th Asia and South Pacific Design Automation Conference (ASP-DAC). IEEE, 2020, pp. 581\u2013586.   \n[68] K. Zhu, M. Liu, H. Chen, Z. Zhao, and D. Z. Pan, \u201cExploring logic optimizations with reinforcement learning and graph convolutional network,\u201d in Proceedings of the 2020 ACM/IEEE Workshop on Machine Learning for CAD (MLCAD), 2020, pp. 145\u2013150.   \n[69] Z. Wang, J. Wang, D. Zuo, J. Yunjie, X. Xia, Y. Ma, H. Jianye, M. Yuan, Y. Zhang, and F. Wu, \u201cA hierarchical adaptive multi-task reinforcement learning framework for multiplier circuit design,\u201d in Fortyfirst International Conference on Machine Learning (ICML), 2024.   \n[70] Z. Wang, L. Chen, J. Wang, Y. Bai, X. Li, X. Li, M. Yuan, H. Jianye, Y. Zhang, and F. Wu, \u201cA circuit domain generalization framework for efficient logic synthesis in chip design,\u201d in Forty-first International Conference on Machine Learning (ICML), 2024.   \n[71] D. Niu, Y. Dong, Z. Jin, C. Zhang, Q. Li, and C. Sun, \u201cOssp-pta: An online stochastic stepping policy for pta on reinforcement learning,\u201d IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems (TCAD), 2023.   \n[72] Z. Jin, H. Pei, Y. Dong, X. Jin, X. Wu, W. W. Xing, and D. Niu, \u201cAccelerating nonlinear dc circuit simulation with reinforcement learning,\u201d in Proceedings of the 59th ACM/IEEE Design Automation Conference (DAC), 2022, pp. 619\u2013624.   \n[73] A. Mirhoseini, A. Goldie, M. Yazgan, J. W. Jiang, E. Songhori, S. Wang, Y.-J. Lee, E. Johnson, O. Pathak, A. Nazi et al., \u201cA graph placement methodology for fast chip design,\u201d Nature, vol. 594, no. 7862, pp. 207\u2013212, 2021.   \n[74] R. Cheng and J. Yan, \u201cOn joint learning for solving placement and routing in chip design,\u201d Advances in Neural Information Processing Systems (NeurIPS), vol. 34, pp. 16 508\u201316 519, 2021.   \n[75] Y. Lai, Y. Mu, and P. Luo, \u201cMaskplace: Fast chip placement via reinforced visual representation learning,\u201d Advances in Neural Information Processing Systems (NeurIPS), vol. 35, pp. 24 019\u201324 030, 2022.   \n[76] Y. Lai, J. Liu, Z. Tang, B. Wang, J. Hao, and P. Luo, \u201cChipformer: Transferable chip placement via offilne decision transformer,\u201d in ICML. PMLR, 2023, pp. 18 346\u201318 364.   \n[77] Y. Shi, K. Xue, L. Song, and C. Qian, \u201cMacro placement by wire-mask-guided black-box optimization,\u201d Advances in Neural Information Processing Systems (NeurIPS), 2023.   \n[78] R. Zhong, J. Ye, Z. Tang, S. Kai, M. Yuan, J. Hao, and J. Yan, \u201cPreroutgnn for timing prediction with order preserving partition: Global circuit pre-training, local delay learning and attentional cell modeling,\u201d in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 38, no. 15, 2024, pp. 17 087\u201317 095.   \n[79] Z. Geng, J. Wang, Z. Liu, S. Xu, Z. Tang, M. Yuan, H. Jianye, Y. Zhang, and F. Wu, \u201cReinforcement learning within tree search for fast macro placement,\u201d in Forty-first International Conference on Machine Learning (ICML), 2024.   \n[80] Z. Wang, Z. Geng, Z. Tu, J. Wang, Y. Qian, Z. Xu, Z. Liu, S. Xu, Z. Tang, S. Kai et al., \u201cBenchmarking end-to-end performance of ai-based chip placement algorithms,\u201d arXiv preprint arXiv:2407.15026, 2024.   \n[81] H. Chen, K.-C. Hsu, W. J. Turner, P.-H. Wei, K. Zhu, D. Z. Pan, and H. Ren, \u201cReinforcement learning guided detailed routing for custom circuits,\u201d in Proceedings of the 2023 International Symposium on Physical Design (ISPD), 2023, pp. 26\u201334.   \n[82] T. Qu, Y. Lin, Z. Lu, Y. Su, and Y. Wei, \u201cAsynchronous reinforcement learning framework for net order exploration in detailed routing,\u201d in 2021 Design, Automation & Test in Europe Conference & Exhibition (DATE). IEEE, 2021, pp. 1815\u20131820.   \n[83] X. Du, C. Wang, R. Zhong, and J. Yan, \u201cHubrouter: Learning global routing via hub generation and pin-hub connection,\u201d Advances in Neural Information Processing Systems (NeurIPS), vol. 36, 2024.   \n[84] S. A. Beheshti-Shirazi, A. Vakil, S. Manoj, I. Savidis, H. Homayoun, and A. Sasan, \u201cA reinforced learning solution for clock skew engineering to reduce peak current and ir drop,\u201d in Proceedings of the 2021 on Great Lakes Symposium on VLSI, 2021, pp. 181\u2013187.   \n[85] H. Wang, K. Wang, J. Yang, L. Shen, N. Sun, H.-S. Lee, and S. Han, \u201cGcn-rl circuit designer: Transferable transistor sizing with graph neural networks and reinforcement learning,\u201d in Proceedings of the Annual Design Automation Conference (DAC). ACM/IEEE, 2020, pp. 1\u20136.   \n[86] Y.-C. Lu, S. Nath, V. Khandelwal, and S. K. Lim, \u201cRl-sizer: Vlsi gate sizing for timing optimization using deep reinforcement learning,\u201d in Proceedings of the Annual Design Automation Conference (DAC). ACM/IEEE, 2021, pp. 733\u2013738.   \n[87] Z. Shi, M. Li, S. Khan, L. Wang, N. Wang, Y. Huang, and Q. Xu, \u201cDeeptpi: Test point insertion with deep reinforcement learning,\u201d in 2022 IEEE International Test Conference (ITC). IEEE, 2022, pp. 194\u2013203. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A Method Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.1 Level Upper Bound ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "When optimizing adders in theoretical metrics, the search process is stratified based on a series of incremental level upper bounds, $L$ . The initial bound is set to $\\log_{2}N$ and is incrementally raised in subsequent stages. For each new stage, the starting configuration state is the adder design with the minimum size obtained from the previous stage\u2019s search, as illustrated in Fig. 10. ", "page_idx": 15}, {"type": "image", "img_path": "5pnhGedG98/tmp/0123a35a23e6fe2c737c5d24199cccc8e25356c342bca4425874afa60e9786c8.jpg", "img_caption": ["Figure 10: Level upper bound $L$ for optimizing theoretical metrics of adders. The example is for 64-bit adder design. The search is divided into stages, and the level upper bound $L$ increases one at a time. The initial state for each search is set to the best adder found in the last search iteration. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "A.2 Two-Level Retrieval ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In our two-level retrieval strategy, we implement a fast synthesis flow with minimal loss of precision. In the fast flow, we keep all other steps, including logic synthesis, clock tree synthesis, and placement, but remove the most expensive routing step. According to our efficiency test in Fig. 9, our fast synthesis flow without the routing step can speed up more than ten times. At the same time, the fast flow can still achieve highly accurate area measurements and $95\\%$ accurate delay estimations as detailed in Table 3. Thus, the fast synthesis flow can help search for as many adders as possible without losing accuracy. At the end of the first stage of two-level retrieval, we use the coordinate (area, delay) as the representative points for adders and compute all distances from these points to the Pareto boundary. We sort the distances in ascending order and use the $K$ -th distance $D$ as the threshold for selecting the adders to the second stage. As shown in Fig. 11, the $K$ adders with the shortest distances to the Pareto boundary\u2014constituting the top $10\\%$ in our efficiency settings\u2014will be selected for full synthesis execution. ", "page_idx": 15}, {"type": "text", "text": "A.3 Synthesis Scripts ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The logical and physical synthesis process for Yosys and OpenROAD is implemented using the Tcl scripting language [46]. We provide the complete Tcl scripts used for the logical synthesis in Fig. 12. ", "page_idx": 15}, {"type": "text", "text": "A.4 Cache, Save, and Recover Design ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Our configuration allows the prefix and compressor trees to be easily saved and restored. The prefix tree is stored as an upper triangular matrix $A_{N\\times N}$ , where a cell $(i,j)$ is marked with $A_{i,j}=1$ if it exists; otherwise, $A_{i,j}=0$ if it does not. The compressor tree is represented by a variable-length sequence ${\\cal S}=\\{a_{0},a_{1},\\stackrel{\\sim}{a_{2}},\\dots,a_{T-1}\\}$ with each $a_{i}\\in0,1$ . Here, $a_{i}=0$ represents the addition of a full adder, and $a_{i}=1$ signifies the addition of a half adder. In the context of our game modeling, the matrix $A$ and the sequence $S$ together can completely reconstruct the prefix and compressor trees, respectively. Furthermore, both structures can be serialized into strings. These strings are then ", "page_idx": 15}, {"type": "image", "img_path": "5pnhGedG98/tmp/4dde394f89f84c550eae9402a601d55661e83b5d6a238020b224bb7849165566.jpg", "img_caption": ["Figure 11: Select adders in two-level retrieval. After the first stage of the two-level retrieval process, each adder is represented by a 2D point based on its delay and area. When selecting the top $K$ adders for the second stage, we sort them according to their distances from these points to the Pareto boundary. The $K$ adders with the smallest distances are selected. The threshold distance, denoted as $D$ , is defined by the distance of the $K$ -th adder to the Pareto boundary. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "5pnhGedG98/tmp/5959d9b02ffa3a0b52b209ec67f13fd9815314993290d33151986432844d1632.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "processed through a hash function to generate fixed-length values that serve as keys in our cache.   \nThis cache stores the results of previous syntheses, which helps in avoiding redundant synthesis runs. ", "page_idx": 16}, {"type": "text", "text": "A.5 Hyperparameter ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Our hyperparameter configuration can be found in Table 5. ", "page_idx": 16}, {"type": "table", "img_path": "5pnhGedG98/tmp/b235f90318ce839205ba2da700c44028c31e6e80bb831b5c92fded12d346ff73.jpg", "table_caption": ["Figure 12: Scripts for logical synthesis. ", "constraint file: abc_constr ", "Table 5: Hyperparameter Configuration "], "table_footnote": ["\\* 0.01 for designing multipliers, 0 (ripple-carry adder as initial state) and 0.001 (others) for designing adders, where the unit of delay is $n s$ , and the unit of area is $\\bar{\\mu}m^{2}$ . "], "page_idx": 16}, {"type": "text", "text": "A.6 Input Selection in Compressor Tree ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In the compressor tree design, both half and full adders are utilized. When assigning input bits to a full or half adder, we prioritize the bits with minimal estimated delays. For instance, consider the case where we are selecting inputs for action $a_{0}$ , and the available input bits have delays $\\{0,0,0,0,1\\}$ . In this situation, the three bits with a delay of 0 would be chosen as inputs for a full adder to minimize the overall delay. The rationale behind this is that adders introduce additional delays, and our objective is to minimize the maximum delay across all bits. A more nuanced strategy is employed when inputs are fed into a full adder: the bit with the highest delay out of the three is connected to the carry input. For example, given input bits with delays $\\{0,0,1\\}$ , the bit with a delay of 1 would be connected to the carry input of the full adder. This strategy is adopted because the delay from the carry input to the output bits involves only two logic gates, which is faster than the three logic gates\u2019 delay from the addend inputs to the outputs. ", "page_idx": 17}, {"type": "text", "text": "A.7 Strategy for Searching Multipliers ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In the search process, each multiplier is tested on two boundary expected delay parameters (50 and $2\\times10^{5}$ ). The average delay and area are then calculated from the results obtained at these two boundary conditions. The performance score for each multiplier is the weighted sum of the average delay and area. The multiplier with the highest score is selected for final evaluation. ", "page_idx": 17}, {"type": "text", "text": "A.8 Module Functionality Verification ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Each module undergoes a rigorous testing protocol comprising 100 addition or multiplication operations to ensure the correctness and reliability of its functionality. For specific test bench details, please refer to our code. ", "page_idx": 17}, {"type": "text", "text": "B Supplementary Results ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "B.1 Adder Design ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In addition to Fig. 5, we present some novel designs of the 128-bit adder discovered by our method in Fig. 13, which achieve minimal sizes under the given levels. ", "page_idx": 17}, {"type": "text", "text": "As illustrated in Fig. 6, we concurrently present the timeline for optimizing key performance metrics in the design of the adder. Our approach ensures sustained efficiency throughout the design process. The primary bottleneck remains in the simulation phase. ", "page_idx": 17}, {"type": "table", "img_path": "5pnhGedG98/tmp/e68e2dddc78aec5810b9a91c73a6c9acbe0e8845894b8c1c8f1074312e21b7bb.jpg", "table_caption": ["Table 6: Time cost for Adder Design (hours). "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "B.2 Multiplier Design ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The design results of the 8-bit multiplier are reported in Fig. 14. ", "page_idx": 17}, {"type": "text", "text": "B.3 Correlation between Metrics ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We investigated the correlation between theoretical and practical metrics to demonstrate the significance of optimizing theoretical metrics. For 64-bit and 128-bit adders, we sampled 6, 000 instances to assess their theoretical and practical metrics. Our results show a high correlation between two groups of metrics: level with delay and size with area, as illustrated in Fig. 15. Thus, structures with lower levels and smaller sizes are more likely to result in adders with lower delays and smaller areas. ", "page_idx": 17}, {"type": "text", "text": "128bit adder, level $=8$ ,size $=273$   \n128bit adder, level $=9$ ,size = 248   \n128bit adder, level $=$ 10, size $=$ 244 ", "page_idx": 18}, {"type": "text", "text": "Figure 13: Additional examples of 128-bit adders. More structures of the 128-bit adder first discovered by our method are shown. ", "page_idx": 18}, {"type": "image", "img_path": "5pnhGedG98/tmp/08e6b4ecf64300d0a09054aee7d647ccdde8d7af104e7b10a1d722453c68a686.jpg", "img_caption": ["Figure 14: Comparison of 8-bit multipliers. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "5pnhGedG98/tmp/489146d8d39803e917ceb73e72cfffece00aff943e16dc5a01fb7b61e7bb2083.jpg", "img_caption": ["Figure 15: Correlation of theoretical and practical metrics. The fitted lines indicate strong correlations in delay-level and area-size. The data are derived from 6k adders for each. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "B.4 Commercial Synthesis Tool Results ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In addition to our tests on open-source tools, we also utilized a commercial synthesis tool, Synopsys Design Compiler 2020 [47], to demonstrate the generalizability of our approach. Table 7 presents the results of the multipliers designed by this tool. We did not incorporate timing constraints when testing the delay of the critical path. The technology library used was the Nangate $45\\mathrm{nm}$ library [39]. The speed of our designed multiplier still holds a significant advantage, illustrating our design approach\u2019s broad applicability and substantial potential. ", "page_idx": 18}, {"type": "text", "text": "B.5 Design Time ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The overall design time is reported in Table 8, and the duration is within an acceptable range for the design process. ", "page_idx": 18}, {"type": "table", "img_path": "5pnhGedG98/tmp/74ef9ee04164f1d2384dfdb8833d17338eed3ff532e817e44e45cb2b476f409a.jpg", "table_caption": ["Table 7: Results of a commercial synthesis tool. All designs are the best-discovered multipliers with the OpenROAD tool. Corresponding Verilog codes are input into the Synopsis Design Compiler for synthesis. "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "5pnhGedG98/tmp/3fd6e2f3f7c2380cde39101f75f33027ece09264896c13b8cac1be4e684e96de.jpg", "table_caption": ["Table 8: Total design time. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "C Related Work ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "C.1 Computer Arithmetic ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In the quest for high performance and low cost, computer arithmetic design plays a crucial role in computer hardware, one of the most fundamental fields in computer science [44]. Issues for study include number representation, arithmetic operations, and real arithmetic. The addition is the most common arithmetic operation and serves as a basic unit for many other operations, making it the most studied module. The most basic adder structure is the ripple carry adder, which propagates the carry bit from low to high bits. Due to its serial structure, both the delay and size are $O(N)$ for an $N$ -bit addition. The carry look-ahead adder has been proposed to improve the delay by computing the carries for each digit simultaneously through an expanded formula. It can achieve ${\\cal O}(\\log N)$ delay and $O(N\\log N)$ size. However, due to the long internal delay of higher-valency gates used in the look-ahead adder [48], various prefix adders have been developed, including the Brent-Kung [43], Sklansky [12], Kogge-Stone [25], and Han-Carlson adders [49]. Most of these designs are variations of prefix adders. Although the minimal delay complexity is still ${\\cal O}(\\log N)$ , these adders can often have lower delays than the carry look-ahead adder because they use faster two-input logic gates [48]. Additionally, different prefix adders can strike a balance between delay and area, making them more suitable for actual hardware design. ", "page_idx": 20}, {"type": "text", "text": "Despite extensive research, human-engineered prefix adders encounter challenges in realizing Paretooptimal designs. Notably, the dimensions of the Sklansky adder can be further minimized whilst maintaining its operational level, as indicated by Roy et al. [14]. Consequently, a plethora of optimization-oriented methodologies have been put forward [42, 50\u201352, 41, 53]. The heuristic algorithm proposed by Roy and colleagues [14] employs a bottom-up enumeration tactic, commencing with a binary adder and iteratively escalating the bit count inductively based on extant structures. To reconcile the disparity between theoretical and empirical metrics, Ma et al. [20] developed a training regimen for a predictive model to estimate actual metrics from theoretical ones. This model utilizes a Pareto active learning approach to selectively scrutinize adders, which exhibit latent high-performance metrics, for empirical validation via synthesis tools. Additionally, Geng et al. [19] have embraced graph neural networks to enhance the precision of the predictive model. However, these methodologies necessitate the pre-selection of a finite set of adders for prediction purposes, representing merely a fraction of the comprehensive feasible space and potentially overlooking superior adder configurations. The foray of reinforcement learning into the domain of adder design was pioneered by Roy et al. [17], integrating a novel approach to address design challenges. Nevertheless, the employed Qnetwork methodology [54] lacks exploration capabilities when applied to expansive problem domains. Moreover, it mandates complete synthesis for each adder design, a prohibitively time-intensive process when attempting to sample a vast array of adder configurations, thereby yielding suboptimal solutions. ", "page_idx": 20}, {"type": "text", "text": "In analog to adder design, foundational research on multipliers has also been rooted in manual methodologies. An $N$ -bit multiplication fundamentally involves generating $N$ partial products by deploying $\\bar{N}^{2}$ AND gates, which correspond to each pair of bits to be multiplied [48]. Subsequently, these partial products are accumulated to yield a $2N$ -bit result. The most straightforward strategy employs $N$ successive accumulation operations over $N$ clock cycles, utilizing a serial approach that requires solely one adder and one register. Nevertheless, this method incurs a delay of $\\bar{O}(N\\log N)$ with the employment of a logarithmic delay adder [44], indicating a super-linear increase relative to the bit count. To elevate computational efficiency, one may adopt a compressor tree structure to compress the partial products concurrently using full and half adders, finalizing the computation with a single $2N$ -bit adder. Given that the compressor tree\u2019s height is roughly ${\\cal O}(\\log N)$ , the delay of the multiplier can be refined to $O(\\log N+\\log(2N))=O(\\log N)$ . Although Wallace [13] and Dadda trees [55]\u2014the predominant compressor trees\u2014share a theoretical logarithmic delay, empirical delays vary [56], underscoring the impact of the specific tree structure on multiplier performance. Xiao et al. [16] translated the design of these trees into an integer linear problem, addressed via a combinatorial solver, yet they did not include practical metrics in their model. Zuo et al. [18] pioneered the use of reinforcement learning to refine the multiplier design. Their approach, which modifies the Wallace tree structure rather than constructing anew, narrows the state space due to the finite action sequence length. Moreover, the synthesis process remains laborious, presenting challenges in optimizing multipliers exceeding 16 bits. Furthermore, the technique has not considered the joint optimization of the compressor and prefix trees within the multiplier, which poses a barrier to identifying a globally optimal design. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "C.2 Reinforcement Learning ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Reinforcement Learning (RL) has surpassed human performance in many domains, including the ancient game of Go [21], the complex strategy game StarCraft [29], optimizing sorting algorithms [57], and improving matrix multiplication techniques [30]. At its heart, RL involves training agents to make a series of decisions to achieve a goal, learning from interactions with their environment by trial and error to maximize a reward over time. There are two primary categories of RL methods: modelbased and model-free. In model-based RL, agents use an explicit model of the environment to inform their decisions [58, 59]. Tools like Monte Carlo Tree Search (MCTS) [28], which simulate various future paths to aid decision-making, are often integrated with these methods. This combination has proven particularly potent for tasks requiring a long sequence of decisions. Conversely, model-free RL methods [60\u201363], such as DQN [64], DDPG [65], and policy gradient approaches [66], operate without an explicit model of the environment. A prominent example of model-free RL is Proximal Policy Optimization (PPO) [24]. This algorithm iteratively refines the agent\u2019s policy, optimizing a surrogate objective function to balance the need for stable policy updates with the desire for efficient exploration. This leads to high sample efficiency and reduced training times. Choosing the right RL method is crucial, as different tasks may require different approaches. By aligning the strengths of specific RL techniques with the demands of the task at hand, agents can navigate complex decision spaces with remarkable effectiveness. ", "page_idx": 21}, {"type": "text", "text": "Recent advancements have shown that reinforcement learning is a powerful tool at every hardware design phase, because circuit design and testing are fundamentally combinatorial optimization problems. These tasks aim to navigate a vast solution space for the most efficient configuration. Notable examples of prior achievements include logic synthesis [67\u201370], circuit simulation [71, 72], chip placement [73\u201380], chip routing [81\u201383], clock tree synthesis [84], circuit gate sizing [85, 86], and hardware testing [87], among others. As such, reinforcement learning\u2019s widespread success in various Electronic Design Automation (EDA) tasks highlights its remarkable capabilities and adaptability as a tool for hardware design optimization. ", "page_idx": 21}, {"type": "text", "text": "D Societal Impact ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The advancements presented in this study have significant implications for various sectors reliant on high-performance computing and artificial intelligence. By optimizing the design of adders and multipliers, we can enhance the efficiency and reduce the physical footprint of hardware systems, leading to more powerful and compact devices. This can result in faster processing speeds and lower energy consumption, contributing to more sustainable technology practices. However, the societal impact extends beyond just technical improvements. As these optimized designs become more prevalent, they could reduce costs in producing advanced computational hardware, making high-performance computing more accessible to a wider range of industries and researchers. This democratization of technology could spur innovation and accelerate advancements in fields such as medicine, environmental science, and education. Nonetheless, the potential for job displacement in traditional hardware design roles should be considered, and efforts should be made to retrain and upskill workers to adapt to these technological advancements. ", "page_idx": 21}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 22}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not include theoretical results. ", "page_idx": 22}, {"type": "text", "text": "\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 23}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 23}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code. ", "page_idx": 23}, {"type": "text", "text": "\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 24}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 24}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). ", "page_idx": 24}, {"type": "text", "text": "\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 25}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 25}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 25}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] Guidelines: ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 25}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 26}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 26}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 26}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "page_idx": 26}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] Guidelines: ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 27}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 27}]