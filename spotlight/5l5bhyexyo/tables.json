[{"figure_path": "5l5bhYexYO/tables/tables_8_1.jpg", "caption": "Table 1: Average reward for each method in MuJoCo environments before and after online finetuning. The best performance for each environment is highlighted in bold font, and any result > 90% of the best performance is underlined. To save space, the name of the environments and datasets are abbreviated as follows: for the environments Ho=Hopper, Ha=HalfCheetah, Wa=Walker2d, An=Ant; for the datasets M=Medium, MR=Medium-Replay, R=Random. The format is \u201cfinal(+increase after finetuning)\". The proposed solution performs well.", "description": "This table presents the average reward achieved by different methods on various MuJoCo environments before and after online finetuning.  The best performing method for each environment is highlighted in bold, and any result within 90% of the best is underlined.  Environment and dataset names are abbreviated for space-saving.", "section": "4.3 MuJoCo Environments"}, {"figure_path": "5l5bhYexYO/tables/tables_14_1.jpg", "caption": "Table 2: Average reward for each method in Adroit Environments before and after online finetuning. The best result for each setting is marked in bold font and all results > 90% of the best performance are underlined. To save space, the name of the environments and datasets are abbreviated as follows: P=Pen, H=Hammer, D=Door, R=Relocate for environment, and E=Expert, C=cloned, H=Human for the dataset. It is apparent that while both IQL, TD3 and TD3+ODT perform decently well before online finetuning, our proposed solution significantly outperforms all baselines on the adroit testbed. DDPG+ODT starts out well in the online stage, but fails probably due to DDPG's training instability compared to TD3.", "description": "This table presents the average reward achieved by different methods (TD3+BC, IQL, ODT, PDT, TD3, DDPG+ODT, and TD3+ODT) on four Adroit manipulation tasks (Pen, Hammer, Door, Relocate) before and after online finetuning. Three different datasets are used for each task: expert, cloned, and human. The best-performing method for each setting is highlighted, and any result within 90% of the best performance is underlined.  The table demonstrates the significant improvement achieved by the proposed TD3+ODT method over other approaches, especially in addressing the training instability issues encountered with DDPG.", "section": "4.1 Adroit Environments"}, {"figure_path": "5l5bhYexYO/tables/tables_15_1.jpg", "caption": "Table 3: Average reward for each method in Antmaze Environments before and after online finetuning. The best result for each setting is marked in bold font and all results > 90% fo the best performance are underlined. To save space, the name of the environments and datasets are abbreviated as follows: U=Umaze, UD=Umaze-Diverse, MP=Medium-Play, MD=Medium-Diverse, LP=Large-Play and LD=Large-Diverse. U+M=Umaze and Medium maze. Our method performs the best on umaze and medium maze, while IQL performs the best on large maze. Both methods are much better than the rest on average. TD3+BC diverges on antmaze in our experiments.", "description": "This table presents the average reward achieved by different methods (TD3+BC, IQL, ODT, PDT, TD3, DDPG+ODT, and TD3+ODT) on various Antmaze environments (Umaze, Umaze-Diverse, Medium-Play, Medium-Diverse, Large-Play, and Large-Diverse) before and after online finetuning.  The results are summarized for each environment and dataset, indicating the final average reward and the increase achieved after finetuning.  The best-performing method for each environment and dataset is highlighted.", "section": "4.2 Antmaze Environments"}, {"figure_path": "5l5bhYexYO/tables/tables_24_1.jpg", "caption": "Table 4: The size and the average and standard deviation of the normalized reward of the Adroit datasets from D4RL [19] used in our experiments.", "description": "This table shows the size and the average and standard deviation of the normalized reward of the Adroit datasets used in the experiments.  The Adroit dataset contains four robotic manipulation tasks (Pen, Hammer, Door, Relocate) and three different data qualities (expert, cloned, human) for each task. The size column represents the number of transitions in each dataset.  The normalized reward represents the average performance achieved on that specific task/dataset by an agent.", "section": "4.1 Adroit Environments"}, {"figure_path": "5l5bhYexYO/tables/tables_25_1.jpg", "caption": "Table 5: The size and the average and standard deviation of the normalized reward of the Antmaze datasets from D4RL [19] used in our experiments.", "description": "This table presents the dataset size and the average normalized reward (along with standard deviation) for six different Antmaze environments from the D4RL benchmark.  These environments vary in terms of maze size and the diversity of starting positions and goals. The data is used in the paper's experiments to evaluate the performance of different reinforcement learning algorithms.  The table allows for a comparison of performance across environments with different levels of complexity.", "section": "4.2 Antmaze Environments"}, {"figure_path": "5l5bhYexYO/tables/tables_26_1.jpg", "caption": "Table 6: The size and the average and standard deviation of the normalized reward of the MuJoCo datasets from D4RL [19] used in our experiments.", "description": "This table presents the characteristics of the MuJoCo datasets used in the experiments.  For each environment (Hopper, HalfCheetah, Walker2d, Ant), three datasets are provided: medium, medium-replay, and random. The \"Size\" column indicates the number of data points in each dataset. The \"Normalized Reward\" column shows the average normalized reward and its standard deviation, indicating the performance level of the data within each dataset. This information is crucial for understanding the context and performance baselines of the experiments involving MuJoCo environments.", "section": "4.3 MuJoCo Environments"}, {"figure_path": "5l5bhYexYO/tables/tables_26_2.jpg", "caption": "Table 2: Average reward for each method in Adroit Environments before and after online finetuning. The best result for each setting is marked in bold font and all results > 90% of the best performance are underlined. To save space, the name of the environments and datasets are abbreviated as follows: P=Pen, H=Hammer, D=Door, R=Relocate for environment, and E=Expert, C=cloned, H=Human for the dataset. It is apparent that while both IQL, TD3 and TD3+ODT perform decently well before online finetuning, our proposed solution significantly outperforms all baselines on the adroit testbed. DDPG+ODT starts out well in the online stage, but fails probably due to DDPG's training instability compared to TD3.", "description": "This table presents the average reward achieved by different methods before and after online finetuning on Adroit environments (Pen, Hammer, Door, Relocate). Three different datasets are used: expert, cloned, and human.  The table highlights the best-performing method for each setting and underlines results within 90% of the best performance.  The caption notes that our proposed method (TD3+ODT) significantly outperforms the baselines, especially considering DDPG+ODT's instability.", "section": "4.1 Adroit Environments"}, {"figure_path": "5l5bhYexYO/tables/tables_27_1.jpg", "caption": "Table 8: The common hyperparameters across all environments used in our experiments.", "description": "This table lists the hyperparameters used across different reinforcement learning environments in the experiments.  These parameters are consistent across all the environments and include details about the embedding dimensions, attention heads, transformer layers, dropout rate, actor optimizer, number of steps collected per epoch, actor activation function, scheduler details, critic layers, critic width, critic activation function, batch size, actor updates per epoch, online exploration noise, TD3 policy noise, TD3 noise clip, and the TD3 target update ratio.  These settings were common to ensure fair comparison across the experiments. ", "section": "F.2 Hyperparameters"}, {"figure_path": "5l5bhYexYO/tables/tables_27_2.jpg", "caption": "Table 9: Environment-specific hyperparameters, where Ttrain and Teval stands for training and evaluation context length, RTGeval and RTGonline represents RTG during evaluation and online rollout respectively, a is the coefficient for RL gradient, y is the discount factor, lrc is the critic learning rate, and lra is the actor learning rate. Buffer size is counted in the number of trajectories. Note RTGs of antmaze have been modified according to our reward shaping.", "description": "This table lists the hyperparameters used in the experiments of the paper, specifically those that vary depending on the environment used.  It provides details such as the training and evaluation context lengths (Ttrain and Teval), the target return-to-go values during evaluation and online rollout (RTGeval and RTGonline), the coefficient for RL gradients (a), the discount factor (\u03b3), the critic and actor learning rates (lrc and lra), weight decay, the number of pretraining steps, and the buffer size.  The table also notes that the return-to-go values for the Antmaze environment were adjusted due to reward shaping.", "section": "F.2 Other Experiments"}, {"figure_path": "5l5bhYexYO/tables/tables_28_1.jpg", "caption": "Table 1: Average reward for each method in MuJoCo environments before and after online finetuning. The best performance for each environment is highlighted in bold font, and any result > 90% of the best performance is underlined. To save space, the name of the environments and datasets are abbreviated as follows: for the environments Ho=Hopper, Ha=HalfCheetah, Wa=Walker2d, An=Ant; for the datasets M=Medium, MR=Medium-Replay, R=Random. The format is \u201cfinal(+increase after finetuning)\". The proposed solution performs well.", "description": "This table presents the average reward achieved by different methods (TD3+BC, IQL, ODT, PDT, TD3, DDPG+ODT, and TD3+ODT) on MuJoCo environments before and after online finetuning.  The environments tested are Hopper, HalfCheetah, Walker2d, and Ant, each with Medium, Medium-Replay, and Random datasets.  The best performing method for each environment is bolded, and results within 90% of the best are underlined. The table shows the final average reward and the improvement gained after online finetuning. The \u2018final\u2019 column indicates the average reward after online finetuning, and the value in parentheses represents the increase in reward compared to the pre-finetuning performance. This table highlights the superior performance of the proposed TD3+ODT method in several of the scenarios.", "section": "4.3 MuJoCo Environments"}, {"figure_path": "5l5bhYexYO/tables/tables_28_2.jpg", "caption": "Table 11: Average reward for each method in Maze2D Environments before and after online finetuning. Our method works slightly worse than IQL but better than all other baselines.", "description": "This table presents the average reward achieved by different offline-to-online reinforcement learning methods on four variants of the Maze2D environment before and after online finetuning.  The methods compared include TD3+BC, IQL, ODT, PDT, TD3, DDPG+ODT, and TD3+ODT (the proposed method). The results show the average reward achieved by each method before and after online fine-tuning, illustrating the improvement achieved by each method after the online finetuning process.  The table highlights that the TD3+ODT method, while performing slightly worse than IQL, substantially outperforms the other methods.", "section": "4.2 Antmaze Environments"}]