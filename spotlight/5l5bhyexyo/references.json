{"references": [{"fullname_first_author": "Chen, L.", "paper_title": "Decision transformer: Reinforcement learning via sequence modeling", "publication_date": "2021-12-12", "reason": "This paper introduces the Decision Transformer, the core subject of the current research."}, {"fullname_first_author": "Brandfonbrener, D.", "paper_title": "When does return-conditioned supervised learning work for offline reinforcement learning?", "publication_date": "2022-12-12", "reason": "This paper provides the theoretical foundation for the analysis of online finetuning in Decision Transformers."}, {"fullname_first_author": "Fujimoto, S.", "paper_title": "Addressing function approximation error in actor-critic methods", "publication_date": "2018-07-07", "reason": "This paper introduces the TD3 algorithm, which is used as a key component in improving the online finetuning of Decision Transformers."}, {"fullname_first_author": "Kostrikov, I.", "paper_title": "Offline reinforcement learning with implicit q-learning", "publication_date": "2021-10-06", "reason": "This paper introduces Implicit Q-learning, one of the important baseline methods used for comparison in this work."}, {"fullname_first_author": "Zheng, Q.", "paper_title": "Online decision transformer", "publication_date": "2022-07-07", "reason": "This paper introduces the Online Decision Transformer (ODT), which serves as the main baseline algorithm for comparison in this study."}]}