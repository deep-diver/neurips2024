[{"figure_path": "NadTwTODgC/tables/tables_5_1.jpg", "caption": "Table 1: Returns on the 26 games of the Atari 100k benchmark after 2 hours of real-time experience, and human-normalized aggregate metrics. Bold numbers indicate the best performing methods.", "description": "This table presents the performance of various world models on the Atari 100k benchmark after only 2 hours of real-world experience.  It compares the scores achieved by different models on each of the 26 games in the benchmark, showing raw scores and human-normalized scores (HNS).  Bold numbers highlight the best-performing model for each game.  The table provides a comprehensive comparison across different algorithms, showcasing the relative strengths and weaknesses of each model in mastering various Atari games with limited experience.", "section": "4.2 Results on the Atari 100k benchmark"}, {"figure_path": "NadTwTODgC/tables/tables_19_1.jpg", "caption": "Table 1: Returns on the 26 games of the Atari 100k benchmark after 2 hours of real-time experience, and human-normalized aggregate metrics. Bold numbers indicate the best performing methods.", "description": "This table presents the performance of DIAMOND and other methods on the Atari 100k benchmark.  For each of the 26 games, it shows the raw scores achieved after only 2 hours of real-world experience (100,000 actions).  It also displays human-normalized scores, which compare agent performance to that of a human player.  Bold numbers highlight the top-performing method for each game.  This allows for comparison of DIAMOND's sample efficiency with other world models and provides an overall performance summary.", "section": "4.1 Atari 100k benchmark"}, {"figure_path": "NadTwTODgC/tables/tables_20_1.jpg", "caption": "Table 1: Returns on the 26 games of the Atari 100k benchmark after 2 hours of real-time experience, and human-normalized aggregate metrics. Bold numbers indicate the best performing methods.", "description": "This table presents the performance of DIAMOND and other methods on the Atari 100k benchmark.  For each of the 26 games, it shows the raw scores achieved after only 2 hours of real-world experience (100k actions).  The scores are also normalized against human performance, providing human-normalized scores (HNS).  The best performing method for each game is highlighted in bold.", "section": "4.2 Results on the Atari 100k benchmark"}, {"figure_path": "NadTwTODgC/tables/tables_21_1.jpg", "caption": "Table 1: Returns on the 26 games of the Atari 100k benchmark after 2 hours of real-time experience, and human-normalized aggregate metrics. Bold numbers indicate the best performing methods.", "description": "This table presents the performance of DIAMOND and several baseline methods on the Atari 100k benchmark.  The benchmark consists of 26 Atari games, and each agent is only allowed to train for 2 hours of real-time experience before evaluation. The table shows the raw scores achieved by each agent in each game, along with human-normalized scores (HNS) and aggregated metrics such as the mean and interquartile mean.  The bold numbers highlight the best-performing methods for each game.  This allows comparison of DIAMOND's performance with other world model-based RL agents.", "section": "4.1 Atari 100k benchmark"}, {"figure_path": "NadTwTODgC/tables/tables_22_1.jpg", "caption": "Table 1: Returns on the 26 games of the Atari 100k benchmark after 2 hours of real-time experience, and human-normalized aggregate metrics. Bold numbers indicate the best performing methods.", "description": "This table presents the performance of different reinforcement learning agents on the Atari 100k benchmark.  The benchmark consists of 26 Atari games, each with a time limit of 100,000 actions (roughly 2 hours of human gameplay). The table shows the raw scores achieved by each agent on each game, along with human-normalized aggregate scores (mean and interquartile mean). The results illustrate the sample efficiency and overall performance of the agents compared to human performance.", "section": "4.2 Results on the Atari 100k benchmark"}, {"figure_path": "NadTwTODgC/tables/tables_23_1.jpg", "caption": "Table 1: Returns on the 26 games of the Atari 100k benchmark after 2 hours of real-time experience, and human-normalized aggregate metrics. Bold numbers indicate the best performing methods.", "description": "This table presents the results of the DIAMOND model and several baseline models on the Atari 100k benchmark.  For each of the 26 games in the benchmark, the table shows the average return achieved by a random agent, a human player, and each of the tested models after a training period of approximately two hours of real-world experience. The results are also presented as a human-normalized score (HNS), which allows for a more meaningful comparison across different games.  Bold values indicate the highest performance for each game.", "section": "4.2 Results on the Atari 100k benchmark"}, {"figure_path": "NadTwTODgC/tables/tables_25_1.jpg", "caption": "Table 8: Results for 3D environments. These metrics compare observations from real trajectories and generated trajectories. The generated trajectories are conditioned on an initial set of L = 6 observations and a real sequence of actions.", "description": "This table presents a comparison of different world models on two 3D environments: CS:GO and Motorway driving.  The models are evaluated based on FID, FVD, and LPIPS scores, which measure the visual quality of generated trajectories.  The table also includes the sampling rate (observations per second) and the number of parameters for each model.  The results show that DIAMOND, particularly the frame-stack version, outperforms the baselines in terms of visual quality, while maintaining a relatively high sampling rate and reasonable parameter count.", "section": "M.4 Analysis"}]