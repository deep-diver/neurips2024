[{"figure_path": "gRG6SzbW9p/figures/figures_1_1.jpg", "caption": "Figure 1: Current RLHF approaches [52] incorrectly assume a unimodal BTL reward model for a diverse population of users. In this example, users have diverging preferences over the level of detail provided in the responses from a large language model. Without additional context, the BTL model considers both responses to be equally likely. In contrast, our method, VPL, is a personalized approach to RLHF. Using a few samples from a particular user, we infer the distribution over their distinct preferences. Based on this distribution, we condition the reward model to more accurately predict rewards, and enable steering the resulting policy to personalize to the specific user. This enables accounting for and serving the preferences of under-represented groups which would otherwise be ignored by the standard BTL model [52].", "description": "The figure illustrates the limitations of current Reinforcement Learning from Human Feedback (RLHF) techniques that assume a unimodal reward model, ignoring diverse user preferences. It contrasts this with the proposed Variational Preference Learning (VPL) method which models multiple reward functions based on inferred user-specific latent variables.  VPL personalizes the rewards and policies to better align with individual user preferences, addressing issues of under-representation in traditional RLHF.", "section": "1 Introduction"}, {"figure_path": "gRG6SzbW9p/figures/figures_6_1.jpg", "caption": "Figure 2: VPL LLM architecture for reward learning. The left and right parts denote the encoder qe and the reward model r(s, z) respectively.", "description": "This figure illustrates the architecture of the Variational Preference Learning (VPL) model for Large Language Models (LLMs).  The left side shows the encoder which takes pairs of prompt and response embeddings as input, processes them through a pair encoder and an attention layer to infer a latent representation z (representing user preferences). The right side shows the reward model, which takes the latent representation z and a new state (prompt and response) as input, to predict a reward.  The model uses a pre-trained LLM to encode prompt and response pairs.", "section": "5 Scaling VPL for Reward Learning in Large Language Models (LLMs)"}, {"figure_path": "gRG6SzbW9p/figures/figures_7_1.jpg", "caption": "Figure 3: Ground truth preferences (a) show that annotators prefer the robot navigate to two different goals. Unimodal BTL (b) averages over the two modes. VPL (c) accurately reconstructs diverse preferences, and learns z conditioned policies to reach either goal.", "description": "This figure compares the performance of three different reward learning methods: Ground Truth (GT), Bradley-Terry-Luce (BTL), and Variational Preference Learning (VPL) on a simulated robot navigation task with two distinct goals.  The GT shows that different users prefer the robot to reach different goals. The BTL approach, which assumes a unimodal reward function, averages over the user preferences, resulting in a reward function that doesn't accurately represent any single user's preferences. The VPL approach, on the other hand, effectively reconstructs the diverse user preferences, capturing the multi-modal nature of the reward function and producing a personalized policy that aligns well with individual user preferences.", "section": "6.2 Can VPL capture multi-modal reward functions from a dataset of diverse preferences?"}, {"figure_path": "gRG6SzbW9p/figures/figures_7_2.jpg", "caption": "Figure 4: Performance of a downstream policy on diverse control and reasoning tasks, using the rewards trained using different baselines. We report the mean and standard error over five seeds. Note: Habitat envs have a one-step greedy policy so reward scaling and SPO+VPL are not required.", "description": "This figure compares the performance of different reinforcement learning methods (Oracle, VPL, VPL+SPO, DPL-MeanVar, DPL-Categorical, and BTL) on four diverse control and reasoning tasks: Maze-Navigation, Ravens-Manipulation, Habitat-Rearrange, and Habitat-Tidy.  The y-axis represents the success rate, indicating the percentage of tasks successfully completed by the agent using policies trained with each method.  The results illustrate that VPL and VPL+SPO consistently outperform the baseline methods, demonstrating their effectiveness in handling multi-modal reward functions, particularly in complex tasks. The note clarifies that for Habitat environments, due to the nature of their one-step greedy policies, reward scaling and the SPO+VPL method were not necessary.", "section": "6.1 Tasks"}, {"figure_path": "gRG6SzbW9p/figures/figures_8_1.jpg", "caption": "Figure 5: Active learning enables personalizing policies to user preferences with fewer queries.", "description": "This figure shows the results of an experiment comparing the performance of active and random query selection strategies in a latent-variable preference-based reward learning method. The x-axis represents the number of queries used, and the y-axis represents the success rate of the resulting policy. The figure demonstrates that active query selection consistently outperforms random selection, achieving the same performance with only half the number of queries, thereby improving the efficiency of preference learning.", "section": "6.4 Can VPL enable active query selection for latent estimation?"}, {"figure_path": "gRG6SzbW9p/figures/figures_14_1.jpg", "caption": "Figure 6: Didactic experiments comparing standard BTL [17], DPL [58] and VPL (Ours). Four Gaussian reward functions generate different binary preference data. The traditional BTL approach [17] averages the different modes, and DPL [58] captures the uncertainty in the rewards due to the multi-modality but cannot accurately predict the true modes. VPL (ours) infers the hidden latent as described in Section 4 and recovers the individual distribution of reward functions.", "description": "This figure shows a didactic example to illustrate the differences between standard BTL, DPL, and the proposed VPL methods for multi-modal reward learning. Four Gaussian reward functions generate diverse binary preference data.  BTL averages the different modes, resulting in an inaccurate representation. DPL captures the uncertainty but fails to accurately predict the individual modes. VPL, however, infers the hidden context (latent variable) and accurately recovers the individual reward functions.", "section": "A Additional Experiments"}, {"figure_path": "gRG6SzbW9p/figures/figures_14_2.jpg", "caption": "Figure 7: Comparing scaling methods on Maze-Navigation.", "description": "This figure compares the performance of different reward scaling methods in the Maze-Navigation task.  It shows that VPL+SPO (Variational Preference Learning with Self-Play Preference Optimization) outperforms other methods, including VPL with different normalization techniques (batchnorm, maxnorm), and VPL without normalization.  The results highlight the importance of appropriately scaling rewards to improve the performance of downstream policies.", "section": "A.2 Does scaling rewards help improve performance?"}, {"figure_path": "gRG6SzbW9p/figures/figures_15_1.jpg", "caption": "Figure 8: Comparison of VPL's scalability on different tasks and user bases.", "description": "This figure compares the performance of VPL and baselines on two different tasks: Maze-Navigation and Habitat-Rearrange.  The Maze-Navigation task demonstrates VPL's ability to handle multiple reward modes, outperforming BTL which averages them. The Habitat-Rearrange task shows VPL's scalability to a much larger dataset of diverse users, highlighting its adaptability to real-world scenarios.  The benefits of reward scaling (VPL + SPO) are also illustrated in the Maze-Navigation experiment.", "section": "6.3 Do distributional reward functions enable learning a steerable multi-task policy?"}, {"figure_path": "gRG6SzbW9p/figures/figures_15_2.jpg", "caption": "Figure 5: Active learning enables personalizing policies to user preferences with fewer queries.", "description": "This figure shows the results of an active learning experiment comparing the performance of active and random query selection strategies for personalizing policies to user preferences. The x-axis represents the number of queries, and the y-axis shows the success rate. The results demonstrate that active query selection achieves higher success rates with fewer queries compared to random query selection.  The dashed line shows the success rate of a baseline using the standard BTL method.", "section": "6.4 Can VPL enable active query selection for latent estimation?"}, {"figure_path": "gRG6SzbW9p/figures/figures_16_1.jpg", "caption": "Figure 10: In the Ravens-Manipulation task, we compare the predicted rewards for states st along timesteps t in oracle trajectories to either of the goals the user prefers. VPL (Ours) can learn the individual reward functions for the two different (closely matching the ground truth rewards for both users) leading to more performant policies (see Figure 4), while DPL [58] learns a high variance reward function due to the multi-modality.", "description": "This figure compares the predicted rewards by different models (Oracle, VPL, and DPL) for states in optimal trajectories leading to two different goals. VPL accurately predicts rewards for both goals, while DPL shows high variance due to the multi-modal nature of the rewards.", "section": "6.3 Do distributional reward functions enable learning a steerable multi-task policy?"}, {"figure_path": "gRG6SzbW9p/figures/figures_16_2.jpg", "caption": "Figure 3: Ground truth preferences (a) show that annotators prefer the robot navigate to two different goals. Unimodal BTL (b) averages over the two modes. VPL (c) accurately reconstructs diverse preferences, and learns z conditioned policies to reach either goal.", "description": "This figure compares the performance of three different methods for learning reward functions: Ground Truth, BTL, and VPL.  The Ground Truth shows two distinct reward functions reflecting different user preferences (each user prefers one of two distinct goals). The BTL (Bradley-Terry-Luce) model, a standard approach in RLHF (Reinforcement Learning from Human Feedback), averages these preferences, resulting in a unimodal reward function that inaccurately reflects the true diversity of preferences. In contrast, VPL (Variational Preference Learning), the proposed method, accurately learns a multimodal reward function that captures the distinct preferences of each user.  This highlights VPL's ability to handle pluralistic preferences (multiple preferences within the user base).", "section": "6 Experimental Evaluation on Simulated Control Tasks"}, {"figure_path": "gRG6SzbW9p/figures/figures_17_1.jpg", "caption": "Figure 12: We train GPT2-based VPL, on the UltraFeedback-P dataset. In this plot, we visualize the t-SNE features of the latent distribution z produced by the encoder qy on a set of annotated prompts and responses {(sa, sb, y)}N=1 from the two users in the dataset. We see that the encoder clusters the users in the latent space, allowing the decoder to personalize the reward models according to multiple objectives preferred by the diverse users belonging to a cluster.", "description": "This figure visualizes the t-SNE features of the latent distribution z learned by the encoder in the VPL model. It shows that the encoder effectively clusters users in the latent space based on their preferences. This allows the decoder to create personalized reward models for different user groups.", "section": "A.6 Analysing the Latent Space"}, {"figure_path": "gRG6SzbW9p/figures/figures_17_2.jpg", "caption": "Figure 13: Reward accuracy with varying levels of noisy labels introduced at test time.", "description": "This figure demonstrates the robustness of VPL to noisy labels during testing.  It shows reward accuracy across different levels of injected noise (0%, 10%, 25%, 50%, 75%, 100%) in the preference labels used for inference at test time.  Different context lengths (2, 4, 8) are also evaluated. The results indicate that VPL maintains relatively high accuracy even with significant noise, especially with longer context lengths.  The BTL baseline is shown for comparison, highlighting the performance improvement achieved by VPL.", "section": "A.7 Does VPL scale to real-world settings with larger and noisy users?"}]