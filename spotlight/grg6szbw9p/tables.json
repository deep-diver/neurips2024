[{"figure_path": "gRG6SzbW9p/tables/tables_9_1.jpg", "caption": "Table 1: We compare the accuracy of different reward models trained on the two datasets. We report the mean and standard deviation of performance of GPT2-based models on three seeds, and one seed for Llama models.", "description": "This table compares the performance of three different reward models (BTL, DPL, and VPL) on two datasets (Pets and UltraFeedback).  It shows the accuracy of each model in predicting user preferences, with VPL significantly outperforming the baselines. The results are presented separately for GPT-2 and Llama2-7b language models.", "section": "7 LLM Experiments"}, {"figure_path": "gRG6SzbW9p/tables/tables_21_1.jpg", "caption": "Table 2: Hyperparameters for learning reward models using VPL. We sweep over these values and report the best results on 5 seeds.", "description": "This table lists the hyperparameters used for training reward models with the Variational Preference Learning (VPL) method.  The hyperparameters include architectural choices (encoder/decoder structure, number of hidden layers, their width), optimization settings (optimizer, learning rate), the dimensionality of the latent space, a regularization parameter (\u03b2), details of the prior distribution over the latent variable (VAE Prior), and the sizes of the data subsets used during training (context set, comparison set, number of annotated sets).  The authors swept over different values for these hyperparameters to find the best performing configuration.", "section": "B.4 Hyperparameters"}, {"figure_path": "gRG6SzbW9p/tables/tables_21_2.jpg", "caption": "Table 3: Hyperparameters for IQL. We use the same parameters across all experiments.", "description": "This table lists the hyperparameters used for Implicit Q-Learning (IQL), the offline reinforcement learning algorithm used in the paper's experiments.  The hyperparameters include architectural details (MLP layers and widths), the optimizer used (Adam), learning rate, discount factor, expectile, temperature and dataset size.  The same hyperparameters were used across all experiments for consistency and reproducibility.", "section": "Training and Evaluation Details"}, {"figure_path": "gRG6SzbW9p/tables/tables_21_3.jpg", "caption": "Table 4: Hyperparameters for LLM experiments", "description": "This table lists the hyperparameters used in the experiments with large language models (LLMs).  It includes details about the pair encoder architecture, hidden and latent dimensions, learning rate and scheduler, context and sampling set sizes, batch size, optimizer, beta value, and computational resources used.", "section": "5 Scaling VPL for Reward Learning in Large Language Models (LLMs)"}]