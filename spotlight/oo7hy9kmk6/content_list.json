[{"type": "text", "text": "Mean-Field Langevin Dynamics for Signed Measures via a Bilevel Approach ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Guillaume Wang\u22171 Alireza Mousavi-Hosseini\u22172 L\u00e9na\u00efc Chizat1 ", "page_idx": 0}, {"type": "text", "text": "1\u00c9cole polytechnique f\u00e9d\u00e9rale de Lausanne   \n2University of Toronto and Vector Institute ", "page_idx": 0}, {"type": "text", "text": "guillaume.wang@epfl.ch, mousavi@cs.toronto.edu, lenaic.chizat@epfl.ch ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Mean-field Langevin dynamics (MLFD) is a class of interacting particle methods that tackle convex optimization over probability measures on a manifold, which are scalable, versatile, and enjoy computational guarantees. However, some important problems \u2013 such as risk minimization for infinite width two-layer neural networks, or sparse deconvolution \u2013 are originally defined over the set of signed, rather than probability, measures. In this paper, we investigate how to extend the MFLD framework to convex optimization problems over signed measures. Among two known reductions from signed to probability measures \u2013 the lifting and the bilevel approaches \u2013 we show that the bilevel reduction leads to stronger guarantees and faster rates (at the price of a higher per-iteration complexity). In particular, we investigate the convergence rate of MFLD applied to the bilevel reduction in the low-noise regime and obtain two results. First, this dynamics is amenable to an annealing schedule, adapted from [SWON23], that results in improved convergence rates to a fixed multiplicative accuracy. Second, we investigate the problem of learning a single neuron with the bilevel approach and obtain local exponential convergence rates that depend polynomially on the dimension and noise level (to compare with the exponential dependence that would result from prior analyses). ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Let $\\mathcal{M}(\\mathcal{W})$ be the set of finite signed measures on a compact Riemannian manifold without boundaries $\\mathcal{W}$ and let $G:\\mathcal{M}(\\mathcal{W})\\to\\mathbb{R}$ be a convex function, assumed smooth in the sense of Assumption 1 below. In this paper, we investigate optimization methods to solve ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\nu\\in\\mathcal{M}(\\mathcal{W})}G_{\\lambda}(\\nu),\\qquad\\qquad\\qquad G_{\\lambda}(\\nu):=G(\\nu)+\\frac{\\lambda}{2}\\|\\nu\\|_{T V}^{2},\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "where $\\Vert\\cdot\\Vert_{T V}$ is the total variation norm and $\\lambda>0$ the regularization level.1 This covers for instance risk minimization for infinite-width 2-layer neural networks (2NN) [BRVDM05; Bac17] by taking $\\mathcal{W}=\\mathbb{S}^{d}$ the unit sphere in $\\mathbb{R}^{d+1}$ or $\\mathcal{W}\\overset{\\cdot}{=}\\mathbb{R}^{d+1}$ and ", "page_idx": 0}, {"type": "equation", "text": "$$\nG(\\nu)=\\mathbb{E}_{(x,y)\\sim\\rho}\\Big[\\ell(h(\\nu,x),y)\\Big]\\qquad\\quad\\mathrm{where}\\qquad\\quad h(\\nu,x)=\\int_{\\mathcal{W}}\\varphi(\\langle x,w\\rangle)\\mathrm{d}\\nu(w).\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "Here $\\varphi:\\mathbb{R}\\rightarrow\\mathbb{R}$ is the activation function, $h(\\nu,\\cdot)$ is the predictor parameterized by $\\nu,\\,G$ is the (population or empirical) risk under the data distribution $\\rho\\in\\mathcal{P}(\\mathbb{R}^{d+1}\\!\\times\\!\\mathbb{R})$ , and $\\ell$ is smooth (uniformly in $y$ ) and convex in its first argument. These 2NNs will be our guiding examples throughout, but note that the class of problems covered by Eq. (1.1) is more general and includes for instance sparse deconvolution via the Beurling-LASSO estimator [DG12] or optimal design [MZ04]. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To tackle such problems, interacting particle methods use the parameterization $\\textstyle\\nu=\\sum_{i=1}^{m}r_{i}\\delta_{w_{i}}$ and apply gradient methods in a well-chosen geometry [Chi22c; YWR23; GCM23]. They have recently gained traction thanks to their scalability and flexibility, and in the context of 2NNs, the usual gradient descent algorithm is an instance of such a method. On the downside, global convergence guarantees remain difficult to obtain due to the nonconvex nature of the reparameterized problem and existing positive results require either very specific settings [LMZ20], or modifications of the dynamics which often limit their scalability2. ", "page_idx": 1}, {"type": "text", "text": "In a related, but slightly different context, mean-field Langevin dynamics (MFLD) solve entropyregularized problems of the form ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mu\\in{\\mathcal P}(\\mathcal{W}^{\\prime})}F_{\\beta}(\\mu),\\qquad\\qquad\\qquad F_{\\beta}(\\mu):=F(\\mu)+\\beta^{-1}H(\\mu),\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\mathcal{P}(\\mathcal{W}^{\\prime})$ is the space of probability measures on a manifold $\\mathcal{W}^{\\prime}$ (typically $\\mathbb{R}^{d}$ ), $F:\\mathcal{P}(\\mathcal{W}^{\\prime})\\rightarrow\\mathbb{R}$ is a (sufficiently regular) convex functional, $\\begin{array}{r}{H(\\mu)=\\int\\log(\\mathrm{d}\\mu/\\mathrm{d}\\,\\mathrm{vol})\\bar{\\mathrm{d}}\\mu}\\end{array}$ is the negative differential entropy and $\\beta>0$ . These dynamics are obtained as the mean-field limit of noisy interacting particles dynamics [MMN18; HR\u0160S21] and converge globally at an exponential rate [NWS22; Chi22b], under two key conditions on $F$ : (i) a notion of regularity, which we refer to as displacement smoothness (see P1 below) and (ii) a uniform log-Sobolev inequality $\\left(L S I\\right)$ condition (see P2 below). These mean-field, continuous-time guarantees have been further refined into computational guarantees for fully discrete algorithms [CRW22; SWN23]. The favorable properties of MFLD naturally lead to the following question: ", "page_idx": 1}, {"type": "text", "text": "Can we efficiently solve problems of the form Eq. (1.1) using MFLD? ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "At first, it is not obvious that MFLD can be applied at all since it is originally defined only for problems over probability measures. However, we can find in the literature two general recipes to reduce a problem over $\\mathcal{M}(\\mathcal{W})$ to a problem over $\\mathcal{P}(\\mathcal{W}^{\\prime})$ , thus amenable to MFLD. The first one is a lifting reduction, that takes $\\mathcal{W}^{\\prime}=\\mathbb{R}\\times\\mathcal{W}$ where the extra dimension serves to encode the signed mass of particles [CB18, Section A.2] [Chi22c]. The second one, that takes $\\mathcal{W}^{\\prime}=\\mathcal{W}$ , is a bilevel reduction [Bac21; TS24] that uses a variational representation of the regularizer $\\Vert\\cdot\\Vert_{T V}^{2}$ , common in the multiple kernel learning literature [LCBGJ04]. A first task is thus to compare the behavior of MFLD on these two approaches. Furthermore, MFLD involves an entropic regularization which is absent from Eq. (1.1). A second task is thus to analyze the behavior of MFLD in the large $\\beta$ regime, when the regularization vanishes. ", "page_idx": 1}, {"type": "text", "text": "In this work, we tackle these two tasks and make the following contributions: ", "page_idx": 1}, {"type": "text", "text": "\u2022 In Sec. 3, we introduce the lifting and bilevel reductions and compare the \u201cdisplacement smoothness\u201d (P1) and \u201cuniform LSI\u201d (P2) properties of the resulting problems. These properties play a central role in the global convergence analysis of MFLD. Specifically, we consider a large class of lifting reductions and show that none satisfies simultaneously (P1) and (P2) unless $\\lambda$ is large. In contrast, the bilevel reduction satisfies both under mild assumptions. So in the sequel we focus on MFLD applied to the bilevel reduction.   \n\u2022 In Sec. 4, we investigate what convergence rates can be obtained for the problem (1.1) by using MFLD on the bilevel formulation. While a classical simulated annealing technique yields convergence in $O(\\log\\log t/\\log t)$ , we show that the structure of the bilevel objective is in fact amenable to a more efficient annealing schedule, adapted from [SWON23], that reaches a fixed multiplicative accuracy, say 1.01 inf $G_{\\lambda}$ , in time $e^{O(\\lambda^{-1}\\log\\lambda^{-1})}$ instead of eO(\u03bb\u22122) for the classical schedule.   \n\u2022 In Sec. 5, to obtain a more complete picture, we investigate the problem of learning a single neuron. Here, using a Lyapunov type argument, we show that the local convergence rate of MFLD applied to the bilevel formulation scales polynomially in $\\beta$ and $d$ , at odds with all previous MFLD analyses which had exponential dependencies. ", "page_idx": 1}, {"type": "text", "text": "All proofs are deferred to the Appendix. ", "page_idx": 1}, {"type": "text", "text": "1.1 Related work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Particle methods and mean-field limits. Interacting particle systems have been studied for decades in various fields, see e.g. [Szn91; CD13; Lac18]. Their more recent connection with the standard training of 2NNs [NS17; SS20; RV22; MMN18] has suggested new settings of analysis, where convexity of the functional plays a key role, and has led to many developments. In particular, the case of MFLD (under study here) quickly progressed from nonquantitative guarantees [MMN18; HR\u0160S21], to mean-field convergence rates [NWS22; Chi22b] and fully discrete computational guarantees [CRW22; SWN23; KZCE+24] in the span of a few years. Recent progress also address its accelerated (underdamped) version [CLRW24; FW23], which could also be of interest in our setting. ", "page_idx": 2}, {"type": "text", "text": "Multiple kernel learning and bilevel training of NNs. The lifting reductions we consider are inspired by the unbalanced optimal transport literature [LMS18], while the bilevel reduction comes from the Multiple Kernel Learning (MKL) literature [CVBM02; LCBGJ04; RBCG08] (see [Bac19] for an account). While the latter is usually studied with a discrete domain $\\mathcal{W}$ (see also [PP21; PP23] for recent computational considerations), it was suggested for the training of large width 2NN in [Bac21] and used in conjonction with MFLD in [TS24] (more details below). Relatedly, a recent line of work studies the (noiseless) training of 2NN in a two-timescale regime, where the outer layer is trained at a much faster rate than the inner layer [BMZ23; MB23; BBP23]. This implicitly corresponds to optimizing the bilevel objective and leads to improved convergence guarantees. ", "page_idx": 2}, {"type": "text", "text": "The work that is closest to ours is [TS24], which considers the MFLD on a 2NN with weight decay where the outer layer is optimized at each step. They interpret the resulting dynamics as a kernel learning dynamics and study properties of the learnt kernel and its associated RKHS. While they do not formulate explicitly the problem Eq. (1.1), it can be shown that our approaches are equivalent when considering $\\mathcal{W}\\overset{\\bullet}{=}\\mathbb{R}^{d+\\frac{\\mathbf{1}}{1}}$ in Eq. (1.2) (and adding an extra regularization). The details are given in Sec. A.2. Key advantages of our formulation with $\\bar{\\mathcal{W}}=\\mathbb{S}^{d}$ are that we cover the case of unbounded homogeneous activation functions (such as ReLU), and can obtain improved LSI. ", "page_idx": 2}, {"type": "text", "text": "2 Background on guarantees for mean-field Langevin dynamics ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The MFLD is defined as the Wasserstein gradient flow $(\\mu_{t})_{t\\in\\mathbb{R}_{+}}$ in ${\\mathcal{P}}(\\Omega)$ of an objective of the form Eq. (1.3). It is characterized as the solution to the partial differential equation (PDE) ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\partial_{t}\\mu_{t}=\\mathrm{div}(\\mu_{t}\\nabla F^{\\prime}[\\mu_{t}])+\\beta^{-1}\\Delta\\mu_{t},\\qquad\\mu_{0}\\in\\mathscr{P}(\\Omega).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $F^{\\prime}[\\mu]:\\Omega\\to\\mathbb{R}$ is the first variation of $F$ at $\\mu$ [San15, Sec. 7.2], defined by $\\begin{array}{r}{\\operatorname*{lim}_{\\epsilon\\downarrow0}\\frac{1}{\\epsilon}(F(\\mu+}\\end{array}$ $\\begin{array}{r}{\\epsilon(\\mu^{\\prime}-\\mu))-F(\\mu))=\\int F^{\\prime}[\\mu]\\mathrm{d}(\\mu^{\\prime}-\\mu)}\\end{array}$ for any $\\mu^{\\prime}\\in\\mathscr{P}(\\Omega)$ . This PDE corresponds to the mean-field limit ( $N\\to\\infty$ ) of the noisy particle gradient flow $\\omega_{t}\\in\\Omega^{N}$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\forall i\\leq N,\\;\\mathrm{d}\\omega_{t}^{i}=-N\\nabla_{\\omega_{t}^{i}}F^{(N)}\\left(\\omega_{t}^{1},...,\\omega_{t}^{N}\\right)\\mathrm{d}t+\\sqrt{2\\beta^{-1}}\\mathrm{d}B_{t}^{i},\\qquad\\omega_{0}^{i}\\overset{\\mathrm{i.i.d.}}{\\sim}\\mu_{0}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\begin{array}{r}{F^{(N)}\\left(\\omega^{1},...,\\omega^{N}\\right)\\,=\\,F\\left(\\frac{1}{N}\\sum_{i=1}^{N}\\delta_{\\omega^{i}}\\right)}\\end{array}$ and the $B_{t}^{i}$ are $N$ independent Brownian motions on $\\Omega$ . The convergence guarantees for MFLD rely on three key properties: ", "page_idx": 2}, {"type": "text", "text": "(P0) (Convexity) $F$ is convex and is such that $F_{\\beta}$ admits a minimizer $\\mu_{\\beta}^{*}$ . ", "page_idx": 2}, {"type": "text", "text": "(P1) (Displacement smoothness) $F$ is $L$ -displacement smooth, in the sense that3 ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\forall\\mu\\in\\mathscr{P}_{2}(\\Omega),\\;\\forall\\omega\\in\\Omega,\\;\\underset{s\\in T_{\\omega}\\Omega}{\\operatorname*{max}}\\;\\left|\\nabla^{2}\\,F^{\\prime}[\\mu](s,s)\\right|\\leq L,}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\left\\|s\\right\\|_{\\omega}\\leq1}\\\\ {\\mathrm{and~}}&{\\quad\\forall\\mu,\\mu^{\\prime}\\in\\mathscr{P}_{2}(\\Omega),\\;\\forall\\omega\\in\\Omega,\\;\\left\\|\\nabla F^{\\prime}[\\mu]-\\nabla F^{\\prime}[\\mu^{\\prime}]\\right\\|_{\\omega}\\leq L\\,W_{2}(\\mu,\\mu^{\\prime}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\nabla^{2}$ denotes the Riemannian Hessian. ", "page_idx": 2}, {"type": "text", "text": "(P2) (Uniform LSI) There exists $\\alpha>0$ such that $\\forall t\\ge0$ , $F_{\\beta}$ satisfies local $\\alpha$ -LSI at $\\mu_{t}$ , as in Def. 2.1. ", "page_idx": 2}, {"type": "text", "text": "Definition 2.1 (Local LSI). We say that a functional $F_{\\beta}\\;=\\;F+\\beta^{-1}H$ satisfies local $\\alpha$ -LSI at $\\mu\\ \\in\\ \\mathscr{P}(\\Omega)$ if $\\begin{array}{r}{Z\\ :=\\ \\int_{\\Omega}\\exp\\left(-\\bar{\\beta}F^{\\prime}[\\mu]\\right)\\mathrm{d}\\omega\\ <\\ \\infty}\\end{array}$ and the proximal Gibbs measure $\\hat{\\mu}\\;:=\\;$ $Z^{-1}\\exp(-\\beta F^{\\prime}[\\mu])\\in\\mathcal{P}(\\Omega)$ satisfies $\\alpha$ -LSI, that is ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\forall\\mu^{\\prime}\\in\\mathcal{P}(\\Omega),\\;H\\left(\\mu^{\\prime}|\\hat{\\mu}\\right)\\leq\\frac{1}{2\\alpha}I(\\mu^{\\prime}|\\hat{\\mu}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the relative entropy and relative Fisher Information are respectively defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\nH\\left(\\mu^{\\prime}|\\hat{\\mu}\\right):=\\int_{\\Omega}\\log\\left(\\frac{\\mathrm{d}\\mu^{\\prime}}{\\mathrm{d}\\hat{\\mu}}\\right)\\mathrm{d}\\mu^{\\prime},\\qquad\\quad I(\\mu^{\\prime}|\\hat{\\mu}):=\\int_{\\Omega}\\left\\lVert\\nabla\\log\\frac{\\mathrm{d}\\mu^{\\prime}}{\\mathrm{d}\\hat{\\mu}}(\\omega)\\right\\rVert_{\\omega}^{2}\\mathrm{d}\\mu^{\\prime}(\\omega),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "and ${\\|\\cdot\\|}_{\\omega}$ denotes the Riemannian metric. ", "page_idx": 3}, {"type": "text", "text": "We review some useful criteria for LSI in App. B. In particular, the uniform LSI property (P2) holds for example when training two-layer neural networks with a frozen second layer, under some technical assumptions such as bounded activation function. In fact in that case, the proximal Gibbs measures $\\hat{\\mu}$ even satisfy LSI uniformly for all $\\mu\\in\\mathscr{P}(\\Omega)$ [Chi22b; NWS22]. ", "page_idx": 3}, {"type": "text", "text": "Note that the Riemannian gradient $\\nabla$ and the Laplace-Beltrami operator $\\Delta$ appearing in (2.1), as well as the definition of Brownian motion, depend on the Riemannian metric of $\\Omega$ . This dependency is reflected in (P1) and (P2). ", "page_idx": 3}, {"type": "text", "text": "The global convergence of MFLD is guaranteed by the following theorem, with a rate. ", "page_idx": 3}, {"type": "text", "text": "Theorem 2.1 ([Chi22b, Thm. 3.2][NWS22, Thm. 1]). Consider $F:{\\mathcal{P}}(\\Omega)\\to\\mathbb{R}$ and $\\left(\\mu_{t}\\right)$ as in (2.1). $H(\\mathbf{P}0),$ (P1) and (P2) are satisfied then for $t\\geq0$ it holds ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\beta^{-1}H(\\mu_{t}|\\mu_{\\beta}^{*})\\leq F_{\\beta}(\\mu_{t})-F_{\\beta}(\\mu_{\\beta}^{*})\\leq\\exp(-2\\beta^{-1}\\alpha\\,t)\\Big(F_{\\beta}(\\mu_{0})-F_{\\beta}(\\mu_{\\beta}^{*})\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Note that although the $L$ -smoothness constant does not appear in Thm. 2.1, it does appear in the discrete-time guarantees of [SWN23], and is thus an important quantity in practice. In this paper, we limit our analysis to the mean-field dynamics (2.1) because its time-discretization has not yet been studied on Riemannian manifolds. In continuous time, the proof of Thm. 2.1 translates directly to Riemannian manifolds thanks to our definition of (P1), see App. B. ", "page_idx": 3}, {"type": "text", "text": "3 Reductions from signed measures to probability measures ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In order to apply the MFLD framework to solve our initial problem over signed measures (1.1), we must first recast it as an optimization problem over probability measures. In this section we build two such reductions, and discuss the properties (P0, P1 and P2) of the resulting problems. ", "page_idx": 3}, {"type": "text", "text": "3.1 Reduction by lifting ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Reductions by lifting consist in representing signed measures as projections of probability measures in the higher dimensional space $\\Omega=\\mathbb{R}\\times\\mathcal{W}$ . This construction involves the 1-homogeneous projection operator4 $h:\\mathcal{P}_{1}(\\Omega)\\to\\bar{\\mathcal{M}}(\\mathcal{W})$ characterized by ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\forall\\varphi\\in\\mathcal{C}(\\mathcal{W},\\mathbb{R}),\\;\\int_{\\mathcal{W}}\\varphi(w)(h\\mu)(\\mathrm{d}w)=\\int_{\\Omega}r\\varphi(w)\\mu(\\mathrm{d}r,\\mathrm{d}w),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathscr{P}_{p}(\\Omega)$ is the subset of ${\\mathcal{P}}(\\Omega)$ for which $\\begin{array}{r}{\\int|r|^{p}\\mathrm{d}\\mu(\\mathrm{d}r,\\mathrm{d}w)\\,<\\,+\\infty}\\end{array}$ . For instance, it acts on discrete measures as $\\begin{array}{r}{\\boldsymbol{h}\\left(\\frac{1}{m}\\sum_{j=1}^{m}\\delta_{(r_{j},w_{j})}\\right)\\,=\\,\\frac{1}{m}\\sum_{j=1}^{m}r_{j}\\delta_{w_{j}}}\\end{array}$ . We also define, for $b\\in[1,2]$ and $\\mu\\in\\mathscr{P}_{b}(\\Omega)$ , $\\begin{array}{r}{\\Psi_{b}(\\mu):=\\left(\\int_{\\Omega}|r|^{b}\\mathrm{d}\\mu(r,w)\\right)^{2/b}}\\end{array}$ . The objective functional of the lifted problem is then defined, for $\\mu\\in\\mathscr{P}_{b}(\\Omega)$ , as ", "page_idx": 3}, {"type": "equation", "text": "$$\nF_{\\lambda,b}(\\mu):=G(h\\mu)+\\frac{\\lambda}{2}\\Psi_{b}(\\mu).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "It is equivalent to minimize $G_{\\lambda}$ or $F_{\\lambda,b}$ , as shown in the following statement. ", "page_idx": 3}, {"type": "text", "text": "aPnrod peoqsuitailoitny  3h.1o.l dLs eft $\\nu\\in\\mathcal{M}(\\mathcal{W})$ $\\begin{array}{r}{\\mu(\\mathrm{d}r,\\mathrm{d}w)=\\delta_{f(w)}(\\mathrm{d}r)\\frac{|\\nu|(\\mathrm{d}w)}{\\|\\nu\\|_{T V}}}\\end{array}$ $\\mu\\in\\mathcal{P}_{b}(\\mathcal{W})$ swuhcehr et $\\begin{array}{r}{f(w)=\\|\\nu\\|_{T V}\\frac{\\mathrm{d}\\nu}{\\mathrm{d}|\\nu|}(w)}\\end{array}$ $h\\mu=\\nu$ $F_{\\lambda,b}(\\mu)\\ge G_{\\lambda}(\\nu)$ , this $\\mu$ when $b>1$ ). In particular, if $G_{\\lambda}$ admits a minimizer then $F_{\\lambda,b}$ does too, and it holds ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mu\\in\\mathcal{P}_{b}(\\Omega)}F_{\\lambda,b}(\\mu)=\\operatorname*{min}_{\\nu\\in\\mathcal{M}(\\mathcal{W})}G_{\\lambda}(\\nu).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "It is not difficult to see that $F_{\\lambda,b}$ satisfies (P0) as long as $G_{\\lambda}$ admits a minimizer. In order to study (P1) and (P2), we need to define a Riemannian metric on $\\Omega$ . Following [Chi22c], we consider a general class of Riemannian metrics on $\\Omega^{\\ast}:=\\mathbb{R}^{\\ast}\\times\\mathcal{W}$ , parameterized by $q_{r},q_{w}\\in\\mathbb{R}$ and $\\Gamma>0$ , defined by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\left\\langle\\left(\\frac{\\delta r_{1}}{\\delta w_{1}}\\right),\\left(\\frac{\\delta r_{2}}{\\delta w_{2}}\\right)\\right\\rangle_{(r,w)}=\\Gamma^{-1}\\left|r\\right|^{q_{r}}\\frac{\\delta r_{1}\\delta r_{2}}{r^{2}}+\\left|r\\right|^{q_{w}}\\left\\langle\\delta w_{1},\\delta w_{2}\\right\\rangle_{w}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This indeed defines an inner product on $T_{(r,w)}\\Omega^{*}:=\\mathbb{R}\\times T_{w}\\mathcal{W}$ that varies smoothly, and so equips $\\Omega^{*}$ with a (disconnected) Riemannian manifold structure [Lee18]. Intuitively, the parameter $\\Gamma$ will govern the relative speed of the weight or position variables along gradient flows; larger $\\Gamma$ means faster weight updates. ", "page_idx": 4}, {"type": "text", "text": "Two particular cases of this construction appear (sometimes implicitly) in the literature on 2NN: ", "page_idx": 4}, {"type": "text", "text": "(i) when $q_{r}=2$ and $q_{w}=0$ , the metric (3.2) extends to the product metric on $\\Omega=\\mathbb{R}\\times\\mathcal{W}$ . With $\\bar{\\mathcal{W}}=\\mathbb{R}^{d+1}$ , this corresponds to the usual parameterization of 2NNs and is the setting of most previous works applying MFLD to 2NN (with a weight decay regularization on the second layer for $b=2$ and $\\lambda>0$ ).   \n(ii) when $q_{r}\\,=\\,q_{w}\\,=\\,1$ , $\\Omega^{*}$ is isometric to the union of two copies of the (tipless) metric cone over $\\mathcal{W}$ [BBI01] (via the mapping $(r,\\omega)\\mapsto(\\mathrm{sign}(r),\\sqrt{|r|},\\omega))$ . This is the natural setting for optimization over signed measures; and with $\\mathcal{W}\\,=\\,\\mathbb{S}^{d}$ , is equivalent to the parameterization of 2NNs with ReLU activation and balanced initialization [CB20, App. H]. ", "page_idx": 4}, {"type": "text", "text": "Issues caused by the disconnectedness of $\\Omega^{*}$ . On the level of the equivalence of variational problems, one can check that the statement of Prop. 3.1 also holds if $\\Omega=\\mathbb{R}\\times\\mathcal{W}$ is replaced by $\\Omega^{*}=\\mathbb{R}^{*}\\times\\mathcal{W}$ . However, when the manifold $\\Omega^{*}$ is truly disconnected,5 then ${\\mathcal{P}}(\\Omega)$ is not connected in the sense of absolutely continuous curves in Wasserstein space. More precisely, $\\Omega^{*}$ is the disjoint union of $\\Omega_{+}^{*}\\,=\\,\\mathbb{R}_{+}^{*}\\,\\times\\,\\mathcal{W}$ and $\\Omega_{-}^{*}\\,=\\,\\mathbb{R}_{-}^{*}\\,\\times\\,\\mathcal{W}$ , and one can show that (for certain choices of $q_{r},q_{w})$ , if $(\\mu_{t})_{t}$ is a Wasserstein gradient flow (or any other absolutely continuous curve), then $\\mu_{t}(\\Omega_{+}^{*})=\\mu_{0}(\\Omega_{+}^{*})$ for all $t$ . ", "page_idx": 4}, {"type": "text", "text": "Moreover, supposing for simplicity that $G_{\\lambda}$ has a unique minimizer $\\nu$ and that $b>1$ , then $F_{\\lambda,b}$ has a unique minimizer $\\mu^{*}$ , and $\\mu^{*}(\\Omega_{+}^{*})\\,=\\,\\nu_{+}(\\mathcal{W})/\\,\\|\\nu\\|_{T V}$ where $\\nu\\,=\\,\\nu_{+}\\,-\\,\\nu_{-}$ is the Jordan decomposition of $\\nu$ . Therefore, Wasserstein gradient flow for $F_{\\lambda,b}$ can only converge to $\\mu^{*}$ if it was initialized such that $\\mu_{0}(\\Omega_{+}^{*})=\\mu^{*}(\\Omega_{+}^{*})$ . In terms of particle methods, this means that the fraction of the particles $(r_{i},w_{i})$ initialized with $r_{i}>0$ must be precisely $\\mu^{*}(\\Omega_{+}^{*})$ . A similar problem arises if we apply MFLD to $F_{\\lambda,b}$ , since it is nothing else than Wasserstein gradient flow for $F_{\\lambda,b}+\\beta^{-1}H$ ; but it is more tedious to discuss formally, as $F_{\\lambda,b}+\\beta^{-1}H$ does not have a minimizer in general. ", "page_idx": 4}, {"type": "text", "text": "In order to bypass this limitation, one may focus on settings where the ratio $\\nu_{+}(\\mathcal{W})/\\left\\lVert\\nu\\right\\rVert_{T V}$ for the optimal $\\nu$ is known in advance, e.g., the problem (1.1) constrained to non-negative measures, or on choices of $q_{r},q_{w}$ for which $\\Omega^{*}$ can be extended into a connected manifold, such as the product metric $q_{r}=2,q_{w}=0$ . However, even in those cases, MFLD on $F_{\\lambda,b}$ presents other limitations. ", "page_idx": 4}, {"type": "text", "text": "Incompatibility with MFLD. We now show that, in spite of the degrees of freedom given by the parameters $q_{r},q_{w}$ and $b$ , satisfying both (P1) and (P2) requires restrictive assumptions. This suggests that the lifting approach is fundamentally incompatible with MFLD. ", "page_idx": 4}, {"type": "text", "text": "Proposition 3.2. Consider $F_{\\lambda,b}$ from Eq. (3.1) and $\\Omega^{*}$ equipped with the metric (3.2). Suppose $G^{\\prime}[\\nu]$ is continuous for all $\\nu$ and that there exists $\\nu$ such that $\\nabla^{2}G^{\\prime}[\\nu]$ is not constant equal to 0. Then ", "page_idx": 5}, {"type": "text", "text": "\u2022 If $\\textstyle q_{r}\\neq1$ or $q_{w}\\neq1$ or $b\\neq1$ , then (P1) does not hold.   \n\u2022 $\\begin{array}{r}{\\lceil f q_{r}=q_{w}=b=1\\rceil}\\end{array}$ , then for any $\\mu\\in\\mathscr{P}_{1}(\\Omega).$ , there exists $\\lambda_{0}>0$ such that $F_{\\lambda,b}+\\beta^{-1}H$ does not satisfy local LSI at $\\mu$ for any $\\lambda<\\lambda_{0}$ (in particular (P2) does not hold unless $\\lambda$ is large enough). ", "page_idx": 5}, {"type": "text", "text": "When $q_{r}=q_{w}=b=1$ and $\\lambda$ is large enough, then it can indeed be shown that Thm. 2.1 applies under natural conditions, see for instance [Chi22b, Sec. 5.1]. ", "page_idx": 5}, {"type": "text", "text": "Remark 3.1. For functionals of the form $\\begin{array}{r}{G_{\\lambda,s}=G(\\nu)\\!+\\!\\frac{\\lambda}{s}\\left\\|\\nu\\right\\|_{T V}^{s}}\\end{array}$ , instead of (1.1) which corresponds to $s\\,=\\,2$ , one can formulate a similar reduction by posing $\\begin{array}{r}{\\Psi_{b,s}(\\mu)\\,=\\,(\\int_{\\Omega}|r|^{b}\\,\\mathrm{d}\\mu(r,w))^{s/b}}\\end{array}$ and $\\begin{array}{r}{F_{\\lambda,b,s}(\\mu)\\,=\\,G(h\\mu)\\,+\\,\\frac{\\lambda}{s}\\Psi_{b,s}(\\mu)}\\end{array}$ . The statements of Prop. 3.1 and Prop. 3.2 hold true with $G_{\\lambda}$ replaced by $G_{\\lambda,s}$ , and $F_{\\lambda,b}^{'}$ by $F_{\\lambda,b,s}$ , for any $1\\leq b\\leq s$ , as can be shown by very simple adaptations of the proofs (only the second inequality in the proof of Lem. C.1, and the definition of $\\lambda^{\\prime}$ in (C.2), need to be adapted). Note that the problem considered in [Chi22c] is of the form $G(\\nu)+\\lambda\\left\\|\\nu\\right\\|_{T V}$ , and they analyzed Wasserstein gradient flow on $F_{\\lambda,1,1}$ with $q_{r}=q_{w}=1$ (in particular the issues caused by the disconnectedness of $\\Omega^{*}$ are bypassed thanks to the choice $b=1$ ). The above discussion shows that applying MFLD to that problem would only yield convergence guarantees for $\\lambda$ large enough. ", "page_idx": 5}, {"type": "text", "text": "3.2 Reduction by bilevel optimization ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We define the bilevel objective functional $J_{\\lambda}$ for $\\eta\\in\\mathcal{P}(\\mathcal{W})$ as6 ", "page_idx": 5}, {"type": "equation", "text": "$$\nJ_{\\lambda}(\\eta):=\\operatorname*{inf}_{\\nu\\in\\mathcal{M}(\\mathcal{W})}G(\\nu)+\\frac{\\lambda}{2}\\int_{\\mathcal{W}}\\frac{\\left|\\nu\\right|^{2}}{\\eta}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "It can be derived using the variational representation of the squared TV-norm [LCBGJ04; Bac19]: for any \u03bd \u2208M(\u2126), one has \u2225\u03bd\u22252T V = min\u03b7\u2208P(W) W |\u03bd\u03b7|2 . By exchanging infima, it thus holds $\\begin{array}{r}{\\operatorname*{inf}_{\\nu\\in{\\mathcal M}(\\mathcal W)}G_{\\lambda}(\\nu)\\;=\\;\\operatorname*{inf}_{\\eta\\in{\\mathcal P}(\\mathcal W),\\nu\\in{\\mathcal M}(\\mathcal W)}G(\\nu)\\,+\\,\\frac{\\lambda}{2}\\int\\frac{|\\nu|^{2}}{\\eta}\\;=\\;\\operatorname*{inf}_{\\eta\\in{\\mathcal P}(\\mathcal W)}J_{\\lambda}(\\eta)}\\end{array}$ . Moreover, the objective minimized in (3.3) is jointly convex in $(\\eta,\\nu)$ and partial minimization preserves convexity, so $J_{\\lambda}$ is convex. Let us gather these crucial remarks in a formal statement. ", "page_idx": 5}, {"type": "text", "text": "Proposition 3.3. The bilevel objective $J_{\\lambda}$ is convex and $\\operatorname*{inf}_{\\mathcal{P}(\\mathcal{W})}J_{\\lambda}=\\operatorname*{inf}_{\\mathcal{M}(\\mathcal{W})}G_{\\lambda}$ . Moreover, $i f$ $G_{\\lambda}$ admits a minimizer $\\nu\\in\\mathcal{M}(\\mathcal{W})$ , then $\\begin{array}{r}{\\arg\\operatorname*{min}J_{\\lambda}=\\left\\{\\frac{|\\nu|}{\\|\\nu\\|_{T V}},\\nu\\in\\arg\\operatorname*{min}G_{\\lambda}\\right\\}}\\end{array}$ ", "page_idx": 5}, {"type": "text", "text": "Link between the lifted and bilevel reductions. The equality case in the statement of Prop. 3.1 shows that we can restrict the lifted reduction to measures $\\mu\\in\\mathscr{P}_{b}(\\Omega)$ of the form $\\mu(\\mathrm{d}r,\\mathrm{d}w)\\,=$ $\\delta_{f(w)}(\\mathrm{d}r)\\eta(\\mathrm{d}w)$ for some $f:\\mathcal{W}\\to\\mathbb{R}$ and $\\eta\\in\\mathcal{P}(\\mathcal{W})$ . Since they satisfy $h\\mu(\\mathrm{d}w)=f(w)\\eta(\\mathrm{d}w)$ , the lifted reduction with $b=2$ thus rewrites ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\eta\\in\\mathcal{P}(\\mathcal{W})}\\operatorname*{min}_{f\\in L^{2}(\\eta)}G(f\\eta)+\\frac{\\lambda}{2}\\int_{\\mathcal{W}}f(w)^{2}\\mathrm{d}\\eta(w).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "After the change of variable $(\\nu,\\eta)=(f\\eta,\\eta)$ , the outer objective is precisely $J_{\\lambda}(\\eta)$ . Thus, Wasserstein gradient flow on $J_{\\lambda}$ can be seen as a two-timescale optimization dynamics: it is the Wasserstein gradient flow on $F_{\\lambda,2}$ in the limit where $\\Gamma\\rightarrow\\infty$ . In the context of 2NN training with the parametrization (i), this amounts to training the output layer infinitely faster than the input layer, as done in [BMZ23; MB23; BBP23; TS24]. This remark allows to implement the bilevel MFLD numerically by discretizing in time the system of SDEs, for fixed large $N$ and $\\Gamma$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\forall i\\leq N,\\ \\mathrm{d}r_{t}^{i}=-\\Gamma\\ \\nabla_{r^{i}}F_{\\lambda,2}^{\\prime}[\\mu_{t}](r_{t}^{i},w_{t}^{i})\\mathrm{d}t\\qquad\\qquad\\qquad\\qquad\\qquad=-\\Gamma\\ \\left(G^{\\prime}[\\nu_{t}](w_{t}^{i})+\\lambda r_{t}^{i}\\right)\\mathrm{d}t\\qquad\\qquad\\qquad(3\\mathrm{~d~}\\Omega)}\\\\ &{}&{\\mathrm{d}w_{t}^{i}=-\\nabla_{w^{i}}F_{\\lambda,2}^{\\prime}[\\mu_{t}](r_{t}^{i},w_{t}^{i})\\mathrm{d}t+\\sqrt{2\\beta^{-1}}\\mathrm{d}B_{t}^{i}=-r_{t}^{i}\\nabla G^{\\prime}[\\nu_{t}](w_{t}^{i})\\mathrm{d}t+\\sqrt{2\\beta^{-1}}\\mathrm{d}B_{t}^{i}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "6We use $\\begin{array}{r}{\\int_{\\mathcal{W}}\\frac{|\\nu|^{2}}{\\eta}}\\end{array}$ as a shorthand for $\\begin{array}{r}{\\int_{\\mathcal{W}}\\big(\\frac{\\mathrm{d}\\nu}{\\mathrm{d}\\eta}\\big(w\\big)\\big)^{2}\\mathrm{d}\\eta(w)}\\end{array}$ . ", "page_idx": 5}, {"type": "text", "text": "where $\\begin{array}{r}{\\mu_{t}\\,=\\,\\frac{1}{N}\\sum_{i=1}^{N}\\delta_{(r_{t}^{i},w_{t}^{i})}}\\end{array}$ and $\\begin{array}{r}{\\nu_{t}\\,=\\,\\frac{1}{N}\\sum_{i=1}^{N}r_{t}^{i}\\delta_{w_{t}^{i}}}\\end{array}$ , and taking $\\begin{array}{r}{\\eta_{t}\\,=\\,\\frac{1}{N}\\sum_{i=1}^{n}\\delta_{w_{t}^{i}}}\\end{array}$ . Notice the absence of noise term on the weight variables $r$ ; it reflects the fact that MFLD for the bilevel objective is not a limit case of MFLD for the lifted objective, as the noise would prevent to reach optimality in the inner problem. ", "page_idx": 6}, {"type": "text", "text": "Compability with MFLD. We now show that, in contrast to the lifting reduction, the bilevel reduction is amenable to MFLD. The main assumption on (1.1) is as follows. ", "page_idx": 6}, {"type": "text", "text": "Assumption 1. $G:\\mathcal{M}(\\mathcal{W})\\;\\to\\;\\mathbb{R}$ is non-negative and admits second variations, and for each $i\\in\\{0,1,2\\}$ , there exist $L_{i},B_{i}<\\infty$ such that $\\left\\|\\nabla^{i}G^{\\prime\\prime}[\\nu](w,w^{\\prime})\\right\\|_{w}\\,\\leq\\,L_{i}$ and $\\left\\|\\nabla^{i}G^{\\prime}[\\nu]\\right\\|_{w}\\leq$ $L_{i}\\left\\|\\nu\\right\\|_{T V}+B_{i}$ for all $\\nu\\,\\in\\,\\mathcal{M}(\\mathcal{W})$ and $w,w^{\\prime}\\in\\mathcal{W}$ . Moreover there exists $\\widetilde{L}_{2}<\\infty$ such that $\\|\\nabla_{w}\\nabla_{w^{\\prime}}G^{\\prime\\prime}[\\nu](w,w^{\\prime})\\|\\le\\widetilde{L}_{2}$ for all $\\nu,w,w^{\\prime}$ . Furthermore, $\\mathcal{W}$ is compact and the uniform probability measure $\\tau$ on $\\mathcal{W}$ satisfies LSI with constant $\\alpha_{\\tau}$ . ", "page_idx": 6}, {"type": "text", "text": "Concrete settings that satisfy Assumption 1 are discussed in Sec. 5. The following proposition confirms the compatibility with MFLD and gives quantitative bounds on the LSI constant. ", "page_idx": 6}, {"type": "text", "text": "Proposition 3.4. Under Assumption $^{\\,l}$ , $J_{\\lambda}$ satisfies (P0), (P1) and (P2). More precisely, for any $\\eta\\in\\mathcal{P}(\\mathcal{W})$ , $J_{\\lambda}+\\beta^{-1}H$ satisfies local $L S I$ at $\\eta$ with the constant $\\begin{array}{r}{\\dot{\\alpha}_{\\hat{\\eta}}\\,=\\,\\alpha_{\\tau}\\,\\mathrm{exp}\\left(-\\frac1\\lambda\\dot{L_{0}}\\dot{\\beta}J_{\\lambda}(\\eta)\\right)}\\end{array}$ . Further, $J_{\\lambda}+\\beta^{-1}H$ satisfies $\\alpha$ -LSI uniformly along the MFLD trajectory $(\\eta_{t})_{t}$ with the constant $\\begin{array}{r}{\\alpha=\\alpha_{\\tau}\\exp\\left(-\\frac{1}{\\lambda}L_{0}\\beta\\operatorname*{min}\\left\\{G(0),J_{\\lambda}(\\eta_{0})+\\beta^{-1}H\\left(\\eta_{0}|\\tau\\right)\\right\\}\\right)}\\end{array}$ . ", "page_idx": 6}, {"type": "text", "text": "In view of the negative result of Prop. 3.2 for the lifting reduction, and the positive result of Prop. 3.4 for the bilevel reduction, in the sequel we focus on MFLD applied on $J_{\\lambda}$ , which we will refer to as MFLD-Bilevel. ", "page_idx": 6}, {"type": "text", "text": "4 Global convergence and annealing for MFLD-Bilevel ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "While the bounds from Prop. 3.4 along with Thm. 2.1 allow to establish global convergence to minimizers of $J_{\\lambda}+\\beta^{-1}H$ , our aim is to minimize the unregularized bilevel objective $J_{\\lambda}$ . This can be achieved by annealing the temperature parameter $\\beta^{-1}$ along the dynamics. Namely, Theorem 4.1 of [Chi22b] guarantees that by choosing $\\beta_{t}=c\\log(t)$ for an appropriate constant $c$ , the annealed MFLD trajectory ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\partial_{t}\\eta_{t}=\\mathrm{div}\\big(\\eta_{t}\\nabla J_{\\lambda}^{\\prime}[\\eta_{t}]\\big)+\\beta_{t}^{-1}\\Delta\\eta_{t}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "satisfies $\\begin{array}{r}{J_{\\lambda}(\\eta_{t})-\\operatorname*{inf}J_{\\lambda}=O\\left(\\frac{\\log\\log t}{\\log t}\\right)}\\end{array}$ This is a very slow rate however. ", "page_idx": 6}, {"type": "text", "text": "In this section, we show that the structure of $J_{\\lambda}$ originating from the bilevel reduction can be exploited to go beyond the generic guarantees from [Chi22b, Thm. 4.1]. Namely, we study in detail an alternative temperature annealing strategy, and we show that it improves upon the classical one $\\beta_{t}\\sim\\log(t)$ in terms of convergence to a fixed multiplicative accuracy. ", "page_idx": 6}, {"type": "text", "text": "4.1 Faster convergence to a fixed multiplicative accuracy ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Definition 4.1. Suppose $0\\not\\in\\arg\\operatorname*{min}G$ , so that $J_{\\lambda}^{\\ast}:=\\operatorname*{inf}\\,J_{\\lambda}>0$ . We will say that MFLD-Bilevel with a given temperature annealing schedule $(\\beta_{t})_{\\geq0}$ converges to $(1+\\Delta)$ -multiplicative accuracy in time-complexity $T_{\\Delta}$ , for a fixed positive constant $\\Delta$ (say $\\Delta=0.01)$ ), if $J_{\\lambda}(\\eta_{T_{\\Delta}})\\leq(1+\\Delta)J_{\\lambda}^{*}$ . ", "page_idx": 6}, {"type": "text", "text": "Note that in machine learning settings where the problem (1.1) corresponds to learning with overparameterized models, it is realistic to assume $J_{\\lambda}^{\\ast}$ to be small (as long as the regularization $\\lambda$ is small), and $T_{\\Delta}$ is the time it takes for the annealed MFLD to achieve a suboptimality of at most $\\Delta J_{\\lambda}^{*}$ . ", "page_idx": 6}, {"type": "text", "text": "For ease of comparison, let us report the time-complexity $T_{\\Delta}$ that can be achieved by simply running MFLD-Bilevel with a constant but well-chosen $\\beta$ , based on the bounds from Prop. 3.4 and Thm. 2.1. ", "page_idx": 6}, {"type": "text", "text": "Proposition 4.1 (Baseline \u201cannealing\u201d schedule: constant $\\beta_{t}$ ). Under Assumption $^{\\,l}$ , let $\\Delta\\,>\\,0$ and assume that $\\begin{array}{r}{\\Delta\\,\\le\\,\\frac{L_{0}L_{1}G(0)}{\\lambda^{2}J_{\\lambda}^{*}}}\\end{array}$ L0\u03bbL21JG\u2217(0). Then, MFLD-Bilevel with the temperature schedule \u2200t, \u03b2t = $\\begin{array}{r}{\\frac{4d}{\\Delta J_{\\lambda}^{*}}\\log\\left(\\frac{C B}{\\Delta J_{\\lambda}^{*}}\\right)}\\end{array}$ converges to $(1+\\Delta)$ -multiplicative accuracy in time ", "page_idx": 6}, {"type": "equation", "text": "$$\nT_{\\Delta}\\leq{\\frac{C^{\\prime}}{\\Delta J_{\\lambda}^{*}}}\\log\\left({\\frac{C B}{\\Delta J_{\\lambda}^{*}}}\\right)\\cdot\\exp\\left({\\frac{C^{\\prime}L_{0}G(0)}{\\lambda\\,\\Delta J_{\\lambda}^{*}}}\\log\\left({\\frac{C B}{\\Delta J_{\\lambda}^{*}}}\\right)\\right)\\cdot\\log\\left({\\frac{2G(0)}{\\Delta J_{\\lambda}^{*}}}+C^{\\prime}H\\left(\\eta_{0}|\\tau\\right)\\right)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $B=\\mathrm{poly}(L_{0},L_{1},B_{1},G(0),\\lambda^{-1})$ and $C,C^{\\prime}$ are constants dependent on $\\mathcal{W}$ (and $d$ and $\\alpha_{\\tau}$ ). ", "page_idx": 6}, {"type": "text", "text": "For the annealing schedule $\\beta_{t}\\,\\sim\\,\\log(t)$ , the time-complexity $T_{\\Delta}$ that can be guaranteed from inspecting the proof of [Chi22b, Thm. 4.1] has the same dependency on $d,\\lambda$ and $J_{\\lambda}^{\\ast}$ as for the baseline $\\beta_{t}=\\mathrm{cst}$ . ", "page_idx": 7}, {"type": "text", "text": "Improved annealing schedule. Recall the result of Prop. 3.4: for any $\\beta>0$ , $J_{\\lambda}+\\beta^{-1}H$ satisfies local $\\alpha_{\\hat{\\eta}}$ -LSI at $\\eta$ with $\\begin{array}{r}{\\alpha_{\\hat{\\eta}}=\\alpha_{\\tau}\\exp\\bigl(-\\frac{L_{0}}{\\lambda}\\beta J_{\\lambda}(\\eta)\\bigr)}\\end{array}$ . Informally, if we manage to control $J_{\\lambda}(\\eta_{t})$ along the annealed MFLD trajectory and show that it decreases, then we can increase $\\beta_{t}$ at the same rate, while retaining the same local LSI constant. This observation and the resulting annealing procedure were introduced in [SWON23], in a 2NN classification setting with the logistic loss. There the optimal value of the loss functional, corresponding to our $J_{\\lambda}^{\\ast}$ , is 0, and the annealing procedure yields favorable rates for global convergence. Here we show that this procedure is also applicable for MFLD-Bilevel, as soon as $G$ satisfies the mild Assumption 1, yielding favorable rates for convergence to a fixed multiplicative accuracy.7 ", "page_idx": 7}, {"type": "text", "text": "Theorem 4.2. Under Assumption $^{\\,l}$ , there exist constants $B\\,=\\,\\mathrm{poly}(L_{i},B_{i},G(0),\\lambda^{-1})$ and $C_{i}$ dependent only on $G(0)$ , $H(\\eta_{0})$ , $\\mathcal{W}$ (and $d$ and $\\alpha_{\\tau}$ ) such that the following holds. For any $\\begin{array}{r}{\\Delta\\leq\\frac{B}{J_{\\lambda}^{*}}}\\end{array}$ , MFLD-Bilevel with the temperature schedule $(\\beta_{t})_{t\\geq0}$ defined by $\\forall k\\leq K,\\forall t\\in[t_{k},t_{k+1}],\\beta_{t}=2^{k}d$ where $t_{0}=0$ and $K=\\lceil2\\log_{2}(B/(\\Delta J_{\\lambda}^{*}))\\rceil$ and ", "page_idx": 7}, {"type": "equation", "text": "$$\nt_{k+1}-t_{k}=C_{1}2^{k}\\;k\\cdot\\exp\\left(\\frac{L_{0}d}{\\lambda}\\left(\\frac{C_{3}}{\\Delta}\\log\\left(\\frac{B}{\\Delta J_{\\lambda}^{*}}\\right)+C_{2}\\right)\\right),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "achieves $(1+\\Delta)$ -multiplicative accuracy, with time-complexity ", "page_idx": 7}, {"type": "equation", "text": "$$\nT_{\\Delta}\\leq t_{K+1}\\leq\\frac{C_{4}}{\\Delta J_{\\lambda}^{*}}\\log\\left(\\frac{B}{\\Delta J_{\\lambda}^{*}}\\right)^{2}\\cdot\\exp\\left(\\frac{L_{0}d}{\\lambda}\\left(\\frac{C_{3}}{\\Delta}\\log\\left(\\frac{B}{\\Delta J_{\\lambda}^{*}}\\right)+C_{2}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Note that assuming that $G$ admits a minimizer $\\nu_{0}$ and that $\\operatorname*{min}G=0$ , as is typically the case in overparametrized machine learning settings, then by the envelope theorem $\\begin{array}{r}{J_{\\lambda}^{*}=\\operatorname*{inf}\\left(\\dot{G}+\\frac{\\lambda}{2}\\left\\lVert\\cdot\\right\\rVert_{T V}^{2}\\right)=}\\end{array}$ \u2225\u03bd02\u22252T V\u03bb+o(\u03bb). So in the regime of small \u03bb, ignoring the subexponential factors, the time complexity bound achieved by the annealing schedule of Thm. 4.2 scales as $\\exp\\left(c\\lambda^{-1}\\log\\lambda^{-1}\\right)$ for a constant $c$ . This improves upon the time complexity bound of the classical annealing procedure $\\beta_{t}\\sim\\log(t)$ (the same as in Prop. 4.1), which scales as $\\dot{\\exp}(c^{\\prime}\\lambda^{-2})$ . ", "page_idx": 7}, {"type": "text", "text": "5 Local LSI constant at optimality for learning a single neuron ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Devising temperature annealing schemes for global convergence, as illustrated in the previous section, relies on bounds on the local LSI constant at every iterate $\\eta_{t}$ of the (annealed) MFLD. Such bounds are readily provided by the widely applicable Holley-Stroock perturbation argument, on which for example our Prop. 3.4 is based, but may be overly pessimistic. Indeed in this section, we demonstrate that for MFLD-Bilevel, the LSI constant at convergence can be independent of $\\beta,\\,\\lambda$ and $d_{:}$ , instead of exponential in $\\beta$ as a global analysis would suggest. ", "page_idx": 7}, {"type": "text", "text": "More precisely, we are interested in $\\alpha^{*}$ , the best local LSI constant of $J_{\\lambda,\\beta}:=J_{\\lambda}+\\beta^{-1}H\\left(\\cdot|\\tau\\right)$ , at $\\eta_{\\lambda,\\beta}:=\\arg\\operatorname*{min}J_{\\lambda,\\beta}$ . In fact the proximal Gibbs measure of the optimum is the optimum itself: $\\widehat{\\eta_{\\lambda,\\beta}}=\\eta_{\\lambda,\\beta}$ , so $\\alpha^{*}$ is precisely the LSI constant of $\\eta_{\\lambda,\\beta}$ . A bound on $\\alpha^{*}$ is of interest, especially in the regime of large $\\beta$ (low entropic regularization), for two reasons. Firstly, it directly implies a local convergence bound on MFLD-Bilevel, as shown in the proposition below. Secondly, characterizing the dependency of $\\alpha^{*}$ on $\\beta$ may open the way to more efficient temperature annealing strategies; but this is out of the scope of this paper. ", "page_idx": 7}, {"type": "text", "text": "Proposition 5.1. Under Assumption $^{\\,l}$ , suppose $\\eta_{\\lambda,\\beta}$ satisfies LSI with some constant $\\alpha_{\\beta}^{*}$ . For any $\\varepsilon>0$ , there exists a sublevel set of $J_{\\lambda,\\beta}$ such that, for any initialization $\\eta_{0}$ in this sublevel set, J\u03bb,\u03b2(\u03b7t) \u2212inf J\u03bb,\u03b2 \u2264(J\u03bb,\u03b2(\u03b70) \u2212inf J\u03bb,\u03b2) e\u2212(\u03b1\u2217\u03b2\u03b2\u22121\u2212\u03b5)t. ", "page_idx": 7}, {"type": "image", "img_path": "Oo7HY9kmK6/tmp/1fd93333cf2758319415e7e91646499ec92acbe6e16f7fe241466d593630e9a7.jpg", "img_caption": ["(a) A comparison of Wasserstein GF on the bilevel objective with (i.e. MFLD) and without noise. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "Oo7HY9kmK6/tmp/0e1f864c3b07b0e01f82a348df3b74786310b70bb10de6516f8387ced5800525.jpg", "img_caption": ["(b) A comparison of MFLD applied to the Bilevel vs. Lifted formulations. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 1: The regularized training loss $G_{\\lambda}(\\nu)$ (1.1) of a 2NN with the ReLU activation, learning a teacher 2NN with the 4th degree Hermite polynomial as its activation. In both plots, $d=10$ and $\\lambda=\\beta^{-1}=10^{-3}$ . The implementation details are provided in Sec. F.4. Plots are averaged over 5 experiments. $G_{\\lambda}^{*}$ is the best value achieved at each experiment. In Fig. (1b), \u201cConic\u201d refers to using the metric (3.2) with $q_{r}=1,q_{w}=1$ , while \u201cCanonical\u201d refers to the choice of $q_{r}=2,q_{w}=0$ . ", "page_idx": 8}, {"type": "text", "text": "For the local LSI analysis, we focus on a specific setting of (1.1), namely, least-squares regression using a 2NN with a normalization constraint on the first-layer weights, and a single-neuron teacher network. See Fig. 1 for an illustrative numerical experiment. Note that Assumption 2, with additional bounded-moment assumptions on $\\varphi$ and $\\rho$ , is a special case of Assumption 1, as shown in Prop. F.4. ", "page_idx": 8}, {"type": "text", "text": "Assumption 2. $\\mathcal{W}=\\mathbb{S}^{d}$ is the Euclidean sphere in $\\mathbb{R}^{d+1}$ and there exist $\\rho$ a covariate distribution over $\\mathbb{R}^{\\bar{d}+1}$ , $y\\in L_{\\rho}^{2}(\\mathbb{R}^{d+1})$ a fixed target function, and $\\varphi:\\mathbb{R}\\rightarrow\\mathbb{R}$ a $\\mathcal{C}^{2}$ activation function such that $\\begin{array}{r}{G(\\nu)=\\frac{1}{2}\\mathbb{E}_{x\\sim\\rho}\\left|\\hat{y}_{\\nu}(x)-y(x)\\right|^{2}}\\end{array}$ where $\\begin{array}{r}{\\hat{y}_{\\nu}(x)=\\int_{\\mathcal{W}}\\varphi(\\langle w,x\\rangle)\\mathrm{d}\\nu(w).}\\end{array}$ . ", "page_idx": 8}, {"type": "text", "text": "Under the above assumption, we show in Prop. F.1 a simplified expression for the bilevel objective and its first variation, ", "page_idx": 8}, {"type": "equation", "text": "$$\nJ_{\\lambda}(\\eta)=\\frac{\\lambda}{2}\\langle y,(K_{\\eta}+\\lambda\\mathrm{id})^{-1}y\\rangle_{L_{\\rho}^{2}},\\qquad J_{\\lambda}^{\\prime}[\\eta](w)=-\\frac{\\lambda}{2}\\langle\\varphi(\\langle w,\\cdot\\rangle),(K_{\\eta}+\\lambda\\mathrm{id})^{-1}y\\rangle_{L_{\\rho}^{2}}^{2},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $K_{\\eta}$ is the integral operator in $L_{\\rho}^{2}$ of the kernel $\\begin{array}{r}{k_{\\eta}(x,x^{\\prime})=\\int\\varphi(\\langle w,x\\rangle)\\varphi(\\langle w,x^{\\prime}\\rangle)\\mathrm{d}\\eta(w)}\\end{array}$ and id is the identity operator on $L_{\\rho}^{2}$ . Additionally, we make the following assumption on the data distribution $\\rho$ and on the response $y$ . ", "page_idx": 8}, {"type": "text", "text": "Assumption 3. $\\rho$ is rotationally invariant and the labels come from a single-index model: $y=$ $\\varphi(\\langle v,x{\\bar{\\rangle}})$ for some fixed $v\\in\\mathcal{W}$ . ", "page_idx": 8}, {"type": "text", "text": "With the above assumptions, we can state the main theorem of this section. ", "page_idx": 8}, {"type": "text", "text": "Theorem 5.2. Under Assumptions 2 and 3, there exists a function $g:[-1,+1]\\to\\mathbb{R}_{+}$ such that $J_{\\lambda}^{\\prime}[\\delta_{v}](w)\\,=\\,-\\lambda g(\\langle w,v\\rangle)$ for any $w\\,\\in\\,\\mathbb{S}^{d}$ . Suppose that $\\lambda\\leq1$ and that there exist constants $c_{i},C_{i}>0$ such that for all $r\\in[-1,+1]$ , ", "page_idx": 8}, {"type": "equation", "text": "$$\nc_{1}\\leq g^{\\prime}(r)\\leq C_{1},\\quad g^{\\prime\\prime}(r)\\geq-C_{2},\\quad\\left|g^{\\prime\\prime}(r)(1-r^{2})^{1/2}\\right|\\leq C_{3},\\quad\\left|g^{\\prime\\prime\\prime}(r)(1-r^{2})^{3/2}\\right|\\leq C_{4}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Then there exist constants $\\alpha_{v}$ , $D_{0}$ (dependent only on the $c_{i},C_{i})$ such that for any $\\beta\\geq D_{0}d\\lambda^{-1}$ , \u03b4 v \u221d e\u2212\u03b2J\u2032\u03bb[\u03b4v]\u03c4 satisfies \u03b1v-LSI. Furthermore, if additionally d12 Ex\u223c\u03c1 \u2225x\u22254 ,  \u03c6(i)  L4(\u03c1) < \u221e for $i\\,\\in\\,\\{0,1,2\\}$ where $\\begin{array}{r}{\\|\\varphi\\|_{L^{p}(\\rho)}^{p}\\,:=\\,\\int|\\varphi(\\langle w,x\\rangle)|^{p}\\mathrm{d}\\rho(x)}\\end{array}$ (independent of $w$ as $\\rho$ is rotationally invariant), then there exists a constant $\\alpha^{*}$ dependent only on those constants and on the $c_{i},C_{i}$ such that, provided that $\\beta\\geq\\mathrm{poly}(d,\\lambda^{-1}),\\,\\eta_{\\lambda,\\beta}$ satisfies $\\alpha^{*}{-}L S I$ . ", "page_idx": 8}, {"type": "text", "text": "The proof is based on the observation that $\\eta_{\\lambda,\\beta}\\,\\approx\\,\\arg\\operatorname*{min}J_{\\lambda}\\,=\\,\\delta_{v}$ the Dirac measure at $v$ , for certain regimes of $\\beta$ and $\\lambda$ , in the Wasserstein metric. Thus we show that $J_{\\lambda}^{\\prime}[\\delta_{v}]$ is amenable to a Lyapunov type argument inspired from [MS14; LE23], and then transfer its properties to ${\\cal J}_{\\lambda}^{\\prime}[\\eta_{\\lambda,\\beta}]$ . ", "page_idx": 8}, {"type": "text", "text": "We now verify the assumptions of Thm. 5.2 for a class of smooth, non-negative, and monotone activations which includes some popular practical choices such as the Softplus $\\bar{\\varphi}(z)=\\ln(1\\!+\\!e^{z})$ and sigmoid $\\varphi(z)=1/(1+e^{-z})$ . While we only consider smooth activations here for simplicity, certain non-smooth activations such as a leaky version of ReLU can also satisfy the conditions of Thm. 5.2. ", "page_idx": 9}, {"type": "text", "text": "Proposition 5.3. Suppose Assumptions 2 and 3 hold, and $b_{1}(d+1)\\leq\\mathbb{E}[\\left\\|x\\right\\|^{2}]\\leq\\mathbb{E}[\\left\\|x\\right\\|^{12}]^{1/6}\\leq$ $b_{2}(d+1)$ for constants $b_{1},b_{2}\\;>\\;0$ . Let $m\\,:=\\,2b_{2}^{3/2}/b_{1}$ . Suppose $\\varphi$ and $\\varphi^{\\prime}$ are non-negative, $\\operatorname*{inf}_{|z|\\leq m}\\varphi(z)\\wedge\\varphi^{\\prime}(z)>0$ and $\\left\\|\\varphi^{(i)}\\right\\|_{L^{4}(\\rho)}<\\infty$ for $i\\leq3$ . Then, $\\varphi$ satisfies the assumptions of Thm. 5.2 with constants that only depend on $b_{1}$ , $b_{2}$ , and $\\varphi$ . ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we investigated how mean-field Langevin dynamics (MFLD), an optimization dynamics over probability measures with global convergence guarantees, can be leveraged to solve convex optimization problems over signed measures of the form (1.1). For a large class of objectives $G$ , we highlighted that MFLD with a lifting approach necessarily runs into some issues, whereas the bilevel approach always inherits the guarantees of MFLD, leading to convergence guarantees for $G_{\\lambda}$ via annealing. Finally, turning to a 2-layer NN learning task which can be stated as an instance of (1.1), we showed that the local LSI constant of MFLD-Bilevel can scale much more favorably with $d$ and $\\beta$ than a generic analysis would suggest. ", "page_idx": 9}, {"type": "text", "text": "Another approach to tackle (1.1) could be to build noisy particle dynamics directly in the space of signed measures, complementing the MFLD updates with, for instance, a birth-death process. A challenge then is to build such dynamics that can be efficiently discretized. It is also an interesting question for future works to find other settings to which MFLD can be extended, beyond signed measures. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[AH12] Kendall Atkinson and Weimin Han. Spherical harmonics and approximations on the unit sphere: an introduction. Vol. 2044. Springer Science & Business Media, 2012.   \n[Bac17] Francis Bach. \u201cBreaking the curse of dimensionality with convex neural networks\u201d. In: The Journal of Machine Learning Research 18.1 (2017), pp. 629\u2013681.   \n[Bac19] Francis Bach. \u201cThe \u201c $\\eta$ -trick\u201d reloaded: multiple kernel learning\u201d. In: (2019).   \n[Bac21] Francis Bach. \u201cThe quest for adaptivity\u201d. In: (2021).   \n[BGL14] Dominique Bakry, Ivan Gentil, and Michel Ledoux. Analysis and geometry of Markov diffusion operators. Vol. 103. Springer, 2014.   \n[BRVDM05] Yoshua Bengio, Nicolas Roux, Pascal Vincent, Olivier Delalleau, and Patrice Marcotte. \u201cConvex neural networks\u201d. In: Advances in neural information processing systems 18 (2005).   \n[BMZ23] Rapha\u00ebl Berthier, Andrea Montanari, and Kangjie Zhou. \u201cLearning time-scales in two-layers neural networks\u201d. In: arXiv preprint arXiv:2303.00055 (2023).   \n[BBP23] Alberto Bietti, Joan Bruna, and Loucas Pillaud-Vivien. \u201cOn learning gaussian multiindex models with gradient flow\u201d. In: arXiv preprint arXiv:2310.19793 (2023).   \n[Bou23] Nicolas Boumal. An introduction to optimization on smooth manifolds. Cambridge University Press, 2023.   \n[BBI01] Dmitri Burago, Yuri Burago, and Sergei Ivanov. A course in metric geometry. Vol. 33. American Mathematical Society Providence, 2001.   \n[CD13] Ren\u00e9 Carmona and Fran\u00e7ois Delarue. \u201cProbabilistic analysis of mean-field games\u201d. In: SIAM Journal on Control and Optimization 51.4 (2013), pp. 2705\u20132734.   \n[CVBM02] Olivier Chapelle, Vladimir Vapnik, Olivier Bousquet, and Sayan Mukherjee. \u201cChoosing multiple parameters for support vector machines\u201d. In: Machine learning 46 (2002), pp. 131\u2013159.   \n[CLRW24] Fan Chen, Yiqing Lin, Zhenjie Ren, and Songbo Wang. \u201cUniform-in-time propagation of chaos for kinetic mean field Langevin dynamics\u201d. In: Electronic Journal of Probability 29 (2024), pp. 1\u201343.   \n[CRW22] Fan Chen, Zhenjie Ren, and Songbo Wang. \u201cUniform-in-time propagation of chaos for mean field langevin dynamics\u201d. In: arXiv preprint arXiv:2212.03050 (2022).   \n[Chi17] L\u00e9na\u00efc Chizat. \u201cUnbalanced optimal transport: Models, numerical methods, applications\u201d. PhD thesis. Universit\u00e9 Paris sciences et lettres, 2017.   \n[Chi22a] L\u00e9na\u00efc Chizat. \u201cConvergence rates of gradient methods for convex optimization in the space of measures\u201d. In: Open Journal of Mathematical Optimization 3 (2022), pp. 1\u201319.   \n[Chi22b] L\u00e9na\u00efc Chizat. \u201cMean-Field Langevin Dynamics: Exponential Convergence and Annealing\u201d. In: Transactions on Machine Learning Research (2022).   \n[Chi22c] L\u00e9na\u00efc Chizat. \u201cSparse optimization on measures with over-parameterized gradient descent\u201d. In: Mathematical Programming 194.1-2 (2022), pp. 487\u2013532.   \n[CB18] L\u00e9na\u00efc Chizat and Francis Bach. \u201cOn the global convergence of gradient descent for over-parameterized models using optimal transport\u201d. In: Advances in neural information processing systems 31 (2018).   \n[CB20] L\u00e9na\u00efc Chizat and Francis Bach. \u201cImplicit bias of gradient descent for wide two-layer neural networks trained with the logistic loss\u201d. In: Conference on Learning Theory. PMLR. 2020, pp. 1305\u20131338.   \n[DG12] Yohann De Castro and Fabrice Gamboa. \u201cExact reconstruction using Beurling minimal extrapolation\u201d. In: Journal of Mathematical Analysis and applications 395.1 (2012), pp. 336\u2013354.   \n[DDPS19] Quentin Denoyelle, Vincent Duval, Gabriel Peyr\u00e9, and Emmanuel Soubies. \u201cThe sliding Frank\u2013Wolfe algorithm and its application to super-resolution microscopy\u201d. In: Inverse Problems 36.1 (2019), p. 014001.   \n[FE12] Christopher Frye and Costas J Efthimiou. \u201cSpherical harmonics in p dimensions\u201d. In: arXiv preprint arXiv:1205.3548 (2012).   \n[FW23] Qiang Fu and Ashia Wilson. \u201cMean-field Underdamped Langevin Dynamics and its Space-Time Discretization\u201d. In: arXiv preprint arXiv:2312.16360 (2023).   \n[GCM23] S\u00e9bastien Gadat, Yohann de Castro, and Cl\u00e9ment Marteau. \u201cFastPart: OverParameterized Stochastic Gradient Descent for Sparse optimisation on Measures\u201d. In: (2023).   \n[GV79] A. Gray and L. Vanhecke. \u201cRiemannian geometry as determined by the volumes of small geodesic balls\u201d. In: Acta Mathematica 142 (1979), pp. 157\u2013198.   \n[GGGM21] Charles Guille-Escuret, Manuela Girotti, Baptiste Goujaud, and Ioannis Mitliagkas. \u201cA study of condition numbers for first-order optimization\u201d. In: International Conference on Artificial Intelligence and Statistics. PMLR. 2021, pp. 1261\u20131269.   \n[HS86] Richard Holley and Daniel W Stroock. \u201cLogarithmic Sobolev inequalities and stochastic Ising models\u201d. In: (1986).   \n[HR\u0160S21] Kaitong Hu, Zhenjie Ren, David \u0160i\u0161ka, and \u0141ukasz Szpruch. \u201cMean-field Langevin dynamics and energy landscape of neural networks\u201d. In: Annales de l\u2019Institut Henri Poincar\u00e9 (B) Probabilit\u00e9s et statistiques. Vol. 57. 4. Institut Henri Poincar\u00e9. 2021, pp. 2043\u20132065.   \n$[\\mathrm{KZCE}{+}24]$ Yunbum Kook, Matthew S Zhang, Sinho Chewi, Murat A Erdogdu, et al. \u201cSampling from the Mean-Field Stationary Distribution\u201d. In: arXiv preprint arXiv:2402.07355 (2024).   \n[Lac18] Daniel Lacker. \u201cMean field games and interacting particle systems\u201d. In: preprint (2018).   \n[LCBGJ04] Gert RG Lanckriet, Nello Cristianini, Peter Bartlett, Laurent El Ghaoui, and Michael I Jordan. \u201cLearning the kernel matrix with semidefinite programming\u201d. In: Journal of Machine Learning Research 5.Jan (2004), pp. 27\u201372.   \n[Lee18] John M Lee. Introduction to Riemannian manifolds. Vol. 2. Springer, 2018.   \n[LE23] Mufan Li and Murat A Erdogdu. \u201cRiemannian langevin algorithm for solving semidefinite programs\u201d. In: Bernoulli 29.4 (2023), pp. 3093\u20133113.   \n[LMZ20] Yuanzhi Li, Tengyu Ma, and Hongyang R Zhang. \u201cLearning over-parametrized twolayer neural networks beyond NTK\u201d. In: Conference on learning theory. PMLR. 2020, pp. 2613\u20132682.   \n[LMS18] Matthias Liero, Alexander Mielke, and Giuseppe Savar\u00e9. \u201cOptimal entropy-transport problems and a new Hellinger\u2013Kantorovich distance between positive measures\u201d. In: Inventiones mathematicae 211.3 (2018), pp. 969\u20131117.   \n[MB23] Pierre Marion and Rapha\u00ebl Berthier. \u201cLeveraging the two-timescale regime to demonstrate convergence of neural networks\u201d. In: Advances in Neural Information Processing Systems 36 (2023).   \n[MMN18] Song Mei, Andrea Montanari, and Phan-Minh Nguyen. \u201cA mean field view of the landscape of two-layer neural networks\u201d. In: Proceedings of the National Academy of Sciences 115.33 (2018), E7665\u2013E7671.   \n[MS14] Georg Menz and Andr\u00e9 Schlichting. \u201cPoincar\u00e9 and logarithmic Sobolev inequalities by decomposition of the energy landscape\u201d. In: The Annals of Probability 42.5 (2014), pp. 1809\u20131884.   \n[MZ04] Ilya Molchanov and Sergei Zuyev. \u201cOptimisation in space of measures and optimal design\u201d. In: ESAIM: Probability and Statistics 8 (2004), pp. 12\u201324.   \n[NS17] Atsushi Nitanda and Taiji Suzuki. \u201cStochastic particle gradient descent for infinite ensembles\u201d. In: arXiv preprint arXiv:1712.05438 (2017).   \n[NWS22] Atsushi Nitanda, Denny Wu, and Taiji Suzuki. \u201cConvex analysis of the mean field Langevin dynamics\u201d. In: International Conference on Artificial Intelligence and Statistics. PMLR. 2022, pp. 9741\u20139757.   \n[OV00] Felix Otto and C\u00e9dric Villani. \u201cGeneralization of an inequality by Talagrand and links with the logarithmic Sobolev inequality\u201d. In: Journal of Functional Analysis 173.2 (2000), pp. 361\u2013400.   \n[PP21] Clarice Poon and Gabriel Peyr\u00e9. \u201cSmooth bilevel programming for sparse regularization\u201d. In: Advances in Neural Information Processing Systems 34 (2021), pp. 1543\u2013 1555.   \n[PP23] Clarice Poon and Gabriel Peyr\u00e9. \u201cSmooth over-parameterized solvers for non-smooth structured optimization\u201d. In: Mathematical Programming (2023), pp. 1\u201356.   \n[RBCG08] Alain Rakotomamonjy, Francis Bach, St\u00e9phane Canu, and Yves Grandvalet. \u201cSimpleMKL\u201d. In: Journal of Machine Learning Research 9 (2008), pp. 2491\u20132521.   \n[RV22] Grant Rotskoff and Eric Vanden-Eijnden. \u201cTrainability and accuracy of artificial neural networks: An interacting particle system approach\u201d. In: Communications on Pure and Applied Mathematics 75.9 (2022), pp. 1889\u20131935.   \n[San15] Filippo Santambrogio. \u201cOptimal transport for applied mathematicians\u201d. In: Birk\u00e4user, NY 55.58-63 (2015), p. 94.   \n[SS20] Justin Sirignano and Konstantinos Spiliopoulos. \u201cMean field analysis of neural networks: A central limit theorem\u201d. In: Stochastic Processes and their Applications 130.3 (2020), pp. 1820\u20131852.   \n[SWN23] Taiji Suzuki, Denny Wu, and Atsushi Nitanda. \u201cMean-field Langevin dynamics: Time-space discretization, stochastic gradient, and variance reduction\u201d. In: Advances in Neural Information Processing Systems 36 (2023).   \n[SWON23] Taiji Suzuki, Denny Wu, Kazusato Oko, and Atsushi Nitanda. \u201cFeature learning via mean-field Langevin dynamics: classifying sparse parities and beyond\u201d. In: Thirtyseventh Conference on Neural Information Processing Systems. 2023.   \n[Szn91] Alain-Sol Sznitman. \u201cTopics in propagation of chaos\u201d. In: Ecole d\u2019\u00e9t\u00e9 de probabilit\u00e9s de Saint-Flour XIX\u20141989 1464 (1991), pp. 165\u2013251.   \n[TS24] Shokichi Takakura and Taiji Suzuki. Mean-field Analysis on Two-layer Neural Networks from a Kernel Perspective. 2024.   \n[Ver18] Roman Vershynin. High-Dimensional Probability: An Introduction with Applications in Data Science. Cambridge University Press, 2018.   \n[Vil09] C\u00e9dric Villani. Optimal transport: old and new. Vol. 338. Springer, 2009.   \n[YWR23] Yuling Yan, Kaizheng Wang, and Philippe Rigollet. \u201cLearning gaussian mixtures using the Wasserstein-Fisher-Rao gradient flow\u201d. In: arXiv preprint arXiv:2301.01766 (2023). ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Details for Sec. 1 (introduction) ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A.1 Using $\\left\\|\\cdot\\right\\|_{T V}^{2}$ vs. $\\lVert\\cdot\\rVert_{T V}$ as the regularization term ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "The optimization problems we consider in this paper are of the form (1.1), that is, for ease of reference, ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\nu\\in\\mathcal{M}(\\mathcal{W})}G_{\\lambda}(\\nu),\\qquad\\qquad\\qquad G_{\\lambda}(\\nu):=G(\\nu)+\\frac{\\lambda}{2}\\left\\lVert\\nu\\right\\rVert_{T V}^{2}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Note the regularization term $\\frac{\\lambda}{2}\\left\\Vert\\nu\\right\\Vert_{T V}^{2}$ . This is to be contrasted with the more usual form of optimization problems ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\nu\\in\\mathcal{M}(\\mathcal{W})}\\widetilde{G}_{\\tilde{\\lambda}}(\\nu),\\qquad\\qquad\\qquad\\widetilde{G}_{\\tilde{\\lambda}}(\\nu):=G(\\nu)+\\tilde{\\lambda}\\,\\|\\nu\\|_{T V}\\,,\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "which uses $\\|\\nu\\|_{T V}$ as the regularization. ", "page_idx": 12}, {"type": "text", "text": "On the level of variational problems, these two classes of problems are equivalent, in the sense that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\{0\\}\\cup\\bigcup_{\\lambda\\geq0}\\arg\\operatorname*{min}G_{\\lambda}=\\{0\\}\\cup\\bigcup_{\\tilde{\\lambda}\\geq0}\\arg\\operatorname*{min}\\widetilde{G}_{\\tilde{\\lambda}}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $\\mathrm{{}^{\\circ}}$ refers to the zero measure on $\\mathcal{W}$ . Indeed, note that by convexity, the argmins are determined by the respective first-order optimality conditions, so that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\bigcup_{\\lambda\\geq0}\\arg\\operatorname*{min}G_{\\lambda}=\\left\\{\\nu\\in\\mathcal{M}(\\mathcal{W});\\,\\forall w,G^{\\prime}[\\nu](w)+\\lambda\\,\\|\\nu\\|_{T V}\\,\\frac{\\nu(\\mathrm{d}w)}{|\\nu(\\mathrm{d}w)|}=0,\\;\\;\\lambda\\in\\mathbb{R}_{+}\\right\\}}\\\\ &{\\displaystyle\\bigcup_{\\lambda\\geq0}\\arg\\operatorname*{min}\\widetilde{G}_{\\widetilde{\\lambda}}=\\left\\{\\nu\\in\\mathcal{M}(\\mathcal{W});\\,\\forall w,G^{\\prime}[\\nu](w)+\\widetilde{\\lambda}\\frac{\\nu(\\mathrm{d}w)}{|\\nu(\\mathrm{d}w)|}=0,\\;\\;\\widetilde{\\lambda}\\in\\mathbb{R}_{+}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "To see that the set on the first line is contained in the second, let $\\nu\\in\\arg\\operatorname*{min}G_{\\lambda}$ , then $\\nu$ satisfies the first-order optimality condition for $\\widetilde{G}_{\\tilde{\\lambda}}$ with $\\tilde{\\lambda}=\\lambda\\,\\|\\nu\\|_{T V}$ . Conversely, if $\\nu\\in\\arg\\operatorname*{min}\\widetilde{G}_{\\tilde{\\lambda}}$ then either $\\nu=0$ or $\\nu\\in\\arg\\operatorname*{min}G_{\\lambda}$ with $\\begin{array}{r}{\\lambda=\\frac{\\tilde{\\lambda}}{\\|\\nu\\|_{T V}}}\\end{array}$ ", "page_idx": 12}, {"type": "text", "text": "In terms of optimization convergence guarantees, when using the reduction by lifting, the problems with $\\lVert\\cdot\\rVert_{T V}$ vs. with $\\left\\|\\cdot\\right\\|_{T V}^{2}$ regularization give rise to similar analyses, as discussed in Rem. 3.1. However when using the reduction by bilevel optimization, it seems that only the problem with $\\left\\|\\cdot\\right\\|_{T V}^{2}$ regularization is amenable to a precise analysis. This is perhaps most apparent in our derivation of the simplified expression for the bilevel objective, Prop. D.2. ", "page_idx": 12}, {"type": "text", "text": "A.2 Detailed comparison with Takakura and Suzuki [TS24] ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In this subsection, we show that the learning dynamics considered by [TS24, Sec. 2, 3] is an instance of a variant of MFLD applied to the bilevel reduction of (1.1). We do this by recalling their setting (in the case of single-task learning for simplicity) in notations that are compatible with ours. ", "page_idx": 12}, {"type": "text", "text": "\u2022 For a set of first-layer weights $w_{i}\\,\\in\\,\\mathcal{W}\\,:=\\,\\mathbb{R}^{d}$ and second-layer weights $a_{i}\\in\\mathbb{R}$ (for $1\\,\\leq\\,i\\,\\leq\\,N)$ , and an activation function $\\varphi:\\mathbb{R}\\rightarrow\\mathbb{R}$ , the associated 2NN is defined as $\\begin{array}{r}{x\\mapsto\\frac{1}{N}\\sum_{i=1}^{N}a_{i}\\varphi(w_{i}^{\\top}x)}\\end{array}$ .   \n\u2022 For $\\mu\\in\\mathcal{P}(\\mathbb{R}\\times\\mathcal{W})$ , the associated infinite-width 2NN is $\\begin{array}{r}{x\\mapsto\\int_{\\mathbb{R}\\times\\mathcal{W}}a\\varphi(w^{\\top}x)\\mathrm{d}\\mu(a,w)}\\end{array}$ . Note that in our notation of Sec. 3.1, this also writes $\\begin{array}{r}{x\\mapsto\\int_{\\mathcal{W}}\\varphi(\\boldsymbol{w}^{\\top}\\boldsymbol{x})\\mathrm{d}[h\\mu](\\boldsymbol{w})}\\end{array}$ .   \n\u2022 Consider a data distribution $\\rho(\\mathrm{d}x,\\mathrm{d}y)\\in\\mathcal{P}(\\mathbb{R}_{x}^{d}\\times\\mathbb{R}_{y})$ . We may define the Hilbert space of predictors $\\mathcal{H}=L_{\\rho^{x}}^{2}(\\mathbb{R}_{x}^{d})$ , and the \u201csingle first-layer neuron predictor\u201d mapping $\\phi:\\mathcal{W}\\to\\mathcal{H}$ by $\\phi(w)(x)=\\varphi(w^{\\top}x)$ . The predictor associated to an infinite-width 2NN parametrized by $\\mu$ is then $\\begin{array}{r}{\\int_{\\mathbb{R}\\times\\mathcal{W}}a\\phi(\\boldsymbol{w})\\mathrm{d}\\mu(\\boldsymbol{a},\\boldsymbol{w})}\\end{array}$ . ", "page_idx": 12}, {"type": "text", "text": "\u2022 Consider a loss function $\\ell(\\hat{y},y):\\mathbb{R}_{y}\\times\\mathbb{R}_{y}\\rightarrow\\mathbb{R},$ , inducing a risk functional over predictors given by $R(h)=\\mathbb{E}_{(x,y)\\sim\\rho}[\\ell(h(x),\\stackrel{\\cdot}{y})]$ . We may define the unregularized risk functional over (infinite-width) 2NN weights by ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\mu)=R\\left(\\int_{\\mathbb{R}\\times\\mathcal{W}}a\\phi(w)\\mathrm{d}\\mu(a,w)\\right)=R\\left(\\int_{\\mathcal{W}}\\phi(w)\\mathrm{d}[h\\mu](w)\\right).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Accordingly, let the operator $\\Phi:\\mathcal{M}(\\mathcal{W})\\to\\mathcal{H}$ such that $\\begin{array}{r}{\\Phi\\nu=\\int_{\\mathcal W}\\phi(w)\\mathrm{d}\\nu(w)}\\end{array}$ , and ", "page_idx": 13}, {"type": "equation", "text": "$$\nG(\\nu)=R(\\Phi\\nu)=R\\left(\\int_{\\mathcal{W}}\\phi(w)\\mathrm{d}\\nu(w)\\right).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Then the unregularized risk is ${\\mathcal{L}}(\\mu)=G(h\\mu)$ . ", "page_idx": 13}, {"type": "text", "text": "\u2022 The regularized risk functional considered in [TS24, Sec. 2.1] is ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathcal{F}(\\mu)=R\\left(\\int_{\\mathbb{R}\\times\\mathcal{W}}a\\phi(w)\\mathrm{d}\\mu(a,w)\\right)+\\frac{\\lambda}{2}\\int_{\\mathbb{R}\\times\\mathcal{W}}a^{2}\\mathrm{d}\\mu(a,w)+\\frac{1}{2\\sigma^{2}}\\int_{\\mathbb{R}\\times\\mathcal{W}}\\left\\|w\\right\\|^{2}\\mathrm{d}\\mu(a,w)}\\\\ {\\displaystyle\\qquad=G(h\\mu)+\\frac{\\lambda}{2}\\int_{\\mathbb{R}\\times\\mathcal{W}}a^{2}\\mathrm{d}\\mu(a,w)+\\frac{1}{2\\sigma^{2}}\\int_{\\mathbb{R}\\times\\mathcal{W}}\\left\\|w\\right\\|^{2}\\mathrm{d}\\mu(a,w).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "(More precisely, \u201c $\\mathbf{\\nabla}\\cdot\\boldsymbol{F}(f,\\eta)^{\\circ}$ in their notation corresponds to our $\\mathcal{F}\\left(\\delta_{f(w)}(\\mathrm{d}a)\\eta(\\mathrm{d}w)\\right)$ , their \u201c $\\::\\overline{{{\\lambda}}}_{a}^{\\,\\,,,,}\\:$ corresponds to our $\\lambda$ , and their $\\textstyle{\\sqrt{\\lambda}}_{w}{}^{,,}$ corresponds to our $1/\\sigma^{2}$ .) Note that, in our notation of Sec. 3.1, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathcal{F}(\\mu)=F_{\\lambda,2}(\\mu)+\\frac{1}{2\\sigma^{2}}\\int_{\\mathbb{R}\\times\\mathcal{W}}\\left\\|w\\right\\|^{2}\\mathrm{d}\\mu(a,w).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "\u2022 The bilevel limiting functional, which is the main object of study of [TS24, Sec. 2.1], is then defined as the mapping $\\mathcal{G}:\\mathcal{P}(\\mathcal{W})\\rightarrow\\mathbb{R}$ such that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathcal{G}(\\boldsymbol{\\eta})=\\operatorname*{inf}_{f:\\mathcal{W}\\to\\mathbb{R}}\\mathcal{F}\\left(\\delta_{f(w)}(\\mathrm{d}r)\\boldsymbol{\\eta}(\\mathrm{d}w)\\right),\\quad\\mathrm{corresponding~precisely~to}}\\\\ {\\displaystyle\\mathcal{G}(\\boldsymbol{\\eta})=J_{\\lambda}(\\boldsymbol{\\eta})+\\frac{1}{2\\sigma^{2}}\\int_{\\mathcal{W}}\\left\\|w\\right\\|^{2}\\mathrm{d}\\eta(w)}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "in our notation of Sec. 3.2 (see the paragraph \u201cLink between the lifted and bilevel reductions\u201d). Interestingly, the convexity of $\\mathcal{G}$ is almost immediate with our presentation, as it is expressed as a partial minimization of a convex function, whereas the proof of the convexity of $\\mathcal{G}$ in [TS24] is quite involved. ", "page_idx": 13}, {"type": "text", "text": "They also introduce a functional \u201c $U^{\\bullet}$ which corresponds precisely to our $J_{\\lambda}(\\eta)$ , and which is an important auxiliary object in their analysis. ", "page_idx": 13}, {"type": "text", "text": "\u2022 The learning dynamics studied from Section 2.3 onwards in [TS24] (except for the label noise procedure in Section 5), is precisely MFLD for $\\mathcal{G}(\\eta)$ : ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\partial_{t}\\eta_{t}=\\beta^{-1}\\Delta\\eta_{t}+\\mathrm{div}\\left(\\eta_{t}\\nabla\\mathcal{G}^{\\prime}[\\eta_{t}]\\right)}\\\\ &{\\qquad=\\beta^{-1}\\Delta\\eta_{t}+\\mathrm{div}\\left(\\eta_{t}\\left(\\nabla J_{\\lambda}^{\\prime}[\\eta_{t}]+\\frac{1}{\\sigma^{2}}w\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "(and their constant \u201c $\\lambda$ \u201d corresponds to our $\\beta^{-1}$ ). ", "page_idx": 13}, {"type": "text", "text": "$\\mathbf{^{6}M F L+}$ confining\u201d dynamics. The PDE (A.2) can be interpreted as a variant of MFLD for $J_{\\lambda}$ in two ultimately equivalent ways: one is as the MFLD PDE (2.1) with an added \u201cconfining\u201d term $-\\textstyle{\\frac{1}{\\sigma^{2}}}w$ , which intuitively encourages the noisy particles to remain close to the origin. Another is as Wasserstein gradient flow for the regularized functional ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle J_{\\lambda,\\beta,\\sigma}=J_{\\lambda}+\\beta^{-1}H+\\frac{1}{2\\sigma^{2}}\\int_{\\mathcal W}\\left\\|w\\right\\|^{2}\\mathrm d\\eta(w)}\\\\ {\\displaystyle\\qquad=J_{\\lambda}+\\beta^{-1}H\\left(\\cdot\\right|\\beta^{-1/2}\\sigma\\gamma\\right)\\quad\\mathrm{where}\\quad\\beta^{-1/2}\\sigma\\gamma:=\\mathcal N(0,\\beta^{-1}\\sigma^{2}I_{d}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "whereas MFLD for $J_{\\lambda}$ is the Wasserstein gradient flow for the functional regularized by entropy only, $J_{\\lambda,\\beta}=J_{\\lambda}+\\beta^{-1}H\\left(\\cdot|\\tau\\right)=J_{\\lambda}+\\beta^{-1}H+\\mathrm{\\bar{c}s t}$ . Unsurprisingly in view of this second interpretation, the distribution $\\beta^{-1/2}\\sigma\\gamma$ plays a similar role in the analysis of convergence of (A.2) [TS24, Lemma 3.5], as played by the uniform measure $\\tau$ in our paper: the local LSI property of $J_{\\lambda,\\beta,\\sigma}$ (resp. $J_{\\lambda,\\beta}$ ) is obtained by applying the Holley-Stroock argument using $\\beta^{-1/2}\\sigma\\gamma$ (resp. $\\tau$ ) as a reference measure. ", "page_idx": 14}, {"type": "text", "text": "Note that the additional confining term $-\\textstyle{\\frac{1}{\\sigma^{2}}}w$ in (A.2) cannot be captured straightforwardly by any additional penalty term on the objective $\\breve{G}$ from (1.1). Indeed, informally, the three terms in (A.1) each have a different homogeneity in the variable $a$ . Rather, the confining term in $\\sigma$ should be viewed as corresponding to another regularization term added to (1.3), besides the entropy one in $\\beta^{-1}$ . ", "page_idx": 14}, {"type": "text", "text": "In short, while our work considers MFLD i.e. Wasserstein gradient flow for $F+\\beta^{-1}H$ as the main \u201calgorithmic primitive\u201d, the work of [TS24] considers a MFL $^{,+}$ confining dynamics, i.e. Wasserstein gradient flow for $F+\\beta^{-1}H\\left(\\cdot|\\beta\\sigma^{2}\\bar{\\gamma}\\right)$ . ", "page_idx": 14}, {"type": "text", "text": "Summary of differences. On a technical level, the learning dynamics considered by [TS24] corresponds to a special case of a variant of the MFLD-bilevel we consider from Sec. 3.2 onwards. Namely, they focus on instances of the problem (1.1) where $G$ has a particular form, corresponding to learning with 2NN; and they consider $\\mathcal{W}=\\mathbb{R}^{d}$ and use an additional confining term \u03c312 w in the MFLD dynamics, while we consider settings where $\\mathcal{W}$ is a compact Riemannian manifold, and no additional confining term is needed. ", "page_idx": 14}, {"type": "text", "text": "We also emphasize that, while our work and that of [TS24] cover some similar settings, our focus is quite different. In that work, the key object of interest is the kernel that is learned by MFLD in a 2NN setting $\\begin{array}{r}{((x,x^{\\prime})\\mapsto\\int\\varphi(x^{\\top}w)\\varphi(x^{\\top}w^{\\prime})\\mathrm{{d}}\\eta(w)}\\end{array}$ in the notation of our second bullet point above). By contrast, our main motivation is a general optimization question: how to use MFLD as an algorithmic primitive for problems of the form (1.1). In particular we do not assume a particular form for $G$ except in Sec. 5, and we pay special attention to the bounds on the local LSI constants of $J_{\\lambda}$ along the MFLD trajectory, instead of using the global uniform LSI bound (compare Prop. 3.4 and [TS24, Lemma 3.5]). ", "page_idx": 14}, {"type": "text", "text": "B Details for Sec. 2 (background about MFLD) ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 The displacement smoothness property ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "For MFLD (Eq. (2.1)) to be well-posed, we require that $F$ is $L$ -smooth along Wasserstein geodesics for some $L<+\\infty$ . More precisely, for any constant-speed Wasserstein geodesic $(\\mu_{t})_{t\\in[0,1]}\\subset\\mathscr{P}_{2}(\\Omega)$ with $W_{2}(\\mu_{0},\\mu_{1})=1$ , $t\\mapsto F(\\mu_{t})$ should be $L$ -smooth in the usual sense of continuous optimization. This property ensures that the PDE defining MFLD has a unique solution [Chi22b, App. A], and is also helpful to ensure convergence of explicit time-discretization schemes [SWN23]. The following proposition gives a practical sufficient condition. ", "page_idx": 14}, {"type": "text", "text": "Proposition B.1. Suppose $F:\\mathcal{P}_{2}((\\Omega,g))\\to\\mathbb{R}$ is twice differentiable in the Wasserstein sense. Let $0\\le L<\\infty$ . Suppose that $F$ satisfies (P1), i.e., ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\forall\\mu\\in\\mathscr{P}_{2}(\\Omega),\\;\\forall\\omega\\in\\Omega,\\;\\underset{s\\in T_{\\omega}\\Omega}{\\operatorname*{max}}\\;\\;\\left|\\nabla^{2}\\,F^{\\prime}[\\mu](s,s)\\right|\\leq L}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\|s\\|_{\\omega}\\leq1}\\\\ {a n d}&{\\quad\\forall\\mu,\\mu^{\\prime}\\in\\mathscr{P}_{2}(\\Omega),\\;\\forall\\omega\\in\\Omega,\\;\\;\\|\\nabla F^{\\prime}[\\mu]-\\nabla F^{\\prime}[\\mu^{\\prime}]\\|_{\\omega}\\leq L\\;W_{2}(\\mu,\\mu^{\\prime})}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\nabla^{2}$ denotes the Riemannian Hessian. Then $F$ is $2L$ -smooth along Wasserstein geodesics. ", "page_idx": 14}, {"type": "text", "text": "The first condition can be stated as $F^{\\prime}[\\mu]:\\Omega\\to\\mathbb{R}$ having Lipschitz-continuous gradients in the Riemannian sense [Bou23, Coroll. 10.47], whereas the second condition can be interpreted as a displacement Lipschitz-continuity of $\\mu\\mapsto F^{\\prime}[\\mu](\\omega)$ for each $\\omega$ uniformly. ", "page_idx": 14}, {"type": "text", "text": "Proof. Let a constant-speed Wasserstein geodesic $(\\mu_{t})_{t\\in[0,1]}\\subset\\mathscr{P}_{2}(\\Omega)$ with $W_{2}(\\mu_{0},\\mu_{1})=1$ , and pose $f(t)~=~F(\\mu_{t})$ . We want to show that $f$ is $2L$ -smooth in the usual sense of continuous optimization, for which it suffices to show that $\\forall t$ , $|f^{\\prime\\prime}(t)|\\leq2L$ . ", "page_idx": 14}, {"type": "text", "text": "By [Vil09, Eq. (13.6)] there exist functions $\\phi_{t}:\\Omega\\to\\mathbb{R}$ such that $\\begin{array}{r}{\\left\\{\\partial_{t}\\mu_{t}=-\\mathrm{div}\\big(\\nabla\\phi_{t}\\mu_{t}\\big)\\right.}\\\\ {\\left.\\partial_{t}\\phi_{t}=-\\frac{1}{2}\\left\\lVert\\nabla\\phi_{t}\\right\\rVert^{2}\\,}\\end{array}$ and $\\begin{array}{r}{\\int\\mathrm{d}\\mu_{t}\\,\\,\\left\\|\\nabla\\phi_{t}\\right\\|^{2}=W_{2}^{2}\\big(\\mu_{0},\\mu_{1}\\big)=1}\\end{array}$ for all $t$ . So we can compute explicitly: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{f^{\\prime}(t)=\\displaystyle\\frac{d}{d t}F(\\mu_{t})=\\int\\mathrm{d}\\mu_{t}\\,\\,\\langle\\nabla F^{\\prime}[\\mu_{t}],\\nabla\\phi_{t}\\rangle}}\\\\ {{f^{\\prime\\prime}(t)=\\displaystyle\\int\\mathrm{d}(\\partial_{t}\\mu_{t})\\,\\,\\langle\\nabla F^{\\prime}[\\mu_{t}],\\nabla\\phi_{t}\\rangle+\\int\\mathrm{d}\\mu_{t}\\,\\,\\frac{d}{d t}\\left\\langle\\nabla F^{\\prime}[\\mu_{t}],\\frac{d}{d t}\\nabla\\phi_{t}\\right\\rangle}}\\\\ {{\\displaystyle=\\int\\mathrm{d}\\mu_{t}\\,\\,\\left\\langle\\nabla\\left[\\left\\langle\\nabla F^{\\prime}[\\mu_{t}],\\nabla\\phi_{t}\\right\\rangle\\right],\\nabla\\phi_{t}\\right\\rangle+\\int\\mathrm{d}\\mu_{t}\\,\\,\\left(\\left\\langle\\nabla F^{\\prime}[\\mu_{t}],\\frac{d}{d t}\\nabla\\phi_{t}\\right\\rangle+\\left\\langle\\frac{d}{d t}\\nabla F^{\\prime}[\\mu_{t}],\\nabla\\phi_{t}\\right\\rangle\\right)}}\\\\ {{\\displaystyle=\\int\\mathrm{d}\\mu_{t}\\,\\,\\nabla^{2}\\,F^{\\prime}[\\mu_{t}](\\nabla\\phi_{t},\\nabla\\phi_{t})}}\\\\ {{\\displaystyle\\ ~+\\int\\mathrm{d}\\mu_{t}\\,\\,\\nabla^{2}\\phi_{t}\\left(\\nabla F^{\\prime}[\\mu_{t}],\\nabla\\phi_{t}\\right)+\\int\\mathrm{d}\\mu_{t}\\,\\,\\langle\\nabla F^{\\prime}[\\mu_{t}],\\nabla\\partial_{t}\\phi_{t}\\rangle}}\\\\ {{\\displaystyle\\ ~+\\int\\mathrm{d}\\mu_{t}\\,\\,\\left\\langle\\frac{d}{d t}\\nabla F^{\\prime}[\\mu_{t}],\\nabla\\phi_{t}\\right\\rangle.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Now the first line can be bounded using the first condition of (P1): writing $\\begin{array}{r}{s_{t}(\\omega)=\\frac{\\nabla\\phi_{t}(\\omega)}{\\|\\nabla\\phi_{t}(\\omega)\\|}}\\end{array}$ for all $t$ and $\\omega$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left|\\int\\mathrm{d}\\mu_{t}\\ \\nabla^{2}\\,F^{\\prime}[\\mu_{t}](\\nabla\\phi_{t},\\nabla\\phi_{t})\\right|=\\left|\\int\\mathrm{d}\\mu_{t}\\ \\|\\nabla\\phi_{t}\\|^{2}\\,\\nabla^{2}\\,F^{\\prime}[\\mu_{t}](s_{t},s_{t})\\right|\\le L\\cdot\\int\\mathrm{d}\\mu_{t}\\ \\|\\nabla\\phi_{t}\\|^{2}=L.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Moreover, one can show by direct computation that the second line is zero, using that $\\partial_{t}\\phi_{t}\\;=\\;$ $\\displaystyle-\\,\\frac{1}{2}\\,\\left\\|\\nabla\\phi_{t}\\right\\|^{2}$ . For the third line, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left|\\int\\mathrm{d}\\mu_{t}\\,\\left\\langle\\frac{d}{d t}\\nabla F^{\\prime}[\\mu_{t}],\\nabla\\phi_{t}\\right\\rangle\\right|\\leq\\int\\mathrm{d}\\mu_{t}\\,\\left\\Vert\\nabla\\phi_{t}\\right\\Vert\\cdot\\operatorname*{sup}_{t\\in[0,1]}\\operatorname*{sup}_{\\omega\\in\\Omega}\\left\\Vert\\frac{d}{d t}\\nabla F^{\\prime}[\\mu_{t}](\\omega)\\right\\Vert\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "since $\\begin{array}{r}{\\left(\\int\\mathrm{d}\\mu_{t}\\ \\|\\nabla\\phi_{t}\\|\\right)^{2}\\leq\\int\\mathrm{d}\\mu_{t}\\ \\left\\|\\nabla\\phi_{t}\\right\\|^{2}=1}\\end{array}$ . Finally, let us show that the second condition of (P1) implies a bound on the last quantity: for all $\\omega\\in{\\Omega}$ , by applying the assumption to $\\mu=\\mu_{t}$ and $\\mu^{\\prime}=\\mu_{s}$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{\\|\\nabla F^{\\prime}[\\mu_{s}](\\omega)-\\nabla F^{\\prime}[\\mu_{t}](\\omega)\\|_{\\omega}}{s-t}\\leq\\frac{L\\,W_{2}(\\mu_{s},\\mu_{t})}{s-t}=L\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "since $(\\mu_{t})_{t}$ is a constant-speed geodesic with $W_{2}(\\mu_{0},\\mu_{1})=1$ . So by letting $s\\rightarrow t$ we obtain that $\\begin{array}{r}{\\|\\frac{d}{d t}\\nabla F^{\\prime}[\\mu_{t}](\\omega)\\|\\leq L}\\end{array}$ for all $t\\in[0,1]$ , $\\omega\\in\\Omega$ . Thus we have shown $|f^{\\prime\\prime}(t)|\\leq2L$ , and so $F$ is $2L$ -smooth along Wasserstein geodesics. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "B.2 Classical sufficient conditions for LSI ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "For ease of reference we reproduce here a classical sufficient condition for a probability measure $\\mu\\in\\mathscr{P}(\\Omega)$ to satisfy LSI. ", "page_idx": 15}, {"type": "text", "text": "Lemma B.2 (Holley-Stroock bounded perturbation argument [HS86]). Let $\\mu,\\mu_{0}\\,\\in\\,\\mathscr{P}(\\Omega)$ such that $\\mu$ is absolutely continuous w.r.t. $\\mu_{0}$ . Suppose that $\\mu_{0}$ satisfies LSI with constant $\\alpha$ and that $\\begin{array}{r}{-M\\leq\\log\\frac{\\mathrm{d}\\mu}{\\mathrm{d}\\mu_{0}}(\\omega)+c\\leq M}\\end{array}$ for all $\\omega\\in\\mathrm{supp}(\\mu_{0})$ , for some $c\\in\\mathbb R$ and $M\\geq0$ . Then $\\mu$ satisfies LSI with constant $\\alpha e^{-M}$ . ", "page_idx": 15}, {"type": "text", "text": "C Details for Sec. 3.1 (reduction by lifting) ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "C.1 Proof of Prop. 3.1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Here we present a slightly stronger version of Prop. 3.1 that uses the $p$ -homogeneous projection operator for arbitrary $p>0$ , in preparation for the next subsection, where we show that one can restrict attention to the case $p=1$ as done in the main text. ", "page_idx": 15}, {"type": "text", "text": "Recall that we let $\\Omega=\\mathbb{R}\\times\\mathcal{W}$ . For any $p>0$ , we denote by $h^{p}:\\mathcal{P}(\\Omega)\\to\\mathcal{M}(\\mathcal{W})$ the signed $p$ -homogeneous projection operator [LMS18] defined by ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\forall\\varphi\\in\\mathcal{C}(\\mathcal{W},\\mathbb{R}),\\;\\int_{\\mathcal{W}}\\varphi(w)(h^{p}\\mu)(\\mathrm{d}w)=\\int_{\\Omega}\\mathrm{sign}(r)\\,|r|^{p}\\,\\varphi(w)\\mu(\\mathrm{d}r,\\mathrm{d}w).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "More concretely, for atomic measures, $\\begin{array}{r}{\\pmb{h}^{p}\\left(\\frac{1}{m}\\sum_{j=1}^{m}\\delta_{(r_{j},w_{j})}\\right)=\\frac{1}{m}\\sum_{j=1}^{m}\\mathrm{sign}(r_{j})\\,|r_{j}|^{p}\\,\\delta_{w_{j}}.}\\end{array}$ ", "page_idx": 16}, {"type": "text", "text": "Lemma C.1. For $b\\in[1,2]$ and $p\\,>\\,0$ , let $\\Psi_{b,p}:\\mathcal{P}(\\Omega)\\,\\rightarrow\\,\\mathbb{R}\\cup\\{+\\infty\\}$ defined by $\\Psi_{b,p}(\\mu):=$ $\\begin{array}{r}{\\left(\\int_{\\Omega}|r|^{p b}\\,\\mathrm{d}\\mu(r,w)\\right)^{2/b}i f\\mu\\in\\mathcal{P}_{p b}(\\Omega).}\\end{array}$ , and $+\\infty$ otherwise. Then ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mu\\;s.t.\\;h^{p}\\mu=\\nu}\\Psi_{b,p}(\\mu)=\\|\\nu\\|_{T V}^{2}\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Moreover, if $b=1$ then the set of minimizers is ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\{\\mu\\in\\mathcal{P}(\\mathcal{W});\\ h^{p}\\mu=\\nu\\,a n d\\,\\forall w,\\mathrm{supp}(\\mu(\\cdot|w))\\subset\\mathbb{R}_{+}\\,o r\\,\\operatorname{supp}(\\mu(\\cdot|w))\\subset\\mathbb{R}_{-}\\}\\,,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "$b>1$ there is a unique minimizer which is $\\begin{array}{r}{\\delta_{f(w)}(\\mathrm{d}r)\\frac{|\\nu|(\\mathrm{d}w)}{\\|\\nu\\|_{T V}}}\\end{array}$ where $\\begin{array}{r}{f(w)=\\|\\nu\\|_{T V}^{1/p}\\,\\frac{\\mathrm{d}\\nu}{\\mathrm{d}|\\nu|}(w).}\\end{array}$ ", "page_idx": 16}, {"type": "text", "text": "Proof. For any $\\mu\\in\\mathscr{P}(\\Omega)$ such that $h^{p}=\\nu$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\|h^{p}\\mu\\|_{T V}=\\operatorname*{max}_{\\phi:W\\to[-1,1]}\\int_{\\Omega}\\mathrm{sign}(r)\\,|r|^{p}\\,\\phi(w)\\mathrm{d}\\mu(r,w)\\le\\int_{\\Omega}|r|^{p}\\,\\mathrm{d}\\mu(r,w)}}\\\\ &{}&{\\quad\\quad\\nu\\|_{T V}^{2}=\\|h^{p}\\mu\\|_{T V}^{2}\\le\\bigg(\\bigg(\\int_{\\Omega}|r|^{p}\\,\\mathrm{d}\\mu(r,w)\\bigg)^{b}\\bigg)^{2/b}\\le\\bigg(\\int_{\\Omega}|r|^{p b}\\,\\mathrm{d}\\mu(r,w)\\bigg)^{2/b}=\\Psi_{b,p}(\\mu),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the first inequality follows from the triangle inequality, and the second inequality follows from Jensen\u2019s inequality since $t\\mapsto t^{b}$ is convex on $\\mathbb{R}_{+}$ . Note that the first inequality above holds with equality if and only if there exists $\\phi\\,:\\,\\mathcal{W}\\,\\rightarrow\\,[-1,1]$ such that $\\mathrm{sign}(r)\\phi(w)\\;\\geq\\;0$ for all $(r,w)\\,\\bar{\\in}\\,\\mathrm{supp}(\\mu)$ , i.e., if the conditional distribution $\\mu(\\mathrm{d}\\boldsymbol{r}|\\boldsymbol{w})$ is either supported on $\\mathbb{R}_{+}$ or supported $\\mathbb{R}_{-}$ $w$ $\\|\\nu\\|_{T V}^{2}$ $\\begin{array}{r}{\\mu(\\mathrm{d}r,\\mathrm{d}w)=\\delta_{f(w)}(\\mathrm{d}r)\\frac{|\\nu|(\\mathrm{d}w)}{\\|\\nu\\|_{T V}}}\\end{array}$ where $\\begin{array}{r}{f(w)=\\|\\nu\\|_{T V}^{1/p}\\,\\frac{\\mathrm{d}\\nu}{\\mathrm{d}|\\nu|}(w)}\\end{array}$ . This proves that $\\begin{array}{r}{\\operatorname*{min}_{\\mu:h^{p}\\mu=\\nu}\\Psi_{b,p}(\\mu)=\\|\\nu\\|_{T V}^{2}.}\\end{array}$ . ", "page_idx": 16}, {"type": "text", "text": "For $b=1$ , $t\\,\\mapsto\\,t^{b}\\,=\\,t$ is linear, so equality always holds in Jensen\u2019s inequality. So the set of minimizers is all of $\\{\\mu\\in\\mathcal{P}(\\mathcal{W})$ ; $h^{p}\\mu=\\nu$ and $\\forall w,\\operatorname{supp}(\\mu(\\cdot|w))\\subset\\mathbb{R}_{+}$ or $\\bar{\\mathrm{supp}}\\bar{(}\\mu(\\cdot|w))\\subset\\mathbb{R}_{-}\\}$ . ", "page_idx": 16}, {"type": "text", "text": "For $b~>~1$ , $t\\,\\mapsto\\,t^{b}$ is strictly convex, the second inequality above holds with equality if and only if there exists a constant $c$ such that $|r|^{p}\\,=\\,c$ for all $(\\dot{r},w)\\,\\in\\,\\mathrm{supp}(\\mu)$ . So for $\\mu$ to be a minimizer, the conditional distribution $\\mu(\\mathrm{d}\\boldsymbol{r}|\\boldsymbol{w})$ must be concentrated on $\\{c^{1/p},-c^{1/p}\\}$ for each $w$ . Moreover, for the first inequality above to hold, the conditional distribution at each $w$ must be either supported on $\\mathbb{R}_{+}$ or suported on $\\mathbb{R}_{-}$ , so there exists a function $f:\\mathcal{W}\\to\\{c^{1/p},-c^{1/p}\\}$ such that $\\mu(\\mathrm{d}r,\\mathrm{d}w)\\,=\\,\\delta_{f(w)}(\\mathrm{d}r)\\mu^{w}(\\mathrm{d}w)$ where $\\mu^{w}\\,\\in\\,\\mathcal{P}(\\mathcal{W})$ denotes the marginal distribution. Since $h^{p}\\mu=\\nu$ , then for all fixed $w$ , $\\begin{array}{r}{\\int_{\\mathbb{R}_{-}}\\mathrm{sign}(r)\\,|r|^{p}\\,\\mu(\\mathrm{d}r,\\mathrm{d}w)=\\mathrm{sign}(f(w))c\\mu^{w}(\\mathrm{d}w)=\\nu(\\mathrm{d}w)}\\end{array}$ . So $\\begin{array}{r}{\\mathrm{sign}(f(w))\\,=\\,\\mathrm{sign}(\\frac{\\mathrm{d}\\nu}{\\mathrm{d}\\mu^{w}}(w))\\,=\\,\\frac{\\mathrm{d}\\nu}{\\mathrm{d}|\\nu|}(w)}\\end{array}$ and $\\begin{array}{r}{\\mu^{w}(\\mathrm{d}w)\\,=\\,\\frac{1}{c}\\,|\\nu|\\,(\\mathrm{d}w)}\\end{array}$ since $\\mu^{w}$ is a probability measure so non-negative, and integrating on both sides over $\\Omega$ shows that $c=\\Vert\\nu\\Vert_{T V}$ . Hence the only minimizer is \u00b5(dr, dw) = \u03b4f(w)(dr) |\u2225\u03bd\u03bd|(\u2225dT wV) where $\\begin{array}{r}{f(w)=c^{1/p}\\frac{\\mathrm{d}\\nu}{\\mathrm{d}|\\nu|}(w)}\\end{array}$ . \u53e3 ", "page_idx": 16}, {"type": "text", "text": "Prop. 3.1 follows directly as a special case of the following proposition with $p=1$ ", "page_idx": 16}, {"type": "text", "text": "Proposition C.2. Let any $p>0$ and $b\\in[1,2]$ and let $\\Psi_{b,p}:\\mathcal{P}(\\Omega)\\to\\mathbb{R}\\cup\\{+\\infty\\}$ as in the lemma above. Consider the optimization problem over probability measures, with $\\lambda>0$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mu\\in\\mathcal{P}(\\Omega)}F_{\\lambda,b,p}(\\mu)\\quad w h e r e\\quad F_{\\lambda,b,p}(\\mu)=G(h^{p}\\mu)+\\frac{\\lambda}{2}\\Psi_{b,p}(\\mu).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then $\\mathrm{min}_{\\mathscr{P}(\\Omega)}\\,F_{\\lambda,b,p}=\\mathrm{min}_{\\mathscr{M}(\\mathscr{W})}\\,G_{\\lambda}.$ . ", "page_idx": 16}, {"type": "text", "text": "Moreover, if $b>1$ then $\\begin{array}{r}{\\arg\\operatorname*{min}F=\\left\\{\\delta_{\\lVert\\nu\\rVert_{T V}^{1/p}\\frac{\\mathrm{d}\\nu}{\\mathrm{d}|\\nu|}(w)}(\\mathrm{d}r)\\frac{\\nu(\\mathrm{d}w)}{\\lVert\\nu\\rVert_{T V}};\\nu\\in\\arg\\operatorname*{min}G\\right\\}}\\end{array}$ ) \u03bd\u03bd(dw) ; \u03bd \u2208arg min G , and otherwise a $\\operatorname{rg\\,min}F=\\{\\mu;\\,h^{p}\\mu\\in\\arg\\operatorname*{min}G$ and $\\forall w$ , $\\operatorname{supp}(\\mu)\\subset\\mathbb{R}_{+}\\,o r\\,\\operatorname{supp}(\\mu)\\subset\\mathbb{R}_{+}\\}$ . Furthermore, $F$ is convex. ", "page_idx": 16}, {"type": "text", "text": "Proof. The fact that $\\mathrm{min}_{\\mathcal{P}(\\Omega)}\\,F_{\\lambda,b,p}=\\mathrm{min}_{\\mathcal{M}(\\mathcal{W})}\\,G_{\\lambda}$ can be seen directly as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{\\mu\\in\\mathcal{P}(\\Omega)}{\\mathrm{min}}\\,F(\\mu)=\\underset{\\mu\\in\\mathcal{P}(\\Omega)}{\\mathrm{min}}\\,G(h^{p}\\mu)+\\frac{\\lambda}{2}\\Psi_{b,p}(\\mu)}&{}\\\\ {=\\underset{\\nu\\in\\mathcal{M}(\\Omega)}{\\mathrm{min}}\\,\\left[\\underset{\\mu\\in\\mathcal{P}(\\Omega):h^{p}=\\nu}{\\mathrm{min}}\\,G(h^{p}\\mu)+\\frac{\\lambda}{2}\\Psi_{b,p}(\\mu)\\right]}&{}\\\\ {=\\underset{\\nu\\in\\mathcal{M}(\\Omega)}{\\mathrm{min}}\\,G(\\nu)+\\frac{\\lambda}{2}\\left[\\underset{\\mu\\mathcal{P}(\\Omega):h^{p}=\\nu}{\\mathrm{min}}\\,\\Psi_{b,p}(\\mu)\\right]}&{}\\\\ {=\\underset{\\nu\\in\\mathcal{M}(\\Omega)}{\\mathrm{min}}\\,G(\\nu)+\\frac{\\lambda}{2}\\left\\lVert\\nu\\right\\rVert_{\\mathcal{T}V}^{2}}&{=\\underset{\\nu\\in\\mathcal{M}(\\Omega)}{\\mathrm{min}}\\,G_{\\lambda}(\\nu)}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where we used the lemma above at the fourth equality. The characterization of $\\operatorname{arg}\\operatorname*{min}F$ in terms of arg min $G$ follows from the characterization of the minimizers of the inner minimization $\\left[\\operatorname*{min}_{\\mu\\in\\mathcal{P}(\\Omega):h^{p}=\\nu}\\Psi_{b}(\\mu)\\right]$ in the third line, which is given by the lemma above. ", "page_idx": 17}, {"type": "text", "text": "Furthermore, $F_{\\lambda,b,p}$ is convex since $G$ and $\\Psi_{b,p}$ are. ", "page_idx": 17}, {"type": "text", "text": "C.2 Equivalence of using $(c p,c q_{r},c q_{w},\\Gamma/c^{2})$ for any $c>0$ by reparametrizing ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Equivalence of Riemannian structures on $\\Omega^{*}$ for $(c q_{r},c q_{w},\\Gamma/c^{2})$ for $c>0$ . Recall that we consider equipping $\\Omega^{*}=\\mathbb{R}^{*}\\times\\mathcal{W}$ with a Riemannian metric of the form (3.2), reproduced here for ease of reference: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left.\\!\\left({\\frac{\\delta r_{1}}{\\delta w_{1}}}\\right),\\left({\\frac{\\delta r_{2}}{\\delta w_{2}}}\\right)\\right\\rangle_{(r,w)}=\\Gamma^{-1}\\left|r\\right|^{q_{r}}{\\frac{\\delta r_{1}\\delta r_{2}}{r^{2}}}+|r\\right|^{q_{w}}\\left\\langle\\delta w_{1},\\delta w_{2}\\right\\rangle_{w}\\,,{\\mathrm{~i.e.,~}}\\,g_{(r,w)}=\\left[{\\begin{array}{c c}{\\Gamma^{-1}\\left|r\\right|^{q_{r}-2}}&{|r|^{q_{w}}\\delta r_{1}}\\\\ {0}&{|r|^{q_{r}-2}}\\end{array}}\\right]\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The following proposition shows that, in fact, different choices of $q_{r},q_{w}$ and $\\Gamma$ lead to the same geometry, up to a reparametrization of the form $(a,w)=(r^{\\alpha},w)$ (for $r>0$ ). Namely it is equivalent to use the metric with exponents $(q_{r},q_{w})$ or with $\\textstyle\\left({\\frac{q_{r}}{\\alpha}},{\\frac{q_{w}}{\\alpha}}\\right)$ , up to adjusting $\\Gamma$ . ", "page_idx": 17}, {"type": "text", "text": "Proposition C.3. For any $q_{r},q_{w}$ , denote by $g_{[q_{r},q_{w},\\Gamma]}$ the metric $g_{(r,w)}=\\left[\\!\\!\\!\\begin{array}{c c}{{\\Gamma^{-1}\\,|r|^{q_{r}-2}}}&{{0}}\\\\ {{0}}&{{|r|^{q_{w}}\\,g_{w}}}\\end{array}\\!\\!\\right]$ on $\\Omega^{*}\\,=\\,\\mathbb{R}^{*}\\,\\times\\,\\mathcal{W}$ . Then for any $q_{r},q_{w}\\,\\in\\,\\mathbb{R}$ and $\\Gamma,\\alpha\\,>\\,0$ , the map $\\bar{T}_{\\alpha}:\\left(\\Omega^{*},g_{[q_{r},q_{w},\\Gamma]}\\right)\\ \\stackrel{}{\\rightarrow}\\ \\ \\$ $\\left(\\Omega^{*},g_{\\left[{\\frac{q_{r}}{\\alpha}},{\\frac{q_{w}}{\\alpha}},\\alpha^{2}\\Gamma\\right]}\\right)$ defined by $T_{\\alpha}(r,w)=(\\mathrm{sign}(r)\\left|r\\right|^{\\alpha},w)$ is an isometry. ", "page_idx": 17}, {"type": "text", "text": "Proof. Since $\\Omega^{*}$ is a disjoint manifold: $\\Omega^{*}=\\mathbb{R}_{+}^{*}\\!\\times\\!\\mathcal{W}\\cup\\mathbb{R}_{-}^{*}\\!\\times\\!\\mathcal{W}$ , and since $T_{\\alpha}(\\mathbb{R}_{+}^{*}\\times\\mathcal{W})=\\mathbb{R}_{+}^{*}\\times\\mathcal{W}_{*}$ , it suffices to check that the restricted map $T_{\\alpha}^{+}:\\left(\\mathbb{R}_{+}^{*}\\times\\mathcal{W},g_{[q_{r},q_{w},\\Gamma]}\\right)\\rightarrow\\left(\\mathbb{R}_{+}^{*}\\times\\mathcal{W},g_{\\left[\\frac{q_{r}}{\\alpha},\\frac{q_{w}}{\\alpha},\\alpha^{2}\\Gamma\\right]}\\right)$ is an isometry (as well as the analogous statement for the restricted map $T_{\\alpha}^{-}$ , but it will follow analogously). ", "page_idx": 17}, {"type": "text", "text": "Indeed, denote by $\\tilde{g}$ the metric on $\\mathbb{R}_{+}^{*}\\times\\mathcal{W}$ induced by $T_{\\alpha}^{+}$ . It is given by, for $(a,w)=T_{\\alpha}^{+}(r,w)=$ $(r^{\\alpha},w)$ , so $\\begin{array}{r}{\\frac{d a}{a}=\\alpha\\frac{d r}{r}}\\end{array}$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\delta r_{1}\\right)\\cdot g_{(r,w)}\\left(\\delta r_{2}\\right)=\\left(\\delta a_{1}\\right)\\cdot\\tilde{g}_{(a,w)}\\left(\\delta a_{2}\\right)=\\left(\\stackrel{\\alpha a_{1}^{-}}{\\delta w_{1}}\\delta r_{1}\\right)\\cdot\\tilde{g}_{(a,w)}\\left(\\stackrel{\\alpha a_{1}^{-}}{\\delta w_{2}}\\delta r_{2}\\right)}\\\\ &{\\mathrm{so}\\ \\tilde{g}_{(a,w)}=\\left[\\stackrel{r}{\\alpha}_{0}^{c}\\ 0\\right]g_{(r,w)}\\left[\\stackrel{r}{\\alpha}_{a}^{r}\\ 0\\right]}\\\\ &{\\qquad\\qquad=\\left[\\stackrel{r^{2}}{\\alpha^{2}a^{2}}\\Gamma^{-1}r^{q_{r}-2}\\quad\\stackrel{0}{r^{q_{w}}}g_{w}\\right]=\\left[\\stackrel{\\Gamma^{-1}\\alpha^{-2}a^{q_{r}/\\alpha-2}}{0}\\right.\\quad\\stackrel{0}{a^{q_{w}/\\alpha}g_{w}}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "So $\\tilde{g}$ is precisely $\\textstyle{\\mathcal{G}}[{\\frac{q r}{\\alpha}},{\\frac{q w}{\\alpha}},\\alpha^{2}\\Gamma]$ on $\\mathbb{R}_{+}^{*}\\times\\mathcal{W}$ , which proves the claim. ", "page_idx": 17}, {"type": "text", "text": "Equivalence of the Wasserstein gradient flow of $F_{\\lambda,b,p}$ for $(c p,c q_{r},c q_{w},\\Gamma/c^{2})$ for any $c>0$ . Proposition C.4. Let $T:(\\Omega_{1},g_{[1]})\\rightarrow(\\Omega_{2},g_{[2]})$ an isometry between Riemannian manifolds. Let $F:\\mathscr{P}(\\Omega_{1})\\to\\mathbb{R}$ (sufficiently regular) and $(\\mu_{t})_{t}$ a Wasserstein gradient flow for $F$ , i.e., $\\partial_{t}\\mu_{t}=$ $-\\mathrm{div}(\\dot{\\mu}_{t}\\nabla F^{\\prime}[\\mu_{t}])$ (where $\\nabla$ denotes Riemannian gradient in $\\left(\\Omega_{1},g_{[1]}\\right);$ . Then, $(\\tilde{\\mu})_{t}:=(T_{\\sharp}\\mu_{t})_{t}$ is $a$ Wasserstein gradient flow for $\\widetilde{F}:\\mathcal{P}(\\Omega_{2})\\rightarrow\\mathbb{R}$ defined by ${\\widetilde{\\cal F}}({\\tilde{\\mu}})={\\cal F}(T_{\\sharp}^{-1}{\\tilde{\\mu}})$ . ", "page_idx": 17}, {"type": "text", "text": "Proof. First note that $g_{[2]}$ is given by, for all $y=T(x)\\in\\Omega_{2}$ , so $d y=D T(x)d x$ where $D$ denotes the differential, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\delta y^{\\top}\\ g_{[2]y}\\ \\delta y^{\\prime}=\\delta x^{\\top}\\ g_{[1]x}\\ \\delta x^{\\prime}=\\delta y^{\\top}((D T(x))^{-1})^{\\top}\\ g_{[1]x}\\ (D T(x))^{-1}\\delta y^{\\prime}}\\\\ &{\\qquad\\qquad\\mathrm{so}\\quad g_{[1]x}^{-1}=(D T(x))^{-1}\\ g_{[2]T(x)}^{-1}\\ ((D T(x))^{-1})^{\\top}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Also note that $\\widetilde{\\cal F}^{\\prime}[\\tilde{\\mu}](y)\\;\\;=\\;\\;{\\cal F}^{\\prime}[T_{\\sharp}^{-1}\\tilde{\\mu}](T^{-1}(y))$ , as one can check directly by computing $\\begin{array}{r l r}{\\operatorname*{lim}_{\\varepsilon\\to0}\\frac{1}{\\varepsilon}\\left[\\widetilde{F}(\\tilde{\\mu}+\\varepsilon\\tilde{\\nu})-\\widetilde{F}(\\tilde{\\mu})\\right]}&{=}&{\\operatorname*{lim}_{\\varepsilon\\to0}\\frac{1}{\\varepsilon}\\left[F(T_{\\sharp}^{-1}\\tilde{\\mu}+\\varepsilon T_{\\sharp}^{-1}\\tilde{\\nu})-F(T_{\\sharp}^{-1}\\tilde{\\mu})\\right]}\\end{array}$ . In particular ${\\cal D}\\widetilde{F}^{\\prime}[\\widetilde{\\mu}](y)={\\cal D}F^{\\prime}[T_{\\sharp}^{-1}\\widetilde{\\mu}](T^{-1}(y))({\\cal D}T(T^{-1}(y)))^{-1}.\\mathrm{~Then~for~and~}$ y $\\varphi:\\Omega_{2}\\to\\mathbb{R}$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\frac{d}{d t}\\int_{\\Omega_{2}}\\varphi\\mathrm{d}\\tilde{\\mu}_{t}=\\frac{d}{d t}\\int_{\\Omega_{1}}\\varphi(T(x))\\mathrm{d}\\mu_{t}(x)}}\\\\ &{\\qquad\\qquad=\\int_{\\Omega_{1}}D\\varphi(T(x))D T(x)\\,g_{[1]}^{-1}\\,D F^{\\prime}[\\mu_{t}](x)\\mathrm{d}\\mu_{t}(x)}\\\\ &{\\qquad\\qquad=\\int_{\\Omega_{1}}D\\varphi(y)\\,g_{[2]}^{-1}\\,D\\tilde{F}^{\\prime}[\\tilde{\\mu}_{t}](y)\\mathrm{d}\\tilde{\\mu}_{t}(y).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "That is, $\\begin{array}{r}{\\partial_{t}\\tilde{\\mu}_{t}=-\\mathrm{div}(\\tilde{\\mu}_{t}g_{[2]}^{-1}D\\widetilde{F}^{\\prime}[\\tilde{\\mu}_{t}])}\\end{array}$ , i.e., $(\\tilde{\\mu}_{t})_{t}$ is a Wasserstein gradient flow for $\\widetilde{F}$ . ", "page_idx": 18}, {"type": "text", "text": "Proposition C.5. Consider the functionals $F_{\\lambda,b,p}$ over ${\\mathcal{P}}(\\Omega)$ from Prop. C.2 and the Riemannian metrics $g_{[q_{r},q_{w},\\Gamma]}$ over $\\Omega^{*}$ from Prop. C.3, where $\\bar{\\Omega}=\\mathbb{R}\\times\\mathcal{W}$ and $\\Omega^{*}=\\mathbb{R}^{*}\\times\\mathcal{W}$ . ", "page_idx": 18}, {"type": "text", "text": "$F i x\\;q_{r},q_{w}\\in\\mathbb{R},\\,\\Gamma,p,\\lambda>0$ and $b\\in[1,2]$ . Let $(\\mu_{t})_{t}$ the Wasserstein gradient flow for $F_{\\lambda,b,p}$ over $\\left(\\Omega^{*},g_{[q_{r},q_{w},\\Gamma]}\\right)$ , starting from some $\\mu_{0}\\in\\mathscr{P}(\\Omega^{*})$ . ", "page_idx": 18}, {"type": "text", "text": "Let $\\alpha>0$ and $T_{\\alpha}:\\Omega^{*}\\to\\Omega^{*}$ defined by $T_{\\alpha}(r,w)=(\\mathrm{sign}(r)\\left|r\\right|^{\\alpha},w)$ . Then $(\\tilde{\\mu}_{t})_{t}:=((T_{\\alpha})_{\\sharp}\\mu_{t})_{t}$ coincides with the Wasserstein gradient flow for $F_{\\tilde{\\lambda},\\tilde{b},\\tilde{p}}$ over $(\\Omega^{*},g_{[\\tilde{q}_{r},\\tilde{q}_{w},\\widetilde{\\Gamma}]})$ starting from $\\tilde{\\mu}_{0}\\,=$ $(T_{\\alpha})_{\\sharp}\\mu_{0}$ , where ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\tilde{p}=\\frac{p}{\\alpha},\\qquad\\tilde{q}_{r}=\\frac{q_{r}}{\\alpha},\\qquad\\tilde{q}_{w}=\\frac{q_{w}}{\\alpha},\\qquad\\tilde{\\Gamma}=\\alpha^{2}\\Gamma,\\qquad\\tilde{\\lambda}=\\lambda,\\qquad\\tilde{b}=b.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. The proposition follows from an application of Prop. C.4 with $T=T_{\\alpha}$ , $\\Omega_{1}=(\\Omega^{*},g_{[q_{r},q_{w},\\Gamma]})$ , $\\Omega_{2}=(\\Omega^{*},g_{[q_{r}^{\\prime},q_{w}^{\\prime},\\Gamma^{\\prime}]})$ and $F=F_{\\lambda,b,p}$ . Indeed the fact that $T_{\\alpha}$ is an isometry from $\\Omega_{1}$ to $\\Omega_{2}$ was shown in Prop.r C.w3. It only remains to show that $F\\circ T_{\\sharp}^{-1}=F_{\\tilde{\\lambda},\\tilde{b},\\tilde{p}}$ . And indeed for any $\\tilde{\\mu}\\in\\mathcal{P}(\\Omega^{*})$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\nF_{\\lambda,b,p}((T_{\\alpha})_{\\sharp}^{-1}{\\tilde{\\mu}})=F_{\\lambda,b,p}((T_{\\alpha^{-1}})_{\\sharp}{\\tilde{\\mu}})=G\\left(h^{p}(T_{\\alpha^{-1}})_{\\sharp}{\\tilde{\\mu}}\\right)+{\\frac{\\lambda}{2}}\\Psi_{b,p}\\left((T_{\\alpha^{-1}})_{\\sharp}{\\tilde{\\mu}}\\right),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and $\\pmb{h}^{p}(T_{\\alpha^{-1}})\\sharp\\tilde{\\mu}=\\pmb{h}^{p/\\alpha}\\tilde{\\mu}$ , since for any $\\varphi:\\mathcal{W}\\to\\mathbb{R}$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\int_{\\mathcal{W}}\\varphi\\mathrm{d}\\left[h^{p}(T_{\\alpha^{-1}})_{\\sharp}\\tilde{\\mu}\\right]=\\int_{\\mathbb{R}}\\int_{\\mathcal{W}}\\varphi(w)\\,\\mathrm{sign}(r)\\,|r|^{p}\\left[(T_{\\alpha^{-1}})_{\\sharp}\\tilde{\\mu}\\right](\\mathrm{d}r,\\mathrm{d}w)}}\\\\ &{=\\int_{\\mathbb{R}}\\int_{\\mathcal{W}}\\varphi(w)\\,\\mathrm{sign}(\\tilde{r})\\,|\\tilde{r}|^{p/\\alpha}\\,\\tilde{\\mu}(\\mathrm{d}\\tilde{r},\\mathrm{d}w)=\\int_{\\mathcal{W}}\\varphi\\mathrm{d}\\left[h^{p/\\alpha}\\tilde{\\mu}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\Psi_{b,p}((T_{\\alpha^{-1}})_{\\sharp}\\tilde{\\mu})=\\left(|r|^{p b}\\,\\mathrm{d}\\left[(T_{\\alpha^{-1}})_{\\sharp}\\tilde{\\mu}\\right]\\right)^{2/b}=\\left(|\\tilde{r}|^{p b/\\alpha}\\,\\mathrm{d}\\tilde{\\mu}(r,w)\\right)^{2/b}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This confirms that $F\\circ T_{\\sharp}^{-1}=F_{\\tilde{\\lambda},\\tilde{b},\\tilde{p}}$ and concludes the proof. ", "page_idx": 18}, {"type": "text", "text": "Thus, it is equivalent to consider the lifting reduction with the hyperparameters $(p,q_{r},q_{w},\\Gamma)$ or with $\\left(c p,c q_{r},c q_{w},\\Gamma/c^{2}\\right)$ for any $c>0$ . ", "page_idx": 18}, {"type": "text", "text": "Remark C.1. The choice $p=q_{r}=q_{w}$ plays a special role, as Wasserstein gradient flows $(\\mu_{t})_{t}$ on $\\mathcal{P}(\\mathbb{R}_{+}^{*}\\times\\mathcal{W})$ for functionals of the form $\\mu\\mapsto G(h^{p}\\mu)$ then correspond to gradient flows $(\\nu_{t})_{t}$ on $\\mathcal{M}_{+}(\\mathcal{W})$ for $G$ in the Wasserstein-Fisher-Rao geometry [Chi22c, Prop. 2.1], via $\\nu_{t}=h^{p}\\mu_{t}$ . This correspondence is lost however for functionals of the form of $F_{\\lambda,b,p}$ as in Prop. C.2 with $\\lambda\\neq0$ . ", "page_idx": 18}, {"type": "text", "text": "Equivalence of MFLD of $F_{\\lambda,b,p}$ for $(c p,c q_{r},c q_{w},\\Gamma/c^{2})$ for any $c>0$ . Since MFLD for $F_{\\lambda,b,p}$ is the Wasserstein gradient flow of $F_{\\lambda,b,p}+\\beta^{-1}H$ , then by Prop. C.4, by proceeding similarly as in the proof of Prop. C.5, it suffices to check that $\\tilde{\\mu}\\mapsto H((T_{\\alpha-1})_{\\sharp}\\tilde{\\mu})$ is equal to $H$ itself, up to an additive constant. And indeed, since $T_{\\alpha^{-1}}$ is invertible, by data processing inequality for differential entropy, we have $H((T_{\\alpha^{-1}})_{\\sharp}\\tilde{\\mu})=H(\\tilde{\\mu})$ for all $\\tilde{\\mu}\\in\\mathcal{P}(\\Omega^{*})$ . ", "page_idx": 19}, {"type": "text", "text": "C.3 Proof of Prop. 3.2 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Lemma C.6. Let $F_{\\lambda,b,p}$ defined in (C.1) and $\\Omega=\\mathcal{W}\\times\\mathbb{R}$ . For any $\\mu\\in\\mathscr{P}(\\Omega)$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\nF_{\\lambda,b,p}^{\\prime}[\\mu](r,w)=\\operatorname{sign}(r)\\left|r\\right|^{p}G^{\\prime}[h^{p}\\mu](w)+\\lambda^{\\prime}\\left|r\\right|^{p b}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\begin{array}{r}{\\lambda^{\\prime}=\\lambda\\frac{1}{b}\\Psi_{b,p}(\\mu)^{1-\\frac{b}{2}}}\\end{array}$ . ", "page_idx": 19}, {"type": "text", "text": "Proof. For any $\\mu^{\\prime}\\in\\mathscr{P}(\\Omega)$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\operatorname*{lim}_{\\varepsilon\\rightarrow0}\\frac{1}{\\varepsilon}\\left[(G\\circ h^{p})(\\mu+\\varepsilon\\mu^{\\prime})-(G\\circ h^{p})(\\mu)\\right]=\\operatorname*{lim}_{\\varepsilon\\rightarrow0}\\frac{1}{\\varepsilon}\\left[G(h^{p}\\mu+\\varepsilon h^{p}\\mu^{\\prime})-G(h^{p}\\mu)\\right]}\\ ~}\\\\ {{\\displaystyle=\\int_{W}G^{\\prime}[h^{p}\\mu](w)\\mathrm{d}\\left[h^{p}\\mu^{\\prime}\\right](w)=\\int_{\\mathbb{R}\\times W}\\mathrm{sign}(r)\\left|r\\right|^{p}G^{\\prime}[h^{p}\\mu](w)\\mathrm{d}\\mu^{\\prime}(r,w)}\\ ~}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and so $(G\\circ h^{p})^{\\prime}\\,[\\mu](r,w)=\\operatorname{sign}(r)\\,|r|^{p}\\,G^{\\prime}[h^{p}\\mu](w).$ . Moreover ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\Psi_{b,p}(\\mu)=\\left(\\int_{\\Omega}|r|^{p b}\\,\\mathrm{d}\\mu(r,w)\\right)^{\\frac{2}{b}}}}\\\\ {{\\displaystyle\\Psi_{b,p}^{\\prime}[\\mu](r,w)=\\frac{2}{b}\\left(\\int_{\\Omega}|r|^{\\prime p b}\\,\\mathrm{d}\\mu(r^{\\prime},w^{\\prime})\\right)^{\\frac{2}{b}-1}\\left|r\\right|^{p b}=\\frac{2}{b}\\Psi_{b,p}(\\mu)^{1-\\frac{b}{2}}\\left|r\\right|^{p b}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Summing the results of these two calculations gives the first variation of $F_{\\lambda,b,p}=G\\circ h^{p}\\!+\\!\\frac{\\lambda}{2}\\Psi_{b,p}$ . ", "page_idx": 19}, {"type": "text", "text": "Lemma C.7. Let $f:\\mathbb{R}_{+}^{*}\\times\\mathcal{W}\\rightarrow\\mathbb{R}$ defined by $f(r,w)\\,=\\,r^{p}\\tilde{\\phi}(w)+\\lambda^{\\prime}r^{p b},$ , for some $p,\\lambda^{\\prime}>0,$ , $b\\in[1,2]$ , and $\\tilde{\\phi}:\\mathcal{W}\\to\\mathbb{R}.$ . Assume that $\\nabla^{2}\\,\\tilde{\\phi}$ is not constant equal to 0. ", "page_idx": 19}, {"type": "text", "text": "Consider $\\mathbb{R}_{+}^{*}\\times\\mathcal{W}$ equipped with the Riemannian metric (3.2). If $f$ has Lipschitz-continuous Riemannian gradients, then necessarily $b=1$ and $p=q_{r}=q_{w}$ , or $b=1$ and $p\\stackrel{\\bar{=}}{=}q_{r}/2=q_{w}/2$ and $\\nabla^{2}\\tilde{\\phi}(w)=\\Gamma p^{2}\\left(\\tilde{\\phi}(w)+\\lambda^{\\prime}\\right)g_{w}$ for all $w$ . ", "page_idx": 19}, {"type": "text", "text": "The proof of Lem. C.7 is technical, so it is deferred to the next section. ", "page_idx": 19}, {"type": "text", "text": "Proof of Prop. 3.2. Let us prove the first item in the proposition. Suppose by contraposition that $F_{\\lambda,b}$ does satisfy (P1). Let any $\\nu\\in\\mathcal{M}(\\mathcal{W})$ such that $\\nabla^{2}G^{\\prime}[\\nu]$ is not constant equal to 0, and consider some $\\mu\\in\\mathscr{P}(\\Omega)$ to be chosen such that $h\\mu=\\nu$ . Then by the first condition of (P1), $f:=\\left.F_{\\lambda,b}^{\\prime}[\\mu]\\right\\vert_{\\mathbb{R}_{+}^{*}\\times\\mathcal{W}}$ the restriction of $F_{\\lambda,b}^{\\prime}[\\mu]$ to $\\mathbb{R}_{+}^{*}\\times\\mathcal{W}$ must have Lipschitz-continuous Riemannian gradients. More explicitly, by (C.2), $f(r,w)\\;=\\;r G^{\\prime}[\\nu](w)+\\lambda_{\\mu}^{\\prime}r^{b}$ where $\\begin{array}{r}{\\lambda_{\\mu}^{\\prime}\\;=\\;\\frac{\\lambda}{b}\\Psi_{b}(\\mu)^{1-\\frac{b}{2}}}\\end{array}$ . So by Lem. C.7, necessarily $b=1$ , and so $\\lambda_{\\mu}^{\\prime}=\\lambda\\Psi_{1}(\\mu)^{1/2}$ . If $\\tilde{\\phi}:=G^{\\prime}[\\nu]$ satisfies $\\nabla^{2}\\tilde{\\phi}(w)=\\Gamma p^{2}\\left(\\tilde{\\phi}(w)+\\lambda_{\\mu}^{\\prime}\\right)g_{w}$ for all $w$ , pick any other $\\mu^{\\prime}$ such that $h\\mu^{\\prime}\\,=\\,\\nu$ and $\\Psi_{1}(\\mu^{\\prime})\\,\\ne\\,\\Psi_{1}(\\mu)$ \u2013 the existence of such a $\\mu^{\\prime}$ follows from the first step in the proof of Lem. C.1. Then by applying the above reasoning to $\\left.\\left[F_{\\lambda,b}^{\\prime}[\\mu^{\\prime}]\\right|_{\\mathbb{R}_{+}^{*}\\times\\mathcal{W}}$ instead of $f$ , since $\\lambda_{\\mu^{\\prime}}^{\\prime}\\neq\\lambda_{\\mu}^{\\prime}$ , we also have by Lem. C.7 that $p=q_{r}=q_{w}$ . This   \nshows that if $F_{\\lambda,b}$ satisfies (P1) then $(q_{r},q_{w},b)\\,=\\,(1,1,1)$ , which was the announced necessary condition. ", "page_idx": 19}, {"type": "text", "text": "We now turn to the second item of the proposition. Suppose that $q_{r}\\,=\\,q_{w}\\,=\\,b\\,=\\,1$ . For any $\\mu\\in\\mathscr{P}_{1}(\\Omega)$ , denote ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\lambda_{0\\mu}=\\operatorname*{sup}_{w\\in\\mathcal{W}}\\frac{|G^{\\prime}[h\\mu](w)|}{\\Psi_{1}(\\mu)^{1/2}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Let us show that if $\\lambda<\\lambda_{0\\mu}$ , then $F_{\\lambda,1}$ does not satisfy local LSI at $\\mu$ . Suppose that $\\lambda<\\lambda_{0\\mu}$ , i.e., there exists $w_{0}\\in\\mathcal{W}$ such that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\Psi_{1}(\\mu)^{1/2}\\lambda<|G^{\\prime}[h\\mu](w_{0})|\\,.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Let us distinguish cases between $G^{\\prime}[h\\mu](w_{0})\\geq0$ or $G^{\\prime}[h\\mu](w_{0})<0.$ ", "page_idx": 20}, {"type": "text", "text": "First suppose $G^{\\prime}[h\\mu](w_{0})\\,\\geq\\,0$ , so that $\\Psi_{1}(\\mu)^{1/2}\\lambda\\,<\\,G^{\\prime}[h\\mu](w_{0})$ . By continuity of $G^{\\prime}[h\\mu]$ , let $N\\subset\\mathcal{W}$ an open neighborhood of $w_{0}$ such that $\\forall w\\in N,\\Psi_{1}(\\mu)^{1/2}\\lambda<G^{\\prime}[h\\mu](w)$ . Then, since $F_{\\lambda,1}^{\\prime}[\\mu](r,w)=|r|\\left(\\mathrm{sign}(r)G^{\\prime}[h\\mu](w)+\\lambda\\Psi_{1}(\\mu)^{1/2}\\right)$ by (C.2), ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\forall r\\in\\mathbb{R}_{-},\\forall w\\in N,\\ F_{\\lambda,1}^{\\prime}[\\mu](r,w)=|r|\\left(-G^{\\prime}[h\\mu](w)+\\lambda\\Psi_{1}(\\mu)^{1/2}\\right)\\leq0}\\\\ {\\mathrm{and~so}}&{\\int_{\\mathbb{R}}\\int_{\\mathcal{W}}e^{-\\beta F_{\\lambda,1}^{\\prime}[\\mu](r,w)}\\mathrm{d}r\\mathrm{d}w\\geq\\int_{\\mathbb{R}_{-}}\\int_{N}e^{-\\beta F_{\\lambda,1}^{\\prime}[\\mu](r,w)}\\mathrm{d}r\\mathrm{d}w}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\geq\\int_{\\mathbb{R}_{-}}\\int_{N}1\\,\\mathrm{d}r\\mathrm{d}w\\ =+\\infty.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "This contradicts the exponential integrability condition in the definition of local LSI, and so $F_{\\lambda,1}$ does not satisfy local LSI at $\\mu$ . ", "page_idx": 20}, {"type": "text", "text": "Likewise, now suppose that $G^{\\prime}[h\\mu](w_{0})<0$ , so that $\\Psi_{1}(\\mu)^{1/2}\\lambda<-G^{\\prime}[h\\mu](w_{0})$ . By continuity of $G^{\\prime}[h\\mu]$ , let $N\\subset\\mathcal{W}$ an open neighborhood of $w_{0}$ such that $\\forall w\\in N,\\Psi_{1}(\\mu)^{1/2}\\lambda<-G^{\\prime}[h\\mu](w)$ . Then ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\forall r\\in\\mathbb{R}_{+},\\forall w\\in N,\\ F_{\\lambda,1}^{\\prime}[\\mu](r,w)=|r|\\left(G^{\\prime}[h\\mu](w)+\\lambda\\Psi_{1}(\\mu)^{1/2}\\right)\\leq0}\\\\ &{\\mathrm{and~so}\\quad\\displaystyle\\int_{\\mathbb{R}}\\int_{\\mathcal{W}}e^{-\\beta F_{\\lambda,1}^{\\prime}[\\mu](r,w)}\\mathrm{d}r\\mathrm{d}w\\geq\\displaystyle\\int_{\\mathbb{R}_{+}}\\int_{N}e^{-\\beta F_{\\lambda,1}^{\\prime}[\\mu](r,w)}\\mathrm{d}r\\mathrm{d}w}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\geq\\displaystyle\\int_{\\mathbb{R}_{+}}\\int_{N}1\\,\\mathrm{d}r\\mathrm{d}w\\ =+\\infty.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "As in the previous case, we conclude that $F_{\\lambda,1}$ does not satisfy local LSI at $\\mu$ . ", "page_idx": 20}, {"type": "text", "text": "C.4 Proof of Lem. C.7 via computing the Hessians under the lifted Riemannian geometry ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We start by a general lemma. We use $D$ to denote differentials, and for a function $f:\\mathbb{R}_{+}^{*}\\times\\mathcal{W}\\xrightarrow{}\\mathbb{R}$ , we will write $\\begin{array}{r}{D_{r}f=\\frac{\\partial f(r,w)}{\\partial r}}\\end{array}$ and $\\begin{array}{r}{D_{w}f=\\frac{\\partial f(r,w)}{\\partial w}}\\end{array}$ . ", "page_idx": 20}, {"type": "text", "text": "Lemma C.8. Let $(\\mathcal{W},g)$ a Riemannian manifold. Let $\\Omega_{+}^{*}=\\mathbb{R}_{+}^{*}\\times\\mathcal{W}$ and consider ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\overline{{g}}_{(r,w)}=\\left[\\!\\!\\begin{array}{c c}{\\alpha(r)^{-1}}&{0}\\\\ {0}&{\\beta(r)^{-1}g_{w}}\\end{array}\\!\\!\\right]\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "for smooth positive functions $\\alpha,\\beta:\\mathbb{R}_{+}^{*}\\rightarrow\\mathbb{R}_{+}^{*}$ . This defines a smooth Riemannian metric $\\overline{{g}}$ on $\\Omega_{+}^{*}$ . ", "page_idx": 20}, {"type": "text", "text": "Denote by $\\overline{{g}}_{(r,w)}$ , $\\overline{\\nabla}$ , \u0393, $\\overline{{\\nabla^{2}}}$ the Riemannian metric, gradient, Christoffel symbols, resp. Hessian on $\\Omega_{+}^{*}$ , and by $g_{w},\\nabla,\\Gamma,\\nabla^{2}$ the corresponding objects on the original space $\\mathcal{W}$ . ", "page_idx": 20}, {"type": "text", "text": "Let $f:\\Omega_{+}^{*}\\to\\mathbb{R}$ a smooth scalar field. Write for convenience $f_{r}(w)=f(r,w)$ , so that for example $\\nabla f_{r}(w)=g_{w}^{-1}D_{w}f(r,w)$ , and note that $D_{r}\\nabla f_{r}(w)=\\nabla D_{r}f_{r}(w)$ . Fix a local coordinate chart on $\\mathcal{W}$ . This induces a local coordinate chart on $\\Omega_{+}^{*}$ by adding the index 0 for the variable $r$ . Then the Riemannian Hessian $f$ at $(r,w)$ is given in coordinates by ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\overline{{{\\nabla^{2}}}}\\,f^{00}=\\alpha(r)^{2}D_{r r}^{2}f+\\displaystyle{\\frac{1}{2}}\\alpha(r)\\alpha^{\\prime}(r)D_{r}f}}\\\\ {{\\overline{{{\\nabla^{2}}}}\\,f^{i0}=\\overline{{{\\nabla^{2}}}}\\,f^{0i}=\\alpha(r)\\beta(r)\\nabla D_{r}f_{r}(w)^{i}+\\displaystyle{\\frac{1}{2}}\\alpha(r)\\beta^{\\prime}(r)\\nabla f_{r}(w)^{i}}}\\\\ {{\\overline{{{\\nabla^{2}}}}\\,f^{i j}=\\beta(r)^{2}\\,\\nabla^{2}\\,f_{r}(w)^{i j}-\\displaystyle{\\frac{1}{2}}\\alpha(r)\\beta^{\\prime}(r)\\cdot D_{r}f\\cdot(g_{w}^{-1})^{i j}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. We will use uppercase letters for indes ranging over $[0,d]$ and lowercase for $[1,d]$ , with the index 0 corresponding to the variable $r$ ; for example $\\overline{{\\nabla}}f(r,w)^{0}\\,=\\,\\alpha(r)D_{r}f(r,w)$ . We will use ", "page_idx": 20}, {"type": "text", "text": "Einstein summation notation freely. With slight abuse of notation we denote $(g^{i j})_{i j}=g^{-1}$ for the inverse matrix of the metric $(g_{i j})_{i j}=g$ , and likewise for $\\overline{{g}}^{I J},\\overline{{g}}_{I J}$ , so that for example $\\overline{{g}}^{00}=\\alpha(r)$ . We start by using that [Lee18, Example 4.22, Eq. (5.10)] ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\overline{{\\nabla^{2}}}\\,f(r,w)^{I J}=\\overline{{g}}^{I K}\\overline{{g}}^{J L}\\left[\\frac{\\partial^{2}f}{\\partial\\omega^{K}\\partial\\omega^{L}}-\\overline{{\\Gamma}}_{K L}^{M}\\frac{\\partial f}{\\partial^{M}\\omega}\\right]}\\\\ &{}&{\\mathrm{and}\\quad\\overline{{\\Gamma}}_{I J}^{M}=\\frac{1}{2}\\overline{{g}}^{M K}\\left[\\frac{\\partial\\overline{{g}}_{K I}}{\\partial\\omega^{J}}+\\frac{\\partial\\overline{{g}}_{K J}}{\\partial\\omega^{I}}-\\frac{\\partial\\overline{{g}}_{I J}}{\\partial\\omega^{K}}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\omega=(r,w)$ , and that the analogous formulas hold for $f_{r}:\\mathcal{W}\\to\\mathbb{R}$ for all $r$ and for $\\Gamma_{i j}^{m}$ the Christoffel symbols of $\\mathcal{W}$ . ", "page_idx": 21}, {"type": "text", "text": "By direct computations, we find that for all $i,j,m\\in[1,d]$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l l l}{{\\overline{{{\\Gamma}}}_{00}^{0}=-{\\displaystyle{\\frac{1}{2}}}{\\frac{\\alpha^{\\prime}(r)}{\\alpha(r)}}\\qquad}}&{{\\overline{{{\\Gamma}}}_{i0}^{0}=\\overline{{{\\Gamma}}}_{0i}^{0}=0}}&{{\\qquad\\overline{{{\\Gamma}}}_{i j}^{0}={\\displaystyle{\\frac{1}{2}}}\\alpha(r){\\frac{\\beta^{\\prime}(r)}{\\beta(r)^{2}}}g_{i j}}}\\\\ {{\\overline{{{\\Gamma}}}_{00}^{m}=0}}&{{\\qquad\\overline{{{\\Gamma}}}_{i0}^{m}=\\overline{{{\\Gamma}}}_{0i}^{m}=-{\\displaystyle{\\frac{1}{2}}}{\\frac{\\beta^{\\prime}(r)}{\\beta(r)}}\\delta_{i}^{m}}}&{{\\qquad\\overline{{{\\Gamma}}}_{i j}^{m}=\\Gamma_{i j}^{m}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "So by direct computations, we find that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\overline{{{\\nabla^{2}}}}\\,f^{00}=\\alpha(r)^{2}D_{r r}^{2}f+\\frac{1}{2}\\alpha(r)\\alpha^{\\prime}(r)D_{r}f}}\\\\ {{\\overline{{{\\nabla^{2}}}}\\,f^{i0}=\\overline{{{\\nabla^{2}}}}\\,f^{0i}=\\alpha(r)\\beta(r)\\nabla D_{r}f_{r}(w)^{i}+\\frac{1}{2}\\alpha(r)\\beta^{\\prime}(r)\\nabla f_{r}(w)^{i}}}\\\\ {{\\overline{{{\\nabla^{2}}}}\\,f^{i j}=\\beta(r)^{2}\\,\\nabla^{2}\\,f_{r}(w)^{i j}-\\frac{1}{2}\\alpha(r)\\beta^{\\prime}(r)\\cdot D_{r}f\\cdot g^{i j},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "as announced. ", "page_idx": 21}, {"type": "text", "text": "Corollary C.9. Let $f:\\Omega_{+}^{*}=\\mathbb{R}_{+}^{*}\\times\\mathcal{W}\\to\\mathbb{R}$ defined by $f(r,w)=r^{p}\\tilde{\\phi}(w)+\\lambda^{\\prime}r^{p b},$ , for some $p>0$ , $b\\in[1,2]$ , $\\lambda^{\\prime}\\geq0$ and $\\tilde{\\phi}:\\mathcal{W}\\to\\mathbb{R}$ . ", "page_idx": 21}, {"type": "text", "text": "Consider $\\Omega_{+}^{*}$ equipped with the Riemannian metric (3.2). Then the Riemannian Hessian of $f$ is given in coordinates by ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\overline{{{\\nabla^{2}}}}\\,f^{00}=\\Gamma^{2}p(p-q_{r}/2)r^{2-2q_{r}+p}\\tilde{\\phi}(w)+\\Gamma^{2}p b\\lambda^{\\prime}(p b-q_{r}/2)r^{2-2q_{r}+p b}}}\\\\ {{\\overline{{{\\nabla^{2}}}}\\,f^{i0}=\\overline{{{\\nabla^{2}}}}\\,f^{0i}=\\Gamma(p-q_{w}/2)r^{1-q_{r}-q_{w}+p}\\nabla\\tilde{\\phi}(w)^{i}}}\\\\ {{\\overline{{{\\nabla^{2}}}}\\,f^{i j}=r^{p-2q_{w}}\\,\\nabla^{2}\\tilde{\\phi}(w)^{i j}+\\frac{1}{2}\\Gamma q_{w}r^{-q_{r}-q_{w}}\\cdot\\left(p r^{p}\\tilde{\\phi}(w)+p b\\lambda^{\\prime}r^{p b}\\right)(g_{w}^{-1})^{i j}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. Continuing with the same notations as in the proof of the lemma above, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{c c}{{D_{r}f=p r^{p-1}\\tilde{\\phi}(w)+p b\\lambda^{\\prime}r^{p b-1}}}&{{D_{r r}^{2}f=p(p-1)r^{p-2}\\tilde{\\phi}(w)+p b(p b-1)\\lambda^{\\prime}r^{p b-2}}}\\\\ {{\\nabla f_{r}(w)^{i}=r^{p}\\nabla\\tilde{\\phi}(w)^{i}}}&{{\\nabla^{2}f_{r}(w)^{i j}=r^{p}\\nabla^{2}\\tilde{\\phi}(w)^{i j}}}\\\\ {{}}&{{\\nabla D_{r}f_{r}(w)^{i}=p r^{p-1}\\nabla\\tilde{\\phi}(w)^{i}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and so ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\overline{{7^{2}}}\\,f^{00}=\\alpha(r)^{2}\\left(p(p-1)r^{p-2}\\tilde{\\phi}(w)+p b(p b-1)\\lambda^{\\prime}r^{p b-2}\\right)+\\frac{1}{2}\\alpha(r)\\alpha^{\\prime}(r)\\left(p r^{p-1}\\tilde{\\phi}(w)+p b\\lambda^{\\prime}r^{p b-1}\\right)}}\\\\ {{\\mathrm{}}}\\\\ {{\\qquad=\\alpha(r)p\\left(\\alpha(r)(p-1)+\\frac{1}{2}r\\alpha^{\\prime}(r)\\right)r^{p-2}\\tilde{\\phi}(w)+\\alpha(r)p b\\lambda^{\\prime}\\left(\\alpha(r)(p b-1)+\\frac{1}{2}r\\alpha^{\\prime}(r)\\right)r^{p b-2}}}\\\\ {{\\overline{{\\nabla^{2}}}\\,f^{i0}=\\overline{{\\nabla^{2}}}\\,f^{0i}=\\alpha(r)\\beta(r)\\cdot p r^{p-1}\\nabla\\tilde{\\phi}(w)^{i}+\\frac{1}{2}\\alpha(r)\\beta^{\\prime}(r)\\cdot r^{p}\\nabla\\tilde{\\phi}(w)^{i}}}\\\\ {{\\mathrm{}}}\\\\ {{\\qquad=\\alpha(r)\\left(\\beta(r)p+\\frac{1}{2}r\\beta^{\\prime}(r)\\right)r^{p-1}\\nabla\\tilde{\\phi}(w)^{i}}}\\\\ {{\\overline{{\\nabla^{2}}}\\,f^{i j}=\\beta(r)^{2}\\cdot r^{p}\\,\\nabla^{2}\\tilde{\\phi}(w)^{i j}-\\frac{1}{2}\\alpha(r)\\beta^{\\prime}(r)\\cdot\\left(p r^{p-1}\\tilde{\\phi}(w)+p b\\lambda^{\\prime}r^{p b-1}\\right)\\cdot g^{i j}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "By substituting $\\alpha(r)^{-1}=\\Gamma^{-1}r^{q_{r}-2}$ and $\\beta(r)^{-1}=r^{q_{w}}$ , i.e. $\\alpha(r)=\\Gamma r^{2-q_{r}}$ and $\\beta(r)=r^{-q_{w}}$ , we obtain the announced formulas. \u53e3 ", "page_idx": 21}, {"type": "text", "text": "Proof of Lem. C.7. Continuing with the same notations as in the proofs of the lemma and of the corollary above, note that $f:\\Omega_{+}^{*}=\\mathbb{R}_{+}^{*}\\times\\mathcal{W}\\to\\mathbb{R}$ having Lipschitz-continuous gradients in the Riemannian sense is equivalent to [Bou23, Coroll. 10.47] ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\omega\\in\\Omega_{+}^{*}}\\ \\operatorname*{sup}_{\\begin{array}{l}{s\\in T_{\\omega}\\Omega_{+}^{*}}\\\\ {\\left\\lVert s\\right\\rVert_{\\omega}=1}\\end{array}}\\left\\lVert\\overline{{\\nabla^{2}}}\\,f(\\omega)^{I J}\\overline{{g}}_{J K}s^{K}\\right\\rVert_{\\omega}<\\infty.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Rewriting everything in coordinates, this means that the matrix $\\begin{array}{r l r}{\\widetilde{H}(\\omega)}&{{}}&{=}\\end{array}$ $\\left(\\sqrt{\\overline{{{g}}}}_{I K}\\;\\,\\overline{{{\\nabla^{2}}}}\\,f(\\omega)^{I J}\\;\\sqrt{\\overline{{{g}}}}_{J L}\\right)_{K L}\\;\\;\\in\\;\\;\\mathbb{R}^{(d+1)\\times(d+1)}$ must be bounded, uniformly in $\\omega\\in\\Omega_{+}^{*}$ , where $(\\sqrt{\\overline{{{g}}}}_{I J})_{I J}\\:=\\:\\sqrt{\\overline{{{g}}}}$ denotes the square root of the positive-definite matrix $\\overline{{g}}$ (pointwise for each $\\omega$ ). Concretely, for all $i,j\\in[1,d]$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sqrt{\\overline{{g}}}_{00}=\\alpha(r)^{-1/2}=\\Gamma^{-1/2}r^{q_{r}/2-1},\\ \\ \\ \\ \\sqrt{\\overline{{g}}}_{i0}=0,\\ \\ \\ \\ \\ \\sqrt{\\overline{{g}}}_{i j}=\\beta(r)^{-1/2}\\sqrt{g}_{i j}=r^{q_{w}/2}\\sqrt{g}_{i j}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{H}(\\omega)_{00}=\\overline{{g}}_{00}\\,\\overline{{\\nabla^{2}}}\\,f^{00}}\\\\ &{\\qquad\\qquad=\\Gamma p(p-q_{r}/2)r^{-q_{r}+p}\\widetilde{\\phi}(w)+\\Gamma p b\\lambda^{\\prime}(p b-q_{r}/2)r^{-q_{r}+p b}}\\\\ &{\\widetilde{H}(\\omega)_{j0}=\\sqrt{\\overline{{g}}}_{00}\\sqrt{\\overline{{g}}}_{j i}\\,\\overline{{\\nabla^{2}}}\\,f^{i0}}\\\\ &{\\qquad\\qquad=\\Gamma^{1/2}(p-q_{w}/2)r^{-q_{r}/2-q_{w}/2+p}\\cdot\\sqrt{g}_{j i}\\nabla\\widetilde{\\phi}(w)^{i}}\\\\ &{\\widetilde{H}(\\omega)_{k l}=\\sqrt{\\overline{{g}}}_{k i}\\sqrt{\\overline{{g}}}_{l j}\\,\\overline{{\\nabla^{2}}}\\,f^{i j}}\\\\ &{\\qquad\\qquad=r^{p-q_{w}}\\cdot\\sqrt{g}_{k i}\\sqrt{g}_{l j}\\,\\nabla^{2}\\,\\widetilde{\\phi}(w)^{i j}+\\Gamma\\frac{1}{2}q_{w}r^{-q_{r}}\\cdot\\left(p r^{p}\\widetilde{\\phi}(w)+p b\\lambda^{\\prime}r^{p b}\\right)\\delta_{k l}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "(Note that here the indes do not respect the covariant/contravariant convention, i.e., $\\dot{\\overline{{g}}}_{I K},$ \u201d and \u201c $\\langle\\widetilde{H}(\\omega)_{K L}{}^{,,}$ do not stand for covariant tensors: we really manipulate everything in coordinates e xplicitly.) ", "page_idx": 22}, {"type": "text", "text": "Now, note that the desired condition means that $\\widetilde{H}(\\omega)_{K L}$ should remain bounded both for $r\\to+\\infty$ and $r\\rightarrow0$ . That is, the exponents of $r$ in the non-zero terms must all be 0. Thus, since we assume that $\\lambda^{\\prime}\\neq0$ , and that $\\nabla^{2}\\tilde{\\phi}$ is not constant equal to 0 and so in particular $\\tilde{\\phi}$ and $\\nabla\\tilde{\\phi}$ are not constant, \u2022 Uniform boundedness of the second term in $\\widetilde{H}(\\omega)_{k l}$ implies that $b=1$ . Indeed $\\lambda^{\\prime}\\neq0$ , and the first term (in $\\nabla^{2}\\tilde{\\phi}$ ) cannot cancel out both the term in $\\tilde{\\phi}(w)r^{p-q_{r}}$ and the term in $\\lambda^{\\prime}r^{p b-q_{r}}$ if they scale differently with $r$ . This also implies that either $p=q_{w}=q_{r}$ or that $q_{w}=q_{r}$ and $\\begin{array}{r}{\\nabla^{2}\\tilde{\\phi}(w)^{i j}=\\frac{1}{2}\\Gamma q_{w}p\\left(\\tilde{\\phi}(w)+\\lambda^{\\prime}\\right)g^{i j}}\\end{array}$ for all $w$ . \u2022 Uniform boundedness of $\\widetilde{H}(\\omega)_{00}$ implies that $p=q_{r}$ or $p=q_{r}/2$ . \u2022 Uniform boundedness of $\\widetilde{H}(\\omega)j0$ implies that $\\textstyle p={\\frac{q_{r}+q_{w}}{2}}$ or $p=q_{w}/2$ . We saw in the first point that $q_{r}=q_{w}$ , so equivalently $p=q_{r}=q_{w}$ or $p\\,\\bar{=}\\,q_{r}/2=q_{w}/2$ . ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "Thus we get that $f$ can have Lipschitz-continuous Riemannian gradients only if $b=1$ and $p=q_{r}=$ $q_{w}$ , or if $b=1$ and $p=q_{r}/2=q_{w}/2$ and $\\nabla^{2}\\tilde{\\phi}(w)=\\Gamma p^{2}\\left(\\tilde{\\phi}(w)+\\lambda^{\\prime}\\right)g_{w}$ for all $w$ . \u53e3 ", "page_idx": 22}, {"type": "text", "text": "D Details for Sec. 3.2 (reduction by bilevel optimization) ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "D.1 Proof of Prop. 3.3 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In preparation for the proof of Prop. 3.3, let us first provide a formal proof of the variational representation of the squared-TV norm mentioned at the beginning of Sec. 3.2, with a characterization of the set of minimizers. See [Chi17, App. 1] for the rigorous justification of these arguments in the more general context of minimization of convex and positively 1-homogeneous integral functionals over the space of signed measures. ", "page_idx": 22}, {"type": "text", "text": "Lemma D.1 (\u201c $\\eta$ -trick\u201d for the squared TV-norm). We have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\|\\nu\\|_{T V}^{2}=\\left(\\int_{\\mathcal{W}}|\\nu(d w)|\\right)^{2}=\\operatorname*{inf}_{\\eta\\in\\mathcal{P}(\\mathcal{W})}\\int_{\\mathcal{W}}\\frac{\\left|\\nu(d w)\\right|^{2}}{\\eta(d w)}=\\operatorname*{inf}_{\\eta\\in\\mathcal{P}(\\mathcal{W}),~f:\\mathcal{W}\\to\\mathbb{R}}\\int_{\\mathcal{W}}|f|^{2}\\,d\\eta.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Moreover the infimum in the third expression is attained at (and only at) $\\begin{array}{r}{\\eta(d w)=\\frac{|\\nu(d w)|}{\\|\\nu\\|_{T V}}}\\end{array}$ , and the infimum in the fourth expression is attained at (and only at) the same $\\eta$ and $\\begin{array}{r}{f=\\frac{\\nu(d w)}{|\\nu(d w)|}\\left\\lVert\\nu\\right\\rVert_{T V}}\\end{array}$ . ", "page_idx": 23}, {"type": "text", "text": "Proof. The infimum in the third expression is the value of a convex constrained minimization problem, whose Lagrangian is $\\begin{array}{r}{\\mathcal{L}(\\eta;\\dot{\\lambda})\\,=\\,\\int\\frac{|\\nu|^{2}}{\\eta}+\\lambda\\,\\bigl(\\int\\mathrm{d}\\eta-1\\bigr)}\\end{array}$ . The dual optimality condition ivmalpulei $\\left\\|\\nu\\right\\|_{T V}^{2}$ $\\begin{array}{r}{\\forall w\\in\\mathrm{supp}(\\eta),\\lambda=\\frac{\\mathrm{d}\\nu}{\\mathrm{d}\\eta}(w)^{2}}\\end{array}$ , so the infinimum is attained at $\\begin{array}{r}{\\eta(\\mathrm{d}w)=\\frac{|\\nu(\\mathrm{d}w)|}{\\|\\nu\\|_{T V}}}\\end{array}$ |\u03bd\u03bd(dw)|, with optimal ", "page_idx": 23}, {"type": "text", "text": "The optimality condition for the infimum in the fourth expression follows directly from the one for the third expression and from the constraint $f\\eta=\\nu$ . ", "page_idx": 23}, {"type": "text", "text": "Proof of Prop. 3.3. By the lemma above, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{\\eta\\in\\mathcal{P}(\\mathcal{W})}{\\operatorname*{inf}}J_{\\lambda}(\\eta)=\\underset{\\eta\\in\\mathcal{P}(\\mathcal{W}),f\\mathcal{W}\\to\\mathbb{R}}{\\operatorname*{inf}}G(f\\eta)+\\frac{\\lambda}{2}\\int_{\\mathcal{W}}|f|^{2}\\,d\\eta}&{}\\\\ &{\\qquad=\\underset{\\nu\\in\\mathcal{M}(\\mathcal{W})}{\\operatorname*{inf}}\\,\\underset{\\eta\\in\\mathcal{P}(\\mathcal{W}),f\\mathcal{W}\\to\\mathbb{R}}{\\operatorname*{inf}}\\,G(f\\eta)+\\frac{\\lambda}{2}\\int_{\\mathcal{W}}|f|^{2}\\,d\\eta}\\\\ &{\\qquad=\\underset{\\nu\\in\\mathcal{M}(\\mathcal{W})}{\\operatorname*{inf}}G(\\nu)+\\frac{\\lambda}{2}\\left[\\underset{\\eta\\in\\mathcal{P}(\\mathcal{W}),f\\mathcal{W}\\to\\mathbb{R}}{\\operatorname*{inf}}\\,\\int_{\\mathcal{W}}|f|^{2}\\,d\\eta\\right]}\\\\ &{\\qquad=\\underset{\\nu\\in\\mathcal{M}(\\mathcal{W})}{\\operatorname*{inf}}G(\\nu)+\\frac{\\lambda}{2}\\left[\\nu\\|_{T V}^{2}\\,\\underset{r\\in\\mathcal{W}}{\\operatorname*{inf}}\\,\\underset{\\nu\\in\\mathcal{M}(\\mathcal{W})}{\\operatorname*{inf}}\\,G_{\\lambda}(\\nu).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Hence the equality of the optimal values. The claimed characterization of $\\arg\\operatorname*{min}J_{\\lambda}$ in terms of arg min $G_{\\lambda}$ follows from the characterization of the minimizers of the inner minimization $\\begin{array}{r}{\\left[\\operatorname*{inf}_{\\eta\\in\\mathcal{P}(\\mathcal{W}),~f:\\mathcal{W}\\to\\mathbb{R}}~\\frac{\\lambda}{2}\\int_{\\mathcal{W}}\\left|f\\right|^{2}d\\eta\\right]}\\end{array}$ in the third line, which is given by the lemma above. ", "page_idx": 23}, {"type": "text", "text": "Furthermore, $J_{\\lambda}$ is convex as the partial minimization of $\\begin{array}{r}{(\\eta,\\nu)\\mapsto G(\\nu)+\\frac{\\lambda}{2}\\int\\frac{|\\nu|^{2}}{\\eta}}\\end{array}$ , which is jointly ", "page_idx": 23}, {"type": "text", "text": "D.2 Proof of the explicit form of the two-timescale SDE (3.4) ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "For ease of reference, we recall here the two-timescale SDE (3.4): ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\forall i\\leq N,\\left\\{\\underset{\\b{\\mathrm{d}}}{\\mathrm{d}}r_{t}^{i}=-\\Gamma\\,\\nabla_{r^{i}}\\boldsymbol{F}_{\\lambda,2}^{\\prime}\\left[\\frac{1}{N}\\sum_{j=1}^{N}\\delta_{(r_{t}^{j},w_{t}^{j})}\\right](r_{t}^{i},w_{t}^{i})\\mathrm{d}t\\right.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "By (C.2) with $b=2$ and $p=1$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F_{\\lambda,2}^{\\prime}[\\mu](r,w)=r G^{\\prime}[h\\mu](w)+\\displaystyle\\frac{\\lambda}{2}\\left|r\\right|^{2}}\\\\ {\\mathrm{so}}&{\\nabla_{r}F_{\\lambda,2}^{\\prime}[\\mu](r,w)=G^{\\prime}[h\\mu](w)+\\lambda r}\\\\ {\\mathrm{and}}&{\\nabla_{w}F_{\\lambda,2}^{\\prime}[\\mu](r,w)=r\\nabla G^{\\prime}[h\\mu](w).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Finally, by definition $\\begin{array}{r}{\\pmb{h}\\left[\\frac{1}{N}\\sum_{j=1}^{N}\\delta_{(r^{j},w^{j})}\\right]=\\frac{1}{N}\\sum_{j=1}^{N}r^{j}\\delta_{w^{j}}}\\end{array}$ . Hence the second part of (3.4). ", "page_idx": 23}, {"type": "text", "text": "D.3 Proof of Prop. 3.4 ( $J_{\\lambda}$ satisfies $\\mathbf{P0}$ , P1 and P2) ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Simplifying the expression of the bilevel objective. The following expressions will be useful throughout our analyses of the bilevel problem (3.3). ", "page_idx": 24}, {"type": "text", "text": "Proposition D.2. We have that $\\begin{array}{r}{J_{\\lambda}(\\eta)=G(f_{\\eta}\\eta)+\\frac{\\lambda}{2}\\int|f_{\\eta}|^{2}\\,\\mathrm{d}\\eta}\\end{array}$ where $f_{\\eta}$ is the unique solution of the fixed-point equation ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\forall w\\in\\mathcal{W},\\;f_{\\eta}(w)=-\\frac{1}{\\lambda}G^{\\prime}[f_{\\eta}\\eta](w).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Furthermore, ", "page_idx": 24}, {"type": "equation", "text": "$$\nJ_{\\lambda}^{\\prime}[\\eta](w)=-\\frac{\\lambda}{2}\\,|f_{\\eta}|^{2}\\,(w).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. Consider the optimization problem defining $J_{\\lambda}(\\eta)$ , for a fixed $\\eta$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{f\\in L_{\\eta}^{2}(\\mathcal W)}G(f\\eta)+\\frac{\\lambda}{2}\\int_{\\mathcal W}\\left|f\\right|^{2}\\mathrm d\\eta.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "This problem is convex since $G$ is, and strongly convex in $L_{\\eta}^{2}(\\mathcal{W})$ thanks to the term in $\\lambda$ . So there exists a unique solution which we denote by $\\tilde{f}_{\\eta}\\in L_{\\eta}^{2}(\\mathcal{W})$ , and it is characterized by the first-order optimality condition: ", "page_idx": 24}, {"type": "equation", "text": "$$\nG^{\\prime}[\\tilde{f}_{\\eta}\\;\\eta]\\;\\eta+\\lambda\\tilde{f}_{\\eta}\\;\\eta=0\\;\\;\\mathrm{in}\\;{\\mathcal M}(\\mathcal{W}).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Now let $\\begin{array}{r}{f_{\\eta}=-\\frac{1}{\\lambda}G^{\\prime}[\\tilde{f}_{\\eta}\\eta]}\\end{array}$ , which is defined over all of $\\mathcal{W}$ . Then $f_{\\eta}$ satisfies the fixed-point equation (D.1) by construction. Conversely, for any solution $g_{\\eta}$ of (D.1), its restriction to $\\operatorname{supp}(\\eta)$ viewed as an element $\\tilde{g}_{\\eta}$ of $L_{\\eta}^{2}(\\mathcal{W})$ must in particular satisfy $G^{\\prime}[\\tilde{g}_{\\eta}\\eta]\\eta+\\lambda\\tilde{g}_{\\eta}\\eta=0$ in $\\mathcal{M}(\\mathcal{W})$ , and so $\\tilde{g}_{\\eta}=\\tilde{f}_{\\eta}$ , and so $\\begin{array}{r}{g_{\\eta}=-\\frac{1}{\\lambda}G^{\\prime}[\\tilde{g}_{\\eta}\\eta]=-\\frac{1}{\\lambda}G^{\\prime}[\\tilde{f}_{\\eta}\\eta]=f_{\\eta}.}\\end{array}$ . ", "page_idx": 24}, {"type": "text", "text": "Furthermore, by differentiability of $G$ then $\\eta\\mapsto\\tilde{f_{\\eta}}$ is continuous (in the total variation sense). So in turn, $\\eta\\mapsto f_{\\eta}(w)$ the unique solution of (D.1) is continuous for each $w$ (in the total variation sense). So by the envelope theorem, since for any fixed $f$ the first variation of $\\begin{array}{r}{\\eta\\mapsto G(f\\eta)+\\frac{\\lambda}{2}\\int|f|^{2}\\,\\mathrm{d}\\eta}\\end{array}$ is $w\\mapsto f(w)G^{\\prime}[f\\eta](w)+{\\textstyle{\\frac{\\lambda}{2}}}\\,|f(w)|^{2}$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle J_{\\lambda}^{\\prime}[\\eta](w)=f_{\\eta}(w)G^{\\prime}[f_{\\eta}\\eta](w)+\\frac{\\lambda}{2}\\left|f_{\\eta}(w)\\right|^{2}}}\\\\ {{\\displaystyle\\qquad\\qquad=-\\frac{\\lambda}{2}\\left|f_{\\eta}(w)\\right|^{2}=-\\frac{1}{2\\lambda}\\left|G^{\\prime}[f_{\\eta}\\eta]\\right|^{2}(w),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "which is precisely Eq. (D.2). ", "page_idx": 24}, {"type": "text", "text": "We remark that the above manipulations rely crucially on the fact that the optimization problem (1.1) is over signed measures and not just non-negative measures \u2013 as otherwise we would additionally need to constrain $f\\geq0-$ , and on the regularization term being $\\|\\nu\\|_{T V}^{2}$ instead of $\\|\\nu\\|_{T V}$ . ", "page_idx": 24}, {"type": "text", "text": "Preliminary estimates. ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Lemma D.3. Under Assumption $^{\\,l}$ , for any $\\nu\\in\\mathcal{M}(\\mathcal{W})$ , we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{w\\in\\mathcal{W}}\\left|G^{\\prime}[\\nu](w)\\right|^{2}\\leq2L_{0}G(\\nu).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. We follow the proof technique of [GGGM21, Appendix D]. Let $w_{0}\\,\\in\\,\\mathcal{W}$ and $\\nu^{\\prime}=\\nu-$ $\\begin{array}{r}{\\frac{1}{L_{0}}\\dot{G^{\\prime}}[\\nu](w_{0})\\delta_{w_{0}}}\\end{array}$ . By mean-value theorem there exists $\\theta\\ \\in\\ (0,1)$ such that $G(\\nu^{\\prime})\\,-\\,G(\\nu)\\,\\,=$ $\\begin{array}{r}{\\int G^{\\prime}[\\nu+\\theta(\\nu^{\\prime}-\\nu)]\\mathrm{d}(\\nu^{\\prime}-\\nu)}\\end{array}$ , and so ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\operatorname*{inf}G\\le G(\\nu^{\\prime})\\le G(\\nu)+\\int G^{\\prime}[\\nu]\\mathrm{d}(\\nu^{\\prime}-\\nu)+\\frac{L_{0}}{2}\\left\\lVert\\nu^{\\prime}-\\nu\\right\\rVert_{T V}^{2}}\\\\ {\\displaystyle=G(\\nu)-\\frac{1}{L_{0}}G^{\\prime}[\\nu](w_{0})^{2}+\\frac{1}{2L_{0}}G^{\\prime}[\\nu](w_{0})^{2}=G(\\nu)-\\frac{1}{2L_{0}}G^{\\prime}[\\nu](w_{0})^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Hence, since $G$ is non-negative by Assumption 1, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\forall w\\in\\mathcal{W},\\;\\frac{1}{2L_{0}}G^{\\prime}[\\nu](w)^{2}\\leq G(\\nu)-\\operatorname*{inf}G\\leq G(\\nu)\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Lemma D.4. Under Assumption $^{\\,l}$ , let $\\eta\\in\\mathcal{P}(\\mathcal{W})$ and let $f_{\\eta}$ as in (D.1). Then ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\mathcal{W}}|f_{\\eta}|\\leq\\frac{1}{\\lambda}\\sqrt{2L_{0}J_{\\lambda}(\\eta)}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and for each $i\\in\\{1,2\\}$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{w\\in\\mathcal{W}}\\left\\|\\nabla^{i}f_{\\eta}\\right\\|_{w}\\le\\frac{L_{i}}{\\lambda^{2}}\\sqrt{2L_{0}J_{\\lambda}(\\eta)}+\\frac{B_{i}}{\\lambda}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Moreover, $J_{\\lambda}(\\eta)\\le G(0)$ for all $\\eta\\in\\mathcal{P}(\\mathcal{W})$ . ", "page_idx": 25}, {"type": "text", "text": "Proof. For the first inequality, by definition $G^{\\prime}[f_{\\eta}\\eta]=-\\lambda f_{\\eta}$ for all $w\\in\\mathscr{W}$ , so ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\lambda^{2}\\left|f_{\\eta}(w)\\right|^{2}=\\left|G^{\\prime}[f_{\\eta}\\eta](w)\\right|^{2}\\leq2L_{0}G(f_{\\eta}\\eta)\\leq2L_{0}\\left(G(f_{\\eta}\\eta)+{\\frac{\\lambda}{2}}\\int|f_{\\eta}|^{2}\\,\\mathrm{d}\\eta\\right)=2L_{0}J_{\\lambda}(\\eta)\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the first inequality follows from Lem. D.3. ", "page_idx": 25}, {"type": "text", "text": "For the second part, by Assumption $1,\\forall\\nu\\in\\mathcal{M}(\\mathcal{W})$ , $\\begin{array}{r}{\\operatorname*{sup}_{w}\\left\\|\\nabla^{i}G^{\\prime}[\\nu]\\right\\|_{w}\\leq L_{i}\\left\\|\\nu\\right\\|_{T V}+B_{i}}\\end{array}$ , so ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\lambda\\left\\|\\nabla^{i}f_{\\eta}\\right\\|_{w}=\\left\\|\\nabla^{i}G^{\\prime}[f_{\\eta}\\eta]\\right\\|_{w}\\leq B_{i}+L_{i}\\left\\|f_{\\eta}\\eta\\right\\|_{T V}=B_{i}+L_{i}\\int|f_{\\eta}|\\,\\mathrm{d}\\eta}}\\\\ &{}&{\\leq B_{i}+L_{i}\\operatorname*{sup}_{W}|f_{\\eta}|\\leq B_{i}+L_{i}\\frac{1}{\\lambda}\\sqrt{2L_{0}J_{\\lambda}(\\eta)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "by the first part of the lemma. ", "page_idx": 25}, {"type": "text", "text": "Finally, the uniform bound on $J_{\\lambda}(\\eta)$ follows by taking $f=0$ in the infimum defining $J_{\\lambda}\\colon J_{\\lambda}(\\eta)=$ $\\begin{array}{r}{\\operatorname*{inf}_{f\\in L_{\\eta}^{2}}G(f\\eta)+\\frac{\\lambda}{2}\\int\\left|f\\right|^{2}\\mathrm{d}\\eta\\leq\\dot{G}(0)}\\end{array}$ . \u53e3 ", "page_idx": 25}, {"type": "text", "text": "Lemma D.5. Under Assumption $^{\\,l}$ , $J_{\\lambda}:\\mathcal{P}(\\mathcal{W})\\rightarrow\\mathbb{R}$ is weakly continuous and ", "page_idx": 25}, {"type": "text", "text": "where ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\forall\\eta,\\eta^{\\prime}\\in\\mathcal{P}(\\mathcal{W}),\\ |J_{\\lambda}(\\eta)-J_{\\lambda}(\\eta^{\\prime})|\\leq B W_{2}(\\eta,\\eta^{\\prime})}\\\\ &{\\Big/2L_{0}G(0)\\cdot\\Big(\\frac{L_{1}}{\\lambda^{2}}\\sqrt{2L_{0}G(0)}+\\frac{B_{1}}{\\lambda}\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. For any $\\eta\\in\\mathcal{P}(\\mathcal{W})$ , letting $f_{\\eta}$ as in (D.1), we have $\\begin{array}{r}{J_{\\lambda}^{\\prime}[\\eta](w)=-\\frac{\\lambda}{2}\\left|f_{\\eta}\\right|^{2}(w)}\\end{array}$ so ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\qquad\\nabla J_{\\boldsymbol{\\lambda}}^{\\prime}[\\eta](w)=-\\lambda f_{\\eta}(w)\\nabla f_{\\eta}(w)}\\\\ &{\\Vert\\nabla J_{\\boldsymbol{\\lambda}}^{\\prime}[\\eta](w)\\Vert_{w}\\le\\lambda\\operatorname*{sup}_{\\mathcal{W}}|f_{\\eta}|\\cdot\\operatorname*{sup}_{\\mathcal{W}}\\Vert\\nabla f_{\\eta}\\Vert}\\\\ &{\\qquad\\qquad\\qquad\\le\\boldsymbol{\\lambda}\\cdot\\frac{1}{\\boldsymbol{\\lambda}}\\sqrt{2L_{0}G(0)}\\cdot\\left(\\frac{L_{1}}{\\boldsymbol{\\lambda}^{2}}\\sqrt{2L_{0}G(0)}+\\frac{B_{1}}{\\boldsymbol{\\lambda}}\\right)=:\\boldsymbol{B}<\\infty}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "by Lem. D.4, uniformly in $\\eta~\\in~\\mathcal{P}(\\mathcal{W})$ and $w\\ \\in\\ w$ . So by Lem. D.8 below, we have $\\vert J_{\\lambda}(\\eta)-J_{\\lambda}(\\eta^{\\prime})\\vert\\le B W_{2}(\\eta,\\eta^{\\prime})$ for all $\\eta,\\eta^{\\prime}\\in\\mathcal{P}(\\mathcal{W})$ . Moreover $W_{2}$ metrizes weak convergence, so $J_{\\lambda}$ is weakly continuous. \u53e3 ", "page_idx": 25}, {"type": "text", "text": "Lemma D.6. Under Assumption $^{\\,l}$ , let $w^{\\prime}\\in\\mathscr{W}$ and $\\eta\\in\\mathcal{P}(\\mathcal{W})$ . Let $h:\\mathcal{W}\\to\\mathbb{R}$ and suppose that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\forall w\\in\\mathcal{W},\\ \\lambda h(w)+\\int G^{\\prime\\prime}[f_{\\eta}\\eta](w,w^{\\prime\\prime})d\\eta(w^{\\prime\\prime})h(w^{\\prime\\prime})=-G^{\\prime\\prime}[f_{\\eta}\\eta](w,w^{\\prime})f_{\\eta}(w^{\\prime}).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Then $\\begin{array}{r}{\\operatorname*{sup}_{w\\in\\mathcal{W}}|h(w)|\\leq\\left(1+\\frac{L_{0}}{\\lambda}\\right)\\frac{L_{0}}{\\lambda}\\sqrt{2L_{0}G(0)}.}\\end{array}$ ", "page_idx": 25}, {"type": "text", "text": "Alternatively, suppose that there exists $s\\in T_{w^{\\prime}}\\mathcal{W}$ with $\\|s\\|_{w^{\\prime}}=1$ such that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\forall w\\in\\mathcal{W},\\ \\lambda h(w)+\\int G^{\\prime\\prime}[f_{\\eta}\\eta](w,w^{\\prime\\prime})d\\eta(w^{\\prime\\prime})h(w^{\\prime\\prime})=-\\left\\langle s^{\\prime},\\nabla_{w^{\\prime}}\\left[G^{\\prime\\prime}[f_{\\eta}\\eta](w,w^{\\prime})f_{\\eta}(w^{\\prime})\\right]\\right\\rangle_{w^{\\prime}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "equation", "text": "$\\begin{array}{r}{\\operatorname*{sup}_{w\\in\\mathcal{W}}|h(w)|\\leq\\left(1+\\frac{L_{0}}{\\lambda}\\right)\\cdot\\Big(\\big(1+\\frac{L_{0}}{\\lambda}\\big)\\,\\frac{L_{1}}{\\lambda}\\sqrt{2L_{0}G(0)}+\\frac{L_{0}B_{1}}{\\lambda}\\Big).}\\end{array}$ ", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. Let $\\mathcal{G}:L_{\\eta}^{2}(\\mathcal{W})\\to L_{\\eta}^{2}(\\mathcal{W})$ the operator ", "page_idx": 26}, {"type": "equation", "text": "$$\n(\\mathcal{G}\\tilde{h})(w)=\\int G^{\\prime\\prime}[f_{\\eta}\\eta](w,w^{\\prime\\prime})d\\eta(w^{\\prime\\prime})\\tilde{h}(w^{\\prime\\prime}).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "$\\mathcal{G}$ is well-defined as a bounded operator, since Assumption 1 implies that $|G^{\\prime\\prime}[f_{\\eta}\\eta](w,w^{\\prime})|\\leq L_{0}$ . Note that $G^{\\prime\\prime}[f_{\\eta}\\eta](w,w^{\\prime\\prime})$ is symmetric in $w$ and $w^{\\prime\\prime}$ , and that by convexity of $G$ , $G^{\\prime\\prime}[f_{\\eta}\\eta](w,w^{\\prime\\prime})\\stackrel{.}{\\geq}\\bar{0}$ for all $w,w^{\\prime\\prime}$ . Consequently, $\\mathcal{G}$ is a symmetric positive-semi-definite operator from $L_{\\eta}^{2}(\\mathcal{W})$ to itself. ", "page_idx": 26}, {"type": "text", "text": "On the other hand, let $V_{1}(\\cdot)=-G^{\\prime\\prime}[f_{\\eta}\\eta](\\cdot,w^{\\prime})f_{\\eta}(w^{\\prime})$ . By Lem. D.4 we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\left\\Vert V_{1}\\right\\Vert_{L_{\\eta}^{2}}\\leq\\operatorname*{sup}_{W}|V_{1}|\\leq\\operatorname*{sup}_{W\\times W}|G^{\\prime\\prime}[f_{\\eta}\\eta]|\\cdot\\operatorname*{sup}_{W}|f_{\\eta}|\\leq L_{0}\\cdot\\frac{1}{\\lambda}\\sqrt{2L_{0}G(0)}=:\\overline{{V}}_{1}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Also let $V_{2}(\\cdot)=-\\left\\langle s^{\\prime},\\nabla_{w^{\\prime}}\\left[G^{\\prime\\prime}[f_{\\eta}\\eta](\\cdot,w^{\\prime})f_{\\eta}(w^{\\prime})\\right]\\right\\rangle_{w^{\\prime}}$ . Then by Lem. D.4, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Vert V_{2}\\Vert_{L_{\\eta}^{2}}\\leq\\underset{\\mathcal{W}}{\\operatorname*{sup}}\\,\\vert V_{2}\\vert\\leq\\underset{w,w^{\\prime}}{\\operatorname*{sup}}\\,\\Vert\\nabla_{w^{\\prime}}G^{\\prime\\prime}[f_{\\eta}\\eta](w,w^{\\prime})\\Vert\\cdot\\underset{\\mathcal{W}}{\\operatorname*{sup}}\\,\\vert f_{\\eta}\\vert+\\underset{\\mathcal{W}\\times\\mathcal{W}}{\\operatorname*{sup}}\\,\\vert G^{\\prime\\prime}[f_{\\eta}\\eta]\\vert\\cdot\\underset{\\mathcal{W}}{\\operatorname*{sup}}\\,\\Vert\\nabla f_{\\eta}\\Vert}\\\\ &{\\qquad\\qquad\\qquad\\leq L_{1}\\cdot\\frac{1}{\\lambda}\\sqrt{2L_{0}G(0)}+L_{0}\\cdot\\left(\\frac{L_{1}}{\\lambda^{2}}\\sqrt{2L_{0}G(0)}+\\frac{B_{1}}{\\lambda}\\right)}\\\\ &{\\qquad\\qquad=\\left(1+\\frac{L_{0}}{\\lambda}\\right)\\frac{L_{1}}{\\lambda}\\sqrt{2L_{0}G(0)}+\\frac{L_{0}B_{1}}{\\lambda}=:\\overline{{V}}_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Denote by $\\tilde{h}$ the restriction of $h$ to $\\operatorname{supp}(\\eta)$ viewed as an element of $L_{\\eta}^{2}(\\mathcal{W})$ . Then, denoting by id the identity operator on $L_{\\eta}^{2}(\\mathcal{W})$ , we may rewrite the assumption as $(\\lambda\\operatorname{id}+\\mathcal{G})\\tilde{h}=V_{j}$ for $j=1$ or 2, and so ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\sqrt{\\int|h|^{2}\\,\\mathrm{d}\\eta}=\\left\\|\\tilde{h}\\right\\|_{L_{\\eta}^{2}}=\\left\\|(\\lambda\\mathrm{id}+\\mathcal{G})^{-1}V_{j}\\right\\|_{L_{\\eta}^{2}}\\leq\\lambda^{-1}\\left\\|V_{j}\\right\\|_{L_{\\eta}^{2}}\\leq\\lambda^{-1}\\overline{{V}}_{j}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "since $\\mathcal{G}$ is positive-semi-definite and $\\lambda>0$ . Thus for any $w\\in\\mathcal{W}$ , we get the point-wise bound ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\lambda h(w)=V_{j}(w)-\\displaystyle\\int d\\eta(w^{\\prime\\prime})G^{\\prime\\prime}[f_{\\eta}\\eta](w,w^{\\prime\\prime})h(w^{\\prime\\prime})}\\\\ &{\\lambda\\left|h(w)\\right|\\le\\left|V_{j}(w)\\right|+\\displaystyle\\int d\\eta(w^{\\prime\\prime})\\left|G^{\\prime\\prime}[f_{\\eta}\\eta](w,w^{\\prime\\prime})\\right|\\left|h(w^{\\prime\\prime})\\right|}\\\\ &{\\qquad\\qquad\\le\\overline{{V}}_{j}+\\|G^{\\prime\\prime}[f_{\\eta}\\eta](w,\\cdot)\\|_{L_{\\eta}^{2}}\\left\\|h\\right\\|_{L_{\\eta}^{2}}}\\\\ &{\\qquad\\qquad\\le\\overline{{V}}_{j}+L_{0}\\cdot\\lambda^{-1}\\overline{{V}}_{j}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Lemma D.7. Under Assumption $^{\\,l}$ , let $\\eta,\\eta^{\\prime}\\in\\mathcal{P}(\\mathcal{W})$ and let $f_{\\eta},f_{\\eta^{\\prime}}$ as in (D.1). Then there exist constants $H,H^{\\prime}$ dependent only on $\\lambda^{-1},G(0)$ and $L_{0},L_{1},B_{1},\\widetilde{L}_{2}$ such that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\mathcal W}|f_{\\eta}-f_{\\eta^{\\prime}}|\\leq H W_{2}(\\eta,\\eta^{\\prime})\\qquad a n d\\qquad\\operatorname*{sup}_{w\\in\\mathcal W}\\|\\nabla f_{\\eta}-\\nabla f_{\\eta^{\\prime}}\\|_{w}\\leq H^{\\prime}W_{2}(\\eta,\\eta^{\\prime}).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. For each $w\\in\\mathscr{W}$ , we denote the first variation of $\\eta\\mapsto f_{\\eta}(w)$ by $w^{\\prime}\\mapsto\\frac{\\delta f_{\\eta}(w)}{\\delta\\eta(\\mathrm{d}w^{\\prime})}$ . Let us show that this quantity is uniformly bounded.8 By definition, for any $w\\in\\mathscr{W}$ and $\\eta\\in\\mathcal{P}(\\mathcal{W})$ and $w^{\\prime}\\in\\mathcal{W}$ , ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\ \\ \\lambda f_{\\eta}(w)+G^{\\prime}[f_{\\eta}\\eta](w)=0}}\\\\ {{\\ \\ \\lambda\\frac{\\delta f_{\\eta}(w)}{\\delta\\eta(w^{\\prime})}+G^{\\prime\\prime}[f_{\\eta}\\eta](w,w^{\\prime})f_{\\eta}(w^{\\prime})+\\displaystyle\\int\\big(G^{\\prime\\prime}[f_{\\eta}\\eta](w,\\cdot)\\big)\\,\\mathrm{d}\\left(\\eta\\frac{\\delta f_{\\eta}(\\cdot)}{\\delta\\eta(w^{\\prime})}\\right)=0}}\\\\ {{\\ \\ \\lambda\\frac{\\delta f_{\\eta}(w)}{\\delta\\eta(w^{\\prime})}+\\displaystyle\\int G^{\\prime\\prime}[f_{\\eta}\\eta](w,w^{\\prime\\prime})\\eta(\\mathrm{d}w^{\\prime\\prime})\\frac{\\delta f_{\\eta}(w^{\\prime\\prime})}{\\delta\\eta(w^{\\prime})}=-G^{\\prime\\prime}[f_{\\eta}\\eta](w,w^{\\prime})f_{\\eta}(w^{\\prime}).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "8The rigorous proof that the first variation\u03b4\u03b4\u03b7f(\u03b7d(ww\u2032)) is well-defined for all $w,w^{\\prime}\\in\\mathcal{W}$ and $\\eta\\in\\mathcal{P}(\\mathcal{W})$ would follow from the same derivations as for the uniform bound, so we omit it here. ", "page_idx": 26}, {"type": "text", "text": "So by Lem. D.6 applied to $\\begin{array}{r}{h=\\frac{\\delta f_{\\eta}(\\cdot)}{\\delta\\eta(w^{\\prime})}}\\end{array}$ , we indeed have that $\\frac{\\delta f_{\\eta}(w)}{\\delta\\eta(w^{\\prime})}$ is bounded by a constant uniformly in $w,w^{\\prime}$ and $\\eta$ . ", "page_idx": 27}, {"type": "text", "text": "Let us now show that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{w\\in\\mathcal{W}}\\operatorname*{sup}_{\\eta\\in\\mathcal{P}(\\mathcal{W})}\\operatorname*{sup}_{w^{\\prime}\\in\\mathcal{W}}\\left\\|\\nabla_{w^{\\prime}}\\frac{\\delta f_{\\eta}(w)}{\\delta\\eta(\\mathrm{d}w^{\\prime})}\\right\\|_{w^{\\prime}}\\leq H\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "for a constant $H$ depending only on $\\lambda^{-1},L_{0},L_{1},B_{1},G(0)$ . Indeed, it suffices to show that for any s\u2032 \u2208Tw\u2032W such that \u2225s\u2032\u2225w\u2032 = 1,   s\u2032, \u2207w\u2032 \u03b4\u03b4\u03b7f(\u03b7d(ww\u2032))  \u2032 . Now, starting from (D.3) \u2013 which holds for all $w,w^{\\prime},\\eta\\,\\cdot$ \u2013 and differentiating with respect to $w^{\\prime}$ in the direction $s^{\\prime}$ , we get that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\lambda\\left\\langle s^{\\prime},\\nabla_{w^{\\prime}}\\frac{\\delta f_{\\eta}(w)}{\\delta\\eta(w^{\\prime})}\\right\\rangle_{w^{\\prime}}+\\int G^{\\prime\\prime}[f_{\\eta}\\eta](w,w^{\\prime\\prime})\\eta(\\mathrm{d}w^{\\prime\\prime})\\left\\langle s^{\\prime},\\nabla_{w^{\\prime}}\\frac{\\delta f_{\\eta}(w^{\\prime\\prime})}{\\delta\\eta(w^{\\prime})}\\right\\rangle_{w^{\\prime}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=-\\left\\langle s^{\\prime},\\nabla_{w^{\\prime}}[G^{\\prime\\prime}[f_{\\eta}\\eta](w,w^{\\prime})f_{\\eta}(w^{\\prime})]\\right\\rangle_{w^{\\prime}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and so h(w) = s\u2032, \u2207w\u2032 \u03b4\u03b4\u03b7f(\u03b7d(ww\u2032)) satisfies the conditions of Lem. D.6, which proves the claim. Next let us show that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{w\\in\\mathcal{W}}\\operatorname*{sup}_{\\left.s\\in T_{w}\\mathcal{W}\\right.\\eta\\in\\mathcal{P}(\\mathcal{W})}\\operatorname*{sup}_{w^{\\prime}\\in\\mathcal{W}}\\left\\|\\nabla_{w^{\\prime}}\\frac{\\delta\\left\\langle s,\\nabla f_{\\eta}(w)\\right\\rangle_{w}}{\\delta\\eta(\\mathrm{d}w^{\\prime})}\\right\\|_{w^{\\prime}}\\le H^{\\prime}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "for a constant $H^{\\prime}$ depending only on $\\lambda^{-1},L_{0},L_{1},B_{1},G(0)$ and ${\\widetilde{L}}_{2}$ . Indeed, starting from (D.3) and differentiating with respect to $w^{\\prime}$ in the direction $s^{\\prime}$ , and diffe rentiating with respect to $w$ in the direction $s$ , we get ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lambda\\left\\langle s^{\\prime},\\nabla_{w^{\\prime}}\\frac{\\delta\\left\\langle s,\\nabla f_{\\eta}(w)\\right\\rangle_{w}}{\\delta\\eta(w^{\\prime})}\\right\\rangle_{w^{\\prime}}+\\int\\nabla_{w}G^{\\prime\\prime}[f_{\\eta}\\eta](w,w^{\\prime\\prime})\\eta(\\mathrm{d}w^{\\prime\\prime})\\left\\langle s^{\\prime},\\nabla_{w^{\\prime}}\\frac{\\delta f_{\\eta}(w^{\\prime\\prime})}{\\delta\\eta(w^{\\prime})}\\right\\rangle_{w^{\\prime}}}&{}&\\\\ {=-\\left\\langle s,\\nabla_{w}\\left\\{\\langle s^{\\prime},\\nabla_{w^{\\prime}}\\left[G^{\\prime\\prime}[f_{\\eta}\\eta](w,w^{\\prime})f_{\\eta}(w^{\\prime})\\right]\\rangle_{w^{\\prime}}\\right\\}\\right\\rangle_{w}}&{}&\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and so ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\quad}&{\\mathbb{\\Vert\\nabla_{w^{\\prime}}}\\frac{\\delta\\left\\langle s,\\nabla f_{\\eta}\\left(w\\right)\\right\\rangle_{w}}{\\delta\\eta(\\mathrm{d}w^{\\prime})}\\bigg\\Vert_{w^{\\prime}}\\leq\\Vert\\nabla_{w}\\nabla_{w^{\\prime}}G^{\\prime\\prime}[f_{\\eta}\\eta]\\Vert\\cdot|f_{\\eta}(w^{\\prime})|+\\Vert\\nabla_{w}G^{\\prime\\prime}[f_{\\eta}\\eta]\\Vert_{w}\\cdot\\Vert\\nabla f_{\\eta}(w^{\\prime})\\Vert_{w^{\\prime}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\underset{w^{\\prime\\prime}\\in W}{\\operatorname*{sup}}\\Vert\\nabla_{w}G^{\\prime\\prime}[f_{\\eta}\\eta](w,w^{\\prime\\prime})\\Vert_{w}\\cdot\\underset{w^{\\prime\\prime}\\in W}{\\operatorname*{sup}}\\left\\Vert\\nabla_{w^{\\prime}}\\frac{\\delta f_{\\eta}\\left(w^{\\prime\\prime}\\right)}{\\delta\\eta(\\mathrm{d}w^{\\prime})}\\right\\Vert_{w^{\\prime}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\tilde{L}_{2}\\cdot\\frac{1}{\\lambda}\\sqrt{2L_{0}G(0)}+L_{1}\\cdot\\left(\\frac{L_{1}}{\\lambda^{2}}\\sqrt{2L_{0}G(0)}+\\frac{B_{1}}{\\lambda}\\right)+L_{1}\\cdot H=:H^{\\prime}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "by Assumption 1. ", "page_idx": 27}, {"type": "text", "text": "Now fix $w\\in\\mathscr{W}$ . By Lem. D.8 below applied to $F(\\eta)=f_{\\eta}(w)$ , we have that ", "page_idx": 27}, {"type": "equation", "text": "$$\n|f_{\\eta}(w)-f_{\\eta^{\\prime}}(w)|\\leq\\operatorname*{sup}_{\\eta^{\\prime\\prime}\\in\\mathcal{P}(\\mathcal{W})}\\operatorname*{sup}_{w^{\\prime}\\in\\mathcal{W}}\\left\\|\\nabla_{w^{\\prime}}\\frac{\\delta f_{\\eta^{\\prime\\prime}}(w)}{\\delta\\eta^{\\prime\\prime}(\\mathrm{d}w^{\\prime})}\\right\\|_{w^{\\prime}}W_{2}(\\eta,\\eta^{\\prime})\\leq H W_{2}(\\eta,\\eta^{\\prime}).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Likewise, fix any $w\\,\\in\\,\\mathcal{W}$ and let $\\begin{array}{r}{s\\,=\\,\\frac{\\nabla f_{\\eta^{\\prime}}(w)-\\nabla f_{\\eta}(w)}{\\left\\|\\nabla f_{\\eta^{\\prime}}(w)-\\nabla f_{\\eta}(w)\\right\\|_{w}}\\,\\in\\,T_{w}\\mathcal{W}}\\end{array}$ . Then by Lem. D.8 below applied to $F(\\eta)=\\langle s,\\nabla f_{\\eta}(w)\\rangle_{w}$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla f_{\\eta^{\\prime}}(w)-\\nabla f_{\\eta}(w)\\|=\\langle s,\\nabla f_{\\eta^{\\prime}}(w)\\rangle_{w}-\\langle s,\\nabla f_{\\eta}(w)\\rangle_{w}\\leq H^{\\prime}W_{2}(\\eta,\\eta^{\\prime}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Lemma D.8. Let $\\mathcal{W}$ a compact Riemannian manifold and $F:\\mathcal{P}(\\mathcal{W})\\rightarrow\\mathbb{R}$ such that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\forall\\eta\\in\\mathcal{P}(\\mathcal{W}),\\forall w\\in\\mathcal{W},\\ \\|\\nabla F^{\\prime}[\\eta](w)\\|_{w}\\leq B.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Then ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\forall\\eta,\\eta^{\\prime}\\in\\mathcal{P}(\\mathcal{W}),\\ |F(\\eta)-F(\\eta^{\\prime})|\\leq B W_{1}(\\eta,\\eta^{\\prime})\\leq B W_{2}(\\eta,\\eta^{\\prime}).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. For any $x,y\\in\\mathcal{W}$ , pose $(\\Sigma_{\\theta}(x,y))_{\\theta\\in[0,1]}$ the constant-speed length-minimizing geodesic in $\\mathcal{W}$ interpolating between $x$ and $y$ . Also pose $\\begin{array}{r}{\\dot{\\Sigma_{\\theta}^{\\prime}}(x,y)=\\frac{d}{d\\theta}\\Sigma_{\\theta}(x,y)\\in T_{\\Sigma_{\\theta}(x,y)}\\mathcal{W}}\\end{array}$ for any $\\theta$ . For example if $\\mathcal{W}=\\mathbb{R}^{d}$ , $\\Sigma_{\\theta}(x,y)=x+\\theta(y-x)$ and $\\Sigma_{\\theta}^{\\prime}(x,y)=y-x$ for all $\\theta$ . ", "page_idx": 28}, {"type": "text", "text": "Let $\\gamma$ the optimal coupling between $\\eta,\\eta^{\\prime}$ in the $W_{1}$ sense, and for all $\\theta\\in[0,1]$ , $\\eta_{\\theta}=(\\Sigma_{\\theta})_{\\sharp}\\gamma$ the pushforward measure of $\\gamma$ by $\\Sigma_{\\theta}$ . Note that for any $\\theta\\in[0,1]$ , ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\frac{d}{d\\theta}F(\\eta_{\\theta})=\\int_{\\mathcal{W}}F^{\\prime}[\\eta_{\\theta}]\\mathrm{d}\\left(\\partial_{\\theta}\\eta_{\\theta}\\right)\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "and that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\forall\\varphi:W\\rightarrow\\mathbb{R},\\ \\frac{d}{d\\theta}\\int_{w}\\varphi\\mathrm{d}\\eta_{\\theta}=\\frac{d}{d\\theta}\\iint_{w\\times w}\\varphi(\\Sigma_{\\theta}(x,y))\\mathrm{d}\\gamma(x,y)}}\\\\ &{=\\iint_{w\\times w}\\frac{d}{d\\theta}\\varphi(\\Sigma_{\\theta}(x,y))\\mathrm{d}\\gamma(x,y)}\\\\ &{=\\iint_{w\\times w}\\langle\\Sigma_{\\theta}^{\\prime}(x,y),\\nabla\\varphi(\\Sigma_{\\theta}(x,y))\\rangle_{\\Sigma_{\\theta}(x,y)}\\,\\mathrm{d}\\gamma(x,y).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "(The interchange of $\\frac{d}{d\\theta}$ and $\\int\\!\\!\\int_{\\mathcal{W}\\times\\mathcal{W}}$ on the second line can be justified by the dominated convergence theorem assuming that $\\varphi$ has bounded $\\mathcal{C}^{1}$ norm, which is the case of $F^{\\prime}[\\eta_{\\theta}]$ by assumption.) So by Cauchy-Schwarz inequality, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{d}{d\\theta}F(\\eta_{\\theta})=\\iint_{\\mathcal W\\times\\mathcal W}\\langle\\Sigma_{\\theta}^{\\prime}(x,y),\\nabla F^{\\prime}[\\eta_{\\theta}](\\Sigma_{\\theta}(x,y))\\rangle_{\\Sigma_{\\theta}(x,y)}\\,{\\mathrm d}\\gamma(x,y)}\\\\ &{\\displaystyle\\Big|\\frac{d}{d\\theta}F(\\eta_{\\theta})\\Big|\\le\\iint_{\\mathcal W\\times\\mathcal W}\\|\\Sigma_{\\theta}^{\\prime}(x,y)\\|_{\\Sigma_{\\theta}(x,y)}\\cdot\\|\\nabla F^{\\prime}[\\eta_{\\theta}](\\Sigma_{\\theta}(x,y))\\|_{\\Sigma_{\\theta}(x,y)}\\,{\\mathrm d}\\gamma(x,y)}\\\\ &{\\displaystyle\\qquad\\qquad\\le\\operatorname*{sup}_{w\\in\\mathcal W}\\operatorname*{sup}_{\\eta^{\\prime}\\in\\mathcal P(\\mathcal W)}\\|\\nabla F^{\\prime}[\\eta](w)\\|_{w}\\cdot\\iint_{\\mathcal W\\times\\mathcal W}\\|\\Sigma_{\\theta}^{\\prime}(x,y)\\|_{\\Sigma_{\\theta}(x,y)}\\,{\\mathrm d}\\gamma(x,y)}\\\\ &{\\displaystyle\\qquad\\qquad\\le B\\cdot\\iint_{\\mathcal W\\times\\mathcal W}\\mathrm{dist}(x,y){\\mathrm d}\\gamma(x,y)=B W_{1}(\\eta,\\eta^{\\prime})}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "by definition of the geodesic $(\\Sigma_{\\theta}(x,y))_{\\theta\\in[0,1]}$ and by definition of the optimal coupling $\\gamma$ . Finally, ", "page_idx": 28}, {"type": "equation", "text": "$$\n|F(\\eta)-F(\\eta^{\\prime})|=\\left|\\int_{0}^{1}\\frac{d}{d\\theta}F(\\eta_{\\theta})\\;\\mathrm{d}\\theta\\right|\\leq\\operatorname*{sup}_{\\theta\\in[0,1]}\\left|\\frac{d}{d\\theta}F(\\eta_{\\theta})\\right|\\leq B W_{1}(\\eta,\\eta^{\\prime}).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof of the Proposition. ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Proof of Prop. 3.4. We first check (P0). The fact that $J_{\\lambda}$ is convex is given by Prop. 3.3. Moreover, let any $\\beta>0$ and let us check that $J_{\\lambda,\\beta}:=J_{\\lambda}+\\beta^{-1}H\\left(\\cdot|\\tau\\right)$ has a minimizer. Indeed, $J_{\\lambda,\\beta}$ is weakly continuous as shown in Lem. D.5, and non-negative so lower-bounded. Since $\\mathcal{W}$ is compact then any set of probability measures on $\\mathcal{W}$ is tight, i.e., any sequence in $\\mathcal{P}(\\mathcal{W})$ has a weakly convergent subsequence. So we conclude by the direct method of calculus of variations: let a sequence $\\bar{(\\eta_{n})_{n}}$ such that $J_{\\lambda,\\beta}(\\eta_{n})\\rightarrow\\operatorname*{inf}_{\\mathcal{P}(\\mathcal{W})}\\bar{J}_{\\lambda,\\beta}$ and extract a weakly convergent subsequence with limit $\\eta_{\\infty}$ ; then by weak continuity $\\eta_{\\infty}$ is a minimizer of $J_{\\lambda,\\beta}$ . ", "page_idx": 28}, {"type": "text", "text": "We now show that $J_{\\lambda}$ satisfies (P1). Recall from (D.2) that $\\begin{array}{r}{J_{\\lambda}^{\\prime}[\\eta](w)=-\\frac{\\lambda}{2}\\left|f_{\\eta}\\right|^{2}(w)}\\end{array}$ with $f_{\\eta}=$ $-{\\textstyle{\\frac{1}{\\lambda}}}G^{\\prime}[f_{\\eta}\\eta]$ over $\\mathcal{W}$ . Let us show the first condition for (P1): ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\forall\\eta\\in\\mathcal{P}_{2}(\\mathcal{W}),\\,\\forall w\\in\\mathcal{W},\\,\\,\\operatorname*{max}_{s\\in T_{w}\\mathcal{W}}\\,\\,\\left|\\nabla^{2}\\,J_{\\lambda}^{\\prime}[\\eta](s,s)\\right|\\leq\\Lambda\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "for some $\\Lambda<\\infty$ , where $\\nabla^{2}$ denotes the Riemannian Hessian. We have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\nabla J_{\\lambda}^{\\prime}[\\eta](w)=-\\lambda f_{\\eta}(w)\\nabla f_{\\eta}(w)}\\\\ &{\\quad\\nabla^{2}\\,J_{\\lambda}^{\\prime}[\\eta](w)=-\\lambda f_{\\eta}(w)\\nabla^{2}f_{\\eta}(w)-\\lambda\\nabla f_{\\eta}(w)\\nabla^{\\top}f_{\\eta}(w)}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "and so, for all $s\\in T_{w}\\mathcal{W}$ such that $\\|s\\|_{w}\\leq1$ , ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\left|\\nabla^{2}\\,J_{\\lambda}^{\\prime}[\\eta](s,s)\\right|\\leq\\lambda\\,|f_{\\eta}|\\,\\big\\|\\nabla^{2}f_{\\eta}\\big\\|+\\lambda\\,\\|\\nabla f_{\\eta}\\|^{2}}\\\\ {\\qquad\\qquad\\qquad\\leq\\sqrt{2L_{0}G(0)}\\left(\\cfrac{L_{2}}{\\lambda^{2}}\\sqrt{2L_{0}G(0)}+\\cfrac{B_{2}}{\\lambda}\\right)+\\lambda\\left(\\cfrac{L_{1}}{\\lambda^{2}}\\sqrt{2L_{0}G(0)}+\\cfrac{B_{1}}{\\lambda}\\right)^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "by Lem. D.4. ", "page_idx": 29}, {"type": "text", "text": "Let us now check the second condition for (P1), namely that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\forall w\\in\\mathcal{W},\\;\\forall\\eta,\\eta^{\\prime}\\in\\mathcal{P}_{2}(\\mathcal{W}),\\;\\;\\|\\nabla J_{\\lambda}^{\\prime}[\\eta]-\\nabla J_{\\lambda}^{\\prime}[\\eta^{\\prime}]\\|_{w}\\leq\\Lambda\\;W_{2}(\\eta,\\eta^{\\prime})\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "for some $\\Lambda<\\infty$ . Indeed, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla J_{\\lambda}^{\\prime}[\\eta]-\\nabla J_{\\lambda}^{\\prime}[\\eta^{\\prime}]\\|_{w}}\\\\ &{\\quad=\\lambda\\,\\|f_{\\eta}\\nabla f_{\\eta}-f_{\\eta^{\\prime}}\\nabla f_{\\eta^{\\prime}}\\|\\leq\\lambda\\,(\\|f_{\\eta}(\\nabla f_{\\eta}-\\nabla f_{\\eta^{\\prime}})\\|+\\|(f_{\\eta}-f_{\\eta^{\\prime}})\\nabla f_{\\eta^{\\prime}}\\|)}\\\\ &{\\quad\\leq\\lambda\\,\\bigg(\\underset{\\eta^{\\prime\\prime}}{\\operatorname*{sup}}\\,\\underset{w}{\\operatorname*{sup}}\\,\\|f_{\\eta^{\\prime\\prime}}\\cdot\\underset{w}{\\operatorname*{sup}}\\,\\|\\nabla f_{\\eta}-\\nabla f_{\\eta^{\\prime}}\\|+\\underset{\\eta^{\\prime\\prime}}{\\operatorname*{sup}}\\,\\underset{w}{\\operatorname*{sup}}\\,\\|\\nabla f_{\\eta^{\\prime\\prime}}\\|\\cdot\\underset{w}{\\operatorname*{sup}}\\,\\|f_{\\eta}-f_{\\eta^{\\prime}}\\|\\bigg)}\\\\ &{\\quad\\leq\\lambda\\,\\bigg(\\frac{1}{\\lambda}\\sqrt{2L_{0}G(0)}\\cdot H^{\\prime}W_{2}(\\eta,\\eta^{\\prime})+\\bigg(\\frac{L_{1}}{\\lambda^{2}}\\sqrt{2L_{0}G(0)}+\\displaystyle\\frac{B_{1}}{\\lambda}\\bigg)\\cdot H W_{2}(\\eta,\\eta^{\\prime})\\bigg)=:\\Lambda W_{2}(\\eta,\\eta^{\\prime})}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "by Lem. D.4 and Lem. D.7. ", "page_idx": 29}, {"type": "text", "text": "We now turn to the proof of (P2) with the quantitative bound on the local LSI constant. Let $\\eta\\in\\mathcal{P}(\\mathcal{W})$ . By the first part of Lem. D.4, we directly have that ", "page_idx": 29}, {"type": "equation", "text": "$$\n|J_{\\lambda}^{\\prime}[\\eta](w)|=\\frac{\\lambda}{2}\\left|f_{\\eta}\\right|^{2}(w)\\le\\frac{L_{0}}{\\lambda}J_{\\lambda}(\\eta).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "In particular, by the Holley-Stroock bounded perturbation argument [HS86], the proximal Gibbs measure $\\hat{\\eta}:=e^{-\\beta J_{\\lambda}^{\\prime}[\\eta]}\\tau/Z$ satisfies LSI with constant $\\begin{array}{r}{\\alpha_{\\hat{\\eta}}=\\alpha_{\\tau}\\exp\\left(-\\frac1\\lambda L_{0}\\beta J_{\\lambda}(\\eta)\\right)}\\end{array}$ . ", "page_idx": 29}, {"type": "text", "text": "Finally, we turn to the proof of the bound on the uniform LSI constant along the MFLD trajectory $(\\eta_{t})_{t\\geq0}$ . Given the bound on the local LSI constants, it suffices to show that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\forall\\eta\\in\\mathcal{P}(\\mathcal{W}),\\;J_{\\lambda}(\\eta)\\leq G(0)\\quad\\mathrm{and}\\quad\\forall t\\geq0,\\;J_{\\lambda}(\\eta_{t})\\leq J_{\\lambda}(\\eta_{0})+\\beta^{-1}H\\left(\\eta_{0}|\\tau\\right).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "The first bound was shown in Lem. D.4. For the second bound, note that $J_{\\lambda}(\\eta_{t})+\\beta^{-1}H\\left(\\eta_{t}\\vert\\tau\\right)$ decreases with $t$ , since MFLD is precisely the Wasserstein gradient flow for $\\eta\\mapsto J_{\\lambda}(\\eta)+\\beta^{-1}H(\\eta)$ and $H(\\eta)$ and $H\\left(\\eta|\\tau\\right)$ differ by a constant. So, since relative entropy is non-negative, ", "page_idx": 29}, {"type": "equation", "text": "$$\nJ_{\\lambda}(\\eta_{t})\\le J_{\\lambda}(\\eta_{t})+\\beta^{-1}H\\left(\\eta_{t}|\\tau\\right)\\le J_{\\lambda}(\\eta_{0})+\\beta^{-1}H\\left(\\eta_{0}|\\tau\\right)\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "for all $t\\geq0$ , as desired. ", "page_idx": 29}, {"type": "text", "text": "E Details for Sec. 4 (global convergence by annealing) ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "The following preliminary lemma allows to control the effect of entropic regularization, using a box-kernel smoothing technique similar to [Chi22a]. ", "page_idx": 29}, {"type": "text", "text": "Lemma E.1. Let $\\mathcal{W}$ a $d$ -dimensional compact Riemannian manifold and denote by $\\tau$ the uniform probability measure over $\\mathcal{W}$ . Let $\\mathcal{J}:\\mathcal{P}(\\mathcal{W})\\to\\mathbb{R}$ and $\\eta^{*}\\in\\mathcal{P}(\\mathcal{W})$ , and suppose that there exist constants $A,B>0$ such that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\forall\\eta\\;\\,s.t.\\;\\;W_{1}(\\eta,\\eta^{*})\\leq A,\\quad\\mathcal{I}(\\eta)-\\mathcal{I}(\\eta^{*})\\leq B W_{\\infty}(\\eta,\\eta^{*}).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Denote $\\mathcal{J}_{\\beta}=\\mathcal{J}+\\beta^{-1}H\\left(\\cdot|\\tau\\right)\\!,$ , for any $\\beta>0$ . Then ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\substack{\\eta:W_{1}(\\eta,\\eta^{*})\\le A}}\\mathcal{I}_{\\beta}(\\eta)\\ \\le\\ \\mathcal{I}(\\eta^{*})+\\operatorname*{inf}_{\\substack{0<\\epsilon\\le\\operatorname*{min}\\{1,A\\}}}\\left[B\\epsilon+\\frac{d}{\\beta}\\log\\left(\\frac{1}{\\epsilon}\\right)+\\frac{\\log C}{\\beta}\\right]\n$$", "text_format": "latex", "page_idx": 29}, {"type": "equation", "text": "$C:=\\big[\\operatorname*{inf}_{w\\in\\mathcal{W}}\\operatorname*{inf}_{0<\\epsilon\\leq1}\\;\\epsilon^{-d}\\cdot\\tau\\left(\\{w^{\\prime};\\mathrm{dist}_{\\mathcal{W}}(w,w^{\\prime})\\leq\\epsilon\\}\\right)\\big]^{-1}.$ ", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Proof. The proof is adapted from [Chi22a]. It is based on constructing an $\\epsilon$ -smoothed version of $\\eta^{*}$ , i.e. a measure $\\eta_{\\epsilon}$ which admits a density w.r.t. $\\tau$ while being close to $\\eta^{*}$ in an appropriate sense. ", "page_idx": 30}, {"type": "text", "text": "Let any $0\\,<\\,\\epsilon\\,\\leq\\,\\operatorname*{min}\\{1,A\\}$ . Given $w\\,\\in\\,\\mathcal{W}$ , define the probability measure $\\gamma_{\\epsilon,w}(\\mathrm{d}w^{\\prime})$ as the uwnoirfdosr, $\\begin{array}{r}{\\frac{\\mathrm{d}\\gamma_{\\epsilon,w}}{\\mathrm{d}\\tau}(w^{\\prime}):=\\frac{\\mathbb{1}(w^{\\prime}\\in B_{\\epsilon}(w))}{\\tau(B_{\\epsilon}(w))}}\\end{array}$ .  tTheh egne, oldete $\\gamma_{\\epsilon}(\\mathrm{d}w,\\mathrm{d}w^{\\prime})=\\eta^{*}(\\mathrm{d}w)\\gamma_{\\epsilon,w}(\\mathrm{d}w^{\\prime})\\in\\mathcal{P}(\\mathcal{W}\\times\\mathcal{W})$ $B_{\\epsilon}(w):=\\{w\\in\\mathcal{W};\\mathrm{dist}(w,w^{\\prime})\\leq\\epsilon\\}$ ,o tahnedr let $\\begin{array}{r}{\\eta_{\\epsilon}(\\mathrm{d}w^{\\prime})=\\int_{w\\in\\mathcal{W}}\\gamma_{\\epsilon}(\\mathrm{d}w,\\mathrm{d}w^{\\prime})}\\end{array}$ its second marginal.   \nOne can then verify that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}\\eta_{\\epsilon}}{\\mathrm{d}\\tau}(w^{\\prime})=\\int_{w\\in\\mathcal{W}}\\frac{\\mathrm{d}\\gamma_{\\epsilon,w}}{\\mathrm{d}\\tau}(w^{\\prime})\\eta^{*}(\\mathrm{d}w)=\\int_{w\\in\\mathcal{W}}\\frac{\\mathbb{1}(w^{\\prime}\\in B_{\\epsilon}(w))}{\\tau(B_{\\epsilon}(w))}\\eta^{*}(\\mathrm{d}w).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Moreover there exists a positive constant $C$ such that $\\tau(B_{\\epsilon}(w))\\,\\geq\\,C^{-1}\\epsilon^{d}$ for all $\\epsilon\\leq1$ [GV79, Theorem 3.3]. As a consequence, ", "page_idx": 30}, {"type": "equation", "text": "$$\nH\\left(\\eta_{\\epsilon}|\\tau\\right)=\\int\\mathrm{d}\\eta_{\\epsilon}(w^{\\prime})\\log\\frac{\\mathrm{d}\\eta_{\\epsilon}}{\\mathrm{d}\\tau}(w^{\\prime})\\leq\\operatorname*{sup}_{w\\in\\mathcal{W}}-\\log\\tau(B_{\\varepsilon}(w))\\leq d\\log(1/\\epsilon)+\\log C.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Furthermore, by definition of the coupling $\\gamma_{\\epsilon}$ , we have $W_{1}(\\eta_{\\epsilon},\\eta^{*})\\;\\leq\\;W_{\\infty}(\\eta_{\\epsilon},\\eta^{*})\\;\\leq\\;\\epsilon\\;\\leq\\;A$ . Therefore, by assumption $\\mathcal{I}(\\eta_{\\epsilon})-\\mathcal{I}(\\eta^{*})\\leq B W_{\\infty}(\\eta_{\\epsilon},\\eta^{*})\\leq B\\epsilon$ , and so ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\eta:W_{1}\\left(\\eta,\\eta^{*}\\right)\\leq A}{\\operatorname*{min}}\\mathcal{I}_{\\beta}(\\eta)\\leq\\mathcal{I}_{\\beta}(\\eta_{\\epsilon})=\\mathcal{I}(\\eta_{\\epsilon})+\\beta^{-1}H\\left(\\eta_{\\epsilon}|\\tau\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\mathcal{I}(\\eta^{*})+B\\epsilon+\\beta^{-1}\\left(d\\log(1/\\epsilon)+\\log C\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "and the inequality of the lemma follows by taking the infimum over $\\epsilon$ . ", "page_idx": 30}, {"type": "text", "text": "E.1 Proof of Prop. 4.1 ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "We state and prove a more precise version of Prop. 4.1 below. ", "page_idx": 30}, {"type": "text", "text": "Proposition E.2. Under Assumption 1, let $\\Delta\\ >\\ 0$ and assume that $\\begin{array}{r}{\\Delta\\ \\leq\\ \\frac{2L_{0}L_{1}G(0)}{\\lambda^{2}J_{\\lambda}^{*}}}\\end{array}$ Then MFLD-Bilevel with the temperature schedule \u2200t, \u03b2t = \u22064Jd\u2217 $\\begin{array}{r}{\\forall t,\\beta_{t}=\\frac{4d}{\\Delta J_{\\lambda}^{*}}\\log\\left(\\frac{4C^{1/d}B}{\\Delta J_{\\lambda}^{*}}\\right)}\\end{array}$ converges to $(1+\\Delta)$ - multiplicative accuracy in time ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{~\\hat{~}A_{\\Delta}\\Delta\\gamma_{\\Delta}^{\\prime}\\log\\left(\\frac{4C^{1/d}B}{\\Delta J_{\\lambda}^{*}}\\right)}\\cdot\\exp\\left(\\frac{4d L_{0}G(0)}{\\lambda\\Delta J_{\\lambda}^{*}}\\log\\left(\\frac{4C^{1/d}B}{\\Delta J_{\\lambda}^{*}}\\right)\\right)\\cdot\\log\\left(\\frac{2J_{\\lambda}(\\eta_{0})}{\\Delta J_{\\lambda}^{*}}+\\frac{H\\left(\\eta_{0}|\\tau\\right)}{2\\log C}\\right)}\\\\ &{{\\mathrm{~}^{\\mathrm{there}}}\\thinspace C=\\operatorname*{max}\\Big\\{1,\\left[\\operatorname*{inf}_{w\\in\\mathcal{W}}\\operatorname*{inf}_{0<\\epsilon\\leq1}\\,\\epsilon^{-d}\\cdot\\tau\\left(\\{w^{\\prime};\\mathrm{dist}_{\\mathcal{W}}(w,w^{\\prime})\\leq\\epsilon\\}\\right)\\right]^{-1}\\Big\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Proof of Prop. E.2. Let $(\\eta)_{t}$ the MFLD-Bilevel trajectory with constant inverse temperature parameter $\\beta$ to be chosen. Denote $J_{\\lambda,\\beta}=J_{\\lambda}+\\beta^{-1}H\\left(\\cdot\\vert\\bar{\\tau}\\right)$ . Recall that by Prop. 3.4, $J_{\\lambda,\\beta}$ satisfies $\\alpha_{\\beta}$ -LSI uniformly along the MFLD trajectory with $\\begin{array}{r}{\\alpha_{\\beta}=\\alpha_{\\tau}\\exp\\left(-\\frac{1}{\\lambda}L_{0}\\beta G(0)\\right)}\\end{array}$ . So by Thm. 2.1, for all $t$ , ", "page_idx": 30}, {"type": "equation", "text": "$$\n{I}_{\\lambda}(\\eta_{t})\\le J_{\\lambda,\\beta}(\\eta_{t})\\le\\operatorname*{inf}J_{\\lambda,\\beta}+e^{-2\\beta^{-1}\\alpha_{\\beta}t}\\,(J_{\\lambda,\\beta}(\\eta_{0})-\\operatorname*{inf}J_{\\lambda,\\beta})\\le\\operatorname*{inf}J_{\\lambda,\\beta}+e^{-2\\beta^{-1}\\alpha_{\\beta}t}J_{\\lambda,\\beta}(\\eta_{0}),\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where in the first inequality we used that $J_{\\lambda,\\beta}-J_{\\lambda}=\\beta^{-1}H\\left(\\cdot|\\tau\\right)\\geq0.$ . ", "page_idx": 30}, {"type": "text", "text": "Furthermore, by applying Lem. E.1 to $\\mathcal{I}=J_{\\lambda}$ , $\\eta^{*}=\\arg\\operatorname*{min}J_{\\lambda}$ , $A=\\infty$ and $B=\\sqrt{2L_{0}G(0)}$ \u00b7 $\\begin{array}{r}{\\left(\\frac{L_{1}}{\\lambda^{2}}\\sqrt{2L_{0}G(0)}+\\frac{B_{1}}{\\lambda}\\right)}\\end{array}$ the constant from Lem. D.5, we find that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\operatorname*{inf}J_{\\lambda,\\beta}\\leq\\operatorname*{inf}J_{\\lambda}+\\operatorname*{inf}_{0<\\epsilon\\leq1}\\left[B\\epsilon+\\frac{d}{\\beta}\\log\\frac{1}{\\epsilon}+\\frac{\\log C}{\\beta}\\right].\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Taking $\\begin{array}{r}{\\beta=\\frac{d}{B}s}\\end{array}$ for some $s\\geq1$ to be chosen, and evaluating at the infimum at $\\begin{array}{r}{\\epsilon=\\frac{d}{\\beta B}}\\end{array}$ \u03b2B , we get ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\operatorname*{inf}J_{\\lambda,\\beta}\\leq J_{\\lambda}^{*}+\\frac{d+\\log C^{\\prime}}{\\beta}-\\frac{d}{\\beta}\\log\\left(\\frac{d}{\\beta B}\\right).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $C^{\\prime}=\\operatorname*{max}\\{1,C\\}$ . So in order to guarantee that $J_{\\lambda}(\\eta_{t})\\leq(1+\\Delta)J_{\\lambda}^{*}$ , it suffices to take $t$ such that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{J_{\\boldsymbol{\\lambda}}^{*}+\\frac{d+\\log C^{\\prime}}{\\beta}-\\frac{d}{\\beta}\\log\\left(\\frac{d}{\\beta B}\\right)+e^{-2\\beta^{-1}\\alpha_{\\beta}t}\\left(J_{\\boldsymbol{\\lambda}}(\\eta_{0})+\\beta^{-1}H\\left(\\eta_{0}|\\tau\\right)\\right)\\leq(1+\\Delta)J_{\\boldsymbol{\\lambda}}^{*}}\\\\ &{\\qquad\\qquad\\mathrm{i.e.}\\quad t\\geq\\frac{\\beta}{2\\alpha_{\\beta}}\\log\\left(\\frac{J_{\\boldsymbol{\\lambda}}\\left(\\eta_{0}\\right)+\\beta^{-1}H\\left(\\eta_{0}|\\tau\\right)}{\\Delta J_{\\boldsymbol{\\lambda}}^{*}-\\left(\\frac{d+\\log C^{\\prime}}{\\beta}-\\frac{d}{\\beta}\\log\\left(\\frac{d}{\\beta B}\\right)\\right)}\\right)=:T_{s},}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "assuming that $\\Delta$ is large enough so that the above expression is well-defined. More explicitly, substituting the value of $\\alpha_{\\beta}$ and of $\\begin{array}{r}{\\beta=\\frac{d}{B}s}\\end{array}$ , we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle T_{s}=\\frac{\\beta}{2\\alpha_{\\tau}}\\cdot\\exp\\left(\\frac{1}{\\lambda}L_{0}\\beta G(0)\\right)\\cdot\\log\\left(\\frac{J_{\\lambda}(\\eta_{0})+\\beta^{-1}H\\left(\\eta_{0}|\\tau\\right)}{\\Delta J_{\\lambda}^{*}-\\left(\\frac{d+\\log C^{\\prime}}{\\beta}-\\frac{d}{\\beta}\\log\\left(\\frac{d}{\\beta B}\\right)\\right)}\\right)}\\\\ {\\displaystyle\\quad=\\frac{s d/B}{2\\alpha_{\\tau}}\\cdot\\exp\\left(s\\frac{1}{\\lambda B}L_{0}d G(0)\\right)\\cdot\\log\\left(\\frac{J_{\\lambda}(\\eta_{0})+\\frac{B}{s d}H\\left(\\eta_{0}|\\tau\\right)}{\\Delta J_{\\lambda}^{*}-\\frac{B}{s}\\left(1+d^{-1}\\log C^{\\prime}+\\log s\\right)}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Noting that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\log\\frac{s\\Delta J_{\\lambda}^{*}}{4B}=\\log s-\\log\\frac{4B}{\\Delta J_{\\lambda}^{*}}\\leq\\frac{s\\Delta J_{\\lambda}^{*}}{4B}-1}&{}\\\\ {\\mathrm{so}}&{\\frac{B}{s}\\left(1+d^{-1}\\log C^{\\prime}+\\log s\\right)\\leq\\frac{B}{s}\\left(d^{-1}\\log C^{\\prime}+\\log\\frac{4B}{\\Delta J_{\\lambda}^{*}}+\\frac{s\\Delta J_{\\lambda}^{*}}{4B}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\frac{B}{s}\\left(d^{-1}\\log C^{\\prime}+\\log\\frac{4B}{\\Delta J_{\\lambda}^{*}}\\right)+\\frac{\\Delta J_{\\lambda}^{*}}{4},}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "choose henceforth $\\begin{array}{r}{s=\\operatorname*{max}\\left\\{1,\\frac{4B}{\\Delta J_{\\lambda}^{*}}\\left(d^{-1}\\log C^{\\prime}+\\log\\frac{4B}{\\Delta J_{\\lambda}^{*}}\\right)\\right\\}}\\end{array}$ , so that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\Delta J_{\\lambda}^{*}-\\frac{B}{s}\\left(1+d^{-1}\\log C^{\\prime}+\\log s\\right)\\geq\\frac{\\Delta J_{\\lambda}^{*}}{2}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "To simplify the final statement, we make the assumption that $\\Delta$ is small enough so that $1\\ \\leq$ $\\begin{array}{r}{\\frac{4B}{\\Delta J_{\\lambda}^{*}}\\left(d^{-1}\\log C^{\\prime}+\\log\\frac{4B}{\\Delta J_{\\lambda}^{*}}\\right)}\\end{array}$ . More explicitly, since we were careful to choose $C^{\\prime}\\geq1$ , ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad1\\leq\\frac{4B}{\\Delta J_{\\boldsymbol{\\lambda}}^{*}}\\left(d^{-1}\\log C^{\\prime}+\\log\\frac{4B}{\\Delta J_{\\boldsymbol{\\lambda}}^{*}}\\right)\\iff\\frac{\\Delta J_{\\boldsymbol{\\lambda}}^{*}}{4B}+\\log\\frac{\\Delta J_{\\boldsymbol{\\lambda}}^{*}}{4B}\\leq d^{-1}\\log C^{\\prime}}\\\\ &{\\Longleftarrow\\frac{\\Delta J_{\\boldsymbol{\\lambda}}^{*}}{4B}\\leq1\\quad\\mathrm{and}\\quad\\log\\frac{\\Delta J_{\\boldsymbol{\\lambda}}^{*}}{4B}\\leq-1\\iff\\frac{\\Delta J_{\\boldsymbol{\\lambda}}^{*}}{4B}\\leq\\operatorname*{min}\\{1,e^{-1}\\}=e^{-1}}\\\\ &{\\Longleftrightarrow\\Delta\\leq\\frac{4B e^{-1}}{J_{\\boldsymbol{\\lambda}}^{*}}=\\frac{4e^{-1}}{J_{\\boldsymbol{\\lambda}}^{*}}\\cdot\\sqrt{2L_{0}G(0)}\\left(\\frac{L_{1}}{\\lambda^{2}}\\sqrt{2L_{0}G(0)}+\\frac{B_{1}}{\\lambda}\\right)}\\\\ &{\\Longleftarrow\\Delta\\leq\\frac{4e^{-1}}{J_{\\boldsymbol{\\lambda}}^{*}}\\cdot\\frac{2L_{0}L_{1}G(0)}{\\lambda^{2}}\\iff\\Delta\\leq\\frac{1}{J_{\\boldsymbol{\\lambda}}^{*}}\\cdot\\frac{2L_{0}L_{1}G(0)}{\\lambda^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Then $\\begin{array}{r}{s=\\frac{4B}{\\Delta J_{\\lambda}^{*}}\\left(d^{-1}\\log C^{\\prime}+\\log\\frac{4B}{\\Delta J_{\\lambda}^{*}}\\right),\\beta=\\frac{4d}{\\Delta J_{\\lambda}^{*}}\\left(d^{-1}\\log C^{\\prime}+\\log\\frac{4B}{\\Delta J_{\\lambda}^{*}}\\right)\\geq\\frac{4}{\\Delta J_{\\lambda}^{*}}\\log C^{\\prime},}\\end{array}$ , and ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\dot{\\overline{{\\mathbf{\\Xi}}}}_{s}\\leq\\frac{\\beta}{2\\alpha_{\\tau}}\\cdot\\exp\\left(\\frac{1}{\\lambda}L_{0}\\beta G(0)\\right)\\cdot\\log\\left(\\frac{J_{\\lambda}(\\eta_{0})+\\beta^{-1}H\\left(\\eta_{0}|\\tau\\right)}{\\Delta J_{\\lambda}^{*}/2}\\right)}\\\\ &{\\leq\\frac{2d}{\\alpha_{\\tau}\\Delta J_{\\lambda}^{*}}\\log\\left(\\frac{4C^{\\prime1/d}B}{\\Delta J_{\\lambda}^{*}}\\right)\\cdot\\exp\\left(\\frac{4d L_{0}G(0)}{\\lambda\\Delta J_{\\lambda}^{*}}\\log\\left(\\frac{4C^{\\prime1/d}B}{\\Delta J_{\\lambda}^{*}}\\right)\\right)\\cdot\\log\\left(\\frac{2J_{\\lambda}\\left(\\eta_{0}\\right)}{\\Delta J_{\\lambda}^{*}}+\\frac{H\\left(\\eta_{0}|\\tau\\right)}{2\\log C^{\\prime}}\\right)=:}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Hence the time-complexity upper bound of $\\overline{{T}}_{\\Delta}$ for reaching $(1+\\Delta)$ -multiplicative accuracy. \u53e3 ", "page_idx": 31}, {"type": "text", "text": "Require: Functional $\\mathcal{J}:\\mathcal{P}(\\mathcal{W})\\to\\mathbb{R}$ . Initialization $\\eta_{0}$ , $\\beta_{0}>0$ . Schedule $K$ , $(T_{k})_{k=0}^{K}$ . 1: $\\eta_{0}^{0}=\\eta_{0}$   \n2: for $k=0,\\ldots,K$ do   \n3: $\\beta_{k}=2^{k}\\beta_{0}$ ", "page_idx": 32}, {"type": "text", "text": "4: Run the MFLD with $\\beta_{k}$ initialized from $\\eta_{0}^{k}$ up to $T_{k}$ , ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\partial_{t}\\eta_{t}^{k}=\\mathrm{div}(\\eta_{t}^{k}\\nabla\\mathcal{J}^{\\prime}[\\eta_{t}^{k}])+\\frac{1}{\\beta_{k}}\\Delta\\eta_{t}^{k}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "5: $\\eta_{0_{-}}^{k+1}=\\eta_{T_{k}}^{k}$   \n6: end for   \n7: return $\\eta_{T_{K}}^{K}$ ", "page_idx": 32}, {"type": "text", "text": "E.2 General annealing procedure and its convergence guarantee ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "The following theorem builds upon and generalizes the idea of [SWON23, Sec. 4.1] to objective functionals $\\mathcal{I}$ that have a positive optimal value. It ensures fast convergence to a fixed multiplicative accuracy. ", "page_idx": 32}, {"type": "text", "text": "Theorem E.3. Let $\\mathcal{W}$ a $d_{\\cdot}$ -dimensional compact Riemannian manifold, so in particular the uniform measure $\\tau$ over $\\mathcal{W}$ satisfies $\\alpha_{\\tau}$ -LSI for some $\\alpha_{\\tau}~>~0$ . Let $\\mathcal{J}:\\mathcal{P}(\\mathcal{W})\\,\\to\\,\\mathbb{R}_{+}$ convex, suppose that $\\mathcal{J}^{*}:=\\operatorname*{min}\\mathcal{I}>0$ and that there exists a minimizer $\\eta^{*}$ . Suppose that there exist constants $\\kappa_{1},C_{L},A>0$ such that ", "page_idx": 32}, {"type": "text", "text": "$\\begin{array}{r}{F i x\\ 0\\ <\\ \\delta\\ \\leq\\ \\frac{C_{L}\\operatorname*{min}\\{1,A\\}}{\\mathcal{I}^{*}}}\\end{array}$ . Let $\\eta_{t}^{k}$ the iterates of the annealing procedure of Algorithm $^{\\,l}$ with initialization $\\beta_{0}=\\bar{d}$ and with the schedule $K=\\lceil\\log_{2}(1/(\\delta\\mathcal{T}^{*}))\\rceil$ and ", "page_idx": 32}, {"type": "equation", "text": "$$\nT_{k}=2^{k-1}d\\log\\left(2^{k}\\mathcal{T}_{\\beta_{0}}(\\eta_{0})\\right)\\cdot\\alpha_{\\tau}^{-1}\\exp\\left(2\\kappa_{1}d\\left(\\delta^{-1}+\\log\\left(\\frac{C_{L}C^{1/d}}{\\delta\\mathcal{T}^{*}}\\right)+2+\\frac{\\mathcal{I}_{\\beta_{0}}(\\eta_{0})}{2}\\right)\\right)\n$$", "text_format": "latex", "page_idx": 32}, {"type": "equation", "text": "$$\nC:=\\big[\\operatorname*{inf}_{w\\in\\mathcal{W}}\\operatorname*{inf}_{0<\\epsilon\\leq1}\\;\\epsilon^{-d}\\cdot\\tau\\left(\\{w^{\\prime};\\mathrm{dist}_{\\mathcal{W}}(w,w^{\\prime})\\leq\\epsilon\\}\\right)\\big]^{-1}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "equation", "text": "$$\n\\sum_{k=0}^{K}T_{k}\\leq\\frac{d}{\\delta\\mathcal{T}^{*}}\\log\\left(\\frac{J_{\\beta_{0}}(\\eta_{0})}{\\delta\\mathcal{T}^{*}}\\right)\\cdot\\alpha_{\\tau}^{-1}\\exp\\left(2\\kappa_{1}d\\left(\\delta^{-1}+\\log\\left(\\frac{C_{L}C^{1/d}}{\\delta\\mathcal{T}^{*}}\\right)+2+\\frac{\\mathcal{I}_{\\beta_{0}}(\\eta_{0})}{2}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Let us discuss the assumptions of Thm. E.3 and possible generalizations. ", "page_idx": 32}, {"type": "text", "text": "\u2022 Note that the condition 2. of the theorem holds as soon as $\\mathcal{J}^{\\prime}[\\eta]:\\mathcal{W}\\to\\mathbb{R}$ is $C_{L}$ -Lipschitz for all $\\eta\\in\\mathcal{P}(\\mathcal{W})$ , as shown in Lem. D.8, since $W_{1}\\le W_{2}\\le W_{\\infty}$ .   \n\u2022 The annealing procedure and its convergence guarantee can be generalized to a non-compact manifold $\\mathcal{W}$ by modifying MFLD to include a confining potential term, as discussed in Sec. A.2.   \n\u2022 Condition 1. of the theorem actually holds for any $\\mathcal{I}$ such that $\\operatorname*{sup}_{\\eta,w,w^{\\prime}}|\\mathcal{J}^{\\prime\\prime}[\\eta](w,w^{\\prime})|\\leq$ $L<\\infty$ and $\\mathcal{I}^{*}>0$ , with the constant $\\begin{array}{r}{\\kappa_{1}=\\sqrt{\\frac{2L}{\\mathcal{T}^{*}}}}\\end{array}$ . Indeed, one can then show similarly to Lem. D.3 that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\left\\lVert\\mathcal{I}^{\\prime}[\\eta]\\right\\rVert_{\\infty}^{2}\\le2L\\left(\\mathcal{I}(\\eta)-\\mathcal{I}^{*}\\right)\\le2L\\mathcal{I}(\\eta)\\le2L\\frac{\\mathcal{I}(\\eta)^{2}}{\\mathcal{I}^{*}}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "However, when plugging in $\\kappa_{1}=\\sqrt{2L/\\mathcal{T}^{*}}$ into the bounds of the theorem, one obtains a less favorable dependency of the total time-complexity in $\\mathcal{I}^{*}$ . In particular, note that the total time-complexity guaranteed by the theorem scales exponentially in $\\kappa_{1}$ and polynomially in $1/\\mathcal{I}^{*}$ . ", "page_idx": 32}, {"type": "text", "text": "\u2022 The way that the condition 1. of the theorem comes into the proof, is that it allows to guarantee a local LSI constant of $J+\\beta_{t}^{-1}H$ at $\\eta_{t}$ of $\\alpha_{\\hat{\\eta}_{t}}=\\mathrm{cst}\\cdot e^{-\\kappa_{1}\\beta_{t}\\mathcal{J}(\\eta_{t})}$ . One could similarly formulate an annealing procedure, and state convergence guarantees, tailored to objectives $\\mathcal{I}$ that satisfy different criteria for LSI, such as the Bakry-Emery curvaturedimension criterion. ", "page_idx": 33}, {"type": "text", "text": "The remainder of this subsection is dedicated to proving Thm. E.3. ", "page_idx": 33}, {"type": "text", "text": "By condition 1. of the theorem and the Holley-Stroock bounded perturbation argument, for any $t,k$ , the proximal Gibbs measure $\\widehat{\\eta_{t}^{k}}\\propto e^{-\\beta_{k}\\mathcal{I}^{\\prime}[\\eta_{t}^{k}]}\\tau$ satisfies LSI with the constant ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\alpha_{\\tau}\\exp\\left(-\\beta_{k}\\kappa_{1}\\mathcal{I}(\\eta_{t}^{k})\\right)\\geq\\operatorname*{inf}_{t^{\\prime}\\geq0}\\alpha_{\\tau}\\exp\\left(-\\beta_{k}\\kappa_{1}\\mathcal{I}(\\eta_{t^{\\prime}}^{k})\\right)=:\\alpha(k).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "That is, for any $k$ , ${\\mathcal{I}}_{\\beta_{k}}$ satisfies $\\alpha(k)$ -LSI at $\\eta_{t}^{k}$ for all $t\\geq0$ . (To see that $\\alpha(k)>0$ , note that for any $k,t,\\mathcal{I}(\\eta_{t}^{k})\\leq\\mathcal{I}_{\\beta_{k}}(\\eta_{t}^{k})\\leq\\mathcal{I}_{\\beta_{k}}(\\eta_{0}^{k})$ , since $H\\left(\\cdot|\\tau\\right)$ is non-negative and $(\\eta_{t}^{k})_{t}$ is a Wasserstein gradient flow of ${\\mathcal{I}}_{\\beta_{k}}$ , and so $\\begin{array}{r}{\\alpha(k)=\\operatorname*{inf}_{t\\ge0}\\alpha_{\\tau}\\exp\\left(-\\beta_{k}\\kappa_{1}\\mathcal{I}(\\eta_{t}^{k})\\right)\\ge\\alpha_{\\tau}\\exp\\left(-\\beta_{k}\\kappa_{1}\\mathcal{I}_{\\beta_{k}}(\\eta_{0}^{k})\\right)>0}\\end{array}$ ; but we will not make use of this rough bound in the sequel.) ", "page_idx": 33}, {"type": "text", "text": "Now let ", "page_idx": 33}, {"type": "equation", "text": "$$\nT_{k}=\\frac{\\beta_{k}}{2\\underline{{\\alpha}}(k)}\\log\\left(\\frac{\\beta_{k}}{d}\\overline{{c}}_{k}\\right)\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "for some $\\underline{{\\alpha}}(k)\\leq\\alpha(k)$ and $\\overline{{c}}_{k}\\ge\\mathcal{I}_{\\beta_{k}}(\\eta_{0}^{k})-\\operatorname*{min}\\mathcal{I}_{\\beta_{k}}$ to be chosen. Then by Thm. 2.1 applied to ${\\mathcal{I}}_{\\beta_{k}}$ , we obtain ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{I}_{\\beta_{k}}(\\eta_{T_{k}}^{k})\\leq\\operatorname*{min}\\mathcal{I}_{\\beta_{k}}+\\exp\\left(-2\\beta_{k}^{-1}\\alpha(k)T_{k}\\right)\\cdot\\left(\\mathcal{I}_{\\beta_{k}}(\\eta_{0}^{k})-\\operatorname*{min}\\mathcal{I}_{\\beta_{k}}\\right)}\\\\ &{\\qquad\\qquad\\leq\\operatorname*{min}\\mathcal{I}_{\\beta_{k}}+\\left[\\frac{\\beta_{k}}{d}\\left(\\mathcal{I}_{\\beta_{k}}(\\eta_{0}^{k})-\\operatorname*{min}\\mathcal{I}_{\\beta_{k}}\\right)\\right]^{-1}\\cdot\\left(\\mathcal{I}_{\\beta_{k}}(\\eta_{0}^{k})-\\operatorname*{min}\\mathcal{I}_{\\beta_{k}}\\right)}\\\\ &{\\qquad\\qquad=\\operatorname*{min}\\mathcal{I}_{\\beta_{k}}+\\frac{d}{\\beta_{k}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Further, by Lem. E.1, ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{I}_{\\beta_{k}}(\\eta_{T_{k}}^{k})\\leq\\mathcal{I}^{*}+\\operatorname*{inf}_{0<\\epsilon\\leq\\operatorname*{min}\\{1,A\\}}\\left[C_{L}\\epsilon+\\frac{d}{\\beta_{k}}\\log\\left(\\frac{1}{\\epsilon}\\right)+\\frac{\\log C}{\\beta_{k}}\\right]+\\frac{d}{\\beta_{k}}}\\\\ &{\\qquad\\qquad\\leq\\mathcal{I}^{*}(1+\\delta)+\\frac{d}{\\beta_{k}}\\log\\left(\\frac{C_{L}}{\\delta\\mathcal{I}^{*}}\\right)+\\frac{d+\\log C}{\\beta_{k}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where the last inequality follows by choosing $\\begin{array}{r}{\\epsilon=\\frac{\\delta\\mathcal{I}^{*}}{C_{L}}\\leq\\operatorname*{min}\\{1,A\\}}\\end{array}$ since $\\begin{array}{r}{\\delta\\leq\\frac{C_{L}\\,\\operatorname*{min}\\left\\{1,A\\right\\}}{\\mathcal{I}^{*}}}\\end{array}$ ", "page_idx": 33}, {"type": "text", "text": "Then, for all $k\\geq1$ and $t\\geq0$ , ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathfrak{z}_{k}\\mathcal{I}(\\eta_{t}^{k})\\le\\beta_{k}\\mathcal{I}_{\\beta_{k}}(\\eta_{t}^{k})\\le\\beta_{k}\\mathcal{I}_{\\beta_{k}}(\\eta_{0}^{k})=\\beta_{k}\\mathcal{I}_{\\beta_{k}}(\\eta_{T_{k-1}}^{k-1})\\le\\beta_{k}\\mathcal{I}_{\\beta_{k-1}}(\\eta_{T_{k-1}}^{k-1})=2\\beta_{k-1}\\mathcal{I}_{\\beta_{k-1}}(\\eta_{T_{k-1}}^{k-1}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where we used successively that $\\mathcal{J}_{\\beta_{k}}-\\mathcal{J}=\\beta_{k}^{-1}H\\left(\\cdot|\\tau\\right)\\geq0$ , that $(\\eta_{t}^{k})_{t}$ is a Wasserstein gradient flow for ${\\mathcal{I}}_{\\beta_{k}}$ , that $\\mathcal{I}_{\\beta_{k-1}}-\\mathcal{I}_{\\beta_{k}}=(\\beta_{k-1}^{-1}-\\beta_{k}^{-1})H\\left(\\cdot|\\tau\\right)\\geq0$ since $(\\beta_{k})_{k}$ is increasing, and that by definition $\\beta_{k}=2^{k}\\beta_{0}$ . So by (E.2), ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\beta_{k}\\mathcal{I}(\\eta_{t}^{k})\\le2\\beta_{k-1}\\mathcal{I}_{\\beta_{k-1}}(\\eta_{T_{k-1}}^{k-1})\\le2\\beta_{k-1}\\mathcal{I}^{*}(1+\\delta)+2d\\log\\left(\\frac{C_{L}}{\\delta\\mathcal{T}^{*}}\\right)+2d+2\\log C}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\le2\\frac{d}{\\delta}(1+\\delta)+2d\\log\\left(\\frac{C_{L}}{\\delta\\mathcal{T}^{*}}\\right)+2d+2\\log C}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=2d\\left(\\delta^{-1}+\\log\\left(\\frac{C_{L}}{\\delta\\mathcal{T}^{*}}\\right)+2+\\frac{\\log C}{d}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "since our choice of $\\beta_{0}=d$ and $K=\\lceil\\log_{2}(1/(\\delta\\mathcal{J}^{*}))\\rceil$ ensures that $\\begin{array}{r}{\\beta_{k-1}\\le\\beta_{K}=2^{K}\\beta_{0}\\le\\frac{d}{\\delta\\mathcal{T}^{*}}}\\end{array}$ . For $k=0$ and all $t\\geq0$ , we have more simply $\\beta_{0}\\mathcal{I}(\\eta_{t}^{0})\\leq\\beta_{0}\\mathcal{I}_{\\beta_{0}}(\\eta_{t}^{0})\\leq\\beta_{0}\\mathcal{I}_{\\beta_{0}}(\\eta_{0})=d\\mathcal{I}_{\\beta_{0}}(\\bar{\\eta_{0}})$ . As a result, for all $k\\geq0$ we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\forall t\\geq0,\\ \\beta_{k}\\mathcal{I}(\\eta_{t}^{k})\\leq2d\\left(\\delta^{-1}+\\log\\left(\\frac{C_{L}}{\\delta\\mathcal{I}^{*}}\\right)+2+\\frac{\\log C}{d}+\\frac{1}{2}\\mathcal{I}_{\\beta_{0}}(\\eta_{0})\\right)\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "and so ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\alpha(k)=\\operatorname*{inf}_{t\\ge0}\\alpha_{\\tau}\\exp\\left(-\\kappa_{1}\\beta_{k}\\mathcal{I}(\\eta_{t}^{k})\\right)}\\\\ &{\\qquad\\ge\\alpha_{\\tau}\\exp\\left(-2\\kappa_{1}d\\left(\\delta^{-1}+\\log\\left(\\displaystyle\\frac{C_{L}}{\\delta\\mathcal{I}^{*}}\\right)+2+\\displaystyle\\frac{\\log C}{d}+\\displaystyle\\frac{1}{2}\\mathcal{I}_{\\beta_{0}}(\\eta_{0})\\right)\\right)=:\\underline{{\\alpha}}(k).}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Moreover, we can choose $\\overline{{c}}_{k}$ as ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{I}_{\\beta_{k}}(\\eta_{0}^{k})=\\mathcal{I}_{\\beta_{k}}(\\eta_{T_{k-1}}^{k-1})\\le\\mathcal{I}_{\\beta_{k-1}}(\\eta_{T_{k-1}}^{k-1})\\le\\mathcal{I}_{\\beta_{k-1}}(\\eta_{0}^{k-1})\\le\\ldots\\le\\mathcal{I}_{\\beta_{0}}(\\eta_{0})\\quad\\mathrm{by~induction},}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\operatorname{so}\\,\\,\\,\\,\\mathcal{I}_{\\beta_{k}}(\\eta_{0}^{k})-\\operatorname*{min}\\mathcal{I}_{\\beta_{k}}\\le\\mathcal{I}_{\\beta_{0}}(\\eta_{0})=:\\overline{{c}}_{k}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Therefore, more explicitly, ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{T_{k}=\\displaystyle\\frac{\\beta_{k}}{2\\underline{{\\alpha}}(k)}\\log\\left(\\frac{\\beta_{k}}{d}\\overline{{c}}_{k}\\right)}}\\\\ {{=\\displaystyle\\frac{\\beta_{k}}{2}\\log\\left(\\frac{\\beta_{k}}{d}\\mathcal{I}_{\\beta_{0}}(\\eta_{0})\\right)\\cdot\\alpha_{\\tau}^{-1}\\exp\\left(2\\kappa_{1}d\\left(\\delta^{-1}+\\log\\left(\\frac{C_{L}}{\\delta\\mathcal{I}^{*}}\\right)+2+\\displaystyle\\frac{\\log C}{d}+\\frac{1}{2}\\mathcal{I}_{\\beta_{0}}(\\eta_{0})\\right)\\right)}}\\\\ {{=2^{k-1}d\\cdot\\log\\left(2^{k}\\mathcal{I}_{\\beta_{0}}(\\eta_{0})\\right)\\cdot\\alpha_{\\tau}^{-1}\\exp\\left(2\\kappa_{1}d\\left(\\delta^{-1}+\\log\\left(\\frac{C_{L}}{\\delta\\mathcal{I}^{*}}\\right)+2+\\displaystyle\\frac{\\log C}{d}+\\frac{1}{2}\\mathcal{I}_{\\beta_{0}}(\\eta_{0})\\right)\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "since $\\beta_{k}=2^{k}\\beta_{0}=2^{k}d.$ . Note that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{k=0}^{K}2^{k-1}\\log\\left(2^{k}\\mathcal{J}_{\\beta_{0}}(\\eta_{0})\\right)=\\displaystyle\\sum_{k=0}^{K}2^{k}\\frac{\\log J_{\\beta_{0}}(\\eta_{0})}{2}+\\displaystyle\\sum_{k=0}^{K}k2^{k-1}\\log(2)}\\\\ {\\displaystyle}&{=(2^{K+1}-1)\\frac{\\log J_{\\beta_{0}}(\\eta_{0})}{2}+\\log(2)\\left((K-1)2^{K}+1\\right)}\\\\ &{\\le2^{K}\\log J_{\\beta_{0}}(\\eta_{0})+\\log(2)K2^{K}}\\\\ &{\\le\\displaystyle\\frac1{\\delta\\mathcal{J}^{*}}\\log J_{\\beta_{0}}(\\eta_{0})+\\frac1{\\delta\\mathcal{J}^{*}}\\log\\left(\\frac1{\\delta\\mathcal{J}^{*}}\\right)}\\\\ &{=\\displaystyle\\frac1{\\delta\\mathcal{J}^{*}}\\log\\left(\\frac{J_{\\beta_{0}}(\\eta_{0})}{\\delta\\mathcal{J}^{*}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "since $K=\\lceil\\log_{2}(1/(\\delta\\mathcal{T}^{*}))\\rceil$ , hence the announced bound on the total time-complexity $\\sum_{k=0}^{K}T_{k}$ . Finally, at round $K=\\lceil\\log_{2}(1/(\\delta\\mathcal{T}^{*}))\\rceil$ , then $\\begin{array}{r}{\\beta_{K}=2^{K}\\beta_{0}=2^{K}d\\in\\left[\\frac{1}{2}\\frac{d}{\\delta\\mathcal{I}^{*}},\\frac{d}{\\delta\\mathcal{I}^{*}}\\right]}\\end{array}$ , so by (E.2), ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathcal{I}(\\eta_{T_{K}}^{K})\\le\\mathcal{I}_{\\beta_{K}}(\\eta_{T_{K}}^{K})\\le\\mathcal{I}^{*}(1+\\delta)+\\frac{d}{\\beta_{K}}\\log\\left(\\frac{C_{L}}{\\delta\\mathcal{T}^{*}}\\right)+\\frac{d+\\log C}{\\beta_{K}}}}\\\\ &{\\le\\mathcal{I}^{*}\\left(1+3\\delta+2\\delta\\frac{\\log(C)}{d}+2\\delta\\log\\left(\\frac{C_{L}}{\\delta\\mathcal{T}^{*}}\\right)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "which completes the proof. ", "page_idx": 34}, {"type": "text", "text": "E.3 Proof of Thm. 4.2 ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "We state a slightly more precise version of Thm. 4.2 below, and prove it as a corollary of the more general Thm. E.3. Then Thm. 4.2 follows by choosing $\\begin{array}{r}{\\delta=\\dot{\\Theta}(\\frac{\\Delta}{\\log(B/(\\Delta J_{\\lambda}^{*}))})}\\end{array}$ , gathering the constants appearing in the bounds, noting that $J_{\\lambda,\\beta_{0}}(\\eta_{0})\\,\\le\\,J_{\\lambda}(\\eta_{0})+d H\\,(\\eta_{0}|\\tau)\\le\\dot{G}(0)+d H(\\eta_{0})\\,+$ $d\\log\\operatorname{vol}(\\bar{\\mathcal{W}})$ . ", "page_idx": 34}, {"type": "text", "text": "Theorem E.4. Under Assumption $^{\\,l}$ , there exists constants $B=\\mathrm{poly}(L_{i},B_{i},G(0),\\lambda^{-1})$ and $C$ dependent only on $\\mathcal{W}$ such that the following holds. For any $\\begin{array}{r}{\\delta\\ \\leq\\ \\frac{B}{J_{\\lambda}^{*}}}\\end{array}$ , MFLD-Bilevel with the temperature schedule $(\\beta_{t})_{t\\geq0}$ defined by $\\forall k\\,\\leq\\,K,\\forall t\\,\\in\\,[t_{k},t_{k+1}],\\beta_{t}\\,=\\,2^{k}d$ where $t_{0}\\,=\\,0$ and $K\\stackrel{\\scriptscriptstyle-}{=}\\lceil\\log_{2}(1/(\\delta\\mathcal{J}^{*}))\\rceil$ and ", "page_idx": 35}, {"type": "text", "text": "t $\\small\\begin{array}{r l}&{k+1\\!-\\!t_{k}=2^{k-1}d\\log\\left(2^{k}J_{\\lambda,\\beta_{0}}(\\eta_{0})\\right)\\!\\cdot\\!\\alpha_{\\tau}^{-1}\\exp\\left(\\displaystyle\\frac{2L_{0}d}{\\lambda}\\left(\\delta^{-1}+\\log\\left(\\frac{B C^{1/d}}{\\delta J_{\\lambda}^{*}}\\right)+2+\\frac{J_{\\lambda,\\beta_{0}}(\\eta_{0})}{2}\\right)\\right),}\\end{array}$ achieves $(1+\\Delta)$ -multiplicative accuracy, where $\\begin{array}{r}{\\Delta=3\\delta+2\\delta\\log{\\left(\\frac{B C^{1/d}}{\\delta J_{\\lambda}^{*}}\\right)}.}\\end{array}$ , with time-complexity $\\Gamma_{\\Delta}\\leq t_{K+1}\\leq\\frac{d}{\\delta J_{\\lambda}^{*}}\\log\\left(\\frac{J_{\\lambda,\\beta_{0}}(\\eta_{0})}{\\delta J_{\\lambda}^{*}}\\right)\\cdot\\alpha_{\\tau}^{-1}\\exp\\left(\\frac{2L_{0}d}{\\lambda}\\left(\\delta^{-1}+\\log\\left(\\frac{B C^{1/d}}{\\delta J_{\\lambda}^{*}}\\right)+2+\\frac{J_{\\lambda,\\beta_{0}}(\\eta_{0})}{2}\\right)\\right).$ ", "page_idx": 35}, {"type": "text", "text": "Proof of Thm. 4.2 . Let us show that the conditions of Thm. E.3 are satisfied, under Assumption 1, for $\\mathcal{I}=J_{\\lambda}$ . $J_{\\lambda}$ is convex and non-negative, and it is implied throughout Sec. 4.1 that inf ${{J}_{\\lambda}}>0$ , for the notion of convergence to a fixed multiplicative accuracy to apply (Def. 4.1). The existence of a minimizer $\\eta^{*}$ is ensured by the weak convexity of $J_{\\lambda}$ , by a similar argument as the proof of (P0) in Sec. D.3. We have the condition 1. with $\\begin{array}{r}{\\kappa_{1}\\,=\\,\\frac{L_{0}}{\\lambda}}\\end{array}$ , i.e. $\\begin{array}{r}{\\|J_{\\lambda}^{\\prime}[\\eta]\\|_{\\infty}\\leq\\,\\frac{L_{0}}{\\lambda}J_{\\lambda}(\\eta)}\\end{array}$ , by the first part of Lem. D.4. We also have condition 2. with $A\\,=\\,\\infty$ and $C_{L}\\,=\\,B\\,:=\\,\\sqrt{2L_{0}G(0)}$ \u00b7 $\\begin{array}{r}{\\left(\\frac{L_{1}}{\\lambda^{2}}\\sqrt{2L_{0}G(0)}+\\frac{B_{1}}{\\lambda}\\right)}\\end{array}$ , as shown in Lem. D.5, since $W_{1}\\le W_{2}\\le W_{\\infty}$ . ", "page_idx": 35}, {"type": "text", "text": "Note that annealed MFLD-Bilevel with the announced temperature annealing schedule $(\\beta_{t})_{t}$ , precisely corresponds to Algorithm 1 with the schedule (E.1) applied to $\\mathcal{I}\\,=\\,J_{\\lambda}$ . So the announced timecomplexity bound follows directly from the application of Thm. E.3. \u53e3 ", "page_idx": 35}, {"type": "text", "text": "F Details for Sec. 5 (estimates of the local LSI constant) ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "We begin by presenting the proof of Prop. 5.1, which states that bounding the LSI constant of $\\eta_{\\lambda,\\beta}$ leads to a local convergence rate. ", "page_idx": 35}, {"type": "text", "text": "Proof of Prop. 5.1. For any $\\eta\\in\\mathcal{P}(\\mathcal{W})$ , we denote $\\hat{\\eta}(\\mathrm{d}w)\\,=\\,e^{-\\beta J_{\\lambda}^{\\prime}[\\eta](w)}\\tau(\\mathrm{d}w)/Z_{\\eta}$ where $Z_{\\eta}\\,=$ $\\int e^{-\\beta J_{\\lambda}^{\\prime}[\\eta]}\\mathrm{d}\\tau$ . First note that for any $\\eta,\\eta^{\\prime}\\in\\mathcal{P}(\\mathcal{W})$ , ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left|\\log\\frac{\\mathrm{d}\\hat{\\eta}}{\\mathrm{d}\\hat{\\eta}^{\\prime}}(w)+(\\log Z_{\\eta}-\\log Z_{\\eta^{\\prime}})\\right|=\\beta\\left|J_{\\lambda}^{\\prime}[\\eta](w)-J_{\\lambda}^{\\prime}[\\eta^{\\prime}](w)\\right|}&{}\\\\ {=\\beta\\frac{\\lambda}{2}\\left|f_{\\eta}(w)^{2}-f_{\\eta^{\\prime}}(w)^{2}\\right|}&{}\\\\ {\\ }&{\\leq\\beta\\frac{\\lambda}{2}\\left(|f_{\\eta}|+|f_{\\eta^{\\prime}}|\\right)(w)\\cdot\\left|f_{\\eta}-f_{\\eta^{\\prime}}\\right|(w)}\\\\ {\\ }&{\\leq\\beta\\frac{\\lambda}{2}\\cdot2\\frac{1}{\\bar{\\lambda}}\\sqrt{2L_{0}G(0)}\\cdot H W_{2}(\\eta,\\eta^{\\prime})=:\\widetilde{H}W_{2}(\\eta,\\eta^{\\prime})}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "by Lem. D.4 and Lem. D.7, where $H$ is a constant dependent only on $\\lambda^{-1},G(0),L_{0},L_{1},B_{1},\\widetilde{L}_{2}$ . ", "page_idx": 35}, {"type": "text", "text": "Now suppose that $\\eta_{\\lambda,\\beta}=\\arg\\operatorname*{min}J_{\\lambda,\\beta}=\\widehat{\\eta_{\\lambda,\\beta}}$ satisfies $\\alpha^{*}$ -LSI. Let $\\varepsilon>0$ and $\\eta_{0}$ in the $\\delta$ -sublevel set of $J_{\\lambda,\\beta}$ , i.e., $\\eta_{0}\\,\\in\\,S_{\\delta}:=\\,J_{\\lambda,\\beta}^{-1}((-\\infty,\\operatorname*{inf}J_{\\lambda,\\beta}+\\delta])$ , for some $\\delta>0$ to be chosen. Denote by $(\\eta_{t})_{t}$ the MFLD trajectory for $J_{\\lambda,\\beta}$ initialized at $\\eta_{0}$ . Note that $S_{\\delta}$ is stable by MFLD since $J_{\\lambda,\\beta}(\\eta_{t})$ decreases with $t$ . So it suffices to show that $J_{\\lambda,\\beta}$ satisfies $\\left(\\alpha^{*}-\\varepsilon\\right)$ -LSI uniformly over $S_{\\delta}$ . ", "page_idx": 35}, {"type": "text", "text": "Choose any $\\eta\\in S_{\\delta}$ , i.e., such that $J_{\\lambda,\\beta}(\\eta)-\\operatorname*{inf}J_{\\lambda,\\beta}\\leq\\delta$ . In particular by Thm. 2.1, it holds ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\beta^{-1}H\\left(\\eta|\\eta_{\\lambda,\\beta}\\right)\\leq J_{\\lambda,\\beta}(\\eta)-\\operatorname*{inf}J_{\\lambda,\\beta}\\leq\\delta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Furthermore, since $\\eta_{\\lambda,\\beta}$ satisfies LSI with constant $\\alpha^{*}$ then it also satisfies the following Talagrand inequality, as shown in [OV00]: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\forall\\eta^{\\prime},\\,W_{2}(\\eta^{\\prime},\\eta_{\\lambda,\\beta})\\leq\\sqrt{\\frac{2}{\\alpha^{*}}H\\left(\\eta^{\\prime}|\\eta_{\\lambda,\\beta}\\right)}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Then by the inequality noted above, we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\left|\\log\\frac{\\mathrm{d}\\hat{\\eta}}{\\mathrm{d}\\eta_{\\lambda,\\beta}}(w)+c\\right|\\le\\widetilde{H}W_{2}(\\eta,\\eta_{\\lambda,\\beta})\\le\\widetilde{H}\\sqrt{\\frac{2}{\\alpha^{*}}H\\left(\\eta|\\eta_{\\lambda,\\beta}\\right)}\\le\\widetilde{H}\\sqrt{\\frac{2}{\\alpha^{*}}}\\cdot\\sqrt{\\beta\\delta}\\ \\ =:M\\sqrt{\\delta}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "for some $c\\in\\mathbb{R}$ , and so by the Holley-Stroock bounded perturbation argument, $\\hat{\\eta}$ satisfies LSI with constant $\\alpha^{*}e^{-M\\sqrt{\\delta}}\\geq\\alpha^{*}-\\varepsilon$ for $\\delta$ small enough. \u53e3 ", "page_idx": 36}, {"type": "text", "text": "F.1 Preliminary estimates for $J_{\\lambda}$ under Assumption 2 ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Throughout the remainder of this appendix, in the context of Assumption 2, we will use the notations ", "page_idx": 36}, {"type": "text", "text": "\u2022 the Hilbert space $\\mathcal{H}=L_{\\rho}^{2}(\\mathbb{R}^{d+1})$ with the inner product $\\langle f,g\\rangle_{\\mathcal{H}}=\\mathbb{E}_{x\\sim\\rho}f(x)g(x)$ ,   \n\u2022 the feature map $\\phi:\\mathcal{W}\\to\\mathcal{H}$ given by $\\phi(w)(x)=\\varphi(\\langle w,x\\rangle)$ ,   \n\u2022 the symmetric positive-semi-definite operator in $\\mathcal{H}$ : $\\begin{array}{r}{K_{\\eta}=\\int\\phi(w)\\phi(w)^{*}\\mathrm{d}\\eta(w)}\\end{array}$ , where $^*$ denotes adjoint in $\\mathcal{H}$ .   \n\u2022 For any $h\\ \\in\\ {\\mathcal{H}}$ , we denote by $\\langle h,\\nabla\\phi(w)\\rangle_{\\mathcal{H}}$ (resp. $\\left\\langle h,\\nabla^{2}\\,\\phi(w)\\right\\rangle_{\\mathcal{H}})$ the gradient (resp. Hessian) at w of w  \u2192\u27e8h, \u03d5(w)\u27e9 . ", "page_idx": 36}, {"type": "text", "text": "The usefuless of these notations is justified by Prop. F.1 below, which gives a simplified expression for $J_{\\lambda}$ and $J_{\\lambda}^{\\prime}$ . ", "page_idx": 36}, {"type": "text", "text": "Proposition F.1. Under Assumption 2, letting the Hilbert space $\\mathcal{H}=L_{\\rho}^{2}(\\mathbb{R}^{d+1})$ and the feature map $\\phi:\\mathcal{W}\\to\\mathcal{H}$ given by $\\phi(w)(x)=\\varphi(\\langle w,x\\rangle)$ , we have ", "page_idx": 36}, {"type": "equation", "text": "$$\nJ_{\\lambda}(\\eta)=\\frac{\\lambda}{2}\\langle y,(K_{\\eta}+\\lambda\\,\\mathrm{id})^{-1}y\\rangle_{\\mathcal{H}},\\ \\ \\ \\ \\ \\ \\ \\ J_{\\lambda}^{\\prime}[\\eta](w)=-\\frac{\\lambda}{2}\\langle\\phi(w),(K_{\\eta}+\\lambda\\,\\mathrm{id})^{-1}y\\rangle_{\\mathcal{H}}^{2},\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "with $\\begin{array}{r}{K_{\\eta}=\\int\\phi(w)\\phi(w)^{*}\\mathrm{d}\\eta(w)}\\end{array}$ , where $^*$ denotes adjoint in $\\mathcal{H}$ . More explicitly, $K_{\\eta}$ is the integral operator of the kernel $\\begin{array}{r}{k_{\\eta}(x,x^{\\prime})\\,=\\,\\int\\varphi(\\langle w,x\\rangle)\\varphi(\\langle w,x^{\\prime}\\rangle)\\mathrm{d}\\eta(w)}\\end{array}$ with respect to the distribution $x\\sim\\rho_{i}$ , i.e., ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\forall h\\in\\mathcal{H}=L_{\\rho}^{2}(\\mathbb{R}^{d+1}),\\;\\;(K_{\\eta}h)(x)=\\mathbb{E}_{x^{\\prime}\\sim\\rho}\\left[k_{\\eta}(x,x^{\\prime})h(x^{\\prime})\\right]\\quad i n\\;L_{\\rho}^{2}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Proof. Under Assumption 2 we have ", "page_idx": 36}, {"type": "equation", "text": "$$\nG(\\nu)=\\frac{1}{2}\\mathbb{E}_{x\\sim\\rho}\\left|\\int_{\\mathcal{W}}\\varphi(\\langle w,x\\rangle)\\mathrm{d}\\nu(w)-y(x)\\right|^{2}=\\frac{1}{2}\\left\\|\\int_{\\mathcal{W}}\\phi(w)\\mathrm{d}\\nu(w)-y\\right\\|_{\\mathcal{H}}^{2},\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "so the optimization problem (3.3) defining $J_{\\lambda}(\\eta)$ , for a fixed $\\eta$ , writes ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{f\\in L_{\\eta}^{2}(\\mathcal{W})}\\frac{1}{2}\\left\\|\\int_{\\mathcal{W}}\\phi(w)f(w)\\mathrm{d}\\eta(w)-y\\right\\|_{\\mathcal{H}}^{2}+\\frac{\\lambda}{2}\\int_{\\mathcal{W}}\\left|f\\right|^{2}(w)\\mathrm{d}\\eta(w).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "This problem is strictly convex thanks to the term in $\\lambda$ , and the FOC is $\\forall w$ , $\\begin{array}{r}{\\left\\langle\\int\\phi f\\mathrm{d}\\eta-y,\\phi(w)\\eta(\\dot{\\mathrm{d}}w)\\right\\rangle_{\\mathcal{H}}+\\lambda f(w)\\eta(\\mathrm{d}w)\\ =\\ 0}\\end{array}$ . So the unique minimum $f_{\\eta}$ is a solution of the fixed point equation $\\begin{array}{r}{f(w)\\;=\\;-\\frac{1}{\\lambda}\\left\\langle\\int\\phi f\\mathrm{d}\\eta-y,\\phi(w)\\right\\rangle_{\\mathcal{H}}}\\end{array}$ in $L_{\\eta}^{2}(\\mathcal{W})$ . In particular, denoting $\\begin{array}{r}{\\hat{h}_{\\eta}=-\\frac{1}{\\lambda}\\left(\\int\\phi f_{\\eta}\\mathrm{d}\\eta-y\\right)}\\end{array}$ , then $f_{\\eta}(w)=\\left\\langle\\hat{h}_{\\eta},\\phi(w)\\right\\rangle_{\\mathcal{H}}$ and, integrating against $\\phi\\eta$ , ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\int_{\\mathcal W}f_{\\eta}(w)\\phi(w)\\mathrm{d}\\eta(w)=\\int_{\\mathcal W}\\phi(w)\\,\\phi(w)^{\\ast}\\hat{h}_{\\eta}\\,\\mathrm{d}\\eta(w)}}\\\\ &{}&{\\iff-\\lambda\\hat{h}_{\\eta}+y=K_{\\eta}\\hat{h}_{\\eta}\\,\\Longleftrightarrow\\,(K_{\\eta}+\\lambda\\mathrm{id})\\hat{h}_{\\eta}=y\\,\\Longleftrightarrow\\,\\hat{h}_{\\eta}=(K_{\\eta}+\\lambda\\mathrm{id})^{-1}y,}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where $a^{*}\\,b=\\langle a,b\\rangle_{\\mathcal{H}}$ and $\\begin{array}{r}{K_{\\eta}=\\int_{\\mathcal W}\\phi(w)\\phi(w)^{*}\\mathrm{d}\\eta(w)}\\end{array}$ . So the optimal value $J_{\\lambda}(\\eta)$ is ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\int_{\\lambda}(\\eta)=\\frac{1}{2}\\left\\|\\int_{\\mathcal{W}}\\phi(w)f_{\\eta}(w)\\mathrm{d}\\eta(w)-y\\right\\|_{\\mathcal{H}}^{2}+\\frac{\\lambda}{2}\\int_{\\mathcal{W}}|f_{\\eta}|^{2}\\left(w\\right)\\mathrm{d}\\eta(w)}\\\\ &{\\displaystyle\\qquad=\\frac{1}{2}\\left\\|\\lambda\\hat{h}_{\\eta}\\right\\|_{\\mathcal{H}}^{2}+\\frac{\\lambda}{2}\\int_{\\mathcal{W}}\\hat{h}_{\\eta}^{*}\\phi(w)\\;\\phi(w)^{*}\\hat{h}_{\\eta}\\;\\mathrm{d}\\eta(w)}\\\\ &{\\displaystyle\\qquad=\\frac{1}{2}\\left\\langle\\lambda\\hat{h}_{\\eta},\\lambda\\hat{h}_{\\eta}\\right\\rangle_{\\mathcal{H}}+\\frac{\\lambda}{2}\\left\\langle\\hat{h}_{\\eta},K_{\\eta}\\hat{h}_{\\eta}\\right\\rangle_{\\mathcal{H}}=\\frac{1}{2}\\left\\langle\\lambda\\hat{h}_{\\eta},\\lambda\\hat{h}_{\\eta}+K_{\\eta}\\hat{h}_{\\eta}\\right\\rangle_{\\mathcal{H}}}\\\\ &{\\displaystyle\\qquad=\\frac{1}{2}\\left\\langle\\lambda\\hat{h}_{\\eta},y\\right\\rangle_{\\mathcal{H}}=\\frac{\\lambda}{2}\\langle y,(K_{\\eta}+\\lambda\\mathrm{id})^{-1}y\\rangle_{\\mathcal{H}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Further, by applying the envelope theorem on (F.1) (and reasoning similarly to the proof of Prop. D.2 to deal with $w\\not\\in\\mathrm{supp}(\\eta)$ , by extending $f_{\\eta}\\in L_{\\eta}^{2}(\\mathcal{W})$ into a function $\\mathcal{W}\\to\\mathbb{R}$ ), we then have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\forall w\\in\\mathcal{W},\\ J_{\\lambda}^{\\prime}[\\eta](w)=\\left\\langle\\int\\phi f_{\\eta}\\mathrm{d}\\eta-y,\\phi(w)f_{\\eta}(w)\\right\\rangle_{\\mathcal{H}}+\\frac{\\lambda}{2}\\left|f_{\\eta}\\right|^{2}(w)}}\\\\ &{=f_{\\eta}(w)\\left\\langle-\\lambda\\hat{h}_{\\eta},\\phi(w)\\right\\rangle_{\\mathcal{H}}+\\frac{\\lambda}{2}\\left|f_{\\eta}\\right|^{2}(w)}\\\\ &{=-\\lambda\\left|f_{\\eta}\\right|^{2}(w)+\\frac{\\lambda}{2}\\left|f_{\\eta}\\right|^{2}(w)\\ =-\\frac{\\lambda}{2}\\left|f_{\\eta}\\right|^{2}(w)=-\\frac{\\lambda}{2}\\left\\langle\\hat{h}_{\\eta},\\phi(w)\\right\\rangle_{\\mathcal{H}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "The characterization of $K_{\\eta}$ as the integral operator in $L_{\\rho}^{2}(\\mathbb{R}^{d+1})$ of the kernel $k_{\\eta}(x,x^{\\prime})\\;\\;=$ $\\begin{array}{r}{\\int_{\\mathcal W}\\phi(w)(x)\\,\\phi(w)(x^{\\prime})\\mathrm{d}\\eta(w)}\\end{array}$ follows directly from the definition $\\begin{array}{r}{K_{\\eta}=\\int_{\\mathcal W}\\phi(w)\\phi(w)^{*}\\mathrm{d}\\eta(w)}\\end{array}$ , since ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\forall h\\in\\mathcal{H},\\,\\,K_{\\eta}h=\\int_{\\mathcal{W}}\\phi(w)\\left<\\phi(w),h\\right>_{\\mathcal{H}}\\mathrm{d}\\eta(w),}\\\\ {\\left(K_{\\eta}h\\right)(x)=\\displaystyle\\int_{\\mathcal{W}}\\phi(w)(x)\\operatorname{\\mathbb{E}}_{x^{\\prime}\\sim\\rho}\\left[\\phi(w)(x^{\\prime})h(x^{\\prime})\\right]\\,\\mathrm{d}\\eta(w)}\\\\ {\\displaystyle\\qquad\\qquad=\\mathbb{E}_{x^{\\prime}\\sim\\rho}\\left[\\int_{\\mathcal{W}}\\phi(w)(x)\\phi(w)(x^{\\prime})\\ h(x^{\\prime})\\,\\mathrm{d}\\eta(w)\\right]}\\\\ {\\displaystyle=\\mathbb{E}_{x^{\\prime}\\sim\\rho}\\left[k_{\\eta}(x,x^{\\prime})h(x^{\\prime})\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "We have the following Wasserstein Lipschitz-continuity properties for the bilevel objective functional $J_{\\lambda}$ . ", "page_idx": 37}, {"type": "text", "text": "Proposition F.2. Under Assumption 2, suppose furthermore that $\\operatorname*{sup}_{w}\\|\\nabla^{i}\\phi(w)\\|_{\\mathcal{H}}\\leq B_{i}<\\infty$ for $i\\in\\{0,1,2\\}$ . Then for any $w\\in\\mathcal{W}=\\mathbb{S}^{d}$ and any $\\eta,\\eta^{\\prime}\\in\\mathcal{P}(\\mathcal{W})$ , it holds ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{c l}{\\displaystyle|J_{\\lambda}(\\eta)-J_{\\lambda}(\\eta^{\\prime})|\\leq\\frac{B_{0}B_{1}}{\\lambda}\\left\\|y\\right\\|_{\\mathcal{H}}^{2}\\cdot W_{1}(\\eta,\\eta^{\\prime})}\\\\ {\\displaystyle a n d\\quad|J_{\\lambda}^{\\prime}[\\eta](w)-J_{\\lambda}^{\\prime}[\\eta^{\\prime}](w)|\\leq\\frac{2B_{0}^{3}B_{1}}{\\lambda^{2}}\\left\\|y\\right\\|_{\\mathcal{H}}^{2}\\cdot W_{1}(\\eta,\\eta^{\\prime})}\\\\ {\\displaystyle a n d\\quad\\left\\|\\nabla J_{\\lambda}^{\\prime}[\\eta](w)-\\nabla J_{\\lambda}^{\\prime}[\\eta^{\\prime}](w)\\right\\|_{w}\\leq\\frac{4B_{0}^{2}B_{1}^{2}}{\\lambda^{2}}\\left\\|y\\right\\|_{\\mathcal{H}}^{2}\\cdot W_{1}(\\eta,\\eta^{\\prime})}\\\\ {\\displaystyle a n d\\quad\\left\\|\\nabla^{2}J_{\\lambda}^{\\prime}[\\eta](w)-\\nabla^{2}J_{\\lambda}^{\\prime}[\\eta^{\\prime}](w)\\right\\|_{\\mathrm{op~}w}\\leq\\frac{4B_{0}B_{1}(B_{0}B_{2}+B_{1}^{2})}{\\lambda^{2}}\\left\\|y\\right\\|_{\\mathcal{H}}^{2}\\cdot W_{1}(\\eta,\\eta^{\\prime}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Proof. By Prop. F.1, ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad J_{\\lambda}^{\\prime}[\\eta](w)=-\\frac{\\lambda}{2}\\left<\\phi(w),(K_{\\eta}+\\lambda\\mathrm{id})^{-1}y\\right>_{\\mathcal{H}}^{2}\\ \\mathrm{where}\\ K_{\\eta}=\\int_{\\mathcal{W}}\\phi(w^{\\prime\\prime})\\phi(w^{\\prime\\prime})^{*}\\,\\mathrm{d}\\eta(w^{\\prime\\prime})}\\\\ &{\\nabla J_{\\lambda}^{\\prime}[\\eta](w)=-\\lambda\\left<\\phi(w),(K_{\\eta}+\\lambda\\mathrm{id})^{-1}y\\right>_{\\mathcal{H}}\\left<\\nabla\\phi(w),(K_{\\eta}+\\lambda\\mathrm{id})^{-1}y\\right>_{\\mathcal{H}}\\ }\\\\ &{\\|\\nabla J_{\\lambda}^{\\prime}[\\eta]\\|_{w}\\leq\\lambda\\left\\|\\phi(w)\\right\\|_{\\mathcal{H}}\\|\\nabla\\phi(w)\\|_{w}\\left\\|(K_{\\eta}+\\lambda\\mathrm{id})^{-1}y\\right\\|_{\\mathcal{H}}^{2}}\\\\ &{\\leq\\lambda B_{0}B_{1}\\left\\|y\\right\\|_{\\mathcal{H}}^{2}\\left\\|(K_{\\eta}+\\lambda)^{-1}\\right\\|_{\\mathrm{op}}^{2}}\\\\ &{\\leq\\frac{1}{\\lambda}B_{0}B_{1}\\left\\|y\\right\\|_{\\mathcal{H}}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "since $K_{\\eta}$ is positive-semi-definite by definition and so $\\begin{array}{r l}{\\left\\|(K_{\\eta}+\\lambda)^{-1}\\right\\|_{\\mathrm{op}}=\\sigma_{\\operatorname*{max}}((K_{\\eta}+\\lambda\\operatorname{id})^{-1})=}&{{}}\\end{array}$ $[\\sigma_{\\mathrm{min}}(K_{\\eta}+\\lambda\\,\\mathrm{id})]^{-1}\\le\\lambda^{-1}$ . So by applying Lem. D.8, this shows the first inequality. ", "page_idx": 37}, {"type": "text", "text": "Moreover, the first variation of $K_{\\eta}$ at any $\\eta$ is $w^{\\prime}\\mapsto\\phi(w^{\\prime})\\phi(w^{\\prime})^{*}$ , thus by the formula $\\partial(X^{-1})=$ $-X^{-1}(\\partial X)X^{-1}$ for the derivative of a matrix inverse, ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\frac{\\delta}{\\delta\\eta(w^{\\prime})}(K_{\\eta}+\\lambda\\,\\mathrm{id})^{-1}=-(K_{\\eta}+\\lambda\\,\\mathrm{id})^{-1}\\cdot\\phi(w^{\\prime})\\phi(w^{\\prime})^{\\ast}\\cdot(K_{\\eta}+\\lambda\\,\\mathrm{id})^{-1},\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "and so, letting for concision $M=(K_{\\eta}+\\lambda\\operatorname{id})^{-1}$ , ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{J_{\\lambda}^{\\prime\\prime}[\\eta](w,w^{\\prime})}\\\\ &{=-\\lambda\\left\\langle\\phi(w),(K_{\\eta}+\\lambda\\mathrm{id})^{-1}y\\right\\rangle_{\\mathcal{H}}\\left\\langle\\phi(w),-(K_{\\eta}+\\lambda\\mathrm{id})^{-1}\\cdot\\phi(w^{\\prime})\\phi(w^{\\prime})^{*}\\cdot(K_{\\eta}+\\lambda\\mathrm{id})^{-1}y\\right\\rangle_{\\mathcal{H}}}\\\\ &{=-\\lambda\\left\\langle\\phi(w),M y\\right\\rangle_{\\mathcal{H}}\\left\\langle\\phi(w),-M\\cdot\\phi(w^{\\prime})\\phi(w^{\\prime})^{*}\\cdot M y\\right\\rangle_{\\mathcal{H}}}\\\\ &{=\\lambda\\ \\langle\\phi(w),M y\\rangle_{\\mathcal{H}}\\ \\ \\langle\\phi(w),M\\phi(w^{\\prime})\\rangle_{\\mathcal{H}}\\ \\ \\langle\\phi(w^{\\prime}),M y\\rangle_{\\mathcal{H}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "As a result, ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{w}J_{\\lambda}^{\\prime\\prime}[\\eta](w,w^{\\prime})=\\lambda\\left\\langle\\phi(w^{\\prime}),M y\\right\\rangle_{\\mathcal{H}}.}\\\\ &{\\qquad\\qquad\\qquad\\left(\\left\\langle\\nabla\\phi(w),M y\\right\\rangle\\cdot\\left\\langle\\phi(w),M\\phi(w^{\\prime})\\right\\rangle_{\\mathcal{H}}+\\left\\langle\\phi(w),M y\\right\\rangle\\cdot\\left\\langle\\nabla\\phi(w),M\\phi(w^{\\prime})\\right\\rangle_{\\mathcal{H}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "and, using again that $\\|M\\|_{\\mathrm{op}}=\\|(K_{\\eta}+\\lambda)^{-1}\\|_{\\mathrm{op}}\\leq\\lambda^{-1}$ , ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla_{w}J_{\\lambda}^{\\prime\\prime}(w,w^{\\prime})\\|_{w}\\leq\\lambda B_{0}\\lambda^{-1}\\left\\|y\\right\\|_{\\mathcal{H}}\\cdot2B_{0}^{2}B_{1}\\lambda^{-2}\\left\\|y\\right\\|_{\\mathcal{H}}=2\\lambda^{-2}B_{0}^{3}B_{1}\\left\\|y\\right\\|_{\\mathcal{H}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Then applying Lem. D.8 shows the second inequality. ", "page_idx": 38}, {"type": "text", "text": "Furthermore, for a fixed $w\\in\\mathcal{W}$ , continuing from the expression of $\\nabla_{w}J_{\\lambda}^{\\prime\\prime}[\\eta](w,w^{\\prime})$ derived above, ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{T}_{w^{\\prime}}\\nabla_{w}J_{\\lambda}^{\\prime\\prime}[\\eta](w,w^{\\prime})}\\\\ &{=\\lambda\\left\\langle\\nabla\\phi(w^{\\prime}),M y\\right\\rangle_{\\mathcal{H}}\\cdot\\left(\\left\\langle\\nabla\\phi(w),M y\\right\\rangle\\cdot\\left\\langle\\phi(w),M\\phi(w^{\\prime})\\right\\rangle_{\\mathcal{H}}+\\left\\langle\\phi(w),M y\\right\\rangle\\cdot\\left\\langle\\nabla\\phi(w),M\\phi(w^{\\prime})\\right\\rangle_{\\mathcal{H}}\\right)}\\\\ &{+\\,\\lambda\\left\\langle\\phi(w^{\\prime}),M y\\right\\rangle_{\\mathcal{H}}\\cdot\\left(\\left\\langle\\nabla\\phi(w),M y\\right\\rangle\\cdot\\left\\langle\\phi(w),M\\nabla\\phi(w^{\\prime})\\right\\rangle_{\\mathcal{H}}+\\left\\langle\\phi(w),M y\\right\\rangle\\cdot\\left\\langle\\nabla\\phi(w),M\\nabla\\phi(w^{\\prime})\\right\\rangle_{\\mathcal{H}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "so $\\begin{array}{r l r}{\\|\\nabla_{w^{\\prime}}\\nabla_{w}J_{\\lambda}^{\\prime\\prime}[\\eta](w,w^{\\prime})\\|}&{{}\\leq}&{4\\lambda^{-2}B_{0}^{2}B_{1}^{2}\\,\\|y\\|_{\\mathcal H}^{2}}\\end{array}$ , and the third inequality follows by applying Lem. D.8 to $\\eta\\mapsto\\langle s,\\nabla J_{\\lambda}^{\\prime}[\\eta](w)\\rangle_{w}$ for $s\\in T_{w}\\mathcal{W}$ arbitrary. ", "page_idx": 38}, {"type": "text", "text": "Finally, by differentiating the expression of $\\nabla_{w^{\\prime}}\\nabla_{w}J_{\\lambda}^{\\prime\\prime}[\\eta](w,w^{\\prime})$ once more with respect to $w$ we get that, for any fixed $w\\in\\mathcal{W}$ , ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{T}_{w^{\\prime}}\\nabla_{w}^{2}J_{\\lambda}^{\\prime\\prime}[\\eta](w,w^{\\prime})}\\\\ &{=\\lambda\\left\\langle\\nabla\\phi(w^{\\prime}),M y\\right\\rangle_{\\mathcal{H}}\\cdot\\left(\\left\\langle\\nabla^{2}\\phi(w),M y\\right\\rangle\\cdot\\left\\langle\\phi(w),M\\phi(w^{\\prime})\\right\\rangle_{\\mathcal{H}}+\\left\\langle\\nabla\\phi(w),M y\\right\\rangle\\cdot\\left\\langle\\nabla\\phi(w),M\\phi(w^{\\prime})\\right\\rangle_{\\mathcal{H}}\\right.}\\\\ &{\\left.\\lambda\\left\\langle\\nabla\\phi(w^{\\prime}),M y\\right\\rangle_{\\mathcal{H}}\\cdot\\left(\\left\\langle\\nabla\\phi(w),M y\\right\\rangle\\cdot\\left\\langle\\nabla\\phi(w),M\\phi(w^{\\prime})\\right\\rangle_{\\mathcal{H}}+\\left\\langle\\phi(w),M y\\right\\rangle\\cdot\\left\\langle\\nabla^{2}\\phi(w),M\\phi(w^{\\prime})\\right\\rangle_{\\mathcal{H}}\\right.\\right.}\\\\ &{\\left.\\left.\\lambda\\left\\langle\\phi(w^{\\prime}),M y\\right\\rangle_{\\mathcal{H}}\\cdot\\left(\\left\\langle\\nabla^{2}\\phi(w),M y\\right\\rangle\\cdot\\left\\langle\\phi(w),M\\nabla\\phi(w^{\\prime})\\right\\rangle_{\\mathcal{H}}+\\left\\langle\\nabla\\phi(w),M y\\right\\rangle\\cdot\\left\\langle\\nabla\\phi(w),M\\nabla\\phi(w^{\\prime})\\right\\rangle_{\\mathcal{H}}\\right.\\right.}\\\\ &{\\left.\\left.\\lambda\\left\\langle\\phi(w^{\\prime}),M y\\right\\rangle_{\\mathcal{H}}\\cdot\\left(\\left\\langle\\nabla\\phi(w),M y\\right\\rangle\\cdot\\left\\langle\\nabla\\phi(w),M\\nabla\\phi(w^{\\prime})\\right\\rangle_{\\mathcal{H}}+\\left\\langle\\phi(w),M y\\right\\rangle\\cdot\\left\\langle\\nabla^{2}\\phi(w),M\\nabla\\phi(w^{\\prime})\\right\\rangle_{\\mathcal{H}}\\right.\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "hence $\\left\\|\\nabla_{w^{\\prime}}\\nabla_{w}^{2}J_{\\lambda}^{\\prime\\prime}[\\eta](w,w^{\\prime})\\right\\|\\le\\lambda^{-2}\\left\\|y\\right\\|^{2}B_{0}B_{1}(4B_{2}B_{0}+4B_{1}^{2})$ , and the fourth inequality follows by applying Lem. D.8 to $\\eta\\mapsto\\langle s,\\nabla^{2}J_{\\lambda}^{\\prime}[\\eta](w)\\cdot s\\rangle_{w}$ for $s\\in T_{w}\\mathcal{W}$ arbitrary. \u53e3 ", "page_idx": 38}, {"type": "text", "text": "The following lemma provides explicit upper estimates of the regularity constants $B_{0},B_{1},B_{2}$ of $\\phi$ appearing in Prop. F.2, in terms of the activation function $\\varphi$ and the data distribution $\\rho$ . ", "page_idx": 38}, {"type": "text", "text": "Lemma F.3. Under Assumption 2, recall that $\\phi:\\mathcal{W}\\to\\mathcal{H}=L_{\\rho}^{2}(\\mathbb{R}^{d+1})$ is defined by $\\phi(w)(x)=$ $\\varphi(\\langle w,x\\rangle)$ , and that $\\varphi:\\mathbb{R}\\rightarrow\\mathbb{R}$ is $\\mathcal{C}^{2}$ . There exists a universal constant $c>0$ such that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{w\\in\\mathbb{S}^{d}}{\\operatorname*{sup}}\\,\\|\\phi(w)\\|_{\\mathcal{H}}\\leq\\|\\varphi\\|_{L^{2}(\\rho)}\\,,}\\\\ &{\\underset{w\\in\\mathbb{S}^{d}}{\\operatorname*{sup}}\\,\\|\\nabla\\phi(w)\\|_{\\mathcal{H}}\\leq\\|\\varphi^{\\prime}\\|_{L^{4}(\\rho)}\\,N_{4}(\\rho),}\\\\ &{\\underset{w\\in\\mathbb{S}^{d}}{\\operatorname*{sup}}\\left\\|\\nabla^{2}\\phi(w)\\right\\|_{\\mathcal{H}}\\leq\\left(\\|\\varphi^{\\prime\\prime}\\|_{L^{4}(\\rho)}+\\|\\varphi^{\\prime}\\|_{L^{4}(\\rho)}\\right)N_{4}(\\rho)\\,}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathrm{V}_{4}(\\rho):=\\operatorname*{sup}_{\\|u\\|_{2}\\leq1}\\Big(\\mathbb{E}_{x\\sim\\rho}\\left\\langle u,x\\right\\rangle^{4}\\Big)^{1/4}\\quad a n d\\quad\\forall f:\\mathbb{R}\\rightarrow\\mathbb{R},\\ \\|f\\|_{L^{p}(\\rho)}:=\\operatorname*{sup}_{w\\in\\mathbb{S}^{d}}\\left(\\mathbb{E}_{x\\sim\\rho}\\left|f(\\left\\langle w,x\\right\\rangle)\\right|^{p}\\right)^{1/p}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Note that if $\\rho$ is rotationally invariant, then $\\mathbb{E}_{x\\sim\\rho}\\,|f(\\langle w,x\\rangle)|^{p}$ is independent of $w$ , and there exists $a$ universal constant c such that $N_{4}(\\rho)\\leq c d^{-1/2}\\bigl(\\mathbb{E}_{x\\sim\\rho}\\left\\|x\\right\\|^{4}\\bigr)^{1/4}$ . ", "page_idx": 38}, {"type": "text", "text": "Proof. For the first inequality, we have by definition ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{w}\\|\\varphi(w)\\|_{\\mathcal{H}}=\\operatorname*{sup}_{w}\\sqrt{\\mathbb{E}_{x\\sim\\rho}\\left|\\varphi(\\langle w,x\\rangle)\\right|^{2}}=\\|\\varphi\\|_{L^{2}(\\rho)}\\,.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "For the second inequality, define the orthogonal projector $\\Pi_{w}=I_{d+1}-w w^{\\top}:\\mathbb{R}^{d+1}\\rightarrow T_{w}\\mathbb{S}^{d}=$ $\\{w\\}^{\\perp}$ for any $w\\in\\dot{\\mathbb{S}}^{d}$ . Then $[\\nabla\\phi(w)]\\,(x)\\,\\bar{=}\\,\\varphi^{\\prime}(\\bar{\\langle w,x\\rangle})\\Pi_{w}x$ , so by Cauchy-Schwarz inequalities, ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla\\phi(w)\\|_{\\mathcal{H}}=\\Bigl(\\underset{\\|f\\|_{L^{2}(\\rho)}\\leq1}{\\operatorname*{sup}}\\underset{\\|s\\|_{w}=1}{\\operatorname*{sup}}\\mathbb{E}_{x\\sim\\rho}\\left[f(x)\\left\\langle s,\\nabla\\phi(w)(x)\\right\\rangle_{w}\\right]\\Bigr)}\\\\ &{=\\underset{s\\in T_{w}\\mathbb{S}^{d}}{\\operatorname*{sup}}\\mathbb{E}_{x\\sim\\rho}\\left[\\left\\langle s,\\nabla\\phi(w)(x)\\right\\rangle_{w}^{2}\\right]^{1/2}=\\underset{s\\in T_{w}\\mathbb{S}^{d}}{\\operatorname*{sup}}\\mathbb{E}_{x\\sim\\rho}\\left[\\left\\vert\\varphi^{\\prime}(\\left\\langle w,x\\right\\rangle)\\right\\vert^{2}\\left\\langle\\Pi_{w}s,x\\right\\rangle^{2}\\right]^{1/2}}\\\\ &{\\leq\\left(\\mathbb{E}_{x\\sim\\rho}\\left[\\left\\vert\\varphi^{\\prime}(\\left\\langle w,x\\right\\rangle)\\right\\vert^{4}\\right]\\right)^{1/4}\\cdot\\underset{\\|u\\|_{\\rho}=1}{\\operatorname*{sup}}\\left(\\mathbb{E}_{x\\sim\\rho}\\left[\\left\\langle u,x\\right\\rangle^{4}\\right]\\right)^{1/4}}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "since $\\left\\|s\\right\\|_{w}=\\left\\|\\Pi_{w}s\\right\\|_{2}$ . ", "page_idx": 39}, {"type": "text", "text": "For the third inequality, the Riemannian Hessian of $\\phi(w)=\\varphi(\\langle w,\\cdot\\rangle):\\mathbb{S}^{d}\\to\\mathbb{R}$ is given by ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nabla^{2}\\phi(w)\\big]\\,(x)=\\nabla_{w}^{2}\\varphi(\\langle w,x\\rangle)=\\nabla_{w}^{\\top}\\left[\\varphi^{\\prime}(\\langle w,x\\rangle)\\Pi_{w}x\\right]=\\Pi_{w}\\left(\\varphi^{\\prime\\prime}(\\langle w,x\\rangle)x x^{\\top}-\\varphi^{\\prime}(\\langle w,x\\rangle)\\,\\langle w,x\\rangle\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "so similarly by Cauchy-Schwarz inequalities, ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\left\\|\\nabla^{2}\\phi(w)\\right\\|_{\\mathcal{H}}\\leq\\operatorname*{sup}_{s\\in T_{w}\\leq t}\\mathbb{E}_{x\\sim\\rho}\\left[\\left|\\varphi^{\\prime\\prime}(\\langle w,x\\rangle)\\right|^{2}\\langle s,\\Pi_{w}x\\rangle^{2}\\right]^{1/2}+\\mathbb{E}_{x\\sim\\rho}\\left[\\left|\\varphi^{\\prime}(\\langle w,x\\rangle)\\right|^{2}\\langle w,x\\rangle^{2}\\right]^{1/2}}}\\\\ &{\\leq\\left(\\mathbb{E}_{x\\sim\\rho}\\left[\\left|\\varphi^{\\prime\\prime}(\\langle w,x\\rangle)\\right|^{4}\\right]\\right)^{1/4}\\cdot\\operatorname*{sup}_{s\\in T_{w}\\leq t}\\left(\\mathbb{E}_{x\\sim\\rho}\\left[\\left\\langle\\Pi_{w}s,x\\right\\rangle^{4}\\right]\\right)^{1/4}}\\\\ &{\\quad\\|s\\|_{w}\\!=\\!1}\\\\ &{\\quad+\\left(\\mathbb{E}_{x\\sim\\rho}\\left[\\left|\\varphi^{\\prime}(\\langle w,x\\rangle)\\right|^{4}\\right]\\right)^{1/4}\\left(\\mathbb{E}_{x\\sim\\rho}\\left[\\langle w,x\\rangle^{4}\\right]\\right)^{1/4}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Finally, suppose that $\\rho$ is rotationally invariant, and let us show that $N_{4}(\\rho)\\leq c d^{-1/2}\\bigl(\\mathbb{E}_{x\\sim\\rho}\\left\\|x\\right\\|^{4}\\bigr)^{1/4}$ for some universal constant $c$ . Indeed, for $x\\sim\\rho$ , we have that $x$ and ${\\overline{{x}}}=x/\\left\\|x\\right\\|$ are independent and that $\\overline{{x}}\\sim\\tau$ . Therefore, ", "page_idx": 39}, {"type": "equation", "text": "$$\nN_{4}^{4}(\\rho)=\\operatorname*{sup}_{\\|u\\|_{2}\\leq1}\\mathbb{E}_{x\\sim\\rho}\\left\\|x\\right\\|^{4}\\left\\langle u,x/\\left\\|x\\right\\|\\right\\rangle^{4}=\\operatorname*{sup}_{\\|u\\|_{2}\\leq1}\\left(\\mathbb{E}_{x\\sim\\rho}\\left\\|x\\right\\|^{4}\\right)\\cdot\\left(\\mathbb{E}_{\\overline{{x}}\\sim\\tau}\\left\\langle u,\\overline{{x}}\\right\\rangle^{4}\\right),\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "and $\\begin{array}{r}{\\operatorname*{sup}_{\\|u\\|_{2}\\leq1}\\mathbb{E}_{\\overline{{x}}\\sim\\tau}\\left\\langle u,\\overline{{x}}\\right\\rangle^{4}\\;\\leq\\;\\frac{c}{(d+1)^{2}}}\\end{array}$ for some universal constant $c$ , which is a direct consequence of the fact that $\\langle u,{\\overline{{x}}}\\rangle$ is sub-Gaussian with sub-Gaussian norm $\\tilde{c}/\\sqrt{d+1}$ for some universal constant $\\tilde{c}$ [Ver18, Theorem 3.4.6], along with the moment bound for sub-Gaussian random variables [Ver18, Proposition 2.5.2] \u53e3 ", "page_idx": 39}, {"type": "text", "text": "Finally, we check rigorously in the following proposition that Assumption 2 with proper additional regularity assumptions on $\\varphi$ and $\\rho$ , is a special case of Assumption 1. ", "page_idx": 39}, {"type": "text", "text": "Proposition F.4. Consider $\\mathcal{W}=\\mathbb{S}^{d}$ and $G:\\mathcal{M}(\\mathcal{W})\\rightarrow\\mathbb{R}$ defined as in Assumption 2. Suppose furthermore that $N_{4}(\\rho),\\|\\varphi\\|_{L^{2}(\\rho)}\\,,\\|\\varphi^{\\prime}\\|_{L^{4}(\\rho)}\\,,\\|\\varphi^{\\prime\\prime}\\|_{L^{4}(\\rho)}<\\infty$ , where $N_{4}(\\rho)$ and $\\left\\|\\cdot\\right\\|_{L^{p}(\\rho)}$ are defined in Lem. F.3. Then, $G$ and $\\mathcal{W}$ satisfy Assumption $^{\\,l}$ . ", "page_idx": 39}, {"type": "text", "text": "Proof. The fact that $\\mathbb{S}^{d}$ satisfies $\\alpha_{\\tau}$ -LSI with $\\alpha_{\\tau}=d-1$ is classical and can be found in [BGL14, Sec. 5.7]. ", "page_idx": 39}, {"type": "text", "text": "By definition, $\\begin{array}{r}{G(\\nu)=\\frac{1}{2}\\left\\|\\int_{\\mathcal{W}}\\phi(w)\\mathrm{d}\\nu(w)-y\\right\\|_{\\mathcal{H}}^{2}}\\end{array}$ , so $G$ is non-negative and admits second variations: for any $\\nu\\in\\mathcal{M}(\\mathcal{W})$ and $w,w^{\\prime}\\in\\mathbb{S}^{d}$ , ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{c}{G^{\\prime}[\\nu](w)=\\displaystyle\\left\\langle\\phi(w),\\int_{\\mathcal{W}}\\phi(w^{\\prime})\\mathrm{d}\\nu(w^{\\prime})-y\\right\\rangle_{\\mathcal{H}}}\\\\ {G^{\\prime\\prime}[\\nu](w,w^{\\prime})=\\langle\\phi(w),\\phi(w^{\\prime})\\rangle_{\\mathcal{H}}}\\\\ {\\mathrm{~and~}\\quad\\nabla_{w}G^{\\prime\\prime}[\\nu](w,w^{\\prime})=\\langle\\nabla\\phi(w),\\phi(w^{\\prime})\\rangle_{\\mathcal{H}}}\\\\ {\\nabla_{w}^{2}G^{\\prime\\prime}[\\nu](w,w^{\\prime})=\\langle\\nabla^{2}\\phi(w),\\phi(w^{\\prime})\\rangle_{\\mathcal{H}}}\\\\ {\\nabla_{w}\\nabla_{w^{\\prime}}G^{\\prime\\prime}[\\nu](w,w^{\\prime})=\\langle\\nabla\\phi(w),\\nabla\\phi(w^{\\prime})\\rangle_{\\mathcal{H}}\\,.}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Consequently, denoting $\\begin{array}{r}{C_{i}=\\operatorname*{sup}_{w\\in\\mathbb{S}^{d}}\\left\\|\\nabla^{i}\\phi\\right\\|_{\\mathcal{H}}}\\end{array}$ for $i\\in\\{0,1,2\\}$ , which are all finite by Lem. F.3, ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{|G^{\\prime\\prime}[\\nu](w,w^{\\prime})|\\le C_{0}^{2}=:L_{0}}\\\\ &{}&{\\|\\nabla_{w}G^{\\prime\\prime}[\\nu](w,w^{\\prime})\\|_{w}\\le C_{0}C_{1}=:L_{1}}\\\\ &{}&{\\|\\nabla_{w}^{2}G^{\\prime\\prime}[\\nu](w,w^{\\prime})\\|_{w}\\le C_{0}C_{2}=:L_{2}}\\\\ &{}&{\\|\\nabla_{w}\\nabla_{w^{\\prime}}G^{\\prime\\prime}[\\nu](w,w^{\\prime})\\|\\le C_{1}^{2}=:\\widetilde{L}_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Now for each $i\\in\\{0,1,2\\}$ , ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left./(\\nu,w,w^{\\prime}),\\lVert\\nabla_{w}^{i}G^{\\prime\\prime}[\\nu](w,w^{\\prime})\\rVert_{w}\\leq L_{i}\\implies\\forall(\\nu,\\nu^{\\prime},w),\\lVert\\nabla^{i}G^{\\prime}[\\nu]-\\nabla^{i}G^{\\prime}[\\nu^{\\prime}]\\rVert_{w}\\leq L_{i}\\lVert\\nu-\\nu^{\\prime}\\rVert_{T V}}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Indeed, the right-hand side can be shown by applying the mean-value theorem to $g(\\theta)\\ =$ $\\big<s,\\nabla^{i}G^{\\prime}[\\nu+\\theta(\\nu^{\\prime}-\\nu)](w)\\big>_{w}$ over $\\theta\\,\\in\\,[0,1]$ for each $s\\,\\in\\,(T_{w}{\\mathcal{W}})^{\\otimes i}$ . Thus, to show the existence of $B_{i}<\\infty$ such that $\\begin{array}{r}{\\check{\\forall}(\\nu,w,w^{\\prime}),\\|\\nabla^{i}G^{\\prime}[\\nu]\\|_{w}\\leq L_{i}\\,\\|\\nu\\|_{T V}+B_{i}}\\end{array}$ , it suffices to check that there exists $\\nu_{0}$ such that $\\|\\nu_{0}\\|_{T V}$ and $\\mathrm{sup}_{w}\\left\\|\\nabla^{i}G^{\\prime}[\\nu_{0}]\\right\\|_{w}<\\infty$ . Note that for any $\\nu$ and $w$ , ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla^{i}G^{\\prime}[\\nu](w)=\\biggl\\langle\\nabla^{i}\\phi(w),\\displaystyle\\int_{\\mathcal{W}}\\phi(w^{\\prime})\\mathrm{d}\\nu(w^{\\prime})-y\\biggr\\rangle_{\\mathcal{H}},}\\\\ &{\\quad\\mathrm{thus}\\quad\\nabla^{i}G^{\\prime}[0](w)=-\\left\\langle\\nabla^{i}\\phi(w),y\\right\\rangle_{\\mathcal{H}}\\qquad}\\\\ {\\mathrm{and}}&{\\underset{w}{\\operatorname*{sup}}\\left\\|\\nabla^{i}G^{\\prime}[0](w)\\right\\|_{w}\\leq C_{i}\\left\\|y\\right\\|_{\\mathcal{H}}<\\infty.}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Hence the existence of the $B_{i}<\\infty$ is verified. This finishes the verification of Assumption 1. \u53e3 ", "page_idx": 40}, {"type": "text", "text": "F.2 Proof of Thm. 5.2 ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "In the single-index setting of Assumption 3, it is intuitive that $\\delta_{v}$ is a minimizer of $J_{\\lambda}$ , for any $\\lambda\\geq0$ , and that $\\eta_{\\lambda,\\beta}$ and $\\delta_{v}$ are close in certain regimes of $\\beta$ and $\\lambda$ . For this reason, we will first investigate the properties of $J_{\\lambda}^{\\prime}[\\delta_{v}]$ as a proxy of $J_{\\lambda}^{\\prime}[\\bar{\\eta}_{\\lambda,\\beta}]$ , to show that it is amenable to a refined analysis for proving LSI, in Sec. F.2.1. This step uses a Lyapunov approach inspired by [MS14; LE23]. We will then prove that these properties carry from $J_{\\lambda}^{\\prime}[\\delta_{v}]$ over to ${\\cal J}_{\\lambda}^{\\prime}[\\eta_{\\lambda,\\beta}]$ , in Sec. F.2.2, thanks to a quantitative bound on $W_{2}(\\bar{\\eta}_{\\lambda,\\bar{\\beta}},\\delta_{v})$ proved in Sec. F.2.3. ", "page_idx": 40}, {"type": "text", "text": "Lemma F.5. Under Assumptions 2 and $^3$ , we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\forall w\\in\\mathbb{S}^{d},J_{\\lambda}^{\\prime}[\\delta_{v}](w)=-\\frac{\\lambda}{2}\\left(\\lambda+\\|\\phi(v)\\|_{\\mathcal{H}}^{2}\\right)^{-2}\\langle\\phi(v),\\phi(w)\\rangle_{\\mathcal{H}}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=-\\frac{\\lambda}{2}\\left(\\lambda+\\|\\varphi\\|_{L^{2}(\\rho)}^{2}\\right)^{-2}\\left|\\mathbb{E}_{x\\sim\\rho}\\varphi(\\langle x,v\\rangle)\\varphi(\\langle x,w\\rangle)\\right|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=-\\lambda g(\\langle v,w\\rangle)}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "for some $g:[-1,+1]\\rightarrow\\mathbb{R}.$ . ", "page_idx": 40}, {"type": "text", "text": "Proof. By Prop. F.1, since $y=\\phi(v)$ , ", "page_idx": 40}, {"type": "equation", "text": "$$\nJ_{\\lambda}^{\\prime}[\\delta_{v}]=-\\frac{\\lambda}{2}\\left\\langle\\phi(w),(K_{\\delta_{v}}+\\lambda\\,\\mathrm{id})^{-1}\\phi(v)\\right\\rangle_{\\mathcal{H}}^{2}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Since $\\phi(v)$ is an eigenvector of $\\begin{array}{r}{K_{\\delta_{v}}\\ =\\ \\int_{\\mathcal W}\\phi(w^{\\prime})\\phi(w^{\\prime})^{*}\\mathrm{d}\\delta_{v}\\ =\\ \\phi(v)\\phi(v)^{*}}\\end{array}$ with eigenvalue $\\left\\|\\phi(v)\\right\\|_{\\mathcal{H}}^{2}=\\mathbb{E}_{x\\sim\\rho}\\varphi(\\langle x,v\\rangle)^{2}=\\left\\|\\varphi\\right\\|_{L^{2}(\\rho)}^{2},$ , it is also an eigenvector of $(K_{\\delta_{v}}+\\lambda\\,\\mathrm{id})^{-1}$ with eigenvalue $(\\|\\varphi(v)\\|_{\\mathcal H}^{2}+\\lambda)^{-1}$ , whence the expression of $J_{\\lambda}^{\\prime}[\\delta_{v}]$ follows. ", "page_idx": 41}, {"type": "text", "text": "Moreover, by rotational invariance of $\\rho$ , $\\mathbb{E}_{x\\sim\\rho}\\varphi(\\langle x,v\\rangle)\\varphi(\\langle x,w\\rangle)$ depends only on $\\langle v,w\\rangle$ , for all $w\\in\\mathbb{S}^{d}$ . In other words, there exists $g$ such that $J_{\\lambda}^{\\prime}[\\delta_{v}]=-\\lambda g(\\langle v,\\cdot\\rangle)$ . \u53e3 ", "page_idx": 41}, {"type": "text", "text": "F.2.1 Lyapunov function analysis for bounding the LSI constant of $\\widehat{\\delta_{v}}\\propto e^{-\\beta J_{\\lambda}^{\\prime}[\\delta_{v}]}\\tau$ ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Observe that by the assumption $g^{\\prime}\\geq c_{1}>0$ of Thm. 5.2, $J_{\\lambda}^{\\prime}[\\delta_{v}]=-\\lambda g(\\langle v,\\cdot\\rangle)$ has a unique global minimum at $v$ . Moreover, our other assumptions on $g$ will imply that the Riemannian Hessian at optimum $\\nabla^{2}\\,J_{\\lambda}^{\\prime}[\\delta_{v}](v)$ is positive definite. This motivates us to follow the strategy of [LE23, Thm. 3.4] for proving LSI for $\\widehat{\\delta_{v}}\\propto e^{-\\beta J_{\\lambda}^{\\prime}[\\delta_{v}]}\\tau.$ . Let us first outline the strategy and recall some useful classical notions. ", "page_idx": 41}, {"type": "text", "text": "The generator of the Langevin diffusion with invariant measure $\\exp(-\\beta f)\\tau/Z$ is ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}=\\Delta-\\beta\\langle\\nabla f,\\nabla\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Define $U\\,=\\,\\{w\\,:\\,\\mathrm{dist}_{w}(w,v)\\,\\le\\,r\\}$ for some $\\boldsymbol{v}\\in\\mathbb{S}^{d}$ , with $r\\,>\\,0$ to be chosen later. We say $W:\\mathbb{S}^{d}\\rightarrow[1,\\infty)$ is a Lyapunov function if $\\begin{array}{r}{\\frac{\\mathcal{L}W}{W}\\leq-\\theta+b\\mathbf{1}_{U}}\\end{array}$ , for constants $\\theta>0$ and $b\\geq0$ . When proving functional inequalities for a Gibbs measure $\\exp(-\\beta f)\\tau/Z$ , a typical choice of Lyapunov function is $W=\\exp({\\bar{\\beta(f-\\operatorname*{min}f)}}/{2})$ , for which the Lyapunov condition writes ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\frac{\\beta\\Delta f}{2}-\\frac{\\beta^{2}\\left\\|\\nabla f\\right\\|^{2}}{4}\\leq-\\theta+b{\\bf1}_{U}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Further, we say a probability measure $\\nu\\in\\mathcal{P}(\\mathbb{S}^{d})$ satisfies a local Poincar\u00e9 inequality on $U$ with constant $\\kappa_{U}$ if ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\int_{U}f^{2}\\mathrm{d}\\nu\\leq\\frac{1}{\\kappa_{U}}\\int_{U}\\left\\|\\nabla f\\right\\|^{2}\\mathrm{d}\\nu,\\quad\\mathrm{for~all~smooth~}f:U\\rightarrow\\mathbb{R}\\mathrm{~such~that~}\\int_{U}f\\mathrm{d}\\nu=0.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Notice that $U$ has a convex boundary, thus we can use the Bakry-\u00c9mery criterion as adapted to manifolds with convex boundaries by [LE23, Proposition B.11] to prove a local Poincar\u00e9 inequality on $U$ . Specifically, it suffices to have $\\operatorname*{inf}_{w\\in U}\\lambda_{\\operatorname*{min}}\\mathring{(}\\nabla^{2}f(w))>0$ . ", "page_idx": 41}, {"type": "text", "text": "In summary, a Lyapunov condition of the form (F.4), along with a control on the eigenspectrum of $\\nabla^{2}f(w)$ , implies an LSI for $e^{-\\beta f}\\tau/Z$ . We record this fact in the theorem below, working out the proper dependence on problem parameters for future use. ", "page_idx": 41}, {"type": "text", "text": "Theorem F.6. Let $v\\in\\mathbb{S}^{d}$ , $0<\\lambda\\leq1$ and $f:\\mathbb{S}^{d}\\to\\mathbb{R}$ of the form $f(w)=-\\lambda g(\\langle w,v\\rangle)$ for some increasing function $g:[-1,1]\\to\\mathbb{R}.$ . Suppose there exist constants $D_{0},D_{1},D_{2},D_{3},D_{4}>0$ , and $r\\in(0,\\pi/2)$ such that if $\\dot{\\boldsymbol\\beta}\\geq\\dot{\\boldsymbol D}_{0}d\\lambda^{-1}$ then ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\forall w\\in\\mathbb{S}^{d},\\ \\frac{1}{2}\\Delta f-\\frac{\\beta}{4}\\left\\|\\nabla f\\right\\|^{2}\\leq D_{1}\\lambda d}\\\\ &{\\quad\\forall w\\in\\mathbb{S}^{d}\\setminus U,\\ \\frac{1}{2}\\Delta f-\\frac{\\beta}{4}\\left\\|\\nabla f\\right\\|^{2}\\leq-D_{2}\\beta\\lambda^{2}}\\\\ &{\\quad\\quad\\forall w\\in\\mathbb{S}^{d},\\ \\lambda_{\\operatorname*{min}}(\\nabla^{2}f(w))\\geq-D_{3}\\lambda}\\\\ &{\\quad\\quad\\forall w\\in U,\\ \\lambda_{\\operatorname*{min}}(\\nabla^{2}f(w))\\geq D_{4}\\lambda}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where $U~=~\\{w\\in\\mathbb{S}^{d}$ ; $\\mathrm{dist}_{\\mathcal{W}}(w,v)\\leq r\\rbrace$ . Then (provided that $\\beta\\ \\geq\\ D_{0}d\\lambda^{-1},$ ) the probability measure $\\nu=\\exp(-\\beta f)\\tau/Z$ satisfies $\\alpha$ -LSI for a constant $\\alpha$ dependent only on the $D_{i}$ and on $r$ . Furthermore, if the condition on $\\beta$ is replaced by $\\beta\\geq D_{0}^{\\prime}d^{4}\\lambda^{-4}$ and $i f(\\mathrm{L}_{\\mathbb{S}^{d}})$ is replaced by ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\forall w\\in\\mathbb{S}^{d},\\,\\,\\,\\frac{1}{2}\\Delta f-\\frac{\\beta}{4}\\left\\|\\nabla f\\right\\|^{2}\\leq D_{1}^{\\prime}\\lambda d\\beta^{3/4},\n$$", "text_format": "latex", "page_idx": 41}, {"type": "equation", "text": "$$\n(\\mathrm{L}_{\\mathbb{S}^{d}}^{\\prime})\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "then (provided that $\\beta\\geq D_{0}^{\\prime}d^{4}\\lambda^{-4},$ ) $\\nu$ satisfies $\\alpha^{\\prime}$ -LSI for a constant $\\alpha^{\\prime}$ dependent only on $D_{0}^{\\prime},D_{1}^{\\prime}$ , $D_{2},D_{3},D_{4}$ and on $r$ . ", "page_idx": 41}, {"type": "text", "text": "Proof. By the Lyapunov criterion for Poincar\u00e9 inequality [BGL14, Thm. 4.6.2], if the generator $\\mathcal{L}$ given by (F.3) satisfies the Lyapunov condition LWW \u2264\u2212\u03b8 +b1U for some \u03b8 > 0, b \u22650, U \u2282Sd and $W:\\mathbb{S}^{d}\\rightarrow\\mathbb{R}$ , and if $\\nu$ satisfies a local Poincar\u00e9 inequality on $U$ with constant $\\kappa_{U}$ , then $\\nu$ satisfies a Poincar\u00e9 inequality on $\\mathbb{S}^{d}$ with constant $\\begin{array}{r}{\\kappa\\geq\\frac{\\theta}{1+\\frac{b}{\\kappa_{U}}}}\\end{array}$ ", "page_idx": 42}, {"type": "text", "text": "Let us apply this to $W=\\exp(\\beta(f-\\operatorname*{min}f)/2)$ . By $(\\mathrm{L}_{\\mathbb{S}^{d}})$ and $(\\mathrm{L}_{U})$ , the Lyapunov condition holds with $\\theta=\\dot{D_{2}}\\beta^{2}\\lambda^{2}$ and $b=D_{1}\\lambda\\beta(d-1)+D_{2}\\beta^{2}\\lambda^{2}$ . Moreover, since $U$ has a convex boundary (the geodesic in $\\mathbb{S}^{d}$ between any two points in $U$ remains in $U$ for $r<\\pi/2$ ), by [LE23, Propostion B.11] $\\nu$ satisfies a local Poincar\u00e9 inequality on $U$ with constant ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\kappa_{U}\\geq\\mathrm{Ric}_{g}+\\beta\\lambda_{\\operatorname*{min}}(\\nabla^{2}f(w))\\geq d-1+\\beta\\lambda D_{4}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where $\\operatorname{Ric}_{g}$ denotes the Ricci curvature of $\\mathbb{S}^{d}$ . As a result, $\\nu$ satisfies Poincar\u00e9 inequality with constant ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\kappa\\geq\\frac{D_{2}\\beta^{2}\\lambda^{2}}{1+\\frac{D_{1}\\lambda\\beta d+D_{2}\\beta^{2}\\lambda^{2}}{d-1+\\beta\\lambda D_{4}}}\\geq C\\beta\\lambda,\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "for some constant $C$ depending only on the $D_{i}$ , where we used that $\\beta\\geq D_{0}d\\lambda^{-1}$ . ", "page_idx": 42}, {"type": "text", "text": "Moreover, by [LE23, Proposition 9.17], if $\\nu\\in\\mathcal{P}(\\mathbb{S}^{d})$ satisfies the Poincar\u00e9 inequality with constant $\\kappa$ , and $\\beta\\nabla^{2}f+\\mathrm{Ric}_{g}\\succcurlyeq-\\beta K$ for some $K>0$ on $\\mathbb{S}^{d}$ , then for $\\beta\\geq1$ , $\\nu$ satisfies the LSI with constant $\\begin{array}{r}{\\alpha=\\frac{\\kappa}{11\\beta K}}\\end{array}$ . By the assumptions of the theorem, this indeed holds with $K=D_{3}\\lambda$ . Consequently, $\\nu$ satisfies LSI with constant $\\alpha=C/(11D_{3})$ , which finishes the proof of the first part of the theorem. ", "page_idx": 42}, {"type": "text", "text": "The second part, with $(\\mathrm{L}_{\\mathbb{S}^{d}}^{\\prime})$ instead of $(\\mathrm{L}_{\\mathbb{S}^{d}})$ , follows by a similar reasoning, except that \u201c $\\,^{\\cdot}D_{1}\\,^{\\cdot}$ should be replaced by \u201c $\\cdot D_{1}^{\\prime}\\beta^{3/4,\\bar{\\bullet}}$ in the calculation of (F.5). This still leads to a bound of the form $\\kappa\\geq C^{\\prime}\\beta\\lambda$ provided that $\\beta\\geq D_{0}^{\\prime}d^{4}\\lambda^{-4}$ , and the rest of the proof follows without change. \u53e3 ", "page_idx": 42}, {"type": "text", "text": "We now verify that $J_{\\lambda}^{\\prime}[\\delta_{v}]$ satisfies the conditions of Thm. F.6. ", "page_idx": 42}, {"type": "text", "text": "Proposition F.7. Under the assumptions of Thm. 5.2, $f_{0}:=J_{\\lambda}^{\\prime}[\\delta_{v}]$ satisfies the conditions of Thm. F.6 with $D_{0},...,D_{4},r$ dependent only on $c_{1},C_{1},C_{2},C_{3}$ . ", "page_idx": 42}, {"type": "text", "text": "Proof. The Riemannian gradient and Hessian of $f_{0}=J_{\\lambda}^{\\prime}[\\delta_{v}]=-\\lambda g(\\langle v,\\cdot\\rangle)$ are given by ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla f_{0}(w)=-\\lambda g^{\\prime}(\\langle w,v\\rangle)\\Pi_{w}v}\\\\ {\\mathrm{and}\\;}&{\\nabla^{2}\\;f_{0}(w)=-\\lambda\\Pi_{w}\\left(g^{\\prime\\prime}(\\langle w,v\\rangle)v v^{\\top}-g^{\\prime}(\\langle w,v\\rangle)\\left\\langle w,v\\right\\rangle I_{d+1}\\right)\\Pi_{w}}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where $\\Pi_{w}\\,=\\,I_{d+1}\\,-\\,w w^{\\top}\\,:\\,\\mathbb{R}^{d+1}\\,\\rightarrow\\,T_{w}\\mathbb{S}^{d}\\,=\\,\\{w\\}^{\\perp}$ for any $w\\,\\in\\,\\mathbb{S}^{d}$ . This can be shown by considering the smooth extension of $f_{0}$ to $\\mathbb{R}^{d+1}\\rightarrow\\mathbb{R}$ defined by $x\\mapsto-\\lambda g(\\langle v,x\\rangle)$ and using that $\\mathbb{S}^{d}$ is a sub-Riemannian manifold of $\\mathbb{R}^{d+1}$ [Bou23, Chap. 5]. In particular since $v^{\\top}\\Pi_{w}\\Pi_{w}v=1\\!-\\!\\langle w,v\\rangle^{2}$ and ${\\mathrm{Tr}}\\,{\\Pi_{w}}=d$ , ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\left\\|\\nabla f_{0}(w)\\right\\|^{2}=\\lambda^{2}g^{\\prime}(\\langle w,v\\rangle)^{2}(1-\\langle w,v\\rangle^{2})}}\\\\ {{\\Delta f_{0}(w)=\\operatorname{Tr}\\nabla^{2}f_{0}(w)=-\\lambda\\left(g^{\\prime\\prime}(\\langle w,v\\rangle)(1-\\langle w,v\\rangle^{2})-g^{\\prime}(\\langle w,v\\rangle)\\,\\langle w,v\\rangle\\,d\\right).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Pose $U=\\left\\{w\\in\\mathbb{S}^{d}:\\mathrm{dist}_{\\mathbb{S}^{d}}(w,v)\\leq r\\right\\}$ for some $r>0$ to be chosen. ", "page_idx": 42}, {"type": "text", "text": "Let us verify $(\\mathrm{L}_{\\mathbb{S}^{d}})$ ). We have for all $w\\in\\mathbb{S}^{d}$ ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\frac{1}{2}\\Delta f_{0}-\\frac{\\beta}{4}\\left\\|\\nabla f_{0}\\right\\|^{2}=-\\frac{\\lambda}{4}\\left(2g^{\\prime\\prime}(\\langle w,v\\rangle)+\\beta\\lambda g^{\\prime}(\\langle w,v\\rangle)^{2}\\right)\\,\\left(1-\\langle w,v\\rangle^{2}\\right)+\\frac{\\lambda}{2}g^{\\prime}(\\langle w,v\\rangle)\\langle w,v\\rangle d\\lambda.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "The second term is bounded by $\\textstyle{\\frac{\\lambda}{2}}C_{1}d$ . We can ensure that the first term is non-positive by appropriately restricting $\\beta$ as follows: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{[-1,1]}{\\operatorname*{inf}}\\left[2g^{\\prime\\prime}+\\beta\\lambda(g^{\\prime})^{2}\\right]\\ge0\\iff2(\\operatorname*{inf}g^{\\prime\\prime})+\\beta\\lambda(\\operatorname*{inf}g^{\\prime})^{2}\\ge0}\\\\ &{\\qquad\\qquad\\qquad\\iff-2C_{2}+\\beta\\lambda c_{1}^{2}\\ge0\\iff\\beta\\ge\\frac{2C_{2}}{c_{1}^{2}}\\lambda^{-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Let us verify $(\\operatorname{L}_{U})$ . We can upper-bound the first term in (F.6) by a negative quantity by restricting $\\beta$ further: by a similar calculation as just above, ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\beta\\geq\\frac{4C_{2}}{c_{1}^{2}\\lambda}\\implies\\underset{[-1,1]}{\\operatorname*{inf}}\\left[2g^{\\prime\\prime}+\\frac{\\beta}{2}\\lambda(g^{\\prime})^{2}\\right]\\geq0\\implies2g^{\\prime\\prime}+\\beta\\lambda(g^{\\prime})^{2}\\geq\\frac{\\beta}{2}\\lambda(g^{\\prime})^{2}\\implies\\mathrm{over}\\ [-1,1].\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Then for all $w\\in\\mathbb{S}^{d}\\setminus U$ , we have $\\begin{array}{r}{r\\leq\\mathrm{dist}_{\\mathcal{W}}(w,v)=\\operatorname{arccos}(\\langle w,v\\rangle)\\leq\\frac{\\pi}{2}\\sqrt{1-\\langle w,v\\rangle^{2}}}\\end{array}$ , and so ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac12\\Delta f_{0}-\\frac{\\beta}{4}\\left\\|\\nabla f_{0}\\right\\|^{2}\\leq-\\frac{\\lambda}{4}\\left(\\frac{1}{2}\\beta\\lambda g^{\\prime}(\\langle w,v\\rangle)^{2}\\right)\\ (1-\\langle w,v\\rangle^{2})+\\frac{\\lambda}{2}g^{\\prime}(\\langle w,v\\rangle)\\langle w,v\\rangle d}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad=\\frac{\\lambda}{4}g^{\\prime}(\\langle w,v\\rangle)\\left\\{-\\frac{\\beta\\lambda}{2}g^{\\prime}(\\langle w,v\\rangle)(1-\\langle w,v\\rangle^{2})+2\\langle w,v\\rangle d\\right\\}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\leq\\frac{\\lambda}{4}g^{\\prime}(\\langle w,v\\rangle)\\left\\{-\\frac{2\\beta\\lambda c_{1}r^{2}}{\\pi^{2}}+2\\langle w,v\\rangle d\\right\\}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\leq-\\frac{\\lambda}{4}g^{\\prime}(\\langle w,v\\rangle)\\cdot\\frac{\\beta\\lambda c_{1}r^{2}}{\\pi^{2}}\\ \\leq-\\frac{c_{1}^{2}}{4\\pi^{2}}\\beta\\lambda^{2}r^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "provided that $\\begin{array}{r}{\\beta\\ge\\frac{2\\pi^{2}d}{\\lambda c_{1}r^{2}}}\\end{array}$ ", "page_idx": 43}, {"type": "text", "text": "To verify $(\\mathrm{C}_{\\mathbb{S}^{d}})$ , simply note that, since $\\begin{array}{r}{\\left\\|\\Pi_{w}v v^{\\top}\\Pi_{w}\\right\\|_{\\mathrm{op}}=\\left\\|\\Pi_{w}v\\right\\|^{2}=1-\\langle w,v\\rangle^{2},}\\end{array}$ ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\forall w,}&{\\left\\Vert\\nabla^{2}f_{0}(w)\\right\\Vert_{\\mathrm{op}}\\leq\\lambda g^{\\prime\\prime}(\\langle w,v\\rangle)(1-\\langle w,v\\rangle^{2})+\\lambda C_{1}}&\\\\ &{\\qquad\\qquad\\qquad\\leq\\lambda\\left[\\displaystyle\\operatorname*{sup}_{s\\in[-1,1]}g^{\\prime\\prime}(s)(1-s^{2})\\right]+\\lambda C_{1}}&{\\leq(C_{3}+C_{1})\\lambda,}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "and therefore, $\\begin{array}{r}{\\operatorname*{inf}_{w\\in\\mathbb{S}^{d}}\\lambda_{\\operatorname*{min}}(\\nabla^{2}f_{0}(w))\\geq-\\left(\\operatorname*{sup}_{w}\\left\\|\\nabla^{2}f_{0}(w)\\right\\|_{\\mathrm{op}}\\right)\\geq-(C_{3}+C_{1})\\lambda.}\\end{array}$ ", "page_idx": 43}, {"type": "text", "text": "Finally, let us verify $(\\mathrm{C}_{U})$ . Indeed, for any $w\\in U$ , ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\lambda_{\\operatorname*{min}}(\\nabla^{2}f_{0}(w))=\\underset{\\|u\\|^{2}=1,\\langle u,w\\rangle=0}{\\operatorname*{min}}-\\lambda g^{\\prime\\prime}(\\langle w,v\\rangle)\\langle u,v\\rangle^{2}+\\lambda g^{\\prime}(\\langle w,v\\rangle)\\langle w,v\\rangle}\\\\ &{\\qquad\\qquad\\geq-\\lambda\\,|g^{\\prime\\prime}(\\langle w,v\\rangle)|\\,\\underset{\\|u\\|^{2}=1,\\langle u,w\\rangle=0}{\\operatorname*{max}}\\,\\langle u,v\\rangle^{2}+\\lambda c_{1}\\langle w,v\\rangle}\\\\ &{\\qquad\\qquad=-\\lambda\\,|g^{\\prime\\prime}(\\langle w,v\\rangle)|\\,\\big(1-\\langle w,v\\rangle^{2}\\big)+\\lambda c_{1}\\langle w,v\\rangle,}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where the bound of the second term follows from $\\langle w,v\\rangle\\geq0$ , which can be ensured by taking $r\\leq\\frac\\pi2$ Since $w\\in U\\iff\\langle w,v\\rangle\\geq\\cos(r)\\geq1-r^{2}$ , it follows that ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\lambda_{\\operatorname*{min}}\\big(\\nabla^{2}f_{0}(w)\\big)\\geq-\\lambda\\left[\\displaystyle\\operatorname*{sup}_{\\cos r\\leq s\\leq1}\\left|g^{\\prime\\prime}(s)\\right|\\left(1-s^{2}\\right)\\right]+\\lambda c_{1}\\cos r}\\\\ &{\\qquad\\qquad\\qquad\\geq-\\lambda C_{3}\\left[\\displaystyle\\operatorname*{sup}_{\\cos r\\leq s\\leq1}\\sqrt{1-s^{2}}\\right]+\\lambda c_{1}\\cos r}\\\\ &{\\qquad\\qquad=\\lambda\\left(-C_{3}\\sin r+c_{1}\\cos r\\right)\\geq\\lambda\\displaystyle\\frac{c_{1}}{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "for a certain choice of $r$ small enough, dependent only on $c_{1}$ and $C_{3}$ . ", "page_idx": 43}, {"type": "text", "text": "F.2.2 Lyapunov function analysis for bounding the LSI constant of $\\eta_{\\lambda,\\beta}$ ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "To prove Thm. 5.2, it only remains to show that the conditions of Thm. F.6 are satisfied for ${\\cal J}_{\\lambda}^{\\prime}[\\eta_{\\lambda,\\beta}]$ instead of $J_{\\lambda}^{\\prime}[\\delta_{v}]$ . ", "page_idx": 43}, {"type": "text", "text": "Lemma F.8. Under the setting of Assumptions 2 and 3, $\\eta_{\\lambda,\\beta}$ is rotationally invariant except for the direction $v$ , or formally $R v=v\\implies R_{\\sharp}\\eta_{\\lambda,\\beta}=\\eta_{\\lambda,\\beta}$ for orthonormal matrices $R_{;}$ , where $R_{\\sharp}\\eta$ denotes the pushforward measure. Moreover, there exists $g_{\\eta}:[-1,1]\\to\\mathbb{R}$ such that for all $w\\in\\mathbb{S}^{d}$ , $J_{\\lambda}^{\\prime}[\\eta_{\\lambda,\\beta}](w\\bar{)}=-\\lambda g_{\\eta}(\\langle w,v\\rangle)$ . ", "page_idx": 43}, {"type": "text", "text": "Proof. The lemma follows directly from the fact that $\\rho$ is rotationally invariant and that $y=\\phi(v)$ . ", "page_idx": 43}, {"type": "text", "text": "Lemma F.9. Under Assumption 2, suppose furthermore that $\\operatorname*{sup}_{w}\\left\\|\\nabla^{i}\\phi(w)\\right\\|_{\\mathcal{H}}\\leq B_{i}<\\infty$ for $i\\in\\{0,1,2\\}$ . Then we have, for any $\\eta,\\eta^{\\prime}\\in\\mathcal{P}(\\mathcal{W})$ , ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\forall w\\in\\mathbb{S}^{d},\\ \\left\\vert\\frac{1}{2}\\Delta J_{\\lambda}^{\\prime}[\\eta]-\\frac{\\beta}{4}\\left\\|\\nabla J_{\\lambda}^{\\prime}[\\eta]\\right\\|^{2}-\\frac{1}{2}\\Delta J_{\\lambda}^{\\prime}[\\eta^{\\prime}]+\\frac{\\beta}{4}\\left\\|\\nabla J_{\\lambda}^{\\prime}[\\eta^{\\prime}]\\right\\|^{2}\\right\\vert}&{}\\\\ {\\leq\\left(d\\frac{2B_{0}B_{1}\\left(B_{0}B_{2}+B_{1}^{2}\\right)}{\\lambda^{2}}\\left\\|y\\right\\|_{\\mathcal{H}}^{2}+\\beta\\frac{2B_{0}^{3}B_{1}^{3}}{\\lambda^{3}}\\left\\|y\\right\\|_{\\mathcal{H}}^{4}\\right)W_{1}(\\eta,\\eta^{\\prime})}&{}\\\\ {a n d}&{\\left\\vert\\lambda_{\\operatorname*{min}}(\\nabla^{2}J_{\\lambda}^{\\prime}[\\eta])-\\lambda_{\\operatorname*{min}}(\\nabla^{2}J_{\\lambda}^{\\prime}[\\eta^{\\prime}])\\right\\vert\\leq\\frac{4B_{0}B_{1}\\left(B_{0}B_{2}+B_{1}^{2}\\right)}{\\lambda^{2}}\\left\\|y\\right\\|_{\\mathcal{H}}^{2}W_{1}(\\eta,\\eta^{\\prime}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Proof. By Prop. F.2, ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\|\\nabla^{2}\\,J_{\\lambda}^{\\prime}[\\eta](w)-\\nabla^{2}\\,J_{\\lambda}^{\\prime}[\\eta^{\\prime}](w)\\|_{\\mathrm{op}}\\leq\\frac{4B_{0}B_{1}(B_{0}B_{2}+B_{1}^{2})}{\\lambda^{2}}\\,\\|y\\|_{\\mathcal H}^{2}\\,W_{1}(\\eta,\\eta^{\\prime})\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "and $\\begin{array}{r}{\\left|\\lambda_{\\operatorname*{min}}(\\nabla^{2}\\,J_{\\lambda}^{\\prime}[\\eta](w))-\\lambda_{\\operatorname*{min}}(\\nabla^{2}\\,J_{\\lambda}^{\\prime}[\\eta^{\\prime}](w))\\right|\\,\\leq\\,\\left\\|\\nabla^{2}\\,J_{\\lambda}^{\\prime}[\\eta](w)-\\nabla^{2}\\,J_{\\lambda}^{\\prime}[\\eta^{\\prime}](w)\\right\\|_{\\mathrm{op}}}\\end{array}$ by Weyl\u2019s inequality. This shows the second inequality of the lemma. ", "page_idx": 44}, {"type": "text", "text": "For the first inequality, we have $\\Delta J_{\\lambda}^{\\prime}[\\eta](w)=\\mathrm{Tr}\\,\\nabla^{2}\\,J_{\\lambda}^{\\prime}[\\eta](W)$ and so ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\frac{1}{2}\\Delta J_{\\lambda}^{\\prime}[\\eta]-\\frac{1}{2}\\Delta J_{\\lambda}^{\\prime}[\\eta^{\\prime}]\\Big|\\leq\\frac{d}{2}\\left\\|\\nabla^{2}\\,J_{\\lambda}^{\\prime}[\\eta](w)-\\nabla^{2}\\,J_{\\lambda}^{\\prime}[\\eta^{\\prime}](w)\\right\\|_{\\mathrm{op}}\\leq\\frac{d}{2}\\frac{4B_{0}B_{1}(B_{0}B_{2}+B_{1}^{2})}{\\lambda^{2}}\\left\\|y\\right\\|_{\\mathcal{H}}^{2}W_{1}(y)\\,,\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Moreover, we showed in (F.2) resp. in Prop. F.2 that ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla J_{\\lambda}^{\\prime}[\\eta]\\|\\leq\\displaystyle\\frac{B_{0}B_{1}}{\\lambda}\\left\\|y\\right\\|_{\\mathcal{H}}^{2}\\quad\\mathrm{and}\\quad\\left\\|\\nabla J_{\\lambda}^{\\prime}[\\eta]-\\nabla J_{\\lambda}^{\\prime}[\\eta^{\\prime}]\\right\\|\\leq\\displaystyle\\frac{4B_{0}^{2}B_{1}^{2}}{\\lambda^{2}}\\left\\|y\\right\\|_{\\mathcal{H}}^{2}W_{1}(\\eta,\\eta^{\\prime}),}\\\\ &{\\left|\\displaystyle\\frac{\\beta}{4}\\left\\|\\nabla J_{\\lambda}^{\\prime}\\eta\\right\\|\\right|^{2}-\\displaystyle\\frac{\\beta}{4}\\left\\|\\nabla J_{\\lambda}^{\\prime}[\\eta^{\\prime}]\\right\\|^{2}\\right|\\leq\\displaystyle\\frac{\\beta}{4}\\cdot2\\frac{B_{0}B_{1}\\left\\|y\\right\\|_{\\mathcal{H}}^{2}}{\\lambda}\\cdot\\frac{4B_{0}^{2}B_{1}^{2}\\left\\|y\\right\\|_{\\mathcal{H}}^{2}}{\\lambda^{2}}W_{1}(\\eta,\\eta^{\\prime})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\beta\\displaystyle\\frac{2B_{0}^{3}B_{1}^{3}\\left\\|y\\right\\|_{\\mathcal{H}}^{4}}{\\lambda^{3}}W_{1}(\\eta,\\eta^{\\prime}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "so ", "page_idx": 44}, {"type": "text", "text": "which implies the first inequality of the lemma by triangle inequality. ", "page_idx": 44}, {"type": "text", "text": "We can now proceed to the proof of Thm. 5.2, thanks to a bound on $W_{2}(\\eta_{\\lambda,\\beta},\\delta_{v})$ under Assumption 3 proved in the next section. ", "page_idx": 44}, {"type": "text", "text": "Proof of Thm. 5.2. For concision, in this proof, we will use the notations $O(\\cdot),\\Omega(\\cdot),\\Theta(\\cdot),\\lesssim$ to hide constants dependent only on $\\|\\varphi\\|_{L^{2}(\\rho)}\\,,\\|\\varphi^{\\prime}\\|_{L^{4}(\\rho)}\\,,\\|\\varphi^{\\prime\\prime}\\|_{L^{4}(\\rho)}$ , $\\mathbb{E}_{x\\sim\\rho}\\left\\|x\\right\\|^{4}/d^{2},c_{1},C_{1},C_{2},C_{3}$ and C4. ", "page_idx": 44}, {"type": "text", "text": "We established in Prop. F.7 that $f_{0}\\;:=\\;J_{\\lambda}^{\\prime}[\\delta_{v}]$ satisfies the conditions $(\\mathrm{L}_{\\mathbb{S}^{d}})~(\\mathrm{L}_{U})~(\\mathrm{C}_{\\mathbb{S}^{d}})~(\\mathrm{C}_{U})$ of Thm. F.6 with some constants $D_{i},r=O(1)$ (in fact only dependent on $c_{1},C_{1},C_{2},C_{3})$ provided that $\\beta\\geq D_{0}d\\lambda^{-1}$ . Thus, the first part of the theorem concerning the LSI of $\\widehat{\\delta_{v}}\\propto e^{-\\beta J_{\\lambda}^{\\prime}[\\delta_{v}]}\\tau$ , follows from Thm. F.6. To prove the second part of the theorem, it suffices to show t hat $f^{*}:=J_{\\lambda}^{\\prime}[\\eta_{\\lambda,\\beta}]$ satisfies the conditions $(\\mathrm{L}_{\\mathbb{S}^{d}}^{\\prime})\\left(\\mathrm{L}_{U}\\right)\\left(\\mathrm{C}_{\\mathbb{S}^{d}}\\right)\\left(\\mathrm{C}_{U}\\right)$ of Thm. F.6 with some constants $\\widetilde{D}_{0}^{\\prime},\\widetilde{D}_{1}^{\\prime},\\widetilde{D}_{2},\\widetilde{D}_{3},\\widetilde{D}_{4},r=\\Theta(1)$ . ", "page_idx": 44}, {"type": "text", "text": "By Lem. F.3, there exist constants $B_{i}=O(1)$ such that $\\operatorname*{sup}_{w}\\left\\|\\nabla^{i}\\phi(w)\\right\\|_{\\mathcal{H}}\\leq B_{i}$ , for $i\\in\\{0,1,2\\}$ . Moreover, by Lem. F.12 below, provided that $\\beta\\geq\\Omega(d\\lambda)$ , one has ", "page_idx": 44}, {"type": "equation", "text": "$$\nW_{2}(\\eta_{\\lambda,\\beta,\\delta_{v}})\\lesssim\\sqrt{\\beta^{-1}d\\lambda^{-1}\\cdot\\log(\\beta d^{-1}\\lambda^{-1})}\\ \\ {=:}\\ \\overline{{W}}.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Now by the conditions $(\\mathrm{L}_{\\mathbb{S}^{d}})\\ (\\mathrm{L}_{U})\\ (\\mathrm{C}_{\\mathbb{S}^{d}})\\ (\\mathrm{C}_{U})$ for $f\\,=\\,f_{0}$ and $D_{i}\\,=\\,\\Theta(1)$ (by Prop. F.7), from Lem. F.9 along with the triangle inequality we have ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\forall w\\in\\mathbb{S}^{d},\\;\\frac{1}{2}\\Delta f^{*}-\\frac{\\beta}{4}\\left\\Vert\\nabla f^{*}\\right\\Vert^{2}\\lesssim\\lambda d+(d\\lambda^{-2}+\\beta\\lambda^{-3})\\overline{{W}}}\\\\ &{\\forall w\\in\\mathbb{S}^{d}\\backslash U,\\;\\frac{1}{2}\\Delta f^{*}-\\frac{\\beta}{4}\\left\\Vert\\nabla f^{*}\\right\\Vert^{2}\\leq-D_{2}\\beta\\lambda^{2}+E_{2}\\cdot(d\\lambda^{-2}+\\beta\\lambda^{-3})\\overline{{W}}}\\\\ &{\\forall w\\in\\mathbb{S}^{d},\\;\\;\\lambda_{\\operatorname*{min}}(\\nabla^{2}f^{*}(w))\\gtrsim-\\lambda-\\lambda^{-2}\\overline{{W}}}\\\\ &{\\quad\\forall w\\in U,\\;\\;\\lambda_{\\operatorname*{min}}(\\nabla^{2}f^{*}(w))\\geq D_{4}\\lambda-E_{4}\\cdot\\lambda^{-2}\\overline{{W}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "for some constants $E_{2},E_{4}=O(1)$ . So, ", "page_idx": 44}, {"type": "text", "text": "\u2022 $(\\mathrm{L}_{\\mathbb{S}^{d}}^{\\prime})$ for $f^{*}$ can be ensured with $\\widetilde{D}_{1}^{\\prime}=O(1)$ provided that $(d\\lambda^{-2}+\\beta\\lambda^{-3})\\overline{{{W}}}=(\\beta^{-1}d\\lambda+$ $1)\\beta\\lambda^{-3}\\overline{{W}}=O(\\lambda d\\beta^{3/4})$ . Since we already assume that $\\beta\\geq\\Omega(d\\lambda)$ , this is equivalent to $\\beta\\lambda^{-3}\\overline{{W}}=O(\\lambda d\\beta^{3/4})$ , i.e., $\\beta^{1/4}\\lambda^{-4}d^{-1}\\overline{{W}}=O(1)$ .   \n\u2022 $(\\mathrm{L}_{U})$ can be ensured with $\\begin{array}{r}{\\widetilde{D}_{2}=\\frac{D_{2}}{2}}\\end{array}$ if $\\beta$ is such that $\\begin{array}{r}{E_{2}(d\\lambda^{-2}+\\beta\\lambda^{-3})\\overline{{W}}\\leq\\frac{D_{2}}{2}\\beta\\lambda^{2}}\\end{array}$ , i.e., $\\begin{array}{r}{(\\beta^{-1}d\\lambda+1)\\lambda^{-5}\\overline{{W}}\\le\\frac{D_{2}}{2E_{2}}}\\end{array}$ . Since we already assume that $\\beta\\geq\\Omega(d\\lambda)$ , this is equivalent to $\\lambda^{-5}\\overline{{W}}\\leq F_{2}$ for a certain $F_{2}=\\Theta(1)$ .   \n\u2022 $(\\mathrm{C}_{\\mathbb{S}^{d}})$ ) can be ensured with $\\widetilde{D}_{3}=O(1)$ provided that $\\lambda^{-2}\\overline{{W}}={\\cal O}(\\lambda)$ , i.e., $\\lambda^{-3}\\overline{{W}}={\\cal O}(1)$ .   \n\u2022 $(\\mathrm{C}_{U})$ can be ensured with $\\begin{array}{r}{\\widetilde{D}_{4}=\\frac{D_{4}}{4}}\\end{array}$ if $\\begin{array}{r}{E_{4}\\lambda^{-2}\\overline{{W}}\\leq\\frac{D_{4}}{2}\\lambda.}\\end{array}$ , i.e., $\\begin{array}{r}{\\lambda^{-3}\\overline{{W}}\\le\\frac{D_{4}}{2E_{4}}=:F_{4}=\\Theta(1)}\\end{array}$ . ", "page_idx": 45}, {"type": "text", "text": "In summary, since we assume $\\lambda\\leq1$ , we have $\\lambda^{-3}\\,\\leq\\,\\lambda^{-5}$ and $\\lambda^{-4}d^{-1}\\leq\\,\\lambda^{-5}$ . Hence we will choose $\\beta$ such that $\\beta^{1/4}d^{-1}\\lambda^{-4}\\overline{{W}}={\\cal O}(1)$ and $\\lambda^{-5}\\overline{{W}}\\,\\leq\\,F_{2}$ for a certain $F_{2}\\,=\\,\\Theta(1)$ , and this will ensure all four conditions with constants $\\widetilde{D}_{1}^{\\prime},\\widetilde{D}_{2},\\widetilde{D}_{3},\\widetilde{D}_{4}=\\Theta(1)$ . For choices of $\\beta$ such that $\\beta\\geq d^{4}\\lambda^{-4}$ , it suffices to have $\\beta^{1/4}d^{-1}\\lambda^{-4}\\overline{{W}}\\,\\leq\\,F_{2}$ .  No w substituting the definition of $\\overline{W}$ , this sufficient condition rewrites ", "page_idx": 45}, {"type": "equation", "text": "$$\n3^{1/4}d^{-1}\\lambda^{-4}\\overline{{W}}\\leq F_{2}\\iff\\beta^{1/2}d^{-2}\\lambda^{-8}\\cdot\\beta^{-1}d\\lambda^{-1}\\log\\left(\\frac{\\beta}{d\\lambda}\\right)=\\beta^{-1/2}\\lambda^{-9}d^{-1}\\log\\left(\\frac{\\beta}{d\\lambda}\\right)\\leq F_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Since $\\forall\\varepsilon,x>0,\\varepsilon\\log x=\\log x^{\\varepsilon}\\leq x^{\\varepsilon}$ , then for any $\\varepsilon>0$ it suffices to choose $\\beta$ such that ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\beta^{-1/2}\\lambda^{-9}d^{-1}\\left(\\frac{\\beta}{d\\lambda}\\right)^{\\varepsilon}\\leq\\varepsilon F_{2}^{2}\\iff\\beta^{1/2-\\varepsilon}\\geq\\varepsilon^{-1}F_{2}^{-2}\\lambda^{-9-\\varepsilon}d^{-1-\\varepsilon}.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Choosing e.g. $\\varepsilon=\\textstyle{\\frac{1}{4}}$ , we get that a sufficient condition is $\\beta\\geq\\Omega(\\mathrm{poly}(\\lambda^{-1},d))$ . ", "page_idx": 45}, {"type": "text", "text": "Hence we may apply the second part of Thm. F.6 to $f^{*}=J_{\\lambda}^{\\prime}[\\eta_{\\lambda,\\beta}]$ with constants $\\widetilde{D}_{1}^{\\prime},\\widetilde{D}_{2},\\widetilde{D}_{3},\\widetilde{D}_{4}=$ $O(1)$ , provided that $\\beta\\,\\geq\\,\\Omega(\\mathrm{poly}(\\lambda^{-1},d))$ . This concludes the proof of the  seco nd p art  of the theorem. \u53e3 ", "page_idx": 45}, {"type": "text", "text": "F.2.3 Bound on $W_{1}(\\eta_{\\lambda,\\beta},\\delta_{v})$ ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "The following lemma shows a form of weak coercivity of $J_{\\lambda}$ . ", "page_idx": 45}, {"type": "text", "text": "Lemma F.10. Under Assumptions 2 and $^3$ , if furthermore there exist $c_{1},C_{1},C_{3},C_{4}>0$ such that ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\forall r\\in[-1,+1],\\;\\;c_{1}\\leq g^{\\prime}(r)\\leq C_{1},\\quad\\left|g^{\\prime\\prime}(r)(1-r^{2})^{1/2}\\right|\\leq C_{3},\\quad\\left|g^{\\prime\\prime\\prime}(r)(1-r^{2})^{3/2}\\right|\\leq C_{4},\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "then there exists a constant $\\alpha_{g}$ dependent only on $c_{1},C_{1},C_{3},C_{4}$ such that ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\forall\\eta,\\,\\,J_{\\lambda}(\\eta)-J_{\\lambda}(\\delta_{v})\\geq\\lambda\\alpha_{g}W_{2}^{2}(\\eta,\\delta_{v}).\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Proof. Since $J_{\\lambda}$ is convex, ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r}{J_{\\boldsymbol{\\lambda}}(\\eta)-J_{\\boldsymbol{\\lambda}}(\\delta_{v})\\geq\\int_{\\mathbb{S}^{d}}J_{\\boldsymbol{\\lambda}}^{\\prime}[\\delta_{v}]d(\\eta-\\delta_{v})=-\\lambda\\int_{\\mathbb{S}^{d}}g(\\langle v,w\\rangle)\\mathrm{d}(\\eta-\\delta_{v})(w)}\\\\ {=\\lambda\\int_{\\mathbb{S}^{d}}\\left[g(1)-g(\\langle v,w\\rangle)\\right]d\\eta(w).}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Now let $U_{r}\\,=\\,\\left\\{w\\in\\mathbb{S}^{d};\\mathrm{dist}_{\\mathbb{S}^{d}}(w,v)\\leq r\\right\\}$ for some $r\\,>\\,0$ to be chosen. We will compute the integral separately on $U_{r}$ and on $\\mathbb{S}^{d}\\setminus U_{r}$ . ", "page_idx": 45}, {"type": "text", "text": "For the part $\\scriptstyle\\int_{U_{r}}$ , we proceed by a second-order Taylor expansion. Namely, for any $w\\in U_{r}\\setminus\\{v\\}$ , let $e\\perp v$ such that $w=\\cos(\\theta)v+\\sin(\\theta)e$ for some $0<\\theta\\leq r$ , since $\\mathrm{dist}_{\\mathbb{S}^{d}}(w,v)=\\operatorname{arccos}(\\langle w,v\\rangle)=$ $\\theta$ . Then $g(\\langle v,w\\rangle)=g(\\cos\\theta)$ , and ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\frac{d}{d\\theta}}g(\\cos\\theta)=-\\sin(\\theta)g^{\\prime}(\\cos\\theta)}}\\\\ {{\\displaystyle{\\frac{d^{2}}{d\\theta^{2}}}g(\\cos\\theta)=\\sin(\\theta)^{2}g^{\\prime\\prime}(\\cos\\theta)-\\cos(\\theta)g^{\\prime}(\\cos\\theta)}}\\\\ {{\\displaystyle{\\frac{d^{3}}{d\\theta^{3}}}g(\\cos\\theta)=-\\sin(\\theta)^{3}g^{\\prime\\prime\\prime}(\\cos\\theta)+3\\sin(\\theta)\\cos(\\theta)g^{\\prime\\prime}(\\cos\\theta)+\\sin(\\theta)g^{\\prime}(\\cos\\theta).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Notice that by our assumptions on $g$ , it is smooth enough at 1 so that $\\sin(\\theta)g^{\\prime}(\\cos\\theta)\\,\\to\\,0$ and $\\sin(\\theta)^{2}g^{\\prime\\prime}(\\cos\\theta)\\rightarrow0$ as $\\theta\\rightarrow0$ . Further, ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\theta}\\frac{\\mathrm{d}^{3}}{\\mathrm{d}\\theta^{3}}g(\\cos\\theta)\\le C_{4}+3C_{3}+C_{1}=:6M_{3,g}.\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Consequently, by a univariate Taylor expansion with remainder in Langrange form around $\\theta=0$ , for all $0<\\theta\\leq r$ , provided that we choose $\\begin{array}{r}{r\\leq\\frac{g^{\\prime}(1)}{2M_{3,g}}}\\end{array}$ , we have ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{g(\\cos\\theta)=g(1)+0+\\displaystyle\\frac{1}{2}(0-g^{\\prime}(1))\\theta^{2}+\\frac{1}{6}(g\\circ\\cos)^{(3)}(u)\\theta^{3}\\mathrm{~\\for~some~}u\\in[0,r]}\\\\ &{\\phantom{g(1)=g(1)+0}\\leq g(1)-\\displaystyle\\frac{1}{2}g^{\\prime}(1)\\theta^{2}+\\frac{1}{6}\\left[\\underset{[0,r]}{\\operatorname*{sup}}\\ (g\\circ\\cos)^{(3)}\\right]\\theta^{3}}\\\\ &{\\phantom{g(1)=g(1)+0}\\leq g(1)-\\displaystyle\\frac{1}{2}g^{\\prime}(1)\\theta^{2}+M_{3,g}\\theta^{3}=g(1)-\\left(\\frac{1}{2}g^{\\prime}(1)-M_{3,g}\\theta\\right)\\theta^{2}}\\\\ &{\\phantom{g(1)=g(1)+0}\\leq g(1)-\\displaystyle\\frac{1}{4}g^{\\prime}(1)\\theta^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "In other words, ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\forall w\\in U_{r},\\,\\,\\,g(1)-g(\\langle v,w\\rangle)\\geq\\displaystyle\\frac{1}{4}g^{\\prime}(1)\\operatorname{dist}_{\\mathbb{S}^{d}}(w,v)^{2},}\\\\ {\\textnormal{d s o},\\quad}&{\\displaystyle\\int_{U_{r}}\\left[g(1)-g(\\langle v,w\\rangle)\\right]\\mathrm{d}\\eta(w)\\geq\\displaystyle\\frac{1}{4}g^{\\prime}(1)\\int_{U_{r}}\\operatorname{dist}_{\\mathbb{S}^{d}}(w,v)^{2}\\,\\,\\mathrm{d}\\eta(w).}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "For the part $\\int_{\\mathbb{S}^{d}\\setminus U_{r}}$ , since $g$ is increasing on $[-1,1]$ since $g^{\\prime}\\geq c_{1}>0$ , we have ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\int_{\\mathbb{S}^{d}\\setminus U_{r}}[g(1)-g(\\langle v,w\\rangle)]\\,\\mathrm{d}\\eta(w)\\ge[g(1)-g(\\cos(r))]\\,[1-\\eta(U_{r})]}}\\\\ &{\\ge\\left[\\frac{1}{4}g^{\\prime}(1)r^{2}\\right]\\,[1-\\eta(U_{r})]}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "where the second inequality follows from the Taylor expansion (F.7) above applied to $\\theta=r$ . Thus we showed ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{J_{\\boldsymbol{\\lambda}}(\\eta)-J_{\\boldsymbol{\\lambda}}(\\delta_{v})\\geq\\boldsymbol{\\lambda}\\left\\{\\left[\\frac{1}{4}g^{\\prime}(1)r^{2}\\right]\\left[1-\\eta(U_{r})\\right]+\\frac{g^{\\prime}(1)}{4}\\int_{U_{r}}\\mathrm{dist}_{\\mathbb{S}^{d}}(w,v)^{2}\\mathrm{d}\\eta(w)\\right\\}}\\\\ &{\\qquad\\qquad\\qquad=\\frac{\\lambda g^{\\prime}(1)}{4}\\left\\{r^{2}\\left[1-\\eta(U_{r})\\right]+\\int_{U_{r}}\\mathrm{dist}_{\\mathbb{S}^{d}}(w,v)^{2}\\mathrm{d}\\eta(w)\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "On the other hand, since $\\mathrm{dist}_{\\mathbb{S}^{d}}(v,w)=\\operatorname{arccos}(\\langle v,w\\rangle)$ , ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{W_{2}^{2}(\\eta,\\delta_{v})=\\displaystyle\\int_{\\mathbb S^{d}\\backslash U_{r}}\\mathrm{dist}_{\\mathbb S^{d}}(v,w)^{2}\\mathrm{d}\\eta(w)+\\displaystyle\\int_{U_{r}}\\mathrm{dist}_{\\mathbb S^{d}}(v,w)^{2}\\mathrm{d}\\eta(w)}\\\\ &{\\qquad\\qquad\\le\\pi^{2}\\left[1-\\eta(U_{r})\\right]+\\displaystyle\\int_{U_{r}}\\mathrm{dist}_{\\mathbb S^{d}}(v,w)^{2}\\mathrm{d}\\eta(w).}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Hence ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{J_{\\boldsymbol{\\lambda}}(\\eta)-J_{\\boldsymbol{\\lambda}}(\\delta_{v})\\geq\\frac{\\boldsymbol{\\lambda}g^{\\prime}(1)}{4}\\cdot\\underset{0\\leq r\\leq\\frac{g^{\\prime}(1)}{2M_{\\lambda,g}}}{\\operatorname*{sup}}\\operatorname*{min}\\left[\\frac{r^{2}}{\\pi^{2}},1\\right]W_{2}^{2}(\\eta,\\delta_{v})}\\\\ &{\\phantom{=}\\lambda\\cdot\\frac{g^{\\prime}(1)}{4}\\operatorname*{min}\\left[\\left(\\frac{g^{\\prime}(1)}{2M_{3,g}}\\right)^{2}/\\pi^{2},1\\right]\\cdot W_{2}^{2}(\\eta,\\delta_{v})}\\\\ &{\\phantom{=}\\lambda\\cdot\\frac{c_{1}}{4}\\operatorname*{min}\\left[\\left(\\frac{c_{1}}{2M_{3,g}}\\right)^{2}/\\pi^{2},1\\right]\\cdot W_{2}^{2}(\\eta,\\delta_{v})\\phantom{x x x x x x}=:\\lambda\\alpha_{g}W_{2}^{2}(\\eta,\\delta_{v}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Notice that $\\alpha_{g}$ only depends on $c_{1},C_{1},C_{3},C_{4}$ . ", "page_idx": 46}, {"type": "text", "text": "We will use the following fact about the surface area of a small hyperspherical cap around a pole for bounding $W_{1}(\\eta_{\\lambda,\\beta},\\delta_{v})$ . It essentially shows that, for $\\mathcal{W}=\\mathbb{S}^{d}$ , the constant called $C$ in the statement of Lem. E.1 scales with dimension as $2^{-d}\\lesssim C^{-1}\\lesssim1/\\sqrt{d}$ . ", "page_idx": 47}, {"type": "text", "text": "Lemma F.11. Fix $d\\geq2$ and $v\\in\\mathbb{S}^{d}$ and denote by $\\tau$ the uniform measure on $\\mathbb{S}^{d}$ . For any $\\epsilon>0$ , let $S_{\\epsilon}=\\left\\{w\\in\\mathbb{S}^{d}:\\mathrm{dist}_{\\mathbb{S}^{d}}(w,v)\\leq\\epsilon\\right\\}$ . There exist universal constants $C_{-},C_{+}>0$ such that ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\forall0<\\epsilon\\leq\\frac{\\pi}{4},\\quad C_{-}^{-1}\\left(\\epsilon/2\\right)^{d}\\leq\\tau(S_{\\epsilon})\\leq C_{+}\\,\\epsilon^{d}/\\sqrt{d}.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Proof. For $w\\sim\\tau$ , the distribution of $\\langle w,v\\rangle$ admits a probability density function $h(z)\\,=\\,(1\\,-$ $z^{2})^{d/2-1}/Z$ , where ", "page_idx": 47}, {"type": "equation", "text": "$$\nZ=\\int_{-1}^{1}(1-z^{2})^{d/2-1}\\mathrm{d}z=B\\left({\\frac{d}{2}},{\\frac{1}{2}}\\right)={\\frac{\\Gamma\\left({\\frac{d}{2}}\\right){\\sqrt{\\pi}}}{\\Gamma\\left({\\frac{d+1}{2}}\\right)}}.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Note that by Gautschi\u2019s inequality $\\begin{array}{r}{\\forall s\\in(0,1),\\forall x>0,\\;\\;x^{1-s}<\\frac{\\Gamma(x+1)}{\\Gamma(x+s)}<(x+1)^{1-s}}\\end{array}$ applied to $s=\\textstyle{\\frac{1}{2}}$ and $\\textstyle x={\\frac{d-1}{2}}$ , we have $\\begin{array}{r}{\\sqrt{\\frac{d-1}{2}}<\\frac{\\Gamma\\left(\\frac{d+1}{2}\\right)}{\\Gamma\\left(\\frac{d}{2}\\right)}<\\sqrt{\\frac{d+1}{2}}}\\end{array}$ , so ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\sqrt{\\frac{2\\pi}{d+1}}\\leq Z\\leq\\sqrt{\\frac{2\\pi}{d-1}}.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "By definition, since $\\begin{array}{r}{\\mathrm{dist}_{\\mathbb{S}^{d}}(w,v)=\\operatorname{arccos}(\\langle w,v\\rangle),\\tau(S_{\\epsilon})=\\int_{\\cos(\\epsilon)}^{1}h(z)\\mathrm{d}z}\\end{array}$ . One can verify ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\forall\\,0<\\epsilon\\leq\\frac{\\pi}{4},\\;\\sqrt{1-\\epsilon^{2}}\\leq\\cos(\\epsilon)\\leq\\sqrt{1-\\frac{\\epsilon^{2}}{4}}.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "So for all $0<\\epsilon\\le\\frac{\\pi}{4}$ , ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tau(S_{\\epsilon})=\\displaystyle\\int_{\\cos(\\epsilon)}^{1}h(z)\\mathrm{d}z\\le\\displaystyle\\int_{\\sqrt{1-\\epsilon^{2}}}^{1}h(z)\\mathrm{d}z}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=Z^{-1}\\displaystyle\\int_{\\sqrt{1-\\epsilon^{2}}}^{1}(1-z^{2})^{d/2-1}\\mathrm{d}z=Z^{-1}\\displaystyle\\int_{1-\\epsilon^{2}}^{1}(1-t)^{d/2-1}\\displaystyle\\frac{\\mathrm{d}t}{2\\sqrt{t}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\le Z^{-1}\\displaystyle\\frac{1}{2\\sqrt{1-\\epsilon^{2}}}\\displaystyle\\int_{1-\\epsilon^{2}}^{1}(1-t)^{d/2-1}\\mathrm{d}t=Z^{-1}\\displaystyle\\frac{1}{2\\sqrt{1-\\epsilon^{2}}}\\displaystyle\\int_{0}^{\\epsilon^{2}}t^{d/2-1}\\mathrm{d}t}\\\\ &{\\qquad\\qquad\\qquad\\qquad=Z^{-1}\\displaystyle\\frac{1}{2\\sqrt{1-\\epsilon^{2}}}\\cdot\\displaystyle\\frac{2}{d}[\\epsilon]^{d/2}\\le Z^{-1}\\displaystyle\\frac{1}{d\\sqrt{1-(\\pi/4)^{2}}}\\epsilon^{d}\\,\\le C_{+}\\epsilon^{d}/\\sqrt{d}}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "for some universal constant $C_{+}$ . In the other direction, ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tau(S_{\\epsilon})\\geq\\displaystyle\\int_{\\sqrt{1-\\epsilon^{2}/4}}^{1}h(z)\\mathrm{d}z=Z^{-1}\\int_{\\sqrt{1-\\epsilon^{2}/4}}^{1}(1-z^{2})^{d/2-1}\\mathrm{d}z=Z^{-1}\\int_{1-\\epsilon^{2}/4}^{1}(1-t)^{d/2-1}\\frac{\\mathrm{d}t}{2\\sqrt{t}}}\\\\ &{\\geq Z^{-1}\\displaystyle\\frac{1}{2}\\int_{1-\\epsilon^{2}/4}^{1}(1-t)^{d/2-1}\\mathrm{d}t=Z^{-1}\\displaystyle\\frac{1}{2}\\int_{0}^{\\epsilon^{2}/4}t^{d/2-1}\\mathrm{d}t}\\\\ &{=Z^{-1}\\displaystyle\\frac{1}{2}\\displaystyle\\frac{2}{d}[\\epsilon^{2}/4]^{d/2}=Z^{-1}\\displaystyle\\frac{1}{d}(\\epsilon/2)^{d}~\\geq c(\\epsilon/2)^{d}/\\sqrt{d}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "for some universal constants $c$ . By repeating the same argument with $\\textstyle{\\sqrt{1-{\\frac{\\epsilon^{2}}{4}}}}$ replaced by $\\textstyle{\\sqrt{1-{\\frac{\\epsilon^{2}}{3.9}}}}$ , we get $\\tau(S_{\\epsilon})\\geq c^{\\prime}(\\epsilon/1.99)^{d}/\\sqrt{d}\\geq C_{-}^{-1}(\\epsilon/2)^{d}$ for some universal constants $c^{\\prime},C_{-}$ . \u53e3 ", "page_idx": 47}, {"type": "text", "text": "The following lemma combines the weak coercivity and weak Lipschitz-continuity of $J_{\\lambda}$ by a $\\Gamma$ -convergence type argument, to show an explicit bound on $W_{1}(\\eta_{\\lambda,\\beta},\\delta_{v})$ . It quantifies the intuitive fact that $\\eta_{\\lambda,\\beta}$ converges weakly to $\\delta_{v}$ when $\\beta^{-1}\\rightarrow0$ or $\\lambda\\to+\\infty$ . ", "page_idx": 47}, {"type": "text", "text": "Lemma F.12. Under Assumptions 2 and $^3$ , $i f\\operatorname*{sup}_{w}\\left\\|\\nabla^{i}\\phi(w)\\right\\|_{\\mathcal{H}}\\leq B_{i}<\\infty$ for $i\\in\\{0,1\\}$ , and if $\\begin{array}{r}{\\beta\\geq\\frac{4d\\lambda}{\\pi}\\big(B_{0}B_{1}\\left\\|y\\right\\|_{\\mathcal{H}}^{2}\\big)^{-1}}\\end{array}$ , then ", "page_idx": 48}, {"type": "equation", "text": "$$\nW_{2}(\\eta_{\\lambda,\\beta},\\delta_{v})\\leq\\sqrt{\\frac{1}{\\alpha_{g}}\\frac{\\beta^{-1}d}{\\lambda}\\left(\\widetilde{C}+\\log\\left(B_{0}B_{1}\\left\\|y\\right\\|_{\\mathcal{H}}^{2}\\right)-\\log\\left(\\beta^{-1}d\\lambda\\right)\\right)}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "where $\\widetilde{C}$ is a universal constant and $\\alpha_{g}$ is the constant from Lem. F.10. ", "page_idx": 48}, {"type": "text", "text": "Proof. Since $\\eta_{\\lambda,\\beta}=\\arg\\operatorname*{min}J_{\\lambda,\\beta}$ and $J_{\\lambda,\\beta}=J+\\beta^{-1}H\\left(\\cdot|\\tau\\right)$ , then for any $\\eta^{\\sigma}\\in\\mathcal{P}(\\mathcal{W})$ , ", "page_idx": 48}, {"type": "equation", "text": "$$\nJ_{\\lambda}(\\eta_{\\lambda,\\beta})\\le J_{\\lambda}(\\eta_{\\lambda,\\beta})+\\beta^{-1}H\\left(\\eta_{\\lambda,\\beta}|\\tau\\right)=J_{\\lambda,\\beta}(\\eta_{\\lambda,\\beta})\\le J_{\\lambda,\\beta}(\\eta^{\\sigma})=J_{\\lambda}(\\eta^{\\sigma})+\\beta^{-1}H\\left(\\eta^{\\sigma}|\\tau\\right).\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Further, we showed in Lem. F.10 that $\\forall\\eta$ , $J_{\\lambda}(\\eta)-J_{\\lambda}(\\delta_{v})\\ge\\lambda\\alpha_{g}\\cdot W_{2}^{2}(\\eta,\\delta_{v})$ , so ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\lambda\\alpha_{g}\\cdot W_{2}^{2}(\\eta_{\\lambda,\\beta},\\delta_{v})\\leq J_{\\lambda}(\\eta_{\\lambda,\\beta})-J_{\\lambda}(\\delta_{v})\\leq J_{\\lambda}(\\eta^{\\sigma})-J_{\\lambda}(\\delta_{v})+\\beta^{-1}H\\left(\\eta^{\\sigma}|\\tau\\right).\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "It remains to upper-bound the right-hand side, which we do by choosing as $\\eta^{\\sigma}$ a box-kernel smoothed version of $\\delta_{v}$ (this part the proof is essentially an instantantiation of Lem. E.1). Specifically, let $\\eta^{\\sigma}$ be the uniform measure over the spherical cap $S_{\\sigma}=\\left\\{w\\in\\mathbb{S}^{d};\\mathrm{dist}_{\\mathbb{S}^{d}}(w,v)\\leq\\sigma\\right\\}^{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!}$ for $\\sigma$ to be chosen. We showed in Prop. F.2 that ", "page_idx": 48}, {"type": "equation", "text": "$$\nJ_{\\lambda}(\\eta^{\\sigma})-J_{\\lambda}(\\delta_{v})\\leq\\frac{B_{0}B_{1}\\left\\lVert y\\right\\rVert_{\\mathcal{H}}^{2}}{\\lambda}\\cdot W_{1}(\\eta^{\\sigma},\\delta_{v})\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "where $\\operatorname*{sup}_{w}\\|\\nabla^{i}\\phi(w)\\|_{\\mathcal{H}}\\leq B_{i}$ , and by definition ", "page_idx": 48}, {"type": "equation", "text": "$$\nW_{1}(\\eta^{\\sigma},\\delta_{v})=\\int\\mathrm{dist}_{\\mathbb{S}^{d}}(w,v)\\;\\mathrm{d}\\eta^{\\sigma}(w)=\\frac{1}{\\mathrm{vol}(S_{\\sigma})}\\int_{S_{\\sigma}}\\mathrm{dist}_{\\mathbb{S}^{d}}(w,v)\\;\\mathrm{d}\\mathrm{vol}(w)\\leq\\sigma.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Moreover by Lem. F.11, provided that $\\begin{array}{r}{0<\\sigma\\le\\frac{\\pi}{4}}\\end{array}$ , ", "page_idx": 48}, {"type": "equation", "text": "$$\nH\\left(\\eta^{\\sigma}|\\tau\\right)=\\int\\mathrm{d}\\eta_{\\sigma}\\log\\frac{\\mathrm{d}\\eta_{\\sigma}}{\\mathrm{d}\\tau}=\\log\\frac{\\mathrm{vol}(\\mathbb{S}^{d})}{\\mathrm{vol}(S_{\\sigma})}=-\\log\\tau(S_{\\sigma})\\le\\log C-d\\log\\frac{\\sigma}{2}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "for some universal constant $C$ , and let us assume w.l.o.g. that $C>1$ , so that $\\log C\\leq d\\log C$ . Thus ", "page_idx": 48}, {"type": "equation", "text": "$$\nJ_{\\lambda}(\\eta^{\\sigma})-J_{\\lambda}(\\delta_{v})+\\beta^{-1}H\\left(\\eta^{\\sigma}|\\tau\\right)\\leq\\frac{B_{0}B_{1}\\left\\Vert y\\right\\Vert_{\\mathcal{H}}^{2}}{\\lambda}\\sigma-\\beta^{-1}d\\log\\sigma+\\beta^{-1}d\\log2C.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Therefore, taking the infimum over $\\begin{array}{r}{0<\\sigma\\le\\frac{\\pi}{4}}\\end{array}$ , ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\lambda\\alpha_{g}\\cdot W_{2}^{2}(\\eta_{\\lambda,\\beta},\\delta_{v})\\leq\\underset{0<\\sigma\\leq\\frac{\\pi}{4}}{\\operatorname*{inf}}\\frac{B_{0}B_{1}\\|y\\|_{\\mathcal{H}}^{2}}{\\lambda}\\sigma-\\beta^{-1}d\\log{\\sigma}+\\beta^{-1}d\\log{2C}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\beta^{-1}d-\\beta^{-1}d\\log{\\frac{\\beta^{-1}d\\lambda}{B_{0}B_{1}\\|y\\|_{\\mathcal{H}}^{2}}}+\\beta^{-1}d\\log{2C}}\\\\ &{\\qquad\\qquad\\qquad=\\beta^{-1}d\\left(1+\\log(2C)-\\log(\\beta^{-1}d\\lambda)+\\log\\left(B_{0}B_{1}\\,\\|y\\|_{\\mathcal{H}}^{2}\\right)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "where on the second line we used that the unconstrained infimum of the right-hand side over $\\sigma>0$ is attained at $\\begin{array}{r}{\\sigma=\\frac{\\beta^{-1}d\\lambda}{B_{0}B_{1}\\parallel y\\parallel_{\\mathcal{H}}^{2}}}\\end{array}$ , which is indeed less than $\\scriptstyle{\\frac{\\pi}{4}}$ by assumption. This shows the bound ", "page_idx": 48}, {"type": "equation", "text": "$$\nW_{2}(\\eta_{\\lambda,\\beta},\\delta_{v})\\leq\\sqrt{\\frac{1}{\\lambda\\alpha_{g}}\\beta^{-1}d\\left(1+\\log(2C)-\\log(\\beta^{-1}d\\lambda)+\\log\\left(B_{0}B_{1}\\left\\|y\\right\\|_{\\mathcal{H}}^{2}\\right)\\right)},\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "and the bound announced in the proposition follows by gathering some universal constants into $\\widetilde{C}$ . ", "page_idx": 48}, {"type": "text", "text": "F.3 Proof of Prop. 5.3 (examples of activations satisfying the assumptions) ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Before presenting the proof, we recall a few concepts from the theory of spherical harmonics, and refer to [AH12; FE12] for more details. Let $\\tau$ be the uniform probability measure on $\\mathbb{S}^{d}$ . The spherical harmonics in dimension $d+1$ form an orthonormal basis of $L^{2}(\\dot{\\tau})$ . We denote them by {Ykj}k,j, where k \u22650 and 1 \u2264j \u2264N(d, k), where N(d, 0) = 1 and N(d, k) = 2k+kd\u22121 kd+d1\u22122 for $k\\geq1$ (for $k=0$ we have $Y_{01}=1$ ). Consequently, any $\\phi\\in L^{2}(\\tau)$ can be written as ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\phi=\\sum_{k=0}^{\\infty}\\sum_{j=1}^{N(d,k)}\\langle\\phi,Y_{k j}\\rangle_{L^{2}(\\tau)}Y_{k j}.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Let $P_{k,d}$ be the Legendre polynomial (a.k.a. Gegenbauer polynomial) of degree $k$ in dimension $d+1$ , normalized such that $P_{k,d}(1)=1$ . Thanks to Rodrigues\u2019 formula [AH12, Theorem 2.23], we can express Legendre polynomials as, ", "page_idx": 49}, {"type": "equation", "text": "$$\nP_{k,d}(t)=\\frac{(-1)^{k}\\Gamma(d/2)}{2^{k}\\Gamma(k+d/2)}(1-t^{2})^{(2-d)/2}\\left(\\frac{\\mathrm{d}}{\\mathrm{d}t}\\right)^{k}(1-t^{2})^{k+(d-2)/2}.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "We now go over some useful properties of spherical harmonics and Legendre polynomials. ", "page_idx": 49}, {"type": "text", "text": "\u2022 (Addition Formula) We have the following formula which relates Legendre polynomials to spherical harmonics [AH12, Theorem 2.9], ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\sum_{j=1}^{N(d,k)}Y_{k j}(w)Y_{k j}(v)=N(d,k)P_{k,d}(\\langle w,v\\rangle),\\quad\\forall w,v\\in\\mathbb{S}^{d}.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "\u2022 (Hecke-Funk Formula) Suppose $\\phi\\in L^{2}(\\tau)$ is given by $\\phi(\\cdot)=\\varphi(\\langle w,\\cdot\\rangle)$ for some $w\\in\\mathbb{S}^{d}$ . Then [AH12, Theorem 2.22], ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\langle\\phi,Y_{k j}\\rangle_{L^{2}(\\tau)}=\\frac{\\Gamma((d+1)/2)}{\\Gamma(d/2)\\sqrt{\\pi}}Y_{k j}(w)\\int_{-1}^{1}\\varphi(t)P_{k}(t)(1-t^{2})^{(d-2)/2}\\mathrm{d}t.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "\u2022 (Orthogonality of Legendre Polynomials) Using the addition formula and orthonormality of spherical harmonics, for every $k,k^{\\prime}\\ge0$ we have, ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\langle P_{k,d}(\\langle w,\\cdot\\rangle),P_{k^{\\prime},d}(\\langle v,\\cdot\\rangle)_{L^{2}(\\tau)}=\\frac{\\delta_{k k^{\\prime}}P_{k,d}(\\langle w,v\\rangle)}{N(d,k)}.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "\u2022 (Derivative of Legendre Polynomials) For every $k\\geq j$ , we have the following identity for derivatives of Legendre polynomials [AH12, Equation (2.89)], ", "page_idx": 49}, {"type": "equation", "text": "$$\nP_{k,d}^{(j)}(t)=c_{j,k,d}P_{k-j,d+2j}(t),\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "where $P_{k,d}^{(j)}$ denotes the $j$ th derivative of $P_{k,d}$ , and ", "page_idx": 49}, {"type": "equation", "text": "$$\nc_{j,k,d}=\\frac{k(k-1)\\ldots(k-j+1)(k+d-1)(k+d)\\ldots(k+d+j-2)}{d(d+2)\\ldots(d+2j-2)}.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Notice that for j > k we have P k(,jd) = . ", "page_idx": 49}, {"type": "text", "text": "We use the tools introduced above to prove the following lemma. ", "page_idx": 49}, {"type": "text", "text": "Lemma F.13. Suppose $\\rho$ is a spherically symmetric probability measure on $\\mathbb{R}^{d+1}$ . Define $q:$ $[-1,1]\\rightarrow\\mathbb{R}$ via $\\begin{array}{r}{q(\\bar{\\langle}w,v\\rangle)=\\int\\bar{\\varphi_{}}(\\langle w,x\\rangle)\\dot{\\varphi}(\\langle v,x\\rangle)\\mathrm{d}\\rho(\\bar{x}).}\\end{array}$ for $w,v\\in\\mathbb{S}^{d}$ . Then, for every $j\\geq1$ , ", "page_idx": 49}, {"type": "equation", "text": "$$\nq^{(j)}(\\langle w,v\\rangle)=\\frac{1}{(d+1)(d+3)\\cdot..\\cdot(d+2j-1)}\\int\\|x\\|^{2j}\\,\\varphi^{(j)}(\\langle w,x\\rangle)\\varphi^{(j)}(\\langle v,x\\rangle)\\mathrm{d}\\rho(x),\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "where $\\varphi^{(j)}$ denotes the jth derivative of $\\varphi$ . ", "page_idx": 49}, {"type": "text", "text": "Proof. We being by introducing the notation $\\varphi_{r}(\\langle w,x\\rangle)=\\varphi(r\\langle w,x\\rangle)$ . Doing so allows us to only consider functions on $\\mathbb{S}^{d}$ by conditioning on the norm of input $\\left\\|x\\right\\|$ . Notice that ", "page_idx": 50}, {"type": "equation", "text": "$$\nq(\\langle w,v\\rangle)=\\mathbb{E}\\left[\\mathbb{E}\\left[\\varphi\\Big(\\|x\\|\\,\\langle w,\\frac{x}{\\|x\\|}\\rangle\\Big)\\varphi\\Big(\\|x\\|\\,\\langle v,\\frac{x}{\\|x\\|}\\rangle\\Big)\\mid\\|x\\|\\right]\\right]=\\mathbb{E}_{\\|x\\|}\\left[q_{\\|x\\|}(\\langle w,v\\rangle)\\right],\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "where ", "page_idx": 50}, {"type": "equation", "text": "$$\nq_{r}(\\langle w,v\\rangle):=\\int\\varphi(r\\langle w,x\\rangle)\\varphi(r\\langle v,x\\rangle)\\mathrm{d}\\tau(x)=\\langle\\varphi_{r}(\\langle w,\\cdot\\rangle),\\varphi_{r}(\\langle v,\\cdot\\rangle)\\rangle_{L^{2}(\\tau)}.\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "By the Hecke-Funk formula, ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\langle\\varphi_{r}(\\langle w,\\cdot\\rangle),Y_{k j}(\\cdot)\\rangle_{L^{2}(\\tau)}=\\bar{\\alpha}_{k,r}Y_{k j}(w):=\\frac{\\alpha_{k,r}}{\\sqrt{N(d,k)}}Y_{k j}(w),\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "where ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\bar{\\alpha}_{k,r}:=\\frac{\\Gamma((d+1)/2)}{\\Gamma(d/2)\\sqrt{\\pi}}\\int_{-1}^{1}\\varphi(r t)P_{k}(t)(1-t^{2})^{(d-2)/2}\\mathrm{d}t.\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Then, by the expansion of $\\varphi_{r}(\\langle w,\\cdot\\rangle)$ in the basis of spherical harmonics, ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\varphi_{r}(\\langle w,\\cdot\\rangle)=\\sum_{k=0}^{\\infty}\\sum_{j=1}^{N(d,k)}\\frac{\\alpha_{k,r}}{\\sqrt{N(d,k)}}Y_{k j}(w)Y_{k j}(\\cdot)=\\sum_{k=0}^{\\infty}\\sqrt{N(d,k)}\\alpha_{k,r}P_{k,d}(\\langle w,\\cdot\\rangle).\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Via the formula for inner products of Legendre polynomials, we obtain ", "page_idx": 50}, {"type": "equation", "text": "$$\nq_{r}(\\langle w,v\\rangle)=\\sum_{k=0}^{\\infty}\\alpha_{k,r}^{2}N(d,k)\\langle P_{k,d}(\\langle w,\\cdot\\rangle,P_{k,d}(\\langle v,\\cdot\\rangle)_{L^{2}(\\tau)}=\\sum_{k=0}^{\\infty}\\alpha_{k,r}^{2}P_{k,d}(\\langle w,v\\rangle).\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "As a result, ", "page_idx": 50}, {"type": "equation", "text": "$$\nq_{r}^{(j)}(\\langle w,v\\rangle)=\\sum_{k=0}^{\\infty}\\alpha_{k,r}^{2}P_{k,d}^{(j)}(\\langle w,v\\rangle)=\\sum_{k=j}^{\\infty}\\alpha_{k,r}^{2}c_{j,k,d}P_{k-j,d+2j}(\\langle w,v\\rangle),\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "where $c_{j,k,d}$ is given by (F.8). On the other hand, we can directly obtain from (F.10), ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\varphi_{r}^{(j)}(\\langle w,x\\rangle)=\\sum_{k=0}^{\\infty}\\sqrt{N(d,k)}\\alpha_{k,r}P_{k,d}^{(j)}(\\langle w,x\\rangle)=\\sum_{k=j}^{\\infty}\\sqrt{N(d,k)}\\alpha_{k,r}c_{j,k,d}P_{k-j,d+2j}(\\langle w,x\\rangle).\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Therefore, ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\langle\\varphi_{r}^{(j)}(\\langle w,\\cdot\\rangle),\\varphi_{r}^{(j)}(\\langle v,\\cdot\\rangle)\\rangle_{L^{2}(\\tau)}=\\sum_{k=j}^{\\infty}\\frac{\\alpha_{k,r}^{2}c_{j,k,d}^{2}N(d,k)}{N(d+2j,k-j)}P_{k-j,d+2j}(\\langle w,v\\rangle).\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Moreover, it is straightforward to verify that ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\frac{c_{j,k,d}N(d,k)}{N(d+2j,k-j)}=(d+1)(d+3)\\ldots(d+2j-1)\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "for $k\\geq j$ . Therefore, ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\varphi_{r}^{(j)}(\\langle w,\\cdot\\rangle),\\varphi_{r}^{(j)}(\\langle v,\\cdot\\rangle)\\rangle_{L^{2}(\\tau)}=(d+1)(d+3)\\dots(d+2j-1)\\displaystyle\\sum_{k=j}^{\\infty}\\alpha_{k,r}^{2}c_{j,k,d}P_{k-j,d+2j}(\\langle w,v\\rangle)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=(d+1)(d+3)\\dots(d+2j-1)q_{r}^{(j)}(\\langle w,v\\rangle),}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "where the last identity follows from (F.11). We can now use the fact that $\\varphi_{r}^{(j)}=r\\varphi^{(j)}$ , and plug the above back into (F.9) to obtain ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{\\Gamma})=\\mathbb{E}_{\\|\\boldsymbol{x}\\|}\\left[q_{\\|\\boldsymbol{x}\\|}^{(j)}(\\langle\\boldsymbol{w},\\boldsymbol{v}\\rangle)\\right]=\\mathbb{E}_{\\|\\boldsymbol{x}\\|}\\int\\frac{\\|\\boldsymbol{x}\\|^{2j}}{(d+1)(d+3)\\dots(d+2j-1)}\\varphi^{(j)}(\\|\\boldsymbol{x}\\|\\langle\\boldsymbol{w},\\bar{\\boldsymbol{x}}\\rangle)\\varphi^{(j)}(\\|\\boldsymbol{x}\\|\\langle\\boldsymbol{v},\\bar{\\boldsymbol{x}}\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\int\\frac{\\|\\boldsymbol{x}\\|^{2j}}{(d+1)(d+3)\\dots(d+2j-1)}\\varphi^{(j)}(\\langle\\boldsymbol{w},\\boldsymbol{x}\\rangle)\\varphi^{(j)}(\\langle\\boldsymbol{v},\\boldsymbol{x}\\rangle)\\mathrm{d}\\rho(\\boldsymbol{x}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "which concludes the proof. ", "page_idx": 50}, {"type": "text", "text": "We are now ready to state the proof of Prop. 5.3 ", "page_idx": 51}, {"type": "text", "text": "Proof of Prop. 5.3. Recall $\\begin{array}{r}{g(\\langle w,v\\rangle)\\,=\\,\\frac{\\langle\\phi(w),\\phi(v)\\rangle_{\\mathcal{H}}^{2}}{2(\\lambda+\\|\\phi(v)\\|_{\\mathcal{H}}^{2})^{2}}}\\end{array}$ . Let $q(\\langle w,v\\rangle)\\;=\\;\\langle\\phi(w),\\phi(v)\\rangle_{\\mathcal{H}}$ . Consequently, ", "page_idx": 51}, {"type": "equation", "text": "$$\ng^{\\prime}=\\frac{q q^{\\prime}}{(\\lambda+\\|\\phi(v)\\|_{\\mathcal{H}}^{2})^{2}},\\quad g^{\\prime\\prime}=\\frac{q q^{\\prime\\prime}+q^{\\prime}{}^{2}}{(\\lambda+\\|\\phi(v)\\|_{\\mathcal{H}}^{2})^{2}},\\quad g^{\\prime\\prime\\prime}=\\frac{3q^{\\prime}q^{\\prime\\prime}+q q^{\\prime\\prime\\prime}}{(\\lambda+\\|\\phi(v)\\|_{\\mathcal{H}}^{2})^{2}}.\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "We proceed to bound each term separately. By non-negativity of $\\phi$ , for any $r>0$ , we have ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{q(\\langle w,v\\rangle)=\\mathbb{E}\\left[\\varphi(\\langle w,x\\rangle)\\varphi(\\langle v,x\\rangle)\\right]}\\\\ &{\\qquad\\qquad\\geq\\mathbb{E}\\left[\\varphi(\\langle w,x\\rangle)\\phi(\\langle v,x\\rangle)\\mathbf{1}\\big(|\\langle w,x\\rangle|\\leq r,|\\langle v,x\\rangle|\\leq r\\big)\\right]}\\\\ &{\\qquad\\qquad\\geq(\\underset{|z|\\leq r}{\\operatorname*{inf}}\\,\\varphi(z))^{2}\\mathbb{P}\\left[\\xi|\\langle w,x\\rangle|\\leq r\\right\\}\\cap\\{|\\langle v,x\\rangle|\\leq r\\}\\right]}\\\\ &{\\qquad\\qquad\\geq(\\underset{|z|\\leq r}{\\operatorname*{inf}}\\,\\varphi(z))^{2}\\left(1-\\mathbb{P}[\\langle w,x\\rangle^{2}>r^{2}]-\\mathbb{P}[\\langle v,x\\rangle^{2}>r^{2}]\\right)}\\\\ &{\\qquad\\qquad\\geq(\\underset{|z|\\leq r}{\\operatorname*{inf}}\\,\\varphi(z))^{2}\\left(1-\\frac{2\\mathbb{E}[\\|x\\|^{2}]}{(d+1)r^{2}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "where the last inequality follows from Markov inequality along with the fact that $\\mathbb{E}[x x^{\\top}]\\;=$ E[d\u2225+x\u222512 ]Id+1 for spherically symmetric distributions. Thus, by choosing r = m = 2b2\u221ab2, we have $q(z)\\ \\geq\\ {\\frac{1}{2}}(\\operatorname{inf}_{|z|\\leq m}\\varphi(z))$ . Furthermore, by the Cauchy-Schwartz inequality, $q(\\langle w,v\\rangle)\\;\\leq$ $\\mathbb{E}[\\varphi(\\langle w,x\\rangle)^{2}]\\,=\\,\\|\\varphi\\|_{L^{2}(\\rho)}^{2}\\,.$ . Next, we move on to bounding $q^{\\prime}$ . Let $\\bar{x}\\sim\\tau$ be a uniform random vector on $\\mathbb{S}^{d}$ . Then, for any $r>0$ , by Lem. F.13, ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\big|(\\langle w,v\\rangle)=\\frac{1}{d+1}\\mathbb{E}\\left[\\|x\\|^{2}\\,\\varphi^{\\prime}(\\langle w,x\\rangle)\\varphi^{\\prime}(\\langle v,x\\rangle)\\right]}\\\\ &{\\qquad\\qquad=\\frac{1}{d+1}\\mathbb{E}\\left[\\|x\\|^{2}\\,\\mathbb{E}\\left[\\varphi^{\\prime}(\\langle w,x\\rangle)\\varphi^{\\prime}(\\langle v,x\\rangle)\\mid\\|x\\|\\right]\\right]}\\\\ &{\\qquad\\qquad\\ge\\frac{(\\operatorname*{inf}_{\\vert z\\vert\\le r}\\varphi^{\\prime}(z))^{2}}{d+1}\\mathbb{E}\\left[\\|x\\|^{2}\\,\\mathbb{P}\\left[\\left\\{\\vert\\langle w,\\bar{x}\\rangle\\vert\\le\\frac{r}{\\|x\\|}\\right\\}\\cap\\left\\{\\vert\\langle v,\\bar{x}\\rangle\\vert\\le\\frac{r}{\\|x\\|}\\right\\}\\mid\\|x\\|\\right]\\right]}\\\\ &{\\qquad\\ge\\frac{(\\operatorname*{inf}_{\\vert z\\vert\\le r}\\varphi^{\\prime}(z))^{2}}{d+1}\\mathbb{E}\\left[\\|x\\|^{2}\\left(1-\\mathbb{P}\\left[\\langle w,\\bar{x}\\rangle^{2}>\\frac{r^{2}}{\\|x\\|^{2}}\\mid\\|x\\|\\right]-\\mathbb{P}\\left[\\langle v,\\bar{x}\\rangle^{2}>\\frac{r^{2}}{\\|x\\|^{2}}\\mid\\|x\\|\\right]\\right)}\\\\ &{\\qquad\\qquad\\ge\\frac{(\\operatorname*{inf}_{\\vert z\\vert\\le r}\\varphi^{\\prime}(z))^{2}}{d+1}\\mathbb{E}\\left[\\|x\\|^{2}\\left(1-\\frac{2\\,\\|x\\|^{2}}{r^{2}(d+1)}\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Consequently, by choosing $\\begin{array}{r}{r=m=\\frac{2b_{2}\\sqrt{b_{2}}}{b_{1}}}\\end{array}$ , we obtain $\\begin{array}{r}{q^{\\prime}\\geq\\frac{b_{1}}{2}(\\operatorname*{inf}_{|z|\\leq m}\\phi^{\\prime}(z))^{2}}\\end{array}$ . Moreover, by the Cauchy-Schwartz inequality, $q^{\\prime}\\leq b_{2}\\left\\|\\varphi^{\\prime}\\right\\|_{L^{4}(\\rho)}^{2}$ . As a result, ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\frac{b_{1}(\\operatorname*{inf}_{|z|\\le m}\\varphi(z))^{2}(\\operatorname*{inf}_{|z|\\le m}\\varphi^{\\prime}(z))^{2}}{(\\lambda+\\|\\varphi\\|_{L^{2}(\\rho)}^{2})^{2}}\\le g^{\\prime}\\le\\frac{b_{2}\\,\\|\\varphi\\|_{L^{2}(\\rho)}^{2}\\,\\|\\varphi^{\\prime}\\|_{L^{4}(\\rho)}^{2}}{(\\lambda+\\|\\varphi\\|_{L^{2}(\\rho)}^{2})^{2}}.\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Furthermore, by Lem. F.13 and the Cauchy-Schwartz inequality, ", "page_idx": 51}, {"type": "equation", "text": "$$\n|q^{\\prime\\prime}|\\leq\\frac{b_{2}^{2}(d+1)}{d+3}\\left\\|\\varphi^{\\prime\\prime}\\right\\|_{L^{4}(\\rho)}^{2},\\quad|q^{\\prime\\prime\\prime}|\\leq\\frac{b_{2}^{3}(d+1)^{2}}{(d+3)(d+5)}\\left\\|\\phi^{\\prime\\prime\\prime}\\right\\|_{L^{4}(\\rho)}^{2}.\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Hence, ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\frac{-b_{2}^{2}\\left\\lVert\\varphi^{\\prime\\prime}\\right\\rVert_{L^{4}(\\rho)}^{2}\\left\\lVert\\varphi\\right\\rVert_{L^{4}(\\rho)}^{2}}{(\\lambda+\\left\\lVert\\varphi\\right\\rVert_{L^{2}(\\rho)}^{2})^{2}}\\le g^{\\prime\\prime}\\le\\frac{b_{2}^{2}\\left\\lVert\\varphi^{\\prime\\prime}\\right\\rVert_{L^{4}(\\rho)}^{2}\\left\\lVert\\varphi\\right\\rVert_{L^{2}(\\rho)}^{2}+b_{2}^{2}\\left\\lVert\\varphi^{\\prime}\\right\\rVert_{L^{4}(\\rho)}^{4}}{(\\lambda+\\left\\lVert\\varphi\\right\\rVert_{L^{2}(\\rho)}^{2})^{2}},\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "and ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\vert g^{\\prime\\prime\\prime}\\vert\\le\\frac{3b_{2}^{3}\\,\\Vert\\phi^{\\prime}\\Vert_{L^{4}(\\rho)}^{2}\\,\\Vert\\phi^{\\prime\\prime}\\Vert_{L^{4}(\\rho)}^{2}+b_{2}^{3}\\,\\Vert\\phi\\Vert_{L^{2}(\\rho)}^{2}\\,\\Vert\\phi^{\\prime\\prime\\prime}\\Vert_{L^{4}(\\rho)}^{2}}{(\\lambda+\\Vert\\varphi\\Vert_{L^{2}(\\rho)}^{2})^{2}},\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "which completes the proof. ", "page_idx": 51}, {"type": "text", "text": "F.4 Implementation details for Fig. 1 ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "We consider the problem (1.1) where $\\mathcal{W}=\\mathbb{S}^{d}$ and $G$ is defined as in Assumption 2, where $d=10$ , $\\lambda=10^{-3}$ and ", "page_idx": 52}, {"type": "text", "text": "\u2022 $y:\\mathbb{R}^{d+1}\\to\\mathbb{R}$ is given by a teacher 2NN with 5 neurons defined as follows. The first-layer weights are orthonormal, drawn from the Haar measure, and the second layer weights are drawn i.i.d. from $\\mathcal{N}(0,1.8I_{d})$ . Its activation is $\\begin{array}{r}{\\varphi_{\\mathrm{teacher}}(z)=\\frac{z^{4}-6z^{2}+3}{\\sqrt{24}}}\\end{array}$ , which is the normalized 4th degree Hermite polynomial. \u2022 $\\rho$ is the empirical distribution of a (covariate) dataset $(x_{i})_{i\\leq n}$ of $n=100$ training samples, sampled i.i.d. from $\\mathcal{N}\\left(\\left(\\!\\!\\begin{array}{c c}{{0_{d}}}\\\\ {{1}}\\end{array}\\!\\!\\right),\\left(\\!\\!\\begin{array}{c c}{{I_{d}}}&{{0}}\\\\ {{0}}&{{0}}\\end{array}\\!\\!\\right)\\right)$ , with the last coordinate representing bias. \u2022 The activation function $\\varphi$ of the student 2NN $\\hat{y}_{\\nu}$ is the ReLU, $\\varphi(z)=\\operatorname*{max}(0,z)$ . ", "page_idx": 52}, {"type": "text", "text": "We performed 5 different runs, each corresponding to a different teacher network $(y)$ and training dataset $(\\rho)$ , and tested all the algorithms considered at each run. So the objective functional $G_{\\lambda}$ is different for each run, which is why the values shown on the $y$ -axis are offset by $G_{\\lambda}^{*}$ , the best value achieved by any of the algorithms considered for each run. ", "page_idx": 52}, {"type": "text", "text": "For the algorithms using the bilevel formulation, we computed the values and the Wasserstein gradients of $J_{\\lambda}$ explicitly by the formulas from Prop. F.1 and (F.2) (the matrix $K_{\\eta}+\\lambda$ id in $L_{\\rho}^{2}\\simeq\\mathbb{R}^{n}$ is inverted explicitly). ", "page_idx": 52}, {"type": "text", "text": "For the algorithms using MFLD, we used $\\beta^{-1}=10^{-3}$ . We ran the Euler-Maruyama discretization of the noisy particle gradient flow SDE described in Sec. 2 (with an inexact simulation of the Brownian increments described below), using $N=1000$ particles \u2013 corresponding to the width of the student $2\\mathrm{NN}-$ , and a step size of $10^{-2}$ for (1a) and $10^{-3}$ for (1b). For Wasserstein GF without noise, we used the same discretization but with $\\beta^{-1}=0$ . ", "page_idx": 52}, {"type": "text", "text": "Concerning the initialization of the particles $(r^{i},w^{i})_{i\\leq N}$ \u2013 corresponding to the second resp. firstlayer weights of the student network \u2013, the $w_{0}^{i}$ are drawn i.i.d. uniformly on $\\mathbb{S}^{d}$ , and for the algorithms using the lifting formulation, the $r_{0}^{i}$ are drawn i.i.d. from ${\\mathcal{N}}(0,1)$ . ", "page_idx": 52}, {"type": "text", "text": "Note that our simulations of Brownian motion are not exact. To implement MFLD on $\\mathbb{S}^{d}$ , we simply took gradient steps in $\\mathbb{R}^{d+1}$ with added Gaussian noise, and projected the weights back to the sphere. ", "page_idx": 52}, {"type": "text", "text": "The code to reproduce this experiment can be found at https://github.com/mousavih/ 2024-MFLD-bilevel. ", "page_idx": 52}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 53}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 53}, {"type": "text", "text": "Justification: The claims made in the abstract and introduction match the paper\u2019s contributions. In particular the three bullet points concluding the introduction summarize the paper\u2019s contributions section by section. ", "page_idx": 53}, {"type": "text", "text": "Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 53}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 53}, {"type": "text", "text": "Justification: The scope and limitations of each optimization dynamics considered is clearly discussed within each section. ", "page_idx": 53}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 53}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 53}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 54}, {"type": "text", "text": "Justification: For each theorem or proposition or corollary or lemma, be it in the main text or in the appendix, the assumptions are clearly stated, and all proofs are provided. ", "page_idx": 54}, {"type": "text", "text": "Guidelines: ", "page_idx": 54}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 54}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 54}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 54}, {"type": "text", "text": "Justification: The contributions of this work are theoretical. A numerical illustration is given in Fig. 1, for which the implementation details allowing to reproduce the experiment are provided in Sec. F.4. ", "page_idx": 54}, {"type": "text", "text": "Guidelines: ", "page_idx": 54}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 54}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 55}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 55}, {"type": "text", "text": "Justification: We provide in Sec. F.4 full details for the small numerical experiment of Fig. 1, which are sufficient to reproduce the experiment. The code we used will also be made public at a later date. ", "page_idx": 55}, {"type": "text", "text": "Guidelines: ", "page_idx": 55}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 55}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 55}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 55}, {"type": "text", "text": "Justification: The setup of the numerical experiment of Fig. 1 is very simple. Moreover full details are provided in Sec. F.4. ", "page_idx": 55}, {"type": "text", "text": "Guidelines: ", "page_idx": 55}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 55}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 55}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 55}, {"type": "text", "text": "Justification: The purpose of the small experiment from Fig. 1 is to compare the qualitative behavior of several algorithms: advantage of MFLD over Wasserstein GF in Fig. 1a, and advantage of MFLD-Bilevel over MFLD-Lifting in Fig. 1b. This qualitative behavior is clear-cut across the 5 runs, all of which are shown. ", "page_idx": 55}, {"type": "text", "text": "Guidelines: ", "page_idx": 55}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 55}, {"type": "text", "text": "\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 56}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 56}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 56}, {"type": "text", "text": "Justification: The very small scale of the numerical experiment of Fig. 1 means that any standard laptop or desktop computer can be used to reproduce it in, with a runtime of a few minutes. ", "page_idx": 56}, {"type": "text", "text": "Guidelines: ", "page_idx": 56}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 56}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 56}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 56}, {"type": "text", "text": "Justification: We have reviewed the Code of Ethics and have not found any deviation of our work from it. ", "page_idx": 56}, {"type": "text", "text": "Guidelines: ", "page_idx": 56}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 56}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 56}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 56}, {"type": "text", "text": "Justification: The contributions of this work are theoretical. ", "page_idx": 57}, {"type": "text", "text": "Guidelines: ", "page_idx": 57}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 57}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 57}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 57}, {"type": "text", "text": "Justification: The contributions of this work are theoretical. Guidelines: ", "page_idx": 57}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 57}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 57}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 57}, {"type": "text", "text": "Justification: The paper does not use existing assets. ", "page_idx": 57}, {"type": "text", "text": "Guidelines: ", "page_idx": 57}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets. \u2022 The authors should cite the original paper that produced the code package or dataset. \u2022 The authors should state which version of the asset is used and, if possible, include a URL. ", "page_idx": 57}, {"type": "text", "text": "\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 58}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 58}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 58}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 58}, {"type": "text", "text": "Guidelines: ", "page_idx": 58}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 58}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 58}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 58}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 58}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 58}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 58}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 58}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 58}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 58}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 59}]