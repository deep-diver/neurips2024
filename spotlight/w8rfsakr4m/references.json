{"references": [{"fullname_first_author": "Gu, A.", "paper_title": "Mamba: Linear-time sequence modeling with selective state spaces", "publication_date": "2023-12-09", "reason": "This paper introduces the Mamba model, a core component upon which the current work builds and significantly improves."}, {"fullname_first_author": "Dosovitskiy, A.", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2021-00-00", "reason": "This work is foundational to the field of vision transformers, a highly relevant and influential model architecture that the current paper relates to."}, {"fullname_first_author": "He, K.", "paper_title": "Deep residual learning for image recognition", "publication_date": "2016-00-00", "reason": "This is a highly influential work that established residual networks as a standard architecture, which is referenced for comparison purposes."}, {"fullname_first_author": "Liu, Z.", "paper_title": "Swin transformer: Hierarchical vision transformer using shifted windows", "publication_date": "2021-00-00", "reason": "This paper established Swin Transformer, another highly relevant and influential model architecture that the current paper builds upon and contrasts with."}, {"fullname_first_author": "Wang, W.", "paper_title": "InternImage: Exploring large-scale vision foundation models with deformable convolutions", "publication_date": "2023-00-00", "reason": "This paper introduces InternImage, a large-scale vision model that serves as a strong comparison baseline for the current work."}]}