[{"type": "text", "text": "MambaTree: Tree Topology is All You Need in State Space Model ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yicheng Xiao1\u2020\u2217, Lin Song2,3 \u2217, Shaoli Huang3, Jiangshan Wang1, Siyu $\\mathbf{Song^{4}}$ , Yixiao $\\mathbf{\\bar{G}e^{2,3}}$ , $\\mathbf{Xiu\\,Li^{1\\times}}$ , Ying Shan2,3 ", "page_idx": 0}, {"type": "text", "text": "1Tsinghua Shenzhen International Graduate School, Tsinghua University 2ARC Lab, Tencent PCG 3Tencent AI Lab 4South China Normal University xiaoyc23@mails.tsinghua.edu.cn ronnysong@tencent.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The state space models, employing recursively propagated features, demonstrate strong representation capabilities comparable to Transformer models and superior efficiency. However, constrained by the inherent geometric constraints of sequences, it still falls short in modeling long-range dependencies. To address this issue, we propose the MambaTree network, which first dynamically generates a tree topology based on spatial relationships and input features. Then, feature propagation is performed based on this graph, thereby breaking the original sequence constraints to achieve stronger representation capabilities. Additionally, we introduce a linear complexity dynamic programming algorithm to enhance long-range interactions without increasing computational cost. MambaTree is a versatile multimodal framework that can be applied to both visual and textual tasks. Extensive experiments demonstrate that our method significantly outperforms existing structured state space models on image classification, object detection and segmentation. Besides, by fine-tuning large language models, our approach achieves consistent improvements in multiple textual tasks at minor training cost. Code is available at https://github.com/EasonXiao-888/GrootVL. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Mainstream fundamental models are primarily based on CNN [30, 62, 44, 32, 13] and Transformer architectures [15, 43, 42, 59, 14], which dominate in visual and language tasks. However, the small receptive field of CNNs and the high complexity of Transformers make it challenging to strike a good balance between effectiveness and efficiency. The state space models (SSMs) [22, 24, 52] attempt to disrupt this impasse, which model sequences in a recurrent form. Different from the previous recurrent neural networks [31, 7], these approaches draw inspiration from control systems, leveraging structural parameter initialization to attain stable optimization and superior computing performance. Nevertheless, it remains susceptible to the intrinsic flaw shared by recurrent neural networks, i.e., a deficiency in capturing long-range dependencies. ", "page_idx": 0}, {"type": "text", "text": "Recently, an improved selection mechanism known as Mamba [19] is proposed to mitigate the challenges of SSMs. This approach introduces weight modulation during the propagation process, which substantially enlarges the effective receptive field and achieves impressive performance in NLP tasks. Besides, numerous studies aim to extend Mamba into computer vision, by employing various pre-defined strategies to map 2D image features into 1D sequences. ViM [77] and VMamba [41] utilize a multi-directional raster-scanning strategy, while LocalMamba [34] further confines its propagation range within a local window. They have successfully adapted Mamba to image inputs. Nevertheless, as shown in Fig. 1(a), both raster-scanning and local-scanning strategies introduce spatial discontinuities between adjacent pixels, and feature transformations in Mamba rely on the feature relationships, thereby impeding the effective information flow in a sequence. Additionally, PlainMamba [69] introduces a continuous scanning strategy, aiming to alleviate this issue by simply adjusting the propagation direction at discontinuous positions. However, all these methods rely on fixed propagation trajectories, which ignore the inherent spatial structure and cannot dynamically adjust the topology based on input. Therefore, this paper endeavors to explore a new perspective: introducing an input-aware topological network for feature propagation in state space models. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To achieve it, we develop a tree state space model and propose a new framework, termed MambaTree, which adaptively generates a tree topology based on the input feature and then performs feature propagation on it. Specifically, two sub-networks, MambaTreeV and MambaTreeL, are designed for visual and language tasks respectively, which are illustrated in Fig. 1(b) and Fig. 1(d). For visual tasks, motivated by [71, 54], we first utilize the dissimilarity between adjacent features to construct a minimum spanning tree on a four-connected planner graph. This process can adaptively encode the spatial and semantic information into a tree graph [71, 54]. Then, we iteratively traverse each pixel, considering it as the root vertex, and aggregate the features of other pixels using the state transition function of Mamba. Intuitively, this operation requires two levels of traversal across the entire pixel set, resulting in an unacceptable quadratic complexity relative to the number of pixels. However, given that the tree graph is acyclic, we propose a dynamic programming algorithm to achieve linear complexity propagation. With such an input-aware tree topology, our approach enables more effective long-range interactions while maintaining consistent linear complexity with Mamba. Furthermore, our method can also be applied to language tasks by constructing a tree topology based on the dissimilarity between token features, which overcomes the geometrical constraints of the text sequence. Using a similar aggregation process as MambaTreeV, MambaTreeL can significantly enhance the language representation of a pre-trained Large Language Model [19]. ", "page_idx": 1}, {"type": "text", "text": "We conduct extensive experiments to validate the effectiveness of MambaTreeV on multiple visual benchmarks, i.e. image classification on ImageNet [12], object detection and instance segmentation on MSCOCO [39] as well as semantic segmentation on ADE20K [75]. Results show that our method notably outperforms existing SSM-based methods for all benchmarks and achieves competitive performance with CNN and Transformer-based approaches. Moreover, with LoRA finetuning [33], MambaTreeL demonstrates consistent improvements for a pre-trained large language model at minor training cost. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Conventional Vision Foundation Models ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "The evolution of deep neural networks has been a significant catalyst in machine vision perception. CNN-based models [30, 51, 35, 25, 61, 72, 38, 55, 73] firstly emerge as pivotal landmarks, with ResNet [30] notably standing out for its inventive residual connection module, garnering widespread adoption across diverse domains of visual recognition. Furthermore, more efficient convolution operations are formulated, such as depth-wise convolutions introduced by MobileNet [32], paving the way for lightweight models. Additionally, deformable convolution [10] has been proposed to enhance the receptive field. Subsequently, ViT [15] has significantly improved the vision recognition paradigm. It reformulates the architecture design and training mechanism by combining transformer architecture in natural language processing, aiming to improve computational efficiency and broaden the scope of applications. After research discourse is centred on hierarchical ViTs [43, 42, 11, 63, 14, 56, 5] which design networks by decreasing feature resolution across the backbone gradually. Furthermore, recent research built on CNN serves to re-emphasize the capabilities of convolutional networks. For example, InternImage [62] presents a large model based on deformable CNN, while UniRepLKNet [13] exhibits significant performance through large kernel convolution. ", "page_idx": 1}, {"type": "text", "text": "2.2 Explorations about State Space Models ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "State space models (SSMs) have emerged as a novel class of models within the deep learning paradigm, showing significant potential for sequence transforming [23, 22, 52]. These methods have attracted significant attention due to their linear scalability with sequence length. The early method, ", "page_idx": 1}, {"type": "image", "img_path": "W8rFsaKr4m/tmp/4ed0f036971ec59784affbd856c9f0dc3b7e93890baf9a0948f9852abbf70187.jpg", "img_caption": ["Figure 1: Comparison of different propagation strategies for multi-modal tasks. For visual tasks, the previous strategies (a) are based on fixed patterns, while our method can adaptively generate the propagation topology according to input features. For textual tasks, compared to previous methods (c), our approach (d) can break the inherent constraints of text sequences, facilitating the effective transmission of long-range information. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "LSSL [23], draws inspiration from continuous state space models in control systems and attempts to address the long-range dependency problem through a combination with HIPPO [20] initialization. S4 [22] proposes to normalize the parameters into a diagonal matrix, prompting a subsequent series of research on structured SSMs [24, 21, 26, 19]. Recently, the Selective State Space Model [19], known as Mamba, strikes a balance between effectiveness and efficiency through the design of an input-dependent parameter initialization strategy, which has emerged as a formidable competitor to both transformer and CNN structures. In addition to showcasing superior outcomes in sequence modeling, Mamba has been seamlessly incorporated into the visual domain [77, 41, 34, 69, 68]. These studies often rely on handcrafted fixed scanning mechanisms to mitigate the execution bias of the selective state space model on 2D non-causal images. However, such simplistic approaches cannot effectively capture spatial relationships in an input-dependent paradigm. To address this limitation, we propose an effective framework MambaTree in this work to enhance long-range modeling for both vision and language tasks by introducing an input-aware tree-based topological structure. ", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we first revisit the selective state space model [19] and then elaborate on our inputaware topology scanning algorithm for state space modeling. With this superior algorithm, we develop a tree SSM and propose a novel framework called MambaTree, which consists of two sub-networks: MambaTreeV for visual tasks and MambaTreeL for fine-tuning a pre-trained language model [19]. ", "page_idx": 2}, {"type": "text", "text": "3.1 Revisiting Selective State Space Model ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "State Space Models (SSMs) are commonly regarded as continuous linear time-invariant systems [64] that map input stimulation $\\boldsymbol{x}(t)\\,\\in\\,\\mathbb{R}^{1\\times\\overset{}{D}}$ to output signal $\\boldsymbol{y}(t)\\,\\in\\,\\mathbb{R}^{1\\times D}$ through a state vector $h(t)\\in\\dot{\\mathbb{R}}^{1\\times\\dot{N}}$ , where $t,D$ and $N$ indicate the time step, channel number of the signal and state size, respectively. These models can be formulated as the following linear ordinary differential equations: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{h^{\\prime}(t)=\\mathbf{A}h(t)+\\mathbf{B}x(t),\\quad y(t)=\\mathbf{C}h(t)+\\mathbf{D}x(t),}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where ${\\bf A}\\in\\mathbb{R}^{N\\times N}$ , ${\\bf B}\\in\\mathbb{R}^{N\\times D}$ , $\\mathbf{C}\\in\\mathbb{R}^{N\\times D}$ and feedthrough coefficient $\\mathbf{D}\\in\\mathbb{R}^{D}$ . ", "page_idx": 2}, {"type": "text", "text": "Discretization. Although SSM serves as a powerful tool in systems and control engineering, its time-continuous nature poses challenges for integration into deep learning architectures. To alleviate this issue, most methods utilize the zero-order hold rule [19] to discretize the continuous system described by Eq. (1) and convert continuous variables $({\\bf A},{\\bf B},{\\bf C},{\\bf D})$ into corresponding discrete parameters $(\\bar{\\mathbf{A}},\\bar{\\mathbf{B}},\\bar{\\mathbf{C}},\\bar{\\mathbf{D}})$ over the specified sampling time-scale $\\Delta\\in\\mathbb{R}^{D}$ : ", "page_idx": 2}, {"type": "image", "img_path": "W8rFsaKr4m/tmp/c70a2123a5c32d104f61e1f304cb3d0247e4c534b20672215ff3355e675f6278.jpg", "img_caption": ["Figure 2: Illustration of Tree State Space Model. With an image feature map $x$ , we perform Tree Scanning Algorithm (TSA) to construct a 4-connected graph with edge weights measured by dissimilarity between pixels. Then, we obtain an MST with vertices set $\\Omega$ through a pruning algorithm and perform the state transition for each vertex in this topology (detailed in Sec. 3.2). Red arrows describe the propagation source of vertex $i$ . "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\bar{\\mathbf{A}}=e^{\\Delta\\mathbf{A}},\\quad\\bar{\\mathbf{B}}=\\left(e^{\\Delta\\mathbf{A}}-I\\right)\\mathbf{A}^{-1}\\mathbf{B},\\quad\\bar{\\mathbf{C}}=\\mathbf{C},\\quad\\bar{\\mathbf{D}}=\\mathbf{D}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In addition, many improved methods [41, 19] use an approximation of $\\bar{\\bf B}$ based on the first-order Taylor Series: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\bar{\\mathbf{B}}=\\left(e^{\\Delta\\mathbf{A}}-I\\right)\\mathbf{A}^{-1}\\mathbf{B}\\approx(\\Delta\\mathbf{A})(\\Delta\\mathbf{A})^{-1}\\Delta\\mathbf{B}=\\Delta\\mathbf{B}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Selective Mechanism . Previous SSMs store information through finite states and inherent timeinvariance, which limits their effectiveness. Therefore, Mamba [19] introduces a dynamic mechanism to selectively filter out input into a sequential state. Specifically, it utilizes Linear Projection to calculate the parameters $\\{\\mathbf{B}_{i}\\}_{i=1}^{L}$ , $\\{{\\bf C}_{i}\\}_{i=1}^{L}$ and $\\{\\Delta_{i}\\}_{i=1}^{\\bar{L}}$ from the input sequence $\\{x_{i}\\}_{i=1}^{L}$ with $\\boldsymbol{x}_{i}\\in\\mathbb{R}^{1\\times D}$ directly to improve the context-aware ability. Then the output sequence $\\{y_{i}\\}_{i=1}^{L}$ can be computed with those input-adaptive discretized parameters as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\nh_{i}=\\bar{\\mathbf{A}}_{i}h_{i-1}+\\bar{\\mathbf{B}}_{i}x_{i},\\quad y_{i}=\\mathbf{C}_{i}h_{i}+\\mathbf{D}x_{i}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "3.2 Tree State Space Model ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Mamba [19] has showcased remarkable performance in modeling the dependencies of consecutive words in a sequence. However, its applicability in long-context tasks, especially visual modeling, still poses certain challenges. For visual tasks, many methods attempt to address this problem by employing fixed scanning strategies, such as multi-directional raster scan [41, 77], local scan [34], and continuous scan [69]. However, these handcrafted scanning methods fail to effectively preserve the 2D structural information of images. ", "page_idx": 3}, {"type": "text", "text": "Following the design in Mamba [19], we construct a transform block as a tree state space model, which is presented in Fig. 2. The only difference between our block and Mamba lies in the replacement of the structured state space block with the proposed tree scanning algorithm. In the tree scanning algorithm, we generate a tree topology and then propagate the state of each vertex along the topological path to obtain strong feature representations. In addition, our algorithm can effectively enhance language representations by incorporating such a tree topology during text processing, which overcomes the geometrical constraints of text sequences. In the following, we elaborate on the proposed tree scanning algorithm and its applications for multi-modal tasks. ", "page_idx": 3}, {"type": "text", "text": "Tree Scanning Algorithm. Given an input feature $X=\\{x_{i}\\}_{i=1}^{L}$ where $L$ is the sequence length (or the number of input pixels), we construct an undirected $m$ -connected graph $G=(V,E)$ for the feature. $m$ is a hyper-parameter that indicates the number of adjacent tokens. Following [71, 54], we set $m=4$ for visual tasks, meaning each pixel is connected to its four neighboring pixels. For language tasks, we set $m=3$ by default, meaning each token is connected to the previous three tokens. In addition, the vertices $V$ represent the pixel (or token) embeddings, and the $E$ indicates the edges of the graph. The edge weight is calculated by the feature dissimilarity between adjacent vertices. Besides, the metric of dissimilarity uses cosine distance by default, and the comparison with other metrics refers to Table 5. ", "page_idx": 3}, {"type": "image", "img_path": "W8rFsaKr4m/tmp/2b4f67a199e660f681b44edb11369f53b9bf2cc67aacd07149debe5b2ce6b2b8.jpg", "img_caption": ["Figure 3: Overview of MambaTreeV. LN means LayerNorm and FFN is a feed-forward network in the basic block. S2 and P1 denote stride of 2 and padding size of 1 in convolution, respectively. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "We use the Contractive Boruvka algorithm [2] to prune the edges with significant dissimilarity, which generates a minimum spanning tree (MST) $\\mathcal{G}_{T}$ whose sum of dissimilarity weights is minimum out of all spanning trees. In the propagation process, we iteratively traverse each vertex, treating it as the root, and aggregate the features of the remaining vertices. Intuitively, applying state propagation within such a geometric configuration makes its preferential interactions among vertices with small spatial and feature distances. Following the Mamba, we employ the data-dependent transition matrix for state propagation. For a vertex $k$ , we denote the transition matrix with its parent as $\\bar{\\mathbf{A}}_{k}$ . Furthermore, following the Eq. (4), the state aggregation process for the $i$ -th vertex can be formulated as: ", "page_idx": 4}, {"type": "equation", "text": "$$\nh_{i}=\\sum_{\\forall j\\in\\Omega}S(E_{i j})\\bar{\\mathbf{B}}_{j}x_{j},\\quad S(E_{i j})=\\prod_{k\\in N_{i j}}\\bar{\\mathbf{A}}_{k},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\Omega$ denotes the index set of all vertices in the tree. $S(E_{i j})$ represents the path weight of hyperedge $E_{i j}$ traced from $j$ -th vertex to $i$ -th vertex in the tree $\\mathcal{G}_{T}$ , and $N_{i j}$ indicates the index set of all vertices on this hyperedge. For visual tasks, we iterate over each vertex, treating it as the root of the spanning tree $\\mathcal{G}_{T}$ , and aggregate the states from the other vertices, thereby obtaining the transformed states $\\{h_{i}\\}_{i=1}^{L}$ . For textual tasks, because of the causal prediction manner in large language models, we only take the last token as root and aggregate from other tokens. To achieve end-to-end training, we derive the derivative of the output hidden state $h_{i}$ to the input variables $\\bar{\\mathbf{A}}_{k}$ , $\\bar{\\mathbf{B}}_{j}$ and $x_{j}$ as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{\\partial h_{i}}{\\partial x_{j}}=S\\left(E_{i j}\\right)\\bar{\\bf B}_{j},\\quad\\frac{\\partial h_{i}}{\\partial\\bar{\\bf B}_{j}}=S\\left(E_{i j}\\right)x_{j}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{\\partial h_{i}}{\\partial\\bar{\\mathbf{A}}_{k}}=\\sum_{\\forall j\\in C_{k}^{i}}\\bar{\\mathbf{B}}_{j}x_{j}S(E_{k j})S(E_{i n}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $C_{k}^{i}$ indicates the children of vertex $k$ in tree $\\mathcal{G}_{T}$ whose root is the vertex $i$ , and $n$ denotes the parent of vertex $k$ in Eq. (7). Finally, the output feature $Y$ can be formulated as: ", "page_idx": 4}, {"type": "equation", "text": "$$\nY=\\mathbf{C}\\odot N o r m(H)+\\mathbf{D}\\odot X,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $Y$ , $H$ and $X$ indicate the stack of $\\{y_{i}\\}_{i=1}^{L}$ , $\\{h_{i}\\}_{i=1}^{L}$ and $\\{x\\}_{i=1}^{L}$ respectively. $\\odot$ denotes the element-wise multiplication. ", "page_idx": 4}, {"type": "text", "text": "Efficient Implementation for Multi-Modality. For visual tasks, the tree scanning algorithm requires two levels of traversal across the entire pixel set, resulting in an unacceptable quadratic complexity relative to the number of pixels $\\mathcal{O}(L^{\\bar{2}})$ . To alleviate this issue, we utilize a dynamic ", "page_idx": 4}, {"type": "text", "text": "Input: Input feature $\\{x_{i}\\}_{i=1}^{L}$ ; Input matrix $\\{\\bar{\\mathbf{B}}_{i}\\}_{i=1}^{L}$ ; State matrix $\\{\\bar{\\mathbf{A}}_{i}\\}_{i=1}^{L}$ ; Gradient of loss to hidden states $\\{\\frac{\\partial L o s s}{\\partial h_{i}}\\}_{i=1}^{L}$ ; Minimum Spanning Tree $\\mathcal{G}_{T}$ .   \nTraverse Path: $R o o t,\\dot{\\dots}\\dots,L e a f\\leftarrow B F S(\\mathcal{G}_{T})$ $\\vartriangleright$ Breadth-first topological order of $\\mathcal{G}_{T}$   \nForward: Initialization: $\\{\\xi_{i}\\}_{i=1}^{L}\\leftarrow\\{x_{i}\\}_{i=1}^{L}$ 2: for $i\\leftarrow L e a f$ to Root do $\\begin{array}{r}{\\xi_{i}=\\bar{\\mathbf{B}}_{i}x_{i}\\;\\!+\\sum_{\\forall j\\in\\{t|\\mathrm{Par}(t)=i\\}}\\xi_{j}\\bar{\\mathbf{A}}_{j}}\\end{array}$ 4: end for for $i\\leftarrow R o o t$ to Leaf do 6: if $i$ is Root then $h_{i}=\\xi_{i}$ 8: else $h_{i}=\\bar{\\mathbf{A}}_{i}(h_{\\mathrm{Par}(i)}-\\bar{\\mathbf{A}}_{i}\\xi_{i})+\\xi_{i}=(1-\\bar{\\mathbf{A}}_{i}^{2})\\xi_{i}+\\bar{\\mathbf{A}}_{i}h_{\\mathrm{Par}(i)}$   \n10: end if end for   \nBackward:   \n12: Initialization: {\u03b7i}iL=1 \u2190{ \u2202\u2202Lhos s}iL=1 for $i\\leftarrow L e a f$ to Root do   \n14: $\\begin{array}{r}{\\eta_{i}=\\bar{\\mathbf{B}}_{i}\\frac{\\partial\\check{L}o s s}{\\partial h_{i}}+\\sum_{\\forall j\\in\\{t|\\mathrm{Par}(t)=i\\}}\\eta_{j}\\bar{\\mathbf{A}}_{j}}\\end{array}$ end for   \n16: for $i\\leftarrow R o o t$ to Leaf do if $i$ is Root then   \n18: $\\begin{array}{r l}&{\\frac{\\partial L o s s}{\\partial x_{i}}=\\eta_{i}\\bar{\\mathbf{B}}_{i}\\,,\\qquad\\frac{\\partial L o s s}{\\partial\\mathbf{B}_{i}}=\\eta_{i}x_{i}\\,,\\qquad\\frac{\\partial L o s s}{\\partial\\mathbf{A}_{i}}=0}\\\\ &{\\frac{s\\mathbf{e}}{\\partial x_{i}}=(1-\\bar{\\mathbf{A}}_{i}^{2})\\eta_{i}\\bar{\\mathbf{B}}_{i}+\\bar{\\mathbf{A}}_{i}\\frac{\\partial L o s s}{\\partial x_{\\mathbf{n}(i)}}\\bar{\\mathbf{B}}_{i}\\,,\\qquad\\frac{\\partial L o s s}{\\partial\\mathbf{B}_{i}}=(1-\\bar{\\mathbf{A}}_{i}^{2})\\eta_{i}x_{i}+\\bar{\\mathbf{A}}_{i}\\frac{\\partial L o s s}{\\partial\\mathbf{B}_{\\mathbf{n}(i)}}x_{i}}\\\\ &{\\frac{\\partial L o s s}{\\partial\\underline{{A}}_{i}}=\\eta_{i}\\ast(h_{i}-\\bar{\\mathbf{A}}_{i}\\xi_{i})+\\xi_{i}\\ast(\\frac{\\partial L o s s}{\\partial x_{i}}-\\bar{\\mathbf{A}}_{i}\\eta_{i})=\\eta_{i}h_{i}+\\xi_{i}\\frac{\\partial L o s s}{\\partial x_{i}}-2\\eta_{i}\\xi_{i}\\bar{\\mathbf{A}}_{i}}\\end{array}$ el   \n20:   \n22: end if end for   \nOutput: Hidden states $\\{h_{i}\\}_{i=1}^{L}$ ; Grad. of loss to input feature $\\{\\frac{\\partial L o s s}{\\partial x_{i}}\\}_{i=1}^{L}$ ; Grad. of loss to input matrix $\\{\\frac{\\partial L o s s}{\\partial\\mathbf{B}_{i}}\\}_{i=1}^{L}$ ; Grad. of loss to state matrix $\\{\\frac{\\partial L o s s}{\\partial{\\bf A}_{i}}\\}_{i=1}^{L}$ . ", "page_idx": 5}, {"type": "text", "text": "programming procedure to accelerate the inference and training processes as elaborated in Algorithm 1, which results in linear complexity $\\mathcal{O}(L)$ . For textual tasks, we perform a unidirectional aggregation approach (shown in Algorithm 2 of Appendix B) in adherence to the causal nature of language. Moreover, we provide the back-propagation process for both Vision Tree Scanning and Language Tree Scanning processes, whose detailed proofs refer to Appendix C. ", "page_idx": 5}, {"type": "text", "text": "3.3 Application for Vision and Language ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "MambaTreeV Given an image with a shape of $H\\times W\\times3$ , our goal is to obtain high-quality visual features for downstream tasks. To this end, we propose an effective vision architecture MambaTreeV which consists of a stem module, several basic blocks and downsampling layers to generate hierarchical representations illustrated in Fig. 3. Overall, our MambaTreeV comprises four stages similar to previous general vision backbones [44, 43, 62, 41]. We integrate the stem module before the first stage to decrease the resolution of the input image signal by a factor of 4, resulting in a feature map with a shape of $\\textstyle{\\frac{H}{4}}\\times{\\frac{W}{4}}\\times C$ . It includes two convolutions, two Layer Normalization (LN) layers and one GELU activation function. The kernel size for both convolutions is 3 with a stride of 2 and padding of 1. Similarly, a downsampling layer consists of a $3\\times3$ convolution with a stride of 2 and padding of 1 and an LN layer. Positioned between two stages, it serves to downsample the input feature map by a factor of 2. Motivated by [62, 41], we devise a residual block with skip connections to integrate our fundamental Tree State Space Model in Sec. 3.2. In detail, we first normalize the input features with LN layer. Spatial priors and long-range dependencies are then obtained through our tree scanning algorithm with residual connections established alongside the input features. Finally, a feedforward neural network is utilized to project the normalized features to output signals as shown in Fig. 3. Based on the above origin components, we develop our MambaTreeV in three scales, i.e., MambaTreeV-Tiny, MambaTreeV-Small and MambaTreeV-Base. ", "page_idx": 5}, {"type": "table", "img_path": "W8rFsaKr4m/tmp/af8c671f0146a049ea727946e48731c1b38dd9835b807ec119e3ba238d3966c6.jpg", "table_caption": [], "table_footnote": ["Table 1: Image classification performance on the ImageNet-1K validation set. T, C and S indicate the model type of Transformer, CNN and SSM, respectively. All models take a scale of $224^{2}$ as input. "], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "MambaTreeL Recurrent neural networks rely on fixed memory to preserve past information, which poses limitations when handling long contexts where relevant words are distant from the current moment. While Mamba [19] employs a selection mechanism to enhance context awareness, its fixed memory size cannot expand over time, resulting in restricted state space. Therefore, the ability to extrapolate decreases during scrolling as the prompt extends. To mitigate this issue, we propose an effective fine-tuning paradigm. Specifically, the tree-based topology branch is built upon one-way scrolling with a scaling factor, enabling state transitions within such a structure. This arrangement facilitates the preferential interaction of semantically related tokens. It is noteworthy that this paradigm does not introduce any additional training parameters. Instead, it utilizes pretrained state transformation parameters to conduct semantic aggregation by incorporating topological structures. Experimental results demonstrate the effectiveness of our approach. ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We conduct extensive experiments to evaluate the effectiveness of MambaTreeV and compare it with advanced CNN-based, Transformer-based, and SSM-based models covering various downstream tasks, including image classification, object detection and semantic segmentation. Furthermore, we validate the capability of MambaTreeL in the field of natural language understanding. ", "page_idx": 6}, {"type": "text", "text": "4.1 Image Classification ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Settings. We assess the classification performance of MambaTreeV on the ImageNet-1k dataset [12]. Following previous practices [43, 44, 62, 41], all MambaTreeV models are trained for 300 epochs from scratch using AdamW optimizer with a warm-up strategy of 20 epochs. During training, we utilize a Cosine Scheduler with an initial learning rate of $1\\times10^{-3}$ and weight decay of 0.05. In addition, the exponential moving average (EMA) is also applied. ", "page_idx": 6}, {"type": "text", "text": "Results. The comparison results summarized in Table 1 show MambaTreeV leading all SSM-based models and competitive with advanced CNNs and Transformers across tiny, small, and base scales. Specifically, MambaTreeV-T achieves $83.4\\%$ Top-1 Acc. boosting ViM-S by $2.9\\%$ , LocalVim-S by $2.2\\%$ , PlainMamba-L2 by $1.8\\%$ and VMamba-T by $0.9\\%$ with similar FLOPs. Additionally, it surpasses ConvNeXt-T by $1.3\\%$ and Swin-T by $2.2\\%$ , demonstrating the effectiveness of our method. ", "page_idx": 6}, {"type": "image", "img_path": "W8rFsaKr4m/tmp/b8c15afb719768c6937a2e9ec2101caee03891f822d2678f081b13484c9bad2e.jpg", "img_caption": ["Figure 4: Visualization of affinity maps in the specific position. The Location is marked by the red cross in each input (a). TP is our tree topology scanning algorithm (b), which captures more detailed structural information and has a larger receptive field compared to raster scanning (c). "], "img_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "W8rFsaKr4m/tmp/6bbe83c09cf921a8cbb198c111660728ccfba9cfda95005f7ea4044201e848bb.jpg", "table_caption": ["Table 2: Semantic segmentation performance on ADE20K val set. The crop size is all set to $512^{2}$ . SS and MS denote singlescale and multi-scale testing, respectively. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.2 Object Detection ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Settings. We verify the detection performance of MambaTreeV on the MSCOCO 2017 dataset [39] with MMDetection library [3]. We follow previous works [41, 62, 43, 34, 53, 55, 74, 70, 6] to validate object detection and instance segmentation tasks with Mask-RCNN [29]. Specifically, We adopt the AdamW optimizer with a learning rate of $1\\times10^{-4}$ and batch size of 16 to optimize the model built upon our pre-trained classification backbones on ImageNet-1K. The training schedules include $1\\times$ (12 epochs) and $3\\times$ (36 epochs) with multi-scale data augmentation. ", "page_idx": 7}, {"type": "text", "text": "Results. As depicted in Table 8 (in Appendix A.), our method outperforms existing methods on most evaluation metrics, especially for instance segmentation. Under $1\\times$ schedule, MambaTreeV-T achieves 47.0 in box mAP $(\\mathbf{A}\\mathbf{P}^{b})$ , which is 1.1 points higher than ViM-S and 0.5 points higher than VMamba-T. It is worth noting that MambaTreeV-T outperforms ViM-S by 1.7 points with $1\\times$ schedule and LocalVMamba-T by 0.4 points with $3\\times$ schedule in mask mAP $(\\mathbf{A}\\mathbf{P}^{m})$ . Moreover, the best $\\mathrm{AP}^{b}\\;50.1$ and $\\mathrm{AP}^{m}~44.6\\$ are obtained by MambaTreeV-S in $3\\times$ schedule with multi-scale training. ", "page_idx": 7}, {"type": "text", "text": "4.3 Semantic Segmentation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Settings. To evaluate the semantic segmentation performance of our MambaTreeV series, we train our models with UperNet [65] initialized by pre-trained classification weights on ADE20K[75] for 160k iterations, following common practices without additional augmentations for fair comparison. ", "page_idx": 7}, {"type": "text", "text": "Results. Our method performs exceptionally well on segmentation tasks shown in Table 2. MambaTreeV-T yields a clear improvement of $+3.6$ in single-scale mIoU compared to ViM-S and $+1.9$ in multi-scale mIoU compared to LocalViM-S. Furthermore, MambaTreeV-S boosts InternImage-S by 0.6 and 0.8 in single-scale and multi-scale respectively. We consider the preservation of intricate structural details through tree topology scanning to be particularly advantageous for segmentation tasks that require pixel-level perception. ", "page_idx": 7}, {"type": "table", "img_path": "W8rFsaKr4m/tmp/ddacfbf2f7820e2d34838b6c50c5272574d9523971c7e51d1e41653cb720f46e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "W8rFsaKr4m/tmp/30b5eb28f6994382ed45f1f1c0a6476ef1749641278d05b5539c8f75f008c9ff.jpg", "table_caption": ["Table 3: Evaluation on language model benchmarks. Arc-E, WG, L-ppl and BQA indicate Arceasy [8], WinoGrande, LAMBADA [49] and Openbookqa [47] benchmark, respectively. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.4 Language Understanding ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We regard Mamba [19] with 130M parameters as the base model. To verify the effectiveness of our MambaTreeL in nature language understanding, we first fine-tune pre-trained Mamba via LoRA [33] and MambaTreeL under the same setting with the Alpaca data [58], which contains 52000 instruction tuning data for supervised fine-tuning. Then we utilize popular language benchmarks provided in the open-sourced lm-evaluation-harness project [18] for evaluation, including PIQA [1], AI2-ARC [8], SST [60], WinoGrande, LAMBADA [49], Race [36] and Openbookqa [47]. The results in Table 3 demonstrate that our MambaTreeL provides a benefti of $+1.1\\%$ in average Acc. compared to LoRA. Since the short prompt length of WinoGrande dataset, the performance degrades with a marginal gap. ", "page_idx": 8}, {"type": "text", "text": "4.5 Ablation Study & Qualitative Results ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we conduct analysis experiments on ImageNet-1K dataset and present some visual results to illustrate the effectiveness of our algorithm. ", "page_idx": 8}, {"type": "text", "text": "Scanning Strategy. We conduct a head-to-head comparison of different scanning strategies, as shown in Table 4. The tree topology scanning outperforms previous strategies by $\\bar{0.8\\%}$ and $0.3\\%$ , highlighting the superiority of our algorithm in vision recognition. ", "page_idx": 8}, {"type": "text", "text": "Distance Metric. Before generating a minimum spanning tree from a connected graph, it is important to measure the edge weights between vertices. Therefore, we validate several distance metrics as illustrated in Table 5. The results indicate that Cosine distance most effectively represents the relationship between vertices, performing $0.5\\%$ better than Manhattan and $0.2\\%$ better than Euclidean. ", "page_idx": 8}, {"type": "text", "text": "Root Setting. We traverse all vertices, treating each as a root, and perform state transitions along the topological path from the other vertices toward the root. This traversal ensures that each vertex captures long-range dependencies. To verify the effectiveness of this operation, we consider only the first and last vertices as the root in Table 6. The results show reductions of $0.5\\%$ and $0.4\\%$ , respectively. ", "page_idx": 8}, {"type": "text", "text": "Inference speed comparison. As shown in Table 7, we report the inference throughputs of our method on an Nvidia V100 GPU. MambaTreeV- $\\cdot\\Gamma^{*}$ refers to each stage sharing the same tree topology structure, which enhances efficiency without compromising accuracy. To achieve better practical inference speed, we also introduce a cuda implementation optimized for GPUs. Compared with other counterparts, our approach exhibits superior effectiveness and faster inference speed. ", "page_idx": 8}, {"type": "text", "text": "Qualitative Results. To better illustrate the superiority of our scanning strategy, we visualize the affinity maps of different positions marked by the red cross in each input image. For example, we set the anchor point in the upper left corner of the sky as shown in the second row of in Fig. 4(a). Our method can easily identify white houses, flagpoles, and the sky, which raster scanning fails to achieve. This demonstrates the capability of our algorithm to preserve detailed structural information. More comparisons can be seen in Fig. 6 (in Appendix D.) ", "page_idx": 8}, {"type": "table", "img_path": "W8rFsaKr4m/tmp/08065901ab6eb37bb3a8d90bd15bee8c11553787b4baf52cf0d818fa11febe3b.jpg", "table_caption": [], "table_footnote": ["Table 7: Runtime comparison on an Nvidia V100 GPU during inference. "], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "5 Conclusion & Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we propose a tree state space model to perform feature propagation on an input-aware topology. Besides, we introduce a linear complexity dynamic programming algorithm to enhance long-range interactions without increasing computational cost. With the proposed techniques, we establish the general multi-modal networks to break the original sequence constraints and achieve stronger representation capabilities. Extensive experiments demonstrate the effectiveness of our method in both visual and language tasks. The limitation of our method is that the tree structure is not a common paradigm, and it needs to be specifically optimized according to the hardware device. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by the STI 2030-Major Projects under Grant 2021ZD0201404. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Bisk, Y., Zellers, R., Gao, J., Choi, Y., et al.: Piqa: Reasoning about physical commonsense in natural language. In: AAAI. pp. 7432\u20137439 (2020) 9   \n[2] Boru\u02davka, O.: O jist\u00e9m probl\u00e9mu minim\u00e1ln\u00edm (1926) 5   \n[3] Chen, K., Wang, J., Pang, J., Cao, Y., Xiong, Y., Li, X., Sun, S., Feng, W., Liu, Z., Xu, J., et al.: Mmdetection: Open mmlab detection toolbox and benchmark. arXiv preprint arXiv:1906.07155 (2019) 8   \n[4] Chen, Z., Duan, Y., Wang, W., He, J., Lu, T., Dai, J., Qiao, Y.: Vision transformer adapter for dense predictions. arXiv preprint arXiv:2205.08534 (2022) 16   \n[5] Cheng, C., Song, L., Xue, R., Wang, H., Sun, H., Ge, Y., Shan, Y.: Meta-adapter: An online few-shot learner for vision-language model. arXiv preprint arXiv:2311.03774 (2023) 2   \n[6] Cheng, T., Song, L., Ge, Y., Liu, W., Wang, X., Shan, Y.: Yolo-world: Real-time openvocabulary object detection. arXiv preprint arXiv:2401.17270 (2024) 8   \n[7] Chung, J., Gulcehre, C., Cho, K., Bengio, Y.: Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555 (2014) 1   \n[8] Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., Tafjord, O.: Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457 (2018) 9   \n[9] Cubuk, E.D., Zoph, B., Mane, D., Vasudevan, V., Le, Q.V.: Autoaugment: Learning augmentation strategies from data. In: CVPR. pp. 113\u2013123 (2019) 15   \n[10] Dai, J., Qi, H., Xiong, Y., Li, Y., Zhang, G., Hu, H., Wei, Y.: Deformable convolutional networks. In: ICCV. pp. 764\u2013773 (2017) 2   \n[11] Dai, Z., Liu, H., Le, Q.V., Tan, M.: Coatnet: Marrying convolution and attention for all data sizes. NeurIPS 34, 3965\u20133977 (2021) 2, 7   \n[12] Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-scale hierarchical image database. In: CVPR. pp. 248\u2013255. Ieee (2009) 2, 7   \n[13] Ding, X., Zhang, Y., Ge, Y., Zhao, S., Song, L., Yue, X., Shan, Y.: Unireplknet: A universal perception large-kernel convnet for audio, video, point cloud, time-series and image recognition. CVPR (2023) 1, 2, 7, 8   \n[14] Dong, X., Bao, J., Chen, D., Zhang, W., Yu, N., Yuan, L., Chen, D., Guo, B.: Cswin transformer: A general vision transformer backbone with cross-shaped windows. In: CVPR. pp. 12124\u201312134 (2022) 1, 2, 16   \n[15] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N.: An image is worth 16x16 words: Transformers for image recognition at scale. In: ICLR (2021) 1, 2   \n[16] Fang, C., He, C., Xiao, F., Zhang, Y., Tang, L., Zhang, Y., Li, K., Li, X.: Real-world image dehazing with coherence-based label generator and cooperative unfolding network. arXiv preprint arXiv:2406.07966 (2024) 15   \n[17] Fu, D., Arora, S., Grogan, J., Johnson, I., Eyuboglu, E.S., Thomas, A., Spector, B., Poli, M., Rudra, A., R\u00e9, C.: Monarch mixer: A simple sub-quadratic gemm-based architecture. NeurIPS 36 (2023) 7   \n[18] Gao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPof,i A., Foster, C., Golding, L., Hsu, J., Le Noac\u2019h, A., Li, H., McDonell, K., Muennighoff, N., Ociepa, C., Phang, J., Reynolds, L., Schoelkopf, H., Skowron, A., Sutawika, L., Tang, E., Thite, A., Wang, B., Wang, K., Zou, A.: A framework for few-shot language model evaluation (12 2023) 9   \n[19] Gu, A., Dao, T.: Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752 (2023) 1, 2, 3, 4, 7, 9   \n[20] Gu, A., Dao, T., Ermon, S., Rudra, A., R\u00e9, C.: Hippo: Recurrent memory with optimal polynomial projections. NeurIPS 33, 1474\u20131487 (2020) 3   \n[21] Gu, A., Goel, K., Gupta, A., R\u00e9, C.: On the parameterization and initialization of diagonal state space models. NeurIPS 35, 35971\u201335983 (2022) 3   \n[22] Gu, A., Goel, K., R\u00e9, C.: Efficiently modeling long sequences with structured state spaces. In: ICLR (2022) 1, 2, 3   \n[23] Gu, A., Johnson, I., Goel, K., Saab, K., Dao, T., Rudra, A., R\u00e9, C.: Combining recurrent, convolutional, and continuous-time models with linear state space layers. NeurIPS 34, 572\u2013585 (2021) 2, 3   \n[24] Gupta, A., Gu, A., Berant, J.: Diagonal state spaces are as effective as structured state spaces. NeurIPS 35, 22982\u201322994 (2022) 1, 3   \n[25] Han, K., Wang, Y., Xu, C., Guo, J., Xu, C., Wu, E., Tian, Q.: Ghostnets on heterogeneous devices via cheap operations. IJCV 130(4), 1050\u20131069 (2022) 2   \n[26] Hasani, R., Lechner, M., Wang, T.H., Chahine, M., Amini, A., Rus, D.: Liquid structural state-space models. arXiv preprint arXiv:2209.12951 (2022) 3   \n[27] He, C., Li, K., Zhang, Y., Xu, G., Tang, L., Zhang, Y., Guo, Z., Li, X.: Weakly-supervised concealed object segmentation with sam-based pseudo labeling and multi-scale feature grouping. Advances in Neural Information Processing Systems 36 (2024) 15   \n[28] He, C., Shen, Y., Fang, C., Xiao, F., Tang, L., Zhang, Y., Zuo, W., Guo, Z., Li, X.: Diffusion models in low-level vision: A survey. arXiv preprint arXiv:2406.11138 (2024) 15 ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "[29] He, K., Gkioxari, G., Doll\u00e1r, P., Girshick, R.: Mask r-cnn. In: ICCV. pp. 2961\u20132969 (2017) 8 ", "page_idx": 11}, {"type": "text", "text": "[30] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: CVPR. pp. 770\u2013778 (2016) 1, 2 [31] Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural computation 9(8), 1735\u20131780 (1997) 1 [32] Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., Andreetto, M., Adam, H.: Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861 (2017) 1, 2 [33] Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W.: Lora: Low-rank adaptation of large language models. In: ICLR (2022) 2, 9 [34] Huang, T., Pei, X., You, S., Wang, F., Qian, C., Xu, C.: Localmamba: Visual state space model with windowed selective scan. arXiv preprint arXiv:2403.09338 (2024) 1, 3, 4, 7, 8, 10, 15, 16 [35] Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep convolutional neural networks. NeurIPS 25 (2012) 2 [36] Lai, G., Xie, Q., Liu, H., Yang, Y., Hovy, E.H.: RACE: large-scale reading comprehension dataset from examinations. In: EMNLP. pp. 785\u2013794. Association for Computational Linguistics (2017) 9 [37] Li, S., Singh, H., Grover, A.: Mamba-nd: Selective state space modeling for multi-dimensional data. arXiv preprint arXiv:2402.05892 (2024) 7 [38] Li, Y., Song, L., Chen, Y., Li, Z., Zhang, X., Wang, X., Sun, J.: Learning dynamic routing for semantic segmentation. In: CVPR (2020) 2 [39] Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll\u00e1r, P., Zitnick, C.L.: Microsoft coco: Common objects in context. In: ECCV. pp. 740\u2013755. Springer (2014) 2, 8 [40] Liu, S., Chen, T., Chen, X., Chen, X., Xiao, Q., Wu, B., K\u00e4rkk\u00e4inen, T., Pechenizkiy, M., Mocanu, D., Wang, Z.: More convnets in the 2020s: Scaling up kernels beyond 51x51 using sparsity. arXiv preprint arXiv:2207.03620 (2022) 7, 8 [41] Liu, Y., Tian, Y., Zhao, Y., Yu, H., Xie, L., Wang, Y., Ye, Q., Liu, Y.: Vmamba: Visual state space model. arXiv preprint arXiv:2401.10166 (2024) 1, 3, 4, 6, 7, 8, 10, 15, 16 [42] Liu, Z., Hu, H., Lin, Y., Yao, Z., Xie, Z., Wei, Y., Ning, J., Cao, Y., Zhang, Z., Dong, L., et al.: Swin transformer v2: Scaling up capacity and resolution. In: CVPR. pp. 12009\u201312019 (2022)   \n1, 2 [43] Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. In: ICCV. pp. 10012\u201310022 (2021) 1, 2,   \n6, 7, 8, 15, 16 [44] Liu, Z., Mao, H., Wu, C.Y., Feichtenhofer, C., Darrell, T., Xie, S.: A convnet for the 2020s. In: CVPR. pp. 11976\u201311986 (2022) 1, 6, 7, 8, 16 [45] Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101 (2017) 15 [46] Luo, Z., Xiao, Y., Liu, Y., Li, S., Wang, Y., Tang, Y., Li, X., Yang, Y.: Soc: Semanticassisted object cluster for referring video object segmentation. Advances in Neural Information Processing Systems 36 (2024) 15 [47] Mihaylov, T., Clark, P., Khot, T., Sabharwal, A.: Can a suit of armor conduct electricity? A new dataset for open book question answering. In: EMNLP. pp. 2381\u20132391. Association for Computational Linguistics (2018) 9   \n[48] Nguyen, E., Goel, K., Gu, A., Downs, G.W., Shah, P., Dao, T., Baccus, S.A., R\u00e9, C.: S4nd: Modeling images and videos as multidimensional signals using state spaces. arXiv preprint arXiv:2210.06583 (2022) 7   \n[49] Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al.: Language models are unsupervised multitask learners. OpenAI blog 1(8), 9 (2019) 9, 19   \n[50] Ren, S., Yang, X., Liu, S., Wang, X.: Sg-former: Self-guided transformer with evolving token reallocation. In: ICCV. pp. 6003\u20136014 (2023) 7   \n[51] Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale image recognition. In: Bengio, Y., LeCun, Y. (eds.) ICLR (2015) 2   \n[52] Smith, J.T., Warrington, A., Linderman, S.W.: Simplified state space layers for sequence modeling. arXiv preprint arXiv:2208.04933 (2022) 1, 2   \n[53] Song, L., Li, Y., Jiang, Z., Li, Z., Sun, H., Sun, J., Zheng, N.: Fine-grained dynamic head for object detection. NIPS (2020) 8   \n[54] Song, L., Li, Y., Li, Z., Yu, G., Sun, H., Sun, J., Zheng, N.: Learnable tree filter for structurepreserving feature transform. NeurIPS 32 (2019) 2, 5   \n[55] Song, L., Zhang, S., Yu, G., Sun, H.: Tacnet: Transition-aware context network for spatiotemporal action detection. In: CVPR (2019) 2, 8   \n[56] Song, L., Zhang, S., Liu, S., Li, Z., He, X., Sun, H., Sun, J., Zheng, N.: Dynamic grained encoder for vision transformers. NIPS (2021) 2   \n[57] Tang, L., Li, K., He, C., Zhang, Y., Li, X.: Source-free domain adaptive fundus image segmentation with class-balanced mean teacher. In: International Conference on Medical Image Computing and Computer-Assisted Intervention. pp. 684\u2013694. Springer (2023) 15   \n[58] Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C., Liang, P., Hashimoto, T.B.: Stanford alpaca: An instruction-following llama model (2023) 9   \n[59] Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., J\u00e9gou, H.: Training data-efficient image transformers & distillation through attention. In: ICML. pp. 10347\u201310357. PMLR (2021) 1, 7   \n[60] Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., Bowman, S.R.: GLUE: A multi-task benchmark and analysis platform for natural language understanding. In: ICLR (2019) 9   \n[61] Wang, J., Song, L., Li, Z., Sun, H., Sun, J., Zheng, N.: End-to-end object detection with fully convolutional network. In: CVPR (2021) 2   \n[62] Wang, W., Dai, J., Chen, Z., Huang, Z., Li, Z., Zhu, X., Hu, X., Lu, T., Lu, L., Li, H., et al.: Internimage: Exploring large-scale vision foundation models with deformable convolutions. In: CVPR. pp. 14408\u201314419 (2023) 1, 2, 6, 7, 8, 15, 16   \n[63] Wang, W., Xie, E., Li, X., Fan, D.P., Song, K., Liang, D., Lu, T., Luo, P., Shao, L.: Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In: ICCV. pp. 568\u2013578 (2021) 2   \n[64] Williams, R.L., Lawrence, D.A., et al.: Linear state-space control systems. John Wiley & Sons (2007) 3   \n[65] Xiao, T., Liu, Y., Zhou, B., Jiang, Y., Sun, J.: Unified perceptual parsing for scene understanding. In: ECCV. pp. 418\u2013434 (2018) 8   \n[66] Xiao, Y., Luo, Z., Liu, Y., Ma, Y., Bian, H., Ji, Y., Yang, Y., Li, X.: Bridging the gap: A unified video comprehension framework for moment retrieval and highlight detection. CVPR (2024) 15   \n[67] Xiao, Y., Ma, Y., Li, S., Zhou, H., Liao, R., Li, X.: Semanticac: Semantics-assisted framework for audio classification. In: ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). pp. 1\u20135. IEEE (2023) 15   \n[68] Xu, Z., Lin, Y., Han, H., Yang, S., Li, R., Zhang, Y., Li, X.: Mambatalk: Efficient holistic gesture synthesis with selective state space models. arXiv preprint arXiv:2403.09471 (2024) 3   \n[69] Yang, C., Chen, Z., Espinosa, M., Ericsson, L., Wang, Z., Liu, J., Crowley, E.J.: Plainmamba: Improving non-hierarchical mamba in visual recognition. arXiv preprint arXiv:2403.17695 (2024) 2, 3, 4, 7, 8, 10   \n[70] Yang, J., Song, L., Liu, S., Li, Z., Li, X., Sun, H., Sun, J., Zheng, N.: Dbq-ssd: Dynamic ball query for efficient 3d object detection. arXiv preprint arXiv:2207.10909 (2022) 8   \n[71] Yang, Q.: Stereo matching using tree filtering. IEEE TPAMI 37(4), 834\u2013846 (2014) 2, 5   \n[72] Yang, R., Song, L., Ge, Y., Li, X.: Boxsnake: Polygonal instance segmentation with box supervision. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (2023) 2   \n[73] Zhang, S., Song, L., Gao, C., Sang, N.: Glnet: Global local network for weakly supervised action localization. IEEE Transactions on Multimedia 22(10), 2610\u20132622 (2019) 2   \n[74] Zhang, S., Song, L., Liu, S., Ge, Z., Li, Z., He, X., Sun, J.: Workshop on autonomous driving at cvpr 2021: Technical report for streaming perception challenge. arXiv preprint arXiv:2108.04230 (2021) 8   \n[75] Zhou, B., Zhao, H., Puig, X., Fidler, S., Barriuso, A., Torralba, A.: Scene parsing through ade20k dataset. In: CVPR. pp. 633\u2013641 (2017) 2, 8   \n[76] Zhou, H., Yang, R., Zhang, Y., Duan, H., Huang, Y., Hu, R., Li, X., Zheng, Y.: Unihead: unifying multi-perception for detection heads. TNNLS (2023) 15   \n[77] Zhu, L., Liao, B., Zhang, Q., Wang, X., Liu, W., Wang, X.: Vision mamba: Efficient visual representation learning with bidirectional state space model. arXiv preprint arXiv:2401.09417 (2024) 1, 3, 4, 7, 8, 16 ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A Detailed Training Settings and Results ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Image Classification. ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We follow the previous works [62, 41, 43] to conduct the experiments. The models are trained with thirty-two 32GB V100 GPUs by default. We set betas and momentum of the AdamW [45, 76, 66] optimizer with (0.9, 0.999) and 0.9, respectively. During training, we utilize a Cosine Scheduler with an initial learning rate of $\\mathrm{1\\times10^{-3}}$ and weight decay of 0.05. We adopt the common training data augmentation strategies following [34, 62], including AutoAugment [9] with rand-m9-mstd0.5-inc1. A MixUp strategy with a ratio of 0.8 is also adopted in each batch. Horizontal flip and Random resized crop strategy are both used in the process of training. ", "page_idx": 14}, {"type": "image", "img_path": "W8rFsaKr4m/tmp/0862dc86cc759c1eaba2ca9816482cf3a4dd8f4850584f8dddd3dfc0a8d1ad24.jpg", "img_caption": ["Figure 5: Classification performance comparison among SSM-based vision foundation models. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "Performance Comparison. We compare various SSM-based visual foundation models as shown in Fig. 5, with different colors representing different models and different shapes indicating different model scales. The size of each shape indicates the number of model parameters. The horizontal axis denotes FLOPs and the vertical axis represents the Top-1 accuracy of the corresponding method on ImageNet-1K val dataset. Fig. 5 demonstrates that MambaTreeV is the best choice in terms of efficiency and effectiveness. ", "page_idx": 14}, {"type": "text", "text": "A.2 Object Detection. ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "For a fair comparison, we conduct the evaluation following common practice [62, 41, 43]. The models are trained with eight 32GB V100 GPUs by default. The input image is resized so that the shorter side is 800 pixels, while the longer side does not exceed 1333 pixels during the $1\\times$ schedule. The number of warmup steps is set to 500 in the $1\\times$ schedule. For $3\\times$ schedule, the shorter side is resized to 480-800 pixels and the longer side does not exceed 1333 pixels. The number of warmup steps is set to 1000 in $3\\times$ schedule. Results shown in Table 8 demonstrate the effectiveness of MambaTreeV in object detection and instance segmentation on COCO val2017. ", "page_idx": 14}, {"type": "text", "text": "A.3 Semantic Segmentation. ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We optimize our MambaTreeV-T/S using AdamW optimizer with an initial learning rate of $6\\times10^{-5}$ which is decayed by a rate of 1.0 with the polynomial decay schedule following [62, 27, 57, 16, 28]. The number of warmup iters is set to 1600 with an initial learning rate of $1\\times\\bar{1}0^{-6}$ [41, 34, 46, 67]. The default input resolution is $512\\times512$ as well as FLOPs are calculated with an input size of $512\\times2048$ . The models are trained with eight 32GB V100 GPUs by default. ", "page_idx": 14}, {"type": "table", "img_path": "W8rFsaKr4m/tmp/41d2072e5118b564ce09b2a398534bfd8ad543d82b62e141e083985a311dc909.jpg", "table_caption": [], "table_footnote": ["Table 8: Object detection and instance segmentation performance on COCO val2017. $\\boldsymbol{\\mathbf{A}}\\boldsymbol{\\mathbf{P}}^{b}$ and $\\mathbf{A}\\mathbf{P}^{m}$ indicate the mAP of detection and segmentation, respectively. MS indicates the multi-scale training strategy. "], "page_idx": 15}, {"type": "text", "text": "B Language Tree Topology Scanning Operator ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Algorithm 2 Language Tree Scanning   \nInput: Input feature $\\{x_{i}\\}_{i=1}^{L}$ ; Input matrix $\\{\\bar{\\mathbf{B}}_{i}\\}_{i=1}^{L}$ ; State matrix $\\{\\bar{\\mathbf{A}}_{i}\\}_{i=1}^{L}$ ; Gradient of loss to hidden states $\\{\\frac{\\partial L o s s}{\\partial h_{i}}\\}_{i=1}^{L}$ ; Minimum Spanning Tree $\\mathcal{G}_{T}$ .   \nTraverse Path: $R o o t,\\dot{\\dots}\\dots,L e a f\\leftarrow B F S(\\mathcal{G}_{T})$ $\\vartriangleright$ Breadth-first topological order of $\\mathcal{G}_{T}$   \nForward: Initialization: $\\{\\xi_{i}\\}_{i=1}^{L}\\leftarrow\\{x_{i}\\}_{i=1}^{L}$ 2: for $i\\leftarrow L e a f$ to Root do $\\begin{array}{r}{\\xi_{i}=\\bar{\\mathbf{B}}_{i}x_{i}+\\sum_{\\forall j\\in\\{t|\\mathrm{Par}(t)=i\\}}\\xi_{j}\\bar{\\mathbf{A}}_{j}}\\end{array}$ 4: end for   \nBackward: for $i\\leftarrow R o o t$ to Leaf do 6: if $i$ is Root then 8: el $\\begin{array}{r l}&{\\frac{\\theta\\Delta\\mathbf{\\rho}_{a s s}^{\\mathbf{\\rho}\\mathbf{\\rho}\\mathbf{\\rho}\\mathbf{\\rho}\\mathbf{\\rho}\\mathbf{m}\\mathbf{\\rho}\\mathbf{m}}}{\\partial x_{i}}=\\eta_{i}\\bar{\\mathbf{B}}_{i}\\,,\\quad\\frac{\\partial L o s s}{\\partial\\bar{\\mathbf{B}}_{i}}=\\eta_{i}x_{i},\\quad\\frac{\\partial L o s s}{\\partial\\bar{\\mathbf{A}}_{i}}=0}\\\\ &{\\frac{\\partial L o s s}{\\partial x_{i}}=\\frac{\\partial L o s s}{\\partial h_{i}}\\bar{\\mathbf{B}}_{i}+\\bar{\\mathbf{A}}_{i}\\frac{\\partial L o s s}{\\partial x_{\\mathrm{pr}(i)}}\\bar{\\mathbf{B}}_{i}\\,,\\quad\\frac{\\partial L o s s}{\\partial\\mathbf{B}_{i}}=\\frac{\\partial L o s s}{\\partial h_{i}}x_{i}+\\bar{\\mathbf{A}}_{i}\\frac{\\partial L o s s}{\\partial\\mathbf{B}_{\\mathrm{pr}(i)}}x_{i}}\\\\ &{\\frac{\\partial L o s s}{\\partial\\bar{\\mathbf{A}}_{i}}=\\frac{\\partial L o s s}{\\partial x_{P a r(i)}^{\\prime}}h_{i}}\\end{array}$   \n10: end if   \n12: end for   \nOutput: Hidden states $\\{h_{i}\\}_{i=1}^{L}$ ; Grad. of loss to input feature $\\{\\frac{\\partial L o s s}{\\partial x_{i}}\\}_{i=1}^{L}$ ; Grad. of loss to input matrix $\\begin{array}{r}{\\{\\frac{\\partial L o s s}{\\partial\\bar{\\bf B}_{i}}\\}_{i=1}^{L}}\\end{array}$ ; Grad. of loss to state matrix $\\begin{array}{r}{\\{\\frac{\\partial L o s s}{\\partial\\mathbf{A}_{i}}\\}_{i=1}^{L}}\\end{array}$ . ", "page_idx": 15}, {"type": "text", "text": "C Algorithm Proof ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we present detailed proofs for our tree scanning algorithm. The definitions of symbols are consistent with those in the main paper. ", "page_idx": 15}, {"type": "text", "text": "C.1 Proof for Algorithm 1. ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We randomly take a vertex in the MST $\\mathcal{G}_{T}$ as the root. According to the definition of the tree scanning algorithm introduced in Sec. 3.2, we can derive $h_{r o o t}$ as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\nh_{r o o t}=\\sum_{\\forall j\\in C_{r o o t}}S(E_{r o o t,j})\\bar{\\mathbf{B}}_{j}x_{j},\\quad S(E_{r o o t,j})=\\prod_{k\\in N_{r o o t,j}}\\bar{\\mathbf{A}}_{k},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which shows a process of aggregation from all leaf vertices to the root. Therefore, each vertex is only related to its child in this period. Taking vertex $m$ as an example, the $\\mathrm{Aggr}_{m}$ can be derived as: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname{Aggr}_{m}(x)={\\bar{\\mathbf{B}}}_{m}x_{m}+\\sum_{\\forall k\\in\\{t|\\mathrm{Par}(t)=i\\}}\\operatorname{Aggr}_{k}(x){\\bar{\\mathbf{A}}}_{k}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We assume that one of the child of $m$ is $n$ and $h_{n}$ can be derived as following: ", "page_idx": 16}, {"type": "equation", "text": "$$\nh_{n}=\\mathbf{Aggr}_{n}(x)+\\widetilde{\\mathbf{A}}_{n}\\widetilde{\\mathbf{Aggr}}_{m}(x),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\widetilde{\\mathrm{Aggr}}_{m}(x)$ indicates the aggregation value from the vertices $\\in\\Omega\\backslash C_{m}^{r o o t}$ to vertex $m$ . Therefore, we can obtain the propagation relationship between the hidden state of parent $m$ and child $n$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{h_{n}=\\operatorname{Aggr}_{n}(x)+\\bar{\\mathbf{A}}_{n}\\widetilde{\\operatorname{Aggr}}_{m}(x)}\\\\ &{\\quad=\\operatorname{Aggr}_{n}(x)+\\bar{\\mathbf{A}}_{n}(h_{m}-\\bar{\\mathbf{A}}_{n}\\mathrm{Aggr}_{n}(x))}\\\\ &{\\quad=\\bar{\\mathbf{A}}_{n}h_{m}+(1-\\bar{\\mathbf{A}}_{n}^{2})\\mathrm{Aggr}_{n}(x)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Through the above derivation, we can calculate $\\{h_{i}\\}_{i=1}^{L}$ with only two traversals (i.e., the aggregation from $l e a f$ to root and the propagation from root to $l e a f$ ) in the forward process as shown in Algorithm 1, thereby reducing the computational complexity from $\\mathcal{O}(L^{2})$ to $\\mathcal{O}(L)$ . ", "page_idx": 16}, {"type": "text", "text": "Next, we analyze the backpropagation process in Algorithm 1. According to the chain rule, we can easily calculate the derivative of loss with respect to $x_{i}$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\displaystyle\\frac{\\partial l o s s}{\\partial x_{i}}=\\sum_{j\\in\\Omega}\\frac{\\partial\\log s}{\\partial h_{j}}\\frac{\\partial h_{j}}{\\partial x_{i}}\\ ~~~~~}\\\\ {\\displaystyle=\\bar{\\mathbf{B}}_{i}\\sum_{j\\in\\Omega}S\\left(E_{j i}\\right)\\frac{\\partial l o s s}{\\partial h_{j}}\\ ~~~~~}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Similarly, the derivative of loss with respect to $\\bar{\\mathbf{B}}_{i}$ is: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\displaystyle\\frac{\\partial l o s s}{\\partial\\bar{\\bf B}_{i}}=\\sum_{j\\in\\Omega}\\displaystyle\\frac{\\partial\\log s}{\\partial h_{j}}\\displaystyle\\frac{\\partial h_{j}}{\\partial\\bar{\\bf B}_{i}}}\\\\ {\\displaystyle=x_{i}\\sum_{j\\in\\Omega}S\\left(E_{j i}\\right)\\displaystyle\\frac{\\partial l o s s}{\\partial h_{j}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The above formulas are equivalent to replacing the input $x$ with $\\frac{\\partial l o s s}{\\partial h}$ during the forward process. ", "page_idx": 16}, {"type": "text", "text": "Subsequently, we assume that the vertex $k$ is the child of vertex $l$ and define $C_{l}^{k}$ indicates the children of vertex $l$ with the root of vertex $k$ . $\\frac{\\partial l o s s}{\\partial{\\bf A}_{k}}$ is formulated as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\frac{\\partial I_{\\mathrm{drss}}}{\\partial\\mathbf{A}_{k}}-\\sum_{\\ell=0}^{\\infty}\\frac{\\partial I_{\\ell-1}}{\\partial h_{\\ell}}\\frac{\\partial I_{\\ell}}{\\partial\\mathbf{A}_{k}}}}\\\\ &{}&{=\\sum_{\\ell=0}^{\\infty}\\frac{\\partial I_{\\ell-1}}{\\partial h_{\\ell}}\\frac{\\partial^{2}\\delta(E_{\\ell})\\mathbf{B}_{p}\\tau_{p}^{\\prime}}{\\partial\\ell\\tau_{\\ell}^{2}}}\\\\ &{}&{=\\sum_{\\ell=\\ell_{1}^{\\prime}}\\frac{\\partial I_{\\ell-3}}{\\partial h_{\\ell}}\\sum_{\\ell=\\ell_{1}^{\\prime}}^{S}S(E_{k})S(E_{\\ell})\\mathbb{B}_{p}\\tau_{p}^{\\prime}+\\sum_{\\ell\\in\\mathcal{C}_{1}}^{\\infty}\\frac{\\partial I_{\\ell-3}}{\\partial h_{\\ell}}\\sum_{\\rho\\in\\mathcal{C}_{1}^{\\prime}}S(E_{k})S(E_{\\mu})\\mathbb{B}_{p}\\tau_{p}^{\\prime}}\\\\ &{}&{=\\sum_{\\ell=\\ell_{1}^{\\prime}}^{\\infty}S(E_{k})\\frac{\\partial\\alpha\\mathbf{s}}{\\partial h_{\\ell}}\\sum_{\\ell=\\ell_{1}^{\\prime}}^{S}S(E_{k})\\mathbb{B}_{p}\\tau_{p}^{\\prime}+\\sum_{\\ell=1}^{S}S(E_{k})\\frac{\\partial\\alpha\\mathbf{s}}{\\partial h_{\\ell}}\\sum_{\\ell=\\ell_{1}^{\\prime}}^{S}S(E_{k})\\mathbb{B}_{p}\\tau_{p}^{\\prime}}\\\\ &{}&{=\\sum_{\\ell=\\ell_{1}^{\\prime}}^{S}(E_{k})\\frac{\\partial\\alpha\\mathbf{s}}{\\partial h_{\\ell}}\\frac{\\partial^{2}}{\\partial\\rho\\tau_{\\ell}^{2}}}\\\\ &{}&{=(\\frac{\\partial E_{\\mathcal{B}}\\mathbf{s}}{\\partial L}-\\Tilde{A}_{\\mathcal{A}}\\mathbb{B}_{\\mathcal{B}_{\\ell}}(\\frac{\\partial\\alpha\\mathbf{s}}{\\partial h_{\\ell}})+\\Delta\\mathbf{g}_{\\mathcal{B}_{\\ell}}(\\boldsymbol{x})+\\Delta\\mathbf{g}_{\\mathcal{B}_{\\ell}}(\\frac{\\partial\\alpha\\mathbf{s}}{\\partial h_ \n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "So far we have completed the proof of forward and back-propagation of Algorithm 1. ", "page_idx": 17}, {"type": "text", "text": "C.2 Proof for Algorithm 2. ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We only take the last token as root and replace the transition source from $\\Omega$ to $C_{i}$ in sequence modeling tasks like nature language understanding to ensure causality. Therefore, only one traversal (from leaf to root) is required for the forward process, and another traversal (from root to $l e a f$ ) is needed for the backpropagation process. The proof is similar to the Algorithm 1. ", "page_idx": 17}, {"type": "image", "img_path": "W8rFsaKr4m/tmp/93de734c51b99f9939652203f078fab2e538233063305cc68fe05b685ec289f8.jpg", "img_caption": ["Figure 6: Visualization of affinity maps in the specific position. The Location is marked by the red cross in each affinity map. TP represents our Tree Scanning Algorithm. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "D More Qualitative Results ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Fig. 6 displays additional qualitative comparisons between our algorithm and previous scanning strategies (e.g., cross-scanning and raster-scanning), which shows our advanced capability to perceive detailed structural information and capture long-range dependencies. ", "page_idx": 18}, {"type": "text", "text": "E Statistical Significance ", "text_level": 1, "page_idx": 18}, {"type": "table", "img_path": "W8rFsaKr4m/tmp/d7aae466e52d89e17afe565f0086ebda92f1e4544458a4f510ebf5410b83a429.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Table 9: Standard error on language model benchmarks. LAM-ppl indicates LAMBADA [49]. We calculate the standard deviation of our MambaTreeL on language model benchmarks in the open-sourced lm-evaluation-harness project as shown in Table 9. ", "page_idx": 18}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: The claims in the abstract and introduction accurately reflect our motivation and contribution. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 19}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: We have discussed the limitations of our work. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 19}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We provide detailed proofs for our tree topology scanning algorithm in both the main paper and the appendix. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 20}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We describe the experimental details both in the main paper and the appendix.   \nWe will make our code publicly available, along with detailed instructions for reproduction. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 20}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We provide the core part of our code in the supplementary material. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 21}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The complete experiment details are introduced in the main paper and the appendix. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 21}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We provide the statistical analysis results in the appendix. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We have described the resources required to perform our experiments in the appendix. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 22}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Our work conforms with the NeurIPS Code of Ethics in every respect ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 22}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 22}, {"type": "text", "text": "Answer: [No] ", "page_idx": 22}, {"type": "text", "text": "Justification: Our work mainly focuses on the research of basic neural network architecture.   \nWe have identified no potential negative social impacts. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 22}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 23}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: Our paper poses no such risks. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 23}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The creators or original owners of assets (e.g., code, data, models), used in the paper, are properly credited as well as the license and terms of use explicitly are mentioned and properly respected. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: No new assets introduced. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 24}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: Our work does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 24}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: Our work does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}]