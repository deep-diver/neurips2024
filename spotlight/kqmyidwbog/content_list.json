[{"type": "text", "text": "Advancing Spiking Neural Networks for Sequential Modeling with Central Pattern Generators ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Changze Lv1\u2217 Dongqi $\\mathbf{Han}^{2\\dagger}$ Yansen Wang2\u2020 Xiaoqing Zheng1\u2020 Xuanjing Huang1 Dongsheng $\\mathbf{Li^{2}}$ ", "page_idx": 0}, {"type": "text", "text": "1School of Computer Science, Fudan University 2Microsoft Research Asia {czlv22}@m.fudan.edu.cn, {zhengxq,xjhuang}@fudan.edu.cn, {yansenwang,dongqihan,dongsli}@microsoft.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Spiking neural networks (SNNs) represent a promising approach to developing artificial neural networks that are both energy-efficient and biologically plausible. However, applying SNNs to sequential tasks, such as text classification and time-series forecasting, has been hindered by the challenge of creating an effective and hardware-friendly spike-form positional encoding (PE) strategy. Drawing inspiration from the central pattern generators (CPGs) in the human brain, which produce rhythmic patterned outputs without requiring rhythmic inputs, we propose a novel PE technique for SNNs, termed CPG-PE. We demonstrate that the commonly used sinusoidal PE is mathematically a specific solution to the membrane potential dynamics of a particular CPG. Moreover, extensive experiments across various domains, including time-series forecasting, natural language processing, and image classification, show that SNNs with CPG-PE outperform their conventional counterparts. Additionally, we perform analysis experiments to elucidate the mechanism through which SNNs encode positional information and to explore the function of CPGs in the human brain. This investigation may offer valuable insights into the fundamental principles of neural computation. Our code is available at https://github.com/microsoft/SeqSNN. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Spiking neural network (SNN) [1] has increasingly attracted research interests in recent years, primarily due to its energy efficiency, event-driven paradigm, biological plausibility, and other distinctive properties. The spiking neurons in SNN are dynamical systems that generate binary signals (spike or non-spike) and communicate these signals like artificial neural networks (ANNs) for computation [2\u20139]. Many advanced architectures and methodologies developed for ANNs are also applicable to SNNs, enhancing their capabilities. Notable among these are backpropagation [10], batch normalization [11, 12], and Transformer architecture [4, 13, 5, 6], which collectively broaden the functional scope of SNNs. ", "page_idx": 0}, {"type": "text", "text": "Despite the promising advances in SNNs, several challenges persist when adapting them to diverse tasks. A fundamental challenge is that SNNs, which are event-triggered, lack robust and effective mechanisms to capture indexing information, rhythmic patterns, and periodic data. This limitation can adversely affect SNNs\u2019 ability to process and analyze different data modalities, including natural language, and time series. Meanwhile, while SNNs aim to emulate the neural circuits of the brain, their reliance on spike-based communication imposes limitations. Consequently, not all deep learning techniques applicable to ANNs can be directly transferred to SNNs. For instance, methods like HiPPO [14] or trigonometric positional encoding [15] are not readily compatible with the spike format used in SNNs. Moreover, even the most state-of-the-art ANNs still lag significantly behind human capabilities in many tasks [16, 17]. Therefore, to enhance the functionality of SNNs, one promising approach is to draw further inspiration from biological neural mechanisms. In this regard, we propose the analogy of central pattern generators (CPGs) [18], a kind of neural circuit well-known in neuroscience, with positional encoding (PE), a technique extensively utilized in deep learning. This analogy is designed to operate within the SNN framework, potentially bridging the gap between biologically inspired models and modern deep learning techniques. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In neuroscience, a CPG (See Figure 2 for an illustration) is a group of neurons capable of producing rhythmic patterned outputs without requiring rhythmic inputs [19, 20]. These neural circuits are found in the spinal cord and brainstem and are responsible for generating the rhythmic signals that control vital activities such as locomotion, respiration, and chewing [21]. ", "page_idx": 1}, {"type": "text", "text": "On the other hand, PE is an important technique for ANNs, particularly within models tailored for sequence processing task [15, 22, 23]. By endowing each element of the input sequence with positional information, typically achieved through diverse mathematical formulations or learnable embeddings, neural networks acquire the capability to discern the order and relative positions of the elements within the sequence. ", "page_idx": 1}, {"type": "text", "text": "We argue that these two concepts, despite seemingly unrelated, can be connected profoundly. Intuitively, CPG and PE both generate periodic outputs (with respect to time for CPG and with respective to position for PE). Moreover, in this paper, we reveal a deeper relationship between these two concepts by showing that the widely used sinusoidal PE is mathematically a particular solution of the membrane potential dynamics of a specific CPG. ", "page_idx": 1}, {"type": "text", "text": "However, current SNN architectures exhibit a notable deficiency in implementing an effective and biologically plausible PE mechanism. Existing so-called positional encoding methods for SNNs [4, 5] rely on input data, often resulting in non-spike and repetitive outputs for different positions. Furthermore, incorporating PE techniques designed for ANNs necessitates the calculation of membrane potentials, which is incompatible with the spike format of SNNs. To address these issues, we draw inspiration from the spiking properties of the CPGs and propose a straightforward yet versatile PE technique for SNNs, termed CPG-PE. This method encodes positional information with multiple neurons with various patterns of spike trains. To summarize the highlights of our study: ", "page_idx": 1}, {"type": "text", "text": "\u2022 Novel Positional Encoding for SNNs. We introduce a bio-plausible and effective PE approach tailored specifically for SNNs. This innovative strategy draws inspiration from the central pattern generator found in the human brain. Additionally, we propose a straightforward implementation of CPG-PE in SNNs, which is also compatible with neuromorphic hardware as it can be realized using circuits of leaky integrate-and-fire neurons. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Consistent Performance Gain. Our proposed methods significantly and consistently enhance the performance of SNNs across a wide range of sequential tasks, including timeseries forecasting and text classification. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Insightful Analysis. Our research represents one of the pioneering efforts to comprehensively analyze (1) the mechanism by which SNNs capture positional information and (2) the role of CPGs in the brain. This analysis provides valuable insights into the underlying principles of neural computation. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Spiking Neural Networks ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "The basic unit in SNNs is the spiking neuron, such as the leaky integrate-and-fire (LIF) neuron [1], which operates based on an input current $I(t)$ and influences the membrane potential $U(t)$ and the ", "page_idx": 1}, {"type": "image", "img_path": "kQMyiDWbOG/tmp/978d0ab34a41597fa10657bc32462717fa8be040cf4c10261cd7921ce88c8c90.jpg", "img_caption": [], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Figure 1: (a) Positional encoding (PE) in ANN Transformers. (b) Relative $\\mathrm{PE}\\,^{\\ddag}$ in Spike Transformers [4\u20136]. (c) Our Proposed CPG-PE method. (d) CPG-PE consistently improves learning performance across various tasks. CPG-PE is an ideal PE method tailored for SNNs, detailed in Section 3. ", "page_idx": 2}, {"type": "text", "text": "spike $S(t)$ at time $t$ . The dynamics of the LIF neuron are described by the following equations: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{U(t)=H(t-\\Delta t)+I(t),\\quad I(t)=f(\\mathbf{x};\\theta),}\\\\ &{H(t)=V_{r e s e t}S(t)+\\left(1-S(t)\\right)\\beta U(t),}\\\\ &{S(t)=\\left\\{1,\\quad\\mathrm{if~}U(t)\\geq U_{\\mathrm{thr}}\\right.}\\\\ &{\\left.\\!\\!\\!S(t)=\\left\\{0,\\,\\,\\,\\,\\mathrm{if~}U(t)<U_{\\mathrm{thr}}\\,.\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Here, $I(t)$ is the spatial input to the LIF neuron at time step $t$ , calculated using the function $f$ with $\\mathbf{x}$ as input and $\\theta$ as learnable parameters. $\\Delta t$ is the discretization constant that determines the granularity of LIF modeling, and $H(t)$ is the temporal output of the neuron at time step $t$ . The spike $\\bar{S}(t)$ is defined as a Heaviside step function based on the membrane potential. When $U(t)$ reaches the threshold $U_{\\mathrm{thr}}$ , the neuron fires, emitting a spike, and the temporal output $H(t)$ resets to $V_{r e s e t}$ . If the membrane potential $U(t)$ does not reach the threshold, no spike is emitted, and $U(t)$ decays to $H(t)$ at a decay rate of $\\beta$ . ", "page_idx": 2}, {"type": "text", "text": "In this paper, we choose direct training with surrogate gradients as our method to train SNNs. we follow [24] to choose the arctangent-like surrogate gradients as our error estimation function when backpropagation, which regards the Heaviside step function as: $\\begin{array}{r}{S(t)\\approx\\frac{1}{\\pi}\\arctan(\\frac{\\pi}{2}\\alpha U(t))+\\frac{1}{2}}\\end{array}$ , where $\\alpha$ is a hyper-parameter to control the frequency of the arctangent function. Therefore, the gradients of S are \u2202\u2202US((tt)) \u03b12(1+( \u03c0 \u03b11U(t))2) and thus the overall model can be trained in an end-to-end manner with back-propagation through time (BPTT) [25]. ", "page_idx": 2}, {"type": "text", "text": "2.2 Positional Encoding ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In the field of sequential tasks, PE is crucial for models like Transformers to understand the sequential order of input tokens. Absolute PE and relative PE are two prominent methods used to incorporate positional information into these models. Absolute PE [15] assigns fixed embeddings to each position in the input sequence using trigonometric functions like sine and cosine. These embeddings are based solely on the position index and are not influenced by the token content, which are predefined and are generated as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathrm{PE}_{(p o s,2i)}=\\sin\\left(\\frac{p o s}{10000^{2i/d}}\\right),\\quad\\mathrm{PE}_{(p o s,2i+1)}=\\cos\\left(\\frac{p o s}{10000^{2i/d}}\\right).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Here, pos is the position and $d$ is the dimension. In contrast, relative PE [26\u201328] captures the relationships between tokens by considering their relative distances. This dynamic approach allows models to learn position-specific patterns and dependencies, which is beneficial for tasks requiring different sequence lengths or hierarchical structures. ", "page_idx": 2}, {"type": "text", "text": "However, existing SNN architectures reveal a notable deficiency in the integration of an effective and biologically plausible PE mechanism. As shown in Figure 1, current Transformer-based SNNs [4, 5] are primarily tailored for image classification and predominantly rely on a convolutional layer to capture the relative positional information of image patches. However, this approach resembles more of a spike-element-wise (SEW) residual connection [2] rather than a classic PE module, as it does not ensure that each image patch has a unique spike-form positional representation. Furthermore, the addition between positional spikes and the original input spikes within these models may yield hardware-unfriendly non-binary integers (i.e., neither 0 nor 1), resulting from the addition of \u201c1\u201d and \u201c1\u201d. Additionally, our investigation reveals that even SNNs designed for sequential tasks, such as SpikeBERT [29, 30], SpikeGPT [31], and SpikeTCN [32], also exhibit a notable absence of an effective spike-form PE mechanism for capturing positional information. ", "page_idx": 3}, {"type": "text", "text": "We think that an effective PE strategy should possess the following characteristics: uniqueness of each position and the capacity to capture positional information from the input data. Furthermore, an optimal PE designed for SNNs should be hardware-friendly and in spike-form. ", "page_idx": 3}, {"type": "text", "text": "2.3 Central Pattern Generators ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Central Pattern Generators (CPGs) are neural networks capable of producing rhythmic patterned outputs without sensory feedback [18, 20]. These networks are fundamental for understanding motor control in vertebrates and invertebrates and are often applied to robotics and neural control systems. Mathematically, CPGs can be modeled using systems of coupled nonlinear oscillators, and the general form can be written as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n{\\dot{\\mathbf{x}}}=\\mathbf{F}(\\mathbf{x})+\\mathbf{G}(\\mathbf{x},\\mathbf{y}),\\quad{\\dot{\\mathbf{y}}}=\\mathbf{H}(\\mathbf{y})+\\mathbf{K}(\\mathbf{x},\\mathbf{y}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathbf{x}$ and $\\mathbf{y}$ are the state variables (can be seen as membrane potential) of two coupled oscillators, $\\mathbf{F}$ and $\\mathbf{H}$ are intrinsic dynamics of the oscillators, and $\\mathbf{G}$ and $\\mathbf{K}$ are the coupling functions. ", "page_idx": 3}, {"type": "text", "text": "3 Methods ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In biological systems, CPGs as well as other neurons do not transmit information directly through membrane potential but through spikes. A burst of spikes will be generated only when the membrane potential of a CPG exceeds a certain threshold. Therefore, we introduced the Heaviside step function in SNN, selecting only the part that exceeds the threshold, to design the CPG-PE. In this section, we will first reveal the relationship between CPGs and PE. Then we will introduce our proposed CPG-PE and its implementations. ", "page_idx": 3}, {"type": "text", "text": "3.1 Relationship between Central Pattern Generators and Positional Encoding ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Consider one of the simplest CPGs with the following assumptions: ", "page_idx": 3}, {"type": "text", "text": "1. The CPG is a coupled nonlinear oscillator with 2 neurons whose states are represented as ${\\bf x}(t)$ and ${\\bf y}(t)$ .   \n2. Both neurons are autonomic neurons and will gain membrane voltage with constant speed, i.e., $\\mathbf{F}(\\mathbf{x})=b>0,\\mathbf{H}(\\mathbf{y})=d>0$ .   \n3. Neuron represented by $\\mathbf{x}$ will inhibits y while y excites x. And the influence is proportional to the other neuron\u2019s state. Formally, $\\mathbf{G}(\\mathbf{x},\\mathbf{y})=a\\mathbf{y},\\mathbf{K}(\\mathbf{x},\\mathbf{y})=-c\\mathbf{x}$ where $a>0,c>0$ . ", "page_idx": 3}, {"type": "text", "text": "Now the coupled oscillators can be represented as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n{\\dot{\\mathbf{x}}}(t)=a\\mathbf{y}(t)+b,\\quad{\\dot{\\mathbf{y}}}(t)=-c\\mathbf{x}(t)+d.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The general solution of this differential equation system is: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathbf{x}(t)=k_{1}\\cos(\\sqrt{a c}\\,t)+k_{2}\\sqrt{\\frac{a}{c}}\\sin(\\sqrt{a c}\\,t)+\\frac{d}{c},}\\\\ {\\mathbf{y}(t)=-k_{1}\\sqrt{\\frac{c}{a}}\\sin(\\sqrt{a c}\\,t)+k_{2}\\cos(\\sqrt{a c}\\,t)-\\frac{b}{a},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $k_{1}$ and $k_{2}$ are arbitrary constants. To simplify, we can further re-parameterize $t$ with $t^{\\prime}=$ $t+\\arctan(k_{1}/a k_{2})$ as is to choose another start point, then we can rewrite Equations (7) and (8) as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{c c c}{\\displaystyle\\mathbf{x}(t^{\\prime})=\\sqrt{k_{1}^{2}+\\frac{a}{c}k_{2}^{2}}\\sin(\\sqrt{a c}\\,t^{\\prime})+\\frac{d}{c}=A_{1}\\sin(w_{1}t^{\\prime})+b_{1},}&\\\\ {\\displaystyle\\mathbf{y}(t^{\\prime})=\\sqrt{\\frac{c}{a}k_{1}^{2}+k_{2}^{2}}\\cos(\\sqrt{a c}\\,t^{\\prime})-\\frac{b}{a}=A_{2}\\cos(w_{2}t^{\\prime})+b_{2}.}&\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Comparing Equations (9) and (10) and Equation (4), we are astonished to find that the PE in Transformers [15] is a particular solution of the membrane potential variations in a specific type of CPG with properly chosen $a,b,c,d.$ . This finding suggests that the use of sinusoidal PE in Transformers is actually a bio-plausible choice that could possibly advance the model\u2019s ability to learn indexing and periodic information. ", "page_idx": 4}, {"type": "image", "img_path": "kQMyiDWbOG/tmp/47114c708ec098c2f1e3ab36b2b5e9d01b441830636f45b746d0ed44496a6957.jpg", "img_caption": ["Figure 2: (a) Illustration of a pair of CPG neurons demonstrating mutual inhibition through spiking activity. The spikes represent neural spikes that inhibit each other, exemplifying the coordination mechanism in CPG networks. (b) Spike trains of the first 4 CPG neurons. The curve represents the membrane potential, while the vertical lines represent spikes. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "3.2 CPG-based Positional Encoding ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Consider a system with $N$ pairs of CPG neurons, resulting in a total of $2N$ cells. Then for $i=$ $1,2,...,N$ , the equations governing the CPG-PE are as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{CPG-PE}^{2i-1}(t)=H\\left(\\cos\\left(\\eta\\frac{t}{\\tau^{\\frac{i}{N}}}\\right)-v^{\\mathrm{thres}}\\right),}\\\\ {\\mathrm{CPG-PE}^{2i}(t)=H\\left(\\sin\\left(\\eta\\frac{t}{\\tau^{\\frac{i}{N}}}\\right)-v^{\\mathrm{thres}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\eta$ is a constant to control the period, $\\tau$ represents the base period, and $v^{\\mathrm{thres}}$ denotes the membrane potential threshold. Note that this threshold is different from the $U_{t h r}$ of spike neurons described in Equation (3). The Heaviside step function $H$ reflects a spike when the membrane potential exceeds the threshold. ", "page_idx": 4}, {"type": "text", "text": "It is important to clarify that the $t$ in Equation (11) and 12 is neither the time step in SNNs nor the position index. Suppose the input spike matrix $X\\in\\{0,1\\}^{T\\times B\\times L\\times D}$ , where $T$ is the time step in SNNs, $B$ is the batch size, $L$ is the sequence length of the input sample, $D$ is the feature dimension. To ensure the uniqueness of each position at every time step, we flatten the dimensions $T$ and $L$ into a new dimension $T\\times L$ . Therefore, $t$ ranges from 0 to $T\\times L$ . Notably, the entire CPG-PE operates in spike-form and is parameter-free. To better understand CPG-PE, we draw a simple approximation of the resulting CPG spiking patterns under the assumption of a sequence length of $L=128$ and $N=20$ pairs of CPG neurons, illustrated in Figure 2 (b). ", "page_idx": 4}, {"type": "text", "text": "3.3 Implementations ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We design a simple implementation to apply CPG-PE to SNNs in a pluggable and hardware-friendly manner, shown in Figure 3. Before diving into the details, we want to emphasize that the data transmitted in SNNs should always be in spike-form. Therefore, the direct addition operation between two spike matrices, as used in [4, 5], should be forbidden. ", "page_idx": 4}, {"type": "image", "img_path": "kQMyiDWbOG/tmp/341225f01b8138fbf99ace7e142979c90c8de15cc2c618a8245021aff1ba0d15.jpg", "img_caption": ["Figure 3: Illustration of applying CPG-PE to SNNs. $X$ , $X^{\\prime}$ , and $X_{o u t p u t}$ are all spike matrices. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Initially, CPG-PE encodes the positional information of the input spike matrix $X$ , resulting in $X^{\\prime}$ . Then, to maintain binary values and avoid introducing non-binary elements, we opt to concatenate $X$ and $X^{\\prime}$ along the feature dimension. Lastly, a linear layer is employed to map the feature dimension from $D+2N$ back to $D$ , where $D$ is the feature dimension of $X$ , and $N$ is the number of CPG pairs. This effectively neutralizes the dimensional increase caused by concatenation. The whole process can be formalized as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{X^{\\prime}=\\mathrm{CPG}\\mathrm{-}\\mathrm{PE}(X),}&&{X\\in\\{0,1\\}^{T\\times B\\times L\\times D},X^{\\prime}\\in\\{0,1\\}^{T\\times B\\times L\\times2N}}\\\\ &{X_{1}=X\\oplus X^{\\prime},}&&{X_{1}\\in\\{0,1\\}^{T\\times B\\times L\\times(D+2N)}}\\\\ &{X_{o u t p u t}=\\mathcal{S}\\mathcal{N}\\left(\\mathrm{BN}\\left(\\mathrm{Linear}\\left(X_{1}\\right)\\right)\\right),}&&{X_{o u t p u t}\\in\\{0,1\\}^{T\\times B\\times L\\times D}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where BN represents batch normalization and $\\mathit{S N}$ is a spike neuron layer. Furthermore, CPG-PE necessitates that input samples be sequential data, making it directly applicable to time series data and natural language. For image data, however, an adaptation is required: images must be segmented into patches similar to the approach used in the Vision Transformer [23]. Considering the compatibility with neuromorphic hardware, we also (1) implement CPG-PE with LIF neurons, and (2) integrate CPG-PE into a classic linear layer. Please refer to Appendices C and D for details. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we conduct experiments to investigate the following research questions: RQ1: Is our design of CPG-PE strategy effective and robust in sequential tasks? ", "page_idx": 5}, {"type": "text", "text": "RQ2: Can CPG-PE work well on image patches that have no inherent order? RQ3: How will CPG\u2019s inner properties influence CPG-PE? RQ4: Does our CPG-PE satisfy the requirements of a good PE tailored for SNNs? ", "page_idx": 5}, {"type": "text", "text": "4.1 Datasets ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To assess the PE capabilities of the compared models and answer RQ1, we conduct two sequential tasks: time-series forecasting, and text classification. Following [32], we choose 4 real-world datasets for time-series forecasting: Metr-la [33]: This dataset contains the average traffic speed data collected from the highways in Los Angeles County. Pems-bay [33]: It consists of average traffic speed data from the Bay Area. Electricity [34]: This dataset captures hourly electricity consumption measured in kilowatt-hours $(\\mathrm{kWh})$ . Solar [34]: It includes data on solar power production. For text classification, we follow [29] to conduct experiments on 6 benchmarks including: Movie Reviews [35], SST-2 [36], SST-5, Subj, ChnSenti, and Waimai. In addition, to answer RQ2, we also conduct image classification experiments on 1 static datasets CIFAR and 1 neuromorphic datasets CIFAR10- DVS [37]. The dataset details and metrics are provided in Appendix A. ", "page_idx": 5}, {"type": "text", "text": "4.2 Time-Series Forecasting ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "As discussed in Section 3.3, our proposed CPG-PE can be seamlessly integrated into any SNN capable of sequence processing. Consequently, we applied CPG-PE to the SNN counterparts of Temporal Convolutional Networks (TCN) [38], Recurrent Neural Networks (RNN) to assess the efficacy of our method in enabling SNNs to capture positional information. The results for TCN, SpikeTCN w/o PE, RNN, and Spike-RNN w/o PE are sourced from the previous study by [32]. In addition, we deliberately conducted experiments on PE in Spikformer to explore whether our specially designed CPG-PE is truly more suitable for SNNs than all previous PEs. Notably, we also investigated the modularization of CPG, i.e., replacing all Linear layers with CPG-Linear layers (See Appendix D), and its impact on the Spikformer model for time-series forecasting, i.e., Spikformer w/ CPG-Full. We report the results on 4 time-series forecasting benchmarks with various prediction lengths in Table 1. We also list results from ANNs for reference. ", "page_idx": 5}, {"type": "table", "img_path": "kQMyiDWbOG/tmp/63bdd2a2f13a615a8ef895ce4c2c1527a2496e2003c0ba016310ea3c9ec34dff.jpg", "table_caption": ["Table 1: Experimental results of time-series forecasting on 4 benchmarks with various prediction lengths 6, 24, 48, 96. \u201cPE\u201d stands for positional encoding. \u201cw/o\u201d denotes \u201cwithout\u201d while \u201cw/\u201d denotes \u201cwith\u201d. The best results of SNNs are formatted in bold font format. $\\uparrow(\\downarrow)$ indicates the higher (lower) the better. Shaded ones are ours. All results are averaged across 3 random seeds. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "In summary, the results presented in Table 1 indicate that SNNs equipped with the CPG-PE module significantly outperform their counterparts lacking the PE feature. This finding effectively addresses RQ1 from a time-series analysis perspective. Detailed findings include: ", "page_idx": 6}, {"type": "text", "text": "(1) CPG-PE enables SNNs to successfully capture positional information. SNNs, including models such as Spike-TCN, Spike-RNN, and Spikformer, when integrated with CPG-PE, show superior performance compared to those without PE. Notably, CPG-PE also reduces the performance disparity between SNNs and traditional ANNs in time-series forecasting tasks, evidenced by an average increase of 0.013 in $\\mathbf{R}^{2}$ and a decrease of 0.022 in RSE. ", "page_idx": 6}, {"type": "text", "text": "(2) CPG-PE is the most suitable position encoding strategy for Spikformer. In addition to CPG-PE, other encoding strategies such as Float-PE (the original PE in Transformer) and RPE (the original PE in Spikformer) were also evaluated. The Spikformer equipped with CPG-PE emerged as the top-performing variant, confirming CPG-PE as the most suitable PE strategy for SNNs. ", "page_idx": 6}, {"type": "text", "text": "(3) CPG-Full module can also effectively model the positional information of time series data. The CPG-Full module\u2019s performance in modeling positional information of time-series data is comparable to that of CPG-PE, with average $\\mathrm{R^{2}}$ values nearly identical to those of Spikformer with CPG-PE and significantly better than those of other models. ", "page_idx": 6}, {"type": "text", "text": "4.3 Text Classification ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In addition to time-series forecasting, natural language processing (NLP) serves as another critical domain to assess the efficacy of the CPG-PE module in encoding positional information. Following the pioneering work of [29], who first employed Spikformer for text classification tasks, we extended this application to 6 benchmark datasets. We also include results from fine-tuned BERT for reference. ", "page_idx": 6}, {"type": "table", "img_path": "kQMyiDWbOG/tmp/c7c59ae85343c062633f2ed8c7917505ddcbe1021a66ffd90f54eff68e1b4b9a.jpg", "table_caption": ["Table 2: Accuracy on 6 text classification benchmarks. The best results of SNNs and ANNs are formatted in bold font format. Experimental results are averaged across 5 random seeds. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "The results presented in Table 2 shows that Spikformer enhanced with CPG-PE achieves the state-ofthe-art performance across 6 benchmarks, effectively addressing RQ1. Meanwhile, we conducted a set of ablation experiments to eliminate the effects of increased parameter counts on model performance. Specifically, we replaced the spike-form positional encoding matrix obtained from CPG with a randomly generated spike matrix (See \u201cSpikformer w/ Random PE\u201d Row). By comparing these two configurations, we confirmed the effectiveness of our proposed CPG-PE. ", "page_idx": 7}, {"type": "text", "text": "4.4 Image Classification ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we aim to answer RQ2. To adapt the CPG-PE for image classification, it is essential to conceptualize the array of image patches as sequential data. Consequently, some SNN models that do not incorporate a concept of \u201csequence length\u201d in their spike matrices, such as SEW-Resnet [2], are incompatible with the integration of a CPG-PE module. Therefore, we only consider ViT-liked SNN, i.e. Spikformer, in this experiment. We also include results from ViTs for reference. ", "page_idx": 7}, {"type": "table", "img_path": "kQMyiDWbOG/tmp/f1a93663ed484f545d2fe6ec8fb6158d671337dc26714e8e26baaf5d86e0bc9e.jpg", "table_caption": ["Table 3: Evaluation on image classification benchmarks. Float-PE denotes the original PE of the Transformer, while RPE denotes the original PE of the Spikformer. Numbers with \u2217denote our implementation. The best results of SNNs and ANNs are formatted in bold font format. All results are averaged across 4 random seeds. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "We report the parameter counts and classification accuracy in Table 5. To elaborate, Spikformer with CPG-PE outperforms other variants, demonstrating the effectiveness of CPG-PE even when the sequence is an array of image patches lacking inherent order. Notably, owing to our streamlined implementation, the parameter count for Spikformer with CPG-PE is significantly reduced compared to the original Spikformer w/ RPE [4], with a reduction of $1.16~\\mathrm{M}$ . What\u2019s more, we conducted ablation experiments on model parameters by reducing the parameter count of Spikformer with CPG-PE to be comparable to Spikformer w/o PE, allowing for a more direct performance comparison, as shown in the last line in Table 5. The results on ImageNet are reported in Appendix E. ", "page_idx": 7}, {"type": "text", "text": "However, it is essential to acknowledge that the improvements in image classification are relatively modest compared to those observed in time series and text applications. This phenomenon can largely be attributed to the intrinsic non-ordered nature of image patches. Unlike text or time series data, where sequential order is crucial and inherently informative, image patches do not possess a natural or fixed sequence. This lack of order means that traditional methods of positional encoding, which significantly benefit ordered data by providing contextual positioning, are less effective. Thus, the application of our positional encoding techniques, optimized for data with inherent sequential order, does not translate as effectively to the domain of image classification. ", "page_idx": 7}, {"type": "text", "text": "4.5 Sweeping CPG properties ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we investigate the influence of CPG properties on the ability to model positional information, addressing RQ3. To this end, we evaluated the Spikformer model with CPG-PE by varying the base period $\\tau$ and the number of CPG pairs $N$ (see Equations (11) and (12)) in time-series forecasting and image classification tasks. ", "page_idx": 7}, {"type": "text", "text": "From Figure 4 (a) and (b), we observe that CPG-PE is insensitive to the base period $\\tau$ (in biological neurons, $\\tau$ is affected by the physiological properties of the CPG circuit such as RC constant and synaptic delay). The sequence lengths $(T\\times L)$ of the time series and image patches are no larger than 672 $\\mathit{4}\\times168)$ for all benchmarks, preventing repetitions in CPGs. Therefore, when $N=20$ , sweeping $\\tau\\,\\in\\,\\{100,1000,5000,10000\\}$ makes minor influence on performance. Furthermore, Figure 4 (c) and (d) demonstrate that when $\\tau\\,=\\,10000$ , increasing the number of CPG pairs $N$ enhances Spikformer\u2019s performance. This is reasonable because more CPG neurons reduce repetitions in positional representations of $X^{\\prime}$ . ", "page_idx": 7}, {"type": "image", "img_path": "kQMyiDWbOG/tmp/b6432d25ef7ac57b26f9da940c8fd591b8a239d953519e3461e78a2fd8843035.jpg", "img_caption": ["Figure 4: (a)(c) $R^{2}$ versus $\\tau$ and $N$ on time-series forecasting tasks. (b)(d) Accuracy versus $\\tau$ and $N$ on image classification tasks. $\\tau\\in\\{100,1000,5000,10000\\}$ , $N\\in\\{5,10,20\\}$ . "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.6 Positional Encoding Analysis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we want to address RQ4. As mentioned in Section 2.2, an ideal PE method for SNNs should include the following characteristics: (1) Uniqueness of each position; (2) Compatibility with neuromorphic hardware; (3) Formulation in spike-form. Our implementations ensure compatibility with neuromorphic hardware (2), and the CPG-PE is inherently formulated in spikeform, satisfying (3). Therefore, in order to assess (1), the uniqueness of each position, we would like to compare the CNN-based RPE in [4, 5] and our proposed CPG-PE, focusing specifically on their capacity to provide distinct positional signals. This analysis was conducted using the CIFAR10-DVS dataset, where we calculated the repetition rate of spike positional representations across all positions. Our findings were notable: the positional spike matrices produced by RPE exhibited a repetition rate as high as $\\mathbf{\\dot{12.19\\%}}$ , which significantly undermines its effectiveness for PE. In contrast, our proposed CPG-PE exhibited no repetition, demonstrating that our CPG-PE is well-suited for serving as the PE module in SNNs. Please refer to Appendix B for details. ", "page_idx": 8}, {"type": "text", "text": "5 Related Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "5.1 Spike Encoding Methods ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Spiking neural networks employ several coding methods to encode input information, each offering unique advantages. Direct coding [5, 40], the simplest form and widely-used in image tasks, directly associates spikes with specific values or events, providing straightforward and interpretable outputs but often lacking efficiency for complex tasks. Rate coding [8, 41], where the input is represented by the frequency of spikes within a given timeframe, is more robust and widely used but can be less precise due to its reliance on averaged spike rates. Temporal coding (a.k.a latency coding) [42, 43] encodes information based on the timing of individual spikes, allowing for high temporal precision and efficient representation of dynamic inputs, though it can be computationally demanding. In addition, delta coding [44] represents changes in input signals through spikes, focusing on differences rather than absolute values, which can enhance efficiency and response times but may introduce complexity in decoding. Each of these methods contributes to the versatility and applicability of SNNs in various domains, from neuroscience to artificial intelligence. The SNNs we considered in this paper should fall into the category of rate coding since back-prop is conducted on spike rate. Meanwhile, CPG-PE can be considered converting temporal information into spike rate of a group of neurons (Equations 11 and 12), and this is why CPG-PE can improve performance for sequential data. It is possible to introducing learning algorithms of temporal coding for the CPG neurons to tackle more complex sequence structure, which remains as future work. ", "page_idx": 8}, {"type": "text", "text": "5.2 Positional Encoding in SNNs ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Currently, few works have demonstrated the importance of PE approaches in SNNs. Spikformer [4] and Spike-driven Transformer [5] utilize a combination of \u201cone convolutional layer $^+$ one batch normalization layer $^+$ one spiking neuron layer \u201d to generate learnable \u201crelative positional encoding\u201d. From our perspective, this strategy is more like a spike-element-wise residual connection [2], rather than a classic PE module. The unique representation of each position is a fundamental requirement for a robust PE module. However, the spike position matrix generated by their method may result in the same spike representation for different positions. Additionally, the addition of the input spike matrix and the position spike matrix will result in the occurrence of non-binary numbers (i.e., 2) due to the addition of 1 and 1. For spiking graph neural networks, [45] proposed learnable positional graph spikes, aiming to capture neighbor information within graphs rather than sequences. Therefore, drawing inspiration from the periodic automatic spike generation pattern of CPGs, we propose a biologically plausible and effective spike-form absolute position encoding method called CPG-PE. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "6 Rethinking the Role of CPGs in Neuroscience ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Our study also provides novel insights into neuroscience on understanding the role of CPG in nervous systems. While traditionally CPG is believed to play a crucial role in producing the rhythmic motor patterns necessary for locomotion and other repetitive movements [18, 20], the analogy to PE in this work reveals that CPG can make a significant contribution in processing sequential data by encoding the positional information into unique spiking patterns at different times. This does not only work for time-series sensory input like auditory signals but also for visual sensory data: e.g. when a person looks at an image, saccades (eye movements) allow retinal neurons to receive different parts of the image at different times. This indicates that CPG neurons could potentially be utilized to encode positional information. Another extensive thought is that as PE can be learnable in ANNs, CPG may also benefit from adaptability to the data [46]. The hypothesis, however, remains to be examined through neuroscientific experiments [47]. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In conclusion, inspired by central pattern generators, we introduce a pioneering position encoding approach termed CPG-PE, specifically tailored to mitigate the constraints associated with current PE techniques within SNNs. We mathematically prove that abstract PE in the Transformer is a particular solution of the membrane potential variations in a specific type of CPG. Furthermore, through comprehensive empirical investigations across diverse domains including time-series forecasting, natural language processing, and image classification, we demonstrate that the CPG-PE satisfies all the requirements of PE tailored for SNNs. The limitations and future work are discussed in Appendix F. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Wofgang Maass. Networks of spiking neurons: the third generation of neural network models. Neural Networks, 14:1659\u20131671, 1997.   \n[2] Wei Fang, Zhaofei Yu, Yanqing Chen, Tiejun Huang, Timoth\u00e9e Masquelier, and Yonghong Tian. Deep residual learning in spiking neural networks. In Neural Information Processing Systems, 2021.   \n[3] Jianhao Ding, Zhaofei Yu, Yonghong Tian, and Tiejun Huang. Optimal ANN-SNN conversion for fast and accurate inference in deep spiking neural networks. In Zhi-Hua Zhou, editor, Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI 2021, pages 2328\u20132336, 2021.   \n[4] Zhaokun Zhou, Yuesheng Zhu, Chao He, Yaowei Wang, Shuicheng Yan, Yonghong Tian, and Li Yuan. Spikformer: When spiking neural network meets transformer. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023, 2023.   \n[5] Man Yao, JiaKui Hu, Zhaokun Zhou, Li Yuan, Yonghong Tian, XU Bo, and Guoqi Li. Spikedriven transformer. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[6] Man Yao, JiaKui Hu, Tianxiang Hu, Yifan Xu, Zhaokun Zhou, Yonghong Tian, Bo XU, and Guoqi Li. Spike-driven transformer v2: Meta spiking neural network architecture inspiring the design of next-generation neuromorphic chips. In The Twelfth International Conference on Learning Representations, 2024.   \n[7] Wei Fang, Zhaofei Yu, Yanqing Chen, Timoth\u00e9e Masquelier, Tiejun Huang, and Yonghong Tian. Incorporating learnable membrane time constant to enhance learning of spiking neural networks. 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 2641\u20132651, 2020.   \n[8] Changze Lv, Jianhan Xu, and Xiaoqing Zheng. Spiking convolutional neural networks for text classification. In The Eleventh International Conference on Learning Representations (ICLR), 2023.   \n[9] Yuhang Li, Tamar Geller, Youngeun Kim, and Priyadarshini Panda. Seenn: Towards temporal spiking early exit neural networks. Advances in Neural Information Processing Systems, 36, 2024.   \n[10] Yujie Wu, Lei Deng, Guoqi Li, and Luping Shi. Spatio-temporal backpropagation for training high-performance spiking neural networks. Frontiers in neuroscience, 12:323875, 2018.   \n[11] Hanle Zheng, Yujie Wu, Lei Deng, Yifan Hu, and Guoqi Li. Going deeper with directly-trained larger spiking neural networks. In Proceedings of the AAAI conference on artificial intelligence, volume 35, pages 11062\u201311070, 2021.   \n[12] Chaoteng Duan, Jianhao Ding, Shiyan Chen, Zhaofei Yu, and Tiejun Huang. Temporal effective batch normalization in spiking neural networks. Advances in Neural Information Processing Systems, 35:34377\u201334390, 2022.   \n[13] Chenlin Zhou, Liutao Yu, Zhaokun Zhou, Han Zhang, Zhengyu Ma, Huihui Zhou, and Yonghong Tian. Spikingformer: Spike-driven residual learning for transformer-based spiking neural network. arXiv preprint arXiv:2304.11954, 2023.   \n[14] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Hippo: Recurrent memory with optimal polynomial projections. Advances in neural information processing systems, 33:1474\u20131487, 2020.   \n[15] Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. ArXiv, abs/1706.03762, 2017.   \n[16] Anthony M Zador. A critique of pure learning and what artificial neural networks can learn from animal brains. Nature communications, 10(1):3770, 2019.   \n[17] Melanie Mitchell. Why ai is harder than we think. In Proceedings of the Genetic and Evolutionary Computation Conference, pages 3\u20133, 2021.   \n[18] Eve Marder and Dirk Bucher. Central pattern generators and the control of rhythmic movements. Current biology, 11(23):R986\u2013R996, 2001.   \n[19] Eve Marder and Ronald L Calabrese. Principles of rhythmic motor pattern generation. Physiological reviews, 76(3):687\u2013717, 1996.   \n[20] Sten Grillner. Biological pattern generation: the cellular and computational logic of networks in motion. Neuron, 52(5):751\u2013766, 2006.   \n[21] Ole Kiehn. Decoding the organization of spinal circuits that control locomotion. Nature Reviews Neuroscience, 17(4):224\u2013238, 2016.   \n[22] Yingfei Liu, Tiancai Wang, Xiangyu Zhang, and Jian Sun. Petr: Position embedding transformation for multi-view 3d object detection. In European Conference on Computer Vision, pages 531\u2013548. Springer, 2022.   \n[23] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021.   \n[24] Wei Fang, Yanqi Chen, Jianhao Ding, Ding Chen, Zhaofei Yu, Huihui Zhou, Yonghong Tian, and other contributors. Spikingjelly, 2020.   \n[25] Paul J. Werbos. Backpropagation through time: What it does and how to do it. Proc. IEEE, 78:1550\u20131560, 1990.   \n[26] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 464\u2013468. Association for Computational Linguistics, 2018.   \n[27] Vighnesh Shiv and Chris Quirk. Novel positional encodings to enable tree-based transformers. Advances in neural information processing systems, 32, 2019.   \n[28] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research, 21(140):1\u201367, 2020.   \n[29] Changze Lv, Tianlong Li, Jianhan Xu, Chenxi Gu, Zixuan Ling, Cenyuan Zhang, Xiaoqing Zheng, and Xuanjing Huang. Spikebert: A language spikformer learned from bert with knowledge distillation. 2023.   \n[30] Malyaban Bal and Abhronil Sengupta. Spikingbert: Distilling bert to train spiking language models using implicit differentiation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 10998\u201311006, 2024.   \n[31] Rui-Jie Zhu, Qihang Zhao, and Jason K Eshraghian. Spikegpt: Generative pre-trained language model with spiking neural networks. arXiv preprint arXiv:2302.13939, 2023.   \n[32] Changze Lv, Yansen Wang, Dongqi Han, Xiaoqing Zheng, Xuanjing Huang, and Dongsheng Li. Efficient and effective time-series forecasting with spiking neural networks. In Forty-first International Conference on Machine Learning (ICML), 2024.   \n[33] Yaguang Li, Rose Yu, Cyrus Shahabi, and Yan Liu. Diffusion convolutional recurrent neural network: Data-driven traffic forecasting. arXiv preprint arXiv:1707.01926, 2017.   \n[34] Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. Modeling long-and short-term temporal patterns with deep neural networks. In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, pages 95\u2013104, 2018.   \n[35] Bo Pang and Lillian Lee. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In ACL, 2005.   \n[36] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, A. $\\mathrm{Ng}$ , and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In EMNLP, 2013.   \n[37] Hongmin Li, Hanchao Liu, Xiangyang Ji, Guoqi Li, and Luping Shi. Cifar10-dvs: An eventstream dataset for object classification. Frontiers in Neuroscience, 11, 2017.   \n[38] Shaojie Bai, J Zico Kolter, and Vladlen Koltun. An empirical evaluation of generic convolutional and recurrent networks for sequence modeling. arXiv preprint arXiv:1803.01271, 2018.   \n[39] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In North American Chapter of the Association for Computational Linguistics, 2019.   \n[40] Zhaokun Zhou, Kaiwei Che, Wei Fang, Keyu Tian, Yuesheng Zhu, Shuicheng Yan, Yonghong Tian, and Li Yuan. Spikformer v2: Join the high accuracy club on imagenet with an snn ticket. arXiv preprint arXiv:2401.02020, 2024.   \n[41] Youngeun Kim, Hyoungseob Park, Abhishek Moitra, Abhiroop Bhattacharjee, Yeshwanth Venkatesha, and Priyadarshini Panda. Rate coding or direct coding: Which one is better for accurate, robust, and energy-efficient spiking neural networks? In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 71\u201375. IEEE, 2022.   \n[42] Bing Han and Kaushik Roy. Deep spiking neural network: Energy efficiency through time based coding. In European conference on computer vision, pages 388\u2013404. Springer, 2020.   \n[43] IM Comsa, K Potempa, L Versari, T Fischbacher, A Gesmundo, and J Alakuijala. Temporal coding in spiking neural networks with alpha synaptic function: Learning with backpropagation. IEEE Transactions on Neural Networks and Learning Systems, 33(10):5939\u20135952, 2022.   \n[44] Young C Yoon. Lif and simplified srm neurons encode signals into spikes via a form of asynchronous pulse sigma\u2013delta modulation. IEEE transactions on neural networks and learning systems, 28(5):1192\u20131205, 2016.   \n[45] Han Zhao, Xu Yang, Cheng Deng, and Junchi Yan. Dynamic reactive spiking graph neural network. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 16970\u201316978, 2024.   \n[46] Rafael Yuste, Jason N MacLean, Jeffrey Smith, and Anders Lansner. The cortex as a central pattern generator. Nature Reviews Neuroscience, 6(6):477\u2013483, 2005.   \n[47] Adam H Marblestone, Greg Wayne, and Konrad P Kording. Toward an integration of deep learning and neuroscience. Frontiers in computational neuroscience, 10:94, 2016.   \n[48] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.   \n[49] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2018. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The authors would like to thank the anonymous reviewers for their valuable comments. This work was supported partially by National Natural Science Foundation of China (No. 62076068). ", "page_idx": 13}, {"type": "text", "text": "Broader Impact ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "This work aims to advance the field of spiking neural networks (SNNs). Unlike artificial neural networks (ANNs) which have been applied widely in people\u2019s lives, SNNs are still undergoing fundamental research. We do not see any negative societal impacts of this work. ", "page_idx": 13}, {"type": "text", "text": "Reproducibility Statement ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The authors have diligently worked to ensure the reproducibility of the empirical results presented in this paper. The datasets, experimental setups, evaluation metrics, and hyperparameters are thoroughly described in Appendices A and B. Furthermore, the source code for the proposed PE method has been available at https://github.com/microsoft/SeqSNN. ", "page_idx": 13}, {"type": "text", "text": "A Datasets ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Time-series Forecasting ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Detailed statistical characteristics and distribution ratios for each dataset are provided in the following: ", "page_idx": 13}, {"type": "table", "img_path": "kQMyiDWbOG/tmp/76b9bb552dc727defa7fe42e9604e85a3d5a1604bfb851a5542542c3602cb06c.jpg", "table_caption": ["Table 4: The statistics of time-series datasets. "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "A.2 Text Classification ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Here are the datasets we used in text classification experiments: ", "page_idx": 13}, {"type": "text", "text": "\u2022 MR: MR, which stands for Movie Review, is a dataset containing movie-review documents labeled based on their overall sentiment polarity (positive or negative) or subjective rating [35].   \n\u2022 SST-5: SST-5 includes 11, 855 sentences from movie reviews for sentiment classification across 5 categories: very negative, negative, neutral, positive, and very positive [36].   \n\u2022 SST-2: SST-2 is the binary version of SST-5, containing only 2 classes: positive and negative.   \n\u2022 Subj: The Subj dataset is designed to classify sentences as either subjective or objective\\*.   \n\u2022 ChnSenti: ChnSenti consists of approximately 7, 000 Chinese hotel reviews, each annotated with a positive or negative label\u2020.   \n\u2022 Waimai: This dataset contains around 12, 000 Chinese user reviews from a food delivery platform, intended for binary sentiment classification (positive and negative)\u2021. ", "page_idx": 13}, {"type": "text", "text": "A.3 Image Classification ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Here are the datasets we used in image classification experiments: CIFAR dataset comprises a collection of 60, 000 images, partitioned into 50, 000 training and 10, 000 testing images, each with a resolution of $32\\times32$ pixels. The CIFAR10-DVS dataset represents a neuromorphic adaptation of this original set, where static images have been transformed to accommodate the recording capabilities of a Dynamic Vision Sensor (DVS) camera. This conversion results in a dataset consisting of $9,000$ training samples and 1, 000 test samples with $128\\times128$ resolution. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "B Experiment Settings ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Time-series Forecasting ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Metrices The metrics we used in time-series forecasting are the coefficient of determination $(\\mathbf{R}^{2})$ and the Root Relative Squared Error (RSE). ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\boldsymbol{R}^{2}=\\frac{1}{M C L}\\displaystyle\\sum_{m=1}^{M}\\sum_{c=1}^{C}\\sum_{l=1}^{L}\\left[1-\\frac{\\big(Y_{c,l}^{m}-\\hat{Y}_{c,l}^{m}\\big)^{2}}{\\big(Y_{c,l}^{m}-\\bar{Y}_{c,l}\\big)^{2}}\\right],}\\\\ &{\\mathrm{RSE}=\\sqrt{\\displaystyle\\frac{\\sum_{m=1}^{M}||\\mathbf{Y}^{m}-\\hat{\\mathbf{Y}}^{m}||^{2}}{\\sum_{m=1}^{M}||\\mathbf{Y}^{m}-\\bar{\\mathbf{Y}}||^{2}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "In these formulas, $M$ symbolizes the size of the test sets, $C$ denotes the number of channels, and $L$ signifies the length of predictions. $\\bar{\\mathbf Y}$ represents the average of $\\mathbf{Y}^{m}$ . The term $Y_{c,l}^{m}$ refers to the $l$ -th future value of the $c$ -th variable for the $m$ -th sample, while $\\bar{Y}_{c,l}$ indicates the mean of $Y_{c,l}^{m}$ across all samples. The symbols $\\hat{\\mathbf{Y}}^{m}$ and $\\hat{Y}_{c,l}^{m}$ are used to represent the ground truth values. Compared to Mean Squared Error (MSE) or Mean Absolute Error (MAE), these metrics offer greater resilience against the absolute values of the datasets, making them particularly useful in the time-series forecasting setting. ", "page_idx": 14}, {"type": "text", "text": "Model Architecture All SNNs take 4 time steps. For SpikeTCNs and SpikeRNNs, we follow the same settings as [32]. We construct all Spikformer as 2 blocks, setting the feature dimension as 256, and the hidden feature dimension in FFN as 1024. For CPG-PE settings, we set $\\tau=10000.0$ , $N=20$ , $\\eta=1$ , and $v^{\\mathrm{thres}}=0.8$ . ", "page_idx": 14}, {"type": "text", "text": "Training Hyper-parameters we set the training batch size as 64 and adopt Adam [48] optimizer with a cosine scheduler of learning rate $1\\times10^{-\\overline{{4}}}$ . An early stopping strategy with a tolerance of 30 epochs is adopted. We conducted time-series forecasting experiments on 24G-V100 GPUs. On average, a single experiment takes about 1 hour under the settings above. ", "page_idx": 14}, {"type": "text", "text": "B.2 Text Classification ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Model Achirecture All models are with 12 encoder blocks and 768 feature embedding dimension. It is important to note that the original implementation of [29] incorporates a layer normalization module that poses challenges to hardware compatibility. To address this, we have substituted layer normalization with batch normalization in our directly-trained Spikformer models for text classification tasks. For CPG-PE settings, we set $\\tau=10000.0$ , $N=20$ , $\\eta=1$ , and $v^{\\mathrm{thres}}=0.8$ . ", "page_idx": 14}, {"type": "text", "text": "Training Hyper-parameters We directly trained Spikformers with arctangent surrogate gradients on all datasets. We use the BERT-Tokenizer in Huggingface\u00a7 to tokenize the sentences to token sequences. We pad all samples to the same sequence length of 256. We conducted text classification experiments on 4 RTX-3090 GPUs, and set the batch size as 32, optimizer as AdamW [49] with weight decay of $5\\times10^{-3}$ , and set a cosine scheduler of starting learning rate of $5\\times10^{-4}$ . What\u2019s more, in order to speed up the training stage, we adopt the automatic mixed precision training strategy. On average, a single experiment takes about 1.5 hours under the settings above. ", "page_idx": 14}, {"type": "text", "text": "B.3 Image Classification ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Model Architecture For all Spikformer models, we standardized the configuration to include 4 time steps. Specifically, for the CIFAR10 and CIFAR100 datasets, the models were uniformized with 4 encoder blocks and a feature embedding dimension of 384. For the CIFAR10-DVS dataset, the models were adjusted to have 2 encoder blocks and a feature embedding dimension of 256. For CPG-PE settings, we set $\\tau=10000.0$ , $N=20$ , $\\eta=2\\pi$ , and $v^{\\mathrm{thres}}=0.8$ . ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Training Hyper-parameters We honestly follow the experimental settings in [4], whose source code and configuration files are available at https://github.com/ZK-Zhou/spikformer. As the training epochs are quite big (300 epochs) in their settings, we choose to use one 80G-A100 GPU, and it takes about 3 hours to conduct a single experiment, on average. ", "page_idx": 15}, {"type": "text", "text": "B.4 Details about Positional Encoding Analysis ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We conducted positional encoding analysis experiments on the CIFAR10-DVS dataset. For the original Spikformer with relative positional encoding (RPE) as described by [4], the input and output channels of Conv2d are both set to 384. In our Spikformer with CPG-PE, the parameters are set to $\\tau=10000.0$ and $N\\,=\\,20$ . Given the time step $T=4$ and the sequence length $L=160$ for the image patches in CIFAR10-DVS samples, the total \"length\" $T\\times L$ in CPG-PE is 640. We then calculated the repetition rate of positions. The results showed that the repetition rate for RPE is $12.19\\%$ , whereas for CPG-PE, it is $0.00\\%$ . ", "page_idx": 15}, {"type": "text", "text": "C Implement CPG-PE with LIF Neurons ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we demonstrate that CPG-PE is a hardware-friendly design. While implementing the sinusoidal potential on the neuromorphic chips is not challenging (e.g., by maintaining additional LC circuits), we show how a CPG-PE neuron can be physically implemented with only 2 LIF neurons defined by Equations (1) to (3) and thus introducing no extra efforts on chip designs. ", "page_idx": 15}, {"type": "text", "text": "A CPG-PE neuron, after discretization, can be viewed as an autonomic neuron that will emit a burst of $K$ spikes after resting for $R$ time steps. The key idea is to set two LIF neurons, namely the Emitter and the Resetter. The emitter will draw constant current from the source, and as soon as its membrane potential reaches the threshold after $R$ time steps, it will start emitting spikes constantly until receiving the reset signal from the resetter. The resetter, which will remain at the resting potential until it receives signals from the emitter, will count the number of spikes and emit a reset signal (inhibition signal) to the emitter after receiving $K$ spikes. ", "page_idx": 15}, {"type": "text", "text": "We first prove the following Lemma, which establishes the relationship between the start time of the first spike and a constant input current. ", "page_idx": 15}, {"type": "text", "text": "Lemma 1. Given an $L I F$ neuron defined by Equations (1) to (3) with decay rate $\\beta$ and threshold $U_{t h r}$ , starting with resting potential $U(0)=0,$ , if fed with the constant current $I(t)=I_{c}>0$ , the first spike will emit at: ", "page_idx": 15}, {"type": "equation", "text": "$$\nT_{m i n}=\\left\\lceil\\log_{\\beta}(\\beta-\\frac{U_{t h r}\\beta(1-\\beta)}{I_{c}})\\right\\rceil.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. By definition, before the time to emit the first spike, we have $S(t)=0$ . Thus Equation (1) can be rewrite as: ", "page_idx": 15}, {"type": "equation", "text": "$$\nU(k\\Delta t)=\\beta U((k-1)\\Delta t)+I_{c}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Simplifying the recurrence relation, we can obtain: ", "page_idx": 15}, {"type": "equation", "text": "$$\nU(k\\Delta t)=\\frac{I_{c}}{\\beta}\\left(\\frac{1-\\beta^{k}}{1-\\beta}-1\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The first spike is generated when $U(k\\Delta t)\\leq U_{t h r}$ , thus we have: ", "page_idx": 15}, {"type": "equation", "text": "$$\nk\\geq\\log_{\\beta}(\\beta-\\frac{U_{t h r}\\beta(1-\\beta)}{I_{c}}),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "that is to say, ", "page_idx": 15}, {"type": "equation", "text": "$$\nT_{m i n}=\\left\\lceil\\log_{\\beta}(\\beta-\\frac{U_{t h r}\\beta(1-\\beta)}{I_{c}})\\right\\rceil.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Now we can implement the CPG-PE with LIF neurons: ", "page_idx": 16}, {"type": "text", "text": "Theorem 1. Given 2 LIF neurons, the emitter and the resetter, with decay rate $\\beta$ , threshold $U_{t h r}$ , and reset potential $V_{r e s e t}$ , starting with resting potential $U_{e}(0)=U_{r}(0)=0$ . If ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I_{e}(t)=I_{c1}+S_{e}(t-\\Delta t)(U_{t h r}-I_{c1}-V_{r e s e t})-S_{r}(t-\\Delta t)U_{t h r},}\\\\ &{I_{r}(t)=S_{e}(t-\\Delta t)I_{c2}-S_{r}(t-\\Delta t)(I_{c2}+V_{r e s e t}),}\\\\ &{\\;\\;\\;\\;I_{c1}=\\frac{U_{t h r}\\beta(1-\\beta)}{\\beta-\\beta^{R}},}\\\\ &{\\;\\;\\;\\;I_{c2}=\\frac{U_{t h r}\\beta(1-\\beta)}{\\beta-\\beta^{K-1}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "then the system will have the period of $T=(R+K)\\Delta t$ , and $\\forall i\\in\\mathbb{N}\\cap[0,R+K-1],k\\in\\mathbb{N}.$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\nS_{e}(i\\Delta t+k T)=\\binom{0,~~0\\leq i<R,}{1,~~R\\leq i<R+K.}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. Assuming the first spike generated by the emitter emits at time step $T_{1}$ . For every $0\\leq t<T_{1}$ , we have: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{S_{e}(t)=S_{r}(t)=0,}\\\\ {I_{e}(t)=I_{c1},I_{r}(t)=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Since the input current of the emitter is a constant, by Lemma 1, we immediately get: ", "page_idx": 16}, {"type": "equation", "text": "$$\nT_{1}=\\left\\lceil\\log_{\\beta}(\\beta-\\frac{U_{t h r}\\beta(1-\\beta)}{I_{c1}})\\right\\rceil=R.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Starting from time step $R$ , let\u2019s assume the first spike generated by the resetter emits at time step $T_{2}$ . Then for every $T_{1}\\leq t<T_{2}$ , we have: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{S_{e}(t)=1,S_{r}(t)=0,}\\\\ {I_{e}(t)=U_{t h r}-V_{r e s e t},I_{r}(t)=I_{c2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Starting from $T_{1}$ , for the emitter, the input current allows a spike event for every time step. And the input current of the resetter is a constant. Again, by applying Lemma 1, we can get: ", "page_idx": 16}, {"type": "equation", "text": "$$\nT_{2}=T_{1}+\\left\\lceil\\log_{\\beta}(\\beta-\\frac{U_{t h r}\\beta(1-\\beta)}{I_{c2}})\\right\\rceil=R+K-1.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Now Consider the state at time step $R+K$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{S_{e}((R+K-1)\\Delta t)=S_{r}((R+K-1)\\Delta t)=1,}\\\\ &{\\ \\ \\ \\ \\ I_{e}((R+K)\\Delta t)=I_{r}((R+K)\\Delta t)=-V_{r e s e t},}\\\\ &{\\ \\ \\ \\ \\ U_{e}((R+K)\\Delta t)=U_{r}((R+K)\\Delta t)=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "This is the same as the membrane potential at time step 0. Therefore, the system will behave periodically with period $T=(R+K)\\Delta T$ . \u53e3 ", "page_idx": 16}, {"type": "text", "text": "Theorem 1 gives a possible CPG-PE design with 2 LIF neurons, with the emitter generating $K$ consecutive spikes every $R+K$ time steps. This demonstrates that incorporating CPG-PE into the current SNN architecture is completely bio-plausible and will not introduce any burden of redesigning hardware. ", "page_idx": 16}, {"type": "text", "text": "D Implement CPG-Linear ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We have developed a simple modularization implementation to integrate our proposed CPG-PE with original linear layers, as depicted in Figure 3 (b). Consider the original linear layer\u2019s input and output dimensions as $D_{i n}$ and $D_{o u t}$ , respectively. Our objective is to incorporate CPG-PE within this framework. Following the application of the CPG-PE module, the modified input $X^{\\prime}$ is obtained. $X$ is then input into $\\mathrm{Linear_{1}}$ and $X^{\\prime}$ into $\\mathrm{Linear_{2}}$ , resulting in outputs $X_{1}$ and $X_{2}$ , respectively. Both ", "page_idx": 16}, {"type": "image", "img_path": "kQMyiDWbOG/tmp/23fcd0dce452cb97fcaa870516a947b61fc2de8429859c32764224251ba83c59.jpg", "img_caption": ["Figure 5: An illustration of the implementation of integrating a CPG-PE into a linear layer. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Linear1 and Linear2 maintain an output dimension of $D_{o u t}$ . The final step involves summing $X_{1}$ and $X_{2}$ to produce $X_{3}$ , which is subsequently processed through batch normalization (BN) and spike normalization $(\\mathcal{S}\\mathcal{N})$ . We term this implementation as \u201cCPG-Linear\u201d and formulize as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{X^{\\prime}=\\mathrm{CPG-PE}(X),}&&{X\\in\\{0,1\\}^{T\\times B\\times L\\times D_{i n}},X^{\\prime}\\in\\{0,1\\}^{T\\times B\\times L\\times2N}}\\\\ &{X_{1}=\\mathrm{Linear}_{1}(X),X_{2}=\\mathrm{Linear}_{2}(X^{\\prime}),}&&{X_{1},X_{2}\\in\\mathbb{R}^{T\\times B\\times L\\times D_{o u t}}}\\\\ &{X_{3}=X_{1}+X_{2},}&&{X_{3}\\in\\mathbb{R}^{T\\times B\\times L\\times D_{o u t}}}\\\\ &{X_{o u t p u t}=S\\mathcal{N}\\left(\\mathrm{BN}\\left(X_{3}\\right)\\right),}&&{X_{o u t}\\in\\{0,1\\}^{T\\times B\\times L\\times D_{o u t}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $^+$ denotes element-wise addition. This implementation described above is fundamentally identical to Figure 3, within the context of a single linear layer. However, the CPG-Linear can seamlessly replace any linear layer in SNNs. ", "page_idx": 17}, {"type": "text", "text": "E Results on ImageNet ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We have conducted experiments with Spikformer without positional encoding (PE), Spikformer with relative positional encoding (RPE), and Spikformer with our proposed CPG-PE on the ImageNet dataset. The results are as follows: ", "page_idx": 17}, {"type": "text", "text": "Table 5: Evaluation on ImageNet benchmarks. We employed 8 encoder blocks and 384 feature embedding dimensions across all models. ", "page_idx": 17}, {"type": "table", "img_path": "kQMyiDWbOG/tmp/88e8cc16545ed6b62838afbcded0e38ebf711606646cd6c21cbbfd3d4ec22d0d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Specifically, we set the depth to 8 and the dimension of representation to 384. From the table, we can see that CPG-PE performs well on large-scale image datasets. We believe that the above results demonstrate the effectiveness of our proposed CPG-PE in positional encoding. ", "page_idx": 17}, {"type": "text", "text": "F Limitations and Future Works ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we will discuss the limitations and future works of our paper. ", "page_idx": 17}, {"type": "text", "text": "F.1 Limitations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "As mentioned in Section 3.3, our CPG-PE can not be directly applied to those SNNs where spike matrices do not have a \u201csequence length\u201d dimension. Our CPG-PE is optimized for processing sequential data, making it ideal for applications involving time series or natural language. This intrinsic design, however, does not naturally extend to image data, which typically beneftis from direct convolutional operations that capture spatial relationships across the entire image dimensions\u2014height and width. In contrast, CPG-PE requires the segmentation of images into patches, a method inspired by the Vision Transformer. This adaptation contrasts with approaches like the Convolutional 2D layer, which applies convolution operations directly across the height and width of an image without requiring segmentation into smaller, discrete patches. The necessity to adapt CPG-PE for image data through patching can introduce complexities and potential performance bottlenecks, as it may not effectively capture the continuous spatial relationships and local features in the image, which are crucial for tasks such as object recognition and scene understanding. ", "page_idx": 18}, {"type": "text", "text": "F.2 Future Works ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "To enhance the applicability of the CPG-PE model to a broader range of data types, especially image data, future research could focus on developing a hybrid model that integrates the strengths of CPG-PE with traditional convolutional layers. This integration could potentially allow the model to handle both sequential and spatial data efficiently without the need for extensive pre-processing or adaptation. Specifically, integrating direct convolution operations that work across the entire spatial dimensions of an image within the CPG-PE architecture could help preserve spatial relationships and improve feature extraction capabilities. Additionally, exploring the use of adaptive patch sizes or dynamically adjusting the patching mechanism based on the nature of the input data could also provide a more flexible and performance-optimized approach. These advancements would make the model more versatile and capable of tackling a wider array of tasks across different domains. ", "page_idx": 18}, {"type": "text", "text": "Additionally, considering that CPG-PE is an absolute positional encoding designed for SNNs, it could be beneficial to explore the potential of implementing learnable relative positional encodings in SNNs. Such encodings would need to be developed to meet specific criteria: they must maintain the spikeform characteristic essential to SNNs and ensure the uniqueness of each position\u2019s encoding. This approach could significantly enhance the model\u2019s ability to capture and utilize the temporal dynamics of input data more effectively, potentially leading to more nuanced and context-aware processing capabilities. Exploring adaptive patch sizes or dynamically adjusting the patching mechanism based on the nature of the input data could also provide a more flexible and performance-optimized approach. These advancements would make the model more versatile and capable of tackling a wider array of tasks across different domains. ", "page_idx": 18}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We have clarified our claims in the abstract and introduction. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 19}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: We have discussed the limitations and future work in Appendix F ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 19}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We have provided the full set of assumptions and a complete (and correct) proof in the Method Section. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 20}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We have shown our experiment results in the Experiment Section. We have submitted our source code in Supplementary Material. We will upload our code and data to Github upon acceptance. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 20}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We have submitted our source code in Supplementary Material. We will upload our code and data to Github upon acceptance. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 21}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: We have shown our experimental settings and implementation details in Appendices A and B respectively. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 21}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: Our reported results are all averaged over several random seeds. We have reported the error bar of the results in Table 2. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 21}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 22}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We have provided the compute resource in Appendix B. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 22}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: The research conducted in the paper conforms, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 22}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We have discussed both potential positive societal impacts and negative societal impacts of the work in Broader Impact Section. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 22}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 23}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper poses no such risks. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 23}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The datasets we used in the paper are all public datasets. Please refer to Appendix A for details of datasets. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 24}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 24}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}]