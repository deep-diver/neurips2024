[{"figure_path": "VFqzxhINFU/figures/figures_1_1.jpg", "caption": "Figure 1: Images and videos generated by our StoryDiffusion. (a) Comic generated by StoryDiffusion telling the story of a man who discovers a treasure while exploring the jungle. (b) Comic generated by StoryDiffusion describing the expedition to the moon by Lecun, with a reference image control [26] same as Fig. 7(b). (c) Videos generated by our StoryDiffusion. Click the image to play the video. Best viewed with Acrobat Reader. More generated videos can be found in the uploaded supplementary file.", "description": "This figure showcases the capabilities of StoryDiffusion in generating both image and video content based on textual descriptions.  Panel (a) demonstrates a comic strip illustrating a jungle adventure, while (b) depicts a comic strip about an astronaut's moon landing, incorporating reference image control for character consistency. Panel (c) displays examples of videos generated using the model, emphasizing smooth transitions and consistent subject representation.", "section": "1 Introduction"}, {"figure_path": "VFqzxhINFU/figures/figures_1_2.jpg", "caption": "Figure 1: Images and videos generated by our StoryDiffusion. (a) Comic generated by StoryDiffusion telling the story of a man who discovers a treasure while exploring the jungle. (b) Comic generated by StoryDiffusion describing the expedition to the moon by Lecun, with a reference image control [26] same as Fig. 7(b). (c) Videos generated by our StoryDiffusion. Click the image to play the video. Best viewed with Acrobat Reader. More generated videos can be found in the uploaded supplementary file.", "description": "This figure showcases the capabilities of StoryDiffusion in generating both image and video content.  Panel (a) shows a comic illustrating a jungle adventure, panel (b) depicts a comic of a moon exploration by Lecun (with a reference image used for control), and panel (c) displays example videos produced, highlighting smooth transitions and consistent subjects.", "section": "1 Introduction"}, {"figure_path": "VFqzxhINFU/figures/figures_3_1.jpg", "caption": "Figure 2: The Pipeline of StoryDiffusion to generating subject-consistent images. To create subject-consistent images to describe a story, we incorporate our Consistent Self-Attention into the pre-trained text-to-image diffusion model. We split a story text into several prompts and generate images using these prompts in a batch. Consistent Self-Attention builds connections among multiple images in a batch for subject consistency.", "description": "This figure illustrates the overall pipeline of StoryDiffusion for generating subject-consistent images.  It shows how a story is split into prompts, fed into a pre-trained text-to-image diffusion model enhanced with Consistent Self-Attention, and how this module builds connections between multiple images within a batch to ensure consistency. The figure also provides a detailed breakdown of the Consistent Self-Attention module itself, showing its internal operations and how it boosts consistency in the generated images.", "section": "3 Method"}, {"figure_path": "VFqzxhINFU/figures/figures_4_1.jpg", "caption": "Figure 3: The pipeline of our method for generating transition videos for obtaining subject-consistent images, as described in Sec. 3.1. To effectively model the character's large motions, we encode the conditional images into the image semantic space for encoding spatial information and predict the transition embeddings. These predicted embeddings are then decoded using the video generation model, with the embeddings serving as control signals in cross-attention to guide the generation of each frame.", "description": "This figure illustrates the process of generating transition videos using StoryDiffusion.  It starts with a sequence of generated consistent images (or user input). These images are encoded into a semantic space using an image encoder. A Semantic Motion Predictor then takes these semantic embeddings and predicts intermediate frames to smoothly transition between the input images. These predicted frames are used as control signals in a video diffusion model which generates the final consistent videos.", "section": "3 Method"}, {"figure_path": "VFqzxhINFU/figures/figures_6_1.jpg", "caption": "Figure 4: Comparison of consistent image generation with recent methods.", "description": "This figure compares the performance of StoryDiffusion with two other state-of-the-art methods (IP-Adapter and PhotoMaker) for generating consistent images given a text prompt. Each row represents a different method, and each column depicts a different activity described by the prompt. The results show that StoryDiffusion generates images with better text controllability, higher face consistency, and better attire cohesion across all the activities compared to the other two methods.", "section": "4.2 Comparisons of consistent image generation"}, {"figure_path": "VFqzxhINFU/figures/figures_6_2.jpg", "caption": "Figure 5: Additional comparison of our StoryDiffusion with recent storybook generation methods, The Chosen One [1], ConsiStory [44] and Zero-shot coherent storybook [22].", "description": "This figure compares the image generation results of StoryDiffusion with three other storybook generation methods: The Chosen One, ConsiStory, and Zero-shot coherent storybook.  Each method is given the same prompt to generate a sequence of images depicting a story. The figure shows that StoryDiffusion generates images that are more consistent in terms of character identity, attire, and setting across the different images in the sequence compared to the other methods. This highlights StoryDiffusion's ability to generate more coherent and visually appealing storybooks.", "section": "4.2 Comparisons of consistent image generation"}, {"figure_path": "VFqzxhINFU/figures/figures_7_1.jpg", "caption": "Figure 6: Comparisons of transition video generation with the recent state-of-the-art methods.", "description": "This figure compares the performance of StoryDiffusion against two state-of-the-art methods (SEINE and SparseCtrl) in generating transition videos. Two example video transitions are shown, each with the generated intermediate frames from the three methods displayed. The figure showcases StoryDiffusion's ability to generate smoother and more physically plausible transitions compared to the other methods.", "section": "4.3 Comparisons of transition videos generation"}, {"figure_path": "VFqzxhINFU/figures/figures_8_1.jpg", "caption": "Figure 7: Ablation study. (a) Evaluations of the impact of different sampling rates in Consistent Self-Attention. (b) We explore the introduction of external control IDs to govern the generation of characters. Our StoryDiffusion can generate consistent images that conform to the ID images.", "description": "This figure presents the results of an ablation study on the StoryDiffusion model.  The left side (a) shows how different sampling rates in the Consistent Self-Attention module affect the consistency of generated images.  The right side (b) demonstrates the ability of StoryDiffusion to generate consistent images conforming to a specified identity (ID) image provided as an external control.", "section": "4.4 Ablation study"}, {"figure_path": "VFqzxhINFU/figures/figures_14_1.jpg", "caption": "Figure 1: Images and videos generated by our StoryDiffusion. (a) Comic generated by StoryDiffusion telling the story of a man who discovers a treasure while exploring the jungle. (b) Comic generated by StoryDiffusion describing the expedition to the moon by Lecun, with a reference image control [26] same as Fig. 7(b). (c) Videos generated by our StoryDiffusion. Click the image to play the video. Best viewed with Acrobat Reader. More generated videos can be found in the uploaded supplementary file.", "description": "This figure showcases examples of image and video generation results using StoryDiffusion.  Panel (a) presents a comic illustrating a jungle adventure story. Panel (b) shows a comic depicting a moon exploration, incorporating a reference image for control. Panel (c) displays video clips generated by the model, highlighting smooth transitions and consistent subject depiction across longer sequences.", "section": "1 Introduction"}, {"figure_path": "VFqzxhINFU/figures/figures_15_1.jpg", "caption": "Figure 9: Additional visual comparison of consistent image generation.", "description": "This figure provides a qualitative comparison of consistent image generation results from StoryDiffusion, PhotoMaker, and IP-Adapter across six different scenarios. Each scenario consists of three image generation tasks with varying prompts designed to test the models' ability to maintain consistency in character appearance and attire, despite changes in the background and actions. The results show that StoryDiffusion excels at maintaining consistency across all scenarios, outperforming PhotoMaker and IP-Adapter which often struggle with attire consistency or fall short in generating images that fully align with the text prompts.", "section": "A.3 Consistent image generation"}, {"figure_path": "VFqzxhINFU/figures/figures_16_1.jpg", "caption": "Figure 10: Additional visual comparison of transition video generation. The red box represents the frames input into the model.", "description": "This figure compares the performance of three different methods (SEINE, SparseCtrl, and StoryDiffusion) in generating intermediate frames for transition videos.  Each row shows a different video transition example, with the start and end frames shown on the left and right, respectively. The generated intermediate frames are displayed in the center.  The red boxes highlight the input frames used for each method.  The comparison highlights StoryDiffusion's superior ability to produce smoother and more coherent intermediate frames, especially when compared to the other two methods that struggle with large motion gaps between start and end frames.", "section": "A.4 Transition video generation"}, {"figure_path": "VFqzxhINFU/figures/figures_17_1.jpg", "caption": "Figure 11: Generation results of our Consistent Self-Attention combined with ControlNet.", "description": "This figure shows the results of combining Consistent Self-Attention with ControlNet for pose-guided image generation.  It demonstrates the effectiveness of Consistent Self-Attention in maintaining subject consistency even when using additional control mechanisms like ControlNet's pose guidance. The figure presents several examples, each showing the generated images with and without Consistent Self-Attention, illustrating how the proposed method improves the consistency of generated images while adhering to the specified pose constraints.", "section": "A.5 Consistent images generation with ControlNet"}, {"figure_path": "VFqzxhINFU/figures/figures_18_1.jpg", "caption": "Figure 12: The pluggable capability of our StoryDiffusion on several popular diffusion models, our Consistent Self-Attention works well across multiple models.", "description": "This figure shows the results of applying StoryDiffusion to three different diffusion models: Stable Diffusion 1.5, Stable Diffusion 2.1, and Stable Diffusion XL.  The consistent results across these different models demonstrate the plug-and-play nature of the Consistent Self-Attention module, highlighting its adaptability and robustness.  The images show a series of consistent images generated for each model, illustrating the subject's consistent identity and attire.", "section": "A.6 Experiments on Plug-and-Play capability"}]