[{"figure_path": "3LKuC8rbyV/figures/figures_1_1.jpg", "caption": "Figure 1: The geometric interpretation of relations between learning and unlearning. (Left) RDP guarantee of the learning process induces a regular polyhedron. Smaller \u03b50 implies an \u201ceasier\u201d unlearning problem. (Right) Learning and unlearning processes on adjacent datasets. It illustrates our main idea and results. More learning iteration gives worse privacy (privacy erosion [12]) while more unlearning iteration gives better privacy, which we termed this phenomenon as privacy recuperation.", "description": "This figure provides a geometric interpretation of the relationship between the learning and unlearning processes within the context of differential privacy.  The left panel illustrates how the stronger initial privacy guarantee from the learning process (smaller epsilon naught) results in a smaller polyhedron, making the unlearning process easier. The right panel shows how the learning process leads to privacy erosion (worse privacy as iterations increase) while the subsequent unlearning process leads to privacy recuperation (improved privacy as iterations increase).  This visualization highlights the core idea of Langevin unlearning, showcasing how the unlearning process can effectively recover privacy lost during the learning phase.", "section": "1 Introduction"}, {"figure_path": "3LKuC8rbyV/figures/figures_6_1.jpg", "caption": "Figure 1: The geometric interpretation of relations between learning and unlearning. (Left) RDP guarantee of the learning process induces a regular polyhedron. Smaller \u03b50 implies an \u201ceasier\u201d unlearning problem. (Right) Learning and unlearning processes on adjacent datasets. It illustrates our main idea and results. More learning iteration gives worse privacy (privacy erosion [12]) while more unlearning iteration gives better privacy, which we termed this phenomenon as privacy recuperation.", "description": "This figure geometrically illustrates the relationship between learning and unlearning. The left panel shows how the learning process's R\u00e9nyi Differential Privacy (RDP) guarantee is represented as a polyhedron, where a smaller \u03b50 (initial privacy loss) indicates an easier unlearning task.  The right panel demonstrates the learning and unlearning processes on neighboring datasets.  It shows that increased learning iterations lead to privacy erosion, whereas increased unlearning iterations result in privacy recuperation.", "section": "3 Langevin Unlearning: Main Results"}, {"figure_path": "3LKuC8rbyV/figures/figures_7_1.jpg", "caption": "Figure 3: Main experiments, where the top and bottom rows are for MNIST and CIFAR10 respectively. (a) Compare to D2D for unlearning one point using limited unlearning iteration. This demonstrates the privacy-utility (e-accuracy) tradeoff under the fixed unlearning complexity (K). For Langevin unlearning, we use only K = 1 unlearning iterations. For D2D, we allow it not only to use K = 1,2,5 unlearning iterations but also to keep the non-private internal state information. (b) Compare to D2D for unlearning 100 points, where all methods achieve (\u20ac, 1/n)-unlearning guarantee with \u20ac = 1. For Langevin unlearning, we vary different unlearning batch sizes S and combine them with the sequential unlearning result. For D2D, we do not allow it to keep the non-private internal state information in this experiment so that there is an inherent lower bound on the unlearning iterations per unlearning request. (c) A detailed investigation of the utility-complexity trade-off of Langevin unlearning with unlearning S = 100 points at once under the fixed privacy constraint \u20ac = 1. For each \u03c3, we report the corresponding \u20ac0 (black dash line) for the initial (60,1/n)-DP guarantee and the utility after unlearning to e 1.", "description": "This figure compares the performance of Langevin Unlearning with D2D and Retraining methods on MNIST and CIFAR10 datasets.  It shows the tradeoff between privacy, utility (accuracy), and computational complexity (number of iterations) for different unlearning scenarios: unlearning one point, unlearning 100 points, and the impact of noise variance on the tradeoff.", "section": "4 Experiments"}, {"figure_path": "3LKuC8rbyV/figures/figures_8_1.jpg", "caption": "Figure 3: Main experiments, where the top and bottom rows are for MNIST and CIFAR10 respectively. (a) Compare to D2D for unlearning one point using limited unlearning iteration. This demonstrates the privacy-utility (e-accuracy) tradeoff under the fixed unlearning complexity (K). For Langevin unlearning, we use only K = 1 unlearning iterations. For D2D, we allow it not only to use K = 1,2,5 unlearning iterations but also to keep the non-private internal state information. (b) Compare to D2D for unlearning 100 points, where all methods achieve (e, 1/n)-unlearning guarantee with e = 1. For Langevin unlearning, we vary different unlearning batch sizes S and combine them with the sequential unlearning result. For D2D, we do not allow it to keep the non-private internal state information in this experiment so that there is an inherent lower bound on the unlearning iterations per unlearning request. (c) A detailed investigation of the utility-complexity trade-off of Langevin unlearning with unlearning S = 100 points at once under the fixed privacy constraint e = 1. For each \u03c3, we report the corresponding e0 (black dash line) for the initial (e0,1/n)-DP guarantee and the utility after unlearning to e = 1.", "description": "The figure compares the performance of Langevin Unlearning with Delete-to-Descent (D2D) and retraining from scratch for unlearning tasks on MNIST and CIFAR10 datasets.  It shows the privacy-utility trade-off, considering different numbers of unlearned data points and unlearning iterations.  Panel (a) focuses on unlearning a single point, (b) on unlearning 100 points, and (c) explores the impact of noise variance on utility.", "section": "4 Experiments"}, {"figure_path": "3LKuC8rbyV/figures/figures_30_1.jpg", "caption": "Figure 3: Main experiments, where the top and bottom rows are for MNIST and CIFAR10 respectively. (a) Compare to D2D for unlearning one point using limited unlearning iteration. This demonstrates the privacy-utility (e-accuracy) tradeoff under the fixed unlearning complexity (K). For Langevin unlearning, we use only K = 1 unlearning iterations. For D2D, we allow it not only to use K = 1,2,5 unlearning iterations but also to keep the non-private internal state information. (b) Compare to D2D for unlearning 100 points, where all methods achieve (e, 1/n)-unlearning guarantee with e = 1. For Langevin unlearning, we vary different unlearning batch sizes S and combine them with the sequential unlearning result. For D2D, we do not allow it to keep the non-private internal state information in this experiment so that there is an inherent lower bound on the unlearning iterations per unlearning request. (c) A detailed investigation of the utility-complexity trade-off of Langevin unlearning with unlearning S = 100 points at once under the fixed privacy constraint e = 1. For each s, we report the corresponding e0 (black dash line) for the initial (e0,1/n)-DP guarantee and the utility after unlearning to e = 1.", "description": "The figure compares the performance of Langevin unlearning against Delete-to-Descent (D2D) and retraining for different unlearning scenarios.  It shows the trade-off between privacy, utility (accuracy), and the complexity of the unlearning process under various settings, such as unlearning one or multiple data points, and batch vs. sequential unlearning requests.  Subplots (a) and (b) compare the methods' accuracy with different unlearning iterations.  Subplot (c) analyzes the utility-complexity tradeoff of Langevin unlearning.", "section": "4 Experiments"}]