[{"figure_path": "RL4FXrGcTw/figures/figures_1_1.jpg", "caption": "Figure 1: Values (down) and gradients (up) of functions of large matrices.", "description": "This figure illustrates the workflow of the proposed method for computing gradients of functions of large matrices. It starts with a parameter \u03b8 that is used to compute a large matrix-vector product via Arnoldi or Lanczos iterations. These iterations are then used to obtain a small factorization, which is subsequently used to compute the function of the large matrix. Finally, the loss is calculated based on the function of the large matrix. The gradients are computed using the adjoint method, propagating the gradient backward through the entire process, making the matrix-free implementation of matrix decompositions differentiable.", "section": "1 Introduction"}, {"figure_path": "RL4FXrGcTw/figures/figures_2_1.jpg", "caption": "Figure 2: Lanczos/Arnoldi iteration.", "description": "This figure shows a schematic of the Lanczos/Arnoldi iteration.  A large matrix A(\u03b8) is implicitly represented through matrix-vector products. The iteration generates an orthonormal matrix Q and a smaller Hessenberg matrix H such that A(\u03b8)Q \u2248 QH + r(ek). Here, ek is the k-th unit vector and r is a residual vector. This approximation is used for computing functions of large matrices in a matrix-free manner.", "section": "3 Problem statement"}, {"figure_path": "RL4FXrGcTw/figures/figures_3_1.jpg", "caption": "Figure 3: Backpropagation vs our adjoint method on a sparse matrix [63\u201365].", "description": "This figure compares the performance of backpropagation against the proposed adjoint method for computing gradients of functions of large sparse matrices using the Lanczos method.  The x-axis represents the Krylov subspace depth (K), and the y-axis represents the wall time (in seconds) for each method. It shows how backpropagation's time complexity increases significantly with K, making it impractical for larger K values, while the adjoint method maintains similar performance to the forward pass, making it more efficient for large-scale computations.", "section": "Limitations and future work"}, {"figure_path": "RL4FXrGcTw/figures/figures_7_1.jpg", "caption": "Figure 5: Arnoldi's superior convergence on the forward pass (A1) is inherited by the gradients (A2; mind the shared y-axis) and ultimately leads to fast training (B). For training, Arnoldi uses ten matrix-vector products, and the other two use 15 (so they have equal error \u2248 10-4 in A1 and A2.)", "description": "This figure compares the performance of three different methods (Arnoldi, Dopri5, and Tsit5) for solving a partial differential equation.  Panel A1 shows the forward error (RMSE) of each method as a function of the number of matrix-vector products. Panel A2 shows the gradient error for each method. Finally, panel B shows the training loss over time for each method. The Arnoldi method consistently outperforms the other two methods in terms of accuracy and training speed.", "section": "Case study: Physics-informed machine learning with PDEs"}, {"figure_path": "RL4FXrGcTw/figures/figures_7_2.jpg", "caption": "Figure 4: All methods find the truth.", "description": "This figure compares the performance of three different solvers, Arnoldi, Dopri5, and Tsit5, in reconstructing a true coefficient field. Each subplot shows a contour plot of the reconstructed coefficient field using a specific solver. The color scale indicates the magnitude of the coefficient field, ranging from 0 to 0.014. The figure demonstrates that all three solvers accurately capture the overall shape and structure of the true coefficient field, although with varying degrees of accuracy and detail.", "section": "6 Case study: Physics-informed machine learning with PDEs"}, {"figure_path": "RL4FXrGcTw/figures/figures_8_1.jpg", "caption": "Figure 6: Lanczos vs diagonal approx. for a Bayesian VAN.", "description": "This figure compares the performance of Lanczos and diagonal approximation methods for optimizing the negative log-marginal likelihood of a Bayesian Visual Attention Network (VAN) during training. The y-axis represents the negative log-marginal likelihood (in millions), and the x-axis shows the number of epochs. The Lanczos method consistently achieves lower negative log-marginal likelihood values compared to the diagonal approximation method, indicating better calibration of the model parameters and ultimately, better performance. This demonstrates the effectiveness of the Lanczos iteration for approximating the log-determinant of the generalized Gauss-Newton matrix in the context of Bayesian neural networks.", "section": "Case study: Calibrating Bayesian neural networks"}, {"figure_path": "RL4FXrGcTw/figures/figures_16_1.jpg", "caption": "Figure 3: Backpropagation vs our adjoint method on a sparse matrix [63\u201365].", "description": "The figure shows a comparison of the performance of three methods for computing gradients of matrix functions. The methods considered are backpropagation, the adjoint method, and the authors' new method. The x-axis represents the depth of the Krylov subspace used, and the y-axis represents wall-clock time in seconds. The figure shows that the authors' method is significantly faster than backpropagation, while maintaining the same linear runtime and memory complexity. The speedup provided by the new algorithm is due to the way gradients of matrix functions are computed. In standard backpropagation, gradients are obtained by backpropagating through the entire computation graph. The authors' method, on the other hand, computes gradients using the adjoint method, which is computationally more efficient and scalable. For small Krylov subspace depths, the computational time is approximately equal for all methods; however, as the Krylov subspace depth increases, backpropagation becomes significantly slower. ", "section": "Limitations and future work"}, {"figure_path": "RL4FXrGcTw/figures/figures_23_1.jpg", "caption": "Figure 3: Backpropagation vs our adjoint method on a sparse matrix [63\u201365].", "description": "The figure compares the performance of three different methods for computing gradients of a function of a sparse matrix, namely, backpropagation, the adjoint method (proposed by the authors), and a forward pass.  The x-axis represents the depth of the Krylov subspace used in the Lanczos iteration, and the y-axis represents the computation time in seconds. The results show that the adjoint method has linear time complexity, while backpropagation exhibits exponential growth in computation time as the Krylov subspace depth increases. The forward pass serves as a baseline.", "section": "4 The method: Adjoints of the Lanczos and Arnoldi iterations"}, {"figure_path": "RL4FXrGcTw/figures/figures_24_1.jpg", "caption": "Figure 10: For matrices with at least 10,000 rows/columns, KeOps remains the state of the art. This experiment uses a square-exponential kernel, on an artificial dataset with d = 3 dimensions.", "description": "This figure compares the runtime performance of different methods for computing matrix-vector products, a crucial operation in many machine learning algorithms involving large matrices.  It shows that for matrices with 10,000 or more rows/columns, the KeOps library significantly outperforms the custom JAX implementations described in the paper, despite the efforts to optimize those implementations using both `map` and `vmap` functions in JAX and varying the number of Krylov subspace iterations (K). This highlights that while the proposed method is efficient for smaller matrices, high-performance libraries like KeOps still offer significant advantages for truly large-scale problems.", "section": "5 Case study: Exact Gaussian processes"}, {"figure_path": "RL4FXrGcTw/figures/figures_26_1.jpg", "caption": "Figure 11: Three exemplary input/output pairs from the PDE dataset.", "description": "This figure shows three example pairs of input and output from the PDE dataset used in the paper. Each pair consists of a 2D spatial representation of the input and the corresponding output. The inputs represent initial conditions, while the outputs are obtained by solving the partial differential equation defined in the paper.  The visualization highlights the relationship between the input and the resulting output after applying the specified operations.", "section": "6 Case study: Physics-informed machine learning with PDEs"}]