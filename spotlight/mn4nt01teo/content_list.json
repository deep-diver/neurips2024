[{"type": "text", "text": "Adaptive Randomized Smoothing: Certified Adversarial Robustness for Multi-Step Defences ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Saiyue Lyu1\u2217, Shadab Shaikh1\u2217, Frederick Shpilevskiy1\u2217, Evan Shelhamer2, Mathias L\u00e9cuyer1 ", "page_idx": 0}, {"type": "text", "text": "1University of British Columbia, 2Google DeepMind {saiyuel, shadabs3, fshpil}@cs.ubc.ca, shelhamer@deepmind.com, mathias.lecuyer@ubc.ca \u2217equal contribution ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We propose Adaptive Randomized Smoothing (ARS) to certify the predictions of our test-time adaptive models against adversarial examples. ARS extends the analysis of randomized smoothing using $f$ -Differential Privacy to certify the adaptive composition of multiple steps. For the first time, our theory covers the sound adaptive composition of general and high-dimensional functions of noisy inputs. We instantiate ARS on deep image classification to certify predictions against adversarial examples of bounded $L_{\\infty}$ norm. In the $L_{\\infty}$ threat model, ARS enables flexible adaptation through high-dimensional inputdependent masking. We design adaptivity benchmarks, based on CIFAR-10 and CelebA, and show that ARS improves standard test accuracy by 1 to $15\\%$ points. On ImageNet, ARS improves certified test accuracy by up to $1.6\\%$ points over standard RS without adaptivity. Our code is available at https: //github.com/ubc-systopia/adaptive-randomized-smoothing. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Despite impressive accuracy, deep learning models still show a worrying susceptibility to adversarial attacks. Such attacks have been shown for a large number of tasks and models (Costa et al., 2023; Chakraborty et al., 2018), including areas where security and safety are critical such as fraud detection (Pumsirirat and Liu, 2018) or autonomous driving (Cao et al., 2021). ", "page_idx": 0}, {"type": "text", "text": "Several rigorous defences have been proposed to certify robustness. Randomized Smoothing (RS) (L\u00e9cuyer et al., 2019; Cohen et al., 2019) does so by averaging predictions over noisy versions of the input at test time, and as such can scale to large deep learning models. However, RS has its limitations: it is inflexible and either degrades accuracy or only certifies against small attacks. ", "page_idx": 0}, {"type": "text", "text": "To address these shortcomings and improve robustness, there has been a recent push to develop defences that adapt to inputs at test time Croce et al. (2022), including for RS (Alfarra et al., 2022a; S\u00faken\u00edk et al., 2021; Hong et al., 2022). Most such adaptive defences are heuristic, unproven, and subject to improved attacks (Croce et al., 2022; Alfarra et al., 2022a; Hong et al., 2022), running the risk of reverting to a hopeless cat and mouse game with attackers (Athalye et al., 2018; Tramer et al., 2020), or only provide limited adaptivity (S\u00faken\u00edk et al., 2021; Hong et al., 2022) and gain (\u00a75). ", "page_idx": 0}, {"type": "text", "text": "We (re)connect RS to Differential Privacy (DP), after its abandonment for a tighter analysis via hypothesis testing (Cohen et al., 2019), and introduce Adaptive Randomized Smoothing (ARS) to provide test-time adaptivity while preserving rigorous bounds. Specifically, we analyze RS through the lens of $f$ -Differential Privacy $f$ -DP), and use this connection to leverage a key strength of DP: the end-to-end analysis of multi-step adaptive computation using composition results (\u00a72). ", "page_idx": 0}, {"type": "text", "text": "We use ARS to design two-step defence against $L_{\\infty}$ adversaries on image classification (Figure 1), which is a challenging setting for RS Blum et al. (2020). The first step computes an input mask that focuses on task-relevant information. This reduces the dimension of the input, which is then passed to the second step for prediction. Thanks to this adaptive dimension reduction, the second step makes its prediction on a less noisy image, improving the performance and certification radius (\u00a73). ", "page_idx": 0}, {"type": "image", "img_path": "MN4nt01TeO/tmp/04ff5304f2ac397e0aeb80c8867917f9e2916b183726795f96fdc5f1047406b9.jpg", "img_caption": ["Figure 1: Two-step ARS for $L_{\\infty}$ -bounded attacks. Step $\\mathcal{M}_{1}$ adds noise to input $X$ and post-processes the result into a mask $w(m_{1})$ . Step $\\mathcal{M}_{2}$ takes masked input $w(m_{1})\\odot X$ and adds noise to get $m_{2}$ . Base classifier $g$ post-processes a weighted average of $m_{1},m_{2}$ to output a label. RS reduces to $\\sigma_{2}=\\sigma$ and $w(.)=1$ (no $\\mathcal{M}_{1})$ ). "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "We evaluate our adaptive randomized smoothing method in three settings (\u00a74). For image classification, we first design a challenging benchmark based on CIFAR-10, and we show that ARS can improve accuracy by up to $15\\%$ . For spatially-localized face attribute classification on the CelebA dataset, we show that ARS improves accuracy by up to $7.7\\%$ . For large-scale image classification on ImageNet, ARS maintains accuracy and improves certified accuracy by up to $1.6\\bar{\\%}$ . ", "page_idx": 1}, {"type": "text", "text": "2 Theory for Adaptivity via Differential Privacy ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "After introducing the necessary background and known results on RS and DP (\u00a72.1), we reconnect RS to its DP roots by showing that the tight analysis of Cohen et al. (2019) can be seen as PixelDP (L\u00e9cuyer et al., 2019) using $f$ -Differential Privacy (Dong et al., 2019; 2022), a hypothesis testing formulation of DP (\u00a72.2). This connection lets us leverage composition results for $f$ -DP to analyze multi-step approaches for provable robustness to adversarial examples, which we name Adaptive Randomized Smoothing (ARS) (\u00a72.3). We leverage ARS to design and analyze a two-step defence against $L_{\\infty}$ -bounded adversaries (\u00a72.4), which we then instantiate as a deep network (\u00a73). ", "page_idx": 1}, {"type": "text", "text": "2.1 Related Work: Adversarial Robustness, Randomized Smoothing, and Differential Privacy ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Adversarial Examples (Szegedy et al., 2014): Consider a classifier $g:\\mathcal{X}\\to\\mathcal{Y}$ , and input $X$ . An adversarial example of radius $r$ in the $L_{p}$ threat model, for model $g$ on input $X$ , is an input $X+e$ such that $g(X+e)\\neq g(X)$ , where $e\\in B_{p}(r)$ , where $B_{p}(r)\\triangleq\\{x\\in\\mathbb{R}^{d}:\\|x\\|_{p}\\leq r\\}$ is the $L_{p}$ ball of radius $r$ . These inputs or attacks are made against classifiers at test time. For more on the active topics of attack and defence, we refer to surveys (Li et al., 2023; Costa et al., 2023; Chakraborty et al., 2018) on current attacks and provable defences. In general, provable defences do not focus on the largest-scale highest-accuracy classifiers, with the notable exception of randomized smoothing. ", "page_idx": 1}, {"type": "text", "text": "Randomized Smoothing (RS) (L\u00e9cuyer et al., 2019; Cohen et al., 2019) is a scalable approach to certifying model predictions against $L_{2}$ -norm adversaries. Specifically, it certifies robustness to any attack $\\in\\,B_{2}(r_{X})$ . The algorithm randomizes a base model $g$ by adding spherical Gaussian noise to its input, and produces a smoothed classifier that returns the class with highest expectation over the noise: $y_{+}\\;\\triangleq\\;\\arg\\operatorname*{max}_{y\\in\\mathcal{Y}}\\mathbb{P}_{z\\sim{\\mathcal{N}}(0,\\sigma^{2}\\mathbb{I}^{d})}\\big(g(X\\,+\\,z)\\;=\\;y\\big)$ . The tightest analysis from Cohen et al. (2019) uses hypothesis testing theory to show that, with $\\underline{{p_{+}}},\\overline{{p_{-}}}\\,\\in\\,[0,1]$ such that $\\mathbb{P}(g(X+z)\\,=\\,y_{+})\\,\\geq\\,\\underline{{p}}_{+}\\,\\geq\\,\\overline{{p_{-}}}\\,\\geq\\,\\operatorname*{max}_{y_{-}\\neq y_{+}}\\mathbb{P}(g(X+z)\\,=\\,y_{-})$ , the certificate size $r_{X}$ for prediction $y_{+}$ is: ", "page_idx": 1}, {"type": "equation", "text": "$$\nr_{X}={\\frac{\\sigma}{2}}\\big(\\Phi^{-1}({\\underline{{p_{+}}}})-\\Phi^{-1}({\\overline{{p_{-}}}})\\big),\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\Phi^{-1}$ is the inverse standard Gaussian CDF, $p_{\\pm}$ lower-bounds the probability of $g(X+z)=y_{+}$ (the most probable class), and $\\overline{{p_{-}}}$ upper-bounds the probability of other classes. ", "page_idx": 1}, {"type": "text", "text": "While sound, RS is static during testing even though attacks may adapt. Recent work aims to make RS adapt at test time (S\u00faken\u00edk et al., 2021; Alfarra et al., 2022a; Hong et al., 2022). While pioneering, these works are restricted in either their soundness or their degree of adaptation and resulting improvement. S\u00faken\u00edk et al. (2021) soundly adapt the variance for RS by the distance between test and train inputs. However, this only provides minimal adaptivity, with only minor improvement to certification. Alfarra et al. (2022a) adapt the variance for RS to each test input, but the analysis is not end-to-end, and hence not sound (S\u00faken\u00edk et al., 2021) (except for their memory-based edition, but this requires storage of all examples). UniCR (Hong et al., 2022) adapts the noise distribution for RS, primarily to the data distribution during training, and optionally to input during testing. The train-time adaptation is sound, but the test-time adaptation is not due to the same issue raised by S\u00faken\u00edk et al. (2021). We propose ARS to advance certified test-time adaptivity: our approach is sound and high-dimensional to flexibly adapt the computation of later steps conditioned on earlier steps by differential privacy theory. ", "page_idx": 2}, {"type": "text", "text": "Differential Privacy (DP) is a rigorous notion of privacy. A randomized mechanism $\\mathcal{M}$ is $(\\epsilon,\\delta)$ -DP if, for any neighbouring inputs $X$ and $X^{\\prime}$ , and any subset of possible outputs $\\mathcal{V}\\,\\subset\\,\\mathrm{Range}(\\mathcal{M})$ , $\\begin{array}{r}{\\mathbb{P}(\\mathcal{M}(\\boldsymbol{X})\\in\\mathcal{Y})\\le e^{\\epsilon}\\mathbb{P}(\\bar{\\mathcal{M}}(\\bar{\\boldsymbol{X}}^{\\prime})\\in\\mathcal{Y})+\\delta}\\end{array}$ . Following L\u00e9cuyer et al. (2019), we define neighbouring based on $L_{p}$ norms: $X$ and $X^{\\prime}$ in $\\mathbb{R}^{d}$ are $L_{p}$ neighbours at radius $r$ if $X-X^{\\prime}\\in B_{p}(r)$ . ", "page_idx": 2}, {"type": "text", "text": "RS was initially analyzed using $(\\epsilon,\\delta)$ -Differential Privacy (L\u00e9cuyer et al., 2019). Intuitively, the randomized classifier $\\mathcal{M}(X)\\triangleq g(X+z)$ , $z\\sim\\mathcal{N}(0,\\sigma^{2}\\mathbb{I}^{d})$ acts as a privacy preserving mechanism (the Gaussian mechanism) that provably \u201chides\u201d small variations in $X$ . This privacy guarantee yields a robustness certificate for the expected predictions. ", "page_idx": 2}, {"type": "text", "text": "$\\pmb{f}$ -DP (Dong et al., 2019) is a notion of privacy that extends $(\\epsilon,\\delta)$ -DP, and defines privacy as a bound on the power of hypothesis tests. Appendix A provides more details on $f$ -DP. The main result we leverage is Theorem 2.7 of Dong et al. (2022) that for a Gaussian mechanism $\\boldsymbol{\\mathcal{M}}(\\boldsymbol{X})=\\boldsymbol{\\theta}(\\boldsymbol{X})+\\boldsymbol{z}$ , $\\begin{array}{r}{z\\sim\\mathcal N(0,\\frac{r^{2}}{\\mu^{2}})}\\end{array}$ , such that for any neighbouring $X,X^{\\prime},\\theta(X)-\\theta(X^{\\prime})\\in B_{2}(r)$ (i.e., the $L_{2}$ sensitivity of $\\theta$ is $r$ ), we have that $\\mathcal{M}$ is $G_{\\mu}$ -DP with the function $G_{\\mu}$ defined as: ", "page_idx": 2}, {"type": "equation", "text": "$$\nG_{\\mu}(\\alpha)=\\Phi\\Big(\\Phi^{-1}(1-\\alpha)-\\mu\\Big),\\;\\mathrm{for}\\,\\alpha\\in[0,1].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "We leverage two key properties of $f$ -DP. First, $f{\\mathrm{-DP}}$ is resilient to post-processing. That is, if mechanism $\\mathcal{M}$ is $f$ -DP, proc $\\circ\\mathcal{M}$ is also $f$ -DP. Second, $f$ -DP is closed under adaptive composition. We refer to $\\S3$ in Dong et al. (2022) for the precise definition and use their Corollary 3.3: the adaptive composition of two Gaussian mechanisms $G_{\\mu_{1}}$ -DP and $G_{\\mu_{2}}$ -DP is itself $G_{\\mu}$ -DP with ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mu=\\sqrt{\\mu_{1}^{2}+\\mu_{2}^{2}}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "$f$ -DP is distinct from the $f$ -divergence from information theory. Dvijotham et al. (2020) use $f$ - divergence bounds between the noise distribution centred on the original input and centred on any perturbed input. This improves RS by broadening the noise distributions and norm-bounds on the adversary that RS can support. In contrast we focus on $f$ -DP, which captures enough information to reconstruct any divergence by post-processing (Proposition B.1. in Dong et al. (2019)). Our main objective is different: we leverage $f$ -DP composition properties to enable multi-step deep learning architectures that adapt to the input at test time with robustness guarantees. ", "page_idx": 2}, {"type": "text", "text": "2.2 Randomized Smoothing from $\\boldsymbol{\\textbf{\\textit{f}}}$ -DP ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We reconnect RS with DP, using $f$ -DP to yield results as strong as that of Equation (1). We start with a general robustness result on $f$ -DP classifiers, which we later build on for our main result. ", "page_idx": 2}, {"type": "text", "text": "Proposition 2.1 ( $f$ -DP Robustness). Let $\\mathcal{M}:\\mathcal{X}\\xrightarrow{}\\mathcal{Y}$ be $f$ -DP for ${\\cal{B}}_{p}(r)$ neighbourhoods, and let $M_{S}:X\\to\\arg\\operatorname*{max}_{y\\in{y}}\\mathbb{P}({M}(X)=y)$ be the associated smoothed classifier. Let $y_{+}\\triangleq M_{S}(X)$ be the prediction on input $X$ , and let $\\underline{{p_{+}}},\\overline{{p_{-}}}\\in[0,1]$ be such that $\\mathbb{P}(\\mathcal{M}(X)=y_{+})\\ge\\underline{{p_{+}}}\\ge\\overline{{p_{-}}}\\ge$ $\\operatorname*{max}_{y_{-}\\neq y_{+}}\\mathbb{P}(\\mathcal{M}(X)=y_{-})$ . Then: ", "page_idx": 2}, {"type": "equation", "text": "$$\nf(1-\\underline{{p_{+}}})\\geq1-f(\\overline{{p_{-}}})\\Rightarrow\\forall e\\in B_{p}(r),\\ M_{S}(X+e)=y_{+}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Proof. See Appendix B1. ", "page_idx": 2}, {"type": "text", "text": "Let us now instantiate Proposition 2.1 for Gaussian RS (see $\\S2.1)$ : ", "page_idx": 3}, {"type": "text", "text": "Corollary 2.2 (RS from $f$ -DP). Let ${\\mathcal{M}}:X\\to g(X+z)$ , $z\\,\\sim\\mathcal{N}(0,\\sigma^{2}\\mathbb{I}^{d})$ , and $M_{S}\\,:\\,X\\,\\rightarrow$ arg $\\mathrm{max}_{y\\in\\mathcal{Y}}\\operatorname{\\mathbb{P}}(\\mathcal{M}(X)\\;=\\;y)$ be the associated smooth model. Let $y_{+}~\\triangleq~M_{S}(X)$ be the prediction on input $X$ , and let $p_{+},\\overline{{p_{-}}}~\\in~[0,1]$ be such that $\\mathbb{P}(\\mathcal{M}(X)\\;=\\;y_{+})\\;\\geq\\;\\underline{{p}}_{+}\\;\\geq\\;\\overline{{p_{-}}}\\;\\geq$ $\\operatorname*{max}_{y_{-}\\neq y_{+}}\\mathbb{P}(\\mathcal{M}(X)=y_{-})$ . Then $\\forall e\\in B_{2}(r_{x})$ , $M_{S}(X+e)=y_{+}$ , with: ", "page_idx": 3}, {"type": "equation", "text": "$$\nr_{X}={\\frac{\\sigma}{2}}\\big(\\Phi^{-1}({\\underline{{p_{+}}}})-\\Phi^{-1}({\\overline{{p_{-}}}})\\big).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Proof. See Appendix B2. Sketch: $\\mathcal{M}$ is a Gaussian mechanism, and is $G_{\\frac{r}{\\alpha}}$ -DP for any $r$ $(B_{2}(r)$ neighbourhood). We apply Proposition 2.1 and maximize $r$ such that $G_{\\frac{r}{\\sigma}}(1\\!-\\!\\underline{{p_{+}}})\\geq1\\!-\\!G_{\\frac{r}{\\sigma}}(\\overline{{p_{-}}})$ . ", "page_idx": 3}, {"type": "text", "text": "2.3 Adaptive Randomized Smoothing ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "While Proposition 2.1 is new, so far we have only used it to reprove the known result of Corollary 2.2. So why is this connection between $f$ -DP and robustness useful? Our key insight is that we can leverage adaptive composition results at the core of DP algorithms to certify multi-step methods that adapt to their inputs at test time. Such adaptive defences have seen recent empirical interest, but either lack formal guarantees, or provide only limited adaptivity in practice (\u00a75). For the first time we derive a sound and high-dimensional adaptive method for certification. ", "page_idx": 3}, {"type": "text", "text": "We formalize adaptive multi-step certification as follows. Consider $k$ randomized Gaussian mechanisms $\\mathcal{M}_{1},\\ldots,\\mathcal{M}_{k}$ (our adaptive steps), such that $m_{i}\\sim\\mathcal{M}_{i}(X|m_{<i})$ , and for all $r\\geq0$ we have that $\\mathcal{M}_{i}$ is $G_{r/\\sigma_{i}}$ -DP for the ${\\cal{B}}_{2}(r)$ neighbouring definition. Note that the computation $\\mathcal{M}_{i}$ can depend on previous results, as long as it is $G_{r/\\sigma_{i}}$ -DP. Further consider a (potentially randomized) post-processing classifier $g(m_{1},\\Tilde{.}\\cdot.\\cdot,m_{k})=y\\in\\mathcal{Y}$ . ", "page_idx": 3}, {"type": "text", "text": "Theorem 2.3 (Main result: Adaptive RS). Using definitions above, let $\\mathcal{M}:X\\rightarrow g(m_{1},\\dots,m_{k})\\in$ $\\boldsymbol{\\wp}$ , $(m_{1},\\ldots,m_{k})\\sim({\\mathcal{M}}_{1}(X),\\bar{\\ldots}\\ldots,{\\mathcal{M}}_{k}(X|m_{<k}))$ , and the associated smoothed model be $M_{S}~:$ $X\\,\\to\\,\\arg\\operatorname*{max}_{m\\in{\\mathcal{Y}}}\\mathbb{P}(M(X)\\,=\\,y)$ . Let $y_{+}\\;\\triangleq\\;M_{S}(X)$ be the prediction on input $X$ , and let $p_{+},\\overline{{p_{-}}}\\in\\,[0,1]$ be such that $\\begin{array}{r}{\\mathbb{P}\\!\\left(\\mathcal{M}(X)=y_{+}\\right)\\geq\\underline{{p}}_{+}\\geq\\overline{{p_{-}}}\\geq\\operatorname*{max}_{y_{-}\\neq y_{+}}\\mathbb{P}\\!\\left(\\mathcal{M}(X)=y_{-}\\right)}\\end{array}$ . Then $\\forall e\\in B_{2}(r_{x})$ , $M_{S}(X+e)=y_{+}$ , with: ", "page_idx": 3}, {"type": "equation", "text": "$$\nr_{X}={\\frac{1}{2\\sqrt{\\sum_{i=1}^{k}{\\frac{1}{\\sigma_{i}^{2}}}}}}\\Big(\\Phi^{-1}(\\underline{{p_{+}}})-\\Phi^{-1}(\\overline{{p_{-}}})\\Big).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Proof. By adaptive composition of Gaussian DP mechanisms (Equation (3)), $\\mathcal{M}$ is $G_{\\mu}$ -DP with $\\begin{array}{r}{\\mu=\\sqrt{\\sum_{i=1}^{k}\\frac{r^{2}}{\\sigma_{i}^{2}}}=r\\sqrt{\\sum_{i=1}^{k}\\frac{1}{\\sigma_{i}^{2}}}}\\end{array}$ . We can then apply Corollary 2.2 with $\\begin{array}{r}{\\sigma=1/\\sqrt{\\sum_{i=1}^{k}\\frac{1}{\\sigma_{i}^{2}}}}\\end{array}$ . \u53e3 ", "page_idx": 3}, {"type": "text", "text": "We focus on Gaussian RS, but a similar argument applies to general $f$ -DP mechanisms for which we can compute $f_{i}$ at any $r$ , and the composition $f_{i}\\otimes\\cdot\\cdot\\otimes f_{k}$ , potentially using numerical techniques such as that of Gopi et al. (2021). For Gaussian noise, Theorem 2.3 leverages strong results from DP to provide a perhaps surprising result: there is no cost to adaptivity, in the sense that $k$ independent measurements of input $X$ with Gaussian noise (without adaptivity) of respective variance $\\sigma_{i}^{\\hat{2}}$ can be averaged to one measurement of variance $\\begin{array}{r}{\\sigma^{2}=1/\\sum_{i=1}^{k}{\\sigma_{i}^{-2}}}\\end{array}$ . To show this, we can use a weighted average to minimize variance (see e.g., Equation 4 in Honaker (2015)), with $c_{j}={\\sigma_{j}^{-2}}/\\sum_{i=1}^{k}{\\sigma_{i}^{-2}}$ yielding $\\begin{array}{r}{\\sigma^{2}\\;=\\;\\sum_{j=1}^{k}c_{j}^{2}\\sigma_{j}^{2}\\;=\\;\\sum_{j=1}^{k}\\sigma_{j}^{-2}\\big/\\big(\\sum_{i=1}^{k}\\sigma_{i}^{-2}\\big)^{2}\\;=\\;1\\big/\\sum_{i=1}^{k}\\sigma_{i}^{-2}}\\end{array}$ . The ARS $r_{X}$ from Theorem 2.3 is identical to that of one step RS from Corollary 2.2 using this variance: adaptivity over multi-step computation comes with no decrease in certified radius. ", "page_idx": 3}, {"type": "text", "text": "2.4 ARS against $L_{\\infty}$ -Bounded Adversaries ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "How can we leverage the multi-step adaptivity from Theorem 2.3 to increase certified accuracy? We focus on two-step certified defence against $L_{\\infty}$ -bounded attacks to increase accuracy by adaptivity. Previous work already notes that RS applies to $L_{\\infty}$ -bounded attackers (L\u00e9cuyer et al., 2019; Cohen ", "page_idx": 3}, {"type": "text", "text": "et al., 2019), usin\u221ag the fact that $\\forall X\\in\\mathbb{R}^{d}$ , $\\|X\\|_{2}\\leq{\\sqrt{d}}\\|X\\|_{\\infty}$ , and hence that $X\\!-\\!X^{\\prime}\\in B_{\\infty}(r^{\\infty})\\Rightarrow$ $X-X^{\\prime}\\in B_{2}(\\sqrt{d}\\cdot r^{\\infty})$ . Using Corollary 2.2, this yields: ", "page_idx": 4}, {"type": "equation", "text": "$$\nr_{X}^{\\infty}=\\frac{\\sigma}{2\\sqrt{d}}\\big(\\Phi^{-1}(\\underline{{{p_{+}}}})-\\Phi^{-1}(\\overline{{{p_{-}}}})\\big).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "While $L_{\\infty}$ -specific RS theory exists (Yang et al., 2020), further work by Blum et al. (2020) has found that Gaussian RS performs advantageo\u221ausly in practice. However, Blum et al. (2020); Kumar et al. (2020); Wu et al. (2021) show that the $\\sqrt{d}$ dependency cannot be avoided for a large family of distributions, leading the authors to speculate that RS might be inherently limited for $L_{\\infty}$ certification of predictions on high dimensional images. To side-step this issue, we use two-steps adaptivity to first select subsets of the image important to the classification task (thereby reducing dimension), and then make the prediction based on the selected subset. Formally: ", "page_idx": 4}, {"type": "text", "text": "Proposition 2.4 (Adaptive RS for $L_{\\infty}$ ). Define the following pair of (adaptive) mechanisms: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{M}_{1}:X\\to X+z_{1}\\triangleq m_{1},\\qquad z_{1}\\sim\\mathcal{N}(0,\\sigma_{1}^{2}\\mathbb{I}^{d})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Then, with any function $w:\\mathbb{R}^{d}\\rightarrow[0,1]^{d}$ (interpreted as a mask): ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{M}_{2}:(X,m_{1})\\to w(m_{1})\\odot X+z_{2}\\triangleq m_{2},\\qquad z_{2}\\sim\\mathcal{N}(0,\\frac{\\|w(m_{1})\\|_{2}^{2}}{d}\\sigma_{2}^{2}\\mathbb{I}^{d})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\odot$ is the element-wise product; and the final prediction function $g:m_{1},m_{2}\\rightarrow\\mathcal{Y}$ . ", "page_idx": 4}, {"type": "text", "text": "Consider the mechanism $\\mathcal{M}$ that samples $m_{1}\\sim\\mathcal{M}_{1}$ , then $m_{2}\\sim\\mathcal{M}_{2}$ , and finally outputs $g(m_{1},m_{2})$ ; and the associated smoothed classifier $M_{S}:X\\to\\arg\\operatorname*{max}_{y\\in{y}}\\mathbb{P}({\\mathcal{M}}(X)=y)$ . Let $y_{+}\\triangleq M_{S}(X)$ be the prediction on input $X$ , and let $p_{+},\\overline{{p_{-}}}\\in[0,1]$ be such that $\\mathbb{P}(\\mathcal{M}(X)=y_{+})\\ge\\underline{{p_{+}}}\\ge\\overline{{p_{-}}}\\ge$ $\\operatorname*{max}_{y_{-}\\neq y_{+}}\\mathbb{P}(\\mathcal{M}(X)=y_{-})$ . Then $\\forall e\\in B_{\\infty}(r_{X}^{\\infty})$ , $M_{S}(X+e)=y_{+}$ , with: ", "page_idx": 4}, {"type": "equation", "text": "$$\nr_{X}^{\\infty}=\\frac{1}{2\\sqrt{d\\big(\\frac{1}{\\sigma_{1}^{2}}+\\frac{1}{\\sigma_{2}^{2}}\\big)}}\\Big(\\Phi^{-1}(\\underline{{{p_{+}}}})-\\Phi^{-1}(\\overline{{{p_{-}}}})\\Big).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "P\u221aroof. Consider any $X,X^{\\prime}$ s.t. $X-X^{\\prime}\\in B_{\\infty}(r^{\\infty})$ . We analyze $\\mathcal{M}_{1}$ and $\\mathcal{M}_{2}$ in\u221a turn. $\\|X-X^{\\prime}\\|_{2}\\leq$ ${\\sqrt{d}}\\|X-X^{\\prime}\\|_{\\infty}$ , so $X-X^{\\prime}\\in B_{2}(\\sqrt{d}r^{\\infty})$ , and $\\mathcal{M}_{1}$ is $G_{\\mu_{1}}$ -DP with $\\begin{array}{r}{\\mu_{1}=\\frac{r^{\\infty}\\sqrt{d}}{\\sigma_{1}}}\\end{array}$ . \u03c31 ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|w(m_{1})\\odot X-w(m_{1})\\odot X^{\\prime}\\|_{2}=\\|w(y_{1})\\odot\\left(X-X^{\\prime}\\right)\\|_{2}\\leq\\|w(y_{1})\\|_{2}\\|X-X^{\\prime}\\|_{\\infty}\\,\\mathrm{so}}\\\\ &{B_{2}(\\|w(y_{1})\\|_{2}r^{\\infty})\\,\\mathrm{and}\\,\\mathcal{M}_{2}\\,\\mathrm{is}\\,G_{\\mu_{2}}\\mathrm{-DP}\\,\\mathrm{with}\\,\\mu_{2}=\\frac{\\|w(y_{1})\\|_{2}r^{\\infty}}{\\|w(y_{1})\\|_{2}\\sigma_{2}/\\sqrt{d}}=\\frac{r^{\\infty}\\sqrt{d}}{\\sigma_{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Applying Theorem 2.3 with $\\begin{array}{r}{\\sqrt{\\frac{(r^{\\infty})^{2}d}{\\sigma_{1}^{2}}+\\frac{(r^{\\infty})^{2}d}{\\sigma_{2}^{2}}}=r^{\\infty}\\sqrt{d\\big(\\frac{1}{\\sigma_{1}^{2}}+\\frac{1}{\\sigma_{2}^{2}}\\big)}}\\end{array}$ concludes the proof. ", "page_idx": 4}, {"type": "text", "text": "Important remarks. 1. $w(.)$ is a masking function, adaptively reducing (if $w_{i}(m_{1})\\ll1;$ the value of $X_{i}$ and thereby the attack surface of an $L_{\\infty}$ attacker. This reduces the effective dimension of the input to $\\mathcal{M}_{2}$ . 2. Reducing the dimension enables a reduction in the noise variance in $\\mathcal{M}_{2}$ , at fixed privacy guarantee $G_{\\mu_{2}}$ . The variance reduction is enabled for all dimensions in the input, even those that are not masked $\\bar{(w_{i}(m_{1})\\approx1)}$ . As a result, the variance of the noise in $\\mathcal{M}_{2}$ scales as $\\|\\boldsymbol{w}(\\boldsymbol{m}_{1})\\|_{2}^{2}\\leq d$ . The more masking, the lower the variance. It may help to consider the change of variables $\\sigma\\gets\\sigma/\\sqrt{d}$ in Equation (4), and $\\sigma_{1,2}\\gets\\sigma_{1,2}/\\sqrt{d}$ in Proposition 2.4, to remove $d$ from $r_{X}^{\\infty}$ and scale the noise variance with $d$ . For RS (Equation (4)), the noise variance scales as $d$ . For ARS, only Equation (5) (the first step) suffers from variance scaled by $d$ , while the second step\u2019s variance (Equation (6)) scales as $||w(\\dot{m}_{1})||_{2}^{2}$ , which can be much smaller than $d$ when a large part of the image is masked. Reduced variance translates into higher accuracy, as well as $p_{\\pm}$ and $\\overline{{p_{-}}}$ being further apart, for a larger $r_{X}^{\\infty}$ . 3. The variance reduction due to masking applies in the translation from the $B_{\\infty}(r^{\\infty})$ bound on the attack to the ${\\cal{B}}_{2}(r)$ sensitivity used in ARS. This variance reduction would not apply to an $L_{2}$ -bounded adversary (an attack that only changes pixels with mask values of one yields no sensitivity improvement). Hence, our two-steps ARS architecture for $L_{\\infty}$ -bounded adversaries does not reduce to bounding $L_{\\infty}$ with $L_{2}$ as the traditional RS application does, and our gains come explicitly from variance reduction enabled by adaptive masking against an $L_{\\infty}$ attack. ", "page_idx": 4}, {"type": "text", "text": "3 Two-Step ARS for $L_{\\infty}$ Certification of Image Classification ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Figure 1 shows our deep learning architecture based on Proposition 2.4. The first step, $\\mathcal{M}_{1}$ , adds noise to input $X$ and post-processes the result into a mask $w(m_{1})$ . The second step, $\\mathcal{M}_{2}$ , takes masked input $w(m_{1})\\odot X$ and adds noise. Finally, the base classifier $g$ post-processes a weighted average of $m_{1},m_{2}$ to output a label. The whole model is trained end-to-end on the classification task. In RS, only the path going through $\\mathcal{M}_{2}$ is present. This is equivalent to setting $\\sigma_{2}=\\sigma$ and $w(.)=1$ , with no $\\mathcal{M}_{1}$ . In both cases, the final predictions are averaged over the noise to create the smoothed classifier. The ARS architecture introduces several new components, which we next describe. ", "page_idx": 5}, {"type": "text", "text": "Budget Splitting: the noise budget $\\sigma$ (Figure 1; red) is split to assign noise levels to the two steps $\\mathcal{M}_{1}$ and $\\mathcal{M}_{2}$ . We parameterize ARS with the same $\\sigma$ as standard RS then split it by the $f$ -DP composition formula from Equation (3). In practice, we assign $\\sigma_{1}\\geq\\sigma$ to $\\mathcal{M}_{1}$ , and then $\\begin{array}{r}{\\sigma_{2}=1/\\sqrt{\\frac{1}{\\sigma^{2}}-\\frac{1}{\\sigma_{1}^{2}}}}\\end{array}$ . We set $\\sigma_{1}$ by either fixing it to a constant or learning it end-to-end. ", "page_idx": 5}, {"type": "text", "text": "Masking: the mask model $w(\\cdot)$ takes the noisy image from $\\mathcal{M}_{1}$ and predicts a weighting (one value in $[0,1]$ per input pixel) that is multiplied with the input element-wise (denoted $\\odot$ in Proposition 2.4). The model is a U-Net architecture, which makes pixel-wise predictions, and acts as a post-processing of $\\mathcal{M}_{1}$ in the $f$ -DP analysis. Our masking enables test-time adaptivity to reduce the noise variance for $\\mathcal{M}_{2}$ , via the mask\u2019s dependence on the input through $m_{1}$ . ", "page_idx": 5}, {"type": "text", "text": "Mechanism output averaging: to fully leverage both steps\u2019 information, we take a weighted average of the outputs $m_{1}$ and $m_{2}$ before passing the result to the base classifier $g$ . For a particular input pixel $i$ , denote $X_{i}$ the value of pixel, $w_{i}\\in[0,1]$ its mask weight (we omit the explicit dependency on $m_{1}$ in $w$ for compactness), and $m_{1,i},m_{2,i}$ the respective values output by $\\mathcal{M}_{1}$ and $\\bar{\\mathcal{M}_{2}}$ . Then, the final value of pixel $i$ in the averaged input will be $\\hat{X}_{i}\\triangleq c_{1,i}m_{1,i}+c_{2,i}m_{2,i}$ . ", "page_idx": 5}, {"type": "text", "text": "We set $c_{1,i},c_{2,i}$ such that $\\hat{X_{i}}$ is the unbiased estimate of $X_{i}$ with smallest variance. First, we set $c_{1,i}+w_{i}c_{2,i}=1$ , such that $\\mathbb{E}[\\hat{X}_{i}]=c_{1,i}X_{i}+c_{2,i}w_{i}X_{i}=X_{i}$ . Second, we minimize the variance. Notice that $\\mathbb{V}[\\hat{X}_{i}]=c_{1,i}^{2}\\sigma_{1}^{2}+c_{2,i}^{2}\\|w\\|_{2}^{2}\\sigma_{2}^{2}=(1-w_{i}c_{2,i})^{2}\\sigma_{1}^{2}+c_{2,i}^{2}\\|w\\|_{2}^{2}\\sigma_{2}^{2}$ : this is a convex function in $c_{2,i}$ minimized when its gradient in $c_{2,i}$ is zero. Plugging back into the constraint to get $c_{1,i}$ , we obtain the following weights: $\\begin{array}{r}{c_{1,i}=\\frac{\\Vert w\\Vert_{2}^{2}\\sigma_{2}^{2}}{\\sigma_{1}^{2}w_{i}^{2}+\\Vert w\\Vert_{2}^{2}\\sigma_{2}^{2}}}\\end{array}$ , and $\\begin{array}{r}{c_{2,i}=\\frac{\\sigma_{1}^{2}w_{i}}{\\sigma_{1}^{2}w_{i}^{2}+\\|w\\|_{2}^{2}\\sigma_{2}^{2}}}\\end{array}$ . ", "page_idx": 5}, {"type": "text", "text": "The averaged noisy input X\u02c6 is finally fed to the base classifier $g$ for prediction. The smoothed classifier $M_{S}$ averages predictions (over noise draws) over the entire pipeline. The parameters of $w$ and $g$ (and $\\sigma_{1}$ if not fixed) are learned during training and are fixed at inference/certification time. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We evaluate on standard and $L_{\\infty}$ -certified test accuracy. Certified accuracy at radius $r^{\\infty}$ is the percentage of test samples that are correctly classified and have an $L_{\\infty}$ certificate radius $r_{X}^{\\infty}\\geq r^{\\infty}$ . Standard accuracy is obtained for $r^{\\infty}=0$ . ", "page_idx": 5}, {"type": "text", "text": "Datasets We evaluate on CIFAR-10 (Krizhevsky, 2009) in $\\S4.1$ , CelebA (Liu et al., 2015) (specifically the unaligned HD-CelebA-Cropper edition) in $\\S4.2$ , and ImageNet (Deng et al., 2009) in $\\S4.3$ . We measure adaptivity on CIFAR-10 and CelebA by designing challenging benchmarks requiring adaptivity, and measure scalability on ImageNet. ", "page_idx": 5}, {"type": "text", "text": "Models We choose the standard ResNet (He et al., 2016) models as base classifiers $g$ with ResNet-110 for CIFAR-10 and ResNet-50 for CelebA and ImageNet. For ARS, our mask model $w$ is a simplified U-Net (Ronneberger et al., 2015) (see Appendix C.1 for details). For the noise budget, \u221awe find that a fixed budget split performs reliably, and so in all experiments we split by $\\sigma_{1}=\\sigma_{2}=\\sqrt{2}\\sigma$ . ", "page_idx": 5}, {"type": "text", "text": "Methods We compare to standard and strong static methods, design a baseline specifically for our masking approach, and evaluate the only sound input-dependent methods prior to ARS. Cohen et al. is the standard approach to RS (Cohen et al., 2019). UniCR (Hong et al., 2022) learns the noise distribution for RS during training but is static during testing (while they propose an input-adaptive variant, it is not sound so we restrict our comparison to the training variant). We tune hyper-parameters, and perform a grid search for $\\beta$ (the parameter of the noise distribution) to maximize certified accuracy. We find that $\\beta\\,=\\,2$ (Gaussian), or $\\beta\\,=\\,2.25$ (close to a Gaussian, with smaller tails), to perform best in high and low $(k,\\sigma)$ values, respectively (see Appendix D) for details. Static Mask is our baseline that learns a fixed mask during training that does not adapt during testing. The mask is directly parameterized as pixel-wise weights that we multiply with the input and optimize jointly with the base classifier. Relative to ARS in Figure 1, this removes $\\mathcal{M}_{1}$ , sets $\\sigma_{2}=\\sigma$ , and makes $w(.)=W$ static parameters rather than an adaptive prediction. S\u00faken\u00edk et al. (2021) conditions the variance $\\sigma$ for RS on the input and is therefore test-time adaptive. We use code provided by the authors as is. Comparing ARS to the static baselines measures the value of test-time adaptivity, and comparing ARS to the variance adaptivity of S\u00faken\u00edk et al. measures the importance of more high-dimensional and expressive adaptation. ", "page_idx": 5}, {"type": "table", "img_path": "MN4nt01TeO/tmp/df0b9083bdfc1e4c177c2e7ba2a9e295a0079c99541838f8b47ba42a10f9aca5.jpg", "table_caption": ["Table 1: Standard Accuracy $(r\\,=\\,0)$ ) on CIFAR-10 (20kBG). Our 20kBG benchmark places CIFAR-10 images on larger background images. We report the mean accuracy and standard deviation over three seeds. ARS achieves higher accuracy across noise $\\sigma$ and input dimension $k$ (\u2206indicates adaptivity). We provide results with more $\\sigma$ levels in Appendix D. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "4.1 CIFAR-10 Benchmark: Classification with Distractor Backgrounds ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Input dimension is a key challenge in $L_{\\infty}$ certification using RS (see $\\S2.4)$ . We design our 20kBG benchmark to vary this parameter without affecting the task: we superimpose CIFAR-10 images onto a larger distractor background from the $20k$ background images dataset Li et al. (2022a). The backgrounds are split into train and test sets, and resized to $k\\times k$ pixels (where $k\\,\\geq\\,32$ ). The CIFAR-10 image (of dimensions $32\\times32\\times3)$ ) is then placed at random along the edges of the background image to maximize spatial variation. The spurious background increases the dimension $(=k\\times k\\times3)$ of the input when $k>32$ , making $L_{\\infty}$ certification with RS more challenging, but is uninformative for the task of CIFAR-10 by construction. Our mask model $\\left(\\mathcal{M}_{1}\\right)$ needs to learn to ignore the background to reduce the effective dimension of the input. For computational reasons, we run all certification results on a 200 sample subset of the CIFAR-10 test set. Appendix D shows extended results, details about hyperparameter tuning (C.3), and results on larger test-sets (D.1). We set the failure probability of the certification procedure to 0.05, use 100 samples to select the most probable class, and 50,000 samples for the Monte Carlo estimate of $\\underline{{p_{+}}}$ . ", "page_idx": 6}, {"type": "text", "text": "Table 1 summarizes the standard accuracy $(r=0$ ) of each approach on the different settings. We vary $\\sigma$ in $\\{0.25,0.5,1.0\\}$ and $k$ in $\\{32,48,64,96\\}$ to show the effect of noise levels and dimensionality on the accuracy of different approaches ( $k=32$ corresponds to original CIFAR-10 images). Figure 2 shows the certified test accuracy at different radii $r^{\\infty}$ for ARS and all baselines we consider. ", "page_idx": 6}, {"type": "text", "text": "We make three observations. First, ARS outperforms all the baselines under distractor backgrounds $[k>32)$ . Static mask is slightly better at $k=32$ , probably because CIFAR-10 images lack enough \"irrelevant\" information for ARS to discard. This is precisely why we introduce this benchmark, where we explicitly add such irrelevant information to input images. Indeed, at $k>32$ , we observe that the standard accuracy of ARS improves, translating to an improved certified accuracy at all certification levels, since more accurate and confident predictions give a larger certified radius $r^{\\infty}$ . ", "page_idx": 6}, {"type": "text", "text": "Second, as we grow the input dimension $k$ , the accuracy of ARS remains either stable or increases, whereas that of other baselines goes down, resulting in an increasing gap. For instance, at $\\sigma=1.0$ , the gap between ARS and the best baseline is $1.3\\%$ points for $k\\,=\\,32$ , $2.5\\%$ points for $k\\,=\\,48$ , ", "page_idx": 6}, {"type": "image", "img_path": "MN4nt01TeO/tmp/72070ee53cfe07dd9e6408f263bd649b3a14df5f8d829495b1f67ddbaf24c44a.jpg", "img_caption": ["Figure 2: Certified Test Accuracy on CIFAR-10 $(20\\mathbf{k}\\mathbf{B}\\mathbf{G})$ ). (a)-(c) show the effect of dimensionality for (a) no background $/\\,k=32$ , (b) $k=48$ , and (c) $k=96$ for constant $\\sigma=0.5$ . (d)-(f) show the effect of noise for (d) $\\sigma=0.25$ , (e) $\\sigma=0.5$ and (f) $\\sigma=1.0$ with dimensionality fixed to $k=64$ . Each line is the mean and the shaded interval covers $+/-$ one standard deviation across seeds. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "MN4nt01TeO/tmp/71dac38de9e4e9775e2aaecd5df5c589aff2fabff75e3f536cc2e6e110c96fd8.jpg", "img_caption": ["Figure 3: (left) Original CIFAR-10 images superimposed on backgrounds for different $k$ (except $k=32$ which is no background), and (right) their corresponding masks (grayscale) inferred by our mask model $w$ . All masks are for $\\sigma=0.5$ . Appendix D.2 shows all the corresponding images across our multi-step architecture. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "$8.2\\%$ points for $k=64$ and $15\\%$ points for $k=96$ . As $k$ grows, the amount of relevant information (a $32\\times32\\times3$ CIFAR-10 image) remains the same, whereas the amount of spurious background information increases. ARS\u2019 mask is able to rule out spurious pixels, reducing the noise in the second step (Figure 3). Thanks to this masking, ARS is much less sensitive to increases in dimensionality. ", "page_idx": 7}, {"type": "text", "text": "Third, we observe that except $k=32$ , ARS improves over all the baselines in low $\\sigma=0.25)$ ) to high $\\sigma=1.0)$ ) noise regimes. In fact, this trend continues to persist in higher noise regimes (Appendix D). Similar to previous observations, we notice that as we increase $\\sigma$ , other baselines\u2019s accuracy drops significantly whereas ARS accuracy drops much less, displaying higher noise tolerance. ", "page_idx": 7}, {"type": "text", "text": "ARS training and inference requires additional computation. To certify a single input $(k=32)$ ), Cohen et al. (2019) takes ${\\sim}12$ seconds while ARS takes ${\\sim}26$ seconds (as measured on an NVIDIA A100 80Gb GPU). This $2\\times$ overhead does however yield improved certified accuracy. ", "page_idx": 7}, {"type": "text", "text": "4.2 CelebA Benchmark: Classification Without Spatial Alignment ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To evaluate ARS on a more realistic task with natural spatial variation, we use the CelebA face dataset in its unaligned version. We focus on the \u201cmouth slightly open\u201d (label 21) binary classification task because mouth location and shape vary. The input part relevant to this task is likely well-localized, which affords an opportunity for the mask model to reduce the effective input dimension. The dataset consists of images with varied resolution, and meta-data about the position of different features, including the mouth. To create a challenging benchmark, we randomly crop all images to $160\\times160$ pixels, which creates spatial variation in the mouth\u2019s position. The only crop constraint is that the mouth is $\\ge~10$ pixels from the edge to ensure sufficient input to solve the task. Figure 5 shows example images from the test set, their respective masks from ARS, and the baseline static mask. ", "page_idx": 7}, {"type": "table", "img_path": "MN4nt01TeO/tmp/21871557a7d3c006810c700234918b793c634a3d01608f9a5de5b071ab819ea1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "MN4nt01TeO/tmp/e4c913d2be6b235e1f7e4225c877827febf86bec2522a4665228a66c3845670d.jpg", "img_caption": ["Table 2: Standard test accuracy $\\left.r\\right.=0$ ) on CelebA (unaligned and cropped). ARS is equal or better. Adaptivity handles the higher spatial dimensions $\\left(160\\times160\\right)$ and variation of these inputs. ", "Figure 4: Certified test accuracy on CelebA (unaligned and cropped). We evaluate static methods and ARS to measure the value of adaptivity. Each line is the mean and the shading covers $\\pm1$ standard deviation across three seeds. Adaptivity helps at all noise levels. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Figure 4 shows the certified accuracy for ARS, Cohen et al. (2019), and static mask, for three levels of the noise $\\sigma$ . First, both baselines perform very similarly. We can see from Figure 5 that the static mask is approximately identity (notice the $\\geq0.99$ scale), with only very slight dimming on the edges. This is because the mouth is not centred in our benchmark, so there is no one-size-fits-all mask. Second, ARS is able to predict a sparse mask that focuses on ", "page_idx": 8}, {"type": "image", "img_path": "MN4nt01TeO/tmp/da3e2752a846958088f56b7b3f600c9148896a57abba2563866849ae4c7e4856.jpg", "img_caption": ["Figure 5: ARS masks are localized and input specific. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "areas likely to have the mouth. The mask adapts to each input at test time, which is what enables the sparsity without performance degradation. Third, this sparse mask leads to a large noise reduction, enabling ARS to drastically improve both standard and certified accuracy. For instance, with $\\sigma=0.5$ , ARS improves the standard accuracy from $91.0\\%$ to $94.3\\%$ (a 3.3 point improvement), while the certified accuracy at $r^{\\infty}=0.004$ jumps from $45.0\\%$ to $79.3\\%$ (a more than 30 point improvement!). At lower noise $\\sigma=0.25)$ there is still an increase in standard accuracy from $94.\\bar{3}\\%$ to $9\\bar{7}.0\\%$ , and an increase in certified accuracy from $68.3\\%$ to $84.7\\%$ at $r^{\\infty}=0.002)$ ). At larger noise $\\sigma=1.0$ ), ARS sees significant increases ( $7.7\\%$ points in standard accuracy, and from $21.3\\%$ to $49.3\\%$ in certified accuracy at $r^{\\infty}=0.008)$ ). ", "page_idx": 8}, {"type": "text", "text": "4.3 ImageNet Benchmark: Classification on the Standard Large-Scale Dataset ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To evaluate the scalability of ARS we experiment on ImageNet (without any modification) with $\\sigma=0.25,0.5,1.0$ . For each noise level, we compare with Cohen et al. (2019), which we reproduce for this large-scale setting. We evaluate two versions of ARS: our regular setting (End-To-End); and a version that fixes the base classifier to the model trained as in Cohen et al. (2019), and only trains our mask model for 10 epochs (Pretrain). The certified accuracy is plotted in Figure 6 and the standard accuracy is reported in Table 3. ", "page_idx": 8}, {"type": "text", "text": "When only training the mask model, certified accuracy remains close to that of Cohen et al. (2019) at all radii and noise levels. ARS trained end-to-end improves both standard and certified accuracy. ", "page_idx": 8}, {"type": "table", "img_path": "MN4nt01TeO/tmp/db3290df11aba1d34c8d8addd00dab7675abb91bfcb74650cc03855c61ca670a.jpg", "table_caption": [], "table_footnote": ["Table 3: Standard test accuracy $(r=0$ ) on ImageNet. ARS maintains standard accuracy. "], "page_idx": 9}, {"type": "text", "text": "The standard accuracy (at $r=0$ ) increases from $57.2\\%$ to $57.4\\%$ and from $43.6\\%$ to $44.5\\%$ when $\\sigma=0.5$ and $\\sigma=1$ , respectively. For $\\sigma=0.25$ , standard accuracy for ARS is close but slightly lower than Cohen et al. (2019), while the pretrained ARS outperforms Cohen et al. (2019) from $66.5\\%$ to $67.4\\%$ . Appendix F discusses other ARS improvements without certification. ", "page_idx": 9}, {"type": "text", "text": "Certified accuracy increases at larger $\\sigma$ . For instance at $\\sigma=0.5$ , ARS improves certified accuracy at $r^{\\infty}=0.001$ from $48.9\\%$ to $50.5\\%$ . At larger noise $\\sigma=1.0$ , ARS improves certified accuracy at $r^{\\infty}=0.005$ from $21.7\\%$ to $23.1\\%$ . This shows that ARS\u2019 adaptivity generalizes outside of the specialized benchmarks we designed, and can scale to large datasets and complex classification tasks. ", "page_idx": 9}, {"type": "image", "img_path": "MN4nt01TeO/tmp/d116410bde994b873ea39de653c364de75bab03a77fa23d8e19a2974e3cb9c9a.jpg", "img_caption": ["Figure 6: Certified test accuracy on ImageNet for $\\sigma=0.25$ , 0.5, 1. We plot the mean and the shading covers $\\pm1$ standard deviation for three seeds. ARS is equal or better than non-adaptive RS (Cohen et al.) at large scale. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "5 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Limitations: The multi-step adaptivity of ARS improves certification at the cost of increased model size and computation for RS. This impacts both training and testing computation, and is especially costly in the context of RS due to the Monte Carlo estimation of the model\u2019s expected predictions (over several forward passes at inference time). While we empirically show improvement by ARS, it would be interesting and important to investigate how it combines with other RS improvements such as adversarial training (Salman et al., 2019), consistency regularization (Jeong and Shin, 2020), higher order certification (Mohapatra et al., 2020), double sampling (Li et al., 2022b), and denoising by diffusion (Carlini et al., 2022). Lastly, our adaptive masking technique provides improved certificates for the $L_{\\infty}$ norm, but does not have the same effect for other norms such as $L_{2}$ (see remarks in $\\S2.4)$ . It is plausible that ARS\u2019 adaptivity can lead to improvements under alternative norms, by leveraging different DP mechanisms and updates. We leave this exploration for future work. ", "page_idx": 9}, {"type": "text", "text": "Implications: Revisiting heuristic adaptive defences (as surveyed in Croce et al. (2022)) through the lens of ARS could help improve the empirical performance of provable defences. ARS may require extensions, but could eventually enable the formal analysis of input purification (e.g., Song and Kim (2018); Nie et al. (2022); Yoon et al. (2021)), or leverage DP-SGD (Abadi et al., 2016) to analyze defences that update by test-time optimization (Alfarra et al., 2022b; Hwang et al., 2022; Mao et al., 2021). Going further, one could leverage the vast DP literature to extend ARS, enabling fully-adaptive variance defences inspired by Alfarra et al. (2022a) by leveraging privacy odometers (Rogers et al., 2016; L\u00e9cuyer, 2021; Whitehouse et al., 2023). ", "page_idx": 9}, {"type": "text", "text": "To conclude: we introduced Adaptive Randomized Smoothing (ARS) to reconnect RS with DP theory, to propose a new two-step defence for deep image classification, and to rigorously analyze such adaptive defences that condition on inputs at test time. This framework opens promising avenues for designing models that are adaptively and soundly robust with provable guarantees about their updates on natural and adversarial inputs. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We thank Krishnamurthy Dvijotham and Ian Goodfellow for reviewing and providing feedback during the Google DeepMind publication process. We thank Motasem Alfarra for discussing variations on input-dependent variance for randomized smoothing. We are grateful for the support of the Natural Sciences and Engineering Research Council of Canada (NSERC) [reference number RGPIN-2022- 04469], as well as a Google Research Scholar award. This research was enabled by computational support provided by the Digital Research Alliance of Canada (alliancecan.ca). ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC conference on computer and communications security, 2016.   \nMotasem Alfarra, Adel Bibi, Philip HS Torr, and Bernard Ghanem. Data dependent randomized smoothing. In Uncertainty in Artificial Intelligence, pages 64\u201374. PMLR, 2022a.   \nMotasem Alfarra, Juan C. Perez, Ali Thabet, Adel Bibi, Philip H.S. Torr, and Bernard Ghanem. Combating adversaries with anti-adversaries. Proceedings of the AAAI Conference on Artificial Intelligence, 2022b.   \nAnish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. In International conference on machine learning, 2018.   \nAvrim Blum, Travis Dick, Naren Manoj, and Hongyang Zhang. Random smoothing might be unable to certify $\\ell_{\\infty}$ robustness for high-dimensional images. The Journal of Machine Learning Research, 2020.   \nYulong Cao, Ningfei Wang, Chaowei Xiao, Dawei Yang, Jin Fang, Ruigang Yang, Qi Alfred Chen, Mingyan Liu, and Bo Li. Invisible for both camera and lidar: Security of multi-sensor fusion based perception in autonomous driving under physical-world attacks. In 2021 IEEE Symposium on Security and Privacy (SP), 2021.   \nNicholas Carlini, Florian Tramer, Krishnamurthy Dj Dvijotham, Leslie Rice, Mingjie Sun, and J Zico Kolter. (certified!!) adversarial robustness for free! In The Eleventh International Conference on Learning Representations, 2022.   \nAnirban Chakraborty, Manaar Alam, Vishal Dey, Anupam Chattopadhyay, and Debdeep Mukhopadhyay. Adversarial attacks and defences: A survey. arXiv preprint arXiv:1810.00069, 2018.   \nJeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized smoothing. In international conference on machine learning, pages 1310\u20131320. PMLR, 2019.   \nJoana C Costa, Tiago Roxo, Hugo Proen\u00e7a, and Pedro RM In\u00e1cio. How deep learning sees the world: A survey on adversarial attacks & defenses. arXiv preprint arXiv:2305.10862, 2023.   \nFrancesco Croce, Sven Gowal, Thomas Brunner, Evan Shelhamer, Matthias Hein, and Taylan Cemgil. Evaluating the adversarial robustness of adaptive test-time defenses. In International Conference on Machine Learning, pages 4421\u20134435. PMLR, 2022.   \nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition. Ieee, 2009.   \nJinshuo Dong, Aaron Roth, and Weijie J Su. Gaussian differential privacy. arXiv preprint arXiv:1905.02383, 2019.   \nJinshuo Dong, Aaron Roth, and Weijie J Su. Gaussian differential privacy. Journal of the Royal Statistical Society Series B: Statistical Methodology, 2022.   \nKrishnamurthy (Dj) Dvijotham, Jamie Hayes, Borja Balle, Zico Kolter, Chongli Qin, Andras Gyorgy, Kai Xiao, Sven Gowal, and Pushmeet Kohli. A framework for robustness certification of smoothed classifiers using f-divergences. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id $=$ SJlKrkSFPH.   \nSivakanth Gopi, Yin Tat Lee, and Lukas Wutschitz. Numerical composition of differential privacy. Advances in Neural Information Processing Systems, 2021.   \nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, 2016.   \nJames Honaker. Efficient use of differentially private binary trees. Theory and Practice of Differential Privacy (TPDP 2015), London, UK, 2015.   \nHanbin Hong, Binghui Wang, and Yuan Hong. Unicr: Universally approximated certified robustness via randomized smoothing. In European Conference on Computer Vision. Springer, 2022.   \nDuhun Hwang, Eunjung Lee, and Wonjong Rhee. Aid-purifier: A light auxiliary network for boosting adversarial defense. In 2022 26th International Conference on Pattern Recognition (ICPR), 2022.   \nJongheon Jeong and Jinwoo Shin. Consistency regularization for certified robustness of smoothed classifiers. Advances in Neural Information Processing Systems, 2020.   \nAlex Krizhevsky. Learning multiple layers of features from tiny images, 2009. URL https: //api.semanticscholar.org/CorpusID:18268744.   \nAounon Kumar, Alexander Levine, Tom Goldstein, and Soheil Feizi. Curse of dimensionality on randomized smoothing for certifiable robustness. In International Conference on Machine Learning. PMLR, 2020.   \nMathias L\u00e9cuyer. Practical privacy fliters and odometers with r\u00e9nyi differential privacy and applications to differentially private deep learning. arXiv preprint arXiv:2103.01379, 2021.   \nMathias L\u00e9cuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman Jana. Certified robustness to adversarial examples with differential privacy. In 2019 IEEE symposium on security and privacy $(S P)$ , pages 656\u2013672. IEEE, 2019.   \nJizhizi Li, Jing Zhang, Stephen J Maybank, and Dacheng Tao. Bridging composite and real: towards end-to-end deep image matting. International Journal of Computer Vision, 2022a.   \nLinyi Li, Jiawei Zhang, Tao Xie, and Bo Li. Double sampling randomized smoothing. In International Conference on Machine Learning. PMLR, 2022b.   \nLinyi Li, Tao Xie, and Bo Li. Sok: Certified robustness for deep neural networks. In 2023 IEEE symposium on security and privacy (SP), 2023.   \nZiwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In Proceedings of International Conference on Computer Vision (ICCV), December 2015.   \nC. Mao, M. Chiquier, H. Wang, J. Yang, and C. Vondrick. Adversarial attacks are reversible with natural supervision. In 2021 IEEE/CVF International Conference on Computer Vision (ICCV), 2021.   \nJeet Mohapatra, Ching-Yun Ko, Tsui-Wei Weng, Pin-Yu Chen, Sijia Liu, and Luca Daniel. Higherorder certification for randomized smoothing. Advances in Neural Information Processing Systems, 2020.   \nWeili Nie, Brandon Guo, Yujia Huang, Chaowei Xiao, Arash Vahdat, and Animashree Anandkumar. Diffusion models for adversarial purification. In International Conference on Machine Learning, 2022.   \nApapan Pumsirirat and Yan Liu. Credit card fraud detection using deep learning based on autoencoder and restricted boltzmann machine. International Journal of advanced computer science and applications, 2018. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Ryan M Rogers, Aaron Roth, Jonathan Ullman, and Salil Vadhan. Privacy odometers and filters: Pay-as-you-go composition. Advances in Neural Information Processing Systems, 2016. ", "page_idx": 12}, {"type": "text", "text": "Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted intervention\u2013MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18. Springer, 2015.   \nHadi Salman, Jerry Li, Ilya Razenshteyn, Pengchuan Zhang, Huan Zhang, Sebastien Bubeck, and Greg Yang. Provably robust deep learning via adversarially trained smoothed classifiers. Advances in neural information processing systems, 2019.   \nYang Song and Taesup Kim. Sebastian nowozin stefano ermon nate kushman. pixeldefend: Leveraging generative models to understand and defend against adversarial examples. In International Conference on Learning Representations, 2018.   \nPeter S\u00faken\u00edk, Aleksei Kuvshinov, and Stephan G\u00fcnnemann. Intriguing properties of input-dependent randomized smoothing. arXiv preprint arXiv:2110.05365, 2021.   \nChristian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In International Conference on Learning Representations, 2014. URL http://arxiv.org/abs/1312.6199.   \nFlorian Tramer, Nicholas Carlini, Wieland Brendel, and Aleksander Madry. On adaptive attacks to adversarial example defenses. Advances in neural information processing systems, 33:1633\u20131645, 2020.   \nJustin Whitehouse, Aaditya Ramdas, Ryan Rogers, and Steven Wu. Fully-adaptive composition in differential privacy. In International Conference on Machine Learning, 2023.   \nYihan Wu, Aleksandar Bojchevski, Aleksei Kuvshinov, and Stephan G\u00fcnnemann. Completing the picture: Randomized smoothing suffers from the curse of dimensionality for a large family of distributions. In Proceedings of The 24th International Conference on Artificial Intelligence and Statistics, Proceedings of Machine Learning Research. PMLR, 2021. URL https://proceedings.mlr.press/v130/wu21d.html.   \nGreg Yang, Tony Duan, J Edward Hu, Hadi Salman, Ilya Razenshteyn, and Jerry Li. Randomized smoothing of all shapes and sizes. In International Conference on Machine Learning. PMLR, 2020.   \nJongmin Yoon, Sung Ju Hwang, and Juho Lee. Adversarial purification with score-based generative ", "page_idx": 12}, {"type": "text", "text": "models. In Proceedings of the 38th International Conference on Machine Learning, 2021. ", "page_idx": 12}, {"type": "text", "text": "A $\\pmb{f}$ -DP Background ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "For this background we use the DP mechanism terminology. A mechanism $\\mathcal{M}(.)$ is a randomized computation taking an input and returning one sample from the distribution of outputs for this input: $m\\sim\\mathcal{M}(x)$ with input $x$ and output $m$ . In ARS, each model step corresponds to an $f$ -DP mechanism. ", "page_idx": 13}, {"type": "text", "text": "Definitions. Dong et al. (2019; 2022) formalize privacy as a bound on the power of hypothesis tests. Consider any two neighbouring inputs: in the most common DP applications, $X,X^{\\prime}$ are two databases differing in one element; in the case of ARS against an $L_{p}$ adversary, $X,X^{\\prime}\\in\\mathbb{R}^{d}$ are any two inputs such that $X-X^{\\prime}\\in B_{p}(r)$ . Intuitively a randomized mechanism $\\mathcal{M}$ is private if, for any such neighbouring inputs, the distributions ${\\mathcal{M}}(X)$ and $\\mathcal{M}(\\boldsymbol{X}^{\\prime})$ are hard to distinguish. That is, by looking at a sample output from mechanism $\\mathcal{M}$ , it is hard to guess whether $\\mathcal{M}$ ran on $X$ or on $X^{\\prime}$ . ", "page_idx": 13}, {"type": "text", "text": "In $f$ -DP (Dong et al., 2019; 2022) \u201chard to distinguish\u201d is defined by a hypothesis testing problem: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{H}_{0}:\\mathrm{the~input~was~}X\\qquad\\quad\\mathrm{vs.}\\qquad\\quad\\mathcal{H}_{1}:\\mathrm{the~input~was~}X^{\\prime}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The output $m\\sim\\mathcal{M}$ serves as input to a rejection rule $\\phi(.)\\in[0,1]$ (note: to preserve typical notations, lower-case $\\phi$ is the rejection rule, and upper-case $\\Phi$ is the standard normal CDF). The rejection rule rejects $\\mathcal{H}_{0}$ with probability $\\phi(m)$ , so $\\bar{\\phi(m)}=0$ predicts that $X$ was the input, and $\\phi(\\bar{m})=1$ that $X^{\\prime}$ was. ", "page_idx": 13}, {"type": "text", "text": "Given a rejection rule $\\phi$ , we define its Type I error $\\alpha_{\\phi}$ and type $\\mathrm{II}$ error (or one minus the power of the rule) $\\beta_{\\phi}$ as: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\alpha_{\\phi}\\triangleq\\mathbb{E}_{m\\sim\\mathcal{M}(X)}[\\phi(m)]\\qquad}&{{}\\beta_{\\phi}\\triangleq1-\\mathbb{E}_{m\\sim\\mathcal{M}(X^{\\prime})}[\\phi(m)]}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Intuitively, $\\alpha_{\\phi}$ is the expected amount of rejection of $\\mathcal{H}_{0}$ when the hypothesis is correct ( $X$ was in input, but we think $X^{\\prime}$ was), also called the level of the rejection rule. On the flip side, $\\beta_{\\phi}$ is the expected amount of non-rejection under $\\mathcal{H}_{1}$ ( $X^{\\prime}$ was in input, but we think $X$ was). $1-\\beta_{\\phi}$ is called the power of the rejection rule. ", "page_idx": 13}, {"type": "text", "text": "For any two distributions ${\\mathcal{M}}(X)$ and $\\mathcal{M}(\\boldsymbol{X}^{\\prime})$ , we define the trade-off function $T\\bigl(\\mathcal{M}(X),\\mathcal{M}(X^{\\prime})\\bigr):$ $[0,1]\\rightarrow[0,1]$ that quantifies the minimum amount of type $\\mathrm{II}$ error achievable at each value of type I error by any (measurable) rule; or equivalently the maximum power of any rule at each level: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\forall\\alpha\\in[0,1],\\ T\\big(\\mathcal{M}(X),\\mathcal{M}(X^{\\prime})\\big)(\\alpha)=\\operatorname*{inf}_{\\phi}\\{\\beta_{\\phi}:\\alpha_{\\phi}\\leq\\alpha\\}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Now we define $f$ -DP: for any trade-off function $f$ , a mechanism $\\mathcal{M}$ is $f$ -DP if, for any neighbouring inputs $X,X^{\\prime}$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\nT\\bigl(\\mathcal{M}(X),\\mathcal{M}(X^{\\prime})\\bigr)\\geq f\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "These definitions are the main technical tools we need to prove Proposition 2.1. Corollary 2.2 only adds the formula for $f$ for the Gaussian mechanism, given in Section 2.1. ", "page_idx": 13}, {"type": "text", "text": "Composition. All other results rely on the above plus the adaptive composition of $f$ -DP mechanisms. Such composition is key to all DP theory and algorithm design. Consider a sequence of $N$ mechanisms $\\mathcal{M}_{i}$ , such that each mechanism is $f_{i}$ -DP with regards to $X,X^{\\prime}$ , and depends on the neighbouring input as well as the output of all previous mechanisms. More formally, under $\\mathcal{H}_{0}$ we have $m_{i}\\sim\\mathcal{M}_{i}(X,m_{<i})$ , and under $\\mathcal{H}_{1}$ we have $m_{i}\\sim\\mathcal{M}_{i}(X,m_{<i})$ , where $m_{<i}\\triangleq(m_{1},\\ldots,m_{i-1})$ . Concretely, each $\\mathcal{M}_{i}$ is $f_{i}$ -DP with regards to $X,X^{\\prime}$ for $f_{i}$ known in advance, but the actual computation made by $\\mathcal{M}_{i}$ can depend on $m_{<i}$ (as long as it is $f_{i}$ -DP). We leverage this adaptivity to lower the noise variance in our method\u2019s second step while keeping $f_{2}$ fixed (see $\\S3$ ). ", "page_idx": 13}, {"type": "text", "text": "We need two more results to define the composition of a sequence of mechanisms. First, Proposition 2.2 in Dong et al. (2019; 2022) shows that for any trade-off function $f$ , there exist two distributions $P_{f},Q_{f}$ such that $T(P_{f},Q_{f})\\,=\\,f$ . Call any such pair of distributions a representative pair of $f$ . Second, we define the composition operator $\\otimes$ by $f\\otimes g=T(P_{f}\\times P_{g},Q_{f}\\times Q_{g})$ . That is, the composition operator between two trade-off functions is the trade-off function between the product distributions on their representative pair. Then Theorem 3.2 in Dong et al. (2019; 2022) shows that: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathcal{M}:X\\rightarrow\\left(\\mathcal{M}_{1}(X),\\ldots,\\mathcal{M}_{N}(X,y_{<i})\\right)\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Concretely, the mechanism that returns the sequence of results for all compute adaptive $\\mathcal{M}_{N}$ is $f_{1}\\otimes...\\otimes f_{N}.\\mathbf{D}\\mathbf{I}$ P. The previous definitions, as well as this composition result, is what we use to prove Theorem 2.3 and Proposition 2.4. ", "page_idx": 13}, {"type": "text", "text": "B Proofs ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Proposition 2.1 ( $\\boldsymbol{\\textbf{\\textit{f}}}$ -DP Robustness). Let $\\mathcal{M}:\\mathcal{X}\\rightarrow\\mathcal{Y}$ be $f$ -DP for ${\\cal{B}}_{p}(r)$ neighbourhoods, and let $M_{S}:X\\to\\arg\\operatorname*{max}_{y\\in{y}}\\mathbb{P}({\\mathcal{M}}(X)=y)$ be the associated smooth classifier. Let $y_{+}\\triangleq M_{S}(X)$ be the prediction on input $X$ , and let $\\underline{{p_{+}}},\\dot{\\overline{{p_{-}}}}\\in\\left[0,1\\right]$ be such that $\\mathbb{P}(\\mathcal{M}(X)=y_{+})\\ge\\underline{{p_{+}}}\\ge\\overline{{p_{-}}}\\ge$ $\\operatorname*{max}_{y_{-}\\neq y_{+}}\\mathbb{P}(\\mathcal{M}(X)=y_{-})$ . Then: ", "page_idx": 14}, {"type": "equation", "text": "$$\nf(1-\\underline{{p_{+}}})\\geq1-f(\\overline{{p_{-}}})\\Rightarrow\\forall e\\in B_{p}(r),\\ M_{S}(X+e)=y_{+}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. Let us first consider any runner-up class $y_{-}$ . Calling $M$ the random variable for $\\mathcal{M}$ \u2019s prediction, consider the rejection rule $\\phi=\\mathbb{1}\\{M=y_{-}\\}$ , where $\\mathbb{1}$ is the indicator function. Denoting $\\alpha\\,\\triangleq\\,\\mathbb{E}_{{\\mathcal{M}}(X)}{\\big(}\\phi{\\big)}$ , and using the fact that $\\mathcal{M}$ is $f{\\mathrm{-DP}}$ for ${\\cal B}_{p}(r)$ neighbourhoods, we have that $\\forall e\\in B_{p}(r)$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{P}(\\mathcal{M}(X+e)=y_{-})=\\mathbb{E}_{\\mathcal{M}(X+e)}(\\phi)}&{{}}\\\\ {\\leq1-f(\\alpha)\\leq1-f(\\overline{{p_{-}}}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the last inequality is because $\\alpha\\,=\\,\\mathbb{E}_{{\\mathcal{M}}(X)}(\\phi)\\,=\\,\\mathbb{P}({\\mathcal{M}}(X)\\,=\\,y_{-})\\,\\leq\\,\\overline{{p_{-}}}$ , and $f$ is nonincreasing so $f(\\alpha)\\geq f({\\overline{{p_{-}}}})$ and hence $1-f(\\alpha)\\leq1-f({\\overline{{p_{-}}}})$ . ", "page_idx": 14}, {"type": "text", "text": "Let us now consider the predicted class $y_{+}$ . Keeping the same notations, and defining the rule ${\\phi}^{\\prime}=\\mathbb{1}\\{M\\neq y_{+}\\}=1-\\mathbf{\\bar{1}}\\{M=y_{+}\\}$ . Then $\\alpha^{\\prime}=\\mathbb{E}_{\\mathcal{M}(X)}(\\phi^{\\prime})=1-\\mathbb{P}(\\mathcal{M}(X)=y_{+})\\overset{\\cdot}{\\leq}1-\\underline{{p}}_{+}$ , and $\\mathbb{E}_{\\mathcal{M}(X+e)}(\\phi^{\\prime})\\le1-f(\\alpha^{\\prime})\\le1-f(1-p_{+})$ , yielding: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}(\\mathcal{M}(X+e)=y_{+})=1-\\mathbb{E}_{\\mathcal{M}(X+e)}(\\phi^{\\prime})}\\\\ &{\\qquad\\qquad\\qquad\\geq f(1-\\underline{{p_{+}}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Putting Equations (8) and (9) together, we have that $\\mathbb{P}(\\mathcal{M}(X\\!+\\!e)=y_{+})\\ge f(1\\!-\\!\\underline{{p}}_{+})\\ge1\\!-\\!f(\\overline{{p_{-}}})\\ge$ $\\mathbb{P}(\\mathcal{M}(X+e)=y_{-})$ and thus $m_{S}(X+e)=y_{+}$ . ", "page_idx": 14}, {"type": "text", "text": "Note that we do not have to chose a rule $\\phi\\in\\{0,1\\}$ , but could instead return any number in $[0,1]$ , such as the logits of the base classification model, yielding the following definition for the smoothed classifier $M_{S}:X\\to\\arg\\operatorname*{max}_{y\\in{\\mathcal{Y}}}\\mathbb{E}({\\mathcal{M}}(X)_{y})$ . ", "page_idx": 14}, {"type": "text", "text": "Proposition 2.2 (RS from $f{\\bf-}{\\bf D}{\\bf P},$ ). Let ${\\mathcal{M}}:X\\to g(X+z)$ , $z\\sim\\mathcal{N}(0,\\sigma^{2}\\mathbb{I}^{d})$ , and $M_{S}:X\\rightarrow$ arg $\\mathrm{max}_{y\\in\\mathcal{Y}}\\operatorname{\\mathbb{P}}(\\mathcal{M}(X)\\;=\\;y)$ be the associated smooth model. Let $y_{+}~\\triangleq~M_{S}(X)$ be the prediction on input $X$ , and let $p_{+},\\overline{{p_{-}}}~\\in~[0,1]$ be such that $\\mathbb{P}(\\mathcal{M}(X)\\;=\\;y_{+})\\;\\geq\\;\\underline{{p_{+}}}\\;\\geq\\;\\overline{{p_{-}}}\\;\\geq\\;$ $\\operatorname*{max}_{y_{-}\\neq y_{+}}\\mathbb{P}(\\mathcal{M}(X)=y_{-})$ . Then $\\forall e\\in B_{2}(r_{x})$ , $M_{S}(X+e)=y_{+}$ , with: ", "page_idx": 14}, {"type": "equation", "text": "$$\nr_{X}={\\frac{\\sigma}{2}}\\bigl(\\Phi^{-1}({\\underline{{p_{+}}}})-\\Phi^{-1}({\\overline{{p_{-}}}})\\bigr).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. $X:\\to X+z,\\;z\\sim\\mathcal{N}(0,\\sigma^{2})$ is a Gaussian mechanism. By Equation (2), for the $\\boldsymbol{B}_{r}(\\boldsymbol{r})$ neighbouring definition, it is $G_{\\frac{r}{\\sigma}}$ -DP. By post-processing $\\mathcal{M}$ is also $G_{\\frac{r}{\\sigma}}$ -DP. ", "page_idx": 14}, {"type": "text", "text": "Applying Proposition 2.1, we have that $G_{\\frac{r}{\\sigma}}(1-\\underline{{p_{+}}})\\geq1-G_{\\frac{r}{\\sigma}}(\\overline{{p_{-}}})\\Rightarrow\\forall e\\in B_{2}(r).$ $m_{S}(X+e)=$ $y_{+}$ . Let us find $r_{X}=\\operatorname*{sup}\\left\\{r:G_{\\frac{r}{\\sigma}}(1-{\\underline{{p_{+}}}})\\geq1-G_{\\frac{r}{\\sigma}}(\\overline{{p_{-}}})\\right\\}$ . Since $G_{\\frac{r}{\\sigma}}(.)$ as a function of $r$ is monotonously decreasing this will happen at $G_{\\frac{r_{X}}{\\sigma}}(1-\\underline{{p_{+}}})=1-G_{\\frac{r_{X}}{\\sigma}}(\\overline{{p_{-}}})$ , that is: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Phi\\Big(\\Phi^{-1}(\\underline{{p_{+}}})-\\frac{r_{X}}{\\sigma}\\Big)=1-\\Phi\\Big(\\Phi^{-1}(1-\\overline{{p_{-}}})-\\frac{r_{X}}{\\sigma}\\Big)}\\\\ &{\\Rightarrow\\;\\Phi^{-1}(\\underline{{p_{+}}})-\\frac{r_{X}}{\\sigma}=-\\Phi^{-1}(1-\\overline{{p_{-}}})+\\frac{r_{X}}{\\sigma}}\\\\ &{\\Rightarrow\\;\\Phi^{-1}(\\underline{{p_{+}}})-\\frac{r_{X}}{\\sigma}=\\Phi^{-1}(\\overline{{p_{-}}})+\\frac{r_{X}}{\\sigma}}\\\\ &{\\Rightarrow\\;r_{X}=\\frac{\\sigma}{2}\\big(\\Phi^{-1}(\\underline{{p_{+}}})-\\Phi^{-1}(\\overline{{p_{-}}})\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the first implication holds because by symmetry of the standard normal $1-\\Phi(x)=\\Phi(-x)$ , and because $\\Phi$ is strictly monotonous ; the second because similarly, $\\Phi^{-1}(1-p)=-\\Phi^{-1}(p)$ . ", "page_idx": 14}, {"type": "text", "text": "C Experiment Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "C.1 Mask Architecture ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Figure 7 shows the architecture of our Mask model $w$ $\\left(\\mathcal{M}_{1}\\right)$ . We adapt a UNet architecture to preserve dimensions, and use a Sigmoid layer at the end of the model to output values between 0 and 1 for mask weights. We set up our UNet hyperparameters as $:$ in_channe $_{X=3}$ , out_channel $s{=}1$ (to out put a mask), base_channe $l{=}32$ , channel_mul $\\div=$ {1,2,4,8}. ", "page_idx": 15}, {"type": "image", "img_path": "MN4nt01TeO/tmp/91d4ef6b1236624eb852117f45c83db89afe88555b2b37f2680ac02768c189b1.jpg", "img_caption": ["Figure 7: UNet structure "], "img_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "MN4nt01TeO/tmp/1e206e793d84923e3616ab2eda62ef2f5860184f52e8def456e36f09f30fdaa9.jpg", "table_caption": [], "table_footnote": ["Table 4: Hyperparameters for training ARS. Check Appendix C.3 for more details of CIFAR-10 hyperparameters. "], "page_idx": 15}, {"type": "text", "text": "C.2 Hyperparameter tuning for CelebA ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Table 4 provides the details of our ARS models\u2019 hyper-parameters. On the CelebA dataset, we tune the hyper-parameters in the ARS, Cohen et al. (2019), and static mask settings at $\\sigma=0.75$ . In all settings, we settle on SGD with learning rate 0.05 and a step learning rate scheduler (step size of 3 and $\\gamma=0.8$ ) for the base classifier. In the static mask setting, we use SGD with learning rate 0.01. ", "page_idx": 15}, {"type": "text", "text": "C.3 Hyperparameter tuning for CIFAR-10 BG20k ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "For the base classifier $g$ in Cohen et al. (2019), Static Mask and ARS experiments, we tune the optimizer and its hyperparameters using $k=64$ , $\\sigma=1.5$ as the testbed. Based on these tuning experiments, we chose AdamW as the optimizer with an initial learning rate of 0.01 and weight decay to 0.0001. We scale the learning rate by 0.1 every $30^{t h}$ epoch, with a batch size of 256. For the mask model $w$ in ARS experiments, we again used the AdamW optimizer with an initial learning rate of 0.001, weight decay of 0.0001, whilst scaling the learning rate by 0.5 every $40^{t h}$ epoch. We used the same hyperparameters for all $k$ \u2019s and $\\sigma$ \u2019s for these setups. ", "page_idx": 15}, {"type": "text", "text": "For S\u00faken\u00edk et al. (2021), for $k\\,=\\,32$ , 48, 64, we tune the hyperparameters using $k\\,=\\,64$ as the testbed. We kept the optimizer same as used in the author\u2019s code (SGD), setting the initial learning rate to 0.01, momentum to 0.9 and weight decay to 0. We scaled the learning rate by 0.1 every $30^{t\\widetilde{h}}$ epoch. We tune $k=96$ \u2019s parameters separately. We start with an initial learning rate 0.1, keeping the rest of hyperparameters same as for $k=32$ , 48, 64. Note that for $k=96$ , we could not get the standard accuracy to improve upon random baseline for $\\sigma=\\{0.5,0.75,1.0,1.5\\}$ , despite extensively tuning the learning rate. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "table", "img_path": "MN4nt01TeO/tmp/c3c14517ddd87ebf6942972334dafbf6364047dc3a2483e2afad5e94fa220ba3.jpg", "table_caption": [], "table_footnote": ["Table 5: UniCR $\\vec{\\beta}$ chosen for each $k$ , $\\sigma$ setting. "], "page_idx": 16}, {"type": "text", "text": "For UniCR, we tune $\\beta$ (the parameter of the generalized normal distribution for the noise) using $\\sigma=0.75$ and $k=48$ as the testbed. We perform grid search on $\\beta$ and find that $\\beta=2.0$ or $\\beta=2.25$ (Gaussian and close to a Gaussian, but with a wider more and shorted tails) perform best. For each $k,\\sigma$ setting, we train 3 models with $\\beta=2.25$ and 2.0 and choose the $\\beta$ giving highest mean standard accuracy. The chosen $\\beta$ for each setting is given in Table 5. For each setting we also tune optimizer hyper-parameters. At $k=32$ , we use SGD optimizer. We use a learning rate of 0.01, momentum of 0.9 and weight decay of 0.0005 with a step learning rate scheduler (30 step and $\\gamma=0.1$ ). At $k=48,64,96$ we use training batch size 256, 100 epochs, and AdamW optimizer. We use a learning rate of 0.001 with step learning rate scheduler (30 step size and $\\gamma=0.5)$ ). ", "page_idx": 16}, {"type": "image", "img_path": "MN4nt01TeO/tmp/4d01067cfa51cbf524f465fc50cbbfd94a6be97531e782ada89b18e22dfd8e2b.jpg", "img_caption": ["Figure 8: $k=32$ certified test accuracy results for CIFAR-10 (20kBG) (a)-(f) show the effect of increasing $\\sigma$ . These results are in our $20\\mathrm{kBG}$ setting where $=$ a CIFAR-10 image is placed randomly along the edges of a background image. Each line is the mean and the shaded interval covers $+/-$ one standard deviation across seeds. ", "Table 6: Standard Accuracy $(r\\,=\\,0)$ ) on CIFAR-10 (20kBG). Our 20kBG benchmark places CIFAR-10 images on larger background images. We report the mean accuracy and standard deviation over three seeds. ARS achieves higher accuracy across noise $\\sigma$ and input dimension $k$ . \u2206indicates adaptivity. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "D Additional Results on CIFAR10 BG20k Benchmark ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We show here full sweep of results for CIFAR- $10~\\mathrm{Bg}20\\mathrm{k}$ benchmark for $k\\,=\\,32,48,64,96$ and $\\sigma\\,=\\,0.12,0.25,0.5,0.75,1.0,1.5$ for all our baselines and ARS. Similar to results presented in Table 1, we report the mean accuracy and standard deviation over three seeds. ", "page_idx": 17}, {"type": "table", "img_path": "MN4nt01TeO/tmp/00de732346f62872b7bd0a375a800bcfc7bc422df5b53cca99cbbab034bfd63a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "MN4nt01TeO/tmp/4244036f1e5ff2be15c5d4175140d08f8f976e21886e3e21a9cbb6a056f2f29a.jpg", "img_caption": ["Figure 9: $k=48$ certified test accuracy results for CIFAR-10 (20kBG) (a)-(f) show the effect of increasing $\\sigma$ . These results are in our 20kBG setting where a CIFAR-10 image is placed randomly along the edges of a background image. Each line is the mean and the shaded interval covers $+/-$ one standard deviation across seeds. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "MN4nt01TeO/tmp/99dca831979fcfe60cfa1f64fd6d0c0ed2af26336226a80b7886a09c454b08c1.jpg", "img_caption": ["Figure 10: $k=64$ certified test accuracy results for CIFAR-10 (20kBG) (a)-(f) show the effect of increasing $\\sigma$ . These results are in our 20kBG setting where a CIFAR-10 image is placed randomly along the edges of a background image. Each line is the mean and the shaded interval covers $+/-$ one standard deviation across seeds. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "D.1 Impact of Certification Test-set Size ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The baseline results in Table 6 are lower than those reported by Cohen et al. (2019) (Figure 6). This is a result of our certifying over a smaller subset of the test set (the released code from Cohen et al. (2019)1uses a subset of size 500, while the paper says the certification was on the full test set). In Table 7 we report the standard accuracy $(r=0)$ ) on the same 500 samples subset of the CIFAR-10 test set as used in the code released by Cohen et al. (2019). We show results for $k=32$ , which is the plain CIFAR-10 task, using both the hyper-parameters from Cohen et al. (2019), our own optimized hyper-parameters (the most notable change is that we use AdamW), and ARS. We make three observations. First, as it turns out, our 200 samples subset used for results in Table 6 is more challenging, and accuracy values are systematically lower (by about $2\\%$ points) than over 500 samples and the full test set (10k samples). Using the same 500 samples subset yields accuracy values very close to those reported by Cohen et al. (2019) on their hyper-parameters. Second, over all samples sizes 200, 500, and 10k, our hyper-parameters significantly improve the accuracy of RS (by about $3\\%$ points consistently), confirming that we are making a fair comparison between the best RS models we could find and ARS. Third, at $k=32$ ARS provides modest improvements over tuned RS, as CIFAR-10 images are well cropped and low-dimensional, providing less opportunity for dimension reduction through masking, and hence lower ARS improvements. ", "page_idx": 18}, {"type": "image", "img_path": "MN4nt01TeO/tmp/4550af8a103da61f3988e6951e2d0bed5fc05e67ad9c45e785fc96101268e274.jpg", "img_caption": ["Figure 11: $k=96$ certified test accuracy results for CIFAR-10 (20kBG) (a)-(f) show the effect of increasing $\\sigma$ . These results are in our 20kBG setting where a CIFAR-10 image is placed randomly along the edges of a background image. Each line is the mean and the shaded interval covers $+/-$ one standard deviation across seeds. ", "Table 7: $k\\,=\\,32$ standard accuracy $(r\\,=\\,0)$ ) on CIFAR-10 (20kBG). We report the mean accuracy and standard deviation over three seeds. We compare three approaches: Cohen et al. (2019) trained with the hyper-parameters they report in their GitHub repository, Cohen et al. (2019) trained with the hyper-parameters we report in 4, and ARS. We train models with each approach at $\\sigma=0.12,0.25,0.5,1.0.$ . The corresponding models are certified on 200, 500, and 10,000 samples. "], "img_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "MN4nt01TeO/tmp/da1ba6424b45ed6e23e78f1dc7d5837ff23617dc417c4fcfab4f33544567ac6e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "D.2 CIFAR-10 BG20k figures ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Figure 12, Figure 13, Figure 14 and Figure 15 shows figures of different stages in our ARS architecture (Figure 1). In all of these figures, $1^{s t}$ row corresponds to input images $X$ , $2^{n d}$ row corresponds to images right after $\\mathcal{M}_{1},3^{r d}$ row corresponds to ARS masks, $4^{t h}$ row corresponds to element-wise product of input $X$ and ARS masks, $5^{t h}$ row corresponds to images right after $\\mathcal{M}_{2}$ and $6^{t h}$ row corresponds to images right after averaging $\\mathcal{M}_{1},\\mathcal{M}_{2}$ . ", "page_idx": 19}, {"type": "image", "img_path": "MN4nt01TeO/tmp/5fa2efda746ae11d8d7dfcc29f829d0fefd1b46cba24db87539c64069ba2f405.jpg", "img_caption": ["Figure 12: Figures at different stages in our ARS architecture for CIFAR-10 $k=32$ input images. Check Appendix D.2 for detailed information about each row "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "MN4nt01TeO/tmp/6fe2ea0cb92337c99dec416e9be293154e741ab03683084da3753c5169378983.jpg", "img_caption": ["Figure 13: Figures at different stages in our ARS architecture for CIFAR-10 BG20k $k=48$ input images. Check Appendix D.2 for detailed information about each row "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "image", "img_path": "MN4nt01TeO/tmp/bb3506e99502f8872ba7ddd47899902540cc4d8943c3883110bf7a50b18249bd.jpg", "img_caption": ["Figure 14: Figures at different stages in our ARS architecture for CIFAR-10 BG20k $k=64$ input images. Check Appendix D.2 for detailed information about each row "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "MN4nt01TeO/tmp/9d921615442d40db04296c26ce4c0f8d4c4c456b674998774d87c400af070cdf.jpg", "img_caption": ["Figure 15: Figures at different stages in our ARS architecture for CIFAR-10 BG20k $k=96$ input images. Check Appendix D.2 for detailed information about each row "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "MN4nt01TeO/tmp/0dce3d45336aff321c9e62f42851e614897e7fcb55b8cedb2830720ec4cbe7e0.jpg", "img_caption": ["Figure 16: The localized ARS masks produce un-noised mouth regions after averaging. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Figure 16 shows how adaptive masking reduces the noise around areas that are important to classification. The images follow our architecture visualized Figure 1. The mask model is provided the first query noised images as input. The learned masks, presented in the bottom left, are sparse and highly concentrated around the area of interest\u2014the mouth area. The second query noised images (after weighted average) use the mask to clearly reduce the noise around the mouth. This large noise reduction enables ARS to outperform static masking and Cohen et al. (2019), as shown on Figure 4. ", "page_idx": 22}, {"type": "text", "text": "We show here a full set of results for the CelebA benchmark over all $\\sigma=0.12,0.25,0.5,0.75,1.0,1.5$ for Cohen et al. (2019), static masking, and ARS. Similarly to the results in Table 2, we report the mean standard accuracy and standard deviation over three seeds in Table 8. The certified accuracies are plotted in Figure 17. We see that at all $\\sigma$ , ARS has the highest standard accuracy. ", "page_idx": 22}, {"type": "table", "img_path": "MN4nt01TeO/tmp/e3656a74756f89ba8d1f60553035e3706c30fca2c298aea53d71ef22440d540c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "MN4nt01TeO/tmp/b5b32a1686fb6597510d14b6966ea603142d12ad1e9e080d8e3e0ecf75d12e0a.jpg", "img_caption": ["Table 8: Standard Accuracy $(r=0)$ ) on CelebA (unaligned and cropped). We report the mean accuracy and standard deviation over three seeds. ", "Figure 17: Certified test accuracy on CelebA (unaligned and cropped). Each line is the mean and the shading covers $\\pm1$ standard deviation across three seeds. Adaptivity helps at all noise levels. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "F Additional Results on ImageNet ", "text_level": 1, "page_idx": 23}, {"type": "image", "img_path": "MN4nt01TeO/tmp/797c1739f33ae31d3895a84b2ec37d715b7e876ef9821b3654f41d2d7bee2a1e.jpg", "img_caption": ["Figure 18: The localized ARS masks produce un-noised object regions after averaging. For $\\sigma=1$ . "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "MN4nt01TeO/tmp/0fcf5ac21f1abb2f193f2b485bc88026fb5cdde0fde1c4d9f3d37ba75ff569ef.jpg", "img_caption": ["Figure 19: For $\\sigma=0.5$ . "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "MN4nt01TeO/tmp/ce4c507aeb7f826d0033883eb1372610dd60342f5a8ba30b3d9705bea54de403.jpg", "img_caption": ["Figure 20: For $\\sigma=0.25$ . "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Figures 18 to 20 show how adaptive masking reduces the noise around areas that are important to classification for ImageNet. The images follow our architecture visualized Figure 1. The mask model is provided the first query noised images as input. The learned masks, presented in the bottom left, are sparse and concentrated around the area of interest (the bird, or any labelled object). The second query noised images after weighted average use the mask to clearly reduce the noise around the bird. ", "page_idx": 23}, {"type": "table", "img_path": "MN4nt01TeO/tmp/09efb728af4a370db2790d6e6b6c58572f7b8fe2686e578103ab52e797a38646.jpg", "table_caption": [], "table_footnote": ["Table 9: Test accuracy without certification on ImageNet. "], "page_idx": 23}, {"type": "text", "text": "An interesting observation is that on ImageNet, ARS yields larger improvements to the test accuracy without certification. We follow the certification procedure of Cohen et al. (2019), which first determines the predicted class, and then certifies it if and only if the probability of this predicted class is such that $\\underline{{p_{+}}}\\geq0.5$ (that is, it groups all other classes into one non-predicted class). If this is not the case, the prediction will count as not certified at $r=0$ , even if the predicted class is still correct. We keep this procedure for consistency with prior work, but on a task with a large number of classes like in ImageNet, the accuracy at $r=0$ can be much lower as the non-certified accuracy. ", "page_idx": 23}, {"type": "text", "text": "We noticed that ARS significantly improves this non certified accuracy, while not improving the accuracy at $r=0$ as much (see Section 4.3). In effect ARS leads to more correct predictions, but this (correct) predicted class has $p_{\\pm}<0.5$ , so the accuracy at $r\\,=\\,0$ does not increase. Table 9 shows this effect by comparing the test accuracy (no certification at all) across all methods. Under ARS, the test accuracy increases from $68.5\\%$ to $70.1\\%$ when $\\sigma=0.25$ , from $60.7\\%$ to $64.2\\%$ when $\\sigma\\,=\\,0.5$ , and from $\\dot{4}7.9\\%$ to $53.8\\%$ for $\\sigma\\,=\\,1.0$ . In summary, ARS achieves best test accuracy (without certification) for all the noise levels. This suggests that ARS helps more than shown by the default certification approach, and that a finer analysis that accounts for the probability of all classes could yield further improvements in certified accuracy. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The abstract and introduction identify the purpose of our paper and our theoretical, technical, and empirical contributions. We provide theoretical results in Section 2 with detailed proofs, and comprehensive experimental results in Section 4 to support our technical claims and contributions for test-time adaptive certification for robustness. Our choice of theoretical framework, task scope, and evaluation datasets are all present in the abstract. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 25}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We identify limitations in Section 5 and in particular call out computational overhead and the need to assess the combination of ARS with other improvements to RS (which are compatible in principle, but require experiment to measure and verify). ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 25}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We present the theoretical results in Section 2, where we cover the main results, and provide Appendix B with further detailed assumptions and proofs. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 26}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The method is described in the text, and to ensure sufficient detail, we additionally provide appendices and supplementary material that includes the anonymized code for our method. The experiments identify their settings and hyperparameters, such as noise levels and input image dimensions, and these settings can be verified by inspection of the results for our method and the baselines like Cohen et al. (2019). By relying on standard base models, such as ResNet-50, our work is made more reproducible by following the conventions of existing papers. For modeling specific to our contribution, such as a the mask model, we have taken care to provide more detail in the appendix. Our adaptivity benchmarks, while our own design, are simple to implement (through padding and cropping) and our choices of how to transform the base public datasets are described in the text. In addition, we have incorporated reviewers feedback in the camera ready version. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.   \n(b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We submit our codes and necessary command in a zip file. For the camera ready version, we have open sourced our code at https://github.com/ubc-systopia/ adaptive-randomized-smoothing/tree/main. The hyperparameter settings are listed in Table 4. The datasets we used are cited and has open access to the public. . ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 27}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: We detail our experimental setting at a high-level in Section 4 and then elaborate in each subsection. We list our experimental setting in Section 4 and the hyperparameters in Table 4. For baselines, we rely on the reference hyperparameter settings from the papers for Cohen et al. (2019), rely on code and hyper-parameters shared by the authors for S\u00faken\u00edk et al. (2021) (the code is not public), and tune hyper-parameters and $\\beta$ for Hong et al. (2022) (details in $\\S C_{\\l,\\l}$ . ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 27}, {"type": "text", "text": "\u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 28}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We ran all experiments end-to-end over three different seeds, and show both mean and standard deviation on plots and tables (except on ImageNet, for resources reasons). The factors of variability are those that are standard and natural to deep learning and robustness by randomized smoothing: initialization of parameters, train-time sampling of the data, train-time sampling of augmentations, and test-time sampling of noise for certification. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 28}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The computing resources are included in Table 4. The resources are single GPUs, which while required are sufficiently standard for this field as to be understandable and accessible. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 28}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We have reviewed the NeurIPS Code of Ethics and can confirm that we conform to them in every respect when we conducted research in the paper. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 29}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: This paper improves the robustness of existing machine learning models to adversarial examples. Adversarial examples have been proposed for a limited number of beneficial use cases, such as censorship or surveillance evasion, but overall more robust and trustworthy ML models can benefti the increasingly broad deployment of machine learning, including in security and safety-critical applications. As a paper with a theoretical emphasis and empirical evaluation on well-established benchmarks, this work does not have additional societal impact beyond the norm. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 29}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: Our work does not have a high risk for misuse, given its theoretical portion on proofs for robustness, and its empirical portion on the robustness evaluation of standard image classifiers. As such no special safeguards are needed. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 30}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: All papers, code, and assets, etc. have been properly cited in the paper. We make use of the the reference code provided by the cited papers by their authors. We make use of standard and existing datasets, and therefore do not collect or introduce additional assets. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 30}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: We do not collect or include new assets. To the extent that our own method code is an asset, it is the creation of the authors, and it is included anonymously and confidentially in our supplementary materials. If accepted, the code will be released under an open license such as BSD-2. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 30}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: No crowdsourcing or human subjects. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 31}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: No crowdsourcing or human subjects. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 31}]