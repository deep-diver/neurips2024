[{"figure_path": "MN4nt01TeO/tables/tables_6_1.jpg", "caption": "Table 1: Standard Accuracy (r = 0) on CIFAR-10 (20kBG). Our 20kBG benchmark places CIFAR-10 images on larger background images. We report the mean accuracy and standard deviation over three seeds. ARS achieves higher accuracy across noise \u03c3 and input dimension k (\u25b3 indicates adaptivity). We provide results with more \u03c3 levels in Appendix D.", "description": "This table presents the standard accuracy (error rate at radius r=0) for different model approaches on the CIFAR-10 dataset with added background images (20kBG benchmark).  The table compares several methods: Cohen et al., Static Mask, UniCR, S\u00faken\u00edk et al., and ARS.  It shows accuracy results across various noise levels (\u03c3) and input dimensions (k).  ARS, an adaptive method, demonstrates higher accuracy in most cases. The full results with more noise levels are in Appendix D.", "section": "4.1 CIFAR-10 Benchmark: Classification with Distractor Backgrounds"}, {"figure_path": "MN4nt01TeO/tables/tables_8_1.jpg", "caption": "Table 2: Standard test accuracy (r = 0) on CelebA (unaligned and cropped). ARS is equal or better. Adaptivity handles the higher spatial dimensions (160 \u00d7 160) and variation of these inputs.", "description": "This table presents the standard test accuracy (when the radius r is 0) for three different approaches on the CelebA dataset.  The three methods compared are Cohen et al., Static Mask, and ARS (the proposed method). The results are shown for three different noise levels (\u03c3 = 0.25, 0.5, and 1.0). The table demonstrates that ARS consistently achieves equal or better accuracy compared to the other two methods, highlighting its ability to handle high spatial dimensions and input variations.", "section": "4.2 CelebA Benchmark: Classification Without Spatial Alignment"}, {"figure_path": "MN4nt01TeO/tables/tables_9_1.jpg", "caption": "Table 3: Standard test accuracy (r = 0) on ImageNet. ARS maintains standard accuracy.", "description": "This table presents the standard test accuracy (at radius r=0) on the ImageNet dataset for three different noise levels (\u03c3 = 0.25, 0.5, 1.0).  It compares the performance of the proposed Adaptive Randomized Smoothing (ARS) method with the standard Randomized Smoothing (RS) method by Cohen et al.  Two versions of ARS are shown: one where only the mask model is trained ('Pretrain'), and another where the entire model is trained end-to-end ('End-To-End'). The results indicate that ARS maintains a similar standard accuracy to the Cohen et al. method, showcasing its scalability to large datasets.", "section": "4.3 ImageNet Benchmark: Classification on the Standard Large-Scale Dataset"}, {"figure_path": "MN4nt01TeO/tables/tables_15_1.jpg", "caption": "Table 4: Hyperparameters for training ARS. Check Appendix C.3 for more details of CIFAR-10 hyperparameters.", "description": "This table lists the hyperparameters used for training the Adaptive Randomized Smoothing (ARS) model.  It breaks down the settings for different aspects of the model training process, including for the Mask Model (UNet) and the Base Classifier.  Different hyperparameters were used for different datasets (CIFAR-10, CelebA, and ImageNet).  The table includes the GPU used, the number of epochs, batch sizes, base channel numbers, optimizers (AdamW and SGD), learning rates, weight decay, momentum, step sizes, and gamma values.  Appendix C.3 offers additional details on hyperparameter tuning for CIFAR-10.", "section": "C Experiment Details"}, {"figure_path": "MN4nt01TeO/tables/tables_16_1.jpg", "caption": "Table 5: UniCR \u03b2 chosen for each k, \u03c3 setting.", "description": "This table presents the values of \u03b2 used for the UniCR method in the experiments.  \u03b2 is a parameter of the generalized normal distribution used for noise in the randomized smoothing technique.  The table shows that \u03b2 was tuned for different values of k (input dimension) and \u03c3 (noise level) in the CIFAR-10 BG20k experiment, resulting in slightly different optimal \u03b2 values for various combinations of k and \u03c3.", "section": "4.1 CIFAR-10 Benchmark: Classification with Distractor Backgrounds"}, {"figure_path": "MN4nt01TeO/tables/tables_17_1.jpg", "caption": "Table 1: Standard Accuracy (r = 0) on CIFAR-10 (20kBG). Our 20kBG benchmark places CIFAR-10 images on larger background images. We report the mean accuracy and standard deviation over three seeds. ARS achieves higher accuracy across noise \u03c3 and input dimension k (\u25b3 indicates adaptivity). We provide results with more \u03c3 levels in Appendix D.", "description": "This table presents the standard accuracy (with no adversarial attack, r=0) on a modified CIFAR-10 dataset (20kBG) where images are superimposed on larger backgrounds, varying the input dimension (k).  The results are shown for different noise levels (\u03c3) and for several methods: Cohen et al. (standard randomized smoothing), Static Mask (a baseline using a fixed mask), UniCR (a test-time adaptive method), S\u00faken\u00edk et al. (another test-time adaptive method), and ARS (the proposed method). The table highlights the improved accuracy of ARS, especially as the input dimension increases.  Standard deviations are also included.", "section": "4.1 CIFAR-10 Benchmark: Classification with Distractor Backgrounds"}, {"figure_path": "MN4nt01TeO/tables/tables_19_1.jpg", "caption": "Table 1: Standard Accuracy (r = 0) on CIFAR-10 (20kBG). Our 20kBG benchmark places CIFAR-10 images on larger background images. We report the mean accuracy and standard deviation over three seeds. ARS achieves higher accuracy across noise \u03c3 and input dimension k (\u25b3 indicates adaptivity). We provide results with more \u03c3 levels in Appendix D.", "description": "This table presents the standard accuracy (when there is no adversarial attack, r=0) for different model approaches on the CIFAR-10 dataset with 20k background images.  It compares the performance of Cohen et al., Static Mask, UniCR, S\u00faken\u00edk et al., and ARS across varying noise levels (\u03c3) and input dimensions (k). The 20kBG benchmark makes the task more challenging by adding distractor background images, highlighting the impact of adaptivity in handling higher dimensional inputs and noise. ARS consistently achieves higher accuracy than other methods.", "section": "4.1 CIFAR-10 Benchmark: Classification with Distractor Backgrounds"}, {"figure_path": "MN4nt01TeO/tables/tables_22_1.jpg", "caption": "Table 2: Standard test accuracy (r = 0) on CelebA (unaligned and cropped). ARS is equal or better. Adaptivity handles the higher spatial dimensions (160 \u00d7 160) and variation of these inputs.", "description": "This table presents the standard test accuracy results for different methods on the CelebA dataset. The accuracy is measured at radius r = 0, meaning no certification is applied. Three methods are compared: Cohen et al., Static Mask, and ARS. The results show that ARS achieves equal or higher accuracy than the other methods.  The experiment uses unaligned and cropped CelebA images, which are 160x160 pixels in size, highlighting the impact of adaptivity in handling high spatial dimensions and variations in input images.", "section": "4.2 CelebA Benchmark: Classification Without Spatial Alignment"}, {"figure_path": "MN4nt01TeO/tables/tables_23_1.jpg", "caption": "Table 3: Standard test accuracy (r = 0) on ImageNet. ARS maintains standard accuracy.", "description": "This table presents the standard test accuracy (with no certification, r=0) on the ImageNet dataset for three different noise levels (\u03c3 = 0.25, 0.5, 1.0).  It compares the results of the standard Randomized Smoothing (Cohen et al.) method with two variations of Adaptive Randomized Smoothing (ARS): one where the mask model is pre-trained (ARS (Pretrain)) and one where the entire model is trained end-to-end (ARS (End-To-End)). The table shows that ARS maintains, or slightly improves, the standard accuracy compared to the baseline RS method.", "section": "4.3 ImageNet Benchmark: Classification on the Standard Large-Scale Dataset"}]