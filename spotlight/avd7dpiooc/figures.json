[{"figure_path": "AVd7DpiooC/figures/figures_1_1.jpg", "caption": "Figure 1: Illustration of Q-K attention with the two versions of Q-K token attention (QKTA) and Q-K channel attention (QKCA). The inputs are binary spikes and there are only sparse additions and mask operations in Q-K attention. As a spike-driven module, Q-K attention efficiently models the token or channel attention through spike-form binary vectors, performing linear complexity to #tokens (or #channels) and high energy efficiency. Spiking Neuron (SN) in this work adopts the Leaky-Integrate-and-Fire (LIF) model, which is shown in Appendix. 7.1.", "description": "This figure illustrates the Q-K attention mechanism, a core component of the QKFormer model. It shows two versions: Q-K token attention (QKTA) and Q-K channel attention (QKCA).  Both versions use binary spike inputs and perform sparse additions and masking operations to model token or channel attention efficiently.  The figure highlights the linear complexity and energy efficiency achieved by using spike-form binary vectors.  The Leaky Integrate-and-Fire (LIF) neuron model is used.", "section": "3 Method"}, {"figure_path": "AVd7DpiooC/figures/figures_4_1.jpg", "caption": "Figure 2: The overview of QKFormer, a hierarchical spiking transformer with Q-K attention.", "description": "The figure illustrates the architecture of QKFormer, a hierarchical spiking transformer. It shows three stages. In stage 1, the input with dimensions of To \u00d7 H \u00d7 W \u00d7 n is processed by SPEDS-1 (Spiking Patch Embedding with Deformed Shortcut) and N1 QKFormer blocks, each containing a Q-K Attention module and a SMLP (Spiking MLP) block.  Stage 2 processes the output of stage 1 with SPEDS-2 and N2 QKFormer blocks, reducing the number of tokens. Finally, stage 3 uses SPEDS-3 and N3 Spikformer blocks (using Spiking Self Attention), further reducing tokens and increasing channels. This hierarchical design enables multi-level spiking feature representation, improving performance.", "section": "3.4 QKFormer"}, {"figure_path": "AVd7DpiooC/figures/figures_7_1.jpg", "caption": "Figure 3: The visualization and memory consumption of QKTA. (a) is the visualization of Q-K token attention. The white dot means value 1, while the black one means value 0. (b) shows the comparison of memory costs between QKTA and SSA under different token numbers. N is the token number.", "description": "This figure visualizes the Q-K token attention mechanism and compares its memory consumption with SSA. The left panel (a) shows heatmaps of query (Q), key (K), and output (X') matrices for Stage 1 and Stage 2 of the QKFormer model.  White pixels indicate a value of 1 (spike), and black pixels represent 0 (no spike). The right panel (b) displays a graph comparing the GPU memory usage of QKTA and SSA across various numbers of tokens (N), demonstrating QKTA's superior memory efficiency, especially as the number of tokens increases.", "section": "4.3 Analyses on Q-K Attention"}, {"figure_path": "AVd7DpiooC/figures/figures_7_2.jpg", "caption": "Figure 4: (a) shows the variance and expectation of SSA, (b) shows the variance and expectation of QKTA. Assume that all the spike elements (either 0 or 1) in SSA and QKTA are independent random variables and subject to Bernoulli distribution.", "description": "This figure visualizes the variance and expectation of both SSA (Spiking Self Attention) and QKTA (Q-K Token Attention) methods.  It assumes that spike elements in both methods are independent and follow a Bernoulli distribution. The plots show how the variance and expectation change with respect to different firing rates (fQ, fK, fV) which represent the probability of a spike occurring for query, key, and value elements, respectively. Panel (a) shows the results for SSA, and panel (b) for QKTA, highlighting the difference between the two methods.  The key takeaway is that QKTA shows significantly smaller variance and expectation than SSA. This is important because it justifies the elimination of scaling factors in QKTA, improving energy efficiency and simplicity.", "section": "4.3 Analyses on Q-K Attention"}, {"figure_path": "AVd7DpiooC/figures/figures_15_1.jpg", "caption": "Figure 5: (a) Spiking Patch Splitting (SPS) module in Spikformer. (b) Spiking Patch Embedding with Deformed Shortcut (SPEDS) module in QKFormer.", "description": "This figure compares the Spiking Patch Splitting (SPS) module used in the Spikformer model with the Spiking Patch Embedding with Deformed Shortcut (SPEDS) module used in the QKFormer model.  It illustrates the architectural differences between these two modules, highlighting how SPEDS integrates deformed shortcuts to enhance the transmission and integration of spiking information.  The comparison showcases a key improvement in QKFormer's design for efficient information flow in the network.", "section": "3.5 Spiking Patch Embedding with Deformed Shortcut"}, {"figure_path": "AVd7DpiooC/figures/figures_17_1.jpg", "caption": "Figure 6: Training loss, test loss, top-1 and top-5 test accuracy of QKFormer on ImageNet-1K. The input resolution of training and testing are 224 \u00d7 224.", "description": "This figure shows the training and testing performance of the QKFormer model on the ImageNet-1K dataset.  It displays four sub-figures: (a) Training loss illustrating the model's loss during training across different model sizes (64.96M, 29.08M, and 16.47M parameters). (b) Testing loss representing the model's performance on unseen data during training. (c) Top-1 accuracy showing the percentage of correctly classified images in the top prediction. (d) Top-5 accuracy showing the percentage of images where the correct class was among the top 5 predictions.  The input image resolution used for both training and testing was 224x224 pixels.", "section": "4.1 Results on ImageNet-1k Classification"}]