{"references": [{"fullname_first_author": "J. Kaplan", "paper_title": "Scaling laws for neural language models", "publication_date": "2020-01-28", "reason": "This paper introduced the seminal scaling laws for neural language models, which the current paper builds upon and attempts to resolve discrepancies within."}, {"fullname_first_author": "J. Hoffmann", "paper_title": "An empirical analysis of compute-optimal large language model training", "publication_date": "2022-12-01", "reason": "This paper challenges the initial scaling laws and proposes a different compute-optimal scaling law that the current paper seeks to reconcile with."}, {"fullname_first_author": "S. Y. Gadre", "paper_title": "Language models scale reliably with over-training and on downstream tasks", "publication_date": "2024-03-18", "reason": "This paper provides additional insights on model scaling, and the current paper references it as having established the superiority of the Chinchilla scaling law."}, {"fullname_first_author": "M. Dehghani", "paper_title": "Scaling vision transformers to 22 billion parameters", "publication_date": "2023-07-01", "reason": "While focused on vision transformers, this paper provides valuable insights into the scaling of large models that the current work draws parallels with."}, {"fullname_first_author": "M. Ivgi", "paper_title": "Scaling laws under the microscope: Predicting transformer performance from small-scale experiments", "publication_date": "2022-12-01", "reason": "This paper examines scaling laws with higher granularity, which is relevant to the current paper's detailed analysis of various factors influencing optimal scaling."}]}