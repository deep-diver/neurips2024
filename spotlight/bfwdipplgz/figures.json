[{"figure_path": "BFWdIPPLgZ/figures/figures_1_1.jpg", "caption": "Figure 1: A phase transition in a toy model of attention. (A) We investigate a tied low-rank attention model in a teacher-student setting. The teacher mixes the L individual tokens of dimension d according to a semantic (as a function of the token's content x) and a positional (as a function of the token's position) attention matrix. The student can only use positional encodings p to fit the positional properties of the teacher. (B) Schematic view of the loss landscape of the teacher, which contains both a positional and a semantic minimum. (C) We find that in the asymptotic high-dimensional limit and as a function of the sample complexity and the composition of the teacher, the global minimum switches, constituting a phase transition between positional and semantic learning.", "description": "This figure shows a phase transition between positional and semantic learning in a toy model of dot-product attention. Panel A illustrates the model setup, where a teacher model uses both positional and semantic information to mix tokens, while a student model only uses positional encodings. Panel B shows a schematic of the loss landscape with both positional and semantic minima. Panel C demonstrates the phase transition, showing how the global minimum switches from positional to semantic learning as the sample complexity increases.", "section": "1 Introduction"}, {"figure_path": "BFWdIPPLgZ/figures/figures_6_1.jpg", "caption": "Figure 2: Mixed positional/semantic teacher for w = 0.3. Setting is rs = rt = 1, L = 2, A = ((0.6, 0.4), (0.4, 0.6)), \u03a3\u2081 = \u03a3\u2082 = 0.25Id, p\u2081 = 1d = \u2212p\u2082, and Q* ~ N(0, Id). (left) Solid lines: difference in training loss \u0394et between the semantic and positional solutions of (7) in Result 4.2. Markers: difference in training loss at convergence achieved by training the model (2) using gradient descent initialized resp. at Q and at p\u2081. Marker color as in Fig. 3. (center) overlap \u03b8 between the learnt weights Q and the target weights Q*; overlap m between the learnt weights Q and the positional embedding p\u2081. Solid lines represent the theoretical characterization of these two summary statistics provided by Result 4.2. Only the solution of (7) corresponding to the lowest found training loss is represented (i.e. the positional solution for \u03b1 < \u03b1c and the semantic otherwise). Markers represent experimental measures of these quantities, for gradient descent at convergence. Gradient descent was initialized at p\u2081 for \u03b1 < \u03b1c and at Q for \u03b1 > \u03b1c. (right) We show the MSE achieved by the dense linear as emin (Result 4.2), and MSE achieved by the dense linear baseline ein (15) (Result 5.1). Markers indicate the MSE experimentally reached by the model (2) trained using gradient descent, initialized previously for the overlaps. All experiments were performed in d = 1,000 with the Pytorch implementation of full-batch gradient descent, for T = 5,000 epochs and learning rate \u03b7 = 0.15. All points are averaged over 24 instances of the problem each.", "description": "This figure shows a phase transition between positional and semantic learning in a toy model of attention. The left panel shows the difference in training loss between the semantic and positional solutions as a function of sample complexity (\u03b1). The middle panel shows the overlap between the learned weights and the target weights (\u03b8) and the overlap between the learned weights and the positional embedding (m). The right panel compares the mean squared error (MSE) of the dot-product attention layer with a linear positional baseline.", "section": "5 Positional-to-semantic phase transition"}, {"figure_path": "BFWdIPPLgZ/figures/figures_7_1.jpg", "caption": "Figure 3: Phase transition between semantic and positional training loss. Setting and experiments were performed identical to Fig. 2. (left) Scaling d and n jointly for a = 1.5 concentrates for \u03b8 and m, in different locations for the positional and semantic local minima each. We show 30 runs for each d \u2208 [10, 15, 23, 36, 56, 87, 135, 209, 323, 500]. (center) The color map represents the difference in training loss at convergence when training the model (2) using the Pytorch implementation of full-batch gradient descent, respectively from an initialization at p\u2081 or at Q*. The green dashed line represents the theoretical prediction for the threshold \u03b1c(w) above which the semantic solution of (7) in Result 4.2 has lower loss than the positional solution. (right) The color map represents the difference in test MSE at convergence when training the attention model (13) using the Pytorch implementation of full-batch gradient descent initialized at Q*, and the dense linear baseline (15). The red dashed lines indicate the theoretical prediction -following from Result 4.2 and Result 15- for the threshold sample complexity \u03b1\u2081(\u03c9) above which the dot-product attention (2) outperforms the baseline (15).", "description": "This figure shows the phase transition between semantic and positional mechanisms in a dot-product attention model.  The left panel shows how scaling the embedding dimension and sample size affects the concentration of the summary statistics. The center and right panels depict color maps visualizing the difference in training loss and test error respectively between semantic and positional mechanisms, showing the sample complexity threshold where the semantic mechanism outperforms the positional mechanism.", "section": "Positional-to-semantic phase transition"}, {"figure_path": "BFWdIPPLgZ/figures/figures_16_1.jpg", "caption": "Figure 2: Mixed positional/semantic teacher for w = 0.3. Setting is rs = rt = 1, L = 2, A = ((0.6, 0.4), (0.4, 0.6)), \u03a3\u2081 = \u03a3\u2082 = 0.25]d, P\u2081 = 1d = \u2212p\u2082 and Q* ~ N(0, Id). (left) Solid lines: difference in training loss \u0394et between the semantic and positional solutions of (7) in Result 4.2. Markers: difference in training loss at convergence achieved by training the model (2) using gradient descent initialized resp. at Q and at p\u2081. Marker color as in Fig. 3. (center) overlap \u03b8 between the learnt weights Q and the target weights Q*, overlap m between the learnt weights Q and the positional embedding p\u2081. Solid lines represent the theoretical characterization of these two summary statistics provided by Result 4.2. Only the solution of (7) corresponding to the lowest found training loss is represented (i.e. the positional solution for a < ac and the semantic otherwise). Markers represent experimental measures of these quantities, for gradient descent at convergence. Gradient descent was initialized at p\u2081 for a < ac and at Q for a > ac. (right) We show the MSE achieved by the dense linear as emin (Result 4.2), and MSE achieved by the dense linear baseline ein (15) (Result 5.1). Markers indicate the MSE experimentally reached by the model (2) trained using gradient descent, initialized previously for the overlaps. All experiments were performed in d = 1,000 with the Pytorch implementation of full-batch gradient descent, for T = 5,000 epochs and learning rate \u03b7 = 0.15. All points are averaged over 24 instances of the problem each.", "description": "This figure shows a phase transition between positional and semantic learning in a toy model of attention.  The left panel shows the difference in training loss between semantic and positional solutions as a function of sample complexity. The center panel shows the overlap between learned weights and target/positional embeddings, comparing theoretical predictions with experimental results from gradient descent. The right panel compares the mean squared error (MSE) of the dot-product attention layer with a linear positional baseline.", "section": "Positional-to-semantic phase transition"}, {"figure_path": "BFWdIPPLgZ/figures/figures_19_1.jpg", "caption": "Figure 2: Mixed positional/semantic teacher for w = 0.3. Setting is rs = rt = 1, L = 2, A = ((0.6, 0.4), (0.4, 0.6)), \u03a3\u2081 = \u03a3\u2082 = 0.25Id, p\u2081 = 1d = \u2212p\u2082, and Q* ~ N(0, Id). (left) Solid lines: difference in training loss \u0394et between the semantic and positional solutions of (7) in Result 4.2. Markers: difference in training loss at convergence achieved by training the model (2) using gradient descent initialized resp. at Q and at p\u2081. Marker color as in Fig. 3. (center) overlap \u03b8 between the learnt weights Q and the target weights Q*, overlap m between the learnt weights Q and the positional embedding p\u2081. Solid lines represent the theoretical characterization of these two summary statistics provided by Result 4.2. Only the solution of (7) corresponding to the lowest found training loss is represented (i.e. the positional solution for \u03b1 < \u03b1c and the semantic otherwise). Markers represent experimental measures of these quantities, for gradient descent at convergence. Gradient descent was initialized at p\u2081 for \u03b1 < \u03b1c and at Q for \u03b1 > \u03b1c. (right) We show the MSE achieved by the dense linear as emin (Result 4.2), and MSE achieved by the dense linear baseline ein (15) (Result 5.1). Markers indicate the MSE experimentally reached by the model (2) trained using gradient descent, initialized previously for the overlaps. All experiments were performed in d = 1,000 with the Pytorch implementation of full-batch gradient descent, for T = 5,000 epochs and learning rate \u03b7 = 0.15. All points are averaged over 24 instances of the problem each.", "description": "This figure shows a phase transition between positional and semantic learning in a low-rank attention model.  The left panel displays the difference in training loss between semantic and positional solutions as a function of sample complexity. The center panel shows the overlap between learned weights and target/positional embeddings, and the right panel compares the mean squared error (MSE) of the attention model to a linear baseline.", "section": "Positional-to-semantic phase transition"}, {"figure_path": "BFWdIPPLgZ/figures/figures_20_1.jpg", "caption": "Figure 2: Mixed positional/semantic teacher for w = 0.3. Setting is rs = rt = 1, L = 2, A = ((0.6, 0.4), (0.4, 0.6)), \u03a3\u2081 = \u03a32 = 0.25]d, P1 = 1d = \u2212p2 and Q* ~ N(0, Id). (left) Solid lines: difference in training loss Aet between the semantic and positional solutions of (7) in Result 4.2. Markers: difference in training loss at convergence achieved by training the model (2) using gradient descent initialized resp. at Q and at p1. Marker color as in Fig. 3. (center) overlap @ between the learnt weights Q and the target weights Q overlap m between the learnt weights Q and the positional embedding p1. Solid lines represent the theoretical characterization of these two summary statistics provided by Result 4.2. Only the solution of (7) corresponding to the lowest found training loss is represented (i.e. the positional solution for a < ac and the semantic otherwise). Markers represent experimental measures of these quantities, for gradient descent at convergence. Gradient descent was initialized at p1 for a < ac and at Q for a > ac. (right) We show the MSE achieved by the dense linear as emin (Result 4.2), and MSE achieved by the dense linear baseline ein (15) (Result 5.1). Markers indicate the MSE experimentally reached by the model (2) trained using gradient descent, initialized previously for the overlaps. All experiments were performed in d = 1,000 with the Pytorch implementation of full-batch gradient descent, for T = 5,000 epochs and learning rate \u03b7 = 0.15. All points are averaged over 24 instances of the problem each.", "description": "This figure shows a phase transition between positional and semantic learning in a toy model of attention.  The plots show the difference in training loss between positional and semantic solutions as a function of sample complexity (\u03b1), overlap between learned weights and target weights, and test error comparison between the dot-product attention layer and a linear baseline.  It demonstrates how increasing sample complexity leads to a transition from positional to semantic learning, where the dot-product attention outperforms the linear baseline when it learns the semantic mechanism.", "section": "Positional-to-semantic phase transition"}, {"figure_path": "BFWdIPPLgZ/figures/figures_36_1.jpg", "caption": "Figure 3: Phase transition between semantic and positional training loss. Setting and experiments were performed identical to Fig. 2. (left) Scaling d and n jointly for a = 1.5 concentrates for 0 and m, in different locations for the positional and semantic local minima each. We show 30 runs for each d \u2208 [10, 15, 23, 36, 56, 87, 135, 209, 323, 500]. (center) The color map represents the difference in training loss at convergence when training the model (2) using the Pytorch implementation of full-batch gradient descent, respectively from an initialization at p\u2081 or at Q*. The green dashed line represents the theoretical prediction for the threshold \u03b1c(\u03c9) above which the semantic solution of (7) in Result 4.2 has lower loss than the positional solution. (right) The color map represents the difference in test MSE at convergence when training the attention model (13) using the Pytorch implementation of full-batch gradient descent initialized at Q*, and the dense linear baseline (15). The red dashed lines indicate the theoretical prediction -following from Result 4.2 and Result 15- for the threshold sample complexity \u03b1\u2081(\u03c9) above which the dot-product attention (2) outperforms the baseline (15).", "description": "This figure shows the phase transition between positional and semantic mechanisms in a dot-product attention model. The left panel shows the concentration of summary statistics in the high-dimensional limit. The center and right panels show color maps representing the difference in training loss and test MSE, respectively, between the positional and semantic solutions as a function of sample complexity (\u03b1) and teacher mix (\u03c9). Dashed lines indicate theoretical predictions for phase transition thresholds.", "section": "5 Positional-to-semantic phase transition"}, {"figure_path": "BFWdIPPLgZ/figures/figures_37_1.jpg", "caption": "Figure 2: Mixed positional/semantic teacher for w = 0.3. Setting is rs = rt = 1, L = 2, A = ((0.6, 0.4), (0.4, 0.6)), \u03a3\u2081 = \u03a32 = 0.25Id, P1 = 1d = \u2212p2 and Q* ~ N(0, Id). (left) Solid lines: difference in training loss Aet between the semantic and positional solutions of (7) in Result 4.2. Markers: difference in training loss at convergence achieved by training the model (2) using gradient descent initialized resp. at Q and at p1. Marker color as in Fig. 3. (center) overlap @ between the learnt weights Q and the target weights Q overlap m between the learnt weights Q and the positional embedding p1. Solid lines represent the theoretical characterization of these two summary statistics provided by Result 4.2. Only the solution of (7) corresponding to the lowest found training loss is represented (i.e. the positional solution for a < ac and the semantic otherwise). Markers represent experimental measures of these quantities, for gradient descent at convergence. Gradient descent was initialized at p1 for a < ac and at Q for a > ac. (right) We show the MSE achieved by the dense linear as emin (Result 4.2), and MSE achieved by the dense linear baseline ein (15) (Result 5.1). Markers indicate the MSE experimentally reached by the model (2) trained using gradient descent, initialized previously for the overlaps. All experiments were performed in d = 1,000 with the Pytorch implementation of full-batch gradient descent, for T = 5,000 epochs and learning rate \u03b7 = 0.15. All points are averaged over 24 instances of the problem each.", "description": "This figure shows the phase transition between positional and semantic learning in a low-rank attention model. The left panel shows the difference in training loss between the semantic and positional solutions as a function of sample complexity. The center panel shows the overlap between the learned weights and the target weights (semantic overlap) and the learned weights and positional embeddings (positional overlap). The right panel compares the MSE of the low-rank attention model with a linear positional baseline, demonstrating that the semantic mechanism outperforms the positional baseline when sufficient data is available.", "section": "Positional-to-semantic phase transition"}, {"figure_path": "BFWdIPPLgZ/figures/figures_37_2.jpg", "caption": "Figure 3: Phase transition between semantic and positional training loss. Setting and experiments were performed identical to Fig. 2. (left) Scaling d and n jointly for a = 1.5 concentrates for 0 and m, in different locations for the positional and semantic local minima each. We show 30 runs for each d\u2208 [10, 15, 23, 36, 56, 87, 135, 209, 323, 500]. (center) The color map represents the difference in training loss at convergence when training the model (2) using the Pytorch implementation of full-batch gradient descent, respectively from an initialization at p\u2081 or at Q*. The green dashed line represents the theoretical prediction for the threshold ac(w) above which the semantic solution of (7) in Result 4.2 has lower loss than the positional solution. (right) The color map represents the difference in test MSE at convergence when training the attention model (13) using the Pytorch implementation of full-batch gradient descent initialized at Q*, and the dense linear baseline (15). The red dashed lines indicate the theoretical prediction -following from Result 4.2 and Result 15- for the threshold sample complexity \u03b1\u2081(\u03c9) above which the dot-product attention (2) outperforms the baseline (15).", "description": "This figure shows the phase transition between semantic and positional learning mechanisms in a dot-product attention model.  The left panel shows the concentration of summary statistics in the high-dimensional limit. The center and right panels display color maps representing the differences in training loss and test error between positional and semantic mechanisms, respectively, as functions of sample complexity (\u03b1) and the teacher's mix parameter (\u03c9). Dashed lines indicate the theoretical predictions for the phase transition thresholds.", "section": "Positional-to-semantic phase transition"}, {"figure_path": "BFWdIPPLgZ/figures/figures_38_1.jpg", "caption": "Figure 1: A phase transition in a toy model of attention. (A) We investigate a tied low-rank attention model in a teacher-student setting. The teacher mixes the L individual tokens of dimension d according to a semantic (as a function of the token's content x) and a positional (as a function of the token's position) attention matrix. The student can only use positional encodings p to fit the positional properties of the teacher. (B) Schematic view of the loss landscape of the teacher, which contains both a positional and a semantic minimum. (C) We find that in the asymptotic high-dimensional limit and as a function of the sample complexity and the composition of the teacher, the global minimum switches, constituting a phase transition between positional and semantic learning.", "description": "This figure shows a phase transition between positional and semantic learning in a simplified self-attention model.  Panel A depicts the model setup: a teacher uses both positional and semantic information, while the student only has access to positional information. Panel B illustrates the loss landscape, showcasing two minima corresponding to positional and semantic attention. Panel C demonstrates the phase transition where the global minimum shifts from positional to semantic attention as sample complexity increases. This transition is controlled by the teacher's mixing of positional and semantic information.", "section": "Introduction"}, {"figure_path": "BFWdIPPLgZ/figures/figures_38_2.jpg", "caption": "Figure 1: A phase transition in a toy model of attention. (A) We investigate a tied low-rank attention model in a teacher-student setting. The teacher mixes the L individual tokens of dimension d according to a semantic (as a function of the token's content x) and a positional (as a function of the token's position) attention matrix. The student can only use positional encodings p to fit the positional properties of the teacher. (B) Schematic view of the loss landscape of the teacher, which contains both a positional and a semantic minimum. (C) We find that in the asymptotic high-dimensional limit and as a function of the sample complexity and the composition of the teacher, the global minimum switches, constituting a phase transition between positional and semantic learning.", "description": "This figure shows a phase transition between positional and semantic learning in a simplified attention model.  Panel A describes the model setup, with a teacher model that uses both positional and semantic information and a student model that only uses positional information. Panel B illustrates the loss landscape of the teacher model which has two minima representing positional and semantic attention.  Panel C shows that as the sample complexity (amount of data) increases, the global minimum of the student model transitions from a positional to a semantic solution. This highlights the emergence of semantic attention capabilities in the dot-product attention model given enough data.", "section": "1 Introduction"}, {"figure_path": "BFWdIPPLgZ/figures/figures_38_3.jpg", "caption": "Figure 3: Phase transition between semantic and positional training loss. Setting and experiments were performed identical to Fig. 2. (left) Scaling d and n jointly for a = 1.5 concentrates for 0 and m, in different locations for the positional and semantic local minima each. We show 30 runs for each d\u2208 [10, 15, 23, 36, 56, 87, 135, 209, 323, 500]. (center) The color map represents the difference in training loss at convergence when training the model (2) using the Pytorch implementation of full-batch gradient descent, respectively from an initialization at p\u2081 or at Q*. The green dashed line represents the theoretical prediction for the threshold ac(w) above which the semantic solution of (7) in Result 4.2 has lower loss than the positional solution. (right) The color map represents the difference in test MSE at convergence when training the attention model (13) using the Pytorch implementation of full-batch gradient descent initialized at Q*, and the dense linear baseline (15). The red dashed lines indicate the theoretical prediction -following from Result 4.2 and Result 15- for the threshold sample complexity \u03b1\u03b9 (\u03c9) above which the dot-product attention (2) outperforms the baseline (15).", "description": "This figure shows the phase transition between semantic and positional learning mechanisms in a dot-product attention model.  The left panel shows the concentration of summary statistics for increasing embedding dimension and training samples. The central and right panels illustrate phase transitions in training loss and test error, respectively, comparing the dot-product attention model with a positional baseline.  The phase transition marks the point where the semantic mechanism becomes superior to the positional one.", "section": "5 Positional-to-semantic phase transition"}, {"figure_path": "BFWdIPPLgZ/figures/figures_39_1.jpg", "caption": "Figure 3: Phase transition between semantic and positional training loss. Setting and experiments were performed identical to Fig. 2. (left) Scaling d and n jointly for a = 1.5 concentrates for 0 and m, in different locations for the positional and semantic local minima each. We show 30 runs for each d\u2208 [10, 15, 23, 36, 56, 87, 135, 209, 323, 500]. (center) The color map represents the difference in training loss at convergence when training the model (2) using the Pytorch implementation of full-batch gradient descent, respectively from an initialization at p\u2081 or at Q*. The green dashed line represents the theoretical prediction for the threshold ac(w) above which the semantic solution of (7) in Result 4.2 has lower loss than the positional solution. (right) The color map represents the difference in test MSE at convergence when training the attention model (13) using the Pytorch implementation of full-batch gradient descent initialized at Q*, and the dense linear baseline (15). The red dashed lines indicate the theoretical prediction -following from Result 4.2 and Result 15- for the threshold sample complexity \u03b1\u2081(\u03c9) above which the dot-product attention (2) outperforms the baseline (15).", "description": "This figure shows a phase transition between semantic and positional mechanisms in a dot-product attention model.  The left panel demonstrates the concentration of summary statistics (\u03b8 and m) for different embedding dimensions (d) and training samples (n) at a fixed ratio.  The center panel displays the difference in training loss between semantic and positional solutions, highlighting the transition point (green dashed line). The right panel shows the difference in test MSE, indicating when the dot-product attention outperforms a linear baseline (red dashed line).", "section": "Positional-to-semantic phase transition"}, {"figure_path": "BFWdIPPLgZ/figures/figures_39_2.jpg", "caption": "Figure 3: Phase transition between semantic and positional training loss. Setting and experiments were performed identical to Fig. 2. (left) Scaling d and n jointly for a = 1.5 concentrates for 0 and m, in different locations for the positional and semantic local minima each. We show 30 runs for each d\u2208 [10, 15, 23, 36, 56, 87, 135, 209, 323, 500]. (center) The color map represents the difference in training loss at convergence when training the model (2) using the Pytorch implementation of full-batch gradient descent, respectively from an initialization at p\u2081 or at Q*. The green dashed line represents the theoretical prediction for the threshold ac(w) above which the semantic solution of (7) in Result 4.2 has lower loss than the positional solution. (right) The color map represents the difference in test MSE at convergence when training the attention model (13) using the Pytorch implementation of full-batch gradient descent initialized at Q*, and the dense linear baseline (15). The red dashed lines indicate the theoretical prediction -following from Result 4.2 and Result 15- for the threshold sample complexity \u03b1\u03b9 (\u03c9) above which the dot-product attention (2) outperforms the baseline (15).", "description": "This figure shows the phase transition between semantic and positional mechanisms in a dot-product attention model. The left panel shows the concentration of summary statistics in the high-dimensional limit. The center and right panels show the difference in training loss and test error between semantic and positional mechanisms, highlighting a phase transition controlled by sample complexity.", "section": "Positional-to-semantic phase transition"}, {"figure_path": "BFWdIPPLgZ/figures/figures_40_1.jpg", "caption": "Figure 3: Phase transition between semantic and positional training loss. Setting and experiments were performed identical to Fig. 2. (left) Scaling d and n jointly for a = 1.5 concentrates for 0 and m, in different locations for the positional and semantic local minima each. We show 30 runs for each d\u2208 [10, 15, 23, 36, 56, 87, 135, 209, 323, 500]. (center) The color map represents the difference in training loss at convergence when training the model (2) using the Pytorch implementation of full-batch gradient descent, respectively from an initialization at p\u2081 or at Q*. The green dashed line represents the theoretical prediction for the threshold ac(w) above which the semantic solution of (7) in Result 4.2 has lower loss than the positional solution. (right) The color map represents the difference in test MSE at convergence when training the attention model (13) using the Pytorch implementation of full-batch gradient descent initialized at Q*, and the dense linear baseline (15). The red dashed lines indicate the theoretical prediction -following from Result 4.2 and Result 15- for the threshold sample complexity \u03b1\u03b9 (\u03c9) above which the dot-product attention (2) outperforms the baseline (15).", "description": "This figure shows a phase transition between positional and semantic learning mechanisms in a dot-product attention model.  The left panel demonstrates the concentration of summary statistics (\u03b8 and m) in different regions for semantic and positional minima as the embedding dimension and number of training samples scale proportionally. The center panel depicts a color map showing the training loss difference between semantic and positional minima, highlighting a transition point based on sample complexity. Finally, the right panel displays a similar color map for the test mean squared error (MSE), comparing the model's performance to a linear positional baseline, revealing a threshold beyond which the attention model, using a semantic mechanism, outperforms the baseline.", "section": "5 Positional-to-semantic phase transition"}]