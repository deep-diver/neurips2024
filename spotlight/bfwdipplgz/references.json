{"references": [{"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-00-00", "reason": "This paper introduced the transformer architecture, which is foundational to the self-attention mechanism analyzed in the current paper."}, {"fullname_first_author": "Samy Jelassi", "paper_title": "Vision transformers provably learn spatial structure", "publication_date": "2022-00-00", "reason": "This paper provides a theoretical analysis of vision transformers, which are closely related to the self-attention models considered in the current paper."}, {"fullname_first_author": "Jason Wei", "paper_title": "Emergent abilities of large language models", "publication_date": "2022-00-00", "reason": "This paper investigates emergent abilities in large language models, which is highly relevant to the study of algorithmic mechanisms in attention models."}, {"fullname_first_author": "Jean Barbier", "paper_title": "Optimal errors and phase transitions in high-dimensional generalized linear models", "publication_date": "2019-00-00", "reason": "This paper provides a theoretical framework for analyzing phase transitions in high-dimensional models, which is essential to the current paper's analysis of phase transitions in attention models."}, {"fullname_first_author": "Riccardo Rende", "paper_title": "Optimal inference of a generalised Potts model by single-layer transformers with factored attention", "publication_date": "2023-00-00", "reason": "This paper offers a theoretical analysis of single-layer transformers with factored attention, providing a solvable model for comparative analysis with the current paper's focus."}]}