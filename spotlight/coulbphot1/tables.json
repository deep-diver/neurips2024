[{"figure_path": "cOuLbPhOT1/tables/tables_6_1.jpg", "caption": "Table 1: Results on VTAB-1K with ViT-B/16. Mean Acc. is the average of group mean values.", "description": "This table presents the results of various parameter-efficient fine-tuning (PEFT) methods on the VTAB-1K benchmark using the ViT-B/16 architecture.  It shows the accuracy achieved by each method across different categories of datasets within VTAB-1K, including Natural, Specialized, and Structured images.  The \"Mean Acc.\" column represents the average accuracy across all dataset categories.  The table allows for a comparison of PACE against other state-of-the-art PEFT methods to highlight its performance.", "section": "4 Experiments"}, {"figure_path": "cOuLbPhOT1/tables/tables_6_2.jpg", "caption": "Table 2: Classification accuracy on Few-shot learning with ViT-B/16 pretrained on ImageNet-21K.", "description": "This table presents the classification accuracy results for few-shot learning experiments using a ViT-B/16 model pre-trained on ImageNet-21K.  The results are broken down by the number of shots (1, 2, 4, 8, 16) and across five different fine-grained image datasets: FGVC-Aircraft, Food101, OxfordFlowers102, OxfordPets, and StanfordCars.  The table compares the performance of the baseline LoRAmul+VPTadd method with and without the PACE enhancement. It shows the average accuracy across these datasets as well. The table helps demonstrate PACE's effectiveness in improving few-shot learning performance.", "section": "4 Experiments"}, {"figure_path": "cOuLbPhOT1/tables/tables_7_1.jpg", "caption": "Table 3: Results on FGVC with ViT-B/16. * denotes using augmented ViT by AugReg [61].", "description": "This table presents the results of experiments conducted on the FGVC benchmark using the ViT-B/16 model.  It compares the performance of various methods, including a baseline and the proposed PACE method, across five fine-grained datasets: CUB-200-2011, NABirds, Oxford Flowers, Stanford Dogs, and Stanford Cars.  The results show the classification accuracy achieved by each method on each dataset. The asterisk (*) indicates that the method used augmented ViT as described in AugReg [61].", "section": "4 Experiments"}, {"figure_path": "cOuLbPhOT1/tables/tables_7_2.jpg", "caption": "Table 4: Results on domain adaptation with ViT-B/16 pretrained on ImageNet-21K.", "description": "This table presents the results of domain adaptation experiments using the ViT-B/16 model pretrained on ImageNet-21K.  The model is evaluated on its performance on ImageNet-Sketch, ImageNet-V2, ImageNet-A, and ImageNet-R datasets.  The results are compared across various parameter-efficient fine-tuning (PEFT) methods, including Full fine-tuning, Linear probing, Adapter, VPT, LoRA, NOAH, GLORA, LoRAmul+VPTadd, and the proposed PACE method. The table shows that PACE improves upon the best-performing baseline (LoRAmul+VPTadd) in terms of mean accuracy across all target datasets, demonstrating its effectiveness in domain adaptation.", "section": "4 Experiments"}, {"figure_path": "cOuLbPhOT1/tables/tables_7_3.jpg", "caption": "Table 5: Results for GLUE w/ ROBERTabase. Matthew's correlation for COLA, Pearson correlation for STSB, and accuracy for others. ", "description": "This table presents the results of experiments conducted on the GLUE benchmark using the ROBERTabase model.  It shows the performance of various methods (Full, BitFit, Adapt, VeRA, LoRA, and PACE) across six different GLUE tasks: COLA, STSB, MRPC, RTE, QNLI, and SST2. The metrics used are Matthew's correlation coefficient for COLA, Pearson correlation coefficient for STSB, and accuracy for the remaining tasks.  The table highlights the improvement achieved by adding PACE to the LoRA model, resulting in better overall performance on the GLUE benchmark.", "section": "4 Experiments"}, {"figure_path": "cOuLbPhOT1/tables/tables_7_4.jpg", "caption": "Table 6: Results for GSM-8K using Phi-3-mini-4k-instruct.", "description": "This table presents the classification accuracy results on the GSM-8K benchmark for different fine-tuning methods.  The results are shown for the pre-trained model, a fully fine-tuned model, LoRA, and LoRA with PACE.  It demonstrates the performance improvement achieved by PACE in comparison to other methods on this mathematical reasoning task.", "section": "4 Experiments"}, {"figure_path": "cOuLbPhOT1/tables/tables_7_5.jpg", "caption": "Table 7: Classification results on domain adaptation and CIFAR-100 in VTAB-1K based different pretrained models. Src. is short for 'source' in Table 4.", "description": "This table presents a comparison of classification accuracy across various methods (full finetuning, linear probing, different PEFT methods, and PACE) on the CIFAR-100 dataset and domain adaptation tasks within the VTAB-1K benchmark. The results are categorized by different pretrained models (ViT-B with ImageNet-21K weights, ViT-B with Laion2B-ImageNet-12K weights, and Swin-B with ImageNet-21K weights).  The source dataset is specified, along with results on various target datasets (ImageNet-Sketch, ImageNet-V2, ImageNet-A, and ImageNet-R).  It helps to evaluate the generalization performance and effectiveness of different fine-tuning approaches across different pretrained models.", "section": "4.1 Comparison with the State of the Arts"}, {"figure_path": "cOuLbPhOT1/tables/tables_9_1.jpg", "caption": "Table 7: Classification results on domain adaptation and CIFAR-100 in VTAB-1K based different pretrained models. Src. is short for 'source' in Table 4.", "description": "This table presents classification accuracy results on domain adaptation and CIFAR-100 tasks, using the VTAB-1K benchmark. It compares the performance of different pre-trained models (ViT-B with ImageNet-21K weights, ViT-B with Laion2B-ImageNet-12K weights, and Swin-B with ImageNet-21K weights) and fine-tuning methods (Full, Linear, LoRAadd, VPTadd, LoRAmul, LoRAmul+VPTadd, and PACE).  The results are broken down by dataset (CIFAR-100 and ImageNet-1K) and target domain (Source, Sketch, V2, A, R).  The table highlights the improved performance of PACE compared to other methods.", "section": "4.1 Comparison with the State of the Arts"}, {"figure_path": "cOuLbPhOT1/tables/tables_18_1.jpg", "caption": "Table 9: GPU memory usage, training time, and accuracy for PACEfast and PACEhalf. \u2018m\u2019 denotes minutes. Both variants outperform the baseline while maintaining similar computational demands.", "description": "This table compares the maximum GPU memory usage, total training time and accuracy of different methods on three datasets: CIFAR-100, Camelyon, and ImageNet.  The methods compared include the baseline LoRAmul+VPTadd and several variants of PACE, including PACEfast and PACEhalf with different values of N.  The results show that PACEfast and PACEhalf achieve similar or better accuracy than the baseline while using significantly less GPU memory and training time.", "section": "4.3 Ablation studies"}, {"figure_path": "cOuLbPhOT1/tables/tables_18_2.jpg", "caption": "Table 10: Comparison of PACEfast memory overhead and baseline GPU memory requirements.", "description": "This table compares the additional memory needed by PACEfast with the baseline GPU memory usage for three different tasks: CIFAR-100 (VTAB-1K), Camelyon (VTAB-1K), and ImageNet (Domain adaptation).  It shows that the memory overhead of PACEfast is insignificant compared to the baseline, ranging from 0.0042% to 0.67%.  This demonstrates the efficiency of PACEfast in terms of memory usage.", "section": "4.3 Ablation studies"}, {"figure_path": "cOuLbPhOT1/tables/tables_18_3.jpg", "caption": "Table 11: Results of PACEfast with reduced batch size and epochs on CIFAR-100 (VTAB-1K w/ ViT-16/B), Camelyon (VTAB-1K w/ Swin-B), ImageNet (Domain adaptaion w/ ViT-16/B). PACEfast outperforms baseline while using less GPU memory and training time.", "description": "This table presents the results of experiments conducted using PACEfast with reduced batch size and epochs on three different datasets: CIFAR-100, Camelyon, and ImageNet.  Each dataset was processed using a different backbone model (ViT-16/B or Swin-B) and the results show significant improvements in memory efficiency and training time compared to the baseline while maintaining superior accuracy.", "section": "4 Experiments"}, {"figure_path": "cOuLbPhOT1/tables/tables_19_1.jpg", "caption": "Table 12: Classification results for different methods on VTAB-1K with different training epochs.", "description": "This table presents classification accuracy results on the VTAB-1K benchmark using various methods and different training epochs (50, 100, 200, 300, and 530).  The results are categorized by dataset group (Natural, Specialized, and Structured) and show the average accuracy across the groups.  It demonstrates how the performance of different methods varies with the number of training epochs.", "section": "D Additional Experiments"}, {"figure_path": "cOuLbPhOT1/tables/tables_19_2.jpg", "caption": "Table 13: Classification results on FGVC using varying percentages of data based on ViT-16/B.", "description": "This table presents the classification accuracy results on five fine-grained visual categorization datasets (FGVC) using the ViT-B/16 model. The results are shown for different training data sizes, namely 50%, 20%, and 10% of the original training data.  The table compares the performance of the baseline LoRAmul+VPTadd method with the proposed PACE method across all five datasets at these varying data sizes.  It demonstrates the ability of PACE to maintain and even improve performance under data scarcity, aligning with the paper's theoretical analyses about better generalization with smaller gradient norms and larger datasets.", "section": "4 Experiments"}, {"figure_path": "cOuLbPhOT1/tables/tables_19_3.jpg", "caption": "Table 14: Classification results on VTAB-1K using self-supervised DINO and MAE, with ViT-16/B pretrained on the ImageNet-1K dataset.", "description": "This table presents the classification accuracy results on four VTAB-1K sub-datasets (SVHN, Camelyon, Clevr-Count, Clevr-Dist) using different methods.  The results are broken down by whether the ViT-16/B model was fully fine-tuned, only linearly probed, fine-tuned using LoRAmul+VPTadd, or fine-tuned using LoRAmul+VPTadd with the proposed PACE method.  The models were pre-trained using either self-supervised DINO or MAE methods on the ImageNet-1K dataset.  The table demonstrates the performance improvements achieved by using PACE in self-supervised scenarios.", "section": "D.3 Experiments on self-supervised pre-trained backbones"}, {"figure_path": "cOuLbPhOT1/tables/tables_20_1.jpg", "caption": "Table 15: Classification results of different PEFT methods based on ViT-16/B.", "description": "This table compares the performance of various parameter-efficient fine-tuning (PEFT) methods, both with and without the proposed PACE method, on two tasks: CIFAR-100 (from the VTAB-1K benchmark) and ImageNet domain adaptation.  It shows the average accuracy across multiple source and target datasets for domain adaptation, highlighting the performance improvement achieved by incorporating PACE into existing PEFT methods such as AdaptFormer, GLORA, COFT, and BOFT.", "section": "D.4 Experiments of Combining PACE with Other PEFT"}, {"figure_path": "cOuLbPhOT1/tables/tables_21_1.jpg", "caption": "Table 16: Hyperparameters for baseline on VTAB-1K with ViT-B/16. A: LoRAmul+VPTadd, B: LoRAadd. lr: learning rate. WD: weight decay.", "description": "This table shows the hyperparameter settings used for the baseline models (LoRAmul+VPTadd and LoRAadd) on the VTAB-1K benchmark using the ViT-B/16 architecture.  It specifies the rank, learning rate, and weight decay for each of the 19 datasets in VTAB-1K, categorized into Natural, Specialized, and Structured sets. The table also indicates which baseline model (A or B) these hyperparameters are for, enabling a better understanding of model variations across different datasets.", "section": "4 Experiments"}, {"figure_path": "cOuLbPhOT1/tables/tables_21_2.jpg", "caption": "Table 2: Classification accuracy on Few-shot learning with ViT-B/16 pretrained on ImageNet-21K.", "description": "This table presents the classification accuracy results for different few-shot learning scenarios using the ViT-B/16 model pretrained on ImageNet-21K.  It compares the performance of different methods (LoRAadd, VPTadd, and LoRAmul+VPTadd) with and without the PACE technique. The results are broken down by the number of shots (1, 2, 4, 8, 16) and across five fine-grained datasets (FGVC-Aircraft, Food101, OxfordFlowers102, OxfordPets, and StanfordCars). The \"Average\" column shows the average accuracy across all datasets for each method and shot number.", "section": "4 Experiments"}, {"figure_path": "cOuLbPhOT1/tables/tables_21_3.jpg", "caption": "Table 18: Hyperparameters for the baseline LoRAmul+VPTadd in FGVC.", "description": "This table lists the hyperparameters used for the LoRAmul+VPTadd baseline model in the FGVC (Fine-Grained Visual Categorization) experiments.  It shows the learning rate, weight decay, and rank used for each of the five fine-grained datasets included in the FGVC benchmark: CUB-200-2011, NABirds, OxfordFlowers, StanfordDogs, and StanfordCars.  The \"Mean Parameter (M)\" column indicates the average number of trainable parameters across all datasets for this baseline configuration.", "section": "4 Experiments"}, {"figure_path": "cOuLbPhOT1/tables/tables_21_4.jpg", "caption": "Table 19: Hyperparameters for baseline LoRAmul+VPTadd in domain adaptation.", "description": "This table shows the hyperparameter settings used for the LoRAmul+VPTadd baseline model in the domain adaptation experiments.  It includes the rank, learning rate, and weight decay values used for different tasks, along with the total number of trainable parameters (in millions). These settings were determined through a process of grid search to optimize performance.", "section": "4 Experiments"}]