[{"figure_path": "ZYrZ5V84ZI/figures/figures_1_1.jpg", "caption": "Figure 1: AR and VR scenarios usually involve complex scenes with multiple objects. Users may interested in only one specific object and gaze is the most natural way to interact with the device.", "description": "This figure shows example scenarios using AR and VR devices.  It highlights that real-world scenes are often complex, containing multiple objects. Users typically focus their attention on a single object of interest at a time.  The figure suggests that gaze tracking is a more natural and intuitive way for users to interact with such devices compared to other input methods.", "section": "2 Leveraging Trace Data as an Alternative Approach to Align VLMs with Gaze Attention"}, {"figure_path": "ZYrZ5V84ZI/figures/figures_2_1.jpg", "caption": "Figure 2: EMD between the mean heatmaps of 1k gaze and trace samples with varying sampling rates.", "description": "This figure shows the Earth Mover's Distance (EMD) between the average heatmaps of 1,000 gaze and trace samples at different sampling rates.  The EMD measures the difference between two probability distributions, in this case representing the similarity between gaze and mouse trace data. The x-axis represents the sampling rate, and the y-axis represents the EMD. A lower EMD indicates greater similarity. The graph shows that the EMD has a local minimum around a sampling rate of 25, suggesting that this sampling rate effectively transforms the trace data to be more similar to gaze data. This finding supports using mouse trace data as a proxy for gaze data in training vision-language models.", "section": "2 Leveraging Trace Data as an Alternative Approach to Align VLMs with Gaze Attention"}, {"figure_path": "ZYrZ5V84ZI/figures/figures_3_1.jpg", "caption": "Figure 3: Automatic Data Annotation Pipeline", "description": "This figure illustrates the pipeline for automatically annotating data using GPT-4. It starts with 100 seed samples from the LN-COCO dataset.  The prompt design is iteratively refined (10 iterations) using manual prompt tuning and GPT-4 to ensure helpful, diverse, and well-formatted grounded QA pairs are created. Once the prompt is finalized, the pipeline processes 25k image pairs from LN-COCO, using the refined prompt and GPT-4 to generate raw grounded QA pairs. A post-processing stage filters these pairs based on reward score and keywords, resulting in the final VOILA-COCO dataset of localized QA pairs, incorporating trace data alignment.", "section": "3 Method"}, {"figure_path": "ZYrZ5V84ZI/figures/figures_5_1.jpg", "caption": "Figure 4: Architecture of the VOILA Model: On the left, gaze fixation is transformed into a heatmap, which is subsequently processed through linear layers to encode the visual attention. This encoded data is then segmented into discrete patches that are spatially correlated with corresponding image patches. These gaze patches are further refined into key embeddings, which undergo modulation by a gating mechanism, designed to incrementally integrate gaze data. The resulting gaze and image key embeddings are then combined and subjected to a self-attention mechanism, which synthesizes the information into a cohesive set of latent perceiver embeddings. On the right, the figure delineates the integration pathway where the gaze heatmap and the image concurrently enter the VOILA Perceiver. This integrated input is subsequently directed through gated cross-attention modules before progressing into the language model layers, culminating in a unified output that encapsulates the interplay between visual attention and linguistic processing.", "description": "This figure shows the architecture of the VOILA model. The left side illustrates how gaze fixation is processed into key embeddings that are integrated with image patches. The right side shows how the combined gaze and image information is processed through gated cross-attention modules and language model layers to produce a unified output reflecting both visual and linguistic information.", "section": "3.3 Model Design"}, {"figure_path": "ZYrZ5V84ZI/figures/figures_7_1.jpg", "caption": "Figure 5: GPT-RANKING ON VOILA-COCO-Testset", "description": "This figure presents a bar chart comparing the performance of three vision-language models: VOILA, Otter, and Kosmos-2, on the VOILA-COCO-Testset. The chart shows the percentage of wins, ties, and losses for each model across three evaluation metrics: Overall, Helpful, and Grounding.  The results illustrate VOILA's superior performance, particularly in terms of helpfulness and grounding capabilities, compared to the baseline models.", "section": "4.2 Main Results"}, {"figure_path": "ZYrZ5V84ZI/figures/figures_7_2.jpg", "caption": "Figure 6: GPT-RANKING ON VOILA-GAZE", "description": "The figure shows the results of GPT-RANKING on the VOILA-GAZE test set.  It compares the performance of Voila-A against two baseline models (Otter and Kosmos-2) across three aspects: overall performance, helpfulness, and grounding. The bar chart displays the percentage of wins, ties, and losses for each comparison.", "section": "4.2 Main Results"}, {"figure_path": "ZYrZ5V84ZI/figures/figures_15_1.jpg", "caption": "Figure 7: Different Scenarios in the Future", "description": "This figure shows four different scenarios where a user wearing a head-mounted display interacts with a vision-language model (VLM) by making a gaze-based query.  Each scenario showcases the system's ability to understand and respond accurately to the user's gaze focus and intent. The scenarios range from simple object identification (e.g., determining the color of cakes) to more complex tasks involving user needs or intentions. The responses from the VLM demonstrate natural language understanding and real-world applicability.", "section": "C Gaze Data Collection"}, {"figure_path": "ZYrZ5V84ZI/figures/figures_16_1.jpg", "caption": "Figure 8: Qualitative Case Study: Top: We show successful predictions of all models. Middle: We demonstrate the problems of baseline models compared with VOILA including coreference queries, gaze grounding methods, etc. Bottom: We display hard challenges for all models.", "description": "This figure shows a qualitative case study comparing the performance of three different models (Otter, Kosmos-2, and Voila) on various types of questions.  The top row shows examples where all models perform well. The middle row highlights scenarios where baseline models struggle with coreference and gaze grounding, while Voila performs well. The bottom row presents challenging scenarios where even Voila faces difficulties.", "section": "4.4 Qualitative studies"}, {"figure_path": "ZYrZ5V84ZI/figures/figures_17_1.jpg", "caption": "Figure 9: Data sample from VOILA-GAZE", "description": "This figure shows example images from the VOILA-GAZE dataset.  The images depict real-life scenarios captured using a gaze-tracking device. Each image shows a grocery store shelf with various fruits.  The gaze data is overlaid onto the images as points indicating where the user was looking while asking the corresponding questions.", "section": "3.2 VOILA-GAZE: Real-life gaze-QA pairs"}, {"figure_path": "ZYrZ5V84ZI/figures/figures_17_2.jpg", "caption": "Figure 9: Data sample from VOILA-GAZE", "description": "This figure shows three example images from the VOILA-GAZE dataset. Each image shows a scene from a real-world setting (supermarket, museum, and home) and includes gaze data. The gaze data is represented by circles around specific locations that the user was focusing on.", "section": "3.2 VOILA-GAZE: Real-life gaze-QA pairs"}, {"figure_path": "ZYrZ5V84ZI/figures/figures_17_3.jpg", "caption": "Figure 14: Annotated Example of VOILA-COCO", "description": "This figure shows example annotations from the VOILA-COCO dataset. Each image shows an egocentric view with gaze points overlaid.  The captions and questions demonstrate how gaze data is used to generate relevant and contextually appropriate questions and answers.  It highlights the integration of image, gaze, and language to simulate real-world human-AI interactions.", "section": "3.1 Automatic Data Annotation For LN-COCO"}, {"figure_path": "ZYrZ5V84ZI/figures/figures_17_4.jpg", "caption": "Figure 10: Visualizations of VOILA-COCO: direct questions", "description": "This figure visualizes the word count distribution and length distribution of direct questions from the VOILA-COCO dataset.  The wordcloud (a) shows the most frequent words used in the direct questions, giving insight into the common topics and vocabulary.  The histogram (b) displays the frequency of questions with different word counts, showing the distribution of question lengths in the dataset.", "section": "D.1 Direct Questions"}, {"figure_path": "ZYrZ5V84ZI/figures/figures_18_1.jpg", "caption": "Figure 11: Visualizations of VOILA-COCO: indirect questions", "description": "This figure visualizes the indirect questions from the VOILA-COCO dataset.  It shows two sub-figures: (a) Wordcloud, which displays a visual representation of the frequency of words used in the indirect questions; and (b) Length Distribution, which presents a histogram showing the distribution of the number of words per indirect question.", "section": "D.2 Indirect Questions"}, {"figure_path": "ZYrZ5V84ZI/figures/figures_18_2.jpg", "caption": "Figure 14: Annotated Example of VOILA-COCO", "description": "This figure shows several annotated examples from the VOILA-COCO dataset.  Each example includes an image, a gaze heatmap, a caption describing the image, a question about the image, an indirect version of the question (more conversational), and a generated answer. The examples demonstrate the pipeline's ability to generate accurate and relevant answers based on the image and gaze information.", "section": "3.1 Automatic Data Annotation For LN-COCO"}, {"figure_path": "ZYrZ5V84ZI/figures/figures_20_1.jpg", "caption": "Figure 14: Annotated Example of VOILA-COCO", "description": "This figure shows several examples from the VOILA-COCO dataset. Each example consists of an image, a gaze heatmap, a caption generated by a human, a question asked by the human, an indirect question that reflects the human's gaze focus, and finally, an answer to both the original question and the indirect question.  The examples illustrate the diverse scenarios and complexities covered in the dataset.", "section": "3.1 Automatic Data Annotation For LN-COCO"}, {"figure_path": "ZYrZ5V84ZI/figures/figures_21_1.jpg", "caption": "Figure 15: GPT-RANKING Procedure", "description": "The figure shows the pseudocode for the GPT-4 ranking procedure used to evaluate the models.  The procedure takes as input a list of keys corresponding to question-answer pairs. For each key, it retrieves the responses from two models, model1 and model2. It then constructs two prompts for GPT-4, one with answer1 preceding answer2, and the other with answer2 preceding answer1. The GPT-4 responses are used to compute a score, which is then appended to a list of scores. The final list of scores represents the GPT-4 ranking evaluation of the two models.", "section": "4.1 Evaluation metrics"}, {"figure_path": "ZYrZ5V84ZI/figures/figures_22_1.jpg", "caption": "Figure 4: Architecture of the VOILA Model: On the left, gaze fixation is transformed into a heatmap, which is subsequently processed through linear layers to encode the visual attention. This encoded data is then segmented into discrete patches that are spatially correlated with corresponding image patches. These gaze patches are further refined into key embeddings, which undergo modulation by a gating mechanism, designed to incrementally integrate gaze data. The resulting gaze and image key embeddings are then combined and subjected to a self-attention mechanism, which synthesizes the information into a cohesive set of latent perceiver embeddings. On the right, the figure delineates the integration pathway where the gaze heatmap and the image concurrently enter the VOILA Perceiver. This integrated input is subsequently directed through gated cross-attention modules before progressing into the language model layers, culminating in a unified output that encapsulates the interplay between visual attention and linguistic processing.", "description": "This figure illustrates the architecture of the VOILA model, showing how gaze information is integrated with image features to improve vision-language model performance.  The left side depicts the processing of gaze data into key embeddings, which are then combined with image embeddings and fed into a self-attention mechanism. The right side details how this combined information is integrated into the language model to generate a unified output.", "section": "3.3 Model Design"}]