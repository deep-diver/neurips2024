[{"figure_path": "5iUxMVJVEV/tables/tables_6_1.jpg", "caption": "Table 1: Long-term forecasting task. The results are averaged from four different series length {96, 192, 336, 720}. See Table 13 and 14 for full results. Red: best, Blue: second best.", "description": "This table presents the results of the long-term forecasting experiments on eight real-world benchmark datasets.  The metrics used are Mean Squared Error (MSE) and Mean Absolute Error (MAE).  The results are averaged across four different forecasting lengths (96, 192, 336, and 720 time steps). The table compares the Peri-midFormer model against several other state-of-the-art models and highlights the best and second-best performing models in red and blue, respectively.", "section": "4.2 Long-term Forecasting"}, {"figure_path": "5iUxMVJVEV/tables/tables_7_1.jpg", "caption": "Table 2: Short-term forecasting task on M4. The prediction lengths are in {6, 48} and results are weighted averaged from several datasets under different sample intervals. (* means former, Station means the Non-stationary Transformer.) See Table 12 for full results. Red: best, Blue: second best.", "description": "This table presents the results of short-term forecasting experiments on the M4 dataset.  It compares the performance of Peri-midFormer against various baseline methods across different prediction lengths (6 and 48). The performance metric used is a weighted average of SMAPE, MASE, and OWA, calculated across multiple datasets with varying sampling intervals.  The table highlights Peri-midFormer's competitive performance and indicates the best and second-best performing models for each metric.", "section": "4.3 Short-term Forecasting"}, {"figure_path": "5iUxMVJVEV/tables/tables_8_1.jpg", "caption": "Table 3: Imputation task. We randomly mask {12.5%, 25%, 37.5%, 50%} time points of length-96 time series. The results are averaged from 4 different mask ratios. (* means former, Station means the Non-stationary Transformer.) See Table 15 for full results. Red: best, Blue: second best.", "description": "This table presents the results of the imputation task, comparing the performance of Peri-midFormer against other baselines.  Different levels of randomly masked data points (12.5%, 25%, 37.5%, and 50%) were used to evaluate the robustness of the methods. The results are averaged over four different mask ratios. The metrics used are Mean Squared Error (MSE) and Mean Absolute Error (MAE).  The best and second-best results are highlighted in red and blue, respectively.", "section": "4.5 Imputation"}, {"figure_path": "5iUxMVJVEV/tables/tables_8_2.jpg", "caption": "Table 16: Full results for the anomaly detection task. The P, R and F1 represent the precision, recall and F1-score (%) respectively. F1-score is the harmonic mean of precision and recall. A higher value of P, R and F1 indicates a better performance. (Station means the Non-stationary Transformer.) The standard deviation is within 1%. We copied the results from GPT4TS [20]. Red: best, Blue: second best.", "description": "This table presents the anomaly detection results on five datasets (SMD, MSL, SMAP, SWaT, and PSM).  For each dataset and each model, the table shows the precision (P), recall (R), and F1-score (harmonic mean of precision and recall).  Higher values for P, R, and F1 indicate better performance. The table compares the proposed Peri-midFormer to various baselines, highlighting its performance relative to the state-of-the-art.", "section": "4.6 Time Series Anomaly Detection"}, {"figure_path": "5iUxMVJVEV/tables/tables_8_3.jpg", "caption": "Table 5: Long-term forecasting task. The results are averaged from four different series length {96, 192, 336, 720}. See Table 13 and 14 for full results. Red: best, Blue: second best.", "description": "This table presents the results of long-term forecasting experiments conducted on eight datasets with four different prediction lengths (96, 192, 336, and 720).  The results are averaged across these lengths.  The table compares the performance of Peri-midFormer against various baselines using the Mean Squared Error (MSE) metric.  Red and blue highlight the best and second-best performing methods for each dataset, respectively.", "section": "4.2 Long-term Forecasting"}, {"figure_path": "5iUxMVJVEV/tables/tables_17_1.jpg", "caption": "Table 6: Dataset descriptions. The dataset size is organized in (Train, Validation, Test).", "description": "This table provides detailed information about the datasets used in the paper's experiments.  For each dataset, it lists the task it was used for (forecasting, imputation, classification, or anomaly detection), the number of dimensions (Dim), the series length, the dataset size (number of samples in train, validation, and test sets), and a description of the data and its frequency.", "section": "C Dataset Details"}, {"figure_path": "5iUxMVJVEV/tables/tables_18_1.jpg", "caption": "Table 7: Experiment configuration of Peri-midFormer. All the experiments use the ADAM [51] optimizer with the default hyperparameter configuration for (\u03b2\u2081, \u03b2\u2082) as (0.9, 0.999).", "description": "This table details the hyperparameters used in the Peri-midFormer model for each of the five tasks examined in the paper: long-term forecasting, short-term forecasting, imputation, classification, and anomaly detection.  It shows the range of values considered for the hyperparameters *k*, number of layers, *d<sub>model</sub>*, and learning rate (LR). The loss function, batch size, and number of epochs used during training are also specified for each task.", "section": "Experimental Details"}, {"figure_path": "5iUxMVJVEV/tables/tables_21_1.jpg", "caption": "Table 8: Complexity and scalability experiments in the long-term forecasting of length 720 on Electricity", "description": "This table presents the computational cost and time efficiency of different models on the Electricity dataset for long-term forecasting with a prediction length of 720.  It compares Peri-midFormer against several baselines, including Time-LLM, GPT4TS, PatchTST, TimesNet, DLinear, and Autoformer.  The metrics shown include FLOPS (floating-point operations) for both training and testing, GPU and CPU memory usage during training and testing, training time, testing time per sample, and the Mean Squared Error (MSE).  This allows for a comparison of computational efficiency and prediction accuracy across different models.", "section": "6 Complexity Analysis"}, {"figure_path": "5iUxMVJVEV/tables/tables_21_2.jpg", "caption": "Table 8: Complexity and scalability experiments in the long-term forecasting of length 720 on Electricity", "description": "This table presents the results of complexity and scalability experiments conducted on the Electricity dataset for long-term forecasting with a prediction length of 720.  It compares Peri-midFormer against several other methods, showing the training and test FLOPS, GPU and CPU memory usage, training and test times, and the resulting MSE (Mean Squared Error).  The table helps to demonstrate the computational efficiency and scalability of Peri-midFormer relative to other approaches.", "section": "6 Complexity Analysis"}, {"figure_path": "5iUxMVJVEV/tables/tables_23_1.jpg", "caption": "Table 10: Ablation Experiments of pre-interpolation in imputation task on ECL dataset.", "description": "This table presents the results of ablation experiments conducted to evaluate the impact of pre-interpolation on the imputation task using the ECL dataset.  It compares the performance of Peri-midFormer and several baseline models (TimesNet, Pyraformer, DLinear, PatchTST, and ETSformer) with and without pre-interpolation.  The results are reported for four different mask ratios (0.125, 0.25, 0.375, and 0.5), indicating varying levels of missing data.  The metrics used for evaluation are MSE and MAE.", "section": "5 Ablations"}, {"figure_path": "5iUxMVJVEV/tables/tables_26_1.jpg", "caption": "Table 11: Full results for the classification task. (* means former, T-LLM means Time-LLM, GPT means GPT4TS, T-Net means TimesNet, Patch means PatchTST, Light means LightTS, Station means the Non-stationary Transformer.) We report the classification accuracy (%) as the result. The standard deviation is within 1%. We reproduced the results of PatchTST by https://github.com/thuml/Time-Series-Library, reproduced TSLANet by https://github.com/emadeldeen24/TSLANet, and copied the others from GPT4TS [20]. Red: best, Blue: second best.", "description": "This table presents the classification accuracy achieved by Peri-midFormer and several other baseline methods across ten benchmark datasets from the UEA archive. The table highlights Peri-midFormer's superior performance compared to other methods.  The accuracy scores represent the average across three repetitions of each experiment, with standard deviations within 1%. Note that results for PatchTST and TSLANet were reproduced using publicly available code, while results for other methods were taken directly from the GPT4TS paper.", "section": "4.1 Main Results"}, {"figure_path": "5iUxMVJVEV/tables/tables_26_2.jpg", "caption": "Table 1: Long-term forecasting task. The results are averaged from four different series length {96, 192, 336, 720}. See Table 13 and 14 for full results. Red: best, Blue: second best.", "description": "This table presents the results of the long-term forecasting experiments on eight datasets with four different forecasting lengths.  The metrics used to evaluate the performance are Mean Squared Error (MSE) and Mean Absolute Error (MAE). The table shows the average performance across the four forecasting lengths and highlights the best and second-best performing models in red and blue, respectively.  More detailed results can be found in Tables 13 and 14.", "section": "4.2 Long-term Forecasting"}, {"figure_path": "5iUxMVJVEV/tables/tables_27_1.jpg", "caption": "Table 13: Full results of 512 look-back window length in long-term forecasting task (since FITS defaults to a look-back window length of 720, its results at 720 length are attached at the end). The standard deviation is within 0.5%. We copied the results of GPT4TS from GPT4TS [20], Time-LLM from TSLANet [27], reproduced TSLANet by https://github.com/emadeldeen24/TSLANet, and reproduced the others by https://github.com/thuml/Time-Series-Library. Red: best, Blue: second best.", "description": "This table presents the results of long-term forecasting experiments using different models.  It compares the performance of Peri-midFormer against several baselines across eight datasets (Weather, ETTh1, ETTh2, ETTm1, ETTm2, Electricity, Traffic, Exchange) with varying prediction lengths (96, 192, 336, 720). The evaluation metrics are Mean Squared Error (MSE) and Mean Absolute Error (MAE).  The results are averaged across four prediction lengths, and the best and second-best performances are highlighted.", "section": "4.2 Long-term Forecasting"}, {"figure_path": "5iUxMVJVEV/tables/tables_28_1.jpg", "caption": "Table 13: Full results of 512 look-back window length in long-term forecasting task (since FITS defaults to a look-back window length of 720, its results at 720 length are attached at the end). The standard deviation is within 0.5%. We copied the results of GPT4TS from GPT4TS [20], Time-LLM from TSLANet [27], reproduced TSLANet by https://github.com/emadeldeen24/TSLANet, and reproduced the others by https://github.com/thuml/Time-Series-Library. Red: best, Blue: second best.", "description": "This table presents the performance comparison of Peri-midFormer against other state-of-the-art models on long-term forecasting tasks using a lookback window of 512 time steps.  The table shows the mean squared error (MSE) and mean absolute error (MAE) for various datasets and prediction lengths (96, 192, 336, and 720).  The best and second-best results are highlighted, indicating Peri-midFormer's competitive performance. The results for FITS, which uses a lookback window of 720, are also included.", "section": "4.2 Long-term Forecasting"}, {"figure_path": "5iUxMVJVEV/tables/tables_29_1.jpg", "caption": "Table 15: Full results for the imputation task. We randomly mask 12.5%, 25%, 37.5% and 50% time points to compare the model performance under different missing degrees. (* means former, Station means the Non-stationary Transformer.) The standard deviation is within 0.5%. We reproduced the results of Pyraformer by https://github.com/thuml/Time-Series-Library, and copied the others from GPT4TS [20]. Red: best, Blue: second best.", "description": "This table presents the results of imputation task on six datasets (ETTm1, ETTm2, ETTh1, ETTh2, Electricity, and Weather) under four mask ratios (12.5%, 25%, 37.5%, and 50%). The results are averaged across all mask ratios. The table compares the performance of Peri-midFormer with other state-of-the-art methods (GPT4TS, TimesNet, PatchTST, ETS*, LightTS, DLinear, FED*, Station, Auto*, Pyra*, In*, and Re*).  The metrics used to evaluate the performance are MSE and MAE.", "section": "4.5 Imputation"}, {"figure_path": "5iUxMVJVEV/tables/tables_30_1.jpg", "caption": "Table 1: Long-term forecasting task. The results are averaged from four different series length {96, 192, 336, 720}. See Table 13 and 14 for full results. Red: best, Blue: second best.", "description": "This table presents the results of long-term forecasting experiments conducted on eight real-world benchmark datasets.  The models are evaluated using Mean Squared Error (MSE) and Mean Absolute Error (MAE) metrics across four different prediction lengths (96, 192, 336, and 720). The table shows the average performance across these lengths, with the best and second-best results highlighted in red and blue, respectively.  Complete results for each prediction length are available in Tables 13 and 14.", "section": "4.2 Long-term Forecasting"}]