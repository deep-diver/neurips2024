{"references": [{"fullname_first_author": "MacKay", "paper_title": "A practical Bayesian framework for backpropagation networks", "publication_date": "1992-00-00", "reason": "This paper is foundational for Laplace approximations in Bayesian neural networks, a central topic of the current paper."}, {"fullname_first_author": "Immer", "paper_title": "Improving predictions of Bayesian neural nets via local linearization", "publication_date": "2021-00-00", "reason": "This paper introduced the linearized Laplace approximation (LLA), a key method analyzed and extended in the current work."}, {"fullname_first_author": "Daxberger", "paper_title": "Laplace redux - effortless Bayesian deep learning", "publication_date": "2021-00-00", "reason": "This paper provides a comprehensive overview of Laplace approximations for Bayesian deep learning, including the GGN approximation, relevant to the current paper's analysis."}, {"fullname_first_author": "Girolami", "paper_title": "Riemann manifold Langevin and Hamiltonian Monte Carlo methods", "publication_date": "2011-00-00", "reason": "This paper introduced Riemann manifold Langevin Monte Carlo, a sampling method relevant to the current paper's proposed Riemannian diffusion sampling approach."}, {"fullname_first_author": "Jacot", "paper_title": "Neural tangent kernel: Convergence and generalization in neural networks", "publication_date": "2018-00-00", "reason": "This paper introduced the Neural Tangent Kernel (NTK), a concept crucial for understanding the geometry of neural network parameter spaces and reparameterizations, as explored in the current work."}]}