[{"figure_path": "X2UMdvcmMo/figures/figures_0_1.jpg", "caption": "Figure 1: Results of facial parts swapping using the proposed FuseAnyPart at 512 \u00d7 512 resolution. The swapped face (central image) is generated by fusing the original face (top-left image) with three facial part reference images (bottom-left, top-right, bottom-right). Notably, FuseAnyPart can seamlessly blend facial parts from multiple reference images with significant differences in appearance, producing high-fidelity and natural-looking swapped faces.", "description": "This figure shows an example of facial part swapping using the FuseAnyPart model. The central image is a generated face, created by combining parts from four different source images: the original face (top-left), a new face and background (bottom-left), eyes (top-right), and a nose (bottom-right).  The result demonstrates FuseAnyPart's ability to blend parts from diverse sources to create a realistic and seamless composite.", "section": "Abstract"}, {"figure_path": "X2UMdvcmMo/figures/figures_3_1.jpg", "caption": "Figure 2: Illustration of FuseAnyPart. The process begins with an open-set detector identifying a facial image to obtain various facial part masks. Following this, an image encoder uses these masks and the facial image to derive the corresponding facial part feature. These facial part features and masks are then fed into the Mask-based Fusion Module to piece together a complete face in latent space. Subsequently, the consolidated feature is dispatched to the Addition-based Injection Module for fusion within the UNet of the diffusion model.", "description": "This figure illustrates the architecture of FuseAnyPart, a diffusion-driven facial parts swapping model.  The process starts with an open-set detector identifying facial parts in the input image. An image encoder then extracts features for each part, guided by the masks generated by the detector. These features are fused in the Mask-based Fusion Module to create a complete latent representation of the face. Finally, this representation is passed to an Addition-based Injection Module to refine the feature map, and finally the UNet diffusion model to generate the final facial image.", "section": "3.2 Overview"}, {"figure_path": "X2UMdvcmMo/figures/figures_6_1.jpg", "caption": "Figure 3: Qualitative comparison of eyes swapping. Our method produces high-fidelity results that maintain the consistency of facial features while ensuring a natural appearance.", "description": "This figure presents a qualitative comparison of eye swapping results from different methods: Stable Diffusion, IP-Adapter, FacePartsSwap, E4S, DiffSwap, and the proposed FuseAnyPart.  The comparison highlights that FuseAnyPart produces results with high fidelity and natural appearance, maintaining consistency in facial features better than other methods.  The other methods show artifacts, inconsistencies, or unnatural appearance in the swapped eyes.", "section": "4.1 Qualitative Comparisons"}, {"figure_path": "X2UMdvcmMo/figures/figures_6_2.jpg", "caption": "Figure 1: Results of facial parts swapping using the proposed FuseAnyPart at 512 \u00d7 512 resolution. The swapped face (central image) is generated by fusing the original face (top-left image) with three facial part reference images (bottom-left, top-right, bottom-right). Notably, FuseAnyPart can seamlessly blend facial parts from multiple reference images with significant differences in appearance, producing high-fidelity and natural-looking swapped faces.", "description": "This figure shows the results of facial parts swapping using the proposed method FuseAnyPart.  The central image is the result of swapping parts from three different faces onto the original face (top left).  This demonstrates the algorithm's ability to seamlessly integrate facial parts with significant appearance differences to create a natural-looking result.", "section": "Abstract"}, {"figure_path": "X2UMdvcmMo/figures/figures_7_1.jpg", "caption": "Figure 3: Qualitative comparison of eyes swapping. Our method produces high-fidelity results that maintain the consistency of facial features while ensuring a natural appearance.", "description": "This figure compares the results of eye swapping using several different methods: Stable Diffusion, IP-Adapter, FacePartsSwap, E4S, DiffSwap, and the authors' proposed method, FuseAnyPart.  Each row shows a different source image for the eyes being swapped onto the same target face. The goal is to see which method produces the most realistic and seamless results, preserving the consistency of facial features and a natural appearance.  The authors' method aims to show superior results.", "section": "4.1 Qualitative Comparisons"}, {"figure_path": "X2UMdvcmMo/figures/figures_7_2.jpg", "caption": "Figure 1: Results of facial parts swapping using the proposed FuseAnyPart at 512 \u00d7 512 resolution. The swapped face (central image) is generated by fusing the original face (top-left image) with three facial part reference images (bottom-left, top-right, bottom-right). Notably, FuseAnyPart can seamlessly blend facial parts from multiple reference images with significant differences in appearance, producing high-fidelity and natural-looking swapped faces.", "description": "This figure shows example results of facial parts swapping using the FuseAnyPart model.  The input consists of a target face and three reference images, each providing a different facial part (eyes, nose, mouth). FuseAnyPart blends these parts seamlessly onto the target face, resulting in a natural and high-fidelity image. The figure highlights the ability of the model to handle significant differences in appearance between the reference images.", "section": "Abstract"}, {"figure_path": "X2UMdvcmMo/figures/figures_9_1.jpg", "caption": "Figure 7: Qualitative comparison of different ablative settings.", "description": "This figure compares the results of several ablation studies on the FuseAnyPart model.  Specifically, it shows the effect of different methods for integrating image features into the UNet of the diffusion model, including using cross-attention, multiple cross-attention, and a proposed addition-based injection method. The results demonstrate the superiority of the addition-based injection approach in terms of image quality and facial feature preservation.", "section": "4.3 Ablation Study"}, {"figure_path": "X2UMdvcmMo/figures/figures_12_1.jpg", "caption": "Figure 3: Qualitative comparison of eyes swapping. Our method produces high-fidelity results that maintain the consistency of facial features while ensuring a natural appearance.", "description": "This figure compares the performance of various face swapping methods, specifically focusing on eye replacement. It shows the original face and the results of StableDiffusion, IP-Adapter, FacePartsSwap, E4S, DiffSwap, and the proposed FuseAnyPart method. The comparison highlights FuseAnyPart's ability to produce high-fidelity results that preserve facial features and create a natural look.", "section": "4.1 Qualitative Comparisons"}, {"figure_path": "X2UMdvcmMo/figures/figures_12_2.jpg", "caption": "Figure 13: Comparison with DiffFace. DiffFace generates images with local distortions in the eyes and mouth, whereas our method produces cleaner results that are more similar to the source image regarding the facial parts.", "description": "This figure compares the results of facial part swapping using the proposed method, FuseAnyPart, against DiffFace.  The comparison highlights the superior quality of FuseAnyPart, showing that FuseAnyPart produces more natural-looking results with fewer distortions, particularly around the eyes and mouth, making the swapped parts blend more seamlessly with the original image.", "section": "4.1 Qualitative Comparisons"}, {"figure_path": "X2UMdvcmMo/figures/figures_12_3.jpg", "caption": "Figure 11: Illustrations of facial parts from significantly different racial and age groups. Facial part swapping between source and target images that significantly differ in age and race.", "description": "This figure shows the results of facial part swapping where the source and target images have significant differences in age and race.  It demonstrates the ability of the FuseAnyPart method to handle such diverse inputs, showcasing its robustness and generalization capabilities. The results highlight the method's capacity to seamlessly blend facial parts from various individuals, regardless of variations in age and ethnicity.", "section": "4.1 Qualitative Comparisons"}, {"figure_path": "X2UMdvcmMo/figures/figures_13_1.jpg", "caption": "Figure 11: Illustrations of facial parts from significantly different racial and age groups. Facial part swapping between source and target images that significantly differ in age and race.", "description": "This figure shows examples of facial part swapping where the source and target images have significant differences in age and race.  The results demonstrate the ability of the FuseAnyPart model to handle such variations.  The top row shows a young Asian's facial features swapped onto an older Black person's face, and the bottom row presents the opposite scenario.", "section": "4.1 Qualitative Comparisons"}, {"figure_path": "X2UMdvcmMo/figures/figures_13_2.jpg", "caption": "Figure 1: Results of facial parts swapping using the proposed FuseAnyPart at 512 \u00d7 512 resolution. The swapped face (central image) is generated by fusing the original face (top-left image) with three facial part reference images (bottom-left, top-right, bottom-right). Notably, FuseAnyPart can seamlessly blend facial parts from multiple reference images with significant differences in appearance, producing high-fidelity and natural-looking swapped faces.", "description": "This figure shows the results of facial part swapping using the FuseAnyPart method.  The central image is the result of combining the original face (top-left) with three different reference images for the eyes, nose, and mouth. It highlights FuseAnyPart's ability to seamlessly blend parts from various sources, even with significant differences in appearance, generating realistic and high-quality results.", "section": "Abstract"}, {"figure_path": "X2UMdvcmMo/figures/figures_14_1.jpg", "caption": "Figure 14: Qualitative results of swapping face parts from different sources to a target face.", "description": "This figure shows the results of swapping different facial parts (eyes, nose, and mouth) from various source images onto the same target face.  The results demonstrate the model's ability to seamlessly blend features from multiple sources while maintaining a cohesive and natural-looking result. Each row represents a different target face and different source images used to swap the facial parts.", "section": "4.1 Qualitative Comparisons"}]