{"references": [{"fullname_first_author": "W.-C. Kang", "paper_title": "Self-attentive sequential recommendation", "publication_date": "2018-00-00", "reason": "This paper introduces the self-attention mechanism into sequential recommendation, a core component of the proposed LLM-ESR framework."}, {"fullname_first_author": "B. Hidasi", "paper_title": "Session-based recommendations with recurrent neural networks", "publication_date": "2016-00-00", "reason": "This paper pioneers the use of recurrent neural networks in session-based recommendation, providing a strong foundation for many subsequent works in sequential recommendation."}, {"fullname_first_author": "J. D. M.-W. C. Kenton", "paper_title": "Bert: Pre-training of deep bidirectional transformers for language understanding", "publication_date": "2019-00-00", "reason": "This paper introduces BERT, a highly influential pre-trained language model, which is the base for many LLMs that the proposed LLM-ESR leverages for semantic information."}, {"fullname_first_author": "S. Jang", "paper_title": "CITIES: Contextual inference of tail-item embeddings for sequential recommendation", "publication_date": "2020-00-00", "reason": "This paper directly addresses the long-tail challenge in sequential recommendation, which is a central problem that LLM-ESR aims to solve."}, {"fullname_first_author": "K. Kim", "paper_title": "MELT: Mutual enhancement of long-tailed user and item for sequential recommendation", "publication_date": "2023-00-00", "reason": "This paper also tackles the long-tail challenge and provides another strong baseline against which LLM-ESR's performance is compared."}]}