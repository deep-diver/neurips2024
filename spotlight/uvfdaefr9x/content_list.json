[{"type": "text", "text": "Exploring Jacobian Inexactness in Second-Order Methods for Variational Inequalities: Lower Bounds, Optimal Algorithms and Quasi-Newton Approximations ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Artem Agafonov1, 2\u2217, Petr Ostroukhov1, 2, Roman Mozhaev2, Konstantin Yakovlev2, Eduard Gorbunov1, Martin Tak\u00e1c\u02c71, Alexander Gasnikov3,2,4, Dmitry Kamzolov1 ", "page_idx": 0}, {"type": "text", "text": "1 Mohamed bin Zayed University of Artificial Intelligence (MBZUAI), Abu Dhabi, UAE 2 Moscow Institute of Physics and Technology (MIPT), Moscow, Russia 3 Innopolis University, Innopolis, Russia 4 Skoltech, Moscow, Russia ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Variational inequalities represent a broad class of problems, including minimization and min-max problems, commonly found in machine learning. Existing second-order and high-order methods for variational inequalities require precise computation of derivatives, often resulting in prohibitively high iteration costs. In this work, we study the impact of Jacobian inaccuracy on second-order methods. For the smooth and monotone case, we establish a lower bound with explicit dependence on the level of Jacobian inaccuracy and propose an optimal algorithm for this key setting. When derivatives are exact, our method converges at the same rate as exact optimal second-order methods. To reduce the cost of solving the auxiliary problem, which arises in all high-order methods with global convergence, we introduce several Quasi-Newton approximations. Our method with Quasi-Newton updates achieves a global sublinear convergence rate. We extend our approach with a tensor generalization for inexact high-order derivatives and support the theory with experiments. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In this paper, we primarily address the problem of solving the Minty Variational Inequality (MVI) [77, 17]. Given a continuous operator $F:\\dot{\\mathcal{X}}\\rightarrow\\mathbb{R}^{d}$ , where $\\bar{\\boldsymbol{x}}\\subseteq\\mathbb{R}^{d}$ is a closed bounded convex subset with a diameter $D=\\operatorname*{max}_{x,y\\in\\mathcal{X}}\\|x-y\\|$ , the objective is to find a point $x^{*}\\in\\mathcal{X}$ such that ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\langle F(x),x-x^{*}\\rangle\\geq0,\\quad{\\mathrm{for~all~}}x\\in\\mathcal{X}.\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "The solution to (1) is referred to as a weak solution of the Variational Inequality (VI) [37]. In contrast, the Stampacchia variational inequality problem [48] consists in finding a point $x^{*}\\in\\mathcal{X}$ such that ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\langle F(x^{*}),x-x^{*}\\rangle\\geq0,\\quad{\\mathrm{for~all~}}x\\in\\mathcal{X}.\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "This solution is often called a strong solution to the variational inequality. When the operator $F$ is both continuous and monotone, the weak and strong solutions are equivalent [37]. ", "page_idx": 0}, {"type": "text", "text": "Assumption 1.1 The operator $F(x)$ is called monotone, if ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\langle F(x)-F(y),x-y\\rangle\\geq0,\\quad{\\mathrm{for}}\\,{\\mathrm{all}}\\,x,y\\in{\\mathcal{X}}.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Another useful assumption is $L_{1}$ -smoothness. ", "page_idx": 1}, {"type": "text", "text": "Assumption 1.2 The operator $F(x)$ is $L_{1}$ -smooth, if it has Lipschitz-continuous first-order derivative ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\|\\nabla F(x)-\\nabla F(y)\\|_{\\mathrm{op}}\\leq L_{1}\\|x-y\\|,\\quad{\\mathrm{for~all}}\\,x,y\\in{\\mathcal{X}}.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "First-order methods. Variational inequalities encompass a wide range of problems, including minimization [19, 90, 93, 15], min-max problems, Nash equilibrium, differential equations, and others [37, 16]. The extensive research on VI methods dates back several decades, with a notable breakthrough in the 1970s\u2014the development of the Extragradient method [62, 6]. Subsequently, it was demonstrated that this method achieves global convergence of $O\\left(\\varepsilon^{-1}\\right)$ [80, 43], matching the convergence rates of other first-order2 methods such as optimistic gradient [92, 78, 63, 44], forward-backward splitting [95], and dual extrapolation [81]. These first-order methods collectively exhibit optimal convergence [89]. ", "page_idx": 1}, {"type": "text", "text": "Second-order and high-order methods. To achieve further notable acceleration of methods for VIs, one can leverage information about higher-order derivatives. For instance, simply incorporating firstorder derivatives (Jacobian) can significantly enhance the convergence speed of the method. Following recent advancements in second-order and high-order methods with global rates for minimization [85, 82, 10, 79, 83, 46, 58, 14, 51, 64, 22], several high-order methods for VIs have been proposed [21, 68, 52, 88, 70, 84, 72, 5]. However, all these methods involve a line-search procedure, resulting in $\\tilde{O}\\left(\\varepsilon^{-2/3}\\right)$ convergence for the case of first-order information (Jacobians). Recent works [69, 2] propose methods with improved rates $O\\left(\\varepsilon^{-2/3}\\right)$ and establish the lower bound $\\Omega\\left(\\varepsilon^{-2/3}\\right)$ , rendering these algorithms optimal. ", "page_idx": 1}, {"type": "text", "text": "Jacobian\u2019s approximation. In the last decade, VIs found new applications in machine learning. There are many problems that could not be reduced to minimization, including reinforcement learning [87, 55], adversarial training [75], GANs [42, 31, 41, 76, 28, 67, 91], classical learning tasks in supervised learning [56, 9], unsupervised learning [98, 8], image denoising [35, 27], robust optimization [13]. Applying second-order methods, without even mentioning high-order ones, described in a previous paragraph to machine learning problems could be a challenging task. Although these methods may theoretically converge faster, computing exact Jacobians and the per-iteration costs can be expensive. Therefore, it seems natural to introduce inexact approximations of the first-order derivatives. In the context of minimization, several works with inexact Hessians were introduced for both convex [40, 3, 7, 4] and nonconvex [24, 25, 23, 61, 99, 94, 74, 11, 12, 34] problems. Regarding VIs, Quasi-Newton (QN) methods [73, 14, 57] can be highlighted, though they, unfortunately, achieve only local convergence in the strongly monotone case [18, 38]. These methods are relatively less advanced for VIs compared to their counterparts in the field of minimization, where they are considered classics in optimization due to their effectiveness and practicality [86]. Modern research on QN approximations for minimization includes methods that exhibit global convergence [57, 54, 53]. A recent work [71] introduces the Newton-MinMax method for convexconcave unconstrained min-max optimization problems, demonstrating an optimal rate under special assumptions on the accuracy of the Jacobian approximation. However, the field of VIs lacks globally convergent inexact second-order methods with an explicit dependence on the accuracy of the Jacobian. This raises several natural questions: ", "page_idx": 1}, {"type": "text", "text": "What are the lower bounds for methods with inexact Jacobians? Can we construct an optimal method with inexact first-order information? ", "page_idx": 1}, {"type": "text", "text": "What is the proper way to approximate the Jacobian to ensure global convergence and reduce the iteration complexity? ", "page_idx": 1}, {"type": "text", "text": "In our work, we attempt to answer these questions in a systematic manner. ", "page_idx": 1}, {"type": "text", "text": "Optimality measure. Most of our results are stated for the monotone setting (Assumption 1.1). In this context, the optimality of a point ${\\hat{x}}\\in{\\mathcal{X}}$ is typically measured by a gap function $\\operatorname{GAP}(\\cdot):\\mathcal{X}\\to$ $\\mathbb{R}_{+}$ [95, 80, 81, 78, 69], defined by ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathrm{GAP}({\\hat{x}})=\\operatorname*{sup}_{x\\in{\\mathcal{X}}}\\left\\langle F(x),{\\hat{x}}-x\\right\\rangle\\leq\\varepsilon,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\varepsilon\\ge0$ is the accuracy of solution. The boundedness of $\\mathcal{X}$ and the existence of a strong solution ensure that the gap function is well-defined. If $\\varepsilon=0$ , we get by (1) that $\\hat{x}$ is a weak solution of VI. We explore the performance of the proposed algorithm in scenarios involving nonmonotone operators $F$ . In such cases, it is essential to assume that the operator satisfies the Minty condition to ensure that the problem is computationally manageable [32]. ", "page_idx": 2}, {"type": "text", "text": "Assumption 1.3 The operator $F(x)$ satisfies Minty condition, if there exists a point $x^{*}$ such that ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\langle F(x),x-x^{*}\\rangle\\geq0,\\quad{\\mathrm{for~all~}}x\\in\\mathcal{X}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The range of applications of nonmonotone VIs satisfying Minty conditions is quite extensive [20, 29, 39, 36, 66, 60]. We note, that this condition is weaker than monotonicity [30, 50, 59] and guarantees the existence of at least one strong solution since $F$ is continuous and $\\mathcal{X}$ is closed and bounded [47]. To measure the optimality of point $\\hat{x}$ we define the residue function $\\operatorname{RES}(\\cdot):\\mathcal{X}\\to\\mathbb{R}_{+}$ [30, 50, 59] ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname{RES}({\\hat{x}})=\\operatorname*{sup}_{x\\in{\\mathcal{X}}}\\left\\langle F({\\hat{x}}),{\\hat{x}}-x\\right\\rangle\\leq\\varepsilon,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The boundedness of $\\mathcal{X}$ and the existence of a strong solution ensure that the residual function is well-defined. If $\\varepsilon=0$ , by (2), we get that $\\hat{x}$ is a strong solution of VI. ", "page_idx": 2}, {"type": "text", "text": "Contributions. The main contribution of this paper lies in the development of a new second-order method robust to inexactness in the Jacobian, a common occurrence in machine learning. We demonstrate the algorithm\u2019s optimality in the monotone case by establishing a lower bound for this key setting. Expanding further: ", "page_idx": 2}, {"type": "text", "text": "1. We introduce a novel second-order algorithm, VIJI (Second-order Method for Variational Inequalitues under Jacoibian Inexactness), designed to handle $\\delta$ -inexact3 Jacobian information. Specifically, in the context of smooth and monotone VIs, VIJI achieves a convergence rate of $\\begin{array}{r}{\\bar{O}\\left(\\frac{\\delta D^{2}}{T}+\\frac{L_{1}D^{3}}{T^{3/2}}\\right)}\\end{array}$ to find weak solution. For smooth nonmonotone VIs satisfying the Minty condition, we demonstrate a convergence rate of $\\begin{array}{r}{{\\cal O}\\left(\\frac{\\delta{\\cal D}^{2}}{\\sqrt{T}}+\\frac{{\\cal L}_{1}{\\cal D}^{3}}{T}\\right)}\\end{array}$ to identify strong solution. Notably, when \u03b4 \u2264 L\u221a1TD , our method matches the convergence rates of optimal exact second-order methods [2, 69].   \n2. We establish the optimal performance of our algorithm on monotone smooth operators by deriving a theoretical complexity lower bound of $\\begin{array}{r}{\\Omega\\left(\\frac{\\bar{\\delta}D^{2}}{T}+\\frac{L_{1}D^{3}}{T^{3/2}}\\right)}\\end{array}$ to find weak solution for the case of $\\delta$ -inexact Jacobians.   \n3. Our algorithm involves solving a variational inequality subproblem. To tackle this challenge, we introduce an approximation condition, which makes the solution computationally feasible.   \n4. We introduce a new Quasi-Newton update for approximating the Jacobian, which significantly decreases the per-iteration cost of the algorithm while maintaining a global sublinear convergence rate. Numerical experiments demonstrate the practical benefits of our method.   \n5. We extend our algorithm for higher-order VIs with inexact high-order derivatives, resulting in $\\begin{array}{r}{O\\left(\\sum_{i=1}^{p-1}\\frac{\\delta_{i}D^{i+1}}{T^{(i+1)/2}}+\\frac{L_{p-1}D^{p+1}}{T^{(p+1)/2}}\\right)}\\end{array}$ rate for monotone and smooth VIs with $\\delta_{i}$ -inexact $i$ -th derivative to find weak solution. Moreover, we extend our proposed high-order method to nonmonotone VIs.   \n6. We propose a restarted version of VIJI for strongly monotone VIs, which exhibits a linear rate. ", "page_idx": 2}, {"type": "text", "text": "Comparison with Lin, Mertikopoulos, and Jordan [71]. To the best of our knowledge, the work [71] is the most closely related to our research.The objective of [71] was to develop a method for convex-concave unconstrained min-max optimization under inexact Jacobian information with an optimal convergence rate $O(\\varepsilon^{-2/3})$ matching the lower bound $\\Omega(\\varepsilon^{-2/3})$ [69]. The authors successfully achieve this goal by proposing a second-order algorithm Inexact-Newton-MinMax method based on the Perseus [69]. To attain optimal convergence, they constrained the Jacobian inexactness with a function that decreases as the method converges and bounded the norm of Jacobian from above. While these assumptions might be suitable for randomized sampling in finite-sum and stochastic problems, they may not hold for many approximation strategies, such as Quasi-Newton algorithms. The aim of our work, however, is to study the impact of Jacobian inaccuracy on the convergence of second-order methods for VIs (a special case of which are min-max problems) and to identify the explicit dependence of the convergence rate on the inexactness. Compared to the work [71], the Jacobian inaccuracy directly affects the step sizes in our algorithm, allowing us to achieve a convergence rate of $O(\\varepsilon^{-\\dot{2}/3}\\!+\\!\\delta\\varepsilon^{\\dot{-}1})$ for any given $\\delta$ . VIJI can be viewed as a generalization of Inexact-Newton-MinMax. With the same assumption on $\\delta$ as in [71], our methods for min-max optimization are equivalent. The inexactness of the subproblem and the solution approach proposed in [71] remain valid for our method even with arbitrary large $\\delta$ . Further details about application of our method to min-max problems can be found in Appendix J. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Notation. Let $\\mathbb{R}^{d}$ be a finite-dimensional vector space with scalar product $\\langle\\cdot,\\cdot\\rangle$ . For vector $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ we denote Euclidean norm as $\\left\\|x\\right\\|$ . For $X\\in\\mathbb{R}^{d_{1}\\times\\dots\\times d_{p}}$ , we define ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{X[z^{1},\\cdot\\cdot\\cdot\\cdot,z^{p}]=\\sum_{1\\leq i_{j}\\leq d_{j},1\\leq j\\leq p}(X_{i_{1},\\cdot\\cdot\\cdot\\cdot,i_{p}})z_{i_{1}}^{1}\\cdot\\cdot\\cdot z_{i_{p}}^{p},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "and $\\|X\\|_{\\mathrm{op}}\\,=\\,\\mathrm{max}_{\\|z^{i}\\|=1,1\\leq j\\leq p}\\,X[z^{1},\\cdot\\cdot\\cdot\\,,z^{p}]$ . Fixing $p\\,\\geq\\,1$ and letting $F\\,:\\,\\mathbb{R}^{d}\\,\\rightarrow\\,\\mathbb{R}^{d}$ be a continuous and high-order differentiable operator, we define $\\nabla^{(p)}F(x)$ as the $p^{\\mathrm{th}}$ -order derivative at a point $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ . To be more precise, letting $z_{1},\\dots,z_{k}\\in\\mathbb{R}^{d}$ , we have ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nabla^{(k)}F(x)[z^{1},\\cdots,z^{k}]=\\sum_{1\\leq i_{1},\\dots,i_{k}\\leq d}\\left(\\frac{\\partial F_{i_{1}}}{\\partial x_{i_{2}}\\cdots\\partial x_{i_{k}}}(x)\\right)z_{i_{1}}^{1}\\cdot\\cdot\\cdot z_{i_{k}}^{k}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Taylor approximation and oracle feedback. The starting point for our method is the first-order Taylor polynomial of the operator $F$ at point $\\boldsymbol{v}\\,:\\,\\Phi_{v}(\\boldsymbol{x})\\:=\\:\\boldsymbol{F}(\\boldsymbol{v})\\,+\\,\\nabla F(\\boldsymbol{v})[\\boldsymbol{x}\\,-\\,\\boldsymbol{v}]$ . Since the computation of Jacobian $\\nabla F(v)$ could be a quite tiresome task, it seems natural to introduce an inexact approximation $J(v)$ . Based on it we introduce inexact Taylor approximation \u2013 one of the main building blocks of our algorithm ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\Psi_{v}(x)=F(v)+J(v)[x-v],\\quad v\\in\\mathbb{R}^{d},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $J(x)$ satisfies the following assumption. ", "page_idx": 3}, {"type": "text", "text": "Assumption 2.1 For given $v\\in\\mathcal{X}\\:\\delta$ -inexact Jacobian satisfies: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla F(v)-J(v)\\|\\leq\\delta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "As it was shown, e.g. in [52], Assumption 1.2 allows to control the quality of approximation of operator $F$ by its Taylor polynomial ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|F(x)-\\Phi_{v}(x)\\|\\le\\frac{L_{1}}{2}\\|x-v\\|^{2},\\quad x,v\\in\\mathcal{X}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The next lemma is counterpart of (9) for the case of inexact Jacobian. ", "page_idx": 3}, {"type": "text", "text": "Lemma 2.2 Let Assumptions 1.2 and 2.1 hold. Then, for any $x,v\\in\\mathcal{X}$ ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\boldsymbol{F}(\\boldsymbol{x})-\\Psi_{v}(\\boldsymbol{x})\\|\\le\\frac{L_{1}}{2}\\|\\boldsymbol{x}-\\boldsymbol{v}\\|^{2}+\\delta\\|\\boldsymbol{x}-\\boldsymbol{v}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "3 VIJI algorithm ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, extending on recent optimal high-order method for MVIs Perseus [69], we present our proposed method, dubbed as VIJI and detailed in Algorithm 1. ", "page_idx": 3}, {"type": "text", "text": "Input: initial point $x_{0}\\in\\mathcal{X}$ , parameters $L_{1}$ , $\\eta$ , sequence $\\{\\beta_{k}\\}$ , and ${\\mathsf{o p t}}\\in\\{0,1,2\\}$ .   \nInitialization: set $s_{0}=0\\in\\mathbb{R}^{d}$ .   \nfor $k=0,1,2,\\ldots,T$ do Compute $\\begin{array}{r}{v_{k+1}=\\operatorname*{argmax}_{v\\in\\mathcal{X}}\\{\\langle s_{k},v-x_{0}\\rangle-\\frac{1}{2}\\|v-x_{0}\\|^{2}\\}}\\end{array}$ . Compute $x_{k+1}\\in\\mathcal{X}$ such that condition (12) holds true. Compute $\\lambda_{k+1}$ such that $\\begin{array}{r}{\\frac{1}{32}\\leq\\lambda_{k+1}\\left(\\frac{L_{1}}{2}\\|x_{k+1}-v_{k+1}\\|+\\beta_{k+1}\\right)\\leq\\frac{1}{22}.}\\end{array}$ Compute $s_{k+1}=s_{k}-\\lambda_{k+1}F(x_{k+1})$ .   \nOutput: x\u02c6 $\\begin{array}{r}{\\mathrm{\\boldmath{~\\sigma~}}:=\\left\\{\\begin{array}{c c}{\\tilde{x}_{T}=\\frac{1}{\\sum_{k=1}^{T}\\lambda_{k}}\\sum_{k=1}^{T}\\lambda_{k}x_{k},}&{\\mathrm{if~opt=0,}}\\\\ {x_{T},}&{\\mathrm{else~if~opt=1,}}\\\\ {x_{k_{T}}\\;\\mathrm{for}\\;k_{T}=\\arg\\!\\operatorname*{min}_{1\\leq k\\leq T}\\|x_{k}-v_{k}\\|,}&{\\mathrm{else~if~opt=2.}}\\end{array}\\right.}\\end{array}$ ", "page_idx": 4}, {"type": "text", "text": "The model of objective and subproblem\u2019s solution. We begin the description of the algorithm by introducing the inexact model of objective $\\Omega_{v}^{\\eta}(x)$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Omega_{v}^{\\eta}(x)=\\Psi_{v}(x)+\\eta\\delta(x-v)+5L_{1}\\|x-v\\|(x-v),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\eta>0$ is given constant. Here, we introduced the additional regularization term $\\eta\\delta(x-v)$ . As we will demonstrate, this term is crucial for ensuring that the method\u2019s subproblem has a solution. For brevity, we use a regularization constant of $5L$ . Using larger coefficients would yield the same convergence rate. As other dual extrapolation-type methods, VIJI includes the following subproblem ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\langle\\Omega_{v_{k+1}}^{\\eta}(x_{k+1}),x-x_{k+1}\\rangle\\geq0{\\mathrm{~for~all~}}x\\in{\\mathcal{X}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "First of all, the strong solution of this VI exists because $\\Omega_{v_{k+1}}^{\\eta}(x)$ is continuous, and $\\mathcal{X}$ is a closed, bounded, and convex set. Next, we demonstrate that in a monotone setting VI (11) is also monotone. ", "page_idx": 4}, {"type": "text", "text": "Lemma 3.1 Let Assumptions (1.1), (1.2), (2.1) hold. Then for any $x,v_{k+1}\\in\\mathcal{X}\\;V I$ (11) is monotone ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{1}{2}\\left(\\nabla\\Omega_{v}(x)+\\nabla\\Omega_{v}(x)^{T}\\right)\\succeq4L_{1}\\|x-v\\|I_{d\\times d}+5L_{1}\\frac{(x-v)(x-v)^{T}}{\\|x-v\\|}+(\\eta-1)\\delta I_{d\\times d}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Following the work [69], one can find a strong solution to such VI using mirror-prox methods from [1], achieving the following approximate condition ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{x\\in\\mathcal{X}}{\\operatorname*{sup}}\\langle\\Omega_{v_{k+1}}(x_{k+1}),x_{k+1}-x\\rangle\\leq\\frac{L_{1}}{2}\\|x_{k+1}-v_{k+1}\\|^{3}+\\delta\\|x_{k+1}-v_{k+1}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This ensures that the subproblem is computationally solvable in the monotone setting. In specific cases, such as minimax optimization, other efficient subsolvers can be employed [49, 2, 71]. ", "page_idx": 4}, {"type": "text", "text": "Adaptive dual stepsizes. Adaptive stepsizes in dual space $\\lambda_{k}$ are another core aspect of the algorithm. Due to inaccuracies in the Jacobian, applying the standard adaptive strategy, such as in the Perseus algorithm [69], can lead to excessively large steps, potentially slowing down the method. To address this issue, an additional term $\\beta_{k}$ is incorporated into the adaptive strategy for selecting $\\lambda_{k}$ . Thus, when $\\|\\boldsymbol{x}_{k}-\\boldsymbol{v}_{k}\\|$ is small (indicating proximity to the optimum), $\\beta_{k}$ has a greater influence on the choice of $\\lambda_{k}$ , preventing the method from taking overly aggressive steps. Similar behavior can be observed in accelerated second-order methods for minimization with inexact Hessians from theoretical [3, Lemma 5], [4, Appendix B, Lemma E.3] and practical [4, Section 7] perspectives. ", "page_idx": 4}, {"type": "text", "text": "Convergence in monotone setting. Now, we are prepared to present the convergence theorem of Algorithm 1 in the monotone case. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.2 Let Assumptions 1.1, 1.2, 2.1 hold. Then, after $T\\geq1$ iterations of Algorithm 1 with parameters $\\beta_{k}=\\delta,\\;\\eta=10,\\;o p t=0,$ , we get the following bound ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{\\mathrm{GAP}}\\big(\\tilde{x}_{T}\\big)=\\displaystyle\\operatorname*{sup}_{x\\in\\mathcal{X}}\\,\\langle F(x),\\tilde{x}_{T}-x\\rangle=O\\left(\\frac{L_{1}D^{3}}{T^{3/2}}+\\frac{\\delta D^{2}}{T}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The upper bound (13) consists of two terms. The first one corresponds to exact convergence and matches the lower bound for second-order VI methods [69]. The second term illustrates the impact of the Jacobian\u2019s inexactness on the convergence rate, aligning with the lower bound for first-order methods [89]. We also notice that one can make the bound from (13) tighter for the so-called restricted gap-function [81] defined as $\\textstyle\\mathrm{RGAP}(y)=\\operatorname*{sup}_{x\\in{\\mathcal{C}}}\\,\\langle F(x),y-x\\rangle$ , where ${\\mathcal{C}}\\subseteq{\\mathcal{X}}$ and the solution set $\\varkappa^{*}$ of (2) satisfies $\\mathcal{X}^{\\ast}\\subseteq\\mathcal{C}$ . In particular, following similar steps as in the proof of Theorem 3.2, one can derive O L13 D/ 23 $\\begin{array}{r}{{\\cal O}\\left(\\frac{L_{1}\\widehat{{\\cal D}}^{3}}{T^{3/2}}+\\frac{\\delta\\widehat{{\\cal D}}^{2}}{T}\\right)}\\end{array}$ bound for $\\mathrm{RGAP}\\big(\\tilde{x}_{T}\\big)$ , where $\\widehat{D}=\\operatorname*{sup}_{x\\in c}\\|x-x_{0}\\|$ . ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "In the next theorem, we show that the method can achieve the optimal convergence rate of secondorder methods under additional assumption on $\\delta$ . ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.3 Let Assumptions 1.1, 1.2, 2.1 hold. Let $\\{x_{k},v_{k}\\}$ be iterates generated by Algorithm $^{\\,I}$ and ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|(\\nabla F(v_{k})-J(v_{k}))[x_{k}-v_{k}]\\|\\leq\\delta_{k}\\|x_{k}-v_{k}\\|,\\quad\\delta_{k}\\leq\\frac{L_{1}}{2}\\|x_{k}-v_{k}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Then, after $T\\geq1$ iterations of Algorithm $^{\\,I}$ with parameters $\\begin{array}{r}{\\beta_{k}=\\frac{L_{1}}{2}\\|x_{k}-v_{k}\\|}\\end{array}$ , \u03b7 = 10, opt = 0, we get the following bound ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{GAP}(\\tilde{x}_{T})=\\underset{x\\in\\mathcal{X}}{\\operatorname*{sup}}\\left\\langle F(x),\\tilde{x}_{T}-x\\right\\rangle\\leq O\\left(\\frac{L_{1}D^{3}}{T^{3/2}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Note, that condition (14) is verifiable, indicating that the method can adjust to $\\delta_{k}$ in cases of controllable inexactness. Specifically, at each iteration, we can solve subproblem (12). If the assumption regarding $\\delta_{k}$ is not met, we can improve Jacobian approximation and repeat procedure. Moreover, similarly to the previous theorem, one can tighten the above bound for $\\mathrm{RGAP}\\big(\\tilde{x}_{T}\\big)$ and get the dependence on $\\widehat{D}=\\operatorname*{sup}_{x\\in c}\\|x-x_{0}\\|$ instead of $D$ . ", "page_idx": 5}, {"type": "text", "text": "Convergence in nonmonotone setting. To begin with, in the nonmonotone case, the subproblem (11) may not exhibit monotonicity, and solving (12) becomes challenging [32]. Yet, in certain specific scenarios, such as unconstrained minimization tasks, it remains feasible to find a solution by leveraging the cubic structure of the subproblem [26]. The following theorem establishes the convergence of VIJI in the nonmonotone setting. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.4 Let Assumptions 1.2, 1.3, 2.1 hold. Then after $T\\geq1$ iterations of Algorithm 1 with parameters $\\beta_{k}=\\delta,\\eta=10$ , $o p t=2$ we get the following bound ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathtt{R E S}(\\hat{x})=\\operatorname*{sup}_{x\\in\\mathcal{X}}\\left\\langle F(\\hat{x}_{T}),\\hat{x}_{T}-x\\right\\rangle=O\\left(\\frac{L_{1}D^{3}}{T}+\\frac{\\delta D^{2}}{\\sqrt{T}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The convergence rate could be improved by additional assumption on inexact Jacobian. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.5 Let Assumptions 1.2, 1.3, 2.1 hold. Let $\\{x_{k},v_{k}\\}$ be iterates generated by Algorithm $^{\\,l}$ that satisfy (14). Then after $T\\geq1$ iterations of Algorithm $^{\\,l}$ with parameters $\\begin{array}{r}{\\beta_{k}=\\frac{L}{2}\\|x_{k}-v_{k}\\|}\\end{array}$ , $\\eta=$ 1 $.0,o p t=2\\,w e$ get the following bound ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{RES}(\\hat{x})=\\underset{x\\in\\mathcal{X}}{\\operatorname*{sup}}\\left\\langle F(\\hat{x}_{T}),\\hat{x}_{T}-x\\right\\rangle=O\\left(\\frac{L_{1}D^{3}}{T}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "4 The lower bound ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we establish a theoretical lower bound for the complexity of first-order algorithms using inexact Jacobians for monotone MVIs. The proof technique draws inspiration from works [33, 4] and based on lower bounds from [69, 89]. ", "page_idx": 5}, {"type": "text", "text": "We start by describing the available information and the method\u2019s structure. The considered class of algorithms relies on data provided by a first-order $\\delta$ -inexact oracle, denoted as $\\mathcal{O}:\\mathcal{X}\\to\\mathbb{R}^{d}\\times\\mathbb{R}^{d\\times d}$ . Given a point $\\bar{x}\\in\\mathcal{X}$ , the oracle returns ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{O}(\\bar{x})=(F(\\bar{x}),J(\\bar{x}))\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The method is able to generate points $\\{x_{k}\\}_{k\\ge0}$ that satisfy the following condition ", "page_idx": 5}, {"type": "text", "text": "where $\\Omega_{\\bar{x}}(h)=a_{1}F(\\bar{x})+a_{2}J(\\bar{x})[h]+b_{1}h+b_{2}\\|h\\|h$ ", "page_idx": 5}, {"type": "text", "text": "Next, we state the primary assumption concerning the method\u2019s ability to generate new points. ", "page_idx": 5}, {"type": "text", "text": "Assumption 4.1 The method generates a recursive sequence of iterates $\\{x_{k}\\}_{k\\ge0}$ that satisfies the following condition: for all $k\\geq0$ , we have that $x_{k+1}\\in\\mathcal{X}$ satisfies that $\\langle\\Omega_{\\bar{x}}(x_{k+1}\\stackrel{-}{-}x_{k}),x{-}x_{k+1}\\rangle\\geq$ 0 for all $x\\in\\mathscr{X}$ , where ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\bar{x}=\\operatorname*{argmax}_{x\\in\\mathcal{X}}\\{\\langle s,x-x_{0}\\rangle-\\frac{1}{2}\\|x-x_{0}\\|^{2}\\}\\mathrm{~and~}s\\in\\operatorname{Lin}(F(x_{0}),\\ldots,F(x_{k})).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "As highlighted in [69], Assumption 4.1 is suitably satisfied by various dual extrapolation methods. However, it might not be applicable to alternative methods for variational inequalities, such as extragradient methods and their variants. We leave the generalization of inexact lower bounds for these algorithms to future research, as even lower bounds for exact algorithms [2, 69] do not address this case. Now, let us introduce the generalization of smoothness ", "page_idx": 6}, {"type": "text", "text": "Assumption 4.2 The operator $F(x)$ is $i$ -th-order $L_{i}$ -smooth $(i\\geq0,$ ), if it has Lipschitz-continious $i$ -th-order derivative ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\|\\nabla^{i}F(x)-\\nabla^{i}F(y)\\|_{\\mathrm{op}}\\leq L_{i}\\|x-y\\|,\\quad{\\mathrm{for}}\\,{\\mathrm{all}}\\;x,y\\in{\\mathcal{X}}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Finally, we present the lower bound theorem for first-order methods with inexact Jacobians. ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.3 Let some first-order method $\\mathcal{M}$ satisfy Assumption 4.1 and have access only $\\delta$ -inexact first-order oracle 15. Assume the method $\\mathcal{M}$ ensures for any $L_{0}$ -zero-order smooth and $L_{1}$ -first-order smooth monotone operator $F$ the following convergence rate ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathtt{G A P}(\\hat{x})\\leq O(1)\\operatorname*{max}\\left\\{\\frac{\\delta D^{2}}{\\Xi_{1}(T)};\\frac{L_{1}D^{3}}{\\Xi_{2}(T)}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Then for all $T\\geq1$ we have ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Xi_{1}(T)\\leq T,\\qquad\\Xi_{2}(T)\\leq T^{3/2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "5 Quasi-Newton Approximation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, inspired by Quasi-Newton (QN) methods for Hessian approximation, we propose QN approximations for Jacobians. Our goal is to create a simple scheme to approximate the first-order derivative and thereby reduce the complexity of the subproblem. We compute $J_{x}$ using a QN update and use it as an inexact Jacobian in the model $\\Omega_{v}^{\\eta}(x)$ (10). ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{J_{x}=J^{r}=J^{0}+\\sum_{i=0}^{r-1}\\!c_{i}u_{i}v_{i}^{\\top}=J^{0}+U^{\\top}C V,}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $r$ is a rank of approximation, $J^{0}\\,\\in\\,\\mathbb{R}^{d\\times d}\\,\\succeq\\,0,u_{i}\\,\\in\\,\\mathbb{R}^{d},v_{i}\\,\\in\\,\\mathbb{R}^{d}$ are known. $U\\,\\in\\,\\mathbb{R}^{r\\times d}$ and $V\\,\\in\\,\\mathbb{R}^{r\\times d}$ are matrices of stacked vectors $U\\,=\\,[u_{0},\\dotsc,u_{r-1}]$ and $V\\,=\\,\\bigl[v_{0},\\dotsc,v_{r-1}\\bigr]$ and $C\\in\\mathbb{R}^{r\\times r}$ is a diagonal matrix $C=\\mathrm{diag}([c_{0},\\dots,c_{r-1}])$ . If $u_{i}=v_{i}$ the update becomes symmetric. ", "page_idx": 6}, {"type": "text", "text": "L-Broyden is a non-symmetric variant of QN approximation [45, 38] of the following form ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{J^{i+1}=J^{i}+\\frac{(y_{i}-J^{i}s_{i})s_{i}^{\\top}}{s_{i}^{\\top}s_{i}},\\quad\\forall i=0,\\ldots,m-1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "In a view of (19), this update is obtained by setting $u_{i}=y_{i}-J^{i}s_{i}$ , $v_{i}=s_{i}$ , $c_{i}=1/(s_{i}^{\\top}s_{i})$ , and the rank $r=m$ is equal to memory size $m$ . ", "page_idx": 6}, {"type": "text", "text": "Damped L-Broyden is another option for QN approximation with non-symmetric damped update ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{l}{J^{i+1}=J^{i}+\\frac{1}{m+1}\\frac{(y_{i}-J^{i}s_{i})s_{i}^{\\top}}{s_{i}^{\\top}s_{i}},\\quad\\forall i=0,\\ldots,m-1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "By choosing $u_{i}=y_{i}-J^{i}s_{i}$ , $v_{i}=s_{i}$ , $c_{i}=1/((m\\!+\\!1)s_{i}^{\\top}s_{i})$ , $r=m$ , we derive this update from (19). We define the matrix $J^{r}(J^{0},U,V,C)=J^{r}(J^{0},Y,S,C)$ , where $Y$ and $S$ are formed by stacking the vectors $[y_{0},\\dotsc,y_{m-1}]$ and $\\left[s_{0},\\ldots,s_{m-1}\\right]$ . The matrix $J^{m}(J^{0},Y,S)$ can be computed for any given pair $(Y,S)$ . Next, we describe two strategies for the choice of $(s,y)$ pairs used in (20), (21). ", "page_idx": 6}, {"type": "text", "text": "QN with operator history is the well-known classic variant where operator differences are stored: ", "page_idx": 6}, {"type": "equation", "text": "$$\ns_{i}=z_{i+1}-z_{i},\\qquad y_{i}=F(z_{i+1})-F(z_{i}).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "This approach is computationally efficient as it does not require additional operator calculations. ", "page_idx": 6}, {"type": "text", "text": "QN with JVP sampling is based on fast computation of Jacobian-Vector Products (JVP): ", "page_idx": 7}, {"type": "equation", "text": "$$\ny_{i}=\\nabla F(x)s_{i},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $s_{i}$ are random vectors uniformly distributed on the unit sphere such that $\\left\\|{s_{i}}\\right\\|\\ =1$ and $s_{0},\\ldots,s_{m-1}$ are linearly independent. Note, for $m\\ll d$ , each $s_{i}$ is linearly independent with high probability. This approach requires only $m$ operator/JVP computations per step, which is considerably fewer than the $d$ JVPs needed for a full Jacobian. Utilizing the current Jacobian information allows to improve the accuracy of the approximation. ", "page_idx": 7}, {"type": "text", "text": "In the following theorem, we demonstrate that these approximations satisfy Assumption 2.1 and condition (8) for both the QN with operator history and JVP sampling methods. ", "page_idx": 7}, {"type": "text", "text": "Theorem 5.1 Let $F(x)$ be $L_{0}$ -zero-order smooth operator. For m-memory $L$ -Broyden approximation of the Jacobian $J_{x}=J^{m}$ defined iteratively by (20) with $0\\preceq J^{0}\\preceq L_{0}I$ , we have $\\delta\\leq(m+2)L_{0}$ . (F2o1r) $m$ -itmh $L$ ,- Bthreo ycdoennd iatipopnr nh $J_{x}=J^{m}$ of the Jacobian defined iteratively by $\\begin{array}{r}{0\\preceq^{\\cdot}J^{0}\\preceq\\frac{\\cdot\\ L_{0}}{m+1}I,}\\end{array}$ $\\delta\\leq2L_{0}$ ", "page_idx": 7}, {"type": "text", "text": "With the primary toolkit for QN approximation in VIs established, we can now discuss efficient way of solving the subproblem (11), which takes the following form ", "page_idx": 7}, {"type": "text", "text": "find $y\\in\\mathcal{X}$ such that $\\langle F(x)+(J_{x}+\\eta\\delta I+5L_{1}\\|y-x\\|I)(y-x),z-y\\rangle\\geq0{\\mathrm{~for~all~}}z\\in{\\mathcal{X}}.$ (22) Let us introduce a parameter $\\tau=\\|y-x\\|$ for a segment search in $\\tau\\in[0;D]$ . To solve (22), we consider another problem ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\langle A_{\\tau}^{-1}F(x)+y_{\\tau}-x,z-y_{\\tau}\\rangle\\geq0\\,\\mathrm{for}\\,\\mathrm{all}\\,z\\in\\mathcal{X},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $A_{\\tau}=J_{x}+(\\eta\\delta+5L_{1}\\tau)I$ . Problems (22) and (23) are equivalent when $\\tau=\\|y_{\\tau}-x\\|$ . The subproblem (23) can be reformulated as minimization problem ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{y_{\\tau}=\\underset{y\\in\\mathcal{X}}{\\mathrm{argmin}}\\left\\lbrace\\left\\langle A_{\\tau}^{-1}F(x),y-x\\right\\rangle+\\frac{1}{2}\\|y-x\\|^{2}\\right\\rbrace,}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where (23) is an optimality condition for (24). The goal is to find $y_{\\tau}$ such that $\\upsilon(\\tau)\\;=\\;$ $|\\tau-\\|y_{\\tau}-x\\||\\,\\leq\\,\\varepsilon$ . As $\\ensuremath{\\boldsymbol{\\upsilon}}(\\tau)$ is a continuous function of $\\tau$ , we can find this solution via bisection segment-search with $\\log_{2}{\\frac{D}{\\varepsilon}}$ iterations. This ray-search procedure is similar to the subproblem solution for the Cubic Regularized Newton subproblem. ", "page_idx": 7}, {"type": "text", "text": "For $r$ -rank QN approximation $J^{r}$ from (19), we can effectively compute $A_{\\tau}^{-1}F(x)$ , where $A_{\\tau}=$ $J^{r}+(\\eta\\delta+5L_{1}\\tau)I=U^{\\top}C V+J^{0}+(\\eta\\delta+5L_{1}\\tau)I=U^{\\top}C V+B$ and $B=J^{0}+(\\eta\\delta+5L_{1}\\tau)I$ by using the Woodbury matrix identity[96, 97]. ", "page_idx": 7}, {"type": "equation", "text": "$$\nA_{\\tau}^{-1}F(x)=\\left(B+U^{\\top}C V\\right)^{-1}F(x)=B^{-1}F(x)-B^{-1}U^{\\top}(C^{-1}+V B^{-1}U^{\\top})^{-1}V B^{-1}F(x).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "For computational efficiency, it is better to choose $J^{0}$ as a diagonal matrix, then inversion $B^{-1}$ is computed by $O(d)$ arithmetical operations, $C^{-1}$ by $O(r)$ operations. $V B^{-1}U^{\\top}$ requires $O(r^{2}d)$ for classical multiplication and can be improved by fast matrix multiplication. $(C^{-1}+V B^{-\\overset{.}{1}}U^{\\overset{.}{\\top}})^{-1}$ can be computed by $O(r^{3})$ , as an inverse of $r$ -rank matrix. The rest of the operations are cheaper. Thus, the total number of arithmetic operations is $O(r^{2}d)$ instead of $O(d^{3})$ for Jacobian inversion. We need to perform this inversion logarithmic number of times. Therefore, the total computational cost with the segment-search procedure is $\\tilde{O}\\left(r^{2}d+r^{3}\\log_{2}(\\frac{D}{\\varepsilon})\\right)$ . ", "page_idx": 7}, {"type": "text", "text": "6 Strongly monotone setting ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Assumption 6.1 The operator $F:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{d}$ is called strongly monotone if there exists a constant $\\mu>0$ such that ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\langle F(x)-F(y),x-y\\rangle\\geq\\mu\\|x-y\\|^{2},\\quad{\\mathrm{for~all~}}x,y\\in{\\mathcal{X}}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "To leverage the strong monotonicity of the objective function and achieve a linear convergence rate, we introduce the restarted version of Algorithm 1 dubbed as VIJI-Restarted. Restart techniques are widely utilized in optimization and typically preserve optimality. In other words, restarting an optimal method for convex functions or monotone problems effectively transforms it into an optimal method for strongly convex or strongly monotone problems. Within iteration of VIJI-Restarted listed as Algorithm 2, we execute VIJI for a predefined number of iterations (26). Next, the output of this run is used as initial point for next run of VIJI with parameters reset, and this iterative process continues. ", "page_idx": 7}, {"type": "text", "text": "Algorithm 2 VIJI-Restarted ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Input: initial point $z_{0}\\in\\mathcal{X}$ , $D=\\operatorname*{max}_{x,y\\in{\\mathcal{X}}}.$ , parameters $L,\\delta$ .   \nInitialization: $\\begin{array}{r}{n=\\lceil\\log\\frac{D}{\\varepsilon}\\rceil}\\end{array}$ , $R=D$ .   \nfor $\\mathrm{i}{=}1$ , ..., n do Set x0 = zi\u22121, Ri\u22121 =2iR\u22121 . Run Algorithm 1 with $\\beta_{k}^{\\overline{{{\\,}}}}=\\delta,\\;\\eta=10,\\;09\\mathsf{f}=0$ for $T_{i}$ iterations, where ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{T_{i}=O(1)\\left\\lceil\\operatorname*{max}\\left\\{\\frac{L_{1}^{2/3}R_{i-1}^{2/3}}{\\mu^{2/3}},\\frac{\\delta}{\\mu}\\right\\}\\right\\rceil.}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Set zi = xTi. ", "page_idx": 8}, {"type": "text", "text": "Theorem 6.2 Let Assumptions 1.2, 2.1, 6.1 hold. Then the total number of iterations of Algorithm 2 to reach desired accuracy $\\|z_{s}-x^{*}\\|\\leq\\varepsilon$ , where $x^{*}$ is the solution of (2) is ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{O\\left(\\left(\\frac{L_{1}D}{\\mu}\\right)^{\\frac{2}{3}}+\\left(\\frac{\\delta}{\\mu}+1\\right)\\left\\lceil\\log\\frac{D}{\\varepsilon}\\right\\rceil\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "7 Tensor generalization ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we generalize the results presented in Section 3 to the $p$ -th order case. We consider a higher-order method with inexact high-order derivatives, satisfying the following assumption. ", "page_idx": 8}, {"type": "text", "text": "Assumption 7.1 For all $x,v\\,\\in\\,\\mathcal{X},\\ i\\,\\geq\\,1,$ , $i$ -th inexact derivative of $F$ , which we denote as $G_{i}$ , satisfies ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\left(\\nabla^{i}F(v)-G_{i}(v)\\right)[x-v]^{i-1}\\right\\|\\leq\\delta_{i}\\|x-v\\|^{i-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Based on inexact $(p-1)$ -th-order Taylor approximation $\\begin{array}{r}{\\Psi_{p,v}(x)=F(v)+\\sum_{i=1}^{p-1}\\frac{1}{i!}\\nabla^{i}G_{i}(v)[x-v]^{i}}\\end{array}$ , we introduce the inexact tensor model $\\Omega_{p,v}(\\boldsymbol{x})$ of the objective ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Omega_{p,v}(x)=\\Psi_{p,v}(x)+\\sum_{i=1}^{p-1}\\frac{\\eta_{i}\\delta_{i}}{i!}\\Vert x-v\\Vert^{i-1}(x-v)+\\frac{5L_{p-1}}{(p-1)!}\\Vert x-v\\Vert^{p-1}(x-v).}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "The tensor generalization of Algorithm 1, referred to as VIHI (High-order Method for Variational Inequalitues under High-order derivatives Inexactness ) and detailed in Appendix G, involves the inexact solution of the subproblem, which satisfies the following condition: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{x\\in\\mathcal{X}}{\\operatorname*{sup}}\\langle\\Omega_{v_{k+1}}(x_{k+1}),x_{k+1}-x\\rangle\\leq\\frac{L_{p-1}}{p!}\\|x_{k+1}-v_{k+1}\\|^{p+1}+\\sum_{i=1}^{p-1}\\frac{\\delta_{i}}{i!}\\|x_{k+1}-v_{k+1}\\|^{i+1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Another difference in VIHI compared to VIJI (Algorithm 1) is the adaptive strategy for $\\lambda_{k+1}$ : ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{1}{4(5p-2)}\\leq\\lambda_{k}\\left(\\frac{L_{p-1}}{p!}\\|x_{k+1}-v_{k+1}\\|^{p+1}+\\sum_{i=1}^{p-1}\\frac{\\delta_{i}}{i!}\\|x_{k+1}-v_{k+1}\\|^{i+1}\\right)\\leq\\frac{1}{2(5p+1)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "The other steps of Algorithm 1 remain unchanged for the higher-order method. Now, we are ready to present the convergence properties of VIHI. ", "page_idx": 8}, {"type": "text", "text": "Theorem 7.2 Let Assumptions 1.1, 4.2 with $i=p-1$ , and 7.1 hold. Then, after $T\\geq1$ iterations of VIHI with parameters $\\eta_{i}=5p$ , $o p t=0$ , we get the following bound ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathtt{G A P}(\\tilde{x}_{T})=\\underset{x\\in\\mathcal{X}}{\\operatorname*{sup}}\\left\\langle F(x),\\tilde{x}_{T}-x\\right\\rangle\\le O\\left(\\frac{L_{p-1}D^{p+1}}{T^{\\frac{p+1}{2}}}+\\sum_{i=1}^{p-1}\\frac{\\delta_{i}D^{i+1}}{T^{\\frac{i+1}{2}}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Finally, we extend our tensor generalization to nonmonotone case and obtain the following result. ", "page_idx": 8}, {"type": "text", "text": "Theorem 7.3 Let Assumptions 1.3, 4.2 with $i=p-1$ , and 7.1 hold. Then after $T\\geq1$ iterations of VIHI with parameters $\\eta=5p$ , opt = 2 we get the following bound ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathtt{R E S}(\\hat{x}):=\\underset{x\\in\\mathcal{X}}{\\operatorname*{sup}}\\left\\langle F(\\hat{x}_{t}),\\hat{x}_{t}-x\\right\\rangle=O\\left(\\frac{L_{p-1}D^{p+1}}{T^{\\frac p2}}+\\sum_{i=1}^{p-1}\\frac{\\delta_{i}D^{i+1}}{T^{\\frac i2}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "8 Experiments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this section, we present numerical experiments to demonstrate the efficiency of our proposed methods. We consider the cubic regularized bilinear min-max problem of the form: ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{x\\in\\mathbb{R}^{d}}{\\operatorname*{min}}\\,\\underset{y\\in\\mathbb{R}^{d}}{\\operatorname*{max}}\\,f(x,y)=y^{\\top}(A x-b)+\\frac{\\rho}{6}\\|x\\|^{3},}\\end{array}\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "where $)>0,b=[1,0,\\dots,0]\\in\\mathbb{R}^{d}$ , and $A\\in\\mathbb{R}^{d\\times d}$ , with all 1 on the main diagonal and all $-1$ on the upper diagonal, the rest elements are 0. To reformulate it as variational inequality, we define $F(x)=[\\nabla_{x}f(x,y),-\\nabla_{y}f(x,y)]$ . This problem is inspired by the first-order lower bound function for variational inequalities and min-max problems and is commonly used to verify the convergence of high-order methods for VI [71, 52]. ", "page_idx": 9}, {"type": "text", "text": "We implement our second-order method for Variational Inequalities with Quasi-Newton Approximation (VIQA) as a PyTorch optimizer. The code is available in the OPTAMI package4 [58]. VIQA Broyden refers to L-Broyden approximation (20) and VIQA Damped Broyden to (21), which are used as inexact Jacobians in VIJI (Algorithm 1). We compare them with the Extragradient method (EG) [62], first-order Perseus (Perseus1), and second-order Perseus with Jacobian (Perseus2). ", "page_idx": 9}, {"type": "image", "img_path": "uvFDaeFR9X/tmp/e93670525cacc2ffbc087eae6445b6bff5260057eba1ca168fa8f20f764bc1a3.jpg", "img_caption": ["Figure 1: Comparison of different methods for $d=50$ , $\\rho=1e-3$ . "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "In Figure 1, one can see that second-order information in Perseus2 significantly accelerates the convergence compared to Perseus1. However, it is expensive to compute Jacobian and solve secondorder subproblems every iteration. VIQA with the proposed Damped Broyden approximation (21) is significantly faster than EG, Perseus1, and VIQA Broyden (20). It shows that this approximation improves the convergence of first-order Perseus1 and confirms the theoretical result that Damped Broyden is a more accurate approximation than classical Broyden from Theorem 5.1. The detailed parameters and setup are presented in Appendix I. ", "page_idx": 9}, {"type": "text", "text": "9 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we introduced a second-order method specifically designed to handle Jacobian inexactness. We demonstrated its optimality in the monotone case by introducing a new lower bound and extended its applicability to tensor methods. However, similar to other high-order methods with global convergence properties, our algorithm involves a subproblem that necessitates an additional subroutine for its solution. To address this challenge, we proposed a computationally feasible criterion for solving the subproblem and implemented Quasi-Newton approximations for Jacobians, resulting in a significant reduction in per-iteration cost. Future investigations could explore incorporating inexactness within the operator itself and developing adaptive schemes to dynamically adjust for the level of inexactness encountered during the optimization process. Another open problem is a design of more accurate Quasi-Newton approximations specifically for Jacobians, focusing on non-symmetrical structure of Jacobian and specific inexactness criteria such as Assumption 2.1. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments. ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The work was supported by MIPT based Center of National Technology Initiatives in the field of Artificial Intelligence for the purposes of the \"road map\" of Artificial Intelligence development up to 2030 and supported by NTI Foundation (agreement No.70-2021-00207 dated 22.11.2021, identifier 000000S507521QYL0002). ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] S. S. Ablaev, A. A. Titov, F. S. Stonyakin, M. S. Alkousa, and A. Gasnikov. Some adaptive first-order methods for variational inequalities with relatively strongly monotone operators and generalized smoothness. In International Conference on Optimization and Applications, pages 135\u2013150. Springer, 2022. [2] D. Adil, B. Bullins, A. Jambulapati, and S. Sachdeva. Optimal methods for higher-order smooth monotone variational inequalities. arXiv preprint arXiv:2205.06167, 2022. [3] A. Agafonov, D. Kamzolov, P. Dvurechensky, A. Gasnikov, and M. Tak\u00e1\u02c7c. Inexact tensor methods and their application to stochastic convex optimization. Optimization Methods and Software, 39(1):42\u201383, 2024. doi: 10.1080/10556788.2023.2261604. URL https://doi. org/10.1080/10556788.2023.2261604. [4] A. Agafonov, D. Kamzolov, A. Gasnikov, A. Kavis, K. Antonakopoulos, V. Cevher, and M. Tak\u00e1\u02c7c. Advancing the lower bounds: an accelerated, stochastic, second-order method with optimal adaptation to inexactness. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id $\\equiv$ otU31x3fus.   \n[5] M. M. Alves and B. F. Svaiter. A search-free $\\mathrm{O}(1/k^{3/2})$ homotopy inexact proximal-newton extragradient algorithm for monotone variational inequalities. arXiv preprint arXiv:2308.05887, 2023.   \n[6] A. Antipin. Method of convex programming using a symmetric modification of lagrange function. Matekon, 14(2):23\u201338, 1978. [7] K. Antonakopoulos, A. Kavis, and V. Cevher. Extra-newton: A first approach to noise-adaptive accelerated second-order methods. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 29859\u201329872. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/ c10804702be5a0cca89331315413f1a2-Paper-Conference.pdf.   \n[8] F. Bach, J. Mairal, and J. Ponce. Convex sparse matrix factorizations. arXiv preprint arXiv:0812.1869, 2008. [9] F. Bach, R. Jenatton, J. Mairal, G. Obozinski, et al. Optimization with sparsity-inducing penalties. Foundations and Trends\u00ae in Machine Learning, 4(1):1\u2013106, 2012.   \n[10] M. Baes. Estimate sequence methods: extensions and approximations. Institute for Operations Research, ETH, Z\u00fcrich, Switzerland, 2(1), 2009.   \n[11] S. Bellavia and G. Gurioli. Stochastic analysis of an adaptive cubic regularization method under inexact gradient evaluations and dynamic hessian accuracy. Optimization, 71:227\u2013261, 2022. doi: 10.1080/02331934.2021.1892104. URL https://doi.org/10.1080/02331934.2021. 1892104.   \n[12] S. Bellavia, G. Gurioli, B. Morini, and P. Toint. Adaptive regularization for nonconvex optimization using inexact function values and randomly perturbed derivatives. Journal of Complexity, 68:101591, 2022. ISSN 0885-064X. doi: https://doi.org/10.1016/j.jco.2021.101591. URL https://www.sciencedirect.com/science/article/pii/S0885064X21000467.   \n[13] A. Ben-Tal, L. El Ghaoui, and A. Nemirovski. Robust optimization, volume 28. Princeton university press, 2009.   \n[14] A. S. Berahas, M. Jahani, P. Richt\u00e1rik, and M. Tak\u00e1\u02c7c. Quasi-newton methods for machine learning: forget the past, just sample. Optimization Methods and Software, 37:1668\u20131704, 2022. doi: 10.1080/10556788.2021.1977806. URL https://doi.org/10.1080/10556788.2021. 1977806.   \n[15] A. Beznosikov, A. Alanov, D. Kovalev, M. Tak\u00e1\u02c7c, and A. Gasnikov. On scaled methods for saddle point problems. arXiv preprint arXiv:2206.08303, 2022.   \n[16] A. Beznosikov, B. Polyak, E. Gorbunov, D. Kovalev, and A. Gasnikov. Smooth monotone stochastic variational inequalities and saddle point problems: A survey. European Mathematical Society Magazine, (127):15\u201328, 2023.   \n[17] A. Beznosikov, M. Tak\u00e1c, and A. Gasnikov. Similarity, compression and local steps: three pillars of efficient communications for distributed variational inequalities. Advances in Neural Information Processing Systems, 36, 2023.   \n[18] J. F. Bonnans. Local analysis of newton-type methods for variational inequalities and nonlinear programming. Applied Mathematics and Optimization, 29:161\u2013186, 1994.   \n[19] L. Bottou, F. E. Curtis, and J. Nocedal. Optimization methods for large-scale machine learning. SIAM review, 60(2):223\u2013311, 2018.   \n[20] L. Brighi and R. John. Characterizations of pseudomonotone maps and economic equilibrium. Journal of Statistics and Management Systems, 5(1-3):253\u2013273, 2002.   \n[21] B. Bullins and K. A. Lai. Higher-order methods for convex-concave min-max optimization and monotone variational inequalities. SIAM Journal on Optimization, 32(3):2208\u20132229, 2022.   \n[22] Y. Carmon, D. Hausler, A. Jambulapati, Y. Jin, and A. Sidford. Optimal and adaptive monteirosvaiter acceleration. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 20338\u201320350. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/ paper/2022/file/7ff97417474268e6b5a38bcbfae04944-Paper-Conference.pdf.   \n[23] C. Cartis and K. Scheinberg. Global convergence rate analysis of unconstrained optimization methods based on probabilistic models. Mathematical Programming, 169:337\u2013375, 2018. ISSN 1436-4646. doi: 10.1007/s10107-017-1137-4. URL https://doi.org/10.1007/ s10107-017-1137-4.   \n[24] C. Cartis, N. I. Gould, and P. L. Toint. Adaptive cubic regularisation methods for unconstrained optimization. part i: motivation, convergence and numerical results. Mathematical Programming, 127(2):245\u2013295, 2011.   \n[25] C. Cartis, N. I. Gould, and P. L. Toint. Adaptive cubic regularisation methods for unconstrained optimization. part ii: worst-case function-and derivative-evaluation complexity. Mathematical programming, 130(2):295\u2013319, 2011.   \n[26] C. Cartis, N. I. Gould, and P. L. Toint. Evaluation Complexity of Algorithms for Nonconvex Optimization: Theory, Computation and Perspectives. SIAM, 2022.   \n[27] A. Chambolle and T. Pock. A first-order primal-dual algorithm for convex problems with applications to imaging. Journal of mathematical imaging and vision, 40:120\u2013145, 2011.   \n[28] T. Chavdarova, G. Gidel, F. Fleuret, and S. Lacoste-Julien. Reducing noise in gan training with variance reduced extragradient. Advances in Neural Information Processing Systems, 32, 2019.   \n[29] S. C. Choi, W. S. DeSarbo, and P. T. Harker. Product positioning under price competition. Management Science, 36(2):175\u2013199, 1990.   \n[30] C. D. Dang and G. Lan. On the convergence properties of non-euclidean extragradient methods for variational inequalities with generalized monotone operators. Computational Optimization and applications, 60:277\u2013310, 2015.   \n[31] C. Daskalakis, A. Ilyas, V. Syrgkanis, and H. Zeng. Training GANs with optimism. In International Conference on Learning Representations, 2018. URL https://openreview. net/forum?id $\\cdot$ SJJySbbAZ.   \n[32] C. Daskalakis, S. Skoulakis, and M. Zampetakis. The complexity of constrained min-max optimization. In Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing, pages 1466\u20131478, 2021.   \n[33] O. Devolder, F. Glineur, and Y. Nesterov. First-order methods of smooth convex optimization with inexact oracle. Mathematical Programming, 146:37\u201375, 2014. ISSN 1436-4646. doi: 10.1007/s10107-013-0677-5. URL https://doi.org/10.1007/s10107-013-0677-5.   \n[34] N. Doikov, E. M. Chayti, and M. Jaggi. Second-order optimization with lazy Hessians. In A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202, pages 8138\u20138161. PMLR, 9 2023. URL https://proceedings.mlr.press/v202/doikov23a.html.   \n[35] E. Esser, X. Zhang, and T. F. Chan. A general framework for a class of first order primal-dual algorithms for convex optimization in imaging science. SIAM Journal on Imaging Sciences, 3 (4):1015\u20131046, 2010.   \n[36] C. Ewerhart. Cournot games with biconcave demand. Games and Economic Behavior, 85: 37\u201347, 2014.   \n[37] F. Facchinei and J.-S. Pang. Finite-dimensional variational inequalities and complementarity problems. Springer, 2003.   \n[38] D. Fern\u00e1ndez. A quasi-newton strategy for the ssqp method for variational inequality and optimization problems. Mathematical Programming, 137:199\u2013223, 2013.   \n[39] G. Gallego and M. Hu. Dynamic pricing of perishable assets under competition. Management Science, 60(5):1241\u20131259, 2014.   \n[40] S. Ghadimi, H. Liu, and T. Zhang. Second-order methods with cubic regularization under inexact information. arXiv preprint arXiv:1710.05782, 2017.   \n[41] G. Gidel, H. Berard, G. Vignoud, P. Vincent, and S. Lacoste-Julien. A variational inequality perspective on generative adversarial networks. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=r1laEnA5Ym.   \n[42] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014.   \n[43] E. Gorbunov, N. Loizou, and G. Gidel. Extragradient method: $\\mathrm{O}(1/k)$ last-iterate convergence for monotone variational inequalities and connections with cocoercivity. In International Conference on Artificial Intelligence and Statistics, pages 366\u2013402. PMLR, 2022.   \n[44] E. Gorbunov, A. Taylor, and G. Gidel. Last-iterate convergence of optimistic gradient method for monotone variational inequalities. Advances in neural information processing systems, 35: 21858\u201321870, 2022.   \n[45] J. Han and D. Sun. Newton-type methods for variational inequalities. In Advances in Nonlinear Programming: Proceedings of the 96 International Conference on Nonlinear Programming, pages 105\u2013118. Springer, 1998.   \n[46] S. Hanzely, D. Kamzolov, D. Pasechnyuk, A. Gasnikov, P. Richt\u00e1rik, and M. Tak\u00e1\u02c7c. A damped Newton method achieves global $\\begin{array}{r}{\\mathcal{O}\\left(\\frac{1}{k^{2}}\\right)}\\end{array}$ and local quadratic convergence rate. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 25320\u201325334. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/ a1f0c0cd6caaa4863af5f12608edf63e-Paper-Conference.pdf.   \n[47] P. T. Harker and J.-S. Pang. Finite-dimensional variational inequality and nonlinear complementarity problems: a survey of theory, algorithms and applications. Mathematical programming, 48(1):161\u2013220, 1990.   \n[48] P. Hartman and G. Stampacchia. On some non-linear elliptic differential-functional equations. Acta Mathematica, 115(1):271\u2013310, 1966.   \n[49] K. Huang, J. Zhang, and S. Zhang. Cubic regularized Newton method for the saddle point models: A global and local convergence analysis. Journal of Scientific Computing, 91(2):60, 2022.   \n[50] A. N. Iusem, A. Jofr\u00e9, R. I. Oliveira, and P. Thompson. Extragradient method with variance reduction for stochastic variational inequalities. SIAM Journal on Optimization, 27(2):686\u2013724, 2017.   \n[51] M. Jahani, X. He, C. Ma, A. Mokhtari, D. Mudigere, A. Ribeiro, and M. Tak\u00e1c. Efficient distributed hessian free algorithm for large-scale empirical risk minimization via accumulating sample strategy. In International Conference on Artificial Intelligence and Statistics, pages 2634\u20132644. PMLR, 2020.   \n[52] R. Jiang and A. Mokhtari. Generalized optimistic methods for convex-concave saddle point problems. arXiv preprint arXiv:2202.09674, 2022.   \n[53] R. Jiang and A. Mokhtari. Accelerated quasi-newton proximal extragradient: Faster rate for smooth convex optimization. Advances in Neural Information Processing Systems, 36, 2024.   \n[54] R. Jiang, Q. Jin, and A. Mokhtari. Online learning guided curvature approximation: A quasinewton method with global non-asymptotic superlinear convergence. In The Thirty Sixth Annual Conference on Learning Theory, pages 1962\u20131992. PMLR, 2023.   \n[55] Y. Jin and A. Sidford. Efficiently solving mdps with stochastic mirror descent. In International Conference on Machine Learning, pages 4890\u20134900. PMLR, 2020.   \n[56] T. Joachims. A support vector method for multivariate performance measures. In Proceedings of the 22nd international conference on Machine learning, pages 377\u2013384, 2005.   \n[57] D. Kamzolov, K. Ziu, A. Agafonov, and M. Tak\u00e1\u02c7c. Cubic regularization is the key! the first accelerated quasi-newton method with a global convergence rate of $o(k^{-2})$ for convex functions. arXiv preprint arXiv:2302.04987, 2023.   \n[58] D. Kamzolov, D. Pasechnyuk, A. Agafonov, A. Gasnikov, and M. Tak\u00e1\u02c7c. Optami: Global superlinear convergence of high-order methods. arXiv preprint arXiv:2410.04083, 2024.   \n[59] A. Kannan and U. V. Shanbhag. Optimal stochastic extragradient schemes for pseudomonotone stochastic variational inequality problems and their variants. Computational Optimization and Applications, 74(3):779\u2013820, 2019.   \n[60] B. Kleinberg, Y. Li, and Y. Yuan. An alternative view: When does sgd escape local minima? In International Conference on Machine Learning, pages 2698\u20132707. PMLR, 2018.   \n[61] J. M. Kohler and A. Lucchi. Sub-sampled cubic regularization for non-convex optimization. In D. Precup and Y. W. Teh, editors, Proceedings of the 34th International Conference on Machine Learning, volume 70, pages 1895\u20131904. PMLR, 5 2017. URL https://proceedings.mlr. press/v70/kohler17a.html.   \n[62] G. M. Korpelevich. The extragradient method for finding saddle points and other problems. Matecon, 12:747\u2013756, 1976.   \n[63] G. Kotsalis, G. Lan, and T. Li. Simple and optimal methods for stochastic variational inequalities, i: operator extrapolation. SIAM Journal on Optimization, 32(3):2041\u20132073, 2022.   \n[64] D. Kovalev and A. Gasnikov. The first optimal acceleration of high-order methods in smooth convex optimization. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 35339\u201335351. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/ paper/2022/file/e56f394bbd4f0ec81393d767caa5a31b-Paper-Conference.pdf.   \n[65] X. Li and F. Orabona. On the convergence of stochastic gradient descent with adaptive stepsizes. In K. Chaudhuri and M. Sugiyama, editors, Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics, volume 89, pages 983\u2013992. PMLR, 4 2019. URL https://proceedings.mlr.press/v89/li19c.html.   \n[66] Y. Li and Y. Yuan. Convergence analysis of two-layer neural networks with relu activation. Advances in neural information processing systems, 30, 2017.   \n[67] T. Liang and J. Stokes. Interaction matters: A note on non-asymptotic local convergence of generative adversarial networks. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 907\u2013915. PMLR, 2019.   \n[68] T. Lin and M. I. Jordan. Monotone inclusions, acceleration, and closed-loop control. Mathematics of Operations Research, 48(4):2353\u20132382, 2023.   \n[69] T. Lin and M. I. Jordan. Perseus: A simple and optimal high-order method for variational inequalities. Mathematical Programming, pages 1\u201342, 2024.   \n[70] T. Lin, M. Jordan, et al. A continuous-time perspective on optimal methods for monotone equation problems. arXiv preprint arXiv:2206.04770, 2022.   \n[71] T. Lin, P. Mertikopoulos, and M. I. Jordan. Explicit second-order min-max optimization methods with optimal convergence guarantee. arXiv preprint arXiv:2210.12860, 2022.   \n[72] C. Liu and L. Luo. Regularized newton methods for monotone variational inequalities with h\\\" older continuous jacobians. arXiv preprint arXiv:2212.07824, 2022.   \n[73] D. C. Liu and J. Nocedal. On the limited memory BFGS method for large scale optimization. Mathematical Programming, 45:503\u2013528, 1989. doi: 10.1007/BF01589116. URL https: //doi.org/10.1007/BF01589116.   \n[74] A. Lucchi and J. Kohler. A sub-sampled tensor method for nonconvex optimization. IMA Journal of Numerical Analysis, 43:2856\u20132891, 10 2023. ISSN 0272-4979. doi: 10.1093/ imanum/drac057. URL https://doi.org/10.1093/imanum/drac057.   \n[75] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=rJzIBfZAb.   \n[76] P. Mertikopoulos, B. Lecouat, H. Zenati, C.-S. Foo, V. Chandrasekhar, and G. Piliouras. Optimistic mirror descent in saddle-point problems: Going the extra(-gradient) mile. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum? id=Bkg8jjC9KQ.   \n[77] G. J. Minty. Monotone (nonlinear) operators in hilbert space. Duke Mathematical Journal, 29 (3), 1962.   \n[78] A. Mokhtari, A. E. Ozdaglar, and S. Pattathil. Convergence rate of $0(1/\\mathrm{k})$ for optimistic gradient and extragradient methods in smooth convex-concave saddle point problems. SIAM Journal on Optimization, 30(4):3230\u20133251, 2020.   \n[79] R. D. C. Monteiro and B. F. Svaiter. An accelerated hybrid proximal extragradient method for convex optimization and its implications to second-order methods. SIAM Journal on Optimization, 23:1092\u20131125, 2013. doi: 10.1137/110833786. URL https://doi.org/10. 1137/110833786.   \n[80] A. Nemirovski. Prox-method with rate of convergence o (1/t) for variational inequalities with lipschitz continuous monotone operators and smooth convex-concave saddle point problems. SIAM Journal on Optimization, 15(1):229\u2013251, 2004.   \n[81] Y. Nesterov. Dual extrapolation and its applications to solving variational inequalities and related problems. Mathematical Programming, 109(2):319\u2013344, 2007.   \n[82] Y. Nesterov. Accelerating the cubic regularization of Newton\u2019s method on convex problems. Mathematical Programming, 112:159\u2013181, 2008. ISSN 1436-4646. doi: 10.1007/ s10107-006-0089-x. URL https://doi.org/10.1007/s10107-006-0089-x.   \n[83] Y. Nesterov. Implementable tensor methods in unconstrained convex optimization. Mathematical Programming, 186:157\u2013183, 2021. ISSN 1436-4646. doi: 10.1007/s10107-019-01449-1. URL https://doi.org/10.1007/s10107-019-01449-1.   \n[84] Y. Nesterov. High-order reduced-gradient methods for composite variational inequalities. arXiv preprint arXiv:2311.15154, 2023.   \n[85] Y. Nesterov and B. T. Polyak. Cubic regularization of Newton method and its global performance. Mathematical Programming, 108:177\u2013205, 2006. doi: 10.1007/s10107-006-0706-8. URL https://doi.org/10.1007/s10107-006-0706-8.   \n[86] J. Nocedal and S. J. Wright. Numerical Optimization. Springer New York, 2 edition, 2006. ISBN 978-0-387-30303-1. doi: 10.1007/978-0-387-40065-5.   \n[87] S. Omidshafiei, J. Pazis, C. Amato, J. P. How, and J. Vian. Deep decentralized multi-task multi-agent reinforcement learning under partial observability. In International Conference on Machine Learning, pages 2681\u20132690. PMLR, 2017.   \n[88] P. Ostroukhov, R. Kamalov, P. Dvurechensky, and A. Gasnikov. Tensor methods for strongly convex strongly concave saddle point problems and strongly monotone variational inequalities. arXiv preprint arXiv:2012.15595, 2020.   \n[89] Y. Ouyang and Y. Xu. Lower complexity bounds of first-order methods for convex-concave bilinear saddle-point problems. Mathematical Programming, 185(1):1\u201335, 2021.   \n[90] D. A. Pasechnyuk, A. Gasnikov, and M. Tak\u00e1\u02c7c. Convergence analysis of stochastic gradient descent with adaptive preconditioning for non-convex and convex functions. arXiv preprint arXiv:2308.14192, 2023.   \n[91] W. Peng, Y.-H. Dai, H. Zhang, and L. Cheng. Training gans with centripetal acceleration. Optimization Methods and Software, 35(5):955\u2013973, 2020.   \n[92] L. D. Popov. A modification of the arrow-hurwitz method of search for saddle points. Mat. Zametki, 28(5):777\u2013784, 1980.   \n[93] A. Sadiev, A. Beznosikov, A. J. Almansoori, D. Kamzolov, R. Tappenden, and M. Tak\u00e1\u02c7c. Stochastic gradient methods with preconditioned updates. Journal of Optimization Theory and Applications, 201:471\u2013489, 2024. ISSN 1573-2878. doi: 10.1007/s10957-023-02365-3. URL https://doi.org/10.1007/s10957-023-02365-3.   \n[94] N. Tripuraneni, M. Stern, C. Jin, J. Regier, and M. I. Jordan. Stochastic cubic regularization for fast nonconvex optimization. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. CesaBianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper_files/ paper/2018/file/db1915052d15f7815c8b88e879465a1e-Paper.pdf.   \n[95] P. Tseng. A modified forward-backward splitting method for maximal monotone mappings. SIAM Journal on Control and Optimization, 38(2):431\u2013446, 2000.   \n[96] M. A. Woodbury. The stability of out-input matrices. Chicago, IL, 9:3\u20138, 1949.   \n[97] M. A. Woodbury. Inverting modified matrices. Department of Statistics, Princeton University, 1950.   \n[98] L. Xu, J. Neufeld, B. Larson, and D. Schuurmans. Maximum margin clustering. Advances in neural information processing systems, 17, 2004.   \n[99] P. Xu, F. Roosta, and M. W. Mahoney. Newton-type methods for non-convex optimization under inexact Hessian information. Mathematical Programming, 184:35\u201370, 2020. ISSN 1436-4646. doi: 10.1007/s10107-019-01405-z. URL https://doi.org/10.1007/ s10107-019-01405-z. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Contents ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "A Proofs of Lemmas 2.2, 3.1 17 ", "page_idx": 16}, {"type": "text", "text": "B Proofs of Theorems 3.2, 3.3 18 ", "page_idx": 16}, {"type": "text", "text": "B.1 Proof of Theorem 3.2 18   \nB.2 Proof of Theorem 3.3 21 ", "page_idx": 16}, {"type": "text", "text": "C Proofs of Theorems 3.4, 3.5 22 ", "page_idx": 16}, {"type": "text", "text": "D Proof of Theorem 4.3 23 ", "page_idx": 16}, {"type": "text", "text": "E Proof of Theorem 5.1 23 ", "page_idx": 16}, {"type": "text", "text": "F Proof of Theorem 6.2 23 ", "page_idx": 16}, {"type": "text", "text": "G Tensor generalization with more details 25 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "G.1 Preliminaries 25   \nG.2 Auxiliary lemmas . . 26   \nG.3 Convergence in monotone case . 27   \nG.4 Convergence in nonmonotone case 31 ", "page_idx": 16}, {"type": "text", "text": "H Subproblem solution 32 ", "page_idx": 16}, {"type": "text", "text": "I Experiment details 33 ", "page_idx": 16}, {"type": "text", "text": "J Application to minmax problems 33 ", "page_idx": 16}, {"type": "text", "text": "J.1 Preliminaries 33   \nJ.2 The method 34   \nJ.3 Convergence analysis 35   \nJ.4 Proof of Theorem J.3 36   \nJ.5 Proof of Theorem J.4 38 ", "page_idx": 16}, {"type": "text", "text": "A Proofs of Lemmas 2.2, 3.1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we let $L:=L_{1}$ . ", "page_idx": 16}, {"type": "text", "text": "Lemma 2.2 Let Assumptions 1.2 and 2.1 hold. Then, for any $x,v\\in\\mathcal{X}$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\boldsymbol{F}(\\boldsymbol{x})-\\Psi_{\\boldsymbol{v}}(\\boldsymbol{x})\\|\\le\\frac{L}{2}\\|\\boldsymbol{x}-\\boldsymbol{v}\\|^{2}+\\delta\\|\\boldsymbol{x}-\\boldsymbol{v}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "roof. For any $x,y\\in\\mathcal{X}$ $\\begin{array}{c}{\\displaystyle\\|F(x)-\\Psi_{v}(x)\\|\\leq\\|F(x)-\\Phi_{v}(x)\\|+\\|\\Phi_{v}(x)-\\Psi_{v}(x)\\|}\\\\ {\\displaystyle\\leq\\frac{(9)}{2}\\|x-v\\|^{2}+\\|(\\nabla F(v)-J(v))[x-v]\\|\\leq\\frac{L}{2}\\|x-v\\|^{2}+\\delta\\|x-v\\|.}\\end{array}$ ", "page_idx": 16}, {"type": "text", "text": "Lemma 3.1 Let Assumptions (1.1), (1.2), (2.1) hold. Then for any $x,v\\in\\mathcal{X}\\;V I$ (11) is monotone ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{1}{2}\\left(\\nabla\\Omega_{v}(x)+\\nabla\\Omega_{v}(x)^{T}\\right)\\succeq4L_{1}\\|x-v\\|I_{d\\times d}+5L_{1}\\frac{{(x-v)(x-v)}^{T}}{\\|x-v\\|}+(\\eta-1)\\delta I_{d\\times d}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. For all $x$ , $v\\in\\mathcal{X}$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{2}\\left(\\nabla\\Omega_{v}(x)+\\nabla\\Omega_{v}(x)^{T}\\right)}\\\\ &{\\overset{(10)}{=}\\frac{1}{2}\\left(J(v)+J(v)^{T}\\right)+\\eta\\delta I_{d\\times d}+5L\\|x-v\\|I_{d\\times d}+5L\\frac{(x-v)(x-v)^{T}}{\\|x-v\\|}}\\\\ &{\\overset{(29)}{\\geq}\\frac{1}{2}\\left(\\nabla F(x)+\\nabla F(x)^{T}\\right)+4L\\|x-v\\|I_{d\\times d}+5L\\frac{(x-v)(x-v)^{T}}{\\|x-v\\|}+(\\eta-1)\\delta I_{d\\times d}}\\\\ &{\\qquad\\qquad\\overset{(3)}{\\geq}4L\\|x-v\\|I_{d\\times d}+5L\\frac{(x-v)(x-v)^{T}}{\\|x-v\\|}+(\\eta-1)\\delta I_{d\\times d}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "B Proofs of Theorems 3.2, 3.3 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "B.1 Proof of Theorem 3.2 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we let $L:=L_{1}$ . To show the convergence of Algorithm 1, we define the following Lyapunov function ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{E}_{k}=\\underset{v\\in\\mathcal{X}}{\\mathrm{max}}\\,\\langle s_{k},v-x_{0}\\rangle-\\frac{1}{2}\\|v-x_{0}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Lemma B.1 Let Assumption 1.2, 2.1 hold. Then, for every integer $T\\geq1$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{T}\\lambda_{k}\\langle F(x_{k}),x_{k}-x\\rangle\\leq\\mathcal{E}_{0}-\\mathcal{E}_{T}+\\langle s_{T},x-x_{0}\\rangle-\\frac{1}{8}\\left(\\sum_{k=1}^{T}\\|x_{k}-v_{k}\\|^{2}\\right),\\quad\\mathrm{for~all~}x\\in\\mathcal{X}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. By the definition of Lyapunov function (30) and Step 2 of Algorithm 1, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{E}_{k}=\\langle s_{k},v_{k+1}-x_{0}\\rangle-\\frac{1}{2}\\|v_{k+1}-x_{0}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then, we have ", "text_level": 1, "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{E}_{k+1}-\\mathcal{E}_{k}=\\langle s_{k+1},v_{k+2}-x_{0}\\rangle-\\langle s_{k},v_{k+1}-x_{0}\\rangle-\\frac{1}{2}\\left(\\|v_{k+2}-x_{0}\\|^{2}-\\|v_{k+1}-x_{0}\\|^{2}\\right)}\\\\ &{=\\langle s_{k+1}-s_{k},v_{k+1}-x_{0}\\rangle+\\langle s_{k+1},v_{k+2}-v_{k+1}\\rangle-\\frac{1}{2}\\left(\\|v_{k+2}-x_{0}\\|^{2}-\\|v_{k+1}-x_{0}\\|^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By the update formula for $v_{k+1}$ , we get ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\langle x-v_{k+1},s_{k}-v_{k+1}+x_{0}\\rangle\\leq0,\\quad{\\mathrm{for~all~}}x\\in{\\mathcal{X}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Letting $x=v_{k+2}$ in this inequality and using $\\begin{array}{r}{\\langle a,b\\rangle=\\frac{1}{2}(\\|a+b\\|^{2}-\\|a\\|^{2}-\\|b\\|^{2})}\\end{array}$ , we have \u27e8sk, $\\begin{array}{r}{v_{k+2}-v_{k+1}\\rangle\\leq\\langle v_{k+1}-x_{0},v_{k+2}-v_{k+1}\\rangle=\\frac{1}{2}\\left(\\|v_{k+2}-x_{0}\\|^{2}-\\|v_{k+1}-x_{0}\\|^{2}-\\|v_{k+2}-v_{k+1}\\|^{2}\\right).}\\end{array}$ (32) ", "page_idx": 17}, {"type": "text", "text": "Plugging Eq. (32) into Eq. (31) and using Step 5 of Algorithm 1, we obtain: ", "page_idx": 17}, {"type": "text", "text": "$\\begin{array}{r l}&{\\quad\\quad\\mathcal{E}_{k+1}-\\mathcal{E}_{k}\\stackrel{(32)}{\\leq}\\langle s_{k+1}-s_{k},v_{k+1}-x_{0}\\rangle+\\langle s_{k+1}-s_{k},v_{k+2}-v_{k+1}\\rangle-\\frac{1}{2}\\|v_{k+2}-v_{k+1}\\|^{2}}\\\\ &{\\quad=\\langle s_{k+1}-s_{k},v_{k+2}-x_{0}\\rangle-\\frac{1}{2}\\|v_{k+2}-v_{k+1}\\|^{2}\\leq\\lambda_{k+1}\\langle F(x_{k+1}),x_{0}-v_{k+2}\\rangle-\\frac{1}{2}\\|v_{k+2}-v_{k+1}\\|}\\\\ &{\\quad\\quad\\quad\\kappa_{+1}\\langle F(x_{k+1}),x_{0}-x\\rangle+\\lambda_{k+1}\\langle F(x_{k+1}),x-x_{k+1}\\rangle+\\lambda_{k+1}\\langle F(x_{k+1}),x_{k+1}-v_{k+2}\\rangle-\\frac{1}{2}\\|v_{k+2}-v_{k+1}\\|}\\end{array}$ 2 = \u03bb vk+1\u22252, for any $x\\in\\mathscr{X}$ . Summing up this inequality over $k=0,1,\\ldots,T-1$ and changing the counter $k+1$ to $k$ yields that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{T}\\lambda_{k}\\langle F(x_{k}),x_{k}-x\\rangle\\leq\\mathcal{E}_{0}-\\mathcal{E}_{T}+\\sum_{k=1}^{T}\\lambda_{k}\\langle F(x_{k}),x_{0}-x\\rangle+\\sum_{k=1}^{T}\\lambda_{k}\\langle F(x_{k}),x_{k}-v_{k+1}\\rangle-\\frac{1}{2}\\|v_{k}-v_{k}\\|_{L^{2}}\\leq\\frac{2}{T},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Using the update formula for $s_{k+1}$ and letting $s_{0}=0_{d}\\in\\mathbb{R}^{d}$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbf{I}=\\sum_{k=1}^{T}\\langle\\lambda_{k}F(x_{k}),x_{0}-x\\rangle=\\sum_{k=1}^{T}\\langle s_{k-1}-s_{k},x_{0}-x\\rangle=\\langle s_{0}-s_{T},x_{0}-x\\rangle=\\langle s_{T},x-x_{0}\\rangle.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Since $x_{k+1}\\in\\mathcal{X}$ satisfies (12), we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\langle\\Omega_{v_{k}}^{\\eta}(x_{k}),x-x_{k}\\rangle\\ge-\\frac{L}{2}\\|x_{k}-v_{k}\\|^{3}-\\delta\\|x_{k}-v_{k}\\|^{2},\\quad\\mathrm{for~all}\\;x\\in\\mathcal{X},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\Omega_{v}^{\\eta}(x):\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{d}$ is defined in (10). Letting $x=v_{k+1}$ in (35), we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\langle\\Omega_{v_{k}}^{\\eta}(x_{k}),x_{k}-v_{k+1}\\rangle\\leq\\frac{L}{2}\\|x_{k}-v_{k}\\|^{3}+\\delta\\|x_{k}-v_{k}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\qquad\\qquad\\langle F(x_{k}),x_{k}-v_{k+1}\\rangle}\\\\ &{=\\langle F(x_{k})-\\Omega_{v_{k}}^{\\eta}(x_{k})+\\eta\\delta(x_{k}-v_{k})+5L\\|x_{k}-v_{k}\\|(x_{k}-v_{k}),x_{k}-v_{k+1}\\rangle}\\\\ &{+\\langle\\Omega_{v_{k}}^{\\eta}(x_{k}),x_{k}-v_{k+1}\\rangle-5L\\|x_{k}-v_{k}\\|\\langle x_{k}-v_{k},x_{k}-v_{k+1}\\rangle-\\eta\\delta\\langle x_{k}-v_{k},x_{k}-v_{k+1}\\rangle}\\\\ &{\\overset{(2.2),(36)}{\\leq}\\frac{L}{2}\\|x_{k}-v_{k}\\|^{2}\\|x_{k}-v_{k+1}\\|+\\delta\\|x_{k}-v_{k}\\|\\|x_{k}-v_{k+1}\\|+\\frac{L}{2}\\|x_{k}-v_{k}\\|^{3}+\\delta\\|x_{k}-v_{k}\\|^{2}}\\\\ &{\\qquad\\qquad-5L\\|x_{k}-v_{k}\\|\\langle x_{k}-v_{k},x_{k}-v_{k+1}\\rangle-\\eta\\delta\\langle x_{k}-v_{k},x_{k}-v_{k+1}\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Next, using $\\langle x_{k}-v_{k},x_{k}-v_{k+1}\\rangle\\geq\\|x_{k}-v_{k}\\|^{2}-\\|x_{k}-v_{k}\\|\\|v_{k}-v_{k+1}\\|$ and $\\|x_{k}-v_{k+1}\\|\\leq$ $\\|x_{k}-v_{k}\\|+\\|v_{k}-v_{k+1}\\|$ , we get ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\langle F(x_{k}),x_{k}-v_{k+1}\\rangle}\\\\ &{}&{\\leq\\frac{L}{2}\\|x_{k}-v_{k}\\|^{3}+\\frac{L}{2}\\|x_{k}-v_{k}\\|^{2}\\|v_{k}-v_{k+1}\\|+\\delta\\|x_{k}-v_{k}\\|^{2}+\\delta\\|x_{k}-v_{k}\\|\\|v_{k}-v_{k+1}\\|}\\\\ &{}&{+\\frac{L}{2}\\|x_{k}-v_{k}\\|^{3}+\\delta\\|x_{k}-v_{k}\\|^{2}-5L\\|x_{k}-v_{k}\\|^{3}+5L\\|x_{k}-v_{k}\\|^{2}\\|v_{k}-v_{k+1}\\|-\\eta\\delta\\|x_{k}-v_{k}\\|}\\\\ &{}&{\\quad+\\eta\\delta\\|x_{k}-v_{k}\\|\\|v_{k}-v_{k+1}\\|}\\\\ &{}&{=\\frac{11L}{2}\\|x_{k}-v_{k}\\|^{2}\\|v_{k}-v_{k+1}\\|-4L\\|x_{k}-v_{k}\\|^{3}+(\\eta+1)\\delta\\|x_{k}-v_{k}\\|\\|v_{k}-v_{k+1}\\|-(\\eta-2)\\delta\\|x_{k}-v_{k+1}\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "2 \u2212vk\u22252 Next, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\mathbf{H}\\leq\\sum_{k=1}^{T}\\left(\\frac{11\\lambda_{k}L}{2}\\|x_{k}-v_{k}\\|^{2}\\|v_{k}-v_{k+1}\\|-4\\lambda_{k}L\\|x_{k}-v_{k}\\|^{3}\\right.}\\\\ {\\displaystyle+(\\eta+1)\\delta\\lambda_{k}\\|x_{k}-v_{k}\\|\\|v_{k}-v_{k+1}\\|-(\\eta-2)\\delta\\lambda_{k}\\|x_{k}-v_{k}\\|^{2}-\\frac{1}{2}\\|v_{k}-v_{k+1}\\|^{2}\\right)}\\\\ {\\displaystyle\\leq\\sum_{k=1}^{T}\\left(\\frac{1}{2}\\|x_{k}-v_{k}\\|\\|v_{k}-v_{k+1}\\|-\\frac{1}{4}\\|x_{k}-v_{k}\\|^{2}-\\frac{1}{2}\\|v_{k}-v_{k+1}\\|^{2}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the last inequality is due to the following choice of $\\textit{\\eta}~=~\\mathrm{~10~}$ and $\\begin{array}{r l r}{\\lambda}&{{}:}&{\\frac{1}{32}\\quad\\le}\\end{array}$ $\\begin{array}{r}{\\lambda_{k}\\left(\\frac{L}{2}\\|x_{k}-v_{k}\\|+\\delta\\right)\\le\\frac{1}{22}}\\end{array}$ . Then, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{II}\\leq\\displaystyle\\sum_{k=1}^{T}\\left(\\frac{1}{2}\\|x_{k}-v_{k}\\|\\|v_{k}-v_{k+1}\\|-\\frac{1}{4}\\|x_{k}-v_{k}\\|^{2}-\\frac{1}{2}\\|v_{k}-v_{k+1}\\|^{2}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq-\\frac{1}{8}\\left(\\sum_{k=1}^{T}\\|x_{k}-v_{k}\\|^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Plugging (34) and (37) into (33) yields that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{T}\\lambda_{k}\\langle F(x_{k}),x_{k}-x\\rangle\\leq\\mathcal{E}_{0}-\\mathcal{E}_{T}+\\langle s_{T},x-x_{0}\\rangle-\\frac{1}{8}\\left(\\sum_{k=1}^{T}\\|x_{k}-v_{k}\\|^{2}\\right).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Lemma B.2 Let Assumptions 1.2, 1.3, 2.1 hold and let $x\\in\\mathscr{X}$ . For every integer $T\\geq1$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{T}\\lambda_{k}\\langle F(x_{k}),x_{k}-x\\rangle\\leq\\frac{1}{2}\\|x-x_{0}\\|^{2},\\qquad\\sum_{k=1}^{T}\\|x_{k}-v_{k}\\|^{2}\\leq4\\|x^{*}-x_{0}\\|^{2},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $x^{*}\\in\\mathcal{X}$ denotes the weak solution to the VI. ", "page_idx": 19}, {"type": "text", "text": "Proof. For any $x\\in\\mathscr{X}$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{E}_{0}-\\mathcal{E}_{T}+\\langle s_{T},x-x_{0}\\rangle=\\mathcal{E}_{0}-\\left(\\underset{v\\in\\mathcal{X}}{\\operatorname*{max}}\\,\\langle s_{T},v-x_{0}\\rangle-\\frac{1}{2}\\|v-x_{0}\\|^{2}\\right)+\\langle s_{T},x-x_{0}\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Since $s_{0}=0_{d}$ , we have $\\mathcal{E}_{0}=0$ and ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{E}_{0}-\\mathcal{E}_{T}+\\langle s_{T},x-x_{0}\\rangle\\leq-\\left(\\langle s_{T},x-x_{0}\\rangle-\\frac12\\|x-x_{0}\\|^{2}\\right)+\\langle s_{T},x-x_{0}\\rangle=\\frac12\\|x-x_{0}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "This together with Lemma B.1 yields that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{T}\\lambda_{k}\\langle F(x_{k}),x_{k}-x\\rangle+\\frac{1}{8}\\left(\\sum_{k=1}^{T}\\|x_{k}-v_{k}\\|^{2}\\right)\\leq\\frac{1}{2}\\|x-x_{0}\\|^{2},\\quad\\mathrm{for}\\,\\exists\\mathrm{ll}\\,x\\in\\mathcal{X},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which implies the first inequality. Since the VI satisfies the Minty condition, there exists $x^{*}\\in\\mathcal{X}$ such that $\\left\\langle F(x_{k}),x_{k}-x^{*}\\right\\rangle\\geq0$ for all $k\\geq1$ . Letting $x=x^{*}$ in the above inequality yields the second inequality. \u25a1 ", "page_idx": 19}, {"type": "text", "text": "Lemma B.3 Let Assumptions 1.2, 1.3, 2.1 hold. For every integer $T\\geq1$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n{\\frac{1}{\\left(\\sum_{k=1}^{T}\\lambda_{k}\\right)^{2}}}\\leq{\\frac{2048L^{2}\\|x^{*}-x_{0}\\|^{2}}{T^{3}}}+{\\frac{2048\\delta^{2}}{T^{2}}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $x^{*}\\in\\mathcal{X}$ denotes the weak solution to the VI. ", "page_idx": 19}, {"type": "text", "text": "Proof. Without loss of generality, we assume that $x_{0}\\neq x^{*}$ . We have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{k=1}^{T}(\\lambda_{k})^{-2}(\\frac{1}{32})^{2}\\leq\\sum_{k=1}^{T}(\\lambda_{k})^{-2}\\left(\\lambda_{k}\\left(\\frac{L}{2}\\|x_{k}-v_{k}\\|+\\delta\\right)\\right)^{2}=\\sum_{k=1}^{T}\\left(\\frac{L}{2}\\|x_{k}-v_{k}\\|+\\delta\\right)^{2}}}\\\\ &{}&{\\leq\\sum_{k=1}^{T}\\frac{L^{2}}{2}\\|x_{k}-v_{k}\\|^{2}+2T\\delta^{2}\\overset{\\mathrm{Lemma~}B.2}{\\leq}2L^{2}\\|x^{*}-x_{0}\\|^{2}+2T\\delta^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "By the H\u00f6lder inequality, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{T}1=\\sum_{k=1}^{T}\\left((\\lambda_{k})^{-2}\\right)^{1/3}(\\lambda_{k})^{2/3}\\leq\\left(\\sum_{k=1}^{T}(\\lambda_{k})^{-2}\\right)^{1/3}\\left(\\sum_{k=1}^{T}\\lambda_{k}\\right)^{2/3}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Putting these pieces together yields that ", "page_idx": 19}, {"type": "equation", "text": "$$\nT\\leq32^{2/3}(2L^{2}\\|x^{*}-x_{0}\\|^{2}+2\\delta^{2}T)^{\\frac{1}{3}}\\left(\\sum_{k=1}^{T}\\lambda_{k}\\right)^{2/3},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Plugging this into the above inequality yields that ", "page_idx": 19}, {"type": "equation", "text": "$$\n{\\frac{1}{\\left(\\sum_{k=1}^{T}\\lambda_{k}\\right)^{2}}}\\leq{\\frac{2048L^{2}\\|x^{*}-x_{0}\\|^{2}}{T^{3}}}+{\\frac{2048\\delta^{2}}{T^{2}}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Theorem 3.2 Let Assumptions 1.2, 1.1, 2.1. Then, after $T\\geq1$ iterations of VIJI with parameters $\\beta=\\delta,\\;\\eta=10,\\;o p t=0$ , we get the following bound ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathtt{G A P}(\\tilde{x}_{T})=\\underset{x\\in\\mathcal{X}}{\\operatorname*{sup}}\\left\\langle F(x),\\tilde{x}_{T}-x\\right\\rangle\\le\\frac{16\\sqrt{2}L D^{3}}{T^{3/2}}+\\frac{16\\sqrt{2}\\delta D^{2}}{T}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. Letting $x\\in\\mathscr{X}$ , we derive from the monotonicity of $F$ and the definition of $\\tilde{x}_{T}$ (i.e., $\\mathsf{o p t}=0$ ) that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\langle F(x),\\tilde{x}_{T}-x\\rangle=\\frac{1}{\\sum_{k=1}^{T}\\lambda_{k}}\\left(\\displaystyle\\sum_{k=1}^{T}\\lambda_{k}\\langle F(x),x_{k}-x\\rangle\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Combining this inequality with the first inequality in Lemma B.2 yields that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\langle F(x),\\tilde{x}_{T}-x\\rangle\\le\\frac{\\|x-x_{0}\\|^{2}}{2(\\sum_{k=1}^{T}\\lambda_{k})},\\quad\\mathrm{for}\\,\\mathrm{all}\\,x\\in\\mathcal{X}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Since $x_{0}\\in\\mathcal{X}$ , we have $\\|x-x_{0}\\|\\leq D$ and hence ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\langle F(x),\\tilde{x}_{T}-x\\rangle\\le\\frac{D^{2}}{2(\\sum_{k=1}^{T}\\lambda_{k})},\\quad\\mathrm{for}\\,\\mathrm{all}\\,x\\in\\mathcal{X}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Then, we combine Lemma B.3 and the fact that $\\|x^{*}-x_{0}\\|\\leq D$ to obtain that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\langle F(x),\\tilde{x}_{T}-x\\rangle\\leq\\frac{D^{2}}{2}\\sqrt{\\frac{2048L^{2}D^{2}}{T^{3}}+\\frac{2048\\delta^{2}}{T^{2}}}\\leq\\frac{16\\sqrt{2}L D^{3}}{T^{3/2}}+\\frac{16\\sqrt{2}\\delta D^{2}}{T},\\quad\\mathrm{for~all~}x\\in\\mathcal{X}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "By the definition of a gap function. (4), we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathtt{G A P}(\\tilde{x}_{T})=\\underset{x\\in\\mathcal{X}}{\\operatorname*{sup}}\\left\\langle F(x),\\tilde{x}_{T}-x\\right\\rangle\\le\\frac{16\\sqrt{2}L D^{3}}{T^{3/2}}+\\frac{16\\sqrt{2}\\delta D^{2}}{T}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "B.2 Proof of Theorem 3.3 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We directly follow the steps of the proof of Theorem 3.2. Lemmas B.1, B.2 remain the same. B1ecause of the choice of $\\begin{array}{r}{\\bar{\\beta_{k+1}}=\\frac{L_{1}}{2}\\|x_{k+1}-v_{k+1}\\|}\\end{array}$ adaptive strategy for $\\lambda_{k+1}$ looks as follows: $\\begin{array}{r}{\\frac{1}{32}\\leq L\\|x_{k+1}-v_{k+1}\\|\\leq\\frac{1}{22}}\\end{array}$ . Next Lemma is a counterpart of Lemma B.3. ", "page_idx": 20}, {"type": "text", "text": "Lemma B.4 Let Assumptions 1.2, 1.3, 2.1 hold. For every integer $T\\geq1$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{1}{\\sum_{k=1}^{T}\\lambda_{k}}\\leq\\frac{64L\\|x^{*}-x_{0}\\|}{T^{3/2}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $x^{*}\\in\\mathcal{X}$ denotes the weak solution to the $V I.$ . ", "page_idx": 20}, {"type": "text", "text": "Proof. Without loss of generality, we assume that $x_{0}\\neq x^{*}$ . We have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{T}(\\lambda_{k})^{-2}\\big(\\frac{1}{32}\\big)^{2}\\leq\\sum_{k=1}^{T}(\\lambda_{k})^{-2}\\left(\\lambda_{k}\\left(L\\|x_{k}-v_{k}\\|\\right)\\right)^{2}\\leq\\sum_{k=1}^{T}L^{2}\\|x_{k}-v_{k}\\|^{2}\\overset{\\mathrm{Lemma~}B.2}{\\leq}4L^{2}\\|x^{*}-x_{0}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "By the H\u00f6lder inequality, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{T}1=\\sum_{k=1}^{T}\\left((\\lambda_{k})^{-2}\\right)^{1/3}(\\lambda_{k})^{2/3}\\leq\\left(\\sum_{k=1}^{T}(\\lambda_{k})^{-2}\\right)^{1/3}\\left(\\sum_{k=1}^{T}\\lambda_{k}\\right)^{2/3}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Putting these pieces together yields that ", "page_idx": 20}, {"type": "equation", "text": "$$\nT\\leq32^{2/3}(4L^{2}\\|x^{*}-x_{0}\\|^{2})^{\\frac{1}{3}}\\left(\\sum_{k=1}^{T}\\lambda_{k}\\right)^{2/3}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Plugging this into the above inequality yields that ", "page_idx": 20}, {"type": "equation", "text": "$$\n{\\frac{1}{\\sum_{k=1}^{T}\\lambda_{k}}}\\leq{\\frac{64L\\|x^{*}-x_{0}\\|}{T^{3/2}}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Then, by following the rest of the proof of Theorem 3.2, we get ", "page_idx": 20}, {"type": "text", "text": "Theorem 3.3 Let Assumptions 1.2, 1.1 hold. Let $\\{x_{k},v_{k}\\}$ be iterates generated by Algorithm 1 and ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|(\\nabla F(v_{k})-J(v_{k}))[z_{k}-v_{k}]\\|\\le\\delta_{k}\\|z_{k}-v_{k}\\|,\\quad\\delta_{k}\\le\\frac{L_{1}}{2}\\|x_{k}-v_{k}\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Then, after $T\\geq1$ iterations of Algorithm $^{\\,I}$ with parameters $\\begin{array}{r}{\\beta_{k}=\\frac{L_{1}}{2}\\|x_{k}-v_{k}\\|}\\end{array}$ , \u03b7 = 10, $o p t=0$ , we get the following bound ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathtt{G A P}(\\tilde{x}_{T})=\\underset{x\\in\\mathcal{X}}{\\operatorname*{sup}}\\left\\langle F(x),\\tilde{x}_{T}-x\\right\\rangle\\leq\\frac{32L D^{3}}{T^{3/2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "C Proofs of Theorems 3.4, 3.5 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this section, we let $L:=L_{1}$ . ", "page_idx": 21}, {"type": "text", "text": "Theorem 3.4 Let Assumptions 1.2, 1.3, 2.1 hold. Then after $T\\geq1$ iterations of Algorithm 1 with parameters $\\beta_{k}=\\delta,\\eta=10$ , $o p t=2$ we get the following bound ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathsf{R E S}(\\hat{x}):=\\operatorname*{sup}_{x\\in\\mathcal{X}}\\left\\langle F(\\hat{x}_{T}),\\hat{x}_{T}-x\\right\\rangle=O\\left(\\frac{L D^{3}}{T}+\\frac{\\delta D^{2}}{\\sqrt{T}}\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. Since we consider $\\mathsf{o p t=2}$ , then ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathrm{RES}(\\hat{x})=\\mathrm{RES}(x_{k_{T}})=\\operatorname*{sup}_{x\\in\\mathcal{X}}\\left\\langle F(x_{k_{T}}),x_{k_{T}}-x\\right\\rangle.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "From (36) we get ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\langle F(x_{k}),x_{k}-x\\rangle=\\langle F(x_{k})-\\Omega_{v_{k}}^{\\eta}(x_{k}),x_{k}-x\\rangle+\\langle\\Omega_{v_{k}}^{\\eta}(x_{k}),x_{k}-x\\rangle}\\\\ {\\mathrm{\\ensuremath{\\stackrel{(36)}{\\leq}}}\\|F(x_{k})-\\Omega_{v_{k}}^{\\eta}(x_{k})\\|\\|x_{k}-x\\|+\\displaystyle\\frac{L}{2}\\|x_{k}-v_{k}\\|^{3}+\\delta\\|x_{k}-v_{k}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Next, from triangle inequality we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\|F(x_{k})-\\Psi_{v_{k}}(x_{k})\\|=\\bigg\\|F(x_{k})-\\Omega_{v_{k}}^{\\eta}(x_{k})+\\eta\\delta(x_{k}-v_{k})+\\frac{5L}{2}\\|x_{k}-v_{k}\\|(x_{k}-v_{k})\\bigg\\|}\\\\ {\\displaystyle\\geq\\|F(x_{k})-\\Omega_{v_{k}}^{\\eta}(x_{k})\\|-\\eta\\delta\\|x_{k}-v_{k}\\|-\\frac{5L}{2}\\|x_{k}-v_{k}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "From this and (29) we get ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\|F(x_{k})-\\Omega_{v_{k}}^{\\eta}(x_{k})\\|\\leq\\frac{L}{2}\\|x_{k}-v_{k}\\|^{2}+\\delta\\|x_{k}-v_{k}\\|+\\eta\\delta\\|x_{k}-v_{k}\\|+\\frac{5L}{2}\\|x_{k}-v_{k}\\|^{2}}\\\\ {=3L\\|x_{k}-v_{k}\\|^{2}+\\delta(\\eta+1)\\|x_{k}-v_{k}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Now, we can return to (43): ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l l}{\\quad}&{\\langle F(x_{k}),x_{k}-x\\rangle}\\\\ {\\overset{(43),(44)}{\\leq}3L\\|x_{k}-v_{k}\\|^{2}\\|x_{k}-x\\|+\\delta(\\eta+1)\\|x_{k}-v_{k}\\|\\|x_{k}-x\\|+\\displaystyle\\frac{L}{2}\\|x_{k}-v_{k}\\|^{3}+\\delta\\|x_{k}-v_{k}\\|^{2}}\\\\ {\\quad}&{=L\\|x_{k}-v_{k}\\|^{2}\\left(3\\|x_{k}-x\\|+\\displaystyle\\frac{1}{2}\\|x_{k}-v_{k}\\|\\right)+\\delta\\|x_{k}-v_{k}\\|\\left((\\eta+1)\\|x_{k}-x\\|+\\|x_{k}-v_{k}\\|\\right).}\\\\ {\\mathrm{ince~}D:=\\operatorname*{max}_{x,y\\in\\mathcal{X}}\\|x-y\\|,}\\\\ {\\quad}&{\\langle F(x_{k}),x_{k}-x\\rangle\\leq\\displaystyle\\frac{7}{2}L D\\|x_{k}-v_{k}\\|^{2}+\\delta(\\eta+2)\\|x_{k}-v_{k}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Next, from second inequality from (38) and from definition of $x_{k_{T}}$ in Algorithm 1 we obtain ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\|x_{k_{T}}-v_{k_{T}}\\|^{2}\\equiv\\operatorname*{min}_{1\\leq k\\leq T}\\|x_{k}-v_{k}\\|^{2}\\overset{(38)}{\\leq}\\frac{1}{T}\\sum_{k=1}^{T}\\|x_{k}-v_{k}\\|^{2}\\leq\\frac{4\\|x^{*}-x_{0}\\|^{2}}{T}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Since (44) holds for any $x\\in\\mathscr{X}$ , we get final result ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tt{R E S}(x_{k_{T}}):=\\operatorname*{sup}_{x\\in\\mathcal{X}}\\left\\langle F(x_{k_{t}}),x_{k_{T}}-x\\right\\rangle\\stackrel{(44),(45)}{\\leq}\\frac{14L D\\|x^{*}-x_{0}\\|^{2}}{T}+\\frac{2\\delta\\left(\\eta+2\\right)D\\|x^{*}-x_{0}\\|}{\\sqrt{T}}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\frac{14L D^{3}}{T}+\\frac{2\\delta\\left(\\eta+2\\right)D^{2}}{\\sqrt{T}}=\\frac{14L D^{3}}{T}+\\frac{24D^{2}}{\\sqrt{T}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Theorem 3.5 Let Assumptions 1.2, 1.3 hold. Let $\\{x_{k},v_{k}\\}$ be iterates generated by Algorithm $^{\\,l}$ that satisfy (14). Then after $T\\geq1$ iterations of Algorithm $^{\\,l}$ with parameters $\\begin{array}{r}{\\beta_{k}=\\frac{L}{2}\\|x_{k}-v_{k}\\|}\\end{array}$ , $\\eta=$ 10, $o p t\\!=2$ we get the following bound ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{RES}(\\hat{x}):=\\underset{x\\in\\mathcal{X}}{\\operatorname*{sup}}\\left\\langle F(\\hat{x}_{T}),\\hat{x}_{T}-x\\right\\rangle=\\frac{38L_{1}D^{3}}{T}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The proof of this theorem repeats the proof of Theorem 3.4 with $\\begin{array}{r}{\\delta\\leq\\frac{L}{2}\\|x_{k}-v_{k}\\|}\\end{array}$ . ", "page_idx": 21}, {"type": "text", "text": "D Proof of Theorem 4.3 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Theorem 4.3 Let some first-order method $\\mathcal{M}$ satisfy Assumption 4.1 and have access only $\\delta$ -inexact first-order oracle 15. Assume the method $\\mathcal{M}$ ensures for any $L_{0}$ -zero-order smooth and $L_{1}$ -first-order smooth monotone operator $F$ the following convergence rate ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathtt{G A P}(\\hat{x})\\leq O(1)\\operatorname*{max}\\left\\{\\frac{\\delta D^{2}}{\\Xi_{1}(T)};\\frac{L_{1}D^{3}}{\\Xi_{2}(T)}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Then for all $T\\geq1$ we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Xi_{1}(T)\\leq T,\\qquad\\Xi_{2}(T)\\leq T^{3/2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. We prove this Theorem by contradiction. Assume the existence of a method $\\mathcal{M}$ that satisfies the conditions of Theorem 4.3 and achieves faster rate in one of the terms (18). ", "page_idx": 22}, {"type": "text", "text": "First, suppose $\\Xi_{1}(T)>T$ . Consider the first-order lower bound from [89], which is established using a quadratic min-max problem as the worst-case function. In this scenario, the operator has a 0-Lipschitz continuous Jacobian. Applying first-order method $\\mathcal{M}$ to this lower bound and using an inexact Jacobian $J(x)=L_{0}I_{d\\times d}$ , yields the rate \u039eL0(DT 2) , \u039e1(T) > T, which is faster than the lower bound $\\Omega\\left({\\frac{L_{0}D^{2}}{T}}\\right)$ , contradicting our assumption. ", "page_idx": 22}, {"type": "text", "text": "Secondly, let us assume $\\Xi_{2}(T)\\;>\\;T^{3/2}$ . The lower bound for exact second-order methods is $\\begin{array}{r}{\\Omega\\left(\\frac{L_{1}D^{3}}{T^{3/2}}\\right)}\\end{array}$ [69]. By taking exact Jacobian in the method $\\mathcal{M}$ $\\mathbf{\\boldsymbol{\\delta}}=\\mathbf{\\boldsymbol{0}}$ ), we place $\\mathcal{M}$ in the class of exact second-order methods. Consequently, we obtain a contradiction with the lower bound. \u25a1 ", "page_idx": 22}, {"type": "text", "text": "E Proof of Theorem 5.1 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Proof. The proof is common for both formulas (20) and (21), where $\\alpha=1$ for classical L-Broyden approximation and $\\alpha=m+1$ for Damped L-Broyden approximation. First, from $L_{0}$ -zero-order smoothness, get ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla F(x)-J_{x}\\|_{\\mathrm{op}}\\leq\\|\\nabla F(x)\\|_{o p}+\\|J_{x}\\|_{o p}\\leq L_{0}+\\|J_{x}\\|_{o p}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Now, we upper-bound $\\|J_{x}\\|_{o p}=\\|J^{m}\\|_{o p}$ by induction: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\left\\|{J^{i+1}}\\right\\|_{o p}\\leq\\left\\|{J^{i}+\\frac{(y_{i}-J^{i}s_{i})s_{i}^{\\top}}{\\alpha s_{i}^{\\top}s_{i}}}\\right\\|_{o p}\\leq\\left\\|{J^{i}\\left(I-\\frac{s_{i}s_{i}^{\\top}}{\\alpha s_{i}^{\\top}s_{i}}\\right)+\\frac{y_{i}s_{i}^{\\top}}{\\alpha s_{i}^{\\top}s_{i}}}\\right\\|_{o p}}\\\\ {\\leq\\left\\|{J^{i}\\left(I-\\frac{s_{i}s_{i}^{\\top}}{\\alpha s_{i}^{\\top}s_{i}}\\right)}\\right\\|_{o p}+\\left\\|{\\frac{y_{i}s_{i}^{\\top}}{\\alpha s_{i}^{\\top}s_{i}}}\\right\\|_{o p}\\leq\\left\\|{J^{i}}\\right\\|_{o p}\\left\\|{I-\\frac{s_{i}s_{i}^{\\top}}{\\alpha s_{i}^{\\top}s_{i}}}\\right\\|_{o p}+\\frac{\\left\\|{y_{i}}\\right\\|\\left\\|{s_{i}}\\right\\|}{\\alpha s_{i}^{\\top}s_{i}}\\leq\\left\\|{J^{i}}\\right\\|_{o p}+\\frac{L_{0}}{\\alpha},}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the last inequality is coming from $L_{0}$ -zero-order smoothness for operator difference or JVP.   \nBy summing up the previous inequality for $i$ in $0,\\ldots,m-1$ , we get $\\begin{array}{r}{\\|J^{m}\\|_{o p}\\leq\\|J^{0}\\|_{o p}+\\frac{m L_{0}}{\\alpha_{\\ r}}}\\end{array}$ .   \nFinally, we prove the result of Theorem 5.1. ", "page_idx": 22}, {"type": "text", "text": "F Proof of Theorem 6.2 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this section, we let $L:=L_{1}$ . ", "page_idx": 22}, {"type": "text", "text": "Theorem 6.2 Let Assumptions 1.2, 2.1, 6.1 hold. Then the total number of iterations of Algorithm 2 to reach desired accuracy $\\|z_{s}-x^{*}\\|\\leq\\varepsilon$ is ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{O\\left(\\left(\\frac{L D}{\\mu}\\right)^{\\frac{2}{3}}+\\left(\\frac{\\delta}{\\mu}+1\\right)\\left\\lceil\\log\\frac{D}{\\varepsilon}\\right\\rceil\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. From the definition of $\\tilde{x}_{T}$ , Jensen inequality, strong monotonicity (25), definition of strong minty problem (2) and Lemmas B.2, B.3 we get ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mu\\Big\\lVert\\bar{x}_{T}-x^{*}\\Big\\rVert^{2}=\\mu\\left\\lVert\\frac1{\\sum_{k=1}^{T}\\lambda_{k}}\\frac{T}{k}\\right\\rVert_{L^{\\frac{1}{\\alpha}}(\\mathbb{R}^{3}\\times\\mathbb{R}^{3})}^{2}\\right\\rVert^{2}}&{}\\\\ &{\\qquad\\le\\frac{\\mu}{\\sum_{k=1}^{T}\\lambda_{k}}\\sum_{k=1}^{T}\\Big(\\frac{\\mu}{\\lambda_{k}}\\Big)|x_{k}-x^{*}|^{2}}\\\\ &{\\overset{(2)}{\\le}\\frac{1}{\\sum_{k=1}^{T}\\lambda_{k}}\\sum_{k=1}^{T}\\Big(F(x_{k})-F(x^{*}),x_{k}-x^{*}\\Big)}\\\\ &{\\qquad\\overset{(2)}{\\le}\\frac{1}{\\sum_{k=1}^{T}\\lambda_{k}}\\sum_{k=1}^{T}\\Big(F(x_{k}),x_{k}-x^{*}\\Big)}\\\\ &{\\overset{(3),(2)}{\\le}\\left(\\frac{2048L^{2}\\left[10_{0}-x^{2}\\right]^{2}}{10_{0}^{3}}+\\frac{204826^{2}}{10_{0}^{2}}\\right)^{\\frac{3}{2}}\\cdot\\frac{1}{2}\\lVert z_{0}-x^{*}\\rVert^{2}}\\\\ &{\\left(2\\operatorname*{max}\\left\\{\\frac{2048L^{2}\\left[10_{0}-x^{2}\\right]^{2}}{T^{0}},\\frac{20488^{2}\\delta^{2}}{T^{0}}\\right\\}\\right)^{\\frac{3}{2}}\\cdot\\frac{1}{2}\\lVert z_{0}-x^{*}\\rVert^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Denote $\\begin{array}{r}{R:=D,\\;R_{i}=\\frac{R}{2^{i}},\\;i\\geq1}\\end{array}$ . Now we run Algorithm 1 in cycle for $i=1,...,n$ and restart it every time its distance to the solution becomes at least twice less than $R_{i-1}$ . Thus, let $T_{i}$ be number of iterations we run Algorithm 1 inside cycle of Algorithm 2. In other words, let $T_{i}$ be such that $\\begin{array}{r}{\\left|\\left|\\tilde{x}_{T_{i-1}}-x^{*}\\right|\\right|\\le\\frac{R_{i-1}}{2}}\\end{array}$ where $\\tilde{x}_{T_{i+1}}$ is the point, where we restart Algorithm 1. Then the number of iterations before the $i$ -th restart is ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu\\|\\tilde{x}_{T_{i}}-x^{*}\\|^{2}\\leq\\bigg(2\\operatorname*{max}\\bigg\\{\\displaystyle\\frac{2048L^{2}R_{i-1}^{2}}{T_{i}^{3}},\\displaystyle\\frac{2048\\delta^{2}}{T_{i}^{2}}\\bigg\\}\\bigg)^{\\frac{1}{2}}\\cdot\\frac{1}{2}R_{i-1}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\displaystyle\\frac{\\mu R_{i-1}^{2}}{4}\\Leftrightarrow}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\n\\Leftrightarrow\\left(\\operatorname*{max}\\left\\{\\frac{2048L^{2}R_{i-1}^{2}}{T_{i}^{3}},\\frac{2048\\delta^{2}}{T_{i}^{2}}\\right\\}\\right)^{\\frac{1}{2}}\\leq\\frac{\\mu}{2\\sqrt{2}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Deriving $T_{i}$ for each case under max, we get ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left\\{{\\cal T}_{i}\\geq\\frac{2^{\\frac{14}{3}}L^{\\frac{2}{3}}R_{i-1}^{\\frac{2}{3}}}{\\mu^{\\frac{2}{3}}}\\right.\\\\ {{\\cal T}_{i}\\geq\\frac{2^{7}\\delta}{\\mu}.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Thus, ", "page_idx": 23}, {"type": "equation", "text": "$$\nT_{i}=\\left.\\left\\lceil\\operatorname*{max}_{\\mu^{\\frac{2}{3}}}\\lambda\\frac{2^{\\frac{14}{3}}L^{\\frac{2}{3}}R_{i-1}^{\\frac{2}{3}}}{\\mu^{\\frac{2}{3}}},\\frac{2^{7}\\delta}{\\mu}\\right\\rceil\\right\\rceil\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Now we calculate the total number of restarts to reach $\\|\\tilde{x}_{T_{n}}-x^{*}\\|\\leq\\varepsilon$ . From (47) we can get that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\tilde{x}_{T_{n}}-x^{*}\\|\\leq\\sqrt{\\frac{1}{\\mu}\\left(2\\operatorname*{max}\\left\\{\\displaystyle\\frac{2048L^{2}R_{n-1}^{2}}{T_{n}^{3}},\\displaystyle\\frac{2048\\delta^{2}}{T_{n}^{2}}\\right\\}\\right)^{\\frac{1}{2}}\\cdot\\frac{1}{2}R_{n-1}^{2}}}\\\\ &{\\overset{(48)}{\\leq}\\displaystyle\\frac{R_{n-1}}{\\sqrt{2\\mu}}\\left(2\\operatorname*{max}\\left\\{\\displaystyle\\frac{\\mu^{2}}{8},\\displaystyle\\frac{\\mu^{2}}{8}\\right\\}\\right)^{\\frac{1}{4}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\frac{R_{n-1}}{2}}\\\\ &{=R\\cdot2^{-(n-1)-1}\\leq\\varepsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Deriving $n$ from last inequality, we get ", "page_idx": 24}, {"type": "equation", "text": "$$\nn=\\left\\lceil\\log{\\frac{R}{\\varepsilon}}\\right\\rceil.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Now we provide auxiliary estimation, that we will need next: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{i=1}^{n}R_{i}^{\\frac{2}{3}}=\\sum_{i=1}^{n}\\frac{\\|x_{0}-x^{*}\\|^{\\frac{2}{3}}}{2^{\\frac{2(i-1)}{3}}}}}\\\\ &{}&{\\quad=\\|x_{0}-x^{*}\\|^{\\frac{2}{3}}\\frac{1-2^{\\frac{-2(n-1)}{3}}}{2^{\\frac{1}{3}}}}\\\\ &{}&{\\quad\\leq\\|x_{0}-x^{*}\\|^{\\frac{2}{3}}2^{-\\frac{1}{3}}}\\\\ &{}&{\\quad\\leq2^{-\\frac{1}{3}}D^{\\frac{2}{3}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Finally, we can get the total number of iterations of Algorithm 1 inside Algorithm 2: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{i=1}^{n}T_{i}\\stackrel{(48)}{=}\\sum_{i=1}^{n}\\left[\\operatorname*{max}\\left\\{\\frac{2^{\\frac{14}{3}}L^{\\frac{2}{3}}R_{i-1}^{\\frac{2}{3}}}{\\mu^{\\frac{2}{3}}},\\frac{2^{7}\\delta}{\\mu}\\right\\}\\right]}}\\\\ &{}&{\\leq\\sum_{i=1}^{n}\\frac{2^{\\frac{14}{3}}L^{\\frac{2}{3}}R_{i-1}^{\\frac{2}{3}}}{\\mu^{\\frac{2}{3}}}+\\frac{2^{7}\\delta}{\\mu}n+n}\\\\ &{}&{=\\frac{2^{\\frac{14}{3}}L^{\\frac{2}{3}}}{\\mu^{\\frac{2}{3}}}\\sum_{i=1}^{n}R_{i-1}^{\\frac{3}{3}}+\\frac{2^{7}\\delta}{\\mu}n+n}\\\\ &{}&{\\stackrel{\\mathrm{(S0)},(49)}{\\leq}\\frac{2^{\\frac{13}{3}}L^{\\frac{2}{3}}D^{\\frac{3}{3}}}{\\mu^{\\frac{2}{3}}}+\\left(\\frac{2^{7}\\delta}{\\mu}+1\\right)\\left[\\log\\frac{D}{\\varepsilon}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Thus, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}T_{i}=O\\left(\\left(\\frac{L D}{\\mu}\\right)^{\\frac{2}{3}}+\\left(\\frac{\\delta}{\\mu}+1\\right)\\left\\lceil\\log\\frac{D}{\\varepsilon}\\right\\rceil\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "This completes the proof. ", "page_idx": 24}, {"type": "text", "text": "G Tensor generalization with more details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "G.1 Preliminaries ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Algorithm 3 VIHI ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Input: initial point $x_{0}\\in\\mathcal{X}$ , parameters $L_{1}$ , $\\eta$ , sequence $\\{\\delta_{i}\\}_{i=1}^{p-1}$ , and ${\\mathsf{o p t}}\\in\\{0,1,2\\}$ .   \nInitialization: set $s_{0}=0\\in\\mathbb{R}^{d}$ .   \nfor $k=0,1,2,\\ldots,T$ do Compute $\\begin{array}{r}{v_{k+1}=\\operatorname*{argmax}_{v\\in\\mathcal{X}}\\{\\langle s_{k},v-x_{0}\\rangle-\\frac{1}{2}\\|v-x_{0}\\|^{2}\\}}\\end{array}$ . Compute $x_{k+1}\\in\\mathcal{X}$ such that condition (53) holds true. Compute $\\lambda_{k+1}$ such that $\\begin{array}{r}{\\frac{1}{4(5p-2)}\\leq\\lambda_{k}\\left(\\frac{L_{p-1}}{p!}\\|x_{k}-v_{k}\\|^{p-1}+\\displaystyle\\sum_{i=1}^{p-1}\\frac{\\delta_{i}}{i!}\\|x_{k}-v_{k}\\|^{i-1}\\right)\\leq\\frac{1}{2(5p+1)}.}\\end{array}$ Compute $s_{k+1}=s_{k}-\\lambda_{k+1}F(x_{k+1})$ .   \nOutput: x\u02c6 $=\\left\\{\\begin{array}{c c}{\\tilde{x}_{T}=\\frac{1}{\\sum_{k=1}^{T}\\lambda_{k}}\\sum_{k=1}^{T}\\lambda_{k}x_{k},}&{\\mathrm{if}\\,\\mathsf{o p t}=0,}\\\\ {x_{T},}&{\\mathrm{else}\\,\\,\\mathrm{if}\\,\\mathsf{o p t}=1,}\\\\ {x_{k_{T}}\\,\\mathrm{for}\\,k_{T}=\\arg\\!\\operatorname*{min}_{1\\leq k\\leq T}\\|x_{k}-v_{k}\\|,}&{\\mathrm{else}\\,\\,\\mathrm{if}\\,\\mathsf{o p t}=2.}\\end{array}\\right.$ ", "page_idx": 24}, {"type": "text", "text": "In this section, we provide more details on the generalization of Algorithm 1 with high-order derivatives. We provide the pseudocode of the resulting method in Algorithm 3. To show the convergence of Algorithm 3, we use the Lyapunov function (30). ", "page_idx": 25}, {"type": "text", "text": "Define the $(p-1)$ -th order approximations of the $F$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\Phi_{p,v}(\\boldsymbol{x})=F(v)+\\sum_{i=1}^{p-1}\\frac{1}{i!}\\nabla^{i}F(v)[\\boldsymbol{x}-\\boldsymbol{v}]^{i}}\\\\ {\\displaystyle\\Psi_{p,v}(\\boldsymbol{x})=F(v)+\\sum_{i=1}^{p-1}\\frac{1}{i!}G_{i}(v)[\\boldsymbol{x}-\\boldsymbol{v}]^{i}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "On each step, our method solves the following subproblem: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{x\\in\\mathcal{X}}\\left\\langle\\Omega_{p,v_{k}}(x_{k}),x_{k}-x\\right\\rangle\\leq\\frac{L_{p-1}}{p!}\\|x_{k}-v_{k}\\|^{p+1}+\\sum_{i=1}^{p-1}\\frac{\\delta_{i}}{i!}\\|x_{k}-v_{k}\\|^{i+1}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Additionally, we will need an auxiliary result from [52], based on Assumption 4.2. The authors show, that this Assumption allows to control the quality of approximation of operator $F$ by its high-order Taylor polynomial: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|F(v)-\\Phi_{p,v}(x)\\|\\leq\\frac{L_{p-1}}{p!}\\|x-v\\|^{p}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "G.2 Auxiliary lemmas ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "First of all, we provide high-order generalizations of auxiliary lemmas for second-order case from Section A. ", "page_idx": 25}, {"type": "text", "text": "Lemma G.1 Let Assumptions 7.1 and 4.2 with $i=p-1$ hold. Then, for any $x,v\\in\\mathcal{X}$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|F(v)-\\Psi_{p,v}(x)\\|\\le\\frac{L_{p-1}}{p!}\\|x-v\\|^{p}+\\displaystyle\\sum_{i=1}^{p-1}\\frac{1}{i!}\\delta_{i}\\|x-v\\|^{i}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\|F(v)-\\Psi_{p,v}(x)\\|\\leq\\|F(v)-\\Phi_{p,v}(x)\\|+\\|\\Phi_{p,v}(x)-\\Psi_{p,v}(x)\\|}\\\\ {\\overset{(54)}{\\leq}\\frac{L_{p-1}}{p!}\\|x-v\\|^{p}+\\displaystyle\\sum_{i-1}^{p-1}\\frac{1}{i!}\\|(\\nabla^{i}F(v)-G_{i}(v))[x-v]^{i-1}\\|\\|x-v\\|}\\\\ {\\overset{(27)}{\\leq}\\frac{L_{p-1}}{p!}\\|x-v\\|^{p}+\\displaystyle\\sum_{i=1}^{p-1}\\frac{\\delta_{i}}{i!}\\|x-v\\|^{i}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Lemma G.2 Let Assumptions 1.1, 7.1 and 4.2 with $i\\,=\\,p\\mathrm{~-~}1$ hold. Then for any $x,v_{k+1}\\in\\mathcal{X}$ VI (11) is relatively strongly monotone if $\\eta_{i}\\geq p$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\qquad\\qquad\\qquad\\frac{1}{2}\\left(\\nabla\\Omega_{p,v}(\\boldsymbol{x})+\\nabla\\Omega_{p,v}(\\boldsymbol{x})^{T}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\frac{4L_{p-1}}{(p-1)!}\\left(\\|\\boldsymbol{x}-\\boldsymbol{v}\\|^{p-1}I_{n\\times n}+\\|\\boldsymbol{x}-\\boldsymbol{v}\\|^{p-3}(\\boldsymbol{x}-\\boldsymbol{v})(\\boldsymbol{x}-\\boldsymbol{v})^{T}\\right)}\\\\ &{+\\displaystyle\\sum_{i=1}^{p-1}\\frac{\\delta_{i}}{i!}\\|\\boldsymbol{x}-\\boldsymbol{v}\\|^{i-3}\\left((\\eta_{i}-i)\\|\\boldsymbol{x}-\\boldsymbol{v}\\|^{2}I_{n\\times n}+\\eta_{i}(i-1)(\\boldsymbol{x}-\\boldsymbol{v})(\\boldsymbol{x}-\\boldsymbol{v})^{T}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{P}_{\\sigma}\\Big\\lVert\\sum_{i=0}^{r-1}\\frac{1}{r}\\Big(Y_{0}\\Big(r_{i}\\Big(r_{i}\\Big)+Y_{1}\\Big(r_{i}\\Big)\\Big(X_{0}\\Big)\\Big(r_{i}^{-1}\\Big)^{-1}\\Big)\\Big)}\\\\ &{=\\frac{1}{r}\\sum_{i=0}^{r-1}\\Big(\\sum_{i=1}^{r}r_{i}\\Big(r_{i}\\Big(r_{i}\\Big)-r_{i}^{-1}\\Big)^{-1}+\\Big(Z_{0}\\Big(r_{i}\\Big)\\Big(r_{i}\\Big)-r_{i}^{-1}\\Big)^{-1}\\Big)\\Big)}\\\\ &{\\quad+\\frac{1}{r^{2}}\\sum_{i=1}^{r}r_{i}\\Big(2r_{i}-r_{i}^{-1}\\Big(r_{i}-r_{i}\\Big)+(1-r_{i})^{-1}\\Big)^{-1}\\sigma^{-1}\\big(x-r_{i}^{-1}\\Big(x-r_{i}^{-1}\\Big)^{-1}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad+\\frac{1}{r^{2}}\\Big)\\Big(1-r_{i}^{-1}\\Big)^{-1}\\sigma^{-1}\\Big(x-r_{i}\\Big)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\frac{1}{r}\\Big(1\\nabla r_{1}^{2}+Y_{0}\\Big(r_{i}^{-1}\\Big)^{-1}\\Big)}\\\\ &{=\\frac{1}{r}\\sum_{i=1}^{r}r_{i}\\Big(r_{i}^{-1}\\Big(r_{i}\\Big)-r_{i}^{-1}\\Big)^{-1}\\Big(1-r_{i}^{-1}\\Big)^{-1}\\sigma^{-1}\\Big(1-r_{i}^{-1}\\Big)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad+\\frac{1}{r^{2}}\\frac{1}{r^{2}}\\Big(1-r_{i}^{-1}r_{i-1}+(-1)\\Big(1\\nabla r_{1}^{-1}\\Big(r_{i}-r_{i}^{-1}\\Big)^{-1}\\Big)}\\\\ &\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "From monotonicity of $F$ we know that $\\begin{array}{r}{\\frac{1}{2}(F(x)-F(x)^{T})\\succeq0}\\end{array}$ . Thus, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\qquad\\qquad\\frac{1}{2}\\left(\\nabla\\Omega_{p,v}({\\boldsymbol x})+\\nabla\\Omega_{p,v}({\\boldsymbol x})^{T}\\right)}\\\\ &{\\qquad\\qquad\\succeq\\frac{4L_{p-1}}{(p-1)!}\\|{\\boldsymbol x}-{\\boldsymbol v}\\|^{p-1}I_{n\\times n}+\\frac{5L_{p-1}}{(p-2)!}\\|{\\boldsymbol x}-{\\boldsymbol v}\\|^{p-3}({\\boldsymbol x}-{\\boldsymbol v})({\\boldsymbol x}-{\\boldsymbol v})^{T}}\\\\ &{\\qquad+\\displaystyle\\sum_{i=1}^{p-1}\\frac{\\delta_{i}}{i!}(\\eta_{i}-i)\\|{\\boldsymbol x}-{\\boldsymbol v}\\|^{i-1}I_{n\\times n}+\\displaystyle\\sum_{i=1}^{p-1}\\frac{\\eta_{i}\\delta_{i}(i-1)}{i!}\\|{\\boldsymbol x}-{\\boldsymbol v}\\|^{i-3}({\\boldsymbol x}-{\\boldsymbol v})({\\boldsymbol x}-{\\boldsymbol v})^{T}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "By rearranging the terms we get ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{2}\\left(\\nabla\\Omega_{p,v}(x)+\\nabla\\Omega_{p,v}(x)^{T}\\right)}\\\\ &{\\qquad\\qquad\\frac{4L_{p-1}}{(p-1)!}\\left(\\|x-v\\|^{p-1}I_{n\\times n}+\\|x-v\\|^{p-3}(x-v)(x-v)^{T}\\right)}\\\\ &{+\\displaystyle\\sum_{i=1}^{p-1}\\frac{\\delta_{i}}{i!}\\|x-v\\|^{i-3}\\left((\\eta_{i}-i)\\|x-v\\|^{2}I_{n\\times n}+\\eta_{i}(i-1)(x-v)(x-v)^{T}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Thus, if $\\eta_{i}\\geq p$ , we get that $\\begin{array}{r}{\\frac{1}{2}\\left(\\nabla\\Omega_{p,v_{k+1}}(x)+\\nabla\\Omega_{p,v_{k+1}}(x)^{T}\\right)}\\end{array}$ is relatively strongly monotone. \u25a1 ", "page_idx": 26}, {"type": "text", "text": "G.3 Convergence in monotone case ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In this subsection we provide theoretical results directly connected to convergence rate of Algorithm 3. Firstly, we need to introduce additional technical lemmas, that generalize corresponding lemmas in Section B. ", "page_idx": 26}, {"type": "text", "text": "Lemma G.3 Let Assumption 1.1 hold and $\\eta_{i}=5p$ . Then, for every $T\\geq1$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{T}\\langle\\lambda_{k}F(x_{k}),x_{0}-x\\rangle\\leq\\mathcal{E}_{0}-\\mathcal{E}_{T}+\\langle s_{t},x-x_{0}\\rangle-\\frac{1}{8}\\sum_{k=1}^{T}\\|x_{k}-v_{k}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. Since the first steps in the proof of this Lemma are the same as in Lemma B.1, we start our reasoning from (33): ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\sum_{=1}^{T}\\lambda_{k}\\langle F(x_{k}),x_{k}-x\\rangle\\leq\\mathcal{E}_{0}-\\mathcal{E}_{T}+\\underbrace{\\sum_{k=1}^{T}\\lambda_{k}\\langle F(x_{k}),x_{0}-x\\rangle}_{\\textbf{I}}+\\underbrace{\\sum_{k=1}^{T}\\lambda_{k}\\langle F(x_{k}),x_{k}-v_{k+1}\\rangle-\\frac{1}{2}\\|v_{k}-v_{k}\\|}_{\\textbf{I}},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Using the update formula for $s_{k+1}$ and letting $s_{0}=0_{d}\\in\\mathbb{R}^{d}$ , we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbf{I}=\\sum_{k=1}^{T}\\left\\langle\\lambda_{k}F(x_{k}),x_{0}-x\\right\\rangle=\\sum_{k=1}^{T}\\left\\langle s_{k-1}-s_{k},x_{0}-x\\right\\rangle=\\left\\langle s_{T},x-x_{0}\\right\\rangle.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Now consider $\\mathbf{II}$ . Using (28) we get ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle F(x_{k}),x_{k}-v_{k+1}\\rangle}\\\\ &{\\overset{(\\mathrm{ga})}{=}\\langle F(x_{k})-\\Psi_{p,v_{k}}(x_{k}),x_{k}-v_{k+1}\\rangle+\\langle\\Omega_{p,v_{k}}(x_{k}),x_{k}-v_{k+1}\\rangle}\\\\ &{\\qquad\\qquad-\\underset{i=1}{\\overset{p-1}{\\prod}}\\frac{v_{i k}}{i!}\\Vert x_{k}-v_{k}\\Vert^{i-1}\\left\\langle x_{k}-v_{k},x_{k}-v_{k+1}\\right\\rangle}\\\\ &{\\qquad\\qquad\\qquad-\\underset{i=1}{\\overset{s_{k}-1}{\\sum}}\\Vert x_{k}-v_{k}\\Vert^{p-1}\\left\\langle x_{k}-v_{k},x_{k}-v_{k+1}\\right\\rangle}\\\\ &{\\overset{(\\mathrm{S}^{3})}{\\leq}\\frac{L_{p-1}}{p!}\\Vert x_{k}-v_{k}\\Vert^{p}\\Vert x_{k}-v_{k+1}\\Vert+\\underset{i=1}{\\overset{p-1}{\\sum}}\\frac{1}{i!}\\delta_{i}\\Vert x_{k}-v_{k}\\Vert^{i}\\Vert x_{k}-v_{k+1}\\Vert+\\langle\\Omega_{p,v_{k}}(x_{k}),x_{k}-v_{k+1}\\rangle}\\\\ &{\\overset{(\\mathrm{S}^{3})}{\\leq}\\underset{i=1}{\\overset{p-1}{\\prod}}\\frac{v_{i k}}{i!}\\Vert x_{k}-v_{k}\\Vert^{i-1}\\left\\langle x_{k}-v_{k},x_{k}-v_{k+1}\\right\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\qquad-\\underset{i=1}{\\overset{p-1}{\\prod}}\\frac{v_{i k}}{i!}\\Vert x_{k}-v_{k}\\Vert^{i-1}\\left\\langle x_{k}-v_{k},x_{k}-v_{k+1}\\right\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\qquad-\\underset{i=1}{\\overset{p-1}{\\prod}}\\Vert x_{k}-v_{k}\\Vert^{p-1}\\left\\langle x_{k}-v_{k},x_{k}-v_{k+1}\\right\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Next, using $\\langle x_{k}-v_{k},x_{k}-v_{k+1}\\rangle\\geq\\|x_{k}-v_{k}\\|^{2}-\\|x_{k}-v_{k}\\|\\|v_{k}-v_{k+1}\\|$ and $\\|x_{k}-v_{k+1}\\|\\leq$ $\\|x_{k}-v_{k}\\|+\\|v_{k}-v_{k+1}\\|$ , we get ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\langle F(x_{k}),x_{k}-v_{k+1}\\rangle}\\\\ {\\displaystyle\\leq\\frac{L_{p-1}}{p!}\\left(\\|x_{k}-v_{k}\\|^{p+1}+\\|x_{k}-v_{k}\\|^{p}\\|v_{k}-v_{k+1}\\|\\right)}\\\\ {\\displaystyle\\quad+\\sum_{i=1}^{p-1}\\frac{1}{i!}\\delta_{i}\\left(\\|x_{k}-v_{k}\\|^{i+1}\\|x_{k}-v_{k}\\|^{i}\\|v_{k}-v_{k+1}\\|\\right)}\\\\ {\\displaystyle-\\sum_{i=1}^{p-1}\\frac{\\eta_{i}\\delta_{i}}{i!}\\left(\\|x_{k}-v_{k}\\|^{i+1}-\\|x_{k}-v_{k}\\|^{i}\\|v_{k}-v_{k+1}\\|\\right)}\\\\ {\\displaystyle-\\frac{5L_{p-1}}{(p-1)!}\\left(\\|x_{k}-v_{k}\\|^{p+1}-\\|x_{k}-v_{k}\\|^{p}\\|v_{k}-v_{k+1}\\|\\right)}\\\\ {\\displaystyle\\quad+\\langle\\Omega_{p,v_{k}}(x_{k}),x_{k}-v_{k+1}\\rangle\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "From definition of the subproblem (53) we have that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\Omega_{p,v_{k}}(x_{k}),x_{k}-v_{k+1}\\rangle\\leq\\operatorname*{sup}_{x\\in\\mathcal{X}}\\langle\\Omega_{p,v_{k}}(x_{k}),x_{k}-x\\rangle\\leq\\frac{L_{p-1}}{p!}\\|x_{k}-v_{k}\\|^{p+1}+\\sum_{i=1}^{p-1}\\frac{\\delta_{i}}{i!}\\|x_{k}-v_{k}\\|^{i+1}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Thus, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\langle F(x_{k}),x_{k}-v_{k+1}\\rangle}\\\\ {\\leq\\displaystyle\\frac{L_{p-1}(1+5p)}{p!}\\|x_{k}-v_{k}\\|^{p}\\|v_{k}-v_{k+1}\\|-\\displaystyle\\frac{L_{p-1}(5p-2)}{p!}\\|x_{k}-v_{k}\\|^{p+1}}\\\\ {+\\displaystyle\\sum_{i=1}^{p-1}\\frac{\\delta_{i}(1+\\eta_{i})}{i!}\\|x_{k}-v_{k}\\|^{i}\\|v_{k}-v_{k+1}\\|-\\displaystyle\\sum_{i=1}^{p-1}\\frac{\\delta_{i}(\\eta_{i}-2)}{i!}\\|x_{k}-v_{k}\\|^{i+1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "From this we get ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\Pi=\\sum_{k=1}^{T}\\lambda_{k}\\left<F(x_{k}),x_{k}-v_{k+1}\\right>-\\frac12\\|v_{k}-v_{k+1}\\|^{2}}}\\\\ {{\\displaystyle\\leq\\sum_{k=1}^{T}\\left[\\lambda_{k}\\frac{L_{k}-1(1+s)}{p!}\\|x_{k}-v_{k}\\|^{p}\\|v_{k}-v_{k+1}\\|-\\frac{\\lambda_{k}L_{p-1}(5p-2)}{p!}\\|x_{k}-v_{k}\\|^{p+1}\\right.}}\\\\ {{\\displaystyle\\left.+\\lambda_{k}\\sum_{i=1}^{p-1}\\frac{\\delta_{i}(1+s_{i})}{i!}\\|x_{k}-v_{k}\\|^{i}\\|v_{k}-v_{k+1}\\|-\\lambda_{k}\\sum_{i=1}^{p-1}\\frac{\\delta_{i}(1\\eta-2)}{i!}\\|x_{k}-v_{k}\\|^{i+1}-\\frac12\\|v_{k}-v_{k+1}\\|^{2}\\right]}}\\\\ {{\\displaystyle\\eta_{i}\\frac{\\alpha_{k}}{=5}\\sum_{k=1}^{T}\\left[\\lambda_{k}\\left(\\frac{L_{p-1}}{p!}\\|x_{k}-v_{k}\\|^{p-1}+\\sum_{i=1}^{p-1}\\frac{\\delta_{i}}{i!}\\|x_{k}-v_{k}\\|^{i-1}\\right)(5p+1)\\|x_{k}-v_{k}\\|\\|v_{k}-v_{k+1}\\|\\right.}}\\\\ {{\\displaystyle\\left.-\\lambda_{k}\\left(\\frac{L_{p-1}}{p!}\\|x_{k}-v_{k}\\|^{p-1}+\\sum_{i=1}^{p-1}\\frac{\\delta_{i}}{i!}\\|x_{k}-v_{k}\\|^{i-1}\\right)(5p-2)\\|x_{k}-v_{k}\\|^{2}-\\frac12\\|v_{k}-v_{k+1}\\|^{2}\\right]}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Now, if we choose $\\lambda_{k}$ in a such way that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{1}{4(5p-2)}\\leq\\lambda_{k}\\left(\\frac{L_{p-1}}{p!}\\|x_{k}-v_{k}\\|^{p-1}+\\displaystyle\\sum_{i=1}^{p-1}\\frac{\\delta_{i}}{i!}\\|x_{k}-v_{k}\\|^{i-1}\\right)\\leq\\frac{1}{2(5p+1)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "we get the following ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathbf{I}(\\leq)\\sum_{k=1}^{T}\\left[\\frac{1}{2}\\|x_{k}-v_{k}\\|\\|v_{k}-v_{k+1}\\|-\\frac{1}{4}\\|x_{k}-v_{k}\\|^{2}-\\frac{1}{2}\\|v_{k}-v_{k+1}\\|^{2}\\right]}}\\\\ &{\\leq\\displaystyle\\sum_{k=1}^{T}\\left[\\operatorname*{max}_{\\gamma}\\left\\{\\frac{1}{2}\\gamma\\|x_{k}-v_{k}\\|-\\frac{1}{2}\\gamma^{2}\\right\\}-\\frac{1}{4}\\|x_{k}-v_{k}\\|^{2}\\right]}\\\\ &{\\leq\\displaystyle\\sum_{k=1}^{T}\\left[\\frac{1}{4}\\|x_{k}-v_{k}\\|^{2}-\\frac{1}{8}\\|x_{k}-v_{k}\\|^{2}-\\frac{1}{4}\\|x_{k}-v_{k}\\|^{2}\\right]}\\\\ &{\\qquad\\qquad\\qquad=-\\frac{1}{8}\\displaystyle\\sum_{k=1}^{T}\\|x_{k}-v_{k}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Finally, combining estimations of $\\mathbf{I}$ and $\\mathbf{II}$ with (57), we get ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\lambda_{k}\\left\\langle F(x_{k}),x_{k}-x\\right\\rangle\\leq\\mathcal{E}_{0}-\\mathcal{E}_{T}+\\left\\langle s_{T},x-x_{0}\\right\\rangle-\\frac{1}{8}\\sum_{k=1}^{T}\\|x_{k}-v_{k}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Lemma G.4 Let Assumptions 4.2, 1.3 hold. For every $T\\geq1$ we have ", "text_level": 1, "page_idx": 28}, {"type": "equation", "text": "$$\n\\frac{1}{\\sum_{k=1}^{T}\\lambda_{k}}\\leq\\frac{2^{p}p^{\\frac{p-1}{2}}(20p-8)\\frac{L_{p-1}}{p!}\\|x^{*}-x_{0}\\|^{p-1}}{T^{\\frac{p+1}{2}}}+2^{\\frac{3p}{2}}p^{\\frac{p-1}{2}}(20p-8)\\sum_{i=1}^{p-1}\\frac{\\delta_{i}\\|x^{*}-x_{0}\\|^{i-1}}{i!T^{\\frac{i+1}{2}}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. From H\u00f6lder inequality ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{T}\\lambda_{k}\\geq{\\frac{T^{\\frac{p+1}{2}}}{\\left(\\sum_{k=1}^{T}\\lambda_{k}^{-{\\frac{2}{p-1}}}\\right)^{\\frac{p-1}{2}}}}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Consider denominator: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i=1}^{T}\\lambda_{k}^{-\\frac{2}{p-1}}\\overset{(59)}{\\leq}\\sum_{k=1}^{T}\\lambda_{k}^{-\\frac{2}{p-1}}\\left(4(5p-2)\\right)^{\\frac{2}{p-1}}\\lambda_{k}^{\\frac{2}{p-1}}\\left(\\frac{L_{p-1}}{p!}\\|x_{k}-v_{k}\\|^{p-1}+\\displaystyle\\sum_{i=1}^{p-1}\\frac{\\delta_{i}}{i!}\\|x_{k}-v_{K}\\|^{i-1}\\right)^{\\frac{2}{p-1}}}\\\\ &{\\quad\\qquad\\qquad=\\displaystyle\\sum_{k=1}^{T}(20p-8)^{\\frac{2}{p-1}}\\left(\\frac{L_{p-1}}{p!}\\|x_{k}-v_{k}\\|^{p-1}+\\displaystyle\\sum_{i=1}^{p-1}\\frac{\\delta_{i}}{i!}\\|x_{k}-v_{k}\\|^{i-1}\\right)^{\\frac{2}{p-1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "For $p\\geq2$ we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\left\\{{\\frac{2}{p-1}}\\right.\\ =2,\\quad p=2,\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Consider some nonnegative sequence $\\{a_{i}|a_{i}\\geq0$ , $\\forall i\\in{\\overline{{1,n}}}\\}$ . For $p=2$ from Jensen inequality we know that $\\begin{array}{r}{\\left(\\sum_{i=1}^{n}a_{i}\\right)^{2}\\leq\\sum_{i=1}^{n}n a_{i}^{2}}\\end{array}$ . From Lemma 7 of [65] we know that $\\forall q\\in[0,1],x,y>$ $0\\to(x+y)^{q}\\leq x^{q}+y^{q}$ . Thus, for $p\\geq3$ we can come to the same conclusion: $(\\sum_{i=1}^{n}a_{i})^{\\frac{2}{p-1}}\\leq$ $\\begin{array}{r}{\\sum_{i=1}^{n}a_{i}^{\\frac{2}{p-1}}\\leq\\sum_{i=1}^{n}n a_{i}^{\\frac{2}{p-1}}}\\end{array}$ . In other words, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\left(\\sum_{i=1}^{n}a_{i}\\right)^{\\frac{2}{p-1}}\\leq\\sum_{i=1}^{n}a_{i}^{\\frac{2}{p-1}}\\leq\\sum_{i=1}^{n}n a_{i}^{\\frac{2}{p-1}},\\quad\\forall p\\geq2,a_{i}\\geq0.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Using this inequality, we get ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{i=1}^{T}\\lambda_{k}^{-\\frac{2}{p-1}}\\overset{(61)}{\\leq}p(20p-8)^{\\frac{2}{p-1}}\\sum_{k=1}^{T}\\left(\\frac{L_{p-1}}{p!}\\right)^{\\frac{2}{p-1}}\\|x_{k}-v_{k}\\|^{2}+p(20p-8)^{\\frac{2}{p-1}}\\sum_{k=1}^{T}\\sum_{i=1}^{p-1}\\left(\\frac{\\delta_{i}}{i!}\\right)^{\\frac{2}{p-1}}\\|x_{k}-v_{k}\\|^{2}}\\\\ {\\overset{\\mathrm{sp}}{\\leq}4p(20p-8)^{\\frac{2}{p-1}}\\left(\\frac{L_{p-1}}{p!}\\right)^{\\frac{2}{p-1}}\\|x^{*}-x_{0}\\|^{2}+p(20p-8)^{\\frac{2}{p-1}}\\sum_{k=1}^{T}\\sum_{i=1}^{p-1}\\left(\\frac{\\delta_{i}}{i!}\\right)^{\\frac{2}{p-1}}\\|x_{k}-v_{k}\\|^{\\frac{2(i-1)}{p-1}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Consider the second factor in this inequality. If we denote $a_{k}\\,=\\,\\|x_{k}\\,-\\,v_{k}\\|^{\\frac{2(i-1)}{p-1}}$ , bk = 1, c = $\\textstyle{\\frac{p-1}{i-1}}$ , $\\begin{array}{r}{d=\\frac{p-1}{p-i}}\\end{array}$ , then we can use H\u00f6lder inequality: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{n}|a_{k}b_{k}|\\leq\\left(\\sum_{k=1}^{n}|a_{k}|^{c}\\right)^{\\frac{1}{c}}\\left(\\sum_{i=1}^{n}|b_{k}|^{d}\\right)^{\\frac{1}{d}},\\quad\\frac{1}{c}+\\frac{1}{d}=1.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "From this we get ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{T}\\|x_{k}-v_{k}\\|^{\\frac{2(i-1)}{p-1}}\\leq\\left(\\sum_{k=1}^{T}\\|x_{k}-v_{k}\\|^{2}\\right)^{\\frac{i-1}{p-1}}\\left(\\sum_{k=1}^{T}1\\right)^{\\frac{p-i}{p-1}}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Thus, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{T}\\lambda_{k}^{-\\frac{2}{p-1}}\\leq4p(20p-8)^{\\frac{2}{p-1}}\\left(\\frac{L_{p-1}}{p!}\\right)^{\\frac{2}{p-1}}\\|x^{*}-x_{0}\\|^{2}+4p(20p-8)^{\\frac{2}{p-1}}\\sum_{i=1}^{p-1}\\left(\\frac{\\delta_{i}}{i!}\\right)^{\\frac{2}{p-1}}\\|x^{*}-x_{0}\\|\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Consider $(a+b)^{\\frac{p-1}{2}}$ , $p\\geq2$ . For $p=2$ from Lemma 7 in [65] we get $(a+b)^{\\frac{1}{2}}\\,\\leq\\,a^{\\frac{1}{2}}+b^{\\frac{1}{2}}\\,<$ $2(a^{\\frac{1}{2}}+b^{\\frac{1}{2}})$ . For $p\\geq3$ we have from Jensen inequality $\\begin{array}{r}{(a+b)^{\\frac{p-1}{2}}\\leq2^{\\frac{p-1}{2}-1}\\left(a^{\\frac{p-1}{2}}+b^{\\frac{p-1}{2}}\\right)<}\\end{array}$ 2p2 ap2\u22121+ bp2\u22121 . In other words, ", "page_idx": 29}, {"type": "equation", "text": "$$\n(a+b)^{\\frac{p-1}{2}}\\leq2^{\\frac{p}{2}}\\left(a^{\\frac{p-1}{2}}+b^{\\frac{p-1}{2}}\\right),\\quad\\forall p\\geq2.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Thus, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\left(\\sum_{k=1}^{T}\\lambda_{k}^{-\\frac{2}{p-1}}\\right)^{\\frac{p-1}{2}}}}\\\\ &{\\leq2^{\\frac{p}{2}}\\left[(4p)^{\\frac{p-1}{2}}(20p-8)\\frac{L_{p-1}}{p!}\\|x^{*}-x_{0}\\|^{p-1}+(4p)^{\\frac{p-1}{2}}(20p-8)\\left(\\sum_{i=1}^{p-1}\\left(\\frac{\\delta_{i}}{i!}\\right)^{\\frac{2}{p-1}}\\|x^{*}-x_{0}\\|^{\\frac{2(i-1)}{p-1}}\\right.\\right.}\\\\ &{}&{\\left.\\left.\\leq2^{p}p\\frac{p-1}{2}(20p-8)\\left[\\frac{L_{p-1}}{p!}\\|x^{*}-x_{0}\\|^{p-1}+2^{\\frac{p}{2}}\\sum_{i=1}^{\\frac{p-1}{i}}\\frac{\\delta_{i}}{i!}\\|x^{*}-x_{0}\\|^{i-1}T^{\\frac{p-i}{2}}\\right]\\right.\\right.}\\\\ &{}&{\\left.\\left.=2^{p}p\\frac{p-1}{2}(20p-8)\\frac{L_{p-1}}{p!}\\|x^{*}-x_{0}\\|^{p-1}+2^{\\frac{3p}{2}}p^{\\frac{p-1}{2}}(20p-8)\\sum_{i=1}^{D-1}\\frac{\\delta_{i}}{i!}\\|x^{*}-x_{0}\\|^{i-1}T^{\\frac{p-i}{2}}.\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Now, we can get the final result ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\qquad\\qquad\\qquad\\displaystyle\\frac{1}{\\sum_{k=1}^{T}\\lambda_{k}}\\le\\frac{\\left(\\sum_{k=1}^{T}\\lambda_{k}^{-\\frac{2}{p-1}}\\right)^{\\frac{p-1}{2}}}{T^{\\frac{p+1}{2}}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\displaystyle\\le\\frac{2^{p}p^{\\frac{p-1}{2}}(20p-8)\\frac{L_{p-1}}{p!}\\|x^{*}-x_{0}\\|^{p-1}}{T^{\\frac{p+1}{2}}}+2^{\\frac{3p}{2}}p^{\\frac{p-1}{2}}(20p-8)\\sum_{i=1}^{p-1}\\frac{\\delta_{i}\\|x^{*}-x_{0}\\|^{i-1}}{i!T^{\\frac{i+1}{2}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Finally, we provide the convergence rate of Algorithm 3 in monotone case. ", "page_idx": 30}, {"type": "text", "text": "Theorem 7.2 Let Assumptions 1.1, 4.2 with $i=p-1$ , and 7.1 hold. Then, after $T\\geq1$ iterations of VIHI with parameters $\\eta_{i}=5p$ , $o p t=0$ , we get the following bound ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathtt{G A P}(\\tilde{x}_{T})=\\underset{x\\in\\mathcal{X}}{\\operatorname*{sup}}\\left\\langle F(x),\\tilde{x}_{T}-x\\right\\rangle\\le O\\left(\\frac{L_{p-1}D^{p+1}}{T^{\\frac{p+1}{2}}}+\\sum_{i=1}^{p-1}\\frac{\\delta_{i}D^{i+1}}{T^{\\frac{i+1}{2}}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Proof. Consider $\\forall x\\in\\mathcal{X},\\,\\mathrm{opt}=0$ . ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\langle F(x),\\tilde{x}_{T}-x\\rangle}}\\\\ {{\\displaystyle=\\frac{1}{\\sum_{k=1}^{T}\\lambda_{k}}\\sum_{k=1}^{T}\\lambda_{k}\\left\\langle F(x_{k}),x_{k}-x\\right\\rangle\\overset{(56)}{\\leq}\\frac{1}{\\sum_{k=1}^{T}\\lambda_{k}}\\frac{1}{2}\\|x-x_{0}\\|^{2}\\leq\\frac{D^{2}}{2\\sum_{i=1}^{T}\\lambda_{k}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "equation", "text": "$$\n\\stackrel{(60)}{\\leq}2^{p-1}p^{\\frac{p-1}{2}}(20p-8)\\frac{L_{p-1}D^{p+1}}{p!T^{\\frac{p+1}{2}}}+2^{\\frac{3p}{2}-1}p^{\\frac{p-1}{2}}(20p-8)\\sum_{i=1}^{p-1}\\frac{\\delta_{i}D^{i+1}}{i!T^{\\frac{i+1}{2}}}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Thus, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathrm{GAP}(\\tilde{x}_{T})=\\operatorname*{sup}_{x\\in\\mathcal{X}}\\left\\langle F(x),\\tilde{x}_{T}-x\\right\\rangle=O\\left(\\frac{L_{p-1}D^{p+1}}{T^{\\frac{p+1}{2}}}+\\sum_{i=1}^{p-1}\\frac{\\delta_{i}D^{i+1}}{T^{\\frac{i+1}{2}}}\\right).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "G.4 Convergence in nonmonotone case ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "To make our paper more self-contained, we provide convergence rate of Algorithm 3 in nonmonotone case. ", "page_idx": 30}, {"type": "text", "text": "Theorem 7.3 Let Assumptions 4.2, 1.3 and 7.1 with $i=p-1$ hold. Then after $T\\geq1$ iterations of Algorithm 3 with parameters $\\eta=5p$ , $\\mathrm{opt}=2$ we get the following bound ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{RES}(\\hat{x}):=\\underset{x\\in\\mathcal{X}}{\\operatorname*{sup}}\\left\\langle F(\\hat{x}_{t}),\\hat{x}_{t}-x\\right\\rangle=O\\left(\\frac{L_{p-1}D^{p+1}}{T^{\\frac p2}}+\\sum_{i=1}^{p-1}\\frac{\\delta_{i}D^{i+1}}{T^{\\frac i2}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Proof. Consider $\\langle F(x_{k}),x_{k}-x\\rangle$ . ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\langle F(x_{k}),x_{k}-x\\rangle=\\langle F(x_{k})-\\Omega_{p,v_{k}}(x_{k}),x_{k}-x\\rangle+\\langle\\Omega_{p,v_{k}}(x_{k}),x_{k}-x\\rangle}\\\\ {\\displaystyle\\overset{(53)}{\\leq}\\|F(x_{k})-\\Omega_{p,v_{k}}(x_{k})\\|\\|x_{k}-x\\|+\\frac{{\\cal L}_{p-1}}{p!}\\|x_{k}-v_{k}\\|^{p+1}+\\displaystyle\\sum_{i=1}^{p-1}\\frac{\\delta_{i}}{i!}\\|x_{k}-v_{k}\\|^{i+1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "From definition of $\\Psi_{p,v_{k}}(x_{k})$ and triangle inequality we get ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\|F(x_{k})-\\Psi_{p,v_{k}}(x_{k})\\|=\\|F(x_{k})-\\Omega_{p,v_{k}}(x_{k})\\|-\\frac{5L_{p-1}}{(p-1)!}\\|x_{k}-v_{k}\\|^{p}-\\sum_{i=1}^{p-1}\\frac{\\eta_{i}\\delta_{i}}{i!}\\|x_{k}-v_{k}\\|^{i}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "From this and (55) we get ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle F({x}_{k})-\\Omega_{p,v_{k}}({x}_{k})\\|\\overset{(55)}{\\leq}\\frac{{L_{p-1}}}{p!}\\|{x}_{k}-{v}_{k}\\|^{p}+\\displaystyle\\sum_{i=1}^{p-1}\\frac{1}{i!}\\delta_{i}\\|{x}_{k}-{v}_{k}\\|^{i}+\\displaystyle\\frac{5L_{p-1}}{(p-1)!}\\|{x}_{k}-{v}_{k}\\|^{p}+\\sum_{i=1}^{p-1}\\frac{\\eta_{i}\\delta_{i}}{i!}\\|{x}_{k}-{v}_{k}\\|^{p}}\\\\ {\\displaystyle=\\frac{L_{p-1}}{p!}(5p+1)\\|{x}_{k}-{v}_{k}\\|^{p}+\\displaystyle\\sum_{i=1}^{p-1}\\frac{\\delta_{i}}{i!}(\\eta_{i}+1)\\|{x}_{k}-{v}_{k}\\|^{i}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Thus, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\langle F(x_{k}),x_{k}-x\\rangle}}\\\\ &{\\leq\\frac{L_{p-1}}{p!}(5p+1)\\|x_{k}-v_{k}\\|^{p}\\|x_{k}-x\\|+\\displaystyle\\sum_{i=1}^{p-1}\\frac{\\delta_{i}}{i!}(\\eta_{i}+1)\\|x_{k}-v_{k}\\|^{i}\\|x_{k}-x\\|}\\\\ &{}&{\\quad\\quad+\\displaystyle\\frac{L_{p-1}}{p!}\\|x_{k}-v_{k}\\|^{p+1}+\\displaystyle\\sum_{i=1}^{p-1}\\frac{\\delta_{i}}{i!}\\|x_{k}-v_{k}\\|^{i+1}}\\\\ &{\\leq\\displaystyle\\frac{L_{p-1}}{p!}\\|x_{k}-v_{k}\\|^{p}(5p+2)D+\\displaystyle\\sum_{i=1}^{p-1}\\frac{\\delta_{i}}{i!}\\|x_{k}-v_{k}\\|^{i}(\\eta_{i}+2)D.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "From second inequality of (38) and definition of $x_{k_{T}}$ we get ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\|x_{k_{T}}-v_{k_{t}}\\|^{2}\\equiv\\operatorname*{min}_{1\\leq k\\leq T}\\|x_{k}-v_{k}\\|^{2}\\leq\\displaystyle\\frac{1}{T}\\sum_{k=1}^{T}\\|x_{k}-v_{k}\\|^{2}\\leq\\frac{4}{T}\\|x^{*}-x_{0}\\|^{2}\\leq\\frac{4D^{2}}{T}}\\\\ {\\displaystyle\\Leftrightarrow\\|x_{k_{T}}-v_{k_{T}}\\|\\leq\\frac{4^{\\frac{i}{2}}D^{i}}{T^{\\frac{i}{2}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Combining all these results, we get ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\mathrm{RES}}(\\hat{x})=\\underset{x\\in\\mathcal{X}}{\\operatorname*{sup}}\\left\\langle F(x_{k}),x_{k}-x\\right\\rangle\\leq\\frac{L_{p-1}}{p!}\\|x_{k}-v_{k}\\|^{p}(5p+2)D+\\underset{i=1}{\\overset{p-1}{\\sum}}\\frac{\\delta_{i}}{i!}\\|x_{k}-v_{k}\\|^{i}(5p+2)D}\\\\ &{\\qquad\\qquad\\qquad\\leq\\frac{2^{p}\\left(5p+2\\right)}{p!}\\frac{L_{p-1}D^{p+1}}{T^{\\frac{p}{2}}}+\\underset{i=1}{\\overset{p-1}{\\sum}}\\frac{2^{i}\\delta_{i}}{i!}(\\eta_{i}+2)\\frac{D^{i+1}}{T^{\\frac{i}{2}}}}\\\\ &{\\qquad\\qquad\\qquad=O\\left(\\frac{L_{p-1}D^{p+1}}{T^{\\frac{p}{2}}}+\\underset{i=1}{\\overset{p-1}{\\sum}}\\frac{\\delta_{i}D^{i+1}}{T^{\\frac{i}{2}}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "H Subproblem solution ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "For $r$ -rank QN approximation $J^{r}$ from (19), we can effectively compute $A_{\\tau}^{-1}F(x)$ , where $A_{\\tau}=$ $\\begin{array}{r}{J^{r}+(\\eta\\delta+\\frac{5L}{2}\\tau)I=U^{\\top}C V+J^{0}+(\\eta\\delta+\\frac{5L}{2}\\tau)I=U^{\\top}C V+B}\\end{array}$ and $\\begin{array}{r}{B=J^{0}+(\\eta\\delta+\\frac{5L}{2}\\tau)I}\\end{array}$ by ", "page_idx": 31}, {"type": "equation", "text": "$$\nA_{\\tau}^{-1}F(x)=\\left(B+U^{\\top}C V\\right)^{-1}F(x)=B^{-1}F(x)-B^{-1}U^{\\top}(C^{-1}+V B^{-1}U^{\\top})^{-1}V B^{-1}F(x).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "For $J_{0}=\\iota I$ , the identity could be simplified even more. Hence, $\\begin{array}{r}{B=\\iota I+(\\eta\\delta+\\frac{5L}{2}\\tau)I=\\chi I}\\end{array}$ , where $\\begin{array}{r}{\\chi=\\iota+\\eta\\delta+\\frac{5L}2\\tau}\\end{array}$ . Then, ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{A_{\\tau}^{-1}F(x)=\\left(\\chi I+U^{\\top}C V\\right)^{-1}F(x)=\\frac{1}{\\chi}F(x)-\\frac{1}{\\chi}U^{\\top}(\\chi C^{-1}+V U^{\\top})^{-1}V F(x).}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "I Experiment details ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "We consider the cubic regularized bilinear min-max problem of the form: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{x\\in\\mathbb{R}^{d}}{\\operatorname*{min}}\\,\\underset{y\\in\\mathbb{R}^{d}}{\\operatorname*{max}}\\,f(x,y)=y^{\\top}(A x-b)+\\frac{\\rho}{6}\\|x\\|^{3},}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $\\rho>0,b=[1,0,\\dots,0]\\in\\mathbb{R}^{d}$ , and $A\\in\\mathbb{R}^{d\\times d}$ such that ", "page_idx": 32}, {"type": "equation", "text": "$$\nA=\\left[\\begin{array}{c c c c c}{1}&{-1}&{0}&{\\cdots}&{0}\\\\ {0}&{1}&{-1}&{\\ddots}&{\\vdots}\\\\ {\\vdots}&{\\ddots}&{\\ddots}&{\\ddots}&{0}\\\\ {0}&{\\cdots}&{0}&{1}&{-1}\\\\ {0}&{\\cdots}&{0}&{0}&{1}\\\\ {.}\\end{array}\\right]\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "To reformulate it as variational inequality, we define $F(x)=[\\nabla_{x}f(x,y),-\\nabla_{y}f(x,y)]$ . Following [71], we plot the restricted primal-dual gap (65), written in a closed form: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathrm{gap}(z,\\beta)=\\frac{\\rho}{6}\\|x\\|^{3}+\\beta\\|A x-b\\|+\\frac{2}{3}\\sqrt{\\frac{2}{\\rho}}\\|A^{\\top}y\\|^{\\frac{3}{2}}+b^{\\top}y,\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $z=(x,y)$ . ", "page_idx": 32}, {"type": "text", "text": "Setup. All methods and experiments were performed using Python 3.11.5, PyTorch 2.1.2, numpy 1.24.3 on a 16-inch MacBook Pro 2023 with an Apple M2 Pro and 32GB memory. ", "page_idx": 32}, {"type": "text", "text": "Parameters. For Figure 1, the dimension of the problem $d\\,=\\,50$ . The regularizer $\\rho$ is set to $\\rho=1e-3$ . The starting point $(x_{0},y_{0})=(0,0)$ is all zeroes. We present results of last iteration of each method or op ${}^{=1}$ from Algorighm 1. The diameter $\\beta$ for the gap is set as $\\beta=1$ . For QN methods, we set memory size $m=r=20$ . We perform a total of 100000 iterations for all methods except Perseus2 with 1000 iterations. We plot each 500 iterations for a better visualisation. In Figure 1, the left plot refers to the gap decrease per iteration and the right plot refers to the gap decrease per JVP/operator computations. EG, Perseus1, VIQA computes 2 operators per iteration and Perseus2 computes $d+1$ JVP/operators. ", "page_idx": 32}, {"type": "text", "text": "We finetuned learning rate and presented the run with the best results. For EG1: $l r\\,=\\,0.5$ . For Perseus1: $L_{0}=0.2334$ . For Perseus2: $L_{1}=0.0001$ . For VIQA Broyden: $L_{1}=0.001$ , $\\delta=J^{0}=$ 0.4. For VIQA Damped Broyden: $L_{1}=0.001$ , $\\delta=J^{0}=0.22$ . Note, that tuned $L_{1}=0.001$ is actually a theoretical smoothness constant for the problem (63) and can be chosen with such value without finetuning. Also, Perseus2 is working with a smaller constant than theoretical. ", "page_idx": 32}, {"type": "text", "text": "J Application to minmax problems ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "J.1 Preliminaries ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "In this section, we consider the problem of finding a global saddle point of the following min-max optimization problem: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x\\in\\mathbb{R}^{m}}\\operatorname*{max}_{y\\in\\mathbb{R}^{n}}\\;f(x,y),\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "i.e., a tuple $(x^{*},y^{*})\\in\\mathbb{R}^{m}\\times\\mathbb{R}^{n}$ such that ", "page_idx": 32}, {"type": "equation", "text": "$$\nf(x^{*},y)\\leq f(x^{*},y^{*})\\leq f(x,y^{*}),\\quad{\\mathrm{for~all~}}x\\in\\mathbb{R}^{m},\\;y\\in\\mathbb{R}^{n},\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where the continuously differentiable objective function $f$ is convex-concave: $f(x,y)$ is convex in $x$ for all $y\\in\\mathbb{R}^{n}$ and concave in $y$ for all $x\\in\\mathbb{R}^{m}$ . ", "page_idx": 32}, {"type": "text", "text": "Input: initial point $z_{0}\\in\\mathcal{X}$ , parameters $L_{1},\\delta,\\eta,\\tau$ .   \nInitialization: set $s_{0}=0\\in\\mathbb{R}^{d}$ .   \nfor $k=0,1,2,\\ldots,T$ do Set $v_{k+1}=z_{0}+s_{k+1}$ . Compute $z_{k+1}\\in\\mathbb{R}^{n+m}$ such that condition (68) holds true. Compute $\\lambda_{k+1}$ such that $\\begin{array}{r}{\\frac{1}{16}\\leq\\lambda_{k+1}\\left(\\frac{L}{2}\\|z_{k+1}-v_{k+1}\\|+\\delta\\right)\\leq\\frac{1}{12}.}\\end{array}$ Compute $s_{k+1}=s_{k}-\\lambda_{k+1}F(z_{k+1})$ .   \nOutput: z\u02dcT = $\\begin{array}{r}{\\tilde{z}_{T}=\\frac{1}{\\sum_{k=1}^{T}\\lambda_{k}}\\sum_{k=1}^{T}\\lambda_{k}z_{k}}\\end{array}$ ", "page_idx": 33}, {"type": "text", "text": "Assumption J.1 The function $f(x,y)\\in{\\mathcal{C}}^{2}$ has $L$ -Lipschitz-continious second-order derivative if ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\|\\nabla^{2}f(z)-\\nabla^{2}(v)\\|\\leq L\\|z-v\\|,\\quad{\\mathrm{for~all~}}z,v\\in\\mathbb{R}^{n+m}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Following [71], we define the restricted gap function to measure the optimality of point $\\hat{z}=(\\hat{x},\\hat{y})$ ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\operatorname{gap}({\\hat{z}},\\beta)=\\operatorname*{max}_{y:\\|y-y^{*}\\|\\leq\\beta}f({\\hat{x}},y)-\\operatorname*{min}_{x:\\|x-x^{*}\\|\\leq\\beta}f(x,{\\hat{y}})\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $\\beta$ is sufficiently large such that $\\|\\hat{z}-z^{*}\\|\\leq\\beta$ . ", "page_idx": 33}, {"type": "text", "text": "Problem (64) is a special case of VI defined by the following operator ", "page_idx": 33}, {"type": "equation", "text": "$$\nF(\\mathbf{z})=\\left[\\,\\sum_{\\mathbf{y}}f(x,y)\\,\\right].\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "The Jacobian of $F$ is defined as follows ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\nabla^{2}F(z)=\\left[\\!\\!\\begin{array}{c c}{\\nabla_{x x}^{2}f(x,y)}&{\\nabla_{x y}^{2}f(x,y)}\\\\ {-\\nabla_{x y}^{2}f(x,y)}&{-\\nabla_{y y}^{2}f(x,y)}\\end{array}\\!\\!\\right].\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "The following lemma [80, 71] provides properties of operator $F$ . ", "page_idx": 33}, {"type": "text", "text": "Lemma J.2 Let Assumption J.1 hold. Then ", "page_idx": 33}, {"type": "text", "text": "1. The operator $F$ is monotone, i.e. satisfies Assumption 1.1.   \n2. The operator $F$ is $L$ -smooth, i.e. satisfies Assumption 1.2.   \n3. $F(z^{*})=0$ for any global saddle point $z^{\\ast}\\in\\mathbb{R}^{n+m}$ of the function $f$ . ", "page_idx": 33}, {"type": "text", "text": "We assume that inexact approximation of Jacobian satisfies Assumption 2.1. ", "page_idx": 33}, {"type": "text", "text": "J.2 The method ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Since we consider unconstrained minmax optimization, Step 2 of VIJI simplifies to $v_{k+1}=z_{0}+s_{k}$ and the subproblem (11) changes to ", "page_idx": 33}, {"type": "text", "text": "Usually, to find the solution of this subproblem it is necessary to run some addition subroutine. Following the work [71], we introduce the following approximate condition: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\Omega_{v_{k+1}}^{\\eta}(z_{k+1})\\|\\leq\\tau\\operatorname*{min}\\left\\{\\frac{L}{2}\\|z_{k+1}-v_{k+1}\\|^{2}+\\delta\\|z_{k+1}-v_{k+1}\\|,\\|F(v_{k+1})\\|\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $\\tau\\in(0,1)$ is a tolerance parameter. ", "page_idx": 33}, {"type": "text", "text": "The version of VIJI for unconstrained min-max problems is referred to as VIJI-MinMax and is detailed in Algorithm 4. This subproblem can solved by strategy proposed in [71], resulting in $O\\left((n+m)^{\\omega}\\varepsilon^{-2/3}+(n+m)^{2}\\varepsilon^{-2/3}\\log\\log(1/\\varepsilon)\\right)$ complexity, where $\\omega\\approx2.3728$ is the matrix multiplication constant. ", "page_idx": 33}, {"type": "text", "text": "J.3 Convergence analysis ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "The following theorem provides convergence rate for Algorithm 4. ", "page_idx": 34}, {"type": "text", "text": "Theorem J.3 Let Assumptions J.1, 2.1 hold. Then, after $T\\geq1$ iterations of VIJI-MinMax with parameters $\\eta=5$ , we get the following bound ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{gap}(\\tilde{z}_{T},\\beta)\\leq\\frac{1152\\sqrt{2}L\\|z_{0}-z^{*}\\|^{3}}{T^{3/2}}+\\frac{576\\sqrt{2}\\delta\\|z_{0}-\\tilde{z}^{*}\\|^{2}}{T},}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where $\\beta=5\\|z_{0}-z^{*}\\|,\\,z^{*}=(x^{*},y^{*})$ . ", "page_idx": 34}, {"type": "text", "text": "Now, let us introduce additional assumption on $\\delta$ , to obtain the convergence with optimal rate $O(\\varepsilon^{-2/3})$ . ", "page_idx": 34}, {"type": "text", "text": "Theorem J.4 Let Assumptions J.1 hold. Let ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\|(\\nabla F(v_{k})-J(v_{k}))[z_{k}-v_{k}]\\|\\leq\\delta_{k}\\|z_{k}-v_{k}\\|\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "hold for iterates $\\{z_{k},v_{k}\\}$ generated by Algorithm 4, where ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\delta_{k}\\leq L\\|z_{k}-v_{k}\\|.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Then, after $T\\geq1$ iterations of VIJI-MinMax with parameters $\\eta=5$ , we get the following bound ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{gap}(\\tilde{z}_{T},\\beta)\\leq\\frac{1944\\sqrt{2}L\\|z_{0}-z^{*}\\|^{3}}{T^{3/2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where $\\beta=5\\|z_{0}-z^{*}\\|$ , $z^{*}=(x^{*},y^{*})$ . ", "page_idx": 34}, {"type": "text", "text": "Note, that it is also possible to change adaptive strategy for $\\lambda_{k}$ in this case to $\\begin{array}{r}{\\frac{1}{16}\\leq\\frac{3}{2}L\\|z_{k}-v_{k}\\|\\leq\\frac{1}{12}}\\end{array}$ (by introducing parameter $\\beta$ as in Algorithm 1) to eliminate the dependence on $\\delta$ from the step size while achieving the same convergence rate. ", "page_idx": 34}, {"type": "text", "text": "Finally, let us show, that we mathch the convergence of [71] under the same assumptions on Jacobian\u2019s inexactness and subproblem\u2019s solution. ", "page_idx": 34}, {"type": "text", "text": "Assumption J.5 ([71]) Let ", "text_level": 1, "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|(\\nabla F(v_{k})-J(v_{k}))[z_{k}-v_{k}]\\|\\le\\delta_{k}\\|z_{k}-v_{k}\\|,\\quad\\|J(v_{k})\\|\\le\\kappa}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "hold for iterates $\\{z_{k},v_{k}\\}$ generated by Algorithm 4, where ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\delta_{k}\\leq\\operatorname*{min}\\left\\{\\tau_{0},\\frac{L(1-\\tau)}{4\\kappa+6L}\\|F(v_{k})\\|\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where $\\begin{array}{r}{\\tau_{0}<\\frac{L}{4}}\\end{array}$ . ", "page_idx": 34}, {"type": "text", "text": "Next, we show, that (69) follows from (70). ", "page_idx": 34}, {"type": "text", "text": "Let us consider two cases. If $\\|z_{k}-v_{k}\\|\\leq1$ , then from (70), we get $\\begin{array}{r}{\\delta_{k}\\leq\\tau_{0}\\|z_{k}-v_{k}\\|\\leq\\frac{L}{2}\\|z_{k}-v_{k}\\|}\\end{array}$ . Otherwise, if $\\|z_{k}-v_{k}\\|\\geq1$ , we obtain from (68) ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|F(v_{k})\\|-\\|J(v_{k})\\|\\|z_{k}-v_{k}\\|-\\delta\\|z_{k}-v_{k}\\|-5L\\|z_{k}-v_{k}\\|\\leq\\|\\nabla\\Omega_{v_{k}}(z_{k})\\|\\leq\\tau\\|F(v_{k})\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Using our assumptions, we get ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(1-\\tau)\\|F(v_{k})\\|\\le\\kappa\\|z_{k}-v_{k}\\|+\\delta\\|z_{k}-v_{k}\\|+5L\\|z_{k}-v_{k}\\|^{2}\\le(\\kappa+\\delta+5L)\\|z_{k}-v_{k}\\|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\le(\\kappa+\\frac{21}{4}L)\\|z_{k}-v_{k}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Next, ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\delta\\leq\\frac{L(1-\\tau)}{4\\kappa+6L}\\|F(v_{k})\\|\\leq L\\|z_{k}-v_{k}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Thus, the assumptions of Theorem J.4 hold, and Algorithm 4 achieves $O(\\varepsilon^{-2/3})$ convergence. ", "page_idx": 34}, {"type": "text", "text": "J.4 Proof of Theorem J.3 ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Again, as in the proof of Theorem 3.2, we introduce the Lyapunov function ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{E}_{k}=\\underset{v\\in\\mathbb{R}^{n+m}}{\\operatorname*{max}}\\left\\langle s_{k},v-z_{0}\\right\\rangle-\\frac{1}{2}\\|v-z_{0}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Note that the scalar product $\\langle s_{k},v-z_{0}\\rangle$ can be omitted (see the proof of [71, Theorems 3.1, 4.1]), which would also eliminate the somewhat redundant Step 2 of Algorithm 4. However, we chose to retain it, as it does not affect the method\u2019s performance. We retain ", "page_idx": 35}, {"type": "text", "text": "Lemma J.6 Let Assumptions J.1, 2.1 hold. Then, for every integer $T\\geq1$ , we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{T}\\lambda_{k}\\langle F(z_{k}),z_{k}-z\\rangle\\leq\\mathcal{E}_{0}-\\mathcal{E}_{T}+\\langle s_{T},z-z_{0}\\rangle-\\frac{1}{8}\\left(\\sum_{k=1}^{T}\\Vert z_{k}-v_{k}\\Vert^{2}\\right),\\quad\\mathrm{for~all~}z\\in\\mathbb{R}^{n+m}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Proof. Following the proof of Lemma B.1, we arrive at (35), where the proofs begin to slightly diverge due to the changes in the subproblem. At this juncture, we have: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{T}\\lambda_{k}\\langle F(z_{k}),z_{k}-z\\rangle\\leq\\mathcal{E}_{0}-\\mathcal{E}_{T}+\\langle s_{T},z-z_{0}\\rangle+\\sum_{k=1}^{T}\\lambda_{k}\\langle F(z_{k}),z_{k}-v_{k+1}\\rangle-\\frac{1}{2}\\|v_{k}-v_{k+1}\\|^{2}\\,.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Then, ", "text_level": 1, "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\qquad\\qquad\\langle F(z_{k}),z_{k}-v_{k+1}\\rangle}\\\\ &{=\\langle F(z_{k})-\\Omega_{v_{k}}^{\\eta}(z_{k})+\\eta\\delta(z_{k}-v_{k})+5L\\|z_{k}-v_{k}\\|(z_{k}-v_{k}),z_{k}-v_{k+1}\\rangle}\\\\ &{\\quad+\\langle\\Omega_{v_{k}}^{\\eta}(z_{k}),z_{k}-v_{k+1}\\rangle-5L\\|z_{k}-v_{k}\\|\\langle z_{k}-v_{k},z_{k}-v_{k+1}\\rangle-\\eta\\delta\\langle z_{k}-v_{k},z_{k}-v_{k+1}\\rangle}\\\\ &{\\stackrel{e m.(2,2),(68)}{\\leq}\\frac{L}{2}\\|z_{k}-v_{k}\\|^{2}\\|z_{k}-v_{k+1}\\|+\\delta\\|z_{k}-v_{k}\\|\\|z_{k}-v_{k+1}\\|+\\frac{\\tau L}{2}\\|z_{k}-v_{k}\\|^{2}\\|z_{k}-v_{k+1}\\|}\\\\ &{\\quad+\\tau\\delta\\|z_{k}-v_{k}\\|\\|z_{k}-v_{k+1}\\|-5L\\|z_{k}-v_{k}\\|\\langle z_{k}-v_{k},z_{k}-v_{k+1}\\rangle-\\eta\\delta\\langle z_{k}-v_{k},z_{k}-v_{k+1}\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Next, using $\\langle z_{k}-v_{k},z_{k}-v_{k+1}\\rangle\\,\\geq\\,\\|z_{k}-v_{k}\\|^{2}-\\|z_{k}-v_{k}\\|\\|v_{k}-v_{k+1}\\|$ and $\\|z_{k}\\textrm{--}v_{k+1}\\|\\leq$ $\\|z_{k}-v_{k}\\|+\\|v_{k}-v_{k+1}\\|$ , we get ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\qquad\\qquad\\qquad\\langle F(z_{k}),z_{k}-v_{k+1}\\rangle}\\\\ &{\\qquad\\qquad\\leq(\\tau+1)\\frac{L}{2}\\|z_{k}-v_{k}\\|^{3}+(\\tau+1)\\frac{L}{2}\\|z_{k}-v_{k}\\|^{2}\\|v_{k}-v_{k+1}\\|+(\\tau+1)\\delta\\|z_{k}-v_{k}\\|^{2}}\\\\ &{+(\\tau+1)\\delta\\|z_{k}-v_{k}\\|\\|v_{k}-v_{k+1}\\|-5L\\|z_{k}-v_{k}\\|\\langle z_{k}-v_{k},z_{k}-v_{k+1}\\rangle-\\eta\\delta\\langle z_{k}-v_{k},z_{k}-v_{k+1}\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\overset{\\tau<1}{\\leq}6L\\|z_{k}-v_{k}\\|^{2}\\|v_{k}-v_{k+1}\\|-4L\\|z_{k}-v_{k}\\|^{3}}\\\\ &{\\qquad\\qquad\\qquad+\\left(\\eta+1\\right)\\delta\\|z_{k}-v_{k}\\|\\|v_{k}-v_{k+1}\\|-(\\eta-1)\\,\\delta\\|z_{k}-v_{k}\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Next, ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{II}\\leq\\displaystyle\\sum_{k=1}^{T}\\left(\\frac{1}{2}\\|z_{k}-v_{k}\\|\\|v_{k}-v_{k+1}\\|-\\frac{1}{4}\\|z_{k}-v_{k}\\|^{2}-\\frac{1}{2}\\|v_{k}-v_{k+1}\\|^{2}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where the last inequality is due to the following choice of $\\eta=10$ and $\\lambda$ : ", "page_idx": 35}, {"type": "text", "text": "$\\begin{array}{r}{\\frac{1}{16}\\leq\\lambda_{k}\\left(L\\|z_{k}-\\stackrel{\\cdot}{v}_{k}\\|\\right.\\dot{+}\\delta\\right)\\leq\\frac{1}{12}}\\end{array}$ . Then, ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{II}\\leq\\displaystyle\\sum_{k=1}^{T}\\frac{1}{2}\\|z_{k}-v_{k}\\|\\|v_{k}-v_{k+1}\\|-\\frac{1}{4}\\|z_{k}-v_{k}\\|^{2}-\\frac{1}{2}\\|v_{k}-v_{k+1}\\|^{2}\\leq-\\frac{1}{8}\\left(\\sum_{k=1}^{T}\\|z_{k}-v_{k}\\|^{2}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Plugging (73) into (72) yields that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{T}\\lambda_{k}\\langle F(z_{k}),z_{k}-z\\rangle\\leq\\mathcal{E}_{0}-\\mathcal{E}_{T}+\\langle s_{T},z-z_{0}\\rangle-\\frac{1}{8}\\left(\\sum_{k=1}^{T}\\|z_{k}-v_{k}\\|^{2}\\right).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Next, Lemma B.2 can be applied. Next Lemma is a counterpart of Lemma B.3 for the current choice of $\\lambda$ . ", "page_idx": 36}, {"type": "text", "text": "Lemma J.7 Let Assumptions J.1, 2.1 hold. For every integer $T\\geq1$ , we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\frac{1}{\\left(\\sum_{k=1}^{T}\\lambda_{k}\\right)^{2}}\\leq\\frac{2048L^{2}\\|z^{*}-z_{0}\\|^{2}}{T^{3}}+\\frac{512\\delta^{2}}{T^{2}},\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where $x^{*}\\in\\mathcal{X}$ denotes the weak solution to the VI. ", "page_idx": 36}, {"type": "text", "text": "Proof. Without loss of generality, we assume that $z_{0}\\neq z^{*}$ . We have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k=1}^{T}(\\lambda_{k})^{-2}16^{-2}\\leq\\displaystyle\\sum_{k=1}^{T}(\\lambda_{k})^{-2}\\left(\\lambda_{k}\\left(L\\|z_{k}-v_{k}\\|+\\delta\\right)\\right)^{2}=\\displaystyle\\sum_{k=1}^{T}\\left(L\\|z_{k}-v_{k}\\|+\\delta\\right)^{2}}\\\\ &{\\qquad\\qquad\\leq\\displaystyle\\sum_{k=1}^{T}2L^{2}\\|z_{k}-v_{k}\\|^{2}+2T\\delta^{2}\\overset{\\mathrm{Lemma~}B.2}{\\leq}8L^{2}\\|z^{*}-z_{0}\\|^{2}+2T\\delta^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "By the H\u00f6lder inequality, we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{T}1=\\sum_{k=1}^{T}\\left((\\lambda_{k})^{-2}\\right)^{1/3}(\\lambda_{k})^{2/3}\\leq\\left(\\sum_{k=1}^{T}(\\lambda_{k})^{-2}\\right)^{1/3}\\left(\\sum_{k=1}^{T}\\lambda_{k}\\right)^{2/3}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Putting these pieces together yields that ", "page_idx": 36}, {"type": "equation", "text": "$$\nT\\leq16^{2/3}(8L^{2}||z^{*}-z_{0}||^{2}+2\\delta^{2}T)^{\\frac{1}{3}}\\left(\\sum_{k=1}^{T}\\lambda_{k}\\right)^{2/3},\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Plugging this into the above inequality yields that ", "page_idx": 36}, {"type": "equation", "text": "$$\n{\\frac{1}{\\left(\\sum_{k=1}^{T}\\lambda_{k}\\right)^{2}}}\\leq{\\frac{2048L^{2}\\|z^{*}-z_{0}\\|^{2}}{T^{3}}}+{\\frac{512\\delta^{2}}{T^{2}}}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Next, by Lemma J.6, we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l l}{\\displaystyle0\\overset{\\mathrm{Lem.I.},2}{\\leq}\\sum_{k=1}^{T}\\lambda_{k}\\langle F(z_{k}),z_{k}-z\\rangle+-\\frac{1}{8}\\left(\\displaystyle\\sum_{k=1}^{T}\\|z_{k}-v_{k}\\|^{2}\\right)\\leq\\mathcal{E}_{0}-\\mathcal{E}_{T}+\\langle s_{T},z-z_{0}\\rangle}&{}\\\\ {\\overset{(71)}{=}\\displaystyle\\frac{1}{2}\\|v_{0}-z_{0}\\|^{2}-\\frac{1}{2}\\|v_{T}-z_{0}\\|^{2}+\\langle v_{T}-z_{0},v^{*}-v_{0}\\rangle.}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "By applying Young\u2019s ineqaulity and the fact that $z_{0}=v_{0}$ , we get ", "page_idx": 36}, {"type": "equation", "text": "$$\n0\\leq-\\frac{1}{2}\\|v_{k}-z_{0}\\|^{2}+\\frac{1}{4}\\|v_{k}-z_{0}\\|^{2}+\\|z^{*}-z_{0}\\|^{2}=-\\frac{1}{4}\\|v_{k}-z_{0}\\|^{2}+\\|z^{*}-z_{0}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Thus, we have $\\|v_{k}-z_{0}\\|\\leq2\\|z^{*}-z_{0}\\|$ . From Lemma B.2 we also have that $\\|z_{k}-v_{k}\\|\\leq2\\|z^{*}-z_{0}\\|$ . Thus, ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|v_{k}-z^{*}\\|\\leq\\|v_{k}-z_{0}\\|+\\|z_{0}-z^{*}\\|\\leq3\\|z_{0}-z^{*}\\|\\leq\\beta,}\\\\ &{\\|z_{k}-z^{*}\\|\\leq\\|z_{k}-v_{k}\\|+\\|z_{k}-z^{*}\\|\\leq5\\|z_{0}-z^{*}\\|=\\beta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Lemma B.2 also implies ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{T}\\lambda_{k}(z_{k}-z)^{\\top}F(z_{k})\\leq{\\textstyle\\frac{1}{2}}\\|z_{0}-z\\|^{2}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "By Proposition [71, Proposition 2.9], we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r}{f(\\tilde{x}_{T},y)-f(x,\\tilde{y}_{T})\\leq\\frac{1}{\\sum_{k=1}^{T}\\lambda_{k}}\\left(\\displaystyle\\sum_{k=1}^{T}\\lambda_{k}(z_{k}-z)^{\\top}F(z_{k})\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Putting these pieces together yields ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r}{f(\\tilde{x}_{T},y)-f(x,\\tilde{y}_{T})\\leq\\frac{1}{2(\\sum_{k=1}^{T}\\lambda_{k})}\\|z_{0}-z\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "This together with Lemma J.7 yields ", "page_idx": 37}, {"type": "equation", "text": "$$\nf(\\tilde{x}_{T},y)-f(x,\\tilde{y}_{T})\\leq\\frac{8\\sqrt{2}L\\|z_{0}-z^{*}\\|\\|z_{0}-z\\|^{2}}{T^{3/2}}+\\frac{4\\sqrt{2}\\delta\\|z_{0}-z\\|^{2}}{T}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Since $\\|z_{k}-z^{*}\\|\\leq\\beta$ for all $k\\geq0$ , we have $\\|\\tilde{z}_{T}-z^{*}\\|\\leq\\beta$ . By the definition of the restricted gap function, we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{gap}(\\tilde{z}_{T},\\beta)\\le\\frac{32\\sqrt{2}L\\|z_{0}-z^{*}\\|(\\|z_{0}-z^{*}\\|+\\beta)^{2}}{T^{3/2}}+\\frac{16\\sqrt{2}\\delta(\\|z_{0}-z^{*}\\|+\\beta)^{2}}{T}\\qquad}\\\\ {\\le\\frac{1152\\sqrt{2}L\\|z_{0}-z^{*}\\|^{3}}{T^{3/2}}+\\frac{576\\sqrt{2}\\delta\\|z_{0}-\\tilde{z}^{*}\\|^{2}}{T}.\\qquad\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Therefore, we conclude from the above inequality that there exists some $T>0$ such that the output $\\hat{z}$ satisfies that $\\mathrm{gap}(\\hat{z},\\beta)\\leq\\epsilon$ . ", "page_idx": 37}, {"type": "text", "text": "J.5 Proof of Theorem J.4 ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Lemma (J.6) hold with slight modification in $\\lambda_{k}$ adaptive strategy. Lemma B.2 also holds. Next we need slight modification of Lemma B.3 for the current choice of $\\lambda$ . ", "page_idx": 37}, {"type": "text", "text": "Lemma J.8 Let Assumption J.1. For every integer $T\\geq1$ , we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\frac{1}{\\left(\\sum_{k=1}^{T}\\lambda_{k}\\right)^{2}}\\leq\\frac{54L^{2}\\|z^{*}-z_{0}\\|^{2}}{T^{3}},\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where $x^{*}\\in\\mathcal{X}$ denotes the weak solution to the $V I.$ . ", "page_idx": 37}, {"type": "text", "text": "Proof. Without loss of generality, we assume that $z_{0}\\neq z^{*}$ . We have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k=1}^{T}(\\lambda_{k})^{-2}16^{-2}\\leq\\displaystyle\\sum_{k=1}^{T}(\\lambda_{k})^{-2}\\left(\\lambda_{k}\\left(L\\|z_{k}-v_{k}\\|+\\delta\\right)\\right)^{2}=\\displaystyle\\sum_{k=1}^{T}\\left(L\\|z_{k}-v_{k}\\|+\\delta\\right)^{2}}\\\\ &{\\overset{(69)}{\\leq}\\displaystyle\\sum_{k=1}^{T}\\frac{9L^{2}}{4}\\|z_{k}-v_{k}\\|^{2}\\overset{\\mathrm{Lemma~}B.2}{\\leq}\\frac{9L^{2}}{4}\\|z^{*}-z_{0}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "By the H\u00f6lder inequality, we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{T}1=\\sum_{k=1}^{T}\\left((\\lambda_{k})^{-2}\\right)^{1/3}(\\lambda_{k})^{2/3}\\leq\\left(\\sum_{k=1}^{T}(\\lambda_{k})^{-2}\\right)^{1/3}\\left(\\sum_{k=1}^{T}\\lambda_{k}\\right)^{2/3}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Putting these pieces together yields that ", "page_idx": 37}, {"type": "equation", "text": "$$\nT\\leq16^{2/3}(\\frac{9}{4}L^{2}\\|z^{*}-z_{0}\\|^{2})^{\\frac{1}{3}}\\left(\\sum_{k=1}^{T}\\lambda_{k}\\right)^{2/3},\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Plugging this into the above inequality yields that ", "page_idx": 37}, {"type": "equation", "text": "$$\n{\\frac{1}{\\left(\\sum_{k=1}^{T}\\lambda_{k}\\right)^{2}}}\\leq{\\frac{54L^{2}\\|z^{*}-z_{0}\\|^{2}}{T^{3}}}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Next, directly following the steps of proof of Theorem J.3, we get the following rate ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{gap}(\\tilde{z}_{T},\\beta)\\leq\\frac{1944\\sqrt{2}L\\|z_{0}-z^{*}\\|^{3}}{T^{3/2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "We proposed an algorithm for variational inequalities with inexact Jacobians in Section 3, supported its optimality by establishing the lower bound in Section 4, introduced QuasiNewton updates in Section 5, studied strongly-convex case in Section 6 and discussed tensor generalization in Section 7. Section 8 provides experimental results to validate our proposed approach. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 38}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Justification: the assumptions used in this paper are classical for methods for variational inequalities [69, 21]. Our methods include the solution of auxilarary subproblem (as other globally convergent second-order methods for VIs [69, 21]), which requires subsolver or addditional subroutine, what we explicitly stated. Moreover, we introduced the way to reduce the complexity and provided the computational cost of the proposed procedure. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best ", "page_idx": 38}, {"type": "text", "text": "judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 39}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: For all theoretical results, we provide proofs in the main paper and Appendix. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 39}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: our algorithm is listed in the paper (Algorithm 1) and the details are fully described (Sections 3 and 5). The expereimantal details for reproducibility are describes in Section 8 and Appendix I. The code is attached. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 39}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 40}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: we describe experimental details in Section 8 and Appendix I. The code is attached, where all the methods implemented as PyTorch optimizers. Our experiments do not include datasets, but we provide the explicit definition of test functions. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 40}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Justification: we describe experimental details in Section 8 and Appendix I. The code is attached. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 40}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: we test our optimizer on deterministic functions. Hence, the iteration of the methods are deterministic and provide deterministic solution. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 41}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: we describe compute resourses in Appendix I. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 41}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: Yes, the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 41}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 42}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: [NA] Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 42}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: see Section 8. Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets. \u2022 The authors should cite the original paper that produced the code package or dataset. \u2022 The authors should state which version of the asset is used and, if possible, include a URL. ", "page_idx": 42}, {"type": "text", "text": "\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 43}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 43}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 43}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 43}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 43}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 43}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 43}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 44}]