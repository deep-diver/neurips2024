[{"figure_path": "SQVns9hWJT/figures/figures_1_1.jpg", "caption": "Figure 1: Conceptual illustration of the decomposition of STE by TEXTCTRL. (a) Text style is disentangled into text background, text foreground, text font glyph and text color features. (b) Text glyph structure is represented by the cluster centroid of various font text features. (c) The explicit style features and structure features guide the generator to perform scene text editing.", "description": "This figure illustrates the TextCtrl method's decomposition of Scene Text Editing (STE) into three sub-tasks.  Panel (a) shows the disentanglement of text style into its constituent features: background, foreground, font glyph, and color.  Panel (b) depicts how the glyph structure is represented using a cluster centroid approach based on various font text features. Panel (c) demonstrates how these extracted style and structure features guide the generator to perform the actual scene text editing.", "section": "1 Introduction"}, {"figure_path": "SQVns9hWJT/figures/figures_3_1.jpg", "caption": "Figure 2: Decomposed framework of TextCtrl. (a) Text glyph structure encoder T with corresponding glyph structure representation pre-training. (b) Text style encoder S with corresponding style disentanglement pre-training. (c) Prior guided diffusion generator G. (d) The improved inference control with the Glyph-adaptive Mutual Self-attention mechanism.", "description": "This figure shows the architecture of the TextCtrl model, which is composed of four main parts: (a) Text Glyph Structure Encoder, (b) Text Style Encoder, (c) Diffusion Generator, and (d) Glyph-adaptive Mutual Self-Attention.  The encoder parts pre-process the input text and image respectively, extracting stylistic and structural information. The generator then uses this information to create a modified image.  Finally, the Glyph-adaptive Mutual Self-Attention mechanism refines the generation by incorporating information from the original image.  The diagram visually outlines the flow of information between each component.", "section": "3 Method"}, {"figure_path": "SQVns9hWJT/figures/figures_7_1.jpg", "caption": "Figure 3: Qualitative comparison among different methods. Note that for the inpainting-based methods [20, 21, 22], we conduct the editing on the full-size images and perform the visualization of the edited text region.", "description": "This figure shows a qualitative comparison of the results of different scene text editing methods, including SRNet, MOSTEL, DiffSTE, TextDiffuser, AnyText, and the proposed TextCtrl method.  Each row demonstrates the results for a different input image and target text change.  It highlights the variations in style preservation, text accuracy, and overall visual quality achieved by each method. The inpainting-based methods edit the full-size images, but only the edited text region is shown here for comparison.", "section": "4.2 Performance Comparison"}, {"figure_path": "SQVns9hWJT/figures/figures_8_1.jpg", "caption": "Figure 4: Qualitative comparison with inpainting-based methods [20, 21, 22] on full-size images.", "description": "This figure compares the results of four different methods (DiffSTE, TextDiffuser, AnyText, and TextCtrl) for scene text editing on three full-size images.  Each row represents a different image and each column shows the results from a specific method. It demonstrates the visual differences between these methods, particularly regarding the accuracy of text rendering, style preservation, and overall image quality.", "section": "4 Experiments"}, {"figure_path": "SQVns9hWJT/figures/figures_8_2.jpg", "caption": "Figure 5: t-SNE [53] visualization of style features by pre-trained text style encoder.", "description": "This figure visualizes the style feature embedding using t-SNE.  It shows that text images with similar color cluster in different regions of feature space, indicating that the style representation captures the image entirety. A closer look reveals that text images sharing the same text style and background are clustered together, regardless of the specific text content, highlighting the effectiveness of the style disentanglement process.", "section": "3.2 Text Style Disentanglement Pre-training"}, {"figure_path": "SQVns9hWJT/figures/figures_9_1.jpg", "caption": "Figure 6: Visualization of text editing result with and without the proposed GaMuSa during inference, which verifies the improvements in background color, text font and glyph texture.", "description": "This figure shows a qualitative comparison of text editing results with and without the Glyph-adaptive Mutual Self-Attention (GaMuSa) mechanism. It demonstrates the effectiveness of GaMuSa in improving background color consistency, text font quality, and glyph texture during the inference stage.  Two rows of examples are given, the top row showing background color regulation, and the bottom row showing glyph texture improvement. In each row, the leftmost image is the source image, the middle image shows the result without GaMuSa, and the rightmost image shows the result with GaMuSa.", "section": "Inference control with Glyph-adaptive Mutual Self-attention"}, {"figure_path": "SQVns9hWJT/figures/figures_15_1.jpg", "caption": "Figure 7: Data collection strategy of ScenePair. (a) Texts with the same style and background often occur in real-world scenery. (b) The pipeline for our data collection.", "description": "This figure illustrates the process of creating the ScenePair dataset.  Part (a) shows an example of real-world signage where multiple text elements share similar styles, demonstrating the dataset's basis. Part (b) is a flowchart detailing the dataset's construction: starting with several scene text datasets, text regions are automatically identified and paired based on style similarity using an OCR (Optical Character Recognition) and a similarity algorithm. Manual review and filtering are then performed to curate the final ScenePair dataset.", "section": "4 Experiments"}, {"figure_path": "SQVns9hWJT/figures/figures_15_2.jpg", "caption": "Figure 2: Decomposed framework of TextCtrl. (a) Text glyph structure encoder T with corresponding glyph structure representation pre-training. (b) Text style encoder S with corresponding style disentanglement pre-training. (c) Prior guided diffusion generator G. (d) The improved inference control with the Glyph-adaptive Mutual Self-attention mechanism.", "description": "This figure shows the architecture of the proposed TextCtrl model. It is decomposed into four parts: (a) a text glyph structure encoder that uses pre-trained glyph structure representation; (b) a text style encoder that uses pre-trained text style disentanglement; (c) a prior-guided diffusion generator that incorporates style and structure guidance; and (d) an improved inference control mechanism with glyph-adaptive mutual self-attention. Each component plays a crucial role in the scene text editing process.", "section": "3 Method"}, {"figure_path": "SQVns9hWJT/figures/figures_16_1.jpg", "caption": "Figure 9: Visualization of edited result on text image by TEXTCTRL on dataset TAMPERSCENE.", "description": "This figure displays pairs of source and edited text images from the TamperScene dataset.  The editing was performed using the TextCtrl model.  Each pair shows the original text and the modified text, illustrating the model's ability to accurately replace the text while maintaining the style and overall quality of the image.", "section": "4.2 Performance Comparison"}, {"figure_path": "SQVns9hWJT/figures/figures_17_1.jpg", "caption": "Figure 10: Visualization of edited result on scene image by TEXTCTRL on dataset ICDAR 2013.", "description": "This figure shows several examples of scene text editing results produced by the proposed TextCtrl model on images from the ICDAR 2013 dataset.  It visually demonstrates the model's ability to modify text within real-world scene images while preserving the overall visual style and context of the scene.", "section": "4 Experiments"}, {"figure_path": "SQVns9hWJT/figures/figures_17_2.jpg", "caption": "Figure 11: Visualization of failure cases on text image by TextCtrl. The problem mainly results from the insufficient geometric prior guidance control.", "description": "This figure shows examples where the TextCtrl model fails to accurately edit text, particularly text with complex shapes. The failures are attributed to insufficient geometric prior guidance during the text editing process.  The images illustrate that while the model often successfully changes the text content, it struggles to maintain the original shape and style of the text, especially when it is curved or irregular.", "section": "5 Limitations and Conclusion"}]