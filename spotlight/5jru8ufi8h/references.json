{"references": [{"fullname_first_author": "S. Lotfi", "paper_title": "Pac-bayes compression bounds so tight that they can explain generalization", "publication_date": "2022-12-01", "reason": "This paper establishes a baseline for the compression-based generalization bounds that are extended in the current paper to LLMs."}, {"fullname_first_author": "O. Catoni", "paper_title": "Pac-bayesian supervised classification: the thermodynamics of statistical learning", "publication_date": "2007-12-01", "reason": "This foundational paper provides a theoretical framework of PAC-Bayes bounds that underlies many of the more recent advances, including those in the current paper."}, {"fullname_first_author": "G. K. Dziugaite", "paper_title": "Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data", "publication_date": "2017-03-01", "reason": "This paper is among the first to provide non-vacuous generalization bounds for deep neural networks, which is a significant development that paved the way for applying similar techniques to LLMs."}, {"fullname_first_author": "E. J. Hu", "paper_title": "Lora: Low-rank adaptation of large language models", "publication_date": "2021-06-01", "reason": "This paper introduces LoRA, a highly influential parameter-efficient fine-tuning method for LLMs that is analyzed in the context of generalization bounds in the current work."}, {"fullname_first_author": "A. Radford", "paper_title": "Language models are unsupervised multitask learners", "publication_date": "2019-01-01", "reason": "This paper introduces GPT-2, a large language model that serves as a benchmark model in the current paper's experimental evaluation of generalization bounds."}]}