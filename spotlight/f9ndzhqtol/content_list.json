[{"type": "text", "text": "Accelerating Diffusion Models with Parallel Sampling: Inference at Sub-Linear Time Complexity ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Haoxuan Chen\\* ICME Stanford University haoxuanc@stanford.edu ", "page_idx": 0}, {"type": "text", "text": "Yinuo Ren\\* ICME Stanford University yinuoren@stanford.edu ", "page_idx": 0}, {"type": "text", "text": "Lexing Ying Department of Mathematics and ICME Stanford University lexing@stanford.edu ", "page_idx": 0}, {"type": "text", "text": "Grant M. Rotskoff Department of Chemistry and ICME Stanford University rotskoff@stanford.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Diffusion models have become a leading method for generative modeling of both image and scientific data. As these models are costly to train and evaluate, reducing the inference cost for diffusion models remains a major goal. Inspired by the recent empirical success in accelerating diffusion models via the parallel sampling technique [1], we propose to divide the sampling process into $\\mathcal{O}(1)$ blocks with parallelizable Picard iterations within each block. Rigorous theoretical analysis reveals that our algorithm achieves $\\widetilde{\\mathcal{O}}(\\mathrm{poly}\\log d)$ overall time complexity, marking the first implementation with provable sub-linear complexity w.r.t. the datadimension $d$ . Our analysis is based on a generalized version of Girsanov's theorem and is compatible with both the SDE and probability fow ODE implementations. Our results shed light on the potential of fast and efficient sampling of high-dimensional data on fast-evolving modern large-memory GPU clusters. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Diffusion and probability flow based models [2-11] are now state-of-the-art in many fields, such as computer vision and image generation [12-22], natural language processing [23, 24], audio and video generation [25-29], optimization [30, 31], sampling and learning of fixed classes of distributions [32-41], solving high-dimensional partial differential equations [42-46], and more recently several applications in physical, chemical and biological fields [47-63]. For a more comprehensive list of related work, one may refer to the following review papers [64-66]. While there are already many variants, such as denoising diffusion probabilistic models (DDPMs) [7], score-based generative models (SGMs) [9], diffusion schrodinger bridges [67], stochastic interpolants and flow matching [2-4], etc., the recurring idea is to design a stochastic process that interpolates between the data distribution and some simple distribution, along which score functions or alike are learned by neural network-based estimators, and then perform inference guided by the learned score functions. ", "page_idx": 0}, {"type": "text", "text": "Due to the sequential nature of the sampling process, the inference of high-quality samples from diffusion models often requires a large number of iterations and, thus, evaluations of the neural network-based score function, which can be computationally expensive [68]. Efforts have been made to accelerate this process by resorting to higher-order or randomized numerical schemes [69- 75], augmented dynamics [76], adaptive step sizes [77], operator learning [78], restart sampling [79], self-consistency [80, 81] and knowledge distillation [82-84]. Recently, several empirical works [1, 85] leverage the Picard iteration and triangular Anderson acceleration to parallelize the sampling procedure of diffusion models and achieve empirical success in large-scale image generation tasks. Some other recent work [86, 87] also combine the parallel sampling technique with the randomized midpoint method [88] to accelerate the inference of diffusion models. ", "page_idx": 0}, {"type": "table", "img_path": "F9NDzHQtOl/tmp/5bb48c6af2758d849a973afc152ceabdebd9a6820fd6a6e9a1155566fa6081cd.jpg", "table_caption": [], "table_footnote": ["Table 1: Comparison of the approximate time complexity (cf. Definition 2.1) of different implementations of diffusion models. $\\eta$ is a small parameter that controls the smooth approximation of the datadistribution( $c f.$ Section 3.1.1). "], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "This effciency issue is closely related to the problem of bounding the required number of steps and evaluations of score functions to approximate an arbitrary data distribution on $\\mathbb{R}^{d}$ to $\\delta$ -accuracy, which has been analyzed extensively in the literature [89-106]. In terms of the dependency on the dimension $d$ , the current state-of-the-art result for the SDE implementation of diffusion models is $\\widetilde O(d)$ [98], improved from the previous $\\widetilde{\\cal O}(d^{2})$ bound [95]. [102] gives a $\\widetilde{\\mathcal{O}}(\\sqrt{d})$ bound for the probability flow ODE implementation by considering a predictor-corrector scheme with the underdamped Langevin Monte Carlo (UMLC) algorithm. ", "page_idx": 1}, {"type": "text", "text": "In this work, we aim to provide parallelization strategies, rigorous analysis, and theoretical guarantees for accelerating the inference process of diffusion models. The time complexity of previous implementations of diffusion models has been largely hindered by the discretization error, which requires the step size to scale with $\\widetilde{\\mathcal{O}}(1/d)$ for the SDE implementation and $\\widetilde{\\mathcal{O}}(1/\\sqrt{d})$ for theprobability fow ODE implementation. We show that the inference process can be first divided into $O(1)$ blocks with parallelizable evaluations of the score function within each, and thus reduce the overall time complexity to $\\widetilde{\\mathcal{O}}(\\mathrm{poly}\\log d)$ . We provide the first implementation of diffusion models with poly-logarithmic complexity, a significant improvement over the current state-of-the-art polynomial results that sheds light on the potential fast and efficient sampling of high-dimensional distributions with diffusion models on fast-developing memory-efficient modern GPU clusters. ", "page_idx": 1}, {"type": "text", "text": "1.1 Contributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "\u00b7 We propose parallelized inference algorithms for diffusion models in both the SDE and probability fow ODE implementations (PIADM-SDE/ODE) with exponential integrators, a shrinking step size scheme towards the data end, and the early stopping technique; \u00b7 We provide a rigorous convergence analysis of PIADM-SDE, showing that our parallelization strategy yields a diffusion model with $\\widetilde{\\mathcal{O}}(\\mathrm{poly}\\log d)$ approximate time complexity; \u00b7 We show that our strategy is also compatible with the probability flow ODE implementation, and PIADM-ODE could improve the space complexity from $\\widetilde{\\cal O}(d^{2})$ to $\\widetilde{\\mathcal{O}}(d^{3/2})$ while maintaining the poly-logarithmic time complexity. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this section, we briefly recapitulate the framework of score-based diffusion models, define notations, and discuss related work. ", "page_idx": 1}, {"type": "text", "text": "2.1  Diffusion Models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In score-based diffusion models, one considers a diffusion process $(\\pmb{x}_{s})_{s\\geq0}$ in $\\mathbb{R}^{d}$ governed by the following stochastic differential equation (SDE): ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathrm{d}\\pmb{x}_{s}=\\beta_{s}(\\pmb{x}_{s})\\mathrm{d}s+\\pmb{\\sigma}_{s}\\mathrm{d}\\pmb{w}_{s},\\quad\\mathrm{with}\\quad\\pmb{x}_{0}\\sim p_{0},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $(\\pmb{w}_{s})_{s\\geq0}$ is a standard Brownian motion, and $p_{0}$ is the target distribution that we would like to sample from. The distribution of $\\pmb{x}_{s}$ is denoted by $p_{s}$ . Once the drift $\\beta_{s}(\\cdot)$ , the diffusion coefficient $\\pmb{\\sigma}_{s}$ , and a sufficiently large time horizon $T$ are specified, (2.1) also corresponds to a backward process $(\\overleftarrow{\\mathbfit{x}}_{t})_{0\\leq t\\leq T}$ for another arbitrary diffusion coefficient $(\\pmb{v}_{s})_{s\\geq0}$ [107]: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathrm{d}\\bar{x}_{t}=\\left[-\\overleftarrow{\\beta}_{t}(\\bar{x}_{t})+\\frac{\\overleftarrow{\\sigma}_{t}\\overleftarrow{\\pmb{\\sigma}}_{t}^{\\top}+\\overleftarrow{\\boldsymbol{v}}_{t}\\overleftarrow{\\boldsymbol{v}}_{t}^{\\top}}{2}\\nabla\\log\\overleftarrow{p}_{t}(\\overleftarrow{\\boldsymbol{x}}_{t})\\right]\\mathrm{d}t+\\overleftarrow{\\boldsymbol{v}}_{t}\\mathrm{d}\\boldsymbol{w}_{t},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\textstyle{\\overleftarrow{*}}_{t}$ denotes $\\ast_{T-t}$ , with $\\overleftarrow{p}\\phantom{\\rule{0ex}{0ex}}=p_{T}$ and $\\overleftarrow{p}_{T}=p_{0}$ ", "page_idx": 2}, {"type": "text", "text": "For notational simplicity, we adopt a simple choice of the drift and the diffusion coefficients in what follows: $\\beta_{t}({\\pmb x})=-\\frac{1}{2}{\\pmb x}$ $\\sigma_{t}=I_{d}$ and $\\pmb{v}=v\\pmb{I}_{d}$ , under which (2.1) is an Ornstein-Uhlenbeck (OU) process converging exponentially to its stationary distribution, i.e. $p_{T}\\approx\\widehat{p}_{T}:=\\mathcal{N}(0,I_{d})$ , and (2.1) and (2.2) reduce to the following form: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathrm{d}\\pmb{x}_{s}=-\\frac{1}{2}\\pmb{x}_{s}\\mathrm{d}s+\\mathrm{d}\\pmb{w}_{s},\\quad\\mathrm{and}\\quad\\mathrm{d}\\ddot{\\pmb{x}}_{t}=\\left[\\frac{1}{2}\\ddot{\\pmb{x}}_{t}+\\frac{1+\\pmb{v}^{2}}{2}\\nabla\\log\\breve{p}_{t}(\\ddot{\\pmb{x}}_{t})\\right]\\mathrm{d}t+\\boldsymbol{v}\\mathrm{d}\\pmb{w}_{t}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "In practice, the score function $\\nabla\\overleftarrow{p}_{t}(\\pmb{\\dot{x}}_{t})$ is often estimated by a neural network (NN) $\\pmb{s}_{t}^{\\theta}(\\pmb{x}_{t})$ ,where $\\theta$ represents its parameters, by minimizing the denoising score-matching loss [108, 109]: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}(\\boldsymbol{\\theta}):=\\mathbb{E}_{\\mathbf{x}_{t}\\sim p_{t}}\\left[\\left\\Vert\\nabla\\log p_{t}(\\mathbf{x}_{t})-s_{t}^{\\theta}(\\mathbf{x}_{t})\\right\\Vert^{2}\\right]}\\\\ &{\\qquad=\\mathbb{E}_{\\mathbf{x}_{0}\\sim p_{0}}\\left[\\mathbb{E}_{\\mathbf{x}_{t}\\sim p_{t}\\mid0}(\\mathbf{x}_{t}|\\mathbf{x}_{0})\\left[\\left\\Vert\\frac{\\mathbf{x}_{t}-\\mathbf{x}_{0}e^{-t/2}}{1-e^{-t}}-s_{t}^{\\theta}(\\mathbf{x}_{t})\\right\\Vert^{2}\\right]\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "and the backward process in (2.3) is approximated by the following SDE thereafter: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathrm{d}y_{t}=\\left[\\frac{1}{2}y_{t}+\\frac{1+v^{2}}{2}s_{t}^{\\theta}(y_{t})\\right]\\mathrm{d}t+v\\mathrm{d}w_{t},\\quad\\mathrm{with}\\quad y_{0}\\sim\\mathcal{N}(0,I_{d}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Implementations. Diffusion models admit multiple implementations depending on the choice of the parameter $\\upsilon$ in the backward process (2.2). The SDE implementation with $\\upsilon\\,=\\,1$ iswidely used in the literature for its simplicity and effciency [10], while recent studies [102] claim that the probability flow ODE implementation with $\\upsilon\\,=\\,0$ may exhibit better time complexity. We refer to [102, 110] for theoretical and [111, 112] for empirical comparisons of different implementations. ", "page_idx": 2}, {"type": "text", "text": "2.2  Parallel Sampling ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Parallel sampling algorithms have been actively explored in the literature, including the parallel tempering method [113-115] and several recent studies [116-118]. For diffusion models, the idea of parallel sampling is based on the Picard iteration [119, 120] for solving nonlinear ODEs. Suppose wehave anODE $\\mathrm{d}\\mathbf{\\boldsymbol{x}}_{t}\\,=\\,f_{t}(\\mathbf{\\boldsymbol{x}}_{t})\\mathrm{d}t$ and we would like to solve it for $t\\,\\in\\,[0,T]$ , then the Picard iteration is defined as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\nx_{t}^{(0)}\\equiv x_{0},\\quad\\mathrm{and}\\quad x_{t}^{(k+1)}:=x_{0}+\\int_{0}^{t}f_{s}(x_{s}^{(k)})\\mathrm{d}s,\\quad\\mathrm{for~}k\\in[0:K-1].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Under assumptions on the Lipschitz continuity of $\\pmb{f}_{t}$ , the Picard iteration converges to the true solutinxetalltin $\\|\\|\\mathbf{x}_{t}^{(k)}\\,-\\,\\mathbf{x}_{t}\\|\\|_{L^{\\infty}([0,T])}\\,\\le\\,\\delta$ with $K\\,=\\,\\mathcal{O}(\\log\\delta^{-1})$ iterations. Unlike high-order ODE solvers, the Picard iteration is intrinsically parallelizable: for any $t\\in[0,T]$ , the computation of (k+1) relies merely on the value of the most recent iteration ) With sufficient computational sources parallelizing the evaluations of $\\boldsymbol{\\textbf{\\textit{f}}}$ , the computational cost of solving the ODE no longer scales with $T$ but with the number of iterations $K$ ", "page_idx": 2}, {"type": "text", "text": "Recently, this idea has been applied to both the Langevin Monte Carlo (LMC) and the underdamped Langevin Monte Carlo (UMLC) contexts [121]. Roughly speaking, it is proposed to simulate the Langevin diffusion process $\\mathrm{d}\\pmb{x}_{t}=-\\nabla V(\\pmb{x}_{t})\\mathrm{d}t+\\mathrm{d}\\pmb{w}_{t}$ with the following iteration resembling (2.6): ", "page_idx": 3}, {"type": "equation", "text": "$$\nx_{t}^{(0)}\\equiv x_{0},\\quad\\mathrm{and}\\quad x_{t}^{(k+1)}:=x_{0}-\\int_{0}^{t}\\nabla V(x_{t}^{(k)})\\mathrm{d}s+w_{t},\\quad\\mathrm{for}\\;k\\in[0:K-1],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where alliterations share a common Wiener process $({\\pmb w}_{t})_{t\\geq0}$ ", "page_idx": 3}, {"type": "text", "text": "It is shown that for well-conditioned log-concave distributions, parallelized LMC would achieve an iteration depth of $K\\ =\\ \\tilde{\\mathcal{O}}(\\mathrm{poly}\\log d)$ that matches the indispensable time horizon $T\\;=$ $\\widetilde{\\mathcal{O}}(\\mathrm{poly}\\log d)$ to achieve exponential ergodicity (cf. [121, Theorem 13]). This promises a significant speedup in sampling high-dimensional distributions from the standard LMC of $T\\,=\\,{\\widetilde{\\mathcal{O}}}(d)$ iterations, hindered by the $o(1/d)$ step size as imposed by the discretization error and now evaded by the parallelization. ", "page_idx": 3}, {"type": "text", "text": "2.3  Approximate Time Complexity ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "A similar situation is expected in diffusion models, where the application bottleneck is largely the inference process with sequential iterations and expensive evaluations of the learned score function $s_{t}^{\\theta}(\\cdot)$ , which is often parametrized by large-scale NNs. Despite several unavoidable costs involving pre- and post-processing, data storage and retrieval, and arithmetic operations, we define the following notion of the approximate time complexity of the inference process of diffusion models: ", "page_idx": 3}, {"type": "text", "text": "Definition 2.1 (Approximate time complexity). For a specific implementation of diffusion models (2.5), we define the approximate time complexity of the sampling process as the number of unparallelizable evaluations of the learned NN-based score function $s_{t}^{\\vec{\\theta(\\cdot)}}$ ", "page_idx": 3}, {"type": "text", "text": "This definition coincides with the notion of the number of steps required to reach a certain accuracy in [95, 91], iteration complexity in [98, 102], etc. in the previous theoretical studies of diffusion models. We have adopted this notion in Table 1 for a comparison of the current state-of-the-art results and our bounds in this work. We will use the notion of space complexity likewise to denote the approximate required storage during the inference. Trivially, the space complexity of the sequential implementation is $O(d)$ . Should no confusion occur, we omit the dependency of the complexities above on the accuracy threshold $\\delta$ , etc., during our discussion, as we focus on applications of diffusion models to high-dimensional data distributions, following the standard practice in the literature. ", "page_idx": 3}, {"type": "text", "text": "3 Main Results ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Inspired by the acceleration achieved by the parallel sampling technique in LMC and ULMC, we aim to accommodate parallel sampling into the theoretical analysis framework of diffusion models. The benefit of the parallel sampling technique in this scenario has been recently confirmed by up to $14\\times$ acceleration achieved by the ParaDiGMS algorithm [1] and ParaTAA [85], where several practical compromises are made to mitigate GPU memory constraints and theoretical guarantees are still lacking. ", "page_idx": 3}, {"type": "text", "text": "In this section, we will propose Parallelized Inference Algorithms for Diffusion Models with both the SDE and probability fow ODE implementations, namely the PIADM-SDE (Algorithm 1) and PIADM-ODE (Algorithm 2), and present theoretical guarantees of our algorithms, including the approximate time complexity and space complexity, for both implementations in Section 3.1 and Section 3.2, respectively. Due to the large number of notations used in the presentation, we give an overview of notations in Appendix A.1 for readers\u2019 convenience. ", "page_idx": 3}, {"type": "text", "text": "3.1 SDE Implementation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We first focus on the approximation, parallelization strategies, and error analysis of diffusion models with the SDE implementation, i.e. the forward and backward process (2.3) and its approximatation (2.5) with $\\upsilon=1$ .We will show that PIADM-SDE achieves an $\\widetilde{\\mathcal{O}}(\\mathrm{poly}\\log d)$ approximatetime complexitywith $\\widetilde{\\cal O}(d^{2})$ spacecomplexity. ", "page_idx": 3}, {"type": "image", "img_path": "F9NDzHQtOl/tmp/3c6181b48bc4dce846c57e0e1f508ee152280909779e92db9754dcb1e3530a80.jpg", "img_caption": ["Figure 1: Ilustration of PIADM-SDE/ODE. The outer iterations are divided into ${\\mathcal{O}}(\\log d)$ blocks of $\\mathcal{O}(1)$ length. Within each block, the inner iterations are parallelized with $\\widetilde O(d)$ steps for SDE $(c f.$ Theorem 3.3), or $\\widetilde{\\mathcal{O}}(\\sqrt{d})$ for probability flow ODE implementation (cf. Theorem 3.5). The overall approximate time complexity is $K N=\\tilde{\\mathcal{O}}(\\mathrm{poly}\\log d)$ . brown, green, blue, and red curves represent the computation graph at $t=t_{n}+\\tau_{n,m}$ for $m=1,2,M_{n}-1,M_{n}$ "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "3.1.1 Algorithm ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "PIADM-SDE is summarized in Algorithm 1 and illustrated in Figure 1. The main idea behind our algorithm is the fact that (2.5) can be efficiently solved by the Picard iteration within a period Oof $\\mathcal{O}(1)$ length, transferring $\\widetilde O(d)$ sequential computations to a parallelizable iteration of depth $\\widetilde{\\mathcal{O}}(\\log d)$ . In the following, we introduce the numerical discretization scheme of our algorithm and the implementation of the Picard iteration in detail. ", "page_idx": 4}, {"type": "text", "text": "Step Size Scheme. In our algorithm, the time horizon $T$ is first segmented into $N$ blocks of length $(h_{n})_{n=0}^{N-1}$ , with each $h_{n}\\leq h:=T/N=\\Omega(1)$ , forming a grid $(t_{n})_{n=0}^{N}$ with $\\textstyle t_{n}=\\sum_{j=1}^{n}h_{j}$ . For any $n\\in[0:N-1]$ , the $n$ th block is further discretized into a grid $(\\tau_{n,m})_{m=0}^{M_{n}}$ with $\\tau_{n,0}=0$ and $\\tau_{n,M_{n}}=h_{n}$ . We denote the step size of the $m$ -th step in the $n$ -th block as $\\epsilon_{n,m}=\\tau_{n,m+1}-\\tau_{n,m}$ and the total number of steps in the $n$ -th block as $M_{n}$ ", "page_idx": 4}, {"type": "text", "text": "For the first $N-1$ blocks, we simply use the unique discretization, i.e. $h_{n}\\,=\\,h$ $\\epsilon_{n,m}\\,=\\,\\epsilon$ , and $M_{n}\\,=\\,M\\,:=\\,h/\\epsilon$ , for $n\\,\\in\\,[0\\,:\\,N\\,-\\,2]$ and $m\\in[0:M-1]$ . Following [95, 98], to curb the potential blow-up of the score function as $t\\rightarrow T$ , which is shown by [98] for $0\\leq s<t<T$ to be of the order ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\int_{s}^{t}\\|\\nabla\\log\\tilde{p}_{\\tau}(\\bar{\\mathbf{x}}_{\\tau})-\\nabla\\log\\tilde{p}_{s}(\\bar{\\mathbf{x}}_{s})\\|^{2}\\mathrm{d}\\tau\\right]\\lesssim d\\left(\\frac{t-s}{T-t}\\right)^{2},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "we apply early stopping at time $t_{N}\\,=\\,T\\,-\\,\\eta$ ,where $\\eta$ is chosen in a way such that the $\\mathcal{O}(\\sqrt{\\eta})$ 2-Wasserstein distance between $\\overleftarrow{p}\\!_{T}$ and its smoothed version $\\overleftarrow{p}_{T-\\eta}$ that we aim to sample from alternatively, is tolerable for the downstream tasks. We also impose the exponential decay of the step size towards the data end in the last block. To be specific, we let $h_{N-1}=h-\\delta$ , and discretize the interval $[t_{N-1},t_{N}]=[(N-1)h,T-\\eta]$ into a grid $\\left(\\tau_{N-1,m}\\right)_{m=0}^{M_{N-1}}$ )m-1 with step sizes (EN-1,m)m=0 satisfying ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\epsilon_{N-1,m}\\leq\\epsilon\\wedge\\epsilon\\left(h-\\tau_{N-1,m+1}\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "As shown in Lemma B.7, this exponential decaying step size scheme towards the data end is crucial to bound the discretization error in the last block. ", "page_idx": 4}, {"type": "text", "text": "For the simplicity of notations, we introduce the following indexing function: for $\\tau\\in[t_{n},t_{n+1}]$ we defne In(T) to be the unique iteger such that =(T) $\\begin{array}{r}{\\sum_{j=1}^{I_{n}(\\tau)}\\epsilon_{n,j}\\:\\le\\:\\tau<\\sum_{j=1}^{I_{n}(\\tau)+1}\\epsilon_{n,j}}\\end{array}$ Wealso definer ", "page_idx": 4}, {"type": "text", "text": "Input: $\\widehat{\\pmb y}_{0}\\sim\\widehat{q}_{0}=\\mathcal{N}(0,\\pmb I_{d})$ a dicretization scheme ( $T,(h_{n})_{n=1}^{N}$ and $\\bigl(\\tau_{n,m}\\bigr)_{n\\in[1:N],m\\in[0:M]})$ satisfying (3.1), the depth of iteration $K$ , the learned NN-based score function $s_{t}^{\\theta}(\\cdot)$ ", "page_idx": 5}, {"type": "text", "text": "Output:A sample $\\widehat{\\pmb{y}}_{t_{N}}\\sim\\widehat{q}_{t_{N}}\\approx\\overleftarrow{p}_{T}$   \n1 for $n=0$ to $N-1$ do 2 $\\widehat{\\pmb{y}}_{t_{n},\\tau_{n},m}^{(0)}\\gets\\widehat{\\pmb{y}}_{t_{n}}$ \uff0c $\\xi_{m}\\sim\\mathcal{N}(0,I_{d})$ for $m\\in[0:M_{n}]$ in parallel; 3 for $k=0$ to $K-1$ do 4 $\\widehat{\\pmb{y}}_{t_{n},0}^{(k)}\\gets\\widehat{\\pmb{y}}_{t_{n}}$ 5 for $m=0$ to $M_{n}$ in parallel do 6 $\\begin{array}{r l}&{\\left|\\begin{array}{l}{{\\widehat{y}}_{t_{n},\\tau_{n,m}}^{(k+1)}\\leftarrow e^{\\frac{\\tau_{n,m}}{2}}{\\widehat{y}}_{t_{n},0}^{(k)}}\\\\ {\\ +\\sum_{j=0}^{m-1}e^{\\frac{\\tau_{n,m}-\\tau_{n,j+1}}{2}}\\left[2\\left(e^{\\epsilon_{n,j}}-1\\right)s_{t_{n}+\\tau_{n,j}}^{\\theta}({\\widehat{y}}_{t_{n},\\tau_{n,j}}^{(k)})+\\sqrt{e^{\\epsilon_{n,j}}-1}\\xi_{j}\\right];}\\end{array}\\right.}\\end{array}$ (3.4) 7 end   \n8 end 9 $\\widehat{\\pmb{y}}_{t_{n+1}}\\gets\\widehat{\\pmb{y}}_{t_{n},\\tau_{n,M_{n}}}^{(K)};$   \n10 end ", "page_idx": 5}, {"type": "text", "text": "a piecewisefunction g such that gn(T) =/(r) . It is easy to check that under the uniform discretization for $n\\in[1:N-1]$ ,wehave $I_{n}\\dot{(}\\tau)=\\lfloor\\tau/\\epsilon\\rfloor$ and $g_{n}(\\tau)=\\lfloor\\tau/\\epsilon\\rfloor\\epsilon$ ", "page_idx": 5}, {"type": "text", "text": "Exponential Integrator.  For each step $\\tau\\in[t_{n}+\\tau_{n,m},t_{n}+\\tau_{n,m+1}]$ we use the following exponential integrator scheme [73], as the numerical discretization of the SDE (2.5): ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\widehat{y}_{t_{n},\\tau_{n,m+1}}=e^{\\epsilon_{n,m}/2}\\widehat{y}_{t_{n},\\tau_{n,m}}+2\\left(e^{\\epsilon_{n,m}/2}-1\\right)s_{t_{n}+\\tau_{n,m}}^{\\theta}(\\widehat{y}_{t_{n}+\\tau_{n,m}})+\\sqrt{e^{\\epsilon_{n,m}}-1}\\xi,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\pmb{\\xi}\\sim\\mathcal{N}(\\mathbf{0},\\pmb{I}_{d})$ . Lemma B.3 shows its equivalence to approximating (2.5) as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathrm{d}\\widehat{y}_{t_{n},\\tau}=\\left[\\frac{1}{2}\\widehat{y}_{t_{n},\\tau}+s_{t_{n}+\\tau_{n,m}}^{\\theta}(\\widehat{y}_{t_{n},\\tau_{n,m}})\\right]\\mathrm{d}\\tau+\\mathrm{d}w_{t_{n}+\\tau},\\quad\\mathrm{for}\\,\\tau\\in[\\tau_{n,m},\\tau_{n,m+1}].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Remark 3.1. One could also implement a straightforward Euler-Maruyama scheme instead of the exponential integrator (3.4), where an additional high-order discretization error term would emerge[95,Theorem $I J,$ which we believe would not affect the overall $\\widetilde{\\mathcal{O}}(\\mathrm{poly}\\log d)$ timecomplexitywithparallel sampling. ", "page_idx": 5}, {"type": "text", "text": "Picard Iteration. Within each block, we apply Picard iteration of depth $K$ :Asshownby Lemma B.3, the discretized scheme (3.4) implements the following iteration for $k\\in[0:K-1]$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathrm{d}\\widehat{y}_{t_{n},\\tau}^{(k+1)}=\\left[\\frac{1}{2}\\widehat{y}_{t_{n},\\tau}^{(k+1)}+s_{t_{n}+g_{n}(\\tau)}^{\\theta}\\Big(\\widehat{y}_{t_{n},g_{n}(\\tau)}^{(k)}\\Big)\\right]\\mathrm{d}\\tau+\\mathrm{d}w_{t_{n}+\\tau},\\quad\\mathrm{for}\\,\\tau\\in[0,h_{n}].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "$\\widehat{\\pmb{y}}_{t_{n},\\tau}^{(K)}$ $\\widehat{q}_{t_{n}+\\tau}$ estimation $s_{t}^{\\theta}$ should be close to the true backward SDE (2.3). One should also notice that the Gaussians $\\xi_{m}$ are only sampled once and used for all iterations. ", "page_idx": 5}, {"type": "text", "text": "The parallelization for (3.4) in Algorithm 1 should be understood as that for any $k\\in[0:K-1]$ each stn+Tru.s(tnTni) for $j\\,\\in\\,[0\\,:\\,M_{n}]$ is evaluated in parallel, with subsequent floating-point operations comparably negligible, resulting in the overall $\\mathcal{O}(N K)$ approximate time complexity. ", "page_idx": 5}, {"type": "text", "text": "3.1.2  Assumptions ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Our theoretical analysis will be built on the following mild assumptions on the regularity of the data distribution and the numerical properties of the neural networks: ", "page_idx": 5}, {"type": "text", "text": "Assumption 3.1 $(L^{2}([0,t_{N}])$ $\\delta$ -accurate learned score). The learned NN-based score $s_{t}^{\\theta}$ is $\\delta_{2}$ accuratein thesense of ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\bar{p}}\\left[\\sum_{n=0}^{N-1}\\sum_{m=0}^{M_{n}-1}\\epsilon_{n,m}\\left\\Vert s_{t_{n}+\\tau_{n,m}}^{\\theta}\\middle(\\tilde{x}_{t_{n}+\\tau_{n,m}}\\right)-\\nabla\\log\\overleftarrow{p}_{t_{n}+\\tau_{n,m}}\\middle(\\tilde{x}_{t_{n}+\\tau_{n,m}}\\right)\\right\\Vert^{2}\\right]\\leq\\delta_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Assumption 3.2 (Regular and normalized data distribution). The data distribution $p_{0}$ hasfinite secondmoments and is normalized such that $\\mathrm{cov}_{p_{0}}(\\pmb{x}_{0})=\\pmb{I}_{d}$ ", "page_idx": 6}, {"type": "text", "text": "Assumption 3.3 (Bounded and Lipschitz learned NN-based score). The learned NN-based score function $s_{t}^{\\theta}$ hasbounded $C^{1}$ norm,i.e. $\\|\\|s_{t}^{\\theta}(\\cdot)\\|\\|_{L^{\\infty}([0,T])}\\leq M_{s}$ withLipschitz.constant $L_{s}$ ", "page_idx": 6}, {"type": "text", "text": "Remark 3.2. Assumption 3.1 and the finite moment assumption in Assumption 3.2 are standard assumptions across previous theoretical works on diffusion models [91, 95, 102], while we adopt the normalization Assumption 3.2 from [98] to simplify true score function-related computations (cf. $e^{\\hat{-}t/2}s_{t}^{\\theta}$ of Assumption 3.1 might be possible, which is left for future work. ", "page_idx": 6}, {"type": "text", "text": "3.1.3  Theoretical Guarantees ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The following theorem summarizes our theoretical analysis for PIADM-SDE (Algorithm 1): ", "page_idx": 6}, {"type": "text", "text": "Theorem 3.3 (Theoretical Guarantees for PIADM-SDE). Under Assumptions 3.1, 3.2, and 3.3, given the following choices of the order of the parameters ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{c}{T=\\mathcal{O}(\\log(d\\delta^{-2})),\\quad h=\\Theta(1),\\quad N=\\mathcal{O}\\left(\\log(d\\delta^{-2})\\right),}\\\\ {\\epsilon=\\Theta\\left(d^{-1}\\delta^{2}\\log^{-1}(d\\delta^{-2})\\right),\\quad M=\\mathcal{O}\\left(d\\delta^{-2}\\log(d\\delta^{-2})\\right),\\quad K=\\widetilde{\\mathcal{O}}(\\log(d\\delta^{-2})),}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "and let $L_{s}^{2}h_{n}e^{\\frac{7}{2}h_{n}}\\ll1,\\,\\delta_{2}\\stackrel{<}{_\\sim}\\delta,\\,T\\stackrel{<}{_\\sim}\\log\\eta^{-1}$ thedistribution $\\widehat{q}_{t_{N}}$ that PIADM-SDE (Algorithm 1) generates samples from satisfies the following error bound: ", "page_idx": 6}, {"type": "equation", "text": "$$\nD_{\\mathrm{KL}}(p_{\\eta}\\|\\widehat{q}_{t_{N}})\\lesssim d e^{-T}+d\\epsilon T+\\delta_{2}^{2}+d T e^{-K}\\lesssim\\delta^{2},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "with a total of $K N=\\tilde{\\mathcal{O}}\\left(\\log^{2}(d\\delta^{-2})\\right)$ approximate time complexity and $d M=\\widetilde{\\cal O}\\left(d^{2}\\delta^{-2}\\right)$ space complexity for parallalizable $\\delta$ -accurate score function computations. ", "page_idx": 6}, {"type": "text", "text": "Remark 3.4. We would like to make the following remarks on the result above: ", "page_idx": 6}, {"type": "text", "text": "\u00b7 The acceleration from $\\widetilde{\\mathcal{O}}(d)\\,t o\\,\\widetilde{\\mathcal{O}}(\\mathrm{poly}\\log d)$ is at the cost of a trade-off with extra memory cost of M = O(d) for computing and updating stn,+Tn. ( $\\{s_{t_{n}+\\tau_{n,j}}^{\\theta}(\\widehat{\\pmb{y}}_{t_{n},\\tau_{n,m}}^{(k)})\\}_{m\\in[0:M_{n}]}$ in,Tn,m)me[0:M] imultaneously during each Picard iteration;   \n\u00b7 Compared with log-concave sampling [121], M being of order $\\widetilde O(d)$ instead of $\\widetilde{\\mathcal{O}}(\\sqrt{d})$ therein is partly due to the time independence of the score function $\\nabla\\log p(\\cdot)$ in general sampling tasks. Besides, the scaling $M=\\widetilde{\\mathcal{O}}(d)$ agreeswith the current state-of-the-art dependency[98]for the SDE implementation of diffusion models;   \n\u00b7As mentioned above, the scale of the step size e within one block is still confined to $\\Theta(1/M)=$ $\\widetilde{\\Theta}\\left(1/d\\right)$ . The block length $h$ despite being required to be small compared to $1/L_{s}$ , is of order $\\Theta(1)$ ,resulting in only $\\Theta(\\log d)$ blocks and thus $\\widetilde{\\mathcal{O}}(\\mathrm{poly}\\log d)$ total iterations. ", "page_idx": 6}, {"type": "text", "text": "3.1.4 Proof Sketch ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The detailed proof of Theorem 3.3 is deferred to Section B. The pipeline of the proof is to (a) first decompose theerror $D_{\\mathrm{KL}}(\\overleftarrow{p}_{t_{N}}\\Vert\\widehat{q}_{t_{N}})$ into blockwise errors using the chain rule of KL divergence; (b) bound the error in each block by invoking Girsanov's theorem; (c) sum up the errors in all blocks. ", "page_idx": 6}, {"type": "text", "text": "The key technical challenge lies in Step (b). Different from all previous theoretical works [91, 95, 102], the Picard iteration in our algorithm generates. $K$ paths recursively in each block using the learned score $\\ensuremath{\\boldsymbol{s}}_{t}^{\\theta}$ . And therefore the final path $(\\widehat{\\pmb{y}}_{t_{n},\\tau}^{(K)})_{\\tau\\in[0,h_{n}]}$ depends on al previous paths $(\\widehat{\\pmb{y}}_{t_{n},\\tau}^{(k)})_{\\tau\\in[0,h_{n}]}$ for $k\\ \\in\\ [0\\ :\\ K\\mathrm{~-~}1]$ , ruling out a direct change of measure argument from the naive application of Girsanov's theorem. To this end, we need a more sophisticated mathematical framework of stochastic processes, as given in Appendix A.2. We define the measurable space $(\\Omega,{\\mathcal{F}})$ with filtrations $(\\mathcal{F}_{t})_{t\\geq0}$ to specify the probability measures on $(\\Omega,{\\mathcal{F}})$ of eachWiener process, and resort to one of the most general forms of Girsanov's theorem ( [122, Theorem 8.6.6]). For example, in the $n$ -th block, we apply the following change of measure procedure: ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "1. Let $q\\vert_{\\mathcal{F}_{t_{n}}}$ be the measure where $\\pmb{w}_{t}(\\omega)$ is the shared Wiener process in the Picard iteration (3.3) for any $\\overset{\\cdot}{k}\\in[0:K-1]$ ", "page_idx": 7}, {"type": "text", "text": "2. Define another process $\\mathrm{d}\\widetilde{\\pmb{w}}_{t_{n}+\\tau}(\\omega)=\\mathrm{d}\\pmb{w}_{t_{n}+\\tau}(\\omega)+\\delta_{t_{n}}(\\tau,\\omega)d\\tau$ where ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\delta_{t_{n}}(\\tau,\\omega):=s_{t_{n}+g_{n}(\\tau)}^{\\theta}(\\widehat{y}_{t_{n},g_{n}(\\tau)}^{(K-1)}(\\omega))-\\nabla\\log\\overleftarrow{p}_{t_{n}+\\tau}(\\widehat{y}_{t_{n}+\\tau}^{(K)}(\\omega));\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "3. Invoke Girsanov's theorem, which yields that the Radon-Nikodym derivative of the measure $\\left\\langle\\bar{p}\\right|_{\\mathcal{F}_{t_{n}}}$ withrespecto $q\\vert_{\\mathcal{F}_{t_{n}}}$ satisfies ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\log\\frac{\\mathrm{d}\\bar{p}|_{\\mathcal{F}_{t_{n}}}}{\\mathrm{d}q|_{\\mathcal{F}_{t_{n}}}}(\\omega)=-\\int_{0}^{h_{n}}\\delta_{t_{n}}(\\tau,\\omega)^{\\top}\\mathrm{d}w_{t_{n}+\\tau}(\\omega)-\\frac{1}{2}\\int_{0}^{h_{n}}\\|\\delta_{t_{n}}(\\tau,\\omega)\\|^{2}\\mathrm{d}\\tau;\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "4. Conclude that $(\\widetilde{\\pmb{w}}_{t_{n}+\\tau})_{\\tau\\geq0}$ is a Wiener process under the measure $\\left\\langle\\bar{p}\\right|_{\\mathcal{F}_{t_{n}}}$ and thus (3.3) at iteration $K$ satisfies the following SDE: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathrm{d}\\widehat{y}_{t_{n},\\tau}^{(K)}(\\omega)=\\left[\\frac{1}{2}\\widehat{y}_{t_{n},\\tau}^{(K)}(\\omega)+\\nabla\\log\\breve{p}_{t_{n}+\\tau}\\big(\\widehat{y}_{t_{n},\\tau}^{(K)}(\\omega)\\big)\\right]\\mathrm{d}\\tau+\\mathrm{d}\\widetilde{w}_{t_{n}+\\tau}(\\omega),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "i.e. the true backward SDE (2.3) with the true score function for $\\tau\\in[t_{n},t_{n+1}]$ ", "page_idx": 7}, {"type": "text", "text": "One should notice that this change of measure argument will cause an additional term in the bound of the discrepancy between the first iteration yi'ih and the inial condition gto)# in LemmaB.5. However, due to the exponential convergence of the Picard iteration, this term does not affect the overall errorbound. ", "page_idx": 7}, {"type": "text", "text": "3.2 Probability Flow ODE Implementation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we will show that our parallelization strategy is also compatible with the probability ODE implementation of diffusion models, i.e. the forward and backward process (2.3) and its approximatation (2.5) with $\\upsilon\\,=\\,0$ . We will demonstrate that PIADM-ODE (Algorithm 2) further improves the space complexity from $\\widetilde{\\cal O}(d^{2})$ to $\\widetilde{\\mathcal{O}}(d^{3/2})$ whilemaintaining the same $\\widetilde{\\mathcal{O}}(\\mathrm{poly}\\log d)$ approximatetimecomplexity. ", "page_idx": 7}, {"type": "text", "text": "3.2.1 Algorithm ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Due to the space limit, we refer the readers to Section C.1 and Algorithm 2 for the details of our parallelization of the probability fow ODE formulation of diffusion models. PIADM-ODE keeps the discretization scheme detailed in Section 3.1.1 that divides the time horizon $T$ into $N$ blocks and uses exponential integrators for all updating rules. Notably, PIADM-ODE has the following distinctions compared with PIADM-SDE (Algorithm 1): ", "page_idx": 7}, {"type": "text", "text": "\u00b7 Instead of applying Picard iteration to the backward SDE as in (3.2), we apply Picard iteration to the probability fow ODE as in (C.3) within each block, which does not require sampling i.i.d. Gaussians to simulate a Wiener process;   \n\u00b7 The most significant difference is the adoption of an additional corrector step [102] after running the probability fow ODE with Picard iteration within one block. During the corrector step, one augments the state space with a Gaussian that represents the initial momentum and then simulates an underdamped Langevin dynamics for $\\mathcal{O}(\\bar{1})$ time with the learned NN-based score function at the time of the block end;   \n\u00b7 We then further parallelize the underdamped Langevin dynamics in the corrector step so that it can also be accomplished with ${\\mathcal{O}}(\\log d)$ approximate time complexity, as a naive implementation would result in $\\widetilde{\\mathcal{O}}(\\sqrt{d})$ [121], which is incompatible with our desired poly-logarithmic guarantee. ", "page_idx": 7}, {"type": "text", "text": "3.2.2  Assumptions ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Due to technicalities specific to this implementation, we need first to modify Assumption 3.1 and add assumption on the Lipschitzness of the true score functions $\\nabla\\log{p_{t}}$ , which is a common practice in related literature [95, 102]. Recent work on the probability fow ODE implementation [103, 105] also adopts stronger assumptions compared to the SDE implementation. ", "page_idx": 8}, {"type": "text", "text": "Assumption 3.1\u2032 $(L^{\\infty}([0,t_{N}])$ $\\delta$ -accurate learned score). For any $n\\in[0:N-1]$ and $m\\in[0:$ Mn-1], the learned NNbased score stn,Tnm is $\\delta_{\\infty}$ -accurate in the sense of ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\tilde{p}_{t_{n}+\\tau_{n,m}}}\\left[\\left\\|s_{t_{n}+\\tau_{n,m}}^{\\theta}\\left(\\tilde{x}_{t_{n}+\\tau_{n,m}}\\right)-\\nabla\\log\\tilde{p}_{t_{n}+\\tau_{n,m}}\\left(\\tilde{x}_{t_{n}+\\tau_{n,m}}\\right)\\right\\|^{2}\\right]\\leq\\delta_{\\infty}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Assumption 3.4 (Bounded and Lipschitz true score). The true score function $\\nabla\\log{p_{t}}$ hasbounded $C^{1}$ norm,i.e. $\\|\\|\\nabla\\log p_{t}(\\cdot)\\|\\|_{L^{\\infty}([0,T])}\\leq M_{p}$ withLipschitz.constant $L_{p}$ ", "page_idx": 8}, {"type": "text", "text": "Further relaxations on Assumption 3.4 to time-dependent assumptions accommodating the blow-up to the data end (e.g. [94, Assumption 1.5]) are left for further work. ", "page_idx": 8}, {"type": "text", "text": "3.2.3 Theoretical Guarantees ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Our results for PIADM-ODE are summarized in the following theorem: ", "page_idx": 8}, {"type": "text", "text": "Theorem 3.5 (Theoretical Guarantees for PIADM-ODE). Under Assumptions 3.1', 3.2, 3.3, and 3.4, given thefollowingchoices of the order of theparameters ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{T=\\mathcal{O}(\\log(d\\delta^{-2})),\\quad h=\\Theta(1),\\quad N=\\mathcal{O}(\\log(d\\delta^{-2})),}}\\\\ {{\\epsilon=\\Theta\\left(d^{-1/2}\\delta\\log^{-1}(d^{-1/2}\\delta^{-1})\\right),\\quad M=\\mathcal{O}(d^{1/2}\\delta^{-1}\\log(d^{1/2}\\delta^{-1})),\\quad K=\\widetilde{\\mathcal{O}}(\\log(d\\delta^{-2})),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "for the outer iteration and ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{c}{T^{\\dagger}=\\mathcal{O}(1)\\lesssim L_{p}^{-1/2}\\wedge L_{s}^{-1/2},\\quad h^{\\dagger}=\\Theta(1),\\quad N^{\\dagger}=\\mathcal{O}(1),}\\\\ {\\epsilon^{\\dagger}=\\Theta(d^{-1/2}\\delta),\\quad M^{\\dagger}=\\mathcal{O}(d^{1/2}\\delta^{-1}),\\quad K^{\\dagger}=\\mathcal{O}(\\log(d\\delta^{-2})),}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "for the inner iteration during the corrector step, and let $L_{s}^{2}h^{2}e^{h}\\vee L_{s}^{2}{h^{\\dagger}}^{2}e^{h^{\\dagger}}/\\gamma\\;\\ll\\;1,$ $\\delta_{\\infty}\\lesssim$ $\\delta\\log^{-1}(d\\delta^{-2})$ and $\\gamma\\gtrsim L_{p}^{1/2}$ then the disribuion $\\widehat{q}_{t_{N}}$ that PIADM-ODE (Algorithm 2) generatessamplesfromsatisfiesthefollowingerrorbound: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{TV}(p_{\\eta},\\widehat{q}_{t_{N}})^{2}\\lesssim d e^{-T}+d\\epsilon^{2}T^{2}+(T^{2}+N^{2})\\delta_{\\infty}^{2}+d N^{2}e^{-K}\\lesssim\\delta^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "with a total of $(K+K^{\\dagger}N^{\\dagger})N=\\widetilde{\\mathcal{O}}\\left(\\log^{2}(d\\delta^{-2})\\right)$ approximate time complexity and $d(M\\vee M^{\\dagger})=$ $\\widetilde{\\Theta}\\left(d^{3/2}\\delta^{-1}\\right)$ space complexity for parallalizable $\\delta$ accurate scorefuntioncomputtios. ", "page_idx": 8}, {"type": "text", "text": "The reduction of space complexity by the probability flow ODE implementation is intuitively owing to the fact that the probability flow ODE process is a deterministic process in time rather than a stochastic process as in the SDE implementation, getting rid of the $O(\\epsilon)$ term derived by Ito's symmetry. This allows the discretization error to be bounded with $O(\\epsilon^{2})$ instead $c f.$ Lemma B.7 and C.5). ", "page_idx": 8}, {"type": "text", "text": "3.2.4 Proof Sketch ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The details of the proof of Theorem 3.5 are provided in Section C. Along with the complexity benefits the deterministic nature of the probability fow ODE may bring, the analysis is technically more involved than that of Theorem 3.3 and requires an intricate interplay between statistical distances. Several major challenges and our corresponding solutions are summarized below: ", "page_idx": 8}, {"type": "text", "text": "\u00b7 The error of the parallelized algorithm within each block may now only be bounded by 2- Wasserstein distance (cf. Theorem C.7) instead of any $f$ -divergence that admits data processing inequality as in the SDE case by Girsanov's theorem. The additional corrector step exactly handles this issue and would intuitively translate 2-Wasserstein proximity to TV distance proximity (cf. Lemma C.18), allowing the decomposition of the overall error into each block; ", "page_idx": 8}, {"type": "text", "text": "\u00b7 For the corrector step, the underdamped Langevin dynamics as a second-order dynamics requires Only $\\mathcal{O}(\\sqrt{d})$ steps to converge, instead of $O(d)$ steps in its overdamped counterpart. We then adapt the parallelization technique mentioned in Section 2.2 to conclude that it can be accomplished with ${\\mathcal{O}}(\\log d)$ approximate time complexity $c f.$ Theorem C.17). The error caused by the approximation to the true score and numerical discretization within this step is bounded in KL divergence by invoking Girsanov's theorem(Theorem A.4) as in the proof of Theorem 3.3; \u00b7 Different from the SDE case, where the chain rule of KL divergence can easily decouple the initial distribution and the subsequent dynamics, we need several interpolating processes between the implementation and the true backward process in this case. The final guarantee is in TV distance as it connects with the KL divergence via Pinsker's inequality and admits data processing inequality. We refer the readers to Figure 2 for an overview of the proof pipeline, as well as the notations and intuitions of the auxiliary and interpolating processes appearing in the proof. ", "page_idx": 9}, {"type": "text", "text": "4  Discussion and Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we have proposed novel parallelization strategies for the inference of diffusion models in both the SDE and probability fow ODE implementations. Our algorithms, namely PIADMSDE and PIADM-ODE, are meticulously designed and rigorously proved to achieve $\\widetilde{\\mathcal{O}}(\\mathrm{poly}\\log d)$ approximate time complexity and $\\widetilde{\\cal O}(d^{2})$ and $\\widetilde{\\mathcal{O}}(d^{3/2})$ space complexity, respectively, marking the first inference algorithm of diffusion and probability flow based models with sub-linear approximate time complexity. Our algorithm intuitively divides the time horizon into several $\\mathcal{O}(1)$ blocks and applies Picard iteration within each block in parallel, transferring the time complexity into space complexity. Our analysis is built on a sophisticated mathematical framework of stochastic processes and provides deeper insights into the mathematical theory of diffusion models. ", "page_idx": 9}, {"type": "text", "text": "Our findings echo and corroborate the recent empirical work [1, 85] that parallel sampling techniques significantly accelerate the inference process of diffusion models. Theoretical exploration of the adaptive block window scheme therein presents an interesting future research potential. Possible future work also includes the investigation of how to apply our parallelization framework to other variants of diffusion models, such as the discrete [23, 123-133] and multi-marginal [134] formulations. Although we anticipate implementing diffusion models in parallel may introduce engineering challenges, e.g. scalability, hardware compatibility, memory bandwidth, etc., we believe that our theoretical contributions lay a solid foundation that not only supports but also motivates the empirical development of parallel inference algorithms for diffusion models since advancements continue in GPU power and memory efficiency. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "LY acknowledges the support of the National Science Foundation under Grant No. DMS-2011699 and DMS-2208163. GMR is supported by a Google Research Scholar Award. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Andy Shih, Suneel Belkhale, Stefano Ermon, Dorsa Sadigh, and Nima Anari. Parallel sampling of diffusion models. Advances in Neural Information Processing Systems, 36, 2024.   \n[2]  Michael S Albergo, Nicholas M Boffi, and Eric Vanden-Eijnden. Stochastic interpolants: A unifying framework for flows and diffusions. arXiv preprint arXiv:2303.08797, 2023.   \n[3]  Michael S Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic interpolants. arXiv preprint arXiv:2209.15571, 2022.   \n[4]  Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022.   \n[5]  Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified fow. arXiv preprint arXiv:2209.03003, 2022.   \n[6] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, pages 2256-2265. PMLR, 2015.   \n[7]  Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840-6851, 2020.   \n[8]  Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon. Maximum likelihood training of score-based diffusion models. Advances in neural information processing systems, 34: 1415-1428, 2021.   \n[9]  Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019.   \n[10] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020.   \n[11] Linfeng Zhang, Weinan E, and Lei Wang. Monge-ampere fow for generative modeling. arXiv preprint arXiv: 1809.10188, 2018.   \n[12] Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel. Multidiffusion: Fusing diffusion paths for controlled image generation. In International Conference on Machine Learning, pages 1737-1752. Pmlr, 2023.   \n[13] Xinlei Chen, Zhuang Liu, Saining Xie, and Kaiming He. Deconstructing denoising diffusion models for self-supervised learning. arXiv preprint arXiv:2401.14404, 2024.   \n[14] Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded difusion models for high fdelityimage generation. Journal of Machine Learning Research, 23(47):1-33, 2022.   \n[15] Nanye Ma, Mark Goldstein, Michael S Albergo, Nicholas M Boff, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. arXiv preprint arXiv:2401.08740, 2024.   \n[16] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073, 2021.   \n[17]  Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning, pages 8821-8831. Pmlr, 2021.   \n[18]  Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022.   \n[19] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684-10695, 2022.   \n[20]  Yang Song, Liyue Shen, Lei Xing, and Stefano Ermon. Solving inverse problems in medical imaging with score-based generative models. arXiv preprint arXiv:2111.08005, 2021.   \n[21]  Yu Sun, Zihui Wu, Yifan Chen, Berthy T Feng, and Katherine L Bouman. Provable probabilistic imaging using score-based generative priors. arXiv preprint arXiv:2310.10835, 2023.   \n[22]  Xingyu Xu and Yuejie Chi. Provably robust score-based diffusion posterior sampling for plug-and-play image reconstruction. arXiv preprint arXiv:2403.17042, 2024.   \n[23] Jacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured denoising diffusion models in discrete state-spaces. Advances in Neural Information Processing Systems, 34:17981-17993, 2021.   \n[24] Xiang Li, John Thickstun, Ishaan Gulrajani, Percy S Liang, and Tatsunori B Hashimoto. Diffusion-lm improves controllable text generation. Advances in Neural Information Processing Systems, 35:4328-4343, 2022.   \n[25] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet. Video diffusion models. Advances in Neural Information Processing Systems, 35:8633-8646, 2022.   \n[26] Yujia Huang, Adishree Ghatare, Yuanzhe Liu, Ziniu Hu, Qinsheng Zhang, Chandramouli S Sastry, Siddharth Gururani, Sageev Oore, and Yisong Yue. Symbolic music generation with non-differentiable rule guided diffusion. arXiv preprint arXiv:2402.14285, 2024.   \n[27] Gautam Mittal, Jesse Engel, Curtis Hawthorne, and Ian Simon. Symbolic music generation with diffusion models. arXiv preprint arXiv:2103.16091, 2021.   \n[28] Flavio Schneider. 2oArchisound: Audio generation with diffusion. arXiv preprint arXiv:2301.13267, 2023.   \n[29]  Ruihan Yang, Prakhar Srivastava, and Stephan Mandt. Diffusion probabilistic modeling for video generation. Entropy, 25(10): 1469, 2023.   \n[30] Zihao Li, Hui Yuan, Kaixuan Huang, Chengzhuo Ni, Yinyu Ye, Minshuo Chen, and Mengdi Wang. Diffusion model for data-driven black-box optimization. arXiv preprint arXiv:2403.13219, 2024.   \n[31] Chen Xu, Jonghyeok Lee, Xiuyuan Cheng, and Yao Xie. Flow-based distributionally robust optimization. IEEE Journal on Selected Areas in Information Theory, 2024.   \n[32]  Ahmed El Alaoui, Andrea Montanari, and Mark Sellke. Sampling from mean-field gibbs measures via diffusion processes. arXiv preprint arXiv:2310.08912, 2023.   \n[33] Sitan Chen, Vasilis Kontonis, and Kulin Shah. Learning general gaussian mixtures with efficient score matching. arXiv preprint arXiv:2404.18893, 2024.   \n[34]  Ahmed El Alaoui, Andrea Montanari, and Mark Sellke. Sampling from the sherringtonkirkpatrick gibbs measure via algorithmic stochastic localization. In 2022 IEEE 63rd Annual Symposium on Foundations of Computer Science (FOCS), pages 323-334. IEEE, 2022.   \n[35] Khashayar Gatmiry, Jonathan Kelner, and Holden Lee.Learning mixtures of gaussians using diffusion models. arXiv preprint arXiv:2404.18869, 2024.   \n[36]  Ye He, Kevin Rojas, and Molei Tao. Zeroth-order sampling methods for non-log-concave distributions: Alleviating metastability by denoising diffusion. arXiv preprint arXiv:2402.17886, 2024.   \n[37] Xunpeng Huang, Hanze Dong, HAO Yifan, Yian Ma, and Tong Zhang. Reverse diffusion monte carlo. In The Twelth International Conference on Learning Representations, 2023.   \n[38]  Brice Huang, Andrea Montanari, and Huy Tuan Pham. Sampling from spherical spin glasses in total variation via algorithmic stochastic localization. arXiv preprint arXiv:2404.15651, 2024.   \n[39] Song Mei and Yuchen Wu. Deep networks as denoising algorithms: Sample-efficient learning of difusion models in high-dimensional graphical models. arXiv preprint arXiv:2309.11420, 2023.   \n[40] Andrea Montanari. Sampling, diffusions, and stochastic localization. arXiv preprint arXiv:2305.10690, 2023.   \n[41]  Andrea Montanari and Yuchen Wu. Posterior sampling from the spiked models via diffusion processes. arXiv preprint arXiv:2304.11449, 2023.   \n[42]  Nicholas M Boffi and Eric Vanden-Eijnden. Probability fow solution of the fokker-planck equation. Machine Learning: Science and Technology, 4(3):035012, 2023.   \n[43]  Yan Huang and Li Wang. A score-based particle method for homogeneous landau equation. arXiv preprint arXiv:2405.05187, 2024.   \n[44]  Lingxiao Li, Samuel Hurault, and Justin M Solomon. Self-consistent velocity matching of probability fows. Advances in Neural Information Processing Systems, 36, 2024.   \n[45]  Jianfeng Lu, Yue Wu, and Yang Xiang. Score-based transport modeling for mean-field fokkerplanck equations. Journal of Computational Physics, 503:112859, 2024.   \n[46] Dimitra Maoutsa, Sebastian Reich, and Manfred Opper. Interacting particle solutions of fokker-planck equations through gradient-log-density estimation. Entropy, 22(8):802, 2020.   \n[47]  Amira Alakhdar, Barnabas Poczos, and Newell Washburn. Diffusion models in de novo drug design. Journal of Chemical Information and Modeling, 2024.   \n[48]  Andrew Campbell, Jason Yim, Regina Barzilay, Tom Rainforth, and Tommi Jaakkola. Generative flows on discrete state-spaces: Enabling multimodal flows with applications to protein co-design. arXiv preprint arXiv:2402.04997, 2024.   \n[49] Hannes Stark, Bowen Jing, Chenyu Wang, Gabriele Corso, Bonnie Berger, Regina Barzilay, and Tommi Jakkola. Dirichlet flow matching with applications to dna sequence design. arXiv preprint arXiv:2402.05841, 2024.   \n[50]  Pavel Avdeyey, Chenlai Shi, Yuhao Tan, Ksenia Dudnyk, and Jian Zhou. Dirichlet diffusion score model for biological sequence generation. In International Conference on Machine Learning, pages 1276-1301. PMLR, 2023.   \n[51] Sarah Alamdari, Nitya Thakkar, Rianne van den Berg, Alex X Lu, Nicolo Fusi, Ava P Amini, and Kevin K Yang. Protein generation with evolutionary diffusion: sequence is all you need. BioRxiv, pages 2023-09, 2023.   \n[52] Joseph L Watson, David Juergens, Nathaniel R Bennett, Brian L Trippe, Jason Yim, Helen E Eisenach, Woody Ahern, Andrew J Borst, Robert J Ragotte, Lukas F Milles, et al. De novo design of protein structure and function with rfdiffusion. Nature, 620(7976): 1089-1100, 2023.   \n[53]  Jordan Cotler and Semon Rezchikov.  Renormalizing diffusion models. arXiv preprint arXiv:2308.12355, 2023.   \n[54] Florian Furrutter, Gorka Munoz-Gil, and Hans J Briegel. Quantum circuit synthesis with diffusion models. arXiv preprint arXiv:2311.02041, 2023.   \n[55] Zhiye Guo, Jian Liu, Yanli Wang, Mengrui Chen, Duolin Wang, Dong Xu, and Jianlin Cheng. Diffusion models in bioinformatics and computational biology. Nature reviews bioengineering, 2(2):136-154, 2024.   \n[56]  Artan Sheshmani, Yi-Zhuang You, Baturalp Buyukates, Amir Ziashahabi, and Salman Avestimehr. Renormalization group fow, optimal transport and difusion-based generative model. arXiv preprint arXiv:2402.17090, 2024.   \n[57] Luke Triplett and Jianfeng Lu. Diffusion methods for generating transition paths. arXiv preprint arXiv:2309.10276, 2023.   \n[58] Lingxiao Wang, Gert Aarts, and Kai Zhou. Generative diffusion models for lattice field theory. arXiv preprint arXiv:2311.03578, 2023.   \n[59] Minkai Xu, Lantao Yu, Yang Song, Chence Shi, Stefano Ermon, and Jian Tang. Geodiff: A geometric diffusion model for molecular conformation generation. arXiv preprint arXiv:2203.02923, 2022.   \n[60] Mengjiao Yang, KwangHwan Cho, Amil Merchant, Pieter Abbeel, Dale Schuurmans, Igor Mordatch, and Ekin Dogus Cubuk. Scalable diffusion for materials generation. arXiv preprint arXiv:2311.09235, 2023.   \n[61] Jiahao Fan, Ziyao Li, Eric Alcaide, Guolin Ke, Huaqing Huang, and Weinan E. Accurate conformation sampling via protein structural diffusion. Journal of Chemical Information and Modeling, 2024.   \n[62] Jason Yim, Hannes Stark, Gabriele Corso, Bowen Jing, Regina Barzilay, and Tommi S Jakkola Diffusion models in protein structure and docking. Wiley Interdisciplinary Reviews: Computational Molecular Science, 14(2):e1711, 2024.   \n[63] Yuchen Zhu, Tianrong Chen, Evangelos A Theodorou, Xie Chen, and Molei Tao. Quantum state generation with structure-preserving diffusion model. arXiv preprint arXiv:2404.06336, 2024.   \n[64] Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao, Wentao Zhang, Bin Cui, and Ming-Hsuan Yang. Diffusion models: A comprehensive survey of methods and applications. ACM Computing Surveys, 56(4):1-39, 2023.   \n[65] Stanley H Chan. Tutorial on diffusion models for imaging and vision. arXiv preprint arXiv:2403.18103, 2024.   \n[66]  Minshuo Chen, Song Mei, Jianqing Fan, and Mengdi Wang. Opportunities and challenges of diffusion models for generative ai. National Science Review, page nwae348, 2024.   \n[67] Valentin De Bortoli, James Thornton, Jeremy Heng, and Arnaud Doucet.  Diffusion schrodinger bridge with applications to score-based generative modeling. Advances in Neural Information Processing Systems, 34:17695-17709, 2021.   \n[68]  Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020.   \n[69]  Tim Dockhorn, Arash Vahdat, and Karsten Kreis. Genie: Higher-order denoising diffusion solvers. Advances in Neural Information Processing Systems, 35:30150-30166, 2022.   \n[70] Gen Li, Yu Huang, Timofey Efmov, Yuting Wei, Yuejie Chi, and Yuxin Chen. Acelerating convergence of score-based diffusion models, provably. arXiv preprint arXiv:2403.03852, 2024.   \n[71] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information Processing Systems, 35:5775-5787, 2022.   \n[72] Tero Karras, Mika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. Advances in Neural Information Processing Systems, 35: 26565-26577, 2022.   \n[73]  Qinsheng Zhang and Yongxin Chen. Fast sampling of diffusion models with exponential integrator. arXiv preprint arXiv:2204.13902, 2022.   \n[74] Saravanan Kandasamy and Dheeraj Nagaraj. The poisson midpoint method for langevin dynamics: Provably eficient discretization for diffusion models. arXiv preprint arXiv:2405.17068, 2024.   \n[75] Yuchen Wu, Yuxin Chen, and Yuting Wei. Stochastic runge-kutta methods: Provable acceleration of diffusion models. arXiv preprint arXiv:2410.04760, 2024.   \n[76] Tim Dockhorn, Arash Vahdat, and Karsten Kreis. Score-based generative modeling with critically-damped langevin diffusion. arXiv preprint arXiv:2112.07068, 2021.   \n[77] Alexia Jolicoeur-Martineau, Ke Li, R\u00e9mi Piche-Taillefer, Tal Kachman, and Ioannis Mitliagkas. Gotta go fast when generating data with score-based models. arXiv preprint arXiv:2105.14080, 2021.   \n[78]  Hongkai Zheng, Weili Nie, Arash Vahdat, Kamyar Azizzadenesheli, and Anima Anandkumar. Fast sampling of diffusion models via operator learning. In International Conference on Machine Learning, pages 42390-42402. PMLR, 2023.   \n[79] Yilun Xu, Mingyang Deng, Xiang Cheng, Yonglong Tian, Ziming Liu, and Tommi Jaakkola. Restart sampling for improving generative processes. Advances in Neural Information Processing Systems, 36:76806-76838, 2023.   \n[80]  Jonathan Heek, Emiel Hoogeboom, and Tim Salimans. Multistep consistency models. arXiv preprint arXiv:2403.06807, 2024.   \n[81]  Yang Song, Prafulla Dhariwal, Mark Chen, and Iya Sutskever. Consistency models. arXiv preprint arXiv:2303.01469, 2023.   \n[82] Eric Luhman and Troy Luhman. Knowledge distiltion in iterative generative models for improved sampling speed. arXiv preprint arXiv:2101.02388, 2021.   \n[83] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans. On distillation of guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14297-14306, 2023.   \n[84]  Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. arXiv preprint arXiv:2202.00512, 2022.   \n[85]  Zhiwei Tang, Jiasheng Tang, Hao Luo, Fan Wang, and Tsung-Hui Chang. Accelerating parallel sampling of diffusion models. arXiv preprint arXiv:2402.09970, 2024.   \n[86]  Shivam Gupta, Linda Cai, and Sitan Chen. Faster diffusion-based sampling with randomized midpoints: Sequential and parallel. arXiv preprint arXiv:2406.00924, 2024.   \n[87]  Gen Li and Yuchen Jiao. Improved convergence rate for diffusion probabilistic models. arXiv preprint arXiv:2410.13738, 2024.   \n[88]  Ruoqi Shen and Yin Tat Lee. The randomized midpoint method for log-concave sampling. Advances in Neural Information Processing Systems, 32, 2019.   \n[89] Belinda Tzen and Maxim Raginsky. Theoretical guarantees for sampling and inference in generative models with latent diffusions. In Conference on Learning Theory, pages 3084- 3114. PMLR, 2019.   \n[90]  Adam Block, Youssef Mroueh, and Alexander Rakhlin. Generative modeling with denoising auto-encoders and langevin sampling. arXiv preprint arXiv:2002.00107, 2020.   \n[91] Sitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim, and Anru R Zhang. Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions. arXiv preprint arXiv:2209.11215, 2022.   \n[92] Holden Lee, Jianfeng Lu, and Yixin Tan. Convergence for score-based generative modeling with polynomial complexity. Advances in Neural Information Processing Systems, 35:22870- 22882, 2022.   \n[93] Francesco Pedrotti, Jan Maas, and Marco Mondelli. Improved convergence of score-based diffusion models via prediction-correction. arXiv preprint arXiv:2305.14164, 2023.   \n[94]  Sitan Chen, Giannis Daras, and Alex Dimakis. Restoration-degradation beyond linear diffusions: A non-asymptotic analysis for ddim-type samplers. In International Conference on Machine Learning, pages 4462-4484. PMLR, 2023.   \n[95]  Hongrui Chen, Holden Lee, and Jianfeng Lu. Improved analysis of score-based generative modeling: User-friendly bounds under minimal smoothness assumptions. In International Conference on Machine Learning, pages 4735-4763. PMLR, 2023.   \n[96]  Holden Lee, Jianfeng Lu, and Yixin Tan. Convergence of score-based generative modeling for general data distributions. In International Conference on Algorithmic Learning Theory, pages 946-985. PMLR, 2023.   \n[97]  Sokhna Diarra Mbacke and Omar Rivasplata. A note on the convergence of denoising difusion probabilistic models. arXiv preprint arXiv:2312.05989, 2023. [98] Joe Benton, Valentin De Bortoli, Arnaud Doucet, and George Deligiannidis. Linear convergence bounds for diffusion models via stochastic localization. arXiv preprint arXiv:2308.03686, 2023.   \n[99] Gen Li, Yuting Wei, Yuxin Chen, and Yuejie Chi Towards faster non-asymptotic convergence for diffusion-based generative models. arXiv preprint arXiv:2306.09251, 2023.   \n[100] Gen Li, Yuting Wei, Yuxin Chen, and Yuejie Chi. Towards non-asymptotic convergence for diffusion-based generative models. In The Twelfth International Conference on Learning Representations, 2024.   \n[101] Gen Li, Yuting Wei, Yuejie Chi, and Yuxin Chen. A sharp convergence theory for the probability flow odes of diffusion models. arXiv preprint arXiv:2408.02320, 2024.   \n[102] Sitan Chen, Sinho Chewi, Holden Lee, Yuanzhi Li, Jianfeng Lu, and Adil Salim. The probability fow ode is provably fast. Advances in Neural Information Processing Systems, 36, 2024.   \n[103] Xuefeng Gao and Lingjiong Zhu. Convergence analysis for general probability flow odes of diffusion models in wasserstein distances. arXiv preprint arXiv:2401.17958, 2024.   \n[104] Yuchen Liang, Peizhong Ju, Yingbin Liang, and Ness Shroff. Non-asymptotic convergence of discrete-time diffusion models: New approach and improved rate. arXiv preprint arXiv:2402.13901, 2024.   \n[105] Daniel Zhengyu Huang, Jiaoyang Huang and Zhengjiang Lin.Convergence analysisof probability fow ode for score-based generative models. arXiv preprint arXiv:2404.09730, 2024.   \n[106]  Gen Li and Yuling Yan. $o(d/t)$ convergence theory for diffusion probabilistic models under minimal assumptions. arXiv preprint arXiv:2409.18959, 2024.   \n[107] Brian DO Anderson. Reverse-time diffusion equation models. Stochastic Processes and their Applications, 12(3):313-326, 1982.   \n[108] Aapo Hyvarinen and Peter Dayan. Estimation of non-normalized statistical models by score matching. Journal of Machine Learning Research, 6(4), 2005.   \n[109]  Pascal Vincent. A connection between score matching and denoising autoencoders. Neural computation, 23(7):1661-1674, 2011.   \n[110] Yu Cao, Jingrun Chen, Yixin Luo, and Xiang Zhou. Exploring the optimal choice for generative processes in diffusion models: Ordinary vs stochastic differential equations. Advances in Neural Information Processing Systems, 36, 2024.   \n[111] Teo Deveney, Jan Stanczuk, Lisa Maria Kreusser, Chris Budd, and Carola-Bibiane Schonlieb. Closing the ode-sde gap in score-based diffusion models through the fokker-planck equation. arXiv preprint arXiv:2311.15996, 2023.   \n[112] Shen Nie, Hanzhong Alan Guo, Cheng Lu, Yuhao Zhou, Chenyu Zheng, and Chongxuan Li. The blessing of randomness: Sde beats ode in general difusion-based image editing. arXiv preprint arXiv:2311.01410, 2023.   \n[113]  Charles J Geyer. Markov chain monte carlo maximum likelihood. In E. M. Kernamidas, editor, Computing Science and Statistics: Proc. 23rd Symposium on the Interface, pages 156-163. Interface Foundation of North America, 1991.   \n[114] Koji Hukushima and Koji Nemoto. Exchange monte carlo method and application to spin glass simulations. Journal of the Physical Society of Japan, 65(6):1604-1608, 1996.   \n[115] Faming Liang. Use of sequential structure in simulation from high-dimensional systems. Physical Review E, 67(5):056101, 2003.   \n[116] Nima Anari, Yizhi Huang, Tianyu Liu, Thuy-Duong Vuong, Brian Xu, and Katherine Yu. Parallel discrete sampling via continuous walks. In Proceedings of the 55th Annual ACM Symposium on Theory of Computing, pages 103-116, 2023.   \n[117] Holden Lee. Parallelising glauber dynamics. arXiv preprint arXiv:2307.07131, 2023.   \n[118] Lu Yu and Arnak Dalalyana. Parallelized midpoint randomization for langevin monte carlo. arXiv preprint arXiv:2402.14434, 2024.   \n[119]  Ernest Lindelof. Sur lapplication de la methode des approximations successives aux quations differentielles ordinaires du premier ordre. Comptes rendus hebdomadaires des seances de lAcademie des sciences, 116(3):454-457, 1894.   \n[120] Emile Picard. Sur les methodes d'approximations successives dans la theorie des \u00e9quations diff\u00e9rentielles. American Journal of Mathematics, pages 87-100, 1898.   \n[121] Nima Anari, Sinho Chewi, and Thuy-Duong Vuong. Fast parall sampling under isoperimetry. arXiv preprint arXiv:2401.09016, 2024.   \n[122] Bernt Oksendal. Stochastic differential equations: an introduction with applications. Springer Science & Business Media, 2013.   \n[123] Emiel Hoogeboom, Alexey A Gritsenko, Jasmijn Bastings, Ben Poole, Rianne van den Berg, and Tim Salimans. Autoregressive diffusion models. arXiv preprint arXiv:2110.02037, 2021.   \n[124] Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forre, and Max Welling. Argmax fows and multinomial difusion: Learning categorical distributions. Advances in Neural Information Processing Systems, 34:12454-12465, 2021.   \n[125] Chenlin Meng, Kristy Choi, Jiaming Song, and Stefano Ermon. Concrete score matching: Generalized score matching for discrete data. Advances in Neural Information Processing Systems, 35:34532-34545, 2022.   \n[126] Haoran Sun, Lijun Yu, Bo Dai, Dale Schuurmans, and Hanjun Dai. Score-based continuoustime discrete diffusion models. arXiv preprint arXiv:2211.16750, 2022.   \n[127] Pierre H Richemond, Sander Dieleman, and Arnaud Doucet. Categorical sdes with simplex diffusion. arXiv preprint arXiv:2210.14784, 2022.   \n[128] Aaron Lou, Chenlin Meng, and Stefano Ermon. Discrete diffusion language modeling by estimating the ratios of the data distribution. arXiv preprint arXiv:2310.16834, 2023.   \n[129] Griffin Floto, Thorsteinn Jonsson, Mihai Nica, Scott Sanner, and Eric Zhengyu Zhu. Diffusion on the probability simplex. arXiv preprint arXiv:2309.02530, 2023.   \n[130] Javier E Santos, Zachary R Fox, Nicholas Lubbers, and Yen Ting Lin. Blackout diffusion: generative diffusion models in discrete-state spaces. In International Conference on Machine Learning, pages 9034-9059. PMLR, 2023.   \n[131] Joe Benton, Yuyang Shi, Valentin De Bortoli, George Deligiannidis, and Arnaud Doucet. From denoising difusions to denoising markov models. Journal of the Royal Statistical Society Series B: Statistical Methodology, 86(2):286-301, 2024.   \n[132] Hongrui Chen and Lexing Ying. Convergence analysis of discrete diffusion model: Exact implementation through uniformization. arXiv preprint arXiv:2402.08095, 2024.   \n[133] Yinuo Ren, Haoxuan Chen, Grant M Rotskoff, and Lexing Ying. How discrete and continuous diffusion meet: Comprehensive analysis of discrete diffusion models via a stochastic integral framework. arXiv preprint arXiv:2410.03601, 2024.   \n[134] Michael S Albergo, Nicholas M Boff, Michael Lindsey, and Eric Vanden-Eijnden. Multimarginal generative modeling with stochastic interpolants. arXiv preprint arXiv:2310.03695, 2023.   \n[135] Jean-Frangois Le Gall. Brownian motion, martingales, and stochastic calculus. Springer, 2016.   \n[136] Arnaud Guillin and Feng-Yu Wang. Degenerate fokker-planck equations: Bismut formula, gradient estimate and harnack inequality. Journal of Differential Equations, 253(1):20-40, 2012. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "A Mathematical Background ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we will summarize used notations and rigorous mathematical framework of Ito processes as necessary in the proofs. We will also present several technical lemmas for later reference. ", "page_idx": 17}, {"type": "text", "text": "A.1 Notations ", "text_level": 1, "page_idx": 17}, {"type": "table", "img_path": "F9NDzHQtOl/tmp/496fac63c69e4c964fa3f6d98287bc6ead4336281e79acf6abb0dbdaffafd8a0.jpg", "table_caption": ["We adopt the following notations throughout the paper: "], "table_footnote": [""], "page_idx": 17}, {"type": "text", "text": "A.2  Preliminaries ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Theorem A.1 (Properties of $f$ -divergence). Suppose $p$ and $q$ are two probability measures on a common measurable space $(\\Omega,{\\mathcal{F}})$ with $p\\ll q$ The $f$ -divergence between $p$ and $q$ is defined as ", "page_idx": 17}, {"type": "equation", "text": "$$\nD_{f}(p\\|q)=\\mathbb{E}_{X\\sim q}\\left[f\\left({\\frac{\\mathrm{d}p}{\\mathrm{d}q}}\\right)\\right],\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "${\\frac{\\mathrm{d}p}{\\mathrm{d}q}}\\,$ $p$ $q$ $f:\\mathbb{R}^{+}\\,\\rightarrow\\,\\mathbb{R}$ $D_{f}(\\cdot\\parallel\\cdot)$ $(K L)$ $f(x)=$ $x\\log x$ and $D_{f}(\\cdot\\parallel\\cdot)=\\mathrm{TV}$ coincides with the total variation (TV) distance when $\\begin{array}{r}{f(x)=\\frac{1}{2}|x-1|}\\end{array}$ ", "page_idx": 17}, {"type": "text", "text": "Forthe $f$ -divergence defined above, we have the following properties: ", "page_idx": 17}, {"type": "text", "text": "1. (Data-processing inequality). Suppose $\\mathcal{H}$ is a sub- $\\sigma$ -algebra of $\\mathcal{F}$ the following inequality holds ", "page_idx": 17}, {"type": "equation", "text": "$$\nD_{f}\\left(p|_{\\mathcal{H}}\\parallel q|_{\\mathcal{H}}\\right)\\le D_{f}(p\\parallel q);\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for any $f$ -divergence $D_{f}(\\cdot\\|\\cdot)$ ", "page_idx": 17}, {"type": "text", "text": "2. (Chain rule). Suppose $X$ is a random variable generating a sub- $\\sigma$ -algebra ${\\mathcal{F}}_{X}$ of $\\mathcal{F}$ \uff0cand $p(\\cdot|X)\\ll q(\\cdot|X)$ holds for any value of $X$ then ", "page_idx": 17}, {"type": "equation", "text": "$$\nD_{\\mathrm{KL}}(p\\|q)=D_{\\mathrm{KL}}(p|_{\\mathcal{F}_{X}}\\|q|_{\\mathcal{F}_{X}})+\\mathbb{E}_{\\mathcal{F}_{X}}[D_{\\mathrm{KL}}(p(\\cdot|X)\\|q(\\cdot|X))].\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "In this paper, we consider a probability space $(\\Omega,{\\mathcal{F}},p)$ on which $(\\pmb{w}_{t}(\\omega))_{t\\geq0}$ is a Wiener process in $\\mathbb{R}^{d}$ . The Wiener process $(\\pmb{w}_{t}(\\omega))_{t\\geq0}$ generates the filtration $\\{\\mathcal{F}_{t}\\}_{t\\ge0}$ on the measurable space $(\\Omega,{\\mathcal{F}})$ . For an Ito process $z_{t}(\\omega)$ with the following governing SDE: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathrm{d}z_{t}(\\omega)=\\alpha(t,\\omega)\\mathrm{d}t+\\Sigma(t,\\omega)\\mathrm{d}w_{t}(\\omega),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for any time $t$ , we denote the marginal distribution of $\\boldsymbol{z}_{t}$ by $p_{t}$ , i.e. ", "page_idx": 18}, {"type": "equation", "text": "$$\np_{t}:=p\\left(z_{t}^{-1}(\\cdot)\\right),\\mathrm{~where~}z_{t}:\\Omega\\rightarrow\\mathbb{R}^{m},\\;\\omega\\mapsto z_{t}(\\omega),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "as well as the path measure of the process $z_{t}$ in the sense of ", "page_idx": 18}, {"type": "equation", "text": "$$\np_{t_{1}:t_{2}}:=p\\left(z_{t_{1}:t_{2}}^{-1}(\\cdot)\\right),\\mathrm{~where~}z_{t_{1}:t_{2}}:\\Omega\\rightarrow\\mathcal{C}([t_{1},t_{2}],\\mathbb{R}^{m}),\\;\\omega\\mapsto(z_{t}(\\omega))_{t\\in[t_{1},t_{2}]}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "For the sake of simplicity, we define the following class of functions: ", "page_idx": 18}, {"type": "text", "text": "Definition A.2. For any $0~\\leq~t_{1}~<~t_{2}$ ,we define $\\mathcal{V}(t_{1},t_{2})$ as the class of functions $f(t,\\omega)$ $[0,+\\infty)\\times\\Omega\\rightarrow\\mathbb{R}$ such that ", "page_idx": 18}, {"type": "text", "text": "1. $f(t,\\omega)$ is $B\\times{\\mathcal{F}}$ -measurable,where $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ is the Borel $\\sigma$ -algebra on $\\mathbb{R}^{d}$ ", "page_idx": 18}, {"type": "text", "text": "2. $f(t,\\omega)$ .s $\\mathcal{F}_{t}$ -adapted for all $t\\geq0$ ", "page_idx": 18}, {"type": "text", "text": "3. The following Novikov condition holds ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\exp\\int_{t_{1}}^{t_{2}}f^{2}(t,\\omega)\\mathrm{d}t\\right]<+\\infty,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and $\\nu=\\cap_{t>0}\\nu(0,t)$ . For vectors and matrices, we say it belongs to $\\mathcal{V}^{n}(t,\\omega)$ or $\\nu^{m\\times n}(t,\\omega)$ each component of the vector or each entry of the matrix belongs to $\\mathcal{V}(t,\\omega)$ ", "page_idx": 18}, {"type": "text", "text": "Remark A.3. Novikov's condition appeared in the third requirement is often relaxed to the squared integrability condition in the general definition of Ito processes, which requires ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\int_{t_{1}}^{t_{2}}f^{2}(t,\\omega)\\mathrm{d}t\\right]<+\\infty.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Here, we adopt the more restricted condition in the spirit of its necessity for Girsanov's theorem to hold,asweshall seelater. ", "page_idx": 18}, {"type": "text", "text": "Similar to previous work [102], here we can avoid checking Novikov's condition throughout our proofs below by using the approximation argument presented in [91]. A review of Girsanov can be found in textbooks like in [122, 135]. We will present the following generalized version of Girsanov's theorem: ", "page_idx": 18}, {"type": "text", "text": "Theorem A.4 (Girsanov's Theorem [122, Theorem 8.6.6]). Let $\\pmb{\\alpha}(t,\\omega)\\in\\mathcal{V}^{m}$ $\\Sigma(t,\\omega)\\in\\mathcal{V}^{m\\times n}$ and $(\\pmb{w}_{t}(\\omega))_{t\\geq0}$ be a Wiener process on the probability space $(\\Omega,{\\mathcal{F}},q)$ .For $t\\,\\in\\,[0,T]$ .suppose $z_{t}(\\omega)$ is an Ito process with thefollowing $S D E$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathrm{d}z_{t}(\\omega)=\\alpha(t,\\omega)\\mathrm{d}t+\\Sigma(t,\\omega)\\mathrm{d}w_{t}(\\omega),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and there exist processes $\\pmb{\\delta}(t,\\omega)\\in\\mathcal{V}^{n}$ and $\\beta(t,\\omega)\\in\\mathcal{V}^{m}$ such that ", "page_idx": 18}, {"type": "text", "text": "2.The process $M_{t}(\\omega)$ as defined below is a martingale with respect to the fltration $\\{\\mathcal{F}_{t}\\}_{t\\ge0}$ and probabilitymeasure $q$ ", "page_idx": 18}, {"type": "equation", "text": "$$\nM_{t}(\\omega)=\\exp\\left(-\\int_{0}^{t}\\delta(s,\\omega)^{\\top}\\mathrm{d}w_{s}(\\omega)-\\frac{1}{2}\\int_{0}^{t}\\|\\delta(s,\\omega)\\|^{2}\\mathrm{d}s\\right),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "then there exists another probability measure $p$ on $(\\Omega,{\\mathcal{F}})$ suchthat ", "page_idx": 18}, {"type": "text", "text": "1. $p\\ll q$ with the Radon-Nikodym derivative (w)=Mr(), ", "page_idx": 18}, {"type": "text", "text": "2. The process $\\widetilde{\\pmb{w}}_{t}(\\omega)$ as defined below is a Wiener process on $(\\Omega,{\\mathcal{F}},p)$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\widetilde{\\pmb{w}}_{t}(\\omega)=\\pmb{w}_{t}(\\omega)+\\int_{0}^{t}\\delta(\\boldsymbol{s},\\omega)\\mathrm{d}\\boldsymbol{s},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "3. Any continuous path in $\\mathcal{C}([t_{1},t_{2}],\\mathbb{R}^{m})$ generated by the process $\\scriptstyle z_{t}$ satisfies the following SDE under the probability measure $p$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{d}\\widetilde{z}_{t}(\\omega)=\\beta(t,\\omega)\\mathrm{d}t+\\Sigma(t,\\omega)\\mathrm{d}\\widetilde{{w}}_{t}(\\omega).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Corollary A.5. Suppose the conditions in Theorem A.4 hold, then for any $t_{1},t_{2}~\\in~[0,T]$ with $t_{1}<t_{2}$ the path measure of the SDE (A.3) under the probability measure $p$ in the sense of $p_{t_{1}:t_{2}}=$ $p\\left(z_{t_{1}:t_{2}}^{-1}(\\cdot)\\right)$ is absolutely continuous with respect to the path measure of the SDE (A.2) in the sense of $\\overline{{q_{t_{1}:t_{2}}}}=\\overline{{q}}\\left(z_{t_{1}:t_{2}}^{-1}(\\cdot)\\right)$ . Moreover, the KL divergence between the two path measures is given by ", "page_idx": 19}, {"type": "equation", "text": "$$\nD_{\\mathrm{KL}}(p_{t_{1}:t_{2}}||q_{t_{1}:t_{2}})=D_{\\mathrm{KL}}(p_{t_{1}}||q_{t_{1}})+\\mathbb{E}_{\\omega\\sim p|\\mathcal{F}_{t_{1}}}\\left[\\frac{1}{2}\\int_{t_{1}}^{t_{2}}||\\delta(t,\\omega)||^{2}\\mathrm{d}t\\right]\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. First, by Theorem A.1, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{\\normalfont~\\left.\\sum_{KL}}\\left(p_{t_{1}:t_{2}}\\right\\Vert q_{t_{1}:t_{2}}\\right)=D_{\\mathrm{KL}}(p|_{\\mathcal{F}_{t_{1}}}\\Vert q|_{\\mathcal{F}_{t_{1}}})+\\mathbb{E}_{z\\sim p|_{\\mathcal{F}_{t_{1}}}}\\left[D_{\\mathrm{KL}}\\bigl(p(\\tilde{z}_{t_{1}:t_{2}}^{-1}(\\cdot))|\\tilde{z}_{t_{1}}=\\tilde{z}\\bigr)\\Vert q(\\tilde{z}_{t_{1}:t_{2}}^{-1}(\\cdot))\\vert\\tilde{z}_{t_{1}}=\\tilde{z}_{t_{1}:t_{2}}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "From Girsanov's theorem (Theorem A.4), we have that the measure $p\\big|_{\\mathcal{F}_{t_{1}}}$ is absolutely continuous with respect to $q|_{\\mathcal{F}_{t_{1}}}$ , which allows us to compute the second term above as follows: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad~~D_{\\mathrm{KL}}\\left(p(\\widetilde{z}_{t_{1}:t_{2}}^{-1}(\\cdot)|\\widetilde{z}_{t_{1}}=\\widetilde{z})||q(\\widetilde{z}_{t_{1}:t_{2}}^{-1}(\\cdot)|\\widetilde{z}_{t_{1}}=\\widetilde{z})\\right)}\\\\ &{=\\mathbb{E}_{\\widetilde{z}_{t_{1}:t_{2}}}\\left[\\log\\frac{\\mathrm{d}p(\\widetilde{z}_{t_{1}:t_{2}}^{-1}(\\cdot)|\\widetilde{z}_{t_{1}}=z)}{\\mathrm{d}q(\\widetilde{z}_{t_{1}:t_{2}}^{-1}(\\cdot)|\\widetilde{z}_{t_{1}}=z)}\\right]=\\mathbb{E}_{\\omega\\sim p|\\boldsymbol{z}_{t_{1}}}\\left[\\log\\frac{\\mathrm{d}p|\\mathcal{F}_{t_{1}}}{\\mathrm{d}q|\\mathcal{F}_{t_{1}}}\\right]}\\\\ &{=\\mathbb{E}_{\\omega\\sim p|\\boldsymbol{z}_{t_{1}}}\\left[-\\int_{t_{1}}^{t_{2}}\\delta(t,\\omega)^{\\top}\\mathrm{d}w_{t}(\\omega)-\\frac{1}{2}\\int_{t_{1}}^{t_{2}}||\\delta(t,\\omega)||^{2}\\mathrm{d}t\\right]}\\\\ &{=\\mathbb{E}_{\\omega\\sim p|\\boldsymbol{z}_{t_{1}}}\\left[-\\int_{t_{1}}^{t_{2}}\\delta(t,\\omega)^{\\top}\\left(\\mathrm{d}\\widetilde{w}_{t}(\\omega)-\\delta(t,\\omega)\\mathrm{d}t\\right)-\\frac{1}{2}\\int_{t_{1}}^{t_{2}}||\\delta(t,\\omega)||^{2}\\mathrm{d}t\\right]}\\\\ &{=\\mathbb{E}_{\\omega\\sim p|\\boldsymbol{z}_{t_{1}}}\\left[\\frac{1}{2}\\int_{t_{1}}^{t_{2}}||\\delta(t,\\omega)||^{2}\\mathrm{d}t\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and therefore ", "page_idx": 19}, {"type": "equation", "text": "$$\nD_{\\mathrm{KL}}(p_{t_{1}:t_{2}}\\|q_{t_{1}:t_{2}})=D_{\\mathrm{KL}}(p_{t_{1}}\\|q_{t_{1}})+\\mathbb{E}_{\\omega\\sim p|_{\\mathcal{F}_{t_{1}}}}\\left[\\frac{1}{2}\\int_{t_{1}}^{t_{2}}\\|\\delta(t,\\omega)\\|^{2}\\mathrm{d}t\\right],\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which completes the proof. ", "page_idx": 19}, {"type": "text", "text": "A.3 Helper Lemmas ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Lemma A.6 ([98, Lemma 2]). For the backward process (2.3), we have for $0\\leq s<t<T$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t}\\left(\\mathbb{E}\\left[\\|\\nabla\\log\\tilde{p}_{t}(\\tilde{x}_{t})-\\nabla\\log\\tilde{p}_{s}(\\tilde{x}_{s})\\|^{2}\\right]\\right)\\leq\\frac{1}{2}\\mathbb{E}\\left[\\|\\nabla\\log\\tilde{p}_{s}(\\tilde{x}_{s})\\|^{2}\\right]+\\mathbb{E}\\left[\\|\\nabla^{2}\\log\\tilde{p}_{t}(\\tilde{x}_{t})\\|_{F}^{2}\\right].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Lemma A.7 ([98, Lemma 3]). For the forward process (2.3), we have for $0\\leq t<T$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\nabla\\log p_{t}(\\mathbf{x}_{t})\\right]\\leq d\\sigma_{t}^{-2},\\;a n d\\,\\mathbb{E}\\left[\\|\\nabla^{2}\\log p_{t}(\\mathbf{x}_{t})\\|_{F}^{2}\\right]\\leq d\\sigma_{t}^{-4}+2{\\frac{\\mathrm{d}}{\\mathrm{d}t}}\\left(\\sigma_{t}^{-4}\\mathbb{E}\\left[\\operatorname{tr}\\Sigma_{t}\\right]\\right),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the posteriorcovariance matrix $\\pmb{\\Sigma}_{t}:=\\mathrm{cov}_{p_{0\\mid t}}(\\pmb{x}_{0})$ and $\\sigma_{t}^{2}=1\\!-\\!e^{-t}$ . Moreover, the posterior covariance matrix $\\Sigma_{t}$ satisfies ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\operatorname{tr}\\Sigma_{t}\\right]\\lesssim d\\wedge d\\sigma_{t}^{2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Lemma A.8. For any $n\\in[0:N-1]$ and $\\tau\\in[0,h_{n}],$ under the assumption $\\mathrm{cov}_{p_{0}}(\\pmb{x}_{0})=\\pmb{I}_{d}$ we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\lVert\\bar{\\pmb{x}}_{t_{n}}\\rVert^{2}\\right]\\leq2d,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\lVert\\bar{\\pmb{x}}_{t_{n}}-\\overleftarrow{\\pmb{x}}_{t_{n}+\\tau}\\right\\rVert^{2}\\right]\\leq3d.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. Conditioned on $\\pmb{x}_{0}$ , we have that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\ddot{\\pmb{x}}_{t_{n}}=\\pmb{x}_{T-t_{n}}\\sim\\mathcal{N}\\left(e^{-\\frac{1}{2}(T-t_{n})}\\pmb{x}_{0},(1-e^{-(T-t_{n})})\\pmb{I}_{d}\\right),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\ddot{x}_{t_{n}+\\tau}=x_{T-t_{n}-\\tau}\\sim{\\cal N}\\left(e^{-\\frac{1}{2}(T-t_{n}-\\tau)}x_{0},(1-e^{-(T-t_{n}-\\tau)})I_{d}\\right)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "for any $\\tau\\in[0,h_{n}]$ . Therefore, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\Vert\\bar{\\pmb{x}}_{t_{n}}\\Vert^{2}\\right]=\\mathbb{E}\\left[\\mathbb{E}\\left[\\Vert\\pmb{x}_{T-t_{n}}\\Vert^{2}\\middle|\\pmb{x}_{0}\\right]\\right]}\\\\ &{\\qquad\\qquad\\leq\\mathbb{E}\\left[\\mathbb{E}\\left[\\Vert\\pmb{x}_{T-t_{n}}-e^{-\\frac{1}{2}(T-t_{n})}\\pmb{x}_{0}\\Vert^{2}\\middle|\\pmb{x}_{0}\\right]+\\Vert e^{-\\frac{1}{2}(T-t_{n})}\\pmb{x}_{0}\\Vert^{2}\\right]}\\\\ &{\\qquad\\qquad\\leq d(1-e^{-(T-t_{n})})+e^{-(T-t_{n})}\\mathbb{E}[\\Vert\\pmb{x}_{0}\\Vert^{2}]\\leq2d.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Taking the difference between them then implies that for any $\\tau\\in[0,h_{n}]$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\Vert\\bar{\\mathbf{x}}_{t_{n}}-\\bar{\\mathbf{x}}_{t_{n}+\\tau}\\right\\Vert^{2}\\right]=\\mathbb{E}\\left[\\mathbb{E}\\left[\\left\\Vert\\mathbf{x}_{T-t_{n}}-\\mathbf{x}_{T-t_{n}-\\tau}\\right\\Vert^{2}|x_{0}\\right]\\right]}\\\\ &{\\qquad\\qquad\\qquad\\leq d\\big(2-e^{-(T-t_{n})}-e^{-(T-t_{n}-\\tau)}\\big)}\\\\ &{\\qquad\\qquad\\qquad+\\left(e^{-\\frac{1}{2}(T-t_{n})}-e^{-\\frac{1}{2}(T-t_{n}-\\tau)}\\right)^{2}\\mathbb{E}[\\left\\Vert x_{0}\\right\\Vert^{2}]}\\\\ &{\\qquad\\qquad\\leq2d+e^{-(T-t_{n}-\\tau)}(1-e^{-\\frac{1}{2}\\tau})^{2}\\mathbb{E}[\\left\\Vert x_{0}\\right\\Vert^{2}]\\leq3d.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Lemma A.9 (Lemma 9 in [95]). For $\\widehat{q}_{0}\\sim\\mathcal{N}(0,\\pmb{I}_{d})$ and $\\overleftarrow{p}_{0}=p_{T}$ is the distribution of the solution to the forward process (2.3), we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathrm{TV}(\\{\\overleftarrow{p}_{0},\\widehat{q}_{0}\\}^{2}\\leq D_{\\mathrm{KL}}(\\{\\overleftarrow{p}_{0}\\|\\widehat{q}_{0}\\}\\lesssim d e^{-T}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "B Details of SDE Implementation ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section, we will present the missing proofs for Theorem 3.3. For readers? convenience, we reiterate the backward process (2.3) ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathrm{d}\\ddot{\\pmb{x}}_{t}=\\left[\\frac{1}{2}\\ddot{\\pmb{x}}_{t}+\\nabla\\log\\ddot{p}_{t}(\\ddot{\\pmb{x}}_{t})\\right]\\mathrm{d}t+\\mathrm{d}\\pmb{w}_{t},\\quad\\mathrm{with}\\quad\\ddot{\\pmb{x}}_{0}\\sim p_{T},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and its approximate version (2.5) with the learned score function ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathrm{d}y_{t}=\\left[\\frac{1}{2}y_{t}+s_{t}^{\\theta}(y_{t})\\right]\\mathrm{d}t+\\mathrm{d}w_{t},\\quad\\mathrm{with}\\quad y_{0}\\sim\\mathcal{N}(0,I_{d}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The filtration $\\mathcal{F}_{t}$ refers to the filtration of the SDE (B.1) up to time $t$ ", "page_idx": 20}, {"type": "text", "text": "B.1 Auxiliary Process ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We would like first to consider the errors that Algorithm 1 may cause within one block of update. To this end, we consider the following auxiliary process for $\\tau\\in[0,h_{n}]$ conditioned on the filtration ${\\mathcal{F}}_{t_{n}}$ atime $t_{n}$ ", "page_idx": 21}, {"type": "text", "text": "Definition B.1 (Auxiliary Process). For any $n\\;\\in\\;[0\\;:\\;N\\;-\\;1],$ .we define the auxiliary process $(\\widehat{\\pmb{y}}_{t_{n},\\tau}^{(k)})_{\\tau\\in[0,h_{n}]}$ as thesolutionto theflowing SDErecwrsvel for $k\\in[0:K-1]$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathrm{d}\\widehat{y}_{t_{n},\\tau}^{(k+1)}(\\omega)=\\Bigg[\\frac{1}{2}\\widehat{y}_{t_{n},\\tau}^{(k+1)}(\\omega)+s_{t_{n}+g_{n}(\\tau)}^{\\theta}\\Big(\\widehat{y}_{t_{n},g_{n}(\\tau)}^{(k)}(\\omega)\\Big)\\Bigg]\\mathrm{d}\\tau+\\mathrm{d}w_{t_{n}+\\tau}(\\omega),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "with theinitialcondition ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\widehat{y}_{t_{n},\\tau}^{(0)}(\\omega)\\equiv\\widehat{y}_{t_{n}}(\\omega)\\,f o r\\,\\tau\\in[0,h_{n}],\\quad a n d\\quad\\widehat{y}_{t_{n},0}^{(k)}(\\omega)\\equiv\\widehat{y}_{t_{n}}(\\omega)\\,f o r\\,k\\in[1:K]}\\\\ &{}&{\\widehat{\\jmath}_{t_{n}}(\\omega)=\\widehat{y}_{t_{n-1},\\tau_{n-1},M_{n-1}}^{(K)}(\\omega)\\,i f n\\in[1:N-1]\\,a n d\\,\\widehat{y}_{t_{0}}(\\omega)\\sim\\mathcal{N}(0,I_{d}).\\qquad}\\end{array}\n$$where ? ", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The iteration should be perceived as a deterministic procedure to each event $\\omega\\ \\in\\ \\Omega$ ,i.e.each realization of the Wiener process $(\\pmb{w}_{t})_{t\\geq0}$ . The following lemma clarifies this fact and proves the well-definedness and parallelability of the iteration in (B.2). ", "page_idx": 21}, {"type": "text", "text": "Lemma B.2. The auxiliary process $(\\widehat{\\pmb{y}}_{t_{n},\\tau}^{(k)}(\\omega))_{\\tau\\in[0,h_{n}]}$ .s $\\mathcal{F}_{t_{n}+\\tau}$ -adapted for any $k\\,\\in\\,[0\\,:\\,K]$ and $n\\in[0:N-1]$ ", "page_idx": 21}, {"type": "text", "text": "Proof. Since the initialization $\\widehat{\\pmb{y}}_{t_{n},\\tau}^{(0)}(\\omega)\\equiv\\widehat{\\pmb{y}}_{t_{n}}(\\omega)$ for $\\tau\\in[0,h_{n}]$ where $\\widehat{\\pmb{y}}_{t_{n}}(\\omega)$ .s ${\\mathcal{F}}_{t_{n}}$ -adapted, it is obvious that $\\widehat{\\pmb{y}}_{t_{n},\\tau}^{(0)}(\\omega)$ .s $\\mathcal{F}_{t_{n}+\\tau}$ -adapted. Now suppose that $(\\widehat{\\pmb{y}}_{t_{n},\\tau}^{(k)}(\\omega))_{\\tau\\in[0,h_{n}]}$ .s $\\mathcal{F}_{t_{n}+\\tau}$ -adapted, since $g_{n}(\\tau)\\leq\\tau$ , we have the following Ito integral well-defined and $\\mathcal{F}_{t_{n}+\\tau}$ -adapted: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\int_{0}^{\\tau}\\mathbf{s}_{t_{n}+g_{n}(\\tau^{\\prime})}^{\\theta}\\Big(\\widehat{\\pmb{y}}_{t_{n},g_{n}(\\tau^{\\prime})}^{(k)}(\\omega)\\Big)\\mathrm{d}\\tau^{\\prime},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and therefore (B.2) has a unique strong solution $(\\widehat{\\pmb{y}}_{t_{n},\\tau}^{(k+1)}(\\omega))_{\\tau\\in[0,h_{n}]}$ that is also ${\\mathcal{F}}_{t_{n}+\\tau}$ -adapted. The lemma follows by induction. \u53e3 ", "page_idx": 21}, {"type": "text", "text": "Lemma B.3 (Equivalence between (3.4) and (B.2). For any $n\\in[0:N\\!-\\!1],$ the update rule (3.4) in Algorithm $^{\\,I}$ is equivalent to the exact solution of the auxiliary process $(B.2)$ forany $k\\in[0:K-1]$ and $\\tau\\in[0,h_{n}]$ ", "page_idx": 21}, {"type": "text", "text": "Proof. The dependency on $\\omega$ will be omitted in the proof below. ", "page_idx": 21}, {"type": "text", "text": "Rewriting (B.2) and multiplying $e^{-\\frac{\\tau}{2}}$ on both sides yield ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathrm{d}\\left[e^{-\\frac{\\tau}{2}}\\widehat{y}_{t_{n},\\tau}^{(k+1)}\\right]=e^{-\\frac{\\tau}{2}}\\left[\\mathrm{d}\\widehat{y}_{t_{n},\\tau}^{(k+1)}-\\frac{1}{2}\\widehat{y}_{t_{n},\\tau}^{(k+1)}\\mathrm{d}\\tau\\right]=e^{-\\frac{\\tau}{2}}\\left[s_{t_{n}+g_{n}(\\tau)}^{\\theta}\\Big(\\widehat{y}_{t_{n},g_{n}(\\tau)}^{(k)}\\Big)\\mathrm{d}\\tau+\\mathrm{d}w_{t_{n}+\\tau}\\right].\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Integrating on both sides from O to $\\tau$ implies ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\cdots\\frac{\\tau}{2}\\hat{y}_{t_{n},\\tau}^{(k+1)}-\\hat{y}_{t_{n},0}^{(k+1)}=\\int_{0}^{\\tau}e^{-\\frac{\\tau^{\\prime}}{2}}\\Bigg(s_{t_{n}+g_{n}(\\tau^{\\prime})}^{\\theta}\\Big(\\hat{y}_{t_{n},g_{n}(\\tau^{\\prime})}^{(k)}\\Big)\\mathrm{d}\\tau^{\\prime}+\\mathrm{d}w_{t_{n}+\\tau^{\\prime}}\\Bigg)}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\sum_{m=0}^{M_{n}}\\int_{\\tau\\wedge t_{n},m}^{\\tau\\wedge\\tau_{n,m+1}}e^{-\\frac{\\tau^{\\prime}}{2}}s_{t_{n}+\\tau_{n,m}}^{\\theta}\\Big(\\hat{y}_{t_{n},\\tau_{n,m}}^{(k)}\\Big)\\mathrm{d}\\tau^{\\prime}+\\int_{0}^{\\tau}e^{-\\frac{\\tau^{\\prime}}{2}}\\mathrm{d}w_{t_{n}+\\tau^{\\prime}}}\\\\ &{\\qquad\\qquad\\qquad=\\displaystyle\\sum_{m=0}^{M_{n}}2\\left(e^{-\\frac{\\tau\\wedge\\tau_{n,m}}{2}}-e^{-\\frac{\\tau\\wedge\\tau_{n,m+1}}{2}}\\right)s_{t_{n}+\\tau_{n,j}}^{\\theta}\\Big(\\hat{y}_{t_{n},\\tau_{n,m}}^{(k)}\\Big)+\\int_{0}^{\\tau}e^{-\\frac{\\tau^{\\prime}}{2}}\\mathrm{d}w_{t_{n}+\\tau^{\\prime}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and then multiplying $e^{\\frac{\\tau}{2}}$ on both sides above yields ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{y}_{t_{n},\\tau}^{(k+1)}=e^{\\frac{\\tau}{2}}\\widehat{y}_{t_{n},0}^{(k+1)}+\\displaystyle\\sum_{m=0}^{M_{n}}2\\left(e^{\\frac{\\tau\\wedge\\tau_{n,m+1}-\\tau\\wedge\\tau_{n,m}}{2}}-1\\right)e^{\\frac{0\\vee(\\tau-\\tau_{n,m+1})}{2}}s_{t_{n}+\\tau_{n,m}}^{\\theta}\\Big(\\widehat{y}_{t_{n},\\tau_{n,m}}^{(k)}\\Big)}\\\\ &{\\qquad+\\displaystyle\\sum_{m=0}^{M_{n}}\\int_{\\tau\\wedge\\tau_{n,m}}^{\\tau\\wedge\\tau_{n,m+1}}e^{\\frac{\\tau-\\tau^{\\prime}}{2}}\\mathrm{d}w_{t_{n}+\\tau^{\\prime}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where, by Ito isometry, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\int_{\\tau\\wedge\\tau_{n,m}}^{\\tau\\wedge\\tau_{n,m+1}}e^{\\frac{\\tau-\\tau^{\\prime}}{2}}\\mathrm{d}w_{t_{n}+\\tau^{\\prime}}\\sim\\mathcal{N}\\left(\\mathbf{0},\\left(e^{\\tau\\wedge\\tau_{n,m+1}-\\tau\\wedge\\tau_{n,m}}-1\\right)e^{0\\vee(\\tau-\\tau_{n,m+1})}I_{d}\\right)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "for $\\tau>\\tau_{n,m}$ and equals to O otherwise. Plugging in $\\tau=\\tau_{j,m}$ gives us (3.4), as desired. ", "page_idx": 22}, {"type": "text", "text": "B.2 Errors within Block ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We shall invoke Girsanov's theorem (Theorem A.4) in the procedure as detailed below: ", "page_idx": 22}, {"type": "text", "text": "1. Setting (A.2) in Theorem A.4 as the auxiliary process (B.2) at iteration $K$ where $\\pmb{w}_{t}(\\omega)$ is a Wiener process under the measure $q\\vert_{\\mathcal{F}_{t_{n}}}$ ", "page_idx": 22}, {"type": "text", "text": "2. Defining another process $\\widetilde{\\pmb{w}}_{t_{n}+\\tau}(\\omega)$ governed by the following SDE: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathrm{d}\\widetilde{\\pmb{w}}_{t_{n}+\\tau}(\\omega)=\\mathrm{d}\\pmb{w}_{t_{n}+\\tau}(\\omega)+\\delta_{t_{n}}(\\tau,\\omega)d\\tau,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\delta_{t_{n}}(\\tau,\\omega):=s_{t_{n}+g_{n}(\\tau)}^{\\theta}(\\widehat{y}_{t_{n},g_{n}(\\tau)}^{(K-1)}(\\omega))-\\nabla\\log\\overleftarrow{p}_{t_{n}+\\tau}(\\widehat{y}_{t_{n}+\\tau}^{(K)}(\\omega)),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and computing the Radon-Nikodym derivative of the measure $\\left\\{\\left.\\overline{{p}}\\right|_{\\mathcal{F}_{t_{n}}}\\right.$ with respect to $q\\vert_{\\mathcal{F}_{t_{n}}}$ as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}\\tilde{p}|_{\\mathcal{F}_{t_{n}}}}{\\mathrm{d}q|_{\\mathcal{F}_{t_{n}}}}(\\omega):=\\exp\\left(-\\int_{0}^{h_{n}}\\delta_{t_{n}}(\\tau,\\omega)^{\\top}\\mathrm{d}w_{t_{n}+\\tau}(\\omega)-\\frac{1}{2}\\int_{0}^{h_{n}}\\|\\delta_{t_{n}}(\\tau,\\omega)\\|^{2}\\mathrm{d}\\tau\\right),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "3. Concluding that (B.2) at iteration $K$ under the measure $q\\vert_{\\mathcal{F}_{t_{n}}}$ satisfies the following SDE: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathrm{d}\\widehat{y}_{t_{n},\\tau}^{(K)}(\\omega)=\\left[\\frac{1}{2}\\widehat{y}_{t_{n},\\tau}^{(K)}(\\omega)+\\nabla\\log\\breve{p}_{t_{n}+\\tau}\\big(\\widehat{y}_{t_{n},\\tau}^{(K)}(\\omega)\\big)\\right]\\mathrm{d}\\tau+\\mathrm{d}\\widetilde{w}_{t_{n}+\\tau}(\\omega),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "wih $(\\widetilde{\\pmb{w}}_{t_{n}+\\tau})_{\\tau\\geq0}$ bngaWce $\\left\\langle\\bar{p}\\right|_{\\mathcal{F}_{t_{n}}}$ If we replaece $\\widehat{\\pmb{y}}_{t_{n},g_{n}(\\tau)}^{(K)}(\\omega)$ by $\\overleftarrow{\\pmb{x}}_{t_{n}+\\tau}(\\omega)$ , one should notice (B.5) is immediately the original backward SDE (2.3) with the true score function on $t\\in[t_{n},t_{n+1}]$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathrm{d}\\ddot{x}_{t_{n}+\\tau}(\\omega)=\\left[\\frac{1}{2}\\ddot{x}_{t_{n}+\\tau}(\\omega)+\\nabla\\log\\ddot{p}_{t_{n}+\\tau}(\\ddot{x}_{t_{n}+\\tau}(\\omega))\\right]\\mathrm{d}\\tau+\\mathrm{d}\\widetilde{w}_{t_{n}+\\tau}(\\omega).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Remark B.4. The applicability of Girsanov's theorem here relies on the $\\mathcal{F}_{\\tau}$ -adaptivity of $\\pmb{s}_{t_{n}+g_{n}(\\tau)}^{\\theta}\\Big(\\widehat{\\pmb{y}}_{t_{n},g_{n}(\\tau)}^{(K-1)}(\\omega)\\Big)$ established by Lemma B.2. One should notice the change of measure procedure above depends on the number of iterations $K$ .and different $K$ would lead to different transform $(B.4)$ ", "page_idx": 22}, {"type": "text", "text": "Then Corollary A.5 provides the following computation ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D_{\\mathrm{KL}}(\\widetilde{p}_{t_{n+1}}\\|\\widehat{q}_{t_{n+1}})\\leq D_{\\mathrm{KL}}(\\widetilde{p}_{t_{n}:t_{n+1}}\\|\\widehat{q}_{t_{n}:t_{n+1}})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=D_{\\mathrm{KL}}(\\widetilde{p}_{t_{n}}\\|\\widehat{q}_{t_{n}})+\\mathbb{E}_{\\omega\\sim q|\\mathcal{F}_{t_{n}}}\\left[\\frac{1}{2}\\int_{0}^{h_{n}}\\|\\delta_{t_{n}}(\\tau,\\omega)\\|^{2}\\mathrm{d}\\tau\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the first inequality is by the data-processing inequality (Theorem A.1). Now, the problem remaining is to bound the discrepancy quantified by ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\int_{0}^{h_{n}}\\lVert\\delta_{t_{n}}(\\tau,\\omega)\\rVert^{2}\\,\\mathrm{d}\\tau}\\\\ &{=\\displaystyle\\int_{0}^{h_{n}}\\Big\\lVert\\delta_{t_{n}+\\hat{\\tau}_{n}(\\tau)}^{\\theta}(\\hat{y}_{t_{n},\\partial_{t}(\\tau)}^{(K-1)}(\\omega))-\\nabla\\log\\tilde{p}_{t_{n}+\\tau}(\\hat{y}_{t_{n},\\tau}^{(K)}(\\omega))\\Big\\rVert^{2}\\,\\mathrm{d}\\tau}\\\\ &{\\le3\\left(\\displaystyle\\int_{0}^{h_{n}}\\Big\\lVert\\nabla\\log\\tilde{p}_{t_{n}+\\hat{\\tau}_{n}(\\tau)}(\\hat{y}_{t_{n},\\partial_{t}(\\tau)}^{(K)}(\\omega))-\\nabla\\log\\tilde{p}_{t_{n}+\\tau}(\\hat{y}_{t_{n},\\tau}^{(K)}(\\omega))\\Big\\rVert^{2}\\,\\mathrm{d}\\tau\\right.}\\\\ &{\\quad\\displaystyle\\left.+\\underbrace{\\int_{0}^{h_{n}}\\Big\\lVert\\delta_{t_{n}+\\hat{\\tau}_{n}(\\tau)}^{\\theta}(\\hat{y}_{t_{n},\\partial_{t}(\\tau)}^{(K)}(\\omega))-\\nabla\\log\\tilde{p}_{t_{n}+\\hat{\\tau}_{n}(\\tau)}(\\hat{y}_{t_{n},\\partial_{t}(\\tau)}^{(K)}(\\omega))\\Big\\rVert^{2}\\,\\mathrm{d}\\tau}_{:a_{h_{n}}(\\omega)}\\right.}\\\\ &{\\quad\\displaystyle\\left.+\\int_{0}^{h_{n}}\\Big\\lVert\\delta_{t_{n}+\\hat{\\tau}_{n}(\\tau)}^{\\theta}(\\hat{y}_{t_{n},\\partial_{t}(\\tau)}^{(K)}(\\omega))-s_{h_{n},\\hat{\\tau}_{n}}^{\\theta}(\\hat{y}_{t_{n},\\partial_{t}(\\tau)}^{(K-1)}(\\omega))\\Big\\rVert^{2}\\,\\mathrm{d}\\tau\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Before we continue our proof, we would like first to provide the following lemma bounding the behavior of the auxiliary process (B.2) when $k=0$ for $\\tau\\in[0,h_{n}]$ ", "page_idx": 23}, {"type": "text", "text": "Lemma B.5. For any $n\\,\\in\\,[0\\,:\\,N\\,-\\,1]$ .suppose the initialization $\\widehat{\\pmb{y}}_{t_{n}}$ in $(B.3)$ of the auxiliary process $(B.2)$ follows the distribution of $\\overleftarrow{\\pmb{x}}_{t_{n}}\\sim\\overleftarrow{\\b{p}}_{t_{n}}$ , then the following estimate holds ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\underset{\\tau\\in[0,h_{n}]}{\\operatorname*{sup}}\\mathbb{E}_{\\omega\\sim\\tilde{p}|\\mathcal{F}_{t_{n}}}\\left[\\|\\widehat{\\pmb{y}}_{t_{n},\\tau}^{(1)}(\\omega)-\\widehat{\\pmb{y}}_{t_{n},\\tau}^{(0)}(\\omega)\\|^{2}\\right]}\\\\ &{\\leq h_{n}e^{\\frac{7}{2}h_{n}}\\left(M_{s}^{2}+2d\\right)+3e^{\\frac{7}{2}h_{n}}\\mathbb{E}_{\\omega\\sim\\tilde{p}|\\mathcal{F}_{t_{n}}}\\left[A_{t_{n}}(\\omega)+B_{t_{n}}(\\omega)\\right]}\\\\ &{+\\,3e^{\\frac{7}{2}h_{n}}h_{n}L_{s}^{2}\\underset{\\tau\\in[0,h_{n}]}{\\operatorname*{sup}}\\mathbb{E}_{\\omega\\sim\\tilde{p}|\\mathcal{F}_{t_{n}}}\\left[\\left\\|\\widehat{\\pmb{y}}_{t_{n},\\tau}^{(K)}(\\omega)-\\widehat{\\pmb{y}}_{t_{n},\\tau}^{(K-1)}(\\omega)\\right\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. Let Ztn,T = Ytn,T $\\pmb{z}_{t_{n},\\tau}=\\widehat{\\pmb{y}}_{t_{n},\\tau}^{(1)}-\\widehat{\\pmb{y}}_{t_{n},\\tau}^{(0)}$ y(). For k = 0, we can rewrite (B.2) as ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathrm{d}z_{t_{n},\\tau}=\\Bigg[\\frac{1}{2}\\left(z_{t_{n},\\tau}+\\widehat{y}_{t_{n},\\tau}^{(0)}\\right)+s_{t_{n}+g_{n}(\\tau)}^{\\theta}\\Big(\\widehat{y}_{t_{n},g_{n}(\\tau)}^{(0)}\\Big)\\Bigg]\\mathrm{d}\\tau+\\mathrm{d}w_{t_{n}+\\tau},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "By applying Ito's lemma and plugging in the expression of ${\\pmb w}_{t_{n}+\\tau}$ given by Theorem A.4, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{d}\\|\\boldsymbol{z}_{t_{n},\\tau}\\|^{2}=\\!\\!\\left[\\|\\boldsymbol{z}_{t_{n},\\tau}\\|^{2}+\\boldsymbol{z}_{t_{n},\\tau}^{\\top}\\hat{y}_{t_{n},\\tau}^{(0)}+2\\boldsymbol{z}_{t_{n},\\tau}^{\\top}\\boldsymbol{s}_{t_{n}+g_{n}(\\tau)}^{\\theta}\\left(\\widehat{y}_{t_{n},g_{n}(\\tau)}^{(0)}\\right)+d\\right]\\mathrm{d}\\tau}\\\\ &{\\phantom{\\mathrm{d}}+2\\boldsymbol{z}_{t_{n},\\tau}^{\\top}\\!\\left(\\mathrm{d}\\widetilde{\\boldsymbol{w}}_{t_{n}+\\tau}(\\omega)-\\delta_{t_{n}}(\\tau,\\omega)d\\tau\\right)\\!,}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "By integrating from O to $\\tau$ and taking the expectation on both sides of (B.9), we obtain that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{\\omega\\sim\\widetilde{p}\\mid\\mathcal{F}_{t_{n}}}\\left[\\Vert z_{t_{n},\\tau}\\Vert^{2}\\right]}\\\\ &{=\\mathbb{E}_{\\omega\\sim\\widetilde{p}\\mid\\mathcal{F}_{t_{n}}}\\left[\\int_{0}^{\\tau}\\Bigg(\\Vert z_{t_{n},\\tau^{\\prime}}\\Vert^{2}+z_{t_{n},\\tau^{\\prime}}^{\\top}\\hat{y}_{t_{n},\\tau^{\\prime}}^{(0)}+2z_{t_{n},\\tau^{\\prime}}^{\\top}s_{t_{n}+g_{n}(\\tau^{\\prime})}^{\\theta}\\Big(\\hat{y}_{t_{n},g_{n}(\\tau^{\\prime})}^{(0)}\\Big)+d\\Bigg)\\mathrm{d}\\tau^{\\prime}\\right]}\\\\ &{+\\;2\\mathbb{E}_{\\omega\\sim\\widetilde{p}\\mid\\mathcal{F}_{t_{n}}}\\left[\\int_{0}^{\\tau}z_{t_{n},\\tau^{\\prime}}^{\\top}\\Big(\\mathrm{d}\\tilde{w}_{t_{n}+\\tau^{\\prime}}(\\omega)-\\delta_{t_{n}}(\\tau^{\\prime},\\omega)\\mathrm{d}\\tau^{\\prime}\\Big)\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and by AM-GM, we further have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{\\omega\\sim\\widetilde{p}\\mid x_{t_{n}}}\\left[\\Vert z_{t_{n},\\tau}\\Vert^{2}\\right]}\\\\ &{\\le\\mathbb{E}_{\\omega\\sim\\widetilde{p}\\mid x_{t_{n}}}\\left[\\int_{0}^{\\tau}\\left[\\frac{7}{2}\\Vert z_{t_{n},\\tau^{\\prime}}\\Vert^{2}+\\frac{1}{2}\\left\\Vert\\hat{y}_{t_{n},\\tau^{\\prime}}^{(0)}\\right\\Vert^{2}+\\left\\Vert s_{t_{n}+g_{n}(\\tau^{\\prime})}^{\\theta}\\left(\\hat{y}_{t_{n},g_{n}(\\tau^{\\prime})}^{(0)}\\right)\\right\\Vert^{2}+d+\\Vert\\delta_{t_{n}}(\\tau,\\omega)\\Vert^{2}\\right]\\mathrm{d}\\tau^{\\prime}\\right.}\\\\ &{\\left.\\le\\int_{0}^{\\tau}\\mathbb{E}_{\\omega\\sim\\widetilde{p}\\mid x_{t_{n}}}\\left[\\frac{7}{2}\\Vert z_{t_{n},\\tau^{\\prime}}\\Vert^{2}+\\Vert\\delta_{t_{n}}(\\tau,\\omega)\\Vert^{2}\\right]\\mathrm{d}\\tau^{\\prime}+\\left(\\frac{1}{2}\\mathbb{E}\\left[\\left\\Vert\\hat{y}_{t_{n},\\tau}^{(0)}\\right\\Vert^{2}\\right]+M_{s}^{2}+d\\right)\\tau,}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\delta_{t_{n}}(\\tau,\\omega)$ is defined in (B.4). Similar to (B.7), we may use triangle inequality to upper bound $\\lVert\\delta_{t_{n}}(\\tau,\\dot{\\omega})\\rVert^{2}$ , which implies that for any $\\tau\\in[0,h_{n}]$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{\\omega\\sim\\hat{\\mathbb{P}}|\\mathcal{E}_{n}}\\left[\\big\\|\\mathcal{E}_{n,\\cdot}\\big\\eta\\big[\\big\\|\\mathcal{Z}_{n}\\big(\\cdot\\big(\\frac{1}{\\cdot}\\big)\\big)\\in\\mathcal{Z}\\right]+\\bigg(\\frac{1}{2}\\mathbb{E}\\bigg[\\bigg\\|\\bar{\\mathcal{F}}_{[n,\\cdot]}\\bigg\\|^{2}\\bigg]+M_{n}^{2}+d\\bigg)\\tau}\\\\ &{\\leq\\frac{7}{2}\\int_{0}^{\\tau}\\mathbb{E}_{\\omega\\sim\\hat{\\mathbb{P}}|\\mathcal{E}_{n}}\\left[\\bigg\\|\\mathcal{Z}_{n}^{\\tau}\\Big\\|^{2}\\,\\mathrm{d}\\tau^{\\prime}+\\bigg(\\frac{1}{2}\\mathbb{E}\\bigg[\\bigg\\|\\bar{\\mathcal{F}}_{n,\\cdot}^{(1)}\\bigg\\|^{2}\\bigg]+M_{n}^{2}+\\bigg)\\tau\\right.}\\\\ &{\\left.\\quad+3\\mathbb{E}_{\\omega\\sim\\hat{\\mathbb{P}}|\\mathcal{E}_{n}}\\left[\\int_{0}^{\\tau}\\bigg\\|\\bar{\\mathcal{E}}_{n_{n}+\\hat{\\mathbb{P}}_{n}(\\tau)}^{\\tau}(\\hat{\\mathbb{P}}_{n}^{(K-1)})(\\omega)-\\mathcal{E}_{n_{n}+\\hat{\\mathbb{P}}_{n}(\\tau)}^{\\tau}(\\hat{\\mathbb{P}}_{n_{n},\\cdot}^{(K)}(\\omega))\\bigg\\|^{2}\\,\\mathrm{d}\\tau^{\\prime}\\right]\\right.}\\\\ &{\\left.\\quad+3\\mathbb{E}_{\\omega\\sim\\hat{\\mathbb{P}}|\\mathcal{E}_{n}}\\left[\\int_{0}^{\\tau}\\bigg\\|\\bar{\\mathcal{E}}_{n_{n}+\\hat{\\mathbb{P}}_{n}(\\tau)}^{\\tau}(\\hat{\\mathbb{P}}_{n_{n},\\cdot}^{(K)}(\\omega))-\\nabla\\log\\bar{\\mathcal{F}}_{n_{n},\\cdot}\\mu_{\\hat{\\mathbb{P}}_{n}(\\tau)}(\\hat{\\mathbb{P}}_{n_{n},\\cdot}^{(K)}(\\omega))\\bigg\\|^{2}\\,\\mathrm{d}\\tau^{\\prime}\\right]}\\\\ &{+3\\mathbb{E}_{\\omega\\sim\\hat{\\mathbb{P}}|\\mathcal{E}_{n}}\\left[\\int_{0}^{\\tau}\\bigg\\|\\nabla\\log\\bar{\\mathcal{F}}_{n_{n}+\\hat{\\mathbb{P}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where in the second inequality above, we have used the fact that $s_{t}^{\\theta}(\\cdot)$ is $L_{s}$ -Lipschitz for any $t$ .By Gronwall's inequality, we have that for any $\\tau\\in[0,h_{n}]$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\omega\\sim\\widetilde{p}\\mid x_{t_{n}}}\\left[\\Vert z_{t_{n},\\tau}\\Vert^{2}\\right]\\leq e^{\\frac{\\tau}{2}\\tau}\\left[\\left(\\frac{1}{2}\\mathbb{E}\\left[\\left\\Vert\\hat{y}_{t_{n},\\tau}^{(0)}\\right\\Vert^{2}\\right]+M_{s}^{2}+d\\right)\\tau\\right]}\\\\ &{\\qquad\\qquad\\qquad+3e^{\\frac{\\tau}{2}\\tau}\\mathbb{E}_{\\omega\\sim\\widetilde{p}\\mid x_{t_{n}}}\\left[A_{t_{n}}(\\omega)+B_{t_{n}}(\\omega)\\right]}\\\\ &{\\qquad\\qquad\\qquad+3e^{\\frac{\\tau}{2}\\tau}L_{s}^{2}\\int_{0}^{\\tau}\\mathbb{E}_{\\omega\\sim\\widetilde{p}\\mid x_{t_{n}}}\\left[\\left\\Vert\\hat{y}_{t_{n},g_{n}(\\tau^{\\prime})}^{(K)}(\\omega)-\\hat{y}_{t_{n},g_{n}(\\tau^{\\prime})}^{(K-1)}(\\omega)\\right\\Vert^{2}\\right]\\mathrm{d}\\tau^{\\prime}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "By assumption, $\\widehat{\\pmb{y}}_{t_{n},\\tau}^{(0)}=\\widehat{\\pmb{y}}_{t_{n}}$ follows the disribution of $\\overleftarrow{\\pmb{x}}_{t_{n}}\\sim\\overleftarrow{p}_{t_{n}}$ which allows us tobound the second moment of $\\widehat{\\pmb{y}}_{t_{n}}$ for any $n\\in[0:N]$ by Lemma A.8: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\|\\widehat{\\pmb{y}}_{t_{n}}\\|^{2}\\right]=\\mathbb{E}\\left[\\|\\bar{\\pmb{x}}_{t_{n}}\\|^{2}\\right]\\leq2d.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Substituting (A.5) into (B.10) then yields that for any $\\tau\\in[0,h_{n}]$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\omega\\sim q\\mid\\mathcal{F}_{t_{n}}}\\left[\\|z_{t_{n},\\tau}\\|^{2}\\right]\\leq\\tau e^{\\frac{\\tau}{2}\\tau}\\left(M_{s}^{2}+2d\\right)+3e^{\\frac{\\tau}{2}\\tau}\\mathbb{E}_{\\omega\\sim\\tilde{p}\\mid\\mathcal{F}_{t_{n}}}\\left[A_{t_{n}}(\\omega)+B_{t_{n}}(\\omega)\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\,3\\tau e^{\\frac{\\tau}{2}\\tau}L_{s}^{2}\\underset{\\tau^{\\prime}\\in[0,h_{n}]}{\\operatorname*{sup}}\\mathbb{E}_{\\omega\\sim\\tilde{p}\\mid\\mathcal{F}_{t_{n}}}\\left[\\left\\|\\widehat{\\pmb{y}}_{t_{n},\\tau^{\\prime}}^{(K)}(\\omega)-\\widehat{\\pmb{y}}_{t_{n},\\tau^{\\prime}}^{(K-1)}(\\omega)\\right\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Taking supremum with respect to $\\tau\\in[0,h_{n}]$ on both sides above completes our proof. ", "page_idx": 24}, {"type": "text", "text": "As utilized in the proof of the existence of solutions of SDEs, the following lemma demonstrates the exponential convergence of the iteration defined in (B.2). ", "page_idx": 24}, {"type": "text", "text": "Lemma B.6 (Exponential convergence of Picard iteration in PIADM-SDE). For any $n\\in[0,N]$ suppose the initialization $\\widehat{\\pmb{y}}_{t_{n}}$ .n $(B.3)$ of the auxiliary process $(B.2)$ followsthedistributionof $\\overleftarrow{\\pmb{x}}_{t_{n}}\\sim\\overleftarrow{\\boldsymbol{p}}_{t_{n}}$ \uff0c then the two ending terms yin,T and ytn,T $\\widehat{\\pmb{y}}_{t_{n},\\tau}^{(K-1)}$ $\\{\\widehat{\\pmb{y}}_{t_{n},\\tau}^{(k)}\\}_{k\\in[0:K-1]}$ exponentialconvergencerate ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\underset{\\tau\\in[0,h_{n}]}{\\operatorname*{sup}}\\mathbb{E}_{\\omega\\sim\\tilde{p}|x_{t_{n}}}\\left[\\left\\|\\tilde{y}_{t_{n},\\tau}^{(K)}(\\omega)-\\hat{y}_{t_{n},\\tau}^{(K-1)}(\\omega)\\right\\|_{2}^{2}\\right]}\\\\ &{\\leq\\frac{\\left(L_{s}^{2}h_{n}e^{2h_{n}}\\right)^{K-1}h e^{\\frac{7}{2}h_{n}}\\,\\left(M_{s}^{2}+2d\\right)}{1-3\\,(L_{s}^{2}h_{n}e^{2h_{n}})^{K-1}e^{\\frac{7}{2}h_{n}}h_{n}L_{s}^{2}}+\\frac{3\\,\\left(L_{s}^{2}h_{n}e^{2h_{n}}\\right)^{K-1}e^{\\frac{7}{2}h_{n}}\\mathbb{E}_{\\omega\\sim\\tilde{p}|x_{t_{n}}}\\,\\left[A_{t_{n}}(\\omega)+B_{t_{n}}(\\omega)\\right]}{1-3\\,(L_{s}^{2}h_{n}e^{2h_{n}})^{K-1}e^{\\frac{7}{2}h_{n}}h_{n}L_{s}^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. For each $\\omega\\,\\in\\,\\Omega$ conditioned on the filtration ${\\mathcal{F}}_{t_{n}}$ , subtracting (B.2) from the process as defined by ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathrm{d}\\widehat{y}_{t_{n},\\tau}^{(k)}(\\omega)=\\left[\\frac{1}{2}\\widehat{y}_{t_{n},\\tau}^{(k)}(\\omega)+s_{t_{n}+g_{n}(\\tau)}^{\\theta}\\big(\\widehat{y}_{t_{n},g_{n}(\\tau)}^{(k-1)}(\\omega)\\big)\\right]\\mathrm{d}\\tau+\\mathrm{d}w_{t_{n}+\\tau}(\\omega),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\textrm{d}\\Big(\\widehat{y}_{t_{n},\\tau}^{(k+1)}(\\omega)-\\widehat{y}_{t_{n},\\tau}^{(k)}(\\omega)\\Big)}\\\\ &{=\\left[\\frac{1}{2}\\left(\\widehat{y}_{t_{n},\\tau}^{(k+1)}(\\omega)-\\widehat{y}_{t_{n},\\tau}^{(k)}(\\omega)\\right)+s_{t_{n}+g_{n}(\\tau)}^{\\theta}\\big(\\widehat{y}_{t_{n},g_{n}(\\tau)}^{(k)}(\\omega)\\big)-s_{t_{n}+g_{n}(\\tau)}^{\\theta}\\big(\\widehat{y}_{t_{n},g_{n}(\\tau)}^{(k-1)}(\\omega)\\big)\\right]\\mathrm{d}\\tau,}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the diffusion term $\\mathrm{d}\\pmb{w}_{t_{n}+\\tau}$ cancels each other out. Now we may use the formula above to compute derivative $\\begin{array}{r}{\\frac{\\mathrm{d}}{\\mathrm{d}\\tau^{\\prime}}\\left\\|\\widehat{\\pmb{y}}_{t_{n},\\tau^{\\prime}}^{(k+1)}(\\omega)-\\widehat{\\pmb{y}}_{t_{n},\\tau^{\\prime}}^{(k)}(\\omega)\\right\\|^{2}}\\end{array}$ explicitly, integrate it from $\\tau^{\\prime}\\,=\\,0$ to $\\tau$ , and obtain the following inequality ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\|\\hat{y}_{t_{n},\\tau}^{(k+1)}(\\omega)-\\hat{y}_{t_{n},\\tau}^{(k)}(\\omega)\\right\\|^{2}}\\\\ &{=\\int_{0}^{\\tau}2\\left(\\hat{y}_{t_{n},\\tau^{\\prime}}^{(k+1)}(\\omega)-\\hat{y}_{t_{n},\\tau^{\\prime}}^{(k)}(\\omega)\\right)^{\\top}\\left(s_{t_{n}+\\hat{y}_{n}(\\tau^{\\prime})}^{\\theta}(\\hat{y}_{t_{n},\\hat{y}_{n}(\\tau^{\\prime})}^{(k)}(\\omega))-s_{t_{n}+\\hat{y}_{n}(\\tau^{\\prime})}^{\\theta}(\\hat{y}_{t_{n},\\hat{y}_{n}(\\tau^{\\prime})}^{(k-1)}(\\omega))\\right)\\mathrm{d}\\tau}\\\\ &{+\\int_{0}^{\\tau}\\left\\|\\hat{y}_{t_{n},\\tau^{\\prime}}^{(k+1)}(\\omega)-\\hat{y}_{t_{n},\\tau^{\\prime}}^{(k)}(\\omega)\\right\\|^{2}\\mathrm{d}\\tau^{\\prime}}\\\\ &{\\leq2\\int_{0}^{\\tau}\\left\\|\\hat{y}_{t_{n},\\tau^{\\prime}}^{(k+1)}(\\omega)-\\hat{y}_{t_{n},\\tau^{\\prime}}^{(k)}(\\omega)\\right\\|^{2}\\mathrm{d}\\tau^{\\prime}}\\\\ &{+\\int_{0}^{\\tau}\\left\\|s_{t_{n}+\\hat{y}_{n}(\\tau^{\\prime})}^{\\theta}(\\hat{y}_{t_{n},\\hat{y}_{n}(\\tau^{\\prime})}^{(k)}(\\omega))-s_{t_{n}+\\hat{y}_{n}(\\tau^{\\prime})}^{\\theta}(\\hat{y}_{t_{n},\\hat{y}_{n}(\\tau^{\\prime})}^{(k-1)}(\\omega))\\right\\|^{2}\\mathrm{d}\\tau^{\\prime}}\\\\ &{\\leq2\\int_{0}^{\\tau}\\left\\|\\hat{y}_{t_{n},\\tau^{\\prime}}^{(k+1)}(\\omega)-\\hat{y}_{t_{n},\\tau^{\\prime}}^{(k)}(\\omega)\\right\\|^{2}\\mathrm{d}\\tau^{\\prime}+L_{s}^{2}\\int_{0}^{\\tau}\\left\\|\\hat{y} \n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "By Gronwall's inequality, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left\\|\\widehat{y}_{t_{n},\\tau}^{(k+1)}(\\omega)-\\widehat{y}_{t_{n},\\tau}^{(k)}(\\omega)\\right\\|^{2}\\leq L_{s}^{2}e^{2\\tau}\\int_{0}^{\\tau}\\left\\|\\widehat{y}_{t_{n},g_{n}(\\tau^{\\prime})}^{(k)}(\\omega)-\\widehat{y}_{t_{n},g_{n}(\\tau^{\\prime})}^{(k-1)}(\\omega)\\right\\|^{2}\\mathrm{d}\\tau^{\\prime}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Taking expectation on both sides above further implies that for any $\\tau\\in[0,h_{n}]$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{\\omega\\sim\\breve{p}\\mid\\mathcal{F}_{t_{n}}}\\left[\\left\\|\\widehat{\\pmb{y}}_{t_{n},\\tau}^{(k+1)}(\\omega)-\\widehat{\\pmb{y}}_{t_{n},\\tau}^{(k)}(\\omega)\\right\\|^{2}\\right]}\\\\ &{\\leq L_{s}^{2}e^{2\\tau}\\displaystyle\\int_{0}^{\\tau}\\mathbb{E}_{\\omega\\sim\\breve{p}\\mid\\mathcal{F}_{t_{n}}}\\left[\\left\\|\\widehat{\\pmb{y}}_{t_{n},g_{n}(\\tau^{\\prime})}^{(k)}(\\omega)-\\widehat{\\pmb{y}}_{t_{n},g_{n}(\\tau^{\\prime})}^{(k-1)}(\\omega)\\right\\|^{2}\\right]\\mathrm{d}\\tau^{\\prime}}\\\\ &{\\leq L_{s}^{2}\\tau e^{2\\tau}\\displaystyle\\operatorname*{sup}_{\\tau^{\\prime}\\in[0,\\tau]}\\mathbb{E}_{\\omega\\sim\\breve{p}\\mid\\mathcal{F}_{t_{n}}}\\left[\\left\\|\\widehat{\\pmb{y}}_{t_{n},\\tau^{\\prime}}^{(k)}(\\omega)-\\widehat{\\pmb{y}}_{t_{n},\\tau^{\\prime}}^{(k-1)}(\\omega)\\right\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Furthermore, we take supremum over $\\tau\\in[0,h_{n}]$ on both sides above and iterate (B.12) over $k\\in\\mathbb{N}$ which indicates ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\underset{\\tau\\in[0,h_{n}]}{\\operatorname*{sup}}\\mathbb{E}_{\\omega\\sim\\tilde{p}_{||\\boldsymbol{x}_{t}|}}\\left[\\left\\|\\hat{y}_{t_{n},\\tau}^{(k+1)}(\\omega)-\\hat{y}_{t_{n},\\tau}^{(k)}(\\omega)\\right\\|^{2}\\right]}\\\\ &{\\leq L_{s}^{2}h_{n}e^{2h_{n}}\\underset{\\tau\\in[0,h_{n}]}{\\operatorname*{sup}}\\underset{\\omega\\sim\\tilde{p}_{||\\boldsymbol{x}_{t}|}}{\\operatorname*{sup}}\\left[\\left\\|\\hat{y}_{t_{n},\\tau}^{(k)}(\\omega)-\\hat{y}_{t_{n},\\tau}^{(k-1)}(\\omega)\\right\\|^{2}\\right]}\\\\ &{\\leq\\left(L_{s}^{2}h_{n}e^{2h_{n}}\\right)^{k}\\underset{\\tau\\in[0,h_{n}]}{\\operatorname*{sup}}\\mathbb{E}\\left[\\left\\|\\hat{y}_{t_{n},\\tau}^{(1)}(\\omega)-\\hat{y}_{t_{n},\\tau}^{(0)}(\\omega)\\right\\|^{2}\\right]}\\\\ &{\\leq\\left(L_{s}^{2}h_{n}e^{2h_{n}}\\right)^{k}h\\underset{\\tau}{\\leq}\\frac{2i d}{h}+2d)+3\\left(L_{s}^{2}h_{n}e^{2h_{n}}\\right)^{k}e^{\\frac{7}{2}h_{n}}\\mathbb{E}_{\\omega\\sim\\tilde{p}_{||\\boldsymbol{x}_{t}|}}\\left[A_{t_{n}}(\\omega)+B_{t_{n}}(\\omega)\\right]}\\\\ &{+3\\left(L_{s}^{2}h_{n}e^{2h_{n}}\\right)^{k}e^{\\frac{7}{2}h_{n}}h_{n}L_{s}^{2}\\underset{\\tau\\in[0,h_{n}]}{\\operatorname*{sup}}\\mathbb{E}_{\\omega\\sim\\tilde{p}_{||\\boldsymbol{x}_{t}|}}\\left[\\left\\|\\hat{y}_{t_{n},\\tau}^{(k)}(\\omega)-\\hat{y}_{t_{n},\\tau}^{(k-1)}(\\omega)\\right\\|^{2}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the last inequality follows from Lemma B.5. By rearranging the inequality above, setting $k=K-1$ and using the assumption that $L_{s}^{2}h_{n}e^{2h_{n}}\\ll1$ weobtain ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\tau\\in[0,h_{n}]}{\\operatorname*{sup}}\\mathbb{E}_{\\omega\\sim\\tilde{p}|\\boldsymbol{\\mathcal{F}}_{t_{n}}}\\left[\\left\\|\\widehat{\\boldsymbol{y}}_{t_{n},\\tau}^{(K)}(\\omega)-\\widehat{\\boldsymbol{y}}_{t_{n},\\tau}^{(K-1)}(\\omega)\\right\\|^{2}\\right]}\\\\ &{\\leq\\!\\frac{\\big(L_{s}^{2}h_{n}e^{2h_{n}}\\big)^{K-1}\\,h e^{\\frac{\\tau}{2}h_{n}}\\,\\big(M_{s}^{2}+2d\\big)+3\\,\\big(L_{s}^{2}h_{n}e^{2h_{n}}\\big)^{K-1}\\,e^{\\frac{\\tau}{2}h_{n}}\\mathbb{E}_{\\omega\\sim\\tilde{p}|\\boldsymbol{\\mathcal{F}}_{t_{n}}}\\left[A_{t_{n}}(\\omega)+B_{t_{n}}(\\omega)\\right]}{1-3\\,\\big(L_{s}^{2}h_{n}e^{2h_{n}}\\big)^{K-1}\\,e^{\\frac{\\tau}{2}h_{n}}h_{n}L_{s}^{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "as desired. ", "page_idx": 26}, {"type": "text", "text": "The following lemma from [98] bounds the expectation of the term $A_{t_{n}}(\\omega)$ in (B.7): ", "page_idx": 26}, {"type": "text", "text": "Lemma B.7 ([98, Section 3.1]). We have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\omega\\sim\\widetilde{p}\\mid x_{t_{n}}}\\left[A_{t_{n}}(\\omega)\\right]\\lesssim\\epsilon d h_{n},\\quad f o r\\,n\\in[0:N-2],\\,\\,a n d\\,\\mathbb{E}_{\\omega\\sim\\widetilde{p}\\mid x_{t_{n}}}\\left[A_{t_{N-1}}(\\omega)\\right]\\lesssim\\epsilon d\\log\\eta^{-1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $\\eta$ is the parameter for early stopping. ", "page_idx": 26}, {"type": "text", "text": "Proof. Notice that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{\\omega\\sim\\widetilde{p}\\mid x_{t_{n}}}\\left[A_{t_{n}}(\\omega)\\right]}\\\\ &{=\\!\\mathbb{E}_{\\omega\\sim\\widetilde{p}\\mid x_{t_{n}}}\\left[\\int_{0}^{h_{n}}\\left\\|\\nabla\\log\\tilde{p}_{t_{n}+g_{n}(\\tau)}(\\hat{y}_{t_{n},g_{n}(\\tau)}^{(K)}(\\omega))-\\nabla\\log\\tilde{p}_{t_{n}+\\tau}(\\hat{y}_{t_{n},g_{n}(\\tau)}^{(K)}(\\omega))\\right\\|^{2}\\mathrm{d}\\tau\\right]}\\\\ &{=\\!\\mathbb{E}_{\\omega\\sim\\widetilde{p}\\mid x_{t_{n}}}\\left[\\displaystyle\\sum_{m=0}^{M_{n}}\\int_{\\tau_{n},m}^{\\tau_{n},m+1}\\left\\|\\nabla\\log\\tilde{p}_{t_{n}+\\tau_{n,m}}(\\hat{y}_{t_{n},\\tau_{n},m}^{(K)}(\\omega))-\\nabla\\log\\tilde{p}_{t_{n}+\\tau}(\\hat{y}_{t_{n},\\tau}^{(K)}(\\omega))\\right\\|^{2}\\mathrm{d}\\tau\\right],}\\\\ &{=\\!\\sum_{m=0}^{M_{n}}\\!\\int_{\\tau_{n},m}^{\\tau_{n,m+1}}\\!\\!\\!\\mathbb{E}_{\\omega\\sim\\widetilde{p}\\mid x_{t_{n}}}\\left[\\left\\|\\nabla\\log\\tilde{p}_{t_{n}+\\tau_{n,m}}(\\hat{x}_{t_{n}+\\tau}(\\omega))-\\nabla\\log\\tilde{p}_{t_{n}+\\tau}(\\hat{y}_{t_{n}+\\tau}^{(K)}(\\omega))\\right\\|^{2}\\right]\\mathrm{d}\\tau,}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "whereft qualtywee tfact tha $\\widehat{\\pmb{y}}_{t_{n},\\tau}^{(K)}(\\omega)$ follows the backward SDE with the true score function under the measure $\\overleftarrow{p}$ In the following, we drop the superscript $\\omega\\sim\\bar{p}|_{\\mathcal{F}_{t_{n}}}$ of the expectation for simplicity. ", "page_idx": 26}, {"type": "text", "text": "By Lemma A.6 and A.7, we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\left[\\left\\|\\nabla\\log\\breve{p}_{t_{n}+\\tau_{n,m}}\\big(\\bar{x}_{t_{n}+\\tau}(\\omega)\\big)-\\nabla\\log\\breve{p}_{t_{n}+\\tau}\\big(\\bar{x}_{t_{n}+\\tau}(\\omega)\\big)\\right\\|^{2}\\right]}\\\\ &{\\leq\\int_{0}^{\\tau}\\left(\\frac{1}{2}\\mathbb{E}\\left[\\|\\nabla\\log\\breve{p}_{t_{n}+\\tau_{n,m}}\\big(\\bar{x}_{t_{n}+\\tau_{n,m}}(\\omega)\\big)\\|^{2}\\right]+\\mathbb{E}\\left[\\|\\nabla^{2}\\log\\breve{p}_{t_{n}+\\tau^{\\prime}}\\big(\\bar{x}_{t_{n}+\\tau^{\\prime}}(\\omega)\\big)\\|_{F}^{2}\\right]\\right)\\mathrm{d}\\tau^{\\prime}}\\\\ &{\\leq\\int_{0}^{\\tau}\\left(\\frac{1}{2}d\\bar{\\sigma}_{\\tau^{\\prime}}^{-2}+d\\bar{\\sigma}_{\\tau^{\\prime}}^{-4}\\right)\\mathrm{d}\\tau^{\\prime}+\\Big(\\bar{\\sigma}_{t_{n}+\\tau_{n,m}}^{-4}\\mathbb{E}\\left[\\mathrm{tr}\\,\\bar{\\Sigma}_{t_{n}+\\tau_{n,m}}\\right]-\\bar{\\sigma}_{t_{n}+\\tau}^{-4}\\mathbb{E}\\left[\\mathrm{tr}\\,\\bar{\\Sigma}_{t_{n}+\\tau}\\right]\\Big)\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Now noticing that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\overleftarrow{\\sigma}_{t}^{2}=\\sigma_{T-t}^{2}\\lesssim T-t,\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "we further have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\int_{\\tau_{n,m}}^{\\tau_{n,m+1}}\\mathbb{E}\\left[\\left\\|\\nabla\\log\\bar{p}_{t_{n}+\\tau_{n,m}}\\left(\\bar{x}_{t_{n}+\\tau}(\\omega)\\right)-\\nabla\\log\\bar{p}_{t_{n}+\\tau}(\\bar{x}_{t_{n}+\\tau}(\\omega))\\right\\|^{2}\\right]\\mathrm{d}\\tau}\\\\ &{\\lesssim\\int_{\\tau_{n,m}}^{\\tau_{n,m+1}}\\int_{0}^{\\tau^{\\prime}}\\frac{d}{\\left(T-t_{n}-\\tau_{n,m+1}\\right)^{2}}\\mathrm{d}\\tau^{\\prime}\\mathrm{d}\\tau+\\frac{\\epsilon_{n,m}\\,\\left(\\mathbb{E}\\left[\\mathrm{tr}\\,\\dot{\\Sigma}_{t_{n}+\\tau_{n,m}}\\right]-\\mathbb{E}\\left[\\mathrm{tr}\\,\\ddot{\\Sigma}_{t_{n}+\\tau_{n,m+1}}\\right]\\right)}{\\left(T-t_{n}-\\tau_{n,m}\\right)^{2}}}\\\\ &{\\lesssim d\\frac{\\epsilon_{n,m}^{2}}{\\left(T-t_{n}-\\tau_{n,m+1}\\right)^{2}}+\\frac{\\epsilon\\left(\\mathbb{E}\\left[\\mathrm{tr}\\,\\dot{\\Sigma}_{t_{n}+\\tau_{n,m}}\\right]-\\mathbb{E}\\left[\\mathrm{tr}\\,\\ddot{\\Sigma}_{t_{n}+\\tau_{n,m+1}}\\right]\\right)}{T-t_{n}-\\tau_{n,m}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "and thus ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{m=0}^{M_{n}}\\int_{\\tau_{n},m}^{\\tau_{n,m+1}}\\mathbb{E}\\left[\\left\\|\\nabla\\log\\tilde{p}_{t_{n}+\\tau_{n,m}}\\big(\\tilde{\\mathbf{x}}_{t_{n}+\\tau}(\\omega)\\big)-\\nabla\\log\\tilde{p}_{t_{n}+\\tau}\\big(\\tilde{\\mathbf{x}}_{t_{n}+\\tau}(\\omega)\\big)\\right\\|^{2}\\right]\\mathrm{d}\\tau}\\\\ {\\displaystyle\\lesssim d\\sum_{m=0}^{M_{n}}\\frac{\\epsilon_{n,m}^{2}}{(T-t_{n}-\\tau_{n,m+1})^{2}}+\\sum_{m=0}^{M_{n}}\\frac{\\epsilon}{T-t_{n}-\\tau_{n,m}}\\left(\\mathbb{E}\\left[\\mathrm{tr}\\,\\tilde{\\Sigma}_{t_{n}+\\tau_{n,m}}\\right]-\\mathbb{E}\\left[\\mathrm{tr}\\,\\tilde{\\Sigma}_{t_{n}+\\tau_{n,m+1}}\\right]\\right)}\\\\ {\\displaystyle\\leq d\\epsilon^{2}M_{n}+\\frac{\\epsilon\\mathbb{E}\\left[\\mathrm{tr}\\,\\tilde{\\Sigma}_{t_{n}+\\tau_{n,0}}\\right]}{T-t_{n}-\\tau_{n,0}}+\\sum_{m=0}^{M_{n}}\\frac{\\epsilon\\epsilon_{n,m}\\mathbb{E}\\left[\\mathrm{tr}\\,\\tilde{\\Sigma}_{t_{n}+\\tau_{n,m}}\\right]}{(T-t_{n}-\\tau_{n,m+1})(T-t_{n}-\\tau_{n,m})}}\\\\ {\\displaystyle\\leq d\\epsilon^{2}M_{n}+\\epsilon d+d\\epsilon^{2}M_{n}\\leq d\\epsilon^{2}M_{n}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "For $n\\in[0,N-2]$ , we have $M_{n}\\epsilon=h_{n}$ and thus $\\mathbb{E}_{\\omega\\sim\\overleftarrow{p}\\mid\\mathcal{F}_{t_{n}}}\\left[A_{t_{n}}(\\omega)\\right]\\lesssim\\epsilon d h_{n}$ and for $n=N-1$ \uff0c we have ", "page_idx": 27}, {"type": "equation", "text": "$$\nM_{N}\\stackrel{<}{\\sim}\\int_{\\eta}^{h}\\frac{1}{\\epsilon\\tau}\\mathrm{d}\\tau=\\log\\eta^{-1}\\epsilon^{-1}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and thus $\\begin{array}{r}{\\mathbb{E}_{\\omega\\sim\\tilde{p}|_{\\mathcal{F}_{t_{n}}}}\\left[A_{t_{N-1}}(\\omega)\\right]\\lesssim\\epsilon^{2}d M_{n}\\lesssim\\epsilon d\\log\\eta^{-1}.}\\end{array}$ ", "page_idx": 27}, {"type": "text", "text": "B.3 Overall Error Bound ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Proof of Theorem 3.3. We first continue the computation in (B.6) and (B.7): ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad D_{\\mathrm{KL}}(\\tilde{p}_{t_{n+1}}\\|\\hat{q}_{t_{n+1}})\\leq D_{\\mathrm{KL}}(\\tilde{p}_{t_{n}}\\|\\hat{q}_{t_{n}})+\\mathbb{E}_{\\omega\\sim\\tilde{p}|\\boldsymbol{J}_{t_{n}}}\\left[\\frac{1}{2}\\int_{0}^{h_{n}}\\|\\delta_{t_{n}}(\\tau,\\omega)\\|^{2}\\mathrm{d}\\tau\\right]}\\\\ &{\\leq D_{\\mathrm{KL}}(\\tilde{p}_{t_{n}}\\|\\hat{q}_{t_{n}})+3\\mathbb{E}_{\\omega\\sim\\tilde{p}|\\boldsymbol{J}_{t_{n}}}\\left[A_{t_{n}}(\\omega)+B_{t_{n}}(\\omega)\\right]}\\\\ &{+3\\mathbb{E}_{\\omega\\sim\\tilde{p}|\\boldsymbol{J}_{t_{n}}}\\left[\\int_{0}^{h_{n}}\\left\\|s_{t_{n}+g_{n}(\\tau)}^{\\theta}(\\hat{p}_{t_{n},g_{n}(\\tau)}^{(K)}(\\omega))-s_{t_{n+9n}(\\tau)}^{\\theta}(\\hat{y}_{t_{n},g_{n}(\\tau)}^{(K-1)}(\\omega))\\right\\|^{2}\\mathrm{d}\\tau\\right]}\\\\ &{\\leq D_{\\mathrm{KL}}(\\tilde{p}_{t_{n}}\\|\\hat{q}_{t_{n}})+3\\mathbb{E}_{\\omega\\sim\\tilde{p}|\\boldsymbol{J}_{t_{n}}}\\left[A_{t_{n}}(\\omega)+B_{t_{n}}(\\omega)+L_{s}^{2}\\int_{0}^{h_{n}}\\left\\|\\hat{y}_{t_{n},g_{n}(\\tau)}^{(K)}(\\omega)-\\hat{y}_{t_{n,g_{n}(\\tau)}}^{(K-1)}(\\omega)\\right\\|^{2}\\mathrm{d}\\tau\\right]}\\\\ &{\\leq D_{\\mathrm{KL}}(\\tilde{p}_{t_{n}}\\|\\hat{q}_{t_{n}})+3\\mathbb{E}_{\\omega\\sim\\tilde{p}|\\boldsymbol{J}_{t_{n}}}\\left[A_{t_{n}}(\\omega)+B_{t_{n}}(\\omega)+h_{n}L_{s}^{2}\\int_{0}^{\\infty}\\left\\|\\hat{y}_{t_{n},r}^{(K)}(\\omega)-\\hat{y}_{t_{n,\\tau}}^{(K-\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Then plugging in the result of Lemma B.6, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad D_{\\mathrm{KL}}(\\Tilde{p}_{t_{n+1}}\\lVert\\hat{q}_{t_{n+1}})}\\\\ &{\\leq D_{\\mathrm{KL}}(\\Tilde{p}_{t_{n}}\\lVert\\hat{q}_{t_{n}})+3\\mathbb{E}_{\\omega\\sim\\Tilde{p}\\mid x_{t_{n}}}\\left[A_{t_{n}}(\\omega)+B_{t_{n}}(\\omega)\\right]+3h_{n}L_{s}^{2}\\frac{\\big(L_{s}^{2}h_{n}e^{2h_{n}}\\big)^{K-1}h\\epsilon^{\\frac{7}{2}h_{n}}\\left(M_{s}^{2}+2d\\right)}{1-3\\left(L_{s}^{2}h_{n}e^{2h_{n}}\\right)^{K-1}e^{\\frac{7}{2}h_{n}}h_{n}L_{s}^{2}}}\\\\ &{\\quad+h_{n}L_{s}^{2}\\frac{9\\left(L_{s}^{2}h_{n}e^{2h_{n}}\\right)^{K-1}e^{\\frac{7}{2}h_{n}}\\mathbb{E}_{\\omega\\sim\\Tilde{p}\\mid x_{t_{n}}}\\left[A_{t_{n}}(\\omega)+B_{t_{n}}(\\omega)\\right]}{1-3\\left(L_{s}^{2}h_{n}e^{2h_{n}}\\right)^{K-1}e^{\\frac{7}{2}h_{n}}h_{n}L_{s}^{2}}}\\\\ &{\\lesssim D_{\\mathrm{KL}}(\\Tilde{p}_{t_{n}}\\lVert\\hat{q}_{t_{n}})+\\frac{1+e^{-K}h_{n}e^{h_{n}}}{1-e^{-K}h_{n}e^{h_{n}}}\\mathbb{E}_{\\omega\\sim\\Tilde{p}\\mid x_{t_{n}}}\\left[A_{t_{n}}(\\omega)+B_{t_{n}}(\\omega)\\right]+e^{-K}h_{n}^{2}e^{h_{n}}d}\\\\ &{\\lesssim D_{\\mathrm{KL}}(\\Tilde{p}_{t_{n}}\\lVert\\hat{q}_{t_{n}})+\\mathbb{E}_{\\omega\\sim\\Tilde{p}\\mid x_{t_{n}}}\\left[A_{t_{n}}(\\omega)+B_{t_{n}}(\\omega)\\right]+e^{-K}h_{n}^{2}e^{h_{n}}d,}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where we used the assumption that $L_{s}^{2}h_{n}e^{\\frac{7}{2}h_{n}}\\ll1$ ", "page_idx": 27}, {"type": "text", "text": "The term $\\begin{array}{r}{\\sum_{n=0}^{N-1}\\mathbb{E}_{\\omega\\sim\\overleftarrow{p}\\mid x_{n}}\\left[B_{t_{n}}(\\omega)\\right]}\\end{array}$ is boundedby Assumption 3.1 as ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\displaystyle\\sum_{n=0}^{N-1}\\mathbb{E}_{\\omega\\sim\\widetilde{p}|x_{t_{n}}}[B_{t_{n}}(\\omega)]}\\\\ {\\displaystyle}&{\\le\\mathbb{E}_{\\omega\\sim\\widetilde{p}|x_{t_{n}}}\\left[\\sum_{n=0}^{N-1}\\int_{0}^{h_{n}}\\left\\|s_{t_{n}+g_{n}(\\tau)}^{\\theta}(\\hat{y}_{t_{n},g_{n}(\\tau)}^{(K)}(\\omega))-\\nabla\\log\\widetilde{p}_{t_{n}+g_{n}(\\tau)}(\\hat{y}_{t_{n},g_{n}(\\tau)}^{(K)}(\\omega))\\right\\|^{2}\\mathrm{d}\\tau\\right]}\\\\ {\\displaystyle}&{=\\mathbb{E}_{\\omega\\sim\\widetilde{p}|x_{t_{n}}}\\left[\\sum_{n=0}^{N-1}\\sum_{m=0}^{M_{n}-1}\\epsilon_{n,m}\\left\\|s_{t_{n}+\\tau_{n},m}^{\\theta}(\\hat{y}_{t_{n},\\tau_{n},m}^{(K)}(\\omega))-\\nabla\\log\\widetilde{p}_{t_{n}+\\tau_{n},m}(\\hat{y}_{t_{n},\\tau_{n},m}^{(K)}(\\omega))\\right\\|^{2}\\right]}\\\\ {\\displaystyle}&{=\\mathbb{E}_{\\omega\\sim\\widetilde{p}|x_{t_{n}}}\\left[\\sum_{n=0}^{N-1}\\sum_{m=0}^{M_{n}-1}\\epsilon_{n,m}\\left\\|s_{t_{n}+\\tau_{n},m}^{\\theta}(\\tilde{x}_{t_{n}+\\tau}^{\\theta}(\\omega))-\\nabla\\log\\widetilde{p}_{t_{n}+\\tau_{n},m}(\\hat{x}_{t_{n}+\\tau}^{\\theta}(\\omega))\\right\\|^{2}\\right]\\le\\delta_{2}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where the last equality is because the process $\\widehat{\\b{y}}_{t_{n},\\tau}^{(K)}(\\omega)$ under measure $\\overleftarrow{p}$ follows the backward SDE (B.5). ", "page_idx": 28}, {"type": "text", "text": "Thus, by Theorem A.1 and plugging in the iteration relations above ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad D_{\\mathrm{KL}}(p_{\\eta}\\|\\widehat{q}_{t_{N}})=D_{\\mathrm{KL}}(\\bar{p}_{t_{N}}\\|\\widehat{q}_{t_{N}})}\\\\ &{\\leq\\!D_{\\mathrm{KL}}(\\bar{p}_{0}\\|\\widehat{q}_{0})+\\displaystyle\\sum_{n=0}^{N-1}\\Big(\\mathbb{E}_{\\omega\\sim\\widetilde{p}|x_{t_{n}}}\\left[A_{t_{n}}(\\omega)+B_{t_{n}}(\\omega)\\right]+e^{-K}h_{n}^{2}e^{h_{n}}d\\Big)}\\\\ &{\\leq\\!D_{\\mathrm{KL}}(\\bar{p}_{0}\\|\\widehat{q}_{0})+\\displaystyle\\sum_{n=0}^{N-2}\\epsilon d h_{n}+\\epsilon d\\log\\eta^{-1}+\\displaystyle\\sum_{n=0}^{N-1}\\mathbb{E}_{\\omega\\sim\\widetilde{p}|x_{t_{n}}}\\left[B_{t_{n}}(\\omega)\\right]+e^{-K}h_{n}^{2}e^{h_{n}}d N}\\\\ &{\\leq\\!d e^{-T}+\\epsilon d(T+\\log\\eta^{-1})+\\delta_{2}^{2}+e^{-K}d T\\leq d e^{-T}+\\epsilon d T+\\delta^{2}+e^{-K}d T,}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "as $T\\gtrsim\\log\\eta^{-1}$ $h_{n}\\lesssim1$ ,and $\\delta_{2}\\lesssim\\delta$ , and then it is straightforward to see that the following choices of parameters ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{T=\\mathcal{O}(\\log(d\\delta^{-2})),\\quad h=\\Theta(1),\\quad N=\\mathcal{O}\\left(\\log(d\\delta^{-2})\\right),\\quad}\\\\ {\\epsilon=\\Theta\\left(d^{-1}\\delta^{2}\\log^{-1}(d\\delta^{-2})\\right),\\quad M=\\mathcal{O}\\left(d\\delta^{-2}\\log(d\\delta^{-2})\\right),\\quad}\\\\ {K=\\widetilde{\\mathcal{O}}(\\log(d\\delta^{-2})),\\quad~}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "would yield an overall error of ${\\mathcal{O}}(\\delta^{2})$ ", "page_idx": 28}, {"type": "text", "text": "C Details of Probability Flow ODE Implementation ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "In this section, we provide the details of the parallelized algorithm for the probability fow ODE formulation of diffusion models. We first introduce the algorithm and define the necessary notations, then discuss the error analysis during the predictor and corrector steps, respectively, and finally provide the proof of Theorem 3.5. ", "page_idx": 28}, {"type": "text", "text": "C.1  Algorithm ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "In the parallelized inference algorithm for diffusion models in the probability flow ODE formulation, we adopt the same discretization scheme as in Section 3.1.1 and the exponential integrator for all updating rules. For each block, we first run a predictor step, which consists of running the probability flow ODE in parallel. Then we run a corrector step, which runs an underdamped Langevin dynamics in parallel to correct the distribution of the samples. The algorithm is summarized In Algorithm 2. ", "page_idx": 28}, {"type": "text", "text": "Parallelized Predictor Step _ The parallelization strategies in the predictor step are similar to those in the SDE algorithm (Algorithm 1). The only difference here is that instead of applying Picard iteration to the backward SDE as in (3.2), we apply Picard iteration to the probability flow ODE as in (C.3), which does not require i.i.d. samples from standard Gaussian distribution. As shown in Lemma C.3, the update rule in the predictor step (C.1) in Algorithm 2 is equivalent to running ", "page_idx": 28}, {"type": "text", "text": "Algorithm 2: PIADM-ODE ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Input: $\\widehat{\\pmb y}_{0}\\sim\\widehat{q}_{0}=\\mathcal{N}(0,\\pmb I_{d})$ adiscretization scheme $(T,(h_{n})_{n=1}^{N}$ and $(\\tau_{n,m})_{n\\in[1:N],m\\in[0:M_{n}]})$ satisfying (3.1), parameters for the corrector step $(T^{\\dagger},N^{\\dagger},h^{\\dagger},M^{\\dagger},\\epsilon^{\\dagger})$ , the depth of iteration $K$ and $\\dot{K}^{\\dag}$ , the learned NN-based score $s_{t}^{\\theta}(\\cdot)$ ", "page_idx": 29}, {"type": "text", "text": "Output: A sample $\\widehat{\\pmb{y}}_{T}\\sim\\widehat{q}_{T}\\approx\\overleftarrow{p}_{T}$ 1 for $n=0$ to $N-1$ do ", "page_idx": 29}, {"type": "text", "text": "2 $\\triangleright$ Predictor Step (Section C.2)   \n3 $\\widehat{\\pmb{y}}_{t_{n},\\tau_{n},m}^{(0)}\\gets\\widehat{\\pmb{y}}_{t_{n}}$ for $m\\in[0:M_{n}]$ ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{\\pmb{y}}_{t_{n},\\tau_{n,m}}^{(k)}\\gets\\!\\frac{1}{2}e^{\\frac{\\tau_{n,m}}{2}}\\widehat{\\pmb{y}}_{t_{n},0}^{(k-1)}}\\\\ &{\\quad\\quad\\quad\\quad+\\,\\frac{1}{2}\\sum_{j=0}^{m-1}e^{\\frac{\\tau_{n,m}-\\tau_{n,j+1}}{2}}\\left(e^{\\epsilon_{n,j}}-1\\right)\\pmb{s}_{t_{n}+\\tau_{n,j}}^{\\theta}(\\widehat{\\pmb{y}}_{t_{n},\\tau_{n,j}}^{(k-1)})}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(\\widehat{\\pmb{u}}_{t_{n},n^{\\dagger}h^{\\dagger},0}^{(k^{\\dagger})},\\widehat{\\pmb{v}}_{t_{n},n^{\\dagger}h^{\\dagger},0}^{(k^{\\dagger})})\\leftarrow(\\widehat{\\pmb{u}}_{t_{n},n^{\\dagger}h^{\\dagger}},\\widehat{\\pmb{v}}_{t_{n},n^{\\dagger}h^{\\dagger}});}\\\\ &{\\xi_{j^{\\dagger}}\\sim\\mathcal{N}\\left(\\mathbf{0},2\\gamma(1+\\gamma^{-2})(1-e^{-\\gamma\\epsilon^{\\dagger}})^{2}e^{-2\\gamma((M^{\\dagger}-j^{\\dagger}+1)\\epsilon^{\\dagger})}I_{d}\\right)\\mathrm{for}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\boxed{\\widehat{u}_{t_{n},n^{\\dagger}\\widehat{\\mathbf{t}}^{\\dagger}}^{(k^{\\dagger})}u_{\\ t^{\\dagger}}\\epsilon^{\\dagger}}\\biggr]\\leftarrow G(m^{\\dagger}\\epsilon^{\\dagger})\\left[\\widehat{u}_{t_{n},n^{\\dagger}\\widehat{\\mathbf{t}}^{\\dagger},0}^{(k^{\\dagger}-1)}\\right]}\\\\ &{\\boxed{\\widehat{v}_{t_{n},n^{\\dagger}\\widehat{\\mathbf{t}}^{\\dagger},m^{\\dagger}\\epsilon^{\\dagger}}^{(k^{\\dagger})}}}\\\\ &{\\quad\\quad\\xrightarrow{m-1}G((m^{\\dagger}-j^{\\dagger}-1)\\epsilon^{\\dagger})\\left(I_{d}-G(\\epsilon^{\\dagger})\\right)\\left[s_{t_{n+1}}^{\\theta}(\\widehat{u}_{t_{n},n^{\\dagger}\\widehat{\\mathbf{t}}^{\\dagger},j^{\\dagger}\\epsilon^{\\dagger}}^{(k^{\\dagger}-1)}\\right]}\\\\ &{\\quad\\quad\\xrightarrow{m-1}G((m^{\\dagger}-j^{\\dagger}-1)\\epsilon^{\\dagger})\\left[\\underline{{\\mathbf{0}}}_{j^{\\dagger}}^{\\theta}\\right];}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\big(\\widehat{\\pmb{u}}_{t_{n},(n^{\\dagger}+1)h^{\\dagger}},\\widehat{\\pmb{v}}_{t_{n},(n^{\\dagger}+1)h^{\\dagger}}\\big)\\leftarrow\\big(\\widehat{\\pmb{u}}_{t_{n},n^{\\dagger}h^{\\dagger},h^{\\dagger}}^{(K^{\\dagger})},\\widehat{\\pmb{v}}_{t_{n},n^{\\dagger}h^{\\dagger},h^{\\dagger}}^{(K^{\\dagger})}\\big);}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "image", "img_path": "F9NDzHQtOl/tmp/0082c60fcef291c0ce1671b646ef5ea8a1018ea86e842360017f68da1af0aee8.jpg", "img_caption": ["Figure 2: Ilustration of the proof pipeline of Theorem 3.5 for PIADM-ODE within the $n$ th block. "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "the auxiliary predictor process (C.3). The auxiliary predictor process takes in the result from the preius corectorstep orthe inializatonif $n=0$ andoutputs $\\widehat{\\pmb{y}}_{t_{n},h_{n}}^{(K)}$ as the initialization for the next corrector step. ", "page_idx": 30}, {"type": "text", "text": "Parallelized Corrector Step  The parallelization of the underdamped Langevin dynamics is similar to that mentioned in Section 2.2. Given a sample resulting from the predictor step, we initialize the auxiliary corrector process (Definition C.8) which is an underdamped Langevin dynamics with the initialization Utn,0 = Utn,hn and the augmented variable $\\widehat{\\pmb{v}}_{t_{n},0}\\sim\\mathcal{N}(0,\\pmb{I}_{d})$ representing the momentum. ", "page_idx": 30}, {"type": "text", "text": "We run the underdamped Langevin dynamics for time $T^{\\dagger}$ , which is set to be of order $\\Omega(1)$ so that it is large enough to correct the distribution of the samples (cf. Lemma C.18) while being comparably short to ensure numerical stability $\\scriptstyle{C f.}$ Theorem C.17). Following a similar strategy as in Section 2.2 and in Algorithm 1, we further divide the time horizon $T^{\\dagger}$ into $N^{\\dagger}$ blocks with step size $h^{\\dagger}$ , and for each block the block length $h^{\\dagger}$ into $M^{\\dagger}$ steps with step size $\\epsilon^{\\dagger}$ . Within each block, we run the underdamped Langevin dynamics in parallel for $K^{\\dagger}$ iterations. As shown in Lemma C.9, the update rule in the corrector step (C.2) in Algorithm 2 is equivalent to running the auxiliary corrector process (C.11). ", "page_idx": 30}, {"type": "text", "text": "In the following subsections, we proceed to provide theoretical guarantees for the algorithm. ", "page_idx": 30}, {"type": "text", "text": "C.2  Parallelized Predictor Step ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Definition C.1 (Auxiliary Predictor Process). For any $n\\,\\in\\,[0\\,:\\,N\\,-\\,1]$ we define the auxiliary predictor process $(\\widehat{\\pmb{y}}_{t_{n},\\tau}^{(k)})_{\\tau\\in[0,h_{n}]}$ as the solution to the following ODE recursively for $k\\ \\in\\ [0\\ :$ $K-1]$ ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathrm{d}\\widehat{\\pmb{y}}_{t_{n},\\tau}^{(k+1)}=\\left[\\frac{1}{2}\\widehat{\\pmb{y}}_{t_{n},\\tau}^{(k+1)}+\\frac{1}{2}\\pmb{s}_{t_{n}+g_{n}(\\tau)}^{\\theta}\\left(\\widehat{\\pmb{y}}_{t_{n},g_{n}(\\tau)}^{(k)}\\right)\\right]\\mathrm{d}\\tau,\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "withtheinitial condition ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{\\pmb{y}}_{t_{n},\\tau}^{(0)}\\equiv\\widehat{\\pmb{y}}_{t_{n}}f o r\\,\\tau\\in[0,h_{n}],\\quad a n d\\quad\\pmb{y}_{t_{n},0}^{(k)}\\equiv\\widehat{\\pmb{y}}_{t_{n}}f o r\\,k\\in[1:K]}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $\\widehat{\\pmb{y}}_{t_{n}}=\\widehat{\\pmb{u}}_{t_{n-1},N^{\\dag}h^{\\dag}}$ i $f n\\in[1:N-1]$ and $\\widehat{\\pmb{y}}_{t_{0}}\\sim\\mathcal{N}(0,\\pmb{I}_{d})$ .Wewillalsdete theprobabilty distribution of $\\widehat{\\pmb{y}}_{t_{n},\\tau}^{(K)}\\,a s\\,\\widehat{q}_{t_{n},\\tau}$ ", "page_idx": 30}, {"type": "text", "text": "Definition C.2 (Interpolating Process). For any $n\\in[0:N-1]$ we define the interpolating process $(\\widetilde{\\pmb{y}}_{t_{n},\\tau}^{(k)})_{\\tau\\in[0,h_{n}]}$ astesoluion tothefllowingODrecursivelfor $k\\in[0:K-1]$ ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathrm{d}\\widetilde{\\pmb{y}}_{t_{n},\\tau}^{(k+1)}=\\left[\\frac{1}{2}\\widetilde{\\pmb{y}}_{t_{n},\\tau}^{(k+1)}+\\frac{1}{2}\\pmb{s}_{t_{n}+g_{n}(\\tau)}^{\\theta}\\left(\\widetilde{\\pmb{y}}_{t_{n},g_{n}(\\tau)}^{(k)}\\right)\\right]\\mathrm{d}\\tau,\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "with initial condition ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widetilde{\\pmb{y}}_{t_{n},\\tau}^{(0)}\\equiv\\widetilde{\\pmb{y}}_{t_{n},0}^{(0)}\\quad f o r\\,\\tau\\in[0,h_{n}],\\;a n d\\,\\widetilde{\\pmb{y}}_{t_{n},0}^{(k)}\\equiv\\widetilde{\\pmb{y}}_{t_{n},0}^{(0)}\\quad f o r\\,k\\in[1:K],}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $\\widetilde{\\pmb{y}}_{t_{n},0}^{(0)}\\sim\\widetilde{\\boldsymbol{p}}_{t_{n}}$ Wewillaldetethpbilirdisi $\\widetilde{\\pmb{y}}_{t_{n},\\tau}^{(K)}\\,a s\\,\\widetilde{q}_{t_{n},\\tau}.$ ", "page_idx": 31}, {"type": "text", "text": "Similar to the equivalence between (3.4) and (B.2), we have the following lemma: ", "page_idx": 31}, {"type": "text", "text": "Lemma C.3 (Equivalence between (C.1) and (C.3)). For any $n\\in[0:N-1],$ theupdaterule $(C.l)$ in Algorithm 2 is equivalent to the exact solution of (C.3) for any $k\\in[0:K-1]$ and $\\tau\\in[0,h_{n}]$ ", "page_idx": 31}, {"type": "text", "text": "Proof. Rewriting (C.3) and multiplying $e^{-\\frac{\\tau}{2}}$ on both sides yield ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathrm{d}\\left[e^{-\\frac{\\tau}{2}}\\widehat{y}_{t_{n},\\tau}^{(k+1)}\\right]=e^{-\\frac{\\tau}{2}}\\left[\\mathrm{d}\\widehat{y}_{t_{n},\\tau}^{(k+1)}-\\frac{1}{2}\\widehat{y}_{t_{n},\\tau}^{(k+1)}\\mathrm{d}\\tau\\right]=\\frac{e^{-\\frac{\\tau}{2}}}{2}s_{t_{n}+g_{n}(\\tau)}^{\\theta}\\Big(\\widehat{y}_{t_{n},g_{n}(\\tau)}^{(k)}\\Big)\\mathrm{d}\\tau\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Integrating on both sides from O to $\\tau$ implies ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad e^{-\\frac{\\tau}{2}}\\widehat{y}_{t_{n},\\tau}^{(k+1)}-\\widehat{y}_{t_{n},0}^{(k+1)}=\\int_{0}^{\\tau}\\frac{e^{-\\frac{\\tau}{2}}}{2}s_{t_{n}+g_{n}(\\tau^{\\prime})}^{\\theta}\\Big(\\widehat{y}_{t_{n},g_{n}(\\tau^{\\prime})}^{(k)}\\Big)\\mathrm{d}\\tau^{\\prime}}\\\\ &{=\\!\\!\\frac{1}{2}\\displaystyle\\sum_{m=0}^{M_{n}}\\int_{\\tau\\wedge t_{n},m}^{\\tau\\wedge\\tau_{n,m+1}}e^{-\\frac{\\tau^{\\prime}}{2}}s_{t_{n}+\\tau_{n,m}}^{\\theta}\\Big({y}_{t_{n},\\tau_{n,m}}^{(k)}\\Big)\\mathrm{d}\\tau^{\\prime}}\\\\ &{=\\displaystyle\\sum_{m=0}^{M_{n}}\\Big(e^{-\\frac{\\tau\\wedge\\tau_{n,m}}{2}}-e^{-\\frac{\\tau\\wedge\\tau_{n,m+1}}{2}}\\Big)\\,s_{t_{n}+\\tau_{n,j}}^{\\theta}\\Big(\\widehat{y}_{t_{n},\\tau_{n,m}}^{(k)}\\Big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "and then multiplying $e^{\\frac{\\tau}{2}}$ on both sides above yields ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\widehat{y}_{t_{n},\\tau}^{(k+1)}=e^{\\frac{\\tau}{2}}\\widehat{y}_{t_{n},0}^{(k+1)}+\\sum_{m=0}^{M_{n}}\\left(e^{\\frac{\\tau\\wedge\\tau_{n,m+1}-\\tau\\wedge\\tau_{n,m}}{2}}-1\\right)e^{\\frac{0\\vee(\\tau-\\tau_{n,m+1})}{2}}s_{t_{n}+\\tau_{n,m}}^{\\theta}\\left(\\widehat{y}_{t_{n},\\tau_{n,m}}^{(k)}\\right).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Plugging in $\\tau=\\tau_{n,m}$ gives us (C.1), as desired. ", "page_idx": 31}, {"type": "text", "text": "Lemma C.4 (Error between the interpolating process and the true process). Under the Picard iteration, we have that the ending process $\\!\\left\\{\\widehat{y}_{t_{n},\\tau}^{(K)}\\right\\}\\!_{\\tau\\in[0,h_{n}]}$ satisies the following exponential convergence rate ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\epsilon[0,h_{n}]}\\mathbb{E}\\left[\\left\\|\\tilde{y}_{t_{n},\\tau}^{(K)}-\\bar{x}_{t_{n}+\\tau}\\right\\|^{2}\\right]\\leq3d\\left(\\frac{h_{n}^{2}e^{h_{n}+\\frac{3}{2}}L_{s}^{2}}{2}\\right)^{K}+\\frac{e^{h_{n}+\\frac{3}{2}}h_{n}/2}{1-h_{n}^{2}e^{h_{n}+\\frac{3}{2}}L_{s}^{2}/2}\\Big(h_{n}\\delta_{\\infty}^{2}+\\mathbb{E}[D_{t_{n}}]\\Big),\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where ", "page_idx": 31}, {"type": "equation", "text": "$$\nD_{t_{n}}:=\\int_{0}^{h_{n}}\\left\\|s_{t_{n}+g_{n}(\\tau^{\\prime})}^{\\theta}\\big(\\ddot{x}_{t_{n}+g_{n}(\\tau^{\\prime})}\\big)-s_{t_{n}+\\tau^{\\prime}}^{\\theta}(\\ddot{x}_{t_{n}+\\tau^{\\prime}})\\right\\|^{2}\\mathrm{d}\\tau^{\\prime}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Proof. Recall that the backward true process $\\{\\overleftarrow{\\pmb{x}}_{t_{n}+\\tau}\\}_{\\tau\\in[0,h_{n}]}$ satisfies the following backward SDE within one block ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathrm{d}\\overleftarrow{\\pmb{x}}_{t_{n}+\\tau}=\\left[\\frac{1}{2}\\overleftarrow{\\pmb{x}}_{t_{n}+\\tau}+\\frac{1}{2}\\nabla\\log\\overleftarrow{p}_{t_{n}+\\tau}(\\overleftarrow{\\pmb{x}}_{t_{n}+\\tau})\\right]\\mathrm{d}\\tau.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "By subtracting (C.6) from (C.5), we obtain that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\frac{\\mathrm{d}}{\\mathrm{d}\\tau}\\left(\\widetilde{y}_{t_{n},\\tau}^{(k+1)}-\\widetilde{x}_{t_{n}+\\tau}\\right)=\\frac{1}{2}\\left[\\widetilde{y}_{t_{n},\\tau}^{(k+1)}-\\widetilde{x}_{t_{n}+\\tau}\\right]}\\quad}&{}\\\\ &{\\quad\\quad\\quad\\quad\\quad+\\frac{1}{2}\\left[s_{t_{n}+g_{n}(\\tau)}^{\\theta}(\\widetilde{y}_{t_{n},g_{n}(\\tau)}^{(k)})-s_{t_{n}+g_{n}(\\tau)}^{\\theta}(\\widetilde{x}_{t_{n}+g_{n}(\\tau)})\\right]}\\\\ &{\\quad\\quad\\quad\\quad\\quad+\\frac{1}{2}\\left[s_{t_{n}+g_{n}(\\tau)}^{\\theta}(\\widetilde{x}_{t_{n}+g_{n}(\\tau)})-\\nabla\\log\\widetilde{p}_{t_{n}+g_{n}(\\tau)}(\\widetilde{x}_{t_{n}+g_{n}(\\tau)})\\right]}\\\\ &{\\quad\\quad\\quad\\quad\\quad+\\frac{1}{2}\\left[\\nabla\\log\\widetilde{p}_{t_{n}+g_{n}(\\tau)}(\\widetilde{x}_{t_{n}+g_{n}(\\tau)})-\\nabla\\log\\widetilde{p}_{t_{n}+\\tau}(\\widetilde{x}_{t_{n}+\\tau})\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Then by ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathrm{d}\\left\\|\\widetilde{\\pmb{y}}_{t_{n},\\tau^{\\prime}}^{(k+1)}-\\widetilde{\\pmb{x}}_{t_{n}+\\tau^{\\prime}}\\right\\|^{2}=2\\left(\\widetilde{\\pmb{y}}_{t_{n},\\tau^{\\prime}}^{(k+1)}-\\widetilde{\\pmb{x}}_{t_{n}+\\tau^{\\prime}}\\right)^{\\top}\\mathrm{d}\\left(\\widetilde{\\pmb{y}}_{t_{n},\\tau^{\\prime}}^{(k+1)}-\\widetilde{\\pmb{x}}_{t_{n}+\\tau^{\\prime}}\\right),\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "and integrating for $\\tau^{\\prime}\\in[0,h_{n}]$ , we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\tilde{y}_{t_{n},\\tau}^{(k+1)}-\\tilde{x}_{t_{n}+\\tau}\\right\\|^{2}}\\\\ &{=\\displaystyle\\int_{0}^{\\tau}\\left(\\tilde{y}_{t_{n},\\tau^{\\prime}}^{(k+1)}-\\tilde{x}_{t_{n}+\\tau^{\\prime}}\\right)^{\\top}\\left(s_{t_{n}+g_{n}(\\tau^{\\prime})}^{\\theta}\\big(\\tilde{y}_{t_{n},g_{n}(\\tau^{\\prime})}^{(k)}\\big)-s_{t_{n}+g_{n}(\\tau^{\\prime})}^{\\theta}\\big(\\tilde{x}_{t_{n}+g_{n}(\\tau^{\\prime})}\\big)\\right)\\mathrm{d}\\tau^{\\prime}}\\\\ &{\\displaystyle+\\int_{0}^{\\tau}\\left(\\tilde{y}_{t_{n},\\tau^{\\prime}}^{(k+1)}-\\tilde{x}_{t_{n}+\\tau^{\\prime}}\\right)^{\\top}\\left(s_{t_{n}+g_{n}(\\tau^{\\prime})}^{\\theta}\\big(\\tilde{x}_{t_{n}+g_{n}(\\tau^{\\prime})}\\big)-\\nabla\\log\\tilde{p}_{t_{n}+g_{n}(\\tau^{\\prime})}\\big(\\tilde{x}_{t_{n}+g_{n}(\\tau^{\\prime})}\\big)\\right)\\mathrm{d}\\tau^{\\prime}}\\\\ &{\\displaystyle+\\int_{0}^{\\tau}\\left(\\tilde{y}_{t_{n},\\tau^{\\prime}}^{(k+1)}-\\tilde{x}_{t_{n}+\\tau^{\\prime}}\\right)^{\\top}\\left(\\nabla\\log\\tilde{p}_{t_{n}+g_{n}(\\tau^{\\prime})}\\big(\\tilde{x}_{t_{n}+g_{n}(\\tau^{\\prime})}\\big)-\\nabla\\log\\tilde{p}_{t_{n}+\\tau^{\\prime}}(\\tilde{x}_{t_{n}+\\tau^{\\prime}})\\right)\\mathrm{d}\\tau^{\\prime}}\\\\ &{\\displaystyle+\\int_{0}^{\\tau}\\left\\|\\tilde{y}_{t_{n},\\tau^{\\prime}}^{(k+1)}-\\tilde{x}_{t_{n}+\\tau^{\\prime}}\\right\\|^{2}\\mathrm{d}\\tau^{\\prime}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Using AM-GM inequality and taking expectations on both sides, we further upper bound the summation above as ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\|\\hat{\\eta}_{t-\\tau}^{(k+1)}-\\tilde{x}_{{t}+\\tau}\\right\\|^{2}\\right]}\\\\ &{\\leq\\left(1+\\frac{3}{2b}\\right)\\int_{0}^{\\tau}\\mathbb{E}\\left[\\left\\|\\hat{\\eta}_{t-\\tau}^{(k+1)}-\\tilde{x}_{{t}+\\tau}\\right\\|^{2}\\right]\\mathrm{d}\\tau^{\\prime}}\\\\ &{+\\frac{h_{m}}{2}\\int_{0}^{\\tau}\\mathbb{E}\\left[\\left\\|u_{t-\\tau+\\tau}^{(k)}(\\gamma)\\hat{\\eta}_{t-\\tau+\\tau}^{(k)}(\\gamma)-\\delta u_{t+\\tau}^{\\theta}(\\gamma)(\\tilde{X}_{t+\\tau+\\tau}(\\gamma))\\right\\|^{2}\\right]\\mathrm{d}\\tau^{\\prime}}\\\\ &{+\\frac{h_{m}}{2}\\int_{0}^{\\tau}\\mathbb{E}\\left[\\left\\|u_{t+\\tau+\\tau}^{(k)}(\\gamma)\\hat{\\pi}_{t-\\tau+\\tau}(\\gamma)-\\nabla\\log\\tilde{\\eta}_{t+\\tau+\\tau}(\\gamma)\\hat{\\pi}_{t+\\tau+\\tau}(\\gamma)\\right\\|^{2}\\right]\\mathrm{d}\\tau^{\\prime}}\\\\ &{+\\frac{h_{m}}{2}\\mathbb{E}\\left[\\left\\|\\nabla u_{t}\\hat{\\eta}_{t-\\tau+\\tau}(\\gamma)\\hat{\\pi}_{t+\\tau+\\tau}(\\gamma)-\\nabla\\log\\tilde{\\eta}_{t}\\hat{\\eta}_{t+\\tau}(\\tilde{X}_{t+\\tau}(\\gamma)\\left\\|\\right\\|^{2}\\right]\\mathrm{d}\\tau^{\\prime}\\right]}\\\\ &{\\leq\\left(1+\\frac{3}{2}\\right)\\int_{0}^{\\tau}\\left[\\mathbb{E}\\left\\|\\nabla u_{t}(\\gamma)\\hat{\\pi}_{t+\\tau}(\\gamma)\\left(\\tilde{X}_{t+\\tau+\\tau}(\\gamma)-\\mathbb{E}\\log\\tilde{\\eta}_{t+\\tau}(\\gamma)\\right)\\right\\|^{2}\\mathrm{d}\\tau^{\\prime}\\right]}\\\\ &{+\\frac{\\displaystyle{\\int_{0}^{\\tau}\\hat{h}_{t}}}{2}\\int_{0}^{\\tau}\\left[\\left\\|\\tilde{\\eta}_{t+\\tau+\\tau}^{(k+1)}-\\tilde{\\pi}_{t+\\tau+\\tau}(\\gamma)\\right\\|^{2}\\right]\\mathrm{d}\\tau^{\\prime}+\\frac{h_{m}}{2}\\left(\\tau\\delta_{m_\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where the last equality is by Assumption 3.1'. ", "page_idx": 32}, {"type": "text", "text": "Applying Gronwall's inequality, we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\left[\\left\\|\\widetilde{y}_{t_{n},\\tau}^{(k+1)}-\\widetilde{x}_{t_{n}+\\tau}\\right\\|^{2}\\right]}\\\\ &{\\le\\!\\frac{e^{\\left(1+\\frac{3}{2h_{n}}\\right)\\tau}L_{s}^{2}h_{n}}{2}\\int_{0}^{\\tau}\\mathbb{E}\\left[\\left\\|\\widetilde{y}_{t_{n},g_{n}(\\tau^{\\prime})}^{(k)}-\\widetilde{x}_{t_{n}+g_{n}(\\tau^{\\prime})}\\right\\|^{2}\\right]\\mathrm{d}\\tau^{\\prime}+\\frac{e^{\\left(1+\\frac{3}{2h_{n}}\\right)\\tau}h_{n}}{2}\\Big(\\tau\\delta_{\\infty}^{2}+\\mathbb{E}[D_{t_{n}}]\\Big)}\\\\ &{\\le\\!\\frac{\\tau e^{\\left(1+\\frac{3}{2h_{n}}\\right)\\tau}L_{s}^{2}h_{n}}{2}\\operatorname*{sup}_{\\tau^{\\prime}\\in[0,\\tau]}\\mathbb{E}\\left[\\left\\|\\widetilde{y}_{t_{n},\\tau^{\\prime}}^{(k)}-\\widetilde{x}_{t_{n}+\\tau^{\\prime}}\\right\\|^{2}\\right]+\\frac{e^{\\left(1+\\frac{3}{2h_{n}}\\right)\\tau}h_{n}}{2}\\Big(\\tau\\delta_{\\infty}^{2}+\\mathbb{E}[D_{t_{n}}]\\Big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "and by taking supremum ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\underset{\\tau\\in[0,h_{n}]}{\\operatorname*{sup}}\\mathbb{E}\\left[\\left\\|\\widetilde{\\pmb{y}}_{t_{n},\\tau}^{(k+1)}-\\ddot{\\pmb{x}}_{t_{n}+\\tau}\\right\\|^{2}\\right]}\\\\ &{\\leq\\!\\frac{h_{n}^{2}e^{h_{n}+\\frac{3}{2}}L_{s}^{2}}{2}\\underset{\\tau^{\\prime}\\in[0,\\tau]}{\\operatorname*{sup}}\\mathbb{E}\\left[\\left\\|\\widetilde{\\pmb{y}}_{t_{n},\\tau^{\\prime}}^{(k)}-\\ddot{\\pmb{x}}_{t_{n}+\\tau^{\\prime}}\\right\\|^{2}\\right]+\\frac{e^{h_{n}+\\frac{3}{2}}h_{n}}{2}\\Big(h_{n}\\delta_{\\infty}^{2}+\\mathbb{E}[D_{t_{n}}]\\Big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Given that constant $h_{n}$ is sufficiently small, which ensures $L_{s}^{2}h_{n}e^{\\frac{5}{2}h_{n}}\\,\\ll\\,1\\$ , iterating the above inequality for $k\\in[0:K-1]$ gives us that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\tau\\in[0,h_{n}]}{\\operatorname*{sup}}\\mathbb{E}\\left[\\left\\|\\tilde{\\mathcal{Y}}_{t_{n},\\tau}^{(K)}-\\tilde{x}_{t_{n}+\\tau}\\right\\|^{2}\\right]}\\\\ &{\\leq\\left(\\frac{h_{n}^{2}e^{h_{n}+\\frac{3}{2}}L_{s}^{2}}{2}\\right)^{K}\\underset{\\tau\\in[0,h_{n}]}{\\operatorname*{sup}}\\mathbb{E}\\left[\\left\\|\\tilde{\\mathcal{Y}}_{t_{n},\\tau}^{(0)}-\\tilde{x}_{t_{n}+\\tau}\\right\\|^{2}\\right]+\\frac{e^{h_{n}+\\frac{3}{2}}h_{n}/2}{1-h_{n}^{2}e^{h_{n}+\\frac{3}{2}}L_{s}^{2}/2}\\Big(h_{n}\\delta_{\\infty}^{2}+\\mathbb{E}[D_{t_{n}}]\\Big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Notice that by Lemma A.8, we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\left\\Vert\\widehat{\\pmb{y}}_{t_{n},\\tau}^{(0)}-\\bar{\\pmb{x}}_{t_{n}+\\tau}\\right\\Vert^{2}\\right]=\\mathbb{E}\\left[\\left\\Vert\\bar{\\pmb{x}}_{t_{n}}-\\bar{\\pmb{x}}_{t_{n}+\\tau}\\right\\Vert^{2}\\right]\\leq3d,}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "substituting which into (C.9) then gives us that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\epsilon[0,h_{n}]}\\mathbb{E}\\left[\\left\\|\\tilde{y}_{t_{n},\\tau}^{(K)}-\\tilde{x}_{t_{n}+\\tau}\\right\\|^{2}\\right]\\leq3d\\left(\\frac{h_{n}^{2}e^{h_{n}+\\frac{3}{2}}L_{s}^{2}}{2}\\right)^{K}+\\frac{e^{h_{n}+\\frac{3}{2}}h_{n}/2}{1-h_{n}^{2}e^{h_{n}+\\frac{3}{2}}L_{s}^{2}/2}\\Big(h_{n}\\delta_{\\infty}^{2}+\\mathbb{E}[D_{t_{n}}]\\Big),\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "as desired. ", "page_idx": 33}, {"type": "text", "text": "Now it remains to bound $\\ C_{t_{n}}$ and $D_{t_{n}}$ in Lemma C.4. We frst bound $D_{t_{n}}$ using the following lemma: ", "page_idx": 33}, {"type": "text", "text": "Lemma C.5. For any $n\\in[0:N-1]$ we have that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[D_{t_{n}}\\right]\\lesssim d\\epsilon^{2}h_{n}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Proof. For any $n\\in[0:N-2]$ , we have $T-t_{n+1}\\gtrsim\\mathcal{O}(1)$ and thus by [102, Corollary 1] that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\left\\|\\nabla\\log\\bar{p}_{t_{N-1}+\\tau_{n,m}}(\\bar{\\mathbf{x}}_{t_{N-1}+\\tau_{n,m}})-\\nabla\\log\\bar{p}_{t_{N-1}+\\tau^{\\prime}}(\\bar{\\mathbf{x}}_{t_{N-1}+\\tau^{\\prime}})\\right\\|^{2}\\right]\\lesssim d\\epsilon_{n,m}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "for any $\\tau^{\\prime}\\in[\\tau_{n,m},\\tau_{n,m+1}]$ and thus ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\left[D_{t_{n}}\\right]=\\int_{0}^{h_{n}}\\mathbb{E}\\left[\\left\\|\\nabla\\log\\breve{p}_{t_{n}+g_{n}(\\tau^{\\prime})}(\\breve{{\\mathbf{x}}}_{t_{n}+g_{n}(\\tau^{\\prime})})-\\nabla\\log\\breve{p}_{t_{n}+\\tau^{\\prime}}(\\breve{{\\mathbf{x}}}_{t_{n}+\\tau^{\\prime}})\\right\\|^{2}\\right]\\mathrm{d}\\tau^{\\prime}}\\\\ &{=\\!\\displaystyle\\sum_{m=0}^{M_{n}}\\int_{\\tau_{n,m}}^{\\tau_{n,m}+1}\\!\\mathbb{E}\\left[\\left\\|\\nabla\\log\\breve{p}_{t_{n}+\\tau_{n,m}}(\\breve{{\\mathbf{x}}}_{t_{n}+\\tau_{n,m}})-\\nabla\\log\\breve{p}_{t_{n}+\\tau^{\\prime}}(\\breve{{\\mathbf{x}}}_{t_{n}+\\tau^{\\prime}})\\right\\|^{2}\\right]\\mathrm{d}\\tau^{\\prime}}\\\\ &{\\lesssim\\displaystyle\\sum_{m=0}^{M_{n}}d\\epsilon_{n,m}^{2}\\epsilon_{n,m}\\leq d\\epsilon^{2}h_{n}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "For $n=N-1$ , notice that by the step size schedule (cf. Section 3.1.1) and suppose $\\epsilon\\leq1/2$ we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\frac{T-\\tau}{2}\\leq T-g_{n}(\\tau)\\leq T-\\tau,\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "and then again [102, Corollary 1] states ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\Vert\\nabla\\log\\breve{p}_{t_{n}+\\epsilon_{n,m}}(\\bar{x}_{t_{n}+\\epsilon_{n,m}})-\\nabla\\log\\breve{p}_{t_{n}+\\tau^{\\prime}}(\\bar{x}_{t_{n}+\\tau^{\\prime}})\\right\\Vert^{2}\\right]\\lesssim\\frac{d\\epsilon_{n,m}^{2}}{T-\\tau_{n,m}},\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "and thus ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[D_{t_{N-1}}\\right]=\\int_{0}^{h_{N-1}}\\mathbb{E}\\left[\\left\\|\\nabla\\log\\tilde{p}_{t_{N-1}+g_{n}(\\tau^{\\prime})}(\\tilde{\\mathbf{x}}_{t_{N-1}+g_{n}(\\tau^{\\prime})})-\\nabla\\log\\tilde{p}_{t_{N-1}+\\tau^{\\prime}}(\\tilde{\\mathbf{x}}_{t_{N-1}+\\tau^{\\prime}})\\right\\|^{2}\\right]\\mathrm{d}\\tau}\\\\ &{=\\displaystyle\\sum_{m=0}^{M_{N-1}}\\int_{\\tau_{n,m}}^{\\tau_{n,m+1}}\\mathbb{E}\\left[\\left\\|\\nabla\\log\\tilde{p}_{t_{N-1}+\\tau_{n,m}}(\\tilde{\\mathbf{x}}_{t_{N-1}+g_{n}(\\tau^{\\prime})})-\\nabla\\log\\tilde{p}_{t_{N-1}+\\tau^{\\prime}}(\\tilde{\\mathbf{x}}_{t_{N-1}+\\tau^{\\prime}})\\right\\|^{2}\\right]\\mathrm{d}\\tau^{\\prime}}\\\\ &{\\displaystyle\\sum_{m=0}^{M_{N-1}}\\frac{d\\epsilon_{n,m}^{2}}{T-\\tau_{n,m}}\\epsilon_{n,m}\\leq\\displaystyle\\sum_{m=0}^{M_{N-1}}d\\epsilon_{n,m}^{2}\\epsilon\\lesssim\\int_{\\delta_{\\infty}}^{T-t_{N-1}}d\\tau\\mathrm{d}\\tau\\lesssim d\\epsilon^{2}h_{N-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Remark C.6. The above lemma is able to achieve a better dependency on E compared to Lemma B.7, because the backward process $(\\overleftarrow{\\pmb{x}}_{t})_{t\\in[0,T]}$ is now a deterministic process in the probability flow ODE formulation, instead of a stochastic process as in the SDE formulation as in Lemma B.7. Thus, intuitively applying Cauchy-Schwarz rather than Ito symmetry gives us a ${\\mathcal{O}}(\\epsilon^{2})$ -dependencyrather than $O(\\epsilon)$ -dependency. ", "page_idx": 34}, {"type": "text", "text": "Theorem C.7. Under Assumptions 3.1', 3.2, 3.3, and 3.4, then the distribution $\\widetilde{q}_{t_{n},h_{n}}$ that the parallelized predictor step generates samples from satisfies the following error bound: ", "page_idx": 34}, {"type": "equation", "text": "$$\nW_{2}(\\tilde{q}_{t_{n},h_{n}},\\tilde{p}_{t_{n+1}})^{2}\\lesssim d e^{-K}+h_{n}^{2}\\delta_{\\infty}^{2}+d\\epsilon^{2}h_{n}^{2},\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "for $n\\in[0:N-1]$ ", "page_idx": 34}, {"type": "text", "text": "Prof ytedeiofWaerstndistae wehaeraycf and tn+hn' ", "page_idx": 34}, {"type": "equation", "text": "$$\nW_{2}(\\widetilde{q}_{t_{n},h_{n}},\\widetilde{p}_{t_{n+1}})^{2}\\leq\\mathbb{E}\\left[\\left\\|\\widetilde{\\pmb{y}}_{t_{n},h_{n}}^{(K)}-\\widetilde{\\pmb{x}}_{t_{n}+h_{n}}\\right\\|^{2}\\right],\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "and therefore ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad W_{2}(\\widetilde{q}_{t_{n},h_{n}},\\widetilde{p}_{t_{n+1}})^{2}\\le\\mathbb{E}\\left[\\left\\|\\widetilde{y}_{t_{n},h_{n}}^{(K)}-\\widetilde{x}_{t_{n}+h_{n}}\\right\\|^{2}\\right]\\le\\operatorname*{sup}_{\\tau\\in[0,h_{n}]}\\mathbb{E}\\left[\\left\\|\\widetilde{y}_{t_{n},\\tau}^{(K)}-\\widetilde{x}_{t_{n}+\\tau}\\right\\|^{2}\\right]}\\\\ &{\\le3d\\left(\\frac{h_{n}^{2}e^{h_{n}+\\frac{3}{2}}L_{s}^{2}}{2}\\right)^{K}+\\frac{e^{h_{n}+\\frac{3}{2}}h_{n}/2}{1-h_{n}^{2}e^{h_{n}+\\frac{3}{2}}L_{s}^{2}/2}\\Big(h_{n}\\delta_{\\infty}^{2}+\\mathbb{E}[D_{t_{n}}]\\Big)}\\\\ &{\\lesssim\\!d e^{-K}+h_{n}^{2}\\delta_{\\infty}^{2}+d\\epsilon^{2}h_{n}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where for the second to last inequality we used Lemma C.4, the last inequality is due to Lemma C.5 and the assumption $h_{n}^{2}e^{h_{n}}L_{s}^{2}\\ll1$ \u53e3 ", "page_idx": 34}, {"type": "text", "text": "C.3 Parallelized Corrector Step ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "After each predictor step, we run the corrector step for $\\mathcal{O}(1)$ time to reduce the error. Particularly, we apply the Parallelized underdamped Langevin dynamics algorithm [121] to the corrector step, which yields $\\mathcal{O}(1)$ approximate time complexity compared to the ordinary implementation of the ULMC dynamics as in [102]. In the following, we will drop the dependency on $\\omega$ for notational simplicity, and we refer readers to Appendix A.2 and B.2 to review the change of measure arguments and the application of Girsanov's theorem A.4. We will also use a general notation $*^{\\dagger}$ to distinguish the time in the backward process and the inner time in the corrector step of the $n$ -thblock. ", "page_idx": 34}, {"type": "text", "text": "We first define the true underdamped Langevin dynamics $(\\mathbf{\\boldsymbol{u}}_{t_{n},t^{\\dagger}},\\mathbf{\\boldsymbol{v}}_{t_{n},t^{\\dagger}})_{t\\geq0}$ ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\int\\!\\mathrm{d}\\boldsymbol{u}_{t_{n},t^{\\dagger}}=\\boldsymbol{v}_{t_{n},t^{\\dagger}}\\mathrm{d}t^{\\dagger}}\\\\ {\\int\\!\\mathrm{d}\\boldsymbol{v}_{t_{n},t^{\\dagger}}=-\\gamma\\boldsymbol{v}_{t_{n},t^{\\dagger}}\\mathrm{d}t^{\\dagger}-\\nabla\\log\\overleftarrow{p}_{t_{n+1}}(\\boldsymbol{u}_{t_{n},t^{\\dagger}})\\mathrm{d}t^{\\dagger}+\\sqrt{2\\gamma}\\mathrm{d}b_{t_{n},t^{\\dagger}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "with initial condition Utn,0 = ytn,hn from the predictor step and $\\pmb{v}_{t_{n},0}~\\sim~\\mathcal{N}(0,\\pmb{I}_{d})$ \uff0cwhere $(b_{t_{n},t^{\\dagger}})_{t\\geq0}$ is a Wiener process. We may also write the system of SDEs above in the following matrixform: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathrm{d}\\left[\\!\\!\\begin{array}{c}{u_{t_{n},t^{\\dagger}}}\\\\ {v_{t_{n},t^{\\dagger}}}\\end{array}\\!\\!t\\right]=\\left[\\!\\!\\left[\\!\\begin{array}{c c}{0}&{I_{d}}\\\\ {0}&{-\\gamma I_{d}}\\end{array}\\!\\!\\right]\\left[\\!\\!u_{t_{n},t^{\\dagger}}\\!\\!\\right]-\\left[\\!\\!\\nabla\\log\\bar{p}_{t_{n+1}}(u_{t_{n},t^{\\dagger}})\\!\\!\\right]\\!\\!\\right]\\mathrm{d}t^{\\dagger}+\\left[\\!\\!\\begin{array}{c c}{0}&{\\!\\!\\!\\mathbf{0}}\\\\ {0}&{\\!\\!\\sqrt{2\\gamma}I_{d}}\\end{array}\\!\\!\\right]\\mathrm{d}\\left[\\!\\!\\begin{array}{c}{b_{t_{n},t^{\\dagger}}^{\\prime}}\\\\ {b_{t_{n},t^{\\dagger}}}\\end{array}\\!\\!\\right].\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "We run this underdamped Langevin dynamics until the pre-determined time horizon $T^{\\dagger}$ .We also define the joint probability distribution of $({\\boldsymbol{u}}_{t_{n},t^{\\dagger}},{\\boldsymbol{v}}_{t_{n},t^{\\dagger}})$ at time $t$ as $\\pi_{t_{n},t^{\\dagger}}\\big({\\boldsymbol{u}}_{t_{n},t^{\\dagger}},{\\boldsymbol{v}}_{t_{n},t^{\\dagger}}\\big)$ and its marginal on $\\pmb{u}_{t_{n},t^{\\dagger}}$ as $\\pi_{t_{n},t^{\\dagger}}^{\\pmb{u}}\\left(\\pmb{u}_{t_{n},t^{\\dagger}}\\right)$ ", "page_idx": 34}, {"type": "text", "text": "Similar to the parallelizing strategy in Section 3.1.1, we discretize the time interval $[0,T^{\\dagger}]$ into $N^{\\dagger}$ blocks with length $h^{\\dagger}=T^{\\dagger}/N^{\\dagger}$ . Within the $n$ -th block, we further divide the block $[n^{\\dagger}h^{\\dagger},(n{+}1)h^{\\dagger}]$ into $M^{\\dagger}$ steps, each with step size $\\epsilon^{\\dagger}=h^{\\dagger}/M^{\\dagger}$ ", "page_idx": 34}, {"type": "text", "text": "Definition C.8 (Auxiliary corrector process). For any ${\\boldsymbol n}^{\\dagger}\\,\\in\\,[0:N^{\\dagger}-1]$ wedefinetheauxiliary $(\\widehat{\\pmb{u}}_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(k^{\\dagger})})_{\\tau^{\\dagger}\\in[0,h^{\\dagger}]}$ $k^{\\dagger}\\ \\in$ $[0:K^{\\dagger}-1]$ ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\int\\!\\mathrm{d}\\widehat{u}_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(k+1)}=\\widehat{v}_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(k+1)}\\mathrm{d}\\tau^{\\dagger},}\\\\ &{\\int\\!\\mathrm{d}\\widehat{v}_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(k+1)}=-\\gamma\\widehat{v}_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(k+1)}\\mathrm{d}\\tau^{\\dagger}-s_{t_{n+1}}\\big(\\widehat{u}_{t_{n},n^{\\dagger}h^{\\dagger},g_{n}(\\tau^{\\dagger})}^{(k^{\\dagger})}\\big)\\mathrm{d}\\tau^{\\dagger}+\\sqrt{2\\gamma}\\mathrm{d}b_{t_{n},n^{\\dagger}h^{\\dagger}+\\tau^{\\dagger}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "with theinitial condition ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\{\\widehat{u}_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(0)}\\equiv\\widehat{u}_{t_{n},n^{\\dagger}h^{\\dagger}}\\right.\\quad_{f o r}\\tau^{\\dagger}\\in[0,h^{\\dagger}],\\;a n d\\left.\\left\\{\\widehat{u}_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(k^{\\dagger})}\\equiv\\widehat{u}_{t_{n},n^{\\dagger}h^{\\dagger}}\\right.\\quad_{f o r}k\\in[1:K^{\\dagger}],}\\\\ {\\left.\\widehat{v}_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(0)}=\\widehat{v}_{t_{n},n^{\\dagger}h^{\\dagger}}\\right.\\quad}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\widehat{\\pmb{u}}_{t_{n},n^{\\dagger}h^{\\dagger}}:=\\widehat{\\pmb{u}}_{t_{n},(n^{\\dagger}-1)h^{\\dagger},h^{\\dagger}}^{(K^{\\dagger})},\\quad\\widehat{\\pmb{v}}_{t_{n},n^{\\dagger}h^{\\dagger}}:=\\widehat{\\pmb{v}}_{t_{n},(n^{\\dagger}-1)h^{\\dagger},h^{\\dagger}}^{(K^{\\dagger})}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "for $n^{\\dag}\\in[1:N^{\\dag}-1]$ , and ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\widehat{\\pmb{u}}_{t_{n},0}=\\pmb{y}_{t_{n},h_{n}}^{(K)},\\quad\\widehat{\\pmb{v}}_{t_{n},0}\\sim\\mathcal{N}(0,\\pmb{I}_{d}).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "We define the joint probability distribution of $(\\widehat{\\pmb{u}}_{t_{n},t^{\\dag}},\\widehat{\\pmb{v}}_{t_{n},t^{\\dag}})$ at time $t$ as $\\widehat{\\pi}_{t_{n},t^{\\dagger}}\\big(\\widehat{\\pmb{u}}_{t_{n},t^{\\dagger}},\\widehat{\\pmb{v}}_{t_{n},t^{\\dagger}}\\big)$ and its marginal on $\\widehat{\\pmb{u}}_{t_{n},t^{\\dag}}$ as $\\widehat{\\pi}_{t_{n},t^{\\dagger}}^{\\widehat{\\mathbf{u}}}(\\widehat{\\pmb{u}}_{t_{n},t^{\\dagger}})$ Wewillldentetheresulin probabilty distrtf $\\widehat{\\pi}_{t_{n},T^{\\dag}}^{\\widehat{\\pmb u}}\\;a s\\;\\widehat{q}_{t_{n+1}}$ ", "page_idx": 35}, {"type": "text", "text": "Lemma C.9 (Equivalence between (C.2) and (C.11). For any $n^{\\dag}\\in[0:N^{\\dag}-1]$ theupdaterule in Algorithm 2 is equivalent to the exact solution of the auxiliary process $(C.I I)$ forany $k^{\\dagger}\\in[0:$ $K^{\\dagger}-1]$ and $\\tau^{\\dagger}\\in[0,h^{\\dagger}]$ ", "page_idx": 35}, {"type": "text", "text": "Proof. Without loss of generality, we will prove the lemma for $m^{\\dagger}=M^{\\dagger}$ . The proof for $m^{\\dag}\\in[0:$ $M^{\\dagger}-1]$ can be done similarly. ", "page_idx": 35}, {"type": "text", "text": "We first rewrite (C.2) into the matrix form: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{d}\\left[\\widetilde{u}_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(k^{\\dagger})}\\right]=\\left[\\left[\\mathbf{0}\\quad\\:\\:I_{d}\\right]\\left[\\widetilde{u}_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(k^{\\dagger})}\\right]-\\left[s_{t_{n+1}}\\!\\left(\\widetilde{u}_{t_{n},n^{\\dagger}h^{\\dagger},g_{n}(\\tau^{\\dagger})}^{(k^{\\dagger})}\\right)\\right]\\right]\\mathrm{d}\\tau^{\\dagger}}\\\\ &{+\\left[\\mathbf{0}\\quad\\:\\:\\left[\\mathbf{0}\\quad\\:\\:\\left[\\mathbf{0}\\right]\\mathrm{d}\\left[\\widetilde{b}_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(k^{\\dagger})}\\right]+\\tau^{\\dagger}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Define the time-dependent matrix $G(\\cdot)$ as ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\pmb{G}(t^{\\dagger}):=\\left[\\pmb{I}_{d}\\quad\\frac{1-e^{-\\gamma t^{\\dagger}}}{\\gamma}\\pmb{I}_{d}\\right]=\\exp\\left(\\pmb{\\left(\\begin{array}{c c}{\\mathbf{0}}&{I_{d}}\\\\ {\\mathbf{0}}&{-\\gamma I_{d}}\\end{array}\\right)}t^{\\dagger}\\right),\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "satisfying that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t^{\\dagger}}G(t^{\\dagger})=\\left[\\mathbf{0}\\mathbf{\\quad}I_{d}\\right]G(t^{\\dagger})=G(t^{\\dagger})\\left[\\mathbf{0}\\mathbf{\\quad}I_{d}\\right].\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Now we multiply $G(-\\tau^{\\dagger})$ on both sides of (C.13) to obtain: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathrm{d}\\left(G(-\\tau^{\\dagger})\\left[\\begin{array}{c}{\\tilde{u}_{t_{n,n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}}^{(k^{\\dagger})}}\\\\ {\\tilde{v}_{t_{n,n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}}^{(k^{\\dagger})}}\\end{array}\\right]\\right)=-\\;G(-\\tau^{\\dagger})\\left[s_{t_{n+1}}(\\tilde{u}_{t_{n,n^{\\dagger}h^{\\dagger},g_{n}(\\tau^{\\dagger})}}^{(k^{\\dagger})})\\right]\\mathrm{d}\\tau^{\\dagger}}\\\\ &{}&{+\\;G(-\\tau^{\\dagger})\\left[\\mathbf{0}\\quad\\begin{array}{c}{\\mathbf{0}}\\\\ {\\sqrt{2\\gamma}I_{d}}\\end{array}\\right]\\mathrm{d}\\left[b_{t_{n,n^{\\dagger}h^{\\dagger}+\\tau^{\\dagger}}}^{\\prime}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Integrating on both sides from O to $h^{\\dagger}$ and multiplying $G(h^{\\dagger})$ on both sides, we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\overline{{u}}_{t+\\tau/\\tau+\\tau/\\tau}^{(\\tau)}\\right|=C(k)\\left|\\overline{{u}}_{t+\\tau/\\tau}^{(\\tau)}\\right|,}\\\\ &{\\left|\\overline{{v}}_{t+\\tau/\\tau+\\tau/\\tau}^{(\\tau)}\\right|,}\\\\ &{=-\\int_{0}^{t}G(k)^{\\prime}-\\tau^{\\prime}\\right|\\left[\\phantom{\\frac{u}{b}}\\!\\!\\!\\!\\!\\!\\alpha_{t+\\tau/\\tau+\\tau/\\tau}^{(\\tau)}\\right]\\mathrm{d}\\tau^{\\prime\\prime}}\\\\ &{+\\int_{0}^{t}G(k)^{\\prime}-\\tau^{\\prime}\\right|\\left[\\phantom{\\frac{u}{b}}\\!\\!\\!\\!\\!\\alpha_{t-\\tau/\\tau}^{(\\tau)}\\mathrm{d}q_{t}\\right]\\left|\\overline{{u}}_{t+\\tau/\\tau+\\tau^{\\prime}}^{(\\tau)}\\right|}\\\\ &{=-\\frac{M^{2}}{\\sum_{\\tau=\\tau}^{\\tau}\\int_{0}^{\\tau+\\tau}\\mathrm{d}\\tau^{\\prime}}G(k^{\\prime}-\\tau^{\\prime})\\mathrm{d}\\tau^{\\prime\\prime}\\left[\\phantom{\\frac{u}{b}}\\!\\!\\!\\!\\!\\alpha_{t+\\tau}^{(\\tau)}\\left(u_{t-\\tau/\\tau-\\tau}^{(\\tau)}\\right)\\right]}\\\\ &{+\\frac{M^{2}}{\\sum_{\\tau=\\tau}^{\\tau}\\int_{0}^{\\tau+\\tau}\\mathrm{d}\\tau^{\\prime\\prime}}\\frac{1}{\\alpha}\\alpha(\\beta^{\\prime}-\\tau^{\\prime\\prime})\\mathrm{d}\\tau^{\\prime\\prime}\\left[\\phantom{\\frac{u}{b}}\\!\\!\\!\\!\\!\\alpha_{t+\\tau}^{(\\tau)}\\left(u_{t-\\tau/\\tau-\\tau}^{(\\tau)}\\right)\\right]}\\\\ &{\\overset{(a)}{\\leq}\\frac{M^{2}}{M^{2}}\\mathrm{d}\\tau^{\\prime\\prime}\\left[\\phantom{\\frac{u}{b}}\\!\\!\\!\\!\\!\\alpha_{t+\\tau}^{(\\tau)}\\mathrm{d}\\tau^{\\prime\\prime}\\right]^{\\prime}\\left[0\\!\\!\\!\\!\\!\\alpha_{t-\\tau}^{(\\tau)}\\frac{1}{\\alpha}\\alpha(\\beta_{t}-u_{t+\\tau/\\tau}^{(\\tau)})\\right]}\\\\ &{=-\\frac{M^{2}}{\\sum_{\\tau=\\tau}^{\\tau}\\int_{0}^{ \n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "By Ito isometry, we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\int_{m^{\\uparrow}\\kappa^{\\uparrow}}^{(m^{\\uparrow}+1)\\epsilon^{\\uparrow}}G(h^{\\dagger}-\\tau^{\\uparrow^{\\prime}})\\left[\\begin{array}{l l}{0}&{0}\\\\ {0}&{\\sqrt{2\\gamma}I_{d}\\right]\\mathrm{d}\\left[b_{t_{n,n^{\\uparrow}}h^{\\dagger}+\\tau^{\\uparrow^{\\prime}}}^{\\prime}\\right]}\\\\ {\\sim\\Lambda^{\\prime}\\Bigg(0,\\left[\\begin{array}{l l}{0}&{0}\\\\ {0}&{\\sqrt{2\\gamma I_{d}}\\right]\\,G((M^{\\uparrow}-m^{\\dagger}-1)\\epsilon^{\\dagger})^{\\top}\\left(G(\\epsilon^{\\dagger})-I_{d}\\right)^{\\top}}\\\\ &{\\qquad\\qquad\\qquad\\quad(G(\\epsilon^{\\dagger})-I_{d})\\,G((M^{\\dagger}-m^{\\dagger}-1)\\epsilon^{\\dagger})\\left[\\begin{array}{l l}{0}&{0}\\\\ {0}&{\\sqrt{2\\gamma I_{d}}}\\end{array}\\right]\\Bigg)}\\\\ &{\\sim\\Bigg[N\\left(0,2\\gamma(1+\\gamma^{-2})(1-e^{-\\gamma\\epsilon^{\\dagger}})^{2}e^{-2\\gamma(M^{\\dagger}-m^{\\dagger}+1)\\epsilon^{\\dagger}})I_{d}\\right)\\Bigg]\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "as desired ", "page_idx": 36}, {"type": "text", "text": "Definition C.10 (Interpolating corrector process). For any $n^{\\dag}\\in[0:N^{\\dag}-1]$ we define the interpolatingcorectorproces $(\\widehat{\\pmb{u}}_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(k^{\\dagger})})_{\\tau^{\\dagger}\\in[0,h^{\\dagger}]}$ as the solution to the fllowing SDE recursively for $k^{\\dagger}\\in[0:K^{\\dagger}-1]$ ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\int\\mathrm{d}\\widetilde{u}_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(k+1)}=\\widetilde{v}_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(k+1)}\\mathrm{d}\\tau^{\\dagger},}\\\\ &{\\quad\\mathrm{d}\\widetilde{v}_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(k+1)}=-\\gamma\\widetilde{v}_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(k+1)}\\mathrm{d}\\tau^{\\dagger}-s_{t_{n+1}}\\big(\\widetilde{u}_{t_{n},n^{\\dagger}h^{\\dagger},g_{n}(\\tau^{\\dagger})}^{(k^{\\dagger})}\\big)\\mathrm{d}\\tau^{\\dagger}+\\sqrt{2\\gamma}\\mathrm{d}b_{t_{n},n^{\\dagger}h^{\\dagger}+\\tau^{\\dagger}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "withtheinitial condition ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\widetilde{u}_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(0)}\\equiv\\widetilde{u}_{t_{n},n^{\\dagger}h^{\\dagger}}}&{\\:\\:f o r\\:\\tau^{\\dagger}\\in[0,h^{\\dagger}],\\:a n d\\left\\{\\begin{array}{l l}{\\widetilde{u}_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(k^{\\dagger})}\\equiv\\widetilde{u}_{t_{n},n^{\\dagger}h^{\\dagger}}}&{\\:\\:f o r\\:k\\in[1:K^{\\dagger}],}\\\\ {\\widetilde{v}_{t_{n},n^{\\dagger}h^{\\dagger},0}^{(k^{\\dagger})}\\equiv\\widetilde{v}_{t_{n},n^{\\dagger}h^{\\dagger}}}&{\\:\\:f o r\\:k\\in[1:K^{\\dagger}],}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\widetilde{\\pmb{u}}_{t_{n},n^{\\dagger}h^{\\dagger}}:=\\widetilde{\\pmb{u}}_{(n^{\\dagger}-1)h^{\\dagger},h^{\\dagger}}^{(K^{\\dagger})},\\quad\\widetilde{\\pmb{v}}_{t_{n},n^{\\dagger}h^{\\dagger}}:=\\widetilde{\\pmb{v}}_{(n^{\\dagger}-1)h^{\\dagger},h^{\\dagger}}^{(K^{\\dagger})}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "for $n^{\\dag}\\in[1:N^{\\dag}-1]$ and ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\widetilde{\\pmb u}_{t_{n},0}=\\widetilde{\\pmb y}_{t_{n},h_{n}}^{(K)},\\quad\\widetilde{\\pmb v}_{t_{n},0}\\sim\\mathcal{N}(0,\\pmb I_{d}).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "We define the joint probability distribution of $(\\widetilde{\\pmb{u}}_{t_{n},t^{\\dagger}},\\widetilde{\\pmb{v}}_{t_{n},t^{\\dagger}})$ at time $t$ as $\\widetilde{\\pi}_{t_{n},t^{\\dagger}}(\\widetilde{\\pmb{u}}_{t_{n},t^{\\dagger}},\\widetilde{\\pmb{v}}_{t_{n},t^{\\dagger}})$ and its marginal on $\\widetilde{\\pmb{u}}_{t_{n},t^{\\dagger}}$ $\\widetilde{\\pi}_{t_{n},t^{\\dagger}}^{\\tilde{\\mathbf{u}}}\\big(\\widetilde{\\pmb{u}}_{t_{n},t^{\\dagger}}\\big)$ ", "page_idx": 37}, {"type": "text", "text": "We invoke Girsanov's theorem (Theorem A.4) again by the following procedure ", "page_idx": 37}, {"type": "text", "text": "1. Setting (A.2) as the auxiliary process (C.15) at iteration $K^{\\dagger}$ \uff0cwhere $b_{t_{n},t^{\\dagger}}(\\omega)$ is a Wiener process under the measure $Q$ ", "page_idx": 37}, {"type": "text", "text": "2. Defining another process $\\widetilde{b}_{t_{n},n^{\\dagger}h^{\\dagger}+\\tau^{\\dagger}}$ governed by the following SDE: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathrm{d}\\widetilde{b}_{t_{n},n^{\\dagger}h^{\\dagger}+\\tau^{\\dagger}}=\\mathrm{d}b_{t_{n},n^{\\dagger}h^{\\dagger}+\\tau^{\\dagger}}-\\phi_{t_{n},n^{\\dagger}h^{\\dagger}}(\\tau^{\\dagger})\\mathrm{d}\\tau^{\\dagger},\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\phi_{t_{n},n^{\\dagger}h^{\\dagger}}(\\tau^{\\dagger})=\\frac{1}{\\sqrt{2\\gamma}}\\left(s_{t_{n+1}}(\\widetilde{u}_{t_{n},n^{\\dagger}h^{\\dagger},\\lfloor\\frac{\\tau^{\\dagger}}{e^{\\dagger}}\\rfloor\\epsilon^{\\dagger}}^{(K^{\\dagger}-1)})-\\nabla\\log\\widetilde{p}_{t_{n+1}}(\\widetilde{u}_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(K^{\\dagger})})\\right)\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "and computing the Radon-Nikodym derivative of the measure $P$ with respect to $Q$ as ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}P}{\\mathrm{d}Q}=\\exp\\left(\\int_{0}^{h^{\\dagger}}\\phi_{t_{n},n^{\\dagger}h^{\\dagger}}(\\tau^{\\dagger})^{\\top}\\mathrm{d}b_{t_{n},n^{\\dagger}h^{\\dagger}+\\tau^{\\dagger}}-\\frac{1}{2}\\int_{0}^{h^{\\dagger}}\\|\\phi_{n h}(\\tau^{\\dagger})\\|^{2}\\mathrm{d}\\tau^{\\dagger}\\right);\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "3. Concluding that (C.15) at iteration $K^{\\dagger}$ under the measure $Q$ satisfies the following SDE: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\int\\mathrm{d}\\widetilde{u}_{n^{\\uparrow}h^{\\uparrow},\\tau^{\\uparrow}}^{(K^{\\uparrow})}=\\widetilde{v}_{n^{\\uparrow}h^{\\uparrow},\\tau^{\\uparrow}}^{(K^{\\uparrow})}\\mathrm{d}\\tau^{\\dagger}}\\\\ &{\\quad\\mathrm{d}\\widetilde{v}_{t_{n},n^{\\uparrow}h^{\\uparrow},\\tau^{\\uparrow}}^{(K^{\\uparrow})}=-\\gamma\\widetilde{v}_{t_{n},n^{\\uparrow}h^{\\uparrow},\\tau^{\\uparrow}}^{(K^{\\uparrow})}\\mathrm{d}\\tau^{\\dagger}-\\nabla\\log\\widetilde{p}_{t_{n+1}}(\\widetilde{u}_{n^{\\uparrow}h^{\\uparrow},\\tau^{\\uparrow}}^{(K^{\\uparrow})})\\mathrm{d}\\tau^{\\dagger}+\\sqrt{2\\gamma}\\mathrm{d}\\widetilde{b}_{t_{n},n^{\\uparrow}h^{\\uparrow}+\\tau^{\\uparrow}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "With $(\\widetilde{b}_{t_{n},n^{\\dagger}h^{\\dagger}+\\tau^{\\dagger}})_{\\tau^{\\dagger}\\geq0}$ being a Wiener process under the measure $P$ .If we replace ,t,by(+,+t),n shd ntic C0isily the original backward SDE (C.10) with the true score function on $t\\in[n^{\\dagger}h^{\\dagger},(n+1)h^{\\dagger}]$ ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\int\\!\\mathrm{d}u_{t_{n},n^{\\dagger}h^{\\dagger}+\\tau^{\\dagger}}=v_{t_{n},n^{\\dagger}h^{\\dagger}+\\tau^{\\dagger}}\\mathrm{d}\\tau^{\\dagger}}\\\\ {\\!\\!\\!\\!\\!\\!\\!\\!\\!\\int\\!\\mathrm{d}v_{t_{n},n^{\\dagger}h^{\\dagger}+\\tau^{\\dagger}}=-\\gamma v_{t_{n},n^{\\dagger}h^{\\dagger}+\\tau^{\\dagger}}\\mathrm{d}\\tau^{\\dagger}-\\nabla\\log\\tilde{p}_{t_{n+1}}(u_{t_{n},n^{\\dagger}h^{\\dagger}+\\tau^{\\dagger}})\\mathrm{d}\\tau^{\\dagger}+\\sqrt{2\\gamma}\\mathrm{d}\\tilde{b}_{t_{n},n^{\\dagger}h^{\\dagger}+\\tau^{\\dagger}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "We further define the joint probability distribution of $(\\boldsymbol{u}_{t_{n},t^{\\dagger}},\\boldsymbol{v}_{t_{n},t^{\\dagger}})$ at time $t$ as $\\pi_{t_{n},t^{\\dagger}}\\big({\\boldsymbol{u}}_{t_{n},t^{\\dagger}},{\\boldsymbol{v}}_{t_{n},t^{\\dagger}}\\big)$ and its margial on $\\pmb{u}_{t_{n},t^{\\dagger}}$ $\\pi_{t_{n},t^{\\dagger}}^{\\pmb{u}}(\\pmb{u}_{t_{n},t^{\\dagger}})$ ", "page_idx": 37}, {"type": "text", "text": "Remark C.11. The application of Girsanov's theorem A.4 is by writing the system of SDEs in the matrixform. ", "page_idx": 37}, {"type": "text", "text": "Definition C.12 (Stationary process). Under the $P$ -measurethat is defined by theRadon-Nikodym derivative (C.19), we may define a stationary underdamped Langevin process for $n^{\\dag}\\in[0:N^{\\dag}-1]$ and $\\tau^{\\dagger}\\in[0,h^{\\dagger}]$ as ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\!\\!\\!\\!\\!\\!\\int\\!\\mathrm{d}\\boldsymbol{u}_{t_{n},n^{\\dagger}h^{\\dagger}+\\tau^{\\dagger}}^{*}=\\boldsymbol{v}_{t_{n},n^{\\dagger}h^{\\dagger}+\\tau^{\\dagger}}^{*}\\mathrm{d}\\tau^{\\dagger},}\\\\ {\\!\\!\\!\\!\\!\\!\\int\\!\\mathrm{d}\\boldsymbol{v}_{t_{n},n^{\\dagger}h^{\\dagger}+\\tau^{\\dagger}}^{*}=-\\gamma\\boldsymbol{v}_{t_{n},n^{\\dagger}h^{\\dagger}+\\tau^{\\dagger}}^{*}\\mathrm{d}\\tau^{\\dagger}-\\nabla\\log\\tilde{p}_{t_{n+1}}(\\boldsymbol{u}_{n^{\\dagger}h^{\\dagger}+\\tau^{\\dagger}}^{*})\\mathrm{d}\\tau^{\\dagger}+\\sqrt{2\\gamma}\\mathrm{d}\\tilde{b}_{t_{n},n^{\\dagger}h^{\\dagger}+\\tau^{\\dagger}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "with the initial condition $\\pmb{u}_{t_{n},n^{\\dagger}h^{\\dagger}}^{*}\\sim\\overleftarrow{p}_{t_{n+1}}$ and $\\pmb{v}_{t_{n},n^{\\downarrow}h^{\\dagger}}^{*}\\sim\\mathcal{N}(0,\\pmb{I}_{d})$ We define the joint probability distribution of (utn,tt,v) at time $t$ as $\\pi_{t_{n},t^{\\dagger}}^{*}(\\pmb{u}_{t_{n},t^{\\dagger}}^{*},\\pmb{v}_{t_{n},t^{\\dagger}}^{*})$ and its marginal on $\\pmb{u}_{t_{n},t^{\\dag}}^{*}$ as $\\pi_{t_{n},t^{\\dagger}}^{*,u^{*}}(u_{t_{n},t^{\\dagger}}^{*})$ ", "page_idx": 37}, {"type": "text", "text": "Thus, from Corollary A.5, we have that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad D_{\\mathrm{KL}}(\\pi_{t_{n},n^{\\dagger}h^{\\dagger}}\\|\\widetilde{\\pi}_{t_{n},n^{\\dagger}h^{\\dagger}})}\\\\ &{\\le D_{\\mathrm{KL}}(\\pi_{t_{n},(n-1)h^{\\dagger}}\\|\\widetilde{\\pi}_{t_{n},(n-1)h^{\\dagger}})+\\displaystyle\\sum_{n=0}^{N^{\\dagger}-1}D_{\\mathrm{KL}}(\\pi_{t_{n},n^{\\dagger}h^{\\dagger};(n+1)h^{\\dagger}}\\|\\widetilde{\\pi}_{t_{n},n^{\\dagger}h^{\\dagger};(n+1)h^{\\dagger}})}\\\\ &{\\le D_{\\mathrm{KL}}(\\pi_{t_{n},(n-1)h^{\\dagger}}\\|\\widetilde{\\pi}_{t_{n},(n-1)h^{\\dagger}})}\\\\ &{+\\displaystyle\\frac{1}{4\\gamma}\\mathbb{E}_{P}\\left[\\int_{0}^{h^{\\dagger}}\\left\\|s_{t_{n+1}}(\\widetilde{\\boldsymbol{u}}_{t_{n},n^{\\dagger}h^{\\dagger},[\\frac{\\tau^{\\dagger}}{\\epsilon^{\\dagger}}]\\epsilon^{\\dagger}})-\\nabla\\log\\bar{p}_{t_{n+1}}(\\widetilde{\\boldsymbol{u}}_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(K^{\\dagger})})\\right\\|^{2}\\mathrm{d}\\tau^{\\dagger}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "By triangle inequality, we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad_{\\mathbf{S}}=\\int_{\\mathbb{R}}^{|\\mathbf{r}|}\\left[\\rho_{\\mathbf{r}_{1}}\\omega_{1}^{\\mathrm{\\tiny(E)}}(\\mathbf{r}_{2}^{\\top}-\\mathbf{r}_{3})\\left(\\mathbf{r}_{3}-\\rho_{\\mathbf{r}_{1}}\\omega_{1}^{\\mathrm{\\tiny(E)}}(\\mathbf{r}_{3}^{\\top})\\mathbf{\\hat{g}}_{1}^{\\mathrm{\\tiny(E)}}\\right)^{2}\\right]d t^{2}}\\\\ &{\\quad+\\int_{\\mathbb{R}}^{|\\mathbf{r}|}\\left[\\rho_{\\mathbf{r}_{2}}\\omega_{1}^{\\mathrm{\\tiny(E)}}(\\mathbf{r}_{3}^{\\top}-\\mathbf{r}_{3})(\\mathbf{r}_{2}^{\\top}-\\mathbf{r}_{3})\\left(\\mathbf{r}_{3}^{\\top}-\\rho_{\\mathbf{r}_{1}}\\omega_{1}^{\\mathrm{\\tiny(E)}}(\\mathbf{r}_{3}^{\\top})\\mathbf{\\hat{g}}_{2}^{\\mathrm{\\tiny(E)}}\\right)^{2}\\right]d t^{2}}\\\\ &{\\quad+\\int_{\\mathbb{R}}^{|\\mathbf{r}|}\\left[\\rho_{\\mathbf{r}_{3}}\\omega_{1}^{\\mathrm{\\tiny(E)}}(\\mathbf{r}_{3}^{\\top}-\\mathbf{r}_{3})(\\mathbf{r}_{3}^{\\top}-\\mathbf{r}_{3})(\\mathbf{r}_{3}^{\\top}-\\rho_{\\mathbf{r}_{1}}(\\mathbf{r}_{3}^{\\top}+\\mathbf{r}_{2})\\mathbf{r}_{3})\\left(\\mathbf{r}_{2}^{\\top}-\\mathbf{r}_{3}\\right)^{2}\\right]d t^{2}}\\\\ &{\\quad+\\int_{\\mathbb{R}}^{|\\mathbf{r}|}\\left[\\rho_{\\mathbf{r}_{3}}\\omega_{1}^{\\mathrm{\\tiny(E)}}(\\mathbf{r}_{3}^{\\top}-\\mathbf{r}_{3})(\\mathbf{r}_{3}^{\\top}-\\mathbf{r}_{3})\\left(\\mathbf{r}_{3}^{\\top}-\\rho_{\\mathbf{r}_{1}}(\\mathbf{r}_{3}^{\\top}+\\mathbf{r}_{2})\\mathbf{r}_{3}\\right)\\right]d t^{2}}\\\\ &{\\quad+\\int_{\\mathbb{R}}^{|\\mathbf{r}|}\\left[\\\n$$where we used the Lipschitz continuity of the learned score function (Assumption 3.3) and the true ", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "score function (Assumption 3.4), and the $\\delta_{\\infty}$ -accuracy of the learned score function at each time step (Assumption 3.1'). ", "page_idx": 38}, {"type": "text", "text": "Now we proceed to bound the terms in the error decomposition (C.24). We frst bound the $F_{t_{n},n^{\\dagger}h^{\\dagger}}$ term by the following lemma: ", "page_idx": 38}, {"type": "text", "text": "Lemma C.13. For any $n\\in[0:N-1]$ and $\\tau^{\\dagger}\\in[0,h^{\\dagger}]$ we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{P}\\left[\\left\\|\\mathbf{u}_{t_{n},n^{\\dagger}h^{\\dagger}+\\lfloor\\frac{\\tau^{\\dagger}}{\\epsilon^{\\dagger}}\\rfloor\\epsilon^{\\dagger}}^{*}-\\mathbf{u}_{t_{n},n^{\\dagger}h^{\\dagger}+\\tau^{\\dagger}}^{*}\\right\\|^{2}\\right]\\leq{d\\epsilon^{\\dagger}}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "and therefore ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{P}\\left[F_{t_{n},n^{\\dagger}h^{\\dagger}}\\right]\\leq d h^{\\dagger}\\epsilon^{\\dagger^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Proof. By the defnition of $(\\boldsymbol{\\mathbf{\\boldsymbol{u}}}_{t_{n}.n^{\\dagger}h^{\\dagger}+\\tau}^{*},\\boldsymbol{\\mathbf{\\boldsymbol{v}}}_{t_{n},n^{\\dagger}h^{\\dagger}+\\tau}^{*})$ as the stationary underdamped Langevindy namics (C.22), we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{P}\\left[\\left\\|u_{t_{n},n^{\\dagger}h^{\\dagger}+\\lfloor\\frac{\\tau^{\\dagger}}{\\epsilon^{\\dagger}}\\rfloor\\epsilon^{\\dagger}}^{*}-u_{t_{n},n^{\\dagger}h^{\\dagger}+\\tau^{\\dagger}}^{*}\\right\\|^{2}\\right]=\\mathbb{E}_{P}\\left[\\left\\|\\int_{\\lfloor\\frac{\\tau^{\\dagger}}{\\epsilon^{\\dagger}}\\rfloor\\epsilon^{\\dagger}}^{\\tau^{\\dagger}}v_{t_{n},n^{\\dagger}h^{\\dagger}+\\tau^{\\dagger}}^{*}\\mathrm{d}\\tau^{\\dagger^{\\prime}}\\right\\|^{2}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\epsilon^{\\dagger}\\int_{\\lfloor\\frac{\\tau^{\\dagger}}{\\epsilon^{\\dagger}}\\rfloor\\epsilon^{\\dagger}}^{\\tau^{\\dagger}}\\mathbb{E}_{P}\\left[\\left\\|v_{t_{n},n^{\\dagger}h^{\\dagger}+\\tau^{\\dagger}}^{*}\\right\\|^{2}\\right]\\mathrm{d}\\tau^{\\dagger^{\\prime}}\\leq d\\epsilon^{\\dagger^{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where the first inequality follows from Cauchy-Schwarz inequality and the last inequality is by the fact that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\boldsymbol{v}_{t_{n},n^{\\dagger}h^{\\dagger}+\\tau^{\\dagger\\prime}}^{*}\\sim\\mathcal{N}(\\boldsymbol{0},I_{d}),\\quad\\mathrm{for\\;any}\\;\\tau^{\\dagger^{\\prime}}\\in[0,h^{\\dagger}].\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Consequently, we have ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\mathbb{E}_{P}\\left[F_{t_{n},n^{\\dagger}h^{\\dagger}}\\right]=\\int_{0}^{h^{\\dagger}}\\mathbb{E}_{P}\\left[\\left\\|u_{t_{n},n^{\\dagger}h^{\\dagger}+\\lfloor\\frac{\\tau^{\\dagger}}{e^{\\dagger}}\\rfloor\\epsilon^{\\dagger}}^{*}-u_{t_{n},n^{\\dagger}h^{\\dagger}+\\tau^{\\dagger}}^{*}\\right\\|^{2}\\right]\\mathrm{d}\\tau^{\\dagger}\\leq d h^{\\dagger}\\epsilon^{\\dagger^{2}}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "The term $E_{t_{n},n^{\\dagger}h^{\\dagger}}$ can be bounded with the following lemma: ", "page_idx": 39}, {"type": "text", "text": "Lemma C.14. For any $n^{\\dagger}\\in[0:N^{\\dagger}-1]$ supposethat $\\gamma\\lesssim L_{p}^{-1/2}$ and $T^{\\dagger}\\lesssim L_{p}^{-1/2}$ thenwe have the following inequality for any $\\tau^{\\dagger}\\in[0,h^{\\dagger}]$ ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{P}\\left[\\left\\|\\widetilde{\\boldsymbol{u}}_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(K^{\\dagger})}-\\boldsymbol{u}_{t_{n},n^{\\dagger}h^{\\dagger}+\\tau^{\\dagger}}^{*}\\right\\|^{2}\\right]\\lesssim W_{2}^{2}(\\widetilde{q}_{t_{n},h_{n}},\\overleftarrow{p}_{t_{n+1}}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "and therefore ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{P}\\left[E_{t_{n},n^{\\dagger}h^{\\dagger}}\\right]\\lesssim h^{\\dagger}(L_{s}^{2}+L_{p}^{2})W_{2}^{2}(\\widetilde{q}_{t_{n},h_{n}},\\overleftarrow{p}_{t_{n+1}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "$P$ $,\\widetilde{\\mathbf{u}}_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(K^{\\dagger})}$ ${\\pmb u}_{t_{n},n}{\\!\\!\\:}_{h^{\\dagger},\\tau^{\\dagger}}$ (C.2).for $\\tau^{\\dagger}\\,\\in\\,[0,h^{\\dagger}]$ which coinides with that of ${\\pmb u}_{t_{n},n^{\\dagger}h^{\\dagger}+\\tau^{\\dagger}}^{*}$ As the only dferene betwen the two processes ${\\pmb u}_{t_{n},n}{\\bf\\dagger}_{h}{\\bf\\dagger}_{,\\tau}{\\bf\\dagger}$ and m,ntht+t is theintial condition,we cannvoke Lmma10 prove in [102] to deduce that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{P}\\left[\\left\\lVert\\widetilde{\\pmb{u}}_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(K^{\\dagger})}-\\pmb{u}_{t_{n},n^{\\dagger}h^{\\dagger}+\\tau^{\\dagger}}^{*}\\right\\rVert^{2}\\right]\\lesssim W_{2}^{2}(\\pi_{t_{n},n^{\\dagger}h^{\\dagger}},\\overleftarrow{p}_{t_{n+1}}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where the assumption that $\\gamma\\lesssim L_{p}^{-1/2}$ and $T^{\\dagger}\\lesssim L_{p}^{-1/2}$ is required. ", "page_idx": 39}, {"type": "text", "text": "Now notice that ${\\pmb u}_{t_{n},n^{\\dagger}h^{\\dagger}+\\tau^{\\dagger}}^{*}$ and ${\\pmb u}_{t_{n},n}{\\bf\\dagger}_{h}{\\bf\\dagger}_{,\\tau}{\\bf\\dagger}$ also follow the same dynamics with the rue score function for $\\tau^{\\dagger}\\in[0,n^{\\dagger}h^{\\dagger}]$ forany coupling of ${\\boldsymbol{u}}_{t_{n},n^{\\dagger}h^{\\dagger}}^{*}$ and ${\\pmb u}_{t_{n},n^{\\dagger}h^{\\dagger}}$ , we have ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad W_{2}^{2}(\\pi_{t_{n},n^{\\dagger}h^{\\dagger}},\\overleftarrow{p}_{t_{n+1}})\\leq\\mathbb{E}\\left[\\|\\pmb{u}_{t_{n},n^{\\dagger}h^{\\dagger}}-\\pmb{u}_{t_{n},n^{\\dagger}h^{\\dagger}}^{*}\\|^{2}\\right]}\\\\ &{\\leq\\!W_{2}^{2}(\\pi_{t_{n},0},\\overleftarrow{p}_{t_{n+1}})=W_{2}^{2}(\\widetilde{q}_{t_{n},h_{n}},\\overleftarrow{p}_{t_{n+1}}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where the last equality is again by [102, Lemma 10]. ", "page_idx": 39}, {"type": "text", "text": "Therefore, we have ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{P}\\left[E_{t_{n},n^{\\dagger}h^{\\dagger}}\\right]}\\\\ &{=\\int_{0}^{h^{\\dagger}}\\mathbb{E}_{P}\\left[L_{s}^{2}\\left\\|\\tilde{u}_{t_{n},n^{\\dagger}h^{\\dagger},\\lfloor\\frac{\\tau^{\\dagger}}{\\epsilon^{\\dagger}}\\rfloor\\epsilon^{\\dagger}}^{(K^{\\dagger})}-u_{t_{n},n^{\\dagger}h^{\\dagger}+\\lfloor\\frac{\\tau^{\\dagger}}{\\epsilon^{\\dagger}}\\rfloor\\epsilon^{\\dagger}}^{*}\\right\\|^{2}+L_{p}^{2}\\left\\|\\tilde{u}_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(K^{\\dagger})}-u_{t_{n},n^{\\dagger}h^{\\dagger}+\\tau^{\\dagger}}^{*}\\right\\|^{2}\\right]\\mathrm{d}\\tau^{\\dagger}}\\\\ &{\\le h^{\\dagger}(L_{s}^{2}+L_{p}^{2})W_{2}^{2}(\\tilde{q}_{t_{n},h_{n}},\\tilde{p}_{t_{n+1}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Now, we provide lemmas that are used to bound the first term in (C.24). ", "page_idx": 39}, {"type": "text", "text": "Lemma C.15. For any ${n^{\\dagger}\\in[0:N^{\\dagger}-1]},$ we have the following estimate: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\underset{\\tau^{\\uparrow}\\in[0,h^{\\uparrow}]}{\\operatorname*{sup}}\\mathbb{E}_{P}\\left[\\left\\|\\widetilde{u}_{t_{n},n^{\\uparrow}h^{\\uparrow},\\tau^{\\uparrow}}^{(1)}-\\widetilde{u}_{t_{n},n^{\\uparrow}h^{\\uparrow},\\tau^{\\uparrow}}^{(0)}\\right\\|^{2}\\right]}\\\\ &{\\leq\\!\\!\\frac{5L_{s}^{2}h^{\\dagger}e^{(3+\\gamma)h^{\\dagger}}}{2\\gamma}\\underset{\\tau^{\\uparrow}\\in[0,h^{\\uparrow}]}{\\operatorname*{sup}}\\mathbb{E}_{P}\\left[\\left\\|\\widetilde{u}_{t_{n},n^{\\uparrow}h^{\\uparrow},\\tau^{\\uparrow}}^{(K^{\\uparrow}-1)}-\\widetilde{u}_{t_{n},n^{\\uparrow}h^{\\uparrow},\\tau^{\\uparrow}}^{(K^{\\uparrow})}\\right\\|^{2}\\right]}\\\\ &{+\\frac{5h^{\\dagger}e^{(3+\\gamma)h^{\\dagger}}}{2\\gamma}\\mathbb{E}_{P}\\left[E_{t_{n},n^{\\uparrow}h^{\\uparrow}}+h^{\\dagger}\\delta_{\\infty}^{2}+L_{p}^{2}F_{t_{n},n^{\\uparrow}h^{\\downarrow}}\\right]+h^{\\dagger}{}^{2}e^{(3+\\gamma)h^{\\dagger}}\\left(3\\gamma d+M_{s}^{2}\\right)+h^{\\dagger}e^{2h^{\\dagger}}d.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Proof Let $\\mu_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}:=\\widetilde{\\pmb{u}}_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(1)}-\\widetilde{\\pmb{u}}_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(0)}$ and $\\begin{array}{r}{\\nu_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}:=\\widetilde{v}_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(1)}-\\widetilde{v}_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(0)}.}\\end{array}$ Then for $k=0$ , we may rewrite (C.15) as follows ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\int\\mathrm{d}\\mu_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}=\\Bigl(\\nu_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}+\\tilde{v}_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(0)}\\Bigr)\\,\\mathrm{d}\\tau^{\\dagger}}\\\\ &{\\int\\mathrm{d}\\nu_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}=-\\gamma(\\nu_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}+\\tilde{v}_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(0)})\\mathrm{d}\\tau^{\\dagger}-s_{t_{n+1}}(\\tilde{u}_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(0)})\\mathrm{d}\\tau^{\\dagger}+\\sqrt{2\\gamma}\\mathrm{d}b_{t_{n},n^{\\dagger}h^{\\dagger}+\\tau^{\\dagger}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "On the one hand, by using the first equation in (C.25), we may compute the derivative ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\frac{\\textrm{d}}{\\mathrm{d}\\tau^{\\dagger}}\\left\\|\\pmb{\\mu}_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger^{\\prime}}}\\right\\|^{2}=2\\pmb{\\mu}_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger^{\\prime}}}^{\\top}\\left(\\pmb{\\nu}_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger^{\\prime}}}+\\pmb{\\tilde{\\vartheta}}_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger^{\\prime}}}^{(0)}\\right)\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "and integrate it for $\\tau^{\\dagger^{\\prime}}\\in[0,\\tau^{\\dagger}]$ , which yields ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\left\\|\\mu_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}\\right\\|^{2}=2\\int_{0}^{\\tau^{\\dagger}}\\mu_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{\\top}(\\nu_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}+\\tilde{v}_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(0)})\\mathrm{d}\\tau^{\\dagger^{\\prime}}}}\\\\ &{}&{\\leq2\\int_{0}^{\\tau^{\\dagger}}\\left\\|\\mu_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger^{\\prime}}}\\right\\|^{2}\\mathrm{d}\\tau^{\\dagger^{\\prime}}+\\int_{0}^{\\tau^{\\dagger}}\\left\\|\\nu_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger^{\\prime}}}\\right\\|^{2}\\mathrm{d}\\tau^{\\dagger^{\\prime}}+\\int_{0}^{\\tau^{\\dagger}}\\left\\|\\tilde{v}_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger^{\\prime}}}^{(0)}\\right\\|^{2}\\mathrm{d}\\tau^{\\dagger^{\\prime}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Applying Gronwall's inequality, we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\left\\|\\mu_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}\\right\\|^{2}\\leq e^{2\\tau^{\\dagger}}\\left(\\int_{0}^{\\tau^{\\dagger}}\\left\\|\\nu_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}\\right\\|^{2}\\mathrm{d}\\tau^{\\dagger^{\\prime}}+\\int_{0}^{\\tau^{\\dagger}}\\left\\|\\widetilde{v}_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger^{\\prime}}}^{(0)}\\right\\|^{2}\\mathrm{d}\\tau^{\\dagger^{\\prime}}\\right).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "We then take expectation with respect to the path measure $P$ and then the supremum with respect to $\\tau^{\\dagger}\\in[0,h^{\\dagger}]$ , implying that ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\underset{\\tau^{\\uparrow}\\in[0,h^{\\uparrow}]}{\\operatorname*{sup}}\\mathbb{E}_{P}\\left[\\left\\Vert\\mu_{t_{n},n^{\\uparrow}h^{\\uparrow},\\tau^{\\uparrow}}\\right\\Vert^{2}\\right]}\\\\ &{\\leq\\underset{\\tau^{\\uparrow}\\in[0,h^{\\uparrow}]}{\\operatorname*{sup}}\\left(e^{2\\tau^{\\uparrow}}\\int_{0}^{\\tau^{\\uparrow}}\\mathbb{E}_{P}\\left[\\left\\Vert\\nu_{t_{n},n^{\\uparrow}h^{\\uparrow},\\tau^{\\uparrow}}\\right\\Vert^{2}\\right]\\mathrm{d}\\tau^{\\dagger^{\\prime}}+e^{2\\tau^{\\uparrow}}\\int_{0}^{\\tau^{\\uparrow}}\\mathbb{E}_{P}\\left[\\left\\Vert\\tilde{v}_{t_{n},n^{\\uparrow}h^{\\uparrow},\\tau^{\\uparrow}}^{(0)}\\right\\Vert^{2}\\right]\\mathrm{d}\\tau^{\\dagger^{\\prime}}\\right)}\\\\ &{\\leq h^{\\dagger}e^{2h^{\\uparrow}}\\underset{\\tau^{\\uparrow}\\in[0,h^{\\uparrow}]}{\\operatorname*{sup}}\\mathbb{E}_{P}\\left[\\left\\Vert\\nu_{t_{n},n^{\\uparrow}h^{\\uparrow},\\tau^{\\uparrow}}\\right\\Vert^{2}\\right]+h^{\\dagger}e^{2h^{\\dagger}}d.}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "On the other hand, by applying Ito's lemma and plugging in the expression of $b_{t_{n},n^{\\dagger}h^{\\dagger}+\\tau^{\\dagger}}$ given by (C.17), we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ \\mathrm{d}\\|\\boldsymbol{\\nu}_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}\\|^{2}}\\\\ &{=-\\left[2\\gamma\\|\\boldsymbol{\\nu}_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}\\|^{2}+2\\gamma\\boldsymbol{\\nu}_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{\\top}\\tilde{v}_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(0)}+2\\boldsymbol{\\nu}_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{\\top}\\boldsymbol{s}_{t_{n+1}}\\Big(\\tilde{u}_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(0)}\\Big)-2\\gamma d\\right]\\mathrm{d}\\tau}\\\\ &{+2\\nu_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{\\top}\\sqrt{2\\gamma}\\big(\\mathrm{d}\\tilde{b}_{t_{n},n^{\\dagger}h^{\\dagger}+\\tau^{\\dagger}}+\\phi_{t_{n},n^{\\dagger}h^{\\dagger}}(\\tau^{\\dagger})\\mathrm{d}\\tau^{\\dagger}\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Then similarly, we may compute the derivative of $\\|\\nu_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}\\|^{2}$ , integrate it for $\\tau^{\\dagger}\\in[0,h^{\\dagger}]$ , and take the supremum with respect to $\\tau^{\\dagger}$ to obtain ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{P}\\left[\\|\\nu_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}\\|^{2}\\right]}\\\\ &{=\\!\\mathbb{E}_{P}\\left[-\\int_{0}^{\\tau^{\\dagger}}\\left(2\\gamma\\|\\nu_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}\\|^{2}+2\\gamma\\nu_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{\\top}\\tilde{\\vartheta}_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(0)}-2\\gamma d\\right)\\!\\mathrm{d}\\tau^{\\dagger^{\\prime}}\\right]}\\\\ &{+\\mathbb{E}_{P}\\left[-\\int_{0}^{\\tau^{\\dagger}}2\\nu_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{\\top}s_{t_{n+1}}\\Big(\\widetilde{u}_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(0)}\\Big)\\mathrm{d}\\tau^{\\dagger^{\\prime}}\\right]}\\\\ &{+2\\sqrt{2\\gamma}\\mathbb{E}_{P}\\left[\\int_{0}^{\\tau^{\\dagger}}\\nu_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{\\top}\\Big(\\widetilde{\\mathbf{d}}\\tilde{b}_{t_{n},n^{\\dagger}h^{\\dagger}+\\tau^{\\dagger}}+\\phi_{t_{n},n^{\\dagger}h^{\\dagger}}\\big(\\tau^{\\dagger^{\\prime}}\\big)\\mathrm{d}\\tau^{\\dagger^{\\prime}}\\Big)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "By Ito's lemma, this equals to ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{P}\\left[\\|\\nu_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}\\|^{2}\\right]}\\\\ &{=\\!\\mathbb{E}_{P}\\left[-\\int_{0}^{\\tau^{\\dagger}}\\Bigg(2\\gamma\\|\\nu_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}\\|^{2}+2\\gamma\\nu_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{\\top}\\tilde{\\vartheta}_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(0)}-2\\gamma d\\Bigg)\\mathrm{d}\\tau^{\\dagger^{\\prime}}\\right]}\\\\ &{+\\mathbb{E}_{P}\\left[-\\int_{0}^{\\tau^{\\dagger}}2\\nu_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{\\top}s_{t_{n+1}}\\Big(\\widetilde{u}_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}\\Big)+2\\sqrt{2\\gamma}\\nu_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{\\top}\\phi_{t_{n},n^{\\dagger}h^{\\dagger}}(\\tau^{\\dagger^{\\prime}})\\mathrm{d}\\tau^{\\dagger^{\\prime}}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Applying AM-GM gives ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{P}\\left[\\left\\|\\boldsymbol{v}_{t_{n},n^{\\mathsf{t}}h^{\\mathsf{t}},\\tau^{\\mathsf{t}}}\\right\\|^{2}\\right]}\\\\ &{\\leq\\int_{0}^{\\tau^{\\mathsf{t}}}\\mathbb{E}_{P}\\left[(1+\\gamma)\\|\\boldsymbol{v}_{t_{n},n^{\\mathsf{t}}h^{\\mathsf{t}},\\tau^{\\mathsf{t}}}\\|^{2}+\\left\\|\\phi_{t_{n},n^{\\mathsf{t}}h^{\\mathsf{t}}}(\\tau^{\\mathsf{t}^{\\prime}})\\right\\|^{2}\\right]\\mathrm{d}\\tau^{\\mathsf{t}^{\\prime}}}\\\\ &{+\\int_{0}^{\\tau^{\\mathsf{t}}}\\mathbb{E}_{P}\\left[\\gamma\\left\\|\\tilde{\\boldsymbol{v}}_{t_{n},n^{\\mathsf{t}}h^{\\mathsf{t}},\\tau^{\\mathsf{t}}}^{(0)}\\right\\|^{2}+\\left\\|\\boldsymbol{s}_{t_{n+1}}\\!\\left(\\tilde{\\boldsymbol{u}}_{t_{n},n^{\\mathsf{t}}h^{\\mathsf{t}},\\tau^{\\mathsf{t}}}^{(0)}\\right)\\right\\|^{2}+2\\gamma d\\right]\\mathrm{d}\\tau^{\\mathsf{t}^{\\prime}}}\\\\ &{\\leq\\int_{0}^{\\tau^{\\mathsf{t}}}\\mathbb{E}_{P}\\left[(1+\\gamma)\\|\\boldsymbol{v}_{t_{n},n^{\\mathsf{t}}h^{\\mathsf{t}},\\tau^{\\mathsf{t}}}\\|^{2}+\\left\\|\\phi_{t_{n},n^{\\mathsf{t}}h^{\\mathsf{t}}}(\\tau^{\\mathsf{t}^{\\prime}})\\right\\|^{2}\\right]\\mathrm{d}\\tau^{\\mathsf{t}^{\\prime}}+\\left(\\gamma\\mathbb{E}\\left[\\left\\|\\tilde{\\boldsymbol{v}}_{t_{n},n^{\\mathsf{t}}h^{\\mathsf{t}},0}^{(0)}\\right\\|^{2}\\right]+M_{s}^{2}+2}\\\\ &{=\\!(1+\\gamma)\\int_{0}^{\\tau^{\\mathsf{t}}}\\mathbb{E}_{P}\\left[\\left\\|\\boldsymbol{v}_{t_{n},n^{\\mathsf{t}}h^{\\mathsf{t}},\\tau^{\\mathsf{t}}}\\right\\|^{2}\\right]\\mathrm{d}\\tau^{\\mathsf{t}^{\\prime}}+\\int_{0}^{\\tau^{\\mathsf{t}}}\\mathbb{E}_{P}\\left[\\left\\|\\phi_{t_{n},n^{\\mathsf\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where in the last equality, we used the initialization of the auxiliary corrector process $\\widetilde{v}_{t_{n},n^{\\dagger}h^{\\dagger},0}^{(0)}\\sim$ $\\mathcal{N}(0,I_{d})$ ", "page_idx": 41}, {"type": "text", "text": "Again, we apply Gronwall's inequality to the above inequality and take the supremum with respect to $\\boldsymbol{\\tau}^{\\dagger}\\in[0,\\dot{h}^{\\dagger}]$ to obtain ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\underset{\\tau^{\\dagger}\\in[0,h^{\\dagger}]}{\\operatorname*{sup}}\\mathbb{E}_{P}\\left[\\|\\nu_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}\\|^{2}\\right]}\\\\ &{\\leq\\!e^{(1+\\gamma)h^{\\dagger}}\\!\\int_{0}^{h^{\\dagger}}\\mathbb{E}_{P}\\left[\\|\\phi_{t_{n},n^{\\dagger}h^{\\dagger}}(\\tau^{\\dagger})\\|^{2}\\right]\\mathrm{d}\\tau^{\\dagger}+h^{\\dagger}e^{(1+\\gamma)h^{\\dagger}}\\left(3\\gamma d+M_{s}^{2}\\right)}\\\\ &{\\leq\\!\\frac{e^{(1+\\gamma)h^{\\dagger}}}{2\\gamma}\\mathbb{E}_{P}\\left[\\int_{0}^{h^{\\dagger}}\\left\\|s_{t_{n+1}}(\\tilde{u}_{t_{n},n^{\\dagger}h^{\\dagger},\\lfloor\\frac{\\tau^{\\dagger}}{\\epsilon^{\\dagger}}\\rfloor\\epsilon^{\\dagger}}^{(K^{\\dagger}-1)})-\\nabla\\log\\tilde{p}_{t_{n+1}}(\\tilde{u}_{t_{n,n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}}^{(K^{\\dagger})})\\right\\|^{2}\\mathrm{d}\\tau^{\\dagger}\\right]}\\\\ &{\\quad+\\,h^{\\dagger}e^{(1+\\gamma)h^{\\dagger}}\\left(3\\gamma d+M_{s}^{2}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "and for the difference term within the expectation, we decompose it again by the triangle inequality in (C.24), i.e. ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\int_{0}^{h^{\\dagger}}\\left\\|s_{t_{n+1}}(\\widetilde{u}_{t_{n},n^{\\dagger}h^{\\dagger},\\lfloor\\frac{\\tau^{\\dagger}}{\\epsilon^{\\dagger}}\\rfloor\\epsilon^{\\dagger}}^{(K^{\\dagger}-1)})-\\nabla\\log\\widetilde{p}_{t_{n+1}}(\\widetilde{u}_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(K^{\\dagger})})\\right\\|^{2}\\mathrm{d}\\tau^{\\dagger}}\\\\ &{\\leq5L_{s}^{2}\\int_{0}^{h^{\\dagger}}\\left\\|\\widetilde{u}_{t_{n},n^{\\dagger}h^{\\dagger},\\lfloor\\frac{\\tau^{\\dagger}}{\\epsilon^{\\dagger}}\\rfloor\\epsilon^{\\dagger}}^{(K^{\\dagger}-1)}-\\widetilde{u}_{t_{n},n^{\\dagger}h^{\\dagger},\\lfloor\\frac{\\tau^{\\dagger}}{\\epsilon^{\\dagger}}\\rfloor\\epsilon^{\\dagger}}^{(K^{\\dagger})}\\right\\|^{2}\\mathrm{d}\\tau^{\\dagger}+5E_{t_{n},n^{\\dagger}h^{\\dagger}}+5h^{\\dagger}\\delta_{\\infty}^{2}+5L_{p}^{2}F_{t_{n},n^{\\dagger}h^{\\dagger}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "to obtain that ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\underset{\\tau^{\\prime}\\in[0,t]^{1}}{\\operatorname*{sup}}\\mathbb{E}_{P}\\left[\\big\\|w_{t_{n},n^{\\mathrm{t}}\\mid t^{\\prime}}\\big\\|^{2}\\right]}\\\\ &{\\leq\\frac{5L_{s}^{2}e^{(1+\\gamma)h^{\\mathrm{t}}}}{2\\gamma}\\mathbb{E}_{P}\\left[\\int_{0}^{h^{\\mathrm{t}}}\\Big\\|\\tilde{u}_{t_{n},n^{\\mathrm{t}}\\mid\\tau_{\\mathrm{f}}^{\\mathrm{t}}}^{(K^{\\mathrm{t}}-1)}\\mathrm{i}\\tau^{-\\mathrm{i}}-\\tilde{u}_{t_{n},n^{\\mathrm{t}}\\mid t^{\\prime},\\tau_{\\mathrm{f}}^{\\mathrm{t}}}^{(K^{\\mathrm{t}})}\\Big\\|^{2}\\,\\mathrm{d}\\tau^{\\mathrm{t}}\\right]}\\\\ &{+\\frac{5e^{(1+\\gamma)h^{\\mathrm{t}}}}{2\\gamma}\\mathbb{E}_{P}\\left[E_{t_{n},n^{\\mathrm{t}}\\mid h^{\\mathrm{t}}}+h^{\\mathrm{t}}\\delta_{\\infty}^{2}+L_{p}^{2}F_{t_{n},n^{\\mathrm{t}}\\mid h^{\\mathrm{t}}}\\right]+h^{\\mathrm{t}}e^{(1+\\gamma)h^{\\mathrm{t}}}\\left(3\\gamma d+M_{s}^{2}\\right)}\\\\ &{\\leq\\frac{5L_{s}^{2}e^{(1+\\gamma)h^{\\mathrm{t}}}}{2\\gamma}h^{\\mathrm{t}}\\underset{\\tau^{\\prime}\\in[0,t]^{1}}{\\operatorname*{sup}}\\mathbb{E}_{P}\\left[\\Big\\|\\tilde{u}_{t_{n},n^{\\mathrm{t}}\\mid\\tau_{\\mathrm{f}}^{\\mathrm{t}}}^{(K^{\\mathrm{t}}-1)}\\mathrm{i}\\tau^{-\\mathrm{i}}-\\tilde{u}_{t_{n},n^{\\mathrm{t}}\\mid\\tau_{\\mathrm{f}}^{\\mathrm{t}}}^{(K^{\\mathrm{t}})}\\Big\\|^{2}\\right]}\\\\ &{+\\frac{5e^{(1+\\gamma)h^{\\mathrm{t}}}}{2\\gamma}\\mathbb{E}_{P}\\left[E_{t_{n},n\\uparrow h^{\\mathrm{t}}}+h^{\\mathrm{t}}\\delta_{\\infty}^{2}+L_{p}^{2}F_{t_{n},n^{\\mathrm{t}}\\mid\\tau}\\right\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Lemma C.16 (Exponential convergence of Picard iteration in the corrector step of PIADM-ODE). For any $n^{\\dag}~\\in~[0,N^{\\dag}\\mathrm{~-~}1]$ then the two ending termns $\\widetilde{\\mathbf{u}}_{n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(K^{\\dagger})}$ and untht,rt of the sequence $\\{\\widetilde{\\pmb{u}}_{n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(k^{\\dagger})}\\}_{k^{\\dagger}\\in[0:K^{\\dagger}-1]}$ satisfy the folowing exponential convergence rate ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\tau^{\\dagger}\\in[0,h^{\\dagger}]}{\\operatorname*{sup}}\\mathbb{E}_{P}\\left[\\left\\|\\widetilde{u}_{n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(K^{\\dagger})}-\\widetilde{u}_{n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(K^{\\dagger}-1)}\\right\\|^{2}\\right]}\\\\ &{\\leq C_{K^{\\dagger}}\\left(\\frac{5h^{\\dagger}e^{(3+\\gamma)h^{\\dagger}}}{2\\gamma}\\mathbb{E}_{P}\\left[E_{t_{n},n^{\\dagger}h^{\\dagger}}+h^{\\dagger}\\delta_{\\infty}^{2}+L_{p}^{2}F_{t_{n},n^{\\dagger}h^{\\dagger}}\\right]+h^{\\dagger^{2}}e^{(3+\\gamma)h^{\\dagger}}\\left(3\\gamma d+M_{s}^{2}\\right)+h^{\\dagger}e^{2h^{\\dagger}}d\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where the coefficient ", "page_idx": 42}, {"type": "equation", "text": "$$\nC_{K^{\\dagger}}=\\left(\\frac{L_{s}^{2}{h^{\\dagger}}^{2}{e^{h^{\\dagger}}}}{2\\gamma}\\right)^{K^{\\dagger}-1}\\left/\\left(1-\\frac{5L_{s}^{2}{h^{\\dagger}}e^{(3+\\gamma)h^{\\dagger}}}{2\\gamma}\\left(\\frac{L_{s}^{2}{h^{\\dagger}}^{2}{e^{h^{\\dagger}}}}{2\\gamma}\\right)^{K^{\\dagger}-1}\\right).\\right.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Proof. We subtract the dynamics of $\\widetilde{\\pmb{u}}_{n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(k+1)}$ ntht,rt and u, ntht,t in (C.15) to obtain ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathrm{d}\\left(\\widetilde{\\pmb{u}}_{n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(k+1)}-\\widetilde{\\pmb{u}}_{n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(k^{\\dagger})}\\right)=\\left(\\widetilde{\\pmb{v}}_{n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(k+1)}-\\widetilde{\\pmb{v}}_{n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(k^{\\dagger})}\\right)\\mathrm{d}\\tau^{\\dagger}.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Then, we use the formula above to compute the derivative ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\frac{\\textrm{d}}{\\mathrm{d}\\tau^{\\dagger}}\\left\\|\\widetilde{\\boldsymbol{u}}_{n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(k+1)}-\\widetilde{\\boldsymbol{u}}_{n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(k^{\\dagger})}\\right\\|^{2}=2\\left(\\widetilde{\\boldsymbol{u}}_{n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(k+1)}-\\widetilde{\\boldsymbol{u}}_{n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(k^{\\dagger})}\\right)^{\\top}\\left(\\widetilde{\\boldsymbol{v}}_{n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(k+1)}-\\widetilde{\\boldsymbol{v}}_{n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(k^{\\dagger})}\\right)\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "and integrate for $\\tau^{\\dagger^{\\prime}}\\in[0,\\tau^{\\dagger}]$ to obtain ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\|\\widetilde{\\mathbf{u}}_{n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(k+1)}-\\widetilde{\\mathbf{u}}_{n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(k^{\\dagger})}\\right\\|^{2}}\\\\ &{=2\\int_{0}^{\\tau^{\\dagger}}\\left(\\widetilde{u}_{n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(k+1)}-\\widetilde{\\mathbf{u}}_{n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(k^{\\dagger})}\\right)^{\\top}\\left(\\widetilde{v}_{n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(k+1)}-\\widetilde{v}_{n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(k^{\\dagger})}\\right)\\mathrm{d}\\tau^{\\dagger^{\\prime}}}\\\\ &{\\le\\int_{0}^{\\tau^{\\dagger}}\\left\\|\\widetilde{u}_{n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(k+1)}-\\widetilde{u}_{n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(k^{\\dagger})}\\right\\|^{2}\\mathrm{d}\\tau^{\\dagger^{\\prime}}+\\int_{0}^{\\tau^{\\dagger}}\\left\\|\\widetilde{v}_{n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(k+1)}-\\widetilde{v}_{n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(k^{\\dagger})}\\right\\|^{2}\\mathrm{d}\\tau^{\\dagger^{\\prime}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Applying Gronwall's inequality gives us that ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\left\\|\\widetilde{\\mathbf{u}}_{n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(k+1)}-\\widetilde{\\mathbf{u}}_{n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(k^{\\dagger})}\\right\\|^{2}\\leq e^{\\tau^{\\dagger}}\\int_{0}^{\\tau^{\\dagger}}\\left\\|\\widetilde{\\mathbf{v}}_{n^{\\dagger}h^{\\dagger},\\tau^{\\dagger\\prime}}^{(k+1)}-\\widetilde{\\mathbf{v}}_{n^{\\dagger}h^{\\dagger},\\tau^{\\dagger\\prime}}^{(k^{\\dagger})}\\right\\|^{2}\\mathrm{d}\\tau^{\\dagger^{\\prime}}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "and taking the supremum with respect to $\\tau^{\\dagger}\\in[0,h^{\\dagger}]$ on both sides above implies ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\tau\\uparrow\\in[0,h^{\\uparrow}]}\\mathbb{E}_{P}\\left[\\left\\|\\widetilde{\\boldsymbol{u}}_{n^{\\uparrow}h^{\\uparrow},\\tau^{\\uparrow}}^{(k+1)}-\\widetilde{\\boldsymbol{u}}_{n^{\\uparrow}h^{\\uparrow},\\tau^{\\uparrow}}^{(k^{\\uparrow})}\\right\\|^{2}\\right]\\leq h^{\\dagger}e^{h^{\\dagger}}\\operatorname*{sup}_{\\tau^{\\uparrow}\\in[0,h^{\\uparrow}]}\\mathbb{E}_{P}\\left[\\left\\|\\widetilde{\\boldsymbol{v}}_{n^{\\uparrow}h^{\\uparrow},\\tau^{\\uparrow}}^{(k+1)}-\\widetilde{\\boldsymbol{v}}_{n^{\\uparrow}h^{\\uparrow},\\tau^{\\uparrow}}^{(k^{\\uparrow})}\\right\\|^{2}\\right].\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "We then apply asimilar argument for (k1) $\\widetilde{\\pmb{v}}_{n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(k+1)}-\\widetilde{\\pmb{v}}_{n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(k^{\\dagger})}$ ntht,t as well ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\textrm{d}\\Bigl(\\widetilde{v}_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(k+1)}-\\widetilde{v}_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(k^{\\dagger})}\\Bigr)}\\\\ &{=-\\gamma\\left(\\widetilde{v}_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(k+1)}-\\widetilde{v}_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(k^{\\dagger})}\\right)\\mathrm{d}\\tau^{\\dagger}-\\Biggl(s_{t_{n+1}}(\\widetilde{u}_{t_{n,n^{\\dagger}h^{\\dagger},\\lfloor\\frac{\\tau^{\\dagger}}{e^{\\dagger}}\\rfloor\\epsilon^{\\dagger}}}^{(k^{\\dagger})})-s_{t_{n+1}}(\\widetilde{u}_{t_{n,n^{\\dagger}h^{\\dagger},\\lfloor\\frac{\\tau^{\\dagger}}{e^{\\dagger}}\\rfloor\\epsilon^{\\dagger}}}^{(k-1)})\\Bigr)\\,\\mathrm{d}\\tau^{\\dagger},}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "integrate which for $\\tau^{\\dagger}\\in[0,\\tau^{\\dagger}]$ to obtain ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\widetilde v_{n^{\\textnormal{t h}},\\tau^{\\top}}^{(k+1)}-\\widetilde v_{n^{\\textnormal{t h}},\\tau^{\\top}}^{(k\\textnormal{t})}\\right\\|^{2}}\\\\ &{=-\\int_{0}^{\\tau^{\\top}}2\\gamma\\left\\|\\widetilde v_{n^{\\textnormal{t h}},\\tau^{\\prime\\prime}}^{(k+1)}-\\widetilde v_{n^{\\textnormal{t h}},\\tau^{\\prime\\prime}}^{(k+1)}\\right\\|^{2}\\mathrm d\\tau^{\\prime}}\\\\ &{-2\\int_{0}^{\\tau^{\\top}}\\left(\\widetilde v_{n^{\\textnormal{t h}},\\tau^{\\prime\\prime}}^{(k+1)}-\\widetilde v_{n^{\\textnormal{t h}},\\tau^{\\prime\\prime}}^{(k+1)}\\right)^{\\top}\\left({\\pmb x}_{t_{n},n^{\\textnormal{t h}},\\lfloor\\frac{\\tau^{\\prime}}{c^{\\prime}}\\rfloor\\in\\tau}^{(k+1)}\\right)-{s}_{t_{n+1}}(\\widetilde u_{t_{n,n}\\nmid h^{\\prime},\\lfloor\\frac{\\tau^{\\prime}}{c^{\\prime}}\\rfloor\\in\\tau}^{(k-1)})\\right)\\mathrm d\\tau^{\\prime\\prime}}\\\\ &{\\leq\\frac{1}{2\\gamma}\\int_{0}^{\\tau^{\\top}}\\left\\|{s}_{t_{n+1}}(\\widetilde u_{t_{n,n}\\nmid h^{\\prime},\\lfloor\\frac{\\tau^{\\prime}}{c^{\\prime}}\\rfloor\\in\\tau}^{(k)})-{s}_{t_{n+1}}(\\widetilde u_{t_{n,n}\\nmid h^{\\prime},\\lfloor\\frac{\\tau^{\\prime}}{c^{\\prime}}\\rfloor\\in\\tau}^{(k-1)})\\right\\|^{2}\\mathrm d\\tau^{\\prime\\prime}}\\\\ &{\\leq\\frac{L_{s}^{2}}{2\\gamma}\\int_{0}^{\\tau^{\\top}}\\left\\|\\widetilde u_{t_{n},n\\uparrow h,\\lfloor\\frac{\\tau^{\\prime}}{c^{\\prime}}\\rfloor\\in\\tau}^{(k+1)}-\\widetilde u_{t_{n,n}\\nmid h^{\\prime},\\lfloor\\frac{\\tau^{\\prime}}{c^{\\prime}}\\rfloor\\in\\tau}^{(k-1)}\\right\\|^{2}\\mathrm d\\tau^{\\prime}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "And then taking the supremum with respect to $\\tau^{\\dagger}\\in[0,h^{\\dagger}]$ on both sides above implies ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\tau\\uparrow\\in[0,h\\uparrow]}\\mathbb{E}_{P}\\left[\\left\\|\\widetilde{v}_{n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(k+1)}-\\widetilde{v}_{n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(k^{\\dagger})}\\right\\|^{2}\\right]\\leq\\frac{h^{\\dagger}L_{s}^{2}}{2\\gamma}\\operatorname*{sup}_{\\tau\\uparrow\\in[0,h\\uparrow]}\\mathbb{E}_{P}\\left[\\left\\|\\widetilde{u}_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(k^{\\dagger})}-\\widetilde{u}_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}^{(k-1)}\\right\\|^{2}\\right]\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Substituting (C.31) into (C.30) and iterating over $k\\in[1:K^{\\dagger}-1]$ , we obtain that ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad_{\\mathbf{v}^{*}}\\underset{\\tau^{*}\\in\\{\\mathbf{0},h\\}^{\\mathbb{T}}}{\\operatorname*{sup}}\\mathbb{E}_{p}\\left[\\left\\|\\tilde{\\mathbf{u}}_{n,n+\\tau^{*}}^{(K^{*})}-\\tilde{\\mathbf{u}}_{n,n+\\tau^{*}}^{(K^{*})-1}\\right\\|^{2}\\right]\\leq\\frac{L_{\\mathbf{x}}^{2}h^{1/\\alpha^{*}}\\rho^{1}}{2\\gamma}\\underset{\\tau^{*}\\in\\{\\mathbf{0},h\\}^{\\mathbb{T}}}{\\operatorname*{sup}}\\mathbb{E}_{p}\\left[\\left\\|\\tilde{\\mathbf{u}}_{t_{n,n},n+\\tau^{*}}^{(K^{*}-1)}-\\tilde{\\mathbf{u}}_{t_{n,n}}^{(K^{*})}\\right\\|^{2}\\right]}\\\\ &{\\leq\\left(\\frac{L_{\\mathbf{x}}^{2}h^{1/\\alpha^{*}}\\rho^{1}}{2\\gamma}\\right)^{K^{*}-1}\\underset{\\tau^{*}\\in\\{\\mathbf{0},h\\}^{\\mathbb{T}}}{\\operatorname*{sup}}\\mathbb{E}_{p}\\left[\\left\\|\\tilde{\\mathbf{u}}_{t_{n,n},n+\\tau^{*}}^{(K_{1})}-\\tilde{\\mathbf{u}}_{t_{n,n}+\\tau^{*}}^{(K_{2})}\\right\\|^{2}\\right]}\\\\ &{\\leq\\left(\\frac{L_{\\mathbf{x}}^{2}h^{1/\\alpha^{*}}\\rho^{1}}{2\\gamma}\\right)^{K^{*}-1}\\frac{5L_{\\mathbf{x}}^{1}\\rho(1+\\gamma^{*})h^{1/\\alpha}}{2\\gamma}\\mathbb{E}_{p}\\left[E_{t_{n,n},n+1}+h^{1}\\delta_{\\mathbf{x}}^{2}+L_{p}^{2}F_{t_{n,n},n+1}\\right]}\\\\ &{+\\left(\\frac{L_{\\mathbf{x}}^{2}h^{1/2}e^{1}}{2\\gamma}\\right)^{K^{*}-1}\\left(h^{1/\\alpha}e^{(3+\\gamma)h^{1}}\\left(3\\gamma d+M_{\\mathbf{x}}^{2}\\right)+h^{1}e^{2N/4}\\right)}\\\\ &{+\\left(\\frac{L_{\\mathbf{x}}^{2}h^{1/\\alpha^{*}}\\rho^{1}}{ \n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where we plug in the results from Lemma C.15 in the last inequality. Rearranging the inequality above completes our proof. \u53e3 ", "page_idx": 43}, {"type": "text", "text": "Theorem C.17. Under Assumptions 3.1', 3.2, 3.3, and 3.4, given the following choices of the order of theparameters ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{T^{\\dagger}=\\mathcal{O}(1),\\quad N^{\\dagger}=\\mathcal{O}(1),\\quad h^{\\dagger}=\\Theta(1)}}\\\\ {{M^{\\dagger}=\\Theta(d^{1/2}\\delta^{-1}),\\quad\\epsilon^{\\dagger}=\\Theta(d^{-1/2}\\delta),\\quad K^{\\dagger}=\\mathcal{O}(\\log(d\\delta^{-2}))}}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "and let ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\frac{L_{s}^{2}{h^{\\dagger}}^{2}e^{h^{\\dagger}}}{2\\gamma}\\ll1,\\quad\\gamma\\lesssim L_{p}^{-1/2},\\quad T^{\\dagger}\\lesssim L_{p}^{-1/2}\\wedge L_{s}^{-1/2},\\quad\\delta_{\\infty}\\lesssim\\delta\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "then the distribution $\\widetilde{\\pi}_{t_{n},T^{\\dag}}$ satisfies the following error bound: ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D_{\\mathrm{KL}}(\\pi_{t_{n},T^{\\dagger}}\\|\\widetilde{\\pi}_{t_{n},T^{\\dagger}})\\lesssim\\!T^{\\dagger}W_{2}^{2}(\\widetilde{q}_{t_{n},h_{n}},\\widetilde{p}_{t_{n+1}})+T^{\\dagger}\\delta_{\\infty}^{2}+d T^{\\dagger}{\\epsilon}^{\\dagger}^{2}+e^{-K^{\\dagger}}T^{\\dagger}h^{\\dagger}d}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\lesssim\\!W_{2}^{2}(\\widetilde{q}_{t_{n},h_{n}},\\widetilde{p}_{t_{n+1}})+\\delta^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "with a total of $^{\\cdot}K^{\\dagger}N^{\\dagger}=\\mathcal{O}\\left(\\log(d\\delta^{-2})\\right)$ approximate time complexity and $M=\\Theta\\left(d^{1/2}\\delta^{-2}\\right)$ space complexity for parallalizabie $\\delta$ -accurate score function computations. ", "page_idx": 43}, {"type": "text", "text": "Proof. Now, we continue the computation by plugging the decomposition in (C.24) and all the error bounds derived above into the equation. First for the last term in (C.23) ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{P}\\left[\\int_{0}^{h^{\\dagger}}\\left\\|s_{t_{n+1}}(\\tilde{u}_{t_{n},n^{\\uparrow}h^{\\dagger},\\lfloor\\frac{\\tau^{\\dagger}}{\\epsilon^{\\dagger}}\\rfloor,\\epsilon}^{(K^{\\dagger}-1)})-\\nabla\\log\\tilde{p}_{t_{n+1}}(\\tilde{u}_{t_{n},n^{\\uparrow}h^{\\dagger},\\tau^{\\dagger}}^{(K^{\\dagger})})\\right\\|^{2}\\mathrm{d}\\tau^{\\dagger}\\right]}\\\\ &{\\leq\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\! \n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "where the last inequality is by Lemma C.16. We further substitute Lemma C.14 and C.13 into (C.23) toobtain ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D_{\\mathrm{KL}}(\\pi_{t_{n},n\\uparrow h}||\\widetilde{\\pi}_{t_{n},n\\uparrow h}|)}\\\\ &{\\le D_{\\mathrm{KL}}(\\pi_{t_{n},(n-1)h}||\\widetilde{\\pi}_{t_{n},(n-1)h^{\\prime}})+5\\frac{2\\gamma+L_{\\delta}^{2}C_{K}!5h!^{2}e^{(3+\\gamma)h^{\\prime}}}{4\\gamma^{2}}\\mathbb{E}_{P}\\left[E_{t_{n},n^{\\prime}h^{\\prime}}+h^{\\mathrm{t}}\\delta_{\\infty}^{2}+L_{p}^{2}F_{t_{n},n^{\\prime}}\\right.}\\\\ &{\\left.+\\frac{5L_{\\delta}^{2}h!C_{K}}{2\\gamma}\\left(h^{\\mathrm{t}2}e^{(3+\\gamma)h^{\\prime}}\\left(3\\gamma\\right)d+M_{s}^{2}\\right)+h^{\\mathrm{t}}e^{2h^{\\prime}}d\\right)}\\\\ &{\\lesssim D_{\\mathrm{KL}}(\\pi_{t_{n},(n-1)h^{\\prime}}||\\widetilde{\\pi}_{t_{n},(n-1)h^{\\prime}}|)}\\\\ &{+5\\frac{2\\gamma+L_{\\delta}^{2}C_{K}!5h^{2}e^{(3+\\gamma)h^{\\prime}}}{4\\gamma^{2}}\\left(h^{\\mathrm{t}}(L_{s}^{2}+L_{p}^{2})W_{2}^{2}(\\widetilde{\\pi}_{t_{n},h_{n}},\\widetilde{\\gamma}_{t_{n+1}})+h^{\\mathrm{t}}\\delta_{\\infty}^{2}+d h^{\\mathrm{t}}\\epsilon^{2}\\right)}\\\\ &{+\\frac{5L_{\\delta}^{2}h^{2}\\gamma}{2\\gamma}\\left(h^{\\mathrm{t}2}e^{(3+\\gamma)h^{\\prime}}\\left(3\\gamma\\check{d}+M_{s}^{2}\\right)+h^{\\mathrm{t}}e^{2h^{\\prime}}\\right)}\\\\ &{\\lesssim D_{\\mathrm{KL}}(\\pi_{t_{n},(n-1)h}||\\widetilde{\\pi}_{t_{n},(n-1)h^{\\prime}})+h^{\\mathrm{t}}W_{2}^{2}(\\widetilde{\\pi}_{t_{n},h_{n}},\\widetilde{\\gamma}_{t_{n+1}})+h^{\\mathrm{t}}\\delta_{\\infty}^{2}+d h^{\\mathrm{t}}\\\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "and then sum over $n$ to obtain ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad D_{\\mathrm{KL}}(\\pi_{t_{n},T^{\\dagger}}\\|\\widetilde{\\pi}_{t_{n},T^{\\dagger}})=D_{\\mathrm{KL}}(\\pi_{t_{n},N^{\\dagger}h^{\\dagger}}\\|\\widetilde{\\pi}_{t_{n},N^{\\dagger}h^{\\dagger}})}\\\\ &{\\lesssim D_{\\mathrm{KL}}(\\pi_{t_{n},0}\\|\\widetilde{\\pi}_{t_{n},0})+N^{\\dagger}h^{\\dagger}W_{2}^{2}(\\widetilde{q}_{t_{n},h_{n}},\\widetilde{p}_{t_{n+1}})+N^{\\dagger}h^{\\dagger}\\delta_{\\infty}^{2}+d N^{\\dagger}h^{\\dagger}\\epsilon^{\\dagger^{2}}+e^{-K^{\\dagger}}N^{\\dagger}h^{\\dagger^{2}}d}\\\\ &{=T^{\\dagger}W_{2}^{2}(\\widetilde{q}_{t_{n},h_{n}},\\widetilde{p}_{t_{n+1}})+T^{\\dagger}\\delta_{\\infty}^{2}+d T^{\\dagger}\\epsilon^{\\dagger^{2}}+e^{-K^{\\dagger}}T^{\\dagger}h^{\\dagger}d.}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Then, it is straightforward to see that when the following order of the parameters holds ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{T^{\\dagger}=\\mathcal{O}(1),\\quad h^{\\dagger}=\\Theta(1),\\quad N^{\\dagger}=\\mathcal{O}(1),}}\\\\ {{\\epsilon^{\\dagger}=\\Theta(d^{-1/2}\\delta),\\quad M^{\\dagger}=\\mathcal{O}(d^{1/2}\\delta^{-1}),\\quad K^{\\dagger}=\\mathcal{O}(\\log(d\\delta^{-2}))}}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "and $\\delta_{\\infty}\\leq\\delta$ , we have ", "page_idx": 44}, {"type": "equation", "text": "$$\nD_{\\mathrm{KL}}(\\pi_{t_{n},T^{\\dagger}}||\\widetilde{\\pi}_{t_{n},T^{\\dagger}})\\lesssim W_{2}^{2}(\\widetilde{q}_{t_{n},h_{n}},\\overleftarrow{p}_{t_{n+1}})+\\delta^{2}.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Lemma C.18. Suppose $T^{\\dagger}\\lesssim L_{p}^{-1/2}$ , then we have ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\mathrm{TV}(\\pi_{t_{n},T^{\\dagger}},\\widetilde{p}_{t_{n+1}})\\leq\\sqrt{D_{\\mathrm{KL}}(\\pi_{t_{n},T^{\\dagger}}\\|\\widetilde{p}_{t_{n+1}})}\\lesssim\\frac{1}{L_{p}^{\\frac{1}{4}}(T^{\\dagger})^{\\frac{3}{2}}}W_{2}(\\pi_{t_{n},0},\\widetilde{p}_{t_{n+1}})\\lesssim W_{2}(\\widetilde{q}_{t_{n},h_{n}},\\widetilde{p}_{t_{n+1}}).\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Proof. A complete proof of the Lemma above is presented in [102, Lemma 9], which is derived based on [136, Corollary 4.7 (1) ]. \u53e3 ", "page_idx": 44}, {"type": "text", "text": "C.4 Overall Error Bound ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "We are now ready to prove Theorem 3.5. ", "page_idx": 45}, {"type": "text", "text": "Proof of Theorem 3.5. Notice that the interpolating corrector process $\\left(\\widetilde{\\mathbf{u}}_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}},\\widetilde{\\mathbf{v}}_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}\\right)$ is constructed to follow the same dynamics as the auxiliary corrector process $\\left(\\widehat{\\pmb{u}}_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}},\\widehat{\\pmb{v}}_{t_{n},n^{\\dagger}h^{\\dagger},\\tau^{\\dagger}}\\right)$ in the corrector step. Therefore, we have by data processing inequality that ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\mathrm{TV}(\\widehat{\\pi}_{t_{n},T^{\\dagger}}^{\\widehat{\\boldsymbol u}},\\widetilde{\\pi}_{t_{n},T^{\\dagger}}^{\\widetilde{\\boldsymbol u}})\\leq\\mathrm{TV}(\\widehat{\\pi}_{t_{n},0}^{\\widehat{\\boldsymbol u}},\\widetilde{\\pi}_{t_{n},0}^{\\widetilde{\\boldsymbol u}})=\\mathrm{TV}(\\widehat{q}_{t_{n},h_{n}},\\widetilde{q}_{t_{n},h_{n}}),\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "and again, since the interpolating predictor process $\\widetilde{\\pmb{y}}_{t_{n},n^{\\dagger}h^{\\dagger}}$ is constructed to follow the same dynamics as the auxiliary predictor process $\\widehat{\\pmb{y}}_{t_{n},n^{\\dagger}h^{\\dagger}}$ in the predictor step, we further have by data processing inequality that ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\mathrm{TV}(\\widehat{q}_{t_{n},h_{n}},\\widetilde{q}_{t_{n},h_{n}})\\leq\\mathrm{TV}(\\widehat{q}_{t_{n},0},\\widetilde{q}_{t_{n},0})=\\mathrm{TV}(\\widehat{q}_{t_{n}},\\widetilde{p}_{t_{n}}).\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Furthermore, applying triangle inequality, Pinsker's inequality along with Theorem C.17 and Theorem C.7 proved above, we may upper bound the second term above as follows ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\mathrm{TV}(\\pi_{t_{n},T^{\\dagger}},\\widetilde{\\pi}_{t_{n},T^{\\dagger}})^{2}\\lesssim D_{\\mathrm{KL}}(\\pi_{t_{n},T^{\\dagger}}\\|\\widetilde{\\pi}_{t_{n},T^{\\dagger}})\\lesssim W_{2}^{2}(\\widetilde{q}_{t_{n},h_{n}},\\widetilde{p}_{t_{n+1}})+\\delta^{2}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Summarizing the above inequalities, we have ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{TV}(\\widetilde{\\pi}_{t_{n},T^{\\dagger}}^{\\tilde{u}},\\widetilde{p}_{t_{n+1}})^{2}=\\mathrm{TV}(\\widetilde{\\pi}_{t_{n},T^{\\dagger}}^{\\tilde{u}},\\pi_{t_{n},T^{\\dagger}}^{*,u^{*}})^{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\mathrm{TV}(\\widetilde{\\pi}_{t_{n},T^{\\dagger}}^{\\tilde{u}},\\pi_{t_{n},T^{\\dagger}}^{u})^{2}+\\mathrm{TV}(\\pi_{t_{n},T^{\\dagger}}^{u},\\pi_{t_{n},T^{\\dagger}}^{*,u^{*}})^{2}}\\\\ &{\\qquad\\qquad\\leq\\mathrm{TV}(\\widetilde{\\pi}_{t_{n},T^{\\dagger}},\\pi_{t_{n},T^{\\dagger}})^{2}+\\mathrm{TV}(\\pi_{t_{n},T^{\\dagger}},\\pi_{t_{n},T^{\\dagger}}^{*})^{2}}\\\\ &{\\qquad\\qquad\\leq\\mathrm{TV}(\\widetilde{\\pi}_{t_{n},T^{\\dagger}},\\pi_{t_{n},T^{\\dagger}})^{2}+\\mathrm{TV}(\\pi_{t_{n},T^{\\dagger}},\\widetilde{p}_{t_{n+1}})^{2}}\\\\ &{\\qquad\\qquad\\lesssim W_{2}^{2}(\\widetilde{\\pi}_{t_{n},h_{n}},\\widetilde{p}_{t_{n+1}})+\\delta^{2}+W_{2}^{2}(\\widetilde{q}_{t_{n},h_{n}},\\widetilde{p}_{t_{n+1}})}\\\\ &{\\qquad\\qquad\\lesssim d e^{-K}+h_{n}^{2}\\delta_{\\infty}^{2}+d\\epsilon^{2}h_{n}^{2}+\\delta^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "where the second last inequality is deduced from Theorem C.17 and Lemma C.18 and the last inequality is derived via Theorem C.7. Therefore, for any $n\\in[0:N\\!-\\!1]$ , applying triangle inequality along with data processing inequality (cf. Theorem A.1) yields ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{TV}(\\widehat{q}_{t_{n+1}},\\breve{p}_{t_{n+1}})=\\mathrm{TV}(\\widehat{\\pi}_{t_{n},T^{\\dagger}}^{\\widehat{u}},\\breve{p}_{t_{n+1}})}\\\\ &{\\qquad\\qquad\\qquad\\leq\\mathrm{TV}(\\widehat{\\pi}_{t_{n},T^{\\dagger}}^{\\widehat{u}},\\widetilde{\\pi}_{t_{n},T^{\\dagger}}^{\\widetilde{u}})+\\mathrm{TV}(\\widetilde{\\pi}_{t_{n},T^{\\dagger}}^{\\widetilde{u}},\\breve{p}_{t_{n+1}})}\\\\ &{\\qquad\\qquad\\qquad\\leq\\mathrm{TV}(q_{t_{n}},\\breve{p}_{t_{n}})+d^{1/2}e^{-K/2}+h_{n}\\delta_{\\infty}+d^{1/2}\\epsilon h_{n}+\\delta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "where the last inequality is derived by plugging in (C.32), (C.33) and (C.36). Applying Lemma A.9 and summing the inequalities above further give us that ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{TV}(\\widehat{q}_{t_{N}},p_{\\eta})=\\mathrm{TV}(\\widehat{q}_{t_{N}},\\overleftarrow{p}_{t_{N}})}\\\\ &{\\qquad\\qquad\\lesssim\\mathrm{TV}(\\widehat{q}_{0},\\overleftarrow{p}_{0})+\\displaystyle\\sum_{n=0}^{N-1}\\left(d^{1/2}e^{-\\frac{K}{2}}+h_{n}\\delta+d^{1/2}\\epsilon h_{n}+\\delta\\right)}\\\\ &{\\qquad\\qquad\\lesssim d^{1/2}e^{-T/2}+N d^{1/2}e^{-K/2}+T\\delta_{\\infty}+d^{1/2}\\epsilon T+\\delta N.}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "By setting the parameters ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{T=\\mathcal{O}(\\log(d\\delta^{-2})),\\quad h=\\Theta(1),\\quad N=\\mathcal{O}(\\log(d\\delta^{-2})),}}\\\\ {{\\epsilon=\\Theta\\left(d^{-1/2}\\delta\\log^{-1}(d^{-1/2}\\delta^{-1})\\right),\\quad M=\\mathcal{O}(d^{1/2}\\delta^{-1}\\log(d^{1/2}\\delta^{-1})),\\quad K=\\widetilde{\\mathcal{O}}(\\log(d\\delta^{-2})),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "and letting $\\delta_{\\infty}\\lesssim\\delta T^{-1}\\lesssim\\delta\\log^{-1}(d\\delta^{-2})$ , we finally obtained the upper bound ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\mathrm{TV}(\\widehat{q}_{t_{N}},p_{\\eta})^{2}\\lesssim d e^{-T}+N^{2}d e^{-K}+\\delta^{2}+d\\epsilon^{2}T^{2}\\leq\\delta^{2}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "as desired. ", "page_idx": 45}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: We have carefully reviewed the abstract and introduction to ensure that they accurately reflect the paper's contributions and scope. ", "page_idx": 46}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: We have discussed the limitations of our work in Section 4 (Discussion and Conclusion). ", "page_idx": 46}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: We have provided the full set of assumptions and complete proofs for all theoretical results in the paper. ", "page_idx": 46}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 46}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 46}, {"type": "text", "text": "Justification: As a theoretical paper, we do not include experiments. ", "page_idx": 46}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 46}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 46}, {"type": "text", "text": "Justification: As a theoretical paper, we do not include experiments. ", "page_idx": 46}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 46}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 46}, {"type": "text", "text": "Justification: As a theoretical paper, we do not include experiments. ", "page_idx": 46}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 46}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 46}, {"type": "text", "text": "Justification: As a theoretical paper, we do not include experiments. ", "page_idx": 46}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce theexperiments? ", "page_idx": 46}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 46}, {"type": "text", "text": "Justification: As a theoretical paper, we do not include experiments. ", "page_idx": 46}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 47}, {"type": "text", "text": "Justification: We have reviewed the NeurIPS Code of Ethics and have ensured that our research conforms to it. ", "page_idx": 47}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 47}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 47}, {"type": "text", "text": "Justification: As a theoretical paper, we do not expect direct societal impacts of our work. ", "page_idx": 47}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 47}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 47}, {"type": "text", "text": "Justification: As a theoretical paper, we do not release data or models that have a high risk for misuse. ", "page_idx": 47}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 47}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 47}, {"type": "text", "text": "Justification: As a theoretical paper, we do not use existing assets. ", "page_idx": 47}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 47}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 47}, {"type": "text", "text": "Justification: As a theoretical paper, we do not introduce new assets. ", "page_idx": 47}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 47}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 47}, {"type": "text", "text": "Justification: As a theoretical paper, we do not involve crowdsourcing nor research with human subjects. ", "page_idx": 47}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 47}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 47}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 47}, {"type": "text", "text": "Justification: As a theoretical paper, we do not involve crowdsourcing nor research with human subjects. ", "page_idx": 47}]