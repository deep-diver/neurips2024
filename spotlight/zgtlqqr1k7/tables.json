[{"figure_path": "ZgtLQQR1K7/tables/tables_5_1.jpg", "caption": "Table 1: Performance comparison on ImageNet-1K. Throughput values are measured with an A100 GPU and an AMD EPYC 7542 CPU, using the toolkit released by [62], following the protocol proposed in [36]. All images are of size 224 \u00d7 224.", "description": "This table compares the performance of various vision models, including VMamba, on the ImageNet-1K image classification benchmark.  Metrics include the number of model parameters (Params), floating point operations (FLOPs), throughput (TP, images per second), and top-1 accuracy.  The table highlights VMamba's competitive performance and efficiency compared to other state-of-the-art models.", "section": "5.1 Image Classification"}, {"figure_path": "ZgtLQQR1K7/tables/tables_6_1.jpg", "caption": "Table 1: Performance comparison on ImageNet-1K. Throughput values are measured with an A100 GPU and an AMD EPYC 7542 CPU, using the toolkit released by [62], following the protocol proposed in [36]. All images are of size 224 \u00d7 224.", "description": "This table compares the performance of various vision models (Transformer-based, ConvNet-based, and SSM-based) on the ImageNet-1K image classification benchmark.  Metrics include model parameters, FLOPs (floating point operations), throughput (images per second), and Top-1 accuracy.  The table highlights VMamba's performance relative to existing state-of-the-art models.", "section": "5.1 Image Classification"}, {"figure_path": "ZgtLQQR1K7/tables/tables_18_1.jpg", "caption": "Table 1: Performance comparison on ImageNet-1K. Throughput values are measured with an A100 GPU and an AMD EPYC 7542 CPU, using the toolkit released by [62], following the protocol proposed in [36]. All images are of size 224 \u00d7 224.", "description": "This table compares the performance of VMamba against other state-of-the-art models on the ImageNet-1K dataset for image classification.  Metrics include the number of parameters (Params), GFLOPs (floating point operations), throughput (TP) in images per second, and top-1 accuracy.  It highlights VMamba's superior performance and throughput compared to models based on Convolutional Neural Networks (CNNs), Vision Transformers (ViTs), and other State Space Models (SSMs).", "section": "5.1 Image Classification"}, {"figure_path": "ZgtLQQR1K7/tables/tables_19_1.jpg", "caption": "Table 1: Performance comparison on ImageNet-1K. Throughput values are measured with an A100 GPU and an AMD EPYC 7542 CPU, using the toolkit released by [62], following the protocol proposed in [36]. All images are of size 224 \u00d7 224.", "description": "This table compares the performance of VMamba against various other transformer-based, convolutional neural network-based, and SSM-based models on the ImageNet-1K dataset.  The metrics used are the number of parameters (in millions), GigaFLOPs (GFLOPs), throughput (images per second), and Top-1 accuracy (%).  It highlights VMamba's superior performance and efficiency relative to its competitors.", "section": "5.1 Image Classification"}, {"figure_path": "ZgtLQQR1K7/tables/tables_19_2.jpg", "caption": "Table 1: Performance comparison on ImageNet-1K. Throughput values are measured with an A100 GPU and an AMD EPYC 7542 CPU, using the toolkit released by [62], following the protocol proposed in [36]. All images are of size 224 \u00d7 224.", "description": "This table compares the performance of VMamba against other state-of-the-art models on the ImageNet-1K dataset.  Metrics include the number of parameters (Params), GigaFLOPS (FLOPS), throughput (TP) in images per second, and Top-1 accuracy. The table is organized to compare VMamba against other Transformer-based models, ConvNet-based models, and other SSM-based models.", "section": "5.1 Image Classification"}, {"figure_path": "ZgtLQQR1K7/tables/tables_20_1.jpg", "caption": "Table 1: Performance comparison on ImageNet-1K. Throughput values are measured with an A100 GPU and an AMD EPYC 7542 CPU, using the toolkit released by [62], following the protocol proposed in [36]. All images are of size 224 \u00d7 224.", "description": "This table compares the performance of VMamba against various other state-of-the-art vision models on the ImageNet-1K image classification benchmark.  Metrics include the number of parameters (Params), GigaFLOPs (FLOPs), throughput (TP, images/second), and top-1 accuracy.  It highlights VMamba's superior performance and efficiency compared to Transformer-based and Convolutional Neural Network-based models.", "section": "5.1 Image Classification"}, {"figure_path": "ZgtLQQR1K7/tables/tables_21_1.jpg", "caption": "Table 7: Object detection and instance segmentation results on COCO dataset. FLOPs are calculated using inputs of size 1280 \u00d7 800. Here, APb and APm denote box AP and mask AP, respectively. \"1x\" indicates models fine-tuned for 12 epochs, while \"3\u00d7MS\" signifies the utilization of multi-scale training for 36 epochs.", "description": "This table presents a comparison of object detection and instance segmentation performance on the MS COCO dataset for different model architectures.  It shows the average precision (AP) metrics, including box AP (APb) and mask AP (APm), for different models with varying parameter counts and FLOPs.  The results are reported for both a 12-epoch fine-tuning schedule and a 36-epoch multi-scale training schedule.", "section": "5.2 Downstream Tasks"}, {"figure_path": "ZgtLQQR1K7/tables/tables_21_2.jpg", "caption": "Table 7: Object detection and instance segmentation results on COCO dataset. FLOPs are calculated using inputs of size 1280 \u00d7 800. Here, APb and APm denote box AP and mask AP, respectively. \"1x\" indicates models fine-tuned for 12 epochs, while \"3\u00d7MS\" signifies the utilization of multi-scale training for 36 epochs.", "description": "This table presents a comparison of the performance of various models on object detection and instance segmentation tasks using the COCO dataset.  It shows the average precision (AP) for both bounding boxes (APb) and masks (APm) at different intersection over union (IoU) thresholds. The models are trained using both a 12-epoch schedule and a 36-epoch multi-scale training schedule.  The table also includes the model parameters (Params) and floating point operations (FLOPs).", "section": "5.2 Downstream Tasks"}, {"figure_path": "ZgtLQQR1K7/tables/tables_22_1.jpg", "caption": "Table 8: Semantic segmentation results on ADE20K using UperNet [63]. We evaluate the performance of semantic segmentation on the ADE20K dataset with UperNet [63]. FLOPs are calculated with input sizes of 512 \u00d7 2048. \"SS\" and \"MS\" denote single-scale and multi-scale testing, respectively.", "description": "This table presents a comparison of semantic segmentation performance on the ADE20K dataset using different models.  It shows the mean Intersection over Union (mIoU) for both single-scale and multi-scale testing, along with model parameters and FLOPs. The models compared include Swin Transformer, ConvNeXt, and various versions of VMamba.", "section": "5.2 Downstream Tasks"}, {"figure_path": "ZgtLQQR1K7/tables/tables_24_1.jpg", "caption": "Table 1: Performance comparison on ImageNet-1K. Throughput values are measured with an A100 GPU and an AMD EPYC 7542 CPU, using the toolkit released by [62], following the protocol proposed in [36]. All images are of size 224 \u00d7 224.", "description": "This table compares the performance of VMamba against other state-of-the-art vision models on the ImageNet-1K dataset.  Metrics include the number of parameters (Params), GigaFLOPs (FLOPs), throughput (TP), and Top-1 accuracy.  The table highlights VMamba's superior performance and efficiency across different model sizes (Tiny, Small, Base).", "section": "5.1 Image Classification"}, {"figure_path": "ZgtLQQR1K7/tables/tables_25_1.jpg", "caption": "Table 10: The performance of VMamba-T with different scanning patterns.", "description": "This table compares the performance of VMamba-T using four different scanning patterns: Unidi-Scan, Bidi-Scan, Cascade-Scan, and Cross-Scan.  The comparison includes parameters (M), FLOPS (G), throughput (TP. img/s), training throughput (Train TP. img/s), and Top-1 accuracy (%).  It shows the impact of the scanning pattern on various performance metrics, with and without depthwise convolutions (dwconv).", "section": "H Ablation Study"}, {"figure_path": "ZgtLQQR1K7/tables/tables_25_2.jpg", "caption": "Table 1: Performance comparison on ImageNet-1K. Throughput values are measured with an A100 GPU and an AMD EPYC 7542 CPU, using the toolkit released by [62], following the protocol proposed in [36]. All images are of size 224 \u00d7 224.", "description": "This table compares the performance of VMamba with other state-of-the-art models on the ImageNet-1K image classification benchmark.  The metrics reported include the number of parameters (Params), GigaFLOPS (FLOPs), throughput (TP) in images per second, and the Top-1 accuracy.  It shows VMamba's performance advantage in accuracy and throughput, especially when considering the computational efficiency.", "section": "5.1 Image Classification"}, {"figure_path": "ZgtLQQR1K7/tables/tables_25_3.jpg", "caption": "Table 6: Performance comparison on ImageNet-1K with an image size of 224. \u2020 indicates that Vim is trained solely in float32 in practice, with a training throughput of 232. [69].", "description": "This table compares the performance of VMamba with other state-of-the-art models on the ImageNet-1K dataset using images of size 224x224.  Metrics include the number of parameters (Params), GigaFLOPs (FLOPs), throughput (TP, images/s), training throughput (Train TP, images/s), and top-1 accuracy.  The table highlights VMamba's competitive performance and efficiency compared to other transformer and convolutional neural network based models. Note that the Vim model's training throughput is obtained from a different source due to the practical use of float32 during its training phase.", "section": "5.1 Image Classification"}, {"figure_path": "ZgtLQQR1K7/tables/tables_25_4.jpg", "caption": "Table 1: Performance comparison on ImageNet-1K. Throughput values are measured with an A100 GPU and an AMD EPYC 7542 CPU, using the toolkit released by [62], following the protocol proposed in [36]. All images are of size 224 \u00d7 224.", "description": "This table compares the performance of various vision models, including VMamba, on ImageNet-1K.  Metrics include the number of parameters, GFLOPs, throughput (images per second), and top-1 accuracy.  It highlights VMamba's performance relative to other state-of-the-art models, showcasing its efficiency and accuracy.", "section": "5.1 Image Classification"}, {"figure_path": "ZgtLQQR1K7/tables/tables_25_5.jpg", "caption": "Table 1: Performance comparison on ImageNet-1K. Throughput values are measured with an A100 GPU and an AMD EPYC 7542 CPU, using the toolkit released by [62], following the protocol proposed in [36]. All images are of size 224 \u00d7 224.", "description": "This table compares the performance of various vision models on the ImageNet-1K image classification benchmark.  The metrics include the number of model parameters (Params), the number of floating point operations (FLOPs), the throughput (TP, images per second), and the top-1 accuracy.  The models are categorized into Transformer-based, ConvNet-based, and SSM-based models, allowing for comparison across different architectural approaches.", "section": "5.1 Image Classification"}, {"figure_path": "ZgtLQQR1K7/tables/tables_25_6.jpg", "caption": "Table 6: Performance comparison on ImageNet-1K with an image size of 224. \u2020 indicates that Vim is trained solely in float32 in practice, with a training throughput of 232. [69].", "description": "This table compares the performance of VMamba with other state-of-the-art vision models on the ImageNet-1K dataset. The models are evaluated based on parameters, FLOPs, throughput (images/second), training throughput, and top-1 accuracy.  The table highlights VMamba's efficiency and competitive performance compared to other Transformer-based, ConvNet-based, and SSM-based models.", "section": "5.1 Image Classification"}]