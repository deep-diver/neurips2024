{"references": [{"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-12-01", "reason": "This paper introduced the Transformer architecture, a foundational model for many subsequent advancements in natural language processing and computer vision, including the current paper's approach."}, {"fullname_first_author": "Ze Liu", "paper_title": "Swin transformer: Hierarchical vision transformer using shifted windows", "publication_date": "2021-10-27", "reason": "This work is a highly influential vision transformer, providing a strong benchmark against which the current paper's model is compared and showing the effectiveness of hierarchical representations for image classification."}, {"fullname_first_author": "Albert Gu", "paper_title": "Mamba: Linear-time sequence modeling with selective state spaces", "publication_date": "2023-12-01", "reason": "This paper introduces the Mamba model, which is the core algorithm adapted in the current paper's VMamba model, demonstrating the efficiency of State Space Models for sequential data."}, {"fullname_first_author": "Alexey Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2021-01-01", "reason": "This work is a pioneering application of the Transformer architecture to computer vision, demonstrating the potential of this approach, despite the high computational cost."}, {"fullname_first_author": "Kaiming He", "paper_title": "Deep residual learning for image recognition", "publication_date": "2016-01-01", "reason": "This paper is a highly influential work in deep learning, introducing residual connections that significantly improved the performance of convolutional neural networks, which are also relevant to the current paper's comparison benchmarks."}]}