[{"type": "text", "text": "$4+3$ Phases of Compute-Optimal Neural Scaling Laws ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Elliot Paquette\u2217 McGill University elliot.paquette@mcgill.ca ", "page_idx": 0}, {"type": "text", "text": "Courtney Paquette Google DeepMind & McGill University courtney.paquette@mcgill.ca ", "page_idx": 0}, {"type": "text", "text": "Lechao Xiao\u2020 Google DeepMind ", "page_idx": 0}, {"type": "text", "text": "Jeffrey Pennington\u2020 Google DeepMind ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We consider the solvable neural scaling model with three parameters: data complexity, target complexity, and model-parameter-count. We use this neural scaling model to derive new predictions about the compute-limited, infinite-data scaling law regime. To train the neural scaling model, we run one-pass stochastic gradient descent on a mean-squared loss. We derive a representation of the loss curves which holds over all iteration counts and improves in accuracy as the model parameter count grows. We then analyze the compute-optimal model-parameter-count, and identify 4 phases $^{+3}$ subphases) in the data-complexity/target-complexity phase-plane. The phase boundaries are determined by the relative importance of model capacity, optimizer noise, and embedding of the features. We furthermore derive, with mathematical proof and extensive numerical evidence, the scalinglaw exponents in all of these phases, in particular computing the optimal modelparameter-count as a function of floating point operation budget. We include a colab notebook nanoChinchilla3 that reproduces some key results of the paper. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The advent of large language models (LLMs) has changed our perceptions of the landscape of optimization and is resulting in the emergence of new interesting questions related to scaling. Prior to LLMs and other large models, we often viewed the large-scale optimization problems as being limited by the amount of data. In training language models, in contrast, data can be effectively infinite. Thus, compute budgets can be the limitation. This leads to the following natural question: given an architecture, given a fixed compute budget, and having unlimited data, how should one select the model size to minimize loss? ", "page_idx": 0}, {"type": "text", "text": "To formally address this question, let us consider the general learning problem, ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta\\in\\mathbb{R}^{d}}\\big\\{\\mathcal{P}(\\theta)=\\mathbb{E}_{x}[\\mathcal{R}(\\theta;x)]\\big\\},\\quad\\mathrm{where~}\\mathcal{R}:\\mathbb{R}^{d}\\rightarrow\\mathbb{R},\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "the number of parameters $d$ is large, and the data vector $x$ is drawn from an unknown distribution. We solve (1) using stochastic algorithms, such as stochastic gradient descent (SGD) with batch size $B$ , under various parameter sizes $d$ , that produce a sequence of iterates $\\{\\theta_{r}\\}$ . A standard formula used in practice to measure compute is the \"6ND\" formula [25], that is, ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\operatorname{Compute}\\ (\\operatorname{flops}^{4}\\!\\uparrow)=\\left(\\operatorname{iterations\\of\\alg.}\\ (r)\\ \\times\\operatorname{batch\\size}\\ (B)\\right)\\ \\times\\ \\operatorname{parameters}\\ (d).\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "Therefore, we can plot the loss curve $\\mathcal{P}(\\theta_{r};d)=\\mathcal{P}(r;d)=\\mathcal{P}(\\mathfrak{f}/(d\\cdot B);d)$ as a function of flops (see Fig. 1). The question now is: given a fixed number of flops $\\mathfrak{f}$ and given batch size $B$ , how should we choose the parameters $d$ so that we get the best loss, i.e. find $d^{\\star}$ solving the constrained problem ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r}{d^{\\star}(\\mathfrak{f})\\in\\arg\\operatorname*{min}_{d}\\mathcal{P}\\big(\\frac{\\mathfrak{f}}{d\\cdot B};d\\big)=\\arg\\operatorname*{min}_{d}\\{\\mathcal{P}(\\theta_{r};d)\\mathrm{~subj.~to~}\\,\\mathfrak{f}=(r\\times B)\\times d\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Main contributions. In this work, we analyze a three parameter simple model, which we call powerlaw random features (PLRF) [29, 5]. The three parameters in the PLRF are the data complexity $(\\alpha)$ , target complexity $(\\beta)$ and model-parameter count $d$ . Using this model, we derive a deterministic equivalent for the expected loss, as a function of $\\alpha,\\beta$ , and $d$ , that captures the training dynamics of one-pass SGD. This can be used to derive numerical predictions for the scaling laws. We also extract exact expressions for the compute-optimal scaling laws and the optimal parameter $\\begin{array}{r}{d^{\\star}(\\mathfrak{f})\\,\\in\\,\\mathrm{argmin}_{d}\\mathcal{P}(\\frac{\\mathfrak{f}}{d\\cdot B};d)}\\end{array}$ for large5 $d,$ and give some estimates on the order of necessary for these scaling laws to take hold. ", "page_idx": 1}, {"type": "text", "text": "We also observe for a large portion of the $(\\alpha,\\beta)$ - phase plane, the optimal parameter is $d^{\\star}(\\mathfrak{f})=\\mathfrak{f}^{1/2}$ , suggesting a regime of universal scaling behavior (see Fig. 4a and Table 2). This verifies theoretically the Chinchilla scaling [23]. ", "page_idx": 1}, {"type": "text", "text": "The PLRF is not only analyzable, but also exhibits a rich behavior of compute-optimal curves/loss curves, which are qualitatively and quantitatively different depending on the strengths of the data $\\left(\\alpha\\right)$ vs. target $(\\beta\\bar{)}$ complexity. Particularly, we show that there are $^{4}$ distinct $+3$ sub phases) compute-optimal curve/loss curve behaviors. ", "page_idx": 1}, {"type": "text", "text": "Model constrained compute-optimal curves. In two of the phases (Phase Ia,b,c and Phase II), it is the underlying model that dictates the curves. The algorithm has little/no impact. This appears in two forms. The first behavior are compute-optimal curves controlled by the capacity of the model (Phase Ia,b,c). Here once the algorithm reaches the limiting risk value possible (capacity), it is better to increase the model-parameter $d$ . Another type of loss dynamics is due to poor model feature embedding (Phase II). Here the features are embedded in a way which is difficult to train. After an initial large decrease in the loss value, this feature embedding distortion frustrates the algorithm and training slows, but it continues to solve. However, solving to capacity wastes compute, in that it is compute-favored to increase the model parameter count $d$ . ", "page_idx": 1}, {"type": "image", "img_path": "aVSxwicpAk/tmp/24fcc7239d5af0d2af3066103a6d27e622d30004bf8b9a1347788e63531792b5.jpg", "img_caption": ["Figure 1: Toy scaling problem. We plot the loss function, $\\mathcal{P}(\\boldsymbol{\\theta}_{r};\\boldsymbol{d})$ as a function of flops $\\mathfrak{f}$ using (2). Consider a fixed number of flops $\\dot{\\mathsf{f}}=1\\bar{0}^{7}$ (dashed line). If we had chosen, e.g., $d\\,=\\,1600$ , we can run for a long time, but our model does not have a lot of capacity and thus the value of the loss function remains high. On the hand, we can increase capacity by choosing a large number of parameters (e.g., $d=51$ , 200), but because our compute is fixed we can not run our algorithm for very long. Thus the loss value is still large. The optimal choice is $d\\approx6$ , 400. When done for every choice of $\\mathfrak{f}$ gives the compute-optimal curve (red line). This choice of $(\\alpha,\\beta)$ (Phase I) is an example of where model capacity controls the compute-optimal curve, but it is not the only behavior we show. In other phases the compute-optimal is controlled by poor model embedding (Phase II, III) and $S G D$ noise (Phase III, IV). "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Algorithm constrained compute-optimal curves. For some choices of $(\\alpha,\\beta)$ (Phase III and IV), it is the noise produced by the SGD algorithm that ultimately controls the tradeoff. Here the algorithm matters. Indeed, another algorithm could change the compute-optimal curves for these phases. ", "page_idx": 1}, {"type": "text", "text": "Related work. The key source of inspiration for this work are [23, 25], which identified compute optimality as a fundamental concept in scaling large language models and made a substantial empirical exploration of it. The problem setup was formulated by [29], where additionally data-limited scalings were considered, but compute optimality was not (nor indeed any algorithmic considerations); see also [8] where gradient flow is considered in the same setting. ", "page_idx": 1}, {"type": "image", "img_path": "aVSxwicpAk/tmp/0ded4fb8a51f6e886ae16f99a5263c0f2917ddb1694d5e27159ce1df9b2484e2.jpg", "img_caption": ["(a) Phase Diagram "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Figure 2: Phase Diagram and Cartoon Plots of Loss Curves in Different Phases. (a) Phase Diagram. Colored regions represent where the training of the risk/compute-optimal curves look qualitatively and quantitatively different depending on $\\alpha$ and $\\beta$ . This, in term, yields different scaling law $(\\eta)$ and parameter count $(\\xi)$ exponents for each of the phases. Critical point at $\\alpha=\\beta=1/\\bar{2}$ where all behaviors are observed. The other plots illustrate the components of $\\mathcal{F}$ (via $\\mathcal{F}_{0},\\mathcal{F}_{p p},\\mathcal{F}_{a c})$ and $\\mathcal{K}_{p p}$ which dominate the loss curve for each phase (see Sec. C.4.1 & Sec. C.4.1 for proofs); tradeoff between the functions where the compute-optimal point occurs is also indicated (see Sec. 2.1 for definitions and Sec. 3.1 & Sec. D for proofs). ", "page_idx": 2}, {"type": "text", "text": "There is a substantial body of work considering scaling laws of losses (trained to minimum-loss) of dataset size vs parameter count, in a variety of settings (linear, random features, deep networks). See especially: [5, 38, 40], wherein a \u201chidden-manifold\u201d model is considered for the data. We note that as we consider one-pass SGD, some dataset/parameter-count scaling laws are implicit from the results here; however, the training method (one-pass SGD) is, in some regimes, suboptimal given unlimited compute. ", "page_idx": 2}, {"type": "text", "text": "For additional related work on random features models (and sample complexity), random matrix theory in machine learning, and other deterministic equivalents for SGD, see Section A. We note that while this paper is fundamentally about computation, but the novel mathematical contributions could also be recast in terms of generalization bounds of one-pass SGD, some of which are new. For a detailed comparison of the convergence rates and sample complexity, see Table 4. ", "page_idx": 2}, {"type": "text", "text": "1.1 Problem Setup: SGD on Power-law Random Features ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this work, we analyze the three parameter power-law random features (PLRF) model, that is, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta\\in\\mathbb{R}^{d}}\\;\\big\\{\\mathcal{P}(\\theta)\\stackrel{\\mathrm{def}}{=}\\mathbb{E}_{x}[(\\langle W^{T}x,\\theta\\rangle-\\langle x,b\\rangle)^{2}]\\big\\}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "We embed the data vector $x\\,\\in\\,\\mathbb{R}^{v}$ in $\\mathbb{R}^{d}$ through the matrix $W\\,\\in\\,\\mathbb{R}^{v\\times d}$ and construct noiseless targets6 by dotting a fixed $b\\in\\mathbb{R}^{v}$ with the sample $x$ . The use of the matrix $W$ allows the model to have variable capacity $(d)$ independent of the data set size. The samples $x\\in\\mathbb{R}^{v}$ and labels $b\\in\\mathbb{R}^{v}$ have power law dependence, whereas the matrix $W$ has entries distributed as $N(0,1/d)$ . ", "page_idx": 2}, {"type": "image", "img_path": "aVSxwicpAk/tmp/aee7f30d6888eefc59ddf2c29aff38792c826d8afd8dcf0fc1cf6046278e33ff.jpg", "img_caption": ["(a) Compute-optimal Front ", "(b) IsoFLOP ", "(c) Optimal Model Size "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 3: Compute-Optimal Front in Phase II-III boundary. (a) The Volterra equations perfectly captures the training dynamics of SGD when model-parameter count ranges from $d=200\\to12800$ . (b) We apply IsoFLOP approach [23] to our toy model to extract the optimal-compute front: (computeoptimal loss) (highlighted in red in (a)) and the optimal model size: (compute-optimal model size) (scattered in purple in (c)). Power-law fitting compute-optimal front gives a measurement of the scaling law exponent 0.648 (vs. theoretical prediction 0.643 in Table 2). In (c), we power-law fti the relation between compute and (empirical) optimal model size via Approach 1 and 2 used in [23]: $d^{\\star}\\asymp\\mathsf{f}^{0.508}$ and $d^{\\star}\\asymp\\bar{\\mathsf{f}}^{0.525}$ , resp. (vs. theory, $d^{\\star}\\asymp\\mathsf{f}^{0.5},$ ). See Sec. J for details. ", "page_idx": 3}, {"type": "text", "text": "Assumption 1 (Data and labels, $\\alpha$ and $\\beta)$ . The samples $x~\\in~\\mathbb{R}^{v}$ are distributed according to $(x_{j})\\sim j^{-\\alpha}z_{j}$ for all $1\\leq j\\leq v$ and $\\{z_{j}\\}_{j=1}^{v}\\sim N(0,1)$ . The labels are scalars constructed by dotting the sample $x$ with a signal $b\\in\\mathbb{R}^{v}$ whose entries $(b_{j})=j^{-\\beta}$ . ", "page_idx": 3}, {"type": "text", "text": "Without the random matrix $W$ , the $\\alpha,\\beta$ are related to what is known in the literature as source and capacity conditions [10, 11, 19, 35]. For a detailed comparison of the parameters and related work, see Section A and Table 3. ", "page_idx": 3}, {"type": "text", "text": "The dimensions we consider throughout are always such that $v\\geq C d$ for $C>1$ . Throughout both $v$ and $d$ need to be large, but for some choices of $\\alpha$ and $\\beta$ , the $v$ will need to be comparable to $d$ . ", "page_idx": 3}, {"type": "text", "text": "Definition 1.1 (Admissible $v$ and $d_{.}$ ). We assume that $v\\geq C d$ with $C>1$ and $v,d\\rightarrow\\infty$ . Above the high-dimensional line, which is when $2\\alpha>1$ , we suppose $v/d\\to r\\in(1,\\infty)\\cup\\{\\infty\\}.$ 7 On the other hand, below the high-dimensional line $2\\alpha<1$ ) we limit $v$ to be $v/d\\to r\\in(1,\\infty)$ .8 ", "page_idx": 3}, {"type": "text", "text": "One can rewrite the expression in (4) using the convenient form: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta\\in\\mathbb R^{d}}\\;\\big\\{\\mathcal P(\\theta)=\\langle D(W\\theta-b),(W\\theta-b)\\rangle\\big\\},\\quad\\mathrm{where~}D=\\mathrm{diag}(j^{-2\\alpha})\\in\\mathbb{R}^{v\\times v}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Algorithmic set-up. To solve the minimization problem in (5), we use one-pass SGD with minibatches of size $B$ (independent of $d)^{9}$ and constant learning rate $\\gamma>0$ : letting $\\theta_{0}=0$ , we iterate ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{drawing~}\\{x_{r}^{i}\\}_{i=1}^{B}\\mathrm{~fresh~iid~samples~and~}\\theta_{r+1}=\\theta_{r}-\\gamma\\sum_{i=1}^{B}W^{T}x_{r}^{i}\\left[\\langle W^{T}x_{r}^{i},\\theta_{r}\\rangle-\\langle x_{r}^{i},b\\rangle\\right].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The learning rate and batch size will need to satisfy a condition to ensure convergence (Prop. 2.1). ", "page_idx": 3}, {"type": "text", "text": "Main goal. Under this setup, our main goal is to characterize the compute-optimal frontier. Precisely, we want to find the parameter count exponent $\\xi$ and scaling law exponent $\\eta$ , such that, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{d^{\\star}(\\mathfrak{f})\\asymp\\mathfrak{f}^{\\xi}\\quad\\mathrm{and}\\quad\\mathcal{P}\\big(\\frac{\\mathfrak{f}}{d^{\\star}B};d^{\\star}\\big)\\asymp\\mathfrak{f}^{-\\eta}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Notation. We use $\\mathcal{P}(\\theta_{r})=\\mathcal{P}(r)$ when we want to emphasize the iteration counter $r$ . We say $\\boldsymbol{\\mathcal{A}}(r,v,d)\\,\\sim\\,\\boldsymbol{\\mathcal{A}}(r,v,d)$ for functions $\\mathcal{A}(r,v,d),\\mathcal{A}(r,v,d)\\;>\\;0$ if for every $\\varepsilon\\ >\\ 0$ and for all admissible $v$ and $d$ , there exists an $r_{0},d_{0}$ such that for all $d>d_{0}$ and $r\\geq r_{0}$ ", "page_idx": 3}, {"type": "equation", "text": "$$\n(1-\\varepsilon)\\mathcal{A}(r,v,d)\\leq\\mathcal{A}(r,v,d)\\leq(1+\\varepsilon)\\mathcal{A}(r,v,d).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "table", "img_path": "aVSxwicpAk/tmp/c1aadf023c0567e22bc134f9b04d4c0c6ab70a3b4f652b5f56bee3b09e0dd56f.jpg", "table_caption": ["Table 1: Large $d$ behavior of the forcing function and kernel function. See Sec. H for proofs. "], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "We write $\\asymp$ if the upper and lower bounds hold with some constants $c,C$ in place of $1\\mp\\varepsilon$ respectively and $\\lesssim,\\gtrsim$ if only one inequality holds. ", "page_idx": 4}, {"type": "text", "text": "2 Learning Dynamics of SGD ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Compute-optimal curves (3) for the random features model (4) rely on accurate predictions for the learning trajectory of SGD. Similar to the works of [32, 34], we show that the expected loss under SGD satisfies a convolution-type Volterra equation (for background on Volterra equations, see Section C.3) ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathcal{P}(\\theta_{r})\\,|\\,W]=\\underbrace{\\mathcal{F}(r)}_{\\mathrm{grad.\\,descent}}+\\underbrace{\\mathcal{K}*\\mathbb{E}\\left[\\mathcal{P}(\\theta_{r})\\,|\\,W\\right]}_{\\mathrm{SGD~noise}},\\,\\mathrm{where}\\;(\\mathcal{K}*f)(r)=\\sum_{s=0}^{r-1}\\mathcal{H}(r-1-s)f(s).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The forcing function ${\\mathcal{F}}(r)$ and kernel function $\\mathcal{H}(r)$ are explicit functions of the matrix $\\hat{K}\\,=$ $D^{1/2}W W^{\\bar{T}}D^{1/2}$ , where $D=\\mathrm{Diag}(j^{-2\\alpha},1\\leq j\\leq v)$ , and $\\Gamma\\subset\\mathbb{C}$ a contour enclosing the spectrum of $\\hat{K}\\in[0,1]$ , ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{F}(r)\\overset{\\mathrm{def}}{=}\\frac{-1}{2\\pi i}\\displaystyle\\oint_{\\Gamma}\\langle(\\hat{K}-z)^{-1}(D^{1/2}b),(D^{1/2}b)\\rangle(1-2\\gamma B z+2\\gamma^{2}B^{2}z^{2})^{r}\\ \\mathrm{d}z}\\\\ &{\\quad\\mathrm{and}\\quad\\mathcal{M}(r)\\overset{\\mathrm{def}}{=}\\frac{-1}{2\\pi i}\\mathrm{Tr}\\bigg(\\displaystyle\\oint_{\\Gamma}(\\hat{K}-z)^{-1}z^{2}(1-2\\gamma B z+2\\gamma^{2}B^{2}z^{2})^{r}\\ \\mathrm{d}z\\bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Intuitively, the forcing function is gradient descent on the random features model and the kernel function is the excess risk due to 1 unit of SGD noise. ", "page_idx": 4}, {"type": "text", "text": "Deterministic equivalent. The forcing function ${\\mathcal{F}}(r)$ and kernel function $\\mathcal{H}(r)$ are random functions depending on the random matrix $\\hat{K}$ . Indeed, it is the resolvent of $\\hat{K}$ , $(\\hat{K}-z)^{-1}$ , which plays a significant role in $\\mathcal{F}$ and $\\mathcal{K}$ . We remove this randomness from the expression by using a deterministic equivalent \u2013 a technique from random matrix theory. ", "page_idx": 4}, {"type": "text", "text": "Formally, we define the deterministic equivalent for the resolvent of $\\hat{K}$ , denoted by $\\mathcal{R}(z)$ , implicitly via a fixed point equation ", "page_idx": 4}, {"type": "equation", "text": "$$\nm(z)\\stackrel{\\mathrm{def}}{=}\\frac1{1+\\frac1d\\sum_{j=1}^{v}\\frac{j^{-2\\alpha}}{j^{-2\\alpha}m(z)-z}}\\quad\\mathrm{where}\\quad\\Re(z)=\\mathrm{Diag}\\Bigg(\\frac1{j^{-2\\alpha}m(z)-z}\\,:\\,1\\leq j\\leq v\\Bigg).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This deterministic equivalent $\\mathcal{R}(z)$ is viewed, roughly, as $\\mathbb{E}_{W}[(\\hat{K}-z)^{-1}]\\approx\\mathcal{R}(z)$ ; though it is not formally the expectation over $W$ . By replacing the resolvent of $\\hat{K}$ with $\\mathcal{R}(z)$ , there exists a ", "page_idx": 4}, {"type": "image", "img_path": "aVSxwicpAk/tmp/18719e2a971c357e3603a38b3e9d0171d9b210b0e84390b9ef5d882dd6d3cef0.jpg", "img_caption": ["(a) Scaling Law Exponent "], "img_footnote": [], "page_idx": 5}, {"type": "image", "img_path": "aVSxwicpAk/tmp/69a2328124349c591af0d1017ad8e0c99b22a3e6003acab101c9654d5cdb3bd0.jpg", "img_caption": ["(b) Empirical vs Theory $(\\alpha=0.7)$ ) "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 4: (a) Scaling Law Exponents. The heatmap displays scaling law exponents $(\\eta)$ in the $(\\alpha,\\beta)$ -plane. Hatched lines represent region with universal scaling behavior, $d^{\\star}\\asymp\\mathsf{f}^{0.5}$ , independent of $(\\alpha,\\beta)$ . (b) Exponent Measurements. Compare empirical exponents (following [23]; see Sec.J for details) to theoretical predictions, traversing the phase diagram horizontally at $\\alpha=0.7$ from Phases $\\mathrm{Ia}\\rightarrow\\mathrm{II}\\rightarrow\\mathrm{III}$ as $\\beta\\uparrow$ . ", "page_idx": 5}, {"type": "text", "text": "deterministic function $\\mathcal{P}(r)$ which solves a convolution Volterra equation, matching (7): ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\Phi(r)=}&{{}\\underbrace{\\mathcal{F}\\tilde{(}r)}_{\\end{array}}+\\underbrace{(\\mathcal{K}*\\mathcal{P})(r)}_{\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{where}\\quad\\mathcal{F}(r)\\stackrel{\\mathrm{def}}{=}\\displaystyle\\frac{1}{2\\pi i}\\int_{\\Gamma}\\langle(\\Re(z)(D^{1/2}b),(D^{1/2}b)\\rangle(1-2\\gamma B z+2\\gamma^{2}B^{2}z^{2})^{r}\\ \\mathrm{d}z}\\\\ &{\\quad\\mathrm{and}\\quad\\mathcal{K}(r)\\stackrel{\\mathrm{def}}{=}\\gamma^{2}B\\cdot\\mathrm{Tr}\\bigg(\\displaystyle\\frac{-1}{2\\pi i}\\oint_{\\Gamma}\\Re(z)z^{2}(1-2\\gamma B z+2\\gamma^{2}B^{2}z^{2})^{r}\\ \\mathrm{d}z\\bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The solution to the Volterra equation with deterministic equivalent (10) numerically exactly matches the training dynamics of SGD, see Fig. 3. A discussion of the deterministic equivalent for $(\\hat{K}-z)^{-1}$ can be found in Sec. E. All our mathematical analysis will be for the deterministic equivalents, going forward.10 The derivation of the Volterra equation for the expected loss can be found in Sec. B. ", "page_idx": 5}, {"type": "text", "text": "An immediate consequence of (10) is that for convolution Volterra equations bounded solutions occur if and only if the forcing function is bounded and the kernel norm $\\begin{array}{r}{\\|\\mathcal{K}\\|\\stackrel{\\mathrm{def}}{=}\\sum_{s=0}^{\\infty}\\mathcal{K}(s)<1}\\end{array}$ . This directly translates into a sufficient condition on the batch size and learning rate of SGD. ", "page_idx": 5}, {"type": "text", "text": "Proposition 2.1 (Sufficient conditions on learning rate and batch). Suppose learning rate $\\gamma$ and batch $B$ satisfy $\\|\\mathcal{K}\\|<1$ and $\\gamma B<1$ . Then $\\mathcal{P}(r)$ is bounded. ", "page_idx": 5}, {"type": "text", "text": "Remark 2.1. Below the line $2\\alpha=1$ , the kernel norm diverges with v for fixed constant $\\gamma$ , and so we must take $\\gamma\\to0$ to ensure bounded solutions. Thus, provided $\\gamma\\sim v^{2\\alpha-1}$ , then ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\left\\|\\mathcal{K}\\right\\|\\sim\\frac{\\gamma}{2}\\sum_{j=1}^{v}j^{-2\\alpha}\\sim\\frac{\\gamma}{2(1-2\\alpha)}v^{1-2\\alpha}\\quad i s\\;o r d e r\\;I.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Thus, the kernel norm, $\\|\\mathcal K\\|$ , is always constant order for all $\\alpha$ . ", "page_idx": 5}, {"type": "text", "text": "The batch $B$ and $\\gamma$ can depend on $d$ . For simplicity, we only consider $B$ order 1 in this work. For a proof of the necessary and sufficient conditions on $\\gamma$ and $B$ , see Prop. C.2, and see Cor. G.1 for the asymptotic on $\\lVert\\mathcal{K}\\rVert$ . ", "page_idx": 5}, {"type": "text", "text": "The Volterra equation in (10) can be analyzed to give a more explicit formula for $\\Phi$ (see Section C.3.2 for proof). ", "page_idx": 5}, {"type": "image", "img_path": "aVSxwicpAk/tmp/afa40f4bfce94ca79b8af4ed9ebc957a3df57450c37471ad35284a966a0547b6.jpg", "img_caption": [], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 5: Finite-size effects. (a) The ratio of the exact solution of eq. (10) to the estimate in eq. (17) is bounded by constants for all $r$ , confirming the validity of eq. (17); shown here is $(\\alpha,\\beta)=(0.\\bar{7},1.2)$ . ${\\bf(b)}$ For non-asymptotic $d$ , the estimate in eq. (17) (solid curves) predicts both the magnitudes and trends of the measured exponents of the empirical compute-optimal frontier (points), shown here for $(\\alpha,\\beta)=(0.7,1.2)$ computed using Approach 0 (see Appendix J) to capture the instantaneous slope; the dashed lines show the asymptotic exponents from Table 2. (c) The finite-size behavior relaxes to the asymptotic predictions over horizons whose length can grow exceedingly large, especially in the vicinity of the phase transition, shown here for $\\beta=0.7$ approaching the Phase $4\\mathrm{a}{\\rightarrow}4\\mathrm{b}$ boundary. ", "page_idx": 6}, {"type": "text", "text": "Theorem 2.1 (Approximation solution for $\\Phi$ ). Suppose $\\gamma$ and $B$ are at most half the convergence threshold and $2\\alpha+2\\beta>1$ , $\\alpha>\\textstyle{\\frac{1}{4}}$ .11 There exists an $M>0$ large and a constant $C=C(\\alpha,\\beta,M)$ , independent of $d,$ , so that for all admissible $v$ and $d,$ , for all $\\gamma B r>M$ , ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{F}(r)+(\\mathcal{K}*\\mathcal{F})(r)\\leq\\mathcal{P}(r)\\leq\\mathcal{F}(r)+C\\times(\\mathcal{K}*\\mathcal{F})(r).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The convolution $\\mathcal{K}*\\mathcal{F}$ further simplifies ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\tilde{c}\\times\\left(\\mathcal{F}(r)+\\frac{1}{\\gamma B}\\cdot\\mathcal{K}(r)\\right)\\leq(\\mathcal{K}*\\mathcal{F})(r)\\leq\\tilde{C}\\times\\big(\\underbrace{\\mathcal{F}^{o r}{\\times}\\rho^{\\mathit{f i n o t}}}_{g r a d.\\;d e s c e n t}+\\frac{1}{\\gamma B}\\cdot\\underbrace{\\mathcal{K}^{(r)}}_{S G D\\;n o i s e}\\big).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "for some constants $\\tilde{c}=\\tilde{c}(\\alpha,\\beta,M)$ and $\\tilde{C}=\\tilde{C}(\\alpha,\\beta,M)>0$ independent of $d,$ . ", "page_idx": 6}, {"type": "text", "text": "Remark 2.2. If we were to run gradient descent instead of SGD (i.e., $\\gamma$ small), then we would only have the forcing term, that is, $\\bar{\\mathcal{P}(\\boldsymbol{r})}=\\mathcal{F}(\\boldsymbol{r})$ . The measurable effect of SGD comes from the second term that contains the kernel function. For this reason, we refer to SGD noise as $\\begin{array}{r}{\\frac{1}{\\gamma B}\\cdot\\mathcal{K}(r)}\\end{array}$ . ", "page_idx": 6}, {"type": "text", "text": "In light of (13) and (14), we have trapped the training loss between the sum of $\\mathcal{F}$ and $\\mathcal{K}$ , so it suffices now to understand the forcing and kernel functions. ", "page_idx": 6}, {"type": "text", "text": "2.1 Forcing function and kernel function ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We decompose the forcing function (11), $\\mathcal{F}$ , and the kernel function, (12), $\\mathcal{K}$ , into ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{F}(r)=\\mathcal{F}_{0}(r)+\\mathcal{F}_{a c}(r)+\\mathcal{F}_{p p}(r)+\\mathrm{errors}_{\\mathcal{F}}(r)\\quad\\mathrm{and}\\quad\\mathcal{K}(r)=\\mathcal{K}_{p p}(r)+\\mathrm{errors}_{\\mathcal{K}}(r).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Each term is explicit and has an asymptotic equivalence (when $1\\lesssim\\gamma B r\\lesssim d^{2\\alpha})$ given by ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{F}_{i}(r,d),\\mathcal{K}_{p p}(r,d)\\sim c\\times r^{-\\tau}\\times d^{-\\sigma}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The two error terms are such that for large $d$ with $1\\lesssim\\gamma B r\\lesssim d^{2\\alpha}$ , ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\mathrm{errors}_{\\mathcal{F}}(r)|\\leq C\\times(\\mathcal{F}_{0}(r)+\\mathcal{F}_{a c}(r)+\\mathcal{F}_{p p}(r))\\quad\\mathrm{and}\\quad|\\mathrm{errors}_{\\mathcal{K}}(r)|\\leq C\\times\\mathcal{K}_{p p}(r),}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "for some constant $C>0$ . For $\\gamma B r\\gtrsim d^{2\\alpha}$ , the forcing function $\\mathcal{F}(r)\\asymp\\mathcal{F}_{0}(r)$ , the limiting risk value.   \nThe terms arise from different parts of the spectrum of the deterministic equivalent for K\u02c6 (see Fig. 6). ", "page_idx": 6}, {"type": "text", "text": "1. Point mass at $0;\\,\\mathcal{F}_{0}(0)=\\mathcal{F}_{0}(r)$ is the limiting value of $\\mathcal{P}(r)\\asymp d^{-2\\alpha+\\operatorname*{max}\\{0,1-2\\beta\\}}$ as $r\\rightarrow\\infty$ . It occurs because the loss is irreducible $(d<v)$ , that is a component of the target is not in the image of the RF model (or equivalently that $\\hat{K}$ has a kernel). ", "page_idx": 6}, {"type": "text", "text": "2. Aligned features: The function $\\mathcal{F}_{p p}(r)$ represents gradient descent on the components of features which are aligned to the underlying population features. Indeed, if we ran gradient descent on the population loss without a random features map (or a diagonal $W$ ), this would be the loss curve.   \n3. Distorted features: The function $\\mathcal{F}_{a c}(\\boldsymbol{r})$ is the result of feature distortion, where the matrix $W$ leads to an embedding where a small component of the leading features is distributed across many different eigenmodes. These are still solvable, and given enough compute these will eventually be used, but they are much slower to solve.   \n4. Aligned kernel: $\\mathcal{K}_{p p}(r)$ is the excess risk due to 1 unit of SGD noise, which is then solved according to population gradient descent. ", "page_idx": 7}, {"type": "text", "text": "Out of brevity, we relegate the exact definitions of ${\\mathcal{F}}_{0}$ , ${\\mathcal F}_{p p}$ , $\\mathcal{F}_{a c}$ , and $\\mathcal{K}_{p p}$ and all proofs of the asymptotics in Table 1 and analyses of the functions to Section F, G, and H. ", "page_idx": 7}, {"type": "text", "text": "3 The 4 Phases ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We now put together a coherent picture of the effect of different choices of $\\alpha$ (data complexity) and $\\beta$ (target complexity) and their impact on the compute-optimal frontier. By Theorem 2.1, we estimate ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{P}(r,d)\\asymp\\mathcal{F}_{p p}(r)+\\mathcal{F}_{a c}(r)+\\mathcal{F}_{0}(r)+\\frac{1}{\\gamma B}\\mathcal{K}_{p p}(r).}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Explicitly, we show that the functional form, or scaling law for the PLRF model is ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathcal{P}(r,d)\\asymp\\underset{\\mathcal{T}_{p_{P}}(r)}{\\sum}+\\underset{\\mathcal{T}_{0}(r)}{\\sum}+\\underset{\\mathcal{T}_{a\\varsigma}(r)}{\\sum}\\frac{1-\\tau_{2}}{\\mathcal{T}_{a\\varsigma}(r)}+\\underset{\\frac{1}{\\gamma_{B}}\\mathcal{K}_{p p}(r)}{\\sum},\\mathrm{~where~}\\sigma_{i},\\tau_{i}>0\\mathrm{~and~explicit,~see~Table~1.}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Fig. 5a. shows empirically that this equivalence of $\\mathcal{P}(r)$ is quite good. The first two terms $\\mathcal{F}_{p p}(r)$ (i.e., $r^{-\\sigma_{1}}$ ) and $\\bar{\\mathcal{F}_{0}(\\boldsymbol{r})}$ (i.e., $d^{-\\tau_{1}}$ ) are often called in the literature as time and model bottlenecks respectively. The functional form using only these two components, i.e., $\\mathcal{P}(r,d)\\asymp r^{-\\sigma_{1}}+d^{-\\tau_{1}}$ were used to find compute-optimal exponents in [23, 9] and in concurrent work [27]. Because the functional form considered in [9, 27] are missing the two other terms (cross-term $\\mathcal{F}_{a c}$ and SGD noise $\\mathcal{K}_{p p})$ , the compute-optimal curves in [9, 27] agree only in Phase Ia with our results. Importantly, we show that the cross-term, i.e., $\\mathcal{F}_{a c}(r)$ , and SGD noise, $\\mathcal{K}_{p p}(r)$ , can indeed affect the compute-optimal exponents. (The cross-term also appeared in concurrent work on ridge regression [17].) ", "page_idx": 7}, {"type": "text", "text": "The 4 distinct phases (see Fig. 2a) decompose the $(\\alpha,\\beta)$ -plane based on the shape of the loss curve $\\mathcal{P}(r)$ , that is, which of the distinct components of the forcing function (i.e., $\\mathcal{F}_{0},\\mathcal{F}_{p p},\\mathcal{F}_{a c},$ and/or kernel function (i.e., $\\mathcal{K}_{p p})$ dominate the loss curve at a given iteration $r$ . See Table 2 for loss description in each phase. Cartoon pictures of the different features of the loss curves are shown in Fig. 2. For each phase, we derive a compute-optimal curve in Section 3.1. ", "page_idx": 7}, {"type": "text", "text": "The high-dimensional line, which occurs where $2\\alpha\\,=\\,1$ , distinguishes the phases where the $v$ - dimension can be big and independent of $d$ (Phase Ia, II, III, $2\\alpha>1$ ) and the phases where $d$ and $v$ must be related to each other (Phase Ib, Ic, IVa, IVb, $2\\alpha<1$ ). When $2\\alpha+2\\beta<1$ , the loss does not exhibit any power-law decay as the limit level stops going to 0 as $d\\to\\infty$ (purely as a consequence of having selected the regime $v>d_{\\l,\\l}$ ). Moreover, there exists an interesting critical point $\\textstyle\\alpha={\\dot{\\beta}}={\\frac{1}{2}}$ where all the parts of the forcing function and kernel mix and interact with each other. The behavior of the loss at the pentuple point (see Fig 2a) we leave for future research. Across each of the phase boundaries the compute-optimal curves are continuous, but not necessarily differentiable; in contrast, $d^{\\star}$ is discontinuous across some phase boundaries. ", "page_idx": 7}, {"type": "text", "text": "3.1 Compute-optimal Curves ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To simplify the computations for compute-optimal curves, we introduce the following curve ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{\\mathcal{P}}(r)\\overset{\\mathrm{def}}{=}\\operatorname*{max}\\big\\{\\mathcal{F}_{p p}(r),\\mathcal{F}_{a c}(r),\\mathcal{F}_{0}(r),\\frac{1}{\\gamma B}\\mathcal{K}_{p p}(r)\\big\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The function $\\tilde{\\mathcal{P}}(r,d)$ achieves the same power law behavior as the original compute-optimal curve $\\mathcal{P}(r,d)$ (i.e., the slope of the compute-optimal curve is correct) and deviates from the true curve by an absolute constant (independent of $d$ and $\\mathsf{f}$ ). Note that some of the terms in the max function ", "page_idx": 7}, {"type": "table", "img_path": "aVSxwicpAk/tmp/6c7cf85125e5f4fd58e3efcc7c3cd4b8996eee48e229647c0c694cad3e3dc3fd.jpg", "table_caption": ["Table 2: Loss description for $\\mathcal{P}(r)$ and compute-optimal curves for $\\tilde{\\mathcal{P}}(\\frac{\\mathfrak{f}}{d\\cdot B},d)$ across the 4 phases. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "(19) should be taken to be 0 when not defined for the different phases. Therefore, we derive the compute-optimal curves by solving the problem ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{d}{\\mathrm{min}}\\ \\tilde{\\mathcal{P}}\\big(\\frac{\\mathfrak{f}}{d\\cdot B},d\\big),\\quad\\mathrm{and\\if\\}d^{\\star}(\\mathfrak{f})\\overset{\\mathrm{def}}{=}\\operatorname{arg\\,min}_{d}\\tilde{\\mathcal{P}}\\big(\\frac{\\mathfrak{f}}{d\\cdot B},d\\big),}\\\\ &{\\mathfrak{m}\\,\\mathrm{the~compute-optimal~curve~is}\\quad\\tilde{\\mathcal{P}}^{\\star}(\\mathfrak{f})\\overset{\\mathrm{def}}{=}\\tilde{\\mathcal{P}}\\big(\\frac{\\mathfrak{f}}{d^{\\star}(\\mathfrak{f})\\cdot B},d^{\\star}(\\mathfrak{f})\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "See Table 2 for the exact expressions for $d^{\\star}(\\mathfrak{f})$ and the compute-optimal curve $\\tilde{\\mathcal{P}}^{\\star}(\\mathfrak{f})$ for each phase.   \nA more detailed description with proofs can be found in Section C.4 and Section D. ", "page_idx": 8}, {"type": "text", "text": "Now to derive $d^{\\star}$ and ${\\tilde{\\mathcal{P}}}^{\\star}$ , we recall that the functions $\\mathcal{F}_{0},\\mathcal{F}_{p p},\\mathcal{F}_{a c},\\mathcal{K}_{p p}$ take the form $c\\times d^{-\\sigma_{i}}\\times$ $\\underline{{\\left(\\frac{\\mathfrak{f}}{d\\cdot B}\\right)}}^{-\\tau_{i}}$ (16). Therefore, $\\tilde{\\mathcal{P}}^{\\star}(\\mathfrak{f}/(d^{\\star}\\cdot B),d^{\\star})$ must occur at corner point where two functions meet. These tradeoffs between the two functions for which the compute-optimal point occurs are shown in Fig. 2 and Table 2. ", "page_idx": 8}, {"type": "text", "text": "Details for each phase. We describe the qualitative and quantitative properties of compute-optimal curves for each phase. These are broken down into model constrained (Phase I, II) vs. algorithm constrained (Phase III, IV), i.e., whether the PLRF model or SGD is the constraining feature. ", "page_idx": 8}, {"type": "text", "text": "Phase Ia, Ib, Ic. Capacity constrained. Phase Ia $(2\\alpha>1,2\\beta<1)$ , Ib $(2\\alpha<1,2\\beta<1,2(\\alpha+$ $\\beta)>1$ ), Ic are characterized by having the simplest loss description, $\\begin{array}{r}{\\dot{\\mathcal{P}}(\\boldsymbol{r})\\asymp\\mathcal{F}_{p p}(\\boldsymbol{r})+\\mathcal{F}_{0}(\\boldsymbol{r})}\\end{array}$ . Here the SGD noise is irrelevant and one would have the same loss (and thus compute-optimal curve) as gradient descent on the population loss. Compute optimality is characterized by training the model completely (to its limit loss) and choosing the model parameter count large enough so that at the end of training, the smallest loss is attained. The main distinctions between Phase Ia, Ib, Ic are the model capacities (i.e., $\\mathcal{F}_{0}(r,d)=d^{-2\\alpha+1-2\\beta}$ in Ia, Ib, and $\\mathcal{F}_{0}(r,d)=d^{-2\\alpha}$ in Ic) and the dependence of dimension in the learning rate due to Ib,Ic being below the high-dimensional line. Consequently, while the qualitative features of the loss curve are the same for Ia, Ib, and Ic, the actual values of the compute-optimal curve vary across the different regions. Notably, in Phase Ib, the compute-optimal parameter is $d^{\\star}=\\mathfrak{f}^{1/2}$ and it is independent of $\\alpha$ and $\\beta$ . ", "page_idx": 8}, {"type": "text", "text": "Phase II. Distortion constrained. Phase II $2\\alpha>1$ , $2\\beta>1$ , $\\beta<\\alpha)$ ) has a loss curve where the $\\mathcal{F}_{a c}$ is important, that is, $\\mathcal{P}(\\boldsymbol{r})\\asymp\\mathcal{F}_{p p}(\\boldsymbol{r})+\\mathcal{F}_{a c}(\\boldsymbol{r})+\\mathcal{F}_{0}(\\boldsymbol{r})$ . The $\\mathcal{F}_{a c}$ term becomes the dominant term after running for some intermediate amount of time $d^{c}$ ; in fact it is compute-optimal to stop at this point, and then select the number of model parameters so to minimize the loss with this early stopping criterion. It transpires that across all phases, it never pays to solve through the $\\mathcal{F}_{a c}$ part of the loss curve \u2013 it is always better to just increase the number of model parameters. ", "page_idx": 9}, {"type": "text", "text": "Phase III. SGD frustrated, distortion constrained. In this phase $(2\\alpha\\,>\\,1,2\\beta\\,>\\,1,\\beta\\,>\\,\\alpha)$ , SGD noise is important. The loss curve is $\\begin{array}{r}{\\mathcal{P}(r)\\asymp\\mathcal{F}_{a c}(r)+\\mathcal{F}_{0}^{\\bar{\\cdot}}(r)+\\frac{1}{\\gamma B}\\mathcal{K}_{p p}(r)}\\end{array}$ . Notably, in this phase, the compute-optimal parameter is $d^{\\star}(\\mathfrak{f})=\\mathfrak{f}^{1/2}$ , which is independent of $\\alpha$ and $\\beta$ . PLRF that fall within this phase have the same scaling law regardless of data complexity and target complexity. Moreover, the tradeoff occurs, like in Phase II, once the optimizer reaches the $\\mathcal{F}_{a c}$ -dominated part of the loss curve. Unlike in Phase II, the optimization is slowed by SGD noise $(\\mathcal{K}_{p p})$ leading up to that point. We note that there is a dimension-independent burn-in period required for SGD noise to dominate, and for small numerical simulations, one may actually observe an $(\\mathcal{F}_{p p},\\mathcal{F}_{a c})$ tradeoff. ", "page_idx": 9}, {"type": "text", "text": "Phase IV. SGD frustrated, capacity constrained. Like Phase III, SGD noise is important. The SGD algorithm in Phase IV will be distinguished from gradient descent. As one approaches the high-dimensional line $2\\alpha=1)$ ) in Phase III, the $\\mathcal{F}_{a c}(r)$ disappears. It becomes too small relative to ${\\mathcal F}_{p p}$ and $\\mathcal{K}_{p p}$ . Moreover at the high-dimensional line, ${\\mathcal F}_{p p}$ becomes important again. Thus, the loss curve in Phase IV (a and b) look like $\\begin{array}{r}{\\mathcal{P}(r,d)\\asymp\\mathcal{F}_{p p}(r,d)\\bar{+}\\,\\mathcal{F}_{0}(r,d)+\\frac{1}{\\gamma B}\\mathcal{K}_{p p}(r,d)}\\end{array}$ . The distinction between Phase I $\\mathsf{V a}\\left(1-\\frac{1}{\\sqrt{2}}<\\alpha<0.5,2\\beta>1\\right)$ and Phase $\\begin{array}{r}{\\left[\\mathrm{Vb}\\left(\\frac{1}{4}<\\alpha<1-\\frac{1}{\\sqrt{2}},2\\beta>1\\right)\\right.}\\end{array}$ is where the compute-optimal tradeoff occurs. It changes from $\\mathcal{K}_{p p}=\\mathcal{F}_{0}$ (Phase IVa) to $\\mathcal{F}_{p p}=\\mathcal{K}_{p p}$ (Phase IVb). In particular it can be (Phase IVb) the SGD noise is so large that increasing the model parameter count is compute-optimal. We note that in this phase $d$ must be taken very large (in particular larger than we could numerically attain) to get quantitative agreement between the exponents and theory. ", "page_idx": 9}, {"type": "text", "text": "Other observations. In Phase III, Ib, and $\\mathrm{IVa}$ , the optimal parameter $d^{\\star}=\\mathfrak{f}^{1/2}$ (see dashed lines in Fig. 4a). These phases, taken together, encompass a large section of the $(\\alpha,\\beta)$ -phase plane. This suggests that there is a potential universal scaling law. Moreover using 1 A100-GPU-day of compute, one reaches scales of $d$ where the observed exponents in the scaling laws \u2013 SGD, the theoretically-derived Volterra equation eq. (10), and the equivalence of $\\mathcal{P}(r)$ eq. (17) \u2013 are still changing (see Fig. 5b and c). This serves as a potential warning for empirically derived scaling laws. Additionally, although we have identified the lower-left of the phase diagram $(\\alpha+\\beta<1/2)$ as \"no power-law\", this designation relies on the assumption $v>d$ , which could be relaxed to interesting effect in more realistic (e.g. non-linear) models. ", "page_idx": 9}, {"type": "text", "text": "Compute-optimal learning rate and batch. Previously, we have used $B=1$ and the maximal learning rate allowed. One can also consider finding the compute-optimal curves with respect to batch size and learning rate, i.e., find $d^{\\star},\\gamma^{\\star},B^{\\star}$ such that ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(d^{\\star},\\gamma^{\\star},B^{\\star})\\in\\arg\\operatorname*{min}_{d,\\gamma,B}\\in\\arg\\operatorname*{min}\\!\\mathcal{P}(\\frac{\\dag}{d B},\\gamma,d)\\quad\\mathrm{s.t.~}\\gamma B<1\\mathrm{~and~}\\|\\mathcal{K}_{p p}\\|<1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "In Section I, we show that $\\textstyle\\mathcal{P}(\\frac{\\mathfrak{f}}{d B},\\gamma,d)$ is monotonic in $B$ and therefore $B=1$ is optimal. Similarly for $\\gamma$ , in Phases I, II, III, the loss $\\textstyle\\mathcal{P}(\\frac{\\mathfrak{f}}{d B},\\gamma,d)$ is monotonic and thus the maximally stable learning rate is optimal. For Phase IV, this is not true. There does exist an optimal $\\gamma^{\\star}$ (with $B=1$ ), ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\gamma^{\\star}\\asymp\\mathfrak{f}^{\\frac{4\\alpha(\\alpha-\\beta)}{4\\alpha\\beta+2\\alpha+2\\beta-1}},\\quad\\mathfrak{d}^{\\star}(\\mathfrak{f})\\asymp\\mathfrak{f}^{\\frac{2\\alpha+2\\beta-1}{4\\alpha\\beta+2\\alpha+2\\beta-1}},\\quad\\mathrm{and}\\quad\\mathcal{P}^{\\star}(\\mathfrak{f})\\asymp\\mathfrak{f}^{\\frac{-2\\alpha(2\\alpha+2\\beta-1)}{4\\alpha\\beta+2\\alpha+2\\beta-1}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "where the tradeoff occurs between $\\begin{array}{r}{\\frac{1}{\\gamma}\\mathcal{K}_{p p}(r)=\\mathcal{F}_{0}(r)}\\end{array}$ and Phase IVa and IVb collapse to a single phase. This is proven in Proposition I.1. ", "page_idx": 9}, {"type": "text", "text": "Conclusion. We analyze a simple three parameter model, PLRF, and derive deterministic expressions for the training dynamics (see Volterra equation (10)). We then extract compute-optimal scaling laws for large $d$ . We identify 4 phases ( $^{+3}$ subphases) in the $(\\alpha,\\beta)$ -phase plane, corresponding to different compute-optimal curve/loss behaviors. These phase boundaries are determined by the relative importance of model capacity (Phase I, IV), poor embedding of the features (Phase II, III), and the noise produced by the SGD algorithm (Phase III, IV). The latter suggesting that another stochastic algorithm might change the compute-optimal curve; we leave this interesting direction to future research. We also show evidence of a universal scaling law which we also leave for future research to explore. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "C. Paquette is a Canadian Institute for Advanced Research (CIFAR) AI chair, Quebec AI Institute (MILA) and a Sloan Research Fellow in Computer Science (2024). C. Paquette was supported by a Discovery Grant from the Natural Science and Engineering Research Council (NSERC) of Canada, NSERC CREATE grant Interdisciplinary Math and Artificial Intelligence Program (INTERMATH-AI), Google research grant, and Fonds de recherche du Qu\u00e9bec \u2013 Nature et technologies (FRQNT) New University Researcher\u2019s Start-Up Program. Research by E. Paquette was supported by a Discovery Grant from the Natural Science and Engineering Council (NSERC). Additional revenues related to this work: C. Paquette has $20\\%$ part-time employment at Google DeepMind. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Luca Arnaboldi, Ludovic Stephan, Florent Krzakala, and Bruno Loureiro. From highdimensional & mean-field dynamics to dimensionless odes: A unifying approach to sgd in two-layers networks. In The Thirty Sixth Annual Conference on Learning Theory (COLT), pages 1199\u20131227. PMLR, 2023.   \n[2] S\u00f8ren Asmussen, Soren Asmussen, and Sren Asmussen. Applied probability and queues, volume 2. Springer, 2003.   \n[3] Krishna B Athreya, Peter E Ney, and PE Ney. Branching processes. Courier Corporation, 2004.   \n[4] Francis Bach. High-dimensional analysis of double descent for linear regression with random projections. SIAM Journal on Mathematics of Data Science, 6(1):26\u201350, 2024.   \n[5] Yasaman Bahri, Ethan Dyer, Jared Kaplan, Jaehoon Lee, and Utkarsh Sharma. Explaining neural scaling laws. Proc. Natl. Acad. Sci. USA, 121(27):Paper No. e2311878121, 8, 2024. [6] Zhidong Bai and Jack W Silverstein. Spectral analysis of large dimensional random matrices, volume 20. Springer, 2010.   \n[7] Tamay Besiroglu, Ege Erdil, Matthew Barnett, and Josh You. Chinchilla Scaling: A replication attempt. arXiv preprint arXiv:2404.10102, 2024.   \n[8] Blake Bordelon, Alexander Atanasov, and Cengiz Pehlevan. A Dynamical Model of Neural Scaling Laws. In Proceedings of the 41st International Conference on Machine Learning (ICML), volume 235 of Proceedings of Machine Learning Research, pages 4345\u20134382. PMLR, 2024.   \n[9] Blake Bordelon and Cengiz Pehlevan. Learning Curves for SGD on Structured Features. In International Conference on Learning Representations (ICLR), 2022.   \n[10] Andrea Caponnetto and Ernesto De Vito. Optimal rates for the regularized least-squares algorithm. Foundations of Computational Mathematics, 7:331\u2013368, 2007.   \n[11] Luigi Carratino, Alessandro Rudi, and Lorenzo Rosasco. Learning with sgd and random features. Advances in Neural Information Processing Systems, 31, 2018.   \n[12] Chen Cheng and Andrea Montanari. Dimension free ridge regression. arXiv preprint arXiv:2210.08571, 2022.   \n[13] Elizabeth Collins-Woodfin, Courtney Paquette, Elliot Paquette, and Inbar Seroussi. Hitting the High-Dimensional Notes: An ODE for SGD learning dynamics on GLMs and multi-index models. arXiv preprint arXiv:2308.08977, 2023.   \n[14] Romain Couillet and Zhenyu Liao. Random Matrix Methods for Machine Learning. Cambridge University Press, 2022.   \n[15] D. Cruz-Uribe and C. J. Neugebauer. An Elementary Proof of Error Estimates for the Trapezoidal Rule. Math. Mag., 76(4):303\u2013306, 2003.   \n[16] St\u00e9phane d\u2019Ascoli, Marylou Gabri\u00e9, Levent Sagun, and Giulio Biroli. On the interplay between data structure and loss function in classification problems. Advances in Neural Information Processing Systems, 34:8506\u20138517, 2021.   \n[17] Leonardo Defliippis, Bruno Loureiro, and Theodor Misiakiewicz. Dimension-free deterministic equivalents for random feature regression. arXiv preprint arXiv:2405.15699, 2024.   \n[18] Ofer Dekel, Ran Gilad-Bachrach, Ohad Shamir, and Lin Xiao. Optimal Distributed Online Prediction Using Mini-Batches. Journal of Machine Learning Research, 13(1), 2012.   \n[19] Aymeric Dieuleveut and Francis Bach. Nonparametric stochastic approximation with large step-sizes. The Annals of Statistics, 44(4):1363 \u2013 1399, 2016.   \n[20] Cedric Gerbelot, Emanuele Troiani, Francesca Mignacco, Florent Krzakala, and Lenka Zdeborova. Rigorous dynamical mean-field theory for stochastic gradient descent methods. SIAM Journal on Mathematics of Data Science, 6(2):400\u2013427, 2024.   \n[21] Gustaf Gripenberg. On the resolvents of nonconvolution Volterra kernels. Funkcial. Ekvac., 23(1):83\u201395, 1980.   \n[22] Walid Hachem, Philippe Loubaton, and Jamal Najim. Deterministic equivalents for certain functionals of large random matrices. Ann. Appl. Probab., 17(3):875\u2013930, 2007.   \n[23] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Oriol Vinyals, Jack Rae, and Laurent Sifre. An empirical analysis of compute-optimal large language model training. In Advances in Neural Information Processing Systems, volume 35, 2022.   \n[24] Hong Hu, Yue M. Lu, and Theodor Misiakiewicz. Asymptotics of Random Feature Regression Beyond the Linear Scaling Regime. arXiv preprint arXiv:2403.08160, 2024.   \n[25] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alex Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.   \n[26] Kiwon Lee, Andrew Cheng, Elliot Paquette, and Courtney Paquette. Trajectory of mini-batch momentum: batch size saturation and convergence in high dimensions. Advances in Neural Information Processing Systems, 35:36944\u201336957, 2022.   \n[27] Licong Lin, Jingfeng Wu, Sham M Kakade, Peter L Bartlett, and Jason D Lee. Scaling Laws in Linear Regression: Compute, Parameters, and Data. arXiv preprint arXiv:2406.08466, 2024.   \n[28] Bruno Loureiro, Cedric Gerbelot, Hugo Cui, Sebastian Goldt, Florent Krzakala, Marc Mezard, and Lenka Zdeborov\u00e1. Learning curves of generic features maps for realistic datasets with a teacher-student model. Advances in Neural Information Processing Systems, 34:18137\u201318151, 2021.   \n[29] Alexander Maloney, Daniel A. Roberts, and James Sully. A Solvable Model of Neural Scaling Laws. arXiv preprint arXiv:2210.16859, 2024.   \n[30] Song Mei and Andrea Montanari. The generalization error of random features regression: Precise asymptotics and the double descent curve. Communications on Pure and Applied Mathematics, 75(4):667\u2013766, 2022.   \n[31] Gabriel Mel and Jeffrey Pennington. Anisotropic random feature regression in high dimensions. In International Conference on Learning Representations, 2021.   \n[32] Courtney Paquette, Kiwon Lee, Fabian Pedregosa, and Elliot Paquette. SGD in the Large: Average-case Analysis, Asymptotics, and Stepsize Criticality. In Proceedings of Thirty Fourth Conference on Learning Theory (COLT), volume 134, pages 3548\u20133626, 2021.   \n[33] Courtney Paquette and Elliot Paquette. Dynamics of stochastic momentum methods on largescale, quadratic models. Advances in Neural Information Processing Systems, 34:9229\u20139240, 2021.   \n[34] Courtney Paquette, Elliot Paquette, Ben Adlam, and Jeffrey Pennington. Homogenization of SGD in high-dimensions: exact dynamics and generalization properties. arXiv preprint arXiv:2205.07069, 2022.   \n[35] Loucas Pillaud-Vivien, Alessandro Rudi, and Francis Bach. Statistical optimality of stochastic gradient descent on hard learning problems through multiple passes. Advances in Neural Information Processing Systems, 31, 2018.   \n[36] Alessandro Rudi and Lorenzo Rosasco. Generalization properties of learning with random features. Advances in Neural Information Processing Systems, 30, 2017.   \n[37] Ohad Shamir and Tong Zhang. Stochastic gradient descent for non-smooth optimization: Convergence results and optimal averaging schemes. In International conference on machine learning, pages 71\u201379. PMLR, 2013.   \n[38] Utkarsha Sharma and Jared Kaplan. A neural scaling law from the dimension of the data manifold. arXiv preprint arXiv:2004.10802, 2020.   \n[39] Jack W Silverstein and Zhi Dong Bai. On the empirical distribution of eigenvalues of a class of large dimensional random matrices. Journal of Multivariate analysis, 54(2):175\u2013192, 1995.   \n[40] James B. Simon, Dhruva Karkada, Nikhil Ghosh, and Mikhail Belkin. More is better in modern machine learning: when infinite overparameterization is optimal and overfitting is obligatory. In International Conference on Learning Representations (ICLR), 2024.   \n[41] Aditya Vardhan Varre, Loucas Pillaud-Vivien, and Nicolas Flammarion. Last iterate convergence of SGD for Least-Squares in the Interpolation regime. In Advances in Neural Information Processing Systems (NeurIPS), volume 34, pages 21581\u201321591, 2021.   \n[42] Difan Zou, Jingfeng Wu, Vladimir Braverman, Quanquan Gu, and Sham M Kakade. Benign overfitting of constant-stepsize SGD for linear regression. Journal of Machine Learning Research, 24(326):1\u201358, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "$4{+3}$ Phases of Compute-Optimal Neural Scaling Laws ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Supplementary material ", "page_idx": 13}, {"type": "text", "text": "Broader Impact Statement. The work presented in this paper is foundational research and it is not tied to any particular application. The set-up is on a simple well-studied random features model with synthetic data and solved using a commonly deployed algorithm \u2013 stochastic gradient descent. We present (theoretical) compute-optimal curves for this model. The results are theoretical and we do not anticipate any direct ethical and societal issues. We believe the results will be used by machine learning practitioners and we encourage them to use it to build a more just, prosperous world. ", "page_idx": 13}, {"type": "text", "text": "Outline of the paper. The remainder of the article is structured as follows: in Section A, we provide additional related work. In Section B, we derive the convolution-type Volterra equation for the expected risk under SGD, (7). In Section C.1, we analyze the Volterra equation under the deterministic equivalent. A discussion on the convergence threshold for $\\mathcal{P}(r)$ including a necessary and sufficient condition for bounded solutions of (10) (Proposition C.2) and a proof of Proposition 2.1 are provided in Section C.2. Some background on Volterra equations and their solutions are provided in Section C.3.1 followed by the proof of Theorem 2.1 in Section C.3.2. We finish this section with a detailed description and proofs for the risk curves in all phases, Section C.4. Section D is devoted to deriving and proving the compute-optimal curves in Table 2. We follow this by Section E which analyzes the deterministic equivalent for the resolvent of $\\hat{K}$ . Here we examine the spectrum of $\\hat{K}$ from a random matrix point of view. In particular, in this section, we prove estimates on the fixed point equation, $m$ , see eq. (9). We then give explicit descriptions of the components of the forcing function, $\\mathcal{F}_{0},\\mathcal{F}_{p p},\\mathcal{F}_{a c}$ , as contour integrals and show that the error terms error $\\mathfrak{F}$ are small, see Section F. We do the same with the kernel function $\\mathcal{K}$ and kernel norm in Section G. In Section H, we derive the asymptotic formulas for the components of the forcing and kernel functions (see Table 1) used in the compute-optimal curve derivations. Finally, we end with some additional numerical experiments (and their experimental setups) as well as detailed descriptions of the different approaches to estimating the exponents in the scaling law and optimal model-parameter, Section J. ", "page_idx": 13}, {"type": "text", "text": "Contents ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Introduction 1.1 Problem Setup: SGD on Power-law Random Features 3 ", "page_idx": 13}, {"type": "text", "text": "2 Learning Dynamics of SGD 5 2.1 Forcing function and kernel function ", "page_idx": 13}, {"type": "text", "text": "3 The 4 Phases 8 ", "page_idx": 13}, {"type": "text", "text": "3.1 Compute-optimal Curves 8 ", "page_idx": 13}, {"type": "text", "text": "A Additional Related Work. 16 ", "page_idx": 13}, {"type": "text", "text": "B Derivation of Volterra equation 18 ", "page_idx": 13}, {"type": "text", "text": "C Analysis of Volterra Equation 21 ", "page_idx": 13}, {"type": "text", "text": "C.1 Deterministic equivalent of the loss under SGD 22   \nC.2 Convergence threshold 23   \nC.3 Simplification of the Volterra Equation 24   \nC.3.1 Background on Volterra equations 24   \nC.3.2 Proof of Theorem 2.1 . 26   \nC.4 Details of risk curves for the phases 27   \nC.4.1 Above the high-dimensional line (Phases Ia, II, III) 28   \nC.4.2 Below the high-dimensional line (Phases IVa, IVb, Ib, Ic) . 31 ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "D Compute-optimal curves 33 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Compute-optimal curves: Above the high-dimensional line (Phases Ia, II, III). . . 34   \nD.1.1 Phase Ia . . . 35   \nD.1.2 Phase II 35   \nD.1.3 Phase III 36   \nD.2 Compute-optimal curves: Below the high-dimensional line (Phase IV, Ib, Ic), $2\\alpha<1$ 37   \nD.2.1 Phase IV (a) and (b) . . 37   \nD.2.2 Phase Ib . . 39   \nD.2.3 Phase Ic 39 ", "page_idx": 14}, {"type": "text", "text": "E Spectrum of $\\hat{K}$ : random matrix theory 40 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "E.1 Self-consistent approximation for $(\\hat{K}-z)^{-1}=(D^{1/2}W W^{T}D^{1/2}-z)^{-1}$ . 40   \nE.2 The region near 0 and the spectral bulk . . . 44   \nE.3 The mesoscopic region . 47   \nE.4 The large $z$ region 53 ", "page_idx": 14}, {"type": "text", "text": "F Approximation of the forcing function 54 ", "page_idx": 14}, {"type": "text", "text": "G Estimation of Kernel function 58 ", "page_idx": 14}, {"type": "text", "text": "H Asymptotics of forcing function and kernel function 61 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "H.1 Pure point forcing term, $\\mathcal{F}_{p p}(r)$ . 61   \nH.2 Model misspecification, point mass at $0$ , $F_{0}(r)$ 63   \nH.3 Absolutely continuous forcing function, $\\mathcal{F}_{a c}(r)$ 64   \nH.4 Kernel function asymptotic. . . . 66 ", "page_idx": 14}, {"type": "text", "text": "I Optimizing over batch and learning rate 69 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "I.1 Optimal batch size 70   \nI.2 Optimal learning rate 70 ", "page_idx": 14}, {"type": "text", "text": "J Experimental Results 70 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "J.1 Measuring the Scaling Law Exponent 71   \nJ.2 Measuring Parameter Count Exponent: Approach 0 71   \nJ.3 Measuring Parameter Count Exponent: Approach 1 72   \nJ.4 Measuring Parameter Count Exponent: Approach 2 . . 72   \nJ.5 Exponents comparison: Theory vs Measurement . . 73   \nJ.6 Instantaneous slope 73   \nJ.7 Negative $\\beta$ . 74 ", "page_idx": 14}, {"type": "text", "text": "A Additional Related Work. ", "text_level": 1, "page_idx": 15}, {"type": "table", "img_path": "aVSxwicpAk/tmp/bee4335e602b29c991910e5a9fecc946a7a2b18c596fa2ed7c6bf6c775a515df.jpg", "table_caption": ["Table 3: Comparison of the source/capacity parameters across various related work. We note this table is taken from Table 1 in $\\mathrm{[DLM24]}^{1}$ with the addition of $[\\mathrm{Lin}{+}24]^{5}$ . We note that both [DLM24] and $[\\mathrm{Lin}+24]$ appeared concurrently with this article. "], "table_footnote": ["1 [DLM24] L. Defliippis, B. Loureiro, T. Misiakiewicz. Dimension-free deterministic equivalents for random feature regression. 2024 2 [Bahri21] Y. Bahri, D. Dyer, J. Kaplan, J. Lee, and U. Sharma. Explaining neural scaling laws. 2024. 3 [MRS22] A. Maloney, D.A. Roberts, J. Sully. A solvable model of neural scaling laws 4 [BAP24] B. Bordelon, A. Atanasov, C. Pehlevan. A dynamical model of neural scaling laws. 2024. 5 [Lin24] L. Lin, J. Wu, S. Kakade, P. Barlett, J.D. Lee. Scaling Laws in Linear Regression: Compute, Parameters, and Data. 2024 "], "page_idx": 15}, {"type": "text", "text": "Random features and random matrices. This paper uses random matrix theory to analyze a random features problem, which in statistical language would be the generalization error of the one-pass SGD estimator. Random matrix theory has played an increasingly large role in machine learning (see for [14] for a modern introduction). ", "page_idx": 15}, {"type": "text", "text": "The input we need for our random matrix analysis is for sample covariance matrices with power-law population covariance (i.e. linear random features). The analysis of sample covariance matrices precedes their usage in machine learning (see e.g. [6]), but to our knowledge, a detailed study of all parts of the spectrum of sample covariance matrices with power-law population covariances has not appeared before. The narrower study of ridge regression has been extensively investigated (see for e.g. [4, 12]), and the concurrent work [17] provides a complete analysis of the ridge regression problem when $2\\alpha>1$ . However, while (roughly speaking) ridge regression requires analysis of resolvent $(A-z\\mathrm{Id})^{-1}$ statistics for negative spectral parameter $z$ (which might be very close to 0), the analysis in this work requires resolvent statistics for essentially all $z$ . ", "page_idx": 15}, {"type": "text", "text": "There is a larger theory of nonlinear random features regression, mostly in the case of isotropic random features. Including nonlinearities in this model is a natural future direction; for isotropic random features with proportional dimension asymptotics this has been explored in works such as [30] and for some classes of anisotropic random features in [31, 16, 28] (we mention that lots of the complexity of the analysis of power-law random features arises from the analysis of the self-consistent equations \u2013 indeed the self-consistent equations we use date to [39], but the analysis of these equations may still be novel). This strongly motivates non-proportional scalings (which would be inevitable in power-law random features with nonlinearities); in the isotropic case, the state of the art is [24]. ", "page_idx": 15}, {"type": "text", "text": "Random features regression, \u2018source/capacity\u2019 conditions, and SGD. A large body of kernel regression and random features literature is formulated for \u201csource/capacity\u201d conditions, which are power-law type assumptions that contain the problem setup here, when $2\\alpha>1$ (the low-dimensional regime). For convenience, we record the parameters ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\alpha_{\\mathrm{source}}=2\\alpha~~~\\mathrm{and}~~~r=\\frac{2\\alpha+2\\beta-1}{4\\alpha}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "table", "img_path": "aVSxwicpAk/tmp/e74e9af30d03540abaecd997d7286388046fc4e2743bfcfe42e64f6e92000859.jpg", "table_caption": ["Table 4: (Nonexhaustive) Comparison of sample-complexity results. Let $\\rho\\ {\\stackrel{\\mathrm{def}}{=}}\\ 2\\alpha+2\\beta-1$ . We use our Phases with $n=$ sample size, $d=$ parameters. We will include derivation of these results in the appendix of our paper. $[\\mathrm{DLM}24]^{I}$ can also be done with $\\mathrm{RR+}$ optimal-ridge, which yields same in Phase Ia, but different in Phase II/III. $[\\mathrm{VPF}21]^{\\circ}$ obtain $\\mathcal{P}\\ll n^{-\\operatorname*{min}\\{1/(2\\alpha),(2\\alpha+2\\beta-1)/(2\\alpha)\\}}$ , that is, they capture the $\\mathcal{F}_{p p}$ , but not $\\mathcal{F}_{a c}$ . The minimax optimal rates never achieve any of the rates (always worse), which can be connected to overly conservative, small stepsizes. For derivation of the minimax rates, we used Cor. 2 from $[\\mathrm{DB}18]^{7}$ . $[\\mathrm{Lin}+24]^{5}$ requires label noise order 1 and also a very small learning rate. "], "table_footnote": ["6 Carratino, Rudi, Rosasco. Learning with sgd and random features. 2018 7 Dieuleveut and Bach. Nonparametric stochastic approximation with large stepsizes. 2016. 8 Pillaud-Vivien, Rudi, Bach. Statistical optimality of SGD on hard learning problems through multiple passes. 2018. 9 Varre, Pillaud-Vivien, Flammarion. Last iterate convergence of SGD for least squares in the interpolation regime 2021. "], "page_idx": 16}, {"type": "text", "text": "Here we have taken $r$ as the limit of those $r$ \u2019s for which the source/capacity conditions hold (see Table 3). We note that in this language $r$ is often interpreted as \u2019hardness\u2019 (lower is harder), and that $r\\in(0,0.5)$ , $r\\in(0.5,1.0)$ and $r\\in(1.0,\\infty)$ correspond to 3 regimes of difficulty which have appeared previously (see the citations below); they are also precisely the 3 phases Ia, II, and III. ", "page_idx": 16}, {"type": "text", "text": "The authors of [36] establish generalization bounds for random feature regression with power-law structures in $2\\alpha>1$ case. These bounds were sharpened and extended in [17] (see also the earlier [10] which shows kernel ridge regression is \u2018minimax optimal\u2019 under various \u2018source-capacity conditions\u2019); we give a comparison to these bounds in Table 4, but we note that the problem setup we have is not captured by \u2018minimax optimality\u2019 (in particular minimax optimality is worst-case behavior over a problem class, and our problem setup is not worst-case for the traditional source/capacity conditions) ", "page_idx": 16}, {"type": "text", "text": "We note that this paper is fundamentally about computation, but the novel mathematical contributions could also be recast in terms of generalization bounds of one-pass SGD, some of which are new. The work of [11] compares SGD to kernel ridge regression, showing that one-pass SGD can attain the same bounds as kernel ridge regression and hence is another minimax optimal method (again under \u2018source-capacity\u2019 conditions). See also [19] which considers similar statements for SGD with iterate averaging and [35] for similar statements for multipass SGD; see also [37, 18] which also prove the single-batch versions of these. These bounds attain the minimax-optimal rate, which are worse than the rates attained in this paper (see Table 4 for a comparison). ", "page_idx": 16}, {"type": "text", "text": "Dynamical deterministic equivalents, Volterra equations and ODEs. Using the deterministic equivalents for random matrix resolvents [22], we in turn derive deterministic equivalents for the risk curves of SGD. ", "page_idx": 16}, {"type": "text", "text": "The method of analysis of the risk curves in this paper is by formulation of a convolutional Volterra equation [32]. This can be equivalently formulated as a system of coupled difference equations for weights of the SGD residual in the observed data covariance, which generalizes beyond the least-squares context [13]; in isotropic instances, this simplifies to a finite-dimensional family of ODES [1]. This can also be generalized to momentum SGD methods [33] and large batch SGD methods [26]. Convolution-Volterra equations are convenient tools, as they are well-studied parts of renewal theory [2] and branching process theory [3]. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "Another method of analysis is dynamical mean field theory. The closest existing work to this one in scientific motivations is [8], which uses this technique. This formally can be considered as a type of Gaussian process approximation, but for a finite family of observables (\u201corder parameters\u201d). In instances of one-pass SGD (including in anisotropic cases), this is rigorously shown to hold in [20]. The analysis of the resulting self-consistent equations is nontrivial, and [8] does some of this analysis under simplifying assumptions on the structure of the solutions of these equations. ", "page_idx": 17}, {"type": "text", "text": "Besides these works, there is a large theory around generalization error of SGD. The work of [41] gives a direct analysis of risks of SGD under \u201csource/capacity\u201d type assumptions which formally capture the $F_{p p}$ parts of the Phase Ia/II loss curves. The risk bounds of [42] give non-asymptotic estimates which again reproduce tight estimates for the $F_{p p}$ parts of the loss (note that to apply these bounds to this case, substantial random matrix theory needs to be worked out first); see also concurrent work [27] where some of this is done. ", "page_idx": 17}, {"type": "text", "text": "B Derivation of Volterra equation ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We begin by deriving a Volterra equation for the population loss $\\mathcal{P}(\\theta)$ , (5). Fix a quadratic $q:$ $\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ , i.e., a function $q(x)=x^{\\bar{T}}A x+e^{T}x+c$ for fixed matrix $A\\in\\mathbb{R}^{d\\times d}$ , vector $e\\in\\mathbb{R}^{d}$ and constant $c\\in\\mathbb{R}$ . Let us consider the flitration $\\mathcal{F}_{r}=\\sigma(W,\\theta_{0},\\ldots,\\theta_{r})$ which conditions on $W$ and the past iterates. Then we have from Taylor theorem, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}[q(\\theta_{r+1})-q(\\theta_{r})\\,|\\,\\mathcal{F}_{r}]=\\mathbb{E}[\\langle\\nabla q(\\theta_{r}),\\theta_{r+1}-\\theta_{r}\\rangle\\,|\\,\\mathcal{F}_{r}]+\\frac{1}{2}\\mathbb{E}[\\langle\\nabla^{2}q,(\\theta_{r+1}-\\theta_{r})^{\\otimes2}\\rangle\\,|\\,\\mathcal{F}_{r}].\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We need to plug the expression for SGD in (6) into the above. The first thing we observe is that we need moments of Gaussians via Wick\u2019s formula: for fixed vectors $v_{i}\\in\\mathbb{R}^{v}$ , $i={1,2,3,4}$ , and $x\\sim N(0,D)$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\qquad\\qquad\\qquad\\mathbb{E}_{x}[x\\langle x,v_{1}\\rangle]=\\mathbb{E}_{x}[\\mathrm{Tr}(x^{T}x)]v_{1}=D v_{1}}\\\\ &{\\mathbb{E}_{x}[\\langle x,v_{1}\\rangle\\langle x,v_{2}\\rangle\\langle x,v_{3}\\rangle\\langle x,v_{4}\\rangle]=\\langle D,v_{1}\\otimes v_{2}\\rangle\\langle D,v_{3}\\otimes v_{4}\\rangle+\\langle D,v_{1}\\otimes v_{3}\\rangle\\langle D,v_{2}\\otimes v_{4}\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\,\\langle D,v_{1}\\otimes v_{4}\\rangle\\langle D,v_{2}\\otimes v_{3}\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Here we recall that the $(v\\times v)$ -matrix $D\\ {\\stackrel{\\mathrm{def}}{=}}\\ \\mathrm{Diag}(j^{-2\\alpha}\\ :\\ 1\\ \\leq\\ j\\ \\leq\\ v)$ . Using these moment calculations, we can compute explicitly each of the terms in (22). ", "page_idx": 17}, {"type": "text", "text": "Gradient term. First, we consider the gradient term in (22). A simple computation yields ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\langle\\nabla q(\\theta_{r}),\\theta_{r+1}-\\theta_{r}\\rangle\\,|\\,\\mathcal{F}_{r}]=-\\gamma\\langle\\nabla q(\\theta_{r}),\\mathbb{E}\\big[\\displaystyle\\sum_{j\\in B_{r}}W^{T}x^{j}\\big(\\langle W^{T}x^{j},\\theta_{r}\\rangle-\\langle x^{j},b\\rangle\\big)\\,|\\,\\mathcal{F}_{r}\\big]\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad}\\\\ &{\\qquad\\qquad\\qquad=-\\gamma B\\langle\\nabla q(\\theta_{r}),W^{T}D W\\theta_{r}-W^{T}D b\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Quadratic term. We now turn to the quadratic term in (22). Supposing $x$ and $\\hat{x}$ are independent samples, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{2}\\mathbb{E}[\\langle\\nabla^{2}q,(\\theta_{r+1}-\\theta_{r})^{\\otimes2}\\rangle\\,|\\,\\mathcal{F}_{r}]}\\\\ &{=\\;\\frac{\\gamma^{2}}{2}\\mathbb{E}\\left[\\langle\\nabla^{2}q,\\left(\\displaystyle\\sum_{j\\in B_{r}}W^{T}x^{j}[\\langle W^{T}x^{j},\\theta_{r}\\rangle-\\langle x^{j},b\\rangle]\\right)\\otimes\\left(\\displaystyle\\sum_{k\\in B_{r}}W^{T}x^{k}[\\langle W^{T}x^{k},\\theta_{r}\\rangle-\\langle x^{k},b\\rangle]\\right)\\rangle|\\mathcal{F}_{r}\\right]}\\\\ &{=\\;\\frac{\\gamma^{2}}{2}\\mathbb{E}\\left[\\displaystyle\\sum_{j\\in B_{r}}\\langle\\nabla^{2}q,\\left(\\displaystyle\\sum_{j\\in B_{r}}W^{T}x^{j}[\\langle W^{T}x^{j},\\theta_{r}\\rangle-\\langle x^{j},b\\rangle]\\right)^{\\otimes2}\\rangle|\\mathcal{F}_{r}\\right]}\\\\ &{+\\;\\frac{\\gamma^{2}}{2}\\mathbb{E}\\left[\\langle\\nabla^{2}q,2\\displaystyle\\sum_{j<k\\in B_{r}}\\left(W^{T}x^{j}[\\langle W^{T}x^{j},\\theta_{r}\\rangle-\\langle x^{j},b\\rangle]\\right)\\otimes\\left(W^{T}x^{k}[\\langle W^{T}x^{k},\\theta_{r}\\rangle-\\langle x^{k},b\\rangle]\\right)\\rangle|\\mathcal{F}_{r}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Continuing, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{2}\\mathbb{E}[\\langle\\nabla^{2}q,(\\theta_{r+1}-\\theta_{r})^{\\otimes2}\\rangle\\,|\\,\\mathcal{F}_{r}]}\\\\ &{=\\frac{\\gamma^{2}B}{2}\\mathbb{E}\\big[\\langle\\nabla^{2}q,(W^{T}x[\\langle W^{T}x,\\theta_{r}\\rangle-\\langle x,b\\rangle])^{\\otimes2}\\rangle\\,|\\mathcal{F}_{r}\\big]}\\\\ &{+\\,\\gamma^{2}B(B-1)\\mathbb{E}\\big[\\langle\\nabla^{2}q,(W^{T}x[\\langle W^{T}x,\\theta_{r}\\rangle-\\langle x,b\\rangle])\\otimes(W^{T}\\hat{x}[\\langle W^{T}\\hat{x},\\theta_{r}\\rangle-\\langle\\hat{x},b\\rangle])\\rangle\\,|\\mathcal{F}_{r}\\big]}\\\\ &{=\\frac{\\gamma^{2}B}{2}\\mathbb{E}\\big[(\\langle W^{T}x,\\theta_{r}\\rangle-\\langle x,b\\rangle)^{2}\\langle\\nabla^{2}q,(W^{T}x)^{\\otimes2}\\rangle\\,|\\,\\mathcal{F}_{r}\\big]}\\\\ &{+\\,\\gamma^{2}B(B-1)\\mathbb{E}\\big[\\langle\\nabla^{2}q,(W^{T}x[\\langle W^{T}x,\\theta_{r}\\rangle-\\langle x,b\\rangle])\\otimes(W^{T}\\hat{x}[\\langle W^{T}\\hat{x},\\theta_{r}\\rangle-\\langle\\hat{x},b\\rangle])\\rangle\\,|\\mathcal{F}_{r}\\big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Let $\\begin{array}{r}{\\nabla^{2}q=\\sum_{j=1}^{v}v_{j}\\otimes\\tilde{v}_{j}}\\end{array}$ . Now we note the following ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(\\langle W^{T}x,\\theta_{r}\\rangle-\\langle x,b\\rangle)^{2}\\langle\\nabla^{2}q,(W^{T}x)^{\\otimes2}\\rangle}\\\\ &{\\quad\\quad\\quad\\quad=\\big(x^{T}W\\nabla^{2}q W^{T}x\\big)[\\langle W^{T}x,\\theta_{r}\\rangle^{2}-2\\langle W^{T}x,\\theta_{r}\\rangle\\langle x,b\\rangle+\\langle x,b\\rangle^{2}]}\\\\ &{\\quad\\quad\\quad\\quad=\\displaystyle\\sum_{j}\\langle x,W v_{j}\\rangle\\langle x,W\\tilde{v}_{j}\\rangle[\\langle W^{T}x,\\theta_{k}\\rangle^{2}-2\\langle x,W\\theta_{r}\\rangle\\langle x,b\\rangle+\\langle x,b\\rangle^{2}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This, after taking expectations, is in the form for us to apply the moment computations in (23). Using these moments, we get the following expression: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\bigg[\\underset{j}{\\sum}(x,W v_{j})\\langle x,W\\tilde{v}_{j}\\rangle[\\langle W^{T}x,\\theta_{r}\\rangle^{2}-2\\langle x,W\\theta_{r}\\rangle\\langle x,b\\rangle+\\langle x,b\\rangle^{2}]\\ \\big|\\ \\mathcal{F}_{r}\\bigg]}\\\\ &{\\quad=\\langle\\nabla^{2}q,W^{T}D W\\rangle\\|D^{1/2}(W\\theta_{r}-b)\\|^{2}}\\\\ &{\\qquad\\quad+2\\underset{j}{\\sum}\\big[\\langle D,W v_{j}\\otimes W\\theta_{r}\\rangle\\langle D,W\\tilde{v}_{j}\\otimes W\\theta_{r}\\rangle-\\langle D,W v_{j}\\otimes W\\theta_{r}\\rangle\\langle D,W\\tilde{v}_{j}\\otimes b\\rangle}\\\\ &{\\qquad\\quad+\\langle D,W v_{j}\\otimes b\\rangle\\langle D,W\\tilde{v}_{j}\\otimes b\\rangle-\\langle D,W\\tilde{v}_{j}\\otimes W\\theta_{r}\\rangle\\langle D,W v_{j}\\otimes b\\rangle\\big]}\\\\ &{\\quad=\\langle\\nabla^{2}q,W^{T}D W\\rangle\\|D^{1/2}(W\\theta_{k}-b)\\|^{2}}\\\\ &{\\qquad\\quad+2\\underset{j}{\\sum}\\langle D,W v_{j}\\otimes(W\\theta_{r}-b)\\rangle\\langle D,W\\tilde{v}_{j}\\otimes(W\\theta_{r}-b)\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Now we simplify the 2nd term in the summand ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{j}\\langle D,W v_{j}\\otimes(W\\theta_{r}-b)\\rangle\\langle D,W\\tilde{v}_{j}\\otimes(W\\theta_{r}-b)\\rangle}\\\\ &{\\quad=\\displaystyle\\sum_{j}\\langle W^{T}D,v_{j}\\otimes(W\\theta_{r}-b)\\rangle\\langle W^{T}D,\\tilde{v}_{j}\\otimes(W\\theta_{r}-b)\\rangle}\\\\ &{\\quad=\\displaystyle\\sum_{j}\\sum_{i,n,\\ell,m}(W^{T}D)_{n i}v_{j n}(W\\theta_{r}-b)_{i}(W^{T}D)_{m\\ell}\\tilde{v}_{j m}(W\\theta_{r}-b)_{\\ell}}\\\\ &{\\quad=2\\langle D W(\\nabla^{2}q)W^{T}D,(W\\theta_{r}-b)^{\\otimes2}\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Moreover, as $x$ and $\\hat{x}$ are independent, we see that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\big[\\langle\\nabla^{2}q,(W^{T}x[\\langle W^{T}x,\\theta_{r}\\rangle-\\langle x,b\\rangle])\\otimes(W^{T}\\hat{x}[\\langle W^{T}\\hat{x},\\theta_{r}\\rangle-\\langle\\hat{x},b\\rangle])\\rangle\\,|\\mathcal{F}_{r}\\big]}\\\\ &{\\quad=\\mathbb{E}\\left[(W\\theta_{r}-b)^{T}x x^{T}W\\nabla^{2}q W^{T}\\hat{x}\\hat{x}^{T}(W\\theta_{r}-b)|\\mathcal{F}_{r}\\right]}\\\\ &{\\quad=(W\\theta_{r}-b)^{T}D W\\nabla^{2}q W^{T}D(W\\theta_{r}-b).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "As a result, we deduce by combining (27), (28), (29), and (30) with (26) gives the following representation for the expected quadratic term ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{1}{2}\\mathbb{E}[\\langle\\nabla^{2}q,(\\theta_{r+1}-\\theta_{r})^{\\otimes2}\\rangle\\,|\\,\\mathcal{F}_{k}]=\\frac{\\gamma^{2}B}{2}\\langle\\nabla^{2}q,W^{T}D W\\rangle\\|D^{1/2}(W\\theta_{r}-b)\\|^{2}\\qquad}\\\\ {+\\,\\gamma^{2}B^{2}\\langle D W(\\nabla^{2}q)W^{T}D,(W\\theta_{r}-b)^{\\otimes2}\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Volterra equation. Using the simplified gradient and quadratic terms, we can now state the expected change in any quadratic $q:\\dot{\\mathbb{R}}^{d}\\rightarrow\\dot{\\mathbb{R}}$ evaluated at an iterate of SGD (6): ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[q(\\theta_{r+1})-q(\\theta_{r})\\,|\\,\\mathcal{F}_{r}]=-\\,\\gamma B\\langle\\nabla q(\\theta_{r}),W^{T}D W\\theta_{r}-W^{T}D b\\rangle\\qquad\\qquad}\\\\ {+\\,\\frac{\\gamma^{2}B}{2}\\langle\\nabla^{2}q,W^{T}D W\\rangle\\|D^{1/2}(W\\theta_{r}-b)\\|^{2}}\\\\ {+\\,\\gamma^{2}B^{2}\\langle D W(\\nabla^{2}q)W^{T}D,(W\\theta_{r}-b)^{\\otimes2}\\rangle.\\qquad\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We can write $\\mathbb{R}^{v}=\\mathrm{Im}(W)\\oplus W^{\\perp}$ . Thus, there exists $\\check{b}\\in\\mathbb{R}^{d}$ and $\\boldsymbol{b}\\in\\mathbb{R}^{v}$ such that one can write $b=W\\check{b}+\\dot{b}$ , that is, something in the image of $W$ and something in the co-ker of $W$ , i.e., ", "page_idx": 19}, {"type": "equation", "text": "$$\nb=W{\\check{b}}+{\\dot{b}},\\quad{\\mathrm{where~}}W^{T}D{\\dot{b}}=0.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "One-step update formula. Using this observation, we have a formula for the expectation of the quadratic $q:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ , $\\begin{array}{r l}&{\\mathbb{E}[q(\\theta_{r+1})-q(\\theta_{r})\\,|\\,\\mathcal{F}_{r}]=-\\gamma B\\langle\\nabla q(\\theta_{r}),W^{T}D W(\\theta_{r}-\\check{b})\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\,\\frac{\\gamma^{2}B}{2}\\langle\\nabla^{2}q,W^{T}D W\\rangle\\|D^{1/2}(W\\theta_{r}-b)\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\,\\gamma^{2}B^{2}\\langle W^{T}D W(\\nabla^{2}q)W^{T}D W,(\\theta_{r}-\\check{b})^{\\otimes2}\\rangle.}\\end{array}$ (34) ", "page_idx": 19}, {"type": "text", "text": "We observe that all the terms on the right hand side of the above (34) involve the matrix $W^{T}D W\\in$ $\\mathbb{R}^{d\\times d}$ . Consequently, let $(\\lambda_{j},w_{j})$ for $j=1,\\dots,d$ be the eigenvalue-eigenvector of $W^{T}D W$ with $\\|w_{j}\\|=1$ . Now define ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\rho_{j}^{2}(r)\\stackrel{\\mathrm{def}}{=}\\langle w_{j}^{\\otimes2},(\\theta_{r}-\\check{b})^{\\otimes2}\\rangle,\\quad\\mathrm{for\\,all}\\;j=1,\\dots,d.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We will write our Volterra equation in terms of $\\rho_{j}$ \u2019s. Note we can express the loss $\\mathcal{P}(\\theta_{r})\\;=$ $\\|D^{1/2}(W\\theta_{r}-b)\\|^{2}$ by ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{P}(\\theta_{r})=\\|D^{1/2}(W\\theta_{r}-b)\\|^{2}=\\sum_{j=1}^{d}\\lambda_{j}^{2}\\rho_{j}^{2}(r)+\\|D^{1/2}\\dot{b}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We can now plug $\\rho_{j}^{2}$ into (34). For this, we need to compute $\\nabla\\rho_{j}^{2}$ and $\\nabla^{2}\\rho_{j}^{2}$ : ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\rho_{j}^{2}(r)=\\langle w_{j}^{\\otimes2},\\theta_{r}-\\check{b}^{\\otimes2}\\rangle,\\quad\\nabla_{\\theta}\\rho_{j}^{2}(r)=2w_{j}\\langle w_{j},\\theta_{r}-\\check{b}\\rangle,\\quad\\mathrm{and}\\quad\\nabla^{2}\\rho_{j}^{2}(r)=2w_{j}\\otimes w_{j}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then we have that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{d}\\rho_{j}^{2}(r)=-2\\gamma B\\langle w_{j},\\theta_{r}-\\check{b}\\rangle\\langle w_{j},W^{T}D W(\\theta_{r}-\\check{b})\\rangle}\\\\ &{\\quad\\quad\\quad\\quad+B\\gamma^{2}\\langle w_{j}\\otimes w_{j},W^{T}D W\\rangle\\|D^{1/2}(W\\theta_{r}-b)\\|^{2}}\\\\ &{\\quad\\quad\\quad\\quad+2B^{2}\\gamma^{2}\\langle W^{T}D W(w_{j}\\otimes w_{j})W^{T}D W,(\\theta_{r}-\\check{b})^{\\otimes2}\\rangle}\\\\ &{\\quad\\quad\\quad=-2\\gamma B\\lambda_{j}\\rho_{j}^{2}(r)+\\gamma^{2}B\\lambda_{j}\\|D^{1/2}(W\\theta_{r}-b)\\|^{2}+2\\gamma^{2}B^{2}\\lambda_{j}^{2}\\rho_{j}^{2}(r)}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Using an integrating factor, we can implicitly solve this expression ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{d}\\rho_{j}^{2}(k)=\\big[-2\\gamma B\\lambda_{j}+2\\gamma^{2}B^{2}\\lambda_{j}^{2}\\big]\\rho_{j}^{2}+\\gamma^{2}B\\lambda_{j}\\|D^{1/2}(W\\theta_{k}-b)\\|^{2},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and thus, we have a discrete Volterra equation ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\rho_{j}^{2}(r)=\\rho_{j}^{2}(0)(1-2\\gamma B\\lambda_{j}+2\\gamma^{2}B^{2}\\lambda_{j}^{2})^{r}}\\\\ &{\\qquad\\qquad+\\,\\gamma^{2}B\\displaystyle\\sum_{s=0}^{r-1}(1-2\\gamma B\\lambda_{j}+2\\gamma^{2}B^{2}\\lambda_{j}^{2})^{r-1-s}\\lambda_{j}\\|D^{1/2}(W\\theta_{s}-b)\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Let us define $\\check{K}$ d=ef $W^{T}D W$ . Using the expression in (36), ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\mathcal{P}(\\theta_{r})\\left|W\\right|=\\displaystyle\\sum_{j=1}^{d}\\lambda_{j}\\rho_{j}^{2}(0)(1-2\\gamma B\\lambda_{j}+2\\gamma^{2}B^{2}\\lambda_{j}^{2})^{r}+\\|D^{1/2}\\dot{b}\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad+\\displaystyle\\sum_{j=1}^{d}\\gamma^{2}B\\lambda_{j}^{2}\\displaystyle\\sum_{s=0}^{r-1}(1-2\\gamma B\\lambda_{j}+2\\gamma^{2}B^{2}\\lambda_{j}^{2})^{r-1-s}\\cdot\\mathbb{E}[\\mathcal{P}(\\theta_{s})\\left|\\,W\\right|].}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Let us define the kernel ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{H}(r)\\stackrel{\\mathrm{def}}{=}\\gamma^{2}B\\sum_{j=1}^{d}\\lambda_{j}^{2}(1-2\\gamma B\\lambda_{j}+2\\gamma^{2}B^{2}\\lambda_{j}^{2})^{r}=\\gamma^{2}B\\cdot\\mathrm{Tr}\\big(\\breve{K}^{2}(I-2\\gamma B\\breve{K}+2\\gamma^{2}B^{2}\\breve{K}^{2})^{r}\\big).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Discrete volterra equation for the loss for $\\check{K}=W^{T}D W$ . Let $r$ be the number of iterates of SGD. Then ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\mathcal{P}(\\theta_{r})\\,|\\,W]=\\langle\\tilde{K}(I-2\\gamma B\\tilde{K}+2\\gamma^{2}B^{2}\\tilde{K}^{2})^{r},(\\theta_{0}-\\tilde{b})^{\\otimes2}\\rangle+\\|D^{1/2}\\dot{b}\\|^{2}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad+\\displaystyle\\sum_{s=0}^{r-1}\\mathcal{H}(r-1-s)\\cdot\\mathbb{E}[\\mathcal{P}(\\theta_{s})\\,|\\,W],}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad w\\mathrm{here\\\\mathcal{X}}(s)=\\gamma^{2}B\\displaystyle\\sum_{j=1}^{d}\\lambda_{j}^{2}(1-2\\gamma B\\lambda_{j}+2\\gamma^{2}B^{2}\\lambda_{j}^{2})^{s}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad=\\gamma^{2}B\\times\\mathrm{Tr}\\big(\\check{K}^{2}(I-2\\gamma B\\tilde{K}+2\\gamma^{2}B^{2}\\check{K}^{2})^{s}\\big)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We can also write (40) in terms of $\\hat{K}\\ {\\stackrel{\\mathrm{def}}{=}}\\ D^{1/2}W W^{T}D^{1/2}$ . To see this, set $D^{1/2}W=V\\sqrt{\\Omega}U^{T}$ where $\\check{K}=U\\Omega U^{T}$ and $\\hat{K}=V\\Omega V^{T}$ . Then we see that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\mathrm{poly}(\\check{K})\\check{K},(\\theta_{0}-\\check{b})^{\\otimes2}\\rangle=\\langle\\mathrm{poly}(\\Omega)\\Omega,(U(\\theta_{0}-\\check{b})^{\\otimes2}\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\langle V\\mathrm{poly}(\\Omega)V^{T},(V\\sqrt{\\Omega}U(\\theta_{0}-\\check{b}))^{\\otimes2}\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\langle\\mathrm{poly}(\\hat{K}),(D^{1/2}W(\\theta_{0}-\\check{b}))^{\\otimes2}\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Discrete volterra equation for the loss with $\\hat{K}=D^{1/2}W W^{T}D^{1/2}$ . Let $r$ be the number of iterates of SGD. Then ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\mathcal{P}(\\theta_{r})\\,|\\,W]=\\langle(I-2\\gamma B\\hat{K}+2\\gamma^{2}B^{2}\\hat{K}^{2})^{r},(D^{1/2}(W\\theta_{0}-b))^{\\otimes2}\\rangle+\\|D^{1/2}\\hat{b}\\|^{2}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad+\\displaystyle\\sum_{s=0}^{r-1}\\mathcal{M}(r-1-s)\\cdot\\mathbb{E}[\\mathcal{P}(\\theta_{s})\\,|\\,W],}\\\\ &{\\mathrm{where~}\\mathcal{K}(s)=\\gamma^{2}B\\displaystyle\\sum_{j=1}^{d}\\lambda_{j}^{2}(1-2\\gamma B\\lambda_{j}+2\\gamma^{2}B^{2}\\lambda_{j}^{2})^{s}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad=\\gamma^{2}B\\times\\mathbf{T}(\\hat{K}^{2}(I-2\\gamma B\\hat{K}+2\\gamma^{2}B^{2}\\hat{K}^{2})^{s})}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\mathrm{and~}\\quad D=\\mathbf{D}\\mathbf{i}\\mathbf{a}\\mathbf{g}(j^{-2\\alpha}:\\,1\\leq j\\leq v).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "C Analysis of Volterra Equation ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "From now on, we consider the setting where the initialization of SGD is $\\theta_{0}=0$ . Let us introduce the forcing function: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{F}(r)\\overset{\\mathrm{def}}{=}\\langle(I-2\\gamma B\\hat{K}+2\\gamma^{2}B^{2}\\hat{K}^{2})^{r},(D^{1/2}(W\\theta_{0}-b))^{\\otimes2}\\rangle+\\Vert D^{1/2}\\dot{b}\\Vert^{2}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and recall the kernel function $\\mathcal{K}(s)$ : ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{M}(s)=\\gamma^{2}B\\cdot\\mathrm{Tr}\\big(\\hat{K}^{2}(I-2\\gamma B\\hat{K}+2\\gamma^{2}B^{2}\\hat{K}^{2})^{s}\\big).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "While these representations are easy to see from the derivation of the Volterra equation, a more useful representation of the forcing function and the kernel function is through contour integrals over the spectrum of $\\hat{K}$ . With this in mind, let $\\Gamma$ be a contour containing $[0,1]$ . Note that by the assumptions ", "page_idx": 20}, {"type": "text", "text": "on $\\hat{K}$ , the largest eigenvalue is normalized to be 1; hence $\\Gamma$ contains the spectrum of $\\hat{K}$ . Then the forcing function takes the form ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{F}(r)=\\frac{-1}{2\\pi i}\\oint_{\\Gamma}\\langle(\\hat{K}-z)^{-1},(D^{1/2}b)^{\\otimes2}\\rangle(1-2\\gamma B z+2\\gamma^{2}B^{2}z^{2})^{r}\\;\\mathrm{d}z.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and the kernel function ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{N}(r)=\\gamma^{2}B\\cdot\\mathrm{Tr}\\bigg(\\frac{-1}{2\\pi i}\\oint_{\\Gamma}z^{2}\\big((1-2\\gamma B z+2\\gamma^{2}B^{2}z^{2})^{r}\\big)(\\hat{K}-z)^{-1}\\ \\mathrm{d}z\\bigg).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then one can write the Volterra equation (41) as the forcing function plus a convolution with the kernel and the expected loss, i.e., ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ \\ \\ \\ \\ \\mathbb{E}[\\mathcal{P}(\\theta_{r})\\,|\\,W]=\\mathcal{F}(r)+\\left(\\mathcal{M}\\ast\\mathbb{E}[\\mathcal{P}(\\theta_{s})\\,|\\,W]\\right),}\\\\ &{\\mathrm{\\quad~where~}(\\mathcal{M}\\ast\\mathbb{E}[\\mathcal{P}(\\theta_{s})\\,|\\,W])(r)=\\displaystyle\\sum_{s=0}^{r-1}\\mathcal{M}(r-1-s)\\mathbb{E}[\\mathcal{P}(\\theta_{s})\\,|\\,W].}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "C.1 Deterministic equivalent of the loss under SGD ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The forcing functions ${\\mathcal{F}}(r)$ and kernel function $\\mathcal{H}(r)$ are random functions as they depend on the random matrix $W$ . Moreover the expressions via contour integration show that both of these functions can be described in terms of the random matrix $\\hat{K}=D^{1/2}W W^{T}D^{1/2}$ . Indeed it is the resolvent of $\\hat{K}$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{R}(\\hat{K},z)\\stackrel{\\mathrm{def}}{=}(\\hat{K}-z)^{-1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "which plays a significant role in $\\mathcal{F}$ and $\\mathcal{M}$ and thus in the expected loss $\\mathbb{E}[{\\mathcal{P}}(\\theta_{r})\\mid W]$ . To analyze the power law behavior of the expected loss, it would be helpful to remove the randomness in $\\hat{K}$ , i.e., $W$ . We do this by finding a deterministic equivalent for the resolvent of $\\hat{K},\\mathcal{R}(\\hat{K},z)=(\\hat{K}-z)^{-1}$ , using techniques from random matrix theory. Intuitively, we want to take the expectation over the random matrix $W$ ; though not formally true. ", "page_idx": 21}, {"type": "text", "text": "Formally, we define the deterministic equivalent for the resolvent $\\mathcal{R}(\\hat{K},z)$ , denoted by $\\mathcal{R}(z)$ implicitly via a fixed point equation ", "page_idx": 21}, {"type": "equation", "text": "$$\nm(z)\\stackrel{\\mathrm{def}}{=}\\frac{1}{1+\\frac{1}{d}\\sum_{j=1}^{v}\\frac{j^{-2\\alpha}}{j^{-2\\alpha}m(z)-z}}\\quad\\mathrm{where}\\quad\\Re(z)=\\mathrm{Diag}\\Bigg(\\frac{1}{j^{-2\\alpha}m(z)-z}\\,:\\,1\\leq j\\leq v\\Bigg).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "As mentioned earlier, this deterministic equivalent, $\\mathcal{R}(z)$ can be viewed, roughly as, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}_{W}[(\\hat{K}-z)^{-1}]=\\mathbb{E}_{W}[\\mathcal{R}(\\hat{K},z)]\\approx\\mathcal{R}(z);\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "though it is not formally the expectation over $W$ . ", "page_idx": 21}, {"type": "text", "text": "Using this deterministic expression for the resolvent of $\\hat{K}$ , we defined deterministic expressions for the forcing function via the contour representation of ${\\mathcal{F}}(r)$ in (43) ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left(\\begin{array}{l}{\\mathrm{~forcing~function}}\\\\ {\\mathrm{~deterministic~equivalent}}\\end{array}\\right)\\,\\mathscr{F}(r)\\stackrel{\\mathrm{def}}{=}\\frac{-1}{2\\pi i}\\oint_{\\Gamma}\\langle\\Re(z),(D^{1/2}b)^{\\otimes2}\\rangle(1-2\\gamma B z+2\\gamma^{2}B^{2}z^{2})^{r}\\;\\mathrm{d}z,}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and the kernel function in (44) ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left(\\begin{array}{l}{\\mathrm{kernel~function}}\\\\ {\\mathrm{deterministic~equivalent}}\\end{array}\\right)\\,\\mathfrak{K}(r)\\stackrel{\\mathrm{def}}{=}\\gamma^{2}B\\cdot\\mathrm{Tr}\\bigg(\\frac{-1}{2\\pi i}\\oint_{\\Gamma}z^{2}(1-2\\gamma B z+2\\gamma^{2}B^{2}z^{2})^{r}\\mathfrak{K}(z)\\,\\,\\mathrm{d}z\\bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Using the deterministic expressions for the forcing function $\\mathcal{F}$ and kernel function $\\mathcal{K}$ , we define the deterministic function $\\mathcal{P}(\\bar{r})\\;:\\;\\mathbb{N}\\rightarrow\\mathbb{R}$ as the solution to the (discrete) convolution-type Volterra equation: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname{\\mathcal{P}}(r)=\\mathcal{F}(r)+(\\mathcal{K}*\\mathcal{P})(r),\\quad\\mathrm{where}~(\\mathcal{K}*\\mathcal{P})(r)=\\sum_{s=0}^{r-1}\\mathcal{K}(r-1-s)\\mathcal{P}(s).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We note the similarity with the Volterra equation for SGD. We conjecture that the two processes are close: for $\\{\\theta_{r}\\}$ the sequence of iterates generated by SGD with $\\theta_{0}=0$ and any $\\varepsilon>0$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n(1-\\varepsilon)\\leq\\operatorname*{sup}_{r\\in\\mathbb{N}}\\left\\{\\frac{\\mathbb{E}[\\mathcal{P}(\\theta_{r})|W]}{\\mathcal{P}(r)}\\right\\}\\leq(1+\\varepsilon),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "for all admissible $v,d$ with probability going to 1 as $d\\to\\infty$ . ", "page_idx": 22}, {"type": "text", "text": "We leave this for future research; we suspect it is true based upon existing of deterministic equivalence theory for random matrices and numerical evidence. ", "page_idx": 22}, {"type": "text", "text": "C.2 Convergence threshold ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "A natural question is: for what choices of batch $B$ and learning rate $\\gamma$ does $\\Phi$ converge? To answer this, we introduce an additional quantity, the kernel norm defined as ", "page_idx": 22}, {"type": "equation", "text": "$$\n(\\mathrm{kernel\\;norm})\\quad\\|\\mathcal{K}\\|\\overset{\\mathrm{def}}{=}\\sum_{s=0}^{\\infty}\\mathcal{K}(s).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proposition C.1 (Kernel norm). The kernel norm is satisfies ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\lVert\\mathcal{K}\\rVert\\sim\\frac{\\gamma}{2}\\sum_{j=1}^{v}\\frac{j^{-2\\alpha}}{1-\\gamma j^{-2\\alpha}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "If $2\\alpha>1$ , then v be taken to equal $\\infty$ , that is, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\|\\mathcal{K}\\|\\sim\\frac{\\gamma}{2}\\sum_{j=1}^{\\infty}\\frac{j^{-2\\alpha}}{1-\\gamma j^{-2\\alpha}}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "In the case that $2\\alpha<1$ , we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\|\\mathcal{K}\\|\\sim\\frac{\\gamma}{2}\\sum_{j=1}^{v}j^{-2\\alpha}\\sim\\frac{\\gamma}{2(1-2\\alpha)}v^{1-2\\alpha}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "In all cases, we choose $\\gamma$ so that the kernel norm is asymptotic to a strictly positive constant. ", "page_idx": 22}, {"type": "text", "text": "A well-known result about convolution-type Volterra such as (49) is that the solution of convolutiontype Volterra equation is bounded if and only the forcing function ${\\mathcal{F}}(r)$ is bounded and the kernel norm $\\|\\mathcal{K}\\|\\,<\\,1$ . This naturally leads to conditions for our specific forcing function and kernel function. ", "page_idx": 22}, {"type": "text", "text": "Remark C.1 (Convergence threshold conditions.). The forcing function $\\mathcal{F}$ is bounded and the kernel norm $\\|\\mathcal{K}\\|<1$ for (47) and (48), respectively, if and only if ", "page_idx": 22}, {"type": "text", "text": "The first term ensures that the forcing function of the Volterra equation in (49) goes to 0 (i.e., bounded) and the second condition is the same kernel norm bound. Moreover, we can think of condition $(i)$ . as the same condition needed for gradient descent to converge while the kernel norm is the effect of noise from SGD. ", "page_idx": 22}, {"type": "text", "text": "We also note that in light of Proposition C.1 the kernel norm does not involve the batch size $B$ .   \nTherefore the condition $\\|\\mathcal{K}\\|<1$ only places a condition on the learning rate (see below). ", "page_idx": 22}, {"type": "text", "text": "We now state necessary/sufficient conditions on the batch size and learning rate (51) (Proof of Prop. 2.1). ", "page_idx": 22}, {"type": "text", "text": "Proposition C.2 (Necessary/Sufficient conditions on learning rate and batch size). The learning rate, $\\gamma>0$ and batch size, $B>0$ , satisfy ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\|{\\mathcal{K}}\\|<1,\\quad\\gamma B<1.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "if and only if the solution $\\mathcal{P}(r)$ to the convolution-type Volterra equation (10) is bounded. ", "page_idx": 22}, {"type": "text", "text": "Proof. From (51), we need that $|1\\,-\\,2\\gamma B\\lambda_{j}\\,+\\,2\\gamma^{2}B^{2}\\lambda_{j}^{2}|\\,<\\,1$ , for all $\\lambda_{j}\\in[0,1]$ . For this, we consider two cases. ", "page_idx": 23}, {"type": "text", "text": "First, suppose that $1-2\\gamma B x+2\\gamma^{2}B^{2}x^{2}<1$ for all $x\\in[0,1]$ . We have that ", "page_idx": 23}, {"type": "equation", "text": "$$\n-2\\gamma B x+2\\gamma^{2}B^{2}x^{2}<0\\ \\Rightarrow\\ x(-2\\gamma B+2\\gamma^{2}B^{2}x)<0.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The roots are precisely $x=0$ and $\\begin{array}{r}{x=\\frac{1}{\\gamma B}}\\end{array}$ . If $1/(\\gamma B)>1$ , then the inequality in (53) always holds.   \nTherefore, we need that $\\gamma B<1$ . ", "page_idx": 23}, {"type": "text", "text": "Now suppose $-1+2\\gamma B x-2\\gamma^{2}B^{2}x^{2}<1$ . Then we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n-\\,2+2\\gamma B x-2\\gamma^{2}B^{2}x^{2}<0,\\quad\\mathrm{for}\\,\\mathrm{all}\\,x\\in[0,1].\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The roots of the left-hand side are complex and thus the inequality always holds. ", "page_idx": 23}, {"type": "text", "text": "Remark C.2. Below the high-dimensional line, $2\\alpha<1$ , the kernel norm diverges with $v$ for fixed constant $\\gamma,$ , and so we must take $\\gamma\\to0$ to ensure bounded solutions. Furthermore, with $\\gamma\\to0$ (at any rate depending on $v$ ) we have the asymptotic equivalence ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\|\\mathcal{K}\\|\\sim\\frac{\\gamma}{2}\\sum_{j=1}^{v}j^{-2\\alpha}\\sim\\frac{\\gamma}{2(1-2\\alpha)}v^{1-2\\alpha}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "For a proof of the asymptotic for $\\lVert\\mathcal{K}\\rVert$ , see Corollary G.1. ", "page_idx": 23}, {"type": "text", "text": "Remark C.3. Similar results hold for the expected SGD loss (via the Volterra equation (45)) by replacing $\\lVert\\mathcal{K}\\rVert$ with $\\lVert\\mathcal{H}\\rVert$ . ", "page_idx": 23}, {"type": "text", "text": "C.3 Simplification of the Volterra Equation ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "While convolution-type Volterra equation such as (49) are quite nice and well studied in the literature (e.g., [21]), we need an approximation of the solution to it to have better understanding of computeoptimal curves. In this section, we show that we can bound (above and below) $\\mathcal{P}(r)$ by a constant multiple of the forcing function $\\mathcal{F}$ and kernel function $\\mathcal{K}$ . ", "page_idx": 23}, {"type": "text", "text": "C.3.1 Background on Volterra equations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "To do this, we need some background on general convolution-type Volterra equations of the form: ", "page_idx": 23}, {"type": "equation", "text": "$$\nP(t)=f(t)+(K*P)(t),\\quad\\mathrm{where}\\;(K*P)(t)=\\sum_{s=0}^{t}K(s)P(t-s).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $f(t)$ is a non-negative forcing function and $K(t)$ is a monotonically decreasing non-negative kernel function. ", "page_idx": 23}, {"type": "text", "text": "Let us define $K^{*n}\\ {\\stackrel{\\mathrm{def}}{=}}\\ (\\underbrace{K*K*\\ldots*K*K}_{n\\ {\\mathrm{tmes}}})(t)$ , the $n$ -fold convolution of $K$ where $K^{*1}=K(t)$ ", "page_idx": 23}, {"type": "text", "text": "Under mild assumptions such as $\\begin{array}{r}{\\|K\\|=\\sum_{t=0}^{\\infty}K(t)<1}\\end{array}$ and the forcing function $f$ is bounded, then there exists a unique (bounded) solu tion ${\\dot{P}}(t)$ to (55) and the solution is given by repeatedly convolving the forcing function with $K$ (see, e.g., [21, Theorem 3.5]), ", "page_idx": 23}, {"type": "equation", "text": "$$\n{\\begin{array}{l}{P(t)=f(t)+\\displaystyle\\sum_{j=1}^{\\infty}K^{*j}*f(t)}\\\\ {\\qquad=f(t)+(K*f)(t)+(K*K*f)(t)+(K*K*K*f)(t)+\\ldots.}\\end{array}}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "This representation of the solution to (55) enables us to get good bounds on $P(t)$ . First, we state and prove a lemma attributed to Kesten\u2019s Lemma [3, Lemma IV.4.7]. ", "page_idx": 23}, {"type": "text", "text": "Lemma C.1 (Kesten\u2019s Lemma). Suppose the kernel function $K$ is positive and monotonically decreasing and $\\|K\\|<\\infty$ . Moreover suppose for some $\\varepsilon>0$ , there exists a $T(\\varepsilon)>0$ such that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sum_{s=0}^{t}K(s)K(t-s)\\leq2(1+\\varepsilon)\\|K\\|K(t)\\quad{\\mathit{f o r}}\\,a l l\\,t\\geq T.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Then for all $n\\geq0$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{t}\\left\\{\\frac{K^{*(n+1)}(t)}{K(t)}\\right\\}\\leq\\left(\\frac{K(0)}{K(T)}+1\\right)\\big(2\\|K\\|(1+\\varepsilon)\\big)^{n}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. Define ", "page_idx": 24}, {"type": "equation", "text": "$$\na_{n}\\ {\\stackrel{\\mathrm{def}}{=}}\\ \\operatorname*{sup}_{t>0}{\\frac{K^{*n}(t)}{K(t)(2\\|K\\|)^{n-1}}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Then $a_{1}=1$ , and we are trying to prove ", "page_idx": 24}, {"type": "equation", "text": "$$\na_{n}\\leq\\left(\\frac{K(0)}{K(T)}+1\\right)(1+\\varepsilon)^{n-1}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "By definition of the convolution, we have that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\frac{K^{*(n+1)}(t)}{(2\\|K\\|)^{n}}=\\sum_{s=0}^{t}\\frac{K(s)K(t-s)}{2\\|K\\|}\\times\\frac{K^{*n}(t-s)}{K(t-s)(2\\|K\\|)^{n-1}}\\le a_{n}\\times\\sum_{s=0}^{t}\\frac{K(s)K(t-s)}{2\\|K\\|}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "By the hypothesis (56), ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathrm{for}\\,t\\geq T,\\qquad\\frac{K^{*(n+1)}(t)}{(2\\|K\\|)^{n}}\\leq a_{n}(1+\\varepsilon)K(t).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "For $t<T$ , we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\frac{K^{*(n+1)}(t)}{(2\\|K\\|)^{n}}=\\displaystyle\\sum_{s=0}^{t}\\frac{K(s)K^{*(n)}(t-s)}{(2\\|K\\|)^{n}}}&{}\\\\ {\\displaystyle(K\\mathrm{\\monotonically\\decreasing})}&{~\\leq K(0)\\displaystyle\\sum_{s=0}^{t}\\frac{K^{*n}(t-s)}{(2\\|K\\|)^{n}}}\\\\ &{~\\leq K(0)\\displaystyle\\frac{\\|K^{*n}\\|}{(2\\|K\\|)^{n}}=K(0)\\big(\\frac{1}{2}\\big)^{n},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the last equality follows by the equality $\\|K^{*n}\\|=\\|K\\|^{n}$ , [21, Theorem 2.2(i)]. ", "page_idx": 24}, {"type": "text", "text": "In conclusion, by monotonicity, we have that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\frac{K^{*(n+1)}(t)}{(2\\|K\\|)^{n}K(t)}\\le\\left\\{\\frac{K(0)}{2^{n}K(T)},\\right.\\quad\\,\\,\\,t\\le T\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Hence we have that ", "page_idx": 24}, {"type": "equation", "text": "$$\na_{n+1}\\leq{\\frac{K(0)}{K(T)2^{n}}}+(1+\\varepsilon)a_{n}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Developing the recursion, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\iota_{n+1}\\leq\\sum_{j=0}^{n-1}\\frac{(1+\\varepsilon)^{j}K(0)}{K(T)}\\times\\left(\\frac{1}{2}\\right)^{n-j}+(1+\\varepsilon)^{n}\\leq(1+\\varepsilon)^{n}\\left[\\frac{1}{1-1/2}-1\\right]\\frac{K(0)}{K(T)}+(1+\\varepsilon)^{n}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "The result is proven. ", "page_idx": 24}, {"type": "text", "text": "Remark C.4. If the assumption (56) holds only for $\\hat{T}>t>T$ , then the statement of Lemma C.1 still holds with ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{t\\leq\\hat{T}}\\left\\{\\frac{K^{*(n+1)}(t)}{K(t)}\\right\\}\\leq\\left(\\frac{K(0)}{K(T)}+1\\right)\\big(2\\|K\\|(1+\\varepsilon)\\big)^{n}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We now give a non-asymptotic bound for the general convolution-type Volterra equation. ", "page_idx": 24}, {"type": "text", "text": "Lemma C.2 (Non-asymptotic Volterra bound). Let $K$ and $f$ be non-negative functions. Suppose $K$ is monotonically decreasing and for some $\\varepsilon>0$ , there exists a $T(\\varepsilon)>\\bar{0}$ such that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\sum_{s=0}^{t}K(s)K(t-s)\\leq2(1+\\varepsilon)\\|K\\|K(t),\\ \\ \\,f o r\\,a l l\\,t\\geq T.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Moreover, suppose the convergence threshold condition $2(1+\\varepsilon)\\|K\\|<1$ holds. Then ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{f(t)+(K*f)(t)\\le P(t)\\le f(t)+C\\times(K*f)(t),}\\\\ {\\frac{\\rvert}{K(T)}+1\\bigg)\\bigg(\\frac{1}{1-2\\lvert|K\\rvert|(1+\\varepsilon)}\\bigg).\\qquad\\qquad\\qquad\\qquad\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where ", "page_idx": 25}, {"type": "text", "text": "Proof. We consider the upper and lower bound separately. ", "page_idx": 25}, {"type": "text", "text": "Lower bound: Since $K$ and $f$ is are non-negative, then $\\begin{array}{r}{\\sum_{j=1}^{\\infty}(K^{*j}\\,*\\,f)(t)\\,\\geq\\,(K^{*1}\\,*\\,f)(t)\\,\\geq\\,}\\end{array}$ $(K*f)(t)$ . Recall the solution to the convolution-type Volterra equation takes the form, ", "page_idx": 25}, {"type": "equation", "text": "$$\nP(t)=f(t)+\\sum_{j=1}^{\\infty}(K^{*j}*f)(t).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "It immediately follows from $\\textstyle\\sum_{j=1}^{\\infty}(K^{*j}*f)(t)\\geq(K*f)(t)$ the lower bound. ", "page_idx": 25}, {"type": "text", "text": "Upper bound: The solution to a Volterra equation (in $L^{1}$ ) is ", "page_idx": 25}, {"type": "equation", "text": "$$\nP(t)=f(t)+\\sum_{j=1}^{\\infty}(K^{*j}*f)(t).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "By Lemma C.1 and the hypothesis, there exists a $T>0$ and $\\varepsilon>0$ such that ", "page_idx": 25}, {"type": "equation", "text": "$$\nK^{*j}(s)\\leq K(s)\\left[\\frac{K(0)}{K(T)}+1\\right](2\\|K\\|(1+\\varepsilon))^{j-1},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and $(2\\|K\\|(1+\\varepsilon))^{j-1}<1$ . Hence, we have that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{j=1}^{\\infty}(K^{*j}*f)(t)=\\displaystyle\\sum_{j=1}^{\\infty}\\left(\\sum_{s=0}^{t}K^{*j}(s)f(t-s)\\right)}\\\\ {\\displaystyle\\qquad\\qquad\\leq\\left(\\frac{K(0)}{K(T)}+1\\right)\\sum_{j=1}^{\\infty}(2\\|K\\|(1+\\varepsilon))^{j-1}(K*f)(t)}\\\\ {\\displaystyle\\qquad\\qquad=\\left(\\frac{K(0)}{K(T)}+1\\right)\\left(\\frac{1}{1-2\\|K\\|(1+\\varepsilon)}\\right)(K*f)(t).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The result is shown. ", "page_idx": 25}, {"type": "text", "text": "C.3.2 Proof of Theorem 2.1 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We are now ready to show one of the main tools used to analyze the loss function, Theorem 2.1. The result relies on approximations for the kernel and forcing functions found in Section F and Section G. We restate the theorem statement to remind the reader of the result. ", "page_idx": 25}, {"type": "text", "text": "Theorem C.1 (Approximation solution for $\\Phi$ ). Suppose $\\gamma$ and $B$ is at most half the convergence threshold and $\\alpha>{\\frac{1}{4}}$ . There exists an $M>0$ large and a constant $C=C(\\alpha,\\beta,M)$ , independent of $d$ , so that for all admissible v and $d$ , for all $M<\\gamma B r$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathcal{F}(r)+(\\mathcal{K}*\\mathcal{F})(r)\\leq\\mathcal{P}(r)\\leq\\mathcal{F}(r)+C\\times(\\mathcal{K}*\\mathcal{F})(r).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The convolution further simplifies. For any $\\epsilon\\,>\\,0$ , there exists an $M\\,>\\,0$ and a constant $C=$ $C(\\alpha,\\beta,M)$ independent of $d$ so that for all $M<\\gamma B r$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n(1-\\epsilon)\\|\\mathcal{K}\\|\\cdot\\mathcal{F}(r)+\\frac{1}{C\\times\\gamma B}\\cdot\\mathcal{K}(r)\\leq(\\mathcal{K}*\\mathcal{F})(r)\\leq C\\times\\big(\\|\\mathcal{K}\\|\\cdot\\mathcal{F}(r)+\\frac{1}{\\gamma B}\\cdot\\mathcal{K}(r)\\big).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof of Theorem C.1 / Theorem 2.1. Note for all $\\gamma B r\\;\\;>\\;\\;1/M d^{2\\alpha}$ , we have that $c\\mathfrak{F}_{0}\\quad\\leq$ $\\mathcal{F}(r),\\dot{\\mathcal{K}}(r)\\,\\leq\\,C\\mathcal{F}_{0}(r)$ for some $C,c\\,>\\,0$ . This is where the limiting level starts to dominate. We begin by showing (58). Fix $\\varepsilon>0$ . From Proposition G.2, we have that there exists an $M>0$ sufficiently large so that the hypothesis for Kesten\u2019s Lemma, i.e., ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\sum_{s=0}^{r}\\mathcal K(s)\\mathcal K(r-s)\\leq2(1+\\varepsilon)\\|\\mathcal K\\|\\mathcal K(r),\\quad\\mathrm{for\\,all\\,}d^{2\\alpha}/M>\\gamma B r>M.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Therefore, we get (58) by Lemma C.2. ", "page_idx": 26}, {"type": "text", "text": "To prove (59), we begin by ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\sum_{s=0}^{r}\\mathcal{K}(r-s)\\mathcal{F}(s)=\\sum_{s=0}^{r/2}\\mathcal{K}(r-s)\\mathcal{F}(s)+\\sum_{s=r/2}^{r}\\mathcal{K}(r-s)\\mathcal{F}(s)\\leq\\mathcal{K}(\\frac{r}{2})\\sum_{s=0}^{r/2}\\mathcal{F}(s)+\\mathcal{F}(\\frac{r}{2})\\sum_{s=0}^{r/2}\\mathcal{K}(s)\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where we used monotonicity of $\\mathcal{F}$ and $\\mathcal{K}$ . ", "page_idx": 26}, {"type": "text", "text": "Using Proposition H.2 and Proposition H.4, for large $d^{2\\alpha}/M\\ge\\gamma B r\\ge M$ , we have that $\\mathcal{F}(\\frac{r}{2})\\asymp$ ${\\mathfrak{F}}(r)$ since $\\mathcal{F}$ is power law for large $r$ (see also Corollary F.1). The same holds for $\\mathcal{K}$ , using Proposition H.5 and Proposition G.2, $\\mathcal{K}(\\frac{r}{2})\\asymp\\mathcal{K}(r)$ for $d^{2\\alpha}/M\\geq\\gamma B r\\geq M$ for some $M>0$ . For small $\\gamma B r\\leq M$ , we have that $\\mathcal{F}(r/2)\\leq C$ and $\\mathcal{K}(r/2)\\leq C$ for some $C>0$ . Since $\\mathcal{F}$ and $\\mathcal{K}$ are monotonic, we can choose a constant so that $\\mathcal{F}(r/2)\\lesssim\\mathcal{F}(r)$ and $\\mathcal{K}(r/2)\\lesssim\\mathcal{K}(r)$ for $\\gamma B r\\leq M$ . Now using Proposition C.1 and Proposition H.6, we have that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\sum_{s=0}^{r}\\mathcal{K}(r-s)\\mathcal{F}(s)\\leq\\mathcal{K}(\\frac{r}{2})\\sum_{s=0}^{r/2}\\mathcal{F}(s)+\\mathcal{F}(\\frac{r}{2})\\sum_{s=0}^{r/2}\\mathcal{K}(s)\\leq\\frac{1}{\\gamma B}\\mathcal{K}(r)+\\mathcal{F}(r)\\|\\mathcal{K}\\|.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "For the lower bound, we have that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\underset{s=0}{\\overset{r}{\\sum}}\\mathcal{K}(r-s)\\mathcal{F}(s)=\\sum_{s=0}^{r/2}\\mathcal{K}(r-s)\\mathcal{F}(s)+\\sum_{s=r/2}^{r}\\mathcal{K}(r-s)\\mathcal{F}(s)\\geq\\mathcal{K}(r)\\sum_{s=0}^{r/2}\\mathcal{F}(s)+\\mathcal{F}(r)\\sum_{s=0}^{r/2}\\mathcal{K}(s),\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where we used monotonicity of $\\mathcal{K}$ and $\\mathcal{F}$ . ", "page_idx": 26}, {"type": "text", "text": "We note that $\\;{\\mathcal{F}}(s)\\asymp C$ for $\\gamma B s\\leq M$ for all $M>0$ . Therefore, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\sum_{s=0}^{r/2}\\mathcal{F}(s)\\geq\\sum_{s=0}^{M/(2\\gamma B)}\\mathcal{F}(s)\\geq\\frac{1}{\\gamma B}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "On the other hand, by Proposition C.1, for any $\\epsilon>0$ , there is an $M$ so that for any $\\gamma B r\\geq M$ , ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\sum_{s=0}^{r/2}\\mathcal{K}(s)\\geq(1-\\epsilon)\\|\\mathcal{K}\\|.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "This proves the lower bound. ", "page_idx": 26}, {"type": "text", "text": "C.4 Details of risk curves for the phases ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We can now put together a coherent picture of the effect of different choices of $\\alpha$ and $\\beta$ and their impact on the Pareto frontier. We will have 4 distinct phases where the expected loss will exhibit a power law decay and 1 region $(\\alpha+\\beta\\leq0.5)$ for which the expected loss has no power law decay (see Figure 2a). We will describe each of the 4 power law phases below marked by their boundaries. ", "page_idx": 26}, {"type": "text", "text": "First, we recall the forcing function $\\mathcal{F}$ and kernel function $\\mathcal{K}$ introduced in Section 2.1. ", "page_idx": 26}, {"type": "table", "img_path": "aVSxwicpAk/tmp/695d91717d67ff7d14a7ac7a944b157c1375733c37104cf7353cb4c832172525.jpg", "table_caption": ["Table 5: Decomposition of the forcing and kernel functions. We express the forcing function ${\\mathcal{F}}(r)$ as the sum of three functions, $\\mathcal{F}_{p p},\\mathcal{F}_{0},\\mathcal{F}_{a c}$ , up to errors and kernel function $\\mathcal{K}(r)$ as $\\mathcal{K}_{p p}(r)$ , up to errors. These functions arise from the different parts of the spectrum of the deterministic equivalent for the resolvent of $\\hat{K}$ . "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "Forcing function. For the forcing function, ", "text_level": 1, "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathcal{F}(r)=\\mathcal{F}_{0}(r)+\\mathcal{F}_{p p}(r)+\\mathcal{F}_{a c}(r)+\\mathrm{errors}_{\\mathcal{F}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The function ${\\mathfrak{F}}_{0}(r)$ is the component of the forcing function corresponding to the point mass at 0, $\\mathcal{F}_{p p}(r)$ is the component of the forcing function corresponding to the pure point part of the spectrum, and lastly, the most complicated part of the spectrum, the forcing function corresponding to the distorted features. In particular, we will show in Section F the exact definitions of $\\mathcal{F}_{0},\\mathcal{F}_{p p},\\mathcal{F}_{a c}$ and $|\\mathrm{error}_{\\mathcal{F}}|$ are small, and, in Section H, we derive asymptotic-like behaviors for these functions. See Table 5 for definitions and asymptotics. ", "page_idx": 27}, {"type": "text", "text": "Kernel function. Similarly, the kernel function $\\mathcal{K}$ is ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{K}(r)=\\mathcal{K}_{p p}(r)+\\mathrm{errors}_{\\mathcal{K}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Note here that the kernel function has a multiplication by the eigenvalue of $\\hat{K}$ and so the point mass at 0 will not contribute. In Section G, we will give an explicit definition of $\\mathcal{K}_{p p}$ and show error terms are small and, in Proposition H.5, we give the asymptotic-like behavior of $\\mathcal{K}_{p p}$ . ", "page_idx": 27}, {"type": "text", "text": "Now we describe in detail the different risk curves that arise for the different phases. ", "page_idx": 27}, {"type": "text", "text": "C.4.1 Above the high-dimensional line (Phases Ia, II, III) ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "This setting is commonly known as the trace class. It is characterized by four components: ", "page_idx": 27}, {"type": "text", "text": "\u2022 learning rate $\\gamma$ can be picked independent of dimension;   \n\u2022 loss curve does not self average, that is, the loss curve does not concentrate around a deterministic function;   \n\u2022 $v\\geq d$ , but $v$ has no upper bound and so we can take $v\\rightarrow\\infty$ ;   \n\u2022 batch, $B$ , is constrained to be small (see Proposition C.2). ", "page_idx": 27}, {"type": "text", "text": "When $2\\alpha>1$ , or the trace class phase, the loss will exhibit 3 different phases. We described these phases in detail below. ", "page_idx": 27}, {"type": "text", "text": "Phase Ia: $(2\\beta<1,2\\alpha>1)$ ). In this phase, it notable for three characteristics: ", "page_idx": 28}, {"type": "text", "text": "\u2022 absolutely continuous part of the forcing function does not participate;   \n\u2022 level at which SGD saturates is affected by $\\beta$ ;   \n\u2022 SGD noise does not participate. ", "page_idx": 28}, {"type": "text", "text": "In this case, the loss curve is just a constant multiple of gradient flow. Hence, we have that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathcal{P}(\\boldsymbol{r})\\asymp\\mathcal{F}_{p p}(\\boldsymbol{r})+\\mathcal{F}_{0}(\\boldsymbol{r}).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proposition C.3 (Phase Ia: $2\\beta<1,2\\alpha>1)$ . Suppose $2\\beta<1$ and $2\\alpha>1$ . Suppose the learning rate $\\gamma$ and batch $B>0$ satisfy at most half the convergence threshold in Proposition C.2. Then there exists an $M>0$ large and constants $C=C(\\alpha,\\beta,M)$ and $c=c(\\alpha,\\beta,M)$ , independent of $d$ , so that for all admissible $v$ and $d,$ , for all $\\gamma B r>M$ ", "page_idx": 28}, {"type": "equation", "text": "$$\nc\\times\\left(\\mathcal{F}_{p p}(r)+\\mathcal{F}_{0}(r)\\right)\\leq\\mathcal{P}(r)\\leq C\\times\\left(\\mathcal{F}_{p p}(r)+\\mathcal{F}_{0}(r)\\right).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. By Theorem C.1, we know that it suffices to look at the forcing function $\\mathcal{F}$ and kernel function $\\mathcal{K}$ . Moreover, in this regime, we have that $\\gamma$ and $B$ are constant (see Proposition C.2). ", "page_idx": 28}, {"type": "text", "text": "The rest of the argument relies on the bounds found in Proposition H.2, $(\\mathfrak{F}_{p p})$ , Proposition H.4 $(\\mathcal{F}_{a c})$ , Proposition H.3, $({\\mathcal{F}}_{0})$ , and Proposition H.5 $(\\mathcal{K}_{p p})$ . ", "page_idx": 28}, {"type": "text", "text": "For the forcing function, $\\mathcal{F}_{a c}(\\boldsymbol{r})=0$ as $2\\beta<1$ (Proposition H.4). Therefore the forcing function is composed of $\\mathcal{F}_{p p}(r)$ and ${\\mathfrak{F}}_{0}(r)$ . ", "page_idx": 28}, {"type": "text", "text": "First, we have that $(\\gamma B r)^{-2+1/(2\\alpha)}<(\\gamma B r)^{-(1+\\beta/\\alpha)+1/(2\\alpha)}$ as $\\beta<\\alpha$ in this phase. Thus, using Proposition H.2 and Proposition H.5, for $\\gamma B r\\,>\\,M$ , where $M$ is some constant, we have that \u03b31B Kpp(r) \u2264C \u00d7 Fpp(r) for some C > 0. Hence the result is shown. \u53e3 ", "page_idx": 28}, {"type": "text", "text": "As a consequence of the argument above, we know that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathcal{P}(r)\\approx\\left\\{\\mathcal{F}_{p p}(r),\\quad\\mathrm{if}\\;\\gamma B r\\leq D_{0}\\right.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Phase II: $(2\\beta>1,2\\alpha>1,\\beta<\\alpha)$ For this phase, we see that ", "page_idx": 28}, {"type": "text", "text": "\u2022 limit level is unaffected by $\\beta$ ;   \n\u2022 absolutely continuous spectrum takes over for $r\\in(M d^{\\alpha},d^{2\\alpha}/M)$ for some $M$ ;   \n\u2022 SGD noise does not participate. ", "page_idx": 28}, {"type": "text", "text": "Therefore, in this case, we have that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathcal{P}(r)\\asymp\\mathcal{F}_{p p}(r)+\\mathcal{F}_{a c}(r)+\\mathcal{F}_{0}(r).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proposition C.4 (Phase II: $2\\beta>1,2\\alpha>1,\\beta<\\alpha)$ . Suppose $2\\beta>1,\\,2\\alpha>1,$ , and $\\beta<\\alpha$ . Suppose the learning rate $\\gamma$ and batch $B>0$ satisfy at most half the convergence threshold in Proposition C.2. Then there exists an $M>0$ large and constants $C=C(\\alpha,\\beta,M)$ and $c=c(\\alpha,\\beta,M)$ , independent of $d,$ , so that for all admissible v and $d,$ , for all $\\gamma B r>M$ ", "page_idx": 28}, {"type": "equation", "text": "$$\nc\\times\\left(\\mathcal{F}_{p p}(r)+\\mathcal{F}_{a c}(r)+\\mathcal{F}_{0}(r)\\right)\\leq\\mathcal{P}(r)\\leq C\\times\\left(\\mathcal{F}_{p p}(r)+\\mathcal{F}_{a c}(r)+\\mathcal{F}_{0}(r)\\right).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. By Theorem C.1, we know that it suffices to look at the forcing function $\\mathcal{F}$ and kernel function $\\mathcal{K}$ . Moreover, in this regime, we have that $\\gamma$ and $B$ are constant (see Proposition C.2). ", "page_idx": 28}, {"type": "text", "text": "The rest of the argument relies on the bounds found in Proposition H.2, $(\\mathfrak{F}_{p p})$ ), Proposition H.4 $(\\mathcal{F}_{a c})$ , Proposition H.3, $({\\mathcal{F}}_{0})$ , and Proposition H.5 $(\\mathcal{K}_{p p})$ . ", "page_idx": 28}, {"type": "text", "text": "$\\gamma B r\\,\\leq\\,M_{0}$ , for some $M_{0}$ : First, we have that $\\begin{array}{r}{\\frac{1}{\\gamma B}\\mathcal{K}_{p p}(r)\\leq C_{0}\\times\\mathcal{F}_{p p}(r)}\\end{array}$ (Proposition H.5) and $\\mathcal{F}_{a c}(r)\\leq C_{0}\\times\\mathcal{F}_{p p}(r)$ (Proposition H.4) for some constant $C_{0}>0$ . The constant $M_{0}$ is where the asymptotic of ${\\mathcal F}_{p p}$ starts to apply. ", "page_idx": 28}, {"type": "text", "text": "$M_{0}\\;\\leq\\;\\gamma B r\\;\\leq\\;M_{1},$ , for some $M_{0}$ and for all $M_{1}~>~M_{0}$ : We see that $(\\gamma B r)^{-2+1/(2\\alpha)}\\ <$ $(\\gamma B r)^{-(1+\\beta/\\alpha)+1/(2\\alpha)}$ as $\\beta<\\alpha$ in this phase. Thus, using Proposition H.2 and Proposition H.5, we have that $\\begin{array}{r}{\\frac{1}{\\gamma B}\\mathcal{K}_{p p}(r)\\:\\leq\\:C_{1}\\:\\times\\:\\mathcal{F}_{p p}(\\:\\:\\stackrel{.}{r})}\\end{array}$ for some $C_{1}~>~0$ . A quick computation shows that $\\mathcal{F}_{a c}(r)\\leq\\mathcal{F}_{p p}(\\dot{r})$ . ", "page_idx": 29}, {"type": "text", "text": "$M_{1}\\,\\leq\\,\\gamma B r\\,\\leq\\,M_{2}d^{2\\alpha}$ , for any $M_{1}$ and some $M_{2}$ : The $M_{2}$ is the smallest of the two endpoints for the asymptotics of ${\\mathcal F}_{p p}$ and $\\mathcal{F}_{a c}$ . As in the previous regime, we have that $\\begin{array}{r}{\\frac{1}{\\gamma B}\\mathcal{K}_{p p}(r)\\lesssim\\dot{\\mathcal{F}}_{p p}(r)}\\end{array}$ In this region, $\\mathcal{F}_{a c}(r)\\,\\asymp\\,d^{-1}(\\gamma B r)^{-1+1/(2\\alpha)}$ and $\\mathcal{F}_{p p}(r)\\,\\asymp\\,(\\gamma B r)^{-(1+\\beta/\\alpha)+1/(2\\alpha)}$ . We see at $(\\gamma B r)=d^{2\\alpha}$ that $(\\gamma B r)^{-\\beta/\\alpha}\\le(d^{2\\alpha})^{-\\beta/\\alpha}=d^{-2\\beta}\\le d^{-1}$ as $2\\beta>1$ . Therefore, at $\\gamma B r=d^{2\\alpha}$ , $\\mathcal{F}_{p p}(\\boldsymbol{r})\\lesssim\\mathcal{F}_{a c}(\\boldsymbol{r})$ and we started, i.e., when $r\\,=\\,M_{1}$ with $\\mathcal{F}_{a c}(r)\\lesssim\\mathcal{F}_{p p}(r)$ . Therefore, we must change in this regime to being $\\mathcal{F}_{a c}$ dominate. ", "page_idx": 29}, {"type": "text", "text": "$M_{2}d^{2\\alpha}\\leq\\gamma B r$ for all $M_{2}$ : In this case, all terms are bounded above by ${\\mathfrak{F}}_{0}(r)$ . ", "page_idx": 29}, {"type": "text", "text": "As a consequence of the argument above, we know that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\Phi(r)\\approx\\left\\{\\!\\!\\!\\begin{array}{l l}{\\!\\!\\mathcal{F}_{p p}(r),}&{\\mathrm{if}\\;\\gamma B r\\leq D_{0}}\\\\ {\\!\\!\\mathcal{F}_{a c}(r),}&{\\mathrm{if}\\;D_{0}\\leq\\gamma B r\\leq D_{1}}\\\\ {\\!\\!\\mathcal{F}_{0}(r),}&{\\mathrm{if}\\;\\gamma B r\\geq D_{1}}\\end{array}\\!\\!\\right.\\;\\mathrm{for~some}\\;D_{0},D_{1}\\;\\mathrm{that~depend~on}\\;d.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Phase III: SGD noise appears, $(2\\beta>1,2\\alpha>1,\\beta>\\alpha)$ In this case, we see that SGD changes the dynamics over gradient flow. In particular, ", "page_idx": 29}, {"type": "text", "text": "\u2022 limit level is unaffected by $\\beta$ ;   \n\u2022 absolutely continuous forcing function takes over for iterations $r\\in(M d,d^{2\\alpha}/M)$ for some $M$ ;   \n\u2022 SGD noise regulates convergence. ", "page_idx": 29}, {"type": "text", "text": "Thus, we have that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{P}(r)\\asymp\\mathcal{F}_{a c}(r)+\\mathcal{F}_{0}(r)+\\frac{1}{\\gamma B}\\mathcal{K}_{p p}(r).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Proposition C.5 (Phase III: $2\\beta\\,>\\,1$ , $2\\alpha\\,>\\,1$ , $\\beta\\,>\\,\\alpha\\!\\mathrm{,}$ ). Suppose $2\\beta>1$ , $2\\alpha\\,>\\,1$ , and $\\beta\\,>\\,\\alpha$ . Suppose the learning rate $\\gamma$ and batch $B~>~0$ satisfy at most half the convergence threshold in Proposition C.2. Then there exists an $M\\ >\\ 0$ large and constants $C\\;=\\;C(\\alpha,\\beta,M)$ and $c=c(\\alpha,\\beta,M)$ , independent of $d_{;}$ , so that for all admissible $v$ and $d,$ , for all $\\gamma B r>M$ ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{c\\times\\left(\\mathcal{F}_{a c}(r)+\\mathcal{F}_{0}(r)+\\frac{1}{\\gamma B}\\mathcal{K}_{p p}(r)\\right)\\leq\\mathcal{P}(r)\\leq C\\times\\left(\\mathcal{F}_{a c}(r)+\\mathcal{F}_{0}(r)+\\frac{1}{\\gamma B}\\mathcal{K}_{p p}(r)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Proof. By Theorem C.1, we know that it suffices to look at the forcing function $\\mathcal{F}$ and kernel function $\\mathcal{K}$ . Moreover, in this regime, we have that $\\gamma$ and $B$ are constant (see Proposition C.2). ", "page_idx": 29}, {"type": "text", "text": "The rest of the argument relies on the bounds found in Proposition H.2, $(\\mathfrak{F}_{p p})$ , Proposition H.4 $(\\mathcal{F}_{a c})$ , Proposition H.3, $({\\mathcal{F}}_{0})$ , and Proposition H.5 $(\\mathcal{K}_{p p})$ . ", "page_idx": 29}, {"type": "text", "text": "$\\gamma B r\\,\\leq\\,M_{0}$ , for some $M_{0}$ : First, we have that $\\begin{array}{r}{\\mathcal{F}_{p p}(r)\\,\\leq\\,C_{0}\\,\\times\\,\\frac{1}{\\gamma B}\\mathcal{K}_{p p}(r)\\,}\\end{array}$ (Proposition H.5) and $\\begin{array}{r}{\\mathcal{F}_{a c}(r)\\leq C_{0}\\times\\frac{1}{\\gamma B}\\mathcal{K}_{p p}(r)}\\end{array}$ (Proposition H.4) for some constant $C_{0}>0$ . The constant $M_{0}$ is where the asymptotic of $\\mathcal{K}_{p p}$ starts to apply. ", "page_idx": 29}, {"type": "text", "text": "$M_{0}\\ \\leq\\ \\gamma B r\\ \\leq\\ M_{1},$ , for some $M_{0}$ and for all $M_{1}~>~M_{0}$ : We see that $(\\gamma B r)^{-2+1/(2\\alpha)}~>$ $(\\gamma B r)^{-(1+\\beta/\\alpha)+1/(2\\alpha)}$ as $\\beta>\\alpha$ in this phase. Thus, using Proposition H.2 and Proposition H.5, we have that $\\begin{array}{r}{\\mathcal{F}_{p p}\\ \\leq\\ C_{1}\\,\\times\\,\\frac{1}{\\gamma B}\\mathcal{K}_{p p}(r)}\\end{array}$ for some $C_{1}~>~0$ . A quick computation shows that $\\mathcal{F}_{a c}(r)\\leq\\mathcal{K}_{p p}(r)$ . ", "page_idx": 29}, {"type": "text", "text": "$M_{1}\\leq\\gamma B r\\leq M_{2}d^{2\\alpha},$ , for any $M_{1}$ and some $M_{2}$ : The $M_{2}$ is the smallest of the two endpoints for the asymptotics of $\\mathcal{K}_{p p}$ and $\\mathcal{F}_{a c}$ . As in the previous regime, we have that $\\begin{array}{r}{\\frac{1}{\\gamma B}\\mathcal{K}_{p p}(r)\\overset{\\cdot}{\\lesssim}\\mathcal{F}_{p p}(r)}\\end{array}$ . In this region, $\\mathfrak{F}_{a c}(r)\\asymp d^{-1}r^{-1+1/(2\\alpha)}$ and $\\mathcal{K}_{p p}(r)\\,\\asymp\\,r^{-2+1/(2\\alpha)}$ . We see at $\\gamma B r\\,=\\,d^{2\\alpha}$ that $(\\gamma B r)^{-1}\\leq(d^{2\\alpha})^{-1}=d^{-2\\alpha}\\leq d^{-1}$ as $2\\alpha>1$ . Therefore, at $(\\gamma B r)\\asymp d^{2\\alpha}$ , $\\mathcal{K}_{p p}(r)\\lesssim\\mathcal{F}_{a c}(r)$ and we started, i.e., when $r=M_{1}$ with $\\mathcal{F}_{a c}(r)\\lesssim\\mathcal{K}_{p p}(r)$ . Therefore, we must change in this regime to ", "page_idx": 29}, {"type": "text", "text": "being $\\mathcal{F}_{a c}$ dominate. ", "page_idx": 30}, {"type": "text", "text": "As a consequence of the argument above, we know that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\Phi(r)\\approx\\left\\{\\!\\!\\!\\begin{array}{l l}{\\mathcal{K}_{p p}(r),}&{\\mathrm{if}\\;\\gamma B r\\leq D_{0}}\\\\ {\\mathcal{F}_{a c}(r),}&{\\mathrm{if}\\;D_{0}\\leq\\gamma B r\\leq D_{1}}\\\\ {\\mathcal{F}_{0}(r),}&{\\mathrm{if}\\;\\gamma B r\\geq D_{1}}\\end{array}\\right.\\quad\\mathrm{for~some}\\;D_{0},D_{1}\\mathrm{~that~depend~on~}d.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "C.4.2 Below the high-dimensional line (Phases IVa, IVb, Ib, Ic) ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "One of the main differences between the previous regime and this regime is that $V$ can not be taken to $\\infty$ independent of $d$ . As a result, we call this below the high-dimensional line and it is precisely bounded by whether $2\\alpha$ is summable or not. ", "page_idx": 30}, {"type": "text", "text": "The four main characteristics of this regime are: ", "page_idx": 30}, {"type": "text", "text": "\u2022 learning rate $\\gamma$ scales like $v^{-1+2\\alpha}$ ;   \n\u2022 SGD loss, i.e., $\\mathbb{E}\\left[\\mathcal{P}(\\theta_{r})\\right]$ self-concentrates;   \n\u2022 $v$ can not be too large, i.e., $d$ and $v$ are proportional;   \n\u2022 batch can be large (i.e., $\\gamma B\\leq1$ ) since the learning rate is small $(\\gamma\\sim v^{-1+2\\alpha})$ . ", "page_idx": 30}, {"type": "text", "text": "In Phases IV, Ib, and Ic, because $j^{-2\\alpha}$ is not summable, the summation of the $j$ depends on the dimension $v$ . Thus, the kernel norm is ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\|\\mathcal{K}\\|\\sim\\frac{\\gamma}{2}\\sum_{j=1}^{v}j^{-2\\alpha}\\sim\\frac{\\gamma}{2(1-2\\alpha)}v^{1-2\\alpha},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where the learning rate $\\gamma$ is chosen so that $\\lVert\\mathcal{K}\\rVert$ is constant, i.e., $\\begin{array}{r}{\\gamma=\\frac{\\tilde{\\gamma}}{\\|\\mathcal{K}\\|}}\\end{array}$ where $\\tilde{\\gamma}>0$ is a constant. ", "page_idx": 30}, {"type": "text", "text": "Phase IV, $\\begin{array}{r}{(2\\beta>1,\\frac{1}{4}<\\alpha<\\frac{1}{2})}\\end{array}$ ). In this phase, we have the following ", "page_idx": 30}, {"type": "text", "text": "\u2022 limiting value of the loss that SGD converges to is unaffected by $\\beta$ ;   \n\u2022 pure point forcing function plays a role;   \n\u2022 absolutely continuous part of the spectrum does not contribute to the forcing function;   \n\u2022 SGD noise affect the loss curves. ", "page_idx": 30}, {"type": "text", "text": "In this phase, the loss curve is ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{P}(r)\\asymp\\mathcal{F}_{p p}(r)+\\mathcal{F}_{0}(r)+\\frac{1}{\\gamma B}\\mathcal{K}_{p p}(r).}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "The following gives the precise statement. ", "page_idx": 30}, {"type": "text", "text": "Proposition C.6 (Phase IV: $2\\beta>1$ , $\\begin{array}{r}{\\frac14<\\alpha<\\frac12}\\end{array}$ ). Suppose $2\\beta>1$ and $\\begin{array}{r}{\\frac14<\\alpha<\\frac12}\\end{array}$ . Suppose the learning rate $\\gamma$ and batch $B>0$ satisfy at most half the convergence threshold in Proposition C.2. Then there exists an $M>0$ large and constants $C=C(\\alpha,\\beta,M)$ and $c=c(\\alpha,\\beta,M)$ , independent of $d,$ , so that for all admissible $v$ and $d$ , for all $\\gamma B r>M$ ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{c\\times\\left(\\mathcal{F}_{p p}(r)+\\mathcal{F}_{0}(r)+\\frac{1}{\\gamma B}\\mathcal{K}_{p p}(r)\\right)\\leq\\mathcal{P}(r)\\leq C\\times\\left(\\mathcal{F}_{p p}(r)+\\mathcal{F}_{0}(r)+\\frac{1}{\\gamma B}\\mathcal{K}_{p p}(r)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Proof. By Theorem C.1, we know that it suffices to look at the forcing function $\\mathcal{F}$ and kernel function $\\mathcal{K}$ . Moreover, in this regime, we have that $\\gamma$ decreases like $d^{2\\alpha-1}$ (see Proposition C.2). ", "page_idx": 30}, {"type": "text", "text": "The rest of the argument relies on the bounds found in Proposition H.2, $(\\mathfrak{F}_{p p})$ ), Proposition H.4 $(\\mathcal{F}_{a c})$ , Proposition H.3, $({\\mathcal F}_{0})$ , and Proposition H.5 $(\\mathcal{K}_{p p})$ . ", "page_idx": 30}, {"type": "text", "text": "We first note there is no $\\mathcal{F}_{a c}(r)\\lesssim\\mathcal{F}_{0}$ and therefore it is too small to contribute. ", "page_idx": 30}, {"type": "text", "text": "$\\gamma B r\\leq M_{0}$ , for some $M_{0}$ : First, we have that $\\begin{array}{r}{\\frac{1}{\\gamma B}\\mathcal{K}_{p p}(r)\\leq C_{0}\\times\\mathcal{F}_{p p}(r)}\\end{array}$ for some constant $C_{0}>0$ .   \nThe constant $M_{0}$ is where the asymptotic of $\\mathfrak{F}_{p p}$ starts to apply. ", "page_idx": 31}, {"type": "text", "text": "$M_{0}~\\leq~\\gamma B r~\\leq~M_{1}$ , for some $M_{0}$ and for all $M_{1}~>~M_{0}$ : We see that $\\gamma(\\gamma B r)^{-2+1/(2\\alpha)}\\ <$ $(\\gamma B r)^{-(1+\\beta/\\alpha)+1/(2\\alpha)}$ since $\\gamma\\asymp d^{2\\alpha-1}$ and $2\\alpha<1$ in this phase. Thus, using Proposition H.2 and Proposition H.5, we have that $\\begin{array}{r}{\\frac{1}{\\gamma B}\\mathcal{K}_{p p}(r)\\leq C_{1}\\times\\mathcal{F}_{p p}}\\end{array}$ for some $C_{1}>0$ . ", "page_idx": 31}, {"type": "text", "text": "$M_{1}\\,\\leq\\,\\gamma B r\\,\\leq\\,M_{2}d^{2\\alpha}$ , for any $M_{1}$ and some $M_{2}$ : The $M_{2}$ is the smallest of the two endpoints for the asymptotics of $\\mathcal{K}_{p p}$ and ${\\mathfrak{F}}_{p p}$ . In this region, $\\mathcal{F}_{p p}(r)~\\asymp~(\\gamma B r)^{-(1+\\beta/\\alpha)+1/(2\\tilde{\\alpha})}$ and $\\begin{array}{r}{\\gamma\\times\\frac{1}{\\gamma B}\\mathcal{K}_{p p}(r)\\,\\asymp\\,\\gamma\\times(\\gamma\\dot{B}r)^{-2+1\\slash(2\\alpha)}\\,\\asymp\\,d^{2\\alpha-1}\\times(\\gamma B r)^{-2+1\\slash(2\\alpha)}}\\end{array}$ . We see at $r\\,=\\,d^{2\\alpha}$ that $d^{2\\alpha-1}(\\gamma B r)^{-1}=d^{-1}\\geq d^{-2\\beta}=(d^{2\\alpha})^{-\\beta/\\alpha}$ . Thus $\\begin{array}{r}{\\mathcal{F}_{p p}(r)\\lesssim\\frac{1}{\\gamma B}\\mathcal{K}_{p p}(r)}\\end{array}$ and we started, i.e., when $r=M_{1}$ with $\\mathcal{K}_{p p}(r)\\lesssim\\mathcal{F}_{p p}(r)$ . Therefore, we must change in this regime to being $\\mathcal{K}_{p p}$ dominate. ", "page_idx": 31}, {"type": "text", "text": "$M_{2}d^{2\\alpha}\\leq\\gamma B r$ for all $M_{2}$ : In this case, all terms are bounded above by ${\\mathfrak{F}}_{0}(r)$ . ", "page_idx": 31}, {"type": "text", "text": "As a consequence of the argument above, we know that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathcal{P}(r)\\approx\\left\\{\\frac{\\mathcal{F}_{p p}(r),}{\\frac{1}{\\gamma B}\\mathcal{K}_{p p}(r),}\\quad\\mathrm{if~}\\gamma B r\\leq D_{0}\\right.}\\\\ {\\left.\\mathcal{F}_{0}(r),\\quad\\mathrm{if~}D_{0}\\leq\\gamma B r\\leq D_{1}\\right.\\quad\\mathrm{for~some~}D_{0},D_{1}\\mathrm{~that~depend~on~}d.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Phase Ib, $(2\\beta<1,0.25<\\alpha<0.5,2(\\alpha+\\beta)>1)$ . Phase Ia, Ib, and Ic are quite similar as the dynamics of SGD only depend on the forcing function pure point and limiting value. In this phase, the learning rate $\\gamma$ is dimension dependent, unlike Phase Ia, and the following hold ", "page_idx": 31}, {"type": "text", "text": "\u2022 limiting value of the loss that SGD converges to is $d^{-2\\alpha+1-2\\beta}$ ;   \n\u2022 absolutely continuous part of the spectrum does not contribute to the forcing function;   \n\u2022 SGD noise not does affect the loss curves. ", "page_idx": 31}, {"type": "text", "text": "In this phase, the loss curve is ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathcal{P}(\\boldsymbol{r})\\asymp\\mathcal{F}_{p p}(\\boldsymbol{r})+\\mathcal{F}_{0}(\\boldsymbol{r}).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Although we did not prove the statement for $\\alpha<0.25$ as we do not have estimates for the kernel function, we believe that statement still holds. We believe that the kernel function stops becoming power law when $\\alpha<0.25$ , but the forcing function is still power law. ", "page_idx": 31}, {"type": "text", "text": "The following gives the precise statement. ", "page_idx": 31}, {"type": "text", "text": "Proposition C.7 (Phase Ib: $2\\beta<1$ , $\\textstyle{\\frac{1}{4}}<\\alpha<{\\frac{1}{2}}$ , $2(\\alpha+\\beta)>1)$ ). Suppose $2\\beta<1$ , $2(\\alpha+\\beta)>1$ , and $\\textstyle{\\frac{1}{4}}<\\alpha<{\\frac{1}{2}}$ . Suppose the learning rate $\\gamma$ and batch $B>0$ satisfy at most half the convergence threshold in Proposition C.2. Then there exists an $M>0$ large and constants $C=C(\\alpha,\\beta,M)$ and $c=c(\\alpha,\\beta,M)$ , independent of $d_{;}$ , so that for all admissible $v$ and $d_{\\cdot}$ , for all $\\gamma B r>M$ ", "page_idx": 31}, {"type": "equation", "text": "$$\nc\\times\\big(\\mathcal{F}_{p p}(r)+\\mathcal{F}_{0}(r)\\big)\\leq\\mathcal{P}(r)\\leq C\\times\\big(\\mathcal{F}_{p p}(r)+\\mathcal{F}_{0}(r)\\big).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Proof. By Theorem C.1, we know that it suffices to look at the forcing function $\\mathcal{F}$ and kernel function $\\mathcal{K}$ . Moreover, in this regime, we have that $\\gamma$ decreases like $d^{2\\alpha-1}$ (see Proposition C.2). ", "page_idx": 31}, {"type": "text", "text": "The rest of the argument relies on the bounds found in Proposition H.2, $(\\mathfrak{F}_{p p})$ , Proposition H.4 $(\\mathcal{F}_{a c})$ , Proposition H.3, $({\\mathcal{F}}_{0})$ , and Proposition H.5 $(\\mathcal{K}_{p p})$ . ", "page_idx": 31}, {"type": "text", "text": "We first note there is no $\\mathcal{F}_{a c}(r)$ . ", "page_idx": 31}, {"type": "text", "text": "$\\gamma B r\\leq M_{0},$ , for some $M_{0}$ : First, we have that $\\begin{array}{r}{\\frac{1}{\\gamma B}\\mathcal{K}_{p p}(r)\\leq C_{0}\\times\\mathcal{F}_{p p}(r)}\\end{array}$ for some constant $C_{0}>0$ .   \nThe constant $M_{0}$ is where the asymptotic of ${\\mathcal F}_{p p}$ starts to apply. ", "page_idx": 31}, {"type": "text", "text": "$M_{0}\\,\\leq\\,\\gamma B r\\,\\leq\\,M_{1}d^{2\\alpha}$ , for any $M_{0}$ and some $M_{1}$ : The $M_{1}$ is the smallest of the two endpoints for the asymptotics of $\\mathcal{K}_{p p}$ and ${\\mathcal F}_{p p}$ . In this region, $\\mathcal{F}_{p p}(r)~\\asymp~(\\gamma B r)^{-(1+\\beta/\\alpha)+1/(2\\tilde{\\alpha})}$ and $\\begin{array}{r}{\\gamma\\times\\frac{1}{\\gamma B}\\mathcal{K}_{p p}(r)\\,\\asymp\\,\\gamma\\times(\\gamma B r)^{-2+1/(2\\alpha)}\\,\\asymp\\,d^{2\\alpha-1}\\times(\\gamma B r)^{-2+1/(2\\alpha)}}\\end{array}$ . We see at $r\\,=\\,d^{2\\alpha}$ that $d^{2\\alpha-1}(\\gamma B r)^{-1}=d^{-1}\\leq d^{-2\\beta}=(d^{2\\alpha})^{-\\beta/\\alpha}$ . Thus $\\begin{array}{r}{\\frac{1}{\\gamma B}\\mathcal{K}_{p p}(r)\\lesssim\\mathcal{F}_{p p}(r)}\\end{array}$ and we started, i.e., when ", "page_idx": 31}, {"type": "text", "text": "$M_{1}d^{2\\alpha}\\leq\\gamma B r$ for all $M_{1}$ : In this case, all terms are bounded above by ${\\mathfrak{F}}_{0}(r)$ . ", "page_idx": 32}, {"type": "text", "text": "We expect Prop. C.7 to hold with the same conclusions for $2\\beta<1,2\\alpha<1,$ , and $2(\\alpha+\\beta)>1$ . As a consequence of the argument above, we know that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathcal{P}(r)\\approx\\left\\{\\mathcal{F}_{p p}(r),\\quad\\mathrm{if}\\;\\gamma B r\\leq D_{0}\\right.\\quad\\mathrm{for\\;some}\\;D_{0}\\;\\mathrm{that\\;depends\\;on}\\;d.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Phase Ic, $(2\\beta>1,0<\\alpha<\\textstyle{\\frac{1}{4}}$ . Lastly, we consider Phase Ic, which is similar to Phases Ia and Ib.   \nThe following holds in this phase. \u2022 limiting value of the loss that SGD converges to is $d^{-2\\alpha+1-2\\beta}$ ;   \n\u2022 absolutely continuous part of the spectrum does not contribute to the forcing function;   \n\u2022 SGD noise not does affect the loss curves. ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "In this phase, the loss curve is ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathcal{P}(\\boldsymbol{r})\\asymp\\mathcal{F}_{p p}(\\boldsymbol{r})+\\mathcal{F}_{0}(\\boldsymbol{r}).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Under the assumption that Theorem C.1 holds for $\\alpha>1/4$ , we get the following. ", "page_idx": 32}, {"type": "text", "text": "Proposition C.8 (Phase Ic: $2\\beta~>~1$ , $\\begin{array}{r}{0\\ <\\ \\alpha\\ <\\ \\frac{1}{4})}\\end{array}$ . Suppose $2\\beta~>~1$ and $\\smash{0\\leq\\alpha\\ <\\ \\frac{1}{4}}$ and Theorem C.1 holds. Suppose the learning rate $\\gamma$ and batch $B>0$ satisfy at most half the convergence threshold in Proposition C.2. Then there exists an $M>0$ large and constants $C=C(\\alpha,\\beta,M)$ and $c=c(\\alpha,\\beta,M)$ , independent of $d_{:}$ , so that for all admissible $v$ and $d,$ , for all $\\gamma B r>M$ ", "page_idx": 32}, {"type": "equation", "text": "$$\nc\\times\\big(\\mathcal{F}_{p p}(r)+\\mathcal{F}_{0}(r)\\big)\\leq\\mathcal{P}(r)\\leq C\\times\\big(\\mathcal{F}_{p p}(r)+\\mathcal{F}_{0}(r)\\big).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "We can not prove this statement as we do not have sharp bounds on the kernel function in this region. We believe that the kernel function stops becoming power law, but the forcing function is still power law. Thus, it should become even more forcing function dominate. ", "page_idx": 32}, {"type": "text", "text": "We believe the loss curve follows similar behavior to Phase Ia and Phase Ib, that is, ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathcal{P}(r)\\approx\\left\\{\\mathcal{F}_{p p}(r),\\quad\\mathrm{if}\\;\\gamma B r\\leq D_{0}\\right.\\quad\\mathrm{for\\;some}\\;D_{0}\\;\\mathrm{that\\;depends\\;on}\\;d.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "D Compute-optimal curves ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Throughout this section, consider the deterministic equivalent loss function $\\mathcal{P}(r)=\\mathcal{P}(r,d)$ . Moreover as batch size $B$ is order 1, it only effects the compute-optimal curves by a constant. Therefore, we can set $B=1$ . For each iteration $r$ , the SGD costs $d$ flops, or equivalently $r/d=\\mathrm{flops},\\,\\upxi$ . The goal is to find the optimal compute line as a function of the number of flops $\\mathfrak{f}$ : ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{d}\\mathcal{P}(\\frac{\\mathfrak{f}}{d},d).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "If $\\begin{array}{r}{d^{\\star}(\\mathfrak{f})\\stackrel{\\mathrm{def}}{=}\\arg\\operatorname*{min}_{d}\\mathcal{P}(\\frac{\\mathfrak{f}}{d},d)}\\end{array}$ , the optimal compute line is precisely $\\begin{array}{r}{\\mathcal{P}\\big(\\frac{\\mathfrak{f}}{d^{\\star}(\\mathfrak{f})},d^{\\star}(\\mathfrak{f})\\big)}\\end{array}$ . ", "page_idx": 32}, {"type": "text", "text": "To do this, we simplify the loss curve $\\textstyle\\mathcal{P}(\\frac{\\mathfrak{f}}{d},d)$ . While it is possible to minimize this as a function of $d$ an alternative function considered is the following ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{\\boldsymbol{\\mathcal{P}}}(\\boldsymbol{r},d)\\stackrel{\\mathrm{def}}{=}\\boldsymbol{\\mathcal{F}}_{p p}(\\boldsymbol{r},d)\\vee\\boldsymbol{\\mathcal{F}}_{a c}(\\boldsymbol{r},d)\\vee\\boldsymbol{\\mathcal{F}}_{0}(\\boldsymbol{r},d)\\vee\\frac{1}{\\gamma B}\\boldsymbol{\\mathcal{K}}_{p p}(\\boldsymbol{r},d),}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "which achieves the right power law behavior as the true compute-optimal curve and deviates from this true curve by an absolutely constant (independent of $d,\\dag)$ (see Theorem C.1). Note here some of the terms should be taken as 0 when not defined for the different phases. ", "page_idx": 32}, {"type": "text", "text": "Using this alternative loss function, $\\tilde{\\mathcal{P}}(r,d)$ , the compute-optimal line must occur at one of the corner points, i.e., where any pair of functions equal each other. The following lemma gives a useful characterization of these points. ", "page_idx": 32}, {"type": "table", "img_path": "aVSxwicpAk/tmp/84ad1010a0cbd5723f1283753b789aa103ef653e220e56bc5c06eeed5b1b327a.jpg", "table_caption": ["Table 6: Summary of the compute-optimal curves for $\\tilde{\\mathcal{P}}(\\boldsymbol{\\underline{{\\mathfrak{f}}}},d)$ for above the high-dimensional line, $2\\alpha>1$ . This includes Phases Ia, II, and III. "], "table_footnote": [], "page_idx": 33}, {"type": "text", "text": "Lemma D.1. Suppose $\\mathcal{C}_{0},\\mathcal{C}_{1}>0$ are constants and $\\gamma_{0},\\gamma_{1},p_{0},p_{1}>0$ exponents such that a function $\\hat{\\mathcal{P}}(r,d)$ equals ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\hat{\\mathcal{P}}(r,d)=\\operatorname*{max}\\left\\{\\mathcal{C}_{0}r^{-\\gamma_{0}}d^{-p_{0}},\\mathcal{C}_{1}r^{-\\gamma_{1}}d^{-p_{1}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Then replacing $\\begin{array}{r}{r\\mapsto\\frac{\\mathfrak{f}}{d}}\\end{array}$ the minimizer in $d$ satisfies ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{d^{\\star}\\stackrel{d e f}{=}a r g\\,m i n_{d}\\,\\{\\hat{\\Phi}(\\mathfrak{f},d)\\}=\\big(\\frac{\\mathfrak{C}_{0}}{\\mathfrak{C}_{1}}\\big)^{1/(\\gamma_{1}-p_{1}-\\gamma_{0}+p_{0})}\\times\\mathfrak{f}^{(-\\gamma_{0}+\\gamma_{1})/(\\gamma_{1}-p_{1}-\\gamma_{0}+p_{0})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "and the optimal value is ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{d}\\hat{\\mathcal{P}}(\\mathfrak{f},d)=\\mathfrak{C}_{0}\\times\\mathfrak{f}^{-\\gamma_{0}}\\times(d^{\\star})^{\\gamma_{0}-p_{0}}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Proof. The proof is a straightforward computation. The minimizer of $\\hat{\\mathcal{P}}(\\mathfrak{f},d)$ in $d$ must occur where the two terms in the maximum are equal, i.e., ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{C}_{0}\\left(\\frac{\\mathfrak{f}}{d}\\right)^{-\\gamma_{0}}d^{-p_{0}}=\\mathcal{C}_{1}\\big(\\frac{\\mathfrak{f}}{d}\\big)^{-\\gamma_{1}}d^{-p_{1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Solving for this $d$ gives $d^{\\star}$ . Plugging in the value of $d^{\\star}$ into $\\hat{\\mathcal{P}}(\\boldsymbol{\\mathfrak{f}},d)$ gives the optimal value. ", "page_idx": 33}, {"type": "text", "text": "Remark D.1. The possible minimal values of (73), i.e., where pairs of functions in the max are equal, can be reduced further. For instance, if $\\mathcal{F}_{a c}(r,d)$ exist for the phase, then for some $0<r_{0}<r_{1}<r_{2}$ ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{\\boldsymbol{\\mathcal{P}}}(\\boldsymbol{r},d)\\approx\\left\\{\\frac{\\mathcal{F}_{p p}(\\boldsymbol{r},d),\\qquad}{\\gamma B}\\right.}\\\\ {\\tilde{\\boldsymbol{\\mathcal{P}}}(\\boldsymbol{r},d)\\approx\\left\\{\\frac{1}{\\gamma B}\\mathcal{K}_{p p}(\\boldsymbol{r},d)\\quad}&{r_{0}<r\\le r_{1}}\\\\ {\\mathcal{F}_{a c}(\\boldsymbol{r},d),\\quad}&{r_{1}<r<r_{2}}\\\\ {\\boldsymbol{\\mathcal{F}}_{0}(\\boldsymbol{r},d),\\quad}&{r_{2}<r}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Thus, there are only a maximum of three points to check in order to find the optimal compute curve. ", "page_idx": 33}, {"type": "text", "text": "Remark D.2. In view of Lemma D.1, to find the optimal compute curves, we first find the potential curves (i.e., all the possible combinations of two functions in the loss curve are equal while still lying on the loss curve). Then the curve which has the smallest exponent on the flops, f, is the optimal compute curve. ", "page_idx": 33}, {"type": "text", "text": "D.1 Compute-optimal curves: Above the high-dimensional line (Phases Ia, II, III). ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "To ease notation, we introduce several constants that will be used only in this Section D.1: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathcal{F}_{p p}(r,d)\\asymp(\\gamma B r)^{-(1+\\beta/\\alpha)+1/(2\\alpha)},\\qquad\\mathcal{F}_{a c}(r,d)\\asymp d^{-1}(\\gamma B r)^{-1+1/(2\\alpha)},}\\\\ &{}&{\\frac{1}{\\gamma B}\\mathcal{K}_{p p}(r,d)\\asymp\\gamma\\times(\\gamma B r)^{-2+1/(2\\alpha)},\\quad\\mathrm{and}\\quad\\mathcal{F}_{0}(r,d)\\asymp d^{-2\\alpha+\\operatorname*{max}\\{0,1-2\\beta\\}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where the asymptotics only hold in specific regions of the space of $\\gamma B r$ . For additional details on the derivation of these asymptotics and the constraints on $\\gamma B r$ where asymptotics hold, see Section $\\mathrm{H}$ . ", "page_idx": 33}, {"type": "text", "text": "Remark D.3. The constants in the asymptotics are dimension independent and only depend on $\\alpha,\\beta$ . ", "page_idx": 33}, {"type": "text", "text": "The compute-optimal curves are summarized in Table 6. ", "page_idx": 33}, {"type": "text", "text": "D.1.1 Phase Ia ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "In this case, the approximate loss curve is given by ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{\\Phi}(\\frac{\\mathfrak{f}}{d},d)=\\operatorname*{max}\\{\\mathcal{F}_{p p}(\\frac{\\mathfrak{f}}{d},d),\\mathcal{F}_{0}(\\frac{\\mathfrak{f}}{d},d)\\}\\asymp\\operatorname*{max}\\{\\big(\\frac{\\mathfrak{f}}{d}\\big)^{-(1+\\beta/\\alpha)+1/(2\\alpha)},\\times d^{-2\\alpha+1-2\\beta}\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "With this, we give a description of the optimal compute curve. ", "page_idx": 34}, {"type": "text", "text": "Proposition D.1 (Phase Ia: Compute-optimal Curve). Suppose we are in Phase Ia, that is, $2\\beta<1$ and $2\\alpha>1$ . The compute-optimal curve using $\\Tilde{\\mathcal{P}}(\\frac{\\mathfrak{f}}{d},d)$ in (74) occurs when $\\begin{array}{r}{\\mathcal{F}_{p p}(\\frac{\\mathfrak{f}}{d},d)=\\mathcal{F}_{0}(\\frac{\\mathfrak{f}}{d},d)}\\end{array}$ . Precisely, the optimal $d^{\\star}$ which minimizes $\\Tilde{\\mathcal{P}}(\\frac{\\mathfrak{f}}{d},d)$ is ", "page_idx": 34}, {"type": "equation", "text": "$$\nd_{P h a s e\\,I a}^{\\star}\\asymp\\mathsf{f}^{1/(2\\alpha+1)},\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "and the compute-optimal curve is ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\tilde{\\mathcal{P}}_{P h a s e\\,I a}^{\\star}(\\mathfrak{f})\\asymp\\mathfrak{f}^{\\bigl(\\frac{1}{2\\alpha+1}-1\\bigr)(1+\\beta/\\alpha-1/(2\\alpha))}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Proof. We apply Lemma D.1 with ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{C}_{0}=1,\\quad\\gamma_{0}=1+\\beta/\\alpha-1/(2\\alpha),\\quad p_{0}=0}\\\\ {\\mathrm{and}\\quad\\mathcal{C}_{1}=1,\\quad\\gamma_{1}=0,\\quad p_{1}=2\\alpha-1+2\\beta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "D.1.2 Phase II ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "In this case, the approximate loss curve has three terms (Proposition C.4 with (63)) ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{\\Phi}(\\frac{\\mathfrak{f}}{d},d)=\\operatorname*{max}\\left\\{\\mathcal{F}_{p p}(\\frac{\\mathfrak{f}}{d},d),\\mathcal{F}_{a c}(\\frac{\\mathfrak{f}}{d},d),\\mathcal{F}_{0}(\\frac{\\mathfrak{f}}{d},d)\\right\\}}\\\\ &{\\qquad\\qquad\\asymp\\operatorname*{max}\\left\\{\\big(\\frac{\\mathfrak{f}}{d}\\big)^{-(1+\\beta/\\alpha)+1/(2\\alpha)},\\big(\\frac{\\mathfrak{f}}{d}\\big)^{-1+1/(2\\alpha)}\\times d^{-1},d^{-2\\alpha}\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Proposition D.2 (Phase II: Compute-optimal Curve). Suppose we are in Phase $I I_{:}$ , that is, $2\\beta>1$ , $2\\alpha>1$ , and $\\beta<\\alpha$ . The compute-optimal curve using $\\tilde{\\mathcal{P}}(\\mathfrak{\\k}_{d}^{\\mathfrak{f}},d)$ in (75) occurs when $\\begin{array}{r}{\\mathcal{F}_{p p}(\\frac{\\mathfrak{f}}{d},d)=}\\end{array}$ $\\begin{array}{r}{\\mathcal{F}_{a c}(\\frac{\\mathfrak{f}}{d},d)}\\end{array}$ . Precisely, the optimal $d^{\\star}$ which minimizes $\\tilde{\\mathcal{P}}(\\mathfrak{{f}},d)$ is ", "page_idx": 34}, {"type": "equation", "text": "$$\nd_{P h a s e\\,I I}^{\\star}\\asymp\\mathfrak{f}^{(\\beta/\\alpha)/(1+\\beta/\\alpha)},\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "and the compute-optimal curve is ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\tilde{\\mathcal{P}}_{P h a s e\\,H}^{\\star}(\\mathfrak{f})\\asymp\\mathfrak{f}^{-\\frac{2\\alpha+2\\beta-1}{2(\\alpha+\\beta)}}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Proof. Using the Remark D.2 after Lemma D.1 and Proposition C.4 with (63), we only need to check two intersections: $\\mathcal{F}_{p p}=\\mathcal{F}_{a c}$ and $\\mathcal{F}_{a c}=\\mathcal{F}_{0}$ . The curve which has the smallest (i.e., largest negative) exponent (i.e, steepest curve on a log-log plot) is the compute-optimal curve. ", "page_idx": 34}, {"type": "text", "text": "Case 1: Consider $\\begin{array}{r}{\\mathcal{F}_{p p}(\\frac{\\mathfrak{f}}{d},d)=\\mathcal{F}_{a c}(\\frac{\\mathfrak{f}}{d},d)}\\end{array}$ . We apply Lemma D.1 with ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathfrak{C}_{0}=1,\\quad\\gamma_{0}=1+\\beta/\\alpha-1/(2\\alpha),\\quad p_{0}=0}\\\\ {\\mathrm{and}\\quad\\mathfrak{C}_{1}=1,\\quad\\gamma_{1}=1-1/(2\\alpha),\\quad p_{1}=1}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "to get that the minimum is ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{d_{1}^{\\star}\\asymp\\mathfrak{f}^{(\\beta/\\alpha)/(1+\\beta/\\alpha)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "and the optimal value is ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\tilde{\\mathcal{P}}_{1}^{\\star}(\\mathfrak{f})\\asymp\\mathfrak{f}^{-\\frac{2\\alpha+2\\beta-1}{2(\\alpha+\\beta)}}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Case 2: Consider $\\begin{array}{r}{\\mathcal{F}_{a c}(\\frac{\\mathfrak{f}}{d},d)=\\mathcal{F}_{0}(\\frac{\\mathfrak{f}}{d},d)}\\end{array}$ . As before, we apply Lemma D.1 with ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathfrak{C}_{0}=1,}&{\\gamma_{0}=0,\\quad p_{0}=2\\alpha}\\\\ {\\mathrm{and}}&{\\mathfrak{C}_{1}=1,\\quad\\gamma_{1}=1-1/(2\\alpha),\\quad p_{1}=1}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "to get that the minimum is ", "page_idx": 35}, {"type": "equation", "text": "$$\nd_{2}^{\\star}\\asymp\\mathfrak{f}^{1/(2\\alpha+1)}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "and the optimal value is ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\tilde{\\mathcal{P}}_{2}^{\\star}(\\mathfrak{f})\\asymp\\mathfrak{f}^{-2\\alpha/(2\\alpha+1)}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "One can check that ", "page_idx": 35}, {"type": "equation", "text": "$$\n-\\frac{2\\alpha+2\\beta-1}{2(\\alpha+\\beta)}<\\frac{-2\\alpha}{2\\alpha+1},\\qquad\\mathrm{for\\,all\\,}2\\beta>1,2\\alpha>1,\\beta<\\alpha.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Therefore, Case 1 is the optimal overall. ", "page_idx": 35}, {"type": "text", "text": "D.1.3 Phase III ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "In this case, the approximate loss curve has three terms (Proposition C.5 with (66)) ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{\\Phi}(\\frac{\\mathfrak{f}}{d},d)=\\operatorname*{max}\\big\\{\\frac{1}{\\gamma B}\\mathcal{K}_{p p}(\\frac{\\mathfrak{f}}{d},d),\\mathcal{F}_{a c}(\\frac{\\mathfrak{f}}{d},d),\\mathcal{F}_{0}(\\frac{\\mathfrak{f}}{d},d)\\big\\}}\\\\ &{\\qquad\\qquad\\asymp\\{\\big(\\frac{\\mathfrak{f}}{d}\\big)^{-2+1/(2\\alpha)},\\big(\\frac{\\mathfrak{f}}{d}\\big)^{-1+1/(2\\alpha)}\\times d^{-1},d^{-2\\alpha}\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Proposition D.3 (Phase III: Compute-optimal Curve). Suppose we are in Phase III, that is, $2\\beta>1$ , $2\\alpha>1$ , and $\\beta>\\alpha$ . The compute-optimal curve using $\\tilde{\\mathcal{P}}(\\boldsymbol{\\frac{\\mathfrak{f}}{d}},d)$ in (76) occurs when $\\begin{array}{r l}{\\frac{1}{\\gamma B}\\mathcal{K}_{p p}(\\frac{\\mathfrak{f}}{d},d)=}&{{}}\\end{array}$ $\\textstyle\\mathcal{F}_{a c}(\\frac{\\mathfrak{f}}{d},d)$ . Precisely, the optimal $d^{\\star}$ which minimizes $\\tilde{\\mathcal{P}}(\\mathfrak{{f}},d)$ is ", "page_idx": 35}, {"type": "equation", "text": "$$\nd_{P h a s e\\,I I I}^{\\star}\\asymp\\mathsf{f}^{1/2},\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "and the compute-optimal curve is ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\tilde{\\Phi}_{P h a s e\\,I I I}^{\\star}(\\mathfrak{f})\\asymp\\mathfrak{f}^{(1-4\\alpha)/(4\\alpha)}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Proof. Using the Remark D.2 after Lemma D.1 and Proposition C.5 with (66), we only need to check two curves: $\\begin{array}{r}{\\frac{1}{\\gamma B}\\mathcal{K}_{p p}=\\mathcal{F}_{a c}}\\end{array}$ and $\\mathcal{F}_{a c}=\\mathcal{F}_{0}$ . The curve which has the smallest (i.e., largest negative) exponent (i.e, steepest curve on a log-log plot) is the compute-optimal curve. ", "page_idx": 35}, {"type": "text", "text": "Case 1: Consider $\\begin{array}{r}{\\mathcal{F}_{a c}(\\frac{\\mathfrak{f}}{d},d)=\\mathcal{F}_{0}(\\frac{\\mathfrak{f}}{d},d)}\\end{array}$ . We did this for Phase II in the proof of Proposition D.2. Thus, we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathfrak{C}_{0}=1,}&{\\gamma_{0}=0,\\quad p_{0}=2\\alpha}\\\\ {\\mathrm{and}}&{\\mathfrak{C}_{1}=1,\\quad\\gamma_{1}=1-1/(2\\alpha),\\quad p_{1}=1}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "to get that the minimum is ", "page_idx": 35}, {"type": "equation", "text": "$$\nd_{1}^{\\star}\\asymp\\mathfrak{f}^{1/(2\\alpha+1)}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "and the optimal value is ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\tilde{\\mathcal{P}}_{1}^{\\star}(\\mathfrak{f})\\asymp\\mathfrak{f}^{-2\\alpha/(2\\alpha+1)}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Case 2: Consider $\\begin{array}{r}{\\frac{1}{\\gamma B}\\mathcal{K}_{p p}(\\frac{\\mathfrak{f}}{d},d)=\\mathcal{F}_{a c}(\\frac{\\mathfrak{f}}{d},d)}\\end{array}$ . We apply Lemma D.1 with ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathfrak{C}_{0}=1,\\quad\\gamma_{0}=2-1/(2\\alpha),\\quad p_{0}=0\\qquad}\\\\ {\\mathrm{and}\\quad\\mathfrak{C}_{1}=1,\\quad\\gamma_{1}=1-1/(2\\alpha),\\quad p_{1}=1}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "to get that the minimum is ", "page_idx": 35}, {"type": "equation", "text": "$$\nd_{2}^{\\star}\\asymp\\mathfrak{f}^{1/2}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "and the optimal value is ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\tilde{\\mathcal{P}}_{2}^{\\star}(\\mathfrak{f})\\asymp\\mathfrak{f}^{(1-4\\alpha)/(4\\alpha)}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "One can check that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\frac{1-4\\alpha}{4\\alpha}<\\frac{-2\\alpha}{2\\alpha+1},\\qquad\\mathrm{for\\,all\\,}2\\beta>1,2\\alpha>1,\\beta>\\alpha.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Therefore, Case 2 is the optimal overall. ", "page_idx": 35}, {"type": "table", "img_path": "aVSxwicpAk/tmp/736a655941d3d5d43a6bc50377d0921f80bb26281ca0b5a4bca6ed5245fba14c.jpg", "table_caption": ["Table 7: Summary of the compute-optimal curves for $\\Tilde{\\mathcal{P}}(\\frac{\\mathfrak{f}}{d},d)$ for below the highdimensional line, $2\\alpha<1$ . This includes Phases IV, Ib, and Ic. "], "table_footnote": [], "page_idx": 36}, {"type": "text", "text": "D.2 Compute-optimal curves: Below the high-dimensional line (Phase IV, Ib, Ic), $2\\alpha<1$ ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "This section main distinction with above the high-dimensional line section is the dependency of the learning rate on $v$ . In deed, we have that $v/d\\to c\\in(0,\\infty)$ and the learning rate is chosen so that the kernel norm is constant, i.e., ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\gamma\\sim2(1-2\\alpha)\\|\\mathcal{K}\\|v^{2\\alpha-1}\\quad\\Rightarrow\\quad\\gamma\\stackrel{\\mathrm{def}}{=}\\tilde{\\gamma}\\times v^{2\\alpha-1},\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where $\\tilde{\\gamma}$ is the positive constant so that $\\gamma=\\tilde{\\gamma}\\times v^{2\\alpha-1}$ . Consequently, we also need to keep track of the learning rate in the various terms. ", "page_idx": 36}, {"type": "text", "text": "In this section, we assume that $v/d\\to r\\in(1,\\infty)$ as $v,d\\rightarrow\\infty$ . ", "page_idx": 36}, {"type": "text", "text": "We state for completeness the $d$ and $r$ dependency on the forcing and kernel function, including the learning rate $\\gamma$ . We note that these asymptotics only hold for a set of $\\gamma B r$ values which depend on the spectral properties of $K$ (see the propositions listed next to the terms for details). ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{I}_{p p}(r)\\asymp(\\gamma\\times r)^{-(1+\\beta/\\alpha)+1/(2\\alpha)}\\asymp\\Big(\\frac{v^{2}\\alpha-1}{d^{2}\\alpha^{-1}}\\Big)^{-(1+\\beta/\\alpha)+1/(2\\alpha)}\\,(d^{2\\alpha-1}\\times r)^{-(1+\\beta/\\alpha)+1/(2\\alpha)}}\\\\ &{\\qquad\\qquad\\qquad\\asymp(d^{2\\alpha-1}\\times r)^{-(1+\\beta/\\alpha)+1/(2\\alpha)},\\qquad\\mathrm{(see~Proposition~H.2)}}\\\\ &{\\frac{1}{\\sqrt{B}}\\mathcal{K}_{p p}(r)\\asymp\\gamma^{-1+1/(2\\alpha)}\\times r^{-2+1/(2\\alpha)}\\asymp\\Big(\\tilde{\\gamma}\\times\\frac{V^{2}\\alpha-1}{d^{2\\alpha-1}}\\Big)^{-1+1/(2\\alpha)}\\times d^{2-1/(2\\alpha)-2\\alpha}\\times r^{-2+1/(2\\alpha)}}\\\\ &{\\qquad\\qquad\\asymp d^{2-1/(2\\alpha)-2\\alpha}\\times r^{-2+1/(2\\alpha)},\\qquad\\mathrm{(see~Proposition~H.5)}}\\\\ &{\\qquad\\mathcal{I}_{0}(r)\\asymp(v^{1-1/(2\\alpha)}\\times d^{1/(2\\alpha)})^{-2\\alpha+\\operatorname*{max}\\{0,1-2\\beta\\}}}\\\\ &{\\qquad\\qquad\\times\\Big(\\frac{v^{1-1/(2\\alpha)}}{d^{1-1/(2\\alpha)}}\\Big)^{-2\\alpha+\\operatorname*{max}\\{0,1-2\\beta\\}}\\times d^{-2\\alpha+\\operatorname*{max}\\{0,1-2\\beta\\}}}\\\\ &{\\qquad\\qquad\\asymp d^{-2\\alpha+\\operatorname*{max}\\{0,1-2\\beta\\}}.\\qquad\\mathrm{(see~Proposition~H.3)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "D.2.1 Phase IV (a) and (b) ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "In these cases, the approximation loss curve is given by (Proposition C.6 with (68)) ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{\\mathcal{P}}(\\frac{\\mathfrak{f}}{d},d)\\asymp\\operatorname*{max}\\{\\mathcal{F}_{p p}(\\frac{\\mathfrak{f}}{d},d),\\frac{1}{\\gamma B}\\mathcal{K}_{p p}(\\frac{\\mathfrak{f}}{d},d),\\mathcal{F}_{0}(\\frac{\\mathfrak{f}}{d},d)\\}}\\\\ &{\\qquad\\qquad\\asymp\\operatorname*{max}\\big\\{\\big(\\frac{\\mathfrak{f}}{d}\\big)^{-(1+\\beta/\\alpha)+1/(2\\alpha)}\\times d^{(2\\alpha-1)(-(1+\\beta/\\alpha)+1/(2\\alpha))},}\\\\ &{\\qquad\\qquad\\qquad\\qquad d^{2-1/(2\\alpha)-2\\alpha}\\times\\big(\\frac{\\mathfrak{f}}{d}\\big)^{-2+1/(2\\alpha)},d^{-2\\alpha}\\big\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "As one crosses the $2\\alpha=1$ , line the $\\mathcal{F}_{a c}$ disappears and ${\\mathcal F}_{p p}$ emerges. Consequently, there leaves two possible corners where the compute-optimal value could occur at. When $\\alpha$ goes below $\\alpha=1/4$ , the $\\mathcal{K}_{p p}$ decreases. ", "page_idx": 36}, {"type": "text", "text": "The difference between IVa and IVb is simply where the compute-optimal occurs. In $\\mathrm{IVa}$ , the tradeoff occurs between $\\mathcal{K}_{p p}$ and ${\\mathcal{F}}_{0}$ , whereas in IVb, the tradeoff occurs at ${\\mathcal F}_{p p}$ and $\\mathcal{K}_{p p}$ . ", "page_idx": 37}, {"type": "text", "text": "We give the compute-optimal curve for Phase IVa. ", "page_idx": 37}, {"type": "text", "text": "Proposition D.4 (Phase IVa: Compute-optimal Curve). Suppose we are in Phase IVa, that is, 2\u03b2 > 1 and \u221a2\u22121 $\\frac{\\sqrt{2}-1}{\\sqrt{2}}\\ <\\ \\alpha\\ <\\ \\frac{1}{2}$ . The compute-optimal curve using $\\tilde{\\mathcal{P}}(\\boldsymbol{\\underline{{\\mathfrak{f}}}},d)$ in (77) occurs when $\\begin{array}{r}{d^{1-2\\alpha}\\mathcal{K}_{p p}(\\frac{\\mathfrak{f}}{d},d)=\\mathcal{F}_{0}(\\frac{\\mathfrak{f}}{d},d)}\\end{array}$ . Precisely, the optimal $d^{\\star}$ which minimizes $\\tilde{\\mathcal{P}}(\\boldsymbol{\\underline{{\\mathfrak{f}}}},d)$ is ", "page_idx": 37}, {"type": "equation", "text": "$$\nd_{P h a s e\\;I V a}^{\\star}\\asymp\\mathsf{f}^{1/2},\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "and the compute-optimal curve is ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\tilde{\\mathcal{P}}_{P h a s e\\,I V a}^{\\star}(\\mathfrak{f})\\asymp\\mathfrak{f}^{-\\alpha}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Proof. Using the Remark D.2 after Lemma D.1 and Proposition C.6 with (68), we only need to check two curves: $\\dot{\\mathcal{F}}_{p p}=d^{1-2\\alpha}\\times\\mathcal{K}_{p p}$ and $\\times d^{1-2\\alpha}\\times\\mathcal{K}_{p p}\\bar{=}\\mathcal{F}_{0}$ . The curve which has the smallest (i.e., largest negative) exponent (i.e, steepest curve on a log-log plot) is the compute-optimal curve. ", "page_idx": 37}, {"type": "text", "text": "Case 1: Consider $\\begin{array}{r}{\\mathcal{F}_{p p}(\\frac{\\mathfrak{f}}{d},d)=d^{1-2\\alpha}\\mathcal{K}_{p p}(\\frac{\\mathfrak{f}}{d},d)}\\end{array}$ . We apply Lemma D.1 with ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{C}_{0}=1,\\quad\\gamma_{0}=1+\\beta/\\alpha-1/(2\\alpha),\\quad p_{0}=(2\\alpha-1)(1+\\beta/\\alpha-1/(2\\alpha))}\\\\ &{\\qquad\\mathrm{and}\\quad\\mathfrak{C}_{1}=1,\\quad\\gamma_{1}=2-1/(2\\alpha),\\quad p_{1}=-2+1/(2\\alpha)+2\\alpha,}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "to get that the minimum is ", "page_idx": 37}, {"type": "equation", "text": "$$\nd_{1}^{\\star}\\asymp\\mathfrak{f}^{(\\alpha-\\beta)/(2\\alpha\\beta+\\alpha-2\\beta)}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "and the optimal value is ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{\\mathfrak{P}}_{1}^{\\star}(\\mathfrak{f})=c_{f}c_{p p}\\big(\\frac{c_{f}c_{p p}}{c_{k}\\tilde{c}_{p p}}\\big)^{(1-\\alpha)(2\\alpha+2\\beta-1)/(2\\alpha\\beta+\\alpha-2\\beta)}\\times\\mathfrak{f}^{(1-2\\alpha)(2\\alpha+2\\beta-1)/(2(2\\alpha\\beta+\\alpha-2\\beta))}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Case 2: Consider $\\begin{array}{r}{d^{1-2\\alpha}\\mathcal{K}_{p p}(\\frac{\\mathfrak{f}}{d},d)=\\mathcal{F}_{0}(\\frac{\\mathfrak{f}}{d},d)}\\end{array}$ . We apply Lemma D.1 with ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\mathfrak{C}_{0}=1,\\quad\\gamma_{0}=0,\\quad p_{0}=2\\alpha\\quad}\\\\ {\\mathrm{and}\\quad\\mathfrak{C}_{1}=1,\\quad\\gamma_{1}=2-1/(2\\alpha),\\quad p_{1}=-2+1/(2\\alpha)+2\\alpha,}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "to get that the minimum is ", "page_idx": 37}, {"type": "equation", "text": "$$\nd_{1}^{\\star}\\asymp\\mathfrak{f}^{1/2}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "and the optimal value is ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{\\mathcal{P}}_{1}^{\\star}(\\mathfrak{f})\\asymp\\mathfrak{f}^{-\\alpha}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "In this region of $\\alpha$ \u2019s and $\\beta$ \u2019s, Case 2 has a smaller exponent on $\\mathfrak{f}$ than in Case 1. ", "page_idx": 37}, {"type": "text", "text": "As for Phase IVb, we have the following. ", "page_idx": 37}, {"type": "text", "text": "Proposition D.5 (Phase I\u221aVb: Compute-optimal Curve). Suppose we are in Phase IVb, that is, $2\\beta\\,>\\,1$ and $\\begin{array}{r}{\\frac{1}{4}\\,<\\,\\alpha\\,<\\,\\frac{\\sqrt{2}-1}{\\sqrt{2}}}\\end{array}$ . The compute-optimal curve using $\\tilde{\\mathcal{P}}(\\boldsymbol{\\underline{{\\mathfrak{f}}}},d)$ in (77) occurs when $\\begin{array}{r}{c_{k}d^{1-2\\alpha}\\mathcal{K}_{p p}(\\frac{\\mathfrak{f}}{d},d)=c_{f}\\mathcal{F}_{p p}(\\frac{\\mathfrak{f}}{d},d)}\\end{array}$ . Precisely, the optimal $d^{\\star}$ which minimizes $\\Tilde{\\mathcal{P}}(\\underline{{\\mathfrak{f}}},d)$ is ", "page_idx": 37}, {"type": "equation", "text": "$$\nd_{P h a s e\\,I V b}^{\\star}\\asymp\\mathfrak{f}^{(\\alpha-\\beta)/(2\\alpha\\beta+\\alpha-2\\beta)},\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "and the compute-optimal curve is ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\tilde{\\mathcal{P}}_{P h a s e\\,I V b}^{\\star}(\\mathfrak{f})\\asymp\\mathfrak{f}^{(1-2\\alpha)(2\\alpha+2\\beta-1)/(2(2\\alpha\\beta+\\alpha-2\\beta))}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Proof. The computations are exactly the same as in Proposition D.4. For the $\\alpha$ \u2019s and $\\beta$ \u2019s in this region, we see that Case 1 has the smaller exponent on $\\mathfrak{f}$ than Case 2. \u53e3 ", "page_idx": 37}, {"type": "text", "text": "D.2.2 Phase Ib ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "In this case, the approximate loss curve is given by (Proposition C.7 with (70)) ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{\\Phi}(\\frac{\\ f}{d},d)\\asymp\\operatorname*{max}\\{\\mathcal{F}_{p p}(\\frac{\\ f}{d},d),\\mathcal{F}_{0}(\\frac{\\ f}{d},d)\\}}\\\\ &{\\qquad\\qquad\\asymp\\operatorname*{max}\\{\\binom{\\ f}{d}^{-(1+\\beta/\\alpha)+1/(2\\alpha)}\\times d^{(2\\alpha-1)(-(1+\\beta/\\alpha)+1/(2\\alpha))},d^{-2\\alpha+1-2\\beta}\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Note this is true for $\\alpha>1/4$ , but we expect this to hold without this extra assumption. ", "page_idx": 38}, {"type": "text", "text": "With this, we give a description of the optimal compute curve. ", "page_idx": 38}, {"type": "text", "text": "Proposition D.6 (Phase Ib: Compute-optimal Curve). Suppose we are in Phase Ib, that is, $2\\beta<1$ , $\\begin{array}{r}{\\frac{1}{4}\\ <\\ \\alpha\\ <\\ \\frac{1}{2}}\\end{array}$ , and $\\alpha\\neq\\beta>{\\frac{1}{2}}$ . The compute-optimal curve using $\\tilde{\\mathcal{P}}(\\mathfrak{{f}},d)$ in (78) occurs when $\\begin{array}{r}{\\mathcal{F}_{p p}(\\frac{\\mathfrak{f}}{d},d)=\\mathcal{F}_{0}(\\frac{\\mathfrak{f}}{d},d)}\\end{array}$ . Precisely, the optimal $d^{\\star}$ which minimizes $\\Tilde{\\mathcal{P}}(\\frac{\\mathfrak{f}}{d},d)$ is ", "page_idx": 38}, {"type": "equation", "text": "$$\nd_{P h a s e\\,I b}^{\\star}\\asymp\\mathfrak{f}^{\\frac{1}{2}},\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "and the compute-optimal curve is ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\tilde{\\mathcal{P}}_{P h a s e\\,I b}^{\\star}(\\mathfrak{f})\\asymp\\mathfrak{f}^{\\frac{1}{2}-\\alpha-\\beta}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Proof. We apply Lemma D.1 with ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathcal{\\ C}_{0}=1,\\quad\\gamma_{0}=1+\\beta/\\alpha-1/(2\\alpha),\\quad p_{0}=(2\\alpha-1)(1+\\beta/\\alpha-1/(2\\alpha))}&{}\\\\ {\\mathrm{and}\\quad\\mathcal{C}_{1}=1,\\quad\\gamma_{1}=0,\\quad p_{1}=2\\alpha-1+2\\beta.}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "We expect the conclusions of Prop. D.6 to hold for the $(\\alpha,\\beta)$ pairs where $2\\beta<1$ , $2\\alpha\\,<\\,1$ , and $2(\\alpha+\\overline{{\\beta}})>1$ . ", "page_idx": 38}, {"type": "text", "text": "D.2.3 Phase Ic ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "In this case, the approximate loss curve is given by (Proposition C.8 with (72)) ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{\\Phi}(\\frac{\\mathfrak{f}}{d},d)\\asymp\\operatorname*{max}\\{\\mathcal{F}_{p p}(\\frac{\\mathfrak{f}}{d},d),\\mathcal{F}_{0}(\\frac{\\mathfrak{f}}{d},d)\\}}\\\\ &{\\qquad\\qquad\\asymp\\operatorname*{max}\\{\\binom{\\frac{\\mathfrak{f}}{d}}{\\mathfrak{f}}^{-(1+\\beta/\\alpha)+1/(2\\alpha)}\\times d^{(2\\alpha-1)(-(1+\\beta/\\alpha)+1/(2\\alpha))},d^{-2\\alpha}\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Note again that this is speculative as we do not have the bounds for the kernel. However we do believe that this is correct. With this, we give a description of the compute-optimal curve. ", "page_idx": 38}, {"type": "text", "text": "Proposition D.7 (Phase Ic: Compute-optimal Curve). Suppose we are in Phase $I c$ , that is, $2\\beta>1$ and $\\begin{array}{r}{0<\\alpha<\\frac{1}{4}}\\end{array}$ and suppose (79) is true. The compute-optimal curve using $\\tilde{\\mathcal{P}}(\\mathfrak{\\k}_{d}^{\\mathfrak{f}},d)$ in (77) occurs when $\\begin{array}{r}{\\mathcal{F}_{p p}(\\frac{\\mathfrak{f}}{d},d)=\\mathcal{F}_{0}(\\frac{\\mathfrak{f}}{d},d)}\\end{array}$ . Precisely, the optimal $d^{\\star}$ which minimizes $\\tilde{\\mathcal{P}}(\\boldsymbol{\\underline{{\\mathfrak{f}}}},d)$ is ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r}{d_{P h a s e\\,I c}^{\\star}\\asymp\\mathfrak{f}^{\\frac{1-2(\\alpha+\\beta)}{2(\\alpha(2\\beta-3)-2\\beta+1)}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "and the compute-optimal curve is ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\tilde{\\Phi}_{P h a s e\\,I c}^{\\star}(\\mathfrak{f})\\asymp\\mathfrak{f}^{\\frac{\\alpha(2\\alpha+2\\beta-1)}{\\alpha(2\\beta-3)-2\\beta+1}}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Proof. We apply Lemma D.1 with ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathcal{\\ C}_{0}=1,}&{\\gamma_{0}=1+\\beta/\\alpha-1/(2\\alpha),\\quad p_{0}=(2\\alpha-1)(1+\\beta/\\alpha-1/(2\\alpha))}\\\\ &{\\qquad\\qquad\\mathrm{and}\\quad\\mathcal{C}_{1}=1,\\quad\\gamma_{1}=0,\\quad p_{1}=2\\alpha.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "image", "img_path": "aVSxwicpAk/tmp/b13fe1562e72b63f8698c2802de66713fa52d3b289f2971adaa40ea9beb47503.jpg", "img_caption": ["Figure 6: Spectra of empirical and theory weighted by $D^{1/2}\\hat{\\beta}$ . Empirical spectra (blue) averaged over 100 randomly generated matrices W \u2208Rv\u00d7d. Point mass at $z\\,=\\,0$ was manually removed. Theory (orange) computed using the resolvent formula (9) and solved with Newton method (10 iterations for each $z$ ; $z$ -values were spaced at $0.1d^{-2\\alpha}$ with an imaginary part at $d^{-2\\alpha}$ ). There is a continuous part that evolves into a pure point outliers. "], "img_footnote": [], "page_idx": 39}, {"type": "text", "text": "E Spectrum of $\\hat{K}$ : random matrix theory ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "In this section, we analyze the spectrum of $\\hat{K}=D^{1/2}W W^{T}D^{1/2}$ . For this, we use standard tools from random matrix theory to derive a fixed point equation for the Stieljes transform of $\\hat{K}$ . Indeed, by knowing the Stieljes transform of $\\hat{K}$ , one can recover the spectral properties. ", "page_idx": 39}, {"type": "text", "text": "In particular, we will need the spectra of $\\hat{K}$ decomposes into 3 parts: ", "page_idx": 39}, {"type": "text", "text": "1. Point mass at $z=0$ : There will be a point mass at $z=0$ of mass $v-d$ for trivial reasons since $v\\gg d$ .   \n2. Pure point outliers: There will be a set of outliers, the pure point spectra, which are at constant order and nearly equal to $j^{-2\\alpha}$ for $j=1,2,\\dots$ ,   \n3. Absolutely continuous part: The spectral bulk, the absolutely continuous part, which form a density on a shrinking window. ", "page_idx": 39}, {"type": "text", "text": "In fact, we will not need to give a complete picture about all the spectra. ", "page_idx": 39}, {"type": "text", "text": "E.1 Self-consistent approximation for $(\\hat{K}-z)^{-1}=(D^{1/2}W W^{T}D^{1/2}-z)^{-1}.$ ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "In this section, we state the deterministic equivalent for the random matrix $(\\hat{K}-z)^{-1}$ and give some properties of its \u201cself-consistent spectra.\u201d The starting point for this is the self-consistent equation ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\d_{m}(z)=\\frac{1}{1+\\frac1d\\sum_{d=1}^{V}\\frac{j^{-2\\alpha}}{j^{-2\\alpha}m(z)-z}}\\quad\\mathrm{where}\\quad(D^{1/2}W W^{T}D^{1/2}-z)_{j j}^{-1}\\longleftrightarrow\\frac1{j^{-2\\alpha}m(z)-z}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "The identification $\\longleftrightarrow$ can be made rigorous by showing that ", "page_idx": 39}, {"type": "equation", "text": "$$\n|\\mathrm{Tr}\\big(A(D^{1/2}W W^{T}D^{1/2}-z)^{-1}\\big)-\\mathrm{Tr}\\big(A\\,\\mathrm{Diag}(j^{-2\\alpha}m(z)-z:j)^{-1}\\big)|\\xrightarrow[d\\rightarrow\\infty]{\\mathrm{Pr}}0,\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "for deterministic sequences of test matices $\\{A\\}$ with bounded nuclear norm and generally with very high probability in $d$ . We note that we would need a more precise quantification of errors to be useful for establishing the scaling law for the actual random matrices. In Figure 6, we solve the theoretical spectrum by solving the fixed point equation for $m(z)$ using a Newton method on a grid of complex z. ", "page_idx": 39}, {"type": "text", "text": "The function $m$ can also be related to the trace of $(\\widehat{K}-z)^{-1}$ . From the definition of $m$ , we can derive the explicit representation theorem. ", "page_idx": 39}, {"type": "text", "text": "Lemma E.1. For any $v,d$ and $z\\in\\mathbb{C}$ with $\\mathrm{Im}(m(z))>0$ ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\sum_{j=1}^{v}\\frac{1}{j^{-2\\alpha}m(z)-z}=\\frac{-v+(1-m(z))d}{z}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Proof. Multiplying through by $z$ we should evaluate ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\sum_{j=1}^{v}{\\frac{z}{j^{-2\\alpha}m(z)-z}}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Adding and subtracting $j^{-2\\alpha}m(z)$ ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\sum_{j=1}^{v}\\frac{z}{j^{-2\\alpha}m(z)-z}=-v+m(z)\\sum_{j=1}^{v}\\frac{j^{-2\\alpha}}{j^{-2\\alpha}m(z)-z}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Using the definition of $m$ , ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\sum_{j=1}^{v}\\frac{j^{-2\\alpha}}{j^{-2\\alpha}m(z)-z}=d\\biggl(\\frac{1}{m(z)}-1\\biggr).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Substituting this in completes the proof. ", "page_idx": 40}, {"type": "text", "text": "While an explicit solution of $m$ is unavailable, we can derive many properties of $m$ , starting with: ", "page_idx": 40}, {"type": "text", "text": "Proposition E.1. Suppose $X$ is a real random variable compactly supported in $[0,\\infty)$ . For every $z\\in\\mathbf{\\bar{H}}=\\{z\\in\\mathbb{C}:\\operatorname{Im}z>0\\}$ , there is a unique solution of (80) satisfying $\\operatorname{Im}m(z)<0$ . Moreover, this solution is an analytic function, and it can be solved by iterating the fixed point map ", "page_idx": 40}, {"type": "equation", "text": "$$\n(m(z):z\\in\\mathbb{H})\\mapsto\\left({\\frac{1}{1+{\\frac{v}{d}}\\mathbb{E}\\left({\\frac{X}{X m(z)-z}}\\right)}}:z\\in\\mathbb{H}\\right)\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "initialized with $m\\equiv1$ . Furthermore, $i f$ we consider the equation for $z\\in\\mathbb{H}$ ", "page_idx": 40}, {"type": "equation", "text": "$$\nF(m;z)\\stackrel{d e f}{=}m+\\frac{v}{d}\\mathbb{E}\\left(\\frac{X m}{X m-z}\\right)=1,\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "then this is solved uniquely by $m(z)$ and moreover it is stable in that $\\partial_{m}F\\neq0$ in a neighborhood of the solution. ", "page_idx": 40}, {"type": "text", "text": "Proof. Let $G$ be the mapping ", "page_idx": 40}, {"type": "equation", "text": "$$\nG(m;z)\\stackrel{\\mathrm{def}}{=}\\frac{1}{1+\\frac{v}{d}\\operatorname{\\mathbb{E}}\\left(\\frac{X}{X m(z)-z}\\right)}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "For each fixed $z\\in\\mathbb{H}$ , this is a strict self map into the lower half-plane of $m$ . Self-maps are strict contractions in the hyperbolic metric (from the Schwarz-Pick lemma), and thus there is a unique solution of $m(z)=G(m(z);z)$ . ", "page_idx": 40}, {"type": "text", "text": "We now introduce $F$ according to the formula ", "page_idx": 40}, {"type": "equation", "text": "$$\nF(m;z)=m+\\frac{v}{d}\\mathbb{E}\\left(\\frac{X m}{X m-z}\\right),\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "so that $F(m;z)=m/G(m;z)$ . Introduce the stability operator ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\partial_{m}F=1-\\frac{v}{d}\\,\\mathbb{E}\\left(\\frac{X z}{(X m-z)^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Then we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\partial_{m}{\\cal F}=\\frac{G-m\\partial_{m}G}{G^{2}}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "By the Schwarz-Pick lemma (in the half-plane version), we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n|\\partial_{m}G|<\\frac{|\\operatorname{Im}G(m;z)|}{|\\operatorname{Im}m|},\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "and hence in some sufficiently small neighborhood of $G(m(z))=m(z)$ , we therefore have ", "page_idx": 40}, {"type": "equation", "text": "$$\n|\\partial_{m}G|<1.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Hence also, in a sufficiently small neighborhood of $m(z)$ ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\partial_{m}F=m\\frac{1-\\partial_{m}G}{G^{2}}+\\frac{G-m}{G^{2}}\\neq0.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "While this proposition does not state what happens on the real line, in regions of the line where $m$ has a finite, real-valued limit, it agrees with its reflection in the lower half-plane. Hence in any open subset of $\\mathbb{R}$ where $\\textstyle\\operatorname*{lim}_{\\eta\\to0}m(x+i\\eta)$ exists and is real, $m$ will be analytic. ", "page_idx": 41}, {"type": "text", "text": "From Proposition E.1, we can derive some explicit estimates on $m$ , which will be sufficient for deriving the estimates on the forcing and kernel functions. We summarize these properties in the following: ", "page_idx": 41}, {"type": "text", "text": "Proposition E.2. Let $X$ be any random variable with support in $(0,1]$ . Then the following hold: ", "page_idx": 41}, {"type": "text", "text": "1. Near 0: Suppose that $a<0$ is a real-valued solution of ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\frac{v}{d}\\mathbb{E}\\!\\left(\\frac{X a}{X a-1}\\right)=1.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Then m is analytic in a neighborhood of $z=0$ and $m(z)=a z+O(z^{2})$ . If furthermore if for some interval $[0,z_{0}]$ the equation ", "page_idx": 41}, {"type": "equation", "text": "$$\n-z a+\\frac{v}{d}\\mathbb{E}\\biggl(\\frac{X a}{X a-1}\\biggr)=1\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "is solvable for $a<0$ , then m is analytic in a complex neighborhood of the whole interval $[0,z_{0}]$ . ", "page_idx": 41}, {"type": "text", "text": "2. Far away: Let $\\varrho_{\\mathrm{0}}(z)$ be the distance of $z$ to $[0,1]$ . Suppose that $z$ is such that $16{\\frac{v}{d}}\\mathbb{E}\\,X\\leq$ $\\varrho_{0}(z)^{2}$ . Then for some absolute constant $C>0$ , ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\vert m(z)-1\\vert\\leq8\\frac{v}{d}\\frac{\\mathbb{E}X}{\\varrho_{0}(z)}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Moreover suppose that $\\begin{array}{r}{\\left|\\frac{v}{d}\\operatorname{\\mathbb{E}}\\!\\left(\\frac{X}{X-z}\\right)\\right|\\leq\\epsilon<\\frac{1}{4}}\\end{array}$ , and suppose that ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\delta\\stackrel{d e f}{=}\\operatorname*{sup}_{m:|m-1|\\leq2\\epsilon}\\frac{v}{d}\\mathbb{E}\\bigg(\\frac{X^{2}}{|X m-z|^{2}}\\bigg)\\leq\\frac{1}{4}\\quad a n d\\quad\\operatorname*{sup}_{m:|m-1|\\leq2\\epsilon}\\frac{v}{d}\\mathbb{E}\\bigg(\\frac{X|z|}{|X m-z|^{2}}\\bigg)\\leq\\frac{1}{4}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Then ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\left|m(z)-1+{\\frac{v}{d}}\\operatorname{\\mathbb{E}}\\!\\left({\\frac{X}{X-z}}\\right)\\right|\\leq2\\delta{\\bigg|}{\\frac{v}{d}}\\operatorname{\\mathbb{E}}\\!\\left({\\frac{X}{X-z}}\\right)\\!\\left|.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "We need a lemma on stability of solutions. ", "page_idx": 41}, {"type": "text", "text": "Lemma E.2. Suppose $F$ is an analytic map of $-\\mathbb{H}\\to\\mathbb{C},$ , Suppose there is a $c,\\delta>0$ and $m_{0}\\in-\\mathbb{H}$ so that for all $m\\in-\\mathbb{H}$ with $|m-m_{0}|\\leq\\delta$ and $\\delta<|\\operatorname{Im}m_{0}|$ , ", "page_idx": 41}, {"type": "equation", "text": "$$\n|\\partial_{m}F(m,z)|\\geq c>0\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "If $m_{0}$ is an approximate solution of $F(m)=1$ in that it satisfies ", "page_idx": 41}, {"type": "equation", "text": "$$\n|1-F(m_{0})|<c\\delta,\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "then there is an solution $m\\in-\\mathbb{H}$ with $F(m)=1\\,s o\\,|m-m_{0}|\\leq\\delta.$ ", "page_idx": 41}, {"type": "text", "text": "Proof. We introduce an ODE (which is the continuous limit of the damped Newton\u2019s method) ", "page_idx": 41}, {"type": "equation", "text": "$$\n{\\frac{\\mathrm{d}m_{t}}{\\mathrm{d}t}}=-{\\frac{1-F(m_{t})}{\\partial_{m}F(m_{t})}},\\quad{\\mathrm{where}}\\quad m_{t}|_{t=0}=m_{0}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Then we note that along this ODE ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}(1-F(m_{t}))}{\\mathrm{d}t}=\\partial_{m}F(m_{t})\\frac{\\mathrm{d}m_{t}}{\\mathrm{d}t}=-(1-F(m_{t},z)).\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Hence we have $(1-F(m_{t}))=(1-F(m_{0}))e^{-t}$ for however long the ODE exists. In particular, if we have that on some open set $U$ of admissible $m$ containing $m_{0}$ that ", "page_idx": 41}, {"type": "equation", "text": "$$\n|\\partial_{m}F(m)|\\geq c,\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "then provided the ODE does not exit $U$ , ", "page_idx": 42}, {"type": "equation", "text": "$$\n|m-m_{0}|=|m_{\\infty}-m_{0}|\\leq\\int_{0}^{\\infty}|(1-F(m_{0}))|e^{-t}/c=|(1-F(m_{0}))|/c.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Hence as there is a neighborhood of $m_{0}$ of size $\\delta$ on which $\\partial_{m}F(m)|\\geq c$ then as $\\lvert(1-F(m_{0},z))\\rvert<$ $c\\delta$ we have ", "page_idx": 42}, {"type": "equation", "text": "$$\n|m-m_{0}|\\leq|(1-F(m_{0},z))|/c=\\delta.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Proof of Proposition. Part 1, Near 0: For the component near 0, we consider a change of variables and look at $q(z)=m(z)/z$ which is therefore the unique solution of the fixed point equation: ", "page_idx": 42}, {"type": "equation", "text": "$$\nF(q(z),z):=z q(z)+\\frac{v}{d}\\,\\mathbb{E}\\!\\left(\\frac{X q(z)}{X q(z)-1}\\right)=1.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Then by hypothesis, we have a solution at $z=0$ given by $F(a,0)=1$ . We wish to continue this solution to a neighborhood of $(a,0)$ , and so it would suffice to know that the differential equation ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\partial_{q}F(q,z){\\frac{\\mathrm{d}q}{\\mathrm{d}z}}+\\partial_{z}F=0\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "has a solution in a neighborhood of the point. Solving for the derivative of $q$ , ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}q}{\\mathrm{d}z}=\\frac{-q}{\\partial_{q}F(q,z)},\\quad\\mathrm{where}\\quad\\partial_{q}F(q,z)=z-\\frac{v}{d}\\,\\mathbb{E}\\biggl(\\frac{X}{(X q(z)-1)^{2}}\\biggr).\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "We note that if we solve the equation along the real line, then $q$ stays real\u2013valued. Furthermore, for all $q$ with $q<0$ we have ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\partial_{q}{\\cal F}(q,z)=z-\\frac{v}{d}\\,\\mathbb{E}\\biggl(\\frac{X}{(X q(z)-1)^{2}}\\biggr).\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "By analyticity, we can extend this solution into a neighborhood in the upper half plane and on an interval of the real line where this solvable. Hence $m$ is analytic in this neighborhood and has boundary values given by $m(z)/z\\to a$ . ", "page_idx": 42}, {"type": "text", "text": "Part 2, Far away: For the other parts, we use that $m$ is the solution of ", "page_idx": 42}, {"type": "equation", "text": "$$\nF(m(z),z):=m(z)+\\frac{v}{d}\\mathbb{E}\\bigg(\\frac{X m(z)}{X m(z)-z}\\bigg)=1.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Hence we have ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}m}{\\mathrm{d}z}=\\frac{-\\partial_{z}F(m,z)}{\\partial_{m}F(m,z)}=\\frac{-\\frac{v}{d}\\,\\mathbb{E}\\bigg(\\frac{X m(z)}{(X m(z)-z)^{2}}\\bigg)}{1-\\frac{v}{d}\\,\\mathbb{E}\\bigg(\\frac{X z}{(X m(z)-z)^{2}}\\bigg)}.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Define a distance ", "page_idx": 42}, {"type": "equation", "text": "$$\nf\\varrho(z):=\\varrho_{\\delta}(z):=\\operatorname*{inf}\\{|X m-z|:|m-1|\\leq\\delta,X\\in[0,1]\\}.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Provided that $|m{-}1|\\leq\\delta$ and $\\begin{array}{r}{\\frac{v}{d}(\\mathbb{E}\\,X)|z|\\le\\varrho(z)^{2}/2}\\end{array}$ (which is trivially satisfied if $\\begin{array}{r}{\\frac{v}{d}(\\mathbb{E}\\,X)/\\varrho(z)\\leq\\frac{1}{2})}\\end{array}$ ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\left|\\frac{\\mathrm{d}m}{\\mathrm{d}z}\\right|\\leq2(1+\\delta)\\frac{v}{d}\\frac{\\mathbb{E}(X)}{\\varrho(z)^{2}}.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "For any $z$ with $\\varrho(z)>0$ , we can find a unit speed geodesic $\\sigma(t)$ connecting $z$ to $\\infty$ so that $\\varrho(\\sigma(t))=$ $\\varrho(z)+t$ (this will just be a straight line). Then along this geodesic, $\\begin{array}{r}{\\frac{v}{d}(\\mathrm{\\bar{E}}\\,X)|\\sigma(t)|\\,\\le\\,\\varrho(\\sigma(t))^{2}/2}\\end{array}$ , since ", "page_idx": 42}, {"type": "text", "text": "$\\begin{array}{r}{\\dot{\\iota}(\\mathbb{E}X)\\vert\\sigma(t)\\vert\\le\\frac{v}{d}(\\mathbb{E}X)(\\vert z\\vert+t)\\le\\frac{v}{d}(\\mathbb{E}X)(\\vert z\\vert+t\\vert z\\vert/\\varrho(z))\\le(\\varrho(z))^{2}(1+t/\\varrho(z))/2\\le(\\varrho(z)+t)^{2}/2.}\\end{array}$ Hence along the geodesic, and provided that $|m-1|\\leq\\delta$ , we conclude ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\left|\\frac{\\mathrm{d}m}{\\mathrm{d}z}(\\sigma(t))\\right|\\le2(1+\\delta)\\frac{v}{d}\\frac{\\mathbb{E}(X)}{(\\varrho(z)+t)^{2}}.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Integrating along this line segment from infinity, we conclude that provided the right hand side is less than $\\delta$ , ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\left|m(z)-1\\right|\\leq\\int_{0}^{\\infty}\\biggl|\\frac{\\mathrm{d}m}{\\mathrm{d}z}(\\sigma(t))\\biggr|\\,\\mathrm{d}t\\leq2(1+\\delta)\\frac{v}{d}\\frac{\\mathbb{E}\\,X}{\\varrho(z)}.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "For the second conclusion, we use Lemma E.2 with this $F$ (holding $z$ fixed). We let $m_{0}=1-$ $\\scriptstyle{\\frac{v}{d}}\\mathbb{E}\\left({\\frac{X}{(X-z)}}\\right)$ , and we consider an $2\\epsilon$ neighborhood of $1,\\mathcal{U}$ By assumption, on $\\boldsymbol{\\mathcal{U}}$ ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\partial_{m}{\\cal F}(m,z)=1-\\frac{v}{d}\\,\\mathbb{E}\\biggl(\\frac{X z}{(X m(z)-z)^{2}}\\biggr)\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "satisfies $\\begin{array}{r}{|\\partial_{m}F|\\geq\\frac{3}{4}}\\end{array}$ . We also have that ", "page_idx": 43}, {"type": "equation", "text": "$$\n|1-F(m_{0},z)|=\\left|\\frac{v}{d}\\mathbb{E}\\!\\left(\\frac{X^{2}(1-m_{0})}{(X m_{0}(z)-z)(X-z)}\\right)\\right|.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Hence applying Cauchy-Schwarz ", "page_idx": 43}, {"type": "equation", "text": "$$\n|1-F(m_{0},z)|\\leq|1-m_{0}|\\delta=\\epsilon\\delta.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "The conclusion now follows from Lemma E.2. ", "page_idx": 43}, {"type": "text", "text": "E.2 The region near 0 and the spectral bulk ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "We now bound the contribution of the region near 0. ", "page_idx": 43}, {"type": "text", "text": "Proposition E.3. The function $m(z)$ is analytic in a neighborhood of $z\\,=\\,0$ of radius $c(\\alpha)d^{-2\\alpha}$ for some $c(\\alpha)>0$ . Furthermore, $m$ is negative on $(0,\\bar{c}d^{-2\\alpha})$ , vanishes at 0, and has $\\vert m^{\\prime}(0)+$ $\\kappa(v/d)d^{2\\alpha}|\\le C d^{2\\alpha-1}$ for all $d$ sufficiently large where ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\kappa(v/d)\\ \\ \\ \\,s o l{\\nu}e s\\ \\ \\ \\ \\int_{0}^{v/d}\\frac{\\kappa\\,{\\mathrm{d}}x}{\\kappa+x^{2\\alpha}}=1.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Moreover, we introduce $f(z;V/d)$ where $f:\\mathbb{H}\\to-\\mathbb{H}$ which solves ", "page_idx": 43}, {"type": "equation", "text": "$$\nf(z;a)+\\int_{0}^{a}{\\frac{f(z;a)\\,\\mathrm{d}x}{f(z;a)-x^{2\\alpha}z}}=1.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "This extends analytically to the interval $[0,c)$ . Then $f$ and m are close in that for any compact subset $K\\subset(\\mathbb{C}\\setminus([c,\\infty]\\cup[-\\infty,0]))$ , we have ", "page_idx": 43}, {"type": "equation", "text": "$$\n|m(z d^{-2\\alpha})-f(z)|\\leq C(K)/d.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "We furthermore have that in the case $2\\beta<1$ ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\left|\\sum_{j=1}^{v}{\\frac{j^{-2\\alpha-2\\beta}}{j^{-2\\alpha}m(z d^{-2\\alpha})-z d^{-2\\alpha}}}-d^{1-2\\beta}\\int_{0}^{a}{\\frac{x^{-2\\beta}\\,\\mathrm{d}x}{f(z)-z x^{2\\alpha}}}\\right|\\leq C(K),\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "and in the case $2\\beta>1$ ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\left|\\sum_{j=1}^{v}\\frac{j^{-2\\alpha-2\\beta}}{j^{-2\\alpha}m(z d^{-2\\alpha})-z d^{-2\\alpha}}-\\frac{c_{\\beta}}{f(z)}\\right|\\leq C(K)d^{-\\operatorname*{min}\\{1,2\\alpha,2\\beta-1\\}}.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Proof. For the first part, we look to apply Proposition E.2 part 1. The equation we need to solve is ", "page_idx": 43}, {"type": "equation", "text": "$$\n-z a+\\frac{1}{d}\\sum_{j=1}^{v}\\frac{j^{-2\\alpha}a}{j^{-2\\alpha}a-1}=1,\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "for $a<0$ . We change variables by setting $a=-d^{2\\alpha}\\varkappa$ and $z=d^{-2\\alpha}{}_{3}$ , in terms of which ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\mathfrak{z}\\varkappa+\\frac{1}{d}\\sum_{j=1}^{v}\\frac{(j/d)^{-2\\alpha}\\varkappa}{(j/d)^{-2\\alpha}\\varkappa+1}=1.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "By monotonicity, for $\\varkappa$ positive, ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\int_{1/d}^{v/d}\\frac{\\varkappa\\,\\mathrm{d}x}{\\varkappa+x^{2\\alpha}}\\le\\frac{1}{d}\\sum_{j=1}^{v}\\frac{(j/d)^{-2\\alpha}\\varkappa}{(j/d)^{-2\\alpha}\\varkappa+1}\\le\\int_{0}^{v/d}\\frac{\\varkappa\\,\\mathrm{d}x}{\\varkappa+x^{2\\alpha}},\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "and moreover the lower bound is only less than the upper bound by at most $\\textstyle{\\frac{1}{d}}$ uniformly in $\\varkappa>0$ . In the case that $2\\alpha<1$ , we can bound for $\\varkappa\\in[0,1]$ ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\varkappa\\frac{(v/d)}{1+(v/d)^{2\\alpha}}\\leq\\int_{0}^{v/d}\\frac{\\varkappa\\,\\mathrm{d}x}{\\varkappa+x^{2\\alpha}}\\leq\\varkappa\\frac{(v/d)^{1-2\\alpha}}{1-2\\alpha}.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Hence there there is an interval $[0,c_{0}]$ (bounded solely in terms of $(v/d)$ ) on which (81) is solvable and is uniformly bounded away from 0 over all $d$ . Hence, the solution of $\\kappa$ of $\\begin{array}{r}{\\int_{0}^{v/d}\\frac{\\kappa\\,\\mathrm{d}x}{\\kappa+x^{2\\alpha}}\\,=\\,1}\\end{array}$ satisfies ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\frac{1}{d}\\sum_{j=1}^{v}\\frac{(j/d)^{-2\\alpha}\\kappa}{(j/d)^{-2\\alpha}\\kappa+1}\\in[1-\\frac{1}{d},1+\\frac{1}{d}].\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Following the same bounds on $\\int_{0}^{V/d}\\frac{\\mathrm{d}x}{\\kappa\\!+\\!x^{2\\alpha}}$ d\u03ba+dxx2\u03b1 shown above, we conclude that the true solution \u03ba of (81) with $_3=0$ satisfies $|\\varkappa-\\kappa|=O(1/d)$ . This concludes the proof when $2\\alpha<1$ . ", "page_idx": 44}, {"type": "text", "text": "In the case that $2\\alpha>1$ , ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\int_{0}^{v/d}\\frac{\\varkappa\\,\\mathrm{d}x}{\\varkappa+x^{2\\alpha}}\\le\\int_{0}^{\\infty}\\frac{\\varkappa\\,\\mathrm{d}x}{\\varkappa+x^{2\\alpha}}=\\varkappa^{1/(2\\alpha)}\\int_{0}^{\\infty}\\frac{\\mathrm{d}x}{1+x^{2\\alpha}}.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "On the other hand for $\\varkappa\\in[0,1]$ ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\int_{0}^{v/d}\\frac{\\varkappa\\,\\mathrm{d}x}{\\varkappa+x^{2\\alpha}}\\ge\\varkappa^{1/(2\\alpha)}\\int_{0}^{(v/d)\\varkappa^{-1/(2\\alpha)}}\\frac{\\mathrm{d}x}{1+x^{2\\alpha}}\\ge\\varkappa^{1/(2\\alpha)}\\int_{0}^{(v/d)}\\frac{\\mathrm{d}x}{1+x^{2\\alpha}}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "and hence once more there is an interval $[0,c_{0}]$ independent of $V/d$ on which this is solvable and moreover the conclusions now follow in the same way as in the case that $2\\alpha<1$ . ", "page_idx": 44}, {"type": "text", "text": "Convergence to f. The existence and uniqueness of $f$ follows from Proposition E.1, where we define ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\mathcal{F}(f;z)\\overset{\\mathrm{def}}{=}f+\\int_{0}^{a}\\frac{f\\,\\mathrm{d}x}{f-x^{2\\alpha}z}=1,\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "(making appropriate choices of $v/d$ and $X$ ). ", "page_idx": 44}, {"type": "text", "text": "We further have, from the previous part, that $f$ takes negative values on an the interval $(0,c)$ . In what follows we fix a compact set $K\\subset(\\mathbb{C}\\setminus([c,\\infty]\\cup[-\\infty,0]))$ . Further, we claim the stability operator ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\partial_{f}{\\mathcal{F}}=1-\\int_{0}^{a}{\\frac{x^{2\\alpha}z\\,\\mathrm{d}x}{(f(z;a)-x^{2\\alpha}z)^{2}}}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "is nonvanishing on $K$ in a neighborhood of $f$ . Off of the real line, this follows from Proposition E.1.   \nOn the real line, it follows from monotonicity of $\\mathcal{F}$ for $f<0$ . ", "page_idx": 44}, {"type": "text", "text": "Hence it follows that on $K$ , there is a constant $C(K)$ and a $\\delta_{0}>0$ so that if $z\\in K$ and $m$ satisfies ", "page_idx": 44}, {"type": "equation", "text": "$$\n|\\mathcal{F}(m;z)-1|<\\delta_{0},\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "then ", "page_idx": 44}, {"type": "equation", "text": "$$\n|m-f|\\leq C(K)|\\mathcal{F}(m;z)-1|.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Define ", "page_idx": 44}, {"type": "equation", "text": "$$\nS(m;z,\\beta)\\stackrel{\\mathrm{def}}{=}\\sum_{j=1}^{v}\\frac{j^{-2\\alpha-2\\beta}}{j^{-2\\alpha}m-z d^{-2\\alpha}}.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Then with $\\beta=0$ ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\frac{1}{d}S(m;z,0)=\\frac{1}{d}\\sum_{j=1}^{v}\\frac{j^{-2\\alpha}}{j^{-2\\alpha}m-z d^{-2\\alpha}}.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "We will define $a=v/d$ , and we will define $\\Delta$ as the separation ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\Delta\\ {\\stackrel{\\mathrm{def}}{=}}\\ \\operatorname*{min}\\left\\{\\operatorname*{min}\\{t\\in[0,(v/d)^{2\\alpha}]:|m-z t|\\},1\\right\\}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Then by bounding the errors in a trapezoid rule approximation ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\left|\\frac{1}{d}S(m;z,0)-\\int_{0}^{a}\\frac{\\mathrm{d}x}{m-z x^{2\\alpha}}\\right|\\stackrel{<}{\\sim}\\frac{1}{d\\Delta^{2}},\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "where we have used a bound on the $x$ derivative of the integrand ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\int_{0}^{a}{\\frac{|z|2\\alpha x^{2\\alpha-1}\\,\\mathrm{d}x}{|m-z x^{2\\alpha}|^{2}}}\\lesssim{\\frac{1}{\\Delta^{2}}}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "(which relies on $z$ being bounded away from 0 and on $z$ being bounded in modulus). Hence if $m(z d^{-2\\alpha})$ is the solution of ", "page_idx": 45}, {"type": "equation", "text": "$$\nm+\\frac{m}{d}S(m;z,0)=1,\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "then we have ", "page_idx": 45}, {"type": "equation", "text": "$$\n|\\mathcal{F}(m;z)-1|=\\left|m+\\int_{0}^{a}\\frac{m\\,\\mathrm{d}x}{m-z x^{2\\alpha}}-1\\right|\\leqslant\\frac{|m|d^{-1}}{\\Delta(m)},\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "provided $m$ is bounded. ", "page_idx": 45}, {"type": "text", "text": "To see that $m$ remains bounded, we let $\\partial_{m}F$ be the stability operator of the equation ", "page_idx": 45}, {"type": "equation", "text": "$$\n1=F(m;z)=m+\\frac{m}{d}S(m;z,0).\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Then once more ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\partial_{m}F=1+\\frac{1}{d}S(m;z,0)+\\frac{m}{d}\\partial_{m}S(m;z,0).\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Approximating the sum for $\\partial_{m}{S}$ ", "page_idx": 45}, {"type": "equation", "text": "$$\n|\\partial_{m}F(m;z)-\\partial_{m}\\mathcal{F}(m;z)|\\lesssim d^{-1}\\left(\\frac{1}{\\Delta^{2}}+\\frac{|m|}{\\Delta^{3}}\\right).\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Differentiating the fixed point equation, we have the differential equation for $m$ ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}m}{\\mathrm{d}z}=\\frac{m}{z}\\frac{1-\\partial_{m}F(m;z)}{\\partial_{m}F(m;z)}.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "As the same equation holds for $f$ and is non-degenerate in a neighborhood of $f$ (as the stability operator does not vanish in a neighborhood of the solution), we conclude that ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\left|\\frac{\\mathrm{d}m}{\\mathrm{d}(z d^{-2\\alpha})}\\right|=O(1)\\quad\\mathrm{and}\\quad|\\mathcal{F}(m;z)-1|\\lesssim\\frac{d^{-1}}{\\Delta(m)}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "uniformly on compact sets for all $d$ sufficiently large ", "page_idx": 45}, {"type": "text", "text": "Sum formula. Hence having approximated $f$ , we can turn to estimating $S(m;z,\\beta)$ . When $2\\beta<1$ , we may repeat the Riemann sum approximation argument. Specifically, we have ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\left|d^{2\\beta-1}S(m;z,\\beta)-\\int_{0}^{a}\\frac{x^{-2\\beta}\\,\\mathrm{d}x}{m-z x^{2\\alpha}}\\right|\\lesssim\\frac{1}{d\\Delta^{2}},\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "where to bound the errors, we now must estimate ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\,^{2\\beta-1}\\int_{1}^{v}\\left|\\frac{\\mathrm{d}}{\\mathrm{d}x}\\frac{x^{-2\\alpha-2\\beta}}{x^{-2\\alpha}m-z d^{-2\\alpha}}\\right|\\mathrm{d}x\\lesssim d^{2\\beta-1}\\int_{1}^{v}\\left(\\left|\\frac{x^{-2\\alpha-2\\beta-1}}{x^{-2\\alpha}m-z d^{-2\\alpha}}\\right|+\\left|\\frac{x^{-4\\alpha-2\\beta-1}m}{(x^{-2\\alpha}m-z d^{-2\\alpha})^{2}}\\right|\\right)\\mathrm{d}x\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Setting $x=w d$ , we arrive at ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\left|d^{2\\beta-1}S(m;z,\\beta)-\\int_{0}^{a}\\frac{x^{-2\\beta}\\,\\mathrm{d}x}{m-z x^{2\\alpha}}\\right|\\lesssim d^{-1}\\int_{(1/d)}^{a}\\left(\\left|\\frac{w^{-2\\beta-1}}{m-z w^{2\\alpha}}\\right|+\\left|\\frac{w^{-2\\beta-1}m}{(m-z w^{2\\alpha})^{2}}\\right|\\right)\\mathrm{d}x}}\\\\ &{}&{\\lesssim d^{2\\beta-1}\\left(\\frac{1}{\\Delta}+\\frac{\\left|m\\right|}{\\Delta^{2}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "We may subsequently replace in this expression $m$ by $f$ . ", "page_idx": 46}, {"type": "text", "text": "In the case that $2\\beta>1$ , we subtract from $S$ the divergence $c_{\\beta}/m$ and then express ", "page_idx": 46}, {"type": "equation", "text": "$$\nS(m;z,\\beta)-1/m\\sum_{j=1}^{v}j^{-2\\beta}\\sum_{j=1}^{v}\\left(\\frac{j^{-2\\alpha-2\\beta}}{j^{-2\\alpha}m-z d^{-2\\alpha}}-\\frac{j^{-2\\alpha-2\\beta}}{j^{-2\\alpha}m}\\right).\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Bounding the difference leads to ", "page_idx": 46}, {"type": "equation", "text": "$$\n|S(m;z,\\beta)-c_{\\beta}/m|\\lesssim c_{\\beta}d^{-2\\alpha}/\\Delta+d^{1-2\\beta}/|m|.\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Remark E.1. There is an exactly solvable case where even more can be said. Note that when $2\\alpha>1$ and $v/d=\\infty,$ , the equation for $f$ becomes ", "page_idx": 46}, {"type": "equation", "text": "$$\nf+\\int_{0}^{\\infty}{\\frac{f\\,\\mathrm{d}x}{f-x^{2\\alpha}z}}=1.\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Changing variables (which requires a contour deformation which restricts the branches considered) by letting $x^{2\\alpha}z=-f y^{2\\alpha}$ , so that $x=(-f/z)^{1/(2\\alpha)}y$ . Then ", "page_idx": 46}, {"type": "equation", "text": "$$\nf+(-f/z)^{1/(2\\alpha)}\\int_{0}^{\\infty}{\\frac{\\mathrm{d}x}{1+x^{2\\alpha}}}=1\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Hence with $\\begin{array}{r}{c_{\\alpha}=\\int_{0}^{\\infty}\\frac{\\mathrm{d}x}{1+x^{2\\alpha}}}\\end{array}$ , we have that $f$ is the solution of ", "page_idx": 46}, {"type": "equation", "text": "$$\nf+(-f/z)^{1/(2\\alpha)}c_{\\alpha}=1.\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "If for example $\\alpha=1$ , then with $g=(-f)^{1/2}$ we have $g$ satisfies the quadratic equation ", "page_idx": 46}, {"type": "equation", "text": "$$\n-g^{2}+g z^{-1/2}c_{1}=1,\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "or solving ", "page_idx": 46}, {"type": "equation", "text": "$$\ng=z^{-1/2}\\frac{c_{1}}{2}\\pm\\sqrt{c_{1}^{2}z^{-1}/4-1},\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "with \u00b1 chosen so that $\\operatorname{Im}g\\geq0$ and $\\mathrm{Re}\\,g>0$ . We note that $\\begin{array}{r}{c_{1}=\\frac{\\pi}{2}}\\end{array}$ and conclude that ", "page_idx": 46}, {"type": "equation", "text": "$$\nf=-{\\frac{1}{z}}\\left({\\frac{\\pi}{4}}\\pm\\sqrt{(\\pi/4)^{2}-z}\\right)^{2},\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "with the branch chosen to ensure $\\operatorname{Im}f<0$ when $\\operatorname{Im}z>0$ . ", "page_idx": 46}, {"type": "text", "text": "E.3 The mesoscopic region ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "We will need the following technical estimate on sums over lattice points. ", "page_idx": 46}, {"type": "text", "text": "Lemma E.3. Suppose that $z$ and $w$ are complex numbers and $-z/w\\notin\\mathbb{Z}$ ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\operatorname{pv}\\sum_{n}{\\frac{1}{w n+z}}=-{\\frac{\\pi}{w}}\\cot(\\pi z/w).\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Moreover, if we suppose $|\\operatorname{Im}(z/w)|\\geq|\\operatorname{Re}(z/w)|$ then there is an aboslute constant $C>0$ so that for any $N\\in\\mathbb{N}$ ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\left|\\mathrm{pv}\\sum_{n}{\\frac{1}{w n+z}}-\\sum_{n=-N}^{N}{\\frac{1}{w n+z}}\\right|\\leq{\\frac{C|z|}{|w|^{2}N}}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Proof. Note that we may remove a factor $\\frac{1}{w}$ from all statements and instead look at the case (with $y=-z/w)$ ) ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\operatorname{pv}\\sum_{n}{\\frac{1}{w n+z}}={\\frac{1}{w}}\\operatorname{pv}\\sum_{n}{\\frac{1}{n-y}}.\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Then by a residue computation (applied to the function $\\textstyle\\pi\\cot(\\pi z){\\frac{1}{z-y}})$ ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\frac{1}{w}\\,\\mathrm{pv}\\sum_{n}\\frac{1}{n-y}=\\frac{1}{w}\\pi\\cot(\\pi y)=-\\frac{\\pi}{w}\\cot(\\pi z/w),\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "where we have used that cot is odd. ", "page_idx": 47}, {"type": "text", "text": "Now by pairing terms, we have ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\left|\\operatorname{pv}\\sum_{n}{\\frac{1}{w n+z}}-\\sum_{n=-N}^{N}{\\frac{1}{w n+z}}\\right|\\leq\\sum_{n=N+1}^{\\infty}\\left|{\\frac{1}{w n+z}}+{\\frac{1}{-w n+z}}\\right|.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Making a common fraction, we have ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\left|\\mathrm{pv}\\sum_{n}\\frac{1}{w n+z}-\\sum_{n=-N}^{N}\\frac{1}{w n+z}\\right|\\leq\\displaystyle\\sum_{n=N+1}^{\\infty}\\Biggl|\\frac{2z}{-(w n)^{2}+z^{2}}\\Biggr|}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\leq\\displaystyle\\frac{|2z|}{|w|^{2}}\\sum_{n=N+1}^{\\infty}\\Biggl|\\frac{1}{-n^{2}+(z/w)^{2}}\\Biggr|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Now $\\mathrm{Re}(z/w)^{2}<0$ , and hence the claim follows. ", "page_idx": 47}, {"type": "text", "text": "Proposition E.4. Let $\\alpha,\\beta\\,\\geq\\,0$ with neither equal to $\\frac{1}{2}$ . We further assume $2\\alpha+\\beta\\neq{\\frac{1}{2}}$ . For $u,\\eta,a,b\\ge0$ consider with $m=a-i b$ ", "page_idx": 47}, {"type": "equation", "text": "$$\nA+i B=\\sum_{j=1}^{v}\\frac{j^{-2\\alpha-2\\beta}}{-u-i\\eta+j^{-2\\alpha}m}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "for real $A,B$ . We suppose that the $a,b,u,\\eta$ and $\\epsilon$ satisfy ", "page_idx": 47}, {"type": "text", "text": "Then there is an $\\epsilon_{0}>0$ , $c>0$ , and $C>0$ so that if $\\epsilon\\in(0,\\epsilon_{0})$ such that ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|B-\\frac{\\pi(u/a)^{\\beta/\\alpha-1/(2\\alpha)}}{2\\alpha a}-c_{\\beta}\\frac{b}{a^{2}}-o_{\\alpha+\\beta}\\frac{(\\eta+u b)v^{1-2\\alpha-2\\beta}}{u^{2}(1-2\\alpha-2\\beta)}\\right|}\\\\ &{\\quad\\le C\\left(\\epsilon u^{\\beta/\\alpha-1/(2\\alpha)}+c_{\\beta}(\\eta+b)u\\log(1/u)+\\epsilon o_{\\alpha+\\beta}\\frac{(\\eta+u b)v^{1-2\\alpha-2\\beta}}{u^{2}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "where $\\begin{array}{r}{c_{\\beta}=\\sum_{j=1}^{\\infty}j^{-2\\beta}}\\end{array}$ (if $\\beta>{\\textstyle{\\frac{1}{2}}},$ ) or $c_{\\beta}=0$ otherwise and where $o_{\\alpha+\\beta}$ is the indicator function of $\\alpha+\\beta<{\\frac{1}{2}}$ . Furthermore, let $\\boldsymbol{\\mathcal{A}}=\\boldsymbol{\\mathcal{A}}(\\boldsymbol{u}+i\\eta)$ be the same sum with $m\\to1$ . Then ", "page_idx": 47}, {"type": "equation", "text": "$$\n|A-A|\\leq C|1-a|\\left(\\frac{1}{\\epsilon}u^{\\beta/\\alpha-1/(2\\alpha)}+c_{\\beta}+o_{\\alpha+\\beta}\\epsilon\\frac{v^{1-2\\alpha-2\\beta}}{u}\\right),\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "and moreover ", "page_idx": 47}, {"type": "equation", "text": "$$\n|\\mathcal{A}|\\le C\\left(u^{\\beta/\\alpha-1/(2\\alpha)}+c_{\\beta}+o_{\\alpha+\\beta}\\frac{v^{1-2\\alpha-2\\beta}}{u}\\right).\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Proof. We look to estimate the expression ", "page_idx": 47}, {"type": "equation", "text": "$$\nA+i B=\\sum_{j=1}^{v}\\frac{j^{-2\\alpha-2\\beta}}{-u-i\\eta+j^{-2\\alpha}m},\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "on the regime considered, where $m=a-i b$ . The dominant contribution of the sum will either come from $j^{-2\\alpha}a\\approx u$ or possibly, when $2\\beta>1$ , from small $j$ . So the analysis will be done by separately considering windows around the transition window $j^{-2\\alpha}a\\approx u$ , and another analysis for large/small $j$ . We use the notation for $I\\subset\\{1,2,\\ldots,v\\}$ that $A_{I}$ and $B_{I}$ are the restrictions of this sum to the range of $j\\in I$ . ", "page_idx": 48}, {"type": "text", "text": "The transition window. We begin by setting $j_{0}$ to be the integer which minimizes $|j_{0}^{-2\\alpha}a-u|$ We can estimate this difference, noting that ", "page_idx": 48}, {"type": "equation", "text": "$$\n|j_{0}^{-2\\alpha}a-u|\\leq\\operatorname*{max}\\{|j_{0}^{-2\\alpha}-(j_{0}\\pm1)^{-2\\alpha}|a\\}\\leq C(\\alpha)j_{0}^{-2\\alpha-1}a\\leq C(\\alpha)^{\\prime}u^{1+1/(2\\alpha)}.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "We can estimate $j^{-2\\alpha}$ by Taylor approximation, giving ", "page_idx": 48}, {"type": "equation", "text": "$$\nj^{-2\\alpha}=j_{0}^{-2\\alpha}-2\\alpha(j-j_{0})j_{0}^{-2\\alpha-1}+O((j-j_{0})^{2}j_{0}^{-2\\alpha-2}).\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Now we divide $j$ according to whether ", "page_idx": 48}, {"type": "equation", "text": "$$\n(j_{0}^{-2\\alpha}a-j^{-2\\alpha}a)^{2}\\leq M(\\eta+j_{0}^{-2\\alpha}b)^{2}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "or if not, for a large $M=M(\\epsilon)\\asymp1/\\epsilon^{2}$ . Let $I$ the largest possible symmetric interval of $j$ around $j_{0}$ that satisfies the above display. ", "page_idx": 48}, {"type": "text", "text": "On this inter\u221aval, we would like to justify that the Taylor approximation holds. For this, we shall require that $\\sqrt{M}(\\eta+j_{0}^{-2\\alpha}b)j_{0}^{2\\alpha}\\le\\epsilon.$ . Note under this condition ", "page_idx": 48}, {"type": "equation", "text": "$$\n|j-j_{0}|/j_{0}+O\\big((j-j_{0})^{2}j_{0}^{-2}\\big)\\asymp|j_{0}^{-2\\alpha}-j^{-2\\alpha}|j_{0}^{2\\alpha}\\leq\\sqrt{M}\\big(\\eta+j_{0}^{-2\\alpha}b\\big)j_{0}^{2\\alpha}\\leq\\epsilon.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Thus the largest difference of $|j\\rrangle-j_{0}|$ on $I$ is bounded above, up to constants, by $\\sqrt{M}(\\eta+$ $u b)u^{-1-1/(2\\alpha)}$ . Hence the Taylor approximation is justified in that on $I$ ", "page_idx": 48}, {"type": "equation", "text": "$$\n|j^{-2\\alpha}-j_{0}^{-2\\alpha}|=(2\\alpha)|j-j_{0}|u^{1+1/(2\\alpha)}(1+O(\\epsilon)),\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "with the implied constants bounded in terms of $|1-a|$ and $c$ . It follows that for terms outside of $I$ , we have $(j_{0}^{-2\\alpha}a-j^{-2\\alpha}a)^{2}>c^{\\prime}M(\\eta+j_{0}^{-2\\alpha}b)^{2}$ for some absolute $c^{\\prime}$ (provided $c(\\alpha)$ was picked sufficiently small). ", "page_idx": 48}, {"type": "text", "text": "The contribution of $I$ terms now follows the same path as was done in the first case: ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\sum_{j\\in I}\\frac{j^{-2\\alpha-2\\beta}}{(a-i b)j^{-2\\alpha}-(u+i\\eta)}=\\sum_{j\\in I}\\frac{j^{-2\\beta}}{(a-i b)-(u+i\\eta)(j_{0}^{2\\alpha}+(j-j_{0})j_{0}^{2\\alpha-1}2\\alpha)}+\\xi_{1}.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "The error terms $\\xi_{1}$ are bounded by ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle|\\xi_{1}|\\lesssim j_{0}^{-2\\beta}\\sum_{j\\in I}\\frac{|u+i\\eta|j_{0}^{2\\alpha-2}|j-j_{0}|^{2}}{(b+j_{0}^{2\\alpha}\\eta)^{2}}\\lesssim\\frac{M^{3/2}u^{b/\\alpha+1/\\alpha}u^{-3-3/(2\\alpha)}(\\eta+u b)^{3}}{(b+j_{0}^{2\\alpha}\\eta)^{2}}}&{}\\\\ &{\\lesssim M^{3/2}(\\eta+u b)u^{\\beta/\\alpha-1-1/(2\\alpha)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "We then do a second replacement, freezing the $j^{-2\\beta}$ in the numerator, and so we need to estimate ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\xi_{2}:=\\sum_{j\\in I}\\frac{j^{-2\\beta}-j_{0}^{-2\\beta}}{(a-i b)-(u+i\\eta)(j_{0}^{2\\alpha}+(j-j_{0})j_{0}^{2\\alpha-1}2\\alpha)},\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "which we do simply by ", "page_idx": 48}, {"type": "equation", "text": "$$\n|\\xi_{2}|\\lesssim j_{0}^{-2\\beta-1}\\operatorname*{max}_{j\\in I}\\frac{|j-j_{0}|^{2}}{b+\\eta/u}\\lesssim M(\\eta+u b)u^{\\beta/\\alpha-1-1/(2\\alpha)}.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Thus with $M\\asymp1/\\epsilon^{2}$ and using the second assumption of the lemma, we get ", "page_idx": 48}, {"type": "equation", "text": "$$\n|\\xi_{1}|+|\\xi_{2}|\\le\\epsilon u^{\\beta/\\alpha-1/(2\\alpha)},\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "where we have expressed ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\sum_{\\in I}\\frac{j^{-2\\alpha-2\\beta}}{(a-i b)j^{-2\\alpha}-(u+i\\eta)}=\\sum_{j\\in I}\\frac{j_{0}^{-2\\beta}}{z+w(j-j_{0})}+\\xi_{1}+\\xi_{2}\\quad\\mathrm{where}\\quad\\left\\{\\vphantom{\\xi_{1}^{2}}z=a-i b-(u+i\\eta)j_{0}^{2\\alpha-1}2\\alpha\\right.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "The sum we can now evaluate using Lemma E.3. Note this makes $z$ nearly $-i(b+\\eta/u)$ and $w$ nearly $-u^{1/(2\\alpha)}(2\\alpha)$ , and hence $z/w$ is almost purely imaginary. Thus the error estimate in the Lemma applies and we have (using $|(\\eta+b u)u^{-1-1/(2\\alpha)}|\\gtrsim\\log(1/\\epsilon)$ and $\\eta<\\epsilon u$ ) ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\left|\\sum_{j\\in I}\\frac{j^{-2\\alpha-2\\beta}}{(a-i b)j^{-2\\alpha}-(u+i\\eta)}-\\frac{i\\pi(u/a)^{\\beta/\\alpha-1/(2\\alpha)}}{2\\alpha}\\right|\\le C\\epsilon u^{\\beta/\\alpha-1/(2\\alpha)}.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "The small $j$ regime, imaginary part. Recall the terms of small $j$ , which\u221a is to say those with $j$ less than those in $I$ , are denoted $S$ . For these terms, we have $j^{-2\\alpha}a-u\\geq c\\sqrt{M}(\\eta+u b)$ . For the real and imaginary parts of the sum we have ", "page_idx": 49}, {"type": "equation", "text": "$$\nA s+i B s=\\sum_{j\\in S}\\frac{j^{-2\\alpha-2\\beta}}{-u+j^{-2\\alpha}a-i(\\eta+j^{-2\\alpha}b)}=\\sum_{j\\in S}\\frac{j^{-2\\alpha-2\\beta}(-u+j^{-2\\alpha}a+i(\\eta+j^{-2\\alpha}b))}{(-u+j^{-2\\alpha}a)^{2}+(\\eta+j^{-2\\alpha}b)^{2}}.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "We shall focus on the imaginary part first. We introduce an approximation for this sum, coming from approximating the denominator by $j^{-4\\alpha}a^{2}$ . Thus we introduce ", "page_idx": 49}, {"type": "equation", "text": "$$\ni B_{S}^{\\prime}\\stackrel{\\mathrm{def}}{=}\\frac{1}{a^{2}}\\sum_{j\\in S}j^{2\\alpha-2\\beta}(i(\\eta+j^{-2\\alpha}b)).\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Let $c_{\\beta}$ be as in the statement of the Proposition. Then ", "page_idx": 49}, {"type": "equation", "text": "$$\n|i B_{S}^{\\prime}-c_{\\beta}(i b/a^{2})|\\lesssim j_{0}^{1+2\\alpha-2\\beta}|\\eta|+j_{0}^{1-2\\beta}(b)\\lesssim\\epsilon u^{\\beta/\\alpha-1/2\\alpha}.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "We turn to estimating the difference of $B_{S}-i B_{S}^{\\prime}$ . Using that $(j^{-2\\alpha}a)^{2}\\mathrm{~-~}(-u\\mathrm{~+~}j^{-2\\alpha}a)^{2}\\;\\leq$ $2u(j^{-2\\alpha}a)$ , we can estimate ", "page_idx": 49}, {"type": "equation", "text": "$$\n|B_{S}-B_{S}^{\\prime}|\\lesssim\\sum_{j\\in S}\\frac{u j^{-2\\beta}(\\eta+j^{-2\\alpha}b)}{(-u+j^{-2\\alpha}a)^{2}}.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "To estimate these differences, we break these sums into scales. We let $S_{k}$ to be those $j$ for which ", "page_idx": 49}, {"type": "equation", "text": "$$\nS_{k}=\\big\\{(\\eta+u b)2^{k-1}\\leq j^{-2\\alpha}a-u\\leq(\\eta+u b)2^{k}\\big\\}.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Then we can estimate the number of terms in each of these $k$ by ", "page_idx": 49}, {"type": "equation", "text": "$$\n|S_{k}|\\le C(\\alpha)(\\eta+u b)2^{k}(u+(\\eta+u b)2^{k})^{-1-1/(2\\alpha)}.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "For small $k$ , i.e. those for which $(\\eta\\!+\\!u b)2^{k}\\leq u$ , we can estimate $|S_{k}|\\le C(\\alpha)(\\eta\\!+\\!u b)2^{k}u^{-1-1/(2\\alpha)}$ Call the small $k$ terms $S^{\\prime}$ and the remainder $S^{\\prime\\prime}$ . Then for larger $S^{\\prime\\prime}$ terms, ", "page_idx": 49}, {"type": "equation", "text": "$$\n|S_{k}|\\leq C(\\alpha)((\\eta+u b)2^{k})^{-1/(2\\alpha)}.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "For the difference of the imaginary parts on small $k$ , we may bound $j^{-2\\beta}$ as a multiple of $j_{0}^{-2\\beta}$ \u22122\u03b2and so we arrive at ", "page_idx": 49}, {"type": "equation", "text": "$$\n|B_{S^{\\prime}}-B_{S^{\\prime}}^{\\prime}|\\lesssim(u^{\\beta/\\alpha+1})\\sum_{k}\\frac{|S_{k}|(\\eta+u b)}{2^{2k}(\\eta+u b)^{2}}\\lesssim(u^{\\beta/\\alpha-1/(2\\alpha)})\\sum_{k}\\frac{1}{2^{k}}\\lesssim\\epsilon u^{\\beta/\\alpha-1/(2\\alpha)}.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Then for the difference of the imaginary parts on large $k$ ", "page_idx": 49}, {"type": "equation", "text": "$$\n|B_{S^{\\prime\\prime}}-B_{S^{\\prime\\prime}}^{\\prime}|\\lesssim\\sum_{k}u\\frac{|S_{k}|(\\eta((\\eta+u b)2^{k})^{\\beta/\\alpha}+b((\\eta+u b)2^{k})^{\\beta/\\alpha+1})}{2^{2k}(\\eta+u b)^{2}}.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "This we further estimate ", "page_idx": 49}, {"type": "equation", "text": "$$\n|B_{S^{\\prime\\prime}}-B_{S^{\\prime\\prime}}^{\\prime}|\\lesssim u\\sum_{k}\\eta((\\eta+u b)2^{k})^{\\beta/\\alpha-2-1/(2\\alpha)}+b((\\eta+u b)2^{k})^{\\beta/\\alpha-1-1/(2\\alpha)}.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "In the event that the exponents are non-negative, which can only occur when $2\\beta>1$ , we may lose a factor which is boundable by the largest $k$ term (which is constant order) or by a logarithm in the case the exponent is 0. If either exponent is negative, the expression is dominated by its smallest $k$ term, for which $(\\eta+u b)2^{k}\\asymp u$ . In all we have ", "page_idx": 49}, {"type": "equation", "text": "$$\n|B_{S}-B_{S}^{\\prime}|\\lesssim\\epsilon u^{\\beta/\\alpha-1/(2\\alpha)}+(\\eta+u b)u^{\\beta/\\alpha-1-1/(2\\alpha)}+c_{\\beta}(\\eta+b)u\\log(1/u).\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "The large $j$ regime, imaginary part. We break the sum into two parts $L^{\\prime}$ and $L^{\\prime\\prime}$ , those with $j<1.1j_{0}$ and those with $j\\geq1.1j_{0}$ . For the terms in $L^{\\prime}$ we again break into scales, much like in the small $j$ regime. We let $L_{k}$ to be those $j$ for which ", "page_idx": 50}, {"type": "equation", "text": "$$\nL_{k}=\\big\\{(\\eta+u b)2^{k-1}\\leq u-j^{-2\\alpha}a\\leq(\\eta+u b)2^{k}\\big\\}.\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Then we can estimate the number of terms in each of these $k$ by ", "page_idx": 50}, {"type": "equation", "text": "$$\n|L_{k}|\\leq C(\\alpha)(\\eta+u b)2^{k}(u)^{-1-1/(2\\alpha)}.\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Then for the imaginary part ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle|B_{L^{\\prime}}|\\lesssim\\sum_{k}\\frac{u^{\\beta/\\alpha+1}|L_{k}|(\\eta+u b)}{2^{2k}(\\eta+u b)^{2}}}}\\\\ {{\\displaystyle\\lesssim\\sum_{k}\\frac{u^{\\beta/\\alpha-1/(2\\alpha)}}{2^{k}}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "This sum is always dominated by the smallest $k$ , and so we have ", "page_idx": 50}, {"type": "equation", "text": "$$\n|B_{L^{\\prime}}|\\lesssim\\epsilon u^{\\beta/\\alpha-1/(2\\alpha)}.\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "As for larger $j$ , we first remove a potentially divergent term, and so define ", "page_idx": 50}, {"type": "equation", "text": "$$\nB_{L^{\\prime\\prime}}^{\\prime}=\\sum_{j\\in L^{\\prime\\prime}}\\frac{j^{-2\\alpha-2\\beta}(\\eta+u b)}{u^{2}}.\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "In the case that $\\alpha+\\beta<1/2$ , we have that (comparing to an integral and using monotonicity) ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\quad}&{|B_{L^{\\prime\\prime}}^{\\prime}-\\frac{(\\eta+u b)V^{1-2\\alpha-2\\beta}}{u^{2}(1-2\\alpha-2\\beta)}|\\lesssim(\\eta+u b)\\bigg(\\frac{j_{0}^{-2\\alpha-2\\beta}}{u^{2}}+\\frac{j_{0}^{1-2\\alpha-2\\beta}}{u^{2}}\\bigg)\\lesssim(\\eta+u b)u^{-1+\\beta/\\alpha-1/(2\\alpha)}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\lesssim\\epsilon u^{\\beta/\\alpha-1/(2\\alpha)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Otherwise, ", "page_idx": 50}, {"type": "equation", "text": "$$\n|B_{L^{\\prime\\prime}}^{\\prime}|\\lesssim(\\eta+u b)u^{-1+\\beta/\\alpha-1/(2\\alpha)}\\lesssim\\epsilon u^{\\beta/\\alpha-1/(2\\alpha)}.\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "As for comparing the this divergence with the sum, we have ", "page_idx": 50}, {"type": "equation", "text": "$$\n|B_{L^{\\prime\\prime}}-B_{L^{\\prime\\prime}}^{\\prime}|\\lesssim\\sum_{j\\in L^{\\prime\\prime}}\\frac{j^{-2\\alpha-2\\beta}(j^{-2\\alpha}(\\eta/u+b))}{(-u+j^{-2\\alpha}a)^{2}}\\lesssim\\sum_{j\\in L^{\\prime\\prime}}\\frac{j^{-4\\alpha-2\\beta}((\\eta+u b))}{u^{3}}.\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Then if $2\\alpha+\\beta>1/2$ , this leaves ", "page_idx": 50}, {"type": "equation", "text": "$$\n|B_{L^{\\prime\\prime}}|\\lesssim(\\eta+u b)u^{\\beta/\\alpha-1-1/(2\\alpha)}\\lesssim\\epsilon u^{\\beta/\\alpha-1/(2\\alpha)}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "or in the case $2\\alpha+\\beta<1/2$ ", "page_idx": 50}, {"type": "equation", "text": "$$\n|B_{L^{\\prime\\prime}}|\\lesssim(\\eta+u b)u^{-3}v^{1-4\\alpha-2\\beta}\\lesssim(\\eta+u b)\\bigl(v^{-2\\alpha}/u\\bigr)u^{-2}v^{1-2\\alpha-2\\beta}.\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "The real part. For the real part, we shall prove a comparison with ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\mathcal{A}=\\mathrm{Re}\\left(\\sum_{j}\\frac{j^{-2\\alpha-2\\beta}}{j^{-2\\alpha}-u-i\\eta}\\right),\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "which we note is a special case of $A$ with $a-i b=1$ . The arguments are now very similar in all regimes to the imaginary parts, and so we just give a summary of the arguments. ", "page_idx": 50}, {"type": "text", "text": "The main difference is for $j\\approx j_{0}$ . Note\u221a that using the previous bounds on the transition window, we may discard an interval of $\\left|j-j_{0}\\right|\\leq\\sqrt{M}\\eta j_{0}^{1+2\\alpha}$ from $\\mathcal{A}$ and incur an error of only $\\epsilon u^{\\beta/\\alpha-1/(2\\alpha)}$ On a larger interval, , given by those $j$ with ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\eta j_{0}^{1+2\\alpha}\\leq|j-j_{0}|\\leq\\epsilon j_{0},\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "by pairing $j_{0}+r$ with $j_{0}-r$ , we can bound ", "page_idx": 50}, {"type": "equation", "text": "$$\n|\\mathcal{A}_{J}|+|A_{J}|\\lesssim\\epsilon j_{0}^{1-2\\beta}\\lesssim\\epsilon u^{\\beta/\\alpha-1/(2\\alpha)}.\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Moreover, the difference we can bound by ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\left|\\mathcal{A}_{J}-A_{J}\\right|\\lesssim|1-a|\\sum_{j\\in J}\\frac{j^{-4\\alpha-2\\beta}}{|(j^{-2\\alpha}a-u)(j^{-2\\alpha}-u)|}\\lesssim\\frac{|1-a|}{\\epsilon}u^{\\beta/\\alpha-1/(2\\alpha)}.\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "For small $j$ , where we redefine $S$ as those $j$ smaller than those in $J$ , we further divide to hose $j$ with $|j-j_{0}|\\le j_{0}/2$ and those $S^{\\prime}$ which are further from $j_{0}$ . ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\left|\\mathcal{A}_{S}-A_{S}\\right|\\lesssim\\frac{\\left|1-a\\right|}{\\epsilon}j_{0}^{1-2\\beta}+\\left|1-a\\right|\\sum_{j\\in S^{\\prime}}j^{-2\\beta}.\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Hence we arrive at ", "page_idx": 51}, {"type": "equation", "text": "$$\n|\\mathcal{A}_{S}-A_{S}|\\lesssim\\epsilon u^{\\beta/\\alpha-1/(2\\alpha)}+c_{\\beta}|1-a|.\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "For the large $j$ terms, we redefine $L$ as those $j$ larger than those in $J$ . Again dividing to those with $|j-j_{0}|\\leq\\bar{j}_{0}\\bar{/}2$ and otherwise, we arrive at ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\left|{\\cal A}_{{\\cal L}}-{\\cal A}_{{\\cal L}}\\right|\\lesssim\\frac{\\left|1-a\\right|}{\\epsilon}j_{0}^{1-2\\beta}+\\left|1-a\\right|\\sum_{j\\in{\\cal L}^{\\prime}}\\frac{j^{-4\\beta-2\\beta}}{u^{2}}.\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "This, as in the large terms for the imaginary part, leads to ", "page_idx": 51}, {"type": "equation", "text": "$$\n|\\mathcal{A}_{L}-A_{L}|\\lesssim\\frac{|1-a|}{\\epsilon}j_{0}^{1-2\\beta}+|1-a|\\epsilon u^{\\beta/\\alpha-1/(2\\alpha)}+|1-a|o_{\\alpha+\\beta}\\epsilon\\frac{v^{1-2\\alpha-2\\beta}}{u}.\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Finally, we observe that $\\mathcal{A}$ satisfies an estimate of the form ", "page_idx": 51}, {"type": "equation", "text": "$$\n|\\mathcal{A}|\\lesssim u^{\\beta/\\alpha-1/(2\\alpha)}+c_{\\beta}+o_{\\alpha+\\beta}\\frac{v^{1-2\\alpha-2\\beta}}{u},\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "which arise from the transitionary region, the small $j$ region and the large $j$ region. ", "page_idx": 51}, {"type": "text", "text": "Proposition E.5. Assume $\\alpha\\neq\\textstyle{\\frac{1}{4}}$ and $\\alpha\\neq\\textstyle{\\frac{1}{2}}$ . With $z=u+i\\eta(u)$ , with ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\eta=(\\log(1/\\epsilon)/c)\\operatorname*{max}\\left\\{u^{1+1/(2\\alpha)},{\\frac{\\pi}{2\\alpha}}{\\frac{u^{1-1/(2\\alpha)}}{d}}\\right\\},\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "there is a $c>0$ and an $\\epsilon_{0}$ so that for all $\\epsilon\\,\\in\\,(0,\\epsilon_{0})$ there is a $c_{\\epsilon}>0$ so for all $u\\in[d^{-2\\alpha}/c_{\\epsilon},c_{\\epsilon}]$ (with $\\boldsymbol{\\mathcal{A}}$ as in Proposition $E.$ 4) ", "page_idx": 51}, {"type": "equation", "text": "$$\n|m(z(u))-1+d^{-1}A(u+i\\eta)+i\\frac{\\pi}{2\\alpha}u^{-1/(2\\alpha)}d^{-1}|\\le C(\\alpha)\\epsilon u^{-1/(2\\alpha)}d^{-1}.\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Proof. We claim that $m$ is approximately equal to ", "page_idx": 51}, {"type": "equation", "text": "$$\nm_{0}\\ {\\stackrel{\\mathrm{def}}{=}}\\ 1-{\\frac{{\\mathcal{A}}(u+i\\eta)}{d}}-i{\\frac{\\pi u^{-1/(2\\alpha)}}{2\\alpha d}},\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "where $m$ is the solution of ", "page_idx": 51}, {"type": "equation", "text": "$$\nF(m(z),z):=m(z)+\\frac{v}{d}\\mathbb{E}\\bigg(\\frac{X m(z)}{X m(z)-z}\\bigg)=1,\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "with $\\operatorname{Im}m<0$ . Hence the result boils down to checking: ", "page_idx": 51}, {"type": "equation", "text": "$$\n|F(m_{0},z)-1|\\leq C\\epsilon\\frac{u^{-1/(2\\alpha)}}{d}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "and secondly that ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|1-\\partial_{m}(F)|\\leq\\frac{1}{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "in a neighborhood of $m_{0}$ , using Lemma E.2. ", "page_idx": 51}, {"type": "text", "text": "For showing that $|F(m_{0},z)-1|$ we first observe that on the contour selected, if $\\alpha<\\frac{1}{2}$ and $\\epsilon_{0},c_{\\epsilon}$ is chosen sufficiently small ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\frac{v^{1-2\\alpha}}{u^{2}d}(\\eta+u b)\\leq\\epsilon\\frac{\\pi u^{-1/(2\\alpha)}}{d}.\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Moreover the claimed estimates on $1-F$ now follow directly from Proposition E.4. For the stability, we have that ", "page_idx": 52}, {"type": "equation", "text": "$$\n1-\\partial_{m}{\\cal F}=\\frac{1}{d}\\sum_{j=1}^{v}\\frac{j^{-2\\alpha}z}{(j^{-2\\alpha}m-z)^{2}}.\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Taking modulus, we have ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\vert1-\\partial_{m}F\\vert\\le\\frac{1}{d}\\sum_{j=1}^{v}\\frac{j^{-2\\alpha}(u+\\eta)}{(j^{-2\\alpha}a-u)^{2}+(j^{-2\\alpha}b+\\eta)^{2}}=:X\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Now we break the estimation of the sum into regions of $j$ , as in the proof of Proposition E.4. We let $j_{0}$ be the integer which minimizes $(j_{0}^{-2\\alpha}a-u)^{2}$ . We define $S,L,I,J$ to be the sets where ", "page_idx": 52}, {"type": "equation", "text": "$$\nj<\\delta j_{0},\\quad j>j_{0}/\\delta,\\quad(j^{-2\\alpha}a-u)^{2}\\leq(u b+\\eta)^{2},\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "and the rest in $J$ , we let $X_{A}$ be the restriction of the sum $X$ to the set of indices $A$ . For the terms in $S$ , ", "page_idx": 52}, {"type": "equation", "text": "$$\nX_{S}\\lesssim\\frac{1}{d}\\sum_{j\\in S}\\frac{j^{2\\alpha}(u+\\eta)}{a^{2}(1-O(\\delta))}\\lesssim\\frac{u^{-1/2\\alpha}}{d}\\delta^{1+2\\alpha},\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "with the final sum holding for all $\\delta>0$ sufficiently small. For the terms in $L$ , ", "page_idx": 52}, {"type": "equation", "text": "$$\nX_{L}\\lesssim\\frac{1}{d}\\sum_{j\\in L}\\frac{j^{-2\\alpha}(u+\\eta)}{u^{2}(1-O(\\delta))}\\lesssim\\frac{u^{-1/2\\alpha}}{d}\\delta^{1+2\\alpha}+o_{\\alpha}\\frac{v^{1-2\\alpha}}{d}\\frac{u}{u^{2}+\\eta^{2}},\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "where $O_{\\alpha}$ is the indicator of $2\\alpha<1$ . For the terms in $I$ we have ", "page_idx": 52}, {"type": "equation", "text": "$$\nX_{I}\\lesssim\\frac{1}{d}\\sum_{j\\in I}\\frac{j^{-2\\alpha}(u+\\eta)}{(u b+\\eta)^{2}}\\lesssim\\frac{1}{d}\\frac{u^{-1/(2\\alpha)}(u+\\eta)}{(u b+\\eta)},\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "where we have used that the number of terms in this regions is on order of $j_{0}^{2\\alpha+1}(u b+\\eta)$ . Now taking $\\eta$ a sufficiently large multiple of $u\\beta$ , we conclude that the terms in $X_{I}\\leq\\frac{1}{8}$ . For the terms in $J$ ", "page_idx": 52}, {"type": "equation", "text": "$$\nX_{J}\\lesssim\\frac{1}{d}\\sum_{j\\in J}\\frac{j^{-2\\alpha}(u+\\eta)}{(j^{-2\\alpha}a-u)^{2}}\\lesssim C(\\delta)\\frac{1}{d}\\sum_{r}\\frac{j_{0}^{1}(u+\\eta)}{j_{0}^{-2\\alpha-1}(r)^{2}};\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "here the range of $r$ is such that at its smallest value $j_{0}^{-2\\alpha-1}(r)\\asymp(u b+\\eta)$ , and so we arrive at ", "page_idx": 52}, {"type": "equation", "text": "$$\nX_{J}\\leq C(\\delta)\\frac{1}{d}\\frac{u^{-1/(2\\alpha)}(u+\\eta)}{(u b+\\eta)}.\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Hence picking $\\delta$ sufficiently small that $X_{S},X_{L}$ are both less than $\\frac18$ , and subsequently increasing the lower bound on $\\eta/(u b)$ sufficiently far, we conclude that all four components can be made less than $\\frac{1}{8}$ and hence that ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|1-\\partial_{m}F|\\leq\\frac{1}{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "E.4 The large $z$ region ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Proposition E.6. For any compact set $U\\subset\\mathbb{C}$ of distance at least $\\delta>0$ from $[0,1]$ and any $\\alpha\\neq1/2$ there is a $C(\\alpha)$ such that ", "page_idx": 52}, {"type": "equation", "text": "$$\n|m(z)-1|\\leq{\\frac{C(\\alpha)}{\\delta\\operatorname*{min}\\{d,d^{2\\alpha}\\}}}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "and such that ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\left|m(z)-1+{\\frac{1}{d}}\\sum_{j=1}^{v}{\\frac{j^{-2\\alpha}}{j^{-2\\alpha}-z}}\\right|\\leq{\\frac{C(\\alpha)}{\\delta\\operatorname*{min}\\{d^{2},d^{4\\alpha}\\}}}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Furthermore, on the same set ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\left|\\sum_{j=1}^{v}\\frac{j^{-2\\alpha-2\\beta}}{j^{-2\\alpha}m-(u+i\\eta)}-\\sum_{j=1}^{v}\\frac{j^{-2\\alpha-2\\beta}}{j^{-2\\alpha}-(u+i\\eta)}\\right|\\leq\\frac{C(\\alpha)}{\\delta\\operatorname*{min}\\{d,d^{2\\alpha}\\}}.\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Proof. We apply Proposition E.2 part 2. We have ", "page_idx": 53}, {"type": "equation", "text": "$$\n{\\frac{v}{d}}\\operatorname{\\mathbb{E}}X={\\frac{1}{d}}\\sum_{j=1}^{v}j^{-2\\alpha}\\lesssim{\\frac{1}{\\operatorname*{min}\\{d,d^{2\\alpha}\\}}},\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "and the result follows directly from Proposition E.2. ", "page_idx": 53}, {"type": "text", "text": "We turn to evaluating the sum ", "page_idx": 53}, {"type": "equation", "text": "$$\nS(m,u+i\\eta)\\stackrel{\\mathrm{def}}{=}\\sum_{j=1}^{v}\\frac{j^{-2\\alpha-2\\beta}}{j^{-2\\alpha}m-(u+i\\eta)}.\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Then taking the partial derivative in $m$ , ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\partial_{m}S(m,u+i\\eta)=\\sum_{j=1}^{v}\\frac{j^{-4\\alpha-2\\beta}}{(j^{-2\\alpha}m-(u+i\\eta))^{2}},\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "which is uniformly bounded on $U$ and on the set $m$ so $|m-1|<\\delta/2$ . It follows that on $U$ ", "page_idx": 53}, {"type": "equation", "text": "$$\n|S(m,u+i\\eta)-S(1,u+i\\eta)|\\lesssim\\frac{1}{\\operatorname*{min}\\{d,d^{2\\alpha}\\}}.\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "For the second part, we start by observing that we can estimate ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\left|\\frac{v}{d}\\mathbb{E}\\left(\\frac{X}{X-z}\\right)\\right|=\\frac{1}{d}\\left|\\sum_{j=1}^{v}\\left(\\frac{j^{-2\\alpha}}{j^{-2\\alpha}-u-i\\eta}\\right)\\right|\\lesssim\\frac{1}{\\delta\\operatorname*{min}\\{d,d^{2\\alpha}\\}}.\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "We further have ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\left|\\frac{v}{d}\\mathbb{E}\\left(\\frac{X^{2}}{(X m-z)^{2}}\\right)\\right|\\lesssim\\frac{1}{d}\\left|\\sum_{j=1}^{v}j^{-4\\alpha}/\\delta^{2}\\right|\\lesssim\\frac{1}{\\delta^{2}\\operatorname*{min}\\{d,d^{4\\alpha}\\}}.\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Hence, combining all these errors we conclude the claim. ", "page_idx": 53}, {"type": "text", "text": "F Approximation of the forcing function ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "We now apply the technical estimate to find good approximations for the function $\\mathcal{F}$ . Recall ", "page_idx": 53}, {"type": "text", "text": "where ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathcal{F}(r)\\overset{\\mathrm{def}}{=}\\frac{1}{2\\pi i}\\oint_{\\Gamma+\\Gamma_{0}}\\langle\\mathcal{R}(z),(D^{1/2}b)^{\\otimes2}\\rangle(1-2\\gamma B z+2\\gamma^{2}B^{2}z^{2})^{r}\\ \\mathrm{d}z,\\ \\ \\ \\ \\ }\\\\ &{}&{\\mathcal{R}(z)=\\mathrm{Diag}\\bigg(\\displaystyle\\frac{j^{-2\\alpha}}{-z+j^{-2\\alpha}m(z)}:1\\leq j\\leq v\\bigg)\\quad\\mathrm{and}\\quad m(z)=\\displaystyle\\frac{1}{1+\\frac{1}{d}\\sum_{j=1}^{v}\\frac{j^{-2\\alpha}}{j^{-2\\alpha}m(z)-z}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "We decompose the forcing function into a sum of three functions ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\mathcal{F}(r)=\\mathcal{F}_{p p}(r)+\\mathcal{F}_{a c}(r)+\\mathcal{F}_{0}(r)+\\mathrm{errors},\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "which will be introduced in the course of the approximation. ", "page_idx": 53}, {"type": "image", "img_path": "aVSxwicpAk/tmp/46f687aec6154414df0da40791c68ea81c6de0f63323a3796ae6321f1bfcf196.jpg", "img_caption": ["Figure 7: Contour of $\\Gamma+\\Gamma_{0}$ . This is used to estimate the $m$ and derive expressions for the forcing function and kernel function. The important part of the contour is $\\Gamma_{0}$ , which contains the point mass at 0 (blue) and $\\Gamma_{C}$ (purple) which contains the bulk of the spectrum of deterministic equivalent of K\u02c6. There is a left spectral gap which occurs at $d^{-2\\alpha}$ . Moreover we have a change of behavior at $d^{-\\alpha}$ in the contour to account for the change of behavior from pure point to absolutely continuous bulk part of the spectrum. "], "img_footnote": [], "page_idx": 54}, {"type": "text", "text": "The contour we select will come in three parts. The contour $\\Gamma_{0}$ is an arbitrarily small contour enclosing 0. The contour $\\Gamma$ will be in three parts which is symmetric under the reflection $z\\mapsto-z$ . The main part will be $\\Gamma_{C}$ parameterized by $z\\,=\\,u+i\\eta(\\dot{u})$ with $\\eta(u)$ as in Proposition E.5 for $u\\in[u_{0},u_{1}]$ where $u_{0}=\\dot{u_{0}(d)}=C d^{-2\\alpha}$ for some large $C>0$ and $u_{1}$ is a small positive constant. This is connected by two curves, one which is a smooth curve $\\Gamma_{L}$ which is on scale $d^{-2\\alpha}$ and which is reflection symmetric, connects $u_{0}+i\\eta(u_{0})$ to its conjugates and crosses the imaginary axis on $[0,c d^{-2\\alpha}]$ (with $c$ as in Proposition E.3). The other $\\Gamma_{R}$ connects $u_{1}+i\\eta(u_{1})$ to its conjugate by a smooth curve which avoids an $\\epsilon$ neighborhood of $[0,1]$ . ", "page_idx": 54}, {"type": "text", "text": "For $\\Gamma_{0}$ , using Proposition E.3, we have ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\mathscr{F}_{0}(r)\\overset{\\mathrm{def}}{=}\\frac{-1}{2\\pi i}\\oint_{\\Gamma_{0}}\\langle\\mathscr{R}(z),(D^{1/2}b)^{\\otimes2}\\rangle(1-2\\gamma B z+2\\gamma^{2}B^{2}z^{2})^{r}\\ \\mathrm{d}z.\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "This can be evaluated explicitly in terms of a residue at $0$ . Proposition F.1. The function ${\\mathfrak{F}}_{0}(r)$ is constant and ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\left|\\mathcal{F}_{0}(0)-\\sum_{j=1}^{v}\\frac{j^{-2\\alpha-2\\beta}}{1+j^{-2\\alpha}d^{2\\alpha}\\kappa(v/d)}\\right|\\le C d^{-2\\alpha+(2\\beta-1)+1}.\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "Proof. From Proposition E.3, we can apply the residue formula. Evaluating the residue and bounding the sum produces the statement. ", "page_idx": 54}, {"type": "text", "text": "The contours $\\Gamma_{R}$ and $\\Gamma_{L}$ both contribute error terms. Define the sum of the two as ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\mathcal{F}_{c a p s}\\overset{\\mathrm{def}}{=}\\frac{-1}{2\\pi i}\\oint_{\\Gamma_{R}+\\Gamma_{L}}\\langle\\mathcal{R}(z),(D^{1/2}b)^{\\otimes2}\\rangle(1-2\\gamma B z+2\\gamma^{2}B^{2}z^{2})^{r}\\ \\mathrm{d}z.\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "Proposition F.2. There are positive functions $f(r)$ and $g(r)$ satisfying $f(r)\\leq C\\exp(-c\\gamma B r d^{-2\\alpha})$ and $g(r)\\leq C\\exp(-c\\gamma B r)$ so that ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\Big|\\mathcal{F}_{c a p s}(r)\\Big|\\leq C f(r)d^{-2\\alpha+(1-2\\beta)_{+}}+C g(r).\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Furthermore, for any $M>1$ we can choose $u_{0}=T d^{-2\\alpha}$ and $u_{1}=1/T$ with $T$ sufficiently large that $\\mathcal{F}_{c a p s}(r)$ satisfies for $\\gamma B r\\leq M$ and $\\gamma B r\\geq M d^{2\\alpha}$ and some other $C>0$ ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\mathcal{F}_{c a p s}(r)\\geq f(C r)d^{-2\\alpha+(1-2\\beta)+}/C+g(r)/C.\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Hence this will appear as essentially constant on the loss curves. When combined with ${\\mathcal{F}}_{0}$ we have that $\\mathcal{F}_{0}(r)+\\mathcal{F}_{c a p s}(r)$ is bounded above and below by constants times $d^{-2\\alpha+(2\\beta-1)+-1}$ . ", "page_idx": 55}, {"type": "text", "text": "Proof. Both the contributions from $\\Gamma_{L}$ and $\\Gamma_{R}$ give exponentially decaying errors, albeit at much different scales and lead to the $f$ and $g$ terms respectively. For the $\\Gamma_{R}$ terms, we simply bound, using Proposition E.6, ", "page_idx": 55}, {"type": "equation", "text": "$$\n|m(z)-1|\\lesssim d^{-\\operatorname*{min}\\left\\{2\\alpha,1\\right\\}}.\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "On the $\\Gamma_{R}$ contour, having picked the contour sufficiently close to $[0,1]$ (independent of $v,d)$ , we have for some $\\delta>0$ ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\left|(1-2\\gamma B z+2\\gamma^{2}B^{2}z^{2})^{r}\\right|\\leq e^{-2\\gamma B\\,\\mathrm{Re}\\,z r(1-\\delta)}.\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Then we have ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\displaystyle\\frac{-1}{2\\pi i}\\oint_{\\Gamma_{R}}\\left(\\langle\\Re(z),(D^{1/2}b)^{\\otimes2}\\rangle-\\displaystyle\\sum_{j=1}^{v}\\frac{j^{-2\\alpha-2\\beta}}{j^{-2\\alpha}-z}\\right)(1-2\\gamma B z+2\\gamma^{2}B^{2}z^{2})^{r}\\;\\mathrm{d}z\\right|}\\\\ &{\\lesssim d^{-\\operatorname*{min}\\{2\\alpha,1\\}}\\times\\left|\\displaystyle\\oint_{\\Gamma_{R}}e^{-2\\gamma B\\,\\mathrm{Re}\\,z r(1-\\delta)}\\,|\\,\\mathrm{d}z|\\right|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Hence this decays exponentially. ", "page_idx": 55}, {"type": "text", "text": "By construction of the $\\Gamma_{R}$ contour, we can close the contour with an additional (nearly vertical) segment $\\Gamma_{V}$ with real part $u$ and height $\\epsilon u$ . Moreover this can be chosen to evenly divide two poles $\\{j^{-2\\alpha}\\}$ , by adding small horizontal segments. Then we can estimate on $\\Gamma_{V}$ (essentially by Proposition E.4, with an extension for very small imaginary part when we split two poles) ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\left|\\sum_{j=1}^{v}\\frac{j^{-2\\alpha-2\\beta}}{j^{-2\\alpha}-z}\\right|\\lesssim u_{1}^{\\beta/\\alpha-1/(2\\alpha)}.\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Then integrating over $\\Gamma_{V}$ , we get ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\left|\\frac{-1}{2\\pi i}\\oint_{\\Gamma\\nu}\\left(\\sum_{j=1}^{v}\\frac{j^{-2\\alpha-2\\beta}}{j^{-2\\alpha}-z}\\right)(1-2\\gamma B z+2\\gamma^{2}B^{2}z^{2})^{r}\\;\\mathrm{d}z\\right|\\lesssim\\epsilon u_{1}^{1+\\beta/\\alpha-1/(2\\alpha)}e^{-2\\gamma B u_{1}r(1-\\delta)}.\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Having enclosed the poles, we can apply the residue formula, and we have ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\frac{-1}{2\\pi i}\\oint_{\\Gamma\\nu+\\Gamma_{R}}\\left(\\sum_{j=1}^{v}\\frac{j^{-2\\alpha-2\\beta}}{j^{-2\\alpha}-z}\\right)(1-2\\gamma B z+2\\gamma^{2}B^{2}z^{2})^{r}\\;\\mathrm{d}z=\\sum_{j=1}^{j_{0}}j^{-2\\alpha-2\\beta}(1-2\\gamma B j^{-2\\alpha}+2\\gamma^{2}B^{2}j^{-4\\gamma})^{r}\\;\\mathrm{d}z\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "for some $j_{0}$ with $j_{0}\\asymp u_{1}^{-1/(2\\alpha)}$ u1\u22121/(2\u03b1). Hence both contributions of \u0393V and \u0393R decay like g(r) for an appropriate choice of $\\delta,C$ . ", "page_idx": 55}, {"type": "text", "text": "For $\\Gamma_{L}$ , we use similar arguments. We use Proposition E.3 to replace the summation by a $d\\!.$ - independent quantity, which also requires rescaling the contour by $d^{-2\\alpha}$ . Then we have ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\displaystyle\\frac{-1}{2\\pi i}\\oint_{\\Gamma_{L}}\\left(\\langle\\Re(z),(D^{1/2}b)^{\\otimes2}\\rangle-d^{1-2\\beta}\\int_{0}^{a}\\frac{x^{-2\\beta}\\,\\mathrm{d}x}{f\\big(z d^{2\\alpha}\\big)-z d^{2\\alpha}x^{2\\alpha}}\\right)(1-2\\gamma B z+2\\gamma^{2}B^{2}z^{2})^{r}\\;\\mathrm{d}z\\right|}\\\\ &{\\lesssim d^{-2\\alpha}\\left|\\displaystyle\\oint_{\\Gamma_{L}d^{2\\alpha}}e^{-2\\gamma B c d^{-2\\alpha}r}\\left|d z\\right|\\right|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Hence we are left with a dominant contribution of ", "page_idx": 56}, {"type": "equation", "text": "$$\nd^{1-2\\beta-2\\alpha}\\frac{-1}{2\\pi i}\\oint_{\\Gamma_{L}d^{2\\alpha}}\\left(\\int_{0}^{a}\\frac{x^{-2\\beta}\\,\\mathrm{d}x}{f(z)-z x^{2\\alpha}}\\right)\\exp(-2\\gamma B r z d^{-2\\alpha})\\,\\mathrm{d}z\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "In the case that $2\\beta>1$ we instead are left with ", "page_idx": 56}, {"type": "equation", "text": "$$\nc_{\\beta}d^{-2\\alpha}\\frac{-1}{2\\pi i}\\oint_{\\Gamma_{L}d^{2\\alpha}}\\left(\\frac{1}{f(z)}\\right)\\exp(-2\\gamma B r z d^{-2\\alpha})\\,\\mathrm{d}z.\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "As the spectral support of $f$ has a left edge, these decay exponentially. In either case, we can then deform the contour to run twice along the real axis and then vertically to the ends of the $\\Gamma_{L}$ contour. The component along the vertical portion can be estimated by ", "page_idx": 56}, {"type": "equation", "text": "$$\nO(d^{(1-2\\beta)_{+}}(u_{0}^{1-1/(2\\alpha)}/d))\\exp(-(2-\\delta)\\gamma B r u_{0})\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "(and using the boundedness of $f,1/f)$ . This can be made to decay faster than the contribution from $f$ . ", "page_idx": 56}, {"type": "text", "text": "Finally, the dominant contributions arise from the contour $\\Gamma_{C}$ . We define: ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathcal{F}_{p p}(\\boldsymbol{r})\\overset{\\mathrm{def}}{=}\\frac{1}{2\\alpha}\\int_{0}^{1}u^{(2\\beta-1)/(2\\alpha)}\\exp(-2\\gamma B r u)\\,\\mathrm{d}u,}\\\\ {\\displaystyle\\mathcal{F}_{a c}(\\boldsymbol{r})\\overset{\\mathrm{def}}{=}\\frac{c_{\\beta}}{2\\alpha}\\int_{d^{-2\\alpha}}^{1}u^{-1/(2\\alpha)}d^{-1}\\exp(-2\\gamma B r u)\\,\\mathrm{d}u,}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "where $c_{\\beta}$ is as in Proposition E.5. Then this gives us the principal contribution to the limit: ", "page_idx": 56}, {"type": "text", "text": "Proposition F.3. Set for $r\\geq0$ ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\mathscr{F}_{C}(r)\\overset{d e f}{=}\\frac{1}{2\\pi i}\\oint_{\\Gamma_{C}}\\langle\\mathscr{R}(z),(D^{1/2}b)^{\\otimes2}\\rangle(1-2\\gamma B z+2\\gamma^{2}B^{2}z^{2})^{r}\\ \\mathrm{d}z.\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "Then $\\mathcal{F}_{C}(r)$ is real-valued and satisfies for some constant $C$ independent of $u_{1},u_{0},\\alpha,\\beta$ ", "page_idx": 56}, {"type": "equation", "text": "$$\n|\\mathcal{F}_{C}(r)|\\leq C(\\mathcal{F}_{p p}(r)+\\mathcal{F}_{a c}(r)).\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "Moreover, there is an $M\\;=\\;M(u_{0},u_{1})\\;>\\;0$ and a positive bounded function $C(r)$ so that if $\\gamma B r\\in[M,d^{2\\alpha}/M]$ then ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\frac{1}{C(r)}(\\mathcal{F}_{p p}(r)+\\mathcal{F}_{a c}(r))\\leq\\mathcal{F}_{C}(r)\\leq C(r)(\\mathcal{F}_{p p}(r)+\\mathcal{F}_{a c}(r))\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "Furthermore, for any $\\epsilon>0$ there is a $M(\\epsilon,u_{0},u_{1})$ large enough that $C(r)\\leq1+\\epsilon$ for $\\gamma B r\\,\\in$ $[M,d^{2\\alpha}/M]$ . ", "page_idx": 56}, {"type": "text", "text": "Proof. These follow in a similar way to the earlier Propositions, and so we do not enter the details.   \nInstead, we give a brief overview, using the estimates given in Proposition E.5 and Proposition E.4. ", "page_idx": 56}, {"type": "text", "text": "Along $\\mathcal{F}_{C}$ , we can approximate $m$ uniformly by ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\left|m(z(u))-\\Big(1-(c(u)+i)\\frac{\\pi}{2\\alpha}u^{-1/(2\\alpha)}d^{-1}\\Big)\\right|\\le\\epsilon u^{-1/(2\\alpha)}d^{-1},\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "where $c$ is real-valued and bounded and $M=M(\\epsilon)$ . Hence using Proposition E.4, ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{j=1}^{v}\\frac{j^{-2\\alpha-2\\beta}}{-u-i\\eta+j^{-2\\alpha}m(z(u))}=(1+O(\\epsilon))A(u)+i(1+O(\\epsilon))\\frac{\\pi u^{\\beta/\\alpha-1/(2\\alpha)}}{2\\alpha}}&{}\\\\ {\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\,i(1+O(\\epsilon))c_{\\beta}\\frac{\\pi}{2\\alpha}u^{-1/(2\\alpha)}d^{-1}}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "for real valued $\\boldsymbol{\\mathcal{A}}$ and $\\begin{array}{r}{c_{\\beta}=\\sum_{j=1}^{\\infty}j^{-2\\beta}}\\end{array}$ if $\\beta>1/2$ or 0 otherwise as in Proposition E.4. Integrating each of these imaginary terms over $\\Gamma_{C}$ produces $\\mathcal{F}_{p p}$ and $\\mathcal{F}_{a c}$ respectively. The real part is negligible, as the contour is close to the real axis (in particular the imaginary part of the contour is smaller than the real part by a factor of $\\epsilon$ ). \u53e3 ", "page_idx": 56}, {"type": "text", "text": "Combining all of these propositions, we have the following conclusion ", "page_idx": 57}, {"type": "text", "text": "Corollary F.1. For any $\\alpha,\\beta$ with $\\alpha,\\beta\\neq1/2$ and $\\alpha+\\beta>{\\frac{1}{2}}$ there is a function $C(r)$ bounded above for all $r$ so that ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{1}{C(r)}(\\mathcal{F}_{p p}(r)+\\mathcal{F}_{a c}(r)+\\mathcal{F}_{0}(r))\\leq\\mathcal{F}(r)\\leq C(r)(\\mathcal{F}_{p p}(r)+\\mathcal{F}_{a c}(r)+\\mathcal{F}_{0}(r)),}\\end{array}\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "Moreover, for any $\\epsilon>0$ there is a $M(\\epsilon)$ large enough that $C(r)\\leq1+\\epsilon$ for $\\gamma B r\\in[M,d^{2\\alpha}/M]$ and for $\\gamma{\\dot{B}}r>{\\dot{M}}d^{2\\alpha}$ . ", "page_idx": 57}, {"type": "text", "text": "Proof. This follows directly from Proposition F.3, F.2 and F.1, and needs that the $\\mathcal{F}$ curve is monotone to flil the gaps on which the approximations are made. ( There are potentially two windows on which the various approximations do not overlap: when $\\gamma B r$ is a large constant and when it is on order $d^{2\\alpha}$ ) \u53e3 ", "page_idx": 57}, {"type": "text", "text": "G Estimation of Kernel function ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "We can now give the approximation of the kernel function, which is represented by ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\mathscr{K}(r)\\stackrel{\\mathrm{def}}{=}\\frac{-\\gamma^{2}B}{2\\pi i}\\oint_{\\Gamma+\\Gamma_{0}}z^{2}\\mathrm{Tr}(\\mathscr{R}(z))(1-2\\gamma B z+2\\gamma^{2}B^{2}z^{2})^{r}\\ \\mathrm{d}z,\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "with the same contours that were used for the forcing function. ", "page_idx": 57}, {"type": "text", "text": "Using Lemma E.1, we therefore can represent the kernel function as ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\mathcal{K}(r)\\stackrel{\\mathrm{def}}{=}\\frac{-\\gamma^{2}B}{2\\pi i}\\oint_{\\Gamma+\\Gamma_{0}}z(-v+(1-m(z))d)(1-2\\gamma B z+2\\gamma^{2}B^{2}z^{2})^{r}\\;\\mathrm{d}z.\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "By the residue theorem, the contribution from $\\Gamma_{0}$ disappears, as does the $V z$ term. Hence we are left with the representations ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\mathcal{K}(r)\\stackrel{\\mathrm{def}}{=}\\frac{-\\gamma^{2}B d}{2\\pi i}\\oint_{\\Gamma}z(1-m(z))(1-2\\gamma B z+2\\gamma^{2}B^{2}z^{2})^{r}\\ \\mathrm{d}z.\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "When $\\alpha>\\textstyle{\\frac{1}{4}}$ , the dominant contribution comes once more from the contour $\\Gamma_{C}$ for which we get ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\mathcal{K}_{p p}(r)\\stackrel{\\mathrm{def}}{=}\\frac{\\gamma^{2}B}{2\\alpha}\\int_{0}^{1}u^{1-1/(2\\alpha)}\\exp(-2\\gamma B u r)\\,\\mathrm{d}u.\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "It now follows swiftly from the estimates on $m$ : ", "page_idx": 57}, {"type": "text", "text": "Proposition G.1. Suppose $\\alpha>\\textstyle{\\frac{1}{4}}$ . There is a positive function $C(r)$ so that ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{1}{C(r)}\\mathcal{K}_{p p}(r)\\leq\\mathcal{K}(r)\\leq C(r)\\mathcal{K}_{p p}(r),}\\end{array}\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "and $C(r)$ is bounded independent of d by a function of $M$ for all $r\\gamma B<d^{2\\alpha}M$ . Moreover for any $\\epsilon>0$ there is an $M$ sufficiently large so that for $r\\gamma B\\in[{\\dot{M}},d^{2\\alpha}/M]$ , $C(r)<1+\\epsilon$ . ", "page_idx": 57}, {"type": "text", "text": "Proof. By reflection symmetry, the real part of contour integral for $\\mathcal{K}(r)$ vanishes. Also, for a given contour $\\Gamma_{A}$ , we will define ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\mathcal{K}_{A}(r)=\\frac{-\\gamma^{2}B}{\\pi}\\,\\mathrm{Im}\\oint_{\\Gamma_{A}}z(1-m(z))d(1-2\\gamma B z+2\\gamma^{2}B^{2}z^{2})^{r}\\ \\mathrm{d}z,\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "and we will estimate each piece of $\\Gamma=\\Gamma_{R}+\\Gamma_{C}+\\Gamma_{L}$ separately. ", "page_idx": 57}, {"type": "text", "text": "We begin with the contributions from $\\Gamma_{R}$ . Using Proposition E.6, we have that on $\\Gamma_{R}$ ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\displaystyle\\frac{-\\gamma^{2}B}{2\\pi i}\\oint_{\\Gamma_{R}}z\\left((1-m(z))d-\\sum_{j=1}^{V}\\frac{j^{-2\\alpha}}{j^{-2\\alpha}-z}\\right)(1-2\\gamma B z+2\\gamma^{2}B^{2}z^{2})^{r}\\;\\mathrm{d}z\\right|}\\\\ &{\\lesssim d^{-\\operatorname*{min}\\{1,4\\alpha-1\\}}\\gamma^{2}B\\exp(-\\delta\\gamma B r).}\\end{array}\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "Following the same steps as in the proof of Proposition F.2, we can extend the $\\Gamma_{R}$ contour by a straight line $\\Gamma_{V}$ to enclose some residues, which leads to ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{-\\gamma^{2}B}{2\\pi i}\\sum_{\\Gamma_{R}+\\Gamma_{V}}z\\left(\\sum_{j=1}^{v}\\frac{j^{-2\\alpha}}{j^{-2\\alpha}-z}\\right)(1-2\\gamma B z+2\\gamma^{2}B^{2}z^{2})^{r}\\;\\mathrm{d}z}}\\\\ &{}&{=\\gamma^{2}B\\sum_{j=1}^{j_{0}}j^{-4\\alpha}(1-2\\gamma B j^{-2\\alpha}+2\\gamma^{2}B^{2}j^{-4\\alpha})^{r},}\\end{array}\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "with $j_{0}\\asymp u_{1}^{-1/(2\\alpha)}$ . Moreover, the contribution of the $\\Gamma_{V}$ contour can be estimated (for some $\\delta>0$ which can be made small by increasing $u_{1}$ ) by ", "page_idx": 58}, {"type": "equation", "text": "$$\nO(u_{1}^{2-1/(2\\alpha)}\\exp(-2\\gamma B u_{1}r(1-\\delta))).\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "Meanwhile making a Riemann sum approximation (and changing variables by $j^{-2\\alpha}=u]$ ) ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\sum_{j=1}^{j_{0}}j^{-4\\alpha}(1-2\\gamma B j^{-2\\alpha}+2\\gamma^{2}B^{2}j^{-4\\alpha})^{r}\\asymp\\frac{1}{2\\alpha}\\int_{u_{1}}^{1}u^{1-1/(2\\alpha)}\\exp(-2\\gamma B r u)\\,\\mathrm{d}u\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "for $\\begin{array}{r}{2\\gamma B r<\\frac{1}{u_{1}}}\\end{array}$ . Hence by taking $u_{1}$ sufficiently small, we conclude that for $\\begin{array}{r}{2\\gamma B r<\\frac{1}{u_{1}}}\\end{array}$ , ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\mathcal{K}_{R}(r)\\asymp\\frac{\\gamma^{2}B}{2\\alpha}\\int_{u_{1}}^{1}u^{1-1/(2\\alpha)}\\exp(-2\\gamma B r u)\\,\\mathrm{d}u,\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "and also that $|\\mathcal{K}_{R}(r)|\\lesssim e^{-2\\gamma B r u_{1}(1-\\delta)}$ for larger $(2\\gamma B r)$ . ", "page_idx": 58}, {"type": "text", "text": "The contributions from $\\Gamma_{C}$ give, in a similar way for $\\begin{array}{r}{2\\gamma B r>\\frac{1}{u_{1}}}\\end{array}$ and $\\begin{array}{r}{2\\gamma B r<\\frac{1}{u_{1}}}\\end{array}$ ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\mathcal{K}_{C}(r)\\asymp\\gamma^{2}B\\int_{u_{0}}^{u_{1}}u^{1-1/(2\\alpha)}\\exp(-2\\gamma B r u)\\,\\mathrm{d}u\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "Moreover for any $\\epsilon>0$ there is an $M>0$ sufficiently large that when $(2\\gamma B r)$ is in $[M,d^{2\\alpha}/M]$ , ", "page_idx": 58}, {"type": "equation", "text": "$$\n1-\\epsilon<\\frac{\\gamma^{2}B}{2\\alpha\\mathcal{K}_{C}(r)}\\int_{u_{0}}^{u_{1}}u^{1-1/(2\\alpha)}\\exp(-2\\gamma B r u)\\,\\mathrm{d}u<1+\\epsilon,\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "by first choosing $\\epsilon>0$ , then choosing the contour as in Proposition E.5 sufficiently far, and then possibly shrinking $[u_{0},u_{1}]$ . For larger $r$ , it further satisfies an estimate that for some $\\delta>0$ , which can be made smaller by increasing $u_{0}$ , $\\mathcal{K}_{L}(r)\\lesssim e^{-2\\gamma B r u_{0}(1-\\delta)}$ . ", "page_idx": 58}, {"type": "text", "text": "Finally, the contributions from $\\Gamma_{L}$ , we have after changing variables ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\mathcal{K}_{L}(r)=\\frac{-\\gamma^{2}B}{\\pi}d^{1-4\\alpha}\\operatorname{Im}\\oint_{\\Gamma_{L}d^{2\\alpha}}z(1-m(z d^{2\\alpha}))(1-2\\gamma B z d^{-2\\alpha}+2\\gamma^{2}B^{2}z^{2}d^{-4\\alpha})^{r}\\ \\mathrm{d}z.\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "This can be compared to the same expression with $m(z d^{2\\alpha})\\rightarrow f(z)$ and replacing $(1-2\\gamma B z d^{-2\\alpha}+$ $2\\gamma^{2}B^{2}z^{2}d^{-4\\alpha})^{\\hat{r}}\\rightarrow\\exp(-2\\gamma B r(\\mathring{\\mathrm{Re}}\\,z)d^{-2\\alpha})$ . This gives for $2\\gamma B r\\lesssim d^{2\\bar{\\alpha}}$ ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\mathcal{K}_{L}(r)\\asymp\\gamma^{2}B d^{1-4\\alpha}\\int_{0}^{u_{0}d^{2\\alpha}}u\\mathfrak{f}(u)\\exp(-2\\gamma B r u d^{-2\\alpha})\\,\\mathrm{d}u,\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "where $\\begin{array}{r}{\\mathfrak{f}(u)=\\frac{-1}{\\pi}\\operatorname*{lim}_{\\epsilon\\to0}f(u+i\\epsilon)}\\end{array}$ is the spectral density corresponding to $f$ . Hence it follows that for $2\\gamma B r\\lesssim d^{2\\alpha}$ , ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\mathcal{K}_{L}(r)\\asymp\\gamma^{2}B\\int_{d^{-2\\alpha}}^{u_{0}}u^{1-1/(2\\alpha)}\\exp(-2\\gamma B r u)\\,\\mathrm{d}u.\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "Remark G.1. In contrast, when $\\alpha<\\frac{1}{4}$ , the dominant contribution to $\\mathcal{K}$ is from $\\mathcal{K}_{L}$ , so that ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\mathcal{K}(r)\\asymp\\gamma^{2}B d^{1-4\\alpha}\\int_{0}^{\\infty}u\\mathfrak{f}(u)\\exp(-2\\gamma B r u d^{-2\\alpha})\\,\\mathrm{d}u,\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "and moreover the density ${\\mathsf{f}}(u)\\lesssim u^{-1/(2\\alpha)}$ so that the integral is convergent (which is implicit in Proposition E.5) ", "page_idx": 58}, {"type": "text", "text": "We conclude with noting that for ther norm of $\\mathcal{K}$ we can directly evaluate it using a contour integral. Summing the contour integral expression ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\|\\mathcal{K}\\|=\\sum_{r=0}^{\\infty}\\mathcal{K}(r)=\\frac{-\\gamma^{2}B d}{2\\pi i}\\oint_{\\Gamma}\\frac{z(1-m(z))}{2\\gamma B z-2\\gamma^{2}B^{2}z^{2}}~\\mathrm{d}z=\\frac{-\\gamma d}{4\\pi i}\\oint_{\\Gamma}\\frac{(1-m(z))}{1-\\gamma B z}~\\mathrm{d}z.\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "We additionally can more generally evaluate a partial norm ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\sum_{s=r}^{\\infty}\\mathcal{K}(s)=\\frac{-\\gamma d}{4\\pi i}\\oint_{\\Gamma}\\frac{(1-m(z))}{1-\\gamma B z}(1-2\\gamma B z+2\\gamma^{2}B^{2}z^{2})^{r}~\\mathrm{d}z.\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "Combining this with Proposition E.6, this leads directly to tight estimates for the kernel norm. Corollary G.1. When $2\\alpha>1$ and $\\gamma B<1$ , ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\|\\mathcal{K}\\|=\\frac{\\gamma}{2}\\sum_{j=1}^{\\infty}\\frac{j^{-2\\alpha}}{1-j^{-2\\alpha}\\gamma B}(1+o(1))\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "When $2\\alpha<1$ (and recalling that we take $\\gamma$ on the order $d^{2\\alpha-1}$ in this case), ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\|\\mathcal{K}\\|=\\frac{\\gamma}{2}\\frac{v^{1-2\\alpha}}{1-2\\alpha}(1+o(1)).\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "Furthermore, for any $\\epsilon>0$ there is an $M>0$ so that $i f\\gamma B r>M$ then ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\frac{1}{\\|\\mathcal{K}\\|}\\sum_{s=r}^{\\infty}\\mathcal{K}(s)<\\epsilon.\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "Proof. For the first case with $2\\alpha>1$ , Proposition E.6 gives on $\\Gamma_{R}$ ", "page_idx": 59}, {"type": "equation", "text": "$$\n1-m=\\frac{1}{d}\\sum_{j=1}^{v}\\frac{j^{-2\\alpha}}{j^{-2\\alpha}-z}+o(1/d)\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "Using this, and completing the $\\Gamma_{R}$ contour via a vertical line, we get a residue contribution which matches the claim, up to some number of terms $j_{0}$ (which can be made as large as desired). Proposition E.5 and E.3 can be used to control the parts of the contour near 0 and in the middle. ", "page_idx": 59}, {"type": "text", "text": "For the second case $2\\alpha<1$ , since $\\gamma$ is small, we may deform the contour to be at a fixed distance from [0, 1], and then once more we can use Proposition E.6 which gives in 1 step that ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\|\\mathcal{K}\\|=\\frac{\\gamma}{2}\\sum_{j=1}^{v}\\frac{j^{-2\\alpha}}{1-j^{-2\\alpha}\\gamma B}+O(\\gamma d^{1-4\\alpha}).\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "For the final statement, under the conditions given on $r$ ", "page_idx": 59}, {"type": "equation", "text": "$$\n|(1-2\\gamma B z+2\\gamma^{2}B^{2}z^{2})^{r}|<\\epsilon\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "uniformly over the contours, and the estimate follows directly. ", "page_idx": 59}, {"type": "text", "text": "From here, we can derive the \u201csub-exponential\u201d property of $\\mathcal{K}$ . ", "page_idx": 59}, {"type": "text", "text": "Proposition G.2. Suppose $\\alpha\\,>\\,{\\frac{1}{4}}$ . For any $\\epsilon\\,>\\,0$ , there is an $M$ sufficiently large so that for $\\gamma B r\\in[M,d^{2\\alpha}/M]$ ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\sum_{s=0}^{r}\\mathcal{K}(s)\\mathcal{K}(r-s)\\leq(2+\\epsilon)\\|\\mathcal{K}\\|\\mathcal{K}(r)\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "Proof. We note that in the range of $r$ given, we can conclude that for any $\\delta,\\epsilon>0$ , by increasing $M$ ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\sum_{r\\delta}^{\\infty}\\mathcal{K}(s)<\\epsilon\\|\\mathcal{K}\\|,\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "furthermore that for $s>r/2$ ", "page_idx": 60}, {"type": "equation", "text": "$$\n(1-\\epsilon)\\mathcal{K}_{p p}(s)<\\mathcal{K}(s)<(1+\\epsilon)\\mathcal{K}_{p p}(s),\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "and that finally for $s>r/2$ ", "page_idx": 60}, {"type": "equation", "text": "$$\n(1-\\epsilon)\\mathcal{K}_{p p}(s)<\\frac{\\gamma^{2}B}{2\\alpha}\\Gamma(2-(1/(2\\alpha)))(2\\gamma B s)^{-2+(1/(2\\alpha))}<(1+\\epsilon)\\mathcal{K}_{p p}(s),\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "where the final estimate follows by estimating ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\int_{0}^{1}u^{1-1/(2\\alpha)}\\exp(-2\\gamma B u r)\\,\\mathrm{d}u\\asymp\\int_{0}^{\\infty}u^{1-1/(2\\alpha)}\\exp(-2\\gamma B u r)\\,\\mathrm{d}u,\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "once $\\gamma B r>M$ , and moreover their ratio tends to 1 as $M\\to\\infty$ . ", "page_idx": 60}, {"type": "text", "text": "With these estimates in place, we can break the estimate up as ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\sum_{s=0}^{r}\\mathcal{K}(s)\\mathcal{K}(r-s)<2\\sum_{s=0}^{r\\delta}\\mathcal{K}(s)\\mathcal{K}(r-s)+\\sum_{s=r\\delta}^{r(1-\\delta)}\\mathcal{K}(s)\\mathcal{K}(r-s).\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "The final sum is bounded by ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\sum_{r\\delta}^{r(1-\\delta)}\\mathcal{K}(s)\\mathcal{K}(r-s)\\lesssim\\epsilon(1+\\epsilon)\\|\\mathcal{K}\\|\\mathcal{K}_{p p}(r/2).\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "Meanwhile, for the first sum, ", "page_idx": 60}, {"type": "equation", "text": "$$\n2\\sum_{s=0}^{r\\delta}\\mathcal{K}(s)\\mathcal{K}(r-s)\\leq2(1+O(\\epsilon))(1+O(\\delta))\\|\\mathcal{K}\\|\\mathcal{K}_{p p}(r),\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "where we have used that $\\begin{array}{r}{\\mathcal{K}_{p p}(r(1-\\delta))\\leq\\frac{1+\\epsilon}{1-\\epsilon}(1+O(\\delta))\\mathcal{K}_{p p}(r).}\\end{array}$ ", "page_idx": 60}, {"type": "text", "text": "H Asymptotics of forcing function and kernel function ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "With this, we now analyze the asymptotics of each of these terms individually. These asymptotics often rely on a result about how close a Riemann sum is to its integral. We state below the main result of this nature that we used: ", "page_idx": 60}, {"type": "text", "text": "Proposition H.1 (Trapezoidal Rule, [15]). If $f$ is continuous, then for each integer $n\\,>\\,0$ , the integral of $f$ on $[a,b]$ is approximated by ", "page_idx": 60}, {"type": "equation", "text": "$$\nT_{n}(f)\\stackrel{d e f}{=}\\frac{b-a}{2n}\\big(f(x_{0})+2f(x_{1})+\\dots+2f(x_{n-1})+f(x_{n})\\big)\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "where $x_{i}=a+i(b-a)/n,$ , $0\\leq i\\leq n$ . Define the error in the trapezoid rule ", "page_idx": 60}, {"type": "equation", "text": "$$\nE_{n}^{T}(f)\\stackrel{d e f}{=}\\left|T_{n}(f)-\\int_{a}^{b}\\,f(t)\\,\\,\\mathrm{d}t\\right|.\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "If $f$ has an integrable first derivative as an improper integral, thens ", "page_idx": 60}, {"type": "equation", "text": "$$\nE_{n}^{T}(f)\\leq\\frac{b-a}{n}\\int_{a}^{b}|f^{\\prime}(t)|~\\mathrm{d}t.\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "H.1 Pure point forcing term, $\\mathcal{F}_{p p}(r)$ ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "In this section, we prove an asymptotic for the pure point forcing term, see (85), ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\mathcal{F}_{p p}(r)=\\frac{1}{2\\alpha}\\int_{0}^{1}u^{(2\\beta-1)/(2\\alpha)}\\exp(-2\\gamma B r u)\\ \\mathrm{d}u.\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "Proposition H.2 (Pure point forcing term). Suppose $2\\alpha+2\\beta>1$ . For any $\\epsilon>0$ , there is an $M>0$ so that for $\\gamma B r\\geq M$ , ", "page_idx": 61}, {"type": "equation", "text": "$$\n|\\mathcal{F}_{p p}(\\boldsymbol{r})-g(\\boldsymbol{r})|\\le\\epsilon\\times g(\\boldsymbol{r})\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "where ", "page_idx": 61}, {"type": "equation", "text": "$$\ng(r)\\stackrel{d e f}{=}(2\\alpha)^{-1}(2\\gamma B)^{1/(2\\alpha)-\\beta/\\alpha-1}\\times\\Gamma\\big(\\frac{\\beta}{\\alpha}-\\frac{1}{2\\alpha}+1\\big)\\times r^{-(1+\\beta/\\alpha)+1/(2\\alpha)}.\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "Furthermore, for any $\\tilde{M}>0$ , there exists some constants $C,\\tilde{C},c>0$ independent of $d$ so that ", "page_idx": 61}, {"type": "equation", "text": "$$\nc\\leq\\mathcal{F}_{p p}(r)\\leq C\\quad i f\\gamma B r<\\tilde{M},\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "and if $r>\\tilde{M}d^{2\\alpha}$ , ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{F}_{p p}(r)\\leq\\tilde{C}\\times\\mathcal{F}_{0}(r).}\\end{array}\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "Proof. First, a simple computation shows that ", "page_idx": 61}, {"type": "equation", "text": "$$\ng(r)=(2\\alpha)^{-1}(2\\gamma B r)^{-(1+\\beta/\\alpha)+1/(2\\alpha)}\\int_{0}^{\\infty}w^{(2\\beta-1)/(2\\alpha)}\\exp(-w)\\ \\mathrm{d}w.\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "Let $\\rho=-(1+\\beta/\\alpha)+1/(2\\alpha)$ . A simple computation, using the change of variables $w=2\\gamma B r u$ , yields ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\mathscr{F}_{p p}(r)=(2\\alpha)^{-1}(2\\gamma B r)^{\\rho}\\times\\int_{0}^{2\\gamma B r}w^{(2\\beta-1)/(2\\alpha)}\\exp(-w)\\ \\mathrm{d}v.\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "Then we have that ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle|\\mathcal{F}_{p p}(r)-g(r)|\\leq(2\\alpha)^{-1}(2\\gamma B r)^{\\rho}\\left(\\int_{2\\gamma B r}^{\\infty}w^{(2\\beta-1)/(2\\alpha)}\\exp(-w)\\ \\mathrm{d}w\\right)}\\\\ {\\displaystyle\\leq(2\\alpha)^{-1}(2\\gamma B r)^{\\rho}\\int_{2M}^{\\infty}w^{(2\\beta-1)/(2\\alpha)}\\exp(-w)\\ \\mathrm{d}w.}\\end{array}\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "Since $\\begin{array}{r}{\\int_{0}^{\\infty}w^{(2\\beta-1)/(2\\alpha)}\\exp(-w)\\ \\mathrm{d}w.}\\end{array}$ , there exists a $M$ large so that ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\int_{2M}^{\\infty}w^{(2\\beta-1)/(2\\alpha)}\\exp(-w)\\ \\mathrm{d}w<\\epsilon.\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "Thus the first result is shown. ", "page_idx": 61}, {"type": "text", "text": "If $\\gamma B r<{\\tilde{M}}$ , then ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\mathcal{F}_{p p}(r)\\leq(2\\alpha)^{-1}\\int_{0}^{1}u^{(2\\beta-1)/(2\\alpha)}\\;\\mathrm{d}u\\leq C.\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "Moreover, we have that $\\exp(-2\\gamma B r u)\\geq\\exp(-2\\tilde{M})$ . Therefore, we get that ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\mathcal{F}_{p p}(r)\\geq\\frac{\\exp(-2\\tilde{M})}{2\\alpha}\\int_{0}^{1}u^{(2\\beta-1)/(2\\alpha)}~\\mathrm{d}u=c.\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "Now suppose $\\gamma B r>\\tilde{M}d^{2\\alpha}$ . By the previous part, we know that $\\mathcal{F}_{p p}(r)\\leq(1+\\epsilon)g(r)$ . Moreover, we see that $g$ is decreasing. As a result, we see that up to constants ", "page_idx": 61}, {"type": "equation", "text": "$$\ng(r)\\leq g(\\tilde{M}d^{2\\alpha})=C\\times d^{-2\\alpha-2\\beta+1}\\leq\\tilde{C}\\times\\mathcal{F}_{0}(r).\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "for some constants $C,\\tilde{C}>0$ . Hence the result is shown. ", "page_idx": 61}, {"type": "text", "text": "H.2 Model misspecification, point mass at 0, $F_{0}(r)$ ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "Recall, from Proposition F.1, the forcing function point mass at 0, satisfies ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\mathcal{F}_{0}(r)=\\sum_{j=1}^{v}\\frac{j^{-2\\alpha-2\\beta}}{1+j^{-2\\alpha}d^{2\\alpha}\\kappa(v/d)}\\big(1+\\mathcal{O}(d^{-1})\\big)\\mathrm{~where~}\\kappa(v/d)\\mathrm{~solves~}1=\\int_{0}^{v/d}\\frac{\\kappa}{\\kappa+u^{2\\alpha}}\\mathrm{~d}u.\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "In this section, we provide an asymptotic for ${\\mathfrak{F}}_{0}(r)$ (see Proposition H.3) which represents the limiting value the loss obtains as $r\\rightarrow\\infty$ . Unlike the pure point process above, this asymptotic depends on whether $2\\beta>1$ . ", "page_idx": 62}, {"type": "text", "text": "We begin by showing that the $\\kappa$ defined implicitly in (87) is uniquely determined and dimensionless. Lemma H.1. Suppose $v$ and $d$ are admissible such that the ratio $\\textstyle{\\frac{v}{d}}>1$ . Then the equation ", "page_idx": 62}, {"type": "equation", "text": "$$\n1=\\int_{0}^{v/d}\\frac{\\kappa}{\\kappa+u^{2\\alpha}}~\\mathrm{d}u\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "has a unique solution \u03ba such that $0<\\kappa<\\infty$ . ", "page_idx": 62}, {"type": "text", "text": "Proof. Let wd=ef \u03ba and F(w)d=ef 0v/dw+1u2\u03b1 du and set G(w) = wF(w). To solve the fixed point as $\\begin{array}{r}{\\operatorname*{lim}_{w\\to\\infty}G(w)\\,=\\,\\frac{v}{d}\\,>\\,1}\\end{array}$ . As $G(w)$ is continuous, it follows that there exists a solution $\\kappa$ to $G(\\kappa)=1$ . ", "page_idx": 62}, {"type": "text", "text": "To show that $\\kappa$ is unique, amounts to showing that $G(w)$ is strictly increasing for $w\\geq0$ . First, we see that ", "page_idx": 62}, {"type": "equation", "text": "$$\nG(w)=\\int_{0}^{v/d}{\\frac{w+u^{2\\alpha}-u^{2\\alpha}}{w+u^{2\\alpha}}}\\ \\mathrm{d}u=\\int_{0}^{v/d}1-{\\frac{u^{2\\alpha}}{w+u^{2\\alpha}}}\\ \\mathrm{d}u.\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "We note that $\\begin{array}{r}{w\\mapsto\\frac{u^{2\\alpha}}{w+u^{2\\alpha}}}\\end{array}$ is strictly decreasing in $w$ . So $\\begin{array}{r}{w\\mapsto1-\\frac{u^{2\\alpha}}{w+u^{2\\alpha}}}\\end{array}$ is strictly increasing in $w$ . Hence $G(v)$ is strictly increasing and there is a unique solution to $G(\\kappa)=1$ . \u53e3 ", "page_idx": 62}, {"type": "text", "text": "Now we give an asymptotic for ${\\mathcal{F}}_{0}$ . ", "page_idx": 62}, {"type": "text", "text": "Proposition H.3 (Asymptotic for ${\\mathcal{F}}_{0}$ ). Suppose v and $d$ are admissible such that the ratio $v/d>1$ and suppose $2\\alpha+2\\beta>1$ . Let $0<\\kappa(v/d)<\\infty$ be the unique solution to ", "page_idx": 62}, {"type": "equation", "text": "$$\n1=\\int_{0}^{v/d}{\\frac{\\kappa}{\\kappa+u^{2\\alpha}}}~\\mathrm{d}u.\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "Then as $d\\to\\infty$ ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\mathcal{F}_{0}(r)\\sim\\left\\{\\frac{d^{-2\\alpha}}{\\kappa}\\left(\\sum_{j=1}^{v}j^{-2\\beta}\\right),\\qquad\\qquad i f2\\beta>1\\right.\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "Proof. We consider 2 cases. Let $\\kappa=\\kappa(v/d)$ . ", "page_idx": 62}, {"type": "text", "text": "Case 1: Suppose $2\\beta>1$ : Let $\\tilde{C}\\ {\\stackrel{\\mathrm{def}}{=}}\\ \\sum_{j=1}^{v}j^{-2\\beta}$ , which is finite as $2\\beta>1$ . Consider the following ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\mathcal E_{1}(r)\\overset{\\mathrm{def}}{=}\\frac{\\left|\\sum_{j=1}^{v}\\frac{j^{-2(\\alpha+\\beta)}}{j^{-2\\alpha}\\kappa d^{2\\alpha}+1}-\\frac{d^{-2\\alpha}}{\\kappa}\\sum_{j=1}^{v}j^{-2\\beta}\\right|}{\\frac{d^{-2\\alpha}\\tilde{C}}{\\kappa}}=\\frac{\\sum_{j=1}^{v}\\frac{j^{-2\\beta}}{j^{-2\\alpha}\\kappa d^{2\\alpha}+1}}{\\tilde{C}}.\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "To handle the large $j$ values, we see that there exists a $j_{0}$ large so that ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\frac{\\sum_{j=j_{0}}^{v}\\frac{j^{-2\\beta}}{j^{-2\\alpha}\\kappa d^{2\\alpha}+1}}{\\tilde{C}}\\le\\frac{1}{\\tilde{C}}\\sum_{j\\ge j_{0}}j^{-2\\beta}<\\epsilon,\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "where we used that $j^{-2\\alpha}\\kappa d^{2\\alpha}+1>1$ . For the small $j$ , we use that $d$ can be large. Hence, ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\sum_{j=1}^{j_{0}}\\frac{j^{-2\\beta}}{j^{-2\\alpha}\\kappa d^{2\\alpha}+1}\\leq\\sum_{j=1}^{j_{0}}\\frac{j^{-2\\beta}}{j_{0}^{-2\\alpha}\\kappa d^{2\\alpha}+1}\\leq\\frac{j_{0}}{j_{0}^{-2\\alpha}\\kappa d^{2\\alpha}+1}.\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "For sufficiently large $d$ , we can make the right-hand-side small. Therefore, $\\mathcal{E}_{1}(r)$ is small for sufficiently large $d$ and hence, the result holds. ", "page_idx": 63}, {"type": "text", "text": "Case 2: Suppose $2\\beta<1$ : To show this case, we define the following errors ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{E}_{21}(r)\\overset{\\mathrm{def}}{=}\\frac{\\left|\\sum_{j=1}^{v}\\frac{j^{-2(\\alpha+\\beta)}}{j^{-2\\alpha}\\kappa d^{2\\alpha}+1}-d^{1-2(\\alpha+\\beta)}\\int_{1/d}^{v/d}\\frac{u^{-2\\beta}\\mathrm{\\Phi}\\mathrm{d}u}{\\kappa+u^{2\\alpha}}\\right|}{d^{1-2(\\alpha+\\beta)}\\int_{0}^{v/d}\\frac{u^{-2\\beta}\\mathrm{\\Phi}\\mathrm{d}u}{\\kappa+u^{2\\alpha}}}}\\\\ &{\\mathcal{E}_{22}(r)\\overset{\\mathrm{def}}{=}\\frac{\\left|d^{1-2(\\alpha+\\beta)}\\int_{1/d}^{v/d}\\frac{u^{-2\\beta}\\mathrm{\\Phi}\\mathrm{d}u}{\\kappa+u^{2\\alpha}}-d^{1-2(\\alpha+\\beta)}\\int_{0}^{v/d}\\frac{u^{-2\\beta}\\mathrm{\\Phi}\\mathrm{d}u}{\\kappa+u^{2\\alpha}}\\right|}{d^{1-2(\\alpha+\\beta)}\\int_{0}^{v/d}\\frac{u^{-2\\beta}\\mathrm{\\Phi}\\mathrm{d}u}{\\kappa+u^{2\\alpha}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "It is clear, for sufficiently large $d,\\,\\mathcal{E}_{22}(\\boldsymbol{r})$ is small. ", "page_idx": 63}, {"type": "text", "text": "For the first error term, we use a Riemann sum approximation, that is, ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\sum_{j=1}^{v}\\frac{j^{-2(\\alpha+\\beta)}}{j^{-2\\alpha}\\kappa d^{2\\alpha}+1}=d^{1-2(\\alpha+\\beta)}\\times\\frac{1}{d}\\sum_{j=1}^{v}\\frac{(j/d)^{-2(\\alpha+\\beta)}}{(j/d)^{-2\\alpha}\\kappa+1}.\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "Letting a = 1/d, b = v/d, n = v \u22121, xj = 1/d + j/d, and f(x) = xx\u2212\u221222(\u03b1\u03b1\u03ba++\u03b21), we can approximate the summation with an integral. Using Prop. H.1, ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\xi_{21}(r)\\leq\\frac{\\frac{1}{d}\\times\\int_{1/d}^{v/d}|f^{\\prime}(x)|\\,\\,\\mathrm{d}x}{\\int_{0}^{v/d}\\frac{u^{-2\\beta}\\,\\,\\mathrm{d}u}{\\kappa+u^{2\\alpha}}}.\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "One can check that $\\begin{array}{r}{\\frac{\\int_{1/d}^{v/d}|f^{\\prime}(x)|\\ \\mathrm{d}x}{\\int_{0}^{v/d}\\frac{u^{-2\\beta}\\ \\mathrm{d}u}{\\kappa+u^{2\\alpha}}}<C}\\end{array}$ where $C$ is independent of $d$ . For sufficiently large $d$ , ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\mathcal{E}_{21}(r)\\leq\\frac{\\frac{1}{d}\\times\\int_{1/d}^{v/d}|f^{\\prime}(x)|\\ \\mathrm{d}x}{\\int_{0}^{v/d}\\frac{u^{-2\\beta}\\ \\mathrm{d}u}{\\kappa+u^{2\\alpha}}}<C\\times\\frac{1}{d}<\\epsilon.\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "Hence, Case 2 is shown. ", "page_idx": 63}, {"type": "text", "text": "H.3 Absolutely continuous forcing function, $\\mathcal{F}_{a c}(r)$ ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "We now turn to the absolutely continuous forcing function, defined as ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\mathcal{F}_{a c}(r)=\\frac{c_{\\beta}}{2\\alpha}\\int_{d^{-2\\alpha}}^{1}u^{-1/(2\\alpha)}d^{-1}\\exp(-2\\gamma B r u)\\,\\mathrm{d}u,\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "where $\\begin{array}{r}{c_{\\beta}\\,=\\,\\sum_{j=1}^{v}j^{-2\\beta}}\\end{array}$ if $2\\beta\\,>\\,1$ and 0 otherwise. From this, we derive a simple asymptotic formula. ", "page_idx": 63}, {"type": "text", "text": "Proposition H.4. There exists a constant $C(\\alpha,\\beta)>0$ such that ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\mathcal{F}_{a c}(r)\\leq\\left\\{\\!\\!\\begin{array}{l l}{C\\times\\mathcal{F}_{0}(r),}&{i f2\\beta>1,\\,2\\alpha<1}\\\\ {0,}&{i f2\\beta<1.}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "Suppose now $2\\alpha>1$ and $2\\beta>1$ . For any $\\epsilon>0$ , there is an $M>0$ so that for $\\gamma B r\\in[M,d^{2\\alpha}/M]$ , ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle|\\mathcal F_{a c}(r)-g(r)|\\leq\\epsilon\\times g(r)\\,}\\\\ {\\displaystyle w h e r e\\quad g(r)\\,\\overset{d e f}{=}\\big(\\sum_{j=1}^{v}j^{-2\\beta}\\big)\\big(2\\gamma B\\big)^{-1+1/(2\\alpha)}\\big(2\\alpha\\big)^{-1}\\Gamma\\big(1-\\frac{1}{2\\alpha}\\big)\\times r^{-1+1/(2\\alpha)}\\times d^{-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "Furthermore, for any $\\tilde{M}>0$ , these exists some constants $C,c>0$ independent of $d$ so that ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\mathcal{F}_{a c}(r)\\leq\\left\\{C\\times d^{-1},\\quad\\mathrm{{i}}f\\gamma B r\\leq\\tilde{M}\\right.}\\\\ {c\\times\\left.\\mathcal{F}_{0}(r),\\quad\\mathrm{{i}}f\\gamma B r\\geq\\tilde{M}d^{2\\alpha}.\\right.\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "Proof. We proceed by cases. The case $2\\beta<1$ is immediate as $c_{\\beta}$ is only non-zero for $2\\beta>1$ . ", "page_idx": 64}, {"type": "text", "text": "Case: $2\\beta\\,>\\,1$ and $2\\alpha\\,<\\,1$ . In this case, we just bound directly bound $\\mathcal{F}_{a c}(\\boldsymbol{r})$ . Dropping the exponential, we get ", "page_idx": 64}, {"type": "equation", "text": "$$\n\\mathcal{F}_{a c}(r)\\leq\\frac{c_{\\beta}}{2\\alpha}\\int_{d^{-2\\alpha}}^{1}u^{-1/(2\\alpha)}d^{-1}\\;\\mathrm{d}u=\\frac{c_{\\beta}}{2\\alpha}d^{-1}\\left(\\frac{d^{1-2\\alpha}-d^{1/2-\\alpha}}{\\frac{1}{2\\alpha}-1}\\right)\\leq\\frac{c_{\\beta}}{2\\alpha(\\frac{1}{2\\alpha}-1)}\\times d^{-2\\alpha}.\n$$", "text_format": "latex", "page_idx": 64}, {"type": "text", "text": "We know that $\\mathcal{F}_{0}(r)\\asymp d^{-2\\alpha+\\operatorname*{max}\\{0,1-2\\beta\\}}$ and thus the result is shown. ", "page_idx": 64}, {"type": "text", "text": "Next we show (89). ", "page_idx": 64}, {"type": "text", "text": "Case: $2\\beta>1$ and $2\\alpha>1$ . First, we make the following observation. The integral is ", "page_idx": 64}, {"type": "equation", "text": "$$\n\\frac{c_{\\beta}}{2\\alpha}\\int_{0}^{\\infty}u^{-1/(2\\alpha)}d^{-1}\\exp(-2\\gamma B r u)\\ \\mathrm{d}u=g(r).\n$$", "text_format": "latex", "page_idx": 64}, {"type": "text", "text": "Define $\\begin{array}{r}{C=\\frac{c_{\\beta}}{2\\alpha}}\\end{array}$ . Let us consider the following ", "page_idx": 64}, {"type": "equation", "text": "$$\n\\mathscr{E}\\stackrel{\\mathrm{def}}{=}\\Bigg|C\\int_{d^{-2\\alpha}}^{d^{-\\alpha}}e^{-2\\gamma B u r}u^{-1/(2\\alpha)}d^{-1}\\;\\mathrm{d}u-C\\int_{0}^{\\infty}e^{-2\\gamma B u r}u^{-1/(2\\alpha)}d^{-1}\\;\\mathrm{d}u\\Bigg|.\n$$", "text_format": "latex", "page_idx": 64}, {"type": "text", "text": "First, we see ", "page_idx": 64}, {"type": "equation", "text": "$$\n\\varepsilon\\leq\\underbrace{\\left|C\\int_{0}^{d^{-2\\alpha}}e^{-2\\gamma B u r}u^{-1/(2\\alpha)}d^{-1}\\ \\mathrm{d}u\\right|}_{\\varepsilon_{1}}+\\underbrace{\\left|C\\int_{1}^{\\infty}e^{-2\\gamma B u r}u^{-1/(2\\alpha)}d^{-1}\\ \\mathrm{d}u\\right|}_{\\varepsilon_{2}}.\n$$", "text_format": "latex", "page_idx": 64}, {"type": "text", "text": "Case: ${\\mathcal{E}}_{1}$ . Suppose $\\gamma B r\\;\\leq\\;1/M d^{2\\alpha}$ . Here we can just use directly the $u$ and disregard the exponential: ", "page_idx": 64}, {"type": "equation", "text": "$$\n\\int_{0}^{d^{-2\\alpha}}e^{-2\\gamma B u r}u^{-1/(2\\alpha)}d^{-1}\\;\\mathrm{d}u\\leq\\int_{0}^{d^{-2\\alpha}}u^{-1/(2\\alpha)}d^{-1}\\;\\mathrm{d}u=\\tilde{c}\\times d^{-2\\alpha}.\n$$", "text_format": "latex", "page_idx": 64}, {"type": "text", "text": "for some constant $\\tilde{c}$ . Now we have that ", "page_idx": 64}, {"type": "equation", "text": "$$\n\\frac{d^{-2\\alpha}}{d^{-1}(\\gamma B r)^{-1+1/(2\\alpha)}}=d^{-2\\alpha+1}(\\gamma B r)^{1-1/(2\\alpha)}\\le d^{-2\\alpha+1}(1/M)^{1-1/(2\\alpha)}d^{2\\alpha-1}=M^{-1+1/(2\\alpha)}\n$$", "text_format": "latex", "page_idx": 64}, {"type": "text", "text": "By choosing $M$ large, this can be made small. ", "page_idx": 64}, {"type": "text", "text": "Case: $\\mathcal{E}_{2}$ . Suppose $\\gamma B r\\geq M$ . Let us consider ", "page_idx": 64}, {"type": "equation", "text": "$$\n\\mathscr{E}_{2}\\leq C\\int_{1}^{\\infty}d^{-1}e^{-2\\gamma B u r}~\\mathrm{d}u\\leq d^{-1}(\\gamma B r)^{-1}\\exp(-2\\gamma B r).\n$$", "text_format": "latex", "page_idx": 64}, {"type": "text", "text": "It follows that ", "page_idx": 64}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{d^{-1}(\\gamma B r)^{-1}\\exp(-2\\gamma B r)}{d^{-1}(\\gamma B r)^{-1+1/(2\\alpha)}}=(\\gamma B r)^{-1/(2\\alpha)}\\exp(-2\\gamma B r d^{-\\alpha})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\le\\exp(-2M)(M)^{-1/(2\\alpha)}=\\exp(-2M)M^{-1/(2\\alpha)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 64}, {"type": "text", "text": "Therefore, by choosing $M$ large, we have that this can be small. This proves (89). ", "page_idx": 64}, {"type": "text", "text": "To finish the proof, let us first suppose that $\\gamma B r\\leq{\\tilde{M}}$ . Then we have that ", "page_idx": 64}, {"type": "equation", "text": "$$\n\\mathcal{F}_{a c}(r)\\leq\\frac{c_{\\beta}}{2{\\alpha}}\\int_{d^{-2\\alpha}}^{1}u^{-1/(2\\alpha)}d^{-1}\\;\\mathrm{d}u\\lesssim d^{-1}.\n$$", "text_format": "latex", "page_idx": 64}, {"type": "text", "text": "When $\\gamma B r\\geq\\tilde{M}d^{2\\alpha}$ , we have that ", "page_idx": 64}, {"type": "equation", "text": "$$\n\\mathcal{F}_{a c}(r)\\lesssim\\int_{d^{-2\\alpha}}^{1}\\exp(-2\\gamma B r u)\\;\\mathrm{d}u\\leq d^{-2\\alpha}\\exp(-2\\gamma B\\tilde{M}d^{2\\alpha}d^{-2\\alpha})\\lesssim\\mathcal{F}_{0}(r).\n$$", "text_format": "latex", "page_idx": 64}, {"type": "text", "text": "H.4 Kernel function asymptotic. ", "text_level": 1, "page_idx": 65}, {"type": "text", "text": "We recall the term $\\mathcal{K}_{p p}$ defined as ", "page_idx": 65}, {"type": "equation", "text": "$$\n\\mathcal{K}_{p p}(r)=\\frac{\\gamma^{2}B}{2\\alpha}\\int_{0}^{1}u^{1-1/(2\\alpha)}\\exp(-2\\gamma B u r)\\,\\mathrm{d}u\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "We now give an asymptotic for such a function. ", "page_idx": 65}, {"type": "text", "text": "Proposition H.5 ( $\\mathcal{K}_{p p}$ asymptotic). Suppose $\\alpha>1/4$ . For any $\\epsilon>0$ , there is an $M>0$ so that for $\\gamma B r\\geq M$ , ", "page_idx": 65}, {"type": "equation", "text": "$$\n|\\mathcal{K}_{p p}(\\boldsymbol{r})-g(\\boldsymbol{r})|\\leq\\epsilon\\times g(\\boldsymbol{r})\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "where ", "page_idx": 65}, {"type": "equation", "text": "$$\n\\begin{array}{r}{g(r)\\stackrel{d e f}{=}(2\\alpha)^{-1}\\gamma^{2}B(2\\gamma B)^{-2+1/(2\\alpha)}\\times\\Gamma\\big(2-\\frac{1}{2\\alpha}\\big)\\times r^{-2+1/(2\\alpha)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "Moreover, for any $\\tilde{M}>0$ , there exists constants $c,C,{\\hat{C}}>0,$ , such that when $2\\alpha>1$ , ", "page_idx": 65}, {"type": "equation", "text": "$$\nc\\leq\\mathcal{K}_{p p}(r)\\leq C,\\quad i f\\gamma B r\\leq\\tilde{M}\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "and when $2\\alpha<1$ , ", "page_idx": 65}, {"type": "equation", "text": "$$\n\\mathcal{K}_{p p}(r)\\leq\\hat{C}\\times d^{2\\alpha-1},\\quad i f\\gamma B r\\leq\\tilde{M}.\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "Furthermore, for any $\\tilde{M}>0$ , there exist a constant $\\tilde{C}>0$ , such that ", "page_idx": 65}, {"type": "equation", "text": "$$\n\\mathcal{K}_{p p}(r)\\leq\\tilde{C}\\times\\mathcal{F}_{0}(r),\\quad i f\\gamma B r\\geq\\tilde{M}d^{2\\alpha}.\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "Proof. The first part of the argument, (90), follows immediately from the proof of $\\mathcal{F}_{p p}$ , Prop. H.2. If $2\\alpha>1$ , then we always have $\\gamma^{2}B$ is constant order. Therefore, using the same argument as $\\mathcal{F}_{p p}$ (see Prop. H.2), we get that there exists constant $c,C>0$ such that $c\\leq\\mathcal{K}_{p p}(r)\\leq C$ for $\\gamma B r\\leq{\\tilde{M}}$ . If $2\\alpha<1$ , then $\\gamma\\asymp d^{2\\alpha-1}$ . Therefore, for $\\gamma B r\\leq{\\tilde{M}}$ , we have that ", "page_idx": 65}, {"type": "equation", "text": "$$\n\\frac{\\gamma^{2}B}{2\\alpha}\\int_{0}^{1}u^{1-1/(2\\alpha)}\\exp(-2\\gamma B u r)\\,\\mathrm{d}u\\asymp d^{2\\alpha-1}\\frac{\\gamma B}{2\\alpha}\\int_{0}^{1}u^{1-1/(2\\alpha)}\\exp(-2\\gamma B u r)\\,\\mathrm{d}u\\le d^{2\\alpha-1}C.\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "The later inequality follows using the same bounding argument as $\\mathcal{F}_{p p}(r)$ . ", "page_idx": 65}, {"type": "text", "text": "As $\\gamma^{2}B\\le C$ , then the same argument in $\\mathcal{F}_{p p}(r)$ shows for $\\gamma B r\\ge\\tilde{M}d^{2\\alpha}$ that $\\mathcal{K}_{p p}(r)\\leq\\tilde{C}\\mathcal{F}_{0}(r)$ . \u5382 ", "page_idx": 65}, {"type": "text", "text": "We now turn to the last quantity that appears in the Volterra equation. ", "page_idx": 65}, {"type": "text", "text": "Proposition H.6 (Forcing function norm). Provided $2\\beta>1$ , we have that ", "page_idx": 65}, {"type": "equation", "text": "$$\n\\sum_{s=0}^{M d^{2\\alpha}/(\\gamma B)}\\mathcal{F}(s)\\lesssim\\frac{1}{\\gamma B},\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "for some constant $M>0$ . ", "page_idx": 65}, {"type": "text", "text": "Next, suppose $2\\beta<1$ . Then there exists an $\\tilde{M},M>0$ such that ", "page_idx": 65}, {"type": "equation", "text": "$$\n\\mathcal{K}(r)\\times\\sum_{s=\\tilde{M}/(\\gamma B)}^{M d^{2\\alpha}/(\\gamma B)}\\mathcal{F}(s)\\leq\\mathcal{F}(r)\\quad f o r\\,a l l\\,\\tilde{M}\\leq\\gamma B r\\leq M d^{2\\alpha},\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "and it follows for all $\\gamma B r\\leq M d^{2\\alpha}$ , ", "page_idx": 65}, {"type": "equation", "text": "$$\n\\mathcal{K}(r)\\times\\sum_{s=0}^{M d^{2\\alpha}/(\\gamma B)}\\mathcal{F}(s)\\lesssim\\mathcal{F}(r)+\\frac{1}{\\gamma B}\\times\\mathcal{K}(r).\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "Proof. Suppose $2\\beta>1$ . First note that in this region sM=d02\u03b1/(\u03b3B)F0(r) \u2272\u03b31B for any fixed M > 0, Proposition H.3. ", "page_idx": 66}, {"type": "text", "text": "Next, let us consider the pure point part of $\\mathcal{F}$ , i.e., ${\\mathfrak{F}}_{p p}$ . Choose $M$ and $\\tilde{M}$ so that ${\\mathfrak{F}}_{p p}$ is behaving like the asymptotic in Proposition H.2, i.e., for $\\gamma B r\\geq{\\tilde{M}}$ , ", "page_idx": 66}, {"type": "equation", "text": "$$\n\\mathcal{F}_{p p}(r)\\asymp(\\gamma B r)^{-(1+\\beta/\\alpha)+1/(2\\alpha)}.\n$$", "text_format": "latex", "page_idx": 66}, {"type": "text", "text": "and for $\\gamma B r\\leq\\tilde{M},\\mathcal{F}_{p p}(r)\\leq C$ for some constant $C>0$ independent of $d$ . It follows then that ", "page_idx": 66}, {"type": "equation", "text": "$$\n\\sum_{r=0}^{\\tilde{M}/(\\gamma B)}\\mathcal{F}_{p p}(r)\\lesssim\\frac{1}{\\gamma B}.\n$$", "text_format": "latex", "page_idx": 66}, {"type": "text", "text": "To handle the rest of the sum, we see that ", "page_idx": 66}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{r=\\infty}^{M d^{\\alpha}/(r,\\tilde{p})}\\mathcal{I}_{p p}(r)\\lesssim}&{{}\\displaystyle\\sum_{r=\\infty}^{M d^{\\alpha}/(r,\\tilde{p})}(\\gamma B_{r})^{-(1+\\beta/\\alpha)+1/(2\\alpha)}\\asymp\\displaystyle\\frac{1}{\\gamma B}\\sum_{r=\\tilde{M}}^{M d^{\\alpha}}r^{-(1+\\beta/\\alpha)+1/(2\\alpha)}}\\\\ {\\displaystyle}&{{}=\\displaystyle\\frac{(d^{2\\alpha})^{-\\beta/\\alpha}\\cdot^{(1+1/(2\\alpha)})}{\\gamma B}\\times\\frac{1}{d^{2\\alpha}}\\,\\sum_{r=\\infty}^{M d^{\\alpha}}\\left(\\frac{r}{d^{2\\alpha}}\\right)^{-(1+\\beta/\\alpha)+1/(2\\alpha)}}\\\\ {\\displaystyle}&{{}\\le\\frac{(d^{2\\alpha})^{-\\beta/\\alpha+1/(2\\alpha)}M}{\\gamma B}\\int_{\\tilde{M}/{\\alpha}^{2n}}^{M+\\tilde{M}/d^{\\alpha}}x^{-(1+\\beta/\\alpha)+1/(2\\alpha)}\\;\\mathrm{d}x}\\\\ {\\displaystyle}&{{}=\\frac{(d^{2\\alpha})^{-\\beta/\\alpha+1/(2\\alpha)}M^{2\\alpha}}{\\gamma B}\\times\\frac{x^{-\\beta/\\alpha+1/(2\\alpha)}}{1-2\\beta}\\bigg|_{\\tilde{M}/d^{\\alpha}}^{M+\\tilde{M}/d^{\\alpha}}}\\\\ {\\displaystyle}&{{}\\lesssim\\frac{1}{\\gamma B}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 66}, {"type": "text", "text": "Here we use that the Riemann sum approximation with $\\begin{array}{r}{a=\\frac{\\tilde{M}}{d^{2\\alpha}}}\\end{array}$ , $\\begin{array}{r}{b=M+\\frac{\\tilde{M}}{d^{2\\alpha}}}\\end{array}$ d2M\u03b1 , n = Md2\u03b1 and $f(x)=x^{-(1+\\beta/\\alpha)+1/(2\\alpha)}$ . ", "page_idx": 66}, {"type": "text", "text": "Using a similar argument for $\\mathcal{F}_{a c}(\\boldsymbol{r})$ (Proposition H.4)when $2\\alpha>1$ (otherwise we do not need to worry about $\\mathcal{F}_{a c.}$ ), we have that ", "page_idx": 66}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{r=\\tilde{M}/(\\gamma B)}^{M d^{2\\alpha}/(\\gamma B)}\\mathcal{F}_{a c}(r)\\asymp\\frac{d^{-1}}{\\gamma B}\\sum_{r=\\tilde{M}}^{M d^{2\\alpha}}s^{-1+1/(2\\alpha)}=\\frac{M}{\\gamma B}d^{-2\\alpha}\\sum_{\\tilde{M}}^{M d^{2\\alpha}}\\left(\\frac{s}{d^{2\\alpha}}\\right)^{-1+1/(2\\alpha)}}\\\\ {\\displaystyle\\qquad\\qquad\\lesssim\\frac{M}{\\gamma B}\\int_{0}^{M}x^{-1+1/(2\\alpha)}\\;\\mathrm{d}x\\asymp\\frac{1}{\\gamma B}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 66}, {"type": "text", "text": "When $r\\le\\tilde{M}/(\\gamma B)$ , we have that $\\Phi_{a c}(r)\\lesssim d^{-1}$ . Hence, $\\begin{array}{r}{\\sum_{r=0}^{\\tilde{M}/(\\gamma B)}\\mathcal{F}_{a c}(r)\\lesssim\\frac{1}{\\gamma B}}\\end{array}$ ", "page_idx": 66}, {"type": "text", "text": "The first result, (91), then follows from Corollary F.1. ", "page_idx": 66}, {"type": "text", "text": "Consider $2\\beta<1$ and $2\\alpha<1$ . We do not need to worry about $\\mathcal{F}_{a c}$ in this region because it does not exist in this region. Choose $M$ and $\\tilde{M}$ so that both $\\mathcal{K}_{p p}$ and ${\\mathcal F}_{p p}$ are in their asymptotic regions and, using Proposition G.1, $\\mathcal{K}(r)\\asymp\\mathcal{K}_{p p}(r)$ . Using a similar argument as above, we estimate the summation of ${\\mathcal F}_{p p}$ as an integral. For any $r\\in[\\tilde{M}/(\\gamma B),M d^{2\\alpha}/(\\gamma B)]$ ", "page_idx": 66}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\sum_{s=\\tilde{M}/(\\gamma B)}^{r}\\mathcal{F}_{p p}(s)\\lesssim\\frac{(d^{2\\alpha})^{-\\beta/\\alpha+1/(2\\alpha)}M2\\alpha}{\\gamma B}\\times\\frac{x^{-\\beta/\\alpha+1/(2\\alpha)}}{1-2\\beta}\\bigg\\rvert_{\\tilde{M}/d^{2\\alpha}}^{r\\gamma B/d^{2\\alpha}+\\tilde{M}/d^{2\\alpha}}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\lesssim\\frac{1}{\\gamma B}(\\gamma B r)^{-\\beta/\\alpha+1/(2\\alpha)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 66}, {"type": "text", "text": "Using the asymptotic for $\\mathcal{K}_{p p}$ (Proposition H.5), ", "page_idx": 67}, {"type": "equation", "text": "$$\n\\mathcal{K}(r)\\times\\sum_{s=\\tilde{M}/(\\gamma B)}^{r}\\mathcal{F}_{p p}(s)\\lesssim\\gamma\\times(\\gamma B r)^{-\\beta/\\alpha+1/(2\\alpha)}\\times(\\gamma B r)^{-2+1/(2\\alpha)}.\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "We will show that this is less than $\\mathcal{F}_{p p}(r)$ . Using the asymptotic for $\\mathcal{F}_{p p}(r)$ (Proposition H.2), let us suppose ", "page_idx": 67}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{c}{\\gamma\\times(\\gamma B r)^{-\\beta/\\alpha+1/(2\\alpha)}\\times(\\gamma B r)^{-2+1/(2\\alpha)}\\leq(\\gamma B r)^{-1-\\beta/\\alpha+1/(2\\alpha)}}\\\\ {\\Leftrightarrow\\quad\\gamma\\leq(\\gamma B r)^{1-1/(2\\alpha)}.}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "In this region, the learning rate is $\\gamma\\asymp d^{2\\alpha-1}$ . Thus, we see that ", "page_idx": 67}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\qquad d^{2\\alpha-1}\\leq(\\gamma B r)^{(2\\alpha-1)/(2\\alpha)}}\\\\ {\\Leftrightarrow}&{\\;d^{(2\\alpha-1)}\\frac{2\\alpha}{2\\alpha-1}\\geq(\\gamma B r)^{\\frac{2\\alpha-1}{2\\alpha}\\cdot\\frac{2\\alpha}{2\\alpha-1}}}\\\\ &{\\qquad\\Leftrightarrow\\quad d^{2\\alpha}\\geq(\\gamma B r).}\\end{array}\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "This is true and so we have that ", "page_idx": 67}, {"type": "equation", "text": "$$\n\\mathcal{K}(r)\\times\\sum_{s=\\tilde{M}/\\gamma B}^{r}\\mathcal{F}_{p p}(r)\\lesssim\\mathcal{F}_{p p}(r),\\qquad\\mathrm{~for~all~}r\\in[\\tilde{M}/(\\gamma B),M d^{2\\alpha}/(\\gamma B)].\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "For ${\\mathcal{F}}_{0}$ , with $r\\in[\\tilde{M}/(\\gamma B),M d^{2\\alpha}/(\\gamma B)]$ ", "page_idx": 67}, {"type": "equation", "text": "$$\n\\sum_{s=\\tilde{M}/(\\gamma B)}^{r}\\mathcal{F}_{0}\\lesssim(\\gamma B r)\\times d^{1-2\\beta-2\\alpha}\\times\\frac{1}{\\gamma B}\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "Therefore, we get that ", "page_idx": 67}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{K}(r)\\times\\displaystyle\\sum_{s=\\tilde{M}/(\\gamma B)}^{r}\\mathcal{F}_{0}(r)\\lesssim(\\gamma B r)\\times d^{1-2\\beta-2\\alpha}\\times\\gamma\\times(\\gamma B r)^{-2+1/(2\\alpha)}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\lesssim d^{-2\\beta}(\\gamma B r)^{-1+1/(2\\alpha)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "We will show that this is less than ${\\mathcal F}_{p p}$ . For this, we see ", "page_idx": 67}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d^{-2\\beta}(\\gamma B r)^{-1+1/(2\\alpha)}\\lesssim(\\gamma r B)^{-\\beta/\\alpha-1+1/(2\\alpha)}}\\\\ &{\\qquad\\qquad\\Leftrightarrow\\quad d^{-2\\beta}\\lesssim(\\gamma B r)^{-\\beta/\\alpha}}\\\\ &{\\qquad\\Leftrightarrow\\quad(\\gamma B r)^{\\beta/\\alpha}\\lesssim d^{2\\beta}}\\\\ &{\\qquad\\Leftrightarrow\\quad(\\gamma B r)\\lesssim d^{2\\alpha}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "Hence, we have that ", "page_idx": 67}, {"type": "equation", "text": "$$\n\\mathcal{K}(r)\\times\\sum_{s=\\tilde{M}/(\\gamma B)}^{r}\\mathcal{F}_{0}\\le\\mathcal{F}_{p p}(r),\\quad\\mathrm{for~all}\\;r\\in[\\tilde{M}/(\\gamma B),M d^{2\\alpha}/(\\gamma B)].\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "Since there is no $\\mathcal{F}_{a c}$ in this region, we immediately get from Corollary F.1 ", "page_idx": 67}, {"type": "equation", "text": "$$\n\\mathcal{K}(r)\\times\\sum_{s=\\tilde{M}/(\\gamma B)}^{r}\\mathcal{F}(r)\\lesssim\\mathcal{F}_{p p}(r),\\quad\\mathrm{for~all~}r\\in[\\tilde{M}/(\\gamma B),M d^{2\\alpha}/(\\gamma B)].\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "For $s\\in[0,\\tilde{M}/(\\gamma B)]$ , we have that $\\mathcal{F}(s)\\lesssim C$ . Thus we immediately get that ", "page_idx": 67}, {"type": "equation", "text": "$$\n\\mathcal{K}(r)\\times\\sum_{s=0}^{\\tilde{M}/(\\gamma B)}\\mathcal{F}(s)\\lesssim\\mathcal{K}(r)\\times\\frac{1}{\\gamma B},\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "for all $r$ . This proves the result for $2\\beta<1$ and $2\\alpha<1$ . ", "page_idx": 67}, {"type": "text", "text": "Consider $2\\beta<1$ and $2\\alpha>1$ . As in the previous case, we do not need to consider $\\mathcal{F}_{a c}$ as it does not exist here. The proof will be similar to the previous case. Choose $M$ and $\\tilde{M}$ so that both $\\mathcal{K}_{p p}$ and ${\\mathcal F}_{p p}$ are in their asymptotic regions and, using Proposition G.1, $\\mathcal{K}(r)\\asymp\\mathcal{K}_{p p}(r)$ . ", "page_idx": 68}, {"type": "text", "text": "First, by the same argument as in $2\\beta<1$ and $2\\alpha>1$ , we immediately have for $s\\in[0,\\tilde{M}/(\\gamma B)]$ , we have that $\\mathcal{F}(s)\\lesssim\\stackrel{}{C}$ , ", "page_idx": 68}, {"type": "equation", "text": "$$\n\\mathcal{K}(r)\\times\\sum_{s=0}^{\\tilde{M}/(\\gamma B)}\\mathcal{F}(s)\\lesssim\\mathcal{K}(r)\\times\\frac{1}{\\gamma B},\n$$", "text_format": "latex", "page_idx": 68}, {"type": "text", "text": "for all $r$ ", "page_idx": 68}, {"type": "text", "text": "As before, we have for any $r\\in[\\tilde{M}/(\\gamma B),M d^{2\\alpha}/(\\gamma B)]$ ", "page_idx": 68}, {"type": "equation", "text": "$$\n\\mathcal{K}(r)\\times\\sum_{s=\\tilde{M}/(\\gamma B)}^{r}\\mathcal{F}_{p p}(s)\\lesssim\\gamma\\times(\\gamma B r)^{-\\beta/\\alpha+1/(2\\alpha)}\\times(\\gamma B r)^{-2+1/(2\\alpha)}.\n$$", "text_format": "latex", "page_idx": 68}, {"type": "text", "text": "Note here that $\\gamma$ is constant. We will show that this is less than $\\mathcal{F}_{p p}(r)$ . Using the asymptotic for $\\mathcal{F}_{p p}(r)$ (Proposition H.2), let us suppose ", "page_idx": 68}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(\\gamma B r)^{-\\beta/\\alpha+1/(2\\alpha)}\\times(\\gamma B r)^{-2+1/(2\\alpha)}\\leq(\\gamma B r)^{-1-\\beta/\\alpha+1/(2\\alpha)}}\\\\ &{\\qquad\\qquad\\qquad\\Leftrightarrow\\quad0\\leq(\\gamma B r)^{1-1/(2\\alpha)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 68}, {"type": "text", "text": "Hence, we have that ", "page_idx": 68}, {"type": "equation", "text": "$$\n\\mathcal{K}(r)\\times\\sum_{s=\\tilde{M}/\\gamma B}^{r}\\mathcal{F}_{p p}(r)\\lesssim\\mathcal{F}_{p p}(r),\\qquad\\mathrm{~for~all~}r\\in[\\tilde{M}/(\\gamma B),M d^{2\\alpha}/(\\gamma B)].\n$$", "text_format": "latex", "page_idx": 68}, {"type": "text", "text": "For ${\\mathcal{F}}_{0}$ , with $r\\in[\\tilde{M}/(\\gamma B),M d^{2\\alpha}/(\\gamma B)]$ ", "page_idx": 68}, {"type": "equation", "text": "$$\n\\sum_{s=\\tilde{M}/(\\gamma B)}^{r}\\mathcal{F}_{0}\\lesssim(\\gamma B r)\\times d^{1-2\\beta-2\\alpha}\\times\\frac{1}{\\gamma B}.\n$$", "text_format": "latex", "page_idx": 68}, {"type": "text", "text": "Therefore, we get that ", "page_idx": 68}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathcal{K}(r)\\times\\sum_{s=\\tilde{M}/(\\gamma B)}^{r}\\mathcal{F}_{0}(r)\\lesssim(\\gamma B r)\\times d^{1-2\\beta-2\\alpha}\\times(\\gamma B r)^{-2+1/(2\\alpha)}}}\\\\ &{}&{\\lesssim d^{-2\\alpha+1-2\\beta}(\\gamma B r)^{-1+1/(2\\alpha)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 68}, {"type": "text", "text": "We will show that this is less than ${\\mathcal F}_{p p}$ . For this, we see ", "page_idx": 68}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d^{-2\\alpha+1-2\\beta}(\\gamma B r)^{-1+1/(2\\alpha)}\\lesssim(\\gamma r B)^{-\\beta/\\alpha-1+1/(2\\alpha)}}\\\\ &{\\qquad\\qquad\\Leftrightarrow\\quad d^{-2\\alpha+1-2\\beta}\\lesssim(\\gamma B r)^{-\\beta/\\alpha}}\\\\ &{\\qquad\\qquad\\Leftrightarrow\\quad(\\gamma B r)^{\\beta/\\alpha}\\lesssim d^{2\\beta+2\\alpha-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 68}, {"type": "text", "text": "Now we see that $(\\gamma B r)^{\\beta/\\alpha}\\lesssim d^{2\\beta}\\lesssim d^{2\\beta+2\\alpha-1}$ . Hence, we have that ", "page_idx": 68}, {"type": "equation", "text": "$$\n\\mathcal{K}(r)\\times\\sum_{s=\\tilde{M}/(\\gamma B)}^{r}\\mathcal{F}_{0}\\le\\mathcal{F}_{p p}(r),\\quad\\mathrm{for~all}\\;r\\in[\\tilde{M}/(\\gamma B),M d^{2\\alpha}/(\\gamma B)].\n$$", "text_format": "latex", "page_idx": 68}, {"type": "text", "text": "The result is thus shown in this case. ", "page_idx": 68}, {"type": "text", "text": "I Optimizing over batch and learning rate ", "text_level": 1, "page_idx": 68}, {"type": "text", "text": "The previous sections use batch size $B=1$ and the maximal learning rate allowed. In this section, we consider optimizing compute-optimal curves with respect to batch size and learning rate, i.e., find $d^{\\star},\\gamma^{\\star},B^{\\star}$ such that ", "page_idx": 68}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(d^{\\star},\\gamma^{\\star},B^{\\star})\\in\\arg\\operatorname*{min}_{d,\\gamma,B}\\in\\arg\\operatorname*{min}\\!\\mathcal{P}(\\frac{\\dag}{d B},d,\\gamma)\\quad\\mathrm{s.t.~}\\gamma B<1\\mathrm{~and~}\\|\\mathcal{K}_{p p}\\|<1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 68}, {"type": "text", "text": "I.1 Optimal batch size ", "text_level": 1, "page_idx": 69}, {"type": "text", "text": "We see that batch essentially scales out of the problem and therefore the batch has no effect on the compute-optimal curves. To see this, we observe from Table 5 that ", "page_idx": 69}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{F}_{p p}(r)\\asymp(\\gamma B r)^{-(1+\\beta/\\alpha)+1/(2\\alpha)}\\quad\\Rightarrow\\quad\\mathcal{F}_{p p}(\\frac{\\mathfrak{f}}{d B})\\asymp(\\frac{\\gamma\\mathfrak{f}}{d})^{-(1+\\beta/\\alpha)+1/(2\\alpha)}}\\\\ &{\\mathcal{F}_{a c}(r)\\asymp(\\gamma B r)^{-1+1/(2\\alpha)}\\times d^{-1}\\quad\\Rightarrow\\quad\\mathcal{F}_{a c}(\\frac{\\mathfrak{f}}{d B})\\asymp(\\frac{\\gamma\\mathfrak{f}}{d})^{-1+1/(2\\alpha)}\\times d^{-1}}\\\\ &{\\frac{1}{\\gamma B}\\mathcal{K}_{p p}(r)\\asymp\\gamma(\\gamma B r)^{-2+1/(2\\alpha)}\\quad\\Rightarrow\\quad\\frac{1}{\\gamma B}\\mathcal{K}_{p p}(\\frac{\\mathfrak{f}}{d B})\\asymp\\gamma(\\frac{\\gamma\\mathfrak{f}}{d})^{-2+1/(2\\alpha)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 69}, {"type": "text", "text": "It immediately follows that the batch size has no effect on the compute-optimal curves. Therefore any batch size (e.g., $B=1$ ) that satisfies the necessary and sufficient condition for convergence (Prop. C.2), that is, $\\gamma B<1$ , will yield the same compute-optimal curves. ", "page_idx": 69}, {"type": "text", "text": "This is not necessarily true for the learning rate as we will see in the next section. ", "page_idx": 69}, {"type": "text", "text": "I.2 Optimal learning rate ", "text_level": 1, "page_idx": 69}, {"type": "text", "text": "Without loss of generality, we let the batch size $B=1$ . From the expressions in (95), we see that ${\\mathcal F}_{p p}$ and $\\mathcal{F}_{a c}$ are monotonically decreasing in learning rate $\\gamma$ . Moreover in Phase III, $\\scriptstyle{\\frac{1}{\\gamma B}}\\mathcal{K}_{p p}$ , is also monotonically decreasing in the learning rate. Therefore, in Phases I, II, and III, the optimal learning rate choice is to choose $\\gamma$ maximally. In the cases of Phase Ia, II, and III, this would mean $\\gamma$ constant (see Prop C.2) and in Phase Ib, Ic, $\\dot{\\gamma}\\sim d^{2\\alpha-1}$ . It remains to understand the effect of the learning rate in Phase IV. ", "page_idx": 69}, {"type": "text", "text": "From Proposition C.6, we know that the loss is given by ", "page_idx": 69}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{P}(\\frac{\\mathfrak{f}}{d},d,\\gamma)\\asymp\\mathcal{F}_{0}(\\frac{\\mathfrak{f}}{d})+\\mathcal{F}_{p p}(\\frac{\\mathfrak{f}}{d})+\\frac{1}{\\gamma B}\\mathcal{K}_{p p}(\\frac{\\mathfrak{f}}{d})\\asymp d^{-2\\alpha}+(\\frac{\\gamma\\mathfrak{f}}{d})^{\\rho}+\\gamma(\\frac{\\gamma\\mathfrak{f}}{d})^{-2+1/(2\\alpha)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 69}, {"type": "text", "text": "By taking derivatives, we see that ", "page_idx": 69}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\gamma^{\\star}\\asymp(\\frac{\\mathfrak{f}}{d^{\\star}})^{\\alpha/\\beta-1}\\quad\\mathrm{and}\\quad d^{\\star}(\\mathfrak{f})\\asymp\\mathfrak{f}^{\\rho/(\\rho-2\\beta)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 69}, {"type": "text", "text": "We need to check that $\\gamma^{\\star}$ is feasible, i.e., $\\gamma^{\\star}<1$ (which it is) and $\\gamma^{\\star}<d^{2\\alpha-1}$ . For the later, a simple check shows that ", "page_idx": 69}, {"type": "equation", "text": "$$\n\\gamma^{\\star}\\asymp d^{\\frac{4\\alpha^{2}-4\\alpha\\beta}{2\\alpha+2\\beta-1}}<d^{2\\alpha-1}\n$$", "text_format": "latex", "page_idx": 69}, {"type": "text", "text": "when $\\alpha>1/4$ and $2\\beta>1$ , i.e., precisely Phase IV. The compute-optimal curve in Phase IV with optimal stepsize is the following. ", "page_idx": 69}, {"type": "text", "text": "Proposition I.1 (Phase IV, optimal $\\gamma$ , compute-optimal curve). Suppose $1/4<\\alpha<1/2$ and $2\\beta>1$ . Then ", "page_idx": 69}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\gamma^{\\star}\\asymp\\mathfrak{f}^{\\frac{4\\alpha(\\alpha-\\beta)}{4\\alpha\\beta+2\\alpha+2\\beta-1}},\\quad\\mathfrak{d}^{\\star}(\\mathfrak{f})\\asymp\\mathfrak{f}^{\\frac{2\\alpha+2\\beta-1}{4\\alpha\\beta+2\\alpha+2\\beta-1}},\\quad a n d\\quad\\mathfrak{P}^{\\star}(\\mathfrak{f})\\asymp\\mathfrak{f}^{\\frac{-2\\alpha(2\\alpha+2\\beta-1)}{4\\alpha\\beta+2\\alpha+2\\beta-1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 69}, {"type": "text", "text": "The trade off occurring where $\\begin{array}{r}{\\frac{1}{\\gamma B}\\mathcal{K}_{p p}=\\mathcal{F}_{0}}\\end{array}$ . ", "page_idx": 69}, {"type": "text", "text": "We note that there is only one Phase IV (and no sub-phases). ", "page_idx": 69}, {"type": "text", "text": "J Experimental Results ", "text_level": 1, "page_idx": 69}, {"type": "text", "text": "To measure the exponents of the scaling law and parameter count, we follow approach12 1 and 2 from [23]. We explain the method below using $\\bar{(\\alpha,\\beta)}=(0.5,0.7)$ as an example. The theoretical prediction of the scaling law and parameter count exponents for this example are $\\eta=0.5$ and $\\xi=0.5$ , resp. (see Table 2). We then repeat this procedure for total of 32 pairs of $(\\alpha,\\beta)$ in the phase diagram; see Fig. 15 and Fig. 16. The theoretical predictions of these two exponents are shown in the heatmaps Fig. 8. ", "page_idx": 69}, {"type": "text", "text": "First, we run SGD for parameter counts ", "page_idx": 69}, {"type": "equation", "text": "$$\nd\\in[200,300,400,600,800,1200,1600,2400,3200,4800,6400,9600,12800].\n$$", "text_format": "latex", "page_idx": 69}, {"type": "text", "text": "The SGD learning curves for $(\\alpha,\\beta)=(0.5,0.7)$ with parameters $d\\in[800,1600,3200,6400,12800]$ are shown in Fig. 9a. ", "page_idx": 69}, {"type": "image", "img_path": "aVSxwicpAk/tmp/d9921ebd8931c6ab4e8171aeb5a3fdf2f3b79977952365e755a9e0cdb7d3fcc5.jpg", "img_caption": ["Figure 8: Theoretical predictions of parameter count and scaling law exponents. "], "img_footnote": [], "page_idx": 70}, {"type": "text", "text": "J.1 Measuring the Scaling Law Exponent ", "text_level": 1, "page_idx": 70}, {"type": "text", "text": "We follow $_\\mathrm{Sec}\\ 3.1$ in [23]. First, we choose an IsoFLOP window $[\\mathfrak{f}_{\\mathrm{min}},\\mathfrak{f}_{\\mathrm{max}}]$ and construct $\\mathfrak{f}_{\\boldsymbol{j}}$ \u2019s using a geometric spacing between $\\up f_{1}\\,=\\,\\up f_{\\mathrm{min}}$ and $\\up f_{n}\\,=\\,\\up f_{\\mathrm{max}}$ . For each IsoFLOP slice, ${\\mathfrak{f}}_{j}$ , (e.g., $\\mathsf{f}_{j}\\,=\\,2e7$ is the vertical line in Fig. 9a), we find the minimum loss across all $d$ . We denote this minimum value by $\\mathcal{P}^{\\star}(\\mathfrak{f}_{j})$ and the associated optimal parameter by $d^{\\star}(\\mathfrak{f}_{j})$ . As an example, in Fig. 9a, $\\mathcal{P}^{\\star}(\\mathfrak{f}_{j})=1.6e\\mathrm{~\\bar{-}~3~}$ and the associated optimal parameter $d^{\\star}(\\mathfrak{f}_{j})=64\\dot{0}\\dot{0}$ . ", "page_idx": 70}, {"type": "text", "text": "We obtain the compute-optional frontier (highlighted in red in Fig. 9b) by plotting ", "page_idx": 70}, {"type": "equation", "text": "$$\n[(\\mathfrak{f}_{j},\\mathcal{P}^{\\star}(\\mathfrak{f}_{j})]_{1\\leq j\\leq n},\n$$", "text_format": "latex", "page_idx": 70}, {"type": "text", "text": "and the optimal parameter count ", "page_idx": 70}, {"type": "equation", "text": "$$\n[(\\mathfrak{f}_{j},d^{\\star}(\\mathfrak{f}_{j})]_{1\\leq j\\leq n}.\n$$", "text_format": "latex", "page_idx": 70}, {"type": "text", "text": "We then fit a power-law curve $\\mathcal{P}^{\\star}(\\mathfrak{f})=a\\times\\mathfrak{f}^{-\\hat{\\eta}}$ to predict the relationship between the compute $\\mathfrak{f}$ and optimal loss ${\\mathcal{P}}^{\\star}$ . This is shown as the dashed line in Fig. 9b. For $(\\alpha,\\bar{\\beta})=(0.5,0.7)$ , this gives ", "page_idx": 70}, {"type": "equation", "text": "$$\n\\mathcal{P}^{\\star}(\\mathfrak{f})=22.61\\times\\mathfrak{f}^{-0.515},\n$$", "text_format": "latex", "page_idx": 70}, {"type": "text", "text": "whereas our theoretical result predicts ", "page_idx": 70}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{P}^{\\star}(\\mathfrak{f})\\asymp\\mathfrak{f}^{-0.5}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 70}, {"type": "text", "text": "J.2 Measuring Parameter Count Exponent: Approach 0 ", "text_level": 1, "page_idx": 70}, {"type": "text", "text": "One benefit of our theoretical framework is that the solution of the Volterra equation (eq. 10) is deterministic. As such, precise numerical evaluation can determine the instantaneous slope of the compute-optimal curves using a new approach that is not necessarily feasible when dealing with noisy ", "page_idx": 70}, {"type": "image", "img_path": "aVSxwicpAk/tmp/d758188e71b5ab0b5e91eb2057b9e85b20095786ab7b291c41334ef5298d68af.jpg", "img_caption": ["Figure 9: Measuring the scaling law exponent for $(\\alpha,\\beta)=(0.5,0.7)$ . ", "(b) Compute-Optimal Frontier "], "img_footnote": [], "page_idx": 70}, {"type": "image", "img_path": "aVSxwicpAk/tmp/e1d51eea82c0d8f76995e6393b65aa49df0e4eba4f312cfc2ecd276416adc440.jpg", "img_caption": ["(a) IsoFLOP Window [1e6, 1e8] "], "img_footnote": [], "page_idx": 71}, {"type": "image", "img_path": "aVSxwicpAk/tmp/0e2e118c18b539243524143282f12ccda7ae2391585777c2fb47f78f3ca49805.jpg", "img_caption": ["(b) IsoFLOP Window [2e6, 0.5e8] "], "img_footnote": [], "page_idx": 71}, {"type": "text", "text": "Figure 10: 2 different IsoFLOP windows for measuring the parameter count exponent with Approach 1 for $(\\alpha,\\beta)=(0.5,0.7)$ . ", "page_idx": 71}, {"type": "text", "text": "SGD curves. Specifically, we search for the unique tangent line that intersects the loss-versus-flops curves for two adjacent values of $d$ , i.e. we numerically solve the following system for $f_{1}$ and $f_{2}$ : ", "page_idx": 71}, {"type": "equation", "text": "$$\nP_{1}^{\\prime}(f_{1})=P_{2}^{\\prime}(f_{2})=\\frac{P_{2}(f_{2})-P_{1}(f_{1})}{f_{2}-f_{1}}\\,,\n$$", "text_format": "latex", "page_idx": 71}, {"type": "text", "text": "where $P_{1}$ is the loss curve for $d=d_{1}$ and $P_{2}$ is the loss curve for $d=d_{2}$ . When $d_{1}$ and $d_{2}$ are close, we obtain an accurate estimate of the parameter count exponent by measuring the discrete logarithmic derivative, $(\\log(d_{2})-\\log(d_{1}))/(\\log(f_{2}^{*})-\\log(f_{1}^{*}))$ . ", "page_idx": 71}, {"type": "text", "text": "J.3 Measuring Parameter Count Exponent: Approach 1 ", "text_level": 1, "page_idx": 71}, {"type": "text", "text": "To predict the optimal parameter count exponent, we fit the function $d^{\\star}=a\\times\\mathfrak{f}^{b}$ , $a,b$ constants, to the measurements in (97) (see e.g., Fig. 10a). For the example $(\\alpha,\\beta)=(0.5,0.7)$ (Fig. 10a), this approach gives ", "page_idx": 71}, {"type": "equation", "text": "$$\nd^{\\star}=.25\\times\\mathfrak{f}^{0.551}.\n$$", "text_format": "latex", "page_idx": 71}, {"type": "text", "text": "Note that the fti of the exponent is very sensitive to the choice of IsoFLOP window. When we change the window from $[1e6,1e8]$ to $[2e6,0.5e8]$ , the parameter count exponent changes from 0.51 to 0.58, as shown in Fig.10b. The theoretical prediction of this exponent is 0.5. ", "page_idx": 71}, {"type": "text", "text": "J.4 Measuring Parameter Count Exponent: Approach 2 ", "text_level": 1, "page_idx": 71}, {"type": "text", "text": "For each IsoFLOP slice, ${\\mathfrak{f}}_{j}$ , we obtain a set of training loss values depending on $d$ , $\\{{\\mathcal{P}}(\\mathfrak{f}_{j},d_{i})\\}_{1\\leq i\\leq m}$ . In our running example, $\\bar{d}_{1}=200$ and $d_{m}=12800$ (see Fig. 9a). We then fit a parabola (quadratic ", "page_idx": 71}, {"type": "image", "img_path": "aVSxwicpAk/tmp/276cae9f499f2a1e889a255b2da4a49415dc65c43a6ffcd76fd1ebc7778e6f1d.jpg", "img_caption": ["(a) IsoFLOP Quadratic fit "], "img_footnote": [], "page_idx": 71}, {"type": "image", "img_path": "aVSxwicpAk/tmp/5155853ff0d3d963f6ef5a8245095f9f8c3f74ded3429d97aafcc9bd9f62f860.jpg", "img_caption": ["Figure 11: Measuring parameter count exponent with Approach 2 for $(\\alpha,\\beta)=(0.5,0.7)$ . ", "(b) Approach 2 "], "img_footnote": [], "page_idx": 71}, {"type": "image", "img_path": "aVSxwicpAk/tmp/96572dc553beb7200aaa5313e88e2ca5a2dc8c162907e27855967d7a655a3e26.jpg", "img_caption": [], "img_footnote": [], "page_idx": 72}, {"type": "text", "text": "Figure 12: The empirical measurements of the scaling law exponent and parameter count exponent are sensitive to the choice of the IsoFLOP windows. Top: larger IsoFLOP window $[1e6,5e8]$ Bottom: smaller IsoFLOP window $[1e6,1e8]$ . ", "page_idx": 72}, {"type": "text", "text": "function) to $\\{(\\log\\mathcal{P}(\\mathfrak{f}_{j},d_{i}),\\log d_{i})\\}_{1\\leq i\\leq m}$ , i.e., we find $(a,b,c)$ such that ", "page_idx": 72}, {"type": "equation", "text": "$$\n\\log\\mathcal{P}(\\mathfrak{f}_{j},d_{i})=a\\log^{2}d_{i}+b\\log d_{i}+c.\n$$", "text_format": "latex", "page_idx": 72}, {"type": "text", "text": "This is shown in Fig. 11a. After solving for $(a,b,c)$ , we find the $d^{\\star}(f_{j})$ that minimizes $a^{2}\\log^{2}d+$ $b\\log d+c$ . Repeating this procedure for all $\\mathfrak{f}_{\\boldsymbol{j}}$ \u2019s gives a set of pairs $\\{(\\widetilde{\\mathfrak{f}_{j}},\\mathfrak{d}_{j}^{\\star})\\}_{1\\le j\\le n}$ . In the final step, we power-law fit this set. For the example $(\\alpha,\\beta)=(0.5,0.7)$ (see Fig. 11b), this gives ", "page_idx": 72}, {"type": "equation", "text": "$$\nd^{\\star}=0.17\\times\\mathfrak{f}^{0.565}.\n$$", "text_format": "latex", "page_idx": 72}, {"type": "text", "text": "J.5 Exponents comparison: Theory vs Measurement ", "text_level": 1, "page_idx": 72}, {"type": "text", "text": "We compare the empirical measurements of the exponents against their theoretical predictions in Fig. 12. We chose three slices across the phase diagram ", "page_idx": 72}, {"type": "text", "text": "1. $\\alpha=0.7$ Slice (Fig. 12a), in which $(\\alpha,\\beta)$ goes from Phase Ia, II and III.   \n2. $\\alpha=0.27$ Slice (Fig. 12b), in which $(\\alpha,\\beta)$ goes from Phase Ib to Phase IVb.   \n3. $\\beta=0.7$ Slice (Fig. 12c)\u201e in which $(\\alpha,\\beta)$ goes from Phase Ic, IVb, IVa, III and to II. ", "page_idx": 72}, {"type": "text", "text": "For the scaling law exponents, the empirical measurement agrees with the theoretical prediction quite well. For the parameter count, the agreement is good but not as good as that of the scaling law exponents. Noticeably, there is disagreement between Approach 1 and Approach 2. Such disagreement is not surprising, as empirical measurements are sensitive to the choice of the IsoFLOP windows and we use the same IsoFLOP window $[1e6,5e8]$ for all $(\\alpha,\\beta)$ . This is clearly suboptimal. We briefly discuss this in the next subsection. ", "page_idx": 72}, {"type": "text", "text": "J.6 Instantaneous slope ", "text_level": 1, "page_idx": 72}, {"type": "text", "text": "In this section, we demonstrate that there can be strong finite-size $d$ effects in the measurements of the scaling law and parameter count exponents. We measure the instantaneous slope as a function of parameter count $d$ for the Volterra equation (10). See Fig. 13. To do so, we generate the Volterra solutions for a geometrically spaced sequence of $d$ \u2019s with ratio 1.05. We then apply Approach 1 with a very dense IsoFLOP window (100 IsoFLOP Slices between $\\left[2e4,2e7\\right])$ . We then slide a smaller IsoFLOP window (20 IsoFLOP slices) from left to right to generate a sequence of scaling law (parameter count) exponent, as shown in the middle (right) plot in Fig. 13. These exponents varying slowly when the window slides from a small flops regime to a large flops regime. For example, the scaling law exponent $\\eta$ changes from $\\eta=0.440$ to $\\eta=0.413$ from left to right, while the parameter count exponent changes from $\\xi=0.450\\rightarrow0.575.$ . Using the global window (100 IsoFLOP) to measure these exponents, we have $\\eta=0.42$ and $\\xi=0.52$ which are very close to their average over all small windows: $\\eta=0.418$ and $\\xi=0.526$ . ", "page_idx": 72}, {"type": "image", "img_path": "aVSxwicpAk/tmp/212005ea9b4eaa8a83cc85566e356e82b3512c6b512ef428acb091972e7458df.jpg", "img_caption": ["Figure 13: Instantaneous Slope. (Left) Volterra equation (10) dynamics for a highly dense grid of $d$ . We also plot the compute-optimal front obtained from using the left small window (small flops regime) and the right window (larger flops regime). (Middle) Shows the evolving measurements of scaling exponents when the flops increases. (Right) Shows the evolving measurements of parameter count exponents when the flops increases. "], "img_footnote": [], "page_idx": 73}, {"type": "text", "text": "J.7 Negative $\\beta$ . ", "text_level": 1, "page_idx": 73}, {"type": "text", "text": "In Fig. 14, we show that our theoretical results work well for $\\beta<0$ . ", "page_idx": 73}, {"type": "image", "img_path": "aVSxwicpAk/tmp/ee7ef7bbceee4396331bfe293b1de47aca51a85f0206a2896926e3395cddab21.jpg", "img_caption": ["Figure 14: Negative $\\beta$ . Sweeping $\\beta=-0.2$ to $\\beta=0.2$ . We see good agreement between Volterra (theory) dynamics and SGD dynamics. "], "img_footnote": [], "page_idx": 73}, {"type": "text", "text": "J.8 Additional plots for different phases ", "text_level": 1, "page_idx": 73}, {"type": "text", "text": "We summarize the measurement of the scaling law exponents and optimal parameter count exponents in Fig. 15 and Fig. 16, resp. ", "page_idx": 73}, {"type": "image", "img_path": "aVSxwicpAk/tmp/979262e6136d8b542fe21715dae5005dad3d8c65e1245360f88aec109a2120f0.jpg", "img_caption": ["Figure 15: Theory vs. empirical scaling law across different phases. "], "img_footnote": [], "page_idx": 74}, {"type": "image", "img_path": "aVSxwicpAk/tmp/f811fb98db4247bc0674e55a37383ba8a9b008914c5d6d1d9dc3ff896dcfbeb8.jpg", "img_caption": ["Figure 16: Theory vs. empirical optimal parameter count across different phases. "], "img_footnote": [], "page_idx": 75}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 76}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 76}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 76}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 76}, {"type": "text", "text": "Justification: All the statements stated in the abstract and in the introduction are proven explicitly in the remainder of the paper \u2013 with most proven in the Appendix. We clearly state that we are studying a simple model with the exact description described in Section 1.1. ", "page_idx": 76}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 76}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 76}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 76}, {"type": "text", "text": "Justification: We clearly state in Section 1.1 that we are working on a simple quadratic power law random features model with assumptions on the data and targets. The algorithmic set-up follows that section. We indicate that we are only considering fixed stepsize SGD. All the results are proven in the Appendix. We also make note that we only prove our main result for $\\alpha>1/4$ . ", "page_idx": 76}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 76}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 76}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 76}, {"type": "text", "text": "Justification: All theorems and results are proven in the Appendix. We make sure to clearly state any assumptions and limitations. For example, we do not prove the setting for $\\alpha<{\\textstyle{\\frac{\\mathrm{i}}{4}}}$ , but we clearly state this (see e.g., remark after Theorem 2.1). ", "page_idx": 76}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 76}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 76}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 76}, {"type": "text", "text": "Justification: In each of the figures, we provide an explicit description of how the image was generated including the numerical set-up. In this case, the experiments are simple and they do not require significant coding or datasets. This is mainly a theory paper and the data was synthetically generated. The model has already been used before in other papers and the data comes from Gaussians. This is all clearly stated in our set-up, Section 1.1. We intend to release the code for the numerical computation of the Volterra equation and how it matches the loss curves under SGD. ", "page_idx": 76}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 76}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 76}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 76}, {"type": "text", "text": "Justification: The paper does not include experiments that require significant code. The model we analyze is a simple random features model with power law applied to synthetic data. As such, the code can be readily reproduced by following the set-up seen in the caption of the figures. We intend to release the code for the numerical computation of the Volterra equation and how it matches the loss curves under SGD. ", "page_idx": 76}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 76}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 76}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 76}, {"type": "text", "text": "Justification: The experimental design, including the power law exponents, fixed stepsizes, choices of $d$ and $v$ , and the numerical simulations for solving Volterra equations are all written in the captions of the figures. We also intend to release the code for numerically computing the Volterra equation. ", "page_idx": 77}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 77}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 77}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 77}, {"type": "text", "text": "Justification: We provide (and explain) in the figures/captions the error bars across multiple runs of SGD. We record how we generate the empirical compute-optimal exponents using statistical tools that were first deployed in other papers such as [23]. We are careful to explain when and why the theory deviates from the numerical simulations. Often this is due to finite $d$ and $v$ effects and the slow behavior of the theory to the asymptotics. ", "page_idx": 77}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 77}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 77}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 77}, {"type": "text", "text": "Justification: As this paper is about compute-optimal curves, we provide details on the exact number of flops required to perform the experiments. Since the model we study is a simple least squares model, the compute resources are also well known in by the community. ", "page_idx": 77}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 77}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 77}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 77}, {"type": "text", "text": "Justification: We have reviewed the code of ethics. We have made our utmost attempt to adhere to the guidelines provided by NeurIPS. We do not use any human subjects nor any datasets. We did our best to cite all the relevent related work. Given that our work is in the foundational research, it is difficult to mitigate all the risks as the downstream effects of theory are long, but we have done our best. The model is completely synthetic using the standard SGD algorithm; thus we don\u2019t, to the best of our knowledge, anticipate any risks. ", "page_idx": 77}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 77}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 77}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 77}, {"type": "text", "text": "Justification: The work presented is purely foundational research and is not directly tied to any particular application. We study a simple random features model with power law data and target and we solve the model using a common algorithm SGD. Given the theoretical nature of this work, we do not anticipate any direct ethical and societal issues. See our Broader Impact Statement in the appendix. ", "page_idx": 77}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 77}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 77}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 77}, {"type": "text", "text": "Justification: The paper poses no such risks. The work is purely theoretical and uses only synthetic data generated from a normal distribution. The models employed are standard statistical model (e.g., least squares) which are textbook learning problems. ", "page_idx": 77}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 77}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 77}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 78}, {"type": "text", "text": "Justification: We acknowledge via citations that the model we study was introduced before by others (e.g., Maloney, Roberts, and Sully paper). We are not using any datasets or other assets beyond numpy and thus do not need to name any license or cite any dataset. ", "page_idx": 78}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 78}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 78}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 78}, {"type": "text", "text": "Justification: We do not intend to release any new assets. The work is a theoretical analysis of SGD on a random features model. ", "page_idx": 78}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 78}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 78}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 78}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects.   \nThe work is purely theoretical on a simple model. ", "page_idx": 78}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 78}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 78}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 78}, {"type": "text", "text": "Justification: The work does not involve crowdsourcing nor research with human subjects.   \nThe data used in this work is generated synthetically. ", "page_idx": 78}]