{"references": [{"fullname_first_author": "Mohammad Gheshlaghi Azar", "paper_title": "Minimax regret bounds for reinforcement learning", "publication_date": "2017-00-00", "reason": "This paper provides foundational minimax regret bounds for reinforcement learning, which are crucial for understanding the sample complexity in RL."}, {"fullname_first_author": "Christoph Dann", "paper_title": "Unifying pac and regret: Uniform pac bounds for episodic reinforcement learning", "publication_date": "2017-00-00", "reason": "This paper unifies PAC and regret analyses, providing a framework for analyzing the sample complexity of reinforcement learning algorithms."}, {"fullname_first_author": "R\u00e9my Degenne", "paper_title": "Gamification of pure exploration for linear bandits", "publication_date": "2020-00-00", "reason": "This paper introduces a novel gamification approach to pure exploration in linear bandits, which is relevant to the pure exploration problem studied in this paper."}, {"fullname_first_author": "Andrew Wagenmaker", "paper_title": "Instance-dependent near-optimal policy identification in linear mdps via online experiment design", "publication_date": "2022-00-00", "reason": "This paper establishes instance-dependent bounds for policy identification in linear MDPs, which are closely related to the results presented in this paper."}, {"fullname_first_author": "Zhaoqi Li", "paper_title": "Instance-optimal PAC algorithms for contextual bandits", "publication_date": "2022-00-00", "reason": "This paper provides instance-optimal algorithms for contextual bandits, which is a related problem to the tabular RL setting considered in this paper."}]}