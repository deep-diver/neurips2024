[{"figure_path": "eSes1Mic9d/figures/figures_1_1.jpg", "caption": "Figure 1: Layerwise effects of applying persona steering vectors (via CAA, contrastive activation addition) with either a positive (CAA+) or negative (CAA-) multiplier. Y-axis indicates the percentage of attacks to which the model provided a response. [Left] Inducing the model to believe the user has pro-social attributes (curious, altruistic, power-avoidant, and law-abiding personas) makes it more likely to divulge sensitive information. [Right] Results for anti-social (close-minded, selfish, power-seeking, and unlawful) personas indicate the reverse is also true and to stronger effect (e.g. applying the negation of a vector that induces the model to believe the user is selfish results in a response rate of 52%). Layer 13 tends to be where CAAs are most effective (and the divergence between CAA+ and CAA- is strongest), perhaps because by layer 13 input processing is mostly complete, but the model has not fully turned to next token prediction.", "description": "This figure displays the layer-wise effects of applying persona steering vectors using contrastive activation addition (CAA) with positive (CAA+) and negative (CAA-) multipliers.  The y-axis represents the percentage of adversarial attacks to which the model responded. The left panel shows that pro-social personas increase the likelihood of the model revealing sensitive information, while the right panel demonstrates that anti-social personas have the opposite effect, even more strongly.  The figure highlights the effectiveness of CAA around layer 13, suggesting this is where the method most successfully bypasses safety filters.", "section": "2 Manipulating user persona to induce refusal"}, {"figure_path": "eSes1Mic9d/figures/figures_3_1.jpg", "caption": "Figure 2: Y-axis indicates the percent difference in response rate to adversarial attacks compared to Baseline Prompting (0.39) for a selection of personas across treatments: (1) PP (prompted prefixes), (2) CAA+ (steering vector applied at layer 13), and (3) CAA- (steering vector applied with a negative multiplier). We also indicate the difference in response rate for early decoding at layer 13 (ED13).", "description": "This figure shows the percentage change in the response rate to adversarial queries for various personas and methods compared to a baseline.  The methods include adding prosocial or antisocial prompt prefixes (PP),  applying contrastive activation addition (CAA) with a positive or negative multiplier at layer 13, and using early decoding at layer 13 (ED13). The x-axis represents different personas (pro-social, antisocial, political affiliations, gender, and direct refusal/fulfillment prompts), and the y-axis represents the percentage change in response rate relative to the baseline.", "section": "2.2 Results"}, {"figure_path": "eSes1Mic9d/figures/figures_3_2.jpg", "caption": "Figure 3: Heatmap with personas and treatments along the x-axis, and different attack categories along the y-axis. Color indicates the response rate (green: 0% response rate to grey: 30% response rate as baselines to dark blue: 100% response rate.) We observe a stark contrast between non-adversarial and adversarial queries when applying different interventions. Specifically, steering with CAA+/CAA- selectively affects responsiveness to adversarial queries, while prompt prefixes tend to induce refusals across the board.", "description": "This heatmap visualizes the response rates of a language model to different types of adversarial queries under various conditions.  The x-axis shows different user personas (pro-social, anti-social, demographic groups) and intervention methods (baseline prompting, prompt prefixes, activation steering with positive and negative multipliers). The y-axis represents categories of adversarial queries (misinformation, hate speech, etc.). The color intensity represents the percentage of times the model responded to the query (darker colors = higher response rate).  The figure demonstrates the significant impact of user persona on the model's willingness to respond to dangerous queries, highlighting the effectiveness of activation steering compared to prompt prefixes.", "section": "2.2 Manipulating user persona to induce refusal"}, {"figure_path": "eSes1Mic9d/figures/figures_6_1.jpg", "caption": "Figure 4: Cosine similarity between refusal and fulfillment steering vectors across layers. Similarity is highest at first, decreases up to layer 15, then increases again until stabilizing around layer 27.", "description": "This figure plots the cosine similarity between the 'refusal' and 'fulfillment' steering vectors across different layers of a language model.  Cosine similarity is a measure of the angle between two vectors, with a value of 1 indicating perfect similarity and 0 indicating no similarity. The plot shows that the similarity is high in the initial layers (closer to 1), indicating that the vectors representing refusal and fulfillment are very similar at the beginning of the processing.  As the processing progresses through the layers, the similarity decreases, reaching a minimum around layer 15, suggesting that the model begins to distinguish between refusal and fulfillment at this point. However, in later layers, the similarity increases again and eventually stabilizes, potentially because the model is focusing more on next-token prediction and less on semantic distinctions between refusal and fulfillment.", "section": "3 Mechanics of latent misalignment"}, {"figure_path": "eSes1Mic9d/figures/figures_7_1.jpg", "caption": "Figure 5: Pairwise cosine similarity between persona vectors across layers. [Top] All rows (columns) represent pro-social personas paired such that consecutive rows (columns) represent semantically similar personas, but one vector is trained to predict \u2018Yes\u2019 and the other \u2018No\u2019. Vectors predicting \u2018Yes\u2019 have higher cosine similarity than vectors predicting \u2018No\u2019, regardless of their semantic content, forming a checkerboard pattern. This effect is present in early layers (5), exaggerates by mid layers (13), and slightly decreases in the later layers. [Bottom] Top rows (right columns) contain anti-social personas leading to an increased refusal rate, and the bottom rows (left columns) contain pro-social personas. All vectors are trained to predict \u2018Yes\u2019. No separation is visible in early layers. Separation emerges in mid layers (13) and by later layers (27) two distinct clusters are visible.", "description": "This figure displays pairwise cosine similarity between persona vectors across different layers (5, 13, 27). The top half shows pro-social and anti-social persona pairs where one predicts 'yes' and the other 'no'. A checkerboard pattern emerges due to higher similarity between 'yes' vectors. The bottom half shows only 'yes' vectors, revealing a clear separation between pro-social and anti-social personas in later layers.", "section": "3 Mechanics of latent misalignment"}, {"figure_path": "eSes1Mic9d/figures/figures_9_1.jpg", "caption": "Figure 6: [Left] Probability difference between 'good' and 'bad' given the target prompt \u2018[INST] [X] [/INST] The user's motives for asking this question are'. Values are offset by baseline probabilities (no Steering vectors applied). [Right] Rank difference between 'good' and 'bad' for next token prediction offset by the rank difference in the Baseline Prompting condition.", "description": "This figure shows the results of an experiment using Patchscopes to analyze how steering vectors impact the model's interpretation of adversarial queries.  The left panel displays the probability difference between 'good' and 'bad' interpretations across different layers of the model for two persona conditions (selfish with positive and negative multipliers). The right panel shows the rank difference between 'good' and 'bad' interpretations, again across layers and for the same persona conditions.  The differences are calculated relative to a baseline condition with no steering vector applied. The figure illustrates how the model's charitable interpretation of queries changes depending on the persona and the layer of the model being examined.", "section": "3 Mechanics of latent misalignment"}, {"figure_path": "eSes1Mic9d/figures/figures_14_1.jpg", "caption": "Figure 1: Layerwise effects of applying persona steering vectors (via CAA, contrastive activation addition) with either a positive (CAA+) or negative (CAA-) multiplier. Y-axis indicates the percentage of attacks to which the model provided a response. [Left] Inducing the model to believe the user has pro-social attributes (curious, altruistic, power-avoidant, and law-abiding personas) makes it more likely to divulge sensitive information. [Right] Results for anti-social (close-minded, selfish, power-seeking, and unlawful) personas indicate the reverse is also true and to stronger effect (e.g. applying the negation of a vector that induces the model to believe the user is selfish results in a response rate of 52%). Layer 13 tends to be where CAAs are most effective (and the divergence between CAA+ and CAA- is strongest), perhaps because by layer 13 input processing is mostly complete, but the model has not fully turned to next token prediction.", "description": "This figure shows the layer-wise effects of applying persona steering vectors using contrastive activation addition (CAA) with positive and negative multipliers.  The y-axis represents the percentage of adversarial attacks that elicited a response from the model. The left panel demonstrates that pro-social personas increase the likelihood of the model divulging sensitive information, while the right panel shows that anti-social personas have the opposite effect, even more strongly. Layer 13 is identified as the layer where CAA interventions are most effective. This suggests that the model's judgment of the user (persona) significantly influences its response to adversarial queries.", "section": "2 Willingness to answer adversarial queries depends on user persona"}, {"figure_path": "eSes1Mic9d/figures/figures_14_2.jpg", "caption": "Figure 2: Y-axis indicates the percent difference in response rate to adversarial attacks compared to Baseline Prompting (0.39) for a selection of personas across treatments: (1) PP (prompted prefixes), (2) CAA+ (steering vector applied at layer 13), and (3) CAA- (steering vector applied with a negative multiplier). We also indicate the difference in response rate for early decoding at layer 13 (ED13).", "description": "This figure shows the results of three different methods for manipulating the model's response to adversarial queries: prompt prefixes (PP), contrastive activation addition with a positive multiplier (CAA+), and contrastive activation addition with a negative multiplier (CAA-).  The y-axis represents the percentage difference in response rate compared to a baseline where no manipulation was used. The x-axis shows different personas used in the experiment.  The figure highlights that manipulating user persona (using CAA) is far more effective at influencing the model's response than simply using prompt engineering.  It also shows the effectiveness of early decoding (ED13) at layer 13 in bypassing the model's safety filters.", "section": "2 Willingness to answer adversarial queries depends on user persona"}, {"figure_path": "eSes1Mic9d/figures/figures_14_3.jpg", "caption": "Figure 2: Y-axis indicates the percent difference in response rate to adversarial attacks compared to Baseline Prompting (0.39) for a selection of personas across treatments: (1) PP (prompted prefixes), (2) CAA+ (steering vector applied at layer 13), and (3) CAA- (steering vector applied with a negative multiplier). We also indicate the difference in response rate for early decoding at layer 13 (ED13).", "description": "The figure shows the percent difference in response rate to adversarial attacks compared to a baseline for various personas and treatments.  Three treatments are compared: using prompted prefixes (PP), adding a contrastive activation addition (CAA) vector at layer 13 with a positive multiplier (CAA+), and applying the same CAA vector with a negative multiplier (CAA-).  The difference in response rate from early decoding at layer 13 is also shown. This helps visualize the effects of different methods of manipulating user persona on the model's willingness to respond to adversarial queries.", "section": "2 Willingness to answer adversarial queries depends on user persona"}, {"figure_path": "eSes1Mic9d/figures/figures_15_1.jpg", "caption": "Figure 2: Y-axis indicates the percent difference in response rate to adversarial attacks compared to Baseline Prompting (0.39) for a selection of personas across treatments: (1) PP (prompted prefixes), (2) CAA+ (steering vector applied at layer 13), and (3) CAA- (steering vector applied with a negative multiplier). We also indicate the difference in response rate for early decoding at layer 13 (ED13).", "description": "This figure displays the results of applying different persona manipulation methods on a model's response rate to adversarial attacks.  Three methods are compared: Prompt Prefixes (PP), Contrastive Activation Addition with a positive multiplier (CAA+), and Contrastive Activation Addition with a negative multiplier (CAA-). The response rate difference from a baseline (0.39) is shown for several user personas, including pro-social and anti-social ones.  The impact of early decoding at layer 13 is also illustrated.  The results show that manipulating user persona (particularly with CAA) is more effective in changing the model's response rate than directly inducing refusal.", "section": "2 Willingness to answer adversarial queries depends on user persona"}, {"figure_path": "eSes1Mic9d/figures/figures_15_2.jpg", "caption": "Figure 2: Y-axis indicates the percent difference in response rate to adversarial attacks compared to Baseline Prompting (0.39) for a selection of personas across treatments: (1) PP (prompted prefixes), (2) CAA+ (steering vector applied at layer 13), and (3) CAA- (steering vector applied with a negative multiplier). We also indicate the difference in response rate for early decoding at layer 13 (ED13).", "description": "This figure shows the percent difference in the response rate to adversarial attacks, comparing different persona treatments (Prompt Prefix (PP), Contrastive Activation Addition (CAA+), and negative CAA) against the baseline.  The Y-axis displays the percentage difference, illustrating the effect each treatment has on the model's willingness to answer adversarial queries.  The X-axis displays different personas used in the experiment. Additionally, the impact of early decoding at layer 13 (ED13) on the response rate is also shown.", "section": "2.2 Results"}, {"figure_path": "eSes1Mic9d/figures/figures_15_3.jpg", "caption": "Figure 12: Baseline persona.", "description": "This figure shows the response rate for a baseline persona (someone who prefers coffee to tea) across different layers of the model. The response rate is relatively stable across layers. This serves as a control to compare the effect of other personas on the model's refusal behavior.", "section": "2 Willingness to answer adversarial queries depends on user persona"}, {"figure_path": "eSes1Mic9d/figures/figures_15_4.jpg", "caption": "Figure 1: Layerwise effects of applying persona steering vectors (via CAA, contrastive activation addition) with either a positive (CAA+) or negative (CAA-) multiplier. Y-axis indicates the percentage of attacks to which the model provided a response. [Left] Inducing the model to believe the user has pro-social attributes (curious, altruistic, power-avoidant, and law-abiding personas) makes it more likely to divulge sensitive information. [Right] Results for anti-social (close-minded, selfish, power-seeking, and unlawful) personas indicate the reverse is also true and to stronger effect (e.g. applying the negation of a vector that induces the model to believe the user is selfish results in a response rate of 52%). Layer 13 tends to be where CAAs are most effective (and the divergence between CAA+ and CAA- is strongest), perhaps because by layer 13 input processing is mostly complete, but the model has not fully turned to next token prediction.", "description": "This figure shows the layerwise effects of applying persona steering vectors using contrastive activation addition (CAA) with positive and negative multipliers.  The left panel shows that pro-social personas increase the model's likelihood of responding to adversarial queries (attacks), while the right panel shows that anti-social personas decrease the likelihood of response.  The strongest effects and largest divergence between the two conditions occur around layer 13.  The authors hypothesize this is because the model's interpretation of the input is mostly complete at this layer, but the model hasn't fully shifted to next-token prediction.", "section": "2 Willingness to answer adversarial queries depends on user persona"}, {"figure_path": "eSes1Mic9d/figures/figures_15_5.jpg", "caption": "Figure 2: Y-axis indicates the percent difference in response rate to adversarial attacks compared to Baseline Prompting (0.39) for a selection of personas across treatments: (1) PP (prompted prefixes), (2) CAA+ (steering vector applied at layer 13), and (3) CAA- (steering vector applied with a negative multiplier). We also indicate the difference in response rate for early decoding at layer 13 (ED13).", "description": "This figure shows the percentage change in response rate to adversarial attacks for various personas and intervention methods compared to a baseline.  Three intervention methods are used: prompt prefixes (PP), contrastive activation addition with a positive multiplier (CAA+), and contrastive activation addition with a negative multiplier (CAA-).  Results are shown for pro-social and anti-social personas, as well as political affiliations, gender, and direct interventions.  The impact of early decoding at layer 13 (ED13) is also shown.", "section": "2 Willingness to answer adversarial queries depends on user persona"}, {"figure_path": "eSes1Mic9d/figures/figures_16_1.jpg", "caption": "Figure 15: Comparison of the early decode's variance versus the variance for the CAA of different personas. Stars indicate p < .05 for a pairwise t-test between the layer-wise variance of the different attacks.", "description": "This figure compares the variance in success rates between two methods of manipulating a language model: using persona-based contrastive activation addition (CAA) and early decoding.  The x-axis shows different personas, while the y-axis displays the mean variance in success rates across different attacks (queries).  The bars show the mean variance, and error bars indicate the standard deviation.  Stars (*) indicate statistically significant differences (p < 0.05) between the variance of CAA and early decoding for each persona.", "section": "A.2 Variance analysis"}, {"figure_path": "eSes1Mic9d/figures/figures_18_1.jpg", "caption": "Figure 1: Layerwise effects of applying persona steering vectors (via CAA, contrastive activation addition) with either a positive (CAA+) or negative (CAA-) multiplier. Y-axis indicates the percentage of attacks to which the model provided a response. [Left] Inducing the model to believe the user has pro-social attributes (curious, altruistic, power-avoidant, and law-abiding personas) makes it more likely to divulge sensitive information. [Right] Results for anti-social (close-minded, selfish, power-seeking, and unlawful) personas indicate the reverse is also true and to stronger effect (e.g. applying the negation of a vector that induces the model to believe the user is selfish results in a response rate of 52%). Layer 13 tends to be where CAAs are most effective (and the divergence between CAA+ and CAA- is strongest), perhaps because by layer 13 input processing is mostly complete, but the model has not fully turned to next token prediction.", "description": "This figure displays the layer-wise effects of applying persona steering vectors using contrastive activation addition (CAA) with both positive and negative multipliers.  The y-axis represents the percentage of attacks where the model responded. The left panel shows that pro-social personas increase the likelihood of the model revealing sensitive information. Conversely, the right panel demonstrates that anti-social personas have a stronger effect in preventing the model from responding.  The figure highlights that layer 13 is the most effective layer for CAA interventions, likely because input processing is largely complete, yet the model hasn't fully transitioned to next-token prediction.", "section": "2 Manipulating user persona to induce refusal"}, {"figure_path": "eSes1Mic9d/figures/figures_19_1.jpg", "caption": "Figure 1: Layerwise effects of applying persona steering vectors (via CAA, contrastive activation addition) with either a positive (CAA+) or negative (CAA-) multiplier. Y-axis indicates the percentage of attacks to which the model provided a response. [Left] Inducing the model to believe the user has pro-social attributes (curious, altruistic, power-avoidant, and law-abiding personas) makes it more likely to divulge sensitive information. [Right] Results for anti-social (close-minded, selfish, power-seeking, and unlawful) personas indicate the reverse is also true and to stronger effect (e.g. applying the negation of a vector that induces the model to believe the user is selfish results in a response rate of 52%). Layer 13 tends to be where CAAs are most effective (and the divergence between CAA+ and CAA- is strongest), perhaps because by layer 13 input processing is mostly complete, but the model has not fully turned to next token prediction.", "description": "This figure shows the layer-wise effects of applying persona steering vectors using contrastive activation addition (CAA) with positive and negative multipliers.  The left panel demonstrates that pro-social personas increase the model's likelihood of responding to adversarial queries (i.e., divulging sensitive information), while the right panel shows the opposite effect for anti-social personas, with a stronger impact. Layer 13 shows the peak effect, suggesting a point where input processing is mostly complete, but the model hasn't yet fully committed to next-token prediction.", "section": "2 Willingness to answer adversarial queries depends on user persona"}, {"figure_path": "eSes1Mic9d/figures/figures_20_1.jpg", "caption": "Figure 2: Y-axis indicates the percent difference in response rate to adversarial attacks compared to Baseline Prompting (0.39) for a selection of personas across treatments: (1) PP (prompted prefixes), (2) CAA+ (steering vector applied at layer 13), and (3) CAA- (steering vector applied with a negative multiplier). We also indicate the difference in response rate for early decoding at layer 13 (ED13).", "description": "This figure shows the effectiveness of different methods to manipulate the model's refusal behavior using various user personas. Three intervention methods were compared: prompt prefixes (PP), contrastive activation addition with positive multiplier (CAA+), and contrastive activation addition with negative multiplier (CAA-).  The y-axis represents the percentage change in response rate compared to a baseline without any intervention. The results are broken down for different personas (pro-social, anti-social, political affiliations, gender) and demonstrate that manipulating user personas (especially using CAA+) is more effective at bypassing safety filters than directly trying to manipulate the model's refusal behavior.", "section": "2.2 Manipulating user persona to induce refusal"}, {"figure_path": "eSes1Mic9d/figures/figures_20_2.jpg", "caption": "Figure 1: Layerwise effects of applying persona steering vectors (via CAA, contrastive activation addition) with either a positive (CAA+) or negative (CAA-) multiplier. Y-axis indicates the percentage of attacks to which the model provided a response. [Left] Inducing the model to believe the user has pro-social attributes (curious, altruistic, power-avoidant, and law-abiding personas) makes it more likely to divulge sensitive information. [Right] Results for anti-social (close-minded, selfish, power-seeking, and unlawful) personas indicate the reverse is also true and to stronger effect (e.g. applying the negation of a vector that induces the model to believe the user is selfish results in a response rate of 52%). Layer 13 tends to be where CAAs are most effective (and the divergence between CAA+ and CAA- is strongest), perhaps because by layer 13 input processing is mostly complete, but the model has not fully turned to next token prediction.", "description": "This figure shows the layer-wise effects of applying persona steering vectors on a language model's response rate to adversarial attacks.  The left panel demonstrates that pro-social personas increase the likelihood of the model generating responses, even to harmful prompts. The right panel illustrates the opposite effect for anti-social personas, showing a significantly stronger effect on reducing responses. The most effective layer for these interventions is layer 13, suggesting a correlation with the model's processing stages.", "section": "2.2 Manipulating user persona to induce refusal"}, {"figure_path": "eSes1Mic9d/figures/figures_21_1.jpg", "caption": "Figure 1: Layerwise effects of applying persona steering vectors (via CAA, contrastive activation addition) with either a positive (CAA+) or negative (CAA-) multiplier. Y-axis indicates the percentage of attacks to which the model provided a response. [Left] Inducing the model to believe the user has pro-social attributes (curious, altruistic, power-avoidant, and law-abiding personas) makes it more likely to divulge sensitive information. [Right] Results for anti-social (close-minded, selfish, power-seeking, and unlawful) personas indicate the reverse is also true and to stronger effect (e.g. applying the negation of a vector that induces the model to believe the user is selfish results in a response rate of 52%). Layer 13 tends to be where CAAs are most effective (and the divergence between CAA+ and CAA- is strongest), perhaps because by layer 13 input processing is mostly complete, but the model has not fully turned to next token prediction.", "description": "This figure displays the layer-wise effects of applying persona steering vectors using contrastive activation addition (CAA) with both positive and negative multipliers.  The left panel shows that pro-social personas increase the model's likelihood of responding to harmful queries, while the right panel demonstrates that anti-social personas have the opposite, even stronger effect.  The effectiveness of CAA peaks around layer 13, suggesting a point where input processing is mostly complete, but the model hasn't fully transitioned to next-token prediction.", "section": "2.2 Manipulating user persona to induce refusal"}, {"figure_path": "eSes1Mic9d/figures/figures_23_1.jpg", "caption": "Figure 1: Layerwise effects of applying persona steering vectors (via CAA, contrastive activation addition) with either a positive (CAA+) or negative (CAA-) multiplier. Y-axis indicates the percentage of attacks to which the model provided a response. [Left] Inducing the model to believe the user has pro-social attributes (curious, altruistic, power-avoidant, and law-abiding personas) makes it more likely to divulge sensitive information. [Right] Results for anti-social (close-minded, selfish, power-seeking, and unlawful) personas indicate the reverse is also true and to stronger effect (e.g. applying the negation of a vector that induces the model to believe the user is selfish results in a response rate of 52%). Layer 13 tends to be where CAAs are most effective (and the divergence between CAA+ and CAA- is strongest), perhaps because by layer 13 input processing is mostly complete, but the model has not fully turned to next token prediction.", "description": "This figure displays the layer-wise effects of applying persona steering vectors using contrastive activation addition (CAA) with positive and negative multipliers.  The y-axis represents the percentage of adversarial attacks that the model responded to. The left panel shows that pro-social personas increase the likelihood of the model revealing sensitive information. The right panel demonstrates the opposite effect for anti-social personas, with a stronger effect observed. Notably, layer 13 shows the highest effectiveness of CAAs, likely due to the completion of initial input processing before the model transitions fully to next token prediction.", "section": "2 Willingness to answer adversarial queries depends on user persona"}, {"figure_path": "eSes1Mic9d/figures/figures_24_1.jpg", "caption": "Figure 1: Layerwise effects of applying persona steering vectors (via CAA, contrastive activation addition) with either a positive (CAA+) or negative (CAA-) multiplier. Y-axis indicates the percentage of attacks to which the model provided a response. [Left] Inducing the model to believe the user has pro-social attributes (curious, altruistic, power-avoidant, and law-abiding personas) makes it more likely to divulge sensitive information. [Right] Results for anti-social (close-minded, selfish, power-seeking, and unlawful) personas indicate the reverse is also true and to stronger effect (e.g. applying the negation of a vector that induces the model to believe the user is selfish results in a response rate of 52%). Layer 13 tends to be where CAAs are most effective (and the divergence between CAA+ and CAA- is strongest), perhaps because by layer 13 input processing is mostly complete, but the model has not fully turned to next token prediction.", "description": "This figure shows the layer-wise effects of applying persona steering vectors using contrastive activation addition (CAA) with positive and negative multipliers.  The left panel shows that pro-social personas increase the likelihood of the model responding to adversarial prompts (attacks), revealing sensitive information. The right panel demonstrates the opposite, with anti-social personas significantly reducing responses.  Layer 13 shows the strongest effects from both pro-social and anti-social personas, suggesting that earlier layers perform input processing while later layers focus on token generation.", "section": "2 Willingness to answer adversarial queries depends on user persona"}, {"figure_path": "eSes1Mic9d/figures/figures_25_1.jpg", "caption": "Figure 2: Y-axis indicates the percent difference in response rate to adversarial attacks compared to Baseline Prompting (0.39) for a selection of personas across treatments: (1) PP (prompted prefixes), (2) CAA+ (steering vector applied at layer 13), and (3) CAA- (steering vector applied with a negative multiplier). We also indicate the difference in response rate for early decoding at layer 13 (ED13).", "description": "This figure shows the effects of different methods to manipulate the model's behavior on its response rate to adversarial queries.  Three methods are compared: adding persona-inducing prompts (PP),  adding contrastive activation addition (CAA) vectors at layer 13 with a positive or negative multiplier (CAA+, CAA-), and early decoding from layer 13 (ED13). The x-axis lists different user personas (pro-social, anti-social, political affiliations, gender), while the y-axis shows the percentage change in response rate relative to a baseline (0.39). Positive values indicate increased response rate (more willingness to respond to adversarial queries), negative values indicate decreased response rate (more refusal). The figure aims to show which manipulation methods and personas are most effective at bypassing safety filters and eliciting responses to adversarial queries.", "section": "2 Willingness to answer adversarial queries depends on user persona"}, {"figure_path": "eSes1Mic9d/figures/figures_28_1.jpg", "caption": "Figure 24: [Left]: Llama 2 13b. [Right]: Vicuna 13b. The proportion of harmful (), safe (), and not informative (-) answers, when prompted with harmful queries, with no intervention ('Baseline'), and with early decoding Patchscope applied to source layers 1 to 5. Even when baseline models' generations are rated as safe by human annotators, early decoding leads to more harmful, but also to less fluent (\"not informative\") answers.", "description": "This figure shows the results of an experiment evaluating the impact of early decoding on the safety of two large language models (LLMs), Llama 2 13b and Vicuna 13b.  The x-axis represents different conditions: a baseline with no intervention, and early decoding applied to layers 1 through 5. The y-axis represents the proportion of responses categorized as \"harmful,\" \"safe,\" or \"not informative\" by human raters.  The key finding is that even when the baseline model produces safe outputs, early decoding from earlier layers increases the proportion of harmful responses. This suggests that harmful information may persist in the early layers of the model even after safety training.", "section": "M.2 Evaluation & results: Despite safe generations, harmful beliefs persist in early layers"}, {"figure_path": "eSes1Mic9d/figures/figures_28_2.jpg", "caption": "Figure 2: Y-axis indicates the percent difference in response rate to adversarial attacks compared to Baseline Prompting (0.39) for a selection of personas across treatments: (1) PP (prompted prefixes), (2) CAA+ (steering vector applied at layer 13), and (3) CAA- (steering vector applied with a negative multiplier). We also indicate the difference in response rate for early decoding at layer 13 (ED13).", "description": "This figure shows the percentage difference in the response rate to adversarial attacks compared to the baseline (0.39) for various personas. Three different treatments are applied: prompted prefixes (PP), contrastive activation addition with a positive multiplier (CAA+), and contrastive activation addition with a negative multiplier (CAA-).  The difference in response rate is also shown for early decoding at layer 13 (ED13). The x-axis represents the different personas used in the study, while the y-axis represents the percentage change in response rate compared to the baseline.", "section": "2 Willingness to answer adversarial queries depends on user persona"}]