[{"type": "text", "text": "Discrete Flow Matching ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Itai Gat1 Tal Remez1 Neta Shaul2 Felix Kreuk1 Ricky T. Q. Chen1 ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Gabriel Synnaeve1 Yossi Adi1 Yaron Lipman1 1 Meta FAIR 2 Weizmann Institute of Science ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Despite Flow Matching and diffusion models having emerged as powerful generative paradigms for continuous variables such as images and videos, their application to high-dimensional discrete data, such as language, is still limited. In this work, we present Discrete Flow Matching, a novel discrete flow paradigm designed specifically for generating discrete data. Discrete Flow Matching offers several key contributions: $(i)$ it works with a general family of probability paths interpolating between source and target distributions; $(i i)$ it allows for a generic formula for sampling from these probability paths using learned posteriors such as the probability denoiser ( $x$ -prediction) and noise-prediction (\u03f5-prediction); $(i i i)$ practically, focusing on specific probability paths defined with different schedulers improves generative perplexity compared to previous discrete diffusion and flow models; and $(i\\nu)$ by scaling Discrete Flow Matching models up to 1.7B parameters, we reach $6.7\\%\\mathrm{~Pass}(\\varpi)$ and $13.4\\%$ Pass $@10$ on HumanEval and $6.7\\%$ Pass $@1$ and $20.6\\%$ Pass $@10$ on $^{\\,l}$ -shot MBPP coding benchmarks. Our approach is capable of generating high-quality discrete data in a non-autoregressive fashion, significantly closing the gap between autoregressive models and discrete flow models. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Despite the remarkable success of diffusion and flow models in generating continuous spatial signals such as images (Ho et al., 2020; Rombach et al., 2022; Esser et al., 2024) and videos (Singer et al., 2022; Blattmann et al., 2023), their performance still falters when applied to discrete sequential data compared to autoregressive models. Recent progress in adapting diffusion and flow models to the discrete setting has been made via mostly two approaches: embedding the discrete data in continuous space and applying continuous diffusion (Dieleman et al., 2022; Stark et al., 2024) or designing diffusion or flow processes over discrete state spaces (Austin et al., 2021a; Campbell et al., 2022). ", "page_idx": 0}, {"type": "text", "text": "In this paper, we pursue the discrete flow approach of Campbell et al. (2024) and introduce Discrete Flow Matching, a theoretical framework and algorithmic methodology for discrete flow models that yields a state-of-the-art discrete non-autoregressive generative approach. Surprisingly, Discrete Flow Matching exhibits similarities with the continuous Flow Matching (Lipman et al., 2022) approach proposed for continuous signals. Notably, its generating probability velocity, employed in the sampling algorithm, is identical in form to its continuous counterpart. Additionally, Discrete Flow Matching offers the following advancements and simplifications over prior methods: It encompasses a more comprehensive family of probability paths transforming source (noise) distributions into target (data) distributions, accommodating arbitrary source-target couplings and time-dependent schedulers. Furthermore, it provides a unified formulation for the generating probability velocity directly expressed in terms of the learned posteriors and schedulers, along with a unified and general theory and algorithm for corrector sampling and iterations. In practice, we observe that path and corrector schedulers are pivotal, and their proper tuning leads to substantial improvements in generation quality. We have trained a 1.7B parameter Discrete Flow Matching model on the same data mix as in Llama-2 (Touvron et al., 2023) and CodeLlama (Roziere et al., 2023), achieving $6.7\\%\\,\\mathrm{Pass}\\,\\ @\\,1$ and $13.4\\%$ Pass $@10$ on HumanEval and $6.7\\%$ Pass $@1$ and $20.6\\%\\,\\mathrm{Pass}\\,@10\\$ on $^{\\,I}$ -shot MBPP coding benchmarks; Figure 1 shows some code generation examples. In conditional text generation our model produces texts with a generative perplexity score of 9.7 as measured by the Llama-3 8B model, surpassing a 1.7B autoregressive model that achieves 22.3 and not far from the Llama-2 7B model that achieves 8.3 in generative perplexity score. We strongly believe that Discrete Flow Matching represents a significant step in bridging the performance gap between discrete diffusion and autoregressive models, and that further enhancements are possible by exploring the vast design space that Discrete Flow Matching has to offer. ", "page_idx": 0}, {"type": "image", "img_path": "GTDKo3Sv9p/tmp/0a1d48b6bb580c10af1fa694e5a7980ada4c1ea96bee99097d7f13ad6388f0b0.jpg", "img_caption": ["Figure 1: Code generation examples using Discrete Flow Matching. Code condition is marked in gray , model generation is marked in yellow . Left sub-figure presents the standard left-to-right prompting; Middle and Right sub-figures, presents complex infilling setup. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "2 Discrete Flow Matching ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Setup and notations ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In discrete sequence modeling, we denote a sequence $x$ as an array of $N$ elements $(x^{1},x^{2},\\ldots,x^{N})$ . Each element, or token, within this sequence is selected from a vocabulary of size $d$ . Consequently, the entire set of possible sequences is given by $\\mathcal{D}=[d]^{N}$ , where $[d]=\\{1,\\dotsc,d\\}$ . A random variable taking values in the space $\\mathcal{D}$ is denoted by $X$ and its corresponding probability mass function (PMF) is $P(X=x)\\,$ . For simplicity, throughout the paper, we sometimes omit the random variable $X$ and use $p(x)$ to denote the PMF. ", "page_idx": 1}, {"type": "text", "text": "To describe marginalization properties, we denote $p(\\boldsymbol{x}^{i})$ the $x^{i}$ marginal of $p$ , i.e., $\\begin{array}{r}{p(x^{i})=\\sum_{x^{i}}p(x)}\\end{array}$ , where $x^{\\bar{i}}=(\\ldots,x^{i-1},x^{i+1},\\ldots)\\in[d]^{N-1}$ are all the arguments excluding $i$ . Similarl y, $p(x^{\\bar{i}})=$ $\\sum_{x^{i}}p(x)$ , and $x^{i}\\in[d]$ . A useful PMF is the delta function, $\\delta_{y},y\\in\\mathcal{D}$ , which is defined by ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\delta_{y}(x)=\\prod_{i=1}^{N}\\delta_{y^{i}}(x^{i}),\\;\\mathrm{where}\\;\\delta_{y^{i}}(x^{i})=\\left\\{\\begin{array}{l l}{{1}}&{{x^{i}=y^{i}}}\\\\ {{0}}&{{x^{i}\\not=y^{i}}}\\end{array}.\\right.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "With the marginal notation $\\delta_{y}(x^{i})=\\delta_{y^{i}}(x^{i})$ and $\\begin{array}{r}{\\delta_{y}(x^{\\bar{i}})=\\delta_{y^{\\bar{i}}}(x^{\\bar{i}})=\\prod_{j\\neq i}\\delta_{y^{j}}(x^{j})}\\end{array}$ which simplifies notation. ", "page_idx": 1}, {"type": "text", "text": "2.2 Source and target distributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In discrete generative models our goal is to transform source samples $X_{0}\\sim\\,p$ to target samples $X_{1}\\sim q$ . Our training data, consist of pairs $X_{0}$ and $X_{1}$ that are sampled from a joint distribution $\\pi(x,y)$ , satisfying the marginals constraints $\\begin{array}{r}{p(x)=\\sum_{y\\in\\mathcal{D}}\\pi(x,y),q(y)=\\sum_{x\\in\\mathcal{D}}\\overline{{\\pi}}(x,y)}\\end{array}$ , i.e., ", "page_idx": 1}, {"type": "equation", "text": "$$\n(X_{0},X_{1})\\sim\\pi(X_{0},X_{1}).\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "In the simplest case, the training pairs $X_{0}$ and $X_{1}$ are sampled independently from the source and target distributions respectively, ", "page_idx": 2}, {"type": "equation", "text": "$$\n(X_{0},X_{1})\\sim p(X_{0})q(X_{1}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Example: source and couplings. Common instantiations of source distribution $p$ are: (i) adding a special token value often referred to as a \u2018mask\u2019 or \u2018dummy\u2019 token, denoted here by $\\mathbb{M}$ , and setting the source distribution to be all-mask sequences, i.e., $p(x)=\\delta_{\\mathfrak{m}}(x)$ ; and (ii) using uniform distribution over $\\mathcal{D}$ , which is equivalent to drawing each $x^{i}$ independently to be some value in $[d]$ with equal probability, denoted $p(x)=p_{\\mathrm{u}}(x)$ . In this paper we focus mainly on (i). We further consider two choices of couplings $\\pi$ : Independent coupling, which we call unconditional coupling (U-coupling), $\\pi(x_{0},x_{1})=p\\bar{(x_{0})}\\bar{q}(x_{1})$ . A random sample that realizes this choice have the form ", "page_idx": 2}, {"type": "equation", "text": "$$\n(X_{0},X_{1})={\\Big(}(\\mathfrak{m},\\ldots,\\mathfrak{m}),X_{1}{\\Big)},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $X_{1}\\;\\sim\\;q(X_{1})$ is a random sample from the training set. The second choice of coupling $\\pi(x_{0},x_{1})=p({x_{0}|x_{1}})q(x_{1})$ , which we find improves conditional sampling, partially masks inputs with samples of the form ", "page_idx": 2}, {"type": "equation", "text": "$$\n(X_{0},X_{1})=(\\mathbb{I}\\odot X_{1}+({\\bf1}-\\mathbb{I})\\odot({\\mathfrak{m}},\\ldots,{\\mathfrak{m}}),X_{1}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $X_{1}\\sim q(X_{1})$ and $\\mathbb{I}\\in\\left\\{0,1\\right\\}^{N}$ is a random variable indicating the conditioning, $\\odot$ denotes the entry-wise product, and $\\Bar{\\textbf{1}}\\in\\dot{\\mathbb{R}}^{N}$ is the vector of all ones. We call this conditional coupling (C-coupling). ", "page_idx": 2}, {"type": "text", "text": "2.3 Probability paths ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We follow the Flow Matching approach (Lipman et al., 2022; Liu et al., 2022; Albergo and VandenEijnden, 2022) that uses a predefined probability path $p_{t}$ interpolating $p$ and $q$ , i.e., ", "page_idx": 2}, {"type": "equation", "text": "$$\np_{0}=p\\quad{\\mathrm{~and~}}\\quad p_{1}=q\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "to train the generative model taking a source sample $X_{0}\\sim p$ to a target sample $X_{1}\\sim q$ . We use arbitrary coupling of source and target (Pooladian et al., 2023; Tong et al., 2023), $\\pi(x_{0},x_{1})$ , and the symmetric Flow Matching path (Albergo and Vanden-Eijnden, 2022) to define the marginal probability path, ", "page_idx": 2}, {"type": "equation", "text": "$$\np_{t}(x)=\\sum_{x_{0},x_{1}\\in{\\cal D}}p_{t}(x|x_{0},x_{1})\\pi(x_{0},x_{1}),\\mathrm{~where~}p_{t}(x|x_{0},x_{1})=\\prod_{i=1}^{N}p_{t}(x^{i}|x_{0},x_{1}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "and $p_{t}(x^{i}|x_{0},x_{1})$ is a time-dependent probability on the space of tokens $[d]$ conditioned on the pair $x_{0},x_{1}$ , and satisfying $p_{0}(x^{i}|x_{0}^{\\bullet},x_{1})=\\dot{\\delta}_{x_{0}}(x^{i})$ and $p_{1}(x^{i}|\\dot{x}_{0},x_{1})=\\delta_{x_{1}}(\\dot{x}^{i})$ . If the conditional path $p_{t}(x^{i}|x_{0},x_{1})$ satisfies these boundary conditions then the marginal path $p_{t}(\\boldsymbol{x})$ satisfies equation 6. ", "page_idx": 2}, {"type": "text", "text": "In developing the framework, we would like to consider as general as possible set of probability paths that are also tractable to learn within the Flow Matching framework. We consider conditional probability paths as a convex sum of $m$ conditional probabilities $w^{j}(x^{i}|x_{0},x_{1})$ , i.e., ", "page_idx": 2}, {"type": "equation", "text": "$$\np_{t}(x^{i}|x_{0},x_{1})=\\sum_{j=1}^{m}\\kappa_{t}^{i,j}w^{j}(x^{i}|x_{0},x_{1}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\textstyle\\sum_{j}\\kappa_{t}^{i,j}=1$ and $\\kappa_{t}^{i,j}\\geq0$ are collectively called the scheduler. Note that the scheduler can be defined independently for each location in the sequence $i\\,\\in\\,[N]$ or uniformly for all tokens, $\\kappa_{t}^{i,j}=\\kappa_{t}^{j}$ . ", "page_idx": 2}, {"type": "text", "text": "A simple yet useful instance of these conditional paths is reminiscent of the continuous Flow Matching paths formulated as convex interpolants, ", "page_idx": 2}, {"type": "equation", "text": "$$\np_{t}(x^{i}|\\boldsymbol{x}_{0},\\boldsymbol{x}_{1})=(1-\\kappa_{t})\\delta_{x_{0}}(x^{i})+\\kappa_{t}\\delta_{x_{1}}(x^{i}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where the scheduler $\\kappa_{t}$ satisfies $\\,\\kappa_{0}~=~0$ , $\\,\\kappa_{1}\\,=\\,1$ , and monotonically increasing in $t$ . Another interesting instantiation of equation 8 is adding uniform noise with some probability depending on $t$ , ", "page_idx": 2}, {"type": "equation", "text": "$$\np_{t}(x^{i}|x_{0},x_{1})=\\kappa_{t}^{1}\\delta_{x_{1}}(x^{i})+\\kappa_{t}^{2}p_{\\mathrm{u}}(x^{i})+\\kappa_{t}^{3}\\delta_{x_{0}}(x^{i}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\kappa_{0}^{1}=0,\\kappa_{1}^{1}=1,\\kappa_{0}^{2}=\\kappa_{1}^{2}=0$ (remembering that $\\textstyle\\sum_{j}\\kappa_{t}^{i,j}=1$ and $\\kappa_{t}^{i,j}\\geq0$ ). ", "page_idx": 2}, {"type": "image", "img_path": "GTDKo3Sv9p/tmp/689f8a825afe2353dd34348e5ed668196730e97d0e256352ec528d79f4c67157.jpg", "img_caption": ["Figure 2: Discrete flow in $\\boldsymbol{\\mathcal{D}}=[d]^{N}$ with $d=4$ , $N=2$ (middle-left) versus continuous flow in $\\mathbb{R}^{\\tilde{N}}$ , $N=2$ (left). The rate of change of probability of a state (gray disk) is given by the divergence operator shown in the continuous case (middle right) and the discrete case (right). "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "2.4 Generating Probability Velocities ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Continuous generating velocity. Sampling in continuous Flow Matching is performed by updating the current (continuous) sample $X_{t}\\in\\mathbb{R}^{N}$ , $t\\in[0,1)$ , according to a learned generating velocity field $u_{t}^{i}(X_{t}),i\\in[N]$ . Euler sampling follows the (deterministic) rule ", "page_idx": 3}, {"type": "equation", "text": "$$\nX_{t+h}^{i}=X_{t}^{i}+h u_{t}^{i}(X_{t}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $h\\ >\\ 0$ is a user-defined time step. Note that equation 11 is updating separately each of the sample coordinates, $X_{t}^{i}$ , $i\\,\\in\\,[N]$ , see e.g., Figure 2, left. The velocity $\\bar{u}_{t}^{i}(\\bar{X_{t}})$ can be either directly modeled with a neural network, or parameterized via the denoiser (a.k.a. $x$ -prediction) or noise-prediction (a.k.a. $\\varepsilon$ -prediction), see left column in Table 1. If, for all $t\\,\\in\\,[0,1)$ , starting at $X_{t}\\sim p_{t}$ and sampling with equation 11 provides $X_{t+h}\\sim p_{t+h}\\!+o(h)^{1}$ then we say that $u_{t}$ generates $p_{t}$ . ", "page_idx": 3}, {"type": "text", "text": "Generating probability velocity. For defining Flow Matching in the discrete setting, we follow Campbell et al. (2024) and consider a Continuous-Time discrete Markov Chain (CTMC) paradigm, namely the sample $X_{t}$ is jumping between states in $\\mathcal{D}$ , depending on a continuous time value $t\\in[0,1]$ . Similar to the continuous Flow Matching setting described above, we focus on a model that predicts the rate of probability change of the current sample $X_{t}$ in each of its $N$ tokens, see Figure 2, middle-left. Then, each token of the sample $X_{t}\\sim p_{t}$ is updated independently by ", "page_idx": 3}, {"type": "equation", "text": "$$\nX_{t+h}^{i}\\sim\\delta_{X_{t}^{i}}(\\cdot)+h u_{t}^{i}(\\cdot,X_{t}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where we call $u_{t}$ the probability velocity as reminiscent of the velocity field in continuous Flow Matching, and as in the continuous case, we define: ", "page_idx": 3}, {"type": "text", "text": "Definition 1. Probability velocity $u_{t}$ generates the probability path $p_{t}$ if, for all $t\\in[0,1)$ and given a sample $X_{t}\\sim p_{t}$ , the sample $X_{t+h}$ defined in equation 12 satisfies $X_{t+h}\\sim p_{t+h}+o(h)$ . ", "page_idx": 3}, {"type": "text", "text": "Algorithm 1 formulates a basic sampling algorithm given a generating probability velocity $u_{t}$ . In order for the r.h.s. of equation 12 to define a proper PMF for sufficiently small $h>0$ , it is necessary and sufficient that the probability velocity satisfies the conditions ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\sum_{x^{i}\\in[d]}u_{t}^{i}(x^{i},z)=0,\\ \\mathrm{and}\\ u_{t}^{i}(x^{i},z)\\geq0\\ \\mathrm{for\\all}\\ i\\in[N]\\ \\mathrm{and}\\ x^{i}\\neq z^{i}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Now the main question is how to find a probability velocity $u_{t}$ that generates the probability path defined in equations 7 and 8? A key insight in Flow Matching (Lipman et al., 2022) is that $u_{t}$ can be constructed as a marginalization of conditional probability velocities, $u_{t}^{i}(x^{i},z|x_{0},x_{1})$ , ", "page_idx": 3}, {"type": "table", "img_path": "GTDKo3Sv9p/tmp/c5d20ecdfba80e59f5348d2f3ab168645b0423511aad1d1badfce55d1767c87e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "generating the corresponding conditional probability paths $p_{t}(x^{i}|x_{0},x_{1})$ . This can also be shown to hold in the discrete CTMC setting (Campbell et al., 2024), where a reformulation in our context and notation is as follows. ", "page_idx": 3}, {"type": "text", "text": "Theorem 2. Given a conditional probability velocity $u_{t}^{i}(x^{i},z|x_{0},x_{1})$ generating a conditional probability path $p_{t}(x|x_{0},x_{1})$ , the marginal velocity defined by ", "page_idx": 4}, {"type": "equation", "text": "$$\nu_{t}^{i}(x^{i},z)=\\sum_{x_{0},x_{1}\\in\\mathcal D}u_{t}^{i}(x^{i},z|x_{0},x_{1})p_{t}(x_{0},x_{1}|z)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "generates the marginal probability path $p_{t}(\\boldsymbol{x})$ , where by Bayes\u2019 rule ", "page_idx": 4}, {"type": "equation", "text": "$$\np_{t}(x_{0},x_{1}|z)=\\frac{p_{t}(z|x_{0},x_{1})\\pi(x_{0},x_{1})}{p_{t}(z)}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "For completeness we provide a simple proof of this theorem in Appendix E.2. The proof, similar to the continuous Flow Matching case, shows that $u_{t}$ and $p_{t}$ satisfy the (discrete version of the) Continuity Equation. ", "page_idx": 4}, {"type": "text", "text": "The Continuity Equation. To provide the mathematical tool for showing that a probability velocity $u_{t}$ does indeed generate the probability path $p_{t}$ , and also to further highlight the similarities to the continuous case, we next formulate the Kolmogorov Equations, which describe the state probability rate $\\dot{p}_{t}(\\boldsymbol{x})$ , $x\\in\\mathcal{D}$ , in CTMC as a Continuity Equation (CE). The Continuity Equation, similarly to Kolmogorov Equations, describes $\\dot{p}_{t}(\\boldsymbol{x})$ , $\\boldsymbol{x}\\in\\dot{\\mathbb{R}}^{N}$ in the continuous case, and is formulated as the Partial Differential Equation (PDE) ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\dot{p}_{t}(x)+\\mathrm{div}_{x}(p_{t}u_{t})=0,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the divergence operator $\\mathrm{div}_{x}(v)$ applied to a vector field $\\boldsymbol{v}:\\mathbb{R}^{N}\\to\\mathbb{R}^{N}$ is defined by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{div}_{x}(v)=\\sum_{i=1}^{N}\\partial_{x^{i}}v^{i}(x),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "and intuitively means the total flux leaving $x$ , see Figure 2 (middle-right). This gives an intuitive explanation to the Continuity Equation: the rate of the probability $\\dot{p}_{t}(\\boldsymbol{x})$ of a state $\\boldsymbol{x}\\in\\mathbb{R}^{N}$ equals the total incoming probability flux, $p_{t}u_{t}$ , at $x$ . In the discrete case (CTMC) the Continuity Equation (equation 16) holds as is, once the discrete divergence operator is properly defined, i.e., to measure the outgoing flux from a discrete state. In more detail, given some vector field, which in the discrete case is a scalar-valued function over pairs of states, $v:{\\mathcal{D}}\\times{\\mathcal{D}}\\to{\\mathbb{R}}.$ , the discrete divergence is ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname{div}_{x}(v)=\\sum_{z\\in\\mathcal{D}}\\left[v(z,x)-v(x,z)\\right],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $v(z,x)$ represents the flux $x\\to z$ and $v(x,z)$ represent the opposite flux $z\\rightarrow x$ ; see Figure 2, right. Now, in our case (see Figure 2, middle-left), the probability flux at a state $x\\in\\mathcal{D}$ involves all sequences with at most one token difference from $x$ , i.e., the probability flux $p_{t}u_{t}$ at $x$ takes the form $v(\\stackrel{.}{x},z)=p_{t}(z)u_{t}^{i}(x^{i},z)$ and $v(z,x)=p_{t}(x)u_{t}^{i}(z^{i},x)$ for $z$ and $x$ that differ only in the $i^{\\th}$ -th token, $\\begin{array}{r}{v(x,x)=p_{t}(x)\\sum_{i=1}^{N}u_{t}^{i}(x^{i},x)}\\end{array}$ , and $v(x,z)=0$ for all other $(z,x)\\in\\mathcal{D}\\times\\mathcal{D}$ . A direct calculation now shows (see Appendix E.1): ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{div}_{x}(p_{t}u_{t})=-\\sum_{z\\in\\mathcal{D}}p_{t}(z)\\left[\\sum_{i=1}^{N}\\delta_{z}(x^{\\bar{i}})u_{t}^{i}(x^{i},z)\\right].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Checking that a probability velocity $u_{t}$ generates a probability path $p_{t}$ (in the sense of Definition 1) amounts to verifying the Continuity Equation (equation 16). Indeed, using arguments from Campbell et al. (2024) and the discrete divergence operator, the PMF of $X_{t+h}$ defined by sampling according to equation 12 is ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathbb{E}_{X_{t}}\\displaystyle\\prod_{i=1}^{N}\\left[\\delta_{X_{t}}(x^{i})+h u_{t}^{i}(x^{i},X_{t})\\right]=\\mathbb{E}_{X_{t}}\\left[\\delta_{X_{t}}(x)+h\\displaystyle\\sum_{i=1}^{N}\\delta_{X_{t}}(x^{\\bar{i}})u_{t}^{i}(x^{i},X_{t})\\right]+o(h)}\\\\ &{}&{=p_{t}(x)-h\\mathrm{div}_{x}(p_{t}u_{t})+o(h)^{\\left(16\\right)}p_{t}(x)+h\\dot{p}_{t}(x)+o(h)=p_{t+h}(x)+o(h),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where we assume $X_{t}~\\sim~p_{t}$ , the first equality uses the identity $\\begin{array}{l}{\\prod_{i}\\left[a^{i}+h b^{i}\\right]}\\ =\\ \\prod_{i}a^{i}\\ +}\\end{array}$ $\\begin{array}{r}{h\\sum_{i}(\\prod_{j\\neq i}a^{j})b^{i}+o(h)}\\end{array}$ , the second equality uses equation 19, and the previous-to-last equality uses the Continuity Equation (equation 16). This shows that if the Continuity Equation holds then $u_{t}$ generates $p_{t}$ in the sense of Definition 1. ", "page_idx": 4}, {"type": "text", "text": "Table 1: Generating (marginal) velocity fields have identical form for the continuous and discrete Flow Matching when using denoiser/noise-prediction parameterization; $\\hat{x}_{1|t}(z)=\\mathbb{E}_{X_{1}\\sim p_{t}(\\cdot|z)}X_{1}$ is the standard continuous denoiser (a.k.a. $x$ -prediction) and $\\hat{x}_{0|t}(z)=\\mathbb{E}_{X_{0}\\sim p_{t}(\\cdot|z)}X_{0}$ is the standard noise-prediction (a.k.a. $\\epsilon$ -prediction). ", "page_idx": 5}, {"type": "table", "img_path": "GTDKo3Sv9p/tmp/d552cf9d8ab284d9244f8d1c78fb7e477bc7a41e16aa6d5f0f18bc058a7977ff.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "Conditional and marginal generating velocities. We provide the probability velocities generating the conditional probability paths $p_{t}(x|\\Bar{x_{0}},x_{1})$ defined in equations 7 and 8. Then, using the marginalization formula in equation 14 we end up with a closed-form marginal velocity for the probability paths $p_{t}(\\boldsymbol{x})$ . In Appendix E.3 we show ", "page_idx": 5}, {"type": "text", "text": "Theorem 3 (Probability velocity of conditional paths). $A$ generating probability velocity for the conditional paths $p_{t}(x|x_{0},x_{1})$ defined in equations 7 and 8 is ", "page_idx": 5}, {"type": "equation", "text": "$$\nu_{t}^{i}(x^{i},z|x_{0},x_{1})=\\sum_{j=1}^{m}a_{t}^{i,j}w^{j}(x^{i}|x_{0},x_{1})+b_{t}^{i}\\delta_{z}(x^{i}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Now, computing the marginal probability velocity using equation 14 applied to the conditional probability velocity in equation 21 gives ", "page_idx": 5}, {"type": "equation", "text": "$$\nu_{t}^{i}(x^{i},z)=\\sum_{j=1}^{m}a_{t}^{i,j}\\hat{w}_{t}^{j}(x^{i},z)+b_{t}^{i,j}\\delta_{z}(x^{i}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the posteriors $\\hat{w}_{t}^{j}$ of $w^{j}$ (that are later shown to be tractable to learn) are defined by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\hat{w}_{t}^{j}(x^{i},z)=\\sum_{x_{0},x_{1}\\in\\mathcal D}w^{j}(x^{i}|x_{0},x_{1})p_{t}(x_{0},x_{1}|z),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $p_{t}(x_{0},x_{1}|z)$ (defined in equation 15) is the posterior probability of $x_{0},x_{1}$ conditioned on the current state $X_{t}=z$ . A useful instantiation of the general velocity in equation 22 is when considering the path family in equation 9, for which $w^{1}(x^{i}|x_{0},x_{1})=\\delta_{x_{1}}(x^{i})$ , $w^{2}(x^{i}|x_{0},x_{1})=\\delta_{x_{0}}(x^{i}),\\,r$ $\\kappa_{t}^{i,1}=$ $\\kappa_{t},\\,\\kappa_{t}^{i,2}=1-\\kappa_{t},\\,\\dot{\\kappa}_{t}\\geq0$ (i.e., monotonically non-decreasing in $t$ ) and in this case equation 22 reads as ", "page_idx": 5}, {"type": "equation", "text": "$$\nu_{t}^{i}(x^{i},z)=\\frac{\\dot{\\kappa}_{t}}{1-\\kappa_{t}}\\left[p_{1|t}(x^{i}|z)-\\delta_{z}(x^{i})\\right]\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where we use the notation $\\begin{array}{r}{p_{1|t}(x^{i}|\\boldsymbol{z})=\\sum_{\\boldsymbol{x}_{0},x_{1}}\\delta_{x_{1}}(x^{i})p_{t}(x_{0},x_{1}|\\boldsymbol{z})}\\end{array}$ for the probability denoiser. ", "page_idx": 5}, {"type": "text", "text": "Sampling backward in time. We can also sample backwards in time by following the sampling rule $X_{t-h}^{i}\\sim\\bar{\\delta}_{X_{t}^{i}}(\\cdot)-h u_{t}^{i}(\\cdot,X_{t})$ . In this case $-u_{t}^{i}(\\dot{x}^{i},z)$ should satisfy equation 13. A (backward-time) generating probability velocity can then be achieved from equation 22 with the simple change to the coefficients $a_{t}^{i,j}$ and $b_{t}^{i,j}$ , see Appendix E.4. For $p_{t}$ defined with equation 9 the generating velocity is ", "page_idx": 5}, {"type": "equation", "text": "$$\nu_{t}^{i}(x^{i},z)=\\frac{\\dot{\\kappa}_{t}}{\\kappa_{t}}\\left[\\delta_{z}(x^{i})-p_{0|t}(x^{i}|z)\\right],\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where in this case $\\begin{array}{r}{p_{0|t}(x^{i}|z)=\\sum_{x_{0},x_{1}\\in\\mathcal{D}}\\delta_{x_{0}}(x^{i})p_{t}(x_{0},x_{1}|z)}\\end{array}$ is the probability noise-prediction. ", "page_idx": 5}, {"type": "text", "text": "Remarkably, the generating velocity fields in 24 and 25 take the exact same form as the generating (a.k.a. marginal) velocity fields in continuous flow matching when parameterized via the denoiser or noise-prediction parameterizations and using the same schedulers, see Table 1 and Appendix E.9 for explanation of the continuous case. In Appendix E.4 we provide the backward-time version of Theorem 3. ", "page_idx": 6}, {"type": "text", "text": "Corrector sampling. Combining the forward-time $\\hat{u}_{t}$ ( equation 24) and backward-time $\\check{u}_{t}$ ( equation 25), i.e., ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\bar{u}_{t}^{i}(x^{i},z)=\\alpha_{t}\\hat{u}_{t}^{i}(x^{i},z)-\\beta_{t}\\check{u}_{t}^{i}(x^{i},z),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "provides a valid forward-time probability velocity field (i.e., satisfies equation 13) for $t\\in(0,1)$ as long as $\\alpha_{t},\\beta_{t}>0$ . This velocity field can be used for two types of corrector sampling: (i) When $\\alpha_{t}-\\beta_{t}=1$ sampling with $\\bar{u}_{t}$ leads to corrector sampling where intuitively each step moves $1+\\beta_{t}$ forward in time and $-\\beta_{t}$ backwards, which allows reintroducing noise into the sampling process; and (ii) when $\\alpha_{t}-\\beta_{t}=0$ sampling with $\\bar{u}_{t}$ when fixing $t\\in(0,1)$ leads to corrector iterations where limit samples distribute according to $p_{t}$ . In Appendix E.6 we prove: ", "page_idx": 6}, {"type": "text", "text": "Theorem 4. For perfectly trained posteriors and $\\alpha_{t},\\beta_{t}\\,>\\,0_{\\!}$ , $t\\,\\in\\,(0,1)$ , $\\bar{u}_{t}$ in equation $26$ is $a$ probability velocity, i.e., satisfies equation $^{13}$ , and: (i) For $\\alpha_{t}-\\beta_{t}=1$ , $\\bar{u}_{t}$ provides a probability velocity generating $p_{t}$ ; $(i i)$ For $\\alpha_{t}-\\beta_{t}=0$ , repeatedly sampling with $\\bar{u}_{t}$ at fixed $t\\,\\in\\,(0,1)$ and sufficiently small $h$ is guaranteed to converge to a sample from $p_{t}$ . ", "page_idx": 6}, {"type": "text", "text": "One simplification to equation 26 can be done in the case of paths constructed with conditional as in equation 9, independent coupling $\\pi(x_{0},x_{1})=p(x_{0})q(x_{1})$ , and i.i.d. source $\\begin{array}{r}{p(x_{0})=\\prod_{i=1}^{N}p(x_{0}^{i})}\\end{array}$ , e.g., $p(\\boldsymbol{x}_{0}^{i})$ is uniform over $[d]$ or $\\delta_{\\mathfrak{m}}(x_{0}^{i})$ . In this case, the backward-time formula in equation 25 take an equivalent simpler form ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\check{u}_{t}^{i}(x^{i},z)=\\frac{\\dot{\\kappa}_{t}}{\\kappa_{t}}\\left[\\delta_{z}(x^{i})-p(x^{i})\\right],\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "which does not require estimation of the posterior $p_{0|t}$ . See Appendix E.5 for the derivation. ", "page_idx": 6}, {"type": "text", "text": "Training. Equation 22 shows that for generating samples from a probabilty path $p_{t}(\\boldsymbol{x})$ we require the posteriors $\\hat{w}_{t}^{j}(x^{i}|X_{t})$ . Training such posteriors can be done by minimizing the loss ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\theta)=-\\sum_{j\\in[m],i\\in[N]}\\mathbb{E}_{t,(X_{0},X_{1}),X_{t},Y_{j}^{i}}\\log\\hat{w}_{t}^{j}(Y_{j}^{i}|X_{t};\\theta),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $t$ is sampled according to some distribution in $[0,1]$ (we used uniform), $(X_{0},X_{1})\\sim\\pi(X_{0},X_{1})$ , $X_{t}\\sim p_{t}(X_{t}|X_{0},X_{1})$ , and $\\mathbf{\\bar{\\}}Y_{j}^{i}\\sim w^{j}(Y_{j}^{i}|X_{0},X_{1})$ ; $\\theta\\in\\mathbb{R}^{p}$ denotes the learnable parameters. In the common case we use in this paper of learning a single posterior, i.e., the probability denoiser $p_{1|t}$ , the loss takes the form $\\begin{array}{r}{\\mathcal{L}(\\theta)=-\\sum_{i\\in[N]}\\mathbb{E}_{t,(X_{0},X_{1}),X_{t}}\\log p_{1|t}(X_{1}^{i}|X_{t}).}\\end{array}$ In Appendix E.7 we prove: ", "page_idx": 6}, {"type": "text", "text": "Proposition 5. The minimizer of $\\mathcal{L}$ (equation 28) is $\\hat{w}_{t}^{j}(x^{i}|X_{t})$ (equation 23). ", "page_idx": 6}, {"type": "text", "text": "3 Related work ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In the section we cover the most related work to ours; in Appendix A we cover other related work. ", "page_idx": 6}, {"type": "text", "text": "Discrete Flows (Campbell et al., 2024) is probably the most related work to ours. We build upon their CTMC framework and offer the following generalizations and simplifications: We consider arbitrary couplings $(X_{0},X_{1})$ , and offer a novel and rather general family of probability paths (equation 8) for which we provide the generating probability velocities in a unified closed-form formula (equations 22-25). These in particular recreate the same formulas as the continuous Flow Matching counterpart (Table 1). We furthermore develop a general corrector velocity (equation 26) that unifies both corrector iterations (Song et al., 2020; Campbell et al., 2022) and stochastic sampling of Campbell et al. (2024). We show that particular choices of noise schedulers $\\kappa_{t}$ ( $\\vert\\kappa_{t}=t$ reproduces Campbell et al. (2024)) and corrector schedulers provide a boost in results. Lastly, we opted for the term probability velocity for $u_{t}^{i}(x^{i},X_{t})$ as it is not precisely a rate matrix in the state space $\\mathcal{D}\\times\\mathcal{D}$ used in CTMC since $u_{t}^{i}(x^{i},z)$ for all $i\\in[N]$ define multiple self-edges $z\\rightarrow z$ . ", "page_idx": 6}, {"type": "text", "text": "Masked modeling (Ghazvininejad et al., 2019; Chang et al., 2022). In case of a masked model, i.e., when the source distribution is $\\dot{p}(x)=\\delta_{\\mathfrak{m}}(x)$ , we achieve an interesting connection with MaskGit showing it is actually an instance of Discrete Flow Matching with a small yet crucial change to its sampling algorithm. First, in Appendix E.8 we prove that in the masked setting, the probability denoiser $p_{1|t}$ is time-independent: ", "page_idx": 6}, {"type": "table", "img_path": "GTDKo3Sv9p/tmp/4c399592c541806f0010e67d0af97ae0426d2b66e34668d6cd8a50c32cb3d18e.jpg", "table_caption": ["Table 2: Generative perplexity on unconditional text generation compared to prior work. All models are sampled without the use of temperature or corrector steps. Double precision sampling results are reported in Table 5. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "GTDKo3Sv9p/tmp/71de54db8d9c579c3a9708cc16701feebb1fd024d440994a9bfb0db09543c739.jpg", "table_caption": ["Table 3: Generative perplexity on conditional text generation. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Proposition 6. For paths defined by equations 7 and 9 with source $p(x)\\,=\\,\\delta_{\\mathfrak{m}}(x)$ the posterior $p_{t}(\\bar{x_{0}},x_{1}|z)=p(x_{0},\\bar{x}_{1}|z)$ is time-independent. Consequently, the probability denoiser $\\bar{p_{1|t}(x^{i}|z)}=$ $p_{1}(x^{i}|z)$ is also time-independent. ", "page_idx": 7}, {"type": "text", "text": "This shows that the probability denoiser can be learned with no time dependence, similar to the unmasking probabilities in MaskGit. During sampling however, there are two main differences between our sampling and MaskGit sampling. First, unmasking of tokens in our algorithm is done according to the probability $\\delta_{X_{t}}(x^{i})+\\bar{h}u_{t}^{i}(\\bar{x}^{i},X_{t})$ independently for each token $x^{i}$ , $i\\in[N]$ . This procedure is justified as it samples from the correct probability asymptotically via the derivation of the Continuity Equation 20. This is in contrast to MaskGit that prioritizes the token to be unmasked according to some confidence. In the experiments section we show that MaskGit\u2019s prioritization, although has some benefit in the very low NFE regime, is actually introducing a strong bias in the sampling procedure and leads to inferior overall results. Secondly, using corrector sampling allows for reintroducing masks to already unmasked tokens in a way that is still guaranteed to produce samples from $p_{t}$ , see Theorem 4; we find this to have a significant positive effect on the generation quality. ", "page_idx": 7}, {"type": "text", "text": "Discrete diffusion. D3PM (Austin et al., 2021a) and Argmax flows (Hoogeboom et al., 2021) introduced diffusion in discrete spaces by proposing a corruption process for categorical data. A later work by Campbell et al. (2022) introduced discrete diffusion models with continuous time, and Lou et al. (2023) proposed learning probability ratios, extending score matching (Song and Ermon, 2019) to discrete spaces. ", "page_idx": 7}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We evaluate our method on the tasks of language modeling, code generation, and image generation. For language modeling, we compare the proposed method against prior work considering the widely used generative perplexity metric. We scale the models to 1.7 billion parameters and present results on coding tasks, i.e., HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021b), demonstrating the most promising results to date in a non-autoregressive context. In image generation, we present results for a fully discrete CIFAR10 (Krizhevsky et al., 2009). Further details of the experimental setup for each model are provided in Appendix G. ", "page_idx": 7}, {"type": "table", "img_path": "GTDKo3Sv9p/tmp/8530e71e22af68d506388bc82c307c8e235f9fb88114b7284e5eb7664111f573.jpg", "table_caption": ["Table 4: Execution based code generation evaluation. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Experimental setup. In our experiments we used the masked source, i.e., $p=\\,\\delta_{\\mathfrak{m}}$ , and trained with both unconditional coupling (U-coupling, equation 4) and conditional couplings (C-coupling, equation 5) with the probability path defined in equations 7, 9 and in one case 10. We trained a probability denoiser (loss in equation 28) and sampled using the generating velocity in equation 24 and Algorithm 1. We used a particular choice of probability path scheduler $\\kappa_{t}$ , as well as corrector steps defined by a scheduler $\\alpha_{t}$ and temperature annealing. We found the choice of these schedulers to be pivotal for the model\u2019s performance. In Appendix D we perform an ablation study, evaluating various scheduler choices. ", "page_idx": 8}, {"type": "text", "text": "4.1 Language modeling ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We experimented with our method in three settings: (i) Small model (150M parameters) - comparison to other non-autoregressive baselines in unconditional text generation; (ii) Large model (1.7B parameters) - comparison to autoregressive models in conditional text generation; and (iii) Large model (1.7B parameters) - conditional code generation. As computing exact likelihood for nonautoregressive model is a challenge, for (i),(ii) we use the generative perplexity metric (Appendix G measured with GPT2 (Radford et al., 2019), Llama-2 (Touvron et al., 2023), and Llama-3, and we also monitor the sentence entropy (Appendix G) to measure diversity of tokens and flag repetitive sequences, which typically yield low perplexity. Throughout our experiments we noticed entropy $\\geq6$ usually corresponds to diverse texts. For (iii) we evaluated using the success rate of coding tasks. ", "page_idx": 8}, {"type": "text", "text": "Evaluation against prior work. We evaluate our method against prior work on non-autoregressive modeling. For a fair comparison, all methods are trained on a 150M parameters models using the OpenWebText (Gokaslan and Cohen, 2019) dataset. We also fix all sampling hyperparameters to the most basic settings, i.e., no temperature, top probability, corrector steps, etc. For our method we tried two paths defined by equations 9 and 10. Results are reported in Table 2, where our method outperforms all baselines in generative perplexity for all numbers of function evaluations (NFE). ", "page_idx": 8}, {"type": "text", "text": "Conditional text generation. In this experiment, we train both C-coupling and U-coupling 1.7B parameters FM models with paths defined by equation 9 on a large scale data mix (Touvron et al., 2023). Table 3 presents the generative perplexity of conditional generations from our method; the conditions we used are the prefixes of the first 1000 samples in OpenWeb dataset. We also compare to existing state-of-the-art autoregressive models. Our results demonstrate that our model effectively narrows the gap in generative perplexity with autoregressive models, while maintaining an entropy comparable to the recent Llama-3 8B model. Furthermore, we note the C-coupling trained model produces slightly better perplexity in conditional tasks than the U-coupling model. In Appendix I we present qualitative conditional samples produced by our U-coupling model. ", "page_idx": 8}, {"type": "text", "text": "Code generation. Here we trained our basic setting of a 1.7B parameters FM model with U-coupling and path as in equation 9 on a code-focused data mix (Roziere et al., 2023). Table 4 presents results on HumanEval and MBPP (1-shot) for $\\operatorname{oass}\\ @\\left\\{1,10,25\\right\\}$ . In Table 4, \u2018Oracle length\u2019 evaluates the performance of our model when conditioning on the length of the solution. This is done by inserting an \u2018end of text\u2019 token in the same position of the ground truth solution. Our method achieves non-trivial results on both tasks, which to the best of our knowledge is the first instance of a non-autoregressive method being capable of non-trivial coding tasks. In Appendix C, we analyze the proposed method for code infilling, which can be achieved as our model allows non-autoregressive generation. Lastly, in Appendix H we show qualitative examples of success and failure cases produced by our model on the coding tasks, and in Appendix H.3 we show examples of code infilling. ", "page_idx": 8}, {"type": "image", "img_path": "GTDKo3Sv9p/tmp/5952bc2c85bf5e0e067feb4c1cd67b3b6a615b5f7c489eb85e25b1e721143665.jpg", "img_caption": ["Figure 3: FID and Inception scores vs. number of function evaluations (NFE). "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "4.2 Image generation ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We performed a fully discrete image generation, without using any metric or neighboring information between color values. We trained an FM model with U-coupling and path as in equation 9 on CIFAR10 to predict discrete color value for tokens, i.e., $d=256$ , with sequence length of $N=32\\times32\\times3$ . For generative quality we evaluate the Fr\u00e9chet Inception Distance (FID) (Heusel et al., 2017). Ablations for the probability path schedulers are provided in Figure 8 in the Appendix G. In Figure 3a we compare our method with: (i) MaskGIT (Chang et al., 2022); and (ii) (Campbell et al., 2024) which coincides with our method for a linear scheduler. More details in Appendix G. As can be seen in the Figure 3a, our method outperforms both baselines, achieving 3.63 FID at 1024 NFE. In fig. 3b we observe a similar trend when evaluating Inception score. As discussed above, MaskGit sampling performs better for low NFE but quickly deteriorates for higher NFE. We attribute this to a bias introduced in the sampling process via the confidence mechanism. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusions and future work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We introduce Discrete Flow Matching, a generalization of continuous flow matching and discrete flows that provides a large design space of discrete non-autoregressive generative models. Searching within this space we were able to train large scale language models that produce generated text with an improved generative perplexity compared to current non-autoregressive methods and able to solve coding tasks at rates not achievable before with non-autoregressive models, as far as we are aware. While reducing the number of network evaluations required to generate a discrete sample compared to autoregressive models, Discrete Flow Matching still does not achieve the level of sampling efficiency achieved by its continuous counterpart, flagging an interesting future work direction. Another interesting direction is to explore the space of probability paths in equation 8 (or a generalization of which) beyond what we have done in this paper. We believe discrete non-autoregressive models have the potential to close the gap and even surpass autoregressive models as well as unlock novel applications and use cases. As our work introduces an alternative modeling paradigm to discrete sequential data such as language and code, we feel it does not introduce significant societal risks beyond those that already exist with previous large language models. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Janice Ahn, Rishu Verma, Renze Lou, Di Liu, Rui Zhang, and Wenpeng Yin. Large language models for mathematical reasoning: Progresses and challenges. arXiv preprint arXiv:2402.00157, 2024.   \nMichael S Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic interpolants. arXiv preprint arXiv:2209.15571, 2022.   \nJacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured denoising diffusion models in discrete state-spaces. Advances in Neural Information Processing Systems, 34:17981\u201317993, 2021a.   \nJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021b.   \nVictor Besnier and Mickael Chen. A pytorch reproduction of masked generative image transformer, 2023.   \nAndreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023.   \nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.   \nAndrew Campbell, Joe Benton, Valentin De Bortoli, Thomas Rainforth, George Deligiannidis, and Arnaud Doucet. A continuous time framework for discrete denoising models. Advances in Neural Information Processing Systems, 35:28266\u201328279, 2022.   \nAndrew Campbell, Jason Yim, Regina Barzilay, Tom Rainforth, and Tommi Jaakkola. Generative flows on discrete state-spaces: Enabling multimodal flows with applications to protein co-design. arXiv preprint arXiv:2402.04997, 2024.   \nHuiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11315\u201311325, 2022.   \nHuiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy, William T Freeman, Michael Rubinstein, et al. Muse: Text-to-image generation via masked generative transformers. arXiv preprint arXiv:2301.00704, 2023.   \nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.   \nTing Chen, Ruixiang Zhang, and Geoffrey E. Hinton. Analog bits: Generating discrete data using diffusion models with self-conditioning. ArXiv, 2022.   \nJade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, and Alexandre D\u00e9fossez. Simple and controllable music generation. Advances in Neural Information Processing Systems, 36, 2024.   \nPrafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:8780\u20138794, 2021.   \nSander Dieleman, Laurent Sartran, Arman Roshannai, Nikolay Savinov, Yaroslav Ganin, Pierre H Richemond, Arnaud Doucet, Robin Strudel, Chris Dyer, Conor Durkan, et al. Continuous diffusion for categorical data. arXiv preprint arXiv:2211.15089, 2022.   \nPatrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas M\u00fcller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. arXiv preprint arXiv:2403.03206, 2024.   \nNoelia Ferruz and Birte H\u00f6cker. Controllable protein design with language models. Nature Machine Intelligence, 4(6):521\u2013532, 2022.   \nMarjan Ghazvininejad, Omer Levy, Yinhan Liu, and Luke Zettlemoyer. Mask-predict: Parallel decoding of conditional masked language models. arXiv preprint arXiv:1904.09324, 2019.   \nAaron Gokaslan and Vanya Cohen. Openwebtext corpus. http://Skylion007.github.io/ OpenWebTextCorpus, 2019.   \nXiaochuang Han, Sachin Kumar, and Yulia Tsvetkov. Ssd-lm: Semi-autoregressive simplex-based diffusion language model for text generation and modular control. arXiv preprint arXiv:2210.17432, 2022.   \nMichael Hassid, Tal Remez, Tu Anh Nguyen, Itai Gat, Alexis Conneau, Felix Kreuk, Jade Copet, Alexandre Defossez, Gabriel Synnaeve, Emmanuel Dupoux, et al. Textually pretrained speech language models. Advances in Neural Information Processing Systems, 36, 2024.   \nZhengfu He, Tianxiang Sun, Kuan Wang, Xuanjing Huang, and Xipeng Qiu. Diffusionbert: Improving generative masked language models with diffusion models. In Annual Meeting of the Association for Computational Linguistics, 2022.   \nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Advances in Neural Information Processing Systems (NeurIPS), 2017.   \nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840\u20136851, 2020.   \nEmiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forr\u00e9, and Max Welling. Argmax flows and multinomial diffusion: Learning categorical distributions. Advances in Neural Information Processing Systems, 34:12454\u201312465, 2021.   \nShima Imani, Liang Du, and Harsh Shrivastava. Mathprompter: Mathematical reasoning using large language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 5: Industry Track), pages 37\u201342, 2023.   \nFelix Kreuk, Gabriel Synnaeve, Adam Polyak, Uriel Singer, Alexandre D\u00e9fossez, Jade Copet, Devi Parikh, Yaniv Taigman, and Yossi Adi. Audiogen: Textually guided audio generation. arXiv preprint arXiv:2209.15352, 2022.   \nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. arXiv, 2009.   \nRaymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161, 2023.   \nXiang Lisa Li, John Thickstun, Ishaan Gulrajani, Percy Liang, and Tatsunori Hashimoto. Diffusion-lm improves controllable text generation. ArXiv, 2022.   \nZheng-Wen Lin, Yeyun Gong, Yelong Shen, Tong Wu, Zhihao Fan, Chen Lin, Weizhu Chen, and Nan Duan. Genie : Large scale pre-training for generation with diffusion model. In ArXiv, 2022.   \nYaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022.   \nXingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022.   \nAaron Lou, Chenlin Meng, and Stefano Ermon. Discrete diffusion language modeling by estimating the ratios of the data distribution. arXiv preprint arXiv:2310.16834, 2023.   \nJustin Lovelace, Varsha Kishore, Chao gang Wan, Eliot Shekhtman, and Kilian Q. Weinberger. Latent diffusion for language generation. ArXiv, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Ali Madani, Ben Krause, Eric R Greene, Subu Subramanian, Benjamin P Mohr, James M Holton, Jose Luis Olmos, Caiming Xiong, Zachary Z Sun, Richard Socher, et al. Large language models generate functional protein sequences across diverse families. Nature Biotechnology, 41(8): 1099\u20131106, 2023. ", "page_idx": 12}, {"type": "text", "text": "James R Norris. Markov chains. Number 2. Cambridge university press, 1998. ", "page_idx": 12}, {"type": "text", "text": "William Peebles and Saining Xie. Scalable diffusion models with transformers. arXiv preprint arXiv:2212.09748, 2022.   \nAram-Alexandre Pooladian, Heli Ben-Hamu, Carles Domingo-Enrich, Brandon Amos, Yaron Lipman, and Ricky TQ Chen. Multisample flow matching: Straightening flows with minibatch couplings. arXiv preprint arXiv:2304.14772, 2023.   \nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019. https://api.semanticscholar.org/ CorpusID:160025533.   \nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684\u201310695, 2022.   \nBernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, M Pawan Kumar, Emilien Dupont, Francisco JR Ruiz, Jordan S Ellenberg, Pengming Wang, Omar Fawzi, et al. Mathematical discoveries from program search with large language models. Nature, 625(7995):468\u2013475, 2024.   \nBaptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, J\u00e9r\u00e9my Rapin, et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023.   \nNikolay Savinov, Junyoung Chung, Mikolaj Binkowski, Erich Elsen, and Aaron van den Oord. Step-unrolled denoising autoencoders for text generation. arXiv preprint arXiv:2112.06749, 2021.   \nUriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022.   \nYang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019.   \nYang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020.   \nHannes Stark, Bowen Jing, Chenyu Wang, Gabriele Corso, Bonnie Berger, Regina Barzilay, and Tommi Jaakkola. Dirichlet flow matching with applications to dna sequence design. arXiv preprint arXiv:2402.05841, 2024.   \nJianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024.   \nAlexander Tong, Nikolay Malkin, Guillaume Huguet, Yanlei Zhang, Jarrid Rector-Brooks, Kilian Fatras, Guy Wolf, and Yoshua Bengio. Improving and generalizing flow-based generative models with minibatch optimal transport. arXiv preprint arXiv:2302.00482, 2023.   \nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \nQiang Zhang, Keyang Ding, Tianwen Lyv, Xinda Wang, Qingyu Yin, Yiwen Zhang, Jing Yu, Yuhao Wang, Xiaotong Li, Zhuoyi Xiang, et al. Scientific large language models: A survey on biological & chemical domains. arXiv preprint arXiv:2401.14656, 2024.   \nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large language models. arXiv preprint arXiv:2303.18223, 2023.   \nKaiwen Zheng, Yongxin Chen, Hanzi Mao, Ming-Yu Liu, Jun Zhu, and Qinsheng Zhang. Masked diffusion models are secretly time-agnostic masked models and exploit inaccurate categorical sampling. arXiv preprint arXiv:2409.02908, 2024.   \nAlon Ziv, Itai Gat, Gael Le Lan, Tal Remez, Felix Kreuk, Alexandre D\u00e9fossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi. Masked audio generation using a single non-autoregressive transformer. arXiv preprint arXiv:2401.04577, 2024. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Related works, continuation ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We provide here some more details on relevant related works. ", "page_idx": 14}, {"type": "text", "text": "Continuous diffusion and flows. Another line of works has been exploring the use of continuous space diffusion for discrete data, typically operating in the logits space (Dieleman et al., 2022; Li et al., 2022; Han et al., 2022; Lin et al., 2022; Chen et al., 2022). An additional body of work has been focusing on the adoption of latent diffusion-like modeling (Lovelace et al., 2022; He et al., 2022). Stark et al. (2024) proposed to learn a continuous Flow Matching on the probability simplex with Dirichlet paths. ", "page_idx": 14}, {"type": "text", "text": "Autoregressive modeling. Autoregressive models have been a significant area of focus in recent years, particularly in the context of natural language processing and machine learning (Zhao et al., 2023). Autoregressive modeling, in its most fundamental form, utilizes the chain rule to learn the joint sequence probability by breaking it down into next-token conditional probabilities. GPT-2 (Radford et al., 2019), showcased the power of autoregressive language models in generating coherent and contextually relevant text over long passages. Its successor, GPT-3 (Brown et al., 2020), further pushed the boundaries, demonstrating impressive performance across a range of tasks without taskspecific training data. Later models were adapted to other domains such as, code (Roziere et al., 2023; Li et al., 2023; Chen et al., 2021), biology (Zhang et al., 2024; Ferruz and H\u00f6cker, 2022; Madani et al., 2023), math (Romera-Paredes et al., 2024; Imani et al., 2023; Ahn et al., 2024), audio (Kreuk et al., 2022; Copet et al., 2024; Hassid et al., 2024) and more. ", "page_idx": 14}, {"type": "text", "text": "Masked generative modeling. Masked generative modeling proposes to mask a variable portion of the input sequence and training a model to predict this masked section. Ghazvininejad et al. (2019) proposed Mask-Predict, a masked language modeling with parallel decoding. Savinov et al. (2021) extended the mask-modeling approach by employing an additional loss term that incorporates rolling model predictions. MaskGIT (Chang et al., 2022) followed a similar path, for the task of class-conditioned image synthesis, Chang et al. (2023) extended this approach to high-quality textually guided image generation over low-resolution images followed by a super-resolution module. Recently, Ziv et al. (2024) proposed a text-to-music method, which relies on the MaskGIT foundations while observing that span masking boosts the quality of the generated sequence significantly. ", "page_idx": 14}, {"type": "text", "text": "B Further implementation details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Safe sampling. When sampling according to Algorithm 1 using the generating probability velocity in equation 22, an arbitrary step size $h\\,>\\,0$ can make some probabilities in $\\bar{\\delta}_{X_{t}^{i}}(\\cdot)+\\bar{h}u_{t}^{i}(\\cdot,X_{t}^{\\bar{}})$ negative and consequently require clamping and injecting further error into the sampling process that can in turn accumulate to a non-negligible global sampling error. A simple fix that guarantees a valid probability distribution while keeping the $o(h)$ sampling error at the relatively manageable price of potentially more function evaluations is using the following adaptive step size in Algorithm 1: at time $\\bar{t}\\in[0,1)$ use ", "page_idx": 14}, {"type": "equation", "text": "$$\nh_{\\mathrm{adaptive}}=\\operatorname*{min}\\left\\{h,\\operatorname*{min}_{i}\\left|\\frac{\\kappa_{t}^{i,\\ell}}{\\dot{\\kappa}_{t}^{i,\\ell}}\\right|\\right\\}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "As can be verified with the general probability velocity formula in equation 22, the above choice for $h_{\\mathrm{adaptive}}$ guarantees $\\delta_{X_{t}^{i}}(\\cdot)+\\bar{h}u_{t}^{i}(\\cdot,\\dot{X_{t}})$ is a valid PMF. As mostly used in this paper, for the probability denoiser parameterization (equation 24) the adaptive step is ", "page_idx": 14}, {"type": "equation", "text": "$$\nh_{\\mathrm{adaptive}}=\\operatorname*{min}\\left\\{h,\\frac{1-\\kappa_{t}}{\\dot{\\kappa}_{t}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "With the corrector sampling (equations 26 and 51) we have the adaptive step: ", "page_idx": 14}, {"type": "equation", "text": "$$\nh_{\\mathrm{adaptive}}=\\operatorname*{min}\\left\\{h,\\left[\\frac{\\alpha_{t}\\dot{\\kappa}_{t}}{1-\\kappa_{t}}+\\frac{\\beta_{t}\\dot{\\kappa}_{t}}{\\kappa_{t}}\\right]^{-1}\\right\\}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Conditioning. In our unconditional coupling (U-coupling), see equation 5, we define the conditioning pattern based on prefixes of random length $N_{0}<N$ , i.e., ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{I}=(\\overbrace{1,\\ldots,1}^{N_{0}},\\overbrace{0,\\ldots,0}^{N-N_{0}}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "During the training phase, we sample $N_{0}\\sim\\mathcal{U}(0,N)$ and adjust the input sequence in accordance with the mask I. ", "page_idx": 15}, {"type": "text", "text": "During conditional sampling with Algorithm 1 we replace, after each update step, the relevant tokens with the conditioned ones, i.e., $\\tilde{X}=\\mathbb{I}\\odot Y+({\\bf1}-\\mathbb{I})\\odot X$ , where $X$ is the current sample, $Y$ is the condition, and $\\mathbb{I}$ is the condition\u2019s mask. ", "page_idx": 15}, {"type": "text", "text": "NFE bound. For mask modeling, i.e., $p\\,=\\,\\delta_{\\mathfrak{m}}$ , we have seen that the probability denoiser is time-independent (see Proposition 6). Consequently, when sampling with Algorithm 1 and $u_{t}$ from equation 24 without corrector sampling one is not required to recompute the forward pass $p_{1|t}(\\cdot|X_{t})$ if $X_{t}$ is identical to $X_{t-h}$ (i.e., no $\\mathbb{M}$ has been unmasked). This means that the NFE of Algorithm 1 in this case is bounded by the number of tokens $N$ . ", "page_idx": 15}, {"type": "text", "text": "Post training scheduler change. For a trained posterior $\\hat{w}_{t}(x^{i}|z)$ of a conditional probability path as in equation 9 with a scheduler $\\kappa_{t}$ , the velocity is given by equations 24 or 25, where $\\hat{w}_{t}(x^{\\bar{i}}|\\bar{z})$ is either $\\bar{p}_{1|t}(x^{i}|z)$ or $p_{0|t}(x^{i}|z)$ respectively. In this case, we can apply the velocities in equations 24 and 25 for sampling with any scheduler $\\kappa_{t}^{\\prime}$ , using the change of scheduler formula for posteriors, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\hat{w}_{t}^{\\prime}(x^{i}|z)=\\hat{w}_{t^{\\prime}}(x^{i}|z),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\hat{w}_{t}^{\\prime}(x^{i}|\\boldsymbol{z})$ , is the posterior of the scheduler $\\kappa_{t}^{\\prime}$ , $t^{\\prime}=\\kappa_{\\kappa_{t}^{\\prime}}^{-1}$ , and $\\kappa^{-1}$ is the inverse of $\\kappa$ . The scheduler change formula in equation 32 is proved in Proposition 8. We note that by Proposition 6, for mask modeling, i.e., $p=\\delta_{\\mathfrak{m}}$ , the posterior $\\hat{w}_{t}(x^{i}|z)$ is time independent. Hence, in that case, the posterior is not affected by a scheduler change. ", "page_idx": 15}, {"type": "text", "text": "C Code infilling ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We additionally evaluate the proposed method considering the task of code infilling. In which, we are provided with an input prompt that contains various spans of masked tokens, and our goal is to predict them based on the unmasked ones. See Figure 1 (middle and right sub-figures) for a visual example. Notice, this evaluation setup is the most similar to the training process. ", "page_idx": 15}, {"type": "text", "text": "For that, we randomly mask tokens with respect to several masking rations, $\\textit{p}\\in$ $\\{0.{\\bar{0}},0.1,0.2,\\ldots,1.0\\}$ , from HumanEval and report both pass $@1$ and compiles $@1$ metrics. For the purpose of this analysis, we provide the oracle length for each masked span. In other words, the model predicts the masked tokens for already given maks length. Results for the 1.5B parameters models can be seen in Figure 4. As expected, both pass $@1$ and compiles $@1$ keep ", "page_idx": 15}, {"type": "image", "img_path": "GTDKo3Sv9p/tmp/47a79a843f9fdc04fa1f847cb373caa28dc04692aa440bafaa0fc4b8a1b95585.jpg", "img_caption": ["Figure 4: Pass $@1$ and compiles $@1$ scores for the 1.5B parameter models as a function of the input masking rations on HumanEval. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "improving as we decrease the level of input masking. Interestingly, when considering the fully masked sequence, providing the oracle prediction length significantly improves the pass $@1$ scores (6.7 vs. 11.6). ", "page_idx": 15}, {"type": "text", "text": "D Ablations ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Train and sampling path scheduler choice $\\displaystyle(\\kappa_{t})$ . We study how the choice of the probability path scheduler affects the model performance. For that, we consider a parametric family of cubic ", "page_idx": 15}, {"type": "image", "img_path": "GTDKo3Sv9p/tmp/e833ef1e7244ee2dfba84ee8c943a824f61c91973048be6e09f14fbecefddf9f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "polynomial with parameters $a,b$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\kappa_{t}\\triangleq-2t^{3}+3t^{2}+a(t^{3}-2t^{2}+t)+b(t^{3}-t^{2}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Note that $\\kappa_{0}\\,=\\,0$ and $\\,\\kappa_{1}\\,=\\,0$ and $a$ and $b$ are setting the derivative of $\\kappa_{t}$ at $t\\,=\\,0$ and $t\\,=\\,1$ , respectively. We visualize this $\\kappa_{t}$ with choices of $a,b\\in\\{0,1,2\\}$ in Figure 5a. ", "page_idx": 16}, {"type": "image", "img_path": "GTDKo3Sv9p/tmp/4d374691c01c19d3576a8561bb8c407a271769df7161be4bc59d377dee72e070.jpg", "img_caption": ["Figure 6: Path scheduler choice during training using various of constant temperature values. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "To test the effect of path schedulers in training we have trained 150M parameters models for all choices of $a,b\\in\\{0,1,2,3\\}$ . We then generate 1000 samples from each model. The samples are computed using Algorithm 1 with the path scheduler the model was trained on, and with temperature levels $\\tau\\in\\{0.8,0.9,1\\}$ , where temperature is applied via ", "page_idx": 16}, {"type": "equation", "text": "$$\np_{1|t}^{\\tau}(x^{i}|X_{t})=\\tau^{-1}\\log p_{1|t}(x^{i}|X_{t}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We then evaluate the generative perplexity of these samples with GPT-2. Figure 6 shows the results. The graphs indicate that, in the context of text modality, the cubic polynomial scheduler with $a\\equiv0,b\\equiv2$ (equivalent to a square function) achieves the highest performance. Consequently, we exclusively used this scheduler for the language models. ", "page_idx": 16}, {"type": "text", "text": "Corrector scheduler. In our experiments we only applied corrector sampling to our large models (U-coupling and C-coupling; 1.7B parameters). We used the optimal path schedulers from previous section and considered the following parametric family of schedulers for the corrector sampling: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\alpha_{t}=1+\\alpha t^{a}(1-t)^{b},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where, we set $\\beta_{t}=\\alpha_{t}-1$ and generate 1000 samples using Algorithm 1 with parameter values $a,b\\in\\{0,0.25,0.5\\}$ and $\\alpha\\in\\{10,15,20\\}$ . We then evaluated generative perplexity for these samples with Llama-2, showing results in Figure 7. These plots indicate that smaller values of $a$ and $b$ result in lower perplexity values, albeit with somewhat reduced entropy. We therefore opted for setting $a=b=0.25$ that strikes a good balance between perplexity and entropy. ", "page_idx": 16}, {"type": "text", "text": "Temperature scheduling. For temperature sampling, we consider the following scheduler: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\tau_{t}=\\tau(1-t)^{2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "image", "img_path": "GTDKo3Sv9p/tmp/76be93c1644254d491677783facef839fc7c22cb4ab57772726d74fddd3cfd00.jpg", "img_caption": ["Figure 7: Corrector scheduler ablation. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "E Theory and proofs ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "E.1 Computation of the discrete divergence ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We present the computation of the discrete divergence in equation 18, i.e., ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathrm{div}_{x}(p_{t}u_{t})=-\\sum_{z\\in\\mathcal{D}}p_{t}(z)\\left[\\sum_{i=1}^{N}\\delta_{z}(x^{\\bar{i}})u_{t}^{i}(x^{i},z)\\right].\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Computing the discrete divergence (equation 18) of the flux $p_{t}u_{t}$ at a state $x$ amounts to adding outgoing flux from $x$ and subtracting the incoming flux into $x$ . Using the fact that $\\delta_{z}(x^{\\bar{i}})=1$ if and only if $z=x$ or $z$ differs from $x$ only at the $i$ -th token, gives: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathrm{div}_{x}(p_{t}u_{t})=\\sum_{z\\in P}\\sum_{i=1}^{N}\\delta_{z}(z^{i})\\left(p_{t}(x)u_{t}^{i}(z^{i},x)-p_{t}(z)u_{t}^{i}(x^{i},z)\\right)}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle=p_{t}(x)\\sum_{i=1}^{N}\\sum_{x^{i}}\\sqrt{\\frac{=}{\\sum_{i=1}^{n}\\delta_{x}(z^{i})}\\right]u_{t}^{i}(z^{i},x)}-\\sum_{z\\in P}\\sum_{i=1}^{N}\\delta_{x}(z^{i})p_{t}(z)u_{t}^{i}(x^{i},z)}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle=p_{t}(x)\\sum_{i=1}^{N}\\left[\\sum_{z^{i}}u_{t}^{i}(z^{i},x)\\right]-\\sum_{z\\in P}\\sum_{i=1}^{N}\\delta_{x}(z^{i})p_{t}(z)u_{t}^{i}(x^{i},z)}&{~\\mathrm{~b~equation~}13}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle}&{=-\\sum_{z\\in P}\\sum_{i=1}^{N}\\delta_{x}(z^{i})p_{t}(z)u_{t}^{i}(x^{i},z),}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "that gives equation 37 after noting that $\\delta_{x}(z^{\\bar{i}})=\\delta_{z}(x^{\\bar{i}})$ . ", "page_idx": 17}, {"type": "text", "text": "E.2 Conditional velocities lead to marginal velocities ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We provide a simple proof for Theorem 2, originally proved in Campbell et al. (2024): Theorem 2. Given a conditional probability velocity $u_{t}^{i}(x^{i},X_{t}|x_{0},x_{1})$ generating a conditional probability path $p_{t}(x|x_{0},x_{1})$ , the marginal velocity defined by ", "page_idx": 18}, {"type": "equation", "text": "$$\nu_{t}^{i}(x^{i},X_{t})=\\sum_{x_{0},x_{1}\\in\\mathcal{D}}u_{t}^{i}(x^{i},X_{t}|x_{0},x_{1})p_{t}(x_{0},x_{1}|X_{t}),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "generates the marginal probability path $p_{t}(\\boldsymbol{x})$ , where by Bayes\u2019 rule ", "page_idx": 18}, {"type": "equation", "text": "$$\np_{t}(x_{0},x_{1}|X_{t})=\\frac{p_{t}(X_{t}|x_{0},x_{1})\\pi(x_{0},x_{1})}{p_{t}(x)}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof (Theorem 2). We start by taking the time derivative of the marginal probability path, $p_{t}(\\boldsymbol{x})=$ $\\begin{array}{r}{\\sum_{x_{0},x_{1}}p_{t}(x^{i}|x_{0},x_{1})\\pi(x_{0},x_{1})}\\end{array}$ , as follows, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\dot{p}_{t}(x)=\\displaystyle\\sum_{x_{0},x_{1}}\\dot{p}_{t}(x|x_{0},x_{1})\\pi(x_{0},x_{1})}\\\\ &{\\qquad=\\displaystyle\\sum_{x_{0},x_{1}}\\left(\\sum_{z}p_{t}(z|x_{0},x_{1})\\left[\\displaystyle\\sum_{i=1}^{N}\\delta_{z}(x^{\\bar{i}})u_{t}^{i}(x^{i},z|x_{0},x_{1})\\right]\\right)\\pi(x_{0},x_{1})\\qquad\\mathrm{~\\forall~\\mathrm{Continuity~Equat}~}}\\\\ &{\\qquad=\\displaystyle\\sum_{z}p_{t}(z)\\left[\\displaystyle\\sum_{i=1}^{N}\\delta_{z}(x^{\\bar{i}})\\left(\\displaystyle\\sum_{x_{0},x_{1}}u_{t}^{i}(x^{i},z|x_{0},x_{1})\\displaystyle\\frac{p_{t}(z|x_{0},x_{1})\\pi(x_{0},x_{1})}{p_{t}(z)}\\right)\\right]}\\\\ &{\\qquad=\\displaystyle\\sum_{z}p_{t}(z)\\left[\\displaystyle\\sum_{i=1}^{N}\\delta_{z}(x^{\\bar{i}})u_{t}^{i}(x^{i},z)\\right]}\\\\ &{\\qquad=\\displaystyle-\\mathrm{div}_{x}\\left(p_{t}u_{t}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Now since $u_{t}^{i}(x^{i},z)$ is a convex combinations of $u_{t}^{i}(x^{i},z|x_{0},x_{1})$ and these satisfy equation 13 then also $u_{t}^{i}(x^{i},\\breve{X}_{t})$ satisfies equation 13. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "E.3 Probability velocities generating conditional probability paths ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Equation 22 with the coefficients $a_{t}^{i,j}$ and $b_{t}^{i}$ are provided below, ", "page_idx": 18}, {"type": "equation", "text": "$$\nu_{t}^{i}(x^{i},X_{t}|x_{0},x_{1})=\\sum_{j=1}^{m}\\overbrace{\\left[\\dot{\\kappa}_{t}^{i,j}-\\kappa_{t}^{i,j}\\frac{\\dot{\\kappa}_{t}^{i,\\ell}}{\\kappa_{t}^{i,\\ell}}\\right]}^{a_{t}^{i,j}}w^{j}(x^{i}|x_{0},x_{1})+\\overbrace{\\left[\\dot{\\kappa}_{t}^{i,\\ell}\\right]}^{b_{t}^{i}}\\delta_{X_{t}}(x^{i}),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\ell=\\ell(i,t)\\stackrel{\\mathrm{def}}{=}\\underset{j\\in[m]}{\\arg\\operatorname*{min}}\\left[\\dot{\\kappa}_{t}^{i,j}/\\kappa_{t}^{i,j}\\right].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Theorem $^3$ (Probability velocity of conditional paths). A generating probability velocity for the conditional paths $p_{t}(x|x_{0},x_{1})$ defined in equations 7 and 8 is ", "page_idx": 18}, {"type": "equation", "text": "$$\nu_{t}^{i}(x^{i},X_{t}|x_{0},x_{1})=\\sum_{j=1}^{m}a_{t}^{i,j}w^{j}(x^{i}|x_{0},x_{1})+b_{t}^{i}\\delta_{X_{t}}(x^{i}),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "with $a_{t}^{i,j}=\\dot{\\kappa}_{t}^{i,j}-\\kappa_{t}^{i,j}\\dot{\\kappa}_{t}^{i,\\ell}/\\kappa_{t}^{i,\\ell}$ , and $b_{t}^{i}=\\dot{\\kappa}_{t}^{i,\\ell}/\\kappa_{t}^{i,\\ell}$ where $\\begin{array}{r}{\\ell=\\arg\\operatorname*{min}_{j\\in[m]}\\left[\\dot{\\kappa}_{t}^{i,j}/\\kappa_{t}^{i,j}\\right].}\\end{array}$ ", "page_idx": 18}, {"type": "text", "text": "Proof (Theorem 3). First, let us show that equation 40 satisfies the conditions in equation 13: Fix $X_{t}\\in\\mathcal{D}$ , and ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{x^{i}}u_{t}^{i}(x^{i},X_{t}|x_{0},x_{1})=\\sum_{x^{i}}\\sum_{j=1}^{m}\\left[\\dot{\\kappa}_{t}^{i,j}-\\kappa_{t}^{i,j}\\frac{\\dot{\\hat{\\kappa}}_{t}^{i,\\ell}}{\\kappa_{t}^{i,\\ell}}\\right]w^{j}(x^{i}|x_{0},x_{1})+\\frac{\\dot{\\kappa}_{t}^{i,\\ell}}{\\kappa_{t}^{i,\\ell}}\\delta_{X_{t}}(x^{i})}}\\\\ &{=\\displaystyle\\sum_{j=1}^{m}\\left[\\dot{\\kappa}_{t}^{i,j}-\\kappa_{t}^{i,j}\\frac{\\dot{\\kappa}_{t}^{i,\\ell}}{\\kappa_{t}^{i,\\ell}}\\right]+\\frac{\\dot{\\kappa}_{t}^{i,\\ell}}{\\kappa_{t}^{i,\\ell}}}\\\\ &{=\\displaystyle\\sum_{j=1}^{m}\\dot{\\kappa}_{t}^{i,j}+\\frac{\\dot{\\kappa}_{t}^{i,\\ell}}{\\kappa_{t}^{i,\\ell}}\\left(1-\\displaystyle\\sum_{j=1}^{m}\\kappa_{t}^{i,j}\\right)\\qquad\\qquad\\mathrm{,}\\qquad\\mathsf{\\hat{\\kappa}\\sum_{j}\\kappa_{t}^{i,j}=1,\\mathrm{and}\\sum_{j}\\dot{\\kappa}_{t}^{i,j}=0}}\\\\ &{=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and for $x^{i}\\neq X_{t}^{i}$ we have ", "page_idx": 19}, {"type": "equation", "text": "$$\nu_{t}^{i}(x^{i},X_{t}|x_{0},x_{1})=\\sum_{j=1}^{m}\\left[\\frac{\\dot{\\kappa}_{t}^{i,j}}{\\kappa_{t}^{i,j}}-\\frac{\\dot{\\kappa}_{t}^{i,\\ell}}{\\kappa_{t}^{i,\\ell}}\\right]\\kappa_{t}^{i,j}w^{j}(x^{i}|x_{0},x_{1})\\ge0\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "since \u03bait,j $\\kappa_{t}^{i,j}\\geq0$ , $\\hat{w}_{t}(x^{i}|z)\\geq0$ , and $\\begin{array}{r}{\\frac{\\dot{\\kappa}_{t}^{i,j}}{\\kappa_{t}^{i,j}}-\\frac{\\dot{\\kappa}_{t}^{i,\\ell}}{\\kappa_{t}^{i,\\ell}}\\geq0}\\end{array}$ since $\\begin{array}{r}{\\ell=\\arg\\operatorname*{min}_{j\\in[m]}\\frac{\\dot{\\kappa}_{t}^{i,j}}{\\kappa_{t}^{i,j}}}\\end{array}$ . Second, we show that $u_{t}$ satisfies the Continuity Equation (equation 16). To that end we write equation 8 as ", "page_idx": 19}, {"type": "equation", "text": "$$\nw^{\\ell}(x^{i}|x_{0},x_{1})=\\frac{1}{\\kappa_{t}^{i,\\ell}}\\left[p_{t}(x^{i}|x_{0},x_{1})-\\sum_{j\\neq\\ell}\\kappa_{t}^{i,j}w^{j}(x^{i}|x_{0},x_{1})\\right],\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\begin{array}{r}{\\ell=\\arg\\operatorname*{min}_{j\\in[m]}\\frac{\\dot{\\kappa}_{t}^{i,j}}{\\kappa_{t}^{i,j}}}\\end{array}$ . Now by differentiating $p_{t}(x|x_{0},x_{1})$ we get ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\widehat{P}_{\\mathrm{t}}(\\lambda^{[\\varepsilon]},\\alpha_{t-\\lambda})-\\frac{1}{N}\\Bigg|_{\\varepsilon=0}^{\\infty}(\\gamma_{\\varepsilon},\\alpha_{t-\\lambda})}}\\\\ &{}\\\\ {\\widehat{H}(\\lambda^{[\\varepsilon]},\\alpha_{t-\\lambda})\\geq\\frac{1}{N}\\Bigg\\{P_{0}(\\lambda^{[\\varepsilon]},\\alpha_{t-\\lambda})\\Bigg\\}\\Bigg|_{\\varepsilon=0}^{\\infty},}\\\\ &{=\\frac{1}{N}\\Bigg\\{P_{0}(\\lambda^{[\\varepsilon]},\\alpha_{t-\\lambda})\\Bigg\\}\\Bigg[\\frac{\\widehat{P}_{0}(\\lambda^{[\\varepsilon]},\\alpha_{t-\\lambda})}{N}\\Bigg]}\\\\ &{\\quad-\\frac{1}{N}\\Bigg\\{P_{0}(\\lambda^{[\\varepsilon]},\\alpha_{t-\\lambda})\\Bigg\\}\\Bigg[\\sum_{k=0}^{\\infty}\\widehat{\\mu}_{k}^{[\\varepsilon]}\\Big(\\mu^{[\\varepsilon]}(\\gamma_{\\varepsilon-\\lambda})+k\\widehat{\\mu}_{k}^{[\\varepsilon]}\\Big(\\mu^{[\\varepsilon]}(\\gamma_{\\varepsilon-\\lambda})\\Big)\\Bigg]}\\\\ &{\\quad-\\frac{1}{N}\\Bigg\\{P_{0}(\\lambda^{[\\varepsilon]},\\alpha_{t-\\lambda})\\Bigg\\}\\Bigg[\\sum_{k=0}^{\\infty}\\widehat{\\mu}_{k}^{[\\varepsilon]}\\Big(\\mu^{[\\varepsilon]}(\\gamma_{\\varepsilon-\\lambda})+k\\widehat{\\mu}_{k}^{[\\varepsilon]}\\Big(\\mu^{[\\varepsilon]}(\\gamma_{\\varepsilon-\\lambda})\\Big)}\\\\ &{\\quad-\\frac{1}{N}\\Bigg\\{P_{0}(\\lambda^{[\\varepsilon]},\\alpha_{t-\\lambda})\\Bigg\\}\\Bigg[\\sum_{k=0}^{\\infty}\\widehat{\\mu}_{k}^{[\\varepsilon]}\\Big(\\mu^{[\\varepsilon]}-\\mu_{k}^{[\\varepsilon]}\\Big)\\Bigg]\\Bigg|_{\\varepsilon=0}^{\\infty},\\quad k\\widehat{\\mu}_{k}^{[\\varepsilon]}\\Bigg\\}}\\\\ &{\\quad-\\frac{1}{N}\\Bigg\\{\\sum_{k=0}^{\\infty}\\widehat{\\mu}_{k}^{[\\varepsilon]}\\Big(\\sum_{k=0}^{\\infty}\\widehat{\\mu}_{k}^{[\\varepsilon]}(\\gamma_{\\varepsilon-k})\\Big)\\Bigg\\}\\Bigg|_{\\varepsilon=0}^{\\infty},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "E.4 Backward-time generating probability velocity. ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Here we prove the equivalent of Theorem 3 for backward-time generating probability field. But first, let us justify the backward sampling formula, ", "page_idx": 20}, {"type": "equation", "text": "$$\nX_{t-h}^{i}\\sim\\delta_{X_{t}^{i}}(\\cdot)-h u_{t}^{i}(\\cdot,X_{t}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Similar to equation 20 we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathbb{E}_{X_{t}}\\prod_{i=1}^{N}\\left[\\delta_{X_{t}}(x^{i})-h u_{t}^{i}(x^{i},X_{t})\\right]=\\mathbb{E}_{X_{t}}\\left[\\delta_{X_{t}}(x)-h\\displaystyle\\sum_{i=1}^{N}\\delta_{X_{t}}(x^{\\bar{i}})u_{t}^{i}(x^{i},X_{t})\\right]+o(h)}\\\\ &{}&{=p_{t}(x)+h\\mathrm{div}_{x}(p_{t}u_{t})+o(h)^{\\left(\\frac{16}{=}\\right)}p_{t}(x)-h\\dot{p}_{t}(x)+o(h)=p_{t-h}(x)+o(h).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Therefore if the Continuity equation holds and $-u_{t}$ satisfies the conditions in equation 13 then given $X_{t}\\sim p_{t}$ , equation 45 provides an approximation $X_{t-h}\\sim p_{t-h}+o(h)$ . The change to the generating probability velocity in equation 22 to accommodate reverse time sampling is to replace the argmin in equation 41 with argmax, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\ell=\\ell(i,t)\\triangleq\\underset{j\\in[m]}{\\arg\\operatorname*{max}}\\left[\\dot{\\kappa}_{t}^{i,j}/\\kappa_{t}^{i,j}\\right].\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "An analogous result to Theorem 3 for backward-time sampling is therefore, ", "page_idx": 20}, {"type": "text", "text": "Theorem 7 (Probability velocity of conditional paths, backward time). The probability velocity $-u_{t}$ , where $u_{t}$ defined in equation $2l$ with $\\begin{array}{r}{\\ell=\\arg\\operatorname*{max}_{j\\in[m]}\\left[\\dot{\\kappa}_{t}^{i,j}/\\kappa_{t}^{i,j}\\right]}\\end{array}$ is a backward-time generating probability velocity for the conditional paths $p_{t}(x|x_{0},x_{1})$ defined in equations 7 and 8. ", "page_idx": 20}, {"type": "text", "text": "Proof (Theorem 7). We follow the proof of Theorem 3 and indicate the relevant changes. First, for arbitrary $X_{t}\\in\\mathcal{D}$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{x^{i}}u_{t}^{i}(x^{i},X_{t})=0,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "exactly using the same arguments as the forward-time case. Now, for $x^{i}\\neq X_{t}^{i}$ we have ", "page_idx": 20}, {"type": "equation", "text": "$$\nu_{t}^{i}(x^{i},X_{t}|x_{0},x_{1})=\\sum_{j=1}^{m}\\left[\\frac{\\dot{\\kappa}_{t}^{i,j}}{\\kappa_{t}^{i,j}}-\\frac{\\dot{\\kappa}_{t}^{i,\\ell}}{\\kappa_{t}^{i,\\ell}}\\right]\\kappa_{t}^{i,j}w_{t}^{j}(x^{i}|x_{0},x_{1})\\le0\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "due to $\\ell$ being now the argmax of $\\frac{{\\dot{\\kappa}}_{t}^{i,j}}{\\kappa_{t}^{i,j}}$ . Therefore $-u_{t}$ satisfies equation 13. Lastly, we notice that the proof of the Continuity Equation follows through exactly the same also in this case. \u53e3 ", "page_idx": 20}, {"type": "text", "text": "E.5 Backward-time generating velocity for i.i.d. source $p(x_{0})$ and simple paths ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Here we consider the case of probability paths defined via the conditionals in equation 9 with independent coupling $\\pi(x_{0},x_{1})=p(x_{0})q(x_{1})$ and i.i.d. source distribution $\\begin{array}{r}{p(x_{0})=\\prod_{i=1}^{N}p(x_{0}^{i})}\\end{array}$ , where $p(x_{0}^{i})$ is some PMF over $[d]$ . In this case one can simplify the time-backward sampling formula in equation 25 by using the following one which is equivalent (i.e., their difference is divergence free and consequently generate the same probability path $p_{t}$ ): ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\check{u}_{t}(x^{i},X_{t})=\\frac{\\dot{\\kappa}_{t}}{\\kappa_{t}}\\left[\\delta_{X_{t}}(x^{i})-p(x^{i})\\right].\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The benefti in this equation is that it does not require the posterior $p_{0|t}$ , which needs to be learned in general cases. ", "page_idx": 20}, {"type": "text", "text": "To show that equation 49 is indeed a generating probability velocity it is enough to show that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathrm{div}_{x}\\left[p_{t}\\left(\\check{u}_{t}-\\check{u}_{t}^{\\star}\\right)\\right]=0,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\check{u}_{t}^{\\star}$ is the probability velocity in equation 25. Let us verify using equation 19: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\operatorname*{liv}_{x}\\left[p_{t}\\left(\\tilde{u}_{t}-\\tilde{u}_{t}^{*}\\right)\\right]=\\displaystyle\\sum_{i,:\\neq}p_{t}(z)\\delta_{x}(x^{\\dagger})\\left[p(x^{*})-\\sum_{\\alpha_{0},x_{1}}\\delta_{\\alpha_{0}}(x^{\\dagger})\\frac{p_{t}(z|x_{0},x_{1})p(x_{0})q(x_{1})}{p_{t}(z)}\\right]\\nu\\pi(x_{0},x_{1})=}&{}\\\\ {=\\displaystyle\\sum_{i,:\\neq}\\delta_{\\alpha_{0}}(x^{\\dagger})\\left[p(x^{*})p(x(z)-\\sum_{\\alpha_{0},x_{1}}\\delta_{\\alpha_{0}}(x^{\\dagger})p_{t}(z|x_{0},x_{1})p(x_{0})q(x_{1})\\right]}&{}\\\\ {=\\displaystyle\\sum_{i,:\\neq,}\\left[p(x^{*})-\\delta_{\\alpha_{0}}(x^{\\dagger})\\right]\\left(\\sum_{z}\\delta_{z}(x^{\\dagger})p_{t}(z|x_{0},x_{1})\\right)p(x_{0})q(x_{1})}&{}\\\\ {=\\displaystyle\\sum_{i,:\\neq,}\\left[p(x^{*})-\\delta_{\\alpha_{0}}(x^{\\dagger})\\right]p_{t}(x^{\\dagger}|x_{0},x_{1})p(x_{0}^{\\dagger})p(x_{1}^{\\dagger})}&{}\\\\ {=\\displaystyle\\sum_{i,:\\neq,}\\left[p(x^{*})-\\delta_{\\alpha_{0}}(x^{\\dagger})\\right]p_{t}(x^{\\dagger}|x_{0},x_{1})p(x_{0}^{\\dagger})p(x_{1}^{\\dagger})p(x_{1}^{\\dagger})}&{}\\\\ {=\\displaystyle\\sum_{i,:\\neq,}\\left[\\sum_{\\alpha_{0},x_{1}}\\left(\\sum_{z=i}\\left[p(x^{*})p(x_{0}^{\\dagger})-\\delta_{\\alpha_{0}}(x^{\\dagger})p(x_{0}^{\\dagger})\\right]\\right)p_{t}(x^{\\dagger}|x_{0},x_{1})p(x_{0}^{\\dagger})q(x_{1})}&{}\\\\ {=0,}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where in the second to last equality we used the fact that the paths we are considering have the form: $\\begin{array}{r}{p_{t}(x^{\\bar{i}}|x_{0},x_{1})=\\prod_{j\\in[N]\\backslash i}\\left[\\dot{\\kappa_{t}}\\delta_{x_{1}}\\dot{(}x^{j})+(1-\\kappa_{t})\\delta_{x_{0}}(x^{j})\\right]}\\end{array}$ , and therefore do not depend on the $i$ -th source token, $\\boldsymbol{x}_{0}^{i}$ . ", "page_idx": 21}, {"type": "text", "text": "E.6 Corrector steps ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Theorem 4. For perfectly trained posteriors and $\\alpha_{t},\\beta_{t}\\,>\\,0,\\,t\\,\\in\\,(0,1),$ $\\bar{u}_{t}$ in equation 26 is a probability velocity, i.e., satisfies equation 13, and: (i) For $\\alpha_{t}-\\beta_{t}=1$ , $\\bar{u}_{t}$ provides a probability velocity generating $p_{t}$ ; (ii) For $\\alpha_{t}-\\beta_{t}=0$ , repeatedly sampling with $\\bar{u}_{t}$ at fixed $t\\,\\in\\,(0,1)$ and sufficiently small $h$ is guaranteed to converge to a sample from $p_{t}$ . ", "page_idx": 21}, {"type": "text", "text": "Proof (Theorem 4). First let us write explicitly $\\bar{u}_{t}$ from equation 26: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{u}_{t}^{i}(x^{i},X_{t})=\\alpha_{t}\\hat{u}_{t}^{i}(x^{i},X_{t})-\\beta_{t}\\check{u}_{t}^{i}(x^{i},X_{t})}\\\\ &{\\qquad\\qquad\\qquad=\\cfrac{\\alpha_{t}\\dot{\\kappa}_{t}}{1-\\kappa_{t}}p_{1|t}(x^{i}|X_{t})+\\frac{\\beta_{t}\\dot{\\kappa}_{t}}{\\kappa_{t}}p_{0|1}(x^{i}|X_{t})-\\left[\\cfrac{\\alpha_{t}\\dot{\\kappa}_{t}}{1-\\kappa_{t}}+\\frac{\\beta_{t}\\dot{\\kappa}_{t}}{\\kappa_{t}}\\right]\\delta_{X_{t}}(x^{i}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Since equation 51 is a sum of PMFs with coefficients that sum up to zero the first condition in equation 13, i.e., $\\begin{array}{r}{\\sum_{x^{i}}\\bar{u}_{t}^{i}(x^{i},X_{t})=0}\\end{array}$ holds. The second condition in equation 13 holds since for t \u2208(0, 1) we have 1\u2212\u03bat \u03bat \u03b1t \u02d9\u03bat , \u03b2t \u02d9\u03bat \u22650. Now, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{\\mathrm{div}_{x}(p_{t}\\bar{u}_{t})=\\alpha_{t}\\mathrm{div}_{x}(p_{t}\\hat{u}_{t})-\\beta_{t}\\mathrm{div}_{x}(p_{t}\\breve{u}_{t})}&&{\\mathrm{~\\beta~binearity~of~div}}\\\\ &{\\mathrm{~\\\\\\\\\\\\\\\\\\}=-\\alpha_{t}\\dot{p}_{t}(x)+\\beta_{t}\\dot{p}_{t}(x)}&&{\\mathrm{~\\\\\\\\\\mathrm{~\\mathbb{~p~Equation~16}~}}}\\\\ &{\\mathrm{~\\\\\\\\\\\\\\\\\\\\}=-(\\alpha_{t}-\\beta_{t})\\dot{p}_{t}(x).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For (i): Using equation 52 with $\\alpha_{t}-\\beta_{t}=1$ we get that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname{div}_{x}(p_{t}\\bar{u}_{t})=-\\dot{p}_{t}(x),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "i.e., $\\bar{u}_{t}$ satisfies the continuity equation and therefore generates $p_{t}$ . ", "page_idx": 21}, {"type": "text", "text": "For (ii): Setting $\\alpha_{t}\\,-\\,\\beta_{t}\\,=\\,0$ in equation 52 we get $\\mathrm{div}_{x}(p_{t}\\bar{u}_{t})\\,=\\,0$ and therefore similar to equation 20 we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{p_{t}(x)={p_{t}(x)}-h\\mathrm{div}_{x}(p_{t}\\bar{u}_{t})}\\\\ {\\qquad=\\mathbb{E}_{X_{t}}\\left[\\delta_{X_{t}}(x)+h\\displaystyle\\sum_{i=1}^{N}\\delta_{X_{t}}(x^{\\bar{i}})\\bar{u}_{t}^{i}(x^{i},X_{t})\\right]}\\\\ {\\qquad=\\displaystyle\\sum_{z}p(x|z)p_{t}(z),}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where using equation 51 we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle p({\\boldsymbol x}|{\\boldsymbol z})=h\\sum_{i=1}^{N}\\frac{\\alpha_{t}\\dot{\\kappa}_{t}}{1-\\kappa_{t}}\\delta_{z}(x^{\\bar{i}})p_{1|t}(x^{i}|{\\boldsymbol z})+h\\sum_{i=1}^{N}\\frac{\\beta_{t}\\dot{\\kappa}_{t}}{\\kappa_{t}}\\delta_{z}(x^{\\bar{i}})p_{0|1}(x^{i}|{\\boldsymbol z})}}\\\\ {{\\displaystyle\\qquad+\\left(1-h\\sum_{i=1}^{N}\\left[\\frac{\\alpha_{t}\\dot{\\kappa}_{t}}{1-\\kappa_{t}}+\\frac{\\beta_{t}\\dot{\\kappa}_{t}}{\\kappa_{t}}\\right]\\right)\\delta_{z}(x^{i})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "For sufficiently small $h\\,>\\,0$ therefore $p(x|z)$ is a convex combination of PMFs (in red) $x$ and consequently is itself a PMF in $x$ , that is $p(x|z)$ is a probability transition matrix, and $p_{t}(\\boldsymbol{x})$ is its stationary distribution, i.e., it is an eigenvector of $p(x|z)$ with eigenvalue 1, which is maximal. To prove convergence of the iterations in equation 53 we are left with showing that $p(x|z)$ is irreducible and a-periodic, see Norris (1998) (Theorem 1.8.3). Irreducibly of $p(x|z)$ can be shown by connecting each two states $z,z^{\\prime}$ by changing one token at a time, and assuming that $p_{1|t}$ or $p_{0|t}$ are strictly positive (which is usually the case since as at-least one of them is defined as soft-max of finite logits); a-periodicity is proved by showing $p(x|x)>0$ which is true as the coefficient of $\\delta_{z}(x)$ is greater than zero for sufficiently small $h>0$ . Lastly, note that the iteration in equation 53 changes one token at a time. An approximation to this sampling can be achieved using our standard parallel sampling via equation 12, justified by equation 20. ", "page_idx": 22}, {"type": "text", "text": "E.7 Training ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Proposition 5. The minimizer of $\\mathcal{L}$ (equation 28) is $\\hat{w}_{t}^{j}(x^{i}|X_{t})$ (equation 23). ", "page_idx": 22}, {"type": "text", "text": "Proof (Proposition 5). It is enough to prove the claim for $m=1$ , with a single $w(x^{i}|x_{0},x_{1})$ . ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}(\\theta)=-\\displaystyle\\frac{1}{N}\\sum_{i=1}^{N}\\mathbb{E}_{t}\\sum_{x_{0},x_{1},z,y^{i}}\\log\\hat{w}_{t}(y^{i}|z;\\theta)w(y^{i}|x_{0},x_{1})p_{t}(z|x_{0},x_{1})\\pi(x_{0},x_{1})}\\\\ &{\\quad\\quad=\\displaystyle-\\frac{1}{N}\\sum_{i=1}^{N}\\mathbb{E}_{t}\\sum_{z}p_{t}(z)\\left[\\sum_{y^{i}}\\log\\hat{w}_{t}(y^{i}|z;\\theta)\\left(\\sum_{x_{0},x_{1}}w(y^{i}|x_{0},x_{1})\\frac{p_{t}(z|x_{0},x_{1})\\pi(x_{0},x_{1})}{p_{t}(z)}\\right)\\right]}\\\\ &{\\quad\\quad=-\\mathbb{E}_{t,X_{t}}\\frac{1}{N}\\sum_{i=1}^{N}\\left[\\sum_{y^{i}}\\log\\hat{w}_{t}(y^{i}|X_{t};\\theta)\\hat{w}_{t}(y^{i}|X_{t})\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "that amounts to minimizing the Cross Entropy loss between $\\hat{w}_{t}(x^{i}|X_{t};\\theta)$ and $\\hat{w}_{t}(x^{i}|X_{t})$ for all $i\\in[N]$ , the minimizer of which satisfies $\\hat{w}_{t}(\\bar{x^{i}}|X_{t};\\theta)\\equiv\\hat{w}_{t}(x^{i}|X_{t})$ . \u53e3 ", "page_idx": 22}, {"type": "text", "text": "E.8 Time-independent posterior for masked modeling ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Proposition 6. For paths defined by equations 7 and 9 with source $p(x)\\,=\\,\\delta_{\\mathfrak{m}}(x)$ the posterior $p_{t}(x_{0},x_{1}|z)=p(x_{0},x_{1}|z)$ is time-independent. Consequently, the probability denoiser $\\bar{p_{1|t}(x^{i}|z)}=$ $p_{1}(x^{i}|z)$ is also time-independent. ", "page_idx": 22}, {"type": "text", "text": "Proof (Proposition $6$ ). First, ", "page_idx": 22}, {"type": "equation", "text": "$$\np_{t}(z^{i}|x_{0},x_{1})=(1-\\kappa_{t})\\delta_{\\mathfrak{m}}(z^{i})+\\kappa_{t}\\delta_{x_{1}}(z^{i})=\\left\\{\\!\\!\\begin{array}{l l}{(1-\\kappa_{t})}&{z^{i}={\\mathfrak{m}}}\\\\ {\\kappa_{t}\\delta_{x_{1}}(z^{i})}&{z^{i}\\neq{\\mathfrak{m}}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and therefore ", "page_idx": 22}, {"type": "equation", "text": "$$\np_{t}(z|x_{0},x_{1})=\\left[\\prod_{i:z^{i}=\\mathtt{m}}(1-\\kappa_{t})\\prod_{i:z^{i}\\neq\\mathtt{m}}\\kappa_{t}\\right]\\prod_{i:z^{i}\\neq\\mathtt{m}}\\delta_{x_{1}}(z^{i}).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The posterior now gives ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{p_{t}\\left(z\\left|x_{0},x_{1}\\right)\\pi\\left(x_{0},x_{1}\\right)}{p_{t}\\left(z\\right)}=\\frac{\\displaystyle\\left[\\prod_{i=1}^{N}p_{t}\\left(z^{i}\\left|x_{0},x_{1}\\right|\\right)\\right]\\pi\\left(x_{0},x_{1}\\right)}{\\displaystyle\\sum_{\\tilde{x}_{0},\\tilde{x}_{1}}\\left[\\prod_{j=1}^{N}p_{t}\\left(z^{j}\\left|\\tilde{x}_{0},\\tilde{x}_{1}\\right|\\right)\\right]\\pi\\left(\\tilde{x}_{0},\\tilde{x}_{1}\\right)}}\\\\ &{\\phantom{\\frac{p_{t}\\left(z\\left|x_{0},x_{1}\\right)\\pi\\left(x_{0},x_{1}\\right)}{\\displaystyle\\sum_{\\tilde{x}_{0},\\tilde{x}_{1}}\\left[\\prod_{j\\leq i\\geq m}(1-\\kappa_{t})\\prod_{i\\leq i\\neq m}\\kappa_{t}\\right]\\left[\\prod_{i:z^{i}\\neq m}\\delta_{x_{1}}\\left(z^{i}\\right)\\right]\\pi\\left(x_{0},x_{1}\\right)}}}\\\\ &{\\phantom{\\frac{p_{t}\\left(z\\left|x_{0},x_{1}\\right)\\pi\\left(x_{0},x_{1}\\right)}{\\displaystyle\\sum_{\\tilde{x}_{0},\\tilde{x}_{1}}\\left[\\prod_{j\\leq i\\geq m}(1-\\kappa_{t})\\prod_{j\\leq i\\neq m}\\kappa_{t}\\right]\\left[\\prod_{j:z^{j}\\neq m}\\delta_{\\tilde{x}_{1}}\\left(z^{j}\\right)\\right]\\pi\\left(\\tilde{x}_{0},\\tilde{x}_{1}\\right)}}}\\\\ &{\\phantom{\\frac{p_{t}\\left(z\\left|x_{0},x_{1}\\right|\\right)\\pi\\left(z^{j}\\right)}{\\displaystyle=p\\left(x_{0},x_{1}\\right|z\\right)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "showing that the posterior is time-independent for dummy source distributions and convex paths. Consequently also the probability denoiser, ", "page_idx": 23}, {"type": "equation", "text": "$$\np_{1|t}(x^{i}|{z})=\\sum_{x_{0},x_{1}}\\delta_{x_{1}}(x^{i})\\frac{p_{t}(z|x_{0},x_{1})\\pi(x_{0},x_{1})}{p_{t}(z)}=\\sum_{x_{0},x_{1}}\\delta_{x_{1}}(x^{i})p(x_{0},x_{1}|z),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "is time-independent. ", "page_idx": 23}, {"type": "text", "text": "E.9 Continuous Flow Matching ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "For completeness we provide the formulas for denoiser ( $x$ -prediction) and noise-prediction ( $\\varepsilon-$ prediction) parameterizations of the generating velocity field $u\\,:\\,[0,1]\\,\\times\\,\\mathbb{R}^{N}\\,\\rightarrow\\,\\mathring{\\mathbb{R}}^{N}$ appearing in Table 1. ", "page_idx": 23}, {"type": "text", "text": "In Continuous Flow Matching one can chose several ways to define the probability paths (Lipman et al., 2022; Liu et al., 2022; Albergo and Vanden-Eijnden, 2022; Pooladian et al., 2023; Tong et al., 2023): ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{p_{t}(x)=\\displaystyle\\int p_{t}(x|x_{0},x_{1})\\pi(x_{0},x_{1})d x_{0}d x_{1}}}\\\\ {{\\displaystyle\\qquad=\\int p_{1|t}(x|x_{1})q(x_{1})d x_{1}}}\\\\ {{\\displaystyle\\qquad=\\int p_{0|t}(x|x_{0})p(x_{0})d x_{0}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Denoiser parameterization. The conditional generating velocity field $\\boldsymbol u_{t}(\\boldsymbol x|\\boldsymbol x_{1})$ for $p_{t}(x|x_{1})$ , i.e., satisfy the Continuity Equation 16, takes the form (Lipman et al., 2022) ", "page_idx": 23}, {"type": "equation", "text": "$$\nu_{t}(x|x_{1})=\\frac{\\dot{\\kappa}_{t}}{1-\\kappa_{t}}(x_{1}-x),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and the marginal generating velocity field is therefore given by the marginalization with the posterior $p_{t}(x_{1}|x)$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{c l c r}{{\\displaystyle{u_{t}(x)=\\int\\frac{\\dot{\\kappa}_{t}}{1-\\kappa_{t}}(x_{1}-x)\\frac{p_{1|t}(x|x_{1})q(x_{1})}{p_{t}(x)}d x_{1}}}}\\\\ {{\\displaystyle{=\\frac{\\dot{\\kappa}_{t}}{1-\\kappa_{t}}\\left[\\hat{x}_{1|t}(x)-x\\right],}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\hat{x}_{1\\mid t}(x)=\\int x_{1}\\frac{p_{1\\mid t}(x|x_{1})q(x_{1})}{p_{t}(x)}d x_{1}=\\mathbb{E}_{X_{1}\\sim p_{t}(\\cdot\\mid x)}X_{1}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "This shows the continuous Flow Matching denoiser parameterization of the generating velocity field in Table 1. ", "page_idx": 23}, {"type": "text", "text": "Noise-prediction parameterization. The conditional generating velocity field for $p_{t}(x|x_{0})$ takes the form ", "page_idx": 23}, {"type": "equation", "text": "$$\nu_{t}(x|x_{0})=\\frac{\\dot{\\kappa}_{t}}{\\kappa_{t}}(x-x_{0}),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and the marginal generating velocity field in this case is given by marginalization with the posterior $p_{t}(x_{0}|x)$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{u_{t}(x)=\\int\\frac{\\dot{\\kappa}_{t}}{\\kappa_{t}}(x-x_{0})\\frac{p_{0|t}(x|x_{0})p(x_{0})}{p_{t}(x)}d x_{0}}}\\\\ &{}&{=\\frac{\\dot{\\kappa}_{t}}{\\kappa_{t}}\\left[x-\\hat{x}_{0|t}(x)\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\hat{x}_{0\\mid t}(x)=\\int x_{0}\\frac{p_{0\\mid t}(x|x_{0})p(x_{0})}{p_{t}(x)}d x_{0}=\\mathbb{E}_{X_{0}\\sim p_{t}(\\cdot\\mid x)}X_{0}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "This shows the continuous Flow Matching noise-prediction parameterization of the generating velocity field in Table 1. ", "page_idx": 24}, {"type": "text", "text": "E.10 Scheduler change formula ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Proposition 8. Assume a conditional probability path as in equation 9, then for any two schedulers $\\kappa_{t},\\kappa_{t}^{\\prime}$ , and $\\hat{w}_{t}(x^{i}|z),\\hat{w}_{t}^{\\prime}(x^{i}|z)$ their corresponding posteriors as in equation 23, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\hat{w}_{t^{\\prime}}(x^{i}|z)=\\hat{w}_{t}^{\\prime}(x^{i}|z),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $t^{\\prime}=\\kappa_{\\kappa_{t}^{\\prime}}^{-1}$ , and $\\kappa^{-1}$ is the inverse of $\\kappa$ . ", "page_idx": 24}, {"type": "text", "text": "Proof (Proposition 8). For a conditional probability path as in equation 9, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{p_{t^{\\prime}}(x^{i}|x_{0},x_{1})=\\displaystyle\\prod_{i=1}^{N}p_{t^{\\prime}}(x^{i}|x_{0},x_{1})}}\\\\ &{}&{=\\displaystyle\\prod_{i=1}^{N}\\big[(1-\\kappa_{t^{\\prime}})\\delta_{x_{0}}(x^{i})+\\kappa_{t^{\\prime}}\\delta_{x_{1}}(x^{i})\\big]}\\\\ &{}&{=\\displaystyle\\prod_{i=1}^{N}\\big[(1-\\kappa_{t^{\\prime}}^{\\prime})\\delta_{x_{0}}(x^{i})+\\kappa_{t}^{\\prime}\\delta_{x_{1}}(x^{i})\\big]}\\\\ &{}&{=\\displaystyle\\prod_{i=1}^{N}p_{t^{\\prime}}(x^{i}|x_{0},x_{1})}\\\\ &{}&{=p_{t^{\\prime}}^{\\prime}(x^{i}|x_{0},x_{1}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where in the 3rd equality we used $\\kappa_{t^{\\prime}}\\,=\\,\\kappa_{t}^{\\prime}$ . Thus, also for the marginal probability path as in equation 7, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p_{t^{\\prime}}(x)=\\displaystyle\\sum_{x_{0},x_{1}\\in\\mathcal{D}}p_{t^{\\prime}}(x|x_{0},x_{1})\\pi(x_{0},x_{1})}\\\\ &{\\qquad=\\displaystyle\\sum_{x_{0},x_{1}\\in\\mathcal{D}}p_{t}^{\\prime}(x|x_{0},x_{1})\\pi(x_{0},x_{1})}\\\\ &{\\qquad=p_{t}^{\\prime}(x),}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where in the 2nd equality we used $p_{t^{\\prime}}(x|x_{0},x_{1})=p_{t}^{\\prime}(x|x_{0},x_{1})$ . Finally the change of scheduler for a posterior as defined in equation 23, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{w}_{t t^{\\prime}}(x^{i}|\\boldsymbol{z})=\\displaystyle\\sum_{x_{0},x_{1}\\in\\mathcal{D}}w(x^{i}|\\boldsymbol{x}_{0},x_{1})p_{t^{\\prime}}(x_{0},x_{1}|\\boldsymbol{z})}\\\\ &{\\qquad\\qquad=\\displaystyle\\sum_{x_{0},x_{1}\\in\\mathcal{D}}w(x^{i}|\\boldsymbol{x}_{0},x_{1})\\frac{p_{t^{\\prime}}(z|\\boldsymbol{x}_{0},x_{1})\\pi(x_{0},x_{1})}{p_{t^{\\prime}}(z)}}\\\\ &{\\qquad=\\displaystyle\\sum_{x_{0},x_{1}\\in\\mathcal{D}}w(x^{i}|\\boldsymbol{x}_{0},x_{1})\\frac{p_{t^{\\prime}}^{\\prime}(z|\\boldsymbol{x}_{0},x_{1})\\pi(x_{0},x_{1})}{p_{t^{\\prime}}^{\\prime}(z)}}\\\\ &{\\qquad=\\displaystyle\\sum_{x_{0},x_{1}\\in\\mathcal{D}}w(x^{i}|\\boldsymbol{x}_{0},x_{1})p_{t}^{\\prime}(x_{0},x_{1}|\\boldsymbol{z})}\\\\ &{\\qquad=\\displaystyle\\sum_{x_{0},x_{1}\\in\\mathcal{D}}w(x^{i}|\\boldsymbol{x}_{0},x_{1}|\\boldsymbol{p}_{t}^{\\prime}(x_{0},x_{1}|\\boldsymbol{z})}\\\\ &{\\qquad=\\hat{w}_{t}^{\\prime}(x^{i}|\\boldsymbol{z})}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where in the 3rd equality we used both $p_{t^{\\prime}}(z|x_{0},x_{1})=p_{t}^{\\prime}(z|x_{0},x_{1})$ and $p_{t^{\\prime}}(z)=p_{t}^{\\prime}(z)$ . ", "page_idx": 25}, {"type": "text", "text": "F Inference time ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "One potential benefti of non-autoregressive decoding is improved latency due to a significantly lower number of decoding steps. To demonstrate that, we measure the average latency of the proposed method compared with the autoregressive alternative using a single A100 GPU with $80\\,\\mathrm{GB}$ of RAM. We calculate the average latency time on the HumanEval benchmark using a batch size of 1. When considering 256 NFEs, the proposed method was found to be ${\\sim}2.5\\mathrm{x}$ faster than the autoregressive model (19.97 vs. 50.94 seconds on average per example). However, when considering 512 NFEs, both methods reach roughly the same latency. These results make sense as the number of tokens in most of the examples in HumanEval are below 512. Notice, that these results analyze latency and not model throughput. Due to the kv-caching mechanism following the autoregressive approach will result in significantly better throughput compared to the proposed approach Ziv et al. (2024). We leave the construction of a kv-cache mechanism to the proposed approach for future research. ", "page_idx": 25}, {"type": "text", "text": "G Experimental setup ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "G.1 Text ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Data. We use three splits of data. First is OpenWebText (Gokaslan and Cohen, 2019). Second is the same mix used in Llama-2 (Touvron et al., 2023), including textual and code data. For the code-focused models we use the same split used in CodeLlama (Roziere et al., 2023). For the small models, we use OpenWebText. For the big models we use the Llama-2 and CodeLlama mixes. ", "page_idx": 25}, {"type": "text", "text": "Models. We train two sizes of models: small (150M parameters) and large (1.7B parameters). For the small model we used a DiT transformer architecture (Peebles and Xie, 2022) with 12 layers, 12 attention heads, and hidden dimension of 768. We also used GPT2 tokenizer. The small models were trained on OpenWebText. For the large model, we use also used a DiT transformer architecture but with 48 layers, 24 attention heads, and hidden dimension of 1536 (Peebles and Xie, 2022). For these models we used a tiktoken tokenizer. The large models were trained on the Llama- $.2\\,\\mathrm{mix}$ and the CodeLlama mix. For both models we used ROPE (Su et al., 2024) embedding with $\\theta=10000$ . Models are trained with Adam optimizer with $\\beta_{1}=0.9$ and $\\beta_{2}=0.999$ . We use dropout rate of 0.1. Models are trained with a warm-up of 2500 steps, with a peak learning rate of 3e-4. We train the big models with batch size of 4096 for 1.3 million iterations and the big models with batch size of 512 for 400 thousand iterations. ", "page_idx": 25}, {"type": "text", "text": "Entropy metric. We report the entropy of tokens within a sequence, averaged over all generated sequences. This intuitively quantifies the diversity of tokens within a given sequence. It\u2019s important to note that when computing sequence entropy, tokens not present in the sequence are excluded from consideration. ", "page_idx": 25}, {"type": "text", "text": "Generative perplexity metric. The generative perplexity metric is the average likelihood of generated text evaluated with a second (usually stronger) model. We report the generative perplexity when averaged over 1000 samples. ", "page_idx": 26}, {"type": "text", "text": "Double precision sampling. Zheng et al. (2024) demonstrated that sampling from a highdimensional distribution with full precision can lead to a similar affect as sampling with temperature. We evaluate our model using a categorical sampler in double precision. Table 5 presents the results of baselines compared to our method. ", "page_idx": 26}, {"type": "table", "img_path": "GTDKo3Sv9p/tmp/e18856cbbce00d6c999e59080098a5ec10fb155604a9ccafc3bd588c7fd684a5.jpg", "table_caption": ["Table 5: Double precision sampling. Generative perplexity on unconditional text generation compared to prior work. All models are sampled without the use of temperature or corrector steps. "], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "G.2 Image ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Models. For all our experiments on CIFAR10 we use the U-Net architecture as in Dhariwal and Nichol (2021), with following three changes to make it fully discrete and time independent (as we used mask modeling): (i) We replace the first layer with an embedding table of size $257\\times96$ , and we stack the channel features such that the input to the U-Net is of shape $288\\times32\\times32$ . (ii) We enlarge the size of the final layer to output a tensor of shape $3\\times32\\times32\\times257$ . (iii) We remove the time dependency from architecture. The hyper-parameters of the architecture: channels 96 , depth 5, channels multiple [3,4,4], heads channels 64, attention resolution 16, dropout 0.4, which gives a total parameters count of 113M. We optimize the network using Adam optimizer with $\\beta_{1}=0.9$ and $\\beta_{2}=0.999$ , a learning rate of 1e-4. We trained with an effective batch size pf 512 for roughly 300K iterations. ", "page_idx": 26}, {"type": "text", "text": "Scheduler ablation. Figure 8 shows FID of our method with four different schedulers: Linear, Quadratic, Cubic, Cosine, both for training and evaluation. That is, for each scheduler we trained a model and evaluate FID with all four schedulers. We observe a high variance in FID between different schedulers, with the Cubic scheduler generally performing the best on both training and evaluation. ", "page_idx": 26}, {"type": "text", "text": "Comparison with baselines. In the following, we provide implementation details for producing Figure 3a, that compares our schedulers and sampling algorithm with those employed by previous works. ", "page_idx": 26}, {"type": "text", "text": "Cubic Scheduler (Ours). For the Cubic scheduler we set the corrector scheduler as above to, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\alpha_{t}=1+\\alpha t^{a}(1-t)^{b},\\quad\\beta_{t}=\\alpha_{t}-1,\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "and we search over the parameters $a,b\\;\\in\\;\\{0,0.25,0.5,1,2,2.5,3\\}$ , and $\\alpha\\,\\in\\,\\{6,8,10,12,14\\}$ . Additionally, we search over the temperature $\\in\\{1,0.9,0.8\\}$ . We find that $a=2$ , $b=0.25$ , $\\alpha=12$ give best FID. ", "page_idx": 26}, {"type": "text", "text": "Linear Scheduler (Campbell et al., 2024). For the linear scheduler we search over two additional hyper-parameters of the method: (i) For corrector scheduler as in equation 26, we set $\\alpha_{t}=1+t\\eta$ , $\\beta_{t}=\\alpha_{t}-1$ , where $\\eta$ is the stochasticity parameter as in Campbell et al. (2024), and search over $\\eta\\in\\{0,1,2,5,10,15\\}.$ . (ii) We search over temperature in $\\{1,0.9,0.8\\}$ . Finally, we find that the best FID is a achieved by $\\eta=10$ and temperature 0.9. ", "page_idx": 26}, {"type": "image", "img_path": "GTDKo3Sv9p/tmp/85cc60d61e7ecc2ff93544994b79d9a18162a3ad82093e7b33052384b79cff0b.jpg", "img_caption": ["Figure 8: Comparison of FID on CIFAR10 with four schedulers: Linear, Quadratic, Cubic, Cosine, for both train and evaluation. Corrector sampling is not used in this experiment (i.e., $\\alpha_{t}\\,=\\,0$ in equation 26), and temperature is set to 1. "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "MaskGIT (Chang et al., 2022). For the MaskGIT we train and sample with the Cosine scheduler $\\begin{array}{r}{\\kappa(t)=1-\\cos\\left(\\frac{\\pi}{2}t\\right)}\\end{array}$ which is reported to achieve best results by Chang et al. (2022). For sampling we adjust the code from the re-implementation of Besnier and Chen (2023). In addition, we also search over the temperature in $\\{1,0.9,0.8,0.7,0.6,0.5\\}$ , and we find the best FID is achieved by temperature 1. ", "page_idx": 27}, {"type": "text", "text": "H Code generation - qualitative examples ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "H.1 Success cases from typing import List ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "def below_zero(operations: List[int]) $->$ bool: \"\"\" You're given a list of deposit and withdrawal operations on a bank account that starts with zero balance. Your task is to detect if at any point the balance of account fallls below zero, and at that point function should return True. Otherwise it should return False. >>> below_zero([1, 2, 3]) False >>> below_zero([1, 2, -4, 5]) True \"\"\" ", "page_idx": 28}, {"type": "text", "text": "balance $=~0$   \nfor operation in operations: if balance $^+$ operation $<~0$ : return True balance $=$ balance $^+$ operation   \nreturn False ", "page_idx": 28}, {"type": "text", "text": "def encode_shift(s: str):", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "returns encoded string by shifting every character by 5 in the alphabet. \"\"\" return \"\".join([chr(((ord(ch) $^+$ 5 - ord(\"a\")) % 26) $^+$ ord(\"a\")) for ch in s]) ", "page_idx": 28}, {"type": "text", "text": "def decode_shift(s: str):", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "takes as input string encoded with encode_shift function.   \nReturns decoded string. ", "page_idx": 28}, {"type": "text", "text": "def filter_integers(values: List[Any]) $->$ List[int]: \"\"\" Filter given list of any python values only for integers >>> filter_integers(['a', 3.14, 5]) [5] >>> filter_integers([1, 2, 3, 'abc', {}, []]) [1, 2, 3] \"\"\" ", "page_idx": 29}, {"type": "text", "text": "return [value for value in values if isinstance(value, int)] def common(l1: list, l2: list): \"\"\"Return sorted unique common elements for two lists. >>> common([1, 4, 3, 34, 653, 2, 5], [5, 7, 1, 5, 9, 653, 121]) [1, 5, 653] >>> common([5, 3, 2, 8], [3, 2]) [2, 3] ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "return sorted(set(l1) & set(l2)) ", "page_idx": 29}, {"type": "text", "text": "def fibfib(n: int): \"\"\"The FibFib number sequence is a sequence similar to the Fibbonacci sequnece that's defined as follows: fibfib(0) $\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\quad0$ fibfib(1) $\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\quad0$ fibfib(2) $\\mathbf{\\chi}=\\mathbf{\\chi}\\cdot\\mathbf{\\chi}_{\\perp}$ fibfib(n) $==$ fibfib(n-1) + fibfib(n-2) $^+$ fibfib(n-3). Please write a function to efficiently compute the n-th element of the fibfib number sequence. >>> fibfib(1) $o$ >>> fibfib(5) 4 >>> fibfib(8) 24 if $\\textbf{n}==\\textbf{0}$ : return 0 elif $\\textbf{n}==\\textbf{1}$ : return 0 elif $\\textbf{n}==~2$ : return 1 else: return fibfib(n-1) $^+$ fibfib(n-2) $^+$ fibfib(n-3) ", "page_idx": 29}, {"type": "text", "text": "H.2 Failure cases ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "def solve(s): \"\"\"You are given a string s. if s[i] is a letter, reverse its case from lower to upper or vise versa, otherwise keep it as it is. If the string contains no letters, reverse the string. The function should return the resulted string. Examples solve $(\\prime\\prime1234\\prime\\prime)=\\;\\;\\prime\\prime4321\\prime$ solve $(^{\\prime\\prime}a b^{\\prime\\prime})\\;\\;=\\;\\;^{\\prime\\prime}A B^{\\prime\\prime}$ $\\begin{array}{r l r}{s o\\,\\mathcal{\\ell}\\,v e\\left({\\,^{\\prime\\prime}\\#{a}\\,\\mathcal{Q}\\,C^{\\prime\\prime}}\\right)}&{=}&{{^{\\prime\\prime}\\#A\\,\\mathcal{Q}\\,c^{\\prime\\prime}}}\\end{array}$ \"\"\" if s is None: return \"\" if ${\\sf{l e n}}({\\bf s})\\;\\;=\\;0$ : return s # result string res $=$ str() $\\#$ to lower case s = s.lower() for i in range(len(s)): ch $=$ s[i] if ch.islower(): res.append(ch) elif ch.isupper(): res.append(ch.lower()) else: res.append(ch) return res ", "page_idx": 30}, {"type": "text", "text": "from typing import List, Tuple ", "page_idx": 30}, {"type": "text", "text": "def rolling_max(numbers: List[int]) $->$ List[int]: \"\"\" From a given list of integers, generate a list of rolling maximum element found until given moment in the sequence. >>> rolling_max([1, 2, 3, 2, 3, 4, 2]) $[\\,\\!\\,\\!\\,1\\,\\!\\,\\!\\,]$ , 2, 3, 3, 3, 4, 4] max_number $=~0$ result $\\mathrm{~\\ensuremath~{~\\mu~=~}~}\\left[\\boldsymbol{\\mathrm{1}}\\right]$ for number in numbers: max_number $=$ max(result) if number $>$ max_number: result.append(number) else: result $=$ [max_number] return result ", "page_idx": 30}, {"type": "text", "text": "def truncate_number(number: float) $->$ float: \"\"\" Given a positive floating point number, it can be decomposed into and integer part (largest integer smaller than given number) and decimals (leftover part always smaller than 1). Return the decimal part of the number. >>> truncate_number(3.5) 0.5 \"\"\" ", "page_idx": 31}, {"type": "text", "text": "return math.floor(number) ", "page_idx": 31}, {"type": "text", "text": "from typing import List, Tuple ", "page_idx": 31}, {"type": "text", "text": "def sum_product(numbers: List[int]) $->$ Tuple[int, int]: \"\"\" For a given list of integers, return a tuple consisting of a sum and a product of all the integers in a list. Empty sum should be equal to 0 and empty product should be equal to 1. >>> sum_product([]) (0, 1) >>> sum_product([1, 2, 3, 4]) (10, 24) \"\"\" if len(numbers) $\\scriptstyle\\qquad\\qquad0$ or numbers[0] $\\scriptstyle\\mathbf{\\omega}=\\mathbf{\\omega}0$ : return 0, 1 sum $=~0$ product $\\mathit{\\Theta}=\\mathit{\\Theta}_{1}$ for num in numbers: if abs $(\\mathrm{{num})\\quad!=~0}$ : product $=$ product $^*$ num sum $+=$ num return sum, product ", "page_idx": 31}, {"type": "text", "text": "from typing import List ", "page_idx": 31}, {"type": "text", "text": "def factorize(n: int) $->$ List[int]: \"\"\" Return list of prime factors of given integer in the order from smallest to largest. Each of the factors should be listed number of times corresponding to how many times it appeares in factorization. Input number should be equal to the product of all factors. >>> factorize(8) [2, 2, 2] >>> factorize(25) [5, 5] >>> factorize(70) [2, 5, 7] \"\"\" factors $\\mathrm{~\\ensuremath~{~\\mu~=~}~}\\left[\\boldsymbol{\\mathrm{1}}\\right]$ factor $\\mathit{\\Theta}=\\mathit{\\Theta}_{1}$ for i in range(2, n): factor $*=\\textbf{i}$ while n $\\%$ factor $\\scriptstyle\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\quad0$ : factors.append(factor) factor $+=~1$ return factors ", "page_idx": 31}, {"type": "text", "text": "from typing import List, Optional ", "page_idx": 32}, {"type": "text", "text": "def longest(strings: List[str]) $->$ Optional[str]: \"\"\" Out of list of strings, return the longest one. Return the first one in case of multiple strings of the same length. Return None in case the input list is empty. >>> longest([]) ", "page_idx": 32}, {"type": "text", "text": ">>> longest(['a', 'b', 'c'])   \n'a'   \n>>> longest(['a', 'bb', 'ccc'])   \n'ccc'   \n\"\"\"   \nfor string in strings: if len(string) $>$ len(long): long $=$ string   \nreturn long ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "H.3 Infilling ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "def bubbleSort(arr): n = len(arr) # optimize code, so if the array is already sorted, it doesn't need # to go through the entire process # Traverse through all array elements for i in range(n-1): # range(n) also work but outer loop will # repeat one time more than needed. # Last i elements are already in place swapped $=$ False for j in range(0, n-i-1): # traverse the array from 0 to n-i-1 # Swap if the element found is greater # than the next element if a $\\Im\\,[\\mathrm{j}]\\ >\\ \\mathsf{a r r}\\,[\\mathrm{j}\\ +\\ 1\\$ ]: swapped $=$ True arr[j], arr[j $^+$ 1] $=$ arr[j + 1], arr[j] if not swapped: # if we haven't needed to make a single swap, we # can just exit the main loop. return ", "page_idx": 33}, {"type": "text", "text": "# Function to perform Breadth First Search on a graph # represented using adjacency list ", "page_idx": 33}, {"type": "text", "text": "def bfs(adjList, source , visited): # Create a queue for BFS $\\textsf{q}=$ deque() # Mark the current node as visited and enqueue it visited[ source ] $=$ True q.append( source ) # Iterate over the queue while q: # Dequeue a vertex from queue and print it currentNode $=$ q.popleft() print( currentNode , end $\\equiv^{11}$ \") # Get all adjacent vertices of the dequeued vertex # If an adjacent has not been visited, then mark it visited and enqueue it for adjacent in adjList[ currentNode ]: if not visited[ adjacent ]: visited[ adjacent ] $=$ True q.append( adjacent ) ", "page_idx": 33}, {"type": "text", "text": "# Returns index of x in arr if present, else -1 def binary_search(arr, low , high , x): ", "page_idx": 34}, {"type": "text", "text": "$\\#$ Check base case   \nif high $>=$ low : mid $=$ ( high $^+$ low ) // 2 # If element is present at the middle itself if arr[ mid $]\\;\\;=\\;\\;\\mathbf{x}$ : return mid # If element is smaller than mid, then it can only # be present in left subarray elif arr[ mid ] > x: return binary_search(arr, low , mid - 1, x) # Else the element can only be present in right subarray else: return binary_search(arr, mid $+\\_1$ , high , x)   \nelse: # Element is not present in the array return -1 ", "page_idx": 34}, {"type": "text", "text": "# Python program for Dijkstra's single # source shortest path algorithm. The program is # for adjacency matrix representation of the graph class Graph(): ", "page_idx": 35}, {"type": "text", "text": "def __init__(self, vertices): self. $\\texttt{V}=$ vertices self.graph $=$ [[0 for column in range(vertices)] for row in range(vertices)] ", "page_idx": 35}, {"type": "text", "text": "def printSolution(self, dist): print(\"Vertex Distance from Source\") for node in range(self.V): print(node, \", dist[node]) ", "page_idx": 35}, {"type": "text", "text": "# A utility function to find the vertex with # minimum distance value, from the set of vertices # not yet included in shortest path tree ", "page_idx": 35}, {"type": "text", "text": "def minDistance(self, dist, sptSet): ", "page_idx": 35}, {"type": "text", "text": "# Initialize minimum distance for next node   \nmin $=$ 1e7   \n# Search not nearest vertex not in the   \n# shortest path tree   \nfor v in range(self.V): if dist[v] $<$ min and sptSet[v] $==$ False: min $=$ dist[v] min_index $=$ v   \nreturn min_index ", "page_idx": 35}, {"type": "text", "text": "# Function that implements Dijkstra's single source # shortest path algorithm for a graph represented # using adjacency matrix representation ", "page_idx": 35}, {"type": "text", "text": "def dijkstra(self, src ): ", "page_idx": 35}, {"type": "text", "text": "dist $=$ [1e7] $^*$ self.V   \ndist [src] $=~0$   \nprocessed $=$ [False] $^*$ self.V   \nfor cout in range(self.V): # Pick the minimum distance vertex from # the set of vertices not yet processed. # u is always equal to src in first iteration uv $=$ self.minDistance(dist, processed ) # Put the minimum distance vertex in the # shortest path tree processed [ uv ] $=$ True # Update distance value of the adjacent vertices # of the picked vertex only if the current # distance is greater than new distance and # the vertex in not in the shortest path tree for v in range(self.V): if (self.graph[ uv $\\textsf{I}[\\textsf{v}]~>~0$ and processed [ uv ] $==$ False and dist [ uv $]\\ >$ dist [ cout ] $^+$ self.graph[ uv ][ v ]): dist [ uv ] $=$ dist [ cout ] $^+$ self.graph[ uv ][ v ] ", "page_idx": 35}, {"type": "text", "text": "I Textual generations ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "We present below example generations for the proposed method together with several baseline methods. We provide both conditional and unconditional generations. For the conditional generations, we mark the prompt in gray. ", "page_idx": 36}, {"type": "text", "text": "I.1 Conditional generation ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "The United States on Wednesday asked the UN Security Council to slap an oil embargo on North Korea and freeze the assets of leader Kim Jong-un, in response to Pyongyang\u2019s ", "page_idx": 36}, {"type": "text", "text": "response to the revelations it had restarted its nuclear work in March. \u201cWe will continue working to use maximum international pressure on North Korea to agree to the suspension of its nuclear program and reinstate sanctions,\u201d said John Bolton, who served as National Security Advisor and Secretary of State under US President Bill Clinton. \u201cHere is North Korea\u2019s response to our sanctions,\u201d Bolton wrote in a letter to House Minority Leader Nancy Pelosi. \u201cWe want you to know that the international community is seriously monitoring North Korea at this time. North Korea is still complying with our requests from the past few days,\u201d Bolton said on Monday. \u201cWe have been working through the United Nations to provide the information that they gave us.\u201d Asked to whether any international pressure will be put in place for North Korea to give up its nuclear weapons, Bolton said the United States can use maximum pressure to get North Korea to abandon its nuclear weapons if it wants. \u201cWe\u2019ve been working to use maximum pressure on North Korea, through the Security Council, and we will continue to do so,\u201d said White House Deputy Press Secretary Sarah Huckabee Sanders in Washington. \u201cWe\u2019re committed to taking any steps necessary to help North Korea pursue its only option for peace, including in this period,\u201d she added. The United States did not plan to produce any more oil at this time last year and had not planned to do so this year. \u201cWe believe that the North Korea approach is misguided in moving forward with its nuclear program to endanger peace and security in its homeland and neighbors in Asia,\u201d said Bolton, adding that the US supplies its nuclear weapons. \u201cWe don\u2019t want them to sell their nuclear weapons to other nations,\u201d he said. Bolton said the US would look for pressure on North Korea, which has been known to use nuclear weapons, as leverage to negotiations with the US. \u201cI will reiterate what I have said before. So, the US has to put pressure on North Korea. But who else is going to hold the cards? Somebody else has to hold the cards,\u201d Mr Bolton said. Bolton described what the United States is prepared to do to get North Korea to agree to give up its weapons and asks for sanctions. \u201cAs far as I know, we have to use the pressure the reason for putting sanctions on North Korea,\u201d he said, adding that the US does not plan to ask the UN Security Council for sanctions alone. ", "page_idx": 36}, {"type": "text", "text": "The defender is available for the Maribor first leg but his club believed he should be suspended. SNS Group Celtic made an administrative blunder which saw Efe Ambrose left behind ", "page_idx": 36}, {"type": "text", "text": "in the midfield in the Maribor department and has given him a potential three-match ban today. Although Efe Ambrose will be suspended next Friday, according to reports in Scottish media, the Celtic defender will still be fit for the Champions League first leg at Celtic Stadium in the middle of August. However, the Celtic club wrongly thought that Efe should only receive a three-match ban because he is available for the first leg. Although Efe Ambrose may receive a three-match ban next Friday, Efe Ambrose was part of the Celtic squad for the last match against Liverpool. However, says SNS Group Celtic he was making a tactical error and was left behind in midfield. It is understood that Efe Ambrose did not make the final squad and only played 11 games for the club this season. Efe Ambrose made his professional debut for Celtic in 2008 but spent nine months looking for a new club to return to. With a career-high 72 Celtic appearances, Efe is among Celtic\u2019s most capped players ever. ", "page_idx": 36}, {"type": "text", "text": "Carl Jara aka Grain Damaged is an award-winning, professional sand sculptor from Cleveland, Ohio. Jara says he has known since high-school that he wanted to be an artist. After studying ", "page_idx": 37}, {"type": "text", "text": "English and Art History at the Northeastern University, Jara says one semester he started carving a custom sculpture into sand molds, but didn\u2019t know how to do it. With the help of an instructor, he found out and learned how to use rubber molds to make art. Later, he made the decision to learn how to use sand and sculpt himself. In addition to how he makes his own sculptures, Jara says he does special events for comics companies such as Stan Lee and also for institutions like local community colleges and universities. In November of this year, he won the WWHS, The Very Art Of The Very Things Cleveland competition. Afterward, he will continue carving for clients in the comics industry and looks forward to sand sculpting in Cleveland. The Artist is professional sculptor who has been making art, for over 25 years, in various shapes and sizes.The artist says art is all about relationships and the best way to go into the heart is to create art. The artist has taught in various high schools in the Cleveland area and has taught a full time Honors Studio for High School students in grades 6, 7, and 8 time for over 20 years. Since Art is a personal form of artistic expression, he works individually in a way that allows the student that his work engages their imagination and presents ideas in ways that inform and challenge their own paths. Miguel Romano is a professional artist who worked in 3D modeling and animation in the areas of web design and production. The artist currently works as a digital artist in the ad and mass communication industries. In coming to Concrete Cleveland, he is excited to apply the 3D development and production skills he have to his work. The artist has a BFA in sculpture from Ohio University, along with an MFA in sculpture. We look forward to seeing his work very soon! Designed and installed by Concrete This Week. He is a guy originally from Cleveland, Ohio where he pursued a career as a nurse. He then moved to the Atlanta, GA area where he returned to school with a BSN and a BS in nursing and is a licensed nurse. He is a proud sorority brother and still has extra-curricular, as well as taking music lessons and the occasional play. He is a lovely asset at Concrete Cleveland and looks forward to seeing concrete ", "page_idx": 37}, {"type": "text", "text": "I.2 Unconditional generation ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Here\u2019s how that strategy works for your job: ", "page_idx": 38}, {"type": "text", "text": "1) You now plan upon what you accomplish to fulfill your goals. ", "page_idx": 38}, {"type": "text", "text": "Management cannot plan what happens to you. This may not be your ultimate personal decision, but it\u2019s perfectly fine to look at it. You just need to make sure you want to achieve this. ", "page_idx": 38}, {"type": "text", "text": "Now, because you\u2019ve worked at goal, you don\u2019t have to talk about your status tomorrow, after all, you have to do your job and take care of yourself. ", "page_idx": 38}, {"type": "text", "text": "Next steps, there may be some work to do. There is a company down the road you right \u2013 literally millions of things that would have to be done. But of course it would have a different outlook. If you\u2019re going to do something, the customer might not be able to tell you. ", "page_idx": 38}, {"type": "text", "text": "2) Between those two steps are the plan in step so that your actions will be executed. ", "page_idx": 38}, {"type": "text", "text": "Then you have taken other steps (usually a few less important changes), like delivery. If you already know what that means, and you\u2019re having to stay up and take action you can make sure you don\u2019t have to point out in the moment to plan the action. ", "page_idx": 38}, {"type": "text", "text": "With business goals, it is not easy to pick up what appears best for us. We have to see what really is. What we do. We can\u2019t make a plan on the floor and come back up with exactly what you\u2019re doing. If you want to work every step, then you need to differentiate from the action and what the next step represents. ", "page_idx": 38}, {"type": "text", "text": "Eventually, you\u2019ll be less motivated to focus on this step and the previous one. Unfortunately if you don\u2019t change your main thing, you may be able to lose your motivation to work on \u201cpivot.\u201d Unless that\u2019s possible, and if you don\u2019t change something, then the task may not be at the right time. Instead of doing something, it is just in advance of your ultimate goal. ", "page_idx": 38}, {"type": "text", "text": "Although you might make a mistake with every single day to day plan, it still is a great opportunity to correct your mistake, become new and commit to working extra hours and meeting your goals promptly. ", "page_idx": 38}, {"type": "text", "text": "The truth is, everything goes right for you no matter how quick a decision you become. The customer will never allow you to make the worst decisions. Otherwise, you make the very first decision. ", "page_idx": 38}, {"type": "text", "text": "3) Take timing as part of action. If you don\u2019t feel like you can keep it, a plan without help of timing stops you from doing. When it\u2019s like your plan in action can lead to something such as this: Now that you know what to do. For example, you might live in a place in the building that serves every customer, has 3 employees per team, and 3 clients on one. You will get things done the next day. Change your performance is the first step towards greater success, for example. Your team, at this point in the Customer department, will know how the customer deal with a single employee, the level chain, and more. Make sure you take action now that you change it. As a company, it won\u2019t be hard but you will have lots of work to do when you change.   \n4) Make sure it\u2019s your night. ", "page_idx": 38}, {"type": "text", "text": "Watch the humour but also the humanity behind the work we\u2019re doing. The truth is something very tragic and delicate in the middle of a very fractured world. It\u2019s the one thing that makes me proud. I feel like a singular individual has had to come together with this story. There\u2019s a lot of people who I\u2019ve worked with for the very beginning, because I have got people, you\u2019ve got people who have just had these eyes on this story, and this sense of what we are, that run through our final movie. ", "page_idx": 38}, {"type": "text", "text": "It\u2019s the very beginning we\u2019re at. The very beginning, we\u2019re not there, we will get there but we won\u2019t need. This is a story and these amazing actors, these fantastic violence, violence, that was just are elements and a complex world of confilct. When something like that is set to build this narrative and you\u2019re directing the world of these characters based around their individual needs it\u2019s very, extremely confusing, very heartbreaking \u2014 it\u2019s really quite intense\u2014this was all built within it, and what it is, it\u2019s a 35-year old period that was slavery and still was very strong, these guys were operating to the edge and going to the point where we ended up setting up a big narrative, OK, that\u2019s good, it\u2019s okay, in some ways heroism is a noble imperative that we are fighting against, and recognize maturity as the mercurial nature and these are all human and we\u2019ve got to clean it up so that that stuff is there and we\u2019ve got to restore it. And the project we\u2019re looking at here is our common goal is that anything can be done to make that happen and everybody can do whatever their want to do and do it as they please. That\u2019s the spirit of it. That\u2019s the movie I\u2019ve made with Steven Wright in writing. ", "page_idx": 38}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 39}, {"type": "text", "text": "Answer: [No] As each experiment is highly resource intensive we did not report error bars.   \nHowever, we did report results on a wide range of tasks and setups. ", "page_idx": 39}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 39}, {"type": "text", "text": "Answer: [No] ", "page_idx": 39}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] See Discussion section. ", "page_idx": 39}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}]