[{"figure_path": "o4coDIby7e/figures/figures_1_1.jpg", "caption": "Figure 1: Computing maximum entropy goal-directedness (MEG).", "description": "This figure illustrates the four steps involved in computing the maximum entropy goal-directedness (MEG). (a) shows a system of interest. (b) models the system as a causal Bayesian network. (c) posits decision and utility variables. (d) measures maximum predictive accuracy to quantify goal-directedness.  The example depicts a mouse choosing between two directions, one with cheese and one without.  The MEG calculation assesses how well the mouse's choices are predicted by assuming it's acting to maximize utility (obtaining cheese).", "section": "1 Introduction"}, {"figure_path": "o4coDIby7e/figures/figures_3_1.jpg", "caption": "Figure 2: Sequential multi-decision mouse example.", "description": "This figure illustrates an example of a sequential multi-decision problem. (a) shows the gridworld environment. The mouse starts in the center and must decide whether to move left or right at each time step. (b) depicts this as a causal Bayesian network, with nodes representing the cheese's location, the mouse's observations and decision, and whether the mouse obtained the cheese. (c) shows this as a causal influence diagram, with utility nodes added to represent the utility of each outcome.", "section": "2 Background"}, {"figure_path": "o4coDIby7e/figures/figures_8_1.jpg", "caption": "Figure 3: (a) The CliffWorld environment. (b) MEG of \u03b5-greedy policies for varying \u03b5. MEG decreases as the policy gets less optimal. (c) MEG for optimal policies for various reward functions. Known-utility MEG decreases as the goal gets easier to satisfy, but unknown-utility MEG stays higher because the optimal policies also do well with respect to a narrower goal.", "description": "This figure presents results from two experiments conducted using the CliffWorld environment.  The first experiment (b) shows how the maximum entropy goal-directedness (MEG) changes for \u03b5-greedy policies with varying \u03b5 values (exploration-exploitation trade-off). As expected, MEG decreases as the policy becomes less optimal (more random). The second experiment (c) examines how MEG varies for optimal policies under different reward functions, each representing a task of varying difficulty. It shows that known-utility MEG (MEG with respect to a known reward function) decreases as the task gets easier (the goal region becomes larger), while the unknown-utility MEG (MEG considering a family of utility functions) remains relatively higher. This is because optimal policies that perform well on the easier task also perform well on more specific, narrower utility functions, thereby maintaining a higher MEG in the unknown utility case.", "section": "Experimental Evaluation"}, {"figure_path": "o4coDIby7e/figures/figures_13_1.jpg", "caption": "Figure 4: Example 1 can be equally well represented with a CBN (a) or mechanised CBN (b), but Kenton et al. [2023]'s algorithm only identifies an agent in (b). (c) shows the resulting mechanised CID. In contrast, MEG is unchanged between (b) and (c). Note also that the causal discovery algorithm identifies T as a utility variable, where where MEG adds a new utility child to T.", "description": "This figure compares three different graphical model representations of Example 1 from the paper.  Panel (a) shows a standard causal Bayesian network (CBN). Panel (b) demonstrates a 'mechanised' CBN, which adds additional variables representing mechanisms.  Kenton et al.'s method would only identify an agent in the mechanised version.  Panel (c) presents a 'mechanised' causal influence diagram (CID) that includes utility variables.  The paper highlights that MEG (Maximum Entropy Goal-Directedness), unlike Kenton et al.'s method, remains consistent across these different representations.", "section": "A Comparison to Discovering Agents"}, {"figure_path": "o4coDIby7e/figures/figures_17_1.jpg", "caption": "Figure 3: (a) The CliffWorld environment. (b) MEG of \u03b5-greedy policies for varying \u03b5. MEG decreases as the policy gets less optimal. (c) MEG for optimal policies for various reward functions. Known-utility MEG decreases as the goal gets easier to satisfy, but unknown-utility MEG stays higher because the optimal policies also do well with respect to a narrower goal.", "description": "This figure shows three subplots related to the CliffWorld environment experiments. (a) shows the environment setup. (b) shows how MEG (maximum entropy goal-directedness) values decrease as the policies become less optimal (using epsilon-greedy policies with varying epsilon values). (c) demonstrates the relationship between MEG and task difficulty (different reward functions with varying lengths of goal regions). This subplot highlights that known-utility MEG decreases as the task becomes easier, but unknown-utility MEG remains higher because optimal policies perform well even with narrower goal definitions.", "section": "Experimental Evaluation"}]