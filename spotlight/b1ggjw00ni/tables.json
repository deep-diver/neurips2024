[{"figure_path": "b1ggjW00NI/tables/tables_5_1.jpg", "caption": "Table 1: Training results on action recognition. RLT significantly reduces fine-tuning time with comparable performance to the baseline on both Kinetics-400 and Something-Something-v2.", "description": "This table presents the results of fine-tuning various video transformers on two action recognition datasets: Kinetics-400 and Something-Something-v2.  The models compared are the baseline ViT, ToMe (Token Merging), a random masking baseline, and the proposed RLT method.  For each model and dataset, the table shows the achieved accuracy and the fine-tuning time on 8 GPUs.  The speedup factor is calculated relative to the baseline ViT. The results demonstrate that RLT achieves comparable accuracy to the baseline while significantly reducing the fine-tuning time.", "section": "4.1 Training"}, {"figure_path": "b1ggjW00NI/tables/tables_6_1.jpg", "caption": "Table 2: Inference-only results on action recognition. With batch size 1, RLT with \u03c4 = 0.1 consistently achieves the closest performance to the baseline, comparable or faster than Token Merging or random masking. We omit ViT-H results on Something-Something-v2 due to lack of existing pre-trained checkpoints.", "description": "This table presents the inference-time results of different video transformer models on Kinetics-400 and Something-Something-v2 datasets.  It compares the top-1 accuracy, GFLOPs (floating-point operations), clips per second (throughput), and speedup relative to the baseline ViT model for standard tokenization, Token Merging, STA, Random Masking, and the proposed RLT method.  The results show that RLT achieves a good balance between accuracy and speed, outperforming other methods in many cases.", "section": "4.2 Inference-Time Results"}, {"figure_path": "b1ggjW00NI/tables/tables_6_2.jpg", "caption": "Table 1: Training results on action recognition. RLT significantly reduces fine-tuning time with comparable performance to the baseline on both Kinetics-400 and Something-Something-v2.", "description": "This table presents the training results of different models on two action recognition datasets: Kinetics-400 and Something-Something-v2.  It compares the standard ViT model against variations using Run-Length Tokenization (RLT), with and without random masking. The table shows the accuracy achieved and the fine-tuning time for each model on both datasets.  RLT demonstrates significant time reduction with comparable accuracy.", "section": "4.1 Training"}, {"figure_path": "b1ggjW00NI/tables/tables_7_1.jpg", "caption": "Table 4: Per-Dataset Token Reduction. RLT reduces tokens significantly across datasets, with higher reductions on higher FPS. On long-video datasets like COIN and Breakfast with mostly static content, RLT achieves almost 80% reduction, demonstrating its promise for scaling training.", "description": "This table shows the number of tokens before and after applying the Run-Length Tokenization (RLT) method on various datasets at different frame rates (FPS).  The percentage reduction in tokens achieved by RLT is also presented, highlighting its effectiveness in reducing computational cost, especially for longer videos with many static frames. Notice that the token reduction is more significant at higher FPS and on datasets with significant amounts of static content.", "section": "4.4 Longer Videos and Higher FPS"}, {"figure_path": "b1ggjW00NI/tables/tables_7_2.jpg", "caption": "Table 1: Training results on action recognition. RLT significantly reduces fine-tuning time with comparable performance to the baseline on both Kinetics-400 and Something-Something-v2.", "description": "This table presents the results of training video transformers on two action recognition datasets: Kinetics-400 and Something-Something-v2.  It compares the performance and training time of four different models: the baseline ViT-B and ViT-L, and versions of these models that incorporate the proposed Run-Length Tokenization (RLT) method. The table shows that RLT significantly reduces the training time (wall-clock time) without a significant drop in accuracy, demonstrating the method's effectiveness in accelerating the training process of video transformers.", "section": "4.1 Training"}]