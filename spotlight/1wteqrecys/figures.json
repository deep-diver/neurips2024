[{"figure_path": "1WtEqReCyS/figures/figures_1_1.jpg", "caption": "Figure 1: Multilingual image-text data adds diversity to the English data distribution in various, significant ways (a) We show some examples of culturally salient concepts that would not exist in \"high-quality\" English data (as determined by CLIP score), such as \"bamboo steamer\", \"kiji\" (the national bird of Japan) and yal\u0131 (a traditional architecture style for Turkish waterside houses) (b) Even for a common everyday object (\"stove\"), non-English and English images portray very different visual representations.", "description": "This figure demonstrates how multilingual data enriches datasets by showcasing culturally specific concepts absent in English-centric datasets (a).  It also highlights the visual differences in representing even common objects like \"stoves\" across different languages (b), illustrating the inherent value of multilingual data in enriching multimodal models.", "section": "1 Introduction"}, {"figure_path": "1WtEqReCyS/figures/figures_5_1.jpg", "caption": "Figure 2: Filtering with translated captions allows substantially more (translated) non-English samples to be included in the final training set. While English data only makes up about one-third of the raw web crawl, it dominates the top 20% of the pool, selected based on DFN score between image and raw caption. With translation, English-translated non-English captions now make up the majority of the \"high-quality\" data and thus are more likely to be selected for training.", "description": "This figure is a bar chart comparing the number of English and non-English data points before and after translation and filtering. Before filtering (using the Data Filtering Network - DFN), English data points constitute only one-third of the data.  After filtering with raw captions, English data points make up the majority of the top 20% (highest quality) samples. However, after translating non-English captions to English and then re-filtering, the majority of the top 20% samples are (translated) non-English data points. This shows how the translation process and the filtering algorithm can drastically increase the amount of non-English data selected, which indicates the potential of non-English data to increase dataset diversity and model performance.", "section": "4 Impacts of using (translated) multilingual captions on standard vision tasks"}, {"figure_path": "1WtEqReCyS/figures/figures_6_1.jpg", "caption": "Figure 3: With the same degree of filtering, training with (image, translated caption) pairs improves performance on 28 out of 38 tasks compared to training with (image, raw caption) pairs, including on ImageNet distribution shifts, retrieval, and tasks with geographically diverse inputs. We compare performance on each task of the DataComp benchmark between training with raw captions and training with translated captions. Both datasets have been filtered with image-text cosine similarities output by the public DFN [13] to select the top 30% examples. We find that using translated captions leads to 1.5 percentage points improvement on average across 38 tasks. We highlight the performance changes on ImageNet distribution shifts (red), retrieval (blue) and fairness-related tasks (dark yellow).", "description": "This figure shows the performance difference between using raw captions and translated captions for training on 38 different tasks from the DataComp benchmark. The results indicate that using translated captions leads to improved performance in most of the tasks, especially those involving ImageNet distribution shifts, retrieval, and fairness-related tasks. The improvement is highlighted in different colors for better visualization.", "section": "4 Impacts of using (translated) multilingual captions on standard vision tasks"}, {"figure_path": "1WtEqReCyS/figures/figures_7_1.jpg", "caption": "Figure 4: On GeoDE, using filtered translated captions leads to improvements across all regions compared to using filtered raw captions, with Africa observing the biggest gain. We break down the GeoDE performance by region and compare training on top 30% translated captions to training on top 30% raw captions. On average, classification accuracy improves by 4.2%, and the improvement applies to all regions in the dataset, especially Africa where the accuracy gain is the biggest at 5.5%.", "description": "This figure shows the performance improvement on GeoDE (a geographically diverse benchmark) when using translated multilingual captions for training, compared to using only English captions.  The bar chart displays a significant increase in accuracy across all geographical regions, most notably in Africa. This visually represents the benefit of incorporating diverse cultural and linguistic data in model training, demonstrating improved performance on tasks beyond those focused on English-speaking regions.", "section": "4 Impacts of using (translated) multilingual captions on standard vision tasks"}, {"figure_path": "1WtEqReCyS/figures/figures_13_1.jpg", "caption": "Figure 1: Multilingual image-text data adds diversity to the English data distribution in various, significant ways (a) We show some examples of culturally salient concepts that would not exist in \"high-quality\" English data (as determined by CLIP score), such as \"bamboo steamer\", \"kiji\" (the national bird of Japan) and yal\u0131 (a traditional architecture style for Turkish waterside houses) (b) Even for a common everyday object (\"stove\"), non-English and English images portray very different visual representations.", "description": "This figure demonstrates how multilingual data enriches vision-language datasets.  Panel (a) showcases examples of culturally specific concepts from non-English datasets that are absent in English-centric datasets like those filtered by CLIP. Panel (b) illustrates how even common objects like stoves are visually depicted differently across languages, highlighting the visual diversity introduced by multilingual data. This diversity is not just about word choices but also about the visual representations of the objects.", "section": "1 Introduction"}, {"figure_path": "1WtEqReCyS/figures/figures_13_2.jpg", "caption": "Figure 1: Multilingual image-text data adds diversity to the English data distribution in various, significant ways (a) We show some examples of culturally salient concepts that would not exist in \"high-quality\" English data (as determined by CLIP score), such as \"bamboo steamer\", \"kiji\" (the national bird of Japan) and yal\u0131 (a traditional architecture style for Turkish waterside houses) (b) Even for a common everyday object (\"stove\"), non-English and English images portray very different visual representations.", "description": "This figure shows how multilingual data adds diversity to English-centric datasets.  Panel (a) illustrates examples of culturally specific concepts (bamboo steamer, kiji bird, yal\u0131 house) rarely found in English-only datasets. Panel (b) demonstrates how even common objects (stoves) have different visual representations across different languages, highlighting the richness and variety of multilingual data.", "section": "1 Introduction"}, {"figure_path": "1WtEqReCyS/figures/figures_13_3.jpg", "caption": "Figure 1: Multilingual image-text data adds diversity to the English data distribution in various, significant ways (a) We show some examples of culturally salient concepts that would not exist in \"high-quality\" English data (as determined by CLIP score), such as \"bamboo steamer\", \"kiji\" (the national bird of Japan) and yal\u0131 (a traditional architecture style for Turkish waterside houses) (b) Even for a common everyday object (\"stove\"), non-English and English images portray very different visual representations.", "description": "This figure demonstrates how multilingual data enriches datasets by showing examples of culturally specific items and variations in visual representations of common objects.  Panel (a) highlights concepts likely absent from English-centric datasets, illustrating the value of including non-English data. Panel (b) shows how even common objects like \"stoves\" are depicted differently across languages, emphasizing the diversity in visual representation.", "section": "1 Introduction"}, {"figure_path": "1WtEqReCyS/figures/figures_14_1.jpg", "caption": "Figure 1: Multilingual image-text data adds diversity to the English data distribution in various, significant ways (a) We show some examples of culturally salient concepts that would not exist in \"high-quality\" English data (as determined by CLIP score), such as \"bamboo steamer\", \"kiji\" (the national bird of Japan) and yal\u0131 (a traditional architecture style for Turkish waterside houses) (b) Even for a common everyday object (\"stove\"), non-English and English images portray very different visual representations.", "description": "This figure demonstrates how multilingual data enriches vision-language datasets by showcasing two key aspects: (a) It illustrates the inclusion of culturally specific concepts not commonly found in English datasets, such as a bamboo steamer, the Japanese national bird (kiji), and a traditional Turkish waterside house (yal\u0131).  These examples highlight the limitations of English-centric datasets. (b) It shows how even common objects, like a stove, are visually represented differently across languages and cultures, demonstrating the value of multilingual data for creating more robust and comprehensive visual representations.", "section": "1 Introduction"}, {"figure_path": "1WtEqReCyS/figures/figures_14_2.jpg", "caption": "Figure 1: Multilingual image-text data adds diversity to the English data distribution in various, significant ways (a) We show some examples of culturally salient concepts that would not exist in \"high-quality\" English data (as determined by CLIP score), such as \"bamboo steamer\", \"kiji\" (the national bird of Japan) and yal\u0131 (a traditional architecture style for Turkish waterside houses) (b) Even for a common everyday object (\"stove\"), non-English and English images portray very different visual representations.", "description": "This figure demonstrates how multilingual data enriches vision-language datasets.  Panel (a) showcases examples of culturally specific objects that are unlikely to appear in English-centric datasets, illustrating the increased conceptual diversity offered by multilingual data. Panel (b) compares the visual representation of a common object (\"stove\") across different languages, highlighting the diverse visual interpretations of the same concept.", "section": "1 Introduction"}, {"figure_path": "1WtEqReCyS/figures/figures_14_3.jpg", "caption": "Figure 1: Multilingual image-text data adds diversity to the English data distribution in various, significant ways (a) We show some examples of culturally salient concepts that would not exist in \"high-quality\" English data (as determined by CLIP score), such as \"bamboo steamer\", \"kiji\" (the national bird of Japan) and yal\u0131 (a traditional architecture style for Turkish waterside houses) (b) Even for a common everyday object (\"stove\"), non-English and English images portray very different visual representations.", "description": "This figure shows how multilingual data introduces diversity into vision-language models.  Panel (a) demonstrates that multilingual datasets include culturally specific concepts absent in English-centric datasets.  Panel (b) illustrates that even for common objects like stoves, the visual representation differs significantly between English and non-English datasets, highlighting the enriching potential of multilingual data.", "section": "1 Introduction"}, {"figure_path": "1WtEqReCyS/figures/figures_14_4.jpg", "caption": "Figure 1: Multilingual image-text data adds diversity to the English data distribution in various, significant ways (a) We show some examples of culturally salient concepts that would not exist in \"high-quality\" English data (as determined by CLIP score), such as \"bamboo steamer\", \"kiji\" (the national bird of Japan) and yal\u0131 (a traditional architecture style for Turkish waterside houses) (b) Even for a common everyday object (\"stove\"), non-English and English images portray very different visual representations.", "description": "This figure shows how multilingual data enriches datasets by providing culturally salient concepts and visually diverse representations of common objects.  Panel (a) demonstrates this by showing examples of items unlikely to be found in English-centric datasets. Panel (b) illustrates how even common objects like stoves are depicted differently in various languages, highlighting the value of multilingual data in improving multimodal models.", "section": "1 Introduction"}, {"figure_path": "1WtEqReCyS/figures/figures_14_5.jpg", "caption": "Figure 1: Multilingual image-text data adds diversity to the English data distribution in various, significant ways (a) We show some examples of culturally salient concepts that would not exist in \"high-quality\" English data (as determined by CLIP score), such as \"bamboo steamer\", \"kiji\" (the national bird of Japan) and yal\u0131 (a traditional architecture style for Turkish waterside houses) (b) Even for a common everyday object (\"stove\"), non-English and English images portray very different visual representations.", "description": "This figure shows how multilingual data adds diversity to English data.  Panel (a) gives examples of concepts prevalent in non-English cultures that are absent from English-centric datasets, highlighting the cultural richness multilingual data offers. Panel (b) illustrates that even for common objects like stoves, the visual representation can differ significantly between English and non-English data sources.", "section": "1 Introduction"}, {"figure_path": "1WtEqReCyS/figures/figures_16_1.jpg", "caption": "Figure 6: Top 20 languages that are most common in top 20% raw captions (left) and top 20% translated multilingual captions (right) (both are filtered with the public DFN model).", "description": "This figure shows the language distribution of the top 20% of image-text pairs selected by the DFN filter from the raw data pool (left) and the translated data pool (right).  The left chart represents the original language distribution before translation.  The right chart shows the distribution *after* all captions have been translated to English.  The visualization helps demonstrate how the translation process and subsequent filtering change the representation of different languages in the dataset.  English dominates the original data, but the translated dataset shows a more diverse distribution, though English remains highly prevalent.", "section": "D.2 Language composition of the filtered subsets"}, {"figure_path": "1WtEqReCyS/figures/figures_17_1.jpg", "caption": "Figure 7: Top languages that see the biggest change (in absolute percentage) in their representation in the final training set when we filter with translated multilingual captions versus with raw web-scraped captions.", "description": "This figure shows the change in the proportion of different languages in the training dataset after applying translation and filtering.  The x-axis represents the percentage change in the proportion of each language after translation and re-filtering compared to before translation.  Positive values indicate an increase in the language's proportion, negative values indicate a decrease. The figure highlights which languages saw the most significant shifts in their representation in the final, high-quality training dataset.", "section": "D.3 Changes in language composition"}, {"figure_path": "1WtEqReCyS/figures/figures_21_1.jpg", "caption": "Figure 3: With the same degree of filtering, training with (image, translated caption) pairs improves performance on 28 out of 38 tasks compared to training with (image, raw caption) pairs, including on ImageNet distribution shifts, retrieval, and tasks with geographically diverse inputs. We compare performance on each task of the DataComp benchmark between training with raw captions and training with translated captions. Both datasets have been filtered with image-text cosine similarities output by the public DFN [13] to select the top 30% examples. We find that using translated captions leads to 1.5 percentage points improvement on average across 38 tasks. We highlight the performance changes on ImageNet distribution shifts (red), retrieval (blue) and fairness-related tasks (dark yellow).", "description": "This figure compares the performance of models trained on datasets with raw captions and translated captions across 38 tasks from the DataComp benchmark. The datasets were filtered using cosine similarity scores to select the top 30% of samples.  The results show that using translated captions leads to performance improvements on many tasks, especially those related to ImageNet distribution shifts, retrieval, and fairness.", "section": "4 Impacts of using (translated) multilingual captions on standard vision tasks"}, {"figure_path": "1WtEqReCyS/figures/figures_22_1.jpg", "caption": "Figure 9: On Dollar Street, using translated multilingual captions leads to performance improvement across all income groups. Dollar Street [39] is another fairness-related task that involves classifying images of everyday items collected from households around the world with different socio-economic backgrounds. We break down the performance on this dataset by income groups and find that training on top-quality translated captions improves the classification accuracy across all groups, compared to training on top-quality raw captions.", "description": "This figure shows the improved performance of using translated multilingual captions on the Dollar Street dataset, which focuses on fairness.  The dataset consists of images of common objects collected from households across various income levels globally.  The results demonstrate that using the translated captions improves accuracy across all income groups compared to using only raw captions.  The improvement underscores the benefit of incorporating multilingual data for enhancing model performance on fairness-related tasks.", "section": "I More performance analysis"}, {"figure_path": "1WtEqReCyS/figures/figures_22_2.jpg", "caption": "Figure 1: Multilingual image-text data adds diversity to the English data distribution in various, significant ways (a) We show some examples of culturally salient concepts that would not exist in \"high-quality\" English data (as determined by CLIP score), such as \"bamboo steamer\", \"kiji\" (the national bird of Japan) and yal\u0131 (a traditional architecture style for Turkish waterside houses) (b) Even for a common everyday object (\"stove\"), non-English and English images portray very different visual representations.", "description": "This figure demonstrates how multilingual data introduces diversity into vision-language datasets.  Panel (a) showcases examples of culturally specific objects, which are unlikely to appear in English-centric datasets. Panel (b) shows how even common objects like stoves are depicted differently across languages, highlighting the visual diversity introduced by non-English data.", "section": "1 Introduction"}]