[{"type": "text", "text": "Energy-based Epistemic Uncertainty for Graph Neural Networks ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Dominik Fuchsgruber, Tom Wollschl\u00e4ger, and Stephan G\u00fcnnemann ", "page_idx": 0}, {"type": "text", "text": "School of Computation, Information and Technology & Munich Data Science Institute Technical University of Munich, Germany {d.fuchsgruber, tom.wollschlaeger, s.guennemann}@tum.de ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In domains with interdependent data, such as graphs, quantifying the epistemic uncertainty of a Graph Neural Network (GNN) is challenging as uncertainty can arise at different structural scales. Existing techniques neglect this issue or only distinguish between structure-aware and structure-agnostic uncertainty without combining them into a single measure. We propose GEBM, an energy-based model (EBM) that provides high-quality uncertainty estimates by aggregating energy at different structural levels that naturally arise from graph diffusion. In contrast to logit-based EBMs, we provably induce an integrable density in the data space by regularizing the energy function. We introduce an evidential interpretation of our EBM that significantly improves the predictive robustness of the GNN. Our framework is a simple and effective post hoc method applicable to any pre-trained GNN that is sensitive to various distribution shifts. It consistently achieves the best separation of in-distribution and out-of-distribution data on 6 out of 7 anomaly types while having the best average rank over shifts on all datasets. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Quantifying and understanding uncertainty is crucial to developing safe and reliable machine learning systems. Many applications such as Reinforcement Learning [42], Active Learning [5, 31] or Outof-Distribution detection [60] benefit from disentangling different facets of uncertainty [54, 17, 46]. Typically, one distinguishes between an irreducible aleatoric component and reducible epistemic factors [29]. The former is inherent to the data, while the latter is rooted in a lack of knowledge. One way to quantify epistemic uncertainty is to define a classifier-dependent density over the data domain [55]. High values indicate similarity to the training data and thus imply low epistemic uncertainty. Energy-based models (EBMs) induce this density by defining an energy function that assumes low values near the training data [38]. A common choice is the logits of the classifier as their magnitude is supposed to correlate with the model confidence [40]. However, many architectures have been shown to produce arbitrarily overconfident predictions far from the training data [28]. The negative implications for logit-based EBMs are only scarcely discussed in the literature [35, 36]. ", "page_idx": 0}, {"type": "text", "text": "While substantial effort has been directed toward uncertainty estimation for independent and identically distributed (i.i.d.) problems [1, 24, 49, 37, 6, 22], graphs have only recently received attention within the community [71, 64, 67, 21]. There, uncertainty can arise at different structural scales, e.g., from only a single node, clusters, or global properties. Previous work only accounts for this implicitly by applying techniques from i.i.d. domains to Graph Neural Networks (GNNs) [67, 49]. A recent approach distinguishes only between structure-aware and structure-agnostic uncertainty [64] while not combining them into a single measure, which is often required in downstream applications [42, 54]. Consequently, estimates are often only sensitive to shifts that match their structural resolution. They may overfit certain anomaly types and not be reliable in general. ", "page_idx": 0}, {"type": "image", "img_path": "6vNPPtWH1Q/tmp/f08158e6f6dd356102c2363fd6e033c6572a035f3bc91e7f23ef8ec9c459a0a4.jpg", "img_caption": ["Figure 1: Overview of the Graph Energy-based Model (GEBM). Graph-agnostic energy (uncertainty) of a trained GNN $f_{\\theta}(\\mathcal{G})$ is first regularized to mitigate overconfidence and then aggregated at a local, cluster, and structure-independent scale by interleaving energy marginalization and graph diffusion. While group energy marginalizes before diffusion, local energy is fine-grained and can pick up conflicting evidence. GEBM assigns high uncertainty to several anomaly types simultaneously. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "We address these shortcomings and propose a novel graph-based EBM (GEBM) that is structureaware at different levels. The core idea behind GEBM is to define energy functions at different structural abstractions and facilitate the flexibility of the EBMs to aggregate them into a single, theoretically grounded measure. Interestingly, we find that interleaving a graph diffusion process with energy marginalization gives rise to energy functions that naturally capture patterns of different granularity. As depicted in Figure 1, we utilize three energy types: (i) Group energy corresponds to evidence smoothing and emphasizes anomalies clusters within the graph while not distinguishing between their type locally. (ii) Local energy is more granular and sensitive to evidence disagreements in the neighborhood of a node. (iii) Independent energy is fully structure-agnostic and describes patterns that are limited to individual nodes. Aggregating them using soft maximum selection induces a single energy measure that assigns high uncertainty to anomalies at various scales. A Gaussian regularizer provably ensures that GEBM converges to low confidence far from the training data. ", "page_idx": 1}, {"type": "text", "text": "We evaluate GEBM over an extensive suite of datasets, distribution shifts, and baselines1. It consistently exhibits state-of-the-art performance in detecting out-of-distribution (o.o.d.) instances, while other approaches are only effective in a subset of settings. On all datasets, GEBM ranks the best on average over all distribution shifts. Beyond o.o.d. detection, we discover a novel theoretical connection between EBMs and evidential models. This interpretation of GEBM enables the GNN backbone to provide accurate predictions even under severe distribution shifts. ", "page_idx": 1}, {"type": "text", "text": "In summary, we tackle three main deficiencies of energy-based uncertainty for graphs by: ", "page_idx": 1}, {"type": "text", "text": "1. Proposing GEBM, an EBM that is aware of interdependence and provides a single uncertainty estimate incorporating different structural scales.   \n2. Formally showing that a Gaussian regularizer mitigates the overconfidence problem of logit-based EBMs and enables GEBM to provably induce an integrable data density.   \n3. Showing how to interpret GEBM as an evidential approach which considerably improves the predictive robustness of the classifier under distribution shifts. ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Semi-Supervised Node Classification. We consider (semi-supervised) node classification on an attributed graph ${\\mathcal{G}}=(A,X)$ with $n$ nodes $\\mathcal{V}$ and $m$ edges $\\mathcal{E}$ that is represented by an adjacency matrix $A=\\{0,1\\}^{n\\times n}$ and a node feature matrix $\\pmb{X}\\in\\mathbb{R}^{n\\times d}$ . Each node has a label $y_{i}\\in\\{1,...\\,C\\}$ Given the labels of training nodes, the task is to infer the labels of the remaining nodes. Lastly, we focus on homophilic graphs, i.e. edges are predominantly present between nodes of the same class. ", "page_idx": 1}, {"type": "text", "text": "Uncertainty in Machine Learning. Usually, uncertainty regarding the prediction of a model is disentangled into aleatoric and epistemic factors $u^{\\mathrm{alea}}$ and $u^{\\mathrm{epi}}$ , respectively. The former encompasses irreducible sources of uncertainty like measurement noise and inherent ambiguities. The latter is commonly defined as the non-aleatoric components of the overall uncertainty and can in general, be reduced, for example by acquiring additional data. ", "page_idx": 2}, {"type": "text", "text": "Energy-based Models. Epistemic uncertainty is commonly defined to be anti-correlated to a classifier-dependent density $p_{\\theta}(x)$ over the input space [55]. We study Energy-based Models (EBMs) [38] which define a joint energy $E_{\\theta}(x,y)$ over features and labels $p_{\\theta}(x,\\stackrel{..}{y})\\propto\\mathrm{exp}\\left(-E_{\\theta}\\left(\\mathbf{x},y\\right)\\right))$ . Marginalization over the labels induces a feature density with normalizer $Z_{\\theta}$ . ", "page_idx": 2}, {"type": "equation", "text": "$$\np_{\\theta}({\\pmb x})=Z_{\\theta}^{-1}\\sum_{y}\\exp\\left(-E_{\\theta}\\left({\\pmb x},y\\right)\\right))=Z_{\\theta}^{-1}\\exp\\left(-E_{\\theta}({\\pmb x})\\right)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Intuitively, the energy $\\begin{array}{r}{E_{\\theta}(\\mathbf{x})=-\\log\\sum_{y}\\exp\\left(-E_{\\theta}(\\mathbf{x},y)\\right)}\\end{array}$ can be interpreted as a soft minimum of the joint energies. It is low in regions of high confidence and high elsewhere. The measure of epistemic uncertainty implied by an EBM therefore is: $u^{\\mathrm{epi}}({\\pmb x})=-\\log p_{\\theta}({\\pmb x})=E_{\\theta}({\\pmb x})+\\epsilon$ const. We can also write the conditional class probabilities as: ", "page_idx": 2}, {"type": "equation", "text": "$$\np_{\\theta}(y\\mid x)=\\exp{(-E_{\\theta}\\left({\\pmb x},y\\right))})/\\!\\sum_{y^{\\prime}}\\exp{\\left(-E_{\\theta}\\left({\\pmb x},y^{\\prime}\\right)\\right)})\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "This is exactly the softmax distribution applied to $-E_{\\theta}({\\bf{x}},y)$ which directly connects the logits $f_{\\theta}(\\mathbf{\\boldsymbol{x}})\\in\\mathbb{R}^{C}$ predicted by a classifier to the joint energy by defining $E_{\\theta}({\\pmb x},y)\\stackrel{!}{:}=-f_{\\theta}({\\pmb x})_{y}$ [40]. ", "page_idx": 2}, {"type": "text", "text": "Evidential Deep Learning. In contrast to first-order methods that directly predict $p(\\boldsymbol{y}\\mid\\boldsymbol{x})$ , evidential methods [62, 43] instead parameterize the corresponding second-order distribution from which firstorder distributions are sampled [32]. In classification, this second-order distribution is a Dirichlet distribution with parameters $\\alpha>0$ called evidence. They indicate the confidence of the classifier. For inference, a first-order prediction can be obtained by taking the expectation: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbb{E}_{p\\sim\\operatorname{Dir}(\\alpha)}\\left[p(y\\mid x)\\right]=\\alpha/\\sum_{y^{\\prime}}\\alpha_{y^{\\prime}}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "3 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Uncertainty Estimation for i.i.d. Data. Disentangling uncertainty has been approached from various perspectives. A family of sampling-based approaches uses a Bayesian Information-theoretic framework [29] that relies on stochastic predictions derived from a posterior over model weights. The total uncertainty is defined on the mean predictor, epistemic uncertainty as the deviation of each sample from that mean, and aleatoric uncertainty as the difference of both estimates [1]. The posterior can either be explicitly modeled by Bayesian Neural Networks (BNNs) [6, 13, 15, 18], Monte-Carlo Dropout (MCD) [22], or implicitly realized by ensemble methods [37, 77]. Test-time augmentation [72, 73] and Stochastic Centering [66] also provide samples from this posterior. Sampling-free approaches are deterministic and estimate uncertainty with a single forward pass. Evidential methods [62, 68] predict a second-order distribution from which epistemic uncertainty is derived. Distancebased approaches quantify epistemic uncertainty as similarity to the training data [46, 48, 20, 65] and are closely related to density-based uncertainty estimation. (Deep) Gaussian Processes (GPs) [52, 45, 39] use a kernel function to measure this similarity but do not disentangle epistemic and aleatoric uncertainty. Posterior Networks combine evidential and density-based approaches by predicting density-based updates to the evidence [11, 10]. ", "page_idx": 2}, {"type": "text", "text": "Uncertainty Estimation for Graphs. Many of these approaches transfer to graphs: DropEdge [59] applies MCD to edges and GPs can utilize a structure-aware kernel [53, 83, 41, 60]. SGCN [82] proposes an evidential student-teacher approach while G- $\\Delta$ UQ [67] applies Stochastic Centering to a GNN and improves on calibration. Graph Posterior Network (GPN) [34] diffuses the densitybased evidence of a Posterior Network but provides separate measures for structure-aware and structure-agnostic uncertainty, each of which is only effective on some distribution shifts. ", "page_idx": 2}, {"type": "text", "text": "Energy-based Models. EBMs [38, 3, 2] are typically employed in generative modelling [14] but have also been applied to uncertainty estimation [16] and anomaly detection [40, 81]. To address the overconfidence of logit-based EBMs far from training data, a Gaussian regularization term has recently been proposed [35] that we employ in our framework as well. While their work empirically validates this adjustment, we formally prove the overconfidence of logit-based EBMs to happen with high probability and Gaussian regularization to mitigate the issue. The HEAT framework [36] learns multiple corrected EBMs via stochastic gradient Langevin dynamics [76] and composes them into a single measure. In contrast, our approach does not require additional training and aggregates energy that emerges naturally at different scales in the graph from a single logit-based joint energy model. Lastly, GNNSafe [79] diffuses the logit-based energy of a GNN to improve its out-of-distribution detection. In contrast, our model considers energy beyond the cluster scale. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "4 Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We develop a simple yet effective Graph-Energy-Based Model (GEBM) for post hoc epistemic uncertainty that is sensitive to a variety of distribution shifts from any pre-trained GNN. ", "page_idx": 3}, {"type": "text", "text": "4.1 Energy at Different Scales ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Previous work either does not distinguish between uncertainty at different structural resolutions at all [79] or only disentangles structure-aware and structure-agnostic factors without combining them [64]. However, for many practical purposes, epistemic uncertainty must be quantified with a single measure [42, 5, 60]. We address these issues by proposing an EBM-based uncertainty that incorporates patterns on different natural abstractions of a graph. The density induced by GEBM, $p_{\\theta}({\\pmb x})$ , serves as a singular uncertainty measure that is sensitive to multiple distribution shifts simultaneously. ", "page_idx": 3}, {"type": "text", "text": "Inspired by the success of graph diffusion [9, 23], we propose energy on different structural levels: (i) Graph-agnostic, for node features in isolation. (ii) Based on the evidence in the local neighborhood of a node. (iii) Within clusters in the graph. Note that defining global energy on the whole graph induces no differences at the node level and therefore we do not consider it in this work. We point to appropriate measures for a potential extension of GEBM in the existing literature [84]. ", "page_idx": 3}, {"type": "text", "text": "We make the intriguing observation that interleaving a diffusion process $P_{A}:\\mathbb{R}^{k}\\,\\rightarrow\\,\\mathbb{R}^{k}$ with the marginalization of structure-agnostic joint energy $E_{\\theta}(x,y)$ as in Equation (1) induces energy functions that describe the aforementioned natural abstraction levels. Based on this insight, we can derive definitions for three types of energy functions: ", "page_idx": 3}, {"type": "text", "text": "Independent Energy. On the finest scale, we consider energy independent of structural effects by omitting the diffusion operator. This term captures uncertainty regarding node features in isolation. ", "page_idx": 3}, {"type": "equation", "text": "$$\n{\\cal E}_{\\theta,I}({\\pmb x})=-\\log\\sum_{y}\\exp\\left(-E_{\\theta}({\\pmb x},y)\\right)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Local Energy. On a coarser scale, we diffuse the joint energy before marginalization. This retains local information like conflicting feature-based evidence within the neighborhood of a node as the class-specific information is marginalized after propagation: ", "page_idx": 3}, {"type": "equation", "text": "$$\n{\\cal E}_{\\theta,L}({\\pmb x})=-\\log\\sum_{y}\\exp\\left[P_{\\cal A}\\left(-E_{\\theta}({\\pmb x},y)\\right)\\right]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Group Energy. By interchanging marginalization and diffusion, we effectively smooth the marginal evidence $E_{\\theta}(x)$ . Therefore, energy gets propagated predominantly within clusters of the graph. ", "page_idx": 3}, {"type": "equation", "text": "$$\nE_{\\theta,G}(\\pmb{x})=-P_{\\pmb{A}}\\left[\\log\\sum_{y}\\exp{(-E_{\\theta}(\\pmb{x},y))}\\right]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Since marginalization is done before propagation, less local information is preserved: The energy of a node will increase when its cluster is anomalous, regardless of whether the type of anomaly matches its own. As can be seen exemplary in Figure 1, this loss of local information comes at the benefit of being less exposed to local variability within coarser clustered patterns. ", "page_idx": 3}, {"type": "text", "text": "Each energy type captures anomalies that affect the corresponding structural scale. We provide synthetic experiments in Appendix C.7 as an additional intuitive explanation for the aforementioned energy terms. In practice, GEBM enables practitioners to augment our framework with further, potentially task-specific, energy functions. We empirically find however that the combination of the three naturally arising energy terms already detects a broad range of distribution shifts. ", "page_idx": 3}, {"type": "text", "text": "4.2 Regularizing Logit-based EBMs ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Following Equation (1), EBMs imply a density over the data domain $p_{\\theta}({\\pmb x})$ with normalization constant: ", "page_idx": 4}, {"type": "equation", "text": "$$\nZ_{\\theta}=\\int_{\\mathcal{X}}\\sum_{y}\\exp\\left(-E_{\\theta}(\\mathbf{x},y)\\right)d\\mathbf{x}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "When quantifying epistemic uncertainty with this density, the energy must be low in regions of the input feature space close to the training data, while high values should be assumed far away. To that end, $p_{\\theta}({\\pmb x})$ must be integrable, that is, have a finite normalizer $Z_{\\theta}$ . However, previous work has shown that piecewise affine classifiers which are the backbone of modern GNNs will converge to overconfident logits far from training data [28]. We remark that previous studies found GNNs to be underconfident on in-distribution data [74, 75] while the aforementioned issue of overconfidence arises from high distance to training data induced by a distribution shift (see Appendix C.8). This overconfidence is problematic, as logit-based energy can suffer from the same issue and $E_{\\theta}(x)$ may assume arbitrarily small values far from the training data. This contradicts the aforementioned desideratum of low confidence and the implied uncertainty measure breaks under severe distribution shifts. The normalizer of the logit-based joint energy $Z_{\\theta}$ is not finite with a high probability and therefore the EBM can not induce an integrable density $p_{\\theta}({\\pmb x})$ . ", "page_idx": 4}, {"type": "text", "text": "Proposition 4.1. Let $f_{\\theta}:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{C}$ be a piecewise affine function and $\\mathbb{R}^{h}=\\cup_{l}^{L}\\,Q_{l}$ be the disjoint set of polytopes on which f\u03b8 is affine, i.e. $\\bar{f}_{\\theta}(\\mathbf{x})=W^{(l)}\\mathbf{x}+b^{(l)}$ for $\\pmb{x}\\in Q_{l}$ .  Assuming the direction of the rows of each $\\boldsymbol{W}^{(l)}$ to be uniformly distributed, the probability that $Z_{\\theta}$ converges decreases exponentially in the number of non-closed linear regions $L^{\\prime}$ and classes $C$ . ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{Pr}\\left[Z_{\\theta}<\\infty\\right]=(1/2)^{C\\cdot L^{\\prime}}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We provide proofs for all claims in Appendix A. Intuitively, since $f_{\\theta}$ behaves like an affine function in the limit $\\|{\\pmb x}\\|_{2}\\rightarrow\\infty$ , its predicted logits may diverge toward $\\infty$ . If for any class the model produces overconfident predictions in one of its affine regions, the marginal energy $\\operatorname{\\bar{\\boldsymbol{E}}}_{\\theta}(\\boldsymbol{\\mathbf{\\bar{x}}})$ will diverge toward maximal confidence. As classifiers are trained to output high values for one of the classes, we expect the actual probability of a well-behaved energy function to be even lower than our theoretical bound that assumes uniform weight directions. In practice, this pathological behavior of logit-based EBMs has been observed in previous work [64, 36, 35]. We mitigate this issue by augmenting the logit-based energy and we prove that this regularization induces an integrable density. ", "page_idx": 4}, {"type": "text", "text": "4.3 Our Model ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Similarly to recent work on EBMs [35], we employ a class-conditional Gaussian prior $\\mathcal{N}(f_{\\theta}(\\bar{\\mathbf{x})_{y.}}|\\;\\mu_{y},\\Sigma_{y})$ as a regularizer for the logit-based energy. We learn the parameters $\\{\\mu_{y},\\Sigma_{y}\\}_{y=1}^{C}$ as the maximum likelihood estimates from the training instances of each class. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{E_{\\theta}}({\\pmb x},y)=E_{\\theta}({\\pmb x},y)-\\gamma\\log{\\mathcal{N}({\\pmb f}_{\\theta}({\\pmb x})_{y}\\mid{\\pmb\\mu}_{y},{\\pmb\\Sigma}_{y})}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The regularization strength $\\gamma>0$ and the choice of the diffusion operator $P_{A}$ are the only hyperparameters of GEBM. Each of them has an intuitive interpretation: $\\gamma$ controls the trade-off between the predictive dependency and distance-awareness of the EBM while $P_{A}$ encapsulates how information is propagated over $\\mathcal{G}$ . Regularization ensures that the corresponding marginal density $\\hat{p}_{\\theta}({\\pmb x})$ has a finite normalizer $\\hat{Z}_{\\theta}$ and is therefore integrable. ", "page_idx": 4}, {"type": "text", "text": "Theorem 4.2. For a piecewise affine classifier $f_{\\theta}$ as in Proposition 4.1, $\\hat{p_{\\theta}}$ is well-defined. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{Z}_{\\theta}=\\int_{\\mathcal{X}}\\sum_{y}\\exp(-E_{\\theta}(\\pmb{x},y))\\mathcal{N}(f_{\\theta}(\\pmb{x})_{y}\\mid\\mu_{y},\\Sigma_{y})^{\\gamma}d\\pmb{x}<\\infty\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This regularized joint energy diverges toward maximal uncertainty in the limit. Consequently, its induced density provides an uncertainty estimator that is reliable even far away from the training data. ", "page_idx": 4}, {"type": "text", "text": "Corollary 4.3. For a piecewise affine classifier $f_{\\theta}$ as in Proposition 4.1, and any $\\pmb{x}\\in\\mathbb{R}^{d}$ almost surely: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{\\alpha\\to\\infty}\\hat{p}_{\\theta}(\\alpha{\\pmb x})=0\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We then combine the regularized energy $\\hat{E}_{\\theta}(x,y)$ at different structural scales (Equations (4) to (6)). ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\hat{E}_{\\theta,\\mathrm{GEBM}}(\\pmb{x})=\\log\\left[\\exp\\left(\\hat{E}_{\\theta,I}(\\pmb{x})\\right)+\\exp\\left(\\hat{E}_{\\theta,L}(\\pmb{x})\\right)+\\exp\\left(\\hat{E}_{\\theta,G}(\\pmb{x})\\right)\\right]\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Since energy is defined per node, most diffusion processes (Appendix C.6) act on individual nodes as an affine function with a positive coefficient (see Appendix A.2). Therefore, each individual energy and their aggregate, $\\hat{E}_{\\theta,\\mathrm{GEBM}}$ , induce an integrable density when using a linear diffusion operator $P_{A}$ . ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.4. For a linear diffusion operator $P_{\\mathbf{}A}(\\mathbf{x})=\\alpha\\mathbf{}\\mathbf{x}+c o n s t$ , $\\alpha\\,>\\,0$ and the regularized energy $\\hat{E}_{\\theta}(x,y)$ , GEBM induces an integrable density: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\int_{\\mathcal{X}}\\exp\\left(-\\hat{E}_{\\theta,G E B M}\\left(\\mathbf{x}\\right)\\right)d\\mathbf{x}<\\infty\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "GEBM is lightweight and can be applied post hoc to any logit-based GNN without additional training. Its induced density is a proxy for epistemic uncertainty: $u^{\\mathrm{epi}}({\\pmb x})=-\\log\\hat{p}_{\\theta}({\\pmb x})$ and we compute aleatoric uncertainty as the entropy of the predictive distribution of the GNN [29]. Following [79], we realize the diffusion operator as a repeated application of a label-propagation scheme (see Appendix C.6). Intuitively, this operator is a smoothing process which is only appropriate when assuming the in-distribution data to be homophilic. In the case of non-smooth (heterophilic) training data, the proposed energy would be high for in-distribution data which is undesired behaviour for an EBM-based uncertainty estimator. To disentangle effects at different structural scales, we define each of GEBM\u2019s components on structure-agnostic regularized joint energies $\\hat{E}_{\\theta}(x,y)$ . To that end, we compute the outputs of the classifier by omitting the structure for the computation of $\\hat{E}_{\\theta}$ only by evaluating $f_{\\theta}(\\boldsymbol{I},\\boldsymbol{X})$ , i.e. setting $A=I$ . These outputs depend only on the node features. ", "page_idx": 5}, {"type": "text", "text": "4.4 EBMs as Evidential Models ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Beyond using epistemic uncertainty to detect anomalies, we also show how the joint density induced by GEBM enables predictions that are robust against distribution shifts. First, we discover a correspondence between first-order predictions of logit-based classifiers (Equation (2)) and evidential predictions (Equation (3)). This connects the joint energy $E_{\\theta}(x,y)$ of an EBM to the evidence $_{\\alpha}$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\alpha\\simeq\\exp\\left(-E_{\\theta}(\\pmb{x},\\pmb{y})\\right)\\propto p_{\\theta}(\\pmb{x},\\pmb{y})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "For an integrable density $p_{\\theta}(x,y)$ , the evidence $_{\\alpha}$ will vanish in the limit far away from training data. Previous work on Posterior Networks [11, 10] ftis a normalizing flow to obtain class-conditional densities $p_{\\theta}(x\\mid y)$ and uses them to compute a Bayesian update to a prior evidence $\\alpha^{\\mathrm{prior}}$ . Its extension to graphs, GPN [64], diffuses these structure-agnostic updates with an operator $P_{A}$ . Similarly, our GEBM framework induces a normalized joint density $\\hat{p}_{\\theta}\\bar{(}{\\pmb x},{\\pmb y})$ through its regularized energy function at no additional cost. Therefore, it can also be interpreted as an evidential classifier that enables inference according to Equation (3). ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\overbrace{\\alpha^{\\mathrm{post}}=\\alpha^{\\mathrm{prior}}+P_{A}\\left(N*p_{\\theta}(x,y)\\right)}^{\\infty^{\\mathrm{post}}}}&{{}\\overbrace{\\alpha^{\\mathrm{post}}=\\alpha^{\\mathrm{prior}}+P_{A}\\left(Z_{\\theta}^{-1}*\\exp(-\\hat{E}_{\\theta}(x,y)\\right)\\right)}^{\\infty^{\\mathrm{prost}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Here, $_{N}$ is called an uncertainty budget and it roughly corresponds to the normalizer of the joint energy. This interpretation of EBMs recovers desirable properties of density-based evidential methods [11, 64]: Predictions will converge toward a prior $\\alpha^{\\mathrm{prior}}$ far from the training distribution. They will be less affected by anomalies in the neighborhood of a node and therefore be more robust against distribution shifts. We remark that while our framework enables this evidential inference scheme, it is also possible to instead use the backbone classifier as-is and preserve its predictive performance. ", "page_idx": 5}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We evaluate the efficacy of GEBM by extending the evaluation proposed in [64] and expose it to a suite of 7 distribution shifts from three families that cover a broad range of anomaly types. ", "page_idx": 5}, {"type": "text", "text": "5.1 Setup", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Datasets and Distribution Shifts. We use seven common benchmark datasets for node classification: The five citation datasets CoraML[4], CoraML-LLM[4, 78] [4], Citeseer [61, 25], PubMed [51], Coauthor-Physics and Coauthor-Computers [63], and the co-purchase graphs Amazon Photos and Amazon Computers [44]. We expose all methods to three families of distribution shifts: ", "page_idx": 6}, {"type": "text", "text": "(i) Structural. We select nodes with the lowest homophily as o.o.d. and train on homophilic nodes only. This induces a shift in the local connectivity pattern of the nodes. Furthermore, we include a setting in which nodes with low Page-Rank (PR) centrality [56] are considered o.o.d.   \n(ii) Leave-out-Class. We withhold nodes belonging to a subset of classes during training and reintroduce them during inference. In practice, this might, for example, correspond to a new type of user joining a social network. We either select the held-out classes randomly or pick those with the lowest average homophily to make them more dissimilar to the retained classes.   \n(iii) Feature Perturbations. We randomly choose a subset of nodes and perturb their features by replacing them with noise. Since most datasets have categorical bag-of-words features, we have fine-grained control over the severity of the shift in this setting. We generate near-o.o.d. data by sampling from a Bernoulli distribution $\\bar{p}$ fitted on the node features of the dataset $\\mathbf{\\deltaX}$ . A stronger shift is induced by fixing the success probability at a value of $p=0.5$ . Drawing node features from ${\\mathcal{N}}(0,1)$ constitutes a domain shift (far-o.o.d.) for categorical data. ", "page_idx": 6}, {"type": "text", "text": "Existing work concerns transductive node classification which enables leakage of o.o.d. information during training [64, 79]. Instead, we study the inductive setting and remove o.o.d. nodes and their edges during training, and provide results for the transductive setting in Appendix C. All results were averaged over 5 splits and 5 initializations each (for standard deviations, see Appendix C). ", "page_idx": 6}, {"type": "text", "text": "Model and Baselines. We compare GEBM to different baselines for (epistemic) uncertainty estimation: Ensembles (GCN-Ens), MC-dropout (GCN-MCD), DropEdge (GCN-DropEdge), a combination of MC-dropout and DropEdge (GCN-MCD $^+$ DropEdge) and a Bayesian GCN (BGCN) are sampling-based approaches. We also compare against the logit-based EBM (GCN-Energy), GNNSafe, and HEAT as EBM baselines. Lastly, we consider GPN and SGCN as evidential methods. For all models, including GEBM, we use the same backbone architecture (see Appendix B.2). ", "page_idx": 6}, {"type": "text", "text": "Metrics. We evaluate epistemic uncertainty by detecting distribution shifts. That is, we report the AUC-ROC and AUC-PR metrics for the binary out-of-distribution detection problem of separating in-distribution (i.d.) from out-of-distribution (o.o.d.) nodes. Additionally, we report how well uncertainty correlates with erroneous (misclassification detection) in Appendix C.3. Since our proposed method does not alter the softmax predictions of the backbone model, it will affect neither the aleatoric estimates nor their calibration. Consequently, our framework is open to additional post hoc calibration methods such as temperature scaling [26]. For completeness, we report the Expected Calibration Error (ECE) [50] and the Brier score [7] in Appendix C.4 for the GNN backbone. ", "page_idx": 6}, {"type": "text", "text": "5.2 Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Out-of-Distribution Detection. Table 1 shows the performance of different aleatoric and epistemic uncertainty methods on various distribution shifts (full results in Table 6). Across all datasets and distribution shifts, our model provides the best or second-best separation of o.o.d. data from i.d. data. In practice, an estimator that is effective on all distribution shifts simultaneously is desirable. Therefore, we rank $\\left(\\downarrow\\right)$ all estimators individually for each shift and dataset and report its average rank over all distribution shifts for each dataset. To not favor one distribution shift family, we adjust the weight such that Leave-out-Class, structural shifts, and feature perturbations contribute the same amount. We rank all epistemic uncertainty proxies both against other epistemic measures and aleatoric uncertainty separately. In both cases, Tables 2 and 7 show that our proposed EBM-based epistemic uncertainty is the only estimator that is effective under different distribution shifts at the same time. On all datasets, GEBM improves the AUC-ROC scores by 5.8 to 10.9 percentage points on average $16\\%{-}32\\%$ relative improvement) over a vanilla EBM, the second best-ranked estimator (Appendix C.2). Since all EBM-based approaches (GCN-EBM, GCNSafe, GEBM) are post hoc methods, they share the aleatoric uncertainty and high predictive accuracy of the backbone. ", "page_idx": 6}, {"type": "text", "text": "Out of all 7 distribution shifts, our framework is only less sensitive to the centrality shift. Many baselines only perform well because of their symmetric diffusion operator that biases any arbitrary signal toward high-degree nodes regardless of the quality of the uncertainty (see Appendix C.6). While this bias could be explicitly incorporated into GEBM by defining an additional energy term, the goal of our work is not to engineer our measure toward certain downstream settings. GEBM is already sensitive to many distribution shifts as-is. Even after accounting for this setting, our estimator achieves the best average rank over all shifts. We defer further discussion to Appendix B. ", "page_idx": 6}, {"type": "table", "img_path": "6vNPPtWH1Q/tmp/dff2bc0b3d28d0a01b23e42f5b83256b8299c1f269020e51edeb638a3da63eaf.jpg", "table_caption": [], "table_footnote": ["Table 1: Out-of-Distribution detection AUC-ROC (\u2191) using aleatoric or epistemic uncertainty (best and runner-up). Our epistemic measure achieves the strongest performance on most datasets and shifts while maintaining the classification accuracy of the GCN backbone. "], "page_idx": 7}, {"type": "table", "img_path": "6vNPPtWH1Q/tmp/17615b45a2bf3bc24f959bf5765cfc1443e61fa0b86e5e6fd33d87598fb3eaf0.jpg", "table_caption": [], "table_footnote": ["Table 2: Average o.o.d. detection rank (\u2193) of epistemic uncertainty versus other epistemic measures / all uncertainty measures over all distribution shifts (best and runner-up). GEBM has the best average performance rank over all distribution shifts and epistemic (and aleatoric) uncertainty estimators. "], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Misclassification Detection. In alignment with previous work [64], we observe aleatoric uncertainty to be more effective for misclassification detection than epistemic estimates. Our method performs competitively among other epistemic measures as shown in Table 16. As previously discussed, improving aleatoric uncertainty for GNNs is beyond the scope of this work. ", "page_idx": 7}, {"type": "text", "text": "Robust Evidential Inference. As described in Section 4.4, GEBM enables evidential inference. We expose our model to feature shifts sampled from $\\mathcal{N}(0,1)$ and increase the fraction of perturbed nodes. Figure 2 shows the predictive performance of the classifier induced by an evidential interpretation of the EBM at different regularization weights $\\gamma$ compared to a vanilla EBM and the evidential model, GPN. We maintain high accuracy like an evidential model, while the performance of the baselines rapidly deteriorates. The hyperparameter $\\gamma$ can be seen as interpolating between an unregularized overconfident logit-based EBM and an uncertainty-aware evidential method. While GPN requires explicit evidential training, our method enables robust inference at negligible additional cost from a pre-trained GNN. Furthermore, GPN notably sacrifices predictive capability on clean data, while GEBM enables more control over the trade-off between accuracy on clean and perturbed data. ", "page_idx": 7}, {"type": "image", "img_path": "6vNPPtWH1Q/tmp/f2dddb5ce5890325351247caa4214ec6e062577544d19254a96163c75d78e2c4.jpg", "img_caption": ["Figure 2: Accuracy of evidential inference at Figure 3: Logit-based, Gaussian, and regularized different regularization $\\gamma$ . GEBM performs on-energy at different magnitudes. Logit-EBMs bepar with evidential methods at increasingly se-come overconfident while the corrected energy is vere perturbations. eventually dominated by its regularizer. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "The advantage of this evidential perspective is that the evidence approaches zero under increasingly severe distribution shifts. Therefore, a fixed number of diffusion steps suffices to effectively counteract the influence of anomalous neighbors when making predictions for a node. In contrast, using diffusion at, for example, the predictive level is inadequate: As shown in Proposition 4.1, the energy is likely to diverge and, therefore, the number of diffusion steps necessary to mitigate arbitrarily severe distribution shifts is unbounded. Consequentially, we find that even compared to models like APPNP that heavily rely on graph diffusion at the predictive level, the evidential interpretation of GEBM notably improves robustness (see Figure 6). ", "page_idx": 8}, {"type": "text", "text": "5.3 Ablations. ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Energy at Different Scales. In Table 3, we compare GEBM to energy at specific structural scales and a variant without regularization $(\\gamma=0)$ ). Each of the former corresponds to one of the energies that the GEBM framework is composed of. Group energy is equivalent to a regularized logit-based EBM which is therefore also ablated implicitly. As expected, each component is sensitive to different shifts. This confirms that interleaving marginalization and diffusion captures patterns at different resolutions. As shown in Table 26, a variant of the aggregate GEBM achieves the best rank on 7 of 8 ", "page_idx": 8}, {"type": "table", "img_path": "6vNPPtWH1Q/tmp/58db3a1258e65a190ffbe803064d2fd596d4fbbfbc7254e78358cbf4df94413b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 3: O.o.d.-detection AUC-ROC(\u2191) using different EBMs (best, runner-up). GEBM combines the beneftis of energy at different scales and ranks best over splits. ", "page_idx": 8}, {"type": "text", "text": "datasets. This shows that scale-awareness is the key ingredient for effective uncertainty estimation on graphs which can be further improved with energy regularization. In far-o.o.d. settings, this regularization is crucial to obtain reliable estimates as it mitigates overconfidence issues. ", "page_idx": 8}, {"type": "text", "text": "Energy Regularization. We also ablate the effect of regularizing the joint logit-based energy in Figure 3. At increasing distance from the training data, logit-based energy becomes overconfident. In contrast, the regularized $\\hat{E_{\\theta}}$ follows the logit-based energy near the training data and is dominated by the regularizer far away. This is reflected in the clear separation between perturbed and unperturbed nodes. When computing structure-aware energy, feature corruptions affect unperturbed nodes through the diffusion operation, making a clean separation difficult. This justifies our choice to compute structure-agnostic joint energy $\\hat{E}_{\\theta}({\\pmb x},{\\pmb y})$ and factor in the graph afterward by applying $P_{A}$ ", "page_idx": 8}, {"type": "text", "text": "Backbone Architecture. Our method can be applied post hoc to any logit-based GNN. Table 4 evaluates the o.o.d.- detection performance of our EBM framework using different commonly used GNN backbones: GCN [34], GAT (v2) [70, 8], GIN [80] and GraphSAGE [27]. Standard deviations and AUC-PR are reported in Appendix C.5. Over different distribution shifts, our proposed approach consistently outperforms the logit-based EBM and the graph-specific GNNSafe variation. This shows the broad applicability of our framework that enables reliable high-quality epistemic uncertainty estimation from a large family of logit-based GNNs. ", "page_idx": 8}, {"type": "table", "img_path": "6vNPPtWH1Q/tmp/4b6c09bd16f5f094c4a552f89a9d738ac39dfba5c91f5358665f197a29208bb7.jpg", "table_caption": [], "table_footnote": ["Table 4: O.o.d. detection AUC-ROC(\u2191) using different backbones. Our method is effective on all architectures. "], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "6 Limitations and Broader Impact ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Limitations. As GEBM is a post hoc epistemic estimator, it does not improve aleatoric uncertainty or its calibration. In particular, the GCN backbone used in this work does not consistently achieve the strongest performance in both tasks. While the structural scales that arise naturally from graph diffusion cover many distribution shifts as-is, specific applications may require augmenting GEBM with additional energy terms, as we also observe for centrality-based shifts. While GEBM enables robust evidential inference, future work may build upon its paradigm of aggregating different structural scales in the graph for fully evidential methods. Lastly, we study homophilic node classification problems and leave an extension of GEBM beyond this setting to future work. ", "page_idx": 9}, {"type": "text", "text": "Broader Impact. GEBM enables cheap and simple uncertainty quantification for GNNs which we believe to contribute to the development of more reliable AI. Nonetheless, we encourage practitioners to actively reevaluate our measure of uncertainty to mitigate risks in safety-critical domains. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We propose GEBM, a simple and efficient EBM for post-hoc epistemic uncertainty estimation for GNNs. To the best of our knowledge, we are the first to consider uncertainty at different structural scales and address this gap by proposing a model that aggregates energy that naturally arises from graph diffusion. It consistently outperforms existing approaches over an extensive suite of datasets at detecting various distribution shifts. We formally and empirically confirm that logit-based EBMs suffer from overconfidence and prove that the regularized GEBM mitigates this issue by inducing an integrable data density. We exploit this property by discovering a link to evidential methods that enables the backbone to provide accurate predictions even under severe distribution shifts. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We want to give special thank to Leo Schwinn and Franz Rieger for giving helpful suggestions on an early draft of the manuscript. The research presented has been performed in the frame of the RADELN project funded by TUM Georg Nemetschek Institute Artificial Intelligence for the Built World (GNI). It is further supported by the Bavarian Ministry of Economic Affairs, Regional Development and Energy with funds from the Hightech Agenda Bayern. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Moloud Abdar, Farhad Pourpanah, Sadiq Hussain, Dana Rezazadegan, Li Liu, Mohammad Ghavamzadeh, Paul Fieguth, Xiaochun Cao, Abbas Khosravi, U Rajendra Acharya, et al. A review of uncertainty quantification in deep learning: Techniques, applications and challenges. Information fusion, 76:243\u2013297, 2021.   \n[2] David H. Ackley, Geoffrey E. Hinton, and Terrence J. Sejnowski. A learning algorithm for boltzmann machines. Cognitive Science, 9(1):147\u2013169, 1985.   \n[3] Michael Arbel, Liang Zhou, and Arthur Gretton. Generalized energy based models. arXiv preprint arXiv:2003.05033, 2020.   \n[4] Sanghamitra Bandyopadhyay, Ujjwal Maulik, Lawrence B Holder, Diane J Cook, and Lise Getoor. Link-based classification. Advanced methods for knowledge discovery from complex data, pages 189\u2013207, 2005.   \n[5] William H. Beluch, Tim Genewein, Andreas Nurnberger, and Jan M. Kohler. The power of ensembles for active learning in image classification. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9368\u20139377, 2018.   \n[6] Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in neural networks, 2015.   \n[7] Glenn W Brier. Verification of forecasts expressed in terms of probability. Monthly weather review, 78(1):1\u20133, 1950.   \n[8] Shaked Brody, Uri Alon, and Eran Yahav. How attentive are graph attention networks? arXiv preprint arXiv:2105.14491, 2021. [9] Ben Chamberlain, James Rowbottom, Maria I Gorinova, Michael Bronstein, Stefan Webb, and Emanuele Rossi. Grand: Graph neural diffusion. In International Conference on Machine Learning, pages 1407\u20131418. PMLR, 2021.   \n[10] Bertrand Charpentier, Oliver Borchert, Daniel Z\u00fcgner, Simon Geisler, and Stephan G\u00fcnnemann. Natural posterior network: Deep bayesian uncertainty for exponential family distributions. arXiv preprint arXiv:2105.04471, 2021.   \n[11] Bertrand Charpentier, Daniel Z\u00fcgner, and Stephan G\u00fcnnemann. Posterior network: Uncertainty estimation without ood samples via density-based pseudo-counts. Advances in neural information processing systems, 33:1356\u20131367, 2020.   \n[12] Fan RK Chung. Spectral graph theory, volume 92. American Mathematical Soc., 1997.   \n[13] Stefan Depeweg, Jos\u00e9 Miguel Hern\u00e1ndez-Lobato, Finale Doshi-Velez, and Steffen Udluft. Learning and policy search in stochastic dynamical systems with bayesian neural networks. arXiv preprint arXiv:1605.07127, 2016.   \n[14] Yilun Du and Igor Mordatch. Implicit generation and modeling with energy based models. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.   \n[15] Michael Dusenberry, Ghassen Jerfel, Yeming Wen, Yian Ma, Jasper Snoek, Katherine Heller, Balaji Lakshminarayanan, and Dustin Tran. Efficient and scalable bayesian neural nets with rank-1 factors. In International conference on machine learning, pages 2782\u20132792. PMLR, 2020.   \n[16] Sven Elflein, Bertrand Charpentier, Daniel Z\u00fcgner, and Stephan G\u00fcnnemann. On out-ofdistribution detection with energy-based models, 2021.   \n[17] Derek Everett, Andre T Nguyen, Luke E Richards, and Edward Raff. Improving outof-distribution detection via epistemic uncertainty adversarial training. arXiv preprint arXiv:2209.03148, 2022.   \n[18] Sebastian Farquhar, Lewis Smith, and Yarin Gal. Liberty or depth: Deep bayesian neural nets do not need complex weight posterior approximations. Advances in Neural Information Processing Systems, 33:4346\u20134357, 2020.   \n[19] Matthias Fey and Jan E. Lenssen. Fast graph representation learning with PyTorch Geometric. In ICLR Workshop on Representation Learning on Graphs and Manifolds, 2019.   \n[20] Stanislav Fort, Jie Ren, and Balaji Lakshminarayanan. Exploring the limits of out-of-distribution detection. Advances in Neural Information Processing Systems, 34:7068\u20137081, 2021.   \n[21] Dominik Fuchsgruber, Tom Wollschl\u00e4ger, Bertrand Charpentier, Antonio Oroz, and Stephan G\u00fcnnemann. Uncertainty for active learning on graphs, 2024.   \n[22] Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty in deep learning, 2016.   \n[23] Johannes Gasteiger, Stefan Wei\u00dfenberger, and Stephan G\u00fcnnemann. Diffusion improves graph learning. Advances in neural information processing systems, 32, 2019.   \n[24] Jakob Gawlikowski, Cedrique Rovile Njieutcheu Tassi, Mohsin Ali, Jongseok Lee, Matthias Humt, Jianxiang Feng, Anna Kruspe, Rudolph Triebel, Peter Jung, Ribana Roscher, et al. A survey of uncertainty in deep neural networks. Artificial Intelligence Review, 56(Suppl 1):1513\u20131589, 2023.   \n[25] C Lee Giles, Kurt D Bollacker, and Steve Lawrence. Citeseer: An automatic citation indexing system. In Proceedings of the third ACM conference on Digital libraries, pages 89\u201398, 1998.   \n[26] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On calibration of modern neural networks. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 1321\u20131330. PMLR, 06\u201311 Aug 2017.   \n[27] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. Advances in neural information processing systems, 30, 2017.   \n[28] Matthias Hein, Maksym Andriushchenko, and Julian Bitterwolf. Why relu networks yield high-confidence predictions far away from the training data and how to mitigate the problem. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 41\u201350, 2019.   \n[29] Eyke H\u00fcllermeier and Willem Waegeman. Aleatoric and epistemic uncertainty in machine learning: An introduction to concepts and methods. Machine learning, 110(3):457\u2013506, 2021.   \n[30] Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, and Ondrej Chum. Label propagation for deep semi-supervised learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 5070\u20135079, 2019.   \n[31] Ajay J. Joshi, Fatih Murat Porikli, and Nikolaos Papanikolopoulos. Multi-class active learning for image classification. 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 2372\u20132379, 2009.   \n[32] Mira J\u00fcrgens, Nis Meinert, Viktor Bengs, Eyke H\u00fcllermeier, and Willem Waegeman. Is epistemic uncertainty faithfully represented by evidential deep learning methods?, 2024.   \n[33] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.   \n[34] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016.   \n[35] Marc Lafon, Cl\u00e9ment Rambour, and Nicolas Thome. Energy correction model in the feature space for out-of-distribution detection. arXiv preprint arXiv:2403.10403, 2024.   \n[36] Marc Lafon, Elias Ramzi, Cl\u00e9ment Rambour, and Nicolas Thome. Hybrid energy based model in the feature space for out-of-distribution detection, 2023.   \n[37] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles, 2017.   \n[38] Yann LeCun, Sumit Chopra, Raia Hadsell, M Ranzato, and Fujie Huang. A tutorial on energybased learning. Predicting structured data, 1(0), 2006.   \n[39] Jeremiah Liu, Zi Lin, Shreyas Padhy, Dustin Tran, Tania Bedrax Weiss, and Balaji Lakshminarayanan. Simple and principled uncertainty estimation with deterministic deep learning via distance awareness. Advances in neural information processing systems, 33:7498\u20137512, 2020.   \n[40] Weitang Liu, Xiaoyun Wang, John D. Owens, and Yixuan Li. Energy-based out-of-distribution detection, 2021.   \n[41] Zhao-Yang Liu, Shao-Yuan Li, Songcan Chen, Yao Hu, and Sheng-Jun Huang. Uncertainty aware graph gaussian process for semi-supervised learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 4957\u20134964, 2020.   \n[42] Owen Lockwood and Mei Si. A review of uncertainty for deep reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, volume 18, pages 155\u2013162, 2022.   \n[43] Andrey Malinin and Mark Gales. Predictive uncertainty estimation via prior networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018.   \n[44] Julian McAuley, Christopher Targett, Qinfeng Shi, and Anton Van Den Hengel. Image-based recommendations on styles and substitutes. In Proceedings of the 38th international ACM SIGIR conference on research and development in information retrieval, pages 43\u201352, 2015.   \n[45] Pablo Morales-Alvarez, Daniel Hern\u00e1ndez-Lobato, Rafael Molina, and Jos\u00e9 Miguel Hern\u00e1ndezLobato. Activation-level uncertainty in deep neural networks. In International Conference on Learning Representations, 2020.   \n[46] Jishnu Mukhoti, Andreas Kirsch, Joost van Amersfoort, Philip HS Torr, and Yarin Gal. Deep deterministic uncertainty: A simple baseline. arXiv preprint arXiv:2102.11582, 2021.   \n[47] Jishnu Mukhoti, Andreas Kirsch, Joost van Amersfoort, Philip H.S. Torr, and Yarin Gal. Deep deterministic uncertainty: A new simple baseline. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 24384\u201324394, June 2023.   \n[48] Jishnu Mukhoti, Joost van Amersfoort, Philip HS Torr, and Yarin Gal. Deep deterministic uncertainty for semantic segmentation. arXiv preprint arXiv:2111.00079, 2021.   \n[49] Sai Munikoti, Deepesh Agarwal, Laya Das, and Balasubramaniam Natarajan. A general framework for quantifying aleatoric and epistemic uncertainty in graph neural networks. Neurocomputing, 521:1\u201310, 2023.   \n[50] Mahdi Pakdaman Naeini, Gregory Cooper, and Milos Hauskrecht. Obtaining well calibrated probabilities using bayesian binning. In Proceedings of the AAAI conference on artificial intelligence, volume 29, 2015.   \n[51] Galileo Namata, Ben London, Lise Getoor, Bert Huang, and U Edu. Query-driven active surveying for collective classification. In 10th international workshop on mining and learning with graphs, volume 8, page 1, 2012.   \n[52] Yin Cheng Ng, Nicol\u00f2 Colombo, and Ricardo Silva. Bayesian semi-supervised learning with graph gaussian processes. Advances in Neural Information Processing Systems, 31, 2018.   \n[53] Yin Cheng Ng, Nicol\u00f2 Colombo, and Ricardo Silva. Bayesian semi-supervised learning with graph gaussian processes. Advances in Neural Information Processing Systems, 31, 2018.   \n[54] Vu-Linh Nguyen, S\u00e9bastien Destercke, and Eyke H\u00fcllermeier. Epistemic uncertainty sampling. In Discovery Science: 22nd International Conference, DS 2019, Split, Croatia, October 28\u201330, 2019, Proceedings 22, pages 72\u201386. Springer, 2019.   \n[55] CAZHAOw SALIH OAZA. Bayesian error bars for regression. 1996.   \n[56] Lawrence Page, Sergey Brin, Rajeev Motwani, Terry Winograd, et al. The pagerank citation ranking: Bringing order to the web. 1999.   \n[57] John Palowitch, Anton Tsitsulin, Brandon Mayer, and Bryan Perozzi. Graphworld: Fake graphs bring real insights for gnns. In Proceedings of the 28th ACM SIGKDD conference on knowledge discovery and data mining, pages 3691\u20133701, 2022.   \n[58] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. In NIPS-W, 2017.   \n[59] Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep graph convolutional networks on node classification. arXiv preprint arXiv:1907.10903, 2019.   \n[60] Adrian Schwaiger, Poulami Sinhamahapatra, Jens Gansloser, and Karsten Roscher. Is uncertainty quantification in deep learning sufficient for out-of-distribution detection? Aisafety@ ijcai, 54, 2020.   \n[61] Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina EliassiRad. Collective classification in network data. AI magazine, 29:93\u201393, 2008.   \n[62] Murat Sensoy, Lance Kaplan, and Melih Kandemir. Evidential deep learning to quantify classification uncertainty. Advances in neural information processing systems, 31, 2018.   \n[63] Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan G\u00fcnnemann. Pitfalls of graph neural network evaluation. arXiv preprint arXiv:1811.05868, 2018.   \n[64] Maximilian Stadler, Bertrand Charpentier, Simon Geisler, Daniel Z\u00fcgner, and Stephan G\u00fcnnemann. Graph posterior network: Bayesian predictive uncertainty for node classification. Advances in Neural Information Processing Systems, 34:18033\u201318048, 2021.   \n[65] Yiyou Sun, Yifei Ming, Xiaojin Zhu, and Yixuan Li. Out-of-distribution detection with deep nearest neighbors. In International Conference on Machine Learning, pages 20827\u201320840. PMLR, 2022.   \n[66] Jayaraman Thiagarajan, Rushil Anirudh, Vivek Sivaraman Narayanaswamy, and Timo Bremer. Single model uncertainty estimation via stochastic data centering. Advances in Neural Information Processing Systems, 35:8662\u20138674, 2022.   \n[67] Puja Trivedi, Mark Heimann, Rushil Anirudh, Danai Koutra, and Jayaraman J Thiagarajan. Accurate and scalable estimation of epistemic uncertainty for graph neural networks. arXiv preprint arXiv:2401.03350, 2024.   \n[68] Dennis Thomas Ulmer. A survey on evidential deep learning for single-pass uncertainty estimation. 2021.   \n[69] Joost Van Amersfoort, Lewis Smith, Andrew Jesson, Oscar Key, and Yarin Gal. On feature collapse and deep kernel learning for single forward pass uncertainty. arXiv preprint arXiv:2102.11409, 2021.   \n[70] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, Yoshua Bengio, et al. Graph attention networks. stat, 1050(20):10\u201348550, 2017.   \n[71] Fangxin Wang, Yuqing Liu, Kay Liu, Yibo Wang, Sourav Medya, and Philip S Yu. Uncertainty in graph neural networks: A survey. arXiv preprint arXiv:2403.07185, 2024.   \n[72] Guotai Wang, Wenqi Li, Michael Aertsen, Jan Deprest, S\u00e9bastien Ourselin, and Tom Vercauteren. Aleatoric uncertainty estimation with test-time augmentation for medical image segmentation with convolutional neural networks. Neurocomputing, 338:34\u201345, 2019.   \n[73] Guotai Wang, Wenqi Li, Michael Aertsen, Jan Deprest, Sebastien Ourselin, and Tom Vercauteren. Test-time augmentation with uncertainty estimation for deep learning-based medical image segmentation. 2022.   \n[74] Min Wang, Hao Yang, Jincai Huang, and Qing Cheng. Moderate message passing improves calibration: A universal way to mitigate confidence bias in graph neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 21681\u201321689, 2024.   \n[75] Xiao Wang, Hongrui Liu, Chuan Shi, and Cheng Yang. Be confident! towards trustworthy graph neural networks via confidence calibration, 2022.   \n[76] Max Welling and Yee Whye Teh. Bayesian learning via stochastic gradient langevin dynamics. In Proceedings of the 28th International Conference on International Conference on Machine Learning, ICML\u201911, page 681\u2013688, Madison, WI, USA, 2011. Omnipress.   \n[77] Yeming Wen, Dustin Tran, and Jimmy Ba. Batchensemble: an alternative approach to efficient ensemble and lifelong learning. arXiv preprint arXiv:2002.06715, 2020.   \n[78] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, et al. Huggingface\u2019s transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019.   \n[79] Qitian Wu, Yiting Chen, Chenxiao Yang, and Junchi Yan. Energy-based out-of-distribution detection for graph neural networks. arXiv preprint arXiv:2302.02914, 2023.   \n[80] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? arXiv preprint arXiv:1810.00826, 2018.   \n[81] Shuangfei Zhai, Yu Cheng, Weining Lu, and Zhongfei Zhang. Deep structured energy based models for anomaly detection. In Maria Florina Balcan and Kilian Q. Weinberger, editors, Proceedings of The 33rd International Conference on Machine Learning, volume 48 of $P r o$ - ceedings of Machine Learning Research, pages 1100\u20131109, New York, New York, USA, 20\u201322 Jun 2016. PMLR.   \n[82] Xujiang Zhao, Feng Chen, Shu Hu, and Jin-Hee Cho. Uncertainty aware semi-supervised learning on graph data. Advances in Neural Information Processing Systems, 33:12827\u201312836, 2020.   \n[83] Yin-Cong Zhi, Yin Cheng Ng, and Xiaowen Dong. Gaussian processes on graphs via spectral kernel learning. IEEE Transactions on Signal and Information Processing over Networks, 2023.   \n[84] Kaixiong Zhou, Xiao Huang, Daochen Zha, Rui Chen, Li Li, Soo-Hyun Choi, and Xia Hu. Dirichlet energy constrained learning for deep graph neural networks. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages 21834\u201321846. Curran Associates, Inc., 2021. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Proofs ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.1 EBMs for i.i.d. Data ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We first consider proofs for formal statements regarding logit-based EBMs in general (i.e. for i.i.d.   \ndata). ", "page_idx": 15}, {"type": "text", "text": "Proposition 4.1. Let $f_{\\theta}:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{C}$ be a piecewise affine function and $\\mathbb{R}^{h}=\\cup_{l}^{L}\\,Q_{l}$ be the disjoint set of polytopes on which $f_{\\theta}$ is affine, i.e. $f_{\\boldsymbol{\\theta}}(\\mathbf{x})=W^{(l)}\\mathbf{x}+b^{(l)}$ for $\\pmb{x}\\in Q_{l}$ . Assuming the direction of the rows of each $\\boldsymbol{W}^{(l)}$ to be uniformly distributed, the probability that $Z_{\\theta}$ converges decreases exponentially in the number of non-closed linear regions $L^{\\prime}$ and classes $C$ . ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathrm{Pr}\\left[Z_{\\theta}<\\infty\\right]\\approx(1/2)^{C\\cdot L^{\\prime}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. Consider an arbitrary direction $\\pmb{x}\\in\\mathbb{R}^{d}$ such that $\\|x\\|=1$ . As shown in [28] (Lemma 3.1), there exists some $t$ and $\\alpha_{0}$ such that for all $\\alpha\\,>\\,\\alpha_{0}$ it holds that $\\alpha\\pmb{x}\\in Q_{t}$ . We have $f_{\\boldsymbol{\\theta}}(\\alpha\\mathbf{x})\\,=$ $\\pmb{W}^{(l)}\\alpha\\pmb{x}+\\pmb{b}^{(l)}$ , and therefore: ", "page_idx": 15}, {"type": "equation", "text": "$$\n-E_{\\theta}(\\alpha{\\pmb x},y)={\\pmb W}_{y}^{(l)}\\alpha{\\pmb x}+{\\pmb b}_{y}^{(l)}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We now distinguish between two cases as $\\alpha\\to\\infty$ : ", "page_idx": 15}, {"type": "text", "text": "If $W_{y}^{(l)}x\\geq0$ , we have: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{\\alpha\\rightarrow\\infty}{\\mathrm{lim}}\\,\\alpha W_{y}^{(l)}x=\\infty\\qquad}\\\\ {\\underset{\\alpha\\rightarrow\\infty}{\\mathrm{lim}}\\,\\alpha W_{y}^{(l)}x+b_{y}^{(l)}=\\infty\\qquad}\\\\ {\\underset{\\alpha\\rightarrow\\infty}{\\mathrm{lim}}\\,E_{\\theta}(\\alpha\\pmb{x},y)=-\\infty}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "And conversely if $W_{y}^{(l)}x\\leq0$ , we have: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{\\alpha\\to\\infty}E_{\\theta}(\\alpha{\\pmb x},y)=\\infty\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Since we assumed the direction of $W_{y}^{(l)}$ to be uniformly distributed, both cases occur with probability 0.5. Now we consider the marginal energy assigned to $_{\\alpha x}$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\cal{E}}_{\\theta}(\\alpha{\\pmb x})=-\\log\\sum_{y}\\exp\\left(-E_{\\theta}(\\alpha{\\pmb x},y)\\right)}\\\\ {\\le\\alpha\\operatorname*{min}_{y}\\left\\{-E_{\\theta}({\\pmb x},y)\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We now analyze when the marginal energy $E_{\\theta}(\\alpha{\\pmb x})$ diverges toward $\\infty$ and $-\\infty$ respectively. If for any $y\\in\\{1,\\ldots,C\\}$ we have that $W_{y}^{(l)}x\\leq0$ , then: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\alpha\\rightarrow\\infty}{\\operatorname*{lim}}\\,E_{\\theta}(\\alpha\\pmb{x})\\leq\\underset{\\alpha\\rightarrow\\infty}{\\operatorname*{lim}}\\,\\alpha\\,\\underset{\\pmb{y}}{\\operatorname*{min}}\\,\\{E_{\\theta}(\\pmb{x},\\boldsymbol{y})\\}}\\\\ &{\\quad\\quad\\quad\\quad\\quad=-\\infty}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Since each $W_{y}^{(l)}x>0$ with probability $1/2$ , the limit diverges toward $-\\infty$ , with probability at most $(1/2)^{C}$ . ", "page_idx": 15}, {"type": "text", "text": "Now consider some open set $Q^{(l)}$ with a non-zero measure for over which the classifier $f_{\\theta}$ is linear.   \nWith probability at least $1/2$ , for $\\pmb{x}\\in Q^{(l)}$ , $f_{\\theta}$ diverges toward $\\infty$ . ", "page_idx": 15}, {"type": "text", "text": "We now can consider the integral over the unnormalized density implied by $f_{\\theta}$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle Z_{\\theta}=\\int_{\\mathbb{R}^{d}}\\exp(-E_{\\theta}({\\pmb x}))d{\\pmb x}}}\\\\ {{\\displaystyle~~~=\\sum_{l}\\int_{Q^{(l)}}\\exp(-E_{\\theta}({\\pmb x}))d{\\pmb x}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "For any open region $Q^{(l)}$ , the energy will diverge toward $-\\infty$ with probability $(1/2)^{C}$ and therefore: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\int_{Q^{(l)}}\\exp(-E_{\\theta}({\\pmb x}))d{\\pmb x}=\\infty\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "For $Z_{\\theta}$ to be finite and well-defined, the integral over all $L^{\\prime}$ open linear regions with non-zero measures need to converge simultaneously, each of which happens independently with probability $(1/2)^{C}$ . Therefore: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{Pr}\\left[Z_{\\theta}<\\infty\\right]=(1/2)^{C\\cdot L^{\\prime}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Remark. Note that the assumption that the rows of each $W^{(l)}$ have a uniformly distributed direction is given when initializing the model from a normal distribution. When training a classifier with the commonly used cross-entropy objective, the training incentivizes the model to assign high logits to exactly one of the classes for each datapoint. Therefore, the model is intuitively encouraged to have at least one row of $W^{(l)}$ for which the $\\textbf{\\em x}$ that are in the corresponding region $Q^{(l)}$ have positive projection W y(l )x $W_{y}^{(l)}x>0$ . In practice, we expect that for a trained model the probability of any $\\textbf{\\em x}$ to have a negative projection onto all rows of the corresponding $W^{(l)}$ to be lower than $(1/2)^{C}$ . ", "page_idx": 16}, {"type": "text", "text": "Theorem 4.2. For a piecewise affine classifier $f_{\\theta}$ as in Proposition 4.1, $\\hat{p_{\\theta}}$ is well-defined. ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\hat{Z}_{\\theta}=\\int_{\\mathcal{X}}\\sum_{y}\\exp(-E_{\\theta}(\\pmb{x},y))\\mathcal{N}(f_{\\theta}(\\pmb{x})\\mid\\mu_{y},\\Sigma_{y})^{\\gamma}d\\pmb{x}<\\infty\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. We consider the corrected joint energy $\\hat{E}_{\\theta}(\\alpha{\\pmb x},y)$ for some sufficiently large $\\alpha>\\alpha_{0}$ and an arbitrary direction $\\pmb{x}\\in\\mathbb{R}^{d}$ . ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{E}_{\\theta}(\\alpha x,y)=E_{\\theta}(\\alpha x,y)-\\gamma\\log\\mathcal{N}(f_{\\theta}(\\alpha x)_{y}\\mid\\mu_{y},\\Sigma_{y})}\\\\ &{\\quad\\quad\\quad\\quad=-f_{\\theta}(\\alpha x)_{y}+\\frac{\\gamma}{2}(f_{\\theta}(\\alpha x)_{y}-\\mu_{y})^{T}\\Sigma_{y}^{-1}(f_{\\theta}(\\alpha x)_{y}-\\mu_{y})+\\mathrm{const}}\\\\ &{\\quad\\quad\\quad\\quad=-W_{y}^{(l)}\\alpha x-b_{y}^{(l)}+\\frac{\\gamma}{2}(W_{y}^{(l)}\\alpha x+b_{y}^{(l)}-\\mu_{y})^{T}\\Sigma_{y}^{-1}(W_{y}^{(l)}\\alpha x+b_{y}^{(l)}-\\mu_{y})+\\mathrm{const}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "For large enough $\\alpha_{0}$ , the terms quadratic in $_{\\alpha x}$ will dominate linear terms and we can write for some $c>0$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{E}_{\\theta}(\\alpha\\pmb{x},y)\\geq c\\ast(\\pmb{W}_{y}^{(l)}\\alpha\\pmb{x})^{T}\\pmb{\\Sigma}_{y}^{-1}(\\pmb{W}_{y}^{(l)}\\alpha\\pmb{x})}\\\\ &{=\\alpha^{2}(\\sqrt{c}\\pmb{W}_{y}^{(l)}\\pmb{x})^{T}\\pmb{\\Sigma}_{y}^{-1}(\\sqrt{c}\\pmb{W}_{y}^{(l)}\\pmb{x})}\\\\ &{\\geq\\pmb{x}^{T}\\pmb{\\Sigma}_{\\ast}^{-1}\\pmb{x}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Here, we pick $\\Sigma_{*}$ such that the equation holds for all $y\\in\\{1,\\ldots,C)$ . Note that this means that in the limit, each joint energy term is bounded by the logarithm of the same Gaussian $\\log\\mathcal{N}(\\mathbf{0},\\pmb{\\Sigma}_{*})$ . For the marginal energy it holds: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{E}_{\\theta}(\\alpha\\alpha)=-\\log\\displaystyle\\sum_{y}\\exp\\left(-\\hat{E}_{\\theta}(\\alpha x,y)\\right)}\\\\ &{\\quad\\quad\\quad\\geq-\\log\\displaystyle\\sum_{y}\\exp\\left(-\\alpha x^{T}\\Sigma_{*}^{-1}\\alpha x\\right)}\\\\ &{\\quad\\quad\\quad=\\alpha x^{T}\\Sigma_{*}^{-1}\\alpha x-\\log C}\\\\ &{\\quad\\quad\\quad\\geq\\alpha x^{T}\\Sigma_{*}^{-1}\\alpha x}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Here, we absorb the constant $\\log C$ into the quadratic term. Similar to the proof of Proposition 4.1, we now analyze the integral of the implied density for each linear region of the classifier $Q^{(l)}$ . ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\int_{Q^{(l)}}\\exp\\left(-\\hat{E}_{\\theta}(x)\\right)d x=\\int_{\\|x\\|_{2}\\leq\\alpha_{0}}\\exp\\left(-\\hat{E}_{\\theta}(x)\\right)d x+\\int_{\\|x\\|_{2}>\\alpha_{0}}\\exp\\left(-\\hat{E}_{\\theta}(x)\\right)d x\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The first term is an integral of a finite function over a finite region, which therefore will also be finite: ", "page_idx": 17}, {"type": "equation", "text": "$$\nI_{\\alpha\\leq\\alpha_{0}}^{(l)}:=\\int_{\\|\\pmb{x}\\|_{2}\\leq\\alpha_{0}}\\exp\\left(-\\hat{E}_{\\theta}(\\pmb{x})\\right)d\\pmb{x}<\\infty\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "For the second term, we notice that it is bounded by the (finite) Gaussian integral: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I_{\\alpha>\\alpha_{0}}^{(l)}:=\\displaystyle\\int_{\\|\\alpha\\|_{2}^{(l)}>\\alpha_{0}}\\exp\\left(-\\hat{E}_{\\theta}(\\pmb{x})\\right)d\\pmb{x}}\\\\ &{\\quad\\quad\\quad\\leq\\displaystyle\\int_{\\|\\pmb{x}\\|_{2}^{(l)}>\\alpha_{0}}\\exp\\left(-\\pmb{x}^{T}\\pmb{\\Sigma}_{\\ast}^{\\prime-1}\\pmb{x}\\right)d\\pmb{x}}\\\\ &{\\quad\\quad\\quad\\leq\\infty}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Lastly, since we can decompose $\\mathbf{R}^{d}$ into a finite set of linear regions over each of which the integral is finite, the entire integral is finite. ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\int_{\\mathbb{R}^{d}}\\exp\\left(-\\hat{E}_{\\theta}(\\mathbf{\\boldsymbol{x}})\\right)d\\mathbf{\\boldsymbol{x}}=\\sum_{l=1}^{L}\\int_{Q^{(l)}}\\exp\\left(-\\hat{E}_{\\theta}(\\mathbf{\\boldsymbol{x}})\\right)d\\mathbf{\\boldsymbol{x}}}}\\\\ &{\\qquad\\qquad\\qquad=\\displaystyle\\sum_{l=1}^{L}\\left(I_{\\alpha\\le\\alpha_{0}}^{(l)}+I_{\\alpha>\\alpha_{0}}^{(l)}\\right)}\\\\ &{\\le\\infty}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Theorem A.1.1. For a piecewise affine classifier $f_{\\theta}$ as in Proposition 4.1, and a piecewise affine feature extractor $g_{\\theta}(\\pmb{x}),\\,\\hat{p_{\\theta}}$ is integrable. ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\hat{Z}_{\\theta}=\\int_{\\mathcal{X}}\\sum_{y}\\exp(-E_{\\theta}(\\pmb{x},y))\\mathcal{N}(g_{\\theta}(\\pmb{x})\\mid\\mu_{y},\\Sigma_{y})^{\\gamma}d\\pmb{x}<\\infty\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. Note that both affine functions $f_{\\theta}$ and $g_{\\boldsymbol{\\theta}}$ partition the input space $\\mathbb{R}^{d}$ into a finite set of linear regions. Intersecting their boundaries again gives rise to a finite set of linear regions. For bounded ", "page_idx": 17}, {"type": "text", "text": "regions, the integral over the induced density will be finite, so we discuss the unbounded regions here.   \nFor any such region $Q^{(l)}$ , we have $f_{\\boldsymbol\\theta}(\\mathbf{x})={\\pmb w}_{f}^{T}{\\pmb x}+{\\pmb b}_{f}$ and $g_{\\boldsymbol\\theta}(\\mathbf x)=W_{g}\\mathbf x+b_{g}$ . ", "page_idx": 18}, {"type": "text", "text": "We now partition the $\\mathbb{R}^{d}\\cap Q^{(l)}$ into four regions using the kernel spaces of $w_{f}$ and $W_{g}$ respectively, i.e. $\\mathbb{R}^{d}\\cap Q^{(l)}=\\mathbf{0}_{f+g}\\cup\\mathbf{0}_{f-g}\\cup\\mathbf{0}_{g-f}\\cup I_{f+g}.$ , where we define $\\mathbf{0}_{f+g}:=\\ker(\\mathbf{\\boldsymbol{w}}_{f})\\cap\\ker(\\mathbf{\\boldsymbol{W}}_{g})\\cap Q^{(l)}$ , $\\mathbf{0}_{f-g}:=\\ker(\\mathbf{\\boldsymbol{w}}_{f})\\setminus\\ker(\\mathbf{\\boldsymbol{W}}_{g})\\cap Q^{(l)}$ , $\\mathbf{0}_{g-f}:=\\ker(W_{g})\\setminus\\ker(\\pmb{w}_{f})\\cap Q^{(l)}$ and $I_{f+g}:=\\mathrm{im}(\\pmb{w}_{f})\\cap$ $\\mathrm{im}({\\pmb W}_{g})\\cap Q^{(l)}$ . ", "page_idx": 18}, {"type": "text", "text": "We can now decompose the integral over $Q^{(l)}$ into integrals over all four regions. Note that of these sets, only $\\pmb{I}_{f+g}$ has a non-zero measure (if the affine classifier is not the null function). Therefore, we only have to focus on this domain. For some $\\alpha_{0}$ and $\\pmb{x}\\in I_{f+g},\\|\\pmb{x}\\|_{2}\\geq\\alpha_{0}$ , we have: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{p}_{\\theta}(x)\\propto\\exp\\left(-E_{\\theta}(x,y)\\right)\\mathcal{N}(g_{\\theta}(x)\\mid\\mu_{y},\\Sigma_{y})}\\\\ &{\\quad\\quad\\quad\\propto\\exp\\left(w_{f}^{T}x+b_{f}\\right)\\exp\\left(-(W_{g}x+b_{g})^{T}\\Sigma_{g}^{-1}(W_{g}x+b_{g})\\right)}\\\\ &{\\quad\\quad\\quad\\leq\\exp\\left(-x^{T}\\Sigma_{*,y}^{-1}x\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "For some $\\Sigma_{*,y}^{-1}$ that bounds the quadratic function in the exponential for $\\|\\pmb{x}\\|_{2}\\geq\\alpha_{0}$ . The integral over the region $Q^{(l)}$ now reduces to: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I^{(l)}=\\displaystyle\\int_{\\alpha\\in Q^{(l)}}\\sum_{y}\\cos\\mathfrak{c o n s t}\\ast\\exp{(-E_{\\theta}(x,y))}\\mathcal{N}(g_{\\theta}(x)\\mid\\mu_{y},\\Sigma_{y})d x}\\\\ &{\\quad=\\displaystyle\\mathrm{const}+\\int_{\\|x\\|_{2}^{\\alpha}\\geq\\alpha_{0}}\\sum_{y}\\mathrm{const}\\ast\\exp{(-E_{\\theta}(x,y))}\\mathcal{N}(g_{\\theta}(x)\\mid\\mu_{y},\\Sigma_{y})d x}\\\\ &{\\quad\\leq\\displaystyle\\mathrm{const}+\\int_{\\|x\\|_{2}^{\\alpha(l)}\\geq\\alpha_{0}}\\sum_{y}\\mathrm{const}\\ast\\exp{(-x^{T}\\Sigma_{*,y}^{-1}x)}\\,d x}\\\\ &{\\quad\\leq\\displaystyle\\mathrm{const}+\\int_{\\|x\\|_{2}^{\\alpha(l)}}\\sum_{0=0}\\mathrm{const}\\ast\\exp{(-x^{T}\\Sigma_{*}^{-1}x)}\\,d x}\\\\ &{\\quad<\\infty}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Here, we pick $\\Sigma_{*}$ to bound the quadratic terms of all $\\Sigma_{y,*}$ . As previously mentioned, the kernel spaces of $w_{f}$ and $W_{g}$ have zero measure and therefore do not contribute to the integral. As previously, the integral over the entire domain $\\mathbb{R}^{d}$ decomposes into finite integrals over all $Q^{(l)}$ . ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\int_{\\mathbb{R}}\\sum_{y}\\operatorname{const}*\\exp\\left(-E_{\\theta}(x,y)\\right){\\mathcal N}(g_{\\theta}(x)\\mid\\mu_{y},\\Sigma_{y})d x=\\operatorname{const}+\\sum_{l=1}^{L}I^{(l)}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Corollary 4.3. For a piecewise affine classifier $f_{\\theta}$ as in Proposition 4.1, and any $\\pmb{x}\\in\\mathbb{R}^{d}$ almost surely: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{\\alpha\\to\\infty}\\hat{p_{\\theta}}(\\alpha{\\pmb x})=0\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. As per the proof of Theorem 4.2, for sufficiently large $\\alpha$ we have for some $\\pmb{\\Sigma}_{*}^{-1}$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\hat{E}_{\\boldsymbol{\\theta}}(\\alpha\\mathbf{x})\\geq\\mathbf{x}^{T}\\Sigma_{*}^{-1}\\mathbf{x}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "From there, it follows directly that: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\alpha\\rightarrow\\infty}{\\mathrm{lim}}\\,\\hat{p}_{\\theta}(\\alpha\\pmb{x})=\\underset{\\alpha\\rightarrow\\infty}{\\mathrm{lim}}\\exp(-\\hat{E}_{\\theta}(\\alpha\\pmb{x}))}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\exp\\left(\\underset{\\alpha\\rightarrow\\infty}{\\mathrm{lim}}-\\hat{E}_{\\theta}(\\alpha\\pmb{x})\\right)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\leq\\exp\\left(\\underset{\\alpha\\rightarrow\\infty}{\\mathrm{lim}}-\\pmb{x}^{T}\\pmb{\\Sigma}_{\\ast}^{-1}\\pmb{x}\\right)}\\\\ &{\\quad\\quad\\quad\\quad=0}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "A.2 Graph-based Energies at Different Scales ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Here, we show that local energy (Equation (5)) and group energy (Equation (6)) induce a valid probability density when using a linear diffusion operator $P_{A}$ and regularized energy according to Equation (8). Note that independent energy (Equation (4)) is shown to induce a well-defined probability density as it is just a graph-independent regularized energy. ", "page_idx": 19}, {"type": "text", "text": "Proposition A.2.1. For a linear diffusion operator $P_{\\mathbf{A}}(\\mathbf{x})=\\alpha\\mathbf{x}+c o n s t,\\,\\alpha>0$ and the regularized energy $\\hat{E}_{\\theta}(x,y)$ , the local energy $\\hat{E}_{L}({\\pmb x})$ induces a well-defined density: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\int_{\\mathcal{X}}\\exp(-\\hat{E}_{\\theta,L}(\\pmb{x}))d\\pmb{x}<\\infty\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{E}_{\\theta,L}(x)=-\\log\\displaystyle\\sum_{y}\\exp\\Big(P_{A}\\left(-\\hat{E}_{\\theta}(x,y)\\right)\\Big)}\\\\ &{\\qquad=-\\log\\displaystyle\\sum_{y}\\exp\\Big(-\\alpha\\hat{E}_{\\theta}(x,y)+\\mathrm{const}\\Big)}\\\\ &{\\qquad\\qquad\\geq-\\log\\displaystyle\\sum_{y}\\exp\\left(-\\alpha x^{T}\\Sigma^{-1}x+\\mathrm{const}\\right)}\\\\ &{\\qquad=\\alpha x^{T}\\Sigma^{-1}x+\\mathrm{const}+\\log C}\\\\ &{\\qquad\\qquad\\geq x^{T}\\Sigma^{-1}x}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Again, we absorbed the constant terms as well as $\\alpha$ into the quadratic term for large enough $\\|{\\pmb x}\\|_{2}$ and used the quadratic bound from Theorem 4.2. From here, it is straightforward that: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\int_{\\mathcal{X}}\\exp\\left(-\\hat{E}_{\\theta,L}(\\pmb{x})\\right)d\\pmb{x}\\leq\\mathrm{const}+\\int_{\\mathcal{X}}\\exp\\left(-\\pmb{x}^{T}\\pmb{\\Sigma}_{\\ast}^{\\prime-1}\\pmb{x}\\right)d\\pmb{x}<\\infty\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proposition A.2.2. For a linear diffustion operator $P_{\\mathbf{A}}(\\mathbf{x})=\\alpha\\mathbf{x}+c o n s t,\\,\\alpha>0$ and the regularized energy $\\hat{E}_{\\theta}(x,y)$ , the group energy $\\hat{E}_{G}(x)$ induces a well-defined density: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\int_{\\mathcal{X}}\\exp(-\\hat{E}_{\\theta,G}(\\pmb{x}))d\\pmb{x}<\\infty\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{E}_{\\boldsymbol{\\theta},G}(\\boldsymbol{x})=P_{A}\\left(\\hat{E}_{\\boldsymbol{\\theta}}(\\boldsymbol{x})\\right)}\\\\ &{\\qquad\\qquad=\\alpha\\hat{E}_{\\boldsymbol{\\theta}}(\\boldsymbol{x})+\\mathrm{const}}\\\\ &{\\qquad\\qquad\\geq\\alpha\\boldsymbol{x}^{T}\\Sigma_{*}^{-1}\\boldsymbol{x}+\\mathrm{const}}\\\\ &{\\qquad\\qquad\\geq\\boldsymbol{x}^{T}{\\Sigma^{\\prime}}_{*}^{-1}\\boldsymbol{x}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Again, we have used the quadratic bound of Theorem 4.2 and absorbed $\\alpha>0$ for large enough $\\|\\pmb{x}\\|_{2}$ . From there, it follows: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\int_{\\mathcal{X}}\\exp\\left(-\\hat{E}_{\\theta,G}(\\pmb{x})\\right)d\\pmb{x}\\leq\\mathrm{const}+\\int_{\\mathcal{X}}\\exp\\left(-\\pmb{x}^{T}\\pmb{\\Sigma}_{\\ast}^{\\prime-1}\\pmb{x}\\right)d\\pmb{x}<\\infty\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Remark. We want to point out that most diffusion operations, especially the ones discussed in Appendix C.6 including the label-propagation smoothing used in our experiments, are of the form $\\bar{P_{A}}(x)=\\alpha{\\pmb x}+$ const with $\\alpha>0$ . Since we only integrate over the features of a single node and keep all other features fixed, all of these diffusion processes can be expressed as a weighted sum over nodes in the graph. Since we consider unweighted graphs, there are no negative edge weights which ensures that $\\alpha>0$ (as long as some sort of self-loop is included). Therefore, linear diffusion processes can be seen as positive affine transformations. ", "page_idx": 20}, {"type": "text", "text": "Theorem 4.4. For a linear diffusion operator $P_{\\mathbf{{A}}}(\\mathbf{x})=\\alpha\\mathbf{{x}}+c o n s t,\\,\\alpha>$ and the regularized energy $\\hat{E}_{\\theta}(x,y)$ , GEBM induces a well-defined density: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\int_{\\mathcal{X}}\\exp\\left(-\\hat{E}_{\\theta,G E B M}\\left(\\mathbf{x}\\right)\\right)d\\mathbf{x}<\\infty\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. We previously established quadratic bounds on all three constituents of the aggregate energy for large enough $\\|\\pmb{x}\\|_{2}$ : ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{E}_{\\theta,I}(\\pmb{x})\\geq\\pmb{x}^{T}\\pmb{\\Sigma}_{I}^{-1}\\pmb{x}}\\\\ {\\hat{E}_{\\theta,L}(\\pmb{x})\\geq\\pmb{x}^{T}\\pmb{\\Sigma}_{L}^{-1}\\pmb{x}}\\\\ {\\hat{E}_{\\theta,G}(\\pmb{x})\\geq\\pmb{x}^{T}\\pmb{\\Sigma}_{G}^{-1}\\pmb{x}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Hence, there must be one $\\mathbf{\\Sigma}_{*}^{-1}\\in\\{\\Sigma_{I}^{-1},\\Sigma_{L}^{-1},\\Sigma_{G}^{-1}\\}$ that bounds all three energy types. We now look at the GEBM model: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{E}_{\\theta,\\mathrm{GEBM}}(\\pmb{x})=\\log\\left(\\exp\\left(\\hat{E}_{\\theta,I}(\\pmb{x})\\right)+\\exp\\left(\\hat{E}_{\\theta,L}(\\pmb{x})\\right)+\\exp\\left(\\hat{E}_{\\theta,G}(\\pmb{x})\\right)\\right)}\\\\ &{\\quad\\quad\\quad\\quad\\geq\\log\\left(3*\\exp\\left(\\pmb{x}^{T}\\pmb{\\Sigma}_{*}^{-1}\\pmb{x}\\right)\\right)}\\\\ &{\\quad\\quad\\quad=\\pmb{x}^{T}\\pmb{\\Sigma}_{*}^{-1}\\pmb{x}+\\log3}\\\\ &{\\quad\\quad\\quad\\geq\\pmb{x}^{T}\\pmb{\\Sigma}_{*}^{\\prime\\,-1}\\pmb{x}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "As before, this bounds the density induced by GEBM: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\int_{\\mathcal{X}}\\exp\\left(-\\hat{E}_{\\theta,\\mathrm{GEBM}}\\left(x\\right)\\right)d x\\leq\\mathrm{const}+\\int_{\\mathcal{X}}\\exp\\left(-x^{T}{\\Sigma^{\\prime}}_{*}^{-1}x\\right)d x<\\infty\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Remark. This directly imposes a restriction on which energies can be included in GEBM beyond the three naturally arising graph-specific terms we propose: As long as the energy term can be bounded by an energy that grows fast enough to ensure convergence, our framework accommodates it. ", "page_idx": 20}, {"type": "text", "text": "We also can explicitly write out the (unnormalized) GEBM density $p_{\\theta,\\mathrm{GEBM}}(\\mathbf{\\emx})$ in terms of its constituents $p_{\\theta,I}(\\pmb{x}),\\,\\stackrel{\\cdot}{p_{\\theta,L}}(\\pmb{x})$ and $p_{\\theta,G}(x)$ . ", "page_idx": 20}, {"type": "equation", "text": "$$\np_{\\theta,\\mathrm{GEBM}}(\\pmb{x})\\propto\\frac{1}{p_{\\theta,I}(\\pmb{x})^{-1}+p_{\\theta,L}(\\pmb{x})^{-1}+p_{\\theta,G}(\\pmb{x})^{-1}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "B Experimental Setup ", "text_level": 1, "page_idx": 21}, {"type": "table", "img_path": "6vNPPtWH1Q/tmp/bcc8f1f877ebe3df3d8c4da6236767cc3856d03e6a10a2484f6d055d78995871.jpg", "table_caption": ["B.1 Datasets and Distribution Shifts "], "table_footnote": ["Table 5: Statistics about the datasets used in this work. "], "page_idx": 21}, {"type": "text", "text": "We use eight datasets in this work that we expose to similar kinds of distribution shifts. For CoraML, we also create a version that uses continuous word embeddings instead of categorical bag-of-word features. We use the all-MiniLM-L6- $_{\\cdot\\nu2}$ sentence transformer provided by Hugging Face [78] to embed the abstracts that are provided for each paper (node) in the citation network. All datasets are taken from PyTorch Geometric [19]. They are licensed as C.C.0 1.0 (CoraML, CoraML LLM), C.C. Attribution-NonCommercial-Share Alike 3.0 (Citeseer), OdbL 1.0 (PubMed). ", "page_idx": 21}, {"type": "text", "text": "We expose each dataset to the same distribution shifts which can be categorized into three families: ", "page_idx": 21}, {"type": "text", "text": "(i) Leave-out-Classes. We pre-select a subset of classes and designate them as out-of-distribution. That is, we remove them from the training set and train the GNN on the remaining set of nodes. We focus on an inductive setting, where we also remove all edges linking o.o.d. left-out-class nodes to the training graph, thus preventing information leakage. In the transductive setting, o.o.d. nodes still contribute to the training signal as they may be connected to i.d. nodes. This distribution shift can be seen as occurring on a cluster level, as we introduce anomalous nodes that are likely to cluster together. We distinguish between two kinds of selection processes for the classes to be left out: We either pick the last classes in the dataset $(L o C\\,(l a s t))$ or choose classes with the most heterophilic connection pattern, which distinguishes them even further from i.d. classes (LoC (hetero.)). ", "page_idx": 21}, {"type": "text", "text": "(ii) Feature Perturbations. We select $50\\%$ of nodes at random to be o.o.d. and perturb their features by replacing them with random noise. We distinguish between three perturbation types that control the similarity of node features to training data: We generate features that are similar to i.d. data (near-o.o.d. in the following way: For each of the $d$ features, we compute how frequent it occurs in the dataset. We then proceed to sample each of these features independently with the corresponding success probability $\\hat{\\pmb{p}}$ from a Bernoulli $\\mathbf{Ber}(\\hat{\\pmb{p}})$ . Instead, we can also set the success probability to $p=0.5$ for each of the features and induce a more severe distribution shift within the bag-of-words domain of the dataset. Lastly, when sampling features according to a normal distribution ${\\mathcal{N}}(0,1)$ , we generate out-of-domain data that should, in theory, be easy to detect far-o.o.d.. For the CoraML-LM and the PubMed datasets, features a not bag-of-word. Therefore, the near-o.o.d shift is omitted and the far-o.o.d. shift needs to be considered within the domain of the data. ", "page_idx": 21}, {"type": "text", "text": "(iii) Structural. We induce structure-related shifts in two ways: The first option we consider is to rank all nodes according to their local homophily. That is, we compute the ratio of neighbors with the same class: $h_{i}\\ {\\overset{}{=}}\\ |\\{v_{j}\\in{\\mathcal{N}}_{i}:{\\pmb y}_{i}={\\pmb y}_{j}\\}|/|{\\mathcal{N}}_{i}|$ . We then designate $50\\%$ of nodes with the highest heterophily (i.e. lowest homophily) as o.o.d (homophily). The second structural shift ranks nodes according to their (approximate) Page Rank centrality and declares nodes with low values as o.o.d. (Page Rank). ", "page_idx": 21}, {"type": "text", "text": "In the inductive setting, we remove the o.o.d. nodes from the training graph and re-introduce them during inference. We evaluate o.o.d. detection metrics on a validation/test set that includes both i.d. and o.o.d. nodes. While the training and validation set are randomized for each split, the test set is shared across all splits to prevent data leakage. All results are reported over five different splits and five independent model weight initializations for each of them. Where appropriate, we also report standard deviations in Appendix C. ", "page_idx": 21}, {"type": "text", "text": "Remarks regarding Page Rank Shifts. We study the centrality-based shift (which heavily correlates with a degree-based shift) for completeness reasons as a similar generation process is used in [79]. However, to which extent methods should be expected to assign high uncertainty to low-degree nodes is questionable: First, centrality and/or degree are feature-irrespective quantities. In contrast to homophily (nodes of similar classes have similar features), the confidence of a classifier may not (and should not) depend on the node degree/centrality. Furthermore, many baselines use symmetric normalization (Appendix C.6) that inherently favors high-degree nodes. Therefore, an evaluation regarding node degree/centrality as a distribution shift will not test the quality of an uncertainty measure but instead favor all models that use appropriate diffusion processes for most baselines. Regardless of our concerns regarding a centrality-based shift, we report results regarding that shift as well in Appendix C. Note that while GEBM is not among the best-performing estimators in this setting, it still achieves the highest average o.o.d. detection rank overall, indicating that its merits in all other settings heavily outweigh its performance on the Page Rank shift. ", "page_idx": 22}, {"type": "text", "text": "B.2 Models and Training ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "At the backbone of all models, we use the same GCN [34] architecture if not specified explicitly otherwise. We use one hidden layer of dimension 64, symmetric normalization Appendix C.6, and add self-loops to the undirected (symmetric) adjacency matrix. We use ReLU nonlinearities, enable the bias term, and use dropout at $p=0.5$ . ", "page_idx": 22}, {"type": "text", "text": "All models are trained with the ADAM optimizer [33] with a learning rate of $10^{-3}$ , weight decay of $10^{-4}$ , and a cross-entropy objective. We use early stopping on the validation loss with a patience of 50, an absolute improvement threshold of $10^{-1}$ , and select the model with the best validation loss. We implement our models in PyTorch [58] and PyTorch Geometric [19] and train on two types of machines: (i) Xeon E5-2630 v4 CPU $\\textcircled{\\omega}2.20\\mathrm{GHz}$ with a NVIDA GTX 1080TI GPU and $128\\:\\mathrm{GB}$ of RAM. (ii) AMD EPYC 7543 CPU $\\textcircled{a}2.80\\mathrm{GHz}$ with a NVIDA A100 GPU and 128 GB of RAM . ", "page_idx": 22}, {"type": "text", "text": "As the GCN backbone used in our experiments is lightweight, model training finishes within a few minutes and VRAM consumption is dominated by the datasets. Our post hoc method can be fitted and evaluated in negligible time $(<\\!1\\mathrm{s})$ . ", "page_idx": 22}, {"type": "text", "text": "For MC-Dropout [22], we use a dropout probability of $p=0.5$ which we also use for DropEdge [59]. At inference, we evaluate 50 samples to compute uncertainty. For ensembles, we train 10 backbones from different weight initializations independently. The Bayesian GNN [6, 15, 18] also is evaluated with 50 samples during inference and uses a KL-loss with weight $10^{-1}$ and is trained with learning rate $10^{-2}$ and no weight-decay. We parametrize the weight distribution using a log transform and initialize the mean and log scale to 1.0 and $-3.0$ respectively. Weight distributions are regularized to follow a standard normal. ", "page_idx": 22}, {"type": "text", "text": "For GPN [64], we follow the hyperparameters suggested by the authors and use warmup training on the normalizing flow for 5 epochs with a learning rate of $1e-2$ , no weight decay during joint training and $1e-2$ during warmup. The flow dimension is 16 and is composed of 10 radial layers. The cross-entropy regularization weight is $10^{-4}$ . We use Page Rank propagation for 10 iterations at a teleport probability of 0.1. ", "page_idx": 22}, {"type": "text", "text": "For SGCN [82], we use the same GCN backbone and a GDK-prior at cutoff distance 10 and scale $\\sigma=1.0$ . For teacher training, we use a learning rate of $10^{-2}$ , weight decay of $5*10^{-4}$ and a KL-loss weight of $10^{-1}$ . ", "page_idx": 22}, {"type": "text", "text": "For HEAT [36], we use the hyperparameters suggested by the authors. In contrast to their study on the image domain, we do not have features with spatial extent and can not use standard deviation pooling on the volume. We set the temperature parameter of the combined HEAT model to $-1$ , following the suggestion of the authors. Similar to their EBM backbone, we imitate the structure of the classifier and use a 2-layer MLP with a hidden dimension of 64 akin to the GCN backbone. ", "page_idx": 22}, {"type": "text", "text": "Our GEBM framework regularizes logit-based joint energy at a strength $\\gamma$ . For downstream tasks, this parameter can, in general, be tuned. We find that an equal weighting of predictive energy and regularization performs well: We choose $\\gamma$ such that the $95\\%$ quantiles of both the training logits and the representations on which the Gaussian density model is fit are in the same range. Furthermore, recent work on deterministic uncertainty argues that density-based epistemic uncertainty should not be estimated directly from the logits [46]. We follow this advice and do not fit and evaluate the regularizer on the output layer (logits) of the GNN but instead on its penultimate layer representations. Note that for the ReLU networks we use, all formal proofs still hold in this setting as the penultimate representation of a node $v$ is also described by a piecewise affine function [28] (see A.1.1. As a smoothing operator $P_{A}$ , we employ label-propagation smoothing with $\\alpha=0.5$ for $t=10$ iterations. Lastly, we found that aggregating the energy terms at different scales (Equation (9)) using a sum operation instead of logsumexp to perform better in practice and use it for our experiments. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "In our ablation regarding different model backbones, we use the default hyperparameters from the GCN backbone (e.g. number of hidden layers). For each model additional hyperparameters are set to the following values: For GATv2 [70, 8] we use 8 heads and use summation to aggregate them. For SAGE [27], we use no normalization at the layers. ", "page_idx": 23}, {"type": "text", "text": "B.3 Uncertainty Evaluation ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "For sampling-based approaches, we compute aleatoric and epistemic uncertainty as entropy and mutual information respectively according to [24]: ", "page_idx": 23}, {"type": "equation", "text": "$$\nu^{\\mathrm{alea}}=\\mathbb{E}_{\\theta\\sim p(\\theta|\\mathcal{D})}\\left[\\mathbb{H}\\left[\\pmb{p}(\\boldsymbol{y}\\mid\\pmb{x})\\right]\\right]\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{u^{\\mathrm{epi}}=\\mathbb{M I}\\left[\\theta,y\\ |\\ x,\\mathcal{D}\\right]=\\mathbb{H}\\left[\\mathbb{E}_{\\theta\\sim p\\left(\\theta\\mid\\mathcal{D}\\right)}\\left[p(y\\mid x)\\right]\\right]-\\mathbb{E}_{\\theta\\sim p\\left(\\theta\\mid\\mathcal{D}\\right)}\\left[\\mathbb{H}\\left[p(y\\mid x)\\right]\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "For evidential approaches, we follow [64] and use the maximum softmax response $\\operatorname*{max}_{c}p(y=c\\mid x)$ as aleatoric uncertainty and the total evidence $\\sum_{c}\\alpha_{c}$ as an epistemic estimate. Since we want to compare single measures of epistemic uncertainty, we evaluate GPN\u2019s epistemic uncertainty in the presence of network effects. For deterministic models (that are at the backbone of EBM-based approaches), we use the predictive entropy as a measure of aleatoric uncertainty $\\mathbb{H}\\left[p(y\\mid x)\\right]$ and the energy $E_{\\theta}(x)$ as a measure of epistemic uncertainty. For all EBMs, we use a temperature of $\\tau=1.0$ . ", "page_idx": 23}, {"type": "text", "text": "C Additional Results ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "C.1 Out-of-Distribution Detection ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We report AUC-ROC and AUC-PR metrics for the o.o.d. detection problem in an inductive and transductive setting with corresponding standard deviations over all five splits and five model initializations each in Tables 6, 8, 10 and 12. Again, we report the average performance ranks are in Tables 2, 9, 11 and 13. As mentioned in Section 5.2, we consider a weighted average that does not favor any distribution shift family. That is, we assign a weight of $1/6$ to structural and leave-out-class settings and factor in feature perturbations at a weight of $1/9$ each. ", "page_idx": 23}, {"type": "text", "text": "In an inductive setting, GEBM outperforms all other baselines and achieves the best rank regarding both AUC-ROC and AUC-PR metrics. In the transductive setting, the two evidential methods GPN and SGCN outperform GEBM on two datasets: We argue that this is due to feature leakage as o.o.d. data is present during training and evidential models are explicitly encouraged to assign low evidence to anomalous nodes. We want to point out that it is unrealistic to assume that o.o.d. data is available in practice and therefore strongly argue in favor of the inductive scenario as a more realistic benchmark. Nonetheless, GEBM is highly effective for transductive problems as well. ", "page_idx": 23}, {"type": "text", "text": "C.2 Improvement over Second Best Method ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We evaluate the improvement in AUC-ROC scores of GEBM over the estimator assigned the overall second-best rank. To that end, we average the model ranks not individually for each estimator and dataset, but instead compute an average over datasets and splits simultaneously. This way, we obtain a global rank (over datasets and shifts) for each epistemic estimate. Again, to not favor certain classes of shifts, we assign weights such that each distribution shift family has an equal contribution. Based on AUC-ROC scores listed in Table 6, we list the rank of each model in Table 14: ", "page_idx": 23}, {"type": "text", "text": "While our approach, GEBM, also ranks the highest globally, a vanilla logit-based EBM is the next best approach. Therefore, we compute the improvement in AUC-ROC scores over this model achieved by ", "page_idx": 23}, {"type": "image", "img_path": "6vNPPtWH1Q/tmp/22defbd341a09b9b6cd9ba17ef69580ea2f1a528c84cf915aa72eb256189d6d1.jpg", "img_caption": [], "img_footnote": ["Table 6: Out-of-Distribution detection AUC-ROC (\u2191) using aleatoric or epistemic uncertainty (best and runner-up) in an inductive setting. "], "page_idx": 24}, {"type": "text", "text": "GEBM in Table 15. On all datasets, GEBM improves the AUC-ROC scores by 5.8 to 10.9 percentage points on average $16\\%{-32\\%}$ relative improvement) over the second-best ranked estimator. ", "page_idx": 24}, {"type": "text", "text": "C.3 Misclassification Detection ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We report AUC-ROC and AUC-PR for misclassification detection in both an inductive and transductive setting with the corresponding standard deviations in Tables 16 to 19. In alignment with previous work [64], we observe that often aleatoric uncertainty is more suitable for misclassification detection than epistemic uncertainty. Overall, GEBM performs similar to other epistemic measures. We again want to point out that the scope of this work does not encompass improvements on aleatoric uncertainty. The results on misclassification detection obtained in our work and previous studies indicate that this problem may be more suitable as a benchmark for aleatoric uncertainty. ", "page_idx": 24}, {"type": "text", "text": "C.4 Calibration ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Our framework, GEBM, is limited toward epistemic uncertainty estimation and leaves the aleatoric estimates and the classifier calibration unchanged. For completeness, we nonetheless report calibration for the GCN used at the backbone of our experiments. Note that since our model does not change the output of the classifier, the calibration could be further improved using post hoc methods like temperature scaling [26]. This, however, is beyond the scope of this work. ", "page_idx": 24}, {"type": "table", "img_path": "6vNPPtWH1Q/tmp/c7f47139905dd69295ba0c7f278ce2185c9c2064b502350c1019e859bcb15aad.jpg", "table_caption": [], "table_footnote": ["Table 7: Average o.o.d. detection rank (AUC-ROC) (\u2193) of epistemic uncertainty versus other epistemic measures / all uncertainty measures over all distribution shifts in an inductive setting (best and runner-up). "], "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "Expected Calibration Error (ECE). The expected calibration error $\\left(\\downarrow\\right)$ [50] measures how well the predicted probabilities that are normalized to the interval $[0,1]$ match the true predictive accuracy. To that end, we bin each prediction into $B=20$ bins according to their confidence (i.e. maximum softmax response $\\operatorname*{max}_{c}\\bar{p}(y=c\\mid x)$ . For each bin, we then compute the average accuracy and compute the ECE as the mean over all bins weighted by their size. ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathrm{ECE}=\\sum_{k}\\frac{|B_{k}|}{n}|\\mathrm{accuracy}(B_{k})-\\mathrm{confidence}(B_{k})|\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Brier Score. A similar metric is the Brier score [7] (\u2193) which computes the mean squared distance between the predicted probabilities and the one-hot encoded true labels. ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname{Brier}={\\frac{1}{n}}\\sum_{i}\\left\\|p(\\pmb{y}\\mid\\pmb{x}_{i})-\\pmb{y}_{i}\\right\\|_{2}^{2}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We report ECE and Brier scores for inductive and transductive settings in Tables 20 to 23. In terms of calibration metrics, none of the considered approaches consistently shows strong merits. We remark that work on GEBM and GNN calibration is somewhat orthogonal and our framework can be applied to any well-calibrated GNN backbone. ", "page_idx": 25}, {"type": "text", "text": "C.5 Backbone Architecture ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We report AUC-ROC and AUC-PR for o.o.d. detection using GEBM and different GNN backbones with corresponding standard deviations in Tables 24 and 25. GEBM is effective on all models and achieves the highest scores on most distribution shifts. ", "page_idx": 25}, {"type": "text", "text": "C.6 Bias of Diffusion Operators ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "At the core of many GNNs and also our GEBM framework lies a diffusion operator $P_{A}:\\mathbb{R}^{n}\\to\\mathbb{R}^{n}$ .   \nHere, we discuss three typical realizations and the bias the introduce. ", "page_idx": 25}, {"type": "text", "text": "Symmetric Diffusion. Symmetric diffusion normalizes the adjacency matrix as $\\begin{array}{r l}{\\hat{A}}&{{}=}\\end{array}$ $D^{-1/2}A D^{-1/2}$ . Here, $D\\;=\\;\\mathrm{diag}(\\mathrm{deg}(v_{1}),\\cdot\\cdot\\cdot\\mathrm{deg}(v_{n}))$ is a diagonal matrix of node degrees. The symmetric diffusion operator has a dominant eigenvector that correlates with the node degree $v_{\\mathrm{max}}\\stackrel{.}{\\propto}d^{1/2}$ and therefore also centrality [12]. Repeated application of this diffusion process to any arbitrary signal will concentrate confidence at high degree nodes. Applying this diffusion to a confidence measure like the logits of GNN will therefore always favor high degree nodes. ", "page_idx": 25}, {"type": "image", "img_path": "6vNPPtWH1Q/tmp/09af1fc2123d795c4330fd48451e942b52d883c1281199b6533683a6e8437198.jpg", "img_caption": ["Table 8: Out-of-Distribution detection AUC-ROC (\u2191) using aleatoric or epistemic uncertainty (best and runner-up) in a transductive setting. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Random-Walk Diffusion. Random-walk diffusion normalizes the adjacency matrix as $\\hat{\\pmb{A}}={\\pmb D}^{-1}{\\pmb A}$ which can be seen as a stochastic transition matrix of random walk on the graph. A different interpretation is that each node distributes its information uniformly to its neighbours which in turn sum incoming signals. It, too, has a dominant eigenvector that correlates with the node degree: $v_{\\mathrm{max}}\\propto d$ [12] and also favors high degree nodes when applied to a confidence measure. ", "page_idx": 26}, {"type": "text", "text": "Label-Propagation Diffusion. Diffusion used by Label Propagation [30] normalizes the adjacency matrix as $\\bar{A}\\ =\\ A D^{-1}$ and can be seen as a smoothing procedure: Each node will update its features based on the average of its neighbours. Obviously, it has a constant dominant eigenvector and therefore does not favor nodes based on structural information. We use a repeated convex combination of the identity and this diffusion process at the backbone of our GEBM framework: $P_{\\cal A}=(\\alpha I{+}(1{-}\\alpha)A D^{-1})^{t}$ . The smoothing behaviour enforces a uniform distribution of confidence within clusters and therefore is suitable candidate for cluster-level anomaly detection. ", "page_idx": 26}, {"type": "text", "text": "C.7 Synthetic Experiments ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We motivate the three energy terms that we use to compose the energy measure of GEBM through synthetic experiments. All of them arise naturally from interleaving graph diffusion and energy marginalization, as discussed in Section 4.3. We want to highlight that GEBM allows to incorporate additional energy functions as well, e.g. if such information is available in designated downstream applications. As shown in this study, we find that the proposed energy functions already suffice to enable GEBM to be highly sensitive to various distribution shifts. ", "page_idx": 26}, {"type": "table", "img_path": "6vNPPtWH1Q/tmp/8ba34fbb14c5adc7b42293d32c04e891e75c66f71ac6ed737728feae3025a19a.jpg", "table_caption": [], "table_footnote": ["Table 9: Average o.o.d. detection rank (AUC-ROC) (\u2193) of epistemic uncertainty versus other epistemic measures / all uncertainty measures over all distribution shifts in a transductive setting (best and runner-up). "], "page_idx": 27}, {"type": "text", "text": "", "page_idx": 27}, {"type": "image", "img_path": "6vNPPtWH1Q/tmp/61435a06bb406015d711795c7d0a637639c78da73ae9b96a6dcb46d3d43a6c86.jpg", "img_caption": ["Figure 4: Energy of different types for anomalies of increasing severity on synthetic data. We vary the size of an o.o.d. cluster on real data (left), insert per-node anomalies to the energies of an SBM (middle), and increase the heterophily in an SBM (right). "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Node-Level Anomalies. We study node level anomalies by generating synthetic CSBM graphs [57] (intra-edge probability 0.05, inter-edge probability 0.001, $n=1000$ , $c=7$ ). We then generate $c$ -dimensional logits (intuitively confidence scores) according to a standard normal distribution that is centered at $l_{\\mathrm{disp}}\\,=\\,4$ for the dimension of the true class label of a node and 0 otherwise. This roughly resembles the logits of a well-trained classifier which peak at the true unknown class. We then perturb a fraction $p=0.05$ of the logits by replacing them with positive noise from a standard normal distribution that is rescaled by an increasing magnitude. For each magnitude, we compute all three energy terms for each node and compare the difference in energy to the corresponding unperturbed energy. Effectively, this monitors how the energy of a node changes under increasingly severe distribution shifts that affect the logits of a classifier. We visualize the median of the energy differences in Figure 4b: While all energies increase with higher magnitudes, the structure-agnostic energy term of GEBM is the most sensitive to anomalies that are located at individual nodes. It does not suffer from the smoothing of anomalous scores induced by graph diffusion. ", "page_idx": 27}, {"type": "text", "text": "Cluster-Level Anomalies. We study anomalies that affect an entire cluster of nodes by iteratively introducing an anomalous cluster to the graph. To that end, we leave out $k$ classes and train a GCN on the Amazon Photos dataset. At inference, we re-introduce nodes (and edges) of the left-out classes which can be seen as anomalous from the perspective of the classifier. Similar to the aforementioned node-level anomaly, we monitor how each energy type changes while iteratively introducing the entire cluster(s) of o.o.d. nodes into the graph. Again, we see that the cluster-level energy term of GEBM is the most sensitive to this distribution shift in Figure 4a. ", "page_idx": 27}, {"type": "image", "img_path": "6vNPPtWH1Q/tmp/52e3d979056a7a50ca88668144fccd4fc016d9066876de0c43aff6ba6aaa9dbe.jpg", "img_caption": ["Table 10: Out-of-Distribution detection AUC-PR (\u2191) using aleatoric or epistemic uncertainty (best and runner-up) in an inductive setting. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "Structural Anomalies. The last anomaly type we consider is creating structural anomalies: To that end, we generate synthetic CSBM graphs similar to the node anomaly experiment. However, we adapt the intra- and inter-class edge probabilities to accommodate varying degrees of homophily: We fix the intra-class connection probability at $p=0.05$ and vary the signal-to-noise ratio $\\sigma_{\\scriptstyle\\mathrm{SNR}}$ which implicitly defines the inter-class edge probability as $p/\\sigma_{\\mathrm{SNR}}$ . We measure the degree of heterophily as $-\\log{\\sigma}_{\\mathrm{SNR}}$ and again compare how individual node energies change for increasingly heterophilic graphs in Figure 4c. We find that the cluster-level energy of GEBM is the only energy term that is sensitive to structural anomalies. ", "page_idx": 28}, {"type": "text", "text": "Overall, these experiments motivate the use of the three naturally arising energy terms in GEBM: Each energy is sensitive to a different family of distribution shifts that originate from anomalies at different structural scales on the graph: At the node level, with the local structure of a node and at the cluster-level. ", "page_idx": 28}, {"type": "text", "text": "C.8 Overconfidence of GNNs ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "We disambiguate between previous studies on the underconfidence of GNNs [74, 75] and the overconfidence issue of piecewise affine networks tackled by our work. The former studies the calibration ", "page_idx": 28}, {"type": "table", "img_path": "6vNPPtWH1Q/tmp/2aa3309eb08b7e2e78c1b429271027e4f5bf4c0a7d5413b0b7ea5021f1702571.jpg", "table_caption": [], "table_footnote": ["Table 11: Average o.o.d. detection rank (AUC-PR) $\\overline{{(\\downarrow)}}$ of epistemic uncertainty versus other epistemic measures / all uncertainty measures over all distribution shifts in an inductive setting (best and runner-up). "], "page_idx": 29}, {"type": "image", "img_path": "6vNPPtWH1Q/tmp/5ec0a594d10da2a604f4d0a20cf1f23b4129d29cfe52b3ebb28ca7631a384552.jpg", "img_caption": [], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "Figure 5: Confidence (Maximum Softmax Probability and negative energy) for different GNNs at increasing distribution shift severity. ", "page_idx": 29}, {"type": "text", "text": "of GNNs on in-distribution data, and provides strong evidence for models to be underconfident. This does not conflict with our claim that with increasing distance from the training distribution, i.e. on out-of-distribution data, piece affine GNNs become overconfident. We empirically verify this in Figure 5: Both the energy and the maximum softmax probability - confidence measures - of the GNN architectures visualized increase (or converge to their maximum) under more severe normal feature perturbations. ", "page_idx": 29}, {"type": "text", "text": "C.9 Robust Evidential Inference ", "text_level": 1, "page_idx": 29}, {"type": "image", "img_path": "6vNPPtWH1Q/tmp/4b7d9058bd4ff2d265c0b12d0f289ea8cb2263a2d9571d3098d70083b781829c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "Figure 6: Robust evidential inference using APPNP as a backbone for GEBM at increasingly severy feature perturbations. ", "page_idx": 29}, {"type": "image", "img_path": "6vNPPtWH1Q/tmp/4fc232f15efe1774ef7ff3f04192f7d6427e4c2bde06942b29b3c67ece980ec3.jpg", "img_caption": ["Table 12: Out-of-Distribution detection AUC-PR (\u2191) using aleatoric or epistemic uncertainty (best and runner-up) in a transductive setting. "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "We apply GEBM to an APPNP model and expose it to increasingly severe feature perturbations. Even though APPNP uses the same number of diffusion steps as GEBM $t=10$ ), it suffers from deteriorating accuracy even when a small portion of node features is perturbed. Since logits and energy are unbounded and likely to diverge under strong enough distribution shifts (see Proposition 4.1), a fixed number of diffusion steps is insufficient to recover from perturbations. At the same time, the regularized energy of GEBM converges to zero and, therefore, a fixed number of diffusion steps can effectively maintain high robustness akin to evidential methods like GPN. ", "page_idx": 30}, {"type": "text", "text": "C.10 Partially Perturbing Node Features ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "The feature perturbation-based distribution shifts discussed in Section 5.1 replace node features with noise. We additionally study a feature-level distribution shift that partially corrupts the features of an instance. To that end, we vary the fraction $p$ with which an individual node feature is perturbed and again measure how well uncertainty estimators can detect this distribution shift. In particular, we replace $d\\cdot p$ uniformly and independently selected node feature $\\pmb{x}_{i}$ with Gaussian noise and keep $d-d\\cdot p$ features as-is. ", "page_idx": 30}, {"type": "table", "img_path": "6vNPPtWH1Q/tmp/af7787c6978765a8f1bbbbb426c06a9aa3077b1fa4c61ea797162254aa9e7e17.jpg", "table_caption": [], "table_footnote": ["Table 13: Average o.o.d. detection rank (AUC-PR) $\\overline{{(\\downarrow)}}$ of epistemic uncertainty versus other epistemic measures / all uncertainty measures over all distribution shifts in a transductive setting (best and runner-up). "], "page_idx": 31}, {"type": "table", "img_path": "6vNPPtWH1Q/tmp/cdf6d42f0f22a2936f64489070d7b872240031802793d524d7ce484917aea21a.jpg", "table_caption": ["Table 14: Global o.o.d. detection rank of each model, averaged over all datasets and distribution shifts in an inductive setting based on AUC-ROC scores (best and runner-up). "], "table_footnote": [], "page_idx": 31}, {"type": "text", "text": "We compare the AUC-ROC of different uncertainty estimators while increasing the fraction of perturbed features for each o.o.d. node in Figure 7. While both the softmax-level uncertainty of the vanilla GCN and the EBM again become overconfident, GEBM can reliably identify this distribution shift. This coincides with observations made for feature perturbations that affect all node features simultaneously and justifies focusing on that distribution shift for the bulk of our study. ", "page_idx": 31}, {"type": "text", "text": "D Ablations ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "D.1 Energy at Different Scales ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "We report AUC-ROC and AUC-PR for o.o.d.-detection ablating GEBM in an inductive and transductive setting with standard deviations in Tables 26 to 29. Additionally, we rank all variants of GEBM (i.e. energies at different structural scales) similar to the o.o.d. detection experiments in Section 5.2. We observe our proposed GEBM to consistently be the most effective over different shifts simultaneously. We can also confirm the effectiveness of the scale-specific energies at detecting distribution shifts that should be covered from their definition. Both the best and second-best ranking methods are GEBM and a variant that uses unregularized energy: This shows the efficacy of a scale-aware EBM for graph problems. ", "page_idx": 31}, {"type": "text", "text": "D.2 Diffusion Process ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "We also ablate the diffusion operator $P_{A}$ and its hyperparameters, i.e. the type of diffusion process (see Appendix C.6), the number of diffusion steps $t$ as well the teleport probability $\\alpha$ , in Figure 8. We evaluate the performance of GEBM on a representative of each distribution shift family. As expected, low $t$ and high $\\alpha$ aid node-level anomaly detection while the opposite holds for cluster and local shifts. Label-Propagation achieves satisfactory performance over the entire range of diffusion hyperparameters which justifes using it in our experiments. In particular, we did not tune any hyperparameters for good o.o.d.-detection. As stated in Appendix C.6, it does not bias the energy toward high-degree nodes which explains its advantages over the other diffusion types. It performs well over a broad range of hyperparameters making GEBM less sensitive to those. ", "page_idx": 31}, {"type": "table", "img_path": "6vNPPtWH1Q/tmp/98c939d98f801fba9dbac0baeb6c921853397c9cc31836522fd5488616c93c66.jpg", "table_caption": [], "table_footnote": [], "page_idx": 32}, {"type": "table", "img_path": "6vNPPtWH1Q/tmp/65e5a6a4d5eabce14504066399dd6db558682ff3ff114b71be4f635290250028.jpg", "table_caption": ["Table 15: Improvement of GEBM in o.o.d. detection AUC-ROC over a vanilla logit-based EBM (GCN-EBM) in an inductive setting. "], "table_footnote": ["Table 16: Misclassification detection AUC-ROC (\u2191) using aleatoric or epistemic uncertainty (best and runner-up) in an inductive setting. "], "page_idx": 32}, {"type": "text", "text": "D.3 Feature Collapse and Spectral Normalization ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "A recent line of work identifies feature collapse as a limitation in uncertainty estimation for deterministic models on i.i.d. data [46, 69]. Intuitively, feature collapse occurs when out-of-distribution data is mapped to the same regions of the model latent space as in-distribution data. Consequently, density-based uncertainty estimators such as the regularizer of GEBM can not separate both data distributions accurately and falsely attribute high confidence to out-of-distribution examples. To mitigate this issue, spectral normalization has been proposed to enforce Lipschitz smoothness in a classifier [46, 47]. We investigate if this regularization benefits GEBM as well in Appendix D.3. ", "page_idx": 32}, {"type": "image", "img_path": "6vNPPtWH1Q/tmp/659a8ac196cb07dc8b38f306127f476672a2b992f69c73cb4a99c858e6565447.jpg", "img_caption": ["Table 17: Misclassification detection AUC-ROC (\u2191) using aleatoric or epistemic uncertainty (best and runner-up) in a transductive setting. "], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "In contrast to work on i.i.d. data, we do not find spectral normalization consistently leading to improved out-of-distribution detection. We provide two possible explanations: First, feature collapse has not been observed to occur for graph problems. The diffusion process at the backbone of many GNNs may mitigate the issue. Second, spectral normalization enforces Lipschitz smoothness in terms of the $L_{2}$ norm of the input features of a node and its representation. This notion neglects the structural influence of other nodes and, therefore, may not be suitable to quantify feature collapse in non-i.i.d. domains such as graphs. ", "page_idx": 33}, {"type": "image", "img_path": "6vNPPtWH1Q/tmp/6d57b0886fab1495cab28c128b5966bfc30fd508212edf514020579f5c5f67a9.jpg", "img_caption": ["Table 18: Misclassification detection AUC-PR (\u2191) using aleatoric or epistemic uncertainty (best and runner-up) in an inductive setting. "], "img_footnote": [], "page_idx": 34}, {"type": "image", "img_path": "6vNPPtWH1Q/tmp/b0c02667deee988e15ee53fd50813ce900bddcd6ca1dd68c10a0b9157cf2693b.jpg", "img_caption": ["Fraction of features replaced "], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "Figure 7: Out-of-distribution detection AUC-ROC for different estimators for an increasing fraction of perturbed node feature dimensions on CoraML. ", "page_idx": 34}, {"type": "image", "img_path": "6vNPPtWH1Q/tmp/84b7b2760a89f7afb32e690416fb5653f75162ca8293193fbd3dde33822a69d1.jpg", "img_caption": ["Table 19: Misclassification detection AUC-PR (\u2191) using aleatoric or epistemic uncertainty (best and runner-up) in a transductive setting. "], "img_footnote": [], "page_idx": 35}, {"type": "table", "img_path": "6vNPPtWH1Q/tmp/33c4b9f2cf44409339e746cb51d790a7131e1ae1b3adc5e9ad9e9b3cbf314804.jpg", "table_caption": [], "table_footnote": [], "page_idx": 36}, {"type": "image", "img_path": "6vNPPtWH1Q/tmp/2d2991ae768b8897a8dcd860bbd2f960a9bef39e8ce162a56d446a77e2eb48d5.jpg", "img_caption": ["Table 20: ECE (\u2193) for different backbones on clean data as well as i.d. and o.o.d. nodes after a distribution shift in an inductive setting (best and runner-up). ", "Figure 8: O.o.d. detection AUC of different diffusion operators on Leave-out-Classes (top), $\\mathcal{N}(0,1)$ (middle) and homophily (bottom) shifts. "], "img_footnote": [], "page_idx": 36}, {"type": "image", "img_path": "6vNPPtWH1Q/tmp/61ab52a138e05b18da57f9d15171d258880f2031e0b4aaac13e819a56186e7ba.jpg", "img_caption": ["Table 21: ECE (\u2193) for different backbones on clean data as well as i.d. and o.o.d. nodes after a distribution shift in a transductive setting (best and runner-up). "], "img_footnote": [], "page_idx": 37}, {"type": "table", "img_path": "6vNPPtWH1Q/tmp/0e24c55af6222496794f4d0a41dd74713e3afc76c4bd7e4f00896e292a34bc09.jpg", "table_caption": [], "table_footnote": [], "page_idx": 38}, {"type": "image", "img_path": "6vNPPtWH1Q/tmp/6941465ace63eb7d7fdc7fbf20fd77fd87c3fada6a3188bab8e34ae3faabd306.jpg", "img_caption": ["Table 23: Brier Score (\u2193) for different backbones on clean data as well as i.d. and o.o.d. nodes after a distribution shift in a transductive setting (best and runner-up). "], "img_footnote": [], "page_idx": 39}, {"type": "image", "img_path": "6vNPPtWH1Q/tmp/56b8974d43c9b66006800b8b4b6af4b1eae6b202f1fc32e607fb1601a914fcb5.jpg", "img_caption": [], "img_footnote": ["Table 24: O.o.d. detection $\\overline{{\\mathrm{AUC-ROC}(\\uparrow)}}$ using different backbones on CoraML in an inductive setting. "], "page_idx": 39}, {"type": "table", "img_path": "6vNPPtWH1Q/tmp/f97cac834cb102dd22cf4e4209e96423021962bb0c554eea7088d81225bf2e0f.jpg", "table_caption": [], "table_footnote": ["Table 25: O.o.d. detection AUC-PR(\u2191) using different backbones on CoraML in an inductive setting. "], "page_idx": 40}, {"type": "table", "img_path": "6vNPPtWH1Q/tmp/8ff4c997aeffab3be4a009766947805a42105273e61947e525eb2b31af35e380.jpg", "table_caption": [], "table_footnote": ["Table 26: O.o.d-detection AUC-ROC (\u2191) using different EBMs in an inductive setting. "], "page_idx": 40}, {"type": "table", "img_path": "6vNPPtWH1Q/tmp/7d910cac434cc7110cc36f01396009b68c20231978074058b2d9f5dddfc04049.jpg", "table_caption": [], "table_footnote": ["Table 27: O.o.d-detection AUC-PR (\u2191) using different EBMs in an inductive setting. "], "page_idx": 41}, {"type": "table", "img_path": "6vNPPtWH1Q/tmp/2132627b2908618d8966ef053a0d19669e76a3fa4c2539912f7e3d55cda9ccf6.jpg", "table_caption": [], "table_footnote": ["Table 28: O.o.d-detection AUC-ROC (\u2191) using different EBMs in a transductive setting. "], "page_idx": 42}, {"type": "table", "img_path": "6vNPPtWH1Q/tmp/0de004fbfae04090785f8b3950c1846ac1bf8793d239778230bfeb48b9827133.jpg", "table_caption": [], "table_footnote": ["Table 29: O.o.d-detection AUC-PR $\\overline{{(\\uparrow)}}$ using different EBMs in a transductive setting. "], "page_idx": 43}, {"type": "image", "img_path": "6vNPPtWH1Q/tmp/93ff859f04035da7e97c3d87ac4bcd16325fa2bbd57ef76bbafc901e7f52b07e.jpg", "img_caption": ["Table 30: O.o.d. detection AUC for APPNP and GCN with spectral normalization at different weight scales $\\sigma$ . "], "img_footnote": [], "page_idx": 43}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 44}, {"type": "text", "text": "Justification: We identify three main contributions in the introduction Section 1. Our novel model is described in Section 4.3, the formal proofs regarding integrability of EBMs and Gaussian regularization are given in Sections 4.2 and 4.3 and a novel evidential interpretation of our EBM framework can be found in Section 4.4. We support the claimed efficacy of our model at different structural scales by its strong out-of-distribution detection performance on a comprehensive suite of distribution shifts (Appendix C). ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 44}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 44}, {"type": "text", "text": "Justification: We discuss limitations in Section 6: We only aim at epistemic uncertainty estimation and can not improve on aleatoric metrics. Our method is tailored toward node classification in a homophilic setting, which is also acknowledged. Our method is less effective on centrality-based splits, which is discussed in Sections 5.2 and 6 and in-depth in Appendix C.6. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best ", "page_idx": 44}, {"type": "text", "text": "judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 45}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: We explicitly state the assumptions that we deal with piecewise affine classifiers and provide formal proofs for all statements in Appendix A. For the core theoretical contribution, we provide intuition in Section 4.2. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 45}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: We provide code to reproduce our experiments online. Furthermore, we additionally also list all hyperparameters for all baselines and our model explicitly in Appendix B. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 45}, {"type": "text", "text": "", "page_idx": 46}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: We provide code to reproduce our experiments. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 46}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Justification: Experimental details, entailing dataset splits, architecture choices, optimization, etc., are provided in Appendix B. Additionally, they are listed as default configurations in the public code. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 46}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 47}, {"type": "text", "text": "Justification: We omit error-bars and standard deviations in the main text for clarity, but supply them in Appendix C. They are explicitly referred to as standard deviations. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 47}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 47}, {"type": "text", "text": "Justification: Details about compute resources and a discussion about memory and training time are provided in Appendix B. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 47}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Justification: Our research aims to improve reliability of GNNs. All datasets are commonly used benchmarks that are publicly available. Our paper fully conforms with the code of ethics. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. \u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. ", "page_idx": 47}, {"type": "text", "text": "The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 48}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 48}, {"type": "text", "text": "Justification: We aim to contribute to making AI more reliable by improving on uncertainty quantification. We discuss the impact of our work beyond commonly known risks and concerns in research on AI in Section 6. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 48}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 48}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Justification: We do not believe that model has an increased risk for misuse. We encourage users for active evaluation of the uncertainty provided in Section 6. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 48}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 49}, {"type": "text", "text": "Justification: We cite papers producing the commonly used node classification benchmark. We downloaded all datasets from PyTorch Geometric, as described in Appendix B. We report licenses where we could find them. All datasets have been used in open research for several years and form a well-establish benchmark. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 49}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 49}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 49}, {"type": "text", "text": "Justification: Our paper does not provide new assets. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 49}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 49}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 49}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 49}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 49}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 50}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 50}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 50}]