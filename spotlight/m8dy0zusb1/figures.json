[{"figure_path": "M8dy0ZuSb1/figures/figures_1_1.jpg", "caption": "Figure 1: Depictions of a pre-activation neuron z = w<sup>T</sup>x in the presence of (a) covariate shift \u03b5, (b) a multiplicative weight perturbation (MWP) equivalent to \u03b5, and (c) random MWPs \u03be. \u03bf denotes the Hadamard product. Figs. (a) and (b) show that for a covariate shift \u03b5, one can always find an equivalent MWP. From this intuition, we propose to inject random MWPs \u03be to the forward pass during training as shown in Fig. (c) to robustify a DNN to covariate shift.", "description": "This figure illustrates the equivalence between input corruptions and multiplicative weight perturbations (MWPs).  It shows three scenarios: (a) a pre-activation neuron with a covariate shift in the input; (b) the same neuron where the covariate shift is mimicked by an equivalent MWP; (c) the same neuron with random MWPs injected during training to make the DNN robust to covariate shifts.  The Hadamard product is used to represent element-wise multiplication.", "section": "Data Augmentation via Multiplicative Perturbations"}, {"figure_path": "M8dy0ZuSb1/figures/figures_2_1.jpg", "caption": "Figure 2: Depiction of how a corruption g affects the output of a DNN. Here xg = g(x). The corruption g creates a shift \u03b4gx(0) = xg \u2212 x in the input x, which propagates into shifts \u03b4gx(h) in the output of each layer. This will eventually cause a shift in the loss \u03b4gl. This figure explains why the model performance tends to degrade under corruption.", "description": "This figure illustrates how an input corruption affects the output of a deep neural network (DNN).  It shows that a corruption applied to the input (x) propagates through each layer of the network, causing a shift in the output of each layer and ultimately a shift in the final loss function.  This visualizes why the performance of a DNN often degrades when presented with corrupted inputs.", "section": "2 Data Augmentation via Multiplicative Perturbations"}, {"figure_path": "M8dy0ZuSb1/figures/figures_6_1.jpg", "caption": "Figure 3: DAMP improves robustness to all corruptions while preserving accuracy on clean images. Results of ResNet18/CIFAR-100 experiments averaged over 5 seeds. The heatmap shows CE described in Eq. (18) (lower is better), where each row corresponds to a tuple of training (method, corruption), while each column corresponds to the test corruption. The Avg column shows the average of the results of the previous columns. none indicates no corruption. We use the models trained under the SGD/none setting (first row) as baselines to calculate the CE. The last five rows are the 5 best training corruptions ranked by the results in the Avg column.", "description": "This figure shows the results of ResNet18 trained on CIFAR-100 dataset with different methods. Each row represents a method and a corruption used during training. Each column shows the performance under a specific corruption during testing. The heatmap visualizes the corruption error (CE), where lower values are better. The figure demonstrates that DAMP consistently improves robustness against various corruptions without compromising accuracy on clean images.", "section": "4.1 Comparing DAMP to directly using corruptions as augmentations"}, {"figure_path": "M8dy0ZuSb1/figures/figures_7_1.jpg", "caption": "Figure 3: DAMP improves robustness to all corruptions while preserving accuracy on clean images. Results of ResNet18/CIFAR-100 experiments averaged over 5 seeds. The heatmap shows CE described in Eq. (18) (lower is better), where each row corresponds to a tuple of training (method, corruption), while each column corresponds to the test corruption. The Avg column shows the average of the results of the previous columns. none indicates no corruption. We use the models trained under the SGD/none setting (first row) as baselines to calculate the CE. The last five rows are the 5 best training corruptions ranked by the results in the Avg column.", "description": "This figure shows the comparison of Corruption error (CE) for different corruption types using different training methods.  The heatmap shows that DAMP consistently improves robustness to all corruption types while maintaining clean image accuracy, unlike using corruptions directly in training.", "section": "4.1 Comparing DAMP to directly using corruptions as augmentations"}, {"figure_path": "M8dy0ZuSb1/figures/figures_15_1.jpg", "caption": "Figure 3: DAMP improves robustness to all corruptions while preserving accuracy on clean images. Results of ResNet18/CIFAR-100 experiments averaged over 5 seeds. The heatmap shows CE described in Eq. (18) (lower is better), where each row corresponds to a tuple of training (method, corruption), while each column corresponds to the test corruption. The Avg column shows the average of the results of the previous columns.  none indicates no corruption. We use the models trained under the SGD/none setting (first row) as baselines to calculate the CE. The last five rows are the 5 best training corruptions ranked by the results in the Avg column.", "description": "The figure shows a heatmap comparing the corruption error (CE) of ResNet18 models trained on CIFAR-100 using different methods. The methods include standard SGD without any corruption and DAMP with different corruption types as data augmentation. The heatmap visualizes the CE for different combinations of training method and corruption type versus various test corruption types. Lower values in the heatmap indicate better robustness. The figure demonstrates that DAMP improves robustness to various corruption types without compromising accuracy on clean images.", "section": "4.1 Comparing DAMP to directly using corruptions as augmentations"}, {"figure_path": "M8dy0ZuSb1/figures/figures_15_2.jpg", "caption": "Figure 3: DAMP improves robustness to all corruptions while preserving accuracy on clean images. Results of ResNet18/CIFAR-100 experiments averaged over 5 seeds. The heatmap shows CE described in Eq. (18) (lower is better), where each row corresponds to a tuple of training (method, corruption), while each column corresponds to the test corruption. The Avg column shows the average of the results of the previous columns. none indicates no corruption. We use the models trained under the SGD/none setting (first row) as baselines to calculate the CE. The last five rows are the 5 best training corruptions ranked by the results in the Avg column.", "description": "The figure shows a heatmap visualizing the corruption error (CE) of ResNet18 models trained on CIFAR-100 dataset using different methods. The rows represent training methods and corruptions, while the columns represent test corruptions. Lower CE values indicate better robustness. DAMP consistently shows lower CE across all test corruptions compared to other methods, indicating improved robustness without compromising accuracy on clean images.", "section": "4.1 Comparing DAMP to directly using corruptions as augmentations"}, {"figure_path": "M8dy0ZuSb1/figures/figures_16_1.jpg", "caption": "Figure 7: DAMP has better corruption robustness than DAAP. We report the predictive errors (lower is better) averaged over 5 seeds. None indicates no corruption. Mild includes severity levels 1, 2 and 3. Severe includes severity levels 4 and 5. We evaluate DAMP and DAAP under different noise standard deviations \u03c3. These results imply that the multiplicative weight perturbations of DAMP are more effective than the additive perturbations of DAAP in improving robustness to corruptions.", "description": "This figure compares the performance of DAMP and DAAP on three datasets (CIFAR-10, CIFAR-100, and TinyImageNet) under different corruption levels.  The results show that DAMP, which uses multiplicative weight perturbations, consistently outperforms DAAP (additive weight perturbations) in terms of robustness to image corruptions across various severity levels. The figure highlights the effectiveness of multiplicative perturbations for improving model robustness.", "section": "4.2 Comparing DAMP to random additive perturbations"}]