{"references": [{"fullname_first_author": "George Cazenavette", "paper_title": "Dataset distillation by matching training trajectories", "publication_date": "2022-00-00", "reason": "This paper introduces a novel dataset distillation method by aligning gradient trajectories, a key concept that many other methods in this paper build upon."}, {"fullname_first_author": "Justin Cui", "paper_title": "Scaling up dataset distillation to imagenet-1k with constant memory", "publication_date": "2023-00-00", "reason": "This paper addresses the scalability challenges of dataset distillation, extending its applicability to large-scale datasets like ImageNet-1K, a significant advancement in the field."}, {"fullname_first_author": "Jiawei Du", "paper_title": "Dataset distillation by matching training trajectories", "publication_date": "2023-00-00", "reason": "This paper focuses on minimizing the accumulated trajectory error during dataset distillation, improving the accuracy and efficiency of the synthesis process."}, {"fullname_first_author": "Bo Zhao", "paper_title": "Dataset condensation with distribution matching", "publication_date": "2021-00-00", "reason": "This paper introduces a dataset condensation method using distribution matching, improving the representativeness of the distilled dataset and serving as a strong baseline for comparison."}, {"fullname_first_author": "Hongxu Yin", "paper_title": "Dreaming to distill: Data-free knowledge transfer via deepinversion", "publication_date": "2020-00-00", "reason": "This paper proposes a data-free knowledge transfer method using deepinversion, a technique that is influential in the development of data-efficient training approaches."}]}