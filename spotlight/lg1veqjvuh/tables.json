[{"figure_path": "lG1VEQJvUH/tables/tables_4_1.jpg", "caption": "Table 1: Unitary GCN with UniConv (Definition 1) and Lie UniConv (Definition 3) layers compared with other GNN architectures on LRGB datasets [DRG+22]. Top performer bolded and second/third underlined. Networks are set to fit within a parameter budget of 500,000 parameters. Complex numbers are counted as two parameters each. See App. G for additional details.", "description": "The table compares the performance of two unitary graph convolutional network (GCN) models (UniConv and Lie UniConv) with other state-of-the-art GCNs on three benchmark datasets from the Long Range Graph Benchmark (LRGB): Peptides, COCO, and Pascal VOC.  The results show the test accuracy (AP) and mean absolute error (MAE) for the Peptides dataset, and the test F1-score for the COCO and Pascal VOC datasets.  The table highlights that the unitary GCN models achieve competitive or superior performance compared to other methods, even with a parameter budget constraint.", "section": "Experiments"}, {"figure_path": "lG1VEQJvUH/tables/tables_7_1.jpg", "caption": "Table 1: Unitary GCN with UniConv (Definition 1) and Lie UniConv (Definition 3) layers compared with other GNN architectures on LRGB datasets [DRG+22]. Top performer bolded and second/third underlined. Networks are set to fit within a parameter budget of 500,000 parameters. Complex numbers are counted as two parameters each. See App. G for additional details.", "description": "This table compares the performance of two proposed unitary graph convolutional network (GCN) models, UniGCN and Lie UniGCN, against several other state-of-the-art GCN models on four benchmark datasets from the Long Range Graph Benchmark (LRGB) [DRG+22].  The results show test accuracy (AP) and mean absolute error (MAE) for two peptide datasets (Peptides-Func and Peptides-Struct), and test F1 scores for COCO and PASCAL VOC datasets.  The table highlights the competitive performance of the proposed unitary models and notes that the models are designed to have a maximum of 500,000 parameters.  Details of hyperparameters and training procedures are provided in Appendix G.", "section": "5 Experiments"}, {"figure_path": "lG1VEQJvUH/tables/tables_8_1.jpg", "caption": "Table 2: Comparison of Unitary GCN with UniConv (Definition 1) and Lie UniConv (Definition 3) layers with other GNN architectures on the Heterophilous Graph Datasets.", "description": "This table compares the performance of Unitary Graph Convolutional Networks (GCNs) using two different types of unitary convolutions (UniConv and Lie UniConv) against other state-of-the-art GNN architectures on several heterophilous graph datasets. Heterophilous datasets are characterized by the presence of nodes with dissimilar labels that are connected together. The table presents the test accuracy, along with confidence intervals, for each method on each dataset.  The results demonstrate that the unitary GCN models achieve competitive performance compared to other methods.", "section": "Experiments"}, {"figure_path": "lG1VEQJvUH/tables/tables_25_1.jpg", "caption": "Table 1: Unitary GCN with UniConv (Definition 1) and Lie UniConv (Definition 3) layers compared with other GNN architectures on LRGB datasets [DRG+22]. Top performer bolded and second/third underlined. Networks are set to fit within a parameter budget of 500,000 parameters. Complex numbers are counted as two parameters each. See App. G for additional details.", "description": "The table compares the performance of two proposed unitary graph convolutional networks (UniGCN and LieUniGCN) with other state-of-the-art Graph Neural Networks (GNNs) on several benchmark datasets from the Long Range Graph Benchmark (LRGB).  The results show that the proposed unitary GCN models perform competitively with or even surpass the existing methods across several evaluation metrics.", "section": "Experiments"}, {"figure_path": "lG1VEQJvUH/tables/tables_25_2.jpg", "caption": "Table 1: Unitary GCN with UniConv (Definition 1) and Lie UniConv (Definition 3) layers compared with other GNN architectures on LRGB datasets [DRG+22]. Top performer bolded and second/third underlined. Networks are set to fit within a parameter budget of 500,000 parameters. Complex numbers are counted as two parameters each. See App. G for additional details.", "description": "The table compares the performance of two variants of unitary graph convolutional networks (UniConv and Lie UniConv) against other state-of-the-art GNNs on benchmark datasets from the Long Range Graph Benchmark (LRGB).  The metrics used are Test AP (Average Precision), Test MAE (Mean Absolute Error), and Test F1 (F1-score). The results highlight the competitive performance of the unitary GCN architectures, especially considering a parameter budget constraint.", "section": "Experiments"}, {"figure_path": "lG1VEQJvUH/tables/tables_28_1.jpg", "caption": "Table 3: Comparison of Unitary GCN with Lie UniConv layers (Definition 3) with other GNN architectures on the TU datasets. Each complex number is counted as two parameters for our architectures, except for wide Unitary GCN which counts a complex numbers as one parameter so that the width of hidden layers roughly matches that of vanilla GCN.", "description": "The table compares the performance of Unitary GCN (with Lie UniConv layers) against other Graph Neural Network (GNN) architectures on four datasets from the TUDataset benchmark.  The performance metric is Test Average Precision (AP).  A \"Wide Unitary GCN\" is also included, where the number of parameters is adjusted to be comparable to standard GCNs.", "section": "F.2 TU Datasets"}, {"figure_path": "lG1VEQJvUH/tables/tables_29_1.jpg", "caption": "Table 1: Unitary GCN with UniConv (Definition 1) and Lie UniConv (Definition 3) layers compared with other GNN architectures on LRGB datasets [DRG+22]. Top performer bolded and second/third underlined. Networks are set to fit within a parameter budget of 500,000 parameters. Complex numbers are counted as two parameters each. See App. G for additional details.", "description": "This table compares the performance of two variants of unitary graph convolutional networks (UniConv and Lie UniConv) against other state-of-the-art Graph Neural Networks (GNNs) on several benchmark datasets from the Long Range Graph Benchmark (LRGB).  The results show the test accuracy and mean absolute error for peptide function classification, peptide structure regression, and node classification tasks on COCO and Pascal VOC datasets.  The table highlights the competitive performance of the unitary GCNs, especially considering a parameter budget constraint.", "section": "Experiments"}, {"figure_path": "lG1VEQJvUH/tables/tables_30_1.jpg", "caption": "Table 1: Unitary GCN with UniConv (Definition 1) and Lie UniConv (Definition 3) layers compared with other GNN architectures on LRGB datasets [DRG+22]. Top performer bolded and second/third underlined. Networks are set to fit within a parameter budget of 500,000 parameters. Complex numbers are counted as two parameters each. See App. G for additional details.", "description": "The table compares the performance of two variants of unitary graph convolutional networks (UniConv and LieUniConv) against other state-of-the-art Graph Neural Networks (GNNs) on benchmark datasets from the Long Range Graph Benchmark (LRGB).  The results show the test accuracy (AP) and mean absolute error (MAE) for peptide function and structure prediction tasks, and test F1 scores for object detection tasks. The unitary GCN models achieve competitive performance, demonstrating the effectiveness of the proposed method in improving stability and long-range dependency learning.", "section": "Experiments"}, {"figure_path": "lG1VEQJvUH/tables/tables_30_2.jpg", "caption": "Table 1: Unitary GCN with UniConv (Definition 1) and Lie UniConv (Definition 3) layers compared with other GNN architectures on LRGB datasets [DRG+22]. Top performer bolded and second/third underlined. Networks are set to fit within a parameter budget of 500,000 parameters. Complex numbers are counted as two parameters each. See App. G for additional details.", "description": "This table compares the performance of Unitary Graph Convolutional Networks (GCNs) using two different types of unitary convolutions (UniConv and LieUniConv) against other state-of-the-art GCN architectures on four datasets from the Long Range Graph Benchmark (LRGB).  The results are presented in terms of Test Average Precision (AP) and Test Mean Absolute Error (MAE) for the Peptides datasets and Test F1 score for the COCO and Pascal VOC datasets.  The table highlights that the unitary GCNs achieve competitive performance with other models, while staying within a specified parameter budget.", "section": "5 Experiments"}, {"figure_path": "lG1VEQJvUH/tables/tables_31_1.jpg", "caption": "Table 7: Hyperparameters on the TU datasets.", "description": "The table shows the hyperparameters used for training different models on the TU datasets.  For each dataset (ENZYMES, IMDB-BINARY, MUTAG, PROTEINS), the table lists the learning rate, dropout rate, number of convolutional layers, hidden dimension, positional encoding scheme (PE/SE), whether edge features were used, batch size, and number of epochs.  Different hyperparameter settings were used for the unitary GCN and Lie unitary GCN models.", "section": "F.2 TU Datasets"}, {"figure_path": "lG1VEQJvUH/tables/tables_31_2.jpg", "caption": "Table 1: Unitary GCN with UniConv (Definition 1) and Lie UniConv (Definition 3) layers compared with other GNN architectures on LRGB datasets [DRG+22]. Top performer bolded and second/third underlined. Networks are set to fit within a parameter budget of 500,000 parameters. Complex numbers are counted as two parameters each. See App. G for additional details.", "description": "The table compares the performance of Unitary GCN with two different convolution methods (UniConv and Lie UniConv) against other state-of-the-art Graph Neural Network (GNN) architectures on four datasets from the Long Range Graph Benchmark (LRGB).  The results show the test accuracy and mean absolute error (MAE) for each model, demonstrating that the Unitary GCN performs competitively with existing models while remaining within a specified parameter budget.", "section": "Experiments"}, {"figure_path": "lG1VEQJvUH/tables/tables_32_1.jpg", "caption": "Table 1: Unitary GCN with UniConv (Definition 1) and Lie UniConv (Definition 3) layers compared with other GNN architectures on LRGB datasets [DRG+22]. Top performer bolded and second/third underlined. Networks are set to fit within a parameter budget of 500,000 parameters. Complex numbers are counted as two parameters each. See App. G for additional details.", "description": "This table compares the performance of Unitary Graph Convolutional Networks (GCNs) with two different types of unitary convolution layers (UniConv and Lie UniConv) against other state-of-the-art GCN architectures on the Long Range Graph Benchmark (LRGB) datasets. The results are presented in terms of test accuracy and mean absolute error (MAE).  The table highlights that the Unitary GCN models achieve competitive or better performance than other models, emphasizing the effectiveness of the unitary convolutions in improving the stability and performance of GCNs.", "section": "Experiments"}]