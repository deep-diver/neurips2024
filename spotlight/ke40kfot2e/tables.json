[{"figure_path": "Ke40kfOT2E/tables/tables_3_1.jpg", "caption": "Table D.1: Training PICs using functional sharing requires comparable resources as PCs. We report the time (in milliseconds, top) and GPU memory (in GiB, bottom) required to perform an Adam optimization step on 256 MNIST-like images by varying: architecture ({QT-CP, QG-CP, QG-TK}), size K ({2}=4), model ({PC, PIC}), and sharing technique ({C, F, N}). For each model we attach a pair (\u00b7,\u00b7) where the first (resp. second) argument specifies the sharing technique for the input (resp. inner) layer(s).", "description": "This table compares the training time and GPU memory usage for Probabilistic Integral Circuits (PICs) with different functional sharing techniques and standard Probabilistic Circuits (PCs). It shows how the choice of architecture, the number of quadrature points (K), and the sharing technique affect the computational cost of training.", "section": "D Additional results"}, {"figure_path": "Ke40kfOT2E/tables/tables_8_1.jpg", "caption": "Table 6: QPCs improve over PCs and other DGM baselines in terms of test-set bpd for the MNIST-family datasets. We compare against SparsePC [10], HCLT [31], RAT-SPN [43], IDF [21], BitSwap [25], BBans [50] and McBits [46]. HCLT results are taken from [18]. Columns QPC and PC report results from this paper, with QG-CP-512 being the best performing architecture for both.", "description": "This table compares the test-set bits-per-dimension (bpd) for different models on MNIST-family datasets (MNIST, Fashion-MNIST, EMNIST).  It shows that QPCs (Quadrature Probabilistic Circuits) generally outperform other models, including various Probabilistic Circuits (PCs) and Deep Generative Models (DGMs). The best performing QPC architecture is highlighted.", "section": "4 Experiments"}, {"figure_path": "Ke40kfOT2E/tables/tables_8_2.jpg", "caption": "Table 1: QPCs improve over PCs. We mark results with * for YCoCg-R and \u2020 for YCoCg. QG-CP-512 (resp. QG-CP-256) is the best performing architecture for QPCs and PCs on CIFAR & ImgNet32 (resp. ImgNet64 & CelebA).", "description": "This table compares the performance of QPCs and PCs on various image datasets (CIFAR, ImageNet32, ImageNet64, and CelebA).  The results show that QPCs generally outperform PCs in terms of bits per dimension (bpd), indicating better model efficiency.  Different preprocessing methods (YCoCg and YCoCg-R) are used for some datasets, affecting the bpd values and are indicated by asterisks and daggers respectively.  The best performing QPC architecture for each dataset is also specified.", "section": "4 Experiments"}, {"figure_path": "Ke40kfOT2E/tables/tables_14_1.jpg", "caption": "Table D.1: Training PICs using functional sharing requires comparable resources as PCs. We report the time (in milliseconds, top) and GPU memory (in GiB, bottom) required to perform an Adam optimization step on 256 MNIST-like images by varying: architecture ({QT-CP, QG-CP, QG-TK}), size K ({2}i=4), model ({PC, PIC}), and sharing technique ({C, F, N}). For each model we attach a pair (\u00b7,\u00b7) where the first (resp. second) argument specifies the sharing technique for the input (resp. inner) layer(s).", "description": "This table compares the training time and GPU memory usage for Probabilistic Integral Circuits (PICs) and Probabilistic Circuits (PCs) with different architectures, sizes, and sharing techniques.  It shows the impact of functional sharing on resource utilization during training, indicating that functional sharing in PICs can reduce the required resources to levels comparable to those needed for PCs.", "section": "D Additional results"}, {"figure_path": "Ke40kfOT2E/tables/tables_15_1.jpg", "caption": "Table D.1: Training PICs using functional sharing requires comparable resources as PCs. We report the time (in milliseconds, top) and GPU memory (in GiB, bottom) required to perform an Adam optimization step on 256 MNIST-like images by varying: architecture ({QT-CP, QG-CP, QG-TK}), size K ({2}=4), model ({PC, PIC}), and sharing technique ({C, F, N}). For each model we attach a pair (\u00b7,\u00b7) where the first (resp. second) argument specifies the sharing technique for the input (resp. inner) layer(s).", "description": "This table presents the results of an experiment comparing the training time and GPU memory usage of Probabilistic Integral Circuits (PICs) with and without functional sharing, and standard PCs.  Different architectures (QT-CP, QG-CP, QG-TK), integration points (K), and sharing techniques (F, C, N) are varied to assess their impact on resource consumption. The results show the computational cost of training different configurations of PICs compared to standard PCs. ", "section": "D Additional results"}, {"figure_path": "Ke40kfOT2E/tables/tables_18_1.jpg", "caption": "Table D.1: Training PICs using functional sharing requires comparable resources as PCs. We report the time (in milliseconds, top) and GPU memory (in GiB, bottom) required to perform an Adam optimization step on 256 MNIST-like images by varying: architecture ({QT-CP, QG-CP, QG-TK}), size K ({2}<sup>i</sup>=4), model ({PC, PIC}), and sharing technique ({C, F, N}). For each model we attach a pair (\u00b7,\u00b7) where the first (resp. second) argument specifies the sharing technique for the input (resp. inner) layer(s).", "description": "This table presents the results of experiments comparing the training time and GPU memory usage of PCs and PICs with different architectures (QT-CP, QG-CP, QG-TK), quadrature points (K), and functional sharing techniques (F, C, N).  The table shows how functional sharing in PICs allows for scaling to larger models compared to PCs and PICs without functional sharing.", "section": "D Additional results"}, {"figure_path": "Ke40kfOT2E/tables/tables_18_2.jpg", "caption": "Table D.1: Training PICs using functional sharing requires comparable resources as PCs. We report the time (in milliseconds, top) and GPU memory (in GiB, bottom) required to perform an Adam optimization step on 256 MNIST-like images by varying: architecture ({QT-CP, QG-CP, QG-TK}), size K ({2}=4), model ({PC, PIC}), and sharing technique ({C, F, N}). For each model we attach a pair (\u00b7,\u00b7) where the first (resp. second) argument specifies the sharing technique for the input (resp. inner) layer(s).", "description": "This table presents the results of an experiment comparing the training time and GPU memory usage of Probabilistic Integral Circuits (PICs) and Probabilistic Circuits (PCs) with various configurations.  The experiment varied the architecture (QT-CP, QG-CP, QG-TK), the number of quadrature points (K), the model type (PC or PIC), and the type of functional sharing used (C, F, N).  The results demonstrate the impact of each configuration on computational resources.", "section": "D Additional results"}, {"figure_path": "Ke40kfOT2E/tables/tables_19_1.jpg", "caption": "Table D.1: Training PICs using functional sharing requires comparable resources as PCs. We report the time (in milliseconds, top) and GPU memory (in GiB, bottom) required to perform an Adam optimization step on 256 MNIST-like images by varying: architecture ({QT-CP, QG-CP, QG-TK}), size K ({2}=4), model ({PC, PIC}), and sharing technique ({C, F, N}). For each model we attach a pair (\u00b7,\u00b7) where the first (resp. second) argument specifies the sharing technique for the input (resp. inner) layer(s).", "description": "This table presents the results of experiments comparing the time and GPU memory required for an Adam optimization step using different model configurations. The configurations vary across several factors: the type of region graph (QT-CP, QG-CP, QG-TK), the size of K (which affects the number of quadrature points), the type of model (PC or PIC), and the sharing techniques used (C, F, N). For each model, a pair (\u00b7,\u00b7) is provided to specify the sharing technique applied to the input and inner layers.  The top part of the table displays the time, and the bottom displays the GPU memory usage.", "section": "D Additional results"}, {"figure_path": "Ke40kfOT2E/tables/tables_19_2.jpg", "caption": "Table D.1: Training PICs using functional sharing requires comparable resources as PCs. We report the time (in milliseconds, top) and GPU memory (in GiB, bottom) required to perform an Adam optimization step on 256 MNIST-like images by varying: architecture ({QT-CP, QG-CP, QG-TK}), size K ({2}=4), model ({PC, PIC}), and sharing technique ({C, F, N}). For each model we attach a pair (\u00b7,\u00b7) where the first (resp. second) argument specifies the sharing technique for the input (resp. inner) layer(s).", "description": "This table compares the training time and GPU memory usage for PCs and PICs with different architectures,  K values, and functional sharing techniques.  It shows that functional sharing in PICs allows scaling to larger models without significant increase in resource consumption.", "section": "D Additional results"}, {"figure_path": "Ke40kfOT2E/tables/tables_20_1.jpg", "caption": "Table D.3: PCs with a shared input layer deliver comparable performance as PCs who do not on the MNIST-family datasets. We compare the bits-per-dimension of PCs with (w/) and without (w/o) a shared input layer considering three different tensorized architectures: QT-CP-512, QG-CP-512 and QG-TK-64.", "description": "This table compares the performance of Probabilistic Circuits (PCs) with and without a shared input layer on MNIST-family datasets.  Three different architectures are used (QT-CP-512, QG-CP-512, QG-TK-64). The results are measured in bits-per-dimension (bpd), showing the impact of the shared input layer on model performance for different datasets within the MNIST family.", "section": "D.2 Additional distribution estimation results"}, {"figure_path": "Ke40kfOT2E/tables/tables_20_2.jpg", "caption": "Table D.3: PCs with a shared input layer deliver comparable performance as PCs who do not on the MNIST-family datasets. We compare the bits-per-dimension of PCs with (w/) and without (w/o) a shared input layer considering three different tensorized architectures: QT-CP-512, QG-CP-512 and QG-TK-64.", "description": "This table compares the performance of Probabilistic Circuits (PCs) with and without a shared input layer on the MNIST-family datasets. Three different architectures (QT-CP-512, QG-CP-512, and QG-TK-64) are used in the comparison.  The bits-per-dimension (bpd) metric is used to evaluate the performance, providing a measure of the model's ability to represent the data efficiently. The results show whether having a shared input layer significantly impacts performance.", "section": "D.2 Additional distribution estimation results"}, {"figure_path": "Ke40kfOT2E/tables/tables_20_3.jpg", "caption": "Table D.4: QPCs consistently improve over PCs on MNIST and FASHIONMNIST. Test-set bits-per-dimension (bpd) on MNIST (top) and FASHIONMNIST (bottom) averaged over 5 runs. All QPCs are materialized from PICs applying F-sharing over input units and C-sharing over groups of integral units, i.e. PIC (F, C). PCs do not apply any form of parameter sharing, as these deliver the best performance for these datasets, as detailed in Table D.3.", "description": "This table compares the performance of QPCs and PCs on MNIST and FashionMNIST datasets.  The QPCs consistently outperform the PCs across different numbers of quadrature points (K).  The QPCs used F-sharing for input units and C-sharing for integral units, while the PCs used no parameter sharing.  The results are averaged over 5 runs.", "section": "D.2 Additional distribution estimation results"}, {"figure_path": "Ke40kfOT2E/tables/tables_21_1.jpg", "caption": "Table D.5: QPCs generally improve over PCs on distribution estimation. We report the average test-set bits-per-dimensions of QT-CP-512, QG-CP-512 and QG-TK-64 for datasets up to image size 32x32, and of QT-CP-256, QG-CP-256 and QG-TK-32 for datasets of image size 64x64. All architectures are trained both as QPCs and PCs. QPCs are materialized from PICs applying F-sharing over the group of input units, and C-sharing over the groups of integral units. PCs do not apply any form of parameter sharing for MNIST-family datasets, as these delivered the best performance for such datasets Table D.3, while they apply F-sharing at the input layer for RGB datasets. We mark with * (resp. \u2020) datasets preprocessed using YCoCg-R (resp. YCoCg). All results are averaged over 5 different runs.", "description": "This table presents a comparison of the performance of QPCs and PCs as density estimators on various image datasets.  It shows the bits-per-dimension (bpd) for different architectures (QT-CP, QG-CP, QG-TK) and sizes (K) of the models.  The results highlight the consistent improvement of QPCs over PCs across different datasets and model configurations.", "section": "D.2 Additional distribution estimation results"}, {"figure_path": "Ke40kfOT2E/tables/tables_21_2.jpg", "caption": "Table D.5: QPCs generally improve over PCs on distribution estimation. We report the average test-set bits-per-dimensions of QT-CP-512, QG-CP-512 and QG-TK-64 for datasets up to image size 32x32, and of QT-CP-256, QG-CP-256 and QG-TK-32 for datasets of image size 64x64. All architectures are trained both as QPCs and PCs. QPCs are materialized from PICs applying F-sharing over the group of input units, and C-sharing over the groups of integral units. PCs do not apply any form of parameter sharing for MNIST-family datasets, as these delivered the best performance for such datasets Table D.3, while they apply F-sharing at the input layer for RGB datasets. We mark with * (resp. \u2020) datasets preprocessed using YCoCg-R (resp. YCoCg). All results are averaged over 5 different runs.", "description": "This table compares the performance of QPCs and PCs on several image datasets, showing the bits-per-dimension (bpd) for various architectures.  It highlights that QPCs generally outperform PCs on these tasks, demonstrating the effectiveness of using QPCs materialized from PICs.", "section": "D.2 Additional distribution estimation results"}]