[{"type": "text", "text": "ECLipsE: Efficient Compositional Lipschitz Constant Estimation for Deep Neural Networks ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yuezhu Xu S. Sivaranjani Edwardson School of Industrial Engineering Edwardson School of Industrial Engineering Purdue University Purdue University West Lafayette, IN, USA West Lafayette, IN, USA xu1732@purdue.edu sseetha@purdue.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The Lipschitz constant plays a crucial role in certifying the robustness of neural networks to input perturbations. Since calculating the exact Lipschitz constant is NP-hard, efforts have been made to obtain tight upper bounds on the Lipschitz constant. Typically, this involves solving a large matrix verification problem, the computational cost of which grows significantly for both deeper and wider networks. In this paper, we provide a compositional approach to estimate Lipschitz constants for deep feed-forward neural networks. We first obtain an exact decomposition of the large matrix verification problem into smaller sub-problems. Then, leveraging the underlying cascade structure of the network, we develop two algorithms. The first algorithm explores the geometric features of the problem and enables us to provide Lipschitz estimates that are comparable to existing methods by solving small semidefinite programs (SDPs) that are only as large as the size of each layer. The second algorithm relaxes these sub-problems and provides a closed-form solution to each sub-problem for extremely fast estimation, altogether eliminating the need to solve SDPs. The two algorithms represent different levels of trade-offs between efficiency and accuracy. Finally, we demonstrate that our approach provides a steep reduction in computation time (as much as several thousand times faster, depending on the algorithm for deeper networks) while yielding Lipschitz bounds that are very close to or even better than those achieved by stateof-the-art approaches in a broad range of experiments\\*. In summary, our approach considerably advances the scalability and efficiency of certifying neural network robustness, making it particularly attractive for online learning tasks. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The Lipschitz constant, which quantifies how a neural networks output varies in response to changes in its inputs, is a crucial measure in providing robustness certificates [1, 2] on downstream tasks such as ensuring resilience against adversarial attacks [3, 4], stability of learning-based models or systems with neural network controllers [5\u20139], enhancing generalizability [10], improving gradientbased optimization methods and controlling the rate of learning [11][12]. The problem of calculating the exact Lipschitz constant is NP-hard [13]. Therefore, efforts have been made to estimate tight upper bounds for the Lipschitz constant of feed-forward neural networks (FNNs) [14\u201318] and other architectures such as convolutional neural networks (CNNs) [19\u201321]. Typical approaches include formulating a polynomial optimization problem [22] or bounding the Lipschitz constant via quadratic constraints and semidefinite programming (SDP) [14], which in turn requires solving a large-scale matrix verification problem whose computational complexity grows significantly with both the depth and width of the network. These approaches have also motivated the development of methods to design neural networks with certifiable robustness guarantees [19, 23\u201325]. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Contribution. In this paper, we provide a scalable compositional approach to estimate Lipschitz constants for deep feed-forward neural networks. We demonstrate steep reductions in computation time (as much as several thousand times faster than the state-of-the-art depending on the experiment), while obtaining Lipschitz estimates that are very close to or even better than those achieved by state-of-the-art approaches. Specifically, we develop two algorithms, representing different levels in the trade-off between accuracy and efficiency, allowing for application-specific choices. The first algorithm, ECLipsE, involves estimating the Lipschitz constant through a compositional layer-bylayer solution of small SDPs that are only as large as the weight matrix in each layer. The second algorithm, ECLipsE-Fast, provides a closed-form solution to estimate the Lipschitz constant, completely eliminating the need to solve any matrix inequality SDPs. Both algorithms provably guarantee the existence of solutions at each step to generate tight Lipschitz estimates. In summary, our work significantly advances scalability and efficiency in certifying neural network robustness, making it applicable to a variety of online learning tasks. ", "page_idx": 1}, {"type": "text", "text": "Theoretical Approach. We begin with the large matrix verification SDP for Lipschitz constant estimation under the well-known framework LipSDP [14]. To avoid handling a large matrix inequality, we employ a sequential Cholesky decomposition technique to obtain an exact decomposition of the large matrix verification problem into a series of smaller, more manageable sub-problems that are only as large as the size of the weight matrix in each layer. Then, observing the cascade structure of the neural network, we develop (i) algorithm ECLipsE, which characterizes the geometric features of the optimization problem and enables us to provide an accurate Lipschitz estimate and (ii) algorithm ECLipsE-Fast, which further relaxes the sub-problems, and yields a closed-form solution for each sub-problem that altogether eliminates the need to solve any SDPs, resulting in extremely fast implementations. ", "page_idx": 1}, {"type": "text", "text": "Related Work. The simplest way to estimate the Lipschitz constans is to provide a naive upper bound using the product of induced weight norms, which is rather conservative [26]. Another approach is to utilize automatic differentiation to approximate a bound, which is not a strict upper bound, although it is often so in practice [13]. Additionally, compositions of nonexpansive averaged operators and affine operators [16], Clarke Jacobian based approaches and other methods focusing on local Lipschitz constants [17][27] have also been studied. Recently, optimization-based approaches such as sparse polynomial optimization [22] and SDP methods such as the canonical LipSDP framework [14] have been successful in providing tighter Lipschitz bounds. SDP-based methods specifically exploit the slope-restrictedness of the activation functions to cast the problem of estimating a Lipschitz constant as a linear matrix verification problem. However, the computational cost of such methods explodes as the number of layers increases. A common strategy to address this is to ignore some coupling constraints among the neurons to reduce the number of decision variables, yielding a more scalable algorithm at the expense of estimation accuracy [14]. Another strategy is to exploit the sparsity of the SDP using graph-theoretic approaches to decompose it into smaller linear matrix inequalities (LMI) [15][28]. Along similar lines, [21] and [29] employ a dissipativitybased method and dynamic convolutional partition respectively to derive layer-wise LMIs that are applicable to both FNNs and CNNs. Very recent developments also focus on enhancing the scalability of SDP-based implementations through eigenvalue optimization and memory improvement [20], which are compatible with autodiff frameworks such as PyTorch and TensorFlow. ", "page_idx": 1}, {"type": "text", "text": "2 Problem Formulation and Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Notation. We define $\\mathbb{Z}_{N}=\\{1,\\dots,N\\}$ , where $N$ is a natural number excluding zero. A symmetric positive-definite matrix $P\\,\\in\\,\\mathbb{R}^{n\\times n}$ is represented as $P\\,>\\,0$ (and as $P\\ge0$ , if it is positive semidefinite). We denote the largest singular value or the spectral norm of matrix $A$ by $\\sigma_{m a x}(A)$ . The set of positive semi-definite diagonal matrices is written as $\\mathbb{D}_{+}$ . ", "page_idx": 1}, {"type": "text", "text": "2.1 Problem Formulation ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We consider a feedforward neural network (FNN) of $l$ layers with input $z\\in\\mathbb{R}^{d_{0}}$ and output $y\\in\\mathbb{R}^{d_{l}}$ defined as $y=f(z)$ . The function $f$ is recursively formulated with layers $\\mathbf{L}_{i},i\\in\\mathbb{Z}_{l}$ , defined as ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathbf{L}_{i}:\\boldsymbol{z}^{(i)}=\\phi(\\boldsymbol{v}^{(i)})\\quad\\forall i\\in\\mathbb{Z}_{l-1},\\quad\\mathbf{L}_{l}:\\boldsymbol{y}=\\boldsymbol{f}(\\boldsymbol{z})=\\boldsymbol{z}^{(l)}=\\boldsymbol{v}^{(l)},\\quad\\boldsymbol{z}^{(0)}=\\boldsymbol{z},\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $v^{(i)}=W_{i}z^{(i-1)}\\!+\\!b_{i}$ with $W_{i}$ and $b_{i}$ representing the weight and bias for layer $\\mathbf{L}_{i}$ respectively, and $\\phi:\\mathbb{R}^{d_{i}}\\rightarrow\\mathbb{R}^{d_{i}}$ is a nonlinear activation function that acts element-wise on its argument. The last layer $\\mathbf{L}_{l}$ is termed the output layer. We denote the number of neurons in layer $\\mathbf{L}_{i}$ by $d_{i}$ , $i\\in\\mathbb{Z}_{l}$ . ", "page_idx": 2}, {"type": "text", "text": "Definition 1. A function $f\\,:\\,\\mathbb{R}^{d_{0}}\\,\\rightarrow\\,\\mathbb{R}^{d_{l}}$ is Lipschitz continuous on $\\mathcal{Z}\\subseteq\\mathbb{R}^{d_{0}}$ if there exists $a$ constant $L>0$ such that $\\|f(z_{1})-f(z_{2})\\|_{2}\\leq L\\|z_{1}-z_{2}\\|_{2},\\forall z_{1},z_{2}\\in\\mathcal{Z}$ . The smallest positive $L$ satisfying this inequality is termed the Lipschitz constant of the function $f$ . ", "page_idx": 2}, {"type": "text", "text": "Without loss of generality, we assume $W_{i}\\neq0$ , $i\\in\\mathbf{Z}_{l}$ , as any weights being 0 will lead to the trivial case where the output corresponding to any input will remain the same after that layer. Our goal is to provide a scalable approach to give an efficient and accurate upper bound for the Lipschitz constant $L>0$ . Note that the proofs of all the theoretical results in this paper are included in Appendix A. ", "page_idx": 2}, {"type": "text", "text": "2.2 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We begin with a slope-restrictedness property satisfied by most activation functions, which is typically leveraged to to derive SDPs for Lipschitz certificates [14]. ", "page_idx": 2}, {"type": "text", "text": "Assumption 1 (Slope-restrictedness). For the neural network defined in (1), the activation function $\\phi$ is slope-restricted in $[\\alpha,\\beta]$ , $\\alpha\\,<\\,\\beta$ in the sense that $\\forall v_{1},v_{2}\\ \\in\\ \\mathbb{R}^{n}$ , we have $\\alpha(v_{1}\\mathrm{~-~}v_{2})\\;\\leq\\;$ $\\phi(v_{1})-\\bar{\\phi}(v_{2})\\leq\\beta(v_{1}-v_{2})$ element-wise. Consequently, we have that for $\\forall\\Lambda\\in\\mathbb{D}_{+}$ , ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\left[\\!\\!\\begin{array}{c c}{v_{1}-v_{2}}\\\\ {\\phi(v_{1})-\\phi(v_{2})}\\end{array}\\!\\!\\right]^{T}\\left[\\!\\!\\begin{array}{c c}{p\\Lambda}&{-m\\Lambda}\\\\ {-m\\Lambda}&{\\Lambda}\\end{array}\\!\\!\\right]\\left[\\!\\!\\begin{array}{c c}{v_{1}-v_{2}}\\\\ {\\phi(v_{1})-\\phi(v_{2})}\\end{array}\\!\\!\\right]\\leq0,\\quad p=\\alpha\\beta,\\quad m=(\\alpha+\\beta)/2.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Now, we can obtain an upper bound for the Lipschitz constant as follows; this result is equivalent to the well-known LipSDP framework [14]. ", "page_idx": 2}, {"type": "text", "text": "Theorem 1 (LipSDP). For the FNN (1) satisfying Assumption $^{\\,l}$ , if there exists $F>0$ and positive diagonal matrices $\\Lambda_{i}\\in\\mathbb{D}_{+}$ , $i\\in\\mathbb{Z}_{l-1}$ such that with $p=\\alpha\\beta$ and $\\begin{array}{r}{\\dot{m}=\\frac{\\alpha+\\beta}{2}}\\end{array}$ , ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left[{\\cal I}+p W_{1}^{T}\\Lambda_{1}W_{1}\\right.\\ }&{\\left.-m W_{1}^{T}\\Lambda_{1}\\right.\\ }&{0}\\\\ {\\left.-m\\Lambda_{1}W_{1}\\ }&{\\Lambda_{1}+p W_{2}^{T}\\Lambda_{2}W_{2}\\ }&{-m W_{2}^{T}\\Lambda_{2}\\right.}&{\\left.-\\right.}&{0}\\\\ {0}&{\\left.-m\\Lambda_{2}W_{2}\\ }&{\\Lambda_{2}+p W_{3}^{T}\\Lambda_{3}W_{3}\\right.}&{\\left.\\cdots\\ }&{0}\\\\ &{\\qquad\\qquad\\left.+\\right.}&{\\left.\\vdots}\\\\ {0}&{\\qquad\\qquad\\cdots\\ }&{\\left.-m\\Lambda_{l-2}W_{l-2}\\ }&{\\Lambda_{l-2}+p W_{l-1}^{T}\\Lambda_{l-1}W_{l-1}\\ }&{\\left.-m W_{l-1}^{T}\\Lambda_{l-1}\\right.}\\\\ {0}&{0}&{\\left.\\cdots\\ }&{-m\\Lambda_{l-1}W_{l-1}\\ }&{\\Lambda_{l-1}-F W_{i+1}^{T}W_{i+1}\\right]}\\end{array}\\right]>0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "then $\\left\\|\\boldsymbol{z}_{2}^{(l)}-\\boldsymbol{z}_{1}^{(l)}\\right\\|_{2}\\leq\\sqrt{1/F}\\left\\|\\boldsymbol{z}_{2}^{(0)}-\\boldsymbol{z}_{1}^{(0)}\\right\\|_{2}$ , which provides a sufficient condition for the Lipschitz consta nt $L$ to be u pper bounde d by $\\sqrt{1/F}$ . ", "page_idx": 2}, {"type": "text", "text": "Remark 1. LipSDP provides three variants that tradeoff accuracy and efficiency, namely, LipSDPNetwork, LipSDP-Neuron, and LipSDP-Layer, whose scalability increases sequentially at the expense of decreased accuracy. However, [30] provides a counterexample showing that the Lipschitz estimate from LipSDP-Network is not a strict upper bound; thus, only LipSDP-Neuron, and LipSDPLayer are valid. Theorem 1 here directly corresponds to LipSDP-Neuron. If all $\\Lambda_{i}$ , $i\\in\\mathbb{Z}_{l-1}$ in (3) are set to multiples of identity matrices, that is, $\\lambda_{i}I$ ${}_{i}I,\\,i\\in\\mathbb{Z}_{l-1}$ , then it corresponds to LipSDP-Layer. ", "page_idx": 2}, {"type": "text", "text": "Assumption 1 holds for all commonly used activation functions; for example, it holds with $\\alpha=0$ , $\\beta=1$ , that is, $p=0,m=1/2$ for the ReLU, sigmoid, tanh, exponential linear functions. Therefore, we focus on this case in this work. ", "page_idx": 2}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We now develop two fast compositional algorithms based on LipSDP-Layer and Lipschitz-Neuron respectively. Both algorithms are not only scalable and significantly faster, but also provide comparable estimates for the Lipschitz constant. ", "page_idx": 2}, {"type": "text", "text": "3.1 Exact Decomposition ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We circumvent direct solution of the large matrix inequality in (3), which becomes computationally prohibitive as the FNN (1) grows deeper. Instead, we develop a sequential block Cholesky decom", "page_idx": 2}, {"type": "text", "text": "position method, akin to the technique introduced in [31], also expanded in [32, 33]. We first restate Lemma 2 of [31] below. ", "page_idx": 3}, {"type": "text", "text": "Theorem 2 (Restatement of Lemma 2 of [31]). $A$ symmetric block tri-diagonal matrix defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\left[\\begin{array}{c c c c c c}{\\mathcal{P}_{1}}&{\\mathcal{R}_{2}}&{0}&{\\cdots}&&{0}\\\\ {\\mathcal{R}_{2}^{T}}&{\\mathcal{P}_{2}}&{\\mathcal{R}_{3}}&{\\cdots}&&{0}\\\\ {0}&{\\mathcal{R}_{3}^{T}}&{\\mathcal{P}_{2}}&{\\mathcal{R}_{3}}&{\\ldots}&{0}\\\\ &&{\\vdots}&&&\\\\ {0}&{\\cdots}&{0}&{\\mathcal{R}_{l-1}^{T}}&{\\mathcal{P}_{l-1}}&{\\mathcal{R}_{l}}\\\\ {0}&{\\cdots}&&{0}&{\\mathcal{R}_{l}^{T}}&{\\mathcal{P}_{l}}\\end{array}\\right]\\,,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "is positive definite if and only if $X_{i}>0,\\forall i\\in\\{0\\}\\cup\\mathbb{Z}_{l-1}$ , where ", "page_idx": 3}, {"type": "equation", "text": "$$\nX_{i}=\\left\\{\\mathcal{P}_{i}\\overset{}{\\longrightarrow}\\mathcal{V}_{i}\\right.-\\mathcal{R}_{i}^{T}X_{i-1}^{-1}\\mathcal{R}_{i}\\quad i f i\\in\\mathbb{Z}_{l-1}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Theorem 3. Let $P_{l}$ be defined as in (3) with $p=0,m=1/2$ . Then, the Lipschitz certificate $P_{l}>0$ holds if and only if the following sequence of matrix inequalities is satisfied: ", "page_idx": 3}, {"type": "equation", "text": "$$\nM_{i}>0,\\quad\\forall i\\in\\mathbb{Z}_{l-2},\\qquad M_{l-1}-F W_{l}^{T}W_{l}>0,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where ", "page_idx": 3}, {"type": "equation", "text": "$$\nM_{i}=\\left\\{{\\cal{I}}\\atop{\\Lambda_{i}-\\frac{1}{4}\\Lambda_{i}W_{i}(M_{i-1})_{l}^{-1}W_{i}^{T}\\Lambda_{i}}\\quad i\\in\\mathbb{Z}_{l-1}\\right..\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Theorem 3 provides an exact decomposition of (3), and allows us to establish necessary and sufficient conditions through small matrix inequalities that scale with the size of the weight matrices of each layer, rather than that of the entire network. To accurately estimate the Lipschitz constant, we need to decide on $\\Lambda_{i},i\\in\\mathbb{Z}_{1-1}$ that generate a tight upper bound at the last stage. In other words, we want $M_{l-1}-F W_{l}^{T}W_{l}>0$ to yield the smallest estimate for $\\sqrt{1/F}$ . In the following subsection, we provide compositional algorithms to decide the appropriate $\\dot{\\Lambda_{i}},i\\in\\mathbb{Z}_{1-1}$ sequentially, so that we only need to solve one small problem corresponding to each layer. ", "page_idx": 3}, {"type": "text", "text": "3.2 Compositional Algorithms ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We first propose two practical algorithms here. The theory supporting the algorithms and the geometric intuition are deliberately deferred, and will be thoroughly discussed in a the next subsection. ", "page_idx": 3}, {"type": "text", "text": "The first algorithm, ECLipsE, explores the geometric features that enables us to provide an accurate Lipschitz estimate by solving small semidefinite programs (SDPs), which are of the size of the weight matrices on each layer. The second algorithm, ECLipsE-Fast relaxes the sub-problems at each stage and yields a closed-form solution for each sub-problem that makes it extremely fast. These algorithms represent different trade-offs between efficiency and accuracy; one may choose ECLipsE if pursuing accuracy, and ECLipsE-Fast for applications where time is of the essence. ", "page_idx": 3}, {"type": "text", "text": "We observe in (7) that $M_{i}$ is obtained in a recursive manner and depends on $\\Lambda_{i}$ and $M_{i-1}$ , $i\\in\\mathbb{Z}_{l-1}$ . Therefore, we decide $\\Lambda_{i}$ and then calculate $M_{i}$ for $i\\in\\mathbb{Z}_{l-1}$ sequentially. Thus, these two algorithms can be implemented layer-by-layer in a compositional manner. ", "page_idx": 3}, {"type": "text", "text": "Concretely, for ECLipsE, we obtain $\\Lambda_{i}$ , $i\\in\\mathbb{Z}_{l-1}$ at each stage $i$ using the information from the next layer, i.e. $W_{i+1}$ , by solving the following small SDP: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{s.t.~}\\left[\\begin{array}{c c}{\\Lambda_{i}-c_{i}W_{i+1}^{T}W_{i+1}}&{\\frac{1}{2}\\Lambda_{i}(W_{i}(M_{i-1})^{-1}W_{i}^{T})^{\\frac{1}{2}}}\\\\ {\\frac{1}{2}(W_{i}(M_{i-1})^{-1}W_{i}^{T})^{\\frac{1}{2}}\\Lambda_{i}}&{I}\\end{array}\\right]>0,\\,\\,\\Lambda_{i}\\in\\mathbb{D}_{+},\\,\\,c_{i}>0\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "For ECLipsE-Fast, $\\Lambda_{i}$ is reduced to $\\lambda_{i}I$ ${}_{i}I,\\,i\\in\\mathbb{Z}_{l-1}$ and $\\lambda_{i}$ is calculated in closed-form as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\lambda_{i}=\\frac{2}{\\sigma_{m a x}\\left(W_{i}(M_{i-1})^{-1}W_{i}^{T}\\right)}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Note that this completely eliminates the need to solve matrix inequality $S D P\\mathbf{s}$ altogether. At last, after all \u039bi ${\\mathrm{s}},\\,i\\in\\mathbb{Z}_{l-1}$ are decided, we obtain the smallest $1/F$ , which yields the smallest Lipschitz estimate $L=\\sqrt{1/F}$ , as follows ", "page_idx": 3}, {"type": "equation", "text": "$$\n1/F=\\sigma_{m a x}\\left(W_{l}^{T}W_{l}(M_{l-1})^{-1}\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Remark 2. We choose to directly calculate the smallest $1/F$ rather than first derive the largest $F$ . This is because obtaining the largest $F$ first involves taking the inverse of $W_{l}^{T}W_{l}$ , which can cause numerical issues due to potential singularity of $W_{l}^{T}W_{l}$ . In contrast, directly calculating the smallest $1/F$ involves taking the inverses of $M_{l-1}$ , which is already guaranteed to be strictly positive definite at layer $l-1$ when deciding $\\Lambda_{l-1}$ . ", "page_idx": 4}, {"type": "text", "text": "We summarize the algorithms as one in Algorithm 1. Algorithms ECLipsE and ECLipsE-Fast are respectively preferable based on whether the priority is on accuracy or speed. ", "page_idx": 4}, {"type": "table", "img_path": "61YYSy078Z/tmp/45f2d0a7856ded3500fe77c694f74474dfc4ae47fa7bec379b7744e5577dc006.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "3.3 Theory ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Now we dive into the cascade structure of feed-forward neural networks and demonstrate the theory behind the two algorithms. We analyze the compositional algorithms in Section 3.2 in a backward manner, starting with the output layer. After all $\\Lambda_{i}$ , $i\\in\\mathbb{Z}_{l-1}$ are decided, $M_{i}>0$ , $i\\in\\mathbb{Z}_{l-2}$ hold. From Theorem 3, it remains to guarantee that $M_{l-1}-F W_{l}^{T}W_{l}>0$ , and consequently, (10), for which we state the following result. ", "page_idx": 4}, {"type": "text", "text": "Proposition 1. For given $\\Lambda_{i}$ , $i\\in\\mathbb{Z}_{l-1}$ that satisfies $M_{i}>0,$ , $i\\in\\mathbb{Z}_{l-2}$ , the tightest upper bound for Lipschitz constant is $L=\\sqrt{\\sigma_{m a x}\\left(W_{l}^{T}W_{l}(M_{l-1})^{-1}\\right)}$ . ", "page_idx": 4}, {"type": "text", "text": "Now, at stage $l-1$ , when deciding $\\Lambda_{l-1}$ , $\\Lambda_{i}$ , $i\\in\\mathbb{Z}_{l-2}$ are fixed and thus $M_{l-2}$ is fixed. According to Proposition 1, we would like to choose $\\Lambda_{l-1}$ such that $\\sigma_{m a x}\\left(W_{l}^{T}W_{l}(M_{l-1})^{-1}\\right)$ , where $M_{l-1}$ is a function of $\\Lambda_{l-1}$ , is as small as possible. We have the following result. ", "page_idx": 4}, {"type": "text", "text": "Lemma 1. If $M_{i}>0,$ , then $W_{i+1}^{T}W_{i+1}(M_{i})^{-1}$ and $W_{i+1}(M_{i})^{-1}(W_{i+1})^{T}$ share the same non-zero eigenvalues. ", "page_idx": 4}, {"type": "text", "text": "Note that at stage $i$ , it is guaranteed that $M_{i}\\;>\\;0$ . Taking $i\\,=\\,l\\,-\\,1$ , Lemma 1 infers that it is equivalent to minimize $\\bar{\\sigma_{m a x}}\\left(W_{l}(M_{l-1})^{-1}W_{l}^{T}\\right)$ when deciding on $\\Lambda_{l-1}$ . Note that $M_{l-1}\\,>\\,0$ , and consequently, the existence of $M_{l-1}^{-1}$ is already guaranteed when we reach the last stage. For the sake of conciseness, we define $\\mathcal{F}_{i}\\triangleq W_{i}(M_{i-1})^{-1}W_{i}^{T}\\quad i\\in\\mathbb{Z}_{l^{-}}$ . From (7), $\\begin{array}{r}{M_{i}=\\Lambda_{i}-\\frac{1}{4}\\Lambda_{i}\\mathcal{F}_{i}\\Lambda_{i}}\\end{array}$ . We further write out the recursive expression for ${\\mathcal{F}}_{i}$ as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{F}_{i+1}=W_{i+1}(M_{i})^{-1}W_{i+1}^{T}=\\left\\{W_{1}W_{1}^{T}\\begin{array}{l l}{i=0}&{i=0}\\\\ {W_{i+1}(\\Lambda_{i}-\\frac{1}{4}\\Lambda_{i}\\mathcal{F}_{i}\\Lambda_{i})^{-1}W_{i+1}^{T}}&{i\\in\\mathbb{Z}_{l-1}}\\end{array}.\\right.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Lemma 2. For any constant $\\gamma\\,\\in\\,(0,1)$ , any $\\Lambda_{i}\\,\\in\\,\\mathbb{D}_{+}$ that satisfies $M_{i}\\,=\\,\\Lambda_{i}\\,-\\,{\\textstyle\\frac{1}{4}}\\Lambda_{i}\\mathcal{F}_{i}\\Lambda_{i}\\,>\\,0$ is also a feasible solution for $\\tilde{M}_{i}\\,\\triangleq\\,\\Lambda_{i}\\,-\\,{\\textstyle\\frac{1}{4}}\\Lambda_{i}(\\gamma\\mathcal{F}_{i})\\Lambda_{i}\\,>\\,0$ . In other words, the feasible region $\\{\\Lambda_{i}:M_{i}>0,\\Lambda_{i}\\in\\mathbb{D}_{+}\\}\\subseteq\\{\\Lambda_{i}:\\tilde{M}_{i}>0,\\Lambda_{i}\\in\\mathbb{D}_{+}\\}$ . ", "page_idx": 4}, {"type": "text", "text": "Lemma 2 gives us the observation that a contraction ${\\mathcal F}_{i}\\;\\to\\;\\gamma{\\mathcal F}_{i},\\gamma\\;\\in\\;(0,1)$ yields a larger feasible space for $\\Lambda_{i}\\ \\in\\ \\mathbb{D}_{+}$ to ensure $M_{i}~>~0$ . Meanwhile, (11) shows that for any given $\\Lambda_{i}$ , a smaller ${\\mathcal{F}}_{i}$ leads to a smaller ${\\mathcal{F}}_{i+1}$ for the next stage. We can characterize how \u2018small\u2019 ${\\mathcal{F}}_{i}$ is by its spectral norm $\\sigma_{m a x}(\\mathcal{F}_{i})$ . Then, minimizing $\\sigma_{m a x}(\\mathcal{F}_{i})$ aligns with our goal of minimizing $\\sigma_{m a x}\\left(W_{l}^{T}W_{l}(M_{\\underline{{{l}}}-1})^{-1}\\right)\\ =\\ \\sigma_{m a x}\\left(W_{l}(M_{l-1})^{-1}W_{l}^{T}\\right)\\ =\\ \\sigma_{m a x}(\\mathcal{F}_{l})$ at the last stage. In other words, a smaller ${\\mathcal{F}}_{1}$ at the start will generally translate to a tighter Lipschitz estimate at output layer if we always choose to minimize the spectral norm $\\sigma_{m a x}(F_{i})$ at each stage. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Now we focus on how to specifically optimize $\\Lambda_{i}$ , $i\\in\\mathbb{Z}_{l-1}$ . At stage $i$ , the goal is to seek for the $\\Lambda_{i}$ that minimizes $\\sigma_{m a x}(\\mathcal{F}_{i+1})$ , where $\\begin{array}{r}{\\mathcal{F}_{i+1}=W_{i+1}(\\Lambda_{i}-\\frac{1}{4}\\Lambda_{i}\\mathcal{F}_{i}\\Lambda_{i})^{-1}W_{i+1}^{T}}\\end{array}$ as in (11). Note that $M_{i-1}$ and ${\\mathcal{F}}_{i}$ are already fixed and can be regarded as constants at the $i$ -th stage. ", "page_idx": 5}, {"type": "text", "text": "Proposition 2. If there exists a singular matrix $N\\ge0$ such that $M_{i}\\,=\\,c_{i}W_{i+1}^{T}W_{i+1}+N_{i}$ , with constant $c_{i}>0$ , then $\\sigma_{m a x}(\\mathcal{F}_{i+1})=1/c_{i},\\,\\forall i\\in\\mathcal{Z}_{l-1}$ . ", "page_idx": 5}, {"type": "text", "text": "In other words, we need to find the largest $c_{i}\\,>\\,0$ to minimize $\\sigma_{m a x}(\\mathcal{F}_{i+1})\\,=\\,1/c_{i}$ . Recall that $\\begin{array}{r}{M_{i}=\\Lambda_{i}-\\frac{1}{4}\\Lambda_{i}\\mathcal{F}_{i}\\Lambda_{i}}\\end{array}$ is a function of $\\Lambda_{i}$ . We state the following proposition that is used to derive the small sub-problems at each stage. ", "page_idx": 5}, {"type": "text", "text": "Proposition 3. Consider the following optimization problem for $\\forall i\\in\\mathcal{Z}_{l-1}$ . ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{c_{i}}\\quad c_{i}\\quad s.t.\\quad\\quad\\Lambda_{i}-\\frac{1}{4}\\Lambda_{i}W_{i}(M_{i-1})^{-1}W_{i}^{T}\\Lambda_{i}-c_{i}(W_{i+1}^{T}W_{i+1})>0,\\quad\\Lambda_{i}\\in\\mathbb{D}_{+},\\quad c_{i}>0\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Then, the optimal value $c_{i}$ is the largest constant such that $M_{i}$ can be written as $\\begin{array}{r l}{M_{i}}&{{}=}\\end{array}$ $c_{i}W_{i+1}^{T}W_{i+1}\\bar{+}\\ N_{i}$ , where $N$ is some singular matrix such that $N~\\ge~0$ . Moreover, the feasible region for the optimization problem is always nonempty. ", "page_idx": 5}, {"type": "text", "text": "Geometric Analysis: We illustrate the process of achieving the largest $c_{i}>0$ in Fig. 1. We geometrically represent a positive semidefinite matrix by the ellipsoid generated by the transformation of a unit ball in the Euclidean space by the matrix. For simplicity of exposition, we refer to this ellipsoid as the \u2018shape\u2019 of the matrix. We plot the shapes of $M_{i}$ and $W_{i+1}^{T}W_{i+1}$ in green and blue, respectively, in 2D. The positive definiteness of the constraint in (12) is equivalent to the ellipsoid of $\\bar{W}_{i+1}^{T}\\bar{W_{i+1}}$ being contained in the ellipsoid corresponding to $M_{i}/c_{i}$ . Specifically, when $c_{i}>1$ , Fig. 1a demonstrates the maximum contraction of $M_{i}$ , corresponding to the largest $c_{i}$ , such that ellipsoid of $W_{i+1}^{T}W_{i+1}$ is still contained in ellipsoid of $c_{i}M_{i}$ . Similarly, for the case where $c_{i}<1$ , Fig. 1b demonstrates the minimum extent (the smallest $1/c_{i}$ ) to which $M_{i}$ needs to expand, such that the ellipsoid of $W_{i+1}^{T}W_{i+1}$ is contained. Algebraically, in both cases, $c_{i}$ is the ratio of the lengths of the green and pink arrows. By Proposition 2, the resulting ellipsoid (depicted in pink) is $M_{i}\\breve{/}c_{i}=W_{i+1}^{T}W_{i+1}+\\dot{N}/c_{i}$ for both cases, and is tangent to the ellipsoid of $W_{i+1}^{T}\\bar{W}_{i+1}$ . Moreover, the vector pointing from the origin to the tangency point aligns with the direction of eigenvectors (the grey vector $v$ in the plots) corresponding to the zero eigenvalues of the singular matrix $N\\geq0$ . ", "page_idx": 5}, {"type": "image", "img_path": "61YYSy078Z/tmp/16e25981caf5e26344c1ac498050031e900bc12475f47d36444ccbcbf04899cc.jpg", "img_caption": ["Figure 1: Geometric Analysis of ECLipsE "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Combining Proposition 2 and 3, we can derive an optimization problem to sequentially find the appropriate $\\Lambda_{i}$ , $i\\in\\mathbb{Z}_{l-1}$ . The first constraint in (12) is quadratic in $\\Lambda_{i}$ , which makes it unattractive for practical purposes. Therefore, we apply the Schur Complement to transform it into the linear matrix inequality (LMI) constraint in (8). Thus, the optimization problem in Proposition 3 becomes equivalent to the SDP in (8), yielding algorithm ECLipsE. Notice that there are several ways to write the Schur complement of the constraint in (12). We choose this specific structure to avoid singularity of the diagonal entries and ensure positive definiteness. ", "page_idx": 5}, {"type": "text", "text": "ECLipsE-Fast achieves remarkable speed by further reducing $\\Lambda_{i}$ , $i\\;\\in\\;\\mathbb{Z}_{l-1}$ to a multiple of identity matrix $\\lambda_{i}I$ , where $\\lambda_{i}\\,>\\,0$ , and by relaxing the sub-problems. While our goal remains to minimize $\\sigma_{m a x}(\\mathcal{F}_{i+1})=\\sigma_{m a x}$ $\\begin{array}{r}{\\bigl(\\dot{W_{i+1}}\\bigl(\\dot{\\Lambda_{i}}-\\frac{1}{4}\\Lambda_{i}\\check{\\mathcal{F}}_{i}\\Lambda_{i}\\bigr)^{-1}\\dot{W_{i+1}^{T}}\\bigr)}\\end{array}$ , we intentionally disregard information from $W_{i+1}$ , and instead focus solely on minimizing the spectral norm of $\\begin{array}{r}{(\\Lambda_{i}-\\frac{1}{4}\\Lambda_{i}\\mathcal{F}_{i}\\Lambda_{i})^{-1}}\\end{array}$ . ", "page_idx": 5}, {"type": "text", "text": "Roughly speaking, a smaller $\\begin{array}{r l}{~}&{{}\\sigma_{m a x}\\left((\\Lambda_{i}-\\frac{1}{4}\\Lambda_{i}\\mathcal{F}_{i}\\Lambda_{i})^{-1}\\right)}\\end{array}$ yields a smaller $\\sigma_{m a x}(\\mathcal{F}_{i+1})$ . This relaxation allows us to derive a closed-form solution for $\\Lambda_{i}$ , $i\\in\\mathbb{Z}_{l-1}$ as follows. ", "page_idx": 6}, {"type": "text", "text": "Proposition 4. Choosing $\\begin{array}{r}{\\lambda_{i}=\\frac{2}{\\sigma_{m a x}(\\mathcal{F}_{i})}>0}\\end{array}$ minimizes $\\sigma_{m a x}$ $\\begin{array}{r}{\\left((\\Lambda_{i}-\\frac{1}{4}\\Lambda_{i}\\mathcal{F}_{i}\\Lambda_{i})^{-1}\\right)}\\end{array}$ where $\\Lambda_{i}={}$ $\\lambda_{i}I$ under the constraint that $M_{i}=\\Lambda_{i}-\\textstyle{\\frac{1}{4}}\\Lambda_{i}\\mathcal{F}_{i}\\Lambda_{i}>0$ . Moreover, this closed-form solution for $\\lambda_{i}$ always satisfies $M_{i}>0,$ , $i\\in\\mathbb{Z}_{l-1}$ . ", "page_idx": 6}, {"type": "text", "text": "By the definition of ${\\mathcal{F}}_{i}$ , Proposition 4 matches with (9), yielding algorithm ECLipsE-Fast. Although this relaxation may result in a loss of tightness, the closed-form solution offers the advantage of significantly increased computational speed. ", "page_idx": 6}, {"type": "text", "text": "Geometric Analysis: We now demonstrate the geometric analysis behind the development of ECLipsE-Fast and compare it with ECLipsE in the case where $c_{i}~>~1$ (Fig. 2). We also include the case $c_{i}<1$ in Appendix B. The key idea behind ECLipsE-Fast is that instead of keeping the shape of $M_{i}$ fixed, and contracting the ellipsoid itself, as in ECLipsE, we first find the largest inscribed ball (dark green) for the ellipsoid of $M_{i}$ . Then, we contract this ball to the maximum extent such that it still contains $W_{i+1}^{T}\\bar{W}_{i+1}$ . The resulting ball (dark blue) is precisely the smallest circumscribing ball for the ellipsoid of $W_{i+1}^{T}W_{i+1}$ . Note that this approach serves as an approximation for the process of contraction depicted in Fig. 7b (corresponding to ECLipsE), thus yielding a smaller $c_{i}$ . We use this approximation to achieve a closed-form solution, which significantly increases the computational speed. ", "page_idx": 6}, {"type": "image", "img_path": "61YYSy078Z/tmp/9e5ce048167fef77ebbdb0894982276005ccb8e5c6b763928ed721fa240c684b.jpg", "img_caption": ["(a) Geometric Intuition of ECLipsE-Fast "], "img_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "61YYSy078Z/tmp/b668018831034bcfae973985e7430d5d3aab6d85be653c00f1e8bad1a05e15e0.jpg", "img_caption": ["Figure 2: Comparison between ECLipsE-Fast and ECLipsE with $c_{i}>1$ ", "(b) Geometric Intuition of ECLipsE "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Remark 3. In Lemma 2, the analysis initially fixes the shape of ${\\mathcal{F}}_{i}$ . However, when optimizing $\\Lambda_{i}$ , the shape of the feasible region depends on ${\\mathcal{F}}_{i}$ , which can vary with different $\\Lambda_{i-1}$ , $i\\in\\mathbb{Z}_{l}$ . Thus, this approximation, which allows for a scalable distributed algorithm to solve the centralized problem (3) introduces an unavoidable but minor tradeoff in achieving global optimality. ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We implement our algorithms\u2020 on randomly generated neural networks and ones trained on the MNIST dataset. The details of the experimental setup, and training of the neural networks (both randomly generated and trained on the MNIST dataset) are described in Appendix D. ", "page_idx": 6}, {"type": "text", "text": "Baselines.\u2021 For ECLipsE, $\\Lambda_{i}$ , $i\\in\\mathrm{~\\mathbb{Z}}_{l-1}$ can have different diagonal entries, which benchmarks to LipSDP-Neuron. For ECLipsE-Fast, $\\Lambda_{i}~=~\\lambda_{i}I$ , $i\\in\\mathrm{~\\mathbb{Z}}_{l-1}$ , which benchmarks to LipSDP-Layer. Additionally, we compare our Lipschitz estimates to the naive upper bound $\\begin{array}{r}{L_{n a i v e}=\\prod_{i=1}^{l}\\|W_{i}\\|_{2}[26]}\\end{array}$ , CPLip [16] and LipDiff [20]. The codes for these baselines are available at [34 , 35, 20]. Note that LipDiff is accelerated using a node with 2 NVIDIA A100 GPUs (80G) and $512\\,\\mathrm{GB}$ of memory. ", "page_idx": 6}, {"type": "text", "text": "4.1 Randomly Generated Neural Networks ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We first consider randomly generated networks, where the number of layers are chosen from $\\{2,5,10,20,30,50,75,100\\}$ , and number of neurons are chosen from $\\{20,40\\overset{.}{,}60,80,100\\}$ , amounting to a total of 40 experiments for each algorithm (including the baselines). We quantify the computation time and tightness of the Lipschitz bounds (raw data in Appendix E). The Lipschitz bounds presented in the following figures are normalized to the trivial upper bound for ease of comparison. ", "page_idx": 6}, {"type": "text", "text": "Case 1: Varying network depth (number of layers). We select a network with 80 neurons per layer, and demonstrate the scalability of our algorithm as network depth increases. Note that all baseline approaches fail to provide a Lipschitz estimate within a computational cutoff time of 15 min for networks larger than this size (see results in Appendix E). As the number of layers increases, the computation time for CPLip algorithm explodes (the algorithm does not return a Lipschitz estimate within the cutoff time beyond 20 layers); however, CPLip provides the most accurate estimates in smaller networks. LipDiff provides inadmissible Lipschitz estimates even for moderate networks, returning as much as 10-100 times the trivial bound (see Table 2a, Appendix E for the estimates). Also, while LipDiff has similar computational time for smaller networks, computational time grows for deeper networks as recorded in Appendix E Table 2b. Consequently, we do not include these results in the plots. LipSDP-Neuron and LipSDP-Layer are also scalable to some extent; however, they fail for a networks of 30 and 50 layers respectively. In contrast, the computation time for ECLipsE and ECLipsE-Fast stays low and grows only linearly with respect to the number of layers (Fig. 3b). Notably, ECLipsE-Fast is significantly faster (thousands of times) than LipSDP-Layer, owing to the closed-form solution at each stage, while ECLipsE is also considerably faster than LipSDP-Neuron. The Lipschitz estimates given by algorithms ECLipsE and ECLipsE-Fast are very close to the ones from LipSDP-Neuron and LipSDP-Layer respectively (Fig. 3a), and outperform the trivial bound. As the number of layers increases, the normalized Lipschitz estimates are smaller, indicating that our algorithms are well-suited to very deep networks. ", "page_idx": 7}, {"type": "image", "img_path": "61YYSy078Z/tmp/0a67cbb2af3a3577930b90c9d9bfccbf2bb397120bdee0d97a14c635e5effa56.jpg", "img_caption": ["(a) Lipschitz estimates normalized to trivial bound "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "61YYSy078Z/tmp/652319c8e9b82f49ca011e4db7bcb8aedc5a709e40060430598c5dc1d97e9b8c.jpg", "img_caption": ["Figure 3: Performance of ECLipsE-Fast and ECLipsE, with respect to baselines for increasing network depth, with 80 neurons per layer. The red x markings indicate that the algorithm fails to provide an estimate within the computational cutoff time beyond this network size. ", "(b) Computation time (seconds) "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Case 2: Varying neural network width (number of neurons per layer). We now examine the performance of our algorithms for wider (more hidden neurons per layer), rather than deeper networks (with more layers), and demonstrate the results for networks with 20 and 50 layers respectively (Fig. 4). While the complete raw data is presented in Appendix E, we discuss the results for 20 and 50 layer networks here, since they represent the network sizes where different baselines fail to return Lipschitz estimates beyond the computation cutoff time of $15~\\mathrm{min}$ . Note that while LipDiff also manages to generate estimates for all network sizes in our 50 layers case, it once again provides inadmissible Lipschitz constants, returning as much as $10^{4}-10^{6}$ times the trivial bound. Therefore, we do not include these results in Fig. 4 (see Tables 3a and 3b in Appendix E for the estimates and computation time.) We can observe from Figs. 4b and 4d that the computation time needed for CPLip, LipSDP-Layer, and LipSDP-Neuron significantly increases with the number of neurons, while the computation time of our method still grows linearly. Meanwhile, the Lipschitz estimates from algorithms ECLipsE and ECLipsE-Fast are close to the ones from LipSDP-Neuron and LipSDP-Layer respectively (Figs. 4a and 4c). Thus, we can conclude that our method significantly improves scalability for wider neural networks. ", "page_idx": 7}, {"type": "text", "text": "Case 3: Comparison with LipSDP implementations. In order to address the scalability issue as the size of the network grows, LipSDP utilizes a splitting approach, where the network is split into smaller sub-networks and the Lipschitz constants for each sub-network are composed at the end to obtain the final estimate. We benchmark our approach with respect to the performance of LipSDPLayer and LipSDP-Neuron considering different sub-network sizes. Note that our algorithms do not require any splitting, since they remain scalable to large networks. As the FNNs are larger than the ones in previous cases, we change the cutoff time to 30 minutes. We conduct two sets of experiments ", "page_idx": 7}, {"type": "image", "img_path": "61YYSy078Z/tmp/f35cc22280ba2c0111c230bb3d958771631ca618b746baa3f840df3d0e2cc7a8.jpg", "img_caption": ["(a) Lipschitz estimates normalized to trivial bound "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "61YYSy078Z/tmp/4b2279c2e1f6f7461ce903b347863e7ce78497047de1259b70840810673dfdad.jpg", "img_caption": ["(b) Computation time (seconds) "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "61YYSy078Z/tmp/b0a0219177da5b510b77122f9f96551b6af9193e38c24e7939daf2fa940be4b5.jpg", "img_caption": ["(c) Lipschitz estimates normalized to trivial bound "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "61YYSy078Z/tmp/01c8fd954f0fed48d5b49b19812bba32b28511a7a8f68e889402f51f26d23865.jpg", "img_caption": ["(d) Computation time (seconds) "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 4: Performance of ECLipsE-Fast and ECLipsE with respect to baselines as network width increases, for a randomly generated network with 20 layers ((a) and (b)) and 50 layers ((c) and (d)). The Red x markings indicate that the algorithms fail to provide an estimate within the computational cutoff time of $15\\;\\mathrm{min}$ beyond this network size. ", "page_idx": 8}, {"type": "text", "text": "to study how our algorithms perform on considerably deep neural networks and how network width affects these results. ", "page_idx": 8}, {"type": "text", "text": "In the first set of experiments, we consider FNNs with 100 layers, with the number of neurons chosen from the set {80,100,120,140,160}. The splitting sizes for LipSDP-Neuron and LipSDP-Layer are 3, 5 and 10. We represent different FNN sizes by shapes and different algorithms by the color in ", "page_idx": 8}, {"type": "image", "img_path": "61YYSy078Z/tmp/d01d65cee2469d2ec73ef5d3a8a4ca35fb843efc1dc9d793744602306cf18c81.jpg", "img_caption": ["Figure 5: Computation time vs estimation accuracy for ECLipsE, ECLipsE-Fast and LipSDP splitting with different sub-network sizes. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Fig. 5. By plotting the normalized Lipschitz estimates and computation times on the two axes, we illustrate how efficient and accurate an algorithm is by how close the corresponding data point is to the origin. We observe that all the data points for ECLipsE-Fast are at the leftmost extreme of the plot, indicating that it is the most efficient algorithm. Further, ECLipsE-Fast also outperforms the red cluster (LipSDP-layer with the network split into 3) in both tightness and speed. Comparing data points of the same shape, ECLipsE-Fast outperforms LipSDPLayer for all sub-network splits both in terms of the Lipschitz estimate and the computation time. Finally, the data points corresponding to ECLipsE are clustered at the bottom left, demonstrating that it is relatively more accurate and efficient than all LipSDP methods, no matter how the network is split. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "In the second set of experiments, we explore even wider networks. Specifically, we choose a fairly deep neural network with 50 layers and vary the width from 150 to 1000. The splitting size for LipSDP-Neuron and LipSDP-Layer is 5. The resulting Lipschitz estimates (normalized with respect to trivial upper bounds) and the computation time are provided in Tables 4a and 4b of Appendix ", "page_idx": 8}, {"type": "text", "text": "E due to space limitations. From these results, we observe that ECLipsE-Fast is extremely fast even for very wide networks, with a running time of only 15.63 seconds for a network width of 1000, while the computation time for LipSDP-Layer grows significantly. Also, while ECLipsE fails when the width reaches 300, it is comparable to LipSDP-Neuron split into 5 sub-networks in terms of time performance. ", "page_idx": 9}, {"type": "text", "text": "Remark 4. We notice that when the neural networks are significantly wide, ECLipsE takes more than 30 minutes while ECLipsE-Fast remains efficient. This observation can be explained by examining the computational complexity of these algorithms. Note that we directly state the computational complexity of each algorithm here for brevity; the detailed derivations are included in Appendix C. Suppose a neural network has $n$ hidden layers with $m$ neurons. Then, the computational cost for LipSDP and ECLipsE are $O(n^{4}m^{4})$ and $\\stackrel{.}{O}(n m^{4})$ respectively. We can observe that the complexity is significantly decreased in terms of the depth, but is the same in terms of the width, immediately indicating the advantage for deep networks. Nevertheless, as $m$ grows, the difference between $O\\!\\left(n^{4}m^{4}\\right)$ and $O(n m^{4})$ is still drastically enhanced, especially with large $n$ . More importantly, for ECLipsE-Fast, the computational cost drops to $\\dot{O}(n m^{3})$ . This is the fastest one can expect if the weights on each layer are treated as a whole. ", "page_idx": 9}, {"type": "text", "text": "4.2 Neural Networks Trained on MNIST. ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We now demonstrate our algorithms on four networks trained on the MNIST dataset (see Appendix D for details) to achieve an accuracy of at least $97\\%$ . The resulting networks are not very deep (3 layers), with 100, 200, 300, and 400 neurons. We set a computational cutoff time of $30\\ \\mathrm{min}$ to obtain Lipschitz estimates. As described in the note on Baselines earlier in this section, ECLipsE is benchmarked against LipSDP-Neuron and ECLipsE-Fast is benchmarked against the faster LipSDP-Layer due to their mathematical structure. From Fig. 6b, we can see that ECLipsE-Fast is significantly faster than LipSDP-Layer, while ECLipsE is also considerably faster than LipSDPNeuron. Note that all algorithms provide very similar Lipschitz estimates (Fig. 6a). Therefore, for networks that are not very deep, such as those in this example, ECLipsE-Fast is the optimal choice, since it significantly outperforms all algorithms in terms of speed, while the approximation error due to the closed-form solution is not too significant compared to the baselines. ", "page_idx": 9}, {"type": "image", "img_path": "61YYSy078Z/tmp/6e213576c46d714ccaf4168bf6622d53235da70703dd6fff9cfb0c6304456a5a.jpg", "img_caption": ["(a) Lipschitz estimates normalized to trivial bound "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "61YYSy078Z/tmp/305f79e2268ff080219aa784ae0437dc77713d25cb85c4b39d3ff7c9e6adfbef.jpg", "img_caption": ["(b) Computation time (seconds) "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Figure 6: Performance of ECLipsE-Fast and ECLipsE, with respect to baselines for increasing number of neurons, for a 3-layer network trained on MNIST. The red x markings indicate that the algorithm fails to provide an estimate within the computational cutoff time beyond this network size. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We propose a scalable approach to estimate Lipschitz constants for deep neural networks by developing a new matrix decomposition that yields two fast algorithms. Our experiments demonstrate that our algorithms significantly outperform the state-of-the-art in terms of computation speed, while providing comparable Lipschitz estimates. We envision that further computational speedup can be achieved through sparse matrix multiplication and eigenvalue estimation techniques, and leveraging autodiff frameworks, along the lines of [20]. While we can unroll the convolutional layers in CNN structure to a large fully connected neural network layer to apply ECLipsE and ECLipsE-Fast to estimate Lipschitz constant, better compositional methods that are tailored to feature the convolutional layers are expected for future work. Similarly, other architectures, such as residual networks, present additional challenges due to their unique structures and will be considered in future research. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgment. This work was partially supported by the Air Force Office of Scientific Research grant, FA9550-23-1-0492. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] M. Fazlyab, M. Morari, and G. J. Pappas, \u201cSafety verification and robustness analysis of neural networks via quadratic constraints and semidefinite programming,\u201d IEEE Transactions on Automatic Control, vol. 67, no. 1, pp. 1\u201315, 2020.   \n[2] T. Miyato, T. Kataoka, M. Koyama, and Y. Yoshida, \u201cSpectral normalization for generative adversarial networks,\u201d in International Conference on Learning Representations, 2018.   \n[3] C. Finlay, A. M. Oberman, and B. Abbasi, \u201cImproved robustness to adversarial examples using lipschitz regularization of the loss,\u201d 2018.   \n[4] Y. Tsuzuku, I. Sato, and M. Sugiyama, \u201cLipschitz-margin training: Scalable certification of perturbation invariance for deep neural networks,\u201d Advances in neural information processing systems, vol. 31, 2018.   \n[5] L. Brunke, M. Greeff, A. W. Hall, Z. Yuan, S. Zhou, J. Panerati, and A. P. Schoellig, \u201cSafe learning in robotics: From learning-based control to safe reinforcement learning,\u201d Annual Review of Control, Robotics, and Autonomous Systems, vol. 5, pp. 411\u2013444, 2022.   \n[6] H. Yin, P. Seiler, and M. Arcak, \u201cStability analysis using quadratic constraints for systems with neural network controllers,\u201d IEEE Transactions on Automatic Control, vol. 67, no. 4, pp. 1980\u2013 1987, 2021.   \n[7] A. Aswani, H. Gonzalez, S. S. Sastry, and C. Tomlin, \u201cProvably safe and robust learning-based model predictive control,\u201d Automatica, vol. 49, no. 5, pp. 1216\u20131226, 2013.   \n[8] W. G. Y. Tan and Z. Wu, \u201cRobust machine learning modeling for predictive control using lipschitz-constrained neural networks,\u201d Computers & Chemical Engineering, vol. 180, p. 108466, 2024.   \n[9] Y. Xu and S. Sivaranjani, \u201cLearning dissipative neural dynamical systems,\u201d IEEE Control Systems Letters, vol. 7, pp. 3531\u20133536, 2023.   \n[10] P. L. Bartlett, D. J. Foster, and M. J. Telgarsky, \u201cSpectrally-normalized margin bounds for neural networks,\u201d Advances in neural information processing systems, vol. 30, 2017.   \n[11] H. Zhang, Z. Zheng, and J. Lavaei, \u201cGradient-based algorithms for convex discrete optimization via simulation,\u201d Operations research, vol. 71, no. 5, pp. 1815\u20131834, 2023.   \n[12] C. Herrera, F. Krach, and J. Teichmann, \u201cEstimating full lipschitz constants of deep neural networks,\u201d stat, vol. 1050, p. 8, 2020.   \n[13] A. Virmaux and K. Scaman, \u201cLipschitz regularity of deep neural networks: analysis and efficient estimation,\u201d Advances in Neural Information Processing Systems, vol. 31, 2018.   \n[14] M. Fazlyab, A. Robey, H. Hassani, M. Morari, and G. Pappas, \u201cEfficient and accurate estimation of lipschitz constants for deep neural networks,\u201d Advances in Neural Information Processing Systems, vol. 32, 2019.   \n[15] A. Xue, L. Lindemann, A. Robey, H. Hassani, G. J. Pappas, and R. Alur, \u201cChordal sparsity for lipschitz constant estimation of deep neural networks,\u201d in 2022 IEEE 61st Conference on Decision and Control (CDC), pp. 3389\u20133396, IEEE, 2022.   \n[16] P. L. Combettes and J.-C. Pesquet, \u201cLipschitz certificates for layered network structures driven by averaged activation operators,\u201d SIAM Journal on Mathematics of Data Science, vol. 2, no. 2, pp. 529\u2013557, 2020.   \n[17] M. Jordan and A. G. Dimakis, \u201cExactly computing the local lipschitz constant of relu networks,\u201d Advances in Neural Information Processing Systems, vol. 33, pp. 7344\u20137353, 2020.   \n[18] Z. Wang, G. Prakriya, and S. Jha, \u201cA quantitative geometric approach to neural-network smoothness,\u201d Advances in Neural Information Processing Systems, vol. 35, pp. 34201\u201334215, 2022.   \n[19] M. Fazlyab, T. Entesari, A. Roy, and R. Chellappa, \u201cCertified robustness via dynamic margin maximization and improved lipschitz regularization,\u201d Advances in Neural Information Processing Systems, vol. 36, 2024.   \n[20] Z. Wang, B. Hu, A. J. Havens, A. Araujo, Y. Zheng, Y. Chen, and S. Jha, \u201cOn the scalability and memory efficiency of semidefinite programs for lipschitz constant estimation of neural networks,\u201d in The Twelfth International Conference on Learning Representations, 2024.   \n[21] P. Pauli, D. Gramlich, and F. Allg\u00f6wer, \u201cLipschitz constant estimation for 1d convolutional neural networks,\u201d in Learning for Dynamics and Control Conference, pp. 1321\u20131332, PMLR, 2023.   \n[22] F. Latorre, P. Rolland, and V. Cevher, \u201cLipschitz constant estimation of neural networks via sparse polynomial optimization,\u201d arXiv preprint arXiv:2004.08688, 2020.   \n[23] R. Wang and I. Manchester, \u201cDirect parameterization of lipschitz-bounded deep networks,\u201d in International Conference on Machine Learning, pp. 36093\u201336110, PMLR, 2023.   \n[24] A. Araujo, A. Havens, B. Delattre, A. Allauzen, and B. Hu, \u201cA unified algebraic perspective on lipschitz neural networks,\u201d arXiv preprint arXiv:2303.03169, 2023.   \n[25] A. Havens, A. Araujo, S. Garg, F. Khorrami, and B. Hu, \u201cExploiting connections between lipschitz structures for certifiably robust deep equilibrium models,\u201d Advances in Neural Information Processing Systems, vol. 36, 2024.   \n[26] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus, \u201cIntriguing properties of neural networks,\u201d arXiv preprint arXiv:1312.6199, 2013.   \n[27] Z. Shi, Y. Wang, H. Zhang, J. Z. Kolter, and C.-J. Hsieh, \u201cEfficiently computing local lipschitz constants of neural networks via bound propagation,\u201d Advances in Neural Information Processing Systems, vol. 35, pp. 2350\u20132364, 2022.   \n[28] M. Newton and A. Papachristodoulou, \u201cExploiting sparsity for neural network verification,\u201d in Learning for Dynamics and Control, pp. 715\u2013727, PMLR, 2021.   \n[29] Y. Sulehman and T. Mu, \u201cScalable lipschitz estimation for cnns,\u201d arXiv preprint arXiv:2403.18613, 2024.   \n[30] P. Pauli, A. Koch, J. Berberich, P. Kohler, and F. Allg\u00f6wer, \u201cTraining robust neural networks using lipschitz bounds,\u201d IEEE Control Systems Letters, vol. 6, pp. 121\u2013126, 2021.   \n[31] E. Agarwal, S. Sivaranjani, V. Gupta, and P. Antsaklis, \u201cSequential synthesis of distributed controllers for cascade interconnected systems,\u201d in 2019 American Control Conference (ACC), pp. 5816\u20135821, IEEE, 2019.   \n[32] E. Agarwal, S. Sivaranjani, V. Gupta, and P. J. Antsaklis, \u201cDistributed synthesis of local controllers for networked systems with arbitrary interconnection topologies,\u201d IEEE Transactions on Automatic Control, vol. 66, no. 2, pp. 683\u2013698, 2020.   \n[33] E. Agarwal, S. Sivaranjani, V. Gupta, and P. Antsaklis, \u201cCompositional verification of passivity for cascade interconnected nonlinear systems,\u201d in 2020 28th Mediterranean Conference on Control and Automation (MED), pp. 319\u2013324, IEEE, 2020.   \n[34] A. Robey, \u201cLipsdp: Lipschitz estimation for neural networks.\u201d https://github.com/ arobey1/LipSDP, 2024. Accessed: 2024-05-22.   \n[35] A. Xue, \u201cChordal-lipsdp: A chordally sparse formulation of the lipsdp technique for bounding lipschitz constants of a feedforward neural network.\u201d https://github.com/ AntonXue/chordal-lipsdp, 2024. Accessed: 2024-05-22. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Appendices ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A Technical Proofs ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Proof of Relation in Assumption 1. We show how the slope-restrictedness of the activation function implies (2). The inequality $\\alpha(v_{1}-v_{2})\\leq\\phi(v_{1})-\\phi(v_{2})\\leq\\beta(v_{1}-v_{2})$ that holds elementwise yields ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\lambda^{i}\\left[(\\phi(v_{1})-\\phi(v_{2}))-\\alpha(v_{1}-v_{2})\\right]_{i}\\times\\left[(\\phi(v_{1})-\\phi(v_{2}))-\\beta(v_{1}-v_{2})\\right]_{i}\\le0,\\,\\forall i\\in\\mathbb{R}^{n},\\forall\\lambda^{i}\\ge0,\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where subscript $i$ indexes the $i^{t h}$ element of the vector. ", "page_idx": 12}, {"type": "text", "text": "Summing all the inequalities for $i\\in\\mathbb{R}^{n}$ and letting $\\Lambda=d i a g(\\lambda^{1},\\lambda^{2},...,\\lambda^{n})$ , we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\left[\\left(\\phi(v_{1})\\!-\\!\\phi(v_{2})\\right)\\!-\\!\\alpha(v_{1}-v_{2})\\right]^{T}\\Lambda\\left[\\left(\\phi(v_{1})\\!-\\!\\phi(v_{2})\\right)\\!-\\!\\beta(v_{1}-v_{2})\\right]\\!\\le\\!0.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "In other words, we have the following quadratic inequality ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(\\phi(v_{1})\\!-\\!\\phi(v_{2}))^{T}\\Lambda(\\phi(v_{1})\\!-\\!\\phi(v_{2}))\\!-\\!\\frac{\\alpha+\\beta}{2}(\\phi(v_{1})\\!-\\!\\phi(v_{2}))^{T}\\Lambda(v_{1}-v_{2})}\\\\ &{-\\frac{\\alpha+\\beta}{2}(v_{1}\\!-\\!v_{2})^{T}\\Lambda(\\phi(v_{1})\\!-\\!\\phi(v_{2}))\\!+\\!\\alpha\\beta(v_{1}\\!-\\!v_{2})^{T}\\Lambda(v_{1}\\!-\\!v_{2})\\le0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "which can be directly rewritten as (2) with $p=\\alpha\\beta$ and $\\begin{array}{r}{m=\\frac{\\alpha+\\beta}{2}}\\end{array}$ . ", "page_idx": 12}, {"type": "text", "text": "Proof of Theorem 1. For any two inputs $z_{1}^{(0)}$ and $z_{2}^{(0)}$ , let $z_{1}^{(i)},z_{2}^{(i)},v_{1}^{(i)},v_{2}^{(i)},i\\in\\mathbb{Z}_{l}$ be computed as in (1). Define $\\Delta z^{(i)}=z_{1}^{(i)}-z_{2}^{(i)}$ , $i\\in\\{0\\}\\cup\\mathbb{Z}_{l}$ and $\\Delta v^{(j)}=v_{1}^{(j)}-v_{2}^{(j)}$ \u2212v(2j ), j \u2208Zl. By both left and right multiplying the left hand side in (3) by the vector $[\\Delta z^{(0)},\\Delta z^{(1)},...,\\Delta z^{(l-1)}]^{T}$ , we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(\\Delta z^{(0)})^{T}\\Delta z^{(0)}+\\displaystyle\\sum_{i=1}^{l-1}\\left(\\left[\\Delta z^{(i-1)}\\right]^{T}\\left[p W_{i}^{T}\\Lambda_{i}W_{i}\\right.\\right.\\left.\\right.-m W_{i}^{T}\\Lambda_{i}\\right]\\left[\\left.\\!\\left.\\Delta z^{(i-1)}\\right]\\right)}\\\\ &{-\\left.\\left.F(\\Delta z^{(l-1)})W_{l}^{T}W_{l}\\Delta z^{(l-1)}>0.\\right.\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Now, we show that every summand in the above inequality (13) is negative semidefinite. In fact, with Assumption 1, using notation $\\Delta v^{(i)}=v_{1}^{(i)}-v_{2}^{(i)}$ and $\\Delta\\phi(v^{(i)})=\\phi(v_{1}^{(i)})-\\phi(v_{2}^{(i)}),\\,i\\in\\mathbb{Z}_{l}$ , and taking $\\Lambda=\\Lambda_{i}$ , we can write ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\left[\\Delta v^{(i)}\\right]^{T}\\left[\\!\\!\\begin{array}{c c}{p\\Lambda_{i}}&{-m\\Lambda_{i}}\\end{array}\\!\\!\\right]\\left[\\!\\!\\begin{array}{c}{\\Delta v^{(i)}}\\\\ {\\Delta\\phi(v^{(i)})\\right]}\\end{array}\\!\\!\\right]\\leq0.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Note that $\\Delta v^{(i)}=(W_{i}z_{1}^{(i-1)}+b_{i})-(W_{i}z_{2}^{(i-1)}+b_{i})=W_{i}\\Delta z^{(i-1)}$ iz2(i\u22121)+ bi) = Wi\u2206z(i\u22121) and \u2206\u03d5(vi) = \u2206zi. We can express these relationships in matrix form as ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\left[\\Delta\\boldsymbol{v}^{(i)}\\right]=\\left[\\begin{array}{l l}{W_{i}}&{0}\\\\ {0}&{I}\\end{array}\\right]\\left[\\boldsymbol{\\Delta z}^{(i-1)}\\right].\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Substituting (15) into (14), we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left[\\!\\!\\begin{array}{c}{\\Delta z^{(i-1)}}\\\\ {\\Delta z^{(i)}}\\end{array}\\!\\!\\right]^{T}\\left[\\!\\!\\begin{array}{c c}{p W_{i}^{T}\\Lambda_{i}W_{i}}&{-m W_{i}^{T}\\Lambda_{i}}\\\\ {-m\\Lambda_{i}W_{i}}&{\\Lambda_{i}}\\end{array}\\!\\!\\right]\\left[\\!\\!\\begin{array}{c}{\\Delta z^{(i-1)}}\\\\ {\\Delta z^{(i)}}\\end{array}\\!\\!\\right]\\leq0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Combining (13) and (16), we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n(\\Delta z^{(0)})^{T}\\Delta z^{(0)}-F(\\Delta z^{(l-1)})W_{l}^{T}W_{l}\\Delta z^{(l-1)}\\geq0.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Similarly, as the last layer is a linear layer, $\\Delta z^{(l)}=\\Delta v^{(l)}=W_{l}\\Delta z^{(l-1)}$ . Then (17) is exactly ", "page_idx": 12}, {"type": "equation", "text": "$$\n(\\Delta z^{l})^{T}\\Delta z^{l}\\leq\\frac{1}{F}(\\Delta z^{(0)})^{T}\\Delta z^{(0)},\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "yielding a upper bound $\\sqrt{1/F}$ for the Lipschitz constant. ", "page_idx": 12}, {"type": "text", "text": "Proof of Theorem 2. Applying Lemma 2 in [31], we define ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathcal{R}_{i}=-\\frac{1}{2}W_{i-1}^{T}\\Lambda_{i-1},i\\in\\mathbb{Z}_{l},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "and ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{P}_{i}=\\left\\{\\overset{\\cal I}{\\Lambda}_{i}^{\\cal I}\\!\\!\\!\\!\\phantom{\\frac{1}{\\Lambda_{i}^{i}}}\\!\\!\\!\\!\\!\\Lambda_{i}^{\\phantom{\\dagger}}=0,\\right.}\\\\ {\\Lambda_{i}^{\\phantom{\\dagger}}-F W_{l}^{T}W_{l}^{\\phantom{\\dagger}}\\quad\\mathrm{if}\\;i=l-1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Then, with $\\mathcal{P}_{i}$ and ${\\mathcal{R}}_{i}$ defined above, we directly have ", "page_idx": 13}, {"type": "equation", "text": "$$\nM_{i}=\\left\\{\\!\\!\\begin{array}{l l}{X_{i}}&{\\mathrm{if}\\;i\\in\\{0\\}\\bigcup\\mathbb{Z}_{l-2},}\\\\ {X_{i}+F W_{l}^{T}W_{l}}&{\\mathrm{if}\\;i=l-1.}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "In other words, the sufficient and necessary condition $X_{i}>0,\\forall i\\in\\{0\\}\\cup\\mathbb{Z}_{l-1}$ is equivalent to ", "page_idx": 13}, {"type": "equation", "text": "$$\nM_{i}>0,\\quad\\forall i\\in\\mathbb{Z}_{l-2},\\qquad M_{l-1}-F W_{l}^{T}W_{l}>0,\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "which is the same as (21). ", "page_idx": 13}, {"type": "text", "text": "Proof of Proposition 1. As $M_{i}~>~0$ , $i\\;\\in\\;\\mathbb{Z}_{l-2}$ has been guaranteed, it remains to ensure that $M_{l-1}-F W_{l}^{\\bar{T}}W_{l}\\,>\\,0$ by Theorem 3. This is equivalent to $M_{l-1}/F\\,>\\,W_{l}^{T}W_{l}$ . Therefore, the smallest possible $1/F$ is $\\sigma_{m a x}(W_{l}^{T}W_{l}(M_{l-1})^{-1})$ . Then by Theorem 1, the upper bound for the Lipschitz constant is $\\sqrt{1/F}=\\sqrt{\\sigma_{m a x}(W_{l}^{T}W_{l}(M_{l-1})^{-1})}$ . ", "page_idx": 13}, {"type": "text", "text": "Proof of Lemma 1. $M_{i}>0$ indicates $(M_{i})^{-1}>0$ . Then for $\\forall v_{0}\\neq0$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{v_{0}^{T}W_{i+1}(M_{i})^{-1}W_{i+1}^{T}v_{0}=(W_{i+1}^{T}v_{0})^{T}(M_{i})^{-1}W_{i+1}^{T}v_{0}\\geq0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "meaning that all eigenvalues of $W_{i+1}(M_{i})^{-1}W_{i+1}^{T}$ are non-negative. As $W_{i+1}\\neq0$ , we know that the largest eigenvalue is positive and should be the same as $\\sigma_{m a x}\\left(W_{i+1}(M_{i})^{-1}W_{i+1}^{T}\\right)>0$ . Now consider any non-zero eigenvalue $\\lambda_{a}$ of matrix $W_{i+1}(M_{i})^{-1}W_{i+1}^{T}$ and let $v_{a}\\ne0$ be its corresponding eigenvector. Then, ", "page_idx": 13}, {"type": "equation", "text": "$$\nW_{i+1}(M_{i})^{-1}W_{i+1}^{T}v_{a}=\\lambda_{a}v_{a}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Left multiplying both sides with $W_{i+1}^{T}$ , we have ", "page_idx": 13}, {"type": "equation", "text": "$$\nW_{i+1}^{T}W_{i+1}(M_{i})^{-1}(W_{i+1}^{T}v_{a})=W_{i+1}^{T}W_{i+1}(M_{i})^{-1}W_{i+1}^{T}v_{a}=W_{i+1}^{T}\\lambda_{a}v_{a}=\\lambda_{a}(W_{i+1}^{T}v_{a}).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "As $\\lambda_{a}\\neq0$ , we know that $W_{i+1}^{T}v_{a}\\,\\neq\\,0$ from (23). (Otherwise, $\\lambda_{a}v_{a}\\,=\\,0$ with $v_{a}\\ne0$ will lead to $\\lambda_{a}\\,=\\,0,$ ). With $W_{i+1}^{T}v_{a}\\,\\neq\\,0$ , (24) implies that $\\lambda_{a}$ is also an eigenvalue of $W_{i+1}^{T}W_{i+1}(M_{i})^{-1}$ corresponding to eigenvector $W_{i+1}^{T}v_{a}\\neq0$ . ", "page_idx": 13}, {"type": "text", "text": "Conversely, for any non-zero eigenvalue $\\lambda_{b}$ of $W_{i+1}^{T}W_{i+1}(M_{i})^{-1}$ corresponding to eigenvector $v_{b}\\neq0$ , we have ", "page_idx": 13}, {"type": "equation", "text": "$$\nW_{i+1}^{T}W_{i+1}(M_{i})^{-1}v_{b}=\\lambda_{b}v_{b}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Let vc =\u03bb1b $\\begin{array}{r}{v_{c}=\\frac{1}{\\lambda_{b}}W_{i+1}(M_{i})^{-1}v_{b}}\\end{array}$ . We have $\\boldsymbol{v}_{b}=W_{i+1}^{T}\\boldsymbol{v}_{c}$ . Then we substitute $v_{b}$ on the both sides in (25) and obtain ", "page_idx": 13}, {"type": "equation", "text": "$$\nW_{i+1}^{T}W_{i+1}(M_{i})^{-1}W_{i+1}^{T}v_{c}=\\lambda_{b}W_{i+1}^{T}v_{c}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Left multiplying both side with $W_{i+1}(M_{i})^{-1}$ , we further have ", "page_idx": 13}, {"type": "equation", "text": "$$\nW_{i+1}(M_{i})^{-1}W_{i+1}^{T}W_{i+1}(M_{i})^{-1}W_{i+1}^{T}v_{c}=\\lambda_{b}W_{i+1}(M_{i})^{-1}W_{i+1}^{T}v_{c}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Let $v_{d}=W_{i+1}(M_{i})^{-1}W_{i+1}^{T}v_{c}$ , we finally have ", "page_idx": 13}, {"type": "equation", "text": "$$\nW_{i+1}(M_{i})^{-1}W_{i+1}^{T}v_{d}=\\lambda_{b}v_{d}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Similarly, we can conclude that $\\lambda_{b}\\neq0$ is the eigenvalue of $W_{i+1}(M_{i})^{-1}W_{i+1}^{T}$ and $v_{d}$ is the corresponding eigenvector. ", "page_idx": 13}, {"type": "text", "text": "Proof of Lemma 2. To prove that $\\{\\Lambda_{i}:M_{i}>0,\\Lambda_{i}\\in\\mathbb{D}_{+}\\}\\subseteq\\{\\Lambda_{i}:\\tilde{M}_{i}>0,\\Lambda_{i}\\in\\mathbb{D}_{+}\\}$ , it suffices to show that $\\tilde{M}_{i}-M_{i}\\geq0$ for any $\\gamma\\in(0,1)$ . In fact, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\tilde{M}_{i}-M_{i}=\\Lambda_{i}-\\frac{1}{4}\\Lambda_{i}\\mathcal{F}_{i}\\Lambda_{i}-\\big(\\Lambda_{i}-\\frac{\\gamma}{4}\\Lambda_{i}\\mathcal{F}_{i}\\Lambda_{i}\\big)=\\frac{1-\\gamma}{4}\\Lambda_{i}\\mathcal{F}_{i}\\Lambda_{i}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "At stage $i$ , $M_{i-1}>0$ is guaranteed. Then, similar to (22), we know that $\\mathcal{F}_{i}=W_{i}(M_{i-1})^{-1}W_{i}^{T}\\geq0$ Since $\\Lambda_{i}\\in\\mathbb{D}_{+}$ and $0<\\gamma<1$ , implying $\\textstyle{\\frac{1-\\gamma}{4}}>0$ , we have $\\begin{array}{r}{\\frac{1-\\gamma}{4}\\Lambda_{i}\\mathcal{F}_{i}\\Lambda_{i}\\geq0}\\end{array}$ . ", "page_idx": 14}, {"type": "text", "text": "Proof of Proposition 2. Recall that by definition, $\\mathcal{F}_{i+1}=W_{i+1}(M_{i})^{-1}W_{i+1}^{T}$ . Applying Lemma 1, we have $\\sigma_{m a x}(\\mathcal{F}_{i+1})=\\sigma_{m a x}(W_{i+1}^{T}W_{i+1}(M_{i})^{-1})$ . Meanwhile, with $M_{i}=c_{i}W_{i+1}^{T}W_{i+1}+N$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{W_{i+1}^{T}W_{i+1}(M_{i})^{-1}=(W_{i+1}^{T}W_{i+1}+\\displaystyle\\frac{N}{c_{i}})(c_{i}W_{i+1}^{T}W_{i+1}+N)^{-1}-\\displaystyle\\frac{N}{c_{i}}(c_{i}W_{i+1}^{T}W_{i+1}+N)^{-1}}}\\\\ {{\\displaystyle\\qquad\\qquad\\qquad=\\frac{1}{c_{i}}I-\\displaystyle\\frac{N}{c_{i}}(M_{i})^{-1}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "WIth $N~\\ge~0$ and $M_{i}~>~0$ after $\\Lambda_{i}$ is decided, we show that $\\frac{N}{c_{i}}(M_{i})^{-1}$ only has non-negative eigenvalues. As $M_{i}>0$ is guaranteed to symmetric according to (7), there exists a symmetric square root for $(M_{i})^{-1}$ and we denote it to be $(M_{i})^{-\\frac{1}{2}}$ . Then $N(M_{i})^{-1}$ is similar to $(\\dot{M_{i}})^{-\\frac{1}{2}}N(\\bar{M_{i}})^{-\\frac{1}{2}}$ , thus sharing the same eigenvalues. Furthermore, for $\\forall x\\neq0$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n{x^{T}(M_{i})^{-\\frac{1}{2}}N(M_{i})^{-\\frac{1}{2}}x=\\left((M_{i})^{-\\frac{1}{2}}x\\right)^{T}N\\left((M_{i})^{-\\frac{1}{2}}x\\right)\\geq0.}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "It indicates $(M_{i})^{-\\frac{1}{2}}N(M_{i})^{-\\frac{1}{2}}$ only has non-negative eigenvalues. The first equation holds by the symmetry of $(M_{i})^{-\\frac{1}{2}}$ and the second inequality is because of the definition of positive semidefiniteness of $N$ . Therefore, with $c_{i}~>~0$ , the eigenvalues of $\\textstyle{\\frac{N}{c_{i}}}(M_{i})^{-1}\\:=\\:{\\frac{1}{c_{i}}}N(M_{i})^{-1}$ are all non-negative. and all eigenvalues of $W_{i+1}^{T}W_{i+1}(M_{i})^{-1}$ should be less than or equal to $1/c_{i}$ . On the other hand, as $N$ is singular, $N(M_{i})^{-1}$ is also singular, thus having eigenvalue 0. Therefore, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\sigma_{m a x}(\\mathcal{F}_{i+1})=\\sigma_{m a x}(W_{i+1}^{T}W_{i+1}(M_{i})^{-1})=1/c_{i}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof of Proposition 3. We first show that if $M_{i-1}>0$ , the feasible region is always non-empty for $\\forall i\\in\\mathcal{Z}_{l-1}$ . We use $\\sigma$ to denote $\\sigma_{m a x}$ $_{2x}\\left(W_{i}(M_{i-1})^{-1}W_{i}^{T}\\right)=\\sigma_{m a x}(\\bar{\\mathcal{F}_{i}})$ . We take $\\begin{array}{r}{\\dot{\\Lambda}_{i}=\\frac{2}{\\sigma}I}\\end{array}$ and ci =\u03c3\u00b7\u03c3max(0W. i9T+1Wi+1). As Wi \u0338= 0, i \u2208Zl, we have \u03c3 > 0 and \u03c3max(Fi) > 0, ensuring \u039bi \u2208D+ and $c_{i}>0$ . Further, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Lambda_{i}-\\cfrac{1}{4}\\Lambda_{i}W_{i}(M_{i-1})^{-1}W_{i}^{T}\\Lambda_{i}-c_{i}\\big(W_{i+1}^{T}W_{i+1}\\big)}\\\\ &{~\\geq\\cfrac{2}{\\sigma}I-\\cfrac{1}{4}\\cfrac{4}{\\sigma^{2}}\\sigma I-\\cfrac{0.9}{\\sigma\\cdot\\sigma_{m a x}\\left(W_{i+1}^{T}W_{i+1}\\right)}\\big(W_{i+1}^{T}W_{i+1}\\big)}\\\\ &{~>\\cfrac{1}{\\sigma}I-\\cfrac{0.9}{\\sigma}I>0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Therefore, the feasible region at least includes the $\\Lambda_{i}$ and $c_{i}$ we specified, and is thus not empty. To make the feasibility complete, we prove that $M_{i}\\,>\\,0$ , $\\forall i\\in\\{0\\}\\cup\\mathcal{Z}_{l-1}$ by induction. When $i=0$ , $M_{i}=M_{0}=I>0$ . When it comes to stage $i$ , we have by ind uction that $M_{i-1}>0$ is true. As $\\Lambda_{i}$ are obtained satisfying (12) with $c_{i}>0$ and recall the recursive relation for $M_{i}$ , $i\\in\\mathbb{Z}_{l-1}$ to be $\\begin{array}{r}{\\dot{M_{i}}=\\Lambda_{i}-\\frac{1}{4}\\Lambda_{i}W_{i}(\\dot{M_{i-1}})^{-1}\\dot{W_{i}}^{T}\\Lambda_{i}}\\end{array}$ , we have $M_{i}>c_{i}W_{i+1}^{T}W_{i+1}\\geq0$ . ", "page_idx": 14}, {"type": "text", "text": "We now prove by contradiction that the optimal value $c_{i}$ is the largest constant such that $M_{i}$ can be written as $\\dot{M_{i}}\\,=\\,c_{i}W_{i+1}^{T}W_{i+1}+N$ , where $N$ is some singular matrix that $N~\\ge~0$ . $M_{i}\\,=$ $c_{i}W_{i+1}^{T}W_{i+1}+N$ . Suppose there exists a $\\hat{c}_{i}\\,>\\,c_{i}$ such that it satisfies all the constraints. Then, from the first constraint in (12), we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n(c_{i}-\\hat{c}_{i})W_{i+1}^{T}W_{i+1}+N=M_{i}-\\hat{c}_{i}(W_{i+1}^{T}W_{i+1})>0\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Let $v\\neq0$ be the eigenvector of $N$ corresponding to eigenvalue 0. Observe that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad v^{T}\\left((c_{i}-\\hat{c}_{i})W_{i+1}^{T}W_{i+1}+N\\right)v}\\\\ &{=v^{T}\\left((c_{i}-\\hat{c}_{i})W_{i+1}^{T}W_{i+1}\\right)v+0}\\\\ &{=(c_{i}-\\hat{c}_{i})\\,v^{T}\\left(W_{i+1}^{T}W_{i+1}\\right)v\\le0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The last step holds because $c_{i}-\\hat{c}_{i}<0$ by definition of ${\\hat{c}}_{i}$ . It contradicts (29). ", "page_idx": 15}, {"type": "text", "text": "Proof of Proposition 4. When $\\Lambda_{i}=\\lambda_{i}I$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left(\\Lambda_{i}-\\frac{1}{4}\\Lambda_{i}\\mathcal{F}_{i}\\Lambda_{i}\\right)^{-1}=\\left(\\lambda_{i}I-\\frac{1}{4}\\lambda_{i}^{2}\\mathcal{F}_{i}\\right)^{-1}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "As $M_{i}=\\Lambda_{i}-\\textstyle{\\frac{1}{4}}\\Lambda_{i}\\mathcal{F}_{i}\\Lambda_{i}>0$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sigma_{m a x}\\left(\\left(\\lambda_{i}I-\\frac{1}{4}\\lambda_{i}^{2}\\mathcal{F}_{i}\\right)^{-1}\\right)=\\frac{1}{\\lambda_{i}-\\lambda_{i}^{2}\\sigma_{m a x}(\\mathcal{F}_{i})/4}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Minimizing the above spectrum is equivalent to maximizing the denominator $\\lambda_{i}-\\lambda_{i}^{2}\\sigma_{m a x}(\\mathcal{F}_{i})/4$ in (31), which is quadratic in $\\lambda_{i}$ . To find the optimal $\\lambda_{i}$ , we set the derivative of the denominator with respect to \u03bbi to be 0, and obtain the closed-form solution \u03bbi = \u03c3max2(Fi). ", "page_idx": 15}, {"type": "text", "text": "Moreover, with $\\begin{array}{r}{\\Lambda_{i}=\\frac{2}{\\sigma_{m a x}(\\mathcal{F}_{i})}I}\\end{array}$ and $\\mathcal{F}_{i}>0$ guaranteed at stage $i$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\nM_{i}=\\Lambda_{i}-\\frac{1}{4}\\Lambda_{i}\\mathcal{F}_{i}\\Lambda_{i}=\\frac{1}{\\sigma_{m a x}(\\mathcal{F}_{i})}I>0.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "B Geometric Analysis for ECLipsE-Fast ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The geometric analysis for algorithm ECLipsE-Fast analogous to Fig. 2, comparing the case where $c_{i}>1$ and in other cases are shown in Fig. 7. ", "page_idx": 15}, {"type": "image", "img_path": "61YYSy078Z/tmp/f49dd2ecc23a85b1f6fedb6e958fb5d57585cf97a28eb96a868f227a9478ef59.jpg", "img_caption": ["Figure 7: Geometric Intuition of ECLipsE-Fastwith $c_{i}>1$ and otherwise. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "C Computational Complexity Derivation ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We derive the time complexity for both ECLipsE and ECLipsE-Fast in detail here. Suppose a neural network has $n$ hidden layers with $m$ neurons. Then, the large matrix in Theorem 1 has dimension $n m+O(1)$ and the decision variable is of size $n m+O(1)$ . Note that the computational complexity for solving an LMI with the size of the matrix constraint being size $A$ and the number of decision variables being $B$ is $O(A^{3}+A^{2}B^{2})$ . Therefore, the computational cost for LipSDP is $O((n m+O(1))^{3}+(n m^{\\bullet}+O(1))^{2}(n m+O(1))^{2})=O(n^{4}m^{4})$ . Contrarily, ECLipsE solves $n$ sub-problems as in Eq. (8), each involving a matrix of size $O(m)$ and $m$ decision variables. The corresponding total computational cost is $\\bar{n^{\\star}}({\\cal O}(m^{3}\\!+m^{2}m^{2}))=\\!{\\cal O}(n m^{4})$ . This directly indicates the advantage of ECLipsE for deep networks. Also, as $m$ grows, the difference between $O(n^{4}m^{4})$ and $O(n m^{\\bar{4}})$ is still significantly enhanced, especially with large $n$ . Regarding ECLipsE-Fast, we note that we do not need to solve any SDPs and the computational cost drops to $n\\stackrel{\\bullet}{\\times}O(m^{3})=$ $O(n m^{3})$ . This is the fastest one can expect if the weights on each layer are treated as a whole. ", "page_idx": 15}, {"type": "text", "text": "D Experimental Setup and Data Generation ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Experimental Setup. All experiments are implemented on a Windows laptop with a 12-core CPU with 16GB of RAM. ", "page_idx": 15}, {"type": "text", "text": "Randomly Generated Neural Networks. We set the input dimension to be 4 and the output dimension to be 1. The activation functionsare chosen to be ReLU, and the number of neurons in each hidden layer is set to be the same. We randomly generate weights for each layer to follow the normal distribution. Also, in order to avoid the case where the Lipschitz constant is too large or too small and may cause numerical issues, we scale the weights on each layer such that the is norm randomly chosen in [0.4, 1.8], following a uniform distribution. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "MNIST. For training on the dataset MNIST, the input dimension is 784 and output dimension is 10, which is compatible with the dataset. he activation functionsare chosen to be ReLU, and the number of neurons in each hidden layer is set to be the same. We train neural networks using the SGD optimizer with a learning rate of 0.01 and momentum of 0.9 until they achieve at least $97\\%$ accuracy on test data. ", "page_idx": 16}, {"type": "text", "text": "E Additional Experimental Results ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The Lipschitz constant estimates and computation times for randomly generated neural networks with the number of layers chosen from $\\{\\bar{2},5,10,20,30,50,75,100\\}$ , and number of neurons are chosen from $\\{20,40,\\dot{6}0,80,100\\}$ , are provided below. ", "page_idx": 17}, {"type": "table", "img_path": "61YYSy078Z/tmp/0650c38106e861b4bfaad7f5c0d25237c84e9f336459e1acb2f1e1f2e95898f3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "61YYSy078Z/tmp/9e44c12c4f03d2123f60234b6a13f8a4e6831edad700706fbed4291bc744c102.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "61YYSy078Z/tmp/95abed299af6bb4cd98970d4a090a70389e4c1b91d26ea1e2de8b36bc6dc0d2a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "61YYSy078Z/tmp/a97fe6c9adce9096bfc2e699e07d296afdbb84544af1789721ae6d2188fb8461.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "61YYSy078Z/tmp/b19764419e3744a47db1c8f1623f94a93eeed642d30390d4e71b3a0e5fb0b2c5.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "61YYSy078Z/tmp/a8b0a0ffe6074ba50287b305070a9afe9220234fe95a3a7fc9658081036bb819.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "61YYSy078Z/tmp/a7b86626944c77a01c855f23c10f4dec14bee27d1951880d4779f8e429b4fe9f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "61YYSy078Z/tmp/5e4042ecf54ce8f926a2376c4415af807355ab7df9398cd217eb0fbf2355a0f6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "F Broader Impacts ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "This work is primarily theoretical and pertains to obtaining upper bounds on the Lipschitz constant, which can serve as a measure of the robustness of deep neural networks, and does not have any direct societal impact. ", "page_idx": 20}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: The main claims in our abstract and introduction accurately reflect the paper\u2019s scope and contributions. All the theoretical and experimental results are aligned with the claims made in the abstract and introduction. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 21}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: We clearly discuss all the theoretical assumptions behind our work and the classes of networks for which this work is applicable. We also discuss and demonstrate the computational efficiency of the proposed algorithms for different network sizes. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 21}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: We provide the complete list of assumptions behind the theoretical results in Sections 2 and 3 of the main paper. We also provide the key ideas and geometric intuition behind the proofs in Section 3. The detailed proofs of all results are provided in the Appendix. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 22}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: All the experimental details are described in the Appendix, and all the code and data are submitted along with the paper. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 22}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 23}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: All the code and data are submitted along with the paper, and will be opensourced upon acceptance. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so No is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 23}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: Yes, the paper details the experimental setting and benchmarks in Section 4 and the Appendix. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 23}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: Our algorithms to compute Lipschitz constant bounds are deterministic, and always yield the same result for a given neural network up within numerical accuracy bounds. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 24}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The computational resources and computation time required for each experiment are provided in the Appendix. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 24}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: We have reviewed the NeurIPS Code of Ethics, and ensured that our paper conforms to these regulations. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 24}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper is primarily theoretical, and does not have any immediate societal impact. We discuss this in the Appendix section F. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 25}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: This paper does not pose any such risk. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 25}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: All the datasets used to train our neural networks, and the code for the papers utilized as benchmarks to evaluate our algorithms are cited in the main text and in our code. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets. ", "page_idx": 25}, {"type": "text", "text": "\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/ datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 26}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: All the code and data are submitted along with the paper, and will be released publicly with detailed documentation upon publication. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 26}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: This paper is mainly theoretical and does not involve crowdsourcing or human subjects. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 26}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: This paper is mainly theoretical and does not involve crowdsourcing or human subjects. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 27}]