[{"type": "text", "text": "Multiclass Transductive Online Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Steve Hanneke   \nDepartment of Computer Science   \nPurdue University   \nWest Lafayette, IN 47907   \nsteve.hanneke@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Vinod Raman Department of Statistics University of Michigan Ann Arbor, MI 48104 vkraman@umich.edu ", "page_idx": 0}, {"type": "text", "text": "Amirreza Shaeri   \nDepartment of Computer Science Purdue University West Lafayette, IN 47907   \namirreza.shaeiri@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Unqiue Subedi Department of Statistics University of Michigan Ann Arbor, MI 48104 subedi@umich.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We consider the problem of multiclass transductive online learning when the number of labels can be unbounded. Previous works by Ben-David et al. [1997] and Hanneke et al. [2023b] only consider the case of binary and finite label spaces, respectively. The latter work determined that their techniques fail to extend to the case of unbounded label spaces, and they pose the question of characterizing the optimal mistake bound for unbounded label spaces. We answer this question by showing that a new dimension, termed the Level-constrained Littlestone dimension, characterizes online learnability in this setting. Along the way, we show that the trichotomy of possible minimax rates of the expected number of mistakes established by Hanneke et al. [2023b] for finite label spaces in the realizable setting continues to hold even when the label space is unbounded. In particular, if the learner plays for $T\\in\\mathbb N$ rounds, its minimax expected number of mistakes can only grow like $\\Theta(T)$ , $\\Theta(\\log T)$ , or $\\Theta(1)$ . To prove this result, we give another combinatorial dimension, termed the Level-constrained Branching dimension, and show that its finiteness characterizes constant minimax expected mistake-bounds. The trichotomy is then determined by a combination of the Level-constrained Littlestone and Branching dimensions. Quantitatively, our upper bounds improve upon existing multiclass upper bounds in Hanneke et al. [2023b] by removing the dependence on the label set size. In doing so, we explicitly construct learning algorithms that can handle extremely large or unbounded label spaces. A key and novel component of our algorithm is a new notion of shattering that exploits the sequential nature of transductive online learning. Finally, we complete our results by proving expected regret bounds in the agnostic setting, extending the result of Hanneke et al. [2023b]. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Imagine you are a famous musician who has released $K\\in\\mathbb N$ songs. You are now on tour visiting $T\\in\\mathbb N$ cities worldwide based on the pre-specified plan, each with unique musical preferences that you have some understanding of. At each city, you can perform only one song in your concert, and following each performance, the audience provides feedback indicating their preferred song from your repertoire. Your goal is to select the song that aligns with the majority\u2019s taste in each city to maximize satisfaction. How can you effectively select songs to ensure the highest audience satisfaction across most cities having minimal assumptions? ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "The above example and similar real-world situations, where entities operate according to a possibly adversarially chosen pre-specified schedule, can be formulated in a framework called Multiclass Transductive Online Learning. Formally, in this setting, an adversary plays a repeated game against the learner over some $T\\in\\mathbb{N}$ rounds. Before the game begins, the adversary selects a sequence of $T$ instances $(x_{1},\\hdots,x_{T})\\in\\mathcal{X}^{T}$ from some non-empty instance space $\\mathcal{X}$ (e.g. images) and reveals it to the learner. Subsequently, during each round $t\\in\\{1,\\ldots,T\\}$ , the learner predicts a label $\\hat{y}_{t}\\in\\mathcal{Y}$ from some non-empty label space $\\boldsymbol{\\wp}$ (e.g. categories of images), the adversary reveals the true label $y_{t}\\in\\mathcal{V}$ , and the learner suffers the 0-1 loss, namely $\\mathbb{1}\\{\\hat{y}_{t}\\neq y_{t}\\}$ . Importantly, the label space $\\boldsymbol{\\wp}$ is not required even to be countable; we assume only standard measure theoretic properties for it. Following the well-established frameworks in learning theory, given a concept class $\\mathcal{C}\\subseteq\\mathcal{V}^{\\mathcal{X}}$ of functions $c:\\mathcal{X}\\to\\mathcal{Y}$ , the goal of the learner is to minimize the number of mistakes relative to the best-fixed concept in $\\mathcal{C}$ . If there exists $c\\in{\\mathcal{C}}$ such that $c(x_{t})=y_{t}$ for all $t\\in\\{1,\\ldots,T\\}$ , we say we are in the realizable setting, and otherwise in the agnostic setting. We briefly note that if the learner\u2019s predictions are randomized, we focus on the expected value of the mentioned objective. ", "page_idx": 1}, {"type": "text", "text": "In this paper, our main contribution is algorithmically answering the following question in the multiclass transductive online learning framework: ", "page_idx": 1}, {"type": "text", "text": "Given a concept class ${\\mathcal{C}}\\subseteq{\\mathcal{V}}^{{\\mathcal{X}}}$ , what is the minimum expected number of mistakes achievable by $a$ learner against any realizable adversary? ", "page_idx": 1}, {"type": "text", "text": "For the special case of binary classification $(|\\mathcal{Y}|=2)$ ), this question was first considered by Ben-David et al. [1997] and then later fully resolved by Hanneke et al. [2023b]. Additionally, Hanneke et al. [2023b] considered the case where $|\\mathcal{Y}|>2$ , but did not resolve this question when $\\boldsymbol{\\wp}$ is unbounded. In fact, the bounds by Hanneke et al. [2023b] break down even when $|\\dot{\\boldsymbol{y}}|\\geq2^{T}$ . As a result, Hanneke et al. [2023b] posed the characterization of the minimax expected number of mistakes in the multiclass setting with an infinite label set as an open question, which we resolve in this paper. ", "page_idx": 1}, {"type": "text", "text": "1.1 Online Learning and Multiclass Classification ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this work, we study transductive online learning framework, where the adversary reveals the entire sequence of instances $(x_{1},\\ldots,x_{T})$ to the learner before the game begins. In the traditional online learning framework, the sequence of instances $(x_{1},\\hdots,x_{T})$ are revealed to the learner sequentially, one at a time. That is, on round $t\\in\\{1,\\ldots,T\\}$ , the learner would have only observed $x_{1},\\ldots,x_{t}$ . The celebrated work of Littlestone [1988] introduced this framework for binary classification and quantified the best achievable number of mistakes against any realizable adversary for a concept class $\\bar{c}\\subseteq\\{0,1\\}^{\\chi}$ in terms of a combinatorial parameter called the Littlestone dimension. Later, the work of Ben-David et al. [2009] showed that the Littlestone dimension of a concept class $\\mathcal{C}\\subseteq\\{0,1\\}^{\\mathcal{X}}$ continues to quantify the expected relative mistakes (i.e expected regret) for the mentioned framework in the more general agnostic setting. More recently, Daniely et al. [2012] and Hanneke et al. [2023a] extended these results to multiclass online learning in the realizable and agnostic settings, respectively. See Section A for more details. ", "page_idx": 1}, {"type": "text", "text": "In traditional online classification, there are two sources of uncertainty: one associated with the sequence of instances, and the other with respect to the true labels. Ben-David et al. [1997] initiated the study of transductive online classification with the aim of understanding how exclusively label uncertainty impacts the optimal number of mistakes. Furthermore, removing the uncertainty with respect to the instances can significantly reduce the optimal number of mistakes. For example, for the concept class of halfspaces in the realizable setting, the optimal number of mistakes grows linearly with the time horizon $T$ in the traditional online binary classification framework, while only growing as $\\Theta(\\log T)$ in the transductive online binary classification framework. So, it is natural to reduce the optimal number of mistakes or extend learnable classes whenever we have additional assumptions. Notably, Ben-David et al. [1997] initially called this setting \u201coffilne learning\u201d, but it was later renamed \u201cTransductive Online Learning\u201d by Hanneke et al. [2023b] due to its close resemblance to transductive PAC learning [Vapnik and Chervonenkis, 1974, Vapnik, 1982, 1998]. See Section A for more details. ", "page_idx": 1}, {"type": "text", "text": "While Ben-David et al. [1997] and Hanneke et al. [2023b] mainly focused on binary classification, in this work, we focus on the more general multiclass classification setting. Natarajan and Tadepalli [1988], Natarajan [1989] and Daniely et al. [2012] initiated the study of multiclass prediction within the foundational PAC framework and traditional online framework, respectively. More recently, following the work by [Brukhim et al., 2021], there has been a growing interest in understanding multiclass learning when the size of the label space is unbounded, including Hanneke et al. [2023c,a], Raman et al. [2023]. This interest is driven by several motivations. Firstly, guarantees for the multiclass setting should not inherently depend on the number of labels, even when it is finite. Secondly, in mathematics, concepts involving infinities often provide cleaner insights. Thirdly, insights from this problem might also advance understanding of real-valued regression problems [Attias et al., 2023]. Finally, on a practical front, many crucial machine learning tasks involve classification into extremely large label spaces. For instance, in image object recognition, the number of classes corresponds to the variety of recognizable objects, and in language models, the class count expands with the dictionary size. See Section A for more details. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "1.2 Main Results and Techniques ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In the following subsection, we present an overview of our main findings along with a summary of our proof techniques. ", "page_idx": 2}, {"type": "text", "text": "1.2.1 Realizable Setting ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In the realizable setting, we assume that the sequence of labeled instances $(x_{1},y_{1}),\\dots,(x_{T},y_{T})$ , played by the adversary, is consistent with at least one concept in $\\mathcal{C}$ . Here, our objective is to minimize the well-known notion of the expected number of mistakes. We provide upper and lower bounds on the best achievable worst-case expected number of mistakes by the learner as a function of $T$ and $\\mathcal{C}$ , which we denote by $\\mathrm{M}^{\\star}(T,{\\mathcal{C}})$ . ", "page_idx": 2}, {"type": "text", "text": "Hanneke et al. [2023b] established a trichotomy of rates in the case of binary classification. That is, for every $\\mathcal{C}\\subseteq\\{0,1\\}^{\\chi}$ , we have that $\\mathrm{M}^{\\star}(T,{\\mathcal{C}})$ can only grow like $\\Theta(T)$ , $\\Theta(\\log T)$ , or $\\Theta(1)$ ; where the Littlestone and Vapnik-Chervonenkis (VC) dimensions of $\\mathcal{C}$ characterize the possible rate. In this work, we extend this trichotomy to the multiclass classification setting, even when $\\boldsymbol{\\wp}$ is unbounded. To do so, we introduce two new combinatorial parameters, termed the Level-constrained Littlestone dimension and Level-constrained Branching dimension. ", "page_idx": 2}, {"type": "text", "text": "To define the Level-constrained Littlestone dimension, we first need to define the Level-constrained Littlestone tree. A Level-constrained Littlestone tree is a Littlestone tree with the additional requirement that the same instance has to label all the internal nodes across a given level. Then, the Level-constrained Littlestone dimension is just the largest natural number $d\\in\\mathbb{N}$ , such that there exists a shattered Level-constrained Littlestone tree $T$ of depth $d$ . To define the Level-constrained Branching dimension, we first need to define the Level-constrained Branching tree. The Level-constrained Branching tree is a Level-constrained Littlestone tree without the restriction that the labels on the two outgoing edges are distinct. Then, the Level-constrained Branching dimension is then the smallest natural number $d\\in\\mathbb{N}$ such that for every shattered Level-constrained Branching tree $T$ , there exists a path down $T$ such that the number of nodes whose outgoing edges are labeled by different elements of $\\boldsymbol{\\wp}$ is at most $d$ . The Level-constrained Littlestone dimension reduces to the VC dimension when $|\\mathcal{Y}|=2$ . Additionally, the finiteness of the Level-constrained Branching and Littlestone dimension coincide when $|\\mathcal{Y}|=2$ . Finally, we note that the Level-constrained Branching dimension is exactly equal to the notion of rank in the work of Ben-David et al. [1997]. However, we believe it is simpler to understated. Using the Level-constrained Littlestone and Branching dimension, we establish the following trichotomy. ", "page_idx": 2}, {"type": "text", "text": "Theorem 1. (Trichotomy) Let ${\\mathcal{C}}\\subseteq{\\mathcal{V}}^{{\\mathcal{X}}}$ be a concept class. Then, we have: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{M}^{\\star}(T,\\mathcal{C})\\in\\displaystyle\\left\\{\\Theta(1),\\begin{array}{l l}{\\quad}&{i f\\,\\mathrm{B}(\\mathcal{C})<\\infty.}\\\\ {\\theta(\\log T)}&{i f\\,\\mathrm{D}(\\mathcal{C})<\\infty\\,a n d\\,\\mathrm{B}(\\mathcal{C})=\\infty.}\\\\ {\\theta(T),}&{i f\\,\\mathrm{D}(\\mathcal{C})=\\infty.}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Here, $\\operatorname{B}({\\mathcal{C}})$ is Level-constrained Branching dimension, and $\\operatorname{D}({\\mathcal{C}})$ is Level-constrained Littlestone dimension defined in Section 2. ", "page_idx": 2}, {"type": "text", "text": "To prove the ${\\cal O}(\\log T)$ upper bound for binary online classification, Hanneke et al. [2023b] run the Halving algorithm on the projection of $\\mathcal{C}$ onto $x_{1},...,x_{T}$ and use the Sauer\u2013Shelah\u2013Perles (SSP) ", "page_idx": 2}, {"type": "text", "text": "lemma to bound the size of this projection by $O(T^{\\mathrm{VC}(C)})$ . However, this approach is not applicable when $\\boldsymbol{\\wp}$ is unbounded. For example, when ${\\mathcal{C}}=\\{x\\mapsto n:n\\in\\mathbb{N}\\}$ is the set of all constant functions over N, the size of the projection of $\\mathcal{C}$ onto even a single $x\\in\\mathscr{X}$ is infinity. Moreover, the mentioned class can be learned with at most one number of mistakes. Thus, fundamentally new techniques are required. To this end, we define a new notion of shattering which makes it possible to apply an analog of the Halving algorithm. Additionally, while the proof of the $O(1)$ upper bound in Hanneke et al. [2023b] follows immediately from the guarantee of Standard Optimal Algorithm (SOA) by Littlestone [1988], our $O(1)$ upper bound in terms of the Level-constrained Branching dimension requires a modification of the SOA. We complement our results by presenting matching lower bounds. See Section 3 for more details. ", "page_idx": 3}, {"type": "text", "text": "In Section F, we provide a comprehensive comparison between our dimensions and existing multiclass combinatorial complexity parameters. ", "page_idx": 3}, {"type": "text", "text": "1.2.2 Agnostic Setting ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In the agnostic setting, we make no assumptions about the sequence $(x_{1},y_{1}),(x_{2},y_{2}),\\dots,(x_{T},y_{T})$ played by the adversary. Here, our focus shifts to the well-established notion of expected regret, which compares the expected number of mistakes made by the algorithm to that made by the best concept in the concept class over the sequence. As in the realizable setting, we aim to establish both upper and lower bounds on the optimal worst-case expected regret achievable by the learner, expressed as a function of $T$ and the concept class $\\mathcal{C}$ , denoted by $\\mathrm{R}^{\\star}(\\bar{T},\\mathcal{C})$ . ", "page_idx": 3}, {"type": "text", "text": "The prior work by Hanneke et al. [2023b] showed that in the case of binary classification, $\\mathrm{R}^{\\star}(T,{\\mathcal{C}})$ is $\\tilde{\\Theta}(\\sqrt{\\mathrm{VC}(\\mathcal{C})\\,T})$ whenever $\\mathrm{VC}({\\mathcal{C}})<\\infty$ and $\\Theta(T)$ otherwise, where $\\Tilde{\\Theta}$ hides logarithmic factors in $T$ . Using the Level-constrained Littlestone dimension in hand, we extend these results to multiclass classification. ", "page_idx": 3}, {"type": "text", "text": "Theorem 2. For every concept class ${\\mathcal{C}}\\subseteq{\\mathcal{V}}^{{\\mathcal{X}}}$ and $T\\geq\\mathrm{D}({\\mathcal{C}})$ , we have the following: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\sqrt{\\frac{T\\mathrm{\\tiny~D}(\\mathcal{C})}{8}}\\leq\\mathrm{\\tiny~R}^{\\star}(T,\\mathcal{C})\\leq\\sqrt{T\\mathrm{\\tiny~D}(\\mathcal{C})}\\mathrm{\\tiny~log}\\Big(\\frac{e T}{\\mathrm{\\tiny~D}(\\mathcal{C})}\\Big),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\operatorname{D}({\\mathcal{C}})$ is Level-constrained Littlestone dimension defined in Section 2. ", "page_idx": 3}, {"type": "text", "text": "Our results in the agnostic setting can be proved using core ideas in the proof of the agnostic results from Ben-David et al. [2009], Hanneke et al. [2023a], and Hanneke et al. [2023b]. See Section E for more details. ", "page_idx": 3}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "2.1 Notation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Let $\\mathcal{X}$ denote an example space and $\\boldsymbol{\\wp}$ denote the label space. We make no assumptions about $\\boldsymbol{\\wp}$ , so it can be unbounded and even uncountable (e.g. $\\ y=\\mathbb{R}_{}$ ). Following the work of Hanneke et al. [2023a], if we consider randomized learning algorithms, the associated $\\sigma$ -algebra is of little consequence, except that singleton sets $\\{y\\}$ should be measurable. Let ${\\mathcal{C}}\\subseteq{\\mathcal{V}}^{{\\mathcal{X}}}$ denote a concept class. We abbreviate a sequence $z_{1},...,z_{T}$ by $z_{1:T}$ . Moreover, we also define $z_{<t}:=(z_{1},\\dotsc,z_{t-1})$ and $z_{\\le t}:=(z_{1},\\dots,z_{t})$ . Finally for $n\\in\\mathbb N$ , we let $[n]:=\\{1,...,n\\}$ . ", "page_idx": 3}, {"type": "text", "text": "2.2 Transductive Online Classification ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In the transductive online classification setting, a learner $\\boldsymbol{\\mathcal{A}}$ plays a repeated game against an adversary over $T$ rounds. Before the game begins, the adversary picks a sequence of labeled instances $(\\bar{x_{1}},y_{1}),...,(x_{T},y_{T})\\in(\\mathcal{X}\\times\\mathcal{Y})^{\\breve{T}}$ and reveals $x_{1:T}$ to the learner. Then, in each round $t\\in[T]$ , using $x_{1:T}$ and $y_{1:t-1}$ , the learner makes a potentially randomized prediction $\\boldsymbol{A}(\\boldsymbol{x}_{t})\\in\\boldsymbol{y}$ . Finally, the adversary reveals the true label $y_{t}$ , and the learner suffers the loss $\\mathbb{1}\\{A(x_{t})\\neq y_{t}\\}$ . Given a concept class $\\mathcal{C}\\doteq\\mathcal{V}^{\\mathcal{X}}$ , the goal of the learner is to output predictions such that its expected regret, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{R}_{A}(T,\\mathcal{C}):=\\operatorname*{sup}_{(x_{1},y_{1}),\\dots,(x_{T},y_{T})}\\left(\\mathbb{E}\\left[\\sum_{t=1}^{T}\\mathbb{1}\\{A(x_{t})\\neq y_{t}\\}\\right]-\\operatorname*{inf}_{c\\in\\mathcal{C}}\\sum_{t=1}^{T}\\mathbb{1}\\{c(x_{t})\\neq y_{t}\\}\\right),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "is small. Moreover, we define $\\operatorname{R}^{\\star}(T,\\mathcal{C}):=\\operatorname*{inf}_{A}\\,\\operatorname{R}_{A}(T,\\mathcal{C})$ , where the infimum is taken over all transductive online algorithms. We say that a concept class is transductive online learnable in the agnostic setting if $\\mathrm{R}^{\\star}(\\bar{T},\\mathcal{C})=o(T)$ . ", "page_idx": 4}, {"type": "text", "text": "If the learner is guaranteed to observe a sequence of examples labeled by some concept $c\\in{\\mathcal{C}}$ , then we say we are in the realizable setting, and the goal of the learner is to minimize its expected cumulative mistakes ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{M}_{\\cal A}(T,\\mathcal{C}):=\\operatorname*{sup}_{c\\in\\mathcal{C}}\\,\\operatorname*{sup}_{x_{1:T}}\\mathbb{E}\\left[\\sum_{t=1}^{T}\\mathbb{1}\\{\\cal A(x_{t})\\ne c(x_{t})\\}\\right].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Similarly, we define $\\operatorname{M}^{\\star}(T,\\mathcal{C}):=\\operatorname*{inf}_{A}\\,\\operatorname{M}_{A}(T,\\mathcal{C})$ , and an analogous definition of transductive online learnability in the realizable setting holds. ", "page_idx": 4}, {"type": "text", "text": "2.3 Combinatorial Dimensions ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Combinatorial dimensions play an important role in providing a tight quantitative characterization of learnability in learning theory. In this section, we review existing combinatorial dimension in online classification and propose two new dimensions that help us establish the minimax rates for transductive online classification. We start by defining the Littlestone dimension which characterizes multiclass online learnability. ", "page_idx": 4}, {"type": "text", "text": "Definition 1 (Littlestone dimension). The Littlestone dimension of $\\mathcal{C}$ , denoted $\\operatorname{L}(\\mathcal{C})$ , is the largest $d\\,\\in\\,\\mathbb{N}$ such that there exists sequences of functions $\\{X_{t}\\}_{t=1}^{d}$ where $X_{t}:\\{0,1\\}^{t-1}\\,\\to\\,\\chi$ and $\\{Y_{t}\\}_{t=1}^{d}$ where $Y_{t}:\\{0,1\\}^{t}\\rightarrow\\mathcal{Y}$ such that for every $\\sigma\\in\\{0,1\\}^{d}$ , the following holds: ", "page_idx": 4}, {"type": "text", "text": "If for every $d\\ \\in\\ \\mathbb{N},$ , there exists sequences $\\{X_{t}\\}_{t=1}^{d}$ and $\\{Y_{t}\\}_{t=1}^{d}$ satisfying $(i)$ and $(i i)$ , we let $\\operatorname{L}({\\mathcal{C}})=\\infty$ . ", "page_idx": 4}, {"type": "text", "text": "On the other hand, in this paper, we show that a different dimension, termed the Level-constrained Littlestone dimension, characterizes transductive online classification. ", "page_idx": 4}, {"type": "text", "text": "Definition 2 (Level-constrained Littlestone dimension). The Level-constrained Littlestone dimension of $\\mathcal{C}$ , denoted $\\operatorname{D}({\\mathcal{C}})$ , is the largest $d\\in\\mathbb{N}$ such that there exists a sequence of instances $x_{1},..,x_{d}\\in\\mathcal{X}^{d}$ and a sequence of functions $\\{Y_{t}\\}_{t=1}^{d}$ where $Y_{t}:\\{0,1\\}^{t}\\rightarrow\\mathcal{Y}_{}$ , such that for every $\\sigma\\in\\{0,1\\}^{d}$ , the following holds: ", "page_idx": 4}, {"type": "equation", "text": "$Y_{t}((\\sigma_{<t},0))\\neq Y_{t}((\\sigma_{<t},1))\\,f o r\\,a l l\\,t\\in[\\epsilon_{}$ ", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "If for every $d\\in\\mathbb{N},$ , there exist sequences $\\{x_{t}\\}_{t=1}^{d}$ and $\\{Y_{t}\\}_{t=1}^{d}$ satisfying (i) and $(i i)$ , we let $\\operatorname{D}({\\mathcal{C}})=$ $\\infty$ . ", "page_idx": 4}, {"type": "text", "text": "The Littlestone and Level-constrained Littlestone dimensions can also be defined in terms of complete binary trees. A Littlestone tree $\\tau$ of depth $d$ is a complete binary tree of depth $d$ where the internal nodes are labeled by elements of $\\mathcal{X}$ and for every internal node, its two outgoing edges are labeled by distinct elements in $\\boldsymbol{\\wp}$ . Such a tree is shattered by $\\mathcal{C}$ if for every root-to-leaf path $\\bar{\\sigma}\\in\\{0,1\\}^{d}$ , there exists a concept $c_{\\sigma}\\in{\\mathcal{C}}$ consistent with the sequence of instance-label pairs obtained by traversing sdhoawttne $\\tau$ d aLloitntlge $\\sigma$ .o nTe hter eeL itotfl edsetoptnhe .i mFernosmio tnh iiss  ptheresnp tehceti vlaer, gtehset $d\\,\\in\\,\\mathbb{N}$ nfso t haenrde $d$ $\\{X_{t}\\}_{t=1}^{d}$ $\\{Y_{t}\\}_{t=1}^{d}$ in Definition 1 provide the labels on the internal nodes and the outgoing edges of $\\tau$ respectively. Analogously, a Level-constrained Littlestone tree is simply a Littlestone tree with the additional requirement that the instances labeling the internal nodes are the same across each level. In Definition 2, $x_{1}$ labels all the internal nodes on level one, $x_{2}$ labels all the internal nodes on level two, and so forth. The functions $\\{Y_{t}\\}_{t=1}^{d}$ provide the labels on the outgoing edges of a Level-constrained Littlestone tree. Then, the Level-constrained Littlestone dimension is the largest $d\\in\\mathbb{N}$ for which there exists a shattered Level-constrained Littlestone tree of depth $d$ . We will use the function-based and tree-based definitions of these dimensions interchangeably. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Moreover, we show that the Level-constrained Branching dimension characterizes when constant minimax rates are possible in transductive online classification. ", "page_idx": 5}, {"type": "text", "text": "Definition 3 (Level-constrained Branching dimension). The Level-constrained Branching dimension   \nof $\\mathcal{C}$ , denoted $\\operatorname{B}({\\mathcal{C}})$ , is the smallest $p\\in\\mathbb N$ such that for every $d\\in\\mathbb{N},$ , every sequence of instances   \n$x_{1},..,x_{d}\\in\\mathcal{X}^{d}$ , and every sequence of functions $\\{Y_{t}\\}_{t=1}^{d}$ where $Y_{t}:\\{0,1\\}^{t}\\rightarrow\\mathcal{Y}$ : $\\forall\\sigma\\in\\{0,1\\}^{d},\\exists c_{\\sigma}\\in\\mathcal{C}$ such that $c_{\\sigma}(x_{t})=Y_{t}(\\sigma_{\\leq t})$ for all $t\\in[d]$ $\\implies\\arg\\operatorname*{min}_{\\sigma\\in\\{0,1\\}^{d}}\\sum_{t=1}^{d}\\mathbb{1}\\{Y_{t}((\\sigma_{<t},0))\\neq Y_{t}((\\sigma_{<t},1))\\}\\leq p.$ ", "page_idx": 5}, {"type": "text", "text": "If no such $p\\in\\mathbb N$ exist, we let $\\mathrm{B}({\\mathcal{C}})=\\infty$ . ", "page_idx": 5}, {"type": "text", "text": "For a given path $\\sigma\\in\\{0,1\\}^{d}$ , we refer to $\\begin{array}{r}{\\sum_{t=1}^{d}\\mathbb{1}\\{Y_{t}((\\sigma_{<t},0))\\neq Y_{t}((\\sigma_{<t},1))\\}}\\end{array}$ as the branching factor of the path. In terms of trees, a L evel-constrained Branching tree is a Level-constrained Littlestone tree without the restriction that the labels on the outgoing edges of any internal node need to be distinct. Given a path in such a tree, the branching factor of a path counts the number of nodes in the path whose two outgoing edges are labeled by distinct labels in $\\boldsymbol{\\wp}$ . Finally, the Level-constrained Branching dimension can be equivalently defined as the smallest $p\\in\\mathbb N$ such that every shattered Level-constrained Branching tree $\\tau$ of depth $d\\in\\mathbb{N}$ must have at least one path with branching factor at most $p$ . ", "page_idx": 5}, {"type": "text", "text": "The following proposition, whose proof is in Appendix B, establishes the relationship between the three dimensions. ", "page_idx": 5}, {"type": "text", "text": "Proposition 1. For every ${\\mathcal{C}}\\subseteq{\\mathcal{V}}^{{\\mathcal{X}}}$ , we have that $\\mathrm{D}({\\mathcal{C}})\\leq\\mathrm{B}({\\mathcal{C}})\\leq\\mathrm{L}({\\mathcal{C}})$ . ", "page_idx": 5}, {"type": "text", "text": "We also compare our dimensions to other existing dimensions in multiclass learning in Section F. ", "page_idx": 5}, {"type": "text", "text": "3 A Trichotomy in the Realizable Setting ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We start by establishing upper and lower bounds on the minimax expected number of mistakes in the realizable setting in terms of the Level-constrained Littlestone dimension and the Level-constrained Branching dimension. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3 (Mistake bound). For every concept class ${\\mathcal{C}}\\subseteq{\\mathcal{V}}^{{\\mathcal{X}}}$ , we have ", "page_idx": 5}, {"type": "equation", "text": "$$\n{\\overset{!}{2}}\\operatorname*{min}\\Big\\{\\operatorname*{max}\\Big\\{\\operatorname \u1e0a \\mathsf \u1e0a ( \u1e0a \\mathcal \u1e0a C \u1e0c ) \u1e0c ,\\left\\lfloor\\log T \u1e0c \\right\\rfloor\\cdot\\mathbb{I}[\\mathrm{B}(\\mathcal{C})=\\infty]\\Big\\},T\\Big\\}\\leq\\operatorname*{M}^{\\star}(T,\\mathcal{C})\\,\\leq\\,\\operatorname*{min}\\bigg\\{\\mathrm{B}(\\mathcal{C}),\\operatorname \u1e0a \\scriptscriptstyle{\\mathrm{D}}(\\mathcal{C})\\,\\log\\bigg({\\frac{e T}{\\operatorname*{D}(\\mathcal{C})}}\\bigg)\\leq\\frac{\\operatorname*{max}\\Big\\{\\operatorname*{max}\\Big\\}}{\\operatorname*{N}(T,\\mathcal{C})}\\,,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "One can trivially upper bound $\\mathrm{M}^{\\star}(T,{\\mathcal{C}})$ by $\\operatorname{L}(\\mathcal{C})$ . However, by Proposition 1, our upper bound in terms of $\\operatorname{B}({\\mathcal{C}})$ is sharper. We can also infer from the proof in Section 3.3 that when $T$ is large enough (namely $T\\gg2^{\\mathrm{B}({\\mathscr{C}})}$ ), the lower bound in the realizable setting is also $\\frac{\\mathrm{B}({\\mathcal{C}})}{2}$ ", "page_idx": 5}, {"type": "text", "text": "Given Theorem 3, we immediately infer a trichotomy in minimax rates. ", "page_idx": 5}, {"type": "text", "text": "Corollary 1 (Trichotomy). For every concept class ${\\mathcal{C}}\\subseteq{\\mathcal{V}}^{{\\mathcal{X}}}$ , we have ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{M}^{\\star}(T,\\mathcal{C})=\\left\\{\\Theta(1),\\begin{array}{l l}{\\qquad\\mathrm{i}f\\,\\mathrm{B}(\\mathcal{C})<\\infty.}\\\\ {\\qquad\\mathrm{i}f\\,\\mathrm{D}(\\mathcal{C})<\\infty\\,a n d\\,\\mathrm{B}(\\mathcal{C})=\\infty.}\\\\ {\\qquad\\mathrm{e}(T),\\qquad\\mathrm{i}f\\,\\mathrm{D}(\\mathcal{C})=\\infty.}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Proof. (of Corollary 1) When $\\mathrm{B}({\\mathcal{C}})<\\infty$ , Theorem 3 gives that $\\textstyle{\\frac{1}{2}}\\operatorname{D}({\\mathcal{C}})\\leq\\operatorname{M}^{\\star}(T,{\\mathcal{C}})\\leq\\operatorname{B}({\\mathcal{C}})$ for $T\\,\\geq\\,\\mathrm{D}({\\mathcal{C}})$ . When $\\mathrm{B}({\\mathcal{C}})\\,=\\,\\infty$ but $\\operatorname{D}({\\mathcal{C}})<\\infty$ , Theorem 3 gives that $\\begin{array}{r l}{\\frac12\\,\\lfloor\\log T\\rfloor}&{\\leq\\,\\mathrm{M}^{\\star}(T,\\mathcal{C})\\,\\leq}\\end{array}$ $\\scriptstyle\\mathrm{D}({\\mathcal{C}})\\,\\log\\left({\\frac{e T}{\\mathrm{D}({\\mathcal{C}})}}\\right)$ for $\\left\\lfloor\\log T\\right\\rfloor\\,\\geq\\,\\operatorname{D}({\\mathcal{C}})$ . Finally, when $\\operatorname{D}({\\mathcal{C}})\\;=\\;\\infty$ , Theorem 3 gives that $\\begin{array}{r}{{\\frac{T}{2}}\\ \\leq}\\end{array}$ $\\mathrm{M}^{\\star}(T,\\mathcal{C})\\stackrel{\\cdot}{\\leq}T$ . \u53e3 ", "page_idx": 5}, {"type": "text", "text": "The remainder of this Section is dedicated to proving Theorem 3. The proof of the lowerbound $\\mathrm{D}(\\mathcal{C})/2$ follows from standard techniques, so we defer it to Appendix C. ", "page_idx": 5}, {"type": "text", "text": "Proof. Fix $n\\in\\mathbb N$ , a sequence of instances $x_{1:n}:=(x_{1},\\ldots,x_{n})\\in\\chi^{n}$ , a sequence of functions $Y_{1:n}\\,=\\,(Y_{1},\\ldots,Y_{n})$ such that $Y_{t}:\\{0,1\\}^{n}\\,\\to\\,{\\mathcal{Y}}$ , and set of concepts $V\\subseteq\\mathcal{C}$ . If $\\forall\\sigma\\,\\in\\,\\{0,1\\}^{n}$ , there exists $c_{\\sigma}\\,\\in\\,V$ such that $c_{\\sigma}(x_{t})\\,=\\,Y_{t}(\\sigma_{\\leq t})$ for all $t\\,\\in\\,[n]$ , then define $\\mathrm{B}(V,x_{1:n},Y_{1:n}):=$ ar $\\begin{array}{r}{\\mathrm{g}\\operatorname*{min}_{\\sigma\\in\\{0,1\\}^{n}}\\sum_{t=1}^{n}\\mathbb{1}\\{Y_{t}((\\sigma_{<t},0))\\ \\neq\\ Y_{t}((\\bar{\\sigma}_{<t},1))}\\end{array}$ . Otherwise, define $\\mathrm{B}(V,x_{1:n},Y_{1:n})\\,:=\\,0\\,$ Recall that we c an represent $x_{1:n}$ and $Y_{1:n}$ with level-constrained trees $\\tau$ of depth $n$ . With the tree representation, $\\mathrm{B}(V,x_{1:n},Y_{1:n}):=0$ if $V$ does not shatter $\\tau$ . If $\\tau$ is shattered by $V$ , then $\\mathrm{B}(V,\\stackrel{\\cdot}{x_{1:n}},Y_{1:n})$ is the minimum branching factor across all the root-to-leaf paths in $\\tau$ . Recall that the branching factor of a path is the number of nodes in this path whose left and right outgoing edges are labeled by two distinct elements of $\\boldsymbol{\\wp}$ . In this proof, we work with an instance-dependent complexity measure of $V\\subseteq{\\mathcal{C}}$ defined as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathrm{B}(V,x_{1:n}):=\\operatorname*{sup}_{Y_{1:n}}\\mathrm{B}(V,x_{1:n},Y_{1:n}).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We now define a learning algorithm that obtains the claimed mistake bound of $\\operatorname{B}({\\mathcal{C}})$ . Fix a time horizon $T\\in\\mathbb N$ , and let $\\boldsymbol{x}_{1:T}=\\left(x_{1},x_{2},...,x_{T}\\right)$ denote the sequence of instances revealed by the adversary. Initialize $V_{1}:=\\mathcal{C}$ . For every $t\\,\\in\\,\\{1,\\ldots,T\\}$ , if we have $\\{c(x_{t})\\,:\\,c\\,\\in\\,V_{t}\\}\\,=\\,\\{y\\}$ , then predict $\\hat{y}_{t}\\,=\\,y$ . Otherwise, define $V_{t}^{y}\\,=\\,\\{c\\,\\in\\,V_{t}\\,:\\,c(x_{t})\\,=\\,y\\}$ for all $y\\in\\mathcal{V}$ , and predict $\\hat{y}_{t}\\,=\\,\\arg\\operatorname*{max}_{y\\in\\mathcal{Y}}\\,\\mathrm{B}(V_{t}^{y},x_{t+1:T})$ . Finally, the learner receives a feedback $y_{t}\\in\\mathcal{V}$ , and updates $V_{t+1}\\gets V_{t}^{y_{t}}$ . When $t=T$ , the sequence $x_{T+1:T}$ is null and we define $\\hat{y}_{T}=\\arg\\operatorname*{max}_{y\\in\\mathcal{Y}}\\mathrm{B}(V_{T}^{y})$ For this learning algorithm, we prove that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathrm{B}(V_{t+1},x_{t+1:T})\\leq\\mathrm{B}(V_{t},x_{t:T})-\\mathbb{1}\\{y_{t}\\neq\\hat{y}_{t}\\}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Rearranging and summing over $t\\in[T]$ rounds, we obtain: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\mathbb{1}\\{y_{t}\\neq\\hat{y}_{t}\\}\\le\\sum_{t=1}^{T}\\Big(\\operatorname{B}(V_{t},x_{t:T})-\\operatorname{B}(V_{t+1},x_{t+1:T})\\Big)=\\operatorname{B}(V_{1},x_{1:T})-\\operatorname{B}(V_{T+1},x_{T+1:T})}\\\\ &{\\leq\\operatorname{B}(V_{1},x_{1:T})\\leq\\operatorname{B}(\\mathcal{C})}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The equality above follows because the sum telescopes. The final inequality follows because $V_{1}\\ =\\ \\mathcal{C}$ and the level-constrained branching dimension of $\\mathcal{C}$ is defined as $\\mathrm{\\boldmath~\\mathrm{\\scriptstyleB}(\\mathcal{C})~}=$ $\\begin{array}{r}{\\operatorname*{sup}_{T\\in\\mathbb{N}}\\,\\operatorname*{sup}_{x_{1:T}\\in\\mathcal{X}^{T}}\\,\\operatorname{B}(\\mathcal{C},x_{1:T})}\\end{array}$ . ", "page_idx": 6}, {"type": "text", "text": "We now prove inequality (1). There are two cases to consider: (a) $y_{t}=\\hat{y}_{t}$ and (b) $y_{t}\\neq\\hat{y}_{t}$ . Starting with (a), let $y_{t}\\;=\\;\\hat{y}_{t}$ . Recall that $\\mathrm{B}(V_{t+1},x_{t+1:T})\\,=\\,\\mathrm{B}(V_{t}^{y_{t}},x_{t+1:T})$ . Since $c(x_{t})\\,=\\,y_{t}$ for all $h\\in V_{t}^{y_{t}}$ , we must have $\\mathrm{B}(V_{t}^{y_{t}},x_{t+1:T})\\leq\\mathrm{B}(V_{t}^{y_{t}},x_{t:T})$ . Finally, using the fact that $V_{t}^{y t}\\subseteq V_{t}$ , we have $\\bar{\\mathrm{B}}(V_{t}^{y_{t}},x_{t:T})\\leq\\mathrm{B}(V_{t},\\bar{x}_{t:T})$ . This establishes (1) for this case. ", "page_idx": 6}, {"type": "text", "text": "Moving to (b), let $y_{t}\\neq{\\hat{y}}_{t}$ . Note that we must have $\\mathrm{B}(V_{t},x_{t:T})>0$ . Otherwise, if $\\mathrm{B}(V_{t},x_{t:T})=0$ , then we have $\\{c(\\bar{x_{t}}):\\bar{c}\\in V_{t}\\}=\\{y\\}$ . So, by our prediction rule, we cannot have $y_{t}\\neq{\\hat{y}}_{t}$ under realizability. To establish (1), we want to show that $\\mathrm{B}(V_{t+1},x_{t+1:T})<\\mathrm{B}(V_{t},x_{t:T})$ . Suppose, for the sake of contradiction, we instead have $\\mathrm{B}(V_{t+1},x_{t+1:T})\\,\\geq\\,\\mathrm{B}(V_{t},x_{t:T})$ . Then, let us define $d:=\\mathrm{B}(V_{t+1},x_{t+1:T})$ . If $d=0$ , then our proof is complete because $\\mathrm{0}\\leq\\mathrm{B}(\\dot{V}_{t},x_{t:T})-\\mathbb{1}\\{y_{t}\\neq\\hat{y}_{t}\\}$ Assume that $d>0$ and recall that $V_{t+1}\\stackrel{=}{=}V_{t}^{y_{t}}$ . By definition of $\\mathrm{B}(V_{t}^{y_{t}},x_{t+1:T})$ and its equivalent shattered-trees representation, there exists a level-constrained tree $\\mathcal{T}_{y_{t}}$ of depth $T-t$ whose internal nodes are labeled by $x_{t},\\ldots,x_{T}$ and is shattered by $V_{t}^{y_{t}}$ . Moreover, every path down $\\mathcal{T}_{y_{t}}$ has branching factor $\\geq d$ . ", "page_idx": 6}, {"type": "text", "text": "Next, as $\\begin{array}{r}{\\hat{y}_{t}=\\arg\\operatorname*{max}_{y\\in\\mathcal{Y}}\\mathrm{B}(V_{t}^{y},x_{t+1:T})}\\end{array}$ , we further have $\\mathrm{B}(V_{t}^{\\hat{y}_{t}},x_{t+1:T})\\geq\\mathrm{B}(V_{t}^{y_{t}},x_{t+1:T})\\geq d$ Thus, there exists another level-constrained tree $\\mathcal{T}_{\\hat{y}_{t}}$ of depth $T-t$ whose internal nodes are labeled by $\\boldsymbol{x}_{t},\\boldsymbol{\\cdot}\\boldsymbol{\\cdot}\\boldsymbol{\\cdot},\\boldsymbol{x}_{T}$ , that is shattered by $V_{t}^{\\hat{y}_{t}}$ , and every path down $\\mathcal{T}_{\\hat{y}_{t}}$ has branching factor $\\geq d$ . Finally, consider a new tree $\\tau$ with root-node labeled by $x_{t}$ , the left-outgoing edge from the root node is labeled by $y_{t}$ , and the right outgoing edge is labeled by $\\hat{y}_{t}$ . Moreover, the subtree following the outgoing edge labeled by $y_{t}$ is $\\mathcal{T}_{y_{t}}$ , and the subtree following the outgoing edge labeled by $\\hat{y}_{t}$ is $\\mathcal{T}_{\\hat{y}_{t}}$ . Since both $\\mathcal{T}_{y_{t}}$ and $\\mathcal{T}_{\\hat{y}_{t}}$ are valid level-constrained trees each with internal nodes labeled by $x_{t+1},\\dots,x_{T}$ , the newly constructed tree $\\tau$ is a also a level-constrained trees of depth $T-t+1$ with internal nodes labeled by $x_{t},\\ldots,x_{T}$ . In addition, as $\\mathcal{T}_{y_{t}}$ and $\\mathcal{T}_{\\hat{y}_{t}}$ are shattered by $V_{t}^{y_{t}}$ and $V_{t}^{\\hat{y}_{t}}$ respectively, the tree $\\tau$ must be shattered by $V_{t}^{y_{t}}\\cup V_{t}^{\\hat{y}_{t}}$ . Finally, as every path down each sub-trees $\\mathcal{T}_{y_{t}}$ and $\\mathcal{T}_{\\hat{y}_{t}}$ has branching factor $\\geq d$ and $y_{t}\\neq{\\hat{y}}_{t}$ , every path of $\\tau$ must have branching factor $\\ge d+1$ . This shows that $\\mathrm{B}(V_{t}^{y_{t}}\\cup V_{t}^{\\hat{y}_{t}},x_{t:T})\\geq d+1$ . And since $V_{t}^{y_{t}}\\cup V_{t}^{\\hat{y}_{t}}\\subseteq V_{t}$ , we have $d+1\\leq\\mathrm{B}(V_{t}^{y_{t}}\\cup V_{t}^{\\hat{y}_{t}},x_{t:T})\\leq\\mathrm{B}(V_{t},x_{t:T})$ by monotonicity. This contradicts our assumption that $d:=\\mathrm{B}(V_{t+1},x_{t+1:T})\\geq\\mathrm{B}(V_{t},x_{t:T}).$ . Therefore, we must have $\\mathrm{B}(V_{t+1},x_{t+1:T})<\\mathrm{B}(V_{t},\\bar{x}_{t:T})$ . This establishes (1), completing our proof. \u53e3 ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Proof. Fix the time horizon $T\\in\\mathbb{N}$ and let $x_{1:T}:=(x_{1},...,x_{T})\\in\\mathcal{X}^{T}$ be the sequence of $T$ instances revealed to the learner at the beginning of the game. We say a subsequence $\\bar{x}_{1:n}^{\\prime}:=(x_{1}^{\\prime},...,x_{n}^{\\prime})$ , preserving the same order as in $x_{1:T}$ , is shattered by $V\\subseteq{\\mathcal{C}}$ if there exists a sequence of functions $\\bar{\\{Y_{t}\\}}_{t=1}^{n}$ , where $Y_{t}:\\{0,1\\}^{t}\\rightarrow\\mathcal{Y}$ , such that for every $\\sigma\\in\\{0,1\\}^{n}$ , we have that ", "page_idx": 7}, {"type": "equation", "text": "$Y_{t}(\\sigma_{<t},0)\\neq Y_{t}(\\sigma_{<t},1)\\;\\mathrm{for}\\;\\mathrm{all}\\;t\\in[r$ ", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "For every $V\\subseteq{\\mathcal{C}}$ , let $\\mathrm{S}(V)$ be the number of subsequences of $x_{1:T}$ shattered by $V$ . In addition, for every $(x,y)\\in\\mathcal{X}\\times\\mathcal{Y}$ , let $V_{(x,y)}:=\\{c\\in V:c(x)\\stackrel{\\cdot}{=}y\\}$ . Consider the following online learner. At tphree dbiectgsi , rr iencietiiavleisz $V^{1}=\\mathcal{C}$ .d  Tuhpedna,t iens $t\\in[T]$ , the learner $\\tilde{y}_{t}\\in\\arg\\operatorname*{max}_{y\\in\\mathcal{Y}}\\mathrm{S}(V_{(x_{t},y)}^{t})$ $y_{t}\\in\\mathcal{V}$ $V^{t+\\bar{1}}\\gets V_{(x_{t},y_{t})}^{t}$ ", "page_idx": 7}, {"type": "text", "text": "For this learning algorithm, we claim that ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathrm{S}(V^{t+1})\\leq\\operatorname*{max}\\Bigl\\{\\mathbb{1}\\{y_{t}=\\hat{y}_{t}\\},\\frac{1}{2}\\Bigr\\}\\mathrm{S}(V^{t})\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "for every round $t\\ \\in\\ [T]$ . This implies the stated mistake bound since $\\begin{array}{r}{\\mathrm{S}(\\mathcal{C})\\;\\leq\\;\\sum_{i=0}^{\\mathrm{D}(\\mathcal{C})}\\binom{T}{i}\\;\\leq}\\end{array}$ $\\left({\\frac{e T}{\\mathrm{D}({\\mathcal{C}})}}\\right)^{\\mathrm{D}({\\mathcal{C}})}$ and the learner can make at most $\\log(\\mathrm{S}(\\mathcal{C}))$ mistakes before $\\mathrm{S}(V^{t})=1$ . We now prove this claim by considering the case where ${\\hat{y}}_{t}=y_{t}$ and $\\hat{y}_{t}\\neq y_{t}$ separately. ", "page_idx": 7}, {"type": "text", "text": "Let $t\\in[T]$ be a round where ${\\hat{y}}_{t}=y_{t}$ . Then, $\\mathrm{S}(V^{t+1})\\leq\\mathrm{S}(V^{t})$ since $V^{t+1}=V_{(x_{t},y_{t})}^{t}\\subseteq V^{t}$ . Now, let $t\\in[T]$ be a round where ${\\hat{y}}_{t}\\neq y_{t}$ . We need to show that $\\begin{array}{r}{\\mathrm{S}(V^{t+1})\\leq\\frac{1}{2}\\,\\mathrm{S}(V^{t})}\\end{array}$ . For any $V\\subseteq{\\mathcal{C}}$ , let $\\operatorname{Sh}(V)$ be the set of all subsequences of $x_{1:T}$ that are shattered by $V$ . Then, for any subsequence $q\\in\\mathrm{Sh}(V^{t})$ , only one of the following properties must be true: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{q\\not\\in\\mathrm{Sh}(V_{(x_{t},y_{t})}^{t})\\mathrm{~and~}q\\not\\in\\mathrm{Sh}(V_{(x_{t},\\hat{y}_{t})}^{t}),}\\\\ &{q\\in\\mathrm{Sh}(V_{(x_{t},y_{t})}^{t})\\Delta\\mathrm{Sh}(V_{(x_{t},\\hat{y}_{t})}^{t}),}\\\\ &{q\\in\\mathrm{Sh}(V_{(x_{t},y_{t})}^{t})\\cap\\mathrm{Sh}(V_{(x_{t},\\hat{y}_{t})}^{t}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\Delta$ denotes the symmetric difference. For every $i\\in\\{1,2,3\\}$ , let $\\mathrm{Sh}^{i}(V^{t})\\subseteq\\mathrm{Sh}(V^{t})$ be the subset of $\\mathrm{Sh}(V^{t})$ that satisfies property $(i)$ . Note that $\\begin{array}{r}{\\mathrm{Sh}(V^{t})=\\bigcup_{i=1}^{3}\\mathrm{Sh}^{i}(V^{t})}\\end{array}$ and $\\{\\mathrm{Sh}^{i}(V^{t})\\}_{i=1}^{3}$ are pairwise disjoint. Therefore, $\\{\\mathrm{Sh}^{i}(V^{t})\\}_{i=1}^{3}$ forms a partition of $\\operatorname{Sh}(V^{t})$ . For each $i\\in\\{1,2,3\\}$ , we compute how many elements of $\\mathrm{Sh}^{i}(V^{t})$ we drop when going from $\\operatorname{Sh}(V^{t})$ to $\\mathrm{Sh}(V^{t+1})$ . We can then upperbound $|\\operatorname{Sh}(V_{(x_{t},y_{t}}^{t})|=\\mathrm{S}(V^{t+1})$ by lower bounding $\\mid\\mathrm{Sh}(V^{t})\\setminus\\mathrm{Sh}(V^{t+1})\\mid$ , the number of elements we drop across all of the subsets $\\{\\mathrm{Sh}^{i}(V^{t})\\}_{i=1}^{3}$ when going from $V^{t}$ to $V^{t+1}$ . ", "page_idx": 7}, {"type": "text", "text": "Starting with $i=1$ , observe that for every $q\\in\\mathrm{Sh}^{1}(V^{t})$ , we have that $q\\not\\in\\mathrm{Sh}(V_{(x_{t},y_{t})}^{t})$ . Therefore, $|\\operatorname{Sh}^{1}(V^{t})\\cap\\operatorname{Sh}(V_{(x_{t},y_{t})}^{t})|=0$ , implying that we drop all the elements from $\\mathrm{Sh}^{1}(V^{t})$ when going from $\\mathrm{Sh}(V^{t})$ to $\\mathrm{Sh}(V^{t+1})$ . ", "page_idx": 7}, {"type": "text", "text": "For the case where $i~=~2$ , note that $\\mathrm{Sh}(V_{(x_{t},y_{t})}^{t}),\\mathrm{Sh}(V_{(x_{t},\\hat{y}_{t})}^{t})\\ \\subseteq\\ \\mathrm{Sh}(V^{t})$ and $\\mathrm{S}(V_{(x_{t},\\hat{y}_{t})}^{t})\\,\\geq$ $\\mathrm{S}(V_{(x_{t},y_{t})}^{t})$ , where the latter inequality is true by the definition of the prediction rule. Moreover, using the fact that $\\{\\mathrm{Sh}^{i}(V^{t})\\}_{i=1}^{3}$ forms a partition of $\\mathrm{Sh}(V^{t})$ , we can write ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathrm S(V_{(x_{t},\\hat{y}_{t})}^{t})=|\\,\\mathrm{Sh}^{3}(V^{t})|+|\\,\\mathrm{Sh}^{2}(V^{t})\\cap\\mathrm{Sh}(V_{(x_{t},\\hat{y}_{t})}^{t})|\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "and ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathrm{S}(V_{(x_{t},y_{t})}^{t})=|\\,\\mathrm{Sh}^{3}(V^{t})|+|\\,\\mathrm{Sh}^{2}(V^{t})\\cap\\mathrm{Sh}(V_{(x_{t},y_{t}}^{t})|.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Since $\\begin{array}{r l r}{\\mathrm{S}(V_{(x_{t},\\hat{y}_{t})}^{t})}&{{}\\ge}&{\\mathrm{S}(V_{(x_{t},y_{t})}^{t})}\\end{array}$ , we get that $|\\,\\mathrm{Sh}^{2}(V^{t})\\,\\cap\\,\\mathrm{Sh}(V_{(x_{t},\\hat{y}_{t})}^{t})|\\,\\,\\,\\geq\\,\\,\\,|\\,\\mathrm{Sh}^{2}(V^{t})\\,\\cap\\,\\,\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\,\\mathrm{Sh}^{2}(V^{t})\\,\\cap\\,\\mathrm{Sh}(V_{(x_{t},\\hat{y}_{t})}^{t})|\\,\\,\\,\\,\\geq\\,\\,\\,|\\,\\mathrm{Sh}^{2}(V^{t})\\,\\cap\\,\\,\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\,\\,\\mathrm{Sh}^{2}(V^{t})\\,\\cap\\,\\mathrm{Sh}(V^{t})\\,,$ $\\mathrm{Sh}(V_{(x_{t},y_{t})}^{t})|$ . This implies that $\\begin{array}{r l r}{|\\operatorname{Sh}^{2}(V^{t})\\cap\\operatorname{Sh}(V_{(x_{t},y_{t})}^{t})|}&{\\leq}&{\\frac{1}{2}|\\operatorname{Sh}^{2}(V^{t})|}\\end{array}$ since $\\left(\\mathrm{Sh}^{2}(V^{t})\\cap$ $\\operatorname{Sh}(V_{(x_{t},\\hat{y}_{t})}^{t}))\\cup\\!\\left(\\operatorname{Sh}^{2}(V^{t})\\cap\\operatorname{Sh}(V_{(x_{t},y_{t})}^{t})\\right)\\!=\\!\\operatorname{Sh}^{2}(V^{t})$ and $\\Big(\\mathrm{Sh}^{2}(V^{t})\\cap\\mathrm{Sh}(V_{(x_{t},\\hat{y}_{t})}^{t})\\Big)\\cap\\Big(\\mathrm{Sh}^{2}(V^{t})\\cap$ $\\operatorname{Sh}(V_{(x_{t},y_{t})}^{t})\\Big)=\\emptyset$ . Thus, we drop at least half the elements from $\\mathrm{Sh}^{2}(V^{t})$ when going from $\\mathrm{Sh}(V^{t})$ to $\\mathrm{Sh}(V^{t+1})$ . ", "page_idx": 8}, {"type": "text", "text": ",  ocuotnpsuitds w ohne $i=3$ .l l $q\\in\\mathrm{Sh}^{3}(V^{t})$ t.i oWn,e t hbaetc $x_{1},...,x_{t}\\notin q$ $c\\in V^{t}$ $y_{j}$ $x_{j}$ $j\\leq t-1$ $x_{t}\\not\\in q$ $q\\in\\mathrm{Sh}(V_{(x_{t},y_{t})}^{t})\\cap\\mathrm{Sh}(V_{(x_{t},\\hat{y}_{t})}^{t})$ and every concept in V (xt,yt) and V (txt,y\u02c6t) outputs yt and y\u02c6t on xt respectively. Thus, the sequence $x_{t}\\circ q$ , obtained by concatenating $x_{t}$ to the front of $q$ , is a valid subsequence of $x_{1:T}$ . Since ${\\hat{y}}_{t}\\neq y_{t}$ , we also have that $x_{t}\\circ q$ is shattered by $V^{t}$ . Using the fact that $x_{t}\\bar{\\textrm{o}}q\\not\\in\\mathrm{Sh}(V_{(x_{t},y_{t})}^{t})$ and $x_{t}\\circ q\\not\\in\\mathrm{Sh}(V_{(x_{t},\\hat{y}_{t})}^{t})$ , gives that $x_{t}\\circ q\\in\\mathrm{Sh}^{1}(V_{t})$ . Since our choice of $q$ was arbitrary, this implies that for every $q\\in\\mathrm{Sh}^{3}(V_{t})$ , there exists a subsequence $q^{\\prime}=x_{t}\\circ q\\in\\mathrm{Sh}^{1}(V^{t})$ , ultimately giving that $\\mid\\mathrm{Sh}^{1}(V^{t})\\mid\\geq\\mid\\mathrm{Sh}^{3}(V^{t})\\mid$ . ", "page_idx": 8}, {"type": "text", "text": "To complete the proof, we lowerbound the total number of dropped elements when going from $\\mathrm{Sh}(V^{t})$ to $\\mathrm{Sh}(V^{t+1})$ by ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\operatorname{Sh}(V^{t})\\setminus\\operatorname{Sh}(V_{(x_{t},y_{t})}^{t})|\\ge|\\operatorname{Sh}^{1}(V^{t})|+\\displaystyle\\frac{|\\operatorname{Sh}^{2}(V^{t})|}{2}}\\\\ &{\\phantom{\\geq\\frac{|\\operatorname{Sh}^{1}(V^{t})|}{2}}\\geq\\displaystyle\\frac{|\\operatorname{Sh}^{1}(V^{t})|}{2}+\\frac{|\\operatorname{Sh}^{2}(V^{t})|}{2}+\\frac{|\\operatorname{Sh}^{3}(V^{t})|}{2}}\\\\ &{=\\displaystyle\\frac{|\\operatorname{Sh}(V^{t})|}{2}=\\frac{\\operatorname{S}(V^{t})}{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "The number of remaining elements is then $\\mathrm{S}(V^{t+1})\\,=\\,|\\,\\mathrm{Sh}(V_{(x_{t},y_{t})}^{t})|\\,=\\,|\\,\\mathrm{Sh}(V^{t})|\\,-\\,|\\,\\mathrm{Sh}(V^{t})\\,\\backslash$ $\\begin{array}{r}{\\mathrm{Sh}(V_{(x_{t},y_{t})}^{t})|\\leq\\frac{1}{2}\\,\\mathrm{S}(V^{t})}\\end{array}$ , as needed. \u53e3 ", "page_idx": 8}, {"type": "text", "text": "We end this section by noting that the algorithm in the proof of Theorem 3 can be made conservative (i.e. does not update when it is correct) with the same mistake bound. This conservative-version of the realizable learner will be used when proving regret bounds in the agnostic setting (see Section E). ", "page_idx": 8}, {"type": "text", "text": "3.3 Proof of Lowerbound $\\frac{\\lfloor\\log T\\rfloor}{2}\\ \\mathbb{1}[\\mathrm{B}(\\mathcal{C})=\\infty]$ ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "If $\\mathrm{B}({\\mathcal{C}})=\\infty$ , then for every $q\\in\\mathbb{N}$ , Definition 3 guarantees the existence of $d\\in\\mathbb{N}$ , a sequence of instances $x_{1},\\ldots,x_{d}$ , and a sequence of functions $Y_{1},\\ldots,Y_{d}$ where $Y_{t}:\\{0,1\\}^{t}\\rightarrow\\mathcal{Y}$ such that the following holds: (i) $\\forall\\sigma\\in\\{0,1\\}^{d}$ , there exists $c_{\\sigma}\\in{\\mathcal{C}}$ such that $c_{\\sigma}(x_{t})=Y_{t}(\\sigma_{\\leq t})$ (ii) $\\forall\\sigma\\in\\{0,1\\}^{d}$ , we have $\\begin{array}{r}{\\sum_{t=1}^{d}\\mathbb{1}\\{Y_{t}((\\sigma_{<t},0))\\neq Y_{t}((\\sigma_{<t},1))\\}\\ge q}\\end{array}$ . Equivalently, there exists a shattered levelconstrain ed branching tree $\\tau$ of depth $d$ with internal nodes labeled by instances $x_{1},\\ldots,x_{d}$ such that every path down the tree $\\tau_{\\mathrm{has}}\\geq q$ branching factor. Recall that the branching factor of a path is the number of nodes in the path whose two outgoing edges are labeled by two distinct elements of $\\boldsymbol{\\wp}$ . We say that an internal node has branching if the left and right outgoing edges from the node are labeled by two distinct elements of $\\boldsymbol{\\wp}$ . ", "page_idx": 8}, {"type": "text", "text": "Without loss of generality, we will assume that the $\\tau$ guaranteed by Definition 3 has the following properties: (a) every path in $\\tau$ has exactly $q$ branching factor and (b) $\\tau$ is symmetric along its non-branching nodes\u2013 that is, for every node in $\\tau$ that has no branching, the subtrees on its left and right outgoing edges are identical. There is no loss in generality because given $\\tau$ without property (a), we can traverse down every path in $\\tau$ , and once the path has branching factor $q$ , label all the subsequent outgoing edges down the path by a concept in $\\mathcal{C}$ that shatters any completion of that path. For property (b), if any non-branching node has two different subtrees, then replace the right subtree with the left subtree. Given such tree $\\tau$ , let $\\operatorname{BN}(\\tau)$ denote the number of levels in $\\tau$ with at least one branching node. The following Lemma, whose proof can be found in Appendix D, provides an upperbound on $\\operatorname{BN}({\\mathcal{T}})$ . ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Lemma 1. Let $\\tau$ be any level-constrained branching tree shattered by $\\mathcal{C}$ such that: (a) every path in $\\tau$ has exactly $q\\in\\mathbb{N}$ branching and $(b)$ for every node in $\\tau$ without branching, the subtrees on its left and right outgoing edges are identical. Then, $\\mathrm{BN}({\\mathcal T})\\leq2^{q}-1$ . ", "page_idx": 9}, {"type": "text", "text": "Given this Lemma, we now prove the claimed lowerbound of $\\frac{\\lfloor\\log T\\rfloor}{2}\\;\\mathbb{1}[\\mathrm{B}(\\mathcal{C})\\;=\\;\\infty]$ . Assume $\\mathrm{B}({\\mathcal{C}})=\\infty$ and take $q=\\lfloor\\log{\\bar{T}}\\rfloor$ . Definition 3 guarantees the existence of shattered Level-constrained branching tree $\\tau$ that satisfies property (a) and (b) specified in Lemma 1. Next, Lemma 1 implies that $\\mathrm{BN}({\\bar{T}})\\leq2^{\\lfloor\\log T\\rfloor}-1\\leq{\\bar{T}}-1$ . Let $d$ be the depth $\\tau$ and $S\\,\\subseteq\\,\\{1,\\ldots,d\\}$ be the levels in $\\tau$ with at least one branching node. By definition, we have $|S|=\\mathrm{BN}({\\mathcal{T}})\\leq T-1$ . Recall that $\\tau$ can be identified by a sequence of instances $x_{1},\\ldots,x_{d}$ and a sequence of functions $Y_{1},\\ldots,Y_{d}$ where $Y_{t}:\\{0,1\\}^{t}\\rightarrow{\\dot{\\boldsymbol{y}}}$ for every $t\\in[d]$ . For any path $\\sigma\\in\\{0,1\\}^{d}$ down $\\tau$ , the set $\\{Y_{t}(\\sigma_{\\leq t})\\}_{t=1}^{d}$ gives the labels along this path. Moreover, as all the branching on occurs on levels in $S$ , we have $\\begin{array}{r}{\\sum_{t=1}^{d}\\mathbb{1}\\{Y_{t}((\\sigma_{<t},0))\\neq Y_{t}((\\sigma_{<t},1))\\}=\\sum_{t\\in S}\\mathbb{1}\\{Y_{t}((\\sigma_{<t},0))\\neq Y_{t}((\\sigma_{<t},1))\\}=\\lfloor\\log T\\rfloor}\\end{array}$ for every path $\\sigma\\in\\{0,1\\}^{d}$ . ", "page_idx": 9}, {"type": "text", "text": "We now specify the stream to be observed by the learner $\\boldsymbol{\\mathcal{A}}$ . Draw $\\sigma\\,\\sim\\,\\mathrm{Uniform}(\\{0,1\\}^{d})$ and consider the stream $\\{(x_{t},Y_{t}(\\sigma_{\\leq t}))\\}_{t\\in S}$ . Repeat $(x_{m},Y(\\sigma_{\\leq m}))$ for remaining $T-|S|$ timepoints, where $m$ is the largest index in set $S$ . Since this stream is a sequence of instance-label pairs along the path $\\sigma$ in the shattered tree $\\tau$ , there exists a $c_{\\sigma}\\in{\\mathcal{C}}$ consistent with the stream. However, using similar arguments as in the proof of the lowerbound $\\mathrm{D}({\\mathcal{C}})/2$ , we can establish ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T}\\mathbb{1}\\{A(x_{t})\\neq Y_{t}(\\sigma_{\\leq t})\\}\\right]}&{}\\\\ &{\\geq\\mathbb{E}\\left[\\displaystyle\\sum_{t\\in S}\\mathbb{1}\\{A(x_{t})\\neq Y_{t}(\\sigma_{\\leq t})\\}\\right]}\\\\ &{\\geq\\displaystyle\\frac{1}{2}\\mathbb{E}\\left[\\displaystyle\\sum_{t\\in S}\\mathbb{1}\\left\\{Y_{t}((\\sigma_{<t},0))\\neq Y_{t}\\big((\\sigma_{<t},1)\\big)\\right\\}\\right]}\\\\ &{=\\displaystyle\\frac{1}{2}[\\log T].}\\end{array}\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "This completes our proof of lower bound. ", "page_idx": 9}, {"type": "text", "text": "4 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we study the problem of multiclass transductive online learning with possibly arbitrary label space. In the realizable setting, we establish a trichotomy in the possible minimax rates of the expected number of mistakes. Furthermore, we show near-tight upper and lower bounds on the optimal expected regret in the agnostic setting. Along the way, we introduce two new combinatorial complexity parameters, called the Level-constrained Littlestone dimension and the Level-constrained Branching dimension. ", "page_idx": 9}, {"type": "text", "text": "Finally, we highlight some future directions of this work. First, can we extend our results to settings such as transductive online learning under bandit feedback, list transductive online learning, and transductive online real-valued regression? Moreover, as our shattering technique is general, can we use similar ideas to establish the possible minimax rates of the number of mistakes in the self-directed and the best-order settings initially studied in [Ben-David et al., 1995, 1997]? ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "VR acknowledges support from the NSF Graduate Research Fellowship Program. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Scott Aaronson, Xinyi Chen, Elad Hazan, Satyen Kale, and Ashwin Nayak. Online learning of quantum states. Advances in neural information processing systems, 31, 2018.   \nNoga Alon, Roi Livni, Maryanthe Malliaris, and Shay Moran. Private pac learning implies finite littlestone dimension. In Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing, pages 852\u2013860, 2019.   \nNoga Alon, Mark Bun, Roi Livni, Maryanthe Malliaris, and Shay Moran. Private and online learnability are equivalent. ACM Journal of the ACM (JACM), 69(4):1\u201334, 2022.   \nIdan Attias, Steve Hanneke, Alkis Kalavasis, Amin Karbasi, and Grigoris Velegkas. Optimal learners for realizable regression: Pac learning and online learning. arXiv preprint arXiv:2307.03848, 2023.   \nShai Ben-David and Nadav Eiron. Self-directed learning and its relation to the vc-dimension and to teacher-directed learning. Machine Learning, 33:87\u2013104, 1998.   \nShai Ben-David, Nicol\u00f2 Cesa-Bianchi, and Philip M. Long. Characterizations of learnability for classes of O, ..., n-valued functions, 1992.   \nShai Ben-David, Nadav Eiron, and Eyal Kushilevitz. On self-directed learning. In Proceedings of the eighth annual conference on Computational learning theory, pages 136\u2013143, 1995.   \nShai Ben-David, Eyal Kushilevitz, and Yishay Mansour. Online learning versus offline learning. Machine Learning, 29:45\u201363, 1997.   \nShai Ben-David, D\u00e1vid P\u00e1l, and Shai Shalev-Shwartz. Agnostic online learning. In Conference on Learning Theory, volume 3, page 1, 2009.   \nOlivier Bousquet, Steve Hanneke, Shay Moran, Ramon Van Handel, and Amir Yehudayoff. A theory of universal learning. In Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing, pages 532\u2013541, 2021.   \nNataly Brukhim, Elad Hazan, Shay Moran, Indraneel Mukherjee, and Robert E Schapire. Multiclass boosting and the cost of weak learning. Advances in Neural Information Processing Systems, 34: 3057\u20133067, 2021.   \nNataly Brukhim, Daniel Carmon, Irit Dinur, Shay Moran, and Amir Yehudayoff. A characterization of multiclass learnability, 2022.   \nMark Bun, Roi Livni, and Shay Moran. An equivalence between private classification and online prediction. In 2020 IEEE 61st Annual Symposium on Foundations of Computer Science (FOCS), pages 389\u2013402. IEEE, 2020.   \nAmit Daniely and Tom Helbertal. The price of bandit information in multiclass online classification. In Conference on Learning Theory, pages 93\u2013104. PMLR, 2013.   \nAmit Daniely and Shai Shalev-Shwartz. Optimal learners for multiclass problems. In Conference on Learning Theory, pages 287\u2013316. PMLR, 2014.   \nAmit Daniely, Sivan Sabato, Shai Ben-David, and Shai Shalev-Shwartz. Multiclass learnability and the erm principle. In Proceedings of the 24th Annual Conference on Learning Theory, pages 207\u2013232. JMLR Workshop and Conference Proceedings, 2011.   \nAmit Daniely, Sivan Sabato, and Shai Shwartz. Multiclass learning approaches: A theoretical comparison with implications. Advances in Neural Information Processing Systems, 25, 2012.   \nOfir David, Shay Moran, and Amir Yehudayoff. Supervised learning through the lens of compression. Advances in Neural Information Processing Systems, 29, 2016.   \nPramith Devulapalli and Steve Hanneke. The dimension of self-directed learning, 2024.   \nJesse Geneson. A note on the price of bandit feedback for mistake-bounded online learning. Theoretical Computer Science, 874:42\u201345, 2021. ", "page_idx": 10}, {"type": "text", "text": "Steve Hanneke and Liu Yang. Bandit learnability can be undecidable. In The Thirty Sixth Annual Conference on Learning Theory, pages 5813\u20135849. PMLR, 2023. ", "page_idx": 11}, {"type": "text", "text": "Steve Hanneke, Shay Moran, Vinod Raman, Unique Subedi, and Ambuj Tewari. Multiclass online learning and uniform convergence. In The Thirty Sixth Annual Conference on Learning Theory, pages 5682\u20135696. PMLR, 2023a. ", "page_idx": 11}, {"type": "text", "text": "Steve Hanneke, Shay Moran, and Jonathan Shafer. A trichotomy for transductive online learning. Advances in Neural Information Processing Systems, 36, 2023b. ", "page_idx": 11}, {"type": "text", "text": "Steve Hanneke, Shay Moran, and Qian Zhang. Universal rates for multiclass learning. In The Thirty Sixth Annual Conference on Learning Theory, pages 5615\u20135681. PMLR, 2023c. ", "page_idx": 11}, {"type": "text", "text": "David Haussler and Philip M Long. A generalization of sauer\u2019s lemma. Journal of Combinatorial Theory, Series A, 71(2):219\u2013240, 1995. ", "page_idx": 11}, {"type": "text", "text": "Wilfrid Hodges. A shorter model theory. Cambridge university press, 1997. ", "page_idx": 11}, {"type": "text", "text": "Nick Littlestone. Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm. Machine learning, 2:285\u2013318, 1988. ", "page_idx": 11}, {"type": "text", "text": "Philip M Long. New bounds on the price of bandit feedback for mistake-bounded online multiclass learning. In International Conference on Algorithmic Learning Theory, pages 3\u201310. PMLR, 2017. ", "page_idx": 11}, {"type": "text", "text": "Preetham Mohan and Ambuj Tewari. Quantum learning theory beyond batch binary classification. arXiv preprint arXiv:2302.07409, 2023. ", "page_idx": 11}, {"type": "text", "text": "Balas K Natarajan. On learning sets and functions. Machine Learning, 4:67\u201397, 1989. ", "page_idx": 11}, {"type": "text", "text": "Balas K Natarajan and Prasad Tadepalli. Two new frameworks for learning. In Machine Learning Proceedings 1988, pages 402\u2013415. Elsevier, 1988. ", "page_idx": 11}, {"type": "text", "text": "Ananth Raman, Vinod Raman, Unique Subedi, and Ambuj Tewari. Multiclass online learnability under bandit feedback, 2023. ", "page_idx": 11}, {"type": "text", "text": "Benjamin Rubinstein, Peter Bartlett, and J Rubinstein. Shifting, one-inclusion mistake bounds and tight multiclass expected risk bounds. Advances in Neural Information Processing Systems, 19, 2006. ", "page_idx": 11}, {"type": "text", "text": "Saharon Shelah. Classification theory: and the number of non-isomorphic models. Elsevier, 1990. ", "page_idx": 11}, {"type": "text", "text": "Vladimir Vapnik and Alexey Chervonenkis. Theory of pattern recognition, 1974. ", "page_idx": 11}, {"type": "text", "text": "Vladimir N. Vapnik. Estimation of Dependencies Based on Empirical Data. Springer-Verlag, New York, NY, 1982. ", "page_idx": 11}, {"type": "text", "text": "Vladimir N. Vapnik. Statistical Learning Theory. Wiley, 1998. ", "page_idx": 11}, {"type": "text", "text": "A Related Work ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Online Learning. Online learning has been a subject of study for more than half a century. Moreover, the seminal work by Littlestone [1988] initiated this line of research within the computer science community. Since that pivotal contribution, online learning has been explored in various settings, from learning under the bandit feedback Daniely et al. [2011], Daniely and Helbertal [2013], Long [2017], Geneson [2021], Raman et al. [2023], Hanneke and Yang [2023] to quantum settings Aaronson et al. [2018], Mohan and Tewari [2023]. Furthermore, it is also linked to a broad set of problems, such as differential privacy, highlighted in studies by Alon et al. [2019], Bun et al. [2020], Alon et al. [2022]. Further, given its fundamental nature, it is not surprising that online learning has found numerous practical applications. ", "page_idx": 12}, {"type": "text", "text": "Transductive and other Online Learning Frameworks. The concept of the transductive learning model traces its origins to seminal works by Vapnik Vapnik and Chervonenkis [1974], Vapnik [1982, 1998], where it was explored within the PAC learning framework. Subsequently, Ben-David et al. [1997] initiated the study of this model under the umbrella of online learning, referring to it as \u201coffline learning\u201d. They utilizes a notion of rank based dimension to prove their results. Notably, as we mentioned in the introduction, our Level-constrained Branching dimension is exactly equal to their rank based dimension. A significant advancement came recently with the work of Hanneke et al. [2023b]. Furthermore, other conceptually related models have also been rigorously studied, as seen in works by Goldman and Sloan [1994], Ben-David et al. [1995], Ben-David and Eiron [1998], Devulapalli and Hanneke [2024]. These studies notably include the self-directed online learning framework, which allows the learning algorithm to select the next instance for prediction from the remaining set of instances in each round, and additionally, the best order, which allows the learner (instead of an adversary) to select the order at the beginning of the game. ", "page_idx": 12}, {"type": "text", "text": "Multiclass Classification. A substantial volume of theoretical research has been conducted on various aspects of multiclass classification, as demonstrated by studies Natarajan and Tadepalli [1988], Natarajan [1989], Ben-David et al. [1992], Haussler and Long [1995], Rubinstein et al. [2006], Daniely et al. [2011, 2012], Daniely and Shalev-Shwartz [2014], Brukhim et al. [2021]. Despite this extensive body of work, a combinatorial characterization of multiclass classification with an infinite number of classes under Valiant\u2019s PAC learning framework in the realizable setting remained open until recently. In pursuit, the seminal paper by Brukhim et al. [2022] provided a combinatorial characterization in the mentioned setting. A key innovation in this breakthrough was the utilization of list learners. This dimension also serves to characterize the agnostic variant of this problem David et al. [2016]. For standard multiclass online learning with potentially unbounded label space, Daniely et al. [2011] presented a characterization for the realizable setting. Building on this, Hanneke et al. [2023a] extended Ben-David et al. [2009] technique to the agnostic setting with infinite label space. Notably, a similar trend can also be observed in the online learning under bandit feedback in the work of Daniely et al. [2011] followed by Raman et al. [2023]. ", "page_idx": 12}, {"type": "text", "text": "B Proof of Proposition 1 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Let ${\\mathcal{C}}\\subseteq{\\mathcal{V}}^{X}$ be any concept class. To see that $\\operatorname{D}({\\mathcal{C}})\\leq\\operatorname{B}({\\mathcal{C}})$ , note that if $\\mathrm{D}({\\mathcal{C}})=d$ , there exists a Level-constrained Littlestone tree $\\tau$ of depth $d$ with branching factor $d$ . Thus, it must be the case that $\\operatorname{B}({\\mathcal{C}})\\geq d$ . ", "page_idx": 12}, {"type": "text", "text": "To prove that $\\operatorname{B}({\\mathcal{C}})\\leq\\operatorname{L}({\\mathcal{C}})$ , it suffices to show that for every shattered Level-constrained Branching tree $\\tau$ with branching factor $n\\in\\mathbb N$ , there exists a shattered Littlestone tree of depth $n$ . In particular, we will prove via induction the following claim: if $\\tau$ is a Level-constrained Branching tree with branching factor $n$ shattered by some ${\\mathcal{C}}^{\\prime}\\subseteq{\\mathcal{C}}$ , then there exists a Littlestone tree $\\mathcal{T}^{\\prime}$ of depth $n$ shattered by $\\mathcal{C}^{\\prime}$ . ", "page_idx": 12}, {"type": "text", "text": "For the base case let $\\tau$ be a Level-constrained Branching tree with branching factor 1 shattered by some ${\\mathcal{C}}^{\\prime}\\subseteq{\\mathcal{C}}$ . Without loss of generality (see Lemma 1), suppose that branching occurs on the root node of $\\tau$ . Then, it is clear that just the root node of $\\tau$ along with its two outgoing edges is a Littlestone tree of depth 1 shattered by $\\mathcal{C}^{\\prime}$ . ", "page_idx": 12}, {"type": "text", "text": "Now for the induction step, suppose the induction hypothesis is true for some $n\\leq\\mathrm{B}(\\mathcal{C})-1$ . Let $\\tau$ be a Level-constrained Branching tree with branching factor $n+1$ shattered by ${\\mathcal{C}}^{\\prime}\\subseteq{\\mathcal{C}}$ . Again, without loss of generality, suppose branching occurs on the root node of $\\tau$ . Let $\\mathcal{T}_{0}$ and $\\tau_{1}$ be the left and right subtrees of $\\tau$ respectively shattered by $\\mathcal{C}_{0}^{\\prime}\\subset\\mathcal{C}^{\\prime}$ and $\\mathcal{C}_{1}^{\\prime}\\subset\\mathcal{C}^{\\prime}$ respectively. Then, note that $\\mathcal{T}_{0}$ and $\\tau_{1}$ must both have a branching factor exactly $n$ . Then, by the induction hypothesis, there exist Littlestone trees $\\mathcal{T}_{0}^{\\prime}$ and $\\mathcal{T}_{1}^{\\prime}$ of depth $n$ shattered by $\\ensuremath{\\mathcal{C}^{\\prime}}_{0}$ and $\\mathcal{C}_{1}^{\\prime}$ respectively. Since branching occurs at the root node, the tree $\\mathcal{T}^{\\prime}$ obtained by keeping the root node and its two outgoing edges of $\\tau$ , but replacing $\\mathcal{T}_{0}$ and $\\mathcal{T}_{1}$ with $\\mathcal{T}_{0}^{\\prime}$ and $\\mathcal{T}_{1}^{\\prime}$ respectively, is a Littlestone tree of depth $n+1$ shattered by $\\mathcal{C}_{0}^{\\prime}\\cup\\mathcal{C}_{1}^{\\prime}\\subseteq\\mathcal{C}^{\\prime}$ . ", "page_idx": 13}, {"type": "text", "text": "C Proof of Lowerbound $\\frac{\\mathrm{D}(\\mathscr{C})}{2}$ ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Proof. Fix a transductive online learner $\\boldsymbol{\\mathcal{A}}$ . We will construct a randomized, hard realizable stream such that the expected number of mistakes made by $\\boldsymbol{\\mathcal{A}}$ is at least D(2C ). Using the probabilistic method then gives the stated lowerbound. ", "page_idx": 13}, {"type": "text", "text": "Let $d:=\\operatorname{D}(\\mathcal{C})$ . Then, by Definition 2, there exists a sequence of instances $x_{1},..,x_{d}\\in\\mathcal{X}^{d}$ and a sequence of functions $\\{Y_{t}\\}_{t=1}^{d}$ where $Y_{t}:\\{0,1\\}^{t}\\rightarrow\\mathcal{Y}$ such that for every $\\sigma\\in\\{0,1\\}^{d}$ , the following holds: ", "page_idx": 13}, {"type": "equation", "text": "$$\nY_{t}(\\sigma_{<t},0)\\neq Y_{t}(\\sigma_{<t},1)\\;{\\mathrm{for~all}}\\;t\\in[d].\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Let $\\sigma\\,\\sim\\,\\{0,1\\}^{d}$ a denote bitstring of length $d$ sampled uniformly at random and consider the stream $(x_{1},Y_{1}(\\sigma_{\\le1})),...,(x_{d},Y_{d}(\\sigma_{\\le d}))$ . By the definition, there exists a concept $c_{\\sigma}\\in{\\mathcal{C}}$ such that $c_{\\sigma}(x_{t})=Y_{t}(\\sigma_{\\leq t})$ for all $t\\in[d]$ . Moreover, observe that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{d}1\\{A(x_{t})\\neq Y_{t}(\\sigma_{\\leq t})\\}\\right]=\\displaystyle\\sum_{t=1}^{d}\\mathbb{E}\\Big[1\\{A(x_{t})\\neq Y_{t}(\\sigma_{\\leq t})\\}\\Big]}&{}\\\\ {=\\displaystyle\\sum_{t=1}^{d}\\mathbb{E}\\Big[\\mathbb{E}\\left[1\\left\\{A(x_{t})\\neq Y_{t}\\big((\\sigma_{<t},\\sigma_{t})\\big)\\right\\}\\;\\Big|\\;\\sigma_{<t}\\right]\\Big]}&{}\\\\ {\\ge\\displaystyle\\frac{1}{2}\\displaystyle\\sum_{t=1}^{d}\\mathbb{E}\\left[\\mathbb{E}\\Big[1\\left\\{Y_{t}\\big((\\sigma_{<t},0)\\big)\\neq Y_{t}\\big((\\sigma_{<t},1)\\big)\\right\\}\\;\\Big|\\;\\sigma_{<t}\\Big]\\right]}&{}\\\\ {=\\displaystyle\\frac{1}{2}\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{d}1\\left\\{Y_{t}\\big((\\sigma_{<t},0)\\big)\\neq Y_{t}\\big((\\sigma_{<t},1)\\big)\\right\\}\\right]=\\displaystyle\\frac{d}{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where the first inequality follows from the fact that $\\sigma_{t}\\,\\sim\\,\\mathrm{Uniform}(\\{0,1\\})$ . This completes the proof. \u53e3 ", "page_idx": 13}, {"type": "text", "text": "D Proof of Lemma 1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Proof. (of Lemma 1) If $\\mathrm{depth}({\\mathcal T})\\leq2^{q}-1$ , the claim holds trivially. So, we assume depth $(\\mathcal T)>$ $2^{q}-1$ . We now proceed by induction on $q$ . For the base case $q=1$ , let $\\tau$ denote a level-constrained tree of depth $(\\tau)>1$ shattered by $\\mathcal{C}$ that satisfies property (a) and (b) specified in Lemma 1. First, consider the case where the root node of $\\tau$ has branching. Since every path in $\\tau$ can have exactly 1 branching, there can be no further branching in $\\tau$ . Next, consider the case when the root node of $\\tau$ is not the branching node and $\\ell$ is the first level in $\\tau$ with branching. There must be $2^{\\ell-1}$ nodes in this level, henceforth denoted by $\\{v_{i}\\}_{i=1}^{2^{\\ell-1}}$ . Moreover, denote ${\\mathcal T}_{v_{i}}$ to be the corresponding subtree in $\\tau$ with $v_{i}$ as the root node. Since $\\tau$ satisfy property (b) and there are no branching nodes before lreovoet l $\\ell$ , dteh,e  tshuerbetr ceaesn $\\{\\mathcal{T}_{v_{i}}\\}_{i=1}^{2^{\\ell-1}}$ hmeru sbtr abnec ihdienngt iicna tl.h eSsien cseu batlrl eseus bbtreeyeos $\\{\\mathcal{T}_{v_{i}}\\}_{i=1}^{2^{\\ell-1}}$ nhoadvee.  bTrahnecrehfionrge ,o tnh tehree cannot be any other levels $\\ell^{\\prime}>\\ell$ in $\\tau$ with branching node. This establishes that $\\ell$ is the only level in $\\tau$ with at least one branching node. In either case, we have ${\\mathrm{BN}}(T)\\leq1=2^{q}-1$ . ", "page_idx": 13}, {"type": "text", "text": "Assume that Lemma 1 is true for some $q\\,=\\,n\\,\\in\\,\\mathbb{N}$ . We now establish Lemma 1 for $q=n+1$ . To that end, let $\\tau$ be a level-constrained tree with branching factor $q=n+1$ shattered by $\\mathcal{C}$ that satisfies (a) and (b). Let $\\ell\\geq1$ be the first level in $\\tau$ with at least one branching node, and $\\{\\bar{\\mathcal{T}}_{i}\\}_{i=1}^{2^{\\ell}-1}$ be all the subtrees with its root node being a node on level $\\ell$ . As argued in the base case, all these subtrees must be identical. Thus, branching occurs on the same set of levels on all these subtrees, which implies that $\\mathrm{BN}(\\mathcal{T})=\\mathrm{BN}(\\mathcal{T}_{i})$ for all $i\\in[2^{\\ell-1}]$ . Let $\\mathcal{T}_{1}^{0}$ and $\\mathcal{T}_{1}^{1}$ denote the left and right subtree following the two outgoing edges from the root node of $\\tau_{1}$ . Since, there is branching on the root-node of $\\mathcal{T}_{1}$ , we must have $\\mathrm{BN}(\\bar{T_{1}}\\bar{)}\\leq1+\\mathrm{BN}(\\bar{T_{1}^{0}})+\\mathrm{BN}(\\bar{T_{1}^{1}})$ . For each $i\\in\\{0,1\\}$ , the subtree $\\tau_{1}^{i}$ is a level-constrained tree shattered by $\\mathcal{C}$ that satisfies properties (a) and (b) for $q=n$ . Using the inductive concept, we have $\\mathrm{BN}({\\mathcal T}_{1}^{i})\\leq2^{n}-1$ for $i\\in\\{0,1\\}$ . Therefore, combining everything ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathrm{BN}({\\mathcal T})=\\mathrm{BN}({\\mathcal T}_{1})\\leq1+\\mathrm{BN}({\\mathcal T}_{1}^{0})+\\mathrm{BN}({\\mathcal T}_{1}^{1})\\leq1+2^{n}-1+2^{n}-1=2^{n+1}-1.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "This completes our induction step. ", "page_idx": 14}, {"type": "text", "text": "E Minimax Rates in the Agnostic Setting ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We go beyond the realizable setting, and establish the minimax regret in the agnostic setting in terms of the Level-constrained Littlestone dimension. ", "page_idx": 14}, {"type": "text", "text": "Theorem 4 (Regret bound). For every concept class ${\\mathcal{C}}\\subseteq{\\mathcal{V}}^{{\\mathcal{X}}}$ and $T\\geq\\mathrm{D}({\\mathcal{C}}).$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\sqrt{\\frac{T\\mathrm{~D}(\\mathcal{C})}{8}}\\leq\\mathrm{R}^{\\star}(T,\\mathcal{C})\\,\\leq\\sqrt{T\\mathrm{~D}(\\mathcal{C})}\\,\\log\\Bigl(\\frac{e T}{\\mathrm{D}(\\mathcal{C})}\\Bigr).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We note that the upper- and lower-bounds in Theorem 4 are only off only by a factor logarithmic in $T$ . We leave it as an open question to establish a matching upper- and lower-bounds. ", "page_idx": 14}, {"type": "text", "text": "Proof. (of upper bound in Theorem 4) To prove the upper bound, we will use the agnostic-torealizable reduction from Hanneke et al. [2023a] to convert our realizable learner in Section 3.2 to an agnostic learner with the claimed upper bound on expected regret. By Theorem 4 in [Hanneke et al., 2023a], any conservative deterministic learner $\\boldsymbol{\\mathcal{A}}$ with mistake bound $M$ can be converted into an agnostic learner with expected regret at most $\\scriptstyle{\\sqrt{T\\,M\\,\\log\\left({\\frac{e T}{M}}\\right)}}$ . Although the proof by Hanneke et al. [2023a] only coverts the conservative Standard Optimal Algorithm to an agnostic learner, the arguments are general enough such that the conversion can be adapted for any conservative deterministic mistake-bound learner. By Theorem 3 and the proof in Section 3.2, there exists a conservative deterministic realizable learner with mistake bound at most D(C) log De(TC) . Using the realizable-to-agnostic conversion from Theorem 4 in [Hanneke et al., 2023a] with the conservativeversion of the realizable learner in Section 3.2 gives an agnostic learner with expected regret at most ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\sqrt{T\\;\\mathrm{D}(\\mathcal{C})\\,\\log\\Big(\\frac{e T}{\\mathrm{D}(\\mathcal{C})}\\Big)\\log\\left(\\frac{e T}{\\mathrm{D}(\\mathcal{C})\\log\\Big(\\frac{e T}{\\mathrm{D}(\\mathcal{C})}\\Big)}\\right)}\\leq\\sqrt{T\\;\\mathrm{D}(\\mathcal{C})}\\,\\log\\Big(\\frac{e T}{\\mathrm{D}(\\mathcal{C})}\\Big),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "completing the proof. ", "page_idx": 14}, {"type": "text", "text": "Proof. (of lower bound in Theorem 4) Our proof of lower bound is identical to the lower bound for the binary setting proved in [Hanneke et al., 2023b, Theorem 6.1], which is just a simple adaptation of standard lower bound technique from [Ben-David et al., 2009]. Thus, we only outline the sketch of the proof here. ", "page_idx": 14}, {"type": "text", "text": "Let $d=\\operatorname{D}({\\mathcal{C}})$ . Consider a sequence of instances $\\{x_{1}^{\\star},\\ldots,x_{d}^{\\star}\\}\\subset\\mathcal{X}$ and a sequence of functions $\\{Y_{i}\\}_{i=1}^{d}$ that is shattered by $\\mathcal{C}$ according to Definition 2. Pick the largest odd number $k\\in\\mathbb N$ such that $k d\\leq T$ . First, the adversary reveals the instances $\\{x_{1},\\dots,x_{T}\\}$ such that $x_{t}=x_{1}^{\\star}$ for $t=1,\\ldots,k$ , followed by $x_{t}=x_{2}^{\\star}$ for $t=k+1,\\ldots,2k$ , and so forth. If $T>k d$ , take $x_{t}=x_{d}^{\\star}$ for all $t>k d$ . As for labels, the adversary will first sample $(\\sigma_{1},\\sigma_{2},\\ldots,\\sigma_{T})\\ \\in\\ \\mathsf{U n i f o r m}(\\{0,1\\}^{T})$ . Then, for $t=1,\\ldots,k$ , the labels are selected as $y_{t}=Y_{1}(\\sigma_{t})$ . For $t=k+1,\\ldots,2k$ , the labels are selected as $y_{t}=Y_{2}\\big((\\bar{\\sigma}_{1},\\sigma_{t})\\big)$ , where $\\begin{array}{r}{\\bar{\\sigma}_{1}=\\mathbb{1}\\Big\\{\\sum_{t=1}^{k}\\mathbb{1}\\big[\\sigma_{1}=0\\big]<\\sum_{t=1}^{k}\\mathbb{1}\\big[\\sigma_{1}=1\\big]\\Big\\}}\\end{array}$ is the majority bit in the first block $t=1,\\ldots,k$ . One can define $y_{t}$ for all $t>2k$ analogously. For this stream, the label $y_{t}$ is essentially equivalent to the bit $\\sigma_{t}\\in\\{0,1\\}$ . Therefore, following the exact same arguments as in [Hanneke et al., 2023b, Theorem 6.1] establishes the lower bound of $\\sqrt{T d/8}$ . This completes the sketch of our proof. \u53e3 ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Remark 1. Let $k\\in\\mathbb N$ be the number of classes. Let $\\mathcal{C}\\subseteq\\{1,2,\\ldots,k\\}^{\\mathcal{X}}$ be a concept class. It is notable that for small number of classes $k$ (i.e. $k<<2^{(l o g T)^{2}},$ ), the Natarajan bound that can be proved using the technique of Hanneke et al. $[2023b]$ can be smaller than the upper bound in terms of the Level-constrained Littlestone dimension. However, for large $k$ (i.e. $k>>\\bar{2}^{(l o g T)^{2}}.$ ), our upper bound in terms of $\\mathrm{D}(C)$ can be better. ", "page_idx": 15}, {"type": "text", "text": "F Comparisons to Existing Combinatorial Dimensions ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we compare the Level-constrained Littlestone dimension 2 and the Level-constrained Branching dimension 3 to existing combinatorial dimensions in multiclass learning. ", "page_idx": 15}, {"type": "text", "text": "F.1 Existing Combinatorial Dimensions ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Definition 4 $\\overrightarrow{\\it1}$ -neighbour). Let $f,g\\in\\mathcal{D}^{d}$ for some $d\\in\\mathbb{N}.$ . For every $i\\in[d]$ , we say that $f$ and $g$ are $i$ -neighbours if $f_{i}\\neq g_{i}$ and $\\forall_{j\\in[d]\\backslash\\{i\\}}\\ f_{j}=g_{j}$ . ", "page_idx": 15}, {"type": "text", "text": "Definition 5 (DS dimension Daniely and Shalev-Shwartz [2014]). Let ${\\mathcal{C}}\\subseteq{\\mathcal{V}}^{{\\mathcal{X}}}$ be a concept class. Let $S\\,\\in\\,\\chi^{d}$ be a sequence for some $d\\in\\mathbb N$ . We say that $S$ is DS-shattered by $\\mathcal{C}$ , if there exists $F\\subseteq{\\mathcal{C}},|F|<\\infty$ such that for all $f\\,\\in\\,\\{g\\mid g\\in\\mathcal{Y}^{d}$ , $\\exists_{g\\in F}\\ \\forall_{i\\in[d]}\\ g_{i}=f(S_{i})\\}$ and for all $i\\in[d]$ , $f$ has at least one $i$ -neighbor. The DS dimension of $\\mathcal{C}$ , denoted ${\\mathrm{DS}}(C)$ , is the maximal size of $a$ sequence $S\\in\\mathcal{X}^{d}$ for some $d\\in\\mathbb{N}$ that is DS-shattered by $\\mathcal{C}$ . ", "page_idx": 15}, {"type": "text", "text": "Definition 6 (Graph dimension). Let $\\mathcal{C}\\subseteq\\mathcal{V}^{\\mathcal{X}}$ be a concept class. Let $S\\subseteq\\mathcal{X}$ . We say that $S$ is G-shattered by $\\mathcal{C}$ , if there exists an $f:S\\to\\mathcal{Y}$ such that for every $T\\subseteq S$ there is a $g\\in{\\mathcal{C}}$ such that: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\forall_{x\\in T}\\;g(x)=f(x)\\;a n d\\;\\forall_{x\\in S-T}\\;g(x)\\neq f(x)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The graph dimension of $\\mathcal{C}$ , denoted $\\operatorname{G}({\\mathcal{C}})$ , is the maximal cardinality of a set $S\\subseteq\\mathcal{X}$ that is G-shattered by $\\mathcal{C}$ . ", "page_idx": 15}, {"type": "text", "text": "Definition 7 (Natarajan Threshold dimension). Let ${\\mathcal{C}}\\subseteq{\\mathcal{V}}^{{\\mathcal{X}}}$ be a concept class. Let $S\\in\\chi^{d}$ be $a$ sequence for some $d\\in\\mathbb{N}$ . We say that $S$ is NT-shattered by $\\mathcal{C}$ , if there exist $f,g:[d]\\rightarrow\\mathcal{Y}$ such that $\\forall_{i\\in[d]}\\ f(i)\\neq g(i)$ , and there exists $\\left(c_{0},c_{1},c_{2},\\ldots,c_{d}\\right)\\in\\mathcal{C}^{d+1}$ such that for every $i\\in[d\\!+\\!1],j\\in[d]$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\nc_{i-1}(S_{j})={\\binom{f(j),\\quad j<i}{g(j),\\quad j\\geq i}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The Natarajan Threshold dimension of $\\mathcal{C}$ , denoted ${\\mathrm{NT}}({\\mathcal{C}})$ , is the maximal size of a sequence $S\\in\\chi^{d}$ for some $d\\in\\mathbb{N}$ that is NT-shattered by $\\mathcal{C}$ . ", "page_idx": 15}, {"type": "text", "text": "F.2 Comparison ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "It is easy to show that for every concept class ${\\mathcal{C}}\\subseteq{\\mathcal{V}}^{{\\mathcal{X}}}$ , its Natarajan dimension is always less than or equal to its DS dimension. Moreover, the work of Brukhim et al. [2022] demonstrated there exists a concept class ${\\mathcal{C}}\\subseteq{\\mathcal{V}}^{{\\mathcal{X}}}$ for which the Natarajan dimension is 1 but $\\mathrm{DS}({\\mathcal{C}})=\\infty$ . Here, we show that for every concept class ${\\mathcal{C}}\\subseteq{\\mathcal{V}}^{{\\mathcal{X}}}$ , its DS dimension is always less than equal to its Level-constrained Littlestone dimension. Furthermore, we demonstrate there exists a concept class ${\\mathcal{C}}^{\\prime}\\subseteq{\\mathcal{V}}^{{\\mathcal{X}}}$ such that $\\mathrm{DS}({\\mathcal{C}}^{\\prime})=1$ but $\\operatorname{D}({\\mathcal{C}}^{\\prime})=\\infty$ . These two results are shown in Proposition 2. ", "page_idx": 15}, {"type": "text", "text": "Proposition 2. For every concept class ${\\mathcal{C}}\\subseteq{\\mathcal{V}}^{{\\mathcal{X}}}$ , we have: $\\operatorname{DS}({\\mathcal{C}})\\leq\\operatorname{D}({\\mathcal{C}})$ . Moreover, there exists $a$ concept class $\\mathcal{C}^{\\prime}\\subseteq\\mathcal{V}^{\\mathcal{X}}$ such that $\\mathrm{DS}({\\mathcal{C}}^{\\prime})=1$ but $\\operatorname{D}({\\mathcal{C}}^{\\prime})=\\infty$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. First, we prove that for every concept class ${\\mathcal{C}}\\subseteq{\\mathcal{V}}^{{\\mathcal{X}}}$ , we have: $\\operatorname{DS}({\\mathcal{C}})\\leq\\operatorname{D}({\\mathcal{C}})$ . Let ${\\mathcal{C}}\\subseteq{\\mathcal{V}}^{{\\mathcal{X}}}$ be a concept class such that ${\\mathrm{DS}}(C)$ is finite. Subsequently, we show that we can construct a Levelconstrained Littlestone tree $\\tau$ of depth ${\\mathrm{DS}}(C)$ , which is shattered by $\\mathcal{C}$ . Thus, $\\operatorname{DS}({\\mathcal{C}})\\leq\\operatorname{D}({\\mathcal{C}})$ . ", "page_idx": 15}, {"type": "text", "text": "Let $S\\,\\in\\,\\chi^{\\mathrm{DS}({\\mathcal C})}$ be a sequence of instances, which is $D S$ -shattered by $\\mathcal{C}$ . We show that we can construct a Level-constrained Littlestone tree $\\tau$ of depth ${\\mathrm{DS}}(C)$ , having members of $S$ as its nodes in order with the first member being its root and so on, which is shattered by $\\mathcal{C}$ . To show the construction, we use induction. If $\\mathrm{DS}({\\mathcal{C}})=1$ , it is clear that we can construct a Level-constrained Littlestone tree $\\tau$ of depth 1, which is shattered by $\\mathcal{C}$ . This is because there must be two concepts in $\\mathcal{C}$ , which disagree on one member of $S$ . We assume that if $\\mathrm{DS}({\\mathcal{C}})=d$ , we can construct a Level-constrained Littlestone tree $\\tau$ of depth $d$ , having members of $S$ as its nodes in order with the first member being its root and so on, which is shattered by $\\mathcal{C}$ , where $S\\in\\chi^{d}$ is a sequence of size $d$ witnessing $\\mathrm{DS}({\\bar{C}})=d$ . Now, we prove that if $\\mathrm{DS}({\\mathcal{C}})=\\dot{d}+1$ , we can construct a Level-constrained Littlestone tree $\\tau$ of depth $d+1$ , having members of $S$ as its nodes in order with the first member being its root and so on, which is shattered by $\\mathcal{C}$ , where $S\\in\\mathcal{X}^{d+1}$ is a sequence of size $d+1$ witnessing $\\mathrm{DS}({\\mathcal{C}})=d+1$ . Let $F\\subset{\\mathcal{C}}$ be a set witnessing $\\mathrm{DS}({\\mathcal{C}})=d+1$ . Take any two distinct concepts $c_{1},c_{2}\\in F$ . Define $F^{\\prime}$ as follows: $F^{\\prime}:=\\{f\\mid f\\in F,f(S_{1})=c_{1}(S_{1})\\}$ . In addition, define $F^{\\prime\\prime}$ as follows: $F^{\\prime\\prime}:=\\{f\\mid f\\in F,f(S_{1})=c_{2}(S_{1})\\}$ . Observe that $\\left(S_{2},S_{3},\\ldots,S_{d+1}\\right)$ and $F^{\\prime}$ can witness $\\mathrm{DS}({\\mathcal{C}})\\geq d$ . Similarly, observe that $\\left(S_{2},S_{3},\\ldots,S_{d+1}\\right)$ and $F^{\\prime\\prime}$ can witness $\\mathrm{DS}({\\mathcal{C}})\\geq d$ . Now, we set the root of $\\tau$ as $S_{1}$ and branches with $c_{1}(S_{1})$ and $c_{2}(S_{1})$ labels. Based on the inductive assumption combined with the facts that we mentioned, we can complete the construction of Level-constrained Littlestone tree $\\tau$ of depth ${\\mathrm{DS}}(C)$ , having members of $S$ as its nodes in order with the first member being its root and so on, which is shattered by $\\mathcal{C}$ . ", "page_idx": 16}, {"type": "text", "text": "Finally, we note that if $\\mathrm{DS}({\\mathcal{C}})=\\infty$ , as we can do the construction for every depth $d\\in\\mathbb{N}$ , we should have $\\dot{\\mathrm{D}}(\\mathcal{C})=\\infty$ . ", "page_idx": 16}, {"type": "text", "text": "Second, we prove that there exists a concept class $\\mathcal{C}^{\\prime}\\subseteq\\mathcal{V}^{\\mathcal{X}}$ such that $\\mathrm{DS}({\\mathcal{C}}^{\\prime})=1$ but $\\operatorname{D}({\\mathcal{C}}^{\\prime})=\\infty$ . To show this, we use our next proposition, namely 3, combined with the well-known fact that for every ${\\mathcal{C}}\\subseteq{\\mathcal{V}}^{{\\mathcal{X}}}$ , we have: $\\mathrm{DS}({\\mathcal{C}})\\leq\\mathrm{G}({\\mathcal{C}})$ . \u53e3 ", "page_idx": 16}, {"type": "text", "text": "Next, we show there that exists a concept class ${\\mathcal{C}}\\subseteq{\\mathcal{V}}^{{\\mathcal{X}}}$ such that $\\mathrm{G}(\\mathcal{C})=1$ and $\\operatorname{D}({\\mathcal{C}})=\\infty$ . On the other hand, we also prove the existence of a concept class ${\\mathcal{C}}\\subseteq{\\mathcal{V}}^{{\\mathcal{X}}}$ such that $\\mathrm{G}({\\mathcal{C}})=\\infty$ and $\\mathrm{D}({\\mathcal{C}})=1$ . These two results, shown in Proposition 3, imply that the Level-constrained Littlestone dimension and the Graph dimension are not comparable. Moreover, our first claim has an interesting consequence. In particular, it illustrates that having a finite Level-constrained Littlestone dimension is not necessary for having a bounded size sample compression scheme. This follows from the fact that having finite Graph dimension is sufficient for having a bounded size sample compression scheme [David et al., 2016]. We also remark that for every concept class ${\\mathcal{C}}\\subseteq{\\mathcal{V}}^{{\\mathcal{X}}}$ , its DS dimension is always less than or equal to its Graph dimension. ", "page_idx": 16}, {"type": "text", "text": "Proposition 3. There exists a concept class ${\\mathcal{C}}\\subseteq{\\mathcal{V}}^{{\\mathcal{X}}}$ such that $\\mathrm{G}(\\mathcal{C})=1$ and $\\operatorname{D}({\\mathcal{C}})=\\infty$ . Moreover, there exists a concept class $\\mathcal{C}^{\\prime}\\subseteq\\mathcal{D}^{\\hat{\\mathcal{X}}}$ such that $\\mathrm{G}({\\mathcal{C}}^{\\prime})=\\infty$ and $\\mathrm{D}({\\mathcal{C}}^{\\prime})=1$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. First, we prove the second claim. To show that, we rely on Example 1 in Hanneke et al. [2023a]. In particular, they showed there exists a concept class $\\dot{\\mathcal{C}}^{\\prime}\\subseteq\\mathcal{V}^{\\mathcal{X}}$ such that $\\mathrm{G}({\\mathcal{C}}^{\\prime})=\\infty$ and $\\operatorname{L}({\\mathcal{C}}^{\\prime})=1$ . As we know $\\dot{\\mathrm{D}(\\mathcal{C}^{\\prime})}\\le\\mathrm{L}(\\mathcal{C})$ , we conclude there exists a concept class ${\\mathcal{C}}^{\\prime}\\subseteq{\\mathcal{V}}^{{\\mathcal{X}}}$ such that $\\mathrm{G}({\\mathcal{C}}^{\\prime})=\\infty$ and $\\mathrm{D}(\\mathcal{C}^{\\prime})=1$ . ", "page_idx": 16}, {"type": "text", "text": "Second, we prove that there exists a concept class $\\mathcal{C}\\subseteq\\mathcal{V}^{\\mathcal{X}}$ such that $\\operatorname{G}(\\mathcal{C})\\,=\\,1$ and $\\operatorname{D}({\\mathcal{C}})=\\infty$ . Let $\\tau$ be an infinite depth rooted perfect binary tree so that all of its levels and edges are labeled by distinct elements. The definition of such a tree is similar to Definition 1.7 in the work of Bousquet et al. [2021]. Let $\\mathcal{X}$ be the elements on the levels of $\\tau$ and $\\boldsymbol{\\wp}$ be the elements on the edges of $\\tau$ . Also, define the concept class ${\\mathcal{C}}\\subseteq{\\mathcal{V}}^{{\\mathcal{X}}}$ as follows: $\\mathcal{C}$ only contains all concepts consistent with a branch of $\\tau$ . Thus, clearly, we have: $\\operatorname{D}({\\mathcal{C}})=\\infty$ . Now, we show that $\\mathrm{G}(\\mathcal{C})=1$ . We prove this by contradiction. Assume $\\operatorname{G}({\\mathcal{C}})\\geq2$ . Thus, there exist $S=(x_{1},x_{2})\\subset\\mathcal{X}$ of size 2 and $\\bar{f}:S\\to\\mathcal{Y}$ witnessing the fact that $\\mathrm{G}({\\mathcal{C}})\\,=\\,2$ . Without loss of generality, we assume that $x_{1}$ is above $x_{2}$ in $\\tau$ . Using the fact that the edges of $\\tau$ are labeled with distinct elements of $\\boldsymbol{\\wp}$ , there cannot exist both $c_{1}\\in{\\mathcal{C}}$ and $c_{2}\\in{\\mathcal{C}}$ such that $c_{1}(x_{1})=f(x_{1})$ , $c_{1}(x_{2})=f(x_{2})$ , $c_{2}(x_{1})\\neq f(x_{1})$ , and $c_{2}(x_{2})=f(x_{2})$ . This is a contradiction, thus $\\mathrm{G}(\\mathcal{C})=1$ . \u53e3 ", "page_idx": 16}, {"type": "text", "text": "It is well-known that for every concept class ${\\mathcal{C}}\\subseteq{\\mathcal{V}}^{{\\mathcal{X}}}$ , its Littlestone dimension is always less than equal to its sequential graph dimension. Moreover, the work of Hanneke et al. [2023a] demonstrated there exists a concept class ${\\mathcal{C}}\\subseteq{\\mathcal{V}}^{{\\mathcal{X}}}$ such that $\\operatorname{L}({\\mathcal{C}})=1$ and $\\operatorname{SG}({\\mathcal{C}})=\\infty$ . Here, we show that for every concept class ${\\mathcal{C}}\\subseteq{\\mathcal{V}}^{{\\mathcal{X}}}$ , its Level-constrained Branching dimension is always less than equal to its Littlestone dimension. Furthermore, we demonstrate that there exists a concept class $\\mathcal{C}^{\\prime}\\subseteq\\mathcal{V}^{\\mathcal{X}}$ such that $\\mathrm{B}({\\mathcal{C}}^{\\prime})\\leq2$ and $\\operatorname{L}({\\mathcal{C}}^{\\prime})=\\infty$ . These two results are shown in Proposition 4. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "Proposition 4. For every concept class $\\mathcal{C}\\subseteq\\mathcal{V}^{\\mathcal{X}}$ , we have: $\\operatorname{B}({\\mathcal{C}})\\leq\\operatorname{L}({\\mathcal{C}})$ . Moreover, there exists a concept class $\\mathcal{C}^{\\prime}\\subseteq\\mathcal{V}^{\\mathcal{X}}$ such that $\\mathrm{B}({\\mathcal{C}}^{\\prime})\\leq2$ and $\\operatorname{L}({\\mathcal{C}}^{\\prime})=\\infty$ . ", "page_idx": 17}, {"type": "text", "text": "Proof. The proof of the following claim: for every concept class ${\\mathcal{C}}\\subseteq{\\mathcal{V}}^{{\\mathcal{X}}}$ , we have that $\\operatorname{B}({\\mathcal{C}})\\leq\\operatorname{L}({\\mathcal{C}})$ is given by Proposition 1. Therefore, we focus on showing that there exists a concept class ${\\mathcal{C}}^{\\prime}\\subseteq{\\mathcal{V}}^{{\\mathcal{X}}}$ such that $\\mathrm{\\bar{B}}({\\mathcal C}^{\\prime})\\leq2$ and $\\operatorname{L}({\\mathcal{C}}^{\\prime})=\\infty$ . Let $\\tau$ be an infinite depth rooted perfect binary tree so that all of its nodes are labeled by distinct elements, all of its left edges are labeled by 0, and all of its right edges are labeled by 1. The definition of such a tree is similar to Definition 1.7 in the work of Bousquet et al. [2021]. Let $\\mathcal{X}$ be the elements on the nodes of $\\tau$ . Also, define the concept class $\\mathcal{C}^{\\prime}$ as follows: $\\mathcal{C}^{\\prime}$ contains only the concepts consistent with a branch of $\\tau$ . Further, each of these concepts predicts a unique label for all instances outside its associated branch. In addition, define $\\boldsymbol{\\wp}$ as the union of $\\{0,1\\}$ and all unique labels used in the definition of $\\mathcal{C}^{\\prime}$ . Thus, we have: $\\operatorname{L}({\\mathcal{C}}^{\\prime})=\\infty$ . Now, we show that $\\mathrm{B}({\\mathcal{C}}^{\\prime})\\,\\leq\\,2$ . To prove this, we demonstrate that for every $T\\,\\in\\,\\mathbb{N}$ , we have: infDeterministic $\\boldsymbol{A}$ $\\mathrm{M}_{\\mathcal{A}}(T,\\mathcal{C}^{\\prime})\\leq2$ . As a result, we can then conclude that $\\mathrm{B}({\\mathcal{C}}^{\\prime})\\leq2$ . ", "page_idx": 17}, {"type": "text", "text": "To see why infDeterministic $\\mathcal{A}$ $\\mathrm{M}_{\\mathcal{A}}(T,\\mathcal{C}^{\\prime})\\leq2$ for every $T\\in\\mathbb N$ implies that $\\mathrm{B}({\\mathcal{C}}^{\\prime})\\leq2$ , suppose for the sake of contradiction that $\\mathrm{B}(\\mathcal{C}^{\\prime})\\geq3$ . So, there exists a Level-constrained Branching tree of depth $d\\in\\mathbb{N}$ such that its branching factor is at least 3. Let $T^{\\prime}=d$ . It is not hard to see that there exists a sequence of instances of size $T^{\\prime}$ such that for every deterministic learner, there exists a realizable labeling of instances that forces the learner to make at least 3 mistakes over $T^{\\prime}$ rounds. This leads to a contradiction. Thus, we conclude that $\\mathrm{B}({\\mathcal{C}}^{\\prime})\\leq2$ . ", "page_idx": 17}, {"type": "text", "text": "We now construct a deterministic learner $\\boldsymbol{\\mathcal{A}}$ such that $\\mathrm{M}_{\\mathcal{A}}(T,\\mathcal{C}^{\\prime})\\leq2$ for every $T\\in\\mathbb N$ . Let $T\\in\\mathbb N$ . Let $S\\chi^{T}$ be the sequence chosen by the adversary at the beginning of the game. Also, let $c^{\\star}\\in\\mathcal{C}^{\\prime}$ be the target concept chosen by the adversary. Further, let $u$ be the root-to-leaf path in $\\tau$ associated with the concept $c^{\\star}$ . In addition, for every $i\\in[T]$ , let $v_{i}$ be a root-to-leaf path in $\\tau$ containing first $i$ members of $S$ , if it exists. Finally, let $i^{\\star}$ be the smallest positive integer such that $v_{i^{\\star}}$ does not exist. If $i^{\\star}$ itself does not exist, let $i^{\\star}=T+1$ . ", "page_idx": 17}, {"type": "text", "text": "Our algorithm $\\boldsymbol{\\mathcal{A}}$ predicts according to the $\\{0,1\\}$ labels associated with the path $v_{i}{\\star}-1$ for the first $i^{\\star}-1$ points in $S$ . Furthermore, if the adversary ever reveals a unique label, we use its corresponding $c\\in{\\mathcal{C}}^{\\prime}$ to make predictions in all future rounds. For the $i^{\\star}$ \u2019th member of $S$ , if it exists, we predict arbitrarily. To see that this algorithm makes at most 2 mistakes, we consider two cases. (1) If $i^{\\star}=T+1$ , then our algorithm makes at most one mistake. In fact, our algorithm makes a mistake: (a) if the adversary switches the label from a bit in $\\{0,1\\}$ to a unique label corresponding to the target concept $c^{\\star}$ . (b) perhaps on the last instance. (2) Otherwise, the algorithm makes at most two mistakes; the first mistake can be on round $i^{\\star}-1$ , and the second mistake can be on round $i^{\\star}$ , after which the true $c^{\\star}$ is known to the learner from its unique label. Indeed, if the adversary switches the label from a bit in $\\{0,1\\}$ to a unique label corresponding to the target concept $c^{\\star}$ before round $i^{\\star}-1$ , we only make one mistake. This completes the proof. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "Finally, the works of Shelah [1990], Hodges [1997] showed that the finiteness of the Littlestone and Threshold dimensions coincide in the binary setting. Here, we show that this is not the case between the Level-constrained Branching dimension and the Natarajan Threshold dimension. More specifically, we show that for every concept class ${\\mathcal{C}}\\subseteq{\\mathcal{V}}^{{\\mathcal{X}}}$ , its Level-constrained Branching dimension is always greater than or equal to the log of its Natarajan Threshold dimension. However, we give a concept class $\\mathcal{C}^{\\prime}\\subseteq\\mathcal{V}^{\\mathcal{X}}$ such that $\\mathrm{NT}(\\Bar{C}^{\\prime})=1$ and $\\mathrm{\\bar{B}}({\\mathcal{C}}^{\\prime})=\\infty$ . These two results are shown in Proposition 5. Notably, the lower bound of Hanneke et al. [2023b], based on the threshold dimension, can be easily generalized to our setting for the Natarajan Threshold dimension. ", "page_idx": 17}, {"type": "text", "text": "Proposition 5. For every concept class ${\\mathcal{C}}\\subseteq{\\mathcal{V}}^{{\\mathcal{X}}}$ , we have: $\\log(\\mathrm{NT}({\\mathcal{C}}))\\leq\\mathrm{B}({\\mathcal{C}})$ . Moreover, there exists a concept class ${\\mathcal{C}}^{\\prime}\\subseteq{\\mathcal{V}}^{{\\mathcal{X}}}$ such that $\\mathrm{NT}(\\mathcal{C}^{\\prime})=1$ and $\\mathrm{B}({\\mathcal{C}}^{\\prime})=\\infty$ . ", "page_idx": 17}, {"type": "text", "text": "Proof. First, we prove that for every concept class $\\mathcal{C}\\subseteq\\mathcal{V}^{\\mathcal{X}}$ , we have: $\\log(\\mathrm{NT}({\\mathcal{C}}))\\leq\\mathrm{B}({\\mathcal{C}})$ . Let $\\mathcal{C}\\subseteq\\mathcal{V}^{\\mathcal{X}}$ be a concept class such that $\\mathrm{NT}({\\mathcal{C}})=d$ for some $d\\in\\mathbb{N}$ . Let $T=d$ . On the one hand, by presenting the sequences of instances that are NT-shattered by $\\mathcal{C}$ to the learner, we can use a similar technique as [Hanneke et al., 2023b, Claim 3.4], to prove a lower bound of $\\log(\\mathrm{NT}(\\mathcal{C}))$ on $\\mathrm{M}^{\\star}(T,{\\mathcal{C}})$ . ", "page_idx": 17}, {"type": "text", "text": "On the other hand, based on Section 3, we can prove an upper bound of $\\operatorname{B}({\\mathcal{C}})$ on $\\mathrm{M}^{\\star}(T,{\\mathcal{C}})$ . Thus, we have $\\log(\\mathrm{NT}({\\mathcal{C}}))\\leq\\mathrm{B}({\\mathcal{C}})$ . ", "page_idx": 18}, {"type": "text", "text": "Second, we prove that there exists a concept class ${\\mathcal{C}}^{\\prime}\\subseteq{\\mathcal{V}}^{{\\mathcal{X}}}$ such that $\\mathrm{NT}(\\mathcal{C}^{\\prime})=1$ and $\\mathrm{B}({\\mathcal{C}}^{\\prime})=\\infty$ . Let $\\tau$ be a rooted binary tree so that it has the following three properties: (1) all of its levels and edges are labeled by distinct elements. (2) each level only contains one node with two children (3) its branching factor is infinite. It is not hard to see that such a tree exists. The definition of such a tree is similar to Definition 1.7 in the work of Bousquet et al. [2021]. Let $\\mathcal{X}$ be the elements on the levels of $\\tau$ and $\\boldsymbol{\\wp}$ be the elements on the edges of $\\tau$ . Also, define the concept class $\\mathcal{C}^{\\prime}\\subseteq\\mathcal{V}^{\\mathcal{X}}$ as follows: $\\mathcal{C}^{\\prime}$ only contains all concepts consistent with a branch of $\\tau$ . Thus, clearly, we have: $\\mathrm{B}({\\mathcal{C}}^{\\prime})=\\infty$ . Now, we show that $\\mathrm{NT}(\\mathcal{C}^{\\prime})=1$ . We prove this by contradiction. Assume $\\mathrm{NT}({\\mathcal{C}}^{\\prime})\\geq2$ . Then, there exist $S=(x_{1},x_{2})\\in\\mathcal{X}^{2}$ and $(c_{0},c_{1},c_{2})\\in\\mathcal{C}^{\\prime3}$ witnessing $\\mathrm{NT}({\\mathcal{C}}^{\\prime})=2$ . Without loss of generality, we assume that $x_{1}$ is above $x_{2}$ in $\\tau$ . Based on our constriction of $\\tau$ , it is simple to see that $c_{0}(\\dot{x_{2}})\\neq c_{1}(x_{2})$ and $c_{0}(x_{2})\\neq c_{2}(x_{2})$ and $c_{1}(x_{2})\\neq c_{2}(x_{2})$ . Thus, ${\\mathrm{NT}}({\\mathcal{C}}^{\\prime})$ can not be even 2, which completes our contradiction-based proof. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: In the abstract and introduction, we claim that we establish trichotomy of rates in the multiclass transductive online learning in the realizable setting and near tight upper and lower bounds in the agnostic setting for arbitrary label spaces. The first claim is proven in Section 3 and the second claim is proven in Section E. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 19}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: In Section 1.2.2, we point out that there is a gap of $\\log T$ between our lower and upper bounds on regret in the agnostic setting. We also point out future directions in the Discussion section. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 19}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: This paper provides a full set of assumptions and a complete proof for every Theoretical result. Proposition 1 is proved in Appendix B. Theorem 1 is proved in Section 3, Theorem 3 is proven in Section 3 and in Appendix C, and Lemma 1 is proved in Appendix D. All theorems and lemmas are properly referenced. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 20}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: This paper does not include experiments. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in ", "page_idx": 20}, {"type": "text", "text": "some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 21}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: This paper does not include experiments. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 21}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: This paper does not include experiments. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 21}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] Justification: This paper does not include experiments. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 21}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 22}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: This paper does not include experiments. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 22}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We have reviewed the NeurIPS Code of Ethics and ensured that our paper conforms, in every respect, with the NeurIPS Code of Ethics. We have also made sure to preserve anonymity. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 22}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: As this paper is completely theoretical in nature, there does not seem to be any soceital impact of the work performed. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 23}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: This paper is theoretical and poses no such risks. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 23}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: This paper does not use existing assets. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: This paper does not release new assets. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 24}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 24}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}]