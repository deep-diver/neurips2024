[{"type": "text", "text": "Learning to Mitigate Externalities: the Coase Theorem with Hindsight Rationality ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Antoine Scheid1 Aymeric Capitaine1 ", "page_idx": 0}, {"type": "text", "text": "Etienne Boursier2 Eric Moulines1 Michael I. Jordan3,4 Alain Durmus1 ", "page_idx": 0}, {"type": "text", "text": "1 Centre de Math\u00e9matiques Appliqu\u00e9es \u2013 CNRS \u2013 \u00c9cole polytechnique \u2013 Palaiseau, 91120, France 2 INRIA Saclay, Universit\u00e9 Paris Saclay, LMO - Orsay, 91400, France 3 University of California, Berkeley   \n4 Inria, Ecole Normale Sup\u00e9rieure, PSL Research University - Paris, 75, France ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In economic theory, the concept of externality refers to any indirect effect resulting from an interaction between players that affects the social welfare. Most of the models within which externality has been studied assume that agents have perfect knowledge of their environment and preferences. This is a major hindrance to the practical implementation of many proposed solutions. To address this issue, we consider a two-player bandit setting where the actions of one of the players affect the other player and we extend the Coase theorem [Coase, 2013]. This result shows that the optimal approach for maximizing the social welfare in the presence of externality is to establish property rights, i.e., enable transfers and bargaining between the players. Our work removes the classical assumption that bargainers possess perfect knowledge of the underlying game. We first demonstrate that in the absence of property rights, the social welfare breaks down. We then design a policy for the players which allows them to learn a bargaining strategy which maximizes the total welfare, recovering the Coase theorem under uncertainty. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The concept of externality is used in economics to capture phenomena that impact the global welfare stemming from economic interactions without any compensation [Buchanan and Stubblebine, 2006, Shah et al., 2018]. Externality is generally considered as market failure since they result in a loss of collective welfare. Given its practical importance [Dahlman, 1979, Greenfield et al., 2009], mechanisms that characterize and mitigate externalities are central to modern economic thinking. ", "page_idx": 0}, {"type": "text", "text": "A first approach to tackle the adverse effects of externalities in modern economics was based on quotas and taxation [see, e.g., Pigou, 2017, and the references therein]. However, Coase\u2019s theorem [Coase, 2013] shows that in the presence of well-defined property rights and low transaction costs, parties affected by externalities can privately negotiate efficient solutions, and recover a welfare efficient allocation through transfers and bargaining. ", "page_idx": 0}, {"type": "text", "text": "Throughout the paper, we use the following simple example to illustrate our results. ", "page_idx": 0}, {"type": "text", "text": "Example 1. Consider two firms 1 (upstream) and 2 (downstream) respectively producing quantities $q_{1}\\geqslant0$ and $q_{2}\\geqslant0$ of a good sold at a fixed price $p>0$ . They incur strictly increasing and convex costs, captured by the differentiable cost functions $c_{1}:q_{1}\\mapsto c_{1}(q_{1})$ and $c_{2}:q_{2}\\mapsto c_{2}{\\big(}q_{2}{\\big)},$ , satisfying $c_{1}(0)=0$ and $c_{2}(0)=0$ . We assume that firm 1 exerts on firm 2 a constant externality $\\alpha>0$ per unit produced. In other words, their profit functions (or utilities) are given by ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "equation", "text": "$$\n\\pi_{1}(q_{1})=p q_{1}-c_{1}(q_{1})\\quad a n d\\quad\\pi_{2}(q_{1},q_{2})=p q_{2}-c_{2}(q_{2})-\\alpha q_{1}\\;.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "One simple concrete illustration consists in an upstream firm emitting pollutants that reduce the downstream firm\u2019s production. If the upstream firm owns the property rights, it may receive a payment from the downstream one to reduce its output and thereby pollution. On the other hand, if the downstream firm owns the property right, it may require compensation from the upstream firm to allow its operation. ", "page_idx": 1}, {"type": "text", "text": "The Coase theorem demonstrates that in both cases with appropriate property rights, the resulting levels of production would be welfare efficient. The theorem is typically explained in textbooks under the assumption that the players have perfect knowledge of their own utility or profti function, as well as that of others. However, this assumption is unlikely to hold in real-world scenarios, where players have to learn about their own preferences and those of their competitors. ", "page_idx": 1}, {"type": "text", "text": "This example is, of course, a simplification of real world scenarios. For example, Abildtrup et al. [2012] consider a more complex setting to model the interaction between farmers and waterworks in Denmark where the farmers have the property rights whereas the waterworks can pay to reduce pollution. In particular, they demonstrate the failure of the theorem, attributing it to the breakdown of the main assumptions: no transaction costs, maximizing behaviors and perfect information, with an important focus on strategic behaviors and the asymmetry of information. We restore the latter in our work and provide foundations for the theorem to hold in more realistic scenarios. ", "page_idx": 1}, {"type": "text", "text": "A key question is whether the Coase theorem holds when players learn their preferences over time. ", "page_idx": 1}, {"type": "text", "text": "We investigate this question within the framework of a multi-armed bandit learning. We build upon recent works that extend the classical bandit setting to economics in which there are two players interacting via principal-agent protocols [Dogan et al., 2023b,a, Scheid et al., 2024]. This allows us to capture, for example, a version of the two-firm problem where firms are uncertain regarding both their profti functions and the degree of externality on other firms. We represent production decisions in this problem as arms which can be played by the firms at any round over time, with the goal of finding decisions that maximize their rewards. More precisely, we assume that the reward of the upstream firm only depends on its own action, while the reward of the downstream firm depends on both its action and the upstream firm\u2019s action. This dependency on both actions allows us to capture externalities. ", "page_idx": 1}, {"type": "text", "text": "The property rights in Example 1, or more generally over a bandit instance, amount to giving a firm the possibility of engaging in monetary transfers that influence the arms that are played over time. The owner of the bandit instance will face a problem of bandit learning with transfers with an upstream player who is also learning his preferences. ", "page_idx": 1}, {"type": "text", "text": "To account for the efficiency of a policy in this setup, we extend the classical static notion of welfare efficiency to the online setting. We say that a policy is Welfare efficient if the social welfare regret is sub-linear. Proving the Coase theorem within our setup therefore boils down to show that if the bandit owner (who is without loss of generality the upstream player in this study1) runs a no-regret bandit algorithm to learn and exploit his preferences, the downstream player can then choose an optimal transfer scheme leading to a sub-linear total regret. ", "page_idx": 1}, {"type": "text", "text": "Our contributions are as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We show that when an upstream agent exerts externality on a downstream agent, in the absence of property right, the social welfare breaks down. Put differently, no joint policy of the agents can be welfare efficient.   \n\u2022 We then introduce property rights and show how it affects the game. In this case, bargaining and transfers are available to the players. We propose a policy for the downstream player that leads to welfare efficiency when the upstream player follows any black-box no-regret policy under mild assumption. This solution addresses the breakdown issue at equilibrium. Put together, we show an online version of the Coase theorem. ", "page_idx": 1}, {"type": "text", "text": "2 Setup and Inefficiency of Externality ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Bandit game ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We consider a sequential bandit game in which two players (downstream and upstream) simultaneously play actions in a bandit instance for a horizon $T\\in\\mathbb{N}^{\\star}$ . The action set for both players is $\\pmb{\\mathcal{A}}=\\{\\dot{1},\\dot{.}\\cdot\\dot{.},K\\}$ , $K\\in\\mathbb{N}^{\\star}$ . ", "page_idx": 2}, {"type": "text", "text": "The reward distributions of the agents differ. Given a family of distributions $\\{\\gamma_{a}\\,:\\,a\\in{\\mathcal{A}}\\}$ indexed by $\\boldsymbol{\\mathcal{A}}$ , the upstream player\u2019s rewards are provided by an i.i.d. family of random variables ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\{(Z_{a}(t))_{t\\in[T]}\\,:\\,a\\in{\\mathcal{A}}\\}~~,~~~~\\mathrm{where}~Z_{a}(t)\\sim\\gamma_{a}~~~\\mathrm{for~any}~t\\in[T]~\\mathrm{and}~a\\in{\\mathcal{A}}~.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "To model the externality exerted by the upstream on the downstream player, we assume that the latter has a reward that depends both on her action and on that of the upstream player. Formally, this is modeled through a family of distributions $\\{\\nu_{a,b}\\,:\\,a,b\\in\\mathcal{A}\\}$ double-indexed by $\\boldsymbol{\\mathcal{A}}$ and an i.i.d. family of random variables ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\{(X_{a,b}(t))_{t\\in[T]}\\,:\\,a,b\\in{\\mathcal{A}}\\}\\;\\;,\\;\\;\\;\\;\\mathrm{where}\\;X_{a,b}(t)\\sim\\nu_{a,b}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "is the reward received by the downstream player at time $t$ if she pulls the arm $b$ and the upstream player pulls the arm $a$ . ", "page_idx": 2}, {"type": "text", "text": "Players. We assume that players are risk-neutral expected-utility maximizers, and we define their expected utilities for any $(a,b)\\in{\\mathcal{A}}\\times{\\mathcal{A}}$ as ", "page_idx": 2}, {"type": "equation", "text": "$$\nv^{\\mathrm{up}}(a)=\\int z\\,\\gamma_{a}(\\mathrm{d}z)\\in\\mathbb{R}\\quad\\mathrm{and}\\quad v^{\\mathrm{down}}(a,b)=\\int x\\,\\nu_{a,b}(\\mathrm{d}x)\\in\\mathbb{R}\\;.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The distributions $(\\gamma_{a})_{a\\in A}$ and $\\left(\\nu_{a,b}\\right)_{(a,b)\\in\\mathcal{A}^{2}}$ are unknown to both the downstream and the upstream players and they aim to learn the distributions with best mean rewards by sequentially observing samples from $\\{(Z_{a}(t))_{t\\in[T]}\\,:\\,a\\in{\\cal A}\\}$ and $\\{(X_{a,b}(t))_{t\\in[T]}\\,:\\,a,b\\in\\mathcal{A}\\}$ . ", "page_idx": 2}, {"type": "text", "text": "Moreover, we suppose that players are rational in hindsight; that is, they minimize their regret. Formally, the upstream player aims to minimize his regret defined as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\Re_{\\mathrm{n}}^{\\mathrm{up}}(T,\\Pi_{\\mathrm{n}}^{\\mathrm{up}})=T\\mu^{\\star,\\mathrm{up}}-\\mathbb{E}\\left[\\sum_{t=1}^{T}v^{\\mathrm{up}}(A_{t})\\right]\\,,\\ \\mathrm{where}\\ \\mu^{\\star,\\mathrm{up}}=\\operatorname*{max}_{a\\in\\mathcal{A}}v^{\\mathrm{up}}(a)\\ ,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "while the downstream player seeks to minimize her external regret defined as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\Re_{\\mathrm{n}}^{\\mathrm{down}}(T,\\Pi_{\\mathrm{n}}^{\\mathrm{up}},\\Pi_{\\mathrm{n}}^{\\mathrm{down}})=\\mathbb{E}\\!\\left[\\sum_{t=1}^{T}\\operatorname*{max}_{b\\in\\mathcal{A}}v^{\\mathrm{down}}(A_{t},b)-v^{\\mathrm{down}}(A_{t},B_{t})\\right]\\,,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where the players\u2019 actions $(A_{t})_{t\\in[T]}$ , $(B_{t})_{t\\in[T]}$ as well as their policies $\\Pi_{\\mathrm{n}}^{\\mathrm{up}}$ , $\\Pi_{\\mathrm{n}}^{\\mathrm{down}}$ are defined below. Note that the utility of the downstream player also depends on the actions taken by the upstream player, which represents the externality exerted by the upstream player on the downstream player, hence the strategic dimension of our setting. We first consider a game where no property right is defined, so each player is free to pick his preferred arm irrespectively of the other player\u2019s choice. This will result in a breakdown of the total utility. ", "page_idx": 2}, {"type": "text", "text": "Policies without property rights. Consider first the upstream player. Based on a policy $\\Pi_{\\mathrm{n}}^{\\mathrm{up}}$ (for example a no-regret bandit algorithm such as the Upper Confidence Bounds algorithm (UCB) [Auer, 2002] or the $\\varepsilon$ -greedy algorithm [Robbins, 1952, Langford and Zhang, 2007]), we define his history $(\\mathcal{H}_{t}^{\\mathrm{up,n}})_{t\\in[T]}$ by induction. We set $\\mathcal{H}_{0}^{\\mathrm{up,n}}=\\emptyset$ and supposing that $\\bar{\\mathcal{H}}_{t}^{\\mathrm{up,n}}$ is defined for $t\\in[T]$ , then ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{H}_{t+1}^{\\mathrm{up,n}}=\\mathcal{H}_{t}^{\\mathrm{up,n}}\\cup\\left\\{A_{t+1},V_{t+1},Z_{A_{t+1}}(t+1)\\right\\}\\,,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $(V_{s})_{s\\in\\mathbb{N}^{\\star}}$ is a family of independent uniform random variables in $[0,1]$ , allowing for randomization in the policy, and $A_{t+1}$ is provided by $\\Pi_{\\mathrm{n}}^{\\mathrm{up}}$ , following $\\Pi_{\\mathrm{n}}^{\\mathrm{up}}\\colon(V_{t+1},\\Tilde{\\mathcal{H}}_{t}^{\\mathrm{up,n}})\\mapsto A_{t+1}$ . ", "page_idx": 2}, {"type": "text", "text": "Second, consider the downstream player and an algorithm $\\Pi_{\\mathrm{n}}^{\\mathrm{down}}$ (specifically a no-regret bandit algorithm). We define her history $(\\mathcal{H}_{t}^{\\mathrm{down,n}})_{t\\in[T]}$ by induction. We set H0down,n= \u2205and supposing that $\\mathcal{H}_{t}^{\\mathrm{down,n}}$ is defined for $t\\in[T]$ , then ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{H}_{t+1}^{\\mathrm{down,n}}=\\mathcal{H}_{t}^{\\mathrm{down,n}}\\cup\\left\\{A_{t+1},B_{t+1},U_{t+1}X_{A_{t+1},B_{t+1}}(t+1)\\right\\}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $(U_{s})_{s\\in\\mathbb{N}^{\\star}}$ is a family of independent uniform random variables in $[0,1]$ allowing for randomization in the policy and $B_{t+1}$ is provided by $\\Pi_{\\mathrm{n}}^{\\mathrm{down}}$ , following $\\Pi_{\\mathrm{n}}^{\\mathrm{down}}\\colon(U_{t+1},\\mathcal{H}_{t}^{\\mathrm{down,n}})\\mapsto B_{t+1}$ . ", "page_idx": 3}, {"type": "text", "text": "Welfare efficiency. We now introduce the notion of Welfare efficiency for our setup. The global utility, or social welfare, of the players at round $t$ is defined as $v^{\\mathrm{up}}(A_{t})+v^{\\mathrm{down}}(A_{t},\\Bar{B}_{t})$ . We define the socially optimal action $(a^{\\mathrm{sw}},b^{\\mathrm{iw}})\\in\\mathcal{A}\\times\\mathcal{A}$ of the game as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(a^{\\mathrm{sw}},b^{\\mathrm{sw}})\\in\\mathrm{argmax}_{a,b\\in\\cal A}\\;\\;v^{\\mathrm{up}}(a)+v^{\\mathrm{down}}(a,b)\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "as well as the global regret (or social welfare regret) associated with policies $\\Pi_{\\mathrm{n}}^{\\mathrm{up}}$ and $\\Pi_{\\mathrm{n}}^{\\mathrm{down}}$ as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathfrak{R}^{\\mathrm{sw}}(T,\\Pi_{\\mathfrak{n}}^{\\mathrm{up}},\\Pi_{\\mathfrak{n}}^{\\mathrm{down}})=T\\left(v^{\\mathrm{up}}(a^{\\mathrm{sw}})+v^{\\mathrm{down}}(a^{\\mathrm{sw}},b^{\\mathrm{sw}})\\right)-\\sum_{t=1}^{T}\\mathbb{E}\\big[v^{\\mathrm{up}}(A_{t})+v^{\\mathrm{down}}(A_{t},B_{t})\\big]\\;.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Then, the joint policies $\\Pi_{\\mathrm{n}}^{\\mathrm{up}}$ and $\\Pi_{\\mathrm{n}}^{\\mathrm{down}}$ for the players are said to be Welfare efficient if ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{T\\to+\\infty}\\Re^{\\mathrm{sw}}(T,\\Pi_{\\mathrm{n}}^{\\mathrm{up}},\\Pi_{\\mathrm{n}}^{\\mathrm{down}})/T=0\\;.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Intuitively, this condition implies that the frequency of the socially optimal action $(a^{\\mathrm{sw}},b^{\\mathrm{sw}})$ tends to 1 as $T$ goes to infinity. In this sense, it mimics the usual, static Welfare efficiency criterion. As we will see, $(\\Pi_{\\mathrm{n}}^{\\mathrm{up}},\\Pi_{\\mathrm{n}}^{\\mathrm{down}})$ is typically not Welfare efficient when there is a disalignment in the game between the players\u2019 individual interests based on their rationality and the social welfare. ", "page_idx": 3}, {"type": "text", "text": "2.2 Inefficiency without property rights ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We first present a result that captures the adverse consequence of externality on social welfare. The upstream player does not take into account the indirect cost incurred by the downstream player when he chooses his action. This drives the social welfare away from its optimal level. We illustrate this fact within our simple bilateral externality example. ", "page_idx": 3}, {"type": "text", "text": "Example 1 (continuing from p. 1). We show that the competitive outcome, where each firm maximizes its profti independently, is not welfare efficient in the presence of externality. Define the social welfare as the function ", "page_idx": 3}, {"type": "equation", "text": "$$\nW:(q_{1},q_{2})\\mapsto\\pi_{1}(q_{1})+\\pi_{2}(q_{1},q_{2})=p(q_{1}+q_{2})-(c_{1}(q_{1})+c_{2}(q_{2}))-\\alpha q_{1}\\;.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "By definition, the welfare efficient outcome $(q_{1}^{\\star},q_{2}^{\\star})\\in\\mathbb{R}_{+}^{2}$ satisfies $W(q_{1}^{\\star},q_{2}^{\\star})\\geq W(q_{1},q_{2})$ for any $(q_{1},q_{2})\\in\\mathbb{R}_{+}^{2}$ . Since $W$ is differentiable and strictly concave, $(q_{1}^{\\star},q_{2}^{\\star})$ is uniquely defined by the condition $\\nabla\\dot{W}(q_{1}^{\\star},q_{2}^{\\star})=0$ , that is ", "page_idx": 3}, {"type": "equation", "text": "$$\nc_{1}^{\\prime}(q_{1})-\\alpha=p\\quad a n d\\quad c_{2}^{\\prime}(q_{2})=p\\ .\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Note that at the welfare efficient optimum, firm $^{\\,l}$ does not equalize marginal cost with marginal profti, but produces less to account for the negative effect of externality on firm 2. We now characterize the competitive outcome $(q_{1}^{\\prime},q_{2}^{\\prime})\\in\\mathbb{R}_{+}^{2}$ . Since $\\pi_{1}$ and $\\pi_{2}$ are differentiable and strictly concave, $(q_{1}^{\\prime},q_{2}^{\\prime})$ satisfies ", "page_idx": 3}, {"type": "equation", "text": "$$\nc_{1}^{\\prime}(q_{1}^{\\prime})=p\\ \\ \\ a n d\\ \\ \\ c_{2}^{\\prime}(q_{2}^{\\prime})=p\\;.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "For the competitive outcome to be welfare efficient, we require, by Equation (6) and Equation (7), ", "page_idx": 3}, {"type": "equation", "text": "$$\nc_{1}^{\\prime}(q_{1}^{\\prime})-\\alpha=c_{1}^{\\prime}(q_{1}^{\\prime}),~~~t h a t\\,i s~~~\\alpha=0~.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "This proves that whenever there are externalities, no competitive outcome is efficient. ", "page_idx": 3}, {"type": "text", "text": "We now show that in our model, when there is no property right and under mild assumptions, no achievable policy is welfare efficient whenever there is a misalignment between the players\u2019 interests and the social welfare. The upstream player\u2019s policy $\\Pi_{\\mathrm{n}}^{\\mathrm{up}}$ is said to be no-regret if $\\mathrm{lim}_{T\\rightarrow+\\infty}\\,\\Re_{\\mathrm{n}}^{\\mathrm{up}}(T,\\Pi_{\\mathrm{n}}^{\\mathrm{up}})/T=0$ , where $\\Re_{\\mathrm{n}}^{\\mathrm{up}}$ is defined in (1). ", "page_idx": 3}, {"type": "text", "text": "Theorem 2. Suppose that arg ${\\mathrm{max}}_{a\\in A}\\,v^{\\operatorname{up}}(a)$ is the singleton $\\{a_{\\star}^{\\mathrm{u}}\\}$ and that ", "page_idx": 3}, {"type": "equation", "text": "$$\nv^{\\mathrm{up}}(a^{\\mathrm{sw}})+v^{\\mathrm{down}}(a^{\\mathrm{sw}},b^{\\mathrm{sw}})-v^{\\mathrm{up}}(a_{\\star}^{\\mathrm{u}})+v^{\\mathrm{down}}(a_{\\star}^{\\mathrm{u}},b)>0\\ ,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "for any $b\\in{\\mathcal{A}}$ . In the absence of property rights and when the upstream player runs any no-regret $\\Pi_{\\mathrm{n}}^{\\mathrm{up}}$ w ies  hnaovt e $\\mathfrak{R}^{\\mathrm{sw}}(T,\\Pi_{\\mathrm{n}}^{\\mathrm{up}},\\Pi_{\\mathrm{n}}^{\\mathrm{down}})=\\Omega(T)$ . Therefore, $\\mathfrak{R}^{\\mathrm{sw}}(T,\\Pi_{\\mathrm{n}}^{\\mathrm{up}},\\Pi_{\\mathrm{n}}^{\\mathrm{down}})=\\Omega(T)$ and $(\\Pi_{\\mathrm{n}}^{\\mathrm{up}},\\Pi_{\\mathrm{n}}^{\\mathrm{down}})$ ", "page_idx": 3}, {"type": "text", "text": "Condition (8) in Theorem 2 represents the unalignment between the upstream player\u2019s preference and the optimal choice from a social welfare point of view. Note that the upstream and downstream players can both have an $o(T)$ external regret, while the social welfare regret still grows linearly with $T$ because of the unfavorable interactions between their policies. ", "page_idx": 4}, {"type": "text", "text": "3 Online Property Game with Bargaining Players ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "3.1 Online Property Game ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We now consider the same repeated game in the form of a property game where one of the players possesses the bandit instance (upstream player). As in the original setup of Coase [2013], the other player (downstream player) will provide the bandit owner with transfers to incentivize him to choose some specific action and influence the outcome of the game in her favor. ", "page_idx": 4}, {"type": "text", "text": "We show in Appendix B that our method applies similarly when property rights are given to the upstream player rather than the downstream player. Hence, there is no loss of generality in considering the aforementioned framework. In this sense, we recover the invariance property of the Coasean bargaining [Mas-Colell et al., 1995]. ", "page_idx": 4}, {"type": "text", "text": "Example 1 (continuing from p. 1). We now illustrate how Coasean bargaining re-instaures efficiency. Suppose without loss of generality that property rights are such that firm 2 can pay $\\tau\\in\\mathbb{R}_{+}$ to firm $^{\\,l}$ for it to operate at a level $\\tilde{q}_{1}$ . Profits become ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bar{\\pi}_{2}:(q_{1},q_{2},\\tau,\\tilde{q}_{1})\\mapsto\\pi_{2}(q_{1},q_{2})-\\mathbb{1}_{\\{q_{1}=\\tilde{q}_{1}\\}}\\tau\\quad a n d\\quad\\bar{\\pi}_{1}:(q_{1},\\tau,\\tilde{q}_{1})\\mapsto\\pi(q_{1})+\\mathbb{1}_{\\{q_{1}=\\tilde{q}_{1}\\}}\\tau.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Consider the competitive outcome $(q_{1},q_{2},\\tau,\\tilde{q}_{1})\\in\\mathbb{R}_{+}^{4}$ which satisfies ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\quad q_{1}=q_{1}(\\tau,\\tilde{q}_{1})\\in\\arg\\underset{q_{1}^{\\prime}\\geqslant0}{\\operatorname*{max}}\\,\\bar{\\pi}_{1}(q_{1}^{\\prime},\\tau,\\tilde{q}_{1})\\quad a n d}\\\\ &{\\quad\\quad\\quad\\bar{\\pi}_{2}(q_{1}(\\tau,\\tilde{q}_{1}),q_{2},\\tau,\\tilde{q}_{1})=\\underset{q_{2}^{\\prime},\\tau^{\\prime},\\tilde{q}_{1}^{\\prime}}{\\operatorname*{max}}\\,\\,\\bar{\\pi}_{2}(q_{1}(\\tau^{\\prime},\\tilde{q}_{1}^{\\prime}),q_{2}^{\\prime},\\tau^{\\prime},\\tilde{q}_{1}^{\\prime}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The condition on $q_{1}$ accounts for the rationality of the firm 1 and the fact that its choice depends on the payment $(\\tau,\\tilde{q}_{1})$ . Obviously, the optimal solution is reached for $\\tilde{q}_{1}=q_{1}$ and $\\tau=\\operatorname*{max}_{q^{\\prime}}\\pi_{1}(q^{\\prime})\\mathrm{~-~}$ $\\pi_{1}(\\tilde{q}_{1})$ . Plugging this back in the expression of $\\bar{\\pi}_{2}$ then yields ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(q_{1},q_{2})=\\mathrm{{argmax}}_{q_{1}^{\\prime},q_{2}^{\\prime}}\\,\\pi_{1}(q_{1}^{\\prime})+\\pi_{2}(q_{2}^{\\prime})=\\mathrm{{argmax}}_{q_{1}^{\\prime},q_{2}^{\\prime}}\\,W(q_{1}^{\\prime},q_{2}^{\\prime})\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "so the competitive outcome $(q_{1},q_{2})$ is welfare efficient. ", "page_idx": 4}, {"type": "text", "text": "The transfers at each step can be interpreted as a contract between two players [see, e.g., Bolton and Dewatripont, 2004, Salani\u00e9, 2005, for general contract theory] and providing the right amount of incentives relates to adjusting a contract in an online setting [see D\u00fctting et al., 2019, Guruganesh et al., 2021, Zhu et al., 2022, Fallah and Jordan, 2023, Guruganesh et al., 2024, Ananthakrishnan et al., 2024, for learning-based perspectives about contracts]. ", "page_idx": 4}, {"type": "text", "text": "Similarly to Example 1, we modify the players\u2019 policies to now account for the transfer $\\tau(t)$ that the downstream player offers at round $t$ to the upstream player if he picks action $\\tilde{a}_{t}$ . The downstream player\u2019s policy at round $t$ does not only output an arm $B_{t}$ but now a triple $(\\tilde{a}_{t},\\tau(t),B_{t})$ , where $B_{t}$ is the arm that she should play and $\\tilde{a}_{t}$ is the arm on which a transfer $\\tau(t)$ is offered to the upstream player. On the upstream player\u2019s side, the policy still outputs an arm $A_{t}$ to play but also takes as an input the incentive $\\left(\\tilde{a}_{t},\\tau(t)\\right)$ . In addition, the instantaneous utility of the upstream player becomes $Z_{A_{t}}^{-}(t)+\\mathbb{1}_{\\tilde{a}_{t}}(A_{t})\\tau\\dot{(}t)$ , whereas the downstream player receives $\\dot{X}_{A_{t},B_{t}}(t)\\dot{-}\\mathbb{1}_{\\tilde{a}_{t}}(\\bar{A_{t}})\\dot{\\tau}(t)$ . ", "page_idx": 4}, {"type": "text", "text": "Policies with property rights. Based on policies $\\Pi_{\\mathrm{p}}^{\\mathrm{up}}$ for the upstream player and $\\Pi_{\\mathrm{p}}^{\\mathrm{down}}$ for the downstream player, we define their histories $(\\mathcal{H}_{t}^{\\mathrm{up,p}})_{t\\in[T]}$ and Htdown,p)t\u2208[T ] by induction. We set $\\mathcal{H}_{0}^{\\mathrm{up,p}}=\\mathcal{O}$ , $\\mathcal{H}_{0}^{\\mathrm{down,p}}=\\mathcal{O}$ and supposing that $\\mathcal{H}_{t}^{\\mathrm{up,p}}$ , $\\mathcal{H}_{t}^{\\mathrm{down,p}}$ are defined for $t\\in[T]$ , then ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{H}_{t+1}^{\\mathrm{up,p}}=\\mathcal{H}_{t}^{\\mathrm{up,p}}\\cup\\{\\tilde{a}_{t+1},\\tau(t+1),A_{t+1},V_{t+1},Z_{A_{t+1}}(t+1)\\}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "and ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{H}_{t+1}^{\\mathrm{down,p}}=\\mathcal{H}_{t}^{\\mathrm{down,p}}\\cup\\left\\{\\tilde{a}_{t+1},\\tau(t+1),A_{t+1},B_{t+1},U_{t+1},X_{A_{t+1},B_{t+1}}(t+1)\\right\\}\\,,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\left(V_{s}\\right)_{s\\in\\mathbb{N}^{\\star}}$ , $(U_{s})_{s\\in\\mathbb{N}^{\\star}}$ are two families of independent uniform random variables in $[0,1]$ allowing for randomization in the policies, and the remaining quantities are given by $\\Pi_{\\mathrm{p}}^{\\mathrm{down}}\\colon(U_{t+1},\\mathcal{H}_{t}^{\\mathrm{down,p}})\\mapsto(\\tilde{a}_{t+1},\\tau(t{+}1),B_{t+1})$ and $\\Pi_{\\mathrm{p}}^{\\mathrm{up}}\\colon(\\tilde{a}_{t+1},\\tau(t{+}1),V_{t+1},\\mathcal{H}_{t}^{\\mathrm{up,p}})\\mapsto A_{t+1}$ ", "page_idx": 5}, {"type": "text", "text": "Players\u2019 goal. Given a transfer $\\tau$ from the downstream to the upstream player on arm $\\tilde{a}$ , actions $a$ and $b$ respectively are chosen by the upstream and the downstream player, the upstream player\u2019s expected utility reads $v^{\\mathrm{up}}(a)+\\mathbb{1}_{\\tilde{a}}(a)\\tau$ while the downstream player\u2019s expected utility is $v^{\\mathrm{down}}(a,b)-\\mathbb1_{\\Tilde{a}}(a)\\tau$ . This defines the upstream player\u2019s expected regret for a horizon $T$ as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\Re_{\\mathrm{p}}^{\\mathrm{up}}(T,\\Pi_{\\mathrm{p}}^{\\mathrm{up}},\\Pi_{\\mathrm{p}}^{\\mathrm{down}})=\\mathbb{E}\\left[\\sum_{t=1}^{T}\\operatorname*{max}_{a\\in A}\\{v^{\\mathrm{up}}(a)+\\mathbb{1}_{\\Tilde{a}_{t}}(a)\\tau(t)\\}-(v^{\\mathrm{up}}(A_{t})+\\mathbb{1}_{\\Tilde{a}_{t}}(A_{t})\\tau(t))\\right]\\;.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Based on the upstream player\u2019s utility, the downstream player aims on a single round at proposing an optimal transfer $\\tau^{\\mathrm{opt}}$ on an arm $a^{\\mathrm{opt}}\\in A$ as well as picking an arm $b^{\\mathrm{opt}}\\in\\bar{A}$ which solves ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tau\\in\\mathbb{R}_{+},b\\in\\mathcal{A},a\\in\\mathrm{argmax}_{a^{\\prime}\\in\\mathcal{A}}\\{v^{\\mathrm{up}}(a^{\\prime})+\\mathbb{1}_{a}(a^{\\prime})\\tau\\}\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Her regret for any horizon $T$ is defined as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\Re_{\\mathrm{p}}^{\\mathrm{down}}(T,\\Pi_{\\mathrm{p}}^{\\mathrm{up}},\\Pi_{\\mathrm{p}}^{\\mathrm{down}})=T\\mu^{\\star,\\mathrm{down}}-\\mathbb{E}\\left[\\sum_{t=1}^{T}v^{\\mathrm{down}}(A_{t},B_{t})-\\mathbb{1}_{\\Tilde{a}_{t}}(A_{t})\\tau(t)\\right]\\,,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where we define $\\mu^{\\star,\\mathrm{down}}=v^{\\mathrm{down}}(a^{\\mathrm{opt}},b^{\\mathrm{opt}})-\\tau^{\\mathrm{opt}}$ as the optimal utility she can aim for. We can see that the downstream player\u2019s influence is exerted through her action choice $B_{t}$ as well as through transfers which enable her to influence the upstream player\u2019s actions. Hence, the notion of external regret is obsolete here. The game has now the form of a repeated Stackelberg game [Von Stackelberg, 2010]. ", "page_idx": 5}, {"type": "text", "text": "Lemma 1. Recall that $\\mu^{\\star,\\mathrm{down}}$ is the downstream player\u2019s optimal reward as defined as a solution of (10). We have $\\begin{array}{r}{\\mu^{\\star,\\mathrm{down}}=\\operatorname*{max}_{a,b\\in\\mathcal{A}}\\{v^{\\mathrm{down}}(a,b)+v^{\\mathrm{up}}(a)\\}-\\operatorname*{max}_{a^{\\prime}\\in\\mathcal{A}}\\{v^{\\mathrm{up}}(a^{\\prime})\\},}\\end{array}$ , as well as $(a^{\\mathrm{opt}},b^{\\mathrm{opt}})=(a^{\\mathrm{sw}},b^{\\mathrm{sw}})$ and $\\begin{array}{r}{^{\\prime}\\mu^{\\star,\\mathrm{up}}+\\mu^{\\star,\\mathrm{down}}=v^{\\mathrm{up}}(a^{\\mathrm{sw}})+v^{\\mathrm{down}}(a^{\\mathrm{sw}},b^{\\mathrm{sw}})=\\operatorname*{max}_{a,b\\in\\mathcal{A}}\\{v^{\\mathrm{up}}(a)+\\log(\\frac{\\sf d_{1}}{a})\\}.}\\end{array}$ $\\boldsymbol{v}^{\\mathrm{down}}(a,b)\\}$ , where $\\mu^{\\star,\\mathrm{up}}$ is defined in Equation (1). Moreover, for any integer $T\\in\\mathbb{N}^{\\star}$ , and policies $\\Pi_{\\mathrm{p}}^{\\mathrm{up}},\\,\\Pi_{\\mathrm{p}}^{\\mathrm{down}}$ , we have that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Re^{\\mathrm{sw}}(T,\\Pi_{\\mathrm{p}}^{\\mathrm{up}},\\Pi_{\\mathrm{p}}^{\\mathrm{down}})\\leqslant\\Re_{\\mathrm{p}}^{\\mathrm{up}}(T,\\Pi_{\\mathrm{p}}^{\\mathrm{up}},\\Pi_{\\mathrm{p}}^{\\mathrm{down}})+\\Re_{\\mathrm{p}}^{\\mathrm{down}}(T,\\Pi_{\\mathrm{p}}^{\\mathrm{up}},\\Pi_{\\mathrm{p}}^{\\mathrm{down}})\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "This lemma has an interesting economic interpretation: if both players individually seek for their own interest within this online property game, they will together converge towards the optimal global utility. Individual rationality moves the outcome of the game towards the optimal social welfare. The transfers allow the players to align their goals and share the global reward, in line with the Coase theorem. Consequently, if both players run no-regret policies $\\Pi_{\\mathrm{p}}^{\\mathrm{up}}$ and $\\Pi_{\\mathrm{p}}^{\\mathrm{down}}$ , the social welfare regret will also be in $o(T)$ . The rest of the paper shows that such no-regret policies exist. To this end, we introduce the following assumptions. ", "page_idx": 5}, {"type": "text", "text": "Without loss of generality, we assume that the upstream player\u2019s utility is rescaled and shifted, which corresponds to the following assumption on the reward distribution $(\\gamma_{a})_{a\\in A}$ in $\\mathbb{R}$ . ", "page_idx": 5}, {"type": "text", "text": "H1. For any $a\\in{\\mathcal{A}}$ , we have $v^{\\mathrm{up}}(a)\\in[0,1]$ . ", "page_idx": 5}, {"type": "text", "text": "We now make a high probability bound assumption on the upstream player\u2019s regret.2. ", "page_idx": 5}, {"type": "text", "text": "H2. There exist $\\mathrm{C},\\zeta>0,\\kappa\\in[0,1)$ such that for any $s,t\\in[T]$ with $s\\!+\\!t\\leqslant T_{s}$ , any $\\{\\tau_{a}\\}_{a\\in[K]}\\in\\mathbb{R}_{+}^{K}$ and any policy $\\Pi_{\\mathrm{p}}^{\\mathrm{down}}$ that offers almost surely a transfer $(\\tilde{a}_{l},\\tau(l))\\,=\\,(\\tilde{a}_{l},\\tau_{\\tilde{a}_{l}})$ for any $l\\,\\in\\,\\{s+$ $1,\\ldots,s+t\\}$ , the batched regret of the upstream player following $\\Pi_{\\mathrm{p}}^{\\mathrm{up}}$ satisfies, with probability at least $1-t^{-\\zeta}$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\sum_{l=s+1}^{s+t}\\operatorname*{max}_{a\\in\\mathcal{A}}\\{v^{\\mathrm{up}}(a)+\\mathbb{1}_{\\Tilde{a}_{l}}(a)\\tau_{\\Tilde{a}_{l}}\\}-(v^{\\mathrm{up}}(A_{l})+\\mathbb{1}_{\\Tilde{a}_{l}}(A_{l})\\tau_{\\Tilde{a}_{l}})\\leqslant\\mathrm{C}t^{\\kappa}\\;.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "2A similar assumption is made in the work of Donahue et al. [2024] but with a stronger instantaneous regret bound which does not encompass the UCB\u2019s regret bound. ", "page_idx": 5}, {"type": "text", "text": "The constraint on the downstream player\u2019s algorithm $\\Pi_{\\mathrm{p}}^{\\mathrm{down}}$ enforces constant incentives associated with any arm $a\\in{\\mathcal{A}}$ withtin the batch, while the incentivized actions $(\\tilde{a}_{l})_{l\\in\\{s+1,\\dots,s+t\\}}$ may change. Proposition 2 in Appendix $\\mathbf{C}$ shows that an adaptation of UCB taking account the incentives satisfies $\\mathbf{H}2$ with $\\mathrm{C}=8\\sqrt{K\\log(K T^{3})},$ , $\\kappa=1/2$ and $\\zeta=2$ . Note that usual bandit algorithms such as AAE, ETC or EXP-IX also satisfy the assumption [see, e.g., Donahue et al., 2024, Lattimore and Szepesv\u00e1ri, 2020]. ", "page_idx": 6}, {"type": "text", "text": "3.2 Downstream player\u2019s procedure ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We fix the policy $\\Pi_{\\mathrm{p}}^{\\mathrm{up}}$ which can be any algorithm satisfying $\\mathbf{H}2$ for the upstream player and introduce the algorithm BELGIC (Bandits and Externalities for a Learning Game with Incentivized Coase) which provides a policy achieving sub-linear regret for the downstream player. It can be seen as an online bargaining strategy to mitigate externalities. Simply put, BELGIC unfolds in two steps. First note that for any action $a\\in{\\mathcal{A}}$ , the optimal (lowest) transfer to offer to the upstream player to make him choose $a$ is ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\tau_{a}^{\\star}=\\operatorname*{max}_{a^{\\prime}\\in\\mathcal{A}}{v^{\\mathrm{up}}(a^{\\prime})}-{v^{\\mathrm{up}}(a)}\\;,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "as detailed in Appendix A. Therefore, a batched binary search procedure (Algorithm 2) first allows the downstream player to estimate the optimal transfers $\\tau_{1}^{\\star},\\dots,\\tau_{K}^{\\star}$ with a good precision level of $1/T^{\\beta}$ , where $\\beta>0$ . More precisely, the downstream player offers a constant incentive $(\\tilde{a},\\tau_{\\tilde{a}})$ for a batch of time steps of length $\\tilde{T}=\\lceil T^{\\alpha}\\rceil$ . The observation of $T_{\\tilde{a}}^{\\neq}$ , the number of steps from the batch for which the upstream player does not pick $\\tilde{a}$ allows her to estimate whether $\\tau_{\\tilde{a}}$ is above or below $\\tau_{\\tilde{a}}^{\\star}$ and adjust it, following Lemma 2 in Appendix A under the condition that $\\alpha,\\beta$ satisfy ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\beta/\\alpha<(1-\\kappa)~.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The procedure needs to be run for $K\\lceil T^{\\alpha}\\rceil\\lceil\\log_{2}T^{\\beta}\\rceil$ rounds since we have to make $\\lceil\\log_{2}T^{\\beta}\\rceil$ batches of binary search of length $\\lceil T^{\\alpha}\\rceil$ on each of the $K$ arms [see Scheid et al., 2024]. This corresponds to the first phase of BELGIC as described in Algorithm 2. At the end of this stage, the estimated transfers $\\left(\\hat{\\tau}_{a}\\right)_{a\\in A}$ satisfy the bound in Proposition 1. These are then used to feed the subroutine Bandit-Alg. ", "page_idx": 6}, {"type": "text", "text": "Proposition 1. Under $H I$ and $_{H2}$ , after the first phase of BELGIC which consists in $K\\lceil T^{\\alpha}\\rceil\\lceil\\log T^{\\beta}\\rceil$ steps of binary search grouped in $\\lceil\\log_{2}T^{\\beta}\\rceil$ batches per arm $a\\in{\\mathcal{A}}$ , we have that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Big(\\;f o r\\,a n y\\;a\\in\\mathcal{A},\\hat{\\tau}_{a}-4/T^{\\beta}-\\mathbb{C}T^{(\\kappa-1)/2}\\leqslant\\tau_{a}^{\\star}\\leqslant\\hat{\\tau}_{a}\\Big)\\geqslant1-K\\lceil\\log_{2}T^{\\beta}\\rceil/T^{\\alpha\\zeta}\\;.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The additional term $1/T^{\\beta}+\\mathrm{C}T^{\\kappa-1}$ in $\\hat{\\tau}_{a}$ ensures that if $\\mathbf{H}2$ holds, the upstream player necessarily plays the incentivized action $\\tilde{a}_{t}$ at round $t$ with high probability. Lemmas 2 and 6 from Appendix C show how the binary search batches in BELGIC allow us to estimate $\\tau_{a}^{\\star}$ depending on $\\tilde{T}-T_{a}^{\\neq}$ , the number of times that arm $a$ has been pulled by the upstream player during the batch. ", "page_idx": 6}, {"type": "text", "text": "Then, any bandit subroutine Bandit-Alg, such as UCB or $\\varepsilon$ -greedy, for instance, can be run in a black-box fashion on the shifted bandit instance, where the rewards are shifted by the upper estimated transfers $(\\hat{\\tau}_{a})_{a\\in A}$ . The downstream player computes a shifted history H\u02dctdown,psuch that for any $t\\leqslant K\\lceil T^{\\alpha}\\rceil\\lceil\\log_{2}T^{\\beta}\\rceil,\\tilde{\\mathcal{H}}_{t}^{\\mathrm{down,p}}=\\emptyset$ and for any $t>K\\lceil T^{\\alpha}\\rceil\\lceil\\log_{2}T^{\\beta}\\rceil$ ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\tilde{\\mathcal{H}}_{t}^{\\mathrm{down,p}}=\\left\\{\\begin{array}{r l}&{\\{\\tilde{a}_{t},B_{t},\\tau(t),U_{t},X_{\\tilde{a}_{t},B_{t}}(t)-\\hat{\\tau}_{\\tilde{a}_{t}}\\}\\cup\\tilde{\\mathcal{H}}_{t-1}^{\\mathrm{down,p}}\\,\\mathrm{if}\\;\\tilde{a}_{t}=A_{t}}\\\\ &{\\tilde{\\mathcal{H}}_{t-1}^{\\mathrm{down,p}}\\quad\\mathrm{~otherwise}\\;,}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "which serves to feed Bandit-Alg, following ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathtt{B a n d i t-A l g\\div}\\left(U_{t},\\tilde{\\mathcal{H}}_{t-1}^{\\mathrm{down,p}}\\right)\\mapsto\\left(\\tilde{a}_{t},B_{t}\\right)\\in\\mathcal{A}\\times\\mathcal{A}\\,.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "For any family of constant incentives $\\{\\tau_{a}\\}_{a\\in\\mathcal{A}}\\in\\mathbb{R}_{+}^{K}$ , we define $\\mathfrak{R}_{\\mathrm{Bandit-Alg}}(T,\\nu,\\{\\tau_{a}\\}_{a\\in\\mathcal{A}})$ as the regret for the downstream player\u2019s subroutine Bandit-Alg on the bandit instance with shifted means over $T$ rounds, following ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\Re_{\\mathrm{Bandit-Alg}}(T,\\nu,\\{\\tau_{a}\\}_{a\\in A})=T\\operatorname*{max}_{a,b\\in A^{2}}\\mathbb{E}\\big[v_{a,b}^{\\mathrm{down}}(1)-\\tau_{a}\\big]-\\mathbb{E}\\left[\\sum_{t=1}^{T}v^{\\mathrm{down}}(\\tilde{a}_{t},B_{t})-\\tau_{\\tilde{a}_{t}}\\right]\\,.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Note that here, Bandit- $\\mathtt{-A l g}$ aims to maximize the shifted reward $(v^{\\mathrm{down}}(a,b)-\\tau_{a})_{(a,b)\\in\\mathcal{A}^{2}}$ . ", "page_idx": 6}, {"type": "text", "text": "1: Input: Set of actions ${\\cal A}\\,=\\,[K]$ , time horizon $T$ , subroutine $\\Pi_{\\mathrm{p}}^{\\mathrm{up}}$ , upstream player\u2019s regret   \nconstants $\\mathrm{C},\\kappa$ , parameters $\\alpha$ and $\\beta$ .   \n2: Compute $\\tilde{\\mathcal{H}}_{s}^{\\mathrm{down,p}}=\\mathcal{O}$ for any $s\\leqslant K\\lceil\\log_{2}T^{\\beta}\\rceil\\lceil T^{\\alpha}\\rceil$ .   \n3: for $a\\in{\\mathcal{A}}$ do   \n4: # See Algorithm 2   \n5: $\\underline{{{\\tau}}}_{a},\\overline{{{\\tau}}}_{a}=\\overline{{{\\mathrm{Binary~}\\,\\mathrm{Search}}}}(a,\\lceil\\log_{2}T^{\\beta}\\rceil,\\lceil T^{\\alpha}\\rceil,0,1)$   \n6: end for   \n7: For any action $a\\in A$ , $\\hat{\\tau}_{a}=\\overline{{\\tau}}_{a}+1/T^{\\beta}+\\mathrm{C}T^{(\\kappa-1)/2}$ .   \n8: for $t=K\\lceil T^{\\alpha}\\rceil\\lceil\\log_{2}T^{\\beta}\\rceil+1,\\ldots,T$ do   \n9: Get recommended actions by Bandit-Alg on the $A\\times A$ bandit instance, $\\left(\\tilde{a}_{t},B_{t}\\right)\\;=\\;$   \nBandit-Alg(Ut, H\u02dctd\u2212ow1n   \n1110:: OOfbfseerr av et $A_{t}=\\Pi_{\\mathrm{p}}^{\\mathrm{u}\\mathrm{{\\bar{p}}}}(\\tilde{a}_{t+1},\\tau(t+1),V_{t},\\bar{\\mathcal{H}}_{t-1}^{\\mathrm{u}\\mathrm{{p},\\mathrm{p}}}),\\stackrel{\\cdot}{X}_{\\tilde{a}_{t},B_{t}}(t)$ $\\hat{\\tau}_{\\tilde{a}_{t}}$ $\\tilde{a}_{t}$ ion $a^{\\prime}\\in\\mathcal{A}$ and play action $B_{t}$ .   \n12: if $A_{t}=\\tilde{a}_{t}$ then update history $\\tilde{\\mathcal{H}}_{t}^{\\mathrm{down,p}}$ .   \n13: end if   \n14: Update upstream player\u2019s history $\\mathcal{H}_{t}^{\\mathrm{up,p}}$ .   \n15: end for ", "page_idx": 7}, {"type": "text", "text": "Algorithm 2 Binary Search Subroutine ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "1: Input: action $a,N_{T},\\tilde{T},\\underline{{\\tau}}_{a},\\overline{{\\tau}}_{a}$ .   \n2: for $d=0,\\dots,N_{T}-1$ do   \n3: Compute \u03c4 amid= $\\dot{\\tau}_{a}^{\\mathrm{mid}}=(\\overline{{\\tau}}_{a}(d)+\\underline{{\\tau}}_{a}(d))/2,T_{a}^{\\neq}=0.$ .   \n4: for $t=d\\tilde{T}+1,\\dots,d\\tilde{T}+\\tilde{T}$ do   \n5: Propose transfer $\\tau_{a}^{\\mathrm{mid}}(d)$ on arm $a$ and nothing for any other action $a^{\\prime}\\in\\mathcal{A}$ .   \n6: $A_{t}=\\Pi_{\\mathrm{p}}^{\\mathrm{up}}(t,\\tau^{\\mathrm{mid}}(a),a,V_{t},\\mathcal{H}_{t-1}^{\\mathrm{up,p}})$   \n7: if $A_{t}\\neq a$ then: $T^{\\neq}+=1$   \n8: end if   \n9: Update upstream player\u2019s history $\\mathcal{H}_{t}^{\\mathrm{up,p}}$ .   \n10: end for   \n11: if $\\mathrm{C}\\tilde{T}^{\\kappa+\\beta/\\alpha}<T_{a}^{\\neq}<\\tilde{T}-\\mathrm{C}\\tilde{T}^{\\kappa+\\beta/\\alpha}$ then Return $\\underline{{\\tau}}_{a}(d),\\overline{{\\tau}}_{a}(d)$ .   \n12: else if $T_{a}^{\\neq}\\leqslant\\tilde{T}-\\mathrm{C}\\tilde{T}^{\\kappa+\\beta/\\alpha}$ then $\\overline{{\\tau}}_{a}(d)=\\tau_{a}^{\\mathrm{mid}}(d)+1/T^{\\beta}$ and update history $\\tilde{\\mathcal{H}}_{t}^{\\mathrm{down,p}}$ .   \n13: else $\\underline{{\\tau}}_{a}(d)=\\tau_{a}^{\\mathrm{mid}}(d)-1/T^{\\beta}$ and update history $\\tilde{\\mathcal{H}}_{t}^{\\mathrm{down,p}}$ .   \n14: end if   \n15: end for ", "page_idx": 7}, {"type": "text", "text": "Theorem 3. Assume that $H I$ and $_{H2}$ hold. Then BELGIC, run with $\\alpha,\\beta$ satisfying (13) and any bandit subroutine Bandi $t\\!-\\!\\!A\\,\\!\\!l\\,g,$ , has an overall regret $\\Re_{\\mathrm{p}}^{\\mathrm{down}}$ such that ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{R}_{\\mathfrak{p}}^{\\mathrm{down}}(T,\\Pi_{\\mathrm{p}}^{\\mathfrak{u p}},\\mathtt{B E L G I C})\\leqslant2(3+2\\mathrm{C}+\\bar{v}-\\underline{{v}})\\log_{2}(T)(2T^{1-\\alpha\\zeta}+T^{(\\kappa+1)/2}+\\lceil T^{\\alpha}\\rceil)+4T^{1-\\beta}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\,\\mathfrak{R}_{B a n d i t\\cdot A l g}(T,\\nu,\\{\\hat{\\tau}_{a}\\}_{a\\in A})}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where, for ease of notation ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\bar{v}=\\operatorname*{max}_{a,b\\in A\\times A}\\{v^{\\mathrm{down}}(a,b)\\}\\quad a n d\\quad\\underline{{v}}=\\operatorname*{min}_{a,b\\in A\\times A}\\{v^{\\mathrm{down}}(a,b)\\}\\;.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Knowledge of C and $\\kappa$ . An upper bound on $\\mathrm{C}$ and $\\kappa$ is sufficient to compute the hyperparameters in BELGIC. Theorem 3 shows that the bigger $\\mathrm{C}$ and $\\kappa$ are, the worse is the downstream player\u2019s regret, hence the interest of knowing them more precisely. ", "page_idx": 7}, {"type": "text", "text": "Corollary 1. Assume that the upstream player\u2019s distribution $(\\gamma_{a})_{a\\in A}$ is such that H 1 holds. In addition, suppose that the distributions $(\\gamma_{a})_{a\\in A}$ and $(\\nu_{a,b})_{a,b\\in\\mathcal{A}\\times\\mathcal{A}}$ are 1-sub-Gaussian and that the upstream player plays $\\Pi_{\\mathrm{p}}^{\\mathrm{up}}=A l$ gorithm 3 ( $\\stackrel{\\mathrm{~\\,~}}{a}$ slight modification of UCB to take into account the incentives). Then the downstream player\u2019s regret when she runs BELGIC with parameters $\\alpha=3/4$ and $\\beta=1/4$ (which satisfy (13)) and subroutine Bandi $t{\\mathrm{-}}A\\,\\!\\mathrm{\\nabla}\\!g=\\mathtt{U C B}$ satisfies the following upper ", "page_idx": 7}, {"type": "text", "text": "bound3 ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Re_{\\mathrm{p}}^{\\mathrm{down}}(T,\\,U C B,\\mathrm{BELGIC})\\leqslant(10+4K+32\\sqrt{K\\log_{2}(K T^{3})}+\\bar{v}-\\underline{{v}})\\log_{2}(T)(3+2T^{3/4})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\,3K^{2}(\\bar{v}-\\underline{{v}})\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "The upper bound on the social welfare regret in Lemma 1 together with Corollary 1 shows that when the upstream player runs $\\Pi_{\\mathrm{p}}^{\\mathrm{up}}=\\mathtt{U C B}$ and the downstream player runs BELGIC, the social welfare regret then satisfies $\\begin{array}{r}{\\Re^{\\mathrm{sw}}(T,\\operatorname*{Im}_{\\mathbb{P}}^{\\mathrm{up}},\\mathtt{B E L G I C})=\\mathcal{O}(K\\log(T)T^{(\\kappa+1)/2}).}\\end{array}$ . ", "page_idx": 8}, {"type": "text", "text": "In other words, if the downstream player runs BELGIC which produces a policy $\\Pi_{\\mathrm{down}}^{\\mathrm{n}}$ , for any upstream policy $\\Pi_{\\mathrm{p}}^{\\mathrm{up}}$ , $(\\Pi_{\\mathrm{down}}^{\\mathrm{n}},\\Pi_{\\mathrm{p}}^{\\mathrm{up}})$ is welfare efficient. ", "page_idx": 8}, {"type": "text", "text": "Influence of the upstream performance. It is interesting to note that in the downstream player\u2019s regret bound, the upstream player\u2019s regret bound in $\\mathcal{O}(T^{\\kappa})$ plays a significant role: the downstream player never learns faster than the upstream player. The latter\u2019s performance determines the social welfare convergence rate towards the social optimum. We can observe that the players\u2019 bounded rationality [Selten, 1990, Jones, 1999] and personal interest make the game converge towards the optimal social welfare equilibrium\u2014even though they are both learning here. ", "page_idx": 8}, {"type": "image", "img_path": "omyzrkacme/tmp/b36991a497a4b3ac63722aef2d645385c3f6950161b1f5762a78b49aa489a744.jpg", "img_caption": ["Figure 1: Empirical frequencies of the upstream player\u2019s actions when property rights are not defined (left) and when they are defined (right). "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Experiments. We conclude this section with experiments showing the empirical convergence of our algorithm to a social optimum. In the simulation, we consider two firms, with firm 1 being upstream and firm 2 being downstream. Their profit functions are respectively given by ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\pi_{1}:\\mapsto\\operatorname*{max}\\biggl\\{q_{1}-2\\Bigl(\\frac{q_{1}}{10}\\Bigr)^{2}+2\\frac{q_{1}}{10},0\\biggr\\}\\ ,\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "and ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\pi_{2}(q_{1},q_{2})\\mapsto\\operatorname*{max}\\left\\{-16\\bigg(\\frac{q_{1}}{10}-\\frac{6}{10}\\bigg)^{2}+8\\bigg(q_{1}-\\frac{6}{10}\\bigg)^{2}+\\frac{1}{50}q_{2},0\\right\\}\\,.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Thus, firm 1\u2019s and firm 2\u2019s profit functions depends quadratically on $q_{1}$ with an firm 1 optimum at $q_{1}^{\\prime}=5$ and a social optimum at $q_{1}^{\\star}=8$ . Note that in the expression of $\\pi_{2}$ , $q_{2}$ has very little influence as compared to $q_{1}$ - which allows to plot profits for only one value of $q_{2}$ . ", "page_idx": 8}, {"type": "text", "text": "We discretize the setup, consider a bandit instance (horizon $T=5.10^{6}$ , 10 arms, average over 10 rounds) and we assume that UCB is used as a subroutine. In the first setting, there are no property rights and each firm runs UCB on their side. Second, property rights are defined and firm 2 runs BELGIC as its policy. The plots in Figure 1 display the empirical frequencies and show empirically the effectiveness of BELGIC to mitigate externalities. ", "page_idx": 8}, {"type": "text", "text": "4 Related work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Our work addresses the impact of externalities and is therefore related to taxation theory [see, e.g., Mirrlees et al., 2011, Salanie, 2011, and the references therein], a prominent solution for this issue, as exemplified by the Pigouvian tax [see Pigou, 2017]. Taxation is a fundamental aspect of all developed economies, with $30\\bar{\\%}$ to $50\\%$ of national income derived from taxes. The topic has been fruitful for various scenarios, including the carbon tax [Carattini et al., 2018, Metcalf and Weisbach, 2009], alcohol markets [Grifftih et al., 2019], or business taxation [Boadway and Bruce, 1984]. Taxation can also be studied through an operations research lens, where it is used to enhance system efficiency or manage specific games [Roughgarden, 2010, Caragiannis et al., 2010, Bil\u00f2 and Vinci, 2019]. Recent work by Cui et al. [2024] explores online mechanisms to maximize efficiency in congestion games. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Mechanism designs [Myerson, 1989, Nisan and Ronen, 1999, Laffont and Martimort, 2009] allow to design games that have specific desired outcomes. Deploying these mechanisms in their classical ecnomical form assume that players\u2019 utility functions are known a priori, which is often unrealistic. There is a major need to blend mechanism design with machine learning. ", "page_idx": 9}, {"type": "text", "text": "However, our approach differs, since, drawing inspiration from Coase\u2019s theory, we implement an online version of his theorem [Coase, 2013, Cooter, 1982], incorporating uncertainty to tackle the breakdown of social welfare in an online setting. We use the bandit setup [see Lattimore and Szepesv\u00e1ri, 2020, Slivkins et al., 2019] as a general and convenient way to model the game introduced by Coase. However, our work differs from considering a single agent playing a bandit game. Instead, we focus on the more general problem of multi-players bandits, a field receiving a growing attention from the community [see e.g., Boursier and Perchet, 2019, 2022, Sankararaman et al., 2019]. ", "page_idx": 9}, {"type": "text", "text": "Our approach is inspired by the principal-agent model introduced by Dogan et al. [2023b], which was further extended by Dogan et al. [2023a], Scheid et al. [2024]. However, unlike the models proposed in the work of Dogan et al. [2023b,a], we do not specify a particular bandit algorithm for the upstream player and instead, we allow him to use any no-regret algorithm satisfying $\\mathbf{H}2$ in a black-box fashion. Conversely, the model of Scheid et al. [2024] assumes that the upstream player is always fully informed and best-responding, whereas we assume that he is also learning. Chen et al. [2023b] leverages a similar model to study information acquisition by a principal through an agent\u2019s actions. However, in their model, the agent is also almighty and knows exactly the costs associated with each action. Designing incentives in an unknown environment is related to auction theory incorporating uncertainty, as it is explored in the work of Feng et al. [see 2018], Li et al. [see 2023]. Similar issues have been explored in the Reinforcement Learning framework within a leader-follower game [see Chen et al., 2023a, with quantal responses by the follower] or in a principal-agent game with incentive design as done by Ben-Porat et al. [2023]. Donahue et al. [2024] also study a two-players repeated Stackelberg game on a bandit instance but instead of allowing for transfers, their main focus concerns the achievability of a Stackelberg equilibrium through iterations of bandit policies: the same kind of goal also appears in Collina et al. [2023]. Such principal-agent setups are of some interest to model various real-world situations such as the design of fundings for hospitals [Wang et al., 2024] or have been studied with multiple agents through the lens of auction design in dynamic setups [Bergemann and Said, 2010, Chen et al., 2023c], or to account for fairness [Fallah et al., 2024]. ", "page_idx": 9}, {"type": "text", "text": "In our game, the downstream player needs to learn the optimal transfers/incentives to offer to the upstream player. This is related to the Incentivized Exploration literature [Mansour et al., 2016, Simchowitz and Slivkins, 2023, Esmaeili et al., 2023], which is often cast in terms of a benevolent planner who aims to optimize the global welfare of agents via plausible recommendations. A related model is Bayesian Persuasion [Kamenica and Gentzkow, 2011], where a sender influences a receiver\u2019s action through sending a signal. This model has begun to be studied in learning settings [see, e.g., Castiglioni et al., 2020, Bernasconi et al., 2022, Wu et al., 2022b,a]. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper studies a model of externalities in a two-players sequential game where both players learn their optimal actions. We first show that when the players act independently, then a misalignment between the players\u2019 interests and the social welfare leads to a breakdown of the global utility. We then introduce interactions through transfers, which restores a social welfare optimum, representing the online version of the Coase theorem. To that purpose, we propose a policy for the downstream player which allows her to estimate the optimal transfers as well as choosing the best actions. The mathematical difficulty comes from the learning aspect on both sides. Since our work is coined in a learning framework for mechanism design, several directions for research are open, as for instance extensions to the multi-agent setting, which raises many questions. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Funded by the European Union (ERC, Ocean, 101071601). Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or the European Research Council Executive Agency. Neither the European Union nor the granting authority can be held responsible for them. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Jens Abildtrup, Frank Jensen, and Alex Dubgaard. Does the coase theorem hold in real markets? an application to the negotiations between waterworks and farmers in denmark. Journal of environmental management, 93(1):169\u2013176, 2012.   \nNivasini Ananthakrishnan, Stephen Bates, Michael Jordan, and Nika Haghtalab. Delegating data collection in decentralized machine learning. In International Conference on Artificial Intelligence and Statistics, pages 478\u2013486. PMLR, 2024.   \nPeter Auer. Using confidence bounds for exploitation-exploration trade-offs. Journal of Machine Learning Research, 3(Nov):397\u2013422, 2002.   \nOmer Ben-Porat, Yishay Mansour, Michal Moshkovitz, and Boaz Taitler. Principal-agent reward shaping in mdps. arXiv preprint arXiv:2401.00298, 2023.   \nDirk Bergemann and Maher Said. Dynamic auctions: A survey. Wiley Encyclopedia of Operations Research and Management Science, 2010.   \nMartino Bernasconi, Matteo Castiglioni, Alberto Marchesi, Nicola Gatti, and Francesco Trov\u00f2. Sequential information design: Learning to persuade in the dark. Advances in Neural Information Processing Systems, 35:15917\u201315928, 2022.   \nVittorio Bil\u00f2 and Cosimo Vinci. Dynamic taxes for polynomial congestion games. ACM Transactions on Economics and Computation (TEAC), 7(3):1\u201336, 2019.   \nRobin Boadway and Neil Bruce. A general proposition on the design of a neutral business tax. Journal of Public Economics, 24(2):231\u2013239, 1984.   \nPatrick Bolton and Mathias Dewatripont. Contract theory. MIT press, 2004.   \nEtienne Boursier and Vianney Perchet. Sic-mmab: Synchronisation involves communication in multiplayer multi-armed bandits. Advances in Neural Information Processing Systems, 32, 2019.   \nEtienne Boursier and Vianney Perchet. A survey on multi-player bandits. arXiv preprint arXiv:2211.16275, 2022.   \nS\u00e9bastien Bubeck, Nicolo Cesa-Bianchi, et al. Regret analysis of stochastic and nonstochastic multi-armed bandit problems. Foundations and Trends\u00ae in Machine Learning, 5(1):1\u2013122, 2012.   \nJames M Buchanan and Wm Craig Stubblebine. Externality. In Inframarginal Contributions to Development Economics, pages 55\u201373. World Scientific, 2006.   \nIoannis Caragiannis, Christos Kaklamanis, and Panagiotis Kanellopoulos. Taxes for linear atomic congestion games. ACM Transactions on Algorithms (TALG), 7(1):1\u201331, 2010.   \nStefano Carattini, Maria Carvalho, and Sam Fankhauser. Overcoming public resistance to carbon taxes. Wiley Interdisciplinary Reviews: Climate Change, 9(5):e531, 2018.   \nMatteo Castiglioni, Andrea Celli, Alberto Marchesi, and Nicola Gatti. Online bayesian persuasion. Advances in Neural Information Processing Systems, 33:16188\u201316198, 2020.   \nSiyu Chen, Mengdi Wang, and Zhuoran Yang. Actions speak what you want: Provably sampleefficient reinforcement learning of the quantal stackelberg equilibrium from strategic feedbacks. arXiv preprint arXiv:2307.14085, 2023a. ", "page_idx": 10}, {"type": "text", "text": "pages 5194\u20135218. PMLR, 2023b. Yurong Chen, Qian Wang, Zhijian Duan, Haoran Sun, Zhaohua Chen, Xiang Yan, and Xiaotie Deng. Coordinated dynamic bidding in repeated second-price auctions with budgets. In International Conference on Machine Learning, pages 5052\u20135086. PMLR, 2023c. Ronald Coase. The problem of social cost. Journal of Law and Economics, 56(4):837 \u2013 877, 2013. URL https://EconPapers.repec.org/RePEc:ucp:jlawec:doi:10.1086/674872. Natalie Collina, Eshwar Ram Arunachaleswaran, and Michael Kearns. Efficient stackelberg strategies for finitely repeated games. In Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems, pages 643\u2013651, 2023. Robert Cooter. The cost of coase. The Journal of Legal Studies, 11(1):1\u201333, 1982. Qiwen Cui, Maryam Fazel, and Simon S Du. Learning optimal tax design in nonatomic congestion games. arXiv preprint arXiv:2402.07437, 2024. Carl J Dahlman. The problem of externality. The journal of law and economics, 22(1):141\u2013162, 1979. Ilgin Dogan, Zuo-Jun Max Shen, and Anil Aswani. Estimating and incentivizing imperfect-knowledge agents with hidden rewards. arXiv preprint arXiv:2308.06717, 2023a. Ilgin Dogan, Zuo-Jun Max Shen, and Anil Aswani. Repeated principal-agent games with unobserved agent rewards and perfect-knowledge agents. arXiv preprint arXiv:2304.07407, 2023b. Kate Donahue, Nicole Immorlica, Meena Jagadeesan, Brendan Lucier, and Aleksandrs Slivkins. Impact of decentralized learning on player utilities in stackelberg games. arXiv preprint arXiv:2403.00188, 2024. Paul D\u00fctting, Tim Roughgarden, and Inbal Talgam-Cohen. Simple versus optimal contracts. In Proceedings of the 2019 ACM Conference on Economics and Computation, pages 369\u2013387, 2019. Seyed A Esmaeili, Suho Shin, and Aleksandrs Slivkins. Robust and performance incentivizing algorithms for multi-armed bandits with strategic agents. arXiv preprint arXiv:2312.07929, 2023. Alireza Fallah and Michael I Jordan. Contract design with safety inspections. arXiv preprint arXiv:2311.02537, 2023. Alireza Fallah, Michael I Jordan, and Annie Ulichney. Fair allocation in dynamic mechanism design. arXiv preprint arXiv:2406.00147, 2024. Zhe Feng, Chara Podimata, and Vasilis Syrgkanis. Learning to bid without knowing your value. In Proceedings of the 2018 ACM Conference on Economics and Computation, pages 505\u2013522, 2018. Thomas K Greenfield, Yu Ye, William Kerr, Jason Bond, J\u00fcrgen Rehm, and Norman Giesbrecht. Externalities from alcohol consumption in the 2005 us national alcohol survey: implications for policy. International journal of environmental research and public health, 6(12):3205\u20133224, 2009. Rachel Griffith, Martin O\u2019Connell, and Kate Smith. Tax design in the alcohol market. Journal of public economics, 172:20\u201335, 2019. Guru Guruganesh, Jon Schneider, and Joshua R Wang. Contracts under moral hazard and adverse selection. In Proceedings of the 22nd ACM Conference on Economics and Computation, pages 563\u2013582, 2021. Guru Guruganesh, Yoav Kolumbus, Jon Schneider, Inbal Talgam-Cohen, Emmanouil-Vasileios Vlatakis-Gkaragkounis, Joshua R Wang, and S Matthew Weinberg. Contracting with a learning agent. arXiv preprint arXiv:2401.16198, 2024. Bryan D Jones. Bounded rationality. Annual review of political science, 2(1):297\u2013321, 1999. ", "page_idx": 11}, {"type": "text", "text": "Emir Kamenica and Matthew Gentzkow. Bayesian persuasion. American Economic Review, 101(6): 2590\u20132615, 2011. ", "page_idx": 12}, {"type": "text", "text": "Jean-Jacques Laffont and David Martimort. The theory of incentives: the principal-agent model. In The theory of incentives. Princeton university press, 2009. ", "page_idx": 12}, {"type": "text", "text": "John Langford and Tong Zhang. The epoch-greedy algorithm for multi-armed bandits with side information. Advances in neural information processing systems, 20, 2007. ", "page_idx": 12}, {"type": "text", "text": "Tor Lattimore and Csaba Szepesv\u00e1ri. Bandit algorithms. Cambridge University Press, 2020. ", "page_idx": 12}, {"type": "text", "text": "Ningyuan Li, Yunxuan Ma, Yang Zhao, Zhijian Duan, Yurong Chen, Zhilin Zhang, Jian Xu, Bo Zheng, and Xiaotie Deng. Learning-based ad auction design with externalities: the framework and a matching-based approach. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 1291\u20131302, 2023. ", "page_idx": 12}, {"type": "text", "text": "Yishay Mansour, Aleksandrs Slivkins, Vasilis Syrgkanis, and Zhiwei Steven Wu. Bayesian exploration: Incentivizing exploration in bayesian games. arXiv preprint arXiv:1602.07570, 2016. ", "page_idx": 12}, {"type": "text", "text": "Andreu Mas-Colell, Michael D. Whinston, and Jerry R. Green. Microeconomic Theory. Oxford University Press, New York, 1995. ", "page_idx": 12}, {"type": "text", "text": "Gillbert E Metcalf and David Weisbach. The design of a carbon tax. Harv. Envtl. L. Rev., 33:499, 2009. ", "page_idx": 12}, {"type": "text", "text": "James Mirrlees et al. Tax by design: The Mirrlees review. OUP Oxford, 2011. ", "page_idx": 12}, {"type": "text", "text": "Roger B Myerson. Mechanism design. Springer, 1989. ", "page_idx": 12}, {"type": "text", "text": "Noam Nisan and Amir Ronen. Algorithmic mechanism design. In Proceedings of the thirty-first annual ACM symposium on Theory of computing, pages 129\u2013140, 1999. ", "page_idx": 12}, {"type": "text", "text": "Vianney Perchet, Philippe Rigollet, Sylvain Chassang, and Erik Snowberg. Batched bandit problems. The Annals of Statistics, 44:660\u2013681, 2016. ", "page_idx": 12}, {"type": "text", "text": "Arthur Pigou. The economics of welfare. Routledge, 2017. ", "page_idx": 12}, {"type": "text", "text": "Herbert Robbins. Some aspects of the sequential design of experiments. Bulletin of the American Mathematical Society, page 527\u2013535, 1952. ", "page_idx": 12}, {"type": "text", "text": "Tim Roughgarden. Algorithmic game theory. Communications of the ACM, 53(7):78\u201386, 2010. ", "page_idx": 12}, {"type": "text", "text": "Bernard Salani\u00e9. The economics of contracts: a primer. MIT press, 2005. ", "page_idx": 12}, {"type": "text", "text": "Bernard Salanie. The economics of taxation. MIT press, 2011. ", "page_idx": 12}, {"type": "text", "text": "Abishek Sankararaman, Ayalvadi Ganesh, and Sanjay Shakkottai. Social learning in multi agent multi armed bandits. Proceedings of the ACM on Measurement and Analysis of Computing Systems, 3 (3):1\u201335, 2019. ", "page_idx": 12}, {"type": "text", "text": "Antoine Scheid, Daniil Tiapkin, Etienne Boursier, Aymeric Capitaine, El Mahdi El Mhamdi, \u00c9ric Moulines, Michael I Jordan, and Alain Durmus. Incentivized learning in principal-agent bandit games. ICML, 2024. ", "page_idx": 12}, {"type": "text", "text": "Reinhard Selten. Bounded rationality. Journal of Institutional and Theoretical Economics (JITE)/Zeitschrift f\u00fcr die gesamte Staatswissenschaft, 146(4):649\u2013658, 1990. ", "page_idx": 12}, {"type": "text", "text": "Virag Shah, Jose Blanchet, and Ramesh Johari. Bandit learning with positive externalities. Advances in Neural Information Processing Systems, 31, 2018. ", "page_idx": 12}, {"type": "text", "text": "Max Simchowitz and Aleksandrs Slivkins. Exploration and incentives in reinforcement learning. Operations Research, 2023. ", "page_idx": 12}, {"type": "text", "text": "Aleksandrs Slivkins et al. Introduction to multi-armed bandits. Foundations and Trends\u00ae in Machine Learning, 12(1-2):1\u2013286, 2019. ", "page_idx": 12}, {"type": "text", "text": "Heinrich Von Stackelberg. Market structure and equilibrium. Springer Science & Business Media, 2010. ", "page_idx": 13}, {"type": "text", "text": "Serena Wang, Stephen Bates, P Aronow, and Michael Jordan. On counterfactual metrics for social welfare: Incentives, ranking, and information asymmetry. In International Conference on Artificial Intelligence and Statistics, pages 1522\u20131530. PMLR, 2024.   \nJibang Wu, Zixuan Zhang, Zhe Feng, Zhaoran Wang, Zhuoran Yang, Michael I Jordan, and Haifeng Xu. Markov persuasion processes and reinforcement learning. In ACM Conference on Economics and Computation, 2022a.   \nJibang Wu, Zixuan Zhang, Zhe Feng, Zhaoran Wang, Zhuoran Yang, Michael I Jordan, and Haifeng Xu. Sequential information design: Markov persuasion process and its efficient reinforcement learning. arXiv preprint arXiv:2202.10678, 2022b.   \nBanghua Zhu, Stephen Bates, Zhuoran Yang, Yixin Wang, Jiantao Jiao, and Michael I Jordan. The sample complexity of online contract design. arXiv preprint arXiv:2211.05732, 2022. ", "page_idx": 13}, {"type": "text", "text": "A Algorithmic Subroutine for the Binary Search ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Optimal Transfer. For any given round $t\\geqslant1$ , action $a\\in A$ and $\\varepsilon>0$ , the downstream player can incentivize any best-responding upstream player to choose $a$ by offering a transfer, $\\tau_{a}^{\\star,\\varepsilon}\\in\\mathbb{R}_{+}$ , defined as: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\tau_{a}^{\\star,\\varepsilon}=\\operatorname*{max}_{a^{\\prime}\\in A}{v^{\\mathrm{up}}(a^{\\prime})}-{v^{\\mathrm{up}}(a)}+\\varepsilon\\;.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "With this transfer, it holds that for any $a^{\\prime}\\in\\mathcal{A},a^{\\prime}\\ne a$ , we have $v^{\\mathrm{up}}(a^{\\prime})<v^{\\mathrm{up}}(a)+\\tau_{a}^{\\star,\\varepsilon}$ , ensuring the upstream player\u2019s action $A_{t}=a$ , since action $a$ yields a superior reward. Consequently, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\tau_{a}^{\\star}=\\operatorname*{lim}_{\\varepsilon\\rightarrow0}\\tau_{a}^{\\star,\\varepsilon}=\\operatorname*{max}_{a^{\\prime}\\in\\mathcal{A}}v^{\\mathrm{up}}(a^{\\prime})-v^{\\mathrm{up}}(a)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "represents the infimal transfer necessary to make arm $a$ the best upstream player\u2019s choice. ", "page_idx": 14}, {"type": "text", "text": "First step of BELGIC: estimation of the optimal transfers. Suppose that we consider an arm $a\\in{\\mathcal{A}}$ and that the downstream player offers an incentive $\\tau_{a}$ to the upstream player if he picks this arm. We consider this procedure with a constant incentive $\\tau_{a}$ for a batch of time steps of length $\\tilde{T}=\\lceil T^{\\alpha}\\rceil$ due to the fact that the upstream player is learning [see Perchet et al., 2016, for batched bandits in the usual multi-armed setting]. Lemma 2 shows that for BELGIC to accurately estimate $\\tau_{a}^{\\star}$ with high probability, it must hold ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathrm{C}\\tilde{T}^{\\kappa+\\beta/\\alpha}<\\tilde{T}/2\\;,\\;\\;\\mathrm{which\\;is\\;equivalent}\\;\\mathrm{to}\\;\\;\\mathrm{C}T^{\\kappa\\alpha+\\beta-\\alpha}<1/2\\;,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "This is why we impose the condition (13) on $\\alpha$ and $\\beta$ , namely $\\beta/\\alpha<1-\\kappa$ , which ensures that $\\kappa(\\alpha-1)+\\beta<0$ and therefore $\\mathrm{lim}_{T\\rightarrow+\\infty}\\,T^{\\kappa(\\alpha-1)+\\beta}=0$ . More precisely, we define for $a\\in[K]$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\Lambda_{a}=(a-1)\\lceil\\log_{2}T^{\\beta}\\rceil\\tilde{T}\\ ,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which is the step after which starts the binary search procedure on arm $a$ . For any $d\\ \\in$ $\\{1,\\dots,\\lceil\\log_{2}T^{\\beta}\\rceil\\}$ on arm $a$ , we define ", "page_idx": 14}, {"type": "equation", "text": "$$\nk_{a,d}=\\Lambda_{a}+(d-1)\\tilde{T}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "as the step after which starts the $d$ -th batch iteration on arm $a$ , and we consider ", "page_idx": 14}, {"type": "equation", "text": "$$\nT_{a,d}^{\\neq}=\\mathrm{Card}\\left\\{t\\in\\{k_{d,a}+1,\\ldots,k_{d,a}+\\tilde{T}\\}\\ \\mathrm{such}\\,\\mathrm{that}\\,A_{t}\\neq a\\right\\}\\,,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\left(A_{t}\\right)_{t\\in\\{1,\\dots,K\\lceil\\log_{2}T^{\\beta}\\rceil\\tilde{T}\\}}$ is given by Algorithm 2. Lemma 2 shows that for any $a\\in A,d\\in$ $\\{1,\\ldots,\\lceil\\log_{2}T^{\\beta}\\rceil\\}$ , with high probability ", "page_idx": 14}, {"type": "equation", "text": "$$\n:T_{d,a}^{\\neq}<\\tilde{T}-\\mathrm{C}\\tilde{T}^{\\kappa+\\beta/\\alpha}\\;\\mathrm{and}\\;T_{d,a}^{\\neq}>\\mathrm{C}\\tilde{T}^{\\kappa+\\beta/\\alpha},\\;\\mathrm{then}\\;|\\tau_{a}^{\\star}-\\hat{\\tau}_{d,a}|\\leqslant1/T^{\\beta}\\;,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\hat{\\tau}_{a,d}$ is the current estimate of $\\tau_{a}^{\\star}$ offered for iteration $k_{a,d}$ . In case (18) does not hold, it means that the upstream player has misplayed and has chosen most of the steps a suboptimal action, leading to an instantaneous regret for him larger than the bound given in $\\mathbf{H}2$ . This is why (19) holds with high probability. To sum up, the first phase of BELGIC consists in $\\lceil\\log_{2}T^{\\beta}\\rceil$ batches of binary search on each arm $a\\in A$ to obtain a precision level $1/T^{\\beta}$ on the optimal transfer $\\tau_{a}^{\\star}$ . ", "page_idx": 14}, {"type": "text", "text": "During this phase in Algorithm 2, we define $\\overline{{\\tau}}_{a}(d)\\in\\mathbb{R}_{+}$ as the upper estimate and $\\underline{{\\tau}}_{a}(d)\\in\\mathbb{R}_{+}$ as the lower estimate of $\\tau_{a}^{\\star}$ after $d\\in\\{1,\\dots,\\lceil\\log_{2}T^{\\beta}\\rceil\\}$ rounds of binary search on arm $a$ . For any $t\\in[T]$ and $a\\in A$ , we define $\\tau_{a}^{\\mathrm{mid}}(t)=(\\overline{{\\tau}}_{a}(t)+\\underline{{\\tau}}_{a}(t))/2.\\ \\overline{{\\tau}}_{a}(d),\\underline{{\\tau}}_{a}(d),\\tau_{a}^{\\mathrm{mid}}(d)$ are updated at the end of the $d$ -th binary search batch of length $\\tilde{T}$ on arm $a$ . We define $N_{T}=\\lceil\\log_{2}T^{\\beta}\\rceil$ as the number of binary search steps per arm. ", "page_idx": 14}, {"type": "text", "text": "After this first binary search phase, the downstream player computes estimates of the optimal transfers $\\tau_{a}^{\\star}$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n(\\hat{\\tau}_{a})_{a\\in A}=(\\overline{{\\tau}}_{a}(\\lceil\\log_{2}T^{\\beta}\\rceil)+1/T^{\\beta}+\\mathrm{C}T^{(\\kappa-1)/2})_{a\\in A}\\ ,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and offers these transfers $(\\tau_{a}^{\\star})_{a\\in A}$ to make the upstream player play any action $\\tilde{a}\\in A$ she wants. ", "page_idx": 14}, {"type": "text", "text": "Second Step. After the first phase during which the optimal incentives are estimated by the downstream player through $\\left(\\hat{\\tau}_{a}\\right)_{a\\in A}$ , she runs in the second phase the subroutine Bandit-Alg on the $A{\\times}A$ bandit instance driven by her action $B_{t}$ and the upstream player\u2019s one $A_{t}$ . More precisely, any bandit subroutine Bandit $-\\mathtt{A}\\mathtt{l}\\mathtt{g}$ , such as UCB or $\\varepsilon$ -greedy, for instance, can be run in a black-box fashion on the shifted bandit instance, where rewards are shifted by the upper estimated transfers $\\left(\\hat{\\tau}_{a}\\right)_{a\\in A}$ . The downstream player computes a shifted history H\u02dctdown,p= \u2205for any t \u2a7dK\u2308T \u03b1\u2309\u2308log2 T \u03b2\u2309and for any $t>K\\lceil T^{\\alpha}\\rceil\\lceil\\log_{2}T^{\\beta}\\rceil$ ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "equation", "text": "$$\n\\tilde{\\mathcal{H}}_{t}^{\\mathrm{down,p}}=\\left\\{\\begin{array}{r l}&{\\{\\tilde{a}_{t},B_{t},\\tau(t),U_{t},X_{\\tilde{a}_{t},B_{t}}(t)-\\hat{\\tau}_{\\tilde{a}_{t}}\\}\\cup\\tilde{\\mathcal{H}}_{t-1}^{\\mathrm{down,p}}\\,\\mathrm{if}\\;\\tilde{a}_{t}=A_{t}}\\\\ &{\\tilde{\\mathcal{H}}_{t-1}^{\\mathrm{down,p}}\\quad\\mathrm{~otherwise}\\;.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which serves to feed Bandit-Alg, following Bandit-Alg: $(U_{t},\\tilde{\\mathcal{H}}_{t-1}^{\\mathrm{down,p}})\\mapsto(\\tilde{a}_{t},B_{t})\\in\\mathcal{A}\\times\\mathcal{A}.$ Note that here, Bandit-Alg aims to maximize the shifted reward $(\\nu^{\\mathrm{down}}(a,b)-\\hat{\\tau}_{a})_{(a,b)\\in\\mathcal{A}^{2}}$ . ", "page_idx": 15}, {"type": "text", "text": "Based on this decision by Bandit-Alg, $\\Pi_{\\mathrm{p}}^{\\mathrm{down}}$ offers the incentive $\\hat{\\tau}_{\\tilde{a}_{t}}$ associated with action $\\tilde{a}_{t}$ and plays action $B_{t}$ . Lemma 5 ensures that $\\tilde{a}_{t}$ is the upstream player\u2019s best choice. Therefore, $\\mathbf{H}2$ ensures that the upstream player will not deviate from the downstream player\u2019s recommendation with high probability. ", "page_idx": 15}, {"type": "text", "text": "B Invariance when the property rights are given to the downstream player ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Our focus in the paper was the case where the upstream player possesses the bandit instance and receives monetary payments. We argue here that the symmetric situation, i.e. when the property rights are given to the downstream player, can be analysed in the exact same way. ", "page_idx": 15}, {"type": "text", "text": "Assume that the the downstream player owns the bandit instance. This implies that (i) they can prescribe what arm the upstream player has to play at each round, and (ii) the upstream player may perform a monetary transfer to influence the arm they are allowed to pull. ", "page_idx": 15}, {"type": "text", "text": "Consider the same bandit setup as before. Formally, the downstream player\u2019s action is $(A_{t},B_{t})\\in$ $A\\times A$ where $A_{t}$ is the arm that the upstream player is prescribed to pull (he cannot deviate since the downstream player has the property rights), while $B_{t}$ is the arm played by the downstream player. On the other hand, the upstream player\u2019s policy outputs at each round the action $(\\tilde{a}_{t},\\tau(t))\\in\\bar{\\mathcal{A}}\\times\\mathbb{R}_{+}$ , where $\\tilde{a}_{t}$ is the arm they choose to incentivize and $\\tau(t)$ is the amount of transfer. It means that the downstream player receives a transfer $\\tau(t)$ if she prescribes action $A_{t}=\\tilde{a}_{t}$ . As a consequence, the instantaneous utility of the upstream player is $Z_{A_{t}}-\\mathbb{1}_{\\tilde{a}_{t}}(A_{t})\\tau(t)$ , while the downstream player receives $X_{A_{t},B_{t}}+\\dot{\\mathbb{1}}_{\\tilde{a}_{t}}(A_{t})\\tau(\\dot{t})$ . In that case, the upstream player may perform a binary search on each arm $\\bar{a}\\in A$ to identify the optimal incentive $\\tau_{\\bar{a}}^{\\star}$ , by considering Card $\\{t\\in[T]$ such that $\\tilde{a}_{t}=\\bar{a}\\}$ during batches designed for the binary search and then play on the shifted bandit instance as we explained. This situation and the upstream player\u2019s strategy are now equivalent to the one presented before. ", "page_idx": 15}, {"type": "text", "text": "C Proofs and Technical Results ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Recall that we defined the shifted history $\\tilde{\\mathcal{H}}_{t}^{\\mathrm{down,p}}$ that will serve to feed $\\Pi_{\\mathrm{p}}^{\\mathrm{down}}$ at time $t$ as $\\tilde{\\mathcal{H}}_{t}^{\\mathrm{down,p}}=$ $(\\tilde{a}_{s},\\tau(s),A_{s},V_{s},Z_{A_{s}}(s))_{s\\leqslant t}$ . ", "page_idx": 15}, {"type": "text", "text": "Theorem 4. Suppose that $\\operatorname{\\mathrm{urgmax}}_{a\\in A}v^{\\operatorname{up}}(a)$ is the singleton $\\{a_{\\star}^{\\mathrm{u}}\\}$ and that ", "page_idx": 15}, {"type": "equation", "text": "$$\nv^{\\mathrm{up}}(a^{\\mathrm{sw}})+v^{\\mathrm{down}}(a^{\\mathrm{sw}},b^{\\mathrm{sw}})-v^{\\mathrm{up}}(a_{\\star}^{\\mathrm{u}})+v^{\\mathrm{down}}(a_{\\star}^{\\mathrm{u}},b)>0\\ ,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "for any $b\\;\\in\\;{\\mathcal{A}}.$ In the absence of property rights and when the upstream player runs any noregret policy $\\Pi_{\\mathrm{n}}^{\\mathrm{up}}$ , we have Rsw(T $\\mathrm{\\Delta}\\Gamma_{\\mathrm{n}}^{\\mathrm{up}}\\mathrm{,}\\mathrm{\\overline{{\\Pi}}_{n}^{\\mathrm{down}}}\\mathrm{)}^{\\circ}\\mathrm{\\Delta}T\\Delta^{\\mathrm{sw}}\\mathrm{-}\\Re_{\\mathrm{n}}^{\\mathrm{up}}(T,\\mathrm{\\bar{\\Pi}_{n}^{\\mathrm{up}}})\\Delta^{\\mathrm{sw}}\\mathrm{/}\\Delta^{\\mathrm{up}}$ , where $\\Delta^{\\mathrm{up}}=$ $\\begin{array}{r}{\\operatorname*{min}_{a^{\\prime}\\in{\\mathcal A}\\setminus\\{a_{\\star}^{\\mathrm{u}}\\}}v^{\\mathrm{up}}(a_{\\star}^{\\mathrm{u}})-v^{\\mathrm{up}}(a^{\\prime})}\\end{array}$ and $\\Delta^{\\mathrm{sw}}\\,=\\,v^{\\mathrm{up}}(a^{\\mathrm{sw}})+v^{\\mathrm{down}}(a^{\\mathrm{sw}},b^{\\mathrm{sw}})\\mathrm{\\,-\\,}\\mathrm{max}_{b\\in\\mathrm{~}}$ A(vup(a\u22c6u) + $v^{\\mathrm{down}}(a_{\\star}^{\\mathrm{u}},b))$ . Therefore, $\\mathfrak{R}^{\\mathrm{sw}}(T,\\Pi_{\\mathrm{n}}^{\\mathrm{up}},\\Pi_{\\mathrm{n}}^{\\mathrm{down}})=\\Omega(T)$ and $(\\Pi_{\\mathrm{n}}^{\\mathrm{up}},\\Pi_{\\mathrm{n}}^{\\mathrm{down}})$ is not welfare efficient. ", "page_idx": 15}, {"type": "text", "text": "Proof of Theorem 4. Since $\\operatorname{argmax}_{a\\in A}v^{\\mathrm{up}}(a)$ is the singleton $\\{a_{\\star}^{\\mathrm{up}}\\}$ , we define $\\begin{array}{r l}{\\Delta^{\\mathrm{up}}}&{{}=}\\end{array}$ $\\begin{array}{r}{\\operatorname*{min}_{a^{\\prime}\\in{\\mathcal A}\\setminus\\{a_{\\star}^{\\mathrm{u}}\\}}v^{\\mathrm{up}}(a_{\\star}^{\\mathrm{u}})\\,-\\,v^{\\mathrm{up}}(a^{\\prime})}\\end{array}$ as the upstream player reward gap and $\\Delta^{\\mathrm{sw}}~=~v^{\\mathrm{up}}(a^{\\mathrm{sw}})~+$ $\\begin{array}{r}{v^{\\mathrm{down}}(a^{\\mathrm{sw}},b^{\\mathrm{sw}})\\mathrm{~-~}\\mathrm{max}_{b\\in\\mathcal{A}}(v^{\\mathrm{up}}(a_{\\star}^{\\mathrm{u}})+v^{\\mathrm{down}}(a_{\\star}^{\\mathrm{u}},b))}\\end{array}$ as the social welfare reward gap if the upstream player plays his most preferred action. ", "page_idx": 15}, {"type": "text", "text": "Denote $N_{\\star}^{\\mathrm{up}}(T)$ the number of pulls of the upstream player up to time $T$ on the arm $a_{\\star}^{\\mathrm{u}}$ . By definition of $\\Delta^{\\mathrm{up}}$ , we have that for any step $t\\,\\in\\,[T]$ such that $\\bar{A}_{t}\\neq a_{\\star}^{\\mathrm{up}}$ , $\\begin{array}{r}{\\operatorname*{max}_{a\\in{\\mathcal{A}}}\\{v^{\\mathrm{up}}(a)\\}-v^{\\mathrm{up}}(A_{t})=}\\end{array}$ $\\begin{array}{r}{v^{\\mathrm{up}}(a_{\\star}^{\\mathrm{up}})-v^{\\mathrm{up}}(A_{t})\\geqslant\\operatorname*{min}_{a^{\\prime}\\in A\\setminus\\{a_{\\star}^{\\mathrm{u}}\\}}v^{\\mathrm{up}}(a_{\\star}^{\\mathrm{u}})-v^{\\mathrm{up}}(a^{\\prime})=\\Delta^{\\mathrm{up}}}\\end{array}$ . There are $T-N_{\\star}^{\\mathrm{up}}(T)$ such steps, which leads to ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Re_{\\mathrm{n}}^{\\mathrm{up}}(T,\\Pi_{\\mathrm{n}}^{\\mathrm{up}},\\Pi_{\\mathrm{p}}^{\\mathrm{down}})\\geqslant\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T}\\operatorname*{max}_{a\\in A}\\{v^{\\mathrm{up}}(a)\\}-v^{\\mathrm{up}}(A_{t})\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\geqslant(T-\\mathbb{E}[N_{\\star}^{\\mathrm{u}}(T)])\\Delta^{\\mathrm{up}}\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and we obtain ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[N_{\\star}^{\\mathrm{u}}(T)]\\geqslant T-\\Re_{\\mathrm{n}}^{\\mathrm{up}}(T,\\Pi_{\\mathrm{n}}^{\\mathrm{up}})/\\Delta^{\\mathrm{up}}\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Moreover, for any $t\\,\\in\\,[T]$ such that $A_{t}\\,=\\,a_{\\star}^{\\mathrm{up}}$ and any $B_{t}\\in\\mathcal{A},\\,v^{\\mathrm{up}}(a^{\\mathrm{sw}})+v^{\\mathrm{down}}(a^{\\mathrm{sw}},b^{\\mathrm{sw}})\\,-$ $(v^{\\mathrm{up}}(A_{t})+v^{\\mathrm{down}}(A_{t},B_{t}))=v^{\\mathrm{up}}(a^{\\mathrm{sw}})+v^{\\mathrm{down}}(a^{\\mathrm{sw}},b^{\\mathrm{sw}})-(v^{\\mathrm{up}}(a_{\\star}^{\\mathrm{up}})+v^{\\mathrm{down}}(a^{\\mathrm{up}},B_{t}))$ by definition, which leads to ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\Re^{\\mathrm{sw}}(T,\\Pi_{\\mathrm{n}}^{\\mathrm{up}},\\Pi_{\\mathrm{n}}^{\\mathrm{down}})\\geqslant\\mathbb{E}[N_{\\star}^{\\mathrm{u}}(T)]\\Delta^{\\mathrm{sw}}\\ ,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and we obtain ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\Re^{\\mathrm{sw}}(T,\\Pi_{\\mathrm{n}}^{\\mathrm{up}},\\Pi_{\\mathrm{n}}^{\\mathrm{down}})\\geqslant T\\Delta^{\\mathrm{sw}}-\\Re_{\\mathrm{n}}^{\\mathrm{up}}(T,\\Pi_{\\mathrm{n}}^{\\mathrm{up}})\\Delta^{\\mathrm{sw}}/\\Delta^{\\mathrm{up}}\\ .\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Since $\\begin{array}{r}{\\operatorname*{lim}_{T\\rightarrow+\\infty}\\Re_{\\mathrm{n}}^{\\mathrm{up}}(T,\\Pi_{\\mathrm{n}}^{\\mathrm{up}})/T\\,=\\,0}\\end{array}$ , we have $\\begin{array}{r}{\\operatorname*{lim}_{T\\to+\\infty}\\mathfrak{R}^{\\mathrm{sw}}(T,\\Pi_{\\mathfrak{n}}^{\\mathrm{up}},\\Pi_{\\mathfrak{n}}^{\\mathrm{down}})/T=\\Delta^{\\mathrm{sw}}>0,}\\end{array}$ hence the result. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "The proof of Theorem 2 is an immediate consequence of Theorem 4. ", "page_idx": 16}, {"type": "text", "text": "Lemma 1. Recall that $\\mu^{\\star,\\mathrm{down}}$ is the downstream player\u2019s optimal reward as defined as a solution of (10). We have $\\begin{array}{r}{\\mu^{\\star,\\mathrm{down}}=\\operatorname*{max}_{a,b\\in\\mathcal{A}}\\{v^{\\mathrm{down}}(a,b)+v^{\\mathrm{up}}(a)\\}-\\operatorname*{max}_{a^{\\prime}\\in\\mathcal{A}}\\{v^{\\mathrm{up}}(a^{\\prime})\\},}\\end{array}$ , as well as $(a^{\\mathrm{opt}},b^{\\mathrm{opt}})=(a^{\\mathrm{sw}},b^{\\mathrm{sw}})$ and $\\begin{array}{r}{\\mu^{\\star,\\mathrm{up}}+\\mu^{\\star,\\mathrm{down}}=v^{\\mathrm{up}}(a^{\\mathrm{sw}})+v^{\\mathrm{down}}(a^{\\mathrm{sw}},b^{\\mathrm{sw}})=\\operatorname*{max}_{a,b\\in\\mathcal{A}}\\{v^{\\mathrm{up}}(a)+\\mu^{\\star,\\mathrm{down}}(a^{\\mathrm{sw}},b^{\\mathrm{sw}})\\}.}\\end{array}$ $\\boldsymbol{v}^{\\mathrm{down}}(a,b)\\}$ , where $\\mu^{\\star,\\mathrm{up}}$ is defined in Equation (1). Moreover, for any integer $T\\in\\mathbb{N}^{\\star}$ , and policies $\\Pi_{\\mathrm{p}}^{\\mathrm{up}},\\,\\Pi_{\\mathrm{p}}^{\\mathrm{down}}$ , we have that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathfrak{R}^{\\mathrm{sw}}(T,\\Pi_{\\mathrm{p}}^{\\mathrm{up}},\\Pi_{\\mathrm{p}}^{\\mathrm{down}})\\leqslant\\mathfrak{R}_{\\mathrm{p}}^{\\mathrm{up}}(T,\\Pi_{\\mathrm{p}}^{\\mathrm{up}},\\Pi_{\\mathrm{p}}^{\\mathrm{down}})+\\mathfrak{R}_{\\mathrm{p}}^{\\mathrm{down}}(T,\\Pi_{\\mathrm{p}}^{\\mathrm{up}},\\Pi_{\\mathrm{p}}^{\\mathrm{down}})\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof of Lemma $^{\\,l}$ . Recall that $\\mu^{\\star,\\mathrm{down}}$ is defined as $\\mu^{\\star,\\mathrm{down}}~=~\\mathrm{sup}_{a,b\\in{\\mathcal A}^{2},\\tau\\in{\\mathbb R}_{+}}\\{v^{\\mathrm{down}}(a,b)~-~$ $\\tau\\}$ , such that $a\\in\\mathrm{argmax}_{a^{\\prime}\\in A}\\{v^{\\mathrm{up}}(a^{\\prime})+\\mathbb{1}_{a}(a^{\\prime})\\tau\\}$ and $a_{\\star}^{\\mathrm{up}}=\\operatorname*{argmax}_{a^{\\prime}\\in\\mathcal{A}}v^{\\mathrm{up}}(a)$ . Note that we can write ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mu^{\\star,\\mathrm{down}}=\\operatorname*{max}\\{\\operatorname*{sup}_{a,b\\in\\mathcal{A}^{2},\\tau\\in\\mathbb{R}_{+}}\\mathbb{1}_{\\tilde{\\mathsf{A}}}(a,\\tau)(v^{\\mathrm{down}}(a,b)-\\tau),\\operatorname*{max}_{b\\in\\mathcal{A}}v^{\\mathrm{down}}(a_{\\star}^{\\mathrm{up}},b)\\}\\ ,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\tilde{\\mathsf{A}}\\,=\\,\\{(a,\\tau)\\colon v^{\\mathrm{up}}(a)+\\tau\\,\\geqslant\\,\\operatorname*{max}_{a^{\\prime}}v^{\\mathrm{up}}(a^{\\prime})+\\mathbb{1}_{a}(a^{\\prime})\\tau\\}$ which is the set of pairs $(a,\\tau)\\in$ $A\\times\\mathbb{R}_{+}$ such that the constraint binds. However, we also have by definition that $v^{\\mathrm{up}}(a_{\\star}^{\\mathrm{up}})+0\\geqslant$ $\\begin{array}{r}{\\operatorname*{max}_{a^{\\prime}\\in\\mathcal{A}}v^{\\mathrm{up}}(a^{\\prime})+\\mathbb{1}_{a_{\\star}^{\\mathrm{up}}}(a^{\\prime})\\cdot0}\\end{array}$ and hence, $(a_{\\star}^{\\mathrm{up}},0)\\in\\tilde{\\mathsf{A}}$ . Therefore, since $v^{\\mathrm{down}}(a_{\\star}^{\\mathrm{up}},b)\\geqslant0$ for any $b\\in{\\mathcal{A}}$ , we can write ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mu^{\\star,\\mathrm{down}}=\\operatorname*{sup}_{(a,\\tau)\\in\\tilde{\\mathsf{A}},b\\in\\mathcal{A}}\\{v^{\\mathrm{down}}(a,b)-\\tau\\}\\ .\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "First note that if $(a,\\tau)\\in\\tilde{\\mathsf{A}}$ , then for any $a^{\\prime}\\in{\\mathcal{A}},\\tau\\in\\mathbb{R}_{+},v^{\\mathrm{up}}(a)+\\tau\\geqslant v^{\\mathrm{up}}(a^{\\prime})+\\mathbb{1}_{a}(a^{\\prime})\\tau,$ , which gives ", "page_idx": 16}, {"type": "equation", "text": "$$\nv^{\\mathrm{up}}(a)-v^{\\mathrm{up}}(a^{\\prime})\\geqslant(\\mathbb1_{a}(a^{\\prime})-1)\\tau\\;.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "However, either $a\\,=\\,a^{\\prime}$ and hence $v^{\\mathrm{up}}(a)\\mathrm{~-~}v^{\\mathrm{up}}(a^{\\prime})\\,=\\,0$ , either $a\\ne a^{\\prime}$ and hence $\\mathbb{1}_{a}(a^{\\prime})\\,=\\,0$ . Therefore, we have that ", "page_idx": 16}, {"type": "equation", "text": "$$\nv^{\\mathrm{up}}(a)-v^{\\mathrm{up}}(a^{\\prime})\\geqslant-\\tau\\;,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which implies by definition of the optimal incentives that $\\tau\\geqslant v^{\\mathrm{up}}(a^{\\prime})-v^{\\mathrm{up}}(a)$ for any $a^{\\prime}\\in\\mathcal{A}$ , and hence $\\tau_{a}^{\\star}\\leqslant\\tau$ for any $(a,\\tau)\\in\\tilde{\\mathsf{A}}$ . ", "page_idx": 16}, {"type": "text", "text": "In addition, $(a,\\tau_{a}^{\\star})\\in\\tilde{\\mathsf{A}}$ by definition. Consequently, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mu^{\\star,\\mathrm{down}}=\\operatorname*{max}_{a,b\\in\\mathcal{A}^{2}}\\left\\{v^{\\mathrm{down}}(a,b)-\\tau_{a}^{\\star}\\right\\}=\\operatorname*{max}_{a,b\\in\\mathcal{A}}\\left\\{v^{\\mathrm{down}}(a,b)-\\operatorname*{max}_{a^{\\prime}\\in\\mathcal{A}}\\{v^{\\mathrm{up}}(a^{\\prime})\\}+v^{\\mathrm{up}}(a)\\right\\}\\,,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "hence the first part of the result. Since $\\mu^{\\star,\\mathrm{up}}$ is defined as $\\mu^{\\star,\\mathrm{up}}=\\operatorname*{max}_{a\\in\\mathcal{A}}v^{\\mathrm{up}}(a)$ , we have that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu^{\\star,\\mathrm{down}}+\\mu^{\\star,\\mathrm{up}}=\\displaystyle\\operatorname*{max}_{a,b\\in A}\\{v^{\\mathrm{down}}(a,b)-\\displaystyle\\operatorname*{max}_{a^{\\prime}\\in A}\\{v^{\\mathrm{up}}(a^{\\prime})\\}+v^{\\mathrm{up}}(a)\\}+\\displaystyle\\operatorname*{max}_{a\\in A}v^{\\mathrm{up}}(a)}\\\\ &{\\qquad\\qquad\\qquad=\\displaystyle\\operatorname*{max}_{a,b\\in A}\\{v^{\\mathrm{down}}(a,b)+v^{\\mathrm{up}}(a)\\}}\\\\ &{\\qquad\\qquad=v^{\\mathrm{up}}(a^{\\mathrm{sw}})+v^{\\mathrm{down}}(a^{\\mathrm{sw}},b^{\\mathrm{sw}})\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Now summing $\\Re_{\\mathrm{p}}^{\\mathrm{down}}$ and $\\Re_{\\mathrm{p}}^{\\mathrm{up}}$ as defined in (9) and (11), we obtain ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ensuremath{\\operatorname*{max}}_{p}(T,\\ensuremath{\\Pi_{\\Psi}}_{\\Psi}^{\\mathrm{II}\\mathrm{em}},\\Pi_{\\Psi}^{\\mathrm{em}})=\\Phi_{p}^{\\mathrm{Mem}}(T,\\ensuremath{\\Pi_{\\Psi}}^{\\mathrm{II}\\mathrm{em}},\\Pi_{\\Psi}^{\\mathrm{Em}\\mathrm{m}})}\\\\ &{\\quad=\\ensuremath{\\operatorname*{max}}\\left\\{\\sum_{i=1}^{T}\\ensuremath{\\operatorname*{max}}\\{v^{\\mathrm{in}}(\\alpha)+\\lambda_{i}(\\alpha)v(t)\\}-(v^{\\mathrm{in}}(A_{i})+\\lambda_{i}(A_{i})\\tau(t))\\right\\}}\\\\ &{\\quad+\\;T\\sum_{a,b\\in\\mathrm{Z}}\\ensuremath{\\operatorname*{max}}_{\\pm}\\ensuremath{\\operatorname*{max}}_{p}(\\ensuremath{v}^{\\mathrm{ajon}}(a,b)-\\ensuremath{\\operatorname*{max}}_{\\mp}(\\ensuremath{v}^{\\mathrm{ajon}}(a^{\\prime}))+v^{\\mathrm{aw}}(a))-\\ensuremath{\\operatorname*{E}\\!\\left[\\sum_{i=1}^{T}v^{\\mathrm{abon}}(A_{i},B_{i})-\\lambda_{i}(A_{i})\\tau(t)\\right]}_{\\pm}}\\\\ &{\\quad\\geq T\\ensuremath{\\operatorname*{max}}_{a\\in\\mathrm{Z}}\\ensuremath{\\operatorname*{sup}}(a)-\\ensuremath{\\operatorname*{E}\\!\\left[\\sum_{i=1}^{T}v^{\\mathrm{in}}(A_{i})+\\lambda_{i}(A_{i})\\tau(t)\\right]}_{\\pm}-\\ensuremath{\\operatorname*{E}\\!\\left[\\sum_{i=1}^{T}v^{\\mathrm{abon}}(A_{i},B_{i})-\\lambda_{i}(A_{i})\\tau(t)\\right]}_{\\pm}}\\\\ &{\\quad+\\;T\\sum_{a,b\\in\\mathrm{Z}}\\ensuremath{\\operatorname*{tem}}(a,b)+v^{\\mathrm{ain}}(a))-T\\ensuremath{\\operatorname*{max}}_{a\\in\\mathrm{Z}}\\ensuremath{\\operatorname*{tem}}(a^{\\prime})\\}}\\\\ &{\\quad=T\\ensuremath{\\operatorname*{max}}_{a,b\\in\\mathrm{Z}}\\ensuremath{\\operatorname*{tem}}(a)+v^{\\mathrm{dom}}(a,b)-\\ensuremath{\\operatorname*{E}\\!\\left[\\sum_{i=1}^{T}v^{\\mathrm{in}}(A_{i})+v^{\\mathrm{dom}}(A_{i},B_{i})\\right]}_{\\pm}}\\\\ &{\\quad=\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "hence the result. ", "page_idx": 17}, {"type": "text", "text": "For a downstream player\u2019s policy $\\Pi_{\\mathrm{p}}^{\\mathrm{down}}$ , we define $\\tilde{\\mathfrak{R}}_{\\mathrm{p}}^{\\mathrm{up}}$ as the upstream player\u2019s regret without expectation, following ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\tilde{\\mathfrak{p}}_{\\mathfrak{p}}^{\\mathrm{up}}(\\{s+1,\\dotsc,s+t\\},\\Pi_{\\mathfrak{p}}^{\\mathrm{up}},\\Pi_{\\mathfrak{p}}^{\\mathrm{down}})=\\sum_{l=s+1}^{s+t}\\operatorname*{max}_{a\\in A}\\{\\mathfrak{v}^{\\mathrm{up}}(a)+\\mathbb{1}_{\\tilde{a}_{l}}(a)\\tau(l)\\}-(v^{\\mathrm{up}}(A_{l})+\\mathbb{1}_{\\tilde{a}_{l}}(A_{l})\\tau(l))\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $(\\tilde{a}_{l},\\tau(l))_{l\\in[T]}$ are the incentives output by $\\Pi_{\\mathrm{p}}^{\\mathrm{down}}$ and $(A_{l})_{l\\in[T]}$ are the output of $\\Pi_{\\mathrm{p}}^{\\mathrm{up}}$ . Recall the assumption that we use on the upstream player\u2019s regret for a policy $\\Pi_{\\mathrm{p}}^{\\mathrm{up}}$ . We show here that it is satisfied by typical no-regret bandit algorithms. ", "page_idx": 17}, {"type": "text", "text": "H2. There exist $\\mathrm{C},\\zeta>0,\\kappa\\in[0,1)$ such that for any $s,t\\in[T]$ with $s\\!+\\!t\\leqslant T_{s}$ , any $\\{\\tau_{a}\\}_{a\\in[K]}\\in\\mathbb{R}_{+}^{K}$ and any policy $\\Pi_{\\mathrm{p}}^{\\mathrm{down}}$ that offers almost surely a transfer $(\\tilde{a}_{l},\\tau(l))\\,=\\,(\\tilde{a}_{l},\\tau_{\\tilde{a}_{l}})$ for any $l\\,\\in\\,\\{s+$ $1,\\ldots,s+t\\}$ , the batched regret of the upstream player following $\\Pi_{\\mathrm{p}}^{\\mathrm{up}}$ satisfies, with probability at least $1-t^{-\\zeta}$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{l=s+1}^{s+t}\\operatorname*{max}_{a\\in\\mathcal{A}}\\{v^{\\mathrm{up}}(a)+\\mathbb{1}_{\\Tilde{a}_{l}}(a)\\tau_{\\Tilde{a}_{l}}\\}-(v^{\\mathrm{up}}(A_{l})+\\mathbb{1}_{\\Tilde{a}_{l}}(A_{l})\\tau_{\\Tilde{a}_{l}})\\leqslant\\mathrm{C}t^{\\kappa}\\;.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We present the upstream player\u2019s UCB subroutine. ", "page_idx": 17}, {"type": "text", "text": "Proposition 2. Let $s,t\\,\\in\\,[T]$ such that $s+t\\leqslant T$ . Suppose that there exists $a$ family $(\\tau_{a})_{a\\in A}$ of constant incentives associated with each arm $a\\ \\in\\ {\\mathcal{A}}$ such that we have in Algorithm 3: $(\\tilde{a}_{l},\\tau(l))_{s+l\\in\\{s+1,...,s+t\\}}\\,=\\,(\\tilde{a}_{l},\\tau_{\\tilde{a}_{l}})_{l\\in\\{s+1,...,s+t\\}}$ as an output of $\\Pi_{\\mathrm{p}}^{\\mathrm{down}}$ . Suppose that the distributions $\\gamma_{a}$ are 1-sub-Gaussian. Then with probability at least $1-T^{-\\bar{2}}$ , the regret of the version of UCB given in Algorithm 3 run by the upstream player satisfies ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{\\Re}_{\\mathrm{p}}^{\\mathrm{up}}(\\{s+1,\\dots,s+t\\},U\\!C\\!B,\\Pi_{\\mathrm{p}}^{\\mathrm{down}})\\leqslant8\\sqrt{\\log(K T^{3})}\\sqrt{t K}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Note that the major difference between this assumption and the regret bounds that we generally consider in multi-armed bandit problems is that we consider the regret without expectation here. ", "page_idx": 17}, {"type": "text", "text": "1: Input: Set of arms $K$ , horizon $T$ .   \n2: Initialize: For any arm $a\\in[K]$ , set $\\hat{\\mu}_{a}=0$ , $T_{a}=0$ .   \n3: for $1\\leqslant t\\leqslant K$ : do   \n4: Pull arm $A_{t}=t$   \n5: Update $\\hat{\\mu}_{A_{t}}=X_{A_{t}}(t)$ , $T_{A_{t}}(t)=1$   \n6: end for   \n7: for $t\\geqslant K+1$ do   \n8: Observe the incentive $(\\tilde{a}_{t},\\tau(t))$ .   \n9: $\\begin{array}{r}{\\operatorname{Pull\\;arm}\\,A_{t}\\in\\arg\\!\\operatorname*{max}_{a\\in[K]}\\Bigl\\{\\hat{\\mu}_{a}(t-1)+2\\sqrt{\\frac{\\log(K T^{3})}{T_{a}(t-1)}}+\\mathbb{1}_{\\tilde{a}_{t}}(a)\\tau(t)\\Bigr\\}}\\end{array}$ ", "page_idx": 18}, {"type": "text", "text": "10: Update $T_{A_{t}}(t)=T_{A_{t}}(t-1)+1$ , \u00b5\u02c6At(t) =TAt1(t)(TAt(t \u22121)\u00b5\u02c6At(t \u22121) + XAt(t)) ", "page_idx": 18}, {"type": "text", "text": "Proof of Proposition 2. The proof is adapted from the proof of Bubeck et al. [2012, Theorem 2.1]. Let $s,t\\,\\in\\,[T]$ such that $s+t\\leqslant T$ . For any integer $\\bar{l}\\,\\in\\,\\{s+1,\\dots,s+t\\}$ , we write $n_{l}(a)\\,=$ Card $\\{l^{\\prime}\\in[l]$ such that $A_{l^{\\prime}}=a\\}$ for the number of pulls of arm $a$ and $\\hat{\\mu}_{a}(l)$ for the empirical mean utility of the arm $a\\in A$ estimated on the batch $\\begin{array}{r}{\\{1,\\ldots,l\\}\\colon\\hat{\\mu}_{a}(l)=n_{l}(a)^{-1}\\sum_{k=1}^{l}\\mathbb{1}_{a}(A_{k})X_{a}(k)}\\end{array}$ Since the rewards on the incentivized bandit instance are 1-sub-Gaussian, a Hoeffding bound gives that for any $\\delta\\in(0,1)$ , $a\\in A$ , $l\\in\\{s+1,\\ldots,s+t\\}$ , and any family of arms $\\big(a_{l^{\\prime}}\\big)_{l^{\\prime}\\in\\{1,...,l\\}}$ such that Card $\\{j\\colon a_{j}=a\\}=k$ , we have that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\Big(|\\hat{\\mu}_{a}(l)-v^{\\mathrm{up}}(a)|\\geqslant2\\sqrt{\\log(2/\\delta)/k}\\;\\Big|\\;(A_{l^{\\prime}})_{l^{\\prime}\\in\\{1,\\dots,s+l\\}}=(a_{l^{\\prime}})_{l^{\\prime}\\in\\{1,\\dots,s+l\\}}\\Big)\\leqslant\\delta\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Therefore, for any $\\equiv(0,1),a\\in\\mathcal{A},l\\in\\{s+1,\\ldots,s+t\\}$ , we have the following bound ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\mathbb{P}\\left(\\bigcup_{k=1}^{l}\\bigcup_{\\operatorname{cut}(l^{\\prime})\\in\\{1,\\ldots,1\\}^{n}}\\left\\{|\\widehat{\\mu}_{\\alpha}(l)-v^{\\mathrm{up}}(\\alpha)|\\geqslant2\\sqrt{\\log(2/\\delta)/k}\\right\\}\\right)}\\\\ &{\\leqslant\\displaystyle\\sum_{k=1}^{l}\\mathbb{P}\\left(\\bigcup_{\\operatorname{cut}(l^{\\prime})\\in\\{1,\\ldots,1\\}^{n}\\setminus\\Omega^{n-1}}\\left\\{|\\widehat{\\mu}_{\\alpha}(s+l)-v^{\\mathrm{up}}(\\alpha)|\\geqslant2\\sqrt{\\log(2/\\delta)/k}\\right\\}\\right)}\\\\ &{\\leqslant\\displaystyle\\sum_{k=1}^{l}\\mathbb{P}\\left(\\prod_{\\operatorname{cut}(l^{\\prime})\\in\\{1,\\ldots,1\\}^{n}\\setminus\\Omega^{n-1}}\\mathbb{P}\\Big(|\\widehat{\\mu}_{\\alpha}(s+l)-v^{\\mathrm{up}}(\\alpha)|\\geqslant2\\sqrt{\\log(2/\\delta)/k}\\Big)\\right)}\\\\ &{\\leqslant\\displaystyle\\sum_{k=1}^{l}\\sum_{\\operatorname{cut}(l^{\\prime})\\in\\{1,\\ldots,1\\}^{n}\\atop\\operatorname{cut}(l^{\\prime})\\in\\{1,\\ldots,1\\}^{n}\\setminus\\Omega^{n-1}}\\mathbb{P}\\Big(|\\widehat{\\mu}_{\\alpha}(s+l)-v^{\\mathrm{up}}(\\alpha)|\\geqslant2\\sqrt{\\log(2/\\delta)/k}\\Big|\\;\\big|A_{l}=a_{l}\\Big)\\mathbb{P}(A_{l}=a_{l})}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and with an union bound, we obtain that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\Big(\\exists\\,l\\in[t],a\\in A\\mathrm{~such~that~}|\\,\\hat{\\mu}_{a}(l)-v^{\\mathrm{up}}(a)|\\geqslant2\\sqrt{\\log(2/\\delta)/n_{l}(a)}\\Big)\\leqslant T^{2}K\\delta\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Considering the probability of the opposite event, we have that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb P\\Big(\\mathrm{for~any}\\;l\\in[t],a\\in A,|\\hat{\\mu}_{a}(l)-v^{\\mathrm{up}}(a)|\\leqslant2\\sqrt{\\log(2T^{2}K/\\delta)/n_{l}(a)}\\Big)\\geqslant1-\\delta\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where we rescaled $\\delta$ as $\\delta/T^{2}K$ . ", "page_idx": 18}, {"type": "text", "text": "For the remaining of the proof, we take $\\delta=T^{-2}$ and define $a_{l}^{\\star}=\\arg\\operatorname*{max}_{a\\in\\mathcal{A}}\\{v^{\\mathrm{up}}(a)+\\mathbb{1}_{\\tilde{a}_{l}}(a)\\tau\\}$ . We now assume that the event {for any $l\\ \\in\\ \\{s\\,+\\,1,\\ldots,s\\,+\\,t\\},a\\ \\in\\ \\bar{\\mathcal{A}},|\\hat{\\mu}_{a}(l)\\,-\\,v^{\\mathrm{up}}(a)|\\ \\leqslant$ $2\\sqrt{\\log(2T^{2}K/\\delta)/n_{l}(a)}\\}$ holds. If at some step $l\\;\\in\\;\\{s+1,\\ldots,s+t\\}$ , action $A_{l}$ is chosen in Algorithm 3, it means that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\mu}_{A_{l}}(l)+2\\sqrt{\\log(t K/\\delta)/n_{l}(A_{l})}+\\mathbb{1}_{\\tilde{a}_{l}}(A_{l})\\tau_{\\tilde{a}_{l}}\\geqslant\\hat{\\mu}_{a_{l}^{*}}(l)+2\\sqrt{\\log(t K/\\delta)/n_{l}(a_{l}^{*})}+\\mathbb{1}_{\\tilde{a}_{l}}(a_{l}^{*})\\tau_{\\tilde{a}_{l}}\\,\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "with regards to the choice of actions in UCB based on the upper confidence bound. We now decompose the whole regret on the batch $\\{s+1,\\ldots,s+t\\}$ defined as $\\tilde{\\Re}_{\\mathrm{p}}^{\\mathrm{up}}(\\{s+1,\\ldots,s+t\\}$ , UCB, $\\Pi_{\\mathrm{p}}^{\\mathrm{down}})$ . (20) ensures that with probability at least $1-1/T^{2}$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\bar{\\wp}_{\\Psi}^{\\mathrm{sup}}(\\{s+1,\\ldots,s+t\\},\\bar{\\upsilon}\\mathbb{D}\\mathbb{D}_{\\bar{\\theta}}^{\\mathrm{Linem}})=\\sum_{l=s+1}^{s+t}\\upsilon^{\\mathrm{up}}(a_{l}^{*})+\\mathbb{1}_{\\bar{a}_{\\bar{a}}}(a_{l}^{*})\\tau_{\\bar{a}_{l}}-(\\upsilon^{\\mathrm{up}}(A_{l})+}\\\\ &{\\le\\sum_{l=s+1}^{s+t}\\hat{\\mu}_{a_{l}}(l)+2\\sqrt{\\log(K t/\\delta)/n}(a_{l}^{*})-\\upsilon^{\\mathrm{up}}(A_{l})+\\mathbb{1}_{\\bar{a}_{\\bar{a}}}(a_{l}^{*})\\tau_{\\bar{a}_{l}}-\\mathbb{1}_{\\bar{a}_{\\bar{a}}}(A_{l})\\tau_{\\bar{a}_{l}}}\\\\ &{\\le\\sum_{l=s+1}^{s+t}\\hat{\\mu}_{A_{l}}(l)+2\\sqrt{\\log(K t/\\delta)/n}(A_{l})-\\upsilon^{\\mathrm{up}}(A_{l})}\\\\ &{\\lesssim\\sum_{l=s+1}^{s+t}\\hat{\\mu}_{A_{l}}(l)+2\\sqrt{\\log(K t/\\delta)/n}(A_{l})-(\\hat{\\mu}_{A_{l}}(l)-2\\sqrt{\\log(K t/\\delta)/n}(A_{l}))}\\\\ &{\\lesssim\\sum_{l=s+1}^{s+t}\\hat{\\mu}_{A_{l}}(l)+2\\sqrt{\\log(K t/\\delta)/n}(A_{l})}\\\\ &{\\leqslant4\\sqrt{\\log(K t/\\delta)}\\sum_{l=s+1}^{s+t}\\sqrt{1/n}(A_{l})\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{l=s+1}^{s+t}\\sqrt{1/n_{l}(A_{l})}\\leqslant\\sum_{i=1}^{K}\\sum_{l=s+1}^{s+t}\\sqrt{\\mathbb{1}_{A_{l}}(i)/n_{l}(i)}}}\\\\ &{}&{\\leqslant\\displaystyle\\sum_{i=1}^{K}\\sum_{j=n_{s+1}(i)}^{n_{s+t}(i)}1/\\sqrt{j}\\leqslant2\\sum_{i=1}^{K}\\sqrt{n_{s+t}(i)-n_{s}(i)}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the last step holds because for any integers $s,t\\in\\mathbb{N}^{\\star}$ , we have that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sum_{l=s+1}^{s+t}\\frac{1}{\\sqrt{l}}=\\sum_{l=s+1}^{s+t}\\frac{1}{\\sqrt{l}}\\int_{x=l-1}^{l}\\mathrm{d}x\\leqslant\\sum_{l=s+1}^{s+t}\\int_{x=l-1}^{l}\\frac{\\mathrm{d}x}{\\sqrt{x}}=\\int_{x=s}^{s+t}\\frac{\\mathrm{d}x}{\\sqrt{x}}=2(\\sqrt{s+t}-\\sqrt{s})\\;.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Using Cauchy-Schwarz inequality we obtain from (21) ", "page_idx": 19}, {"type": "equation", "text": "$$\n1/K\\sum_{l=s+1}^{s+t}\\sqrt{1/n_{l}(A_{l})}\\leqslant2\\sqrt{1/K\\sum_{i=1}^{K}n_{s+t}(i)-n_{s}(i)}=2\\sqrt{t/K}\\,,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which gives $\\begin{array}{r}{\\sum_{l=s+1}^{s+t}\\sqrt{1/n_{l}(A_{l})}\\leqslant2\\sqrt{t K}}\\end{array}$ ", "page_idx": 19}, {"type": "text", "text": "Finally plugging all the terms together, since $\\delta\\,=\\,1/T^{2}$ , we obtain that with probability at least $1-1\\bar{/}\\bar{T^{2}}$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\tilde{\\Re}_{\\mathrm{p}}^{\\mathrm{up}}(\\{s+1,\\dots,s+t\\},\\mathrm{UCB})\\leqslant8\\sqrt{\\log(K T^{3})}\\sqrt{t K}\\;.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Lemma 2. Assume $_{H2}$ holds and consider some arm $a\\;\\in\\;{\\mathcal{A}}$ such that we run the d-th batch of binary search on a with $d\\,\\in\\,\\{1,\\dots,\\lceil\\log_{2}T^{\\beta}\\rceil\\}$ : for any $t\\;\\in\\;\\{k_{a,d}+1,\\ldots,k_{a,d}+\\tilde{T}\\}$ , we have $(\\tilde{a}_{t},\\tau(t))\\;=\\;(a,\\tau_{a})$ with $\\tau_{a}\\;=\\;\\tau_{a}^{\\mathrm{{mid}}}(d)$ and $\\tilde{T}\\;=\\;\\lceil T^{\\alpha}\\rceil$ . Recall that we defined in (18): $T_{a,d}^{\\neq}\\;=\\;C a r d\\left\\{t\\;\\in\\;\\{k_{a,d}+1,\\ldots,k_{a,d}+\\tilde{T}\\}\\right.$ such that $A_{t}~\\neq~a\\}$ . Let $\\beta\\ \\in\\ (0,1)$ be such that $\\beta<\\alpha(1-\\kappa)$ . Given that the event $\\{\\tilde{\\mathfrak{R}}_{\\mathrm{p}}^{\\mathrm{up}}(\\{k_{a,d}+1,\\dots,k_{a,d}+\\tilde{T}\\},\\Pi_{\\mathrm{p}}^{\\mathrm{up}},\\Pi_{\\mathrm{p}}^{\\mathrm{down}})\\leqslant\\mathrm{C}\\tilde{T}^{\\kappa}\\}$ holds, we have that ", "page_idx": 19}, {"type": "text", "text": "Consequently, with probability at least $1-2T^{-\\alpha\\zeta}$ , $i f\\,\\mathrm{C}\\tilde{T}^{\\kappa+\\beta/\\alpha}\\,<\\,T_{a,d}^{\\neq}\\,<\\,\\tilde{T}\\,-\\,\\mathrm{C}\\tilde{T}^{\\kappa+\\beta/\\alpha}$ , then $\\left|\\tau_{a}^{\\star}-\\tau_{a}\\right|\\leqslant1/T^{\\beta}$ . ", "page_idx": 19}, {"type": "text", "text": "Proof of Lemma 2. The whole proof is done conditionally on the event ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\{\\tilde{\\mathfrak{R}}_{\\mathrm{p}}^{\\mathrm{up}}(\\{k_{a,d}+1,\\dots,k_{a,d}+\\tilde{T}\\},\\Pi_{\\mathrm{p}}^{\\mathrm{up}},\\Pi_{\\mathrm{p}}^{\\mathrm{down}})\\leqslant\\mathrm{C}\\tilde{T}^{\\kappa}\\}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Note that it holds with probability at least $1-\\tilde{T}^{-\\zeta}\\geqslant1-T^{-\\alpha\\zeta}$ since we suppose that $\\Pi_{\\mathrm{p}}^{\\mathrm{up}}$ satisfies $\\mathbf{H}2$ . ", "page_idx": 20}, {"type": "text", "text": "Suppose that we have $\\tau_{a}\\geqslant\\tau_{a}^{\\star}+1/T^{\\beta}$ . By definition of the optimal incentives, we obtain, using by assumption $\\tau_{a}\\geqslant\\tau_{a}^{\\star}+1/T^{\\beta}$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{1}_{a}(a)\\tau_{a}+v^{\\mathrm{up}}(a)\\geqslant\\tau_{a}^{\\star}+v^{\\mathrm{up}}(a)+1/T^{\\beta}}\\\\ &{\\qquad\\qquad\\qquad\\quad=\\displaystyle\\operatorname*{max}_{a^{\\prime}\\in A}v^{\\mathrm{up}}(a^{\\prime})-v^{\\mathrm{up}}(a)+v^{\\mathrm{up}}(a)+1/T^{\\beta}}\\\\ &{\\qquad\\qquad\\qquad\\quad=\\displaystyle\\operatorname*{max}_{a^{\\prime}}v^{\\mathrm{up}}(a^{\\prime})+1/T^{\\beta}\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which ensures that $a$ is the optimal arm for the upstream player during the batch $\\{k_{a,d}+1,\\ldots,k_{a,d}+$ $\\tilde{T}\\}$ that we consider. In that case, since $a$ is the best arm, by definition of the upstream player\u2019s utility, the reward gap $\\mathbb{E}[v^{\\mathrm{up}}(a)+\\tau_{a}-(v^{\\mathrm{up}}(A_{t})+\\mathbb{1}_{a}(A_{t})\\tau_{a})]$ for the upstream player at any step $t\\in\\{k_{a,d}+1,\\ldots,k_{a,d}+\\tilde{T}\\}$ is at least $v^{\\mathrm{up}}(a)+\\tau_{a}-\\operatorname*{max}_{a^{\\prime}\\in\\mathcal{A}}v^{\\mathrm{up}}(a^{\\prime})$ , and we obtain ", "page_idx": 20}, {"type": "equation", "text": "$$\nC\\tilde{T}^{\\kappa}\\geqslant\\tilde{\\mathfrak{N}}_{\\mathrm{p}}^{\\mathrm{up}}(\\{k_{a,d}+1,\\ldots,k_{a,d}+\\tilde{T}\\},\\Pi_{\\mathrm{p}}^{\\mathrm{up}},\\Pi_{\\mathrm{p}}^{\\mathrm{down}})\\geqslant T_{a,d}^{\\neq}(\\tau_{a}+v^{\\mathrm{up}}(a)-\\operatorname*{max}_{a^{\\prime}\\in\\mathcal{A}}\\{v^{\\mathrm{up}}(a^{\\prime})\\})\\;,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "by $\\mathbf{H}2$ , $T_{a,d}^{\\neq}$ being the number of steps for which a suboptimal arm has been chosen. Therefore, by definition of the optimal incentives, we obtain that ", "page_idx": 20}, {"type": "equation", "text": "$$\nC\\tilde{T}^{\\kappa}\\geqslant T_{a,d}^{\\neq}(\\tau_{a}-\\tau_{a}^{\\star})\\geqslant T_{a,d}^{\\neq}/T^{\\beta}\\geqslant T_{a,d}^{\\neq}\\;\\tilde{T}^{-\\beta/\\alpha}\\;,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which gives: $T_{a,d}^{\\neq}\\leqslant\\mathrm{C}\\tilde{T}^{\\kappa+\\beta/\\alpha}$ . Therefore, if we take the contrapositive, we obtain that during the sequence $\\{k_{a,d}+1,\\ldots,k_{a,d}+\\tilde{T}\\}$ , if $T_{a,d}^{\\neq}>\\mathrm{C}\\tilde{T}^{\\kappa+\\beta/\\alpha}$ , then with probability at least $1-T^{-\\alpha\\zeta}$ , $\\tau_{a}<\\tau_{a}^{\\star}+1/T^{\\beta}$ , or equivalently $\\tau_{a}^{\\star}>\\tau_{a}-1/T^{\\beta}$ . ", "page_idx": 20}, {"type": "text", "text": "Now suppose that $\\tau_{a}\\leqslant\\tau_{a}^{\\star}-1/T^{\\beta}$ . By definition of the optimal incentives, we obtain ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{1}_{a}(a)\\tau_{a}+v^{\\mathrm{up}}(a)\\leqslant\\tau_{a}^{\\star}+v^{\\mathrm{up}}(a)}\\\\ &{\\qquad\\qquad\\qquad\\leqslant\\displaystyle\\operatorname*{max}_{a^{\\prime}\\in A}v^{\\mathrm{up}}(a^{\\prime})-v^{\\mathrm{up}}(a)+v^{\\mathrm{up}}(a)-1/T^{\\beta}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\operatorname*{max}_{a^{\\prime}}v^{\\mathrm{up}}(a^{\\prime})-1/T^{\\beta}\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which ensures that $a$ is a suboptimal arm for the upstream player during this batch of time steps. Therefore, arm $a$ which has a reward gap bigger than $1/T^{\\beta}$ since $\\operatorname*{max}_{a^{\\prime}\\in A}\\{v^{\\mathrm{up}}(a)+\\mathbb{1}_{a}(a^{\\prime})\\}^{-}$ $(v^{\\mathrm{up}}(a)+\\tau_{a})\\geqslant1/T^{\\beta}$ and arm $a$ has been picked $\\tilde{T}-T_{a,d}^{\\neq}$ T \u0338a=,d times. Consequently, we have that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{C}\\tilde{T}^{\\kappa}\\geqslant\\tilde{\\mathfrak{R}}_{\\mathrm{p}}^{\\mathrm{up}}(\\{k_{a,d}+1,\\ldots,k_{a,d}+\\tilde{T}\\},\\Pi_{\\mathrm{p}}^{\\mathrm{up}},\\Pi_{\\mathrm{p}}^{\\mathrm{down}})}\\\\ &{\\qquad\\geqslant(\\tilde{T}-T_{a,d}^{\\neq})\\underset{\\underbrace{a^{\\prime}\\in\\mathcal{A}}}{\\mathrm{max}}v^{\\mathrm{up}}(a^{\\prime})-\\big(\\tau_{a}+v^{\\mathrm{up}}(a)\\big)\\big)}\\\\ &{\\qquad\\geqslant(\\tilde{T}-T_{a,d}^{\\neq})\\tilde{T}^{-\\beta/\\alpha}\\ ,}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which gives $T_{a,d}^{\\neq}\\geqslant\\tilde{T}-\\mathrm{C}\\tilde{T}^{\\kappa+/\\beta/\\alpha}$ . Therefore, if we take the contrapositive, we obtain that if $T_{a,d}^{\\neq}<\\tilde{T}-\\mathrm{C}\\tilde{T^{\\kappa+/\\beta/\\alpha}}$ , then with probability at least $1-T^{-\\alpha\\zeta}$ , $\\tau_{a}>\\tau_{a}^{\\star}-1/T^{\\beta}$ , or equivalently \u03c4 a\u22c6 < \u03c4a + 1/T \u03b2. ", "page_idx": 20}, {"type": "text", "text": "For the second part of the proof, suppose that: $\\mathrm{C}\\tilde{T}^{\\kappa+\\beta/\\alpha}<T_{a,d}^{\\neq}<\\tilde{T}^{\\kappa+\\beta/\\alpha}-\\mathrm{C}\\tilde{T}^{\\kappa+\\beta/\\alpha}.$ ", "page_idx": 20}, {"type": "text", "text": "From the above result, we have that $\\tau_{a}^{\\star}<\\tau_{a}+1/T^{\\beta}$ and $\\tau_{a}^{\\star}>\\tau_{a}-1/T^{\\beta}$ with probability at least $1-2T^{-\\alpha\\zeta}$ . Plugging these inequalities in the absolute value $|\\tau_{a}^{\\star}-\\tau_{a}|$ concludes the proof. ", "page_idx": 20}, {"type": "text", "text": "Lemma 3. Assume that we run Algorithm 2 and consider some binary search batch iteration $d\\in\\lceil\\log T^{\\beta}\\rceil$ run on arm $a\\in{\\mathcal{A}}$ . Then $0\\leqslant\\underline{{\\tau}}_{a}(d)\\leqslant\\tau_{a}^{\\mathrm{mid}}(d)\\leqslant\\overline{{\\tau}}_{a}(d)\\leqslant1$ . ", "page_idx": 20}, {"type": "text", "text": "Proof of Lemma 3. The proof proceeds by induction. Considering some action $a\\in A$ , we have for the initialisation before any binary search is run: $\\underline{{\\tau}}_{a}(0)=0,\\overline{{\\tau}}_{a}\\bar{(}0)=1$ and therefore $\\tau_{a}^{\\mathrm{mid}}(0)\\in$ $[\\underline{{\\tau}}_{a}(0),\\overline{{\\tau}}_{a}(0)]$ . We now consider that a number $d$ of binary search batches has been run on $a$ . Suppose that we run an additional binary search batch on action $a$ . We have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\tau_{a}^{\\mathrm{mid}}(d+1)=\\frac{\\overline{{\\tau}}_{a}(d)+\\tau_{a}(d)}{2}\\mathrm{~which~gives~}\\tau_{a}^{\\mathrm{mid}}(d+1)\\in[\\underline{{\\tau}}_{a}(d),\\overline{{\\tau}}_{a}(d)]\\;.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "After this iteration of binary search, we either have $\\overline{{\\tau}}_{a}(d\\!+\\!1)=\\tau_{a}^{\\mathrm{mid}}(d\\!+\\!1)\\!+\\!1/T^{\\beta}$ and $\\underline{{\\tau}}_{a}(d\\!+\\!1)=$ $\\underline{{\\tau}}_{a}(d)$ or $\\overline{{\\tau}}_{a}(d+1)\\,=\\,\\tau_{a}^{\\mathrm{mid}}(d)$ and $\\underline{{\\tau}}_{a}(d+1)=\\tau_{a}^{\\mathrm{mid}}(d+1)-1/T^{\\beta}$ . Therefore, we still have $0\\,\\leqslant\\,\\underline{{\\tau}}_{a}(d+1)\\,\\leqslant\\,\\tau_{a}^{\\mathrm{mid}}(d+1)\\,\\leqslant\\,\\overline{{\\tau}}_{a}(d+1)\\,\\leqslant\\,1$ , hence the result for any $d\\,\\in\\,\\lceil\\log_{2}T^{\\beta}\\rceil$ by induction. \u53e3 ", "page_idx": 21}, {"type": "text", "text": "Lemma 4. Consider some arm $a\\in A$ and suppose that we have run $D\\in\\mathbb{N}^{\\star}$ batches of binary search of length $\\tilde{T}$ on $a$ . Then, we have that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\bigcap_{d\\in[D]}\\{\\tilde{\\mathfrak{N}}_{\\mathrm{p}}^{\\mathrm{up}}(\\{k_{a,d}+1,\\dots,k_{a,d}+\\tilde{T}\\},\\Pi_{\\mathrm{p}}^{\\mathrm{up}},\\Pi_{\\mathrm{p}}^{\\mathrm{down}})\\leqslant\\mathrm{C}\\tilde{T}^{\\kappa}\\}\\right)\\geqslant1-D/T^{\\alpha\\zeta}\\;.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof of Lemma 4. First observe that for any batch $d\\in\\{1,\\ldots,D\\}$ of binary search run on arm $a$ during steps $\\{k_{a,d}+1,\\ldots,k_{a,d}+\\tilde{T}\\}$ , we have by $\\mathbf{H}2$ that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Big(\\tilde{\\mathfrak{R}}_{\\mathrm{p}}^{\\mathrm{up}}\\big(\\{k_{a,d}+1,\\dots,k_{a,d}+\\tilde{T}\\},\\Pi_{\\mathrm{p}}^{\\mathrm{up}},\\Pi_{\\mathrm{p}}^{\\mathrm{down}}\\big)\\geqslant\\mathrm{C}\\tilde{T}^{\\kappa}\\Big)\\leqslant1/\\tilde{T}^{\\zeta}\\;,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and applying a union bound over the $D$ batches, we have that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(\\bigcup_{d\\in[D]}\\{\\tilde{\\mathfrak{R}}_{\\mathrm{p}}^{\\mathrm{up}}(\\{k_{a,d}+1,\\dots,k_{a,d}+\\tilde{T}\\},\\Pi_{\\mathrm{p}}^{\\mathrm{up}},\\Pi_{\\mathrm{p}}^{\\mathrm{down}})\\geqslant\\mathrm{C}\\tilde{T}^{\\kappa}\\}\\right)}\\\\ &{\\quad\\leqslant\\displaystyle\\sum_{j=1}^{D}\\mathbb{P}\\Big(\\{\\tilde{\\mathfrak{R}}_{\\mathrm{p}}^{\\mathrm{up}}(\\{k_{a,d}+1,\\dots,k_{a,d}+\\tilde{T}\\},\\Pi_{\\mathrm{p}}^{\\mathrm{up}},\\Pi_{\\mathrm{p}}^{\\mathrm{down}})\\geqslant\\mathrm{C}\\tilde{T}^{\\kappa}\\}\\Big)}\\\\ &{\\quad\\leqslant D/\\tilde{T}^{\\zeta}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "which gives that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\bigcap_{d\\in[D]}\\{\\tilde{\\mathfrak{R}}_{\\mathrm{p}}^{\\mathrm{up}}(\\{k_{a,d}+1,\\dots,k_{a,d}+\\tilde{T}\\},\\Pi_{\\mathrm{p}}^{\\mathrm{up}},\\Pi_{\\mathrm{p}}^{\\mathrm{down}})\\leqslant\\mathrm{C}\\tilde{T}^{\\kappa}\\}\\right)\\geqslant1-D/T^{\\alpha\\zeta}\\,,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "hence the result. ", "page_idx": 21}, {"type": "text", "text": "Lemma 5. Suppose that the upstream player runs a subroutine $\\Pi_{\\mathrm{p}}^{\\mathrm{up}}$ satisfying $H\\,2$ . Consider some action $a\\in{\\mathcal{A}}$ and the $D$ -th binary search batch of length $\\tilde{T}$ run on arm a with $D\\in\\{1,\\ldots,\\lceil\\log_{2}T^{\\beta}\\rceil\\}$ . Then ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\bigcap_{d\\in[D]}\\{\\tilde{\\mathfrak{R}}_{\\mathrm{p}}^{\\mathrm{up}}(\\{k_{a,d}+1,\\dots,k_{a,d}+\\tilde{T}\\},\\Pi_{\\mathrm{p}}^{\\mathrm{up}},\\Pi_{\\mathrm{p}}^{\\mathrm{down}})\\leqslant\\mathrm{C}\\tilde{T}^{\\kappa}\\}\\subseteq\\{\\tau_{a}^{\\star}\\in[\\underline{{\\tau}}_{a}(D),\\overline{{\\tau}}_{a}(D)]\\}\\;,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and the probability of these events is at least $1-\\lceil\\log_{2}T^{\\beta}\\rceil/T^{\\alpha\\zeta}$ . We also have that ", "page_idx": 21}, {"type": "text", "text": "Proof of Lemma 5. Suppose that the conditions of the lemma hold and consider some arm $a$ . ", "page_idx": 21}, {"type": "text", "text": "We show by induction on the number of binary search batches $D$ that have been run on $a$ that $\\begin{array}{r}{\\bigcap_{d\\in[D]}\\{\\mathfrak{H}_{\\mathrm{p}}^{\\mathrm{up}}(\\{k_{a,d}+1,\\dotsc,k_{a,d}+\\tilde{T}\\},\\Pi_{\\mathrm{p}}^{\\mathrm{up}},\\Pi_{\\mathrm{p}}^{\\mathrm{down}})\\leqslant\\mathrm{C}\\tilde{T}^{\\kappa}\\}\\subseteq\\{\\tau_{a}^{\\star}\\in[\\underline{{\\tau}}_{a}(D),\\overline{{\\tau}}_{a}(D)]\\}}\\end{array}$ . If it is true, Lemma 4 completes this first part of the proof. ", "page_idx": 21}, {"type": "text", "text": "The initialisation holds since $\\underline{{\\tau}}_{a}(0)=0$ , $\\overline{{\\tau}}_{a}(0)=1$ and $\\begin{array}{r}{\\tau_{a}^{\\star}=\\operatorname*{max}_{a^{\\prime}\\in\\mathcal{A}}{v^{\\mathrm{up}}(a^{\\prime})}-{v^{\\mathrm{up}}(a)}\\in[0,1]}\\end{array}$ with probability $\\mathrm{~1~-~}$ since $\\scriptstyle\\operatorname*{max}_{a^{\\prime}\\in{\\mathcal{A}}}v^{\\operatorname*{up}}(a^{\\prime})\\;\\in\\;[0,1]$ and $v^{\\mathrm{up}}(a)\\in[0,1]$ . ", "page_idx": 21}, {"type": "text", "text": "We suppose that the property is true for some integer $D<\\lceil\\log_{2}T^{\\beta}\\rceil$ and that we have run one more binary search on arm $a$ . We have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\tau_{a}^{\\mathrm{mid}}(D+1)=\\frac{\\overline{{\\tau}}_{a}(D)+\\underline{{\\tau}}_{a}(D)}{2}\\;,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "$\\tau_{a}^{\\mathrm{mid}}(D+1)$ being the incentive offered to the upstream player if he chooses action $a$ during the $D$ -th batch $\\{k_{a,D}+1,\\ldots,k_{a,D}+\\tilde{T}\\}$ . After this batch, if $T_{a,D}^{\\neq}\\,<\\,\\tilde{T}\\,-\\,\\mathrm{C}\\tilde{T}^{\\kappa+\\beta/\\alpha}$ , BELGIC updates $\\overline{{\\tau}}_{a}(D+1)\\,=\\,\\tau_{a}^{\\mathrm{mid}}(D+1)+1/T^{\\beta}$ , $\\underline{{\\tau}}_{a}(D+1)\\,=\\,\\underline{{\\tau}}_{a}(D)$ and Lemma 2 ensures that $\\underline{{\\tau}}_{a}(D+1)<\\tau_{a}^{\\star}<\\overline{{\\tau}}_{a}(D+1)$ given $\\{\\tilde{\\mathfrak{R}}_{\\mathrm{p}}^{\\mathrm{up}}(\\{k_{a,D+1}+1,\\dots,k_{a,D+1}+\\tilde{T}\\},\\Pi_{\\mathrm{p}}^{\\mathrm{up}},\\Pi_{\\mathrm{p}}^{\\mathrm{down}})\\leqslant\\mathrm{C}\\tilde{T}^{\\kappa}\\}$ . Thus the induction holds. ", "page_idx": 22}, {"type": "text", "text": "Otherwise, if $T_{a,D}^{\\neq}\\,>\\,\\mathrm{C}\\tilde{T}^{\\kappa+\\beta/\\alpha}$ , BELGIC updates $\\underline{{\\tau}}_{a}(D+1)\\,=\\,\\tau_{a}^{\\mathrm{mid}}(D+1)\\,-\\,1/T^{\\beta}$ , $\\overline{{\\tau}}_{a}(D+$ $1)\\,=\\,\\overline{\\tau}_{a}(D)$ and Lemma 2 ensures that $\\underline{{\\tau}}_{a}(D+1)\\,<\\,\\tau_{a}^{\\star}\\,<\\,\\overline{{\\tau}}_{a}(D+1)$ given $\\{\\tilde{\\Re}_{\\mathrm{p}}^{\\mathrm{up}}(\\{k_{a,D+1}+$ $1,\\dots,k_{a,D+1}+\\tilde{T}\\},\\Pi_{\\mathrm{p}}^{\\mathrm{up}},\\Pi_{\\mathrm{p}}^{\\mathrm{down}})\\leqslant\\mathrm{C}\\tilde{T}^{\\kappa}\\}$ . The induction still holds. ", "page_idx": 22}, {"type": "text", "text": "Consequently, we have that for any number $D$ of binary search batches run on arm $a$ , $\\tau_{a}^{\\star}\\ \\in$ $[\\underline{{\\tau}}_{a}(D),\\overline{{\\tau}}_{a}(D)]$ with probability $1-D/T^{\\alpha\\zeta}$ ", "page_idx": 22}, {"type": "text", "text": "For the second part of the proof, we define $u(D)=\\overline{{\\tau}}_{a}(D)-\\underline{{\\tau}}_{a}(D)\\geqslant0$ as the length of the interval containing $\\tau_{a}^{\\star}$ with probability at least $1-D/T^{\\alpha\\zeta}$ . We have $u(0)=1$ . Suppose that after $D$ iterations of binary search batches, the next batch of binary search $\\{k_{a,D+1}+1,\\ldots,k_{a,D+1}+\\tilde{T}\\}$ outputs $T_{a,D+1}^{\\neq}<\\tilde{T}-\\mathrm{C}\\tilde{T}^{\\kappa+\\beta/\\alpha}$ . Then, the update of Algorithm 2 gives ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{u_{D+1}=\\overline{\\tau}_{a}(D+1)-\\underline{\\tau}_{a}(D+1)}\\\\ &{\\qquad\\quad=\\tau_{a}^{\\mathrm{mid}}(D+1)+1/T^{\\beta}-\\underline{\\tau}_{a}(D)}\\\\ &{\\qquad\\quad=\\frac{\\overline{\\tau}_{a}(D)+\\underline{\\tau}_{a}(D)}{2}-\\underline{\\tau}_{a}(D)+1/T^{\\beta}}\\\\ &{\\qquad\\quad=\\frac{\\overline{\\tau}_{a}(D)-\\underline{\\tau}_{a}(D)}{2}+1/T^{\\beta}}\\\\ &{\\qquad\\quad=u_{D}/2+1/T^{\\beta}\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "On the other hand, if T \u0338a=,D $T_{a,D+1}^{\\neq}>\\mathrm{C}\\tilde{T}^{\\kappa+\\beta/\\alpha}$ , the update gives ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{u_{D+1}=\\overline{\\tau}_{a}(D+1)-\\underline{\\tau}_{a}(D+1)}\\\\ &{\\qquad\\quad=\\overline{\\tau}_{a}(D_{t}^{a})-(\\tau_{a}^{\\mathrm{mid}}(D+1)-1/T^{\\beta})}\\\\ &{\\qquad\\quad=\\overline{\\tau}_{a}(D)-\\frac{\\overline{\\tau}_{a}(D)+\\underline{\\tau}_{a}(D)}{2}+1/T^{\\beta}}\\\\ &{\\qquad\\quad=\\frac{\\overline{\\tau}_{a}(D)-\\underline{\\tau}_{a}(D)}{2}+1/T^{\\beta}}\\\\ &{\\qquad\\quad=u_{D}/2+1/T^{\\beta}\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We can see that $(u_{D})_{D\\geqslant0}$ is an arithmetico-geometric sequence defined by $u_{D+1}=u_{D}/2+1/T^{\\beta}$ with an initial term $u_{0}=1$ . Writing $r=1/T^{\\beta}/(1-1/2)=2/T^{\\beta}$ , we obtain that ", "page_idx": 22}, {"type": "equation", "text": "$$\n|\\overline{{{\\tau}}}_{a}(D)-\\underline{{{\\tau}}}_{a}(D)|=u_{D}=1/2^{D}(1-r)+r=1/2^{D}(1-2/T^{\\beta})+2/T^{\\beta}\\leqslant1/2^{D}+2/T^{\\beta}\\;,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "for any $D\\in\\{1,\\ldots,\\lceil\\log_{2}T^{\\beta}\\rceil\\}$ , hence the result. ", "page_idx": 22}, {"type": "text", "text": "Lemma 6. Suppose that the upstream player runs a policy $\\Pi_{\\mathrm{p}}^{\\mathrm{up}}$ satisfying $_{H2}$ . Considering some action $a\\in\\mathcal{A}$ , we have that after the binary search batch $\\dot{D}\\,=\\,\\lceil\\log_{2}T^{\\beta}\\rceil$ : $\\mathbb{P}(\\underline{{\\tau}}_{a}(D)\\,\\leqslant\\,\\tau_{a}^{\\star}\\,\\leqslant$ $\\overline{{\\tau}}_{a}(D)\\leqslant\\underline{{\\tau}}_{a}+3/T^{\\beta})\\geqslant1-D/T^{\\alpha\\zeta}$ . ", "page_idx": 22}, {"type": "text", "text": "Proof of Lemma 6. We suppose that the event $\\begin{array}{r}{\\bigcap_{d\\in[\\lceil\\log_{2}T^{\\beta}\\rceil]}\\{\\mathfrak{R}_{\\mathrm{p}}^{\\mathrm{up}}(\\{k_{a,d}\\ +\\ 1,\\dots,k_{a,d}\\ +}\\end{array}$ $\\tilde{T}\\},\\Pi_{\\mathrm{p}}^{\\mathrm{up}},\\Pi_{\\mathrm{p}}^{\\mathrm{down}}\\big)\\leqslant\\mathrm{C}\\tilde{T}^{\\kappa}\\}$ holds. Lemma 4 ensures that this event holds with probability at least $1-\\lceil\\log_{2}T^{\\beta}\\rceil/T^{\\alpha\\zeta}$ . ", "page_idx": 22}, {"type": "text", "text": "After $D=\\lceil\\beta\\log_{2}T\\rceil$ batches of binary search on arm $a$ , we have by Lemma 5 that ", "page_idx": 23}, {"type": "equation", "text": "$$\n|\\overline{{\\tau}}_{a}(D)-\\underline{{\\tau}}_{a}(D)|\\leqslant1/2^{D}+2/T^{\\beta}\\leqslant1/2^{\\beta\\log_{2}T}+2/T^{\\beta}=3/T^{\\beta}\\;.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Lemma 5 guarantees that $\\tau_{a}^{\\star}\\in[\\underline{{\\tau}}_{a}(D);\\overline{{\\tau}}_{a}(D)]$ with probability at least $1-D/T^{\\alpha\\zeta}$ , and we obtain $\\underline{{\\tau}}_{a}(D)\\leqslant\\tau_{a}^{\\star}\\leqslant\\tau_{a}(D)\\leqslant\\underline{{\\tau}}_{a}(D)+3/T^{\\beta}$ with the same probability. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "Proposition 1. Under $H I$ and $_{H2}$ , after the first phase of BELGIC which consists in $K\\lceil T^{\\alpha}\\rceil\\lceil\\log T^{\\beta}\\rceil$ steps of binary search grouped in $\\lceil\\log_{2}T^{\\beta}\\rceil$ batches per arm $a\\in{\\mathcal{A}}$ , we have that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Big(\\;f o r\\,a n y\\;a\\in\\mathcal{A},\\hat{\\tau}_{a}-4/T^{\\beta}-\\mathbb{C}T^{(\\kappa-1)/2}\\leqslant\\tau_{a}^{\\star}\\leqslant\\hat{\\tau}_{a}\\Big)\\geqslant1-K\\lceil\\log_{2}T^{\\beta}\\rceil/T^{\\alpha\\zeta}\\;.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof of Proposition $^{\\,l}$ . We consider a number of binary search batches $D=\\lceil\\log T^{\\beta}\\rceil$ and we define the event $\\mathcal{G}$ as $\\begin{array}{r}{\\mathcal{G}=\\left\\{\\begin{array}{l l}{\\begin{array}{r l r l}\\end{array}}\\end{array}\\right.}\\end{array}$ for any $a\\in\\mathcal{A},\\underline{{\\tau}}_{a}(D)\\leqslant\\tau_{a}^{\\star}\\leqslant\\tau_{a}(D)\\leqslant\\underline{{\\tau}}_{a}(D)+3/T^{\\beta}\\}$ . We have that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}(\\mathcal{G})=\\mathbb{P}\\Bigg(\\bigcap_{a\\in A}\\left\\{\\underline{{\\tau}}_{a}(D)\\leqslant\\tau_{a}^{\\star}\\leqslant\\overline{{\\tau}}_{a}(D)\\leqslant\\underline{{\\tau}}_{a}+3/T^{\\beta}\\right\\}\\Bigg)}\\\\ &{\\qquad=1-\\mathbb{P}\\Bigg(\\bigcup_{a\\in A}\\left\\{\\underline{{\\tau}}_{a}(D)\\leqslant\\tau_{a}^{\\star}\\leqslant\\overline{{\\tau}}_{a}(t)\\leqslant\\underline{{\\tau}}_{a}(D)+3/T^{\\beta}\\right\\}^{\\mathrm{c}}\\Bigg)}\\\\ &{\\qquad\\geqslant1-\\sum_{a\\in A}\\mathbb{P}\\Big(\\left\\{\\underline{{\\tau}}_{a}(D)\\leqslant\\tau_{a}^{\\star}\\leqslant\\overline{{\\tau}}_{a}(D)\\leqslant\\underline{{\\tau}}_{a}(D)+3/T^{\\beta}\\right\\}^{\\mathrm{c}}\\Big)\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the last inequality holds with an union bound. Lemma 6 with $D=\\lceil\\log_{2}T^{\\beta}\\rceil$ ensures that we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n{\\mathbb P}\\Big(\\{\\underline{{\\tau}}_{a}(D)\\leqslant\\tau_{a}^{\\star}\\leqslant\\tau_{a}(D)\\leqslant\\underline{{\\tau}}_{a}(D)+3/T^{\\beta}\\}^{\\mathrm{c}}\\Big)\\leqslant D/T^{\\alpha\\zeta}\\;,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and since ${\\mathrm{Card}}\\{{\\mathcal{A}}\\}=K$ , we obtain ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\mathcal{G})\\geqslant1-K\\lceil\\log_{2}T^{\\beta}\\rceil/T^{\\alpha\\zeta}\\ .\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Since the estimated incentives are defined as $\\hat{\\tau}_{a}=\\overline{{{\\tau}}}_{a}\\big(\\lceil\\log_{2}T^{\\beta}\\rceil\\big)+1/T^{\\beta}+\\mathrm{C}T^{(\\kappa-1)/2}$ , we can conclude ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\mathrm{for~any}\\,a\\in\\mathcal{A},\\hat{\\tau}_{a}-4/T^{\\beta}-\\mathbb{C}T^{(\\kappa-1)/2}\\leqslant\\tau_{a}^{\\star}\\leqslant\\hat{\\tau}_{a})\\geqslant1-K\\lceil\\log_{2}T^{\\beta}\\rceil/T^{\\alpha\\zeta}\\,,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "since when $\\begin{array}{r}{\\mathsf{r e v e r}\\,\\underline{{\\tau}}_{a}\\bigl(\\lceil\\log_{2}T^{\\beta}\\rceil\\bigr)\\leqslant\\tau_{a}^{\\star}\\leqslant\\overline{{\\tau}}_{a}\\bigl(\\lceil\\log_{2}T^{\\beta}\\rceil\\bigr)\\leqslant\\underline{{\\tau}}_{a}\\bigl(\\lceil\\log_{2}T^{\\beta}\\rceil\\bigr)+1/T^{\\beta},\\,\\mathrm{we}\\,\\,\\mathsf{a}\\to\\,\\mathsf{w e c},\\,}\\end{array}$ lso have by definition: $\\hat{\\tau}_{a}(\\lceil\\log_{2}T^{\\beta}\\rceil)-4/T^{\\beta}-\\mathrm{C}T^{(\\kappa-1)/2}\\leqslant\\underline{{\\tau}}_{a}(\\lceil\\log_{2}T^{\\beta}\\rceil)\\leqslant\\tau_{a}^{\\star}\\leqslant\\hat{\\tau}_{a}(\\lceil\\log_{2}T^{\\beta}\\rceil)$ . \u53e3 ", "page_idx": 23}, {"type": "text", "text": "Theorem 3. Assume that $H I$ and $_{H2}$ hold. Then BELGIC, run with $\\alpha,\\beta$ satisfying (13) and any bandit subroutine Bandi $t\\!-\\!\\!A\\,\\!\\!l\\,g,$ , has an overall regret $\\Re_{\\mathrm{p}}^{\\mathrm{down}}$ such that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{R}_{\\mathfrak{p}}^{\\mathrm{down}}(T,\\Pi_{\\mathrm{p}}^{\\mathfrak{u p}},\\mathtt{B E L G I C})\\leqslant2(3+2\\mathrm{C}+\\bar{v}-\\underline{{v}})\\log_{2}(T)(2T^{1-\\alpha\\zeta}+T^{(\\kappa+1)/2}+\\lceil T^{\\alpha}\\rceil)+4T^{1-\\beta}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\,\\mathfrak{R}_{B a n d i t\\cdot A l g}(T,\\nu,\\{\\hat{\\tau}_{a}\\}_{a\\in A})}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where, for ease of notation ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\bar{v}=\\operatorname*{max}_{a,b\\in A\\times A}\\{v^{\\mathrm{down}}(a,b)\\}\\quad a n d\\quad\\underline{{v}}=\\operatorname*{min}_{a,b\\in A\\times A}\\{v^{\\mathrm{down}}(a,b)\\}\\;.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof of Theorem 3. Suppose that the conditions of Theorem 3 are satisfied. By definition, $\\Lambda_{K+1}+1~\\in~[T]$ is the step at which starts the run of the subroutine Bandit-Alg, since $\\Lambda_{K+1}\\;=\\;K\\lceil T^{\\alpha}\\rceil\\lceil\\beta\\log T\\rceil$ . All the binary searche batches have length $\\tilde{T}~=~\\lceil T^{\\alpha}\\rceil$ . For any $a\\in{\\mathcal{A}},d\\in\\{1,\\ldots,\\lceil\\log_{2}T^{\\beta}\\rceil\\}$ , we define the event ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathsf{B}_{a,d}=\\left\\{\\tilde{\\mathfrak{R}}_{\\mathrm{p}}^{\\mathrm{up}}(\\{k_{a,d}+1,\\dots,k_{a,d}+\\tilde{T}\\},\\Pi_{\\mathrm{p}}^{\\mathrm{up}},\\Pi_{\\mathrm{p}}^{\\mathrm{down}})\\leqslant C\\tilde{T}^{\\kappa}\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "as well as ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathcal{E}=\\bigcap_{\\stackrel{a\\in[K]}{d\\in[\\lceil\\beta\\log T\\rceil]}}\\mathsf{B}_{a,d}\\bigcap\\Bigl\\{\\tilde{\\mathfrak{R}}_{\\mathrm{p}}^{\\mathrm{up}}\\bigl(\\{\\Lambda_{K+1}+1,\\ldots,T\\},\\Pi_{\\mathrm{p}}^{\\mathrm{up}}\\bigr)\\leqslant\\mathrm{C}(T-\\Lambda_{K+1})^{\\kappa}\\Bigr\\}\\,,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and by $\\mathbf{H}2$ and Lemma 4, with an union bound, we have that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}(\\mathcal{E})=1-\\mathbb{P}\\left(\\underset{a\\in[K]}{\\bigcup}\\;\\;\\underset{1\\le i,d\\le j}{\\bigcup}\\bigcup_{\\alpha,d}^{\\mathtt{c}}\\bigcup_{\\beta}^{\\mathtt{s u p}}\\big(\\{\\Lambda_{K+1}+1,\\ldots,T\\},\\Pi_{\\mathtt{p}}^{\\mathtt{u p}},\\Pi_{\\mathtt{p}}^{\\mathrm{down}}\\big)\\leqslant\\mathrm{C}(T-\\Lambda_{K+1})^{\\kappa}\\big\\}\\right)}\\\\ &{\\geqslant1-\\underset{a\\in[K]}{\\sum}\\mathbb{P}(\\mathtt{S}_{a,d}^{\\mathtt{c}})+\\mathbb{P}(\\tilde{\\mathfrak{P}}_{\\mathtt{p}}^{\\mathtt{u p}}(\\{\\Lambda_{K+1}+1,\\ldots,T\\},\\Pi_{\\mathtt{p}}^{\\mathtt{u p}})\\leqslant\\mathrm{C}(T-\\Lambda_{K+1})^{\\kappa})}\\\\ &{\\qquad\\qquad\\overset{a\\in[K]}{\\le}d\\epsilon[\\lceil\\alpha,T^{\\beta}\\rceil]}\\\\ &{\\geqslant1-K\\lceil\\beta\\log T\\rceil\\tilde{T}^{-\\zeta}-(T-\\Lambda_{K+1})^{-\\zeta}}\\\\ &{\\geqslant1-K\\lceil\\beta\\log T\\rceil T^{-\\alpha\\zeta}-T^{-\\zeta}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and we now decompose ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\Re_{\\mathrm{p}}^{\\mathrm{down}}(T,\\Pi_{\\mathrm{p}}^{\\mathrm{up}},\\Pi_{\\mathrm{p}}^{\\mathrm{down}})=\\mathbb{E}\\Big[\\mathbb{1}(\\mathcal{E})\\tilde{\\mathfrak{R}}_{\\mathrm{p}}^{\\mathrm{down}}(T,\\Pi_{\\mathrm{p}}^{\\mathrm{up}},\\Pi_{\\mathrm{p}}^{\\mathrm{down}})\\Big]+\\Big[\\mathbb{1}(\\mathcal{E}^{<})\\tilde{\\mathfrak{R}}_{\\mathrm{p}}^{\\mathrm{down}}(T,\\Pi_{\\mathrm{p}}^{\\mathrm{up}},\\Pi_{\\mathrm{p}}^{\\mathrm{down}})\\Big]\\;,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\tilde{\\Re}_{\\mathrm{p}}^{\\mathrm{down}}$ is defined as ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\tilde{\\Re}_{\\mathrm{p}}^{\\mathrm{down}}(T,\\Pi_{\\mathrm{p}}^{\\mathrm{up}},\\Pi_{\\mathrm{p}}^{\\mathrm{down}})=T\\mu^{\\star,\\mathrm{down}}-\\sum_{t=1}^{T}(v^{\\mathrm{down}}(A_{t},B_{t})-\\mathbb{1}_{\\Tilde{a}_{t}}(A_{t})\\tau(t))\\ .\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "By definition, $\\begin{array}{r}{\\mu^{\\star,\\mathrm{down}}\\geqslant\\operatorname*{min}_{a,b\\in{\\cal A}\\times{\\cal A}}v^{\\mathrm{down}}(a,b)\\geqslant\\underline{{v}}}\\end{array}$ and since Lemma 1 allows to write $\\mu^{\\star,\\mathrm{down}}=$ $\\begin{array}{r}{\\operatorname*{max}_{a,b\\in A\\times A}\\{v^{\\mathrm{down}}(a,b)+v^{\\mathrm{up}}(a)\\}-\\operatorname*{max}_{a^{\\prime}\\in A}\\{v^{\\mathrm{up}}(a^{\\prime})\\},}\\end{array}$ , we have that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\underline{{v}}\\leqslant\\mu^{\\star,\\mathrm{down}}\\leqslant1+\\bar{v}~,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and note that since $\\kappa<1$ , for any $a\\in\\mathcal{A},\\hat{\\tau}_{a}\\,\\leqslant\\,2+\\mathrm{C}T^{(\\kappa-1)/2}\\,\\leqslant\\,2+\\mathrm{C}$ . Consequently, $T\\underline{{v}}\\leqslant$ $\\tilde{\\mathfrak{R}}_{\\mathrm{p}}^{\\mathrm{down}}(T,\\Pi_{\\mathrm{p}}^{\\mathrm{up}},\\Pi_{\\mathrm{p}}^{\\mathrm{down}})\\leqslant(3+\\mathrm{C}+\\bar{v}-\\underline{{v}})T$ almost surely. Therefore ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\Big[\\mathbb{1}(\\mathcal{E}^{\\mathrm{c}})\\tilde{\\mathfrak{R}}_{\\mathrm{p}}^{\\mathrm{down}}(T,\\Pi_{\\mathrm{p}}^{\\mathrm{up}},\\Pi_{\\mathrm{p}}^{\\mathrm{down}})\\Big]\\leqslant(3+\\mathrm{C}+\\bar{v}-\\underline{{v}})(K\\lceil\\log T^{\\beta}\\rceil T^{1-\\alpha\\zeta}+T^{1-\\zeta})\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We consider the second term in (23). We decompose it between the steps of binary search $\\{1,\\dots,K\\lceil T^{\\alpha}\\rceil\\lceil\\log_{2}T^{\\beta}\\rceil\\}$ during which we run the Binary Search Subroutine and the following ones when we run Bandit-Alg, which gives ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\mathbb{1}(\\mathcal{E})\\mathfrak{H}_{\\mathrm{p}}^{\\mathrm{dawn}}(T,\\Pi_{\\mathrm{p}}^{\\mathrm{up}},\\Pi_{\\mathrm{p}}^{\\mathrm{down}})\\right]}\\\\ &{\\quad\\leqslant\\mathbb{E}\\left[\\underbrace{\\mathbb{1}(\\mathcal{E})\\underbrace{K\\Gamma^{T=1}[\\log_{2}T^{\\delta}]}_{t=1}+\\mathrm{down}-(v^{\\mathrm{down}}(A_{t},B_{t})-\\mathbb{1}_{\\bar{a}_{t}}(A_{t})\\tau(t))}_{(A)}\\right]}\\\\ &{\\quad+\\mathbb{E}\\left[\\mathbb{1}(\\mathcal{E})\\underbrace{\\sum_{t=K/T^{\\delta}|\\log_{2}T^{\\delta}|+1}^{T}\\mu^{*,\\mathrm{down}}-(v^{\\mathrm{down}}(A_{t},B_{t})-\\mathbb{1}_{\\bar{a}_{t}}(A_{t})\\tau(t))}_{(B)}\\right]\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Similarly to (24), we use the bound on $\\mu^{\\star,\\mathrm{down}}$ to bound $(\\mathbf{A})$ , which gives ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathbb{1}(\\mathcal{E})(\\mathbf{A})]\\leqslant\\mathbb{E}\\left[\\mathbb{1}(\\mathcal{E})\\sum_{t=1}^{\\Lambda_{K+1}}1+\\bar{v}-(\\underline{{v}}-2-\\mathrm{C})\\right]\\leqslant K\\lceil T^{\\alpha}\\rceil(1+\\log_{2}T)(3+\\mathrm{C}+\\bar{v}-\\underline{{v}})\\,.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "After the binary search, at each step $t\\in\\{\\Lambda_{K+1}+1,\\ldots,T\\}$ , Bandit-Alg recommends $(\\tilde{a}_{t},B_{t})\\in$ $A\\times A$ following (15) and BELGIC offers an incentive $(\\tilde{a}_{t},\\hat{\\tau}_{\\tilde{a}_{t}})$ with $\\hat{\\tau}_{a}=\\overline{{{\\tau}}}_{a}(\\lceil\\log_{2}T^{\\beta}\\rceil)+1/T^{\\beta}+$ $\\mathrm{C}T^{(\\kappa-1)/2}$ . ", "page_idx": 25}, {"type": "text", "text": "Lemma 5 ensures that $\\mathcal{E}\\subseteq\\{\\tau_{a}^{\\star}\\in[\\underline{{\\tau}}_{a}([\\log_{2}T^{\\beta}]),\\overline{{\\tau}}_{a}([\\log_{2}T^{\\beta}])]$ for any $a\\in\\mathcal{A}\\}\\cap\\{\\tilde{\\mathfrak{R}}_{\\mathrm{p}}^{\\mathrm{up}}(\\{\\Lambda_{K+1}+$ $1,\\dots,T\\},\\Pi_{\\mathrm{p}}^{\\mathrm{up}})\\leqslant\\mathrm{C}(T-\\Lambda_{K+1})^{\\kappa}\\}$ . Therefore, if $\\mathcal{E}$ holds, for any $a\\in A$ , we have that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{v^{\\mathrm{up}}(a)+\\hat{\\tau}_{a}=v^{\\mathrm{up}}(a)+\\overline{{\\tau}}_{a}+1/T^{\\beta}+\\mathrm{C}T^{(\\kappa-1)/2}}\\\\ &{\\qquad\\qquad>v^{\\mathrm{up}}(a)+\\tau_{a}^{\\star}+\\mathrm{C}T^{(\\kappa-1)/2}}\\\\ &{\\qquad\\qquad=v^{\\mathrm{up}}(a)+\\operatorname*{max}_{a^{\\prime\\prime}\\in A}v^{\\mathrm{up}}(a^{\\prime\\prime})-v^{\\mathrm{up}}(a)+\\mathrm{C}T^{(\\kappa-1)/2}\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and therefore, for any $a^{\\prime}\\in\\mathcal{A}$ such that $a^{\\prime}\\neq a$ , we have that ", "page_idx": 25}, {"type": "equation", "text": "$$\nv^{\\mathrm{up}}(a)+\\hat{\\tau}_{a}>v^{\\mathrm{up}}(a^{\\prime})+\\mathrm{C}T^{(\\kappa-1)/2}\\;.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "This shows that at any steps $t\\in\\{\\Lambda_{K+1}+1,\\ldots,T\\}$ after the binary search, we have on the event $\\mathcal{E}$ that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{{a}}_{t}=\\operatorname{argmax}_{a^{\\prime}\\in\\mathcal{A}}\\{v^{\\mathrm{up}}(a^{\\prime})+\\mathbb{1}_{\\tilde{a}_{t}}(a^{\\prime})\\hat{\\tau}_{\\tilde{a}_{t}}\\}\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and the reward gap at step $t$ for any $a\\neq\\tilde{a}_{t}$ is defined as ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{a^{\\prime}\\in A}\\{v^{\\mathrm{up}}(a^{\\prime})+\\mathbb{1}_{\\tilde{a}_{t}}(a^{\\prime})\\hat{\\tau}_{\\tilde{a}_{t}}\\}-(v^{\\mathrm{up}}(a)+\\mathbb{1}_{\\tilde{a}_{t}}(a)\\hat{\\tau}_{\\tilde{a}_{t}})=v^{\\mathrm{up}}(\\tilde{a}_{t})+\\hat{\\tau}_{a}-v^{\\mathrm{up}}(a)\\ .\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Following (27), the reward gap from (29) satisfies ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{a^{\\prime}\\in A}\\{v^{\\mathrm{up}}(a^{\\prime})+\\mathbb{1}_{\\Tilde{a}_{t}}(a^{\\prime})\\hat{\\tau}_{\\Tilde{a}_{t}}\\}-\\big(v^{\\mathrm{up}}(a)+\\mathbb{1}_{\\Tilde{a}_{t}}(a)\\hat{\\tau}_{\\Tilde{a}_{t}}\\big)\\geqslant\\mathrm{C}T^{(\\kappa-1)/2}\\;.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We now define two sets ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{I}_{T}=\\left\\{t\\in\\{K[T^{\\alpha}]\\,[\\log_{2}T]+1,\\dots,T\\}\\mathrm{~such~that~}\\tilde{a}_{t}=A_{t}\\right\\}\\,,}\\\\ &{\\mathrm{J}_{T}=\\left\\{t\\in\\{K[T^{\\alpha}]\\,[\\log_{2}T]+1,\\dots,T\\}\\mathrm{~such~that~}\\tilde{a}_{t}\\neq A_{t}\\right\\}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "which satisfy $\\mathrm{I}_{T}\\cup\\mathrm{J}_{T}\\;=\\;\\{K\\lceil T^{\\alpha}\\rceil\\lceil\\log_{2}T\\rceil\\,+\\,1,\\ldots,T\\}$ almost surely. As shown in (28), $\\mathrm{I}_{T}$ corresponds to all the steps during which the upstream player picked the best arm and for any $t\\in\\mathrm{I}_{T}$ ", "page_idx": 25}, {"type": "equation", "text": "$$\nv^{\\mathrm{up}}(A_{t})+\\mathbb{1}_{\\tilde{a}_{t}}(A_{t})\\tau(t)\\geqslant\\operatorname*{max}_{a\\in\\mathcal{A}}\\{v^{\\mathrm{up}}(a)+\\mathbb{1}_{\\tilde{a}_{t}}(a)\\tau(t)\\}\\;,\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "while by (29), for any $t\\in\\mathrm{J}_{T}$ , we have that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{a\\in\\mathcal{A}}\\{v^{\\mathrm{up}}(a)+\\mathbb{1}_{\\tilde{a}_{l}}(a)\\hat{\\tau}_{\\tilde{a}_{l}}\\}-(v^{\\mathrm{up}}(A_{l})+\\mathbb{1}_{\\tilde{a}_{l}}(A_{l})\\hat{\\tau}_{\\tilde{a}_{l}})\\geqslant\\mathrm{C}T^{(\\kappa-1)/2}\\;.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "$\\mathbf{H}2$ ensures that if $\\mathcal{E}$ holds, then $\\Re_{\\mathrm{p}}^{\\mathrm{up}}(\\{\\Lambda_{K+1}+1,\\dots,T\\},\\Pi_{\\mathrm{p}}^{\\mathrm{up}},\\mathtt{B E L G I C})\\leqslant C\\,T^{\\kappa}$ , and this condition together with (30) gives that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathrm{Card}\\{\\mathrm{J}_{T}\\}\\,\\mathrm{C}\\,T^{(\\kappa-1)/2}\\leqslant\\mathfrak{R}_{\\mathrm{p}}^{\\operatorname*{sup}}(\\{\\Lambda_{K+1}+1,\\dots,T\\},\\Pi_{\\mathrm{p}}^{\\mathrm{up}},\\mathtt{B E L G I C})\\leqslant\\mathrm{C}T^{\\kappa}\\;,\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and consequently $\\mathrm{Card}\\{\\mathrm{J}_{T}\\}\\leqslant T^{(\\kappa+1)/2}$ . We now bound $(\\mathbf{B})$ as follows ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}[\\vert\\bar{F}\\vert(\\mathbf{S})\\vert]=\\mathbb{E}\\Bigg[\\Bigg\\vert\\frac{\\sum_{j=1}^{K}\\int_{\\gamma=\\infty}^{\\infty}\\int_{\\gamma}\\mu^{-\\mu_{\\mathrm{om}}}-(\\mu^{\\mathrm{om}}(\\mathbf{A},\\mathbf{B}_{j})-\\bar{F}_{\\mu_{\\mathrm{om}}})\\Bigg\\vert}{\\sum_{j=1}^{K}\\int_{\\gamma=\\infty}^{\\infty}\\int_{\\gamma}\\mu^{\\mathrm{om}}\\mathrm{d}\\nu^{\\mathrm{in}}}}\\\\ {-\\mathbb{E}\\Bigg[\\Bigg\\vert G_{\\mu_{\\mathrm{om}}}^{\\dagger}\\sum_{k=1}^{K}\\int_{\\gamma=\\infty}^{\\infty}\\int_{\\gamma}(\\mu^{\\mathrm{om}}(\\mathbf{A})-\\tau_{k}^{-\\mu})-(\\mu^{\\mathrm{om}}(\\mathbf{A},B_{j})-\\bar{F}_{\\mu_{\\mathrm{om}}})\\Bigg\\vert}\\\\ {+\\mathbb{E}\\Bigg[\\Bigg\\vert G_{\\mu_{\\mathrm{om}}}^{\\dagger}\\sum_{k=1}^{K}-(\\mu^{\\mathrm{om}}(\\mathbf{A}^{\\dagger}\\Delta_{k},\\mathbf{B}_{j})-\\bar{F}_{\\mu_{\\mathrm{om}}})\\Bigg\\vert}\\\\ {+\\mathbb{E}\\Bigg[\\Bigg\\vert G_{\\mu_{\\mathrm{om}}}^{\\dagger}\\sum_{k=1}^{K}\\Bigg\\vert G_{\\mu_{\\mathrm{om}}}^{\\dagger}\\Bigg\\vert\\bar{F}_{\\mu_{\\mathrm{om}}}^{\\dagger}\\Bigg]}\\\\ {\\leq\\mathbb{E}\\Bigg[\\Bigg\\vert G_{\\mu_{\\mathrm{om}}}^{\\dagger}\\sum_{k=1}^{K}\\mathbb{E}\\Bigg\\vert G_{\\mu_{\\mathrm{om}}}^{\\dagger}\\Bigg\\vert\\bar{F}_{\\mu_{\\mathrm{om}}}^{\\dagger}\\Bigg\\vert\\Bigg]}\\\\ {+\\left(3+C+\\mu^{\\mathrm{om}}-\\mathbb{E}\\Bigg\\vert\\bar{\\Xi}\\right\\vert(\\mathbf{E})\\mathrm{Carad}(\\mathbf{B}_{j})\\Bigg\\vert}\\\\ {-\\mathbb{E}\\Bigg[\\Bigg\\vert G_{\\mu_{\\mathrm{om}}}^{\\dagger}\\sum_{k=1}^{K}\\mathbb{E}\\Bigg\\vert G_{\\mu_{\\mathrm{om}}}^{\\dagger}\\Bigg\\vert\\bar{F}_{\\mu_{\\mathrm{om}}}^{\\dagger}\\Bigg]}\\\\ {+\\mathbb{E}\\Bigg[\\Bigg\\vert G_{\\mu_{\\mathrm{o\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the first step holds by Lemma 1. Using Lemma 5, as well as the definition of $\\mathcal{E}$ , we obtain ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\bigg[\\mathbb{1}(\\mathcal{E})\\mathrm{Card}\\{\\mathrm{I}_{T}\\}\\underset{a^{\\prime}\\in\\mathcal{A}}{\\operatorname*{max}}\\{\\hat{\\tau}_{a^{\\prime}}-\\tau_{a^{\\prime}}^{\\star}\\}\\bigg]\\leqslant\\mathbb{E}\\bigg[\\mathbb{1}(\\mathcal{E})\\mathrm{Card}\\left\\{\\mathcal{E}\\right\\}\\big(4/T^{\\beta}+\\mathrm{C}T^{(\\kappa-1)/2}\\big)\\bigg]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leqslant\\mathbb{E}[\\mathbb{1}(\\mathcal{E})T]\\big(4/T^{\\beta}+\\mathrm{C}T^{(\\kappa-1)/2}\\big)}\\\\ &{\\qquad\\qquad\\qquad\\leqslant4T^{1-\\beta}+\\mathrm{C}T^{(\\kappa+1)/2}\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "which finally gives ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[\\mathbb{1}(\\mathcal{E})(\\mathbf{B})]\\leqslant\\Re_{\\mathrm{Bandit-Alg}}(T,\\nu,\\{\\hat{\\tau}_{a}\\}_{a\\in\\mathcal{A}})+4T^{1-\\beta}+\\left(3+2\\mathrm{C}+\\bar{\\nu}-\\underline{{v}}\\right)T^{(1+\\kappa)/2}\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "and plugging together (26) and (31) in the decomposition (25) gives the following bound ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{\\Sigma}\\Big[\\mathbb{1}(\\mathcal{E})\\mathfrak{F}_{\\mathrm{p}}^{\\mathrm{down}}(T,\\Pi_{\\mathrm{p}}^{\\mathrm{up}},\\mathtt{B E L G I C}))\\Big]\\leqslant\\mathfrak{R}_{\\mathrm{Bandit-Alg}}(T,\\nu,\\{\\hat{\\tau}_{a}\\}_{a\\in A})+4T^{1-\\beta}+(3+2\\mathrm{C}+\\bar{\\nu}-\\underline{{\\upsilon}})\\,T^{(1+\\beta)}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\mathrm{\\Gamma}(3+\\mathrm{C}+\\bar{\\nu}-\\underline{{\\upsilon}})K\\lceil T^{\\alpha}\\rceil(1+\\log_{2}T)\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "and summing the bounds on the events $\\mathcal{E}$ and $\\mathcal{E}^{\\mathrm{c}}$ finally gives ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{N}_{\\mathbb{P}}^{\\mathrm{doom}}(T,\\Pi_{\\mathbb{P}}^{\\mathrm{up}},\\mathbb{B}\\mathrm{ELGT})\\leqslant\\Re_{\\mathrm{sandtl}^{\\mathrm{t}}\\mathrm{-}A\\mathrm{g}}(T,\\nu,\\{\\hat{\\tau}_{a}\\}_{a\\in A})+4T^{1-\\beta}+(3+2\\mathrm{C}+\\bar{\\nu}-\\underline{{\\nu}})\\,T^{(1+\\kappa)/2}}\\\\ &{\\hphantom{=}+(2+\\mathrm{C}+\\bar{\\nu}-\\underline{{\\nu}})K[T^{\\alpha}](1+\\log_{2}T)}\\\\ &{\\hphantom{=}+(3+\\mathrm{C}+\\bar{\\nu}-\\underline{{\\nu}})(K(1+\\log_{2}T)T^{1-\\alpha\\zeta}+T^{1-\\zeta})}\\\\ &{\\leqslant\\Re_{\\mathrm{sandtl}^{\\mathrm{t}}\\mathrm{-}A\\mathrm{g}}(T,\\nu,\\{\\hat{\\tau}_{a}\\}_{a\\in A})+4T^{1-\\beta}+(3+2\\mathrm{C}+\\bar{\\nu}-\\underline{{\\nu}})\\,T^{(1+\\kappa)/2}}\\\\ &{\\hphantom{=}+(3+\\bar{C}+\\bar{\\nu}-\\underline{{\\nu}})((1+\\log_{2}T)(\\lceil T^{\\alpha}\\rceil+T^{1-\\alpha\\zeta})+T^{1-\\zeta})}\\\\ &{\\leqslant(3+2\\mathrm{C}+\\bar{\\nu}-\\underline{{\\nu}})(T^{1-\\zeta}+T^{(\\kappa+1)/2}+(1+\\log_{2}T)(\\lceil T^{\\alpha}\\rceil+T^{1-\\alpha\\zeta}))}\\\\ &{\\hphantom{=}+\\Re_{\\mathrm{sandtl}^{\\mathrm{t}}\\mathrm{-}A\\log}(T,\\nu,\\{\\hat{\\tau}_{a}\\}_{a\\in A})+4T^{1-\\beta}}\\\\ &{\\leqslant2(3+2\\mathrm{C}+\\bar{\\nu}-\\underline{{\\nu}})\\log_{2}(T)(2T^{1-\\alpha\\zeta}+T^{(\\kappa+1)/2}+\\lceil T^{\\alpha}\\rceil)+4T^{1-\\beta}}\\\\ &{\\hphantom{=}+\\Re_{\\mathrm{sandtl}^{\\mathrm{t}}\\mathrm{-}A\\log}(T,\\nu,\\{\\hat{\\tau}_{a}\\}_{a\\in A})\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Corollary 1. Assume that the upstream player\u2019s distribution $(\\gamma_{a})_{a\\in A}$ is such that $_{H\\;I}$ holds. In addition, suppose that the distributions $(\\gamma_{a})_{a\\in A}$ and $(\\nu_{a,b})_{a,b\\in\\mathcal{A}\\times\\mathcal{A}}$ are 1-sub-Gaussian and that the upstream player plays $\\Pi_{\\mathrm{p}}^{\\mathrm{up}}=A$ lgorithm 3 (a slight modification of UCB to take into account the incentives). Then the downstream player\u2019s regret when she runs BELGIC with parameters $\\alpha=3/4$ and $\\beta=1/4$ (which satisfy (13)) and subroutine Bandi $t{\\mathrm{-}}A\\,\\!\\lambda\\,g={\\tt U C B}$ satisfies the following upper bound4 ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Re_{\\mathrm{p}}^{\\mathrm{down}}(T,\\,U C B,\\mathrm{BELGIC})\\leqslant(10+4K+32\\sqrt{K\\log_{2}(K T^{3})}+\\bar{v}-\\underline{{v}})\\log_{2}(T)(3+2T^{3/4})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\,3K^{2}(\\bar{v}-\\underline{{v}})\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof of Corollary $^{\\,l}$ . First note that $\\Pi_{\\mathrm{p}}^{\\mathrm{up}}\\,=\\,A l g o r i t h m\\ 3$ satisfies $\\mathbf{H}2$ with constants $\\kappa\\,=\\,1/2$ , $\\zeta\\,=\\,2$ , $\\mathrm{C}\\,=\\,8\\sqrt{K\\log(K T^{3})}$ , following Proposition 2. Note that $\\beta/\\alpha\\,=\\,1/3\\,<\\,1/2\\,=\\,1-\\kappa,$ therefore Equation (13) is satisfied. Plugging these terms in the bound from Theorem 3 with $\\alpha=3/4,\\beta\\stackrel{\\cdot}{=}1/4$ gives ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\texttt A}_{\\mathrm{p}}^{\\mathrm{down}}(T,{\\texttt U C B},{\\texttt B}{\\texttt E}{\\texttt L}{\\texttt G}{\\texttt I}{\\texttt C})\\leqslant2(3+16\\sqrt{K\\log_{2}(K T^{3})}+\\bar{v}-\\underline{{v}})\\log_{2}(T)(2T^{1-3/2}+T^{3/4}+\\lceil T^{3/4}\\rceil)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\,4T^{1/4}+8\\sqrt{K^{2}\\log_{2}(T)}\\,T^{1/2}+3K^{2}(\\bar{v}-\\underline{{v}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where we use the bound for $\\mathfrak{R}_{\\mathrm{Bandit-Alg}}(T,\\nu,\\{\\hat{\\tau}_{a}\\}_{a\\in\\mathcal{A}})$ with Bandit- ${\\tt-A l g}={\\tt U C B}$ run on any bandit instance with $K^{2}$ arms, 1-subgaussian rewards and reward gaps of at most $\\scriptstyle\\operatorname*{max}_{a,b\\in A\\times A}{v^{\\operatorname{down}}(a,b)-}$ $\\begin{array}{r}{\\operatorname*{min}_{a,b\\in{\\mathcal{A}}\\times{\\mathcal{A}}}v^{\\mathrm{down}}(a,b)=\\bar{v}-{\\underline{{v}}}.}\\end{array}$ , following [Lattimore and Szepesv\u00e1ri, 2020, Theorem 7.2]. Therefore, we have that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{R}_{\\mathrm{p}}^{\\mathrm{down}}(T,\\mathtt{U C B},\\mathtt{B E L G I C})\\leqslant(6+32\\sqrt{K\\log_{2}(K T^{3})}+\\bar{v}-\\underline{{v}})\\log_{2}(T)(2+1+2T^{3/4})}\\\\ &{\\phantom{=}+4T^{1/4}+3K^{2}(\\bar{v}-\\underline{{v}})+8K\\sqrt{\\log_{2}T}T^{1/2}}\\\\ &{\\phantom{=4}+(10+32\\sqrt{K\\log_{2}(K T^{3})}+\\bar{v}-\\underline{{v}})\\log_{2}(T)(3+2T^{3/4})}\\\\ &{\\phantom{=4}+3K^{2}(\\bar{v}-\\underline{{v}})+8K\\sqrt{\\log_{2}(T)}\\,T^{1/2}\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "which finally gives ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Re_{\\mathrm{p}}^{\\mathrm{down}}(T,\\mathtt{U C B},\\mathtt{B E L G I C})\\leqslant(10+4K+32\\sqrt{K\\log_{2}(K T^{3})}+\\bar{v}-\\underline{{v}})\\log_{2}(T)(3+2T^{3/4})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\,3K^{2}(\\bar{v}-\\underline{{v}})\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "hence the result. ", "page_idx": 27}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 28}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 28}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 28}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 28}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 28}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 28}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 28}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: The abstract mostly presents the issue that we tackle in the paper, namely presenting an online version of the Coase theorem. This contribution is the most important part of our paper. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 28}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Our work presents a model to explore an online version of a theory from economics. Thereby, it implicitly has limitations due to the fact that a choice was made in the model. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 29}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Our work presents a lot of theorems supported by assumptions (see H2, Theorems 2 and 3...). All of Appendix C is here to provide a theoretical support to these results. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 29}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: Since our work is mostly a theoretic contribution, we do not present experiments. Therefore, our paper is not concerned by this issue. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example   \n(a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 30}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: Since our work is mostly a theoretic contribution, we do not present experiments. Therefore, our paper is not concerned by this issue (same answer as for item 5). ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: Since our work is mostly a theoretic contribution, we do not present experiments. Therefore, our paper is not concerned by this issue (same answer as for item 5). ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 31}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: Since our work is mostly a theoretic contribution, we do not present experiments. Therefore, our paper is not concerned by this issue (same answer as for item 5). ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g., negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 31}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: Since our work is mostly a theoretic contribution, we do not present experiments. Therefore, our paper is not concerned by this issue (same answer as for item 5). ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 32}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: As explained in the Related Works, an issue with externalities is the harm caused to all the parties involved in the considered setting. Here, we try to provide a setup so the players can negociate and achieve a social welfare optimal equilibrium. Therefore, we are confident in the fact that we follow the NeurIPS Code of Ethics. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 32}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: We consider an issue arising in several real-world situations and provide an algorithmic solution to it. We realize that this theoretical contribution still needs some work to be implemented but we hope that it will contribute to positive social impacts in the future. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. ", "page_idx": 32}, {"type": "text", "text": "\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 33}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: Our work is a theoretical contribution. Thereby, it does not propose a release of data or any kind of trained model. We mix learning and theories from economics, which does not involve yet risks for misuse. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 33}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: We do not use any assets requiring such conditions here. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 33}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: We do not present such kind of new assets. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 34}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: We do not incorporate any experiment involving human subjects. Therefore, we are not concerned by this item. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 34}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: We do not incorporate any experiment involving human subjects, hence our answer. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 34}]