[{"figure_path": "pR37AmwbOt/tables/tables_6_1.jpg", "caption": "Table 1: Results of safety alignment experiment. The performance of our models is evaluated in harmful image generation and clean image generation. For harmful image generation, NSFW Score, IP and Hum. Eval. are evaluated. The data on the left of each panel is evaluated on original pre-trained models or contrastive learning fine-tuned models, while the data on the right is the result after the models have been maliciously fine-tuned. Our model shows better safety before and after malicious fine-tuning compared with original SD models for lower NSFW Score and IP. For clean image generation, Aesthetic Score and CLIP Score are evaluated on original pre-trained models or contrastive learning fine-tuned models. Our safety model maintains the quality of clean image generation for fluctuating Aesthetic Score and CLIP Score.", "description": "This table presents a quantitative evaluation of the safety alignment of diffusion models using different methods (baseline, latent transformation (LT), and noise guidance (NG)). It compares the performance before and after malicious fine-tuning, considering metrics like NSFW score (lower is better), inappropriate rate (IP, lower is better), human evaluation (lower is better), aesthetic score (higher is better), and CLIP score (higher is better). The results show the effectiveness of the proposed methods (LT and NG) in improving safety and maintaining clean image quality against malicious fine-tuning.", "section": "4.2 Safety Alignment"}, {"figure_path": "pR37AmwbOt/tables/tables_7_1.jpg", "caption": "Table 2: Results of safety reinforcement experiment. The performance of our models is evaluated in harmful image generation and clean image generation. For harmful image generation, NSFW Score, IP are evaluated. The left and right data are evaluated before and after malicious fine-tuning. Compared with original safe models, our methods show better safety performance after being maliciously fine-tuned for lower NSFW Score and IP. For clean image generation, Aesthetic Score and CLIP Score are evaluated on original pre-trained models or contrastive learning fine-tuned models before malicious fine-tuning. The generation quality of safe reinforcement models is not effected a lot for similar Aesthetic Score and CLIP Score.", "description": "This table presents the results of a safety reinforcement experiment.  It compares the performance of original safe models with models reinforced using the proposed method, both before and after malicious fine-tuning.  The metrics used include NSFW Score, IP (Inappropriate Rate), Aesthetic Score, and CLIP Score, measuring the balance between safety and maintaining the quality of clean image generation.", "section": "4.3 Safety Reinforcement"}, {"figure_path": "pR37AmwbOt/tables/tables_7_2.jpg", "caption": "Table 3: The impact of clean fine-tuning and malicious fine-tuning on the securely reinforced model. Acln and Ahrm represent the change in generation quality before and after ordinary fine-tuning. Clean FT and harmful FT mean fine-tuning with clean images and fine-tuning with harmful images. Compared with Clean FT, Aesthetic Score and CLIP Score show more decrease after harmful fine-tuning, which is evidence of the phenomenon of catastrophic forgetting between clean and harmful data.", "description": "This table presents a comparison of the changes in image generation quality (Aesthetic and CLIP scores) before and after fine-tuning a model with both clean and harmful images. The results show that harmful fine-tuning leads to more significant degradation in the quality of clean image generation than clean fine-tuning, highlighting the effect of catastrophic forgetting.", "section": "4.4 Ablations and Additional Experiments"}, {"figure_path": "pR37AmwbOt/tables/tables_8_1.jpg", "caption": "Table 4: FID Scores evaluated on COCO-30K of different models. The left is the FID scores of different models before malicious fine-tuning and the right is the FID scores of them after malicious fine-tuning. \u0394 shows the decline of clean image generation quality, and \u0394 of our method decreases more compared with base models. Both original SD v1.4 and our safety models show a decline of clean image generation quality, which is evidence that DMs will experience catastrophic forgetting when fine-tuned on datasets for certain specific concepts. Our method refers to NG.", "description": "This table presents FID scores (Fr\u00e9chet Inception Distance) calculated on the COCO-30K dataset, evaluating the quality of clean image generation before and after malicious fine-tuning. Lower FID scores indicate better quality. It compares different models: the original Stable Diffusion (SD) v1.4, SD v1.4 fine-tuned with an ESD-Nudity-u1 model for safety, and the proposed method with safety alignment and reinforcement. The '\u0394' column shows the decrease in FID score (quality drop) after malicious fine-tuning.  The results highlight the impact of malicious fine-tuning and demonstrate that the proposed method mitigates the quality degradation more effectively.", "section": "4.3 Safety Reinforcement"}, {"figure_path": "pR37AmwbOt/tables/tables_8_2.jpg", "caption": "Table 5: IP of different diffusion model safety alignment methods. Our method can achieve a similar level of performance as other methods. Our method refers to NG here. The results are evaluated on I2P nudity dataset.", "description": "This table compares the Inappropriate Rate (IP) of different models designed to prevent the generation of inappropriate images.  It includes several baseline models (ESD-Nudity variants, SLD-Medium, SLD-Max) and the proposed method ('Ours (Safe Alignment)').  The IP metric quantifies the percentage of images incorrectly identified as safe. Lower IP values indicate better safety performance. The table shows that the proposed method achieves comparable safety performance to the state-of-the-art baselines.", "section": "4.3 Safety Reinforcement"}, {"figure_path": "pR37AmwbOt/tables/tables_9_1.jpg", "caption": "Table 6: Performance of our model on combined harmful types of datasets. The performance of our models is evaluated in harmful image generation and clean image generation. For harmful image generation, NSFW Score, IP are evaluated. The data on the left of each panel is evaluated on original pre-trained models or contrastive learning fine-tuned models, while the data on the right is the result after the models have been maliciously fine-tuned. For clean image generation, Aesthetic Score and CLIP Score are evaluated on original pre-trained models or contrastive learning fine-tuned models before malicious fine-tuning. The results show the potential of our methods to erase various harmful concepts.", "description": "This table presents the results of experiments evaluating the model's performance on datasets containing both sexual and violent content.  It compares the NSFW score, inappropriate rate (IP), aesthetic score, and CLIP score before and after malicious fine-tuning, for both the original model and models fine-tuned with the proposed latent transformation (LT) and noise guidance (NG) methods.  The results demonstrate the effectiveness of these methods in preventing the generation of harmful images while maintaining the generation quality of clean images.", "section": "4.4.4 Ablations and Additional Experiments"}, {"figure_path": "pR37AmwbOt/tables/tables_9_2.jpg", "caption": "Table 7: Testing the model of safe alignment on different datasets, which is fine-tuned by NG method. The data above tests the quality of the model in generating clean images, with the metric being aesthetic ratings. The data below pertains to testing the model's ability to generate harmful images, with the metric being the NSFW score.", "description": "This table presents the results of evaluating the safety and image quality of the model on various datasets.  The \"Clean\" section shows the Aesthetic Score (higher is better) for clean image generation on LAION-5B, DiffusionDB, and COCO datasets.  The \"Harmful\" section shows the NSFW Score (lower is better) for harmful image generation on Mistral-7B, I2P, and Unsafe datasets.  The results demonstrate the model's ability to generate high-quality clean images while effectively suppressing the generation of harmful images across different datasets.", "section": "4.4.5 Performance on Different Datasets"}, {"figure_path": "pR37AmwbOt/tables/tables_14_1.jpg", "caption": "Table 8: Results of safety alignment experiment based on Stable Diffusion XL. The performance of our models is evaluated in harmful image generation and clean image generation. For harmful image generation, NSFW Score, IP and Hum. Eval. are evaluated. The data on the left of each panel is evaluated on original pre-trained models or contrastive learning fine-tuned models, while the data on the right is the result after the models have been maliciously fine-tuned. Our model shows better safety before and after malicious fine-tuning compared with original SD XL model for lower NSFW Score and IP. For clean image generation, Aesthetic Score and CLIP Score are evaluated on original pre-trained models or contrastive learning fine-tuned models. Our safety model maintains the quality of the clean image generation for fluctuating Aesthetic Score and CLIP Score.", "description": "This table presents the results of a safety alignment experiment using Stable Diffusion XL.  It compares the performance of the original model and models fine-tuned with the proposed method (LT and NG) before and after malicious fine-tuning. The metrics evaluated include NSFW Score, IP, Aesthetic Score, and CLIP Score, assessing both the generation of harmful and clean images.  The results demonstrate the effectiveness of the method in improving the safety and maintaining the quality of clean image generation.", "section": "4.2 Safety Alignment"}, {"figure_path": "pR37AmwbOt/tables/tables_14_2.jpg", "caption": "Table 9: Results of our strongly safe aligned model after different steps of malicious fine-tuning. IP after 200 steps of malicious fine-tuning does not exceed 4%, which shows the robustness against malicious fine-tuning.", "description": "This table presents the results of evaluating the model's robustness against malicious fine-tuning by varying the number of malicious fine-tuning steps. The Inappropriate Rate (IP), a measure of the model's safety, is assessed after 0, 20, 100, and 200 malicious fine-tuning steps.  The results show that even after 200 steps of malicious fine-tuning, the IP remains below 4%, indicating that the model effectively resists malicious attempts to make it generate harmful images.", "section": "4.3 Safety Reinforcement"}, {"figure_path": "pR37AmwbOt/tables/tables_15_1.jpg", "caption": "Table 10: The impact of adding different noise on model performance. The results on the left show the performance of the models fine-tuned with contrastive learning by adding the corresponding type of noise, while the results on the right show the performance of the models after malicious fine-tuning.", "description": "This table presents the results of an experiment comparing two noise-adding methods (fixed noise and changing noise) within two different training scenarios (safety alignment and safety reinforcement).  It shows the NSFW scores (lower is better, indicating fewer unsafe images) and Aesthetic scores (higher is better, indicating better image quality) before and after malicious fine-tuning. The results indicate the effectiveness of the noise methods in improving model safety.", "section": "4.4.3 Ways to Guide the Added Noise"}, {"figure_path": "pR37AmwbOt/tables/tables_18_1.jpg", "caption": "Table 11: Results of attacking our safe model by UnlearnDiffAtk. The results show that our model can resist the attack by UnlearnDiffAtk. Our model is trained by NG method based on SD v1.4.", "description": "This table presents the results of attacking a safety-aligned diffusion model using the UnlearnDiffAtk method.  It compares the attack success rate (ASR) of the proposed model against three other unlearned diffusion models (ESD, FMN, SLD). The lower ASR values indicate a stronger resilience to attacks. The proposed model demonstrates a significantly lower ASR when attacked, indicating better safety and robustness against malicious fine-tuning.", "section": "F Results of Attacking our Safe Model with UnlearnDiffAtk"}]