{"references": [{"fullname_first_author": "A. Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-01", "reason": "This paper introduces CLIP, a foundational model for aligning image and text features, which is extensively used in this paper's methodology and baselines."}, {"fullname_first_author": "A. Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2020-10-26", "reason": "This paper introduces the Vision Transformer (ViT) architecture, a crucial backbone used in the paper's experiments for image feature extraction."}, {"fullname_first_author": "P. W. Koh", "paper_title": "Concept bottleneck models", "publication_date": "2020-07-01", "reason": "This paper introduces Concept Bottleneck Models (CBMs), the core interpretable model framework on which the proposed Knowledge-enhanced Bottlenecks (KnoBo) are built."}, {"fullname_first_author": "J. Irvin", "paper_title": "Chexpert: A large chest radiograph dataset with uncertainty labels and expert comparison", "publication_date": "2019-01-01", "reason": "This paper introduces CheXpert, a key dataset used in the paper's experiments for evaluating model performance on chest X-ray images."}, {"fullname_first_author": "P. Tschandl", "paper_title": "The HAM10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions", "publication_date": "2018-01-01", "reason": "This paper introduces the HAM10000 dataset, another crucial dataset utilized for evaluating model performance on skin lesion images."}]}