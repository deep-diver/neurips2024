[{"figure_path": "8iytZCnXIu/tables/tables_5_1.jpg", "caption": "Table 1: Overview of BricksRL Robot and Environment Settings. Environments marked with an asterisk (*) utilize LEGO sensors and image inputs as observations for the agent. Environments indicated by a dagger (\u2020) denote simulations of the real robot and do not use the real robot for training.", "description": "This table shows the robots used in the experiments and the environments they were tested in.  The asterisk (*) indicates environments that use both LEGO sensors and image data, while the dagger (\u2020) indicates simulated environments that do not involve the physical robot.", "section": "4 Experiments"}, {"figure_path": "8iytZCnXIu/tables/tables_5_2.jpg", "caption": "Table 2: The table displays the mean and standard deviation of evaluation rewards for the trained TD3, SAC, DroQ algorithms, and a random policy, based on experiments conducted across 5 evaluation episodes and 5 different seeds.", "description": "This table presents the mean and standard deviation of the evaluation rewards obtained for four different reinforcement learning algorithms (TD3, SAC, DroQ) and a random policy.  The results are averaged across five evaluation episodes and five different random seeds for each algorithm. The table shows the performance of each algorithm on different robotic tasks and environments described in the paper, allowing for a comparison of their effectiveness.  Note that the environments are also grouped by robot type.", "section": "4 Experiments"}, {"figure_path": "8iytZCnXIu/tables/tables_6_1.jpg", "caption": "Table 3: Comparison of success rates for different agents in the RoboArm-v0 and RoboArm-mixed-v0 environments. Success is defined as the agent reaching the goal or goal position within a specified threshold. Agents marked with an asterisk (*) were initially trained in the RoboArmSim-v0 environment. Each algorithm was evaluated for 5 epochs with 5 different seeds, totaling 25 experiments per agent and task.", "description": "This table presents the success rates of different reinforcement learning algorithms (TD3, SAC, DroQ, and a random policy) on two RoboArm tasks: RoboArm-v0 (using only LEGO sensors) and RoboArm-mixed-v0 (incorporating a webcam).  Success is measured by reaching the goal position within a time limit. The table also shows results for agents pre-trained in a simulated environment (RoboArmSim-v0), highlighting the potential of sim-to-real transfer.", "section": "4 Experiments"}, {"figure_path": "8iytZCnXIu/tables/tables_8_1.jpg", "caption": "Table 2: The table displays the mean and standard deviation of evaluation rewards for the trained TD3, SAC, DroQ algorithms, and a random policy, based on experiments conducted across 5 evaluation episodes and 5 different seeds.", "description": "This table presents the mean and standard deviation of the evaluation rewards obtained for different reinforcement learning algorithms (TD3, SAC, DroQ) and a random policy.  The results are based on experiments conducted across five evaluation episodes and five different random seeds, providing a measure of the algorithms' performance and variability.", "section": "4 Experiments"}, {"figure_path": "8iytZCnXIu/tables/tables_12_1.jpg", "caption": "Table 5: Combined action and observation specifications for the RunAway-v0 environment.", "description": "This table shows the specifications for actions and observations used in the RunAway-v0 environment.  Actions consist of a single motor control value (ranging from -1 to 1), and observations consist of left and right motor angles (0.0 to 360.0 degrees), pitch angle (-90.0 to 90.0 degrees), roll angle (-90.0 to 90.0 degrees), and distance from an ultrasonic sensor (0.0 to 2000.0 mm).", "section": "A.2.1 RunAway-v0"}, {"figure_path": "8iytZCnXIu/tables/tables_13_1.jpg", "caption": "Table 6: Combined action and observation specifications for the Spinning-v0 environment.", "description": "This table details the specifications for actions and observations within the Spinning-v0 environment.  It shows the minimum and maximum values for each parameter, including left and right motor angles, pitch and roll angles (obtained from the robot's IMU), angular velocity (wz), and the direction of rotation.  The action space is continuous, defined by two floating-point values representing the rotation angles applied to the left and right motors. Note that the values are initially within the [-1,1] range, then transformed to [-100, 100] before being applied to the motors.", "section": "3.2 Environment"}, {"figure_path": "8iytZCnXIu/tables/tables_14_1.jpg", "caption": "Table 7: Combined action and observation specifications for the Walker-v0 environment.", "description": "This table details the specifications for actions and observations within the Walker-v0 environment.  For actions, it lists the motor controls (left front, right front, left back, right back) and their ranges.  The observation section lists the motor angles, pitch, roll, and distance readings and their associated ranges.  These specifications define the input and output data exchanged between the RL agent and the simulated environment during training.", "section": "A.2.3 Walker-v0"}, {"figure_path": "8iytZCnXIu/tables/tables_15_1.jpg", "caption": "Table 8: Combined action and observation specifications for the RoboArm-v0 environment.", "description": "This table details the specifications for actions and observations within the RoboArm-v0 environment.  For actions, it lists the type of motor (rotation, low, high, grab), its numerical index, and the minimum and maximum values. For observations, it shows the motor angles (current and goal) and their corresponding ranges for each motor type.", "section": "3.3 LEGO Robots"}, {"figure_path": "8iytZCnXIu/tables/tables_16_1.jpg", "caption": "Table 9: Combined action and observation specifications for the RoboArm_mixed-v0 environment.", "description": "This table presents the specifications for actions and observations in the RoboArm-mixed-v0 environment.  Actions consist of three continuous values controlling the rotation, low, and high motors. Observations include these three motor angles and an image observation with dimensions (64, 64).  The minimum and maximum values for each specification are given.", "section": "3.3 LEGO Robots"}, {"figure_path": "8iytZCnXIu/tables/tables_21_1.jpg", "caption": "Table 10: Hyperparameter for the agents DroQ, SAC, and TD3", "description": "This table lists the hyperparameters used for training the three reinforcement learning agents: DroQ, SAC, and TD3.  The parameters include learning rate, batch size, UTD ratio (for DroQ only), prefill episodes, number of cells in the network, gamma, soft update epsilon, alpha initial (for SAC only), whether alpha is fixed, normalization method, dropout rate, buffer size, and exploration noise (for TD3 only). These hyperparameters were used to fine-tune the model and may affect the final results of the experiment.", "section": "4 Experiments"}, {"figure_path": "8iytZCnXIu/tables/tables_21_2.jpg", "caption": "Table 11: Dataset Statistics", "description": "This table summarizes the statistics of the datasets used in the experiments.  For each task (Walker-v0, RoboArm-v0, RunAway-v0, Spinning-v0), it shows the mean reward obtained by an expert policy, the number of expert transitions collected, the number of random transitions collected and the number of episodes used in data collection. This provides information on the quality of the datasets and the amount of data available for training reinforcement learning models.", "section": "4 Experiments"}, {"figure_path": "8iytZCnXIu/tables/tables_22_1.jpg", "caption": "Table 12: Hyperparameter for the agents BC, IQL, and CQL", "description": "This table lists the hyperparameters used for training three offline reinforcement learning algorithms: Behavior Cloning (BC), Implicit Q-Learning (IQL), and Conservative Q-Learning (CQL).  It shows the settings for parameters such as learning rate, batch size, number of cells in the network architecture, gamma (discount factor), soft update epsilon, loss function, temperature (for IQL and CQL),  expectile (for IQL), minimum and maximum Q-weight,  whether deterministic backup and the use of Lagrange were applied, and other regularization parameters.  These hyperparameters were used to tune the performance of each algorithm during offline training.", "section": "4.4 Offline Training"}, {"figure_path": "8iytZCnXIu/tables/tables_22_2.jpg", "caption": "Table 1: Overview of BricksRL Robot and Environment Settings. Environments marked with an asterisk (*) utilize LEGO sensors and image inputs as observations for the agent. Environments indicated by a dagger (\u2020) denote simulations of the real robot and do not use the real robot for training.", "description": "This table shows the robots used in the experiments and their corresponding environments.  It indicates whether each environment uses the actual robot or a simulation, and highlights those utilizing additional LEGO sensors (marked with *) or incorporating image data as observations. The dagger symbol (\u2020) denotes that only a simulation of the robot was used for training in that environment.", "section": "4 Experiments"}]